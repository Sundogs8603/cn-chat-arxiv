<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#40065;&#26834;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#26426;&#22120;&#20154;&#33258;&#20027;&#28165;&#27905;&#31995;&#32479;&#20013;&#30340;&#28151;&#21512;&#20219;&#21153;&#20998;&#37197;&#38382;&#39064;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#21253;&#25324;100&#20010;&#23454;&#20363;&#30340;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2303.06531</link><description>&lt;p&gt;
&#38754;&#21521;&#33258;&#20027;&#28165;&#27905;&#30340;&#22810;&#26426;&#22120;&#20154;&#28151;&#21512;&#20219;&#21153;&#20998;&#37197;&#30340;&#23454;&#36341;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Practical Multi-Robot Hybrid Tasks Allocation for Autonomous Cleaning. (arXiv:2303.06531v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06531
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#40065;&#26834;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#26426;&#22120;&#20154;&#33258;&#20027;&#28165;&#27905;&#31995;&#32479;&#20013;&#30340;&#28151;&#21512;&#20219;&#21153;&#20998;&#37197;&#38382;&#39064;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#21253;&#25324;100&#20010;&#23454;&#20363;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel robust mixed-integer linear programming model for multi-robot hybrid-task allocation in uncertain autonomous cleaning systems, and establishes a dataset of 100 instances.
&lt;/p&gt;
&lt;p&gt;
&#20219;&#21153;&#20998;&#37197;&#22312;&#22810;&#26426;&#22120;&#20154;&#33258;&#20027;&#28165;&#27905;&#31995;&#32479;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#22810;&#20010;&#26426;&#22120;&#20154;&#19968;&#36215;&#24037;&#20316;&#20197;&#28165;&#27905;&#22823;&#38754;&#31215;&#21306;&#22495;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#30456;&#20851;&#30740;&#31350;&#23384;&#22312;&#20960;&#20010;&#38382;&#39064;&#12290;&#22823;&#22810;&#25968;&#24403;&#21069;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#20110;&#30830;&#23450;&#24615;&#30340;&#21333;&#20219;&#21153;&#20998;&#37197;&#65292;&#32780;&#19981;&#32771;&#34385;&#19981;&#30830;&#23450;&#24037;&#20316;&#29615;&#22659;&#20013;&#30340;&#28151;&#21512;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#32570;&#20047;&#30456;&#20851;&#30740;&#31350;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#20026;&#19981;&#30830;&#23450;&#30340;&#33258;&#20027;&#28165;&#27905;&#31995;&#32479;&#30340;&#22810;&#26426;&#22120;&#20154;&#28151;&#21512;&#20219;&#21153;&#20998;&#37197;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#40065;&#26834;&#20248;&#21270;&#27169;&#22411;&#26469;&#24314;&#27169;&#28165;&#27905;&#29615;&#22659;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#40065;&#26834;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#27169;&#22411;&#65292;&#20854;&#20013;&#21253;&#25324;&#28151;&#21512;&#28165;&#27905;&#20219;&#21153;&#39034;&#24207;&#21644;&#26426;&#22120;&#20154;&#30340;&#33021;&#21147;&#31561;&#23454;&#38469;&#32422;&#26463;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;100&#20010;&#23454;&#20363;&#65292;&#27599;&#20010;&#23454;&#20363;&#37117;&#26377;2D&#25163;&#21160;&#26631;&#35760;&#30340;&#22270;&#20687;&#21644;3D&#27169;&#22411;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#25152;&#25552;&#20986;&#27169;&#22411;&#30340;&#20840;&#38754;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Task allocation plays a vital role in multi-robot autonomous cleaning systems, where multiple robots work together to clean a large area. However, there are several problems in relevant research to date. Most current studies mainly focus on deterministic, single-task allocation for cleaning robots, without considering hybrid tasks in uncertain working environments. Moreover, there is a lack of datasets and benchmarks for relevant research. In this paper, we contribute to multi-robot hybrid-task allocation for uncertain autonomous cleaning systems by addressing these problems. First, we model the uncertainties in the cleaning environment via robust optimization and propose a novel robust mixed-integer linear programming model with practical constraints including hybrid cleaning task order and robot's ability. Second, we establish a dataset of 100 instances made from floor plans, each of which has 2D manually-labeled images and a 3D model. Third, we provide comprehensive results on the c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20351;&#29992;&#25209;&#26631;&#20934;&#21270;&#21644;&#32676;&#32452;&#24402;&#19968;&#21270;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;&#22312;&#36866;&#24403;&#30340;&#22788;&#29702;&#19979;&#65292;&#25209;&#26631;&#20934;&#21270;&#21487;&#20197;&#22312;&#24191;&#27867;&#30340;&#32852;&#37030;&#23398;&#20064;&#35774;&#32622;&#20013;&#20855;&#26377;&#24456;&#39640;&#30340;&#31454;&#20105;&#21147;&#65292;&#32780;&#19988;&#36825;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#25110;&#36890;&#20449;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2303.06530</link><description>&lt;p&gt;
&#22312;&#32852;&#37030;&#28145;&#24230;&#23398;&#20064;&#20013;&#20248;&#21270;&#25209;&#26631;&#20934;&#21270;
&lt;/p&gt;
&lt;p&gt;
Making Batch Normalization Great in Federated Deep Learning. (arXiv:2303.06530v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06530
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20351;&#29992;&#25209;&#26631;&#20934;&#21270;&#21644;&#32676;&#32452;&#24402;&#19968;&#21270;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;&#22312;&#36866;&#24403;&#30340;&#22788;&#29702;&#19979;&#65292;&#25209;&#26631;&#20934;&#21270;&#21487;&#20197;&#22312;&#24191;&#27867;&#30340;&#32852;&#37030;&#23398;&#20064;&#35774;&#32622;&#20013;&#20855;&#26377;&#24456;&#39640;&#30340;&#31454;&#20105;&#21147;&#65292;&#32780;&#19988;&#36825;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#25110;&#36890;&#20449;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the use of batch normalization and group normalization in federated learning, and finds that with proper treatments, batch normalization can be highly competitive across a wide range of federated learning settings, and this requires no additional training or communication costs.
&lt;/p&gt;
&lt;p&gt;
&#25209;&#26631;&#20934;&#21270;&#65288;BN&#65289;&#36890;&#24120;&#29992;&#20110;&#29616;&#20195;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#20013;&#65292;&#20197;&#25552;&#39640;&#31283;&#23450;&#24615;&#24182;&#21152;&#36895;&#38598;&#20013;&#24335;&#35757;&#32451;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#22312;&#20855;&#26377;&#38750;IID&#20998;&#25955;&#25968;&#25454;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20013;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#35266;&#23519;&#21040;&#20351;&#29992;BN&#36827;&#34892;&#35757;&#32451;&#21487;&#33021;&#20250;&#30001;&#20110;&#35757;&#32451;&#21644;&#27979;&#35797;&#20043;&#38388;&#30340;BN&#32479;&#35745;&#19981;&#21305;&#37197;&#32780;&#38459;&#30861;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#32676;&#32452;&#24402;&#19968;&#21270;&#65288;GN&#65289;&#26356;&#24120;&#29992;&#20110;FL&#20316;&#20026;BN&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#25105;&#20204;&#22312;&#21508;&#31181;FL&#35774;&#32622;&#19979;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;BN&#21644;GN&#20043;&#38388;&#27809;&#26377;&#19968;&#33268;&#30340;&#20248;&#32988;&#32773;&#12290;&#36825;&#20419;&#20351;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;FL&#20013;&#24402;&#19968;&#21270;&#23618;&#30340;&#20351;&#29992;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#36866;&#24403;&#30340;&#22788;&#29702;&#19979;&#65292;BN&#21487;&#20197;&#22312;&#24191;&#27867;&#30340;FL&#35774;&#32622;&#20013;&#20855;&#26377;&#24456;&#39640;&#30340;&#31454;&#20105;&#21147;&#65292;&#32780;&#19988;&#36825;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#25110;&#36890;&#20449;&#25104;&#26412;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#30740;&#31350;&#21487;&#20197;&#25104;&#20026;FL&#26410;&#26469;&#23454;&#38469;&#20351;&#29992;&#21644;&#29702;&#35770;&#20998;&#26512;&#30340;&#26377;&#20215;&#20540;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;
Batch Normalization (BN) is commonly used in modern deep neural networks (DNNs) to improve stability and speed up convergence during centralized training. In federated learning (FL) with non-IID decentralized data, previous works observed that training with BN could hinder performance due to the mismatch of the BN statistics between training and testing. Group Normalization (GN) is thus more often used in FL as an alternative to BN. However, from our empirical study across various FL settings, we see no consistent winner between BN and GN. This leads us to revisit the use of normalization layers in FL. We find that with proper treatments, BN can be highly competitive across a wide range of FL settings, and this requires no additional training or communication costs. We hope that our study could serve as a valuable reference for future practical usage and theoretical analysis in FL.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#35745;&#31639;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20998;&#31867;&#20013;Shap&#35299;&#37322;&#20998;&#25968;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20108;&#36827;&#21046;&#31070;&#32463;&#32593;&#32476;&#36716;&#25442;&#20026;&#24067;&#23572;&#30005;&#36335;&#65292;&#24182;&#20351;&#29992;&#30693;&#35782;&#32534;&#35793;&#25216;&#26415;&#65292;&#23558;&#30005;&#36335;&#35270;&#20026;&#24320;&#25918;&#24335;&#27169;&#22411;&#65292;&#36890;&#36807;&#26368;&#36817;&#30340;&#39640;&#25928;&#31639;&#27861;&#35745;&#31639;Shap&#20998;&#25968;&#65292;&#30456;&#27604;&#20110;&#23558;BNN&#35270;&#20026;&#40657;&#30418;&#27169;&#22411;&#30452;&#25509;&#35745;&#31639;Shap&#65292;&#24615;&#33021;&#26377;&#20102;&#26174;&#33879;&#30340;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2303.06516</link><description>&lt;p&gt;
&#25171;&#24320;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#20197;&#35745;&#31639;Shap&#20998;&#25968;
&lt;/p&gt;
&lt;p&gt;
Opening Up the Neural Network Classifier for Shap Score Computation. (arXiv:2303.06516v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#35745;&#31639;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20998;&#31867;&#20013;Shap&#35299;&#37322;&#20998;&#25968;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20108;&#36827;&#21046;&#31070;&#32463;&#32593;&#32476;&#36716;&#25442;&#20026;&#24067;&#23572;&#30005;&#36335;&#65292;&#24182;&#20351;&#29992;&#30693;&#35782;&#32534;&#35793;&#25216;&#26415;&#65292;&#23558;&#30005;&#36335;&#35270;&#20026;&#24320;&#25918;&#24335;&#27169;&#22411;&#65292;&#36890;&#36807;&#26368;&#36817;&#30340;&#39640;&#25928;&#31639;&#27861;&#35745;&#31639;Shap&#20998;&#25968;&#65292;&#30456;&#27604;&#20110;&#23558;BNN&#35270;&#20026;&#40657;&#30418;&#27169;&#22411;&#30452;&#25509;&#35745;&#31639;Shap&#65292;&#24615;&#33021;&#26377;&#20102;&#26174;&#33879;&#30340;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an efficient method for computing Shap explanation scores in machine learning model classification by transforming binary neural networks into Boolean circuits and treating the resulting circuit as an open-box model, which leads to a significant improvement in performance compared to computing Shap directly on the BNN treated as a black-box model.
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#20998;&#31867;&#30340;Shap&#35299;&#37322;&#20998;&#25968;&#30340;&#39640;&#25928;&#35745;&#31639;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23558;&#20108;&#36827;&#21046;&#31070;&#32463;&#32593;&#32476;&#65288;BNN&#65289;&#36716;&#25442;&#20026;&#30830;&#23450;&#24615;&#21644;&#21487;&#20998;&#35299;&#30340;&#24067;&#23572;&#30005;&#36335;&#65292;&#20351;&#29992;&#30693;&#35782;&#32534;&#35793;&#25216;&#26415;&#12290;&#25152;&#24471;&#21040;&#30340;&#30005;&#36335;&#34987;&#35270;&#20026;&#24320;&#25918;&#24335;&#27169;&#22411;&#65292;&#36890;&#36807;&#26368;&#36817;&#30340;&#39640;&#25928;&#31639;&#27861;&#35745;&#31639;Shap&#20998;&#25968;&#12290;&#35814;&#32454;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#23558;BNN&#35270;&#20026;&#40657;&#30418;&#27169;&#22411;&#30452;&#25509;&#35745;&#31639;Shap&#30456;&#27604;&#65292;&#24615;&#33021;&#26377;&#20102;&#26174;&#33879;&#30340;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address the problem of efficiently computing Shap explanation scores for classifications with machine learning models. With this goal, we show the transformation of binary neural networks (BNNs) for classification into deterministic and decomposable Boolean circuits, for which knowledge compilation techniques are used. The resulting circuit is treated as an open-box model, to compute Shap scores by means of a recent efficient algorithm for this class of circuits. Detailed experiments show a considerable gain in performance in comparison with computing Shap directly on the BNN treated as a black-box model.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#22686;&#24378;&#22411;&#38543;&#26426;&#26862;&#26519;&#20998;&#31867;&#22120;&#21644;&#21512;&#25104;&#23569;&#25968;&#31867;&#36807;&#37319;&#26679;&#25216;&#26415;&#26469;&#35299;&#20915;&#20449;&#29992;&#21345;&#27450;&#35784;&#26816;&#27979;&#20013;&#30340;&#19981;&#24179;&#34913;&#25968;&#25454;&#38382;&#39064;&#65292;&#33719;&#24471;&#20102;98%&#30340;&#20934;&#30830;&#24230;&#21644;F1&#20998;&#25968;&#20540;&#65292;&#20855;&#26377;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2303.06514</link><description>&lt;p&gt;
&#20351;&#29992;&#22686;&#24378;&#22411;&#38543;&#26426;&#26862;&#26519;&#20998;&#31867;&#22120;&#26816;&#27979;&#19981;&#24179;&#34913;&#25968;&#25454;&#20013;&#30340;&#20449;&#29992;&#21345;&#27450;&#35784;
&lt;/p&gt;
&lt;p&gt;
Credit Card Fraud Detection Using Enhanced Random Forest Classifier for Imbalanced Data. (arXiv:2303.06514v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06514
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#22686;&#24378;&#22411;&#38543;&#26426;&#26862;&#26519;&#20998;&#31867;&#22120;&#21644;&#21512;&#25104;&#23569;&#25968;&#31867;&#36807;&#37319;&#26679;&#25216;&#26415;&#26469;&#35299;&#20915;&#20449;&#29992;&#21345;&#27450;&#35784;&#26816;&#27979;&#20013;&#30340;&#19981;&#24179;&#34913;&#25968;&#25454;&#38382;&#39064;&#65292;&#33719;&#24471;&#20102;98%&#30340;&#20934;&#30830;&#24230;&#21644;F1&#20998;&#25968;&#20540;&#65292;&#20855;&#26377;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an enhanced random forest classifier and synthetic minority over-sampling technique to address the issue of imbalanced data in credit card fraud detection, achieving an accuracy of 98% and F1-score value of about 98%, with potential practical applications.
&lt;/p&gt;
&lt;p&gt;
&#20449;&#29992;&#21345;&#24050;&#25104;&#20026;&#22312;&#32447;&#21644;&#31163;&#32447;&#20132;&#26131;&#20013;&#26368;&#27969;&#34892;&#30340;&#25903;&#20184;&#26041;&#24335;&#12290;&#38543;&#30528;&#25216;&#26415;&#30340;&#21457;&#23637;&#21644;&#27450;&#35784;&#26696;&#20214;&#30340;&#22686;&#21152;&#65292;&#21019;&#24314;&#27450;&#35784;&#26816;&#27979;&#31639;&#27861;&#20197;&#31934;&#30830;&#35782;&#21035;&#21644;&#20572;&#27490;&#27450;&#35784;&#27963;&#21160;&#30340;&#24517;&#35201;&#24615;&#20063;&#38543;&#20043;&#20135;&#29983;&#12290;&#26412;&#25991;&#23454;&#29616;&#20102;&#38543;&#26426;&#26862;&#26519;&#65288;RF&#65289;&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#20102;&#19968;&#32452;&#20449;&#29992;&#21345;&#20132;&#26131;&#25968;&#25454;&#38598;&#12290;&#22312;&#22788;&#29702;&#20449;&#29992;&#21345;&#27450;&#35784;&#26816;&#27979;&#26102;&#30340;&#20027;&#35201;&#38382;&#39064;&#26159;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#20132;&#26131;&#37117;&#26159;&#38750;&#27450;&#35784;&#20132;&#26131;&#12290;&#20026;&#20102;&#20811;&#26381;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#65292;&#20351;&#29992;&#20102;&#21512;&#25104;&#23569;&#25968;&#31867;&#36807;&#37319;&#26679;&#25216;&#26415;&#65288;SMOTE&#65289;&#12290;&#23454;&#29616;&#36229;&#21442;&#25968;&#25216;&#26415;&#20197;&#22686;&#24378;&#38543;&#26426;&#26862;&#26519;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;RF&#20998;&#31867;&#22120;&#33719;&#24471;&#20102;98&#65285;&#30340;&#20934;&#30830;&#24230;&#21644;&#32422;98&#65285;&#30340;F1&#20998;&#25968;&#20540;&#65292;&#36825;&#26159;&#20196;&#20154;&#20852;&#22859;&#30340;&#12290;&#25105;&#20204;&#36824;&#30456;&#20449;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#30456;&#23545;&#23481;&#26131;&#24212;&#29992;&#65292;&#24182;&#19988;&#21487;&#20197;&#20811;&#26381;&#20449;&#29992;&#21345;&#27450;&#35784;&#26816;&#27979;&#20013;&#19981;&#24179;&#34913;&#25968;&#25454;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
The credit card has become the most popular payment method for both online and offline transactions. The necessity to create a fraud detection algorithm to precisely identify and stop fraudulent activity arises as a result of both the development of technology and the rise in fraud cases. This paper implements the random forest (RF) algorithm to solve the issue in the hand. A dataset of credit card transactions was used in this study. The main problem when dealing with credit card fraud detection is the imbalanced dataset in which most of the transaction are non-fraud ones. To overcome the problem of the imbalanced dataset, the synthetic minority over-sampling technique (SMOTE) was used. Implementing the hyperparameters technique to enhance the performance of the random forest classifier. The results showed that the RF classifier gained an accuracy of 98% and about 98% of F1-score value, which is promising. We also believe that our model is relatively easy to apply and can overcome the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#32479;&#35745;&#21644;&#31616;&#21333;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#39044;&#27979;&#20840;&#29699;&#24179;&#22343;&#28201;&#24230;&#30340;&#20248;&#21183;&#65292;&#21457;&#29616;&#25968;&#25454;&#36716;&#25442;&#25216;&#26415;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.06468</link><description>&lt;p&gt;
&#36890;&#36807;&#25968;&#25454;&#36716;&#25442;&#25216;&#26415;&#20934;&#30830;&#39044;&#27979;&#20840;&#29699;&#24179;&#22343;&#28201;&#24230;
&lt;/p&gt;
&lt;p&gt;
Accurate Prediction of Global Mean Temperature through Data Transformation Techniques. (arXiv:2303.06468v1 [stat.AP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06468
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#32479;&#35745;&#21644;&#31616;&#21333;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#39044;&#27979;&#20840;&#29699;&#24179;&#22343;&#28201;&#24230;&#30340;&#20248;&#21183;&#65292;&#21457;&#29616;&#25968;&#25454;&#36716;&#25442;&#25216;&#26415;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper examines the advantage of using statistical and simpler machine learning methods to predict global mean temperature, and finds that data transformation techniques can improve prediction accuracy.
&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#20840;&#29699;&#24179;&#22343;&#28201;&#24230;&#65288;GMT&#65289;&#22312;&#26410;&#26469;&#20960;&#21313;&#24180;&#20869;&#30340;&#21464;&#21270;&#36235;&#21183;&#38750;&#24120;&#37325;&#35201;&#12290;&#39044;&#27979;&#21382;&#21490;&#25968;&#25454;&#26159;&#23454;&#29616;&#38271;&#26399;&#39044;&#27979;&#30446;&#26631;&#30340;&#24517;&#35201;&#31532;&#19968;&#27493;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#32479;&#35745;&#21644;&#31616;&#21333;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#27861;&#30340;&#20248;&#21183;&#65292;&#32780;&#19981;&#26159;&#30452;&#25509;&#20351;&#29992;&#22797;&#26434;&#30340;ML&#31639;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#12290;&#22312;&#24212;&#29992;&#19981;&#21516;&#31639;&#27861;&#20043;&#21069;&#65292;&#32463;&#24120;&#34987;&#24573;&#35270;&#30340;&#25968;&#25454;&#36716;&#25442;&#26041;&#27861;&#34987;&#29992;&#20316;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#25163;&#27573;&#12290;GMT&#26102;&#38388;&#24207;&#21015;&#26082;&#34987;&#35270;&#20026;&#21333;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#65292;&#20063;&#34987;&#35270;&#20026;&#22238;&#24402;&#38382;&#39064;&#12290;&#21457;&#29616;&#19968;&#20123;&#25968;&#25454;&#36716;&#25442;&#27493;&#39588;&#26159;&#26377;&#25928;&#30340;&#12290;&#21508;&#31181;&#31616;&#21333;&#30340;ML&#26041;&#27861;&#34920;&#29616;&#24471;&#21644;&#26356;&#20026;&#30693;&#21517;&#30340;&#26041;&#27861;&#19968;&#26679;&#22909;&#29978;&#33267;&#26356;&#22909;&#65292;&#36825;&#34920;&#26126;&#23581;&#35797;&#22823;&#37327;&#31639;&#27861;&#20316;&#20026;&#31532;&#19968;&#27493;&#26159;&#26377;&#20215;&#20540;&#30340;&#12290;56&#31181;&#31639;&#27861;&#32463;&#36807;Box-Cox&#12289;Yeo-Johnson&#21644;&#19968;&#38454;&#24046;&#20998;&#22788;&#29702;&#21518;&#36827;&#34892;&#27604;&#36739;&#65292;&#19982;&#27809;&#26377;&#36827;&#34892;&#22788;&#29702;&#30340;&#24773;&#20917;&#36827;&#34892;&#27604;&#36739;&#12290;&#39044;&#27979;&#32467;&#26524;&#34920;&#26126;&#65292;&#25968;&#25454;&#36716;&#25442;&#25216;&#26415;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is important to predict how the Global Mean Temperature (GMT) will evolve in the next few decades. The ability to predict historical data is a necessary first step toward the actual goal of making long-range forecasts. This paper examines the advantage of statistical and simpler Machine Learning (ML) methods instead of directly using complex ML algorithms and Deep Learning Neural Networks (DNN). Often neglected data transformation methods prior to applying different algorithms have been used as a means of improving predictive accuracy. The GMT time series is treated both as a univariate time series and also cast as a regression problem. Some steps of data transformations were found to be effective. Various simple ML methods did as well or better than the more well-known ones showing merit in trying a large bouquet of algorithms as a first step. Fifty-six algorithms were subject to Box-Cox, Yeo-Johnson, and first-order differencing and compared with the absence of them. Predictions f
&lt;/p&gt;</description></item><item><title>ZeroNLG&#26159;&#19968;&#20010;&#38646;&#26679;&#26412;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#22788;&#29702;&#22810;&#20010;NLG&#20219;&#21153;&#65292;&#21253;&#25324;&#22270;&#20687;&#21040;&#25991;&#26412;&#12289;&#35270;&#39057;&#21040;&#25991;&#26412;&#21644;&#25991;&#26412;&#21040;&#25991;&#26412;&#65292;&#36328;&#36234;&#33521;&#35821;&#12289;&#20013;&#25991;&#12289;&#24503;&#35821;&#21644;&#27861;&#35821;&#12290;&#23427;&#19981;&#38656;&#35201;&#20219;&#20309;&#26631;&#35760;&#30340;&#19979;&#28216;&#23545;&#36827;&#34892;&#35757;&#32451;&#65292;&#36890;&#36807;&#23558;&#19981;&#21516;&#30340;&#39046;&#22495;&#25237;&#24433;&#21040;&#20849;&#20139;&#30340;&#20844;&#20849;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#30456;&#24212;&#22352;&#26631;&#65292;&#26725;&#25509;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2303.06458</link><description>&lt;p&gt;
ZeroNLG: &#23558;&#39046;&#22495;&#23545;&#40784;&#21644;&#33258;&#32534;&#30721;&#29992;&#20110;&#38646;&#26679;&#26412;&#22810;&#27169;&#24577;&#21644;&#22810;&#35821;&#35328;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
ZeroNLG: Aligning and Autoencoding Domains for Zero-Shot Multimodal and Multilingual Natural Language Generation. (arXiv:2303.06458v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06458
&lt;/p&gt;
&lt;p&gt;
ZeroNLG&#26159;&#19968;&#20010;&#38646;&#26679;&#26412;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#22788;&#29702;&#22810;&#20010;NLG&#20219;&#21153;&#65292;&#21253;&#25324;&#22270;&#20687;&#21040;&#25991;&#26412;&#12289;&#35270;&#39057;&#21040;&#25991;&#26412;&#21644;&#25991;&#26412;&#21040;&#25991;&#26412;&#65292;&#36328;&#36234;&#33521;&#35821;&#12289;&#20013;&#25991;&#12289;&#24503;&#35821;&#21644;&#27861;&#35821;&#12290;&#23427;&#19981;&#38656;&#35201;&#20219;&#20309;&#26631;&#35760;&#30340;&#19979;&#28216;&#23545;&#36827;&#34892;&#35757;&#32451;&#65292;&#36890;&#36807;&#23558;&#19981;&#21516;&#30340;&#39046;&#22495;&#25237;&#24433;&#21040;&#20849;&#20139;&#30340;&#20844;&#20849;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#30456;&#24212;&#22352;&#26631;&#65292;&#26725;&#25509;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
ZeroNLG is a zero-shot learning framework that can handle multiple NLG tasks, including image-to-text, video-to-text, and text-to-text, across English, Chinese, German, and French. It does not require any labeled downstream pairs for training, and bridges the differences between different domains by projecting them to corresponding coordinates in a shared common latent space.
&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#25509;&#21463;&#20197;&#22270;&#20687;&#12289;&#35270;&#39057;&#25110;&#25991;&#26412;&#24418;&#24335;&#30340;&#36755;&#20837;&#25968;&#25454;&#65292;&#24182;&#29983;&#25104;&#30456;&#24212;&#30340;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#20316;&#20026;&#36755;&#20986;&#12290;&#29616;&#26377;&#30340;NLG&#26041;&#27861;&#20027;&#35201;&#37319;&#29992;&#30417;&#30563;&#26041;&#27861;&#65292;&#24182;&#19988;&#20005;&#37325;&#20381;&#36182;&#20110;&#32806;&#21512;&#30340;&#25968;&#25454;&#21040;&#25991;&#26412;&#23545;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#35768;&#22810;&#26377;&#38024;&#23545;&#24615;&#30340;&#22330;&#26223;&#21644;&#38750;&#33521;&#35821;&#35821;&#35328;&#65292;&#24448;&#24448;&#27809;&#26377;&#36275;&#22815;&#25968;&#37327;&#30340;&#26631;&#35760;&#25968;&#25454;&#12290;&#20026;&#20102;&#25918;&#26494;&#23545;&#19979;&#28216;&#20219;&#21153;&#26631;&#35760;&#25968;&#25454;&#30340;&#20381;&#36182;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#30452;&#35266;&#26377;&#25928;&#30340;&#38646;&#26679;&#26412;&#23398;&#20064;&#26694;&#26550;ZeroNLG&#65292;&#23427;&#21487;&#20197;&#22788;&#29702;&#22810;&#20010;NLG&#20219;&#21153;&#65292;&#21253;&#25324;&#22270;&#20687;&#21040;&#25991;&#26412;&#65288;&#22270;&#20687;&#23383;&#24149;&#65289;&#12289;&#35270;&#39057;&#21040;&#25991;&#26412;&#65288;&#35270;&#39057;&#23383;&#24149;&#65289;&#21644;&#25991;&#26412;&#21040;&#25991;&#26412;&#65288;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65289;&#65292;&#36328;&#36234;&#33521;&#35821;&#12289;&#20013;&#25991;&#12289;&#24503;&#35821;&#21644;&#27861;&#35821;&#22312;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#20869;&#12290;ZeroNLG&#19981;&#38656;&#35201;&#20219;&#20309;&#26631;&#35760;&#30340;&#19979;&#28216;&#23545;&#36827;&#34892;&#35757;&#32451;&#12290;&#22312;&#35757;&#32451;&#26399;&#38388;&#65292;ZeroNLG&#65288;i&#65289;&#23558;&#19981;&#21516;&#30340;&#39046;&#22495;&#65288;&#36328;&#27169;&#24577;&#21644;&#35821;&#35328;&#65289;&#25237;&#24433;&#21040;&#20849;&#20139;&#30340;&#20844;&#20849;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#30456;&#24212;&#22352;&#26631;&#65307;&#65288;ii&#65289;&#26725;&#25509;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
Natural Language Generation (NLG) accepts input data in the form of images, videos, or text and generates corresponding natural language text as output. Existing NLG methods mainly adopt a supervised approach and rely heavily on coupled data-to-text pairs. However, for many targeted scenarios and for non-English languages, sufficient quantities of labeled data are often not available. To relax the dependency on labeled data of downstream tasks, we propose an intuitive and effective zero-shot learning framework, ZeroNLG, which can deal with multiple NLG tasks, including image-to-text (image captioning), video-to-text (video captioning), and text-to-text (neural machine translation), across English, Chinese, German, and French within a unified framework. ZeroNLG does not require any labeled downstream pairs for training. During training, ZeroNLG (i) projects different domains (across modalities and languages) to corresponding coordinates in a shared common latent space; (ii) bridges diff
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20351;&#29992;&#20132;&#20114;&#32593;&#32476;&#36827;&#34892;&#19978;&#19979;&#25991;&#23884;&#20837;&#65292;&#29992;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#22788;&#29702;&#12290;&#35813;&#27169;&#22411;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#26368;&#36817;&#30340;&#28145;&#24230;&#23398;&#20064;&#22522;&#20934;&#35843;&#26597;&#65292;&#24182;&#19988;&#19982;&#25552;&#21319;&#26641;&#35299;&#20915;&#26041;&#26696;&#30456;&#27604;&#20063;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.06455</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#19978;&#19979;&#25991;&#23884;&#20837;&#22312;&#34920;&#26684;&#25968;&#25454;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Network contextual embedding for Deep Learning on Tabular Data. (arXiv:2303.06455v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06455
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20351;&#29992;&#20132;&#20114;&#32593;&#32476;&#36827;&#34892;&#19978;&#19979;&#25991;&#23884;&#20837;&#65292;&#29992;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#22788;&#29702;&#12290;&#35813;&#27169;&#22411;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#26368;&#36817;&#30340;&#28145;&#24230;&#23398;&#20064;&#22522;&#20934;&#35843;&#26597;&#65292;&#24182;&#19988;&#19982;&#25552;&#21319;&#26641;&#35299;&#20915;&#26041;&#26696;&#30456;&#27604;&#20063;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel deep learning model based on Graph Neural Network (GNN) with Interaction Network (IN) for contextual embedding, which outperforms the recent DL benchmark on five public datasets and achieves competitive results compared to boosted-tree solutions in tabular data processing.
&lt;/p&gt;
&lt;p&gt;
&#25152;&#26377;&#34892;&#19994;&#37117;&#35797;&#22270;&#21033;&#29992;&#29616;&#26377;&#30340;&#22823;&#25968;&#25454;&#36827;&#34892;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#24212;&#29992;&#65292;&#36825;&#20123;&#25968;&#25454;&#36890;&#24120;&#20197;&#25152;&#35859;&#30340;&#34920;&#26684;&#24418;&#24335;&#23384;&#22312;&#65292;&#20854;&#20013;&#27599;&#20010;&#35760;&#24405;&#30001;&#35768;&#22810;&#24322;&#26500;&#30340;&#36830;&#32493;&#21644;&#20998;&#31867;&#21015;&#32452;&#25104;&#65292;&#20063;&#31216;&#20026;&#29305;&#24449;&#12290;&#28145;&#24230;&#23398;&#20064;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#19982;&#20154;&#31867;&#25216;&#33021;&#30456;&#20851;&#30340;&#39046;&#22495;&#20013;&#24050;&#32463;&#21462;&#24471;&#20102;&#37325;&#22823;&#31361;&#30772;&#65292;&#20294;&#20854;&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#30340;&#24212;&#29992;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#26356;&#32463;&#20856;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#22914;&#22522;&#20110;&#26641;&#30340;&#38598;&#25104;&#27169;&#22411;&#36890;&#24120;&#34920;&#29616;&#26356;&#22909;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#23427;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#65292;&#26356;&#20855;&#20307;&#22320;&#35828;&#26159;&#20132;&#20114;&#32593;&#32476;&#65288;IN&#65289;&#65292;&#36827;&#34892;&#19978;&#19979;&#25991;&#23884;&#20837;&#12290;&#20854;&#32467;&#26524;&#20248;&#20110;&#26368;&#36817;&#21457;&#24067;&#30340;&#22522;&#20110;&#20116;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#30340;&#28145;&#24230;&#23398;&#20064;&#22522;&#20934;&#35843;&#26597;&#65292;&#19982;&#25552;&#21319;&#26641;&#35299;&#20915;&#26041;&#26696;&#30456;&#27604;&#20063;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
All industries are trying to leverage Artificial Intelligence (AI) based on their existing big data which is available in so called tabular form, where each record is composed of a number of heterogeneous continuous and categorical columns also known as features. Deep Learning (DL) has consituted a major breathrough for AI in fields related to human skills like natural language processing, but its applicability to tabular data has been more challenging. More classical Machine Learning (ML) models like tree-based ensemble ones usually perform better. In this manuscript a novel DL model that uses Graph Neural Network (GNN), more specifically Interaction Network (IN), for contextual embedding is introduced. Its results outperform those of the recently published survey with DL benchmark based on five public datasets, achieving also competitive results when compared to boosted-tree solutions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#20869;&#23481;&#29983;&#25104;&#20219;&#21153;&#21450;&#20854;&#30456;&#24212;&#30340;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20132;&#20114;&#27169;&#24335;&#65292;&#40723;&#21169;&#30740;&#31350;&#31038;&#21306;&#19987;&#27880;&#20110;&#26356;&#22797;&#26434;&#21644;&#30456;&#20114;&#20381;&#36182;&#30340;&#20219;&#21153;&#65292;&#36825;&#20123;&#20219;&#21153;&#38656;&#35201;&#26356;&#39640;&#27700;&#24179;&#30340;&#20154;&#31867;&#21442;&#19982;&#12290;</title><link>http://arxiv.org/abs/2303.06430</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#25991;&#26412;&#21327;&#20316;&#20219;&#21153;&#20013;&#20132;&#20114;&#35774;&#35745;&#31354;&#38388;&#30340;&#26144;&#23556;
&lt;/p&gt;
&lt;p&gt;
Mapping the Design Space of Interactions in Human-AI Text Co-creation Tasks. (arXiv:2303.06430v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06430
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#20869;&#23481;&#29983;&#25104;&#20219;&#21153;&#21450;&#20854;&#30456;&#24212;&#30340;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20132;&#20114;&#27169;&#24335;&#65292;&#40723;&#21169;&#30740;&#31350;&#31038;&#21306;&#19987;&#27880;&#20110;&#26356;&#22797;&#26434;&#21644;&#30456;&#20114;&#20381;&#36182;&#30340;&#20219;&#21153;&#65292;&#36825;&#20123;&#20219;&#21153;&#38656;&#35201;&#26356;&#39640;&#27700;&#24179;&#30340;&#20154;&#31867;&#21442;&#19982;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a spectrum of content generation tasks and their corresponding human-AI interaction patterns, encouraging the research community to focus on more complex and interdependent tasks that require greater levels of human involvement.
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25991;&#26412;&#29983;&#25104;&#33021;&#21147;&#65292;&#20419;&#20351;&#25105;&#20204;&#37325;&#26032;&#32771;&#34385;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21327;&#20316;&#30340;&#26410;&#26469;&#20197;&#21450;&#20154;&#31867;&#22914;&#20309;&#19982;LLMs&#20132;&#20114;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#20869;&#23481;&#29983;&#25104;&#20219;&#21153;&#21450;&#20854;&#30456;&#24212;&#30340;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20132;&#20114;&#27169;&#24335;&#12290;&#36825;&#20123;&#20219;&#21153;&#21253;&#25324;&#65306;1&#65289;&#20855;&#26377;&#26368;&#23567;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20132;&#20114;&#30340;&#22266;&#23450;&#33539;&#22260;&#20869;&#23481;&#31574;&#21010;&#20219;&#21153;&#65292;2&#65289;&#20855;&#26377;&#31934;&#30830;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20132;&#20114;&#30340;&#29420;&#31435;&#21019;&#24847;&#20219;&#21153;&#65292;&#20197;&#21450;3&#65289;&#20855;&#26377;&#36845;&#20195;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20132;&#20114;&#30340;&#22797;&#26434;&#19988;&#30456;&#20114;&#20381;&#36182;&#30340;&#21019;&#24847;&#20219;&#21153;&#12290;&#25105;&#20204;&#40723;&#21169;&#29983;&#25104;AI&#21644;HCI&#30740;&#31350;&#31038;&#21306;&#19987;&#27880;&#20110;&#26356;&#22797;&#26434;&#21644;&#30456;&#20114;&#20381;&#36182;&#30340;&#20219;&#21153;&#65292;&#36825;&#20123;&#20219;&#21153;&#38656;&#35201;&#26356;&#39640;&#27700;&#24179;&#30340;&#20154;&#31867;&#21442;&#19982;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated impressive text generation capabilities, prompting us to reconsider the future of human-AI co-creation and how humans interact with LLMs. In this paper, we present a spectrum of content generation tasks and their corresponding human-AI interaction patterns. These tasks include: 1) fixed-scope content curation tasks with minimal human-AI interactions, 2) independent creative tasks with precise human-AI interactions, and 3) complex and interdependent creative tasks with iterative human-AI interactions. We encourage the generative AI and HCI research communities to focus on the more complex and interdependent tasks, which require greater levels of human involvement.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;MIMO-NOMA IoT&#31995;&#32479;&#21151;&#29575;&#20998;&#37197;&#26041;&#27861;&#65292;&#20197;&#26368;&#23567;&#21270;AoI&#21644;&#33021;&#32791;&#12290;</title><link>http://arxiv.org/abs/2303.06411</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;MIMO-NOMA IoT&#31995;&#32479;&#21151;&#29575;&#20998;&#37197;&#65292;&#20197;&#26368;&#23567;&#21270;AoI&#21644;&#33021;&#32791;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning Based Power Allocation for Minimizing AoI and Energy Consumption in MIMO-NOMA IoT Systems. (arXiv:2303.06411v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06411
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;MIMO-NOMA IoT&#31995;&#32479;&#21151;&#29575;&#20998;&#37197;&#26041;&#27861;&#65292;&#20197;&#26368;&#23567;&#21270;AoI&#21644;&#33021;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a deep reinforcement learning based power allocation method for MIMO-NOMA IoT systems to minimize AoI and energy consumption.
&lt;/p&gt;
&lt;p&gt;
&#22810;&#36755;&#20837;&#22810;&#36755;&#20986;&#21644;&#38750;&#27491;&#20132;&#22810;&#22336;&#65288;MIMO-NOMA&#65289;&#29289;&#32852;&#32593;&#65288;IoT&#65289;&#31995;&#32479;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20449;&#36947;&#23481;&#37327;&#21644;&#39057;&#35889;&#25928;&#29575;&#65292;&#20197;&#25903;&#25345;&#23454;&#26102;&#24212;&#29992;&#12290;&#26102;&#24310;&#65288;AoI&#65289;&#26159;&#23454;&#26102;&#24212;&#29992;&#30340;&#37325;&#35201;&#25351;&#26631;&#65292;&#20294;&#27809;&#26377;&#25991;&#29486;&#26368;&#23567;&#21270;MIMO-NOMA IoT&#31995;&#32479;&#30340;AoI&#65292;&#36825;&#20419;&#20351;&#25105;&#20204;&#36827;&#34892;&#36825;&#39033;&#24037;&#20316;&#12290;&#22312;MIMO-NOMA IoT&#31995;&#32479;&#20013;&#65292;&#22522;&#31449;&#65288;BS&#65289;&#30830;&#23450;&#26679;&#26412;&#25910;&#38598;&#35201;&#27714;&#24182;&#20026;&#27599;&#20010;IoT&#35774;&#22791;&#20998;&#37197;&#20256;&#36755;&#21151;&#29575;&#12290;&#27599;&#20010;&#35774;&#22791;&#26681;&#25454;&#26679;&#26412;&#25910;&#38598;&#35201;&#27714;&#30830;&#23450;&#26159;&#21542;&#37319;&#26679;&#25968;&#25454;&#65292;&#24182;&#37319;&#29992;&#20998;&#37197;&#30340;&#21151;&#29575;&#23558;&#37319;&#26679;&#30340;&#25968;&#25454;&#36890;&#36807;MIMO-NOMA&#20449;&#36947;&#20256;&#36755;&#21040;BS&#12290;&#28982;&#21518;&#65292;BS&#37319;&#29992;&#36830;&#32493;&#24178;&#25200;&#28040;&#38500;&#65288;SIC&#65289;&#25216;&#26415;&#35299;&#30721;&#27599;&#20010;&#35774;&#22791;&#20256;&#36755;&#30340;&#25968;&#25454;&#20449;&#21495;&#12290;&#26679;&#26412;&#25910;&#38598;&#35201;&#27714;&#21644;&#21151;&#29575;&#20998;&#37197;&#23558;&#24433;&#21709;&#31995;&#32479;&#30340;AoI&#21644;&#33021;&#32791;&#12290;&#36825;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-input multi-out and non-orthogonal multiple access (MIMO-NOMA) internet-of-things (IoT) systems can improve channel capacity and spectrum efficiency distinctly to support the real-time applications. Age of information (AoI) is an important metric for real-time application, but there is no literature have minimized AoI of the MIMO-NOMA IoT system, which motivates us to conduct this work. In MIMO-NOMA IoT system, the base station (BS) determines the sample collection requirements and allocates the transmission power for each IoT device. Each device determines whether to sample data according to the sample collection requirements and adopts the allocated power to transmit the sampled data to the BS over MIMO-NOMA channel. Afterwards, the BS employs successive interference cancelation (SIC) technique to decode the signal of the data transmitted by each device. The sample collection requirements and power allocation would affect AoI and energy consumption of the system. It is critical
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#31471;&#21040;&#31471;&#33041;&#32593;&#32476;&#29983;&#25104;&#27169;&#22411;Brain Diffuser&#65292;&#30452;&#25509;&#20174;DTI&#20013;&#24418;&#25104;&#32467;&#26500;&#24615;&#33041;&#32593;&#32476;&#12290;&#23545;&#20110;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30340;&#24773;&#20917;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;ADNI&#25968;&#25454;&#24211;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#24037;&#20855;&#21253;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.06410</link><description>&lt;p&gt;
Brain Diffuser&#65306;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#33041;&#22270;&#20687;&#21040;&#33041;&#32593;&#32476;&#31649;&#36947;
&lt;/p&gt;
&lt;p&gt;
Brain Diffuser: An End-to-End Brain Image to Brain Network Pipeline. (arXiv:2303.06410v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06410
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#31471;&#21040;&#31471;&#33041;&#32593;&#32476;&#29983;&#25104;&#27169;&#22411;Brain Diffuser&#65292;&#30452;&#25509;&#20174;DTI&#20013;&#24418;&#25104;&#32467;&#26500;&#24615;&#33041;&#32593;&#32476;&#12290;&#23545;&#20110;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30340;&#24773;&#20917;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;ADNI&#25968;&#25454;&#24211;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#24037;&#20855;&#21253;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a diffusion based end-to-end brain network generative model Brain Diffuser that directly shapes the structural brain networks from DTI. For the case of Alzheimer's disease, the proposed model performs better than the results from existing toolkits on the Alzheimer's Disease Neuroimaging Initiative (ADNI) database.
&lt;/p&gt;
&lt;p&gt;
&#33041;&#32593;&#32476;&#20998;&#26512;&#23545;&#20110;&#35786;&#26029;&#21644;&#24178;&#39044;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#65288;AD&#65289;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#30740;&#31350;&#20027;&#35201;&#20381;&#36182;&#20110;&#29305;&#23450;&#30340;&#32791;&#26102;&#21644;&#20027;&#35266;&#30340;&#24037;&#20855;&#21253;&#12290;&#21482;&#26377;&#23569;&#25968;&#24037;&#20855;&#21487;&#20197;&#20174;&#33041;&#25193;&#25955;&#24352;&#37327;&#22270;&#20687;&#65288;DTI&#65289;&#20013;&#33719;&#21462;&#32467;&#26500;&#24615;&#33041;&#32593;&#32476;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#31471;&#21040;&#31471;&#33041;&#32593;&#32476;&#29983;&#25104;&#27169;&#22411;Brain Diffuser&#65292;&#30452;&#25509;&#20174;DTI&#20013;&#24418;&#25104;&#32467;&#26500;&#24615;&#33041;&#32593;&#32476;&#12290;&#19982;&#29616;&#26377;&#24037;&#20855;&#21253;&#30456;&#27604;&#65292;Brain Diffuser&#36890;&#36807;&#20998;&#26512;&#21463;&#35797;&#32773;&#20043;&#38388;&#32467;&#26500;&#24615;&#33041;&#32593;&#32476;&#30340;&#24046;&#24322;&#65292;&#21033;&#29992;&#26356;&#22810;&#30340;&#32467;&#26500;&#36830;&#25509;&#29305;&#24449;&#21644;&#19982;&#30142;&#30149;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;&#23545;&#20110;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30340;&#24773;&#20917;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#31070;&#32463;&#24433;&#20687;&#23398;&#20513;&#35758;&#65288;ADNI&#65289;&#25968;&#25454;&#24211;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#24037;&#20855;&#21253;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Brain network analysis is essential for diagnosing and intervention for Alzheimer's disease (AD). However, previous research relied primarily on specific time-consuming and subjective toolkits. Only few tools can obtain the structural brain networks from brain diffusion tensor images (DTI). In this paper, we propose a diffusion based end-to-end brain network generative model Brain Diffuser that directly shapes the structural brain networks from DTI. Compared to existing toolkits, Brain Diffuser exploits more structural connectivity features and disease-related information by analyzing disparities in structural brain networks across subjects. For the case of Alzheimer's disease, the proposed model performs better than the results from existing toolkits on the Alzheimer's Disease Neuroimaging Initiative (ADNI) database.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#33258;&#21160;&#26816;&#27979;&#36741;&#21161;&#29356;&#39044;&#27979;&#20154;&#31867;&#30315;&#30187;&#21457;&#20316;&#26102;&#30340;&#20449;&#21495;&#34892;&#20026;&#65292;&#20197;&#25552;&#39640;&#30315;&#30187;&#24739;&#32773;&#30340;&#29983;&#27963;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2303.06407</link><description>&lt;p&gt;
&#33258;&#21160;&#26816;&#27979;&#36741;&#21161;&#29356;&#39044;&#27979;&#20154;&#31867;&#30315;&#30187;&#21457;&#20316;&#26102;&#30340;&#20449;&#21495;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Automatic Detection of Signalling Behaviour from Assistance Dogs as they Forecast the Onset of Epileptic Seizures in Humans. (arXiv:2303.06407v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06407
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#33258;&#21160;&#26816;&#27979;&#36741;&#21161;&#29356;&#39044;&#27979;&#20154;&#31867;&#30315;&#30187;&#21457;&#20316;&#26102;&#30340;&#20449;&#21495;&#34892;&#20026;&#65292;&#20197;&#25552;&#39640;&#30315;&#30187;&#24739;&#32773;&#30340;&#29983;&#27963;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study explores how to automatically detect signalling behaviour from assistance dogs as they forecast the onset of epileptic seizures in humans, to improve the quality of life for epilepsy patients.
&lt;/p&gt;
&lt;p&gt;
&#30315;&#30187;&#26159;&#19990;&#30028;&#19978;&#26368;&#24120;&#35265;&#30340;&#31070;&#32463;&#31995;&#32479;&#30142;&#30149;&#20043;&#19968;&#65292;&#24433;&#21709;&#30528;&#25968;&#30334;&#19975;&#20154;&#12290;&#30315;&#30187;&#21457;&#20316;&#36890;&#24120;&#26159;&#30001;&#20110;&#20154;&#33041;&#20013;&#30340;&#38750;&#21327;&#35843;&#30005;&#25918;&#30005;&#24341;&#36215;&#30340;&#65292;&#21487;&#33021;&#20250;&#36896;&#25104;&#20260;&#23475;&#65292;&#21253;&#25324;&#20498;&#22320;&#21644;&#22833;&#21435;&#24847;&#35782;&#12290;&#22914;&#26524;&#33021;&#22815;&#39044;&#27979;&#30315;&#30187;&#21457;&#20316;&#30340;&#24320;&#22987;&#65292;&#37027;&#20040;&#21487;&#20197;&#23558;&#21463;&#35797;&#32773;&#32622;&#20110;&#23433;&#20840;&#30340;&#29615;&#22659;&#25110;&#20301;&#32622;&#65292;&#20197;&#26368;&#23567;&#21270;&#30001;&#20110;&#20498;&#22320;&#32780;&#23548;&#33268;&#30340;&#33258;&#25105;&#20260;&#23475;&#12290;&#28982;&#32780;&#65292;&#22312;&#26085;&#24120;&#30340;&#19981;&#21463;&#25511;&#21046;&#30340;&#29615;&#22659;&#20013;&#65292;&#27809;&#26377;&#26126;&#30830;&#30340;&#26041;&#27861;&#26469;&#39044;&#27979;&#30315;&#30187;&#21457;&#20316;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23456;&#29289;&#29399;&#26377;&#33021;&#21147;&#36890;&#36807;&#21957;&#25506;&#21463;&#35797;&#32773;&#22312;&#30315;&#30187;&#21457;&#20316;&#21069;&#30382;&#32932;&#25955;&#21457;&#30340;&#29305;&#24449;&#25381;&#21457;&#24615;&#26377;&#26426;&#21270;&#21512;&#29289;&#26469;&#26816;&#27979;&#30315;&#30187;&#21457;&#20316;&#30340;&#24320;&#22987;&#65292;&#26377;&#20123;&#36741;&#21161;&#29356;&#32463;&#36807;&#35757;&#32451;&#65292;&#21487;&#20197;&#21521;&#20854;&#20027;&#20154;/&#35757;&#32451;&#21592;&#21457;&#20986;&#20449;&#21495;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#22914;&#20309;&#33258;&#21160;&#26816;&#27979;&#20449;&#21495;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Epilepsy or the occurrence of epileptic seizures, is one of the world's most well-known neurological disorders affecting millions of people. Seizures mostly occur due to non-coordinated electrical discharges in the human brain and may cause damage, including collapse and loss of consciousness. If the onset of a seizure can be forecast then the subject can be placed into a safe environment or position so that self-injury as a result of a collapse can be minimised. However there are no definitive methods to predict seizures in an everyday, uncontrolled environment. Previous studies have shown that pet dogs have the ability to detect the onset of an epileptic seizure by scenting the characteristic volatile organic compounds exuded through the skin by a subject prior a seizure occurring and there are cases where assistance dogs, trained to scent the onset of a seizure, can signal this to their owner/trainer. In this work we identify how we can automatically detect the signalling behaviours
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#31227;&#21160;&#21069;&#27839;&#12289;&#25968;&#25454;&#20998;&#35299;&#21644;&#28145;&#24230;&#23398;&#20064;&#65292;&#29992;&#20110;&#39044;&#27979;&#22797;&#26434;&#26102;&#38388;&#24207;&#21015;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#32463;&#39564;&#23567;&#27874;&#21464;&#25442;&#23558;&#26102;&#38388;&#24207;&#21015;&#20998;&#35299;&#25104;&#26356;&#31616;&#21333;&#30340;&#32452;&#25104;&#24207;&#21015;&#65292;&#24182;&#20351;&#29992;&#31227;&#21160;&#21069;&#27839;&#26041;&#27861;&#38450;&#27490;&#25968;&#25454;&#27844;&#28431;&#12290;</title><link>http://arxiv.org/abs/2303.06394</link><description>&lt;p&gt;
&#19968;&#31181;&#32467;&#21512;&#31227;&#21160;&#21069;&#27839;&#12289;&#25968;&#25454;&#20998;&#35299;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#29992;&#20110;&#39044;&#27979;&#22797;&#26434;&#26102;&#38388;&#24207;&#21015;
&lt;/p&gt;
&lt;p&gt;
A Novel Method Combines Moving Fronts, Data Decomposition and Deep Learning to Forecast Intricate Time Series. (arXiv:2303.06394v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06394
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#31227;&#21160;&#21069;&#27839;&#12289;&#25968;&#25454;&#20998;&#35299;&#21644;&#28145;&#24230;&#23398;&#20064;&#65292;&#29992;&#20110;&#39044;&#27979;&#22797;&#26434;&#26102;&#38388;&#24207;&#21015;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#32463;&#39564;&#23567;&#27874;&#21464;&#25442;&#23558;&#26102;&#38388;&#24207;&#21015;&#20998;&#35299;&#25104;&#26356;&#31616;&#21333;&#30340;&#32452;&#25104;&#24207;&#21015;&#65292;&#24182;&#20351;&#29992;&#31227;&#21160;&#21069;&#27839;&#26041;&#27861;&#38450;&#27490;&#25968;&#25454;&#27844;&#28431;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel method that combines moving fronts, data decomposition, and deep learning to forecast intricate time series. The method decomposes the time series into simpler constituent series using empirical wavelet transform and prevents data leakage using the moving front method.
&lt;/p&gt;
&lt;p&gt;
&#39640;&#21464;&#24322;&#24615;&#30340;&#21333;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#29978;&#33267;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#20063;&#26500;&#25104;&#25361;&#25112;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38382;&#39064;&#65292;&#21333;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#34987;&#20998;&#35299;&#25104;&#26356;&#31616;&#21333;&#30340;&#32452;&#25104;&#24207;&#21015;&#65292;&#23427;&#20204;&#30340;&#24635;&#21644;&#31561;&#20110;&#21407;&#22987;&#24207;&#21015;&#12290;&#26412;&#25991;&#28436;&#31034;&#20102;&#20256;&#32479;&#30340;&#19968;&#27425;&#20998;&#35299;&#25216;&#26415;&#23384;&#22312;&#25968;&#25454;&#27844;&#28431;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31227;&#21160;&#21069;&#27839;&#65288;MF&#65289;&#26041;&#27861;&#26469;&#38450;&#27490;&#25968;&#25454;&#27844;&#28431;&#65292;&#20351;&#20998;&#35299;&#21518;&#30340;&#24207;&#21015;&#21487;&#20197;&#20687;&#20854;&#20182;&#26102;&#38388;&#24207;&#21015;&#19968;&#26679;&#22788;&#29702;&#12290;&#21360;&#24230;&#22799;&#23395;&#23395;&#39118;&#38477;&#38632;&#65288;ISMR&#65289;&#26159;&#19968;&#20010;&#38750;&#24120;&#22797;&#26434;&#30340;&#26102;&#38388;&#24207;&#21015;&#65292;&#23545;DNN&#26500;&#25104;&#25361;&#25112;&#65292;&#22240;&#27492;&#34987;&#36873;&#20026;&#31034;&#20363;&#12290;&#20174;&#20247;&#22810;&#21487;&#29992;&#30340;&#20449;&#21495;&#22788;&#29702;&#24037;&#20855;&#20013;&#65292;&#32463;&#39564;&#23567;&#27874;&#21464;&#25442;&#65288;EWT&#65289;&#34987;&#36873;&#25321;&#29992;&#20110;&#23558;ISMR&#20998;&#35299;&#25104;&#26356;&#31616;&#21333;&#30340;&#32452;&#25104;&#24207;&#21015;&#65292;&#22240;&#20026;&#23427;&#34987;&#21457;&#29616;&#27604;&#20854;&#20182;&#27969;&#34892;&#30340;&#31639;&#27861;&#65292;&#22914;&#33258;&#36866;&#24212;&#22122;&#22768;&#23436;&#20840;&#38598;&#21512;&#32463;&#39564;&#27169;&#24577;&#20998;&#35299;&#65288;CEEMDAN&#65289;&#26356;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
A univariate time series with high variability can pose a challenge even to Deep Neural Network (DNN). To overcome this, a univariate time series is decomposed into simpler constituent series, whose sum equals the original series. As demonstrated in this article, the conventional one-time decomposition technique suffers from a leak of information from the future, referred to as a data leak. In this work, a novel Moving Front (MF) method is proposed to prevent data leakage, so that the decomposed series can be treated like other time series. Indian Summer Monsoon Rainfall (ISMR) is a very complex time series, which poses a challenge to DNN and is therefore selected as an example. From the many signal processing tools available, Empirical Wavelet Transform (EWT) was chosen for decomposing the ISMR into simpler constituent series, as it was found to be more effective than the other popular algorithm, Complete Ensemble Empirical Mode Decomposition with Adaptive Noise (CEEMDAN). The propose
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#20498;&#25968;&#27010;&#29575;&#20998;&#25968;&#20272;&#35745;&#22120;&#65288;UIPS&#65289;&#65292;&#29992;&#20110;&#25913;&#36827;&#31163;&#32447;&#23398;&#20064;&#65292;&#36890;&#36807;&#26126;&#30830;&#27169;&#25311;&#20272;&#35745;&#30340;&#35760;&#24405;&#31574;&#30053;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#30456;&#23545;&#20110;&#24191;&#27867;&#30340;&#26368;&#20808;&#36827;&#22522;&#32447;&#20855;&#26377;&#20248;&#36234;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.06389</link><description>&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#31163;&#32447;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-Aware Off-Policy Learning. (arXiv:2303.06389v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06389
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#20498;&#25968;&#27010;&#29575;&#20998;&#25968;&#20272;&#35745;&#22120;&#65288;UIPS&#65289;&#65292;&#29992;&#20110;&#25913;&#36827;&#31163;&#32447;&#23398;&#20064;&#65292;&#36890;&#36807;&#26126;&#30830;&#27169;&#25311;&#20272;&#35745;&#30340;&#35760;&#24405;&#31574;&#30053;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#30456;&#23545;&#20110;&#24191;&#27867;&#30340;&#26368;&#20808;&#36827;&#22522;&#32447;&#20855;&#26377;&#20248;&#36234;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an Uncertainty-aware Inverse Propensity Score estimator (UIPS) for improved off-policy learning, which explicitly models the uncertainty in the estimated logging policy and demonstrates advantageous sample efficiency against an extensive list of state-of-the-art baselines on synthetic and three real-world recommendation datasets.
&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#23398;&#20064;&#26159;&#25351;&#20165;&#36890;&#36807;&#35760;&#24405;&#30340;&#21453;&#39304;&#25968;&#25454;&#36827;&#34892;&#31574;&#30053;&#20248;&#21270;&#30340;&#36807;&#31243;&#65292;&#22312;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#20013;&#26174;&#31034;&#20986;&#37325;&#35201;&#24615;&#65292;&#20363;&#22914;&#25628;&#32034;&#24341;&#25806;&#12289;&#25512;&#33616;&#31995;&#32479;&#31561;&#12290;&#34429;&#28982;&#29983;&#25104;&#35760;&#24405;&#25968;&#25454;&#30340;&#30495;&#23454;&#35760;&#24405;&#31574;&#30053;&#36890;&#24120;&#26159;&#26410;&#30693;&#30340;&#65292;&#20294;&#20197;&#21069;&#30340;&#24037;&#20316;&#20165;&#22312;&#31163;&#32447;&#23398;&#20064;&#20013;&#37319;&#29992;&#20854;&#20272;&#35745;&#20540;&#65292;&#24573;&#30053;&#20102;&#30001;&#20110;&#36825;&#31181;&#20272;&#35745;&#22120;&#23548;&#33268;&#30340;&#39640;&#20559;&#24046;&#21644;&#39640;&#26041;&#24046;&#65292;&#29305;&#21035;&#26159;&#22312;&#20855;&#26377;&#23567;&#19988;&#20272;&#35745;&#19981;&#20934;&#30830;&#30340;&#35760;&#24405;&#27010;&#29575;&#30340;&#26679;&#26412;&#19978;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26126;&#30830;&#22320;&#27169;&#25311;&#20102;&#20272;&#35745;&#30340;&#35760;&#24405;&#31574;&#30053;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#20498;&#25968;&#27010;&#29575;&#20998;&#25968;&#20272;&#35745;&#22120;&#65288;UIPS&#65289;&#26469;&#25913;&#36827;&#31163;&#32447;&#23398;&#20064;&#12290;&#22312;&#21512;&#25104;&#21644;&#19977;&#20010;&#30495;&#23454;&#30340;&#25512;&#33616;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;UIPS&#20272;&#35745;&#22120;&#30456;&#23545;&#20110;&#24191;&#27867;&#30340;&#26368;&#20808;&#36827;&#22522;&#32447;&#20855;&#26377;&#20248;&#36234;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Off-policy learning, referring to the procedure of policy optimization with access only to logged feedback data, has shown importance in various real-world applications, such as search engines, recommender systems, and etc. While the ground-truth logging policy, which generates the logged data, is usually unknown, previous work simply takes its estimated value in off-policy learning, ignoring both high bias and high variance resulted from such an estimator, especially on samples with small and inaccurately estimated logging probabilities. In this work, we explicitly model the uncertainty in the estimated logging policy and propose a Uncertainty-aware Inverse Propensity Score estimator (UIPS) for improved off-policy learning. Experiment results on synthetic and three real-world recommendation datasets demonstrate the advantageous sample efficiency of the proposed UIPS estimator against an extensive list of state-of-the-art baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#25506;&#31350;&#22330;&#26223;&#24863;&#30693;&#30340;&#31070;&#32463;&#34920;&#24449;&#22312;&#28023;&#39532;&#20381;&#36182;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#22411;&#22330;&#26223;&#24863;&#30693;&#22522;&#20934;&#65292;&#35777;&#26126;&#20102;DNNs&#21487;&#20197;&#23398;&#20064;&#20174;&#19981;&#21516;&#33258;&#25105;&#20013;&#24515;&#35270;&#35282;&#35266;&#23519;&#30340;&#22330;&#26223;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.06367</link><description>&lt;p&gt;
&#21033;&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#25506;&#31350;&#22330;&#26223;&#24863;&#30693;&#30340;&#31070;&#32463;&#34920;&#24449;&#22312;&#28023;&#39532;&#20381;&#36182;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Probing neural representations of scene perception in a hippocampally dependent task using artificial neural networks. (arXiv:2303.06367v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06367
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#25506;&#31350;&#22330;&#26223;&#24863;&#30693;&#30340;&#31070;&#32463;&#34920;&#24449;&#22312;&#28023;&#39532;&#20381;&#36182;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#22411;&#22330;&#26223;&#24863;&#30693;&#22522;&#20934;&#65292;&#35777;&#26126;&#20102;DNNs&#21487;&#20197;&#23398;&#20064;&#20174;&#19981;&#21516;&#33258;&#25105;&#20013;&#24515;&#35270;&#35282;&#35266;&#23519;&#30340;&#22330;&#26223;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study uses artificial neural networks to explore neural representations of scene perception in a hippocampally dependent task, and demonstrates that DNNs can learn the ability to transform scenes viewed from different egocentric perspectives, using a novel scene perception benchmark.
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#35757;&#32451;&#30340;&#28145;&#24230;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;(DNNs)&#25552;&#20379;&#20102;&#21754;&#20083;&#21160;&#29289;&#35270;&#35273;&#31995;&#32479;&#30340;&#26377;&#25928;&#27169;&#22411;&#65292;&#20934;&#30830;&#22320;&#25429;&#25417;&#20102;&#20174;&#21021;&#32423;&#35270;&#35273;&#30382;&#23618;&#21040;&#19979;&#39070;&#30382;&#36136;(IT)&#30340;&#31070;&#32463;&#21709;&#24212;&#23618;&#27425;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32593;&#32476;&#35299;&#37322;&#26356;&#39640;&#30382;&#23618;&#21306;&#22495;&#30340;&#34920;&#24449;&#33021;&#21147;&#30456;&#23545;&#36739;&#24369;&#65292;&#30740;&#31350;&#20063;&#30456;&#23545;&#36739;&#23569;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#20010;&#21463;&#28023;&#39532;&#20381;&#36182;&#20219;&#21153;&#21551;&#21457;&#30340;&#26032;&#22411;&#22330;&#26223;&#24863;&#30693;&#22522;&#20934;&#65292;&#26088;&#22312;&#25506;&#31350;DNNs&#36716;&#25442;&#20174;&#19981;&#21516;&#33258;&#25105;&#20013;&#24515;&#35270;&#35282;&#35266;&#23519;&#30340;&#22330;&#26223;&#30340;&#33021;&#21147;&#12290;&#20351;&#29992;&#21463;&#39070;&#21494;&#32467;&#26500;&#21644;&#28023;&#39532;&#20043;&#38388;&#36830;&#25509;&#21551;&#21457;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20351;&#29992;&#19977;&#20803;&#32452;&#25439;&#22833;&#35757;&#32451;&#30340;DNNs&#21487;&#20197;&#23398;&#20064;&#36825;&#20010;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#24378;&#21046;&#25191;&#34892;&#20998;&#35299;&#30340;&#28508;&#22312;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Deep artificial neural networks (DNNs) trained through backpropagation provide effective models of the mammalian visual system, accurately capturing the hierarchy of neural responses through primary visual cortex to inferior temporal cortex (IT). However, the ability of these networks to explain representations in higher cortical areas is relatively lacking and considerably less well researched. For example, DNNs have been less successful as a model of the egocentric to allocentric transformation embodied by circuits in retrosplenial and posterior parietal cortex. We describe a novel scene perception benchmark inspired by a hippocampal dependent task, designed to probe the ability of DNNs to transform scenes viewed from different egocentric perspectives. Using a network architecture inspired by the connectivity between temporal lobe structures and the hippocampus, we demonstrate that DNNs trained using a triplet loss can learn this task. Moreover, by enforcing a factorized latent space
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34394;&#25311;&#26816;&#26597;&#23618;&#65292;&#23558;&#26102;&#38388;&#24207;&#21015;&#36716;&#25442;&#20026;&#21487;&#35299;&#37322;&#30340;&#34920;&#31034;&#65292;&#24182;&#20801;&#35768;&#36890;&#36807;&#26412;&#22320;XAI&#26041;&#27861;&#23558;&#30456;&#20851;&#24615;&#24402;&#22240;&#20256;&#25773;&#21040;&#35813;&#34920;&#31034;&#12290;&#25105;&#20204;&#23558;&#19968;&#31995;&#21015;XAI&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#25193;&#23637;&#21040;&#38656;&#35201;&#36716;&#25442;&#21518;&#25165;&#33021;&#35299;&#37322;&#36755;&#20837;&#30340;&#39046;&#22495;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;DFT-LRP&#22312;&#21508;&#31181;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#35774;&#32622;&#20013;&#30340;&#26377;&#29992;&#24615;&#65292;&#22914;&#38899;&#39057;&#21644;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#12290;</title><link>http://arxiv.org/abs/2303.06365</link><description>&lt;p&gt;
&#36890;&#36807;&#34394;&#25311;&#26816;&#26597;&#23618;&#23454;&#29616;&#26102;&#38388;&#24207;&#21015;&#30340;&#21487;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Explainable AI for Time Series via Virtual Inspection Layers. (arXiv:2303.06365v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06365
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34394;&#25311;&#26816;&#26597;&#23618;&#65292;&#23558;&#26102;&#38388;&#24207;&#21015;&#36716;&#25442;&#20026;&#21487;&#35299;&#37322;&#30340;&#34920;&#31034;&#65292;&#24182;&#20801;&#35768;&#36890;&#36807;&#26412;&#22320;XAI&#26041;&#27861;&#23558;&#30456;&#20851;&#24615;&#24402;&#22240;&#20256;&#25773;&#21040;&#35813;&#34920;&#31034;&#12290;&#25105;&#20204;&#23558;&#19968;&#31995;&#21015;XAI&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#25193;&#23637;&#21040;&#38656;&#35201;&#36716;&#25442;&#21518;&#25165;&#33021;&#35299;&#37322;&#36755;&#20837;&#30340;&#39046;&#22495;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;DFT-LRP&#22312;&#21508;&#31181;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#35774;&#32622;&#20013;&#30340;&#26377;&#29992;&#24615;&#65292;&#22914;&#38899;&#39057;&#21644;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a virtual inspection layer that transforms time series into an interpretable representation and allows for relevance attributions to be propagated to this representation via local XAI methods. The applicability of a family of XAI methods is extended to domains where the input is only interpretable after a transformation. The usefulness of DFT-LRP is demonstrated in various time series classification settings, such as audio and electronic health records.
&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#39046;&#22495;&#21462;&#24471;&#20102;&#24456;&#22823;&#36827;&#23637;&#65292;&#20294;&#20027;&#35201;&#26159;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#12290;&#23545;&#20110;&#26102;&#38388;&#24207;&#21015;&#65292;&#30001;&#20110;&#36755;&#20837;&#36890;&#24120;&#19981;&#21487;&#35299;&#37322;&#65292;&#22240;&#27492;&#21482;&#26377;&#26377;&#38480;&#30340;XAI&#30740;&#31350;&#21487;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#34394;&#25311;&#26816;&#26597;&#23618;&#65292;&#23558;&#26102;&#38388;&#24207;&#21015;&#36716;&#25442;&#20026;&#21487;&#35299;&#37322;&#30340;&#34920;&#31034;&#65292;&#24182;&#20801;&#35768;&#36890;&#36807;&#26412;&#22320;XAI&#26041;&#27861;&#65288;&#22914;&#36880;&#23618;&#30456;&#20851;&#20256;&#25773;&#65288;LRP&#65289;&#65289;&#23558;&#30456;&#20851;&#24615;&#24402;&#22240;&#20256;&#25773;&#21040;&#35813;&#34920;&#31034;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#23558;&#19968;&#31995;&#21015;XAI&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#25193;&#23637;&#21040;&#38656;&#35201;&#36716;&#25442;&#21518;&#25165;&#33021;&#35299;&#37322;&#36755;&#20837;&#30340;&#39046;&#22495;&#65288;&#20363;&#22914;&#35821;&#38899;&#65289;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#20613;&#37324;&#21494;&#21464;&#25442;&#65292;&#36825;&#22312;&#26102;&#38388;&#24207;&#21015;&#35299;&#37322;&#21644;LRP&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#65292;&#24182;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#31216;&#20026;DFT-LRP&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;DFT-LRP&#22312;&#21508;&#31181;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#35774;&#32622;&#20013;&#30340;&#26377;&#29992;&#24615;&#65292;&#22914;&#38899;&#39057;&#21644;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;DFT-LRP&#26469;&#21487;&#35270;&#21270;&#21644;&#35299;&#37322;&#27169;&#22411;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of eXplainable Artificial Intelligence (XAI) has greatly advanced in recent years, but progress has mainly been made in computer vision and natural language processing. For time series, where the input is often not interpretable, only limited research on XAI is available. In this work, we put forward a virtual inspection layer, that transforms the time series to an interpretable representation and allows to propagate relevance attributions to this representation via local XAI methods like layer-wise relevance propagation (LRP). In this way, we extend the applicability of a family of XAI methods to domains (e.g. speech) where the input is only interpretable after a transformation. Here, we focus on the Fourier transformation which is prominently applied in the interpretation of time series and LRP and refer to our method as DFT-LRP. We demonstrate the usefulness of DFT-LRP in various time series classification settings like audio and electronic health records. We showcase how 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#21512;&#20316;&#21487;&#35265;&#20809;&#23450;&#20301;&#26041;&#26696;&#65292;&#36890;&#36807;&#20849;&#21516;&#35757;&#32451;&#36866;&#24212;&#29615;&#22659;&#21464;&#21270;&#30340;&#20840;&#23616;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#22312;&#38750;&#38745;&#24577;&#29615;&#22659;&#19979;&#30340;&#23450;&#20301;&#31934;&#24230;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.06361</link><description>&lt;p&gt;
&#38754;&#21521;&#38750;&#38745;&#24577;&#29615;&#22659;&#30340;&#38544;&#31169;&#20445;&#25252;&#21512;&#20316;&#21487;&#35265;&#20809;&#23450;&#20301;&#65306;&#32852;&#37030;&#23398;&#20064;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Privacy-Preserving Cooperative Visible Light Positioning for Nonstationary Environment: A Federated Learning Perspective. (arXiv:2303.06361v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06361
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#21512;&#20316;&#21487;&#35265;&#20809;&#23450;&#20301;&#26041;&#26696;&#65292;&#36890;&#36807;&#20849;&#21516;&#35757;&#32451;&#36866;&#24212;&#29615;&#22659;&#21464;&#21270;&#30340;&#20840;&#23616;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#22312;&#38750;&#38745;&#24577;&#29615;&#22659;&#19979;&#30340;&#23450;&#20301;&#31934;&#24230;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a cooperative visible light positioning scheme based on federated learning, which improves the positioning accuracy and generalization capability in nonstationary environments by jointly training a global model adaptive to environmental changes without sharing private data of users.
&lt;/p&gt;
&lt;p&gt;
&#21487;&#35265;&#20809;&#23450;&#20301;&#65288;VLP&#65289;&#20316;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#23460;&#20869;&#23450;&#20301;&#25216;&#26415;&#65292;&#24050;&#32463;&#24341;&#36215;&#20102;&#36275;&#22815;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22312;&#38750;&#38745;&#24577;&#29615;&#22659;&#19979;&#65292;&#30001;&#20110;&#39640;&#24230;&#26102;&#21464;&#30340;&#20449;&#36947;&#65292;VLP&#30340;&#24615;&#33021;&#21463;&#21040;&#38480;&#21046;&#12290;&#20026;&#20102;&#25552;&#39640;&#38750;&#38745;&#24577;&#29615;&#22659;&#19979;&#30340;&#23450;&#20301;&#31934;&#24230;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#30340;&#21512;&#20316;VLP&#26041;&#26696;&#12290;&#21033;&#29992;FL&#26694;&#26550;&#65292;&#29992;&#25143;&#21487;&#20197;&#20849;&#21516;&#35757;&#32451;&#36866;&#24212;&#29615;&#22659;&#21464;&#21270;&#30340;&#20840;&#23616;&#27169;&#22411;&#65292;&#32780;&#19981;&#20849;&#20139;&#29992;&#25143;&#30340;&#31169;&#26377;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21512;&#20316;&#21487;&#35265;&#20809;&#23450;&#20301;&#32593;&#32476;&#65288;CVPosNet&#65289;&#65292;&#20197;&#21152;&#36895;&#25910;&#25947;&#36895;&#24230;&#21644;&#25552;&#39640;&#23450;&#20301;&#31934;&#24230;&#12290;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#26696;&#22312;&#38750;&#38745;&#24577;&#29615;&#22659;&#19979;&#20248;&#20110;&#22522;&#20934;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visible light positioning (VLP) has drawn plenty of attention as a promising indoor positioning technique. However, in nonstationary environments, the performance of VLP is limited because of the highly time-varying channels. To improve the positioning accuracy and generalization capability in nonstationary environments, a cooperative VLP scheme based on federated learning (FL) is proposed in this paper. Exploiting the FL framework, a global model adaptive to environmental changes can be jointly trained by users without sharing private data of users. Moreover, a Cooperative Visible-light Positioning Network (CVPosNet) is proposed to accelerate the convergence rate and improve the positioning accuracy. Simulation results show that the proposed scheme outperforms the benchmark schemes, especially in nonstationary environments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26174;&#24335;&#30340;FL&#21098;&#26525;&#26694;&#26550;FedLP&#65292;&#37319;&#29992;&#23616;&#37096;&#35757;&#32451;&#21644;&#32852;&#37030;&#26356;&#26032;&#20013;&#30340;&#23618;&#27425;&#21098;&#26525;&#65292;&#23545;&#19981;&#21516;&#31867;&#22411;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20855;&#26377;&#26222;&#36866;&#24615;&#65292;&#21487;&#20197;&#32531;&#35299;&#36890;&#20449;&#21644;&#35745;&#31639;&#30340;&#31995;&#32479;&#29942;&#39048;&#65292;&#24182;&#19988;&#24615;&#33021;&#19979;&#38477;&#36739;&#23567;&#12290;</title><link>http://arxiv.org/abs/2303.06360</link><description>&lt;p&gt;
FedLP: &#19968;&#31181;&#29992;&#20110;&#36890;&#20449;&#35745;&#31639;&#39640;&#25928;&#30340;&#32852;&#37030;&#23398;&#20064;&#30340;&#23618;&#27425;&#21098;&#26525;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
FedLP: Layer-wise Pruning Mechanism for Communication-Computation Efficient Federated Learning. (arXiv:2303.06360v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06360
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26174;&#24335;&#30340;FL&#21098;&#26525;&#26694;&#26550;FedLP&#65292;&#37319;&#29992;&#23616;&#37096;&#35757;&#32451;&#21644;&#32852;&#37030;&#26356;&#26032;&#20013;&#30340;&#23618;&#27425;&#21098;&#26525;&#65292;&#23545;&#19981;&#21516;&#31867;&#22411;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20855;&#26377;&#26222;&#36866;&#24615;&#65292;&#21487;&#20197;&#32531;&#35299;&#36890;&#20449;&#21644;&#35745;&#31639;&#30340;&#31995;&#32479;&#29942;&#39048;&#65292;&#24182;&#19988;&#24615;&#33021;&#19979;&#38477;&#36739;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an explicit FL pruning framework, FedLP, which adopts layer-wise pruning in local training and federated updating, and is model-agnostic and universal for different types of deep learning models. FedLP can relieve the system bottlenecks of communication and computation with marginal performance decay.
&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#39640;&#25928;&#19988;&#38544;&#31169;&#20445;&#25252;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#26041;&#26696;&#12290;&#26412;&#25991;&#20027;&#35201;&#20851;&#27880;FL&#20013;&#35745;&#31639;&#21644;&#36890;&#20449;&#30340;&#20248;&#21270;&#65292;&#37319;&#29992;&#23616;&#37096;&#35757;&#32451;&#21644;&#32852;&#37030;&#26356;&#26032;&#20013;&#30340;&#23618;&#27425;&#21098;&#26525;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26174;&#24335;&#30340;FL&#21098;&#26525;&#26694;&#26550;FedLP&#65288;Federated Layer-wise Pruning&#65289;&#65292;&#35813;&#26694;&#26550;&#23545;&#19981;&#21516;&#31867;&#22411;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20855;&#26377;&#26222;&#36866;&#24615;&#12290;&#20026;&#20855;&#26377;&#21516;&#36136;&#26412;&#22320;&#27169;&#22411;&#21644;&#24322;&#36136;&#26412;&#22320;&#27169;&#22411;&#30340;&#22330;&#26223;&#35774;&#35745;&#20102;&#20004;&#31181;&#29305;&#23450;&#30340;FedLP&#26041;&#26696;&#12290;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#39564;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;FedLP&#21487;&#20197;&#32531;&#35299;&#36890;&#20449;&#21644;&#35745;&#31639;&#30340;&#31995;&#32479;&#29942;&#39048;&#65292;&#24182;&#19988;&#24615;&#33021;&#19979;&#38477;&#36739;&#23567;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;FedLP&#26159;&#31532;&#19968;&#20010;&#27491;&#24335;&#23558;&#23618;&#27425;&#21098;&#26525;&#24341;&#20837;FL&#30340;&#26694;&#26550;&#12290;&#22312;&#32852;&#37030;&#23398;&#20064;&#33539;&#22260;&#20869;&#65292;&#21487;&#20197;&#22522;&#20110;FedLP&#36827;&#19968;&#27493;&#35774;&#35745;&#26356;&#22810;&#30340;&#21464;&#20307;&#21644;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) has prevailed as an efficient and privacy-preserved scheme for distributed learning. In this work, we mainly focus on the optimization of computation and communication in FL from a view of pruning. By adopting layer-wise pruning in local training and federated updating, we formulate an explicit FL pruning framework, FedLP (Federated Layer-wise Pruning), which is model-agnostic and universal for different types of deep learning models. Two specific schemes of FedLP are designed for scenarios with homogeneous local models and heterogeneous ones. Both theoretical and experimental evaluations are developed to verify that FedLP relieves the system bottlenecks of communication and computation with marginal performance decay. To the best of our knowledge, FedLP is the first framework that formally introduces the layer-wise pruning into FL. Within the scope of federated learning, more variants and combinations can be further designed based on FedLP.
&lt;/p&gt;</description></item><item><title>AutoMLP&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#24207;&#21015;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#33258;&#21160;&#21270;&#21644;&#33258;&#36866;&#24212;&#25628;&#32034;&#31639;&#27861;&#65292;&#26356;&#22909;&#22320;&#27169;&#25311;&#29992;&#25143;&#30340;&#38271;&#26399;/&#30701;&#26399;&#20852;&#36259;&#65292;&#23454;&#29616;&#26356;&#22909;&#30340;&#25512;&#33616;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.06337</link><description>&lt;p&gt;
AutoMLP: &#33258;&#21160;&#21270;MLP&#29992;&#20110;&#24207;&#21015;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
AutoMLP: Automated MLP for Sequential Recommendations. (arXiv:2303.06337v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06337
&lt;/p&gt;
&lt;p&gt;
AutoMLP&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#24207;&#21015;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#33258;&#21160;&#21270;&#21644;&#33258;&#36866;&#24212;&#25628;&#32034;&#31639;&#27861;&#65292;&#26356;&#22909;&#22320;&#27169;&#25311;&#29992;&#25143;&#30340;&#38271;&#26399;/&#30701;&#26399;&#20852;&#36259;&#65292;&#23454;&#29616;&#26356;&#22909;&#30340;&#25512;&#33616;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
AutoMLP is a novel sequential recommender system that models users' long/short-term interests through an automated and adaptive search algorithm, achieving better recommendation performance.
&lt;/p&gt;
&lt;p&gt;
&#24207;&#21015;&#25512;&#33616;&#31995;&#32479;&#26088;&#22312;&#26681;&#25454;&#29992;&#25143;&#30340;&#21382;&#21490;&#20132;&#20114;&#26469;&#39044;&#27979;&#20182;&#20204;&#19979;&#19968;&#20010;&#24863;&#20852;&#36259;&#30340;&#39033;&#30446;&#12290;&#28982;&#32780;&#65292;&#38271;&#26399;&#23384;&#22312;&#30340;&#38382;&#39064;&#26159;&#22914;&#20309;&#21306;&#20998;&#29992;&#25143;&#30340;&#38271;&#26399;/&#30701;&#26399;&#20852;&#36259;&#65292;&#36825;&#21487;&#33021;&#26159;&#24322;&#36136;&#30340;&#24182;&#23545;&#19979;&#19968;&#20010;&#25512;&#33616;&#20135;&#29983;&#19981;&#21516;&#30340;&#36129;&#29486;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#36890;&#36807;&#31351;&#20030;&#25628;&#32034;&#25110;&#32463;&#39564;&#32463;&#39564;&#35774;&#32622;&#39044;&#23450;&#20041;&#30340;&#30701;&#26399;&#20852;&#36259;&#38271;&#24230;&#65292;&#36825;&#26082;&#39640;&#24230;&#20302;&#25928;&#21448;&#20135;&#29983;&#27425;&#20248;&#32467;&#26524;&#12290;&#26368;&#36817;&#30340;&#20808;&#36827;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#27169;&#22411;&#21487;&#20197;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#23613;&#31649;&#23384;&#22312;&#19978;&#36848;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#23545;&#36755;&#20837;&#24207;&#21015;&#30340;&#38271;&#24230;&#20855;&#26377;&#20108;&#27425;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24207;&#21015;&#25512;&#33616;&#31995;&#32479;AutoMLP&#65292;&#26088;&#22312;&#26356;&#22909;&#22320;&#27169;&#25311;&#29992;&#25143;&#30340;&#38271;&#26399;/&#30701;&#26399;&#20852;&#36259;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#21644;&#33258;&#36866;&#24212;&#25628;&#32034;&#31639;&#27861;&#65292;&#20197;&#36890;&#36807;&#31471;&#21040;&#31471;&#20248;&#21270;&#33719;&#24471;&#26356;&#22909;&#30340;&#30701;&#26399;&#20852;&#36259;&#38271;&#24230;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;AutoMLP&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequential recommender systems aim to predict users' next interested item given their historical interactions. However, a long-standing issue is how to distinguish between users' long/short-term interests, which may be heterogeneous and contribute differently to the next recommendation. Existing approaches usually set pre-defined short-term interest length by exhaustive search or empirical experience, which is either highly inefficient or yields subpar results. The recent advanced transformer-based models can achieve state-of-the-art performances despite the aforementioned issue, but they have a quadratic computational complexity to the length of the input sequence. To this end, this paper proposes a novel sequential recommender system, AutoMLP, aiming for better modeling users' long/short-term interests from their historical interactions. In addition, we design an automated and adaptive search algorithm for preferable short-term interest length via end-to-end optimization. Through ext
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#35780;&#20272;&#26694;&#26550;Parachute&#65292;&#29992;&#20110;&#20132;&#20114;&#24335;&#20849;&#21516;&#25776;&#20889;&#31995;&#32479;&#30340;&#35780;&#20272;&#65292;&#35813;&#26694;&#26550;&#21253;&#21547;&#20102;&#20998;&#31867;&#30340;&#23454;&#29992;&#25351;&#26631;&#65292;&#21487;&#20197;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#20849;&#21516;&#25776;&#20889;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2303.06333</link><description>&lt;p&gt;
&#38477;&#33853;&#20254;&#65306;&#35780;&#20272;&#20132;&#20114;&#24335;&#20154;&#26426;&#20849;&#21516;&#25776;&#20889;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Parachute: Evaluating Interactive Human-LM Co-writing Systems. (arXiv:2303.06333v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06333
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#35780;&#20272;&#26694;&#26550;Parachute&#65292;&#29992;&#20110;&#20132;&#20114;&#24335;&#20849;&#21516;&#25776;&#20889;&#31995;&#32479;&#30340;&#35780;&#20272;&#65292;&#35813;&#26694;&#26550;&#21253;&#21547;&#20102;&#20998;&#31867;&#30340;&#23454;&#29992;&#25351;&#26631;&#65292;&#21487;&#20197;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#20849;&#21516;&#25776;&#20889;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a human-centered evaluation framework, Parachute, for interactive co-writing systems, which includes categorized practical metrics and can be used to evaluate and compare co-writing systems.
&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#39134;&#36895;&#21457;&#23637;&#24341;&#36215;&#20102;&#20154;&#20204;&#23545;&#20110;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#20849;&#21516;&#25776;&#20889;&#31995;&#32479;&#30340;&#26497;&#22823;&#20852;&#36259;&#65292;&#20854;&#20013;&#20154;&#31867;&#21644;&#35821;&#35328;&#27169;&#22411;&#20132;&#20114;&#22320;&#20026;&#20849;&#21516;&#30340;&#20889;&#20316;&#25104;&#26524;&#20570;&#20986;&#36129;&#29486;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#23545;&#20110;&#20132;&#20114;&#24335;&#29615;&#22659;&#19979;&#20849;&#21516;&#25776;&#20889;&#31995;&#32479;&#30340;&#35780;&#20272;&#30740;&#31350;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#35780;&#20272;&#26694;&#26550;Parachute&#65292;&#29992;&#20110;&#20132;&#20114;&#24335;&#20849;&#21516;&#25776;&#20889;&#31995;&#32479;&#30340;&#35780;&#20272;&#12290;Parachute&#23637;&#31034;&#20102;&#20132;&#20114;&#35780;&#20272;&#30340;&#32508;&#21512;&#35270;&#35282;&#65292;&#20854;&#20013;&#27599;&#20010;&#35780;&#20272;&#26041;&#38754;&#37117;&#21253;&#21547;&#20102;&#20998;&#31867;&#30340;&#23454;&#29992;&#25351;&#26631;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20351;&#29992;&#26696;&#20363;&#26469;&#28436;&#31034;&#22914;&#20309;&#20351;&#29992;Parachute&#35780;&#20272;&#21644;&#27604;&#36739;&#20849;&#21516;&#25776;&#20889;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
A surge of advances in language models (LMs) has led to significant interest in using LMs to build co-writing systems, in which humans and LMs interactively contribute to a shared writing artifact. However, there is a lack of studies assessing co-writing systems in interactive settings. We propose a human-centered evaluation framework, Parachute, for interactive co-writing systems. Parachute showcases an integrative view of interaction evaluation, where each evaluation aspect consists of categorized practical metrics. Furthermore, we present Parachute with a use case to demonstrate how to evaluate and compare co-writing systems using Parachute.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#24182;&#34892;&#31639;&#27861;&#65292;&#32467;&#21512;&#20102;&#24352;&#37327;&#12289;&#19987;&#23478;&#21644;&#25968;&#25454;&#24182;&#34892;&#65292;&#20197;&#23454;&#29616;MoE&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#20854;&#22522;&#26412;&#27169;&#22411;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;DeepSpeed-MoE&#22823;4-8&#20493;&#12290;</title><link>http://arxiv.org/abs/2303.06318</link><description>&lt;p&gt;
&#19968;&#31181;&#26032;&#30340;&#24352;&#37327;&#19987;&#23478;&#28151;&#21512;&#24182;&#34892;&#26041;&#27861;&#26469;&#25193;&#23637;&#28151;&#21512;&#19987;&#23478;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
A Novel Tensor-Expert Hybrid Parallelism Approach to Scale Mixture-of-Experts Training. (arXiv:2303.06318v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06318
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#24182;&#34892;&#31639;&#27861;&#65292;&#32467;&#21512;&#20102;&#24352;&#37327;&#12289;&#19987;&#23478;&#21644;&#25968;&#25454;&#24182;&#34892;&#65292;&#20197;&#23454;&#29616;MoE&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#20854;&#22522;&#26412;&#27169;&#22411;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;DeepSpeed-MoE&#22823;4-8&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel hybrid parallel algorithm that combines tensor, expert, and data parallelism to enable the training of MoE models with 4-8x larger base models than the current state-of-the-art -- DeepSpeed-MoE.
&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Mixture-of-Experts&#65288;MoE&#65289;&#30340;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#28155;&#21152;&#31232;&#30095;&#28608;&#27963;&#30340;&#19987;&#23478;&#22359;&#26469;&#22686;&#21152;&#31070;&#32463;&#32593;&#32476;&#65288;&#22522;&#26412;&#27169;&#22411;&#65289;&#30340;&#21442;&#25968;&#65292;&#32780;&#19981;&#25913;&#21464;&#35757;&#32451;&#25110;&#25512;&#29702;&#30340;&#24635;&#28014;&#28857;&#25805;&#20316;&#25968;&#12290;&#29702;&#35770;&#19978;&#65292;&#36825;&#31181;&#26550;&#26500;&#20801;&#35768;&#25105;&#20204;&#35757;&#32451;&#20219;&#24847;&#22823;&#30340;&#27169;&#22411;&#65292;&#21516;&#26102;&#20445;&#25345;&#35745;&#31639;&#25104;&#26412;&#19982;&#22522;&#26412;&#27169;&#22411;&#30456;&#21516;&#12290;&#28982;&#32780;&#65292;&#22312;64&#21040;128&#20010;&#19987;&#23478;&#22359;&#20043;&#22806;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#35266;&#23519;&#21040;&#36825;&#20123;MoE&#27169;&#22411;&#30340;&#27979;&#35797;&#20934;&#30830;&#24615;&#36882;&#20943;&#12290;&#22240;&#27492;&#65292;&#35757;&#32451;&#39640;&#36136;&#37327;&#30340;MoE&#27169;&#22411;&#38656;&#35201;&#25105;&#20204;&#25193;&#23637;&#22522;&#26412;&#27169;&#22411;&#30340;&#22823;&#23567;&#20197;&#21450;&#19987;&#23478;&#22359;&#30340;&#25968;&#37327;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19977;&#32500;&#28151;&#21512;&#24182;&#34892;&#31639;&#27861;&#65292;&#32467;&#21512;&#20102;&#24352;&#37327;&#12289;&#19987;&#23478;&#21644;&#25968;&#25454;&#24182;&#34892;&#65292;&#20197;&#23454;&#29616;MoE&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#20854;&#22522;&#26412;&#27169;&#22411;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;DeepSpeed-MoE&#22823;4-8&#20493;&#12290;&#25105;&#20204;&#22312;&#20248;&#21270;&#22120;&#27493;&#39588;&#20013;&#25552;&#20986;&#20102;&#20869;&#23384;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
A new neural network architecture called Mixture-of-Experts (MoE) has been proposed recently that increases the parameters of a neural network (the base model) by adding sparsely activated expert blocks, without changing the total number of floating point operations for training or inference. In theory, this architecture allows us to train arbitrarily large models while keeping the computational costs same as that of the base model. However, beyond 64 to 128 experts blocks, prior work has observed diminishing returns in the test accuracies of these MoE models. Thus, training high quality MoE models requires us to scale the size of the base models, along with the number of expert blocks. In this work, we propose a novel, three-dimensional, hybrid parallel algorithm that combines tensor, expert, and data parallelism to enable the training of MoE models with 4-8x larger base models than the current state-of-the-art -- DeepSpeed-MoE. We propose memory optimizations in the optimizer step, a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#26426;&#22120;&#23398;&#20064;&#32593;&#32476;&#20013;&#30340;&#23545;&#25239;&#25915;&#20987;&#21644;&#38450;&#24481;&#25216;&#26415;&#65292;&#37325;&#28857;&#20851;&#27880;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#31867;&#27169;&#22411;&#12290;&#23545;&#26368;&#36817;&#30340;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#21644;&#26368;&#20808;&#36827;&#30340;&#23545;&#25239;&#38450;&#24481;&#25216;&#26415;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#31867;&#65292;&#24182;&#20197;&#35270;&#35273;&#19978;&#21560;&#24341;&#20154;&#30340;&#34920;&#26684;&#21644;&#26641;&#29366;&#22270;&#30340;&#24418;&#24335;&#21576;&#29616;&#12290;&#26041;&#27861;&#20998;&#31867;&#20026;&#21453;&#25915;&#20987;&#26816;&#27979;&#21644;&#40065;&#26834;&#24615;&#22686;&#24378;&#65292;&#29305;&#21035;&#20851;&#27880;&#20110;&#22522;&#20110;&#27491;&#21017;&#21270;&#30340;&#22686;&#24378;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#12290;&#36824;&#25506;&#35752;&#20102;&#26032;&#30340;&#25915;&#20987;&#36884;&#24452;&#65292;&#21253;&#25324;&#22522;&#20110;&#25628;&#32034;&#30340;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2303.06302</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#32593;&#32476;&#20013;&#30340;&#23545;&#25239;&#25915;&#20987;&#19982;&#38450;&#24481;&#65306;&#29616;&#20195;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Adversarial Attacks and Defenses in Machine Learning-Powered Networks: A Contemporary Survey. (arXiv:2303.06302v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06302
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#26426;&#22120;&#23398;&#20064;&#32593;&#32476;&#20013;&#30340;&#23545;&#25239;&#25915;&#20987;&#21644;&#38450;&#24481;&#25216;&#26415;&#65292;&#37325;&#28857;&#20851;&#27880;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#31867;&#27169;&#22411;&#12290;&#23545;&#26368;&#36817;&#30340;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#21644;&#26368;&#20808;&#36827;&#30340;&#23545;&#25239;&#38450;&#24481;&#25216;&#26415;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#31867;&#65292;&#24182;&#20197;&#35270;&#35273;&#19978;&#21560;&#24341;&#20154;&#30340;&#34920;&#26684;&#21644;&#26641;&#29366;&#22270;&#30340;&#24418;&#24335;&#21576;&#29616;&#12290;&#26041;&#27861;&#20998;&#31867;&#20026;&#21453;&#25915;&#20987;&#26816;&#27979;&#21644;&#40065;&#26834;&#24615;&#22686;&#24378;&#65292;&#29305;&#21035;&#20851;&#27880;&#20110;&#22522;&#20110;&#27491;&#21017;&#21270;&#30340;&#22686;&#24378;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#12290;&#36824;&#25506;&#35752;&#20102;&#26032;&#30340;&#25915;&#20987;&#36884;&#24452;&#65292;&#21253;&#25324;&#22522;&#20110;&#25628;&#32034;&#30340;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
This survey provides a comprehensive overview of recent advancements in adversarial attack and defense techniques in machine learning and deep neural networks, with a focus on deep neural network-based classification models. The methods are classified into counter-attack detection and robustness enhancement, with a specific focus on regularization-based methods for enhancing robustness. New avenues of attack are also explored, including search-based attacks.
&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#28145;&#24230;&#23398;&#20064;&#22312;&#20114;&#32852;&#32593;&#21644;&#30456;&#20851;&#22330;&#26223;&#20013;&#30340;&#24555;&#36895;&#22686;&#38271;&#24212;&#29992;&#65292;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#23545;&#25239;&#25915;&#20987;&#21644;&#38450;&#24481;&#24050;&#32463;&#24341;&#36215;&#20102;&#26497;&#22823;&#30340;&#20851;&#27880;&#12290;&#26412;&#32508;&#36848;&#20840;&#38754;&#27010;&#36848;&#20102;&#23545;&#25239;&#25915;&#20987;&#21644;&#38450;&#24481;&#25216;&#26415;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#37325;&#28857;&#20851;&#27880;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#31867;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26681;&#25454;&#25915;&#20987;&#21407;&#29702;&#23545;&#26368;&#36817;&#30340;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#21644;&#26368;&#20808;&#36827;&#30340;&#23545;&#25239;&#38450;&#24481;&#25216;&#26415;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#31867;&#65292;&#24182;&#20197;&#35270;&#35273;&#19978;&#21560;&#24341;&#20154;&#30340;&#34920;&#26684;&#21644;&#26641;&#29366;&#22270;&#30340;&#24418;&#24335;&#21576;&#29616;&#12290;&#36825;&#26159;&#22522;&#20110;&#23545;&#29616;&#26377;&#24037;&#20316;&#30340;&#20005;&#26684;&#35780;&#20272;&#65292;&#21253;&#25324;&#23545;&#20854;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#30340;&#20998;&#26512;&#12290;&#25105;&#20204;&#36824;&#23558;&#26041;&#27861;&#20998;&#31867;&#20026;&#21453;&#25915;&#20987;&#26816;&#27979;&#21644;&#40065;&#26834;&#24615;&#22686;&#24378;&#65292;&#29305;&#21035;&#20851;&#27880;&#20110;&#22522;&#20110;&#27491;&#21017;&#21270;&#30340;&#22686;&#24378;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#12290;&#36824;&#25506;&#35752;&#20102;&#26032;&#30340;&#25915;&#20987;&#36884;&#24452;&#65292;&#21253;&#25324;&#22522;&#20110;&#25628;&#32034;&#30340;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial attacks and defenses in machine learning and deep neural network have been gaining significant attention due to the rapidly growing applications of deep learning in the Internet and relevant scenarios. This survey provides a comprehensive overview of the recent advancements in the field of adversarial attack and defense techniques, with a focus on deep neural network-based classification models. Specifically, we conduct a comprehensive classification of recent adversarial attack methods and state-of-the-art adversarial defense techniques based on attack principles, and present them in visually appealing tables and tree diagrams. This is based on a rigorous evaluation of the existing works, including an analysis of their strengths and limitations. We also categorize the methods into counter-attack detection and robustness enhancement, with a specific focus on regularization-based methods for enhancing robustness. New avenues of attack are also explored, including search-base
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;Transformer&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#21457;&#29616;&#20302;&#27880;&#24847;&#21147;&#29109;&#20276;&#38543;&#30528;&#39640;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;$\sigma$Reparam&#65292;&#25104;&#21151;&#22320;&#38450;&#27490;&#20102;&#27880;&#24847;&#21147;&#23618;&#20013;&#30340;&#29109;&#23849;&#28291;&#65292;&#20419;&#36827;&#20102;&#26356;&#31283;&#23450;&#30340;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2303.06296</link><description>&lt;p&gt;
&#38450;&#27490;&#27880;&#24847;&#21147;&#29109;&#23849;&#28291;&#30340;Transformer&#35757;&#32451;&#31283;&#23450;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Stabilizing Transformer Training by Preventing Attention Entropy Collapse. (arXiv:2303.06296v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06296
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Transformer&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#21457;&#29616;&#20302;&#27880;&#24847;&#21147;&#29109;&#20276;&#38543;&#30528;&#39640;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;$\sigma$Reparam&#65292;&#25104;&#21151;&#22320;&#38450;&#27490;&#20102;&#27880;&#24847;&#21147;&#23618;&#20013;&#30340;&#29109;&#23849;&#28291;&#65292;&#20419;&#36827;&#20102;&#26356;&#31283;&#23450;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the training dynamics of Transformers and proposes a simple and efficient solution, $\sigma$Reparam, to prevent entropy collapse in the attention layers, promoting more stable training.
&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#31283;&#23450;&#24615;&#23545;&#20110;Transformer&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#27880;&#24847;&#21147;&#23618;&#30340;&#28436;&#21464;&#26469;&#25506;&#31350;Transformer&#30340;&#35757;&#32451;&#21160;&#24577;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36319;&#36394;&#27599;&#20010;&#27880;&#24847;&#21147;&#22836;&#30340;&#27880;&#24847;&#21147;&#29109;&#65292;&#36825;&#26159;&#27169;&#22411;&#38160;&#24230;&#30340;&#20195;&#29702;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#19981;&#21516;&#30340;&#26550;&#26500;&#21644;&#20219;&#21153;&#20013;&#23384;&#22312;&#19968;&#31181;&#24120;&#35265;&#27169;&#24335;&#65292;&#21363;&#20302;&#27880;&#24847;&#21147;&#29109;&#20276;&#38543;&#30528;&#39640;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#65292;&#36825;&#21487;&#33021;&#37319;&#21462;&#25391;&#33633;&#25439;&#22833;&#25110;&#21457;&#25955;&#30340;&#24418;&#24335;&#12290;&#25105;&#20204;&#23558;&#30149;&#24577;&#20302;&#27880;&#24847;&#21147;&#29109;&#65292;&#23545;&#24212;&#39640;&#24230;&#38598;&#20013;&#30340;&#27880;&#24847;&#21147;&#20998;&#25968;&#65292;&#31216;&#20026;$\textit{&#29109;&#23849;&#28291;}$&#12290;&#20316;&#20026;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;$\sigma$Reparam&#65292;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20854;&#20013;&#25105;&#20204;&#20351;&#29992;&#35889;&#24402;&#19968;&#21270;&#21644;&#39069;&#22806;&#30340;&#23398;&#20064;&#26631;&#37327;&#37325;&#26032;&#21442;&#25968;&#21270;&#25152;&#26377;&#32447;&#24615;&#23618;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#37325;&#26032;&#21442;&#25968;&#21270;&#25104;&#21151;&#22320;&#38450;&#27490;&#20102;&#27880;&#24847;&#21147;&#23618;&#20013;&#30340;&#29109;&#23849;&#28291;&#65292;&#20419;&#36827;&#20102;&#26356;&#31283;&#23450;&#30340;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;
&lt;/p&gt;
&lt;p&gt;
Training stability is of great importance to Transformers. In this work, we investigate the training dynamics of Transformers by examining the evolution of the attention layers. In particular, we track the attention entropy for each attention head during the course of training, which is a proxy for model sharpness. We identify a common pattern across different architectures and tasks, where low attention entropy is accompanied by high training instability, which can take the form of oscillating loss or divergence. We denote the pathologically low attention entropy, corresponding to highly concentrated attention scores, as $\textit{entropy collapse}$. As a remedy, we propose $\sigma$Reparam, a simple and efficient solution where we reparametrize all linear layers with spectral normalization and an additional learned scalar. We demonstrate that the proposed reparameterization successfully prevents entropy collapse in the attention layers, promoting more stable training. Additionally, we 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;ChatGPT&#30340;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#21457;&#29616;&#23613;&#31649;&#23427;&#20855;&#26377;&#26356;&#22909;&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#65292;&#20294;&#20173;&#28982;&#32463;&#24120;&#26080;&#27861;&#29983;&#25104;&#36923;&#36753;&#19978;&#27491;&#30830;&#30340;&#39044;&#27979;&#12290;&#22240;&#27492;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#38656;&#35201;&#36827;&#19968;&#27493;&#32771;&#34385;&#65292;&#29305;&#21035;&#26159;&#22312;&#39118;&#38505;&#26041;&#38754;&#12290;</title><link>http://arxiv.org/abs/2303.06273</link><description>&lt;p&gt;
ChatGPT&#30340;&#19968;&#33268;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Consistency Analysis of ChatGPT. (arXiv:2303.06273v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06273
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;ChatGPT&#30340;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#21457;&#29616;&#23613;&#31649;&#23427;&#20855;&#26377;&#26356;&#22909;&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#65292;&#20294;&#20173;&#28982;&#32463;&#24120;&#26080;&#27861;&#29983;&#25104;&#36923;&#36753;&#19978;&#27491;&#30830;&#30340;&#39044;&#27979;&#12290;&#22240;&#27492;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#38656;&#35201;&#36827;&#19968;&#27493;&#32771;&#34385;&#65292;&#29305;&#21035;&#26159;&#22312;&#39118;&#38505;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the consistency issue of ChatGPT and finds that although it has improved language understanding ability, it frequently fails to generate logically correct predictions. Therefore, further consideration is needed for its real-world applications, especially in terms of risk.
&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38382;&#31572;&#23545;&#35805;&#31995;&#32479;&#65292;&#33258;&#25512;&#20986;&#20197;&#26469;&#24191;&#21463;&#27426;&#36814;&#12290;&#34429;&#28982;&#23427;&#22312;&#27861;&#24459;&#12289;&#21307;&#23398;&#21644;&#37329;&#34701;&#31561;&#39046;&#22495;&#30340;&#19987;&#19994;&#32771;&#35797;&#20013;&#21462;&#24471;&#20102;&#19981;&#38169;&#30340;&#25104;&#32489;&#65292;&#20294;&#20063;&#26377;&#20154;&#23545;&#20854;&#21487;&#38752;&#24615;&#21644;&#20449;&#20219;&#24230;&#34920;&#31034;&#24576;&#30097;&#12290;&#26412;&#25991;&#38024;&#23545;ChatGPT&#22312;&#36923;&#36753;&#19968;&#33268;&#24615;&#26041;&#38754;&#30340;&#21487;&#20449;&#24230;&#36827;&#34892;&#20102;&#35843;&#26597;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#23613;&#31649;ChatGPT&#20284;&#20046;&#20855;&#26377;&#26356;&#22909;&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#65292;&#20294;&#23427;&#20173;&#28982;&#32463;&#24120;&#26080;&#27861;&#29983;&#25104;&#36923;&#36753;&#19978;&#27491;&#30830;&#30340;&#39044;&#27979;&#12290;&#22240;&#27492;&#65292;&#34429;&#28982;ChatGPT&#26159;&#19968;&#31181;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#21644;&#26377;&#21069;&#36884;&#30340;&#26032;&#25216;&#26415;&#65292;&#20294;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#22914;&#26524;&#27809;&#26377;&#32463;&#36807;&#24443;&#24213;&#30340;&#20154;&#24037;&#26816;&#26597;&#65292;&#23427;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#38656;&#35201;&#36827;&#19968;&#27493;&#32771;&#34385;&#65292;&#29305;&#21035;&#26159;&#22312;&#39118;&#38505;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT, a question-and-answer dialogue system based on a large language model, has gained huge popularity since its introduction. Its positive aspects have been reported through many media platforms, and some analyses even showed that ChatGPT achieved a decent grade in professional exams, including the law, medical, and finance domains, adding extra support to the claim that AI now can assist and, even, replace humans in industrial fields. Others, however, doubt its reliability and trustworthiness. In this paper, we investigate ChatGPT's trustworthiness regarding logically consistent behaviours. Our findings suggest that, although ChatGPT seems to achieve an improved language understanding ability, it still fails to generate logically correct predictions frequently. Hence, while it is true that ChatGPT is an impressive and promising new technique, we conclude that its usage in real-world applications without thorough human inspection requires further consideration, especially for risk
&lt;/p&gt;</description></item><item><title>STAIR&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#24322;&#24120;&#20540;&#27719;&#24635;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#32452;&#32039;&#20945;&#30340;&#20154;&#31867;&#21487;&#29702;&#35299;&#35268;&#21017;&#65292;&#20197;&#27719;&#24635;&#21644;&#35299;&#37322;&#24322;&#24120;&#26816;&#27979;&#32467;&#26524;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#20197;&#20934;&#30830;&#22320;&#24635;&#32467;&#26816;&#27979;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.06261</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#24322;&#24120;&#20540;&#27719;&#24635;
&lt;/p&gt;
&lt;p&gt;
Interpretable Outlier Summarization. (arXiv:2303.06261v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06261
&lt;/p&gt;
&lt;p&gt;
STAIR&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#24322;&#24120;&#20540;&#27719;&#24635;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#32452;&#32039;&#20945;&#30340;&#20154;&#31867;&#21487;&#29702;&#35299;&#35268;&#21017;&#65292;&#20197;&#27719;&#24635;&#21644;&#35299;&#37322;&#24322;&#24120;&#26816;&#27979;&#32467;&#26524;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#20197;&#20934;&#30830;&#22320;&#24635;&#32467;&#26816;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
STAIR proposes an interpretable outlier summarization method by learning a compact set of human understandable rules to summarize and explain the anomaly detection results, which has strong interpretability to accurately summarize the detection results.
&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#20540;&#26816;&#27979;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#20197;&#38450;&#27490;&#37329;&#34701;&#27450;&#35784;&#12289;&#38450;&#24481;&#32593;&#32476;&#20837;&#20405;&#25110;&#26816;&#27979;&#21363;&#23558;&#21457;&#29983;&#30340;&#35774;&#22791;&#25925;&#38556;&#12290;&#20026;&#20102;&#20943;&#23569;&#20154;&#21147;&#35780;&#20272;&#24322;&#24120;&#20540;&#26816;&#27979;&#32467;&#26524;&#30340;&#24037;&#20316;&#37327;&#65292;&#24182;&#26377;&#25928;&#22320;&#23558;&#24322;&#24120;&#20540;&#36716;&#21270;&#20026;&#21487;&#25805;&#20316;&#30340;&#35265;&#35299;&#65292;&#29992;&#25143;&#36890;&#24120;&#24076;&#26395;&#31995;&#32479;&#33258;&#21160;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#24322;&#24120;&#20540;&#26816;&#27979;&#32467;&#26524;&#30340;&#23376;&#32452;&#30340;&#27719;&#24635;&#12290;&#28982;&#32780;&#65292;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#27809;&#26377;&#36825;&#26679;&#30340;&#31995;&#32479;&#23384;&#22312;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;STAIR&#65292;&#23427;&#23398;&#20064;&#20102;&#19968;&#32452;&#32039;&#20945;&#30340;&#20154;&#31867;&#21487;&#29702;&#35299;&#35268;&#21017;&#65292;&#20197;&#27719;&#24635;&#21644;&#35299;&#37322;&#24322;&#24120;&#26816;&#27979;&#32467;&#26524;&#12290;STAIR&#19981;&#20351;&#29992;&#32463;&#20856;&#30340;&#20915;&#31574;&#26641;&#31639;&#27861;&#26469;&#20135;&#29983;&#36825;&#20123;&#35268;&#21017;&#65292;&#32780;&#26159;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20248;&#21270;&#30446;&#26631;&#65292;&#20197;&#20135;&#29983;&#23569;&#37327;&#35268;&#21017;&#65292;&#20855;&#26377;&#26368;&#23567;&#30340;&#22797;&#26434;&#24615;&#65292;&#22240;&#27492;&#20855;&#26377;&#24378;&#22823;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#20197;&#20934;&#30830;&#22320;&#24635;&#32467;&#26816;&#27979;&#32467;&#26524;&#12290;STAIR&#30340;&#23398;&#20064;&#31639;&#27861;&#36890;&#36807;&#36845;&#20195;&#20998;&#21106;&#22823;&#35268;&#21017;&#26469;&#20135;&#29983;&#35268;&#21017;&#38598;&#65292;&#24182;&#22312;&#27599;&#20010;i&#20013;&#26368;&#22823;&#21270;&#36825;&#20010;&#30446;&#26631;&#65292;&#26159;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Outlier detection is critical in real applications to prevent financial fraud, defend network intrusions, or detecting imminent device failures. To reduce the human effort in evaluating outlier detection results and effectively turn the outliers into actionable insights, the users often expect a system to automatically produce interpretable summarizations of subgroups of outlier detection results. Unfortunately, to date no such systems exist. To fill this gap, we propose STAIR which learns a compact set of human understandable rules to summarize and explain the anomaly detection results. Rather than use the classical decision tree algorithms to produce these rules, STAIR proposes a new optimization objective to produce a small number of rules with least complexity, hence strong interpretability, to accurately summarize the detection results. The learning algorithm of STAIR produces a rule set by iteratively splitting the large rules and is optimal in maximizing this objective in each i
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#29615;&#22659;&#22122;&#22768;&#21644;&#20809;&#32447;&#20449;&#24687;&#65292;&#24320;&#21457;&#20102;&#31532;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;ICU&#24739;&#32773;&#35893;&#22916;&#39044;&#27979;&#27169;&#22411;&#65292;&#20026;&#35893;&#22916;&#30340;&#39044;&#38450;&#21644;&#27835;&#30103;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;</title><link>http://arxiv.org/abs/2303.06253</link><description>&lt;p&gt;
ICU&#29615;&#22659;&#22122;&#22768;&#21644;&#20809;&#32447;&#20449;&#24687;&#39044;&#27979;&#35893;&#22916;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Predicting risk of delirium from ambient noise and light information in the ICU. (arXiv:2303.06253v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06253
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#29615;&#22659;&#22122;&#22768;&#21644;&#20809;&#32447;&#20449;&#24687;&#65292;&#24320;&#21457;&#20102;&#31532;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;ICU&#24739;&#32773;&#35893;&#22916;&#39044;&#27979;&#27169;&#22411;&#65292;&#20026;&#35893;&#22916;&#30340;&#39044;&#38450;&#21644;&#27835;&#30103;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study developed the first deep-learning based delirium prediction model for ICU patients using only ambient noise and light information, providing new insights for the prevention and treatment of delirium.
&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#37325;&#30151;&#30417;&#25252;&#23460;&#65288;ICU&#65289;&#35893;&#22916;&#39044;&#27979;&#27169;&#22411;&#27809;&#26377;&#32771;&#34385;&#29615;&#22659;&#22240;&#32032;&#65292;&#23613;&#31649;&#26377;&#24378;&#26377;&#21147;&#30340;&#35777;&#25454;&#34920;&#26126;&#23427;&#20204;&#23545;&#35893;&#22916;&#26377;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;Thunderboard&#12289;ActiGraph&#20256;&#24863;&#22120;&#21644;iPod with AudioTools&#24212;&#29992;&#31243;&#24207;&#65292;&#20165;&#20351;&#29992;&#29615;&#22659;&#22122;&#22768;&#21644;&#20809;&#32447;&#20449;&#24687;&#65292;&#25253;&#36947;&#20102;&#31532;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;ICU&#24739;&#32773;&#35893;&#22916;&#39044;&#27979;&#27169;&#22411;&#12290;&#36825;&#20123;&#27979;&#37327;&#25968;&#25454;&#20174;2021&#24180;5&#26376;&#33267;2022&#24180;9&#26376;&#25910;&#38598;&#33258;102&#21517;&#24739;&#32773;&#30340;ICU&#30149;&#25151;&#65292;&#20998;&#20026;&#30333;&#22825;&#65288;0700&#33267;1859&#65289;&#21644;&#22812;&#38388;&#65288;1900&#33267;0659&#65289;&#12290;&#20351;&#29992;&#36825;&#20123;&#25968;&#25454;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#39044;&#27979;ICU&#20303;&#38498;&#26399;&#38388;&#25110;&#20986;&#38498;&#21518;4&#22825;&#20869;&#30340;&#35893;&#22916;&#21457;&#29983;&#29575;&#12290;&#26368;&#21518;&#65292;&#20998;&#26512;&#32467;&#26524;&#24471;&#20998;&#20197;&#35780;&#20272;&#27599;&#20010;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#21644;&#26041;&#21521;&#24615;&#12290;&#30333;&#22825;&#30340;&#22122;&#22768;&#27700;&#24179;&#26174;&#33879;&#39640;&#20110;&#22812;&#38388;&#30340;&#22122;&#22768;&#27700;&#24179;&#12290;&#24403;&#20165;&#20351;&#29992;&#22122;&#22768;&#29305;&#24449;&#25110;&#22122;&#22768;&#21644;&#20809;&#29305;&#24449;&#30340;&#32452;&#21512;&#26102;&#65292;1-D&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Existing Intensive Care Unit (ICU) delirium prediction models do not consider environmental factors despite strong evidence of their influence on delirium. This study reports the first deep-learning based delirium prediction model for ICU patients using only ambient noise and light information. Ambient light and noise intensities were measured from ICU rooms of 102 patients from May 2021 to September 2022 using Thunderboard, ActiGraph sensors and an iPod with AudioTools application. These measurements were divided into daytime (0700 to 1859) and nighttime (1900 to 0659). Deep learning models were trained using this data to predict the incidence of delirium during ICU stay or within 4 days of discharge. Finally, outcome scores were analyzed to evaluate the importance and directionality of every feature. Daytime noise levels were significantly higher than nighttime noise levels. When using only noise features or a combination of noise and light features 1-D convolutional neural networks 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;AI&#22686;&#24378;&#30340;&#37325;&#30151;&#30417;&#25252;&#23460;&#31995;&#32479;&#65292;&#36890;&#36807;&#26222;&#36866;&#24863;&#30693;&#21644;&#25968;&#25454;&#22788;&#29702;&#65292;&#21487;&#20197;&#25913;&#21892;&#24739;&#32773;&#30340;&#35270;&#35273;&#30417;&#27979;&#21644;&#35780;&#20272;&#65292;&#25552;&#39640;&#25252;&#29702;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2303.06252</link><description>&lt;p&gt;
AI&#22686;&#24378;&#30340;&#37325;&#30151;&#30417;&#25252;&#23460;&#65306;&#36890;&#36807;&#26222;&#36866;&#24863;&#30693;&#38761;&#21629;&#24615;&#22320;&#25913;&#21892;&#24739;&#32773;&#25252;&#29702;
&lt;/p&gt;
&lt;p&gt;
AI-Enhanced Intensive Care Unit: Revolutionizing Patient Care with Pervasive Sensing. (arXiv:2303.06252v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06252
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;AI&#22686;&#24378;&#30340;&#37325;&#30151;&#30417;&#25252;&#23460;&#31995;&#32479;&#65292;&#36890;&#36807;&#26222;&#36866;&#24863;&#30693;&#21644;&#25968;&#25454;&#22788;&#29702;&#65292;&#21487;&#20197;&#25913;&#21892;&#24739;&#32773;&#30340;&#35270;&#35273;&#30417;&#27979;&#21644;&#35780;&#20272;&#65292;&#25552;&#39640;&#25252;&#29702;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces an AI-enhanced ICU system that improves patient visual monitoring and assessment, and ultimately enhances the quality of care, through pervasive sensing and data processing.
&lt;/p&gt;
&lt;p&gt;
&#37325;&#30151;&#30417;&#25252;&#23460;&#65288;ICU&#65289;&#26159;&#19968;&#20010;&#19987;&#38376;&#30340;&#21307;&#38498;&#31354;&#38388;&#65292;&#29992;&#20110;&#25509;&#21463;&#21361;&#37325;&#30149;&#20154;&#30340;&#23494;&#38598;&#25252;&#29702;&#21644;&#30417;&#27979;&#12290;&#20840;&#38754;&#30340;&#30417;&#27979;&#23545;&#20110;&#35780;&#20272;&#24739;&#32773;&#30340;&#30149;&#24773;&#65292;&#29305;&#21035;&#26159;&#30149;&#24773;&#30340;&#20005;&#37325;&#31243;&#24230;&#20197;&#21450;&#26368;&#32456;&#30340;&#25252;&#29702;&#36136;&#37327;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26102;&#38388;&#38480;&#21046;&#21644;&#21307;&#25252;&#20154;&#21592;&#30340;&#24037;&#20316;&#37327;&#65292;ICU&#20013;&#30340;&#24739;&#32773;&#30417;&#27979;&#33539;&#22260;&#21463;&#21040;&#38480;&#21046;&#12290;&#30446;&#21069;&#65292;&#21253;&#25324;&#38754;&#37096;&#34920;&#24773;&#12289;&#23039;&#21183;&#21644;&#27963;&#21160;&#33021;&#21147;&#31561;&#32454;&#33410;&#30340;&#35270;&#35273;&#35780;&#20272;&#20165;&#20598;&#23572;&#34987;&#25429;&#25417;&#21040;&#65292;&#25110;&#32773;&#26681;&#26412;&#27809;&#26377;&#34987;&#25429;&#25417;&#21040;&#12290;&#36825;&#20123;&#25163;&#21160;&#35266;&#23519;&#26159;&#20027;&#35266;&#30340;&#65292;&#23481;&#26131;&#20986;&#29616;&#25991;&#26723;&#38169;&#35823;&#65292;&#24182;&#32473;&#25252;&#29702;&#20154;&#21592;&#24102;&#26469;&#39069;&#22806;&#30340;&#24037;&#20316;&#37327;&#12290;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#31995;&#32479;&#30001;&#20110;&#20854;&#20986;&#33394;&#30340;&#23398;&#20064;&#33021;&#21147;&#65292;&#26377;&#28508;&#21147;&#22686;&#24378;&#24739;&#32773;&#30340;&#35270;&#35273;&#30417;&#27979;&#21644;&#35780;&#20272;&#12290;&#36825;&#26679;&#30340;&#31995;&#32479;&#38656;&#35201;&#24378;&#22823;&#30340;&#27880;&#37322;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#26222;&#36866;&#24863;&#30693;&#21644;&#25968;&#25454;&#22788;&#29702;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
The intensive care unit (ICU) is a specialized hospital space where critically ill patients receive intensive care and monitoring. Comprehensive monitoring is imperative in assessing patients conditions, in particular acuity, and ultimately the quality of care. However, the extent of patient monitoring in the ICU is limited due to time constraints and the workload on healthcare providers. Currently, visual assessments for acuity, including fine details such as facial expressions, posture, and mobility, are sporadically captured, or not captured at all. These manual observations are subjective to the individual, prone to documentation errors, and overburden care providers with the additional workload. Artificial Intelligence (AI) enabled systems has the potential to augment the patient visual monitoring and assessment due to their exceptional learning capabilities. Such systems require robust annotated data to train. To this end, we have developed pervasive sensing and data processing s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21306;&#22495;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#31227;&#21160;&#24863;&#30693;&#25968;&#25454;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#23558;&#29289;&#29702;&#31354;&#38388;&#21010;&#20998;&#20026;&#22320;&#29702;&#21306;&#22495;&#65292;&#24182;&#26144;&#23556;&#21040;&#31227;&#21160;&#36793;&#32536;&#20113;&#31995;&#32479;&#26550;&#26500;&#65292;&#20197;&#23454;&#29616;&#33391;&#22909;&#30340;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#27599;&#20010;&#21306;&#22495;&#37117;&#26377;&#19968;&#20010;&#32852;&#21512;&#35757;&#32451;&#27169;&#22411;&#65292;&#33021;&#22815;&#24456;&#22909;&#22320;&#36866;&#24212;&#35813;&#21306;&#22495;&#29992;&#25143;&#30340;&#25968;&#25454;&#21644;&#34892;&#20026;&#65292;&#24182;&#20445;&#25252;&#29992;&#25143;&#25968;&#25454;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2303.06246</link><description>&lt;p&gt;
&#22522;&#20110;&#21306;&#22495;&#30340;&#32852;&#37030;&#23398;&#20064;&#29992;&#20110;&#31227;&#21160;&#24863;&#30693;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Zone-based Federated Learning for Mobile Sensing Data. (arXiv:2303.06246v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06246
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21306;&#22495;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#31227;&#21160;&#24863;&#30693;&#25968;&#25454;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#23558;&#29289;&#29702;&#31354;&#38388;&#21010;&#20998;&#20026;&#22320;&#29702;&#21306;&#22495;&#65292;&#24182;&#26144;&#23556;&#21040;&#31227;&#21160;&#36793;&#32536;&#20113;&#31995;&#32479;&#26550;&#26500;&#65292;&#20197;&#23454;&#29616;&#33391;&#22909;&#30340;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#27599;&#20010;&#21306;&#22495;&#37117;&#26377;&#19968;&#20010;&#32852;&#21512;&#35757;&#32451;&#27169;&#22411;&#65292;&#33021;&#22815;&#24456;&#22909;&#22320;&#36866;&#24212;&#35813;&#21306;&#22495;&#29992;&#25143;&#30340;&#25968;&#25454;&#21644;&#34892;&#20026;&#65292;&#24182;&#20445;&#25252;&#29992;&#25143;&#25968;&#25454;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a zone-based federated learning method for training deep learning models with mobile sensing data. The method divides the physical space into geographical zones and maps them to a mobile-edge-cloud system architecture for good model accuracy and scalability. Each zone has a federated training model that adapts well to the data and behaviors of users in that zone, while protecting user data privacy.
&lt;/p&gt;
&lt;p&gt;
&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#65292;&#22914;mHealth&#21644;&#20581;&#24247;&#24212;&#29992;&#31243;&#24207;&#65292;&#21487;&#20197;&#20174;&#20351;&#29992;&#26234;&#33021;&#25163;&#26426;&#25110;&#21487;&#31359;&#25140;&#35774;&#22791;&#25910;&#38598;&#30340;&#31227;&#21160;&#24863;&#30693;&#25968;&#25454;&#35757;&#32451;&#30340;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#20013;&#21463;&#30410;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#27809;&#26377;&#31227;&#21160;&#24863;&#30693;DL&#31995;&#32479;&#33021;&#22815;&#21516;&#26102;&#23454;&#29616;&#33391;&#22909;&#30340;&#27169;&#22411;&#20934;&#30830;&#24615;&#65292;&#36866;&#24212;&#29992;&#25143;&#30340;&#31227;&#21160;&#34892;&#20026;&#65292;&#38543;&#30528;&#29992;&#25143;&#25968;&#37327;&#30340;&#22686;&#21152;&#32780;&#25193;&#23637;&#65292;&#24182;&#20445;&#25252;&#29992;&#25143;&#25968;&#25454;&#38544;&#31169;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#21306;&#22495;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;ZoneFL&#65289;&#26469;&#35299;&#20915;&#36825;&#20123;&#35201;&#27714;&#12290;ZoneFL&#23558;&#29289;&#29702;&#31354;&#38388;&#21010;&#20998;&#20026;&#22320;&#29702;&#21306;&#22495;&#65292;&#26144;&#23556;&#21040;&#31227;&#21160;&#36793;&#32536;&#20113;&#31995;&#32479;&#26550;&#26500;&#65292;&#20197;&#23454;&#29616;&#33391;&#22909;&#30340;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#27599;&#20010;&#21306;&#22495;&#37117;&#26377;&#19968;&#20010;&#32852;&#21512;&#35757;&#32451;&#27169;&#22411;&#65292;&#31216;&#20026;&#21306;&#22495;&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#24456;&#22909;&#22320;&#36866;&#24212;&#35813;&#21306;&#22495;&#29992;&#25143;&#30340;&#25968;&#25454;&#21644;&#34892;&#20026;&#12290;&#21463;&#30410;&#20110;FL&#35774;&#35745;&#65292;ZoneFL&#22521;&#35757;&#26399;&#38388;&#20445;&#25252;&#29992;&#25143;&#25968;&#25454;&#38544;&#31169;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#21306;&#22495;&#30340;&#32852;&#21512;&#35757;&#32451;&#31639;&#27861;&#26469;&#20248;&#21270;&#21306;&#22495;&#27169;&#22411;&#20197;&#36866;&#24212;&#29992;&#25143;&#30340;&#31227;&#21160;&#34892;&#20026;&#65306;&#21306;&#22495;&#21512;&#24182;&#21644;&#20998;&#35010;&#65288;ZMS&#65289;&#21644;Zo
&lt;/p&gt;
&lt;p&gt;
Mobile apps, such as mHealth and wellness applications, can benefit from deep learning (DL) models trained with mobile sensing data collected by smart phones or wearable devices. However, currently there is no mobile sensing DL system that simultaneously achieves good model accuracy while adapting to user mobility behavior, scales well as the number of users increases, and protects user data privacy. We propose Zone-based Federated Learning (ZoneFL) to address these requirements. ZoneFL divides the physical space into geographical zones mapped to a mobile-edge-cloud system architecture for good model accuracy and scalability. Each zone has a federated training model, called a zone model, which adapts well to data and behaviors of users in that zone. Benefiting from the FL design, the user data privacy is protected during the ZoneFL training. We propose two novel zone-based federated training algorithms to optimize zone models to user mobility behavior: Zone Merge and Split (ZMS) and Zo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36229;&#20284;&#26354;&#33258;&#36866;&#24212;&#27169;&#22411;&#65288;HYSP&#65289;&#29992;&#20110;&#23398;&#20064;&#22522;&#20110;&#39592;&#26550;&#30340;&#21160;&#20316;&#34920;&#31034;&#65292;&#37319;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#26469;&#29983;&#25104;&#21516;&#19968;&#26679;&#26412;&#30340;&#20004;&#20010;&#35270;&#22270;&#65292;&#24182;&#36890;&#36807;&#23558;&#19968;&#20010;&#35270;&#22270;&#19982;&#21478;&#19968;&#20010;&#35270;&#22270;&#21305;&#37197;&#26469;&#23398;&#20064;&#65292;&#20351;&#29992;&#36229;&#20284;&#26354;&#19981;&#30830;&#23450;&#24615;&#26469;&#30830;&#23450;&#31639;&#27861;&#23398;&#20064;&#36895;&#24230;&#65292;&#20551;&#35774;&#19981;&#30830;&#23450;&#24615;&#36739;&#23567;&#30340;&#26679;&#26412;&#24212;&#26356;&#24378;&#28872;&#22320;&#25512;&#21160;&#35757;&#32451;&#65292;&#20855;&#26377;&#26356;&#22823;&#30340;&#26435;&#37325;&#21644;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.06242</link><description>&lt;p&gt;
&#22522;&#20110;&#21452;&#35270;&#35282;&#30340;&#36229;&#20284;&#26354;&#33258;&#36866;&#24212;&#23398;&#20064;&#29992;&#20110;&#33258;&#30417;&#30563;&#39592;&#26550;&#21160;&#20316;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
HYperbolic Self-Paced Learning for Self-Supervised Skeleton-based Action Representations. (arXiv:2303.06242v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06242
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36229;&#20284;&#26354;&#33258;&#36866;&#24212;&#27169;&#22411;&#65288;HYSP&#65289;&#29992;&#20110;&#23398;&#20064;&#22522;&#20110;&#39592;&#26550;&#30340;&#21160;&#20316;&#34920;&#31034;&#65292;&#37319;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#26469;&#29983;&#25104;&#21516;&#19968;&#26679;&#26412;&#30340;&#20004;&#20010;&#35270;&#22270;&#65292;&#24182;&#36890;&#36807;&#23558;&#19968;&#20010;&#35270;&#22270;&#19982;&#21478;&#19968;&#20010;&#35270;&#22270;&#21305;&#37197;&#26469;&#23398;&#20064;&#65292;&#20351;&#29992;&#36229;&#20284;&#26354;&#19981;&#30830;&#23450;&#24615;&#26469;&#30830;&#23450;&#31639;&#27861;&#23398;&#20064;&#36895;&#24230;&#65292;&#20551;&#35774;&#19981;&#30830;&#23450;&#24615;&#36739;&#23567;&#30340;&#26679;&#26412;&#24212;&#26356;&#24378;&#28872;&#22320;&#25512;&#21160;&#35757;&#32451;&#65292;&#20855;&#26377;&#26356;&#22823;&#30340;&#26435;&#37325;&#21644;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel HYperbolic Self-Paced model (HYSP) for learning skeleton-based action representations, which adopts self-supervision and uses data augmentations to generate two views of the same sample, and learns by matching one to the other. It uses hyperbolic uncertainty to determine the algorithmic learning pace, assuming that less uncertain samples should be more strongly driving the training, with a larger weight and pace.
&lt;/p&gt;
&lt;p&gt;
&#33258;&#36866;&#24212;&#23398;&#20064;&#22312;&#19968;&#20123;&#20219;&#21153;&#20013;&#26377;&#30410;&#65292;&#20363;&#22914;&#24369;&#30417;&#30563;&#23398;&#20064;&#21644;&#39046;&#22495;&#33258;&#36866;&#24212;&#65292;&#21487;&#20197;&#36873;&#25321;&#21644;&#25490;&#24207;&#35757;&#32451;&#26679;&#26412;&#24207;&#21015;&#65292;&#20174;&#26131;&#21040;&#38590;&#12290;&#28982;&#32780;&#65292;&#23427;&#22312;&#26080;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#36866;&#29992;&#24615;&#20173;&#26410;&#34987;&#25506;&#32034;&#65292;&#20854;&#20013;&#20219;&#21153;&#30340;&#30693;&#35782;&#22312;&#35757;&#32451;&#26399;&#38388;&#25104;&#29087;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36229;&#20284;&#26354;&#33258;&#36866;&#24212;&#27169;&#22411;&#65288;HYSP&#65289;&#29992;&#20110;&#23398;&#20064;&#22522;&#20110;&#39592;&#26550;&#30340;&#21160;&#20316;&#34920;&#31034;&#12290;HYSP&#37319;&#29992;&#33258;&#30417;&#30563;&#65306;&#23427;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#26469;&#29983;&#25104;&#21516;&#19968;&#26679;&#26412;&#30340;&#20004;&#20010;&#35270;&#22270;&#65292;&#24182;&#36890;&#36807;&#23558;&#19968;&#20010;&#35270;&#22270;&#65288;&#31216;&#20026;&#22312;&#32447;&#65289;&#19982;&#21478;&#19968;&#20010;&#35270;&#22270;&#65288;&#30446;&#26631;&#65289;&#21305;&#37197;&#26469;&#23398;&#20064;&#12290;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#36229;&#20284;&#26354;&#19981;&#30830;&#23450;&#24615;&#26469;&#30830;&#23450;&#31639;&#27861;&#23398;&#20064;&#36895;&#24230;&#65292;&#20551;&#35774;&#19981;&#30830;&#23450;&#24615;&#36739;&#23567;&#30340;&#26679;&#26412;&#24212;&#26356;&#24378;&#28872;&#22320;&#25512;&#21160;&#35757;&#32451;&#65292;&#20855;&#26377;&#26356;&#22823;&#30340;&#26435;&#37325;&#21644;&#36895;&#24230;&#12290;&#36229;&#20284;&#26354;&#19981;&#30830;&#23450;&#24615;&#26159;&#37319;&#29992;&#30340;&#36229;&#20284;&#26354;&#31070;&#32463;&#32593;&#32476;&#30340;&#21103;&#20135;&#21697;&#65292;&#23427;&#22312;&#35757;&#32451;&#26399;&#38388;&#25104;&#29087;&#65292;&#19982;&#39069;&#22806;&#25104;&#26412;&#30456;&#27604;&#65292;&#27809;&#26377;&#39069;&#22806;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-paced learning has been beneficial for tasks where some initial knowledge is available, such as weakly supervised learning and domain adaptation, to select and order the training sample sequence, from easy to complex. However its applicability remains unexplored in unsupervised learning, whereby the knowledge of the task matures during training. We propose a novel HYperbolic Self-Paced model (HYSP) for learning skeleton-based action representations. HYSP adopts self-supervision: it uses data augmentations to generate two views of the same sample, and it learns by matching one (named online) to the other (the target). We propose to use hyperbolic uncertainty to determine the algorithmic learning pace, under the assumption that less uncertain samples should be more strongly driving the training, with a larger weight and pace. Hyperbolic uncertainty is a by-product of the adopted hyperbolic neural networks, it matures during training and it comes with no extra cost, compared to the e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#35757;&#32451;&#25968;&#25454;&#38598;&#36827;&#34892;&#31579;&#36873;&#65292;&#20165;&#20351;&#29992;&#26131;&#21463;&#23545;&#25239;&#25915;&#20987;&#30340;&#26679;&#26412;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2303.06241</link><description>&lt;p&gt;
&#23545;&#25239;&#35757;&#32451;&#26159;&#21542;&#38656;&#35201;&#20351;&#29992;&#25972;&#20010;&#35757;&#32451;&#25968;&#25454;&#38598;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do we need entire training data for adversarial training?. (arXiv:2303.06241v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06241
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#35757;&#32451;&#25968;&#25454;&#38598;&#36827;&#34892;&#31579;&#36873;&#65292;&#20165;&#20351;&#29992;&#26131;&#21463;&#23545;&#25239;&#25915;&#20987;&#30340;&#26679;&#26412;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a new adversarial training method that reduces training time by selecting only the adversarially-prone samples from the training dataset.
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#34987;&#29992;&#20110;&#35299;&#20915;&#35768;&#22810;&#39046;&#22495;&#30340;&#38382;&#39064;&#65292;&#21253;&#25324;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#21644;&#21307;&#23398;&#22270;&#20687;&#31561;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#12290;DNN&#23545;&#25239;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#24050;&#32463;&#34987;&#24191;&#27867;&#20851;&#27880;&#12290;&#36817;&#24180;&#26469;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#26041;&#27861;&#26469;&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#20960;&#20046;&#25152;&#26377;&#30340;&#26041;&#27861;&#37117;&#20250;&#20026;&#25972;&#20010;&#35757;&#32451;&#25968;&#25454;&#38598;&#29983;&#25104;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#20174;&#32780;&#22823;&#22823;&#22686;&#21152;&#20102;&#35757;&#32451;&#26102;&#38388;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#20165;&#20351;&#29992;&#35757;&#32451;&#25968;&#25454;&#30340;&#23376;&#38598;&#36827;&#34892;&#23545;&#25239;&#35757;&#32451;&#65292;&#21487;&#20197;&#20943;&#23569;&#20219;&#20309;&#23545;&#25239;&#35757;&#32451;&#31639;&#27861;&#30340;&#35757;&#32451;&#26102;&#38388;&#12290;&#20026;&#20102;&#36873;&#25321;&#23376;&#38598;&#65292;&#25105;&#20204;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#36807;&#28388;&#20986;&#26131;&#21463;&#23545;&#25239;&#25915;&#20987;&#30340;&#26679;&#26412;&#12290;&#25105;&#20204;&#23545;&#25152;&#26377;&#35757;&#32451;&#26679;&#26412;&#25191;&#34892;&#31616;&#21333;&#30340;&#23545;&#25239;&#25915;&#20987;&#65292;&#20197;&#36807;&#28388;&#20986;&#36825;&#20010;&#23376;&#38598;&#12290;&#22312;&#36825;&#20010;&#25915;&#20987;&#20013;&#65292;&#25105;&#20204;&#21521;&#27599;&#20010;&#20687;&#32032;&#28155;&#21152;&#19968;&#20010;&#23567;&#25200;&#21160;&#21644;&#20960;&#26465;&#32593;&#26684;&#32447;&#21040;&#36755;&#20837;&#22270;&#20687;&#20013;&#12290;&#25105;&#20204;&#23545;&#26131;&#21463;&#23545;&#25239;&#25915;&#20987;&#30340;&#23376;&#38598;&#36827;&#34892;&#23545;&#25239;&#35757;&#32451;&#65292;&#24182;&#19988;...
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks (DNNs) are being used to solve a wide range of problems in many domains including safety-critical domains like self-driving cars and medical imagery. DNNs suffer from vulnerability against adversarial attacks. In the past few years, numerous approaches have been proposed to tackle this problem by training networks using adversarial training. Almost all the approaches generate adversarial examples for the entire training dataset, thus increasing the training time drastically. We show that we can decrease the training time for any adversarial training algorithm by using only a subset of training data for adversarial training. To select the subset, we filter the adversarially-prone samples from the training data. We perform a simple adversarial attack on all training examples to filter this subset. In this attack, we add a small perturbation to each pixel and a few grid lines to the input image.  We perform adversarial training on the adversarially-prone subset and mi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#34917;&#20805;&#31232;&#30095;&#21270;&#30340;&#27169;&#22411;&#21098;&#26525;&#26426;&#21046;&#65292;&#36890;&#36807;&#22312;&#26381;&#21153;&#22120;&#21644;&#23458;&#25143;&#31471;&#20043;&#38388;&#36827;&#34892;&#20114;&#34917;&#21644;&#21327;&#20316;&#30340;&#21098;&#26525;&#26469;&#28385;&#36275;&#32852;&#37030;&#23398;&#20064;&#20013;&#20302;&#21452;&#21521;&#36890;&#20449;&#24320;&#38144;&#12289;&#23458;&#25143;&#31471;&#20302;&#35745;&#31639;&#24320;&#38144;&#21644;&#33391;&#22909;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2303.06237</link><description>&lt;p&gt;
&#20302;&#24320;&#38144;&#27169;&#22411;&#21098;&#26525;&#65306;&#38754;&#21521;&#32852;&#37030;&#23398;&#20064;&#30340;&#34917;&#20805;&#31232;&#30095;&#21270;
&lt;/p&gt;
&lt;p&gt;
Complement Sparsification: Low-Overhead Model Pruning for Federated Learning. (arXiv:2303.06237v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06237
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#34917;&#20805;&#31232;&#30095;&#21270;&#30340;&#27169;&#22411;&#21098;&#26525;&#26426;&#21046;&#65292;&#36890;&#36807;&#22312;&#26381;&#21153;&#22120;&#21644;&#23458;&#25143;&#31471;&#20043;&#38388;&#36827;&#34892;&#20114;&#34917;&#21644;&#21327;&#20316;&#30340;&#21098;&#26525;&#26469;&#28385;&#36275;&#32852;&#37030;&#23398;&#20064;&#20013;&#20302;&#21452;&#21521;&#36890;&#20449;&#24320;&#38144;&#12289;&#23458;&#25143;&#31471;&#20302;&#35745;&#31639;&#24320;&#38144;&#21644;&#33391;&#22909;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a model pruning mechanism called Complement Sparsification (CS), which satisfies the requirements of low bidirectional communication overhead between the server and the clients, low computation overhead at the clients, and good model accuracy in federated learning through complementary and collaborative pruning done at the server and the clients.
&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#30340;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#33539;&#20363;&#65292;&#28041;&#21450;&#22823;&#37327;&#36890;&#20449;&#21644;&#35745;&#31639;&#24037;&#20316;&#65292;&#36825;&#23545;&#20110;&#36164;&#28304;&#21463;&#38480;&#30340;&#31227;&#21160;&#21644;&#29289;&#32852;&#32593;&#35774;&#22791;&#26159;&#19968;&#20010;&#38382;&#39064;&#12290;&#27169;&#22411;&#21098;&#26525;/&#31232;&#30095;&#21270;&#24320;&#21457;&#20102;&#21487;&#20197;&#35299;&#20915;&#27492;&#38382;&#39064;&#30340;&#31232;&#30095;&#27169;&#22411;&#65292;&#20294;&#29616;&#26377;&#30340;&#31232;&#30095;&#21270;&#35299;&#20915;&#26041;&#26696;&#19981;&#33021;&#21516;&#26102;&#28385;&#36275;&#26381;&#21153;&#22120;&#21644;&#23458;&#25143;&#31471;&#20043;&#38388;&#20302;&#21452;&#21521;&#36890;&#20449;&#24320;&#38144;&#12289;&#23458;&#25143;&#31471;&#20302;&#35745;&#31639;&#24320;&#38144;&#21644;&#33391;&#22909;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#35201;&#27714;&#65292;&#22312;FL&#20551;&#35774;&#19979;&#65292;&#26381;&#21153;&#22120;&#26080;&#27861;&#35775;&#38382;&#21407;&#22987;&#25968;&#25454;&#20197;&#24494;&#35843;&#20462;&#21098;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#34917;&#20805;&#31232;&#30095;&#21270;&#65288;CS&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21098;&#26525;&#26426;&#21046;&#65292;&#36890;&#36807;&#22312;&#26381;&#21153;&#22120;&#21644;&#23458;&#25143;&#31471;&#20043;&#38388;&#36827;&#34892;&#20114;&#34917;&#21644;&#21327;&#20316;&#30340;&#21098;&#26525;&#26469;&#28385;&#36275;&#25152;&#26377;&#36825;&#20123;&#35201;&#27714;&#12290;&#22312;&#27599;&#19968;&#36718;&#20013;&#65292;CS&#21019;&#24314;&#19968;&#20010;&#20840;&#23616;&#31232;&#30095;&#27169;&#22411;&#65292;&#20854;&#20013;&#21253;&#21547;&#25429;&#33719;&#25152;&#26377;&#23458;&#25143;&#31471;&#30340;&#19968;&#33324;&#25968;&#25454;&#20998;&#24067;&#30340;&#26435;&#37325;&#65292;&#32780;&#23458;&#25143;&#31471;&#21017;&#21019;&#24314;&#26412;&#22320;&#31232;&#30095;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a privacy-preserving distributed deep learning paradigm that involves substantial communication and computation effort, which is a problem for resource-constrained mobile and IoT devices. Model pruning/sparsification develops sparse models that could solve this problem, but existing sparsification solutions cannot satisfy at the same time the requirements for low bidirectional communication overhead between the server and the clients, low computation overhead at the clients, and good model accuracy, under the FL assumption that the server does not have access to raw data to fine-tune the pruned models. We propose Complement Sparsification (CS), a pruning mechanism that satisfies all these requirements through a complementary and collaborative pruning done at the server and the clients. At each round, CS creates a global sparse model that contains the weights that capture the general data distribution of all clients, while the clients create local sparse model
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#35780;&#20272;&#65292;&#23558;&#30456;&#23545;&#25104;&#29087;&#30340;&#21487;&#35299;&#37322;AI&#39046;&#22495;&#19982;&#24555;&#36895;&#21457;&#23637;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#36827;&#34892;&#20102;&#31867;&#27604;&#65292;&#35748;&#20026;&#20154;&#31867;&#30340;&#38656;&#27714;&#24212;&#35813;&#25104;&#20026;LLMs&#35780;&#20272;&#30340;&#26680;&#24515;&#12290;</title><link>http://arxiv.org/abs/2303.06223</link><description>&lt;p&gt;
&#35841;&#22312;&#24605;&#32771;&#65311;&#20351;&#29992;XAI Playbook&#25512;&#21160;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;LLMs&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Who's Thinking? A Push for Human-Centered Evaluation of LLMs using the XAI Playbook. (arXiv:2303.06223v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06223
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#35780;&#20272;&#65292;&#23558;&#30456;&#23545;&#25104;&#29087;&#30340;&#21487;&#35299;&#37322;AI&#39046;&#22495;&#19982;&#24555;&#36895;&#21457;&#23637;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#36827;&#34892;&#20102;&#31867;&#27604;&#65292;&#35748;&#20026;&#20154;&#31867;&#30340;&#38656;&#27714;&#24212;&#35813;&#25104;&#20026;LLMs&#35780;&#20272;&#30340;&#26680;&#24515;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores human-centered evaluation of AI-based systems, drawing parallels between the relatively mature field of explainable AI and the rapidly evolving research boom around large language models. The authors argue that humans' needs should be held front and center in evaluating LLMs.
&lt;/p&gt;
&lt;p&gt;
&#37096;&#32626;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#32463;&#24120;&#24433;&#21709;&#20154;&#31867;&#65292;&#32780;&#35780;&#20272;&#36825;&#20123;&#24037;&#20855;&#27809;&#26377;&#19968;&#31181;&#36866;&#21512;&#25152;&#26377;&#24773;&#20917;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;AI&#31995;&#32479;&#35780;&#20272;&#32467;&#21512;&#20102;&#23450;&#37327;&#21644;&#23450;&#24615;&#20998;&#26512;&#20197;&#21450;&#20154;&#31867;&#36755;&#20837;&#12290;&#23427;&#24050;&#32463;&#22312;&#21487;&#35299;&#37322;&#30340;AI&#65288;XAI&#65289;&#21644;&#20154;&#26426;&#20132;&#20114;&#65288;HCI&#65289;&#31038;&#21306;&#20013;&#24471;&#21040;&#20102;&#28145;&#20837;&#25506;&#35752;&#12290;&#20173;&#28982;&#23384;&#22312;&#24046;&#36317;&#65292;&#20294;&#31038;&#21306;&#24050;&#32463;&#25509;&#21463;&#20102;&#20154;&#31867;&#19982;AI&#21450;&#20854;&#38468;&#24102;&#30340;&#35299;&#37322;&#36827;&#34892;&#20132;&#20114;&#65292;&#20197;&#21450;&#24212;&#35813;&#23558;&#20154;&#31867;&#30340;&#38656;&#27714;&#65288;&#21253;&#25324;&#20182;&#20204;&#30340;&#35748;&#30693;&#20559;&#35265;&#21644;&#24618;&#30294;&#65289;&#25918;&#22312;&#39318;&#20301;&#30340;&#22522;&#26412;&#29702;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#30456;&#23545;&#25104;&#29087;&#30340;XAI&#39046;&#22495;&#19982;&#24555;&#36895;&#21457;&#23637;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30740;&#31350;&#28909;&#28526;&#20043;&#38388;&#36827;&#34892;&#20102;&#31867;&#27604;&#12290;&#25509;&#21463;&#30340;LLMs&#35780;&#20272;&#25351;&#26631;&#19981;&#26159;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#22312;&#35752;&#35770;LLMs&#26102;&#65292;XAI&#31038;&#21306;&#22312;&#36807;&#21435;&#21313;&#24180;&#20013;&#36208;&#36807;&#30340;&#35768;&#22810;&#30456;&#21516;&#36335;&#24452;&#23558;&#34987;&#37325;&#26032;&#36367;&#19978;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35748;&#20026;&#20154;&#31867;&#30340;&#20542;&#21521; - &#20877;&#27425;&#23436;&#20840;&#21253;&#25324;&#20182;&#20204;&#30340;&#35748;&#30693;&#20559;&#35265;&#21644;&#24618;&#30294; - &#24212;&#35813;&#25104;&#20026;LLMs&#35780;&#20272;&#30340;&#26680;&#24515;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deployed artificial intelligence (AI) often impacts humans, and there is no one-size-fits-all metric to evaluate these tools. Human-centered evaluation of AI-based systems combines quantitative and qualitative analysis and human input. It has been explored to some depth in the explainable AI (XAI) and human-computer interaction (HCI) communities. Gaps remain, but the basic understanding that humans interact with AI and accompanying explanations, and that humans' needs -- complete with their cognitive biases and quirks -- should be held front and center, is accepted by the community. In this paper, we draw parallels between the relatively mature field of XAI and the rapidly evolving research boom around large language models (LLMs). Accepted evaluative metrics for LLMs are not human-centered. We argue that many of the same paths tread by the XAI community over the past decade will be retread when discussing LLMs. Specifically, we argue that humans' tendencies -- again, complete with the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20960;&#20010;AI&#31995;&#32479;&#30456;&#23545;&#20110;&#20154;&#31867;&#23436;&#25104;&#30456;&#21516;&#20219;&#21153;&#30340;&#25490;&#25918;&#37327;&#65292;&#21457;&#29616;AI&#20889;&#20316;&#21644;&#25554;&#22270;&#30340;&#30899;&#25490;&#25918;&#27604;&#20154;&#31867;&#20302;130&#21040;1500&#20493;&#21644;310&#21040;2900&#20493;&#65292;&#30446;&#21069;&#20351;&#29992;AI&#26377;&#21487;&#33021;&#20197;&#27604;&#20154;&#31867;&#26356;&#20302;&#30340;&#25490;&#25918;&#27700;&#24179;&#23436;&#25104;&#20960;&#39033;&#37325;&#35201;&#27963;&#21160;&#12290;</title><link>http://arxiv.org/abs/2303.06219</link><description>&lt;p&gt;
AI&#20889;&#20316;&#21644;&#25554;&#22270;&#30340;&#30899;&#25490;&#25918;&#27604;&#20154;&#31867;&#20302;
&lt;/p&gt;
&lt;p&gt;
The Carbon Emissions of Writing and Illustrating Are Lower for AI than for Humans. (arXiv:2303.06219v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06219
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20960;&#20010;AI&#31995;&#32479;&#30456;&#23545;&#20110;&#20154;&#31867;&#23436;&#25104;&#30456;&#21516;&#20219;&#21153;&#30340;&#25490;&#25918;&#37327;&#65292;&#21457;&#29616;AI&#20889;&#20316;&#21644;&#25554;&#22270;&#30340;&#30899;&#25490;&#25918;&#27604;&#20154;&#31867;&#20302;130&#21040;1500&#20493;&#21644;310&#21040;2900&#20493;&#65292;&#30446;&#21069;&#20351;&#29992;AI&#26377;&#21487;&#33021;&#20197;&#27604;&#20154;&#31867;&#26356;&#20302;&#30340;&#25490;&#25918;&#27700;&#24179;&#23436;&#25104;&#20960;&#39033;&#37325;&#35201;&#27963;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper analyzes the emissions of several AI systems relative to those of humans completing the same tasks, finding that AI writing and illustrating emit 130 to 1500 times less CO2e and 310 to 2900 times less, respectively. The use of AI holds the potential to carry out several major activities at much lower emission levels than can humans.
&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;AI&#31995;&#32479;&#30340;&#26222;&#21450;&#65292;&#23427;&#20204;&#30340;&#28201;&#23460;&#27668;&#20307;&#25490;&#25918;&#23545;&#20154;&#31867;&#31038;&#20250;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#20960;&#20010;AI&#31995;&#32479;&#65288;ChatGPT&#12289;BLOOM&#12289;DALL-E2&#12289;Midjourney&#65289;&#30456;&#23545;&#20110;&#20154;&#31867;&#23436;&#25104;&#30456;&#21516;&#20219;&#21153;&#30340;&#25490;&#25918;&#37327;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#19968;&#20010;AI&#20889;&#19968;&#39029;&#25991;&#26412;&#25490;&#25918;&#30340;&#20108;&#27687;&#21270;&#30899;&#24403;&#37327;&#27604;&#19968;&#20010;&#20154;&#23569;130&#21040;1500&#20493;&#12290;&#21516;&#26679;&#65292;&#19968;&#20010;AI&#21019;&#36896;&#19968;&#24352;&#22270;&#29255;&#25490;&#25918;&#30340;&#20108;&#27687;&#21270;&#30899;&#24403;&#37327;&#27604;&#20154;&#31867;&#23569;310&#21040;2900&#20493;&#12290;&#25490;&#25918;&#20998;&#26512;&#27809;&#26377;&#32771;&#34385;&#21040;&#31038;&#20250;&#24433;&#21709;&#65292;&#22914;&#32844;&#19994;&#26367;&#20195;&#12289;&#21512;&#27861;&#24615;&#21644;&#21453;&#24377;&#25928;&#24212;&#12290;&#27492;&#22806;&#65292;AI&#24182;&#19981;&#33021;&#26367;&#20195;&#25152;&#26377;&#30340;&#20154;&#31867;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20351;&#29992;AI&#26377;&#21487;&#33021;&#20197;&#27604;&#20154;&#31867;&#26356;&#20302;&#30340;&#25490;&#25918;&#27700;&#24179;&#23436;&#25104;&#20960;&#39033;&#37325;&#35201;&#27963;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
As AI systems proliferate, their greenhouse gas emissions are an increasingly important concern for human societies. We analyze the emissions of several AI systems (ChatGPT, BLOOM, DALL-E2, Midjourney) relative to those of humans completing the same tasks. We find that an AI writing a page of text emits 130 to 1500 times less CO2e than a human doing so. Similarly, an AI creating an image emits 310 to 2900 times less. Emissions analysis do not account for social impacts such as professional displacement, legality, and rebound effects. In addition, AI is not a substitute for all human tasks. Nevertheless, at present, the use of AI holds the potential to carry out several major activities at much lower emission levels than can humans.
&lt;/p&gt;</description></item><item><title>CHGNN&#26159;&#19968;&#31181;&#21322;&#30417;&#30563;&#23545;&#27604;&#36229;&#22270;&#23398;&#20064;&#32593;&#32476;&#65292;&#21033;&#29992;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#20174;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#65292;&#21253;&#25324;&#33258;&#36866;&#24212;&#36229;&#22270;&#35270;&#22270;&#29983;&#25104;&#22120;&#12289;&#25913;&#36827;&#30340;&#36229;&#22270;&#32534;&#30721;&#22120;&#21644;&#32852;&#21512;&#25439;&#22833;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2303.06213</link><description>&lt;p&gt;
CHGNN: &#19968;&#31181;&#21322;&#30417;&#30563;&#23545;&#27604;&#36229;&#22270;&#23398;&#20064;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
CHGNN: A Semi-Supervised Contrastive Hypergraph Learning Network. (arXiv:2303.06213v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06213
&lt;/p&gt;
&lt;p&gt;
CHGNN&#26159;&#19968;&#31181;&#21322;&#30417;&#30563;&#23545;&#27604;&#36229;&#22270;&#23398;&#20064;&#32593;&#32476;&#65292;&#21033;&#29992;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#20174;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#65292;&#21253;&#25324;&#33258;&#36866;&#24212;&#36229;&#22270;&#35270;&#22270;&#29983;&#25104;&#22120;&#12289;&#25913;&#36827;&#30340;&#36229;&#22270;&#32534;&#30721;&#22120;&#21644;&#32852;&#21512;&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
CHGNN is a semi-supervised contrastive hypergraph learning network that exploits self-supervised contrastive learning techniques to learn from labeled and unlabeled data. It includes an adaptive hypergraph view generator, an improved hypergraph encoder, and a joint loss function.
&lt;/p&gt;
&lt;p&gt;
&#36229;&#22270;&#21487;&#20197;&#27169;&#25311;&#24212;&#29992;&#31243;&#24207;&#20013;&#21457;&#29616;&#30340;&#25968;&#25454;&#23545;&#35937;&#20043;&#38388;&#30340;&#39640;&#38454;&#20851;&#31995;&#65292;&#20363;&#22914;&#31038;&#20132;&#32593;&#32476;&#21644;&#29983;&#29289;&#20449;&#24687;&#23398;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#36229;&#22270;&#23398;&#20064;&#30740;&#31350;&#23558;&#22270;&#21367;&#31215;&#32593;&#32476;&#25193;&#23637;&#21040;&#36229;&#22270;&#65292;&#20294;&#26080;&#27861;&#26377;&#25928;&#22320;&#20174;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#29305;&#24449;&#20013;&#23398;&#20064;&#12290;&#20026;&#20102;&#36827;&#34892;&#36825;&#26679;&#30340;&#23398;&#20064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;CHGNN&#65292;&#23427;&#21033;&#29992;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#20174;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#12290;&#39318;&#20808;&#65292;CHGNN&#21253;&#25324;&#19968;&#20010;&#33258;&#36866;&#24212;&#36229;&#22270;&#35270;&#22270;&#29983;&#25104;&#22120;&#65292;&#37319;&#29992;&#33258;&#21160;&#22686;&#24378;&#31574;&#30053;&#65292;&#24182;&#23398;&#20064;&#26368;&#23567;&#20805;&#20998;&#35270;&#22270;&#30340;&#25200;&#21160;&#27010;&#29575;&#20998;&#24067;&#12290;&#20854;&#27425;&#65292;CHGNN&#21253;&#21547;&#19968;&#20010;&#25913;&#36827;&#30340;&#36229;&#22270;&#32534;&#30721;&#22120;&#65292;&#32771;&#34385;&#21040;&#36229;&#36793;&#30340;&#21516;&#36136;&#24615;&#65292;&#20197;&#26377;&#25928;&#22320;&#34701;&#21512;&#20449;&#24687;&#12290;&#31532;&#19977;&#65292;CHGNN&#37197;&#22791;&#20102;&#19968;&#20010;&#32852;&#21512;&#25439;&#22833;&#20989;&#25968;&#65292;&#32467;&#21512;&#20102;&#35270;&#22270;&#29983;&#25104;&#22120;&#30340;&#30456;&#20284;&#24615;&#25439;&#22833;&#12289;&#33410;&#28857;&#20998;&#31867;&#25439;&#22833;&#21644;&#36229;&#36793;&#21516;&#36136;&#24615;&#25439;&#22833;&#65292;&#27880;&#20837;&#30417;&#30563;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hypergraphs can model higher-order relationships among data objects that are found in applications such as social networks and bioinformatics. However, recent studies on hypergraph learning that extend graph convolutional networks to hypergraphs cannot learn effectively from features of unlabeled data. To such learning, we propose a contrastive hypergraph neural network, CHGNN, that exploits self-supervised contrastive learning techniques to learn from labeled and unlabeled data. First, CHGNN includes an adaptive hypergraph view generator that adopts an auto-augmentation strategy and learns a perturbed probability distribution of minimal sufficient views. Second, CHGNN encompasses an improved hypergraph encoder that considers hyperedge homogeneity to fuse information effectively. Third, CHGNN is equipped with a joint loss function that combines a similarity loss for the view generator, a node classification loss, and a hyperedge homogeneity loss to inject supervision signals. It also i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20855;&#26377;&#20108;&#20803;&#36229;&#27169;&#25104;&#26412;&#20989;&#25968;&#30340;&#20195;&#29702;&#20043;&#38388;&#20998;&#37197;&#19981;&#21487;&#20998;&#21106;&#23478;&#21153;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#20844;&#24179;&#20998;&#37197;&#26694;&#26550;&#65292;&#36866;&#29992;&#20110;&#36825;&#31867;&#20272;&#20540;&#20989;&#25968;&#12290;&#35813;&#26694;&#26550;&#21487;&#20197;&#26377;&#25928;&#22320;&#35745;&#31639;&#28385;&#36275;&#21152;&#26435;&#20844;&#24179;&#24615;&#27010;&#24565;&#30340;&#20998;&#37197;&#12290;</title><link>http://arxiv.org/abs/2303.06212</link><description>&lt;p&gt;
&#24102;&#26377;&#20108;&#20803;&#36229;&#27169;&#30697;&#38453;&#30340;&#20844;&#24179;&#24615;&#21152;&#26435;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
Weighted Notions of Fairness with Binary Supermodular Chores. (arXiv:2303.06212v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06212
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20855;&#26377;&#20108;&#20803;&#36229;&#27169;&#25104;&#26412;&#20989;&#25968;&#30340;&#20195;&#29702;&#20043;&#38388;&#20998;&#37197;&#19981;&#21487;&#20998;&#21106;&#23478;&#21153;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#20844;&#24179;&#20998;&#37197;&#26694;&#26550;&#65292;&#36866;&#29992;&#20110;&#36825;&#31867;&#20272;&#20540;&#20989;&#25968;&#12290;&#35813;&#26694;&#26550;&#21487;&#20197;&#26377;&#25928;&#22320;&#35745;&#31639;&#28385;&#36275;&#21152;&#26435;&#20844;&#24179;&#24615;&#27010;&#24565;&#30340;&#20998;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the problem of allocating indivisible chores among agents with binary supermodular cost functions and presents a general framework for fair allocation with this class of valuation functions. The framework allows for efficient computation of allocations that satisfy weighted notions of fairness.
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#20855;&#26377;&#20108;&#20803;&#36229;&#27169;&#25104;&#26412;&#20989;&#25968;&#30340;&#20195;&#29702;&#20043;&#38388;&#20998;&#37197;&#19981;&#21487;&#20998;&#21106;&#23478;&#21153;&#30340;&#38382;&#39064;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#27599;&#20010;&#23478;&#21153;&#30340;&#36793;&#38469;&#25104;&#26412;&#20026;$0$&#25110;$1$&#65292;&#24182;&#19988;&#23478;&#21153;&#21576;&#29616;&#20986;&#36882;&#22686;&#30340;&#36793;&#38469;&#25104;&#26412;&#65288;&#25110;&#36882;&#20943;&#30340;&#36793;&#38469;&#25928;&#29992;&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32467;&#21512;&#20102;Viswanathan&#21644;Zick&#65288;2022&#65289;&#20197;&#21450;Barman&#31561;&#20154;&#65288;2023&#65289;&#30340;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#20844;&#24179;&#20998;&#37197;&#26694;&#26550;&#65292;&#36866;&#29992;&#20110;&#36825;&#31867;&#20272;&#20540;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20801;&#35768;&#25105;&#20204;&#25512;&#24191;Barman&#31561;&#20154;&#65288;2023&#65289;&#30340;&#32467;&#26524;&#65292;&#24182;&#26377;&#25928;&#22320;&#35745;&#31639;&#28385;&#36275;&#21152;&#26435;&#20844;&#24179;&#24615;&#27010;&#24565;&#65288;&#22914;&#21152;&#26435;leximin&#25110;min&#21152;&#26435;$p$-mean malfare&#65292;&#20854;&#20013;$p \ge 1$&#65289;&#30340;&#20998;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of allocating indivisible chores among agents with binary supermodular cost functions. In other words, each chore has a marginal cost of $0$ or $1$ and chores exhibit increasing marginal costs (or decreasing marginal utilities). In this note, we combine the techniques of Viswanathan and Zick (2022) and Barman et al. (2023) to present a general framework for fair allocation with this class of valuation functions. Our framework allows us to generalize the results of Barman et al. (2023) and efficiently compute allocations which satisfy weighted notions of fairness like weighted leximin or min weighted $p$-mean malfare for any $p \ge 1$.
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;POV&#30340;&#39640;&#36895;&#20844;&#36335;&#36710;&#36742;&#36712;&#36857;&#25968;&#25454;&#38598;&#21644;&#39044;&#27979;&#26550;&#26500;&#65292;&#20854;&#20013;&#30340;Carolinas Highway Dataset&#65288;CHD&#65289;&#21253;&#25324;160&#19975;&#24103;&#21644;338,000&#20010;&#36710;&#36742;&#36712;&#36857;&#12290;</title><link>http://arxiv.org/abs/2303.06202</link><description>&lt;p&gt;
&#22522;&#20110;POV&#30340;&#39640;&#36895;&#20844;&#36335;&#36710;&#36742;&#36712;&#36857;&#25968;&#25454;&#38598;&#21644;&#39044;&#27979;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
A POV-based Highway Vehicle Trajectory Dataset and Prediction Architecture. (arXiv:2303.06202v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06202
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;POV&#30340;&#39640;&#36895;&#20844;&#36335;&#36710;&#36742;&#36712;&#36857;&#25968;&#25454;&#38598;&#21644;&#39044;&#27979;&#26550;&#26500;&#65292;&#20854;&#20013;&#30340;Carolinas Highway Dataset&#65288;CHD&#65289;&#21253;&#25324;160&#19975;&#24103;&#21644;338,000&#20010;&#36710;&#36742;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Introducing a POV-based highway vehicle trajectory dataset and prediction architecture, including Carolinas Highway Dataset (CHD) with 1.6 million frames and 338,000 vehicle trajectories captured at eight locations in Carolinas.
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20379;&#22810;&#20010;&#35270;&#35282;&#65288;POV&#65289;&#30340;&#36710;&#36742;&#36712;&#36857;&#25968;&#25454;&#38598;&#23545;&#20110;&#21508;&#31181;&#20132;&#36890;&#23433;&#20840;&#21644;&#31649;&#29702;&#24212;&#29992;&#38750;&#24120;&#26377;&#20215;&#20540;&#12290;&#23613;&#31649;&#36712;&#36857;&#25968;&#25454;&#38598;&#24456;&#20016;&#23500;&#65292;&#20294;&#24456;&#23569;&#25552;&#20379;&#20840;&#38754;&#21644;&#22810;&#26679;&#21270;&#30340;&#39550;&#39542;&#22330;&#26223;&#65292;&#25429;&#25417;&#21508;&#31181;&#39640;&#36895;&#20844;&#36335;&#24067;&#23616;&#12289;&#21512;&#24182;&#36710;&#36947;&#21644;&#37197;&#32622;&#30340;&#22810;&#20010;&#35270;&#28857;&#12290;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#25429;&#25417;&#39550;&#39542;&#21592;&#12289;&#36710;&#36742;&#21644;&#36947;&#36335;&#22522;&#30784;&#35774;&#26045;&#20043;&#38388;&#24494;&#22937;&#20114;&#21160;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;Carolinas Highway Dataset&#65288;CHD&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#36710;&#36742;&#36712;&#36857;&#12289;&#26816;&#27979;&#21644;&#36319;&#36394;&#25968;&#25454;&#38598;&#12290;CHD&#26159;&#22312;Carolinas&#30340;&#20843;&#20010;&#20301;&#32622;&#25293;&#25668;&#30340;&#39640;&#36895;&#20844;&#36335;&#35270;&#39057;&#20013;&#25429;&#33719;&#30340;160&#19975;&#24103;&#65292;&#21253;&#25324;338,000&#20010;&#36710;&#36742;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vehicle Trajectory datasets that provide multiple point-of-views (POVs) can be valuable for various traffic safety and management applications. Despite the abundance of trajectory datasets, few offer a comprehensive and diverse range of driving scenes, capturing multiple viewpoints of various highway layouts, merging lanes, and configurations. This limits their ability to capture the nuanced interactions between drivers, vehicles, and the roadway infrastructure. We introduce the \emph{Carolinas Highway Dataset (CHD\footnote{\emph{CHD} available at: \url{https://github.com/TeCSAR-UNCC/Carolinas\_Dataset}})}, a vehicle trajectory, detection, and tracking dataset. \emph{CHD} is a collection of 1.6 million frames captured in highway-based videos from eye-level and high-angle POVs at eight locations across Carolinas with 338,000 vehicle trajectories. The locations, timing of recordings, and camera angles were carefully selected to capture various road geometries, traffic patterns, lighting 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;FedFBN&#65292;&#19968;&#20010;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#32593;&#32476;&#20316;&#20026;&#27169;&#22411;&#21518;&#31471;&#65292;&#24182;&#22312;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#20013;&#20923;&#32467;&#25209;&#37327;&#24402;&#19968;&#21270;&#23618;&#65292;&#20197;&#20248;&#21270;&#20998;&#24067;&#24335;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#21644;&#37096;&#20998;&#26631;&#31614;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2303.06180</link><description>&lt;p&gt;
&#38024;&#23545;&#20998;&#24067;&#24335;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#21644;&#37096;&#20998;&#26631;&#31614;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#65292;&#20248;&#21270;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Optimizing Federated Learning for Medical Image Classification on Distributed Non-iid Datasets with Partial Labels. (arXiv:2303.06180v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06180
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;FedFBN&#65292;&#19968;&#20010;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#32593;&#32476;&#20316;&#20026;&#27169;&#22411;&#21518;&#31471;&#65292;&#24182;&#22312;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#20013;&#20923;&#32467;&#25209;&#37327;&#24402;&#19968;&#21270;&#23618;&#65292;&#20197;&#20248;&#21270;&#20998;&#24067;&#24335;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#21644;&#37096;&#20998;&#26631;&#31614;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes FedFBN, a federated learning framework that uses pretrained networks as the model backend and freezes the batch normalization layers throughout the training process to optimize medical image classification on distributed non-iid datasets with partial labels.
&lt;/p&gt;
&lt;p&gt;
&#22823;&#37327;&#30340;&#33016;&#37096;X&#20809;&#25968;&#25454;&#38598;&#24050;&#32463;&#24102;&#22836;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#19987;&#27880;&#20110;&#26816;&#27979;&#21487;&#33021;&#23384;&#22312;&#30340;&#19968;&#37096;&#20998;&#30142;&#30149;&#26631;&#31614;&#65292;&#22240;&#27492;&#20351;&#23427;&#20204;&#25104;&#20026;&#20998;&#24067;&#24335;&#21644;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#37096;&#20998;&#26631;&#31614;&#25968;&#25454;&#38598;&#12290;&#26368;&#36817;&#30340;&#25991;&#29486;&#25351;&#20986;&#65292;&#25209;&#37327;&#24402;&#19968;&#21270;&#23618;&#23545;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#25910;&#25947;&#20855;&#26377;&#24433;&#21709;&#65292;&#22240;&#20026;&#23427;&#20204;&#19982;&#20855;&#26377;&#37096;&#20998;&#26631;&#31614;&#30340;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#30456;&#20851;&#30340;&#22495;&#28418;&#31227;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FedFBN&#65292;&#36825;&#26159;&#19968;&#20010;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#20174;&#36801;&#31227;&#23398;&#20064;&#20013;&#27762;&#21462;&#28789;&#24863;&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#32593;&#32476;&#20316;&#20026;&#27169;&#22411;&#21518;&#31471;&#65292;&#24182;&#22312;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#20013;&#20923;&#32467;&#25209;&#37327;&#24402;&#19968;&#21270;&#23618;&#12290;&#25105;&#20204;&#20351;&#29992;&#21512;&#25104;iid&#29609;&#20855;&#25968;&#25454;&#38598;&#21644;&#22823;&#35268;&#27169;&#38750;iid&#25968;&#25454;&#38598;&#35780;&#20272;FedFBN&#19982;&#24403;&#21069;FL&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;FedFBN&#20248;&#20110;&#20351;&#29992;&#20998;&#24067;&#24335;&#21644;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#35757;&#32451;&#20840;&#23616;&#27169;&#22411;&#30340;&#24403;&#21069;&#32858;&#21512;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Numerous large-scale chest x-ray datasets have spearheaded expert-level detection of abnormalities using deep learning. However, these datasets focus on detecting a subset of disease labels that could be present, thus making them distributed and non-iid with partial labels. Recent literature has indicated the impact of batch normalization layers on the convergence of federated learning due to domain shift associated with non-iid data with partial labels. To that end, we propose FedFBN, a federated learning framework that draws inspiration from transfer learning by using pretrained networks as the model backend and freezing the batch normalization layers throughout the training process. We evaluate FedFBN with current FL strategies using synthetic iid toy datasets and large-scale non-iid datasets across scenarios with partial and complete labels. Our results demonstrate that FedFBN outperforms current aggregation strategies for training global models using distributed and non-iid data w
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36716;&#31227;&#23398;&#20064;&#25216;&#26415;&#65292;&#21033;&#29992;&#21487;&#29992;&#25968;&#25454;&#38598;&#29983;&#25104;&#19968;&#20010;&#27169;&#22411;&#65292;&#20197;&#26816;&#27979;&#19981;&#21516;&#32534;&#31243;&#35821;&#35328;&#20013;&#30340;&#24120;&#35265;&#28431;&#27934;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#20197;&#24179;&#22343;&#21484;&#22238;&#29575;&#20026;72&#65285;&#26816;&#27979;C&#21644;Java&#20195;&#30721;&#20013;&#30340;&#28431;&#27934;&#12290;</title><link>http://arxiv.org/abs/2303.06177</link><description>&lt;p&gt;
&#32534;&#31243;&#35821;&#35328;&#20043;&#38388;&#30340;&#36719;&#20214;&#28431;&#27934;&#39044;&#27979;&#30693;&#35782;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
Software Vulnerability Prediction Knowledge Transferring Between Programming Languages. (arXiv:2303.06177v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06177
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36716;&#31227;&#23398;&#20064;&#25216;&#26415;&#65292;&#21033;&#29992;&#21487;&#29992;&#25968;&#25454;&#38598;&#29983;&#25104;&#19968;&#20010;&#27169;&#22411;&#65292;&#20197;&#26816;&#27979;&#19981;&#21516;&#32534;&#31243;&#35821;&#35328;&#20013;&#30340;&#24120;&#35265;&#28431;&#27934;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#20197;&#24179;&#22343;&#21484;&#22238;&#29575;&#20026;72&#65285;&#26816;&#27979;C&#21644;Java&#20195;&#30721;&#20013;&#30340;&#28431;&#27934;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study proposes a transfer learning technique to detect common vulnerabilities in different programming languages by leveraging available datasets. The results show that the proposed model detects vulnerabilities in both C and Java codes with an average recall of 72%.
&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#33258;&#21160;&#21270;&#21644;&#26234;&#33021;&#30340;&#36719;&#20214;&#28431;&#27934;&#26816;&#27979;&#27169;&#22411;&#19968;&#30452;&#21463;&#21040;&#30740;&#31350;&#21644;&#24320;&#21457;&#31038;&#21306;&#30340;&#20851;&#27880;&#12290;&#36825;&#20010;&#39046;&#22495;&#26368;&#22823;&#30340;&#25361;&#25112;&#20043;&#19968;&#26159;&#32570;&#20047;&#25152;&#26377;&#19981;&#21516;&#32534;&#31243;&#35821;&#35328;&#30340;&#20195;&#30721;&#26679;&#26412;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#36716;&#31227;&#23398;&#20064;&#25216;&#26415;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21033;&#29992;&#21487;&#29992;&#25968;&#25454;&#38598;&#29983;&#25104;&#19968;&#20010;&#27169;&#22411;&#65292;&#20197;&#26816;&#27979;&#19981;&#21516;&#32534;&#31243;&#35821;&#35328;&#20013;&#30340;&#24120;&#35265;&#28431;&#27934;&#12290;&#25105;&#20204;&#20351;&#29992;C&#28304;&#20195;&#30721;&#26679;&#26412;&#35757;&#32451;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#27169;&#22411;&#65292;&#28982;&#21518;&#20351;&#29992;Java&#28304;&#20195;&#30721;&#26679;&#26412;&#26469;&#37319;&#29992;&#21644;&#35780;&#20272;&#23398;&#20064;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#20195;&#30721;&#26679;&#26412;&#65306;NIST&#36719;&#20214;&#20445;&#38556;&#21442;&#32771;&#25968;&#25454;&#38598;&#65288;SARD&#65289;&#21644;Draper VDISC&#25968;&#25454;&#38598;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#20197;&#24179;&#22343;&#21484;&#22238;&#29575;&#20026;72&#65285;&#26816;&#27979;C&#21644;Java&#20195;&#30721;&#20013;&#30340;&#28431;&#27934;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#21487;&#35299;&#37322;&#30340;AI&#26469;&#35843;&#26597;&#27599;&#20010;&#29305;&#24449;&#23545;&#30693;&#35782;&#36716;&#31227;&#26426;&#21046;&#30340;&#36129;&#29486;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developing automated and smart software vulnerability detection models has been receiving great attention from both research and development communities. One of the biggest challenges in this area is the lack of code samples for all different programming languages. In this study, we address this issue by proposing a transfer learning technique to leverage available datasets and generate a model to detect common vulnerabilities in different programming languages. We use C source code samples to train a Convolutional Neural Network (CNN) model, then, we use Java source code samples to adopt and evaluate the learned model. We use code samples from two benchmark datasets: NIST Software Assurance Reference Dataset (SARD) and Draper VDISC dataset. The results show that proposed model detects vulnerabilities in both C and Java codes with average recall of 72\%. Additionally, we employ explainable AI to investigate how much each feature contributes to the knowledge transfer mechanisms between 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#24335;&#23398;&#20064;&#36895;&#24230;&#26694;&#26550;&#65292;&#29992;&#20110;&#32479;&#19968;&#29702;&#35299;Grokking&#21644;&#21452;&#37325;&#19979;&#38477;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#27169;&#22411;&#26234;&#33021;Grokking&#30340;&#31532;&#19968;&#20010;&#28436;&#31034;&#12290;</title><link>http://arxiv.org/abs/2303.06173</link><description>&lt;p&gt;
&#32479;&#19968;&#29702;&#35299;Grokking&#21644;&#21452;&#37325;&#19979;&#38477;
&lt;/p&gt;
&lt;p&gt;
Unifying Grokking and Double Descent. (arXiv:2303.06173v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06173
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#24335;&#23398;&#20064;&#36895;&#24230;&#26694;&#26550;&#65292;&#29992;&#20110;&#32479;&#19968;&#29702;&#35299;Grokking&#21644;&#21452;&#37325;&#19979;&#38477;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#27169;&#22411;&#26234;&#33021;Grokking&#30340;&#31532;&#19968;&#20010;&#28436;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a framework of pattern learning speeds to unify the understanding of Grokking and double descent, and provides the first demonstration of model-wise Grokking.
&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#65292;&#23545;&#27867;&#21270;&#30340;&#21407;&#21017;&#24615;&#29702;&#35299;&#21487;&#33021;&#38656;&#35201;&#23558;&#19981;&#21516;&#30340;&#35266;&#23519;&#32467;&#26524;&#32479;&#19968;&#21040;&#19968;&#20010;&#27010;&#24565;&#26694;&#26550;&#19979;&#12290;&#20197;&#21069;&#30340;&#24037;&#20316;&#30740;&#31350;&#20102;&#8220;Grokking&#8221;&#65292;&#36825;&#26159;&#19968;&#31181;&#35757;&#32451;&#21160;&#24577;&#65292;&#20854;&#20013;&#25345;&#32493;&#30340;&#36817;&#20046;&#23436;&#32654;&#30340;&#35757;&#32451;&#34920;&#29616;&#21644;&#36817;&#20046;&#20598;&#28982;&#30340;&#27979;&#35797;&#34920;&#29616;&#26368;&#32456;&#20250;&#23548;&#33268;&#27867;&#21270;&#65292;&#20197;&#21450;&#34920;&#38754;&#19978;&#31867;&#20284;&#30340;&#8220;&#21452;&#37325;&#19979;&#38477;&#8221;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#36825;&#20123;&#20027;&#39064;&#24050;&#32463;&#34987;&#23396;&#31435;&#22320;&#30740;&#31350;&#12290;&#25105;&#20204;&#20551;&#35774;Grokking&#21644;&#21452;&#37325;&#19979;&#38477;&#21487;&#20197;&#34987;&#29702;&#35299;&#20026;&#27169;&#24335;&#23398;&#20064;&#36895;&#24230;&#26694;&#26550;&#20869;&#30456;&#21516;&#23398;&#20064;&#21160;&#24577;&#30340;&#23454;&#20363;&#12290;&#25105;&#20204;&#25552;&#20986;&#65292;&#24403;&#25913;&#21464;&#27169;&#22411;&#23481;&#37327;&#32780;&#19981;&#26159;&#20248;&#21270;&#27493;&#39588;&#26102;&#65292;&#35813;&#26694;&#26550;&#20063;&#36866;&#29992;&#65292;&#24182;&#25552;&#20379;&#20102;&#27169;&#22411;&#26234;&#33021;Grokking&#30340;&#31532;&#19968;&#20010;&#28436;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
A principled understanding of generalization in deep learning may require unifying disparate observations under a single conceptual framework. Previous work has studied \emph{grokking}, a training dynamic in which a sustained period of near-perfect training performance and near-chance test performance is eventually followed by generalization, as well as the superficially similar \emph{double descent}. These topics have so far been studied in isolation. We hypothesize that grokking and double descent can be understood as instances of the same learning dynamics within a framework of pattern learning speeds. We propose that this framework also applies when varying model capacity instead of optimization steps, and provide the first demonstration of model-wise grokking.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#24191;&#20041;&#28436;&#21592;-&#35780;&#35770;&#23478;QD-RL&#26694;&#26550;&#65292;&#29992;&#20110;QD-RL&#35774;&#32622;&#20013;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#28145;&#24230;RL&#26041;&#27861;&#12290;&#35813;&#26694;&#26550;&#24341;&#20837;&#20102;&#20004;&#31181;&#26032;&#31639;&#27861;&#65292;PGA-ME&#65288;SAC&#65289;&#21644;PGA-ME&#65288;DroQ&#65289;&#65292;&#23558;&#28145;&#24230;RL&#30340;&#26368;&#26032;&#36827;&#23637;&#24212;&#29992;&#20110;QD-RL&#35774;&#32622;&#65292;&#24182;&#35299;&#20915;&#20102;&#29616;&#26377;QD-RL&#31639;&#27861;&#26080;&#27861;&#35299;&#20915;&#30340;&#20154;&#24418;&#29615;&#22659;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.06164</link><description>&lt;p&gt;
&#29702;&#35299;&#36136;&#37327;&#22810;&#26679;&#24615;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Understanding the Synergies between Quality-Diversity and Deep Reinforcement Learning. (arXiv:2303.06164v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06164
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#24191;&#20041;&#28436;&#21592;-&#35780;&#35770;&#23478;QD-RL&#26694;&#26550;&#65292;&#29992;&#20110;QD-RL&#35774;&#32622;&#20013;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#28145;&#24230;RL&#26041;&#27861;&#12290;&#35813;&#26694;&#26550;&#24341;&#20837;&#20102;&#20004;&#31181;&#26032;&#31639;&#27861;&#65292;PGA-ME&#65288;SAC&#65289;&#21644;PGA-ME&#65288;DroQ&#65289;&#65292;&#23558;&#28145;&#24230;RL&#30340;&#26368;&#26032;&#36827;&#23637;&#24212;&#29992;&#20110;QD-RL&#35774;&#32622;&#65292;&#24182;&#35299;&#20915;&#20102;&#29616;&#26377;QD-RL&#31639;&#27861;&#26080;&#27861;&#35299;&#20915;&#30340;&#20154;&#24418;&#29615;&#22659;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a Generalized Actor-Critic QD-RL framework for actor-critic deep RL methods in the QD-RL setting. The framework introduces two new algorithms, PGA-ME (SAC) and PGA-ME (DroQ), which apply recent advancements in Deep RL to the QD-RL setting and solve the humanoid environment problem that existing QD-RL algorithms cannot solve.
&lt;/p&gt;
&lt;p&gt;
&#36136;&#37327;&#22810;&#26679;&#24615;&#65288;QD&#65289;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#24050;&#32463;&#23548;&#33268;&#20102;&#24378;&#22823;&#30340;&#28151;&#21512;QD-RL&#31639;&#27861;&#65292;&#23637;&#31034;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#24182;&#24102;&#26469;&#20102;&#20004;&#20010;&#39046;&#22495;&#30340;&#26368;&#20339;&#23454;&#36341;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#20854;&#20182;RL&#31639;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#22312;&#20808;&#21069;&#30340;&#28151;&#21512;&#26041;&#27861;&#20013;&#20165;&#20351;&#29992;&#20102;&#21333;&#20010;&#28145;&#24230;RL&#31639;&#27861;&#65288;TD3&#65289;&#12290;&#27492;&#22806;&#65292;QD&#21644;RL&#20043;&#38388;&#30340;&#20248;&#21270;&#36807;&#31243;&#23384;&#22312;&#26681;&#26412;&#24046;&#24322;&#65292;&#38656;&#35201;&#26356;&#21152;&#21407;&#21017;&#24615;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#24191;&#20041;&#28436;&#21592;-&#35780;&#35770;&#23478;QD-RL&#65292;&#36825;&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#27169;&#22359;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;QD-RL&#35774;&#32622;&#20013;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#28145;&#24230;RL&#26041;&#27861;&#12290;&#35813;&#26694;&#26550;&#25552;&#20379;&#20102;&#19968;&#26465;&#30740;&#31350;&#28145;&#24230;RL&#22312;QD-RL&#35774;&#32622;&#20013;&#30340;&#35265;&#35299;&#30340;&#36335;&#24452;&#65292;&#36825;&#26159;&#22312;QD-RL&#20013;&#21462;&#24471;&#36827;&#23637;&#30340;&#37325;&#35201;&#19988;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#26032;&#31639;&#27861;&#65292;PGA-ME&#65288;SAC&#65289;&#21644;PGA-ME&#65288;DroQ&#65289;&#65292;&#23558;&#28145;&#24230;RL&#30340;&#26368;&#26032;&#36827;&#23637;&#24212;&#29992;&#20110;QD-RL&#35774;&#32622;&#65292;&#24182;&#35299;&#20915;&#20102;&#29616;&#26377;QD-RL&#31639;&#27861;&#26080;&#27861;&#35299;&#20915;&#30340;&#20154;&#24418;&#29615;&#22659;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The synergies between Quality-Diversity (QD) and Deep Reinforcement Learning (RL) have led to powerful hybrid QD-RL algorithms that have shown tremendous potential, and brings the best of both fields. However, only a single deep RL algorithm (TD3) has been used in prior hybrid methods despite notable progress made by other RL algorithms. Additionally, there are fundamental differences in the optimization procedures between QD and RL which would benefit from a more principled approach. We propose Generalized Actor-Critic QD-RL, a unified modular framework for actor-critic deep RL methods in the QD-RL setting. This framework provides a path to study insights from Deep RL in the QD-RL setting, which is an important and efficient way to make progress in QD-RL. We introduce two new algorithms, PGA-ME (SAC) and PGA-ME (DroQ) which apply recent advancements in Deep RL to the QD-RL setting, and solves the humanoid environment which was not possible using existing QD-RL algorithms. However, we 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#23383;&#23402;&#29983;&#36741;&#21161;&#30340;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#24322;&#26500;&#24615;&#38382;&#39064;&#65292;&#29992;&#25143;&#21487;&#20197;&#36873;&#25321;&#33258;&#24049;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#24182;&#20174;&#22823;&#22411;&#25945;&#24072;&#27169;&#22411;&#20013;&#33976;&#39311;&#30693;&#35782;&#65292;&#21516;&#26102;&#21033;&#29992;&#25968;&#23383;&#23402;&#29983;&#22312;&#26381;&#21153;&#22120;&#19978;&#35757;&#32451;&#22823;&#22411;&#25945;&#24072;&#27169;&#22411;&#65292;&#26368;&#32456;&#36890;&#36807;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#21644;Q-learning&#31639;&#27861;&#23454;&#29616;&#27169;&#22411;&#36873;&#25321;&#21644;&#36164;&#28304;&#20998;&#37197;&#12290;</title><link>http://arxiv.org/abs/2303.06155</link><description>&lt;p&gt;
&#25968;&#23383;&#23402;&#29983;&#36741;&#21161;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#30340;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Digital Twin-Assisted Knowledge Distillation Framework for Heterogeneous Federated Learning. (arXiv:2303.06155v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06155
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#23383;&#23402;&#29983;&#36741;&#21161;&#30340;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#24322;&#26500;&#24615;&#38382;&#39064;&#65292;&#29992;&#25143;&#21487;&#20197;&#36873;&#25321;&#33258;&#24049;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#24182;&#20174;&#22823;&#22411;&#25945;&#24072;&#27169;&#22411;&#20013;&#33976;&#39311;&#30693;&#35782;&#65292;&#21516;&#26102;&#21033;&#29992;&#25968;&#23383;&#23402;&#29983;&#22312;&#26381;&#21153;&#22120;&#19978;&#35757;&#32451;&#22823;&#22411;&#25945;&#24072;&#27169;&#22411;&#65292;&#26368;&#32456;&#36890;&#36807;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#21644;Q-learning&#31639;&#27861;&#23454;&#29616;&#27169;&#22411;&#36873;&#25321;&#21644;&#36164;&#28304;&#20998;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a digital twin-assisted knowledge distillation framework for heterogeneous federated learning, where users can select their own neural network models and distill knowledge from a big teacher model, and the teacher model can be trained on a digital twin located in the server. The joint problem of model selection and training offloading and resource allocation for users is formulated as a mixed integer programming problem and solved using Q-learning and optimization algorithms.
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#33976;&#39311;&#39537;&#21160;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#24212;&#23545;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#24322;&#26500;&#24615;&#65292;&#20854;&#20013;&#27599;&#20010;&#29992;&#25143;&#21487;&#20197;&#26681;&#25454;&#38656;&#35201;&#36873;&#25321;&#20854;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#33258;&#24049;&#30340;&#31169;&#26377;&#25968;&#25454;&#38598;&#20174;&#22823;&#22411;&#25945;&#24072;&#27169;&#22411;&#20013;&#33976;&#39311;&#30693;&#35782;&#12290;&#20026;&#20102;&#20811;&#26381;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#29992;&#25143;&#35774;&#22791;&#19978;&#35757;&#32451;&#22823;&#22411;&#25945;&#24072;&#27169;&#22411;&#30340;&#25361;&#25112;&#65292;&#21033;&#29992;&#25968;&#23383;&#23402;&#29983;&#30340;&#26041;&#24335;&#65292;&#25945;&#24072;&#27169;&#22411;&#21487;&#20197;&#22312;&#20855;&#26377;&#36275;&#22815;&#35745;&#31639;&#36164;&#28304;&#30340;&#26381;&#21153;&#22120;&#19978;&#30340;&#25968;&#23383;&#23402;&#29983;&#20013;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#21518;&#65292;&#22312;&#27169;&#22411;&#33976;&#39311;&#26399;&#38388;&#65292;&#27599;&#20010;&#29992;&#25143;&#21487;&#20197;&#22312;&#29289;&#29702;&#23454;&#20307;&#25110;&#25968;&#23383;&#20195;&#29702;&#22788;&#26356;&#26032;&#20854;&#27169;&#22411;&#30340;&#21442;&#25968;&#12290;&#20026;&#29992;&#25143;&#36873;&#25321;&#27169;&#22411;&#21644;&#35757;&#32451;&#21368;&#36733;&#21644;&#36164;&#28304;&#20998;&#37197;&#21046;&#23450;&#20102;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#65288;MIP&#65289;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#32852;&#21512;&#20351;&#29992;Q-learning&#21644;&#20248;&#21270;&#65292;&#20854;&#20013;Q-learning&#20026;&#29992;&#25143;&#36873;&#25321;&#27169;&#22411;&#24182;&#30830;&#23450;&#26159;&#22312;&#26412;&#22320;&#36824;&#26159;&#22312;&#26381;&#21153;&#22120;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#20248;&#21270;&#21017;&#29992;&#20110;&#36164;&#28304;&#20998;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, to deal with the heterogeneity in federated learning (FL) systems, a knowledge distillation (KD) driven training framework for FL is proposed, where each user can select its neural network model on demand and distill knowledge from a big teacher model using its own private dataset. To overcome the challenge of train the big teacher model in resource limited user devices, the digital twin (DT) is exploit in the way that the teacher model can be trained at DT located in the server with enough computing resources. Then, during model distillation, each user can update the parameters of its model at either the physical entity or the digital agent. The joint problem of model selection and training offloading and resource allocation for users is formulated as a mixed integer programming (MIP) problem. To solve the problem, Q-learning and optimization are jointly used, where Q-learning selects models for users and determines whether to train locally or on the server, and optimiz
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#28436;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#36890;&#29992;&#20989;&#25968;&#34920;&#31034;&#35821;&#35328;&#21644;&#26694;&#26550;&#26469;&#34920;&#31034;&#29305;&#23450;&#23545;&#35937;&#21450;&#20854;&#21442;&#19982;&#25903;&#25345;&#20854;&#35774;&#35745;&#30340;&#36807;&#31243;&#65292;&#20174;&#32780;&#23454;&#29616;&#28145;&#20837;&#30340;&#27010;&#24565;&#29702;&#35299;&#65292;&#21487;&#35299;&#37322;&#24615;&#30340;&#21151;&#33021;&#65292;&#20351;&#31995;&#32479;&#33021;&#22815;&#22238;&#31572;&#8220;&#20026;&#20160;&#20040;&#8221;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.06152</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;&#36825;&#26159;&#19968;&#20010;&#22909;&#30340;&#25110;&#19981;&#26159;&#19968;&#20010;&#22909;&#30340;&#29006;&#38149;&#65311;&#8212;&#8212;&#23545;&#35937;&#21644;&#24037;&#20855;&#21151;&#33021;&#30340;&#30693;&#35782;&#34920;&#31034;&#65292;&#29992;&#20110;&#35774;&#35745;&#29702;&#35299;&#12289;&#25913;&#36827;&#21644;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Why is That a Good or Not a Good Frying Pan? -- Knowledge Representation for Functions of Objects and Tools for Design Understanding, Improvement, and Generation for Design Understanding, Improvement, and Generation. (arXiv:2303.06152v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06152
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28436;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#36890;&#29992;&#20989;&#25968;&#34920;&#31034;&#35821;&#35328;&#21644;&#26694;&#26550;&#26469;&#34920;&#31034;&#29305;&#23450;&#23545;&#35937;&#21450;&#20854;&#21442;&#19982;&#25903;&#25345;&#20854;&#35774;&#35745;&#30340;&#36807;&#31243;&#65292;&#20174;&#32780;&#23454;&#29616;&#28145;&#20837;&#30340;&#27010;&#24565;&#29702;&#35299;&#65292;&#21487;&#35299;&#37322;&#24615;&#30340;&#21151;&#33021;&#65292;&#20351;&#31995;&#32479;&#33021;&#22815;&#22238;&#31572;&#8220;&#20026;&#20160;&#20040;&#8221;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper demonstrates how a particular object and its participation in the processes it is designed to support can be represented in a general function representational language and framework, leading to a deep conceptual understanding with explainability of functionalities that allows the system to answer "why" questions.
&lt;/p&gt;
&lt;p&gt;
&#23545;&#35937;&#21644;&#24037;&#20855;&#30340;&#21151;&#33021;&#26041;&#38754;&#30340;&#29702;&#35299;&#23545;&#20110;&#25903;&#25345;&#26234;&#33021;&#31995;&#32479;&#22312;&#29615;&#22659;&#20013;&#23548;&#33322;&#21644;&#19982;&#21508;&#31181;&#23545;&#35937;&#12289;&#32467;&#26500;&#21644;&#31995;&#32479;&#36827;&#34892;&#20132;&#20114;&#20197;&#24110;&#21161;&#23454;&#29616;&#20854;&#30446;&#26631;&#33267;&#20851;&#37325;&#35201;&#12290;&#23545;&#21151;&#33021;&#30340;&#35814;&#32454;&#29702;&#35299;&#20063;&#21487;&#20197;&#23548;&#33268;&#35774;&#35745;&#25913;&#36827;&#21644;&#26032;&#39062;&#30340;&#35774;&#35745;&#65292;&#20174;&#32780;&#22686;&#24378;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#25805;&#20316;&#65292;&#19968;&#26041;&#38754;&#65292;&#22686;&#24378;&#20154;&#31867;&#29983;&#27963;&#12290;&#26412;&#25991;&#28436;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#36890;&#29992;&#20989;&#25968;&#34920;&#31034;&#35821;&#35328;&#21644;&#26694;&#26550;&#26469;&#34920;&#31034;&#29305;&#23450;&#23545;&#35937;&#65288;&#22312;&#26412;&#20363;&#20013;&#20026;&#29006;&#38149;&#65289;&#21450;&#20854;&#21442;&#19982;&#25903;&#25345;&#20854;&#35774;&#35745;&#30340;&#36807;&#31243;&#65288;&#22312;&#26412;&#20363;&#20013;&#20026;&#29006;&#28856;&#36807;&#31243;&#65289;&#65292;&#20174;&#32780;&#21487;&#20197;&#29992;&#20110;&#35814;&#32454;&#35828;&#26126;&#28041;&#21450;&#30340;&#36807;&#31243;&#21644;&#21151;&#33021;&#65292;&#20174;&#32780;&#23454;&#29616;&#28145;&#20837;&#30340;&#27010;&#24565;&#29702;&#35299;&#65292;&#21487;&#35299;&#37322;&#24615;&#30340;&#21151;&#33021;&#65292;&#20351;&#31995;&#32479;&#33021;&#22815;&#22238;&#31572;&#8220;&#20026;&#20160;&#20040;&#8221;&#38382;&#39064;&#8212;&#8212;&#20026;&#20160;&#20040;&#36825;&#26159;&#19968;&#20010;&#22909;&#30340;&#29006;&#38149;&#65292;&#25110;&#32773;&#20026;&#20160;&#20040;&#26576;&#20010;&#37096;&#20214;&#22312;&#29006;&#38149;&#20013;&#30340;&#20301;&#32622;&#24456;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
The understanding of the functional aspects of objects and tools is of paramount importance in supporting an intelligent system in navigating around in the environment and interacting with various objects, structures, and systems, to help fulfil its goals. A detailed understanding of functionalities can also lead to design improvements and novel designs that would enhance the operations of AI and robotic systems on the one hand, and human lives on the other. This paper demonstrates how a particular object - in this case, a frying pan - and its participation in the processes it is designed to support - in this case, the frying process - can be represented in a general function representational language and framework, that can be used to flesh out the processes and functionalities involved, leading to a deep conceptual understanding with explainability of functionalities that allows the system to answer "why" questions - why is something a good frying pan, say, or why a certain part on t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;NoiseCAM&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#23450;&#20301;&#26131;&#21463;&#25915;&#20987;&#23618;&#24182;&#26816;&#27979;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#21516;&#26102;&#19981;&#20250;&#23545;&#28151;&#20837;&#36755;&#20837;&#30340;&#39640;&#26031;&#38543;&#26426;&#22122;&#22768;&#20570;&#20986;&#21453;&#24212;&#12290;</title><link>http://arxiv.org/abs/2303.06151</link><description>&lt;p&gt;
NoiseCAM: &#29992;&#20110;&#22122;&#22768;&#21644;&#23545;&#25239;&#25915;&#20987;&#36793;&#30028;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
NoiseCAM: Explainable AI for the Boundary Between Noise and Adversarial Attacks. (arXiv:2303.06151v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06151
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;NoiseCAM&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#23450;&#20301;&#26131;&#21463;&#25915;&#20987;&#23618;&#24182;&#26816;&#27979;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#21516;&#26102;&#19981;&#20250;&#23545;&#28151;&#20837;&#36755;&#20837;&#30340;&#39640;&#26031;&#38543;&#26426;&#22122;&#22768;&#20570;&#20986;&#21453;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a NoiseCAM algorithm that can locate vulnerable layers and detect adversarial examples, while not responding to Gaussian random noise mixed in the inputs.
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#20010;&#39046;&#22495;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#23545;&#25239;&#25915;&#20987;&#24456;&#23481;&#26131;&#35823;&#23548;&#31070;&#32463;&#32593;&#32476;&#24182;&#23548;&#33268;&#38169;&#35823;&#20915;&#31574;&#12290;&#22312;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#39640;&#24230;&#38656;&#35201;&#38450;&#24481;&#26426;&#21046;&#12290;&#26412;&#25991;&#39318;&#20808;&#20351;&#29992;&#26799;&#24230;&#31867;&#28608;&#27963;&#26144;&#23556;&#65288;GradCAM&#65289;&#20998;&#26512;VGG-16&#32593;&#32476;&#22312;&#20854;&#36755;&#20837;&#19982;&#23545;&#25239;&#25200;&#21160;&#25110;&#39640;&#26031;&#22122;&#22768;&#28151;&#21512;&#26102;&#30340;&#34892;&#20026;&#20559;&#24046;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#23450;&#20301;&#23545;&#25239;&#25200;&#21160;&#21644;&#39640;&#26031;&#22122;&#22768;&#25935;&#24863;&#30340;&#26131;&#21463;&#25915;&#20987;&#23618;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#26131;&#21463;&#25915;&#20987;&#23618;&#30340;&#34892;&#20026;&#20559;&#24046;&#21487;&#20197;&#29992;&#20110;&#26816;&#27979;&#23545;&#25239;&#24615;&#31034;&#20363;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;NoiseCAM&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#38598;&#25104;&#20102;&#20840;&#23616;&#21644;&#20687;&#32032;&#32423;&#21152;&#26435;&#31867;&#28608;&#27963;&#26144;&#23556;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#25200;&#21160;&#30340;&#24433;&#21709;&#65292;&#19981;&#20250;&#23545;&#28151;&#20837;&#36755;&#20837;&#30340;&#39640;&#26031;&#38543;&#26426;&#22122;&#22768;&#20570;&#20986;&#21453;&#24212;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#26816;&#27979;&#23545;&#25239;&#24615;&#31034;&#20363;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Learning (DL) and Deep Neural Networks (DNNs) are widely used in various domains. However, adversarial attacks can easily mislead a neural network and lead to wrong decisions. Defense mechanisms are highly preferred in safety-critical applications. In this paper, firstly, we use the gradient class activation map (GradCAM) to analyze the behavior deviation of the VGG-16 network when its inputs are mixed with adversarial perturbation or Gaussian noise. In particular, our method can locate vulnerable layers that are sensitive to adversarial perturbation and Gaussian noise. We also show that the behavior deviation of vulnerable layers can be used to detect adversarial examples. Secondly, we propose a novel NoiseCAM algorithm that integrates information from globally and pixel-level weighted class activation maps. Our algorithm is susceptible to adversarial perturbations and will not respond to Gaussian random noise mixed in the inputs. Third, we compare detecting adversarial examples 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35268;&#21010;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21644;&#30495;&#23454;&#30005;&#21147;&#32593;&#29615;&#22659;&#30340;&#31995;&#32479;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#23454;&#29616;&#21457;&#30005;&#26426;&#30340;&#35268;&#21010;&#21644;&#26356;&#32454;&#30340;&#26102;&#38388;&#20998;&#36776;&#29575;&#35843;&#25972;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#30005;&#32593;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.05205</link><description>&lt;p&gt;
&#22522;&#20110;&#35268;&#21010;&#24378;&#21270;&#23398;&#20064;&#30340;&#21487;&#20877;&#29983;&#33021;&#28304;&#30005;&#21147;&#31995;&#32479;&#23454;&#26102;&#35843;&#24230;
&lt;/p&gt;
&lt;p&gt;
Real-time scheduling of renewable power systems through planning-based reinforcement learning. (arXiv:2303.05205v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05205
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35268;&#21010;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21644;&#30495;&#23454;&#30005;&#21147;&#32593;&#29615;&#22659;&#30340;&#31995;&#32479;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#23454;&#29616;&#21457;&#30005;&#26426;&#30340;&#35268;&#21010;&#21644;&#26356;&#32454;&#30340;&#26102;&#38388;&#20998;&#36776;&#29575;&#35843;&#25972;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#30005;&#32593;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a systematic solution based on the state-of-the-art reinforcement learning algorithm and the real power grid environment, which enables planning and finer time resolution adjustments of power generators, including unit commitment and economic dispatch, thus increasing the grid's ability.
&lt;/p&gt;
&lt;p&gt;
&#19981;&#26029;&#22686;&#38271;&#30340;&#21487;&#20877;&#29983;&#33021;&#28304;&#26469;&#28304;&#23545;&#20256;&#32479;&#30005;&#21147;&#35843;&#24230;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#36816;&#33829;&#21830;&#38590;&#20197;&#33719;&#24471;&#20934;&#30830;&#30340;&#21487;&#20877;&#29983;&#33021;&#28304;&#21457;&#30005;&#26085;&#21069;&#39044;&#27979;&#65292;&#22240;&#27492;&#38656;&#35201;&#26410;&#26469;&#35843;&#24230;&#31995;&#32479;&#26681;&#25454;&#36229;&#30701;&#26399;&#39044;&#27979;&#36827;&#34892;&#23454;&#26102;&#35843;&#24230;&#20915;&#31574;&#12290;&#21463;&#35745;&#31639;&#36895;&#24230;&#38480;&#21046;&#65292;&#20256;&#32479;&#30340;&#22522;&#20110;&#20248;&#21270;&#30340;&#26041;&#27861;&#26080;&#27861;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#26368;&#36817;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#21457;&#23637;&#24050;&#32463;&#23637;&#31034;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;RL&#26041;&#27861;&#22312;&#32422;&#26463;&#22797;&#26434;&#24615;&#12289;&#31639;&#27861;&#24615;&#33021;&#21644;&#29615;&#22659;&#20445;&#30495;&#24230;&#26041;&#38754;&#19981;&#36275;&#12290;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#25552;&#20986;&#22522;&#20110;&#26368;&#20808;&#36827;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21644;&#30495;&#23454;&#30005;&#21147;&#32593;&#29615;&#22659;&#30340;&#31995;&#32479;&#35299;&#20915;&#26041;&#26696;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#21457;&#30005;&#26426;&#30340;&#35268;&#21010;&#21644;&#26356;&#32454;&#30340;&#26102;&#38388;&#20998;&#36776;&#29575;&#35843;&#25972;&#25104;&#20026;&#21487;&#33021;&#65292;&#21253;&#25324;&#26426;&#32452;&#32452;&#21512;&#21644;&#32463;&#27982;&#35843;&#24230;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#30005;&#32593;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The growing renewable energy sources have posed significant challenges to traditional power scheduling. It is difficult for operators to obtain accurate day-ahead forecasts of renewable generation, thereby requiring the future scheduling system to make real-time scheduling decisions aligning with ultra-short-term forecasts. Restricted by the computation speed, traditional optimization-based methods can not solve this problem. Recent developments in reinforcement learning (RL) have demonstrated the potential to solve this challenge. However, the existing RL methods are inadequate in terms of constraint complexity, algorithm performance, and environment fidelity. We are the first to propose a systematic solution based on the state-of-the-art reinforcement learning algorithm and the real power grid environment. The proposed approach enables planning and finer time resolution adjustments of power generators, including unit commitment and economic dispatch, thus increasing the grid's abilit
&lt;/p&gt;</description></item><item><title>NASTyLinker&#26159;&#19968;&#31181;NIL&#24863;&#30693;&#30340;&#23454;&#20307;&#38142;&#25509;&#22120;&#65292;&#23427;&#36890;&#36807;&#29983;&#25104;&#25552;&#21450;&#31751;&#26469;&#34920;&#31034;NIL&#23454;&#20307;&#65292;&#24182;&#22312;&#20445;&#25345;&#24050;&#30693;&#23454;&#20307;&#39640;&#38142;&#25509;&#24615;&#33021;&#30340;&#21516;&#26102;&#35299;&#20915;&#20914;&#31361;&#12290;</title><link>http://arxiv.org/abs/2303.04426</link><description>&lt;p&gt;
NASTyLinker&#65306;NIL&#24863;&#30693;&#21487;&#25193;&#23637;&#22522;&#20110;Transformer&#30340;&#23454;&#20307;&#38142;&#25509;&#22120;
&lt;/p&gt;
&lt;p&gt;
NASTyLinker: NIL-Aware Scalable Transformer-based Entity Linker. (arXiv:2303.04426v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04426
&lt;/p&gt;
&lt;p&gt;
NASTyLinker&#26159;&#19968;&#31181;NIL&#24863;&#30693;&#30340;&#23454;&#20307;&#38142;&#25509;&#22120;&#65292;&#23427;&#36890;&#36807;&#29983;&#25104;&#25552;&#21450;&#31751;&#26469;&#34920;&#31034;NIL&#23454;&#20307;&#65292;&#24182;&#22312;&#20445;&#25345;&#24050;&#30693;&#23454;&#20307;&#39640;&#38142;&#25509;&#24615;&#33021;&#30340;&#21516;&#26102;&#35299;&#20915;&#20914;&#31361;&#12290;
&lt;/p&gt;
&lt;p&gt;
NASTyLinker is a NIL-aware entity linker that represents NIL entities by producing mention clusters and resolves conflicts while maintaining high linking performance for known entities.
&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#38142;&#25509;&#65288;EL&#65289;&#26159;&#26816;&#27979;&#25991;&#26412;&#20013;&#23454;&#20307;&#25552;&#21450;&#24182;&#23558;&#20854;&#28040;&#27495;&#20026;&#21442;&#32771;&#30693;&#35782;&#24211;&#30340;&#20219;&#21153;&#12290;&#22823;&#22810;&#25968;&#27969;&#34892;&#30340;EL&#26041;&#27861;&#20551;&#23450;&#21442;&#32771;&#30693;&#35782;&#24211;&#26159;&#23436;&#25972;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#38656;&#35201;&#22788;&#29702;&#38142;&#25509;&#21040;&#19981;&#21253;&#21547;&#22312;&#30693;&#35782;&#24211;&#20013;&#30340;&#23454;&#20307;&#65288;NIL&#23454;&#20307;&#65289;&#30340;&#24773;&#20917;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#32771;&#34385;&#25552;&#21450;&#20043;&#38388;&#30340;&#20146;&#21644;&#21147;&#21487;&#20197;&#29992;&#20110;&#34920;&#31034;NIL&#23454;&#20307;&#65292;&#26041;&#27861;&#26159;&#36890;&#36807;&#29983;&#25104;&#25552;&#21450;&#31751;&#12290;&#21516;&#26102;&#65292;&#25552;&#21450;&#20043;&#38388;&#30340;&#20146;&#21644;&#21147;&#21487;&#20197;&#24110;&#21161;&#26174;&#33879;&#25552;&#39640;&#24050;&#30693;&#23454;&#20307;&#30340;&#38142;&#25509;&#24615;&#33021;&#12290;&#36890;&#36807;NASTyLinker&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;EL&#26041;&#27861;&#65292;&#23427;&#30693;&#36947;NIL&#23454;&#20307;&#24182;&#20135;&#29983;&#30456;&#24212;&#30340;&#25552;&#21450;&#31751;&#65292;&#21516;&#26102;&#20445;&#25345;&#24050;&#30693;&#23454;&#20307;&#30340;&#39640;&#38142;&#25509;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;Transformer&#30340;&#23494;&#38598;&#34920;&#31034;&#23545;&#25552;&#21450;&#21644;&#23454;&#20307;&#36827;&#34892;&#32858;&#31867;&#65292;&#24182;&#35299;&#20915;&#20914;&#31361;&#65288;&#22914;&#26524;&#19968;&#20010;&#23454;&#20307;&#26377;&#22810;&#20010;&#25552;&#21450;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entity Linking (EL) is the task of detecting mentions of entities in text and disambiguating them to a reference knowledge base. Most prevalent EL approaches assume that the reference knowledge base is complete. In practice, however, it is necessary to deal with the case of linking to an entity that is not contained in the knowledge base (NIL entity). Recent works have shown that, instead of focusing only on affinities between mentions and entities, considering inter-mention affinities can be used to represent NIL entities by producing clusters of mentions. At the same time, inter-mention affinities can help to substantially improve linking performance for known entities. With NASTyLinker, we introduce an EL approach that is aware of NIL entities and produces corresponding mention clusters while maintaining high linking performance for known entities. The approach clusters mentions and entities based on dense representations from Transformers and resolves conflicts (if more than one en
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;AudioLDM&#20316;&#20026;&#22768;&#38899;&#29983;&#25104;&#30340;&#39592;&#24178;&#30340;&#20248;&#21183;&#65292;&#35777;&#26126;&#20102;&#22312;&#25968;&#25454;&#31232;&#32570;&#24773;&#20917;&#19979;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#21040;&#22768;&#38899;&#29983;&#25104;&#30340;&#20248;&#21183;&#65292;&#24182;&#22312;&#20960;&#20010;&#24120;&#29992;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#30456;&#21516;&#30340;&#35780;&#20272;&#21327;&#35758;&#35780;&#20272;&#20102;&#21508;&#31181;&#25991;&#26412;&#21040;&#22768;&#38899;&#29983;&#25104;&#31995;&#32479;&#65292;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2303.03857</link><description>&lt;p&gt;
&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;AudioLDM&#36827;&#34892;&#25991;&#26412;&#21040;&#22768;&#38899;&#29983;&#25104;&#65306;&#22522;&#20934;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Leveraging Pre-trained AudioLDM for Text to Sound Generation: A Benchmark Study. (arXiv:2303.03857v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03857
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;AudioLDM&#20316;&#20026;&#22768;&#38899;&#29983;&#25104;&#30340;&#39592;&#24178;&#30340;&#20248;&#21183;&#65292;&#35777;&#26126;&#20102;&#22312;&#25968;&#25454;&#31232;&#32570;&#24773;&#20917;&#19979;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#21040;&#22768;&#38899;&#29983;&#25104;&#30340;&#20248;&#21183;&#65292;&#24182;&#22312;&#20960;&#20010;&#24120;&#29992;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#30456;&#21516;&#30340;&#35780;&#20272;&#21327;&#35758;&#35780;&#20272;&#20102;&#21508;&#31181;&#25991;&#26412;&#21040;&#22768;&#38899;&#29983;&#25104;&#31995;&#32479;&#65292;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the advantages of using pre-trained AudioLDM as the backbone for sound generation, demonstrates the benefits of using pre-trained models for text-to-sound generation in data-scarcity scenarios, and evaluates various text-to-sound generation systems on several frequently used datasets under the same evaluation protocols to provide a basis for future research.
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26368;&#36817;&#22312;&#25991;&#26412;&#25552;&#31034;&#19979;&#23454;&#29616;&#20102;&#22768;&#38899;&#29983;&#25104;&#30340;&#31361;&#30772;&#12290;&#23613;&#31649;&#23427;&#20204;&#30340;&#34920;&#29616;&#24456;&#26377;&#21069;&#36884;&#65292;&#20294;&#24403;&#21069;&#30340;&#25991;&#26412;&#21040;&#22768;&#38899;&#29983;&#25104;&#27169;&#22411;&#22312;&#23567;&#35268;&#27169;&#25968;&#25454;&#38598;&#65288;&#20363;&#22914;&#36807;&#24230;&#25311;&#21512;&#65289;&#19978;&#38754;&#20020;&#38382;&#39064;&#65292;&#20174;&#32780;&#26174;&#33879;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;AudioLDM&#20316;&#20026;&#22768;&#38899;&#29983;&#25104;&#30340;&#39592;&#24178;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#35777;&#26126;&#20102;&#22312;&#25968;&#25454;&#31232;&#32570;&#24773;&#20917;&#19979;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#21040;&#22768;&#38899;&#29983;&#25104;&#30340;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;&#23454;&#39564;&#34920;&#26126;&#65292;&#19981;&#21516;&#30340;&#35757;&#32451;&#31574;&#30053;&#65288;&#20363;&#22914;&#35757;&#32451;&#26465;&#20214;&#65289;&#21487;&#33021;&#20250;&#24433;&#21709;AudioLDM&#22312;&#19981;&#21516;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#20419;&#36827;&#26410;&#26469;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#36824;&#22312;&#20960;&#20010;&#24120;&#29992;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#30456;&#21516;&#30340;&#35780;&#20272;&#21327;&#35758;&#35780;&#20272;&#20102;&#21508;&#31181;&#25991;&#26412;&#21040;&#22768;&#38899;&#29983;&#25104;&#31995;&#32479;&#65292;&#36825;&#20123;&#21327;&#35758;&#20801;&#35768;&#22312;&#20849;&#21516;&#22522;&#30784;&#19978;&#20844;&#24179;&#27604;&#36739;&#21644;&#22522;&#20934;&#27979;&#35797;&#36825;&#20123;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks have recently achieved breakthroughs in sound generation with text prompts. Despite their promising performance, current text-to-sound generation models face issues on small-scale datasets (e.g., overfitting), significantly limiting their performance. In this paper, we investigate the use of pre-trained AudioLDM, the state-of-the-art model for text-to-audio generation, as the backbone for sound generation. Our study demonstrates the advantages of using pre-trained models for text-to-sound generation, especially in data-scarcity scenarios. In addition, experiments show that different training strategies (e.g., training conditions) may affect the performance of AudioLDM on datasets of different scales. To facilitate future studies, we also evaluate various text-to-sound generation systems on several frequently used datasets under the same evaluation protocols, which allow fair comparisons and benchmarking of these methods on the common ground.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#27604;&#21464;&#20998;&#20449;&#24687;&#29942;&#39048;&#26694;&#26550;&#65288;CVIB&#65289;&#65292;&#20197;&#20943;&#23569;&#26041;&#38754;&#24773;&#24863;&#20998;&#26512;&#65288;ABSA&#65289;&#20013;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#12290;&#35813;&#26694;&#26550;&#30001;&#19968;&#20010;&#21407;&#22987;&#32593;&#32476;&#21644;&#19968;&#20010;&#33258;&#21098;&#26525;&#32593;&#32476;&#32452;&#25104;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#21516;&#26102;&#36827;&#34892;&#20248;&#21270;&#65292;&#20174;&#32780;&#20002;&#24323;&#20102;&#36755;&#20837;&#29305;&#24449;&#21644;&#39044;&#27979;&#26631;&#31614;&#20043;&#38388;&#30340;&#22810;&#20313;&#27169;&#24335;&#25110;&#34394;&#20551;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.02846</link><description>&lt;p&gt;
&#36890;&#36807;&#21464;&#20998;&#20449;&#24687;&#29942;&#39048;&#21644;&#23545;&#27604;&#23398;&#20064;&#20943;&#23569;&#26041;&#38754;&#24773;&#24863;&#20998;&#26512;&#20013;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
Reducing Spurious Correlations for Aspect-Based Sentiment Analysis with Variational Information Bottleneck and Contrastive Learning. (arXiv:2303.02846v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02846
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#27604;&#21464;&#20998;&#20449;&#24687;&#29942;&#39048;&#26694;&#26550;&#65288;CVIB&#65289;&#65292;&#20197;&#20943;&#23569;&#26041;&#38754;&#24773;&#24863;&#20998;&#26512;&#65288;ABSA&#65289;&#20013;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#12290;&#35813;&#26694;&#26550;&#30001;&#19968;&#20010;&#21407;&#22987;&#32593;&#32476;&#21644;&#19968;&#20010;&#33258;&#21098;&#26525;&#32593;&#32476;&#32452;&#25104;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#21516;&#26102;&#36827;&#34892;&#20248;&#21270;&#65292;&#20174;&#32780;&#20002;&#24323;&#20102;&#36755;&#20837;&#29305;&#24449;&#21644;&#39044;&#27979;&#26631;&#31614;&#20043;&#38388;&#30340;&#22810;&#20313;&#27169;&#24335;&#25110;&#34394;&#20551;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel Contrastive Variational Information Bottleneck framework (CVIB) to reduce spurious correlations for aspect-based sentiment analysis (ABSA). The proposed CVIB framework is composed of an original network and a self-pruned network, and these two networks are optimized simultaneously via contrastive learning, which discards the superfluous patterns or spurious correlations between input features and prediction labels.
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#26041;&#38754;&#24773;&#24863;&#20998;&#26512;&#65288;ABSA&#65289;&#30340;&#25991;&#29486;&#20013;&#21344;&#25454;&#20027;&#23548;&#22320;&#20301;&#65292;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#28145;&#24230;&#27169;&#22411;&#36890;&#24120;&#22312;&#36755;&#20837;&#29305;&#24449;&#21644;&#36755;&#20986;&#26631;&#31614;&#20043;&#38388;&#23384;&#22312;&#34394;&#20551;&#30456;&#20851;&#24615;&#38382;&#39064;&#65292;&#36825;&#20250;&#32473;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#24102;&#26469;&#37325;&#22823;&#38556;&#30861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#21464;&#20998;&#20449;&#24687;&#29942;&#39048;&#26694;&#26550;&#65288;&#31216;&#20026;CVIB&#65289;&#65292;&#20197;&#20943;&#23569;ABSA&#20013;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;CVIB&#26694;&#26550;&#30001;&#19968;&#20010;&#21407;&#22987;&#32593;&#32476;&#21644;&#19968;&#20010;&#33258;&#21098;&#26525;&#32593;&#32476;&#32452;&#25104;&#65292;&#36825;&#20004;&#20010;&#32593;&#32476;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#21516;&#26102;&#36827;&#34892;&#20248;&#21270;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37319;&#29992;&#21464;&#20998;&#20449;&#24687;&#29942;&#39048;&#65288;VIB&#65289;&#21407;&#21017;&#20174;&#21407;&#22987;&#32593;&#32476;&#20013;&#23398;&#20064;&#19968;&#20010;&#20449;&#24687;&#20016;&#23500;&#19988;&#21387;&#32553;&#30340;&#32593;&#32476;&#65288;&#33258;&#21098;&#26525;&#32593;&#32476;&#65289;&#65292;&#35813;&#32593;&#32476;&#20002;&#24323;&#20102;&#36755;&#20837;&#29305;&#24449;&#21644;&#39044;&#27979;&#26631;&#31614;&#20043;&#38388;&#30340;&#22810;&#20313;&#27169;&#24335;&#25110;&#34394;&#20551;&#30456;&#20851;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#33258;&#21098;&#26525;&#23545;&#27604;&#23398;&#20064;&#65292;&#20197;&#23558;&#20004;&#20010;&#32593;&#32476;&#25289;&#22312;&#19968;&#36215;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning techniques have dominated the literature on aspect-based sentiment analysis (ABSA), yielding state-of-the-art results. However, these deep models generally suffer from spurious correlation problems between input features and output labels, which creates significant barriers to robustness and generalization capability. In this paper, we propose a novel Contrastive Variational Information Bottleneck framework (called CVIB) to reduce spurious correlations for ABSA. The proposed CVIB framework is composed of an original network and a self-pruned network, and these two networks are optimized simultaneously via contrastive learning. Concretely, we employ the Variational Information Bottleneck (VIB) principle to learn an informative and compressed network (self-pruned network) from the original network, which discards the superfluous patterns or spurious correlations between input features and prediction labels. Then, self-pruning contrastive learning is devised to pull together
&lt;/p&gt;</description></item><item><title>Prismer&#26159;&#19968;&#31181;&#25968;&#25454;&#21644;&#21442;&#25968;&#39640;&#25928;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#21033;&#29992;&#20102;&#19968;&#32452;&#39046;&#22495;&#19987;&#23478;&#30340;&#38598;&#21512;&#65292;&#36890;&#36807;&#27719;&#38598;&#36825;&#20123;&#19987;&#23478;&#30693;&#35782;&#24182;&#23558;&#20854;&#36866;&#24212;&#20110;&#21508;&#31181;&#35270;&#35273;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#65292;&#23454;&#29616;&#20102;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#27169;&#22411;&#31454;&#20105;&#30340;&#24494;&#35843;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#24615;&#33021;&#65292;&#21516;&#26102;&#38656;&#35201;&#23569;&#33267;&#20004;&#20010;&#25968;&#37327;&#32423;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2303.02506</link><description>&lt;p&gt;
Prismer: &#19968;&#31181;&#20855;&#26377;&#19987;&#23478;&#38598;&#21512;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Prismer: A Vision-Language Model with An Ensemble of Experts. (arXiv:2303.02506v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02506
&lt;/p&gt;
&lt;p&gt;
Prismer&#26159;&#19968;&#31181;&#25968;&#25454;&#21644;&#21442;&#25968;&#39640;&#25928;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#21033;&#29992;&#20102;&#19968;&#32452;&#39046;&#22495;&#19987;&#23478;&#30340;&#38598;&#21512;&#65292;&#36890;&#36807;&#27719;&#38598;&#36825;&#20123;&#19987;&#23478;&#30693;&#35782;&#24182;&#23558;&#20854;&#36866;&#24212;&#20110;&#21508;&#31181;&#35270;&#35273;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#65292;&#23454;&#29616;&#20102;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#27169;&#22411;&#31454;&#20105;&#30340;&#24494;&#35843;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#24615;&#33021;&#65292;&#21516;&#26102;&#38656;&#35201;&#23569;&#33267;&#20004;&#20010;&#25968;&#37327;&#32423;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prismer is a data- and parameter-efficient vision-language model that leverages an ensemble of domain experts, achieving fine-tuned and few-shot learning performance competitive with current state-of-the-art models, whilst requiring up to two orders of magnitude less training data.
&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#22810;&#27169;&#24577;&#29983;&#25104;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36890;&#24120;&#23427;&#20204;&#38656;&#35201;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#24222;&#22823;&#30340;&#27169;&#22411;&#12290;&#20316;&#20026;&#19968;&#31181;&#26356;&#21487;&#25193;&#23637;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Prismer&#65292;&#19968;&#31181;&#25968;&#25454;&#21644;&#21442;&#25968;&#39640;&#25928;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#21033;&#29992;&#20102;&#19968;&#32452;&#39046;&#22495;&#19987;&#23478;&#30340;&#38598;&#21512;&#12290;Prismer&#21482;&#38656;&#35201;&#35757;&#32451;&#23569;&#37327;&#32452;&#20214;&#65292;&#22823;&#37096;&#20998;&#32593;&#32476;&#26435;&#37325;&#20174;&#29616;&#25104;&#30340;&#39044;&#35757;&#32451;&#39046;&#22495;&#19987;&#23478;&#20013;&#32487;&#25215;&#65292;&#24182;&#22312;&#35757;&#32451;&#26399;&#38388;&#20445;&#25345;&#20923;&#32467;&#29366;&#24577;&#12290;&#36890;&#36807;&#21033;&#29992;&#26469;&#33258;&#21508;&#31181;&#39046;&#22495;&#30340;&#19987;&#23478;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Prismer&#21487;&#20197;&#26377;&#25928;&#22320;&#27719;&#38598;&#36825;&#20123;&#19987;&#23478;&#30693;&#35782;&#24182;&#23558;&#20854;&#36866;&#24212;&#20110;&#21508;&#31181;&#35270;&#35273;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Prismer&#23454;&#29616;&#20102;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#27169;&#22411;&#31454;&#20105;&#30340;&#24494;&#35843;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#24615;&#33021;&#65292;&#21516;&#26102;&#38656;&#35201;&#23569;&#33267;&#20004;&#20010;&#25968;&#37327;&#32423;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/NVlabs/prismer&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent vision-language models have shown impressive multi-modal generation capabilities. However, typically they require training huge models on massive datasets. As a more scalable alternative, we introduce Prismer, a data- and parameter-efficient vision-language model that leverages an ensemble of domain experts. Prismer only requires training of a small number of components, with the majority of network weights inherited from readily-available, pre-trained domain experts, and kept frozen during training. By leveraging experts from a wide range of domains, we show that Prismer can efficiently pool this expert knowledge and adapt it to various vision-language reasoning tasks. In our experiments, we show that Prismer achieves fine-tuned and few-shot learning performance which is competitive with current state-of-the-art models, whilst requiring up to two orders of magnitude less training data. Code is available at https://github.com/NVlabs/prismer.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#23398;&#20064;&#24433;&#21709;&#20154;&#31867;&#34892;&#20026;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#20154;&#31867;&#22312;&#21327;&#20316;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.02265</link><description>&lt;p&gt;
&#36890;&#36807;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#23398;&#20064;&#24433;&#21709;&#20154;&#31867;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Learning to Influence Human Behavior with Offline Reinforcement Learning. (arXiv:2303.02265v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02265
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#23398;&#20064;&#24433;&#21709;&#20154;&#31867;&#34892;&#20026;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#20154;&#31867;&#22312;&#21327;&#20316;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a method of learning to influence human behavior through offline reinforcement learning, which can improve human performance in collaborative tasks.
&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#23398;&#20064;&#20195;&#29702;&#19982;&#20154;&#31867;&#20114;&#21160;&#26159;&#26368;&#22797;&#26434;&#30340;&#35774;&#32622;&#20043;&#19968;&#65292;&#22240;&#20026;&#20154;&#31867;&#24448;&#24448;&#30001;&#20110;&#22797;&#26434;&#30340;&#20559;&#35265;&#32780;&#34920;&#29616;&#20986;&#27425;&#20248;&#30340;&#12289;&#19981;&#21487;&#39044;&#27979;&#30340;&#34892;&#20026;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#19982;&#20154;&#31867;&#20114;&#21160;&#30340;&#20195;&#29702;&#26368;&#32456;&#20250;&#24433;&#21709;&#36825;&#20123;&#20154;&#25152;&#37319;&#21462;&#30340;&#34892;&#21160;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20351;&#20195;&#29702;&#33021;&#22815;&#21033;&#29992;&#36825;&#31181;&#24433;&#21709;&#26469;&#25552;&#39640;&#20154;&#31867;&#22312;&#21327;&#20316;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#38543;&#30528;&#20219;&#21153;&#30340;&#23637;&#24320;&#12290;&#19982;&#20197;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#19981;&#20551;&#35774;&#19982;&#20154;&#21592;&#36827;&#34892;&#22312;&#32447;&#22521;&#35757;&#65288;&#36825;&#24448;&#24448;&#22826;&#26114;&#36149;&#21644;&#19981;&#23433;&#20840;&#65289;&#65292;&#20063;&#19981;&#20551;&#35774;&#26377;&#39640;&#20445;&#30495;&#24230;&#29615;&#22659;&#27169;&#25311;&#22120;&#30340;&#35775;&#38382;&#26435;&#38480;&#12290;&#25105;&#20204;&#30340;&#24819;&#27861;&#26159;&#65292;&#36890;&#36807;&#37319;&#29992;&#21508;&#31181;&#20808;&#21069;&#35266;&#23519;&#21040;&#30340;&#20154;&#31867;-&#20154;&#31867;&#20132;&#20114;&#25968;&#25454;&#24182;&#23558;&#20854;&#26631;&#35760;&#20026;&#20219;&#21153;&#22870;&#21169;&#65292;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#21487;&#20197;&#23398;&#20064;&#32452;&#21512;&#34892;&#20026;&#30340;&#32452;&#20214;&#65292;&#24182;&#21457;&#29616;&#23548;&#33268;&#26356;&#29702;&#24819;&#30340;&#20154;&#31867;&#34892;&#20026;&#30340;&#34892;&#21160;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#31163;&#32447;RL&#21487;&#20197;&#23398;&#20064;&#31574;&#30053;&#26469;&#24433;&#21709;&#21644;&#25913;&#21892;&#20154;&#31867;&#34892;&#20026;&#65292;&#23613;&#31649;&#36825;&#20123;&#31574;&#30053;&#21487;&#33021;&#19982;&#20154;&#31867;&#30340;&#26399;&#26395;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the real world, some of the most complex settings for learned agents involve interaction with humans, who often exhibit suboptimal, unpredictable behavior due to sophisticated biases. Agents that interact with people in such settings end up influencing the actions that these people take. Our goal in this work is to enable agents to leverage that influence to improve the human's performance in collaborative tasks, as the task unfolds. Unlike prior work, we do not assume online training with people (which tends to be too expensive and unsafe), nor access to a high fidelity simulator of the environment. Our idea is that by taking a variety of previously observed human-human interaction data and labeling it with the task reward, offline reinforcement learning (RL) can learn to combine components of behavior, and uncover actions that lead to more desirable human actions. First, we show that offline RL can learn strategies to influence and improve human behavior, despite those strategies 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Fisher&#20449;&#24687;&#30340;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#39640;&#25968;&#25454;&#19981;&#30830;&#23450;&#24615;&#26679;&#26412;&#20294;&#27880;&#37322;&#20026;one-hot&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#35777;&#25454;&#23398;&#20064;&#36807;&#31243;&#34987;&#36807;&#24230;&#24809;&#32602;&#24182;&#21463;&#21040;&#38459;&#30861;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.02045</link><description>&lt;p&gt;
&#22522;&#20110;Fisher&#20449;&#24687;&#30340;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Uncertainty Estimation by Fisher Information-based Evidential Deep Learning. (arXiv:2303.02045v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02045
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Fisher&#20449;&#24687;&#30340;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#39640;&#25968;&#25454;&#19981;&#30830;&#23450;&#24615;&#26679;&#26412;&#20294;&#27880;&#37322;&#20026;one-hot&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#35777;&#25454;&#23398;&#20064;&#36807;&#31243;&#34987;&#36807;&#24230;&#24809;&#32602;&#24182;&#21463;&#21040;&#38459;&#30861;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a Fisher Information-based Evidential Deep Learning method to address the problem of over-penalization and hindrance in evidence learning for high data uncertainty samples annotated with one-hot labels.
&lt;/p&gt;
&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26159;&#20351;&#28145;&#24230;&#23398;&#20064;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21487;&#38752;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#26368;&#36817;&#25552;&#20986;&#30340;&#35777;&#25454;&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;&#23558;&#32593;&#32476;&#36755;&#20986;&#35270;&#20026;&#35777;&#25454;&#26469;&#21442;&#25968;&#21270;&#29380;&#21033;&#20811;&#38647;&#20998;&#24067;&#65292;&#26126;&#30830;&#32771;&#34385;&#19981;&#21516;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#22312;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#39640;&#25968;&#25454;&#19981;&#30830;&#23450;&#24615;&#26679;&#26412;&#20294;&#27880;&#37322;&#20026;one-hot&#26631;&#31614;&#30340;&#24773;&#20917;&#65292;&#36825;&#20123;&#38169;&#35823;&#26631;&#35760;&#30340;&#31867;&#21035;&#30340;&#35777;&#25454;&#23398;&#20064;&#36807;&#31243;&#20250;&#34987;&#36807;&#24230;&#24809;&#32602;&#24182;&#21463;&#21040;&#38459;&#30861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;Fisher&#20449;&#24687;&#30340;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#65288;$\mathcal{I}$-EDL&#65289;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#24341;&#20837;Fisher&#20449;&#24687;&#30697;&#38453;&#65288;FIM&#65289;&#26469;&#34913;&#37327;&#27599;&#20010;&#26679;&#26412;&#25152;&#25658;&#24102;&#30340;&#35777;&#25454;&#30340;&#20449;&#24687;&#37327;&#65292;&#26681;&#25454;&#36825;&#20010;&#20449;&#24687;&#37327;&#65292;&#25105;&#20204;&#21487;&#20197;&#21160;&#24577;&#22320;&#37325;&#26032;&#21152;&#26435;&#30446;&#26631;&#25439;&#22833;&#39033;&#65292;&#20351;&#32593;&#32476;&#26356;&#21152;&#19987;&#27880;&#20110;&#19981;&#30830;&#23450;&#31867;&#21035;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#36890;&#36807;&#20248;&#21270;&#36827;&#19968;&#27493;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Uncertainty estimation is a key factor that makes deep learning reliable in practical applications. Recently proposed evidential neural networks explicitly account for different uncertainties by treating the network's outputs as evidence to parameterize the Dirichlet distribution, and achieve impressive performance in uncertainty estimation. However, for high data uncertainty samples but annotated with the one-hot label, the evidence-learning process for those mislabeled classes is over-penalized and remains hindered. To address this problem, we propose a novel method, Fisher Information-based Evidential Deep Learning ($\mathcal{I}$-EDL). In particular, we introduce Fisher Information Matrix (FIM) to measure the informativeness of evidence carried by each sample, according to which we can dynamically reweight the objective loss terms to make the network more focused on the representation learning of uncertain classes. The generalization ability of our network is further improved by opt
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32454;&#31890;&#24230;&#21487;&#25511;&#24773;&#24863;TTS&#65292;&#32771;&#34385;&#20102;&#20869;&#37096;&#21644;&#22806;&#37096;&#31867;&#36317;&#31163;&#65292;&#24182;&#33021;&#22815;&#21512;&#25104;&#20855;&#26377;&#21487;&#35782;&#21035;&#24378;&#24230;&#24046;&#24322;&#30340;&#35821;&#38899;&#12290;</title><link>http://arxiv.org/abs/2303.01508</link><description>&lt;p&gt;
&#25991;&#26412;&#36716;&#35821;&#38899;&#30340;&#32454;&#31890;&#24230;&#24773;&#24863;&#25511;&#21046;&#65306;&#23398;&#20064;&#25490;&#21517;&#20869;&#37096;&#21644;&#22806;&#37096;&#31867;&#24773;&#24863;&#24378;&#24230;
&lt;/p&gt;
&lt;p&gt;
Fine-grained Emotional Control of Text-To-Speech: Learning To Rank Inter- And Intra-Class Emotion Intensities. (arXiv:2303.01508v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01508
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32454;&#31890;&#24230;&#21487;&#25511;&#24773;&#24863;TTS&#65292;&#32771;&#34385;&#20102;&#20869;&#37096;&#21644;&#22806;&#37096;&#31867;&#36317;&#31163;&#65292;&#24182;&#33021;&#22815;&#21512;&#25104;&#20855;&#26377;&#21487;&#35782;&#21035;&#24378;&#24230;&#24046;&#24322;&#30340;&#35821;&#38899;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a fine-grained controllable emotional TTS, that considers both interand intra-class distances and be able to synthesize speech with recognizable intensity difference.
&lt;/p&gt;
&lt;p&gt;
&#26368;&#20808;&#36827;&#30340;&#25991;&#26412;&#36716;&#35821;&#38899;&#65288;TTS&#65289;&#27169;&#22411;&#33021;&#22815;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#35821;&#38899;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#30340;&#35821;&#38899;&#36890;&#24120;&#22312;&#24773;&#24863;&#34920;&#36798;&#19978;&#26159;&#20013;&#24615;&#30340;&#65292;&#32780;&#24456;&#22810;&#26102;&#20505;&#20154;&#20204;&#24076;&#26395;&#23545;&#21333;&#35789;&#25110;&#38899;&#32032;&#36827;&#34892;&#32454;&#31890;&#24230;&#30340;&#24773;&#24863;&#25511;&#21046;&#12290;&#34429;&#28982;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#20294;&#26368;&#36817;&#24050;&#32463;&#25552;&#20986;&#20102;&#31532;&#19968;&#25209;TTS&#27169;&#22411;&#65292;&#33021;&#22815;&#36890;&#36807;&#25163;&#21160;&#20998;&#37197;&#24773;&#24863;&#24378;&#24230;&#26469;&#25511;&#21046;&#35821;&#38899;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#30001;&#20110;&#24573;&#30053;&#20102;&#20869;&#37096;&#31867;&#36317;&#31163;&#65292;&#24378;&#24230;&#24046;&#24322;&#32463;&#24120;&#26080;&#27861;&#35782;&#21035;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32454;&#31890;&#24230;&#21487;&#25511;&#24773;&#24863;TTS&#65292;&#32771;&#34385;&#20102;&#20869;&#37096;&#21644;&#22806;&#37096;&#31867;&#36317;&#31163;&#65292;&#24182;&#33021;&#22815;&#21512;&#25104;&#20855;&#26377;&#21487;&#35782;&#21035;&#24378;&#24230;&#24046;&#24322;&#30340;&#35821;&#38899;&#12290;&#25105;&#20204;&#30340;&#20027;&#35266;&#21644;&#23458;&#35266;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#21487;&#25511;&#24615;&#12289;&#24773;&#24863;&#34920;&#36798;&#21644;&#33258;&#28982;&#24230;&#26041;&#38754;&#36229;&#36807;&#20102;&#20004;&#20010;&#26368;&#20808;&#36827;&#30340;&#21487;&#25511;TTS&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art Text-To-Speech (TTS) models are capable of producing high-quality speech. The generated speech, however, is usually neutral in emotional expression, whereas very often one would want fine-grained emotional control of words or phonemes. Although still challenging, the first TTS models have been recently proposed that are able to control voice by manually assigning emotion intensity. Unfortunately, due to the neglect of intra-class distance, the intensity differences are often unrecognizable. In this paper, we propose a fine-grained controllable emotional TTS, that considers both interand intra-class distances and be able to synthesize speech with recognizable intensity difference. Our subjective and objective experiments demonstrate that our model exceeds two state-of-the-art controllable TTS models for controllability, emotion expressiveness and naturalness.
&lt;/p&gt;</description></item></channel></rss>