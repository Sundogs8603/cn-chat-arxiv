<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;MIDA&#26041;&#27861;&#65292;&#20197;&#22810;&#24037;&#20316;&#27969;&#21487;&#20449;&#24230;&#21644;&#25968;&#25454;&#21487;&#35266;&#27979;&#24615;&#20026;&#22522;&#30784;&#65292;&#23454;&#29616;&#20102;&#36731;&#37327;&#32423;&#30340;&#36816;&#34892;&#26102;&#38598;&#25104;&#25968;&#25454;&#20998;&#26512;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23450;&#20041;&#25968;&#25454;&#21487;&#35266;&#27979;&#31574;&#30053;&#21644;&#36866;&#24212;&#24615;&#26041;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#24322;&#26500;&#31185;&#23398;&#29615;&#22659;&#19979;&#22810;&#20010;&#25903;&#25345;&#24037;&#20855;&#21644;&#39640;&#25928;&#30340;HPC&#25191;&#34892;&#12290;</title><link>http://arxiv.org/abs/2308.09004</link><description>&lt;p&gt;
&#20197;&#22810;&#24037;&#20316;&#27969;&#21487;&#20449;&#24230;&#21644;&#25968;&#25454;&#21487;&#35266;&#27979;&#24615;&#20026;&#22522;&#30784;&#30340;&#36731;&#37327;&#32423;&#25968;&#25454;&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
Towards Lightweight Data Integration using Multi-workflow Provenance and Data Observability. (arXiv:2308.09004v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09004
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;MIDA&#26041;&#27861;&#65292;&#20197;&#22810;&#24037;&#20316;&#27969;&#21487;&#20449;&#24230;&#21644;&#25968;&#25454;&#21487;&#35266;&#27979;&#24615;&#20026;&#22522;&#30784;&#65292;&#23454;&#29616;&#20102;&#36731;&#37327;&#32423;&#30340;&#36816;&#34892;&#26102;&#38598;&#25104;&#25968;&#25454;&#20998;&#26512;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23450;&#20041;&#25968;&#25454;&#21487;&#35266;&#27979;&#31574;&#30053;&#21644;&#36866;&#24212;&#24615;&#26041;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#24322;&#26500;&#31185;&#23398;&#29615;&#22659;&#19979;&#22810;&#20010;&#25903;&#25345;&#24037;&#20855;&#21644;&#39640;&#25928;&#30340;HPC&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#22823;&#35268;&#27169;&#31185;&#23398;&#21457;&#29616;&#38656;&#35201;&#36328;&#22810;&#20010;&#35745;&#31639;&#35774;&#26045;&#36827;&#34892;&#22810;&#23398;&#31185;&#21512;&#20316;&#65292;&#21253;&#25324;&#39640;&#24615;&#33021;&#35745;&#31639;&#65288;HPC&#65289;&#26426;&#22120;&#21644;&#36793;&#32536;&#21040;&#20113;&#30340;&#36830;&#32493;&#20307;&#12290;&#38598;&#25104;&#30340;&#25968;&#25454;&#20998;&#26512;&#22312;&#31185;&#23398;&#21457;&#29616;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#24403;&#21069;&#20154;&#24037;&#26234;&#33021;&#26102;&#20195;&#65292;&#36890;&#36807;&#25903;&#25345;&#36127;&#36131;&#20219;&#30340;AI&#24320;&#21457;&#12289;FAIR&#12289;&#21487;&#22797;&#29616;&#24615;&#21644;&#29992;&#25143;&#25805;&#25511;&#12290;&#28982;&#32780;&#65292;&#31185;&#23398;&#30340;&#24322;&#26500;&#24615;&#24102;&#26469;&#20102;&#25361;&#25112;&#65292;&#22914;&#22788;&#29702;&#22810;&#20010;&#25903;&#25345;&#24037;&#20855;&#12289;&#36328;&#35774;&#26045;&#29615;&#22659;&#21644;&#39640;&#25928;&#30340;HPC&#25191;&#34892;&#12290;&#22312;&#25968;&#25454;&#21487;&#35266;&#27979;&#24615;&#12289;&#36866;&#37197;&#22120;&#31995;&#32479;&#35774;&#35745;&#21644;&#21487;&#20449;&#24230;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MIDA&#65306;&#19968;&#31181;&#36731;&#37327;&#32423;&#36816;&#34892;&#26102;&#22810;&#24037;&#20316;&#27969;&#38598;&#25104;&#25968;&#25454;&#20998;&#26512;&#30340;&#26041;&#27861;&#12290;MIDA&#23450;&#20041;&#20102;&#21508;&#31181;&#24182;&#34892;&#31995;&#32479;&#21644;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#30340;&#25968;&#25454;&#21487;&#35266;&#27979;&#31574;&#30053;&#21644;&#36866;&#24212;&#24615;&#26041;&#27861;&#12290;&#36890;&#36807;&#21487;&#35266;&#27979;&#24615;&#65292;&#23427;&#22312;&#21518;&#21488;&#25318;&#25130;&#25968;&#25454;&#27969;&#65292;&#26080;&#38656;&#20202;&#22120;&#21270;&#65292;&#21516;&#26102;&#38598;&#25104;&#39046;&#22495;&#12289;&#21487;&#20449;&#24230;&#21644;&#36965;&#27979;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern large-scale scientific discovery requires multidisciplinary collaboration across diverse computing facilities, including High Performance Computing (HPC) machines and the Edge-to-Cloud continuum. Integrated data analysis plays a crucial role in scientific discovery, especially in the current AI era, by enabling Responsible AI development, FAIR, Reproducibility, and User Steering. However, the heterogeneous nature of science poses challenges such as dealing with multiple supporting tools, cross-facility environments, and efficient HPC execution. Building on data observability, adapter system design, and provenance, we propose MIDA: an approach for lightweight runtime Multi-workflow Integrated Data Analysis. MIDA defines data observability strategies and adaptability methods for various parallel systems and machine learning tools. With observability, it intercepts the dataflows in the background without requiring instrumentation while integrating domain, provenance, and telemetry 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#34892;&#20026;&#26641;&#25511;&#21046;&#22120;&#30340;&#25910;&#25947;&#24615;&#65292;&#24191;&#27867;&#25512;&#24191;&#20102;&#26089;&#26399;&#30340;&#32467;&#26524;&#65292;&#24182;&#21253;&#25324;&#20102;&#25991;&#29486;&#20013;&#26410;&#28085;&#30422;&#30340;&#26032;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2308.08994</link><description>&lt;p&gt;
&#34892;&#20026;&#26641;&#25511;&#21046;&#22120;&#30340;&#24310;&#20280;&#25910;&#25947;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
An Extended Convergence Result for Behaviour Tree Controllers. (arXiv:2308.08994v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08994
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#34892;&#20026;&#26641;&#25511;&#21046;&#22120;&#30340;&#25910;&#25947;&#24615;&#65292;&#24191;&#27867;&#25512;&#24191;&#20102;&#26089;&#26399;&#30340;&#32467;&#26524;&#65292;&#24182;&#21253;&#25324;&#20102;&#25991;&#29486;&#20013;&#26410;&#28085;&#30422;&#30340;&#26032;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34892;&#20026;&#26641;&#65288;BTs&#65289;&#26159;&#19968;&#31181;&#36890;&#36807;&#26641;&#32467;&#26500;&#20174;&#19968;&#32452;&#20302;&#32423;&#25511;&#21046;&#31574;&#30053;&#32452;&#21512;&#25104;&#20998;&#23618;&#28151;&#21512;&#25511;&#21046;&#31574;&#30053;&#30340;&#26368;&#20248;&#27169;&#22359;&#21270;&#26694;&#26550;&#12290;&#35768;&#22810;&#26426;&#22120;&#20154;&#20219;&#21153;&#33258;&#28982;&#22320;&#20998;&#35299;&#20026;&#25511;&#21046;&#20219;&#21153;&#30340;&#23618;&#27425;&#32467;&#26500;&#65292;&#24182;&#19988;&#27169;&#22359;&#21270;&#26159;&#22788;&#29702;&#22797;&#26434;&#24615;&#30340;&#20247;&#25152;&#21608;&#30693;&#30340;&#24037;&#20855;&#65292;&#22240;&#27492;&#34892;&#20026;&#26641;&#22312;&#26426;&#22120;&#20154;&#31038;&#21306;&#24191;&#27867;&#20351;&#29992;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;BT&#30340;&#25910;&#25947;&#24615;&#65292;&#21363;&#36798;&#21040;&#29366;&#24577;&#31354;&#38388;&#30340;&#26399;&#26395;&#37096;&#20998;&#12290;&#20808;&#21069;&#20851;&#20110;BT&#25910;&#25947;&#30340;&#32467;&#26524;&#36890;&#24120;&#38024;&#23545;&#20351;&#29992;&#19981;&#21516;&#35774;&#35745;&#21407;&#21017;&#21019;&#24314;&#30340;&#29305;&#23450;BT&#26063;&#32676;&#36827;&#34892;&#20102;&#35843;&#25972;&#12290;&#26412;&#25991;&#30340;&#32467;&#26524;&#25512;&#24191;&#20102;&#20197;&#21069;&#30340;&#32467;&#26524;&#65292;&#36824;&#21253;&#25324;&#20102;&#25991;&#29486;&#20013;&#26410;&#28085;&#30422;&#30340;&#24490;&#29615;&#20999;&#25442;&#30340;&#26032;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Behavior trees (BTs) are an optimally modular framework to assemble hierarchical hybrid control policies from a set of low-level control policies using a tree structure. Many robotic tasks are naturally decomposed into a hierarchy of control tasks, and modularity is a well-known tool for handling complexity, therefor behavior trees have garnered widespread usage in the robotics community. In this paper, we study the convergence of BTs, in the sense of reaching a desired part of the state space. Earlier results on BT convergence were often tailored to specific families of BTs, created using different design principles. The results of this paper generalize the earlier results and also include new cases of cyclic switching not covered in the literature.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#35270;&#35282;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#12290;&#36890;&#36807;&#35266;&#23519;&#25200;&#21160;&#24402;&#22240;&#29305;&#24449;&#23545;&#27169;&#22411;&#34892;&#20026;&#30340;&#24433;&#21709;&#65292;&#36825;&#31181;&#26041;&#27861;&#25581;&#31034;&#20102;&#24402;&#22240;&#29305;&#24449;&#30340;&#20934;&#30830;&#24615;&#21644;&#23436;&#25972;&#24615;&#65292;&#20351;&#20854;&#33021;&#22815;&#23450;&#37327;&#35780;&#20272;&#29305;&#24449;&#24402;&#22240;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2308.08949</link><description>&lt;p&gt;
&#19968;&#31181;&#21452;&#37325;&#35270;&#35282;&#35780;&#20272;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Dual-Perspective Approach to Evaluating Feature Attribution Methods. (arXiv:2308.08949v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08949
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#35270;&#35282;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#12290;&#36890;&#36807;&#35266;&#23519;&#25200;&#21160;&#24402;&#22240;&#29305;&#24449;&#23545;&#27169;&#22411;&#34892;&#20026;&#30340;&#24433;&#21709;&#65292;&#36825;&#31181;&#26041;&#27861;&#25581;&#31034;&#20102;&#24402;&#22240;&#29305;&#24449;&#30340;&#20934;&#30830;&#24615;&#21644;&#23436;&#25972;&#24615;&#65292;&#20351;&#20854;&#33021;&#22815;&#23450;&#37327;&#35780;&#20272;&#29305;&#24449;&#24402;&#22240;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#35797;&#22270;&#36890;&#36807;&#35782;&#21035;&#30456;&#20851;&#29305;&#24449;&#26469;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#24314;&#31435;&#19968;&#20010;&#35780;&#20272;&#29305;&#24449;&#24402;&#22240;&#30340;&#32479;&#19968;&#26694;&#26550;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#20960;&#20010;&#35270;&#35282;&#26469;&#35780;&#20272;&#29305;&#24449;&#24402;&#22240;&#12290;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#35270;&#35282;&#26159;&#35266;&#23519;&#25200;&#21160;&#24402;&#22240;&#29305;&#24449;&#23545;&#27169;&#22411;&#34892;&#20026;&#30340;&#24433;&#21709;&#65288;&#21363;&#24544;&#23454;&#24230;&#65289;&#12290;&#23613;&#31649;&#25552;&#20379;&#20102;&#26377;&#29992;&#30340;&#27934;&#35265;&#65292;&#20294;&#29616;&#26377;&#30340;&#24544;&#23454;&#24230;&#35780;&#20272;&#23384;&#22312;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#25581;&#31034;&#30340;&#32570;&#28857;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24544;&#23454;&#24230;&#33539;&#24335;&#20869;&#30340;&#20004;&#20010;&#26032;&#35270;&#35282;&#65292;&#25581;&#31034;&#20102;&#30452;&#35266;&#30340;&#23646;&#24615;&#65306;&#27491;&#30830;&#24615;&#21644;&#23436;&#25972;&#24615;&#12290;&#27491;&#30830;&#24615;&#35780;&#20272;&#24402;&#22240;&#29305;&#24449;&#30495;&#27491;&#26159;&#39044;&#27979;&#24615;&#29305;&#24449;&#30340;&#31243;&#24230;&#65292;&#32780;&#23436;&#25972;&#24615;&#26816;&#26597;&#25152;&#24471;&#24402;&#22240;&#22914;&#20309;&#24456;&#22909;&#22320;&#25581;&#31034;&#25152;&#26377;&#39044;&#27979;&#24615;&#29305;&#24449;&#12290;&#36825;&#20004;&#20010;&#35270;&#35282;&#22522;&#20110;&#22362;&#23454;&#30340;&#25968;&#23398;&#22522;&#30784;&#65292;&#24182;&#25552;&#20379;&#20102;&#36890;&#36807;&#39640;&#25928;&#31639;&#27861;&#35745;&#31639;&#30340;&#23450;&#37327;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Feature attribution methods attempt to explain neural network predictions by identifying relevant features. However, establishing a cohesive framework for assessing feature attribution remains a challenge. There are several views through which we can evaluate attributions. One principal lens is to observe the effect of perturbing attributed features on the model's behavior (i.e., faithfulness). While providing useful insights, existing faithfulness evaluations suffer from shortcomings that we reveal in this paper. In this work, we propose two new perspectives within the faithfulness paradigm that reveal intuitive properties: soundness and completeness. Soundness assesses the degree to which attributed features are truly predictive features, while completeness examines how well the resulting attribution reveals all the predictive features. The two perspectives are based on a firm mathematical foundation and provide quantitative metrics that are computable through efficient algorithms. W
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#20892;&#20316;&#29289;&#20135;&#37327;&#39044;&#27979;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26089;&#26399;&#34701;&#21512;&#26041;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#20855;&#26377;&#19981;&#21516;&#26102;&#38388;&#21644;&#31354;&#38388;&#20998;&#36776;&#29575;&#30340;&#22810;&#20010;&#36755;&#20837;&#27169;&#24577;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#39640;&#20998;&#36776;&#29575;&#30340;&#20892;&#20316;&#29289;&#20135;&#37327;&#22320;&#22270;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#37319;&#29992;&#20102;&#20892;&#20316;&#29289;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26080;&#20851;&#30340;&#26041;&#27861;&#36827;&#34892;&#20122;&#30000;&#32423;&#21035;&#30340;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#20840;&#29699;&#35206;&#30422;&#30340;&#36755;&#20837;&#27169;&#24577;&#65292;&#24182;&#24378;&#35843;&#20102;&#36755;&#20837;&#27169;&#24577;&#23545;&#20110;&#20135;&#37327;&#39044;&#27979;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.08948</link><description>&lt;p&gt;
&#29992;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#20892;&#20316;&#29289;&#20135;&#37327;&#65306;&#22312;&#30000;&#22320;&#21644;&#20122;&#30000;&#32423;&#21035;&#19978;&#23545;&#36755;&#20837;&#27169;&#24577;&#21644;&#27169;&#22411;&#30340;&#24191;&#27867;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Predicting Crop Yield With Machine Learning: An Extensive Analysis Of Input Modalities And Models On a Field and sub-field Level. (arXiv:2308.08948v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08948
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#20892;&#20316;&#29289;&#20135;&#37327;&#39044;&#27979;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26089;&#26399;&#34701;&#21512;&#26041;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#20855;&#26377;&#19981;&#21516;&#26102;&#38388;&#21644;&#31354;&#38388;&#20998;&#36776;&#29575;&#30340;&#22810;&#20010;&#36755;&#20837;&#27169;&#24577;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#39640;&#20998;&#36776;&#29575;&#30340;&#20892;&#20316;&#29289;&#20135;&#37327;&#22320;&#22270;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#37319;&#29992;&#20102;&#20892;&#20316;&#29289;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26080;&#20851;&#30340;&#26041;&#27861;&#36827;&#34892;&#20122;&#30000;&#32423;&#21035;&#30340;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#20840;&#29699;&#35206;&#30422;&#30340;&#36755;&#20837;&#27169;&#24577;&#65292;&#24182;&#24378;&#35843;&#20102;&#36755;&#20837;&#27169;&#24577;&#23545;&#20110;&#20135;&#37327;&#39044;&#27979;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26089;&#26399;&#34701;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#20855;&#26377;&#19981;&#21516;&#26102;&#38388;&#21644;&#31354;&#38388;&#20998;&#36776;&#29575;&#30340;&#22810;&#20010;&#36755;&#20837;&#27169;&#24577;&#30340;&#20892;&#20316;&#29289;&#20135;&#37327;&#39044;&#27979;&#12290;&#25105;&#20204;&#20351;&#29992;&#39640;&#20998;&#36776;&#29575;&#30340;&#20892;&#20316;&#29289;&#20135;&#37327;&#22320;&#22270;&#20316;&#20026;&#35757;&#32451;&#25968;&#25454;&#65292;&#22312;&#20122;&#30000;&#32423;&#21035;&#19978;&#20351;&#29992;&#20892;&#20316;&#29289;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26080;&#20851;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;Sentinel-2&#21355;&#26143;&#22270;&#20687;&#20316;&#20026;&#20027;&#35201;&#30340;&#36755;&#20837;&#25968;&#25454;&#27169;&#24577;&#65292;&#20197;&#21450;&#20854;&#20182;&#34917;&#20805;&#30340;&#27169;&#24577;&#65292;&#21253;&#25324;&#22825;&#27668;&#12289;&#22303;&#22756;&#21644;DEM&#25968;&#25454;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#29992;&#20102;&#20855;&#26377;&#20840;&#29699;&#35206;&#30422;&#33539;&#22260;&#30340;&#36755;&#20837;&#27169;&#24577;&#65292;&#20351;&#35813;&#26694;&#26550;&#20855;&#26377;&#20840;&#29699;&#20280;&#32553;&#24615;&#12290;&#25105;&#20204;&#26126;&#30830;&#24378;&#35843;&#20102;&#36755;&#20837;&#27169;&#24577;&#23545;&#20110;&#20892;&#20316;&#29289;&#20135;&#37327;&#39044;&#27979;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#24378;&#35843;&#20102;&#26368;&#20339;&#32452;&#21512;&#30340;&#36755;&#20837;&#27169;&#24577;&#21462;&#20915;&#20110;&#22320;&#21306;&#12289;&#20316;&#29289;&#21644;&#36873;&#25321;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a simple yet effective early fusion method for crop yield prediction that handles multiple input modalities with different temporal and spatial resolutions. We use high-resolution crop yield maps as ground truth data to train crop and machine learning model agnostic methods at the sub-field level. We use Sentinel-2 satellite imagery as the primary modality for input data with other complementary modalities, including weather, soil, and DEM data. The proposed method uses input modalities available with global coverage, making the framework globally scalable. We explicitly highlight the importance of input modalities for crop yield prediction and emphasize that the best-performing combination of input modalities depends on region, crop, and chosen model.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;IGNNet&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22788;&#29702;&#34920;&#26684;&#25968;&#25454;&#26102;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#65292;&#20174;&#21407;&#22987;&#36755;&#20837;&#29305;&#24449;&#31934;&#30830;&#35745;&#31639;&#39044;&#27979;&#32467;&#26524;&#65292;&#24182;&#19988;&#22312;&#24615;&#33021;&#19978;&#19982;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#24615;&#33021;&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2308.08945</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#26684;&#25968;&#25454;&#22788;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Interpretable Graph Neural Networks for Tabular Data. (arXiv:2308.08945v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08945
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;IGNNet&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22788;&#29702;&#34920;&#26684;&#25968;&#25454;&#26102;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#65292;&#20174;&#21407;&#22987;&#36755;&#20837;&#29305;&#24449;&#31934;&#30830;&#35745;&#31639;&#39044;&#27979;&#32467;&#26524;&#65292;&#24182;&#19988;&#22312;&#24615;&#33021;&#19978;&#19982;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#24615;&#33021;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;&#34920;&#26684;&#26684;&#24335;&#30340;&#25968;&#25454;&#32463;&#24120;&#20986;&#29616;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#36817;&#26399;&#34987;&#25193;&#23637;&#20197;&#26377;&#25928;&#22788;&#29702;&#27492;&#31867;&#25968;&#25454;&#65292;&#36890;&#36807;&#34920;&#31034;&#23398;&#20064;&#25429;&#25417;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#26412;&#36136;&#19978;&#20135;&#29983;&#20102;&#40657;&#30418;&#27169;&#22411;&#65292;&#20197;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24418;&#24335;&#23384;&#22312;&#65292;&#20351;&#24471;&#29992;&#25143;&#26080;&#27861;&#29702;&#35299;&#27169;&#22411;&#39044;&#27979;&#30340;&#36923;&#36753;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;IGNNet&#65288;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#34920;&#26684;&#25968;&#25454;&#22788;&#29702;&#26041;&#27861;&#65289;&#30340;&#26041;&#27861;&#65292;&#23427;&#38480;&#21046;&#23398;&#20064;&#31639;&#27861;&#20197;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#23637;&#31034;&#20102;&#22914;&#20309;&#20174;&#21407;&#22987;&#36755;&#20837;&#29305;&#24449;&#20934;&#30830;&#35745;&#31639;&#39044;&#27979;&#32467;&#26524;&#12290;&#36890;&#36807;&#22823;&#35268;&#27169;&#23454;&#35777;&#30740;&#31350;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;IGNNet&#19982;&#38754;&#21521;&#34920;&#26684;&#25968;&#25454;&#30340;&#26368;&#20808;&#36827;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65288;&#21253;&#25324;XGBoost&#65292;Random Forests&#21644;TabNet&#65289;&#24615;&#33021;&#30456;&#24403;&#12290;&#21516;&#26102;&#65292;&#32467;&#26524;&#26174;&#31034;&#20174;IGNNet&#33719;&#24471;&#30340;&#35299;&#37322;&#19982;&#30495;&#23454;&#24773;&#20917;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data in tabular format is frequently occurring in real-world applications. Graph Neural Networks (GNNs) have recently been extended to effectively handle such data, allowing feature interactions to be captured through representation learning. However, these approaches essentially produce black-box models, in the form of deep neural networks, precluding users from following the logic behind the model predictions. We propose an approach, called IGNNet (Interpretable Graph Neural Network for tabular data), which constrains the learning algorithm to produce an interpretable model, where the model shows how the predictions are exactly computed from the original input features. A large-scale empirical investigation is presented, showing that IGNNet is performing on par with state-of-the-art machine-learning algorithms that target tabular data, including XGBoost, Random Forests, and TabNet. At the same time, the results show that the explanations obtained from IGNNet are aligned with the true
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#33258;&#21160;&#21270;&#20607;&#36824;&#25216;&#26415;&#20538;&#21153;&#30340;&#28508;&#21147;&#65292;&#24182;&#25506;&#32034;&#20102;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#27169;&#22411;&#23454;&#29616;&#35813;&#30446;&#26631;&#30340;&#26041;&#27861;&#12290;&#23454;&#35777;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#31574;&#30053;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#22312;&#19981;&#21516;&#39033;&#30446;&#20013;&#26377;&#25928;&#22320;&#35299;&#20915;&#33258;&#25215;&#35748;&#25216;&#26415;&#20538;&#21153;&#12290;</title><link>http://arxiv.org/abs/2308.08943</link><description>&lt;p&gt;
&#33258;&#21160;&#35299;&#20915;&#33258;&#25215;&#35748;&#25216;&#26415;&#20538;&#21153;&#30340;&#25506;&#32034;&#65306;&#25105;&#20204;&#31163;&#30446;&#26631;&#26377;&#22810;&#36828;&#65311;
&lt;/p&gt;
&lt;p&gt;
Towards Automatically Addressing Self-Admitted Technical Debt: How Far Are We?. (arXiv:2308.08943v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08943
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#33258;&#21160;&#21270;&#20607;&#36824;&#25216;&#26415;&#20538;&#21153;&#30340;&#28508;&#21147;&#65292;&#24182;&#25506;&#32034;&#20102;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#27169;&#22411;&#23454;&#29616;&#35813;&#30446;&#26631;&#30340;&#26041;&#27861;&#12290;&#23454;&#35777;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#31574;&#30053;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#22312;&#19981;&#21516;&#39033;&#30446;&#20013;&#26377;&#25928;&#22320;&#35299;&#20915;&#33258;&#25215;&#35748;&#25216;&#26415;&#20538;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36719;&#20214;&#21457;&#23637;&#36807;&#31243;&#20013;&#65292;&#32452;&#32455;&#21644;&#20010;&#20154;&#24320;&#21457;&#32773;&#38656;&#35201;&#33457;&#36153;&#22823;&#37327;&#30340;&#31934;&#21147;&#26469;&#36824;&#28165;&#25216;&#26415;&#20538;&#21153;&#65292;&#21363;&#36719;&#20214;&#21457;&#24067;&#26102;&#23384;&#22312;&#30340;&#19981;&#23436;&#21892;&#65292;&#20363;&#22914;&#21151;&#33021;&#12289;&#21487;&#38752;&#24615;&#25110;&#21487;&#32500;&#25252;&#24615;&#26041;&#38754;&#12290;&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#26469;&#25506;&#35752;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#27169;&#22411;&#30340;&#33258;&#21160;&#21270;&#25216;&#26415;&#20538;&#21153;&#20607;&#36824;&#31243;&#24230;&#65292;&#29305;&#21035;&#26159;&#21033;&#29992;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#31574;&#30053;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#39318;&#20808;&#20174;595&#20010;&#24320;&#28304;&#39033;&#30446;&#20013;&#25552;&#21462;&#20102;5,039&#20010;&#33258;&#25215;&#35748;&#25216;&#26415;&#20538;&#21153;&#65288;SATD&#65289;&#30340;&#31227;&#38500;&#23454;&#20363;&#12290;SATD&#26159;&#25351;&#24320;&#21457;&#20154;&#21592;&#36890;&#36807;&#20195;&#30721;&#27880;&#37322;&#31561;&#24418;&#24335;&#26469;&#35760;&#24405;&#30340;&#25216;&#26415;&#20538;&#21153;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20010;&#25968;&#25454;&#38598;&#26469;&#23454;&#39564;&#19971;&#20010;&#19981;&#21516;&#30340;&#29983;&#25104;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#37197;&#32622;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#19981;&#21516;&#32452;&#21512;&#30340;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#30446;&#26631;&#65292;&#21253;&#25324;&#20462;&#22797;&#36890;&#29992;&#20195;&#30721;&#26356;&#25913;&#65292;SATD&#31227;&#38500;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Upon evolving their software, organizations and individual developers have to spend a substantial effort to pay back technical debt, i.e., the fact that software is released in a shape not as good as it should be, e.g., in terms of functionality, reliability, or maintainability. This paper empirically investigates the extent to which technical debt can be automatically paid back by neural-based generative models, and in particular models exploiting different strategies for pre-training and fine-tuning. We start by extracting a dateset of 5,039 Self-Admitted Technical Debt (SATD) removals from 595 open-source projects. SATD refers to technical debt instances documented (e.g., via code comments) by developers. We use this dataset to experiment with seven different generative deep learning (DL) model configurations. Specifically, we compare transformers pre-trained and fine-tuned with different combinations of training objectives, including the fixing of generic code changes, SATD removal
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22522;&#20110;&#23545;&#27604;&#25439;&#22833;&#30340;&#31163;&#32447;&#25163;&#20889;&#31614;&#21517;&#39564;&#35777;&#27169;&#22411;&#30340;&#30333;&#30418;&#35823;&#25253;&#23545;&#25239;&#25915;&#20987;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25915;&#20987;&#35270;&#20026;&#20070;&#20889;&#39118;&#26684;&#20043;&#38388;&#30340;&#39118;&#26684;&#36716;&#25442;&#65292;&#24341;&#20837;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#29983;&#25104;&#27450;&#39575;&#24615;&#22270;&#20687;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.08925</link><description>&lt;p&gt;
&#22522;&#20110;&#23545;&#27604;&#25439;&#22833;&#30340;&#31163;&#32447;&#25163;&#20889;&#31614;&#21517;&#39564;&#35777;&#27169;&#22411;&#30340;&#30333;&#30418;&#35823;&#25253;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A White-Box False Positive Adversarial Attack Method on Contrastive Loss-Based Offline Handwritten Signature Verification Models. (arXiv:2308.08925v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08925
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22522;&#20110;&#23545;&#27604;&#25439;&#22833;&#30340;&#31163;&#32447;&#25163;&#20889;&#31614;&#21517;&#39564;&#35777;&#27169;&#22411;&#30340;&#30333;&#30418;&#35823;&#25253;&#23545;&#25239;&#25915;&#20987;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25915;&#20987;&#35270;&#20026;&#20070;&#20889;&#39118;&#26684;&#20043;&#38388;&#30340;&#39118;&#26684;&#36716;&#25442;&#65292;&#24341;&#20837;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#29983;&#25104;&#27450;&#39575;&#24615;&#22270;&#20687;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#22522;&#20110;&#23545;&#27604;&#25439;&#22833;&#30340;&#31163;&#32447;&#25163;&#20889;&#31614;&#21517;&#39564;&#35777;&#27169;&#22411;&#30340;&#30333;&#30418;&#35823;&#25253;&#23545;&#25239;&#25915;&#20987;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#23558;&#25915;&#20987;&#35270;&#20026;&#22312;&#23494;&#20999;&#30456;&#20851;&#20294;&#19981;&#21516;&#30340;&#20070;&#20889;&#39118;&#26684;&#20043;&#38388;&#36827;&#34892;&#39118;&#26684;&#36716;&#25442;&#12290;&#20026;&#20102;&#24341;&#23548;&#27450;&#39575;&#24615;&#22270;&#20687;&#30340;&#29983;&#25104;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#25200;&#21160;&#21407;&#22987;&#26679;&#26412;&#19982;&#21512;&#25104;&#26679;&#26412;&#30340;&#23884;&#20837;&#21521;&#37327;&#20043;&#38388;&#30340;&#27431;&#27663;&#36317;&#31163;&#26469;&#25552;&#39640;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#21516;&#26102;&#36890;&#36807;&#20943;&#23567;&#29983;&#25104;&#22270;&#20687;&#19982;&#21407;&#22987;&#22270;&#20687;&#20043;&#38388;&#30340;&#24046;&#24322;&#26469;&#20445;&#35777;&#26368;&#23567;&#30340;&#25200;&#21160;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23545;&#27604;&#25439;&#22833;&#30340;&#31163;&#32447;&#25163;&#20889;&#31614;&#21517;&#39564;&#35777;&#27169;&#22411;&#30340;&#30333;&#30418;&#25915;&#20987;&#20013;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#30340;&#20851;&#38190;&#36129;&#29486;&#21253;&#25324;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35823;&#25253;&#25915;&#20987;&#26041;&#27861;&#12289;&#20004;&#20010;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#12289;&#26377;&#25928;&#30340;&#20070;&#20889;&#39118;&#26684;&#36716;&#25442;&#20197;&#21450;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we tackle the challenge of white-box false positive adversarial attacks on contrastive loss-based offline handwritten signature verification models. We propose a novel attack method that treats the attack as a style transfer between closely related but distinct writing styles. To guide the generation of deceptive images, we introduce two new loss functions that enhance the attack success rate by perturbing the Euclidean distance between the embedding vectors of the original and synthesized samples, while ensuring minimal perturbations by reducing the difference between the generated image and the original image. Our method demonstrates state-of-the-art performance in white-box attacks on contrastive loss-based offline handwritten signature verification models, as evidenced by our experiments. The key contributions of this paper include a novel false positive attack method, two new loss functions, effective style transfer in handwriting styles, and superior performance in
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#27979;&#34920;&#31034;&#23398;&#20064;&#30340;&#27169;&#20223;&#22686;&#24378;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#33258;&#21160;&#20570;&#24066;&#20013;&#24212;&#29992;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20511;&#37492;&#19987;&#19994;&#20154;&#31867;&#20570;&#24066;&#21830;&#30340;&#24037;&#20316;&#27969;&#31243;&#65292;&#32467;&#21512;&#23376;&#20248;&#20449;&#21495;&#19987;&#23478;&#30340;&#30693;&#35782;&#21644;&#30452;&#25509;&#31574;&#30053;&#20132;&#20114;&#65292;&#24320;&#21457;&#20102;&#36866;&#29992;&#20110;&#22810;&#20215;&#26684;&#27700;&#24179;&#30340;&#20570;&#24066;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2308.08918</link><description>&lt;p&gt;
&#22522;&#20110;&#39044;&#27979;&#34920;&#31034;&#23398;&#20064;&#30340;&#27169;&#20223;&#22686;&#24378;&#23398;&#20064;&#26041;&#27861;&#22312;&#33258;&#21160;&#20570;&#24066;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
IMM: An Imitative Reinforcement Learning Approach with Predictive Representation Learning for Automatic Market Making. (arXiv:2308.08918v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08918
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#27979;&#34920;&#31034;&#23398;&#20064;&#30340;&#27169;&#20223;&#22686;&#24378;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#33258;&#21160;&#20570;&#24066;&#20013;&#24212;&#29992;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20511;&#37492;&#19987;&#19994;&#20154;&#31867;&#20570;&#24066;&#21830;&#30340;&#24037;&#20316;&#27969;&#31243;&#65292;&#32467;&#21512;&#23376;&#20248;&#20449;&#21495;&#19987;&#23478;&#30340;&#30693;&#35782;&#21644;&#30452;&#25509;&#31574;&#30053;&#20132;&#20114;&#65292;&#24320;&#21457;&#20102;&#36866;&#29992;&#20110;&#22810;&#20215;&#26684;&#27700;&#24179;&#30340;&#20570;&#24066;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20570;&#24066;&#65288;MM&#65289;&#22312;&#37329;&#34701;&#20132;&#26131;&#20013;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#22312;&#30830;&#20445;&#24066;&#22330;&#27969;&#21160;&#24615;&#26041;&#38754;&#20855;&#26377;&#33267;&#20851;&#37325;&#35201;&#30340;&#21151;&#33021;&#12290;&#22312;&#39034;&#24207;&#20915;&#31574;&#26041;&#38754;&#20855;&#26377;&#24378;&#22823;&#33021;&#21147;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#25216;&#26415;&#22312;&#37327;&#21270;&#20132;&#26131;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#22522;&#20110;RL&#30340;MM&#26041;&#27861;&#37117;&#19987;&#27880;&#20110;&#20248;&#21270;&#21333;&#19968;&#20215;&#26684;&#27700;&#24179;&#31574;&#30053;&#65292;&#32780;&#23545;&#20110;&#39057;&#32321;&#25764;&#38144;&#35746;&#21333;&#21644;&#20002;&#22833;&#38431;&#21015;&#20248;&#20808;&#32423;&#31561;&#38382;&#39064;&#26080;&#27861;&#35299;&#20915;&#12290;&#28041;&#21450;&#22810;&#20010;&#20215;&#26684;&#27700;&#24179;&#30340;&#31574;&#30053;&#26356;&#31526;&#21512;&#23454;&#38469;&#20132;&#26131;&#22330;&#26223;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22810;&#20215;&#26684;&#27700;&#24179;&#31574;&#30053;&#28041;&#21450;&#21040;&#20840;&#38754;&#30340;&#20132;&#26131;&#34892;&#20026;&#31354;&#38388;&#30340;&#22797;&#26434;&#24615;&#65292;&#26377;&#25928;&#35757;&#32451;&#30408;&#21033;&#30340;RL&#20195;&#29702;&#20154;&#22312;MM&#26041;&#38754;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#21463;&#21040;&#19987;&#19994;&#20570;&#24066;&#21830;&#39640;&#25928;&#24037;&#20316;&#27969;&#31243;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;RL&#26694;&#26550;&#65292;&#21363;&#27169;&#20223;&#24066;&#22330;&#20570;&#24066;&#21830;&#65288;IMM&#65289;&#65292;&#23427;&#21033;&#29992;&#23376;&#20248;&#20449;&#21495;&#19987;&#23478;&#30340;&#30693;&#35782;&#21644;&#30452;&#25509;&#31574;&#30053;&#20132;&#20114;&#30340;&#26041;&#24335;&#26469;&#24320;&#21457;&#22810;&#20215;&#26684;&#27700;&#24179;&#30340;MM&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Market making (MM) has attracted significant attention in financial trading owing to its essential function in ensuring market liquidity. With strong capabilities in sequential decision-making, Reinforcement Learning (RL) technology has achieved remarkable success in quantitative trading. Nonetheless, most existing RL-based MM methods focus on optimizing single-price level strategies which fail at frequent order cancellations and loss of queue priority. Strategies involving multiple price levels align better with actual trading scenarios. However, given the complexity that multi-price level strategies involves a comprehensive trading action space, the challenge of effectively training profitable RL agents for MM persists. Inspired by the efficient workflow of professional human market makers, we propose Imitative Market Maker (IMM), a novel RL framework leveraging both knowledge from suboptimal signal-based experts and direct policy interactions to develop multi-price level MM strategi
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20914;&#31361;&#24863;&#30693;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#20026;&#27599;&#20010;&#25351;&#26631;&#25552;&#20379;&#29420;&#29305;&#30340;&#32467;&#26500;&#26469;&#32531;&#35299;&#25351;&#26631;&#22238;&#24402;&#30446;&#26631;&#20043;&#38388;&#30340;&#20914;&#31361;&#12290;</title><link>http://arxiv.org/abs/2308.08915</link><description>&lt;p&gt;
&#36229;&#36234;&#20849;&#20139;&#65306;&#20914;&#31361;&#24863;&#30693;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Beyond Sharing: Conflict-Aware Multivariate Time Series Anomaly Detection. (arXiv:2308.08915v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08915
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20914;&#31361;&#24863;&#30693;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#20026;&#27599;&#20010;&#25351;&#26631;&#25552;&#20379;&#29420;&#29305;&#30340;&#32467;&#26500;&#26469;&#32531;&#35299;&#25351;&#26631;&#22238;&#24402;&#30446;&#26631;&#20043;&#38388;&#30340;&#20914;&#31361;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#37327;&#30340;&#20851;&#38190;&#32489;&#25928;&#25351;&#26631;(KPI)&#20197;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;(MTS)&#30340;&#24418;&#24335;&#36827;&#34892;&#30417;&#27979;&#65292;&#20197;&#30830;&#20445;&#36719;&#20214;&#24212;&#29992;&#31243;&#24207;&#21644;&#26381;&#21153;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#12290;&#20934;&#30830;&#26816;&#27979;MTS&#30340;&#24322;&#24120;&#23545;&#20110;&#21518;&#32493;&#30340;&#25925;&#38556;&#25490;&#38500;&#38750;&#24120;&#20851;&#38190;&#12290;&#24322;&#24120;&#30340;&#31232;&#32570;&#24615;&#21644;&#25163;&#21160;&#26631;&#35760;&#23548;&#33268;&#20102;&#21508;&#31181;&#33258;&#30417;&#30563;&#30340;MTS&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#30340;&#21457;&#23637;&#65292;&#36825;&#20123;&#26041;&#27861;&#20248;&#21270;&#20102;&#19968;&#20010;&#28085;&#30422;&#25152;&#26377;&#25351;&#26631;&#22238;&#24402;&#30446;&#26631;/&#25439;&#22833;&#30340;&#25972;&#20307;&#30446;&#26631;/&#25439;&#22833;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#21457;&#29616;&#20102;&#25351;&#26631;&#22238;&#24402;&#30446;&#26631;&#20043;&#38388;&#20914;&#31361;&#30340;&#26222;&#36941;&#23384;&#22312;&#65292;&#23548;&#33268;MTS&#27169;&#22411;&#22312;&#19981;&#21516;&#30340;&#25439;&#22833;&#20013;&#25379;&#25166;&#12290;&#36825;&#19968;&#20851;&#38190;&#26041;&#38754;&#26174;&#33879;&#24433;&#21709;&#26816;&#27979;&#24615;&#33021;&#65292;&#20294;&#22312;&#29616;&#26377;&#26041;&#27861;&#20013;&#34987;&#24573;&#35270;&#20102;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36890;&#36807;&#27169;&#20223;&#22810;&#38376;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;(MMoE)&#30340;&#35774;&#35745;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;CAD&#65292;&#19968;&#31181;&#20914;&#31361;&#24863;&#30693;&#30340;&#22810;&#21464;&#37327;KPI&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#12290;CAD&#20026;&#27599;&#20010;&#25351;&#26631;&#25552;&#20379;&#20102;&#19968;&#20010;&#29420;&#29305;&#30340;&#32467;&#26500;&#65292;&#20197;&#32531;&#35299;&#20914;&#31361;&#12290;
&lt;/p&gt;
&lt;p&gt;
Massive key performance indicators (KPIs) are monitored as multivariate time series data (MTS) to ensure the reliability of the software applications and service system. Accurately detecting the abnormality of MTS is very critical for subsequent fault elimination. The scarcity of anomalies and manual labeling has led to the development of various self-supervised MTS anomaly detection (AD) methods, which optimize an overall objective/loss encompassing all metrics' regression objectives/losses. However, our empirical study uncovers the prevalence of conflicts among metrics' regression objectives, causing MTS models to grapple with different losses. This critical aspect significantly impacts detection performance but has been overlooked in existing approaches. To address this problem, by mimicking the design of multi-gate mixture-of-experts (MMoE), we introduce CAD, a Conflict-aware multivariate KPI Anomaly Detection algorithm. CAD offers an exclusive structure for each metric to mitigate
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#28145;&#24230;&#23398;&#20064;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#22120;&#23545;&#25239;&#25915;&#20987;&#30340;&#23454;&#29992;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#38543;&#26426;&#24179;&#28369;&#21270;&#26469;&#21093;&#31163;&#21487;&#25191;&#34892;&#25991;&#20214;&#19968;&#23450;&#27604;&#20363;&#23383;&#33410;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#21093;&#31163;&#29256;&#26412;&#30340;&#22522;&#20998;&#31867;&#22120;&#36827;&#34892;&#35757;&#32451;&#21644;&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2308.08906</link><description>&lt;p&gt;
&#38024;&#23545;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#22120;&#30340;&#23545;&#25239;&#25915;&#20987;&#30340;&#23454;&#29992;&#38450;&#24481;&#26041;&#27861;&#65306;&#38543;&#26426;&#24179;&#28369;&#21270;
&lt;/p&gt;
&lt;p&gt;
Towards a Practical Defense against Adversarial Attacks on Deep Learning-based Malware Detectors via Randomized Smoothing. (arXiv:2308.08906v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08906
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#28145;&#24230;&#23398;&#20064;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#22120;&#23545;&#25239;&#25915;&#20987;&#30340;&#23454;&#29992;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#38543;&#26426;&#24179;&#28369;&#21270;&#26469;&#21093;&#31163;&#21487;&#25191;&#34892;&#25991;&#20214;&#19968;&#23450;&#27604;&#20363;&#23383;&#33410;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#21093;&#31163;&#29256;&#26412;&#30340;&#22522;&#20998;&#31867;&#22120;&#36827;&#34892;&#35757;&#32451;&#21644;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#22120;&#23545;&#24694;&#24847;&#36719;&#20214;&#26679;&#26412;&#30340;&#26816;&#27979;&#26131;&#21463;&#21040;&#24694;&#24847;&#31713;&#25913;&#30340;&#24433;&#21709;&#65292;&#21363;&#23545;&#25239;&#24615;&#24694;&#24847;&#36719;&#20214;&#26679;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;&#38450;&#24481;&#26041;&#27861;&#26469;&#23545;&#25239;&#23545;&#25239;&#24615;&#24694;&#24847;&#36719;&#20214;&#26679;&#26412;&#65292;&#35813;&#26041;&#27861;&#21463;&#21040;&#38543;&#26426;&#24179;&#28369;&#21270;&#30340;&#21551;&#21457;&#12290;&#25105;&#20204;&#20351;&#29992;&#38543;&#26426;&#21093;&#31163;&#24179;&#28369;&#21270;&#26041;&#26696;&#65292;&#32780;&#19981;&#26159;&#22312;&#38543;&#26426;&#21270;&#36755;&#20837;&#26102;&#20351;&#29992;&#39640;&#26031;&#25110;&#25289;&#26222;&#25289;&#26031;&#22122;&#22768;&#65292;&#35813;&#26041;&#26696;&#21093;&#31163;&#20102;&#21487;&#25191;&#34892;&#25991;&#20214;&#20013;&#30340;&#19968;&#23450;&#27604;&#20363;&#23383;&#33410;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#30340;&#38543;&#26426;&#21093;&#31163;&#24179;&#28369;&#21270;&#26041;&#26696;&#20351;&#29992;&#21093;&#31163;&#29256;&#26412;&#30340;&#21487;&#25191;&#34892;&#25991;&#20214;&#26469;&#35757;&#32451;&#22522;&#20998;&#31867;&#22120;&#12290;&#22312;&#27979;&#35797;&#26102;&#65292;&#32473;&#23450;&#36755;&#20837;&#21487;&#25191;&#34892;&#25991;&#20214;&#30340;&#26368;&#32456;&#20998;&#31867;&#34987;&#35270;&#20026;&#22312;&#19968;&#32452;&#21093;&#31163;&#29256;&#26412;&#30340;&#21407;&#22987;&#21487;&#25191;&#34892;&#25991;&#20214;&#19978;&#30001;&#20998;&#31867;&#22120;&#26368;&#24120;&#39044;&#27979;&#30340;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Malware detectors based on deep learning (DL) have been shown to be susceptible to malware examples that have been deliberately manipulated in order to evade detection, a.k.a. adversarial malware examples. More specifically, it has been show that deep learning detectors are vulnerable to small changes on the input file. Given this vulnerability of deep learning detectors, we propose a practical defense against adversarial malware examples inspired by randomized smoothing. In our work, instead of employing Gaussian or Laplace noise when randomizing inputs, we propose a randomized ablation-based smoothing scheme that ablates a percentage of the bytes within an executable. During training, our randomized ablation-based smoothing scheme trains a base classifier based on ablated versions of the executable files. At test time, the final classification for a given input executable is taken as the class most commonly predicted by the classifier on a set of ablated versions of the original exec
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#29992;&#20110;&#30140;&#30171;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#27169;&#22411;&#65292;&#20197;&#20415;&#30740;&#31350;&#30140;&#30171;&#27010;&#24565;&#21450;&#20854;&#20851;&#31995;&#12290;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#21487;&#20197;&#23558;&#22823;&#22411;&#22270;&#35889;&#36716;&#21270;&#20026;&#20302;&#32500;&#21521;&#37327;&#65292;&#20174;&#32780;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#65292;&#24182;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2308.08904</link><description>&lt;p&gt;
&#20026;&#30140;&#30171;&#24320;&#21457;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Development of a Knowledge Graph Embeddings Model for Pain. (arXiv:2308.08904v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08904
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#29992;&#20110;&#30140;&#30171;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#27169;&#22411;&#65292;&#20197;&#20415;&#30740;&#31350;&#30140;&#30171;&#27010;&#24565;&#21450;&#20854;&#20851;&#31995;&#12290;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#21487;&#20197;&#23558;&#22823;&#22411;&#22270;&#35889;&#36716;&#21270;&#20026;&#20302;&#32500;&#21521;&#37327;&#65292;&#20174;&#32780;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#65292;&#24182;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30140;&#30171;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#27010;&#24565;&#65292;&#21487;&#20197;&#19982;&#20854;&#20182;&#27010;&#24565;&#30456;&#20114;&#20851;&#32852;&#65292;&#27604;&#22914;&#21487;&#33021;&#24341;&#36215;&#30140;&#30171;&#30340;&#30142;&#30149;&#65292;&#21487;&#33021;&#32531;&#35299;&#30140;&#30171;&#30340;&#33647;&#29289;&#31561;&#31561;&#12290;&#20026;&#20102;&#23436;&#20840;&#29702;&#35299;&#20010;&#20307;&#25110;&#25972;&#20010;&#20154;&#32676;&#25152;&#32463;&#21382;&#30340;&#30140;&#30171;&#32972;&#26223;&#65292;&#25105;&#20204;&#38656;&#35201;&#30740;&#31350;&#19982;&#30140;&#30171;&#30456;&#20851;&#30340;&#25152;&#26377;&#27010;&#24565;&#21450;&#20854;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#24403;&#24314;&#27169;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#35760;&#24405;&#30340;&#30140;&#30171;&#26102;&#65292;&#36825;&#23588;&#20026;&#26377;&#29992;&#12290;&#30693;&#35782;&#22270;&#35889;&#36890;&#36807;&#19968;&#20010;&#30456;&#20114;&#38142;&#25509;&#30340;&#32593;&#32476;&#26469;&#34920;&#31034;&#27010;&#24565;&#21644;&#23427;&#20204;&#30340;&#20851;&#31995;&#65292;&#20197;&#19968;&#31181;&#35745;&#31639;&#21487;&#22788;&#29702;&#30340;&#26041;&#24335;&#23454;&#29616;&#35821;&#20041;&#21644;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#25512;&#29702;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22270;&#35889;&#21487;&#33021;&#36807;&#22823;&#20197;&#33267;&#20110;&#35745;&#31639;&#25928;&#29575;&#20302;&#19979;&#12290;&#32780;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#26377;&#21161;&#20110;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#23427;&#23558;&#22270;&#35889;&#34920;&#31034;&#20026;&#20302;&#32500;&#21521;&#37327;&#31354;&#38388;&#12290;&#28982;&#21518;&#65292;&#36825;&#20123;&#23884;&#20837;&#21487;&#20197;&#29992;&#20110;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#65292;&#22914;&#20998;&#31867;&#21644;&#38142;&#25509;&#39044;&#27979;&#12290;&#26500;&#24314;&#36825;&#26679;&#19968;&#20010;&#30693;&#35782;&#22270;&#35889;&#25152;&#38656;&#30340;&#19982;&#30140;&#30171;&#30456;&#20851;&#30340;&#21508;&#31181;&#20851;&#31995;&#21487;&#20197;o
&lt;/p&gt;
&lt;p&gt;
Pain is a complex concept that can interconnect with other concepts such as a disorder that might cause pain, a medication that might relieve pain, and so on. To fully understand the context of pain experienced by either an individual or across a population, we may need to examine all concepts related to pain and the relationships between them. This is especially useful when modeling pain that has been recorded in electronic health records. Knowledge graphs represent concepts and their relations by an interlinked network, enabling semantic and context-based reasoning in a computationally tractable form. These graphs can, however, be too large for efficient computation. Knowledge graph embeddings help to resolve this by representing the graphs in a low-dimensional vector space. These embeddings can then be used in various downstream tasks such as classification and link prediction. The various relations associated with pain which are required to construct such a knowledge graph can be o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#33258;&#30001;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#38646;&#21644;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#20013;&#23454;&#29616;&#19982;&#27169;&#22411;&#20026;&#22522;&#30784;&#31639;&#27861;&#30456;&#21516;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#39318;&#27425;&#35777;&#26126;&#20102;&#27169;&#22411;&#33258;&#30001;&#31639;&#27861;&#21487;&#20197;&#22312;&#26102;&#38388;&#27573;&#20381;&#36182;&#24615;&#26041;&#38754;&#36798;&#21040;&#21516;&#26679;&#30340;&#20248;&#21270;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.08858</link><description>&lt;p&gt;
&#27809;&#26377;&#27169;&#22411;&#30340;&#31639;&#27861;&#22312;&#38646;&#21644;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#20013;&#25552;&#39640;&#20102;&#26679;&#26412;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Model-Free Algorithm with Improved Sample Efficiency for Zero-Sum Markov Games. (arXiv:2308.08858v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08858
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#33258;&#30001;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#38646;&#21644;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#20013;&#23454;&#29616;&#19982;&#27169;&#22411;&#20026;&#22522;&#30784;&#31639;&#27861;&#30456;&#21516;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#39318;&#27425;&#35777;&#26126;&#20102;&#27169;&#22411;&#33258;&#30001;&#31639;&#27861;&#21487;&#20197;&#22312;&#26102;&#38388;&#27573;&#20381;&#36182;&#24615;&#26041;&#38754;&#36798;&#21040;&#21516;&#26679;&#30340;&#20248;&#21270;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20004;&#20154;&#38646;&#21644;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#38382;&#39064;&#22312;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#29702;&#35770;&#30740;&#31350;&#20013;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20852;&#36259;&#12290;&#29305;&#21035;&#26159;&#23545;&#20110;&#26377;&#38480;&#26102;&#38388;&#27573;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#24050;&#32463;&#35777;&#26126;&#20102;&#27169;&#22411;&#20026;&#22522;&#30784;&#30340;&#31639;&#27861;&#21487;&#20197;&#36890;&#36807;&#26679;&#26412;&#22797;&#26434;&#24230;&#20026;$O(H^3SAB/\epsilon^2)$&#25214;&#21040;$\epsilon$-&#26368;&#20248;&#30340;&#32435;&#20160;&#22343;&#34913;&#65288;NE&#65289;&#65292;&#20854;&#20013;$H$&#26159;&#26102;&#38388;&#27573;&#65292;$S$&#26159;&#29366;&#24577;&#25968;&#37327;&#65288;$A$&#21644;$B$&#20998;&#21035;&#34920;&#31034;&#20004;&#20010;&#29609;&#23478;&#30340;&#21160;&#20316;&#25968;&#37327;&#65289;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#27809;&#26377;&#19968;&#31181;&#29616;&#26377;&#30340;&#27169;&#22411;&#33258;&#30001;&#31639;&#27861;&#21487;&#20197;&#36798;&#21040;&#36825;&#26679;&#30340;&#20248;&#21270;&#25928;&#26524;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#33258;&#30001;&#30340;&#38454;&#27573;&#24615;Q&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#23637;&#31034;&#23427;&#23454;&#29616;&#20102;&#19982;&#26368;&#20339;&#27169;&#22411;&#20026;&#22522;&#30784;&#31639;&#27861;&#30456;&#21516;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#22240;&#27492;&#39318;&#27425;&#35777;&#26126;&#20102;&#27169;&#22411;&#33258;&#30001;&#31639;&#27861;&#21487;&#20197;&#22312;&#26102;&#38388;&#27573;&#20381;&#36182;&#24615;&#26041;&#38754;&#20139;&#21463;&#19982;&#27169;&#22411;&#20026;&#22522;&#30784;&#31639;&#27861;&#30456;&#21516;&#30340;&#20248;&#21270;&#25928;&#26524;&#12290;&#23545;&#20110;$H$&#30340;&#20381;&#36182;&#24615;&#30340;&#20027;&#35201;&#25913;&#36827;&#26469;&#28304;&#20110;...
&lt;/p&gt;
&lt;p&gt;
The problem of two-player zero-sum Markov games has recently attracted increasing interests in theoretical studies of multi-agent reinforcement learning (RL). In particular, for finite-horizon episodic Markov decision processes (MDPs), it has been shown that model-based algorithms can find an $\epsilon$-optimal Nash Equilibrium (NE) with the sample complexity of $O(H^3SAB/\epsilon^2)$, which is optimal in the dependence of the horizon $H$ and the number of states $S$ (where $A$ and $B$ denote the number of actions of the two players, respectively). However, none of the existing model-free algorithms can achieve such an optimality. In this work, we propose a model-free stage-based Q-learning algorithm and show that it achieves the same sample complexity as the best model-based algorithm, and hence for the first time demonstrate that model-free algorithms can enjoy the same optimality in the $H$ dependence as model-based algorithms. The main improvement of the dependency on $H$ arises by
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#20351;&#29992;&#33258;&#36866;&#24212;&#19981;&#30830;&#23450;&#24615;&#20998;&#24067;&#65292;&#21306;&#20998;&#25509;&#36817;&#34920;&#38754;&#30340;&#28857;&#21644;&#20854;&#20182;&#28857;&#65292;&#20174;&#32780;&#22312;&#20687;&#32032;&#23545;&#40784;&#24418;&#29366;&#24674;&#22797;&#20013;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2308.08857</link><description>&lt;p&gt;
D-IF: &#36890;&#36807;&#38544;&#24335;&#20998;&#24067;&#22330;&#23454;&#29616;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#20154;&#20307;&#25968;&#23383;&#21270;
&lt;/p&gt;
&lt;p&gt;
D-IF: Uncertainty-aware Human Digitization via Implicit Distribution Field. (arXiv:2308.08857v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08857
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#20351;&#29992;&#33258;&#36866;&#24212;&#19981;&#30830;&#23450;&#24615;&#20998;&#24067;&#65292;&#21306;&#20998;&#25509;&#36817;&#34920;&#38754;&#30340;&#28857;&#21644;&#20854;&#20182;&#28857;&#65292;&#20174;&#32780;&#22312;&#20687;&#32032;&#23545;&#40784;&#24418;&#29366;&#24674;&#22797;&#20013;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36924;&#30495;&#30340;&#34394;&#25311;&#20154;&#22312;&#35768;&#22810;&#34892;&#19994;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#20363;&#22914;&#20803;&#23431;&#23449;&#12289;&#26234;&#33021;&#21307;&#30103;&#21644;&#33258;&#21160;&#39550;&#39542;&#27169;&#25311;&#12290;&#20294;&#26159;&#22823;&#35268;&#27169;&#21019;&#36896;&#20855;&#26377;&#39640;&#24230;&#36924;&#30495;&#24230;&#30340;&#34394;&#25311;&#20154;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#28145;&#24230;&#38544;&#24335;&#20989;&#25968;&#30340;&#24212;&#29992;&#24320;&#21551;&#20102;&#22522;&#20110;&#22270;&#20687;&#30340;&#19977;&#32500;&#31359;&#30528;&#20154;&#20307;&#37325;&#24314;&#30340;&#26032;&#26102;&#20195;&#65292;&#23454;&#29616;&#20102;&#20855;&#26377;&#32454;&#33410;&#30340;&#20687;&#32032;&#23545;&#40784;&#24418;&#29366;&#24674;&#22797;&#12290;&#38543;&#21518;&#65292;&#32477;&#22823;&#37096;&#20998;&#24037;&#20316;&#36890;&#36807;&#22238;&#24402;&#27599;&#20010;&#28857;&#30340;&#30830;&#23450;&#24615;&#38544;&#24335;&#20540;&#26469;&#23450;&#20301;&#34920;&#38754;&#12290;&#28982;&#32780;&#65292;&#26159;&#21542;&#24212;&#35813;&#19981;&#32771;&#34385;&#19982;&#34920;&#38754;&#30340;&#36317;&#31163;&#32780;&#23558;&#25152;&#26377;&#28857;&#37117;&#19968;&#35270;&#21516;&#20161;&#21602;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#29992;&#33258;&#36866;&#24212;&#19981;&#30830;&#23450;&#24615;&#20998;&#24067;&#26367;&#25442;&#38544;&#24335;&#20540;&#65292;&#26681;&#25454;&#28857;&#19982;&#34920;&#38754;&#30340;&#36317;&#31163;&#23545;&#23427;&#20204;&#36827;&#34892;&#21306;&#20998;&#12290;&#36825;&#31181;&#31616;&#21333;&#30340;&#8220;&#20540;&#21040;&#20998;&#24067;&#8221;&#30340;&#36716;&#21464;&#26174;&#33879;&#25913;&#36827;&#20102;&#20960;&#20046;&#25152;&#26377;&#30340;&#22522;&#20934;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#23450;&#24615;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#25105;&#20204;&#30340;&#19981;&#30830;&#23450;&#24615;&#20998;&#24067;&#25439;&#22833;&#35757;&#32451;&#30340;&#27169;&#22411;&#21487;&#20197;&#20135;&#29983;&#20196;&#20154;&#28385;&#24847;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Realistic virtual humans play a crucial role in numerous industries, such as metaverse, intelligent healthcare, and self-driving simulation. But creating them on a large scale with high levels of realism remains a challenge. The utilization of deep implicit function sparks a new era of image-based 3D clothed human reconstruction, enabling pixel-aligned shape recovery with fine details. Subsequently, the vast majority of works locate the surface by regressing the deterministic implicit value for each point. However, should all points be treated equally regardless of their proximity to the surface? In this paper, we propose replacing the implicit value with an adaptive uncertainty distribution, to differentiate between points based on their distance to the surface. This simple ``value to distribution'' transition yields significant improvements on nearly all the baselines. Furthermore, qualitative results demonstrate that the models trained using our uncertainty distribution loss, can ca
&lt;/p&gt;</description></item><item><title>CMB&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#20013;&#25991;&#21307;&#23398;&#22522;&#20934;&#65292;&#22522;&#20110;&#20013;&#22269;&#26412;&#22303;&#35821;&#35328;&#21644;&#25991;&#21270;&#26694;&#26550;&#35774;&#35745;&#65292;&#33021;&#22815;&#35299;&#20915;&#23558;&#33521;&#35821;&#21307;&#23398;&#35780;&#20272;&#32763;&#35793;&#21040;&#26412;&#22320;&#29615;&#22659;&#20013;&#30340;&#19978;&#19979;&#25991;&#19981;&#19968;&#33268;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.08833</link><description>&lt;p&gt;
CMB&#65306;&#19968;&#20010;&#20840;&#38754;&#30340;&#20013;&#25991;&#21307;&#23398;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
CMB: A Comprehensive Medical Benchmark in Chinese. (arXiv:2308.08833v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08833
&lt;/p&gt;
&lt;p&gt;
CMB&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#20013;&#25991;&#21307;&#23398;&#22522;&#20934;&#65292;&#22522;&#20110;&#20013;&#22269;&#26412;&#22303;&#35821;&#35328;&#21644;&#25991;&#21270;&#26694;&#26550;&#35774;&#35745;&#65292;&#33021;&#22815;&#35299;&#20915;&#23558;&#33521;&#35821;&#21307;&#23398;&#35780;&#20272;&#32763;&#35793;&#21040;&#26412;&#22320;&#29615;&#22659;&#20013;&#30340;&#19978;&#19979;&#25991;&#19981;&#19968;&#33268;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20026;&#22312;&#21307;&#23398;&#39046;&#22495;&#21462;&#24471;&#37325;&#22823;&#31361;&#30772;&#25552;&#20379;&#20102;&#21487;&#33021;&#24615;&#12290;&#24314;&#31435;&#19968;&#20010;&#26631;&#20934;&#21270;&#30340;&#21307;&#23398;&#22522;&#20934;&#25104;&#20026;&#34913;&#37327;&#36827;&#23637;&#30340;&#22522;&#30707;&#12290;&#28982;&#32780;&#65292;&#19981;&#21516;&#22320;&#21306;&#30340;&#21307;&#23398;&#29615;&#22659;&#20855;&#26377;&#21508;&#33258;&#30340;&#29305;&#28857;&#65292;&#20363;&#22914;&#22312;&#20013;&#22269;&#22659;&#20869;&#20256;&#32479;&#20013;&#21307;&#30340;&#26222;&#36941;&#24615;&#21644;&#37325;&#35201;&#24615;&#12290;&#22240;&#27492;&#65292;&#20165;&#20165;&#32763;&#35793;&#22522;&#20110;&#33521;&#35821;&#30340;&#21307;&#23398;&#35780;&#20272;&#21487;&#33021;&#23548;&#33268;&#24403;&#22320;&#29615;&#22659;&#20013;&#30340;&#8220;&#19978;&#19979;&#25991;&#19981;&#19968;&#33268;&#8221;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CMB&#65288;Comprehensive Medical Benchmark in Chinese&#65289;&#30340;&#26412;&#22320;&#21270;&#21307;&#23398;&#22522;&#20934;&#65292;&#23436;&#20840;&#35774;&#35745;&#21644;&#26681;&#26893;&#20110;&#20013;&#22269;&#26412;&#22303;&#30340;&#35821;&#35328;&#21644;&#25991;&#21270;&#26694;&#26550;&#12290;&#23613;&#31649;&#20256;&#32479;&#20013;&#21307;&#26159;&#36825;&#20010;&#35780;&#20272;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#20294;&#23427;&#24182;&#19981;&#26500;&#25104;&#20854;&#20840;&#37096;&#12290;&#20351;&#29992;&#36825;&#20010;&#22522;&#20934;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#20960;&#20010;&#30693;&#21517;&#30340;&#22823;&#35268;&#27169;LLMs&#65292;&#21253;&#25324;ChatGPT&#12289;GPT-4&#12289;&#19987;&#38376;&#30340;&#20013;&#25991;LLMs&#21644;&#19987;&#38376;&#29992;&#20110;&#21307;&#23398;&#39046;&#22495;&#30340;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) provide a possibility to make a great breakthrough in medicine. The establishment of a standardized medical benchmark becomes a fundamental cornerstone to measure progression. However, medical environments in different regions have their local characteristics, e.g., the ubiquity and significance of traditional Chinese medicine within China. Therefore, merely translating English-based medical evaluation may result in \textit{contextual incongruities} to a local region. To solve the issue, we propose a localized medical benchmark called CMB, a Comprehensive Medical Benchmark in Chinese, designed and rooted entirely within the native Chinese linguistic and cultural framework. While traditional Chinese medicine is integral to this evaluation, it does not constitute its entirety. Using this benchmark, we have evaluated several prominent large-scale LLMs, including ChatGPT, GPT-4, dedicated Chinese LLMs, and LLMs specialized in the medical domain. It is worth not
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#31216;&#21152;&#26435;&#19968;&#38454;&#27169;&#22411;&#25277;&#26679;&#30340;&#25552;&#21319;&#31639;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#25193;&#23637;&#20102;&#35745;&#25968;&#37327;&#35789;&#30340;&#19968;&#38454;&#36923;&#36753;&#30340;&#8220;&#22495;&#21487;&#25552;&#21319;&#25277;&#26679;&#8221;&#30340;&#20869;&#23481;&#65292;&#35777;&#26126;&#20102;&#21152;&#26435;&#27169;&#22411;&#25277;&#26679;&#38382;&#39064;&#20063;&#23384;&#22312;&#21487;&#22788;&#29702;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2308.08828</link><description>&lt;p&gt;
&#23545;&#31216;&#21152;&#26435;&#19968;&#38454;&#27169;&#22411;&#25277;&#26679;&#30340;&#25552;&#21319;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Lifted Algorithms for Symmetric Weighted First-Order Model Sampling. (arXiv:2308.08828v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08828
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#31216;&#21152;&#26435;&#19968;&#38454;&#27169;&#22411;&#25277;&#26679;&#30340;&#25552;&#21319;&#31639;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#25193;&#23637;&#20102;&#35745;&#25968;&#37327;&#35789;&#30340;&#19968;&#38454;&#36923;&#36753;&#30340;&#8220;&#22495;&#21487;&#25552;&#21319;&#25277;&#26679;&#8221;&#30340;&#20869;&#23481;&#65292;&#35777;&#26126;&#20102;&#21152;&#26435;&#27169;&#22411;&#25277;&#26679;&#38382;&#39064;&#20063;&#23384;&#22312;&#21487;&#22788;&#29702;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21152;&#26435;&#27169;&#22411;&#35745;&#25968;&#65288;WMC&#65289;&#26159;&#35745;&#31639;&#21629;&#39064;&#20844;&#24335;&#30340;&#25152;&#26377;&#28385;&#36275;&#35299;&#65288;&#27169;&#22411;&#65289;&#30340;&#26435;&#37325;&#20043;&#21644;&#30340;&#20219;&#21153;&#12290;&#31867;&#20284;&#22320;&#65292;&#21152;&#26435;&#27169;&#22411;&#25277;&#26679;&#65288;WMS&#65289;&#26088;&#22312;&#20197;&#27010;&#29575;&#19982;&#23427;&#20204;&#30456;&#24212;&#30340;&#26435;&#37325;&#25104;&#27604;&#20363;&#22320;&#38543;&#26426;&#29983;&#25104;&#27169;&#22411;&#12290;WMC&#21644;WMS&#37117;&#38590;&#20197;&#31934;&#30830;&#27714;&#35299;&#65292;&#23646;&#20110;\#P-hard&#22797;&#26434;&#24230;&#31867;&#21035;&#12290;&#28982;&#32780;&#65292;&#24050;&#30693;&#22914;&#26524;&#21629;&#39064;&#20844;&#24335;&#21487;&#20197;&#20197;&#32039;&#20945;&#30340;&#26041;&#24335;&#34920;&#31034;&#24182;&#29992;&#19968;&#38454;&#36923;&#36753;&#34920;&#36798;&#65292;&#26377;&#26102;&#35745;&#25968;&#38382;&#39064;&#21487;&#33021;&#26159;&#21487;&#20197;&#22788;&#29702;&#30340;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#27169;&#22411;&#35745;&#25968;&#38382;&#39064;&#21487;&#20197;&#22312;&#22495;&#22823;&#23567;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#35299;&#20915;&#65292;&#24182;&#34987;&#31216;&#20026;&#8220;&#22495;&#21487;&#25552;&#21319;&#8221;&#12290;&#28982;&#21518;&#65292;&#23601;&#20250;&#20986;&#29616;&#20197;&#19979;&#38382;&#39064;&#65306;&#21152;&#26435;&#27169;&#22411;&#25277;&#26679;&#26159;&#21542;&#20063;&#26159;&#22914;&#27492;&#65311;&#26412;&#25991;&#22238;&#31572;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#32943;&#23450;&#22320;&#22238;&#31572;&#20102;&#23427;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#25193;&#23637;&#20102;&#35745;&#25968;&#37327;&#35789;&#30340;&#19968;&#38454;&#36923;&#36753;&#30340;&#8220;&#22495;&#21487;&#25552;&#21319;&#25277;&#26679;&#8221;&#30340;&#20869;&#23481;&#65292;&#35777;&#26126;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Weighted model counting (WMC) is the task of computing the weighted sum of all satisfying assignments (i.e., models) of a propositional formula. Similarly, weighted model sampling (WMS) aims to randomly generate models with probability proportional to their respective weights. Both WMC and WMS are hard to solve exactly, falling under the \#P-hard complexity class. However, it is known that the counting problem may sometimes be tractable, if the propositional formula can be compactly represented and expressed in first-order logic. In such cases, model counting problems can be solved in time polynomial in the domain size, and are known as \textit{domain-liftable}. The following question then arises: Is it also the case for weighted model sampling? This paper addresses this question and answers it affirmatively. Specifically, we prove the \textit{domain-liftability under sampling} for the two-variables fragment of first-order logic with counting quantifiers in this paper, by devising an e
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21270;&#30340;&#38750;&#20010;&#24615;&#21270;&#26041;&#27861;PARE&#65292;&#36890;&#36807;&#39044;&#27979;&#26368;&#39640;&#27969;&#34892;&#24230;&#30340;&#39033;&#30446;&#36827;&#34892;&#25512;&#33616;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#25512;&#33616;&#26041;&#27861;&#24573;&#30053;&#39033;&#30446;&#27969;&#34892;&#24230;&#30340;&#19981;&#36275;&#12290;&#23454;&#39564;&#35777;&#26126;PARE&#30340;&#24615;&#33021;&#20248;&#20110;&#22797;&#26434;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.08799</link><description>&lt;p&gt;
&#25429;&#25417;&#27969;&#34892;&#36235;&#21183;&#65306;&#22686;&#24378;&#39033;&#30446;&#25512;&#33616;&#30340;&#31616;&#21270;&#38750;&#20010;&#24615;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Capturing Popularity Trends: A Simplistic Non-Personalized Approach for Enhanced Item Recommendation. (arXiv:2308.08799v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08799
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21270;&#30340;&#38750;&#20010;&#24615;&#21270;&#26041;&#27861;PARE&#65292;&#36890;&#36807;&#39044;&#27979;&#26368;&#39640;&#27969;&#34892;&#24230;&#30340;&#39033;&#30446;&#36827;&#34892;&#25512;&#33616;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#25512;&#33616;&#26041;&#27861;&#24573;&#30053;&#39033;&#30446;&#27969;&#34892;&#24230;&#30340;&#19981;&#36275;&#12290;&#23454;&#39564;&#35777;&#26126;PARE&#30340;&#24615;&#33021;&#20248;&#20110;&#22797;&#26434;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#65292;&#25512;&#33616;&#31995;&#32479;&#24050;&#32463;&#36234;&#26469;&#36234;&#21463;&#21040;&#30740;&#31350;&#30340;&#20851;&#27880;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#25512;&#33616;&#26041;&#27861;&#20391;&#37325;&#20110;&#36890;&#36807;&#21382;&#21490;&#30340;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#26469;&#25429;&#25417;&#29992;&#25143;&#30340;&#20010;&#24615;&#21270;&#20559;&#22909;&#65292;&#36825;&#21487;&#33021;&#20250;&#20405;&#29359;&#29992;&#25143;&#30340;&#38544;&#31169;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#24120;&#24120;&#24573;&#35270;&#20102;&#39033;&#30446;&#27969;&#34892;&#24230;&#30340;&#26102;&#38388;&#27874;&#21160;&#23545;&#29992;&#25143;&#20915;&#31574;&#30340;&#37325;&#35201;&#24615;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Popularity-Aware Recommender&#65288;PARE&#65289;&#65292;&#36890;&#36807;&#39044;&#27979;&#23558;&#36798;&#21040;&#26368;&#39640;&#27969;&#34892;&#24230;&#30340;&#39033;&#30446;&#26469;&#36827;&#34892;&#38750;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;PARE&#30001;&#22235;&#20010;&#27169;&#22359;&#32452;&#25104;&#65292;&#20998;&#21035;&#20851;&#27880;&#19981;&#21516;&#30340;&#26041;&#38754;&#65306;&#27969;&#34892;&#24230;&#21382;&#21490;&#12289;&#26102;&#38388;&#24433;&#21709;&#12289;&#21608;&#26399;&#24615;&#24433;&#21709;&#21644;&#38468;&#21152;&#20449;&#24687;&#12290;&#26368;&#21518;&#65292;&#21033;&#29992;&#27880;&#24847;&#21147;&#23618;&#34701;&#21512;&#22235;&#20010;&#27169;&#22359;&#30340;&#36755;&#20986;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#26126;&#30830;&#24314;&#27169;&#39033;&#30446;&#27969;&#34892;&#24230;&#30340;&#24037;&#20316;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;PARE&#30340;&#24615;&#33021;&#19982;&#22797;&#26434;&#30340;&#26041;&#27861;&#30456;&#24403;&#29978;&#33267;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender systems have been gaining increasing research attention over the years. Most existing recommendation methods focus on capturing users' personalized preferences through historical user-item interactions, which may potentially violate user privacy. Additionally, these approaches often overlook the significance of the temporal fluctuation in item popularity that can sway users' decision-making. To bridge this gap, we propose Popularity-Aware Recommender (PARE), which makes non-personalized recommendations by predicting the items that will attain the highest popularity. PARE consists of four modules, each focusing on a different aspect: popularity history, temporal impact, periodic impact, and side information. Finally, an attention layer is leveraged to fuse the outputs of four modules. To our knowledge, this is the first work to explicitly model item popularity in recommendation systems. Extensive experiments show that PARE performs on par or even better than sophisticated st
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;CodeCoT&#21644;Beyond&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#24110;&#21161;&#27169;&#22411;&#22312;&#22788;&#29702;&#20219;&#21153;&#26102;&#20174;&#23569;&#37327;&#29305;&#23450;&#25968;&#25454;&#20013;&#36827;&#34892;&#36866;&#24212;&#12290;&#36890;&#36807;&#38142;&#24335;&#24605;&#32500;&#24341;&#23548;&#65292;&#27169;&#22411;&#21487;&#20197;&#25581;&#31034;&#22810;&#27493;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#35748;&#30693;&#36807;&#31243;&#65292;&#24182;&#36890;&#36807;&#33258;&#25105;&#26816;&#26597;&#19981;&#26029;&#20248;&#21270;&#36755;&#20986;&#12290;</title><link>http://arxiv.org/abs/2308.08784</link><description>&lt;p&gt;
CodeCoT&#21450;&#20854;&#36827;&#23637;&#65306;&#23398;&#20064;&#20687;&#24320;&#21457;&#32773;&#19968;&#26679;&#32534;&#31243;&#21644;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
CodeCoT and Beyond: Learning to Program and Test like a Developer. (arXiv:2308.08784v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08784
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;CodeCoT&#21644;Beyond&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#24110;&#21161;&#27169;&#22411;&#22312;&#22788;&#29702;&#20219;&#21153;&#26102;&#20174;&#23569;&#37327;&#29305;&#23450;&#25968;&#25454;&#20013;&#36827;&#34892;&#36866;&#24212;&#12290;&#36890;&#36807;&#38142;&#24335;&#24605;&#32500;&#24341;&#23548;&#65292;&#27169;&#22411;&#21487;&#20197;&#25581;&#31034;&#22810;&#27493;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#35748;&#30693;&#36807;&#31243;&#65292;&#24182;&#36890;&#36807;&#33258;&#25105;&#26816;&#26597;&#19981;&#26029;&#20248;&#21270;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;OpenAI&#24320;&#21457;&#30340;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;GPT-x&#27169;&#22411;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#29616;&#29366;&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#20855;&#26377;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#22788;&#29702;&#19982;&#20854;&#35757;&#32451;&#25968;&#25454;&#19981;&#21516;&#30340;&#20219;&#21153;&#26102;&#24120;&#24120;&#36935;&#21040;&#25361;&#25112;&#65292;&#36896;&#25104;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20986;&#29616;&#20102;&#19968;&#31181;&#34987;&#31216;&#20026;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#26377;&#20215;&#20540;&#25216;&#26415;&#65292;&#20801;&#35768;LLM&#22312;&#26368;&#23569;&#30340;&#20219;&#21153;&#29305;&#23450;&#25968;&#25454;&#19978;&#36827;&#34892;&#36866;&#24212;&#12290;&#19968;&#31181;&#21019;&#26032;&#30340;&#31574;&#30053;&#65292;&#31216;&#20026;&#24605;&#32500;&#38142;&#25552;&#31034;&#65288;CoT&#65289;&#65292;&#24050;&#34987;&#24341;&#20837;&#20197;&#25351;&#23548;LLM&#22312;&#22810;&#27493;&#25512;&#29702;&#36807;&#31243;&#20013;&#25581;&#31034;&#35748;&#30693;&#36807;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Code Chain-of-Thought&#65288;CodeCoT&#65289;&#65292;&#23427;&#30001;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#32452;&#25104;&#65306;&#32463;&#20856;CodeCoT&#21644;&#33258;&#25105;&#26816;&#26597;CodeCoT&#12290;&#21518;&#32773;&#21152;&#20837;&#20102;&#33258;&#25105;&#26816;&#26597;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#36845;&#20195;&#29983;&#25104;&#20195;&#30721;&#65292;&#21046;&#23450;&#27979;&#35797;&#29992;&#20363;&#24182;&#25913;&#21892;&#20854;&#36755;&#20986;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#36807;&#31243;&#21253;&#25324;&#27169;&#22411;&#29983;&#25104;&#19982;&#20998;&#31867;&#21035;&#29305;&#24449;&#23545;&#24212;&#30340;&#27979;&#35797;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
In natural language processing, transformer-based large language models (LLMs) like GPT-x models developed by OpenAI have revolutionized the landscape. Despite their impressive capabilities, these models often encounter challenges when handling tasks that differ from their training data, resulting in compromised performance. To address this, few-shot learning has emerged as a valuable technique, allowing LLMs to adapt with minimal task-specific data. One innovative strategy, known as Chain-of-Thought Prompting (CoT), has been introduced to guide LLMs in revealing cognitive processes during multi-step reasoning. In this paper, we propose Code Chain-of-Thought~(CodeCoT), which consists of two components: the Vanilla CodeCoT and the Self-exam CodeCoT. The latter incorporates self-examination, empowering the model to iteratively generate code, formulate test cases, and refine its outputs. Specifically, the process entails the generation of test examples by the model corresponding to the co
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#28436;&#31034;&#38598;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#22312;&#32473;&#23450;&#20219;&#21153;&#30340;&#36755;&#20837;&#36755;&#20986;&#23545;&#20013;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#36890;&#36807;&#23558;&#28436;&#31034;&#20998;&#25104;&#23376;&#38598;&#24182;&#32452;&#21512;&#21508;&#23376;&#38598;&#30340;&#36755;&#20986;&#27010;&#29575;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#26368;&#32456;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.08780</link><description>&lt;p&gt;
&#25506;&#32034;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#28436;&#31034;&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
Exploring Demonstration Ensembling for In-context Learning. (arXiv:2308.08780v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#28436;&#31034;&#38598;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#22312;&#32473;&#23450;&#20219;&#21153;&#30340;&#36755;&#20837;&#36755;&#20986;&#23545;&#20013;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#36890;&#36807;&#23558;&#28436;&#31034;&#20998;&#25104;&#23376;&#38598;&#24182;&#32452;&#21512;&#21508;&#23376;&#38598;&#30340;&#36755;&#20986;&#27010;&#29575;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#26368;&#32456;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#23398;&#20064;&#36890;&#36807;&#21521;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#36755;&#20837;-&#36755;&#20986;&#23545;&#30340;&#31034;&#20363;&#26469;&#36827;&#34892;&#25805;&#20316;&#65292;&#21363;&#28436;&#31034;&#12290;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26631;&#20934;&#26041;&#27861;&#26159;&#23558;&#28436;&#31034;&#19982;&#27979;&#35797;&#36755;&#20837;&#36830;&#25509;&#36215;&#26469;&#25552;&#31034;&#32473;&#35821;&#35328;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#23384;&#22312;&#19968;&#20123;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#36830;&#25509;&#26041;&#27861;&#20960;&#20046;&#26080;&#27861;&#25511;&#21046;&#27599;&#20010;&#28436;&#31034;&#23545;&#27169;&#22411;&#39044;&#27979;&#30340;&#36129;&#29486;&#12290;&#24403;&#19968;&#20123;&#28436;&#31034;&#19982;&#27979;&#35797;&#31034;&#20363;&#26080;&#20851;&#26102;&#65292;&#36825;&#21487;&#33021;&#19981;&#26159;&#26368;&#20248;&#30340;&#12290;&#20854;&#27425;&#65292;&#30001;&#20110;&#26576;&#20123;&#21464;&#25442;&#22120;&#27169;&#22411;&#23545;&#36755;&#20837;&#38271;&#24230;&#26377;&#38480;&#21046;&#65292;&#23558;&#35768;&#22810;&#31034;&#20363;&#25918;&#20837;&#19978;&#19979;&#25991;&#20013;&#21487;&#33021;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#38271;&#36755;&#20837;&#20219;&#21153;&#26102;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#28436;&#31034;&#38598;&#25104;&#65288;DENSE&#65289;&#20316;&#20026;&#31616;&#21333;&#36830;&#25509;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#27169;&#22411;&#20351;&#29992;&#28436;&#31034;&#30340;&#23376;&#38598;&#65288;&#21363;bucket&#65289;&#26469;&#39044;&#27979;&#36755;&#20986;&#65292;&#28982;&#21518;&#23558;&#27599;&#20010;&#23376;&#38598;&#24471;&#21040;&#30340;&#36755;&#20986;&#27010;&#29575;&#32452;&#21512;&#36215;&#26469;&#29983;&#25104;&#26368;&#32456;&#39044;&#27979;&#32467;&#26524;&#12290;&#25105;&#20204;&#20351;&#29992;GPT-j&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#24182;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-context learning (ICL) operates by showing language models (LMs) examples of input-output pairs for a given task, i.e., demonstrations. The standard approach for ICL is to prompt the LM with concatenated demonstrations followed by the test input. This approach suffers from some issues. First, concatenation offers almost no control over the contribution of each demo to the model prediction. This can be sub-optimal when some demonstrations are irrelevant to the test example. Second, due to the input length limit of some transformer models, it might be infeasible to fit many examples into the context, especially when dealing with long-input tasks. In this work, we explore Demonstration Ensembling (DENSE) as an alternative to simple concatenation. \model predicts outputs using subsets (i.e., buckets) of the demonstrations and then combines the output probabilities resulting from each subset to produce the final prediction. We study different ensembling methods using GPT-j and experiment
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#20013;&#22269;&#21171;&#21160;&#24066;&#22330;&#30340;&#28508;&#22312;&#24433;&#21709;&#65292;&#24182;&#20998;&#26512;&#20102;&#32844;&#19994;&#23545;LLM&#33021;&#21147;&#30340;&#26292;&#38706;&#31243;&#24230;&#21450;&#20854;&#19982;&#24037;&#36164;&#27700;&#24179;&#21644;&#32463;&#39564;&#28322;&#20215;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#39640;&#34218;&#21644;&#32463;&#39564;&#23494;&#38598;&#22411;&#24037;&#20316;&#21487;&#33021;&#38754;&#20020;&#26356;&#22823;&#30340;&#26367;&#20195;&#39118;&#38505;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#32771;&#34385;&#34892;&#19994;&#26292;&#38706;&#30340;&#32463;&#27982;&#22686;&#38271;&#27169;&#22411;&#65292;&#20197;&#37327;&#21270;AI&#37319;&#29992;&#23545;&#29983;&#20135;&#21147;&#21644;&#23601;&#19994;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#36825;&#39033;&#30740;&#31350;&#20026;&#29702;&#35299;&#20013;&#22269;&#21171;&#21160;&#24066;&#22330;&#20013;&#36234;&#26469;&#36234;&#24378;&#22823;&#30340;AI&#31995;&#32479;&#30340;&#24433;&#21709;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2308.08776</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20013;&#22269;&#21171;&#21160;&#24066;&#22330;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Large Language Models at Work in China's Labor Market. (arXiv:2308.08776v1 [econ.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08776
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#20013;&#22269;&#21171;&#21160;&#24066;&#22330;&#30340;&#28508;&#22312;&#24433;&#21709;&#65292;&#24182;&#20998;&#26512;&#20102;&#32844;&#19994;&#23545;LLM&#33021;&#21147;&#30340;&#26292;&#38706;&#31243;&#24230;&#21450;&#20854;&#19982;&#24037;&#36164;&#27700;&#24179;&#21644;&#32463;&#39564;&#28322;&#20215;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#39640;&#34218;&#21644;&#32463;&#39564;&#23494;&#38598;&#22411;&#24037;&#20316;&#21487;&#33021;&#38754;&#20020;&#26356;&#22823;&#30340;&#26367;&#20195;&#39118;&#38505;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#32771;&#34385;&#34892;&#19994;&#26292;&#38706;&#30340;&#32463;&#27982;&#22686;&#38271;&#27169;&#22411;&#65292;&#20197;&#37327;&#21270;AI&#37319;&#29992;&#23545;&#29983;&#20135;&#21147;&#21644;&#23601;&#19994;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#36825;&#39033;&#30740;&#31350;&#20026;&#29702;&#35299;&#20013;&#22269;&#21171;&#21160;&#24066;&#22330;&#20013;&#36234;&#26469;&#36234;&#24378;&#22823;&#30340;AI&#31995;&#32479;&#30340;&#24433;&#21709;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#20013;&#22269;&#21171;&#21160;&#24066;&#22330;&#30340;&#28508;&#22312;&#24433;&#21709;&#12290;&#25105;&#20204;&#36890;&#36807;&#32467;&#21512;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#21644;LLM&#20998;&#31867;&#65292;&#25353;&#29031;Eloundou&#31561;&#20154;&#65288;2023&#65289;&#30340;&#26041;&#27861;&#20998;&#26512;&#20102;&#32844;&#19994;&#23545;LLM&#33021;&#21147;&#30340;&#26292;&#38706;&#31243;&#24230;&#12290;&#28982;&#21518;&#23558;&#32844;&#19994;&#26292;&#38706;&#31243;&#24230;&#32858;&#21512;&#21040;&#34892;&#19994;&#27700;&#24179;&#19978;&#65292;&#24471;&#21040;&#34892;&#19994;&#26292;&#38706;&#24471;&#20998;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#32844;&#19994;&#26292;&#38706;&#21644;&#24037;&#36164;&#27700;&#24179;/&#32463;&#39564;&#28322;&#20215;&#20043;&#38388;&#23384;&#22312;&#27491;&#30456;&#20851;&#20851;&#31995;&#65292;&#34920;&#26126;&#39640;&#34218;&#21644;&#32463;&#39564;&#23494;&#38598;&#22411;&#30340;&#24037;&#20316;&#21487;&#33021;&#38754;&#20020;&#30528;LLM&#39537;&#21160;&#36719;&#20214;&#30340;&#26356;&#22823;&#26367;&#20195;&#39118;&#38505;&#12290;&#34892;&#19994;&#26292;&#38706;&#24471;&#20998;&#19982;&#19987;&#23478;&#35780;&#20272;&#21644;&#32463;&#27982;&#30452;&#35273;&#30456;&#19968;&#33268;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#32771;&#34385;&#34892;&#19994;&#26292;&#38706;&#30340;&#32463;&#27982;&#22686;&#38271;&#27169;&#22411;&#65292;&#20197;&#37327;&#21270;AI&#37319;&#29992;&#24102;&#26469;&#30340;&#29983;&#20135;&#21147;&#21644;&#23601;&#19994;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#24635;&#20307;&#26469;&#35828;&#65292;&#26412;&#30740;&#31350;&#20026;&#29702;&#35299;&#20013;&#22269;&#36234;&#26469;&#36234;&#24378;&#22823;&#30340;AI&#31995;&#32479;&#23545;&#21171;&#21160;&#24066;&#22330;&#30340;&#24433;&#21709;&#25552;&#20379;&#20102;&#20998;&#26512;&#22522;&#30784;&#12290;&#20027;&#35201;&#21019;&#26032;&#21253;&#25324;&#32844;&#19994;&#27700;&#24179;&#30340;&#26292;&#38706;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the potential impacts of large language models (LLMs) on the Chinese labor market. We analyze occupational exposure to LLM capabilities by incorporating human expertise and LLM classifications, following Eloundou et al. (2023)'s methodology. We then aggregate occupation exposure to the industry level to obtain industry exposure scores. The results indicate a positive correlation between occupation exposure and wage levels/experience premiums, suggesting higher-paying and experience-intensive jobs may face greater displacement risks from LLM-powered software. The industry exposure scores align with expert assessments and economic intuitions. We also develop an economic growth model incorporating industry exposure to quantify the productivity-employment trade-off from AI adoption. Overall, this study provides an analytical basis for understanding the labor market impacts of increasingly capable AI systems in China. Key innovations include the occupation-level exposure
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#35821;&#35328;&#21387;&#32553;&#12289;&#35821;&#35328;&#20844;&#24179;&#24615;&#21644;&#36879;&#26126;&#24615;&#31561;&#26041;&#38754;&#30340;&#35201;&#27714;&#65292;&#24182;&#21457;&#29616;&#24046;&#20998;&#38544;&#31169;&#19982;&#35757;&#32451;&#25968;&#25454;&#24433;&#21709;&#31232;&#30095;&#24615;&#20043;&#38388;&#23384;&#22312;&#30456;&#20114;&#21046;&#32422;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2308.08774</link><description>&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#12289;&#35821;&#35328;&#20844;&#24179;&#24615;&#21644;&#35757;&#32451;&#25968;&#25454;&#24433;&#21709;&#65306;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#21487;&#33021;&#24615;&#21644;&#21487;&#33021;&#24615;&#23450;&#29702;
&lt;/p&gt;
&lt;p&gt;
Differential Privacy, Linguistic Fairness, and Training Data Influence: Impossibility and Possibility Theorems for Multilingual Language Models. (arXiv:2308.08774v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08774
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#35821;&#35328;&#21387;&#32553;&#12289;&#35821;&#35328;&#20844;&#24179;&#24615;&#21644;&#36879;&#26126;&#24615;&#31561;&#26041;&#38754;&#30340;&#35201;&#27714;&#65292;&#24182;&#21457;&#29616;&#24046;&#20998;&#38544;&#31169;&#19982;&#35757;&#32451;&#25968;&#25454;&#24433;&#21709;&#31232;&#30095;&#24615;&#20043;&#38388;&#23384;&#22312;&#30456;&#20114;&#21046;&#32422;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22914;mBERT&#12289;XLM-R&#21644;BLOOM&#26088;&#22312;&#23454;&#29616;&#22810;&#35821;&#35328;&#27010;&#25324;&#25110;&#21387;&#32553;&#65292;&#20197;&#20415;&#20110;&#36716;&#31227;&#21040;&#22823;&#37327;&#65288;&#21487;&#33021;&#26410;&#30693;&#30340;&#65289;&#35821;&#35328;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#36824;&#24212;&#35813;&#20855;&#22791;&#38544;&#31169;&#24615;&#12289;&#35821;&#35328;&#20844;&#24179;&#24615;&#21644;&#36879;&#26126;&#24615;&#65292;&#21363;&#23558;&#23427;&#20204;&#30340;&#39044;&#27979;&#19982;&#35757;&#32451;&#25968;&#25454;&#30456;&#20851;&#32852;&#12290;&#36825;&#20123;&#35201;&#27714;&#21487;&#20197;&#21516;&#26102;&#28385;&#36275;&#21527;&#65311;&#25105;&#20204;&#34920;&#26126;&#65292;&#22810;&#35821;&#35328;&#21387;&#32553;&#21644;&#35821;&#35328;&#20844;&#24179;&#24615;&#19982;&#24046;&#20998;&#38544;&#31169;&#26159;&#20860;&#23481;&#30340;&#65292;&#20294;&#24046;&#20998;&#38544;&#31169;&#19982;&#35757;&#32451;&#25968;&#25454;&#24433;&#21709;&#31232;&#30095;&#24615;&#26159;&#30456;&#24726;&#30340;&#65292;&#21518;&#32773;&#26159;&#36879;&#26126;&#24615;&#30340;&#30446;&#26631;&#12290;&#25105;&#20204;&#36824;&#23545;&#20004;&#20010;&#24120;&#35265;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#38544;&#31169;&#20445;&#35777;&#19979;&#35780;&#20272;&#20102;&#22810;&#35821;&#35328;&#21387;&#32553;&#21644;&#35757;&#32451;&#25968;&#25454;&#24433;&#21709;&#31232;&#30095;&#24615;&#65292;&#26356;&#35814;&#32454;&#22320;&#25506;&#35752;&#20102;&#36825;&#20123;&#26435;&#34913;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#38656;&#35201;&#24320;&#21457;&#19968;&#31181;&#20849;&#21516;&#20248;&#21270;&#36825;&#20123;&#30446;&#26631;&#30340;&#26041;&#27861;&#65292;&#20197;&#25214;&#21040;&#23454;&#38469;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models such as mBERT, XLM-R, and BLOOM aim to achieve multilingual generalization or compression to facilitate transfer to a large number of (potentially unseen) languages. However, these models should ideally also be private, linguistically fair, and transparent, by relating their predictions to training data. Can these requirements be simultaneously satisfied? We show that multilingual compression and linguistic fairness are compatible with differential privacy, but that differential privacy is at odds with training data influence sparsity, an objective for transparency. We further present a series of experiments on two common NLP tasks and evaluate multilingual compression and training data influence sparsity under different privacy guarantees, exploring these trade-offs in more detail. Our results suggest that we need to develop ways to jointly optimize for these objectives in order to find practical trade-offs.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#31163;&#25955;&#25552;&#31034;&#21387;&#32553;&#26041;&#27861;&#65288;PCRL&#65289;&#65292;&#20197;&#35299;&#20915;&#25351;&#20196;&#35843;&#25972;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#23884;&#20837;&#35757;&#32451;&#30340;&#25361;&#25112;&#12290;PCRL&#37319;&#29992;&#20102;&#19968;&#31181;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#31574;&#30053;&#32593;&#32476;&#30452;&#25509;&#32534;&#36753;&#25552;&#31034;&#65292;&#21487;&#20197;&#28789;&#27963;&#24212;&#29992;&#20110;&#21508;&#31181;&#31867;&#22411;&#30340;LM&#65292;&#32780;&#19981;&#38656;&#35201;&#26799;&#24230;&#35775;&#38382;&#25110;&#26631;&#35760;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2308.08758</link><description>&lt;p&gt;
&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#31163;&#25955;&#25552;&#31034;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Discrete Prompt Compression with Reinforcement Learning. (arXiv:2308.08758v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08758
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#31163;&#25955;&#25552;&#31034;&#21387;&#32553;&#26041;&#27861;&#65288;PCRL&#65289;&#65292;&#20197;&#35299;&#20915;&#25351;&#20196;&#35843;&#25972;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#23884;&#20837;&#35757;&#32451;&#30340;&#25361;&#25112;&#12290;PCRL&#37319;&#29992;&#20102;&#19968;&#31181;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#31574;&#30053;&#32593;&#32476;&#30452;&#25509;&#32534;&#36753;&#25552;&#31034;&#65292;&#21487;&#20197;&#28789;&#27963;&#24212;&#29992;&#20110;&#21508;&#31181;&#31867;&#22411;&#30340;LM&#65292;&#32780;&#19981;&#38656;&#35201;&#26799;&#24230;&#35775;&#38382;&#25110;&#26631;&#35760;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#25972;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#34987;&#29992;&#25143;&#24191;&#27867;&#20351;&#29992;&#26469;&#35299;&#20915;&#19982;&#20219;&#21153;&#29305;&#23450;&#25552;&#31034;&#30456;&#20851;&#30340;&#21508;&#31181;&#38382;&#39064;&#12290;&#30001;&#20110;&#19978;&#19979;&#25991;&#31383;&#21475;&#38271;&#24230;&#21644;&#35745;&#31639;&#25104;&#26412;&#30340;&#38480;&#21046;&#65292;&#40723;&#21169;&#24320;&#21457;&#21387;&#32553;&#25552;&#31034;&#30340;&#26041;&#27861;&#12290;&#29616;&#26377;&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#20110;&#35757;&#32451;&#23884;&#20837;&#65292;&#36825;&#20123;&#23884;&#20837;&#34987;&#35774;&#35745;&#20026;&#23481;&#32435;&#22810;&#20010;&#35760;&#21495;&#21547;&#20041;&#12290;&#36825;&#22312;&#35299;&#37322;&#24615;&#12289;&#22266;&#23450;&#25968;&#37327;&#30340;&#23884;&#20837;&#35760;&#21495;&#12289;&#22312;&#19981;&#21516;LM&#20043;&#38388;&#30340;&#21487;&#37325;&#29992;&#24615;&#20197;&#21450;&#19982;&#40657;&#30418;API&#20132;&#20114;&#26102;&#30340;&#19981;&#36866;&#29992;&#24615;&#26041;&#38754;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#25552;&#31034;&#21387;&#32553;&#26041;&#27861;&#65288;PCRL&#65289;&#65292;&#23427;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;PCRL&#37319;&#29992;&#20102;&#19968;&#31181;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#31574;&#30053;&#32593;&#32476;&#65292;&#30452;&#25509;&#32534;&#36753;&#25552;&#31034;&#12290;PCRL&#30340;&#35757;&#32451;&#26041;&#27861;&#21487;&#20197;&#28789;&#27963;&#22320;&#24212;&#29992;&#20110;&#21508;&#31181;&#31867;&#22411;&#30340;LM&#65292;&#20197;&#21450;&#21482;&#26377;&#35299;&#30721;&#22120;&#21644;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#32780;&#19981;&#38656;&#35201;&#20351;&#29992;&#26799;&#24230;&#35775;&#38382;LM&#25110;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction-tuned Language Models (LMs) are widely used by users to address various problems with task-specific prompts. Constraints associated with the context window length and computational costs encourage the development of compressed prompts. Existing methods rely heavily on training embeddings, which are designed to accommodate multiple token meanings. This presents challenges in terms of interpretability, a fixed number of embedding tokens, reusability across different LMs, and inapplicability when interacting with black-box APIs. This study proposes prompt compression with reinforcement learning (PCRL), a novel discrete prompt compression method that addresses these issues. PCRL employs a computationally efficient policy network that directly edits prompts. The PCRL training approach can be flexibly applied to various types of LMs, as well as decoder-only and encoder-decoder architecture, and can be trained without gradient access to LMs or labeled data. PCRL achieves an averag
&lt;/p&gt;</description></item><item><title>SurgicalSAM&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#25163;&#26415;&#22120;&#26800;&#20998;&#21106;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;SurgicalSAM&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#24212;&#29992;Segment Anything Model (SAM) &#36827;&#34892;&#25163;&#26415;&#22120;&#26800;&#20998;&#21106;&#65292;&#35299;&#20915;&#20102;SAM&#22312;&#25163;&#26415;&#22120;&#26800;&#39046;&#22495;&#30340;&#27867;&#21270;&#33021;&#21147;&#24046;&#21644;&#22797;&#26434;&#22810;&#38454;&#27573;&#27969;&#31243;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.08746</link><description>&lt;p&gt;
SurgicalSAM: &#39640;&#25928;&#30340;&#21487;&#25552;&#31034;&#30340;&#25163;&#26415;&#22120;&#26800;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
SurgicalSAM: Efficient Class Promptable Surgical Instrument Segmentation. (arXiv:2308.08746v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08746
&lt;/p&gt;
&lt;p&gt;
SurgicalSAM&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#25163;&#26415;&#22120;&#26800;&#20998;&#21106;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;SurgicalSAM&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#24212;&#29992;Segment Anything Model (SAM) &#36827;&#34892;&#25163;&#26415;&#22120;&#26800;&#20998;&#21106;&#65292;&#35299;&#20915;&#20102;SAM&#22312;&#25163;&#26415;&#22120;&#26800;&#39046;&#22495;&#30340;&#27867;&#21270;&#33021;&#21147;&#24046;&#21644;&#22797;&#26434;&#22810;&#38454;&#27573;&#27969;&#31243;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Segment Anything Model (SAM) &#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#22270;&#20687;&#20998;&#21106;&#12290;&#20026;&#20102;&#23558;SAM&#24212;&#29992;&#20110;&#25163;&#26415;&#22120;&#26800;&#20998;&#21106;&#65292;&#24120;&#35265;&#30340;&#26041;&#27861;&#26159;&#23450;&#20301;&#22120;&#26800;&#30340;&#31934;&#30830;&#28857;&#25110;&#26694;&#65292;&#24182;&#23558;&#20854;&#29992;&#20316;SAM&#30340;&#25552;&#31034;&#65292;&#20197;&#38646;&#26679;&#26412;&#26041;&#24335;&#36827;&#34892;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#36825;&#31181;&#31616;&#21333;&#30340;&#27969;&#31243;&#23384;&#22312;&#20004;&#20010;&#38382;&#39064;&#65306;&#65288;1&#65289;&#33258;&#28982;&#29289;&#20307;&#21644;&#25163;&#26415;&#22120;&#26800;&#20043;&#38388;&#30340;&#39046;&#22495;&#24046;&#36317;&#23548;&#33268;SAM&#30340;&#27867;&#21270;&#33021;&#21147;&#24046;&#65307;&#65288;2&#65289;SAM&#20381;&#36182;&#20110;&#31934;&#30830;&#30340;&#28857;&#25110;&#26694;&#20301;&#32622;&#36827;&#34892;&#20934;&#30830;&#30340;&#20998;&#21106;&#65292;&#35201;&#27714;&#35201;&#20040;&#32463;&#36807;&#24191;&#27867;&#30340;&#25163;&#21160;&#24341;&#23548;&#65292;&#35201;&#20040;&#20351;&#29992;&#24615;&#33021;&#33391;&#22909;&#30340;&#19987;&#38376;&#26816;&#27979;&#22120;&#36827;&#34892;&#25552;&#31034;&#20934;&#22791;&#65292;&#36825;&#23548;&#33268;&#20102;&#19968;&#20010;&#22797;&#26434;&#30340;&#22810;&#38454;&#27573;&#27969;&#31243;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;SurgicalSAM&#65292;&#19968;&#31181;&#26032;&#30340;&#31471;&#21040;&#31471;&#39640;&#25928;&#35843;&#20248;&#26041;&#27861;&#65292;&#20197;&#26377;&#25928;&#22320;&#23558;&#25163;&#26415;&#29305;&#23450;&#20449;&#24687;&#19982;SAM&#30340;&#39044;&#35757;&#32451;&#30693;&#35782;&#30456;&#32467;&#21512;&#65292;&#20197;&#25913;&#36827;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Segment Anything Model (SAM) is a powerful foundation model that has revolutionised image segmentation. To apply SAM to surgical instrument segmentation, a common approach is to locate precise points or boxes of instruments and then use them as prompts for SAM in a zero-shot manner. However, we observe two problems with this naive pipeline: (1) the domain gap between natural objects and surgical instruments leads to poor generalisation of SAM; and (2) SAM relies on precise point or box locations for accurate segmentation, requiring either extensive manual guidance or a well-performing specialist detector for prompt preparation, which leads to a complex multi-stage pipeline. To address these problems, we introduce SurgicalSAM, a novel end-to-end efficient-tuning approach for SAM to effectively integrate surgical-specific information with SAM's pre-trained knowledge for improved generalisation. Specifically, we propose a lightweight prototype-based class prompt encoder for tuning, wh
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#20998;&#26512;Transformer&#27169;&#22411;&#20013;&#30340;&#38544;&#34255;&#29366;&#24577;&#65292;&#21457;&#29616;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#32534;&#30721;&#20102;&#26576;&#20123;&#36890;&#29992;&#30693;&#35782;&#25552;&#21462;&#27169;&#24335;&#65292;&#22240;&#27492;&#22312;&#36827;&#34892;&#27169;&#22411;&#32534;&#36753;&#26102;&#65292;&#19981;&#38656;&#35201;&#26356;&#26032;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#30340;&#26435;&#37325;&#12290;</title><link>http://arxiv.org/abs/2308.08742</link><description>&lt;p&gt;
PMET: &#22312;Transformer&#20013;&#30340;&#31934;&#30830;&#27169;&#22411;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
PMET: Precise Model Editing in a Transformer. (arXiv:2308.08742v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08742
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#20998;&#26512;Transformer&#27169;&#22411;&#20013;&#30340;&#38544;&#34255;&#29366;&#24577;&#65292;&#21457;&#29616;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#32534;&#30721;&#20102;&#26576;&#20123;&#36890;&#29992;&#30693;&#35782;&#25552;&#21462;&#27169;&#24335;&#65292;&#22240;&#27492;&#22312;&#36827;&#34892;&#27169;&#22411;&#32534;&#36753;&#26102;&#65292;&#19981;&#38656;&#35201;&#26356;&#26032;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#30340;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#32534;&#36753;&#25216;&#26415;&#21487;&#20197;&#20197;&#36739;&#20302;&#30340;&#25104;&#26412;&#20462;&#25913;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#23569;&#37327;&#30693;&#35782;&#65292;&#24182;&#19988;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#29616;&#26377;&#26041;&#27861;&#20551;&#35774;Transformer&#23618;&#38544;&#34255;&#29366;&#24577;&#26159;&#21069;&#39304;&#32593;&#32476;&#30340;&#38190;&#20540;&#20869;&#23384;&#30340;&#20540;&#12290;&#23427;&#20204;&#36890;&#24120;&#20248;&#21270;Transformer&#23618;&#38544;&#34255;&#29366;&#24577;&#26469;&#35760;&#24518;&#30446;&#26631;&#30693;&#35782;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#26356;&#26032;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21069;&#39304;&#32593;&#32476;&#30340;&#26435;&#37325;&#12290;&#28982;&#32780;&#65292;Transformer&#23618;&#38544;&#34255;&#29366;&#24577;&#30340;&#20449;&#24687;&#27969;&#26469;&#33258;&#19977;&#20010;&#37096;&#20998;&#65306;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#12289;&#21069;&#39304;&#32593;&#32476;&#21644;&#27531;&#24046;&#36830;&#25509;&#12290;&#29616;&#26377;&#26041;&#27861;&#24573;&#35270;&#20102;Transformer&#23618;&#38544;&#34255;&#29366;&#24577;&#21253;&#21547;&#20102;&#21069;&#39304;&#32593;&#32476;&#29305;&#21035;&#38656;&#35201;&#30340;&#20449;&#24687;&#36825;&#19968;&#20107;&#23454;&#12290;&#22240;&#27492;&#65292;&#27169;&#22411;&#32534;&#36753;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#23454;&#29616;&#26356;&#31934;&#30830;&#30340;&#27169;&#22411;&#32534;&#36753;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#21644;&#21069;&#39304;&#32593;&#32476;&#30340;&#38544;&#34255;&#29366;&#24577;&#65292;&#21457;&#29616;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#32534;&#30721;&#20102;&#26576;&#20123;&#36890;&#29992;&#30693;&#35782;&#25552;&#21462;&#27169;&#24335;&#12290;&#36825;&#24847;&#21619;&#30528;&#24403;&#24341;&#20837;&#26032;&#30693;&#35782;&#26102;&#65292;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#30340;&#26435;&#37325;&#19981;&#38656;&#35201;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model editing techniques modify a minor proportion of knowledge in Large Language Models (LLMs) at a relatively low cost, which have demonstrated notable success. Existing methods assume Transformer Layer (TL) hidden states are values of key-value memories of the Feed-Forward Network (FFN). They usually optimize the TL hidden states to memorize target knowledge and use it to update the weights of the FFN in LLMs. However, the information flow of TL hidden states comes from three parts: Multi-Head Self-Attention (MHSA), FFN, and residual connections. Existing methods neglect the fact that the TL hidden states contains information not specifically required for FFN. Consequently, the performance of model editing decreases. To achieve more precise model editing, we analyze hidden states of MHSA and FFN, finding that MHSA encodes certain general knowledge extraction patterns. This implies that MHSA weights do not require updating when new knowledge is introduced. Based on above findings, we
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ReProHRL&#30340;&#23618;&#27425;&#21270;&#20195;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#20102;&#22810;&#30446;&#26631;&#23548;&#33322;&#20219;&#21153;&#65292;&#24182;&#20351;&#29992;&#29289;&#20307;&#26816;&#27979;&#22120;&#36827;&#34892;&#39044;&#22788;&#29702;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;ReProHRL&#26041;&#27861;&#22312;&#20223;&#30495;&#21644;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.08737</link><description>&lt;p&gt;
ReProHRL: &#38754;&#21521;&#22810;&#30446;&#26631;&#23548;&#33322;&#30340;&#30495;&#23454;&#19990;&#30028;&#23618;&#27425;&#21270;&#20195;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ReProHRL: Towards Multi-Goal Navigation in the Real World using Hierarchical Agents. (arXiv:2308.08737v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08737
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ReProHRL&#30340;&#23618;&#27425;&#21270;&#20195;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#20102;&#22810;&#30446;&#26631;&#23548;&#33322;&#20219;&#21153;&#65292;&#24182;&#20351;&#29992;&#29289;&#20307;&#26816;&#27979;&#22120;&#36827;&#34892;&#39044;&#22788;&#29702;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;ReProHRL&#26041;&#27861;&#22312;&#20223;&#30495;&#21644;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#24050;&#25104;&#21151;&#29992;&#20110;&#39640;&#31934;&#24230;&#20219;&#21153;&#30340;&#25191;&#34892;&#12290;&#22312;&#31232;&#30095;&#22870;&#21169;&#21644;&#22810;&#30446;&#26631;&#30340;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#65292;&#23398;&#20064;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#65292;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#38590;&#20197;&#23398;&#20064;&#21040;&#22909;&#30340;&#31574;&#30053;&#12290;&#22312;&#20223;&#30495;&#29615;&#22659;&#20013;&#36827;&#34892;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#30495;&#23454;&#19990;&#30028;&#36827;&#34892;&#24494;&#35843;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36866;&#24212;&#30495;&#23454;&#19990;&#30028;&#30340;&#29615;&#22659;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"ReProHRL"&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#20102;&#20998;&#23618;&#22810;&#30446;&#26631;&#23548;&#33322;&#20219;&#21153;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#29289;&#20307;&#26816;&#27979;&#22120;&#20316;&#20026;&#39044;&#22788;&#29702;&#27493;&#39588;&#65292;&#23398;&#20064;&#22810;&#30446;&#26631;&#23548;&#33322;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;ReProHRL&#26041;&#27861;&#22312;&#20223;&#30495;&#21644;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#30340;&#35757;&#32451;&#26102;&#38388;&#21644;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#22522;&#20934;&#26041;&#27861;&#12290;&#34429;&#28982;&#36825;&#20004;&#31181;&#26041;&#27861;&#22312;&#21333;&#30446;&#26631;&#23548;&#33322;&#30340;&#31616;&#21333;&#29615;&#22659;&#20013;&#22343;&#23454;&#29616;&#20102;100%&#30340;&#25104;&#21151;&#29575;&#65292;&#20294;&#26159;&#22312;&#22810;&#30446;&#26631;&#23548;&#33322;&#38382;&#39064;&#19978;&#65292;ReProHRL&#26041;&#27861;&#34920;&#29616;&#26356;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robots have been successfully used to perform tasks with high precision. In real-world environments with sparse rewards and multiple goals, learning is still a major challenge and Reinforcement Learning (RL) algorithms fail to learn good policies. Training in simulation environments and then fine-tuning in the real world is a common approach. However, adapting to the real-world setting is a challenge. In this paper, we present a method named Ready for Production Hierarchical RL (ReProHRL) that divides tasks with hierarchical multi-goal navigation guided by reinforcement learning. We also use object detectors as a pre-processing step to learn multi-goal navigation and transfer it to the real world. Empirical results show that the proposed ReProHRL method outperforms the state-of-the-art baseline in simulation and real-world environments in terms of both training time and performance. Although both methods achieve a 100% success rate in a simple environment for single goal-based navigati
&lt;/p&gt;</description></item><item><title>LLM-FuncMapper&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;LLM&#23454;&#29616;&#23545;&#24314;&#31569;&#27861;&#35268;&#20013;&#22797;&#26434;&#26465;&#27454;&#30340;&#20989;&#25968;&#35782;&#21035;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23450;&#20041;&#21407;&#23376;&#20989;&#25968;&#21644;&#24320;&#21457;&#25552;&#31034;&#27169;&#26495;&#26469;&#35299;&#20915;&#20256;&#32479;&#36923;&#36753;&#34920;&#31034;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2308.08728</link><description>&lt;p&gt;
LLM-FuncMapper:&#36890;&#36807;LLM&#35299;&#37322;&#24314;&#31569;&#27861;&#35268;&#20013;&#30340;&#22797;&#26434;&#26465;&#27454;&#30340;&#20989;&#25968;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
LLM-FuncMapper: Function Identification for Interpreting Complex Clauses in Building Codes via LLM. (arXiv:2308.08728v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08728
&lt;/p&gt;
&lt;p&gt;
LLM-FuncMapper&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;LLM&#23454;&#29616;&#23545;&#24314;&#31569;&#27861;&#35268;&#20013;&#22797;&#26434;&#26465;&#27454;&#30340;&#20989;&#25968;&#35782;&#21035;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23450;&#20041;&#21407;&#23376;&#20989;&#25968;&#21644;&#24320;&#21457;&#25552;&#31034;&#27169;&#26495;&#26469;&#35299;&#20915;&#20256;&#32479;&#36923;&#36753;&#34920;&#31034;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#33258;&#21160;&#21270;&#35268;&#21017;&#26816;&#26597;&#65288;ARC&#65289;&#30340;&#20851;&#38190;&#38454;&#27573;&#65292;&#23545;&#30417;&#31649;&#24615;&#25991;&#26412;&#30340;&#35268;&#21017;&#35299;&#37322;&#38656;&#35201;&#30456;&#24403;&#22823;&#30340;&#21162;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#39046;&#22495;&#30693;&#35782;&#21644;&#20256;&#32479;&#36923;&#36753;&#34920;&#31034;&#30340;&#34920;&#36798;&#33021;&#21147;&#26377;&#38480;&#65292;&#35299;&#37322;&#20855;&#26377;&#38544;&#24335;&#23646;&#24615;&#25110;&#22797;&#26434;&#35745;&#31639;&#36923;&#36753;&#30340;&#30417;&#31649;&#26465;&#27454;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22240;&#27492;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#35782;&#21035;&#21508;&#31181;&#30417;&#31649;&#26465;&#27454;&#25152;&#38656;&#30340;&#39044;&#23450;&#20041;&#20989;&#25968;&#30340;&#26041;&#27861;LLM-FuncMapper&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#23545;&#24314;&#31569;&#27861;&#35268;&#36827;&#34892;&#31995;&#32479;&#20998;&#26512;&#65292;&#23450;&#20041;&#20102;&#19968;&#31995;&#21015;&#21407;&#23376;&#20989;&#25968;&#65292;&#20197;&#25429;&#25417;&#38544;&#24335;&#23646;&#24615;&#21644;&#22797;&#26434;&#32422;&#26463;&#30340;&#20849;&#20139;&#35745;&#31639;&#36923;&#36753;&#65292;&#21019;&#24314;&#20102;&#24120;&#35265;&#22359;&#30340;&#25968;&#25454;&#24211;&#65292;&#29992;&#20110;&#35299;&#37322;&#30417;&#31649;&#26465;&#27454;&#12290;&#28982;&#21518;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#20855;&#26377;&#24605;&#32500;&#38142;&#30340;&#25552;&#31034;&#27169;&#26495;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;&#20998;&#31867;&#30340;&#35843;&#20248;&#31574;&#30053;&#36827;&#19968;&#27493;&#22686;&#24378;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#20989;&#25968;&#35782;&#21035;&#21151;&#33021;&#12290;&#26368;&#21518;&#65292;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
As a vital stage of automated rule checking (ARC), rule interpretation of regulatory texts requires considerable effort. However, interpreting regulatory clauses with implicit properties or complex computational logic is still challenging due to the lack of domain knowledge and limited expressibility of conventional logic representations. Thus, LLM-FuncMapper, an approach to identifying predefined functions needed to interpret various regulatory clauses based on the large language model (LLM), is proposed. First, by systematically analysis of building codes, a series of atomic functions are defined to capture shared computational logics of implicit properties and complex constraints, creating a database of common blocks for interpreting regulatory clauses. Then, a prompt template with the chain of thought is developed and further enhanced with a classification-based tuning strategy, to enable common LLMs for effective function identification. Finally, the proposed approach is validated
&lt;/p&gt;</description></item><item><title>EdgeMA&#26159;&#19968;&#20010;&#23454;&#29992;&#39640;&#25928;&#30340;&#35270;&#39057;&#20998;&#26512;&#31995;&#32479;&#65292;&#36890;&#36807;&#25552;&#21462;&#32479;&#35745;&#32441;&#29702;&#29305;&#24449;&#21644;&#20351;&#29992;&#37325;&#35201;&#24615;&#21152;&#26435;&#30340;&#27169;&#22411;&#36866;&#24212;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#36793;&#32536;&#35774;&#22791;&#19978;&#23454;&#26102;&#35270;&#39057;&#20998;&#26512;&#20013;&#30340;&#25968;&#25454;&#28418;&#31227;&#38382;&#39064;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#25512;&#29702;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.08717</link><description>&lt;p&gt;
EdgeMA: &#36793;&#32536;&#35774;&#22791;&#19978;&#23454;&#26102;&#35270;&#39057;&#20998;&#26512;&#30340;&#27169;&#22411;&#36866;&#24212;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
EdgeMA: Model Adaptation System for Real-Time Video Analytics on Edge Devices. (arXiv:2308.08717v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08717
&lt;/p&gt;
&lt;p&gt;
EdgeMA&#26159;&#19968;&#20010;&#23454;&#29992;&#39640;&#25928;&#30340;&#35270;&#39057;&#20998;&#26512;&#31995;&#32479;&#65292;&#36890;&#36807;&#25552;&#21462;&#32479;&#35745;&#32441;&#29702;&#29305;&#24449;&#21644;&#20351;&#29992;&#37325;&#35201;&#24615;&#21152;&#26435;&#30340;&#27169;&#22411;&#36866;&#24212;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#36793;&#32536;&#35774;&#22791;&#19978;&#23454;&#26102;&#35270;&#39057;&#20998;&#26512;&#20013;&#30340;&#25968;&#25454;&#28418;&#31227;&#38382;&#39064;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#25512;&#29702;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36793;&#32536;&#35774;&#22791;&#19978;&#23454;&#26102;&#35270;&#39057;&#20998;&#26512;&#38754;&#23545;&#22330;&#26223;&#21464;&#21270;&#20173;&#28982;&#26159;&#19968;&#39033;&#22256;&#38590;&#30340;&#20219;&#21153;&#12290;&#30001;&#20110;&#36793;&#32536;&#35774;&#22791;&#30340;&#36164;&#28304;&#36890;&#24120;&#26377;&#38480;&#65292;&#36793;&#32536;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30456;&#27604;&#19968;&#33324;&#30340;DNN&#20855;&#26377;&#26356;&#23569;&#30340;&#26435;&#37325;&#21644;&#36739;&#27973;&#30340;&#26550;&#26500;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#21482;&#22312;&#26377;&#38480;&#30340;&#22330;&#26223;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#23545;&#25968;&#25454;&#28418;&#31227;&#25935;&#24863;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;EdgeMA&#65292;&#19968;&#31181;&#23454;&#29992;&#39640;&#25928;&#30340;&#35270;&#39057;&#20998;&#26512;&#31995;&#32479;&#65292;&#26088;&#22312;&#36866;&#24212;&#23454;&#38469;&#22330;&#26223;&#19979;&#35270;&#39057;&#27969;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#27169;&#22411;&#65292;&#35299;&#20915;&#25968;&#25454;&#28418;&#31227;&#38382;&#39064;&#12290;EdgeMA&#25552;&#21462;&#22522;&#20110;&#28784;&#24230;&#20849;&#29983;&#30697;&#38453;&#30340;&#32479;&#35745;&#32441;&#29702;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#38543;&#26426;&#26862;&#26519;&#20998;&#31867;&#22120;&#26816;&#27979;&#39046;&#22495;&#28418;&#31227;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#34701;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#37325;&#35201;&#24615;&#21152;&#26435;&#30340;&#27169;&#22411;&#36866;&#24212;&#26041;&#27861;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#26356;&#26032;&#27169;&#22411;&#20197;&#24212;&#23545;&#26631;&#31614;&#20998;&#24067;&#30340;&#21464;&#21270;&#12290;&#36890;&#36807;&#23545;&#19968;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#23545;EdgeMA&#36827;&#34892;&#20102;&#20005;&#26684;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;EdgeMA&#26174;&#33879;&#25552;&#39640;&#20102;&#25512;&#29702;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-time video analytics on edge devices for changing scenes remains a difficult task. As edge devices are usually resource-constrained, edge deep neural networks (DNNs) have fewer weights and shallower architectures than general DNNs. As a result, they only perform well in limited scenarios and are sensitive to data drift. In this paper, we introduce EdgeMA, a practical and efficient video analytics system designed to adapt models to shifts in real-world video streams over time, addressing the data drift problem. EdgeMA extracts the gray level co-occurrence matrix based statistical texture feature and uses the Random Forest classifier to detect the domain shift. Moreover, we have incorporated a method of model adaptation based on importance weighting, specifically designed to update models to cope with the label distribution shift. Through rigorous evaluation of EdgeMA on a real-world dataset, our results illustrate that EdgeMA significantly improves inference accuracy.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#33021;&#22815;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#30340;&#26426;&#22120;&#65292;&#23558;&#23450;&#37327;&#25512;&#29702;&#31995;&#32479;&#20998;&#20026;&#24605;&#32500;&#36807;&#31243;&#21644;&#35748;&#30693;&#36807;&#31243;&#20004;&#37096;&#20998;&#65292;&#24182;&#25552;&#20379;&#20102;&#26550;&#26500;&#30340;&#27010;&#29575;&#24615;&#25551;&#36848;&#12290;</title><link>http://arxiv.org/abs/2308.08714</link><description>&lt;p&gt;
&#25968;&#23398;&#25512;&#29702;&#30340;&#26550;&#26500;&#30340;&#27010;&#29575;&#24615;&#32467;&#26524;&#19982;&#35748;&#30693;&#20132;&#26367;&#65288;arXiv:2308.08714v1[cs.AI]&#65289;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Results on the Architecture of Mathematical Reasoning Aligned by Cognitive Alternation. (arXiv:2308.08714v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08714
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#33021;&#22815;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#30340;&#26426;&#22120;&#65292;&#23558;&#23450;&#37327;&#25512;&#29702;&#31995;&#32479;&#20998;&#20026;&#24605;&#32500;&#36807;&#31243;&#21644;&#35748;&#30693;&#36807;&#31243;&#20004;&#37096;&#20998;&#65292;&#24182;&#25552;&#20379;&#20102;&#26550;&#26500;&#30340;&#27010;&#29575;&#24615;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35774;&#24819;&#19968;&#21488;&#33021;&#22815;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#30340;&#26426;&#22120;&#12290;&#23558;&#23450;&#37327;&#25512;&#29702;&#31995;&#32479;&#20998;&#20026;&#24605;&#32500;&#36807;&#31243;&#21644;&#35748;&#30693;&#36807;&#31243;&#20004;&#37096;&#20998;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26550;&#26500;&#30340;&#27010;&#29575;&#24615;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
We envision a machine capable of solving mathematical problems. Dividing the quantitative reasoning system into two parts: thought processes and cognitive processes, we provide probabilistic descriptions of the architecture.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20005;&#35880;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#24403;&#21069;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#36827;&#34892;&#35814;&#32454;&#35780;&#20272;&#26469;&#25506;&#35752;&#20154;&#24037;&#26234;&#33021;&#30340;&#24847;&#35782;&#38382;&#39064;&#12290;&#30740;&#31350;&#20013;&#23545;&#20960;&#31181;&#31185;&#23398;&#24847;&#35782;&#29702;&#35770;&#36827;&#34892;&#27010;&#36848;&#65292;&#24182;&#36890;&#36807;&#35745;&#31639;&#26041;&#27861;&#25512;&#23548;&#20986;&#24847;&#35782;&#30340;&#8220;&#25351;&#31034;&#24615;&#29305;&#24449;&#8221;&#12290;&#20998;&#26512;&#32467;&#26524;&#34920;&#26126;&#30446;&#21069;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#23578;&#19981;&#20855;&#22791;&#24847;&#35782;&#65292;&#20294;&#24314;&#31435;&#20855;&#26377;&#24847;&#35782;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#24182;&#26080;&#26126;&#26174;&#30340;&#38556;&#30861;&#12290;</title><link>http://arxiv.org/abs/2308.08708</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#24847;&#35782;&#65306;&#26469;&#33258;&#24847;&#35782;&#31185;&#23398;&#30340;&#27934;&#35265;
&lt;/p&gt;
&lt;p&gt;
Consciousness in Artificial Intelligence: Insights from the Science of Consciousness. (arXiv:2308.08708v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08708
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20005;&#35880;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#24403;&#21069;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#36827;&#34892;&#35814;&#32454;&#35780;&#20272;&#26469;&#25506;&#35752;&#20154;&#24037;&#26234;&#33021;&#30340;&#24847;&#35782;&#38382;&#39064;&#12290;&#30740;&#31350;&#20013;&#23545;&#20960;&#31181;&#31185;&#23398;&#24847;&#35782;&#29702;&#35770;&#36827;&#34892;&#27010;&#36848;&#65292;&#24182;&#36890;&#36807;&#35745;&#31639;&#26041;&#27861;&#25512;&#23548;&#20986;&#24847;&#35782;&#30340;&#8220;&#25351;&#31034;&#24615;&#29305;&#24449;&#8221;&#12290;&#20998;&#26512;&#32467;&#26524;&#34920;&#26126;&#30446;&#21069;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#23578;&#19981;&#20855;&#22791;&#24847;&#35782;&#65292;&#20294;&#24314;&#31435;&#20855;&#26377;&#24847;&#35782;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#24182;&#26080;&#26126;&#26174;&#30340;&#38556;&#30861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#25110;&#36817;&#26399;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#26159;&#21542;&#33021;&#20855;&#26377;&#24847;&#35782;&#25104;&#20026;&#31185;&#23398;&#30028;&#20851;&#27880;&#30340;&#35805;&#39064;&#65292;&#20063;&#24341;&#36215;&#20102;&#20844;&#20247;&#30340;&#25285;&#24551;&#12290;&#26412;&#25253;&#21578;&#25552;&#20986;&#24182;&#20030;&#20363;&#20102;&#19968;&#31181;&#20005;&#35880;&#19988;&#32463;&#39564;&#22522;&#30784;&#30340;&#20154;&#24037;&#26234;&#33021;&#24847;&#35782;&#26041;&#27861;&#65306;&#26681;&#25454;&#25105;&#20204;&#30446;&#21069;&#26368;&#21487;&#20449;&#30340;&#31070;&#32463;&#31185;&#23398;&#29702;&#35770;&#23545;&#29616;&#26377;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#36827;&#34892;&#35814;&#32454;&#35780;&#20272;&#12290;&#25105;&#20204;&#27010;&#36848;&#20102;&#20960;&#31181;&#24191;&#27867;&#35748;&#21487;&#30340;&#31185;&#23398;&#24847;&#35782;&#29702;&#35770;&#65292;&#21253;&#25324;&#24490;&#29615;&#22788;&#29702;&#29702;&#35770;&#12289;&#20840;&#23616;&#24037;&#20316;&#31354;&#38388;&#29702;&#35770;&#12289;&#39640;&#38454;&#29702;&#35770;&#12289;&#39044;&#27979;&#22788;&#29702;&#29702;&#35770;&#21644;&#27880;&#24847;&#27169;&#24335;&#29702;&#35770;&#12290;&#20174;&#36825;&#20123;&#29702;&#35770;&#20013;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#19968;&#20123;&#24847;&#35782;&#30340;&#8220;&#25351;&#31034;&#24615;&#29305;&#24449;&#8221;&#65292;&#24182;&#36890;&#36807;&#35745;&#31639;&#26041;&#27861;&#26469;&#35780;&#20272;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#26159;&#21542;&#20855;&#22791;&#36825;&#20123;&#29305;&#24449;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#25351;&#31034;&#24615;&#29305;&#24449;&#26469;&#35780;&#20272;&#20102;&#20960;&#20010;&#36817;&#26399;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#31995;&#32479;&#22914;&#20309;&#23454;&#29616;&#36825;&#20123;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#30446;&#21069;&#27809;&#26377;&#29616;&#26377;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20855;&#26377;&#24847;&#35782;&#65292;&#20294;&#21516;&#26102;&#20063;&#26174;&#31034;&#20986;&#27809;&#26377;&#26126;&#26174;&#30340;&#24314;&#31435;&#20855;&#26377;&#24847;&#35782;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#38556;&#30861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Whether current or near-term AI systems could be conscious is a topic of scientific interest and increasing public concern. This report argues for, and exemplifies, a rigorous and empirically grounded approach to AI consciousness: assessing existing AI systems in detail, in light of our best-supported neuroscientific theories of consciousness. We survey several prominent scientific theories of consciousness, including recurrent processing theory, global workspace theory, higher-order theories, predictive processing, and attention schema theory. From these theories we derive "indicator properties" of consciousness, elucidated in computational terms that allow us to assess AI systems for these properties. We use these indicator properties to assess several recent AI systems, and we discuss how future systems might implement them. Our analysis suggests that no current AI systems are conscious, but also shows that there are no obvious barriers to building conscious AI systems.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22797;&#26434;&#39550;&#39542;&#29615;&#22659;&#20013;&#30340;&#24322;&#24120;&#20998;&#21106;&#30340;&#22810;&#31890;&#24230;&#36328;&#39046;&#22495;&#23545;&#40784;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#28304;&#39046;&#22495;&#23545;&#25239;&#35757;&#32451;&#21644;&#36328;&#39046;&#22495;&#24322;&#24120;&#24863;&#30693;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#26222;&#36866;&#24615;&#65292;&#24182;&#26080;&#32541;&#38598;&#25104;&#20102;&#22810;&#39046;&#22495;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2308.08696</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#31890;&#24230;&#36328;&#39046;&#22495;&#23545;&#40784;&#25913;&#36827;&#24322;&#24120;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Improving Anomaly Segmentation with Multi-Granularity Cross-Domain Alignment. (arXiv:2308.08696v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08696
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22797;&#26434;&#39550;&#39542;&#29615;&#22659;&#20013;&#30340;&#24322;&#24120;&#20998;&#21106;&#30340;&#22810;&#31890;&#24230;&#36328;&#39046;&#22495;&#23545;&#40784;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#28304;&#39046;&#22495;&#23545;&#25239;&#35757;&#32451;&#21644;&#36328;&#39046;&#22495;&#24322;&#24120;&#24863;&#30693;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#26222;&#36866;&#24615;&#65292;&#24182;&#26080;&#32541;&#38598;&#25104;&#20102;&#22810;&#39046;&#22495;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#20998;&#21106;&#22312;&#35782;&#21035;&#22270;&#20687;&#20013;&#30340;&#24322;&#24120;&#23545;&#35937;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#36825;&#26377;&#21161;&#20110;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#36947;&#36335;&#24322;&#24120;&#26816;&#27979;&#12290;&#23613;&#31649;&#29616;&#26377;&#26041;&#27861;&#22312;&#20351;&#29992;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#36827;&#34892;&#24322;&#24120;&#20998;&#21106;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#65292;&#20294;&#24448;&#24448;&#24573;&#35270;&#20102;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#19982;&#30495;&#23454;&#27979;&#35797;&#25968;&#25454;&#20043;&#38388;&#30340;&#39046;&#22495;&#24046;&#24322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#29992;&#20110;&#22797;&#26434;&#39550;&#39542;&#29615;&#22659;&#20013;&#30340;&#24322;&#24120;&#20998;&#21106;&#30340;&#22810;&#31890;&#24230;&#36328;&#39046;&#22495;&#23545;&#40784;&#65288;MGCDA&#65289;&#26694;&#26550;&#12290;&#23427;&#29420;&#29305;&#22320;&#32467;&#21512;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#28304;&#39046;&#22495;&#23545;&#25239;&#35757;&#32451;&#65288;MDAT&#65289;&#27169;&#22359;&#21644;&#19968;&#31181;&#26032;&#39062;&#30340;&#36328;&#39046;&#22495;&#24322;&#24120;&#24863;&#30693;&#23545;&#27604;&#23398;&#20064;&#65288;CACL&#65289;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#26222;&#36866;&#24615;&#65292;&#26080;&#32541;&#22320;&#22312;&#22330;&#26223;&#21644;&#26679;&#26412;&#32423;&#21035;&#38598;&#25104;&#22810;&#39046;&#22495;&#25968;&#25454;&#12290;&#22810;&#28304;&#39046;&#22495;&#23545;&#25239;&#25439;&#22833;&#21644;&#21160;&#24577;&#26631;&#31614;&#24179;&#28369;&#31574;&#30053;&#34987;&#25972;&#21512;&#21040;MDAT&#27169;&#22359;&#20013;&#65292;&#20197;&#20419;&#36827;&#22312;&#22330;&#26223;&#32423;&#21035;&#19978;&#33719;&#24471;&#39046;&#22495;&#19981;&#21464;&#29305;&#24449;&#65292;&#36890;&#36807;&#23545;&#25239;&#29983;&#25104;&#39046;&#22495;&#19981;&#21464;&#30340;&#29305;&#24449;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly segmentation plays a crucial role in identifying anomalous objects within images, which facilitates the detection of road anomalies for autonomous driving. Although existing methods have shown impressive results in anomaly segmentation using synthetic training data, the domain discrepancies between synthetic training data and real test data are often neglected. To address this issue, the Multi-Granularity Cross-Domain Alignment (MGCDA) framework is proposed for anomaly segmentation in complex driving environments. It uniquely combines a new Multi-source Domain Adversarial Training (MDAT) module and a novel Cross-domain Anomaly-aware Contrastive Learning (CACL) method to boost the generality of the model, seamlessly integrating multi-domain data at both scene and sample levels. Multi-source domain adversarial loss and a dynamic label smoothing strategy are integrated into the MDAT module to facilitate the acquisition of domain-invariant features at the scene level, through adver
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PiZero&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#20195;&#29702;&#33021;&#22815;&#22312;&#33258;&#24049;&#21019;&#24314;&#30340;&#25277;&#35937;&#25628;&#32034;&#31354;&#38388;&#20013;&#36827;&#34892;&#39640;&#32423;&#35268;&#21010;&#65292;&#19981;&#21463;&#30495;&#23454;&#29615;&#22659;&#38480;&#21046;&#65292;&#21487;&#22312;&#20219;&#24847;&#26102;&#38388;&#23610;&#24230;&#36827;&#34892;&#35268;&#21010;&#65292;&#24182;&#22788;&#29702;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#21644;&#37096;&#20998;&#21487;&#35266;&#23519;&#24615;&#30340;&#35774;&#32622;&#12290;&#22312;&#22810;&#20010;&#39046;&#22495;&#30340;&#23454;&#39564;&#20013;&#65292;&#35813;&#26041;&#27861;&#32988;&#36807;&#21487;&#27604;&#30340;&#20808;&#21069;&#26041;&#27861;&#32780;&#26080;&#38656;&#20551;&#35774;&#35775;&#38382;&#29615;&#22659;&#27169;&#25311;&#22120;&#12290;</title><link>http://arxiv.org/abs/2308.08693</link><description>&lt;p&gt;
&#35745;&#21010;&#22312;&#24819;&#35937;&#20013;&#65306;&#22522;&#20110;&#23398;&#20064;&#25277;&#35937;&#25628;&#32034;&#31354;&#38388;&#30340;&#39640;&#32423;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Planning in the imagination: High-level planning on learned abstract search spaces. (arXiv:2308.08693v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08693
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PiZero&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#20195;&#29702;&#33021;&#22815;&#22312;&#33258;&#24049;&#21019;&#24314;&#30340;&#25277;&#35937;&#25628;&#32034;&#31354;&#38388;&#20013;&#36827;&#34892;&#39640;&#32423;&#35268;&#21010;&#65292;&#19981;&#21463;&#30495;&#23454;&#29615;&#22659;&#38480;&#21046;&#65292;&#21487;&#22312;&#20219;&#24847;&#26102;&#38388;&#23610;&#24230;&#36827;&#34892;&#35268;&#21010;&#65292;&#24182;&#22788;&#29702;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#21644;&#37096;&#20998;&#21487;&#35266;&#23519;&#24615;&#30340;&#35774;&#32622;&#12290;&#22312;&#22810;&#20010;&#39046;&#22495;&#30340;&#23454;&#39564;&#20013;&#65292;&#35813;&#26041;&#27861;&#32988;&#36807;&#21487;&#27604;&#30340;&#20808;&#21069;&#26041;&#27861;&#32780;&#26080;&#38656;&#20551;&#35774;&#35775;&#38382;&#29615;&#22659;&#27169;&#25311;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;PiZero&#65292;&#23427;&#20351;&#20195;&#29702;&#33021;&#22815;&#22312;&#33258;&#24049;&#21019;&#24314;&#30340;&#25277;&#35937;&#25628;&#32034;&#31354;&#38388;&#20013;&#36827;&#34892;&#35745;&#21010;&#65292;&#35813;&#25628;&#32034;&#31354;&#38388;&#19982;&#30495;&#23454;&#29615;&#22659;&#23436;&#20840;&#35299;&#32806;&#12290;&#19982;&#20808;&#21069;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#36825;&#20351;&#24471;&#20195;&#29702;&#33021;&#22815;&#20197;&#20219;&#24847;&#26102;&#38388;&#23610;&#24230;&#36827;&#34892;&#39640;&#32423;&#35268;&#21010;&#65292;&#24182;&#20197;&#22797;&#21512;&#25110;&#26102;&#38388;&#25193;&#23637;&#21160;&#20316;&#30340;&#24418;&#24335;&#36827;&#34892;&#25512;&#29702;&#65292;&#36825;&#22312;&#38656;&#35201;&#25191;&#34892;&#22823;&#37327;&#22522;&#26412;&#24494;&#25805;&#20316;&#20197;&#25191;&#34892;&#30456;&#20851;&#23439;&#25805;&#20316;&#30340;&#29615;&#22659;&#20013;&#38750;&#24120;&#26377;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#21487;&#27604;&#30340;&#20808;&#21069;&#26041;&#27861;&#26356;&#36890;&#29992;&#65292;&#22240;&#20026;&#23427;&#22788;&#29702;&#20855;&#26377;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#21644;&#37096;&#20998;&#21487;&#35266;&#23519;&#24615;&#30340;&#35774;&#32622;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#39046;&#22495;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21253;&#25324;&#23548;&#33322;&#20219;&#21153;&#21644;Sokoban&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#23427;&#22312;&#27809;&#26377;&#20551;&#35774;&#35775;&#38382;&#29615;&#22659;&#27169;&#25311;&#22120;&#30340;&#24773;&#20917;&#19979;&#32988;&#36807;&#21487;&#27604;&#30340;&#20808;&#21069;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new method, called PiZero, that gives an agent the ability to plan in an abstract search space of its own creation that is completely decoupled from the real environment. Unlike prior approaches, this enables the agent to perform high-level planning at arbitrary timescales and reason in terms of compound or temporally-extended actions, which can be useful in environments where large numbers of base-level micro-actions are needed to perform relevant macro-actions. In addition, our method is more general than comparable prior methods because it handles settings with continuous action spaces and partial observability. We evaluate our method on multiple domains, including navigation tasks and Sokoban. Experimentally, it outperforms comparable prior methods without assuming access to an environment simulator.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32039;&#20945;&#23884;&#20837;&#32467;&#26500;&#65292;&#36890;&#36807;&#29306;&#29298;&#37096;&#20998;&#20934;&#30830;&#24230;&#65292;&#20943;&#23569;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#23384;&#21344;&#29992;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#32467;&#26500;&#21487;&#20197;&#23454;&#29616;&#36229;&#36807;99.8%&#30340;&#21387;&#32553;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.08688</link><description>&lt;p&gt;
&#36731;&#37327;&#32423;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#23376;&#31354;&#38388;&#23884;&#20837;&#36827;&#34892;&#36866;&#24212;&#24615;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Lightweight Adaptation of Neural Language Models via Subspace Embedding. (arXiv:2308.08688v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08688
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32039;&#20945;&#23884;&#20837;&#32467;&#26500;&#65292;&#36890;&#36807;&#29306;&#29298;&#37096;&#20998;&#20934;&#30830;&#24230;&#65292;&#20943;&#23569;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#23384;&#21344;&#29992;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#32467;&#26500;&#21487;&#20197;&#23454;&#29616;&#36229;&#36807;99.8%&#30340;&#21387;&#32553;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#31070;&#32463;&#35789;&#23884;&#20837;&#36890;&#24120;&#20381;&#36182;&#20110;&#26356;&#20016;&#23500;&#30340;&#35789;&#27719;&#22810;&#26679;&#24615;&#12290;&#28982;&#32780;&#65292;&#35821;&#35328;&#27169;&#22411;&#20542;&#21521;&#20110;&#36890;&#36807;&#21333;&#35789;&#23884;&#20837;&#21442;&#25968;&#26469;&#35206;&#30422;&#20027;&#35201;&#35789;&#27719;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#36890;&#24120;&#35206;&#30422;&#20854;&#25972;&#20307;&#23398;&#20064;&#21442;&#25968;&#20013;&#30340;&#37325;&#35201;&#37096;&#20998;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#26469;&#35828;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32039;&#20945;&#23884;&#20837;&#32467;&#26500;&#65292;&#36890;&#36807;&#29306;&#29298;&#39640;&#36798;4%&#30340;&#32477;&#23545;&#20934;&#30830;&#24230;&#26469;&#20943;&#23569;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#23384;&#21344;&#29992;&#12290;&#23884;&#20837;&#21521;&#37327;&#30340;&#37325;&#24314;&#36981;&#24490;&#19968;&#32452;&#23376;&#31354;&#38388;&#23884;&#20837;&#21644;&#36890;&#36807;&#20174;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26631;&#35760;&#20043;&#38388;&#30340;&#19978;&#19979;&#25991;&#20851;&#31995;&#36827;&#34892;&#30340;&#20998;&#37197;&#36807;&#31243;&#12290;&#23376;&#31354;&#38388;&#23884;&#20837;&#32467;&#26500;&#36866;&#24212;&#20102;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#22312;&#30456;&#20284;&#24615;&#21644;&#25991;&#26412;&#34164;&#28085;&#20219;&#21153;&#12289;&#21477;&#23376;&#21644;&#37322;&#20041;&#20219;&#21153;&#20013;&#35780;&#20272;&#25105;&#20204;&#30340;&#32039;&#20945;&#23884;&#20837;&#32467;&#26500;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;&#19982;&#21407;&#22987;e&#30456;&#27604;&#65292;&#23376;&#31354;&#38388;&#23884;&#20837;&#23454;&#29616;&#20102;&#36229;&#36807;99.8%&#30340;&#21387;&#32553;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional neural word embeddings are usually dependent on a richer diversity of vocabulary. However, the language models recline to cover major vocabularies via the word embedding parameters, in particular, for multilingual language models that generally cover a significant part of their overall learning parameters. In this work, we present a new compact embedding structure to reduce the memory footprint of the pre-trained language models with a sacrifice of up to 4% absolute accuracy. The embeddings vectors reconstruction follows a set of subspace embeddings and an assignment procedure via the contextual relationship among tokens from pre-trained language models. The subspace embedding structure calibrates to masked language models, to evaluate our compact embedding structure on similarity and textual entailment tasks, sentence and paraphrase tasks. Our experimental evaluation shows that the subspace embeddings achieve compression rates beyond 99.8% in comparison with the original e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#36807;&#25311;&#21512;&#25351;&#25968;&#65288;OI&#65289;&#65292;&#36890;&#36807;&#23545;&#20083;&#33146;&#36229;&#22768;&#22270;&#20687;&#25968;&#25454;&#38598;&#21644;MNIST&#25968;&#25454;&#38598;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#20351;&#29992;&#20102;&#22810;&#31181;&#26550;&#26500;&#65292;&#23637;&#31034;&#20102;OI&#30340;&#23454;&#29992;&#24615;&#21644;&#21306;&#20998;&#33021;&#21147;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19981;&#21516;&#26550;&#26500;&#30340;&#36807;&#25311;&#21512;&#34892;&#20026;&#23384;&#22312;&#24046;&#24322;&#65292;&#24182;&#24378;&#35843;&#20102;&#25968;&#25454;&#22686;&#24378;&#23545;&#20110;&#36739;&#23567;&#21644;&#26356;&#19987;&#19994;&#30340;&#25968;&#25454;&#38598;&#30340;&#32531;&#35299;&#24433;&#21709;&#12290;OI&#20026;&#35299;&#20915;&#36807;&#25311;&#21512;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#36884;&#24452;&#12290;</title><link>http://arxiv.org/abs/2308.08682</link><description>&lt;p&gt;
&#37327;&#21270;&#36807;&#25311;&#21512;: &#24341;&#20837;&#36807;&#25311;&#21512;&#25351;&#25968;
&lt;/p&gt;
&lt;p&gt;
Quantifying Overfitting: Introducing the Overfitting Index. (arXiv:2308.08682v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08682
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#36807;&#25311;&#21512;&#25351;&#25968;&#65288;OI&#65289;&#65292;&#36890;&#36807;&#23545;&#20083;&#33146;&#36229;&#22768;&#22270;&#20687;&#25968;&#25454;&#38598;&#21644;MNIST&#25968;&#25454;&#38598;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#20351;&#29992;&#20102;&#22810;&#31181;&#26550;&#26500;&#65292;&#23637;&#31034;&#20102;OI&#30340;&#23454;&#29992;&#24615;&#21644;&#21306;&#20998;&#33021;&#21147;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19981;&#21516;&#26550;&#26500;&#30340;&#36807;&#25311;&#21512;&#34892;&#20026;&#23384;&#22312;&#24046;&#24322;&#65292;&#24182;&#24378;&#35843;&#20102;&#25968;&#25454;&#22686;&#24378;&#23545;&#20110;&#36739;&#23567;&#21644;&#26356;&#19987;&#19994;&#30340;&#25968;&#25454;&#38598;&#30340;&#32531;&#35299;&#24433;&#21709;&#12290;OI&#20026;&#35299;&#20915;&#36807;&#25311;&#21512;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24555;&#36895;&#21457;&#23637;&#30340;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#30830;&#20445;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#12290;&#36807;&#25311;&#21512;&#26159;&#25351;&#27169;&#22411;&#22312;&#35757;&#32451;&#25968;&#25454;&#19978;&#34920;&#29616;&#33391;&#22909;&#20294;&#22312;&#26410;&#35265;&#25968;&#25454;&#19978;&#34920;&#29616;&#19981;&#20339;&#30340;&#29616;&#35937;&#65292;&#19968;&#30452;&#26159;&#20010;&#19981;&#23481;&#24573;&#35270;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#36807;&#25311;&#21512;&#25351;&#25968;&#65288;OI&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#23450;&#37327;&#35780;&#20272;&#27169;&#22411;&#30340;&#36807;&#25311;&#21512;&#20542;&#21521;&#12290;&#36890;&#36807;&#23545;&#20083;&#33146;&#36229;&#22768;&#22270;&#20687;&#25968;&#25454;&#38598;&#65288;BUS&#65289;&#21644;MNIST&#25968;&#25454;&#38598;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#20351;&#29992;&#20102;MobileNet&#12289;U-Net&#12289;ResNet&#12289;Darknet&#21644;ViT-32&#31561;&#26550;&#26500;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;OI&#30340;&#23454;&#29992;&#24615;&#21644;&#21306;&#20998;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#31361;&#20986;&#20102;&#19981;&#21516;&#26550;&#26500;&#20043;&#38388;&#30340;&#36807;&#25311;&#21512;&#34892;&#20026;&#30340;&#21464;&#21270;&#65292;&#24182;&#24378;&#35843;&#20102;&#25968;&#25454;&#22686;&#24378;&#23545;&#36739;&#23567;&#21644;&#26356;&#19987;&#19994;&#30340;&#25968;&#25454;&#38598;&#30340;&#32531;&#35299;&#24433;&#21709;&#12290;ViT-32&#22312;MNIST&#19978;&#30340;&#34920;&#29616;&#36827;&#19968;&#27493;&#24378;&#35843;&#20102;&#26576;&#20123;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#25968;&#25454;&#38598;&#30340;&#20840;&#38754;&#24615;&#12290;&#36890;&#36807;&#25552;&#20379;&#23458;&#35266;&#35270;&#35282;&#26469;&#35780;&#20272;&#36807;&#25311;&#21512;&#65292;OI&#20026;&#35299;&#20915;&#36807;&#25311;&#21512;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the rapidly evolving domain of machine learning, ensuring model generalizability remains a quintessential challenge. Overfitting, where a model exhibits superior performance on training data but falters on unseen data, is a recurrent concern. This paper introduces the Overfitting Index (OI), a novel metric devised to quantitatively assess a model's tendency to overfit. Through extensive experiments on the Breast Ultrasound Images Dataset (BUS) and the MNIST dataset using architectures such as MobileNet, U-Net, ResNet, Darknet, and ViT-32, we illustrate the utility and discernment of the OI. Our results underscore the variable overfitting behaviors across architectures and highlight the mitigative impact of data augmentation, especially on smaller and more specialized datasets. The ViT-32's performance on MNIST further emphasizes the robustness of certain models and the dataset's comprehensive nature. By providing an objective lens to gauge overfitting, the OI offers a promising aven
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#20174;&#32500;&#22522;&#30334;&#31185;&#29983;&#25104;&#30340;&#19968;&#32452;&#26126;&#30830;&#38382;&#39064;&#30340;&#25968;&#25454;&#24211;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22238;&#31572;&#27169;&#31946;&#38382;&#39064;&#30340;&#26032;&#25216;&#26415;&#65292;&#22312;&#22238;&#31572;&#24615;&#33021;&#21644;&#27495;&#20041;&#38382;&#39064;&#28040;&#38500;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2308.08661</link><description>&lt;p&gt;
&#20351;&#29992;&#38382;&#39064;&#12289;&#31572;&#26696;&#21644;&#20462;&#35746;&#30340;&#25968;&#25454;&#24211;&#22238;&#31572;&#27169;&#31946;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Answering Ambiguous Questions with a Database of Questions, Answers, and Revisions. (arXiv:2308.08661v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08661
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#20174;&#32500;&#22522;&#30334;&#31185;&#29983;&#25104;&#30340;&#19968;&#32452;&#26126;&#30830;&#38382;&#39064;&#30340;&#25968;&#25454;&#24211;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22238;&#31572;&#27169;&#31946;&#38382;&#39064;&#30340;&#26032;&#25216;&#26415;&#65292;&#22312;&#22238;&#31572;&#24615;&#33021;&#21644;&#27495;&#20041;&#38382;&#39064;&#28040;&#38500;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#24320;&#25918;&#39046;&#22495;&#30340;&#38382;&#39064;&#37117;&#27809;&#26377;&#26126;&#30830;&#30340;&#35268;&#23450;&#65292;&#22240;&#27492;&#21487;&#33021;&#26377;&#22810;&#20010;&#21487;&#33021;&#30340;&#31572;&#26696;&#65292;&#27599;&#20010;&#31572;&#26696;&#22312;&#38382;&#39064;&#30340;&#19981;&#21516;&#35299;&#37322;&#19979;&#37117;&#26159;&#27491;&#30830;&#30340;&#12290;&#22238;&#31572;&#36825;&#26679;&#27169;&#31946;&#30340;&#38382;&#39064;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#20174;&#22810;&#20010;&#27573;&#33853;&#20013;&#26816;&#32034;&#24182;&#25512;&#29702;&#20986;&#19981;&#21516;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22238;&#31572;&#27169;&#31946;&#38382;&#39064;&#30340;&#26368;&#26032;&#25216;&#26415;&#65292;&#21033;&#29992;&#20174;&#32500;&#22522;&#30334;&#31185;&#29983;&#25104;&#30340;&#19968;&#32452;&#26126;&#30830;&#38382;&#39064;&#30340;&#25968;&#25454;&#24211;&#12290;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;ASQA&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21484;&#22238;&#29575;&#27979;&#37327;&#19978;&#25552;&#39640;&#20102;15%&#65288;&#30456;&#23545;&#25913;&#36827;&#65289;&#65292;&#22312;&#35780;&#20272;&#20174;&#39044;&#27979;&#36755;&#20986;&#20013;&#28040;&#38500;&#27495;&#20041;&#38382;&#39064;&#30340;&#24230;&#37327;&#19978;&#25552;&#39640;&#20102;10%&#12290;&#20174;&#29983;&#25104;&#30340;&#38382;&#39064;&#25968;&#25454;&#24211;&#20013;&#26816;&#32034;&#36824;&#22823;&#22823;&#25552;&#39640;&#20102;&#22810;&#26679;&#30340;&#27573;&#33853;&#26816;&#32034;&#65288;&#36890;&#36807;&#38388;&#25509;&#21305;&#37197;&#29992;&#25143;&#38382;&#39064;q&#21040;&#27573;&#33853;p&#65292;&#36890;&#36807;&#20174;p&#29983;&#25104;&#30340;&#38382;&#39064;q'&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many open-domain questions are under-specified and thus have multiple possible answers, each of which is correct under a different interpretation of the question. Answering such ambiguous questions is challenging, as it requires retrieving and then reasoning about diverse information from multiple passages. We present a new state-of-the-art for answering ambiguous questions that exploits a database of unambiguous questions generated from Wikipedia. On the challenging ASQA benchmark, which requires generating long-form answers that summarize the multiple answers to an ambiguous question, our method improves performance by 15% (relative improvement) on recall measures and 10% on measures which evaluate disambiguating questions from predicted outputs. Retrieving from the database of generated questions also gives large improvements in diverse passage retrieval (by matching user questions q to passages p indirectly, via questions q' generated from p).
&lt;/p&gt;</description></item><item><title>AdaptEx&#26159;&#19968;&#20010;&#33258;&#21161;&#19978;&#19979;&#25991;&#36172;&#21338;&#24179;&#21488;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#33218;&#36172;&#21338;&#31639;&#27861;&#20010;&#24615;&#21270;&#29992;&#25143;&#20307;&#39564;&#24182;&#25552;&#20379;&#26368;&#20248;&#35299;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#20256;&#32479;&#27979;&#35797;&#26041;&#27861;&#30340;&#25104;&#26412;&#21644;&#26102;&#38388;&#12290;&#23427;&#33021;&#22815;&#22312;&#19981;&#26029;&#21464;&#21270;&#30340;&#20869;&#23481;&#21644;&#8220;&#20919;&#21551;&#21160;&#8221;&#24773;&#20917;&#19979;&#24555;&#36895;&#36845;&#20195;&#12290;</title><link>http://arxiv.org/abs/2308.08650</link><description>&lt;p&gt;
AdaptEx&#65306;&#19968;&#20010;&#33258;&#21161;&#19978;&#19979;&#25991;&#36172;&#21338;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
AdaptEx: A Self-Service Contextual Bandit Platform. (arXiv:2308.08650v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08650
&lt;/p&gt;
&lt;p&gt;
AdaptEx&#26159;&#19968;&#20010;&#33258;&#21161;&#19978;&#19979;&#25991;&#36172;&#21338;&#24179;&#21488;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#33218;&#36172;&#21338;&#31639;&#27861;&#20010;&#24615;&#21270;&#29992;&#25143;&#20307;&#39564;&#24182;&#25552;&#20379;&#26368;&#20248;&#35299;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#20256;&#32479;&#27979;&#35797;&#26041;&#27861;&#30340;&#25104;&#26412;&#21644;&#26102;&#38388;&#12290;&#23427;&#33021;&#22815;&#22312;&#19981;&#26029;&#21464;&#21270;&#30340;&#20869;&#23481;&#21644;&#8220;&#20919;&#21551;&#21160;&#8221;&#24773;&#20917;&#19979;&#24555;&#36895;&#36845;&#20195;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;AdaptEx&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;Expedia Group&#24191;&#27867;&#20351;&#29992;&#30340;&#33258;&#21161;&#19978;&#19979;&#25991;&#36172;&#21338;&#24179;&#21488;&#65292;&#23427;&#21033;&#29992;&#22810;&#33218;&#36172;&#21338;&#31639;&#27861;&#20197;&#35268;&#27169;&#21270;&#30340;&#26041;&#24335;&#20010;&#24615;&#21270;&#29992;&#25143;&#20307;&#39564;&#12290;AdaptEx&#32771;&#34385;&#20102;&#27599;&#20010;&#35775;&#38382;&#32773;&#30340;&#29420;&#29305;&#19978;&#19979;&#25991;&#65292;&#36873;&#25321;&#20102;&#26368;&#20248;&#30340;&#21464;&#20307;&#65292;&#24182;&#33021;&#22815;&#24555;&#36895;&#23398;&#20064;&#27599;&#27425;&#20114;&#21160;&#12290;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#26082;&#33021;&#25913;&#21892;&#29992;&#25143;&#20307;&#39564;&#65292;&#21516;&#26102;&#21448;&#33021;&#26368;&#22823;&#38480;&#24230;&#22320;&#20943;&#23569;&#20256;&#32479;&#27979;&#35797;&#26041;&#27861;&#25152;&#38656;&#30340;&#25104;&#26412;&#21644;&#26102;&#38388;&#12290;&#35813;&#24179;&#21488;&#33021;&#22815;&#22312;&#20869;&#23481;&#19981;&#26029;&#21464;&#21270;&#21644;&#25345;&#32493;&#8220;&#20919;&#21551;&#21160;&#8221;&#24773;&#20917;&#19979;&#65292;&#20248;&#38597;&#22320;&#24555;&#36895;&#36845;&#20195;&#26397;&#30528;&#26368;&#20248;&#35299;&#21069;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents AdaptEx, a self-service contextual bandit platform widely used at Expedia Group, that leverages multi-armed bandit algorithms to personalize user experiences at scale. AdaptEx considers the unique context of each visitor to select the optimal variants and learns quickly from every interaction they make. It offers a powerful solution to improve user experiences while minimizing the costs and time associated with traditional testing methods. The platform unlocks the ability to iterate towards optimal product solutions quickly, even in ever-changing content and continuous "cold start" situations gracefully.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#26497;&#20302;&#20869;&#23384;&#21344;&#29992;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#26694;&#26550;&#65292;&#24182;&#35774;&#35745;&#20102;&#21487;&#36870;SNN&#33410;&#28857;&#21644;&#31616;&#21270;&#30340;&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#20869;&#23384;&#20351;&#29992;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.08649</link><description>&lt;p&gt;
&#23454;&#29616;&#38646;&#20869;&#23384;&#21344;&#29992;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Towards Zero Memory Footprint Spiking Neural Network Training. (arXiv:2308.08649v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08649
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#26497;&#20302;&#20869;&#23384;&#21344;&#29992;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#26694;&#26550;&#65292;&#24182;&#35774;&#35745;&#20102;&#21487;&#36870;SNN&#33410;&#28857;&#21644;&#31616;&#21270;&#30340;&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#20869;&#23384;&#20351;&#29992;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#29983;&#29289;&#21551;&#21457;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#20197;&#31163;&#25955;&#26102;&#38388;&#20107;&#20214;&#65288;&#33033;&#20914;&#65289;&#32780;&#38750;&#36830;&#32493;&#20540;&#22788;&#29702;&#20449;&#24687;&#65292;&#30001;&#20110;&#20854;&#23545;&#30828;&#20214;&#21451;&#22909;&#21644;&#39640;&#33021;&#25928;&#30340;&#29305;&#28857;&#65292;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;SNN&#30340;&#35757;&#32451;&#38656;&#35201;&#22823;&#37327;&#30340;&#20869;&#23384;&#21344;&#29992;&#65292;&#22240;&#20026;&#38656;&#35201;&#39069;&#22806;&#23384;&#20648;&#33033;&#20914;&#25110;&#20107;&#20214;&#65292;&#23548;&#33268;&#22797;&#26434;&#30340;&#32467;&#26500;&#21644;&#21160;&#24577;&#35774;&#32622;&#12290;&#20026;&#20102;&#35299;&#20915;SNN&#35757;&#32451;&#20013;&#30340;&#20869;&#23384;&#38480;&#21046;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#20855;&#26377;&#24322;&#24120;&#20302;&#30340;&#20869;&#23384;&#21344;&#29992;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21487;&#36870;&#30340;SNN&#33410;&#28857;&#65292;&#20445;&#25345;&#39640;&#31934;&#24230;&#65292;&#24182;&#23454;&#29616;&#20102;&#19982;&#24403;&#21069;SNN&#33410;&#28857;&#30456;&#27604;&#30340;58.65&#20493;&#30340;&#20869;&#23384;&#20351;&#29992;&#20943;&#23569;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#31639;&#27861;&#26469;&#31616;&#21270;&#25105;&#20204;&#21487;&#36870;SNN&#33410;&#28857;&#30340;&#21453;&#21521;&#20256;&#25773;&#36807;&#31243;&#12290;&#36825;&#26174;&#33879;&#20943;&#23569;&#20102;&#21453;&#21521;&#28014;&#28857;&#25968;&#36816;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
Biologically-inspired Spiking Neural Networks (SNNs), processing information using discrete-time events known as spikes rather than continuous values, have garnered significant attention due to their hardware-friendly and energy-efficient characteristics. However, the training of SNNs necessitates a considerably large memory footprint, given the additional storage requirements for spikes or events, leading to a complex structure and dynamic setup. In this paper, to address memory constraint in SNN training, we introduce an innovative framework, characterized by a remarkably low memory footprint. We \textbf{(i)} design a reversible SNN node that retains a high level of accuracy. Our design is able to achieve a $\mathbf{58.65\times}$ reduction in memory usage compared to the current SNN node. We \textbf{(ii)} propose a unique algorithm to streamline the backpropagation process of our reversible SNN node. This significantly trims the backward Floating Point Operations Per Second (FLOPs), 
&lt;/p&gt;</description></item><item><title>FedPop&#26159;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#36229;&#21442;&#25968;&#35843;&#20248;&#38382;&#39064;&#30340;&#26032;&#31639;&#27861;&#65292;&#23427;&#37319;&#29992;&#22522;&#20110;&#20154;&#21475;&#30340;&#36827;&#21270;&#31639;&#27861;&#26469;&#20248;&#21270;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#19978;&#30340;&#36229;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2308.08634</link><description>&lt;p&gt;
FedPop: &#32852;&#37030;&#24335;&#22522;&#20110;&#20154;&#21475;&#30340;&#36229;&#21442;&#25968;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
FedPop: Federated Population-based Hyperparameter Tuning. (arXiv:2308.08634v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08634
&lt;/p&gt;
&lt;p&gt;
FedPop&#26159;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#36229;&#21442;&#25968;&#35843;&#20248;&#38382;&#39064;&#30340;&#26032;&#31639;&#27861;&#65292;&#23427;&#37319;&#29992;&#22522;&#20110;&#20154;&#21475;&#30340;&#36827;&#21270;&#31639;&#27861;&#26469;&#20248;&#21270;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#19978;&#30340;&#36229;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#33539;&#24335;&#65292;&#22810;&#20010;&#23458;&#25143;&#31471;&#22312;&#19981;&#38598;&#20013;&#26412;&#22320;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20849;&#21516;&#35757;&#32451;ML&#27169;&#22411;&#12290;&#19982;&#20256;&#32479;&#30340;ML&#27969;&#31243;&#31867;&#20284;&#65292;FL&#20013;&#30340;&#23458;&#25143;&#31471;&#26412;&#22320;&#20248;&#21270;&#21644;&#26381;&#21153;&#22120;&#32858;&#21512;&#36807;&#31243;&#23545;&#36229;&#21442;&#25968;&#65288;HP&#65289;&#30340;&#36873;&#25321;&#38750;&#24120;&#25935;&#24863;&#12290;&#23613;&#31649;&#22312;&#38598;&#20013;&#24335;ML&#20013;&#23545;&#35843;&#20248;HP&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#23558;&#36825;&#20123;&#26041;&#27861;&#24212;&#29992;&#20110;FL&#26102;&#20250;&#20135;&#29983;&#27425;&#20248;&#32467;&#26524;&#12290;&#36825;&#20027;&#35201;&#26159;&#22240;&#20026;&#23427;&#20204;&#30340;&#8220;&#35843;&#20248;&#21518;&#35757;&#32451;&#8221;&#26694;&#26550;&#23545;&#20110;&#35745;&#31639;&#33021;&#21147;&#26377;&#38480;&#30340;FL&#19981;&#21512;&#36866;&#12290;&#34429;&#28982;&#19968;&#20123;&#26041;&#27861;&#24050;&#32463;&#25552;&#20986;&#29992;&#20110;FL&#20013;&#30340;HP&#35843;&#20248;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#20165;&#38480;&#20110;&#23458;&#25143;&#31471;&#26412;&#22320;&#26356;&#26032;&#30340;HP&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32852;&#37030;&#24335;&#22522;&#20110;&#20154;&#21475;&#30340;&#36229;&#21442;&#25968;&#35843;&#20248;&#65288;FedPop&#65289;&#30340;&#26032;&#22411;HP&#35843;&#20248;&#31639;&#27861;&#65292;&#20197;&#35299;&#20915;&#36825;&#20010;&#37325;&#35201;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;FedPop&#37319;&#29992;&#22522;&#20110;&#20154;&#21475;&#30340;&#36827;&#21270;&#31639;&#27861;&#26469;&#20248;&#21270;HP&#65292;&#27492;&#31639;&#27861;&#36866;&#29992;&#20110;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#19978;&#30340;&#21508;&#31181;HP&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a distributed machine learning (ML) paradigm, in which multiple clients collaboratively train ML models without centralizing their local data. Similar to conventional ML pipelines, the client local optimization and server aggregation procedure in FL are sensitive to the hyperparameter (HP) selection. Despite extensive research on tuning HPs for centralized ML, these methods yield suboptimal results when employed in FL. This is mainly because their "training-after-tuning" framework is unsuitable for FL with limited client computation power. While some approaches have been proposed for HP-Tuning in FL, they are limited to the HPs for client local updates. In this work, we propose a novel HP-tuning algorithm, called Federated Population-based Hyperparameter Tuning (FedPop), to address this vital yet challenging problem. FedPop employs population-based evolutionary algorithms to optimize the HPs, which accommodates various HP types at both client and server sides
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22522;&#20110;LSTM&#32593;&#32476;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22635;&#20805;GRACE&#21355;&#26143;&#21152;&#36895;&#35745;&#25968;&#25454;&#38388;&#26029;&#30340;&#26041;&#27861;&#65292;&#24182;&#25104;&#21151;&#39044;&#27979;&#20102;&#19977;&#20010;&#36724;&#19978;&#30340;&#21152;&#36895;&#35745;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2308.08621</link><description>&lt;p&gt;
&#22522;&#20110;LSTM&#30340;GRACE&#21152;&#36895;&#35745;&#25968;&#25454;&#39044;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LSTM-Based Forecasting Model for GRACE Accelerometer Data. (arXiv:2308.08621v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08621
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22522;&#20110;LSTM&#32593;&#32476;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22635;&#20805;GRACE&#21355;&#26143;&#21152;&#36895;&#35745;&#25968;&#25454;&#38388;&#26029;&#30340;&#26041;&#27861;&#65292;&#24182;&#25104;&#21151;&#39044;&#27979;&#20102;&#19977;&#20010;&#36724;&#19978;&#30340;&#21152;&#36895;&#35745;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37325;&#21147;&#24674;&#22797;&#19982;&#27668;&#20505;&#23454;&#39564;&#65288;GRACE&#65289;&#21355;&#26143;&#20219;&#21153;&#20174;2002&#24180;&#21040;2017&#24180;&#65292;&#20026;&#30417;&#27979;&#22320;&#29699;&#37325;&#21147;&#22330;&#21464;&#21270;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#25968;&#25454;&#38598;&#65292;&#20026;&#22320;&#29699;&#29289;&#29702;&#23398;&#21644;&#27700;&#25991;&#23398;&#31561;&#22810;&#31181;&#24212;&#29992;&#25552;&#20379;&#20102;&#25903;&#25345;&#12290;&#38543;&#21518;&#65292;&#22312;2018&#24180;&#65292;GRACE Follow-On&#32487;&#32493;&#36827;&#34892;&#25968;&#25454;&#25910;&#38598;&#12290;&#20174;&#21355;&#26143;&#19978;&#19981;&#21516;&#20202;&#22120;&#30340;&#38598;&#25104;&#23548;&#20986;&#30340;&#26376;&#24230;&#22320;&#29699;&#37325;&#21147;&#22330;&#25968;&#25454;&#65292;&#30001;&#20110;&#22810;&#31181;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#21253;&#25324;GRACE&#20219;&#21153;&#24320;&#22987;&#20197;&#26469;&#26576;&#20123;&#20202;&#22120;&#30340;&#35266;&#27979;&#38388;&#26029;&#65292;&#23384;&#22312;&#19981;&#19968;&#33268;&#24615;&#12290;&#37492;&#20110;&#29616;&#22312;&#24050;&#32463;&#26377;&#20108;&#21313;&#22810;&#24180;&#30340;GRACE&#21644;GRACE Follow-On&#25968;&#25454;&#21487;&#29992;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22635;&#20805;&#25968;&#25454;&#38388;&#26029;&#24182;&#39044;&#27979;GRACE&#21152;&#36895;&#35745;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30528;&#37325;&#20110;&#21152;&#36895;&#35745;&#25968;&#25454;&#65292;&#24182;&#37319;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#32593;&#32476;&#26469;&#35757;&#32451;&#19968;&#20010;&#33021;&#22815;&#39044;&#27979;&#19977;&#20010;&#36724;&#19978;&#21152;&#36895;&#35745;&#25968;&#25454;&#30340;&#27169;&#22411;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#39044;&#22788;&#29702;&#21152;&#36895;&#35745;&#25968;&#25454;&#30340;&#26041;&#27861;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Gravity Recovery and Climate Experiment (GRACE) satellite mission, spanning from 2002 to 2017, has provided a valuable dataset for monitoring variations in Earth's gravity field, enabling diverse applications in geophysics and hydrology. The mission was followed by GRACE Follow-On in 2018, continuing data collection efforts. The monthly Earth gravity field, derived from the integration different instruments onboard satellites, has shown inconsistencies due to various factors, including gaps in observations for certain instruments since the beginning of the GRACE mission.  With over two decades of GRACE and GRACE Follow-On data now available, this paper proposes an approach to fill the data gaps and forecast GRACE accelerometer data. Specifically, we focus on accelerometer data and employ Long Short-Term Memory (LSTM) networks to train a model capable of predicting accelerometer data for all three axes.  In this study, we describe the methodology used to preprocess the accelerometer
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#25216;&#26415;&#8212;&#8212;&#24605;&#32500;&#22270;&#65288;GoT&#65289;&#65292;&#36890;&#36807;&#22312;&#19977;&#20010;&#19981;&#26029;&#21319;&#32423;&#30340;&#25361;&#25112;&#20013;&#30340;&#27979;&#35797;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#27493;&#36923;&#36753;&#25512;&#29702;&#38382;&#39064;&#19978;&#34920;&#29616;&#20248;&#20110;GPT-4&#65292;&#24182;&#19988;&#30456;&#27604;&#26368;&#20808;&#36827;&#30340;&#25552;&#31034;&#26041;&#27861;&#24605;&#32500;&#26641;&#65288;ToT&#65289;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2308.08614</link><description>&lt;p&gt;
&#36890;&#36807;&#26032;&#30340;&#26694;&#26550;&#8212;&#8212;&#24605;&#32500;&#22270;&#65292;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Boosting Logical Reasoning in Large Language Models through a New Framework: The Graph of Thought. (arXiv:2308.08614v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08614
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#25216;&#26415;&#8212;&#8212;&#24605;&#32500;&#22270;&#65288;GoT&#65289;&#65292;&#36890;&#36807;&#22312;&#19977;&#20010;&#19981;&#26029;&#21319;&#32423;&#30340;&#25361;&#25112;&#20013;&#30340;&#27979;&#35797;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#27493;&#36923;&#36753;&#25512;&#29702;&#38382;&#39064;&#19978;&#34920;&#29616;&#20248;&#20110;GPT-4&#65292;&#24182;&#19988;&#30456;&#27604;&#26368;&#20808;&#36827;&#30340;&#25552;&#31034;&#26041;&#27861;&#24605;&#32500;&#26641;&#65288;ToT&#65289;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#22823;&#35268;&#27169;&#27169;&#22411;&#65288;&#22914;GPT-4&#65289;&#30340;&#36827;&#23637;&#23637;&#31034;&#20102;&#22312;&#35299;&#20915;&#26631;&#20934;&#26597;&#35810;&#26041;&#38754;&#30340;&#21331;&#36234;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#38754;&#23545;&#38656;&#35201;&#22810;&#27493;&#36923;&#36753;&#25512;&#29702;&#30340;&#22797;&#26434;&#38382;&#39064;&#26102;&#65292;&#23427;&#20204;&#30340;&#20934;&#30830;&#24615;&#24613;&#21095;&#19979;&#38477;&#12290;&#24403;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#25506;&#32034;&#20102;&#25552;&#31034;&#24037;&#31243;&#39046;&#22495;&#65292;&#20197;&#22686;&#24378;&#36825;&#20123;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#25552;&#31034;&#25216;&#26415;&#65292;&#31216;&#20026;&#8220;&#24605;&#32500;&#22270;&#65288;GoT&#65289;&#8221;&#12290;&#36890;&#36807;&#22312;&#19977;&#20010;&#19981;&#26029;&#21319;&#32423;&#30340;&#25361;&#25112;&#19978;&#36827;&#34892;&#27979;&#35797;&#65306;24&#28857;&#28216;&#25103;&#65292;&#39640;&#38454;&#22810;&#39033;&#24335;&#26041;&#31243;&#30340;&#35299;&#26512;&#65292;&#20197;&#21450;&#36882;&#24402;&#25968;&#21015;&#30340;&#20844;&#24335;&#25512;&#23548;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;GPT-4&#65292;&#22312;&#27599;&#20010;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;$89.7\%$&#12289;$86\%$&#21644;$56\%$&#30340;&#20934;&#30830;&#24615;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#25552;&#31034;&#26041;&#27861;&#8220;&#24605;&#32500;&#26641;&#65288;ToT&#65289;&#8221;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#24179;&#22343;&#20934;&#30830;&#24615;&#25552;&#21319;&#20102;$23\%$&#12289;$24\%$&#21644;$15\%$&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in large-scale models, such as GPT-4, have showcased remarkable capabilities in addressing standard queries. However, when facing complex problems that require multi-step logical reasoning, their accuracy dramatically decreases. Current research has explored the realm of \textit{prompting engineering} to bolster the inferential capacities of these models. Our paper unveils a pioneering prompting technique, dubbed \textit{Graph of Thoughts (GoT)}. Through testing on a trio of escalating challenges: the 24-point game, resolution of high-degree polynomial equations, and derivation of formulas for recursive sequences, our method outperformed GPT-4, achieving accuracy improvements of $89.7\%$, $86\%$, and $56\%$ for each respective task. Moreover, when juxtaposed with the state-of-the-art (SOTA) prompting method, \textit{Tree of Thought (ToT)}, our approach registered an average accuracy boost of $23\%$, $24\%$, and $15\%$.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#28145;&#24230;Q&#32593;&#32476;&#65288;DQN&#65289;&#20248;&#21270;&#20809;&#20239;&#31995;&#32479;&#22312;&#20892;&#19994;&#20013;&#30340;&#23433;&#35013;&#20915;&#31574;&#21046;&#23450;&#12290;&#35813;&#30740;&#31350;&#23545;&#20110;&#25552;&#39640;&#33021;&#28304;&#25928;&#29575;&#12289;&#20943;&#23569;&#29615;&#22659;&#24433;&#21709;&#24182;&#22686;&#21152;&#20892;&#19994;&#21033;&#28070;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2308.08611</link><description>&lt;p&gt;
&#20892;&#19994;&#20013;&#25972;&#21512;&#21487;&#20877;&#29983;&#33021;&#28304;&#65306;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Integrating Renewable Energy in Agriculture: A Deep Reinforcement Learning-based Approach. (arXiv:2308.08611v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08611
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#28145;&#24230;Q&#32593;&#32476;&#65288;DQN&#65289;&#20248;&#21270;&#20809;&#20239;&#31995;&#32479;&#22312;&#20892;&#19994;&#20013;&#30340;&#23433;&#35013;&#20915;&#31574;&#21046;&#23450;&#12290;&#35813;&#30740;&#31350;&#23545;&#20110;&#25552;&#39640;&#33021;&#28304;&#25928;&#29575;&#12289;&#20943;&#23569;&#29615;&#22659;&#24433;&#21709;&#24182;&#22686;&#21152;&#20892;&#19994;&#21033;&#28070;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#28145;&#24230;Q&#32593;&#32476;&#65288;DQN&#65289;&#26469;&#20248;&#21270;&#20892;&#19994;&#39046;&#22495;&#20809;&#20239;&#31995;&#32479;&#23433;&#35013;&#30340;&#20915;&#31574;&#21046;&#23450;&#12290;&#35813;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;DQN&#26694;&#26550;&#65292;&#20197;&#24110;&#21161;&#20892;&#19994;&#25237;&#36164;&#32773;&#26681;&#25454;&#23433;&#35013;&#39044;&#31639;&#12289;&#25919;&#24220;&#28608;&#21169;&#25514;&#26045;&#12289;&#33021;&#28304;&#38656;&#27714;&#12289;&#31995;&#32479;&#25104;&#26412;&#21644;&#38271;&#26399;&#25928;&#30410;&#31561;&#22240;&#32032;&#20316;&#20986;&#26126;&#26234;&#20915;&#31574;&#12290;&#36890;&#36807;&#23454;&#29616;&#19968;&#20010;&#22870;&#21169;&#26426;&#21046;&#65292;DQN&#23398;&#20064;&#22914;&#20309;&#22312;&#20809;&#20239;&#31995;&#32479;&#38598;&#25104;&#26041;&#38754;&#20570;&#20986;&#25968;&#25454;&#39537;&#21160;&#30340;&#20915;&#31574;&#12290;&#35813;&#20998;&#26512;&#25552;&#20379;&#20102;&#28145;&#20837;&#20102;&#35299;DQN&#22914;&#20309;&#25903;&#25345;&#25237;&#36164;&#32773;&#22312;&#20892;&#19994;&#20013;&#36827;&#34892;&#20809;&#20239;&#31995;&#32479;&#23433;&#35013;&#20915;&#31574;&#30340;&#32508;&#21512;&#29702;&#35299;&#12290;&#36825;&#39033;&#30740;&#31350;&#23545;&#20110;&#20419;&#36827;&#21487;&#25345;&#32493;&#21644;&#39640;&#25928;&#30340;&#20892;&#19994;&#23454;&#36341;&#65292;&#21516;&#26102;&#20026;&#26410;&#26469;&#35813;&#39046;&#22495;&#30340;&#36827;&#23637;&#38138;&#24179;&#20102;&#36947;&#36335;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#36890;&#36807;&#21033;&#29992;DQN&#65292;&#20892;&#19994;&#25237;&#36164;&#32773;&#21487;&#20197;&#20570;&#20986;&#20248;&#21270;&#20915;&#31574;&#65292;&#25552;&#39640;&#33021;&#28304;&#25928;&#29575;&#65292;&#20943;&#23569;&#29615;&#22659;&#24433;&#21709;&#65292;&#24182;&#22686;&#21152;&#21033;&#28070;&#12290;&#35813;&#30740;&#31350;&#23545;&#25512;&#21160;&#20892;&#19994;&#39046;&#22495;&#30340;&#36827;&#27493;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article investigates the use of Deep Q-Networks (DQNs) to optimize decision-making for photovoltaic (PV) systems installations in the agriculture sector. The study develops a DQN framework to assist agricultural investors in making informed decisions considering factors such as installation budget, government incentives, energy requirements, system cost, and long-term benefits. By implementing a reward mechanism, the DQN learns to make data-driven decisions on PV integration. The analysis provides a comprehensive understanding of how DQNs can support investors in making decisions about PV installations in agriculture. This research has significant implications for promoting sustainable and efficient farming practices while also paving the way for future advancements in this field. By leveraging DQNs, agricultural investors can make optimized decisions that improve energy efficiency, reduce environmental impact, and enhance profitability. This study contributes to the advancement o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22312;&#26368;&#23567;&#35774;&#32622;&#19978;&#36827;&#34892;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#24320;&#21457;&#23454;&#39564;&#65292;&#30740;&#31350;&#21457;&#29616;&#20934;&#30830;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#20851;&#38190;&#22312;&#20110;&#36866;&#24403;&#30340;&#25968;&#25454;&#38598;&#20869;&#23481;&#21644;&#35757;&#32451;&#31574;&#30053;&#65292;&#32780;&#19981;&#26159;&#31070;&#32463;&#21442;&#25968;&#25968;&#37327;&#12289;&#35757;&#32451;&#26102;&#38271;&#25110;&#25968;&#25454;&#38598;&#22823;&#23567;&#12290;&#36890;&#36807;&#23545;&#19968;&#20010;10&#20159;&#21442;&#25968;&#35268;&#27169;&#30340;&#36890;&#29992;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#20351;&#29992;&#21830;&#19994;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#30340;&#31934;&#31616;&#27573;&#33853;&#21644;&#38382;&#31572;&#23545;&#26500;&#24314;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#26377;&#25928;&#35299;&#37322;&#36275;&#29699;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2308.08610</link><description>&lt;p&gt;
FootGPT&#65306;&#22312;&#26368;&#23567;&#35774;&#32622;&#19978;&#36827;&#34892;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#24320;&#21457;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
FootGPT : A Large Language Model Development Experiment on a Minimal Setting. (arXiv:2308.08610v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08610
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22312;&#26368;&#23567;&#35774;&#32622;&#19978;&#36827;&#34892;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#24320;&#21457;&#23454;&#39564;&#65292;&#30740;&#31350;&#21457;&#29616;&#20934;&#30830;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#20851;&#38190;&#22312;&#20110;&#36866;&#24403;&#30340;&#25968;&#25454;&#38598;&#20869;&#23481;&#21644;&#35757;&#32451;&#31574;&#30053;&#65292;&#32780;&#19981;&#26159;&#31070;&#32463;&#21442;&#25968;&#25968;&#37327;&#12289;&#35757;&#32451;&#26102;&#38271;&#25110;&#25968;&#25454;&#38598;&#22823;&#23567;&#12290;&#36890;&#36807;&#23545;&#19968;&#20010;10&#20159;&#21442;&#25968;&#35268;&#27169;&#30340;&#36890;&#29992;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#20351;&#29992;&#21830;&#19994;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#30340;&#31934;&#31616;&#27573;&#33853;&#21644;&#38382;&#31572;&#23545;&#26500;&#24314;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#26377;&#25928;&#35299;&#37322;&#36275;&#29699;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#23454;&#35777;&#35266;&#23519;&#34920;&#26126;&#65292;&#30456;&#27604;&#20110;&#31070;&#32463;&#21442;&#25968;&#25968;&#37327;&#12289;&#35757;&#32451;&#26102;&#38271;&#25110;&#25968;&#25454;&#38598;&#22823;&#23567;&#65292;&#24320;&#21457;&#20934;&#30830;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#37325;&#35201;&#26041;&#38754;&#21487;&#33021;&#26159;&#36866;&#24403;&#30340;&#25968;&#25454;&#38598;&#20869;&#23481;&#21644;&#35757;&#32451;&#31574;&#30053;&#12290;&#22522;&#20110;&#36825;&#20010;&#35266;&#28857;&#65292;&#25105;&#20204;&#36873;&#25321;&#20351;&#29992;&#20302;&#31209;&#36866;&#24212;&#24615;&#23545;&#19968;&#20010;10&#20159;&#21442;&#25968;&#35268;&#27169;&#30340;&#36890;&#29992;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#25968;&#25454;&#38598;&#30001;&#24847;&#22823;&#21033;&#36275;&#29699;&#32852;&#36187;&#21069;&#21313;&#36718;&#30340;&#29699;&#38431;&#32479;&#35745;&#20449;&#24687;&#26500;&#24314;&#65292;&#24182;&#20351;&#29992;&#24378;&#22823;&#30340;&#21830;&#19994;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#30340;&#31934;&#31616;&#27573;&#33853;&#21644;&#38382;&#31572;&#23545;&#12290;&#25105;&#20204;&#23558;&#35757;&#32451;&#26102;&#38271;&#20445;&#25345;&#30456;&#23545;&#36739;&#30701;&#65292;&#20197;&#25552;&#20379;&#25105;&#20204;&#26368;&#23567;&#35774;&#32622;&#25506;&#32034;&#30340;&#22522;&#30784;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20998;&#20139;&#20102;&#19982;&#20351;&#29992;&#26377;&#38480;&#36164;&#28304;&#35299;&#37322;&#36275;&#29699;&#25968;&#25454;&#30340;&#29305;&#23450;&#30446;&#30340;&#35821;&#35328;&#27169;&#22411;&#24320;&#21457;&#30456;&#20851;&#30340;&#20851;&#38190;&#35266;&#23519;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
With recent empirical observations, it has been argued that the most significant aspect of developing accurate language models may be the proper dataset content and training strategy compared to the number of neural parameters, training duration or dataset size. Following this argument, we opted to fine tune a one billion parameter size trained general purpose causal language model with a dataset curated on team statistics of the Italian football league first ten game weeks, using low rank adaptation. The limited training dataset was compiled based on a framework where a powerful commercial large language model provides distilled paragraphs and question answer pairs as intended. The training duration was kept relatively short to provide a basis for our minimal setting exploration. We share our key observations on the process related to developing a specific purpose language model which is intended to interpret soccer data with constrained resources in this article.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20851;&#20110;&#20154;&#31867;/&#35748;&#30693;&#21512;&#22863;&#20013;&#30340;&#35748;&#30693;&#20934;&#30830;&#24615;&#21644;&#35748;&#30693;&#31934;&#24230;&#30340;&#30740;&#31350;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#35748;&#30693;&#31995;&#32479;&#25552;&#20379;&#30340;&#20449;&#24687;&#65292;&#20154;&#31867;&#30340;&#35748;&#30693;&#20934;&#30830;&#24615;&#21644;&#35748;&#30693;&#31934;&#24230;&#24471;&#21040;&#20102;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2308.08581</link><description>&lt;p&gt;
&#20851;&#20110;&#22686;&#24378;&#20154;&#31867;/&#35748;&#30693;&#21512;&#22863;&#20013;&#30340;&#35748;&#30693;&#20934;&#30830;&#24615;&#21644;&#35748;&#30693;&#31934;&#24230;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Augmentation of Cognitive Accuracy and Cognitive Precision in Human/Cog Ensembles. (arXiv:2308.08581v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08581
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20851;&#20110;&#20154;&#31867;/&#35748;&#30693;&#21512;&#22863;&#20013;&#30340;&#35748;&#30693;&#20934;&#30830;&#24615;&#21644;&#35748;&#30693;&#31934;&#24230;&#30340;&#30740;&#31350;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#35748;&#30693;&#31995;&#32479;&#25552;&#20379;&#30340;&#20449;&#24687;&#65292;&#20154;&#31867;&#30340;&#35748;&#30693;&#20934;&#30830;&#24615;&#21644;&#35748;&#30693;&#31934;&#24230;&#24471;&#21040;&#20102;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27599;&#24403;&#20154;&#31867;&#20351;&#29992;&#24037;&#20855;&#26102;&#65292;&#20154;&#31867;&#30340;&#34920;&#29616;&#20250;&#24471;&#21040;&#22686;&#24378;&#12290;&#35748;&#30693;&#31995;&#32479;&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#24037;&#20855;&#65292;&#19981;&#26029;&#22686;&#24378;&#35748;&#30693;&#33021;&#21147;&#65292;&#29616;&#22312;&#21487;&#20197;&#25191;&#34892;&#39640;&#32423;&#35748;&#30693;&#20219;&#21153;&#65292;&#36825;&#20123;&#20219;&#21153;&#20197;&#21069;&#34987;&#35748;&#20026;&#21482;&#26377;&#20154;&#31867;&#33021;&#22815;&#23436;&#25104;&#12290;&#20351;&#29992;&#36825;&#31181;&#24037;&#20855;&#65292;&#20063;&#34987;&#31216;&#20026;&#35748;&#30693;(cog)&#65292;&#39044;&#35745;&#23558;&#20250;&#23548;&#33268;&#20154;&#31867;&#35748;&#30693;&#22686;&#24378;&#27700;&#24179;&#30340;&#36827;&#19968;&#27493;&#25552;&#39640;&#12290;&#22312;&#20154;&#31867;&#35748;&#30693;&#21512;&#22863;&#20013;&#65292;&#20154;&#31867;&#21644;&#35748;&#30693;&#31995;&#32479;&#20043;&#38388;&#36827;&#34892;&#30528;&#21512;&#20316;&#12289;&#28857;&#23545;&#28857;&#21644;&#21327;&#20316;&#30340;&#23545;&#35805;&#65292;&#20154;&#31867;&#35748;&#30693;&#33021;&#21147;&#22240;&#27492;&#24471;&#21040;&#22686;&#24378;&#12290;&#22240;&#27492;&#65292;&#20154;&#31867;&#35748;&#30693;&#21512;&#22863;&#33021;&#22815;&#23454;&#29616;&#27604;&#21333;&#29420;&#30340;&#20154;&#31867;&#25110;&#35748;&#30693;&#31995;&#32479;&#26356;&#22810;&#30340;&#24037;&#20316;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#39033;&#30740;&#31350;&#32467;&#26524;&#65292;&#26088;&#22312;&#34913;&#37327;&#35748;&#30693;&#30340;&#20934;&#30830;&#24615;&#8212;&#8212;&#20135;&#29983;&#27491;&#30830;&#32467;&#26524;&#30340;&#33021;&#21147;&#65292;&#20197;&#21450;&#35748;&#30693;&#30340;&#31934;&#24230;&#8212;&#8212;&#21482;&#20135;&#29983;&#27491;&#30830;&#32467;&#26524;&#30340;&#20542;&#21521;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20449;&#24687;&#30340;&#20351;&#29992;&#21487;&#20197;&#25552;&#39640;&#35748;&#30693;&#20934;&#30830;&#24615;&#21644;&#35748;&#30693;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Whenever humans use tools human performance is enhanced. Cognitive systems are a new kind of tool continually increasing in cognitive capability and are now performing high level cognitive tasks previously thought to be explicitly human. Usage of such tools, known as cogs, are expected to result in ever increasing levels of human cognitive augmentation. In a human cog ensemble, a cooperative, peer to peer, and collaborative dialog between a human and a cognitive system, human cognitive capability is augmented as a result of the interaction. The human cog ensemble is therefore able to achieve more than just the human or the cog working alone. This article presents results from two studies designed to measure the effect information supplied by a cog has on cognitive accuracy, the ability to produce the correct result, and cognitive precision, the propensity to produce only the correct result. Both cognitive accuracy and cognitive precision are shown to be increased by information of diff
&lt;/p&gt;</description></item><item><title>PEvoLM&#26159;&#19968;&#31181;&#34507;&#30333;&#36136;&#24207;&#21015;&#36827;&#21270;&#20449;&#24687;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#21033;&#29992;&#23884;&#20837;&#24335;&#35821;&#35328;&#27169;&#22411;&#23558;&#34507;&#30333;&#36136;&#24207;&#21015;&#36716;&#25442;&#20026;&#25968;&#23383;&#21521;&#37327;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2308.08578</link><description>&lt;p&gt;
PEvoLM: &#34507;&#30333;&#36136;&#24207;&#21015;&#36827;&#21270;&#20449;&#24687;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PEvoLM: Protein Sequence Evolutionary Information Language Model. (arXiv:2308.08578v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08578
&lt;/p&gt;
&lt;p&gt;
PEvoLM&#26159;&#19968;&#31181;&#34507;&#30333;&#36136;&#24207;&#21015;&#36827;&#21270;&#20449;&#24687;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#21033;&#29992;&#23884;&#20837;&#24335;&#35821;&#35328;&#27169;&#22411;&#23558;&#34507;&#30333;&#36136;&#24207;&#21015;&#36716;&#25442;&#20026;&#25968;&#23383;&#21521;&#37327;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#34507;&#30333;&#36136;&#24207;&#21015;&#25968;&#25454;&#24211;&#30340;&#25351;&#25968;&#22686;&#38271;&#65292;&#22810;&#24207;&#21015;&#27604;&#23545;(PSI-BLAST&#31561;)&#36890;&#36807;&#32791;&#36153;&#26102;&#38388;&#30340;&#25968;&#25454;&#24211;&#25628;&#32034;&#26469;&#33719;&#21462;&#36827;&#21270;&#20449;&#24687;&#12290;&#36825;&#20123;&#25628;&#32034;&#24341;&#25806;&#29983;&#25104;&#30340;&#20301;&#32622;&#29305;&#24322;&#24615;&#35780;&#20998;&#30697;&#38453;(PSSMs)&#23545;&#20110;&#29983;&#29289;&#20449;&#24687;&#23398;&#21644;&#35745;&#31639;&#29983;&#29289;&#23398;&#39046;&#22495;&#30340;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#35828;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#36755;&#20837;&#12290;&#34507;&#30333;&#24207;&#21015;&#26159;&#30001;&#31216;&#20026;&#27688;&#22522;&#37240;(AAs)&#30340;&#36830;&#32493;&#26631;&#35760;&#25110;&#23383;&#31526;&#32452;&#25104;&#30340;&#12290;&#31867;&#27604;&#33258;&#28982;&#35821;&#35328;&#20351;&#25105;&#20204;&#33021;&#22815;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#23558;NLP&#30340;&#26368;&#20808;&#36827;&#31639;&#27861;&#24212;&#29992;&#20110;&#29983;&#29289;&#20449;&#24687;&#23398;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23884;&#20837;&#24335;&#35821;&#35328;&#27169;&#22411;(ELMo)&#65292;&#23558;&#34507;&#30333;&#36136;&#24207;&#21015;&#36716;&#25442;&#20026;&#25968;&#23383;&#21521;&#37327;&#34920;&#31034;&#12290;&#23613;&#31649;&#21407;&#22987;ELMo&#37319;&#29992;&#20102;&#19968;&#20010;&#20004;&#23618;&#21452;&#21521;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;(LSTMs)&#20197;&#21450;&#19968;&#20010;&#29992;&#20110;&#27491;&#21521;&#22788;&#29702;&#30340;&#21452;&#36335;&#24452;&#26550;&#26500;&#65292;
&lt;/p&gt;
&lt;p&gt;
With the exponential increase of the protein sequence databases over time, multiple-sequence alignment (MSA) methods, like PSI-BLAST, perform exhaustive and time-consuming database search to retrieve evolutionary information. The resulting position-specific scoring matrices (PSSMs) of such search engines represent a crucial input to many machine learning (ML) models in the field of bioinformatics and computational biology. A protein sequence is a collection of contiguous tokens or characters called amino acids (AAs). The analogy to natural language allowed us to exploit the recent advancements in the field of Natural Language Processing (NLP) and therefore transfer NLP state-of-the-art algorithms to bioinformatics. This research presents an Embedding Language Model (ELMo), converting a protein sequence to a numerical vector representation. While the original ELMo trained a 2-layer bidirectional Long Short-Term Memory (LSTMs) network following a two-path architecture, one for the forwar
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#27604;&#20998;&#26512;&#20102;12&#31181;&#33258;&#28982;&#21551;&#21457;&#30340;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;&#22312;&#39044;&#27979;&#23398;&#29983;&#34920;&#29616;&#20013;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#21033;&#29992;&#36825;&#20123;&#31639;&#27861;&#36827;&#34892;&#29305;&#24449;&#36873;&#25321;&#24182;&#32467;&#21512;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#24182;&#20943;&#23569;&#29305;&#24449;&#38598;&#22823;&#23567;&#12290;</title><link>http://arxiv.org/abs/2308.08574</link><description>&lt;p&gt;
&#33258;&#28982;&#21551;&#21457;&#30340;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;&#22312;&#39044;&#27979;&#23398;&#29983;&#34920;&#29616;&#20013;&#30340;&#33021;&#21147;&#30340;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Comparative Analysis of the Capabilities of Nature-inspired Feature Selection Algorithms in Predicting Student Performance. (arXiv:2308.08574v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08574
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#27604;&#20998;&#26512;&#20102;12&#31181;&#33258;&#28982;&#21551;&#21457;&#30340;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;&#22312;&#39044;&#27979;&#23398;&#29983;&#34920;&#29616;&#20013;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#21033;&#29992;&#36825;&#20123;&#31639;&#27861;&#36827;&#34892;&#29305;&#24449;&#36873;&#25321;&#24182;&#32467;&#21512;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#24182;&#20943;&#23569;&#29305;&#24449;&#38598;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#23398;&#29983;&#34920;&#29616;&#23545;&#20110;&#26377;&#25928;&#38450;&#27490;&#39118;&#38505;&#23398;&#29983;&#22833;&#36133;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#19968;&#22871;12&#20010;&#33258;&#28982;&#21551;&#21457;&#31639;&#27861;&#22312;&#39044;&#27979;&#23398;&#29983;&#34920;&#29616;&#20013;&#30340;&#30456;&#23545;&#24615;&#33021;&#65292;&#21253;&#25324;&#22522;&#20110;&#23454;&#20363;&#30340;&#28857;&#20987;&#27969;&#25968;&#25454;&#12289;&#35838;&#20869;&#21333;&#19968;&#35838;&#31243;&#34920;&#29616;&#20197;&#21450;&#21516;&#26102;&#21442;&#21152;&#22810;&#20010;&#35838;&#31243;&#26102;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#21033;&#29992;&#33258;&#28982;&#21551;&#21457;&#30340;&#31639;&#27861;&#36827;&#34892;&#29305;&#24449;&#36873;&#25321;&#24182;&#32467;&#21512;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20998;&#31867;&#65292;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#24182;&#20943;&#23569;&#29305;&#24449;&#38598;&#22823;&#23567;&#30340;2/3&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting student performance is key in leveraging effective pre-failure interventions for at-risk students. In this paper, I have analyzed the relative performance of a suite of 12 nature-inspired algorithms when used to predict student performance across 3 datasets consisting of instance-based clickstream data, intra-course single-course performance, and performance when taking multiple courses simultaneously. I found that, for all datasets, leveraging an ensemble approach using NIAs for feature selection and traditional ML algorithms for classification increased predictive accuracy while also reducing feature set size by 2/3.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#21021;&#32423;&#32534;&#31243;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#24471;&#20986;&#20102;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#32534;&#31243;&#25945;&#32946;&#21644;&#35780;&#20272;&#30340;&#25512;&#35770;&#12290;</title><link>http://arxiv.org/abs/2308.08572</link><description>&lt;p&gt;
&#21021;&#32423;&#32534;&#31243;&#25945;&#32946;&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;ChatGPT&#22312;&#24615;&#33021;&#21644;&#23545;&#35780;&#20272;&#30340;&#24433;&#21709;&#19978;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Large Language Models in Introductory Programming Education: ChatGPT's Performance and Implications for Assessments. (arXiv:2308.08572v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08572
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#21021;&#32423;&#32534;&#31243;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#24471;&#20986;&#20102;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#32534;&#31243;&#25945;&#32946;&#21644;&#35780;&#20272;&#30340;&#25512;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;ChatGPT-3.5&#21644;GPT-4&#22312;&#35299;&#20915;&#21021;&#32423;&#32534;&#31243;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#22522;&#20110;&#24615;&#33021;&#65292;&#24471;&#20986;&#20102;&#21033;&#29992;LLMs&#36827;&#34892;&#25945;&#23398;&#22330;&#26223;&#21644;&#35780;&#20272;&#26684;&#24335;&#30340;&#25512;&#35770;&#12290;&#20026;&#20102;&#20998;&#26512;&#65292;&#20174;&#20813;&#36153;&#32593;&#31449;CodingBat&#20013;&#36873;&#25321;&#20102;72&#20010;&#29992;&#20110;&#21021;&#23398;&#32773;&#30340;Python&#20219;&#21153;&#12290;&#23436;&#25972;&#30340;&#20219;&#21153;&#25551;&#36848;&#34987;&#29992;&#20316;LLMs&#30340;&#36755;&#20837;&#65292;&#29983;&#25104;&#30340;&#22238;&#22797;&#21017;&#20351;&#29992;CodingBat&#30340;&#21333;&#20803;&#27979;&#35797;&#36827;&#34892;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#36824;&#20998;&#26512;&#20102;&#25991;&#26412;&#35299;&#37322;&#21644;&#31243;&#24207;&#20195;&#30721;&#30340;&#26222;&#36941;&#21487;&#29992;&#24615;&#12290;&#32467;&#26524;&#26174;&#31034;&#20102;94.4%&#33267;95.8%&#30340;&#27491;&#30830;&#22238;&#31572;&#39640;&#20998;&#25968;&#20197;&#21450;&#21487;&#38752;&#30340;&#25991;&#26412;&#35299;&#37322;&#21644;&#31243;&#24207;&#20195;&#30721;&#21487;&#29992;&#24615;&#65292;&#36825;&#20026;&#23558;LLMs&#32435;&#20837;&#32534;&#31243;&#25945;&#32946;&#21644;&#35780;&#20272;&#25552;&#20379;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the performance of the Large Language Models (LLMs) ChatGPT-3.5 and GPT-4 in solving introductory programming tasks. Based on the performance, implications for didactic scenarios and assessment formats utilizing LLMs are derived. For the analysis, 72 Python tasks for novice programmers were selected from the free site CodingBat. Full task descriptions were used as input to the LLMs, while the generated replies were evaluated using CodingBat's unit tests. In addition, the general availability of textual explanations and program code was analyzed. The results show high scores of 94.4 to 95.8% correct responses and reliable availability of textual explanations and program code, which opens new ways to incorporate LLMs into programming education and assessment.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#30340;&#22810;&#26041;&#20301;&#26694;&#26550;&#65288;KMF&#65289;&#65292;&#29992;&#20110;&#38646;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#25552;&#21462;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#20027;&#39064;&#26469;&#22686;&#24378;&#26631;&#31614;&#35821;&#20041;&#65292;&#20197;&#25913;&#21892;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.08563</link><description>&lt;p&gt;
KMF: &#22522;&#20110;&#30693;&#35782;&#30340;&#22810;&#26041;&#20301;&#34920;&#31034;&#23398;&#20064;&#29992;&#20110;&#38646;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
KMF: Knowledge-Aware Multi-Faceted Representation Learning for Zero-Shot Node Classification. (arXiv:2308.08563v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08563
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#30340;&#22810;&#26041;&#20301;&#26694;&#26550;&#65288;KMF&#65289;&#65292;&#29992;&#20110;&#38646;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#25552;&#21462;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#20027;&#39064;&#26469;&#22686;&#24378;&#26631;&#31614;&#35821;&#20041;&#65292;&#20197;&#25913;&#21892;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#38646;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;&#65288;ZNC&#65289;&#22312;&#22270;&#25968;&#25454;&#20998;&#26512;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#35813;&#20219;&#21153;&#26088;&#22312;&#39044;&#27979;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26410;&#35266;&#23519;&#21040;&#30340;&#26410;&#30693;&#31867;&#21035;&#30340;&#33410;&#28857;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#20027;&#35201;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#23558;&#29305;&#24449;&#30340;&#21407;&#22411;&#21644;&#26631;&#31614;&#30340;&#35821;&#20041;&#32852;&#31995;&#36215;&#26469;&#65292;&#20174;&#32780;&#23454;&#29616;&#20174;&#24050;&#35266;&#23519;&#21040;&#30340;&#31867;&#21035;&#21040;&#26410;&#35266;&#23519;&#21040;&#30340;&#31867;&#21035;&#30340;&#30693;&#35782;&#36801;&#31227;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#30740;&#31350;&#24573;&#35270;&#20102;&#29305;&#24449;-&#35821;&#20041;&#23545;&#40784;&#20013;&#22810;&#26041;&#20301;&#35821;&#20041;&#26041;&#21521;&#30340;&#23384;&#22312;&#65292;&#21363;&#33410;&#28857;&#30340;&#20869;&#23481;&#36890;&#24120;&#28085;&#30422;&#19982;&#22810;&#20010;&#26631;&#31614;&#30340;&#35821;&#20041;&#30456;&#20851;&#30340;&#19981;&#21516;&#20027;&#39064;&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#21306;&#20998;&#21644;&#21028;&#26029;&#24433;&#21709;&#35748;&#30693;&#33021;&#21147;&#30340;&#35821;&#20041;&#22240;&#32032;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#30340;&#22810;&#26041;&#20301;&#26694;&#26550;&#65288;KMF&#65289;&#65292;&#36890;&#36807;&#25552;&#21462;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#30340;&#20027;&#39064;&#26469;&#22686;&#24378;&#26631;&#31614;&#35821;&#20041;&#30340;&#20016;&#23500;&#24615;&#12290;&#28982;&#21518;&#65292;&#23558;&#27599;&#20010;&#33410;&#28857;&#30340;&#20869;&#23481;&#37325;&#26500;&#20026;&#20027;&#39064;&#32423;&#21035;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Zero-Shot Node Classification (ZNC) has been an emerging and crucial task in graph data analysis. This task aims to predict nodes from unseen classes which are unobserved in the training process. Existing work mainly utilizes Graph Neural Networks (GNNs) to associate features' prototypes and labels' semantics thus enabling knowledge transfer from seen to unseen classes. However, the multi-faceted semantic orientation in the feature-semantic alignment has been neglected by previous work, i.e. the content of a node usually covers diverse topics that are relevant to the semantics of multiple labels. It's necessary to separate and judge the semantic factors that tremendously affect the cognitive ability to improve the generality of models. To this end, we propose a Knowledge-Aware Multi-Faceted framework (KMF) that enhances the richness of label semantics via the extracted KG (Knowledge Graph)-based topics. And then the content of each node is reconstructed to a topic-level repre
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;QMLS&#30340;&#26032;&#27010;&#24565;&#65292;&#36890;&#36807;&#32467;&#21512;&#26426;&#22120;&#23398;&#20064;&#21644;&#37327;&#23376;&#27169;&#25311;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#32553;&#30701;&#33647;&#29289;&#30740;&#21457;&#30340;&#26102;&#38388;&#21644;&#38477;&#20302;&#25104;&#26412;&#12290;&#36890;&#36807;&#29983;&#25104;&#21629;&#20013;&#29289;&#21644;&#20248;&#21270;&#20998;&#23376;&#30340;&#36807;&#31243;&#65292;&#21487;&#20197;&#22823;&#22823;&#25552;&#39640;&#33647;&#29289;&#21457;&#29616;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.08561</link><description>&lt;p&gt;
&#26410;&#26469;&#33647;&#29289;&#21457;&#29616;&#30340;&#23454;&#26045;&#65306;&#22522;&#20110;&#37327;&#23376;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#25311;(QMLS)&#12290;
&lt;/p&gt;
&lt;p&gt;
Implementation of The Future of Drug Discovery: QuantumBased Machine Learning Simulation (QMLS). (arXiv:2308.08561v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08561
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;QMLS&#30340;&#26032;&#27010;&#24565;&#65292;&#36890;&#36807;&#32467;&#21512;&#26426;&#22120;&#23398;&#20064;&#21644;&#37327;&#23376;&#27169;&#25311;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#32553;&#30701;&#33647;&#29289;&#30740;&#21457;&#30340;&#26102;&#38388;&#21644;&#38477;&#20302;&#25104;&#26412;&#12290;&#36890;&#36807;&#29983;&#25104;&#21629;&#20013;&#29289;&#21644;&#20248;&#21270;&#20998;&#23376;&#30340;&#36807;&#31243;&#65292;&#21487;&#20197;&#22823;&#22823;&#25552;&#39640;&#33647;&#29289;&#21457;&#29616;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33647;&#29289;&#30740;&#21457;&#30340;&#30740;&#31350;&#19982;&#24320;&#21457;(R&amp;D)&#38454;&#27573;&#26159;&#19968;&#20010;&#28459;&#38271;&#32780;&#26114;&#36149;&#30340;&#36807;&#31243;&#12290;&#20026;&#20102;&#25913;&#38761;&#36825;&#20010;&#36807;&#31243;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26032;&#27010;&#24565;QMLS&#65292;&#23558;&#25972;&#20010;R&amp;D&#38454;&#27573;&#32553;&#30701;&#21040;&#19977;&#21040;&#20845;&#20010;&#26376;&#65292;&#25104;&#26412;&#20165;&#20026;&#20116;&#21040;&#20843;&#19975;&#32654;&#20803;&#12290;&#23545;&#20110;&#21629;&#20013;&#20135;&#29983;&#65292;&#26426;&#22120;&#23398;&#20064;&#20998;&#23376;&#29983;&#25104;(MLMG)&#26681;&#25454;&#30446;&#26631;&#34507;&#30333;&#30340;&#20998;&#23376;&#32467;&#26500;&#29983;&#25104;&#21487;&#33021;&#30340;&#21629;&#20013;&#29289;&#65292;&#32780;&#37327;&#23376;&#27169;&#25311;(QS)&#26681;&#25454;&#19982;&#30446;&#26631;&#34507;&#30333;&#30340;&#21453;&#24212;&#21644;&#32467;&#21512;&#25928;&#26524;&#36807;&#28388;&#21407;&#22987;&#23454;&#39564;&#20013;&#30340;&#20998;&#23376;&#12290;&#28982;&#21518;&#65292;&#23545;&#20110;&#38085;&#20248;&#21270;&#65292;&#20174;MLMG&#21644;QS&#29983;&#25104;&#21644;&#36807;&#28388;&#30340;&#32467;&#26524;&#20998;&#23376;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#20998;&#23376;&#21464;&#24322;(MLMV)&#23558;&#37027;&#20123;&#20986;&#29616;&#22312;&#20004;&#20010;&#36807;&#31243;&#20013;&#30340;&#20998;&#23376;&#21046;&#25104;&#25968;&#21313;&#31181;&#20998;&#23376;&#21464;&#20307;&#65292;&#32780;&#20854;&#20182;&#20998;&#23376;&#21482;&#21046;&#25104;&#20960;&#31181;&#21464;&#20307;&#12290;&#26368;&#21518;&#65292;&#25152;&#26377;&#20248;&#21270;&#30340;&#20998;&#23376;&#23558;&#32463;&#36807;&#22810;&#36718;&#39640;&#26631;&#20934;&#30340;QS&#36807;&#28388;&#65292;&#20197;&#30830;&#20445;&#21453;&#24212;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Research &amp; Development (R&amp;D) phase of drug development is a lengthy and costly process. To revolutionize this process, we introduce our new concept QMLS to shorten the whole R&amp;D phase to three to six months and decrease the cost to merely fifty to eighty thousand USD. For Hit Generation, Machine Learning Molecule Generation (MLMG) generates possible hits according to the molecular structure of the target protein while the Quantum Simulation (QS) filters molecules from the primary essay based on the reaction and binding effectiveness with the target protein. Then, For Lead Optimization, the resultant molecules generated and filtered from MLMG and QS are compared, and molecules that appear as a result of both processes will be made into dozens of molecular variations through Machine Learning Molecule Variation (MLMV), while others will only be made into a few variations. Lastly, all optimized molecules would undergo multiple rounds of QS filtering with a high standard for reaction ef
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24335;&#21305;&#37197;&#30340;&#27604;&#29305;&#24065;&#20449;&#24687;&#26816;&#32034;&#39044;&#27979;&#27169;&#22411;(BIRP)&#65292;&#36890;&#36807;&#25490;&#21015;&#30456;&#20284;&#30340;&#36807;&#21435;&#22270;&#34920;&#36816;&#21160;&#26469;&#25552;&#39640;&#39044;&#27979;&#33021;&#21147;&#65292;&#24212;&#29992;&#20110;&#27604;&#29305;&#24065;&#24066;&#22330;&#12290;</title><link>http://arxiv.org/abs/2308.08558</link><description>&lt;p&gt;
BIRP: &#22522;&#20110;&#22810;&#27169;&#24335;&#21305;&#37197;&#30340;&#27604;&#29305;&#24065;&#20449;&#24687;&#26816;&#32034;&#39044;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
BIRP: Bitcoin Information Retrieval Prediction Model Based on Multimodal Pattern Matching. (arXiv:2308.08558v1 [q-fin.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08558
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24335;&#21305;&#37197;&#30340;&#27604;&#29305;&#24065;&#20449;&#24687;&#26816;&#32034;&#39044;&#27979;&#27169;&#22411;(BIRP)&#65292;&#36890;&#36807;&#25490;&#21015;&#30456;&#20284;&#30340;&#36807;&#21435;&#22270;&#34920;&#36816;&#21160;&#26469;&#25552;&#39640;&#39044;&#27979;&#33021;&#21147;&#65292;&#24212;&#29992;&#20110;&#27604;&#29305;&#24065;&#24066;&#22330;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38543;&#26426;&#34892;&#36208;&#20551;&#35774;&#19979;&#65292;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#34987;&#35748;&#20026;&#26159;&#38789;&#36807;&#31243;&#12290;&#20026;&#20102;&#21457;&#29616;&#37329;&#34701;&#24066;&#22330;&#20013;&#38544;&#34255;&#30340;&#21487;&#37325;&#22797;&#27169;&#24335;&#65292;&#20154;&#20204;&#24320;&#21457;&#20102;&#21508;&#31181;&#22810;&#27169;&#24335;&#21305;&#37197;&#31639;&#27861;&#65292;&#32780;&#19981;&#20165;&#20165;&#20381;&#38752;&#21407;&#22987;&#20215;&#26684;&#36827;&#34892;&#25237;&#36164;&#20915;&#31574;&#12290;&#35768;&#22810;&#22522;&#20110;&#22270;&#34920;&#30340;&#27169;&#24335;&#21305;&#37197;&#24037;&#20855;&#21482;&#33021;&#22312;&#24403;&#21069;&#22270;&#34920;&#27169;&#24335;&#30340;&#22522;&#30784;&#19978;&#26816;&#32034;&#31867;&#20284;&#30340;&#36807;&#21435;&#22270;&#34920;&#27169;&#24335;&#65292;&#24182;&#23558;&#25972;&#20010;&#35299;&#37322;&#21644;&#39044;&#27979;&#20998;&#26512;&#20197;&#21450;&#26368;&#32456;&#30340;&#25237;&#36164;&#20915;&#31574;&#30041;&#32473;&#25237;&#36164;&#32773;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24403;&#21069;&#22270;&#34920;&#20449;&#24687;&#23545;&#30456;&#20284;&#36807;&#21435;&#22270;&#34920;&#36816;&#21160;&#36827;&#34892;&#25490;&#24207;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#21033;&#29992;&#36825;&#20123;&#39069;&#22806;&#29305;&#24449;&#21487;&#20197;&#25552;&#39640;&#25105;&#20204;&#27169;&#22411;&#30340;&#26041;&#21521;&#39044;&#27979;&#33021;&#21147;&#12290;&#30001;&#20110;&#27604;&#29305;&#24065;&#20855;&#26377;&#39640;&#24230;&#27874;&#21160;&#30340;&#20215;&#26684;&#65292;&#20351;&#20854;&#38590;&#20197;&#39044;&#27979;&#20854;&#26410;&#26469;&#36208;&#21183;&#65292;&#22240;&#27492;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#25490;&#24207;&#21644;&#26041;&#21521;&#39044;&#27979;&#24314;&#27169;&#26041;&#27861;&#24212;&#29992;&#20110;&#27604;&#29305;&#24065;&#12290;
&lt;/p&gt;
&lt;p&gt;
Financial time series have historically been assumed to be a martingale process under the Random Walk hypothesis. Instead of making investment decisions using the raw prices alone, various multimodal pattern matching algorithms have been developed to help detect subtly hidden repeatable patterns within the financial market. Many of the chart-based pattern matching tools only retrieve similar past chart (PC) patterns given the current chart (CC) pattern, and leaves the entire interpretive and predictive analysis, thus ultimately the final investment decision, to the investors. In this paper, we propose an approach of ranking similar PC movements given the CC information and show that exploiting this as additional features improves the directional prediction capacity of our model. We apply our ranking and directional prediction modeling methodologies on Bitcoin due to its highly volatile prices that make it challenging to predict its future movements.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#23558;&#20581;&#24247;&#24212;&#29992;&#25968;&#25454;&#21644;&#20010;&#20154;&#20449;&#24687;&#31649;&#29702;&#24037;&#20855;&#36827;&#34892;&#20851;&#32852;&#65292;&#35299;&#20915;&#20102;&#20581;&#24247;&#25968;&#25454;&#21487;&#35270;&#21270;&#20013;&#30340;&#21464;&#37327;&#20851;&#31995;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#21033;&#29992;&#20010;&#20154;&#20449;&#24687;&#31649;&#29702;&#24037;&#20855;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#26469;&#22686;&#24378;&#20581;&#24247;&#21644;&#31119;&#31049;&#24212;&#29992;&#31243;&#24207;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.08556</link><description>&lt;p&gt;
&#22522;&#20110;&#22240;&#26524;&#20851;&#31995;&#30340;&#20581;&#24247;&#24212;&#29992;&#25968;&#25454;&#21644;&#20010;&#20154;&#20449;&#24687;&#31649;&#29702;&#24037;&#20855;&#30340;&#32852;&#25509;
&lt;/p&gt;
&lt;p&gt;
Causally Linking Health Application Data and Personal Information Management Tools. (arXiv:2308.08556v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08556
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#23558;&#20581;&#24247;&#24212;&#29992;&#25968;&#25454;&#21644;&#20010;&#20154;&#20449;&#24687;&#31649;&#29702;&#24037;&#20855;&#36827;&#34892;&#20851;&#32852;&#65292;&#35299;&#20915;&#20102;&#20581;&#24247;&#25968;&#25454;&#21487;&#35270;&#21270;&#20013;&#30340;&#21464;&#37327;&#20851;&#31995;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#21033;&#29992;&#20010;&#20154;&#20449;&#24687;&#31649;&#29702;&#24037;&#20855;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#26469;&#22686;&#24378;&#20581;&#24247;&#21644;&#31119;&#31049;&#24212;&#29992;&#31243;&#24207;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#22269;&#23478;&#30340;&#28040;&#36153;&#32773;&#20581;&#24247;&#35774;&#22791;&#65288;&#22914;&#26234;&#33021;&#25163;&#34920;&#12289;&#30561;&#30496;&#30417;&#27979;&#22120;&#12289;&#26234;&#33021;&#31216;&#31561;&#65289;&#30340;&#26222;&#21450;&#19981;&#20165;&#24341;&#36215;&#20102;&#23545;&#20581;&#24247;&#30417;&#27979;&#30340;&#20852;&#36259;&#30340;&#22686;&#38271;&#65292;&#36824;&#24341;&#21457;&#20102;&#22823;&#37327;&#25903;&#25345;&#26222;&#36890;&#20844;&#20247;&#25506;&#32034;&#27492;&#31867;&#25968;&#25454;&#30340;&#8220;&#26234;&#33021;&#8221;&#24212;&#29992;&#31243;&#24207;&#30340;&#24320;&#21457;&#65292;&#26377;&#26102;&#36824;&#19982;&#19987;&#19994;&#20581;&#24247;&#26381;&#21153;&#36827;&#34892;&#25972;&#21512;&#12290;&#23613;&#31649;&#36825;&#20123;&#35774;&#22791;&#21521;&#29992;&#25143;&#25552;&#20379;&#20102;&#21508;&#31181;&#20581;&#24247;&#25968;&#25454;&#27969;&#65292;&#20294;&#36825;&#20123;&#25968;&#25454;&#27969;&#36890;&#24120;&#20197;&#29420;&#31435;&#30340;&#26102;&#38388;&#24207;&#21015;&#21487;&#35270;&#21270;&#24418;&#24335;&#21576;&#29616;&#65292;&#20854;&#20013;&#20581;&#24247;&#21464;&#37327;&#20043;&#38388;&#30340;&#28508;&#22312;&#20851;&#31995;&#24182;&#26410;&#26126;&#30830;&#21487;&#35265;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;&#24037;&#20316;&#21644;&#31038;&#20132;&#36830;&#25509;&#31561;&#29983;&#27963;&#30340;&#20854;&#20182;&#26041;&#38754;&#36234;&#26469;&#36234;&#25968;&#23383;&#21270;&#65292;&#20294;&#20581;&#24247;&#21644;&#31119;&#31049;&#24212;&#29992;&#31243;&#24207;&#24456;&#23569;&#21033;&#29992;&#24191;&#27867;&#20351;&#29992;&#30340;&#20010;&#20154;&#20449;&#24687;&#31649;&#29702;&#24037;&#20855;&#65288;&#22914;&#20849;&#20139;&#26085;&#21382;&#21644;&#30005;&#23376;&#37038;&#20214;&#31995;&#32479;&#65289;&#25552;&#20379;&#30340;&#28508;&#22312;&#26377;&#29992;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
The proliferation of consumer health devices such as smart watches, sleep monitors, smart scales, etc, in many countries, has not only led to growing interest in health monitoring, but also to the development of a countless number of ``smart'' applications to support the exploration of such data by members of the general public, sometimes with integration into professional health services. While a variety of health data streams has been made available by such devices to users, these streams are often presented as separate time-series visualizations, in which the potential relationships between health variables are not explicitly made visible. Furthermore, despite the fact that other aspects of life, such as work and social connectivity, have become increasingly digitised, health and well-being applications make little use of the potentially useful contextual information provided by widely used personal information management tools, such as shared calendar and email systems. This paper 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#20998;&#26512;&#21382;&#21490;&#25968;&#25454;&#21644;&#21306;&#22359;&#38142;&#21442;&#25968;&#65292;&#25214;&#20986;&#24433;&#21709;&#21152;&#23494;&#36135;&#24065;&#20215;&#26684;&#30340;&#22240;&#32032;&#24182;&#35782;&#21035;&#39118;&#38505;&#21152;&#23494;&#36135;&#24065;&#12290;</title><link>http://arxiv.org/abs/2308.08554</link><description>&lt;p&gt;
AI&#36741;&#21161;&#35843;&#26597;&#21306;&#22359;&#38142;&#21442;&#25968;&#65306;&#39118;&#38505;&#21152;&#23494;&#36135;&#24065;&#21644;&#20215;&#26684;&#22240;&#32032;
&lt;/p&gt;
&lt;p&gt;
AI-Assisted Investigation of On-Chain Parameters: Risky Cryptocurrencies and Price Factors. (arXiv:2308.08554v1 [q-fin.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08554
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#20998;&#26512;&#21382;&#21490;&#25968;&#25454;&#21644;&#21306;&#22359;&#38142;&#21442;&#25968;&#65292;&#25214;&#20986;&#24433;&#21709;&#21152;&#23494;&#36135;&#24065;&#20215;&#26684;&#30340;&#22240;&#32032;&#24182;&#35782;&#21035;&#39118;&#38505;&#21152;&#23494;&#36135;&#24065;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#21152;&#23494;&#36135;&#24065;&#24050;&#25104;&#20026;&#25237;&#36164;&#32773;&#21644;&#23398;&#32773;&#24191;&#27867;&#30740;&#31350;&#30340;&#28909;&#38376;&#35805;&#39064;&#12290;&#20026;&#20102;&#20570;&#20986;&#26126;&#26234;&#30340;&#25237;&#36164;&#20915;&#31574;&#65292;&#20102;&#35299;&#24433;&#21709;&#21152;&#23494;&#36135;&#24065;&#20215;&#26684;&#30340;&#22240;&#32032;&#24182;&#35782;&#21035;&#39118;&#38505;&#21152;&#23494;&#36135;&#24065;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#26412;&#25991;&#37325;&#28857;&#20998;&#26512;&#21382;&#21490;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#23545;&#21306;&#22359;&#38142;&#21442;&#25968;&#36827;&#34892;&#20998;&#26512;&#65292;&#20197;&#35782;&#21035;&#24433;&#21709;&#21152;&#23494;&#36135;&#24065;&#20215;&#26684;&#30340;&#22240;&#32032;&#24182;&#25214;&#21040;&#39118;&#38505;&#21152;&#23494;&#36135;&#24065;&#12290;&#25105;&#20204;&#23545;&#21382;&#21490;&#21152;&#23494;&#36135;&#24065;&#30340;&#38142;&#19978;&#25968;&#25454;&#36827;&#34892;&#20998;&#26512;&#65292;&#24182;&#27979;&#37327;&#20102;&#20215;&#26684;&#19982;&#20854;&#20182;&#21442;&#25968;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#32858;&#31867;&#21644;&#20998;&#31867;&#26041;&#27861;&#26469;&#26356;&#22909;&#22320;&#29702;&#35299;&#21152;&#23494;&#36135;&#24065;&#65292;&#24182;&#23558;&#20854;&#20998;&#31867;&#20026;&#39118;&#38505;&#25110;&#38750;&#39118;&#38505;&#12290;&#20998;&#26512;&#32467;&#26524;&#26174;&#31034;&#65292;&#22823;&#37096;&#20998;&#21152;&#23494;&#36135;&#24065;&#65288;39&#65285;&#65289;&#24050;&#36864;&#20986;&#24066;&#22330;&#65292;&#32780;&#21482;&#26377;&#24456;&#23567;&#19968;&#37096;&#20998;&#65288;10&#65285;&#65289;&#23384;&#27963;&#20102;1000&#22810;&#22825;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#19968;&#20010;&#26174;&#33879;&#30340;&#36127;&#30456;&#20851;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Cryptocurrencies have become a popular and widely researched topic of interest in recent years for investors and scholars. In order to make informed investment decisions, it is essential to comprehend the factors that impact cryptocurrency prices and to identify risky cryptocurrencies. This paper focuses on analyzing historical data and using artificial intelligence algorithms on on-chain parameters to identify the factors affecting a cryptocurrency's price and to find risky cryptocurrencies. We conducted an analysis of historical cryptocurrencies' on-chain data and measured the correlation between the price and other parameters. In addition, we used clustering and classification in order to get a better understanding of a cryptocurrency and classify it as risky or not. The analysis revealed that a significant proportion of cryptocurrencies (39%) disappeared from the market, while only a small fraction (10%) survived for more than 1000 days. Our analysis revealed a significant negative
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#28857;&#23545;&#27880;&#24847;&#21147;&#24863;&#30693;&#26426;&#21046;&#30340;&#21452;&#21521;&#23545;&#24212;&#39044;&#27979;&#32593;&#32476;&#65292;&#36890;&#36807;&#21033;&#29992;&#27169;&#22411;&#28857;&#21644;&#22330;&#26223;&#28857;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#36827;&#34892;&#28857;&#23545;&#21305;&#37197;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#23545;&#35266;&#23519;&#36136;&#37327;&#21644;&#36974;&#25377;&#30340;&#20381;&#36182;&#24615;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#29289;&#20307;&#23039;&#24577;&#20272;&#35745;&#20219;&#21153;&#19978;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.08518</link><description>&lt;p&gt;
&#22522;&#20110;&#21452;&#21521;&#39044;&#27979;&#30340;6D&#29289;&#20307;&#23039;&#24577;&#20272;&#35745;&#20013;&#30340;&#28857;&#23545;&#27880;&#24847;&#21147;&#30340;&#21033;&#29992;
&lt;/p&gt;
&lt;p&gt;
Exploiting Point-Wise Attention in 6D Object Pose Estimation Based on Bidirectional Prediction. (arXiv:2308.08518v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08518
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#28857;&#23545;&#27880;&#24847;&#21147;&#24863;&#30693;&#26426;&#21046;&#30340;&#21452;&#21521;&#23545;&#24212;&#39044;&#27979;&#32593;&#32476;&#65292;&#36890;&#36807;&#21033;&#29992;&#27169;&#22411;&#28857;&#21644;&#22330;&#26223;&#28857;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#36827;&#34892;&#28857;&#23545;&#21305;&#37197;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#23545;&#35266;&#23519;&#36136;&#37327;&#21644;&#36974;&#25377;&#30340;&#20381;&#36182;&#24615;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#29289;&#20307;&#23039;&#24577;&#20272;&#35745;&#20219;&#21153;&#19978;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#20960;&#20309;&#27880;&#20876;&#20272;&#35745;&#26041;&#27861;&#20165;&#38544;&#24335;&#22320;&#21033;&#29992;CAD&#27169;&#22411;&#65292;&#36825;&#23548;&#33268;&#23427;&#20204;&#23545;&#35266;&#23519;&#36136;&#37327;&#30340;&#20381;&#36182;&#24615;&#21644;&#23545;&#36974;&#25377;&#30340;&#19981;&#36275;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#28857;&#23545;&#27880;&#24847;&#21147;&#24863;&#30693;&#26426;&#21046;&#30340;&#21452;&#21521;&#23545;&#24212;&#39044;&#27979;&#32593;&#32476;&#12290;&#35813;&#32593;&#32476;&#19981;&#20165;&#35201;&#27714;&#27169;&#22411;&#28857;&#39044;&#27979;&#23545;&#24212;&#20851;&#31995;&#65292;&#36824;&#26126;&#30830;&#22320;&#23545;&#35266;&#23519;&#21644;&#27169;&#22411;&#20808;&#39564;&#20043;&#38388;&#30340;&#20960;&#20309;&#30456;&#20284;&#24615;&#36827;&#34892;&#24314;&#27169;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35265;&#35299;&#26159;&#65292;&#27599;&#20010;&#27169;&#22411;&#28857;&#21644;&#22330;&#26223;&#28857;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#20026;&#23398;&#20064;&#28857;&#23545;&#21305;&#37197;&#25552;&#20379;&#20102;&#20851;&#38190;&#20449;&#24687;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#35299;&#20915;&#29305;&#24449;&#20998;&#24067;&#20998;&#27495;&#24102;&#26469;&#30340;&#30456;&#20851;&#24615;&#22122;&#22768;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#20266;&#23402;&#29983;&#32593;&#32476;&#26469;&#25913;&#21892;&#29305;&#24449;&#30340;&#19968;&#33268;&#24615;&#12290;&#22312;&#32447;MOD&#12289;YCB-Video&#21644;Occ-LineMOD&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional geometric registration based estimation methods only exploit the CAD model implicitly, which leads to their dependence on observation quality and deficiency to occlusion.To address the problem,the paper proposes a bidirectional correspondence prediction network with a point-wise attention-aware mechanism. This network not only requires the model points to predict the correspondence but also explicitly models the geometric similarities between observations and the model prior.} Our key insight is that the correlations between each model point and scene point provide essential information for learning point-pair matches. To further tackle the correlation noises brought by feature distribution divergence, we design a simple but effective pseudo-siamese network to improve feature homogeneity.Experimental results on the public datasets of LineMOD, YCB-Video, and Occ-LineMOD show that the proposed method achieves better performance than other state-of-the-art methods under the sa
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#22810;&#35270;&#35282;&#28145;&#24230;&#32593;&#32476;&#26041;&#27861;&#35770;&#65292;&#24212;&#29992;&#20110;&#23454;&#39564;&#29289;&#29702;&#20013;&#30340;&#22810;&#31181;&#25104;&#20687;&#34920;&#36798;&#20998;&#26512;&#12290;&#35813;&#26041;&#27861;&#35770;&#35299;&#20915;&#20102;&#22810;&#35270;&#35282;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.08206</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#22810;&#35270;&#35282;&#28145;&#24230;&#32593;&#32476;&#26041;&#27861;&#35770;&#22312;&#23454;&#39564;&#29289;&#29702;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Explainable Multi-View Deep Networks Methodology for Experimental Physics. (arXiv:2308.08206v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08206
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#22810;&#35270;&#35282;&#28145;&#24230;&#32593;&#32476;&#26041;&#27861;&#35770;&#65292;&#24212;&#29992;&#20110;&#23454;&#39564;&#29289;&#29702;&#20013;&#30340;&#22810;&#31181;&#25104;&#20687;&#34920;&#36798;&#20998;&#26512;&#12290;&#35813;&#26041;&#27861;&#35770;&#35299;&#20915;&#20102;&#22810;&#35270;&#35282;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#23454;&#39564;&#24120;&#28041;&#21450;&#22810;&#31181;&#25104;&#20687;&#34920;&#36798;&#65292;&#22914;X&#23556;&#32447;&#25195;&#25551;&#21644;&#26174;&#24494;&#22270;&#20687;&#12290;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#36825;&#20123;&#23454;&#39564;&#30340;&#30417;&#30563;&#20998;&#26512;&#20013;&#12290;&#21512;&#24182;&#19981;&#21516;&#30340;&#22270;&#20687;&#34920;&#36798;&#32463;&#24120;&#38656;&#35201;&#27491;&#30830;&#20998;&#26512;&#21644;&#20570;&#20986;&#20915;&#31574;&#12290;&#22240;&#27492;&#65292;&#22810;&#35270;&#35282;&#25968;&#25454;&#24212;&#36816;&#32780;&#29983; - &#25968;&#25454;&#38598;&#20013;&#30340;&#27599;&#20010;&#26679;&#26412;&#30001;&#26469;&#33258;&#19981;&#21516;&#35282;&#24230;&#12289;&#26469;&#28304;&#25110;&#27169;&#24577;&#30340;&#35270;&#22270;&#25551;&#36848;&#12290;&#22810;&#35270;&#35282;&#23398;&#20064;&#30340;&#27010;&#24565;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;&#29702;&#35299;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#20915;&#31574;&#36807;&#31243;&#23545;&#20110;&#21487;&#38752;&#21644;&#21487;&#20449;&#30340;&#20998;&#26512;&#33267;&#20851;&#37325;&#35201;&#12290;&#22240;&#27492;&#65292;&#26368;&#36817;&#25552;&#20986;&#20102;&#35768;&#22810;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22810;&#35270;&#35282;&#27169;&#22411;&#32570;&#20047;&#36866;&#24403;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#30001;&#20110;&#20854;&#26550;&#26500;&#30340;&#22797;&#26434;&#24615;&#65292;&#38590;&#20197;&#35299;&#37322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#35270;&#35273;&#39046;&#22495;&#30340;&#19981;&#21516;&#22810;&#35270;&#35282;&#26550;&#26500;&#65292;&#27599;&#20010;&#26550;&#26500;&#37117;&#36866;&#21512;&#35299;&#20915;&#19981;&#21516;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#37322;&#22810;&#35270;&#35282;&#27169;&#22411;&#30340;&#26041;&#27861;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physical experiments often involve multiple imaging representations, such as X-ray scans and microscopic images. Deep learning models have been widely used for supervised analysis in these experiments. Combining different image representations is frequently required to analyze and make a decision properly. Consequently, multi-view data has emerged - datasets where each sample is described by views from different angles, sources, or modalities. These problems are addressed with the concept of multi-view learning. Understanding the decision-making process of deep learning models is essential for reliable and credible analysis. Hence, many explainability methods have been devised recently. Nonetheless, there is a lack of proper explainability in multi-view models, which are challenging to explain due to their architectures. In this paper, we suggest different multi-view architectures for the vision domain, each suited to another problem, and we also present a methodology for explaining th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#31232;&#30095;&#35270;&#35282;&#35270;&#39057;&#20013;&#21019;&#24314;&#21487;&#37325;&#20809;&#21644;&#21487;&#21160;&#21270;&#30340;&#31070;&#32463;&#21270;&#36523;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;Hierarchical Distance Query&#65288;HDQ&#65289;&#31639;&#27861;&#26469;&#36817;&#20284;&#20219;&#24847;&#20154;&#20307;&#23039;&#24577;&#19979;&#30340;&#19990;&#30028;&#31354;&#38388;&#36317;&#31163;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#36317;&#31163;&#26469;&#36827;&#34892;&#26448;&#26009;&#24674;&#22797;&#21644;&#37325;&#20809;&#12290;</title><link>http://arxiv.org/abs/2308.07903</link><description>&lt;p&gt;
&#20174;&#31232;&#30095;&#35270;&#35282;&#35270;&#39057;&#20013;&#29983;&#25104;&#21487;&#37325;&#20809;&#21644;&#21487;&#21160;&#21270;&#30340;&#31070;&#32463;&#21270;&#36523;
&lt;/p&gt;
&lt;p&gt;
Relightable and Animatable Neural Avatar from Sparse-View Video. (arXiv:2308.07903v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07903
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#31232;&#30095;&#35270;&#35282;&#35270;&#39057;&#20013;&#21019;&#24314;&#21487;&#37325;&#20809;&#21644;&#21487;&#21160;&#21270;&#30340;&#31070;&#32463;&#21270;&#36523;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;Hierarchical Distance Query&#65288;HDQ&#65289;&#31639;&#27861;&#26469;&#36817;&#20284;&#20219;&#24847;&#20154;&#20307;&#23039;&#24577;&#19979;&#30340;&#19990;&#30028;&#31354;&#38388;&#36317;&#31163;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#36317;&#31163;&#26469;&#36827;&#34892;&#26448;&#26009;&#24674;&#22797;&#21644;&#37325;&#20809;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#20174;&#26410;&#30693;&#29031;&#26126;&#26465;&#20214;&#19979;&#30340;&#31232;&#30095;&#35270;&#35282;&#65288;&#29978;&#33267;&#21333;&#30446;&#65289;&#35270;&#39057;&#20013;&#21019;&#24314;&#21487;&#37325;&#20809;&#21644;&#21487;&#21160;&#21270;&#30340;&#31070;&#32463;&#21270;&#36523;&#30340;&#25361;&#25112;&#12290;&#19982;&#24037;&#20316;&#23460;&#29615;&#22659;&#30456;&#27604;&#65292;&#36825;&#20010;&#35774;&#32622;&#26356;&#23454;&#38469;&#21644;&#21487;&#34892;&#65292;&#20294;&#26159;&#38754;&#20020;&#19968;&#20010;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;&#36870;&#38382;&#39064;&#12290;&#20043;&#21069;&#30340;&#31070;&#32463;&#20154;&#31867;&#37325;&#24314;&#26041;&#27861;&#33021;&#22815;&#20351;&#29992;&#21464;&#24418;&#26377;&#31526;&#21495;&#36317;&#31163;&#22330;&#65288;SDF&#65289;&#20174;&#31232;&#30095;&#35270;&#35282;&#37325;&#24314;&#21487;&#21160;&#21270;&#30340;&#21270;&#36523;&#65292;&#20294;&#26080;&#27861;&#24674;&#22797;&#29992;&#20110;&#37325;&#20809;&#30340;&#26448;&#26009;&#21442;&#25968;&#12290;&#34429;&#28982;&#21487;&#24494;&#36870;&#28210;&#26579;&#26041;&#27861;&#24050;&#25104;&#21151;&#22320;&#24674;&#22797;&#20102;&#38745;&#24577;&#23545;&#35937;&#30340;&#26448;&#26009;&#65292;&#20294;&#23545;&#20110;&#21160;&#24577;&#20154;&#31867;&#65292;&#23558;&#20854;&#25193;&#23637;&#20026;&#21160;&#24577;&#20154;&#20307;&#26159;&#19981;&#30452;&#35266;&#30340;&#65292;&#22240;&#20026;&#22312;&#21464;&#24418;SDF&#19978;&#35745;&#31639;&#20687;&#32032;-&#34920;&#38754;&#30456;&#20132;&#21644;&#20809;&#33021;&#35265;&#24230;&#23545;&#20110;&#36870;&#28210;&#26579;&#26469;&#35828;&#26159;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#36317;&#31163;&#26597;&#35810;&#65288;HDQ&#65289;&#31639;&#27861;&#26469;&#36817;&#20284;&#20219;&#24847;&#20154;&#20307;&#23039;&#24577;&#19979;&#30340;&#19990;&#30028;&#31354;&#38388;&#36317;&#31163;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20272;&#31639;&#20102;&#31895;&#30053;&#30340;&#36317;&#31163;&#20540;&#65292;&#28982;&#21518;&#20351;&#29992;&#36845;&#20195;&#36807;&#31243;&#26469;&#25552;&#39640;&#36317;&#31163;&#20272;&#31639;&#30340;&#31934;&#24230;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#20272;&#31639;&#20986;&#30340;&#36317;&#31163;&#20540;&#36827;&#34892;&#26448;&#26009;&#24674;&#22797;&#21644;&#37325;&#20809;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper tackles the challenge of creating relightable and animatable neural avatars from sparse-view (or even monocular) videos of dynamic humans under unknown illumination. Compared to studio environments, this setting is more practical and accessible but poses an extremely challenging ill-posed problem. Previous neural human reconstruction methods are able to reconstruct animatable avatars from sparse views using deformed Signed Distance Fields (SDF) but cannot recover material parameters for relighting. While differentiable inverse rendering-based methods have succeeded in material recovery of static objects, it is not straightforward to extend them to dynamic humans as it is computationally intensive to compute pixel-surface intersection and light visibility on deformed SDFs for inverse rendering. To solve this challenge, we propose a Hierarchical Distance Query (HDQ) algorithm to approximate the world space distances under arbitrary human poses. Specifically, we estimate coarse
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#21453;&#21521;&#25512;&#29702;&#36827;&#34892;&#39564;&#35777;&#30340;&#26041;&#27861;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#23631;&#34109;&#38382;&#39064;&#20013;&#30340;&#19968;&#20010;&#26631;&#35760;&#65292;&#24182;&#35201;&#27714;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#34987;&#23631;&#34109;&#30340;&#26631;&#35760;&#26469;&#39564;&#35777;&#20505;&#36873;&#31572;&#26696;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#27491;&#21521;&#21644;&#21453;&#21521;&#25512;&#29702;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#20505;&#36873;&#31572;&#26696;&#30340;&#27010;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.07758</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#21453;&#21521;&#25512;&#29702;&#36827;&#34892;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Backward Reasoning in Large Language Models for Verification. (arXiv:2308.07758v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07758
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#21453;&#21521;&#25512;&#29702;&#36827;&#34892;&#39564;&#35777;&#30340;&#26041;&#27861;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#23631;&#34109;&#38382;&#39064;&#20013;&#30340;&#19968;&#20010;&#26631;&#35760;&#65292;&#24182;&#35201;&#27714;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#34987;&#23631;&#34109;&#30340;&#26631;&#35760;&#26469;&#39564;&#35777;&#20505;&#36873;&#31572;&#26696;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#27491;&#21521;&#21644;&#21453;&#21521;&#25512;&#29702;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#20505;&#36873;&#31572;&#26696;&#30340;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38142;&#24335;&#24605;&#32771;&#65288;Chain-of-Though, CoT&#65289;&#25552;&#31034;&#22312;&#21508;&#31181;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;&#26368;&#36817;&#65292;Self-Consistency&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#37319;&#26679;&#19968;&#32452;&#19981;&#21516;&#30340;&#25512;&#29702;&#38142;&#65292;&#36825;&#20123;&#38142;&#21487;&#33021;&#23548;&#33268;&#19981;&#21516;&#30340;&#31572;&#26696;&#65292;&#28982;&#21518;&#36873;&#25321;&#24471;&#31080;&#26368;&#22810;&#30340;&#31572;&#26696;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;&#22312;&#39564;&#35777;&#20505;&#36873;&#31572;&#26696;&#26102;&#20351;&#29992;&#21453;&#21521;&#25512;&#29702;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#31616;&#21333;&#30340;&#27169;&#26495;&#65292;&#21363;``&#22914;&#26524;&#25105;&#20204;&#30693;&#36947;&#19978;&#36848;&#38382;&#39064;&#30340;&#31572;&#26696;&#26159;&#20505;&#36873;&#31572;&#26696;&#65292;&#37027;&#20040;&#26410;&#30693;&#21464;&#37327;x&#30340;&#20540;&#26159;&#22810;&#23569;&#65311;''&#65292;&#23558;&#38382;&#39064;&#20013;&#30340;&#19968;&#20010;&#26631;&#35760;&#23631;&#34109;&#65292;&#24182;&#35201;&#27714;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#34987;&#23631;&#34109;&#30340;&#26631;&#35760;&#12290;&#30452;&#35266;&#19978;&#35762;&#65292;&#22914;&#26524;&#25552;&#20379;&#30340;&#20505;&#36873;&#31572;&#26696;&#26159;&#27491;&#30830;&#30340;&#65292;&#35821;&#35328;&#27169;&#22411;&#24212;&#35813;&#33021;&#22815;&#25104;&#21151;&#39044;&#27979;&#34987;&#23631;&#34109;&#30340;&#26631;&#35760;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;FOBAR&#26041;&#27861;&#65292;&#23558;&#27491;&#21521;&#21644;&#21453;&#21521;&#25512;&#29702;&#32467;&#21512;&#36215;&#26469;&#20272;&#35745;&#20505;&#36873;&#31572;&#26696;&#30340;&#27010;&#29575;&#12290;&#25105;&#20204;&#22312;&#20845;&#20010;&#25968;&#25454;&#38598;&#21644;&#19977;&#20010;&#23454;&#39564;&#20013;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Though (CoT) prompting has shown promising performance in various reasoning tasks. Recently, Self-Consistency \citep{wang2023selfconsistency} proposes to sample a diverse set of reasoning chains which may lead to different answers while the answer that receives the most votes is selected. In this paper, we propose a novel method to use backward reasoning in verifying candidate answers. We mask a token in the question by ${\bf x}$ and ask the LLM to predict the masked token when a candidate answer is provided by \textit{a simple template}, i.e., ``\textit{\textbf{If we know the answer of the above question is \{a candidate answer\}, what is the value of unknown variable ${\bf x}$?}}'' Intuitively, the LLM is expected to predict the masked token successfully if the provided candidate answer is correct. We further propose FOBAR to combine forward and backward reasoning for estimating the probability of candidate answers. We conduct extensive experiments on six data sets and three
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22411;&#21387;&#32553;&#32508;&#36848;&#65292;&#25506;&#35752;&#20102;&#37327;&#21270;&#12289;&#20462;&#21098;&#12289;&#30693;&#35782;&#33976;&#39311;&#31561;&#19981;&#21516;&#26041;&#27861;&#65292;&#24182;&#31361;&#20986;&#20171;&#32461;&#20102;&#26368;&#26032;&#36827;&#23637;&#21644;&#21019;&#26032;&#26041;&#27861;&#65292;&#20026;&#23454;&#29616;&#39640;&#25928;&#30340;&#37096;&#32626;&#25552;&#20379;&#20102;&#37325;&#35201;&#24605;&#36335;&#12290;</title><link>http://arxiv.org/abs/2308.07633</link><description>&lt;p&gt;
&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22411;&#21387;&#32553;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Model Compression for Large Language Models. (arXiv:2308.07633v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07633
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22411;&#21387;&#32553;&#32508;&#36848;&#65292;&#25506;&#35752;&#20102;&#37327;&#21270;&#12289;&#20462;&#21098;&#12289;&#30693;&#35782;&#33976;&#39311;&#31561;&#19981;&#21516;&#26041;&#27861;&#65292;&#24182;&#31361;&#20986;&#20171;&#32461;&#20102;&#26368;&#26032;&#36827;&#23637;&#21644;&#21019;&#26032;&#26041;&#27861;&#65292;&#20026;&#23454;&#29616;&#39640;&#25928;&#30340;&#37096;&#32626;&#25552;&#20379;&#20102;&#37325;&#35201;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20197;&#24778;&#20154;&#30340;&#25104;&#21151;&#24443;&#24213;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24222;&#22823;&#30340;&#20307;&#37327;&#21644;&#35745;&#31639;&#38656;&#27714;&#22312;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#19979;&#30340;&#23454;&#38469;&#37096;&#32626;&#20013;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#38543;&#30528;&#36825;&#20123;&#25361;&#25112;&#26085;&#30410;&#32039;&#36843;&#65292;&#27169;&#22411;&#21387;&#32553;&#39046;&#22495;&#24050;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#26088;&#22312;&#32531;&#35299;&#36825;&#20123;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20221;&#20840;&#38754;&#30340;&#32508;&#36848;&#65292;&#25506;&#35752;&#19987;&#38376;&#38024;&#23545;LLMs&#30340;&#27169;&#22411;&#21387;&#32553;&#25216;&#26415;&#12290;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;&#21508;&#31181;&#26041;&#27861;&#65292;&#21253;&#25324;&#37327;&#21270;&#12289;&#20462;&#21098;&#12289;&#30693;&#35782;&#33976;&#39311;&#31561;&#65292;&#20197;&#24212;&#23545;&#39640;&#25928;&#37096;&#32626;&#30340;&#36843;&#20999;&#38656;&#27714;&#12290;&#22312;&#27599;&#31181;&#25216;&#26415;&#20013;&#65292;&#25105;&#20204;&#37325;&#28857;&#20171;&#32461;&#20102;&#26368;&#26032;&#36827;&#23637;&#21644;&#21019;&#26032;&#26041;&#27861;&#65292;&#20026;LLM&#30740;&#31350;&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#36129;&#29486;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#29992;&#20110;&#35780;&#20272;&#25928;&#26524;&#30340;&#22522;&#20934;&#31574;&#30053;&#21644;&#35780;&#20272;&#25351;&#26631;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have revolutionized natural language processing tasks with remarkable success. However, their formidable size and computational demands present significant challenges for practical deployment, especially in resource-constrained environments. As these challenges become increasingly pertinent, the field of model compression has emerged as a pivotal research area to alleviate these limitations. This paper presents a comprehensive survey that navigates the landscape of model compression techniques tailored specifically for LLMs. Addressing the imperative need for efficient deployment, we delve into various methodologies, encompassing quantization, pruning, knowledge distillation, and more. Within each of these techniques, we highlight recent advancements and innovative approaches that contribute to the evolving landscape of LLM research. Furthermore, we explore benchmarking strategies and evaluation metrics that are essential for assessing the effectiveness of 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;EGP&#30340;&#21019;&#26032;&#30340;&#29109;&#24341;&#23548;&#21098;&#26525;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#36890;&#36807;&#20248;&#20808;&#21098;&#38500;&#29109;&#36739;&#20302;&#30340;&#23618;&#20013;&#30340;&#36830;&#25509;&#26469;&#26377;&#25928;&#21387;&#32553;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#31454;&#20105;&#24615;&#33021;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2308.06619</link><description>&lt;p&gt;
&#33021;&#21542;&#36890;&#36807;&#38750;&#32467;&#26500;&#21270;&#21098;&#26525;&#26469;&#20943;&#23569;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#23618;&#25968;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Unstructured Pruning Reduce the Depth in Deep Neural Networks?. (arXiv:2308.06619v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06619
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;EGP&#30340;&#21019;&#26032;&#30340;&#29109;&#24341;&#23548;&#21098;&#26525;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#36890;&#36807;&#20248;&#20808;&#21098;&#38500;&#29109;&#36739;&#20302;&#30340;&#23618;&#20013;&#30340;&#36830;&#25509;&#26469;&#26377;&#25928;&#21387;&#32553;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#31454;&#20105;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21098;&#26525;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#20943;&#23567;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#22823;&#23567;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26159;&#22312;&#26377;&#32467;&#26500;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#31181;&#25216;&#26415;&#20063;&#24456;&#38590;&#20174;&#27169;&#22411;&#20013;&#23436;&#20840;&#21435;&#38500;&#25972;&#20010;&#23618;&#65306;&#36825;&#26159;&#19968;&#20010;&#21487;&#20197;&#35299;&#20915;&#30340;&#20219;&#21153;&#21527;&#65311;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;EGP&#30340;&#21019;&#26032;&#30340;&#29109;&#24341;&#23548;&#21098;&#26525;&#31639;&#27861;&#65292;&#26088;&#22312;&#20943;&#23567;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#22823;&#23567;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#24615;&#33021;&#12290;EGP&#30340;&#20851;&#38190;&#37325;&#28857;&#26159;&#20248;&#20808;&#21098;&#38500;&#29109;&#36739;&#20302;&#30340;&#23618;&#20013;&#30340;&#36830;&#25509;&#65292;&#26368;&#32456;&#23436;&#20840;&#21435;&#38500;&#36825;&#20123;&#23618;&#12290;&#36890;&#36807;&#22312;ResNet-18&#21644;Swin-T&#31561;&#27969;&#34892;&#27169;&#22411;&#19978;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;EGP&#33021;&#22815;&#26377;&#25928;&#21387;&#32553;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#21516;&#26102;&#20445;&#25345;&#31454;&#20105;&#24615;&#33021;&#27700;&#24179;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#19981;&#20165;&#25581;&#31034;&#20102;&#38750;&#32467;&#26500;&#21270;&#21098;&#26525;&#20248;&#21183;&#32972;&#21518;&#30340;&#26426;&#21046;&#65292;&#36824;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#22797;&#26434;&#30340;&#20851;&#31995;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pruning is a widely used technique for reducing the size of deep neural networks while maintaining their performance. However, such a technique, despite being able to massively compress deep models, is hardly able to remove entire layers from a model (even when structured): is this an addressable task? In this study, we introduce EGP, an innovative Entropy Guided Pruning algorithm aimed at reducing the size of deep neural networks while preserving their performance. The key focus of EGP is to prioritize pruning connections in layers with low entropy, ultimately leading to their complete removal. Through extensive experiments conducted on popular models like ResNet-18 and Swin-T, our findings demonstrate that EGP effectively compresses deep neural networks while maintaining competitive performance levels. Our results not only shed light on the underlying mechanism behind the advantages of unstructured pruning, but also pave the way for further investigations into the intricate relations
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;EgoPoser&#65292;&#19968;&#31181;&#33021;&#22815;&#22312;&#22823;&#22330;&#26223;&#20013;&#40065;&#26834;&#22320;&#23454;&#26102;&#20272;&#35745;&#33258;&#25105;&#36523;&#20307;&#23039;&#21183;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#37325;&#26032;&#24605;&#32771;&#36755;&#20837;&#34920;&#31034;&#12289;&#24341;&#20837;&#26032;&#30340;&#36816;&#21160;&#20998;&#35299;&#26041;&#27861;&#20197;&#21450;&#24314;&#27169;&#36523;&#20307;&#23039;&#21183;&#65292;EgoPoser&#22312;&#23450;&#24615;&#21644;&#23450;&#37327;&#19978;&#22343;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#20855;&#26377;&#36739;&#39640;&#30340;&#25512;&#29702;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2308.06493</link><description>&lt;p&gt;
EgoPoser&#65306;&#22823;&#22330;&#26223;&#19979;&#40065;&#26834;&#30340;&#23454;&#26102;&#33258;&#25105;&#36523;&#20307;&#23039;&#21183;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
EgoPoser: Robust Real-Time Ego-Body Pose Estimation in Large Scenes. (arXiv:2308.06493v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06493
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;EgoPoser&#65292;&#19968;&#31181;&#33021;&#22815;&#22312;&#22823;&#22330;&#26223;&#20013;&#40065;&#26834;&#22320;&#23454;&#26102;&#20272;&#35745;&#33258;&#25105;&#36523;&#20307;&#23039;&#21183;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#37325;&#26032;&#24605;&#32771;&#36755;&#20837;&#34920;&#31034;&#12289;&#24341;&#20837;&#26032;&#30340;&#36816;&#21160;&#20998;&#35299;&#26041;&#27861;&#20197;&#21450;&#24314;&#27169;&#36523;&#20307;&#23039;&#21183;&#65292;EgoPoser&#22312;&#23450;&#24615;&#21644;&#23450;&#37327;&#19978;&#22343;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#20855;&#26377;&#36739;&#39640;&#30340;&#25512;&#29702;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22836;&#37096;&#21644;&#25163;&#37096;&#23039;&#21183;&#20165;&#36890;&#36807;&#23436;&#25972;&#36523;&#20307;&#33258;&#25105;&#23039;&#21183;&#20272;&#35745;&#24050;&#25104;&#20026;&#30740;&#31350;&#30340;&#19968;&#20010;&#28909;&#28857;&#39046;&#22495;&#65292;&#20197;&#20026;&#22836;&#25140;&#24335;&#24179;&#21488;&#19978;&#30340;&#34394;&#25311;&#35282;&#33394;&#34920;&#36798;&#25552;&#20379;&#21160;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#36807;&#20110;&#20381;&#36182;&#25968;&#25454;&#38598;&#35760;&#24405;&#26102;&#30340;&#36816;&#21160;&#25429;&#25417;&#31354;&#38388;&#30340;&#38480;&#21046;&#65292;&#21516;&#26102;&#20551;&#35774;&#36830;&#32493;&#25429;&#25417;&#20851;&#33410;&#36816;&#21160;&#21644;&#22343;&#21248;&#36523;&#20307;&#23610;&#23544;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EgoPoser&#65292;&#36890;&#36807;&#20197;&#19979;&#26041;&#24335;&#20811;&#26381;&#20102;&#36825;&#20123;&#38480;&#21046;&#65306;1&#65289;&#37325;&#26032;&#24605;&#32771;&#22522;&#20110;&#22836;&#25140;&#24335;&#24179;&#21488;&#30340;&#33258;&#25105;&#23039;&#21183;&#20272;&#35745;&#30340;&#36755;&#20837;&#34920;&#31034;&#65292;&#24182;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#36816;&#21160;&#20998;&#35299;&#26041;&#27861;&#26469;&#39044;&#27979;&#19982;&#20840;&#23616;&#20301;&#32622;&#26080;&#20851;&#30340;&#23436;&#25972;&#36523;&#20307;&#23039;&#21183;&#65292;2&#65289;&#20174;&#22836;&#25140;&#24335;&#35774;&#22791;&#35270;&#37326;&#20869;&#30340;&#38388;&#27463;&#24615;&#25163;&#37096;&#23039;&#21183;&#36319;&#36394;&#20013;&#40065;&#26834;&#22320;&#24314;&#27169;&#36523;&#20307;&#23039;&#21183;&#65292;3&#65289;&#38024;&#23545;&#19981;&#21516;&#29992;&#25143;&#30340;&#21508;&#31181;&#36523;&#20307;&#23610;&#23544;&#36827;&#34892;&#36890;&#29992;&#21270;&#25512;&#24191;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;EgoPoser&#22312;&#23450;&#24615;&#21644;&#23450;&#37327;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#65292;&#24182;&#20445;&#25345;&#36739;&#39640;&#30340;&#25512;&#29702;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Full-body ego-pose estimation from head and hand poses alone has become an active area of research to power articulate avatar representation on headset-based platforms. However, existing methods over-rely on the confines of the motion-capture spaces in which datasets were recorded, while simultaneously assuming continuous capture of joint motions and uniform body dimensions. In this paper, we propose EgoPoser, which overcomes these limitations by 1) rethinking the input representation for headset-based ego-pose estimation and introducing a novel motion decomposition method that predicts full-body pose independent of global positions, 2) robustly modeling body pose from intermittent hand position and orientation tracking only when inside a headset's field of view, and 3) generalizing across various body sizes for different users. Our experiments show that EgoPoser outperforms state-of-the-art methods both qualitatively and quantitatively, while maintaining a high inference speed of over
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38899;&#39057;-&#35270;&#35273;&#31354;&#38388;&#34701;&#21512;&#32593;&#32476;&#65292;&#36890;&#36807;&#25972;&#21512;&#38899;&#39057;&#21644;&#35270;&#35273;&#27169;&#24577;&#30340;&#31354;&#38388;&#32447;&#32034;&#26469;&#27169;&#20223;&#20154;&#31867;&#26816;&#27979;&#22768;&#38899;&#20135;&#29983;&#29289;&#20307;&#30340;&#34892;&#20026;&#65292;&#24182;&#24341;&#20837;&#36882;&#24402;&#27880;&#24847;&#21147;&#32593;&#32476;&#26469;&#24471;&#21040;&#26356;&#20934;&#30830;&#30340;&#27880;&#24847;&#21147;&#21306;&#22495;&#12290;&#36890;&#36807;&#38899;&#39057;&#21644;&#35270;&#35273;&#27169;&#24577;&#30340;&#31354;&#38388;&#32447;&#32034;&#21644;&#36882;&#24402;&#32858;&#28966;&#31574;&#30053;&#65292;&#26041;&#27861;&#22312;&#22768;&#28304;&#23450;&#20301;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.06087</link><description>&lt;p&gt;
&#40065;&#26834;&#30340;&#22768;&#28304;&#23450;&#20301;&#30340;&#38899;&#39057;-&#35270;&#35273;&#31354;&#38388;&#34701;&#21512;&#21644;&#36882;&#24402;&#27880;&#24847;&#21147;
&lt;/p&gt;
&lt;p&gt;
Audio-Visual Spatial Integration and Recursive Attention for Robust Sound Source Localization. (arXiv:2308.06087v1 [cs.MM] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06087
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38899;&#39057;-&#35270;&#35273;&#31354;&#38388;&#34701;&#21512;&#32593;&#32476;&#65292;&#36890;&#36807;&#25972;&#21512;&#38899;&#39057;&#21644;&#35270;&#35273;&#27169;&#24577;&#30340;&#31354;&#38388;&#32447;&#32034;&#26469;&#27169;&#20223;&#20154;&#31867;&#26816;&#27979;&#22768;&#38899;&#20135;&#29983;&#29289;&#20307;&#30340;&#34892;&#20026;&#65292;&#24182;&#24341;&#20837;&#36882;&#24402;&#27880;&#24847;&#21147;&#32593;&#32476;&#26469;&#24471;&#21040;&#26356;&#20934;&#30830;&#30340;&#27880;&#24847;&#21147;&#21306;&#22495;&#12290;&#36890;&#36807;&#38899;&#39057;&#21644;&#35270;&#35273;&#27169;&#24577;&#30340;&#31354;&#38388;&#32447;&#32034;&#21644;&#36882;&#24402;&#32858;&#28966;&#31574;&#30053;&#65292;&#26041;&#27861;&#22312;&#22768;&#28304;&#23450;&#20301;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22768;&#28304;&#23450;&#20301;&#20219;&#21153;&#30340;&#30446;&#26631;&#26159;&#35753;&#26426;&#22120;&#33021;&#22815;&#22312;&#35270;&#35273;&#22330;&#26223;&#20013;&#26816;&#27979;&#20986;&#22768;&#38899;&#20135;&#29983;&#29289;&#20307;&#30340;&#20301;&#32622;&#12290;&#34429;&#28982;&#38899;&#39057;&#27169;&#24577;&#25552;&#20379;&#20102;&#23450;&#20301;&#22768;&#28304;&#30340;&#31354;&#38388;&#32447;&#32034;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#20165;&#23558;&#38899;&#39057;&#20316;&#20026;&#35270;&#35273;&#27169;&#24577;&#31354;&#38388;&#21306;&#22495;&#27604;&#36739;&#30340;&#36741;&#21161;&#35282;&#33394;&#12290;&#32780;&#20154;&#31867;&#21017;&#21033;&#29992;&#38899;&#39057;&#21644;&#35270;&#35273;&#27169;&#24577;&#20316;&#20026;&#23450;&#20301;&#22768;&#28304;&#30340;&#31354;&#38388;&#32447;&#32034;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38899;&#39057;-&#35270;&#35273;&#31354;&#38388;&#34701;&#21512;&#32593;&#32476;&#65292;&#36890;&#36807;&#25972;&#21512;&#20004;&#31181;&#27169;&#24577;&#30340;&#31354;&#38388;&#32447;&#32034;&#26469;&#27169;&#20223;&#20154;&#31867;&#26816;&#27979;&#22768;&#38899;&#20135;&#29983;&#29289;&#20307;&#26102;&#30340;&#34892;&#20026;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#36882;&#24402;&#27880;&#24847;&#21147;&#32593;&#32476;&#26469;&#27169;&#20223;&#20154;&#31867;&#36845;&#20195;&#22320;&#32858;&#28966;&#23545;&#35937;&#65292;&#20174;&#32780;&#24471;&#21040;&#26356;&#20934;&#30830;&#30340;&#27880;&#24847;&#21147;&#21306;&#22495;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#32534;&#30721;&#20004;&#31181;&#27169;&#24577;&#30340;&#31354;&#38388;&#20449;&#24687;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38899;&#39057;-&#35270;&#35273;&#37197;&#23545;&#21305;&#37197;&#25439;&#22833;&#21644;&#31354;&#38388;&#21306;&#22495;&#23545;&#40784;&#25439;&#22833;&#12290;&#36890;&#36807;&#21033;&#29992;&#38899;&#39057;-&#35270;&#35273;&#27169;&#24577;&#30340;&#31354;&#38388;&#32447;&#32034;&#21644;&#36882;&#24402;&#32858;&#28966;&#30340;&#31574;&#30053;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22768;&#28304;&#23450;&#20301;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The objective of the sound source localization task is to enable machines to detect the location of sound-making objects within a visual scene. While the audio modality provides spatial cues to locate the sound source, existing approaches only use audio as an auxiliary role to compare spatial regions of the visual modality. Humans, on the other hand, utilize both audio and visual modalities as spatial cues to locate sound sources. In this paper, we propose an audio-visual spatial integration network that integrates spatial cues from both modalities to mimic human behavior when detecting sound-making objects. Additionally, we introduce a recursive attention network to mimic human behavior of iterative focusing on objects, resulting in more accurate attention regions. To effectively encode spatial information from both modalities, we propose audio-visual pair matching loss and spatial region alignment loss. By utilizing the spatial cues of audio-visual modalities and recursively focusing
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#26159;&#39318;&#27425;&#25506;&#32034;&#22522;&#20110;&#23618;&#27425;&#20869;&#23384;&#22238;&#25918;&#30340;&#25345;&#32493;&#23398;&#20064;&#30340;&#35774;&#35745;&#31354;&#38388;&#65292;&#26088;&#22312;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#23454;&#29616;&#25104;&#26412;&#25928;&#30410;&#12290;&#25552;&#20986;&#20102;Miro&#65292;&#19968;&#20010;&#36890;&#36807;&#21160;&#24577;&#37197;&#32622;&#25345;&#32493;&#23398;&#20064;&#31995;&#32479;&#30340;&#26032;&#39062;&#31995;&#32479;&#36816;&#34892;&#26102;&#65292;&#20197;&#23454;&#29616;&#26368;&#20339;&#30340;&#25104;&#26412;&#25928;&#30410;&#12290;&#24191;&#27867;&#30340;&#35780;&#20272;&#26174;&#31034;Miro&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2308.06053</link><description>&lt;p&gt;
&#22312;&#20869;&#23384;&#23618;&#27425;&#32467;&#26500;&#19978;&#20855;&#26377;MiRo&#30340;&#25104;&#26412;&#25928;&#30410;&#30340;&#35774;&#22791;&#19978;&#30340;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Cost-effective On-device Continual Learning over Memory Hierarchy with Miro. (arXiv:2308.06053v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06053
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#26159;&#39318;&#27425;&#25506;&#32034;&#22522;&#20110;&#23618;&#27425;&#20869;&#23384;&#22238;&#25918;&#30340;&#25345;&#32493;&#23398;&#20064;&#30340;&#35774;&#35745;&#31354;&#38388;&#65292;&#26088;&#22312;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#23454;&#29616;&#25104;&#26412;&#25928;&#30410;&#12290;&#25552;&#20986;&#20102;Miro&#65292;&#19968;&#20010;&#36890;&#36807;&#21160;&#24577;&#37197;&#32622;&#25345;&#32493;&#23398;&#20064;&#31995;&#32479;&#30340;&#26032;&#39062;&#31995;&#32479;&#36816;&#34892;&#26102;&#65292;&#20197;&#23454;&#29616;&#26368;&#20339;&#30340;&#25104;&#26412;&#25928;&#30410;&#12290;&#24191;&#27867;&#30340;&#35780;&#20272;&#26174;&#31034;Miro&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#26159;&#20174;&#25345;&#32493;&#30340;&#20219;&#21153;&#27969;&#20013;&#36880;&#27493;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#20026;&#20102;&#35760;&#20303;&#20808;&#21069;&#23398;&#21040;&#30340;&#30693;&#35782;&#65292;&#20043;&#21069;&#30340;&#30740;&#31350;&#23558;&#26087;&#26679;&#26412;&#23384;&#20648;&#22312;&#19968;&#20010;&#20869;&#23384;&#23618;&#27425;&#32467;&#26500;&#20013;&#65292;&#24182;&#22312;&#26032;&#20219;&#21153;&#21040;&#26469;&#26102;&#36827;&#34892;&#22238;&#25918;&#12290;&#37319;&#29992;&#25345;&#32493;&#23398;&#20064;&#20197;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#36793;&#32536;&#35774;&#22791;&#36890;&#24120;&#23545;&#33021;&#28304;&#25935;&#24863;&#65292;&#22240;&#27492;&#38656;&#35201;&#22312;&#19981;&#25439;&#23475;&#33021;&#28304;&#25928;&#29575;&#30340;&#24773;&#20917;&#19979;&#20445;&#25345;&#39640;&#27169;&#22411;&#20934;&#30830;&#24230;&#65292;&#21363;&#25104;&#26412;&#25928;&#30410;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26159;&#39318;&#27425;&#25506;&#32034;&#22522;&#20110;&#23618;&#27425;&#20869;&#23384;&#22238;&#25918;&#30340;&#25345;&#32493;&#23398;&#20064;&#30340;&#35774;&#35745;&#31354;&#38388;&#65292;&#20197;&#33719;&#24471;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#25104;&#26412;&#25928;&#30410;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Miro&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;&#31995;&#32479;&#36816;&#34892;&#26102;&#65292;&#36890;&#36807;&#20351;&#20854;&#33021;&#22815;&#26681;&#25454;&#36164;&#28304;&#29366;&#24577;&#21160;&#24577;&#37197;&#32622;&#25345;&#32493;&#23398;&#20064;&#31995;&#32479;&#65292;&#20174;&#32780;&#23558;&#25105;&#20204;&#30340;&#35265;&#35299;&#31934;&#30830;&#22320;&#25972;&#21512;&#21040;&#25345;&#32493;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;&#20197;&#23454;&#29616;&#26368;&#20339;&#25104;&#26412;&#25928;&#30410;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;Miro&#36824;&#23545;&#24102;&#26377;&#26126;&#30830;&#20934;&#30830;&#24230;-&#33021;&#37327;&#24179;&#34913;&#30340;&#21442;&#25968;&#36827;&#34892;&#22312;&#32447;&#20998;&#26512;&#65292;&#24182;&#20197;&#20302;&#24320;&#38144;&#22320;&#36866;&#24212;&#26368;&#20339;&#20540;&#12290;&#24191;&#27867;&#30340;&#35780;&#20272;&#26174;&#31034;Miro&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual learning (CL) trains NN models incrementally from a continuous stream of tasks. To remember previously learned knowledge, prior studies store old samples over a memory hierarchy and replay them when new tasks arrive. Edge devices that adopt CL to preserve data privacy are typically energy-sensitive and thus require high model accuracy while not compromising energy efficiency, i.e., cost-effectiveness. Our work is the first to explore the design space of hierarchical memory replay-based CL to gain insights into achieving cost-effectiveness on edge devices. We present Miro, a novel system runtime that carefully integrates our insights into the CL framework by enabling it to dynamically configure the CL system based on resource states for the best cost-effectiveness. To reach this goal, Miro also performs online profiling on parameters with clear accuracy-energy trade-offs and adapts to optimal values with low overhead. Extensive evaluations show that Miro significantly outperfo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#20219;&#21153;&#29305;&#23450;&#30340;&#24213;&#23618;&#34920;&#31034;&#32593;&#32476;&#65288;DTRN&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#20219;&#21153;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#36127;&#36801;&#31227;&#38382;&#39064;&#65292;&#36890;&#36807;&#26126;&#30830;&#33719;&#21462;&#27599;&#20010;&#20219;&#21153;&#30340;&#24213;&#23618;&#34920;&#31034;&#26469;&#25913;&#21892;&#20219;&#21153;&#29305;&#23450;&#29305;&#24449;&#30340;&#25429;&#25417;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.05996</link><description>&lt;p&gt;
&#28145;&#24230;&#20219;&#21153;&#29305;&#23450;&#30340;&#24213;&#23618;&#34920;&#31034;&#32593;&#32476;&#29992;&#20110;&#22810;&#20219;&#21153;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Deep Task-specific Bottom Representation Network for Multi-Task Recommendation. (arXiv:2308.05996v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05996
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#20219;&#21153;&#29305;&#23450;&#30340;&#24213;&#23618;&#34920;&#31034;&#32593;&#32476;&#65288;DTRN&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#20219;&#21153;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#36127;&#36801;&#31227;&#38382;&#39064;&#65292;&#36890;&#36807;&#26126;&#30830;&#33719;&#21462;&#27599;&#20010;&#20219;&#21153;&#30340;&#24213;&#23618;&#34920;&#31034;&#26469;&#25913;&#21892;&#20219;&#21153;&#29305;&#23450;&#29305;&#24449;&#30340;&#25429;&#25417;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#24182;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#12290;&#26368;&#36817;&#30340;&#28145;&#24230;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#65288;&#22914;MMoE&#12289;PLE&#65289;&#19987;&#27880;&#20110;&#35774;&#35745;&#22522;&#20110;&#36719;&#38376;&#25511;&#30340;&#21442;&#25968;&#20849;&#20139;&#32593;&#32476;&#65292;&#38544;&#24335;&#22320;&#23398;&#20064;&#27599;&#20010;&#20219;&#21153;&#30340;&#27867;&#21270;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#24403;&#22788;&#29702;&#20914;&#31361;&#20219;&#21153;&#26102;&#65292;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#21487;&#33021;&#20250;&#36973;&#21463;&#24615;&#33021;&#36864;&#21270;&#65292;&#22240;&#20026;&#36127;&#36801;&#31227;&#25928;&#24212;&#21487;&#33021;&#21457;&#29983;&#22312;&#20219;&#21153;&#20849;&#20139;&#30340;&#24213;&#23618;&#34920;&#31034;&#19978;&#12290;&#36825;&#21487;&#33021;&#23548;&#33268;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#25429;&#25417;&#20219;&#21153;&#29305;&#23450;&#29305;&#24449;&#30340;&#33021;&#21147;&#38477;&#20302;&#65292;&#26368;&#32456;&#24433;&#21709;&#20854;&#25928;&#26524;&#65292;&#24182;&#22952;&#30861;&#20854;&#22312;&#25152;&#26377;&#20219;&#21153;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#25512;&#33616;&#31995;&#32479;&#20013;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#24213;&#23618;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#25552;&#20986;&#20102;&#28145;&#24230;&#20219;&#21153;&#29305;&#23450;&#30340;&#24213;&#23618;&#34920;&#31034;&#32593;&#32476;&#65288;DTRN&#65289;&#20197;&#32531;&#35299;&#36127;&#36801;&#31227;&#38382;&#39064;&#12290;DTRN&#36890;&#36807;&#20351;&#27599;&#20010;&#20219;&#21153;&#20855;&#26377;&#33258;&#24049;&#30340;&#34920;&#31034;&#23398;&#20064;&#26126;&#30830;&#22320;&#33719;&#21462;&#20219;&#21153;&#29305;&#23450;&#30340;&#24213;&#23618;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural-based multi-task learning (MTL) has gained significant improvement, and it has been successfully applied to recommendation system (RS). Recent deep MTL methods for RS (e.g. MMoE, PLE) focus on designing soft gating-based parameter-sharing networks that implicitly learn a generalized representation for each task. However, MTL methods may suffer from performance degeneration when dealing with conflicting tasks, as negative transfer effects can occur on the task-shared bottom representation. This can result in a reduced capacity for MTL methods to capture task-specific characteristics, ultimately impeding their effectiveness and hindering the ability to generalize well on all tasks. In this paper, we focus on the bottom representation learning of MTL in RS and propose the Deep Task-specific Bottom Representation Network (DTRN) to alleviate the negative transfer problem. DTRN obtains task-specific bottom representation explicitly by making each task has its own representation learni
&lt;/p&gt;</description></item><item><title>DiLogics&#26159;&#19968;&#20010;&#36890;&#36807;&#28436;&#31034;&#32534;&#31243;&#30340;&#31995;&#32479;&#65292;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24110;&#21161;&#29992;&#25143;&#21019;&#24314;&#22788;&#29702;&#22810;&#26679;&#21270;&#35268;&#33539;&#30340;Web&#33258;&#21160;&#21270;&#31243;&#24207;&#12290;</title><link>http://arxiv.org/abs/2308.05828</link><description>&lt;p&gt;
DiLogics&#65306;&#21033;&#29992;&#19981;&#21516;&#36923;&#36753;&#21019;&#24314;Web&#33258;&#21160;&#21270;&#31243;&#24207;
&lt;/p&gt;
&lt;p&gt;
DiLogics: Creating Web Automation Programs With Diverse Logics. (arXiv:2308.05828v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05828
&lt;/p&gt;
&lt;p&gt;
DiLogics&#26159;&#19968;&#20010;&#36890;&#36807;&#28436;&#31034;&#32534;&#31243;&#30340;&#31995;&#32479;&#65292;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24110;&#21161;&#29992;&#25143;&#21019;&#24314;&#22788;&#29702;&#22810;&#26679;&#21270;&#35268;&#33539;&#30340;Web&#33258;&#21160;&#21270;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#24037;&#20316;&#32773;&#32463;&#24120;&#36935;&#21040;&#37325;&#22797;&#30340;&#32593;&#32476;&#25968;&#25454;&#36755;&#20837;&#20219;&#21153;&#65292;&#20363;&#22914;&#26356;&#26032;&#35760;&#24405;&#25110;&#19979;&#35746;&#21333;&#12290;&#32593;&#32476;&#33258;&#21160;&#21270;&#21487;&#20197;&#25552;&#39640;&#29983;&#20135;&#21147;&#65292;&#20294;&#20934;&#30830;&#22320;&#23558;&#20219;&#21153;&#36716;&#21270;&#20026;&#32593;&#32476;&#25805;&#20316;&#24182;&#25193;&#23637;&#21040;&#26032;&#30340;&#35268;&#33539;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#29616;&#26377;&#30340;&#24037;&#20855;&#21487;&#20197;&#33258;&#21160;&#21270;&#25191;&#34892;&#30456;&#21516;UI&#25805;&#20316;&#30340;&#20219;&#21153;&#65288;&#20363;&#22914;&#65292;&#25353;&#39034;&#24207;&#22312;&#27599;&#20010;&#23383;&#27573;&#20013;&#36755;&#20837;&#25991;&#26412;&#65289;&#65292;&#20294;&#19981;&#25903;&#25345;&#26681;&#25454;&#19981;&#21516;&#30340;&#36755;&#20837;&#26465;&#20214;&#36827;&#34892;&#19981;&#21516;&#25191;&#34892;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;DiLogics&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#36807;&#28436;&#31034;&#32534;&#31243;&#30340;&#31995;&#32479;&#65292;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24110;&#21161;&#29992;&#25143;&#21019;&#24314;&#22788;&#29702;&#22810;&#26679;&#21270;&#35268;&#33539;&#30340;Web&#33258;&#21160;&#21270;&#31243;&#24207;&#12290;DiLogics&#39318;&#20808;&#23558;&#36755;&#20837;&#25968;&#25454;&#35821;&#20041;&#20998;&#21106;&#20026;&#32467;&#26500;&#21270;&#30340;&#20219;&#21153;&#27493;&#39588;&#12290;&#36890;&#36807;&#20026;&#27599;&#20010;&#27493;&#39588;&#35760;&#24405;&#29992;&#25143;&#28436;&#31034;&#65292;DiLogics&#23558;&#32593;&#32476;&#23439;&#27867;&#21270;&#20026;&#26032;&#39062;&#20294;&#22312;&#35821;&#20041;&#19978;&#30456;&#20284;&#30340;&#20219;&#21153;&#35201;&#27714;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;&#38750;&#19987;&#23478;&#21487;&#20197;&#26377;&#25928;&#20351;&#29992;DiLogics&#21019;&#24314;&#28385;&#36275;&#22810;&#26679;&#21270;&#36755;&#20837;&#25351;&#20196;&#30340;&#33258;&#21160;&#21270;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge workers frequently encounter repetitive web data entry tasks, like updating records or placing orders. Web automation increases productivity, but translating tasks to web actions accurately and extending to new specifications is challenging. Existing tools can automate tasks that perform the same logical trace of UI actions (e.g., input text in each field in order), but do not support tasks requiring different executions based on varied input conditions. We present DiLogics, a programming-by-demonstration system that utilizes NLP to assist users in creating web automation programs that handle diverse specifications. DiLogics first semantically segments input data to structured task steps. By recording user demonstrations for each step, DiLogics generalizes the web macros to novel but semantically similar task requirements. Our evaluation showed that non-experts can effectively use DiLogics to create automation programs that fulfill diverse input instructions. DiLogics provide
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#22522;&#20110;&#39592;&#39612;&#30340;&#20154;&#20307;&#21160;&#20316;&#35782;&#21035;&#26041;&#27861;&#30340;&#33030;&#24369;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30828;&#24615;&#26080;&#30418;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#36816;&#21160;&#27969;&#24418;&#21644;&#23450;&#20041;&#39592;&#39612;-&#21160;&#20316;&#30693;&#24773;&#26799;&#24230;&#26469;&#25915;&#20987;&#27169;&#22411;&#65292;&#25581;&#31034;&#20102;&#36825;&#31181;&#33030;&#24369;&#24615;&#30340;&#23384;&#22312;&#12290;</title><link>http://arxiv.org/abs/2308.05681</link><description>&lt;p&gt;
&#22522;&#20110;&#39592;&#39612;&#30340;&#20154;&#20307;&#21160;&#20316;&#35782;&#21035;&#38754;&#20020;&#30340;&#30828;&#24615;&#26080;&#30418;&#23545;&#25239;&#25915;&#20987;&#21644;&#39592;&#39612;-&#21160;&#20316;&#30693;&#24773;&#26799;&#24230;
&lt;/p&gt;
&lt;p&gt;
Hard No-Box Adversarial Attack on Skeleton-Based Human Action Recognition with Skeleton-Motion-Informed Gradient. (arXiv:2308.05681v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05681
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#22522;&#20110;&#39592;&#39612;&#30340;&#20154;&#20307;&#21160;&#20316;&#35782;&#21035;&#26041;&#27861;&#30340;&#33030;&#24369;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30828;&#24615;&#26080;&#30418;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#36816;&#21160;&#27969;&#24418;&#21644;&#23450;&#20041;&#39592;&#39612;-&#21160;&#20316;&#30693;&#24773;&#26799;&#24230;&#26469;&#25915;&#20987;&#27169;&#22411;&#65292;&#25581;&#31034;&#20102;&#36825;&#31181;&#33030;&#24369;&#24615;&#30340;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#39592;&#39612;&#30340;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#26041;&#27861;&#24050;&#34987;&#35777;&#26126;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#25915;&#20987;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25915;&#20987;&#26041;&#27861;&#35201;&#27714;&#35201;&#20040;&#23436;&#20840;&#20102;&#35299;&#21463;&#23475;&#32773;&#65288;&#21363;&#30333;&#30418;&#25915;&#20987;&#65289;&#65292;&#35201;&#20040;&#26377;&#35775;&#38382;&#35757;&#32451;&#25968;&#25454;&#65288;&#21363;&#22522;&#20110;&#36716;&#31227;&#30340;&#25915;&#20987;&#65289;&#65292;&#25110;&#32773;&#39057;&#32321;&#26597;&#35810;&#27169;&#22411;&#65288;&#21363;&#40657;&#30418;&#25915;&#20987;&#65289;&#12290;&#25152;&#26377;&#36825;&#20123;&#35201;&#27714;&#37117;&#38750;&#24120;&#38480;&#21046;&#24615;&#65292;&#24341;&#21457;&#20102;&#23545;&#33030;&#24369;&#24615;&#30340;&#36136;&#30097;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#33030;&#24369;&#24615;&#30830;&#23454;&#23384;&#22312;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#26032;&#30340;&#25915;&#20987;&#20219;&#21153;&#65306;&#25915;&#20987;&#32773;&#26080;&#27861;&#35775;&#38382;&#21463;&#23475;&#32773;&#27169;&#22411;&#25110;&#35757;&#32451;&#25968;&#25454;&#25110;&#26631;&#31614;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#30828;&#24615;&#26080;&#30418;&#25915;&#20987;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#23398;&#20064;&#19968;&#20010;&#36816;&#21160;&#27969;&#24418;&#65292;&#28982;&#21518;&#23450;&#20041;&#19968;&#20010;&#29992;&#20110;&#35745;&#31639;&#25915;&#20987;&#30340;&#23545;&#25239;&#25439;&#22833;&#20989;&#25968;&#65292;&#31216;&#20026;&#39592;&#39612;-&#21160;&#20316;&#30693;&#24773;&#26799;&#24230;&#65288;SMI&#26799;&#24230;&#65289;&#12290;&#25105;&#20204;&#30340;&#26799;&#24230;&#21253;&#21547;&#36816;&#21160;&#21160;&#21147;&#23398;&#30340;&#20449;&#24687;&#65292;&#36825;&#19982;&#29616;&#26377;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#25915;&#20987;&#26041;&#27861;&#19981;&#21516;&#65292;&#21518;&#32773;&#20551;&#35774;&#25439;&#22833;&#26799;&#24230;&#26159;&#36890;&#36807;&#35745;&#31639;&#32780;&#26469;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, methods for skeleton-based human activity recognition have been shown to be vulnerable to adversarial attacks. However, these attack methods require either the full knowledge of the victim (i.e. white-box attacks), access to training data (i.e. transfer-based attacks) or frequent model queries (i.e. black-box attacks). All their requirements are highly restrictive, raising the question of how detrimental the vulnerability is. In this paper, we show that the vulnerability indeed exists. To this end, we consider a new attack task: the attacker has no access to the victim model or the training data or labels, where we coin the term hard no-box attack. Specifically, we first learn a motion manifold where we define an adversarial loss to compute a new gradient for the attack, named skeleton-motion-informed (SMI) gradient. Our gradient contains information of the motion dynamics, which is different from existing gradient-based attack methods that compute the loss gradient assuming 
&lt;/p&gt;</description></item><item><title>&#20803;&#35748;&#30693;&#25552;&#31034; (MP) &#26159;&#19968;&#31181;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#29702;&#35299;&#33021;&#21147;&#30340;&#31574;&#30053;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;MP&#30340;PaLM&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#25509;&#36817;&#20110;GPT-4&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2308.05342</link><description>&lt;p&gt;
&#20803;&#35748;&#30693;&#25552;&#31034;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29702;&#35299;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Metacognitive Prompting Improves Understanding in Large Language Models. (arXiv:2308.05342v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05342
&lt;/p&gt;
&lt;p&gt;
&#20803;&#35748;&#30693;&#25552;&#31034; (MP) &#26159;&#19968;&#31181;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#29702;&#35299;&#33021;&#21147;&#30340;&#31574;&#30053;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;MP&#30340;PaLM&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#25509;&#36817;&#20110;GPT-4&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#20013;&#65292;&#36890;&#36807;&#26377;&#25928;&#30340;&#25552;&#31034;&#35774;&#35745;&#65292;&#20219;&#21153;&#29305;&#23450;&#24615;&#33021;&#19968;&#30452;&#22312;&#19981;&#26029;&#25552;&#39640;&#12290;&#23613;&#31649;&#26368;&#36817;&#20851;&#20110;&#25552;&#31034;&#30340;&#30740;&#31350;&#22686;&#24378;&#20102;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20294;&#22312;&#36827;&#19968;&#27493;&#25552;&#39640;&#23427;&#20204;&#30340;&#29702;&#35299;&#33021;&#21147;&#26041;&#38754;&#20173;&#23384;&#22312;&#24046;&#36317;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20803;&#35748;&#30693;&#25552;&#31034; (MP)&#65292;&#36825;&#26159;&#19968;&#31181;&#21463;&#20154;&#31867;&#20869;&#30465;&#25512;&#29702;&#36807;&#31243;&#21551;&#21457;&#30340;&#31574;&#30053;&#12290;&#20351;&#29992;MP&#65292;LLMs&#32463;&#21382;&#19968;&#31995;&#21015;&#26377;&#32467;&#26500;&#12289;&#33258;&#25105;&#24847;&#35782;&#30340;&#35780;&#20272;&#65292;&#21033;&#29992;&#20854;&#20016;&#23500;&#30340;&#20869;&#22312;&#30693;&#35782;&#21644;&#26032;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#28041;&#21450;&#20116;&#20010;&#24120;&#35265;&#30340;LLMs&#65306;Llama2&#12289;Vicuna&#12289;PaLM&#12289;GPT-3.5&#21644;GPT-4&#65292;&#23427;&#20204;&#37117;&#28085;&#30422;&#20102;&#26469;&#33258;GLUE&#21644;SuperGLUE&#22522;&#20934;&#27979;&#35797;&#30340;&#21508;&#31181;&#36890;&#29992;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299; (NLU) &#20219;&#21153;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;GPT-4&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#20013;&#22987;&#32456;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#37197;&#22791;MP&#30340;PaLM&#25509;&#36817;&#20854;&#24615;&#33021;&#27700;&#24179;&#12290;&#27492;&#22806;&#65292;&#36328;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#65292;MP&#22987;&#32456;&#20248;&#20110;&#29616;&#26377;&#30340;&#25552;&#31034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Large Language Models (LLMs), there have been consistent advancements in task-specific performance, largely influenced by effective prompt design. While recent research on prompting has enhanced the reasoning capabilities of LLMs, a gap remains in further improving their understanding abilities. In this study, we introduce metacognitive prompting (MP), a strategy inspired by human introspective reasoning processes. Using MP, LLMs undergo a systematic series of structured, self-aware evaluations, drawing on both their vast inherent knowledge and new insights. Our experiments involve five prevalent LLMs: Llama2, Vicuna, PaLM, GPT-3.5, and GPT-4, all of which span various general natural language understanding (NLU) tasks from the GLUE and SuperGLUE benchmarks. Results indicate that, although GPT-4 consistently excels in most tasks, PaLM, when equipped with MP, approaches its performance level. Furthermore, across models and datasets, MP consistently outperforms existing prompting meth
&lt;/p&gt;</description></item><item><title>AI&#29983;&#25104;&#24335;&#27169;&#22411;&#21487;&#33021;&#20250;&#20135;&#29983;&#20855;&#26377;&#28508;&#22312;&#36131;&#20219;&#39118;&#38505;&#30340;&#26377;&#23475;&#35328;&#35770;&#12290;&#35299;&#20915;&#27169;&#22411;&#21019;&#24314;&#32773;&#21644;&#37096;&#32626;&#32773;&#30340;&#27861;&#24459;&#36131;&#20219;&#38382;&#39064;&#30340;&#20851;&#38190;&#22312;&#20110;&#31639;&#27861;&#35774;&#35745;&#30340;&#25216;&#26415;&#32454;&#33410;&#12290;&#38656;&#35201;&#36827;&#34892;&#28145;&#20837;&#30340;Section 230&#20813;&#36131;&#20998;&#26512;&#20197;&#21450;&#19979;&#28216;&#36131;&#20219;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2308.04635</link><description>&lt;p&gt;
AI&#26377;&#23475;&#35328;&#35770;&#30340;&#36131;&#20219;&#22312;&#21738;&#37324;&#65311;
&lt;/p&gt;
&lt;p&gt;
Where's the Liability in Harmful AI Speech?. (arXiv:2308.04635v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04635
&lt;/p&gt;
&lt;p&gt;
AI&#29983;&#25104;&#24335;&#27169;&#22411;&#21487;&#33021;&#20250;&#20135;&#29983;&#20855;&#26377;&#28508;&#22312;&#36131;&#20219;&#39118;&#38505;&#30340;&#26377;&#23475;&#35328;&#35770;&#12290;&#35299;&#20915;&#27169;&#22411;&#21019;&#24314;&#32773;&#21644;&#37096;&#32626;&#32773;&#30340;&#27861;&#24459;&#36131;&#20219;&#38382;&#39064;&#30340;&#20851;&#38190;&#22312;&#20110;&#31639;&#27861;&#35774;&#35745;&#30340;&#25216;&#26415;&#32454;&#33410;&#12290;&#38656;&#35201;&#36827;&#34892;&#28145;&#20837;&#30340;Section 230&#20813;&#36131;&#20998;&#26512;&#20197;&#21450;&#19979;&#28216;&#36131;&#20219;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;&#29305;&#21035;&#26159;&#22522;&#20110;&#25991;&#26412;&#30340;&#8220;&#22522;&#30784;&#27169;&#22411;&#8221;&#65289;&#21487;&#20197;&#29983;&#25104;&#21487;&#33021;&#22312;&#24191;&#27867;&#30340;&#36131;&#20219;&#21046;&#24230;&#19979;&#24341;&#21457;&#38382;&#39064;&#30340;&#35328;&#35770;&#12290;&#26426;&#22120;&#23398;&#20064;&#20174;&#19994;&#32773;&#32463;&#24120;&#23545;&#27169;&#22411;&#36827;&#34892;&#8220;&#32418;&#38431;&#8221;&#27979;&#35797;&#65292;&#20197;&#35782;&#21035;&#21644;&#20943;&#36731;&#27492;&#31867;&#38382;&#39064;&#35328;&#35770;&#65292;&#20174;&#38169;&#35823;&#25351;&#36131;&#20005;&#37325;&#19981;&#31471;&#34892;&#20026;&#30340;&#8220;&#24187;&#35273;&#8221;&#21040;&#26500;&#36896;&#21407;&#23376;&#24377;&#30340;&#39135;&#35889;&#12290;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#26159;&#36825;&#20123;&#32418;&#38431;&#27979;&#35797;&#34892;&#20026;&#26159;&#21542;&#30495;&#30340;&#23545;&#27169;&#22411;&#21019;&#24314;&#32773;&#21644;&#37096;&#32626;&#32773;&#26500;&#25104;&#20219;&#20309;&#27861;&#24459;&#36131;&#20219;&#39118;&#38505;&#65292;&#20174;&#32780;&#28608;&#21169;&#25237;&#36164;&#20110;&#23433;&#20840;&#26426;&#21046;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19977;&#31181;&#36131;&#20219;&#21046;&#24230;&#65292;&#24182;&#23558;&#20854;&#19982;&#32418;&#38431;&#27979;&#35797;&#27169;&#22411;&#34892;&#20026;&#30340;&#24120;&#35265;&#20363;&#23376;&#32852;&#31995;&#36215;&#26469;&#65306;&#35837;&#35876;&#12289;&#26500;&#25104;&#29359;&#32618;&#34892;&#20026;&#30340;&#35328;&#35770;&#21644;&#38169;&#35823;&#33268;&#27515;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20219;&#20309;Section 230&#20813;&#36131;&#20998;&#26512;&#25110;&#19979;&#28216;&#36131;&#20219;&#20998;&#26512;&#37117;&#19982;&#31639;&#27861;&#35774;&#35745;&#30340;&#25216;&#26415;&#32454;&#33410;&#23494;&#20999;&#30456;&#20851;&#12290;&#32780;&#35201;&#30495;&#27491;&#25214;&#21040;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#26041;&#27861;&#26377;&#24456;&#22810;&#38556;&#30861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative AI, in particular text-based "foundation models" (large models trained on a huge variety of information including the internet), can generate speech that could be problematic under a wide range of liability regimes. Machine learning practitioners regularly "red team" models to identify and mitigate such problematic speech: from "hallucinations" falsely accusing people of serious misconduct to recipes for constructing an atomic bomb. A key question is whether these red-teamed behaviors actually present any liability risk for model creators and deployers under U.S. law, incentivizing investments in safety mechanisms. We examine three liability regimes, tying them to common examples of red-teamed model behaviors: defamation, speech integral to criminal conduct, and wrongful death. We find that any Section 230 immunity analysis or downstream liability analysis is intimately wrapped up in the technical details of algorithm design. And there are many roadblocks to truly finding mo
&lt;/p&gt;</description></item><item><title>&#20256;&#32479;&#30340;&#31526;&#21495;AI&#26041;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;AI&#26041;&#27861;&#26080;&#27861;&#28385;&#36275;&#21019;&#24314;&#24378;&#22823;&#21644;&#21487;&#20449;&#36182;&#30340;AI&#30340;&#25361;&#25112;&#65292;&#28982;&#32780;&#65292;&#21457;&#23637;&#33073;&#38772;&#27861;&#36890;&#36807;&#27169;&#20223;&#20154;&#31867;&#20799;&#31461;&#30340;&#33021;&#21147;&#21457;&#23637;&#36807;&#31243;&#65292;&#20026;&#21019;&#24314;&#31283;&#20581;&#21487;&#38752;&#30340;AI&#25552;&#20379;&#20102;&#24076;&#26395;&#12290;</title><link>http://arxiv.org/abs/2308.04586</link><description>&lt;p&gt;
AIs&#30340;&#21457;&#23637;&#33073;&#38772;&#27861;
&lt;/p&gt;
&lt;p&gt;
Developmental Bootstrapping of AIs. (arXiv:2308.04586v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04586
&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#31526;&#21495;AI&#26041;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;AI&#26041;&#27861;&#26080;&#27861;&#28385;&#36275;&#21019;&#24314;&#24378;&#22823;&#21644;&#21487;&#20449;&#36182;&#30340;AI&#30340;&#25361;&#25112;&#65292;&#28982;&#32780;&#65292;&#21457;&#23637;&#33073;&#38772;&#27861;&#36890;&#36807;&#27169;&#20223;&#20154;&#31867;&#20799;&#31461;&#30340;&#33021;&#21147;&#21457;&#23637;&#36807;&#31243;&#65292;&#20026;&#21019;&#24314;&#31283;&#20581;&#21487;&#38752;&#30340;AI&#25552;&#20379;&#20102;&#24076;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24403;&#21069;&#19968;&#20123;AI&#22312;&#23553;&#38381;&#30340;&#19990;&#30028;&#65292;&#22914;&#26827;&#30424;&#28216;&#25103;&#20013;&#36229;&#36234;&#20102;&#20154;&#31867;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#28151;&#20081;&#30340;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#34920;&#29616;&#26377;&#38480;&#12290;&#23427;&#20204;&#20250;&#29359;&#22855;&#24618;&#30340;&#38169;&#35823;&#32780;&#19988;&#27809;&#26377;&#24847;&#35782;&#21040;&#12290;&#23427;&#20204;&#24456;&#38590;&#21463;&#21040;&#25351;&#23548;&#65292;&#19981;&#33021;&#36816;&#29992;&#24120;&#35782;&#65292;&#32570;&#20047;&#22909;&#22855;&#24515;&#12290;&#23427;&#20204;&#19981;&#33021;&#25104;&#20026;&#33391;&#22909;&#30340;&#21512;&#20316;&#32773;&#12290;&#20256;&#32479;&#25163;&#21160;&#26500;&#24314;&#30340;&#31526;&#21495;AI&#26041;&#27861;&#26500;&#24314;&#30340;&#31995;&#32479;&#21644;&#20351;&#29992;&#29983;&#25104;&#21644;&#28145;&#24230;&#23398;&#20064;AI&#26041;&#27861;(&#21253;&#25324;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;)&#26500;&#24314;&#30340;&#31995;&#32479;&#37117;&#26080;&#27861;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;&#23427;&#20204;&#19981;&#36866;&#21512;&#21019;&#24314;&#24378;&#22823;&#21644;&#21487;&#20449;&#36182;&#30340;AI&#12290;&#23613;&#31649;&#27492;&#26041;&#27861;&#19981;&#23646;&#20110;&#20027;&#27969;&#30340;AI&#26041;&#27861;&#65292;&#20294;&#21457;&#23637;&#33073;&#38772;&#27861;&#26174;&#31034;&#20986;&#24076;&#26395;&#12290;&#22312;&#21457;&#23637;&#33073;&#38772;&#27861;&#20013;&#65292;AI&#20687;&#20154;&#31867;&#20799;&#31461;&#19968;&#26679;&#21457;&#23637;&#33021;&#21147;&#12290;&#23427;&#20204;&#20174;&#20808;&#22825;&#33021;&#21147;&#24320;&#22987;&#12290;&#20687;&#20154;&#31867;&#19968;&#26679;&#65292;&#23427;&#20204;&#19982;&#29615;&#22659;&#20114;&#21160;&#65292;&#24182;&#20174;&#20114;&#21160;&#20013;&#23398;&#20064;&#12290;&#23427;&#20204;&#36890;&#36807;&#33258;&#25105;&#21457;&#23637;&#30340;&#33021;&#21147;&#36880;&#27493;&#25193;&#23637;&#20808;&#22825;&#33021;&#21147;&#12290;&#23427;&#20204;&#20114;&#21160;&#24182;&#36880;&#28176;&#23558;&#25152;&#23398;&#24212;&#29992;&#20110;&#23454;&#38469;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although some current AIs surpass human abilities especially in closed worlds such as board games, their performance in the messy real world is limited. They make strange mistakes and do not notice them. They cannot be instructed easily, fail to use common sense, and lack curiosity. They do not make good collaborators. Neither systems built using the traditional manually-constructed symbolic AI approach nor systems built using generative and deep learning AI approaches including large language models (LLMs) can meet the challenges. They are not well suited for creating robust and trustworthy AIs. Although it is outside of mainstream AI approaches, developmental bootstrapping shows promise. In developmental bootstrapping, AIs develop competences like human children do. They start with innate competences. Like humans, they interact with the environment and learn from their interactions. They incrementally extend their innate competences with self-developed competences. They interact and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20174;&#29992;&#25143;&#34892;&#20026;&#20449;&#24687;&#20013;&#25552;&#21462;&#21644;&#34701;&#21512;&#24322;&#26500;&#30693;&#35782;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25351;&#20196;&#35843;&#25972;&#23454;&#29616;&#20010;&#24615;&#21270;&#25512;&#33616;&#65292;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#25512;&#33616;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.03333</link><description>&lt;p&gt;
&#24322;&#26500;&#30693;&#35782;&#34701;&#21512;: &#36890;&#36807;LLM&#36827;&#34892;&#20010;&#24615;&#21270;&#25512;&#33616;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous Knowledge Fusion: A Novel Approach for Personalized Recommendation via LLM. (arXiv:2308.03333v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03333
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20174;&#29992;&#25143;&#34892;&#20026;&#20449;&#24687;&#20013;&#25552;&#21462;&#21644;&#34701;&#21512;&#24322;&#26500;&#30693;&#35782;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25351;&#20196;&#35843;&#25972;&#23454;&#29616;&#20010;&#24615;&#21270;&#25512;&#33616;&#65292;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#25512;&#33616;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#26512;&#21644;&#25366;&#25496;&#29992;&#25143;&#24322;&#26500;&#34892;&#20026;&#23545;&#20110;&#25512;&#33616;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#23558;&#21508;&#31181;&#31867;&#22411;&#30340;&#24322;&#26500;&#34892;&#20026;&#32435;&#20837;&#25512;&#33616;&#27169;&#22411;&#30340;&#24120;&#35268;&#26041;&#27861;&#20250;&#23548;&#33268;&#29305;&#24449;&#31232;&#30095;&#21644;&#30693;&#35782;&#30862;&#29255;&#21270;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20174;&#29992;&#25143;&#24322;&#26500;&#34892;&#20026;&#20449;&#24687;&#20013;&#25552;&#21462;&#21644;&#34701;&#21512;&#24322;&#26500;&#30693;&#35782;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#24322;&#26500;&#30693;&#35782;&#21644;&#25512;&#33616;&#20219;&#21153;&#32467;&#21512;&#65292;&#23545;LLM&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#20197;&#23454;&#29616;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#25972;&#21512;&#29992;&#25143;&#24322;&#26500;&#34892;&#20026;&#24182;&#26174;&#33879;&#25552;&#39640;&#25512;&#33616;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The analysis and mining of user heterogeneous behavior are of paramount importance in recommendation systems. However, the conventional approach of incorporating various types of heterogeneous behavior into recommendation models leads to feature sparsity and knowledge fragmentation issues. To address this challenge, we propose a novel approach for personalized recommendation via Large Language Model (LLM), by extracting and fusing heterogeneous knowledge from user heterogeneous behavior information. In addition, by combining heterogeneous knowledge and recommendation tasks, instruction tuning is performed on LLM for personalized recommendations. The experimental results demonstrate that our method can effectively integrate user heterogeneous behavior and significantly improve recommendation performance.
&lt;/p&gt;</description></item><item><title>DOMINO&#26159;&#19968;&#31181;&#39046;&#22495;&#19981;&#21464;&#30340;&#39640;&#32500;&#20998;&#31867;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#22810;&#20256;&#24863;&#22120;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65307;&#35299;&#20915;&#20102;&#20998;&#24067;&#20559;&#31227;&#21644;&#35745;&#31639;&#36164;&#28304;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.03295</link><description>&lt;p&gt;
DOMINO: &#22810;&#20010;&#20256;&#24863;&#22120;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#39046;&#22495;&#19981;&#21464;&#30340;&#39640;&#32500;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
DOMINO: Domain-invariant Hyperdimensional Classification for Multi-Sensor Time Series Data. (arXiv:2308.03295v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03295
&lt;/p&gt;
&lt;p&gt;
DOMINO&#26159;&#19968;&#31181;&#39046;&#22495;&#19981;&#21464;&#30340;&#39640;&#32500;&#20998;&#31867;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#22810;&#20256;&#24863;&#22120;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65307;&#35299;&#20915;&#20102;&#20998;&#24067;&#20559;&#31227;&#21644;&#35745;&#31639;&#36164;&#28304;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29289;&#32852;&#32593;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#35768;&#22810;&#29616;&#23454;&#24212;&#29992;&#21033;&#29992;&#24322;&#26500;&#36830;&#25509;&#30340;&#20256;&#24863;&#22120;&#26469;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#20449;&#24687;&#12290;&#36793;&#32536;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#24120;&#34987;&#29992;&#20110;&#20998;&#26512;&#26412;&#22320;&#25910;&#38598;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#39537;&#21160;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#38754;&#20020;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#21363;&#20998;&#24067;&#20559;&#31227;&#12290;&#24403;&#19968;&#20010;&#27169;&#22411;&#37096;&#32626;&#22312;&#19982;&#20854;&#35757;&#32451;&#19981;&#21516;&#30340;&#25968;&#25454;&#20998;&#24067;&#19978;&#26102;&#65292;&#20998;&#24067;&#20559;&#31227;&#20250;&#20005;&#37325;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#36234;&#26469;&#36234;&#22797;&#26434;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#24050;&#34987;&#25552;&#20986;&#29992;&#20110;&#25429;&#25417;&#22810;&#20010;&#20256;&#24863;&#22120;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;&#32780;&#36825;&#38656;&#35201;&#36229;&#20986;&#24403;&#20170;&#36793;&#32536;&#35774;&#22791;&#23481;&#37327;&#30340;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#12290;&#32780;&#22823;&#33041;&#21551;&#21457;&#30340;&#39640;&#32500;&#35745;&#31639;(HDC)&#20316;&#20026;&#36793;&#32536;&#23398;&#20064;&#30340;&#19968;&#31181;&#36731;&#37327;&#32423;&#35299;&#20915;&#26041;&#26696;&#24050;&#34987;&#24341;&#20837;&#65292;&#20294;&#29616;&#26377;&#30340;HDC&#22312;&#38754;&#23545;&#20998;&#24067;&#20559;&#31227;&#25361;&#25112;&#26102;&#20173;&#28982;&#33030;&#24369;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;DOMINO&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;HDC&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rapid evolution of the Internet of Things, many real-world applications utilize heterogeneously connected sensors to capture time-series information. Edge-based machine learning (ML) methodologies are often employed to analyze locally collected data. However, a fundamental issue across data-driven ML approaches is distribution shift. It occurs when a model is deployed on a data distribution different from what it was trained on, and can substantially degrade model performance. Additionally, increasingly sophisticated deep neural networks (DNNs) have been proposed to capture spatial and temporal dependencies in multi-sensor time series data, requiring intensive computational resources beyond the capacity of today's edge devices. While brain-inspired hyperdimensional computing (HDC) has been introduced as a lightweight solution for edge-based learning, existing HDCs are also vulnerable to the distribution shift challenge. In this paper, we propose DOMINO, a novel HDC learning fr
&lt;/p&gt;</description></item><item><title>ChatGPT&#20316;&#20026;&#26816;&#27979;&#22120;&#33021;&#21542;&#26377;&#25928;&#26816;&#27979;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#65292;&#25105;&#20204;&#36890;&#36807;&#35780;&#20272;&#20854;&#22312;&#20154;&#24037;&#32534;&#20889;&#25991;&#26412;&#19982;AI&#29983;&#25104;&#25991;&#26412;&#20043;&#38388;&#30340;&#21306;&#20998;&#33021;&#21147;&#65292;&#24182;&#22312;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#24471;&#20986;&#20102;&#20851;&#20110;ChatGPT&#22312;&#33258;&#21160;&#21270;&#26816;&#27979;&#27969;&#31243;&#20013;&#30340;&#24212;&#29992;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2308.01284</link><description>&lt;p&gt;
&#29992;&#28779;&#25915;&#28779;&#65306;ChatGPT&#33021;&#22815;&#26816;&#27979;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Fighting Fire with Fire: Can ChatGPT Detect AI-generated Text?. (arXiv:2308.01284v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01284
&lt;/p&gt;
&lt;p&gt;
ChatGPT&#20316;&#20026;&#26816;&#27979;&#22120;&#33021;&#21542;&#26377;&#25928;&#26816;&#27979;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#65292;&#25105;&#20204;&#36890;&#36807;&#35780;&#20272;&#20854;&#22312;&#20154;&#24037;&#32534;&#20889;&#25991;&#26412;&#19982;AI&#29983;&#25104;&#25991;&#26412;&#20043;&#38388;&#30340;&#21306;&#20998;&#33021;&#21147;&#65292;&#24182;&#22312;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#24471;&#20986;&#20102;&#20851;&#20110;ChatGPT&#22312;&#33258;&#21160;&#21270;&#26816;&#27979;&#27969;&#31243;&#20013;&#30340;&#24212;&#29992;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;ChatGPT&#65289;&#34987;&#29992;&#20110;&#21508;&#31181;&#29992;&#20363;&#65292;&#21253;&#25324;&#35268;&#27169;&#21270;&#30340;&#25991;&#26412;&#20869;&#23481;&#29983;&#25104;&#12290;&#34429;&#28982;&#24050;&#32463;&#23384;&#22312;&#38024;&#23545;&#36825;&#31181;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#26816;&#27979;&#26041;&#27861;&#65292;&#20294;&#25105;&#20204;&#30740;&#31350;&#20102;ChatGPT&#22312;&#36825;&#31181;AI&#29983;&#25104;&#25991;&#26412;&#19978;&#30340;&#26816;&#27979;&#24615;&#33021;&#65292;&#21463;&#21040;&#23558;ChatGPT&#29992;&#20316;&#25968;&#25454;&#26631;&#27880;&#22120;&#25110;&#27880;&#37322;&#22120;&#30340;&#30740;&#31350;&#21551;&#21457;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;ChatGPT&#22312;&#20154;&#24037;&#32534;&#20889;&#25991;&#26412;&#19982;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#38646;-shot&#24615;&#33021;&#65292;&#24182;&#22312;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#20102;ChatGPT&#22312;&#26816;&#27979;AI&#29983;&#25104;&#25991;&#26412;&#25110;&#20154;&#24037;&#32534;&#20889;&#25991;&#26412;&#26041;&#38754;&#26159;&#21542;&#20855;&#26377;&#23545;&#31216;&#25928;&#24212;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25581;&#31034;&#20102;&#36890;&#36807;&#31616;&#21333;&#20851;&#27880;&#38382;&#39064;&#30340;&#29305;&#23450;&#26041;&#38754;&#24182;&#20174;&#35813;&#35299;&#20915;&#26041;&#26696;&#20013;&#25512;&#23548;&#20986;&#20854;&#20313;&#37096;&#20998;&#65292;&#22914;&#20309;&#21033;&#29992;ChatGPT&#21644;&#31867;&#20284;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#21160;&#21270;&#26816;&#27979;&#27969;&#31243;&#20013;&#21457;&#25381;&#20316;&#29992;&#12290;&#25152;&#26377;&#20195;&#30721;&#21644;&#25968;&#25454;&#21487;&#22312; \url{https://github.com/AmritaBh/ChatGPT-as-Detector} &#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) such as ChatGPT are increasingly being used for various use cases, including text content generation at scale. Although detection methods for such AI-generated text exist already, we investigate ChatGPT's performance as a detector on such AI-generated text, inspired by works that use ChatGPT as a data labeler or annotator. We evaluate the zero-shot performance of ChatGPT in the task of human-written vs. AI-generated text detection, and perform experiments on publicly available datasets. We empirically investigate if ChatGPT is symmetrically effective in detecting AI-generated or human-written text. Our findings provide insight on how ChatGPT and similar LLMs may be leveraged in automated detection pipelines by simply focusing on solving a specific aspect of the problem and deriving the rest from that solution. All code and data is available at \url{https://github.com/AmritaBh/ChatGPT-as-Detector}.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22522;&#20110;&#21453;&#20107;&#23454;&#27169;&#25311;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#27604;&#36739;&#19981;&#21516;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#22312;&#19981;&#21516;&#25805;&#20316;&#35774;&#35745;&#39046;&#22495;&#20013;&#34892;&#20026;&#39118;&#38505;&#12290;&#36890;&#36807;&#24341;&#20837;&#21453;&#20107;&#23454;&#23433;&#20840;&#36793;&#30028;&#30340;&#27010;&#24565;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#25214;&#21040;&#26368;&#20851;&#38190;&#30340;&#24773;&#26223;&#65292;&#24182;&#35780;&#20272;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#39118;&#38505;&#39057;&#29575;&#21644;&#20005;&#37325;&#31243;&#24230;&#12290;&#35813;&#26041;&#27861;&#21363;&#20351;&#22312;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#34892;&#20026;&#31574;&#30053;&#26410;&#30693;&#30340;&#24773;&#20917;&#19979;&#20063;&#36866;&#29992;&#65292;&#23545;&#22806;&#37096;&#31532;&#19977;&#26041;&#39118;&#38505;&#35780;&#20272;&#26426;&#26500;&#26377;&#29992;&#12290;</title><link>http://arxiv.org/abs/2308.01050</link><description>&lt;p&gt;
&#23545;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#39118;&#38505;&#35780;&#20272;&#30340;&#21453;&#20107;&#23454;&#23433;&#20840;&#36793;&#30028;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
A Counterfactual Safety Margin Perspective on the Scoring of Autonomous Vehicles' Riskiness. (arXiv:2308.01050v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01050
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;&#21453;&#20107;&#23454;&#27169;&#25311;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#27604;&#36739;&#19981;&#21516;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#22312;&#19981;&#21516;&#25805;&#20316;&#35774;&#35745;&#39046;&#22495;&#20013;&#34892;&#20026;&#39118;&#38505;&#12290;&#36890;&#36807;&#24341;&#20837;&#21453;&#20107;&#23454;&#23433;&#20840;&#36793;&#30028;&#30340;&#27010;&#24565;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#25214;&#21040;&#26368;&#20851;&#38190;&#30340;&#24773;&#26223;&#65292;&#24182;&#35780;&#20272;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#39118;&#38505;&#39057;&#29575;&#21644;&#20005;&#37325;&#31243;&#24230;&#12290;&#35813;&#26041;&#27861;&#21363;&#20351;&#22312;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#34892;&#20026;&#31574;&#30053;&#26410;&#30693;&#30340;&#24773;&#20917;&#19979;&#20063;&#36866;&#29992;&#65292;&#23545;&#22806;&#37096;&#31532;&#19977;&#26041;&#39118;&#38505;&#35780;&#20272;&#26426;&#26500;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;AVs&#65289;&#26377;&#28508;&#21147;&#25552;&#20379;&#35832;&#22810;&#31038;&#20250;&#25928;&#30410;&#65292;&#22914;&#20943;&#23569;&#36947;&#36335;&#20107;&#25925;&#21644;&#25552;&#39640;&#20132;&#36890;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#21382;&#21490;&#25968;&#25454;&#21644;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#37327;&#21270;AVs&#30340;&#39118;&#38505;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#27604;&#36739;&#19981;&#21516;AVs&#22312;&#21508;&#31181;&#25805;&#20316;&#35774;&#35745;&#39046;&#22495;&#65288;ODDs&#65289;&#20013;&#34892;&#20026;&#30340;&#39118;&#38505;&#65292;&#35813;&#26694;&#26550;&#22522;&#20110;&#23545;&#8220;&#19981;&#33391;&#8221;&#36947;&#36335;&#29992;&#25143;&#36827;&#34892;&#21453;&#20107;&#23454;&#27169;&#25311;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#21453;&#20107;&#23454;&#23433;&#20840;&#36793;&#30028;&#30340;&#27010;&#24565;&#65292;&#34920;&#31034;&#21487;&#33021;&#23548;&#33268;&#30896;&#25758;&#30340;&#26368;&#23567;&#20559;&#31163;&#27491;&#24120;&#34892;&#20026;&#30340;&#37327;&#12290;&#35813;&#27010;&#24565;&#26377;&#21161;&#20110;&#25214;&#21040;&#26368;&#20851;&#38190;&#30340;&#24773;&#26223;&#65292;&#21516;&#26102;&#20063;&#26377;&#21161;&#20110;&#35780;&#20272;AVs&#30340;&#39118;&#38505;&#39057;&#29575;&#21644;&#20005;&#37325;&#31243;&#24230;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#21363;&#20351;AV&#30340;&#34892;&#20026;&#31574;&#30053;&#26159;&#26410;&#30693;&#30340;&#65292;&#25552;&#20986;&#30340;&#26041;&#27861;&#20173;&#28982;&#36866;&#29992;&#20110;&#26368;&#22351;&#21644;&#26368;&#20339;&#24773;&#20917;&#20998;&#26512;&#65292;&#20351;&#35813;&#26041;&#27861;&#23545;&#22806;&#37096;&#31532;&#19977;&#26041;&#39118;&#38505;&#35780;&#20272;&#26426;&#26500;&#20063;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous Vehicles (AVs) have the potential to provide numerous societal benefits, such as decreased road accidents and increased overall transportation efficiency. However, quantifying the risk associated with AVs is challenging due to the lack of historical data and the rapidly evolving technology. This paper presents a data-driven framework for comparing the risk of different AVs' behaviors in various operational design domains (ODDs), based on counterfactual simulations of "misbehaving" road users. We introduce the concept of counterfactual safety margin, which represents the minimum deviation from normal behavior that could lead to a collision. This concept helps to find the most critical scenarios but also to assess the frequency and severity of risk of AVs. We show that the proposed methodology is applicable even when the AV's behavioral policy is unknown -- through worst- and best-case analyses -- making the method useful also to external third-party risk assessors. Our experi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24863;&#30693;&#28023;&#26862;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#20915;&#31574;&#31995;&#32479;&#20248;&#21270;&#20013;&#26799;&#24230;&#21453;&#39304;&#31232;&#32570;&#25110;&#26080;&#25928;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#32039;&#20945;&#30340;&#22810;&#23618;&#26550;&#26500;&#21644;&#35282;&#33394;&#27010;&#24565;&#65292;&#24182;&#21033;&#29992;&#24863;&#30693;&#28023;&#26862;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#23545;&#21442;&#25968;&#36827;&#34892;&#20248;&#21270;&#65292;&#20316;&#32773;&#23454;&#29616;&#20102;&#23545;&#22797;&#26434;&#20915;&#31574;&#31995;&#32479;&#30340;&#39640;&#25928;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2308.00629</link><description>&lt;p&gt;
Hessian-Aware Bayesian Optimization for Decision Making Systems - &#24863;&#30693;&#28023;&#26862;&#36125;&#21494;&#26031;&#20248;&#21270;&#22312;&#20915;&#31574;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Hessian-Aware Bayesian Optimization for Decision Making Systems. (arXiv:2308.00629v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00629
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24863;&#30693;&#28023;&#26862;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#20915;&#31574;&#31995;&#32479;&#20248;&#21270;&#20013;&#26799;&#24230;&#21453;&#39304;&#31232;&#32570;&#25110;&#26080;&#25928;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#32039;&#20945;&#30340;&#22810;&#23618;&#26550;&#26500;&#21644;&#35282;&#33394;&#27010;&#24565;&#65292;&#24182;&#21033;&#29992;&#24863;&#30693;&#28023;&#26862;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#23545;&#21442;&#25968;&#36827;&#34892;&#20248;&#21270;&#65292;&#20316;&#32773;&#23454;&#29616;&#20102;&#23545;&#22797;&#26434;&#20915;&#31574;&#31995;&#32479;&#30340;&#39640;&#25928;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#20248;&#21270;&#20915;&#31574;&#31995;&#32479;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#26799;&#24230;&#26041;&#27861;&#65292;&#38656;&#35201;&#20174;&#29615;&#22659;&#20013;&#33719;&#21462;&#26377;&#20449;&#24687;&#37327;&#30340;&#21453;&#39304;&#12290;&#28982;&#32780;&#65292;&#24403;&#21453;&#39304;&#31232;&#32570;&#25110;&#32773;&#26080;&#20449;&#24687;&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#24615;&#33021;&#36739;&#24046;&#12290;&#36125;&#21494;&#26031;&#20248;&#21270;&#31561;&#26080;&#23548;&#25968;&#26041;&#27861;&#21487;&#20197;&#20943;&#23569;&#23545;&#26799;&#24230;&#21453;&#39304;&#36136;&#37327;&#30340;&#20381;&#36182;&#65292;&#20294;&#22312;&#22797;&#26434;&#20915;&#31574;&#31995;&#32479;&#30340;&#39640;&#32500;&#29615;&#22659;&#20013;&#24448;&#24448;&#38590;&#20197;&#25193;&#23637;&#12290;&#22914;&#26524;&#31995;&#32479;&#38656;&#35201;&#22810;&#20010;&#21442;&#19982;&#32773;&#20043;&#38388;&#30340;&#20114;&#21160;&#26469;&#23454;&#29616;&#20849;&#21516;&#30446;&#26631;&#65292;&#36825;&#20010;&#38382;&#39064;&#23601;&#21152;&#21095;&#20102;&#12290;&#20026;&#20102;&#35299;&#20915;&#32500;&#24230;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32039;&#20945;&#30340;&#22810;&#23618;&#26550;&#26500;&#65292;&#36890;&#36807;&#35282;&#33394;&#30340;&#27010;&#24565;&#26469;&#24314;&#27169;&#21442;&#19982;&#32773;&#20043;&#38388;&#30340;&#21160;&#24577;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#24863;&#30693;&#28023;&#26862;&#36125;&#21494;&#26031;&#20248;&#21270;&#26469;&#39640;&#25928;&#22320;&#20248;&#21270;&#30001;&#22823;&#37327;&#21442;&#25968;&#21442;&#25968;&#21270;&#30340;&#22810;&#23618;&#26550;&#26500;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;(HA-GP-UCB)&#22312;&#25928;&#26524;&#19978;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many approaches for optimizing decision making systems rely on gradient based methods requiring informative feedback from the environment. However, in the case where such feedback is sparse or uninformative, such approaches may result in poor performance. Derivative-free approaches such as Bayesian Optimization mitigate the dependency on the quality of gradient feedback, but are known to scale poorly in the high-dimension setting of complex decision making systems. This problem is exacerbated if the system requires interactions between several actors cooperating to accomplish a shared goal. To address the dimensionality challenge, we propose a compact multi-layered architecture modeling the dynamics of actor interactions through the concept of role. Additionally, we introduce Hessian-aware Bayesian Optimization to efficiently optimize the multi-layered architecture parameterized by a large number of parameters. Experimental results demonstrate that our method (HA-GP-UCB) works effectiv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;DMFC-GraspNet&#65292;&#22312;&#22810;&#25351;&#26426;&#22120;&#20154;&#25235;&#21462;&#29983;&#25104;&#39046;&#22495;&#20570;&#20986;&#20102;&#20004;&#20010;&#20027;&#35201;&#36129;&#29486;&#65306;&#19968;&#26159;&#25552;&#20986;&#20102;&#21487;&#24494;&#30340;&#22810;&#25351;&#25235;&#21462;&#35268;&#21010;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22810;&#26679;&#21270;&#21644;&#31264;&#23494;&#30340;&#25235;&#21462;&#39044;&#27979;&#65307;&#20108;&#26159;&#24320;&#21457;&#20102;&#19968;&#31181;&#31264;&#23494;&#26631;&#27880;&#26041;&#27861;&#65292;&#20351;&#24471;&#22810;&#25351;&#26426;&#22120;&#20154;&#25163;&#19982;&#30495;&#23454;&#25235;&#21462;&#23494;&#20999;&#20851;&#32852;&#12290;&#32467;&#26524;&#34920;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.00456</link><description>&lt;p&gt;
DMFC-GraspNet: &#22810;&#25351;&#26426;&#22120;&#20154;&#22312;&#26434;&#20081;&#22330;&#26223;&#20013;&#21487;&#24494;&#30340;&#25235;&#21462;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
DMFC-GraspNet: Differentiable Multi-Fingered Robotic Grasp Generation in Cluttered Scenes. (arXiv:2308.00456v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00456
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;DMFC-GraspNet&#65292;&#22312;&#22810;&#25351;&#26426;&#22120;&#20154;&#25235;&#21462;&#29983;&#25104;&#39046;&#22495;&#20570;&#20986;&#20102;&#20004;&#20010;&#20027;&#35201;&#36129;&#29486;&#65306;&#19968;&#26159;&#25552;&#20986;&#20102;&#21487;&#24494;&#30340;&#22810;&#25351;&#25235;&#21462;&#35268;&#21010;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22810;&#26679;&#21270;&#21644;&#31264;&#23494;&#30340;&#25235;&#21462;&#39044;&#27979;&#65307;&#20108;&#26159;&#24320;&#21457;&#20102;&#19968;&#31181;&#31264;&#23494;&#26631;&#27880;&#26041;&#27861;&#65292;&#20351;&#24471;&#22810;&#25351;&#26426;&#22120;&#20154;&#25163;&#19982;&#30495;&#23454;&#25235;&#21462;&#23494;&#20999;&#20851;&#32852;&#12290;&#32467;&#26524;&#34920;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#25235;&#21462;&#26159;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;&#24517;&#22791;&#30340;&#22522;&#26412;&#25216;&#33021;&#12290;&#27169;&#20223;&#20154;&#25163;&#32467;&#26500;&#30340;&#22810;&#25351;&#26426;&#22120;&#20154;&#25163;&#21487;&#20197;&#36827;&#34892;&#22797;&#26434;&#30340;&#29289;&#20307;&#25805;&#20316;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#22810;&#25351;&#26426;&#22120;&#20154;&#25235;&#21462;&#25216;&#26415;&#36890;&#24120;&#22312;&#27599;&#27425;&#25512;&#29702;&#20013;&#21482;&#33021;&#39044;&#27979;&#19968;&#27425;&#25235;&#21462;&#65292;&#38480;&#21046;&#20102;&#20854;&#22810;&#26679;&#24615;&#21644;&#25928;&#29575;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#24494;&#30340;&#22810;&#25351;&#25235;&#21462;&#29983;&#25104;&#32593;&#32476;&#65288;DMFC-GraspNet&#65289;&#65292;&#38024;&#23545;&#36825;&#19968;&#25361;&#25112;&#20570;&#20986;&#20102;&#20004;&#20010;&#20027;&#35201;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#25235;&#21462;&#35268;&#21010;&#22120;&#65292;&#39044;&#27979;&#20102;&#19968;&#31181;&#26032;&#30340;&#25235;&#21462;&#34920;&#31034;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22810;&#26679;&#21270;&#32780;&#31264;&#23494;&#30340;&#25235;&#21462;&#39044;&#27979;&#12290;&#20854;&#27425;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#22330;&#26223;&#21019;&#24314;&#21644;&#26631;&#31614;&#26144;&#23556;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#25351;&#26426;&#22120;&#20154;&#25163;&#30340;&#31264;&#23494;&#26631;&#27880;&#65292;&#23454;&#29616;&#20102;&#19982;&#30495;&#23454;&#25235;&#21462;&#30340;&#23494;&#20999;&#20851;&#32852;&#12290;&#36890;&#36807;&#20223;&#30495;&#30740;&#31350;&#23545;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#19982;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robotic grasping is a fundamental skill required for object manipulation in robotics. Multi-fingered robotic hands, which mimic the structure of the human hand, can potentially perform complex object manipulations. Nevertheless, current techniques for multi-fingered robotic grasping frequently predict only a single grasp for each inference time, limiting their versatility and efficiency. This paper proposes a differentiable multi-fingered grasp generation network (DMFC-GraspNet) with two main contributions to address this challenge. Firstly, a novel neural grasp planner is proposed, which predicts a new grasp representation to enable versatile and dense grasp predictions. Secondly, a scene creation and label mapping method is developed for dense labeling of multi-fingered robotic hands, which allows a dense association of ground truth grasps. The proposed approach is evaluated through simulation studies and compared to existing approaches. The results demonstrate the effectiveness of t
&lt;/p&gt;</description></item><item><title>MetaGPT&#26159;&#19968;&#20010;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#30340;&#21019;&#26032;&#26694;&#26550;&#65292;&#23558;&#26377;&#25928;&#30340;&#20154;&#24037;&#24037;&#20316;&#27969;&#24341;&#20837;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#21327;&#20316;&#20013;&#12290;&#23427;&#37319;&#29992;&#20803;&#32534;&#31243;&#26041;&#27861;&#65292;&#23558;&#26631;&#20934;&#25805;&#20316;&#35268;&#31243;&#32534;&#30721;&#20026;&#25552;&#31034;&#65292;&#20419;&#36827;&#32467;&#26500;&#21270;&#21327;&#35843;&#65292;&#24182;&#35201;&#27714;&#27169;&#22359;&#21270;&#36755;&#20986;&#65292;&#36171;&#20104;&#26234;&#33021;&#20307;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#65292;&#20197;&#39564;&#35777;&#36755;&#20986;&#24182;&#20943;&#23569;&#38169;&#35823;&#12290;&#36825;&#31181;&#26694;&#26550;&#21033;&#29992;&#20102;&#27969;&#27700;&#32447;&#24037;&#20316;&#27169;&#24335;&#26469;&#20998;&#37197;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2308.00352</link><description>&lt;p&gt;
MetaGPT: &#20803;&#32534;&#31243;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
MetaGPT: Meta Programming for Multi-Agent Collaborative Framework. (arXiv:2308.00352v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00352
&lt;/p&gt;
&lt;p&gt;
MetaGPT&#26159;&#19968;&#20010;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#30340;&#21019;&#26032;&#26694;&#26550;&#65292;&#23558;&#26377;&#25928;&#30340;&#20154;&#24037;&#24037;&#20316;&#27969;&#24341;&#20837;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#21327;&#20316;&#20013;&#12290;&#23427;&#37319;&#29992;&#20803;&#32534;&#31243;&#26041;&#27861;&#65292;&#23558;&#26631;&#20934;&#25805;&#20316;&#35268;&#31243;&#32534;&#30721;&#20026;&#25552;&#31034;&#65292;&#20419;&#36827;&#32467;&#26500;&#21270;&#21327;&#35843;&#65292;&#24182;&#35201;&#27714;&#27169;&#22359;&#21270;&#36755;&#20986;&#65292;&#36171;&#20104;&#26234;&#33021;&#20307;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#65292;&#20197;&#39564;&#35777;&#36755;&#20986;&#24182;&#20943;&#23569;&#38169;&#35823;&#12290;&#36825;&#31181;&#26694;&#26550;&#21033;&#29992;&#20102;&#27969;&#27700;&#32447;&#24037;&#20316;&#27169;&#24335;&#26469;&#20998;&#37197;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22312;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#26234;&#33021;&#20307;&#21327;&#20316;&#20013;&#65292;&#33258;&#21160;&#20219;&#21153;&#35299;&#20915;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#31616;&#21333;&#20219;&#21153;&#19978;&#65292;&#32570;&#20047;&#23545;&#22797;&#26434;&#20219;&#21153;&#30340;&#25506;&#32034;&#21644;&#30740;&#31350;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#24187;&#35273;&#38382;&#39064;&#12290;&#36825;&#31181;&#24187;&#35273;&#22312;&#22810;&#20010;&#26234;&#33021;&#20307;&#30456;&#20114;&#20316;&#29992;&#26102;&#34987;&#26080;&#38480;&#25918;&#22823;&#65292;&#23548;&#33268;&#22312;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#26102;&#22833;&#36133;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MetaGPT&#65292;&#36825;&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#22312;LLM&#39537;&#21160;&#30340;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#20013;&#37319;&#29992;&#26377;&#25928;&#30340;&#20154;&#24037;&#24037;&#20316;&#27969;&#20316;&#20026;&#20803;&#32534;&#31243;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;MetaGPT&#39318;&#20808;&#23558;&#26631;&#20934;&#25805;&#20316;&#35268;&#31243;&#65288;SOPs&#65289;&#32534;&#30721;&#20026;&#25552;&#31034;&#65292;&#20419;&#36827;&#32467;&#26500;&#21270;&#21327;&#35843;&#12290;&#28982;&#21518;&#65292;&#23427;&#36827;&#19968;&#27493;&#35201;&#27714;&#27169;&#22359;&#21270;&#36755;&#20986;&#65292;&#36171;&#20104;&#26234;&#33021;&#20307;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#65292;&#19982;&#20154;&#31867;&#19987;&#19994;&#20154;&#21592;&#24179;&#34892;&#39564;&#35777;&#36755;&#20986;&#24182;&#20943;&#23569;&#38169;&#35823;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;MetaGPT&#21033;&#29992;&#27969;&#27700;&#32447;&#24037;&#20316;&#27169;&#24335;&#26469;&#20998;&#37197;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Recently, remarkable progress has been made in automated task-solving through the use of multi-agents driven by large language models (LLMs). However, existing works primarily focuses on simple tasks lacking exploration and investigation in complicated tasks mainly due to the hallucination problem. This kind of hallucination gets amplified infinitely as multiple intelligent agents interact with each other, resulting in failures when tackling complicated problems.Therefore, we introduce MetaGPT, an innovative framework that infuses effective human workflows as a meta programming approach into LLM-driven multi-agent collaboration. In particular, MetaGPT first encodes Standardized Operating Procedures (SOPs) into prompts, fostering structured coordination. And then, it further mandates modular outputs, bestowing agents with domain expertise paralleling human professionals to validate outputs and reduce compounded errors. In this way, MetaGPT leverages the assembly line work model to assig
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;GPT3.5&#65289;&#20316;&#20026;AI&#21161;&#25163;&#26469;&#22686;&#24378;&#28183;&#36879;&#27979;&#35797;&#20154;&#21592;&#30340;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#39640;&#32423;&#20219;&#21153;&#35268;&#21010;&#21644;&#20302;&#32423;&#28431;&#27934;&#23547;&#25214;&#20004;&#31181;&#29992;&#20363;&#65292;&#21462;&#24471;&#20102;&#26377;&#21069;&#26223;&#30340;&#21021;&#27493;&#32467;&#26524;&#65292;&#24182;&#23601;&#25552;&#20379;&#35813;&#25216;&#26415;&#30340;&#20262;&#29702;&#38382;&#39064;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;</title><link>http://arxiv.org/abs/2308.00121</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#28183;&#36879;&#27979;&#35797;&#65306;AI&#20316;&#20026;&#36741;&#21161;
&lt;/p&gt;
&lt;p&gt;
Getting pwn'd by AI: Penetration Testing with Large Language Models. (arXiv:2308.00121v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00121
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;GPT3.5&#65289;&#20316;&#20026;AI&#21161;&#25163;&#26469;&#22686;&#24378;&#28183;&#36879;&#27979;&#35797;&#20154;&#21592;&#30340;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#39640;&#32423;&#20219;&#21153;&#35268;&#21010;&#21644;&#20302;&#32423;&#28431;&#27934;&#23547;&#25214;&#20004;&#31181;&#29992;&#20363;&#65292;&#21462;&#24471;&#20102;&#26377;&#21069;&#26223;&#30340;&#21021;&#27493;&#32467;&#26524;&#65292;&#24182;&#23601;&#25552;&#20379;&#35813;&#25216;&#26415;&#30340;&#20262;&#29702;&#38382;&#39064;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36719;&#20214;&#23433;&#20840;&#27979;&#35797;&#39046;&#22495;&#65292;&#23588;&#20854;&#26159;&#28183;&#36879;&#27979;&#35797;&#26159;&#19968;&#39033;&#38656;&#35201;&#39640;&#27700;&#24179;&#19987;&#19994;&#30693;&#35782;&#30340;&#27963;&#21160;&#65292;&#24182;&#28041;&#21450;&#35768;&#22810;&#25163;&#21160;&#27979;&#35797;&#21644;&#20998;&#26512;&#27493;&#39588;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;GPT3.5&#65289;&#26469;&#22686;&#24378;&#28183;&#36879;&#27979;&#35797;&#20154;&#21592;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#29992;&#20363;&#65306;&#29992;&#20110;&#23433;&#20840;&#27979;&#35797;&#20219;&#21153;&#30340;&#39640;&#32423;&#20219;&#21153;&#35268;&#21010;&#21644;&#22312;&#26131;&#21463;&#25915;&#20987;&#30340;&#34394;&#25311;&#26426;&#20013;&#36827;&#34892;&#20302;&#32423;&#28431;&#27934;&#23547;&#25214;&#12290;&#23545;&#20110;&#21518;&#32773;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#19968;&#20010;&#38381;&#29615;&#21453;&#39304;&#65292;&#23558;&#30001;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#20302;&#32423;&#25805;&#20316;&#19982;&#26131;&#21463;&#25915;&#20987;&#30340;&#34394;&#25311;&#26426;&#65288;&#36890;&#36807;SSH&#36830;&#25509;&#65289;&#30456;&#36830;&#65292;&#24182;&#20801;&#35768;&#35821;&#35328;&#27169;&#22411;&#20998;&#26512;&#34394;&#25311;&#26426;&#29366;&#24577;&#20197;&#23547;&#25214;&#28431;&#27934;&#65292;&#24182;&#25552;&#20379;&#20855;&#20307;&#30340;&#25915;&#20987;&#21521;&#37327;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#26377;&#21069;&#26223;&#30340;&#21021;&#27493;&#32467;&#26524;&#65292;&#35814;&#32454;&#20171;&#32461;&#20102;&#25913;&#36827;&#30340;&#36884;&#24452;&#65292;&#24182;&#23601;&#25552;&#20379;&#35813;&#25216;&#26415;&#30340;&#20262;&#29702;&#38382;&#39064;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of software security testing, more specifically penetration testing, is an activity that requires high levels of expertise and involves many manual testing and analysis steps. This paper explores the potential usage of large-language models, such as GPT3.5, to augment penetration testers with AI sparring partners. We explore the feasibility of supplementing penetration testers with AI models for two distinct use cases: high-level task planning for security testing assignments and low-level vulnerability hunting within a vulnerable virtual machine. For the latter, we implemented a closed-feedback loop between LLM-generated low-level actions with a vulnerable virtual machine (connected through SSH) and allowed the LLM to analyze the machine state for vulnerabilities and suggest concrete attack vectors which were automatically executed within the virtual machine. We discuss promising initial results, detail avenues for improvement, and close deliberating on the ethics of providi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32593;&#32476;&#22810;&#26234;&#33021;&#20307;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#38382;&#39064;&#20013;&#30340;&#20998;&#24067;&#24335;&#21160;&#24577;&#35268;&#21010;&#21644;&#20998;&#24067;&#24335;TD&#23398;&#20064;&#31639;&#27861;&#12290;&#20854;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#20998;&#24067;&#24335;DP&#31639;&#27861;&#21644;&#20998;&#24067;&#24335;TD&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;&#25910;&#25947;&#24615;&#65292;&#25552;&#20986;&#20102;&#20004;&#20010;&#20851;&#38190;&#28857;&#12290;&#35813;&#20998;&#24067;&#24335;DP&#31639;&#27861;&#20855;&#26377;&#20004;&#20010;&#29420;&#31435;&#30340;&#21160;&#24577;&#31995;&#32479;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2307.16706</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#21160;&#24577;&#35268;&#21010;&#21644;&#20998;&#24067;&#24335;TD-Learning&#30340;&#32593;&#32476;&#22810;&#26234;&#33021;&#20307;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;ODE&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Distributed Dynamic Programming and an O.D.E. Framework of Distributed TD-Learning for Networked Multi-Agent Markov Decision Processes. (arXiv:2307.16706v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16706
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32593;&#32476;&#22810;&#26234;&#33021;&#20307;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#38382;&#39064;&#20013;&#30340;&#20998;&#24067;&#24335;&#21160;&#24577;&#35268;&#21010;&#21644;&#20998;&#24067;&#24335;TD&#23398;&#20064;&#31639;&#27861;&#12290;&#20854;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#20998;&#24067;&#24335;DP&#31639;&#27861;&#21644;&#20998;&#24067;&#24335;TD&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;&#25910;&#25947;&#24615;&#65292;&#25552;&#20986;&#20102;&#20004;&#20010;&#20851;&#38190;&#28857;&#12290;&#35813;&#20998;&#24067;&#24335;DP&#31639;&#27861;&#20855;&#26377;&#20004;&#20010;&#29420;&#31435;&#30340;&#21160;&#24577;&#31995;&#32479;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20027;&#35201;&#30740;&#31350;&#32593;&#32476;&#22810;&#26234;&#33021;&#20307;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#38382;&#39064;&#20013;&#30340;&#20998;&#24067;&#24335;&#21160;&#24577;&#35268;&#21010;&#65288;DP&#65289;&#21644;&#20998;&#24067;&#24335;&#26102;&#24207;&#24046;&#20998;&#65288;TD&#65289;&#23398;&#20064;&#31639;&#27861;&#12290;&#25105;&#20204;&#37319;&#29992;&#20998;&#24067;&#24335;&#22810;&#26234;&#33021;&#20307;&#26694;&#26550;&#65292;&#20854;&#20013;&#21508;&#20010;&#26234;&#33021;&#20307;&#21482;&#33021;&#35775;&#38382;&#33258;&#24049;&#30340;&#22870;&#21169;&#65292;&#32570;&#20047;&#23545;&#20854;&#20182;&#26234;&#33021;&#20307;&#22870;&#21169;&#30340;&#20102;&#35299;&#12290;&#27492;&#22806;&#65292;&#27599;&#20010;&#26234;&#33021;&#20307;&#37117;&#33021;&#36890;&#36807;&#19968;&#20010;&#30001;&#22270;&#34920;&#31034;&#30340;&#36890;&#20449;&#32593;&#32476;&#19982;&#30456;&#37051;&#26234;&#33021;&#20307;&#20849;&#20139;&#20854;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#21487;&#20197;&#24635;&#32467;&#20026;&#20004;&#20010;&#20851;&#38190;&#28857;&#65306;1&#65289;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#21463;&#36830;&#32493;&#26102;&#38388;&#21306;&#38388;&#20869;&#30340;&#24179;&#22343;&#19968;&#33268;&#24615;&#26041;&#27861;&#21551;&#21457;&#30340;&#20998;&#24067;&#24335;DP&#12290;&#36890;&#36807;&#25511;&#21046;&#29702;&#35770;&#30340;&#35270;&#35282;&#35780;&#20272;&#20102;&#35813;DP&#30340;&#25910;&#25947;&#24615;&#12290;2&#65289;&#22522;&#20110;&#19978;&#36848;DP&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#24067;&#24335;TD&#23398;&#20064;&#31639;&#27861;&#24182;&#35777;&#26126;&#20102;&#20854;&#25910;&#25947;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#20998;&#24067;&#24335;DP&#30340;&#19968;&#20010;&#26174;&#33879;&#29305;&#28857;&#26159;&#20854;&#21253;&#21547;&#20102;&#20004;&#20010;&#29420;&#31435;&#30340;&#21160;&#24577;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
The primary objective of this paper is to investigate distributed dynamic programming (DP) and distributed temporal difference (TD) learning algorithms for networked multi-agent Markov decision problems (MAMDPs). In our study, we adopt a distributed multi-agent framework where individual agents have access only to their own rewards, lacking insights into the rewards of other agents. Additionally, each agent has the ability to share its parameters with neighboring agents through a communication network, represented by a graph. Our contributions can be summarized in two key points: 1) We introduce a novel distributed DP, inspired by the averaging consensus method in the continuous-time domain. The convergence of this DP is assessed through control theory perspectives. 2) Building upon the aforementioned DP, we devise a new distributed TD-learning algorithm and prove its convergence. A standout feature of our proposed distributed DP is its incorporation of two independent dynamic systems,
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#36828;&#31243;&#29983;&#29289;&#24863;&#24212;&#25216;&#26415;rPPG&#30340;&#20844;&#24320;&#28304;&#22522;&#20934;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#20844;&#24179;&#35780;&#20272;rPPG&#30340;&#20934;&#30830;&#24615;&#38382;&#39064;&#24182;&#35299;&#20915;&#19982;&#30382;&#32932;&#39068;&#33394;&#12289;&#30456;&#26426;&#29305;&#24615;&#21644;&#29615;&#22659;&#20809;&#31561;&#22240;&#32032;&#30456;&#20851;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.12644</link><description>&lt;p&gt;
&#36828;&#31243;&#29983;&#29289;&#24863;&#24212;&#65306;&#20844;&#24320;&#28304;&#22522;&#20934;&#26694;&#26550;&#29992;&#20110;&#20844;&#24179;&#35780;&#20272;rPPG
&lt;/p&gt;
&lt;p&gt;
Remote Bio-Sensing: Open Source Benchmark Framework for Fair Evaluation of rPPG. (arXiv:2307.12644v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12644
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#36828;&#31243;&#29983;&#29289;&#24863;&#24212;&#25216;&#26415;rPPG&#30340;&#20844;&#24320;&#28304;&#22522;&#20934;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#20844;&#24179;&#35780;&#20272;rPPG&#30340;&#20934;&#30830;&#24615;&#38382;&#39064;&#24182;&#35299;&#20915;&#19982;&#30382;&#32932;&#39068;&#33394;&#12289;&#30456;&#26426;&#29305;&#24615;&#21644;&#29615;&#22659;&#20809;&#31561;&#22240;&#32032;&#30456;&#20851;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
rPPG&#65288;&#36828;&#31243;&#20809;&#30005;&#23481;&#31215;&#33033;&#25615;&#22270;&#65289;&#26159;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;&#25668;&#20687;&#22836;&#25429;&#25417;&#21040;&#30340;&#34880;&#32418;&#34507;&#30333;&#30340;&#20809;&#21560;&#25910;&#29305;&#24615;&#26469;&#27979;&#37327;&#21644;&#20998;&#26512;BVP&#65288;&#34880;&#23481;&#37327;&#33033;&#25615;&#65289;&#30340;&#25216;&#26415;&#12290;&#20998;&#26512;&#25152;&#27979;&#37327;&#30340;BVP&#21487;&#20197;&#24471;&#20986;&#21508;&#31181;&#29983;&#29702;&#20449;&#21495;&#65292;&#22914;&#24515;&#29575;&#12289;&#21387;&#21147;&#27700;&#24179;&#21644;&#34880;&#21387;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#21508;&#31181;&#24212;&#29992;&#65292;&#22914;&#36828;&#31243;&#21307;&#30103;&#12289;&#36828;&#31243;&#24739;&#32773;&#30417;&#25252;&#21644;&#24515;&#34880;&#31649;&#30142;&#30149;&#30340;&#26089;&#26399;&#39044;&#27979;&#12290;&#23613;&#31649;&#22312;&#36825;&#20010;&#39046;&#22495;&#24050;&#32463;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#21162;&#21147;&#21644;&#36827;&#23637;&#65292;&#20294;&#20173;&#28982;&#23384;&#22312;&#20005;&#37325;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#19982;&#30382;&#32932;&#39068;&#33394;&#12289;&#30456;&#26426;&#29305;&#24615;&#12289;&#29615;&#22659;&#20809;&#21644;&#20854;&#20182;&#22122;&#22768;&#21644;&#20266;&#36857;&#26469;&#28304;&#26377;&#20851;&#30340;&#38382;&#39064;&#65292;&#36825;&#20123;&#38382;&#39064;&#38477;&#20302;&#20102;&#20934;&#30830;&#24615;&#33021;&#12290;&#25105;&#20204;&#35748;&#20026;&#36843;&#20999;&#38656;&#35201;&#20844;&#27491;&#21487;&#35780;&#20272;&#30340;&#22522;&#20934;&#27979;&#35797;&#26469;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
rPPG (Remote photoplethysmography) is a technology that measures and analyzes BVP (Blood Volume Pulse) by using the light absorption characteristics of hemoglobin captured through a camera. Analyzing the measured BVP can derive various physiological signals such as heart rate, stress level, and blood pressure, which can be applied to various applications such as telemedicine, remote patient monitoring, and early prediction of cardiovascular disease. rPPG is rapidly evolving and attracting great attention from both academia and industry by providing great usability and convenience as it can measure biosignals using a camera-equipped device without medical or wearable devices. Despite extensive efforts and advances in this field, serious challenges remain, including issues related to skin color, camera characteristics, ambient lighting, and other sources of noise and artifacts, which degrade accuracy performance. We argue that fair and evaluable benchmarking is urgently required to overc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;GradObstinate&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#39037;&#22266;&#23545;&#25239;&#26679;&#26412;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#33258;&#21160;&#29983;&#25104;&#24847;&#20041;&#25913;&#21464;&#20294;&#27169;&#22411;&#39044;&#27979;&#32467;&#26524;&#20445;&#25345;&#19981;&#21464;&#30340;&#23545;&#25239;&#26679;&#26412;&#65292;&#26080;&#38656;&#20154;&#24037;&#35774;&#35745;&#32422;&#26463;&#12290;</title><link>http://arxiv.org/abs/2307.12507</link><description>&lt;p&gt;
&#22522;&#20110;&#26799;&#24230;&#30340;&#35789;&#26367;&#25442;&#29992;&#20110;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#20013;&#39037;&#22266;&#23545;&#25239;&#26679;&#26412;
&lt;/p&gt;
&lt;p&gt;
Gradient-Based Word Substitution for Obstinate Adversarial Examples Generation in Language Models. (arXiv:2307.12507v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12507
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;GradObstinate&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#39037;&#22266;&#23545;&#25239;&#26679;&#26412;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#33258;&#21160;&#29983;&#25104;&#24847;&#20041;&#25913;&#21464;&#20294;&#27169;&#22411;&#39044;&#27979;&#32467;&#26524;&#20445;&#25345;&#19981;&#21464;&#30340;&#23545;&#25239;&#26679;&#26412;&#65292;&#26080;&#38656;&#20154;&#24037;&#35774;&#35745;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#36890;&#36807;&#35789;&#26367;&#25442;&#29983;&#25104;&#39037;&#22266;&#65288;&#36229;&#31283;&#23450;&#24615;&#65289;&#23545;&#25239;&#26679;&#26412;&#30340;&#38382;&#39064;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#36755;&#20837;&#25991;&#26412;&#30340;&#24847;&#20041;&#21457;&#29983;&#20102;&#25913;&#21464;&#65292;&#20294;&#27169;&#22411;&#30340;&#39044;&#27979;&#21364;&#27809;&#26377;&#21464;&#21270;&#65292;&#23613;&#31649;&#24212;&#35813;&#21457;&#29983;&#21464;&#21270;&#12290;&#20197;&#24448;&#30340;&#35789;&#26367;&#25442;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#25163;&#21160;&#35774;&#35745;&#30340;&#21453;&#20041;&#35789;&#31574;&#30053;&#19978;&#65292;&#29992;&#20110;&#29983;&#25104;&#39037;&#22266;&#23545;&#25239;&#26679;&#26412;&#65292;&#36825;&#21046;&#32422;&#20102;&#23427;&#30340;&#24212;&#29992;&#65292;&#22240;&#20026;&#36825;&#20123;&#31574;&#30053;&#21482;&#33021;&#25214;&#21040;&#37096;&#20998;&#39037;&#22266;&#23545;&#25239;&#26679;&#26412;&#65292;&#24182;&#19988;&#38656;&#35201;&#20154;&#24037;&#21162;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35789;&#26367;&#25442;&#26041;&#27861;&#65292;&#21517;&#20026;GradObstinate&#65292;&#23427;&#26159;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#29983;&#25104;&#39037;&#22266;&#23545;&#25239;&#26679;&#26412;&#65292;&#19981;&#21463;&#25628;&#32034;&#31354;&#38388;&#38480;&#21046;&#25110;&#38656;&#27714;&#20154;&#24037;&#35774;&#35745;&#21407;&#21017;&#30340;&#32422;&#26463;&#12290;&#20026;&#20102;&#32463;&#39564;&#24615;&#22320;&#35780;&#20272;GradObstinate&#30340;&#25928;&#26524;&#65292;&#25105;&#20204;&#22312;&#19982;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22522;&#20934;&#27169;&#22411;&#65288;Electra&#12289;ALBERT&#12289;Roberta&#12289;DistillBERT&#21644;CLIP&#65289;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study the problem of generating obstinate (over-stability) adversarial examples by word substitution in NLP, where input text is meaningfully changed but the model's prediction does not, even though it should. Previous word substitution approaches have predominantly focused on manually designed antonym-based strategies for generating obstinate adversarial examples, which hinders its application as these strategies can only find a subset of obstinate adversarial examples and require human efforts. To address this issue, in this paper, we introduce a novel word substitution method named GradObstinate, a gradient-based approach that automatically generates obstinate adversarial examples without any constraints on the search space or the need for manual design principles. To empirically evaluate the efficacy of GradObstinate, we conduct comprehensive experiments on five representative models (Electra, ALBERT, Roberta, DistillBERT, and CLIP) finetuned on four NLP benchmark
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#24335;&#36523;&#20221;&#34920;&#31034;&#26465;&#20214;&#21270;&#35760;&#24518;&#34917;&#20607;&#32593;&#32476;&#65292;&#29992;&#20110;&#39640;&#20445;&#30495;&#24230;&#30340;&#33258;&#28982;&#22836;&#37096;&#35270;&#39057;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2307.09906</link><description>&lt;p&gt;
&#38544;&#24335;&#36523;&#20221;&#34920;&#31034;&#26465;&#20214;&#21270;&#35760;&#24518;&#34917;&#20607;&#32593;&#32476;&#29992;&#20110;&#29983;&#25104;&#33258;&#28982;&#22836;&#37096;&#35270;&#39057;
&lt;/p&gt;
&lt;p&gt;
Implicit Identity Representation Conditioned Memory Compensation Network for Talking Head video Generation. (arXiv:2307.09906v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09906
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#24335;&#36523;&#20221;&#34920;&#31034;&#26465;&#20214;&#21270;&#35760;&#24518;&#34917;&#20607;&#32593;&#32476;&#65292;&#29992;&#20110;&#39640;&#20445;&#30495;&#24230;&#30340;&#33258;&#28982;&#22836;&#37096;&#35270;&#39057;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22836;&#37096;&#35270;&#39057;&#29983;&#25104;&#26088;&#22312;&#36890;&#36807;&#20174;&#30446;&#26631;&#39537;&#21160;&#35270;&#39057;&#20013;&#25552;&#21462;&#30340;&#21160;&#24577;&#23039;&#21183;&#21644;&#34920;&#24773;&#26469;&#32473;&#38745;&#24577;&#22270;&#20687;&#20013;&#30340;&#20154;&#33080;&#28155;&#21152;&#21160;&#30011;&#25928;&#26524;&#65292;&#21516;&#26102;&#20445;&#25345;&#28304;&#22270;&#20687;&#20013;&#30340;&#20010;&#20154;&#36523;&#20221;&#12290;&#28982;&#32780;&#65292;&#39537;&#21160;&#35270;&#39057;&#20013;&#25103;&#21095;&#24615;&#21644;&#22797;&#26434;&#30340;&#36816;&#21160;&#20250;&#23548;&#33268;&#29983;&#25104;&#27169;&#31946;&#19981;&#28165;&#65292;&#22240;&#20026;&#38745;&#24577;&#28304;&#22270;&#20687;&#26080;&#27861;&#25552;&#20379;&#36275;&#22815;&#30340;&#22806;&#35266;&#20449;&#24687;&#26469;&#22788;&#29702;&#34987;&#36974;&#25377;&#21306;&#22495;&#25110;&#24494;&#22937;&#30340;&#34920;&#24773;&#21464;&#21270;&#65292;&#36825;&#20250;&#20135;&#29983;&#20005;&#37325;&#30340;&#20266;&#24433;&#24182;&#20005;&#37325;&#38477;&#20302;&#29983;&#25104;&#36136;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23398;&#20064;&#20840;&#23616;&#20154;&#33080;&#34920;&#31034;&#31354;&#38388;&#30340;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38544;&#24335;&#36523;&#20221;&#34920;&#31034;&#26465;&#20214;&#21270;&#35760;&#24518;&#34917;&#20607;&#32593;&#32476;&#65292;&#31216;&#20026;MCNet&#65292;&#29992;&#20110;&#39640;&#20445;&#30495;&#24230;&#30340;&#22836;&#37096;&#35270;&#39057;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Talking head video generation aims to animate a human face in a still image with dynamic poses and expressions using motion information derived from a target-driving video, while maintaining the person's identity in the source image. However, dramatic and complex motions in the driving video cause ambiguous generation, because the still source image cannot provide sufficient appearance information for occluded regions or delicate expression variations, which produces severe artifacts and significantly degrades the generation quality. To tackle this problem, we propose to learn a global facial representation space, and design a novel implicit identity representation conditioned memory compensation network, coined as MCNet, for high-fidelity talking head generation.~Specifically, we devise a network module to learn a unified spatial facial meta-memory bank from all training samples, which can provide rich facial structure and appearance priors to compensate warped source facial features 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#36866;&#29992;&#20110;&#22810;&#31867;&#35757;&#32451;&#35774;&#32622;&#30340;&#26032;&#22411;ReDB&#26694;&#26550;&#26469;&#35299;&#20915;&#29616;&#26377;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#22312;&#22810;&#31867;&#35757;&#32451;&#35774;&#32622;&#19979;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#20135;&#29983;&#21487;&#38752;&#30340;&#12289;&#22810;&#26679;&#21270;&#30340;&#21644;&#31867;&#24179;&#34913;&#30340;&#20266;&#19977;&#32500;&#26694;&#26469;&#24341;&#23548;&#30446;&#26631;&#39046;&#22495;&#30340;&#33258;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2307.07944</link><description>&lt;p&gt;
&#36890;&#36807;&#21487;&#38752;&#30340;&#12289;&#22810;&#26679;&#21270;&#30340;&#21644;&#31867;&#24179;&#34913;&#30340;&#20266;&#26631;&#31614;&#26469;&#37325;&#26032;&#23457;&#35270;&#39046;&#22495;&#33258;&#36866;&#24212;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Revisiting Domain-Adaptive 3D Object Detection by Reliable, Diverse and Class-balanced Pseudo-Labeling. (arXiv:2307.07944v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07944
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#36866;&#29992;&#20110;&#22810;&#31867;&#35757;&#32451;&#35774;&#32622;&#30340;&#26032;&#22411;ReDB&#26694;&#26550;&#26469;&#35299;&#20915;&#29616;&#26377;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#22312;&#22810;&#31867;&#35757;&#32451;&#35774;&#32622;&#19979;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#20135;&#29983;&#21487;&#38752;&#30340;&#12289;&#22810;&#26679;&#21270;&#30340;&#21644;&#31867;&#24179;&#34913;&#30340;&#20266;&#19977;&#32500;&#26694;&#26469;&#24341;&#23548;&#30446;&#26631;&#39046;&#22495;&#30340;&#33258;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#19982;&#20266;&#26631;&#31614;&#25216;&#26415;&#30340;&#36741;&#21161;&#24050;&#32463;&#25104;&#20026;&#39046;&#22495;&#33258;&#36866;&#24212;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#30340;&#20851;&#38190;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#22312;&#24212;&#29992;&#20110;&#22810;&#31867;&#35757;&#32451;&#35774;&#32622;&#26102;&#24615;&#33021;&#22823;&#24133;&#19979;&#38477;&#65292;&#21407;&#22240;&#26159;&#20266;&#26631;&#31614;&#30340;&#36136;&#37327;&#20302;&#21644;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#20849;&#23384;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#38024;&#23545;&#21516;&#26102;&#23398;&#20064;&#26816;&#27979;&#25152;&#26377;&#31867;&#21035;&#30340;&#26032;&#22411;ReDB&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20135;&#29983;&#21487;&#38752;&#30340;&#12289;&#22810;&#26679;&#21270;&#30340;&#21644;&#31867;&#24179;&#34913;&#30340;&#20266;&#19977;&#32500;&#26694;&#65292;&#36890;&#36807;&#36845;&#20195;&#22320;&#24341;&#23548;&#19981;&#21516;&#20998;&#24067;&#30340;&#30446;&#26631;&#39046;&#22495;&#30340;&#33258;&#35757;&#32451;&#12290;&#20026;&#20102;&#20943;&#36731;&#29615;&#22659;&#24046;&#24322;&#65288;&#20363;&#22914;&#65292;&#20809;&#26463;&#25968;&#37327;&#65289;&#24102;&#26469;&#30340;&#24178;&#25200;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36328;&#22495;&#26816;&#26597;&#65288;CDE&#65289;&#65292;&#36890;&#36807;&#23558;&#30446;&#26631;&#23454;&#20363;&#22797;&#21046;&#31896;&#36148;&#21040;&#28304;&#29615;&#22659;&#20013;&#24182;&#27979;&#37327;&#39044;&#27979;&#30340;&#19968;&#33268;&#24615;&#26469;&#35780;&#20272;&#20266;&#26631;&#31614;&#30340;&#27491;&#30830;&#24615;&#12290;&#20026;&#20102;&#20943;&#23569;&#35745;&#31639;&#24320;&#38144;&#21644;&#32531;&#35299;&#29289;&#20307;&#30340;&#36716;&#31227;&#65288;&#20363;&#22914;&#65292;
&lt;/p&gt;
&lt;p&gt;
Unsupervised domain adaptation (DA) with the aid of pseudo labeling techniques has emerged as a crucial approach for domain-adaptive 3D object detection. While effective, existing DA methods suffer from a substantial drop in performance when applied to a multi-class training setting, due to the co-existence of low-quality pseudo labels and class imbalance issues. In this paper, we address this challenge by proposing a novel ReDB framework tailored for learning to detect all classes at once. Our approach produces Reliable, Diverse, and class-Balanced pseudo 3D boxes to iteratively guide the self-training on a distributionally different target domain. To alleviate disruptions caused by the environmental discrepancy (e.g., beam numbers), the proposed cross-domain examination (CDE) assesses the correctness of pseudo labels by copy-pasting target instances into a source environment and measuring the prediction consistency. To reduce computational overhead and mitigate the object shift (e.g.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#35745;&#31639;&#30149;&#29702;&#23398;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#35782;&#21035;&#21644;&#23450;&#37327;&#24515;&#32908;&#32452;&#32455;&#28814;&#30151;&#28024;&#28070;&#22312;&#25968;&#23383;HE&#26579;&#33394;&#22270;&#20687;&#20013;&#65292;&#20026;&#24515;&#32908;&#28814;&#25552;&#20379;&#23450;&#37327;&#32452;&#32455;&#23398;&#35786;&#26029;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25351;&#26631;LND&#30340;&#23450;&#37327;&#26469;&#21028;&#26029;&#26159;&#21542;&#23384;&#22312;&#24515;&#32908;&#28814;&#30151;&#28024;&#28070;&#12290;</title><link>http://arxiv.org/abs/2307.01098</link><description>&lt;p&gt;
&#33258;&#21160;&#35782;&#21035;&#21644;&#23450;&#37327;&#24515;&#32908;&#32452;&#32455;&#28814;&#30151;&#28024;&#28070;&#22312;&#25968;&#23383;&#32452;&#32455;&#20999;&#29255;&#22270;&#20687;&#20013;&#20197;&#35786;&#26029;&#24515;&#32908;&#28814;
&lt;/p&gt;
&lt;p&gt;
Automated identification and quantification of myocardial inflammatory infiltration in digital histological images to diagnose myocarditis. (arXiv:2307.01098v1 [physics.med-ph] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01098
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#35745;&#31639;&#30149;&#29702;&#23398;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#35782;&#21035;&#21644;&#23450;&#37327;&#24515;&#32908;&#32452;&#32455;&#28814;&#30151;&#28024;&#28070;&#22312;&#25968;&#23383;HE&#26579;&#33394;&#22270;&#20687;&#20013;&#65292;&#20026;&#24515;&#32908;&#28814;&#25552;&#20379;&#23450;&#37327;&#32452;&#32455;&#23398;&#35786;&#26029;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25351;&#26631;LND&#30340;&#23450;&#37327;&#26469;&#21028;&#26029;&#26159;&#21542;&#23384;&#22312;&#24515;&#32908;&#28814;&#30151;&#28024;&#28070;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#26032;&#30340;&#35745;&#31639;&#30149;&#29702;&#23398;&#26041;&#27861;&#65292;&#33258;&#21160;&#35782;&#21035;&#21644;&#23450;&#37327;&#24515;&#32908;&#32452;&#32455;&#28814;&#30151;&#28024;&#28070;&#22312;&#25968;&#23383;HE&#26579;&#33394;&#22270;&#20687;&#20013;&#65292;&#20197;&#25552;&#20379;&#24515;&#32908;&#28814;&#30340;&#23450;&#37327;&#32452;&#32455;&#23398;&#35786;&#26029;&#12290;&#26412;&#30740;&#31350;&#21253;&#25324;154&#21517;&#24515;&#33039;&#31227;&#26893;&#30149;&#20154;&#35786;&#26029;&#20026;&#24515;&#32908;&#28814;&#25110;&#25193;&#24352;&#22411;&#24515;&#32908;&#30149;&#30340;898&#20010;HE&#26579;&#33394;&#20840;&#20999;&#29255;&#22270;&#20687;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;DL&#30340;&#33258;&#21160;&#35745;&#31639;&#30149;&#29702;&#23398;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#32454;&#32990;&#26680;&#24182;&#26816;&#27979;&#24515;&#32908;&#32452;&#32455;&#28814;&#30151;&#28024;&#28070;&#65292;&#20351;&#24471;&#21487;&#20197;&#23450;&#37327;&#27979;&#37327;&#24515;&#32908;&#20840;&#20999;&#29255;&#22270;&#20687;&#19978;&#30340;&#28107;&#24052;&#32454;&#32990;&#26680;&#23494;&#24230;&#65288;LND&#65289;&#12290;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;LND&#23450;&#37327;&#30340;&#25130;&#26029;&#20540;&#26469;&#30830;&#23450;&#26159;&#21542;&#23384;&#22312;&#24515;&#32908;&#28814;&#30151;&#28024;&#28070;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#20116;&#25240;&#20132;&#21449;&#39564;&#35777;&#23454;&#39564;&#35780;&#20272;&#24615;&#33021;&#65292;&#20351;&#29992;&#26469;&#33258;&#24515;&#32908;&#28814;&#32452;&#30340;&#20869;&#37096;&#27979;&#35797;&#38598;&#36827;&#34892;&#27979;&#35797;&#65292;&#24182;&#36890;&#36807;&#21452;&#30450;&#35797;&#39564;&#30340;&#22806;&#37096;&#27979;&#35797;&#36827;&#34892;&#30830;&#35748;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study aims to develop a new computational pathology approach that automates the identification and quantification of myocardial inflammatory infiltration in digital HE-stained images to provide a quantitative histological diagnosis of myocarditis.898 HE-stained whole slide images (WSIs) of myocardium from 154 heart transplant patients diagnosed with myocarditis or dilated cardiomyopathy (DCM) were included in this study. An automated DL-based computational pathology approach was developed to identify nuclei and detect myocardial inflammatory infiltration, enabling the quantification of the lymphocyte nuclear density (LND) on myocardial WSIs. A cutoff value based on the quantification of LND was proposed to determine if the myocardial inflammatory infiltration was present. The performance of our approach was evaluated with a five-fold cross-validation experiment, tested with an internal test set from the myocarditis group, and confirmed by an external test from a double-blind trial
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;3D&#20154;&#20307;&#36816;&#21160;&#37325;&#24314;&#30340;&#22320;&#38754;&#24863;&#30693;&#36816;&#21160;&#27169;&#22411;&#65288;GraMMaR&#65289;&#65292;&#36890;&#36807;&#23398;&#20064;&#23039;&#21183;&#21644;&#20851;&#33410;&#19982;&#22320;&#38754;&#20043;&#38388;&#30340;&#20114;&#21160;&#30340;&#36807;&#28193;&#20998;&#24067;&#65292;&#26126;&#30830;&#20419;&#36827;&#36816;&#21160;&#21644;&#19982;&#22320;&#38754;&#36317;&#31163;&#21464;&#21270;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.16736</link><description>&lt;p&gt;
GraMMaR: &#29992;&#20110;3D&#20154;&#20307;&#36816;&#21160;&#37325;&#24314;&#30340;&#22320;&#38754;&#24863;&#30693;&#36816;&#21160;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
GraMMaR: Ground-aware Motion Model for 3D Human Motion Reconstruction. (arXiv:2306.16736v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16736
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;3D&#20154;&#20307;&#36816;&#21160;&#37325;&#24314;&#30340;&#22320;&#38754;&#24863;&#30693;&#36816;&#21160;&#27169;&#22411;&#65288;GraMMaR&#65289;&#65292;&#36890;&#36807;&#23398;&#20064;&#23039;&#21183;&#21644;&#20851;&#33410;&#19982;&#22320;&#38754;&#20043;&#38388;&#30340;&#20114;&#21160;&#30340;&#36807;&#28193;&#20998;&#24067;&#65292;&#26126;&#30830;&#20419;&#36827;&#36816;&#21160;&#21644;&#19982;&#22320;&#38754;&#36317;&#31163;&#21464;&#21270;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#20934;&#30830;&#21644;&#30495;&#23454;&#30340;&#20174;RGB&#35270;&#39057;&#20013;&#37325;&#24314;3D&#20154;&#20307;&#36816;&#21160;&#65292;&#35299;&#23494;&#22797;&#26434;&#30340;&#20154;&#22320;&#20114;&#21160;&#23545;&#20110;&#20445;&#35777;&#20154;&#31867;&#21644;&#22320;&#38754;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#20197;&#24448;&#30340;&#26041;&#27861;&#35201;&#20040;&#38544;&#24335;&#22320;&#27169;&#25311;&#20154;&#22320;&#20114;&#21160;&#65292;&#35201;&#20040;&#20197;&#31232;&#30095;&#30340;&#26041;&#24335;&#27169;&#25311;&#65292;&#24448;&#24448;&#22312;&#38754;&#23545;&#22122;&#22768;&#21644;&#19981;&#30830;&#23450;&#24615;&#26102;&#23548;&#33268;&#19981;&#30495;&#23454;&#21644;&#19981;&#27491;&#30830;&#30340;&#36816;&#21160;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20197;&#19968;&#31181;&#23494;&#38598;&#21644;&#36830;&#32493;&#30340;&#26041;&#24335;&#26126;&#30830;&#34920;&#31034;&#36825;&#20123;&#20114;&#21160;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;3D&#20154;&#20307;&#36816;&#21160;&#37325;&#24314;&#30340;&#22320;&#38754;&#24863;&#30693;&#36816;&#21160;&#27169;&#22411;&#65292;&#31216;&#20026;GraMMaR&#65292;&#23427;&#22312;&#36816;&#21160;&#24207;&#21015;&#20013;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#20013;&#21516;&#26102;&#23398;&#20064;&#23039;&#21183;&#21644;&#27599;&#20010;&#20851;&#33410;&#19982;&#22320;&#38754;&#20043;&#38388;&#30340;&#20114;&#21160;&#30340;&#36807;&#28193;&#20998;&#24067;&#12290;&#23427;&#34987;&#35757;&#32451;&#29992;&#20110;&#26126;&#30830;&#20419;&#36827;&#36816;&#21160;&#21644;&#19982;&#22320;&#38754;&#36317;&#31163;&#21464;&#21270;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;&#35757;&#32451;&#21518;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#31181;&#32852;&#21512;&#20248;&#21270;&#31574;&#30053;&#65292;&#21033;&#29992;GraMMaR&#20316;&#20026;&#21452;&#37325;&#20808;&#39564;&#65292;&#35268;&#33539;&#20248;&#21270;&#36807;&#31243;&#26397;&#30528;
&lt;/p&gt;
&lt;p&gt;
Demystifying complex human-ground interactions is essential for accurate and realistic 3D human motion reconstruction from RGB videos, as it ensures consistency between the humans and the ground plane. Prior methods have modeled human-ground interactions either implicitly or in a sparse manner, often resulting in unrealistic and incorrect motions when faced with noise and uncertainty. In contrast, our approach explicitly represents these interactions in a dense and continuous manner. To this end, we propose a novel Ground-aware Motion Model for 3D Human Motion Reconstruction, named GraMMaR, which jointly learns the distribution of transitions in both pose and interaction between every joint and ground plane at each time step of a motion sequence. It is trained to explicitly promote consistency between the motion and distance change towards the ground. After training, we establish a joint optimization strategy that utilizes GraMMaR as a dual-prior, regularizing the optimization towards 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#27169;&#25311;&#24179;&#21488;&#19978;&#23545;&#20004;&#31181;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#21644;&#27604;&#36739;&#65292;&#20197;&#24320;&#21457;&#33258;&#21160;&#39550;&#39542;&#27169;&#22411;&#12290;&#36890;&#36807;&#23450;&#21046;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#23545;&#20934;&#30830;&#24230;&#12289;&#25928;&#29575;&#12289;&#23433;&#20840;&#24615;&#21644;&#33298;&#36866;&#24230;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2306.11466</link><description>&lt;p&gt;
&#22312;&#21508;&#31181;&#27169;&#25311;&#39550;&#39542;&#25805;&#20316;&#20013;&#23545;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#20840;&#38754;&#22521;&#35757;&#21644;&#35780;&#20272;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Comprehensive Training and Evaluation on Deep Reinforcement Learning for Automated Driving in Various Simulated Driving Maneuvers. (arXiv:2306.11466v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11466
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#27169;&#25311;&#24179;&#21488;&#19978;&#23545;&#20004;&#31181;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#21644;&#27604;&#36739;&#65292;&#20197;&#24320;&#21457;&#33258;&#21160;&#39550;&#39542;&#27169;&#22411;&#12290;&#36890;&#36807;&#23450;&#21046;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#23545;&#20934;&#30830;&#24230;&#12289;&#25928;&#29575;&#12289;&#23433;&#20840;&#24615;&#21644;&#33298;&#36866;&#24230;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#24320;&#21457;&#21644;&#27979;&#35797;&#33258;&#21160;&#39550;&#39542;&#27169;&#22411;&#21487;&#33021;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#29978;&#33267;&#21361;&#38505;&#30340;&#65292;&#32780;&#27169;&#25311;&#21487;&#20197;&#24110;&#21161;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#39550;&#39542;&#25805;&#20316;&#12290;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#36890;&#36807;&#23398;&#20064;&#21644;&#19982;&#29615;&#22659;&#20132;&#20114;&#26469;&#22788;&#29702;&#22797;&#26434;&#30340;&#20915;&#31574;&#21644;&#25511;&#21046;&#20219;&#21153;&#65292;&#22240;&#27492;&#38750;&#24120;&#36866;&#21512;&#24320;&#21457;&#33258;&#21160;&#39550;&#39542;&#65292;&#22312;&#36825;&#26041;&#38754;&#30340;&#20855;&#20307;&#30740;&#31350;&#36824;&#19981;&#22810;&#12290;&#26412;&#30740;&#31350;&#22312;highway-env&#27169;&#25311;&#24179;&#21488;&#19978;&#23454;&#26045;&#12289;&#35780;&#20272;&#21644;&#27604;&#36739;&#20102;&#20004;&#20010;DRL&#31639;&#27861;&#65292;Deep Q-networks&#65288;DQN&#65289;&#21644;Trust Region Policy Optimization&#65288;TRPO&#65289;&#65292;&#20197;&#22521;&#35757;&#33258;&#21160;&#39550;&#39542;&#12290;&#21516;&#26102;&#24320;&#21457;&#20102;&#26377;&#25928;&#19988;&#23450;&#21046;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#24182;&#36890;&#36807;&#20934;&#30830;&#24230;&#65288;&#36710;&#36742;&#22312;&#36947;&#36335;&#19978;&#30340;&#34892;&#39542;&#24773;&#20917;&#65289;&#12289;&#25928;&#29575;&#65288;&#36710;&#36742;&#30340;&#34892;&#39542;&#36895;&#24230;&#65289;&#12289;&#23433;&#20840;&#24615;&#65288;&#36710;&#36742;&#36991;&#20813;&#19982;&#38556;&#30861;&#29289;&#30896;&#25758;&#30340;&#21487;&#33021;&#24615;&#65289;&#21644;&#33298;&#36866;&#24230;&#65288;&#36710;&#36742;&#39550;&#39542;&#30340;&#33298;&#36866;&#31243;&#24230;&#65289;&#26469;&#35780;&#20272;&#24050;&#23454;&#26045;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developing and testing automated driving models in the real world might be challenging and even dangerous, while simulation can help with this, especially for challenging maneuvers. Deep reinforcement learning (DRL) has the potential to tackle complex decision-making and controlling tasks through learning and interacting with the environment, thus it is suitable for developing automated driving while not being explored in detail yet. This study carried out a comprehensive study by implementing, evaluating, and comparing the two DRL algorithms, Deep Q-networks (DQN) and Trust Region Policy Optimization (TRPO), for training automated driving on the highway-env simulation platform. Effective and customized reward functions were developed and the implemented algorithms were evaluated in terms of onlane accuracy (how well the car drives on the road within the lane), efficiency (how fast the car drives), safety (how likely the car is to crash into obstacles), and comfort (how much the car ma
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#32852;&#37030;&#23398;&#20064;&#21442;&#32771;&#26550;&#26500;&#65292;&#36890;&#36807;&#32467;&#21512;&#32852;&#37030;&#23398;&#20064;&#21644;&#21306;&#22359;&#38142;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#21435;&#20013;&#24515;&#21270;&#12289;&#21327;&#20316;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#65292;&#24182;&#20445;&#25252;&#20102;&#25968;&#25454;&#38544;&#31169;&#21644;&#29992;&#25143;&#25511;&#21046;&#30340;&#36523;&#20221;&#12290;&#35813;&#26550;&#26500;&#20351;&#29992;&#21435;&#20013;&#24515;&#21270;&#26631;&#35782;&#31526;&#36827;&#34892;&#36523;&#20221;&#39564;&#35777;&#65292;&#36890;&#36807;&#26234;&#33021;&#21512;&#32422;&#23454;&#29616;&#24378;&#22823;&#30340;&#23433;&#20840;&#24615;&#21644;&#39640;&#25928;&#30340;&#21435;&#20013;&#24515;&#21270;&#65292;&#24182;&#33021;&#26681;&#25454;&#38656;&#27714;&#38598;&#25104;&#21508;&#31181;&#39069;&#22806;&#30340;&#20803;&#32032;&#65292;&#26159;&#19968;&#20010;&#36866;&#29992;&#33539;&#22260;&#24191;&#27867;&#30340; BCFL &#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2306.10841</link><description>&lt;p&gt;
&#21306;&#22359;&#38142;&#25903;&#25345;&#30340;&#32852;&#37030;&#23398;&#20064;&#65306;&#21442;&#32771;&#26550;&#26500;&#35774;&#35745;&#12289;&#23454;&#29616;&#21644;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Blockchain-Enabled Federated Learning: A Reference Architecture Design, Implementation, and Verification. (arXiv:2306.10841v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10841
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#32852;&#37030;&#23398;&#20064;&#21442;&#32771;&#26550;&#26500;&#65292;&#36890;&#36807;&#32467;&#21512;&#32852;&#37030;&#23398;&#20064;&#21644;&#21306;&#22359;&#38142;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#21435;&#20013;&#24515;&#21270;&#12289;&#21327;&#20316;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#65292;&#24182;&#20445;&#25252;&#20102;&#25968;&#25454;&#38544;&#31169;&#21644;&#29992;&#25143;&#25511;&#21046;&#30340;&#36523;&#20221;&#12290;&#35813;&#26550;&#26500;&#20351;&#29992;&#21435;&#20013;&#24515;&#21270;&#26631;&#35782;&#31526;&#36827;&#34892;&#36523;&#20221;&#39564;&#35777;&#65292;&#36890;&#36807;&#26234;&#33021;&#21512;&#32422;&#23454;&#29616;&#24378;&#22823;&#30340;&#23433;&#20840;&#24615;&#21644;&#39640;&#25928;&#30340;&#21435;&#20013;&#24515;&#21270;&#65292;&#24182;&#33021;&#26681;&#25454;&#38656;&#27714;&#38598;&#25104;&#21508;&#31181;&#39069;&#22806;&#30340;&#20803;&#32032;&#65292;&#26159;&#19968;&#20010;&#36866;&#29992;&#33539;&#22260;&#24191;&#27867;&#30340; BCFL &#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;BCFL&#65289;&#21442;&#32771;&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#23558;&#32852;&#37030;&#23398;&#20064;&#21644;&#21306;&#22359;&#38142;&#25216;&#26415;&#30340;&#20248;&#21183;&#32467;&#21512;&#36215;&#26469;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#20010;&#21435;&#20013;&#24515;&#21270;&#30340;&#12289;&#21327;&#20316;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#65292;&#23562;&#37325;&#25968;&#25454;&#38544;&#31169;&#21644;&#29992;&#25143;&#25511;&#21046;&#30340;&#36523;&#20221;&#12290;&#25105;&#20204;&#30340;&#26550;&#26500;&#25112;&#30053;&#24615;&#22320;&#37319;&#29992;&#22522;&#20110;&#21435;&#20013;&#24515;&#21270;&#26631;&#35782;&#31526;&#65288;DID&#65289;&#30340;&#36523;&#20221;&#39564;&#35777;&#31995;&#32479;&#65292;&#20801;&#35768;&#21442;&#19982;&#32773;&#20351;&#29992;&#20854;&#33258;&#20027; DID &#23433;&#20840;&#22320;&#35748;&#35777;&#24182;&#33719;&#24471;&#23545;&#32852;&#37030;&#23398;&#20064;&#24179;&#21488;&#30340;&#35775;&#38382;&#26435;&#38480;&#65292;&#36825;&#20123;&#20449;&#24687;&#34987;&#35760;&#24405;&#22312;&#21306;&#22359;&#38142;&#19978;&#12290;&#36890;&#36807;&#25191;&#34892;&#26234;&#33021;&#21512;&#32422;&#26469;&#30830;&#20445;&#24378;&#22823;&#30340;&#23433;&#20840;&#24615;&#21644;&#39640;&#25928;&#30340;&#21435;&#20013;&#24515;&#21270;&#26159;&#25105;&#20204;&#26041;&#27861;&#30340;&#20851;&#38190;&#26041;&#38754;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340; BCFL &#21442;&#32771;&#26550;&#26500;&#25552;&#20379;&#20102;&#26174;&#33879;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#33021;&#22815;&#26681;&#25454;&#29305;&#23450;&#38656;&#27714;&#21644;&#29992;&#20363;&#38598;&#25104;&#21508;&#31181;&#39069;&#22806;&#30340;&#20803;&#32032;&#65292;&#20351;&#20854;&#25104;&#20026;&#24191;&#27867;&#36866;&#29992;&#30340; BCFL &#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents an innovative reference architecture for blockchain-enabled federated learning (BCFL), a state-of-the-art approach that amalgamates the strengths of federated learning and blockchain technology. This results in a decentralized, collaborative machine learning system that respects data privacy and user-controlled identity. Our architecture strategically employs a decentralized identifier (DID)-based authentication system, allowing participants to authenticate and then gain access to the federated learning platform securely using their self-sovereign DIDs, which are recorded on the blockchain. Ensuring robust security and efficient decentralization through the execution of smart contracts is a key aspect of our approach. Moreover, our BCFL reference architecture provides significant extensibility, accommodating the integration of various additional elements, as per specific requirements and use cases, thereby rendering it an adaptable solution for a wide range of BCFL 
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#29983;&#25104;&#30340;&#20869;&#23481;&#31361;&#30772;&#20102;&#33402;&#26415;&#12289;&#38899;&#20048;&#21644;&#23186;&#20307;&#39046;&#22495;&#65292;&#24341;&#21457;&#20102;&#25991;&#21270;&#36716;&#21464;&#12290;&#23427;&#36890;&#36807;&#25913;&#21464;&#20154;&#20204;&#30340;&#35282;&#33394;&#12289;&#36716;&#21464;&#20215;&#20540;&#35266;&#20197;&#21450;&#25361;&#25112;&#20256;&#32479;&#23454;&#36341;&#26041;&#24335;&#65292;&#20026;&#33402;&#26415;&#30340;&#26410;&#26469;&#25171;&#24320;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.10054</link><description>&lt;p&gt;
&#36890;&#36807;&#20154;&#24037;&#26234;&#33021;&#24341;&#36215;&#30340;&#33402;&#26415;&#23454;&#36341;&#30340;&#36716;&#21464;
&lt;/p&gt;
&lt;p&gt;
A Shift In Artistic Practices through Artificial Intelligence. (arXiv:2306.10054v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10054
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#29983;&#25104;&#30340;&#20869;&#23481;&#31361;&#30772;&#20102;&#33402;&#26415;&#12289;&#38899;&#20048;&#21644;&#23186;&#20307;&#39046;&#22495;&#65292;&#24341;&#21457;&#20102;&#25991;&#21270;&#36716;&#21464;&#12290;&#23427;&#36890;&#36807;&#25913;&#21464;&#20154;&#20204;&#30340;&#35282;&#33394;&#12289;&#36716;&#21464;&#20215;&#20540;&#35266;&#20197;&#21450;&#25361;&#25112;&#20256;&#32479;&#23454;&#36341;&#26041;&#24335;&#65292;&#20026;&#33402;&#26415;&#30340;&#26410;&#26469;&#25171;&#24320;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#29983;&#25104;&#30340;&#22823;&#37327;&#20869;&#23481;&#30340;&#29190;&#28856;&#24341;&#21457;&#20102;&#33402;&#26415;&#12289;&#38899;&#20048;&#21644;&#23186;&#20307;&#39046;&#22495;&#30340;&#25991;&#21270;&#36716;&#21464;&#65292;&#35282;&#33394;&#21464;&#21270;&#12289;&#20215;&#20540;&#35266;&#36716;&#21464;&#21644;&#20256;&#32479;&#21463;&#21040;&#25361;&#25112;&#12290;&#20114;&#32852;&#32593;&#19978;&#21487;&#33719;&#24471;&#30340;&#24191;&#38420;&#25968;&#25454;&#38598;&#20026;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#35757;&#32451;&#21019;&#36896;&#20102;&#19968;&#20010;&#29615;&#22659;&#12290;AI&#27169;&#22411;&#30340;&#20844;&#24320;&#20849;&#20139;&#21644;&#20840;&#29699;&#20351;&#29992;&#65292;&#22914;&#20309;&#25361;&#25112;&#33402;&#26415;&#23454;&#36341;&#20013;&#30340;&#29616;&#29366;&#65311;AI&#25216;&#26415;&#23558;&#32473;&#38899;&#20048;&#12289;&#33402;&#26415;&#21644;&#26032;&#23186;&#20307;&#24102;&#26469;&#20160;&#20040;&#26679;&#30340;&#21464;&#38761;&#65311;
&lt;/p&gt;
&lt;p&gt;
The explosion of content generated by Artificial Intelligence models has initiated a cultural shift in arts, music, and media, where roles are changing, values are shifting, and conventions are challenged. The readily available, vast dataset of the internet has created an environment for AI models to be trained on any content on the web. With AI models shared openly, and used by many, globally, how does this new paradigm shift challenge the status quo in artistic practices? What kind of changes will AI technology bring into music, arts, and new media?
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35843;&#26597;&#20102;&#35299;&#20154;&#20204;&#23545;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#30340;&#35748;&#30693;&#65292;&#21457;&#29616;&#21442;&#19982;&#32773;&#24847;&#35782;&#21040;&#35813;&#25216;&#26415;&#30340;&#39118;&#38505;&#21644;&#21361;&#38505;&#65292;&#20294;&#21482;&#26377;&#23569;&#25968;&#20154;&#35748;&#20026;&#36825;&#39033;&#25216;&#26415;&#23545;&#20010;&#20154;&#26500;&#25104;&#39118;&#38505;&#65292;&#32780;&#23545;&#20182;&#20154;&#30340;&#39118;&#38505;&#36739;&#26131;&#34987;&#35748;&#35782;&#21040;&#65292;&#29305;&#21035;&#26159;&#33402;&#26415;&#23478;&#34987;&#35270;&#20026;&#39118;&#38505;&#32676;&#20307;&#12290;</title><link>http://arxiv.org/abs/2306.08363</link><description>&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#30340;&#35748;&#30693;&#19982;&#29616;&#23454;
&lt;/p&gt;
&lt;p&gt;
Perceptions and Realities of Text-to-Image Generation. (arXiv:2306.08363v2 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08363
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35843;&#26597;&#20102;&#35299;&#20154;&#20204;&#23545;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#30340;&#35748;&#30693;&#65292;&#21457;&#29616;&#21442;&#19982;&#32773;&#24847;&#35782;&#21040;&#35813;&#25216;&#26415;&#30340;&#39118;&#38505;&#21644;&#21361;&#38505;&#65292;&#20294;&#21482;&#26377;&#23569;&#25968;&#20154;&#35748;&#20026;&#36825;&#39033;&#25216;&#26415;&#23545;&#20010;&#20154;&#26500;&#25104;&#39118;&#38505;&#65292;&#32780;&#23545;&#20182;&#20154;&#30340;&#39118;&#38505;&#36739;&#26131;&#34987;&#35748;&#35782;&#21040;&#65292;&#29305;&#21035;&#26159;&#33402;&#26415;&#23478;&#34987;&#35270;&#20026;&#39118;&#38505;&#32676;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#26159;&#19968;&#31181;&#24191;&#27867;&#27969;&#34892;&#30340;&#25216;&#26415;&#65292;&#23558;&#28145;&#21051;&#24433;&#21709;&#31038;&#20250;&#21644;&#20010;&#20154;&#12290;&#19981;&#21040;&#21313;&#24180;&#21069;&#65292;&#20154;&#20204;&#35748;&#20026;&#21019;&#36896;&#24615;&#24037;&#20316;&#23558;&#26159;&#26368;&#21518;&#33258;&#21160;&#21270;&#30340;&#39046;&#22495;&#20043;&#19968;&#65292;&#28982;&#32780;&#22914;&#20170;&#65292;&#25105;&#20204;&#30475;&#21040;&#20154;&#24037;&#26234;&#33021;&#27491;&#22312;&#20405;&#21344;&#35768;&#22810;&#21019;&#36896;&#24615;&#39046;&#22495;&#12290;&#26412;&#25991;&#36890;&#36807;&#19968;&#39033;&#35843;&#26597;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#20154;&#20204;&#23545;&#20110;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#30340;&#35748;&#30693;&#12290;&#25105;&#20204;&#28041;&#21450;&#21442;&#19982;&#32773;&#23545;&#36825;&#19968;&#26032;&#20852;&#25216;&#26415;&#30340;&#25216;&#26415;&#29702;&#35299;&#65292;&#20182;&#20204;&#30340;&#25285;&#24551;&#21644;&#39038;&#34385;&#65292;&#20197;&#21450;&#23545;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#23545;&#20010;&#20154;&#21644;&#31038;&#20250;&#30340;&#39118;&#38505;&#21644;&#21361;&#38505;&#30340;&#30475;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649;&#21442;&#19982;&#32773;&#24847;&#35782;&#21040;&#19982;&#36825;&#39033;&#25216;&#26415;&#30456;&#20851;&#30340;&#39118;&#38505;&#21644;&#21361;&#38505;&#65292;&#20294;&#21482;&#26377;&#23569;&#25968;&#21442;&#19982;&#32773;&#35748;&#20026;&#36825;&#39033;&#25216;&#26415;&#23545;&#20010;&#20154;&#26500;&#25104;&#20102;&#39118;&#38505;&#12290;&#23545;&#20854;&#20182;&#20154;&#30340;&#39118;&#38505;&#26356;&#23481;&#26131;&#34987;&#21442;&#19982;&#32773;&#35748;&#35782;&#21040;&#12290;&#33402;&#26415;&#23478;&#23588;&#20854;&#34987;&#35270;&#20026;&#39118;&#38505;&#32676;&#20307;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#37027;&#20123;&#23581;&#35797;&#36807;&#36825;&#39033;&#25216;&#26415;&#30340;&#21442;&#19982;&#32773;&#23545;&#20854;&#26410;&#26469;&#37325;&#35201;&#24615;&#30340;&#35780;&#20215;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative artificial intelligence (AI) is a widely popular technology that will have a profound impact on society and individuals. Less than a decade ago, it was thought that creative work would be among the last to be automated - yet today, we see AI encroaching on many creative domains. In this paper, we present the findings of a survey study on people's perceptions of text-to-image generation. We touch on participants' technical understanding of the emerging technology, their fears and concerns, and thoughts about risks and dangers of text-to-image generation to the individual and society. We find that while participants were aware of the risks and dangers associated with the technology, only few participants considered the technology to be a personal risk. The risks for others were more easy to recognize for participants. Artists were particularly seen at risk. Interestingly, participants who had tried the technology rated its future importance lower than those who had not tried i
&lt;/p&gt;</description></item><item><title>&#31471;&#21040;&#31471;&#39550;&#39542;&#27169;&#22411;&#23384;&#22312;&#20559;&#35265;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#27425;&#35201;&#32452;&#20214;&#30340;&#26356;&#25913;&#12290;&#26412;&#25991;&#38024;&#23545;&#30446;&#21069;&#22810;&#25968;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20013;&#25152;&#23384;&#22312;&#30340;&#20004;&#31181;&#20559;&#35265;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;&#21512;&#29702;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#24320;&#21457;&#20102;TF ++&#65292;&#22312;CARLA&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2306.07957</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#39550;&#39542;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Hidden Biases of End-to-End Driving Models. (arXiv:2306.07957v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07957
&lt;/p&gt;
&lt;p&gt;
&#31471;&#21040;&#31471;&#39550;&#39542;&#27169;&#22411;&#23384;&#22312;&#20559;&#35265;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#27425;&#35201;&#32452;&#20214;&#30340;&#26356;&#25913;&#12290;&#26412;&#25991;&#38024;&#23545;&#30446;&#21069;&#22810;&#25968;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20013;&#25152;&#23384;&#22312;&#30340;&#20004;&#31181;&#20559;&#35265;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;&#21512;&#29702;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#24320;&#21457;&#20102;TF ++&#65292;&#22312;CARLA&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#31471;&#21040;&#31471;&#30340;&#39550;&#39542;&#31995;&#32479;&#22312;CARLA&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#24555;&#36895;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#22312;&#20027;&#35201;&#36129;&#29486;&#30340;&#22522;&#30784;&#19978;&#65292;&#36825;&#20123;&#31995;&#32479;&#20063;&#20250;&#24341;&#20837;&#23545;&#27425;&#35201;&#31995;&#32479;&#32452;&#20214;&#30340;&#25913;&#21464;&#12290;&#22240;&#27492;&#65292;&#31995;&#32479;&#30340;&#25913;&#36827;&#28304;&#24182;&#19981;&#28165;&#26970;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#20960;&#20046;&#25152;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20013;&#37117;&#23384;&#22312;&#20004;&#31181;&#20559;&#35265;&#65292;&#36825;&#20123;&#20559;&#35265;&#23545;&#20110;&#22312;CARLA&#19978;&#35266;&#23519;&#21040;&#30340;&#36827;&#23637;&#33267;&#20851;&#37325;&#35201;&#65306;(1) &#36890;&#36807;&#23545;&#30446;&#26631;&#28857;&#36319;&#38543;&#30340;&#24378;&#24402;&#32435;&#20559;&#35265;&#26469;&#36827;&#34892;&#27178;&#21521;&#24674;&#22797;&#65292;(2) &#36890;&#36807;&#22810;&#27169;&#24577;&#33322;&#36335;&#28857;&#39044;&#27979;&#30340;&#32437;&#21521;&#24179;&#22343;&#26469;&#20943;&#36895;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20123;&#20559;&#35265;&#30340;&#32570;&#28857;&#65292;&#24182;&#30830;&#23450;&#20102;&#21512;&#29702;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#36890;&#36807;&#32467;&#21512;&#25105;&#20204;&#30340;&#35265;&#35299;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;TF ++&#65292;&#19968;&#31181;&#31616;&#21333;&#30340;&#31471;&#21040;&#31471;&#26041;&#27861;&#65292;&#22312;Longest6&#21644;LAV&#22522;&#20934;&#27979;&#35797;&#20013;&#25490;&#21517;&#31532;&#19968;&#65292;&#22312;Longest6&#19978;&#27604;&#26368;&#20339;&#21069;&#26399;&#24037;&#20316;&#25552;&#39640;&#20102;14&#20010;&#39550;&#39542;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
End-to-end driving systems have recently made rapid progress, in particular on CARLA. Independent of their major contribution, they introduce changes to minor system components. Consequently, the source of improvements is unclear. We identify two biases that recur in nearly all state-of-the-art methods and are critical for the observed progress on CARLA: (1) lateral recovery via a strong inductive bias towards target point following, and (2) longitudinal averaging of multimodal waypoint predictions for slowing down. We investigate the drawbacks of these biases and identify principled alternatives. By incorporating our insights, we develop TF++, a simple end-to-end method that ranks first on the Longest6 and LAV benchmarks, gaining 14 driving score over the best prior work on Longest6.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20855;&#26377;&#31867;&#20154;&#30452;&#35273;&#34892;&#20026;&#21644;&#35748;&#30693;&#38169;&#35823;&#30340;&#29305;&#28857;&#65292;&#32780;&#39640;&#32423;&#35821;&#35328;&#27169;&#22411;&#21017;&#36890;&#36807;&#23398;&#20064;&#36991;&#20813;&#36825;&#31867;&#38169;&#35823;&#24182;&#34920;&#29616;&#20986;&#36229;&#29702;&#24615;&#30340;&#26041;&#24335;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#20351;&#29992;&#24515;&#29702;&#23398;&#30740;&#31350;&#30340;&#26041;&#27861;&#25506;&#27979;LLMs&#65292;&#21487;&#20197;&#25581;&#31034;&#20854;&#26032;&#29983;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.07622</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20013;&#20986;&#29616;&#30340;&#31867;&#20154;&#30452;&#35273;&#34892;&#20026;&#21644;&#25512;&#29702;&#20559;&#24046;&#8212;&#8212;&#20197;&#21450;&#22312;GPT-4&#20013;&#28040;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human-Like Intuitive Behavior and Reasoning Biases Emerged in Language Models -- and Disappeared in GPT-4. (arXiv:2306.07622v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07622
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20855;&#26377;&#31867;&#20154;&#30452;&#35273;&#34892;&#20026;&#21644;&#35748;&#30693;&#38169;&#35823;&#30340;&#29305;&#28857;&#65292;&#32780;&#39640;&#32423;&#35821;&#35328;&#27169;&#22411;&#21017;&#36890;&#36807;&#23398;&#20064;&#36991;&#20813;&#36825;&#31867;&#38169;&#35823;&#24182;&#34920;&#29616;&#20986;&#36229;&#29702;&#24615;&#30340;&#26041;&#24335;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#20351;&#29992;&#24515;&#29702;&#23398;&#30740;&#31350;&#30340;&#26041;&#27861;&#25506;&#27979;LLMs&#65292;&#21487;&#20197;&#25581;&#31034;&#20854;&#26032;&#29983;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30446;&#21069;&#22788;&#20110;&#23558;AI&#31995;&#32479;&#19982;&#20154;&#31867;&#20132;&#27969;&#21644;&#26085;&#24120;&#29983;&#27963;&#20132;&#32455;&#22312;&#19968;&#36215;&#30340;&#21069;&#27839;&#12290;&#22240;&#27492;&#65292;&#35780;&#20272;&#23427;&#20204;&#30340;&#26032;&#20852;&#33021;&#21147;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;LLM&#65288;&#23588;&#20854;&#26159;GPT-3&#65289;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#31867;&#20154;&#30452;&#35273;&#34892;&#20026;&#65292;&#20197;&#21450;&#36981;&#24490;&#36825;&#31181;&#34892;&#20026;&#32780;&#26469;&#30340;&#35748;&#30693;&#38169;&#35823;&#12290;&#28982;&#32780;&#65292;&#20855;&#26377;&#26356;&#39640;&#35748;&#30693;&#33021;&#21147;&#30340;LLM&#65292;&#29305;&#21035;&#26159;ChatGPT&#21644;GPT-4&#65292;&#23398;&#20250;&#20102;&#36991;&#20813;&#23624;&#26381;&#20110;&#36825;&#20123;&#38169;&#35823;&#24182;&#34920;&#29616;&#20986;&#36229;&#29702;&#24615;&#30340;&#26041;&#24335;&#12290;&#23545;&#20110;&#25105;&#20204;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;Cognitive Reflection Test&#65288;CRT&#65289;&#21450;&#29992;&#20110;&#30740;&#31350;&#20154;&#31867;&#30452;&#35273;&#20915;&#31574;&#30340;&#35821;&#20041;&#24187;&#35273;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#31350;&#20102;&#31867;&#20154;&#30452;&#35273;&#20915;&#31574;&#30340;&#31283;&#23450;&#20542;&#21521;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#24515;&#29702;&#23398;&#26041;&#27861;&#35843;&#26597;LLM&#26377;&#28508;&#21147;&#25581;&#31034;&#21542;&#21017;&#26410;&#30693;&#30340;&#26032;&#29983;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are currently at the forefront of intertwining AI systems with human communication and everyday life. Therefore, it is of great importance to evaluate their emerging abilities. In this study, we show that LLMs, most notably GPT-3, exhibit behavior that strikingly resembles human-like intuition -- and the cognitive errors that come with it. However, LLMs with higher cognitive capabilities, in particular ChatGPT and GPT-4, learned to avoid succumbing to these errors and perform in a hyperrational manner. For our experiments, we probe LLMs with the Cognitive Reflection Test (CRT) as well as semantic illusions that were originally designed to investigate intuitive decision-making in humans. Moreover, we probe how sturdy the inclination for intuitive-like decision-making is. Our study demonstrates that investigating LLMs with methods from psychology has the potential to reveal otherwise unknown emergent traits.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;$DaL$&#30340;&#22522;&#20110;&#20998;&#21106;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#39640;&#24230;&#37197;&#32622;&#30340;&#36719;&#20214;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.06651</link><description>&lt;p&gt;
&#20351;&#29992;&#20998;&#21106;&#23398;&#20064;&#39044;&#27979;&#36719;&#20214;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Predicting Software Performance with Divide-and-Learn. (arXiv:2306.06651v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06651
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;$DaL$&#30340;&#22522;&#20110;&#20998;&#21106;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#39640;&#24230;&#37197;&#32622;&#30340;&#36719;&#20214;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#39640;&#24230;&#37197;&#32622;&#30340;&#36719;&#20214;&#31995;&#32479;&#30340;&#24615;&#33021;&#26159;&#24615;&#33021;&#27979;&#35797;&#21644;&#36136;&#37327;&#20445;&#35777;&#30340;&#22522;&#30784;&#12290;&#20026;&#27492;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#20381;&#38752;&#26426;&#22120;/&#28145;&#24230;&#23398;&#20064;&#26469;&#24314;&#27169;&#36719;&#20214;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#33267;&#20851;&#37325;&#35201;&#20294;&#26410;&#35299;&#20915;&#30340;&#25361;&#25112;&#26159;&#22914;&#20309;&#28385;&#36275;&#37197;&#32622;&#26223;&#35266;&#20013;&#32487;&#25215;&#30340;&#31232;&#30095;&#24615;&#65306;&#37197;&#32622;&#36873;&#39033;&#65288;&#29305;&#24449;&#65289;&#30340;&#24433;&#21709;&#21644;&#25968;&#25454;&#26679;&#26412;&#30340;&#20998;&#24067;&#37117;&#38750;&#24120;&#31232;&#30095;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#8220;&#20998;&#21106;&#23398;&#20064;&#8221;&#27010;&#24565;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;$DaL$&#12290;&#22522;&#26412;&#24605;&#24819;&#26159;&#65292;&#20026;&#20102;&#22788;&#29702;&#26679;&#26412;&#31232;&#30095;&#24615;&#65292;&#25105;&#20204;&#23558;&#37197;&#32622;&#26223;&#35266;&#20013;&#30340;&#26679;&#26412;&#21010;&#20998;&#20026;&#36828;&#31163;&#30340;&#37096;&#20998;&#65292;&#23545;&#20110;&#27599;&#20010;&#37096;&#20998;&#65292;&#25105;&#20204;&#24314;&#31435;&#19968;&#20010;&#35268;&#33539;&#21270;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#26412;&#22320;&#27169;&#22411;&#26469;&#22788;&#29702;&#29305;&#24449;&#31232;&#30095;&#24615;&#12290;&#28982;&#21518;&#65292;&#26032;&#32473;&#23450;&#30340;&#37197;&#32622;&#23558;&#34987;&#20998;&#37197;&#32473;&#26368;&#32456;&#39044;&#27979;&#30340;&#27491;&#30830;&#27169;&#22411;&#12290;&#20843;&#20010;&#30495;&#23454;&#31995;&#32479;&#21644;&#20116;&#32452;&#35757;&#32451;&#25968;&#25454;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;
&lt;/p&gt;
&lt;p&gt;
Predicting the performance of highly configurable software systems is the foundation for performance testing and quality assurance. To that end, recent work has been relying on machine/deep learning to model software performance. However, a crucial yet unaddressed challenge is how to cater for the sparsity inherited from the configuration landscape: the influence of configuration options (features) and the distribution of data samples are highly sparse.  In this paper, we propose an approach based on the concept of 'divide-and-learn', dubbed $DaL$. The basic idea is that, to handle sample sparsity, we divide the samples from the configuration landscape into distant divisions, for each of which we build a regularized Deep Neural Network as the local model to deal with the feature sparsity. A newly given configuration would then be assigned to the right model of division for the final prediction.  Experiment results from eight real-world systems and five sets of training data reveal that
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#36830;&#32493;&#26102;&#38388;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#30740;&#31350;&#65292;&#21457;&#29616;&#23545;&#25239;&#24615;&#35757;&#32451;&#20013;&#30340;&#25193;&#25955;&#39033;&#20915;&#23450;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#27867;&#21270;&#33021;&#21147;&#65292;&#36827;&#32780;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;AT&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2306.02618</link><description>&lt;p&gt;
&#25552;&#39640;&#25193;&#25955;&#20197;&#25913;&#21892;&#40065;&#26834;&#24615;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Enhance Diffusion to Improve Robust Generalization. (arXiv:2306.02618v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02618
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#36830;&#32493;&#26102;&#38388;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#30740;&#31350;&#65292;&#21457;&#29616;&#23545;&#25239;&#24615;&#35757;&#32451;&#20013;&#30340;&#25193;&#25955;&#39033;&#20915;&#23450;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#27867;&#21270;&#33021;&#21147;&#65292;&#36827;&#32780;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;AT&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23481;&#26131;&#21463;&#21040;&#20154;&#31867;&#38590;&#20197;&#23519;&#35273;&#30340;&#23545;&#25239;&#24615;&#25200;&#21160;&#30340;&#24433;&#21709;&#12290;&#20854;&#20013;&#19968;&#31181;&#26368;&#24378;&#30340;&#38450;&#24481;&#26426;&#21046;&#26159;&#23545;&#25239;&#24615;&#35757;&#32451;&#65288;AT&#65289;&#12290;&#26412;&#25991;&#26088;&#22312;&#35299;&#20915;AT&#20013;&#30340;&#20004;&#20010;&#20027;&#35201;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#22312;AT&#30740;&#31350;&#20013;&#22914;&#20309;&#35774;&#32622;&#20855;&#26377;&#24615;&#33021;&#20445;&#35777;&#30340;&#36229;&#21442;&#25968;&#20173;&#28982;&#23384;&#22312;&#20105;&#35758;&#65292;&#23450;&#21046;&#21270;&#35774;&#32622;&#22952;&#30861;&#19981;&#21516;&#27169;&#22411;&#35774;&#35745;&#22312;AT&#30740;&#31350;&#20013;&#30340;&#20844;&#24179;&#27604;&#36739;&#12290;&#20854;&#27425;&#65292;&#32463;&#36807;&#40065;&#26834;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;&#27867;&#21270;&#26102;&#38754;&#20020;&#22256;&#38590;&#65292;&#24182;&#19988;&#21463;&#21040;&#20005;&#37325;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#30340;&#22256;&#25200;&#12290;&#26412;&#25991;&#32858;&#28966;&#20110;&#20027;&#35201;&#30340;AT&#26694;&#26550; - &#25237;&#24433;&#26799;&#24230;&#19979;&#38477;&#23545;&#25239;&#24615;&#35757;&#32451;&#65288;PGD-AT&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#36830;&#32493;&#26102;&#38388;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;SDE&#65289;&#36817;&#20284;PGD-AT&#30340;&#21160;&#24577;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;SDE&#30340;&#25193;&#25955;&#39033;&#20915;&#23450;&#20102;&#40065;&#26834;&#27867;&#21270;&#12290;&#35813;&#29702;&#35770;&#21457;&#29616;&#30340;&#19968;&#20010;&#30452;&#25509;&#25512;&#35770;&#26159;&#65292;&#40065;&#26834;&#27867;&#21270;&#19982;&#23398;&#20064;&#29575;&#21644;&#25209;&#27425;&#22823;&#23567;&#20043;&#27604;&#21576;&#27491;&#30456;&#20851;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks are susceptible to human imperceptible adversarial perturbations. One of the strongest defense mechanisms is \emph{Adversarial Training} (AT). In this paper, we aim to address two predominant problems in AT. First, there is still little consensus on how to set hyperparameters with a performance guarantee for AT research, and customized settings impede a fair comparison between different model designs in AT research. Second, the robustly trained neural networks struggle to generalize well and suffer from tremendous overfitting. This paper focuses on the primary AT framework - Projected Gradient Descent Adversarial Training (PGD-AT). We approximate the dynamic of PGD-AT by a continuous-time Stochastic Differential Equation (SDE), and show that the diffusion term of this SDE determines the robust generalization. An immediate implication of this theoretical finding is that robust generalization is positively correlated with the ratio between learning rate and batch siz
&lt;/p&gt;</description></item><item><title>InGram&#26159;&#19968;&#31181;&#26032;&#30340;&#24402;&#32435;&#24335;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#25512;&#29702;&#26102;&#29983;&#25104;&#26032;&#20851;&#31995;&#21644;&#23454;&#20307;&#30340;&#23884;&#20837;&#65292;&#24182;&#20351;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#27719;&#24635;&#37051;&#23621;&#23884;&#20837;&#29983;&#25104;&#20851;&#31995;&#21644;&#23454;&#20307;&#23884;&#20837;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20934;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.19987</link><description>&lt;p&gt;
InGram&#65306;&#36890;&#36807;&#20851;&#31995;&#22270;&#36827;&#34892;&#24402;&#32435;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
InGram: Inductive Knowledge Graph Embedding via Relation Graphs. (arXiv:2305.19987v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19987
&lt;/p&gt;
&lt;p&gt;
InGram&#26159;&#19968;&#31181;&#26032;&#30340;&#24402;&#32435;&#24335;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#25512;&#29702;&#26102;&#29983;&#25104;&#26032;&#20851;&#31995;&#21644;&#23454;&#20307;&#30340;&#23884;&#20837;&#65292;&#24182;&#20351;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#27719;&#24635;&#37051;&#23621;&#23884;&#20837;&#29983;&#25104;&#20851;&#31995;&#21644;&#23454;&#20307;&#23884;&#20837;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24402;&#32435;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#34987;&#35270;&#20026;&#39044;&#27979;&#35757;&#32451;&#26399;&#38388;&#26410;&#35266;&#23519;&#21040;&#30340;&#26032;&#23454;&#20307;&#20043;&#38388;&#30340;&#32570;&#22833;&#19977;&#20803;&#32452;&#30340;&#20219;&#21153;&#12290;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26041;&#27861;InGram&#65292;&#23427;&#21487;&#20197;&#22312;&#25512;&#29702;&#26102;&#29983;&#25104;&#26032;&#20851;&#31995;&#21644;&#23454;&#20307;&#30340;&#23884;&#20837;&#65292;&#24182;&#22522;&#20110;&#20851;&#31995;&#22270;&#21644;&#21407;&#22987;&#30693;&#35782;&#22270;&#35889;&#20351;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#27719;&#24635;&#37051;&#23621;&#23884;&#20837;&#20197;&#29983;&#25104;&#20851;&#31995;&#21644;&#23454;&#20307;&#23884;&#20837;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;InGram&#30340;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inductive knowledge graph completion has been considered as the task of predicting missing triplets between new entities that are not observed during training. While most inductive knowledge graph completion methods assume that all entities can be new, they do not allow new relations to appear at inference time. This restriction prohibits the existing methods from appropriately handling real-world knowledge graphs where new entities accompany new relations. In this paper, we propose an INductive knowledge GRAph eMbedding method, InGram, that can generate embeddings of new relations as well as new entities at inference time. Given a knowledge graph, we define a relation graph as a weighted graph consisting of relations and the affinity weights between them. Based on the relation graph and the original knowledge graph, InGram learns how to aggregate neighboring embeddings to generate relation and entity embeddings using an attention mechanism. Experimental results show that InGram outper
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#20171;&#32461;&#20102;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#25552;&#20986;&#20102;&#21028;&#21035;&#24335;LLMs&#21644;&#29983;&#25104;&#24335;LLMs&#20004;&#31181;&#27169;&#22411;&#33539;&#24335;&#65292;&#24635;&#32467;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24378;&#35843;&#20102;&#35813;&#39046;&#22495;&#30340;&#25361;&#25112;&#21644;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2305.19860</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#33616;&#31995;&#32479;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Large Language Models for Recommendation. (arXiv:2305.19860v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19860
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#20171;&#32461;&#20102;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#25552;&#20986;&#20102;&#21028;&#21035;&#24335;LLMs&#21644;&#29983;&#25104;&#24335;LLMs&#20004;&#31181;&#27169;&#22411;&#33539;&#24335;&#65292;&#24635;&#32467;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24378;&#35843;&#20102;&#35813;&#39046;&#22495;&#30340;&#25361;&#25112;&#21644;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#24182;&#22312;&#25512;&#33616;&#31995;&#32479;&#39046;&#22495;&#24341;&#36215;&#20102;&#37325;&#35270;&#12290;&#36825;&#20123;&#27169;&#22411;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#28023;&#37327;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24050;&#22312;&#23398;&#20064;&#36890;&#29992;&#34920;&#31034;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#30528;&#25104;&#21151;&#65292;&#24182;&#26377;&#21487;&#33021;&#36890;&#36807;&#19968;&#20123;&#26377;&#25928;&#30340;&#36716;&#31227;&#25216;&#26415;&#65288;&#22914;&#24494;&#35843;&#21644;&#25552;&#31034;&#35843;&#25972;&#65289;&#31561;&#25163;&#27573;&#25552;&#39640;&#25512;&#33616;&#31995;&#32479;&#30340;&#21508;&#20010;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#25512;&#33616;&#36136;&#37327;&#30340;&#20851;&#38190;&#26159;&#21033;&#29992;&#23427;&#20204;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#29305;&#24449;&#34920;&#31034;&#21644;&#22823;&#37327;&#30340;&#22806;&#37096;&#30693;&#35782;&#35206;&#30422;&#65292;&#24314;&#31435;&#39033;&#30446;&#21644;&#29992;&#25143;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#20026;&#20102;&#20840;&#38754;&#20102;&#35299;&#29616;&#26377;&#22522;&#20110;LLM&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#26412;&#32508;&#36848;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#31867;&#27861;&#65292;&#23558;&#36825;&#20123;&#27169;&#22411;&#20998;&#20026;&#20004;&#31181;&#20027;&#35201;&#33539;&#24335;&#65292;&#20998;&#21035;&#26159;&#21028;&#21035;&#24335;LLMs&#21644;&#29983;&#25104;&#24335;LLMs&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#36825;&#20123;&#33539;&#24335;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#24378;&#35843;&#20102;&#36825;&#20010;&#26032;&#20852;&#39046;&#22495;&#30340;&#25361;&#25112;&#21644;&#24320;&#25918;&#24615;&#30740;&#31350;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have emerged as powerful tools in the field of Natural Language Processing (NLP) and have recently gained significant attention in the domain of Recommendation Systems (RS). These models, trained on massive amounts of data using self-supervised learning, have demonstrated remarkable success in learning universal representations and have the potential to enhance various aspects of recommendation systems by some effective transfer techniques such as fine-tuning and prompt tuning, and so on. The crucial aspect of harnessing the power of language models in enhancing recommendation quality is the utilization of their high-quality representations of textual features and their extensive coverage of external knowledge to establish correlations between items and users. To provide a comprehensive understanding of the existing LLM-based recommendation systems, this survey presents a taxonomy that categorizes these models into two major paradigms, respectively Discrimi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;HyNT&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#36229;&#20851;&#31995;&#22411;&#30693;&#35782;&#22270;&#30340;&#34920;&#31034;&#65292;&#21253;&#25324;&#25968;&#20540;&#25991;&#23383;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#19978;&#19979;&#25991;Transformer&#21644;&#39044;&#27979;Transformer&#65292;&#36890;&#36807;&#23398;&#20064;&#19977;&#20803;&#32452;&#21644;&#20854;&#38480;&#23450;&#35789;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#20197;&#21450;&#25968;&#20540;&#20449;&#24687;&#26469;&#33719;&#24471;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.18256</link><description>&lt;p&gt;
&#29992;Transformer&#23398;&#20064;&#36229;&#20851;&#31995;&#22411;&#21644;&#25968;&#20540;&#30693;&#35782;&#22270;&#20013;&#30340;&#34920;&#24449;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Representation Learning on Hyper-Relational and Numeric Knowledge Graphs with Transformers. (arXiv:2305.18256v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18256
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;HyNT&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#36229;&#20851;&#31995;&#22411;&#30693;&#35782;&#22270;&#30340;&#34920;&#31034;&#65292;&#21253;&#25324;&#25968;&#20540;&#25991;&#23383;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#19978;&#19979;&#25991;Transformer&#21644;&#39044;&#27979;Transformer&#65292;&#36890;&#36807;&#23398;&#20064;&#19977;&#20803;&#32452;&#21644;&#20854;&#38480;&#23450;&#35789;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#20197;&#21450;&#25968;&#20540;&#20449;&#24687;&#26469;&#33719;&#24471;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30740;&#31350;&#20102;&#19968;&#20010;&#36229;&#20851;&#31995;&#22411;&#30693;&#35782;&#22270;&#35889;&#65292;&#20854;&#20013;&#19977;&#20803;&#32452;&#19982;&#38480;&#23450;&#35789;&#38598;&#21512;&#30456;&#20851;&#32852;; &#19968;&#20010;&#38480;&#23450;&#35789;&#30001;&#20851;&#31995;&#21644;&#23454;&#20307;&#32452;&#25104;&#65292;&#20026;&#19977;&#20803;&#32452;&#25552;&#20379;&#36741;&#21161;&#20449;&#24687;&#12290;&#29616;&#26377;&#30340;&#36229;&#20851;&#31995;&#22411;&#30693;&#35782;&#22270;&#23884;&#20837;&#26041;&#27861;&#20551;&#23450;&#23454;&#20307;&#26159;&#31163;&#25955;&#23545;&#35937;&#65292;&#20294;&#26377;&#20123;&#20449;&#24687;&#24212;&#20351;&#29992;&#25968;&#20540;&#34920;&#31034;&#65292;&#20363;&#22914;(J.R.R.&#65292;&#20986;&#29983;&#20110;&#65292;1892)&#12290;&#21516;&#26102;&#65292;&#19977;&#20803;&#32452;(J.R.R.&#65292;&#23601;&#35835;&#20110;&#65292;&#29275;&#27941;&#22823;&#23398;)&#21487;&#20197;&#19982;&#38480;&#23450;&#35789;(&#24320;&#22987;&#26102;&#38388;&#65292;1911)&#30456;&#20851;&#32852;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;HyNT&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#21253;&#21547;&#19977;&#20803;&#32452;&#25110;&#38480;&#23450;&#35789;&#20013;&#25968;&#20540;&#25991;&#23383;&#30340;&#36229;&#20851;&#31995;&#22411;&#30693;&#35782;&#22270;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#19978;&#19979;&#25991;Transformer&#21644;&#19968;&#20010;&#39044;&#27979;Transformer&#65292;&#26469;&#23398;&#20064;&#34920;&#31034;&#65292;&#19981;&#20165;&#22522;&#20110;&#19977;&#20803;&#32452;&#21644;&#20854;&#38480;&#23450;&#35789;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#36824;&#22522;&#20110;&#25968;&#20540;&#20449;&#24687;&#12290;&#36890;&#36807;&#23398;&#20064;&#19977;&#20803;&#32452;&#21644;&#38480;&#23450;&#35789;&#30340;&#32039;&#20945;&#34920;&#31034;&#65292;&#24182;&#23558;&#23427;&#20204;&#39304;&#36865;&#32473;Transformer&#26469;&#33719;&#24471;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A hyper-relational knowledge graph has been recently studied where a triplet is associated with a set of qualifiers; a qualifier is composed of a relation and an entity, providing auxiliary information for a triplet. While existing hyper-relational knowledge graph embedding methods assume that the entities are discrete objects, some information should be represented using numeric values, e.g., (J.R.R., was born in, 1892). Also, a triplet (J.R.R., educated at, Oxford Univ.) can be associated with a qualifier such as (start time, 1911). In this paper, we propose a unified framework named HyNT that learns representations of a hyper-relational knowledge graph containing numeric literals in either triplets or qualifiers. We define a context transformer and a prediction transformer to learn the representations based not only on the correlations between a triplet and its qualifiers but also on the numeric information. By learning compact representations of triplets and qualifiers and feeding 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;YOLOv8&#31639;&#27861;&#36827;&#34892;&#24515;&#24459;&#22833;&#24120;&#26816;&#27979;&#30340;&#26032;&#24212;&#29992;&#31243;&#24207;&#65292;&#20854;&#27169;&#22411;&#33021;&#22815;&#23454;&#29616;&#25345;&#32493;&#30417;&#27979;&#65292;&#24182;&#20197;&#39640;&#20934;&#30830;&#24615;&#36827;&#34892;&#23454;&#26102;&#24515;&#24459;&#22833;&#24120;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2305.16727</link><description>&lt;p&gt;
YOLOv8&#23454;&#26102;&#24515;&#24459;&#22833;&#24120;&#26816;&#27979;&#30340;&#26032;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
A novel application for real-time arrhythmia detection using YOLOv8. (arXiv:2305.16727v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16727
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;YOLOv8&#31639;&#27861;&#36827;&#34892;&#24515;&#24459;&#22833;&#24120;&#26816;&#27979;&#30340;&#26032;&#24212;&#29992;&#31243;&#24207;&#65292;&#20854;&#27169;&#22411;&#33021;&#22815;&#23454;&#29616;&#25345;&#32493;&#30417;&#27979;&#65292;&#24182;&#20197;&#39640;&#20934;&#30830;&#24615;&#36827;&#34892;&#23454;&#26102;&#24515;&#24459;&#22833;&#24120;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#38477;&#20302;&#36828;&#31243;&#24515;&#34880;&#31649;&#20581;&#24247;&#30417;&#25252;&#30340;&#21307;&#30103;&#36153;&#29992;&#38656;&#27714;&#36234;&#26469;&#36234;&#39640;&#12290;&#26816;&#27979;&#21644;&#20998;&#31867;&#24515;&#33039;&#24515;&#24459;&#22833;&#24120;&#23545;&#20110;&#35786;&#26029;&#24515;&#33039;&#24322;&#24120;&#24739;&#32773;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24212;&#29992;&#31243;&#24207;&#65292;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;You-Only-Look-Once&#65288;YOLO&#65289;v8&#31639;&#27861;&#23545;&#21333;&#23548;&#32852;&#24515;&#30005;&#22270;&#20449;&#21495;&#36827;&#34892;&#20998;&#31867;&#65292;&#36827;&#34892;&#24515;&#24459;&#22833;&#24120;&#26816;&#27979;&#12290;&#36890;&#36807;&#23545;MIT-BIH&#25968;&#25454;&#38598;&#36827;&#34892;&#24494;&#35843;&#21518;&#65292;&#24314;&#31435;&#36215;&#19968;&#20010;&#23450;&#21046;&#30340;YOLOv8&#27169;&#22411;&#65292;&#33021;&#22815;&#23454;&#26102;&#26816;&#27979;&#24515;&#24459;&#22833;&#24120;&#65292;&#20197;&#23454;&#29616;&#25345;&#32493;&#30417;&#27979;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;NVIDIA Tesla V100&#19978;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#20197;0.002&#31186;&#30340;&#26816;&#27979;&#26102;&#38388;&#21644;0.961&#30340;mAP@50&#26816;&#27979;&#24515;&#36339;&#12290;&#30740;&#31350;&#35777;&#26126;&#20102;&#23454;&#26102;&#24515;&#24459;&#22833;&#24120;&#26816;&#27979;&#30340;&#28508;&#21147;&#65292;&#27169;&#22411;&#36755;&#20986;&#21487;&#20197;&#34987;&#35270;&#35273;&#35299;&#37322;&#65292;&#36866;&#29992;&#20110;&#23478;&#24237;&#29992;&#25143;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#21487;&#20197;&#24310;&#20280;&#21040;&#24320;&#21457;&#23454;&#26102;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#24212;&#29992;&#20110;&#24515;&#34880;&#31649;&#20581;&#24247;&#30417;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, there has been an increasing need to reduce healthcare costs in remote monitoring of cardiovascular health. Detecting and classifying cardiac arrhythmia is critical to diagnosing patients with cardiac abnormalities. This paper shows that complex systems such as electrocardiograms (ECG) can be applicable for at-home monitoring. This paper proposes a novel application for arrhythmia detection using the state-of-the-art You-Only-Look-Once (YOLO)v8 algorithm to classify single-lead ECG signals. A custom YOLOv8 model was fine-tuned on the MIT-BIH dataset to detect arrhythmia in real-time to allow continuous monitoring. Results show that our model can detect heartbeats with a mAP@50 of 0.961 with a detection time of 0.002s on an NVIDIA Tesla V100. Our study demonstrated the potential of real-time arrhythmia detection, where the model output can be visually interpreted for at-home users. Furthermore, this study could be extended into a real-time XAI model, deployed in the hea
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#23545;&#20110;&#22797;&#26434;&#25351;&#20196;&#36981;&#24490;&#20195;&#29702;&#30340;&#23398;&#20064;&#65292;&#35821;&#35328;&#22870;&#21169;&#22609;&#36896;&#25216;&#26415;&#21487;&#33021;&#20250;&#24433;&#21709;&#20195;&#29702;&#31243;&#24207;&#30340;&#23398;&#20064;&#65292;&#20854;&#20013;&#34920;&#38754;&#19978;&#30340;&#25104;&#21151;&#21487;&#33021;&#26159;&#33030;&#24369;&#30340;&#12290;</title><link>http://arxiv.org/abs/2305.16621</link><description>&lt;p&gt;
&#35821;&#35328;&#22870;&#21169;&#22609;&#36896;&#21487;&#33021;&#24433;&#21709;&#25351;&#20196;&#36981;&#24490;&#20195;&#29702;&#30340;&#23398;&#20064;&#65306;&#25552;&#37266;&#20854;&#33030;&#24369;&#24615;
&lt;/p&gt;
&lt;p&gt;
A Reminder of its Brittleness: Language Reward Shaping May Hinder Learning for Instruction Following Agents. (arXiv:2305.16621v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16621
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#23545;&#20110;&#22797;&#26434;&#25351;&#20196;&#36981;&#24490;&#20195;&#29702;&#30340;&#23398;&#20064;&#65292;&#35821;&#35328;&#22870;&#21169;&#22609;&#36896;&#25216;&#26415;&#21487;&#33021;&#20250;&#24433;&#21709;&#20195;&#29702;&#31243;&#24207;&#30340;&#23398;&#20064;&#65292;&#20854;&#20013;&#34920;&#38754;&#19978;&#30340;&#25104;&#21151;&#21487;&#33021;&#26159;&#33030;&#24369;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25945;&#23548;&#20195;&#29702;&#31243;&#24207;&#36981;&#23432;&#22797;&#26434;&#30340;&#20070;&#38754;&#25351;&#20196;&#19968;&#30452;&#26159;&#19968;&#20010;&#37325;&#35201;&#32780;&#21448;&#38590;&#20197;&#23454;&#29616;&#30340;&#30446;&#26631;&#12290;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#30340;&#19968;&#31181;&#25216;&#26415;&#26159;&#35821;&#35328;&#22870;&#21169;&#22609;&#36896;&#65288;LRS&#65289;&#65292;&#23427;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#29992;&#20110;&#22870;&#21169;&#20195;&#34920;&#26397;&#30528;&#31232;&#30095;&#22870;&#21169;&#30340;&#36827;&#23637;&#26041;&#21521;&#30340;&#34892;&#21160;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;LRS&#30340;&#34920;&#38754;&#25104;&#21151;&#26159;&#33030;&#24369;&#30340;&#65292;&#24182;&#19988;&#20043;&#21069;&#30340;&#31215;&#26497;&#32467;&#26524;&#21487;&#33021;&#24402;&#22240;&#20110;&#24369;&#30340;&#24378;&#21270;&#23398;&#20064;&#22522;&#32447;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#22870;&#21169;&#37096;&#20998;&#21305;&#37197;&#36712;&#36857;&#30340;&#27425;&#20248;LRS&#35774;&#35745;&#65292;&#24182;&#22522;&#20110;&#25918;&#23485;&#20219;&#21153;&#32422;&#26463;&#30340;&#27010;&#24565;&#65292;&#23545;&#19968;&#31181;&#26032;&#22411;&#30340;&#22870;&#21169;&#25200;&#21160;&#36827;&#34892;&#20102;&#34920;&#24449;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#21644;&#23454;&#35777;&#35777;&#25454;&#34920;&#26126;&#65292;&#20351;&#29992;LRS&#22870;&#21169;&#35757;&#32451;&#30340;&#20195;&#29702;&#31243;&#24207;&#36739;&#32431;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#31243;&#24207;&#25910;&#25947;&#36895;&#24230;&#36739;&#24930;&#12290;
&lt;/p&gt;
&lt;p&gt;
Teaching agents to follow complex written instructions has been an important yet elusive goal. One technique for improving learning efficiency is language reward shaping (LRS), which is used in reinforcement learning (RL) to reward actions that represent progress towards a sparse reward. We argue that the apparent success of LRS is brittle, and prior positive findings can be attributed to weak RL baselines. Specifically, we identified suboptimal LRS designs that reward partially matched trajectories, and we characterised a novel type of reward perturbation that addresses this issue based on the concept of loosening task constraints. We provided theoretical and empirical evidence that agents trained using LRS rewards converge more slowly compared to pure RL agents.
&lt;/p&gt;</description></item><item><title>KeyPosS&#26159;&#19968;&#31181;&#38754;&#37096;&#26631;&#35760;&#26816;&#27979;&#26694;&#26550;&#65292;&#37319;&#29992;&#30495;&#23454;&#36317;&#31163;&#22810;&#36793;&#23450;&#20301;&#31639;&#27861;&#23454;&#29616;&#24555;&#36895;&#32780;&#20934;&#30830;&#30340;&#26816;&#27979;&#65292;&#36991;&#20813;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#30340;&#35745;&#31639;&#36127;&#25285;&#21644;&#37327;&#21270;&#35823;&#24046;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.16437</link><description>&lt;p&gt;
KeyPosS: &#22522;&#20110; GPS &#28789;&#24863;&#30340;&#30495;&#23454;&#36317;&#31163;&#22810;&#36793;&#23450;&#20301;&#30340;&#21363;&#25554;&#21363;&#29992;&#38754;&#37096;&#26631;&#35760;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
KeyPosS: Plug-and-Play Facial Landmark Detection through GPS-Inspired True-Range Multilateration. (arXiv:2305.16437v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16437
&lt;/p&gt;
&lt;p&gt;
KeyPosS&#26159;&#19968;&#31181;&#38754;&#37096;&#26631;&#35760;&#26816;&#27979;&#26694;&#26550;&#65292;&#37319;&#29992;&#30495;&#23454;&#36317;&#31163;&#22810;&#36793;&#23450;&#20301;&#31639;&#27861;&#23454;&#29616;&#24555;&#36895;&#32780;&#20934;&#30830;&#30340;&#26816;&#27979;&#65292;&#36991;&#20813;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#30340;&#35745;&#31639;&#36127;&#25285;&#21644;&#37327;&#21270;&#35823;&#24046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38754;&#37096;&#20998;&#26512;&#39046;&#22495;&#65292;&#20934;&#30830;&#30340;&#26631;&#35760;&#26816;&#27979;&#23545;&#20110;&#21508;&#31181;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#65292;&#21253;&#25324;&#20154;&#33080;&#35782;&#21035;&#21644;&#34920;&#24773;&#20998;&#26512;&#31561;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#28909;&#21147;&#22270;&#25110;&#22352;&#26631;&#22238;&#24402;&#25216;&#26415;&#32463;&#24120;&#38754;&#20020;&#35745;&#31639;&#36127;&#25285;&#21644;&#37327;&#21270;&#35823;&#24046;&#31561;&#25361;&#25112;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; KeyPoint Positioning System&#65288;KeyPosS&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31361;&#30772;&#24615;&#30340;&#38754;&#37096;&#26631;&#35760;&#26816;&#27979;&#26694;&#26550;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#12290;KeyPosS&#39318;&#27425;&#37319;&#29992;&#30495;&#23454;&#36317;&#31163;&#22810;&#36793;&#23450;&#20301;&#31639;&#27861;&#65292;&#19968;&#31181;&#26368;&#21021;&#29992;&#20110;GPS&#31995;&#32479;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#19981;&#20381;&#36182;&#20110;&#35745;&#31639;&#23494;&#38598;&#22411;&#22238;&#24402;&#26041;&#27861;&#23454;&#29616;&#24555;&#36895;&#32780;&#20934;&#30830;&#30340;&#38754;&#37096;&#26631;&#35760;&#26816;&#27979;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#23436;&#20840;&#21367;&#31215;&#32593;&#32476;&#39044;&#27979;&#36317;&#31163;&#22270;&#65292;&#35745;&#31639;&#24863;&#20852;&#36259;&#28857;&#65288;POI&#65289;&#19982;&#22810;&#20010;&#38170;&#28857;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#36890;&#36807;&#24039;&#22937;&#22320;&#21033;&#29992;&#36825;&#20123;&#38170;&#28857;&#26469;&#19977;&#35282;&#27979;&#37327;POI&#30340;&#20301;&#32622;&#65292;&#23454;&#29616;&#38754;&#37096;&#26631;&#35760;&#30340;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the realm of facial analysis, accurate landmark detection is crucial for various applications, ranging from face recognition and expression analysis to animation. Conventional heatmap or coordinate regression-based techniques, however, often face challenges in terms of computational burden and quantization errors. To address these issues, we present the KeyPoint Positioning System (KeyPosS), a groundbreaking facial landmark detection framework that stands out from existing methods. For the first time, KeyPosS employs the True-range Multilateration algorithm, a technique originally used in GPS systems, to achieve rapid and precise facial landmark detection without relying on computationally intensive regression approaches. The framework utilizes a fully convolutional network to predict a distance map, which computes the distance between a Point of Interest (POI) and multiple anchor points. These anchor points are ingeniously harnessed to triangulate the POI's position through the Tru
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#20960;&#20010;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#21475;&#35821;&#29702;&#35299;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#26368;&#22823;&#27169;&#22411;&#21487;&#20197;&#22312;&#38646;-shot&#23398;&#20064;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#36798;&#21040;&#19982;&#30417;&#30563;&#27169;&#22411;&#30456;&#36817;&#30340;&#24847;&#22270;&#20998;&#31867;&#20934;&#30830;&#24230;&#65292;&#20294;&#22312;&#27133;&#22635;&#20805;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#65292;&#19988;&#23545;ASR&#38169;&#35823;&#25935;&#24863;&#12290;</title><link>http://arxiv.org/abs/2305.13512</link><description>&lt;p&gt;
&#33021;ChatGPT&#26816;&#27979;&#20986;&#24847;&#22270;&#21527;&#65311;&#35780;&#20272;&#29992;&#20110;&#21475;&#35821;&#29702;&#35299;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can ChatGPT Detect Intent? Evaluating Large Language Models for Spoken Language Understanding. (arXiv:2305.13512v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13512
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#20960;&#20010;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#21475;&#35821;&#29702;&#35299;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#26368;&#22823;&#27169;&#22411;&#21487;&#20197;&#22312;&#38646;-shot&#23398;&#20064;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#36798;&#21040;&#19982;&#30417;&#30563;&#27169;&#22411;&#30456;&#36817;&#30340;&#24847;&#22270;&#20998;&#31867;&#20934;&#30830;&#24230;&#65292;&#20294;&#22312;&#27133;&#22635;&#20805;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#65292;&#19988;&#23545;ASR&#38169;&#35823;&#25935;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#65292;&#29305;&#21035;&#20307;&#29616;&#22312;&#36890;&#36807;&#25552;&#31034;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#38646;-shot&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#12290;&#20026;&#20102;&#35780;&#20272;&#23427;&#20204;&#23545;&#21475;&#35821;&#29702;&#35299;&#65288;SLU&#65289;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#20960;&#20010;&#19981;&#21516;&#22823;&#23567;&#30340;ChatGPT&#21644;OPT&#27169;&#22411;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;&#26368;&#22823;&#27169;&#22411;&#29305;&#26377;&#30340;&#26032;&#20852;&#33021;&#21147;&#65292;&#21363;&#22312;&#32473;&#23450;Oracle&#36716;&#24405;&#30340;&#21508;&#31181;&#35821;&#35328;&#19978;&#65292;&#20854;&#21487;&#20197;&#25509;&#36817;&#20110;&#30417;&#30563;&#27169;&#22411;&#30340;&#24847;&#22270;&#20998;&#31867;&#20934;&#30830;&#24230;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#36866;&#21512;&#21333;&#20010;GPU&#30340;&#36739;&#23567;&#27169;&#22411;&#30340;&#32467;&#26524;&#36828;&#36828;&#33853;&#21518;&#12290;&#25105;&#20204;&#27880;&#24847;&#21040;&#38169;&#35823;&#26696;&#20363;&#36890;&#24120;&#26469;&#33258;&#25968;&#25454;&#38598;&#30340;&#27880;&#37322;&#26041;&#26696;&#65307;ChatGPT&#30340;&#21709;&#24212;&#20173;&#28982;&#26159;&#21512;&#29702;&#30340;&#12290;&#20294;&#26159;&#25105;&#20204;&#21457;&#29616;&#65292;&#35813;&#27169;&#22411;&#22312;&#27133;&#22635;&#20805;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#65292;&#32780;&#19988;&#23545;ASR&#38169;&#35823;&#38750;&#24120;&#25935;&#24863;&#65292;&#22240;&#27492;&#34920;&#26126;&#20102;&#23558;&#36825;&#20123;&#25991;&#26412;&#27169;&#22411;&#24212;&#29992;&#20110;&#21475;&#35821;&#29702;&#35299;&#30340;&#20005;&#23803;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, large pretrained language models have demonstrated strong language understanding capabilities. This is particularly reflected in their zero-shot and in-context learning abilities on downstream tasks through prompting. To assess their impact on spoken language understanding (SLU), we evaluate several such models like ChatGPT and OPT of different sizes on multiple benchmarks. We verify the emergent ability unique to the largest models as they can reach intent classification accuracy close to that of supervised models with zero or few shots on various languages given oracle transcripts. By contrast, the results for smaller models fitting a single GPU fall far behind. We note that the error cases often arise from the annotation scheme of the dataset; responses from ChatGPT are still reasonable. We show, however, that the model is worse at slot filling, and its performance is sensitive to ASR errors, suggesting serious challenges for the application of those textual models on SLU.
&lt;/p&gt;</description></item><item><title>&#20020;&#24202;&#39558;&#39548;&#26159;&#19968;&#31181;&#22522;&#20110;&#23545;&#35805;&#30340;&#30693;&#35782;&#32534;&#30721;&#30340;&#24320;&#28304;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#26377;&#24456;&#39640;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#20020;&#24202;&#30456;&#20851;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.12031</link><description>&lt;p&gt;
&#20020;&#24202;&#39558;&#39548;&#65306;&#19968;&#31181;&#20855;&#26377;&#22522;&#20110;&#23545;&#35805;&#30340;&#30693;&#35782;&#32534;&#30721;&#30340;&#24320;&#28304;&#19987;&#23478;&#32423;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Clinical Camel: An Open-Source Expert-Level Medical Language Model with Dialogue-Based Knowledge Encoding. (arXiv:2305.12031v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12031
&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#39558;&#39548;&#26159;&#19968;&#31181;&#22522;&#20110;&#23545;&#35805;&#30340;&#30693;&#35782;&#32534;&#30721;&#30340;&#24320;&#28304;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#26377;&#24456;&#39640;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#20020;&#24202;&#30456;&#20851;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21307;&#30103;&#39046;&#22495;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#25968;&#25454;&#38544;&#31169;&#12289;&#30417;&#31649;&#21512;&#35268;&#24615;&#21644;&#27169;&#22411;&#31283;&#23450;&#24615;&#31561;&#38382;&#39064;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#23545;&#35805;&#30340;&#30693;&#35782;&#32534;&#30721;&#65288;DBKE&#65289;&#12290;DBKE&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#38544;&#24335;&#30693;&#35782;&#24211;&#65292;&#20351;&#20854;&#20855;&#26377;&#26356;&#24378;&#30340;&#23545;&#35805;&#33021;&#21147;&#65292;&#20026;&#21518;&#32493;&#29992;&#20363;&#25552;&#20379;&#20102;&#36719;&#23545;&#40784;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Clinical Camel&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#12289;&#19987;&#27880;&#20110;&#21307;&#30103;&#20445;&#20581;&#30340;&#20250;&#35805;&#27169;&#22411;&#65292;&#26469;&#23637;&#31034;DBKE&#30340;&#26377;&#25928;&#24615;&#12290;Clinical Camel&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#39640;&#27700;&#24179;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#20020;&#24202;&#30456;&#20851;&#24615;&#12290;&#23427;&#36824;&#20026;&#21307;&#30103;&#24212;&#29992;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#20449;&#36182;&#30340;&#12289;&#24320;&#25918;&#28304;&#20195;&#30721;&#30340;&#26367;&#20195;&#21697;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) present immense potential in the medical field, yet concerns over data privacy, regulatory compliance, and model stability restrict their widespread adoption. Although the distillation of high-performing closed-source LLMs has proven effective for general tasks, their application in healthcare is limited due to reduced domain knowledge and remnants of alignment behavior hindering clinical tasks. To address these challenges, we propose Dialogue-Based Knowledge Encoding (DBKE). DBKE enhances models' implicit knowledge base and primes them for conversational recall, augmenting their conversational capabilities and enabling a soft alignment for subsequent use cases. By transforming dense academic source text into synthetic dialogue, DBKE broadens the model's knowledge base and enables a soft alignment that guides downstream behaviours. We present Clinical Camel, an open-source, healthcare-focused conversational model, to showcase the effectiveness of DBKE. Clin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#33258;&#21160;&#36873;&#25321;&#21307;&#23398;&#20307;&#24449;&#21644;&#22238;&#24402;&#25554;&#20540;&#30340;&#26041;&#27861;&#65288;MedLens&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#30005;&#23376;&#30149;&#21382;&#20013;&#21307;&#23398;&#20307;&#24449;&#25968;&#25454;&#32570;&#22833;&#29575;&#36807;&#39640;&#23548;&#33268;&#39044;&#27979;&#24615;&#33021;&#38477;&#20302;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.11742</link><description>&lt;p&gt;
MedLens: &#36890;&#36807;&#36873;&#25321;&#21307;&#23398;&#20307;&#24449;&#21644;&#22238;&#24402;&#25554;&#20540;&#26469;&#25552;&#39640;&#27515;&#20129;&#29575;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
MedLens: Improve mortality prediction via medical signs selecting and regression interpolation. (arXiv:2305.11742v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11742
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#33258;&#21160;&#36873;&#25321;&#21307;&#23398;&#20307;&#24449;&#21644;&#22238;&#24402;&#25554;&#20540;&#30340;&#26041;&#27861;&#65288;MedLens&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#30005;&#23376;&#30149;&#21382;&#20013;&#21307;&#23398;&#20307;&#24449;&#25968;&#25454;&#32570;&#22833;&#29575;&#36807;&#39640;&#23548;&#33268;&#39044;&#27979;&#24615;&#33021;&#38477;&#20302;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30417;&#27979;&#24739;&#32773;&#30340;&#20581;&#24247;&#29366;&#20917;&#24182;&#25552;&#21069;&#39044;&#27979;&#27515;&#20129;&#29575;&#23545;&#21450;&#26102;&#25552;&#20379;&#24739;&#32773;&#25252;&#29702;&#21644;&#27835;&#30103;&#33267;&#20851;&#37325;&#35201;&#12290;&#30005;&#23376;&#30149;&#21382;&#20013;&#30340;&#22823;&#37327;&#21307;&#23398;&#20307;&#24449;&#34987;&#29992;&#20110;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#36827;&#34892;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#21407;&#22987;&#20020;&#24202;&#20307;&#24449;&#30340;&#25968;&#25454;&#36136;&#37327;&#38382;&#39064;&#22312;&#25991;&#29486;&#20013;&#34987;&#36739;&#23569;&#35752;&#35770;&#12290;&#36890;&#36807;&#23545;&#21508;&#31181;&#21307;&#23398;&#20307;&#24449;&#21644;&#22823;&#37327;&#24739;&#32773;&#20303;&#38498;&#35760;&#24405;&#20013;&#30340;&#32570;&#22833;&#29575;&#21644;&#30456;&#20851;&#20998;&#25968;&#36827;&#34892;&#28145;&#20837;&#27979;&#37327;&#65292;&#25105;&#20204;&#21457;&#29616;&#32508;&#21512;&#32570;&#22833;&#29575;&#38750;&#24120;&#39640;&#65292;&#22823;&#37327;&#26080;&#29992;&#30340;&#20307;&#24449;&#21487;&#33021;&#20250;&#25439;&#23475;&#39044;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#21482;&#26377;&#25913;&#21892;&#25968;&#25454;&#36136;&#37327;&#25165;&#33021;&#25552;&#39640;&#19981;&#21516;&#39044;&#27979;&#31639;&#27861;&#30340;&#22522;&#32447;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;MedLens&#65292;&#36890;&#36807;&#32479;&#35745;&#33258;&#21160;&#36873;&#25321;&#37325;&#35201;&#21307;&#23398;&#20307;&#24449;&#65292;&#24182;&#20351;&#29992;&#28789;&#27963;&#30340;&#25554;&#20540;&#26041;&#27861;&#22788;&#29702;&#39640;&#32570;&#22833;&#29575;&#26102;&#38388;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
Monitoring the health status of patients and predicting mortality in advance is vital for providing patients with timely care and treatment. Massive medical signs in electronic health records (EHR) are fitted into advanced machine learning models to make predictions. However, the data-quality problem of original clinical signs is less discussed in the literature. Based on an in-depth measurement of the missing rate and correlation score across various medical signs and a large amount of patient hospital admission records, we discovered the comprehensive missing rate is extremely high, and a large number of useless signs could hurt the performance of prediction models. Then we concluded that only improving data-quality could improve the baseline accuracy of different prediction algorithms. We designed MEDLENS, with an automatic vital medical signs selection approach via statistics and a flexible interpolation approach for high missing rate time series. After augmenting the data-quality 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#20915;&#31574;&#26641;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#32852;&#21512;&#20248;&#21270;&#25152;&#26377;&#26641;&#30340;&#21442;&#25968;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#36138;&#24515;&#31639;&#27861;&#36896;&#25104;&#27425;&#20248;&#35299;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#20108;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#24182;&#22312;&#22810;&#31867;&#20219;&#21153;&#20013;&#36798;&#21040;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.03515</link><description>&lt;p&gt;
&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#20915;&#31574;&#26641;
&lt;/p&gt;
&lt;p&gt;
Learning Decision Trees with Gradient Descent. (arXiv:2305.03515v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03515
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#20915;&#31574;&#26641;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#32852;&#21512;&#20248;&#21270;&#25152;&#26377;&#26641;&#30340;&#21442;&#25968;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#36138;&#24515;&#31639;&#27861;&#36896;&#25104;&#27425;&#20248;&#35299;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#20108;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#24182;&#22312;&#22810;&#31867;&#20219;&#21153;&#20013;&#36798;&#21040;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#26641;&#26159;&#29992;&#20110;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#30340;&#24120;&#35265;&#24037;&#20855;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#39640;&#24230;&#30340;&#35299;&#37322;&#24615;&#12290;&#28982;&#32780;&#65292;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#20915;&#31574;&#26641;&#26159;&#19968;&#20010;&#22256;&#38590;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#26159;&#38750;&#20984;&#21644;&#38750;&#21487;&#24494;&#30340;&#12290;&#22240;&#27492;&#65292;&#36890;&#24120;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#19968;&#31181;&#36138;&#23146;&#29983;&#38271;&#31639;&#27861;&#26469;&#23398;&#20064;&#20915;&#31574;&#26641;&#65292;&#22312;&#27599;&#20010;&#20869;&#37096;&#33410;&#28857;&#19978;&#23616;&#37096;&#26368;&#23567;&#21270;&#19981;&#32431;&#24230;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#31181;&#36138;&#24515;&#36807;&#31243;&#21487;&#33021;&#20250;&#23548;&#33268;&#27425;&#20248;&#30340;&#20915;&#31574;&#26641;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#38590;&#20197;&#22788;&#29702;&#30340;&#36724;&#23545;&#40784;&#20915;&#31574;&#26641;&#30340;&#26032;&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#29992;&#21453;&#21521;&#20256;&#25773;&#21644;&#30452;&#36890;&#31639;&#23376;&#22312;&#23494;&#38598;&#30340;&#20915;&#31574;&#26641;&#34920;&#31034;&#19978;&#32852;&#21512;&#20248;&#21270;&#25152;&#26377;&#26641;&#30340;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20108;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#22312;&#22810;&#31867;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decision Trees (DTs) are commonly used for many machine learning tasks due to their high degree of interpretability. However, learning a DT from data is a difficult optimization problem, as it is non-convex and non-differentiable. Therefore, common approaches learn DTs using a greedy growth algorithm that minimizes the impurity locally at each internal node. Unfortunately, this greedy procedure can lead to suboptimal trees. In this paper, we present a novel approach for learning hard, axis-aligned DTs with gradient descent. The proposed method uses backpropagation with a straight-through operator on a dense DT representation to jointly optimize all tree parameters. Our approach outperforms existing methods on binary classification benchmarks and achieves competitive results for multi-class tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;Platt&#32553;&#25918;&#21450;&#20854;&#26657;&#20934;&#26041;&#27861;&#65292;&#20854;&#29702;&#35770;&#22522;&#30784;&#24378;&#22823;&#65292;&#21487;&#20197;&#22788;&#29702;&#20998;&#24067;&#28418;&#31227;&#21644;&#23545;&#25239;&#24615;&#32467;&#26524;&#24207;&#21015;&#65292;&#26080;&#38656;&#36229;&#21442;&#25968;&#35843;&#25972;&#65292;&#22312;&#19968;&#31995;&#21015;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.00070</link><description>&lt;p&gt;
&#22312;&#32447;Platt&#32553;&#25918;&#21450;&#20854;&#26657;&#20934;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Online Platt Scaling with Calibeating. (arXiv:2305.00070v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00070
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;Platt&#32553;&#25918;&#21450;&#20854;&#26657;&#20934;&#26041;&#27861;&#65292;&#20854;&#29702;&#35770;&#22522;&#30784;&#24378;&#22823;&#65292;&#21487;&#20197;&#22788;&#29702;&#20998;&#24067;&#28418;&#31227;&#21644;&#23545;&#25239;&#24615;&#32467;&#26524;&#24207;&#21015;&#65292;&#26080;&#38656;&#36229;&#21442;&#25968;&#35843;&#25972;&#65292;&#22312;&#19968;&#31995;&#21015;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#21518;&#26657;&#20934;&#26041;&#27861;&#65292;&#31216;&#20026;&#22312;&#32447;Platt&#32553;&#25918;(OPS)&#65292;&#23427;&#23558;Platt&#32553;&#25918;&#25216;&#26415;&#19982;&#22312;&#32447;&#36923;&#36753;&#22238;&#24402;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;OPS&#22914;&#20309;&#22312;&#20998;&#24067;&#28418;&#31227;&#30340;i.i.d.&#21644;&#38750;i.i.d.&#24773;&#20917;&#19979;&#24179;&#31283;&#36866;&#24212;&#12290;&#27492;&#22806;&#65292;&#24403;&#26368;&#20339;&#30340;Platt&#32553;&#25918;&#27169;&#22411;&#26412;&#36523;&#34987;&#38169;&#35823;&#26657;&#20934;&#26102;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#26368;&#36817;&#24320;&#21457;&#30340;&#31216;&#20026;calibeating&#30340;&#25216;&#26415;&#26469;&#22686;&#24378;OPS&#65292;&#20351;&#20854;&#26356;&#21152;&#40065;&#26834;&#12290;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#24471;&#21040;&#30340;OPS+calibeating&#26041;&#27861;&#23545;&#20110;&#23545;&#25239;&#24615;&#32467;&#26524;&#24207;&#21015;&#26159;&#20445;&#35777;&#26657;&#20934;&#30340;&#12290;&#22312;&#23454;&#39564;&#19978;&#65292;&#23427;&#22312;&#19968;&#31995;&#21015;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#22343;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#26080;&#38656;&#36229;&#21442;&#25968;&#35843;&#25972;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#25152;&#26377;OPS&#24605;&#24819;&#25193;&#23637;&#21040;beta&#32553;&#25918;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an online post-hoc calibration method, called Online Platt Scaling (OPS), which combines the Platt scaling technique with online logistic regression. We demonstrate that OPS smoothly adapts between i.i.d. and non-i.i.d. settings with distribution drift. Further, in scenarios where the best Platt scaling model is itself miscalibrated, we enhance OPS by incorporating a recently developed technique called calibeating to make it more robust. Theoretically, our resulting OPS+calibeating method is guaranteed to be calibrated for adversarial outcome sequences. Empirically, it is effective on a range of synthetic and real-world datasets, with and without distribution drifts, achieving superior performance without hyperparameter tuning. Finally, we extend all OPS ideas to the beta scaling method.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;CRN&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;&#30456;&#26426;&#38647;&#36798;&#34701;&#21512;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#22270;&#20687;&#35270;&#22270;&#29305;&#24449;&#36716;&#25442;&#20026;&#40479;&#30640;&#29305;&#24449;&#22270;&#21644;&#20351;&#29992;&#22810;&#27169;&#24577;&#21487;&#21464;&#24418;&#27880;&#24847;&#21147;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#12289;&#31283;&#20581;&#12289;&#39640;&#25928;&#30340;3D&#24863;&#30693;&#20219;&#21153;</title><link>http://arxiv.org/abs/2304.00670</link><description>&lt;p&gt;
CRN&#65306;&#29992;&#20110;&#20934;&#30830;&#12289;&#31283;&#20581;&#12289;&#39640;&#25928;&#30340;3D&#24863;&#30693;&#30340;&#30456;&#26426;&#38647;&#36798;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
CRN: Camera Radar Net for Accurate, Robust, Efficient 3D Perception. (arXiv:2304.00670v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00670
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;CRN&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;&#30456;&#26426;&#38647;&#36798;&#34701;&#21512;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#22270;&#20687;&#35270;&#22270;&#29305;&#24449;&#36716;&#25442;&#20026;&#40479;&#30640;&#29305;&#24449;&#22270;&#21644;&#20351;&#29992;&#22810;&#27169;&#24577;&#21487;&#21464;&#24418;&#27880;&#24847;&#21147;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#12289;&#31283;&#20581;&#12289;&#39640;&#25928;&#30340;3D&#24863;&#30693;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#38656;&#35201;&#19968;&#20010;&#20934;&#30830;&#24555;&#36895;&#30340;3D&#24863;&#30693;&#31995;&#32479;&#65292;&#21253;&#25324;3D&#29289;&#20307;&#26816;&#27979;&#12289;&#36319;&#36394;&#21644;&#20998;&#21106;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#20302;&#25104;&#26412;&#22522;&#20110;&#30456;&#26426;&#30340;&#26041;&#27861;&#26174;&#31034;&#20986;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#26159;&#23427;&#20204;&#23481;&#26131;&#21463;&#21040;&#31967;&#31957;&#30340;&#20809;&#29031;&#25110;&#24694;&#21155;&#30340;&#22825;&#27668;&#26465;&#20214;&#30340;&#24433;&#21709;&#65292;&#24182;&#19988;&#20855;&#26377;&#36739;&#22823;&#30340;&#23450;&#20301;&#35823;&#24046;&#12290;&#22240;&#27492;&#65292;&#23558;&#30456;&#26426;&#19982;&#20302;&#25104;&#26412;&#38647;&#36798;&#30456;&#32467;&#21512;&#65292;&#21518;&#32773;&#21487;&#20197;&#22312;&#25152;&#26377;&#29615;&#22659;&#20013;&#25552;&#20379;&#31934;&#30830;&#30340;&#36828;&#31243;&#27979;&#37327;&#24182;&#21487;&#38752;&#36816;&#34892;&#65292;&#26159;&#26377;&#24076;&#26395;&#30340;&#65292;&#20294;&#23578;&#26410;&#24471;&#21040;&#20840;&#38754;&#30340;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Camera Radar Net&#65288;CRN&#65289;&#30340;&#26032;&#39062;&#30340;&#30456;&#26426;&#38647;&#36798;&#34701;&#21512;&#26694;&#26550;&#65292;&#20026;&#21508;&#31181;&#20219;&#21153;&#29983;&#25104;&#19968;&#20010;&#35821;&#20041;&#20016;&#23500;&#12289;&#31354;&#38388;&#31934;&#30830;&#30340;&#40479;&#30640;&#29305;&#24449;&#22270;&#65288;BEV&#65289;&#12290;&#20026;&#20102;&#20811;&#26381;&#22270;&#20687;&#20013;&#32570;&#20047;&#31354;&#38388;&#20449;&#24687;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#31232;&#30095;&#20294;&#20934;&#30830;&#30340;&#38647;&#36798;&#28857;&#23558;&#36879;&#35270;&#35270;&#22270;&#22270;&#20687;&#29305;&#24449;&#36716;&#25442;&#20026;BEV&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20351;&#29992;&#22810;&#27169;&#24577;&#21487;&#21464;&#24418;&#27880;&#24847;&#21147;&#22312;BEV&#20013;&#32858;&#21512;&#22270;&#20687;&#21644;&#38647;&#36798;&#29305;&#24449;&#22270;&#65292;&#20197;&#35299;&#20915;&#31354;&#38388;&#23545;&#40784;&#38169;&#35823;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Autonomous driving requires an accurate and fast 3D perception system that includes 3D object detection, tracking, and segmentation. Although recent low-cost camera-based approaches have shown promising results, they are susceptible to poor illumination or bad weather conditions and have a large localization error. Hence, fusing camera with low-cost radar, which provides precise long-range measurement and operates reliably in all environments, is promising but has not yet been thoroughly investigated. In this paper, we propose Camera Radar Net (CRN), a novel camera-radar fusion framework that generates a semantically rich and spatially accurate bird's-eye-view (BEV) feature map for various tasks. To overcome the lack of spatial information in an image, we transform perspective view image features to BEV with the help of sparse but accurate radar points. We further aggregate image and radar feature maps in BEV using multi-modal deformable attention designed to tackle the spatial misalig
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#36828;&#31243;&#35780;&#20272;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#36816;&#21160;&#34920;&#29616;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#37325;&#22797;&#29992;&#20110;&#31867;&#20284;&#30340;&#36816;&#21160;&#20219;&#21153;&#65292;&#25317;&#26377;&#36739;&#39640;&#30340;&#21487;&#38752;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.17573</link><description>&lt;p&gt;
&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#22312;&#23478;&#20013;&#27979;&#37327;&#24085;&#37329;&#26862;&#30149;&#30340;&#20005;&#37325;&#31243;&#24230;
&lt;/p&gt;
&lt;p&gt;
Using AI to Measure Parkinson's Disease Severity at Home. (arXiv:2303.17573v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17573
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#36828;&#31243;&#35780;&#20272;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#36816;&#21160;&#34920;&#29616;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#37325;&#22797;&#29992;&#20110;&#31867;&#20284;&#30340;&#36816;&#21160;&#20219;&#21153;&#65292;&#25317;&#26377;&#36739;&#39640;&#30340;&#21487;&#38752;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#36828;&#31243;&#35780;&#20272;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#36816;&#21160;&#34920;&#29616;&#30340;&#26041;&#27861;&#12290;&#21442;&#19982;&#32773;&#22312;&#32593;&#32476;&#25668;&#20687;&#22836;&#21069;&#23436;&#25104;&#20102;&#36816;&#21160;&#20219;&#21153;&#65288;&#21363;&#28857;&#20987;&#25163;&#25351;&#65289;&#65292;250&#21517;&#20840;&#29699;&#21442;&#19982;&#32773;&#30340;&#25968;&#25454;&#25353;&#29031;&#36816;&#21160;&#38556;&#30861;&#21327;&#20250;&#32479;&#19968;&#24085;&#37329;&#26862;&#30149;&#35780;&#20998;&#37327;&#34920; (MDS-UPDRS) &#30340;&#26631;&#20934;&#30001;&#19977;&#21517;&#19987;&#23478;&#31070;&#32463;&#23398;&#23478;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#31070;&#32463;&#23398;&#23478;&#30340;&#35780;&#20272;&#20855;&#26377;&#39640;&#24230;&#30340;&#21487;&#38752;&#24615;&#65292;&#20869;&#37096;&#19968;&#33268;&#24615;&#31995;&#25968;&#65288;ICC&#65289;&#20026;0.88&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#35745;&#31639;&#26426;&#31639;&#27861;&#26469;&#33719;&#24471;&#19982;MDS-UPDRS&#25351;&#21335;&#19968;&#33268;&#19988;&#19982;&#31070;&#32463;&#23398;&#23478;&#30340;&#35780;&#20272;&#39640;&#24230;&#30456;&#20851;&#30340;&#23458;&#35266;&#27979;&#37327;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#36825;&#20123;&#25351;&#26631;&#30340;&#35757;&#32451;&#19979;&#34920;&#29616;&#20248;&#20110;&#19968;&#20010;MDS-UPDRS&#35748;&#35777;&#30340;&#35780;&#20998;&#32773;&#65292;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#65288;MAE&#65289;&#20026;0.59&#65292;&#32780;&#35780;&#20998;&#32773;&#30340;MAE&#20026;0.79&#12290;&#28982;&#32780;&#65292;&#35813;&#27169;&#22411;&#30340;&#34920;&#29616;&#30053;&#36874;&#20110;&#19987;&#23478;&#31070;&#32463;&#23398;&#23478;&#65288;0.53 MAE&#65289;&#12290;&#35813;&#26041;&#27861;&#21487;&#37325;&#22797;&#29992;&#20110;&#31867;&#20284;&#30340;&#36816;&#21160;&#20219;&#21153;&#65292;&#25552;&#20379;&#20102;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an artificial intelligence system to remotely assess the motor performance of individuals with Parkinson's disease (PD). Participants performed a motor task (i.e., tapping fingers) in front of a webcam, and data from 250 global participants were rated by three expert neurologists following the Movement Disorder Society Unified Parkinson's Disease Rating Scale (MDS-UPDRS). The neurologists' ratings were highly reliable, with an intra-class correlation coefficient (ICC) of 0.88. We developed computer algorithms to obtain objective measurements that align with the MDS-UPDRS guideline and are strongly correlated with the neurologists' ratings. Our machine learning model trained on these measures outperformed an MDS-UPDRS certified rater, with a mean absolute error (MAE) of 0.59 compared to the rater's MAE of 0.79. However, the model performed slightly worse than the expert neurologists (0.53 MAE). The methodology can be replicated for similar motor tasks, providing the possibili
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#39640;&#20302;&#39057;&#29575;&#20851;&#31995;&#30340;&#26080;&#20559;&#20506;&#30340;&#20840;&#26223;&#22330;&#26223;&#22270;&#29983;&#25104;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#38271;&#23614;&#38382;&#39064;&#21644;&#20027;&#20307;-&#23545;&#35937;&#23545;&#25317;&#26377;&#22810;&#20010;&#37325;&#21472;&#20851;&#31995;&#30340;&#38382;&#39064;&#12290;&#22312;&#24191;&#27867;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.15994</link><description>&lt;p&gt;
HiLo: &#21033;&#29992;&#39640;&#20302;&#39057;&#29575;&#20851;&#31995;&#36827;&#34892;&#26080;&#20559;&#20506;&#30340;&#20840;&#26223;&#22330;&#26223;&#22270;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
HiLo: Exploiting High Low Frequency Relations for Unbiased Panoptic Scene Graph Generation. (arXiv:2303.15994v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15994
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#39640;&#20302;&#39057;&#29575;&#20851;&#31995;&#30340;&#26080;&#20559;&#20506;&#30340;&#20840;&#26223;&#22330;&#26223;&#22270;&#29983;&#25104;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#38271;&#23614;&#38382;&#39064;&#21644;&#20027;&#20307;-&#23545;&#35937;&#23545;&#25317;&#26377;&#22810;&#20010;&#37325;&#21472;&#20851;&#31995;&#30340;&#38382;&#39064;&#12290;&#22312;&#24191;&#27867;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#26223;&#22330;&#26223;&#22270;&#29983;&#25104;&#65288;PSG&#65289;&#26159;&#19968;&#39033;&#26368;&#36817;&#25552;&#20986;&#30340;&#22270;&#20687;&#22330;&#26223;&#29702;&#35299;&#20219;&#21153;&#65292;&#26088;&#22312;&#23545;&#22270;&#20687;&#36827;&#34892;&#20998;&#21106;&#21644;&#25552;&#21462;&#20027;&#20307;&#12289;&#23545;&#35937;&#21450;&#20854;&#20851;&#31995;&#19977;&#20803;&#32452;&#20197;&#26500;&#24314;&#22330;&#26223;&#22270;&#12290;&#30001;&#20110;&#20851;&#31995;&#31867;&#21035;&#30340;&#38271;&#23614;&#38382;&#39064;&#65292;&#36825;&#39033;&#20219;&#21153;&#29305;&#21035;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#20351;&#24471;&#21333;&#32431;&#30340;&#26377;&#20559;&#20506;&#26041;&#27861;&#26356;&#20542;&#21521;&#20110;&#39640;&#39057;&#29575;&#20851;&#31995;&#12290;&#29616;&#26377;&#30340;&#26080;&#20559;&#20506;&#26041;&#27861;&#36890;&#36807;&#25968;&#25454;/&#25439;&#22833;&#37325;&#26032;&#24179;&#34913;&#20197;&#25903;&#25345;&#20302;&#39057;&#29575;&#20851;&#31995;&#26469;&#35299;&#20915;&#38271;&#23614;&#38382;&#39064;&#12290;&#20854;&#27425;&#65292;&#19968;&#20010;&#20027;&#20307;-&#23545;&#35937;&#23545;&#21487;&#20197;&#26377;&#20004;&#20010;&#25110;&#26356;&#22810;&#22312;&#35821;&#20041;&#19978;&#37325;&#21472;&#30340;&#20851;&#31995;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#26041;&#27861;&#20542;&#21521;&#20110;&#20351;&#29992;&#19968;&#20010;&#32780;&#19981;&#26159;&#21478;&#19968;&#20010;&#65292;&#20294;&#25105;&#20204;&#25552;&#20986;&#30340;HiLo&#26694;&#26550;&#35753;&#19981;&#21516;&#30340;&#32593;&#32476;&#20998;&#25903;&#19987;&#38376;&#22788;&#29702;&#20302;&#39057;&#21644;&#39640;&#39057;&#20851;&#31995;&#65292;&#24378;&#21046;&#23454;&#26045;&#23427;&#20204;&#30340;&#19968;&#33268;&#24615;&#24182;&#34701;&#21512;&#32467;&#26524;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#26159;&#39318;&#27425;&#25552;&#20986;&#26126;&#30830;&#26080;&#20559;PSG&#26041;&#27861;&#30340;&#20154;&#12290;&#22312;&#24191;&#27867;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;HiLo&#26694;&#26550;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Panoptic Scene Graph generation (PSG) is a recently proposed task in image scene understanding that aims to segment the image and extract triplets of subjects, objects and their relations to build a scene graph. This task is particularly challenging for two reasons. First, it suffers from a long-tail problem in its relation categories, making naive biased methods more inclined to high-frequency relations. Existing unbiased methods tackle the long-tail problem by data/loss rebalancing to favor low-frequency relations. Second, a subject-object pair can have two or more semantically overlapping relations. While existing methods favor one over the other, our proposed HiLo framework lets different network branches specialize on low and high frequency relations, enforce their consistency and fuse the results. To the best of our knowledge we are the first to propose an explicitly unbiased PSG method. In extensive experiments we show that our HiLo framework achieves state-of-the-art results on
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#24403;&#21069;&#36719;&#20214;&#24320;&#21457;&#34892;&#19994;&#37319;&#29992;&#29983;&#25104;&#24335; AI&#65288;GAI&#65289;&#21161;&#25163;&#36827;&#34892;&#36719;&#20214;&#24320;&#21457;&#30340;&#29616;&#29366;&#21644;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#26410;&#26469;&#36719;&#20214;&#24320;&#21457;&#25945;&#32946;&#30340;&#24895;&#26223;&#21644;&#25945;&#23398;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2303.13936</link><description>&lt;p&gt;
&#36719;&#20214;&#24320;&#21457;&#25945;&#32946;&#20013;&#30340;&#29983;&#25104;&#24335; AI &#21161;&#25163;
&lt;/p&gt;
&lt;p&gt;
Generative AI Assistants in Software Development Education. (arXiv:2303.13936v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13936
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#24403;&#21069;&#36719;&#20214;&#24320;&#21457;&#34892;&#19994;&#37319;&#29992;&#29983;&#25104;&#24335; AI&#65288;GAI&#65289;&#21161;&#25163;&#36827;&#34892;&#36719;&#20214;&#24320;&#21457;&#30340;&#29616;&#29366;&#21644;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#26410;&#26469;&#36719;&#20214;&#24320;&#21457;&#25945;&#32946;&#30340;&#24895;&#26223;&#21644;&#25945;&#23398;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36719;&#20214;&#24320;&#21457;&#34892;&#19994;&#27491;&#22312;&#36827;&#34892;&#19968;&#27425;&#28508;&#22312;&#30340;&#39072;&#35206;&#24615;&#30340;&#33539;&#24335;&#21464;&#38761;&#8212;&#8212;&#37319;&#29992;&#29983;&#25104;&#24335; AI&#65288;GAI&#65289;&#21161;&#25163;&#36827;&#34892;&#36719;&#20214;&#24320;&#21457;&#12290;&#34429;&#28982; AI &#24050;&#32463;&#22312;&#36719;&#20214;&#24037;&#31243;&#30340;&#21508;&#20010;&#39046;&#22495;&#20013;&#34987;&#20351;&#29992;&#65292;&#20294;&#26159;&#20687; GitHub Copilot &#21644; ChatGPT &#36825;&#26679;&#30340; GAI &#25216;&#26415;&#24050;&#32463;&#28608;&#21457;&#20102;&#35768;&#22810;&#20154;&#30340;&#24819;&#35937;&#21147;&#65288;&#21644;&#24656;&#24807;&#65289;&#12290;&#23613;&#31649;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#35813;&#34892;&#19994;&#23558;&#22914;&#20309;&#37319;&#29992;&#21644;&#36866;&#24212;&#36825;&#20123;&#25216;&#26415;&#65292;&#20294;&#24494;&#36719;&#65288;GitHub&#12289;&#24517;&#24212;&#65289;&#21644;&#35895;&#27468;&#65288;Bard&#65289;&#31561;&#22823;&#22411;&#36719;&#20214;&#20844;&#21496;&#23558;&#36825;&#20123;&#25216;&#26415;&#25972;&#21512;&#21040;&#26356;&#24191;&#27867;&#30340;&#34892;&#19994;&#20013;&#30340;&#20030;&#21160;&#26159;&#26126;&#30830;&#30340;&#24847;&#22270;&#21644;&#26041;&#21521;&#12290;&#25105;&#20204;&#19982;&#34892;&#19994;&#19987;&#19994;&#20154;&#22763;&#36827;&#34892;&#20102;&#25506;&#32034;&#24615;&#35775;&#35848;&#65292;&#20197;&#20102;&#35299;&#24403;&#21069;&#30340;&#23454;&#36341;&#21644;&#25361;&#25112;&#65292;&#23558;&#20854;&#32435;&#20837;&#25105;&#20204;&#23545;&#26410;&#26469;&#36719;&#20214;&#24320;&#21457;&#25945;&#32946;&#30340;&#24895;&#26223;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#25945;&#23398;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
The software development industry is amid another potentially disruptive paradigm change--adopting the use of generative AI (GAI) assistants for software development. Whilst AI is already used in various areas of software engineering, GAI technologies, such as GitHub Copilot and ChatGPT, have ignited the imaginations (and fears) of many people. Whilst it is unclear how the industry will adopt and adapt to these technologies, the move to integrate these technologies into the wider industry by large software companies, such as Microsoft (GitHub, Bing) and Google (Bard), is a clear indication of intent and direction. We performed exploratory interviews with industry professionals to understand current practices and challenges, which we incorporate into our vision of a future of software development education and make some pedagogical recommendations.
&lt;/p&gt;</description></item><item><title>ROBOSAC&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20849;&#35782;&#30340;&#21453;&#23545;&#25239;&#40065;&#26834;&#21327;&#21516;&#24863;&#30693;&#38450;&#24481;&#31574;&#30053;&#65292;&#20351;&#29992;&#38543;&#26426;&#23376;&#38598;&#30340;&#38431;&#21451;&#26469;&#23545;&#27604;&#21327;&#21516;&#24863;&#30693;&#21644;&#21333;&#20010;&#24863;&#30693;&#30340;&#32467;&#26524;&#65292;&#20197;&#25490;&#38500;&#28508;&#22312;&#25915;&#20987;&#32773;&#65292;&#24182;&#25512;&#23548;&#20986;&#30830;&#20445;&#33719;&#24471;&#25152;&#38656;&#26080;&#25915;&#20987;&#32773;&#23376;&#38598;&#25152;&#38656;&#30340;&#37319;&#26679;&#35797;&#39564;&#20010;&#25968;&#12290;</title><link>http://arxiv.org/abs/2303.09495</link><description>&lt;p&gt;
Among Us: &#22522;&#20110;&#20849;&#35782;&#30340;&#21453;&#23545;&#25239;&#40065;&#26834;&#21327;&#21516;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
Among Us: Adversarially Robust Collaborative Perception by Consensus. (arXiv:2303.09495v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09495
&lt;/p&gt;
&lt;p&gt;
ROBOSAC&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20849;&#35782;&#30340;&#21453;&#23545;&#25239;&#40065;&#26834;&#21327;&#21516;&#24863;&#30693;&#38450;&#24481;&#31574;&#30053;&#65292;&#20351;&#29992;&#38543;&#26426;&#23376;&#38598;&#30340;&#38431;&#21451;&#26469;&#23545;&#27604;&#21327;&#21516;&#24863;&#30693;&#21644;&#21333;&#20010;&#24863;&#30693;&#30340;&#32467;&#26524;&#65292;&#20197;&#25490;&#38500;&#28508;&#22312;&#25915;&#20987;&#32773;&#65292;&#24182;&#25512;&#23548;&#20986;&#30830;&#20445;&#33719;&#24471;&#25152;&#38656;&#26080;&#25915;&#20987;&#32773;&#23376;&#38598;&#25152;&#38656;&#30340;&#37319;&#26679;&#35797;&#39564;&#20010;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20010;&#26426;&#22120;&#20154;&#20043;&#38388;&#30340;&#21327;&#21516;&#24863;&#30693;&#33021;&#22815;&#27604;&#21333;&#20010;&#26426;&#22120;&#20154;&#26356;&#22909;&#22320;&#24863;&#30693;&#22330;&#26223;(&#20363;&#22914;&#65292;&#26816;&#27979;&#29289;&#20307;)&#65292;&#20294;&#22312;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26102;&#24456;&#23481;&#26131;&#21463;&#21040;&#25932;&#23545;&#25915;&#20987;&#12290;&#36825;&#19968;&#38382;&#39064;&#21487;&#36890;&#36807;&#23545;&#25239;&#24615;&#38450;&#24481;&#26469;&#35299;&#20915;&#65292;&#20294;&#35757;&#32451;&#38656;&#35201;&#20102;&#35299;&#25915;&#20987;&#26426;&#21046;&#65292;&#32780;&#36825;&#36890;&#24120;&#26159;&#26410;&#30693;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; ROBOSAC&#65292;&#19968;&#31181;&#22522;&#20110;&#37319;&#26679;&#30340;&#26032;&#22411;&#38450;&#24481;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#65292;&#33021;&#24212;&#23545;&#26410;&#30693;&#30340;&#25915;&#20987;&#32773;&#12290;&#25105;&#20204;&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#65292;&#21327;&#21516;&#24863;&#30693;&#24212;&#35813;&#27604;&#21333;&#20010;&#24863;&#30693;&#26356;&#33021;&#36798;&#25104;&#19968;&#33268;&#65292;&#32780;&#19981;&#24212;&#30456;&#20114;&#20135;&#29983;&#20998;&#27495;&#12290;&#36825;&#23548;&#33268;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20551;&#35828;&#21644;&#39564;&#35777;&#30340;&#26694;&#26550;&#65306;&#21033;&#29992;&#19968;&#32452;&#38543;&#26426;&#36873;&#25321;&#30340;&#38431;&#21451;&#65292;&#23545;&#21327;&#21516;&#24863;&#30693;&#19982;&#21333;&#20010;&#24863;&#30693;&#30340;&#32467;&#26524;&#36827;&#34892;&#27604;&#36739;&#65292;&#30452;&#21040;&#36798;&#25104;&#20849;&#35782;&#12290;&#22312;&#36825;&#26679;&#30340;&#26694;&#26550;&#19979;&#65292;&#26356;&#22810;&#30340;&#38431;&#21451;&#36890;&#24120;&#24847;&#21619;&#30528;&#26356;&#22909;&#30340;&#24863;&#30693;&#34920;&#29616;&#65292;&#20294;&#38656;&#35201;&#26356;&#38271;&#30340;&#37319;&#26679;&#26102;&#38388;&#26469;&#25490;&#38500;&#28508;&#22312;&#30340;&#25915;&#20987;&#32773;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#38656;&#35201;&#22810;&#23569;&#20010;&#37319;&#26679;&#35797;&#39564;&#25165;&#33021;&#30830;&#20445;&#33719;&#24471;&#25152;&#38656;&#30340;&#26080;&#25915;&#20987;&#32773;&#23376;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multiple robots could perceive a scene (e.g., detect objects) collaboratively better than individuals, although easily suffer from adversarial attacks when using deep learning. This could be addressed by the adversarial defense, but its training requires the often-unknown attacking mechanism. Differently, we propose ROBOSAC, a novel sampling-based defense strategy generalizable to unseen attackers. Our key idea is that collaborative perception should lead to consensus rather than dissensus in results compared to individual perception. This leads to our hypothesize-and-verify framework: perception results with and without collaboration from a random subset of teammates are compared until reaching a consensus. In such a framework, more teammates in the sampled subset often entail better perception performance but require longer sampling time to reject potential attackers. Thus, we derive how many sampling trials are needed to ensure the desired size of an attacker-free subset, or equival
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#31383;&#21475;&#30340;&#26089;&#26399;&#36864;&#20986;&#38598;&#25104;&#26041;&#27861;&#65292;&#20197;&#22312;&#20445;&#25345;&#27169;&#22411;&#21487;&#25193;&#23637;&#24615;&#30340;&#21516;&#26102;&#23454;&#29616;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#20219;&#21153;&#30340;&#39640;&#25928;&#23454;&#29616;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#19978;&#37117;&#36798;&#21040;&#20102;&#26368;&#26032;&#30340;&#30740;&#31350;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.08010</link><description>&lt;p&gt;
&#22522;&#20110;&#31383;&#21475;&#30340;&#26089;&#26399;&#36864;&#20986;&#32423;&#32852;&#29992;&#20110;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65306;&#24403;&#28145;&#24230;&#38598;&#25104;&#27604;&#21333;&#19968;&#27169;&#22411;&#26356;&#26377;&#25928;&#26102;
&lt;/p&gt;
&lt;p&gt;
Window-Based Early-Exit Cascades for Uncertainty Estimation: When Deep Ensembles are More Efficient than Single Models. (arXiv:2303.08010v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08010
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#31383;&#21475;&#30340;&#26089;&#26399;&#36864;&#20986;&#38598;&#25104;&#26041;&#27861;&#65292;&#20197;&#22312;&#20445;&#25345;&#27169;&#22411;&#21487;&#25193;&#23637;&#24615;&#30340;&#21516;&#26102;&#23454;&#29616;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#20219;&#21153;&#30340;&#39640;&#25928;&#23454;&#29616;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#19978;&#37117;&#36798;&#21040;&#20102;&#26368;&#26032;&#30340;&#30740;&#31350;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#38598;&#25104;&#26159;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#39044;&#27979;&#24615;&#33021;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#31616;&#21333;&#12289;&#21487;&#38752;&#21644;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38656;&#35201;&#37096;&#32626;&#22810;&#20010;&#29420;&#31435;&#27169;&#22411;&#65292;&#23427;&#20204;&#34987;&#24191;&#27867;&#25209;&#35780;&#20026;&#35745;&#31639;&#24320;&#38144;&#22823;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25361;&#25112;&#20102;&#36825;&#31181;&#35266;&#28857;&#65292;&#34920;&#26126;&#23545;&#20110;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#38598;&#25104;&#21487;&#20197;&#27604;&#22312;&#21516;&#19968;&#26550;&#26500;&#26063;&#20013;&#32553;&#25918;&#21333;&#19968;&#27169;&#22411;&#22312;&#25512;&#29702;&#26102;&#26356;&#20855;&#35745;&#31639;&#25928;&#29575;&#12290;&#36890;&#36807;&#36890;&#36807;&#26089;&#26399;&#36864;&#20986;&#26041;&#27861;&#32423;&#32852;&#38598;&#25104;&#25104;&#21592;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#22914;&#20309;&#23558;&#36825;&#20123;&#25928;&#29575;&#25552;&#39640;&#25193;&#23637;&#21040;&#19982;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30456;&#20851;&#30340;&#20219;&#21153;&#12290;&#30001;&#20110;&#35768;&#22810;&#36825;&#26679;&#30340;&#20219;&#21153;&#65292;&#20363;&#22914;&#36873;&#25321;&#24615;&#20998;&#31867;&#65292;&#37117;&#26159;&#20108;&#20998;&#31867;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#20851;&#38190;&#26032;&#39062;&#35265;&#35299;&#26159;&#20165;&#23558;&#25509;&#36817;&#20108;&#20998;&#20915;&#31574;&#36793;&#30028;&#30340;&#26679;&#26412;&#20256;&#36882;&#21040;&#21518;&#32493;&#32423;&#32852;&#38454;&#27573;&#12290;&#22312;ImageNet&#35268;&#27169;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#22522;&#20110;&#31383;&#21475;&#30340;&#26089;&#26399;&#36864;&#20986;&#38598;&#25104;&#22312;&#20351;&#29992;&#27604;&#22522;&#32447;&#26356;&#23569;&#30340;&#27169;&#22411;&#35780;&#20272;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#39044;&#27979;&#24615;&#33021;&#19978;&#19982;&#23436;&#25972;&#38598;&#25104;&#30456;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Ensembles are a simple, reliable, and effective method of improving both the predictive performance and uncertainty estimates of deep learning approaches. However, they are widely criticised as being computationally expensive, due to the need to deploy multiple independent models. Recent work has challenged this view, showing that for predictive accuracy, ensembles can be more computationally efficient (at inference) than scaling single models within an architecture family. This is achieved by cascading ensemble members via an early-exit approach. In this work, we investigate extending these efficiency gains to tasks related to uncertainty estimation. As many such tasks, e.g. selective classification, are binary classification, our key novel insight is to only pass samples within a window close to the binary decision boundary to later cascade stages. Experiments on ImageNet-scale data across a number of network architectures and uncertainty tasks show that the proposed window-base
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#37325;&#35201;&#20294;&#40092;&#20026;&#20154;&#30693;&#30340;&#38382;&#39064;&#65306;&#32852;&#37030;&#31867;&#24335;&#25345;&#32493;&#23398;&#20064;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#21160;&#24577;&#28155;&#21152;&#26032;&#30340;&#31867;&#21035;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;TARGET&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#36890;&#36807;&#26080;&#26679;&#26412;&#33976;&#39311;&#26469;&#20943;&#36731;FCCL&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#65292;&#24182;&#20445;&#25252;&#23458;&#25143;&#25968;&#25454;&#30340;&#38544;&#31169;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20808;&#21069;&#35757;&#32451;&#30340;&#20840;&#23616;&#27169;&#22411;&#22312;&#27169;&#22411;&#23618;&#38754;&#19978;&#20256;&#36882;&#26087;&#20219;&#21153;&#30340;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;&#22120;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#26469;&#27169;&#25311;&#25968;&#25454;&#30340;&#20840;&#23616;&#20998;&#24067;&#12290;&#19982;&#20808;&#21069;&#30340;FCCL&#26041;&#27861;&#30456;&#27604;&#65292;TARGET&#26080;&#38656;&#39069;&#22806;&#30340;&#25968;&#25454;&#38598;&#25110;&#23384;&#20648;&#20808;&#21069;&#20219;&#21153;&#30340;&#31169;&#26377;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2303.06937</link><description>&lt;p&gt;
TARGET: &#36890;&#36807;&#26080;&#26679;&#26412;&#33976;&#39311;&#23454;&#29616;&#32852;&#37030;&#31867;&#24335;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
TARGET: Federated Class-Continual Learning via Exemplar-Free Distillation. (arXiv:2303.06937v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06937
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#37325;&#35201;&#20294;&#40092;&#20026;&#20154;&#30693;&#30340;&#38382;&#39064;&#65306;&#32852;&#37030;&#31867;&#24335;&#25345;&#32493;&#23398;&#20064;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#21160;&#24577;&#28155;&#21152;&#26032;&#30340;&#31867;&#21035;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;TARGET&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#36890;&#36807;&#26080;&#26679;&#26412;&#33976;&#39311;&#26469;&#20943;&#36731;FCCL&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#65292;&#24182;&#20445;&#25252;&#23458;&#25143;&#25968;&#25454;&#30340;&#38544;&#31169;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20808;&#21069;&#35757;&#32451;&#30340;&#20840;&#23616;&#27169;&#22411;&#22312;&#27169;&#22411;&#23618;&#38754;&#19978;&#20256;&#36882;&#26087;&#20219;&#21153;&#30340;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;&#22120;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#26469;&#27169;&#25311;&#25968;&#25454;&#30340;&#20840;&#23616;&#20998;&#24067;&#12290;&#19982;&#20808;&#21069;&#30340;FCCL&#26041;&#27861;&#30456;&#27604;&#65292;TARGET&#26080;&#38656;&#39069;&#22806;&#30340;&#25968;&#25454;&#38598;&#25110;&#23384;&#20648;&#20808;&#21069;&#20219;&#21153;&#30340;&#31169;&#26377;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#19968;&#20010;&#40092;&#20026;&#20154;&#30693;&#20294;&#37325;&#35201;&#30340;&#38382;&#39064;&#36827;&#34892;&#30740;&#31350;&#65306;&#32852;&#37030;&#31867;&#24335;&#25345;&#32493;&#23398;&#20064;&#65288;FCCL&#65289;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#21160;&#24577;&#28155;&#21152;&#26032;&#30340;&#31867;&#21035;&#12290;&#24050;&#26377;&#30340;FCCL&#26041;&#27861;&#23384;&#22312;&#21508;&#31181;&#38480;&#21046;&#65292;&#22914;&#38656;&#35201;&#39069;&#22806;&#30340;&#25968;&#25454;&#38598;&#25110;&#23384;&#20648;&#20808;&#21069;&#20219;&#21153;&#30340;&#31169;&#26377;&#25968;&#25454;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#25968;&#25454;&#21152;&#21095;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#8212;&#8212;TARGET&#65288;&#36890;&#36807;&#26080;&#26679;&#26412;&#33976;&#39311;&#23454;&#29616;&#32852;&#37030;&#31867;&#24335;&#25345;&#32493;&#23398;&#20064;&#65289;&#65292;&#35813;&#26041;&#27861;&#22312;&#20943;&#36731;FCCL&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#30340;&#21516;&#26102;&#20445;&#25252;&#23458;&#25143;&#25968;&#25454;&#30340;&#38544;&#31169;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20808;&#21069;&#35757;&#32451;&#30340;&#20840;&#23616;&#27169;&#22411;&#65292;&#22312;&#27169;&#22411;&#23618;&#38754;&#19978;&#23558;&#26087;&#20219;&#21153;&#30340;&#30693;&#35782;&#20256;&#36882;&#32473;&#24403;&#21069;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35757;&#32451;&#19968;&#20010;&#29983;&#25104;&#22120;&#26469;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#65292;&#20197;&#27169;&#25311;&#27599;&#20010;&#23458;&#25143;&#31471;&#19978;&#25968;&#25454;&#30340;&#20840;&#23616;&#20998;&#24067;&#12290;&#19982;&#20808;&#21069;&#30340;FCCL&#26041;&#27861;&#30456;&#27604;&#65292;TARGET&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#25968;&#25454;&#38598;&#25110;&#23384;&#20648;&#20808;&#21069;&#20219;&#21153;&#30340;&#31169;&#26377;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper focuses on an under-explored yet important problem: Federated Class-Continual Learning (FCCL), where new classes are dynamically added in federated learning. Existing FCCL works suffer from various limitations, such as requiring additional datasets or storing the private data from previous tasks. In response, we first demonstrate that non-IID data exacerbates catastrophic forgetting issue in FL. Then we propose a novel method called TARGET (federat\textbf{T}ed cl\textbf{A}ss-continual lea\textbf{R}nin\textbf{G} via \textbf{E}xemplar-free dis\textbf{T}illation), which alleviates catastrophic forgetting in FCCL while preserving client data privacy. Our proposed method leverages the previously trained global model to transfer knowledge of old tasks to the current task at the model level. Moreover, a generator is trained to produce synthetic data to simulate the global distribution of data on each client at the data level. Compared to previous FCCL methods, TARGET does not requi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21463;&#38480;&#20195;&#29702;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25511;&#21046;&#28145;&#24230;&#24207;&#25968;&#20998;&#31867;&#20013;&#30340;&#31867;&#24067;&#23616;&#12290;</title><link>http://arxiv.org/abs/2303.00396</link><description>&lt;p&gt;
&#36890;&#36807;&#21463;&#38480;&#20195;&#29702;&#23398;&#20064;&#25511;&#21046;&#28145;&#24230;&#24207;&#25968;&#20998;&#31867;&#20013;&#30340;&#31867;&#24067;&#23616;
&lt;/p&gt;
&lt;p&gt;
Controlling Class Layout for Deep Ordinal Classification via Constrained Proxies Learning. (arXiv:2303.00396v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00396
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21463;&#38480;&#20195;&#29702;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25511;&#21046;&#28145;&#24230;&#24207;&#25968;&#20998;&#31867;&#20013;&#30340;&#31867;&#24067;&#23616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#28145;&#24230;&#24207;&#25968;&#20998;&#31867;&#20219;&#21153;&#65292;&#23398;&#20064;&#29305;&#23450;&#20110;&#24207;&#25968;&#20998;&#31867;&#30340;&#33391;&#22909;&#32467;&#26500;&#21270;&#29305;&#24449;&#31354;&#38388;&#26377;&#21161;&#20110;&#24688;&#24403;&#22320;&#25429;&#25417;&#31867;&#20043;&#38388;&#30340;&#24207;&#25968;&#23646;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21463;&#38480;&#20195;&#29702;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20026;&#27599;&#20010;&#24207;&#25968;&#31867;&#23398;&#20064;&#19968;&#20010;&#20195;&#29702;&#65292;&#28982;&#21518;&#36890;&#36807;&#38480;&#21046;&#36825;&#20123;&#20195;&#29702;&#26469;&#35843;&#25972;&#31867;&#30340;&#20840;&#23616;&#24067;&#23616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#31574;&#30053;&#65306;&#30828;&#24067;&#23616;&#32422;&#26463;&#21644;&#36719;&#24067;&#23616;&#32422;&#26463;&#12290;&#30828;&#24067;&#23616;&#32422;&#26463;&#36890;&#36807;&#30452;&#25509;&#25511;&#21046;&#20195;&#29702;&#30340;&#29983;&#25104;&#26469;&#23454;&#29616;&#65292;&#20197;&#24378;&#21046;&#23558;&#20854;&#25918;&#32622;&#22312;&#20005;&#26684;&#30340;&#32447;&#24615;&#24067;&#23616;&#25110;&#21322;&#22278;&#24418;&#24067;&#23616;&#65288;&#21363;&#20005;&#26684;&#24207;&#25968;&#24067;&#23616;&#30340;&#20004;&#31181;&#23454;&#20363;&#65289;&#20013;&#12290;&#36719;&#24067;&#23616;&#32422;&#26463;&#36890;&#36807;&#24341;&#20837;&#27491;&#21017;&#21270;&#39033;&#21040;&#25439;&#22833;&#20989;&#25968;&#20013;&#26469;&#23454;&#29616;&#65292;&#35813;&#39033;&#24809;&#32602;&#20559;&#31163;&#29702;&#24819;&#24207;&#25968;&#24067;&#23616;&#30340;&#24773;&#20917;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;CPL&#26041;&#27861;&#22312;&#28145;&#24230;&#24207;&#25968;&#20998;&#31867;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
For deep ordinal classification, learning a well-structured feature space specific to ordinal classification is helpful to properly capture the ordinal nature among classes. Intuitively, when Euclidean distance metric is used, an ideal ordinal layout in feature space would be that the sample clusters are arranged in class order along a straight line in space. However, enforcing samples to conform to a specific layout in the feature space is a challenging problem. To address this problem, in this paper, we propose a novel Constrained Proxies Learning (CPL) method, which can learn a proxy for each ordinal class and then adjusts the global layout of classes by constraining these proxies. Specifically, we propose two kinds of strategies: hard layout constraint and soft layout constraint. The hard layout constraint is realized by directly controlling the generation of proxies to force them to be placed in a strict linear layout or semicircular layout (i.e., two instantiations of strict ordi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#23376;&#32676;&#20307;&#36716;&#21464;&#30340;&#21508;&#31181;&#26426;&#21046;&#65292;&#23545;20&#20010;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#22312;12&#20010;&#39046;&#22495;&#20869;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#21457;&#29616;&#29616;&#26377;&#31639;&#27861;&#21482;&#33021;&#24212;&#23545;&#26576;&#20123;&#36716;&#21464;&#65292;&#36827;&#19968;&#27493;&#22320;&#65292;&#25552;&#20986;&#19968;&#31181;&#31616;&#21333;&#26131;&#34892;&#30340;&#36873;&#25321;&#26631;&#20934;&#26469;&#25913;&#21892;&#29616;&#26377;&#31639;&#27861;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.12254</link><description>&lt;p&gt;
&#25913;&#21464;&#24456;&#38590;&#65306;&#23376;&#32676;&#20307;&#36716;&#21464;&#30340;&#28145;&#20837;&#25506;&#31350;
&lt;/p&gt;
&lt;p&gt;
Change is Hard: A Closer Look at Subpopulation Shift. (arXiv:2302.12254v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12254
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#23376;&#32676;&#20307;&#36716;&#21464;&#30340;&#21508;&#31181;&#26426;&#21046;&#65292;&#23545;20&#20010;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#22312;12&#20010;&#39046;&#22495;&#20869;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#21457;&#29616;&#29616;&#26377;&#31639;&#27861;&#21482;&#33021;&#24212;&#23545;&#26576;&#20123;&#36716;&#21464;&#65292;&#36827;&#19968;&#27493;&#22320;&#65292;&#25552;&#20986;&#19968;&#31181;&#31616;&#21333;&#26131;&#34892;&#30340;&#36873;&#25321;&#26631;&#20934;&#26469;&#25913;&#21892;&#29616;&#26377;&#31639;&#27861;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36890;&#24120;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#20195;&#34920;&#24615;&#19981;&#36275;&#30340;&#23376;&#32676;&#20307;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#23548;&#33268;&#23376;&#32676;&#20307;&#36716;&#21464;&#30340;&#26426;&#21046;&#20197;&#21450;&#31639;&#27861;&#22312;&#22914;&#27492;&#19981;&#21516;&#30340;&#36716;&#21464;&#20013;&#22914;&#20309;&#36827;&#34892;&#26222;&#36941;&#21270;&#65292;&#25105;&#20204;&#30693;&#20043;&#29978;&#23569;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;&#23376;&#32676;&#20307;&#36716;&#21464;&#36827;&#34892;&#20102;&#32454;&#33268;&#30340;&#20998;&#26512;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#26469;&#21078;&#26512;&#21644;&#35299;&#37322;&#23376;&#32676;&#20307;&#20013;&#30340;&#24120;&#35265;&#36716;&#21464;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;&#35270;&#35273;&#12289;&#35821;&#35328;&#21644;&#21307;&#30103;&#39046;&#22495;&#30340;12&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#23545;20&#20010;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#36890;&#36807;&#35757;&#32451;10,000&#22810;&#20010;&#27169;&#22411;&#24471;&#21040;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#26410;&#26469;&#22312;&#36825;&#20010;&#39046;&#22495;&#21462;&#24471;&#36827;&#23637;&#30340;&#26377;&#36259;&#35266;&#23519;&#32467;&#26524;&#12290;&#39318;&#20808;&#65292;&#29616;&#26377;&#31639;&#27861;&#20165;&#33021;&#22312;&#26576;&#20123;&#31867;&#22411;&#30340;&#36716;&#21464;&#19978;&#25552;&#39640;&#23376;&#32676;&#20307;&#30340;&#40065;&#26834;&#24615;&#65292;&#32780;&#22312;&#20854;&#20182;&#31867;&#22411;&#30340;&#36716;&#21464;&#19978;&#21017;&#19981;&#33021;&#12290;&#27492;&#22806;&#65292;&#34429;&#28982;&#24403;&#21069;&#31639;&#27861;&#20381;&#36182;&#20110;&#32676;&#20307;&#26631;&#27880;&#30340;&#39564;&#35777;&#25968;&#25454;&#36827;&#34892;&#27169;&#22411;&#36873;&#25321;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#22522;&#20110;&#26368;&#24046;&#31867;&#21035;&#20934;&#30830;&#24230;&#30340;&#31616;&#21333;&#36873;&#25321;&#26631;&#20934;&#20854;&#23454;&#38750;&#24120;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models often perform poorly on subgroups that are underrepresented in the training data. Yet, little is understood on the variation in mechanisms that cause subpopulation shifts, and how algorithms generalize across such diverse shifts at scale. In this work, we provide a fine-grained analysis of subpopulation shift. We first propose a unified framework that dissects and explains common shifts in subgroups. We then establish a comprehensive benchmark of 20 state-of-the-art algorithms evaluated on 12 real-world datasets in vision, language, and healthcare domains. With results obtained from training over 10,000 models, we reveal intriguing observations for future progress in this space. First, existing algorithms only improve subgroup robustness over certain types of shifts but not others. Moreover, while current algorithms rely on group-annotated validation data for model selection, we find that a simple selection criterion based on worst-class accuracy is surprisingly
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#36776;&#21035;&#30446;&#26631;&#21644;&#29983;&#25104;&#30446;&#26631;&#65292;&#26126;&#30830;&#22320;&#23454;&#29616;&#20102;&#35299;&#21078;&#19981;&#21464;&#24615;&#24314;&#27169;&#21644;&#35821;&#20041;&#23545;&#40784;&#12290;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#65292;&#40723;&#21169;&#19981;&#21516;&#26469;&#28304;&#20294;&#20855;&#26377;&#19968;&#33268;&#39640;&#32423;&#35821;&#20041;&#30340;&#35270;&#22270;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#20197;&#23398;&#20064;&#19981;&#21464;&#30340;&#35299;&#21078;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2302.05615</link><description>&lt;p&gt;
&#22312;3D&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#35299;&#21078;&#19981;&#21464;&#24615;&#24314;&#27169;&#21644;&#35821;&#20041;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Anatomical Invariance Modeling and Semantic Alignment for Self-supervised Learning in 3D Medical Image Analysis. (arXiv:2302.05615v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05615
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#36776;&#21035;&#30446;&#26631;&#21644;&#29983;&#25104;&#30446;&#26631;&#65292;&#26126;&#30830;&#22320;&#23454;&#29616;&#20102;&#35299;&#21078;&#19981;&#21464;&#24615;&#24314;&#27169;&#21644;&#35821;&#20041;&#23545;&#40784;&#12290;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#65292;&#40723;&#21169;&#19981;&#21516;&#26469;&#28304;&#20294;&#20855;&#26377;&#19968;&#33268;&#39640;&#32423;&#35821;&#20041;&#30340;&#35270;&#22270;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#20197;&#23398;&#20064;&#19981;&#21464;&#30340;&#35299;&#21078;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26368;&#36817;&#22312;3D&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#12290;&#22823;&#22810;&#25968;&#24403;&#21069;&#26041;&#27861;&#36981;&#24490;&#21407;&#26412;&#35774;&#35745;&#29992;&#20110;&#29031;&#29255;&#25110;&#33258;&#28982;&#22270;&#20687;&#30340;&#29616;&#26377;SSL&#33539;&#24335;&#65292;&#26080;&#27861;&#26126;&#30830;&#22320;&#21644;&#24443;&#24213;&#22320;&#21033;&#29992;&#19981;&#21516;&#21307;&#23398;&#22270;&#20687;&#20013;&#20869;&#22312;&#30340;&#30456;&#20284;&#35299;&#21078;&#32467;&#26500;&#12290;&#36825;&#23454;&#38469;&#19978;&#21487;&#33021;&#36890;&#36807;&#26368;&#22823;&#21270;&#21253;&#21547;&#31354;&#38388;&#19981;&#23545;&#40784;&#20449;&#24687;&#21644;&#19981;&#21516;&#35299;&#21078;&#35821;&#20041;&#30340;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#26469;&#38477;&#20302;&#25152;&#23398;&#28145;&#24230;&#34920;&#31034;&#30340;&#36136;&#37327;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#21517;&#20026;Alice&#65292;&#36890;&#36807;&#24039;&#22937;&#22320;&#32467;&#21512;&#36776;&#21035;&#30446;&#26631;&#21644;&#29983;&#25104;&#30446;&#26631;&#65292;&#26126;&#30830;&#22320;&#23454;&#29616;&#20102;&#35299;&#21078;&#19981;&#21464;&#24615;&#24314;&#27169;&#21644;&#35821;&#20041;&#23545;&#40784;&#12290;Alice&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#65292;&#40723;&#21169;&#19981;&#21516;&#26469;&#28304;&#20294;&#20855;&#26377;&#19968;&#33268;&#39640;&#32423;&#35821;&#20041;&#30340;&#35270;&#22270;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#20197;&#23398;&#20064;&#19981;&#21464;&#30340;&#35299;&#21078;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#20849;&#29983;&#23398;&#20064;&#31574;&#30053;&#65292;&#23558;&#28508;&#22312;&#22122;&#22768;&#22270;&#20687;&#19982;&#33391;&#24615;&#22270;&#20687;&#36827;&#34892;&#23545;&#27604;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) has recently achieved promising performance for 3D medical image analysis tasks. Most current methods follow existing SSL paradigm originally designed for photographic or natural images, which cannot explicitly and thoroughly exploit the intrinsic similar anatomical structures across varying medical images. This may in fact degrade the quality of learned deep representations by maximizing the similarity among features containing spatial misalignment information and different anatomical semantics. In this work, we propose a new self-supervised learning framework, namely Alice, that explicitly fulfills Anatomical invariance modeling and semantic alignment via elaborately combining discriminative and generative objectives. Alice introduces a new contrastive learning strategy which encourages the similarity between views that are diversely mined but with consistent high-level semantics, in order to learn invariant anatomical features. Moreover, we design a co
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;PromptMize&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#30340;&#34920;&#26684;&#21040;&#25991;&#26412;&#29983;&#25104;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#21253;&#21547;&#25552;&#31034;&#31574;&#21010;&#21644;&#30693;&#35782;&#36866;&#37197;&#22120;&#20004;&#20010;&#26041;&#38754;&#65292;&#36890;&#36807;&#29983;&#25104;&#25552;&#31034;&#20449;&#21495;&#21644;&#21033;&#29992;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#26469;&#25913;&#21892;&#25991;&#26412;&#29983;&#25104;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.04415</link><description>&lt;p&gt;
&#24102;&#26377;&#25552;&#31034;&#31574;&#21010;&#21644;&#30693;&#35782;&#35760;&#24518;&#30340;&#23569;&#26679;&#26412;&#34920;&#26684;&#21040;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Few-Shot Table-to-Text Generation with Prompt Planning and Knowledge Memorization. (arXiv:2302.04415v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04415
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;PromptMize&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#30340;&#34920;&#26684;&#21040;&#25991;&#26412;&#29983;&#25104;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#21253;&#21547;&#25552;&#31034;&#31574;&#21010;&#21644;&#30693;&#35782;&#36866;&#37197;&#22120;&#20004;&#20010;&#26041;&#38754;&#65292;&#36890;&#36807;&#29983;&#25104;&#25552;&#31034;&#20449;&#21495;&#21644;&#21033;&#29992;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#26469;&#25913;&#21892;&#25991;&#26412;&#29983;&#25104;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#34920;&#26684;&#21040;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#26631;&#35760;&#30340;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#21644;&#34920;&#26684;&#25968;&#25454;&#19982;&#25991;&#26412;&#20043;&#38388;&#30340;&#25299;&#25169;&#24046;&#36317;&#20351;&#24471;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#38590;&#20197;&#29983;&#25104;&#20934;&#30830;&#30340;&#25991;&#26412;&#12290;&#22312;&#20302;&#36164;&#28304;&#29983;&#25104;&#20013;&#65292;&#36825;&#20010;&#39046;&#22495;&#38754;&#20020;&#30528;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#21463;&#21040;&#20154;&#31867;&#22914;&#20309;&#20351;&#29992;&#20808;&#21069;&#30340;&#30693;&#35782;&#25551;&#36848;&#34920;&#26684;&#25968;&#25454;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65306;PromptMize&#65292;&#35813;&#26694;&#26550;&#38024;&#23545;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#30340;&#34920;&#26684;&#21040;&#25991;&#26412;&#29983;&#25104;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#30340;&#35774;&#35745;&#21253;&#21547;&#20004;&#20010;&#26041;&#38754;&#65306;&#25552;&#31034;&#31574;&#21010;&#21644;&#30693;&#35782;&#36866;&#37197;&#22120;&#12290;&#25552;&#31034;&#31574;&#21010;&#30340;&#30446;&#26631;&#26159;&#29983;&#25104;&#19968;&#20010;&#25552;&#31034;&#20449;&#21495;&#65292;&#20026;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#23454;&#20363;&#25351;&#23548;&#65292;&#20197;&#24357;&#21512;&#34920;&#26684;&#25968;&#25454;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#25299;&#25169;&#24046;&#36317;&#12290;&#27492;&#22806;&#65292;&#30693;&#35782;&#36866;&#37197;&#22120;&#20174;&#26410;&#26631;&#35760;&#30340;&#35821;&#26009;&#24211;&#20013;&#35760;&#24518;&#39046;&#22495;&#29305;&#23450;&#30340;&#30693;&#35782;&#65292;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#25552;&#20379;&#24517;&#35201;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#23545;&#19977;&#20010;&#24320;&#25918;&#39046;&#22495;&#30340;&#23569;&#26679;&#26412;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#21644;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models (PLM) have achieved remarkable advancement in table-to-text generation tasks. However, the lack of labeled domain-specific knowledge and the topology gap between tabular data and text make it difficult for PLMs to yield faithful text. Low-resource generation likewise faces unique challenges in this domain. Inspired by how humans descript tabular data with prior knowledge, we suggest a new framework: PromptMize, which targets table-to-text generation under few-shot settings. The design of our framework consists of two aspects: a prompt planner and a knowledge adapter. The prompt planner aims to generate a prompt signal that provides instance guidance for PLMs to bridge the topology gap between tabular data and text. Moreover, the knowledge adapter memorizes domain-specific knowledge from the unlabelled corpus to supply essential information during generation. Extensive experiments and analyses are investigated on three open domain few-shot NLG datasets: human
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;REAP&#30340;&#22823;&#35268;&#27169;&#30495;&#23454;&#23545;&#25239;&#36148;&#32440;&#22522;&#20934;&#27979;&#35797;&#65292;&#35813;&#22522;&#20934;&#27979;&#35797;&#20801;&#35768;&#29992;&#25143;&#22312;&#30495;&#23454;&#22270;&#20687;&#21644;&#30495;&#23454;&#29615;&#22659;&#26465;&#20214;&#19979;&#35780;&#20272;&#23545;&#25239;&#36148;&#32440;&#25915;&#20987;&#65292;&#20026;&#35299;&#20915;&#20381;&#36182;&#25668;&#20687;&#22836;&#30340;&#29289;&#29702;&#31995;&#32479;&#38754;&#20020;&#30340;&#20005;&#37325;&#23041;&#32961;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2212.05680</link><description>&lt;p&gt;
REAP&#65306;&#19968;&#20010;&#22823;&#35268;&#27169;&#30495;&#23454;&#23545;&#25239;&#36148;&#32440;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
REAP: A Large-Scale Realistic Adversarial Patch Benchmark. (arXiv:2212.05680v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.05680
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;REAP&#30340;&#22823;&#35268;&#27169;&#30495;&#23454;&#23545;&#25239;&#36148;&#32440;&#22522;&#20934;&#27979;&#35797;&#65292;&#35813;&#22522;&#20934;&#27979;&#35797;&#20801;&#35768;&#29992;&#25143;&#22312;&#30495;&#23454;&#22270;&#20687;&#21644;&#30495;&#23454;&#29615;&#22659;&#26465;&#20214;&#19979;&#35780;&#20272;&#23545;&#25239;&#36148;&#32440;&#25915;&#20987;&#65292;&#20026;&#35299;&#20915;&#20381;&#36182;&#25668;&#20687;&#22836;&#30340;&#29289;&#29702;&#31995;&#32479;&#38754;&#20020;&#30340;&#20005;&#37325;&#23041;&#32961;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#25200;&#21160;&#30340;&#24433;&#21709;&#12290;&#20854;&#20013;&#19968;&#31181;&#33879;&#21517;&#30340;&#25915;&#20987;&#26041;&#24335;&#26159;&#23545;&#25239;&#36148;&#32440;&#65292;&#36825;&#26159;&#19968;&#31181;&#24102;&#26377;&#29305;&#23450;&#22270;&#26696;&#30340;&#36148;&#32440;&#65292;&#20351;&#24471;&#27169;&#22411;&#22312;&#36148;&#32440;&#25152;&#36148;&#29289;&#20307;&#19978;&#30340;&#39044;&#27979;&#38169;&#35823;&#12290;&#36825;&#31181;&#25915;&#20987;&#23545;&#20110;&#20381;&#36182;&#20110;&#25668;&#20687;&#22836;&#30340;&#29289;&#29702;&#31995;&#32479;&#65292;&#22914;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#65292;&#26500;&#25104;&#20102;&#20005;&#37325;&#23041;&#32961;&#12290;&#23613;&#31649;&#38382;&#39064;&#30340;&#37325;&#35201;&#24615;&#65292;&#20294;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#36827;&#34892;&#30740;&#31350;&#26159;&#22256;&#38590;&#30340;&#65307;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#35780;&#20272;&#25915;&#20987;&#21644;&#38450;&#24481;&#31574;&#30053;&#25104;&#26412;&#39640;&#26114;&#65292;&#32780;&#21512;&#25104;&#25968;&#25454;&#21017;&#19981;&#22815;&#30495;&#23454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;REAP&#65288;&#30495;&#23454;&#23545;&#25239;&#36148;&#32440;&#65289;&#22522;&#20934;&#27979;&#35797;&#65292;&#36825;&#26159;&#19968;&#20010;&#25968;&#23383;&#22522;&#20934;&#27979;&#35797;&#65292;&#20801;&#35768;&#29992;&#25143;&#22312;&#30495;&#23454;&#22270;&#20687;&#21644;&#30495;&#23454;&#29615;&#22659;&#26465;&#20214;&#19979;&#35780;&#20272;&#23545;&#25239;&#36148;&#32440;&#25915;&#20987;&#12290;&#22522;&#20110;Mapillary Vistas&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#21253;&#21547;&#36229;&#36807;14,000&#20010;&#20132;&#36890;&#26631;&#24535;&#12290;&#27599;&#20010;&#26631;&#24535;&#37117;&#32463;&#36807;&#20960;&#20309;&#21644;&#20809;&#29031;&#21464;&#25442;&#30340;&#25913;&#21464;&#65292;&#36825;&#21487;&#20197;&#29992;&#26469;&#23558;&#25968;&#23383;&#29983;&#25104;&#30340;&#36148;&#32440;&#30495;&#23454;&#22320;&#24212;&#29992;&#21040;&#22270;&#20687;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models are known to be susceptible to adversarial perturbation. One famous attack is the adversarial patch, a sticker with a particularly crafted pattern that makes the model incorrectly predict the object it is placed on. This attack presents a critical threat to cyber-physical systems that rely on cameras such as autonomous cars. Despite the significance of the problem, conducting research in this setting has been difficult; evaluating attacks and defenses in the real world is exceptionally costly while synthetic data are unrealistic. In this work, we propose the REAP (REalistic Adversarial Patch) benchmark, a digital benchmark that allows the user to evaluate patch attacks on real images, and under real-world conditions. Built on top of the Mapillary Vistas dataset, our benchmark contains over 14,000 traffic signs. Each sign is augmented with a pair of geometric and lighting transformations, which can be used to apply a digitally generated patch realistically onto t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#36895;&#29575;VAE&#65288;MR-VAE&#65289;&#30340;&#26694;&#26550;&#65292;&#21487;&#22312;&#21333;&#27425;&#35757;&#32451;&#20013;&#23398;&#20064;&#19982;&#19981;&#21516;&#946;&#23545;&#24212;&#30340;&#26368;&#20248;&#21442;&#25968;&#65292;&#36890;&#36807;&#20351;&#29992;&#36229;&#32593;&#32476;&#23558;&#946;&#26144;&#23556;&#21040;&#26368;&#20248;&#21442;&#25968;&#65292;&#20197;&#23454;&#29616;&#29575;&#22833;&#30495;&#26354;&#32447;&#30340;&#23436;&#25972;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2212.03905</link><description>&lt;p&gt;
&#22810;&#36895;&#29575;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65306;&#19968;&#27425;&#35757;&#32451;&#65292;&#24471;&#21040;&#23436;&#25972;&#30340;&#29575;&#22833;&#30495;&#26354;&#32447;
&lt;/p&gt;
&lt;p&gt;
Multi-Rate VAE: Train Once, Get the Full Rate-Distortion Curve. (arXiv:2212.03905v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.03905
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#36895;&#29575;VAE&#65288;MR-VAE&#65289;&#30340;&#26694;&#26550;&#65292;&#21487;&#22312;&#21333;&#27425;&#35757;&#32451;&#20013;&#23398;&#20064;&#19982;&#19981;&#21516;&#946;&#23545;&#24212;&#30340;&#26368;&#20248;&#21442;&#25968;&#65292;&#36890;&#36807;&#20351;&#29992;&#36229;&#32593;&#32476;&#23558;&#946;&#26144;&#23556;&#21040;&#26368;&#20248;&#21442;&#25968;&#65292;&#20197;&#23454;&#29616;&#29575;&#22833;&#30495;&#26354;&#32447;&#30340;&#23436;&#25972;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAEs&#65289;&#26159;&#19968;&#31181;&#29992;&#20110;&#23398;&#20064;&#25968;&#25454;&#30340;&#28508;&#22312;&#34920;&#31034;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#24212;&#29992;&#39046;&#22495;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;VAEs&#36890;&#24120;&#38656;&#35201;&#22810;&#27425;&#35757;&#32451;&#26469;&#36873;&#25321;&#28508;&#22312;&#21464;&#37327;&#24212;&#35813;&#20445;&#30041;&#30340;&#20449;&#24687;&#37327;&#12290;&#37325;&#26500;&#35823;&#24046;&#65288;&#22833;&#30495;&#65289;&#21644;KL&#25955;&#24230;&#65288;&#29575;&#65289;&#20043;&#38388;&#30340;&#26435;&#34913;&#36890;&#24120;&#30001;&#36229;&#21442;&#25968;&#946;&#21442;&#25968;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22810;&#36895;&#29575;VAE&#65288;MR-VAE&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#21333;&#27425;&#35757;&#32451;&#20013;&#23398;&#20064;&#19982;&#19981;&#21516;&#946;&#23545;&#24212;&#30340;&#26368;&#20248;&#21442;&#25968;&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#20351;&#29992;&#36229;&#32593;&#32476;&#26126;&#30830;&#22320;&#21046;&#23450;&#19968;&#20010;&#21709;&#24212;&#20989;&#25968;&#65292;&#23558;&#946;&#26144;&#23556;&#21040;&#26368;&#20248;&#21442;&#25968;&#12290;MR-VAEs&#26500;&#24314;&#20102;&#19968;&#20010;&#32039;&#20945;&#30340;&#21709;&#24212;&#36229;&#32593;&#32476;&#65292;&#20854;&#20013;&#30340;&#39044;&#28608;&#27963;&#26681;&#25454;&#946;&#36827;&#34892;&#26377;&#26465;&#20214;&#30340;&#38376;&#25511;&#12290;&#36890;&#36807;&#20998;&#26512;&#32447;&#24615;VAEs&#24182;&#23637;&#31034;&#23427;&#33021;&#22815;&#20934;&#30830;&#34920;&#31034;&#32447;&#24615;VAEs&#30340;&#21709;&#24212;&#20989;&#25968;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26550;&#26500;&#30340;&#21512;&#29702;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variational autoencoders (VAEs) are powerful tools for learning latent representations of data used in a wide range of applications. In practice, VAEs usually require multiple training rounds to choose the amount of information the latent variable should retain. This trade-off between the reconstruction error (distortion) and the KL divergence (rate) is typically parameterized by a hyperparameter $\beta$. In this paper, we introduce Multi-Rate VAE (MR-VAE), a computationally efficient framework for learning optimal parameters corresponding to various $\beta$ in a single training run. The key idea is to explicitly formulate a response function that maps $\beta$ to the optimal parameters using hypernetworks. MR-VAEs construct a compact response hypernetwork where the pre-activations are conditionally gated based on $\beta$. We justify the proposed architecture by analyzing linear VAEs and showing that it can represent response functions exactly for linear VAEs. With the learned hypernetw
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#29289;&#32852;&#32593;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#29992;&#20110;&#36890;&#36807;&#21361;&#38505;&#22240;&#32032;&#39044;&#27979;&#31958;&#23615;&#30149;&#65292;&#20197;&#30830;&#20445;&#29992;&#25143;&#25968;&#25454;&#30340;&#23433;&#20840;&#24615;&#21644;&#38544;&#31169;&#24615;&#65292;&#24182;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#19981;&#21516;&#21307;&#30103;&#20256;&#24863;&#22120;&#12289;&#35774;&#22791;&#21644;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2211.07643</link><description>&lt;p&gt;
&#23558;&#23433;&#20840;&#21644;&#38544;&#31169;&#20445;&#25252;&#30340;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#25805;&#20316;&#25972;&#21512;&#21040;&#31471;&#21040;&#31471;&#30340;&#32508;&#21512;&#29289;&#32852;&#32593;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#21306;&#22359;&#38142;&#30417;&#25511;&#31995;&#32479;&#20013;&#36827;&#34892;&#31958;&#23615;&#30149;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Secure and Privacy-Preserving Automated Machine Learning Operations into End-to-End Integrated IoT-Edge-Artificial Intelligence-Blockchain Monitoring System for Diabetes Mellitus Prediction. (arXiv:2211.07643v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.07643
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#29289;&#32852;&#32593;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#29992;&#20110;&#36890;&#36807;&#21361;&#38505;&#22240;&#32032;&#39044;&#27979;&#31958;&#23615;&#30149;&#65292;&#20197;&#30830;&#20445;&#29992;&#25143;&#25968;&#25454;&#30340;&#23433;&#20840;&#24615;&#21644;&#38544;&#31169;&#24615;&#65292;&#24182;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#19981;&#21516;&#21307;&#30103;&#20256;&#24863;&#22120;&#12289;&#35774;&#22791;&#21644;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31958;&#23615;&#30149;&#26159;&#20840;&#29699;&#27515;&#20129;&#20027;&#35201;&#21407;&#22240;&#20043;&#19968;&#65292;&#30446;&#21069;&#26080;&#27861;&#27835;&#24840;&#65292;&#22914;&#26524;&#19981;&#27835;&#30103;&#65292;&#21487;&#33021;&#23548;&#33268;&#20005;&#37325;&#30340;&#20581;&#24247;&#24182;&#21457;&#30151;&#65292;&#22914;&#35270;&#32593;&#33180;&#30149;&#21464;&#12289;&#32930;&#20307;&#25130;&#32930;&#12289;&#24515;&#34880;&#31649;&#30142;&#30149;&#21644;&#31070;&#32463;&#30142;&#30149;&#12290;&#22240;&#27492;&#65292;&#37319;&#21462;&#39044;&#38450;&#25514;&#26045;&#20197;&#36991;&#20813;/&#39044;&#27979;&#31958;&#23615;&#30149;&#30340;&#21457;&#29983;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#24050;&#32463;&#25552;&#20986;&#21644;&#35780;&#20272;&#20102;&#29992;&#20110;&#31958;&#23615;&#30149;&#39044;&#27979;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21361;&#38505;&#22240;&#32032;&#30340;&#29289;&#32852;&#32593;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#21306;&#22359;&#38142;&#31995;&#32479;&#36827;&#34892;&#31958;&#23615;&#30149;&#39044;&#27979;&#12290;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#24314;&#31435;&#22312;&#21306;&#22359;&#38142;&#25216;&#26415;&#30340;&#22522;&#30784;&#19978;&#65292;&#20174;&#19981;&#21516;&#21307;&#38498;&#30340;&#24739;&#32773;&#20013;&#33719;&#21462;&#21361;&#38505;&#22240;&#32032;&#25968;&#25454;&#30340;&#25972;&#20307;&#35270;&#22270;&#65292;&#24182;&#30830;&#20445;&#29992;&#25143;&#25968;&#25454;&#30340;&#23433;&#20840;&#24615;&#21644;&#38544;&#31169;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#31995;&#32479;&#20013;&#19981;&#21516;&#30340;&#21307;&#30103;&#20256;&#24863;&#22120;&#12289;&#35774;&#22791;&#21644;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#65292;&#20197;&#27979;&#37327;&#21644;&#25910;&#38598;&#21361;&#38505;&#22240;&#32032;&#30340;&#20540;&#12290;&#36827;&#34892;&#20102;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#21644;&#27604;&#36739;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diabetes Mellitus, one of the leading causes of death worldwide, has no cure to date and can lead to severe health complications, such as retinopathy, limb amputation, cardiovascular diseases, and neuronal disease, if left untreated. Consequently, it becomes crucial to take precautionary measures to avoid/predict the occurrence of diabetes. Machine learning approaches have been proposed and evaluated in the literature for diabetes prediction. This paper proposes an IoT-edge-Artificial Intelligence (AI)-blockchain system for diabetes prediction based on risk factors. The proposed system is underpinned by the blockchain to obtain a cohesive view of the risk factors data from patients across different hospitals and to ensure security and privacy of the user's data. Furthermore, we provide a comparative analysis of different medical sensors, devices, and methods to measure and collect the risk factors values in the system. Numerical experiments and comparative analysis were carried out bet
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#31070;&#32463;&#22270;&#27169;&#22411;&#65288;NGMs&#65289;&#65292;&#23427;&#21487;&#20197;&#20197;&#21512;&#29702;&#30340;&#35745;&#31639;&#25104;&#26412;&#34920;&#31034;&#22797;&#26434;&#30340;&#29305;&#24449;&#20381;&#36182;&#20851;&#31995;&#65292;&#36866;&#24212;&#22810;&#31181;&#22270;&#32467;&#26500;&#21644;&#28151;&#21512;&#36755;&#20837;&#25968;&#25454;&#31867;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#39640;&#25928;&#30340;&#23398;&#20064;&#12289;&#25512;&#26029;&#21644;&#37319;&#26679;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2210.00453</link><description>&lt;p&gt;
&#31070;&#32463;&#22270;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Neural Graphical Models. (arXiv:2210.00453v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.00453
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#31070;&#32463;&#22270;&#27169;&#22411;&#65288;NGMs&#65289;&#65292;&#23427;&#21487;&#20197;&#20197;&#21512;&#29702;&#30340;&#35745;&#31639;&#25104;&#26412;&#34920;&#31034;&#22797;&#26434;&#30340;&#29305;&#24449;&#20381;&#36182;&#20851;&#31995;&#65292;&#36866;&#24212;&#22810;&#31181;&#22270;&#32467;&#26500;&#21644;&#28151;&#21512;&#36755;&#20837;&#25968;&#25454;&#31867;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#39640;&#25928;&#30340;&#23398;&#20064;&#12289;&#25512;&#26029;&#21644;&#37319;&#26679;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#29575;&#22270;&#27169;&#22411;&#32463;&#24120;&#34987;&#29992;&#26469;&#29702;&#35299;&#31995;&#32479;&#30340;&#21160;&#24577;&#12290;&#23427;&#20204;&#21487;&#20197;&#24314;&#27169;&#29305;&#24449;&#65288;&#33410;&#28857;&#65289;&#20043;&#38388;&#30340;&#20851;&#31995;&#21644;&#24213;&#23618;&#20998;&#24067;&#12290;&#29702;&#35770;&#19978;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#34920;&#31034;&#38750;&#24120;&#22797;&#26434;&#30340;&#20381;&#36182;&#20989;&#25968;&#65292;&#20294;&#22312;&#23454;&#36341;&#20013;&#65292;&#30001;&#20110;&#19982;&#22270;&#25805;&#20316;&#30456;&#20851;&#30340;&#35745;&#31639;&#38480;&#21046;&#65292;&#36890;&#24120;&#20250;&#20570;&#31616;&#21270;&#20551;&#35774;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31070;&#32463;&#22270;&#27169;&#22411;&#65288;NGMs&#65289;&#65292;&#35797;&#22270;&#20197;&#21512;&#29702;&#30340;&#35745;&#31639;&#25104;&#26412;&#34920;&#31034;&#22797;&#26434;&#30340;&#29305;&#24449;&#20381;&#36182;&#20851;&#31995;&#12290;&#32473;&#23450;&#29305;&#24449;&#20851;&#31995;&#22270;&#21644;&#30456;&#24212;&#26679;&#26412;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;&#26469;&#25429;&#25417;&#29305;&#24449;&#20043;&#38388;&#30340;&#20381;&#36182;&#32467;&#26500;&#20197;&#21450;&#23427;&#20204;&#30340;&#22797;&#26434;&#20989;&#25968;&#34920;&#31034;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#39640;&#25928;&#30340;&#23398;&#20064;&#12289;&#25512;&#26029;&#21644;&#37319;&#26679;&#31639;&#27861;&#12290;NGMs&#21487;&#20197;&#36866;&#24212;&#36890;&#29992;&#30340;&#22270;&#32467;&#26500;&#65292;&#21253;&#25324;&#26377;&#21521;&#22270;&#12289;&#26080;&#21521;&#22270;&#21644;&#28151;&#21512;&#36793;&#22270;&#65292;&#21516;&#26102;&#25903;&#25345;&#28151;&#21512;&#36755;&#20837;&#25968;&#25454;&#31867;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#32463;&#39564;&#30740;&#31350;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;NGMs&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Graphical Models are often used to understand dynamics of a system. They can model relationships between features (nodes) and the underlying distribution. Theoretically these models can represent very complex dependency functions, but in practice often simplifying assumptions are made due to computational limitations associated with graph operations. In this work we introduce Neural Graphical Models (NGMs) which attempt to represent complex feature dependencies with reasonable computational costs. Given a graph of feature relationships and corresponding samples, we capture the dependency structure between the features along with their complex function representations by using a neural network as a multi-task learning framework. We provide efficient learning, inference and sampling algorithms. NGMs can fit generic graph structures including directed, undirected and mixed-edge graphs as well as support mixed input data types. We present empirical studies that show NGMs' cap
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#21644;&#36890;&#20449;&#24310;&#36831;&#20272;&#35745;&#20248;&#21270;&#26694;&#26550;&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#21160;&#24577;&#20998;&#37197;&#36793;&#32536;&#30340;&#24863;&#30693;&#21644;&#35745;&#31639;&#36164;&#28304;&#65292;&#20174;&#32780;&#20351;&#26234;&#33021;&#20256;&#24863;&#22120;&#32593;&#32476;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#35745;&#31639;&#20013;&#23454;&#29616;&#26356;&#20339;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2209.02166</link><description>&lt;p&gt;
&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#35745;&#31639;&#20013;&#36827;&#34892;&#33258;&#36866;&#24212;&#26234;&#33021;&#20256;&#24863;&#65306;&#35745;&#31639;&#36824;&#26159;&#19981;&#35745;&#31639;&#65311;
&lt;/p&gt;
&lt;p&gt;
To Compute or not to Compute? Adaptive Smart Sensing in Resource-Constrained Edge Computing. (arXiv:2209.02166v2 [cs.DC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.02166
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#21644;&#36890;&#20449;&#24310;&#36831;&#20272;&#35745;&#20248;&#21270;&#26694;&#26550;&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#21160;&#24577;&#20998;&#37197;&#36793;&#32536;&#30340;&#24863;&#30693;&#21644;&#35745;&#31639;&#36164;&#28304;&#65292;&#20174;&#32780;&#20351;&#26234;&#33021;&#20256;&#24863;&#22120;&#32593;&#32476;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#35745;&#31639;&#20013;&#23454;&#29616;&#26356;&#20339;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#20010;&#26234;&#33021;&#20256;&#24863;&#22120;&#32593;&#32476;&#65292;&#24212;&#29992;&#20110;&#36793;&#32536;&#35745;&#31639;&#20013;&#65292;&#23545;&#24863;&#20852;&#36259;&#30340;&#20449;&#21495;&#36827;&#34892;&#37319;&#26679;&#24182;&#21521;&#22522;&#31449;&#21457;&#36865;&#26356;&#26032;&#20197;&#36827;&#34892;&#36828;&#31243;&#20840;&#23616;&#30417;&#25511;&#12290;&#20256;&#24863;&#22120;&#37197;&#22791;&#26377;&#24863;&#30693;&#21644;&#35745;&#31639;&#21151;&#33021;&#65292;&#21487;&#20197;&#22312;&#20256;&#36755;&#20043;&#21069;&#22312;&#26495;&#19978;&#22788;&#29702;&#21407;&#22987;&#25968;&#25454;&#25110;&#30452;&#25509;&#21457;&#36865;&#21407;&#22987;&#25968;&#25454;&#12290;&#36793;&#32536;&#30340;&#30828;&#20214;&#36164;&#28304;&#26377;&#38480;&#65292;&#20135;&#29983;&#20102;&#22522;&#26412;&#30340;&#24310;&#36831; - &#31934;&#24230;&#26435;&#34913;&#65306;&#21407;&#22987;&#27979;&#37327;&#19981;&#20934;&#30830;&#20294;&#21450;&#26102;&#65292;&#32780;&#32463;&#36807;&#35745;&#31639;&#24310;&#36831;&#21518;&#65292;&#20934;&#30830;&#30340;&#22788;&#29702;&#26356;&#26032;&#23601;&#21487;&#29992;&#12290;&#21478;&#22806;&#65292;&#22914;&#26524;&#20256;&#24863;&#22120;&#26495;&#19978;&#22788;&#29702;&#28041;&#21450;&#25968;&#25454;&#21387;&#32553;&#65292;&#21017;&#30001;&#20110;&#26080;&#32447;&#36890;&#20449;&#24341;&#36215;&#30340;&#24310;&#36831;&#21487;&#33021;&#20250;&#26356;&#39640;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#20915;&#23450;&#20309;&#26102;&#20256;&#24863;&#22120;&#24212;&#35813;&#20256;&#36755;&#21407;&#22987;&#27979;&#37327;&#25968;&#25454;&#25110;&#20381;&#36182;&#26412;&#22320;&#22788;&#29702;&#20197;&#26368;&#22823;&#21270;&#25972;&#20307;&#32593;&#32476;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#20256;&#24863;&#35774;&#35745;&#38382;&#39064;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#23884;&#20837;&#35745;&#31639;&#21644;&#36890;&#20449;&#24310;&#36831;&#30340;&#20272;&#35745;&#20248;&#21270;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#21160;&#24577;&#20998;&#37197;&#36793;&#32536;&#30340;&#24863;&#30693;&#21644;&#35745;&#31639;&#36164;&#28304;&#12290;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#36890;&#36807;&#23454;&#29616;&#26356;&#20302;&#30340;&#20272;&#35745;&#35823;&#24046;&#12289;&#26356;&#39640;&#30340;&#21534;&#21520;&#37327;&#21644;&#26356;&#20302;&#30340;&#33021;&#32791;&#65292;&#20248;&#20110;&#20256;&#32479;&#30340;&#20256;&#36755;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider a network of smart sensors for edge computing application that sample a signal of interest and send updates to a base station for remote global monitoring. Sensors are equipped with sensing and compute, and can either send raw data or process them on-board before transmission. Limited hardware resources at the edge generate a fundamental latency-accuracy trade-off: raw measurements are inaccurate but timely, whereas accurate processed updates are available after computational delay. Also, if sensor on-board processing entails data compression, latency caused by wireless communication might be higher for raw measurements. Hence, one needs to decide when sensors should transmit raw measurements or rely on local processing to maximize overall network performance. To tackle this sensing design problem, we model an estimation-theoretic optimization framework that embeds computation and communication delays, and propose a Reinforcement Learning-based approach to dynamically alloc
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GHN-Q&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22270;&#24418;&#36229;&#32593;&#32476;&#26469;&#39044;&#27979;&#26410;&#35265;&#37327;&#21270;&#21367;&#31215;&#26550;&#26500;&#30340;&#21442;&#25968;&#65292;&#20197;&#25552;&#39640;&#37327;&#21270;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2208.12489</link><description>&lt;p&gt;
GHN-Q&#65306;&#36890;&#36807;&#22270;&#24418;&#36229;&#32593;&#32476;&#39044;&#27979;&#26410;&#35265;&#37327;&#21270;&#21367;&#31215;&#26550;&#26500;&#30340;&#21442;&#25968;
&lt;/p&gt;
&lt;p&gt;
GHN-Q: Parameter Prediction for Unseen Quantized Convolutional Architectures via Graph Hypernetworks. (arXiv:2208.12489v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.12489
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GHN-Q&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22270;&#24418;&#36229;&#32593;&#32476;&#26469;&#39044;&#27979;&#26410;&#35265;&#37327;&#21270;&#21367;&#31215;&#26550;&#26500;&#30340;&#21442;&#25968;&#65292;&#20197;&#25552;&#39640;&#37327;&#21270;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#36890;&#36807;&#36845;&#20195;&#20248;&#21270;&#35757;&#32451;&#24050;&#32463;&#21462;&#24471;&#20102;&#20196;&#20154;&#38590;&#20197;&#32622;&#20449;&#30340;&#25104;&#21151;&#65292;&#25214;&#21040;&#20102;&#26368;&#20339;&#21442;&#25968;&#12290;&#28982;&#32780;&#65292;&#29616;&#20195;CNN&#26550;&#26500;&#36890;&#24120;&#21253;&#21547;&#25968;&#30334;&#19975;&#20010;&#21442;&#25968;&#12290;&#22240;&#27492;&#65292;&#23545;&#20110;&#21333;&#20010;&#26550;&#26500;&#30340;&#20219;&#20309;&#32473;&#23450;&#27169;&#22411;&#37117;&#23384;&#22312;&#19968;&#20010;&#24222;&#22823;&#30340;&#21442;&#25968;&#31354;&#38388;&#12290;&#20855;&#26377;&#30456;&#20284;&#25439;&#22833;&#30340;&#27169;&#22411;&#21487;&#33021;&#20855;&#26377;&#25130;&#28982;&#19981;&#21516;&#30340;&#29305;&#24615;&#65292;&#22914;&#23545;&#25239;&#40065;&#26834;&#24615;&#12289;&#27867;&#21270;&#33021;&#21147;&#21644;&#37327;&#21270;&#40065;&#26834;&#24615;&#12290;&#23545;&#20110;&#36793;&#32536;&#19978;&#30340;&#28145;&#24230;&#23398;&#20064;&#65292;&#37327;&#21270;&#40065;&#26834;&#24615;&#36890;&#24120;&#33267;&#20851;&#37325;&#35201;&#12290;&#25214;&#21040;&#19968;&#20010;&#37327;&#21270;&#40065;&#26834;&#30340;&#27169;&#22411;&#26377;&#26102;&#21487;&#33021;&#38656;&#35201;&#24456;&#22823;&#30340;&#21162;&#21147;&#12290;&#26368;&#36817;&#20351;&#29992;&#22270;&#24418;&#36229;&#32593;&#32476;&#65288;GHN&#65289;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23427;&#22312;&#39044;&#27979;&#19981;&#21516;CNN&#26550;&#26500;&#30340;&#39640;&#24615;&#33021;&#21442;&#25968;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#21463;&#21040;&#36825;&#20123;&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24819;&#30693;&#36947;GHN-2&#30340;&#22270;&#24418;&#34920;&#31034;&#26159;&#21542;&#20063;&#21487;&#20197;&#29992;&#20110;&#39044;&#27979;&#37327;&#21270;&#40065;&#26834;&#30340;&#21442;&#25968;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;GHN-Q&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#26377;&#21490;&#20197;&#26469;&#31532;&#19968;&#27425;&#25506;&#32034;&#20351;&#29992;&#22270;&#24418;&#36229;&#32593;&#32476;&#26469;&#39044;&#27979;&#37327;&#21270;&#40065;&#26834;&#21442;&#25968;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep convolutional neural network (CNN) training via iterative optimization has had incredible success in finding optimal parameters. However, modern CNN architectures often contain millions of parameters. Thus, any given model for a single architecture resides in a massive parameter space. Models with similar loss could have drastically different characteristics such as adversarial robustness, generalizability, and quantization robustness. For deep learning on the edge, quantization robustness is often crucial. Finding a model that is quantization-robust can sometimes require significant efforts. Recent works using Graph Hypernetworks (GHN) have shown remarkable performance predicting high-performant parameters of varying CNN architectures. Inspired by these successes, we wonder if the graph representations of GHN-2 can be leveraged to predict quantization-robust parameters as well, which we call GHN-Q. We conduct the first-ever study exploring the use of graph hypernetworks for predi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#27491;&#24335;&#21512;&#21516;&#30340;&#27010;&#24565;&#65292;&#35299;&#20915;&#20102;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#20010;&#20307;&#28608;&#21169;&#21644;&#38598;&#20307;&#28608;&#21169;&#20998;&#27495;&#23548;&#33268;&#30340;&#27425;&#20248;&#34892;&#20026;&#38382;&#39064;&#12290;&#29702;&#35770;&#21644;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#22312;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#20013;&#24341;&#20837;&#26377;&#32422;&#26463;&#30340;&#29366;&#24577;&#20381;&#36182;&#22870;&#21169;&#36716;&#31227;&#65292;&#23454;&#29616;&#20102;&#25152;&#26377;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#30340;&#23376;&#21338;&#24328;&#23436;&#32654;&#22343;&#34913;&#34920;&#29616;&#20986;&#31038;&#20250;&#26368;&#20248;&#34892;&#20026;&#65292;&#24182;&#25552;&#21319;&#20102;&#31639;&#27861;&#30340;&#31038;&#20250;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2208.10469</link><description>&lt;p&gt;
&#20889;&#19979;&#26469;&#21543;&#65306;&#27491;&#24335;&#21512;&#21516;&#32531;&#35299;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#31038;&#20250;&#22256;&#22659;
&lt;/p&gt;
&lt;p&gt;
Get It in Writing: Formal Contracts Mitigate Social Dilemmas in Multi-Agent RL. (arXiv:2208.10469v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.10469
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#27491;&#24335;&#21512;&#21516;&#30340;&#27010;&#24565;&#65292;&#35299;&#20915;&#20102;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#20010;&#20307;&#28608;&#21169;&#21644;&#38598;&#20307;&#28608;&#21169;&#20998;&#27495;&#23548;&#33268;&#30340;&#27425;&#20248;&#34892;&#20026;&#38382;&#39064;&#12290;&#29702;&#35770;&#21644;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#22312;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#20013;&#24341;&#20837;&#26377;&#32422;&#26463;&#30340;&#29366;&#24577;&#20381;&#36182;&#22870;&#21169;&#36716;&#31227;&#65292;&#23454;&#29616;&#20102;&#25152;&#26377;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#30340;&#23376;&#21338;&#24328;&#23436;&#32654;&#22343;&#34913;&#34920;&#29616;&#20986;&#31038;&#20250;&#26368;&#20248;&#34892;&#20026;&#65292;&#24182;&#25552;&#21319;&#20102;&#31639;&#27861;&#30340;&#31038;&#20250;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#26159;&#35757;&#32451;&#22312;&#20849;&#21516;&#29615;&#22659;&#20013;&#29420;&#31435;&#34892;&#21160;&#30340;&#33258;&#21160;&#21270;&#31995;&#32479;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#24403;&#20010;&#20307;&#28608;&#21169;&#21644;&#38598;&#20307;&#28608;&#21169;&#20986;&#29616;&#20998;&#27495;&#26102;&#65292;&#23427;&#21487;&#33021;&#23548;&#33268;&#27425;&#20248;&#34892;&#20026;&#12290;&#20154;&#31867;&#22312;&#35299;&#20915;&#36825;&#20123;&#31038;&#20250;&#22256;&#22659;&#26041;&#38754;&#20855;&#26377;&#38750;&#20961;&#30340;&#33021;&#21147;&#12290;&#22312;MARL&#20013;&#22797;&#21046;&#36825;&#31181;&#21512;&#20316;&#34892;&#20026;&#23545;&#20110;&#33258;&#31169;&#30340;&#26234;&#33021;&#20307;&#26469;&#35828;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20511;&#37492;&#20102;&#32463;&#27982;&#23398;&#20013;&#27491;&#24335;&#21512;&#21516;&#30340;&#24605;&#24819;&#65292;&#20197;&#20811;&#26381;MARL&#20013;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#28608;&#21169;&#20998;&#27495;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#36827;&#34892;&#22686;&#24378;&#30340;&#26041;&#27861;&#65292;&#26234;&#33021;&#20307;&#33258;&#24895;&#21516;&#24847;&#22312;&#39044;&#20808;&#35268;&#23450;&#30340;&#26465;&#20214;&#19979;&#36827;&#34892;&#26377;&#32422;&#26463;&#30340;&#29366;&#24577;&#20381;&#36182;&#22870;&#21169;&#36716;&#31227;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26159;&#29702;&#35770;&#30340;&#21644;&#23454;&#35777;&#30340;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#22686;&#24378;&#20351;&#24471;&#25152;&#26377;&#23436;&#20840;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#30340;&#23376;&#21338;&#24328;&#23436;&#32654;&#22343;&#34913;&#37117;&#34920;&#29616;&#20986;&#31038;&#20250;&#26368;&#20248;&#34892;&#20026;&#65292;&#21482;&#35201;&#21512;&#21516;&#31354;&#38388;&#36275;&#22815;&#20016;&#23500;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#26368;&#20808;&#36827;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#22686;&#24378;&#21518;&#30340;MARL&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#31038;&#20250;&#24615;&#33021;&#26469;&#34917;&#20805;&#25105;&#20204;&#30340;&#21338;&#24328;&#35770;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-agent reinforcement learning (MARL) is a powerful tool for training automated systems acting independently in a common environment. However, it can lead to sub-optimal behavior when individual incentives and group incentives diverge. Humans are remarkably capable at solving these social dilemmas. It is an open problem in MARL to replicate such cooperative behaviors in selfish agents. In this work, we draw upon the idea of formal contracting from economics to overcome diverging incentives between agents in MARL. We propose an augmentation to a Markov game where agents voluntarily agree to binding state-dependent transfers of reward, under pre-specified conditions. Our contributions are theoretical and empirical. First, we show that this augmentation makes all subgame-perfect equilibria of all fully observed Markov games exhibit socially optimal behavior, given a sufficiently rich space of contracts. Next, we complement our game-theoretic analysis by showing that state-of-the-art R
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#25233;&#21046;&#24615;/&#36127;&#21521;&#36830;&#25509;&#26159;&#20026;&#20102;&#23398;&#20064;&#26356;&#22810;&#30340;&#21151;&#33021;&#65292;&#36127;&#26435;&#37325;&#22312;&#34920;&#24449;&#33021;&#21147;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#24182;&#19988;&#38750;&#36127;&#28145;&#24230;&#32593;&#32476;&#26080;&#27861;&#34920;&#31034;&#26576;&#20123;&#34920;&#24449;&#31354;&#38388;&#30340;&#20960;&#20309;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2208.03211</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;&#32593;&#32476;&#20855;&#26377;&#25233;&#21046;&#24615;/&#36127;&#21521;&#36830;&#25509;&#65311;
&lt;/p&gt;
&lt;p&gt;
Why do networks have inhibitory/negative connections?. (arXiv:2208.03211v8 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.03211
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#25233;&#21046;&#24615;/&#36127;&#21521;&#36830;&#25509;&#26159;&#20026;&#20102;&#23398;&#20064;&#26356;&#22810;&#30340;&#21151;&#33021;&#65292;&#36127;&#26435;&#37325;&#22312;&#34920;&#24449;&#33021;&#21147;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#24182;&#19988;&#38750;&#36127;&#28145;&#24230;&#32593;&#32476;&#26080;&#27861;&#34920;&#31034;&#26576;&#20123;&#34920;&#24449;&#31354;&#38388;&#30340;&#20960;&#20309;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#33041;&#20026;&#20160;&#20040;&#20855;&#26377;&#25233;&#21046;&#24615;&#36830;&#25509;&#65311;&#28145;&#24230;&#32593;&#32476;&#20026;&#20160;&#20040;&#20855;&#26377;&#36127;&#26435;&#37325;&#65311;&#25105;&#20204;&#20174;&#34920;&#24449;&#33021;&#21147;&#30340;&#35282;&#24230;&#25552;&#20986;&#20102;&#19968;&#20010;&#31572;&#26696;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#22312;&#33258;&#28982;&#26234;&#33021;&#20013;&#65292;&#22823;&#33041;&#30340;&#20027;&#35201;&#20316;&#29992;&#26159;&#34920;&#24449;&#21151;&#33021;&#65292;&#22312;&#20154;&#24037;&#26234;&#33021;&#20013;&#65292;&#28145;&#24230;&#32593;&#32476;&#30340;&#20027;&#35201;&#20316;&#29992;&#20063;&#26159;&#22914;&#27492;&#12290;&#25105;&#20204;&#30340;&#31572;&#26696;&#26159;&#20026;&#20160;&#20040;&#23384;&#22312;&#25233;&#21046;&#24615;/&#36127;&#21521;&#26435;&#37325;&#65306;&#20026;&#20102;&#23398;&#20064;&#26356;&#22810;&#30340;&#21151;&#33021;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#65292;&#22312;&#27809;&#26377;&#36127;&#26435;&#37325;&#30340;&#24773;&#20917;&#19979;&#65292;&#20855;&#26377;&#38750;&#36882;&#22686;&#28608;&#27963;&#20989;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#26080;&#27861;&#25104;&#20026;&#26222;&#36866;&#36817;&#20284;&#22120;&#12290;&#23613;&#31649;&#36825;&#21487;&#33021;&#23545;&#19968;&#20123;&#20154;&#26469;&#35828;&#26159;&#19968;&#31181;&#30452;&#35266;&#30340;&#32467;&#26524;&#65292;&#20294;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#26080;&#35770;&#26159;&#22312;&#26426;&#22120;&#23398;&#20064;&#36824;&#26159;&#31070;&#32463;&#31185;&#23398;&#39046;&#22495;&#65292;&#37117;&#27809;&#26377;&#25552;&#20379;&#27491;&#24335;&#29702;&#35770;&#26469;&#35777;&#26126;&#20026;&#20160;&#20040;&#22312;&#34920;&#24449;&#33021;&#21147;&#30340;&#32972;&#26223;&#19979;&#65292;&#36127;&#26435;&#37325;&#33267;&#20851;&#37325;&#35201;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#38750;&#36127;&#28145;&#24230;&#32593;&#32476;&#26080;&#27861;&#34920;&#31034;&#30340;&#34920;&#24449;&#31354;&#38388;&#30340;&#20960;&#20309;&#29305;&#24615;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#20123;&#35265;&#35299;&#33021;&#22815;&#24102;&#26469;&#23545;&#26356;&#22797;&#26434;&#30340;&#24402;&#32435;&#36807;&#31243;&#30340;&#26356;&#28145;&#20837;&#20102;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Why do brains have inhibitory connections? Why do deep networks have negative weights? We propose an answer from the perspective of representation capacity. We believe representing functions is the primary role of both (i) the brain in natural intelligence, and (ii) deep networks in artificial intelligence. Our answer to why there are inhibitory/negative weights is: to learn more functions. We prove that, in the absence of negative weights, neural networks with non-decreasing activation functions are not universal approximators. While this may be an intuitive result to some, to the best of our knowledge, there is no formal theory, in either machine learning or neuroscience, that demonstrates why negative weights are crucial in the context of representation capacity. Further, we provide insights on the geometric properties of the representation space that non-negative deep networks cannot represent. We expect these insights will yield a deeper understanding of more sophisticated inducti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#22312;&#36229;&#22797;&#25968;&#31354;&#38388;&#20013;&#25972;&#21512;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#21644;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#36229;&#22797;&#25968;&#20195;&#25968;&#26469;&#34920;&#31034;&#21333;&#27169;&#24577;&#23884;&#20837;&#20197;&#21450;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#24182;&#19988;&#33021;&#22815;&#26356;&#22909;&#22320;&#21033;&#29992;&#32467;&#26500;&#24615;&#21644;&#25991;&#26412;&#24615;&#30693;&#35782;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2208.02743</link><description>&lt;p&gt;
&#22312;&#36229;&#22797;&#25968;&#31354;&#38388;&#20013;&#25972;&#21512;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#21644;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Integrating Knowledge Graph embedding and pretrained Language Models in Hypercomplex Spaces. (arXiv:2208.02743v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.02743
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#22312;&#36229;&#22797;&#25968;&#31354;&#38388;&#20013;&#25972;&#21512;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#21644;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#36229;&#22797;&#25968;&#20195;&#25968;&#26469;&#34920;&#31034;&#21333;&#27169;&#24577;&#23884;&#20837;&#20197;&#21450;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#24182;&#19988;&#33021;&#22815;&#26356;&#22909;&#22320;&#21033;&#29992;&#32467;&#26500;&#24615;&#21644;&#25991;&#26412;&#24615;&#30693;&#35782;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#22914;Wikidata&#22312;&#34920;&#31034;&#30693;&#35782;&#26102;&#21253;&#21547;&#20102;&#32467;&#26500;&#24615;&#21644;&#25991;&#26412;&#24615;&#30693;&#35782;&#12290;&#38024;&#23545;&#36825;&#20004;&#31181;&#27169;&#24577;&#65292;&#19987;&#38376;&#30340;&#22270;&#23884;&#20837;&#21644;&#35821;&#35328;&#27169;&#22411;&#26041;&#27861;&#23398;&#20064;&#20102;&#33021;&#22815;&#39044;&#27979;&#26032;&#30340;&#32467;&#26500;&#24615;&#30693;&#35782;&#30340;&#27169;&#24335;&#12290;&#30446;&#21069;&#21482;&#26377;&#23569;&#25968;&#26041;&#27861;&#23558;&#23398;&#20064;&#21644;&#25512;&#29702;&#19982;&#20004;&#31181;&#27169;&#24577;&#25972;&#21512;&#36215;&#26469;&#65292;&#32780;&#19988;&#29616;&#26377;&#26041;&#27861;&#21482;&#33021;&#37096;&#20998;&#22320;&#21033;&#29992;&#32467;&#26500;&#24615;&#21644;&#25991;&#26412;&#24615;&#30693;&#35782;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#29616;&#26377;&#24378;&#22823;&#30340;&#21333;&#27169;&#24577;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#36229;&#22797;&#25968;&#20195;&#25968;&#34920;&#31034;&#21333;&#27169;&#24577;&#23884;&#20837;&#20197;&#21450;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#20197;&#21450;&#23427;&#20204;&#20316;&#20026;&#30693;&#35782;&#34920;&#31034;&#30340;&#20114;&#34917;&#25163;&#27573;&#30340;&#20132;&#20114;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#22235;&#32500;&#36229;&#22797;&#25968;&#30340;&#20108;&#38754;&#20307;&#21644;&#22235;&#20803;&#25968;&#34920;&#31034;&#26469;&#25972;&#21512;&#22235;&#31181;&#27169;&#24577;&#65292;&#21363;&#32467;&#26500;&#24615;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#12289;&#35789;&#32423;&#34920;&#31034;&#65288;&#20363;&#22914;Word2vec&#12289;Fasttext&#65289;&#12289;&#21477;&#32423;&#34920;&#31034;&#65288;Sen
&lt;/p&gt;
&lt;p&gt;
Knowledge Graphs, such as Wikidata, comprise structural and textual knowledge in order to represent knowledge. For each of the two modalities dedicated approaches for graph embedding and language models learn patterns that allow for predicting novel structural knowledge. Few approaches have integrated learning and inference with both modalities and these existing ones could only partially exploit the interaction of structural and textual knowledge. In our approach, we build on existing strong representations of single modalities and we use hypercomplex algebra to represent both, (i), single-modality embedding as well as, (ii), the interaction between different modalities and their complementary means of knowledge representation. More specifically, we suggest Dihedron and Quaternion representations of 4D hypercomplex numbers to integrate four modalities namely structural knowledge graph embedding, word-level representations (e.g.\ Word2vec, Fasttext), sentence-level representations (Sen
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23450;&#37327;&#26041;&#27861;&#8212;&#8212;&#27169;&#31946;&#26631;&#31614;&#21270;&#65292;&#29992;&#20110;&#35780;&#20272;&#27169;&#31946;&#35770;&#35777;&#31995;&#32479;&#20013;&#30340;&#35770;&#35777;&#24378;&#24230;&#12290;&#36890;&#36807;&#20351;&#29992;&#21487;&#25509;&#21463;&#24230;&#12289;&#21487;&#25298;&#32477;&#24230;&#21644;&#19981;&#30830;&#23450;&#24230;&#19977;&#20010;&#31243;&#24230;&#65292;&#35813;&#26041;&#27861;&#20026;&#30028;&#23450;&#35770;&#35777;&#24378;&#24230;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#65292;&#24182;&#23545;&#35770;&#35777;&#30340;&#29366;&#24577;&#36827;&#34892;&#20102;&#28145;&#20837;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2207.07339</link><description>&lt;p&gt;
&#29992;&#20110;&#23450;&#37327;&#35770;&#35777;&#30340;&#27169;&#31946;&#26631;&#31614;&#35821;&#20041;
&lt;/p&gt;
&lt;p&gt;
Fuzzy Labeling Semantics for Quantitative Argumentation. (arXiv:2207.07339v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.07339
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23450;&#37327;&#26041;&#27861;&#8212;&#8212;&#27169;&#31946;&#26631;&#31614;&#21270;&#65292;&#29992;&#20110;&#35780;&#20272;&#27169;&#31946;&#35770;&#35777;&#31995;&#32479;&#20013;&#30340;&#35770;&#35777;&#24378;&#24230;&#12290;&#36890;&#36807;&#20351;&#29992;&#21487;&#25509;&#21463;&#24230;&#12289;&#21487;&#25298;&#32477;&#24230;&#21644;&#19981;&#30830;&#23450;&#24230;&#19977;&#20010;&#31243;&#24230;&#65292;&#35813;&#26041;&#27861;&#20026;&#30028;&#23450;&#35770;&#35777;&#24378;&#24230;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#65292;&#24182;&#23545;&#35770;&#35777;&#30340;&#29366;&#24577;&#36827;&#34892;&#20102;&#28145;&#20837;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25277;&#35937;&#35770;&#35777;&#39046;&#22495;&#20013;&#65292;&#35780;&#20272;&#23450;&#37327;&#35770;&#35777;&#31995;&#32479;&#20013;&#30340;&#35770;&#35777;&#24378;&#24230;&#26085;&#30410;&#21463;&#21040;&#20851;&#27880;&#12290;&#21487;&#25509;&#21463;&#24230;&#27010;&#24565;&#22312;&#28176;&#36827;&#35821;&#20041;&#20013;&#34987;&#24191;&#27867;&#37319;&#29992;&#65292;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#21487;&#33021;&#19981;&#36275;&#22815;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23450;&#37327;&#26041;&#27861;&#65292;&#31216;&#20026;&#27169;&#31946;&#26631;&#31614;&#21270;&#65292;&#29992;&#20110;&#27169;&#31946;&#35770;&#35777;&#31995;&#32479;&#65292;&#20854;&#20013;&#20351;&#29992;&#21487;&#25509;&#21463;&#24230;&#12289;&#21487;&#25298;&#32477;&#24230;&#21644;&#19981;&#30830;&#23450;&#24230;&#19977;&#20010;&#31243;&#24230;&#35780;&#20272;&#35770;&#35777;&#30340;&#24378;&#24230;&#12290;&#36825;&#31181;&#35774;&#23450;&#20026;&#30028;&#23450;&#35770;&#35777;&#24378;&#24230;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#65292;&#24182;&#28145;&#20837;&#29702;&#35299;&#20102;&#35770;&#35777;&#30340;&#29366;&#24577;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#27169;&#31946;&#26631;&#31614;&#30340;&#20551;&#35774;&#65292;&#36825;&#20123;&#20551;&#35774;&#25552;&#20986;&#20102;&#20851;&#20110;&#21487;&#25509;&#21463;&#24230;&#12289;&#21487;&#25298;&#32477;&#24230;&#21644;&#19981;&#30830;&#23450;&#24230;&#30340;&#35821;&#20041;&#30340;&#21512;&#29702;&#24615;&#35201;&#27714;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31867;&#31526;&#21512;&#19978;&#36848;&#20551;&#35774;&#30340;&#27169;&#31946;&#26631;&#31614;&#35821;&#20041;&#65292;&#24182;&#30740;&#31350;&#20102;&#27169;&#31946;&#26631;&#31614;&#35821;&#20041;&#19982;&#29616;&#26377;&#24037;&#20316;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluating argument strength in quantitative argumentation systems has received increasing attention in the field of abstract argumentation. The concept of acceptability degree is widely adopted in gradual semantics, however, it may not be sufficient in many practical applications. In this paper, we provide a novel quantitative method called fuzzy labeling for fuzzy argumentation systems, in which a triple of acceptability, rejectability, and undecidability degrees is used to evaluate argument strength. Such a setting sheds new light on defining argument strength and provides a deeper understanding of the status of arguments. More specifically, we investigate the postulates of fuzzy labeling, which present the rationality requirements for semantics concerning the acceptability, rejectability, and undecidability degrees. We then propose a class of fuzzy labeling semantics conforming to the above postulates and investigate the relations between fuzzy labeling semantics and existing work 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#32452;&#21512;&#23567;&#22411;&#38750;&#21487;&#20998;&#31163;&#31243;&#24207;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#36923;&#36753;&#31243;&#24207;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#23398;&#20064;&#26102;&#38388;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2206.01614</link><description>&lt;p&gt;
&#36890;&#36807;&#32452;&#21512;&#31243;&#24207;&#23398;&#20064;&#36923;&#36753;&#31243;&#24207;
&lt;/p&gt;
&lt;p&gt;
Learning logic programs by combining programs. (arXiv:2206.01614v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.01614
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#32452;&#21512;&#23567;&#22411;&#38750;&#21487;&#20998;&#31163;&#31243;&#24207;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#36923;&#36753;&#31243;&#24207;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#23398;&#20064;&#26102;&#38388;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;&#30340;&#30446;&#26631;&#26159;&#24402;&#32435;&#20986;&#19968;&#20010;&#36923;&#36753;&#31243;&#24207;&#65288;&#19968;&#32452;&#36923;&#36753;&#35268;&#21017;&#65289;&#65292;&#20197;&#27010;&#25324;&#35757;&#32451;&#26679;&#20363;&#12290;&#24402;&#32435;&#20855;&#26377;&#22810;&#20010;&#35268;&#21017;&#21644;&#25991;&#23383;&#30340;&#31243;&#24207;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#23398;&#20064;&#23567;&#22411;&#30340;&#19981;&#21487;&#20998;&#31163;&#30340;&#31243;&#24207;&#65292;&#24182;&#23558;&#23427;&#20204;&#32452;&#21512;&#36215;&#26469;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#22522;&#20110;&#32422;&#26463;&#30340;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;&#31995;&#32479;&#20013;&#23454;&#29616;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#23398;&#20064;&#26368;&#20248;&#21644;&#36882;&#24402;&#31243;&#24207;&#65292;&#24182;&#36827;&#34892;&#35859;&#35789;&#21457;&#26126;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#39046;&#22495;&#65288;&#21253;&#25324;&#28216;&#25103;&#29609;&#27861;&#21644;&#31243;&#24207;&#21512;&#25104;&#65289;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#23398;&#20064;&#26102;&#38388;&#26041;&#38754;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#26377;&#26174;&#33879;&#30340;&#20248;&#21183;&#65292;&#26377;&#26102;&#23558;&#23398;&#20064;&#26102;&#38388;&#20174;&#19968;&#20010;&#23567;&#26102;&#38477;&#20302;&#21040;&#20960;&#31186;&#38047;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of inductive logic programming is to induce a logic program (a set of logical rules) that generalises training examples. Inducing programs with many rules and literals is a major challenge. To tackle this challenge, we introduce an approach where we learn small non-separable programs and combine them. We implement our approach in a constraint-driven ILP system. Our approach can learn optimal and recursive programs and perform predicate invention. Our experiments on multiple domains, including game playing and program synthesis, show that our approach can drastically outperform existing approaches in terms of predictive accuracies and learning times, sometimes reducing learning times from over an hour to a few seconds.
&lt;/p&gt;</description></item><item><title>IDEAL&#26159;&#19968;&#31181;&#26080;&#38656;&#30495;&#23454;&#25968;&#25454;&#30340;&#39640;&#25928;&#26597;&#35810;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#40657;&#30418;&#27169;&#22411;API&#20013;&#23398;&#20064;&#24182;&#35757;&#32451;&#19968;&#20010;&#20248;&#31168;&#30340;&#23398;&#29983;&#27169;&#22411;&#12290;&#23427;&#36890;&#36807;&#22312;&#20004;&#20010;&#38454;&#27573;&#36827;&#34892;&#35757;&#32451;&#65292;&#21363;&#25968;&#25454;&#29983;&#25104;&#21644;&#27169;&#22411;&#33976;&#39311;&#65292;&#21482;&#38656;&#35201;&#23545;&#25945;&#24072;&#27169;&#22411;&#36827;&#34892;&#23569;&#37327;&#30340;&#26597;&#35810;&#12290;</title><link>http://arxiv.org/abs/2205.11158</link><description>&lt;p&gt;
IDEAL: &#26080;&#38656;&#25968;&#25454;&#23398;&#20064;&#40657;&#30418;&#27169;&#22411;&#30340;&#39640;&#25928;&#26597;&#35810;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
IDEAL: Query-Efficient Data-Free Learning from Black-box Models. (arXiv:2205.11158v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.11158
&lt;/p&gt;
&lt;p&gt;
IDEAL&#26159;&#19968;&#31181;&#26080;&#38656;&#30495;&#23454;&#25968;&#25454;&#30340;&#39640;&#25928;&#26597;&#35810;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#40657;&#30418;&#27169;&#22411;API&#20013;&#23398;&#20064;&#24182;&#35757;&#32451;&#19968;&#20010;&#20248;&#31168;&#30340;&#23398;&#29983;&#27169;&#22411;&#12290;&#23427;&#36890;&#36807;&#22312;&#20004;&#20010;&#38454;&#27573;&#36827;&#34892;&#35757;&#32451;&#65292;&#21363;&#25968;&#25454;&#29983;&#25104;&#21644;&#27169;&#22411;&#33976;&#39311;&#65292;&#21482;&#38656;&#35201;&#23545;&#25945;&#24072;&#27169;&#22411;&#36827;&#34892;&#23569;&#37327;&#30340;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#26159;&#19968;&#31181;&#20351;&#29992;&#32463;&#36807;&#33391;&#22909;&#35757;&#32451;&#30340;&#25945;&#24072;&#27169;&#22411;&#24110;&#21161;&#35757;&#32451;&#36731;&#37327;&#32423;&#23398;&#29983;&#27169;&#22411;&#30340;&#20856;&#22411;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;KD&#26041;&#27861;&#35201;&#27714;&#35775;&#38382;&#25945;&#24072;&#30340;&#35757;&#32451;&#25968;&#25454;&#25110;&#27169;&#22411;&#21442;&#25968;&#65292;&#36825;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#22312;&#26080;&#25968;&#25454;&#21644;&#40657;&#30418;&#35774;&#32622;&#19979;&#30740;&#31350;&#20102;KD&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#30340;&#26597;&#35810;&#25945;&#24072;&#27169;&#22411;&#65292;&#36825;&#20250;&#36896;&#25104;&#26174;&#33879;&#30340;&#37329;&#38065;&#21644;&#35745;&#31639;&#25104;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21517;&#20026;IDEAL&#65288;query-effIcient Data-free lEarning from blAck-box modeLs&#65289;&#65292;&#26088;&#22312;&#36890;&#36807;&#26597;&#35810;&#40657;&#30418;&#27169;&#22411;API&#39640;&#25928;&#22320;&#20174;&#20013;&#23398;&#20064;&#65292;&#24182;&#19988;&#26080;&#38656;&#20219;&#20309;&#30495;&#23454;&#25968;&#25454;&#26469;&#35757;&#32451;&#19968;&#20010;&#22909;&#30340;&#23398;&#29983;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;IDEAL&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#36827;&#34892;&#23398;&#29983;&#27169;&#22411;&#30340;&#35757;&#32451;&#65306;&#25968;&#25454;&#29983;&#25104;&#21644;&#27169;&#22411;&#33976;&#39311;&#12290;&#35831;&#27880;&#24847;&#65292;IDEAL&#19981;&#38656;&#35201;&#22312;&#25968;&#25454;&#29983;&#25104;&#38454;&#27573;&#36827;&#34892;&#20219;&#20309;&#26597;&#35810;&#65292;&#24182;&#19988;&#22312;&#33976;&#39311;&#38454;&#27573;&#23545;&#27599;&#20010;&#26679;&#26412;&#21482;&#26597;&#35810;&#25945;&#24072;&#27169;&#22411;&#19968;&#27425;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge Distillation (KD) is a typical method for training a lightweight student model with the help of a well-trained teacher model. However, most KD methods require access to either the teacher's training data or model parameters, which is unrealistic. To tackle this problem, recent works study KD under data-free and black-box settings. Nevertheless, these works require a large number of queries to the teacher model, which incurs significant monetary and computational costs. To address these problems, we propose a novel method called \emph{query-effIcient Data-free lEarning from blAck-box modeLs} (IDEAL), which aims to query-efficiently learn from black-box model APIs to train a good student without any real data. In detail, IDEAL trains the student model in two stages: data generation and model distillation. Note that IDEAL does not require any query in the data generation stage and queries the teacher only once for each sample in the distillation stage. Extensive experiments on v
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35774;&#35745;&#26368;&#23567;&#25104;&#26412;&#30340;&#24178;&#39044;&#38598;&#21512;&#26469;&#35782;&#21035;&#22240;&#26524;&#25928;&#24212;&#30340;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#38382;&#39064;&#30340;NP-hard&#24615;&#36136;&#65292;&#24182;&#25552;&#20986;&#20102;&#21487;&#20197;&#25214;&#21040;&#26368;&#20248;&#35299;&#25110;&#36817;&#20284;&#35299;&#30340;&#31639;&#27861;&#65292;&#20351;&#29992;&#22810;&#39033;&#24335;&#26102;&#38388;&#21551;&#21457;&#24335;&#31639;&#27861;&#26469;&#35299;&#20915;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#24182;&#22312;&#38543;&#26426;&#22270;&#19978;&#36827;&#34892;&#20102;&#27169;&#25311;&#23454;&#39564;&#39564;&#35777;&#31639;&#27861;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2205.02232</link><description>&lt;p&gt;
&#22240;&#26524;&#25928;&#24212;&#35782;&#21035;&#30340;&#23454;&#39564;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Experimental Design for Causal Effect Identification. (arXiv:2205.02232v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.02232
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35774;&#35745;&#26368;&#23567;&#25104;&#26412;&#30340;&#24178;&#39044;&#38598;&#21512;&#26469;&#35782;&#21035;&#22240;&#26524;&#25928;&#24212;&#30340;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#38382;&#39064;&#30340;NP-hard&#24615;&#36136;&#65292;&#24182;&#25552;&#20986;&#20102;&#21487;&#20197;&#25214;&#21040;&#26368;&#20248;&#35299;&#25110;&#36817;&#20284;&#35299;&#30340;&#31639;&#27861;&#65292;&#20351;&#29992;&#22810;&#39033;&#24335;&#26102;&#38388;&#21551;&#21457;&#24335;&#31639;&#27861;&#26469;&#35299;&#20915;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#24182;&#22312;&#38543;&#26426;&#22270;&#19978;&#36827;&#34892;&#20102;&#27169;&#25311;&#23454;&#39564;&#39564;&#35777;&#31639;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Pearl&#30340;&#20570;&#27861;&#26159;&#19968;&#31181;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#23398;&#20064;&#21487;&#35782;&#21035;&#30340;&#22240;&#26524;&#25928;&#24212;&#30340;&#23436;&#25972;&#20844;&#29702;&#26041;&#27861;&#12290;&#24403;&#36825;&#31181;&#25928;&#24212;&#19981;&#21487;&#35782;&#21035;&#26102;&#65292;&#38656;&#35201;&#25191;&#34892;&#19968;&#31995;&#21015;&#36890;&#24120;&#26114;&#36149;&#30340;&#24178;&#39044;&#26469;&#23398;&#20064;&#22240;&#26524;&#25928;&#24212;&#12290;&#26412;&#30740;&#31350;&#32771;&#34385;&#35774;&#35745;&#26368;&#23567;&#25104;&#26412;&#30340;&#24178;&#39044;&#38598;&#21512;&#26469;&#35782;&#21035;&#25152;&#38656;&#25928;&#24212;&#30340;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#38382;&#39064;&#26159;NP-hard&#30340;&#65292;&#38543;&#21518;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#65292;&#21487;&#20197;&#25214;&#21040;&#26368;&#20248;&#35299;&#25110;&#20854;&#23545;&#25968;&#22240;&#23376;&#30340;&#36817;&#20284;&#35299;&#12290;&#36825;&#26159;&#36890;&#36807;&#24314;&#31435;&#25105;&#20204;&#30340;&#38382;&#39064;&#19982;&#26368;&#23567;&#21629;&#20013;&#38598;&#38382;&#39064;&#20043;&#38388;&#30340;&#32852;&#31995;&#23454;&#29616;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#31181;&#22810;&#39033;&#24335;&#26102;&#38388;&#21551;&#21457;&#24335;&#31639;&#27861;&#26469;&#35299;&#20915;&#38382;&#39064;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;&#23613;&#31649;&#36825;&#20123;&#31639;&#27861;&#21487;&#33021;&#25214;&#21040;&#27425;&#20248;&#35299;&#65292;&#20294;&#25105;&#20204;&#30340;&#27169;&#25311;&#32467;&#26524;&#34920;&#26126;&#23427;&#20204;&#22312;&#38543;&#26426;&#22270;&#19978;&#21462;&#24471;&#20102;&#36739;&#23567;&#30340;&#36951;&#25022;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pearl's do calculus is a complete axiomatic approach to learn the identifiable causal effects from observational data. When such an effect is not identifiable, it is necessary to perform a collection of often costly interventions in the system to learn the causal effect. In this work, we consider the problem of designing the collection of interventions with the minimum cost to identify the desired effect. First, we prove that this problem is NP-hard, and subsequently propose an algorithm that can either find the optimal solution or a logarithmic-factor approximation of it. This is done by establishing a connection between our problem and the minimum hitting set problem. Additionally, we propose several polynomial-time heuristic algorithms to tackle the computational complexity of the problem. Although these algorithms could potentially stumble on sub-optimal solutions, our simulations show that they achieve small regrets on random graphs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#40654;&#26364;&#27969;&#24418;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#26426;&#22120;&#20154;&#36816;&#21160;&#23398;&#20064;&#65292;&#36890;&#36807;&#23398;&#20064;&#40654;&#26364;&#24230;&#37327;&#21644;&#20351;&#29992;&#27979;&#22320;&#32447;&#29983;&#25104;&#30340;&#36816;&#21160;&#33021;&#36866;&#24212;&#26032;&#30340;&#29615;&#22659;&#26465;&#20214;&#65292;&#24182;&#36890;&#36807;&#35843;&#25972;&#23398;&#20064;&#21040;&#30340;&#27969;&#24418;&#23454;&#29616;&#36991;&#38556;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2203.07761</link><description>&lt;p&gt;
&#22312;&#23398;&#20064;&#30340;&#40654;&#26364;&#27969;&#24418;&#19978;&#36827;&#34892;&#21453;&#24212;&#24335;&#36816;&#21160;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Reactive Motion Generation on Learned Riemannian Manifolds. (arXiv:2203.07761v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.07761
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#40654;&#26364;&#27969;&#24418;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#26426;&#22120;&#20154;&#36816;&#21160;&#23398;&#20064;&#65292;&#36890;&#36807;&#23398;&#20064;&#40654;&#26364;&#24230;&#37327;&#21644;&#20351;&#29992;&#27979;&#22320;&#32447;&#29983;&#25104;&#30340;&#36816;&#21160;&#33021;&#36866;&#24212;&#26032;&#30340;&#29615;&#22659;&#26465;&#20214;&#65292;&#24182;&#36890;&#36807;&#35843;&#25972;&#23398;&#20064;&#21040;&#30340;&#27969;&#24418;&#23454;&#29616;&#36991;&#38556;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#20960;&#21313;&#24180;&#20013;&#65292;&#36816;&#21160;&#23398;&#20064;&#30340;&#36827;&#23637;&#20351;&#24471;&#26426;&#22120;&#20154;&#33021;&#22815;&#22312;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#29615;&#22659;&#20013;&#33719;&#24471;&#26032;&#30340;&#25216;&#33021;&#24182;&#36866;&#24212;&#26410;&#30693;&#26465;&#20214;&#12290;&#23454;&#36341;&#20013;&#65292;&#36816;&#21160;&#23398;&#20064;&#26041;&#27861;&#25429;&#25417;&#30456;&#20851;&#27169;&#24335;&#24182;&#35843;&#25972;&#23427;&#20204;&#20197;&#36866;&#24212;&#21160;&#24577;&#36991;&#38556;&#25110;&#21487;&#21464;&#30446;&#26631;&#31561;&#26032;&#26465;&#20214;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#40654;&#26364;&#27969;&#24418;&#30340;&#35282;&#24230;&#30740;&#31350;&#26426;&#22120;&#20154;&#36816;&#21160;&#23398;&#20064;&#33539;&#24335;&#12290;&#25105;&#20204;&#35748;&#20026;&#36890;&#36807;&#20154;&#31867;&#31034;&#25945;&#21487;&#20197;&#23398;&#20064;&#21040;&#40654;&#26364;&#27969;&#24418;&#65292;&#20854;&#20013;&#27979;&#22320;&#32447;&#26159;&#33258;&#28982;&#30340;&#36816;&#21160;&#25216;&#33021;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#26032;&#22411;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#29983;&#25104;&#20351;&#29992;&#23398;&#20064;&#21040;&#30340;&#40654;&#26364;&#24230;&#37327;&#20135;&#29983;&#30340;&#27979;&#22320;&#32447;&#65292;&#35813;&#33258;&#32534;&#30721;&#22120;&#29305;&#21035;&#29992;&#20110;&#24674;&#22797;&#20840;&#20301;&#23039;&#26411;&#31471;&#25191;&#34892;&#22120;&#29366;&#24577;&#21644;&#20851;&#33410;&#31354;&#38388;&#37197;&#32622;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#37325;&#22609;&#23398;&#20064;&#21040;&#30340;&#27969;&#24418;&#26469;&#20419;&#36827;&#26411;&#31471;&#25191;&#34892;&#22120;/&#22810;&#32930;&#20307;&#36991;&#38556;&#30340;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#20351;&#29992;&#20102;&#19968;&#20010;&#33021;&#22815;&#24863;&#30693;&#38556;&#30861;&#29289;&#30340;&#29615;&#22659;&#24230;&#37327;&#12290;&#20351;&#29992;&#36825;&#20123;&#27979;&#22320;&#32447;&#29983;&#25104;&#30340;&#36816;&#21160;&#21487;&#20197;...
&lt;/p&gt;
&lt;p&gt;
In recent decades, advancements in motion learning have enabled robots to acquire new skills and adapt to unseen conditions in both structured and unstructured environments. In practice, motion learning methods capture relevant patterns and adjust them to new conditions such as dynamic obstacle avoidance or variable targets. In this paper, we investigate the robot motion learning paradigm from a Riemannian manifold perspective. We argue that Riemannian manifolds may be learned via human demonstrations in which geodesics are natural motion skills. The geodesics are generated using a learned Riemannian metric produced by our novel variational autoencoder (VAE), which is especially intended to recover full-pose end-effector states and joint space configurations. In addition, we propose a technique for facilitating on-the-fly end-effector/multiple-limb obstacle avoidance by reshaping the learned manifold using an obstacle-aware ambient metric. The motion generated using these geodesics may
&lt;/p&gt;</description></item><item><title>SuperAnimal&#26159;&#19968;&#31181;&#33021;&#22815;&#24320;&#21457;&#21644;&#37096;&#32626;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#20154;&#24037;&#26631;&#27880;&#21644;&#27169;&#22411;&#35757;&#32451;&#65292;&#21516;&#26102;&#21487;&#29992;&#20110;&#36229;&#36807;45&#20010;&#29289;&#31181;&#36827;&#34892;&#35270;&#39057;&#25512;&#26029;&#65292;&#24182;&#20855;&#26377;&#24494;&#35843;&#27169;&#22411;&#25928;&#29575;&#39640;&#30340;&#20248;&#28857;&#12290;</title><link>http://arxiv.org/abs/2203.07436</link><description>&lt;p&gt;
&#36229;&#32423;&#21160;&#29289;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#65292;&#29992;&#20110;&#21160;&#29289;&#34892;&#20026;&#30340;&#21363;&#25554;&#21363;&#29992;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
SuperAnimal models pretrained for plug-and-play analysis of animal behavior. (arXiv:2203.07436v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.07436
&lt;/p&gt;
&lt;p&gt;
SuperAnimal&#26159;&#19968;&#31181;&#33021;&#22815;&#24320;&#21457;&#21644;&#37096;&#32626;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#20154;&#24037;&#26631;&#27880;&#21644;&#27169;&#22411;&#35757;&#32451;&#65292;&#21516;&#26102;&#21487;&#29992;&#20110;&#36229;&#36807;45&#20010;&#29289;&#31181;&#36827;&#34892;&#35270;&#39057;&#25512;&#26029;&#65292;&#24182;&#20855;&#26377;&#24494;&#35843;&#27169;&#22411;&#25928;&#29575;&#39640;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34892;&#20026;&#37327;&#21270;&#22312;&#31070;&#32463;&#31185;&#23398;&#12289;&#20861;&#21307;&#21644;&#21160;&#29289;&#20445;&#25252;&#31561;&#24212;&#29992;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#34892;&#20026;&#20998;&#26512;&#30340;&#20851;&#38190;&#27493;&#39588;&#26159;&#39318;&#20808;&#25552;&#21462;&#19982;&#21160;&#29289;&#30456;&#20851;&#30340;&#20851;&#38190;&#28857;&#65292;&#21363;&#23039;&#21183;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#21487;&#38752;&#30340;&#23039;&#21183;&#25512;&#26029;&#30446;&#21069;&#38656;&#35201;&#39046;&#22495;&#30693;&#35782;&#21644;&#25163;&#21160;&#26631;&#27880;&#26469;&#26500;&#24314;&#30417;&#30563;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#25216;&#26415;&#21019;&#26032;&#65292;&#20351;&#19968;&#31181;&#21517;&#20026;SuperAnimal&#30340;&#26032;&#26041;&#27861;&#33021;&#22815;&#24320;&#21457;&#21644;&#37096;&#32626;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#20154;&#24037;&#26631;&#27880;&#21644;&#27169;&#22411;&#35757;&#32451;&#12290;SuperAnimal&#20801;&#35768;&#23545;45&#22810;&#20010;&#29289;&#31181;&#36827;&#34892;&#35270;&#39057;&#25512;&#26029;&#65292;&#21516;&#26102;&#21482;&#20351;&#29992;&#20004;&#31181;&#20840;&#23616;&#21160;&#29289;&#23039;&#21183;&#27169;&#22411;&#12290;&#22914;&#26524;&#38656;&#35201;&#24494;&#35843;&#27169;&#22411;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;SuperAnimal&#27169;&#22411;&#20855;&#26377;10&#20493;&#30340;&#25968;&#25454;&#25928;&#29575;&#65292;&#24182;&#19988;&#32988;&#36807;&#20808;&#21069;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#39057;&#33258;&#36866;&#24212;&#26041;&#27861;&#26469;&#25191;&#34892;&#26080;&#30417;&#30563;&#30340;&#35270;&#39057;&#32454;&#21270;&#65292;&#24182;&#19988;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#34892;&#20026;&#20998;&#31867;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantification of behavior is critical in applications ranging from neuroscience, veterinary medicine and animal conservation efforts. A common key step for behavioral analysis is first extracting relevant keypoints on animals, known as pose estimation. However, reliable inference of poses currently requires domain knowledge and manual labeling effort to build supervised models. We present a series of technical innovations that enable a new method, collectively called SuperAnimal, to develop and deploy deep learning models that require zero additional human labels and model training. SuperAnimal allows video inference on over 45 species with only two global classes of animal pose models. If the models need fine-tuning, we show SuperAnimal models are 10$\times$ more data efficient and outperform prior transfer learning approaches. Moreover, we provide a new video-adaptation method to perform unsupervised refinement of videos, and we illustrate the utility of our model in behavioral clas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#27169;&#22411;&#20844;&#24179;&#24615;&#30340;&#26032;&#23450;&#20041;&#65292;&#24182;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#12290;&#35813;&#30740;&#31350;&#20851;&#27880;&#25968;&#25454;&#39537;&#21160;&#39044;&#27979;&#27169;&#22411;&#22312;&#31038;&#20250;&#32972;&#26223;&#19979;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#36890;&#36807;&#36873;&#25321;&#19981;&#21516;&#39044;&#27979;&#22120;&#36827;&#34892;&#27169;&#22411;&#22810;&#26679;&#24615;&#26102;&#21487;&#33021;&#23548;&#33268;&#20010;&#20154;&#21463;&#20260;&#23475;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2203.07139</link><description>&lt;p&gt;
&#36328;&#27169;&#22411;&#20844;&#24179;&#24615;&#65306;&#22810;&#27169;&#22411;&#24773;&#20917;&#19979;&#30340;&#20844;&#24179;&#24615;&#19982;&#20262;&#29702;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Cross-model Fairness: Empirical Study of Fairness and Ethics Under Model Multiplicity. (arXiv:2203.07139v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.07139
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#27169;&#22411;&#20844;&#24179;&#24615;&#30340;&#26032;&#23450;&#20041;&#65292;&#24182;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#12290;&#35813;&#30740;&#31350;&#20851;&#27880;&#25968;&#25454;&#39537;&#21160;&#39044;&#27979;&#27169;&#22411;&#22312;&#31038;&#20250;&#32972;&#26223;&#19979;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#36890;&#36807;&#36873;&#25321;&#19981;&#21516;&#39044;&#27979;&#22120;&#36827;&#34892;&#27169;&#22411;&#22810;&#26679;&#24615;&#26102;&#21487;&#33021;&#23548;&#33268;&#20010;&#20154;&#21463;&#20260;&#23475;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#39044;&#27979;&#27169;&#22411;&#26159;&#19968;&#20010;&#20005;&#26684;&#30340;&#25216;&#26415;&#26500;&#36896;&#65292;&#20294;&#23427;&#20204;&#21487;&#33021;&#22312;&#31038;&#20250;&#32972;&#26223;&#19979;&#36816;&#20316;&#65292;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#21892;&#24847;&#30340;&#24037;&#31243;&#36873;&#25321;&#21487;&#33021;&#24102;&#26469;&#38544;&#21547;&#30340;&#12289;&#38388;&#25509;&#30340;&#21644;&#24847;&#24819;&#19981;&#21040;&#30340;&#29616;&#23454;&#21518;&#26524;&#12290;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#65292;&#36825;&#20123;&#31995;&#32479;&#30340;&#20844;&#24179;&#24615;&#65292;&#28041;&#21450;&#21040;&#20010;&#20154;&#21644;&#32676;&#20307;&#65292;&#26159;&#19968;&#20010;&#30456;&#20851;&#30340;&#32771;&#34385;&#22240;&#32032;&#65307;&#23427;&#22312;&#25968;&#25454;&#25429;&#25417;&#21487;&#23548;&#33268;&#20154;&#20204;&#21463;&#21040;&#27495;&#35270;&#30340;&#21463;&#20445;&#25252;&#29305;&#24449;&#26102;&#20986;&#29616;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#36825;&#20010;&#27010;&#24565;&#20027;&#35201;&#38024;&#23545;&#22266;&#23450;&#27169;&#22411;&#36827;&#34892;&#30740;&#31350;&#65292;&#36890;&#24120;&#22312;&#19981;&#21516;&#30340;&#20998;&#31867;&#38408;&#20540;&#19979;&#36827;&#34892;&#30740;&#31350;&#65292;&#21147;&#22270;&#35782;&#21035;&#21644;&#28040;&#38500;&#20854;&#36816;&#20316;&#20013;&#19981;&#24076;&#26395;&#30340;&#12289;&#20855;&#26377;&#27495;&#35270;&#24615;&#21644;&#21487;&#33021;&#36829;&#27861;&#30340;&#26041;&#38754;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22238;&#28335;&#20102;&#36825;&#20010;&#22266;&#23450;&#27169;&#22411;&#30340;&#20551;&#35774;&#65292;&#25552;&#20986;&#24182;&#25506;&#32034;&#20102;&#19968;&#31181;&#26032;&#30340;&#36328;&#27169;&#22411;&#20844;&#24179;&#24615;&#23450;&#20041;&#65292;&#21363;&#22312;&#20174;&#19968;&#32452;&#34920;&#29616;&#21516;&#26679;&#20986;&#33394;&#30340;&#27169;&#22411;&#20013;&#29305;&#23450;&#36873;&#25321;&#39044;&#27979;&#22120;&#30340;&#24773;&#20917;&#19979;&#65292;&#20010;&#20154;&#21487;&#33021;&#21463;&#21040;&#20260;&#23475;&#65292;&#21363;&#22312;&#22522;&#20110;&#25928;&#29992;&#30340;&#27169;&#22411;&#22810;&#26679;&#24615;&#30340;&#35270;&#22270;&#19979;&#12290;&#30001;&#20110;&#19968;&#20010;&#20154;&#22312;&#19981;&#21516;&#30340;&#27169;&#22411;&#19979;&#21487;&#33021;&#34987;&#20998;&#31867;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
While data-driven predictive models are a strictly technological construct, they may operate within a social context in which benign engineering choices entail implicit, indirect and unexpected real-life consequences. Fairness of such systems -- pertaining both to individuals and groups -- is one relevant consideration in this space; it arises when data capture protected characteristics upon which people may be discriminated. To date, this notion has predominantly been studied for a fixed model, often under different classification thresholds, striving to identify and eradicate undesirable, discriminative and possibly unlawful aspects of its operation. Here, we backtrack on this fixed model assumption to propose and explore a novel definition of cross-model fairness where individuals can be harmed when one predictor is chosen ad hoc from a group of equally-well performing models, i.e., in view of utility-based model multiplicity. Since a person may be classified differently across mode
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Grad-Align&#65292;&#19968;&#31181;&#28176;&#36827;&#32593;&#32476;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#24378;&#19968;&#33268;&#24615;&#33410;&#28857;&#23545;&#36880;&#27493;&#21457;&#29616;&#33410;&#28857;&#23545;&#12290;&#35813;&#26041;&#27861;&#39318;&#20808;&#29983;&#25104;&#33410;&#28857;&#23884;&#20837;&#65292;&#28982;&#21518;&#35745;&#31639;&#21452;&#24863;&#30693;&#30456;&#20284;&#24615;&#24230;&#37327;&#36880;&#27493;&#23545;&#40784;&#33410;&#28857;&#12290;</title><link>http://arxiv.org/abs/2201.10945</link><description>&lt;p&gt;
&#20851;&#20110;&#20351;&#29992;&#21452;&#24863;&#30693;&#30456;&#20284;&#24615;&#36827;&#34892;&#28176;&#36827;&#32593;&#32476;&#23545;&#40784;&#30340;&#33021;&#21147;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Power of Gradual Network Alignment Using Dual-Perception Similarities. (arXiv:2201.10945v2 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.10945
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Grad-Align&#65292;&#19968;&#31181;&#28176;&#36827;&#32593;&#32476;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#24378;&#19968;&#33268;&#24615;&#33410;&#28857;&#23545;&#36880;&#27493;&#21457;&#29616;&#33410;&#28857;&#23545;&#12290;&#35813;&#26041;&#27861;&#39318;&#20808;&#29983;&#25104;&#33410;&#28857;&#23884;&#20837;&#65292;&#28982;&#21518;&#35745;&#31639;&#21452;&#24863;&#30693;&#30456;&#20284;&#24615;&#24230;&#37327;&#36880;&#27493;&#23545;&#40784;&#33410;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#23545;&#40784;&#65288;NA&#65289;&#26159;&#22522;&#20110;&#32593;&#32476;&#32467;&#26500;&#21644;&#33410;&#28857;&#23646;&#24615;&#26597;&#25214;&#20004;&#20010;&#32593;&#32476;&#20043;&#38388;&#33410;&#28857;&#23545;&#24212;&#20851;&#31995;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21160;&#26426;&#22312;&#20110;&#65292;&#30001;&#20110;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;NA&#26041;&#27861;&#37117;&#35797;&#22270;&#19968;&#27425;&#24615;&#21457;&#29616;&#25152;&#26377;&#33410;&#28857;&#23545;&#65292;&#22240;&#27492;&#23427;&#20204;&#27809;&#26377;&#21033;&#29992;&#36890;&#36807;&#33410;&#28857;&#23545;&#24212;&#20851;&#31995;&#30340;&#20013;&#38388;&#21457;&#29616;&#26469;&#26356;&#20934;&#30830;&#22320;&#25214;&#21040;&#33410;&#28857;&#21305;&#37197;&#36807;&#31243;&#20013;&#30340;&#19979;&#19968;&#20010;&#23545;&#24212;&#20851;&#31995;&#30340;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Grad-Align&#65292;&#19968;&#31181;&#26032;&#30340;&#28176;&#36827;&#32593;&#32476;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#22312;&#28176;&#36827;&#21305;&#37197;&#30340;&#26089;&#26399;&#38454;&#27573;&#23481;&#26131;&#21457;&#29616;&#30340;&#33410;&#28857;&#23545;&#26469;&#36880;&#27493;&#21457;&#29616;&#33410;&#28857;&#23545;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;Grad-Align&#39318;&#20808;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#25105;&#20204;&#30340;&#36880;&#23618;&#37325;&#26500;&#25439;&#22833;&#29983;&#25104;&#20004;&#20010;&#32593;&#32476;&#30340;&#33410;&#28857;&#23884;&#20837;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#35745;&#31639;&#21452;&#24863;&#30693;&#30456;&#20284;&#24615;&#24230;&#37327;&#36880;&#27493;&#23545;&#40784;&#33410;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Network alignment (NA) is the task of finding the correspondence of nodes between two networks based on the network structure and node attributes. Our study is motivated by the fact that, since most of existing NA methods have attempted to discover all node pairs at once, they do not harness information enriched through interim discovery of node correspondences to more accurately find the next correspondences during the node matching. To tackle this challenge, we propose Grad-Align, a new NA method that gradually discovers node pairs by making full use of node pairs exhibiting strong consistency, which are easy to be discovered in the early stage of gradual matching. Specifically, Grad-Align first generates node embeddings of the two networks based on graph neural networks along with our layer-wise reconstruction loss, a loss built upon capturing the first-order and higher-order neighborhood structures. Then, nodes are gradually aligned by computing dual-perception similarity measures 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21382;&#21490;&#24847;&#35782;&#30340;&#22810;&#27169;&#24577;&#21464;&#21387;&#22120;(HAMT)&#65292;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#32534;&#30721;&#36807;&#21435;&#30340;&#20840;&#26223;&#35266;&#23519;&#65292;&#32508;&#21512;&#32771;&#34385;&#20102;&#25991;&#26412;&#12289;&#21382;&#21490;&#21644;&#24403;&#21069;&#35266;&#23519;&#65292;&#23454;&#29616;&#33258;&#20027;&#23548;&#33322;&#12290;&#22312;&#35757;&#32451;&#20013;&#65292;&#36890;&#36807;&#22810;&#20010;&#20195;&#29702;&#20219;&#21153;&#21644;&#24378;&#21270;&#23398;&#20064;&#36827;&#19968;&#27493;&#25552;&#39640;&#23548;&#33322;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2110.13309</link><description>&lt;p&gt;
&#22522;&#20110;&#21382;&#21490;&#24847;&#35782;&#30340;&#22810;&#27169;&#24577;&#21464;&#21387;&#22120;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
History Aware Multimodal Transformer for Vision-and-Language Navigation. (arXiv:2110.13309v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.13309
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21382;&#21490;&#24847;&#35782;&#30340;&#22810;&#27169;&#24577;&#21464;&#21387;&#22120;(HAMT)&#65292;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#32534;&#30721;&#36807;&#21435;&#30340;&#20840;&#26223;&#35266;&#23519;&#65292;&#32508;&#21512;&#32771;&#34385;&#20102;&#25991;&#26412;&#12289;&#21382;&#21490;&#21644;&#24403;&#21069;&#35266;&#23519;&#65292;&#23454;&#29616;&#33258;&#20027;&#23548;&#33322;&#12290;&#22312;&#35757;&#32451;&#20013;&#65292;&#36890;&#36807;&#22810;&#20010;&#20195;&#29702;&#20219;&#21153;&#21644;&#24378;&#21270;&#23398;&#20064;&#36827;&#19968;&#27493;&#25552;&#39640;&#23548;&#33322;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;&#26088;&#22312;&#26500;&#24314;&#33021;&#22815;&#36981;&#24490;&#25351;&#20196;&#24182;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#23548;&#33322;&#30340;&#33258;&#20027;&#35270;&#35273;&#20195;&#29702;&#12290;&#20026;&#20102;&#35760;&#20303;&#20808;&#21069;&#35775;&#38382;&#36807;&#30340;&#20301;&#32622;&#21644;&#37319;&#21462;&#30340;&#34892;&#21160;&#65292;&#22823;&#22810;&#25968;&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;&#26041;&#27861;&#20351;&#29992;&#36882;&#24402;&#29366;&#24577;&#23454;&#29616;&#35760;&#24518;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21382;&#21490;&#24847;&#35782;&#22810;&#27169;&#24577;&#21464;&#21387;&#22120;(History Aware Multimodal Transformer&#65292;HAMT)&#65292;&#23558;&#38271;&#26399;&#21382;&#21490;&#32435;&#20837;&#22810;&#27169;&#24577;&#20915;&#31574;&#20013;&#12290;HAMT&#36890;&#36807;&#23618;&#27425;&#35270;&#35273;&#21464;&#21387;&#22120;(ViT)&#39640;&#25928;&#32534;&#30721;&#25152;&#26377;&#36807;&#21435;&#30340;&#20840;&#26223;&#35266;&#23519;&#32467;&#26524;&#65292;&#39318;&#20808;&#20351;&#29992;ViT&#23545;&#21333;&#20010;&#22270;&#20687;&#36827;&#34892;&#32534;&#30721;&#65292;&#28982;&#21518;&#27169;&#22411;&#21270;&#20840;&#26223;&#35266;&#23519;&#20013;&#22270;&#20687;&#20043;&#38388;&#30340;&#31354;&#38388;&#20851;&#31995;&#65292;&#26368;&#21518;&#32771;&#34385;&#21382;&#21490;&#20013;&#20840;&#26223;&#35266;&#23519;&#20043;&#38388;&#30340;&#26102;&#38388;&#20851;&#31995;&#12290;&#28982;&#21518;&#65292;&#23427;&#23558;&#25991;&#26412;&#12289;&#21382;&#21490;&#21644;&#24403;&#21069;&#35266;&#23519;&#20849;&#21516;&#32452;&#21512;&#36215;&#26469;&#39044;&#27979;&#19979;&#19968;&#27493;&#34892;&#21160;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#22810;&#20010;&#20195;&#29702;&#20219;&#21153;&#23545;HAMT&#36827;&#34892;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;&#21253;&#25324;&#21333;&#27493;&#34892;&#21160;&#39044;&#27979;&#21644;&#31354;&#38388;&#20851;&#31995;&#39044;&#27979;&#65292;&#28982;&#21518;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#36827;&#19968;&#27493;&#25552;&#39640;&#23548;&#33322;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-and-language navigation (VLN) aims to build autonomous visual agents that follow instructions and navigate in real scenes. To remember previously visited locations and actions taken, most approaches to VLN implement memory using recurrent states. Instead, we introduce a History Aware Multimodal Transformer (HAMT) to incorporate a long-horizon history into multimodal decision making. HAMT efficiently encodes all the past panoramic observations via a hierarchical vision transformer (ViT), which first encodes individual images with ViT, then models spatial relation between images in a panoramic observation and finally takes into account temporal relation between panoramas in the history. It, then, jointly combines text, history and current observation to predict the next action. We first train HAMT end-to-end using several proxy tasks including single step action prediction and spatial relation prediction, and then use reinforcement learning to further improve the navigation policy
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20026;&#20102;&#25903;&#25345;&#19981;&#26029;&#21457;&#23637;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#35770;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#27010;&#24565;&#27169;&#22411;&#65292;&#29992;&#20110;&#25551;&#36848;&#30693;&#35782;&#34920;&#31034;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#25512;&#29702;&#20195;&#29702;&#20154;&#36827;&#34892;&#30340;&#20449;&#24687;&#20256;&#36755;&#12290;&#36890;&#36807;&#23545;&#30693;&#35782;&#29366;&#24577;&#21644;&#20854;&#21160;&#24577;&#30340;&#20195;&#25968;&#25551;&#36848;&#65292;&#25105;&#20204;&#33021;&#22815;&#27604;&#36739;&#21644;&#32452;&#21512;&#30693;&#35782;&#29366;&#24577;&#65292;&#20197;&#34920;&#31034;&#26356;&#26032;&#12290;</title><link>http://arxiv.org/abs/2110.11482</link><description>&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#20030;&#25514;&#20013;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#30340;&#34920;&#36798;&#21450;&#20854;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
Representations of epistemic uncertainty and its perception in data-driven initiatives. (arXiv:2110.11482v4 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.11482
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20026;&#20102;&#25903;&#25345;&#19981;&#26029;&#21457;&#23637;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#35770;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#27010;&#24565;&#27169;&#22411;&#65292;&#29992;&#20110;&#25551;&#36848;&#30693;&#35782;&#34920;&#31034;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#25512;&#29702;&#20195;&#29702;&#20154;&#36827;&#34892;&#30340;&#20449;&#24687;&#20256;&#36755;&#12290;&#36890;&#36807;&#23545;&#30693;&#35782;&#29366;&#24577;&#21644;&#20854;&#21160;&#24577;&#30340;&#20195;&#25968;&#25551;&#36848;&#65292;&#25105;&#20204;&#33021;&#22815;&#27604;&#36739;&#21644;&#32452;&#21512;&#30693;&#35782;&#29366;&#24577;&#65292;&#20197;&#34920;&#31034;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20511;&#21161;&#20154;&#24037;&#26234;&#33021;&#30340;&#20986;&#29616;&#25512;&#21160;&#30340;&#26032;&#20852;&#25968;&#25454;&#39537;&#21160;&#31574;&#30053;&#27491;&#22312;&#37325;&#22609;&#20915;&#31574;&#36807;&#31243;&#65292;&#36828;&#31163;&#23545;&#30452;&#25509;&#25968;&#25454;&#20132;&#20114;&#30340;&#20256;&#32479;&#20381;&#36182;&#12290;&#36825;&#31181;&#33539;&#24335;&#36716;&#21464;&#24341;&#20837;&#20102;&#35780;&#20272;&#25968;&#25454;&#39537;&#21160;&#20030;&#25514;&#24433;&#21709;&#30340;&#26032;&#25361;&#25112;&#12290;&#20026;&#20102;&#25903;&#25345;&#36825;&#20123;&#19981;&#26029;&#21457;&#23637;&#30340;&#26041;&#27861;&#35770;&#65292;&#36843;&#20999;&#38656;&#35201;&#26032;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#25551;&#36848;&#28304;&#20110;&#26377;&#38480;&#25968;&#25454;&#21487;&#35266;&#27979;&#24615;&#20197;&#21450;&#30001;&#27492;&#20135;&#29983;&#30340;&#20915;&#31574;&#20013;&#30340;&#27495;&#20041;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#27010;&#24565;&#27169;&#22411;&#65292;&#26088;&#22312;&#22788;&#29702;&#30693;&#35782;&#34920;&#31034;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#25512;&#29702;&#20195;&#29702;&#20154;&#36827;&#34892;&#20449;&#24687;&#20256;&#36755;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#20511;&#37492;&#30446;&#21069;&#29992;&#20110;&#35780;&#20272;&#25968;&#25454;&#39537;&#21160;&#20030;&#25514;&#20135;&#29983;&#30340;&#20215;&#20540;&#30340;&#22810;&#32500;&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#30693;&#35782;&#29366;&#24577;&#21450;&#20854;&#21160;&#24577;&#30340;&#20195;&#25968;&#25551;&#36848;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36171;&#20104;&#25105;&#20204;&#30340;&#27169;&#22411;&#19968;&#31181;&#24418;&#24335;&#21270;&#32467;&#26500;&#65292;&#29992;&#20110;&#27604;&#36739;&#21644;&#32452;&#21512;&#30693;&#35782;&#29366;&#24577;&#65307;&#36890;&#36807;&#36825;&#20123;&#32452;&#21512;&#26469;&#34920;&#31034;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emerging data-driven strategies, powered by the advent of AI, are reshaping decision-making processes, moving away from traditional reliance on direct data interaction. This paradigm shift introduces new challenges in assessing the impact of data-driven initiatives. To support these evolving methodologies, there is a crucial need for new models capable of describing the uncertainties stemming from limited data observability and the resulting ambiguities in decision-making. This contribution presents a novel conceptual model designed to deal with uncertainty in knowledge representations and reasoning about information transfer mediated by agents. Drawing from the multidimensional frameworks currently adopted to assess the value generated in data-driven initiatives, we provide an algebraic description of knowledge states and their dynamics. Specifically, we endow our model with a formal structure to compare and combine knowledge states; an update is represented through these combinations
&lt;/p&gt;</description></item></channel></rss>