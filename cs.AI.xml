<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#23545;&#24694;&#24847;&#35328;&#35770;&#26816;&#27979;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#21457;&#29616;&#29616;&#26377;&#26041;&#27861;&#22312;&#19978;&#19979;&#25991;&#24863;&#30693;&#26041;&#38754;&#23384;&#22312;&#26174;&#33879;&#38480;&#21046;&#12290;&#32780;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#28508;&#21147;&#20316;&#20026;&#19978;&#19979;&#25991;&#24863;&#30693;&#24694;&#24847;&#35328;&#35770;&#26816;&#27979;&#30340;&#30693;&#35782;&#24211;&#65292;&#20294;&#30446;&#21069;&#32570;&#20047;&#26377;&#25928;&#25552;&#31034;&#36825;&#20123;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.03346</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#23545;&#24694;&#24847;&#35328;&#35770;&#26816;&#27979;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
An Investigation of Large Language Models for Real-World Hate Speech Detection. (arXiv:2401.03346v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03346
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#23545;&#24694;&#24847;&#35328;&#35770;&#26816;&#27979;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#21457;&#29616;&#29616;&#26377;&#26041;&#27861;&#22312;&#19978;&#19979;&#25991;&#24863;&#30693;&#26041;&#38754;&#23384;&#22312;&#26174;&#33879;&#38480;&#21046;&#12290;&#32780;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#28508;&#21147;&#20316;&#20026;&#19978;&#19979;&#25991;&#24863;&#30693;&#24694;&#24847;&#35328;&#35770;&#26816;&#27979;&#30340;&#30693;&#35782;&#24211;&#65292;&#20294;&#30446;&#21069;&#32570;&#20047;&#26377;&#25928;&#25552;&#31034;&#36825;&#20123;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24694;&#24847;&#35328;&#35770;&#24050;&#32463;&#25104;&#20026;&#22256;&#25200;&#25105;&#20204;&#31038;&#20132;&#31354;&#38388;&#30340;&#19968;&#20010;&#20027;&#35201;&#38382;&#39064;&#12290;&#34429;&#28982;&#22312;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#19978;&#24050;&#32463;&#20570;&#20986;&#20102;&#19968;&#20123;&#26174;&#33879;&#30340;&#21162;&#21147;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#22312;&#26377;&#25928;&#26816;&#27979;&#22312;&#32447;&#24694;&#24847;&#35328;&#35770;&#26041;&#38754;&#20173;&#28982;&#23384;&#22312;&#26174;&#33879;&#38480;&#21046;&#12290;&#29616;&#26377;&#26041;&#27861;&#30340;&#19968;&#20010;&#20027;&#35201;&#38480;&#21046;&#26159;&#24694;&#24847;&#35328;&#35770;&#26816;&#27979;&#26159;&#19968;&#20010;&#39640;&#24230;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#38382;&#39064;&#65292;&#32780;&#36825;&#20123;&#26041;&#27861;&#26080;&#27861;&#23436;&#20840;&#25429;&#25417;&#24694;&#24847;&#35328;&#35770;&#30340;&#19978;&#19979;&#25991;&#20197;&#36827;&#34892;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;&#26368;&#36817;&#65292;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20960;&#20010;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;LLMs&#32463;&#36807;&#22823;&#37327;&#30340;&#33258;&#28982;&#35821;&#35328;&#25968;&#25454;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35757;&#32451;&#65292;&#20351;&#20854;&#33021;&#22815;&#25484;&#25569;&#22797;&#26434;&#30340;&#19978;&#19979;&#25991;&#32454;&#33410;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#21487;&#20197;&#29992;&#20316;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#24694;&#24847;&#35328;&#35770;&#26816;&#27979;&#30340;&#30693;&#35782;&#24211;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;LLMs&#26816;&#27979;&#24694;&#24847;&#35328;&#35770;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#26159;&#27809;&#26377;&#20851;&#20110;&#26377;&#25928;&#25552;&#31034;LLMs&#36827;&#34892;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#24694;&#24847;&#35328;&#35770;&#26816;&#27979;&#30340;&#30740;&#31350;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#30740;&#31350;&#65292;&#35843;&#26597;&#24694;&#24847;&#35328;&#35770;&#26816;&#27979;&#26041;&#38754;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hate speech has emerged as a major problem plaguing our social spaces today. While there have been significant efforts to address this problem, existing methods are still significantly limited in effectively detecting hate speech online. A major limitation of existing methods is that hate speech detection is a highly contextual problem, and these methods cannot fully capture the context of hate speech to make accurate predictions. Recently, large language models (LLMs) have demonstrated state-of-the-art performance in several natural language tasks. LLMs have undergone extensive training using vast amounts of natural language data, enabling them to grasp intricate contextual details. Hence, they could be used as knowledge bases for context-aware hate speech detection. However, a fundamental problem with using LLMs to detect hate speech is that there are no studies on effectively prompting LLMs for context-aware hate speech detection. In this study, we conduct a large-scale study of hat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#19968;&#20010;&#26032;&#39062;&#30340;&#29983;&#24179;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#25552;&#20379;&#20102;&#24211;&#22270;&#19982;&#20449;&#24687;&#31185;&#23398;&#39046;&#22495;&#20808;&#39537;&#20154;&#29289;S.R. Ranganathan&#30340;360&#24230;&#35270;&#35282;&#65292;&#36825;&#31181;&#19987;&#38376;&#30340;&#34920;&#31034;&#22312;&#33539;&#22260;&#21644;&#35206;&#30422;&#33539;&#22260;&#19978;&#26080;&#21487;&#27604;&#25311;&#65292;&#24182;&#21628;&#21505;&#25972;&#20010;&#31038;&#21306;&#20849;&#21516;&#21162;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.03343</link><description>&lt;p&gt;
&#37325;&#26032;&#21457;&#29616;Ranganathan&#65306;&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#30340;&#20809;&#35889;&#35270;&#35282;&#20102;&#35299;&#20182;&#30340;&#29983;&#24179;
&lt;/p&gt;
&lt;p&gt;
Rediscovering Ranganathan: A Prismatic View of His Life through the Knowledge Graph Spectrum. (arXiv:2401.03343v1 [cs.DL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03343
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#19968;&#20010;&#26032;&#39062;&#30340;&#29983;&#24179;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#25552;&#20379;&#20102;&#24211;&#22270;&#19982;&#20449;&#24687;&#31185;&#23398;&#39046;&#22495;&#20808;&#39537;&#20154;&#29289;S.R. Ranganathan&#30340;360&#24230;&#35270;&#35282;&#65292;&#36825;&#31181;&#19987;&#38376;&#30340;&#34920;&#31034;&#22312;&#33539;&#22260;&#21644;&#35206;&#30422;&#33539;&#22260;&#19978;&#26080;&#21487;&#27604;&#25311;&#65292;&#24182;&#21628;&#21505;&#25972;&#20010;&#31038;&#21306;&#20849;&#21516;&#21162;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#24211;&#22270;&#19982;&#20449;&#24687;&#31185;&#23398;&#39046;&#22495;&#30340;&#20808;&#39537;&#20154;&#29289;S.R. Ranganathan&#25945;&#25480;&#30340;&#26032;&#39062;&#29983;&#24179;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#12290;&#21457;&#29616;&#20851;&#20110;Ranganathan&#30340;&#30456;&#20851;&#20107;&#23454;&#23384;&#22312;&#20110;&#21508;&#31181;&#36164;&#28304;&#20013;&#65292;&#20197;&#30862;&#29255;&#21270;&#21644;&#38646;&#25955;&#30340;&#26041;&#24335;&#25552;&#20379;&#20449;&#24687;&#12290;&#36890;&#36807;&#36825;&#20010;&#19987;&#38376;&#30340;KG&#65292;&#25105;&#20204;&#24076;&#26395;&#20026;&#20182;&#30340;&#29983;&#24179;&#21644;&#25104;&#23601;&#25552;&#20379;&#19968;&#20010;360&#24230;&#30340;&#35270;&#35282;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#31181;&#19987;&#38376;&#30340;&#34920;&#31034;&#22312;&#20854;&#33539;&#22260;&#21644;&#35206;&#30422;&#33539;&#22260;&#19978;&#26159;&#26080;&#21487;&#27604;&#25311;&#30340;&#65306;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#20379;&#20219;&#20309;&#20154;&#20844;&#24320;&#35775;&#38382;&#12289;&#20351;&#29992;/&#20877;&#20351;&#29992;&#21644;&#36129;&#29486;&#12290;&#21463;Ranganathan&#30340;&#29702;&#35770;&#21644;&#24605;&#24819;&#30340;&#21551;&#21457;&#65292;KG&#20351;&#29992;&#8220;&#22522;&#20110;&#29305;&#24449;&#30340;&#26041;&#27861;&#35770;&#8221;&#22312;&#20004;&#20010;&#23618;&#38754;&#19978;&#36827;&#34892;&#20102;&#21457;&#23637;&#65306;&#22312;&#20851;&#38190;&#30340;&#29983;&#24179;&#26041;&#38754;&#30340;&#26631;&#35782;&#21644;&#26412;&#20307;&#27169;&#22411;&#30340;&#24320;&#21457;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#36825;&#39033;&#30740;&#31350;&#65292;&#25105;&#20204;&#21628;&#21505;&#25972;&#20010;&#31038;&#21306;&#20849;&#21516;&#21162;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The present study puts forward a novel biographical knowledge graph (KG) on Prof. S. R. Ranganathan, one of the pioneering figures in the Library and Information Science (LIS) domain. It has been found that most of the relevant facts about Ranganathan exist in a variety of resources (e.g., books, essays, journal articles, websites, blogs, etc.), offering information in a fragmented and piecemeal way. With this dedicated KG (henceforth known as RKG), we hope to furnish a 360-degree view of his life and achievements. To the best of our knowledge, such a dedicated representation is unparalleled in its scope and coverage: using state-of-the-art technology for anyone to openly access, use/re-use, and contribute. Inspired by Ranganathan's theories and ideas, the KG was developed using a "facet-based methodology" at two levels: in the identification of the vital biographical aspects and the development of the ontological model. Finally, with this study, we call for a community-driven effort t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23618;&#27425;&#21270;&#24378;&#21270;&#23398;&#20064;&#30340;&#22810;&#27493;&#24577;&#36866;&#24212;&#22320;&#24418;&#22235;&#36275;&#25511;&#21046;&#22120;&#65288;MTAC&#65289;&#65292;&#33021;&#22815;&#22312;&#21160;&#24577;&#21644;&#31895;&#31961;&#30340;&#22320;&#24418;&#29615;&#22659;&#20013;&#23454;&#29616;&#22810;&#31181;&#33258;&#36866;&#24212;&#27493;&#24577;&#65292;&#24182;&#19988;&#20855;&#26377;&#39640;&#25928;&#35299;&#20915;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.03337</link><description>&lt;p&gt;
MTAC: &#22522;&#20110;&#23618;&#27425;&#21270;&#24378;&#21270;&#23398;&#20064;&#30340;&#22810;&#27493;&#24577;&#36866;&#24212;&#22320;&#24418;&#22235;&#36275;&#25511;&#21046;&#22120;
&lt;/p&gt;
&lt;p&gt;
MTAC: Hierarchical Reinforcement Learning-based Multi-gait Terrain-adaptive Quadruped Controller. (arXiv:2401.03337v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03337
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23618;&#27425;&#21270;&#24378;&#21270;&#23398;&#20064;&#30340;&#22810;&#27493;&#24577;&#36866;&#24212;&#22320;&#24418;&#22235;&#36275;&#25511;&#21046;&#22120;&#65288;MTAC&#65289;&#65292;&#33021;&#22815;&#22312;&#21160;&#24577;&#21644;&#31895;&#31961;&#30340;&#22320;&#24418;&#29615;&#22659;&#20013;&#23454;&#29616;&#22810;&#31181;&#33258;&#36866;&#24212;&#27493;&#24577;&#65292;&#24182;&#19988;&#20855;&#26377;&#39640;&#25928;&#35299;&#20915;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22478;&#24066;&#25628;&#25937;&#20219;&#21153;&#38656;&#35201;&#24555;&#36895;&#21709;&#24212;&#20197;&#26368;&#23567;&#21270;&#20154;&#21592;&#20260;&#20129;&#21644;&#25439;&#22833;&#12290;&#36890;&#24120;&#65292;&#36825;&#20123;&#21162;&#21147;&#20250;&#30001;&#20154;&#36947;&#20027;&#20041;&#26426;&#22120;&#20154;&#21327;&#21161;&#65292;&#36825;&#20123;&#26426;&#22120;&#20154;&#38656;&#35201;&#22788;&#29702;&#21160;&#24577;&#25805;&#20316;&#26465;&#20214;&#65292;&#22914;&#19981;&#24179;&#22374;&#21644;&#31895;&#31961;&#30340;&#22320;&#24418;&#65292;&#23588;&#20854;&#26159;&#22312;&#31867;&#20284;&#22320;&#38663;&#31561;&#22823;&#35268;&#27169;&#20260;&#20129;&#20107;&#20214;&#20013;&#12290;&#22235;&#36275;&#26426;&#22120;&#20154;&#30001;&#20110;&#20854;&#22810;&#21151;&#33021;&#35774;&#35745;&#65292;&#20855;&#22791;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#21327;&#21161;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36825;&#20123;&#26426;&#22120;&#20154;&#20855;&#26377;&#22810;&#33258;&#30001;&#24230;&#65292;&#25511;&#21046;&#20854;&#22312;&#21160;&#24577;&#21644;&#31895;&#31961;&#30340;&#22320;&#24418;&#29615;&#22659;&#20013;&#30340;&#36816;&#21160;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#24403;&#21069;&#38024;&#23545;&#22235;&#36275;&#26426;&#22120;&#20154;&#30340;&#36816;&#21160;&#25511;&#21046;&#22120;&#22312;&#20135;&#29983;&#22810;&#31181;&#33258;&#36866;&#24212;&#27493;&#24577;&#12289;&#22312;&#26102;&#38388;&#21644;&#36164;&#28304;&#19978;&#39640;&#25928;&#35299;&#20915;&#20219;&#21153;&#20197;&#21450;&#38656;&#35201;&#32321;&#29712;&#30340;&#35757;&#32451;&#21644;&#25163;&#21160;&#35843;&#35797;&#36807;&#31243;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MTAC&#65306;&#19968;&#31181;&#22810;&#27493;&#24577;&#36866;&#24212;&#22320;&#24418;&#25511;&#21046;&#22120;&#65292;&#35813;&#25511;&#21046;&#22120;&#21033;&#29992;&#20102;&#23618;&#27425;&#21270;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#26102;&#38388;&#21644;&#20869;&#23384;&#25928;&#29575;&#19978;&#36827;&#34892;&#20102;&#20248;&#21270;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#25552;&#35758;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Urban search and rescue missions require rapid first response to minimize loss of life and damage. Often, such efforts are assisted by humanitarian robots which need to handle dynamic operational conditions such as uneven and rough terrains, especially during mass casualty incidents like an earthquake. Quadruped robots, owing to their versatile design, have the potential to assist in such scenarios. However, control of quadruped robots in dynamic and rough terrain environments is a challenging problem due to the many degrees of freedom of these robots. Current locomotion controllers for quadrupeds are limited in their ability to produce multiple adaptive gaits, solve tasks in a time and resource-efficient manner, and require tedious training and manual tuning procedures. To address these challenges, we propose MTAC: a multi-gait terrain-adaptive controller, which utilizes a Hierarchical reinforcement learning (HRL) approach while being time and memory-efficient. We show that our propos
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#21644;&#33258;&#32534;&#30721;&#22120;&#30340;&#28151;&#21512;&#27169;&#22411;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#30340;&#22312;&#32447;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#65292;&#36890;&#36807;&#32467;&#21512;&#27880;&#24847;&#21147;&#21644;&#33258;&#32534;&#30721;&#22120;&#65292;&#24182;&#22312;&#33258;&#32534;&#30721;&#22120;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#39044;&#27979;&#19979;&#19968;&#20010;&#26102;&#38388;&#27493;&#39588;&#31383;&#21475;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.03322</link><description>&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#21644;&#33258;&#32534;&#30721;&#22120;&#30340;&#28151;&#21512;&#27169;&#22411;&#29992;&#20110;&#26080;&#30417;&#30563;&#22312;&#32447;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Attention and Autoencoder Hybrid Model for Unsupervised Online Anomaly Detection. (arXiv:2401.03322v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03322
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#21644;&#33258;&#32534;&#30721;&#22120;&#30340;&#28151;&#21512;&#27169;&#22411;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#30340;&#22312;&#32447;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#65292;&#36890;&#36807;&#32467;&#21512;&#27880;&#24847;&#21147;&#21644;&#33258;&#32534;&#30721;&#22120;&#65292;&#24182;&#22312;&#33258;&#32534;&#30721;&#22120;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#39044;&#27979;&#19979;&#19968;&#20010;&#26102;&#38388;&#27493;&#39588;&#31383;&#21475;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#21644;&#33258;&#32534;&#30721;&#22120;&#30340;&#28151;&#21512;&#27169;&#22411;&#65292;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#30340;&#26080;&#30417;&#30563;&#22312;&#32447;&#24322;&#24120;&#26816;&#27979;&#12290;&#33258;&#32534;&#30721;&#22120;&#25429;&#25417;&#30701;&#23884;&#20837;&#20013;&#30340;&#23616;&#37096;&#32467;&#26500;&#27169;&#24335;&#65292;&#32780;&#27880;&#24847;&#21147;&#27169;&#22411;&#23398;&#20064;&#38271;&#26399;&#29305;&#24449;&#65292;&#36890;&#36807;&#20301;&#32622;&#32534;&#30721;&#23454;&#29616;&#24182;&#34892;&#35745;&#31639;&#12290;&#22312;&#26041;&#27861;&#19978;&#29420;&#29305;&#30340;&#26159;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#28151;&#21512;&#27169;&#22411;&#39318;&#27425;&#22312;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#20013;&#32467;&#21512;&#20102;&#27880;&#24847;&#21147;&#21644;&#33258;&#32534;&#30721;&#22120;&#12290;&#23427;&#37319;&#29992;&#20102;&#31867;&#20284;&#28145;&#24230;&#21464;&#25442;&#22120;&#27169;&#22411;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#24182;&#23545;&#33258;&#32534;&#30721;&#22120;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#39044;&#27979;&#19979;&#19968;&#20010;&#26102;&#38388;&#27493;&#39588;&#31383;&#21475;&#36827;&#34892;&#20102;&#20851;&#38190;&#26550;&#26500;&#20462;&#25913;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#39564;&#35777;&#25968;&#25454;&#38598;&#20013;&#30340;&#38408;&#20540;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#26512;&#35823;&#24046;&#30340;&#31532;&#19968;&#32479;&#35745;&#30697;&#26041;&#27861;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#32780;&#19981;&#20381;&#36182;&#20110;&#39564;&#35777;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#22312;&#22810;&#26679;&#30340;&#23454;&#38469;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#19982;&#20854;&#20182;&#24191;&#20026;&#35748;&#21487;&#30340;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#65292;&#35777;&#23454;&#20102;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a hybrid attention and autoencoder (AE) model for unsupervised online anomaly detection in time series. The autoencoder captures local structural patterns in short embeddings, while the attention model learns long-term features, facilitating parallel computing with positional encoding. Unique in its approach, our proposed hybrid model combines attention and autoencoder for the first time in time series anomaly detection. It employs an attention-based mechanism, akin to the deep transformer model, with key architectural modifications for predicting the next time step window in the autoencoder's latent space. The model utilizes a threshold from the validation dataset for anomaly detection and introduces an alternative method based on analyzing the first statistical moment of error, improving accuracy without dependence on a validation dataset. Evaluation on diverse real-world benchmark datasets and comparing with other well-established models, confirms the effective
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;&#32447;&#24615;&#22238;&#24402;&#12289;&#22810;&#23618;&#24863;&#30693;&#26426;&#21644;&#26799;&#24230;&#25552;&#21319;&#22238;&#24402;&#19977;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#24494;&#26381;&#21153;&#35843;&#29992;&#29575;&#39044;&#27979;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;&#26799;&#24230;&#25552;&#21319;&#22238;&#24402;&#27169;&#22411;&#22312;&#20943;&#23567;&#35823;&#24046;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#24494;&#26381;&#21153;&#25152;&#38656;&#30340;&#21103;&#26412;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2401.03319</link><description>&lt;p&gt;
&#24494;&#26381;&#21153;&#22797;&#21046;&#22312;&#20113;&#20013;&#30340;&#35843;&#29992;&#29575;&#39044;&#27979;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Comparison of Microservice Call Rate Predictions for Replication in the Cloud. (arXiv:2401.03319v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03319
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;&#32447;&#24615;&#22238;&#24402;&#12289;&#22810;&#23618;&#24863;&#30693;&#26426;&#21644;&#26799;&#24230;&#25552;&#21319;&#22238;&#24402;&#19977;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#24494;&#26381;&#21153;&#35843;&#29992;&#29575;&#39044;&#27979;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;&#26799;&#24230;&#25552;&#21319;&#22238;&#24402;&#27169;&#22411;&#22312;&#20943;&#23567;&#35823;&#24046;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#24494;&#26381;&#21153;&#25152;&#38656;&#30340;&#21103;&#26412;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#35768;&#22810;&#29992;&#25143;&#22312;&#19968;&#32676;&#20113;&#26426;&#22120;&#19978;&#37096;&#32626;&#22522;&#20110;&#24494;&#26381;&#21153;&#30340;&#24212;&#29992;&#31243;&#24207;&#65292;&#36825;&#20123;&#24212;&#29992;&#31243;&#24207;&#20855;&#26377;&#21508;&#31181;&#30456;&#20114;&#36830;&#25509;&#65292;&#24182;&#21463;&#21040;&#21160;&#24577;&#29992;&#25143;&#38656;&#27714;&#30340;&#38543;&#26426;&#21464;&#21270;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#19977;&#31181;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#22522;&#20110;&#24494;&#26381;&#21153;&#26102;&#38388;&#39044;&#27979;&#24494;&#26381;&#21153;&#35843;&#29992;&#29575;&#65292;&#24182;&#26088;&#22312;&#20272;&#35745;&#21487;&#25193;&#23637;&#24615;&#35201;&#27714;&#12290;&#25105;&#20204;&#22312;&#38463;&#37324;&#24052;&#24052;&#30340;&#24494;&#26381;&#21153;&#36319;&#36394;&#25968;&#25454;&#19978;&#24212;&#29992;&#20102;&#32447;&#24615;&#22238;&#24402;&#65288;LR&#65289;&#12289;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;MLP&#65289;&#21644;&#26799;&#24230;&#25552;&#21319;&#22238;&#24402;&#65288;GBR&#65289;&#27169;&#22411;&#12290;&#39044;&#27979;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;GBR&#21644;MLP&#27169;&#22411;&#30456;&#27604;&#65292;LR&#27169;&#22411;&#30340;&#35757;&#32451;&#26102;&#38388;&#26356;&#30701;&#12290;&#28982;&#32780;&#65292;&#19982;LR&#21644;MLP&#27169;&#22411;&#30456;&#27604;&#65292;GBR&#27169;&#22411;&#38477;&#20302;&#20102;&#22343;&#26041;&#24046;&#21644;&#24179;&#22343;&#32477;&#23545;&#30334;&#20998;&#27604;&#35823;&#24046;&#12290;&#27492;&#22806;&#65292;&#39044;&#27979;&#32467;&#26524;&#26174;&#31034;&#65292;&#26799;&#24230;&#25552;&#21319;&#27169;&#22411;&#23545;&#27599;&#20010;&#24494;&#26381;&#21153;&#25152;&#38656;&#30340;&#21103;&#26412;&#25968;&#37327;&#19982;&#23454;&#38469;&#27979;&#35797;&#25968;&#25454;&#38750;&#24120;&#25509;&#36817;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#20219;&#20309;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Today, many users deploy their microservice-based applications with various interconnections on a cluster of Cloud machines, subject to stochastic changes due to dynamic user requirements. To address this problem, we compare three machine learning (ML) models for predicting the microservice call rates based on the microservice times and aiming at estimating the scalability requirements. We apply the linear regression (LR), multilayer perception (MLP), and gradient boosting regression (GBR) models on the Alibaba microservice traces. The prediction results reveal that the LR model reaches a lower training time than the GBR and MLP models. However, the GBR reduces the mean absolute error and the mean absolute percentage error compared to LR and MLP models. Moreover, the prediction results show that the required number of replicas for each microservice by the gradient boosting model is close to the actual test data without any prediction.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;212&#20010;&#30495;&#23454;&#30340;&#24694;&#24847;&#26381;&#21153;&#65288;Malla&#65289;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#22320;&#19979;&#24066;&#22330;&#30340;&#25193;&#25955;&#21644;&#23545;&#20844;&#20849;LLM&#26381;&#21153;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;&#20854;&#20351;&#29992;&#30340;&#31574;&#30053;&#21644;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2401.03315</link><description>&lt;p&gt;
Malla: &#25581;&#31192;&#29616;&#23454;&#19990;&#30028;&#20013;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#25972;&#21512;&#24694;&#24847;&#26381;&#21153;
&lt;/p&gt;
&lt;p&gt;
Malla: Demystifying Real-world Large Language Model Integrated Malicious Services. (arXiv:2401.03315v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03315
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;212&#20010;&#30495;&#23454;&#30340;&#24694;&#24847;&#26381;&#21153;&#65288;Malla&#65289;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#22320;&#19979;&#24066;&#22330;&#30340;&#25193;&#25955;&#21644;&#23545;&#20844;&#20849;LLM&#26381;&#21153;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;&#20854;&#20351;&#29992;&#30340;&#31574;&#30053;&#21644;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#22320;&#19979;&#21033;&#29992;&#65292;&#20063;&#31216;&#20026;Malla&#65292;&#27491;&#22312;&#22686;&#21152;&#65292;&#21152;&#21095;&#20102;&#32593;&#32476;&#23433;&#20840;&#23041;&#32961;&#65292;&#24182;&#23545;LLMs&#25216;&#26415;&#30340;&#21487;&#20449;&#24230;&#25552;&#20986;&#20102;&#30097;&#38382;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#24456;&#23569;&#26377;&#24037;&#20316;&#21162;&#21147;&#21435;&#20102;&#35299;&#36825;&#31181;&#26032;&#22411;&#32593;&#32476;&#29359;&#32618;&#30340;&#35268;&#27169;&#12289;&#24433;&#21709;&#21644;&#25216;&#26415;&#12290;&#26412;&#25991;&#26159;&#31532;&#19968;&#27425;&#23545;212&#20010;&#30495;&#23454;&#30340;Malla&#36827;&#34892;&#31995;&#32479;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#22320;&#19979;&#24066;&#22330;&#30340;&#25193;&#25955;&#65292;&#24182;&#25581;&#31034;&#20102;&#23427;&#20204;&#30340;&#25805;&#20316;&#27169;&#24335;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#24320;&#20102;Malla&#29983;&#24577;&#31995;&#32479;&#65292;&#25581;&#31034;&#20102;&#20854;&#26174;&#33879;&#30340;&#22686;&#38271;&#23545;&#24403;&#20170;&#20844;&#20849;LLM&#26381;&#21153;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#23545;212&#20010;Mallas&#36827;&#34892;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;8&#20010;&#21518;&#31471;LLMs&#65292;&#20197;&#21450;182&#20010;&#32469;&#36807;&#20844;&#20849;LLM API&#20445;&#25252;&#25514;&#26045;&#30340;&#25552;&#31034;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25581;&#31034;&#20102;Mallas&#20351;&#29992;&#30340;&#31574;&#30053;&#65292;&#21253;&#25324;&#28389;&#29992;&#26410;&#32463;&#23457;&#26597;&#30340;LLMs&#21644;&#36890;&#36807;&#36234;&#29425;&#25552;&#31034;&#21033;&#29992;&#20844;&#20849;LLM API&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#29702;&#35299;Malla&#29359;&#32618;&#34892;&#20026;&#30340;&#23454;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
The underground exploitation of large language models (LLMs) for malicious services (i.e., Malla) is witnessing an uptick, amplifying the cyber threat landscape and posing questions about the trustworthiness of LLM technologies. However, there has been little effort to understand this new cybercrime, in terms of its magnitude, impact, and techniques. In this paper, we conduct the first systematic study on 212 real-world Mallas, uncovering their proliferation in underground marketplaces and exposing their operational modalities. Our study discloses the Malla ecosystem, revealing its significant growth and impact on today's public LLM services. Through examining 212 Mallas, we uncovered eight backend LLMs used by Mallas, along with 182 prompts that circumvent the protective measures of public LLM APIs. We further demystify the tactics employed by Mallas, including the abuse of uncensored LLMs and the exploitation of public LLM APIs through jailbreak prompts. Our findings enable a better 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26469;&#25552;&#39640;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#24615;&#33021;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#24052;&#27931;&#21452;&#32990;&#32974;&#25439;&#22833;&#26368;&#22823;&#21270;&#20114;&#20449;&#24687;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#19978;&#19979;&#25991;&#22686;&#24378;&#26469;&#25552;&#21319;&#24615;&#33021;&#65292;&#32780;&#19981;&#38656;&#35201;&#26126;&#30830;&#22320;&#22686;&#21152;&#25968;&#25454;&#25110;&#20174;&#22836;&#24320;&#22987;&#23398;&#20064;&#23884;&#20837;&#12290;</title><link>http://arxiv.org/abs/2401.03314</link><description>&lt;p&gt;
&#25552;&#21319;&#23545;&#27604;&#24230;&#30340;&#19978;&#19979;&#25991;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Enhancing Context Through Contrast. (arXiv:2401.03314v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03314
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26469;&#25552;&#39640;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#24615;&#33021;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#24052;&#27931;&#21452;&#32990;&#32974;&#25439;&#22833;&#26368;&#22823;&#21270;&#20114;&#20449;&#24687;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#19978;&#19979;&#25991;&#22686;&#24378;&#26469;&#25552;&#21319;&#24615;&#33021;&#65292;&#32780;&#19981;&#38656;&#35201;&#26126;&#30830;&#22320;&#22686;&#21152;&#25968;&#25454;&#25110;&#20174;&#22836;&#24320;&#22987;&#23398;&#20064;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#21463;&#30410;&#20110;&#35821;&#20041;&#20016;&#23500;&#30340;&#34920;&#31034;&#12290;&#36890;&#36807;&#35821;&#35328;&#24314;&#27169;&#21644;&#23545;&#27604;&#23398;&#20064;&#20351;&#29992;&#20114;&#20449;&#24687;&#26368;&#22823;&#21270;&#30446;&#26631;&#65292;&#24050;&#32463;&#23454;&#29616;&#20102;&#23545;&#36825;&#31181;&#34920;&#31034;&#30340;&#22823;&#24133;&#36827;&#23637;&#12290;&#35821;&#35328;&#24314;&#27169;&#30340;&#20381;&#36182;&#24615;&#20351;&#24471;&#22312;&#23398;&#20064;&#34920;&#31034;&#30340;&#36890;&#29992;&#24615;&#21644;&#27169;&#22411;&#22312;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#12290;&#34429;&#28982;&#23545;&#27604;&#23398;&#20064;&#25913;&#36827;&#20102;&#24615;&#33021;&#65292;&#20294;&#20854;&#25104;&#21151;&#19981;&#33021;&#20165;&#24402;&#22240;&#20110;&#20114;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19978;&#19979;&#25991;&#22686;&#24378;&#27493;&#39588;&#65292;&#36890;&#36807;&#20351;&#29992;&#24052;&#27931;&#21452;&#32990;&#32974;&#25439;&#22833;&#26368;&#22823;&#21270;&#20114;&#20449;&#24687;&#26469;&#25552;&#39640;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#24615;&#33021;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#19981;&#26159;&#26174;&#24335;&#22320;&#22686;&#21152;&#25968;&#25454;&#65292;&#32780;&#26159;&#23558;&#35821;&#35328;&#35270;&#20026;&#38544;&#21547;&#30340;&#22686;&#24378;&#65292;&#28040;&#38500;&#20102;&#30772;&#22351;&#35821;&#20041;&#20449;&#24687;&#30340;&#39118;&#38505;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20250;&#20174;&#22836;&#24320;&#22987;&#23398;&#20064;&#23884;&#20837;&#65292;&#24182;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#20219;&#20309;&#19968;&#32452;&#39044;&#35757;&#32451;&#30340;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural machine translation benefits from semantically rich representations. Considerable progress in learning such representations has been achieved by language modelling and mutual information maximization objectives using contrastive learning. The language-dependent nature of language modelling introduces a trade-off between the universality of the learned representations and the model's performance on the language modelling tasks. Although contrastive learning improves performance, its success cannot be attributed to mutual information alone. We propose a novel Context Enhancement step to improve performance on neural machine translation by maximizing mutual information using the Barlow Twins loss. Unlike other approaches, we do not explicitly augment the data but view languages as implicit augmentations, eradicating the risk of disrupting semantic information. Further, our method does not learn embeddings from scratch and can be generalised to any set of pre-trained embeddings. Fin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20855;&#26377;&#23618;&#27425;&#32467;&#26500;&#30340;&#25968;&#25454;&#20316;&#20026;&#23545;&#27604;&#23398;&#20064;&#26032;&#27169;&#24577;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#25945;&#22530;&#30340;&#27010;&#24565;&#34920;&#31034;&#30340;&#23398;&#20064;&#12290;&#36890;&#36807;&#21033;&#29992;&#25968;&#25454;&#30340;&#31354;&#38388;&#23618;&#27425;&#32467;&#26500;&#65292;&#35813;&#26041;&#27861;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;&#23454;&#39564;&#35777;&#26126;&#25968;&#25454;&#38598;&#32467;&#26500;&#26159;&#19968;&#31181;&#26377;&#20215;&#20540;&#30340;&#24369;&#30417;&#30563;&#23398;&#20064;&#27169;&#24577;&#12290;</title><link>http://arxiv.org/abs/2401.03312</link><description>&lt;p&gt;
&#21033;&#29992;&#25968;&#25454;&#23618;&#27425;&#32467;&#26500;&#20316;&#20026;&#23545;&#27604;&#23398;&#20064;&#30340;&#26032;&#27169;&#24577;
&lt;/p&gt;
&lt;p&gt;
Exploiting Data Hierarchy as a New Modality for Contrastive Learning. (arXiv:2401.03312v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03312
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20855;&#26377;&#23618;&#27425;&#32467;&#26500;&#30340;&#25968;&#25454;&#20316;&#20026;&#23545;&#27604;&#23398;&#20064;&#26032;&#27169;&#24577;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#25945;&#22530;&#30340;&#27010;&#24565;&#34920;&#31034;&#30340;&#23398;&#20064;&#12290;&#36890;&#36807;&#21033;&#29992;&#25968;&#25454;&#30340;&#31354;&#38388;&#23618;&#27425;&#32467;&#26500;&#65292;&#35813;&#26041;&#27861;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;&#23454;&#39564;&#35777;&#26126;&#25968;&#25454;&#38598;&#32467;&#26500;&#26159;&#19968;&#31181;&#26377;&#20215;&#20540;&#30340;&#24369;&#30417;&#30563;&#23398;&#20064;&#27169;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#20855;&#26377;&#23618;&#27425;&#32467;&#26500;&#30340;&#25968;&#25454;&#24110;&#21161;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#25945;&#22530;&#30340;&#27010;&#24565;&#34920;&#31034;&#12290;&#22522;&#20110;WikiScenes&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#25945;&#22530;&#32452;&#20214;&#30340;&#31354;&#38388;&#26377;&#24207;&#30340;&#23618;&#27425;&#32467;&#26500;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23618;&#27425;&#23545;&#27604;&#35757;&#32451;&#26041;&#27861;&#65292;&#21033;&#29992;&#19977;&#20803;&#32452;&#36793;&#30028;&#25439;&#22833;&#22312;&#32534;&#30721;&#22120;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#34920;&#31034;&#25968;&#25454;&#30340;&#31354;&#38388;&#23618;&#27425;&#32467;&#26500;&#12290;&#22240;&#27492;&#65292;&#35813;&#26041;&#27861;&#25506;&#31350;&#20102;&#25968;&#25454;&#38598;&#32467;&#26500;&#26159;&#21542;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#20449;&#24687;&#12290;&#25105;&#20204;&#20351;&#29992;t-SNE&#23545;&#24471;&#21040;&#30340;&#28508;&#22312;&#31354;&#38388;&#36827;&#34892;&#21487;&#35270;&#21270;&#65292;&#24182;&#36890;&#36807;&#23558;&#20854;&#19982;&#20854;&#20182;&#29305;&#23450;&#25968;&#25454;&#38598;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#22312;&#20844;&#20849;&#19979;&#28216;&#20998;&#31867;&#20219;&#21153;&#20013;&#36827;&#34892;&#27604;&#36739;&#26469;&#35780;&#20272;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#22312;&#21487;&#24369;&#30417;&#30563;&#21644;&#22522;&#20934;&#26041;&#27861;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#25968;&#25454;&#38598;&#32467;&#26500;&#26159;&#19968;&#31181;&#26377;&#20215;&#20540;&#30340;&#24369;&#30417;&#30563;&#23398;&#20064;&#27169;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work investigates how hierarchically structured data can help neural networks learn conceptual representations of cathedrals. The underlying WikiScenes dataset provides a spatially organized hierarchical structure of cathedral components. We propose a novel hierarchical contrastive training approach that leverages a triplet margin loss to represent the data's spatial hierarchy in the encoder's latent space. As such, the proposed approach investigates if the dataset structure provides valuable information for self-supervised learning. We apply t-SNE to visualize the resultant latent space and evaluate the proposed approach by comparing it with other dataset-specific contrastive learning methods using a common downstream classification task. The proposed method outperforms the comparable weakly-supervised and baseline methods. Our findings suggest that dataset structure is a valuable modality for weakly-supervised learning.
&lt;/p&gt;</description></item><item><title>CAVIAR&#26159;&#19968;&#31181;&#29992;&#20110;&#25968;&#23383;&#23402;&#29983;&#30340;6G&#36890;&#20449;&#65292;3D&#22330;&#26223;&#21644;AI&#30340;&#21327;&#21516;&#20223;&#30495;&#26041;&#27861;&#65292;&#20854;&#20027;&#35201;&#36129;&#29486;&#21253;&#25324;&#19981;&#21516;&#26550;&#26500;&#30340;&#35814;&#32454;&#25551;&#36848;&#65292;&#24212;&#29992;&#20110;6G&#20013;&#30340;&#26080;&#20154;&#26426;&#25628;&#32034;&#21644;&#25937;&#25588;&#20219;&#21153;&#65292;&#24182;&#29983;&#25104;&#26377;&#20851;&#35745;&#31639;&#36164;&#28304;&#20351;&#29992;&#24773;&#20917;&#30340;&#22522;&#20934;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2401.03310</link><description>&lt;p&gt;
CAVIAR: &#29992;&#20110;&#25968;&#23383;&#23402;&#29983;&#30340;6G&#36890;&#20449;&#65292;3D&#22330;&#26223;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#21327;&#21516;&#20223;&#30495;
&lt;/p&gt;
&lt;p&gt;
CAVIAR: Co-simulation of 6G Communications, 3D Scenarios and AI for Digital Twins. (arXiv:2401.03310v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03310
&lt;/p&gt;
&lt;p&gt;
CAVIAR&#26159;&#19968;&#31181;&#29992;&#20110;&#25968;&#23383;&#23402;&#29983;&#30340;6G&#36890;&#20449;&#65292;3D&#22330;&#26223;&#21644;AI&#30340;&#21327;&#21516;&#20223;&#30495;&#26041;&#27861;&#65292;&#20854;&#20027;&#35201;&#36129;&#29486;&#21253;&#25324;&#19981;&#21516;&#26550;&#26500;&#30340;&#35814;&#32454;&#25551;&#36848;&#65292;&#24212;&#29992;&#20110;6G&#20013;&#30340;&#26080;&#20154;&#26426;&#25628;&#32034;&#21644;&#25937;&#25588;&#20219;&#21153;&#65292;&#24182;&#29983;&#25104;&#26377;&#20851;&#35745;&#31639;&#36164;&#28304;&#20351;&#29992;&#24773;&#20917;&#30340;&#22522;&#20934;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#23402;&#29983;&#26159;&#25512;&#36827;&#31227;&#21160;&#36890;&#20449;&#25216;&#26415;&#30340;&#37325;&#35201;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#22312;&#38656;&#35201;&#21516;&#26102;&#27169;&#25311;&#26080;&#32447;&#20449;&#36947;&#65292;3D&#22330;&#26223;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#24212;&#29992;&#22330;&#26223;&#20013;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38656;&#27714;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#27169;&#22359;&#21270;&#21327;&#21516;&#20223;&#30495;&#26041;&#27861;&#65292;&#31216;&#20026;CAVIAR&#12290;&#22312;&#36825;&#37324;&#65292;CAVIAR&#34987;&#21319;&#32423;&#20197;&#25903;&#25345;&#28040;&#24687;&#20256;&#36882;&#24211;&#65292;&#24182;&#21033;&#29992;&#19981;&#21516;&#30340;6G&#30456;&#20851;&#27169;&#25311;&#22120;&#23454;&#29616;&#25968;&#23383;&#23402;&#29983;&#31995;&#32479;&#30340;&#34394;&#25311;&#23545;&#24212;&#29289;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#36129;&#29486;&#21253;&#25324;&#23545;&#19981;&#21516;CAVIAR&#26550;&#26500;&#30340;&#35814;&#32454;&#25551;&#36848;&#65292;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#35780;&#20272;&#22522;&#20110;&#26080;&#20154;&#26426;&#30340;&#25628;&#32034;&#21644;&#25937;&#25588;&#20219;&#21153;&#65288;SAR&#65289;&#30340;6G&#29992;&#20363;&#65292;&#24182;&#29983;&#25104;&#26377;&#20851;&#35745;&#31639;&#36164;&#28304;&#20351;&#29992;&#24773;&#20917;&#30340;&#22522;&#20934;&#25968;&#25454;&#12290;&#20026;&#20102;&#25191;&#34892;SAR&#21327;&#21516;&#20223;&#30495;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#20116;&#20010;&#24320;&#28304;&#35299;&#20915;&#26041;&#26696;&#65306;&#29289;&#29702;&#21644;&#38142;&#36335;&#32423;&#32593;&#32476;&#27169;&#25311;&#22120;Sionna&#65292;&#33258;&#20027;&#36710;&#36742;&#27169;&#25311;&#22120;AirSim&#65292;scikit-learn&#29992;&#20110;&#35757;&#32451;&#29992;&#20110;MIMO&#27874;&#26463;&#36873;&#25321;&#30340;&#20915;&#31574;&#26641;&#12290;
&lt;/p&gt;
&lt;p&gt;
Digital twins are an important technology for advancing mobile communications, specially in use cases that require simultaneously simulating the wireless channel, 3D scenes and machine learning. Aiming at providing a solution to this demand, this work describes a modular co-simulation methodology called CAVIAR. Here, CAVIAR is upgraded to support a message passing library and enable the virtual counterpart of a digital twin system using different 6G-related simulators. The main contributions of this work are the detailed description of different CAVIAR architectures, the implementation of this methodology to assess a 6G use case of UAV-based search and rescue mission (SAR), and the generation of benchmarking data about the computational resource usage. For executing the SAR co-simulation we adopt five open-source solutions: the physical and link level network simulator Sionna, the simulator for autonomous vehicles AirSim, scikit-learn for training a decision tree for MIMO beam selectio
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#22411;&#20540;&#25193;&#23637;&#21644;&#31574;&#30053;&#27491;&#21017;&#21270;&#65292;&#21487;&#20197;&#22312;&#39640;&#32500;&#39046;&#22495;&#20013;&#26377;&#25928;&#22320;&#36827;&#34892;&#31163;&#32447;&#39044;&#35757;&#32451;&#21644;&#22312;&#32447;&#24494;&#35843;&#30340;&#24378;&#21270;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#31163;&#32447;&#21040;&#22312;&#32447;&#24494;&#35843;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#12289;&#31163;&#25955;&#21160;&#21147;&#23398;&#25968;&#25454;&#21644;&#38750;&#31283;&#24577;&#22870;&#21169;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.03306</link><description>&lt;p&gt;
MOTO: &#31163;&#32447;&#39044;&#35757;&#32451;&#19982;&#22312;&#32447;&#24494;&#35843;&#29992;&#20110;&#22522;&#20110;&#27169;&#22411;&#30340;&#26426;&#22120;&#20154;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
MOTO: Offline Pre-training to Online Fine-tuning for Model-based Robot Learning. (arXiv:2401.03306v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#22411;&#20540;&#25193;&#23637;&#21644;&#31574;&#30053;&#27491;&#21017;&#21270;&#65292;&#21487;&#20197;&#22312;&#39640;&#32500;&#39046;&#22495;&#20013;&#26377;&#25928;&#22320;&#36827;&#34892;&#31163;&#32447;&#39044;&#35757;&#32451;&#21644;&#22312;&#32447;&#24494;&#35843;&#30340;&#24378;&#21270;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#31163;&#32447;&#21040;&#22312;&#32447;&#24494;&#35843;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#12289;&#31163;&#25955;&#21160;&#21147;&#23398;&#25968;&#25454;&#21644;&#38750;&#31283;&#24577;&#22870;&#21169;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#29616;&#23454;&#26426;&#22120;&#20154;&#20219;&#21153;&#20013;&#65292;&#20174;&#39640;&#32500;&#35266;&#27979;&#20013;&#36827;&#34892;&#31163;&#32447;&#39044;&#35757;&#32451;&#21644;&#22312;&#32447;&#24494;&#35843;&#30340;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#12290;&#26368;&#36817;&#30340;&#31163;&#32447;&#26080;&#27169;&#22411;&#26041;&#27861;&#25104;&#21151;&#22320;&#20351;&#29992;&#22312;&#32447;&#24494;&#35843;&#26469;&#25552;&#39640;&#26234;&#33021;&#20307;&#22312;&#25968;&#25454;&#25910;&#38598;&#31574;&#30053;&#19978;&#30340;&#24615;&#33021;&#65292;&#25110;&#32773;&#36866;&#24212;&#26032;&#30340;&#20219;&#21153;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#26679;&#26412;&#25928;&#29575;&#21644;&#35299;&#20915;&#30340;&#20219;&#21153;&#22797;&#26434;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#22312;&#24494;&#35843;&#26041;&#38754;&#20173;&#28982;&#34987;&#20302;&#20272;&#20102;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#29616;&#26377;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#39640;&#32500;&#39046;&#22495;&#20013;&#19981;&#36866;&#29992;&#20110;&#31163;&#32447;&#21040;&#22312;&#32447;&#24494;&#35843;&#65292;&#21407;&#22240;&#26159;&#20998;&#24067;&#20559;&#31227;&#12289;&#31163;&#25955;&#21160;&#21147;&#23398;&#25968;&#25454;&#21644;&#38750;&#31283;&#24577;&#22870;&#21169;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#22411;&#20540;&#25193;&#23637;&#21644;&#31574;&#30053;&#27491;&#21017;&#21270;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#37325;&#29992;&#20808;&#21069;&#30340;&#25968;&#25454;&#65292;&#21516;&#26102;&#36890;&#36807;&#25511;&#21046;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#26469;&#38450;&#27490;&#27169;&#22411;&#30340;&#21033;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of offline pre-training and online fine-tuning for reinforcement learning from high-dimensional observations in the context of realistic robot tasks. Recent offline model-free approaches successfully use online fine-tuning to either improve the performance of the agent over the data collection policy or adapt to novel tasks. At the same time, model-based RL algorithms have achieved significant progress in sample efficiency and the complexity of the tasks they can solve, yet remain under-utilized in the fine-tuning setting. In this work, we argue that existing model-based offline RL methods are not suitable for offline-to-online fine-tuning in high-dimensional domains due to issues with distribution shifts, off-dynamics data, and non-stationary rewards. We propose an on-policy model-based method that can efficiently reuse prior data through model-based value expansion and policy regularization, while preventing model exploitation by controlling epistemic uncertainty
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#20917;&#19979;&#26816;&#27979;&#21644;&#20998;&#31867;&#33041;&#32959;&#30244;&#65292;&#24182;&#35299;&#20915;&#20102;&#22312;&#32597;&#35265;&#24773;&#20917;&#19979;&#30340;&#32959;&#30244;&#26816;&#27979;&#38382;&#39064;&#12290;&#30740;&#31350;&#20351;&#29992;&#20102;&#26469;&#33258;&#22269;&#23478;&#33041;&#26144;&#23556;&#23454;&#39564;&#23460;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#20462;&#25913;&#26679;&#26412;&#25968;&#37327;&#21644;&#24739;&#32773;&#20998;&#24067;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#24212;&#23545;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#30340;&#24322;&#24120;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2401.03302</link><description>&lt;p&gt;
&#34892;&#21160;&#20013;&#30340;&#29616;&#23454;&#20027;&#20041;&#65306;&#20351;&#29992;YOLOv8&#21644;DeiT&#20174;&#21307;&#23398;&#22270;&#20687;&#20013;&#35786;&#26029;&#33041;&#32959;&#30244;&#30340;&#24322;&#24120;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
Realism in Action: Anomaly-Aware Diagnosis of Brain Tumors from Medical Images Using YOLOv8 and DeiT. (arXiv:2401.03302v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03302
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#20917;&#19979;&#26816;&#27979;&#21644;&#20998;&#31867;&#33041;&#32959;&#30244;&#65292;&#24182;&#35299;&#20915;&#20102;&#22312;&#32597;&#35265;&#24773;&#20917;&#19979;&#30340;&#32959;&#30244;&#26816;&#27979;&#38382;&#39064;&#12290;&#30740;&#31350;&#20351;&#29992;&#20102;&#26469;&#33258;&#22269;&#23478;&#33041;&#26144;&#23556;&#23454;&#39564;&#23460;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#20462;&#25913;&#26679;&#26412;&#25968;&#37327;&#21644;&#24739;&#32773;&#20998;&#24067;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#24212;&#23545;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#30340;&#24322;&#24120;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#23398;&#31185;&#23398;&#39046;&#22495;&#65292;&#30001;&#20110;&#33041;&#32959;&#30244;&#22312;&#24739;&#32773;&#20013;&#30340;&#32597;&#35265;&#31243;&#24230;&#65292;&#21487;&#38752;&#22320;&#26816;&#27979;&#21644;&#20998;&#31867;&#33041;&#32959;&#30244;&#20173;&#28982;&#26159;&#19968;&#20010;&#33392;&#24040;&#30340;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#22312;&#24322;&#24120;&#24773;&#20917;&#19979;&#26816;&#27979;&#32959;&#30244;&#30340;&#33021;&#21147;&#23545;&#20110;&#30830;&#20445;&#21450;&#26102;&#24178;&#39044;&#21644;&#25913;&#21892;&#24739;&#32773;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#20917;&#19979;&#26816;&#27979;&#21644;&#20998;&#31867;&#33041;&#32959;&#30244;&#12290;&#26469;&#33258;&#22269;&#23478;&#33041;&#26144;&#23556;&#23454;&#39564;&#23460;&#65288;NBML&#65289;&#30340;&#31934;&#36873;&#25968;&#25454;&#38598;&#21253;&#25324;81&#21517;&#24739;&#32773;&#65292;&#20854;&#20013;&#21253;&#25324;30&#20363;&#32959;&#30244;&#30149;&#20363;&#21644;51&#20363;&#27491;&#24120;&#30149;&#20363;&#12290;&#26816;&#27979;&#21644;&#20998;&#31867;&#27969;&#31243;&#34987;&#20998;&#20026;&#20004;&#20010;&#36830;&#32493;&#30340;&#20219;&#21153;&#12290;&#26816;&#27979;&#38454;&#27573;&#21253;&#25324;&#20840;&#38754;&#30340;&#25968;&#25454;&#20998;&#26512;&#21644;&#39044;&#22788;&#29702;&#65292;&#20197;&#20462;&#25913;&#22270;&#20687;&#26679;&#26412;&#21644;&#27599;&#20010;&#31867;&#21035;&#30340;&#24739;&#32773;&#25968;&#37327;&#65292;&#20197;&#31526;&#21512;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#30340;&#24322;&#24120;&#20998;&#24067;&#65288;9&#20010;&#27491;&#24120;&#26679;&#26412;&#23545;&#24212;1&#20010;&#32959;&#30244;&#26679;&#26412;&#65289;&#12290;&#27492;&#22806;&#65292;&#22312;&#27979;&#35797;&#20013;&#38500;&#20102;&#24120;&#35265;&#30340;&#35780;&#20272;&#25351;&#26631;&#22806;&#65292;&#25105;&#20204;&#36824;&#37319;&#29992;&#20102;... [&#25688;&#35201;&#38271;&#24230;&#24050;&#36798;&#21040;&#19978;&#38480;]
&lt;/p&gt;
&lt;p&gt;
In the field of medical sciences, reliable detection and classification of brain tumors from images remains a formidable challenge due to the rarity of tumors within the population of patients. Therefore, the ability to detect tumors in anomaly scenarios is paramount for ensuring timely interventions and improved patient outcomes. This study addresses the issue by leveraging deep learning (DL) techniques to detect and classify brain tumors in challenging situations. The curated data set from the National Brain Mapping Lab (NBML) comprises 81 patients, including 30 Tumor cases and 51 Normal cases. The detection and classification pipelines are separated into two consecutive tasks. The detection phase involved comprehensive data analysis and pre-processing to modify the number of image samples and the number of patients of each class to anomaly distribution (9 Normal per 1 Tumor) to comply with real world scenarios. Next, in addition to common evaluation metrics for the testing, we emplo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#20154;&#26426;&#28909;&#32418;&#22806;&#29289;&#20307;&#26816;&#27979;&#26694;&#26550;&#65292;&#21033;&#29992;FLIR&#30456;&#26426;&#21644;YOLO&#27169;&#22411;&#23454;&#29616;&#20102;&#23545;&#22270;&#29255;&#21644;&#35270;&#39057;&#20013;&#20154;&#20307;&#30340;&#23454;&#26102;&#26816;&#27979;&#65292;&#24179;&#22343;&#31934;&#24230;&#20026;72.5&#65285;&#12290;</title><link>http://arxiv.org/abs/2401.03275</link><description>&lt;p&gt;
&#26080;&#20154;&#26426;&#23454;&#26102;&#20154;&#20307;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Real Time Human Detection by Unmanned Aerial Vehicles. (arXiv:2401.03275v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03275
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#20154;&#26426;&#28909;&#32418;&#22806;&#29289;&#20307;&#26816;&#27979;&#26694;&#26550;&#65292;&#21033;&#29992;FLIR&#30456;&#26426;&#21644;YOLO&#27169;&#22411;&#23454;&#29616;&#20102;&#23545;&#22270;&#29255;&#21644;&#35270;&#39057;&#20013;&#20154;&#20307;&#30340;&#23454;&#26102;&#26816;&#27979;&#65292;&#24179;&#22343;&#31934;&#24230;&#20026;72.5&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#36965;&#24863;&#20013;&#30340;&#19968;&#20010;&#26368;&#37325;&#35201;&#38382;&#39064;&#26159;&#29289;&#20307;&#26816;&#27979;&#65292;&#21363;&#22312;&#22270;&#20687;&#20013;&#35782;&#21035;&#29305;&#23450;&#31867;&#21035;&#30340;&#22810;&#31181;&#20107;&#29289;&#12290;&#26080;&#20154;&#26426;&#65288;UAV&#65289;&#20135;&#29983;&#30340;&#28909;&#32418;&#22806;&#65288;TIR&#65289;&#36965;&#24863;&#22810;&#22330;&#26223;&#29031;&#29255;&#21644;&#35270;&#39057;&#26159;&#20844;&#20849;&#23433;&#20840;&#30340;&#20004;&#20010;&#37325;&#35201;&#25968;&#25454;&#26469;&#28304;&#12290;&#30001;&#20110;&#30446;&#26631;&#23610;&#24230;&#23567;&#12289;&#22330;&#26223;&#20449;&#24687;&#22797;&#26434;&#12289;&#30456;&#23545;&#21487;&#35270;&#35270;&#39057;&#20998;&#36776;&#29575;&#20302;&#21644;&#32570;&#20047;&#20844;&#24320;&#21487;&#29992;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#21644;&#35757;&#32451;&#27169;&#22411;&#65292;&#23427;&#20204;&#30340;&#29289;&#20307;&#26816;&#27979;&#36807;&#31243;&#20173;&#28982;&#22256;&#38590;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22270;&#29255;&#21644;&#35270;&#39057;&#30340;&#26080;&#20154;&#26426;TIR&#29289;&#20307;&#26816;&#27979;&#26694;&#26550;&#12290;&#20351;&#29992;&#29992;&#20110;&#25910;&#38598;&#22320;&#38754;TIR&#29031;&#29255;&#21644;&#35270;&#39057;&#30340;&#21069;&#35270;&#32418;&#22806;&#65288;FLIR&#65289;&#30456;&#26426;&#65292;&#21019;&#24314;&#20102;&#22522;&#20110;CNN&#26550;&#26500;&#30340;&#8220;You Only Look Once&#8221;&#65288;YOLO&#65289;&#27169;&#22411;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#39564;&#35777;&#20219;&#21153;&#20013;&#65292;&#20154;&#20307;&#29289;&#20307;&#30340;IOU&#65288;&#20132;&#24182;&#27604;&#65289;= 0.5&#26102;&#30340;&#24179;&#22343;&#31934;&#24230;&#20026;72.5&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the most important problems in computer vision and remote sensing is object detection, which identifies particular categories of diverse things in pictures. Two crucial data sources for public security are the thermal infrared (TIR) remote sensing multi-scenario photos and videos produced by unmanned aerial vehicles (UAVs). Due to the small scale of the target, complex scene information, low resolution relative to the viewable videos, and dearth of publicly available labeled datasets and training models, their object detection procedure is still difficult. A UAV TIR object detection framework for pictures and videos is suggested in this study. The Forward-looking Infrared (FLIR) cameras used to gather ground-based TIR photos and videos are used to create the ``You Only Look Once'' (YOLO) model, which is based on CNN architecture. Results indicated that in the validating task, detecting human object had an average precision at IOU (Intersection over Union) = 0.5, which was 72.5\%
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#24212;&#29992;CNN-DNN&#32593;&#32476;&#34701;&#21512;&#36827;&#34892;&#33258;&#20027;&#23548;&#33322;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#20223;&#23398;&#20064;&#20351;&#29992;LiDAR&#21644;&#25668;&#20687;&#22836;&#25968;&#25454;&#23545;&#26426;&#22120;&#20154;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807;Monte-Carlo&#27979;&#35797;&#20102;&#20854;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.03267</link><description>&lt;p&gt;
&#22797;&#26434;&#29615;&#22659;&#20013;&#30340;&#33258;&#20027;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
Autonomous Navigation in Complex Environments. (arXiv:2401.03267v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03267
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#24212;&#29992;CNN-DNN&#32593;&#32476;&#34701;&#21512;&#36827;&#34892;&#33258;&#20027;&#23548;&#33322;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#20223;&#23398;&#20064;&#20351;&#29992;LiDAR&#21644;&#25668;&#20687;&#22836;&#25968;&#25454;&#23545;&#26426;&#22120;&#20154;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807;Monte-Carlo&#27979;&#35797;&#20102;&#20854;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#24212;&#29992;CNN-DNN&#32593;&#32476;&#34701;&#21512;&#26469;&#26500;&#24314;&#26426;&#22120;&#20154;&#23548;&#33322;&#25511;&#21046;&#22120;&#30340;&#26041;&#27861;&#12290;&#27169;&#25311;&#29615;&#22659;&#29992;&#20110;&#27169;&#25311;&#22320;&#19979;&#25937;&#25588;&#24773;&#26223;&#65292;&#23558;&#33258;&#20027;&#20195;&#29702;&#20219;&#21153;&#35774;&#23450;&#20026;&#22312;&#26410;&#30693;&#30340;&#27934;&#31348;&#31995;&#32479;&#20013;&#23547;&#25214;&#30446;&#26631;&#12290;&#37319;&#29992;&#27169;&#20223;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;LiDAR&#21644;&#25668;&#20687;&#22836;&#25968;&#25454;&#26469;&#35757;&#32451;&#25511;&#21046;&#31639;&#27861;&#20197;&#36827;&#34892;&#31354;&#38388;&#23548;&#33322;&#24182;&#25214;&#21040;&#30446;&#26631;&#12290;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#36890;&#36807;Monte-Carlo&#36827;&#34892;&#40065;&#26834;&#24615;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the application of CNN-DNN network fusion to construct a robot navigation controller within a simulated environment. The simulated environment is constructed to model a subterranean rescue situation, such that an autonomous agent is tasked with finding a goal within an unknown cavernous system. Imitation learning is used to train the control algorithm to use LiDAR and camera data to navigate the space and find the goal. The trained model is then tested for robustness using Monte-Carlo.
&lt;/p&gt;</description></item><item><title>SeqNAS&#26159;&#19968;&#31181;&#38024;&#23545;&#20107;&#20214;&#24207;&#21015;&#20998;&#31867;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#31616;&#21333;&#32780;&#34920;&#36798;&#21147;&#24378;&#30340;&#25628;&#32034;&#31354;&#38388;&#65292;&#21033;&#29992;&#24120;&#29992;&#30340;&#26500;&#24314;&#22359;&#36827;&#34892;&#25628;&#32034;&#65292;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#20219;&#21153;&#29305;&#23450;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2401.03246</link><description>&lt;p&gt;
SeqNAS: &#20107;&#20214;&#24207;&#21015;&#20998;&#31867;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
SeqNAS: Neural Architecture Search for Event Sequence Classification. (arXiv:2401.03246v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03246
&lt;/p&gt;
&lt;p&gt;
SeqNAS&#26159;&#19968;&#31181;&#38024;&#23545;&#20107;&#20214;&#24207;&#21015;&#20998;&#31867;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#31616;&#21333;&#32780;&#34920;&#36798;&#21147;&#24378;&#30340;&#25628;&#32034;&#31354;&#38388;&#65292;&#21033;&#29992;&#24120;&#29992;&#30340;&#26500;&#24314;&#22359;&#36827;&#34892;&#25628;&#32034;&#65292;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#20219;&#21153;&#29305;&#23450;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;(NAS)&#26041;&#27861;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#20010;&#34892;&#19994;&#65292;&#20197;&#33719;&#24471;&#39640;&#36136;&#37327;&#30340;&#20219;&#21153;&#29305;&#23450;&#35299;&#20915;&#26041;&#26696;&#65292;&#26368;&#22823;&#38480;&#24230;&#22320;&#20943;&#23569;&#20154;&#20026;&#24178;&#39044;&#12290;&#20107;&#20214;&#24207;&#21015;&#22312;&#21508;&#31181;&#24037;&#19994;&#24212;&#29992;&#20013;&#24471;&#21040;&#24191;&#27867;&#20351;&#29992;&#65292;&#21253;&#25324;&#27969;&#22833;&#39044;&#27979;&#12289;&#23458;&#25143;&#20998;&#21106;&#12289;&#27450;&#35784;&#26816;&#27979;&#21644;&#25925;&#38556;&#35786;&#26029;&#31561;&#12290;&#36825;&#20123;&#25968;&#25454;&#21253;&#25324;&#20855;&#26377;&#19981;&#35268;&#21017;&#26102;&#38388;&#25139;&#30340;&#20998;&#31867;&#21644;&#23454;&#25968;&#20540;&#32452;&#20214;&#12290;&#23613;&#31649;NAS&#26041;&#27861;&#30340;&#26377;&#29992;&#24615;&#65292;&#20294;&#20043;&#21069;&#30340;&#26041;&#27861;&#20165;&#24212;&#29992;&#20110;&#20854;&#20182;&#39046;&#22495;&#30340;&#22270;&#20687;&#12289;&#25991;&#26412;&#25110;&#26102;&#38388;&#24207;&#21015;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;NAS&#31639;&#27861;SeqNAS&#26469;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#35813;&#31639;&#27861;&#19987;&#38376;&#29992;&#20110;&#20107;&#20214;&#24207;&#21015;&#20998;&#31867;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#34920;&#36798;&#21147;&#24378;&#30340;&#25628;&#32034;&#31354;&#38388;&#65292;&#21033;&#29992;&#24120;&#29992;&#30340;&#20107;&#20214;&#24207;&#21015;&#20998;&#31867;&#26500;&#24314;&#22359;&#65292;&#21253;&#25324;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#12289;&#21367;&#31215;&#21644;&#24490;&#29615;&#21333;&#20803;&#12290;&#20026;&#20102;&#36827;&#34892;&#25628;&#32034;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#39034;&#24207;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#24182;&#21033;&#29992;&#20808;&#21069;&#35757;&#32451;&#36807;&#30340;&#27169;&#22411;&#20316;&#20026;&#25945;&#24072;&#27169;&#22411;&#30340;&#38598;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Architecture Search (NAS) methods are widely used in various industries to obtain high quality taskspecific solutions with minimal human intervention. Event Sequences find widespread use in various industrial applications including churn prediction customer segmentation fraud detection and fault diagnosis among others. Such data consist of categorical and real-valued components with irregular timestamps. Despite the usefulness of NAS methods previous approaches only have been applied to other domains images texts or time series. Our work addresses this limitation by introducing a novel NAS algorithm SeqNAS specifically designed for event sequence classification. We develop a simple yet expressive search space that leverages commonly used building blocks for event sequence classification including multihead self attention convolutions and recurrent cells. To perform the search we adopt sequential Bayesian Optimization and utilize previously trained models as an ensemble of teache
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22238;&#39038;&#20102;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#22312;&#36816;&#31609;&#23398;&#20013;&#30340;&#24212;&#29992;&#65292;&#26088;&#22312;&#36890;&#36807;&#25913;&#36827;&#36816;&#31609;&#23398;&#36807;&#31243;&#30340;&#19981;&#21516;&#38454;&#27573;&#26469;&#25552;&#39640;&#20854;&#25928;&#26524;&#21644;&#25928;&#29575;&#12290;&#20154;&#24037;&#26234;&#33021;&#19982;&#36816;&#31609;&#23398;&#30340;&#21327;&#21516;&#20316;&#29992;&#23558;&#25512;&#21160;&#39046;&#22495;&#20869;&#30340;&#21019;&#26032;&#21644;&#20915;&#31574;&#21046;&#23450;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2401.03244</link><description>&lt;p&gt;
&#36816;&#31609;&#23398;&#30340;&#20154;&#24037;&#26234;&#33021;&#65306;&#25913;&#21464;&#36816;&#31609;&#23398;&#36807;&#31243;&#30340;&#38761;&#21629;&#24615;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence for Operations Research: Revolutionizing the Operations Research Process. (arXiv:2401.03244v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03244
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22238;&#39038;&#20102;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#22312;&#36816;&#31609;&#23398;&#20013;&#30340;&#24212;&#29992;&#65292;&#26088;&#22312;&#36890;&#36807;&#25913;&#36827;&#36816;&#31609;&#23398;&#36807;&#31243;&#30340;&#19981;&#21516;&#38454;&#27573;&#26469;&#25552;&#39640;&#20854;&#25928;&#26524;&#21644;&#25928;&#29575;&#12290;&#20154;&#24037;&#26234;&#33021;&#19982;&#36816;&#31609;&#23398;&#30340;&#21327;&#21516;&#20316;&#29992;&#23558;&#25512;&#21160;&#39046;&#22495;&#20869;&#30340;&#21019;&#26032;&#21644;&#20915;&#31574;&#21046;&#23450;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#36805;&#36895;&#21457;&#23637;&#20026;&#21508;&#20010;&#39046;&#22495;&#65292;&#21253;&#25324;&#36816;&#31609;&#23398;&#65292;&#24320;&#36767;&#20102;&#26032;&#30340;&#26426;&#36935;&#12290;&#26412;&#35843;&#26597;&#35770;&#25991;&#25506;&#35752;&#20102;&#22312;&#36816;&#31609;&#23398;&#36807;&#31243;&#20013;&#30340;&#20154;&#24037;&#26234;&#33021;&#25972;&#21512;&#65288;AI4OR&#65289;&#65292;&#20197;&#25552;&#39640;&#20854;&#22312;&#21442;&#25968;&#29983;&#25104;&#12289;&#27169;&#22411;&#26500;&#24314;&#21644;&#27169;&#22411;&#20248;&#21270;&#31561;&#22810;&#20010;&#38454;&#27573;&#30340;&#25928;&#26524;&#21644;&#25928;&#29575;&#12290;&#36890;&#36807;&#20840;&#38754;&#27010;&#36848;&#29616;&#26377;&#25216;&#26415;&#30340;&#21516;&#26102;&#65292;&#30740;&#31350;&#20154;&#24037;&#26234;&#33021;&#25913;&#21464;&#36816;&#31609;&#23398;&#30340;&#28508;&#21147;&#65292;&#26412;&#25991;&#26088;&#22312;&#28608;&#21457;&#36827;&#19968;&#27493;&#30740;&#31350;&#21644;&#21019;&#26032;&#65292;&#24320;&#21457;&#20154;&#24037;&#26234;&#33021;&#22686;&#24378;&#30340;&#36816;&#31609;&#23398;&#26041;&#27861;&#21644;&#24037;&#20855;&#12290;&#20154;&#24037;&#26234;&#33021;&#21644;&#36816;&#31609;&#23398;&#30340;&#21327;&#21516;&#20316;&#29992;&#23558;&#25512;&#21160;&#21508;&#20010;&#39046;&#22495;&#30340;&#37325;&#22823;&#36827;&#23637;&#21644;&#26032;&#22411;&#35299;&#20915;&#26041;&#26696;&#65292;&#26368;&#32456;&#23454;&#29616;&#26356;&#21152;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#20915;&#31574;&#21046;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid advancement of artificial intelligence (AI) techniques has opened up new opportunities to revolutionize various fields, including operations research (OR). This survey paper explores the integration of AI within the OR process (AI4OR) to enhance its effectiveness and efficiency across multiple stages, such as parameter generation, model formulation, and model optimization. By providing a comprehensive overview of the state-of-the-art and examining the potential of AI to transform OR, this paper aims to inspire further research and innovation in the development of AI-enhanced OR methods and tools. The synergy between AI and OR is poised to drive significant advancements and novel solutions in a multitude of domains, ultimately leading to more effective and efficient decision-making.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#20351;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#35780;&#20272;&#25945;&#24072;&#22312;&#23398;&#29983;&#25968;&#23398;&#38169;&#35823;&#21453;&#24212;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#30740;&#31350;&#32467;&#26524;&#21457;&#29616;&#65292;GPT-3.5-Turbo&#21644;GPT-4&#22312;&#35780;&#20272;&#25945;&#24072;&#30340;&#34920;&#29616;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#29087;&#32451;&#24230;&#65292;&#20294;&#22312;&#35782;&#21035;&#23398;&#29983;&#38656;&#35201;&#39069;&#22806;&#24110;&#21161;&#30340;&#24773;&#20917;&#19978;&#20173;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.03238</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#25945;&#24072;&#23545;&#23398;&#29983;&#25968;&#23398;&#38169;&#35823;&#21453;&#24212;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Using Large Language Models to Assess Tutors' Performance in Reacting to Students Making Math Errors. (arXiv:2401.03238v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03238
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#20351;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#35780;&#20272;&#25945;&#24072;&#22312;&#23398;&#29983;&#25968;&#23398;&#38169;&#35823;&#21453;&#24212;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#30740;&#31350;&#32467;&#26524;&#21457;&#29616;&#65292;GPT-3.5-Turbo&#21644;GPT-4&#22312;&#35780;&#20272;&#25945;&#24072;&#30340;&#34920;&#29616;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#29087;&#32451;&#24230;&#65292;&#20294;&#22312;&#35782;&#21035;&#23398;&#29983;&#38656;&#35201;&#39069;&#22806;&#24110;&#21161;&#30340;&#24773;&#20917;&#19978;&#20173;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#34920;&#26126;&#65292;&#25945;&#24072;&#22312;&#24212;&#23545;&#20302;&#25928;&#23398;&#29983;&#30340;&#25968;&#23398;&#38169;&#35823;&#26102;&#24212;&#37319;&#21462;&#25112;&#30053;&#24615;&#30340;&#26041;&#27861;&#12290;&#25945;&#24072;&#19981;&#24212;&#30452;&#25509;&#25351;&#20986;&#38169;&#35823;&#65292;&#32780;&#24212;&#24341;&#23548;&#23398;&#29983;&#33258;&#24049;&#21457;&#29616;&#21644;&#32416;&#27491;&#38169;&#35823;&#12290;&#34429;&#28982;&#25945;&#24072;&#35838;&#31243;&#24050;&#32463;&#24341;&#20837;&#20102;&#36825;&#31181;&#25945;&#23398;&#25216;&#33021;&#65292;&#20294;&#20154;&#24037;&#35780;&#20215;&#25945;&#24072;&#22312;&#23454;&#38469;&#24212;&#29992;&#35813;&#31574;&#30053;&#26102;&#26159;&#22256;&#38590;&#19988;&#32791;&#26102;&#30340;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#23454;&#38469;&#36741;&#23548;&#36807;&#31243;&#20013;&#21521;&#25945;&#24072;&#25552;&#20379;&#23454;&#26102;&#35780;&#20272;&#30340;&#28508;&#21147;&#26174;&#31034;&#20986;&#26469;&#65292;&#20294;&#22312;&#36825;&#31181;&#24773;&#22659;&#19979;&#65292;&#23545;&#20854;&#20934;&#30830;&#24615;&#20102;&#35299;&#29978;&#23569;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#35780;&#20272;&#23454;&#38469;&#25945;&#24072;&#23545;&#23398;&#29983;&#25968;&#23398;&#38169;&#35823;&#21453;&#24212;&#34920;&#29616;&#20013;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#20998;&#26512;50&#20010;&#30495;&#23454;&#36741;&#23548;&#23545;&#35805;&#65292;&#25105;&#20204;&#21457;&#29616;GPT-3.5-Turbo&#21644;GPT-4&#22312;&#35780;&#20272;&#19982;&#23398;&#29983;&#29359;&#38169;&#21453;&#24212;&#30456;&#20851;&#30340;&#26631;&#20934;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#29087;&#32451;&#24230;&#12290;&#28982;&#32780;&#65292;&#36825;&#20004;&#20010;&#27169;&#22411;&#22312;&#35782;&#21035;&#23398;&#29983;&#38656;&#35201;&#39069;&#22806;&#24110;&#21161;&#30340;&#24773;&#20917;&#19978;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Research suggests that tutors should adopt a strategic approach when addressing math errors made by low-efficacy students. Rather than drawing direct attention to the error, tutors should guide the students to identify and correct their mistakes on their own. While tutor lessons have introduced this pedagogical skill, human evaluation of tutors applying this strategy is arduous and time-consuming. Large language models (LLMs) show promise in providing real-time assessment to tutors during their actual tutoring sessions, yet little is known regarding their accuracy in this context. In this study, we investigate the capacity of generative AI to evaluate real-life tutors' performance in responding to students making math errors. By analyzing 50 real-life tutoring dialogues, we find both GPT-3.5-Turbo and GPT-4 demonstrate proficiency in assessing the criteria related to reacting to students making errors. However, both models exhibit limitations in recognizing instances where the student 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#26368;&#22823;&#21270;&#27169;&#22411;&#25910;&#25947;&#36895;&#29575;&#30340;Split Learning&#32908;&#30005;&#20551;&#32930;&#25511;&#21046;&#20013;&#30340;&#20999;&#23618;&#36873;&#25321;&#31639;&#27861;&#65292;&#36890;&#36807;&#21152;&#36895;&#25910;&#25947;&#36807;&#31243;&#25552;&#39640;&#20102;&#20551;&#32930;&#25511;&#21046;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.03233</link><description>&lt;p&gt;
&#22522;&#20110;Split Learning&#30340;&#32908;&#30005;&#20551;&#32930;&#25511;&#21046;&#20013;&#30340;&#25910;&#25947;&#36895;&#29575;&#26368;&#22823;&#21270;
&lt;/p&gt;
&lt;p&gt;
Convergence Rate Maximization for Split Learning-based Control of EMG Prosthetic Devices. (arXiv:2401.03233v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03233
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#26368;&#22823;&#21270;&#27169;&#22411;&#25910;&#25947;&#36895;&#29575;&#30340;Split Learning&#32908;&#30005;&#20551;&#32930;&#25511;&#21046;&#20013;&#30340;&#20999;&#23618;&#36873;&#25321;&#31639;&#27861;&#65292;&#36890;&#36807;&#21152;&#36895;&#25910;&#25947;&#36807;&#31243;&#25552;&#39640;&#20102;&#20551;&#32930;&#25511;&#21046;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Split Learning (SL)&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#29615;&#22659;&#20013;&#24212;&#29992;&#20110;&#22522;&#20110;&#32908;&#30005;&#30340;&#20551;&#32930;&#25511;&#21046;&#12290;&#19982;&#28145;&#24230;&#23398;&#20064;&#21644;&#32852;&#37030;&#23398;&#20064;&#31561;&#20854;&#20182;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;SL&#33021;&#22815;&#25552;&#20379;&#26356;&#20248;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#22240;&#20026;&#20551;&#32930;&#35774;&#22791;&#22312;&#22788;&#29702;&#33021;&#21147;&#21644;&#30005;&#27744;&#23551;&#21629;&#26041;&#38754;&#38750;&#24120;&#26377;&#38480;&#12290;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#65292;&#23454;&#29616;SL&#30340;&#21487;&#34892;&#24615;&#28304;&#20110;&#20854;&#22266;&#26377;&#30340;&#27169;&#22411;&#20998;&#21106;&#65292;&#20854;&#20013;&#23458;&#25143;&#31471;&#25191;&#34892;&#36739;&#23567;&#30340;&#27169;&#22411;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#36873;&#25321;&#19981;&#24688;&#24403;&#30340;&#20999;&#23618;&#20250;&#38459;&#30861;SL&#31995;&#32479;&#30340;&#35757;&#32451;&#36807;&#31243;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26368;&#22823;&#21270;&#27169;&#22411;&#25910;&#25947;&#36895;&#29575;&#30340;&#20999;&#23618;&#36873;&#25321;&#31639;&#27861;&#12290;&#24615;&#33021;&#35780;&#20272;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#25913;&#21892;&#20551;&#32930;&#25511;&#21046;&#30340;&#32908;&#30005;&#27169;&#24335;&#35782;&#21035;&#20219;&#21153;&#20013;&#26174;&#33879;&#21152;&#36895;&#20102;&#25910;&#25947;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Split Learning (SL) is a promising Distributed Learning approach in electromyography (EMG) based prosthetic control, due to its applicability within resource-constrained environments. Other learning approaches, such as Deep Learning and Federated Learning (FL), provide suboptimal solutions, since prosthetic devices are extremely limited in terms of processing power and battery life. The viability of implementing SL in such scenarios is caused by its inherent model partitioning, with clients executing the smaller model segment. However, selecting an inadequate cut layer hinders the training process in SL systems. This paper presents an algorithm for optimal cut layer selection in terms of maximizing the convergence rate of the model. The performance evaluation demonstrates that the proposed algorithm substantially accelerates the convergence in an EMG pattern recognition task for improving prosthetic device control.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25552;&#31034;&#37325;&#26032;&#25551;&#36848;&#30340;&#31574;&#30053;&#26469;&#31283;&#23450;&#38646;&#26679;&#26412;&#22270;&#20687;&#32763;&#35793;&#20013;&#30340;&#25193;&#25955;&#36807;&#31243;&#65292;&#35299;&#20915;&#20102;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#22312;&#21453;&#28436;&#36807;&#31243;&#20013;&#30340;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.03221</link><description>&lt;p&gt;
MirrorDiffusion: &#36890;&#36807;&#25552;&#31034;&#37325;&#26032;&#25551;&#36848;&#26469;&#31283;&#23450;&#38646;&#26679;&#26412;&#22270;&#20687;&#32763;&#35793;&#20013;&#30340;&#25193;&#25955;&#36807;&#31243;&#21450;&#20854;&#21518;&#32493;&#24037;&#20316;
&lt;/p&gt;
&lt;p&gt;
MirrorDiffusion: Stabilizing Diffusion Process in Zero-shot Image Translation by Prompts Redescription and Beyond. (arXiv:2401.03221v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03221
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25552;&#31034;&#37325;&#26032;&#25551;&#36848;&#30340;&#31574;&#30053;&#26469;&#31283;&#23450;&#38646;&#26679;&#26412;&#22270;&#20687;&#32763;&#35793;&#20013;&#30340;&#25193;&#25955;&#36807;&#31243;&#65292;&#35299;&#20915;&#20102;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#22312;&#21453;&#28436;&#36807;&#31243;&#20013;&#30340;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#25104;&#20026;&#22270;&#20687;&#22788;&#29702;&#39046;&#22495;&#30340;&#26032;&#33539;&#20363;&#65292;&#21253;&#25324;&#20869;&#23481;&#29983;&#25104;&#12289;&#22270;&#20687;&#24674;&#22797;&#21644;&#22270;&#20687;&#21040;&#22270;&#20687;&#30340;&#32763;&#35793;&#12290;&#32473;&#23450;&#19968;&#20010;&#30446;&#26631;&#25552;&#31034;&#65292;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPM&#65289;&#33021;&#22815;&#29983;&#25104;&#36924;&#30495;&#32780;&#21512;&#36866;&#30340;&#22270;&#20687;&#12290;&#20973;&#20511;&#36825;&#20010;&#21560;&#24341;&#20154;&#30340;&#29305;&#24615;&#65292;&#22270;&#20687;&#32763;&#35793;&#20219;&#21153;&#26377;&#28508;&#21147;&#22312;&#27809;&#26377;&#30446;&#26631;&#22270;&#20687;&#26679;&#26412;&#36827;&#34892;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#12290;&#36890;&#36807;&#20351;&#29992;&#30446;&#26631;&#25991;&#26412;&#25552;&#31034;&#36827;&#34892;&#39046;&#22495;&#33258;&#36866;&#24212;&#65292;&#25193;&#25955;&#27169;&#22411;&#33021;&#22815;&#20248;&#21183;&#22320;&#23454;&#29616;&#38646;&#26679;&#26412;&#22270;&#20687;&#21040;&#22270;&#20687;&#30340;&#32763;&#35793;&#12290;&#28982;&#32780;&#65292;DDPM&#30340;&#37319;&#26679;&#21644;&#21453;&#28436;&#36807;&#31243;&#26159;&#38543;&#26426;&#30340;&#65292;&#22240;&#27492;&#21453;&#28436;&#36807;&#31243;&#24120;&#24120;&#26080;&#27861;&#23436;&#20840;&#37325;&#26500;&#36755;&#20837;&#20869;&#23481;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#25193;&#25955;&#21644;&#21453;&#28436;&#36807;&#31243;&#20013;&#65292;&#20301;&#31227;&#25928;&#24212;&#20250;&#36880;&#28176;&#31215;&#32047;&#65292;&#23548;&#33268;&#37325;&#26500;&#32467;&#26524;&#20559;&#31163;&#28304;&#22495;&#12290;&#20026;&#20102;&#20351;&#37325;&#26500;&#26356;&#21152;&#26126;&#30830;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25552;&#31034;&#37325;&#26032;&#25551;&#36848;&#31574;&#30053;&#26469;&#23454;&#29616;&#38236;&#20687;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, text-to-image diffusion models become a new paradigm in image processing fields, including content generation, image restoration and image-to-image translation. Given a target prompt, Denoising Diffusion Probabilistic Models (DDPM) are able to generate realistic yet eligible images. With this appealing property, the image translation task has the potential to be free from target image samples for supervision. By using a target text prompt for domain adaption, the diffusion model is able to implement zero-shot image-to-image translation advantageously. However, the sampling and inversion processes of DDPM are stochastic, and thus the inversion process often fail to reconstruct the input content. Specifically, the displacement effect will gradually accumulated during the diffusion and inversion processes, which led to the reconstructed results deviating from the source domain. To make reconstruction explicit, we propose a prompt redescription strategy to realize a mirror effect
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#26159;&#31532;&#19968;&#27425;&#20934;&#30830;&#20998;&#26512;&#38750;&#32447;&#24615;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#30340;&#23398;&#20064;&#32467;&#26524;&#65292;&#36890;&#36807;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#35757;&#32451;&#27169;&#22411;&#24182;&#35777;&#26126;&#20854;&#25910;&#25947;&#21040;&#23616;&#37096;&#26368;&#23567;&#20540;&#65292;&#22312;&#29702;&#35299;&#25968;&#25454;&#34920;&#31034;&#21487;&#23398;&#20064;&#24615;&#26041;&#38754;&#20570;&#20986;&#20102;&#37325;&#35201;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2401.03214</link><description>&lt;p&gt;
&#29702;&#35299;&#38750;&#32447;&#24615;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#34920;&#31034;&#21487;&#23398;&#20064;&#24615;
&lt;/p&gt;
&lt;p&gt;
Understanding Representation Learnability of Nonlinear Self-Supervised Learning. (arXiv:2401.03214v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03214
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#26159;&#31532;&#19968;&#27425;&#20934;&#30830;&#20998;&#26512;&#38750;&#32447;&#24615;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#30340;&#23398;&#20064;&#32467;&#26524;&#65292;&#36890;&#36807;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#35757;&#32451;&#27169;&#22411;&#24182;&#35777;&#26126;&#20854;&#25910;&#25947;&#21040;&#23616;&#37096;&#26368;&#23567;&#20540;&#65292;&#22312;&#29702;&#35299;&#25968;&#25454;&#34920;&#31034;&#21487;&#23398;&#20064;&#24615;&#26041;&#38754;&#20570;&#20986;&#20102;&#37325;&#35201;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#22312;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#20013;&#32463;&#39564;&#35777;&#26126;&#20854;&#25968;&#25454;&#34920;&#31034;&#21487;&#23398;&#20064;&#24615;&#12290;&#20851;&#20110;&#25968;&#25454;&#34920;&#31034;&#21487;&#23398;&#20064;&#24615;&#30340;&#29702;&#35770;&#30740;&#31350;&#24456;&#23569;&#65292;&#20854;&#20013;&#35768;&#22810;&#23558;&#38750;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#35270;&#20026;&#8220;&#40657;&#31665;&#8221;&#65292;&#20165;&#20851;&#27880;&#26368;&#32456;&#30340;&#25968;&#25454;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#31070;&#32463;&#32593;&#32476;&#30340;&#20934;&#30830;&#23398;&#20064;&#32467;&#26524;&#23545;&#20110;&#25551;&#36848;SSL&#27169;&#22411;&#23398;&#21040;&#30340;&#25968;&#25454;&#20998;&#24067;&#29305;&#24449;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#30340;&#35770;&#25991;&#26159;&#39318;&#27425;&#20934;&#30830;&#20998;&#26512;&#38750;&#32447;&#24615;SSL&#27169;&#22411;&#30340;&#23398;&#20064;&#32467;&#26524;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#21253;&#21547;&#20004;&#20010;&#29305;&#24449;&#30340;&#29609;&#20855;&#25968;&#25454;&#20998;&#24067;&#65306;&#19982;&#26631;&#31614;&#30456;&#20851;&#30340;&#29305;&#24449;&#21644;&#38544;&#34255;&#29305;&#24449;&#12290;&#19982;&#20197;&#24448;&#30340;&#20381;&#36182;&#20110;&#38381;&#24335;&#35299;&#30340;&#32447;&#24615;&#35774;&#32622;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#22312;&#29305;&#23450;&#30340;&#21021;&#22987;&#21270;&#21306;&#22495;&#19979;&#35757;&#32451;&#19968;&#20010;1&#23618;&#38750;&#32447;&#24615;SSL&#27169;&#22411;&#65292;&#24182;&#35777;&#26126;&#27169;&#22411;&#25910;&#25947;&#21040;&#19968;&#20010;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;&#27492;&#22806;&#65292;&#19982;&#22797;&#26434;&#30340;&#36845;&#20195;&#20998;&#26512;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#26512;&#36807;&#31243;&#65292;&#20351;&#29992;t
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) has empirically shown its data representation learnability in many downstream tasks. There are only a few theoretical works on data representation learnability, and many of those focus on final data representation, treating the nonlinear neural network as a ``black box". However, the accurate learning results of neural networks are crucial for describing the data distribution features learned by SSL models. Our paper is the first to analyze the learning results of the nonlinear SSL model accurately. We consider a toy data distribution that contains two features: the label-related feature and the hidden feature. Unlike previous linear setting work that depends on closed-form solutions, we use the gradient descent algorithm to train a 1-layer nonlinear SSL model with a certain initialization region and prove that the model converges to a local minimum. Furthermore, different from the complex iterative analysis, we propose a new analysis process which uses t
&lt;/p&gt;</description></item><item><title>&#22312;&#38750;&#31283;&#23450;&#29615;&#22659;&#19979;&#30340;&#20915;&#31574;&#21046;&#23450;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;--&#31574;&#30053;&#22686;&#24378;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65288;PA-MCTS&#65289;&#65292;&#23427;&#23558;&#22312;&#32447;&#25628;&#32034;&#19982;&#31574;&#30053;&#23398;&#20064;&#30456;&#32467;&#21512;&#12290;</title><link>http://arxiv.org/abs/2401.03197</link><description>&lt;p&gt;
&#22312;&#38750;&#31283;&#23450;&#29615;&#22659;&#19979;&#30340;&#20915;&#31574;&#21046;&#23450;&#19982;&#31574;&#30053;&#22686;&#24378;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Decision Making in Non-Stationary Environments with Policy-Augmented Search. (arXiv:2401.03197v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03197
&lt;/p&gt;
&lt;p&gt;
&#22312;&#38750;&#31283;&#23450;&#29615;&#22659;&#19979;&#30340;&#20915;&#31574;&#21046;&#23450;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;--&#31574;&#30053;&#22686;&#24378;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65288;PA-MCTS&#65289;&#65292;&#23427;&#23558;&#22312;&#32447;&#25628;&#32034;&#19982;&#31574;&#30053;&#23398;&#20064;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#37325;&#35201;&#38382;&#39064;&#20013;&#65292;&#23384;&#22312;&#30528;&#19981;&#30830;&#23450;&#24615;&#19979;&#30340;&#36830;&#32493;&#20915;&#31574;&#21046;&#23450;&#12290;&#38024;&#23545;&#36825;&#31867;&#38382;&#39064;&#65292;&#20256;&#32479;&#30340;&#26041;&#27861;&#21253;&#25324;&#24378;&#21270;&#23398;&#20064;&#21644;&#22312;&#32447;&#25628;&#32034;&#65288;&#22914;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65289;&#12290;&#21069;&#32773;&#36890;&#36807;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#26469;&#23398;&#20064;&#31574;&#30053;&#65288;&#36890;&#24120;&#22312;&#25191;&#34892;&#20043;&#21069;&#23436;&#25104;&#65289;&#65292;&#32780;&#21518;&#32773;&#22312;&#20915;&#31574;&#26102;&#20351;&#29992;&#29615;&#22659;&#30340;&#29983;&#25104;&#27169;&#22411;&#26469;&#37319;&#26679;&#26377;&#21069;&#26223;&#30340;&#34892;&#21160;&#36712;&#36857;&#12290;&#22312;&#38750;&#31283;&#23450;&#29615;&#22659;&#19979;&#30340;&#20915;&#31574;&#21046;&#23450;&#23588;&#20026;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#20195;&#29702;&#25805;&#20316;&#30340;&#29615;&#22659;&#21487;&#33021;&#38543;&#26102;&#38388;&#21464;&#21270;&#12290;&#20004;&#31181;&#26041;&#27861;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#37117;&#23384;&#22312;&#32570;&#38519;--&#19968;&#26041;&#38754;&#65292;&#25191;&#34892;&#20043;&#21069;&#23398;&#20064;&#30340;&#31574;&#30053;&#22312;&#29615;&#22659;&#25913;&#21464;&#26102;&#21464;&#24471;&#38472;&#26087;&#65292;&#37325;&#26032;&#23398;&#20064;&#38656;&#35201;&#26102;&#38388;&#21644;&#35745;&#31639;&#36164;&#28304;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22312;&#32447;&#25628;&#32034;&#22312;&#20801;&#35768;&#30340;&#36816;&#34892;&#26102;&#38388;&#26377;&#38480;&#26102;&#21487;&#33021;&#20250;&#36820;&#22238;&#27425;&#20248;&#34892;&#21160;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;--&#31574;&#30053;&#22686;&#24378;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65288;PA-MCTS&#65289;&#65292;&#23427;&#23558;&#22312;&#32447;&#25628;&#32034;&#19982;&#31574;&#30053;&#23398;&#20064;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequential decision-making under uncertainty is present in many important problems. Two popular approaches for tackling such problems are reinforcement learning and online search (e.g., Monte Carlo tree search). While the former learns a policy by interacting with the environment (typically done before execution), the latter uses a generative model of the environment to sample promising action trajectories at decision time. Decision-making is particularly challenging in non-stationary environments, where the environment in which an agent operates can change over time. Both approaches have shortcomings in such settings -- on the one hand, policies learned before execution become stale when the environment changes and relearning takes both time and computational effort. Online search, on the other hand, can return sub-optimal actions when there are limitations on allowed runtime. In this paper, we introduce \textit{Policy-Augmented Monte Carlo tree search} (PA-MCTS), which combines actio
&lt;/p&gt;</description></item><item><title>SecureReg&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#22810;&#23618;&#24863;&#30693;&#22120;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22495;&#21517;&#27880;&#20876;&#36807;&#31243;&#20013;&#20027;&#21160;&#26292;&#38706;&#24694;&#24847;&#22495;&#21517;&#27880;&#20876;&#65292;&#25552;&#20379;&#20102;&#26089;&#26399;&#23041;&#32961;&#26816;&#27979;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#28431;&#27934;&#31383;&#21475;&#65292;&#24182;&#20026;&#20027;&#21160;&#39044;&#38450;&#24615;&#25805;&#20316;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2401.03196</link><description>&lt;p&gt;
SecureReg:&#19968;&#20010;&#32467;&#21512;&#26041;&#27861;&#29992;&#20110;&#20027;&#21160;&#26292;&#38706;&#24694;&#24847;&#22495;&#21517;&#27880;&#20876;
&lt;/p&gt;
&lt;p&gt;
SecureReg: A Combined Framework for Proactively Exposing Malicious Domain Name Registrations. (arXiv:2401.03196v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03196
&lt;/p&gt;
&lt;p&gt;
SecureReg&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#22810;&#23618;&#24863;&#30693;&#22120;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22495;&#21517;&#27880;&#20876;&#36807;&#31243;&#20013;&#20027;&#21160;&#26292;&#38706;&#24694;&#24847;&#22495;&#21517;&#27880;&#20876;&#65292;&#25552;&#20379;&#20102;&#26089;&#26399;&#23041;&#32961;&#26816;&#27979;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#28431;&#27934;&#31383;&#21475;&#65292;&#24182;&#20026;&#20027;&#21160;&#39044;&#38450;&#24615;&#25805;&#20316;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#32593;&#32476;&#23433;&#20840;&#23041;&#32961;&#19981;&#26029;&#22686;&#21152;&#65292;&#19981;&#27861;&#20998;&#23376;&#27599;&#22825;&#27880;&#20876;&#25968;&#21315;&#20010;&#26032;&#22495;&#21517;&#36827;&#34892;&#22403;&#22334;&#37038;&#20214;&#12289;&#32593;&#32476;&#38035;&#40060;&#21644;&#39537;&#21160;&#19979;&#36733;&#31561;&#20114;&#32852;&#32593;&#25915;&#20987;&#65292;&#24378;&#35843;&#20102;&#21019;&#26032;&#26816;&#27979;&#26041;&#27861;&#30340;&#38656;&#27714;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#27880;&#20876;&#36807;&#31243;&#24320;&#22987;&#26102;&#35782;&#21035;&#21487;&#30097;&#22495;&#21517;&#12290;&#38468;&#24102;&#30340;&#25968;&#25454;&#27969;&#31243;&#36890;&#36807;&#27604;&#36739;&#26032;&#22495;&#21517;&#19982;&#27880;&#20876;&#22495;&#21517;&#20135;&#29983;&#20851;&#38190;&#29305;&#24449;&#65292;&#24378;&#35843;&#20102;&#20851;&#38190;&#30456;&#20284;&#24230;&#24471;&#20998;&#12290;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#25216;&#26415;&#30340;&#26032;&#39062;&#32452;&#21512;&#65292;&#21253;&#25324;&#39044;&#35757;&#32451;&#30340;Canine&#27169;&#22411;&#21644;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#20998;&#26512;&#35821;&#20041;&#21644;&#25968;&#20540;&#23646;&#24615;&#65292;&#20026;&#26089;&#26399;&#23041;&#32961;&#26816;&#27979;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#32508;&#21512;&#26041;&#27861;&#26174;&#33879;&#20943;&#23569;&#20102;&#28431;&#27934;&#31383;&#21475;&#65292;&#21152;&#24378;&#20102;&#23545;&#28508;&#22312;&#23041;&#32961;&#30340;&#38450;&#24481;&#12290;&#30740;&#31350;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#32508;&#21512;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#20026;&#24320;&#21457;&#20027;&#21160;&#39044;&#38450;&#24615;&#25805;&#20316;&#30340;&#21162;&#21147;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rising cyber threats, with miscreants registering thousands of new domains daily for Internet-scale attacks like spam, phishing, and drive-by downloads, emphasize the need for innovative detection methods. This paper introduces a cutting-edge approach for identifying suspicious domains at the onset of the registration process. The accompanying data pipeline generates crucial features by comparing new domains to registered domains,emphasizing the crucial similarity score. Leveraging a novel combination of Natural Language Processing (NLP) techniques, including a pretrained Canine model, and Multilayer Perceptron (MLP) models, our system analyzes semantic and numerical attributes, providing a robust solution for early threat detection. This integrated approach significantly reduces the window of vulnerability, fortifying defenses against potential threats. The findings demonstrate the effectiveness of the integrated approach and contribute to the ongoing efforts in developing proactive s
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#23398;&#20064;&#21160;&#24577;&#32593;&#32476;&#20013;&#30340;&#25345;&#20037;&#31038;&#21306;&#32467;&#26500;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#24341;&#20837;&#26368;&#23567;&#21270;&#25299;&#25169;&#21464;&#21270;&#30340;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#22270;&#32858;&#31867;&#26694;&#26550;&#65292;&#24182;&#24341;&#20837;&#20102;&#31070;&#32463;&#32593;&#32476;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#30830;&#20445;&#26102;&#38388;&#19968;&#33268;&#24615;&#21644;&#32858;&#31867;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.03194</link><description>&lt;p&gt;
&#36890;&#36807;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#23398;&#20064;&#21160;&#24577;&#32593;&#32476;&#20013;&#30340;&#25345;&#20037;&#31038;&#21306;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Learning Persistent Community Structures in Dynamic Networks via Topological Data Analysis. (arXiv:2401.03194v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03194
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#23398;&#20064;&#21160;&#24577;&#32593;&#32476;&#20013;&#30340;&#25345;&#20037;&#31038;&#21306;&#32467;&#26500;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#24341;&#20837;&#26368;&#23567;&#21270;&#25299;&#25169;&#21464;&#21270;&#30340;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#22270;&#32858;&#31867;&#26694;&#26550;&#65292;&#24182;&#24341;&#20837;&#20102;&#31070;&#32463;&#32593;&#32476;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#30830;&#20445;&#26102;&#38388;&#19968;&#33268;&#24615;&#21644;&#32858;&#31867;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#31038;&#21306;&#26816;&#27979;&#26041;&#27861;&#36890;&#24120;&#32570;&#20047;&#26377;&#25928;&#30340;&#26426;&#21046;&#26469;&#30830;&#20445;&#26102;&#38388;&#19968;&#33268;&#24615;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#23545;&#32593;&#32476;&#28436;&#21270;&#30340;&#20998;&#26512;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#22270;&#32858;&#31867;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#23567;&#26102;&#38388;&#38388;&#38548;&#20869;&#30340;&#31038;&#21306;&#38388;&#32467;&#26500;&#36827;&#34892;&#25299;&#25169;&#21464;&#21270;&#30340;&#26368;&#23567;&#21270;&#65292;&#26469;&#20445;&#35777;&#26102;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#20026;&#20102;&#35299;&#20915;&#34920;&#31034;&#22604;&#32553;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#22522;&#20110;&#30697;&#38453;&#20998;&#35299;&#30340;&#28145;&#24230;&#22270;&#32858;&#31867;&#31639;&#27861;MFC&#65292;&#20445;&#30041;&#20102;&#33410;&#28857;&#23884;&#20837;&#12290;&#22522;&#20110;&#38745;&#24577;&#32858;&#31867;&#32467;&#26524;&#65292;&#25105;&#20204;&#26500;&#24314;&#27010;&#29575;&#31038;&#21306;&#32593;&#32476;&#65292;&#24182;&#35745;&#31639;&#23427;&#20204;&#30340;&#25345;&#20037;&#21516;&#35843;&#65292;&#36825;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#25299;&#25169;&#24230;&#37327;&#65292;&#29992;&#20110;&#35780;&#20272;&#23427;&#20204;&#20043;&#38388;&#30340;&#32467;&#26500;&#30456;&#20284;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#27491;&#21017;&#21270;&#26041;&#27861;TopoReg&#65292;&#29992;&#20110;&#30830;&#20445;&#31038;&#21306;&#38388;&#32467;&#26500;&#30340;&#25299;&#25169;&#30456;&#20284;&#24615;&#38543;&#26102;&#38388;&#38388;&#38548;&#30340;&#20445;&#25345;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;&#30495;&#23454;&#32593;&#32476;&#19978;&#30340;&#26102;&#38388;&#19968;&#33268;&#24615;&#21644;&#32858;&#31867;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamic community detection methods often lack effective mechanisms to ensure temporal consistency, hindering the analysis of network evolution. In this paper, we propose a novel deep graph clustering framework with temporal consistency regularization on inter-community structures, inspired by the concept of minimal network topological changes within short intervals. Specifically, to address the representation collapse problem, we first introduce MFC, a matrix factorization-based deep graph clustering algorithm that preserves node embedding. Based on static clustering results, we construct probabilistic community networks and compute their persistence homology, a robust topological measure, to assess structural similarity between them. Moreover, a novel neural network regularization TopoReg is introduced to ensure the preservation of topological similarity between inter-community structures over time intervals. Our approach enhances temporal consistency and clustering accuracy on real-
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22810;&#35821;&#35328;&#34917;&#19969;&#31070;&#32463;&#20803;&#26469;&#35299;&#20915;&#22810;&#35821;&#35328;&#27169;&#22411;&#30340;&#36328;&#35821;&#35328;&#30693;&#35782;&#21516;&#27493;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.03190</link><description>&lt;p&gt;
MPN: &#21033;&#29992;&#22810;&#35821;&#35328;&#34917;&#19969;&#31070;&#32463;&#20803;&#36827;&#34892;&#36328;&#35821;&#35328;&#27169;&#22411;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
MPN: Leveraging Multilingual Patch Neuron for Cross-lingual Model Editing. (arXiv:2401.03190v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03190
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22810;&#35821;&#35328;&#34917;&#19969;&#31070;&#32463;&#20803;&#26469;&#35299;&#20915;&#22810;&#35821;&#35328;&#27169;&#22411;&#30340;&#36328;&#35821;&#35328;&#30693;&#35782;&#21516;&#27493;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#32534;&#30721;&#20102;&#22823;&#37327;&#30340;&#20107;&#23454;&#30693;&#35782;&#65292;&#20294;&#30001;&#20110;&#22806;&#37096;&#20449;&#24687;&#30340;&#19981;&#26029;&#21464;&#21270;&#65292;&#23427;&#20204;&#32463;&#24120;&#36807;&#26102;&#12290;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#26159;&#21033;&#29992;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#20197;&#39640;&#25928;&#30340;&#26041;&#24335;&#26356;&#26032;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#22823;&#37096;&#20998;&#29616;&#26377;&#30340;&#27169;&#22411;&#32534;&#36753;&#25216;&#26415;&#23616;&#38480;&#20110;&#21333;&#35821;&#26694;&#26550;&#65292;&#22240;&#27492;&#26080;&#27861;&#35299;&#20915;&#22810;&#35821;&#35328;&#27169;&#22411;&#30340;&#36328;&#35821;&#35328;&#30693;&#35782;&#21516;&#27493;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21363;&#35757;&#32451;&#22810;&#35821;&#35328;&#34917;&#19969;&#31070;&#32463;&#20803;&#26469;&#23384;&#20648;&#36328;&#35821;&#35328;&#30693;&#35782;&#12290;&#23427;&#21487;&#20197;&#36731;&#26494;&#36866;&#24212;&#29616;&#26377;&#26041;&#27861;&#65292;&#22686;&#24378;&#23427;&#20204;&#30340;&#36328;&#35821;&#35328;&#32534;&#36753;&#33021;&#21147;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#20351;&#29992;XNLI&#25968;&#25454;&#38598;&#21644;&#33258;&#24314;&#30340;XFEVER&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#36328;&#35821;&#35328;&#32534;&#36753;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#25913;&#36827;&#30340;&#24615;&#33021;&#65292;&#32780;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models are known for encoding a vast amount of factual knowledge, but they often becomes outdated due to the ever-changing nature of external information. A promising solution to this challenge is the utilization of model editing methods to update the knowledge in an efficient manner. However, the majority of existing model editing techniques are limited to monolingual frameworks, thus failing to address the crucial issue of cross-lingual knowledge synchronization for multilingual models. To tackle this problem, we propose a simple yet effective method that trains multilingual patch neuron to store cross-lingual knowledge. It can be easily adapted to existing approaches to enhance their cross-lingual editing capabilities. To evaluate our method, we conduct experiments using both the XNLI dataset and a self-constructed XFEVER dataset. Experimental results demonstrate that our proposed method achieves improved performance in cross-lingual editing tasks without requiring ex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;Bodo&#35821;&#35789;&#24615;&#26631;&#27880;&#22120;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Bodo&#35821;&#35328;&#27169;&#22411;BodoBERT&#65292;&#36825;&#39033;&#24037;&#20316;&#26159;&#31532;&#19968;&#20010;&#20026;Bodo&#24320;&#21457;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;Bodo&#35789;&#24615;&#26631;&#27880;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#20102;BiLSTM&#12289;CRF&#21644;BodoBERT&#19982;BytePairEmbeddings&#30340;&#32452;&#21512;&#12290;&#23613;&#31649;&#30740;&#31350;&#24050;&#32463;&#22312;&#36164;&#28304;&#20016;&#23500;&#30340;&#35821;&#35328;&#20013;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#35789;&#24615;&#26631;&#27880;&#27169;&#22411;&#30340;&#30740;&#31350;&#65292;&#20294;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#22914;Bodo&#65292;&#20173;&#28982;&#32570;&#20047;&#30456;&#20851;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2401.03175</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;Bodo&#35821;&#35789;&#24615;&#26631;&#27880;&#22120;
&lt;/p&gt;
&lt;p&gt;
Part-of-Speech Tagger for Bodo Language using Deep Learning approach. (arXiv:2401.03175v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03175
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;Bodo&#35821;&#35789;&#24615;&#26631;&#27880;&#22120;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Bodo&#35821;&#35328;&#27169;&#22411;BodoBERT&#65292;&#36825;&#39033;&#24037;&#20316;&#26159;&#31532;&#19968;&#20010;&#20026;Bodo&#24320;&#21457;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;Bodo&#35789;&#24615;&#26631;&#27880;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#20102;BiLSTM&#12289;CRF&#21644;BodoBERT&#19982;BytePairEmbeddings&#30340;&#32452;&#21512;&#12290;&#23613;&#31649;&#30740;&#31350;&#24050;&#32463;&#22312;&#36164;&#28304;&#20016;&#23500;&#30340;&#35821;&#35328;&#20013;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#35789;&#24615;&#26631;&#27880;&#27169;&#22411;&#30340;&#30740;&#31350;&#65292;&#20294;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#22914;Bodo&#65292;&#20173;&#28982;&#32570;&#20047;&#30456;&#20851;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#22914;&#35789;&#24615;&#26631;&#27880;&#12289;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12289;&#26426;&#22120;&#32763;&#35793;&#12289;&#35821;&#38899;&#35782;&#21035;&#21644;&#35821;&#35328;&#24314;&#27169;&#22312;&#36164;&#28304;&#20016;&#23500;&#30340;&#35821;&#35328;&#20013;&#24050;&#32463;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#19968;&#20123;&#20302;&#36164;&#28304;&#35821;&#35328;&#65292;&#22914;Bodo&#12289;Mizo&#12289;Nagamese&#31561;&#65292;&#36825;&#20123;&#31995;&#32479;&#30340;&#30740;&#31350;&#35201;&#20040;&#23578;&#26410;&#24320;&#22987;&#65292;&#35201;&#20040;&#22788;&#20110;&#36215;&#27493;&#38454;&#27573;&#12290;&#35821;&#35328;&#27169;&#22411;&#22312;&#29616;&#20195;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#23545;&#20110;&#36164;&#28304;&#20016;&#23500;&#30340;&#35821;&#35328;&#24050;&#32463;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#20687;Bodo&#12289;Rabha&#21644;Mising&#36825;&#26679;&#30340;&#35821;&#35328;&#20173;&#28982;&#32570;&#20047;&#30456;&#20851;&#30740;&#31350;&#12290;&#26412;&#30740;&#31350;&#39318;&#20808;&#25552;&#20986;&#20102;BodoBERT&#65292;&#19968;&#20010;&#29992;&#20110;Bodo&#35821;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#39033;&#24037;&#20316;&#26159;&#31532;&#19968;&#20010;&#20026;Bodo&#24320;&#21457;&#35821;&#35328;&#27169;&#22411;&#30340;&#21162;&#21147;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#38598;&#25104;&#28145;&#24230;&#23398;&#20064;&#30340;Bodo&#35789;&#24615;&#26631;&#27880;&#27169;&#22411;&#12290;&#35813;&#35789;&#24615;&#26631;&#27880;&#27169;&#22411;&#22522;&#20110;BiLSTM&#21644;CRF&#30340;&#32452;&#21512;&#65292;&#20197;&#21450;BodoBERT&#19982;BytePairEmbeddings&#30340;&#22534;&#21472;&#23884;&#20837;&#12290;&#25105;&#20204;&#28085;&#30422;&#20102;&#22810;&#31181;&#35821;&#35328;...&#65288;&#25688;&#35201;&#34987;&#25130;&#26029;&#65289;
&lt;/p&gt;
&lt;p&gt;
Language Processing systems such as Part-of-speech tagging, Named entity recognition, Machine translation, Speech recognition, and Language modeling (LM) are well-studied in high-resource languages. Nevertheless, research on these systems for several low-resource languages, including Bodo, Mizo, Nagamese, and others, is either yet to commence or is in its nascent stages. Language model plays a vital role in the downstream tasks of modern NLP. Extensive studies are carried out on LMs for high-resource languages. Nevertheless, languages such as Bodo, Rabha, and Mising continue to lack coverage. In this study, we first present BodoBERT, a language model for the Bodo language. To the best of our knowledge, this work is the first such effort to develop a language model for Bodo. Secondly, we present an ensemble DL-based POS tagging model for Bodo. The POS tagging model is based on combinations of BiLSTM with CRF and stacked embedding of BodoBERT with BytePairEmbeddings. We cover several lan
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#38738;&#23569;&#24180;&#20154;&#21475;&#26222;&#26597;&#25968;&#25454;&#26469;&#39044;&#27979;&#25233;&#37057;&#39118;&#38505;&#65292;&#20851;&#27880;&#20799;&#31461;&#23545;&#25233;&#37057;&#30340;&#32463;&#21382;&#21644;&#26085;&#24120;&#29983;&#27963;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2401.03171</link><description>&lt;p&gt;
&#22522;&#20110;&#20154;&#21475;&#26222;&#26597;&#35843;&#26597;&#21644;&#19968;&#33324;&#29983;&#27963;&#38382;&#39064;&#30340;&#38738;&#23569;&#24180;&#25233;&#37057;&#39118;&#38505;&#39044;&#27979;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Exploration of Adolescent Depression Risk Prediction Based on Census Surveys and General Life Issues. (arXiv:2401.03171v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03171
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#38738;&#23569;&#24180;&#20154;&#21475;&#26222;&#26597;&#25968;&#25454;&#26469;&#39044;&#27979;&#25233;&#37057;&#39118;&#38505;&#65292;&#20851;&#27880;&#20799;&#31461;&#23545;&#25233;&#37057;&#30340;&#32463;&#21382;&#21644;&#26085;&#24120;&#29983;&#27963;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20195;&#31038;&#20250;&#65292;&#29983;&#27963;&#21644;&#24037;&#20316;&#30340;&#21387;&#21147;&#19981;&#26029;&#22686;&#21152;&#65292;&#24515;&#29702;&#30142;&#30149;&#24050;&#25104;&#20026;&#29616;&#20195;&#20581;&#24247;&#20851;&#27880;&#30340;&#37325;&#35201;&#38382;&#39064;&#65292;&#32780;COVID-19&#30123;&#24773;&#36827;&#19968;&#27493;&#20984;&#26174;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#38738;&#23569;&#24180;&#25233;&#37057;&#30340;&#24739;&#30149;&#29575;&#27491;&#22312;&#31283;&#27493;&#19978;&#21319;&#65292;&#32780;&#20256;&#32479;&#30340;&#35786;&#26029;&#26041;&#27861;&#65292;&#22914;&#37327;&#34920;&#25110;&#38754;&#35797;&#65292;&#23545;&#20110;&#26816;&#27979;&#38738;&#23569;&#24180;&#25233;&#37057;&#26174;&#24471;&#23588;&#20026;&#19981;&#36275;&#12290;&#38754;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#20986;&#29616;&#20102;&#35768;&#22810;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#26041;&#27861;&#26469;&#36741;&#21161;&#35786;&#26029;&#24515;&#29702;&#20581;&#24247;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#36825;&#20123;&#26041;&#27861;&#37117;&#38598;&#20013;&#22312;&#37327;&#34920;&#23384;&#22312;&#30340;&#22522;&#26412;&#38382;&#39064;&#19978;&#65292;&#25110;&#32773;&#37319;&#29992;&#22810;&#27169;&#24577;&#26041;&#27861;&#65292;&#22914;&#38754;&#37096;&#34920;&#24773;&#35782;&#21035;&#12290;&#26681;&#25454;&#26085;&#24120;&#20064;&#24815;&#21644;&#34892;&#20026;&#26469;&#35786;&#26029;&#25233;&#37057;&#39118;&#38505;&#30340;&#30740;&#31350;&#23616;&#38480;&#20110;&#23567;&#35268;&#27169;&#30340;&#23450;&#24615;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21033;&#29992;&#38738;&#23569;&#24180;&#20154;&#21475;&#26222;&#26597;&#25968;&#25454;&#26469;&#39044;&#27979;&#25233;&#37057;&#39118;&#38505;&#65292;&#37325;&#28857;&#20851;&#27880;&#20799;&#31461;&#23545;&#25233;&#37057;&#30340;&#32463;&#21382;&#21644;&#20182;&#20204;&#30340;&#26085;&#24120;&#29983;&#27963;&#24773;&#20917;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
In contemporary society, the escalating pressures of life and work have propelled psychological disorders to the forefront of modern health concerns, an issue that has been further accentuated by the COVID-19 pandemic. The prevalence of depression among adolescents is steadily increasing, and traditional diagnostic methods, which rely on scales or interviews, prove particularly inadequate for detecting depression in young people. Addressing these challenges, numerous AI-based methods for assisting in the diagnosis of mental health issues have emerged. However, most of these methods center around fundamental issues with scales or use multimodal approaches like facial expression recognition. Diagnosis of depression risk based on everyday habits and behaviors has been limited to small-scale qualitative studies. Our research leverages adolescent census data to predict depression risk, focusing on children's experiences with depression and their daily life situations. We introduced a method
&lt;/p&gt;</description></item><item><title>PosDiffNet&#26159;&#19968;&#31181;&#38024;&#23545;&#22823;&#35270;&#35282;&#25200;&#21160;&#19979;&#30340;&#28857;&#20113;&#37197;&#20934;&#30340;&#27169;&#22411;&#65292;&#20351;&#29992;&#22522;&#20110;Beltrami&#27969;&#21644;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#22270;&#31070;&#32463;PDE&#20197;&#21450;&#20301;&#32622;&#23884;&#20837;&#26469;&#23454;&#29616;&#28857;&#20113;&#30340;&#39640;&#32500;&#29305;&#24449;&#34920;&#31034;&#21644;&#34917;&#19969;&#38388;&#23545;&#40784;&#65292;&#36827;&#32780;&#36827;&#34892;&#37197;&#20934;&#12290;</title><link>http://arxiv.org/abs/2401.03167</link><description>&lt;p&gt;
PosDiffNet:&#20855;&#26377;&#25200;&#21160;&#30340;&#22823;&#35270;&#35282;&#28857;&#20113;&#37197;&#20934;&#30340;&#20301;&#32622;&#31070;&#32463;&#25193;&#25955;
&lt;/p&gt;
&lt;p&gt;
PosDiffNet: Positional Neural Diffusion for Point Cloud Registration in a Large Field of View with Perturbations. (arXiv:2401.03167v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03167
&lt;/p&gt;
&lt;p&gt;
PosDiffNet&#26159;&#19968;&#31181;&#38024;&#23545;&#22823;&#35270;&#35282;&#25200;&#21160;&#19979;&#30340;&#28857;&#20113;&#37197;&#20934;&#30340;&#27169;&#22411;&#65292;&#20351;&#29992;&#22522;&#20110;Beltrami&#27969;&#21644;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#22270;&#31070;&#32463;PDE&#20197;&#21450;&#20301;&#32622;&#23884;&#20837;&#26469;&#23454;&#29616;&#28857;&#20113;&#30340;&#39640;&#32500;&#29305;&#24449;&#34920;&#31034;&#21644;&#34917;&#19969;&#38388;&#23545;&#40784;&#65292;&#36827;&#32780;&#36827;&#34892;&#37197;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28857;&#20113;&#37197;&#20934;&#26159;&#19977;&#32500;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#19968;&#39033;&#20851;&#38190;&#25216;&#26415;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#33539;&#22260;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#20219;&#21153;&#22312;&#20855;&#26377;&#21160;&#24577;&#23545;&#35937;&#12289;&#29615;&#22659;&#22122;&#22768;&#25110;&#20854;&#20182;&#25200;&#21160;&#30340;&#22823;&#35270;&#35282;&#20013;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PosDiffNet&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#31383;&#21475;&#32423;&#12289;&#34917;&#19969;&#32423;&#21644;&#28857;&#32423;&#23545;&#24212;&#36827;&#34892;&#20998;&#23618;&#37197;&#20934;&#12290;&#25105;&#20204;&#21033;&#29992;&#22522;&#20110;Beltrami&#27969;&#30340;&#22270;&#31070;&#32463;&#20559;&#24494;&#20998;&#26041;&#31243;(PDE)&#33719;&#24471;&#28857;&#20113;&#30340;&#39640;&#32500;&#29305;&#24449;&#21644;&#20301;&#32622;&#23884;&#20837;&#12290;&#25105;&#20204;&#23558;&#20301;&#32622;&#23884;&#20837;&#24341;&#20837;&#22522;&#20110;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;(ODE)&#30340;Transformer&#27169;&#22359;&#20013;&#65292;&#20197;&#39640;&#25928;&#34920;&#31034;&#28857;&#20869;&#30340;&#34917;&#19969;&#12290;&#25105;&#20204;&#21033;&#29992;&#39640;&#29305;&#24449;&#30456;&#20284;&#24615;&#20998;&#25968;&#23548;&#20986;&#30340;&#22810;&#32423;&#23545;&#24212;&#26469;&#20419;&#36827;&#28857;&#20113;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;SVD&#30340;&#31639;&#27861;&#31561;&#37197;&#20934;&#26041;&#27861;&#26469;&#39044;&#27979;&#21464;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;
Point cloud registration is a crucial technique in 3D computer vision with a wide range of applications. However, this task can be challenging, particularly in large fields of view with dynamic objects, environmental noise, or other perturbations. To address this challenge, we propose a model called PosDiffNet. Our approach performs hierarchical registration based on window-level, patch-level, and point-level correspondence. We leverage a graph neural partial differential equation (PDE) based on Beltrami flow to obtain high-dimensional features and position embeddings for point clouds. We incorporate position embeddings into a Transformer module based on a neural ordinary differential equation (ODE) to efficiently represent patches within points. We employ the multi-level correspondence derived from the high feature similarity scores to facilitate alignment between point clouds. Subsequently, we use registration methods such as SVD-based algorithms to predict the transformation using c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;&#20154;&#26426;&#21327;&#20316;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20154;&#31867;&#26234;&#33021;&#27880;&#20837;&#21040;AI&#20013;&#23454;&#29616;&#28151;&#21512;&#20132;&#36890;&#32534;&#38431;&#20013;&#30340;&#23433;&#20840;&#39640;&#25928;&#33258;&#21160;&#39550;&#39542;&#12290;&#35813;&#26041;&#27861;&#23558;&#20154;&#31867;&#19987;&#23478;&#20316;&#20026;&#23548;&#24072;&#65292;&#20801;&#35768;&#20195;&#29702;&#22312;&#19981;&#30830;&#23450;&#29615;&#22659;&#20013;&#36827;&#34892;&#25506;&#32034;&#65292;&#21516;&#26102;&#22312;&#21361;&#38505;&#24773;&#20917;&#19979;&#25509;&#31649;&#25511;&#21046;&#20197;&#36991;&#20813;&#20107;&#25925;&#65292;&#24182;&#25351;&#23548;&#20195;&#29702;&#20943;&#23567;&#20132;&#36890;&#27969;&#24178;&#25200;&#65292;&#20248;&#21270;&#20132;&#36890;&#27969;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.03160</link><description>&lt;p&gt;
&#20154;&#20316;&#20026;AI&#23548;&#24072;&#65306;&#22686;&#24378;&#20154;&#26426;&#21327;&#20316;&#24378;&#21270;&#23398;&#20064;&#20197;&#23454;&#29616;&#23433;&#20840;&#39640;&#25928;&#30340;&#33258;&#21160;&#39550;&#39542;
&lt;/p&gt;
&lt;p&gt;
Human as AI Mentor: Enhanced Human-in-the-loop Reinforcement Learning for Safe and Efficient Autonomous Driving. (arXiv:2401.03160v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03160
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;&#20154;&#26426;&#21327;&#20316;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20154;&#31867;&#26234;&#33021;&#27880;&#20837;&#21040;AI&#20013;&#23454;&#29616;&#28151;&#21512;&#20132;&#36890;&#32534;&#38431;&#20013;&#30340;&#23433;&#20840;&#39640;&#25928;&#33258;&#21160;&#39550;&#39542;&#12290;&#35813;&#26041;&#27861;&#23558;&#20154;&#31867;&#19987;&#23478;&#20316;&#20026;&#23548;&#24072;&#65292;&#20801;&#35768;&#20195;&#29702;&#22312;&#19981;&#30830;&#23450;&#29615;&#22659;&#20013;&#36827;&#34892;&#25506;&#32034;&#65292;&#21516;&#26102;&#22312;&#21361;&#38505;&#24773;&#20917;&#19979;&#25509;&#31649;&#25511;&#21046;&#20197;&#36991;&#20813;&#20107;&#25925;&#65292;&#24182;&#25351;&#23548;&#20195;&#29702;&#20943;&#23567;&#20132;&#36890;&#27969;&#24178;&#25200;&#65292;&#20248;&#21270;&#20132;&#36890;&#27969;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;AVs&#65289;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#30830;&#20445;AVs&#30340;&#23433;&#20840;&#24615;&#21644;&#20132;&#36890;&#27969;&#25928;&#29575;&#30340;&#39550;&#39542;&#31574;&#30053;&#30340;&#21457;&#23637;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;&#20154;&#26426;&#21327;&#20316;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;&#22522;&#20110;&#20154;&#20316;&#20026;AI&#23548;&#24072;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;HAIM-DRL&#65289;&#26694;&#26550;&#65292;&#20197;&#22312;&#28151;&#21512;&#20132;&#36890;&#32534;&#38431;&#20013;&#23454;&#29616;&#23433;&#20840;&#39640;&#25928;&#30340;&#33258;&#21160;&#39550;&#39542;&#12290;&#20174;&#20154;&#31867;&#23398;&#20064;&#36807;&#31243;&#20013;&#27762;&#21462;&#28789;&#24863;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#26377;&#25928;&#22320;&#23558;&#20154;&#31867;&#26234;&#33021;&#27880;&#20837;&#21040;AI&#20013;&#65292;&#31216;&#20026;&#20154;&#20316;&#20026;AI&#23548;&#24072;&#65288;HAIM&#65289;&#12290;&#22312;&#36825;&#20010;&#33539;&#24335;&#20013;&#65292;&#20154;&#31867;&#19987;&#23478;&#20316;&#20026;&#23548;&#24072;&#20026;AI&#20195;&#29702;&#25552;&#20379;&#24110;&#21161;&#12290;&#22312;&#20801;&#35768;&#20195;&#29702;&#22312;&#19981;&#30830;&#23450;&#29615;&#22659;&#20013;&#36827;&#34892;&#20805;&#20998;&#25506;&#32034;&#30340;&#21516;&#26102;&#65292;&#20154;&#31867;&#19987;&#23478;&#21487;&#20197;&#22312;&#21361;&#38505;&#24773;&#20917;&#19979;&#25509;&#31649;&#25511;&#21046;&#65292;&#24182;&#23637;&#31034;&#27491;&#30830;&#30340;&#34892;&#21160;&#20197;&#36991;&#20813;&#28508;&#22312;&#20107;&#25925;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#21487;&#20197;&#25351;&#23548;&#20195;&#29702;&#20943;&#23567;&#20132;&#36890;&#27969;&#24178;&#25200;&#65292;&#20174;&#32780;&#20248;&#21270;&#20132;&#36890;&#27969;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite significant progress in autonomous vehicles (AVs), the development of driving policies that ensure both the safety of AVs and traffic flow efficiency has not yet been fully explored. In this paper, we propose an enhanced human-in-the-loop reinforcement learning method, termed the Human as AI mentor-based deep reinforcement learning (HAIM-DRL) framework, which facilitates safe and efficient autonomous driving in mixed traffic platoon. Drawing inspiration from the human learning process, we first introduce an innovative learning paradigm that effectively injects human intelligence into AI, termed Human as AI mentor (HAIM). In this paradigm, the human expert serves as a mentor to the AI agent. While allowing the agent to sufficiently explore uncertain environments, the human expert can take control in dangerous situations and demonstrate correct actions to avoid potential accidents. On the other hand, the agent could be guided to minimize traffic flow disturbance, thereby optimizi
&lt;/p&gt;</description></item><item><title>&#22235;&#27493;&#25512;&#29702;&#65288;QLFR&#65289;&#26694;&#26550;&#26159;&#19968;&#31181;&#36890;&#36807;&#24341;&#20837;&#21477;&#27861;&#21644;&#35821;&#20041;&#20016;&#23500;&#30340;CoT&#26469;&#25552;&#21319;&#30701;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.03158</link><description>&lt;p&gt;
&#22235;&#27493;&#25512;&#29702;&#65288;QLFR&#65289;&#26694;&#26550;&#65306;&#25512;&#36827;&#30701;&#25991;&#26412;&#20998;&#31867;&#30340;&#22235;&#27493;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Quartet Logic: A Four-Step Reasoning (QLFR) framework for advancing Short Text Classification. (arXiv:2401.03158v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03158
&lt;/p&gt;
&lt;p&gt;
&#22235;&#27493;&#25512;&#29702;&#65288;QLFR&#65289;&#26694;&#26550;&#26159;&#19968;&#31181;&#36890;&#36807;&#24341;&#20837;&#21477;&#27861;&#21644;&#35821;&#20041;&#20016;&#23500;&#30340;CoT&#26469;&#25552;&#21319;&#30701;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30701;&#25991;&#26412;&#20998;&#31867;&#65288;STC&#65289;&#23545;&#20110;&#22788;&#29702;&#21644;&#29702;&#35299;&#24403;&#20195;&#25968;&#23383;&#24179;&#21488;&#19978;&#27969;&#34892;&#30340;&#31616;&#27905;&#32780;&#37325;&#35201;&#30340;&#20869;&#23481;&#33267;&#20851;&#37325;&#35201;&#12290;STC&#22312;&#25235;&#20303;&#35821;&#20041;&#21644;&#21477;&#27861;&#22797;&#26434;&#24615;&#26041;&#38754;&#36935;&#21040;&#22256;&#38590;&#65292;&#36825;&#20010;&#38382;&#39064;&#22312;&#20256;&#32479;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#24456;&#26126;&#26174;&#12290;&#23613;&#31649;&#22270;&#21367;&#31215;&#32593;&#32476;&#36890;&#36807;&#25972;&#21512;&#22806;&#37096;&#30693;&#35782;&#24211;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#21463;&#21040;&#24212;&#29992;&#30693;&#35782;&#36136;&#37327;&#21644;&#33539;&#22260;&#30340;&#38480;&#21046;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#30340;&#20986;&#29616;&#26174;&#33879;&#25552;&#39640;&#20102;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#19968;&#20123;&#30740;&#31350;&#25351;&#20986;&#20102;&#23427;&#20204;&#22312;&#22522;&#30784;NLP&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#36816;&#29992;CoT&#26469;&#30740;&#31350;LLMs&#22312;STC&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#22235;&#27493;&#25512;&#29702;&#65288;QLFR&#65289;&#26694;&#26550;&#12290;&#36825;&#20010;&#26694;&#26550;&#20027;&#35201;&#21253;&#25324;&#21477;&#27861;&#21644;&#35821;&#20041;&#20016;&#23500;&#30340;CoT&#65292;&#26377;&#25928;&#21033;&#29992;&#20102;LLMs&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Short Text Classification (STC) is crucial for processing and comprehending the brief but substantial content prevalent on contemporary digital platforms. The STC encounters difficulties in grasping semantic and syntactic intricacies, an issue that is apparent in traditional pre-trained language models. Although Graph Convolutional Networks enhance performance by integrating external knowledge bases, these methods are limited by the quality and extent of the knowledge applied. Recently, the emergence of Large Language Models (LLMs) and Chain-of-Thought (CoT) has significantly improved the performance of complex reasoning tasks. However, some studies have highlighted the limitations of their application in fundamental NLP tasks. Consequently, this study sought to employ CoT to investigate the capabilities of LLMs in STC tasks. This study introduces Quartet Logic: A Four-Step Reasoning (QLFR) framework. This framework primarily incorporates Syntactic and Semantic Enrichment CoT, effectiv
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22788;&#29702;&#22810;&#30446;&#26631;&#36319;&#36394;&#30340;&#20998;&#25955;&#24335;&#22810;&#26234;&#33021;&#20307;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22312;&#26234;&#33021;&#20307;&#25968;&#37327;&#23569;&#20110;&#30446;&#26631;&#25968;&#37327;&#26102;&#23454;&#29616;&#20027;&#21160;&#25628;&#32034;&#21644;&#36319;&#36394;&#65292;&#24182;&#20351;&#29992;&#24322;&#27493;&#26234;&#33021;&#20307;&#36890;&#20449;&#26469;&#21327;&#35843;&#21160;&#20316;&#12290;</title><link>http://arxiv.org/abs/2401.03154</link><description>&lt;p&gt;
&#20998;&#25955;&#24335;&#22810;&#26234;&#33021;&#20307;&#20027;&#21160;&#25628;&#32034;&#21644;&#36319;&#36394;&#24403;&#30446;&#26631;&#36229;&#36807;&#26234;&#33021;&#20307;&#25968;&#37327;&#26102;
&lt;/p&gt;
&lt;p&gt;
Decentralized Multi-Agent Active Search and Tracking when Targets Outnumber Agents. (arXiv:2401.03154v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03154
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22788;&#29702;&#22810;&#30446;&#26631;&#36319;&#36394;&#30340;&#20998;&#25955;&#24335;&#22810;&#26234;&#33021;&#20307;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22312;&#26234;&#33021;&#20307;&#25968;&#37327;&#23569;&#20110;&#30446;&#26631;&#25968;&#37327;&#26102;&#23454;&#29616;&#20027;&#21160;&#25628;&#32034;&#21644;&#36319;&#36394;&#65292;&#24182;&#20351;&#29992;&#24322;&#27493;&#26234;&#33021;&#20307;&#36890;&#20449;&#26469;&#21327;&#35843;&#21160;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#22810;&#30446;&#26631;&#36319;&#36394;&#22312;&#37326;&#29983;&#21160;&#29289;&#24033;&#36923;&#12289;&#23433;&#20840;&#30417;&#25511;&#25110;&#29615;&#22659;&#30417;&#27979;&#31561;&#39046;&#22495;&#26377;&#24191;&#27867;&#24212;&#29992;&#12290;&#29616;&#26377;&#31639;&#27861;&#24120;&#24120;&#20570;&#20986;&#19968;&#20123;&#38480;&#21046;&#24615;&#20551;&#35774;&#65306;&#30446;&#26631;&#25968;&#37327;&#21644;&#21021;&#22987;&#20301;&#32622;&#24050;&#30693;&#65292;&#25110;&#32773;&#26234;&#33021;&#20307;&#24050;&#34987;&#39044;&#20998;&#37197;&#21040;&#30417;&#25511;&#29615;&#22659;&#30340;&#19981;&#37325;&#21472;&#20998;&#21306;&#65292;&#20943;&#36731;&#20102;&#25506;&#32034;&#30340;&#36127;&#25285;&#12290;&#28982;&#32780;&#65292;&#24403;&#26234;&#33021;&#20307;&#25968;&#37327;&#23569;&#20110;&#30446;&#26631;&#25968;&#37327;&#26102;&#65292;&#36825;&#31181;&#20551;&#35774;&#20250;&#38480;&#21046;&#31639;&#27861;&#30340;&#36866;&#29992;&#24615;&#65292;&#22240;&#20026;&#26234;&#33021;&#20307;&#26080;&#27861;&#25345;&#32493;&#36319;&#36394;&#20854;&#35270;&#37326;&#20013;&#30340;&#30446;&#26631;&#12290;&#27492;&#22806;&#65292;&#22810;&#26234;&#33021;&#20307;&#36319;&#36394;&#31639;&#27861;&#36824;&#20551;&#35774;&#26234;&#33021;&#20307;&#38388;&#35266;&#27979;&#30340;&#21516;&#27493;&#65292;&#25110;&#32773;&#38656;&#35201;&#19968;&#20010;&#20013;&#22830;&#25511;&#21046;&#22120;&#26469;&#21327;&#35843;&#32852;&#21512;&#21160;&#20316;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#20851;&#27880;&#20110;&#20998;&#25955;&#24335;&#22810;&#26234;&#33021;&#20307;&#12289;&#22810;&#30446;&#26631;&#12289;&#21516;&#26102;&#20027;&#21160;&#25628;&#32034;&#21644;&#36319;&#36394;&#30340;&#35774;&#32622;&#65292;&#20854;&#20013;&#26234;&#33021;&#20307;&#38388;&#36890;&#20449;&#26159;&#24322;&#27493;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;DecSTER&#20351;&#29992;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#20551;&#35774;&#23494;&#24230;&#28388;&#27874;&#22120;&#30340;&#39034;&#24207;&#33945;&#29305;&#21345;&#27931;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-agent multi-target tracking has a wide range of applications, including wildlife patrolling, security surveillance or environment monitoring. Such algorithms often make restrictive assumptions: the number of targets and/or their initial locations may be assumed known, or agents may be pre-assigned to monitor disjoint partitions of the environment, reducing the burden of exploration. This also limits applicability when there are fewer agents than targets, since agents are unable to continuously follow the targets in their fields of view. Multi-agent tracking algorithms additionally assume inter-agent synchronization of observations, or the presence of a central controller to coordinate joint actions. Instead, we focus on the setting of decentralized multi-agent, multi-target, simultaneous active search-and-tracking with asynchronous inter-agent communication. Our proposed algorithm DecSTER uses a sequential monte carlo implementation of the probability hypothesis density filter fo
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#22320;&#29702;&#21333;&#20803;&#20132;&#36890;&#27969;&#25968;&#25454;&#26469;&#25913;&#36827;&#20132;&#36890;&#35780;&#20272;&#21644;&#39044;&#27979;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#32467;&#21512;&#22810;&#21464;&#37327;&#12289;&#26102;&#38388;&#21644;&#31354;&#38388;&#26041;&#38754;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#26041;&#27861;&#22312;&#38271;&#26399;&#39044;&#27979;&#20013;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#20934;&#30830;&#29575;&#65292;&#24182;&#23637;&#31034;&#20102;&#23558;&#22320;&#29702;&#21333;&#20803;&#20132;&#36890;&#27969;&#25972;&#21512;&#21040;&#20132;&#36890;&#31995;&#32479;&#20013;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.03138</link><description>&lt;p&gt;
TelTrans&#65306;&#36890;&#36807;&#22810;&#26041;&#38754;&#30340;&#22270;&#27169;&#22411;&#23558;&#22810;&#31181;&#31867;&#22411;&#30340;&#30005;&#20449;&#25968;&#25454;&#24212;&#29992;&#20110;&#20132;&#36890;&#35780;&#20272;&#21644;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
TelTrans: Applying Multi-Type Telecom Data to Transportation Evaluation and Prediction via Multifaceted Graph Modeling. (arXiv:2401.03138v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03138
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#22320;&#29702;&#21333;&#20803;&#20132;&#36890;&#27969;&#25968;&#25454;&#26469;&#25913;&#36827;&#20132;&#36890;&#35780;&#20272;&#21644;&#39044;&#27979;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#32467;&#21512;&#22810;&#21464;&#37327;&#12289;&#26102;&#38388;&#21644;&#31354;&#38388;&#26041;&#38754;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#26041;&#27861;&#22312;&#38271;&#26399;&#39044;&#27979;&#20013;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#20934;&#30830;&#29575;&#65292;&#24182;&#23637;&#31034;&#20102;&#23558;&#22320;&#29702;&#21333;&#20803;&#20132;&#36890;&#27969;&#25972;&#21512;&#21040;&#20132;&#36890;&#31995;&#32479;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35299;&#20915;&#22522;&#20110;&#20301;&#32622;&#36793;&#30028;&#26816;&#27979;&#22120;&#30340;&#20132;&#36890;&#39044;&#27979;&#30340;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22320;&#29702;&#21333;&#20803;&#20132;&#36890;&#27969;&#65288;GCT&#65289;&#27969;&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;&#24191;&#27867;&#30340;&#34562;&#31389;&#20132;&#36890;&#35206;&#30422;&#26469;&#25429;&#25417;&#31227;&#21160;&#27169;&#24335;&#30340;&#26032;&#22411;&#25968;&#25454;&#26469;&#28304;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#20998;&#26512;&#39564;&#35777;&#20102;&#23427;&#22312;&#20132;&#36890;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;&#38024;&#23545;&#19982;&#36710;&#36742;&#30456;&#20851;&#30340;GCT&#27969;&#39044;&#27979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#32467;&#21512;&#20102;&#22810;&#21464;&#37327;&#12289;&#26102;&#38388;&#21644;&#31354;&#38388;&#26041;&#38754;&#65292;&#20197;&#25552;&#39640;&#20934;&#30830;&#29575;&#12290;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#38271;&#26399;&#39044;&#27979;&#20013;&#30340;&#20248;&#36234;&#24615;&#65292;&#25105;&#20204;&#36824;&#24378;&#35843;&#20102;&#23558;GCT&#27969;&#25972;&#21512;&#21040;&#20132;&#36890;&#31995;&#32479;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
To address the limitations of traffic prediction from location-bound detectors, we present Geographical Cellular Traffic (GCT) flow, a novel data source that leverages the extensive coverage of cellular traffic to capture mobility patterns. Our extensive analysis validates its potential for transportation. Focusing on vehicle-related GCT flow prediction, we propose a graph neural network that integrates multivariate, temporal, and spatial facets for improved accuracy. Experiments reveal our model's superiority over baselines, especially in long-term predictions. We also highlight the potential for GCT flow integration into transportation systems.
&lt;/p&gt;</description></item><item><title>SPQR&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#23574;&#23792;&#38543;&#26426;&#27169;&#22411;&#26469;&#25511;&#21046;&#24378;&#21270;&#23398;&#20064;&#20013;Q-&#38598;&#21512;&#30340;&#29420;&#31435;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#30340;&#27491;&#21017;&#21270;&#25439;&#22833;&#26469;&#20811;&#26381;&#36807;&#39640;&#20272;&#35745;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2401.03137</link><description>&lt;p&gt;
SPQR:&#20351;&#29992;&#23574;&#23792;&#38543;&#26426;&#27169;&#22411;&#25511;&#21046;Q-&#38598;&#21512;&#30340;&#29420;&#31435;&#24615;&#65292;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
SPQR: Controlling Q-ensemble Independence with Spiked Random Model for Reinforcement Learning. (arXiv:2401.03137v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03137
&lt;/p&gt;
&lt;p&gt;
SPQR&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#23574;&#23792;&#38543;&#26426;&#27169;&#22411;&#26469;&#25511;&#21046;&#24378;&#21270;&#23398;&#20064;&#20013;Q-&#38598;&#21512;&#30340;&#29420;&#31435;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#30340;&#27491;&#21017;&#21270;&#25439;&#22833;&#26469;&#20811;&#26381;&#36807;&#39640;&#20272;&#35745;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32531;&#35299;&#36807;&#39640;&#20272;&#35745;&#20559;&#24046;&#26159;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#26356;&#22797;&#26434;&#20219;&#21153;&#25110;&#21253;&#21547;&#36229;&#20986;&#20998;&#24067;&#25968;&#25454;&#30340;&#31163;&#32447;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#25104;&#21151;&#34920;&#29616;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;&#20026;&#20102;&#20811;&#26381;&#36807;&#39640;&#20272;&#35745;&#20559;&#24046;&#65292;&#30740;&#31350;&#20102;Q-learning&#30340;&#38598;&#25104;&#26041;&#27861;&#26469;&#21033;&#29992;&#22810;&#20010;Q&#20989;&#25968;&#30340;&#22810;&#26679;&#24615;&#12290;&#30001;&#20110;&#32593;&#32476;&#21021;&#22987;&#21270;&#19968;&#30452;&#26159;&#20419;&#36827;Q&#20989;&#25968;&#22810;&#26679;&#24615;&#30340;&#20027;&#35201;&#26041;&#27861;&#65292;&#22240;&#27492;&#22312;&#25991;&#29486;&#20013;&#30740;&#31350;&#20102;&#21551;&#21457;&#24335;&#35774;&#35745;&#30340;&#22810;&#26679;&#24615;&#27880;&#20837;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#24182;&#26410;&#23581;&#35797;&#20174;&#29702;&#35770;&#35282;&#24230;&#20445;&#35777;&#38598;&#25104;&#30340;&#29420;&#31435;&#24615;&#12290;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#30340;Q-&#38598;&#21512;&#29420;&#31435;&#24615;&#30340;&#26032;&#22411;&#27491;&#21017;&#21270;&#25439;&#22833;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#23574;&#23792;Wishart Q-&#38598;&#21512;&#29420;&#31435;&#24615;&#27491;&#21017;&#21270;&#26041;&#27861;&#65288;SPQR&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Alleviating overestimation bias is a critical challenge for deep reinforcement learning to achieve successful performance on more complex tasks or offline datasets containing out-of-distribution data. In order to overcome overestimation bias, ensemble methods for Q-learning have been investigated to exploit the diversity of multiple Q-functions. Since network initialization has been the predominant approach to promote diversity in Q-functions, heuristically designed diversity injection methods have been studied in the literature. However, previous studies have not attempted to approach guaranteed independence over an ensemble from a theoretical perspective. By introducing a novel regularization loss for Q-ensemble independence based on random matrix theory, we propose spiked Wishart Q-ensemble independence regularization (SPQR) for reinforcement learning. Specifically, we modify the intractable hypothesis testing criterion for the Q-ensemble independence into a tractable KL divergence 
&lt;/p&gt;</description></item><item><title>TimeGraphs&#26159;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#26102;&#38388;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21160;&#24577;&#20132;&#20114;&#24314;&#27169;&#20026;&#20998;&#23618;&#26102;&#38388;&#22270;&#65292;&#23454;&#29616;&#20102;&#23545;&#19981;&#21516;&#26102;&#38388;&#23610;&#24230;&#30340;&#33258;&#36866;&#24212;&#25512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2401.03134</link><description>&lt;p&gt;
TimeGraphs: &#22522;&#20110;&#22270;&#30340;&#26102;&#38388;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
TimeGraphs: Graph-based Temporal Reasoning. (arXiv:2401.03134v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03134
&lt;/p&gt;
&lt;p&gt;
TimeGraphs&#26159;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#26102;&#38388;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21160;&#24577;&#20132;&#20114;&#24314;&#27169;&#20026;&#20998;&#23618;&#26102;&#38388;&#22270;&#65292;&#23454;&#29616;&#20102;&#23545;&#19981;&#21516;&#26102;&#38388;&#23610;&#24230;&#30340;&#33258;&#36866;&#24212;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#31995;&#32479;&#23637;&#31034;&#20102;&#26102;&#38388;&#19978;&#30340;&#21160;&#24577;&#34892;&#20026;&#65292;&#36825;&#20123;&#34892;&#20026;&#21487;&#20197;&#34987;&#25429;&#25417;&#20026;&#22797;&#26434;&#20195;&#29702;&#20132;&#20114;&#30340;&#26102;&#38388;&#24207;&#21015;&#12290;&#20026;&#20102;&#36827;&#34892;&#26102;&#38388;&#25512;&#29702;&#65292;&#30446;&#21069;&#30340;&#26041;&#27861;&#20027;&#35201;&#36890;&#36807;&#31616;&#21333;&#30340;&#24207;&#21015;&#27169;&#22411;&#26469;&#32534;&#30721;&#26102;&#38388;&#21160;&#24577;&#12290;&#28982;&#32780;&#65292;&#19968;&#33324;&#26469;&#35828;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#26377;&#25928;&#25429;&#25417;&#36755;&#20837;&#20013;&#20016;&#23500;&#21160;&#24577;&#30340;&#20840;&#35889;&#26102;&#24448;&#24448;&#22833;&#36133;&#65292;&#22240;&#20026;&#21160;&#24577;&#24182;&#19981;&#22343;&#21248;&#20998;&#24067;&#12290;&#29305;&#21035;&#26159;&#65292;&#26377;&#20851;&#20449;&#24687;&#21487;&#33021;&#26356;&#38590;&#25552;&#21462;&#65292;&#24182;&#19988;&#21363;&#20351;&#23427;&#20204;&#19981;&#21253;&#21547;&#37325;&#22823;&#21464;&#21270;&#25110;&#26032;&#20449;&#24687;&#65292;&#20063;&#20250;&#28010;&#36153;&#35745;&#31639;&#33021;&#21147;&#26469;&#22788;&#29702;&#25152;&#26377;&#21333;&#29420;&#30340;&#26102;&#38388;&#27493;&#38271;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TimeGraphs&#65292;&#19968;&#31181;&#23558;&#21160;&#24577;&#20132;&#20114;&#20316;&#20026;&#20998;&#23618;&#26102;&#38388;&#22270;&#26469;&#34920;&#24449;&#30340;&#26032;&#26041;&#27861;&#65292;&#19982;&#20256;&#32479;&#30340;&#39034;&#24207;&#34920;&#31034;&#19981;&#21516;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#32039;&#20945;&#30340;&#22522;&#20110;&#22270;&#30340;&#34920;&#31034;&#26469;&#24314;&#27169;&#20132;&#20114;&#65292;&#23454;&#29616;&#20102;&#23545;&#19981;&#21516;&#26102;&#38388;&#23610;&#24230;&#30340;&#33258;&#36866;&#24212;&#25512;&#29702;&#12290;&#37319;&#29992;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;TimeGraphs&#26500;&#24314;&#20102;&#19968;&#20010;&#22810;&#23618;&#32423;&#20107;&#20214;&#23618;&#27425;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Many real-world systems exhibit temporal, dynamic behaviors, which are captured as time series of complex agent interactions. To perform temporal reasoning, current methods primarily encode temporal dynamics through simple sequence-based models. However, in general these models fail to efficiently capture the full spectrum of rich dynamics in the input, since the dynamics is not uniformly distributed. In particular, relevant information might be harder to extract and computing power is wasted for processing all individual timesteps, even if they contain no significant changes or no new information. Here we propose TimeGraphs, a novel approach that characterizes dynamic interactions as a hierarchical temporal graph, diverging from traditional sequential representations. Our approach models the interactions using a compact graph-based representation, enabling adaptive reasoning across diverse time scales. Adopting a self-supervised method, TimeGraphs constructs a multi-level event hierar
&lt;/p&gt;</description></item><item><title>EdGeo&#24037;&#20855;&#21253;&#21033;&#29992;&#29289;&#29702;&#21407;&#29702;&#25351;&#23548;&#30340;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#39640;&#20445;&#30495;&#24230;&#30340;&#22320;&#19979;&#36895;&#24230;&#22270;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#22768;&#27874;&#26041;&#31243;&#29983;&#25104;&#22320;&#38663;&#27874;&#24418;&#25968;&#25454;&#26469;&#25913;&#21892;&#27169;&#22411;&#20462;&#21098;&#21518;&#30340;ML&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.03131</link><description>&lt;p&gt;
&#29992;&#29289;&#29702;&#25351;&#23548;&#30340;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#21253;&#36827;&#34892;&#22320;&#29699;&#29289;&#29702;&#30417;&#27979;
&lt;/p&gt;
&lt;p&gt;
A Physics-guided Generative AI Toolkit for Geophysical Monitoring. (arXiv:2401.03131v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03131
&lt;/p&gt;
&lt;p&gt;
EdGeo&#24037;&#20855;&#21253;&#21033;&#29992;&#29289;&#29702;&#21407;&#29702;&#25351;&#23548;&#30340;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#39640;&#20445;&#30495;&#24230;&#30340;&#22320;&#19979;&#36895;&#24230;&#22270;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#22768;&#27874;&#26041;&#31243;&#29983;&#25104;&#22320;&#38663;&#27874;&#24418;&#25968;&#25454;&#26469;&#25913;&#21892;&#27169;&#22411;&#20462;&#21098;&#21518;&#30340;ML&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#27874;&#24418;&#21453;&#28436;(FWI)&#22312;&#22320;&#29699;&#31185;&#23398;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#29992;&#20110;&#25506;&#32034;&#22320;&#19979;&#12290;&#23427;&#21033;&#29992;&#22320;&#38663;&#27874;&#26469;&#25104;&#20687;&#22320;&#19979;&#36895;&#24230;&#22270;&#12290;&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;(ML)&#25216;&#26415;&#30340;&#21457;&#23637;&#65292;&#20351;&#29992;ML&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#24050;&#32463;&#20986;&#29616;&#65292;&#29992;&#20110;FWI&#20219;&#21153;&#65292;&#30456;&#27604;&#20256;&#32479;&#22522;&#20110;&#29289;&#29702;&#30340;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#26356;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#22320;&#29699;&#31185;&#23398;&#20013;&#19968;&#20010;&#24120;&#35265;&#30340;&#25361;&#25112;&#26159;&#27809;&#26377;&#25968;&#25454;&#65292;&#20005;&#37325;&#38480;&#21046;&#20102;ML&#30340;&#26377;&#25928;&#24615;&#12290;&#36825;&#20010;&#38382;&#39064;&#22312;&#27169;&#22411;&#20462;&#21098;&#36807;&#31243;&#20013;&#21464;&#24471;&#26356;&#21152;&#20005;&#37325;&#65292;&#36825;&#26159;&#22320;&#29699;&#31185;&#23398;&#20013;&#24517;&#19981;&#21487;&#23569;&#30340;&#19968;&#27493;&#65292;&#22240;&#20026;&#29615;&#22659;&#22797;&#26434;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;EdGeo&#24037;&#20855;&#21253;&#65292;&#23427;&#20351;&#29992;&#29289;&#29702;&#21407;&#29702;&#24341;&#23548;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#29983;&#25104;&#39640;&#20445;&#30495;&#24230;&#30340;&#36895;&#24230;&#22270;&#12290;&#35813;&#24037;&#20855;&#21253;&#20351;&#29992;&#22768;&#27874;&#26041;&#31243;&#26469;&#29983;&#25104;&#30456;&#24212;&#30340;&#22320;&#38663;&#27874;&#24418;&#25968;&#25454;&#65292;&#20415;&#20110;&#20462;&#21098;&#21518;&#30340;ML&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#24773;&#20917;&#19979;&#65292;SSIM&#20998;&#25968;&#37117;&#26377;&#26174;&#33879;&#25552;&#39640;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;MAE&#21644;MSE&#12290;
&lt;/p&gt;
&lt;p&gt;
Full-waveform inversion (FWI) plays a vital role in geoscience to explore the subsurface. It utilizes the seismic wave to image the subsurface velocity map. As the machine learning (ML) technique evolves, the data-driven approaches using ML for FWI tasks have emerged, offering enhanced accuracy and reduced computational cost compared to traditional physics-based methods. However, a common challenge in geoscience, the unprivileged data, severely limits ML effectiveness. The issue becomes even worse during model pruning, a step essential in geoscience due to environmental complexities. To tackle this, we introduce the EdGeo toolkit, which employs a diffusion-based model guided by physics principles to generate high-fidelity velocity maps. The toolkit uses the acoustic wave equation to generate corresponding seismic waveform data, facilitating the fine-tuning of pruned ML models. Our results demonstrate significant improvements in SSIM scores and reduction in both MAE and MSE across vario
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27969;&#24418;&#30340;Shapley&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#39640;&#32500;&#29305;&#24449;&#25237;&#24433;&#21040;&#20302;&#32500;&#27969;&#24418;&#29305;&#24449;&#20013;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;Shapley&#22312;&#39640;&#32500;&#27169;&#22411;&#20197;&#21450;&#22797;&#26434;&#22330;&#26223;&#19979;&#30340;&#35299;&#37322;&#38382;&#39064;&#21644;&#21487;&#35299;&#37322;&#24615;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.03128</link><description>&lt;p&gt;
&#22522;&#20110;&#27969;&#24418;&#30340;Shapley&#22312;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#65288;SAR&#65289;&#35782;&#21035;&#32593;&#32476;&#35299;&#37322;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Manifold-based Shapley for SAR Recognization Network Explanation. (arXiv:2401.03128v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03128
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27969;&#24418;&#30340;Shapley&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#39640;&#32500;&#29305;&#24449;&#25237;&#24433;&#21040;&#20302;&#32500;&#27969;&#24418;&#29305;&#24449;&#20013;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;Shapley&#22312;&#39640;&#32500;&#27169;&#22411;&#20197;&#21450;&#22797;&#26434;&#22330;&#26223;&#19979;&#30340;&#35299;&#37322;&#38382;&#39064;&#21644;&#21487;&#35299;&#37322;&#24615;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#22312;&#22686;&#24378;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#36879;&#26126;&#24230;&#21644;&#21487;&#20449;&#24230;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#29305;&#21035;&#26159;&#22312;&#19968;&#20123;&#39118;&#38505;&#39640;&#21644;&#25104;&#26412;&#39640;&#30340;&#22330;&#26223;&#20013;&#65292;&#22914;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#65288;SAR&#65289;&#12290;Shapley&#26159;&#19968;&#31181;&#20855;&#26377;&#22362;&#23454;&#25968;&#23398;&#22522;&#30784;&#30340;&#22522;&#20110;&#28216;&#25103;&#30340;&#35299;&#37322;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;Shapley&#20551;&#35774;&#27169;&#22411;&#30340;&#29305;&#24449;&#26159;&#29420;&#31435;&#30340;&#65292;&#36825;&#20351;&#24471;&#23545;&#20110;&#39640;&#32500;&#27169;&#22411;&#65292;Shapley&#30340;&#35299;&#37322;&#26080;&#25928;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#27969;&#24418;&#30340;Shapley&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#39640;&#32500;&#29305;&#24449;&#25237;&#24433;&#21040;&#20302;&#32500;&#27969;&#24418;&#29305;&#24449;&#20013;&#65292;&#24182;&#38543;&#21518;&#33719;&#24471;Fusion-Shap&#65292;&#26088;&#22312;&#35299;&#20915;&#20256;&#32479;Shap&#36935;&#21040;&#30340;&#38169;&#35823;&#35299;&#37322;&#38382;&#39064;&#21644;&#22797;&#26434;&#22330;&#26223;&#19979;&#30340;&#21487;&#35299;&#37322;&#24615;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainable artificial intelligence (XAI) holds immense significance in enhancing the deep neural network's transparency and credibility, particularly in some risky and high-cost scenarios, like synthetic aperture radar (SAR). Shapley is a game-based explanation technique with robust mathematical foundations. However, Shapley assumes that model's features are independent, rendering Shapley explanation invalid for high dimensional models. This study introduces a manifold-based Shapley method by projecting high-dimensional features into low-dimensional manifold features and subsequently obtaining Fusion-Shap, which aims at (1) addressing the issue of erroneous explanations encountered by traditional Shap; (2) resolving the challenge of interpretability that traditional Shap faces in complex scenarios.
&lt;/p&gt;</description></item><item><title>&#19968;&#31181;&#35299;&#20915;&#20154;&#24037;&#26234;&#33021;&#40657;&#30418;&#38382;&#39064;&#30340;&#30333;&#30418;&#35299;&#20915;&#26041;&#26696;&#26159;&#20351;&#29992;&#22522;&#20110;&#30456;&#20851;&#39046;&#22495;&#19968;&#33324;&#29702;&#35770;&#30340;&#30830;&#23450;&#24615;&#36923;&#36753;&#32454;&#32990;&#33258;&#21160;&#26426;&#30340;&#35268;&#21017;&#65292;&#35813;&#32454;&#32990;&#33258;&#21160;&#26426;&#23454;&#29616;&#33258;&#21160;&#24182;&#34892;&#36923;&#36753;&#25512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2401.03093</link><description>&lt;p&gt;
&#19968;&#31181;&#35299;&#20915;&#20154;&#24037;&#26234;&#33021;&#40657;&#30418;&#38382;&#39064;&#30340;&#30333;&#30418;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
A white box solution to the black box problem of AI. (arXiv:2401.03093v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03093
&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#35299;&#20915;&#20154;&#24037;&#26234;&#33021;&#40657;&#30418;&#38382;&#39064;&#30340;&#30333;&#30418;&#35299;&#20915;&#26041;&#26696;&#26159;&#20351;&#29992;&#22522;&#20110;&#30456;&#20851;&#39046;&#22495;&#19968;&#33324;&#29702;&#35770;&#30340;&#30830;&#23450;&#24615;&#36923;&#36753;&#32454;&#32990;&#33258;&#21160;&#26426;&#30340;&#35268;&#21017;&#65292;&#35813;&#32454;&#32990;&#33258;&#21160;&#26426;&#23454;&#29616;&#33258;&#21160;&#24182;&#34892;&#36923;&#36753;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20154;&#24037;&#26234;&#33021;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#36879;&#26126;&#24615;&#65292;&#23545;&#20854;&#21487;&#38752;&#24615;&#21644;&#23433;&#20840;&#24615;&#23384;&#22312;&#25285;&#24551;&#12290;&#36825;&#23601;&#26159;&#20154;&#24037;&#26234;&#33021;&#30340;&#40657;&#30418;&#38382;&#39064;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#31526;&#21495; AI &#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#31526;&#21495; AI &#20855;&#26377;&#36879;&#26126;&#30340;&#30333;&#30418;&#24615;&#36136;&#12290;&#31526;&#21495; AI &#30340;&#24191;&#27867;&#24212;&#29992;&#21463;&#21040;&#25968;&#23398;&#27169;&#22411;&#21644;&#33258;&#28982;&#35821;&#35328;&#26415;&#35821;&#30340;&#19981;&#36879;&#26126;&#24615;&#12289;&#32570;&#20047;&#32479;&#19968;&#26412;&#20307;&#35770;&#20197;&#21450;&#25628;&#32034;&#36873;&#39033;&#30340;&#32452;&#21512;&#29190;&#28856;&#30340;&#38459;&#30861;&#12290;&#20026;&#20102;&#35299;&#20915;&#20154;&#24037;&#26234;&#33021;&#30340;&#40657;&#30418;&#38382;&#39064;&#24182;&#23454;&#29616;&#36890;&#29992;&#30340;&#31526;&#21495; AI&#65292;&#25105;&#20204;&#25552;&#35758;&#20351;&#29992;&#22522;&#20110;&#30456;&#20851;&#39046;&#22495;&#19968;&#33324;&#29702;&#35770;&#30340;&#30830;&#23450;&#24615;&#36923;&#36753;&#32454;&#32990;&#33258;&#21160;&#26426;&#30340;&#35268;&#21017;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#30456;&#20851;&#39046;&#22495;&#30340;&#19968;&#33324;&#29702;&#35770;&#36215;&#21040;&#20102;&#32454;&#32990;&#33258;&#21160;&#26426;&#25512;&#29702;&#30340;&#30693;&#35782;&#24211;&#30340;&#20316;&#29992;&#12290;&#32454;&#32990;&#33258;&#21160;&#26426;&#22312;&#22797;&#26434;&#31995;&#32479;&#30340;&#19977;&#20010;&#23618;&#27425;&#19978;&#23454;&#29616;&#33258;&#21160;&#24182;&#34892;&#36923;&#36753;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence based on neural networks has made significant progress. However, there are concerns about the reliability and security of this approach due to its lack of transparency. This is the black box problem of AI. Here we show how this problem can be solved using symbolic AI, which has a transparent white box nature. The widespread use of symbolic AI is hindered by the opacity of mathematical models and natural language terms, the lack of a unified ontology, and the combinatorial explosion of search options. To solve the AI black box problem and to implement general-purpose symbolic AI, we propose to use deterministic logic cellular automata with rules based on first principles of the general theory of the relevant domain. In this case, the general theory of the relevant domain plays the role of a knowledge base for the cellular automaton inference. A cellular automaton implements automatic parallel logical inference at three levels of organization of a complex system. 
&lt;/p&gt;</description></item><item><title>UMIE&#26159;&#19968;&#31181;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#20449;&#24687;&#25552;&#21462;&#22120;&#65292;&#20351;&#29992;&#25351;&#23548;&#35843;&#33410;&#30340;&#26041;&#27861;&#33021;&#22815;&#36328;&#20219;&#21153;&#36827;&#34892;&#20449;&#24687;&#25552;&#21462;&#65292;&#24182;&#19988;&#20855;&#26377;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.03082</link><description>&lt;p&gt;
UMIE&#65306;&#24102;&#26377;&#25351;&#23548;&#35843;&#33410;&#30340;&#32479;&#19968;&#22810;&#27169;&#24577;&#20449;&#24687;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
UMIE: Unified Multimodal Information Extraction with Instruction Tuning. (arXiv:2401.03082v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03082
&lt;/p&gt;
&lt;p&gt;
UMIE&#26159;&#19968;&#31181;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#20449;&#24687;&#25552;&#21462;&#22120;&#65292;&#20351;&#29992;&#25351;&#23548;&#35843;&#33410;&#30340;&#26041;&#27861;&#33021;&#22815;&#36328;&#20219;&#21153;&#36827;&#34892;&#20449;&#24687;&#25552;&#21462;&#65292;&#24182;&#19988;&#20855;&#26377;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22810;&#23186;&#20307;&#20869;&#23481;&#30340;&#26222;&#21450;&#65292;&#22810;&#27169;&#24577;&#20449;&#24687;&#25552;&#21462;&#65288;MIE&#65289;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;MIE&#26041;&#27861;&#36890;&#24120;&#37319;&#29992;&#29305;&#23450;&#20219;&#21153;&#30340;&#27169;&#22411;&#32467;&#26500;&#65292;&#23548;&#33268;&#36328;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#26377;&#38480;&#65292;&#24182;&#19988;&#26410;&#20805;&#20998;&#21033;&#29992;&#20849;&#20139;&#30340;MIE&#20219;&#21153;&#30693;&#35782;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;UMIE&#65292;&#19968;&#31181;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#20449;&#24687;&#25552;&#21462;&#22120;&#65292;&#23558;&#19977;&#20010;MIE&#20219;&#21153;&#32479;&#19968;&#20026;&#19968;&#20010;&#29983;&#25104;&#38382;&#39064;&#65292;&#20351;&#29992;&#25351;&#23548;&#35843;&#33410;&#33021;&#22815;&#26377;&#25928;&#25552;&#21462;&#25991;&#26412;&#21644;&#35270;&#35273;&#25552;&#21450;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#21333;&#19968;UMIE&#22312;&#19977;&#20010;&#20219;&#21153;&#30340;&#20845;&#20010;MIE&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#21508;&#31181;&#26368;&#20808;&#36827;&#65288;SoTA&#65289;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#28145;&#20837;&#20998;&#26512;&#35777;&#26126;UMIE&#22312;&#38646;-shot&#35774;&#32622;&#19979;&#20855;&#26377;&#24456;&#24378;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#23545;&#25351;&#23548;&#21464;&#20307;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#33021;&#25552;&#20379;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26159;&#26397;&#30528;&#32479;&#19968;&#30340;MIE&#27169;&#22411;&#36808;&#20986;&#30340;&#31532;&#19968;&#27493;&#65292;&#21516;&#26102;&#20063;&#24320;&#22987;&#25506;&#32034;&#25351;&#23548;&#35843;&#33410;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;MI&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal information extraction (MIE) gains significant attention as the popularity of multimedia content increases. However, current MIE methods often resort to using task-specific model structures, which results in limited generalizability across tasks and underutilizes shared knowledge across MIE tasks. To address these issues, we propose UMIE, a unified multimodal information extractor to unify three MIE tasks as a generation problem using instruction tuning, being able to effectively extract both textual and visual mentions. Extensive experiments show that our single UMIE outperforms various state-of-the-art (SoTA) methods across six MIE datasets on three tasks. Furthermore, in-depth analysis demonstrates UMIE's strong generalization in the zero-shot setting, robustness to instruction variants, and interpretability. Our research serves as an initial step towards a unified MIE model and initiates the exploration into both instruction tuning and large language models within the MI
&lt;/p&gt;</description></item><item><title>CRUXEval&#26159;&#19968;&#20010;&#21253;&#21547;800&#20010;Python&#20989;&#25968;&#30340;&#20195;&#30721;&#25512;&#29702;&#12289;&#29702;&#35299;&#21644;&#25191;&#34892;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#36890;&#36807;&#35780;&#20272;&#20108;&#21313;&#20010;&#20195;&#30721;&#27169;&#22411;&#65292;&#21457;&#29616;&#35768;&#22810;&#22312;HumanEval&#19978;&#24471;&#20998;&#39640;&#30340;&#27169;&#22411;&#22312;&#35813;&#22522;&#20934;&#27979;&#35797;&#19978;&#27809;&#26377;&#30456;&#21516;&#30340;&#25913;&#36827;&#12290;&#20351;&#29992;CoT&#30340;GPT-4&#23637;&#29616;&#20102;&#26368;&#20339;&#24615;&#33021;&#65292;&#20294;&#20173;&#26410;&#35299;&#20915;&#38382;&#39064;&#12290;&#19982;&#24320;&#28304;&#27169;&#22411;&#30456;&#27604;&#65292;&#38381;&#28304;&#27169;&#22411;&#30340;&#24615;&#33021;&#24046;&#36317;&#26356;&#22823;&#12290;</title><link>http://arxiv.org/abs/2401.03065</link><description>&lt;p&gt;
CRUXEval: &#19968;&#20010;&#20195;&#30721;&#25512;&#29702;&#12289;&#29702;&#35299;&#21644;&#25191;&#34892;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
CRUXEval: A Benchmark for Code Reasoning, Understanding and Execution. (arXiv:2401.03065v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03065
&lt;/p&gt;
&lt;p&gt;
CRUXEval&#26159;&#19968;&#20010;&#21253;&#21547;800&#20010;Python&#20989;&#25968;&#30340;&#20195;&#30721;&#25512;&#29702;&#12289;&#29702;&#35299;&#21644;&#25191;&#34892;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#36890;&#36807;&#35780;&#20272;&#20108;&#21313;&#20010;&#20195;&#30721;&#27169;&#22411;&#65292;&#21457;&#29616;&#35768;&#22810;&#22312;HumanEval&#19978;&#24471;&#20998;&#39640;&#30340;&#27169;&#22411;&#22312;&#35813;&#22522;&#20934;&#27979;&#35797;&#19978;&#27809;&#26377;&#30456;&#21516;&#30340;&#25913;&#36827;&#12290;&#20351;&#29992;CoT&#30340;GPT-4&#23637;&#29616;&#20102;&#26368;&#20339;&#24615;&#33021;&#65292;&#20294;&#20173;&#26410;&#35299;&#20915;&#38382;&#39064;&#12290;&#19982;&#24320;&#28304;&#27169;&#22411;&#30456;&#27604;&#65292;&#38381;&#28304;&#27169;&#22411;&#30340;&#24615;&#33021;&#24046;&#36317;&#26356;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;CRUXEval&#65288;&#20195;&#30721;&#25512;&#29702;&#12289;&#29702;&#35299;&#21644;&#25191;&#34892;&#35780;&#20272;&#65289;&#65292;&#19968;&#20010;&#21253;&#21547;800&#20010;Python&#20989;&#25968;&#65288;3-13&#34892;&#65289;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#27599;&#20010;&#20989;&#25968;&#37117;&#26377;&#19968;&#20010;&#36755;&#20837;&#36755;&#20986;&#23545;&#65292;&#21487;&#20197;&#36827;&#34892;&#20004;&#20010;&#33258;&#28982;&#20219;&#21153;&#65306;&#36755;&#20837;&#39044;&#27979;&#21644;&#36755;&#20986;&#39044;&#27979;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#29983;&#25104;&#25191;&#34892;&#22522;&#20934;&#27979;&#35797;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#26469;&#21019;&#24314;&#26410;&#26469;&#21464;&#31181;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#22312;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#35780;&#20272;&#20102;&#20108;&#21313;&#20010;&#20195;&#30721;&#27169;&#22411;&#65292;&#24182;&#21457;&#29616;&#35768;&#22810;&#26368;&#36817;&#22312;HumanEval&#19978;&#24471;&#20998;&#39640;&#30340;&#27169;&#22411;&#22312;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#19978;&#27809;&#26377;&#21462;&#24471;&#30456;&#21516;&#30340;&#25913;&#36827;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#31616;&#21333;&#30340;CoT&#21644;&#24494;&#35843;&#26041;&#26696;&#21487;&#20197;&#25913;&#21892;&#22312;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#24615;&#33021;&#65292;&#20294;&#20173;&#28982;&#36828;&#26410;&#35299;&#20915;&#38382;&#39064;&#12290;&#26368;&#20339;&#35774;&#32622;&#65292;&#20351;&#29992;CoT&#30340;GPT-4&#65292;&#22312;&#36755;&#20837;&#21644;&#36755;&#20986;&#39044;&#27979;&#19978;&#30340;pass@1&#20998;&#21035;&#36798;&#21040;&#20102;75%&#21644;81%&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;Code Llama 34B&#22312;&#36755;&#20837;&#21644;&#36755;&#20986;&#39044;&#27979;&#19978;&#30340;pass@1&#20998;&#21035;&#20026;50%&#21644;46%&#65292;&#31361;&#26174;&#20102;&#24320;&#28304;&#27169;&#22411;&#21644;&#38381;&#28304;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present CRUXEval (Code Reasoning, Understanding, and eXecution Evaluation), a benchmark consisting of 800 Python functions (3-13 lines). Each function comes with an input-output pair, leading to two natural tasks: input prediction and output prediction. First, we propose a generic recipe for generating our execution benchmark which can be used to create future variation of the benchmark. Second, we evaluate twenty code models on our benchmark and discover that many recent high-scoring models on HumanEval do not show the same improvements on our benchmark. Third, we show that simple CoT and fine-tuning schemes can improve performance on our benchmark but remain far from solving it. The best setup, GPT-4 with chain of thought (CoT), achieves a pass@1 of 75% and 81% on input and output prediction, respectively. In contrast, Code Llama 34B achieves a pass@1 of 50% and 46% on input and output prediction, highlighting the gap between open and closed source models. As no model is close to 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;QoS-aware UE&#25509;&#20837;&#25511;&#21046;&#31574;&#30053;&#65292;&#26088;&#22312;&#22312;&#20851;&#32852;URRLC UE&#19982;&#23567;&#21306;&#20043;&#21069;&#65292;&#20934;&#30830;&#20272;&#35745;QoS&#24182;&#36991;&#20813;&#23567;&#21306;&#36807;&#36733;&#12290;</title><link>http://arxiv.org/abs/2401.03059</link><description>&lt;p&gt;
&#20026;URLLC&#27969;&#37327;&#20248;&#21270;&#21487;&#38752;&#24615;&#30340;&#29992;&#25143;&#25509;&#20837;&#25511;&#21046;&#65306;&#19968;&#31181;&#31070;&#32463;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Reliability-Optimized User Admission Control for URLLC Traffic: A Neural Contextual Bandit Approach. (arXiv:2401.03059v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03059
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;QoS-aware UE&#25509;&#20837;&#25511;&#21046;&#31574;&#30053;&#65292;&#26088;&#22312;&#22312;&#20851;&#32852;URRLC UE&#19982;&#23567;&#21306;&#20043;&#21069;&#65292;&#20934;&#30830;&#20272;&#35745;QoS&#24182;&#36991;&#20813;&#23567;&#21306;&#36807;&#36733;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#21487;&#38752;&#20302;&#24310;&#36831;&#36890;&#20449;&#65288;URLLC&#65289;&#26159;&#19979;&#19968;&#20195;&#26080;&#32447;&#32593;&#32476;&#20013;&#24191;&#27867;&#33539;&#22260;&#30340;&#26032;&#20852;&#26381;&#21153;&#30340;&#22522;&#30707;&#12290;URLLC&#22522;&#26412;&#19978;&#20381;&#36182;&#20110;&#32593;&#32476;&#33021;&#22815;&#20027;&#21160;&#30830;&#23450;&#26159;&#21542;&#26377;&#36275;&#22815;&#30340;&#36164;&#28304;&#26469;&#25903;&#25345;URLLC&#27969;&#37327;&#65292;&#24182;&#22240;&#27492;&#38450;&#27490;&#25152;&#35859;&#30340;&#23567;&#21306;&#36807;&#36733;&#12290;&#28982;&#32780;&#65292;&#20026;URLLC&#29992;&#25143;&#35774;&#22791;&#65288;UE&#65289;&#23454;&#29616;&#20934;&#30830;&#30340;&#26381;&#21153;&#36136;&#37327;&#65288;QoS&#65289;&#39044;&#27979;&#24182;&#38450;&#27490;&#23567;&#21306;&#36229;&#36733;&#26159;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#36825;&#26159;&#30001;&#20110;QoS&#25351;&#26631;&#65288;&#24310;&#36831;&#21644;&#21487;&#38752;&#24615;&#65289;&#20381;&#36182;&#20110;&#27969;&#37327;&#21644;&#20449;&#36947;&#32479;&#35745;&#25968;&#25454;&#12289;&#29992;&#25143;&#30340;&#31227;&#21160;&#24615;&#21644;UE&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;QoS&#24863;&#30693;UE&#25509;&#20837;&#25511;&#21046;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#23558;&#20854;&#19982;&#23567;&#21306;&#20851;&#32852;&#20043;&#21069;&#65292;&#20027;&#21160;&#20272;&#35745;URLLC UE&#30340;QoS&#65292;&#24182;&#30456;&#24212;&#22320;&#20165;&#25509;&#32435;&#19981;&#20250;&#23548;&#33268;&#23567;&#21306;&#36807;&#36733;&#30340;UE&#23376;&#38598;&#12290;&#20026;&#27492;&#65292;&#21046;&#23450;&#20102;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#26469;&#25214;&#21040;&#19968;&#31181;&#26377;&#25928;&#30340;UE&#25509;&#20837;&#25511;&#21046;&#31574;&#30053;&#65292;
&lt;/p&gt;
&lt;p&gt;
Ultra-reliable low-latency communication (URLLC) is the cornerstone for a broad range of emerging services in next-generation wireless networks. URLLC fundamentally relies on the network's ability to proactively determine whether sufficient resources are available to support the URLLC traffic, and thus, prevent so-called cell overloads. Nonetheless, achieving accurate quality-of-service (QoS) predictions for URLLC user equipment (UEs) and preventing cell overloads are very challenging tasks. This is due to dependency of the QoS metrics (latency and reliability) on traffic and channel statistics, users' mobility, and interdependent performance across UEs. In this paper, a new QoS-aware UE admission control approach is developed to proactively estimate QoS for URLLC UEs, prior to associating them with a cell, and accordingly, admit only a subset of UEs that do not lead to a cell overload. To this end, an optimization problem is formulated to find an efficient UE admission control policy,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;AccidentGPT&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#20132;&#36890;&#20107;&#25925;&#20998;&#26512;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#12290;&#23427;&#21033;&#29992;&#22810;&#27169;&#24577;&#36755;&#20837;&#25968;&#25454;&#33258;&#21160;&#37325;&#24314;&#20107;&#25925;&#36807;&#31243;&#35270;&#39057;&#65292;&#24182;&#25552;&#20379;&#22810;&#27169;&#24577;&#36755;&#20986;&#30340;&#22810;&#20219;&#21153;&#20998;&#26512;&#12290;&#27492;&#22806;&#65292;AccidentGPT&#36824;&#37319;&#29992;&#20102;&#22810;&#27169;&#24577;&#25552;&#31034;&#19982;&#21453;&#39304;&#12289;&#28151;&#21512;&#35757;&#32451;&#27169;&#24335;&#21644;&#36793;&#32536;-&#20113;&#20998;&#21106;&#37197;&#32622;&#20197;&#22686;&#24378;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#30740;&#31350;&#26426;&#20250;&#12290;</title><link>http://arxiv.org/abs/2401.03040</link><description>&lt;p&gt;
AccidentGPT:&#29992;&#20110;&#20132;&#36890;&#20107;&#25925;&#20998;&#26512;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
AccidentGPT: Large Multi-Modal Foundation Model for Traffic Accident Analysis. (arXiv:2401.03040v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03040
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;AccidentGPT&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#20132;&#36890;&#20107;&#25925;&#20998;&#26512;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#12290;&#23427;&#21033;&#29992;&#22810;&#27169;&#24577;&#36755;&#20837;&#25968;&#25454;&#33258;&#21160;&#37325;&#24314;&#20107;&#25925;&#36807;&#31243;&#35270;&#39057;&#65292;&#24182;&#25552;&#20379;&#22810;&#27169;&#24577;&#36755;&#20986;&#30340;&#22810;&#20219;&#21153;&#20998;&#26512;&#12290;&#27492;&#22806;&#65292;AccidentGPT&#36824;&#37319;&#29992;&#20102;&#22810;&#27169;&#24577;&#25552;&#31034;&#19982;&#21453;&#39304;&#12289;&#28151;&#21512;&#35757;&#32451;&#27169;&#24335;&#21644;&#36793;&#32536;-&#20113;&#20998;&#21106;&#37197;&#32622;&#20197;&#22686;&#24378;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#30740;&#31350;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#20107;&#25925;&#20998;&#26512;&#23545;&#20110;&#25552;&#39640;&#20844;&#20849;&#23433;&#20840;&#21644;&#21046;&#23450;&#36947;&#36335;&#35268;&#31456;&#21046;&#24230;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#26041;&#27861;&#34429;&#28982;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#24120;&#24120;&#21463;&#38480;&#20110;&#25163;&#21160;&#20998;&#26512;&#36807;&#31243;&#12289;&#20027;&#35266;&#20915;&#31574;&#12289;&#21333;&#27169;&#24577;&#36755;&#20986;&#20197;&#21450;&#19982;&#25935;&#24863;&#25968;&#25454;&#30456;&#20851;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;AccidentGPT&#30340;&#27010;&#24565;&#65292;&#36825;&#26159;&#19968;&#20010;&#20132;&#36890;&#20107;&#25925;&#20998;&#26512;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#23427;&#23558;&#22810;&#27169;&#24577;&#36755;&#20837;&#25968;&#25454;&#29992;&#20110;&#33258;&#21160;&#37325;&#24314;&#20107;&#25925;&#36807;&#31243;&#35270;&#39057;&#24182;&#25552;&#20379;&#22810;&#27169;&#24577;&#36755;&#20986;&#30340;&#22810;&#20219;&#21153;&#20998;&#26512;&#12290;AccidentGPT&#30340;&#35774;&#35745;&#37319;&#29992;&#20102;&#22810;&#27169;&#24577;&#25552;&#31034;&#19982;&#21453;&#39304;&#65292;&#29992;&#20110;&#20219;&#21153;&#23548;&#21521;&#30340;&#36866;&#24212;&#24615;&#65292;&#28151;&#21512;&#35757;&#32451;&#27169;&#24335;&#20197;&#21033;&#29992;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#25968;&#25454;&#65292;&#20197;&#21450;&#36793;&#32536;-&#20113;&#20998;&#21106;&#37197;&#32622;&#20197;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;&#20026;&#20102;&#20805;&#20998;&#21457;&#25381;&#35813;&#27169;&#22411;&#30340;&#21151;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#30740;&#31350;&#26426;&#20250;&#12290;&#26412;&#25991;&#23558;&#22635;&#34917;&#20256;&#32479;&#26041;&#27861;&#20013;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic accident analysis is pivotal for enhancing public safety and developing road regulations. Traditional approaches, although widely used, are often constrained by manual analysis processes, subjective decisions, uni-modal outputs, as well as privacy issues related to sensitive data. This paper introduces the idea of AccidentGPT, a foundation model of traffic accident analysis, which incorporates multi-modal input data to automatically reconstruct the accident process video with dynamics details, and furthermore provide multi-task analysis with multi-modal outputs. The design of the AccidentGPT is empowered with a multi-modality prompt with feedback for task-oriented adaptability, a hybrid training schema to leverage labelled and unlabelled data, and a edge-cloud split configuration for data privacy. To fully realize the functionalities of this model, we proposes several research opportunities. This paper serves as the stepping stone to fill the gaps in traditional approaches of t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20379;&#20102;&#23545;&#36825;&#20123;&#27169;&#22411;&#30340;&#20840;&#38754;&#32972;&#26223;&#20449;&#24687;&#21644;&#35814;&#32454;&#35828;&#26126;&#65292;&#21516;&#26102;&#20063;&#23545;&#23427;&#20204;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#21644;&#24444;&#27492;&#20043;&#38388;&#30340;&#27604;&#36739;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#20854;&#36129;&#29486;&#21253;&#25324;&#23545;&#25193;&#25955;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#24212;&#29992;&#30340;&#24443;&#24213;&#25506;&#32034;&#21644;&#25353;&#26102;&#38388;&#39034;&#24207;&#25490;&#24207;&#30340;&#27169;&#22411;&#27010;&#36848;&#12290;&#36825;&#26159;&#19968;&#20221;&#23545;&#20154;&#24037;&#26234;&#33021;&#21644;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#26469;&#35828;&#20855;&#26377;&#20215;&#20540;&#30340;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2401.03006</link><description>&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#25193;&#25955;&#27169;&#22411;&#30340;&#20852;&#36215;
&lt;/p&gt;
&lt;p&gt;
The Rise of Diffusion Models in Time-Series Forecasting. (arXiv:2401.03006v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03006
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20379;&#20102;&#23545;&#36825;&#20123;&#27169;&#22411;&#30340;&#20840;&#38754;&#32972;&#26223;&#20449;&#24687;&#21644;&#35814;&#32454;&#35828;&#26126;&#65292;&#21516;&#26102;&#20063;&#23545;&#23427;&#20204;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#21644;&#24444;&#27492;&#20043;&#38388;&#30340;&#27604;&#36739;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#20854;&#36129;&#29486;&#21253;&#25324;&#23545;&#25193;&#25955;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#24212;&#29992;&#30340;&#24443;&#24213;&#25506;&#32034;&#21644;&#25353;&#26102;&#38388;&#39034;&#24207;&#25490;&#24207;&#30340;&#27169;&#22411;&#27010;&#36848;&#12290;&#36825;&#26159;&#19968;&#20221;&#23545;&#20154;&#24037;&#26234;&#33021;&#21644;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#26469;&#35828;&#20855;&#26377;&#20215;&#20540;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#25506;&#35752;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#12290;&#25193;&#25955;&#27169;&#22411;&#22312;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#30340;&#21508;&#20010;&#39046;&#22495;&#20013;&#23637;&#31034;&#20986;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#26412;&#25991;&#21253;&#25324;&#23545;&#25193;&#25955;&#27169;&#22411;&#30340;&#20840;&#38754;&#32972;&#26223;&#20449;&#24687;&#65292;&#35814;&#32454;&#20171;&#32461;&#20854;&#26465;&#20214;&#26041;&#27861;&#65292;&#24182;&#23457;&#26597;&#20102;&#20854;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#12290;&#20998;&#26512;&#28085;&#30422;&#20102;11&#20010;&#20855;&#20307;&#30340;&#26102;&#38388;&#24207;&#21015;&#23454;&#29616;&#65292;&#23427;&#20204;&#30340;&#30452;&#35273;&#21644;&#29702;&#35770;&#22522;&#30784;&#65292;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#20197;&#21450;&#24444;&#27492;&#20043;&#38388;&#30340;&#27604;&#36739;&#12290;&#35813;&#24037;&#20316;&#30340;&#20851;&#38190;&#36129;&#29486;&#26159;&#23545;&#25193;&#25955;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#24212;&#29992;&#30340;&#24443;&#24213;&#25506;&#32034;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#25353;&#26102;&#38388;&#39034;&#24207;&#25490;&#24207;&#30340;&#27169;&#22411;&#27010;&#36848;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#23545;&#35813;&#39046;&#22495;&#30340;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#36827;&#34892;&#20102;&#28145;&#20837;&#35752;&#35770;&#65292;&#24182;&#27010;&#36848;&#20102;&#28508;&#22312;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;&#36825;&#23545;&#20110;&#20154;&#24037;&#26234;&#33021;&#21644;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#26469;&#35828;&#26159;&#19968;&#20221;&#23453;&#36149;&#30340;&#36164;&#28304;&#65292;&#25552;&#20379;&#20102;&#23545;&#26368;&#26032;&#36827;&#23637;&#21644;&#26410;&#26469;&#21457;&#23637;&#30340;&#28165;&#26224;&#35270;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
This survey delves into the application of diffusion models in time-series forecasting. Diffusion models are demonstrating state-of-the-art results in various fields of generative AI. The paper includes comprehensive background information on diffusion models, detailing their conditioning methods and reviewing their use in time-series forecasting. The analysis covers 11 specific time-series implementations, the intuition and theory behind them, the effectiveness on different datasets, and a comparison among each other. Key contributions of this work are the thorough exploration of diffusion models' applications in time-series forecasting and a chronologically ordered overview of these models. Additionally, the paper offers an insightful discussion on the current state-of-the-art in this domain and outlines potential future research directions. This serves as a valuable resource for researchers in AI and time-series analysis, offering a clear view of the latest advancements and future p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#22810;&#27169;&#24577;&#24773;&#24863;&#35782;&#21035;&#36716;&#21270;&#20026;&#26356;&#23454;&#29992;&#19988;&#36164;&#28304;&#39640;&#25928;&#30340;&#21333;&#27169;&#24577;&#12289;&#20165;&#35821;&#38899;&#30340;&#24773;&#24863;&#35782;&#21035;&#12290;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#21644;&#23631;&#34109;&#35757;&#32451;&#25216;&#26415;&#26469;&#35299;&#20915;&#29616;&#26377;&#27169;&#22411;&#25152;&#20381;&#36182;&#30340;&#22810;&#27169;&#24577;&#36755;&#20837;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21487;&#33021;&#19981;&#21487;&#34892;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.03000</link><description>&lt;p&gt;
&#36328;&#27169;&#24577;&#36830;&#25509;&#65306;&#30693;&#35782;&#33976;&#39311;&#21644;&#23631;&#34109;&#35757;&#32451;&#29992;&#20110;&#23558;&#22810;&#27169;&#24577;&#24773;&#24863;&#35782;&#21035;&#36716;&#21270;&#20026;&#21333;&#27169;&#24577;&#12289;&#20165;&#35821;&#38899;&#30340;&#24773;&#24863;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Bridging Modalities: Knowledge Distillation and Masked Training for Translating Multi-Modal Emotion Recognition to Uni-Modal, Speech-Only Emotion Recognition. (arXiv:2401.03000v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03000
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#22810;&#27169;&#24577;&#24773;&#24863;&#35782;&#21035;&#36716;&#21270;&#20026;&#26356;&#23454;&#29992;&#19988;&#36164;&#28304;&#39640;&#25928;&#30340;&#21333;&#27169;&#24577;&#12289;&#20165;&#35821;&#38899;&#30340;&#24773;&#24863;&#35782;&#21035;&#12290;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#21644;&#23631;&#34109;&#35757;&#32451;&#25216;&#26415;&#26469;&#35299;&#20915;&#29616;&#26377;&#27169;&#22411;&#25152;&#20381;&#36182;&#30340;&#22810;&#27169;&#24577;&#36755;&#20837;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21487;&#33021;&#19981;&#21487;&#34892;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#23558;&#22810;&#27169;&#24577;&#24773;&#24863;&#35782;&#21035;&#27169;&#22411;&#36716;&#21270;&#20026;&#26356;&#23454;&#29992;&#19988;&#36164;&#28304;&#39640;&#25928;&#30340;&#21333;&#27169;&#24577;&#24773;&#24863;&#35782;&#21035;&#27169;&#22411;&#30340;&#25361;&#25112;&#65292;&#20855;&#20307;&#20851;&#27880;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#12290;&#20174;&#35821;&#38899;&#20449;&#21495;&#20013;&#35782;&#21035;&#24773;&#24863;&#26159;&#19968;&#39033;&#20851;&#38190;&#20219;&#21153;&#65292;&#24212;&#29992;&#20110;&#20154;&#26426;&#20132;&#20114;&#12289;&#24773;&#24863;&#35745;&#31639;&#21644;&#24515;&#29702;&#20581;&#24247;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#36890;&#24120;&#20381;&#36182;&#20110;&#22810;&#27169;&#24577;&#36755;&#20837;&#65292;&#21253;&#25324;&#26469;&#33258;&#22810;&#20010;&#26469;&#28304;&#30340;&#20449;&#24687;&#65292;&#22914;&#38754;&#37096;&#34920;&#24773;&#21644;&#25163;&#21183;&#65292;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#21487;&#33021;&#19981;&#26131;&#33719;&#24471;&#25110;&#19981;&#21487;&#34892;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#30693;&#35782;&#33976;&#39311;&#21644;&#23631;&#34109;&#35757;&#32451;&#25216;&#26415;&#30340;&#26032;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents an innovative approach to address the challenges of translating multi-modal emotion recognition models to a more practical and resource-efficient uni-modal counterpart, specifically focusing on speech-only emotion recognition. Recognizing emotions from speech signals is a critical task with applications in human-computer interaction, affective computing, and mental health assessment. However, existing state-of-the-art models often rely on multi-modal inputs, incorporating information from multiple sources such as facial expressions and gestures, which may not be readily available or feasible in real-world scenarios. To tackle this issue, we propose a novel framework that leverages knowledge distillation and masked training techniques.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#20219;&#21153;&#20998;&#35299;&#65292;&#25552;&#20986;&#20102;Blar-SQL&#26694;&#26550;&#65292;&#23558;&#20004;&#20010;&#19981;&#21516;&#30340;&#27169;&#22411;&#32452;&#21512;&#65292;&#20998;&#21035;&#19987;&#27880;&#20110;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#20174;&#32780;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;NL2SQL&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#12290;&#35813;&#27169;&#22411;&#27604;GPT-4&#26356;&#23567;&#12289;&#26356;&#24555;&#12289;&#26356;&#20415;&#23452;&#12290;</title><link>http://arxiv.org/abs/2401.02997</link><description>&lt;p&gt;
Blar-SQL: &#26356;&#24555;&#12289;&#26356;&#24378;&#12289;&#26356;&#23567;&#30340;NL2SQL
&lt;/p&gt;
&lt;p&gt;
Blar-SQL: Faster, Stronger, Smaller NL2SQL. (arXiv:2401.02997v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02997
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#20219;&#21153;&#20998;&#35299;&#65292;&#25552;&#20986;&#20102;Blar-SQL&#26694;&#26550;&#65292;&#23558;&#20004;&#20010;&#19981;&#21516;&#30340;&#27169;&#22411;&#32452;&#21512;&#65292;&#20998;&#21035;&#19987;&#27880;&#20110;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#20174;&#32780;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;NL2SQL&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#12290;&#35813;&#27169;&#22411;&#27604;GPT-4&#26356;&#23567;&#12289;&#26356;&#24555;&#12289;&#26356;&#20415;&#23452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#21040;SQL&#20219;&#21153;&#65288;NL2SQL&#65289;&#39046;&#22495;&#21462;&#24471;&#20102;&#21487;&#35266;&#30340;&#22768;&#35465;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#20219;&#21153;&#20998;&#35299;&#26469;&#26497;&#22823;&#22320;&#25913;&#21892;LLMs&#22312;&#25968;&#25454;&#24211;&#29702;&#35299;&#21644;&#26597;&#35810;&#29983;&#25104;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#20197;&#20415;&#20351;&#29992;&#19968;&#20010;SQL&#26597;&#35810;&#26469;&#22238;&#31572;&#20154;&#31867;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#32452;&#21512;&#20004;&#20010;&#19981;&#21516;&#27169;&#22411;&#65292;&#20998;&#21035;&#19987;&#27880;&#20110;&#20004;&#20010;&#20219;&#21153;&#65292;&#23545;&#24320;&#28304;&#27169;&#22411;&#65288;&#29305;&#21035;&#26159;Llama-2&#21644;Code Llama&#65289;&#36827;&#34892;&#20102;&#31934;&#35843;&#65292;&#20197;&#21033;&#29992;&#27599;&#20010;&#27169;&#22411;&#30340;&#26680;&#24515;&#31454;&#20105;&#21147;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#26368;&#32456;SQL&#26597;&#35810;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#26469;&#23558;&#27169;&#24335;&#21010;&#20998;&#20026;&#22359;&#65292;&#20197;&#23558;&#26356;&#22810;&#20449;&#24687;&#36866;&#24212;&#26377;&#38480;&#30340;&#19978;&#19979;&#25991;&#20013;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#19982;GPT-4&#33719;&#24471;&#30340;&#32467;&#26524;&#30456;&#24403;&#65292;&#21516;&#26102;&#27604;GPT-4&#26356;&#23567;135&#20493;&#12289;&#26356;&#24555;90&#20493;&#65292;&#24182;&#19988;&#27604;GPT-4&#20415;&#23452;100&#20493;&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have gained considerable notoriety in the field of natural language to SQL tasks (NL2SQL). In this study, we show how task decomposition can greatly benefit LLMs in database understanding and query generation in order to answer human questions with an SQL query.  We fined-tuned open source models, specifically Llama-2 and Code Llama, by combining 2 different models each designated to focus on one of two tasks in order to leverage each model's core competency to further increase the accuracy of the final SQL query.  We propose a new framework to divide the schema into chunks in order to fit more information into a limited context. Our results are comparable with those obtained by GPT-4 at the same time being 135 times smaller, 90 times faster and more than 100 times cheaper than GPT-4.
&lt;/p&gt;</description></item><item><title>CANAMRF &#26159;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#22810;&#27169;&#24577;&#25233;&#37057;&#26816;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#32771;&#34385;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#65292;&#26377;&#25928;&#22320;&#36827;&#34892;&#22810;&#27169;&#24577;&#34920;&#31034;&#65292;&#24182;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.02995</link><description>&lt;p&gt;
CANAMRF&#65306;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#22810;&#27169;&#24577;&#25233;&#37057;&#26816;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CANAMRF: An Attention-Based Model for Multimodal Depression Detection. (arXiv:2401.02995v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02995
&lt;/p&gt;
&lt;p&gt;
CANAMRF &#26159;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#22810;&#27169;&#24577;&#25233;&#37057;&#26816;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#32771;&#34385;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#65292;&#26377;&#25928;&#22320;&#36827;&#34892;&#22810;&#27169;&#24577;&#34920;&#31034;&#65292;&#24182;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#25233;&#37057;&#26816;&#27979;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#35838;&#39064;&#65292;&#26088;&#22312;&#21033;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#39044;&#27979;&#20154;&#31867;&#30340;&#24515;&#29702;&#29366;&#24577;&#12290;&#20043;&#21069;&#30340;&#26041;&#27861;&#24179;&#31561;&#23545;&#24453;&#19981;&#21516;&#30340;&#27169;&#24577;&#65292;&#24182;&#36890;&#36807;&#31616;&#21333;&#30340;&#25968;&#23398;&#36816;&#31639;&#34701;&#21512;&#27599;&#20010;&#27169;&#24577;&#65292;&#27809;&#26377;&#23545;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#36827;&#34892;&#34913;&#37327;&#65292;&#36825;&#19981;&#33021;&#33719;&#24471;&#36866;&#29992;&#20110;&#19979;&#28216;&#25233;&#37057;&#20219;&#21153;&#30340;&#33391;&#22909;&#22810;&#27169;&#24577;&#34920;&#31034;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#33258;&#36866;&#24212;&#22810;&#27169;&#24577;&#24490;&#29615;&#34701;&#21512;&#30340;&#36328;&#27169;&#24577;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;CANAMRF&#65289;&#29992;&#20110;&#22810;&#27169;&#24577;&#25233;&#37057;&#26816;&#27979;&#12290;CANAMRF&#30001;&#22810;&#27169;&#24577;&#29305;&#24449;&#25552;&#21462;&#22120;&#12289;&#33258;&#36866;&#24212;&#22810;&#27169;&#24577;&#24490;&#29615;&#34701;&#21512;&#27169;&#22359;&#21644;&#28151;&#21512;&#27880;&#24847;&#21147;&#27169;&#22359;&#26500;&#25104;&#12290;&#36890;&#36807;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;CANAMRF&#23637;&#31034;&#20986;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#31361;&#26174;&#20102;&#25105;&#20204;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal depression detection is an important research topic that aims to predict human mental states using multimodal data. Previous methods treat different modalities equally and fuse each modality by na\"ive mathematical operations without measuring the relative importance between them, which cannot obtain well-performed multimodal representations for downstream depression tasks. In order to tackle the aforementioned concern, we present a Cross-modal Attention Network with Adaptive Multi-modal Recurrent Fusion (CANAMRF) for multimodal depression detection. CANAMRF is constructed by a multimodal feature extractor, an Adaptive Multimodal Recurrent Fusion module, and a Hybrid Attention Module. Through experimentation on two benchmark datasets, CANAMRF demonstrates state-of-the-art performance, underscoring the effectiveness of our proposed approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#28151;&#21512;&#8221;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32452;&#21512;&#22810;&#20010;&#36866;&#24230;&#35268;&#27169;&#30340;&#32842;&#22825;AI&#27169;&#22411;&#65292;&#21487;&#20197;&#36798;&#21040;&#25110;&#36229;&#36234;&#27604;&#23427;&#20204;&#26356;&#22823;&#30340;&#27169;&#22411;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2401.02994</link><description>&lt;p&gt;
&#22522;&#20110;&#28151;&#21512;&#26041;&#27861;&#30340;&#32842;&#22825;AI&#27169;&#22411;&#65306;&#30456;&#23545;&#20110;&#19975;&#20159;&#32423;&#21442;&#25968;&#27169;&#22411;&#30340;&#26356;&#24265;&#20215;&#12289;&#26356;&#22909;&#30340;&#26367;&#20195;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Blending Is All You Need: Cheaper, Better Alternative to Trillion-Parameters LLM. (arXiv:2401.02994v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02994
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#28151;&#21512;&#8221;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32452;&#21512;&#22810;&#20010;&#36866;&#24230;&#35268;&#27169;&#30340;&#32842;&#22825;AI&#27169;&#22411;&#65292;&#21487;&#20197;&#36798;&#21040;&#25110;&#36229;&#36234;&#27604;&#23427;&#20204;&#26356;&#22823;&#30340;&#27169;&#22411;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20250;&#35805;&#22411;AI&#30740;&#31350;&#20013;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#27169;&#22411;&#37319;&#29992;&#20102;&#26356;&#22810;&#30340;&#21442;&#25968;&#65292;&#22914;ChatGPT&#31561;&#27169;&#22411;&#12290;&#34429;&#28982;&#36825;&#20123;&#24222;&#22823;&#30340;&#27169;&#22411;&#24448;&#24448;&#33021;&#29983;&#25104;&#26356;&#22909;&#30340;&#32842;&#22825;&#22238;&#22797;&#65292;&#20294;&#23427;&#20204;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#21644;&#20869;&#23384;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65306;&#33021;&#21542;&#36890;&#36807;&#32452;&#21512;&#36739;&#23567;&#30340;&#27169;&#22411;&#26469;&#36798;&#21040;&#19982;&#21333;&#20010;&#22823;&#27169;&#22411;&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#65311;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#28151;&#21512;&#8221;&#30340;&#26041;&#27861;&#65292;&#23427;&#26159;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#23558;&#22810;&#20010;&#32842;&#22825;AI&#38598;&#25104;&#22312;&#19968;&#36215;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#35777;&#25454;&#34920;&#26126;&#65292;&#24403;&#29305;&#23450;&#36739;&#23567;&#30340;&#27169;&#22411;&#21327;&#21516;&#28151;&#21512;&#26102;&#65292;&#23427;&#20204;&#21487;&#20197;&#28508;&#22312;&#22320;&#36229;&#36234;&#25110;&#21305;&#25932;&#22823;&#22411;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20363;&#22914;&#65292;&#20165;&#38598;&#25104;&#19977;&#20010;&#36866;&#24230;&#35268;&#27169;&#30340;&#27169;&#22411;&#65288;6B/13B&#21442;&#25968;&#65289;&#23601;&#21487;&#20197;&#36798;&#21040;&#25110;&#29978;&#33267;&#36229;&#36234;ChatGPT&#65288;175B+&#21442;&#25968;&#65289;&#31561;&#22823;&#22411;&#27169;&#22411;&#30340;&#24615;&#33021;&#25351;&#26631;&#12290;&#36825;&#20010;&#20551;&#35774;&#32463;&#36807;&#20102;&#20005;&#26684;&#30340;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
In conversational AI research, there's a noticeable trend towards developing models with a larger number of parameters, exemplified by models like ChatGPT. While these expansive models tend to generate increasingly better chat responses, they demand significant computational resources and memory. This study explores a pertinent question: Can a combination of smaller models collaboratively achieve comparable or enhanced performance relative to a singular large model? We introduce an approach termed "blending", a straightforward yet effective method of integrating multiple chat AIs. Our empirical evidence suggests that when specific smaller models are synergistically blended, they can potentially outperform or match the capabilities of much larger counterparts. For instance, integrating just three models of moderate size (6B/13B paramaeters) can rival or even surpass the performance metrics of a substantially larger model like ChatGPT (175B+ paramaters). This hypothesis is rigorously tes
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#39640;&#25928;&#30340;&#26816;&#32034;&#34920;&#31034;&#34701;&#21512;&#26041;&#27861;ReFusion&#65292;&#36890;&#36807;&#23558;&#26816;&#32034;&#34920;&#31034;&#30452;&#25509;&#34701;&#21512;&#21040;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#35299;&#20915;&#20102;&#23558;&#22806;&#37096;&#25968;&#25454;&#24211;&#30340;&#30693;&#35782;&#34701;&#20837;&#38750;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.02993</link><description>&lt;p&gt;
&#20351;&#29992;&#35745;&#31639;&#39640;&#25928;&#30340;&#26816;&#32034;&#34920;&#31034;&#34701;&#21512;&#25552;&#39640;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improving Natural Language Understanding with Computation-Efficient Retrieval Representation Fusion. (arXiv:2401.02993v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02993
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#39640;&#25928;&#30340;&#26816;&#32034;&#34920;&#31034;&#34701;&#21512;&#26041;&#27861;ReFusion&#65292;&#36890;&#36807;&#23558;&#26816;&#32034;&#34920;&#31034;&#30452;&#25509;&#34701;&#21512;&#21040;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#35299;&#20915;&#20102;&#23558;&#22806;&#37096;&#25968;&#25454;&#24211;&#30340;&#30693;&#35782;&#34701;&#20837;&#38750;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#22806;&#37096;&#25968;&#25454;&#24211;&#20013;&#33719;&#21462;&#30693;&#35782;&#24182;&#34701;&#20837;&#35821;&#35328;&#27169;&#22411;&#30340;&#26816;&#32034;&#22686;&#24378;&#26041;&#27861;&#22312;&#21508;&#31181;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#65288;&#20363;&#22914;&#38382;&#31572;&#21644;&#25991;&#26412;&#29983;&#25104;&#65289;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22312;&#38750;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#65288;&#20363;&#22914;&#25991;&#26412;&#20998;&#31867;&#65289;&#20013;&#38598;&#25104;&#26816;&#32034;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#23558;&#26816;&#32034;&#20869;&#23481;&#25340;&#25509;&#21040;&#36755;&#20837;&#20013;&#24418;&#25104;&#25552;&#31034;&#24615;&#30340;&#36755;&#20837;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#35201;&#27714;&#35821;&#35328;&#27169;&#22411;&#20855;&#22791;&#22788;&#29702;&#38271;&#25991;&#26412;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25512;&#26029;&#36825;&#31181;&#25340;&#25509;&#25968;&#25454;&#20063;&#20250;&#28040;&#32791;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#39640;&#25928;&#30340;&#26816;&#32034;&#34920;&#31034;&#34701;&#21512;&#26041;&#27861;ReFusion&#65292;&#37319;&#29992;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#12290;&#20027;&#35201;&#24605;&#24819;&#26159;&#30452;&#25509;&#23558;&#26816;&#32034;&#34920;&#31034;&#19982;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#34701;&#21512;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#26816;&#32034;&#27169;&#22359;&#65292;&#37325;&#26032;&#26816;&#32034;...
&lt;/p&gt;
&lt;p&gt;
Retrieval-based augmentations that aim to incorporate knowledge from an external database into language models have achieved great success in various knowledge-intensive (KI) tasks, such as question-answering and text generation. However, integrating retrievals in non-knowledge-intensive (NKI) tasks, such as text classification, is still challenging. Existing works focus on concatenating retrievals to inputs as context to form the prompt-based inputs. Unfortunately, such methods require language models to have the capability to handle long texts. Besides, inferring such concatenated data would also consume a significant amount of computational resources.  To solve these challenges, we propose \textbf{ReFusion} in this paper, a computation-efficient \textbf{Re}trieval representation \textbf{Fusion} with neural architecture search. The main idea is to directly fuse the retrieval representations into the language models. Specifically, we first propose an online retrieval module that retri
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#35770;&#65292;&#21033;&#29992;"&#38750;&#32467;&#26500;&#21270;&#26680;&#24515;&#24211;"&#65292;&#23558;ESG&#25253;&#21578;&#36716;&#25442;&#20026;&#32467;&#26500;&#21270;&#12289;&#21487;&#20998;&#26512;&#30340;&#26684;&#24335;&#65292;&#24182;&#22312;&#25991;&#26412;&#28165;&#27927;&#12289;&#20174;&#22270;&#20687;&#20013;&#25935;&#38160;&#35782;&#21035;&#21644;&#25552;&#21462;&#25991;&#26412;&#20197;&#21450;&#26631;&#20934;&#21270;&#25253;&#21578;&#20013;&#30340;&#34920;&#26684;&#31561;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20026;&#24037;&#19994;&#29983;&#24577;&#23398;&#21644;&#20225;&#19994;&#21487;&#25345;&#32493;&#24615;&#35780;&#20272;&#39046;&#22495;&#30340;&#21457;&#23637;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2401.02992</link><description>&lt;p&gt;
ESG&#25253;&#21578;&#30340;&#39640;&#32423;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#22788;&#29702;&#65306;&#32467;&#26500;&#21270;&#36716;&#25442;&#21644;&#22686;&#24378;&#20998;&#26512;&#30340;&#26041;&#27861;&#35770;
&lt;/p&gt;
&lt;p&gt;
Advanced Unstructured Data Processing for ESG Reports: A Methodology for Structured Transformation and Enhanced Analysis. (arXiv:2401.02992v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02992
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#35770;&#65292;&#21033;&#29992;"&#38750;&#32467;&#26500;&#21270;&#26680;&#24515;&#24211;"&#65292;&#23558;ESG&#25253;&#21578;&#36716;&#25442;&#20026;&#32467;&#26500;&#21270;&#12289;&#21487;&#20998;&#26512;&#30340;&#26684;&#24335;&#65292;&#24182;&#22312;&#25991;&#26412;&#28165;&#27927;&#12289;&#20174;&#22270;&#20687;&#20013;&#25935;&#38160;&#35782;&#21035;&#21644;&#25552;&#21462;&#25991;&#26412;&#20197;&#21450;&#26631;&#20934;&#21270;&#25253;&#21578;&#20013;&#30340;&#34920;&#26684;&#31561;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20026;&#24037;&#19994;&#29983;&#24577;&#23398;&#21644;&#20225;&#19994;&#21487;&#25345;&#32493;&#24615;&#35780;&#20272;&#39046;&#22495;&#30340;&#21457;&#23637;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20225;&#19994;&#21487;&#25345;&#32493;&#24615;&#21457;&#23637;&#39046;&#22495;&#65292;&#20998;&#26512;&#38750;&#32467;&#26500;&#21270;&#30340;&#29615;&#22659;&#12289;&#31038;&#20250;&#21644;&#27835;&#29702;&#65288;ESG&#65289;&#25253;&#21578;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#21508;&#31181;&#26684;&#24335;&#21644;&#22797;&#26434;&#30340;&#20869;&#23481;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#35770;&#65292;&#21033;&#29992;"&#38750;&#32467;&#26500;&#21270;&#26680;&#24515;&#24211;"&#65292;&#19987;&#38376;&#38024;&#23545;&#36825;&#20123;&#25361;&#25112;&#23558;ESG&#25253;&#21578;&#36716;&#25442;&#20026;&#32467;&#26500;&#21270;&#12289;&#21487;&#20998;&#26512;&#30340;&#26684;&#24335;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25991;&#26412;&#28165;&#27927;&#12289;&#20174;&#22270;&#20687;&#20013;&#25935;&#38160;&#22320;&#35782;&#21035;&#21644;&#25552;&#21462;&#25991;&#26412;&#20197;&#21450;&#26631;&#20934;&#21270;&#36825;&#20123;&#25253;&#21578;&#20013;&#30340;&#34920;&#26684;&#31561;&#26041;&#38754;&#26174;&#33879;&#25512;&#36827;&#20102;&#29616;&#26377;&#30740;&#31350;&#12290;&#24378;&#35843;&#20854;&#22788;&#29702;&#19981;&#21516;&#34892;&#19994;&#30340;&#19981;&#21516;&#39029;&#38754;&#24067;&#23616;&#21644;&#25253;&#21578;&#26679;&#24335;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#35813;&#26041;&#27861;&#29087;&#32451;&#22320;&#31649;&#29702;&#21508;&#31181;&#25968;&#25454;&#31867;&#22411;&#65292;&#21253;&#25324;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#34920;&#26684;&#12290;&#36825;&#39033;&#30740;&#31350;&#23545;&#20110;&#24037;&#19994;&#29983;&#24577;&#23398;&#21644;&#20225;&#19994;&#21487;&#25345;&#32493;&#24615;&#35780;&#20272;&#39046;&#22495;&#20855;&#26377;&#37325;&#22823;&#24847;&#20041;&#65292;&#20026;&#24212;&#29992;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the evolving field of corporate sustainability, analyzing unstructured Environmental, Social, and Governance (ESG) reports is a complex challenge due to their varied formats and intricate content. This study introduces an innovative methodology utilizing the "Unstructured Core Library", specifically tailored to address these challenges by transforming ESG reports into structured, analyzable formats. Our approach significantly advances the existing research by offering high-precision text cleaning, adept identification and extraction of text from images, and standardization of tables within these reports. Emphasizing its capability to handle diverse data types, including text, images, and tables, the method adeptly manages the nuances of differing page layouts and report styles across industries. This research marks a substantial contribution to the fields of industrial ecology and corporate sustainability assessment, paving the way for the application of advanced NLP technologies an
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#22810;&#22836;&#21518;&#39564;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#23454;&#20307;&#30340;&#20803;&#29305;&#24449;&#21644;&#27169;&#22411;&#30340;&#34920;&#31034;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#20316;&#20026;&#24230;&#37327;&#26631;&#20934;&#65292;&#26377;&#25928;&#35780;&#20272;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2401.02987</link><description>&lt;p&gt;
&#20320;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#26377;&#25913;&#36827;&#21527;&#65311;&#19968;&#31181;&#22522;&#20110;&#22810;&#22836;&#21518;&#39564;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Has Your Pretrained Model Improved? A Multi-head Posterior Based Approach. (arXiv:2401.02987v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02987
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#22810;&#22836;&#21518;&#39564;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#23454;&#20307;&#30340;&#20803;&#29305;&#24449;&#21644;&#27169;&#22411;&#30340;&#34920;&#31034;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#20316;&#20026;&#24230;&#37327;&#26631;&#20934;&#65292;&#26377;&#25928;&#35780;&#20272;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20986;&#29616;&#23545;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#20851;&#31995;&#22411;&#25968;&#25454;&#38598;&#31561;&#39046;&#22495;&#20135;&#29983;&#20102;&#26174;&#33879;&#24433;&#21709;&#12290;&#20256;&#32479;&#19978;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#36807;&#19979;&#28216;&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#36825;&#24341;&#21457;&#20102;&#22914;&#20309;&#26356;&#39640;&#25928;&#12289;&#26356;&#26377;&#25928;&#22320;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;&#21033;&#29992;&#19982;&#27599;&#20010;&#23454;&#20307;&#30456;&#20851;&#30340;&#20803;&#29305;&#24449;&#20316;&#20026;&#19990;&#30028;&#30693;&#35782;&#30340;&#26469;&#28304;&#65292;&#24182;&#21033;&#29992;&#27169;&#22411;&#30340;&#23454;&#20307;&#34920;&#31034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#36825;&#20123;&#34920;&#31034;&#21644;&#20803;&#29305;&#24449;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#20316;&#20026;&#35780;&#20272;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#20010;&#39046;&#22495;&#34920;&#29616;&#20986;&#20102;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#20855;&#26377;&#20851;&#31995;&#22411;&#25968;&#25454;&#38598;&#12289;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22270;&#20687;&#27169;&#22411;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of pretrained models has significantly impacted from Natural Language Processing (NLP) and Computer Vision to relational datasets. Traditionally, these models are assessed through fine-tuned downstream tasks. However, this raises the question of how to evaluate these models more efficiently and more effectively. In this study, we explore a novel approach where we leverage the meta features associated with each entity as a source of worldly knowledge and employ entity representations from the models. We propose using the consistency between these representations and the meta features as a metric for evaluating pretrained models. Our method's effectiveness is demonstrated across various domains, including models with relational datasets, large language models and images models.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#22522;&#20110;&#23884;&#20837;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25490;&#24207;&#26041;&#27861;&#65292;&#29983;&#25104;AI&#27169;&#22411;&#20197;&#21450;&#20247;&#21253;&#21644;&#19987;&#23478;&#39537;&#21160;&#26041;&#27861;&#65292;&#26088;&#22312;&#30740;&#31350;&#22914;&#20309;&#36741;&#21161;&#27861;&#24459;&#21644;&#39046;&#22495;&#19987;&#23478;&#35782;&#21035;&#19982;&#19994;&#21153;&#27969;&#31243;&#30456;&#20851;&#30340;&#30417;&#31649;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2401.02986</link><description>&lt;p&gt;
&#35782;&#21035;&#19982;&#19994;&#21153;&#27969;&#31243;&#30456;&#20851;&#30340;&#30417;&#31649;&#35201;&#27714;&#65306;&#19968;&#39033;&#20851;&#20110;&#29983;&#25104;AI&#12289;&#22522;&#20110;&#23884;&#20837;&#30340;&#25490;&#24207;&#12289;&#20247;&#21253;&#21644;&#19987;&#23478;&#39537;&#21160;&#26041;&#27861;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Identification of Regulatory Requirements Relevant to Business Processes: A Comparative Study on Generative AI, Embedding-based Ranking, Crowd and Expert-driven Methods. (arXiv:2401.02986v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02986
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#22522;&#20110;&#23884;&#20837;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25490;&#24207;&#26041;&#27861;&#65292;&#29983;&#25104;AI&#27169;&#22411;&#20197;&#21450;&#20247;&#21253;&#21644;&#19987;&#23478;&#39537;&#21160;&#26041;&#27861;&#65292;&#26088;&#22312;&#30740;&#31350;&#22914;&#20309;&#36741;&#21161;&#27861;&#24459;&#21644;&#39046;&#22495;&#19987;&#23478;&#35782;&#21035;&#19982;&#19994;&#21153;&#27969;&#31243;&#30456;&#20851;&#30340;&#30417;&#31649;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32452;&#32455;&#38754;&#20020;&#30528;&#30830;&#20445;&#36981;&#23432;&#21508;&#31181;&#30417;&#31649;&#25991;&#20214;&#20013;&#36234;&#26469;&#36234;&#22810;&#35201;&#27714;&#30340;&#25361;&#25112;&#12290;&#21738;&#20123;&#35201;&#27714;&#26159;&#30456;&#20851;&#30340;&#21462;&#20915;&#20110;&#22914;&#32452;&#32455;&#30340;&#22320;&#29702;&#20301;&#32622;&#12289;&#39046;&#22495;&#12289;&#35268;&#27169;&#21644;&#19994;&#21153;&#27969;&#31243;&#31561;&#26041;&#38754;&#30340;&#22240;&#32032;&#12290;&#32771;&#34385;&#21040;&#36825;&#20123;&#24773;&#22659;&#22240;&#32032;&#65292;&#39318;&#20808;&#38656;&#35201;&#35782;&#21035;&#30456;&#20851;&#25991;&#20214;&#65288;&#20363;&#22914;&#27861;&#24459;&#12289;&#35268;&#21017;&#12289;&#25351;&#20196;&#12289;&#25919;&#31574;&#65289;&#65292;&#28982;&#21518;&#35814;&#32454;&#20998;&#26512;&#35782;&#21035;&#25991;&#20214;&#30340;&#21738;&#20123;&#37096;&#20998;&#19982;&#32473;&#23450;&#19994;&#21153;&#27969;&#31243;&#30340;&#21738;&#20010;&#27493;&#39588;&#30456;&#20851;&#12290;&#30446;&#21069;&#65292;&#35782;&#21035;&#19982;&#19994;&#21153;&#27969;&#31243;&#30456;&#20851;&#30340;&#30417;&#31649;&#35201;&#27714;&#20027;&#35201;&#30001;&#39046;&#22495;&#21644;&#27861;&#24459;&#19987;&#23478;&#25163;&#21160;&#23436;&#25104;&#65292;&#23545;&#20182;&#20204;&#26469;&#35828;&#26159;&#19968;&#39033;&#24040;&#22823;&#30340;&#24037;&#20316;&#37327;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#21487;&#33021;&#32463;&#24120;&#21464;&#21270;&#30340;&#22823;&#37327;&#30417;&#31649;&#25991;&#20214;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#36741;&#21161;&#27861;&#24459;&#21644;&#39046;&#22495;&#19987;&#23478;&#35780;&#20272;&#30456;&#20851;&#35201;&#27714;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#22522;&#20110;&#23884;&#20837;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25490;&#24207;&#26041;&#27861;&#65292;&#29983;&#25104;AI&#27169;&#22411;&#21450;&#20247;&#21253;&#21644;&#19987;&#23478;&#39537;&#21160;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Organizations face the challenge of ensuring compliance with an increasing amount of requirements from various regulatory documents. Which requirements are relevant depends on aspects such as the geographic location of the organization, its domain, size, and business processes. Considering these contextual factors, as a first step, relevant documents (e.g., laws, regulations, directives, policies) are identified, followed by a more detailed analysis of which parts of the identified documents are relevant for which step of a given business process. Nowadays the identification of regulatory requirements relevant to business processes is mostly done manually by domain and legal experts, posing a tremendous effort on them, especially for a large number of regulatory documents which might frequently change. Hence, this work examines how legal and domain experts can be assisted in the assessment of relevant requirements. For this, we compare an embedding-based NLP ranking method, a generativ
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#19971;&#20010;&#20027;&#35201;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;GMAT&#32771;&#35797;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;&#22823;&#22810;&#25968;&#27169;&#22411;&#20248;&#20110;&#20154;&#31867;&#32771;&#29983;&#65292;&#20854;&#20013;GPT-4 Turbo&#19981;&#20165;&#22312;&#20854;&#20182;&#27169;&#22411;&#20043;&#19978;&#65292;&#32780;&#19988;&#36229;&#36807;&#20102;&#39030;&#32423;&#21830;&#23398;&#38498;&#30740;&#31350;&#29983;&#30340;&#24179;&#22343;&#20998;&#25968;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#65292;&#26412;&#30740;&#31350;&#36824;&#32771;&#23519;&#20102;GPT-4 Turbo&#22312;&#35299;&#37322;&#31572;&#26696;&#12289;&#35780;&#20272;&#22238;&#31572;&#12289;&#35782;&#21035;&#38169;&#35823;&#12289;&#35843;&#25972;&#25351;&#23548;&#21644;&#29983;&#25104;&#26367;&#20195;&#22330;&#26223;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.02985</link><description>&lt;p&gt;
&#22312;GMAT&#19978;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#23545;&#26410;&#26469;&#21830;&#19994;&#25945;&#32946;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Evaluating Large Language Models on the GMAT: Implications for the Future of Business Education. (arXiv:2401.02985v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02985
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#19971;&#20010;&#20027;&#35201;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;GMAT&#32771;&#35797;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;&#22823;&#22810;&#25968;&#27169;&#22411;&#20248;&#20110;&#20154;&#31867;&#32771;&#29983;&#65292;&#20854;&#20013;GPT-4 Turbo&#19981;&#20165;&#22312;&#20854;&#20182;&#27169;&#22411;&#20043;&#19978;&#65292;&#32780;&#19988;&#36229;&#36807;&#20102;&#39030;&#32423;&#21830;&#23398;&#38498;&#30740;&#31350;&#29983;&#30340;&#24179;&#22343;&#20998;&#25968;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#65292;&#26412;&#30740;&#31350;&#36824;&#32771;&#23519;&#20102;GPT-4 Turbo&#22312;&#35299;&#37322;&#31572;&#26696;&#12289;&#35780;&#20272;&#22238;&#31572;&#12289;&#35782;&#21035;&#38169;&#35823;&#12289;&#35843;&#25972;&#25351;&#23548;&#21644;&#29983;&#25104;&#26367;&#20195;&#22330;&#26223;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#23588;&#20854;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#36805;&#36895;&#21457;&#23637;&#65292;&#20026;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#24320;&#36767;&#20102;&#26032;&#30340;&#36884;&#24452;&#65292;&#28982;&#32780;&#20854;&#22312;&#21830;&#19994;&#25945;&#32946;&#20013;&#30340;&#20316;&#29992;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#39318;&#20010;&#22522;&#20934;&#26469;&#35780;&#20272;&#19971;&#20010;&#37325;&#35201;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21253;&#25324;OpenAI&#30340;&#27169;&#22411;&#65288;GPT-3.5 Turbo&#12289;GPT-4&#21644;GPT-4 Turbo&#65289;&#12289;Google&#30340;&#27169;&#22411;&#65288;PaLM 2&#12289;Gemini 1.0 Pro&#65289;&#21644;Anthropic&#30340;&#27169;&#22411;&#65288;Claude 2&#21644;Claude 2.1&#65289;&#65292;&#22312;GMAT&#32771;&#35797;&#19978;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#26174;&#31034;&#65292;&#22823;&#22810;&#25968;&#35821;&#35328;&#27169;&#22411;&#20248;&#20110;&#20154;&#31867;&#32771;&#29983;&#65292;&#20854;&#20013;GPT-4 Turbo&#19981;&#20165;&#22312;&#20854;&#20182;&#27169;&#22411;&#20043;&#19978;&#65292;&#32780;&#19988;&#36229;&#36807;&#20102;&#39030;&#32423;&#21830;&#23398;&#38498;&#30740;&#31350;&#29983;&#30340;&#24179;&#22343;&#20998;&#25968;&#12290;&#36890;&#36807;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#26412;&#30740;&#31350;&#32771;&#23519;&#20102;GPT-4 Turbo&#22312;&#35299;&#37322;&#31572;&#26696;&#12289;&#35780;&#20272;&#22238;&#31572;&#12289;&#35782;&#21035;&#38169;&#35823;&#12289;&#35843;&#25972;&#25351;&#23548;&#21644;&#29983;&#25104;&#26367;&#20195;&#22330;&#26223;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid evolution of artificial intelligence (AI), especially in the domain of Large Language Models (LLMs) and generative AI, has opened new avenues for application across various fields, yet its role in business education remains underexplored. This study introduces the first benchmark to assess the performance of seven major LLMs, OpenAI's models (GPT-3.5 Turbo, GPT-4, and GPT-4 Turbo), Google's models (PaLM 2, Gemini 1.0 Pro), and Anthropic's models (Claude 2 and Claude 2.1), on the GMAT, which is a key exam in the admission process for graduate business programs. Our analysis shows that most LLMs outperform human candidates, with GPT-4 Turbo not only outperforming the other models but also surpassing the average scores of graduate students at top business schools. Through a case study, this research examines GPT-4 Turbo's ability to explain answers, evaluate responses, identify errors, tailor instructions, and generate alternative scenarios. The latest LLM versions, GPT-4 Turbo,
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#30740;&#31350;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24515;&#29702;&#20581;&#24247;&#25252;&#29702;&#20013;&#30340;&#24212;&#29992;&#21644;&#32467;&#26524;&#36827;&#34892;&#20102;&#32508;&#21512;&#20998;&#26512;&#65292;&#21457;&#29616;&#20854;&#22312;&#35786;&#26029;&#12289;&#27835;&#30103;&#21644;&#24739;&#32773;&#21442;&#19982;&#22686;&#24378;&#31561;&#26041;&#38754;&#20855;&#26377;&#22810;&#26679;&#21270;&#30340;&#24212;&#29992;&#12290;&#21516;&#26102;&#65292;&#35813;&#30740;&#31350;&#36824;&#35782;&#21035;&#21644;&#35752;&#35770;&#20102;&#22312;&#36825;&#20123;&#19987;&#19994;&#39046;&#22495;&#20013;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2401.02984</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24515;&#29702;&#20581;&#24247;&#25252;&#29702;&#20013;&#30340;&#24212;&#29992;&#65306;&#19968;&#39033;&#32508;&#36848;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Large Language Models in Mental Health Care: a Scoping Review. (arXiv:2401.02984v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02984
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#30740;&#31350;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24515;&#29702;&#20581;&#24247;&#25252;&#29702;&#20013;&#30340;&#24212;&#29992;&#21644;&#32467;&#26524;&#36827;&#34892;&#20102;&#32508;&#21512;&#20998;&#26512;&#65292;&#21457;&#29616;&#20854;&#22312;&#35786;&#26029;&#12289;&#27835;&#30103;&#21644;&#24739;&#32773;&#21442;&#19982;&#22686;&#24378;&#31561;&#26041;&#38754;&#20855;&#26377;&#22810;&#26679;&#21270;&#30340;&#24212;&#29992;&#12290;&#21516;&#26102;&#65292;&#35813;&#30740;&#31350;&#36824;&#35782;&#21035;&#21644;&#35752;&#35770;&#20102;&#22312;&#36825;&#20123;&#19987;&#19994;&#39046;&#22495;&#20013;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20351;&#29992;&#36234;&#26469;&#36234;&#24191;&#27867;&#65292;&#38656;&#35201;&#23545;&#23427;&#20204;&#22312;&#24515;&#29702;&#20581;&#24247;&#25252;&#29702;&#39046;&#22495;&#30340;&#24212;&#29992;&#21644;&#32467;&#26524;&#36827;&#34892;&#20840;&#38754;&#30340;&#32508;&#36848;&#12290;&#26412;&#32508;&#36848;&#30740;&#31350;&#26088;&#22312;&#23545;LLMs&#22312;&#24515;&#29702;&#20581;&#24247;&#25252;&#29702;&#20013;&#30340;&#29616;&#26377;&#21457;&#23637;&#21644;&#24212;&#29992;&#36827;&#34892;&#25209;&#21028;&#24615;&#20998;&#26512;&#65292;&#31361;&#20986;&#23427;&#20204;&#30340;&#25104;&#21151;&#65292;&#24182;&#35782;&#21035;&#36825;&#20123;&#19987;&#19994;&#39046;&#22495;&#20013;&#30340;&#25361;&#25112;&#21644;&#38480;&#21046;&#12290;&#26448;&#26009;&#21644;&#26041;&#27861;&#65306;2023&#24180;11&#26376;&#65292;&#22312;PubMed&#12289;Web of Science&#12289;Google Scholar&#12289;arXiv&#12289;medRxiv&#21644;PsyArXiv&#20845;&#20010;&#25968;&#25454;&#24211;&#20013;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#25991;&#29486;&#25628;&#32034;&#65292;&#36981;&#24490;2020&#24180;&#29256;&#30340;&#8220;&#31995;&#32479;&#35780;&#20215;&#21644;Meta&#20998;&#26512;&#30340;&#39318;&#36873;&#25253;&#21578;&#39033;&#30446;&#8221;&#65288;PRISMA&#65289;&#25351;&#21335;&#12290;&#26368;&#21021;&#35782;&#21035;&#20102;313&#31687;&#20986;&#29256;&#29289;&#65292;&#25353;&#29031;&#30740;&#31350;&#32435;&#20837;&#26631;&#20934;&#65292;&#26368;&#32456;&#36873;&#25321;&#20102;34&#31687;&#20986;&#29256;&#29289;&#36827;&#34892;&#32508;&#36848;&#12290;&#32467;&#26524;&#65306;&#25105;&#20204;&#21457;&#29616;&#20102;LLMs&#22312;&#24515;&#29702;&#20581;&#24247;&#25252;&#29702;&#20013;&#30340;&#22810;&#31181;&#24212;&#29992;&#65292;&#21253;&#25324;&#35786;&#26029;&#12289;&#27835;&#30103;&#12289;&#24739;&#32773;&#21442;&#19982;&#22686;&#24378;&#31561;&#12290;&#20851;&#38190;&#25361;&#25112;&#21644;&#38480;&#21046;&#26041;&#38754;&#30340;&#21457;&#29616;&#23558;&#34987;&#24635;&#32467;&#21644;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Objective: The growing use of large language models (LLMs) stimulates a need for a comprehensive review of their applications and outcomes in mental health care contexts. This scoping review aims to critically analyze the existing development and applications of LLMs in mental health care, highlighting their successes and identifying their challenges and limitations in these specialized fields. Materials and Methods: A broad literature search was conducted in November 2023 using six databases (PubMed, Web of Science, Google Scholar, arXiv, medRxiv, and PsyArXiv) following the 2020 version of the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines. A total of 313 publications were initially identified, and after applying the study inclusion criteria, 34 publications were selected for the final review. Results: We identified diverse applications of LLMs in mental health care, including diagnosis, therapy, patient engagement enhancement, etc. Key challen
&lt;/p&gt;</description></item><item><title>BIBench&#26159;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21830;&#19994;&#26234;&#33021;&#65288;BI&#65289;&#25968;&#25454;&#20998;&#26512;&#39046;&#22495;&#20013;&#33021;&#21147;&#30340;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#36890;&#36807;&#27979;&#35797;&#27169;&#22411;&#22312;BI&#22522;&#30784;&#30693;&#35782;&#12289;&#24212;&#29992;&#30693;&#35782;&#21644;&#25216;&#26415;&#25216;&#33021;&#19977;&#20010;&#32500;&#24230;&#19978;&#30340;&#34920;&#29616;&#26469;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2401.02982</link><description>&lt;p&gt;
BIBench: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25968;&#25454;&#20998;&#26512;&#30693;&#35782;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
BIBench: Benchmarking Data Analysis Knowledge of Large Language Models. (arXiv:2401.02982v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02982
&lt;/p&gt;
&lt;p&gt;
BIBench&#26159;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21830;&#19994;&#26234;&#33021;&#65288;BI&#65289;&#25968;&#25454;&#20998;&#26512;&#39046;&#22495;&#20013;&#33021;&#21147;&#30340;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#36890;&#36807;&#27979;&#35797;&#27169;&#22411;&#22312;BI&#22522;&#30784;&#30693;&#35782;&#12289;&#24212;&#29992;&#30693;&#35782;&#21644;&#25216;&#26415;&#25216;&#33021;&#19977;&#20010;&#32500;&#24230;&#19978;&#30340;&#34920;&#29616;&#26469;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#25968;&#25454;&#20998;&#26512;&#30340;&#19987;&#19994;&#39046;&#22495;&#20013;&#30340;&#29087;&#32451;&#24230;&#21644;&#21487;&#38752;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#20197;&#25968;&#25454;&#39537;&#21160;&#24605;&#32500;&#20026;&#37325;&#28857;&#30340;&#39046;&#22495;&#20013;&#65292;&#20173;&#28982;&#23384;&#22312;&#19981;&#30830;&#23450;&#24615;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;BIBench&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;LLMs&#22312;&#21830;&#19994;&#26234;&#33021;&#65288;BI&#65289;&#30340;&#32972;&#26223;&#19979;&#30340;&#25968;&#25454;&#20998;&#26512;&#33021;&#21147;&#12290;BIBench&#36890;&#36807;&#19977;&#20010;&#32500;&#24230;&#35780;&#20272;LLMs&#65306;1&#65289;BI&#22522;&#30784;&#30693;&#35782;&#65292;&#35780;&#20272;&#27169;&#22411;&#30340;&#25968;&#20540;&#25512;&#29702;&#33021;&#21147;&#21644;&#23545;&#37329;&#34701;&#27010;&#24565;&#30340;&#29087;&#24713;&#31243;&#24230;&#65307;2&#65289;BI&#30693;&#35782;&#24212;&#29992;&#65292;&#30830;&#23450;&#27169;&#22411;&#24555;&#36895;&#29702;&#35299;&#25991;&#26412;&#20449;&#24687;&#24182;&#20174;&#22810;&#20010;&#35270;&#35282;&#29983;&#25104;&#20998;&#26512;&#38382;&#39064;&#30340;&#33021;&#21147;&#65307;3&#65289;BI&#25216;&#26415;&#25216;&#33021;&#65292;&#26816;&#26597;&#27169;&#22411;&#20351;&#29992;&#25216;&#26415;&#30693;&#35782;&#35299;&#20915;&#29616;&#23454;&#25968;&#25454;&#20998;&#26512;&#25361;&#25112;&#30340;&#33021;&#21147;&#12290;BIBench&#21253;&#25324;11&#20010;&#23376;&#20219;&#21153;&#65292;&#28085;&#30422;&#20998;&#31867;&#12289;&#25552;&#21462;&#21644;&#29983;&#25104;&#19977;&#31181;&#20219;&#21153;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated impressive capabilities across a wide range of tasks. However, their proficiency and reliability in the specialized domain of Data Analysis, particularly with a focus on data-driven thinking, remain uncertain. To bridge this gap, we introduce BIBench, a comprehensive benchmark designed to evaluate the data analysis capabilities of LLMs within the context of Business Intelligence (BI). BIBench assesses LLMs across three dimensions: 1) BI foundational knowledge, evaluating the models' numerical reasoning and familiarity with financial concepts; 2) BI knowledge application, determining the models' ability to quickly comprehend textual information and generate analysis questions from multiple views; and 3) BI technical skills, examining the models' use of technical knowledge to address real-world data analysis challenges. BIBench comprises 11 sub-tasks, spanning three categories of task types: classification, extraction, and generation. Additi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#39046;&#22495;&#29305;&#23450;LLM&#30340;&#24494;&#35843;&#21644;&#21033;&#29992;&#26041;&#27861;&#65292;&#20197;&#37329;&#34701;&#39046;&#22495;&#20026;&#20363;&#65292;&#35814;&#32454;&#20171;&#32461;&#20102;&#25968;&#25454;&#38598;&#36873;&#25321;&#12289;&#39044;&#22788;&#29702;&#12289;&#27169;&#22411;&#36873;&#25321;&#20197;&#21450;&#22312;&#37329;&#34701;&#39046;&#22495;LLM&#24494;&#35843;&#20013;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#30740;&#31350;&#25506;&#35752;&#20102;&#39046;&#22495;&#29305;&#23450;&#35789;&#27719;&#30340;&#26500;&#24314;&#21644;&#23433;&#20840;&#21512;&#35268;&#24615;&#30340;&#32771;&#34385;&#22240;&#32032;&#65292;&#24182;&#25552;&#20379;&#20102;&#22312;&#37329;&#34701;&#39046;&#22495;&#29983;&#25104;&#39046;&#22495;&#29305;&#23450;LLM&#30340;&#36807;&#31243;&#21644;&#23454;&#26045;&#26041;&#27861;&#12290;&#22810;&#31181;&#37329;&#34701;&#26696;&#20363;&#34987;&#28085;&#30422;&#22312;&#20869;&#65292;&#21253;&#25324;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;&#12289;&#37329;&#34701;&#26032;&#38395;&#24773;&#32490;&#20998;&#26512;&#12289;&#33258;&#21160;&#25991;&#26723;&#22788;&#29702;&#12289;&#30740;&#31350;&#21644;&#20449;&#24687;&#25552;&#21462;&#31561;&#12290;</title><link>http://arxiv.org/abs/2401.02981</link><description>&lt;p&gt;
&#39046;&#22495;&#29305;&#23450;LLM&#30340;&#24494;&#35843;&#21644;&#21033;&#29992;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning and Utilization Methods of Domain-specific LLMs. (arXiv:2401.02981v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02981
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#39046;&#22495;&#29305;&#23450;LLM&#30340;&#24494;&#35843;&#21644;&#21033;&#29992;&#26041;&#27861;&#65292;&#20197;&#37329;&#34701;&#39046;&#22495;&#20026;&#20363;&#65292;&#35814;&#32454;&#20171;&#32461;&#20102;&#25968;&#25454;&#38598;&#36873;&#25321;&#12289;&#39044;&#22788;&#29702;&#12289;&#27169;&#22411;&#36873;&#25321;&#20197;&#21450;&#22312;&#37329;&#34701;&#39046;&#22495;LLM&#24494;&#35843;&#20013;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#30740;&#31350;&#25506;&#35752;&#20102;&#39046;&#22495;&#29305;&#23450;&#35789;&#27719;&#30340;&#26500;&#24314;&#21644;&#23433;&#20840;&#21512;&#35268;&#24615;&#30340;&#32771;&#34385;&#22240;&#32032;&#65292;&#24182;&#25552;&#20379;&#20102;&#22312;&#37329;&#34701;&#39046;&#22495;&#29983;&#25104;&#39046;&#22495;&#29305;&#23450;LLM&#30340;&#36807;&#31243;&#21644;&#23454;&#26045;&#26041;&#27861;&#12290;&#22810;&#31181;&#37329;&#34701;&#26696;&#20363;&#34987;&#28085;&#30422;&#22312;&#20869;&#65292;&#21253;&#25324;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;&#12289;&#37329;&#34701;&#26032;&#38395;&#24773;&#32490;&#20998;&#26512;&#12289;&#33258;&#21160;&#25991;&#26723;&#22788;&#29702;&#12289;&#30740;&#31350;&#21644;&#20449;&#24687;&#25552;&#21462;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#21457;&#24067;&#30340;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#65292;&#20294;&#20851;&#20110;&#39046;&#22495;&#29305;&#23450;LLM&#30340;&#24494;&#35843;&#21644;&#24212;&#29992;&#30340;&#30740;&#31350;&#20173;&#28982;&#24456;&#23569;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#39046;&#22495;&#29305;&#23450;LLM&#30340;&#24494;&#35843;&#21644;&#21033;&#29992;&#26041;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;LLM&#30340;&#36235;&#21183;&#12289;&#22522;&#30784;&#27169;&#22411;&#21644;&#29992;&#20110;&#39046;&#22495;&#29305;&#23450;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#12290;&#20197;&#37329;&#34701;&#34892;&#19994;&#20026;&#20363;&#65292;&#35814;&#32454;&#20171;&#32461;&#20102;&#25968;&#25454;&#38598;&#36873;&#25321;&#12289;&#39044;&#22788;&#29702;&#12289;&#27169;&#22411;&#36873;&#25321;&#20197;&#21450;&#22312;&#37329;&#34701;&#39046;&#22495;LLM&#24494;&#35843;&#20013;&#20851;&#38190;&#30340;&#32771;&#34385;&#22240;&#32032;&#12290;&#38024;&#23545;&#37329;&#34701;&#25968;&#25454;&#30340;&#29420;&#29305;&#29305;&#28857;&#65292;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#39046;&#22495;&#29305;&#23450;&#35789;&#27719;&#30340;&#26500;&#24314;&#65292;&#20197;&#21450;&#23433;&#20840;&#24615;&#21644;&#21512;&#35268;&#24615;&#30340;&#32771;&#34385;&#22240;&#32032;&#12290;&#22312;LLM&#24494;&#35843;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#26412;&#30740;&#31350;&#27010;&#36848;&#20102;&#22312;&#37329;&#34701;&#39046;&#22495;&#29983;&#25104;&#39046;&#22495;&#29305;&#23450;LLM&#30340;&#36807;&#31243;&#21644;&#23454;&#26045;&#26041;&#27861;&#12290;&#21253;&#25324;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;&#12289;&#37329;&#34701;&#26032;&#38395;&#24773;&#32490;&#20998;&#26512;&#12289;&#33258;&#21160;&#25991;&#26723;&#22788;&#29702;&#12289;&#30740;&#31350;&#21644;&#20449;&#24687;&#25552;&#21462;&#31561;&#22810;&#31181;&#37329;&#34701;&#26696;&#20363;&#34987;&#28085;&#30422;&#22312;&#20869;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent releases of pre-trained Large Language Models (LLMs) have gained considerable traction, yet research on fine-tuning and employing domain-specific LLMs remains scarce. This study investigates approaches for fine-tuning and leveraging domain-specific LLMs, highlighting trends in LLMs, foundational models, and methods for domain-specific pre-training. Focusing on the financial sector, it details dataset selection, preprocessing, model choice, and considerations crucial for LLM fine-tuning in finance. Addressing the unique characteristics of financial data, the study explores the construction of domain-specific vocabularies and considerations for security and regulatory compliance. In the practical application of LLM fine-tuning, the study outlines the procedure and implementation for generating domain-specific LLMs in finance. Various financial cases, including stock price prediction, sentiment analysis of financial news, automated document processing, research, information extract
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#23545;&#34920;&#29616;&#21147;&#38050;&#29748;&#28436;&#22863;&#29305;&#24449;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#27979;&#35797;&#20102;&#20116;&#20010;&#23884;&#20837;&#27169;&#22411;&#21450;&#20854;&#30456;&#20284;&#24615;&#32467;&#26500;&#19982;&#30495;&#20540;&#30340;&#23545;&#24212;&#20851;&#31995;&#65292;&#24182;&#36827;&#34892;&#20102;&#36827;&#19968;&#27493;&#35780;&#20272;&#12290;&#23884;&#20837;&#27169;&#22411;&#30340;&#36136;&#37327;&#22312;&#36825;&#26041;&#38754;&#26174;&#31034;&#20986;&#24456;&#22823;&#30340;&#24046;&#24322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.02979</link><description>&lt;p&gt;
&#25105;&#20204;&#22312;&#25551;&#36848;&#21516;&#26679;&#30340;&#22768;&#38899;&#21527;&#65311;&#23545;&#34920;&#29616;&#21147;&#38050;&#29748;&#28436;&#22863;&#35789;&#23884;&#20837;&#31354;&#38388;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Are we describing the same sound? An analysis of word embedding spaces of expressive piano performance. (arXiv:2401.02979v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02979
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#23545;&#34920;&#29616;&#21147;&#38050;&#29748;&#28436;&#22863;&#29305;&#24449;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#27979;&#35797;&#20102;&#20116;&#20010;&#23884;&#20837;&#27169;&#22411;&#21450;&#20854;&#30456;&#20284;&#24615;&#32467;&#26500;&#19982;&#30495;&#20540;&#30340;&#23545;&#24212;&#20851;&#31995;&#65292;&#24182;&#36827;&#34892;&#20102;&#36827;&#19968;&#27493;&#35780;&#20272;&#12290;&#23884;&#20837;&#27169;&#22411;&#30340;&#36136;&#37327;&#22312;&#36825;&#26041;&#38754;&#26174;&#31034;&#20986;&#24456;&#22823;&#30340;&#24046;&#24322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#23884;&#20837;&#22312;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#30340;&#20449;&#24687;&#26816;&#32034;&#20013;&#36215;&#21040;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#23884;&#20837;&#27169;&#22411;&#23558;&#21333;&#35789;&#21644;&#19978;&#19979;&#25991;&#34920;&#31034;&#20026;&#21521;&#37327;&#65292;&#20854;&#31354;&#38388;&#37197;&#32622;&#26159;&#26681;&#25454;&#22823;&#22411;&#25991;&#26412;&#35821;&#26009;&#24211;&#20013;&#21333;&#35789;&#30340;&#20998;&#24067;&#23548;&#20986;&#30340;&#12290;&#23613;&#31649;&#36825;&#20123;&#34920;&#31034;&#19968;&#33324;&#38750;&#24120;&#24378;&#22823;&#65292;&#20294;&#23427;&#20204;&#21487;&#33021;&#26410;&#33021;&#32771;&#34385;&#21040;&#32454;&#31890;&#24230;&#30340;&#39046;&#22495;&#29305;&#23450;&#32454;&#24494;&#24046;&#21035;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#36825;&#31181;&#23545;&#34920;&#29616;&#21147;&#38050;&#29748;&#28436;&#22863;&#29305;&#24449;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#38899;&#20048;&#30740;&#31350;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#33258;&#30001;&#25991;&#26412;&#28436;&#22863;&#29305;&#24449;&#30340;&#27880;&#37322;&#65292;&#24182;&#36827;&#34892;&#20102;&#19968;&#20010;&#21518;&#32493;&#30740;&#31350;&#65292;&#23558;&#27880;&#37322;&#20998;&#31867;&#25104;&#19981;&#21516;&#30340;&#32858;&#31867;&#12290;&#25105;&#20204;&#24471;&#20986;&#20102;&#19968;&#20010;&#29305;&#23450;&#39046;&#22495;&#35821;&#20041;&#30456;&#20284;&#24615;&#32467;&#26500;&#30340;&#30495;&#20540;&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;&#20116;&#20010;&#23884;&#20837;&#27169;&#22411;&#21450;&#20854;&#30456;&#20284;&#24615;&#32467;&#26500;&#19982;&#30495;&#20540;&#30340;&#23545;&#24212;&#20851;&#31995;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35780;&#20272;&#20102;&#19978;&#19979;&#25991;&#25552;&#31034;&#12289;&#20013;&#24515;&#24230;&#38477;&#20302;&#12289;&#36328;&#27169;&#24577;&#30456;&#20284;&#24615;&#21644;k-means&#32858;&#31867;&#30340;&#24433;&#21709;&#12290;&#23884;&#20837;&#27169;&#22411;&#30340;&#36136;&#37327;&#22312;&#36825;&#26041;&#38754;&#26174;&#31034;&#20986;&#24456;&#22823;&#30340;&#24046;&#24322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic embeddings play a crucial role in natural language-based information retrieval. Embedding models represent words and contexts as vectors whose spatial configuration is derived from the distribution of words in large text corpora. While such representations are generally very powerful, they might fail to account for fine-grained domain-specific nuances. In this article, we investigate this uncertainty for the domain of characterizations of expressive piano performance. Using a music research dataset of free text performance characterizations and a follow-up study sorting the annotations into clusters, we derive a ground truth for a domain-specific semantic similarity structure. We test five embedding models and their similarity structure for correspondence with the ground truth. We further assess the effects of contextualizing prompts, hubness reduction, cross-modal similarity, and k-means clustering. The quality of embedding models shows great variability with respect to this 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#19982;&#34394;&#25311;&#20249;&#20276;Zo&#20114;&#21160;&#30340;&#21160;&#26426;&#65292;&#24635;&#32467;&#20986;&#20102;&#22810;&#31181;&#22686;&#21152;&#20114;&#21160;&#24615;&#30340;&#26041;&#27861;&#65292;&#20026;&#29983;&#25104;&#24335;AI&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#20511;&#37492;&#12290;</title><link>http://arxiv.org/abs/2401.02978</link><description>&lt;p&gt;
&#20174;&#29983;&#25104;&#24335;AI&#21069;&#36744;&#20013;&#23398;&#20064;&#8212;&#8212;&#19982;&#23545;&#35805;&#20195;&#29702;&#20114;&#21160;&#30340;&#35768;&#22810;&#21160;&#26426;
&lt;/p&gt;
&lt;p&gt;
Learning from a Generative AI Predecessor -- The Many Motivations for Interacting with Conversational Agents. (arXiv:2401.02978v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02978
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#19982;&#34394;&#25311;&#20249;&#20276;Zo&#20114;&#21160;&#30340;&#21160;&#26426;&#65292;&#24635;&#32467;&#20986;&#20102;&#22810;&#31181;&#22686;&#21152;&#20114;&#21160;&#24615;&#30340;&#26041;&#27861;&#65292;&#20026;&#29983;&#25104;&#24335;AI&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#20511;&#37492;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35201;&#20351;&#29983;&#25104;&#24335;AI&#25104;&#21151;&#65292;&#23427;&#24517;&#39035;&#20855;&#22791;&#22810;&#20040;&#24341;&#20154;&#20837;&#32988;&#30340;&#23545;&#35805;&#33021;&#21147;&#65311;&#36817;60&#24180;&#26469;&#65292;&#19968;&#20123;&#23545;&#35805;&#20195;&#29702;&#20250;&#22238;&#24212;&#20219;&#20309;&#38382;&#39064;&#25110;&#35780;&#35770;&#20197;&#20445;&#25345;&#23545;&#35805;&#30340;&#36827;&#34892;&#12290;&#36817;&#24180;&#26469;&#65292;&#35768;&#22810;&#20154;&#24320;&#22987;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25110;&#22797;&#26434;&#30340;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#20363;&#22914;Tay&#12289;Xiaoice&#12289;Zo&#12289;Hugging Face&#12289;Kuki&#21644;Replika&#12290;&#19982;&#29983;&#25104;&#24335;AI&#19981;&#21516;&#30340;&#26159;&#65292;&#23427;&#20204;&#20851;&#27880;&#30340;&#26159;&#20114;&#21160;&#24615;&#65292;&#32780;&#19981;&#26159;&#19987;&#19994;&#30693;&#35782;&#12290;&#25968;&#30334;&#19975;&#20154;&#34987;&#28608;&#21457;&#36215;&#19982;&#23427;&#20204;&#36827;&#34892;&#20114;&#21160;&#12290;&#37027;&#20040;&#36825;&#20123;&#21560;&#24341;&#21147;&#26159;&#20160;&#20040;&#21602;&#65311;&#22914;&#26524;&#29983;&#25104;&#24335;AI&#21516;&#26679;&#24341;&#20154;&#20837;&#32988;&#65292;&#23427;&#20250;&#20570;&#24471;&#26356;&#22909;&#21527;&#65292;&#36824;&#26159;&#24212;&#35813;&#20943;&#23569;&#20114;&#21160;&#24615;&#65311;&#22312;&#29983;&#25104;&#24335;AI&#20986;&#29616;&#20043;&#21069;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#23450;&#37327;&#21644;&#23450;&#24615;&#20998;&#26512;&#65292;&#20197;&#20102;&#35299;&#25968;&#30334;&#19975;&#20154;&#19982;&#24494;&#36719;&#20249;&#20276;Zo&#36827;&#34892;&#20114;&#21160;&#30340;&#21160;&#26426;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;2000&#20010;&#21311;&#21517;&#29992;&#25143;&#30340;&#23436;&#25972;&#32842;&#22825;&#35760;&#24405;&#65292;&#24182;&#30830;&#23450;&#20102;&#25968;&#21313;&#31181;&#20154;&#20204;&#19982;&#35813;&#36719;&#20214;&#36827;&#34892;&#20114;&#21160;&#30340;&#21160;&#26426;&#12290;&#35774;&#35745;&#24072;&#20204;&#23398;&#20250;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#20114;&#21160;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
For generative AI to succeed, how engaging a conversationalist must it be? For almost sixty years, some conversational agents have responded to any question or comment to keep a conversation going. In recent years, several utilized machine learning or sophisticated language processing, such as Tay, Xiaoice, Zo, Hugging Face, Kuki, and Replika. Unlike generative AI, they focused on engagement, not expertise. Millions of people were motivated to engage with them. What were the attractions? Will generative AI do better if it is equally engaging, or should it be less engaging? Prior to the emergence of generative AI, we conducted a large-scale quantitative and qualitative analysis to learn what motivated millions of people to engage with one such 'virtual companion,' Microsoft's Zo. We examined the complete chat logs of 2000 anonymized people. We identified over a dozen motivations that people had for interacting with this software. Designers learned different ways to increase engagement. 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;GPT&#27169;&#22411;&#20013;&#20998;&#26512;&#21644;&#20462;&#25913;&#23454;&#20307;&#20851;&#31995;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20851;&#31995;&#36861;&#36394;&#25216;&#26415;&#65292;&#25105;&#20204;&#35782;&#21035;&#20102;MLP&#27169;&#22359;&#21644;&#27880;&#24847;&#26426;&#21046;&#22312;&#22788;&#29702;&#20851;&#31995;&#20449;&#24687;&#26041;&#38754;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#29305;&#24322;&#24615;&#21644;&#27867;&#21270;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#24179;&#34913;&#30340;&#25913;&#21892;&#12290;</title><link>http://arxiv.org/abs/2401.02976</link><description>&lt;p&gt;
&#22312;GPT&#27169;&#22411;&#20013;&#36861;&#36394;&#21644;&#32534;&#36753;&#20851;&#31995;&#20851;&#32852;
&lt;/p&gt;
&lt;p&gt;
Trace and Edit Relation Associations in GPT. (arXiv:2401.02976v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02976
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;GPT&#27169;&#22411;&#20013;&#20998;&#26512;&#21644;&#20462;&#25913;&#23454;&#20307;&#20851;&#31995;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20851;&#31995;&#36861;&#36394;&#25216;&#26415;&#65292;&#25105;&#20204;&#35782;&#21035;&#20102;MLP&#27169;&#22359;&#21644;&#27880;&#24847;&#26426;&#21046;&#22312;&#22788;&#29702;&#20851;&#31995;&#20449;&#24687;&#26041;&#38754;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#29305;&#24322;&#24615;&#21644;&#27867;&#21270;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#24179;&#34913;&#30340;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#26512;&#21644;&#20462;&#25913;GPT&#27169;&#22411;&#20013;&#30340;&#23454;&#20307;&#20851;&#31995;&#65292;&#19982;ROME&#30340;&#22522;&#20110;&#23454;&#20307;&#30340;&#26041;&#27861;&#19981;&#21516;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#20851;&#31995;&#36861;&#36394;&#25216;&#26415;&#65292;&#20197;&#20102;&#35299;&#35821;&#35328;&#27169;&#22411;&#35745;&#31639;&#23545;&#20851;&#31995;&#21028;&#26029;&#30340;&#24433;&#21709;&#12290;&#20351;&#29992;FewRel&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35782;&#21035;&#20102;MLP&#27169;&#22359;&#21644;&#27880;&#24847;&#26426;&#21046;&#22312;&#22788;&#29702;&#20851;&#31995;&#20449;&#24687;&#26041;&#38754;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#19978;&#19982;ROME&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#26174;&#31034;&#20986;&#22312;&#29305;&#24322;&#24615;&#21644;&#27867;&#21270;&#24615;&#26041;&#38754;&#30340;&#24179;&#34913;&#25913;&#21892;&#65292;&#31361;&#26174;&#20102;&#25805;&#32437;&#26089;&#26399;&#23618;&#27169;&#22359;&#20197;&#25552;&#39640;&#27169;&#22411;&#29702;&#35299;&#21644;&#20934;&#30830;&#24615;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study introduces a novel approach for analyzing and modifying entity relationships in GPT models, diverging from ROME's entity-focused methods. We develop a relation tracing technique to understand the influence of language model computations on relationship judgments. Using the FewRel dataset, we identify key roles of MLP modules and attention mechanisms in processing relationship information. Our method, tested against ROME on a new dataset, shows improved balance in specificity and generalization, underscoring the potential of manipulating early-layer modules for enhanced model understanding and accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26816;&#27979;&#22312;&#32447;&#20844;&#24320;&#23041;&#32961;&#30340;&#25928;&#21147;&#12290;&#36890;&#36807;&#23454;&#39564;&#21457;&#29616;&#65292;&#19981;&#21516;&#30340;LLMs&#22312;&#23041;&#32961;&#21644;&#38750;&#23041;&#32961;&#35782;&#21035;&#26041;&#38754;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#20854;&#20013;GPT-4&#30340;&#34920;&#29616;&#26368;&#20339;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;PaLM API&#30340;&#23450;&#20215;&#38750;&#24120;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;LLMs&#21487;&#20197;&#26377;&#25928;&#22320;&#22686;&#24378;&#20154;&#24037;&#20869;&#23481;&#23457;&#26597;&#65292;&#24110;&#21161;&#20943;&#36731;&#26032;&#20852;&#30340;&#22312;&#32447;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2401.02974</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26816;&#27979;&#22312;&#32447;&#20844;&#24320;&#23041;&#32961;&#30340;&#25928;&#21147;
&lt;/p&gt;
&lt;p&gt;
Efficacy of Utilizing Large Language Models to Detect Public Threat Posted Online. (arXiv:2401.02974v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02974
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26816;&#27979;&#22312;&#32447;&#20844;&#24320;&#23041;&#32961;&#30340;&#25928;&#21147;&#12290;&#36890;&#36807;&#23454;&#39564;&#21457;&#29616;&#65292;&#19981;&#21516;&#30340;LLMs&#22312;&#23041;&#32961;&#21644;&#38750;&#23041;&#32961;&#35782;&#21035;&#26041;&#38754;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#20854;&#20013;GPT-4&#30340;&#34920;&#29616;&#26368;&#20339;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;PaLM API&#30340;&#23450;&#20215;&#38750;&#24120;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;LLMs&#21487;&#20197;&#26377;&#25928;&#22320;&#22686;&#24378;&#20154;&#24037;&#20869;&#23481;&#23457;&#26597;&#65292;&#24110;&#21161;&#20943;&#36731;&#26032;&#20852;&#30340;&#22312;&#32447;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26816;&#27979;&#22312;&#32447;&#20844;&#24320;&#23041;&#32961;&#30340;&#25928;&#21147;&#12290;&#22312;&#23545;&#23041;&#32961; retoric &#30340;&#20256;&#25773;&#21644;&#26292;&#21147;&#39044;&#21578;&#30340;&#22686;&#38271;&#36234;&#26469;&#36234;&#25285;&#24551;&#30340;&#32972;&#26223;&#19979;&#65292;&#33258;&#21160;&#20869;&#23481;&#20998;&#26512;&#25216;&#26415;&#21487;&#20197;&#24110;&#21161;&#26089;&#26399;&#21457;&#29616;&#21644;&#22788;&#29702;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#33258;&#23450;&#20041;&#30340;&#25968;&#25454;&#25910;&#38598;&#24037;&#20855;&#65292;&#20174;&#19968;&#20010;&#28909;&#38376;&#30340;&#38889;&#22269;&#22312;&#32447;&#31038;&#21306;&#25910;&#38598;&#20102;500&#20010;&#38750;&#23041;&#32961;&#31034;&#20363;&#21644;20&#20010;&#23041;&#32961;&#31034;&#20363;&#30340;&#24086;&#23376;&#26631;&#39064;&#12290;&#21508;&#31181;LLMs (GPT-3.5, GPT-4, PaLM) &#34987;&#25552;&#31034;&#23558;&#21333;&#20010;&#24086;&#23376;&#20998;&#31867;&#20026;"&#23041;&#32961;"&#25110;"&#23433;&#20840;"&#12290;&#32479;&#35745;&#20998;&#26512;&#21457;&#29616;&#25152;&#26377;&#27169;&#22411;&#22312;&#23041;&#32961;&#21644;&#38750;&#23041;&#32961;&#35782;&#21035;&#26041;&#38754;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#36890;&#36807;&#21345;&#26041;&#25311;&#21512;&#24230;&#26816;&#39564;&#20063;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;GPT-4 &#30340;&#25972;&#20307;&#34920;&#29616;&#26368;&#22909;&#65292;&#38750;&#23041;&#32961;&#31934;&#24230;&#36798;&#21040;&#20102;97.9%&#65292;&#23041;&#32961;&#31934;&#24230;&#36798;&#21040;&#20102;100%&#12290;&#21487;&#34892;&#24615;&#20998;&#26512;&#36824;&#26174;&#31034;PaLM API&#30340;&#23450;&#20215;&#38750;&#24120;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;LLMs &#22312;&#35268;&#27169;&#21270;&#29615;&#22659;&#20013;&#21487;&#20197;&#26377;&#25928;&#22320;&#22686;&#24378;&#20154;&#24037;&#20869;&#23481;&#23457;&#26597;&#65292;&#20197;&#24110;&#21161;&#20943;&#36731;&#26032;&#20852;&#30340;&#22312;&#32447;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper examines the efficacy of utilizing large language models (LLMs) to detect public threats posted online. Amid rising concerns over the spread of threatening rhetoric and advance notices of violence, automated content analysis techniques may aid in early identification and moderation. Custom data collection tools were developed to amass post titles from a popular Korean online community, comprising 500 non-threat examples and 20 threats. Various LLMs (GPT-3.5, GPT-4, PaLM) were prompted to classify individual posts as either "threat" or "safe." Statistical analysis found all models demonstrated strong accuracy, passing chi-square goodness of fit tests for both threat and non-threat identification. GPT-4 performed best overall with 97.9% non-threat and 100% threat accuracy. Affordability analysis also showed PaLM API pricing as highly cost-efficient. The findings indicate LLMs can effectively augment human content moderation at scale to help mitigate emerging online risks. Howe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#22522;&#20110;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25991;&#26412;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#22788;&#29702;&#20219;&#21153;&#26469;&#25552;&#39640;&#26368;&#26032;&#25216;&#26415;&#65292;&#24182;&#22312;&#21322;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#23545;&#20004;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2401.02971</link><description>&lt;p&gt;
&#25991;&#26412;&#20013;&#30340;&#28145;&#24230;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Deep Anomaly Detection in Text. (arXiv:2401.02971v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02971
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#22522;&#20110;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25991;&#26412;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#22788;&#29702;&#20219;&#21153;&#26469;&#25552;&#39640;&#26368;&#26032;&#25216;&#26415;&#65292;&#24182;&#22312;&#21322;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#23545;&#20004;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#35832;&#22914;&#22534;&#21472;&#33258;&#21160;&#32534;&#30721;&#22120;&#12289;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#31561;&#26041;&#27861;&#22823;&#22823;&#25913;&#36827;&#20102;&#26368;&#26032;&#25216;&#26415;&#12290;&#20854;&#20182;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#36866;&#24403;&#30340;&#26680;&#20989;&#25968;&#26469;&#22686;&#24378;&#32463;&#20856;&#27169;&#22411;&#65288;&#22914;&#21333;&#31867;&#25903;&#25345;&#21521;&#37327;&#26426;&#65289;&#12290;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#22312;&#24322;&#24120;&#26816;&#27979;&#39046;&#22495;&#20013;&#30340;&#20195;&#34920;&#32447;&#23398;&#20064;&#26041;&#38754;&#30340;&#26368;&#26032;&#21457;&#23637;&#35777;&#26126;&#22312;&#36825;&#19968;&#32972;&#26223;&#19979;&#38750;&#24120;&#26377;&#30410;&#12290;&#21463;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#20013;&#20351;&#29992;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#30340;&#36827;&#23637;&#21551;&#21457;&#65292;&#26412;&#35770;&#25991;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#20026;&#25991;&#26412;&#35821;&#26009;&#24211;&#37327;&#36523;&#23450;&#21046;&#30340;&#39044;&#22788;&#29702;&#20219;&#21153;&#26469;&#24320;&#21457;&#19968;&#31181;&#26816;&#27979;&#24322;&#24120;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;20Newsgroups&#21644;AG News&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#22823;&#22823;&#25913;&#36827;&#20102;&#21322;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#30340;&#24322;&#24120;&#26816;&#27979;&#26368;&#26032;&#25216;&#26415;&#65292;&#20174;&#32780;&#35777;&#26126;&#20102;&#33258;&#25105;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#22120;&#22312;&#33258;&#28982;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep anomaly detection methods have become increasingly popular in recent years, with methods like Stacked Autoencoders, Variational Autoencoders, and Generative Adversarial Networks greatly improving the state-of-the-art. Other methods rely on augmenting classical models (such as the One-Class Support Vector Machine), by learning an appropriate kernel function using Neural Networks. Recent developments in representation learning by self-supervision are proving to be very beneficial in the context of anomaly detection. Inspired by the advancements in anomaly detection using self-supervised learning in the field of computer vision, this thesis aims to develop a method for detecting anomalies by exploiting pretext tasks tailored for text corpora. This approach greatly improves the state-of-the-art on two datasets, 20Newsgroups, and AG News, for both semi-supervised and unsupervised anomaly detection, thus proving the potential for self-supervised anomaly detectors in the field of natural
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22788;&#29702;&#35270;&#32593;&#33180;&#22270;&#20687;&#20013;&#34880;&#31649;&#23450;&#20301;&#30340;&#20004;&#31181;&#33258;&#21160;&#26041;&#27861;&#65292;&#36890;&#36807;&#20943;&#23569;&#26126;&#20142;&#30149;&#21464;&#30340;&#24433;&#21709;&#24182;&#20351;&#29992;&#22810;&#23610;&#24230;&#32447;&#31639;&#23376;&#23450;&#20301;&#34880;&#31649;&#32467;&#26500;&#65292;&#20197;&#27492;&#25552;&#39640;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.02962</link><description>&lt;p&gt;
&#35270;&#32593;&#33180;&#22270;&#20687;&#20013;&#34880;&#31649;&#30340;&#33258;&#21160;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Automated Localization of Blood Vessels in Retinal Images. (arXiv:2401.02962v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02962
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22788;&#29702;&#35270;&#32593;&#33180;&#22270;&#20687;&#20013;&#34880;&#31649;&#23450;&#20301;&#30340;&#20004;&#31181;&#33258;&#21160;&#26041;&#27861;&#65292;&#36890;&#36807;&#20943;&#23569;&#26126;&#20142;&#30149;&#21464;&#30340;&#24433;&#21709;&#24182;&#20351;&#29992;&#22810;&#23610;&#24230;&#32447;&#31639;&#23376;&#23450;&#20301;&#34880;&#31649;&#32467;&#26500;&#65292;&#20197;&#27492;&#25552;&#39640;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34880;&#31649;&#32467;&#26500;&#26159;&#35270;&#32593;&#33180;&#20013;&#26368;&#37325;&#35201;&#30340;&#37096;&#20998;&#20043;&#19968;&#65292;&#21307;&#29983;&#21487;&#20197;&#36890;&#36807;&#20998;&#26512;&#20854;&#29305;&#24449;&#26469;&#26816;&#27979;&#35768;&#22810;&#30142;&#30149;&#12290;&#35270;&#32593;&#33180;&#22270;&#20687;&#20013;&#34880;&#31649;&#30340;&#23450;&#20301;&#26159;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#36807;&#31243;&#12290;&#23384;&#22312;&#26126;&#20142;&#30149;&#21464;&#21644;&#40657;&#26263;&#30149;&#21464;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#20010;&#36807;&#31243;&#20063;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#26412;&#35770;&#25991;&#20998;&#26512;&#20102;&#20004;&#31181;&#33258;&#21160;&#23450;&#20301;&#34880;&#31649;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#20581;&#24247;&#21644;&#19981;&#20581;&#24247;&#65288;&#30149;&#29702;&#24615;&#65289;&#30340;&#35270;&#32593;&#33180;&#22270;&#20687;&#12290;&#27599;&#31181;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#27493;&#39588;&#65292;&#20854;&#20013;&#31532;&#20108;&#20010;&#27493;&#39588;&#22312;&#20004;&#31181;&#26041;&#27861;&#20013;&#26159;&#30456;&#21516;&#30340;&#12290;&#22312;&#31532;&#19968;&#27493;&#20013;&#65292;&#20351;&#29992;&#31639;&#27861;&#20943;&#23569;&#26126;&#20142;&#30149;&#21464;&#30340;&#24433;&#21709;&#12290;&#22312;&#26041;&#27861;1&#20013;&#65292;&#35813;&#31639;&#27861;&#22522;&#20110;K-&#22343;&#20540;&#20998;&#21106;&#65292;&#22312;&#26041;&#27861;2&#20013;&#65292;&#23427;&#22522;&#20110;&#27491;&#21017;&#21270;&#36807;&#31243;&#12290;&#22312;&#20004;&#31181;&#26041;&#27861;&#30340;&#31532;&#20108;&#27493;&#20013;&#65292;&#20351;&#29992;&#22810;&#23610;&#24230;&#32447;&#31639;&#23376;&#23450;&#20301;&#32447;&#29366;&#34880;&#31649;&#32467;&#26500;&#65292;&#24182;&#24573;&#30053;&#36890;&#24120;&#34987;&#35748;&#20026;&#20855;&#26377;&#19981;&#35268;&#21017;&#27169;&#24335;&#30340;&#40657;&#26263;&#30149;&#21464;&#12290;&#20171;&#32461;&#20102;&#26041;&#27861;&#21518;&#65292;&#26681;&#25454;&#26041;&#27861;&#30340;&#20171;&#32461;&#65292;&#19968;&#31181;&#19968;&#31181;&#35814;&#32454;&#30340;&#35780;&#20272;&#26041;&#27861;indicating&#20854;&#22312;&#34880;&#31649;&#23450;&#20301;&#19978;&#30340;&#34920;&#29616;&#22909;&#22351;.
&lt;/p&gt;
&lt;p&gt;
Vessel structure is one of the most important parts of the retina which physicians can detect many diseases by analysing its features. Localization of blood vessels in retina images is an important process in medical image analysis. This process is also more challenging with the presence of bright and dark lesions. In this thesis, two automated vessel localization methods to handle both healthy and unhealthy (pathological) retina images are analyzed. Each method consists of two major steps and the second step is the same in the two methods. In the first step, an algorithm is used to decrease the effect of bright lesions. In Method 1, this algorithm is based on K- Means segmentation, and in Method 2, it is based on a regularization procedure. In the second step of both methods, a multi-scale line operator is used to localize the line-shaped vascular structures and ignore the dark lesions which are generally assumed to have irregular patterns. After the introduction of the methods, a det
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#24863;&#30693;&#32852;&#37030;&#20316;&#19994;&#35843;&#24230;&#65288;FairFedJS&#65289;&#26041;&#27861;&#65292;&#20197;&#30830;&#20445;&#23558;&#39640;&#38656;&#27714;&#30340;FL&#23458;&#25143;&#31471;&#25968;&#25454;&#38598;&#20844;&#24179;&#20998;&#37197;&#32473;&#38656;&#35201;&#23427;&#20204;&#30340;FL&#20316;&#19994;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2401.02740</link><description>&lt;p&gt;
&#20026;&#22810;&#20219;&#21153;&#32852;&#37030;&#23398;&#20064;&#25552;&#20379;&#20844;&#24179;&#24615;&#24863;&#30693;&#30340;&#20316;&#19994;&#35843;&#24230;
&lt;/p&gt;
&lt;p&gt;
Fairness-Aware Job Scheduling for Multi-Job Federated Learning. (arXiv:2401.02740v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02740
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#24863;&#30693;&#32852;&#37030;&#20316;&#19994;&#35843;&#24230;&#65288;FairFedJS&#65289;&#26041;&#27861;&#65292;&#20197;&#30830;&#20445;&#23558;&#39640;&#38656;&#27714;&#30340;FL&#23458;&#25143;&#31471;&#25968;&#25454;&#38598;&#20844;&#24179;&#20998;&#37197;&#32473;&#38656;&#35201;&#23427;&#20204;&#30340;FL&#20316;&#19994;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20351;&#22810;&#20010;&#25968;&#25454;&#25152;&#26377;&#32773;&#65288;&#21363;FL&#23458;&#25143;&#31471;&#65289;&#33021;&#22815;&#22312;&#19981;&#27844;&#38706;&#25935;&#24863;&#31169;&#20154;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20849;&#21516;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#29616;&#26377;&#30340;FL&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#22404;&#26029;&#22330;&#26223;&#65292;&#22312;&#35813;&#22330;&#26223;&#20013;&#65292;&#21333;&#20010;FL&#26381;&#21153;&#22120;&#22312;&#27599;&#36718;&#35757;&#32451;&#20013;&#36873;&#25321;&#19968;&#37096;&#20998;FL&#23458;&#25143;&#31471;&#26469;&#26356;&#26032;&#20854;&#26412;&#22320;&#27169;&#22411;&#12290;&#23454;&#38469;&#19978;&#65292;&#21487;&#33021;&#20250;&#26377;&#22810;&#20010;FL&#26381;&#21153;&#22120;&#21516;&#26102;&#23581;&#35797;&#20174;&#21516;&#19968;&#20010;&#27744;&#20013;&#36873;&#25321;&#23458;&#25143;&#31471;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39318;&#21019;&#30340;&#20844;&#24179;&#24863;&#30693;&#32852;&#37030;&#20316;&#19994;&#35843;&#24230;&#65288;FairFedJS&#65289;&#26041;&#27861;&#26469;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#12290;&#22522;&#20110;Lyapunov&#20248;&#21270;&#65292;&#23427;&#36890;&#36807;&#21516;&#26102;&#32771;&#34385;&#24403;&#21069;&#38656;&#27714;&#21644;&#20316;&#19994;&#20184;&#27454;&#20986;&#20215;&#65292;&#30830;&#20445;&#23558;&#39640;&#38656;&#27714;&#30340;FL&#23458;&#25143;&#31471;&#25968;&#25454;&#38598;&#20844;&#24179;&#20998;&#37197;&#32473;&#38656;&#35201;&#23427;&#20204;&#30340;FL&#20316;&#19994;&#65292;&#20197;&#38450;&#27490;&#31561;&#24453;&#26102;&#38388;&#36807;&#38271;&#12290;&#22522;&#20110;&#20004;&#20010;&#25968;&#25454;&#38598;&#23545;FairFedJS&#19982;&#22235;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26174;&#33879;&#20248;&#21183;&#12290;&#23427;&#22312;&#24179;&#22343;&#19978;&#20987;&#36133;&#20102;&#26368;&#20339;&#22522;&#20934;&#32447;31.9%&#21644;1.0%&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) enables multiple data owners (a.k.a. FL clients) to collaboratively train machine learning models without disclosing sensitive private data. Existing FL research mostly focuses on the monopoly scenario in which a single FL server selects a subset of FL clients to update their local models in each round of training. In practice, there can be multiple FL servers simultaneously trying to select clients from the same pool. In this paper, we propose a first-of-its-kind Fairness-aware Federated Job Scheduling (FairFedJS) approach to bridge this gap. Based on Lyapunov optimization, it ensures fair allocation of high-demand FL client datasets to FL jobs in need of them, by jointly considering the current demand and the job payment bids, in order to prevent prolonged waiting. Extensive experiments comparing FairFedJS against four state-of-the-art approaches on two datasets demonstrate its significant advantages. It outperforms the best baseline by 31.9% and 1.0% on avera
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#31232;&#30095;&#21046;&#20316;&#30340;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#19987;&#23478;&#28151;&#21512;&#24335;&#26550;&#26500;&#23558;&#23494;&#38598;&#27169;&#22411;&#36716;&#25442;&#20026;&#31232;&#30095;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#22312;&#27169;&#22411;&#23481;&#37327;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#21644;&#27867;&#21270;&#33021;&#21147;&#22686;&#24378;&#12290;</title><link>http://arxiv.org/abs/2401.02731</link><description>&lt;p&gt;
&#21442;&#25968;&#39640;&#25928;&#31232;&#30095;&#21046;&#20316;&#65306;&#20174;&#23494;&#38598;&#22411;&#21040;&#19987;&#23478;&#28151;&#21512;&#24335;&#29992;&#20110;&#36890;&#29992;&#20219;&#21153;&#30340;&#25351;&#20196;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Parameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for Instruction Tuning on General Tasks. (arXiv:2401.02731v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#31232;&#30095;&#21046;&#20316;&#30340;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#19987;&#23478;&#28151;&#21512;&#24335;&#26550;&#26500;&#23558;&#23494;&#38598;&#27169;&#22411;&#36716;&#25442;&#20026;&#31232;&#30095;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#22312;&#27169;&#22411;&#23481;&#37327;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#21644;&#27867;&#21270;&#33021;&#21147;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#36890;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#30456;&#24403;&#30340;&#29087;&#32451;&#31243;&#24230;&#12290;&#25351;&#20196;&#35843;&#25972;&#20316;&#20026;&#19968;&#31181;&#25104;&#21151;&#30340;&#33539;&#20363;&#65292;&#22686;&#24378;&#20102;LLMs&#36981;&#24490;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27169;&#22411;&#23481;&#37327;&#38480;&#21046;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#32463;&#24120;&#36935;&#21040;&#24615;&#33021;&#38480;&#21046;&#12290;&#22312;&#25351;&#20196;&#35843;&#25972;&#38454;&#27573;&#25193;&#23637;&#27169;&#22411;&#23481;&#37327;&#38754;&#20020;&#30528;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21442;&#25968;&#39640;&#25928;&#31232;&#30095;&#21046;&#20316;(PESC)&#65292;&#23427;&#20351;&#29992;&#19987;&#23478;&#28151;&#21512;&#24335;(MoE)&#26550;&#26500;&#23558;&#23494;&#38598;&#27169;&#22411;&#36716;&#25442;&#20026;&#31232;&#30095;&#27169;&#22411;&#12290;PESC&#23558;&#36866;&#37197;&#22120;&#38598;&#25104;&#21040;&#31232;&#30095;&#27169;&#22411;&#30340;MoE&#23618;&#20013;&#65292;&#21306;&#20998;&#19981;&#21516;&#30340;&#19987;&#23478;&#32780;&#19981;&#25913;&#21464;&#36825;&#20123;&#23618;&#20013;&#30340;&#20010;&#20307;&#26435;&#37325;&#12290;&#36825;&#31181;&#26041;&#27861;&#26174;&#33879;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#21644;GPU&#20869;&#23384;&#38656;&#27714;&#65292;&#36890;&#36807;&#26368;&#23567;&#30340;&#22686;&#21152;&#23454;&#29616;&#20102;&#27169;&#22411;&#23481;&#37327;&#30340;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated considerable proficiency in general natural language processing (NLP) tasks. Instruction tuning, a successful paradigm, enhances the ability of LLMs to follow natural language instructions and exhibit robust generalization across a wide range of tasks. However, these models often encounter performance limitations across multiple tasks due to constrained model capacity. Expanding this capacity during the instruction tuning phase poses significant challenges. To address this issue, we introduce a novel approach, Parameter-Efficient Sparsity Crafting (PESC), which transitions dense models to sparse models using a Mixture of Experts (MoE) architecture. PESC integrates adapters into the MoE layers of sparse models, differentiating experts without altering the individual weights within these layers. This method significantly reduces computational costs and GPU memory requirements, facilitating model capacity expansion through a minimal increase 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35843;&#26597;&#20102;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20013;&#35745;&#31639;&#20998;&#27495;&#23545;&#23398;&#26415;&#36129;&#29486;&#21644;&#23457;&#26597;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#35745;&#31639;&#20998;&#27495;&#23548;&#33268;&#23398;&#26415;&#30028;&#22312;&#35745;&#31639;&#23494;&#38598;&#22411;&#30740;&#31350;&#20027;&#39064;&#20013;&#30340;&#24433;&#21709;&#21147;&#38477;&#20302;&#65292;&#24182;&#19988;&#23398;&#26415;&#30740;&#31350;&#36235;&#21521;&#20110;&#20351;&#29992;&#24037;&#19994;&#30028;&#24320;&#21457;&#30340;&#24320;&#28304;&#12289;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#24314;&#35758;&#36890;&#36807;&#22269;&#23478;&#25903;&#25345;&#30340;&#35745;&#31639;&#22522;&#30784;&#35774;&#26045;&#19982;&#24320;&#25918;&#31185;&#23398;&#20513;&#35758;&#30340;&#32467;&#21512;&#26469;&#25299;&#23637;&#23398;&#26415;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2401.02452</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#35745;&#31639;&#20998;&#27495;&#65306;&#23545;&#23398;&#26415;&#36129;&#29486;&#21644;&#23457;&#26597;&#30340;&#23041;&#32961;&#65311;
&lt;/p&gt;
&lt;p&gt;
The Compute Divide in Machine Learning: A Threat to Academic Contribution and Scrutiny?. (arXiv:2401.02452v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02452
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35843;&#26597;&#20102;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20013;&#35745;&#31639;&#20998;&#27495;&#23545;&#23398;&#26415;&#36129;&#29486;&#21644;&#23457;&#26597;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#35745;&#31639;&#20998;&#27495;&#23548;&#33268;&#23398;&#26415;&#30028;&#22312;&#35745;&#31639;&#23494;&#38598;&#22411;&#30740;&#31350;&#20027;&#39064;&#20013;&#30340;&#24433;&#21709;&#21147;&#38477;&#20302;&#65292;&#24182;&#19988;&#23398;&#26415;&#30740;&#31350;&#36235;&#21521;&#20110;&#20351;&#29992;&#24037;&#19994;&#30028;&#24320;&#21457;&#30340;&#24320;&#28304;&#12289;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#24314;&#35758;&#36890;&#36807;&#22269;&#23478;&#25903;&#25345;&#30340;&#35745;&#31639;&#22522;&#30784;&#35774;&#26045;&#19982;&#24320;&#25918;&#31185;&#23398;&#20513;&#35758;&#30340;&#32467;&#21512;&#26469;&#25299;&#23637;&#23398;&#26415;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#19994;&#21644;&#23398;&#26415;&#20154;&#24037;&#26234;&#33021;&#23454;&#39564;&#23460;&#22312;&#20351;&#29992;&#35745;&#31639;&#36164;&#28304;&#26041;&#38754;&#23384;&#22312;&#26126;&#26174;&#24046;&#24322;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#39033;&#25968;&#25454;&#39537;&#21160;&#30340;&#35843;&#26597;&#65292;&#20197;&#20102;&#35299;&#35745;&#31639;&#20998;&#27495;&#23545;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#35745;&#31639;&#20998;&#27495;&#19982;&#35745;&#31639;&#23494;&#38598;&#22411;&#30740;&#31350;&#20027;&#39064;&#20013;&#23398;&#26415;&#29420;&#31435;&#30740;&#31350;&#22242;&#38431;&#30340;&#34920;&#31034;&#20943;&#23569;&#26377;&#20851;&#65292;&#29305;&#21035;&#26159;&#22522;&#30784;&#27169;&#22411;&#26041;&#38754;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#23398;&#26415;&#30028;&#22312;&#25512;&#36827;&#30456;&#20851;&#25216;&#26415;&#12289;&#25552;&#20379;&#20851;&#38190;&#30340;&#35780;&#20272;&#21644;&#23457;&#26597;&#20197;&#21450;&#22312;&#36825;&#20123;&#27169;&#22411;&#30340;&#20256;&#25773;&#26041;&#38754;&#21487;&#33021;&#21457;&#25381;&#30340;&#20316;&#29992;&#23558;&#36739;&#23567;&#12290;&#19982;&#30740;&#31350;&#37325;&#24515;&#30340;&#36825;&#31181;&#21464;&#21270;&#21516;&#26102;&#65292;&#23398;&#26415;&#30740;&#31350;&#22312;&#20513;&#23548;&#24037;&#19994;&#30028;&#24320;&#21457;&#30340;&#24320;&#28304;&#12289;&#39044;&#35757;&#32451;&#27169;&#22411;&#26041;&#38754;&#20986;&#29616;&#20102;&#26126;&#26174;&#36716;&#21464;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#36235;&#21183;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#23588;&#20854;&#26159;&#23545;&#24433;&#21709;&#21147;&#27169;&#22411;&#30340;&#23457;&#26597;&#20943;&#23569;&#65292;&#25105;&#20204;&#24314;&#35758;&#37319;&#21462;&#19968;&#20123;&#26041;&#27861;&#26469;&#26377;&#38024;&#23545;&#24615;&#22320;&#25299;&#23637;&#23398;&#26415;&#35265;&#35299;&#12290;&#36825;&#20123;&#26041;&#27861;&#21253;&#25324;&#22269;&#23478;&#25903;&#25345;&#30340;&#35745;&#31639;&#22522;&#30784;&#35774;&#26045;&#19982;&#24320;&#25918;&#31185;&#23398;&#20513;&#35758;&#30340;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
There are pronounced differences in the extent to which industrial and academic AI labs use computing resources. We provide a data-driven survey of the role of the compute divide in shaping machine learning research. We show that a compute divide has coincided with a reduced representation of academic-only research teams in compute intensive research topics, especially foundation models. We argue that, academia will likely play a smaller role in advancing the associated techniques, providing critical evaluation and scrutiny, and in the diffusion of such models. Concurrent with this change in research focus, there is a noticeable shift in academic research towards embracing open source, pre-trained models developed within the industry. To address the challenges arising from this trend, especially reduced scrutiny of influential models, we recommend approaches aimed at thoughtfully expanding academic insights. Nationally-sponsored computing infrastructure coupled with open science initia
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#20020;&#24202;&#35821;&#20041;&#25628;&#32034;&#26041;&#38754;&#65292;&#36890;&#29992;&#23884;&#20837;&#27169;&#22411;&#27604;&#19987;&#19994;&#23884;&#20837;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#65292;&#36825;&#34920;&#26126;&#29616;&#26377;&#30340;&#20020;&#24202;&#19987;&#19994;&#21270;&#27169;&#22411;&#23545;&#36755;&#20837;&#30340;&#24494;&#23567;&#21464;&#21270;&#26356;&#25935;&#24863;&#12290;</title><link>http://arxiv.org/abs/2401.01943</link><description>&lt;p&gt;
&#36890;&#29992;&#23884;&#20837;&#27169;&#22411;&#22312;&#30701;&#35821;&#22659;&#20020;&#24202;&#35821;&#20041;&#25628;&#32034;&#26041;&#38754;&#34920;&#29616;&#27604;&#19987;&#19994;&#23884;&#20837;&#27169;&#22411;&#26356;&#22909;
&lt;/p&gt;
&lt;p&gt;
Generalist embedding models are better at short-context clinical semantic search than specialized embedding models. (arXiv:2401.01943v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01943
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#20020;&#24202;&#35821;&#20041;&#25628;&#32034;&#26041;&#38754;&#65292;&#36890;&#29992;&#23884;&#20837;&#27169;&#22411;&#27604;&#19987;&#19994;&#23884;&#20837;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#65292;&#36825;&#34920;&#26126;&#29616;&#26377;&#30340;&#20020;&#24202;&#19987;&#19994;&#21270;&#27169;&#22411;&#23545;&#36755;&#20837;&#30340;&#24494;&#23567;&#21464;&#21270;&#26356;&#25935;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24037;&#20855;&#21644;&#35299;&#20915;&#26041;&#26696;&#22312;&#21307;&#30103;&#39046;&#22495;&#30340;&#24212;&#29992;&#26085;&#30410;&#22686;&#22810;&#65292;&#36825;&#24050;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#36235;&#21183;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#20010;&#39640;&#24230;&#20851;&#38190;&#21644;&#25935;&#24863;&#30340;&#39046;&#22495;&#20013;&#20351;&#29992;&#23427;&#20204;&#23545;&#20854;&#31283;&#20581;&#24615;&#20135;&#29983;&#20102;&#37325;&#35201;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#23545;&#36755;&#20837;&#21464;&#21270;&#21644;&#29983;&#25104;&#30340;&#36755;&#20986;&#30340;&#21487;&#38752;&#24615;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#26500;&#24314;&#22522;&#20110;ICD-10-CM&#20195;&#30721;&#25551;&#36848;&#30340;&#25991;&#26412;&#25968;&#25454;&#38598;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#35813;&#25968;&#25454;&#38598;&#24191;&#27867;&#24212;&#29992;&#20110;&#32654;&#22269;&#21307;&#38498;&#65292;&#21253;&#21547;&#35768;&#22810;&#20020;&#24202;&#26415;&#35821;&#21450;&#20854;&#26131;&#20110;&#22797;&#21046;&#30340;&#25913;&#20889;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;&#35821;&#20041;&#25628;&#32034;&#20219;&#21153;&#20013;&#23545;&#29616;&#26377;&#30340;&#36890;&#29992;&#25110;&#20020;&#24202;&#19987;&#19994;&#21270;&#30340;&#23884;&#20837;&#27169;&#22411;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#30446;&#26631;&#26159;&#27491;&#30830;&#21305;&#37197;&#25913;&#20889;&#30340;&#25991;&#26412;&#19982;&#21407;&#22987;&#25551;&#36848;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#29992;&#27169;&#22411;&#27604;&#20020;&#24202;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#65292;&#36825;&#34920;&#26126;&#29616;&#26377;&#30340;&#20020;&#24202;&#19987;&#19994;&#21270;&#27169;&#22411;&#23545;&#36755;&#20837;&#30340;&#24494;&#23567;&#21464;&#21270;&#26356;&#25935;&#24863;&#65292;&#20174;&#32780;&#20351;&#20854;&#22256;&#24785;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing use of tools and solutions based on Large Language Models (LLMs) for various tasks in the medical domain has become a prominent trend. Their use in this highly critical and sensitive domain has thus raised important questions about their robustness, especially in response to variations in input, and the reliability of the generated outputs. This study addresses these questions by constructing a textual dataset based on the ICD-10-CM code descriptions, widely used in US hospitals and containing many clinical terms, and their easily reproducible rephrasing. We then benchmarked existing embedding models, either generalist or specialized in the clinical domain, in a semantic search task where the goal was to correctly match the rephrased text to the original description. Our results showed that generalist models performed better than clinical models, suggesting that existing clinical specialized models are more sensitive to small changes in input that confuse them. The highl
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;AIGCBench&#65292;&#19968;&#20010;&#20840;&#38754;&#35780;&#20272;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#21040;&#35270;&#39057;&#20869;&#23481;&#30340;&#22522;&#20934;&#12290;&#36890;&#36807;&#24341;&#20837;&#22810;&#26679;&#21270;&#19988;&#24320;&#25918;&#39046;&#22495;&#30340;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#38598;&#65292;AIGCBench&#35299;&#20915;&#20102;&#29616;&#26377;&#22522;&#20934;&#30340;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#24314;&#31435;&#32479;&#19968;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#35813;&#22522;&#20934;&#21253;&#25324;11&#20010;&#24230;&#37327;&#25351;&#26631;&#65292;&#28085;&#30422;&#25511;&#21046;&#35270;&#39057;&#23545;&#40784;&#12289;&#21160;&#24577;&#25928;&#26524;&#12289;&#26102;&#38388;&#19968;&#33268;&#24615;&#21644;&#35270;&#39057;&#36136;&#37327;&#31561;&#26041;&#38754;&#12290;</title><link>http://arxiv.org/abs/2401.01651</link><description>&lt;p&gt;
AIGCBench&#65306;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#21040;&#35270;&#39057;&#20869;&#23481;&#30340;&#20840;&#38754;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
AIGCBench: Comprehensive Evaluation of Image-to-Video Content Generated by AI. (arXiv:2401.01651v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01651
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;AIGCBench&#65292;&#19968;&#20010;&#20840;&#38754;&#35780;&#20272;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#21040;&#35270;&#39057;&#20869;&#23481;&#30340;&#22522;&#20934;&#12290;&#36890;&#36807;&#24341;&#20837;&#22810;&#26679;&#21270;&#19988;&#24320;&#25918;&#39046;&#22495;&#30340;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#38598;&#65292;AIGCBench&#35299;&#20915;&#20102;&#29616;&#26377;&#22522;&#20934;&#30340;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#24314;&#31435;&#32479;&#19968;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#35813;&#22522;&#20934;&#21253;&#25324;11&#20010;&#24230;&#37327;&#25351;&#26631;&#65292;&#28085;&#30422;&#25511;&#21046;&#35270;&#39057;&#23545;&#40784;&#12289;&#21160;&#24577;&#25928;&#26524;&#12289;&#26102;&#38388;&#19968;&#33268;&#24615;&#21644;&#35270;&#39057;&#36136;&#37327;&#31561;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20869;&#23481;&#65288;AIGC&#65289;&#39046;&#22495;&#27491;&#22312;&#36805;&#36895;&#21457;&#23637;&#65292;&#23588;&#20854;&#26159;&#35270;&#39057;&#29983;&#25104;&#12290;&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;AIGCBench&#65292;&#36825;&#26159;&#19968;&#31181;&#24320;&#21019;&#24615;&#30340;&#32508;&#21512;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#22522;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;&#21508;&#31181;&#35270;&#39057;&#29983;&#25104;&#20219;&#21153;&#65292;&#20027;&#35201;&#20851;&#27880;&#22270;&#20687;&#21040;&#35270;&#39057;&#65288;I2V&#65289;&#29983;&#25104;&#12290;AIGCBench&#35299;&#20915;&#20102;&#29616;&#26377;&#22522;&#20934;&#30340;&#23616;&#38480;&#24615;&#65292;&#36825;&#20123;&#22522;&#20934;&#32570;&#20047;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#21253;&#25324;&#19968;&#20010;&#22810;&#26679;&#21270;&#19988;&#24320;&#25918;&#39046;&#22495;&#30340;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#19981;&#21516;&#30340;&#26368;&#20808;&#36827;&#31639;&#27861;&#22312;&#30456;&#31561;&#26465;&#20214;&#19979;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25991;&#26412;&#32452;&#21512;&#22120;&#21644;GPT-4&#26469;&#21019;&#24314;&#20016;&#23500;&#30340;&#25991;&#26412;&#25552;&#31034;&#65292;&#28982;&#21518;&#20351;&#29992;&#20808;&#36827;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#29983;&#25104;&#22270;&#20687;&#12290;&#20026;&#20102;&#24314;&#31435;&#19968;&#20010;&#32479;&#19968;&#30340;&#35270;&#39057;&#29983;&#25104;&#20219;&#21153;&#35780;&#20272;&#26694;&#26550;&#65292;&#25105;&#20204;&#30340;&#22522;&#20934;&#21253;&#25324;11&#20010;&#24230;&#37327;&#25351;&#26631;&#65292;&#28085;&#30422;&#22235;&#20010;&#32500;&#24230;&#65292;&#20197;&#35780;&#20272;&#31639;&#27861;&#24615;&#33021;&#12290;&#36825;&#20123;&#32500;&#24230;&#26159;&#25511;&#21046;&#35270;&#39057;&#23545;&#40784;&#65292;&#21160;&#24577;&#25928;&#26524;&#65292;&#26102;&#38388;&#19968;&#33268;&#24615;&#21644;&#35270;&#39057;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The burgeoning field of Artificial Intelligence Generated Content (AIGC) is witnessing rapid advancements, particularly in video generation. This paper introduces AIGCBench, a pioneering comprehensive and scalable benchmark designed to evaluate a variety of video generation tasks, with a primary focus on Image-to-Video (I2V) generation. AIGCBench tackles the limitations of existing benchmarks, which suffer from a lack of diverse datasets, by including a varied and open-domain image-text dataset that evaluates different state-of-the-art algorithms under equivalent conditions. We employ a novel text combiner and GPT-4 to create rich text prompts, which are then used to generate images via advanced Text-to-Image models. To establish a unified evaluation framework for video generation tasks, our benchmark includes 11 metrics spanning four dimensions to assess algorithm performance. These dimensions are control-video alignment, motion effects, temporal consistency, and video quality. These 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22522;&#20110;&#36855;&#22240;&#30340;&#31038;&#20132;&#34384;&#24453;&#30740;&#31350;&#23545;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#23433;&#20840;&#27934;&#23519;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#32508;&#21512;&#30340;&#36855;&#22240;&#22522;&#20934;&#27979;&#35797;&#38598;GOAT-Bench&#65292;&#35780;&#20272;&#21508;&#31181;LMMs&#22312;&#35782;&#21035;&#21644;&#22238;&#24212;&#36855;&#22240;&#20013;&#20307;&#29616;&#30340;&#24494;&#22937;&#31038;&#20132;&#34384;&#24453;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.01523</link><description>&lt;p&gt;
GOAT-Bench: &#36890;&#36807;&#22522;&#20110;&#36855;&#22240;&#30340;&#31038;&#20132;&#34384;&#24453;&#30740;&#31350;&#23545;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#23433;&#20840;&#27934;&#23519;
&lt;/p&gt;
&lt;p&gt;
GOAT-Bench: Safety Insights to Large Multimodal Models through Meme-Based Social Abuse. (arXiv:2401.01523v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01523
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#36855;&#22240;&#30340;&#31038;&#20132;&#34384;&#24453;&#30740;&#31350;&#23545;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#23433;&#20840;&#27934;&#23519;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#32508;&#21512;&#30340;&#36855;&#22240;&#22522;&#20934;&#27979;&#35797;&#38598;GOAT-Bench&#65292;&#35780;&#20272;&#21508;&#31181;LMMs&#22312;&#35782;&#21035;&#21644;&#22238;&#24212;&#36855;&#22240;&#20013;&#20307;&#29616;&#30340;&#24494;&#22937;&#31038;&#20132;&#34384;&#24453;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#30340;&#25351;&#25968;&#32423;&#22686;&#38271;&#28145;&#21051;&#25913;&#21464;&#20102;&#20449;&#24687;&#30340;&#21019;&#36896;&#12289;&#20256;&#25773;&#21644;&#21560;&#25910;&#26041;&#24335;&#65292;&#22312;&#25968;&#23383;&#26102;&#20195;&#20135;&#29983;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#24433;&#21709;&#12290;&#36951;&#25022;&#30340;&#26159;&#65292;&#36825;&#20010;&#29190;&#28856;&#20063;&#23548;&#33268;&#20102;&#32593;&#32476;&#36855;&#22240;&#30340;&#28389;&#29992;&#25968;&#37327;&#26174;&#33879;&#22686;&#21152;&#12290;&#35780;&#20272;&#36855;&#22240;&#30340;&#36127;&#38754;&#24433;&#21709;&#26159;&#30456;&#24403;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#24120;&#20855;&#26377;&#24494;&#22937;&#21644;&#38544;&#26214;&#30340;&#21547;&#20041;&#65292;&#36825;&#20123;&#21547;&#20041;&#19981;&#33021;&#30452;&#25509;&#36890;&#36807;&#26174;&#24615;&#30340;&#25991;&#26412;&#21644;&#22270;&#20687;&#20256;&#36798;&#20986;&#26469;&#12290;&#37492;&#20110;&#27492;&#65292;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;(LMMs)&#20316;&#20026;&#22788;&#29702;&#22810;&#26679;&#21270;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;&#21331;&#36234;&#33021;&#21147;&#30340;&#28966;&#28857;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20852;&#36259;&#12290;&#38024;&#23545;&#36825;&#19968;&#21457;&#23637;&#65292;&#25105;&#20204;&#30340;&#35770;&#25991;&#26088;&#22312;&#28145;&#20837;&#30740;&#31350;&#21508;&#31181;LMMs(&#22914;GPT-4V)&#35782;&#21035;&#21644;&#22238;&#24212;&#36855;&#22240;&#20013;&#20307;&#29616;&#30340;&#24494;&#22937;&#31038;&#20132;&#34384;&#24453;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#32508;&#21512;&#30340;&#36855;&#22240;&#22522;&#20934;&#27979;&#35797;&#38598;GOAT-Bench&#65292;&#20854;&#20013;&#21253;&#21547;&#36229;&#36807;6K&#20010;&#22810;&#26679;&#30340;&#36855;&#22240;&#65292;&#28085;&#30422;&#30340;&#20027;&#39064;&#21253;&#25324;&#38544;&#24615;&#20167;&#24680;&#35328;&#35770;&#12289;&#24615;&#21035;&#27495;&#35270;&#21644;&#32593;&#32476;&#27450;&#20940;&#31561;&#12290;&#21033;&#29992;GOAT-Be
&lt;/p&gt;
&lt;p&gt;
The exponential growth of social media has profoundly transformed how information is created, disseminated, and absorbed, exceeding any precedent in the digital age. Regrettably, this explosion has also spawned a significant increase in the online abuse of memes. Evaluating the negative impact of memes is notably challenging, owing to their often subtle and implicit meanings, which are not directly conveyed through the overt text and imagery. In light of this, large multimodal models (LMMs) have emerged as a focal point of interest due to their remarkable capabilities in handling diverse multimodal tasks. In response to this development, our paper aims to thoroughly examine the capacity of various LMMs (e.g. GPT-4V) to discern and respond to the nuanced aspects of social abuse manifested in memes. We introduce the comprehensive meme benchmark, GOAT-Bench, comprising over 6K varied memes encapsulating themes such as implicit hate speech, sexism, and cyberbullying, etc. Utilizing GOAT-Be
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#24515;&#29702;&#23398;&#24212;&#29992;&#20013;&#30340;&#21069;&#27839;&#65292;&#21253;&#25324;&#22914;&#20309;&#27169;&#25311;&#20154;&#31867;&#35748;&#30693;&#21644;&#34892;&#20026;&#12289;&#25552;&#20379;&#21019;&#26032;&#24037;&#20855;&#36827;&#34892;&#25991;&#29486;&#22238;&#39038;&#12289;&#20551;&#35774;&#29983;&#25104;&#12289;&#23454;&#39564;&#35774;&#35745;&#31561;&#12290;</title><link>http://arxiv.org/abs/2401.01519</link><description>&lt;p&gt;
&#25506;&#32034;LLMs&#22312;&#24515;&#29702;&#23398;&#24212;&#29992;&#20013;&#30340;&#21069;&#27839;&#65306;&#19968;&#20221;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Exploring the Frontiers of LLMs in Psychological Applications: A Comprehensive Review. (arXiv:2401.01519v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01519
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#24515;&#29702;&#23398;&#24212;&#29992;&#20013;&#30340;&#21069;&#27839;&#65292;&#21253;&#25324;&#22914;&#20309;&#27169;&#25311;&#20154;&#31867;&#35748;&#30693;&#21644;&#34892;&#20026;&#12289;&#25552;&#20379;&#21019;&#26032;&#24037;&#20855;&#36827;&#34892;&#25991;&#29486;&#22238;&#39038;&#12289;&#20551;&#35774;&#29983;&#25104;&#12289;&#23454;&#39564;&#35774;&#35745;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#24515;&#29702;&#23398;&#24212;&#29992;&#20013;&#30340;&#21069;&#27839;&#12290;&#24515;&#29702;&#23398;&#32463;&#21382;&#20102;&#20960;&#27425;&#29702;&#35770;&#21464;&#38761;&#65292;&#24403;&#21069;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#65292;&#29305;&#21035;&#26159;LLMs&#30340;&#20351;&#29992;&#26377;&#26395;&#24320;&#21551;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#25105;&#20204;&#35814;&#32454;&#25506;&#35752;&#20102;LLMs&#22914;ChatGPT&#22312;&#24515;&#29702;&#23398;&#30740;&#31350;&#20013;&#30340;&#36716;&#21464;&#12290;&#25991;&#31456;&#35752;&#35770;&#20102;LLMs&#22312;&#35748;&#30693;&#19982;&#34892;&#20026;&#24515;&#29702;&#23398;&#12289;&#20020;&#24202;&#19982;&#21672;&#35810;&#24515;&#29702;&#23398;&#12289;&#25945;&#32946;&#19982;&#21457;&#23637;&#24515;&#29702;&#23398;&#20197;&#21450;&#31038;&#20250;&#19982;&#25991;&#21270;&#24515;&#29702;&#23398;&#31561;&#24515;&#29702;&#23398;&#20998;&#25903;&#20013;&#30340;&#24433;&#21709;&#65292;&#24378;&#35843;&#20102;&#23427;&#20204;&#27169;&#25311;&#20154;&#31867;&#35748;&#30693;&#21644;&#34892;&#20026;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#36825;&#20123;&#27169;&#22411;&#27169;&#25311;&#20154;&#31867;&#25991;&#26412;&#29983;&#25104;&#30340;&#33021;&#21147;&#65292;&#20026;&#24515;&#29702;&#23398;&#20013;&#30340;&#25991;&#29486;&#22238;&#39038;&#12289;&#20551;&#35774;&#29983;&#25104;&#12289;&#23454;&#39564;&#35774;&#35745;&#12289;&#23454;&#39564;&#23545;&#35937;&#12289;&#25968;&#25454;&#20998;&#26512;&#12289;&#23398;&#26415;&#20889;&#20316;&#21644;&#21516;&#34892;&#35780;&#23457;&#31561;&#25552;&#20379;&#21019;&#26032;&#24037;&#20855;&#12290;&#34429;&#28982;LLMs&#22312;&#25512;&#21160;&#30740;&#31350;&#26041;&#27861;&#23398;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;
&lt;/p&gt;
&lt;p&gt;
This paper explores the frontiers of large language models (LLMs) in psychology applications. Psychology has undergone several theoretical changes, and the current use of Artificial Intelligence (AI) and Machine Learning, particularly LLMs, promises to open up new research directions. We provide a detailed exploration of how LLMs like ChatGPT are transforming psychological research. It discusses the impact of LLMs across various branches of psychology, including cognitive and behavioral, clinical and counseling, educational and developmental, and social and cultural psychology, highlighting their potential to simulate aspects of human cognition and behavior. The paper delves into the capabilities of these models to emulate human-like text generation, offering innovative tools for literature review, hypothesis generation, experimental design, experimental subjects, data analysis, academic writing, and peer review in psychology. While LLMs are essential in advancing research methodologie
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#20196;&#29260;&#20256;&#25773;&#25511;&#21046;&#22120;&#65288;TPC&#65289;&#65292;&#36890;&#36807;&#32467;&#21512;&#26242;&#20572;&#27010;&#29575;&#21644;&#37325;&#26032;&#24320;&#22987;&#27010;&#29575;&#65292;&#23454;&#29616;&#20102;&#23545;&#20196;&#29260;&#30340;&#20943;&#23569;&#21644;&#37325;&#22797;&#21033;&#29992;&#30340;&#25511;&#21046;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#35270;&#35273;Transformer&#30340;&#25928;&#29575;&#21644;&#20196;&#29260;&#21033;&#29992;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.01470</link><description>&lt;p&gt;
&#39640;&#25928;&#35270;&#35273;Transformer&#30340;&#20196;&#29260;&#20256;&#25773;&#25511;&#21046;&#22120;
&lt;/p&gt;
&lt;p&gt;
Token Propagation Controller for Efficient Vision Transformer. (arXiv:2401.01470v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01470
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#20196;&#29260;&#20256;&#25773;&#25511;&#21046;&#22120;&#65288;TPC&#65289;&#65292;&#36890;&#36807;&#32467;&#21512;&#26242;&#20572;&#27010;&#29575;&#21644;&#37325;&#26032;&#24320;&#22987;&#27010;&#29575;&#65292;&#23454;&#29616;&#20102;&#23545;&#20196;&#29260;&#30340;&#20943;&#23569;&#21644;&#37325;&#22797;&#21033;&#29992;&#30340;&#25511;&#21046;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#35270;&#35273;Transformer&#30340;&#25928;&#29575;&#21644;&#20196;&#29260;&#21033;&#29992;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;Transformer&#65288;ViTs&#65289;&#22312;&#21508;&#31181;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#28982;&#32780;&#23427;&#20204;&#22312;&#36755;&#20837;&#20196;&#29260;&#25968;&#37327;&#19978;&#30340;&#20108;&#27425;&#22797;&#26434;&#24230;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#19979;&#30340;&#24212;&#29992;&#12290;&#20197;&#21069;&#30340;&#26041;&#27861;&#37319;&#29992;&#36880;&#28176;&#20943;&#23569;&#20196;&#29260;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#20551;&#35774;&#19968;&#20010;&#23618;&#20013;&#30340;&#20196;&#29260;&#20887;&#20313;&#24847;&#21619;&#30528;&#25152;&#26377;&#21518;&#32493;&#23618;&#20013;&#20063;&#26377;&#20887;&#20313;&#12290;&#25105;&#20204;&#32463;&#39564;&#35777;&#26126;&#36825;&#20010;&#20551;&#35774;&#36890;&#24120;&#26159;&#19981;&#27491;&#30830;&#30340;&#65292;&#21363;&#19968;&#20010;&#23618;&#20013;&#22810;&#20313;&#30340;&#20196;&#29260;&#22312;&#21518;&#38754;&#30340;&#23618;&#20013;&#21487;&#20197;&#26159;&#26377;&#29992;&#30340;&#12290;&#22522;&#20110;&#36825;&#20010;&#20851;&#38190;&#27934;&#23519;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20196;&#29260;&#20256;&#25773;&#25511;&#21046;&#22120;&#65288;TPC&#65289;&#65292;&#23427;&#32467;&#21512;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#20196;&#29260;&#20998;&#24067;&#65292;&#21363;&#26242;&#20572;&#27010;&#29575;&#21644;&#37325;&#26032;&#24320;&#22987;&#27010;&#29575;&#65292;&#29992;&#26469;&#25511;&#21046;&#20196;&#29260;&#30340;&#20943;&#23569;&#21644;&#37325;&#22797;&#21033;&#29992;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#20196;&#29260;&#21033;&#29992;&#12290;&#20026;&#20102;&#25913;&#21892;&#20196;&#29260;&#20998;&#24067;&#30340;&#20272;&#35745;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24179;&#28369;&#26426;&#21046;&#65292;&#20316;&#20026;&#27491;&#21017;&#21270;&#22120;&#65292;&#26377;&#21161;&#20110;&#21435;&#38500;&#22122;&#22768;&#24322;&#24120;&#20540;&#12290;&#27492;&#22806;
&lt;/p&gt;
&lt;p&gt;
Vision transformers (ViTs) have achieved promising results on a variety of Computer Vision tasks, however their quadratic complexity in the number of input tokens has limited their application specially in resource-constrained settings. Previous approaches that employ gradual token reduction to address this challenge assume that token redundancy in one layer implies redundancy in all the following layers. We empirically demonstrate that this assumption is often not correct, i.e., tokens that are redundant in one layer can be useful in later layers. We employ this key insight to propose a novel token propagation controller (TPC) that incorporates two different token-distributions, i.e., pause probability and restart probability to control the reduction and reuse of tokens respectively, which results in more efficient token utilization. To improve the estimates of token distributions, we propose a smoothing mechanism that acts as a regularizer and helps remove noisy outliers. Furthermore
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32852;&#37030;&#22810;&#36712;&#36857;GNN&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31232;&#32570;&#25968;&#25454;&#39044;&#27979;&#23156;&#20799;&#33041;&#36830;&#25509;&#24615;&#12290;&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#65292;&#25105;&#20204;&#36890;&#36807;&#32858;&#21512;&#22810;&#20010;&#21307;&#38498;&#30340;&#26412;&#22320;&#23398;&#20064;&#32467;&#26524;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2401.01383</link><description>&lt;p&gt;
&#20351;&#29992;&#31232;&#32570;&#25968;&#25454;&#21644;&#32852;&#37030;&#22810;&#36712;&#36857;GNN&#39044;&#27979;&#23156;&#20799;&#33041;&#36830;&#25509;&#24615;
&lt;/p&gt;
&lt;p&gt;
Predicting Infant Brain Connectivity with Federated Multi-Trajectory GNNs using Scarce Data. (arXiv:2401.01383v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01383
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32852;&#37030;&#22810;&#36712;&#36857;GNN&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31232;&#32570;&#25968;&#25454;&#39044;&#27979;&#23156;&#20799;&#33041;&#36830;&#25509;&#24615;&#12290;&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#65292;&#25105;&#20204;&#36890;&#36807;&#32858;&#21512;&#22810;&#20010;&#21307;&#38498;&#30340;&#26412;&#22320;&#23398;&#20064;&#32467;&#26524;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#35782;&#21035;&#26089;&#26399;&#33041;&#36830;&#25509;&#24615;&#21457;&#23637;&#30340;&#21160;&#24577;&#36807;&#31243;&#65292;&#20102;&#35299;&#23156;&#20799;&#33041;&#32593;&#32476;&#22312;&#20986;&#29983;&#21518;&#30340;&#31532;&#19968;&#24180;&#20013;&#30340;&#22797;&#26434;&#28436;&#21270;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#23384;&#22312;&#19977;&#20010;&#20027;&#35201;&#23616;&#38480;&#24615;&#12290;&#39318;&#20808;&#65292;&#23427;&#20204;&#19981;&#33021;&#27867;&#21270;&#21040;&#22810;&#36712;&#36857;&#39044;&#27979;&#20219;&#21153;&#65292;&#20854;&#20013;&#27599;&#20010;&#22270;&#36712;&#36857;&#23545;&#24212;&#20110;&#29305;&#23450;&#30340;&#25104;&#20687;&#27169;&#24577;&#25110;&#36830;&#25509;&#31867;&#22411;&#65288;&#20363;&#22914;T1-w MRI&#65289;&#12290;&#20854;&#27425;&#65292;&#29616;&#26377;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#25165;&#33021;&#36798;&#21040;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#65292;&#32780;&#36825;&#24448;&#24448;&#24456;&#38590;&#33719;&#21462;&#12290;&#31532;&#19977;&#65292;&#23427;&#20204;&#19981;&#33021;&#26377;&#25928;&#21033;&#29992;&#19981;&#23436;&#25972;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;FedGmTE-Net++&#65292;&#19968;&#31181;&#32852;&#37030;&#22270;&#24418;&#22810;&#36712;&#36857;&#28436;&#21270;&#32593;&#32476;&#12290;&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#30340;&#21147;&#37327;&#65292;&#25105;&#20204;&#22312;&#26377;&#38480;&#30340;&#21307;&#38498;&#25968;&#25454;&#38598;&#20013;&#32858;&#21512;&#20102;&#19981;&#21516;&#21307;&#38498;&#30340;&#26412;&#22320;&#23398;&#20064;&#32467;&#26524;&#12290;&#32467;&#26524;&#21363;&#21487;&#25552;&#39640;&#27599;&#20010;&#21307;&#38498;&#26412;&#22320;&#29983;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;
The understanding of the convoluted evolution of infant brain networks during the first postnatal year is pivotal for identifying the dynamics of early brain connectivity development. Existing deep learning solutions suffer from three major limitations. First, they cannot generalize to multi-trajectory prediction tasks, where each graph trajectory corresponds to a particular imaging modality or connectivity type (e.g., T1-w MRI). Second, existing models require extensive training datasets to achieve satisfactory performance which are often challenging to obtain. Third, they do not efficiently utilize incomplete time series data. To address these limitations, we introduce FedGmTE-Net++, a federated graph-based multi-trajectory evolution network. Using the power of federation, we aggregate local learnings among diverse hospitals with limited datasets. As a result, we enhance the performance of each hospital's local generative model, while preserving data privacy. The three key innovation
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20840;&#38754;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#32534;&#36753;&#65292;&#26088;&#22312;&#26377;&#25928;&#20462;&#25913;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#21516;&#26102;&#20445;&#25345;&#25972;&#20307;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.01286</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#32534;&#36753;&#20840;&#38754;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Study of Knowledge Editing for Large Language Models. (arXiv:2401.01286v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01286
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20840;&#38754;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#32534;&#36753;&#65292;&#26088;&#22312;&#26377;&#25928;&#20462;&#25913;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#21516;&#26102;&#20445;&#25345;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22312;&#29702;&#35299;&#21644;&#29983;&#25104;&#19982;&#20154;&#31867;&#20132;&#27969;&#32039;&#23494;&#30456;&#20284;&#30340;&#25991;&#26412;&#26041;&#38754;&#23637;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20854;&#20027;&#35201;&#38480;&#21046;&#22312;&#20110;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#26174;&#33879;&#35745;&#31639;&#38656;&#27714;&#65292;&#36825;&#26159;&#30001;&#20110;&#20854;&#24191;&#27867;&#30340;&#21442;&#25968;&#21270;&#36896;&#25104;&#30340;&#12290;&#36825;&#19968;&#25361;&#25112;&#22312;&#20110;&#19990;&#30028;&#30340;&#21160;&#24577;&#24615;&#65292;&#38656;&#35201;&#39057;&#32321;&#26356;&#26032;LLM&#20197;&#20462;&#27491;&#36807;&#26102;&#30340;&#20449;&#24687;&#25110;&#38598;&#25104;&#26032;&#30693;&#35782;&#65292;&#20174;&#32780;&#30830;&#20445;&#20854;&#25345;&#32493;&#30340;&#30456;&#20851;&#24615;&#12290;&#35768;&#22810;&#24212;&#29992;&#38656;&#35201;&#22312;&#35757;&#32451;&#21518;&#36827;&#34892;&#25345;&#32493;&#30340;&#27169;&#22411;&#35843;&#25972;&#65292;&#20197;&#35299;&#20915;&#32570;&#38519;&#25110;&#19981;&#33391;&#34892;&#20026;&#12290;&#36817;&#24180;&#26469;&#65292;&#23545;&#20110;LLM&#30340;&#30693;&#35782;&#32534;&#36753;&#25216;&#26415;&#30340;&#20852;&#36259;&#36234;&#26469;&#36234;&#39640;&#65292;&#22312;&#29305;&#23450;&#39046;&#22495;&#20869;&#26377;&#25928;&#22320;&#20462;&#25913;LLM&#30340;&#34892;&#20026;&#65292;&#21516;&#26102;&#20445;&#25345;&#25972;&#20307;&#24615;&#33021;&#22312;&#21508;&#31181;&#36755;&#20837;&#20013;&#30340;&#34920;&#29616;&#12290;&#26412;&#25991;&#39318;&#20808;&#23450;&#20041;&#20102;&#30693;&#35782;&#32534;&#36753;&#30340;&#30446;&#26631;&#21644;&#25361;&#25112;&#65292;&#28982;&#21518;&#32508;&#36848;&#20102;&#29616;&#26377;&#30340;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#21644;&#25216;&#26415;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#24212;&#29992;&#21644;&#26410;&#26469;&#21457;&#23637;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown extraordinary capabilities in understanding and generating text that closely mirrors human communication. However, a primary limitation lies in the significant computational demands during training, arising from their extensive parameterization. This challenge is further intensified by the dynamic nature of the world, necessitating frequent updates to LLMs to correct outdated information or integrate new knowledge, thereby ensuring their continued relevance. Note that many applications demand continual model adjustments post-training to address deficiencies or undesirable behaviors. There is an increasing interest in efficient, lightweight methods for on-the-fly model modifications. To this end, recent years have seen a burgeoning in the techniques of knowledge editing for LLMs, which aim to efficiently modify LLMs' behaviors within specific domains while preserving overall performance across various inputs. In this paper, we first define the kno
&lt;/p&gt;</description></item><item><title>PPBFL&#26159;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#30340;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#24212;&#29992;&#21306;&#22359;&#38142;&#21644;&#33258;&#36866;&#24212;&#24046;&#20998;&#38544;&#31169;&#28155;&#21152;&#31639;&#27861;&#65292;&#22686;&#24378;&#20102;&#32852;&#37030;&#23398;&#20064;&#30340;&#23433;&#20840;&#24615;&#21644;&#33410;&#28857;&#30340;&#31215;&#26497;&#21442;&#19982;&#12290;&#21516;&#26102;&#24341;&#20837;&#20102;&#20132;&#26131;&#28151;&#21512;&#26426;&#21046;&#65292;&#26356;&#22909;&#22320;&#20445;&#25252;&#26412;&#22320;&#35757;&#32451;&#30340;&#36523;&#20221;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2401.01204</link><description>&lt;p&gt;
PPBFL: &#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#30340;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PPBFL: A Privacy Protected Blockchain-based Federated Learning Model. (arXiv:2401.01204v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01204
&lt;/p&gt;
&lt;p&gt;
PPBFL&#26159;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#30340;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#24212;&#29992;&#21306;&#22359;&#38142;&#21644;&#33258;&#36866;&#24212;&#24046;&#20998;&#38544;&#31169;&#28155;&#21152;&#31639;&#27861;&#65292;&#22686;&#24378;&#20102;&#32852;&#37030;&#23398;&#20064;&#30340;&#23433;&#20840;&#24615;&#21644;&#33410;&#28857;&#30340;&#31215;&#26497;&#21442;&#19982;&#12290;&#21516;&#26102;&#24341;&#20837;&#20102;&#20132;&#26131;&#28151;&#21512;&#26426;&#21046;&#65292;&#26356;&#22909;&#22320;&#20445;&#25252;&#26412;&#22320;&#35757;&#32451;&#30340;&#36523;&#20221;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#30340;&#24555;&#36895;&#21457;&#23637;&#21644;&#23545;&#25968;&#25454;&#38544;&#31169;&#30340;&#26085;&#30410;&#20851;&#27880;&#65292;&#32852;&#37030;&#23398;&#20064;&#25104;&#20026;&#19968;&#20010;&#36234;&#26469;&#36234;&#31361;&#20986;&#30340;&#28966;&#28857;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#21442;&#25968;&#25915;&#20987;&#21644;&#32570;&#20047;&#28608;&#21169;&#26426;&#21046;&#31561;&#25361;&#25112;&#38459;&#30861;&#20102;&#32852;&#37030;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#30340;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#65288;PPBFL&#65289;&#65292;&#20197;&#22686;&#24378;&#32852;&#37030;&#23398;&#20064;&#30340;&#23433;&#20840;&#24615;&#24182;&#20419;&#36827;&#33410;&#28857;&#22312;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#31215;&#26497;&#21442;&#19982;&#12290;&#21306;&#22359;&#38142;&#30830;&#20445;&#20102;&#23384;&#20648;&#22312;&#26143;&#38469;&#25991;&#20214;&#31995;&#32479;&#65288;IPFS&#65289;&#20013;&#30340;&#27169;&#22411;&#21442;&#25968;&#19981;&#34987;&#31713;&#25913;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#24046;&#20998;&#38544;&#31169;&#28155;&#21152;&#31639;&#27861;&#65292;&#21516;&#26102;&#24212;&#29992;&#20110;&#26412;&#22320;&#27169;&#22411;&#21644;&#20840;&#23616;&#27169;&#22411;&#65292;&#20445;&#25252;&#26412;&#22320;&#27169;&#22411;&#30340;&#38544;&#31169;&#21516;&#26102;&#38450;&#27490;&#22240;&#32852;&#37030;&#23398;&#20064;&#20013;&#23384;&#22312;&#22823;&#37327;&#26412;&#22320;&#27169;&#22411;&#32780;&#38477;&#20302;&#20840;&#23616;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#20132;&#26131;&#28151;&#21512;&#26426;&#21046;&#65292;&#20197;&#26356;&#22909;&#22320;&#20445;&#25252;&#26412;&#22320;&#35757;&#32451;&#30340;&#36523;&#20221;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rapid development of machine learning and growing concerns about data privacy, federated learning has become an increasingly prominent focus. However, challenges such as attacks on model parameters and the lack of incentive mechanisms hinder the effectiveness of federated learning. Therefore, we propose a Privacy Protected Blockchain-based Federated Learning Model (PPBFL) to enhance the security of federated learning and promote the active participation of nodes in model training. Blockchain ensures that model parameters stored in the InterPlanetary File System (IPFS) remain unaltered. A novel adaptive differential privacy addition algorithm is simultaneously applied to local and global models, preserving the privacy of local models and preventing a decrease in the security of the global model due to the presence of numerous local models in federated learning. Additionally, we introduce a new mix transactions mechanism to better protect the identity privacy of local training c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#30333;&#32454;&#32990;&#26816;&#27979;&#26041;&#27861;&#65292;&#20351;&#29992;&#22810;&#32423;&#29305;&#24449;&#34701;&#21512;&#21644;&#21464;&#24418;&#33258;&#27880;&#24847;DETR&#65292;&#36890;&#36807;&#35299;&#20915;&#30333;&#32454;&#32990;&#23610;&#24230;&#24046;&#24322;&#38382;&#39064;&#21644;&#25552;&#39640;&#26816;&#27979;&#31934;&#24230;&#65292;&#20197;&#25913;&#21892;&#20256;&#32479;&#34880;&#28082;&#26816;&#27979;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.00926</link><description>&lt;p&gt;
&#20934;&#30830;&#30340;&#21464;&#24418;DETR&#21644;&#22810;&#32423;&#29305;&#24449;&#34701;&#21512;&#29992;&#20110;&#36741;&#21161;&#34880;&#28082;&#30142;&#30149;&#35786;&#26029;&#30340;&#30333;&#32454;&#32990;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Accurate Leukocyte Detection Based on Deformable-DETR and Multi-Level Feature Fusion for Aiding Diagnosis of Blood Diseases. (arXiv:2401.00926v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00926
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#30333;&#32454;&#32990;&#26816;&#27979;&#26041;&#27861;&#65292;&#20351;&#29992;&#22810;&#32423;&#29305;&#24449;&#34701;&#21512;&#21644;&#21464;&#24418;&#33258;&#27880;&#24847;DETR&#65292;&#36890;&#36807;&#35299;&#20915;&#30333;&#32454;&#32990;&#23610;&#24230;&#24046;&#24322;&#38382;&#39064;&#21644;&#25552;&#39640;&#26816;&#27979;&#31934;&#24230;&#65292;&#20197;&#25913;&#21892;&#20256;&#32479;&#34880;&#28082;&#26816;&#27979;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26631;&#20934;&#21307;&#38498;&#34880;&#28082;&#26816;&#27979;&#20013;&#65292;&#20256;&#32479;&#30340;&#26041;&#27861;&#38656;&#35201;&#21307;&#29983;&#20351;&#29992;&#26174;&#24494;&#38236;&#20174;&#24739;&#32773;&#30340;&#34880;&#28082;&#26174;&#24494;&#22270;&#20687;&#20013;&#25163;&#21160;&#20998;&#31163;&#30333;&#32454;&#32990;&#12290;&#28982;&#21518;&#36890;&#36807;&#33258;&#21160;&#30333;&#32454;&#32990;&#20998;&#31867;&#22120;&#23545;&#36825;&#20123;&#20998;&#31163;&#30340;&#30333;&#32454;&#32990;&#36827;&#34892;&#20998;&#31867;&#65292;&#20197;&#30830;&#23450;&#34880;&#26679;&#20013;&#19981;&#21516;&#31867;&#22411;&#30333;&#32454;&#32990;&#30340;&#27604;&#20363;&#21644;&#20307;&#31215;&#65292;&#20174;&#32780;&#21327;&#21161;&#30142;&#30149;&#35786;&#26029;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#32791;&#26102;&#12289;&#32791;&#21147;&#65292;&#32780;&#19988;&#23481;&#26131;&#20986;&#29616;&#38169;&#35823;&#65292;&#22240;&#20026;&#22270;&#20687;&#36136;&#37327;&#21644;&#29615;&#22659;&#26465;&#20214;&#31561;&#22240;&#32032;&#65292;&#21487;&#33021;&#23548;&#33268;&#21518;&#32493;&#20998;&#31867;&#38169;&#35823;&#21644;&#35823;&#35786;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#30333;&#32454;&#32990;&#26816;&#27979;&#26041;&#27861;&#65306;&#22810;&#32423;&#29305;&#24449;&#34701;&#21512;&#21644;&#21464;&#24418;&#33258;&#27880;&#24847;DETR&#65288;MFDS-DETR&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#30333;&#32454;&#32990;&#23610;&#24230;&#24046;&#24322;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#39640;&#32423;&#31579;&#36873;&#29305;&#24449;&#34701;&#21512;&#37329;&#23383;&#22612;&#65288;HS-FPN&#65289;&#65292;&#23454;&#29616;&#20102;&#22810;&#32423;&#34701;&#21512;&#12290;&#35813;&#27169;&#22411;&#20351;&#29992;&#39640;&#32423;&#29305;&#24449;&#20316;&#20026;&#29305;&#24449;&#34701;&#21512;&#30340;&#36755;&#20837;&#65292;&#21516;&#26102;&#37319;&#29992;&#21464;&#24418;&#33258;&#27880;&#24847;DETR&#23454;&#29616;&#31934;&#30830;&#30340;&#30333;&#32454;&#32990;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
In standard hospital blood tests, the traditional process requires doctors to manually isolate leukocytes from microscopic images of patients' blood using microscopes. These isolated leukocytes are then categorized via automatic leukocyte classifiers to determine the proportion and volume of different types of leukocytes present in the blood samples, aiding disease diagnosis. This methodology is not only time-consuming and labor-intensive, but it also has a high propensity for errors due to factors such as image quality and environmental conditions, which could potentially lead to incorrect subsequent classifications and misdiagnosis. To address these issues, this paper proposes an innovative method of leukocyte detection: the Multi-level Feature Fusion and Deformable Self-attention DETR (MFDS-DETR). To tackle the issue of leukocyte scale disparity, we designed the High-level Screening-feature Fusion Pyramid (HS-FPN), enabling multi-level fusion. This model uses high-level features as 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#30456;&#26426;&#38519;&#38449;&#22270;&#20687;&#30340;&#32467;&#26500;&#21270;&#19978;&#19979;&#25991;&#65292;&#25552;&#39640;&#20854;&#22312;&#29289;&#31181;&#35782;&#21035;&#20219;&#21153;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#35299;&#20915;&#20102;&#25968;&#25454;&#31232;&#32570;&#21644;&#27867;&#21270;&#33021;&#21147;&#22686;&#24378;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.00608</link><description>&lt;p&gt;
&#23558;&#19978;&#19979;&#25991;&#24102;&#22238;&#26469;&#65306;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#19978;&#30340;&#30456;&#26426;&#38519;&#38449;&#29289;&#31181;&#35782;&#21035;&#20316;&#20026;&#38142;&#25509;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Bringing Back the Context: Camera Trap Species Identification as Link Prediction on Multimodal Knowledge Graphs. (arXiv:2401.00608v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00608
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#30456;&#26426;&#38519;&#38449;&#22270;&#20687;&#30340;&#32467;&#26500;&#21270;&#19978;&#19979;&#25991;&#65292;&#25552;&#39640;&#20854;&#22312;&#29289;&#31181;&#35782;&#21035;&#20219;&#21153;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#35299;&#20915;&#20102;&#25968;&#25454;&#31232;&#32570;&#21644;&#27867;&#21270;&#33021;&#21147;&#22686;&#24378;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#26426;&#38519;&#38449;&#22312;&#21160;&#29289;&#29983;&#24577;&#23398;&#20013;&#26159;&#23453;&#36149;&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#29983;&#29289;&#22810;&#26679;&#24615;&#30417;&#27979;&#21644;&#20445;&#25252;&#12290;&#28982;&#32780;&#65292;&#25361;&#25112;&#22914;&#22312;&#26032;&#30340;&#26410;&#30693;&#20301;&#32622;&#37096;&#32626;&#26102;&#30340;&#31967;&#31957;&#27867;&#21270;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#22270;&#20687;&#33258;&#28982;&#19982;&#21487;&#33021;&#22312;&#19981;&#21516;&#27169;&#24577;&#19979;&#30340;&#24322;&#36136;&#19978;&#19979;&#25991;&#30456;&#20851;&#32852;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#19982;&#30456;&#26426;&#38519;&#38449;&#22270;&#20687;&#30456;&#20851;&#32852;&#30340;&#32467;&#26500;&#21270;&#19978;&#19979;&#25991;&#65292;&#25913;&#21892;&#22312;&#30456;&#26426;&#38519;&#38449;&#20013;&#29289;&#31181;&#35782;&#21035;&#36825;&#20010;&#20219;&#21153;&#30340;&#36229;&#20986;&#20998;&#24067;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20363;&#22914;&#65292;&#19968;&#24352;&#37326;&#29983;&#21160;&#29289;&#30340;&#29031;&#29255;&#21487;&#33021;&#19982;&#25293;&#25668;&#22320;&#28857;&#21644;&#26102;&#38388;&#20197;&#21450;&#20851;&#20110;&#21160;&#29289;&#29289;&#31181;&#30340;&#32467;&#26500;&#21270;&#29983;&#29289;&#23398;&#30693;&#35782;&#30456;&#20851;&#32852;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#24037;&#20316;&#36890;&#24120;&#24573;&#35270;&#36825;&#19968;&#28857;&#65292;&#20294;&#23558;&#36825;&#26679;&#30340;&#19978;&#19979;&#25991;&#24102;&#22238;&#26469;&#21487;&#20197;&#24102;&#26469;&#19968;&#20123;&#28508;&#22312;&#30340;&#22909;&#22788;&#65292;&#22914;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#21644;&#22686;&#24378;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#26377;&#25928;&#22320;&#23558;&#36825;&#26679;&#30340;&#24322;&#36136;&#19978;&#19979;&#25991;&#25972;&#21512;&#21040;&#35270;&#35273;&#39046;&#22495;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Camera traps are valuable tools in animal ecology for biodiversity monitoring and conservation. However, challenges like poor generalization to deployment at new unseen locations limit their practical application. Images are naturally associated with heterogeneous forms of context possibly in different modalities. In this work, we leverage the structured context associated with the camera trap images to improve out-of-distribution generalization for the task of species identification in camera traps. For example, a photo of a wild animal may be associated with information about where and when it was taken, as well as structured biology knowledge about the animal species. While typically overlooked by existing work, bringing back such context offers several potential benefits for better image understanding, such as addressing data scarcity and enhancing generalization. However, effectively integrating such heterogeneous context into the visual domain is a challenging problem. To address
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#24418;&#34920;&#31034;&#65292;&#21452;&#21521;&#26102;&#38388;&#35745;&#21010;&#22270;&#65288;BTPG&#65289;&#65292;&#20801;&#35768;&#22312;&#25191;&#34892;&#36807;&#31243;&#20013;&#20999;&#25442;&#36890;&#34892;&#39034;&#24207;&#65292;&#36991;&#20813;&#19981;&#24517;&#35201;&#30340;&#31561;&#24453;&#26102;&#38388;&#65292;&#35299;&#20915;&#20102;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#20013;&#30340;&#25191;&#34892;&#25928;&#29575;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.00315</link><description>&lt;p&gt;
&#21452;&#21521;&#26102;&#38388;&#35745;&#21010;&#22270;&#65306;&#20026;&#26356;&#39640;&#25928;&#30340;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#25191;&#34892;&#25552;&#20379;&#21487;&#20999;&#25442;&#30340;&#36890;&#34892;&#39034;&#24207;
&lt;/p&gt;
&lt;p&gt;
Bidirectional Temporal Plan Graph: Enabling Switchable Passing Orders for More Efficient Multi-Agent Path Finding Plan Execution. (arXiv:2401.00315v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00315
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#24418;&#34920;&#31034;&#65292;&#21452;&#21521;&#26102;&#38388;&#35745;&#21010;&#22270;&#65288;BTPG&#65289;&#65292;&#20801;&#35768;&#22312;&#25191;&#34892;&#36807;&#31243;&#20013;&#20999;&#25442;&#36890;&#34892;&#39034;&#24207;&#65292;&#36991;&#20813;&#19981;&#24517;&#35201;&#30340;&#31561;&#24453;&#26102;&#38388;&#65292;&#35299;&#20915;&#20102;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#20013;&#30340;&#25191;&#34892;&#25928;&#29575;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#65288;MAPF&#65289;&#38382;&#39064;&#28041;&#21450;&#20026;&#20849;&#20139;&#29615;&#22659;&#20013;&#30340;&#22810;&#20010;&#26234;&#33021;&#20307;&#35268;&#21010;&#26080;&#30896;&#25758;&#36335;&#24452;&#12290;&#22823;&#22810;&#25968;MAPF&#35299;&#31639;&#22120;&#20381;&#36182;&#20110;&#26234;&#33021;&#20307;&#33021;&#22815;&#22312;&#29305;&#23450;&#26102;&#38388;&#27493;&#21040;&#36798;&#29305;&#23450;&#20301;&#32622;&#30340;&#20551;&#35774;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#19990;&#30028;&#30340;&#25191;&#34892;&#19981;&#30830;&#23450;&#24615;&#21487;&#33021;&#23548;&#33268;&#26234;&#33021;&#20307;&#20559;&#31163;&#27492;&#20551;&#35774;&#65292;&#20174;&#32780;&#23548;&#33268;&#30896;&#25758;&#21644;&#27515;&#38145;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#36807;&#35753;&#26234;&#33021;&#20307;&#36981;&#24490;&#19968;&#20010;&#26102;&#38388;&#35745;&#21010;&#22270;&#65288;TPG&#65289;&#65292;&#22312;&#27599;&#20010;&#20301;&#32622;&#24378;&#21046;&#25191;&#34892;MAPF&#35745;&#21010;&#20013;&#23450;&#20041;&#30340;&#19968;&#33268;&#36890;&#34892;&#39034;&#24207;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;TPG&#36807;&#20110;&#20005;&#26684;&#65292;&#22240;&#20026;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#28385;&#36275;&#36890;&#34892;&#39034;&#24207;&#38656;&#35201;&#26234;&#33021;&#20307;&#26080;&#24517;&#35201;&#22320;&#31561;&#24453;&#65292;&#23548;&#33268;&#25191;&#34892;&#26102;&#38388;&#21464;&#38271;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#24418;&#34920;&#31034;&#65292;&#31216;&#20026;&#21452;&#21521;&#26102;&#38388;&#35745;&#21010;&#22270;&#65288;BTPG&#65289;&#65292;&#20801;&#35768;&#22312;&#25191;&#34892;&#36807;&#31243;&#20013;&#20999;&#25442;&#36890;&#34892;&#39034;&#24207;&#65292;&#36991;&#20813;&#19981;&#24517;&#35201;&#30340;&#31561;&#24453;&#26102;&#38388;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#31181;&#26500;&#24314;BTPG&#30340;&#20219;&#24847;&#26102;&#38388;&#31639;&#27861;&#65306;
&lt;/p&gt;
&lt;p&gt;
The Multi-Agent Path Finding (MAPF) problem involves planning collision-free paths for multiple agents in a shared environment. The majority of MAPF solvers rely on the assumption that an agent can arrive at a specific location at a specific timestep. However, real-world execution uncertainties can cause agents to deviate from this assumption, leading to collisions and deadlocks. Prior research solves this problem by having agents follow a Temporal Plan Graph (TPG), enforcing a consistent passing order at every location as defined in the MAPF plan. However, we show that TPGs are overly strict because, in some circumstances, satisfying the passing order requires agents to wait unnecessarily, leading to longer execution time. To overcome this issue, we introduce a new graphical representation called a Bidirectional Temporal Plan Graph (BTPG), which allows switching passing orders during execution to avoid unnecessary waiting time. We design two anytime algorithms for constructing a BTPG:
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#22270;&#28789;&#30340;&#32654;&#20029;&#30340;&#24605;&#32500;&#23454;&#39564;&#36827;&#34892;&#20102;&#21382;&#21490;&#37325;&#24314;&#65292;&#25552;&#20379;&#20102;&#22823;&#37327;&#35777;&#25454;&#21644;&#19968;&#20123;&#21407;&#21019;&#31572;&#26696;&#65292;&#21516;&#26102;&#22238;&#31572;&#20102;&#22270;&#28789;&#27979;&#35797;&#30340;&#26680;&#24515;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.00009</link><description>&lt;p&gt;
&#22270;&#28789;&#27979;&#35797;&#65292;&#19968;&#20010;&#32654;&#20029;&#30340;&#24605;&#32500;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
Turing's Test, a Beautiful Thought Experiment. (arXiv:2401.00009v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00009
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22270;&#28789;&#30340;&#32654;&#20029;&#30340;&#24605;&#32500;&#23454;&#39564;&#36827;&#34892;&#20102;&#21382;&#21490;&#37325;&#24314;&#65292;&#25552;&#20379;&#20102;&#22823;&#37327;&#35777;&#25454;&#21644;&#19968;&#20123;&#21407;&#21019;&#31572;&#26696;&#65292;&#21516;&#26102;&#22238;&#31572;&#20102;&#22270;&#28789;&#27979;&#35797;&#30340;&#26680;&#24515;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#28010;&#28526;&#20013;&#65292;&#20851;&#20110;&#22270;&#28789;&#27979;&#35797;&#21450;&#20854;&#23545;&#20154;&#24037;&#26234;&#33021;&#30340;&#20215;&#20540;&#30340;&#20105;&#35770;&#21644;&#38382;&#39064;&#37325;&#26032;&#20852;&#36215;&#65292;&#24182;&#24341;&#21457;&#20102;&#25968;&#21313;&#24180;&#26469;&#23454;&#38469;&#30340;&#8220;&#22270;&#28789;&#8221;&#27979;&#35797;&#12290;&#22914;&#26524;&#20154;&#24037;&#26234;&#33021;&#26159;&#37327;&#23376;&#29289;&#29702;&#23398;&#65292;&#29616;&#22312;&#21487;&#33021;&#24050;&#26377;&#20960;&#21482;&#8220;&#34203;&#23450;&#35860;&#30340;&#8221;&#29483;&#34987;&#26432;&#27515;&#20102;&#12290;&#36831;&#21040;&#24635;&#27604;&#19981;&#21040;&#22909;&#65292;&#29616;&#22312;&#26159;&#23545;&#22270;&#28789;&#32654;&#20029;&#30340;&#24605;&#32500;&#23454;&#39564;&#36827;&#34892;&#21382;&#21490;&#37325;&#24314;&#30340;&#26102;&#20505;&#20102;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#25552;&#20379;&#20102;&#22823;&#37327;&#35777;&#25454;&#65292;&#21253;&#25324;&#26032;&#30340;&#26723;&#26696;&#26469;&#28304;&#65292;&#38024;&#23545;&#22270;&#28789;1950&#24180;&#35770;&#25991;&#30340;&#20960;&#20010;&#26410;&#35299;&#20915;&#38382;&#39064;&#32473;&#20986;&#20102;&#21407;&#21019;&#31572;&#26696;&#65292;&#24182;&#22238;&#31572;&#20102;&#22270;&#28789;&#27979;&#35797;&#30340;&#26680;&#24515;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the wake of large language models, there has been a resurgence of claims and questions about the Turing test and its value for AI, which are reminiscent of decades of practical "Turing" tests. If AI were quantum physics, by now several "Schr\"odinger's" cats could have been killed. Better late than never, it is time for a historical reconstruction of Turing's beautiful thought experiment. In this paper I present a wealth of evidence, including new archival sources, give original answers to several open questions about Turing's 1950 paper, and address the core question of the value of Turing's test.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#33258;&#21160;&#21270;&#29983;&#25104;&#34892;&#21160;&#39033;&#39537;&#21160;&#30340;&#20250;&#35758;&#25688;&#35201;&#65292;&#36890;&#36807;&#36882;&#24402;&#29983;&#25104;&#20998;&#27573;&#25688;&#35201;&#24182;&#20351;&#29992;&#34892;&#21160;&#39033;&#25552;&#21462;&#31639;&#27861;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#36824;&#24341;&#20837;&#20102;&#19977;&#31181;&#29992;&#20110;&#23558;&#38271;&#35760;&#24405;&#20998;&#21106;&#25104;&#20027;&#39064;&#37096;&#20998;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#31639;&#27861;&#30340;&#25928;&#29575;&#21644;&#35299;&#20915;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2312.17581</link><description>&lt;p&gt;
&#38271;&#26102;&#38388;&#20250;&#35758;&#35760;&#24405;&#30340;&#34892;&#21160;&#39033;&#39537;&#21160;&#25688;&#35201;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Action-Item-Driven Summarization of Long Meeting Transcripts. (arXiv:2312.17581v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.17581
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#33258;&#21160;&#21270;&#29983;&#25104;&#34892;&#21160;&#39033;&#39537;&#21160;&#30340;&#20250;&#35758;&#25688;&#35201;&#65292;&#36890;&#36807;&#36882;&#24402;&#29983;&#25104;&#20998;&#27573;&#25688;&#35201;&#24182;&#20351;&#29992;&#34892;&#21160;&#39033;&#25552;&#21462;&#31639;&#27861;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#36824;&#24341;&#20837;&#20102;&#19977;&#31181;&#29992;&#20110;&#23558;&#38271;&#35760;&#24405;&#20998;&#21106;&#25104;&#20027;&#39064;&#37096;&#20998;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#31639;&#27861;&#30340;&#25928;&#29575;&#21644;&#35299;&#20915;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22312;&#32447;&#20250;&#35758;&#30340;&#27969;&#34892;&#19979;&#65292;&#33258;&#21160;&#29983;&#25104;&#20250;&#35758;&#25688;&#35201;&#30340;&#27169;&#22411;&#21464;&#24471;&#26356;&#21152;&#23454;&#29992;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#33258;&#21160;&#21270;&#29983;&#25104;&#20250;&#35758;&#25688;&#35201;&#12290;&#24403;&#21069;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#21482;&#29983;&#25104;&#19968;&#33324;&#32780;&#22522;&#26412;&#30340;&#25688;&#35201;&#65292;&#23558;&#20250;&#35758;&#31616;&#21333;&#22320;&#35270;&#20026;&#19968;&#20010;&#38271;&#23545;&#35805;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#26032;&#31639;&#27861;&#21487;&#20197;&#26681;&#25454;&#20250;&#35758;&#35760;&#24405;&#20013;&#30340;&#34892;&#21160;&#39033;&#29983;&#25104;&#25277;&#35937;&#30340;&#20250;&#35758;&#25688;&#35201;&#12290;&#36825;&#26159;&#36890;&#36807;&#36882;&#24402;&#29983;&#25104;&#25688;&#35201;&#24182;&#24182;&#34892;&#36816;&#34892;&#25105;&#20204;&#30340;&#34892;&#21160;&#39033;&#25552;&#21462;&#31639;&#27861;&#26469;&#23454;&#29616;&#30340;&#12290;&#25152;&#26377;&#36825;&#20123;&#31456;&#33410;&#25688;&#35201;&#28982;&#21518;&#21512;&#24182;&#24182;&#24635;&#32467;&#22312;&#19968;&#36215;&#65292;&#20197;&#21019;&#24314;&#19968;&#20010;&#36830;&#36143;&#19988;&#20197;&#34892;&#21160;&#39033;&#20026;&#23548;&#21521;&#30340;&#25688;&#35201;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#20171;&#32461;&#20102;&#19977;&#31181;&#23558;&#38271;&#35760;&#24405;&#20998;&#21106;&#25104;&#22522;&#20110;&#20027;&#39064;&#30340;&#37096;&#20998;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#31639;&#27861;&#30340;&#26102;&#38388;&#25928;&#29575;&#65292;&#24182;&#35299;&#20915;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increased prevalence of online meetings has significantly enhanced the practicality of a model that can automatically generate the summary of a given meeting. This paper introduces a novel and effective approach to automate the generation of meeting summaries. Current approaches to this problem generate general and basic summaries, considering the meeting simply as a long dialogue. However, our novel algorithms can generate abstractive meeting summaries that are driven by the action items contained in the meeting transcript. This is done by recursively generating summaries and employing our action-item extraction algorithm for each section of the meeting in parallel. All of these sectional summaries are then combined and summarized together to create a coherent and action-item-driven summary. In addition, this paper introduces three novel methods for dividing up long transcripts into topic-based sections to improve the time efficiency of our algorithm, as well as to resolve the iss
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#37325;&#35201;&#24615;&#25277;&#26679;&#36827;&#34892;&#26368;&#22823;&#20559;&#22909;&#20248;&#21270;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#30452;&#25509;&#20248;&#21270;&#29983;&#25104;&#31574;&#30053;&#26469;&#28040;&#38500;&#23545;&#22870;&#21169;&#27169;&#22411;&#30340;&#38656;&#27714;&#65292;&#25552;&#39640;&#20102;&#25968;&#25454;&#21033;&#29992;&#29575;&#21644;&#31283;&#23450;&#24615;&#65292;&#24182;&#36890;&#36807;&#35299;&#20915;KL&#27491;&#21017;&#21270;&#38382;&#39064;&#26469;&#25913;&#21892;&#20559;&#22909;&#23398;&#20064;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2312.16430</link><description>&lt;p&gt;
&#20559;&#22909;&#20316;&#20026;&#22870;&#21169;&#65292;&#20351;&#29992;&#37325;&#35201;&#24615;&#25277;&#26679;&#36827;&#34892;&#26368;&#22823;&#20559;&#22909;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Preference as Reward, Maximum Preference Optimization with Importance Sampling. (arXiv:2312.16430v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.16430
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#37325;&#35201;&#24615;&#25277;&#26679;&#36827;&#34892;&#26368;&#22823;&#20559;&#22909;&#20248;&#21270;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#30452;&#25509;&#20248;&#21270;&#29983;&#25104;&#31574;&#30053;&#26469;&#28040;&#38500;&#23545;&#22870;&#21169;&#27169;&#22411;&#30340;&#38656;&#27714;&#65292;&#25552;&#39640;&#20102;&#25968;&#25454;&#21033;&#29992;&#29575;&#21644;&#31283;&#23450;&#24615;&#65292;&#24182;&#36890;&#36807;&#35299;&#20915;KL&#27491;&#21017;&#21270;&#38382;&#39064;&#26469;&#25913;&#21892;&#20559;&#22909;&#23398;&#20064;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20559;&#22909;&#23398;&#20064;&#26159;&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#23545;&#40784;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#20174;&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#26159;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#20248;&#21270;&#20559;&#22909;&#23398;&#20064;&#65292;&#39318;&#20808;&#25311;&#21512;&#20559;&#22909;&#20998;&#25968;&#30340;&#22870;&#21169;&#27169;&#22411;&#65292;&#28982;&#21518;&#20351;&#29992;&#22522;&#20110;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#36827;&#34892;&#20248;&#21270;&#65292;&#20197;&#26368;&#22823;&#21270;&#22870;&#21169;&#12290;RLHF&#30340;&#22788;&#29702;&#36807;&#31243;&#22797;&#26434;&#12289;&#32791;&#26102;&#19988;&#19981;&#31283;&#23450;&#12290;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#31639;&#27861;&#20351;&#29992;&#31163;&#31574;&#30053;&#31639;&#27861;&#30452;&#25509;&#20248;&#21270;&#29983;&#25104;&#31574;&#30053;&#65292;&#28040;&#38500;&#20102;&#23545;&#22870;&#21169;&#27169;&#22411;&#30340;&#38656;&#27714;&#65292;&#20855;&#26377;&#39640;&#25928;&#21644;&#31283;&#23450;&#30340;&#25968;&#25454;&#21033;&#29992;&#29575;&#12290;DPO&#20351;&#29992;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#27169;&#22411;&#21644;&#23545;&#25968;&#25439;&#22833;&#65292;&#23548;&#33268;&#22312;&#20559;&#22909;&#25509;&#36817;&#30830;&#23450;&#24615;&#26102;&#24573;&#30053;&#20102;KL&#27491;&#21017;&#21270;&#39033;&#32780;&#36807;&#24230;&#25311;&#21512;&#20559;&#22909;&#25968;&#25454;&#12290;IPO&#20351;&#29992;&#19968;&#31181;&#22522;&#20110;&#26681;&#26597;&#25214;&#30340;&#25104;&#23545;&#22343;&#26041;&#35823;&#24046;&#25439;&#22833;&#26469;&#35299;&#20915;&#24573;&#30053;KL&#27491;&#21017;&#21270;&#38382;&#39064;&#65292;&#24182;&#23398;&#20064;&#21040;&#26368;&#20248;&#31574;&#30053;&#12290;&#20294;&#26159;IPO&#30340;&#25104;&#23545;&#25439;&#22833;&#20173;&#28982;&#26080;&#27861;&#20351;KL&#27491;&#21017;&#21270;&#29983;&#25928;&#12290;&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#20351;&#29992;&#37325;&#35201;&#24615;&#25277;&#26679;&#25216;&#26415;&#26469;&#35299;&#20915;&#20559;&#22909;&#23398;&#20064;&#20013;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Preference learning is a key technology for aligning language models with human values. Reinforcement Learning from Human Feedback (RLHF) is a model based algorithm to optimize preference learning, which first fitting a reward model for preference score, and then optimizing generating policy with on-policy PPO algorithm to maximize the reward. The processing of RLHF is complex, time-consuming and unstable. Direct Preference Optimization (DPO) algorithm using off-policy algorithm to direct optimize generating policy and eliminating the need for reward model, which is data efficient and stable. DPO use Bradley-Terry model and log-loss which leads to over-fitting to the preference data at the expense of ignoring KL-regularization term when preference near deterministic. IPO uses a root-finding pairwise MSE loss to solve the ignoring KL-regularization problem, and learning an optimal policy. But IPO's pairwise loss still can't s make the KL-regularization to work. In this paper, we design 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#22686;&#24378;&#23545;&#35805;&#25351;&#23548;&#30340;&#36890;&#29992;&#20449;&#24687;&#25277;&#21462;&#35843;&#20248;&#26694;&#26550;&#65288;YAYI-UIE&#65289;&#65292;&#21033;&#29992;&#23545;&#35805;&#25968;&#25454;&#21644;&#20449;&#24687;&#25277;&#21462;&#25968;&#25454;&#20849;&#21516;&#22686;&#24378;&#20449;&#24687;&#25277;&#21462;&#24615;&#33021;&#65292;&#22312;&#20013;&#25991;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#19994;&#30028;&#39046;&#20808;&#30340;&#24615;&#33021;&#65292;&#22312;&#33521;&#25991;&#25968;&#25454;&#38598;&#19978;&#20063;&#36798;&#21040;&#20102;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2312.15548</link><description>&lt;p&gt;
YAYI-UIE: &#19968;&#20010;&#22686;&#24378;&#23545;&#35805;&#25351;&#23548;&#30340;&#36890;&#29992;&#20449;&#24687;&#25277;&#21462;&#35843;&#20248;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
YAYI-UIE: A Chat-Enhanced Instruction Tuning Framework for Universal Information Extraction. (arXiv:2312.15548v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.15548
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#22686;&#24378;&#23545;&#35805;&#25351;&#23548;&#30340;&#36890;&#29992;&#20449;&#24687;&#25277;&#21462;&#35843;&#20248;&#26694;&#26550;&#65288;YAYI-UIE&#65289;&#65292;&#21033;&#29992;&#23545;&#35805;&#25968;&#25454;&#21644;&#20449;&#24687;&#25277;&#21462;&#25968;&#25454;&#20849;&#21516;&#22686;&#24378;&#20449;&#24687;&#25277;&#21462;&#24615;&#33021;&#65292;&#22312;&#20013;&#25991;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#19994;&#30028;&#39046;&#20808;&#30340;&#24615;&#33021;&#65292;&#22312;&#33521;&#25991;&#25968;&#25454;&#38598;&#19978;&#20063;&#36798;&#21040;&#20102;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#25277;&#21462;&#20219;&#21153;&#30340;&#38590;&#28857;&#22312;&#20110;&#22788;&#29702;&#29305;&#23450;&#20219;&#21153;&#30340;&#26631;&#31614;&#27169;&#24335;&#21644;&#24322;&#26500;&#25968;&#25454;&#32467;&#26500;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#26469;&#32479;&#19968;&#24314;&#27169;&#19981;&#21516;&#30340;&#20449;&#24687;&#25277;&#21462;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#29616;&#26377;&#26041;&#27861;&#22312;&#38500;&#20102;&#33521;&#35821;&#20197;&#22806;&#30340;&#20013;&#25991;&#35821;&#35328;&#30340;&#20449;&#24687;&#25552;&#21462;&#33021;&#21147;&#19978;&#23384;&#22312;&#19981;&#36275;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#22686;&#24378;&#23545;&#35805;&#25351;&#23548;&#30340;&#36890;&#29992;&#20449;&#24687;&#25277;&#21462;&#35843;&#20248;&#26694;&#26550;&#65288;YAYI-UIE&#65289;&#65292;&#25903;&#25345;&#20013;&#25991;&#21644;&#33521;&#25991;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;&#23545;&#35805;&#25968;&#25454;&#21644;&#20449;&#24687;&#25277;&#21462;&#25968;&#25454;&#20849;&#21516;&#22686;&#24378;&#20449;&#24687;&#25277;&#21462;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#20013;&#25991;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#19994;&#30028;&#39046;&#20808;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#22312;&#26377;&#30417;&#30563;&#21644;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#22312;&#33521;&#25991;&#25968;&#25454;&#38598;&#19978;&#20063;&#36798;&#21040;&#20102;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The difficulty of the information extraction task lies in dealing with the task-specific label schemas and heterogeneous data structures. Recent work has proposed methods based on large language models to uniformly model different information extraction tasks. However, these existing methods are deficient in their information extraction capabilities for Chinese languages other than English. In this paper, we propose an end-to-end chat-enhanced instruction tuning framework for universal information extraction (YAYI-UIE), which supports both Chinese and English. Specifically, we utilize dialogue data and information extraction data to enhance the information extraction performance jointly. Experimental results show that our proposed framework achieves state-of-the-art performance on Chinese datasets while also achieving comparable performance on English datasets under both supervised settings and zero-shot settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#25353;&#38656;&#20849;&#20056;&#36710;&#36742;&#27966;&#36963;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#20114;&#20449;&#24687;&#20316;&#20026;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#20869;&#22312;&#22870;&#21169;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#31639;&#27861;&#20013;&#21482;&#32771;&#34385;&#25910;&#20837;&#26368;&#22823;&#21270;&#32780;&#26080;&#27861;&#28385;&#36275;&#24322;&#24120;&#20998;&#24067;&#35831;&#27714;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2312.15195</link><description>&lt;p&gt;
&#20114;&#20449;&#24687;&#20316;&#20026;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#20869;&#22312;&#22870;&#21169;&#65292;&#29992;&#20110;&#25353;&#38656;&#20849;&#20056;&#30340;&#36710;&#36742;&#27966;&#36963;
&lt;/p&gt;
&lt;p&gt;
Mutual Information as Intrinsic Reward of Reinforcement Learning Agents for On-demand Ride Pooling. (arXiv:2312.15195v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.15195
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#25353;&#38656;&#20849;&#20056;&#36710;&#36742;&#27966;&#36963;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#20114;&#20449;&#24687;&#20316;&#20026;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#20869;&#22312;&#22870;&#21169;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#31639;&#27861;&#20013;&#21482;&#32771;&#34385;&#25910;&#20837;&#26368;&#22823;&#21270;&#32780;&#26080;&#27861;&#28385;&#36275;&#24322;&#24120;&#20998;&#24067;&#35831;&#27714;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25353;&#38656;&#20849;&#20056;&#26381;&#21153;&#30340;&#20986;&#29616;&#20801;&#35768;&#27599;&#36742;&#36710;&#21516;&#26102;&#20026;&#22810;&#21517;&#20056;&#23458;&#25552;&#20379;&#26381;&#21153;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#21496;&#26426;&#30340;&#25910;&#20837;&#65292;&#24182;&#20351;&#20056;&#23458;&#33021;&#20197;&#36739;&#20302;&#30340;&#20215;&#26684;&#26053;&#34892;&#65292;&#32780;&#19981;&#20687;UberX&#21644;Lyft&#31561;&#20986;&#31199;&#36710;/&#25353;&#38656;&#26381;&#21153;&#21482;&#33021;&#20026;&#19968;&#21517;&#20056;&#23458;&#20998;&#37197;&#19968;&#36742;&#36710;&#12290;&#23613;&#31649;&#25353;&#38656;&#20849;&#20056;&#26381;&#21153;&#21487;&#20197;&#24102;&#26469;&#22914;&#27492;&#22810;&#30340;&#22909;&#22788;&#65292;&#20294;&#20849;&#20056;&#26381;&#21153;&#38656;&#35201;&#19968;&#20010;&#26126;&#30830;&#23450;&#20041;&#30340;&#21305;&#37197;&#31574;&#30053;&#65292;&#20197;&#26368;&#22823;&#31243;&#24230;&#22320;&#20026;&#25152;&#26377;&#21508;&#26041;&#65288;&#20056;&#23458;&#65292;&#21496;&#26426;&#65292;&#32858;&#21512;&#20844;&#21496;&#21644;&#29615;&#22659;&#65289;&#25552;&#20379;&#21033;&#30410;&#65292;&#20854;&#20013;&#21306;&#22495;&#35843;&#24230;&#36710;&#36742;&#23545;&#21305;&#37197;&#21644;&#25910;&#20837;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#29616;&#26377;&#31639;&#27861;&#36890;&#24120;&#20165;&#32771;&#34385;&#25910;&#20837;&#26368;&#22823;&#21270;&#65292;&#36825;&#20351;&#24471;&#35831;&#27714;&#20855;&#26377;&#24322;&#24120;&#20998;&#24067;&#30340;&#20056;&#23458;&#38590;&#20197;&#33719;&#24471;&#20056;&#36710;&#12290;&#22914;&#20309;&#22312;&#30830;&#20445;&#21512;&#29702;&#35831;&#27714;&#20998;&#37197;&#30340;&#21516;&#26102;&#22686;&#21152;&#25910;&#20837;&#65292;&#23545;&#20849;&#20056;&#26381;&#21153;&#20844;&#21496;&#65288;&#32858;&#21512;&#20844;&#21496;&#65289;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36710;&#36742;&#27966;&#36963;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20849;&#20056;&#26381;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of on-demand ride pooling services allows each vehicle to serve multiple passengers at a time, thus increasing drivers' income and enabling passengers to travel at lower prices than taxi/car on-demand services (only one passenger can be assigned to a car at a time like UberX and Lyft). Although on-demand ride pooling services can bring so many benefits, ride pooling services need a well-defined matching strategy to maximize the benefits for all parties (passengers, drivers, aggregation companies and environment), in which the regional dispatching of vehicles has a significant impact on the matching and revenue. Existing algorithms often only consider revenue maximization, which makes it difficult for requests with unusual distribution to get a ride. How to increase revenue while ensuring a reasonable assignment of requests brings a challenge to ride pooling service companies (aggregation companies). In this paper, we propose a framework for vehicle dispatching for ride po
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#24322;&#27493;&#31232;&#30095;&#21270;&#37327;&#21270;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65288;TEASQ-Fed&#65289;&#65292;&#21033;&#29992;&#36793;&#32536;&#35774;&#22791;&#30340;&#24182;&#34892;&#21442;&#19982;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#35774;&#22791;&#25302;&#24930;&#35757;&#32451;&#21644;&#36890;&#20449;&#29942;&#39048;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2312.15186</link><description>&lt;p&gt;
&#39640;&#25928;&#24322;&#27493;&#31232;&#30095;&#21270;&#37327;&#21270;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Efficient Asynchronous Federated Learning with Sparsification and Quantization. (arXiv:2312.15186v2 [cs.DC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.15186
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#24322;&#27493;&#31232;&#30095;&#21270;&#37327;&#21270;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65288;TEASQ-Fed&#65289;&#65292;&#21033;&#29992;&#36793;&#32536;&#35774;&#22791;&#30340;&#24182;&#34892;&#21442;&#19982;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#35774;&#22791;&#25302;&#24930;&#35757;&#32451;&#21644;&#36890;&#20449;&#29942;&#39048;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#25454;&#20998;&#24067;&#22312;&#22810;&#20010;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#24773;&#20917;&#19979;&#65292;&#32852;&#37030;&#23398;&#20064;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#65292;&#23427;&#21487;&#20197;&#22312;&#19981;&#20256;&#36755;&#21407;&#22987;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21512;&#20316;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#20256;&#32479;&#30340;&#32852;&#37030;&#23398;&#20064;&#20351;&#29992;&#21442;&#25968;&#26381;&#21153;&#22120;&#21644;&#22823;&#37327;&#36793;&#32536;&#35774;&#22791;&#26469;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#65292;&#27599;&#36718;&#36873;&#25321;&#20960;&#20010;&#35774;&#22791;&#21442;&#19982;&#12290;&#28982;&#32780;&#65292;&#26377;&#20123;&#35774;&#22791;&#21487;&#33021;&#20250;&#25302;&#24930;&#35757;&#32451;&#36807;&#31243;&#29978;&#33267;&#23548;&#33268;&#31995;&#32479;&#23849;&#28291;&#65292;&#32780;&#20854;&#20182;&#31354;&#38386;&#35774;&#22791;&#21017;&#38386;&#32622;&#19981;&#29992;&#12290;&#30001;&#20110;&#35774;&#22791;&#21644;&#26381;&#21153;&#22120;&#20043;&#38388;&#30340;&#24102;&#23485;&#30456;&#23545;&#36739;&#20302;&#65292;&#20013;&#38388;&#25968;&#25454;&#30340;&#36890;&#20449;&#25104;&#20026;&#29942;&#39048;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#31232;&#30095;&#21270;&#21644;&#37327;&#21270;&#30340;&#26102;&#38388;&#39640;&#25928;&#24322;&#27493;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65288;TEASQ-Fed&#65289;&#65292;&#23427;&#21487;&#20197;&#20805;&#20998;&#21033;&#29992;&#36793;&#32536;&#35774;&#22791;&#20027;&#21160;&#21442;&#19982;&#35757;&#32451;&#36807;&#31243;&#12290;&#25105;&#20204;&#21033;&#29992;&#25511;&#21046;&#21442;&#25968;&#26469;&#36873;&#25321;&#36866;&#24403;&#25968;&#37327;&#30340;&#24182;&#34892;&#36793;&#32536;&#35774;&#22791;&#12290;
&lt;/p&gt;
&lt;p&gt;
While data is distributed in multiple edge devices, Federated Learning (FL) is attracting more and more attention to collaboratively train a machine learning model without transferring raw data. FL generally exploits a parameter server and a large number of edge devices during the whole process of the model training, while several devices are selected in each round. However, straggler devices may slow down the training process or even make the system crash during training. Meanwhile, other idle edge devices remain unused. As the bandwidth between the devices and the server is relatively low, the communication of intermediate data becomes a bottleneck. In this paper, we propose Time-Efficient Asynchronous federated learning with Sparsification and Quantization, i.e., TEASQ-Fed. TEASQ-Fed can fully exploit edge devices to asynchronously participate in the training process by actively applying for tasks. We utilize control parameters to choose an appropriate number of parallel edge device
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22522;&#20110;Forward-Forward&#31639;&#27861;&#23558;&#20854;&#24212;&#29992;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#30340;&#35757;&#32451;&#65292;&#37319;&#29992;&#20102;&#26032;&#39062;&#30340;&#31354;&#38388;&#25193;&#23637;&#26631;&#31614;&#25216;&#26415;&#65292;&#22312;MNIST&#25163;&#20889;&#25968;&#23383;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;99.16%&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2312.14924</link><description>&lt;p&gt;
&#20351;&#29992;Forward-Forward&#31639;&#27861;&#35757;&#32451;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Training Convolutional Neural Networks with the Forward-Forward algorithm. (arXiv:2312.14924v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.14924
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;Forward-Forward&#31639;&#27861;&#23558;&#20854;&#24212;&#29992;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#30340;&#35757;&#32451;&#65292;&#37319;&#29992;&#20102;&#26032;&#39062;&#30340;&#31354;&#38388;&#25193;&#23637;&#26631;&#31614;&#25216;&#26415;&#65292;&#22312;MNIST&#25163;&#20889;&#25968;&#23383;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;99.16%&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#22270;&#20687;&#36827;&#34892;&#20998;&#26512;&#30340;&#26368;&#26032;&#25104;&#21151;&#20960;&#20046;&#20840;&#37096;&#23454;&#29616;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#12290;&#36825;&#20123;CNN&#20197;&#21450;&#25152;&#26377;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#35757;&#32451;&#37117;&#20351;&#29992;&#20102;&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#65292;&#23558;&#32593;&#32476;&#30340;&#36755;&#20986;&#19982;&#26399;&#26395;&#32467;&#26524;&#36827;&#34892;&#27604;&#36739;&#65292;&#21033;&#29992;&#24046;&#24322;&#26469;&#35843;&#25972;&#32593;&#32476;&#26435;&#37325;&#20197;&#36798;&#21040;&#26399;&#26395;&#30340;&#36755;&#20986;&#12290;&#22312;2022&#24180;&#30340;&#19968;&#31687;&#39044;&#21360;&#26412;&#20013;&#65292;Geoffrey Hinton&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#30340;&#35757;&#32451;&#26041;&#24335;&#65292;&#21363;&#22312;&#32593;&#32476;&#30340;&#36755;&#20837;&#20013;&#21516;&#26102;&#20256;&#36882;&#26399;&#26395;&#30340;&#32467;&#26524;&#21644;&#22270;&#20687;&#12290;&#36825;&#31181;&#31216;&#20026;Forward Forward&#65288;FF&#65289;&#31639;&#27861;&#21040;&#30446;&#21069;&#20026;&#27490;&#20165;&#22312;&#20840;&#36830;&#25509;&#32593;&#32476;&#20013;&#20351;&#29992;&#36807;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;FF&#33539;&#24335;&#25193;&#23637;&#21040;CNN&#20013;&#12290;&#25105;&#20204;&#30340;FF&#35757;&#32451;&#30340;CNN&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31354;&#38388;&#25193;&#23637;&#26631;&#31614;&#25216;&#26415;&#65292;&#22312;MNIST&#25163;&#20889;&#25968;&#23383;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;99.16%&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19981;&#21516;&#36229;&#21442;&#25968;&#23545;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent successes in analyzing images with deep neural networks are almost exclusively achieved with Convolutional Neural Networks (CNNs). The training of these CNNs, and in fact of all deep neural network architectures, uses the backpropagation algorithm where the output of the network is compared with the desired result and the difference is then used to tune the weights of the network towards the desired outcome. In a 2022 preprint, Geoffrey Hinton suggested an alternative way of training which passes the desired results together with the images at the input of the network. This so called Forward Forward (FF) algorithm has up to now only been used in fully connected networks. In this paper, we show how the FF paradigm can be extended to CNNs. Our FF-trained CNN, featuring a novel spatially-extended labeling technique, achieves a classification accuracy of 99.16% on the MNIST hand-written digits dataset. We show how different hyperparameters affect the performance of the proposed 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#36873;&#21306;&#35268;&#21010;&#38382;&#39064;&#22312;&#24179;&#38754;&#22270;&#20013;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;&#22312;$\lambda$-&#22806;&#24179;&#38754;&#22270;&#20013;&#65292;&#38382;&#39064;&#21487;&#20197;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#35299;&#20915;&#65292;&#20294;&#22312;&#19968;&#33324;&#30340;&#24179;&#38754;&#22270;&#20013;&#26159;NP-complete&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#22312;&#20505;&#36873;&#20154;&#25968;&#36739;&#22823;&#26102;&#30340;&#36817;&#20284;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2312.14721</link><description>&lt;p&gt;
&#36873;&#21306;&#65288;gerrymandering&#65289;&#24179;&#38754;&#22270;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Gerrymandering Planar Graphs. (arXiv:2312.14721v2 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.14721
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#36873;&#21306;&#35268;&#21010;&#38382;&#39064;&#22312;&#24179;&#38754;&#22270;&#20013;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;&#22312;$\lambda$-&#22806;&#24179;&#38754;&#22270;&#20013;&#65292;&#38382;&#39064;&#21487;&#20197;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#35299;&#20915;&#65292;&#20294;&#22312;&#19968;&#33324;&#30340;&#24179;&#38754;&#22270;&#20013;&#26159;NP-complete&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#22312;&#20505;&#36873;&#20154;&#25968;&#36739;&#22823;&#26102;&#30340;&#36817;&#20284;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#36873;&#21306;&#35268;&#21010;&#38382;&#39064;&#65288;gerrymandering&#65289;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;&#25968;&#23398;&#19978;&#65292;&#36873;&#21306;&#35774;&#35745;&#32773;&#65288;gerrymanderer&#65289;&#35797;&#22270;&#23558;&#19968;&#20010;&#21152;&#26435;&#22270;&#21010;&#20998;&#20026;$k$&#20010;&#36830;&#36890;&#20998;&#37327;&#65288;&#36873;&#21306;&#65289;&#65292;&#20197;&#20351;&#20854;&#20505;&#36873;&#20154;&#65288;&#25919;&#20826;&#65289;&#36194;&#24471;&#23613;&#21487;&#33021;&#22810;&#30340;&#36873;&#21306;&#12290;&#20043;&#21069;&#30340;&#24037;&#20316;&#20027;&#35201;&#20851;&#27880;&#22270;&#26159;&#36335;&#24452;&#25110;&#26641;&#30340;&#29305;&#27530;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#37325;&#28857;&#26159;&#22270;&#26159;&#24179;&#38754;&#22270;&#30340;&#23454;&#38469;&#24773;&#20917;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;$\lambda$-&#22806;&#24179;&#38754;&#22270;&#20013;&#65292;&#24403;&#20505;&#36873;&#20154;&#25968;&#21644;$\lambda$&#26159;&#24120;&#25968;&#65292;&#39030;&#28857;&#26435;&#37325;&#65288;&#25237;&#31080;&#26435;&#37325;&#65289;&#26159;&#22810;&#39033;&#24335;&#30028;&#38480;&#26102;&#65292;&#36873;&#21306;&#35268;&#21010;&#38382;&#39064;&#21487;&#20197;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#35299;&#20915;&#12290;&#30456;&#21453;&#65292;&#22312;&#19968;&#33324;&#30340;&#24179;&#38754;&#22270;&#20013;&#65292;&#21363;&#20351;&#21482;&#26377;&#20004;&#20010;&#20505;&#36873;&#20154;&#65292;&#35813;&#38382;&#39064;&#20063;&#26159;NP&#23436;&#20840;&#30340;&#12290;&#36825;&#20419;&#20351;&#25105;&#20204;&#30740;&#31350;&#36873;&#21306;&#35268;&#21010;&#24179;&#38754;&#22270;&#30340;&#36817;&#20284;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#24403;&#20505;&#36873;&#20154;&#25968;&#36739;&#22823;&#26102;&#65292;&#25105;&#20204;&#35777;&#26126;&#24456;&#38590;&#21306;&#20998;&#36873;&#21306;&#35774;&#35745;&#32773;&#21487;&#20197;&#20174;&#36825;&#20123;&#39033;&#20013;&#33073;&#39062;&#32780;&#20986;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the computational complexity of the map redistricting problem (gerrymandering). Mathematically, the electoral district designer (gerrymanderer) attempts to partition a weighted graph into $k$ connected components (districts) such that its candidate (party) wins as many districts as possible. Prior work has principally concerned the special cases where the graph is a path or a tree. Our focus concerns the realistic case where the graph is planar. We prove that the gerrymandering problem is solvable in polynomial time in $\lambda$-outerplanar graphs, when the number of candidates and $\lambda$ are constants and the vertex weights (voting weights) are polynomially bounded. In contrast, the problem is NP-complete in general planar graphs even with just two candidates. This motivates the study of approximation algorithms for gerrymandering planar graphs. However, when the number of candidates is large, we prove it is hard to distinguish between instances where the gerrymanderer can
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;GAN&#37492;&#21035;&#22120;&#31867;&#21035;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#30340;&#29305;&#24449;&#39592;&#24178;&#32593;&#32476;&#29983;&#25104;&#39640;&#24230;&#36924;&#30495;&#30340;&#22270;&#20687;&#65292;&#24182;&#24341;&#20837;&#20102;&#26356;&#22909;&#30340;&#19978;&#19979;&#25991;&#24314;&#27169;&#21644;&#20132;&#21449;&#27880;&#24847;&#21147;&#25216;&#26415;&#65292;&#29983;&#25104;&#26356;&#22810;&#26679;&#21270;&#30340;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2312.13314</link><description>&lt;p&gt;
&#35299;&#38145;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#39592;&#24178;&#29992;&#20110;&#35821;&#20041;&#22270;&#20687;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Unlocking Pre-trained Image Backbones for Semantic Image Synthesis. (arXiv:2312.13314v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.13314
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;GAN&#37492;&#21035;&#22120;&#31867;&#21035;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#30340;&#29305;&#24449;&#39592;&#24178;&#32593;&#32476;&#29983;&#25104;&#39640;&#24230;&#36924;&#30495;&#30340;&#22270;&#20687;&#65292;&#24182;&#24341;&#20837;&#20102;&#26356;&#22909;&#30340;&#19978;&#19979;&#25991;&#24314;&#27169;&#21644;&#20132;&#21449;&#27880;&#24847;&#21147;&#25216;&#26415;&#65292;&#29983;&#25104;&#26356;&#22810;&#26679;&#21270;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#22270;&#20687;&#21512;&#25104;&#26159;&#19968;&#39033;&#37325;&#35201;&#30340;&#26465;&#20214;&#24615;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#29992;&#25143;&#25552;&#20379;&#30340;&#35821;&#20041;&#26631;&#31614;&#22270;&#29983;&#25104;&#22270;&#20687;&#65292;&#21516;&#26102;&#25511;&#21046;&#29983;&#25104;&#22270;&#20687;&#30340;&#20869;&#23481;&#21644;&#31354;&#38388;&#24067;&#23616;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;GAN&#37492;&#21035;&#22120;&#31867;&#21035;&#65292;&#29992;&#20110;&#35821;&#20041;&#22270;&#20687;&#21512;&#25104;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#30340;&#29305;&#24449;&#39592;&#24178;&#32593;&#32476;&#29983;&#25104;&#39640;&#24230;&#36924;&#30495;&#30340;&#22270;&#20687;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#22120;&#26550;&#26500;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#19978;&#19979;&#25991;&#24314;&#27169;&#33021;&#21147;&#65292;&#24182;&#20351;&#29992;&#20132;&#21449;&#27880;&#24847;&#21147;&#23558;&#22122;&#22768;&#27880;&#20837;&#28508;&#21464;&#37327;&#65292;&#20174;&#32780;&#29983;&#25104;&#26356;&#22810;&#26679;&#21270;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic image synthesis, i.e., generating images from user-provided semantic label maps, is an important conditional image generation task as it allows to control both the content as well as the spatial layout of generated images. Although diffusion models have pushed the state of the art in generative image modeling, the iterative nature of their inference process makes them computationally demanding. Other approaches such as GANs are more efficient as they only need a single feed-forward pass for generation, but the image quality tends to suffer on large and diverse datasets. In this work, we propose a new class of GAN discriminators for semantic image synthesis that generates highly realistic images by exploiting feature backbone networks pre-trained for tasks such as image classification. We also introduce a new generator architecture with better context modeling and using cross-attention to inject noise into latent variables, leading to more diverse generated images. Our model, w
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22522;&#20110;"&#24425;&#31080;&#31080;&#25454;&#20551;&#35774;"&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#31232;&#30095;&#23376;&#32593;&#32476;&#21644;FSO&#36827;&#34892;&#20219;&#21153;&#22686;&#37327;&#23398;&#20064;&#12289;&#23569;&#26679;&#26412;&#31867;&#22686;&#37327;&#23398;&#20064;&#21644;&#35270;&#39057;&#22686;&#37327;&#23398;&#20064;&#65292;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#21644;&#26377;&#25928;&#30340;&#26435;&#37325;&#37325;&#29992;&#12290;</title><link>http://arxiv.org/abs/2312.11973</link><description>&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;: &#38754;&#21521;&#35270;&#39057;&#34920;&#31034;&#30340;&#20813;&#36951;&#24536;&#20248;&#32988;&#23376;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Continual Learning: Forget-free Winning Subnetworks for Video Representations. (arXiv:2312.11973v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.11973
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22522;&#20110;"&#24425;&#31080;&#31080;&#25454;&#20551;&#35774;"&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#31232;&#30095;&#23376;&#32593;&#32476;&#21644;FSO&#36827;&#34892;&#20219;&#21153;&#22686;&#37327;&#23398;&#20064;&#12289;&#23569;&#26679;&#26412;&#31867;&#22686;&#37327;&#23398;&#20064;&#21644;&#35270;&#39057;&#22686;&#37327;&#23398;&#20064;&#65292;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#21644;&#26377;&#25928;&#30340;&#26435;&#37325;&#37325;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;"&#24425;&#31080;&#31080;&#25454;&#20551;&#35774;"&#65288;LTH&#65289;&#30340;&#21551;&#21457;&#65292;&#35813;&#20551;&#35774;&#24378;&#35843;&#22312;&#36739;&#22823;&#30340;&#23494;&#38598;&#32593;&#32476;&#20013;&#23384;&#22312;&#39640;&#25928;&#23376;&#32593;&#32476;&#65292;&#30740;&#31350;&#20102;&#22312;&#36866;&#24403;&#30340;&#31232;&#30095;&#26465;&#20214;&#19979;&#34920;&#29616;&#20248;&#31168;&#30340;&#20248;&#32988;&#23376;&#32593;&#32476;&#65288;WSN&#65289;&#22312;&#21508;&#31181;&#36830;&#32493;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;&#23427;&#21033;&#29992;&#26469;&#33258;&#23494;&#38598;&#32593;&#32476;&#30340;&#39044;&#20808;&#23384;&#22312;&#30340;&#26435;&#37325;&#65292;&#22312;&#20219;&#21153;&#22686;&#37327;&#23398;&#20064;&#65288;TIL&#65289;&#22330;&#26223;&#20013;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#12290;&#22312;&#23569;&#26679;&#26412;&#31867;&#22686;&#37327;&#23398;&#20064;&#65288;FSCIL&#65289;&#20013;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#31216;&#20026;&#36719;&#23376;&#32593;&#32476;&#65288;SoftNet&#65289;&#30340;WSN&#21464;&#20307;&#65292;&#20197;&#38450;&#27490;&#25968;&#25454;&#26679;&#26412;&#31232;&#32570;&#26102;&#30340;&#36807;&#25311;&#21512;&#12290;&#27492;&#22806;&#65292;&#32771;&#34385;&#20102;WSN&#26435;&#37325;&#30340;&#31232;&#30095;&#37325;&#29992;&#65292;&#29992;&#20110;&#35270;&#39057;&#22686;&#37327;&#23398;&#20064;&#65288;VIL&#65289;&#12290;&#32771;&#34385;&#20102;&#22312;WSN&#20013;&#20351;&#29992;&#20613;&#31435;&#21494;&#23376;&#31070;&#32463;&#36816;&#31639;&#22120;&#65288;FSO&#65289;&#65292;&#23427;&#33021;&#22815;&#23545;&#35270;&#39057;&#36827;&#34892;&#32039;&#20945;&#32534;&#30721;&#65292;&#24182;&#22312;&#19981;&#21516;&#24102;&#23485;&#19979;&#35782;&#21035;&#21487;&#37325;&#29992;&#30340;&#23376;&#32593;&#32476;&#12290;&#25105;&#20204;&#23558;FSO&#38598;&#25104;&#21040;&#19981;&#21516;&#30340;&#36830;&#32493;&#23398;&#20064;&#26550;&#26500;&#20013;&#65292;&#21253;&#25324;VIL&#12289;TIL&#21644;FSCIL&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspired by the Lottery Ticket Hypothesis (LTH), which highlights the existence of efficient subnetworks within larger, dense networks, a high-performing Winning Subnetwork (WSN) in terms of task performance under appropriate sparsity conditions is considered for various continual learning tasks. It leverages pre-existing weights from dense networks to achieve efficient learning in Task Incremental Learning (TIL) scenarios. In Few-Shot Class Incremental Learning (FSCIL), a variation of WSN referred to as the Soft subnetwork (SoftNet) is designed to prevent overfitting when the data samples are scarce. Furthermore, the sparse reuse of WSN weights is considered for Video Incremental Learning (VIL). The use of Fourier Subneural Operator (FSO) within WSN is considered. It enables compact encoding of videos and identifies reusable subnetworks across varying bandwidths. We have integrated FSO into different architectural frameworks for continual learning, including VIL, TIL, and FSCIL. Our c
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"&#21407;&#25991;&#25913;&#20889;"&#30340;&#20219;&#21153;&#26469;&#22788;&#29702;&#38271;&#25991;&#26412;&#38382;&#31572;&#65292;&#36890;&#36807;&#20302;&#25104;&#26412;&#39640;&#25928;&#30340;&#26041;&#27861;&#25104;&#21151;&#25193;&#23637;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#33267;32k&#65292;&#24182;&#22312;&#22810;&#25991;&#26723;&#38382;&#31572;&#20013;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2312.11193</link><description>&lt;p&gt;
"&#21407;&#25991;&#25913;&#20889;"&#25552;&#39640;&#20102;&#39640;&#31934;&#24230;&#38271;&#25991;&#26412;&#38382;&#31572;&#30340;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
"Paraphrasing The Original Text" Makes High Accuracy Long-Context QA. (arXiv:2312.11193v6 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.11193
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"&#21407;&#25991;&#25913;&#20889;"&#30340;&#20219;&#21153;&#26469;&#22788;&#29702;&#38271;&#25991;&#26412;&#38382;&#31572;&#65292;&#36890;&#36807;&#20302;&#25104;&#26412;&#39640;&#25928;&#30340;&#26041;&#27861;&#25104;&#21151;&#25193;&#23637;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#33267;32k&#65292;&#24182;&#22312;&#22810;&#25991;&#26723;&#38382;&#31572;&#20013;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#38754;&#23545;&#38271;&#25991;&#26412;&#26102;&#65292;&#22823;&#22810;&#25968;&#24320;&#28304;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#38480;&#21046;&#22312;4k&#20197;&#20869;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#33021;&#21147;&#12290;&#21363;&#20351;&#26159;&#20855;&#26377;&#26356;&#38271;&#19978;&#19979;&#25991;&#31383;&#21475;&#30340;&#27169;&#22411;&#20063;&#26080;&#27861;&#22312;&#38271;&#19978;&#19979;&#25991;&#38382;&#39064;&#19978;&#20445;&#35777;&#20196;&#20154;&#28385;&#24847;&#30340;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20174;&#35757;&#32451;&#25968;&#25454;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#25552;&#39640;&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#33021;&#21147;&#38656;&#35201;&#30340;&#26159;"&#26377;&#25928;"&#32780;&#19981;&#20165;&#20165;&#26159;"&#38271;"&#30340;&#25968;&#25454;&#12290;&#22522;&#20110;&#36825;&#20010;&#27934;&#35265;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;"&#21407;&#25991;&#25913;&#20889;"&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#19968;&#31181;&#20302;&#25104;&#26412;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#25104;&#21151;&#23558;&#29616;&#26377;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#25193;&#23637;&#21040;32k&#12290;&#25105;&#20204;&#30340;&#24494;&#35843;&#27169;&#22411;&#22312;&#20855;&#26377;&#30456;&#36817;&#35268;&#27169;&#30340;&#27169;&#22411;&#20013;&#22312;&#22810;&#25991;&#26723;&#38382;&#31572;&#26041;&#38754;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#12290;&#27169;&#22411;&#21644;&#35757;&#32451;&#25968;&#25454;&#24050;&#32463;&#22312;HuggingFace&#65288;https://huggingface.co/yuyijiong/Qwen-14b-chat-yarn-32k&#65289;&#21644;WiseModel&#65288;https://wisemodel.cn/models/yuyijiong/Qwen-14b-chat-yarn-32k&#65289;&#19978;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most open-source generative language models currently have a context window of no more than 4k, limiting their ability when facing long text. Even models with longer context windows cannot guarantee satisfactory accuracy on long-context problems. To tackle this issue, we explore from the perspective of training data and theoretically demonstrate that improving the capability to handle long contexts requires "effective" rather than simply "long" data. Based on this insight, we propose using the "original text paraphrasing" task and successfully extend the context window of existing models to 32k through a low-cost and effective method. Our fine-tuned model achieves state-of-the-art accuracy in multi-document-QA among models of comparable scale. The model and training data have been made available on HuggingFace(https://huggingface.co/yuyijiong/Qwen-14b-chat-yarn-32k) and WiseModel(https://wisemodel.cn/models/yuyijiong/Qwen-14b-chat-yarn-32k).
&lt;/p&gt;</description></item><item><title>SAME&#26159;&#19968;&#31181;&#38450;&#24481;&#27169;&#22411;&#25552;&#21462;&#25915;&#20987;&#30340;&#26032;&#26041;&#27861;&#65292;&#22522;&#20110;&#26679;&#26412;&#37325;&#24314;&#30340;&#27010;&#24565;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#35775;&#38382;&#65292;&#24182;&#19988;&#20855;&#26377;&#26356;&#23454;&#29992;&#30340;&#20445;&#25252;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2312.10578</link><description>&lt;p&gt;
SAME: &#38450;&#33539;&#27169;&#22411;&#25552;&#21462;&#25915;&#20987;&#30340;&#26679;&#26412;&#37325;&#24314;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SAME: Sample Reconstruction against Model Extraction Attacks. (arXiv:2312.10578v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.10578
&lt;/p&gt;
&lt;p&gt;
SAME&#26159;&#19968;&#31181;&#38450;&#24481;&#27169;&#22411;&#25552;&#21462;&#25915;&#20987;&#30340;&#26032;&#26041;&#27861;&#65292;&#22522;&#20110;&#26679;&#26412;&#37325;&#24314;&#30340;&#27010;&#24565;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#35775;&#38382;&#65292;&#24182;&#19988;&#20855;&#26377;&#26356;&#23454;&#29992;&#30340;&#20445;&#25252;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#30340;&#37096;&#32626;&#38656;&#35201;&#22823;&#37327;&#30340;&#36164;&#28304;&#21644;&#20808;&#36827;&#30340;&#35745;&#31639;&#22522;&#30784;&#35774;&#26045;&#12290;&#20316;&#20026;&#35299;&#20915;&#26041;&#26696;&#65292;&#26426;&#22120;&#23398;&#20064;&#21363;&#26381;&#21153;&#65288;MLaaS&#65289;&#24212;&#36816;&#32780;&#29983;&#65292;&#38477;&#20302;&#20102;&#29992;&#25143;&#21457;&#24067;&#25110;&#20135;&#21697;&#21270;&#20182;&#20204;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#38376;&#27099;&#12290;&#28982;&#32780;&#65292;&#20043;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#24378;&#35843;&#20102;&#19982;MLaaS&#30456;&#20851;&#30340;&#28508;&#22312;&#38544;&#31169;&#21644;&#23433;&#20840;&#39118;&#38505;&#65292;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#23041;&#32961;&#26159;&#27169;&#22411;&#25552;&#21462;&#25915;&#20987;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26377;&#35768;&#22810;&#38450;&#24481;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#23427;&#20204;&#37117;&#23384;&#22312;&#19981;&#20999;&#23454;&#38469;&#30340;&#20551;&#35774;&#21644;&#27867;&#21270;&#38382;&#39064;&#65292;&#20351;&#23427;&#20204;&#23545;&#21487;&#38752;&#30340;&#20445;&#25252;&#19981;&#22815;&#23454;&#29992;&#12290;&#21463;&#21040;&#36825;&#20123;&#38480;&#21046;&#30340;&#39537;&#21160;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#26679;&#26412;&#37325;&#24314;&#27010;&#24565;&#30340;&#26032;&#22411;&#38450;&#24481;&#26426;&#21046;SAME&#12290;&#35813;&#31574;&#30053;&#23545;&#38450;&#24481;&#32773;&#30340;&#33021;&#21147;&#35201;&#27714;&#26368;&#23567;&#65292;&#28040;&#38500;&#20102;&#23545;&#36741;&#21161;&#30340;&#31163;&#32676;&#20998;&#24067;&#65288;OOD&#65289;&#25968;&#25454;&#38598;&#12289;&#29992;&#25143;&#26597;&#35810;&#21382;&#21490;&#12289;&#30333;&#30418;&#27169;&#22411;&#35775;&#38382;&#21644;&#39069;&#22806;&#24178;&#39044;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
While deep learning models have shown significant performance across various domains, their deployment needs extensive resources and advanced computing infrastructure. As a solution, Machine Learning as a Service (MLaaS) has emerged, lowering the barriers for users to release or productize their deep learning models. However, previous studies have highlighted potential privacy and security concerns associated with MLaaS, and one primary threat is model extraction attacks. To address this, there are many defense solutions but they suffer from unrealistic assumptions and generalization issues, making them less practical for reliable protection. Driven by these limitations, we introduce a novel defense mechanism, SAME, based on the concept of sample reconstruction. This strategy imposes minimal prerequisites on the defender's capabilities, eliminating the need for auxiliary Out-of-Distribution (OOD) datasets, user query history, white-box model access, and additional intervention during m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31526;&#21495;&#27169;&#24335;&#35268;&#21010;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32534;&#30721;&#38382;&#39064;&#20026;&#19968;&#20010;&#20844;&#24335;&#21487;&#20197;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#26377;&#25928;&#22320;&#23547;&#25214;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#35268;&#21010;&#22120;Patty&#22312;&#20170;&#24180;&#30340;IPC&#38382;&#39064;&#19978;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2312.09963</link><description>&lt;p&gt;
&#20855;&#26377;&#27169;&#24335;&#30340;&#31526;&#21495;&#25968;&#20540;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Symbolic Numeric Planning with Patterns. (arXiv:2312.09963v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.09963
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31526;&#21495;&#27169;&#24335;&#35268;&#21010;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32534;&#30721;&#38382;&#39064;&#20026;&#19968;&#20010;&#20844;&#24335;&#21487;&#20197;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#26377;&#25928;&#22320;&#23547;&#25214;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#35268;&#21010;&#22120;Patty&#22312;&#20170;&#24180;&#30340;IPC&#38382;&#39064;&#19978;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#32447;&#24615;&#25968;&#20540;&#35268;&#21010;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#65292;&#31216;&#20026;&#31526;&#21495;&#27169;&#24335;&#35268;&#21010;&#12290;&#32473;&#23450;&#19968;&#20010;&#35268;&#21010;&#38382;&#39064;&#928;&#65292;&#19968;&#20010;&#30028;&#38480;n&#21644;&#19968;&#20010;&#27169;&#24335; - &#23450;&#20041;&#20026;&#20219;&#24847;&#21160;&#20316;&#30340;&#24207;&#21015; - &#25105;&#20204;&#23558;&#23547;&#25214;&#19968;&#20010;&#38024;&#23545;&#928;&#21644;&#30028;&#38480;n&#30340;&#35745;&#21010;&#38382;&#39064;&#32534;&#30721;&#20026;&#19968;&#20010;&#20844;&#24335;&#65292;&#35813;&#20844;&#24335;&#30340;&#21464;&#37327;&#21644;/&#25110;&#23376;&#21477;&#27604;&#26368;&#20808;&#36827;&#30340;rolled-up&#21644;&#26494;&#24347;-&#26494;&#24347;-$\exists$&#32534;&#30721;&#26356;&#23569;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#20219;&#20309;&#32473;&#23450;&#30340;&#30028;&#38480;&#65292;&#21518;&#20004;&#31181;&#32534;&#30721;&#37117;&#19981;&#21487;&#33021;&#25214;&#21040;&#19968;&#20010;&#26377;&#25928;&#30340;&#35745;&#21010;&#65292;&#32780;&#25105;&#20204;&#30340;&#32534;&#30721;&#21487;&#20197;&#12290;&#22312;&#23454;&#39564;&#26041;&#38754;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20854;&#20182;6&#20010;&#35268;&#21010;&#31995;&#32479; - &#21253;&#25324;&#21442;&#21152;&#20170;&#24180;&#22269;&#38469;&#35268;&#21010;&#31454;&#36187;&#65288;IPC&#65289;&#30340;&#31995;&#32479; - &#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#35268;&#21010;&#22120;Patty&#22312;&#20170;&#24180;IPC&#38382;&#39064;&#19978;&#30340;&#38750;&#24120;&#22909;&#30340;&#27604;&#36739;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a novel approach for solving linear numeric planning problems, called Symbolic Pattern Planning. Given a planning problem $\Pi$, a bound $n$ and a pattern -- defined as an arbitrary sequence of actions -- we encode the problem of finding a plan for $\Pi$ with bound $n$ as a formula with fewer variables and/or clauses than the state-of-the-art rolled-up and relaxed-relaxed-$\exists$ encodings. More importantly, we prove that for any given bound, it is never the case that the latter two encodings allow finding a valid plan while ours does not. On the experimental side, we consider 6 other planning systems -- including the ones which participated in this year's International Planning Competition (IPC) -- and we show that our planner Patty has remarkably good comparative performances on this year's IPC problems.
&lt;/p&gt;</description></item><item><title>LLMind&#26159;&#19968;&#20010;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#20013;&#22830;&#21327;&#35843;&#22120;&#30340;AI&#26694;&#26550;&#65292;&#23558;LLMs&#19982;&#39046;&#22495;&#29305;&#23450;&#30340;AI&#27169;&#22359;&#25972;&#21512;&#65292;&#20351;&#24471;&#29289;&#32852;&#32593;&#35774;&#22791;&#33021;&#22815;&#26377;&#25928;&#21327;&#21516;&#25191;&#34892;&#22797;&#26434;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2312.09007</link><description>&lt;p&gt;
LLMind: &#20026;&#22797;&#26434;&#20219;&#21153;&#25191;&#34892;&#19982;AI&#21644;&#29289;&#32852;&#32593;&#36827;&#34892;&#21327;&#35843;&#30340;LLM&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
LLMind: Orchestrating AI and IoT with LLMs for Complex Task Execution. (arXiv:2312.09007v2 [cs.IT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.09007
&lt;/p&gt;
&lt;p&gt;
LLMind&#26159;&#19968;&#20010;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#20013;&#22830;&#21327;&#35843;&#22120;&#30340;AI&#26694;&#26550;&#65292;&#23558;LLMs&#19982;&#39046;&#22495;&#29305;&#23450;&#30340;AI&#27169;&#22359;&#25972;&#21512;&#65292;&#20351;&#24471;&#29289;&#32852;&#32593;&#35774;&#22791;&#33021;&#22815;&#26377;&#25928;&#21327;&#21516;&#25191;&#34892;&#22797;&#26434;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;LLMind&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#20013;&#22830;&#21327;&#35843;&#22120;&#30340;AI&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#23558;LLMs&#19982;&#39046;&#22495;&#29305;&#23450;&#30340;AI&#27169;&#22359;&#25972;&#21512;&#65292;&#20351;&#24471;&#29289;&#32852;&#32593;&#35774;&#22791;&#33021;&#22815;&#26377;&#25928;&#21327;&#21516;&#25191;&#34892;&#22797;&#26434;&#20219;&#21153;&#12290;LLMs&#36890;&#36807;&#29992;&#25143;&#21451;&#22909;&#30340;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19982;&#29992;&#25143;&#36827;&#34892;&#33258;&#28982;&#23545;&#35805;&#65292;&#25552;&#20986;&#25191;&#34892;&#22797;&#26434;&#20219;&#21153;&#30340;&#35745;&#21010;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22797;&#26434;&#20219;&#21153;&#30340;&#25191;&#34892;&#26159;&#36890;&#36807;&#25511;&#21046;&#33050;&#26412;&#23454;&#29616;&#30340;&#65292;&#36825;&#21487;&#33021;&#28041;&#21450;&#22810;&#20010;&#39046;&#22495;&#29305;&#23450;&#30340;AI&#27169;&#22359;&#21644;&#29289;&#32852;&#32593;&#35774;&#22791;&#30340;&#21327;&#20316;&#12290;LLMs&#20351;&#29992;&#22522;&#20110;&#26377;&#38480;&#29366;&#24577;&#26426;&#65288;FSMs&#65289;&#30340;&#35821;&#35328;&#32534;&#30721;&#36716;&#25442;&#26041;&#27861;&#29983;&#25104;&#25511;&#21046;&#33050;&#26412;&#12290;&#35813;&#26694;&#26550;&#36824;&#32467;&#21512;&#20102;&#35821;&#20041;&#20998;&#26512;&#21644;&#21709;&#24212;&#20248;&#21270;&#25216;&#26415;&#65292;&#20197;&#25552;&#39640;&#36895;&#24230;&#21644;&#25928;&#26524;&#12290;&#26368;&#32456;&#65292;&#35813;&#26694;&#26550;&#30340;&#35774;&#35745;&#19981;&#20165;&#26088;&#22312;&#21019;&#26032;&#29289;&#32852;&#32593;&#35774;&#22791;&#25511;&#21046;&#21644;&#20016;&#23500;&#29992;&#25143;&#20307;&#39564;&#65292;&#36824;&#20419;&#36827;&#26234;&#33021;&#21644;&#38598;&#25104;&#30340;&#29289;&#32852;&#32593;&#35774;&#22791;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce LLMind, an AI framework that utilizes large language models (LLMs) as a central orchestrator. The framework integrates LLMs with domain-specific AI modules, enabling IoT devices to collaborate effectively in executing complex tasks. The LLM engages in natural conversations with human users via a user-friendly social media platform to come up with a plan to execute complex tasks. In particular, the execution of a complex task, which may involve the collaborations of multiple domain-specific AI modules and IoT devices, is realized through a control script. The LLM generates the control script using a Language-Code transformation approach based on finite-state machines (FSMs). The framework also incorporates semantic analysis and response optimization techniques to enhance speed and effectiveness. Ultimately, this framework is designed not only to innovate IoT device control and enrich user experiences but also to foster an intelligent and integrated IoT device
&lt;/p&gt;</description></item><item><title>MoLE&#26159;&#19968;&#31181;&#28151;&#21512;&#32447;&#24615;&#19987;&#23478;&#27169;&#22411;&#65292;&#36890;&#36807;&#35757;&#32451;&#22810;&#20010;&#32447;&#24615;&#20013;&#24515;&#27169;&#22411;&#21644;&#19968;&#20010;&#36335;&#30001;&#27169;&#22411;&#65292;&#33021;&#22815;&#36866;&#24212;&#26102;&#38388;&#24207;&#21015;&#27169;&#24335;&#30340;&#21608;&#26399;&#24615;&#21464;&#21270;&#65292;&#24182;&#26174;&#33879;&#38477;&#20302;&#20102;&#39044;&#27979;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2312.06786</link><description>&lt;p&gt;
&#28151;&#21512;&#32447;&#24615;&#19987;&#23478;&#29992;&#20110;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Mixture-of-Linear-Experts for Long-term Time Series Forecasting. (arXiv:2312.06786v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.06786
&lt;/p&gt;
&lt;p&gt;
MoLE&#26159;&#19968;&#31181;&#28151;&#21512;&#32447;&#24615;&#19987;&#23478;&#27169;&#22411;&#65292;&#36890;&#36807;&#35757;&#32451;&#22810;&#20010;&#32447;&#24615;&#20013;&#24515;&#27169;&#22411;&#21644;&#19968;&#20010;&#36335;&#30001;&#27169;&#22411;&#65292;&#33021;&#22815;&#36866;&#24212;&#26102;&#38388;&#24207;&#21015;&#27169;&#24335;&#30340;&#21608;&#26399;&#24615;&#21464;&#21270;&#65292;&#24182;&#26174;&#33879;&#38477;&#20302;&#20102;&#39044;&#27979;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;(LTSF)&#26088;&#22312;&#39044;&#27979;&#32473;&#23450;&#36807;&#21435;&#20540;&#30340;&#26102;&#38388;&#24207;&#21015;&#30340;&#26410;&#26469;&#20540;&#12290;&#24403;&#21069;&#22312;&#36825;&#20010;&#38382;&#39064;&#19978;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;(SOTA)&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#26159;&#30001;&#20197;&#32447;&#24615;&#20026;&#20013;&#24515;&#30340;&#27169;&#22411;&#23454;&#29616;&#30340;&#65292;&#36825;&#20123;&#27169;&#22411;&#20027;&#35201;&#20855;&#26377;&#32447;&#24615;&#26144;&#23556;&#23618;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#22266;&#26377;&#30340;&#31616;&#21333;&#24615;&#65292;&#23427;&#20204;&#19981;&#33021;&#22815;&#36866;&#24212;&#26102;&#38388;&#24207;&#21015;&#27169;&#24335;&#30340;&#21608;&#26399;&#24615;&#21464;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#19987;&#23478;&#39118;&#26684;&#30340;&#22686;&#24378;&#32447;&#24615;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#28151;&#21512;&#32447;&#24615;&#19987;&#23478;(MoLE)&#12290;MoLE&#19981;&#26159;&#35757;&#32451;&#21333;&#20010;&#27169;&#22411;&#65292;&#32780;&#26159;&#35757;&#32451;&#22810;&#20010;&#20197;&#32447;&#24615;&#20026;&#20013;&#24515;&#30340;&#27169;&#22411;(&#21363;&#19987;&#23478;)&#21644;&#19968;&#20010;&#26435;&#34913;&#21644;&#28151;&#21512;&#20854;&#36755;&#20986;&#30340;&#36335;&#30001;&#27169;&#22411;&#12290;&#34429;&#28982;&#25972;&#20010;&#26694;&#26550;&#26159;&#31471;&#21040;&#31471;&#35757;&#32451;&#30340;&#65292;&#20294;&#27599;&#20010;&#19987;&#23478;&#37117;&#23398;&#20250;&#19987;&#38376;&#22788;&#29702;&#29305;&#23450;&#30340;&#26102;&#38388;&#27169;&#24335;&#65292;&#32780;&#36335;&#30001;&#27169;&#22411;&#21017;&#23398;&#20250;&#33258;&#36866;&#24212;&#22320;&#32452;&#21512;&#19987;&#23478;&#20204;&#30340;&#36755;&#20986;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;MoLE&#38477;&#20302;&#20102;&#32447;&#24615;&#20013;&#24515;&#27169;&#22411;(DLinear&#65292;RLinear&#21644;RMLP)&#30340;&#39044;&#27979;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Long-term time series forecasting (LTSF) aims to predict future values of a time series given the past values. The current state-of-the-art (SOTA) on this problem is attained in some cases by linear-centric models, which primarily feature a linear mapping layer. However, due to their inherent simplicity, they are not able to adapt their prediction rules to periodic changes in time series patterns. To address this challenge, we propose a Mixture-of-Experts-style augmentation for linear-centric models and propose Mixture-of-Linear-Experts (MoLE). Instead of training a single model, MoLE trains multiple linear-centric models (i.e., experts) and a router model that weighs and mixes their outputs. While the entire framework is trained end-to-end, each expert learns to specialize in a specific temporal pattern, and the router model learns to compose the experts adaptively. Experiments show that MoLE reduces forecasting error of linear-centric models, including DLinear, RLinear, and RMLP, in 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#32447;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#65288;OTTA&#65289;&#30340;&#27010;&#24565;&#65292;&#26080;&#38656;&#26657;&#20934;&#19988;&#20445;&#25252;&#38544;&#31169;&#65292;&#20197;&#23454;&#29616;&#26080;&#30417;&#30563;&#30340;&#36830;&#32493;&#33258;&#36866;&#24212;&#27169;&#22411;&#36816;&#34892;&#12290;&#30740;&#31350;&#20351;&#29992;&#20102;&#33041;&#30005;&#36816;&#21160;&#24819;&#35937;&#35299;&#30721;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#36731;&#37327;&#32423;&#26550;&#26500;&#21644;&#19981;&#21516;&#30340;OTTA&#25216;&#26415;&#26469;&#25552;&#39640;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2311.18520</link><description>&lt;p&gt;
&#26080;&#38656;&#26657;&#20934;&#30340;&#22312;&#32447;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#24212;&#29992;&#20110;&#33041;&#30005;&#36816;&#21160;&#24819;&#35937;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Calibration-free online test-time adaptation for electroencephalography motor imagery decoding. (arXiv:2311.18520v2 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.18520
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#32447;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#65288;OTTA&#65289;&#30340;&#27010;&#24565;&#65292;&#26080;&#38656;&#26657;&#20934;&#19988;&#20445;&#25252;&#38544;&#31169;&#65292;&#20197;&#23454;&#29616;&#26080;&#30417;&#30563;&#30340;&#36830;&#32493;&#33258;&#36866;&#24212;&#27169;&#22411;&#36816;&#34892;&#12290;&#30740;&#31350;&#20351;&#29992;&#20102;&#33041;&#30005;&#36816;&#21160;&#24819;&#35937;&#35299;&#30721;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#36731;&#37327;&#32423;&#26550;&#26500;&#21644;&#19981;&#21516;&#30340;OTTA&#25216;&#26415;&#26469;&#25552;&#39640;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;-&#35745;&#31639;&#26426;&#25509;&#21475;&#65288;BCI&#65289;&#22312;&#35299;&#30721;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#36825;&#20026;&#23558;&#20154;&#31867;&#22823;&#33041;&#19982;&#22806;&#37096;&#35774;&#22791;&#30456;&#36830;&#25552;&#20379;&#20102;&#19968;&#26465;&#26377;&#24076;&#26395;&#30340;&#36884;&#24452;&#65292;&#36825;&#20027;&#35201;&#24471;&#30410;&#20110;&#24840;&#21457;&#22797;&#26434;&#30340;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#28145;&#24230;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#22330;&#26223;&#20013;&#23454;&#29616;&#39640;&#20934;&#30830;&#29575;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#22240;&#20026;&#19981;&#21516;&#27979;&#35797;&#21644;&#21463;&#35797;&#32773;&#20043;&#38388;&#23384;&#22312;&#20998;&#24067;&#24046;&#24322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#25506;&#35752;&#22312;&#32447;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#65288;OTTA&#65289;&#30340;&#27010;&#24565;&#65292;&#20197;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#25345;&#32493;&#33258;&#36866;&#24212;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#22312;&#33258;&#36866;&#24212;&#36807;&#31243;&#20013;&#19981;&#38656;&#35201;&#35775;&#38382;&#28304;&#25968;&#25454;&#26469;&#20445;&#35777;&#38544;&#31169;&#30340;&#20445;&#25252;&#12290;&#27492;&#22806;&#65292;OTTA&#36890;&#36807;&#19981;&#38656;&#35201;&#20219;&#20309;&#29305;&#23450;&#20110;&#20250;&#35805;&#25110;&#21463;&#35797;&#32773;&#30340;&#25968;&#25454;&#26469;&#23454;&#29616;&#26080;&#38656;&#26657;&#20934;&#36816;&#34892;&#12290;&#25105;&#20204;&#23558;&#20351;&#29992;&#36731;&#37327;&#32423;&#26550;&#26500;&#20197;&#21450;&#19981;&#21516;&#30340;OTTA&#25216;&#26415;&#65288;&#22914;&#23545;&#40784;&#65292;&#33258;&#36866;&#24212;&#25209;&#37327;&#24402;&#19968;&#21270;&#65289;&#26469;&#30740;&#31350;&#33041;&#30005;&#36816;&#21160;&#24819;&#35937;&#35299;&#30721;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Providing a promising pathway to link the human brain with external devices, Brain-Computer Interfaces (BCIs) have seen notable advancements in decoding capabilities, primarily driven by increasingly sophisticated techniques, especially deep learning. However, achieving high accuracy in real-world scenarios remains a challenge due to the distribution shift between sessions and subjects. In this paper we will explore the concept of online test-time adaptation (OTTA) to continuously adapt the model in an unsupervised fashion during inference time. Our approach guarantees the preservation of privacy by eliminating the requirement to access the source data during the adaptation process. Additionally, OTTA achieves calibration-free operation by not requiring any session- or subject-specific data. We will investigate the task of electroencephalography (EEG) motor imagery decoding using a lightweight architecture together with different OTTA techniques like alignment, adaptive batch normaliza
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31227;&#21160;&#32593;&#26684;PDE&#30340;&#31227;&#21160;&#37319;&#26679;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;(MMPDE-Net)&#65292;&#36890;&#36807;&#35299;&#20915;&#31227;&#21160;&#32593;&#26684;PDE&#26469;&#33258;&#36866;&#24212;&#29983;&#25104;&#26032;&#30340;&#37319;&#26679;&#28857;&#65292;&#24182;&#19988;&#32467;&#21512;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#25552;&#20986;&#20102;&#31227;&#21160;&#37319;&#26679;PINN&#65288;MS-PINN&#65289;&#30340;&#26694;&#26550;&#12290;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;MS-PINN&#30456;&#23545;&#20110;PINN&#30340;&#24615;&#33021;&#25913;&#21892;&#12290;</title><link>http://arxiv.org/abs/2311.16167</link><description>&lt;p&gt;
&#22522;&#20110;&#31227;&#21160;&#32593;&#26684;PDE&#30340;&#31227;&#21160;&#37319;&#26679;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Moving Sampling Physics-informed Neural Networks induced by Moving Mesh PDE. (arXiv:2311.16167v2 [math.NA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.16167
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31227;&#21160;&#32593;&#26684;PDE&#30340;&#31227;&#21160;&#37319;&#26679;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;(MMPDE-Net)&#65292;&#36890;&#36807;&#35299;&#20915;&#31227;&#21160;&#32593;&#26684;PDE&#26469;&#33258;&#36866;&#24212;&#29983;&#25104;&#26032;&#30340;&#37319;&#26679;&#28857;&#65292;&#24182;&#19988;&#32467;&#21512;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#25552;&#20986;&#20102;&#31227;&#21160;&#37319;&#26679;PINN&#65288;MS-PINN&#65289;&#30340;&#26694;&#26550;&#12290;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;MS-PINN&#30456;&#23545;&#20110;PINN&#30340;&#24615;&#33021;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31227;&#21160;&#32593;&#26684;&#26041;&#27861;&#30340;&#31471;&#21040;&#31471;&#33258;&#36866;&#24212;&#37319;&#26679;&#31070;&#32463;&#32593;&#32476;&#65288;MMPDE-Net&#65289;&#65292;&#36890;&#36807;&#27714;&#35299;&#31227;&#21160;&#32593;&#26684;PDE&#65292;&#21487;&#20197;&#33258;&#36866;&#24212;&#29983;&#25104;&#26032;&#30340;&#37319;&#26679;&#28857;&#12290;&#35813;&#27169;&#22411;&#26088;&#22312;&#25913;&#21892;&#37319;&#26679;&#28857;&#29983;&#25104;&#30340;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22522;&#20110;MMPDE-Net&#24320;&#21457;&#20102;&#19968;&#31181;&#36845;&#20195;&#31639;&#27861;&#65292;&#20351;&#24471;&#37319;&#26679;&#28857;&#26356;&#21152;&#31934;&#30830;&#21644;&#21487;&#25511;&#12290;&#30001;&#20110;MMPDE-Net&#26159;&#29420;&#31435;&#20110;&#28145;&#24230;&#23398;&#20064;&#27714;&#35299;&#22120;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#23558;&#20854;&#19982;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#31227;&#21160;&#37319;&#26679;PINN&#65288;MS-PINN&#65289;&#65292;&#24182;&#22312;&#19968;&#20123;&#20551;&#35774;&#19979;&#36890;&#36807;&#35823;&#24046;&#20998;&#26512;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#22235;&#20010;&#20856;&#22411;&#23454;&#20363;&#30340;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;MS-PINN&#30456;&#23545;&#20110;PINN&#30340;&#24615;&#33021;&#25913;&#21892;&#65292;&#20174;&#32780;&#25968;&#20540;&#19978;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose an end-to-end adaptive sampling neural network (MMPDE-Net) based on the moving mesh method, which can adaptively generate new sampling points by solving the moving mesh PDE. This model focuses on improving the quality of sampling points generation. Moreover, we develop an iterative algorithm based on MMPDE-Net, which makes the sampling points more precise and controllable. Since MMPDE-Net is a framework independent of the deep learning solver, we combine it with physics-informed neural networks (PINN) to propose moving sampling PINN (MS-PINN) and demonstrate its effectiveness by error analysis under some assumptions. Finally, we demonstrate the performance improvement of MS-PINN compared to PINN through numerical experiments of four typical examples, which numerically verify the effectiveness of our method.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#22478;&#24066;&#36712;&#36947;&#20132;&#36890;&#33258;&#20027;&#36816;&#33829;&#21015;&#36710;&#30340;&#23433;&#20840;&#26234;&#33021;&#25511;&#21046;&#30340;SSA-DRL&#26694;&#26550;&#65292;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#22522;&#30784;&#19978;&#32467;&#21512;&#20102;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#12289;&#24378;&#21270;&#23398;&#20064;&#21644;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#29983;&#25104;&#28385;&#36275;&#36895;&#24230;&#32422;&#26463;&#21644;&#36827;&#24230;&#32422;&#26463;&#30340;&#23433;&#20840;&#25511;&#21046;&#21629;&#20196;&#24207;&#21015;&#12290;</title><link>http://arxiv.org/abs/2311.14457</link><description>&lt;p&gt;
&#22914;&#20309;&#30830;&#20445;&#23433;&#20840;&#30340;&#25511;&#21046;&#31574;&#30053;&#65311;&#36808;&#21521;&#22478;&#24066;&#20132;&#36890;&#33258;&#20027;&#36816;&#33829;&#30340;SRL
&lt;/p&gt;
&lt;p&gt;
How to ensure a safe control strategy? Towards a SRL for urban transit autonomous operation. (arXiv:2311.14457v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.14457
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#22478;&#24066;&#36712;&#36947;&#20132;&#36890;&#33258;&#20027;&#36816;&#33829;&#21015;&#36710;&#30340;&#23433;&#20840;&#26234;&#33021;&#25511;&#21046;&#30340;SSA-DRL&#26694;&#26550;&#65292;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#22522;&#30784;&#19978;&#32467;&#21512;&#20102;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#12289;&#24378;&#21270;&#23398;&#20064;&#21644;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#29983;&#25104;&#28385;&#36275;&#36895;&#24230;&#32422;&#26463;&#21644;&#36827;&#24230;&#32422;&#26463;&#30340;&#23433;&#20840;&#25511;&#21046;&#21629;&#20196;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#36880;&#28176;&#23637;&#31034;&#20986;&#20854;&#22312;&#22478;&#24066;&#36712;&#36947;&#20132;&#36890;&#33258;&#20027;&#36816;&#33829;&#20013;&#28508;&#22312;&#30340;&#20915;&#31574;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24378;&#21270;&#23398;&#20064;&#26080;&#27861;&#22312;&#23398;&#20064;&#21644;&#25191;&#34892;&#36807;&#31243;&#20013;&#20445;&#35777;&#23433;&#20840;&#24615;&#65292;&#36825;&#20173;&#28982;&#26159;&#23454;&#38469;&#24212;&#29992;&#24378;&#21270;&#23398;&#20064;&#38754;&#20020;&#30340;&#20027;&#35201;&#38556;&#30861;&#20043;&#19968;&#12290;&#37492;&#20110;&#36825;&#20010;&#32570;&#28857;&#65292;&#22312;&#23433;&#20840;&#20851;&#38190;&#30340;&#33258;&#20027;&#36816;&#33829;&#39046;&#22495;&#24212;&#29992;&#24378;&#21270;&#23398;&#20064;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#26080;&#27861;&#29983;&#25104;&#36991;&#20813;&#36229;&#36895;&#25805;&#20316;&#30340;&#23433;&#20840;&#25511;&#21046;&#21629;&#20196;&#24207;&#21015;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#22478;&#24066;&#36712;&#36947;&#20132;&#36890;&#33258;&#20027;&#36816;&#33829;&#21015;&#36710;&#30340;&#23433;&#20840;&#26234;&#33021;&#25511;&#21046;&#30340;SSA-DRL&#26694;&#26550;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#32467;&#21512;&#20102;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#12289;&#24378;&#21270;&#23398;&#20064;&#21644;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65292;&#24182;&#21253;&#25324;&#22235;&#20010;&#20027;&#35201;&#27169;&#22359;&#65306;&#21518;&#32622;&#23631;&#34109;&#12289;&#25628;&#32034;&#26641;&#27169;&#22359;&#12289;DRL&#26694;&#26550;&#21644;&#39069;&#22806;&#30340;&#34892;&#21160;&#32773;&#12290;&#27492;&#22806;&#65292;&#26694;&#26550;&#30340;&#36755;&#20986;&#21487;&#20197;&#28385;&#36275;&#36895;&#24230;&#32422;&#26463;&#21644;&#36827;&#24230;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning has gradually shown its latent decision-making ability in urban rail transit autonomous operation. However, since reinforcement learning can not neither guarantee safety during learning nor execution, this is still one of the major obstacles to the practical application of reinforcement learning. Given this drawback, reinforcement learning applied in the safety-critical autonomous operation domain remains challenging without generating a safe control command sequence that avoids overspeed operations. Therefore, a SSA-DRL framework is proposed in this paper for safe intelligent control of urban rail transit autonomous operation trains. The proposed framework is combined with linear temporal logic, reinforcement learning and Monte Carlo tree search and consists of four mainly module: a post-posed shielding, a searching tree module, a DRL framework and an additional actor. Furthermore, the output of the framework can meet speed constraint, schedule constraint a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26080;&#30417;&#30563;&#24322;&#26500;&#39046;&#22495;&#36866;&#24212;&#30340;&#24320;&#25918;&#38598;dandelion&#32593;&#32476;&#65288;OSDN&#65289;&#29992;&#20110;&#29289;&#32852;&#32593;&#20837;&#20405;&#26816;&#27979;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#20174;&#30693;&#35782;&#20016;&#23500;&#30340;&#28304;&#32593;&#32476;&#20837;&#20405;&#39046;&#22495;&#36827;&#34892;&#20837;&#20405;&#30693;&#35782;&#20256;&#36755;&#65292;&#20197;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#20837;&#20405;&#26816;&#27979;&#12290;&#22312;&#24320;&#25918;&#38598;&#35774;&#32622;&#19979;&#65292;&#23427;&#33021;&#22815;&#26816;&#27979;&#21040;&#22312;&#28304;&#39046;&#22495;&#20013;&#26410;&#35266;&#27979;&#21040;&#30340;&#26032;&#20852;&#30446;&#26631;&#39046;&#22495;&#20837;&#20405;&#12290;</title><link>http://arxiv.org/abs/2311.11249</link><description>&lt;p&gt;
&#24320;&#25918;&#38598;dandelion&#32593;&#32476;&#29992;&#20110;&#29289;&#32852;&#32593;&#20837;&#20405;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Open Set Dandelion Network for IoT Intrusion Detection. (arXiv:2311.11249v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.11249
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26080;&#30417;&#30563;&#24322;&#26500;&#39046;&#22495;&#36866;&#24212;&#30340;&#24320;&#25918;&#38598;dandelion&#32593;&#32476;&#65288;OSDN&#65289;&#29992;&#20110;&#29289;&#32852;&#32593;&#20837;&#20405;&#26816;&#27979;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#20174;&#30693;&#35782;&#20016;&#23500;&#30340;&#28304;&#32593;&#32476;&#20837;&#20405;&#39046;&#22495;&#36827;&#34892;&#20837;&#20405;&#30693;&#35782;&#20256;&#36755;&#65292;&#20197;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#20837;&#20405;&#26816;&#27979;&#12290;&#22312;&#24320;&#25918;&#38598;&#35774;&#32622;&#19979;&#65292;&#23427;&#33021;&#22815;&#26816;&#27979;&#21040;&#22312;&#28304;&#39046;&#22495;&#20013;&#26410;&#35266;&#27979;&#21040;&#30340;&#26032;&#20852;&#30446;&#26631;&#39046;&#22495;&#20837;&#20405;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29289;&#32852;&#32593;&#35774;&#22791;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#20445;&#25252;&#23427;&#20204;&#20813;&#21463;&#24694;&#24847;&#20837;&#20405;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29289;&#32852;&#32593;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#38480;&#21046;&#20102;&#20256;&#32479;&#20837;&#20405;&#26816;&#27979;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#65292;&#36825;&#20123;&#26041;&#27861;&#39640;&#24230;&#20381;&#36182;&#20110;&#25968;&#25454;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#25552;&#20986;&#20102;&#22522;&#20110;&#26080;&#30417;&#30563;&#24322;&#26500;&#39046;&#22495;&#36866;&#24212;&#30340;&#24320;&#25918;&#38598;dandelion&#32593;&#32476;&#65288;OSDN&#65289;&#12290;OSDN&#27169;&#22411;&#36890;&#36807;&#20174;&#30693;&#35782;&#20016;&#23500;&#30340;&#28304;&#32593;&#32476;&#20837;&#20405;&#39046;&#22495;&#36827;&#34892;&#20837;&#20405;&#30693;&#35782;&#20256;&#36755;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#25968;&#25454;&#31232;&#32570;&#30340;&#30446;&#26631;&#29289;&#32852;&#32593;&#20837;&#20405;&#39046;&#22495;&#30340;&#26356;&#20934;&#30830;&#30340;&#20837;&#20405;&#26816;&#27979;&#12290;&#22312;&#24320;&#25918;&#38598;&#35774;&#32622;&#19979;&#65292;&#23427;&#36824;&#33021;&#26816;&#27979;&#21040;&#22312;&#28304;&#39046;&#22495;&#20013;&#26410;&#35266;&#27979;&#21040;&#30340;&#26032;&#20852;&#30446;&#26631;&#39046;&#22495;&#20837;&#20405;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;OSDN&#27169;&#22411;&#23558;&#28304;&#39046;&#22495;&#24418;&#25104;&#19968;&#20010;&#31867;&#20284;&#33970;&#20844;&#33521;&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#20854;&#20013;&#27599;&#20010;&#20837;&#20405;&#31867;&#21035;&#34987;&#32039;&#23494;&#20998;&#32452;&#24182;&#19988;&#19981;&#21516;&#30340;&#20837;&#20405;&#31867;&#21035;&#34987;&#20998;&#38548;&#24320;&#65292;&#21363;&#21516;&#26102;&#24378;&#35843;&#20102;&#31867;&#21035;&#38388;&#30340;&#21487;&#20998;&#24615;&#21644;&#31867;&#21035;&#20869;&#30340;&#32039;&#20945;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As IoT devices become widely, it is crucial to protect them from malicious intrusions. However, the data scarcity of IoT limits the applicability of traditional intrusion detection methods, which are highly data-dependent. To address this, in this paper we propose the Open-Set Dandelion Network (OSDN) based on unsupervised heterogeneous domain adaptation in an open-set manner. The OSDN model performs intrusion knowledge transfer from the knowledge-rich source network intrusion domain to facilitate more accurate intrusion detection for the data-scarce target IoT intrusion domain. Under the open-set setting, it can also detect newly-emerged target domain intrusions that are not observed in the source domain. To achieve this, the OSDN model forms the source domain into a dandelion-like feature space in which each intrusion category is compactly grouped and different intrusion categories are separated, i.e., simultaneously emphasising inter-category separability and intra-category compactn
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36716;&#21464;&#24615;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#65292;&#21033;&#29992;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#25216;&#26415;&#33258;&#21160;&#21270;&#21270;&#23398;&#20013;&#30340;&#21453;&#24212;&#26465;&#20214;&#25512;&#33616;&#65288;RCR&#65289;&#20219;&#21153;&#65292;&#36890;&#36807;&#27169;&#25311;&#19987;&#23478;&#21270;&#23398;&#23478;&#30340;&#31574;&#30053;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#26032;&#21453;&#24212;&#25351;&#32441;&#65292;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#20154;&#24037;&#26234;&#33021;&#12290;&#27492;&#31995;&#32479;&#21487;&#20197;&#20943;&#36731;&#21270;&#23398;&#23478;&#30340;&#24037;&#20316;&#36127;&#25285;&#65292;&#20351;&#20182;&#20204;&#33021;&#22815;&#26356;&#19987;&#27880;&#20110;&#26356;&#22522;&#30784;&#21644;&#21019;&#36896;&#24615;&#30340;&#31185;&#23398;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2311.10776</link><description>&lt;p&gt;
&#22312;&#21270;&#23398;&#21512;&#25104;&#20013;&#30340;&#21453;&#24212;&#26465;&#20214;&#25512;&#33616;&#20013;&#65292;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Retrieval-Augmented Generative Agent for Reaction Condition Recommendation in Chemical Synthesis. (arXiv:2311.10776v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.10776
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36716;&#21464;&#24615;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#65292;&#21033;&#29992;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#25216;&#26415;&#33258;&#21160;&#21270;&#21270;&#23398;&#20013;&#30340;&#21453;&#24212;&#26465;&#20214;&#25512;&#33616;&#65288;RCR&#65289;&#20219;&#21153;&#65292;&#36890;&#36807;&#27169;&#25311;&#19987;&#23478;&#21270;&#23398;&#23478;&#30340;&#31574;&#30053;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#26032;&#21453;&#24212;&#25351;&#32441;&#65292;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#20154;&#24037;&#26234;&#33021;&#12290;&#27492;&#31995;&#32479;&#21487;&#20197;&#20943;&#36731;&#21270;&#23398;&#23478;&#30340;&#24037;&#20316;&#36127;&#25285;&#65292;&#20351;&#20182;&#20204;&#33021;&#22815;&#26356;&#19987;&#27880;&#20110;&#26356;&#22522;&#30784;&#21644;&#21019;&#36896;&#24615;&#30340;&#31185;&#23398;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20026;&#21270;&#23398;&#31038;&#20250;&#20013;&#30340;&#33258;&#21160;&#21270;&#21270;&#23398;&#21453;&#24212;&#38138;&#24179;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26410;&#26469;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36716;&#21464;&#24615;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#65292;&#21033;&#29992;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#25216;&#26415;&#33258;&#21160;&#21270;&#21270;&#23398;&#20013;&#30340;&#21453;&#24212;&#26465;&#20214;&#25512;&#33616;&#65288;RCR&#65289;&#20219;&#21153;&#12290;&#36890;&#36807;&#27169;&#25311;&#19987;&#23478;&#21270;&#23398;&#23478;&#30340;&#25628;&#32034;&#21644;&#20998;&#26512;&#31574;&#30053;&#65292;&#35813;&#20195;&#29702;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#26597;&#35810;&#20998;&#23376;&#25968;&#25454;&#24211;&#65292;&#24182;&#20174;&#22312;&#32447;&#25991;&#29486;&#20013;&#25552;&#21462;&#20851;&#38190;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#35813;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#36824;&#37197;&#22791;&#20102;&#25105;&#20204;&#20026;RCR&#20219;&#21153;&#24320;&#21457;&#30340;&#26032;&#21453;&#24212;&#25351;&#32441;&#12290;&#30001;&#20110;RAG&#25216;&#26415;&#30340;&#20351;&#29992;&#65292;&#25105;&#20204;&#30340;&#20195;&#29702;&#20351;&#29992;&#26356;&#26032;&#30340;&#22312;&#32447;&#25968;&#25454;&#24211;&#20316;&#20026;&#30693;&#35782;&#28304;&#65292;&#26174;&#33879;&#20248;&#20110;&#20165;&#21463;&#20854;&#35757;&#32451;&#25968;&#25454;&#22266;&#23450;&#30693;&#35782;&#38480;&#21046;&#30340;&#20256;&#32479;&#20154;&#24037;&#26234;&#33021;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#31995;&#32479;&#21487;&#20197;&#26174;&#33879;&#20943;&#36731;&#21270;&#23398;&#23478;&#30340;&#24037;&#20316;&#36127;&#25285;&#65292;&#20351;&#20182;&#20204;&#33021;&#22815;&#26356;&#19987;&#27880;&#20110;&#26356;&#22522;&#30784;&#21644;&#21019;&#36896;&#24615;&#30340;&#31185;&#23398;&#38382;&#39064;&#12290;&#36825;&#19968;&#37325;&#22823;&#36827;&#23637;&#23558;&#35745;&#31639;&#25216;&#26415;&#19982;&#21270;&#23398;&#31038;&#20250;&#26356;&#32039;&#23494;&#32852;&#31995;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent artificial intelligence (AI) research plots a promising future of automatic chemical reactions within the chemistry society. This study presents a transformative AI agent that automates the reaction condition recommendation (RCR) task in chemistry using retrieval-augmented generation (RAG) technology. By emulating expert chemists search and analysis strategies, the agent employs large language models (LLMs) to interrogate molecular databases and distill critical data from online literature. Further, the AI agent is equipped with our novel reaction fingerprint developed for the RCR task. Thanks to the RAG technology, our agent uses updated online databases as knowledge sources, significantly outperforming conventional AIs confined to the fixed knowledge within its training data. The resulting system can significantly reduce chemists workload, allowing them to focus on more fundamental and creative scientific problems. This significant advancement brings closer computational techn
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#20998;&#31867;&#20013;&#27010;&#24565;&#32423;&#21035;&#30340;&#35823;&#30456;&#20851;&#24615;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;ChatGPT&#20998;&#37197;&#27010;&#24565;&#26631;&#31614;&#21644;&#24341;&#20837;&#25968;&#25454;&#20877;&#24179;&#34913;&#25216;&#26415;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2311.08648</link><description>&lt;p&gt;
&#22312;&#25991;&#26412;&#20998;&#31867;&#20013;&#25506;&#32034;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#27010;&#24565;&#32423;&#21035;&#30340;&#35823;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
Explore Spurious Correlations at the Concept Level in Language Models for Text Classification. (arXiv:2311.08648v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.08648
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#20998;&#31867;&#20013;&#27010;&#24565;&#32423;&#21035;&#30340;&#35823;&#30456;&#20851;&#24615;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;ChatGPT&#20998;&#37197;&#27010;&#24565;&#26631;&#31614;&#21644;&#24341;&#20837;&#25968;&#25454;&#20877;&#24179;&#34913;&#25216;&#26415;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#20247;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#37319;&#29992;&#20102;&#24494;&#35843;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#12290;&#34429;&#28982;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#20294;&#30001;&#20110;&#35757;&#32451;&#25968;&#25454;&#20013;&#26631;&#31614;&#20998;&#24067;&#19981;&#24179;&#34913;&#25110;&#19978;&#19979;&#25991;&#23398;&#20064;&#23454;&#20363;&#20135;&#29983;&#30340;&#35823;&#30456;&#20851;&#24615;&#65292;&#23427;&#20204;&#38754;&#20020;&#30528;&#40065;&#26834;&#24615;&#25361;&#25112;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#35789;&#35821;&#12289;&#30701;&#35821;&#21644;&#21477;&#27861;&#29305;&#24449;&#19978;&#65292;&#24573;&#35270;&#20102;&#27010;&#24565;&#32423;&#21035;&#30340;&#30740;&#31350;&#65292;&#36825;&#24448;&#24448;&#26159;&#30001;&#20110;&#32570;&#20047;&#27010;&#24565;&#26631;&#31614;&#21644;&#38590;&#20197;&#30830;&#23450;&#36755;&#20837;&#25991;&#26412;&#20013;&#30340;&#27010;&#24565;&#20869;&#23481;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#20027;&#35201;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;ChatGPT&#20026;&#25991;&#26412;&#20998;&#37197;&#27010;&#24565;&#26631;&#31614;&#65292;&#35780;&#20272;&#27169;&#22411;&#22312;&#24494;&#35843;&#25110;&#19978;&#19979;&#25991;&#23398;&#20064;&#27979;&#35797;&#25968;&#25454;&#20013;&#30340;&#27010;&#24565;&#20559;&#24046;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24403;&#35821;&#35328;&#27169;&#22411;&#22312;&#35757;&#32451;&#25110;&#25552;&#31034;&#20013;&#36935;&#21040;&#27010;&#24565;&#21644;&#26631;&#31614;&#20043;&#38388;&#30340;&#35823;&#30456;&#20851;&#24615;&#26102;&#65292;&#20250;&#37319;&#21462;&#39044;&#27979;&#30340;&#25463;&#24452;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#25968;&#25454;&#20877;&#24179;&#34913;&#25216;&#26415;&#65292;&#23558;ChatGPT&#29983;&#25104;&#30340;&#21453;&#20107;&#23454;&#25968;&#25454;&#32435;&#20837;&#20854;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models (LMs) have achieved notable success in numerous NLP tasks, employing both fine-tuning and in-context learning (ICL) methods. While language models demonstrate exceptional performance, they face robustness challenges due to spurious correlations arising from imbalanced label distributions in training data or ICL exemplars. Previous research has primarily concentrated on word, phrase, and syntax features, neglecting the concept level, often due to the absence of concept labels and difficulty in identifying conceptual content in input texts. This paper introduces two main contributions. First, we employ ChatGPT to assign concept labels to texts, assessing concept bias in models during fine-tuning or ICL on test data. We find that LMs, when encountering spurious correlations between a concept and a label in training or prompts, resort to shortcuts for predictions. Second, we introduce a data rebalancing technique that incorporates ChatGPT-generated counterfactual data, ther
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#24067;&#24335;&#32852;&#37030;&#23398;&#20064;&#32593;&#32476;&#20013;&#23545;&#25239;&#33410;&#28857;&#37096;&#32626;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#32447;&#31574;&#30053;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25915;&#20987;&#31639;&#27861;&#65292;&#20248;&#20808;&#32771;&#34385;&#23545;&#25239;&#30340;&#20998;&#25955;&#24615;&#32780;&#19981;&#26159;&#20013;&#24515;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.07946</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#32852;&#37030;&#23398;&#20064;&#32593;&#32476;&#20013;&#23545;&#25239;&#33410;&#28857;&#37096;&#32626;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The Impact of Adversarial Node Placement in Decentralized Federated Learning Networks. (arXiv:2311.07946v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.07946
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#24067;&#24335;&#32852;&#37030;&#23398;&#20064;&#32593;&#32476;&#20013;&#23545;&#25239;&#33410;&#28857;&#37096;&#32626;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#32447;&#31574;&#30053;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25915;&#20987;&#31639;&#27861;&#65292;&#20248;&#20808;&#32771;&#34385;&#23545;&#25239;&#30340;&#20998;&#25955;&#24615;&#32780;&#19981;&#26159;&#20013;&#24515;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#26032;&#30340;&#20998;&#24067;&#24335;&#26694;&#26550;&#20063;&#36234;&#26469;&#36234;&#26222;&#21450;&#12290;&#36825;&#20123;&#26694;&#26550;&#21033;&#29992;&#20998;&#24067;&#24335;&#29615;&#22659;&#30340;&#20248;&#21183;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#21644;&#33410;&#33021;&#30340;&#35774;&#22791;&#38388;&#36890;&#20449;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#22686;&#38271;&#20063;&#21152;&#22823;&#20102;&#23545;&#24378;&#22823;&#23433;&#20840;&#25514;&#26045;&#30340;&#38656;&#27714;&#12290;&#23613;&#31649;&#29616;&#26377;&#30740;&#31350;&#24050;&#32463;&#25506;&#35752;&#20102;FL&#23433;&#20840;&#30340;&#21508;&#20010;&#26041;&#38754;&#65292;&#20294;&#26159;&#23545;&#20110;&#20998;&#24067;&#24335;&#32593;&#32476;&#20013;&#23545;&#25239;&#33410;&#28857;&#37096;&#32626;&#30340;&#20316;&#29992;&#20173;&#28982;&#36739;&#23569;&#30740;&#31350;&#12290;&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#19981;&#21516;&#23545;&#25239;&#37096;&#32626;&#31574;&#30053;&#19979;&#20998;&#24067;&#24335;FL&#30340;&#24615;&#33021;&#65292;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#20004;&#31181;&#22522;&#32447;&#31574;&#30053;&#26469;&#37096;&#32626;&#23545;&#25239;&#33410;&#28857;&#65306;&#38543;&#26426;&#37096;&#32626;&#21644;&#22522;&#20110;&#32593;&#32476;&#20013;&#24515;&#24615;&#30340;&#37096;&#32626;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25915;&#20987;&#31639;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#23545;&#25239;&#33410;&#28857;&#20043;&#38388;&#30340;&#24179;&#22343;&#32593;&#32476;&#36317;&#31163;&#65292;&#20248;&#20808;&#32771;&#34385;&#23545;&#25239;&#30340;&#20998;&#25955;&#24615;&#32780;&#19981;&#26159;&#20013;&#24515;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As Federated Learning (FL) grows in popularity, new decentralized frameworks are becoming widespread. These frameworks leverage the benefits of decentralized environments to enable fast and energy-efficient inter-device communication. However, this growing popularity also intensifies the need for robust security measures. While existing research has explored various aspects of FL security, the role of adversarial node placement in decentralized networks remains largely unexplored. This paper addresses this gap by analyzing the performance of decentralized FL for various adversarial placement strategies when adversaries can jointly coordinate their placement within a network. We establish two baseline strategies for placing adversarial node: random placement and network centrality-based placement. Building on this foundation, we propose a novel attack algorithm that prioritizes adversarial spread over adversarial centrality by maximizing the average network distance between adversaries.
&lt;/p&gt;</description></item><item><title>GTP-ViT&#26159;&#19968;&#31181;&#22522;&#20110;&#22270;&#20256;&#25773;&#30340;&#39640;&#25928;&#35270;&#35273;Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#19981;&#22826;&#37325;&#35201;&#30340;&#20196;&#29260;&#20449;&#24687;&#20256;&#25773;&#32473;&#26356;&#37325;&#35201;&#30340;&#20196;&#29260;&#65292;&#23454;&#29616;&#20102;&#27169;&#22411;&#30340;&#39640;&#25928;&#21644;&#20449;&#24687;&#20445;&#30041;&#12290;</title><link>http://arxiv.org/abs/2311.03035</link><description>&lt;p&gt;
GTP-ViT: &#22522;&#20110;&#22270;&#20256;&#25773;&#30340;&#39640;&#25928;&#35270;&#35273;Transformer&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
GTP-ViT: Efficient Vision Transformers via Graph-based Token Propagation. (arXiv:2311.03035v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.03035
&lt;/p&gt;
&lt;p&gt;
GTP-ViT&#26159;&#19968;&#31181;&#22522;&#20110;&#22270;&#20256;&#25773;&#30340;&#39640;&#25928;&#35270;&#35273;Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#19981;&#22826;&#37325;&#35201;&#30340;&#20196;&#29260;&#20449;&#24687;&#20256;&#25773;&#32473;&#26356;&#37325;&#35201;&#30340;&#20196;&#29260;&#65292;&#23454;&#29616;&#20102;&#27169;&#22411;&#30340;&#39640;&#25928;&#21644;&#20449;&#24687;&#20445;&#30041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;Transformer&#27169;&#22411;&#65288;ViTs&#65289;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#24341;&#36215;&#20102;&#38761;&#21629;&#65292;&#20294;&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#19978;&#37096;&#32626;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#35745;&#31639;&#38656;&#27714;&#36739;&#39640;&#12290;&#20026;&#20102;&#21152;&#24555;&#39044;&#35757;&#32451;&#30340;ViTs&#65292;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#20102;&#20196;&#29260;&#20462;&#21098;&#21644;&#20196;&#29260;&#21512;&#24182;&#26041;&#27861;&#65292;&#26088;&#22312;&#20943;&#23569;&#21442;&#19982;&#35745;&#31639;&#30340;&#20196;&#29260;&#25968;&#37327;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20173;&#28982;&#26377;&#19968;&#20123;&#23616;&#38480;&#24615;&#65292;&#22914;&#20462;&#21098;&#20196;&#29260;&#23548;&#33268;&#30340;&#22270;&#20687;&#20449;&#24687;&#20002;&#22833;&#21644;&#20196;&#29260;&#21305;&#37197;&#36807;&#31243;&#30340;&#20302;&#25928;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#22270;&#20256;&#25773;&#30340;&#20196;&#29260;&#20256;&#25773;&#65288;GTP&#65289;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#39640;&#25928;ViTs&#20013;&#24179;&#34913;&#27169;&#22411;&#25928;&#29575;&#21644;&#20449;&#24687;&#20445;&#30041;&#30340;&#25361;&#25112;&#12290;&#21463;&#21040;&#22270;&#25688;&#35201;&#31639;&#27861;&#30340;&#21551;&#21457;&#65292;GTP&#23558;&#19981;&#22826;&#37325;&#35201;&#30340;&#20196;&#29260;&#20449;&#24687;&#32454;&#33268;&#22320;&#20256;&#25773;&#32473;&#31354;&#38388;&#21644;&#35821;&#20041;&#19978;&#30456;&#20851;&#30340;&#26356;&#37325;&#35201;&#30340;&#20196;&#29260;&#12290;&#22240;&#27492;&#65292;&#21097;&#19979;&#30340;&#23569;&#25968;&#20196;&#29260;&#20316;&#20026;&#25972;&#20010;&#20196;&#29260;&#22270;&#30340;&#25688;&#35201;&#65292;&#20351;&#24471;&#35813;&#26041;&#27861;&#33021;&#22815;&#20943;&#23569;&#35745;&#31639;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision Transformers (ViTs) have revolutionized the field of computer vision, yet their deployments on resource-constrained devices remain challenging due to high computational demands. To expedite pre-trained ViTs, token pruning and token merging approaches have been developed, which aim at reducing the number of tokens involved in the computation. However, these methods still have some limitations, such as image information loss from pruned tokens and inefficiency in the token-matching process. In this paper, we introduce a novel Graph-based Token Propagation (GTP) method to resolve the challenge of balancing model efficiency and information preservation for efficient ViTs. Inspired by graph summarization algorithms, GTP meticulously propagates less significant tokens' information to spatially and semantically connected tokens that are of greater importance. Consequently, the remaining few tokens serve as a summarization of the entire token graph, allowing the method to reduce computa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#23545;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#27169;&#22411;&#21450;&#20854;&#21069;&#39537;&#36827;&#34892;&#20998;&#31867;&#12290;&#35813;&#26694;&#26550;&#24341;&#20837;&#20102;&#19981;&#21516;&#23618;&#27425;&#30340;AGI&#24615;&#33021;&#12289;&#24191;&#27867;&#24615;&#21644;&#33258;&#20027;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#20849;&#21516;&#30340;&#35821;&#35328;&#29992;&#20110;&#27604;&#36739;&#27169;&#22411;&#12289;&#35780;&#20272;&#39118;&#38505;&#65292;&#24182;&#34913;&#37327;&#22312;AGI&#36335;&#24452;&#19978;&#30340;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2311.02462</link><description>&lt;p&gt;
AGI&#30340;&#23618;&#27425;&#65306;&#23558;AGI&#36335;&#24452;&#19978;&#30340;&#36827;&#23637;&#21487;&#25805;&#20316;&#21270;
&lt;/p&gt;
&lt;p&gt;
Levels of AGI: Operationalizing Progress on the Path to AGI. (arXiv:2311.02462v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.02462
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#23545;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#27169;&#22411;&#21450;&#20854;&#21069;&#39537;&#36827;&#34892;&#20998;&#31867;&#12290;&#35813;&#26694;&#26550;&#24341;&#20837;&#20102;&#19981;&#21516;&#23618;&#27425;&#30340;AGI&#24615;&#33021;&#12289;&#24191;&#27867;&#24615;&#21644;&#33258;&#20027;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#20849;&#21516;&#30340;&#35821;&#35328;&#29992;&#20110;&#27604;&#36739;&#27169;&#22411;&#12289;&#35780;&#20272;&#39118;&#38505;&#65292;&#24182;&#34913;&#37327;&#22312;AGI&#36335;&#24452;&#19978;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#27169;&#22411;&#21450;&#20854;&#21069;&#39537;&#30340;&#33021;&#21147;&#21644;&#34892;&#20026;&#36827;&#34892;&#20998;&#31867;&#12290;&#35813;&#26694;&#26550;&#24341;&#20837;&#20102;AGI&#24615;&#33021;&#12289;&#24191;&#27867;&#24615;&#21644;&#33258;&#20027;&#24615;&#30340;&#23618;&#27425;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#20010;&#26694;&#26550;&#33021;&#22815;&#20687;&#33258;&#21160;&#39550;&#39542;&#30340;&#23618;&#27425;&#19968;&#26679;&#65292;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#20849;&#21516;&#30340;&#35821;&#35328;&#26469;&#27604;&#36739;&#27169;&#22411;&#12289;&#35780;&#20272;&#39118;&#38505;&#65292;&#24182;&#34913;&#37327;&#22312;AGI&#36335;&#24452;&#19978;&#30340;&#36827;&#23637;&#12290;&#20026;&#20102;&#24320;&#21457;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#29616;&#26377;&#30340;AGI&#23450;&#20041;&#65292;&#24182;&#25552;&#21462;&#20986;&#20102;&#19968;&#20010;&#26377;&#29992;&#30340;AGI&#26412;&#20307;&#35770;&#24212;&#28385;&#36275;&#30340;&#20845;&#20010;&#21407;&#21017;&#12290;&#36825;&#20123;&#21407;&#21017;&#21253;&#25324;&#20851;&#27880;&#33021;&#21147;&#32780;&#19981;&#26159;&#26426;&#21046;&#65307;&#20998;&#21035;&#35780;&#20272;&#24191;&#27867;&#24615;&#21644;&#24615;&#33021;&#65307;&#23450;&#20041;AGI&#36335;&#24452;&#19978;&#30340;&#38454;&#27573;&#65292;&#32780;&#19981;&#26159;&#19987;&#27880;&#20110;&#32456;&#28857;&#12290;&#22522;&#20110;&#36825;&#20123;&#21407;&#21017;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;AGI&#30340;&#23618;&#27425;&#8221;&#65292;&#26681;&#25454;&#33021;&#21147;&#30340;&#28145;&#24230;&#65288;&#24615;&#33021;&#65289;&#21644;&#24191;&#24230;&#65288;&#24191;&#27867;&#24615;&#65289;&#65292;&#24182;&#24605;&#32771;&#24403;&#21069;&#31995;&#32479;&#22914;&#20309;&#31526;&#21512;&#36825;&#20010;&#26412;&#20307;&#35770;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#23545;&#23454;&#29616;AGI&#25152;&#25552;&#20986;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a framework for classifying the capabilities and behavior of Artificial General Intelligence (AGI) models and their precursors. This framework introduces levels of AGI performance, generality, and autonomy. It is our hope that this framework will be useful in an analogous way to the levels of autonomous driving, by providing a common language to compare models, assess risks, and measure progress along the path to AGI. To develop our framework, we analyze existing definitions of AGI, and distill six principles that a useful ontology for AGI should satisfy. These principles include focusing on capabilities rather than mechanisms; separately evaluating generality and performance; and defining stages along the path toward AGI, rather than focusing on the endpoint. With these principles in mind, we propose 'Levels of AGI' based on depth (performance) and breadth (generality) of capabilities, and reflect on how current systems fit into this ontology. We discuss the challenging req
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#33539;&#24335;&#24212;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#30340;&#21487;&#33021;&#24615;&#21644;&#25361;&#25112;&#65292;&#25552;&#20986;&#24320;&#21457;&#19968;&#31181;&#36890;&#29992;&#25512;&#33616;&#31995;&#32479;&#65292;&#21487;&#20197;&#29992;&#20110;&#23569;&#26679;&#26412;&#23398;&#20064;&#65292;&#24182;&#22312;&#26410;&#30693;&#26032;&#39046;&#22495;&#20013;&#24555;&#36895;&#36866;&#24212;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.19251</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#25512;&#33616;&#31995;&#32479;&#65306;&#19968;&#31181;&#22240;&#26524;&#21435;&#20559;&#35265;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Pre-trained Recommender Systems: A Causal Debiasing Perspective. (arXiv:2310.19251v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19251
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#33539;&#24335;&#24212;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#30340;&#21487;&#33021;&#24615;&#21644;&#25361;&#25112;&#65292;&#25552;&#20986;&#24320;&#21457;&#19968;&#31181;&#36890;&#29992;&#25512;&#33616;&#31995;&#32479;&#65292;&#21487;&#20197;&#29992;&#20110;&#23569;&#26679;&#26412;&#23398;&#20064;&#65292;&#24182;&#22312;&#26410;&#30693;&#26032;&#39046;&#22495;&#20013;&#24555;&#36895;&#36866;&#24212;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;&#20110;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;/&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;&#34920;&#26126;&#20102;&#19968;&#31181;&#26032;&#30340;&#12289;&#26377;&#21069;&#26223;&#30340;&#35299;&#20915;&#26041;&#26696;&#24314;&#31435;&#33539;&#24335;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#20854;&#20013;&#27169;&#22411;&#21487;&#20197;&#22312;&#24191;&#27867;&#25551;&#36848;&#36890;&#29992;&#20219;&#21153;&#31354;&#38388;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#25104;&#21151;&#22320;&#36866;&#24212;&#35299;&#20915;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#65292;&#21363;&#20351;&#35757;&#32451;&#25968;&#25454;&#38750;&#24120;&#26377;&#38480;&#65288;&#22914;&#22312;&#38646;&#26679;&#26412;&#23398;&#20064;&#25110;&#23569;&#26679;&#26412;&#23398;&#20064;&#22330;&#26223;&#20013;&#65289;&#12290;&#21463;&#21040;&#36825;&#26679;&#30340;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#30740;&#31350;&#20102;&#23558;&#36825;&#31181;&#33539;&#24335;&#35843;&#25972;&#21040;&#25512;&#33616;&#31995;&#32479;&#39046;&#22495;&#30340;&#21487;&#33021;&#24615;&#21644;&#25361;&#25112;&#65292;&#36825;&#19968;&#39046;&#22495;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#35270;&#35282;&#19979;&#36739;&#23569;&#34987;&#35843;&#26597;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#25552;&#20986;&#24320;&#21457;&#19968;&#31181;&#36890;&#29992;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#23545;&#20174;&#19981;&#21516;&#39046;&#22495;&#20013;&#25552;&#21462;&#30340;&#36890;&#29992;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#25429;&#25417;&#21040;&#36890;&#29992;&#30340;&#20132;&#20114;&#27169;&#24335;&#65292;&#28982;&#21518;&#21487;&#20197;&#24555;&#36895;&#36866;&#24212;&#25552;&#21319;&#23569;&#26679;&#26412;&#23398;&#20064;&#24615;&#33021;&#65292;&#22312;&#26410;&#30693;&#26032;&#39046;&#22495;&#65288;&#25968;&#25454;&#26377;&#38480;&#65289;&#20013;&#21457;&#25381;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies on pre-trained vision/language models have demonstrated the practical benefit of a new, promising solution-building paradigm in AI where models can be pre-trained on broad data describing a generic task space and then adapted successfully to solve a wide range of downstream tasks, even when training data is severely limited (e.g., in zero- or few-shot learning scenarios). Inspired by such progress, we investigate in this paper the possibilities and challenges of adapting such a paradigm to the context of recommender systems, which is less investigated from the perspective of pre-trained model. In particular, we propose to develop a generic recommender that captures universal interaction patterns by training on generic user-item interaction data extracted from different domains, which can then be fast adapted to improve few-shot learning performance in unseen new domains (with limited data).  However, unlike vision/language data which share strong conformity in the semant
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#22810;&#30446;&#26631;&#23398;&#20064;&#65288;FMOL&#65289;&#26694;&#26550;&#65292;&#22312;&#28385;&#36275;&#22810;&#20195;&#29702;&#22810;&#20219;&#21153;&#23398;&#20064;&#24212;&#29992;&#30340;&#20998;&#24067;&#24335;&#24615;&#36136;&#21644;&#25968;&#25454;&#38544;&#31169;&#38656;&#27714;&#30340;&#21516;&#26102;&#65292;&#25903;&#25345;&#19981;&#21516;&#23458;&#25143;&#31471;&#19978;&#30340;&#19981;&#21516;&#30446;&#26631;&#20989;&#25968;&#38598;&#21512;&#12290;&#36890;&#36807;&#24341;&#20837;&#32852;&#37030;&#23398;&#20064;&#30340;&#33539;&#24335;&#65292;&#23558;&#22810;&#30446;&#26631;&#20248;&#21270;&#65288;MOO&#65289;&#25512;&#24191;&#21040;&#32852;&#37030;&#23398;&#20064;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2310.09866</link><description>&lt;p&gt;
&#32852;&#37030;&#22810;&#30446;&#26631;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Multi-Objective Learning. (arXiv:2310.09866v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09866
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#22810;&#30446;&#26631;&#23398;&#20064;&#65288;FMOL&#65289;&#26694;&#26550;&#65292;&#22312;&#28385;&#36275;&#22810;&#20195;&#29702;&#22810;&#20219;&#21153;&#23398;&#20064;&#24212;&#29992;&#30340;&#20998;&#24067;&#24335;&#24615;&#36136;&#21644;&#25968;&#25454;&#38544;&#31169;&#38656;&#27714;&#30340;&#21516;&#26102;&#65292;&#25903;&#25345;&#19981;&#21516;&#23458;&#25143;&#31471;&#19978;&#30340;&#19981;&#21516;&#30446;&#26631;&#20989;&#25968;&#38598;&#21512;&#12290;&#36890;&#36807;&#24341;&#20837;&#32852;&#37030;&#23398;&#20064;&#30340;&#33539;&#24335;&#65292;&#23558;&#22810;&#30446;&#26631;&#20248;&#21270;&#65288;MOO&#65289;&#25512;&#24191;&#21040;&#32852;&#37030;&#23398;&#20064;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#20960;&#24180;&#20013;&#65292;&#22810;&#30446;&#26631;&#20248;&#21270;&#65288;MOO&#65289;&#20316;&#20026;&#35768;&#22810;&#22810;&#20195;&#29702;&#22810;&#20219;&#21153;&#23398;&#20064;&#24212;&#29992;&#30340;&#22522;&#30784;&#38382;&#39064;&#20986;&#29616;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;MOO&#31639;&#27861;&#20173;&#23616;&#38480;&#20110;&#38598;&#20013;&#24335;&#23398;&#20064;&#29615;&#22659;&#65292;&#26080;&#27861;&#28385;&#36275;&#36825;&#20123;&#22810;&#20195;&#29702;&#22810;&#20219;&#21153;&#23398;&#20064;&#24212;&#29992;&#30340;&#20998;&#24067;&#24335;&#24615;&#36136;&#21644;&#25968;&#25454;&#38544;&#31169;&#38656;&#27714;&#12290;&#36825;&#28608;&#21457;&#20102;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#22810;&#30446;&#26631;&#23398;&#20064;&#65288;FMOL&#65289;&#26694;&#26550;&#65292;&#20854;&#20013;&#22810;&#20010;&#23458;&#25143;&#31471;&#22312;&#20445;&#25345;&#20182;&#20204;&#30340;&#35757;&#32451;&#25968;&#25454;&#31169;&#23494;&#30340;&#21516;&#26102;&#65292;&#20998;&#24067;&#24335;&#21327;&#20316;&#35299;&#20915;&#19968;&#20010;MOO&#38382;&#39064;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;FMOL&#26694;&#26550;&#20801;&#35768;&#19981;&#21516;&#23458;&#25143;&#31471;&#19978;&#30340;&#19981;&#21516;&#30446;&#26631;&#20989;&#25968;&#38598;&#21512;&#65292;&#20197;&#25903;&#25345;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#36825;&#39318;&#27425;&#23558;MOO&#24418;&#24335;&#21270;&#25512;&#24191;&#21040;&#32852;&#37030;&#23398;&#20064;&#33539;&#24335;&#20013;&#12290;&#23545;&#20110;&#36825;&#20010;FMOL&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#32852;&#37030;&#22810;&#30446;&#26631;&#20248;&#21270;&#65288;FMOO&#65289;&#31639;&#27861;&#65292;&#31216;&#20026;&#32852;&#37030;&#22810;&#26799;&#24230;&#19979;&#38477;&#24179;&#22343;&#65288;FMGDA&#65289;&#21644;&#32852;&#37030;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;Federated SGD&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, multi-objective optimization (MOO) emerges as a foundational problem underpinning many multi-agent multi-task learning applications. However, existing algorithms in MOO literature remain limited to centralized learning settings, which do not satisfy the distributed nature and data privacy needs of such multi-agent multi-task learning applications. This motivates us to propose a new federated multi-objective learning (FMOL) framework with multiple clients distributively and collaboratively solving an MOO problem while keeping their training data private. Notably, our FMOL framework allows a different set of objective functions across different clients to support a wide range of applications, which advances and generalizes the MOO formulation to the federated learning paradigm for the first time. For this FMOL framework, we propose two new federated multi-objective optimization (FMOO) algorithms called federated multi-gradient descent averaging (FMGDA) and federated stoc
&lt;/p&gt;</description></item><item><title>&#22312;&#21327;&#21464;&#37327;&#36716;&#31227;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#34920;&#31034;&#21305;&#37197;&#25439;&#22833;&#26469;&#20248;&#21270;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#22312;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#26435;&#34913;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#31639;&#27861;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#19968;&#31181;&#26410;&#32463;&#30740;&#31350;&#30340;&#38750;&#23545;&#31216;&#21327;&#21464;&#37327;&#36716;&#31227;&#35774;&#32622;&#12290;</title><link>http://arxiv.org/abs/2310.07535</link><description>&lt;p&gt;
&#22312;&#21327;&#21464;&#37327;&#36716;&#31227;&#19979;&#65292;&#20165;&#20973;&#23569;&#37327;&#27979;&#35797;&#26679;&#26412;&#25913;&#21892;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#30340;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Improving Fairness-Accuracy tradeoff with few Test Samples under Covariate Shift. (arXiv:2310.07535v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07535
&lt;/p&gt;
&lt;p&gt;
&#22312;&#21327;&#21464;&#37327;&#36716;&#31227;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#34920;&#31034;&#21305;&#37197;&#25439;&#22833;&#26469;&#20248;&#21270;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#22312;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#26435;&#34913;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#31639;&#27861;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#19968;&#31181;&#26410;&#32463;&#30740;&#31350;&#30340;&#38750;&#23545;&#31216;&#21327;&#21464;&#37327;&#36716;&#31227;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27979;&#35797;&#25968;&#25454;&#20013;&#30340;&#21327;&#21464;&#37327;&#36716;&#31227;&#20250;&#26174;&#33879;&#38477;&#20302;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#34920;&#29616;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#30830;&#20445;&#19981;&#21516;&#25935;&#24863;&#32676;&#20307;&#20043;&#38388;&#30340;&#20844;&#24179;&#24615;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#36825;&#28041;&#21450;&#21040;&#35832;&#22914;&#21009;&#20107;&#21496;&#27861;&#31561;&#31038;&#20250;&#24433;&#21709;&#12290;&#25105;&#20204;&#22312;&#26080;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#25805;&#20316;&#65292;&#21482;&#26377;&#19968;&#23567;&#32452;&#26080;&#26631;&#31614;&#30340;&#27979;&#35797;&#26679;&#26412;&#21644;&#19968;&#20010;&#24102;&#26631;&#31614;&#30340;&#35757;&#32451;&#38598;&#21487;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#20010;&#36129;&#29486;&#12290;&#31532;&#19968;&#20010;&#36129;&#29486;&#26159;&#22522;&#20110;&#26032;&#22411;&#22797;&#21512;&#21152;&#26435;&#29109;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#29992;&#20110;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#24182;&#36890;&#36807;&#34920;&#31034;&#21305;&#37197;&#25439;&#22833;&#26469;&#20248;&#21270;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#20351;&#29992;&#25105;&#20204;&#30340;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#20248;&#21270;&#65292;&#22312;&#20960;&#20010;&#26631;&#20934;&#25968;&#25454;&#38598;&#19978;&#22312;&#20844;&#24179;&#24615;-&#20934;&#30830;&#24615;&#26435;&#34913;&#26041;&#38754;&#20248;&#20110;&#35768;&#22810;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#31532;&#20108;&#20010;&#36129;&#29486;&#26159;&#19968;&#20010;&#26032;&#30340;&#35774;&#32622;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#38750;&#23545;&#31216;&#21327;&#21464;&#37327;&#36716;&#31227;&#65292;&#22312;&#25105;&#20204;&#25152;&#30693;&#30340;&#33539;&#22260;&#20869;&#23578;&#26410;&#30740;&#31350;&#36807;&#38750;&#23545;&#31216;&#21327;&#21464;&#37327;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;
Covariate shift in the test data can significantly downgrade both the accuracy and the fairness performance of the model. Ensuring fairness across different sensitive groups in such settings is of paramount importance due to societal implications like criminal justice. We operate under the unsupervised regime where only a small set of unlabeled test samples along with a labeled training set is available. Towards this problem, we make three contributions. First is a novel composite weighted entropy based objective for prediction accuracy which is optimized along with a representation matching loss for fairness. We experimentally verify that optimizing with our loss formulation outperforms a number of state-of-the-art baselines in the pareto sense with respect to the fairness-accuracy tradeoff on several standard datasets. Our second contribution is a new setting we term Asymmetric Covariate Shift that, to the best of our knowledge, has not been studied before. Asymmetric covariate shift
&lt;/p&gt;</description></item><item><title>&#36880;&#27493;&#21151;&#33021;&#37325;&#26500;&#30340;&#20851;&#31995;&#27010;&#24565;&#20998;&#26512;&#65288;RCA&#65289;&#26159;&#24418;&#24335;&#27010;&#24565;&#20998;&#26512;&#30340;&#25193;&#23637;&#65292;&#36890;&#36807;&#23450;&#20041;&#33391;&#26500;&#35299;&#20915;&#26041;&#26696;&#30340;&#31354;&#38388;&#21644;&#30456;&#20851;&#20989;&#25968;&#65292;&#35299;&#20915;&#20102;RCA&#22312;&#24490;&#29615;&#20381;&#36182;&#25968;&#25454;&#19978;&#36820;&#22238;&#21333;&#19968;&#27010;&#24565;&#26684;&#23478;&#26063;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.06441</link><description>&lt;p&gt;
&#36880;&#27493;&#21151;&#33021;&#37325;&#26500;&#30340;&#20851;&#31995;&#27010;&#24565;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Stepwise functional refoundation of relational concept analysis. (arXiv:2310.06441v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06441
&lt;/p&gt;
&lt;p&gt;
&#36880;&#27493;&#21151;&#33021;&#37325;&#26500;&#30340;&#20851;&#31995;&#27010;&#24565;&#20998;&#26512;&#65288;RCA&#65289;&#26159;&#24418;&#24335;&#27010;&#24565;&#20998;&#26512;&#30340;&#25193;&#23637;&#65292;&#36890;&#36807;&#23450;&#20041;&#33391;&#26500;&#35299;&#20915;&#26041;&#26696;&#30340;&#31354;&#38388;&#21644;&#30456;&#20851;&#20989;&#25968;&#65292;&#35299;&#20915;&#20102;RCA&#22312;&#24490;&#29615;&#20381;&#36182;&#25968;&#25454;&#19978;&#36820;&#22238;&#21333;&#19968;&#27010;&#24565;&#26684;&#23478;&#26063;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#27010;&#24565;&#20998;&#26512;&#65288;RCA&#65289;&#26159;&#24418;&#24335;&#27010;&#24565;&#20998;&#26512;&#30340;&#25193;&#23637;&#65292;&#20801;&#35768;&#21516;&#26102;&#22788;&#29702;&#22810;&#20010;&#30456;&#20851;&#30340;&#35821;&#22659;&#12290;&#23427;&#34987;&#35774;&#35745;&#29992;&#20110;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#25551;&#36848;&#36923;&#36753;&#29702;&#35770;&#65292;&#24182;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#20351;&#29992;&#12290;&#20851;&#20110;RCA&#30340;&#19968;&#20010;&#20196;&#20154;&#22256;&#24785;&#30340;&#35266;&#23519;&#26159;&#65292;&#23613;&#31649;&#25968;&#25454;&#23384;&#22312;&#24490;&#29615;&#20381;&#36182;&#20851;&#31995;&#65292;&#23427;&#36820;&#22238;&#19968;&#20010;&#21333;&#19968;&#30340;&#27010;&#24565;&#26684;&#23478;&#26063;&#65292;&#20854;&#20182;&#35299;&#20915;&#26041;&#26696;&#21487;&#33021;&#34987;&#35748;&#20026;&#26159;&#21487;&#25509;&#21463;&#30340;&#12290;RCA&#30340;&#35821;&#20041;&#20197;&#25805;&#20316;&#26041;&#24335;&#25552;&#20379;&#65292;&#23545;&#27492;&#38382;&#39064;&#24182;&#27809;&#26377;&#25552;&#20379;&#26126;&#30830;&#30340;&#35299;&#37322;&#12290;&#22312;&#26412;&#25253;&#21578;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#21487;&#25509;&#21463;&#30340;&#35299;&#20915;&#26041;&#26696;&#23450;&#20041;&#20026;&#23646;&#20110;&#21021;&#22987;&#35821;&#22659;&#30830;&#23450;&#30340;&#31354;&#38388;&#30340;&#27010;&#24565;&#26684;&#23478;&#26063;&#65288;&#33391;&#26500;&#65289;&#65292;&#19981;&#33021;&#25193;&#23637;&#26032;&#23646;&#24615;&#65288;&#39281;&#21644;&#65289;&#65292;&#24182;&#19988;&#20165;&#28041;&#21450;&#35813;&#23478;&#26063;&#30340;&#27010;&#24565;&#65288;&#33258;&#25903;&#25345;&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#23450;&#20041;&#33391;&#26500;&#35299;&#20915;&#26041;&#26696;&#30340;&#31354;&#38388;&#20197;&#21450;&#35813;&#31354;&#38388;&#19978;&#30340;&#20004;&#20010;&#20989;&#25968;&#65288;&#19968;&#20010;&#25193;&#24352;&#20989;&#25968;&#21644;&#19968;&#20010;&#25910;&#32553;&#20989;&#25968;&#65289;&#65292;&#37319;&#29992;&#21151;&#33021;&#35270;&#22270;&#26469;&#25551;&#36848;RCA&#36807;&#31243;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#25509;&#21463;&#30340;&#35299;&#20915;&#26041;&#26696;&#8230;
&lt;/p&gt;
&lt;p&gt;
Relational concept analysis (RCA) is an extension of formal concept analysis allowing to deal with several related contexts simultaneously. It has been designed for learning description logic theories from data and used within various applications. A puzzling observation about RCA is that it returns a single family of concept lattices although, when the data feature circular dependencies, other solutions may be considered acceptable. The semantics of RCA, provided in an operational way, does not shed light on this issue. In this report, we define these acceptable solutions as those families of concept lattices which belong to the space determined by the initial contexts (well-formed), cannot scale new attributes (saturated), and refer only to concepts of the family (self-supported). We adopt a functional view on the RCA process by defining the space of well-formed solutions and two functions on that space: one expansive and the other contractive. We show that the acceptable solutions a
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20998;&#26512;&#20154;&#31867;&#34892;&#20026;&#30340;&#32479;&#19968;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#26469;&#25429;&#25417;&#24207;&#21015;&#20013;&#30340;&#39640;&#38454;&#20381;&#36182;&#20851;&#31995;&#65292;&#20197;&#25913;&#36827;Web&#27983;&#35272;&#25110;&#20132;&#36890;&#23548;&#33322;&#31561;&#24212;&#29992;&#30340;&#24213;&#23618;&#22522;&#30784;&#35774;&#26045;&#25110;&#29992;&#25143;&#30028;&#38754;&#12290;</title><link>http://arxiv.org/abs/2310.04477</link><description>&lt;p&gt;
&#39640;&#38454;DeepTrails&#65306;&#23545;*Trails&#30340;&#32479;&#19968;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Higher-Order DeepTrails: Unified Approach to *Trails. (arXiv:2310.04477v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04477
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20998;&#26512;&#20154;&#31867;&#34892;&#20026;&#30340;&#32479;&#19968;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#26469;&#25429;&#25417;&#24207;&#21015;&#20013;&#30340;&#39640;&#38454;&#20381;&#36182;&#20851;&#31995;&#65292;&#20197;&#25913;&#36827;Web&#27983;&#35272;&#25110;&#20132;&#36890;&#23548;&#33322;&#31561;&#24212;&#29992;&#30340;&#24213;&#23618;&#22522;&#30784;&#35774;&#26045;&#25110;&#29992;&#25143;&#30028;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#26512;&#12289;&#29702;&#35299;&#21644;&#25551;&#36848;&#20154;&#31867;&#34892;&#20026;&#22312;&#19981;&#21516;&#30340;&#29615;&#22659;&#20013;&#20855;&#26377;&#20248;&#21183;&#65292;&#22914;&#32593;&#32476;&#27983;&#35272;&#25110;&#20132;&#36890;&#23548;&#33322;&#12290;&#29702;&#35299;&#20154;&#31867;&#34892;&#20026;&#33258;&#28982;&#26377;&#21161;&#20110;&#25913;&#36827;&#21644;&#20248;&#21270;&#24213;&#23618;&#22522;&#30784;&#35774;&#26045;&#25110;&#29992;&#25143;&#30028;&#38754;&#12290;&#36890;&#24120;&#65292;&#20154;&#31867;&#23548;&#33322;&#26159;&#30001;&#29366;&#24577;&#38388;&#36716;&#25442;&#30340;&#24207;&#21015;&#34920;&#31034;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#24314;&#35758;&#20351;&#29992;&#20551;&#35774;&#26469;&#20998;&#26512;&#36825;&#20123;&#36716;&#25442;&#65292;&#20195;&#34920;&#19981;&#21516;&#30340;&#23548;&#33322;&#30452;&#35266;&#12290;&#20026;&#20102;&#22312;&#25968;&#23398;&#19978;&#25235;&#20303;&#36825;&#20010;&#35774;&#32622;&#65292;&#20351;&#29992;&#19968;&#38454;&#39532;&#23572;&#21487;&#22827;&#38142;&#26469;&#25429;&#25417;&#34892;&#20026;&#65292;&#22240;&#27492;&#21487;&#20197;&#24212;&#29992;&#19981;&#21516;&#31867;&#22411;&#30340;&#22270;&#27604;&#36739;&#65292;&#20294;&#26159;&#20250;&#26377;&#25439;&#22833;&#24207;&#21015;&#20013;&#30340;&#39640;&#38454;&#20381;&#36182;&#20449;&#24687;&#30340;&#22266;&#26377;&#32570;&#28857;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#26469;&#20998;&#26512;&#25972;&#20010;&#24207;&#21015;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#24120;&#29992;&#20110;&#24314;&#27169;&#24207;&#21015;&#20013;&#30340;&#39640;&#38454;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#36731;&#26494;&#22320;&#36866;&#24212;&#19981;&#21516;&#30340;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
Analyzing, understanding, and describing human behavior is advantageous in different settings, such as web browsing or traffic navigation. Understanding human behavior naturally helps to improve and optimize the underlying infrastructure or user interfaces. Typically, human navigation is represented by sequences of transitions between states. Previous work suggests to use hypotheses, representing different intuitions about the navigation to analyze these transitions. To mathematically grasp this setting, first-order Markov chains are used to capture the behavior, consequently allowing to apply different kinds of graph comparisons, but comes with the inherent drawback of losing information about higher-order dependencies within the sequences. To this end, we propose to analyze entire sequences using autoregressive language models, as they are traditionally used to model higher-order dependencies in sequences. We show that our approach can be easily adapted to model different settings in
&lt;/p&gt;</description></item><item><title>STAMP&#26159;&#19968;&#31181;&#22522;&#20110;Stein&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#24182;&#34892;&#21270;&#21644;&#21487;&#24494;&#20223;&#30495;&#39640;&#25928;&#22320;&#25628;&#32034;&#22810;&#20010;&#22810;&#26679;&#21270;&#30340;&#20219;&#21153;&#21644;&#36816;&#21160;&#35268;&#21010;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2310.01775</link><description>&lt;p&gt;
STAMP&#65306;&#36890;&#36807;Stein&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;&#23454;&#29616;&#21487;&#24494;&#30340;&#20219;&#21153;&#21644;&#36816;&#21160;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
STAMP: Differentiable Task and Motion Planning via Stein Variational Gradient Descent. (arXiv:2310.01775v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01775
&lt;/p&gt;
&lt;p&gt;
STAMP&#26159;&#19968;&#31181;&#22522;&#20110;Stein&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#24182;&#34892;&#21270;&#21644;&#21487;&#24494;&#20223;&#30495;&#39640;&#25928;&#22320;&#25628;&#32034;&#22810;&#20010;&#22810;&#26679;&#21270;&#30340;&#20219;&#21153;&#21644;&#36816;&#21160;&#35268;&#21010;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#25805;&#20316;&#20219;&#21153;&#65292;&#22914;&#20351;&#29992;&#24037;&#20855;&#25110;&#35013;&#37197;&#38646;&#20214;&#65292;&#24448;&#24448;&#38656;&#35201;&#31526;&#21495;&#21644;&#20960;&#20309;&#25512;&#29702;&#12290;&#20219;&#21153;&#21644;&#36816;&#21160;&#35268;&#21010;&#65288;TAMP&#65289;&#31639;&#27861;&#36890;&#24120;&#36890;&#36807;&#23545;&#39640;&#32423;&#20219;&#21153;&#24207;&#21015;&#36827;&#34892;&#26641;&#25628;&#32034;&#24182;&#26816;&#26597;&#36816;&#21160;&#23398;&#21644;&#21160;&#21147;&#23398;&#21487;&#34892;&#24615;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#34429;&#28982;&#24615;&#33021;&#33391;&#22909;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#31639;&#27861;&#30340;&#25928;&#29575;&#38750;&#24120;&#20302;&#65292;&#22240;&#20026;&#20854;&#26102;&#38388;&#22797;&#26434;&#24615;&#38543;&#21487;&#33021;&#21160;&#20316;&#21644;&#29289;&#20307;&#25968;&#37327;&#30340;&#22686;&#21152;&#21576;&#25351;&#25968;&#22686;&#38271;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#21482;&#33021;&#25214;&#21040;&#21333;&#20010;&#35299;&#20915;&#26041;&#26696;&#65292;&#32780;&#21487;&#33021;&#23384;&#22312;&#35768;&#22810;&#21487;&#34892;&#30340;&#35745;&#21010;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Stein&#20219;&#21153;&#21644;&#36816;&#21160;&#35268;&#21010;&#65288;STAMP&#65289;&#30340;&#26032;&#31639;&#27861;&#65292;&#23427;&#21033;&#29992;&#24182;&#34892;&#21270;&#21644;&#21487;&#24494;&#20223;&#30495;&#26469;&#39640;&#25928;&#22320;&#25628;&#32034;&#22810;&#20010;&#22810;&#26679;&#21270;&#30340;&#35745;&#21010;&#12290;STAMP&#23558;&#31163;&#25955;&#21644;&#36830;&#32493;&#30340;TAMP&#38382;&#39064;&#36716;&#21270;&#20026;&#21487;&#20197;&#20351;&#29992;&#21464;&#20998;&#25512;&#26029;&#35299;&#20915;&#30340;&#36830;&#32493;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#22522;&#20110;Stein&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;&#65292;&#19968;&#31181;&#27010;&#29575;&#25512;&#26029;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Planning for many manipulation tasks, such as using tools or assembling parts, often requires both symbolic and geometric reasoning. Task and Motion Planning (TAMP) algorithms typically solve these problems by conducting a tree search over high-level task sequences while checking for kinematic and dynamic feasibility. While performant, most existing algorithms are highly inefficient as their time complexity grows exponentially with the number of possible actions and objects. Additionally, they only find a single solution to problems in which many feasible plans may exist. To address these limitations, we propose a novel algorithm called Stein Task and Motion Planning (STAMP) that leverages parallelization and differentiable simulation to efficiently search for multiple diverse plans. STAMP relaxes discrete-and-continuous TAMP problems into continuous optimization problems that can be solved using variational inference. Our algorithm builds upon Stein Variational Gradient Descent, a gra
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#36731;&#37327;&#32423;&#33258;&#30417;&#30563;&#21333;&#30524;&#28145;&#24230;&#20272;&#35745;&#30340;&#28145;&#24230;&#37051;&#23621;&#23618;&#32858;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#19978;&#19979;&#25991;&#29305;&#24449;&#34701;&#21512;&#21644;&#36731;&#37327;&#32423;&#36890;&#36947;&#27880;&#24847;&#21147;&#65292;&#22312;&#20943;&#23569;&#21442;&#25968;&#25968;&#37327;&#30340;&#21516;&#26102;&#20445;&#25345;&#20934;&#30830;&#24615;&#65292;&#22312;KITTI&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.09272</link><description>&lt;p&gt;
&#28145;&#24230;&#37051;&#23621;&#23618;&#32858;&#21512;&#29992;&#20110;&#36731;&#37327;&#32423;&#33258;&#30417;&#30563;&#21333;&#30524;&#28145;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Deep Neighbor Layer Aggregation for Lightweight Self-Supervised Monocular Depth Estimation. (arXiv:2309.09272v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09272
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#36731;&#37327;&#32423;&#33258;&#30417;&#30563;&#21333;&#30524;&#28145;&#24230;&#20272;&#35745;&#30340;&#28145;&#24230;&#37051;&#23621;&#23618;&#32858;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#19978;&#19979;&#25991;&#29305;&#24449;&#34701;&#21512;&#21644;&#36731;&#37327;&#32423;&#36890;&#36947;&#27880;&#24847;&#21147;&#65292;&#22312;&#20943;&#23569;&#21442;&#25968;&#25968;&#37327;&#30340;&#21516;&#26102;&#20445;&#25345;&#20934;&#30830;&#24615;&#65292;&#22312;KITTI&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#33258;&#30417;&#30563;&#21333;&#30524;&#28145;&#24230;&#20272;&#35745;&#22312;&#26426;&#22120;&#20154;&#21644;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#30340;&#39057;&#32321;&#20351;&#29992;&#65292;&#27169;&#22411;&#30340;&#25928;&#29575;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#30446;&#21069;&#22823;&#22810;&#25968;&#26041;&#27861;&#37319;&#29992;&#26356;&#22823;&#12289;&#26356;&#22797;&#26434;&#30340;&#32593;&#32476;&#26469;&#25552;&#39640;&#28145;&#24230;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;&#19968;&#20123;&#30740;&#31350;&#32773;&#23558;Transformer&#24341;&#20837;&#21040;&#33258;&#30417;&#30563;&#21333;&#30524;&#28145;&#24230;&#20272;&#35745;&#20013;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#23548;&#33268;&#21442;&#25968;&#21644;&#35745;&#31639;&#37327;&#36739;&#39640;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#19978;&#19979;&#25991;&#29305;&#24449;&#34701;&#21512;&#30340;&#23436;&#20840;&#21367;&#31215;&#28145;&#24230;&#20272;&#35745;&#32593;&#32476;&#12290;&#19982;UNet++&#21644;HRNet&#19981;&#21516;&#65292;&#25105;&#20204;&#20351;&#29992;&#39640;&#20998;&#36776;&#29575;&#21644;&#20302;&#20998;&#36776;&#29575;&#29305;&#24449;&#26469;&#20445;&#30041;&#23567;&#30446;&#26631;&#21644;&#24555;&#36895;&#31227;&#21160;&#29289;&#20307;&#30340;&#20449;&#24687;&#65292;&#32780;&#19981;&#26159;&#36827;&#34892;&#36828;&#31243;&#34701;&#21512;&#12290;&#22312;&#35299;&#30721;&#22120;&#38454;&#27573;&#65292;&#25105;&#20204;&#36824;&#37319;&#29992;&#22522;&#20110;&#21367;&#31215;&#30340;&#36731;&#37327;&#32423;&#36890;&#36947;&#27880;&#24847;&#21147;&#26469;&#25552;&#21319;&#28145;&#24230;&#20272;&#35745;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20943;&#23569;&#20102;&#21442;&#25968;&#25968;&#37327;&#32780;&#19981;&#25439;&#22833;&#20934;&#30830;&#24615;&#12290;&#22312;KITTI&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the frequent use of self-supervised monocular depth estimation in robotics and autonomous driving, the model's efficiency is becoming increasingly important. Most current approaches apply much larger and more complex networks to improve the precision of depth estimation. Some researchers incorporated Transformer into self-supervised monocular depth estimation to achieve better performance. However, this method leads to high parameters and high computation. We present a fully convolutional depth estimation network using contextual feature fusion. Compared to UNet++ and HRNet, we use high-resolution and low-resolution features to reserve information on small targets and fast-moving objects instead of long-range fusion. We further promote depth estimation results employing lightweight channel attention based on convolution in the decoder stage. Our method reduces the parameters without sacrificing accuracy. Experiments on the KITTI benchmark show that our method can get better result
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#36830;&#32493;&#24314;&#27169;&#26041;&#27861;&#65292;&#29992;&#20110;&#35821;&#38899;&#22686;&#24378;&#21435;&#22122;&#36807;&#31243;&#12290;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#34920;&#31034;&#19981;&#21516;&#29366;&#24577;&#21464;&#37327;&#65292;&#24182;&#20351;&#29992;&#25511;&#21046;&#22240;&#23376;&#23454;&#29616;&#21487;&#25511;&#30340;&#38477;&#22122;&#27700;&#24179;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#28165;&#26224;&#30446;&#26631;&#20013;&#20445;&#30041;&#23569;&#37327;&#22122;&#22768;&#21487;&#20197;&#25913;&#21892;&#35821;&#38899;&#22686;&#24378;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.09270</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35821;&#38899;&#22686;&#24378;&#21435;&#22122;&#36807;&#31243;&#30340;&#36830;&#32493;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Continuous Modeling of the Denoising Process for Speech Enhancement Based on Deep Learning. (arXiv:2309.09270v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09270
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#36830;&#32493;&#24314;&#27169;&#26041;&#27861;&#65292;&#29992;&#20110;&#35821;&#38899;&#22686;&#24378;&#21435;&#22122;&#36807;&#31243;&#12290;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#34920;&#31034;&#19981;&#21516;&#29366;&#24577;&#21464;&#37327;&#65292;&#24182;&#20351;&#29992;&#25511;&#21046;&#22240;&#23376;&#23454;&#29616;&#21487;&#25511;&#30340;&#38477;&#22122;&#27700;&#24179;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#28165;&#26224;&#30446;&#26631;&#20013;&#20445;&#30041;&#23569;&#37327;&#22122;&#22768;&#21487;&#20197;&#25913;&#21892;&#35821;&#38899;&#22686;&#24378;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36830;&#32493;&#24314;&#27169;&#30340;&#28145;&#24230;&#23398;&#20064;&#35821;&#38899;&#22686;&#24378;&#21435;&#22122;&#26041;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#21435;&#22122;&#36807;&#31243;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#29366;&#24577;&#21464;&#37327;&#26469;&#34920;&#31034;&#21435;&#22122;&#36807;&#31243;&#65292;&#36215;&#22987;&#29366;&#24577;&#20026;&#22122;&#22768;&#35821;&#38899;&#65292;&#32467;&#26463;&#29366;&#24577;&#20026;&#28165;&#26224;&#35821;&#38899;&#12290;&#29366;&#24577;&#21464;&#37327;&#20013;&#30340;&#22122;&#22768;&#25104;&#20998;&#38543;&#30528;&#29366;&#24577;&#25351;&#25968;&#30340;&#21464;&#21270;&#32780;&#36880;&#28176;&#20943;&#23569;&#65292;&#30452;&#21040;&#22122;&#22768;&#25104;&#20998;&#20026;0&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#31867;&#20284;UNet&#30340;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#20272;&#35745;&#20174;&#36830;&#32493;&#21435;&#22122;&#36807;&#31243;&#20013;&#37319;&#26679;&#24471;&#21040;&#30340;&#27599;&#20010;&#29366;&#24577;&#21464;&#37327;&#12290;&#22312;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#19968;&#20010;&#25511;&#21046;&#22240;&#23376;&#20316;&#20026;&#23884;&#20837;&#21521;&#37327;&#65292;&#33539;&#22260;&#20174;&#38646;&#21040;&#19968;&#65292;&#29992;&#20110;&#25511;&#21046;&#38477;&#22122;&#27700;&#24179;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#23454;&#29616;&#21487;&#25511;&#30340;&#35821;&#38899;&#22686;&#24378;&#65292;&#24182;&#21487;&#20197;&#36866;&#24212;&#21508;&#31181;&#24212;&#29992;&#22330;&#26223;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#28165;&#26224;&#30446;&#26631;&#20013;&#20445;&#30041;&#23569;&#37327;&#22122;&#22768;&#26377;&#21161;&#20110;&#35821;&#38899;&#22686;&#24378;&#65292;&#35777;&#26126;&#20102;&#23458;&#35266;&#35821;&#38899;&#25351;&#26631;&#21644;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we explore a continuous modeling approach for deep-learning-based speech enhancement, focusing on the denoising process. We use a state variable to indicate the denoising process. The starting state is noisy speech and the ending state is clean speech. The noise component in the state variable decreases with the change of the state index until the noise component is 0. During training, a UNet-like neural network learns to estimate every state variable sampled from the continuous denoising process. In testing, we introduce a controlling factor as an embedding, ranging from zero to one, to the neural network, allowing us to control the level of noise reduction. This approach enables controllable speech enhancement and is adaptable to various application scenarios. Experimental results indicate that preserving a small amount of noise in the clean target benefits speech enhancement, as evidenced by improvements in both objective speech measures and automatic speech recogniti
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#29702;&#35299;&#22797;&#26434;&#25351;&#20196;&#33021;&#21147;&#30340;&#22522;&#20934;&#8212;&#8212;CELLO&#12290;&#36890;&#36807;&#35774;&#35745;&#22797;&#26434;&#25351;&#20196;&#30340;&#20843;&#20010;&#29305;&#24449;&#24182;&#26500;&#24314;&#20840;&#38754;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#35299;&#20915;&#29616;&#26377;&#22522;&#20934;&#30340;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2309.09150</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#29702;&#35299;&#30495;&#23454;&#19990;&#30028;&#22797;&#26434;&#25351;&#20196;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models Understand Real-World Complex Instructions?. (arXiv:2309.09150v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09150
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#29702;&#35299;&#22797;&#26434;&#25351;&#20196;&#33021;&#21147;&#30340;&#22522;&#20934;&#8212;&#8212;CELLO&#12290;&#36890;&#36807;&#35774;&#35745;&#22797;&#26434;&#25351;&#20196;&#30340;&#20843;&#20010;&#29305;&#24449;&#24182;&#26500;&#24314;&#20840;&#38754;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#35299;&#20915;&#29616;&#26377;&#22522;&#20934;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#22815;&#29702;&#35299;&#20154;&#31867;&#25351;&#20196;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#20256;&#32479;NLP&#20219;&#21153;&#20043;&#22806;&#30340;&#23454;&#29992;&#24212;&#29992;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20173;&#28982;&#22312;&#22797;&#26434;&#25351;&#20196;&#19978;&#23384;&#22312;&#22256;&#38590;&#65292;&#36825;&#20123;&#25351;&#20196;&#21487;&#20197;&#26159;&#38656;&#35201;&#22810;&#20010;&#20219;&#21153;&#21644;&#32422;&#26463;&#30340;&#22797;&#26434;&#20219;&#21153;&#25551;&#36848;&#65292;&#25110;&#32773;&#21253;&#21547;&#38271;&#31687;&#32972;&#26223;&#12289;&#22122;&#22768;&#12289;&#24322;&#26500;&#20449;&#24687;&#21644;&#22810;&#36718;&#26684;&#24335;&#30340;&#22797;&#26434;&#36755;&#20837;&#12290;&#30001;&#20110;&#36825;&#20123;&#29305;&#28857;&#65292;LLMs&#24120;&#24120;&#24573;&#30053;&#20219;&#21153;&#25551;&#36848;&#20013;&#30340;&#35821;&#20041;&#32422;&#26463;&#65292;&#20135;&#29983;&#38169;&#35823;&#30340;&#26684;&#24335;&#65292;&#36829;&#21453;&#38271;&#24230;&#25110;&#26679;&#26412;&#35745;&#25968;&#30340;&#32422;&#26463;&#65292;&#23545;&#36755;&#20837;&#25991;&#26412;&#19981;&#24544;&#23454;&#12290;&#29616;&#26377;&#30340;&#22522;&#20934;&#19981;&#36275;&#20197;&#35780;&#20272;LLMs&#29702;&#35299;&#22797;&#26434;&#25351;&#20196;&#30340;&#33021;&#21147;&#65292;&#22240;&#20026;&#23427;&#20204;&#26159;&#23553;&#38381;&#24335;&#21644;&#31616;&#21333;&#30340;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#38388;&#38553;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CELLO&#65292;&#19968;&#20010;&#29992;&#20110;&#31995;&#32479;&#35780;&#20272;LLMs&#36981;&#24490;&#22797;&#26434;&#25351;&#20196;&#33021;&#21147;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#20026;&#22797;&#26434;&#25351;&#20196;&#35774;&#35745;&#20102;&#20843;&#20010;&#29305;&#24449;&#65292;&#24182;&#20174;&#29616;&#23454;&#22330;&#26223;&#20013;&#26500;&#24314;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36824;&#24314;&#31435;&#20102;&#22235;&#20010;&#35780;&#20215;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) can understand human instructions, showing their potential for pragmatic applications beyond traditional NLP tasks. However, they still struggle with complex instructions, which can be either complex task descriptions that require multiple tasks and constraints, or complex input that contains long context, noise, heterogeneous information and multi-turn format. Due to these features, LLMs often ignore semantic constraints from task descriptions, generate incorrect formats, violate length or sample count constraints, and be unfaithful to the input text. Existing benchmarks are insufficient to assess LLMs' ability to understand complex instructions, as they are close-ended and simple. To bridge this gap, we propose CELLO, a benchmark for evaluating LLMs' ability to follow complex instructions systematically. We design eight features for complex instructions and construct a comprehensive evaluation dataset from real-world scenarios. We also establish four crit
&lt;/p&gt;</description></item><item><title>TextBind&#26159;&#19968;&#20010;&#27880;&#37322;&#26497;&#23569;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#36739;&#22823;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#36171;&#20104;&#22810;&#36718;&#20132;&#38169;&#22810;&#27169;&#24577;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#22270;&#20687;-&#26631;&#39064;&#23545;&#29983;&#25104;&#22810;&#36718;&#22810;&#27169;&#24577;&#25351;&#20196;-&#22238;&#24212;&#23545;&#35805;&#12290;&#36825;&#20010;&#26694;&#26550;&#23545;&#20110;&#35299;&#20915;&#23454;&#38469;&#20219;&#21153;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#21644;&#28436;&#31034;&#12290;</title><link>http://arxiv.org/abs/2309.08637</link><description>&lt;p&gt;
TextBind: &#22810;&#36718;&#20132;&#38169;&#22810;&#27169;&#24577;&#25351;&#20196;&#36319;&#38543;
&lt;/p&gt;
&lt;p&gt;
TextBind: Multi-turn Interleaved Multimodal Instruction-following. (arXiv:2309.08637v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08637
&lt;/p&gt;
&lt;p&gt;
TextBind&#26159;&#19968;&#20010;&#27880;&#37322;&#26497;&#23569;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#36739;&#22823;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#36171;&#20104;&#22810;&#36718;&#20132;&#38169;&#22810;&#27169;&#24577;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#22270;&#20687;-&#26631;&#39064;&#23545;&#29983;&#25104;&#22810;&#36718;&#22810;&#27169;&#24577;&#25351;&#20196;-&#22238;&#24212;&#23545;&#35805;&#12290;&#36825;&#20010;&#26694;&#26550;&#23545;&#20110;&#35299;&#20915;&#23454;&#38469;&#20219;&#21153;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#21644;&#28436;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20135;&#29983;&#20102;&#38761;&#21629;&#24615;&#30340;&#24433;&#21709;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#36807;&#20854;&#33258;&#28982;&#35821;&#35328;&#30028;&#38754;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21487;&#20197;&#35299;&#20915;&#21508;&#31181;&#23454;&#38469;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#39640;&#36136;&#37327;&#30340;&#31034;&#20363;&#25968;&#25454;&#65292;&#32780;&#36825;&#24448;&#24448;&#24456;&#38590;&#33719;&#24471;&#12290;&#24403;&#28041;&#21450;&#21040;&#22810;&#27169;&#24577;&#25351;&#20196;&#36319;&#38543;&#26102;&#65292;&#36825;&#20010;&#25361;&#25112;&#21464;&#24471;&#26356;&#21152;&#20005;&#23803;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;TextBind&#65292;&#36825;&#26159;&#19968;&#20010;&#20960;&#20046;&#19981;&#38656;&#35201;&#27880;&#37322;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#36171;&#20104;&#36739;&#22823;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#22810;&#36718;&#20132;&#38169;&#22810;&#27169;&#24577;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20165;&#38656;&#35201;&#22270;&#20687;-&#26631;&#39064;&#23545;&#65292;&#24182;&#20174;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#22810;&#36718;&#22810;&#27169;&#24577;&#25351;&#20196;-&#22238;&#24212;&#23545;&#35805;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#21644;&#28436;&#31034;&#65292;&#20197;&#20419;&#36827;&#26410;&#26469;&#22312;&#22810;&#27169;&#24577;&#25351;&#20196;&#36319;&#38543;&#39046;&#22495;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models with instruction-following abilities have revolutionized the field of artificial intelligence. These models show exceptional generalizability to tackle various real-world tasks through their natural language interfaces. However, their performance heavily relies on high-quality exemplar data, which is often difficult to obtain. This challenge is further exacerbated when it comes to multimodal instruction following. We introduce TextBind, an almost annotation-free framework for empowering larger language models with the multi-turn interleaved multimodal instruction-following capabilities. Our approach requires only image-caption pairs and generates multi-turn multimodal instruction-response conversations from a language model. We release our dataset, model, and demo to foster future research in the area of multimodal instruction following.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#35774;&#32622;&#19979;&#30340;&#19968;&#33324;&#24615;&#23376;&#27169;&#26368;&#22823;&#21270;&#38382;&#39064;&#65292;&#24182;&#23558;&#19968;&#31867;&#22823;&#22411;&#23376;&#27169;&#20989;&#25968;&#24402;&#32422;&#21040;&#22312;&#32447;&#20984;&#20248;&#21270;&#38382;&#39064;&#20013;&#12290;&#36825;&#31181;&#24402;&#32422;&#26041;&#24335;&#21487;&#22312;&#32452;&#21512;&#20248;&#21270;&#20013;&#23454;&#29616;&#27425;&#32447;&#24615;&#36951;&#25022;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#35768;&#22810;&#19981;&#21516;&#29256;&#26412;&#30340;&#22312;&#32447;&#23398;&#20064;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.04339</link><description>&lt;p&gt;
&#36890;&#36807;&#22312;&#32447;&#20984;&#20248;&#21270;&#23454;&#29616;&#22312;&#32447;&#23376;&#27169;&#26368;&#22823;&#21270;
&lt;/p&gt;
&lt;p&gt;
Online Submodular Maximization via Online Convex Optimization. (arXiv:2309.04339v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04339
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#35774;&#32622;&#19979;&#30340;&#19968;&#33324;&#24615;&#23376;&#27169;&#26368;&#22823;&#21270;&#38382;&#39064;&#65292;&#24182;&#23558;&#19968;&#31867;&#22823;&#22411;&#23376;&#27169;&#20989;&#25968;&#24402;&#32422;&#21040;&#22312;&#32447;&#20984;&#20248;&#21270;&#38382;&#39064;&#20013;&#12290;&#36825;&#31181;&#24402;&#32422;&#26041;&#24335;&#21487;&#22312;&#32452;&#21512;&#20248;&#21270;&#20013;&#23454;&#29616;&#27425;&#32447;&#24615;&#36951;&#25022;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#35768;&#22810;&#19981;&#21516;&#29256;&#26412;&#30340;&#22312;&#32447;&#23398;&#20064;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32447;&#35774;&#32622;&#19979;&#30340;&#19968;&#33324;&#24615;&#23376;&#27169;&#26368;&#22823;&#21270;&#38382;&#39064;&#22312;&#19968;&#33324;&#24615;&#27169;&#24615;&#32422;&#26463;&#19979;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#32447;&#20248;&#21270;&#19968;&#31867;&#22823;&#22411;&#23376;&#27169;&#20989;&#25968;&#65292;&#21363;&#21152;&#26435;&#38408;&#20540;&#21183;&#20989;&#25968;&#65292;&#21487;&#20197;&#24402;&#32422;&#21040;&#22312;&#32447;&#20984;&#20248;&#21270;(OCO)&#38382;&#39064;&#12290;&#36825;&#26159;&#22240;&#20026;&#36825;&#20010;&#31867;&#21035;&#30340;&#20989;&#25968;&#21487;&#20197;&#36827;&#34892;&#20985;&#26494;&#24347;;&#22240;&#27492;&#65292;&#32467;&#21512;&#36866;&#24403;&#30340;&#33293;&#20837;&#26041;&#26696;&#65292;OCO&#31574;&#30053;&#21487;&#20197;&#22312;&#32452;&#21512;&#35774;&#32622;&#20013;&#23454;&#29616;&#27425;&#32447;&#24615;&#36951;&#25022;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#31616;&#21270;&#26041;&#24335;&#21487;&#20197;&#24212;&#29992;&#22312;&#35768;&#22810;&#19981;&#21516;&#29256;&#26412;&#30340;&#22312;&#32447;&#23398;&#20064;&#38382;&#39064;&#20013;&#65292;&#21253;&#25324;&#21160;&#24577;&#36951;&#25022;&#12289;&#24378;&#30423;&#21644;&#20048;&#35266;&#23398;&#20064;&#31561;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study monotone submodular maximization under general matroid constraints in the online setting. We prove that online optimization of a large class of submodular functions, namely, weighted threshold potential functions, reduces to online convex optimization (OCO). This is precisely because functions in this class admit a concave relaxation; as a result, OCO policies, coupled with an appropriate rounding scheme, can be used to achieve sublinear regret in the combinatorial setting. We show that our reduction extends to many different versions of the online learning problem, including the dynamic regret, bandit, and optimistic-learning settings.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#31034;&#31574;&#30053;&#65292;&#26412;&#35770;&#25991;&#25506;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#24212;&#29992;&#35282;&#33394;&#25198;&#28436;&#21644;&#24605;&#32500;&#38142;&#25552;&#31034;&#31574;&#30053;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#24773;&#24863;&#20998;&#26512;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#19977;&#20010;&#19981;&#21516;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2309.02045</link><description>&lt;p&gt;
&#36890;&#36807;&#25552;&#31034;&#31574;&#30053;&#22686;&#24378;&#35780;&#35770;&#25991;&#26412;&#30340;&#22810;&#39046;&#22495;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Enhance Multi-domain Sentiment Analysis of Review Texts through Prompting Strategies. (arXiv:2309.02045v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02045
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#31034;&#31574;&#30053;&#65292;&#26412;&#35770;&#25991;&#25506;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#24212;&#29992;&#35282;&#33394;&#25198;&#28436;&#21644;&#24605;&#32500;&#38142;&#25552;&#31034;&#31574;&#30053;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#24773;&#24863;&#20998;&#26512;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#19977;&#20010;&#19981;&#21516;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#31185;&#23398;&#30740;&#31350;&#21644;&#23454;&#38469;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#29616;&#26377;&#30740;&#31350;&#24050;&#35777;&#26126;&#20102;LLMs&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#26368;&#26032;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#36890;&#36807;&#25552;&#31034;&#31574;&#30053;&#36827;&#19968;&#27493;&#22686;&#24378;LLMs&#22312;&#29305;&#23450;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#36890;&#36807;&#24212;&#29992;&#25552;&#31034;&#31574;&#30053;&#26469;&#25552;&#39640;LLMs&#22312;&#24773;&#24863;&#20998;&#26512;&#20013;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23545;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#30340;&#25552;&#31034;&#36807;&#31243;&#36827;&#34892;&#20102;&#24314;&#27169;&#65292;&#24182;&#20171;&#32461;&#20102;&#20004;&#31181;&#38024;&#23545;&#24773;&#24863;&#20998;&#26512;&#30340;&#26032;&#39062;&#31574;&#30053;&#65306;&#35282;&#33394;&#25198;&#28436;&#65288;RP&#65289;&#25552;&#31034;&#21644;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#25552;&#31034;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;RP-CoT&#25552;&#31034;&#31574;&#30053;&#65292;&#23427;&#26159;RP&#25552;&#31034;&#21644;CoT&#25552;&#31034;&#30340;&#32467;&#21512;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#19981;&#21516;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27604;&#36739;&#23454;&#39564;&#65292;&#20197;&#35780;&#20272;&#25152;&#25552;&#20986;&#30340;&#24773;&#24863;&#20998;&#26512;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have made significant strides in both scientific research and practical applications. Existing studies have demonstrated the state-of-the-art (SOTA) performance of LLMs in various natural language processing tasks. However, the question of how to further enhance LLMs' performance in specific task using prompting strategies remains a pivotal concern. This paper explores the enhancement of LLMs' performance in sentiment analysis through the application of prompting strategies. We formulate the process of prompting for sentiment analysis tasks and introduce two novel strategies tailored for sentiment analysis: RolePlaying (RP) prompting and Chain-of-thought (CoT) prompting. Specifically, we also propose the RP-CoT prompting strategy which is a combination of RP prompting and CoT prompting. We conduct comparative experiments on three distinct domain datasets to evaluate the effectiveness of the proposed sentiment analysis strategies. The results demonstrate tha
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23646;&#24615;&#20998;&#35299;&#32858;&#21512;&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#24320;&#25918;&#35789;&#27719;&#35821;&#20041;&#20998;&#21106;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#35813;&#26694;&#26550;&#21463;&#21040;&#20154;&#31867;&#35748;&#30693;&#35299;&#37322;&#26032;&#27010;&#24565;&#30340;&#21551;&#21457;&#12290;</title><link>http://arxiv.org/abs/2309.00096</link><description>&lt;p&gt;
&#24320;&#25918;&#35789;&#27719;&#35821;&#20041;&#20998;&#21106;&#36890;&#36807;&#23646;&#24615;&#20998;&#35299;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
Open-Vocabulary Semantic Segmentation via Attribute Decomposition-Aggregation. (arXiv:2309.00096v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00096
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23646;&#24615;&#20998;&#35299;&#32858;&#21512;&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#24320;&#25918;&#35789;&#27719;&#35821;&#20041;&#20998;&#21106;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#35813;&#26694;&#26550;&#21463;&#21040;&#20154;&#31867;&#35748;&#30693;&#35299;&#37322;&#26032;&#27010;&#24565;&#30340;&#21551;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#35789;&#27719;&#35821;&#20041;&#20998;&#21106;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#22312;&#25512;&#29702;&#26102;&#23545;&#26032;&#30340;&#23545;&#35937;&#31867;&#21035;&#36827;&#34892;&#20998;&#21106;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#26469;&#22788;&#29702;&#36825;&#20010;&#20219;&#21153;&#65292;&#20294;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#23384;&#22312;&#19981;&#20999;&#23454;&#38469;&#30340;&#20551;&#35774;&#65292;&#21363;&#20302;&#36136;&#37327;&#30340;&#25991;&#26412;&#31867;&#21035;&#21517;&#31216;&#12290;&#20363;&#22914;&#65292;&#36825;&#31181;&#33539;&#24335;&#20551;&#35774;&#26032;&#30340;&#25991;&#26412;&#31867;&#21035;&#23558;&#34987;&#20934;&#30830;&#23436;&#25972;&#22320;&#25552;&#20379;&#65292;&#24182;&#19988;&#23384;&#22312;&#20110;&#39044;&#35757;&#32451;&#26399;&#38388;&#30340;&#35789;&#20856;&#20013;&#12290;&#28982;&#32780;&#65292;&#24403;&#36935;&#21040;&#31616;&#30701;&#25110;&#19981;&#23436;&#25972;&#30340;&#21517;&#31216;&#12289;&#22312;&#39044;&#35757;&#32451;&#30340;&#35789;&#20856;&#20013;&#19981;&#23384;&#22312;&#30340;&#26032;&#35789;&#20197;&#21450;&#38590;&#20197;&#25551;&#36848;&#30340;&#31867;&#21035;&#26102;&#65292;&#24322;&#24120;&#24773;&#20917;&#32463;&#24120;&#21457;&#29983;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#35299;-&#32858;&#21512;&#26694;&#26550;&#65292;&#21463;&#20154;&#31867;&#35748;&#30693;&#22312;&#29702;&#35299;&#26032;&#27010;&#24565;&#26041;&#38754;&#30340;&#21551;&#21457;&#12290;&#20855;&#20307;&#22320;&#65292;&#22312;&#20998;&#35299;&#38454;&#27573;&#65292;&#25105;&#20204;&#23558;&#31867;&#21035;&#21517;&#31216;&#20998;&#35299;&#20026;&#22810;&#26679;&#30340;&#23646;&#24615;&#25551;&#36848;&#65292;&#20197;&#20016;&#23500;&#35821;&#20041;&#19978;&#19979;&#25991;&#12290;&#35774;&#35745;&#20102;&#20004;&#31181;&#23646;&#24615;&#26500;&#24314;&#31574;&#30053;&#65306;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
Open-vocabulary semantic segmentation is a challenging task that requires segmenting novel object categories at inference time. Recent works explore vision-language pre-training to handle this task, but suffer from unrealistic assumptions in practical scenarios, i.e., low-quality textual category names. For example, this paradigm assumes that new textual categories will be accurately and completely provided, and exist in lexicons during pre-training. However, exceptions often happen when meet with ambiguity for brief or incomplete names, new words that are not present in the pre-trained lexicons, and difficult-to-describe categories for users. To address these issues, this work proposes a novel decomposition-aggregation framework, inspired by human cognition in understanding new concepts. Specifically, in the decomposition stage, we decouple class names into diverse attribute descriptions to enrich semantic contexts. Two attribute construction strategies are designed: using large langu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#22522;&#20110;&#25552;&#31034;&#30340;&#34892;&#21160;&#36716;&#25442;&#65292;&#35299;&#20915;&#20102;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#20219;&#21153;&#20013;&#20174;&#20223;&#30495;&#21040;&#30495;&#23454;&#30340;&#36801;&#31227;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.14284</link><description>&lt;p&gt;
LLM&#24378;&#21270;&#20102;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#30340;&#20174;&#20223;&#30495;&#21040;&#30495;&#23454;&#30340;&#36801;&#31227;
&lt;/p&gt;
&lt;p&gt;
LLM Powered Sim-to-real Transfer for Traffic Signal Control. (arXiv:2308.14284v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14284
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#22522;&#20110;&#25552;&#31034;&#30340;&#34892;&#21160;&#36716;&#25442;&#65292;&#35299;&#20915;&#20102;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#20219;&#21153;&#20013;&#20174;&#20223;&#30495;&#21040;&#30495;&#23454;&#30340;&#36801;&#31227;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24050;&#32463;&#26377;&#24456;&#22810;&#35299;&#20915;&#26041;&#26696;&#29992;&#20110;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#65288;TSC&#65289;&#20219;&#21153;&#65292;&#26088;&#22312;&#25552;&#20379;&#39640;&#25928;&#30340;&#20132;&#36890;&#21644;&#20943;&#36731;&#25317;&#22581;&#28010;&#36153;&#65292;&#20294;&#20173;&#28982;&#23384;&#22312;&#24615;&#33021;&#24046;&#36317;&#65292;&#24403;&#22312;&#20223;&#30495;&#22120;&#20013;&#35757;&#32451;&#30340;&#31574;&#30053;&#37096;&#32626;&#21040;&#29616;&#23454;&#19990;&#30028;&#26102;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#22522;&#20110;&#25552;&#31034;&#30340;&#25166;&#26681;&#34892;&#21160;&#36716;&#25442;&#65292;&#26469;&#29702;&#35299;&#21644;&#25551;&#36848;&#31995;&#32479;&#21160;&#24577;&#12290;&#36890;&#36807;&#25509;&#21463;&#22635;&#31354;&#25552;&#31034;&#27169;&#26495;&#65292;&#24182;&#26681;&#25454;&#21487;&#20197;&#35775;&#38382;&#30340;&#19978;&#19979;&#25991;&#22635;&#20889;&#31572;&#26696;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;LLM&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#24212;&#29992;&#20110;&#23545;&#31995;&#32479;&#21160;&#24577;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Numerous solutions are proposed for the Traffic Signal Control (TSC) tasks aiming to provide efficient transportation and mitigate congestion waste. In recent, promising results have been attained by Reinforcement Learning (RL) methods through trial and error in simulators, bringing confidence in solving cities' congestion headaches. However, there still exist performance gaps when simulator-trained policies are deployed to the real world. This issue is mainly introduced by the system dynamic difference between the training simulator and the real-world environments. The Large Language Models (LLMs) are trained on mass knowledge and proved to be equipped with astonishing inference abilities. In this work, we leverage LLMs to understand and profile the system dynamics by a prompt-based grounded action transformation. Accepting the cloze prompt template, and then filling in the answer based on accessible context, the pre-trained LLM's inference ability is exploited and applied to understa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#25351;&#20196;&#35843;&#25972;&#30340;&#26684;&#24335;&#19968;&#33268;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#32479;&#19968;&#25351;&#20196;&#35843;&#25972;&#65288;UIT&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#21160;&#26684;&#24335;&#36716;&#25442;&#26469;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#12290;&#35813;&#30740;&#31350;&#24378;&#35843;&#20102;&#26684;&#24335;&#19968;&#33268;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.15504</link><description>&lt;p&gt;
&#25506;&#32034;&#25351;&#20196;&#35843;&#25972;&#30340;&#26684;&#24335;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
Exploring Format Consistency for Instruction Tuning. (arXiv:2307.15504v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15504
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#25351;&#20196;&#35843;&#25972;&#30340;&#26684;&#24335;&#19968;&#33268;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#32479;&#19968;&#25351;&#20196;&#35843;&#25972;&#65288;UIT&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#21160;&#26684;&#24335;&#36716;&#25442;&#26469;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#12290;&#35813;&#30740;&#31350;&#24378;&#35843;&#20102;&#26684;&#24335;&#19968;&#33268;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#25972;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36981;&#24490;&#20154;&#31867;&#25351;&#20196;&#33021;&#21147;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#20013;&#25351;&#20196;&#30340;&#22810;&#26679;&#24615;&#21644;&#25968;&#37327;&#21487;&#20197;&#25345;&#32493;&#25552;&#21319;&#27867;&#21270;&#24615;&#33021;&#65292;&#20174;&#32780;&#20419;&#36827;&#20102;&#26368;&#36817;&#30340;&#19968;&#39033;&#21162;&#21147;&#65292;&#21363;&#25910;&#38598;&#21508;&#31181;&#25351;&#20196;&#24182;&#23558;&#29616;&#26377;&#30340;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#38598;&#25972;&#21512;&#21040;&#26356;&#22823;&#30340;&#38598;&#21512;&#20013;&#12290;&#28982;&#32780;&#65292;&#19981;&#21516;&#29992;&#25143;&#26377;&#20854;&#29420;&#29305;&#30340;&#34920;&#36798;&#25351;&#20196;&#30340;&#26041;&#24335;&#65292;&#19981;&#21516;&#25968;&#25454;&#38598;&#20043;&#38388;&#36890;&#24120;&#23384;&#22312;&#25351;&#20196;&#39118;&#26684;&#21644;&#26684;&#24335;&#30340;&#21464;&#21270;&#65292;&#21363;&#26684;&#24335;&#19981;&#19968;&#33268;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26684;&#24335;&#19981;&#19968;&#33268;&#24615;&#22914;&#20309;&#24433;&#21709;&#25351;&#20196;&#35843;&#25972;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;&#32479;&#19968;&#25351;&#20196;&#35843;&#25972;&#8221;&#65288;UIT&#65289;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35843;&#29992;OpenAI&#30340;API&#23454;&#29616;&#22312;&#19981;&#21516;&#30340;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#33258;&#21160;&#26684;&#24335;&#36716;&#25442;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;UIT&#25104;&#21151;&#25552;&#39640;&#20102;&#22312;&#26410;&#35265;&#25351;&#20196;&#19978;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#24378;&#35843;&#20102;&#26684;&#24335;&#19968;&#33268;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction tuning has emerged as a promising approach to enhancing large language models in following human instructions. It is shown that increasing the diversity and number of instructions in the training data can consistently enhance generalization performance, which facilitates a recent endeavor to collect various instructions and integrate existing instruction tuning datasets into larger collections. However, different users have their unique ways of expressing instructions, and there often exist variations across different datasets in the instruction styles and formats, i.e., format inconsistency. In this work, we study how format inconsistency may impact the performance of instruction tuning. We propose a framework called "Unified Instruction Tuning" (UIT), which calls OpenAI APIs for automatic format transfer among different instruction tuning datasets. We show that UIT successfully improves the generalization performance on unseen instructions, which highlights the importance
&lt;/p&gt;</description></item><item><title>RL$^3$&#26159;&#19968;&#31181;&#21407;&#21017;&#24615;&#28151;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#23398;&#21040;&#30340;&#20219;&#21153;&#29305;&#23450;&#21160;&#20316;&#20540;&#20316;&#20026;&#20803;&#24378;&#21270;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20837;&#65292;&#25552;&#39640;&#20102;&#20803;&#24378;&#21270;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.15909</link><description>&lt;p&gt;
RL$^3$:&#36890;&#36807;RL&#20869;&#37096;&#30340;RL$^2$&#25552;&#21319;&#20803;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
RL$^3$: Boosting Meta Reinforcement Learning via RL inside RL$^2$. (arXiv:2306.15909v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15909
&lt;/p&gt;
&lt;p&gt;
RL$^3$&#26159;&#19968;&#31181;&#21407;&#21017;&#24615;&#28151;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#23398;&#21040;&#30340;&#20219;&#21153;&#29305;&#23450;&#21160;&#20316;&#20540;&#20316;&#20026;&#20803;&#24378;&#21270;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20837;&#65292;&#25552;&#39640;&#20102;&#20803;&#24378;&#21270;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20803;&#24378;&#21270;&#23398;&#20064;&#65288;meta-RL&#65289;&#26041;&#27861;&#65292;&#22914;RL$^2$&#65292;&#24050;&#32463;&#25104;&#20026;&#23398;&#20064;&#38024;&#23545;&#32473;&#23450;&#20219;&#21153;&#20998;&#24067;&#30340;&#25968;&#25454;&#39640;&#25928;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#38271;&#26399;&#20219;&#21153;&#21644;&#36229;&#20986;&#20998;&#24067;&#20219;&#21153;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#22240;&#20026;&#23427;&#20204;&#20381;&#36182;&#20110;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#26469;&#22788;&#29702;&#32463;&#39564;&#24207;&#21015;&#65292;&#32780;&#19981;&#26159;&#23558;&#23427;&#20204;&#24635;&#32467;&#20026;&#19968;&#33324;&#30340;&#24378;&#21270;&#23398;&#20064;&#32452;&#20214;&#65292;&#20363;&#22914;&#20215;&#20540;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;&#26159;transformers&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#25104;&#26412;&#21464;&#24471;&#31105;&#27490;&#20043;&#21069;&#20063;&#23545;&#23427;&#20204;&#21487;&#20197;&#26377;&#25928;&#25512;&#29702;&#30340;&#21382;&#21490;&#38271;&#24230;&#26377;&#23454;&#38469;&#38480;&#21046;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#25968;&#25454;&#25928;&#29575;&#26041;&#38754;&#19981;&#36275;&#65292;&#22240;&#20026;&#23427;&#20204;&#27809;&#26377;&#21033;&#29992;&#39046;&#22495;&#30693;&#35782;&#65292;&#20294;&#38543;&#30528;&#26356;&#22810;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#65292;&#23427;&#20204;&#20250;&#25910;&#25947;&#21040;&#26368;&#20248;&#31574;&#30053;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RL$^3$&#65292;&#19968;&#31181;&#32452;&#21512;&#20102;&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#21644;&#20803;&#24378;&#21270;&#23398;&#20064;&#30340;&#21407;&#21017;&#24615;&#28151;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#36890;&#36807;&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#23398;&#20064;&#21040;&#30340;&#29305;&#23450;&#20219;&#21153;&#21160;&#20316;&#20540;&#20316;&#20026;&#20803;&#24378;&#21270;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#30340;&#19968;&#20010;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Meta reinforcement learning (meta-RL) methods such as RL$^2$ have emerged as promising approaches for learning data-efficient RL algorithms tailored to a given task distribution. However, these RL algorithms struggle with long-horizon tasks and out-of-distribution tasks since they rely on recurrent neural networks to process the sequence of experiences instead of summarizing them into general RL components such as value functions. Moreover, even transformers have a practical limit to the length of histories they can efficiently reason about before training and inference costs become prohibitive. In contrast, traditional RL algorithms are data-inefficient since they do not leverage domain knowledge, but they do converge to an optimal policy as more data becomes available. In this paper, we propose RL$^3$, a principled hybrid approach that combines traditional RL and meta-RL by incorporating task-specific action-values learned through traditional RL as an input to the meta-RL neural netw
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#22797;&#26434;&#24230;&#20026;$O(\log(n))$&#30340;&#20108;&#27425;&#35268;&#21010;&#20248;&#21270;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#20005;&#26684;&#30340;&#29702;&#35770;&#35777;&#26126;&#39564;&#35777;&#20102;&#35813;&#31639;&#27861;&#30340;&#21487;&#34892;&#24615;&#12290;&#36825;&#19968;&#37325;&#22823;&#31361;&#30772;&#20351;&#24471;&#25105;&#20204;&#20174;$O(\sqrt{n})$&#30340;&#20248;&#21270;&#31639;&#27861;&#36807;&#28193;&#21040;$O(\log(n))$&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#20854;&#22312;&#22823;&#25968;&#25454;&#21644;&#20154;&#24037;&#26234;&#33021;&#26102;&#20195;&#20855;&#26377;&#37325;&#35201;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2306.15079</link><description>&lt;p&gt;
&#20174;$O(\sqrt{n})$&#21040;$O(\log(n))$&#30340;&#20108;&#27425;&#35268;&#21010;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
From $O(\sqrt n)$ to $O(\log n)$ in Quadratic Programming. (arXiv:2306.15079v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15079
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#22797;&#26434;&#24230;&#20026;$O(\log(n))$&#30340;&#20108;&#27425;&#35268;&#21010;&#20248;&#21270;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#20005;&#26684;&#30340;&#29702;&#35770;&#35777;&#26126;&#39564;&#35777;&#20102;&#35813;&#31639;&#27861;&#30340;&#21487;&#34892;&#24615;&#12290;&#36825;&#19968;&#37325;&#22823;&#31361;&#30772;&#20351;&#24471;&#25105;&#20204;&#20174;$O(\sqrt{n})$&#30340;&#20248;&#21270;&#31639;&#27861;&#36807;&#28193;&#21040;$O(\log(n))$&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#20854;&#22312;&#22823;&#25968;&#25454;&#21644;&#20154;&#24037;&#26234;&#33021;&#26102;&#20195;&#20855;&#26377;&#37325;&#35201;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#24180;&#26469;&#65292;&#25968;&#20540;&#20248;&#21270;&#29702;&#35770;&#19968;&#30452;&#23384;&#22312;&#19968;&#20010;&#22256;&#25200;&#65292;&#21363;&#26159;&#21542;&#23384;&#22312;&#19968;&#20010;&#36845;&#20195;&#22797;&#26434;&#24230;&#20026;$O(\log(n))$&#30340;&#20248;&#21270;&#31639;&#27861;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#20840;&#26032;&#30340;&#20248;&#21270;&#31639;&#27861;&#21644;&#20005;&#26684;&#30340;&#29702;&#35770;&#35777;&#26126;&#26469;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#20197;&#26377;&#30028;&#30418;&#20108;&#27425;&#35268;&#21010;&#38382;&#39064;&#65288;Box-QP&#65289;&#20026;&#36215;&#28857;&#65292;&#35768;&#22810;&#23454;&#38469;&#20248;&#21270;&#38382;&#39064;&#21487;&#20197;&#36890;&#36807;&#23545;&#20598;&#29702;&#35770;&#36716;&#21270;&#20026;Box-QP&#38382;&#39064;&#12290;&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#20010;&#36845;&#20195;&#22797;&#26434;&#24230;&#20026;$O(\log(n))$&#30340;QP&#31639;&#27861;&#65292;&#23588;&#20854;&#26159;&#20854;&#34920;&#29616;&#31867;&#20284;&#20110;&#8220;&#30452;&#25509;&#8221;&#26041;&#27861;&#65306;&#25152;&#38656;&#36845;&#20195;&#27425;&#25968;&#26159;&#30830;&#23450;&#24615;&#30340;&#65292;&#31934;&#30830;&#20540;&#20026;$\left\lceil\log\left(\frac{3.125n}{\epsilon}\right)/\log(1.5625)\right\rceil$&#12290;&#36825;&#19968;&#37325;&#22823;&#31361;&#30772;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#20174;$O(\sqrt{n})$&#30340;&#20248;&#21270;&#31639;&#27861;&#36807;&#28193;&#21040;$O(\log(n))$&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#20854;&#20986;&#33394;&#30340;&#21487;&#25193;&#23637;&#24615;&#22312;&#24403;&#20170;&#30340;&#22823;&#25968;&#25454;&#21644;&#20154;&#24037;&#26234;&#33021;&#26102;&#20195;&#23588;&#20026;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
A "dark cloud" hangs over numerical optimization theory for decades, namely, whether an optimization algorithm $O(\log(n))$ iteration complexity exists. "Yes", this paper answers, with a new optimization algorithm and strict theory proof. It starts with box-constrained quadratic programming (Box-QP), and many practical optimization problems fall into Box-QP. Smooth quadratic programming (QP) and nonsmooth Lasso can be reformulated as Box-QP via duality theory. It is the first time to present an $O(\log(n))$ iteration complexity QP algorithm, in particular, which behaves like a "direct" method: the required number of iterations is deterministic with exact value $\left\lceil\log\left(\frac{3.125n}{\epsilon}\right)/\log(1.5625)\right\rceil$. This significant breakthrough enables us to transition from the $O(\sqrt{n})$ to the $O(\log(n))$ optimization algorithm, whose amazing scalability is particularly relevant in today's era of big data and artificial intelligence.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#36924;&#30495;&#26426;&#22120;&#20154;&#25805;&#20316;&#27169;&#25311;&#22120;&#30340;&#22522;&#20934;&#35780;&#20272;RM-PRT&#65292;&#24182;&#25552;&#20986;&#20102;&#36890;&#29992;&#30340;&#35780;&#20272;&#27969;&#31243;&#65292;&#26088;&#22312;&#36890;&#36807;&#22823;&#35268;&#27169;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#21644;&#22810;&#27169;&#24577;&#25552;&#31034;&#23545;&#26426;&#22120;&#20154;&#25805;&#20316;&#36827;&#34892;&#35814;&#32454;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2306.11335</link><description>&lt;p&gt;
RM-PRT: &#30495;&#23454;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#27169;&#25311;&#22120;&#21644;&#22522;&#20110;&#28176;&#36827;&#25512;&#29702;&#20219;&#21153;&#30340;&#22522;&#20934;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
RM-PRT: Realistic Robotic Manipulation Simulator and Benchmark with Progressive Reasoning Tasks. (arXiv:2306.11335v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11335
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#36924;&#30495;&#26426;&#22120;&#20154;&#25805;&#20316;&#27169;&#25311;&#22120;&#30340;&#22522;&#20934;&#35780;&#20272;RM-PRT&#65292;&#24182;&#25552;&#20986;&#20102;&#36890;&#29992;&#30340;&#35780;&#20272;&#27969;&#31243;&#65292;&#26088;&#22312;&#36890;&#36807;&#22823;&#35268;&#27169;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#21644;&#22810;&#27169;&#24577;&#25552;&#31034;&#23545;&#26426;&#22120;&#20154;&#25805;&#20316;&#36827;&#34892;&#35814;&#32454;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#39044;&#35757;&#32451;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#22914;ChatGPT&#21644;GPT-4&#30340;&#20986;&#29616;&#65292;&#26174;&#30528;&#25512;&#36827;&#20102;&#26426;&#22120;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#12290;&#36825;&#19968;&#31361;&#30772;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#36825;&#20123;&#24320;&#28304;LLM&#26080;&#32541;&#22320;&#38598;&#25104;&#21040;&#32479;&#19968;&#30340;&#26426;&#22120;&#20154;&#27169;&#25311;&#22120;&#29615;&#22659;&#20013;&#65292;&#20197;&#24110;&#21161;&#26426;&#22120;&#20154;&#20934;&#30830;&#29702;&#35299;&#21644;&#25191;&#34892;&#20154;&#31867;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#36924;&#30495;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#27169;&#25311;&#22120;&#65292;&#24182;&#22312;&#27492;&#22522;&#30784;&#19978;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;&#28176;&#36827;&#25512;&#29702;&#20219;&#21153;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#22522;&#20934;&#65288;RM-PRT&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;RM-PRT&#22522;&#20934;&#35780;&#20272;&#22522;&#20110;Unreal Engine 5&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#39640;&#20445;&#30495;&#25968;&#23383;&#21452;&#32990;&#32974;&#22330;&#26223;&#65292;&#20854;&#20013;&#21253;&#25324;782&#20010;&#31867;&#21035;&#65292;2023&#20010;&#29289;&#20307;&#65292;&#24182;&#20351;&#29992;ChatGPT&#29983;&#25104;&#20102;15,000&#20010;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#65292;&#20197;&#35814;&#32454;&#35780;&#20272;&#26426;&#22120;&#20154;&#25805;&#20316;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;RM-PRT&#22522;&#20934;&#35780;&#20272;&#27969;&#31243;&#65292;&#35813;&#27969;&#31243;&#25509;&#21463;&#21253;&#21547;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#30340;&#22810;&#27169;&#24577;&#25552;&#31034;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#33258;&#21160;&#36755;&#20986;&#34892;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, the advent of pre-trained large-scale language models (LLMs) like ChatGPT and GPT-4 have significantly advanced the machine's natural language understanding capabilities. This breakthrough has allowed us to seamlessly integrate these open-source LLMs into a unified robot simulator environment to help robots accurately understand and execute human natural language instructions. To this end, in this work, we introduce a realistic robotic manipulation simulator and build a Robotic Manipulation with Progressive Reasoning Tasks (RM-PRT) benchmark on this basis. Specifically, the RM-PRT benchmark builds a new high-fidelity digital twin scene based on Unreal Engine 5, which includes 782 categories, 2023 objects, and 15K natural language instructions generated by ChatGPT for a detailed evaluation of robot manipulation. We propose a general pipeline for the RM-PRT benchmark that takes as input multimodal prompts containing natural language instructions and automatically outputs action
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#35299;&#20915;&#38646;&#26679;&#26412;&#20154;&#24037;&#26234;&#33021;&#21327;&#21516;&#20013;&#21327;&#20316;&#19981;&#20860;&#23481;&#24615;&#30340;COLE&#26694;&#26550;&#65292;&#21033;&#29992;&#22270;&#24418;&#29702;&#35770;&#30340;&#35270;&#35282;&#65292;&#36890;&#36807;&#21046;&#23450;&#24320;&#25918;&#24335;&#30446;&#26631;&#65292;&#22312;&#28041;&#21450;&#19981;&#29087;&#24713;&#30340;&#20154;&#31867;&#24773;&#22659;&#20013;&#30830;&#20445;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#21644;&#38431;&#21451;&#20043;&#38388;&#30340;&#21327;&#35843;&#12290;</title><link>http://arxiv.org/abs/2306.03034</link><description>&lt;p&gt;
&#35299;&#20915;&#38646;&#26679;&#26412;&#20154;&#24037;&#26234;&#33021;&#21327;&#21516;&#20013;&#30340;&#21327;&#20316;&#19981;&#20860;&#23481;&#24615;
&lt;/p&gt;
&lt;p&gt;
Tackling Cooperative Incompatibility for Zero-Shot Human-AI Coordination. (arXiv:2306.03034v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03034
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#35299;&#20915;&#38646;&#26679;&#26412;&#20154;&#24037;&#26234;&#33021;&#21327;&#21516;&#20013;&#21327;&#20316;&#19981;&#20860;&#23481;&#24615;&#30340;COLE&#26694;&#26550;&#65292;&#21033;&#29992;&#22270;&#24418;&#29702;&#35770;&#30340;&#35270;&#35282;&#65292;&#36890;&#36807;&#21046;&#23450;&#24320;&#25918;&#24335;&#30446;&#26631;&#65292;&#22312;&#28041;&#21450;&#19981;&#29087;&#24713;&#30340;&#20154;&#31867;&#24773;&#22659;&#20013;&#30830;&#20445;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#21644;&#38431;&#21451;&#20043;&#38388;&#30340;&#21327;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28041;&#21450;&#19981;&#29087;&#24713;&#30340;&#20154;&#31867;&#30340;&#24773;&#22659;&#20013;&#65292;&#30830;&#20445;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#21644;&#38431;&#21451;&#65288;&#20154;&#31867;&#29609;&#23478;&#25110;&#32773;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#65289;&#20043;&#38388;&#30340;&#21327;&#35843;&#20173;&#28982;&#26159;&#38646;&#26679;&#26412;&#21327;&#21516;&#20013;&#30340;&#37325;&#22823;&#25361;&#25112;&#12290;&#24403;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#26080;&#27861;&#19982;&#26576;&#20123;&#20808;&#21069;&#26410;&#30693;&#30340;&#21512;&#20316;&#20249;&#20276;&#36827;&#34892;&#21516;&#27493;&#26102;&#65292;&#21327;&#20316;&#19981;&#20860;&#23481;&#24615;&#38382;&#39064;&#29305;&#21035;&#31361;&#20986;&#12290;&#20256;&#32479;&#31639;&#27861;&#36890;&#36807;&#20248;&#21270;&#22266;&#23450;&#30340;&#30446;&#26631;&#22312;&#20154;&#21475;&#20013;&#21512;&#20316;&#65292;&#20419;&#36827;&#31574;&#30053;&#21644;&#34892;&#20026;&#30340;&#22810;&#26679;&#24615;&#26469;&#19982;&#21512;&#20316;&#20249;&#20276;&#21512;&#20316;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25216;&#26415;&#21487;&#33021;&#23548;&#33268;&#23398;&#20064;&#25439;&#22833;&#65292;&#24182;&#19988;&#26080;&#27861;&#19982;&#20154;&#21475;&#20013;&#30340;&#29305;&#23450;&#31574;&#30053;&#21512;&#20316;&#65292;&#36825;&#31181;&#29616;&#35937;&#34987;&#31216;&#20026;&#23398;&#20064;&#20013;&#30340;&#21327;&#20316;&#19981;&#20860;&#23481;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#23398;&#20064;&#20013;&#30340;&#21327;&#20316;&#19981;&#20860;&#23481;&#24615;&#24182;&#26377;&#25928;&#22320;&#35299;&#20915;ZSC&#20013;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102; Cooperative Open-ended LEarning&#65288;COLE&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#22270;&#24418;&#29702;&#35770;&#30340;&#35270;&#35282;&#65292;&#23545;&#20004;&#20010;&#29609;&#23478;&#30340;&#21512;&#20316;&#28216;&#25103;&#21046;&#23450;&#24320;&#25918;&#24335;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Securing coordination between AI agent and teammates (human players or AI agents) in contexts involving unfamiliar humans continues to pose a significant challenge in Zero-Shot Coordination. The issue of cooperative incompatibility becomes particularly prominent when an AI agent is unsuccessful in synchronizing with certain previously unknown partners. Traditional algorithms have aimed to collaborate with partners by optimizing fixed objectives within a population, fostering diversity in strategies and behaviors. However, these techniques may lead to learning loss and an inability to cooperate with specific strategies within the population, a phenomenon named cooperative incompatibility in learning. In order to solve cooperative incompatibility in learning and effectively address the problem in the context of ZSC, we introduce the Cooperative Open-ended LEarning (COLE) framework, which formulates open-ended objectives in cooperative games with two players using perspectives of graph th
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#19977;&#27493;&#36328;&#35821;&#35328;&#36801;&#31227;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#29616;&#26377;&#26694;&#26550;&#20013;&#22686;&#21152;&#19968;&#27493;&#35821;&#20041;&#30693;&#35782;&#33976;&#39311;&#65292;&#35813;&#26041;&#27861;&#26377;&#25928;&#22320;&#22686;&#24378;&#20102;&#33258;&#21160;&#35821;&#38899;&#32763;&#35793;&#20013;&#20174;&#39640;&#36164;&#28304;&#35821;&#35328;&#21040;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#36328;&#35821;&#35328;&#36801;&#31227;&#33021;&#21147;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#32763;&#35793;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#65292;&#24182;&#20943;&#23569;&#20102;&#36328;&#35821;&#35328;&#36801;&#31227;&#38388;&#38553;(TRFGap)&#12290;</title><link>http://arxiv.org/abs/2306.00789</link><description>&lt;p&gt;
&#20302;&#36164;&#28304;&#35821;&#38899;&#32763;&#35793;&#30340;&#36328;&#35821;&#35328;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Cross-Lingual Transfer Learning for Low-Resource Speech Translation. (arXiv:2306.00789v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00789
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#19977;&#27493;&#36328;&#35821;&#35328;&#36801;&#31227;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#29616;&#26377;&#26694;&#26550;&#20013;&#22686;&#21152;&#19968;&#27493;&#35821;&#20041;&#30693;&#35782;&#33976;&#39311;&#65292;&#35813;&#26041;&#27861;&#26377;&#25928;&#22320;&#22686;&#24378;&#20102;&#33258;&#21160;&#35821;&#38899;&#32763;&#35793;&#20013;&#20174;&#39640;&#36164;&#28304;&#35821;&#35328;&#21040;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#36328;&#35821;&#35328;&#36801;&#31227;&#33021;&#21147;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#32763;&#35793;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#65292;&#24182;&#20943;&#23569;&#20102;&#36328;&#35821;&#35328;&#36801;&#31227;&#38388;&#38553;(TRFGap)&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19977;&#27493;&#36328;&#35821;&#35328;&#36801;&#31227;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22686;&#24378;&#33258;&#21160;&#35821;&#38899;&#32763;&#35793;&#20013;&#20174;&#39640;&#36164;&#28304;&#35821;&#35328;&#21040;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#36328;&#35821;&#35328;&#36801;&#31227;&#33021;&#21147;&#12290;&#35813;&#26041;&#27861;&#23558;&#35821;&#20041;&#30693;&#35782;&#33976;&#39311;&#27493;&#39588;&#38598;&#25104;&#21040;&#29616;&#26377;&#30340;&#20004;&#27493;&#36328;&#35821;&#35328;&#36801;&#31227;&#23398;&#20064;&#26694;&#26550;XLS-R&#20013;&#12290;&#36825;&#19968;&#39069;&#22806;&#30340;&#27493;&#39588;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#26080;&#26631;&#31614;&#35821;&#38899;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#26469;&#23545;&#22810;&#35821;&#35328;&#35821;&#38899;&#32534;&#30721;&#22120;&#36827;&#34892;&#39044;&#35757;&#32451;&#20197;&#32534;&#30721;&#35821;&#20041;&#30693;&#35782;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#19977;&#27493;&#36328;&#35821;&#35328;&#36801;&#31227;&#23398;&#20064;&#26694;&#26550;&#35299;&#20915;&#20102;XLS-R&#26694;&#26550;&#20013;&#39640;&#36164;&#28304;&#35821;&#35328;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#20043;&#38388;&#23384;&#22312;&#30340;&#22823;&#30340;&#36328;&#35821;&#35328;&#36801;&#31227;&#24046;&#36317;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;CoVoST-2&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#21644;&#27604;&#36739;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#25552;&#35758;&#65292;&#32467;&#26524;&#26174;&#31034;&#22312;&#32763;&#35793;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#65292;&#24182;&#19988;&#36328;&#35821;&#35328;&#36801;&#31227;&#38388;&#38553;(TRFGap)&#26377;&#26126;&#26174;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper presents a novel three-step transfer learning framework for enhancing cross-lingual transfer from high- to low-resource languages in the downstream application of Automatic Speech Translation. The approach integrates a semantic knowledge-distillation step into the existing two-step cross-lingual transfer learning framework XLS-R. This extra step aims to encode semantic knowledge in the multilingual speech encoder pre-trained via Self-Supervised Learning using unlabeled speech. Our proposed three-step cross-lingual transfer learning framework addresses the large cross-lingual transfer gap (TRFGap) observed in the XLS-R framework between high-resource and low-resource languages. We validate our proposal through extensive experiments and comparisons on the CoVoST-2 benchmark, showing significant improvements in translation performance, especially for low-resource languages, and a notable reduction in the TRFGap.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;YOLOv8&#31639;&#27861;&#36827;&#34892;&#24515;&#24459;&#22833;&#24120;&#26816;&#27979;&#30340;&#26032;&#24212;&#29992;&#31243;&#24207;&#65292;&#20854;&#27169;&#22411;&#33021;&#22815;&#23454;&#29616;&#25345;&#32493;&#30417;&#27979;&#65292;&#24182;&#20197;&#39640;&#20934;&#30830;&#24615;&#36827;&#34892;&#23454;&#26102;&#24515;&#24459;&#22833;&#24120;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2305.16727</link><description>&lt;p&gt;
YOLOv8&#23454;&#26102;&#24515;&#24459;&#22833;&#24120;&#26816;&#27979;&#30340;&#26032;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
A novel application for real-time arrhythmia detection using YOLOv8. (arXiv:2305.16727v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16727
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;YOLOv8&#31639;&#27861;&#36827;&#34892;&#24515;&#24459;&#22833;&#24120;&#26816;&#27979;&#30340;&#26032;&#24212;&#29992;&#31243;&#24207;&#65292;&#20854;&#27169;&#22411;&#33021;&#22815;&#23454;&#29616;&#25345;&#32493;&#30417;&#27979;&#65292;&#24182;&#20197;&#39640;&#20934;&#30830;&#24615;&#36827;&#34892;&#23454;&#26102;&#24515;&#24459;&#22833;&#24120;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#38477;&#20302;&#36828;&#31243;&#24515;&#34880;&#31649;&#20581;&#24247;&#30417;&#25252;&#30340;&#21307;&#30103;&#36153;&#29992;&#38656;&#27714;&#36234;&#26469;&#36234;&#39640;&#12290;&#26816;&#27979;&#21644;&#20998;&#31867;&#24515;&#33039;&#24515;&#24459;&#22833;&#24120;&#23545;&#20110;&#35786;&#26029;&#24515;&#33039;&#24322;&#24120;&#24739;&#32773;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24212;&#29992;&#31243;&#24207;&#65292;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;You-Only-Look-Once&#65288;YOLO&#65289;v8&#31639;&#27861;&#23545;&#21333;&#23548;&#32852;&#24515;&#30005;&#22270;&#20449;&#21495;&#36827;&#34892;&#20998;&#31867;&#65292;&#36827;&#34892;&#24515;&#24459;&#22833;&#24120;&#26816;&#27979;&#12290;&#36890;&#36807;&#23545;MIT-BIH&#25968;&#25454;&#38598;&#36827;&#34892;&#24494;&#35843;&#21518;&#65292;&#24314;&#31435;&#36215;&#19968;&#20010;&#23450;&#21046;&#30340;YOLOv8&#27169;&#22411;&#65292;&#33021;&#22815;&#23454;&#26102;&#26816;&#27979;&#24515;&#24459;&#22833;&#24120;&#65292;&#20197;&#23454;&#29616;&#25345;&#32493;&#30417;&#27979;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;NVIDIA Tesla V100&#19978;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#20197;0.002&#31186;&#30340;&#26816;&#27979;&#26102;&#38388;&#21644;0.961&#30340;mAP@50&#26816;&#27979;&#24515;&#36339;&#12290;&#30740;&#31350;&#35777;&#26126;&#20102;&#23454;&#26102;&#24515;&#24459;&#22833;&#24120;&#26816;&#27979;&#30340;&#28508;&#21147;&#65292;&#27169;&#22411;&#36755;&#20986;&#21487;&#20197;&#34987;&#35270;&#35273;&#35299;&#37322;&#65292;&#36866;&#29992;&#20110;&#23478;&#24237;&#29992;&#25143;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#21487;&#20197;&#24310;&#20280;&#21040;&#24320;&#21457;&#23454;&#26102;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#24212;&#29992;&#20110;&#24515;&#34880;&#31649;&#20581;&#24247;&#30417;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, there has been an increasing need to reduce healthcare costs in remote monitoring of cardiovascular health. Detecting and classifying cardiac arrhythmia is critical to diagnosing patients with cardiac abnormalities. This paper shows that complex systems such as electrocardiograms (ECG) can be applicable for at-home monitoring. This paper proposes a novel application for arrhythmia detection using the state-of-the-art You-Only-Look-Once (YOLO)v8 algorithm to classify single-lead ECG signals. A custom YOLOv8 model was fine-tuned on the MIT-BIH dataset to detect arrhythmia in real-time to allow continuous monitoring. Results show that our model can detect heartbeats with a mAP@50 of 0.961 with a detection time of 0.002s on an NVIDIA Tesla V100. Our study demonstrated the potential of real-time arrhythmia detection, where the model output can be visually interpreted for at-home users. Furthermore, this study could be extended into a real-time XAI model, deployed in the hea
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AlpacaFarm&#30340;&#20302;&#25104;&#26412;&#27169;&#25311;&#22120;&#65292;&#35813;&#27169;&#25311;&#22120;&#20026;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#30340;&#30740;&#31350;&#21644;&#24320;&#21457;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#35774;&#35745;LLM&#25552;&#31034;&#26469;&#27169;&#25311;&#20154;&#31867;&#21453;&#39304;&#65292;&#25552;&#20986;&#33258;&#21160;&#35780;&#20272;&#24182;&#25552;&#20379;&#21442;&#32771;&#23454;&#29616;&#65292;&#20811;&#26381;&#20102;&#25968;&#25454;&#25910;&#38598;&#30340;&#39640;&#26114;&#25104;&#26412;&#12289;&#32570;&#20047;&#21487;&#20449;&#30340;&#35780;&#20272;&#21644;&#32570;&#20047;&#21442;&#32771;&#26041;&#27861;&#23454;&#29616;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.14387</link><description>&lt;p&gt;
AlpacaFarm: &#19968;&#31181;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#30340;&#26041;&#27861;&#27169;&#25311;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback. (arXiv:2305.14387v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14387
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AlpacaFarm&#30340;&#20302;&#25104;&#26412;&#27169;&#25311;&#22120;&#65292;&#35813;&#27169;&#25311;&#22120;&#20026;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#30340;&#30740;&#31350;&#21644;&#24320;&#21457;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#35774;&#35745;LLM&#25552;&#31034;&#26469;&#27169;&#25311;&#20154;&#31867;&#21453;&#39304;&#65292;&#25552;&#20986;&#33258;&#21160;&#35780;&#20272;&#24182;&#25552;&#20379;&#21442;&#32771;&#23454;&#29616;&#65292;&#20811;&#26381;&#20102;&#25968;&#25454;&#25910;&#38598;&#30340;&#39640;&#26114;&#25104;&#26412;&#12289;&#32570;&#20047;&#21487;&#20449;&#30340;&#35780;&#20272;&#21644;&#32570;&#20047;&#21442;&#32771;&#26041;&#27861;&#23454;&#29616;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#22240;&#20854;&#33391;&#22909;&#30340;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#32780;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;&#24320;&#21457;&#36825;&#20123;LLMs&#38656;&#35201;&#20351;&#29992;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#35757;&#32451;&#30340;&#22797;&#26434;&#19988;&#23578;&#19981;&#26126;&#30830;&#30340;&#24037;&#20316;&#27969;&#31243;&#12290;&#23558;&#27492;&#25351;&#20196;&#36319;&#38543;&#36807;&#31243;&#22797;&#21046;&#21644;&#29702;&#35299;&#38754;&#20020;&#19977;&#22823;&#25361;&#25112;&#65306; &#25968;&#25454;&#25910;&#38598;&#30340;&#39640;&#26114;&#25104;&#26412;&#65292;&#32570;&#20047;&#21487;&#20449;&#30340;&#35780;&#20272;&#21644;&#32570;&#20047;&#21442;&#32771;&#26041;&#27861;&#23454;&#29616;&#12290;&#25105;&#20204;&#36890;&#36807;AlpacaFarm&#35299;&#20915;&#20102;&#36825;&#20123;&#25361;&#25112;&#65292;&#36825;&#26159;&#19968;&#20010;&#20302;&#25104;&#26412;&#30340;&#27169;&#25311;&#22120;&#65292;&#21487;&#29992;&#20110;&#20174;&#21453;&#39304;&#20013;&#23398;&#20064;&#30340;&#30740;&#31350;&#21644;&#24320;&#21457;&#12290;&#31532;&#19968;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;LLM&#25552;&#31034;&#26469;&#27169;&#25311;&#20154;&#31867;&#21453;&#39304;&#65292;&#20854;&#25104;&#26412;&#27604;&#20247;&#21253;&#24037;&#20316;&#32773;&#20415;&#23452;45&#20493;&#65292;&#24182;&#19988;&#19982;&#20154;&#31867;&#21453;&#39304;&#20855;&#26377;&#39640;&#24230;&#19968;&#33268;&#24615;&#12290;&#31532;&#20108;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#19982;&#30495;&#23454;&#19990;&#30028;&#20132;&#20114;&#20013;&#33719;&#24471;&#30340;&#20154;&#31867;&#25351;&#20196;&#36827;&#34892;&#39564;&#35777;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#20026;&#20960;&#31181;&#20174;&#37197;&#23545;&#21453;&#39304;&#20013;&#23398;&#20064;&#30340;&#26041;&#27861;&#65288;PPO&#65292;best-of-n&#65292;expert iteration&#31561;&#65289;&#25552;&#20379;&#20102;&#21442;&#32771;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) such as ChatGPT have seen widespread adoption due to their ability to follow user instructions well. Developing these LLMs involves a complex yet poorly understood workflow requiring training with human feedback. Replicating and understanding this instruction-following process faces three major challenges: the high cost of data collection, the lack of trustworthy evaluation, and the absence of reference method implementations. We address these challenges with AlpacaFarm, a simulator that enables research and development for learning from feedback at a low cost. First, we design LLM prompts to simulate human feedback that are 45x cheaper than crowdworkers and display high agreement with humans. Second, we propose an automatic evaluation and validate it against human instructions obtained on real-world interactions. Third, we contribute reference implementations for several methods (PPO, best-of-n, expert iteration, and more) that learn from pairwise feedback
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#20551;&#35774;&#30340;&#35770;&#35777;&#65288;ABA&#65289;&#30340;&#19968;&#20010;&#38480;&#21046;&#65292;&#21363;&#21482;&#33021;&#20551;&#35774;&#32780;&#19981;&#33021;&#25512;&#23548;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#26497;&#35770;&#35777;&#26694;&#26550;&#65288;BAFs&#65289;&#65292;&#21487;&#20197;&#23454;&#20363;&#21270;&#19968;&#33324;&#30340;ABA&#26694;&#26550;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2305.12453</link><description>&lt;p&gt;
&#38750;&#24179;&#22374;ABA&#26159;&#21452;&#26497;&#35770;&#35777;&#30340;&#23454;&#20363;
&lt;/p&gt;
&lt;p&gt;
Non-flat ABA is an Instance of Bipolar Argumentation. (arXiv:2305.12453v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12453
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#20551;&#35774;&#30340;&#35770;&#35777;&#65288;ABA&#65289;&#30340;&#19968;&#20010;&#38480;&#21046;&#65292;&#21363;&#21482;&#33021;&#20551;&#35774;&#32780;&#19981;&#33021;&#25512;&#23548;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#26497;&#35770;&#35777;&#26694;&#26550;&#65288;BAFs&#65289;&#65292;&#21487;&#20197;&#23454;&#20363;&#21270;&#19968;&#33324;&#30340;ABA&#26694;&#26550;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20551;&#35774;&#30340;&#35770;&#35777;&#65288;ABA&#65289;&#26159;&#19968;&#20010;&#24191;&#20026;&#20154;&#30693;&#30340;&#32467;&#26500;&#21270;&#35770;&#35777;&#24418;&#24335;&#65292;&#20854;&#20013;&#35770;&#35777;&#21644;&#23427;&#20204;&#20043;&#38388;&#30340;&#25915;&#20987;&#26159;&#22522;&#20110;&#35268;&#21017;&#12289;&#21487;&#24223;&#24323;&#30340;&#20551;&#35774;&#21450;&#20854;&#30456;&#21453;&#30340;&#35266;&#28857;&#12290;&#23545;ABA&#26694;&#26550;&#65288;ABAFs&#65289;&#26045;&#21152;&#30340;&#19968;&#20010;&#24120;&#35265;&#38480;&#21046;&#26159;&#23427;&#20204;&#26159;&#24179;&#22374;&#30340;&#65292;&#21363;&#27599;&#20010;&#21487;&#24223;&#24323;&#30340;&#20551;&#35774;&#21482;&#33021;&#34987;&#20551;&#35774;&#65292;&#32780;&#19981;&#33021;&#34987;&#25512;&#23548;&#20986;&#26469;&#12290;&#23613;&#31649;&#24050;&#30693;&#24179;&#22374;&#30340;ABAFs&#21487;&#20197;&#34987;&#36716;&#21270;&#20026;Dung&#25552;&#20986;&#30340;&#25277;&#35937;&#35770;&#35777;&#26694;&#26550;&#65288;AFs&#65289;&#65292;&#20294;&#19981;&#23384;&#22312;&#20174;&#19968;&#33324;&#30340;&#12289;&#21487;&#33021;&#38750;&#24179;&#22374;&#30340;ABAFs&#21040;&#20219;&#20309;&#31867;&#22411;&#30340;&#25277;&#35937;&#35770;&#35777;&#24418;&#24335;&#30340;&#36716;&#25442;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22635;&#34917;&#20102;&#36825;&#20010;&#31354;&#30333;&#65292;&#24182;&#23637;&#31034;&#20102;&#21452;&#26497;AFs&#65288;BAFs&#65289;&#21487;&#20197;&#23454;&#20363;&#21270;&#19968;&#33324;&#30340;ABAFs&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#21512;&#36866;&#30340;&#12289;&#26032;&#39062;&#30340;BAF&#35821;&#20041;&#65292;&#20511;&#37492;&#20102;&#25512;&#23548;&#25903;&#25345;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#25105;&#20204;&#30340;BAFs&#30340;&#22522;&#26412;&#23646;&#24615;&#65292;&#21253;&#25324;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#24182;&#22312;&#20960;&#31181;&#35821;&#20041;&#19979;&#35777;&#26126;&#20102;&#19982;ABAFs&#30340;&#26399;&#26395;&#20851;&#31995;&#12290;&#26368;&#21518;&#65292;&#20026;&#20102;&#25903;&#25345;&#35745;&#31639;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#36716;&#25442;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Assumption-based Argumentation (ABA) is a well-known structured argumentation formalism, whereby arguments and attacks between them are drawn from rules, defeasible assumptions and their contraries. A common restriction imposed on ABA frameworks (ABAFs) is that they are flat, i.e., each of the defeasible assumptions can only be assumed, but not derived. While it is known that flat ABAFs can be translated into abstract argumentation frameworks (AFs) as proposed by Dung, no translation exists from general, possibly non-flat ABAFs into any kind of abstract argumentation formalism. In this paper, we close this gap and show that bipolar AFs (BAFs) can instantiate general ABAFs. To this end we develop suitable, novel BAF semantics which borrow from the notion of deductive support. We investigate basic properties of our BAFs, including computational complexity, and prove the desired relation to ABAFs under several semantics. Finally, in order to support computation and explainability, we prop
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#33258;&#28982;&#35821;&#35328;&#22270;&#24418;(NLGraph)&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#22522;&#20110;&#22270;&#24418;&#38382;&#39064;&#35299;&#20915;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;LLM&#22312;&#25991;&#26412;&#25551;&#36848;&#30340;&#22270;&#24418;&#32467;&#26500;&#21644;&#22270;&#24418;&#35299;&#20915;&#26041;&#26696;&#26041;&#38754;&#30340;&#22788;&#29702;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LLM(GPT-3/4)&#20855;&#26377;&#30456;&#24212;&#30340;&#22270;&#24418;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.10037</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#29992;&#33258;&#28982;&#35821;&#35328;&#35299;&#20915;&#22270;&#38382;&#39064;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Language Models Solve Graph Problems in Natural Language?. (arXiv:2305.10037v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10037
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#33258;&#28982;&#35821;&#35328;&#22270;&#24418;(NLGraph)&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#22522;&#20110;&#22270;&#24418;&#38382;&#39064;&#35299;&#20915;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;LLM&#22312;&#25991;&#26412;&#25551;&#36848;&#30340;&#22270;&#24418;&#32467;&#26500;&#21644;&#22270;&#24418;&#35299;&#20915;&#26041;&#26696;&#26041;&#38754;&#30340;&#22788;&#29702;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LLM(GPT-3/4)&#20855;&#26377;&#30456;&#24212;&#30340;&#22270;&#24418;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36234;&#26469;&#36234;&#22810;&#22320;&#24212;&#29992;&#20110;&#19968;&#20123;&#20855;&#26377;&#38544;&#24335;&#22270;&#24418;&#32467;&#26500;&#30340;&#20219;&#21153;&#65292;&#20363;&#22914;&#26426;&#22120;&#20154;&#35268;&#21010;&#12289;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;&#25110;&#30693;&#35782;&#25506;&#32034;&#12289;&#32467;&#26500;&#21270;&#24120;&#35782;&#25512;&#29702;&#31561;&#31561;&#12290;&#34429;&#28982;LLM&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#24050;&#32463;&#21462;&#24471;&#20102;&#19968;&#23450;&#30340;&#36827;&#23637;&#65292;&#20294;&#26159;LLM&#26159;&#21542;&#33021;&#22815;&#26174;&#24335;&#22788;&#29702;&#22270;&#24418;&#30340;&#25991;&#26412;&#25551;&#36848;&#65292;&#23558;&#23427;&#20204;&#26144;&#23556;&#21040;&#22522;&#20110;&#27010;&#24565;&#30340;&#31354;&#38388;&#20013;&#65292;&#24182;&#25191;&#34892;&#32467;&#26500;&#21270;&#25805;&#20316;&#20173;&#28982;&#23578;&#26410;&#24471;&#21040;&#36275;&#22815;&#30340;&#30740;&#31350;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#28982;&#35821;&#35328;&#22270;&#24418;(NLGraph)&#65292;&#23427;&#26159;&#19968;&#20010;&#35774;&#35745;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#30340;&#22522;&#20110;&#22270;&#24418;&#38382;&#39064;&#35299;&#20915;&#20840;&#38754;&#27979;&#35797;&#12290;NLGraph&#21253;&#21547;29,370&#20010;&#38382;&#39064;&#65292;&#28085;&#30422;&#20102;&#20843;&#20010;&#22270;&#24418;&#25512;&#29702;&#20219;&#21153;&#65292;&#20174;&#31616;&#21333;&#30340;&#36830;&#25509;&#21644;&#26368;&#30701;&#36335;&#24452;&#21040;&#22797;&#26434;&#30340;&#26368;&#22823;&#27969;&#21644;&#27169;&#25311;&#22270;&#31070;&#32463;&#32593;&#32476;&#31561;&#20219;&#21153;&#19981;&#31561;&#12290;&#25105;&#20204;&#22312;NLGraph&#22522;&#20934;&#27979;&#35797;&#19978;&#35780;&#20272;&#20102;LLM(GPT-3/4)&#65292;&#24182;&#21457;&#29616;1)&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#30456;&#24212;&#30340;&#22270;&#24418;&#25512;&#29702;&#33021;&#21147;&#65307;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are increasingly adopted for a variety of tasks with implicit graphical structures, such as planning in robotics, multi-hop question answering or knowledge probing, structured commonsense reasoning, and more. While LLMs have advanced the state-of-the-art on these tasks with structure implications, whether LLMs could explicitly process textual descriptions of graphs and structures, map them to grounded conceptual spaces, and perform structured operations remains underexplored. To this end, we propose NLGraph (Natural Language Graph), a comprehensive benchmark of graph-based problem solving designed in natural language. NLGraph contains 29,370 problems, covering eight graph reasoning tasks with varying complexity from simple tasks such as connectivity and shortest path up to complex problems such as maximum flow and simulating graph neural networks. We evaluate LLMs (GPT-3/4) with various prompting approaches on the NLGraph benchmark and find that 1) language
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#38382;&#39064;&#35299;&#26512;&#21644;&#25191;&#34892;&#26694;&#26550;&#65292;&#22312;&#25991;&#23383;&#38382;&#31572;&#31995;&#32479;&#20013;&#23454;&#29616;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#65292;&#36890;&#36807;&#35299;&#26512;&#38382;&#39064;&#20026;H&#34920;&#36798;&#24335;&#24182;&#35774;&#35745;&#28151;&#21512;&#25191;&#34892;&#22120;&#23454;&#29616;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#29575;&#21644;&#25928;&#29575;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.07789</link><description>&lt;p&gt;
&#28151;&#21512;&#38382;&#39064;&#35299;&#26512;&#19982;&#25191;&#34892;&#31572;&#26696;&#22797;&#26434;&#38382;&#39064;&#30340;&#25991;&#23383;&#38382;&#31572;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Answering Complex Questions over Text by Hybrid Question Parsing and Execution. (arXiv:2305.07789v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07789
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#38382;&#39064;&#35299;&#26512;&#21644;&#25191;&#34892;&#26694;&#26550;&#65292;&#22312;&#25991;&#23383;&#38382;&#31572;&#31995;&#32479;&#20013;&#23454;&#29616;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#65292;&#36890;&#36807;&#35299;&#26512;&#38382;&#39064;&#20026;H&#34920;&#36798;&#24335;&#24182;&#35774;&#35745;&#28151;&#21512;&#25191;&#34892;&#22120;&#23454;&#29616;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#29575;&#21644;&#25928;&#29575;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#38382;&#31572;&#31995;&#32479;&#30340;&#20027;&#23548;&#27169;&#24335;&#26159;&#22522;&#20110;&#31471;&#21040;&#31471;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#20854;&#22312;&#22238;&#31572;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#31361;&#20986;&#65292;&#20294;&#22312;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#19981;&#36275;&#12290;&#36825;&#19982;&#22522;&#20110;&#35821;&#20041;&#35299;&#26512;&#30340;&#26041;&#27861;&#22312;&#32467;&#26500;&#21270;&#25968;&#25454;&#28304;&#65288;&#22914;&#20851;&#31995;&#25968;&#25454;&#24211;&#12289;&#30693;&#35782;&#22270;&#35889;&#65289;&#19978;&#24191;&#27867;&#36866;&#24212;&#24418;&#25104;&#23545;&#27604;&#65292;&#21518;&#32773;&#23558;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#36716;&#25442;&#20026;&#36923;&#36753;&#24418;&#24335;&#65292;&#24182;&#21033;&#29992;&#26597;&#35810;&#24341;&#25806;&#36827;&#34892;&#25191;&#34892;&#12290;&#20026;&#20102;&#32467;&#21512;&#31070;&#32463;&#21644;&#31526;&#21495;&#26041;&#27861;&#30340;&#20248;&#21183;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#25991;&#26412;&#38382;&#31572;&#31995;&#32479;&#20013;&#36827;&#34892;&#35299;&#26512;&#21644;&#25191;&#34892;&#38382;&#39064;&#30340;&#26694;&#26550;&#12290;&#23427;&#21253;&#25324;&#20004;&#20010;&#20013;&#24515;&#25903;&#26609;&#65306;&#65288;1&#65289;&#25105;&#20204;&#23558;&#21508;&#31181;&#22797;&#26434;&#38382;&#39064;&#35299;&#26512;&#25104;&#20013;&#38388;&#34920;&#31034;&#65292;&#31216;&#20026;H&#34920;&#36798;&#24335;&#65292;&#23427;&#30001;&#31616;&#21333;&#38382;&#39064;&#32452;&#25104;&#21407;&#35821;&#21644;&#34920;&#31034;&#23427;&#20204;&#20043;&#38388;&#20851;&#31995;&#30340;&#31526;&#21495;&#25805;&#20316;&#32452;&#25104;&#65307;&#65288;2&#65289;&#20026;&#20102;&#25191;&#34892;&#20135;&#29983;&#30340;H&#34920;&#36798;&#24335;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#28151;&#21512;&#25191;&#34892;&#22120;&#65292;&#23427;&#38598;&#25104;&#20102;&#30830;&#23450;&#35268;&#21017;&#26469;&#32763;&#35793;&#31526;&#21495;&#25805;&#20316;&#65292;&#19982;&#22788;&#29702;&#21407;&#22987;&#38382;&#39064;&#30340;&#25554;&#20837;&#31070;&#32463;&#27169;&#22359;&#12290;&#25105;&#20204;&#22312;&#21253;&#21547;&#22797;&#26434;&#38382;&#39064;&#30340;&#22823;&#35268;&#27169;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#25351;&#26631;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
The dominant paradigm of textual question answering systems is based on end-to-end neural networks, which excels at answering natural language questions but falls short on complex ones. This stands in contrast to the broad adaptation of semantic parsing approaches over structured data sources (e.g., relational database, knowledge graphs), that convert natural language questions to logical forms and execute them with query engines. Towards combining the strengths of neural and symbolic methods, we propose a framework of question parsing and execution on textual QA. It comprises two central pillars: (1) We parse the question of varying complexity into an intermediate representation, named H-expression, which is composed of simple questions as the primitives and symbolic operations representing the relationships among them; (2) To execute the resulting H-expressions, we design a hybrid executor, which integrates the deterministic rules to translate the symbolic operations with a drop-in n
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37319;&#29992;&#22522;&#20110;&#25968;&#25454;&#20869;&#26680;&#30340;&#26041;&#27861;&#27604;&#36739;&#22522;&#30784;&#27169;&#22411;&#65292;&#19981;&#21463;&#24230;&#37327;&#25351;&#26631;&#30340;&#32422;&#26463;&#65292;&#36890;&#36807;&#23884;&#20837;&#31354;&#38388;&#20960;&#20309;&#23454;&#29616;&#28857;&#23545;&#28857;&#21644;&#22810;&#27169;&#22411;&#27604;&#36739;&#65292;&#24182;&#25104;&#21151;&#35825;&#23548;&#20102;&#19968;&#32452;&#19982;&#19979;&#28216;&#25351;&#26631;&#24378;&#30456;&#20851;&#30340;&#27169;&#22411;&#36317;&#31163;&#20989;&#25968;&#27969;&#24418;&#12290;</title><link>http://arxiv.org/abs/2305.05126</link><description>&lt;p&gt;
&#20351;&#29992;&#25968;&#25454;&#20869;&#26680;&#27604;&#36739;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Comparing Foundation Models using Data Kernels. (arXiv:2305.05126v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05126
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37319;&#29992;&#22522;&#20110;&#25968;&#25454;&#20869;&#26680;&#30340;&#26041;&#27861;&#27604;&#36739;&#22522;&#30784;&#27169;&#22411;&#65292;&#19981;&#21463;&#24230;&#37327;&#25351;&#26631;&#30340;&#32422;&#26463;&#65292;&#36890;&#36807;&#23884;&#20837;&#31354;&#38388;&#20960;&#20309;&#23454;&#29616;&#28857;&#23545;&#28857;&#21644;&#22810;&#27169;&#22411;&#27604;&#36739;&#65292;&#24182;&#25104;&#21151;&#35825;&#23548;&#20102;&#19968;&#32452;&#19982;&#19979;&#28216;&#25351;&#26631;&#24378;&#30456;&#20851;&#30340;&#27169;&#22411;&#36317;&#31163;&#20989;&#25968;&#27969;&#24418;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#33258;&#20027;&#23398;&#20064;&#21644;&#31070;&#32463;&#32593;&#32476;&#25193;&#23637;&#30340;&#36827;&#23637;&#20351;&#24471;&#21487;&#20197;&#21019;&#24314;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#36731;&#26494;&#22320;&#36866;&#24212;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#12290;&#30446;&#21069;&#27604;&#36739;&#22522;&#30784;&#27169;&#22411;&#30340;&#33539;&#24335;&#28041;&#21450;&#22312;&#21508;&#31181;&#31574;&#21010;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#32858;&#21512;&#25351;&#26631;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#31181;&#27169;&#22411;&#27604;&#36739;&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#20110;&#24230;&#37327;&#25351;&#26631;&#30340;&#36873;&#25321;&#65292;&#36825;&#20351;&#24471;&#23427;&#22312;&#29702;&#24819;&#24230;&#37327;&#19981;&#26126;&#26174;&#25110;&#19981;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#19981;&#36866;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27809;&#26377;&#24230;&#37327;&#25351;&#26631;&#30340;&#22522;&#30784;&#27169;&#22411;&#27604;&#36739;&#26041;&#27861;&#65292;&#36890;&#36807;&#23427;&#20204;&#30340;&#23884;&#20837;&#31354;&#38388;&#20960;&#20309;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#38543;&#26426;&#22270;&#29702;&#35770;&#65292;&#24182;&#20419;&#36827;&#28857;&#23545;&#28857;&#21644;&#22810;&#27169;&#22411;&#27604;&#36739;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#25105;&#20204;&#30340;&#26694;&#26550;&#35825;&#23548;&#19968;&#32452;&#37197;&#22791;&#26377;&#19982;&#19968;&#20123;&#19979;&#28216;&#25351;&#26631;&#24378;&#30456;&#20851;&#30340;&#36317;&#31163;&#20989;&#25968;&#30340;&#27169;&#22411;&#27969;&#24418;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in self-supervised learning and neural network scaling have enabled the creation of large models -- known as foundation models -- which can be easily adapted to a wide range of downstream tasks. The current paradigm for comparing foundation models involves benchmarking them with aggregate metrics on various curated datasets. Unfortunately, this method of model comparison is heavily dependent on the choice of metric, which makes it unsuitable for situations where the ideal metric is either not obvious or unavailable. In this work, we present a metric-free methodology for comparing foundation models via their embedding space geometry. Our methodology is grounded in random graph theory, and facilitates both pointwise and multi-model comparison. Further, we demonstrate how our framework can be used to induce a manifold of models equipped with a distance function that correlates strongly with several downstream metrics.
&lt;/p&gt;</description></item><item><title>DroidBot-GPT&#26159;&#19968;&#27454;&#21033;&#29992;GPT&#27169;&#22411;&#33258;&#21160;&#21270;Android&#24212;&#29992;&#31243;&#24207;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#26681;&#25454;&#20219;&#21153;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#33258;&#21160;&#29983;&#25104;&#24182;&#25191;&#34892;&#25805;&#20316;&#65292;&#26377;&#26395;&#25552;&#39640;&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#30340;&#27979;&#35797;&#21644;&#24320;&#21457;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.07061</link><description>&lt;p&gt;
DroidBot-GPT&#65306;&#22522;&#20110;GPT&#30340;Android UI&#33258;&#21160;&#21270;
&lt;/p&gt;
&lt;p&gt;
DroidBot-GPT: GPT-powered UI Automation for Android. (arXiv:2304.07061v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07061
&lt;/p&gt;
&lt;p&gt;
DroidBot-GPT&#26159;&#19968;&#27454;&#21033;&#29992;GPT&#27169;&#22411;&#33258;&#21160;&#21270;Android&#24212;&#29992;&#31243;&#24207;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#26681;&#25454;&#20219;&#21153;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#33258;&#21160;&#29983;&#25104;&#24182;&#25191;&#34892;&#25805;&#20316;&#65292;&#26377;&#26395;&#25552;&#39640;&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#30340;&#27979;&#35797;&#21644;&#24320;&#21457;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;DroidBot-GPT&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;&#31867;&#20284;GPT&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#33258;&#21160;&#21270;&#19982;Android&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#20132;&#20114;&#30340;&#24037;&#20855;&#12290;&#32473;&#23450;&#25152;&#38656;&#20219;&#21153;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#65292;DroidBot-GPT&#21487;&#20197;&#33258;&#21160;&#29983;&#25104;&#24182;&#25191;&#34892;&#25805;&#20316;&#65292;&#23548;&#33322;&#24212;&#29992;&#31243;&#24207;&#20197;&#23436;&#25104;&#20219;&#21153;&#12290;&#23427;&#36890;&#36807;&#23558;&#24212;&#29992;&#31243;&#24207;GUI&#29366;&#24577;&#20449;&#24687;&#21644;&#26234;&#33021;&#25163;&#26426;&#23631;&#24149;&#19978;&#21487;&#29992;&#30340;&#25805;&#20316;&#36716;&#25442;&#20026;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#65292;&#24182;&#35201;&#27714;LLM&#36873;&#25321;&#21160;&#20316;&#26469;&#23454;&#29616;&#12290;&#30001;&#20110;LLM&#36890;&#24120;&#21463;&#36807;&#22823;&#37327;&#25968;&#25454;&#30340;&#35757;&#32451;&#65292;&#21253;&#25324;&#21508;&#31181;&#36719;&#20214;&#24212;&#29992;&#31243;&#24207;&#30340;&#25805;&#20316;&#25351;&#21335;&#65292;&#22240;&#27492;&#23427;&#20855;&#26377;&#26681;&#25454;&#25552;&#20379;&#30340;&#20449;&#24687;&#20316;&#20986;&#21512;&#29702;&#21160;&#20316;&#36873;&#25321;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#33258;&#21019;&#24314;&#30340;&#25968;&#25454;&#38598;&#23545;DroidBot-GPT&#36827;&#34892;&#35780;&#20272;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#26469;&#33258;10&#20010;&#31867;&#21035;&#30340;17&#20010;Android&#24212;&#29992;&#31243;&#24207;&#30340;33&#20010;&#20219;&#21153;&#12290;&#23427;&#21487;&#20197;&#25104;&#21151;&#23436;&#25104;39.39%&#30340;&#20219;&#21153;&#65292;&#24182;&#19988;&#24179;&#22343;&#37096;&#20998;&#23436;&#25104;&#36827;&#24230;&#32422;&#20026;66.76%&#12290;&#37492;&#20110;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#23436;&#20840;&#33258;&#21160;&#30340;&#65292;&#24182;&#19988;&#29992;&#20110;&#35757;&#32451;LLM&#30340;&#25968;&#25454;&#26159;&#24191;&#27867;&#21487;&#29992;&#30340;&#65292;&#25105;&#20204;&#35748;&#20026;DroidBot-GPT&#22312;&#25913;&#21892;&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#30340;&#27979;&#35797;&#21644;&#24320;&#21457;&#25928;&#29575;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces DroidBot-GPT, a tool that utilizes GPT-like large language models (LLMs) to automate the interactions with Android mobile applications. Given a natural language description of a desired task, DroidBot-GPT can automatically generate and execute actions that navigate the app to complete the task. It works by translating the app GUI state information and the available actions on the smartphone screen to natural language prompts and asking the LLM to make a choice of actions. Since the LLM is typically trained on a large amount of data including the how-to manuals of diverse software applications, it has the ability to make reasonable choices of actions based on the provided information. We evaluate DroidBot-GPT with a self-created dataset that contains 33 tasks collected from 17 Android applications spanning 10 categories. It can successfully complete 39.39% of the tasks, and the average partial completion progress is about 66.76%. Given the fact that our method is f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Transformer&#20869;&#22312;&#34917;&#19969;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#26696;&#65292;&#21487;&#22312;&#23494;&#38598;&#19979;&#28216;&#39044;&#27979;&#20219;&#21153;&#20013;&#21463;&#30410;&#12290;&#35813;&#26041;&#26696;&#36866;&#29992;&#20110;&#25152;&#26377;&#35270;&#35273;Transformer&#26550;&#26500;&#65292;&#26131;&#20110;&#23454;&#29616;&#65292;&#24182;&#19988;&#26080;&#38656;&#22823;&#35268;&#27169;&#25209;&#22788;&#29702;&#12290;&#23545;&#27604;Transformer&#26041;&#26696;&#22312;&#33322;&#31354;&#22270;&#20687;&#20998;&#21106;&#20013;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2303.14806</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#20869;&#22312;&#34917;&#19969;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
A Contrastive Learning Scheme with Transformer Innate Patches. (arXiv:2303.14806v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14806
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Transformer&#20869;&#22312;&#34917;&#19969;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#26696;&#65292;&#21487;&#22312;&#23494;&#38598;&#19979;&#28216;&#39044;&#27979;&#20219;&#21153;&#20013;&#21463;&#30410;&#12290;&#35813;&#26041;&#26696;&#36866;&#29992;&#20110;&#25152;&#26377;&#35270;&#35273;Transformer&#26550;&#26500;&#65292;&#26131;&#20110;&#23454;&#29616;&#65292;&#24182;&#19988;&#26080;&#38656;&#22823;&#35268;&#27169;&#25209;&#22788;&#29702;&#12290;&#23545;&#27604;Transformer&#26041;&#26696;&#22312;&#33322;&#31354;&#22270;&#20687;&#20998;&#21106;&#20013;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Transformer&#20869;&#22312;&#34917;&#19969;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#26696;&#65292;&#31216;&#20026;&#23545;&#27604;Transformer&#12290;&#23545;&#27604;Transformer&#33021;&#22815;&#20351;&#24471;&#29616;&#26377;&#30340;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#22312;&#23494;&#38598;&#19979;&#28216;&#39044;&#27979;&#20219;&#21153;&#65288;&#22914;&#35821;&#20041;&#20998;&#21106;&#65289;&#20013;&#21463;&#30410;&#12290;&#35813;&#26041;&#26696;&#36890;&#36807;&#30417;&#30563;&#34917;&#19969;&#32423;&#23545;&#27604;&#23398;&#20064;&#65292;&#22312;&#36873;&#25321;&#34917;&#19969;&#26102;&#22522;&#20110;&#22320;&#38754;&#30495;&#20540;&#25513;&#33180;&#65292;&#21518;&#32493;&#29992;&#20110;&#38590;&#36127;&#26679;&#26412;&#21644;&#38590;&#27491;&#26679;&#26412;&#37319;&#26679;&#12290;&#35813;&#26041;&#26696;&#36866;&#29992;&#20110;&#25152;&#26377;&#35270;&#35273;Transformer&#26550;&#26500;&#65292;&#26131;&#20110;&#23454;&#29616;&#65292;&#24182;&#19988;&#24341;&#20837;&#30340;&#39069;&#22806;&#20869;&#23384;&#24320;&#38144;&#24456;&#23567;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#26696;&#26080;&#38656;&#22823;&#35268;&#27169;&#25209;&#22788;&#29702;&#65292;&#22240;&#20026;&#27599;&#20010;&#34917;&#19969;&#34987;&#35270;&#20026;&#19968;&#24352;&#22270;&#20687;&#12290;&#25105;&#20204;&#23558;&#23545;&#27604;Transformer&#24212;&#29992;&#20110;&#33322;&#31354;&#22270;&#20687;&#20998;&#21106;&#30340;&#26696;&#20363;&#20013;&#65292;&#36825;&#31181;&#24773;&#20917;&#19979;&#23384;&#22312;&#20302;&#20998;&#36776;&#29575;&#25968;&#25454;&#12289;&#22823;&#31867;&#21035;&#19981;&#24179;&#34913;&#21644;&#30456;&#20284;&#35821;&#20041;&#31867;&#21035;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#23545;&#27604;Transformer&#26041;&#26696;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents Contrastive Transformer, a contrastive learning scheme using the Transformer innate patches. Contrastive Transformer enables existing contrastive learning techniques, often used for image classification, to benefit dense downstream prediction tasks such as semantic segmentation. The scheme performs supervised patch-level contrastive learning, selecting the patches based on the ground truth mask, subsequently used for hard-negative and hard-positive sampling. The scheme applies to all vision-transformer architectures, is easy to implement, and introduces minimal additional memory footprint. Additionally, the scheme removes the need for huge batch sizes, as each patch is treated as an image.  We apply and test Contrastive Transformer for the case of aerial image segmentation, known for low-resolution data, large class imbalance, and similar semantic classes. We perform extensive experiments to show the efficacy of the Contrastive Transformer scheme on the ISPRS Potsda
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#20272;&#35745;&#21644;&#32500;&#25252;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#36755;&#20837;-&#36755;&#20986;&#23646;&#24615;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#19981;&#20165;&#32771;&#34385;&#30452;&#25509;&#24433;&#21709;&#65292;&#36824;&#33021;&#32771;&#34385;&#38388;&#25509;&#24433;&#21709;&#12290;&#27492;&#26041;&#27861;&#33021;&#22815;&#22312;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#21516;&#26102;&#26377;&#25928;&#22320;&#37327;&#21270;&#22240;&#26524;&#24402;&#22240;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#23398;&#20064;&#22240;&#26524;&#24402;&#22240;&#12290;</title><link>http://arxiv.org/abs/2303.13850</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#22240;&#26524;&#24402;&#22240;&#23398;&#20064;&#65306;&#36229;&#36234;&#30452;&#25509;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Learning Causal Attributions in Neural Networks: Beyond Direct Effects. (arXiv:2303.13850v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13850
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#20272;&#35745;&#21644;&#32500;&#25252;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#36755;&#20837;-&#36755;&#20986;&#23646;&#24615;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#19981;&#20165;&#32771;&#34385;&#30452;&#25509;&#24433;&#21709;&#65292;&#36824;&#33021;&#32771;&#34385;&#38388;&#25509;&#24433;&#21709;&#12290;&#27492;&#26041;&#27861;&#33021;&#22815;&#22312;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#21516;&#26102;&#26377;&#25928;&#22320;&#37327;&#21270;&#22240;&#26524;&#24402;&#22240;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#23398;&#20064;&#22240;&#26524;&#24402;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#25429;&#25417;&#21644;&#32500;&#25252;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#29992;&#22240;&#26524;&#26041;&#27861;&#20272;&#35745;&#21644;&#32500;&#25252;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#30340;&#36755;&#20837;-&#36755;&#20986;&#23646;&#24615;&#12290;&#29305;&#21035;&#22320;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#20165;&#20551;&#35774;&#36755;&#20837;&#21464;&#37327;&#29420;&#31435;&#65288;&#30001;&#20110;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65289;&#65292;&#22240;&#27492;&#20165;&#30740;&#31350;&#30452;&#25509;&#24433;&#21709;&#12290;&#25105;&#20204;&#23558;&#31070;&#32463;&#32593;&#32476;&#35270;&#20026;&#32467;&#26500;&#24615;&#22240;&#26524;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#22312;&#36755;&#20837;&#29305;&#24449;&#20013;&#24341;&#20837;&#36793;&#32536;&#20197;&#25429;&#25417;&#21644;&#32500;&#25252;&#30452;&#25509;&#21644;&#38388;&#25509;&#22240;&#26524;&#25928;&#24212;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#26377;&#25928;&#30340;&#36817;&#20284;&#31574;&#30053;&#26469;&#37327;&#21270;&#39640;&#32500;&#25968;&#25454;&#30340;&#22240;&#26524;&#24402;&#22240;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#21508;&#31181;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23398;&#20064;&#20102;&#25509;&#36817;&#22522;&#26412;&#20107;&#23454;&#25928;&#26524;&#30340;&#30452;&#25509;&#21644;&#38388;&#25509;&#22240;&#26524;&#24402;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
There has been a growing interest in capturing and maintaining causal relationships in Neural Network (NN) models in recent years. We study causal approaches to estimate and maintain input-output attributions in NN models in this work. In particular, existing efforts in this direction assume independence among input variables (by virtue of the NN architecture), and hence study only direct causal effects. Viewing an NN as a structural causal model (SCM), we instead focus on going beyond direct effects, introduce edges among input features, and provide a simple yet effective methodology to capture and maintain direct and indirect causal effects while training an NN model. We also propose effective approximation strategies to quantify causal attributions in high dimensional data. Our wide range of experiments on synthetic and real-world datasets show that the proposed ante-hoc method learns causal attributions for both direct and indirect causal effects close to the ground truth effects.
&lt;/p&gt;</description></item><item><title>&#36825;&#26412;&#35770;&#25991;&#26159;&#20316;&#32773;&#22312;&#26031;&#22374;&#31119;&#22823;&#23398;&#21644;&#24494;&#36719;&#30740;&#31350;&#38498;&#24037;&#20316;&#32463;&#39564;&#30340;&#25216;&#26415;&#22238;&#24518;&#24405;&#65292;&#28041;&#21450;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#22522;&#26412;&#27010;&#24565;&#12289;&#24212;&#29992;&#20197;&#21450;&#21019;&#36896;&#36807;&#31243;&#20013;&#30340;&#25925;&#20107;&#12290;</title><link>http://arxiv.org/abs/2302.05449</link><description>&lt;p&gt;
Heckerthoughts.&#65288;arXiv:2302.05449v4 [cs.AI] UPDATED&#65289;
&lt;/p&gt;
&lt;p&gt;
Heckerthoughts. (arXiv:2302.05449v4 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05449
&lt;/p&gt;
&lt;p&gt;
&#36825;&#26412;&#35770;&#25991;&#26159;&#20316;&#32773;&#22312;&#26031;&#22374;&#31119;&#22823;&#23398;&#21644;&#24494;&#36719;&#30740;&#31350;&#38498;&#24037;&#20316;&#32463;&#39564;&#30340;&#25216;&#26415;&#22238;&#24518;&#24405;&#65292;&#28041;&#21450;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#22522;&#26412;&#27010;&#24565;&#12289;&#24212;&#29992;&#20197;&#21450;&#21019;&#36896;&#36807;&#31243;&#20013;&#30340;&#25925;&#20107;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26159;&#20851;&#20110;&#25105;&#22312;&#26031;&#22374;&#31119;&#22823;&#23398;&#21644;&#24494;&#36719;&#30740;&#31350;&#38498;&#24037;&#20316;&#30340;&#25216;&#26415;&#22238;&#24518;&#24405;&#12290;&#21253;&#25324;&#20102;&#19982;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#30456;&#20851;&#30340;&#22522;&#26412;&#27010;&#24565;&#65292;&#36825;&#20123;&#27010;&#24565;&#30340;&#24212;&#29992;&#20197;&#21450;&#20854;&#21019;&#36896;&#32972;&#21518;&#30340;&#25925;&#20107;&#12290;
&lt;/p&gt;
&lt;p&gt;
This manuscript is technical memoir about my work at Stanford and Microsoft Research. Included are fundamental concepts central to machine learning and artificial intelligence, applications of these concepts, and stories behind their creation.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#39118;&#38505;&#20998;&#35299;&#65292;&#25552;&#20986;&#22235;&#20010;&#35823;&#24046;&#37096;&#20998;&#35780;&#20272;&#33258;&#30417;&#30563;&#23398;&#20064;&#23545;169&#20010;&#35270;&#35273;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#20026;SSL&#30340;&#35774;&#35745;&#21644;&#20351;&#29992;&#25552;&#20379;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2302.03068</link><description>&lt;p&gt;
&#36890;&#36807;&#39118;&#38505;&#20998;&#35299;&#35780;&#20272;&#33258;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Evaluating Self-Supervised Learning via Risk Decomposition. (arXiv:2302.03068v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03068
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#39118;&#38505;&#20998;&#35299;&#65292;&#25552;&#20986;&#22235;&#20010;&#35823;&#24046;&#37096;&#20998;&#35780;&#20272;&#33258;&#30417;&#30563;&#23398;&#20064;&#23545;169&#20010;&#35270;&#35273;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#20026;SSL&#30340;&#35774;&#35745;&#21644;&#20351;&#29992;&#25552;&#20379;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#30340;&#27969;&#31243;&#35774;&#35745;&#28041;&#21450;&#26550;&#26500;&#12289;&#22686;&#24378;&#21644;&#39044;&#35757;&#32451;&#25968;&#25454;&#31561;&#35832;&#22810;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;SSL&#36890;&#24120;&#20351;&#29992;&#21333;&#19968;&#24230;&#37327;&#26469;&#35780;&#20272;&#65292;&#36825;&#24182;&#19981;&#33021;&#25552;&#20379;&#28145;&#20837;&#30340;&#27934;&#23519;&#21644;&#25913;&#36827;&#26041;&#26696;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;SSL&#39118;&#38505;&#20998;&#35299;&#65292;&#20174;&#36924;&#36817;&#12289;&#34920;&#31034;&#21487;&#29992;&#24615;&#12289;&#25506;&#38024;&#27867;&#21270;&#21644;&#32534;&#30721;&#22120;&#27867;&#21270;&#31561;&#35282;&#24230;&#23545;&#38169;&#35823;&#36827;&#34892;&#20998;&#35299;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;30&#20010;&#35774;&#35745;&#36873;&#25321;&#23545;169&#20010;&#22312;ImageNet&#19978;&#35780;&#20272;&#30340;SSL&#35270;&#35273;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#20026;&#27599;&#20010;&#32452;&#20214;&#25552;&#20379;&#20102;&#39640;&#25928;&#30340;&#20272;&#35745;&#22120;&#65292;&#20026;SSL&#27169;&#22411;&#30340;&#35774;&#35745;&#21644;&#20351;&#29992;&#25552;&#20379;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) pipelines differ in many design choices such as the architecture, augmentations, or pretraining data. Yet SSL is typically evaluated using a single metric: linear probing on ImageNet. This does not provide much insight into why or when a model is better, now how to improve it. To address this, we propose an SSL risk decomposition, which generalizes the classical supervised approximation-estimation decomposition by considering errors arising from the representation learning step. Our decomposition consists of four error components: approximation, representation usability, probe generalization, and encoder generalization. We provide efficient estimators for each component and use them to analyze the effect of 30 design choices on 169 SSL vision models evaluated on ImageNet. Our analysis gives valuable insights for designing and using SSL models. For example, it highlights the main sources of error and shows how to improve SSL in specific settings (full- vs 
&lt;/p&gt;</description></item><item><title>LEXTREME&#26159;&#19968;&#20010;&#22810;&#35821;&#35328;&#21644;&#22810;&#20219;&#21153;&#30340;&#27861;&#24459;&#39046;&#22495;&#22522;&#20934;&#65292;&#35813;&#22522;&#20934;&#25552;&#20379;&#20102;11&#20010;&#25968;&#25454;&#38598;&#28085;&#30422;24&#31181;&#35821;&#35328;&#30340;&#27979;&#35780;&#65292;&#26368;&#20339;&#27169;&#22411;&#65288;XLM-R large&#65289;&#22312;&#25968;&#25454;&#38598;&#21644;&#35821;&#35328;&#32508;&#21512;&#35780;&#20998;&#19978;&#22343;&#36798;&#21040;&#20102;61.3&#12290;&#36825;&#20351;&#24471;LEXTREME&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#24182;&#19988;&#26377;&#25913;&#36827;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2301.13126</link><description>&lt;p&gt;
LEXTREME&#65306;&#22810;&#35821;&#35328;&#21644;&#22810;&#20219;&#21153;&#30340;&#27861;&#24459;&#39046;&#22495;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
LEXTREME: A Multi-Lingual and Multi-Task Benchmark for the Legal Domain. (arXiv:2301.13126v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13126
&lt;/p&gt;
&lt;p&gt;
LEXTREME&#26159;&#19968;&#20010;&#22810;&#35821;&#35328;&#21644;&#22810;&#20219;&#21153;&#30340;&#27861;&#24459;&#39046;&#22495;&#22522;&#20934;&#65292;&#35813;&#22522;&#20934;&#25552;&#20379;&#20102;11&#20010;&#25968;&#25454;&#38598;&#28085;&#30422;24&#31181;&#35821;&#35328;&#30340;&#27979;&#35780;&#65292;&#26368;&#20339;&#27169;&#22411;&#65288;XLM-R large&#65289;&#22312;&#25968;&#25454;&#38598;&#21644;&#35821;&#35328;&#32508;&#21512;&#35780;&#20998;&#19978;&#22343;&#36798;&#21040;&#20102;61.3&#12290;&#36825;&#20351;&#24471;LEXTREME&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#24182;&#19988;&#26377;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22312;transformer&#26550;&#26500;&#30340;&#26174;&#33879;&#36827;&#23637;&#25512;&#21160;&#19979;&#65292;&#27861;&#24459;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#22686;&#38271;&#12290;&#20026;&#20102;&#34913;&#37327;&#36827;&#23637;&#65292;&#31934;&#24515;&#31574;&#21010;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#22522;&#20934;&#21482;&#33021;&#22788;&#29702;&#33521;&#25991;&#65292;&#32780;&#22312;&#27861;&#24459;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#23578;&#26410;&#26377;&#22810;&#35821;&#35328;&#22522;&#20934;&#21487;&#29992;&#12290;&#27492;&#22806;&#65292;&#35768;&#22810;&#22522;&#20934;&#24050;&#32463;&#39281;&#21644;&#65292;&#26368;&#20339;&#27169;&#22411;&#26126;&#26174;&#20248;&#20110;&#26368;&#20339;&#20154;&#31867;&#65292;&#24182;&#36798;&#21040;&#36817;&#20046;&#23436;&#32654;&#30340;&#20998;&#25968;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#27861;&#24459;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25991;&#29486;&#65292;&#24182;&#36873;&#25321;&#20102;11&#20010;&#28085;&#30422;24&#31181;&#35821;&#35328;&#30340;&#25968;&#25454;&#38598;&#65292;&#21019;&#24314;&#20102;LEXTREME&#12290;&#20026;&#20102;&#36827;&#34892;&#20844;&#24179;&#27604;&#36739;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#32508;&#21512;&#35780;&#20998;&#65292;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#38598;&#65292;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#12290;&#26368;&#20339;&#22522;&#32447;&#27169;&#22411;&#65288;XLM-R large&#65289;&#22312;&#25968;&#25454;&#38598;&#32508;&#21512;&#35780;&#20998;&#21644;&#35821;&#35328;&#32508;&#21512;&#35780;&#20998;&#19978;&#22343;&#36798;&#21040;&#20102;61.3&#12290;&#36825;&#34920;&#26126;LEXTREME&#20173;&#28982;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#24182;&#19988;&#20026;&#25913;&#36827;&#30041;&#19979;&#20102;&#20805;&#36275;&#31354;&#38388;&#12290;&#20026;&#20102;&#26041;&#20415;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#20351;&#29992;&#65292;&#25105;&#20204;&#23558;LEXTREME&#19982;&#25152;&#26377;&#25968;&#25454;&#19968;&#36215;&#21457;&#24067;&#22312;huggingface&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lately, propelled by the phenomenal advances around the transformer architecture, the legal NLP field has enjoyed spectacular growth. To measure progress, well curated and challenging benchmarks are crucial. However, most benchmarks are English only and in legal NLP specifically there is no multilingual benchmark available yet. Additionally, many benchmarks are saturated, with the best models clearly outperforming the best humans and achieving near perfect scores. We survey the legal NLP literature and select 11 datasets covering 24 languages, creating LEXTREME. To provide a fair comparison, we propose two aggregate scores, one based on the datasets and one on the languages. The best baseline (XLM-R large) achieves both a dataset aggregate score a language aggregate score of 61.3. This indicates that LEXTREME is still very challenging and leaves ample room for improvement. To make it easy for researchers and practitioners to use, we release LEXTREME on huggingface together with all the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29702;&#35770;&#65292;&#20801;&#35768;&#22312;&#21387;&#32553;&#30340;&#25913;&#21464;&#27010;&#29575;&#19978;&#20445;&#25345;&#25511;&#21046;&#65292;&#24182;&#33719;&#24471;&#20102;&#32039;&#23494;&#30340;&#26377;&#38480;&#26679;&#26412;&#36793;&#30028;&#26469;&#35780;&#20272;&#21387;&#32553;&#30340;&#25913;&#21464;&#27010;&#29575;&#12290;&#36825;&#23545;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;&#38169;&#35823;&#20998;&#31867;&#21644;&#38169;&#35823;&#39044;&#27979;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2301.12767</link><description>&lt;p&gt;
&#21387;&#32553;&#12289;&#27867;&#21270;&#21644;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Compression, Generalization and Learning. (arXiv:2301.12767v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12767
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29702;&#35770;&#65292;&#20801;&#35768;&#22312;&#21387;&#32553;&#30340;&#25913;&#21464;&#27010;&#29575;&#19978;&#20445;&#25345;&#25511;&#21046;&#65292;&#24182;&#33719;&#24471;&#20102;&#32039;&#23494;&#30340;&#26377;&#38480;&#26679;&#26412;&#36793;&#30028;&#26469;&#35780;&#20272;&#21387;&#32553;&#30340;&#25913;&#21464;&#27010;&#29575;&#12290;&#36825;&#23545;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;&#38169;&#35823;&#20998;&#31867;&#21644;&#38169;&#35823;&#39044;&#27979;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21387;&#32553;&#20989;&#25968;&#26159;&#19968;&#31181;&#23558;&#35266;&#27979;&#38598;&#32553;&#23567;&#20026;&#23610;&#23544;&#20943;&#23567;&#30340;&#23376;&#38598;&#30340;&#26144;&#23556;&#65292;&#21516;&#26102;&#20445;&#30041;&#20854;&#20449;&#24687;&#20869;&#23481;&#12290;&#22312;&#22810;&#20010;&#24212;&#29992;&#20013;&#65292;&#26032;&#35266;&#27979;&#20351;&#21387;&#32553;&#38598;&#21457;&#29983;&#21464;&#21270;&#30340;&#26465;&#20214;&#34987;&#35299;&#37322;&#20026;&#26032;&#35266;&#27979;&#24102;&#26469;&#20102;&#39069;&#22806;&#30340;&#20449;&#24687;&#65292;&#22312;&#23398;&#20064;&#29702;&#35770;&#20013;&#65292;&#36825;&#23545;&#24212;&#20110;&#38169;&#35823;&#20998;&#31867;&#25110;&#38169;&#35823;&#39044;&#27979;&#12290;&#26412;&#25991;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#29702;&#35770;&#30340;&#22522;&#30784;&#65292;&#20801;&#35768;&#22312;&#21387;&#32553;&#30340;&#25913;&#21464;&#27010;&#29575;&#19978;&#20445;&#25345;&#25511;&#21046;&#65288;&#19982;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;&#32479;&#35745;&#8220;&#39118;&#38505;&#8221;&#30456;&#23545;&#24212;&#65289;&#12290;&#22312;&#36866;&#24403;&#30340;&#26465;&#20214;&#19979;&#65292;&#21387;&#32553;&#38598;&#30340;&#22522;&#25968;&#34987;&#35777;&#26126;&#26159;&#21387;&#32553;&#30340;&#25913;&#21464;&#27010;&#29575;&#30340;&#19968;&#33268;&#20272;&#35745;&#37327;&#65288;&#19981;&#23545;&#21387;&#32553;&#38598;&#30340;&#23610;&#23544;&#35774;&#32622;&#19978;&#38480;&#65289;&#65307;&#27492;&#22806;&#65292;&#22312;&#26222;&#36941;&#36866;&#29992;&#30340;&#20559;&#22909;&#26465;&#20214;&#19979;&#33719;&#24471;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#32039;&#23494;&#30340;&#26377;&#38480;&#26679;&#26412;&#36793;&#30028;&#26469;&#35780;&#20272;&#21387;&#32553;&#30340;&#25913;&#21464;&#27010;&#29575;&#12290;&#25152;&#26377;&#32467;&#26524;&#37117;&#21487;&#20197;&#22312;&#23436;&#20840;&#24212;&#29992;&#20013;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
A compression function is a map that slims down an observational set into a subset of reduced size, while preserving its informational content. In multiple applications, the condition that one new observation makes the compressed set change is interpreted that this observation brings in extra information and, in learning theory, this corresponds to misclassification, or misprediction. In this paper, we lay the foundations of a new theory that allows one to keep control on the probability of change of compression (which maps into the statistical "risk" in learning applications). Under suitable conditions, the cardinality of the compressed set is shown to be a consistent estimator of the probability of change of compression (without any upper limit on the size of the compressed set); moreover, unprecedentedly tight finite-sample bounds to evaluate the probability of change of compression are obtained under a generally applicable condition of preference. All results are usable in a fully 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38416;&#36848;&#20102;&#23545;&#20110;&#20013;&#31561;&#35268;&#27169;&#30340;&#27169;&#22411;&#31867;&#65292;&#20219;&#20309;&#23436;&#25972;&#30340;&#32447;&#24615;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#37117;&#26080;&#27861;&#32988;&#20219;&#25512;&#26029;&#27169;&#22411;&#34892;&#20026;&#30340;&#25913;&#36827;&#65292;&#21551;&#31034;&#25105;&#20204;&#22312;&#20855;&#20307;&#23450;&#20041;&#26368;&#32456;&#20219;&#21153;&#30340;&#37325;&#35201;&#24615;&#26041;&#38754;&#35201;&#27762;&#21462;&#32463;&#39564;&#65292;&#37319;&#29992;&#37325;&#22797;&#27169;&#22411;&#35780;&#20272;&#30340;&#31616;&#21333;&#30452;&#25509;&#26041;&#27861;&#21487;&#20197;&#32988;&#36807;&#35768;&#22810;&#20854;&#20182;&#22797;&#26434;&#30340;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2212.11870</link><description>&lt;p&gt;
&#29305;&#24449;&#24402;&#22240;&#30340;&#19981;&#21487;&#33021;&#23450;&#29702;
&lt;/p&gt;
&lt;p&gt;
Impossibility Theorems for Feature Attribution. (arXiv:2212.11870v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.11870
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38416;&#36848;&#20102;&#23545;&#20110;&#20013;&#31561;&#35268;&#27169;&#30340;&#27169;&#22411;&#31867;&#65292;&#20219;&#20309;&#23436;&#25972;&#30340;&#32447;&#24615;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#37117;&#26080;&#27861;&#32988;&#20219;&#25512;&#26029;&#27169;&#22411;&#34892;&#20026;&#30340;&#25913;&#36827;&#65292;&#21551;&#31034;&#25105;&#20204;&#22312;&#20855;&#20307;&#23450;&#20041;&#26368;&#32456;&#20219;&#21153;&#30340;&#37325;&#35201;&#24615;&#26041;&#38754;&#35201;&#27762;&#21462;&#32463;&#39564;&#65292;&#37319;&#29992;&#37325;&#22797;&#27169;&#22411;&#35780;&#20272;&#30340;&#31616;&#21333;&#30452;&#25509;&#26041;&#27861;&#21487;&#20197;&#32988;&#36807;&#35768;&#22810;&#20854;&#20182;&#22797;&#26434;&#30340;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26377;&#35768;&#22810;&#21487;&#20135;&#29983;&#21512;&#29702;&#35299;&#37322;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#20294;&#35813;&#39046;&#22495;&#20063;&#32463;&#39564;&#24615;&#22320;&#30475;&#21040;&#20102;&#35768;&#22810;&#22833;&#36133;&#26696;&#20363;&#12290;&#37492;&#20110;&#36825;&#20123;&#32467;&#26524;&#65292;&#23545;&#20110;&#23454;&#36341;&#32773;&#22914;&#20309;&#20197;&#21407;&#21017;&#24615;&#26041;&#24335;&#20351;&#29992;&#36825;&#20123;&#26041;&#27861;&#24182;&#22312;&#23427;&#20204;&#20043;&#38388;&#36827;&#34892;&#36873;&#25321;&#20173;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#20110;&#20013;&#31561;&#35268;&#27169;&#30340;&#27169;&#22411;&#31867;&#65288;&#31070;&#32463;&#32593;&#32476;&#23481;&#26131;&#28385;&#36275;&#65289;&#65292;&#20219;&#20309;&#23436;&#25972;&#30340;&#32447;&#24615;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#65288;&#20363;&#22914;Integrated Gradients&#21644;SHAP&#65289;&#21487;&#20197;&#34987;&#35777;&#26126;&#23545;&#20110;&#25512;&#26029;&#27169;&#22411;&#34892;&#20026;&#30340;&#25913;&#36827;&#37117;&#26080;&#27861;&#32988;&#20219;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#36866;&#29992;&#20110;&#24120;&#35265;&#30340;&#26368;&#32456;&#20219;&#21153;&#65292;&#22914;&#25551;&#36848;&#23616;&#37096;&#27169;&#22411;&#34892;&#20026;&#12289;&#35782;&#21035;&#34394;&#20551;&#29305;&#24449;&#21644;&#31639;&#27861;&#22238;&#28335;&#12290;&#25105;&#20204;&#24037;&#20316;&#30340;&#19968;&#20010;&#37325;&#35201;&#21551;&#31034;&#26159;&#20855;&#20307;&#23450;&#20041;&#26368;&#32456;&#20219;&#21153;&#30340;&#37325;&#35201;&#24615;&#65306;&#19968;&#26086;&#36825;&#26679;&#30340;&#26368;&#32456;&#20219;&#21153;&#34987;&#23450;&#20041;&#65292;&#19968;&#20010;&#31616;&#21333;&#21644;&#30452;&#25509;&#30340;&#26041;&#27861;&#8212;&#8212;&#37325;&#22797;&#27169;&#22411;&#35780;&#20272;&#8212;&#8212;&#21487;&#20197;&#32988;&#36807;&#35768;&#22810;&#20854;&#20182;&#22797;&#26434;&#30340;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite a sea of interpretability methods that can produce plausible explanations, the field has also empirically seen many failure cases of such methods. In light of these results, it remains unclear for practitioners how to use these methods and choose between them in a principled way. In this paper, we show that for moderately rich model classes (easily satisfied by neural networks), any feature attribution method that is complete and linear -- for example, Integrated Gradients and SHAP -- can provably fail to improve on random guessing for inferring model behaviour. Our results apply to common end-tasks such as characterizing local model behaviour, identifying spurious features, and algorithmic recourse. One takeaway from our work is the importance of concretely defining end-tasks: once such an end-task is defined, a simple and direct approach of repeated model evaluations can outperform many other complex feature attribution methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#39068;&#33394;-&#20107;&#20214;&#32479;&#19968;&#36319;&#36394;&#30340;&#21333;&#38454;&#27573;&#39592;&#24178;&#32593;&#32476;&#65288;CEUTrack&#65289;&#65292;&#36890;&#36807;&#23558;&#20107;&#20214;&#28857;&#21644;RGB&#24103;&#36716;&#25442;&#20026;&#20307;&#32032;&#65292;&#23558;&#35009;&#21098;&#21518;&#30340;&#27169;&#26495;&#21644;&#25628;&#32034;&#21306;&#22495;&#25237;&#24433;&#21040;&#20196;&#29260;&#20013;&#65292;&#24182;&#36890;&#36807;&#32479;&#19968;&#30340;Transformer&#39592;&#24178;&#32593;&#32476;&#23454;&#29616;&#20102;&#30446;&#26631;&#23545;&#35937;&#30340;&#23450;&#20301;&#12290;&#36825;&#19968;&#26041;&#27861;&#31616;&#21333;&#12289;&#26377;&#25928;&#19988;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2211.11010</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#22522;&#20110;&#39068;&#33394;&#20107;&#20214;&#30340;&#36319;&#36394;&#65306;&#19968;&#20010;&#32479;&#19968;&#30340;&#32593;&#32476;&#12289;&#25968;&#25454;&#38598;&#21644;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
Revisiting Color-Event based Tracking: A Unified Network, Dataset, and Metric. (arXiv:2211.11010v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11010
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#39068;&#33394;-&#20107;&#20214;&#32479;&#19968;&#36319;&#36394;&#30340;&#21333;&#38454;&#27573;&#39592;&#24178;&#32593;&#32476;&#65288;CEUTrack&#65289;&#65292;&#36890;&#36807;&#23558;&#20107;&#20214;&#28857;&#21644;RGB&#24103;&#36716;&#25442;&#20026;&#20307;&#32032;&#65292;&#23558;&#35009;&#21098;&#21518;&#30340;&#27169;&#26495;&#21644;&#25628;&#32034;&#21306;&#22495;&#25237;&#24433;&#21040;&#20196;&#29260;&#20013;&#65292;&#24182;&#36890;&#36807;&#32479;&#19968;&#30340;Transformer&#39592;&#24178;&#32593;&#32476;&#23454;&#29616;&#20102;&#30446;&#26631;&#23545;&#35937;&#30340;&#23450;&#20301;&#12290;&#36825;&#19968;&#26041;&#27861;&#31616;&#21333;&#12289;&#26377;&#25928;&#19988;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23558;&#39068;&#33394;&#21644;&#20107;&#20214;&#30456;&#26426;&#65288;&#20063;&#31216;&#20026;&#21160;&#24577;&#35270;&#35273;&#20256;&#24863;&#22120;&#65292;DVS&#65289;&#32467;&#21512;&#36215;&#26469;&#36827;&#34892;&#40065;&#26834;&#30340;&#30446;&#26631;&#36319;&#36394;&#26159;&#19968;&#20010;&#26032;&#20852;&#30340;&#30740;&#31350;&#35838;&#39064;&#12290;&#29616;&#26377;&#30340;&#39068;&#33394;-&#20107;&#20214;&#36319;&#36394;&#26694;&#26550;&#36890;&#24120;&#21253;&#21547;&#22810;&#20010;&#31163;&#25955;&#30340;&#27169;&#22359;&#65292;&#21487;&#33021;&#23548;&#33268;&#20302;&#25928;&#29575;&#21644;&#39640;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#21253;&#25324;&#29305;&#24449;&#25552;&#21462;&#12289;&#34701;&#21512;&#12289;&#21305;&#37197;&#12289;&#20132;&#20114;&#23398;&#20064;&#31561;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#39068;&#33394;-&#20107;&#20214;&#32479;&#19968;&#36319;&#36394;&#65288;CEUTrack&#65289;&#30340;&#21333;&#38454;&#27573;&#39592;&#24178;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#21516;&#26102;&#23454;&#29616;&#20102;&#19978;&#36848;&#21151;&#33021;&#12290;&#32473;&#23450;&#20107;&#20214;&#28857;&#21644;RGB&#24103;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#28857;&#36716;&#25442;&#20026;&#20307;&#32032;&#65292;&#24182;&#20998;&#21035;&#35009;&#21098;&#27169;&#26495;&#21644;&#25628;&#32034;&#21306;&#22495;&#30340;&#20004;&#31181;&#27169;&#24577;&#12290;&#28982;&#21518;&#65292;&#36825;&#20123;&#21306;&#22495;&#34987;&#25237;&#24433;&#21040;&#20196;&#29260;&#20013;&#65292;&#24182;&#24182;&#34892;&#36755;&#20837;&#21040;&#32479;&#19968;&#30340;Transformer&#39592;&#24178;&#32593;&#32476;&#12290;&#36755;&#20986;&#29305;&#24449;&#23558;&#34987;&#36755;&#20837;&#21040;&#36319;&#36394;&#22836;&#37096;&#36827;&#34892;&#30446;&#26631;&#23545;&#35937;&#23450;&#20301;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;CEUTrack&#31616;&#21333;&#12289;&#26377;&#25928;&#12289;&#39640;&#25928;&#65292;&#36798;&#21040;&#20102;&#36229;&#36807;75 FPS&#21644;&#26032;&#30340;SOTA&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Combining the Color and Event cameras (also called Dynamic Vision Sensors, DVS) for robust object tracking is a newly emerging research topic in recent years. Existing color-event tracking framework usually contains multiple scattered modules which may lead to low efficiency and high computational complexity, including feature extraction, fusion, matching, interactive learning, etc. In this paper, we propose a single-stage backbone network for Color-Event Unified Tracking (CEUTrack), which achieves the above functions simultaneously. Given the event points and RGB frames, we first transform the points into voxels and crop the template and search regions for both modalities, respectively. Then, these regions are projected into tokens and parallelly fed into the unified Transformer backbone network. The output features will be fed into a tracking head for target object localization. Our proposed CEUTrack is simple, effective, and efficient, which achieves over 75 FPS and new SOTA perform
&lt;/p&gt;</description></item><item><title>NESTER&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#31070;&#32463;&#31526;&#21495;&#21270;&#26041;&#27861;&#36827;&#34892;&#27835;&#30103;&#25928;&#26524;&#35780;&#20272;&#65292;&#23558;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#30340;&#25152;&#26377;&#35201;&#27714;&#38598;&#25104;&#21040;&#19968;&#20010;&#26694;&#26550;&#20013;&#65292;&#35813;&#26041;&#27861;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#24615;&#33021;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2211.04370</link><description>&lt;p&gt;
NESTER&#65306;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#31070;&#32463;&#31526;&#21495;&#21270;&#26041;&#27861;&#36827;&#34892;&#27835;&#30103;&#25928;&#26524;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
NESTER: An Adaptive Neurosymbolic Method for Treatment Effect Estimation. (arXiv:2211.04370v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.04370
&lt;/p&gt;
&lt;p&gt;
NESTER&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#31070;&#32463;&#31526;&#21495;&#21270;&#26041;&#27861;&#36827;&#34892;&#27835;&#30103;&#25928;&#26524;&#35780;&#20272;&#65292;&#23558;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#30340;&#25152;&#26377;&#35201;&#27714;&#38598;&#25104;&#21040;&#19968;&#20010;&#26694;&#26550;&#20013;&#65292;&#35813;&#26041;&#27861;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#24615;&#33021;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#36827;&#34892;&#27835;&#30103;&#25928;&#26524;&#35780;&#20272;&#26159;&#22240;&#26524;&#25512;&#26029;&#20013;&#30340;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#12290;&#22522;&#20110;&#28508;&#22312;&#32467;&#26524;&#26694;&#26550;&#30340;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#22240;&#26524;&#25512;&#26029;&#20013;&#30340;&#24402;&#32435;&#20559;&#32622;&#21644;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#27599;&#31181;&#29616;&#26377;&#30340;&#25216;&#26415;&#37117;&#36890;&#36807;&#35774;&#35745;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#21644;&#27491;&#21017;&#21270;&#22120;&#26469;&#35299;&#20915;&#27835;&#30103;&#25928;&#26524;&#35780;&#20272;&#30340;&#29305;&#23450;&#26041;&#38754;&#65292;&#20363;&#22914;&#25511;&#21046;&#20542;&#21521;&#24471;&#20998;&#12289;&#24378;&#21046;&#38543;&#26426;&#21270;&#31561;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#31216;&#20026;&#31070;&#32463;&#31526;&#21495;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#22120;&#65288;NESTER&#65289;&#65292;&#23427;&#26159;&#19968;&#31181;&#27835;&#30103;&#25928;&#26524;&#35780;&#20272;&#30340;&#36890;&#29992;&#26041;&#27861;&#12290;NESTER&#23558;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#30340;&#25152;&#26377;&#35201;&#27714;&#38598;&#25104;&#21040;&#19968;&#20010;&#26694;&#26550;&#20013;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#25991;&#29486;&#20013;&#20351;&#29992;&#30340;&#24402;&#32435;&#20559;&#32622;&#30340;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#30340;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#65288;DSL&#65289;&#12290;&#25105;&#20204;&#36824;&#22312;&#29702;&#35770;&#19978;&#30740;&#31350;&#20102;NESTER&#22312;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#20840;&#38754;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#65292;NESTER&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#25928;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Treatment effect estimation from observational data is a central problem in causal inference. Methods based on potential outcomes framework solve this problem by exploiting inductive biases and heuristics from causal inference. Each existing technique addresses a specific aspect of treatment effect estimation, such as controlling propensity score, enforcing randomization, etc., by designing neural network architectures and regularizers. In this paper, we propose an adaptive method called Neurosymbolic Treatment Effect Estimator (NESTER), a generalized method for treatment effect estimation. NESTER brings together all the desiderata for treatment effect estimation into one framework. For this purpose, we design a Domain Specific Language (DSL) for the treatment effect estimation based on inductive biases used in literature. We also theoretically study NESTER's capability for the treatment effect estimation task. Our comprehensive empirical results show that NESTER performs better on ben
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DGPO&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#35299;&#20915;&#20219;&#21153;&#26102;&#21457;&#29616;&#22810;&#31181;&#31574;&#30053;&#65292;&#20174;&#32780;&#25552;&#39640;&#31574;&#30053;&#40065;&#26834;&#24615;&#21644;&#19982;&#29992;&#25143;&#20132;&#20114;&#30340;&#20048;&#36259;&#12290;</title><link>http://arxiv.org/abs/2207.05631</link><description>&lt;p&gt;
DGPO: &#20351;&#29992;&#22810;&#26679;&#21270;&#31574;&#30053;&#20248;&#21270;&#21457;&#29616;&#22810;&#31181;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
DGPO: Discovering Multiple Strategies with Diversity-Guided Policy Optimization. (arXiv:2207.05631v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.05631
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DGPO&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#35299;&#20915;&#20219;&#21153;&#26102;&#21457;&#29616;&#22810;&#31181;&#31574;&#30053;&#65292;&#20174;&#32780;&#25552;&#39640;&#31574;&#30053;&#40065;&#26834;&#24615;&#21644;&#19982;&#29992;&#25143;&#20132;&#20114;&#30340;&#20048;&#36259;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#37117;&#35797;&#22270;&#23547;&#25214;&#35299;&#20915;&#32473;&#23450;&#20219;&#21153;&#30340;&#21333;&#20010;&#26368;&#20339;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#23398;&#20064;&#22810;&#31181;&#35299;&#20915;&#26041;&#26696;&#36890;&#24120;&#26159;&#26377;&#20215;&#20540;&#30340;&#65292;&#20363;&#22914;&#65292;&#20351;&#26234;&#33021;&#20307;&#19982;&#29992;&#25143;&#30340;&#20132;&#20114;&#26356;&#21152;&#26377;&#36259;&#65292;&#25110;&#32773;&#25552;&#39640;&#31574;&#30053;&#23545;&#24847;&#22806;&#24178;&#25200;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#26679;&#21270;&#31574;&#30053;&#20248;&#21270;&#65288;DGPO&#65289;&#30340;&#22312;&#32447;&#31639;&#27861;&#65292;&#29992;&#20110;&#21457;&#29616;&#35299;&#20915;&#32473;&#23450;&#20219;&#21153;&#30340;&#22810;&#31181;&#31574;&#30053;&#12290;&#19982;&#29616;&#26377;&#24037;&#20316;&#19981;&#21516;&#30340;&#26159;&#65292;&#23427;&#36890;&#36807;&#22312;&#21333;&#27425;&#36816;&#34892;&#20013;&#35757;&#32451;&#20849;&#20139;&#31574;&#30053;&#32593;&#32476;&#23454;&#29616;&#27492;&#30446;&#30340;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#29702;&#35770;&#22810;&#26679;&#24615;&#30446;&#26631;&#30340;&#20869;&#22312;&#22870;&#21169;&#12290;&#25105;&#20204;&#30340;&#26368;&#32456;&#30446;&#26631;&#20132;&#26367;&#32422;&#26463;&#31574;&#30053;&#22810;&#26679;&#24615;&#21644;&#22806;&#22312;&#22870;&#21169;&#12290;&#25105;&#20204;&#23558;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#36716;&#21270;&#20026;&#27010;&#29575;&#25512;&#26029;&#20219;&#21153;&#65292;&#24182;&#20351;&#29992;&#31574;&#30053;&#36845;&#20195;&#26469;&#26368;&#22823;&#21270;&#24471;&#21040;&#30340;&#19979;&#30028;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#21508;&#31181;&#29615;&#22659;&#20013;&#26377;&#25928;&#22320;&#21457;&#29616;&#22810;&#26679;&#21270;&#30340;&#31574;&#30053;&#65292;&#21253;&#25324; Atari &#28216;&#25103;&#21644; Mujoco &#27169;&#25311;&#22120;&#65292;&#24182;&#19988;&#33021;&#22815;&#25552;&#20379;&#19968;&#31995;&#21015;&#24615;&#33021;&#21644;&#22810;&#26679;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most reinforcement learning algorithms seek a single optimal strategy that solves a given task. However, it can often be valuable to learn a diverse set of solutions, for instance, to make an agent's interaction with users more engaging, or improve the robustness of a policy to an unexpected perturbance. We propose Diversity-Guided Policy Optimization (DGPO), an on-policy algorithm that discovers multiple strategies for solving a given task. Unlike prior work, it achieves this with a shared policy network trained over a single run. Specifically, we design an intrinsic reward based on an information-theoretic diversity objective. Our final objective alternately constraints on the diversity of the strategies and on the extrinsic reward. We solve the constrained optimization problem by casting it as a probabilistic inference task and use policy iteration to maximize the derived lower bound. Experimental results show that our method efficiently discovers diverse strategies in a wide variet
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#27010;&#24565;&#30456;&#20851;&#20256;&#25773;&#65288;CRP&#65289;&#26041;&#27861;&#65292;&#23558;&#23616;&#37096;&#21644;&#20840;&#23616;&#35266;&#28857;&#32467;&#21512;&#36215;&#26469;&#65292;&#20026;&#20010;&#21035;&#39044;&#27979;&#25552;&#20379;&#20102;&#8220;&#20309;&#22320;&#8221;&#21644;&#8220;&#20309;&#29289;&#8221;&#20004;&#20010;&#38382;&#39064;&#30340;&#35299;&#31572;&#12290;&#35813;&#26041;&#27861;&#25552;&#20379;&#20102;&#26356;&#20154;&#31867;&#21487;&#35299;&#37322;&#30340;&#35299;&#37322;&#65292;&#24182;&#36890;&#36807;&#27010;&#24565;&#22270;&#35889;&#28145;&#20837;&#20102;&#35299;&#27169;&#22411;&#30340;&#34920;&#31034;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2206.03208</link><description>&lt;p&gt;
&#20174;&#24402;&#22240;&#22270;&#21040;&#21487;&#29702;&#35299;&#30340;&#20154;&#31867;&#35299;&#37322;&#65306;&#36890;&#36807;&#27010;&#24565;&#30456;&#20851;&#20256;&#25773;
&lt;/p&gt;
&lt;p&gt;
From Attribution Maps to Human-Understandable Explanations through Concept Relevance Propagation. (arXiv:2206.03208v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.03208
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#27010;&#24565;&#30456;&#20851;&#20256;&#25773;&#65288;CRP&#65289;&#26041;&#27861;&#65292;&#23558;&#23616;&#37096;&#21644;&#20840;&#23616;&#35266;&#28857;&#32467;&#21512;&#36215;&#26469;&#65292;&#20026;&#20010;&#21035;&#39044;&#27979;&#25552;&#20379;&#20102;&#8220;&#20309;&#22320;&#8221;&#21644;&#8220;&#20309;&#29289;&#8221;&#20004;&#20010;&#38382;&#39064;&#30340;&#35299;&#31572;&#12290;&#35813;&#26041;&#27861;&#25552;&#20379;&#20102;&#26356;&#20154;&#31867;&#21487;&#35299;&#37322;&#30340;&#35299;&#37322;&#65292;&#24182;&#36890;&#36807;&#27010;&#24565;&#22270;&#35889;&#28145;&#20837;&#20102;&#35299;&#27169;&#22411;&#30340;&#34920;&#31034;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#39046;&#22495;&#26088;&#22312;&#20351;&#24403;&#20170;&#24378;&#22823;&#20294;&#19981;&#36879;&#26126;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21464;&#24471;&#36879;&#26126;&#12290;&#32780;&#23616;&#37096;&#30340;XAI&#26041;&#27861;&#36890;&#36807;&#24402;&#22240;&#22270;&#35299;&#37322;&#20010;&#21035;&#39044;&#27979;&#65292;&#20174;&#32780;&#30830;&#23450;&#37325;&#35201;&#29305;&#24449;&#20986;&#29616;&#30340;&#20301;&#32622;&#65288;&#20294;&#19981;&#25552;&#20379;&#26377;&#20851;&#23427;&#20204;&#20195;&#34920;&#20160;&#20040;&#30340;&#20449;&#24687;&#65289;&#65292;&#32780;&#20840;&#23616;&#35299;&#37322;&#25216;&#26415;&#21017;&#21487;&#35270;&#21270;&#27169;&#22411;&#36890;&#24120;&#23398;&#20064;&#32534;&#30721;&#30340;&#27010;&#24565;&#12290;&#22240;&#27492;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#21482;&#25552;&#20379;&#20102;&#37096;&#20998;&#27934;&#23519;&#21147;&#65292;&#24182;&#23558;&#35299;&#37322;&#27169;&#22411;&#30340;&#36127;&#25285;&#30041;&#32473;&#29992;&#25143;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#27010;&#24565;&#30456;&#20851;&#20256;&#25773;&#65288;CRP&#65289;&#26041;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#23616;&#37096;&#21644;&#20840;&#23616;&#30340;&#35266;&#28857;&#65292;&#20174;&#32780;&#33021;&#22815;&#22238;&#31572;&#20010;&#21035;&#39044;&#27979;&#30340;&#8220;&#20309;&#22320;&#8221;&#21644;&#8220;&#20309;&#29289;&#8221;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#22312;&#21508;&#31181;&#35774;&#32622;&#20013;&#30340;&#33021;&#21147;&#65292;&#23637;&#31034;&#20102;CRP&#22914;&#20309;&#25552;&#20379;&#26356;&#20855;&#20154;&#31867;&#35299;&#37322;&#24615;&#30340;&#35299;&#37322;&#65292;&#24182;&#36890;&#36807;&#27010;&#24565;&#22270;&#35889;&#28145;&#20837;&#20102;&#35299;&#27169;&#22411;&#30340;&#34920;&#31034;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of eXplainable Artificial Intelligence (XAI) aims to bring transparency to today's powerful but opaque deep learning models. While local XAI methods explain individual predictions in form of attribution maps, thereby identifying where important features occur (but not providing information about what they represent), global explanation techniques visualize what concepts a model has generally learned to encode. Both types of methods thus only provide partial insights and leave the burden of interpreting the model's reasoning to the user. In this work we introduce the Concept Relevance Propagation (CRP) approach, which combines the local and global perspectives and thus allows answering both the "where" and "what" questions for individual predictions. We demonstrate the capability of our method in various settings, showcasing that CRP leads to more human interpretable explanations and provides deep insights into the model's representation and reasoning through concept atlases, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25299;&#23637;&#20102;&#33041;&#30005;&#22270;-&#25991;&#26412;&#35299;&#30721;&#30340;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#24320;&#25918;&#35789;&#27719;&#30340;&#35299;&#30721;&#21644;&#38646;-shot&#24773;&#24863;&#20998;&#31867;&#12290;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#35299;&#30721;&#21644;&#24773;&#24863;&#20998;&#31867;&#20219;&#21153;&#19978;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2112.02690</link><description>&lt;p&gt;
&#24320;&#25918;&#35789;&#27719;&#30340;&#33041;&#30005;&#22270;-&#25991;&#26412;&#35299;&#30721;&#19982;&#38646;-shot&#24773;&#24863;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Open Vocabulary Electroencephalography-To-Text Decoding and Zero-shot Sentiment Classification. (arXiv:2112.02690v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.02690
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25299;&#23637;&#20102;&#33041;&#30005;&#22270;-&#25991;&#26412;&#35299;&#30721;&#30340;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#24320;&#25918;&#35789;&#27719;&#30340;&#35299;&#30721;&#21644;&#38646;-shot&#24773;&#24863;&#20998;&#31867;&#12290;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#35299;&#30721;&#21644;&#24773;&#24863;&#20998;&#31867;&#20219;&#21153;&#19978;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#26032;&#30340;&#33041;&#30005;&#22270;-&#25991;&#26412;&#31995;&#32479;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#20174;&#33041;&#20449;&#21495;&#20013;&#30452;&#25509;&#35299;&#30721;&#35821;&#35328;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#20165;&#38480;&#20110;&#23567;&#22411;&#23553;&#38381;&#30340;&#35789;&#27719;&#34920;&#65292;&#36825;&#23545;&#20110;&#33258;&#28982;&#20132;&#27969;&#26469;&#35828;&#36828;&#36828;&#19981;&#22815;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#39640;&#24615;&#33021;&#26041;&#27861;&#38656;&#35201;&#26469;&#33258;&#20405;&#20837;&#24615;&#35774;&#22791;&#65288;&#22914;ECoG&#65289;&#30340;&#25968;&#25454;&#12290;&#26412;&#25991;&#23558;&#38382;&#39064;&#25193;&#23637;&#21040;&#24320;&#25918;&#35789;&#27719;&#30340;&#33041;&#30005;&#22270;-&#25991;&#26412;&#24207;&#21015;&#21040;&#24207;&#21015;&#35299;&#30721;&#21644;&#33258;&#28982;&#38405;&#35835;&#20219;&#21153;&#30340;&#38646;-shot&#21477;&#23376;&#24773;&#24863;&#20998;&#31867;&#12290;&#25105;&#20204;&#20551;&#35774;&#20154;&#33041;&#21487;&#20197;&#20316;&#20026;&#19968;&#20010;&#29305;&#27530;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;BART&#65289;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#33041;&#30005;&#22270;-&#25991;&#26412;&#35299;&#30721;&#26041;&#38754;&#21462;&#24471;&#20102;40.1%&#30340;BLEU-1&#24471;&#20998;&#65292;&#22312;&#22522;&#20110;&#33041;&#30005;&#22270;&#30340;&#38646;-shot&#19977;&#20803;&#24773;&#24863;&#20998;&#31867;&#26041;&#38754;&#21462;&#24471;&#20102;55.6%&#30340;F1&#24471;&#20998;&#65292;&#26174;&#33879;&#20248;&#20110;&#26377;&#30417;&#30563;&#30340;&#22522;&#32447;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#21487;&#20197;&#22788;&#29702;&#26469;&#33258;&#19981;&#21516;&#34987;&#35797;&#21644;&#26469;&#28304;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art brain-to-text systems have achieved great success in decoding language directly from brain signals using neural networks. However, current approaches are limited to small closed vocabularies which are far from enough for natural communication. In addition, most of the high-performing approaches require data from invasive devices (e.g., ECoG). In this paper, we extend the problem to open vocabulary Electroencephalography(EEG)-To-Text Sequence-To-Sequence decoding and zero-shot sentence sentiment classification on natural reading tasks. We hypothesis that the human brain functions as a special text encoder and propose a novel framework leveraging pre-trained language models (e.g., BART). Our model achieves a 40.1% BLEU-1 score on EEG-To-Text decoding and a 55.6% F1 score on zero-shot EEG-based ternary sentiment classification, which significantly outperforms supervised baselines. Furthermore, we show that our proposed model can handle data from various subjects and sourc
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#36923;&#36753;&#35780;&#20272;&#20844;&#24335;&#65288;LAF&#65289;&#65292;&#36890;&#36807;&#36923;&#36753;&#25512;&#29702;&#25581;&#31034;&#20102;&#20854;&#22312;&#20855;&#26377;&#19981;&#20934;&#30830;&#23454;&#38469;&#26631;&#31614;&#30340;&#35780;&#20272;&#20013;&#30340;&#21407;&#21017;&#12290;LAF&#21487;&#20197;&#22312;&#22256;&#38590;&#30340;&#20219;&#21153;&#20013;&#24212;&#29992;&#20110;&#20855;&#26377;&#19981;&#20934;&#30830;&#23454;&#38469;&#26631;&#31614;&#30340;&#35780;&#20272;&#65292;&#24182;&#21512;&#29702;&#24037;&#20316;&#65307;&#20063;&#21487;&#20197;&#20174;&#36923;&#36753;&#35282;&#24230;&#24212;&#29992;&#20110;&#31616;&#21333;&#30340;&#20219;&#21153;&#20013;&#65292;&#20294;&#26080;&#27861;&#20687;&#24120;&#35268;&#31574;&#30053;&#19968;&#26679;&#33258;&#20449;&#24037;&#20316;&#12290;</title><link>http://arxiv.org/abs/2110.11567</link><description>&lt;p&gt;
&#35780;&#20272;&#20855;&#26377;&#19981;&#20934;&#30830;&#22320;&#23454;&#38469;&#26631;&#31614;&#30340;&#26041;&#27861;&#30340;&#36923;&#36753;&#35780;&#20272;&#20844;&#24335;&#21450;&#20854;&#21407;&#21017;
&lt;/p&gt;
&lt;p&gt;
Logical Assessment Formula and Its Principles for Evaluations with Inaccurate Ground-Truth Labels. (arXiv:2110.11567v4 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.11567
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#36923;&#36753;&#35780;&#20272;&#20844;&#24335;&#65288;LAF&#65289;&#65292;&#36890;&#36807;&#36923;&#36753;&#25512;&#29702;&#25581;&#31034;&#20102;&#20854;&#22312;&#20855;&#26377;&#19981;&#20934;&#30830;&#23454;&#38469;&#26631;&#31614;&#30340;&#35780;&#20272;&#20013;&#30340;&#21407;&#21017;&#12290;LAF&#21487;&#20197;&#22312;&#22256;&#38590;&#30340;&#20219;&#21153;&#20013;&#24212;&#29992;&#20110;&#20855;&#26377;&#19981;&#20934;&#30830;&#23454;&#38469;&#26631;&#31614;&#30340;&#35780;&#20272;&#65292;&#24182;&#21512;&#29702;&#24037;&#20316;&#65307;&#20063;&#21487;&#20197;&#20174;&#36923;&#36753;&#35282;&#24230;&#24212;&#29992;&#20110;&#31616;&#21333;&#30340;&#20219;&#21153;&#20013;&#65292;&#20294;&#26080;&#27861;&#20687;&#24120;&#35268;&#31574;&#30053;&#19968;&#26679;&#33258;&#20449;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#30340;&#39044;&#27979;&#27169;&#22411;&#35780;&#20272;&#20013;&#65292;&#24120;&#24120;&#20351;&#29992;&#20855;&#26377;&#20934;&#30830;&#23454;&#38469;&#26631;&#31614;&#30340;&#35780;&#20272;&#26469;&#36827;&#34892;&#12290;&#28982;&#32780;&#65292;&#22312;&#26576;&#20123;&#29305;&#23450;&#39046;&#22495;&#65292;&#20363;&#22914;&#21307;&#23398;&#32452;&#32455;&#30149;&#29702;&#23398;&#20840;&#20999;&#29255;&#22270;&#20687;&#20998;&#26512;&#20013;&#65292;&#20934;&#30830;&#23454;&#38469;&#26631;&#31614;&#30340;&#23450;&#20041;&#24448;&#24448;&#22256;&#38590;&#65292;&#29978;&#33267;&#19981;&#23384;&#22312;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#31181;&#24773;&#20917;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36923;&#36753;&#35780;&#20272;&#20844;&#24335;&#65288;LAF&#65289;&#65292;&#24182;&#36890;&#36807;&#19981;&#30830;&#23450;&#29366;&#24577;&#19979;&#30340;&#36923;&#36753;&#25512;&#29702;&#25581;&#31034;&#20102;&#20854;&#22312;&#20855;&#26377;&#19981;&#20934;&#30830;&#23454;&#38469;&#26631;&#31614;&#30340;&#35780;&#20272;&#20013;&#30340;&#21407;&#21017;&#12290;&#26681;&#25454;LAF&#30340;&#21407;&#21017;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;LAF&#30340;&#23454;&#29992;&#24615;&#65306;1&#65289;LAF&#21487;&#24212;&#29992;&#20110;&#26356;&#22256;&#38590;&#30340;&#12289;&#20855;&#26377;&#19981;&#20934;&#30830;&#23454;&#38469;&#26631;&#31614;&#30340;&#35780;&#20272;&#20219;&#21153;&#20013;&#65292;&#24182;&#33021;&#20687;&#24120;&#35268;&#31574;&#30053;&#19968;&#26679;&#21512;&#29702;&#22320;&#24037;&#20316;&#65307;2&#65289;LAF&#21487;&#20174;&#36923;&#36753;&#35282;&#24230;&#24212;&#29992;&#20110;&#26356;&#31616;&#21333;&#30340;&#12289;&#20855;&#26377;&#19981;&#20934;&#30830;&#23454;&#38469;&#26631;&#31614;&#30340;&#35780;&#20272;&#20219;&#21153;&#20013;&#65292;&#20294;&#26080;&#27861;&#20687;&#24120;&#35268;&#31574;&#30053;&#19968;&#26679;&#33258;&#20449;&#22320;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluations with accurate ground-truth labels (AGTLs) have been widely employed to assess predictive models for artificial intelligence applications. However, in some specific fields, such as medical histopathology whole slide image analysis, it is quite usual the situation that AGTLs are difficult to be precisely defined or even do not exist. To alleviate this situation, we propose logical assessment formula (LAF) and reveal its principles for evaluations with inaccurate ground-truth labels (IAGTLs) via logical reasoning under uncertainty. From the revealed principles of LAF, we summarize the practicability of LAF: 1) LAF can be applied for evaluations with IAGTLs on a more difficult task, able to act like usual strategies for evaluations with AGTLs reasonably; 2) LAF can be applied for evaluations with IAGTLs from the logical perspective on an easier task, unable to act like usual strategies for evaluations with AGTLs confidently.
&lt;/p&gt;</description></item><item><title>INVIGORATE&#26159;&#19968;&#20010;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#19982;&#20154;&#31867;&#36827;&#34892;&#20132;&#20114;&#24182;&#22312;&#26434;&#20081;&#29615;&#22659;&#20013;&#25235;&#21462;&#25351;&#23450;&#29289;&#20307;&#30340;&#26426;&#22120;&#20154;&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#33021;&#22815;&#25512;&#26029;&#30446;&#26631;&#29289;&#20307;&#12289;&#25512;&#26029;&#29289;&#20307;&#38459;&#25377;&#20851;&#31995;&#65292;&#24182;&#29983;&#25104;&#22810;&#27493;&#35745;&#21010;&#26469;&#28040;&#38500;&#27495;&#20041;&#24182;&#25104;&#21151;&#25235;&#21462;&#30446;&#26631;&#29289;&#20307;&#12290;</title><link>http://arxiv.org/abs/2108.11092</link><description>&lt;p&gt;
INVIGORATE: &#20114;&#21160;&#35270;&#35273;&#22330;&#26223;&#29702;&#35299;&#21644;&#25235;&#21462;
&lt;/p&gt;
&lt;p&gt;
INVIGORATE: Interactive Visual Grounding and Grasping in Clutter. (arXiv:2108.11092v1 [cs.RO] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.11092
&lt;/p&gt;
&lt;p&gt;
INVIGORATE&#26159;&#19968;&#20010;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#19982;&#20154;&#31867;&#36827;&#34892;&#20132;&#20114;&#24182;&#22312;&#26434;&#20081;&#29615;&#22659;&#20013;&#25235;&#21462;&#25351;&#23450;&#29289;&#20307;&#30340;&#26426;&#22120;&#20154;&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#33021;&#22815;&#25512;&#26029;&#30446;&#26631;&#29289;&#20307;&#12289;&#25512;&#26029;&#29289;&#20307;&#38459;&#25377;&#20851;&#31995;&#65292;&#24182;&#29983;&#25104;&#22810;&#27493;&#35745;&#21010;&#26469;&#28040;&#38500;&#27495;&#20041;&#24182;&#25104;&#21151;&#25235;&#21462;&#30446;&#26631;&#29289;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;INVIGORATE&#65292;&#19968;&#20010;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#19982;&#20154;&#31867;&#36827;&#34892;&#20132;&#20114;&#24182;&#22312;&#26434;&#20081;&#29615;&#22659;&#20013;&#25235;&#21462;&#25351;&#23450;&#29289;&#20307;&#30340;&#26426;&#22120;&#20154;&#31995;&#32479;&#12290;&#22312;&#36825;&#31181;&#26434;&#20081;&#29615;&#22659;&#20013;&#65292;&#29289;&#20307;&#21487;&#33021;&#20250;&#30456;&#20114;&#36974;&#25377;&#12289;&#38459;&#25377;&#29978;&#33267;&#21472;&#25918;&#22312;&#19968;&#36215;&#12290;INVIGORATE&#38754;&#20020;&#30528;&#20960;&#20010;&#25361;&#25112;&#65306;&#65288;i&#65289;&#20174;&#36755;&#20837;&#30340;&#35821;&#35328;&#34920;&#36798;&#21644;RGB&#22270;&#20687;&#20013;&#25512;&#26029;&#20986;&#30446;&#26631;&#29289;&#20307;&#65292;&#32780;&#24573;&#30053;&#20854;&#20182;&#36974;&#25377;&#29289;&#20307;&#65307;&#65288;ii&#65289;&#20174;&#22270;&#20687;&#20013;&#25512;&#26029;&#20986;&#29289;&#20307;&#30340;&#38459;&#25377;&#20851;&#31995;&#65288;OBR&#65289;&#65307;&#65288;iii&#65289;&#29983;&#25104;&#19968;&#20010;&#22810;&#27493;&#35745;&#21010;&#65292;&#36890;&#36807;&#25552;&#38382;&#28040;&#38500;&#30446;&#26631;&#29289;&#20307;&#30340;&#27495;&#20041;&#24182;&#25104;&#21151;&#25235;&#21462;&#23427;&#12290;&#25105;&#20204;&#20026;&#30446;&#26631;&#26816;&#27979;&#12289;&#35270;&#35273;&#22330;&#26223;&#29702;&#35299;&#12289;&#38382;&#39064;&#29983;&#25104;&#21644;OBR&#26816;&#27979;&#20197;&#21450;&#25235;&#21462;&#35757;&#32451;&#20102;&#29420;&#31435;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#23427;&#20204;&#21487;&#20197;&#22788;&#29702;&#20219;&#24847;&#30340;&#29289;&#20307;&#31867;&#21035;&#21644;&#35821;&#35328;&#34920;&#36798;&#65292;&#21482;&#35201;&#26377;&#30456;&#24212;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#35270;&#35273;&#24863;&#30693;&#20013;&#30340;&#35823;&#24046;&#20197;&#21450;&#20154;&#31867;&#35821;&#35328;&#20013;&#30340;&#27495;&#20041;&#19981;&#21487;&#36991;&#20813;&#22320;&#20250;&#23545;&#26426;&#22120;&#20154;&#30340;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#19981;&#30830;&#23450;&#24615;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents INVIGORATE, a robot system that interacts with human through natural language and grasps a specified object in clutter. The objects may occlude, obstruct, or even stack on top of one another. INVIGORATE embodies several challenges: (i) infer the target object among other occluding objects, from input language expressions and RGB images, (ii) infer object blocking relationships (OBRs) from the images, and (iii) synthesize a multi-step plan to ask questions that disambiguate the target object and to grasp it successfully. We train separate neural networks for object detection, for visual grounding, for question generation, and for OBR detection and grasping. They allow for unrestricted object categories and language expressions, subject to the training datasets. However, errors in visual perception and ambiguity in human languages are inevitable and negatively impact the robot's performance. To overcome these uncertainties, we build a partially observable Markov decis
&lt;/p&gt;</description></item></channel></rss>