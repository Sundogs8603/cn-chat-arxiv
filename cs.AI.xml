<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>POMP&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#21160;&#24577;&#30340;&#12289;&#22522;&#20110;&#25277;&#26679;&#30340;&#22810;&#36741;&#21161;&#35821;&#35328;&#22270;&#24418;&#65292;&#25552;&#39640;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#30340;&#32763;&#35793;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.05596</link><description>&lt;p&gt;
POMP:&#29992;&#20110;&#20302;&#36164;&#28304;&#26080;&#30417;&#30563;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#27010;&#29575;&#39537;&#21160;&#20803;&#22270;&#25552;&#31034;&#22120;
&lt;/p&gt;
&lt;p&gt;
POMP: Probability-driven Meta-graph Prompter for LLMs in Low-resource Unsupervised Neural Machine Translation. (arXiv:2401.05596v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05596
&lt;/p&gt;
&lt;p&gt;
POMP&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#21160;&#24577;&#30340;&#12289;&#22522;&#20110;&#25277;&#26679;&#30340;&#22810;&#36741;&#21161;&#35821;&#35328;&#22270;&#24418;&#65292;&#25552;&#39640;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#30340;&#32763;&#35793;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#36164;&#28304;&#35821;&#35328;&#22312;&#26377;&#38480;&#30340;&#24179;&#34892;&#25968;&#25454;&#19979;&#38754;&#20020;&#30528;&#22312;&#30417;&#30563;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#25361;&#25112;&#65292;&#22240;&#27492;&#30740;&#31350;&#26080;&#30417;&#30563;&#26041;&#27861;&#12290;&#26080;&#30417;&#30563;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#26041;&#27861;&#65292;&#21253;&#25324;&#21453;&#21521;&#32763;&#35793;&#12289;&#36801;&#31227;&#23398;&#20064;&#21644;&#22522;&#20110;&#26530;&#36724;&#30340;&#32763;&#35793;&#65292;&#20026;&#20302;&#36164;&#28304;&#35821;&#35328;&#32763;&#35793;&#25552;&#20379;&#20102;&#23454;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#26159;&#23427;&#20204;&#21463;&#21040;&#21512;&#25104;&#25968;&#25454;&#22122;&#22768;&#12289;&#35821;&#35328;&#20559;&#24046;&#21644;&#38169;&#35823;&#20256;&#25773;&#31561;&#38382;&#39064;&#30340;&#24433;&#21709;&#65292;&#36825;&#20123;&#38382;&#39064;&#21487;&#20197;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32531;&#35299;&#12290;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#26377;&#30417;&#30563;&#24494;&#35843;&#26041;&#27861;&#25913;&#36827;&#20102;NMT&#65292;&#20294;&#26159;&#35757;&#32451;&#25968;&#25454;&#19981;&#36275;&#20351;&#24471;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#19978;&#30340;&#24615;&#33021;&#36739;&#24046;&#12290;&#25105;&#20204;&#35748;&#20026;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#36741;&#21161;&#35821;&#35328;&#20943;&#23569;&#35821;&#35328;&#22122;&#22768;&#65292;&#25552;&#39640;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#32763;&#35793;&#36136;&#37327;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;POMP&#30340;&#27010;&#29575;&#39537;&#21160;&#20803;&#22270;&#25552;&#31034;&#22120;&#65292;&#23427;&#37319;&#29992;&#20102;&#22522;&#20110;&#21160;&#24577;&#25277;&#26679;&#30340;&#22810;&#20010;&#36741;&#21161;&#35821;&#35328;&#30340;&#22270;&#24418;&#65292;&#20197;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#19978;&#30340;&#32763;&#35793;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Low-resource languages (LRLs) face challenges in supervised neural machine translation due to limited parallel data, prompting research into unsupervised methods. Unsupervised neural machine translation (UNMT) methods, including back-translation, transfer learning, and pivot-based translation, offer practical solutions for LRL translation, but they are hindered by issues like synthetic data noise, language bias, and error propagation, which can potentially be mitigated by Large Language Models (LLMs). LLMs have advanced NMT with in-context learning (ICL) and supervised fine-tuning methods, but insufficient training data results in poor performance in LRLs. We argue that LLMs can mitigate the linguistic noise with auxiliary languages to improve translations in LRLs. In this paper, we propose Probability-driven Meta-graph Prompter (POMP), a novel approach employing a dynamic, sampling-based graph of multiple auxiliary languages to enhance LLMs' translation capabilities for LRLs. POMP inv
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#25512;&#29702;&#27493;&#38271;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#22312;&#25552;&#31034;&#20013;&#22686;&#21152;&#25512;&#29702;&#27493;&#39588;&#33021;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#32780;&#20943;&#23569;&#25512;&#29702;&#27493;&#39588;&#21017;&#20250;&#38477;&#20302;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.04925</link><description>&lt;p&gt;
&#25512;&#29702;&#27493;&#38271;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The Impact of Reasoning Step Length on Large Language Models. (arXiv:2401.04925v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04925
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#25512;&#29702;&#27493;&#38271;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#22312;&#25552;&#31034;&#20013;&#22686;&#21152;&#25512;&#29702;&#27493;&#39588;&#33021;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#32780;&#20943;&#23569;&#25512;&#29702;&#27493;&#39588;&#21017;&#20250;&#38477;&#20302;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24605;&#32500;&#38142;&#26465;&#65288;CoT&#65289;&#23545;&#20110;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;CoT&#30340;&#26377;&#25928;&#24615;&#19982;&#25552;&#31034;&#20013;&#25512;&#29702;&#27493;&#39588;&#30340;&#38271;&#24230;&#20043;&#38388;&#30340;&#20851;&#31995;&#20173;&#28982;&#19981;&#20026;&#20154;&#25152;&#30693;&#12290;&#20026;&#20102;&#25581;&#31034;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20960;&#20010;&#23454;&#35777;&#23454;&#39564;&#26469;&#25506;&#32034;&#36825;&#20123;&#20851;&#31995;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20123;&#23454;&#39564;&#65292;&#25193;&#23637;&#21644;&#21387;&#32553;CoT&#28436;&#31034;&#20013;&#30340;&#21512;&#29702;&#25512;&#29702;&#27493;&#39588;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#20182;&#22240;&#32032;&#19981;&#21464;&#12290;&#25105;&#20204;&#24471;&#20986;&#20102;&#20197;&#19979;&#20027;&#35201;&#21457;&#29616;&#12290;&#39318;&#20808;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25552;&#31034;&#20013;&#24310;&#38271;&#25512;&#29702;&#27493;&#39588;&#65292;&#21363;&#20351;&#27809;&#26377;&#21521;&#25552;&#31034;&#20013;&#28155;&#21152;&#26032;&#20449;&#24687;&#65292;&#20063;&#20250;&#26174;&#33879;&#25552;&#39640;LLM&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#30456;&#21453;&#65292;&#32553;&#30701;&#25512;&#29702;&#27493;&#39588;&#65292;&#21363;&#20351;&#20445;&#30041;&#20851;&#38190;&#20449;&#24687;&#65292;&#20063;&#20250;&#26174;&#33879;&#38477;&#20302;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#36825;&#19968;&#21457;&#29616;&#31361;&#26174;&#20102;CoT&#25552;&#31034;&#20013;&#27493;&#39588;&#25968;&#37327;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#23454;&#38469;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chain of Thought (CoT) is significant in improving the reasoning abilities of large language models (LLMs). However, the correlation between the effectiveness of CoT and the length of reasoning steps in prompts remains largely unknown. To shed light on this, we have conducted several empirical experiments to explore the relations. Specifically, we design experiments that expand and compress the rationale reasoning steps within CoT demonstrations, while keeping all other factors constant. We have the following key findings. First, the results indicate that lengthening the reasoning steps in prompts, even without adding new information into the prompt, considerably enhances LLMs' reasoning abilities across multiple datasets. Alternatively, shortening the reasoning steps, even while preserving the key information, significantly diminishes the reasoning abilities of models. This finding highlights the importance of the number of steps in CoT prompts and provides practical guidance to make 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#26234;&#33021;&#20307;&#21464;&#24471;&#26234;&#33021;&#30340;&#22240;&#32032;&#65292;&#24378;&#35843;&#20102;&#25484;&#25569;&#29305;&#24449;&#21644;&#25511;&#21046;&#22810;&#20010;&#20445;&#23432;&#30456;&#20114;&#20316;&#29992;&#30340;&#23376;&#31995;&#32479;&#30340;&#33021;&#21147;&#12290;&#26234;&#33021;&#30340;&#26680;&#24515;&#26159;&#8220;&#38598;&#20307;&#22914;&#19968;&#20307;&#8221;&#21644;&#8220;&#20102;&#35299;&#23616;&#37096;&#34892;&#21160;&#30340;&#25972;&#20307;&#32467;&#26524;&#8221;&#12290;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#38598;&#20307;&#20445;&#23432;&#31995;&#32479;&#36827;&#34892;&#25511;&#21046;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.04846</link><description>&lt;p&gt;
&#21463;&#36807;&#33391;&#22909;&#25945;&#32946;&#30340;&#26234;&#33021;&#30340;&#20869;&#22312;&#21892;&#33391;
&lt;/p&gt;
&lt;p&gt;
The inherent goodness of well educated intelligence. (arXiv:2401.04846v1 [econ.TH])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04846
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#26234;&#33021;&#20307;&#21464;&#24471;&#26234;&#33021;&#30340;&#22240;&#32032;&#65292;&#24378;&#35843;&#20102;&#25484;&#25569;&#29305;&#24449;&#21644;&#25511;&#21046;&#22810;&#20010;&#20445;&#23432;&#30456;&#20114;&#20316;&#29992;&#30340;&#23376;&#31995;&#32479;&#30340;&#33021;&#21147;&#12290;&#26234;&#33021;&#30340;&#26680;&#24515;&#26159;&#8220;&#38598;&#20307;&#22914;&#19968;&#20307;&#8221;&#21644;&#8220;&#20102;&#35299;&#23616;&#37096;&#34892;&#21160;&#30340;&#25972;&#20307;&#32467;&#26524;&#8221;&#12290;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#38598;&#20307;&#20445;&#23432;&#31995;&#32479;&#36827;&#34892;&#25511;&#21046;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#25506;&#35752;&#20351;&#19968;&#20010;&#26234;&#33021;&#20307;&#21464;&#24471;&#26234;&#33021;&#30340;&#22240;&#32032;&#65292;&#26080;&#35770;&#26159;&#29983;&#29289;&#20307;&#36824;&#26159;&#35745;&#31639;&#26426;&#19978;&#30340;&#20154;&#24037;&#26234;&#33021;&#12290;&#29305;&#21035;&#20851;&#27880;&#30340;&#26159;&#33021;&#22815;&#34920;&#24449;&#21644;&#25511;&#21046;&#22810;&#20010;&#20445;&#23432;&#30456;&#20114;&#20316;&#29992;&#30340;&#30456;&#21516;&#23376;&#31995;&#32479;&#30340;&#33021;&#21147;&#12290;&#26234;&#33021;&#30340;&#26412;&#36136;&#23558;&#34987;&#21457;&#29616;&#26159;&#40644;&#37329;&#27861;&#21017;&#8212;&#8212;&#8220;&#38598;&#20307;&#34892;&#21160;&#22914;&#19968;&#20307;&#8221;&#25110;&#8220;&#20102;&#35299;&#23616;&#37096;&#34892;&#21160;&#30340;&#25972;&#20307;&#32467;&#26524;&#8221;&#12290;&#38598;&#20307;&#30340;&#27969;&#21160;&#26159;&#30001;&#25484;&#25511;&#30528;&#23569;&#37327;&#23383;&#31526;&#20018;&#30340;&#25805;&#32437;&#32773;&#20915;&#23450;&#30340;&#65292;&#26681;&#25454;&#23545;&#31216;&#24615;&#30830;&#23450;&#30340;&#26368;&#23567;&#20316;&#29992;&#36335;&#24452;&#30340;&#27979;&#22320;&#32447;&#36816;&#21160;&#12290;&#25511;&#21046;&#38598;&#20307;&#20445;&#23432;&#31995;&#32479;&#26159;&#22256;&#38590;&#30340;&#65292;&#21382;&#21490;&#19978;&#19968;&#30452;&#36890;&#36807;&#20026;&#31995;&#32479;&#28155;&#21152;&#26174;&#33879;&#40655;&#24615;&#26469;&#31283;&#23450;&#26399;&#26395;&#30340;&#26368;&#22823;&#24615;&#33021;&#30340;&#20122;&#31283;&#24179;&#34913;&#29366;&#24577;&#65292;&#20294;&#36825;&#20250;&#22312;&#36807;&#31243;&#20013;&#38477;&#20302;&#25110;&#30772;&#22351;&#23427;&#20204;&#12290;&#26377;&#19968;&#31181;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper will examine what makes a being intelligent, whether that be a biological being or an artificial silicon being on a computer. Special attention will be paid to the being having the ability to characterize and control a collective system of many identical conservative sub-systems conservatively interacting. The essence of intelligence will be found to be the golden rule -- "the collective acts as one" or "knowing the global consequences of local actions". The flow of the collective is a small set of twinkling textures, that are governed by a puppeteer who is pulling a small number of strings according to a geodesic motion of least action, determined by the symmetries. Controlling collective conservative systems is difficult and has historically been done by adding significant viscosity to the system to stabilize the desirable meta stable equilibriums of maximum performance, but it degrades or destroys them in the process. There is an alternative. Once the optimum twinkling te
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37319;&#26679;&#30340;&#38750;&#20984;&#20248;&#21270;&#26041;&#27861;&#65292;&#37319;&#29992;Monte Carlo Tree Search (MCTS)&#26469;&#25552;&#39640;&#25928;&#29575;&#65292;&#24182;&#21033;&#29992;&#25968;&#20540;&#19978;&#20272;&#35745;&#30340;&#19981;&#30830;&#23450;&#24230;&#25351;&#26631;&#21644;&#37319;&#26679;&#20272;&#35745;&#30340;&#19968;&#38454;&#21644;&#20108;&#38454;&#20449;&#24687;&#65292;&#36991;&#20813;&#22266;&#23450;&#32452;&#21512;&#27169;&#24335;&#30340;&#26641;&#29983;&#38271;&#65292;&#31215;&#26497;&#32553;&#23567;&#21040;&#26377;&#24076;&#26395;&#30340;&#21306;&#22495;&#65292;&#21516;&#26102;&#24179;&#34913;&#25506;&#32034;&#21644;&#24320;&#21457;&#12290;</title><link>http://arxiv.org/abs/2401.04812</link><description>&lt;p&gt;
&#37319;&#26679;&#19982;&#26463;&#32538;&#29992;&#20110;&#38750;&#20984;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Sample-and-Bound for Non-Convex Optimization. (arXiv:2401.04812v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04812
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37319;&#26679;&#30340;&#38750;&#20984;&#20248;&#21270;&#26041;&#27861;&#65292;&#37319;&#29992;Monte Carlo Tree Search (MCTS)&#26469;&#25552;&#39640;&#25928;&#29575;&#65292;&#24182;&#21033;&#29992;&#25968;&#20540;&#19978;&#20272;&#35745;&#30340;&#19981;&#30830;&#23450;&#24230;&#25351;&#26631;&#21644;&#37319;&#26679;&#20272;&#35745;&#30340;&#19968;&#38454;&#21644;&#20108;&#38454;&#20449;&#24687;&#65292;&#36991;&#20813;&#22266;&#23450;&#32452;&#21512;&#27169;&#24335;&#30340;&#26641;&#29983;&#38271;&#65292;&#31215;&#26497;&#32553;&#23567;&#21040;&#26377;&#24076;&#26395;&#30340;&#21306;&#22495;&#65292;&#21516;&#26102;&#24179;&#34913;&#25506;&#32034;&#21644;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#38750;&#20984;&#20989;&#25968;&#30340;&#20840;&#23616;&#20248;&#21270;&#26631;&#20934;&#26041;&#27861;&#65292;&#22914;&#20998;&#25903;&#21644;&#26463;&#32538;&#65292;&#32500;&#25252;&#21487;&#29992;&#20110;&#31995;&#32479;&#21098;&#26525;&#30340;&#20998;&#21306;&#26641;&#12290;&#26641;&#30340;&#22823;&#23567;&#38543;&#32500;&#24230;&#30340;&#22686;&#21152;&#32780;&#21576;&#25351;&#25968;&#22686;&#38271;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#37319;&#26679;&#30340;&#38750;&#20984;&#20248;&#21270;&#26041;&#27861;&#65292;&#23427;&#25913;&#36827;&#20102;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;(MCTS)&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#19981;&#20877;&#20351;&#29992;&#26631;&#20934;&#30340;&#35775;&#38382;&#35745;&#25968;&#26469;&#20316;&#20026;&#19981;&#30830;&#23450;&#24230;&#25351;&#26631;&#65292;&#32780;&#26159;&#21033;&#29992;&#30446;&#26631;&#30340;&#25968;&#20540;&#19978;&#20272;&#35745;&#20316;&#20026;&#19981;&#30830;&#23450;&#24230;&#25351;&#26631;&#65292;&#24182;&#32771;&#34385;&#37319;&#26679;&#20272;&#35745;&#30340;&#19968;&#38454;&#21644;&#20108;&#38454;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#30340;&#33945;&#29305;&#21345;&#32599;&#26641;&#36991;&#20813;&#20102;&#36890;&#24120;&#22266;&#23450;&#32452;&#21512;&#27169;&#24335;&#30340;&#26641;&#29983;&#38271;&#65292;&#24182;&#31215;&#26497;&#22320;&#32553;&#23567;&#21040;&#26377;&#24076;&#26395;&#30340;&#21306;&#22495;&#65292;&#21516;&#26102;&#24179;&#34913;&#25506;&#32034;&#21644;&#24320;&#21457;&#12290;&#25105;&#20204;&#23558;&#25552;&#20986;&#30340;&#31639;&#27861;&#19982;&#31454;&#20105;&#22522;&#32447;&#22312;&#39640;&#32500;&#38750;&#20984;&#20248;&#21270;&#22522;&#20934;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#20998;&#26512;&#36229;&#21442;&#25968;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Standard approaches for global optimization of non-convex functions, such as branch-and-bound, maintain partition trees to systematically prune the domain. The tree size grows exponentially in the number of dimensions. We propose new sampling-based methods for non-convex optimization that adapts Monte Carlo Tree Search (MCTS) to improve efficiency. Instead of the standard use of visitation count in Upper Confidence Bounds, we utilize numerical overapproximations of the objective as an uncertainty metric, and also take into account of sampled estimates of first-order and second-order information. The Monte Carlo tree in our approach avoids the usual fixed combinatorial patterns in growing the tree, and aggressively zooms into the promising regions, while still balancing exploration and exploitation. We evaluate the proposed algorithms on high-dimensional non-convex optimization benchmarks against competitive baselines and analyze the effects of the hyper parameters.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Lightning Attention-2&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#23454;&#29616;&#32447;&#24615;&#27880;&#24847;&#21147;&#29702;&#35770;&#35745;&#31639;&#20248;&#21183;&#30340;&#32447;&#24615;&#27880;&#24847;&#21147;&#23454;&#29616;&#12290;&#36890;&#36807;&#21033;&#29992;&#24179;&#38138;&#30340;&#24605;&#24819;&#65292;&#20998;&#21035;&#22788;&#29702;&#20102;&#32447;&#24615;&#27880;&#24847;&#21147;&#35745;&#31639;&#20013;&#30340;&#20869;&#37096;&#22359;&#21644;&#22806;&#37096;&#22359;&#32452;&#20214;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#37319;&#29992;&#20256;&#32479;&#30340;&#27880;&#24847;&#21147;&#35745;&#31639;&#26426;&#21046;&#22788;&#29702;&#20869;&#37096;&#22359;&#65292;&#24182;&#20351;&#29992;&#26032;&#30340;&#32047;&#31215;&#27714;&#21644;&#26041;&#27861;&#22788;&#29702;&#22806;&#37096;&#22359;&#12290;</title><link>http://arxiv.org/abs/2401.04658</link><description>&lt;p&gt;
Lightning Attention-2:&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#22788;&#29702;&#26080;&#38480;&#24207;&#21015;&#38271;&#24230;&#30340;"&#20813;&#36153;&#21320;&#39184;"
&lt;/p&gt;
&lt;p&gt;
Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models. (arXiv:2401.04658v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04658
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Lightning Attention-2&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#23454;&#29616;&#32447;&#24615;&#27880;&#24847;&#21147;&#29702;&#35770;&#35745;&#31639;&#20248;&#21183;&#30340;&#32447;&#24615;&#27880;&#24847;&#21147;&#23454;&#29616;&#12290;&#36890;&#36807;&#21033;&#29992;&#24179;&#38138;&#30340;&#24605;&#24819;&#65292;&#20998;&#21035;&#22788;&#29702;&#20102;&#32447;&#24615;&#27880;&#24847;&#21147;&#35745;&#31639;&#20013;&#30340;&#20869;&#37096;&#22359;&#21644;&#22806;&#37096;&#22359;&#32452;&#20214;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#37319;&#29992;&#20256;&#32479;&#30340;&#27880;&#24847;&#21147;&#35745;&#31639;&#26426;&#21046;&#22788;&#29702;&#20869;&#37096;&#22359;&#65292;&#24182;&#20351;&#29992;&#26032;&#30340;&#32047;&#31215;&#27714;&#21644;&#26041;&#27861;&#22788;&#29702;&#22806;&#37096;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32447;&#24615;&#27880;&#24847;&#21147;&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#26368;&#36817;&#34987;&#35748;&#20026;&#26159;&#20256;&#32479;softmax&#27880;&#24847;&#21147;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#32447;&#24615;&#27880;&#24847;&#21147;&#29702;&#35770;&#19978;&#33021;&#22815;&#22312;&#32447;&#24615;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#19979;&#22788;&#29702;&#26080;&#38480;&#38271;&#24230;&#30340;&#24207;&#21015;&#65292;&#32780;&#19981;&#20250;&#29306;&#29298;&#36895;&#24230;&#65292;&#21363;&#22312;&#22266;&#23450;&#30340;&#20869;&#23384;&#28040;&#32791;&#19979;&#65292;&#33021;&#22815;&#20197;&#24658;&#23450;&#30340;&#35757;&#32451;&#36895;&#24230;&#22788;&#29702;&#19981;&#21516;&#38271;&#24230;&#30340;&#24207;&#21015;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32047;&#31215;&#27714;&#21644;&#65288;cumsum&#65289;&#30340;&#38382;&#39064;&#65292;&#24403;&#21069;&#30340;&#32447;&#24615;&#27880;&#24847;&#21147;&#31639;&#27861;&#26080;&#27861;&#22312;&#22240;&#26524;&#35774;&#32622;&#19979;&#23637;&#29616;&#20854;&#29702;&#35770;&#20248;&#21183;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Lightning Attention-2&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#23454;&#29616;&#32447;&#24615;&#27880;&#24847;&#21147;&#29702;&#35770;&#35745;&#31639;&#20248;&#21183;&#30340;&#32447;&#24615;&#27880;&#24847;&#21147;&#23454;&#29616;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#24179;&#38138;&#65288;tiling&#65289;&#30340;&#24605;&#24819;&#65292;&#20998;&#21035;&#22788;&#29702;&#20102;&#32447;&#24615;&#27880;&#24847;&#21147;&#35745;&#31639;&#20013;&#30340;&#20869;&#37096;&#22359;&#21644;&#22806;&#37096;&#22359;&#32452;&#20214;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#20256;&#32479;&#30340;&#27880;&#24847;&#21147;&#35745;&#31639;&#26426;&#21046;&#26469;&#22788;&#29702;&#20869;&#37096;&#22359;&#65292;&#28982;&#21518;&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;&#32047;&#31215;&#27714;&#21644;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#22806;&#37096;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
Linear attention is an efficient attention mechanism that has recently emerged as a promising alternative to conventional softmax attention. With its ability to process tokens in linear computational complexities, linear attention, in theory, can handle sequences of unlimited length without sacrificing speed, i.e., maintaining a constant training speed for various sequence lengths with a fixed memory consumption. However, due to the issue with cumulative summation (cumsum), current linear attention algorithms cannot demonstrate their theoretical advantage in a causal setting. In this paper, we present Lightning Attention-2, the first linear attention implementation that enables linear attention to realize its theoretical computational benefits. To achieve this, we leverage the thought of tiling, separately handling the intra-block and inter-block components in linear attention calculation. Specifically, we utilize the conventional attention computation mechanism for the intra-blocks an
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#35299;&#21078;&#22810;&#35270;&#22270;&#25968;&#25454;&#39044;&#27979;&#38750;&#25104;&#20687;&#34920;&#22411;&#30340;&#28145;&#24230;&#32593;&#32476;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#22810;&#35270;&#22270;&#32593;&#32476;&#65288;EMV-Net&#65289;&#65292;&#36890;&#36807;&#34701;&#21512;&#19981;&#21516;&#35299;&#21078;&#35270;&#22270;&#26469;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.04579</link><description>&lt;p&gt;
&#20351;&#29992;&#35299;&#37322;&#22411;&#22810;&#35270;&#22270;&#25968;&#25454;&#39044;&#27979;&#38750;&#25104;&#20687;&#34920;&#22411;&#30340;&#28145;&#24230;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
A Deep Network for Explainable Prediction of Non-Imaging Phenotypes using Anatomical Multi-View Data. (arXiv:2401.04579v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04579
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#35299;&#21078;&#22810;&#35270;&#22270;&#25968;&#25454;&#39044;&#27979;&#38750;&#25104;&#20687;&#34920;&#22411;&#30340;&#28145;&#24230;&#32593;&#32476;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#22810;&#35270;&#22270;&#32593;&#32476;&#65288;EMV-Net&#65289;&#65292;&#36890;&#36807;&#34701;&#21512;&#19981;&#21516;&#35299;&#21078;&#35270;&#22270;&#26469;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#25968;&#25454;&#38598;&#36890;&#24120;&#21253;&#21547;&#22810;&#20010;&#19981;&#21516;&#30340;&#29305;&#24449;&#38598;&#25110;&#35270;&#22270;&#65292;&#36825;&#20123;&#35270;&#22270;&#25552;&#20379;&#20102;&#20114;&#34917;&#20449;&#24687;&#65292;&#21487;&#20197;&#36890;&#36807;&#22810;&#35270;&#22270;&#23398;&#20064;&#26041;&#27861;&#21033;&#29992;&#23427;&#20204;&#26469;&#25552;&#39640;&#32467;&#26524;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#35299;&#21078;&#22810;&#35270;&#22270;&#25968;&#25454;&#65292;&#20854;&#20013;&#27599;&#20010;&#33041;&#35299;&#21078;&#32467;&#26500;&#29992;&#22810;&#20010;&#29305;&#24449;&#38598;&#25551;&#36848;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#20851;&#27880;&#26469;&#33258;&#25193;&#25955;MR&#65288;diffusion MRI&#65289;&#30340;&#30333;&#36136;&#24494;&#32467;&#26500;&#21644;&#36830;&#25509;&#29305;&#24449;&#38598;&#65292;&#20197;&#21450;&#26469;&#33258;&#32467;&#26500;MR&#65288;structural MRI&#65289;&#30340;&#28784;&#36136;&#38754;&#31215;&#21644;&#21402;&#24230;&#29305;&#24449;&#38598;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#24212;&#29992;&#22810;&#35270;&#22270;&#26041;&#27861;&#25913;&#36827;&#38750;&#25104;&#20687;&#34920;&#22411;&#65288;&#21253;&#25324;&#20154;&#21475;&#32479;&#35745;&#23398;&#29305;&#24449;&#65288;&#24180;&#40836;&#65289;&#12289;&#36816;&#21160;&#65288;&#21147;&#37327;&#65289;&#21644;&#35748;&#30693;&#65288;&#35789;&#27719;&#22270;&#20687;&#65289;&#65289;&#39044;&#27979;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#22810;&#35270;&#22270;&#32593;&#32476;&#65288;EMV-Net&#65289;&#65292;&#21487;&#20197;&#21033;&#29992;&#19981;&#21516;&#30340;&#35299;&#21078;&#35270;&#22270;&#26469;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;&#22312;&#36825;&#20010;&#32593;&#32476;&#20013;&#65292;&#27599;&#20010;&#20010;&#20307;&#35299;&#21078;&#35270;&#22270;&#37117;&#32463;&#36807;&#35270;&#22270;&#29305;&#23450;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#22788;&#29702;&#65292;&#28982;&#21518;&#23558;&#27599;&#20010;&#35270;&#22270;&#25552;&#21462;&#30340;&#20449;&#24687;&#34701;&#21512;&#22312;&#19968;&#36215;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large datasets often contain multiple distinct feature sets, or views, that offer complementary information that can be exploited by multi-view learning methods to improve results. We investigate anatomical multi-view data, where each brain anatomical structure is described with multiple feature sets. In particular, we focus on sets of white matter microstructure and connectivity features from diffusion MRI, as well as sets of gray matter area and thickness features from structural MRI. We investigate machine learning methodology that applies multi-view approaches to improve the prediction of non-imaging phenotypes, including demographics (age), motor (strength), and cognition (picture vocabulary). We present an explainable multi-view network (EMV-Net) that can use different anatomical views to improve prediction performance. In this network, each individual anatomical view is processed by a view-specific feature extractor and the extracted information from each view is fused using a l
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#25968;&#25454;&#20026;&#20013;&#24515;&#8221;&#30340;&#35270;&#35282;&#65292;&#25506;&#35752;&#20102;&#25968;&#25454;&#25910;&#38598;&#12289;&#22788;&#29702;&#21644;&#20998;&#26512;&#22312;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#20316;&#29992;&#12290;&#30740;&#31350;&#23558;&#29616;&#26377;&#24037;&#20316;&#20998;&#20026;&#19977;&#20010;&#31867;&#21035;&#65306;&#28145;&#24230;&#27169;&#22411;&#30340;&#35299;&#37322;&#12289;&#35757;&#32451;&#25968;&#25454;&#30340;&#24433;&#21709;&#21644;&#39046;&#22495;&#30693;&#35782;&#30340;&#35265;&#35299;&#12290;&#36890;&#36807;&#25968;&#25454;&#25366;&#25496;&#25805;&#20316;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#36825;&#20123;XAI&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.04374</link><description>&lt;p&gt;
&#36808;&#21521;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#65306;&#19968;&#20010;&#25968;&#25454;&#25366;&#25496;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Towards Explainable Artificial Intelligence (XAI): A Data Mining Perspective. (arXiv:2401.04374v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04374
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#25968;&#25454;&#20026;&#20013;&#24515;&#8221;&#30340;&#35270;&#35282;&#65292;&#25506;&#35752;&#20102;&#25968;&#25454;&#25910;&#38598;&#12289;&#22788;&#29702;&#21644;&#20998;&#26512;&#22312;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#20316;&#29992;&#12290;&#30740;&#31350;&#23558;&#29616;&#26377;&#24037;&#20316;&#20998;&#20026;&#19977;&#20010;&#31867;&#21035;&#65306;&#28145;&#24230;&#27169;&#22411;&#30340;&#35299;&#37322;&#12289;&#35757;&#32451;&#25968;&#25454;&#30340;&#24433;&#21709;&#21644;&#39046;&#22495;&#30693;&#35782;&#30340;&#35265;&#35299;&#12290;&#36890;&#36807;&#25968;&#25454;&#25366;&#25496;&#25805;&#20316;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#36825;&#20123;XAI&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#22797;&#26434;&#24615;&#21644;&#36879;&#26126;&#24230;&#19981;&#36275;&#65292;&#20154;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#21162;&#21147;&#65292;&#20197;&#20351;&#36825;&#20123;&#31995;&#32479;&#26356;&#20855;&#35299;&#37322;&#24615;&#25110;&#22312;&#21487;&#35775;&#38382;&#30340;&#26415;&#35821;&#20013;&#35299;&#37322;&#20854;&#34892;&#20026;&#12290;&#19982;&#22823;&#22810;&#25968;&#35780;&#35770;&#19981;&#21516;&#65292;&#35813;&#24037;&#20316;&#37319;&#29992;&#8220;&#25968;&#25454;&#20026;&#20013;&#24515;&#8221;&#30340;&#35266;&#28857;&#65292;&#30740;&#31350;&#25968;&#25454;&#25910;&#38598;&#65292;&#22788;&#29702;&#21644;&#20998;&#26512;&#22914;&#20309;&#20419;&#25104;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#12290;&#25105;&#20204;&#23558;&#29616;&#26377;&#24037;&#20316;&#20998;&#20026;&#19977;&#31867;&#65292;&#26681;&#25454;&#20854;&#30446;&#30340;&#36827;&#34892;&#20998;&#31867;&#65306;&#28145;&#24230;&#27169;&#22411;&#30340;&#35299;&#37322;&#65292;&#28041;&#21450;&#23558;&#25968;&#25454;&#28857;&#19982;&#27169;&#22411;&#36755;&#20986;&#30456;&#20851;&#32852;&#30340;&#29305;&#24449;&#24402;&#22240;&#21644;&#25512;&#29702;&#36807;&#31243;&#65307;&#35757;&#32451;&#25968;&#25454;&#30340;&#24433;&#21709;&#65292;&#30740;&#31350;&#35757;&#32451;&#25968;&#25454;&#32454;&#24494;&#24046;&#24322;&#65288;&#22914;&#25968;&#25454;&#35780;&#20272;&#21644;&#26679;&#26412;&#24322;&#24120;&#65289;&#23545;&#20915;&#31574;&#36807;&#31243;&#30340;&#24433;&#21709;&#65307;&#20197;&#21450;&#39046;&#22495;&#30693;&#35782;&#30340;&#35265;&#35299;&#65292;&#20174;&#25968;&#25454;&#21644;&#27169;&#22411;&#20013;&#21457;&#29616;&#28508;&#22312;&#27169;&#24335;&#65292;&#24182;&#20419;&#36827;&#31038;&#20250;&#20215;&#20540;&#21644;&#31185;&#23398;&#21457;&#29616;&#30340;&#26032;&#30693;&#35782;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;XAI&#26041;&#27861;&#35770;&#25552;&#28860;&#20026;&#25968;&#25454;&#25366;&#25496;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given the complexity and lack of transparency in deep neural networks (DNNs), extensive efforts have been made to make these systems more interpretable or explain their behaviors in accessible terms. Unlike most reviews, which focus on algorithmic and model-centric perspectives, this work takes a "data-centric" view, examining how data collection, processing, and analysis contribute to explainable AI (XAI). We categorize existing work into three categories subject to their purposes: interpretations of deep models, referring to feature attributions and reasoning processes that correlate data points with model outputs; influences of training data, examining the impact of training data nuances, such as data valuation and sample anomalies, on decision-making processes; and insights of domain knowledge, discovering latent patterns and fostering new knowledge from data and models to advance social values and scientific discovery. Specifically, we distill XAI methodologies into data mining op
&lt;/p&gt;</description></item><item><title>MobileAgent&#36890;&#36807;&#20154;&#26426;&#20132;&#20114;&#21644;SOP&#38598;&#25104;&#65292;&#25552;&#39640;&#20102;&#31227;&#21160;&#25511;&#21046;&#30340;&#25928;&#29575;&#21644;&#20010;&#24615;&#21270;&#29992;&#25143;&#38656;&#27714;&#30340;&#28385;&#36275;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#38544;&#31169;&#38382;&#39064;&#21644;&#20195;&#29702;&#23398;&#20064;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.04124</link><description>&lt;p&gt;
MobileAgent&#65306;&#36890;&#36807;&#20154;&#26426;&#20132;&#20114;&#21644;SOP&#38598;&#25104;&#22686;&#24378;&#31227;&#21160;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
MobileAgent: enhancing mobile control via human-machine interaction and SOP integration. (arXiv:2401.04124v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04124
&lt;/p&gt;
&lt;p&gt;
MobileAgent&#36890;&#36807;&#20154;&#26426;&#20132;&#20114;&#21644;SOP&#38598;&#25104;&#65292;&#25552;&#39640;&#20102;&#31227;&#21160;&#25511;&#21046;&#30340;&#25928;&#29575;&#21644;&#20010;&#24615;&#21270;&#29992;&#25143;&#38656;&#27714;&#30340;&#28385;&#36275;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#38544;&#31169;&#38382;&#39064;&#21644;&#20195;&#29702;&#23398;&#20064;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#22312;&#20197;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20026;&#20013;&#24515;&#30340;&#20195;&#29702;&#33021;&#22815;&#20026;&#29992;&#25143;&#33258;&#21160;&#21270;&#31227;&#21160;&#35774;&#22791;&#25805;&#20316;&#12290;&#22312;&#38024;&#23545;&#23398;&#20064;&#29992;&#25143;&#30340;&#31227;&#21160;&#25805;&#20316;&#36827;&#34892;&#24494;&#35843;&#21518;&#65292;&#36825;&#20123;&#20195;&#29702;&#21487;&#20197;&#22312;&#32447;&#36981;&#24490;&#39640;&#32423;&#29992;&#25143;&#25351;&#20196;&#12290;&#23427;&#20204;&#25191;&#34892;&#30446;&#26631;&#20998;&#35299;&#12289;&#23376;&#30446;&#26631;&#24207;&#21015;&#21270;&#21644;&#20132;&#20114;&#24335;&#29615;&#22659;&#25506;&#32034;&#31561;&#20219;&#21153;&#65292;&#30452;&#21040;&#23454;&#29616;&#26368;&#32456;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#22312;&#31227;&#21160;&#25805;&#20316;&#20013;&#23384;&#22312;&#19982;&#20010;&#24615;&#21270;&#29992;&#25143;&#25968;&#25454;&#30456;&#20851;&#30340;&#38544;&#31169;&#38382;&#39064;&#65292;&#38656;&#35201;&#29992;&#25143;&#30830;&#35748;&#12290;&#27492;&#22806;&#65292;&#29992;&#25143;&#30340;&#30495;&#23454;&#25805;&#20316;&#26159;&#25506;&#32034;&#24615;&#30340;&#65292;&#34892;&#21160;&#25968;&#25454;&#22797;&#26434;&#19988;&#20887;&#20313;&#65292;&#32473;&#20195;&#29702;&#23398;&#20064;&#24102;&#26469;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#22312;&#25105;&#20204;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20195;&#29702;&#19982;&#20154;&#20043;&#38388;&#30340;&#20132;&#20114;&#20219;&#21153;&#65292;&#20197;&#35782;&#21035;&#25935;&#24863;&#20449;&#24687;&#24182;&#19982;&#20010;&#24615;&#21270;&#29992;&#25143;&#38656;&#27714;&#23545;&#40784;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#38598;&#25104;&#20102;&#26631;&#20934;&#25805;&#20316;&#35268;&#31243;&#65288;SOP&#65289;&#20449;&#24687;&#65292;&#20197;&#22686;&#24378;&#20195;&#29702;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Agents centered around Large Language Models (LLMs) are now capable of automating mobile device operations for users. After fine-tuning to learn a user's mobile operations, these agents can adhere to high-level user instructions online. They execute tasks such as goal decomposition, sequencing of sub-goals, and interactive environmental exploration, until the final objective is achieved. However, privacy concerns related to personalized user data arise during mobile operations, requiring user confirmation. Moreover, users' real-world operations are exploratory, with action data being complex and redundant, posing challenges for agent learning. To address these issues, in our practical application, we have designed interactive tasks between agents and humans to identify sensitive information and align with personalized user needs. Additionally, we integrated Standard Operating Procedure (SOP) information within the model's in-context learning to enhance the agent's comprehension of comp
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#22810;&#22836;&#21518;&#39564;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#23454;&#20307;&#30340;&#20803;&#29305;&#24449;&#21644;&#27169;&#22411;&#30340;&#34920;&#31034;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#20316;&#20026;&#24230;&#37327;&#26631;&#20934;&#65292;&#26377;&#25928;&#35780;&#20272;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2401.02987</link><description>&lt;p&gt;
&#20320;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#26377;&#25913;&#36827;&#21527;&#65311;&#19968;&#31181;&#22522;&#20110;&#22810;&#22836;&#21518;&#39564;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Has Your Pretrained Model Improved? A Multi-head Posterior Based Approach. (arXiv:2401.02987v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02987
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#22810;&#22836;&#21518;&#39564;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#23454;&#20307;&#30340;&#20803;&#29305;&#24449;&#21644;&#27169;&#22411;&#30340;&#34920;&#31034;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#20316;&#20026;&#24230;&#37327;&#26631;&#20934;&#65292;&#26377;&#25928;&#35780;&#20272;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20986;&#29616;&#23545;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#20851;&#31995;&#22411;&#25968;&#25454;&#38598;&#31561;&#39046;&#22495;&#20135;&#29983;&#20102;&#26174;&#33879;&#24433;&#21709;&#12290;&#20256;&#32479;&#19978;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#36807;&#19979;&#28216;&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#36825;&#24341;&#21457;&#20102;&#22914;&#20309;&#26356;&#39640;&#25928;&#12289;&#26356;&#26377;&#25928;&#22320;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;&#21033;&#29992;&#19982;&#27599;&#20010;&#23454;&#20307;&#30456;&#20851;&#30340;&#20803;&#29305;&#24449;&#20316;&#20026;&#19990;&#30028;&#30693;&#35782;&#30340;&#26469;&#28304;&#65292;&#24182;&#21033;&#29992;&#27169;&#22411;&#30340;&#23454;&#20307;&#34920;&#31034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#36825;&#20123;&#34920;&#31034;&#21644;&#20803;&#29305;&#24449;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#20316;&#20026;&#35780;&#20272;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#20010;&#39046;&#22495;&#34920;&#29616;&#20986;&#20102;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#20855;&#26377;&#20851;&#31995;&#22411;&#25968;&#25454;&#38598;&#12289;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22270;&#20687;&#27169;&#22411;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of pretrained models has significantly impacted from Natural Language Processing (NLP) and Computer Vision to relational datasets. Traditionally, these models are assessed through fine-tuned downstream tasks. However, this raises the question of how to evaluate these models more efficiently and more effectively. In this study, we explore a novel approach where we leverage the meta features associated with each entity as a source of worldly knowledge and employ entity representations from the models. We propose using the consistency between these representations and the meta features as a metric for evaluating pretrained models. Our method's effectiveness is demonstrated across various domains, including models with relational datasets, large language models and images models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#32852;&#37030;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#26410;&#27880;&#37322;&#30340;&#30446;&#26631;&#22495;&#20013;&#20351;&#29992;&#22810;&#20010;&#24102;&#26377;&#27880;&#37322;&#30340;&#28304;&#39046;&#22495;&#30340;&#30693;&#35782;&#26469;&#36827;&#34892;MRI&#22270;&#20687;&#20998;&#21106;&#12290;</title><link>http://arxiv.org/abs/2401.02941</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#30340;&#32852;&#37030;&#39046;&#22495;&#36866;&#24212;&#29992;&#20110;MRI&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Federated Domain Adaptation for Segmentation of MRI Images. (arXiv:2401.02941v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02941
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#32852;&#37030;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#26410;&#27880;&#37322;&#30340;&#30446;&#26631;&#22495;&#20013;&#20351;&#29992;&#22810;&#20010;&#24102;&#26377;&#27880;&#37322;&#30340;&#28304;&#39046;&#22495;&#30340;&#30693;&#35782;&#26469;&#36827;&#34892;MRI&#22270;&#20687;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#33258;&#21160;&#23545;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;MRI&#65289;&#22270;&#20687;&#36827;&#34892;&#35821;&#20041;&#20998;&#21106;&#26497;&#22823;&#22320;&#26377;&#21161;&#20110;&#35780;&#20272;&#21644;&#35268;&#21010;&#21508;&#31181;&#20020;&#24202;&#24212;&#29992;&#30340;&#27835;&#30103;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#26631;&#27880;&#25968;&#25454;&#26469;&#23454;&#26045;&#31471;&#21040;&#31471;&#30340;&#30417;&#30563;&#23398;&#20064;&#36807;&#31243;&#12290;&#21363;&#20351;&#25105;&#20204;&#26631;&#27880;&#20102;&#36275;&#22815;&#30340;&#25968;&#25454;&#65292;MRI&#22270;&#20687;&#30001;&#20110;&#24739;&#32773;&#12289;MRI&#25195;&#25551;&#20202;&#21644;&#25104;&#20687;&#21327;&#35758;&#30340;&#24046;&#24322;&#32780;&#26174;&#31034;&#20986;&#30456;&#24403;&#22823;&#30340;&#21464;&#24322;&#24615;&#12290;&#36825;&#31181;&#21464;&#24322;&#24615;&#38656;&#35201;&#23545;&#27599;&#20010;&#29305;&#23450;&#30340;&#24212;&#29992;&#39046;&#22495;&#37325;&#26032;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#65292;&#36825;&#21448;&#38656;&#35201;&#19987;&#23478;&#25918;&#23556;&#31185;&#21307;&#29983;&#23545;&#25152;&#26377;&#26032;&#39046;&#22495;&#36827;&#34892;&#25163;&#24037;&#27880;&#37322;&#12290;&#20026;&#20102;&#20943;&#36731;&#23545;&#25345;&#32493;&#25968;&#25454;&#27880;&#37322;&#30340;&#38656;&#27714;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#32852;&#37030;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#22810;&#20010;&#24102;&#26377;&#27880;&#37322;&#30340;&#28304;&#39046;&#22495;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#20174;&#22810;&#20010;&#24102;&#26377;&#27880;&#37322;&#30340;&#28304;&#22495;&#20013;&#36716;&#31227;&#30693;&#35782;&#65292;&#20197;&#36866;&#24212;&#22312;&#26410;&#27880;&#37322;&#30340;&#30446;&#26631;&#22495;&#20013;&#30340;&#26377;&#25928;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic semantic segmentation of magnetic resonance imaging (MRI) images using deep neural networks greatly assists in evaluating and planning treatments for various clinical applications. However, training these models is conditioned on the availability of abundant annotated data to implement the end-to-end supervised learning procedure. Even if we annotate enough data, MRI images display considerable variability due to factors such as differences in patients, MRI scanners, and imaging protocols. This variability necessitates retraining neural networks for each specific application domain, which, in turn, requires manual annotation by expert radiologists for all new domains. To relax the need for persistent data annotation, we develop a method for unsupervised federated domain adaptation using multiple annotated source domains. Our approach enables the transfer of knowledge from several annotated source domains to adapt a model for effective use in an unannotated target domain. Init
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#26469;&#25552;&#39640;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#35757;&#32451;&#30340;&#40065;&#26834;&#24615;&#21644;&#25910;&#25947;&#24615;&#12290;&#32463;&#36807;&#20004;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#21457;&#29616;&#36801;&#31227;&#23398;&#20064;&#21487;&#20197;&#26377;&#25928;&#22320;&#35757;&#32451;PINN&#26469;&#36817;&#20284;&#35299;&#20915;&#26041;&#26696;&#65292;&#20174;&#20302;&#39057;&#29575;&#38382;&#39064;&#21040;&#39640;&#39057;&#29575;&#38382;&#39064;&#65292;&#32780;&#19981;&#22686;&#21152;&#32593;&#32476;&#21442;&#25968;&#65292;&#19988;&#38656;&#35201;&#26356;&#23569;&#30340;&#25968;&#25454;&#28857;&#21644;&#26356;&#30701;&#30340;&#35757;&#32451;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2401.02810</link><description>&lt;p&gt;
&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#39640;&#39057;&#29575;&#21644;&#22810;&#23610;&#24230;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Physics-Informed Neural Networks for High-Frequency and Multi-Scale Problems using Transfer Learning. (arXiv:2401.02810v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02810
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#26469;&#25552;&#39640;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#35757;&#32451;&#30340;&#40065;&#26834;&#24615;&#21644;&#25910;&#25947;&#24615;&#12290;&#32463;&#36807;&#20004;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#21457;&#29616;&#36801;&#31227;&#23398;&#20064;&#21487;&#20197;&#26377;&#25928;&#22320;&#35757;&#32451;PINN&#26469;&#36817;&#20284;&#35299;&#20915;&#26041;&#26696;&#65292;&#20174;&#20302;&#39057;&#29575;&#38382;&#39064;&#21040;&#39640;&#39057;&#29575;&#38382;&#39064;&#65292;&#32780;&#19981;&#22686;&#21152;&#32593;&#32476;&#21442;&#25968;&#65292;&#19988;&#38656;&#35201;&#26356;&#23569;&#30340;&#25968;&#25454;&#28857;&#21644;&#26356;&#30701;&#30340;&#35757;&#32451;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#26159;&#19968;&#31181;&#29992;&#20110;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;ODEs/PDEs&#65289;&#30340;&#25968;&#25454;&#39537;&#21160;&#27714;&#35299;&#22120;&#65292;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#26694;&#26550;&#26469;&#22788;&#29702;&#21069;&#21521;&#21644;&#21453;&#21521;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30446;&#26631;&#20989;&#25968;&#30340;&#22797;&#26434;&#24615;&#24120;&#24120;&#23548;&#33268;&#35757;&#32451;&#22833;&#36133;&#12290;&#24403;&#35299;&#20915;&#39640;&#39057;&#29575;&#21644;&#22810;&#23610;&#24230;&#38382;&#39064;&#26102;&#65292;&#36825;&#20010;&#38382;&#39064;&#23588;&#20026;&#31361;&#20986;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#26469;&#25552;&#39640;&#35757;&#32451;PINN&#30340;&#40065;&#26834;&#24615;&#21644;&#25910;&#25947;&#24615;&#65292;&#20174;&#20302;&#39057;&#29575;&#38382;&#39064;&#24320;&#22987;&#35757;&#32451;&#65292;&#24182;&#36880;&#28176;&#25509;&#36817;&#39640;&#39057;&#29575;&#38382;&#39064;&#12290;&#36890;&#36807;&#20004;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;&#36801;&#31227;&#23398;&#20064;&#21487;&#20197;&#26377;&#25928;&#22320;&#35757;&#32451;PINN&#26469;&#36817;&#20284;&#35299;&#20915;&#26041;&#26696;&#65292;&#20174;&#20302;&#39057;&#29575;&#38382;&#39064;&#21040;&#39640;&#39057;&#29575;&#38382;&#39064;&#65292;&#32780;&#19981;&#22686;&#21152;&#32593;&#32476;&#21442;&#25968;&#12290;&#27492;&#22806;&#65292;&#23427;&#38656;&#35201;&#26356;&#23569;&#30340;&#25968;&#25454;&#28857;&#21644;&#26356;&#30701;&#30340;&#35757;&#32451;&#26102;&#38388;&#12290;&#25105;&#20204;&#35814;&#32454;&#25551;&#36848;&#20102;&#25105;&#20204;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#21253;&#25324;&#20248;&#21270;&#22120;&#30340;&#36873;&#25321;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#26469;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physics-informed neural network (PINN) is a data-driven solver for partial and ordinary differential equations(ODEs/PDEs). It provides a unified framework to address both forward and inverse problems. However, the complexity of the objective function often leads to training failures. This issue is particularly prominent when solving high-frequency and multi-scale problems. We proposed using transfer learning to boost the robustness and convergence of training PINN, starting training from low-frequency problems and gradually approaching high-frequency problems. Through two case studies, we discovered that transfer learning can effectively train PINN to approximate solutions from low-frequency problems to high-frequency problems without increasing network parameters. Furthermore, it requires fewer data points and less training time. We elaborately described our training strategy, including optimizer selection, and suggested guidelines for using transfer learning to train neural networks 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#29305;&#24449;&#31354;&#38388;&#24494;&#35843;AE&#65292;&#26174;&#33879;&#25552;&#39640;&#29616;&#26377;&#25915;&#20987;&#30340;&#30446;&#26631;&#20256;&#36882;&#24615;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21482;&#38656;&#23569;&#37327;&#30340;&#24494;&#35843;&#21363;&#21487;&#26222;&#36941;&#22320;&#22686;&#24378;&#25915;&#20987;&#30340;&#20256;&#36882;&#33021;&#21147;&#65292;&#24182;&#26174;&#31034;&#20986;&#31616;&#21333;&#30340;&#36845;&#20195;&#25915;&#20987;&#21487;&#20197;&#19982;&#36164;&#28304;&#23494;&#38598;&#22411;&#26041;&#27861;&#30456;&#23218;&#32654;&#29978;&#33267;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2401.02727</link><description>&lt;p&gt;
&#36890;&#36807;&#29305;&#24449;&#31354;&#38388;&#24494;&#35843;&#25552;&#39640;&#30446;&#26631;&#21487;&#20256;&#36882;&#24615;
&lt;/p&gt;
&lt;p&gt;
Enhancing targeted transferability via feature space fine-tuning. (arXiv:2401.02727v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02727
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#29305;&#24449;&#31354;&#38388;&#24494;&#35843;AE&#65292;&#26174;&#33879;&#25552;&#39640;&#29616;&#26377;&#25915;&#20987;&#30340;&#30446;&#26631;&#20256;&#36882;&#24615;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21482;&#38656;&#23569;&#37327;&#30340;&#24494;&#35843;&#21363;&#21487;&#26222;&#36941;&#22320;&#22686;&#24378;&#25915;&#20987;&#30340;&#20256;&#36882;&#33021;&#21147;&#65292;&#24182;&#26174;&#31034;&#20986;&#31616;&#21333;&#30340;&#36845;&#20195;&#25915;&#20987;&#21487;&#20197;&#19982;&#36164;&#28304;&#23494;&#38598;&#22411;&#26041;&#27861;&#30456;&#23218;&#32654;&#29978;&#33267;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#23545;&#38544;&#31169;&#20445;&#25252;&#30340;&#28508;&#21147;&#21644;&#28608;&#21457;&#40065;&#26834;&#24615;&#31070;&#32463;&#32593;&#32476;&#30340;&#33021;&#21147;&#65292;&#23545;&#25239;&#24615;&#31034;&#20363;&#65288;AEs&#65289;&#24050;&#32463;&#34987;&#24191;&#27867;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#20351;&#30446;&#26631;AE&#22312;&#26410;&#30693;&#27169;&#22411;&#20043;&#38388;&#21487;&#20256;&#36882;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#20026;&#20102;&#20943;&#36731;&#29616;&#26377;&#31616;&#21333;&#36845;&#20195;&#25915;&#20987;&#25152;&#29983;&#25104;&#30340;AE&#20013;&#24120;&#35265;&#30340;&#36807;&#25311;&#21512;&#22256;&#22659;&#65292;&#25105;&#20204;&#25552;&#20986;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#23545;&#20854;&#36827;&#34892;&#24494;&#35843;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20174;&#22522;&#32447;&#25915;&#20987;&#29983;&#25104;&#30340;AE&#24320;&#22987;&#65292;&#22312;&#28304;&#27169;&#22411;&#30340;&#20013;&#38388;&#23618;&#20013;&#40723;&#21169;&#26377;&#21161;&#20110;&#30446;&#26631;&#31867;&#21035;&#30340;&#29305;&#24449;&#65292;&#38459;&#30861;&#26377;&#21161;&#20110;&#21407;&#22987;&#31867;&#21035;&#30340;&#29305;&#24449;&#12290;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#20165;&#20351;&#29992;&#20960;&#27425;&#24494;&#35843;&#21363;&#21487;&#26174;&#33879;&#21644;&#26222;&#36941;&#22320;&#25552;&#39640;&#29616;&#26377;&#25915;&#20987;&#30340;&#30446;&#26631;&#20256;&#36882;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#36824;&#39564;&#35777;&#20102;&#31616;&#21333;&#30340;&#36845;&#20195;&#25915;&#20987;&#21487;&#20197;&#20135;&#29983;&#19982;&#36164;&#28304;&#23494;&#38598;&#22411;&#26041;&#27861;&#30456;&#24403;&#29978;&#33267;&#26356;&#22909;&#30340;&#20256;&#36882;&#24615;&#65292;&#21518;&#32773;&#20381;&#36182;&#20110;&#35757;&#32451;&#29305;&#23450;&#30446;&#26631;&#20998;&#31867;&#22120;&#25110;&#29983;&#25104;&#29305;&#23450;&#30340;AE&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial examples (AEs) have been extensively studied due to their potential for privacy protection and inspiring robust neural networks. However, making a targeted AE transferable across unknown models remains challenging. In this paper, to alleviate the overfitting dilemma common in an AE crafted by existing simple iterative attacks, we propose fine-tuning it in the feature space. Specifically, starting with an AE generated by a baseline attack, we encourage the features that contribute to the target class and discourage the features that contribute to the original class in a middle layer of the source model. Extensive experiments demonstrate that only a few iterations of fine-tuning can boost existing attacks in terms of targeted transferability nontrivially and universally. Our results also verify that the simple iterative attacks can yield comparable or even better transferability than the resource-intensive methods, which rely on training target-specific classifiers or generat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#31639;&#27861;&#26469;&#24212;&#23545;&#38750;&#31283;&#24577;&#29615;&#22659;&#19979;&#30340;&#20915;&#31574;&#38382;&#39064;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#23545;&#29615;&#22659;&#21160;&#24577;&#20551;&#35774;&#30340;&#38480;&#21046;&#21644;&#35268;&#21010;&#36807;&#31243;&#30340;&#24754;&#35266;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.01841</link><description>&lt;p&gt;
&#25353;&#29031;&#20320;&#30340;&#23398;&#20064;&#34892;&#21160;&#65306;&#38750;&#31283;&#24577;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#33258;&#36866;&#24212;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Act as You Learn: Adaptive Decision-Making in Non-Stationary Markov Decision Processes. (arXiv:2401.01841v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01841
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#31639;&#27861;&#26469;&#24212;&#23545;&#38750;&#31283;&#24577;&#29615;&#22659;&#19979;&#30340;&#20915;&#31574;&#38382;&#39064;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#23545;&#29615;&#22659;&#21160;&#24577;&#20551;&#35774;&#30340;&#38480;&#21046;&#21644;&#35268;&#21010;&#36807;&#31243;&#30340;&#24754;&#35266;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39034;&#24207;&#20915;&#31574;&#20013;&#65292;&#22788;&#29702;&#38750;&#31283;&#24577;&#29615;&#22659;&#26159;&#19968;&#20010;&#22522;&#26412;&#65288;&#19988;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#26159;&#26410;&#35299;&#20915;&#30340;&#65289;&#25361;&#25112;&#65292;&#20854;&#20013;&#22806;&#37096;&#29615;&#22659;&#26465;&#20214;&#38543;&#26102;&#38388;&#21464;&#21270;&#12290;&#36825;&#31867;&#38382;&#39064;&#36890;&#24120;&#34987;&#24314;&#27169;&#20026;&#38750;&#31283;&#24577;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;NSMDP&#65289;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;NSMDP&#20915;&#31574;&#26041;&#27861;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#32570;&#28857;&#65306;&#39318;&#20808;&#65292;&#23427;&#20204;&#20551;&#35774;&#24403;&#21069;&#26102;&#21051;&#26356;&#26032;&#30340;&#29615;&#22659;&#21160;&#24577;&#26159;&#24050;&#30693;&#30340;&#65288;&#23613;&#31649;&#26410;&#26469;&#21160;&#24577;&#21487;&#33021;&#20250;&#25913;&#21464;&#65289;&#65307;&#20854;&#27425;&#65292;&#35268;&#21010;&#36807;&#31243;&#20027;&#35201;&#26159;&#24754;&#35266;&#30340;&#65292;&#21363;&#20195;&#29702;&#20154;&#20250;&#8220;&#23433;&#20840;&#34892;&#21160;&#8221;&#20197;&#32771;&#34385;&#29615;&#22659;&#30340;&#38750;&#31283;&#24577;&#28436;&#21464;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#20004;&#20010;&#20551;&#35774;&#22312;&#23454;&#36341;&#20013;&#26159;&#26080;&#25928;&#30340;-&#26356;&#26032;&#30340;&#29615;&#22659;&#26465;&#20214;&#24456;&#23569;&#26159;&#24050;&#30693;&#30340;&#65292;&#24182;&#19988;&#24403;&#20195;&#29702;&#20154;&#19982;&#29615;&#22659;&#20132;&#20114;&#26102;&#65292;&#23427;&#21487;&#20197;&#23398;&#20064;&#26356;&#26032;&#30340;&#21160;&#24577;&#24182;&#36991;&#20813;&#24754;&#35266;&#65292;&#33267;&#23569;&#22312;&#20854;&#23545;&#21160;&#24577;&#26377;&#20449;&#24515;&#30340;&#29366;&#24577;&#19979;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21551;&#21457;&#24335;&#25628;&#32034;&#31639;&#27861;&#65292;&#31216;&#20026;&#33258;&#36866;&#24212;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
A fundamental (and largely open) challenge in sequential decision-making is dealing with non-stationary environments, where exogenous environmental conditions change over time. Such problems are traditionally modeled as non-stationary Markov decision processes (NSMDP). However, existing approaches for decision-making in NSMDPs have two major shortcomings: first, they assume that the updated environmental dynamics at the current time are known (although future dynamics can change); and second, planning is largely pessimistic, i.e., the agent acts ``safely'' to account for the non-stationary evolution of the environment. We argue that both these assumptions are invalid in practice -updated environmental conditions are rarely known, and as the agent interacts with the environment, it can learn about the updated dynamics and avoid being pessimistic, at least in states whose dynamics it is confident about. We present a heuristic search algorithm called \textit{Adaptive Monte Carlo Tree Se
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#32852;&#21512;&#23454;&#20307;&#21644;&#20851;&#31995;&#25277;&#21462;&#38382;&#39064;&#20316;&#20026;&#26465;&#20214;&#24207;&#21015;&#29983;&#25104;&#38382;&#39064;&#26469;&#35299;&#20915;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#22522;&#20110;&#36328;&#24230;&#30340;&#22270;&#29983;&#25104;&#26041;&#24335;&#65292;&#24182;&#36890;&#36807;&#25351;&#21521;&#26426;&#21046;&#23558;&#29983;&#25104;&#30340;&#36755;&#20986;&#19982;&#21407;&#22987;&#25991;&#26412;&#23545;&#40784;&#12290;&#35780;&#20272;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#33719;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.01326</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#32852;&#21512;&#23454;&#20307;&#21644;&#20851;&#31995;&#25277;&#21462;&#30340;&#33258;&#22238;&#24402;&#25991;&#26412;&#21040;&#22270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
An Autoregressive Text-to-Graph Framework for Joint Entity and Relation Extraction. (arXiv:2401.01326v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01326
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#32852;&#21512;&#23454;&#20307;&#21644;&#20851;&#31995;&#25277;&#21462;&#38382;&#39064;&#20316;&#20026;&#26465;&#20214;&#24207;&#21015;&#29983;&#25104;&#38382;&#39064;&#26469;&#35299;&#20915;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#22522;&#20110;&#36328;&#24230;&#30340;&#22270;&#29983;&#25104;&#26041;&#24335;&#65292;&#24182;&#36890;&#36807;&#25351;&#21521;&#26426;&#21046;&#23558;&#29983;&#25104;&#30340;&#36755;&#20986;&#19982;&#21407;&#22987;&#25991;&#26412;&#23545;&#40784;&#12290;&#35780;&#20272;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#33719;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#30340;&#32852;&#21512;&#23454;&#20307;&#21644;&#20851;&#31995;&#25277;&#21462;&#38382;&#39064;&#20316;&#20026;&#26465;&#20214;&#24207;&#21015;&#29983;&#25104;&#38382;&#39064;&#26469;&#35299;&#20915;&#12290;&#19982;&#20256;&#32479;&#30340;&#29983;&#25104;&#24335;&#20449;&#24687;&#25277;&#21462;&#27169;&#22411;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#22522;&#20110;&#36328;&#24230;&#30340;&#65292;&#23427;&#29983;&#25104;&#19968;&#20010;&#32447;&#24615;&#21270;&#30340;&#22270;&#65292;&#20854;&#20013;&#33410;&#28857;&#34920;&#31034;&#25991;&#26412;&#36328;&#24230;&#65292;&#36793;&#34920;&#31034;&#20851;&#31995;&#19977;&#20803;&#32452;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#20102;&#19968;&#20010;&#20855;&#26377;&#25351;&#21521;&#26426;&#21046;&#30340;&#36716;&#25442;&#22120;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#20351;&#29992;&#19968;&#20010;&#21160;&#24577;&#35789;&#27719;&#34920;&#26469;&#34920;&#31034;&#36328;&#24230;&#21644;&#20851;&#31995;&#31867;&#22411;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#36328;&#24230;&#34920;&#31034;&#25429;&#25417;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#32467;&#26500;&#29305;&#24449;&#21644;&#36793;&#30028;&#65292;&#21516;&#26102;&#36890;&#36807;&#25351;&#21521;&#26426;&#21046;&#23558;&#29983;&#25104;&#30340;&#36755;&#20986;&#19982;&#21407;&#22987;&#25991;&#26412;&#36827;&#34892;&#23545;&#40784;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#23637;&#31034;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/urchade/ATG&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a novel method for joint entity and relation extraction from unstructured text by framing it as a conditional sequence generation problem. In contrast to conventional generative information extraction models that are left-to-right token-level generators, our approach is \textit{span-based}. It generates a linearized graph where nodes represent text spans and edges represent relation triplets. Our method employs a transformer encoder-decoder architecture with pointing mechanism on a dynamic vocabulary of spans and relation types. Our model can capture the structural characteristics and boundaries of entities and relations through span representations while simultaneously grounding the generated output in the original text thanks to the pointing mechanism. Evaluation on benchmark datasets validates the effectiveness of our approach, demonstrating competitive results. Code is available at https://github.com/urchade/ATG.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#23376;&#31354;&#38388;&#21644;&#23376;&#25277;&#26679;&#38598;&#21512;&#30340;Dirichlet&#36807;&#31243;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#21644;&#26816;&#27979;&#22120;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.00773</link><description>&lt;p&gt;
&#20351;&#29992;&#38543;&#26426;&#23376;&#31354;&#38388;&#21644;Dirichlet&#36807;&#31243;&#28151;&#21512;&#27169;&#22411;&#30340;&#23376;&#25277;&#26679;&#38598;&#21512;&#36827;&#34892;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Outlier Detection using Random Subspace and Subsampling Ensembles of Dirichlet Process Mixtures. (arXiv:2401.00773v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00773
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#23376;&#31354;&#38388;&#21644;&#23376;&#25277;&#26679;&#38598;&#21512;&#30340;Dirichlet&#36807;&#31243;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#21644;&#26816;&#27979;&#22120;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#29575;&#28151;&#21512;&#27169;&#22411;&#34987;&#35748;&#20026;&#26159;&#19968;&#31181;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#35299;&#37322;&#24615;&#65292;&#24182;&#19988;&#22312;&#32479;&#35745;&#21407;&#29702;&#19978;&#26377;&#30452;&#35266;&#22522;&#30784;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20869;&#65292;Dirichlet&#36807;&#31243;&#28151;&#21512;&#27169;&#22411;&#20316;&#20026;&#20256;&#32479;&#26377;&#38480;&#28151;&#21512;&#27169;&#22411;&#22312;&#32858;&#31867;&#21644;&#24322;&#24120;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#19968;&#20010;&#24341;&#20154;&#27880;&#30446;&#30340;&#26367;&#20195;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#23427;&#20204;&#26126;&#26174;&#20855;&#26377;&#20248;&#21183;&#65292;&#20294;&#22312;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#20013;&#24191;&#27867;&#37319;&#29992;Dirichlet&#36807;&#31243;&#28151;&#21512;&#27169;&#22411;&#21463;&#21040;&#19982;&#26500;&#24314;&#26816;&#27979;&#22120;&#36807;&#31243;&#20013;&#30340;&#35745;&#31639;&#25928;&#29575;&#21644;&#23545;&#24322;&#24120;&#20540;&#30340;&#25935;&#24863;&#24615;&#26377;&#20851;&#30340;&#25361;&#25112;&#30340;&#38459;&#30861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Dirichlet&#36807;&#31243;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#38598;&#21512;&#30340;&#26032;&#22411;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26159;&#19968;&#31181;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#31639;&#27861;&#65292;&#21033;&#29992;&#20102;&#38543;&#26426;&#23376;&#31354;&#38388;&#21644;&#23376;&#25277;&#26679;&#38598;&#21512;&#65292;&#19981;&#20165;&#30830;&#20445;&#20102;&#39640;&#25928;&#35745;&#31639;&#65292;&#36824;&#22686;&#24378;&#20102;&#32467;&#26524;&#24322;&#24120;&#26816;&#27979;&#22120;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Probabilistic mixture models are acknowledged as a valuable tool for unsupervised outlier detection owing to their interpretability and intuitive grounding in statistical principles. Within this framework, Dirichlet process mixture models emerge as a compelling alternative to conventional finite mixture models for both clustering and outlier detection tasks. However, despite their evident advantages, the widespread adoption of Dirichlet process mixture models in unsupervised outlier detection has been hampered by challenges related to computational inefficiency and sensitivity to outliers during the construction of detectors. To tackle these challenges, we propose a novel outlier detection method based on ensembles of Dirichlet process Gaussian mixtures. The proposed method is a fully unsupervised algorithm that capitalizes on random subspace and subsampling ensembles, not only ensuring efficient computation but also enhancing the robustness of the resulting outlier detector. Moreover,
&lt;/p&gt;</description></item><item><title>ToolEyes&#26159;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#30495;&#23454;&#24773;&#26223;&#20013;&#30340;&#24037;&#20855;&#23398;&#20064;&#33021;&#21147;&#30340;&#32454;&#31890;&#24230;&#31995;&#32479;&#65292;&#36890;&#36807;&#23545;&#19971;&#20010;&#30495;&#23454;&#24773;&#26223;&#30340;&#35814;&#32454;&#20998;&#26512;&#65292;&#35780;&#20272;&#20102;LLMs&#22312;&#24037;&#20855;&#23398;&#20064;&#30340;&#20116;&#20010;&#20851;&#38190;&#32500;&#24230;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#25317;&#26377;600&#31181;&#24037;&#20855;&#30340;&#24037;&#20855;&#24211;&#20316;&#20026;&#20013;&#20171;&#12290;</title><link>http://arxiv.org/abs/2401.00741</link><description>&lt;p&gt;
ToolEyes&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23454;&#38469;&#24773;&#26223;&#20013;&#30340;&#24037;&#20855;&#23398;&#20064;&#33021;&#21147;&#30340;&#32454;&#31890;&#24230;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
ToolEyes: Fine-Grained Evaluation for Tool Learning Capabilities of Large Language Models in Real-world Scenarios. (arXiv:2401.00741v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00741
&lt;/p&gt;
&lt;p&gt;
ToolEyes&#26159;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#30495;&#23454;&#24773;&#26223;&#20013;&#30340;&#24037;&#20855;&#23398;&#20064;&#33021;&#21147;&#30340;&#32454;&#31890;&#24230;&#31995;&#32479;&#65292;&#36890;&#36807;&#23545;&#19971;&#20010;&#30495;&#23454;&#24773;&#26223;&#30340;&#35814;&#32454;&#20998;&#26512;&#65292;&#35780;&#20272;&#20102;LLMs&#22312;&#24037;&#20855;&#23398;&#20064;&#30340;&#20116;&#20010;&#20851;&#38190;&#32500;&#24230;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#25317;&#26377;600&#31181;&#24037;&#20855;&#30340;&#24037;&#20855;&#24211;&#20316;&#20026;&#20013;&#20171;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#24037;&#20855;&#23398;&#20064;&#35780;&#20272;&#20027;&#35201;&#38598;&#20013;&#20110;&#39564;&#35777;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36873;&#25321;&#30340;&#24037;&#20855;&#19982;&#26399;&#26395;&#32467;&#26524;&#30340;&#19968;&#33268;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20381;&#36182;&#20110;&#19968;&#32452;&#26377;&#38480;&#30340;&#24773;&#26223;&#65292;&#22312;&#36825;&#20123;&#24773;&#26223;&#20013;&#31572;&#26696;&#21487;&#20197;&#20107;&#20808;&#30830;&#23450;&#65292;&#19982;&#30495;&#23454;&#38656;&#27714;&#32972;&#36947;&#32780;&#39536;&#12290;&#27492;&#22806;&#65292;&#20165;&#20851;&#27880;&#32467;&#26524;&#24573;&#35270;&#20102;LLMs&#26377;&#25928;&#21033;&#29992;&#24037;&#20855;&#25152;&#38656;&#30340;&#22797;&#26434;&#33021;&#21147;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ToolEyes&#65292;&#36825;&#26159;&#19968;&#20010;&#29305;&#21035;&#38024;&#23545;LLMs&#24037;&#20855;&#23398;&#20064;&#33021;&#21147;&#22312;&#30495;&#23454;&#24773;&#26223;&#20013;&#35780;&#20272;&#30340;&#32454;&#31890;&#24230;&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#35814;&#32454;&#20998;&#26512;&#20102;&#19971;&#20010;&#30495;&#23454;&#24773;&#26223;&#65292;&#20998;&#26512;&#20102;&#23545;LLMs&#22312;&#24037;&#20855;&#23398;&#20064;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#20116;&#20010;&#32500;&#24230;&#65306;&#26684;&#24335;&#23545;&#40784;&#65292;&#24847;&#22270;&#29702;&#35299;&#65292;&#34892;&#20026;&#35268;&#21010;&#65292;&#24037;&#20855;&#36873;&#25321;&#21644;&#31572;&#26696;&#32452;&#32455;&#12290;&#27492;&#22806;&#65292;ToolEyes&#36824;&#21253;&#21547;&#19968;&#20010;&#25317;&#26377;&#32422;600&#31181;&#24037;&#20855;&#30340;&#24037;&#20855;&#24211;&#65292;&#20316;&#20026;LLMs&#19982;&#29289;&#29702;&#19990;&#30028;&#20043;&#38388;&#30340;&#20013;&#20171;&#12290;&#22312;&#28041;&#21450;&#19977;&#20010;&#31867;&#21035;&#30340;&#21313;&#20010;LLMs&#30340;&#35780;&#20272;&#20013;&#65292;ToolEyes&#21462;&#24471;&#20102;&#22914;&#19979;&#30340;&#21019;&#26032;&#19982;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing evaluations of tool learning primarily focus on validating the alignment of selected tools for large language models (LLMs) with expected outcomes. However, these approaches rely on a limited set of scenarios where answers can be pre-determined, diverging from genuine needs. Furthermore, a sole emphasis on outcomes disregards the intricate capabilities essential for LLMs to effectively utilize tools. To tackle this issue, we propose ToolEyes, a fine-grained system tailored for the evaluation of the LLMs' tool learning capabilities in authentic scenarios. The system meticulously examines seven real-world scenarios, analyzing five dimensions crucial to LLMs in tool learning: format alignment, intent comprehension, behavior planning, tool selection, and answer organization. Additionally, ToolEyes incorporates a tool library boasting approximately 600 tools, serving as an intermediary between LLMs and the physical world. Evaluations involving ten LLMs across three categories revea
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MVRE&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20851;&#31995;&#35299;&#32806;&#20026;&#19981;&#21516;&#30340;&#35270;&#35282;&#65292;&#29983;&#25104;&#22810;&#35270;&#35282;&#20851;&#31995;&#34920;&#31034;&#65292;&#24182;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#30340;&#33021;&#21147;&#26469;&#25552;&#39640;&#20302;&#36164;&#28304;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2312.17267</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#35270;&#35282;&#35299;&#32806;&#23398;&#20064;&#25913;&#36827;&#20302;&#36164;&#28304;&#30340;&#22522;&#20110;&#25552;&#31034;&#30340;&#20851;&#31995;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Improving Low-resource Prompt-based Relation Representation with Multi-view Decoupling Learning. (arXiv:2312.17267v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.17267
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MVRE&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20851;&#31995;&#35299;&#32806;&#20026;&#19981;&#21516;&#30340;&#35270;&#35282;&#65292;&#29983;&#25104;&#22810;&#35270;&#35282;&#20851;&#31995;&#34920;&#31034;&#65292;&#24182;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#30340;&#33021;&#21147;&#26469;&#25552;&#39640;&#20302;&#36164;&#28304;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#36827;&#34892;&#25552;&#31034;&#35843;&#25972;&#24050;&#32463;&#23637;&#31034;&#20986;&#20102;&#26174;&#33879;&#30340;&#20851;&#31995;&#25277;&#21462;&#65288;RE&#65289;&#20219;&#21153;&#30340;&#22686;&#24378;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#20302;&#36164;&#28304;&#22330;&#26223;&#20013;&#65292;&#21363;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#30001;&#20110;&#23545;&#20851;&#31995;&#30340;&#34920;&#23618;&#29702;&#35299;&#65292;&#20808;&#21069;&#22522;&#20110;&#25552;&#31034;&#30340;&#26041;&#27861;&#21487;&#33021;&#20173;&#28982;&#34920;&#29616;&#19981;&#20339;&#65292;&#29992;&#20110;&#34920;&#31034;&#23398;&#20064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24378;&#35843;&#22312;&#20302;&#36164;&#28304;&#22330;&#26223;&#20013;&#23398;&#20064;&#39640;&#36136;&#37327;&#20851;&#31995;&#34920;&#31034;&#23545;&#20110;RE&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#25552;&#31034;&#30340;&#20851;&#31995;&#34920;&#31034;&#26041;&#27861;&#65292;&#21517;&#20026;MVRE&#65288;&#22810;&#35270;&#35282;&#20851;&#31995;&#25277;&#21462;&#65289;&#65292;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;PLMs&#30340;&#33021;&#21147;&#26469;&#25913;&#21892;&#20302;&#36164;&#28304;&#25552;&#31034;&#35843;&#25972;&#33539;&#24335;&#19979;&#30340;RE&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;MVRE&#23558;&#27599;&#20010;&#20851;&#31995;&#35299;&#32806;&#20026;&#19981;&#21516;&#30340;&#35270;&#35282;&#65292;&#20197;&#21253;&#21547;&#22810;&#35270;&#35282;&#30340;&#20851;&#31995;&#34920;&#31034;&#65292;&#20197;&#26368;&#22823;&#21270;&#20851;&#31995;&#25512;&#26029;&#36807;&#31243;&#20013;&#30340;&#20284;&#28982;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#20840;&#23616;&#24615;&#30340;&#20302;&#39046;&#22495;&#20219;&#21153;&#23398;&#20064;&#31574;&#30053;&#65292;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#20851;&#31995;&#34920;&#31034;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, prompt-tuning with pre-trained language models (PLMs) has demonstrated the significantly enhancing ability of relation extraction (RE) tasks. However, in low-resource scenarios, where the available training data is scarce, previous prompt-based methods may still perform poorly for prompt-based representation learning due to a superficial understanding of the relation. To this end, we highlight the importance of learning high-quality relation representation in low-resource scenarios for RE, and propose a novel prompt-based relation representation method, named MVRE (\underline{M}ulti-\underline{V}iew \underline{R}elation \underline{E}xtraction), to better leverage the capacity of PLMs to improve the performance of RE within the low-resource prompt-tuning paradigm. Specifically, MVRE decouples each relation into different perspectives to encompass multi-view relation representations for maximizing the likelihood during relation inference. Furthermore, we also design a Global-Lo
&lt;/p&gt;</description></item><item><title>TAPE&#25552;&#20986;&#20102;&#19968;&#31181;&#20195;&#29702;&#25299;&#25169;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#26234;&#33021;&#20307;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#20013;&#30340;&#38598;&#20013;-&#20998;&#25955;&#19981;&#21305;&#37197;&#65288;CDM&#65289;&#38382;&#39064;&#65292;&#36890;&#36807;&#24179;&#34913;&#21512;&#20316;&#21644;&#20943;&#36731;CDM&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2312.15667</link><description>&lt;p&gt;
TAPE: &#21033;&#29992;&#20195;&#29702;&#25299;&#25169;&#36827;&#34892;&#21327;&#20316;&#22810;&#26234;&#33021;&#20307;&#31574;&#30053;&#26799;&#24230;
&lt;/p&gt;
&lt;p&gt;
TAPE: Leveraging Agent Topology for Cooperative Multi-Agent Policy Gradient. (arXiv:2312.15667v2 [cs.MA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.15667
&lt;/p&gt;
&lt;p&gt;
TAPE&#25552;&#20986;&#20102;&#19968;&#31181;&#20195;&#29702;&#25299;&#25169;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#26234;&#33021;&#20307;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#20013;&#30340;&#38598;&#20013;-&#20998;&#25955;&#19981;&#21305;&#37197;&#65288;CDM&#65289;&#38382;&#39064;&#65292;&#36890;&#36807;&#24179;&#34913;&#21512;&#20316;&#21644;&#20943;&#36731;CDM&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#31574;&#30053;&#26799;&#24230;&#65288;MAPG&#65289;&#22312;&#36817;&#24180;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#26368;&#20808;&#36827;&#30340;MAPG&#26041;&#27861;&#20013;&#30340;&#38598;&#20013;&#24335;&#35780;&#35770;&#22120;&#20173;&#28982;&#38754;&#20020;&#38598;&#20013;-&#20998;&#25955;&#19981;&#21305;&#37197;&#65288;CDM&#65289;&#38382;&#39064;&#65292;&#36825;&#24847;&#21619;&#30528;&#19968;&#20123;&#26234;&#33021;&#20307;&#30340;&#27425;&#20248;&#34892;&#21160;&#20250;&#24433;&#21709;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#31574;&#30053;&#23398;&#20064;&#12290;&#34429;&#28982;&#20351;&#29992;&#21333;&#29420;&#30340;&#35780;&#35770;&#22120;&#21487;&#20197;&#36991;&#20813;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#20005;&#37325;&#38480;&#21046;&#20102;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#21327;&#20316;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20195;&#29702;&#25299;&#25169;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20915;&#23450;&#20102;&#22312;&#31574;&#30053;&#26799;&#24230;&#20013;&#26159;&#21542;&#24212;&#32771;&#34385;&#20854;&#20182;&#20195;&#29702;&#65292;&#24182;&#22312;&#20419;&#36827;&#21512;&#20316;&#21644;&#20943;&#36731;CDM&#38382;&#39064;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;&#20195;&#29702;&#25299;&#25169;&#20801;&#35768;&#20195;&#29702;&#20351;&#29992;&#21512;&#20316;&#25928;&#29992;&#20316;&#20026;&#23398;&#20064;&#30446;&#26631;&#65292;&#32780;&#19981;&#26159;&#30001;&#38598;&#20013;&#24335;&#35780;&#35770;&#22120;&#30830;&#23450;&#30340;&#20840;&#23616;&#25928;&#29992;&#25110;&#32773;&#30001;&#20010;&#20307;&#35780;&#35770;&#22120;&#30830;&#23450;&#30340;&#23616;&#37096;&#25928;&#29992;&#12290;&#20026;&#20102;&#26500;&#24314;&#20195;&#29702;&#25299;&#25169;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#31181;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#25299;&#25169;&#30340;&#22810;&#26234;&#33021;&#20307;&#31574;&#30053;&#26799;&#24230;&#65288;TAPE&#65289;&#65292;&#36866;&#29992;&#20110;&#38543;&#26426;&#21644;&#30830;&#23450;&#24615;&#30340;MAPG&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-Agent Policy Gradient (MAPG) has made significant progress in recent years. However, centralized critics in state-of-the-art MAPG methods still face the centralized-decentralized mismatch (CDM) issue, which means sub-optimal actions by some agents will affect other agent's policy learning. While using individual critics for policy updates can avoid this issue, they severely limit cooperation among agents. To address this issue, we propose an agent topology framework, which decides whether other agents should be considered in policy gradient and achieves compromise between facilitating cooperation and alleviating the CDM issue. The agent topology allows agents to use coalition utility as learning objective instead of global utility by centralized critics or local utility by individual critics. To constitute the agent topology, various models are studied. We propose Topology-based multi-Agent Policy gradiEnt (TAPE) for both stochastic and deterministic MAPG methods. We prove the po
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#35780;&#20272;&#26041;&#27861;&#26469;&#27604;&#36739;&#19987;&#26377;LLMs&#21644;&#24320;&#28304;SLMs&#30340;&#26435;&#34913;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#20998;&#26512;&#24037;&#20855;SLaM&#26469;&#27979;&#35797;&#20135;&#21697;&#21151;&#33021;&#12290;&#22312;&#23454;&#38469;&#20135;&#21697;&#21151;&#33021;&#26367;&#25442;&#26102;&#65292;&#23545;&#20110;&#29616;&#26377;&#33021;&#21147;&#26159;&#21542;&#33021;&#22815;&#34987;&#24320;&#28304;SLMs&#20195;&#26367;&#65292;&#36824;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2312.14972</link><description>&lt;p&gt;
&#22312;&#29983;&#20135;&#20013;&#29992;&#24320;&#28304;SLMs&#26367;&#20195;&#19987;&#26377;LLMs&#30340;&#26435;&#34913;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Trade-off Analysis of Replacing Proprietary LLMs with Open Source SLMs in Production. (arXiv:2312.14972v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.14972
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#35780;&#20272;&#26041;&#27861;&#26469;&#27604;&#36739;&#19987;&#26377;LLMs&#21644;&#24320;&#28304;SLMs&#30340;&#26435;&#34913;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#20998;&#26512;&#24037;&#20855;SLaM&#26469;&#27979;&#35797;&#20135;&#21697;&#21151;&#33021;&#12290;&#22312;&#23454;&#38469;&#20135;&#21697;&#21151;&#33021;&#26367;&#25442;&#26102;&#65292;&#23545;&#20110;&#29616;&#26377;&#33021;&#21147;&#26159;&#21542;&#33021;&#22815;&#34987;&#24320;&#28304;SLMs&#20195;&#26367;&#65292;&#36824;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#20844;&#21496;&#20381;&#36182;&#20110;&#31649;&#29702;&#30340;AI&#27169;&#22411;&#30340;API&#65292;&#22914;OpenAI&#30340;GPT-4&#65292;&#20197;&#22312;&#20854;&#20135;&#21697;&#20013;&#21019;&#24314;AI&#22686;&#24378;&#20307;&#39564;&#12290;&#38500;&#20102;&#20351;&#29992;&#20415;&#21033;&#21644;&#32553;&#30701;&#29983;&#20135;&#26102;&#38388;&#30340;&#22909;&#22788;&#22806;&#65292;&#20381;&#36182;&#19987;&#26377;API&#36824;&#20855;&#26377;&#27169;&#22411;&#25511;&#21046;&#12289;&#24615;&#33021;&#21487;&#38752;&#24615;&#12289;&#19978;&#32447;&#21487;&#39044;&#27979;&#24615;&#21644;&#25104;&#26412;&#26041;&#38754;&#30340;&#32570;&#28857;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#24050;&#32463;&#28044;&#29616;&#20102;&#35768;&#22810;&#20379;&#21830;&#19994;&#20351;&#29992;&#30340;&#24320;&#28304;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;SLMs&#65289;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#26367;&#20195;&#29616;&#26377;&#33021;&#21147;&#30340;&#20934;&#22791;&#24773;&#20917;&#23578;&#19981;&#28165;&#26970;&#65292;&#24182;&#19988;&#27809;&#26377;&#29616;&#25104;&#30340;&#31995;&#32479;&#26041;&#27861;&#26469;&#27979;&#35797;&#36825;&#20123;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23545;&#29616;&#20195;&#24320;&#28304;SLMs&#21450;&#20854;&#22312;&#26367;&#20195;&#30495;&#23454;&#20135;&#21697;&#21151;&#33021;&#30340;&#19987;&#26377;LLM APIs&#26102;&#25152;&#20570;&#30340;&#26435;&#34913;&#36827;&#34892;&#20102;&#34920;&#24449;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21517;&#20026;SLaM&#30340;&#33258;&#21160;&#21270;&#20998;&#26512;&#24037;&#20855;&#65292;&#20351;&#24471;&#21487;&#20197;&#23450;&#37327;&#21644;&#23450;&#24615;&#22320;&#27979;&#35797;&#20351;&#29992;&#20219;&#24847;SLMs&#30340;&#20135;&#21697;&#21151;&#33021;&#12290;&#20351;&#29992;SLaM&#65292;&#25105;&#20204;&#23545;&#26426;&#22120;&#20154;&#36827;&#34892;&#20102;&#26816;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many companies rely on APIs of managed AI models such as OpenAI's GPT-4 to create AI-enabled experiences in their products. Along with the benefits of ease of use and shortened time to production, this reliance on proprietary APIs has downsides in terms of model control, performance reliability, up-time predictability, and cost. At the same time, there has been a flurry of open source small language models (SLMs) that have been made available for commercial use. However, their readiness to replace existing capabilities remains unclear, and a systematic approach to test these models is not readily available. In this paper, we present a systematic evaluation methodology for, and characterization of, modern open source SLMs and their trade-offs when replacing a proprietary LLM APIs for a real-world product feature. We have designed SLaM, an automated analysis tool that enables the quantitative and qualitative testing of product features utilizing arbitrary SLMs. Using SLaM, we examine bot
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;Turbulence&#26469;&#31995;&#32479;&#35780;&#20272;&#38024;&#23545;&#20195;&#30721;&#29983;&#25104;&#30340;&#25351;&#20196;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#27491;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#26500;&#24314;&#19968;&#32452;&#38382;&#39064;&#27169;&#26495;&#65292;&#21487;&#20197;&#35780;&#20272;LLMs&#22312;&#35299;&#20915;&#30456;&#20284;&#32534;&#31243;&#38382;&#39064;&#26102;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#21457;&#29616;&#20854;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#30340;&#32570;&#38519;&#21644;&#24322;&#24120;&#24773;&#20917;&#12290;&#36825;&#39033;&#30740;&#31350;&#22312;&#20116;&#20010;LLMs&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2312.14856</link><description>&lt;p&gt;
&#31995;&#32479;&#21270;&#21644;&#33258;&#21160;&#21270;&#27979;&#35797;&#38024;&#23545;&#20195;&#30721;&#30340;&#25351;&#20196;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28065;&#27969;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Turbulence: Systematically and Automatically Testing Instruction-Tuned Large Language Models for Code. (arXiv:2312.14856v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.14856
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;Turbulence&#26469;&#31995;&#32479;&#35780;&#20272;&#38024;&#23545;&#20195;&#30721;&#29983;&#25104;&#30340;&#25351;&#20196;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#27491;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#26500;&#24314;&#19968;&#32452;&#38382;&#39064;&#27169;&#26495;&#65292;&#21487;&#20197;&#35780;&#20272;LLMs&#22312;&#35299;&#20915;&#30456;&#20284;&#32534;&#31243;&#38382;&#39064;&#26102;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#21457;&#29616;&#20854;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#30340;&#32570;&#38519;&#21644;&#24322;&#24120;&#24773;&#20917;&#12290;&#36825;&#39033;&#30740;&#31350;&#22312;&#20116;&#20010;LLMs&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;Turbulence&#65292;&#31995;&#32479;&#35780;&#20272;&#38024;&#23545;&#20195;&#30721;&#29983;&#25104;&#30340;&#25351;&#20196;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#27491;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#12290;Turbulence&#21253;&#21547;&#19968;&#32452;&#22823;&#37327;&#30340;&#33258;&#28982;&#35821;&#35328;&#8220;&#38382;&#39064;&#27169;&#26495;&#8221;&#65292;&#27599;&#20010;&#27169;&#26495;&#37117;&#26159;&#19968;&#20010;&#32534;&#31243;&#38382;&#39064;&#65292;&#21442;&#25968;&#21270;&#20351;&#24471;&#21487;&#20197;&#20197;&#22810;&#31181;&#19981;&#21516;&#24418;&#24335;&#25552;&#38382;&#12290;&#27599;&#20010;&#38382;&#39064;&#27169;&#26495;&#37117;&#26377;&#19968;&#20010;&#30456;&#20851;&#30340;&#8220;&#27979;&#35797;&#39044;&#27979;&#22120;&#8221;&#65292;&#29992;&#26469;&#21028;&#26029;LLM&#36820;&#22238;&#30340;&#20195;&#30721;&#35299;&#20915;&#26041;&#26696;&#26159;&#21542;&#27491;&#30830;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#19968;&#20010;&#38382;&#39064;&#27169;&#26495;&#65292;&#21487;&#20197;&#21521;LLM&#25552;&#38382;&#19968;&#20010;&#38750;&#24120;&#30456;&#20284;&#30340;&#32534;&#31243;&#38382;&#39064;&#8220;&#37051;&#22495;&#8221;&#65292;&#24182;&#35780;&#20272;&#27599;&#20010;&#38382;&#39064;&#36820;&#22238;&#30340;&#32467;&#26524;&#30340;&#27491;&#30830;&#24615;&#12290;&#36825;&#20801;&#35768;&#35782;&#21035;LLM&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#30340;&#24046;&#36317;&#65292;&#21253;&#25324;LLM&#22312;&#37051;&#22495;&#20013;&#35299;&#20915;&#8220;&#20960;&#20046;&#25152;&#26377;&#8221;&#38382;&#39064;&#20294;&#23545;&#29305;&#23450;&#21442;&#25968;&#23454;&#20363;&#21270;&#22833;&#36133;&#30340;&#8220;&#24322;&#24120;&#8221;&#12290;&#25105;&#20204;&#38024;&#23545;OpenAI&#12289;Co&#31561;&#20116;&#20010;LLM&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a method for systematically evaluating the correctness and robustness of instruction-tuned large language models (LLMs) for code generation via a new benchmark, Turbulence. Turbulence consists of a large set of natural language $\textit{question templates}$, each of which is a programming problem, parameterised so that it can be asked in many different forms. Each question template has an associated $\textit{test oracle}$ that judges whether a code solution returned by an LLM is correct. Thus, from a single question template, it is possible to ask an LLM a $\textit{neighbourhood}$ of very similar programming questions, and assess the correctness of the result returned for each question. This allows gaps in an LLM's code generation abilities to be identified, including $\textit{anomalies}$ where the LLM correctly solves $\textit{almost all}$ questions in a neighbourhood but fails for particular parameter instantiations. We present experiments against five LLMs from OpenAI, Co
&lt;/p&gt;</description></item><item><title>WellFactor&#26159;&#19968;&#31181;&#20351;&#29992;&#32508;&#21512;&#23884;&#20837;&#21307;&#30103;&#25968;&#25454;&#30340;&#24739;&#32773;&#20998;&#31867;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#21463;&#32422;&#26463;&#30340;&#20302;&#31209;&#36924;&#36817;&#12289;&#32467;&#21512;&#26631;&#31614;&#20449;&#24687;&#26469;&#20248;&#21270;&#23884;&#20837;&#32467;&#26524;&#65292;&#21516;&#26102;&#20855;&#26377;&#21363;&#26102;&#35745;&#31639;&#26032;&#25968;&#25454;&#23884;&#20837;&#30340;&#29305;&#28857;&#12290;&#22312;&#23454;&#38469;&#21307;&#30103;&#25968;&#25454;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2312.14129</link><description>&lt;p&gt;
WellFactor:&#20351;&#29992;&#32508;&#21512;&#23884;&#20837;&#21307;&#30103;&#25968;&#25454;&#30340;&#24739;&#32773;&#20998;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
WellFactor: Patient Profiling using Integrative Embedding of Healthcare Data. (arXiv:2312.14129v1 [cs.LG] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.14129
&lt;/p&gt;
&lt;p&gt;
WellFactor&#26159;&#19968;&#31181;&#20351;&#29992;&#32508;&#21512;&#23884;&#20837;&#21307;&#30103;&#25968;&#25454;&#30340;&#24739;&#32773;&#20998;&#31867;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#21463;&#32422;&#26463;&#30340;&#20302;&#31209;&#36924;&#36817;&#12289;&#32467;&#21512;&#26631;&#31614;&#20449;&#24687;&#26469;&#20248;&#21270;&#23884;&#20837;&#32467;&#26524;&#65292;&#21516;&#26102;&#20855;&#26377;&#21363;&#26102;&#35745;&#31639;&#26032;&#25968;&#25454;&#23884;&#20837;&#30340;&#29305;&#28857;&#12290;&#22312;&#23454;&#38469;&#21307;&#30103;&#25968;&#25454;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24555;&#36895;&#21457;&#23637;&#30340;&#21307;&#30103;&#34892;&#19994;&#20013;&#65292;&#24179;&#21488;&#29616;&#22312;&#19981;&#20165;&#21487;&#20197;&#35775;&#38382;&#20256;&#32479;&#30340;&#21307;&#30103;&#35760;&#24405;&#65292;&#36824;&#21487;&#20197;&#33719;&#21462;&#28085;&#30422;&#21508;&#31181;&#24739;&#32773;&#20114;&#21160;&#30340;&#21508;&#31181;&#25968;&#25454;&#38598;&#65292;&#20363;&#22914;&#26469;&#33258;&#21307;&#30103;&#32593;&#31449;&#30340;&#25968;&#25454;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#31181;&#20016;&#23500;&#30340;&#25968;&#25454;&#22810;&#26679;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;WellFactor&#65306;&#19968;&#31181;&#36890;&#36807;&#25972;&#21512;&#36825;&#20123;&#26469;&#28304;&#20449;&#24687;&#26469;&#24471;&#20986;&#24739;&#32773;&#20998;&#31867;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#21033;&#29992;&#21463;&#32422;&#26463;&#30340;&#20302;&#31209;&#36924;&#36817;&#12290;WellFactor&#34987;&#20248;&#21270;&#20026;&#22788;&#29702;&#21307;&#30103;&#25968;&#25454;&#20013;&#32463;&#24120;&#23384;&#22312;&#30340;&#31232;&#30095;&#24615;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#32467;&#21512;&#29305;&#23450;&#20219;&#21153;&#30340;&#26631;&#31614;&#20449;&#24687;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25913;&#36827;&#20102;&#23884;&#20837;&#32467;&#26524;&#65292;&#25552;&#20379;&#20102;&#26356;&#21152;&#26126;&#26234;&#30340;&#24739;&#32773;&#35270;&#35282;&#12290;WellFactor&#30340;&#19968;&#20010;&#37325;&#35201;&#29305;&#28857;&#26159;&#33021;&#22815;&#21363;&#26102;&#35745;&#31639;&#26032;&#30340;&#12289;&#20197;&#21069;&#26410;&#35266;&#23519;&#21040;&#30340;&#24739;&#32773;&#25968;&#25454;&#30340;&#23884;&#20837;&#65292;&#28040;&#38500;&#20102;&#37325;&#26032;&#35775;&#38382;&#25972;&#20010;&#25968;&#25454;&#38598;&#25110;&#37325;&#26032;&#35745;&#31639;&#23884;&#20837;&#30340;&#38656;&#35201;&#12290;&#23545;&#23454;&#38469;&#21307;&#30103;&#25968;&#25454;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#35777;&#26126;&#20102;WellFactor&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the rapidly evolving healthcare industry, platforms now have access to not only traditional medical records, but also diverse data sets encompassing various patient interactions, such as those from healthcare web portals. To address this rich diversity of data, we introduce WellFactor: a method that derives patient profiles by integrating information from these sources. Central to our approach is the utilization of constrained low-rank approximation. WellFactor is optimized to handle the sparsity that is often inherent in healthcare data. Moreover, by incorporating task-specific label information, our method refines the embedding results, offering a more informed perspective on patients. One important feature of WellFactor is its ability to compute embeddings for new, previously unobserved patient data instantaneously, eliminating the need to revisit the entire data set or recomputing the embedding. Comprehensive evaluations on real-world healthcare data demonstrate WellFactor's eff
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#21463;&#38480;&#29627;&#23572;&#20857;&#26364;&#26426;&#36827;&#34892;&#22270;&#20687;&#32858;&#31867;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;RBMs&#23558;&#22270;&#20687;&#36716;&#25442;&#20026;&#22270;&#20687;&#23884;&#20837;&#65292;&#24182;&#20351;&#29992;&#33258;&#24213;&#21521;&#19978;&#30340;&#32858;&#31867;&#25216;&#26415;&#36827;&#34892;&#32858;&#31867;&#12290;&#20026;&#20102;&#24212;&#23545;&#26377;&#38480;&#30340;&#27979;&#35797;&#22270;&#20687;&#25968;&#25454;&#65292;&#25552;&#20986;&#20102;AHC-RBM&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#36890;&#29992;&#30340;RBM&#27169;&#22411;&#21644;&#36866;&#24212;&#24615;RBM&#27169;&#22411;&#26469;&#29983;&#25104;RBM&#21521;&#37327;&#65292;&#20174;&#32780;&#23454;&#29616;&#26377;&#25928;&#30340;&#22270;&#20687;&#32858;&#31867;&#12290;</title><link>http://arxiv.org/abs/2312.13845</link><description>&lt;p&gt;
&#20351;&#29992;&#21463;&#38480;&#29627;&#23572;&#20857;&#26364;&#26426;&#30340;&#22270;&#20687;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Image Clustering using Restricted Boltzman Machine. (arXiv:2312.13845v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.13845
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#21463;&#38480;&#29627;&#23572;&#20857;&#26364;&#26426;&#36827;&#34892;&#22270;&#20687;&#32858;&#31867;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;RBMs&#23558;&#22270;&#20687;&#36716;&#25442;&#20026;&#22270;&#20687;&#23884;&#20837;&#65292;&#24182;&#20351;&#29992;&#33258;&#24213;&#21521;&#19978;&#30340;&#32858;&#31867;&#25216;&#26415;&#36827;&#34892;&#32858;&#31867;&#12290;&#20026;&#20102;&#24212;&#23545;&#26377;&#38480;&#30340;&#27979;&#35797;&#22270;&#20687;&#25968;&#25454;&#65292;&#25552;&#20986;&#20102;AHC-RBM&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#36890;&#29992;&#30340;RBM&#27169;&#22411;&#21644;&#36866;&#24212;&#24615;RBM&#27169;&#22411;&#26469;&#29983;&#25104;RBM&#21521;&#37327;&#65292;&#20174;&#32780;&#23454;&#29616;&#26377;&#25928;&#30340;&#22270;&#20687;&#32858;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#31181;&#39564;&#35777;&#31995;&#32479;&#20013;&#65292;&#21463;&#38480;&#29627;&#23572;&#20857;&#26364;&#26426;&#65288;RBMs&#65289;&#24050;&#32463;&#35777;&#26126;&#20102;&#22312;&#21069;&#31471;&#21644;&#21518;&#31471;&#22788;&#29702;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;RBMs&#36827;&#34892;&#22270;&#20687;&#32858;&#31867;&#20219;&#21153;&#12290;RBMs&#34987;&#35757;&#32451;&#25104;&#23558;&#22270;&#20687;&#36716;&#25442;&#20026;&#22270;&#20687;&#23884;&#20837;&#12290;&#25105;&#20204;&#37319;&#29992;&#20256;&#32479;&#30340;&#33258;&#24213;&#21521;&#19978;&#30340;&#32858;&#31867;&#65288;AHC&#65289;&#25216;&#26415;&#12290;&#20026;&#20102;&#35299;&#20915;&#26377;&#38480;&#30340;&#27979;&#35797;&#33080;&#37096;&#22270;&#20687;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#21463;&#38480;&#29627;&#23572;&#20857;&#26364;&#26426;&#30340;&#33258;&#24213;&#21521;&#19978;&#32858;&#31867;&#22270;&#20687;&#32858;&#31867;&#26041;&#27861;&#65288;AHC-RBM&#65289;&#30340;&#20004;&#20010;&#20027;&#35201;&#27493;&#39588;&#12290;&#39318;&#20808;&#65292;&#20351;&#29992;&#25152;&#26377;&#21487;&#29992;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#35757;&#32451;&#19968;&#20010;&#36890;&#29992;&#30340;RBM&#27169;&#22411;&#12290;&#38543;&#21518;&#65292;&#20351;&#29992;&#27599;&#20010;&#27979;&#35797;&#22270;&#20687;&#30340;&#25968;&#25454;&#35757;&#32451;&#19968;&#20010;&#36866;&#24212;&#24615;RBM&#27169;&#22411;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#36830;&#25509;&#36825;&#20123;&#36866;&#24212;&#24615;&#27169;&#22411;&#30340;&#21487;&#35265;&#21040;&#38544;&#34255;&#26435;&#37325;&#30697;&#38453;&#21644;&#20559;&#32622;&#21521;&#37327;&#29983;&#25104;RBM&#21521;&#37327;&#65292;&#36825;&#20123;&#21521;&#37327;&#26377;&#25928;&#22320;&#20445;&#30041;&#20102;&#31867;&#21035;&#29305;&#23450;&#30340;&#20449;&#24687;&#65292;&#24182;&#22312;&#22270;&#20687;&#32858;&#31867;&#20013;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
In various verification systems, Restricted Boltzmann Machines (RBMs) have demonstrated their efficacy in both front-end and back-end processes. In this work, we propose the use of RBMs to the image clustering tasks. RBMs are trained to convert images into image embeddings. We employ the conventional bottom-up Agglomerative Hierarchical Clustering (AHC) technique. To address the challenge of limited test face image data, we introduce Agglomerative Hierarchical Clustering based Method for Image Clustering using Restricted Boltzmann Machine (AHC-RBM) with two major steps. Initially, a universal RBM model is trained using all available training dataset. Subsequently, we train an adapted RBM model using the data from each test image. Finally, RBM vectors which is the embedding vector is generated by concatenating the visible-to-hidden weight matrices of these adapted models, and the bias vectors. These vectors effectively preserve class-specific information and are utilized in image cluste
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#20132;&#20114;&#24335;&#23545;&#27604;&#23398;&#20064;&#30340;&#30693;&#35782;&#22270;&#35889;&#38169;&#35823;&#26816;&#27979;&#27169;&#22411;CCA&#65292;&#36890;&#36807;&#32508;&#21512;&#25991;&#26412;&#21644;&#22270;&#32467;&#26500;&#20449;&#24687;&#65292;&#26356;&#22909;&#22320;&#21306;&#20998;&#35821;&#20041;&#65292;&#20174;&#32780;&#22312;&#26816;&#27979;&#22122;&#22768;&#26041;&#38754;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2312.12108</link><description>&lt;p&gt;
&#20351;&#29992;&#23545;&#27604;&#32622;&#20449;&#24230;&#35843;&#25972;&#30340;&#30693;&#35782;&#22270;&#35889;&#38169;&#35823;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph Error Detection with Contrastive Confidence Adaption. (arXiv:2312.12108v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.12108
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#20132;&#20114;&#24335;&#23545;&#27604;&#23398;&#20064;&#30340;&#30693;&#35782;&#22270;&#35889;&#38169;&#35823;&#26816;&#27979;&#27169;&#22411;CCA&#65292;&#36890;&#36807;&#32508;&#21512;&#25991;&#26412;&#21644;&#22270;&#32467;&#26500;&#20449;&#24687;&#65292;&#26356;&#22909;&#22320;&#21306;&#20998;&#35821;&#20041;&#65292;&#20174;&#32780;&#22312;&#26816;&#27979;&#22122;&#22768;&#26041;&#38754;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#20013;&#24120;&#24120;&#21253;&#21547;&#21508;&#31181;&#38169;&#35823;&#12290;&#20197;&#24448;&#20851;&#20110;KG&#38169;&#35823;&#26816;&#27979;&#30340;&#30740;&#31350;&#20027;&#35201;&#20381;&#36182;&#20110;&#20174;&#22270;&#32467;&#26500;&#20013;&#23884;&#20837;&#30340;&#19977;&#20803;&#32452;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#21457;&#29616;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#21306;&#20998;&#22122;&#22768;&#21644;&#35821;&#20041;&#30456;&#20284;&#30340;&#27491;&#30830;&#19977;&#20803;&#32452;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;KG&#38169;&#35823;&#26816;&#27979;&#27169;&#22411;CCA&#65292;&#36890;&#36807;&#20174;&#19977;&#20803;&#32452;&#37325;&#26500;&#20013;&#32508;&#21512;&#25991;&#26412;&#21644;&#22270;&#32467;&#26500;&#20449;&#24687;&#65292;&#26356;&#22909;&#22320;&#21306;&#20998;&#35821;&#20041;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#20132;&#20114;&#24335;&#23545;&#27604;&#23398;&#20064;&#26469;&#25429;&#25417;&#25991;&#26412;&#21644;&#32467;&#26500;&#27169;&#24335;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#20855;&#26377;&#35821;&#20041;&#30456;&#20284;&#22122;&#22768;&#21644;&#23545;&#25239;&#24615;&#22122;&#22768;&#30340;&#23454;&#38469;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CCA&#22312;&#26816;&#27979;&#35821;&#20041;&#30456;&#20284;&#22122;&#22768;&#21644;&#23545;&#25239;&#24615;&#22122;&#22768;&#26041;&#38754;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graphs (KGs) often contain various errors. Previous works on detecting errors in KGs mainly rely on triplet embedding from graph structure. We conduct an empirical study and find that these works struggle to discriminate noise from semantically-similar correct triplets. In this paper, we propose a KG error detection model CCA to integrate both textual and graph structural information from triplet reconstruction for better distinguishing semantics. We design interactive contrastive learning to capture the differences between textual and structural patterns. Furthermore, we construct realistic datasets with semantically-similar noise and adversarial noise. Experimental results demonstrate that CCA outperforms state-of-the-art baselines, especially in detecting semantically-similar noise and adversarial noise.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#20013;&#26377;&#35760;&#24518;&#30165;&#36857;&#65292;&#23481;&#26131;&#21463;&#21040;&#25968;&#25454;&#25552;&#21462;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2312.11658</link><description>&lt;p&gt;
&#12298;&#20195;&#30721;&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35760;&#24518;&#30165;&#36857;&#12299;
&lt;/p&gt;
&lt;p&gt;
Traces of Memorisation in Large Language Models for Code. (arXiv:2312.11658v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.11658
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#20013;&#26377;&#35760;&#24518;&#30165;&#36857;&#65292;&#23481;&#26131;&#21463;&#21040;&#25968;&#25454;&#25552;&#21462;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22240;&#20854;&#29983;&#25104;&#31867;&#20284;&#20154;&#31867;&#25991;&#26412;&#30340;&#33021;&#21147;&#21644;&#22312;&#36719;&#20214;&#24037;&#31243;&#31561;&#39046;&#22495;&#30340;&#28508;&#22312;&#24212;&#29992;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#20195;&#30721;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#26159;&#22312;&#20174;&#20114;&#32852;&#32593;&#19978;&#25235;&#21462;&#30340;&#22823;&#35268;&#27169;&#26410;&#32463;&#36807;&#28388;&#30340;&#28304;&#20195;&#30721;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#36825;&#20123;&#25968;&#25454;&#38598;&#30340;&#20869;&#23481;&#34987;&#35760;&#20303;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#25968;&#25454;&#25552;&#21462;&#25915;&#20987;&#34987;&#25915;&#20987;&#32773;&#25552;&#21462;&#20986;&#26469;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22823;&#22411;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35760;&#24518;&#30165;&#36857;&#65292;&#24182;&#23558;&#35760;&#24518;&#30165;&#36857;&#29575;&#19982;&#38024;&#23545;&#33258;&#28982;&#35821;&#35328;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#20010;&#29616;&#26377;&#30340;&#33258;&#28982;&#35821;&#35328;&#22522;&#20934;&#65292;&#24182;&#36890;&#36807;&#35782;&#21035;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#30340;&#26679;&#26412;&#26500;&#24314;&#20102;&#19968;&#20010;&#20195;&#30721;&#22522;&#20934;&#12290;&#25105;&#20204;&#36816;&#34892;&#20102;&#20004;&#20010;&#22522;&#20934;&#23545;&#22810;&#31181;&#27169;&#22411;&#36827;&#34892;&#27979;&#35797;&#65292;&#24182;&#36827;&#34892;&#20102;&#25968;&#25454;&#25552;&#21462;&#25915;&#20987;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20195;&#30721;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20687;&#23427;&#20204;&#30340;&#33258;&#28982;&#35821;&#35328;&#23545;&#24212;&#29289;&#19968;&#26679;&#23481;&#26131;&#21463;&#21040;&#25968;&#25454;&#25552;&#21462;&#25915;&#20987;&#12290;&#20174;&#34987;&#30830;&#23450;&#20026;&#28508;&#22312;&#21463;&#25915;&#20987;&#30340;&#35757;&#32451;&#25968;&#25454;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;...
&lt;/p&gt;
&lt;p&gt;
Large language models have gained significant popularity because of their ability to generate human-like text and potential applications in various fields, such as Software Engineering. Large language models for code are commonly trained on large unsanitised corpora of source code scraped from the internet. The content of these datasets is memorised and can be extracted by attackers with data extraction attacks. In this work, we explore memorisation in large language models for code and compare the rate of memorisation with large language models trained on natural language. We adopt an existing benchmark for natural language and construct a benchmark for code by identifying samples that are vulnerable to attack. We run both benchmarks against a variety of models, and perform a data extraction attack. We find that large language models for code are vulnerable to data extraction attacks, like their natural language counterparts. From the training data that was identified to be potentiall
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"&#21407;&#25991;&#25913;&#20889;"&#30340;&#20219;&#21153;&#26469;&#22788;&#29702;&#38271;&#25991;&#26412;&#38382;&#31572;&#65292;&#36890;&#36807;&#20302;&#25104;&#26412;&#39640;&#25928;&#30340;&#26041;&#27861;&#25104;&#21151;&#25193;&#23637;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#33267;32k&#65292;&#24182;&#22312;&#22810;&#25991;&#26723;&#38382;&#31572;&#20013;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2312.11193</link><description>&lt;p&gt;
"&#21407;&#25991;&#25913;&#20889;"&#25552;&#39640;&#20102;&#39640;&#31934;&#24230;&#38271;&#25991;&#26412;&#38382;&#31572;&#30340;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
"Paraphrasing The Original Text" Makes High Accuracy Long-Context QA. (arXiv:2312.11193v6 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.11193
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"&#21407;&#25991;&#25913;&#20889;"&#30340;&#20219;&#21153;&#26469;&#22788;&#29702;&#38271;&#25991;&#26412;&#38382;&#31572;&#65292;&#36890;&#36807;&#20302;&#25104;&#26412;&#39640;&#25928;&#30340;&#26041;&#27861;&#25104;&#21151;&#25193;&#23637;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#33267;32k&#65292;&#24182;&#22312;&#22810;&#25991;&#26723;&#38382;&#31572;&#20013;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#38754;&#23545;&#38271;&#25991;&#26412;&#26102;&#65292;&#22823;&#22810;&#25968;&#24320;&#28304;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#38480;&#21046;&#22312;4k&#20197;&#20869;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#33021;&#21147;&#12290;&#21363;&#20351;&#26159;&#20855;&#26377;&#26356;&#38271;&#19978;&#19979;&#25991;&#31383;&#21475;&#30340;&#27169;&#22411;&#20063;&#26080;&#27861;&#22312;&#38271;&#19978;&#19979;&#25991;&#38382;&#39064;&#19978;&#20445;&#35777;&#20196;&#20154;&#28385;&#24847;&#30340;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20174;&#35757;&#32451;&#25968;&#25454;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#25552;&#39640;&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#33021;&#21147;&#38656;&#35201;&#30340;&#26159;"&#26377;&#25928;"&#32780;&#19981;&#20165;&#20165;&#26159;"&#38271;"&#30340;&#25968;&#25454;&#12290;&#22522;&#20110;&#36825;&#20010;&#27934;&#35265;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;"&#21407;&#25991;&#25913;&#20889;"&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#19968;&#31181;&#20302;&#25104;&#26412;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#25104;&#21151;&#23558;&#29616;&#26377;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#25193;&#23637;&#21040;32k&#12290;&#25105;&#20204;&#30340;&#24494;&#35843;&#27169;&#22411;&#22312;&#20855;&#26377;&#30456;&#36817;&#35268;&#27169;&#30340;&#27169;&#22411;&#20013;&#22312;&#22810;&#25991;&#26723;&#38382;&#31572;&#26041;&#38754;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#12290;&#27169;&#22411;&#21644;&#35757;&#32451;&#25968;&#25454;&#24050;&#32463;&#22312;HuggingFace&#65288;https://huggingface.co/yuyijiong/Qwen-14b-chat-yarn-32k&#65289;&#21644;WiseModel&#65288;https://wisemodel.cn/models/yuyijiong/Qwen-14b-chat-yarn-32k&#65289;&#19978;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most open-source generative language models currently have a context window of no more than 4k, limiting their ability when facing long text. Even models with longer context windows cannot guarantee satisfactory accuracy on long-context problems. To tackle this issue, we explore from the perspective of training data and theoretically demonstrate that improving the capability to handle long contexts requires "effective" rather than simply "long" data. Based on this insight, we propose using the "original text paraphrasing" task and successfully extend the context window of existing models to 32k through a low-cost and effective method. Our fine-tuned model achieves state-of-the-art accuracy in multi-document-QA among models of comparable scale. The model and training data have been made available on HuggingFace(https://huggingface.co/yuyijiong/Qwen-14b-chat-yarn-32k) and WiseModel(https://wisemodel.cn/models/yuyijiong/Qwen-14b-chat-yarn-32k).
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22330;&#26223;&#22270;&#30693;&#35782;&#25512;&#36827;&#20102;&#25163;&#26415;&#29615;&#22659;&#20013;&#30340;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#65292;&#35299;&#20915;&#20102;&#25163;&#26415;VQA&#31995;&#32479;&#20013;&#30340;&#38382;&#39064;&#26465;&#20214;&#20559;&#20506;&#21644;&#32570;&#20047;&#22330;&#26223;&#24863;&#30693;&#25512;&#29702;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2312.10251</link><description>&lt;p&gt;
&#36890;&#36807;&#22330;&#26223;&#22270;&#30693;&#35782;&#25512;&#36827;&#22806;&#31185;&#35270;&#35273;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Advancing Surgical VQA with Scene Graph Knowledge. (arXiv:2312.10251v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.10251
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22330;&#26223;&#22270;&#30693;&#35782;&#25512;&#36827;&#20102;&#25163;&#26415;&#29615;&#22659;&#20013;&#30340;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#65292;&#35299;&#20915;&#20102;&#25163;&#26415;VQA&#31995;&#32479;&#20013;&#30340;&#38382;&#39064;&#26465;&#20214;&#20559;&#20506;&#21644;&#32570;&#20047;&#22330;&#26223;&#24863;&#30693;&#25512;&#29702;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#25163;&#26415;&#23460;&#36234;&#26469;&#36234;&#22797;&#26434;&#65292;&#38656;&#35201;&#21019;&#26032;&#30340;&#26415;&#20013;&#25903;&#25345;&#31995;&#32479;&#12290;&#23613;&#31649;&#22806;&#31185;&#25968;&#25454;&#31185;&#23398;&#30340;&#37325;&#28857;&#20027;&#35201;&#22312;&#20110;&#35270;&#39057;&#20998;&#26512;&#65292;&#20294;&#23558;&#22806;&#31185;&#35745;&#31639;&#26426;&#35270;&#35273;&#19982;&#35821;&#35328;&#33021;&#21147;&#30456;&#32467;&#21512;&#25104;&#20026;&#24517;&#35201;&#30340;&#36235;&#21183;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26088;&#22312;&#36890;&#36807;&#22330;&#26223;&#22270;&#30693;&#35782;&#25512;&#36827;&#25163;&#26415;&#29615;&#22659;&#20013;&#30340;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#65292;&#35299;&#20915;&#24403;&#21069;&#25163;&#26415;VQA&#31995;&#32479;&#20013;&#30340;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#28040;&#38500;&#25163;&#26415;VQA&#25968;&#25454;&#38598;&#20013;&#30340;&#38382;&#39064;&#26465;&#20214;&#20559;&#20506;&#65292;&#20197;&#21450;&#22312;&#25163;&#26415;VQA&#27169;&#22411;&#35774;&#35745;&#20013;&#34701;&#20837;&#22330;&#26223;&#24863;&#30693;&#25512;&#29702;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25163;&#26415;&#22330;&#26223;&#22270;&#30340;&#25968;&#25454;&#38598;SSG-QA&#65292;&#36890;&#36807;&#22312;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#24212;&#29992;&#20998;&#21106;&#21644;&#26816;&#27979;&#27169;&#22411;&#26469;&#29983;&#25104;&#12290;&#25105;&#20204;&#20351;&#29992;&#20202;&#22120;&#21644;&#35299;&#21078;&#32467;&#26500;&#30340;&#31354;&#38388;&#21644;&#21160;&#20316;&#20449;&#24687;&#26500;&#24314;&#25163;&#26415;&#22330;&#26223;&#22270;&#12290;&#36825;&#20123;&#22270;&#34987;&#36755;&#20837;&#21040;&#19968;&#20010;&#38382;&#39064;&#24341;&#25806;&#20013;&#65292;&#20135;&#29983;&#22810;&#26679;&#21270;&#30340;&#38382;&#31572;&#23545;&#12290;&#25105;&#20204;&#30340;SSG-QA&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#19968;&#20010;&#26356;&#22797;&#26434;&#12289;&#22810;&#26679;&#21270;&#12289;&#20960;&#20309;&#22522;&#30784;&#12289;&#26080;&#20559;&#20506;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern operating room is becoming increasingly complex, requiring innovative intra-operative support systems. While the focus of surgical data science has largely been on video analysis, integrating surgical computer vision with language capabilities is emerging as a necessity. Our work aims to advance Visual Question Answering (VQA) in the surgical context with scene graph knowledge, addressing two main challenges in the current surgical VQA systems: removing question-condition bias in the surgical VQA dataset and incorporating scene-aware reasoning in the surgical VQA model design. First, we propose a Surgical Scene Graph-based dataset, SSG-QA, generated by employing segmentation and detection models on publicly available datasets. We build surgical scene graphs using spatial and action information of instruments and anatomies. These graphs are fed into a question engine, generating diverse QA pairs. Our SSG-QA dataset provides a more complex, diverse, geometrically grounded, unbiase
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;&#27604;&#29305;&#24449;&#37325;&#26500;&#21644;&#32858;&#21512;&#65288;CARAT&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#27169;&#24577;&#22810;&#26631;&#31614;&#24773;&#32490;&#35782;&#21035;&#12290;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#27169;&#24577;&#20998;&#31163;&#21644;&#26631;&#31614;&#29305;&#23450;&#30340;&#29305;&#24449;&#65292;CARAT&#33021;&#26356;&#22909;&#22320;&#24314;&#27169;&#32454;&#31890;&#24230;&#30340;&#27169;&#24577;&#23545;&#26631;&#31614;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2312.10201</link><description>&lt;p&gt;
CARAT: &#23545;&#20110;&#22810;&#27169;&#24577;&#22810;&#26631;&#31614;&#24773;&#32490;&#35782;&#21035;&#30340;&#23545;&#27604;&#29305;&#24449;&#37325;&#26500;&#21644;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
CARAT: Contrastive Feature Reconstruction and Aggregation for Multi-Modal Multi-Label Emotion Recognition. (arXiv:2312.10201v3 [cs.MM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.10201
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;&#27604;&#29305;&#24449;&#37325;&#26500;&#21644;&#32858;&#21512;&#65288;CARAT&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#27169;&#24577;&#22810;&#26631;&#31614;&#24773;&#32490;&#35782;&#21035;&#12290;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#27169;&#24577;&#20998;&#31163;&#21644;&#26631;&#31614;&#29305;&#23450;&#30340;&#29305;&#24449;&#65292;CARAT&#33021;&#26356;&#22909;&#22320;&#24314;&#27169;&#32454;&#31890;&#24230;&#30340;&#27169;&#24577;&#23545;&#26631;&#31614;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22810;&#26631;&#31614;&#24773;&#32490;&#35782;&#21035;&#26088;&#22312;&#20174;&#22810;&#20010;&#27169;&#24577;&#20013;&#35782;&#21035;&#30456;&#20851;&#24773;&#32490;&#12290;MMER&#30340;&#25361;&#25112;&#22312;&#20110;&#22914;&#20309;&#26377;&#25928;&#22320;&#25429;&#25417;&#26469;&#33258;&#24322;&#26500;&#25968;&#25454;&#30340;&#22810;&#20010;&#26631;&#31614;&#30340;&#21028;&#21035;&#29305;&#24449;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20027;&#35201;&#33268;&#21147;&#20110;&#25506;&#32034;&#21508;&#31181;&#34701;&#21512;&#31574;&#30053;&#65292;&#23558;&#22810;&#27169;&#24577;&#20449;&#24687;&#38598;&#25104;&#21040;&#32479;&#19968;&#34920;&#31034;&#20013;&#30340;&#25152;&#26377;&#26631;&#31614;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#23398;&#20064;&#26041;&#26696;&#19981;&#20165;&#24573;&#35270;&#20102;&#27599;&#31181;&#27169;&#24577;&#30340;&#29305;&#27530;&#24615;&#65292;&#20063;&#26080;&#27861;&#25429;&#25417;&#19981;&#21516;&#26631;&#31614;&#30340;&#20010;&#21035;&#21028;&#21035;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#26631;&#31614;&#21644;&#27169;&#24577;&#30340;&#20381;&#36182;&#20851;&#31995;&#20063;&#26080;&#27861;&#26377;&#25928;&#24314;&#27169;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;MMER&#20219;&#21153;&#30340;&#23545;&#27604;&#29305;&#24449;&#37325;&#26500;&#21644;&#32858;&#21512;&#65288;CARAT&#65289;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#37325;&#26500;&#30340;&#34701;&#21512;&#26426;&#21046;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#27169;&#24577;&#20998;&#31163;&#21644;&#26631;&#31614;&#29305;&#23450;&#30340;&#29305;&#24449;&#65292;&#26356;&#22909;&#22320;&#24314;&#27169;&#32454;&#31890;&#24230;&#30340;&#27169;&#24577;&#23545;&#26631;&#31614;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-modal multi-label emotion recognition (MMER) aims to identify relevant emotions from multiple modalities. The challenge of MMER is how to effectively capture discriminative features for multiple labels from heterogeneous data. Recent studies are mainly devoted to exploring various fusion strategies to integrate multi-modal information into a unified representation for all labels. However, such a learning scheme not only overlooks the specificity of each modality but also fails to capture individual discriminative features for different labels. Moreover, dependencies of labels and modalities cannot be effectively modeled. To address these issues, this paper presents ContrAstive feature Reconstruction and AggregaTion (CARAT) for the MMER task. Specifically, we devise a reconstruction-based fusion mechanism to better model fine-grained modality-to-label dependencies by contrastively learning modal-separated and label-specific features. To further exploit the modality complementarity
&lt;/p&gt;</description></item><item><title>&#24418;&#24577;&#23398;&#29305;&#24449;&#20998;&#26512;&#22312;&#34920;&#22411;&#33647;&#29289;&#21457;&#29616;&#20013;&#20855;&#26377;&#37325;&#35201;&#20215;&#20540;&#12290;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#20998;&#26512;&#22823;&#35268;&#27169;&#39640;&#20869;&#23481;&#22270;&#20687;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20419;&#36827;&#20102;&#33647;&#29289;&#20316;&#29992;&#26426;&#21046;&#30340;&#29702;&#35299;&#21644;&#26032;&#27835;&#30103;&#26041;&#27861;&#30340;&#24320;&#21457;&#12290;</title><link>http://arxiv.org/abs/2312.07899</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26102;&#20195;&#30340;&#33647;&#29289;&#21457;&#29616;&#20013;&#30340;&#24418;&#24577;&#23398;&#29305;&#24449;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Morphological Profiling for Drug Discovery in the Era of Deep Learning. (arXiv:2312.07899v2 [q-bio.QM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.07899
&lt;/p&gt;
&lt;p&gt;
&#24418;&#24577;&#23398;&#29305;&#24449;&#20998;&#26512;&#22312;&#34920;&#22411;&#33647;&#29289;&#21457;&#29616;&#20013;&#20855;&#26377;&#37325;&#35201;&#20215;&#20540;&#12290;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#20998;&#26512;&#22823;&#35268;&#27169;&#39640;&#20869;&#23481;&#22270;&#20687;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20419;&#36827;&#20102;&#33647;&#29289;&#20316;&#29992;&#26426;&#21046;&#30340;&#29702;&#35299;&#21644;&#26032;&#27835;&#30103;&#26041;&#27861;&#30340;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24418;&#24577;&#23398;&#29305;&#24449;&#20998;&#26512;&#26159;&#34920;&#22411;&#33647;&#29289;&#21457;&#29616;&#20013;&#30340;&#26377;&#20215;&#20540;&#24037;&#20855;&#12290;&#39640;&#36890;&#37327;&#33258;&#21160;&#25104;&#20687;&#30340;&#20986;&#29616;&#20351;&#24471;&#33021;&#22815;&#20197;&#21333;&#32454;&#32990;&#20998;&#36776;&#29575;&#25429;&#25417;&#32454;&#32990;&#25110;&#29983;&#29289;&#20307;&#23545;&#24178;&#25200;&#30340;&#24418;&#24577;&#23398;&#29305;&#24449;&#30340;&#24191;&#27867;&#33539;&#22260;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#65292;&#23588;&#20854;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#30340;&#37325;&#22823;&#36827;&#23637;&#65292;&#20351;&#24471;&#22312;&#39640;&#36890;&#37327;&#19979;&#20998;&#26512;&#22823;&#35268;&#27169;&#39640;&#20869;&#23481;&#22270;&#20687;&#30340;&#33021;&#21147;&#24471;&#21040;&#20102;&#26174;&#33879;&#25552;&#39640;&#12290;&#36825;&#20123;&#21162;&#21147;&#20419;&#36827;&#20102;&#23545;&#21270;&#21512;&#29289;&#20316;&#29992;&#26426;&#21046;&#12289;&#33647;&#29289;&#20877;&#21033;&#29992;&#12289;&#24178;&#25200;&#19979;&#32454;&#32990;&#24418;&#24577;&#21160;&#21147;&#23398;&#29305;&#24449;&#30340;&#20102;&#35299;&#65292;&#24182;&#26368;&#32456;&#26377;&#21161;&#20110;&#26032;&#27835;&#30103;&#26041;&#27861;&#30340;&#24320;&#21457;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#27010;&#36848;&#20102;&#24418;&#24577;&#23398;&#29305;&#24449;&#20998;&#26512;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#25105;&#20204;&#24635;&#32467;&#20102;&#22270;&#20687;&#20998;&#26512;&#24037;&#20316;&#27969;&#31243;&#65292;&#35843;&#26597;&#20102;&#21253;&#25324;&#29305;&#24449;&#24037;&#31243;&#21644;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20998;&#26512;&#31574;&#30053;&#22312;&#20869;&#30340;&#24191;&#27867;&#33539;&#22260;&#30340;&#20998;&#26512;&#31574;&#30053;&#65292;&#24182;&#35752;&#35770;&#20102;&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#28508;&#22312;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Morphological profiling is a valuable tool in phenotypic drug discovery. The advent of high-throughput automated imaging has enabled the capturing of a wide range of morphological features of cells or organisms in response to perturbations at the single-cell resolution. Concurrently, significant advances in machine learning and deep learning, especially in computer vision, have led to substantial improvements in analyzing large-scale high-content images at high-throughput. These efforts have facilitated understanding of compound mechanism-of-action (MOA), drug repurposing, characterization of cell morphodynamics under perturbation, and ultimately contributing to the development of novel therapeutics. In this review, we provide a comprehensive overview of the recent advances in the field of morphological profiling. We summarize the image profiling analysis workflow, survey a broad spectrum of analysis strategies encompassing feature engineering- and deep learning-based approaches, and i
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#29305;&#24449;&#24341;&#23548;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#22823;&#23610;&#24230;&#30340;&#23548;&#21521;&#19979;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#38750;&#32447;&#24615;&#26657;&#27491;&#65292;&#20197;&#22686;&#24378;&#23545;&#22270;&#20687;&#29983;&#25104;&#30340;&#25511;&#21046;&#33021;&#21147;&#65292;&#20943;&#23569;&#39068;&#33394;&#21644;&#26333;&#20809;&#38382;&#39064;&#65292;&#24182;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#26174;&#31034;&#20986;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2312.07586</link><description>&lt;p&gt;
&#29305;&#24449;&#24341;&#23548;&#65306;&#22823;&#23610;&#24230;&#23548;&#21521;&#19979;&#25193;&#25955;&#27169;&#22411;&#30340;&#38750;&#32447;&#24615;&#26657;&#27491;
&lt;/p&gt;
&lt;p&gt;
Characteristic Guidance: Non-linear Correction for Diffusion Model at Large Guidance Scale. (arXiv:2312.07586v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.07586
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#29305;&#24449;&#24341;&#23548;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#22823;&#23610;&#24230;&#30340;&#23548;&#21521;&#19979;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#38750;&#32447;&#24615;&#26657;&#27491;&#65292;&#20197;&#22686;&#24378;&#23545;&#22270;&#20687;&#29983;&#25104;&#30340;&#25511;&#21046;&#33021;&#21147;&#65292;&#20943;&#23569;&#39068;&#33394;&#21644;&#26333;&#20809;&#38382;&#39064;&#65292;&#24182;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#26174;&#31034;&#20986;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#34892;&#30340;&#23548;&#24341;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;(DDPM)&#32447;&#24615;&#22320;&#23558;&#19981;&#21516;&#30340;&#26465;&#20214;&#27169;&#22411;&#32452;&#21512;&#22312;&#19968;&#36215;&#65292;&#20197;&#25552;&#20379;&#23545;&#26679;&#26412;&#30340;&#22686;&#24378;&#25511;&#21046;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#24573;&#35270;&#20102;&#24403;&#23548;&#21521;&#23610;&#24230;&#21464;&#22823;&#26102;&#20135;&#29983;&#30340;&#38750;&#32447;&#24615;&#25928;&#24212;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29305;&#24449;&#24341;&#23548;&#65292;&#19968;&#31181;&#37319;&#26679;&#26041;&#27861;&#65292;&#20026;&#26080;&#20998;&#31867;&#22120;&#23548;&#21521;&#30340;DDPM&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;&#21407;&#29702;&#30340;&#38750;&#32447;&#24615;&#26657;&#27491;&#12290;&#36825;&#31181;&#26657;&#27491;&#36843;&#20351;&#23548;&#21521;&#30340;DDPM&#36981;&#23432;&#20854;&#24213;&#23618;&#25193;&#25955;&#36807;&#31243;&#30340;&#31119;&#20811;-&#26222;&#26391;&#20811;&#26041;&#31243;&#65292;&#36825;&#31181;&#26041;&#27861;&#26080;&#38656;&#35757;&#32451;&#65292;&#26080;&#38656;&#23548;&#25968;&#65292;&#19982;&#29616;&#26377;&#30340;&#37319;&#26679;&#26041;&#27861;&#20860;&#23481;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#29305;&#24449;&#24341;&#23548;&#22686;&#24378;&#20102;&#23545;&#22270;&#20687;&#29983;&#25104;&#20013;&#30340;&#25511;&#21046;&#33021;&#21147;&#65292;&#24182;&#20943;&#23569;&#20102;&#39068;&#33394;&#21644;&#26333;&#20809;&#38382;&#39064;&#65292;&#23545;&#20174;&#28508;&#22312;&#31354;&#38388;&#37319;&#26679;&#21040;&#35299;&#20915;&#29289;&#29702;&#38382;&#39064;&#22914;&#30913;&#30456;&#21464;&#30340;&#21508;&#31181;&#24212;&#29992;&#37117;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Popular guidance for denoising diffusion probabilistic model (DDPM) linearly combines distinct conditional models together to provide enhanced control over samples. However, this approach overlooks nonlinear effects that become significant when guidance scale is large. To address this issue, we propose characteristic guidance, a sampling method that provides first-principle non-linear correction for classifier-free guided DDPMs. Such correction forces the guided DDPMs to respect the Fokker-Planck equation of their underlying diffusion process, in a way that is training-free, derivative-free, and compatible with existing sampling methods. Experiments show that characteristic guidance enhances control and reduces color and exposure issues in image generation, proving effective in diverse applications ranging from latent space sampling to solving physics problems like magnet phase transitions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#22312;AIOps&#20013;&#19982;&#20379;&#24212;&#21830;&#21512;&#20316;&#26102;&#25968;&#25454;&#23433;&#20840;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#37319;&#29992;&#26368;&#20339;&#23454;&#36341;&#26469;&#20445;&#25252;&#25968;&#25454;&#21644;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2312.06008</link><description>&lt;p&gt;
&#20449;&#20219;&#23432;&#25252;&#32773;&#65306;&#36890;&#36807;&#20379;&#24212;&#21830;&#21512;&#20316;&#22312;AIOps&#20013;&#23548;&#33322;&#25968;&#25454;&#23433;&#20840;
&lt;/p&gt;
&lt;p&gt;
Guardians of Trust: Navigating Data Security in AIOps through Vendor Partnerships. (arXiv:2312.06008v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.06008
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#22312;AIOps&#20013;&#19982;&#20379;&#24212;&#21830;&#21512;&#20316;&#26102;&#25968;&#25454;&#23433;&#20840;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#37319;&#29992;&#26368;&#20339;&#23454;&#36341;&#26469;&#20445;&#25252;&#25968;&#25454;&#21644;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
IT&#36816;&#32500;&#20013;&#20154;&#24037;&#26234;&#33021;&#65288;AIOps&#65289;&#26159;&#19968;&#20010;&#24555;&#36895;&#21457;&#23637;&#30340;&#39046;&#22495;&#65292;&#24212;&#29992;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#26469;&#33258;&#21160;&#21270;&#21644;&#20248;&#21270;IT&#36816;&#32500;&#12290;AIOps&#20379;&#24212;&#21830;&#25552;&#20379;&#26381;&#21153;&#65292;&#21487;&#20197;&#25509;&#25910;&#31471;&#21040;&#31471;&#26085;&#24535;&#12289;&#36319;&#36394;&#21644;&#25351;&#26631;&#65292;&#25552;&#20379;&#23545;IT&#31995;&#32479;&#30340;&#20840;&#26632;&#30417;&#25511;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25968;&#25454;&#28304;&#21487;&#33021;&#21253;&#21547;&#25935;&#24863;&#20449;&#24687;&#65292;&#20363;&#22914;&#20869;&#37096;IP&#22320;&#22336;&#12289;&#20027;&#26426;&#21517;&#12289;HTTP&#22836;&#12289;SQL&#12289;&#26041;&#27861;/&#21442;&#25968;&#30340;&#36820;&#22238;&#20540;&#12289;URL&#21644;&#20010;&#20154;&#35782;&#21035;&#20449;&#24687;&#65288;PII&#65289;&#25110;&#26426;&#23494;&#19994;&#21153;&#25968;&#25454;&#12290;&#22240;&#27492;&#65292;&#22312;&#19982;AIOps&#20379;&#24212;&#21830;&#21512;&#20316;&#26102;&#65292;&#25968;&#25454;&#23433;&#20840;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#35752;&#35770;&#19981;&#21516;&#20379;&#24212;&#21830;&#25152;&#25552;&#20379;&#30340;&#23433;&#20840;&#29305;&#24615;&#65292;&#20197;&#21450;&#22914;&#20309;&#37319;&#29992;&#26368;&#20339;&#23454;&#36341;&#26469;&#30830;&#20445;&#25968;&#25454;&#30340;&#20445;&#25252;&#21644;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence for IT Operations (AIOps) is a rapidly growing field that applies artificial intelligence and machine learning to automate and optimize IT operations. AIOps vendors provide services that ingest end-to-end logs, traces, and metrics to offer a full stack observability of IT systems. However, these data sources may contain sensitive information such as internal IP addresses, hostnames, HTTP headers, SQLs, method/argument return values, URLs, personal identifiable information (PII), or confidential business data. Therefore, data security is a crucial concern when working with AIOps vendors. In this article, we will discuss the security features offered by different vendors and how we can adopt best practices to ensure data protection and privacy.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#24102;&#26377;&#22122;&#22768;&#26631;&#31614;&#30340;&#35838;&#31243;&#23398;&#20064;&#65292;&#28145;&#20837;&#30740;&#31350;&#20102;&#22312;&#31574;&#30053;&#28216;&#25103;&#20013;&#24212;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#22855;&#20598;&#24615;&#25361;&#25112;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#21363;&#20351;&#23384;&#22312;&#26368;&#23567;&#30340;&#26631;&#31614;&#22122;&#22768;&#65292;&#20063;&#20250;&#26126;&#26174;&#38459;&#30861;&#31070;&#32463;&#32593;&#32476;&#36776;&#21035;&#26377;&#25928;&#31574;&#30053;&#30340;&#33021;&#21147;&#65292;&#36825;&#23545;RL&#35757;&#32451;&#25552;&#20986;&#20102;&#36843;&#20999;&#38656;&#35201;&#20808;&#36827;&#26041;&#27861;&#30340;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2312.05379</link><description>&lt;p&gt;
&#36890;&#36807;&#24102;&#26377;&#22122;&#22768;&#26631;&#31614;&#30340;&#35838;&#31243;&#23398;&#20064;&#26469;&#25506;&#32034;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#22855;&#20598;&#24615;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Exploring Parity Challenges in Reinforcement Learning through Curriculum Learning with Noisy Labels. (arXiv:2312.05379v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.05379
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#24102;&#26377;&#22122;&#22768;&#26631;&#31614;&#30340;&#35838;&#31243;&#23398;&#20064;&#65292;&#28145;&#20837;&#30740;&#31350;&#20102;&#22312;&#31574;&#30053;&#28216;&#25103;&#20013;&#24212;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#22855;&#20598;&#24615;&#25361;&#25112;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#21363;&#20351;&#23384;&#22312;&#26368;&#23567;&#30340;&#26631;&#31614;&#22122;&#22768;&#65292;&#20063;&#20250;&#26126;&#26174;&#38459;&#30861;&#31070;&#32463;&#32593;&#32476;&#36776;&#21035;&#26377;&#25928;&#31574;&#30053;&#30340;&#33021;&#21147;&#65292;&#36825;&#23545;RL&#35757;&#32451;&#25552;&#20986;&#20102;&#36843;&#20999;&#38656;&#35201;&#20808;&#36827;&#26041;&#27861;&#30340;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#28145;&#20837;&#30740;&#31350;&#20102;&#22312;&#31574;&#30053;&#28216;&#25103;&#20013;&#24212;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#65292;&#29305;&#21035;&#26159;&#22312;&#22260;&#26827;&#21644;&#22269;&#38469;&#35937;&#26827;&#31561;&#29305;&#23450;&#23616;&#38754;&#20197;&#21450;&#26356;&#24191;&#27867;&#30340;&#19981;&#20559;&#21338;&#24328;&#20013;&#23384;&#22312;&#30340;&#22855;&#20598;&#24615;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#25311;&#23398;&#20064;&#36807;&#31243;&#65292;&#32467;&#26500;&#21270;&#22312;&#35838;&#31243;&#23398;&#20064;&#26694;&#26550;&#20869;&#65292;&#24182;&#36890;&#36807;&#22122;&#22768;&#26631;&#31614;&#36827;&#34892;&#22686;&#24378;&#65292;&#20197;&#27169;&#25311;&#33258;&#25105;&#23545;&#24328;&#23398;&#20064;&#22330;&#26223;&#30340;&#22797;&#26434;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#24443;&#24213;&#20998;&#26512;&#20102;&#31070;&#32463;&#32593;&#32476;&#65288;NNs&#65289;&#22914;&#20309;&#20174;&#22522;&#26412;&#30340;&#21040;&#36234;&#26469;&#36234;&#22797;&#26434;&#30340;&#28216;&#25103;&#23616;&#38754;&#20013;&#36866;&#24212;&#21644;&#28436;&#21464;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#21363;&#20351;&#23384;&#22312;&#26368;&#23567;&#30340;&#26631;&#31614;&#22122;&#22768;&#65292;&#20063;&#20250;&#23545;NNs&#36776;&#21035;&#26377;&#25928;&#31574;&#30053;&#30340;&#33021;&#21147;&#36896;&#25104;&#26126;&#26174;&#30340;&#38459;&#30861;&#65292;&#36825;&#19968;&#22256;&#38590;&#38543;&#30528;&#28216;&#25103;&#23616;&#38754;&#22797;&#26434;&#24230;&#30340;&#22686;&#21152;&#32780;&#21152;&#21095;&#12290;&#36825;&#20123;&#21457;&#29616;&#24378;&#35843;&#20102;&#22312;RL&#35757;&#32451;&#20013;&#36843;&#20999;&#38656;&#35201;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#19987;&#38376;&#24212;&#23545;&#30001;&#22122;&#22768;&#35780;&#20272;&#25152;&#24102;&#26469;&#30340;&#38556;&#30861;&#12290;&#30740;&#21457;&#36825;&#26679;&#30340;&#26041;&#27861;&#23545;&#20110;&#25552;&#39640;NN&#30340;&#19987;&#19994;&#27700;&#24179;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper delves into applying reinforcement learning (RL) in strategy games, particularly those characterized by parity challenges, as seen in specific positions of Go and Chess and a broader range of impartial games. We propose a simulated learning process, structured within a curriculum learning framework and augmented with noisy labels, to mirror the intricacies of self-play learning scenarios. This approach thoroughly analyses how neural networks (NNs) adapt and evolve from elementary to increasingly complex game positions. Our empirical research indicates that even minimal label noise can significantly impede NNs' ability to discern effective strategies, a difficulty that intensifies with the growing complexity of the game positions. These findings underscore the urgent need for advanced methodologies in RL training, specifically tailored to counter the obstacles imposed by noisy evaluations. The development of such methodologies is crucial not only for enhancing NN proficiency 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;NLP&#20219;&#21153;&#65292;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#22240;&#26524;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#20316;&#32773;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;CLadder&#65292;&#24182;&#21033;&#29992;oracle&#22240;&#26524;&#25512;&#29702;&#24341;&#25806;&#23558;&#31526;&#21495;&#38382;&#39064;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#22810;&#20010;LLMs&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#24341;&#20837;&#24182;&#35780;&#20272;&#20102;&#19968;&#31181;&#23450;&#21046;&#30340;&#38142;&#24335;&#25512;&#29702;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2312.04350</link><description>&lt;p&gt;
CLadder: &#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22240;&#26524;&#25512;&#29702;&#33021;&#21147;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
CLadder: A Benchmark to Assess Causal Reasoning Capabilities of Language Models. (arXiv:2312.04350v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.04350
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;NLP&#20219;&#21153;&#65292;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#22240;&#26524;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#20316;&#32773;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;CLadder&#65292;&#24182;&#21033;&#29992;oracle&#22240;&#26524;&#25512;&#29702;&#24341;&#25806;&#23558;&#31526;&#21495;&#38382;&#39064;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#22810;&#20010;LLMs&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#24341;&#20837;&#24182;&#35780;&#20272;&#20102;&#19968;&#31181;&#23450;&#21046;&#30340;&#38142;&#24335;&#25512;&#29702;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36827;&#34892;&#22240;&#26524;&#25512;&#29702;&#30340;&#33021;&#21147;&#34987;&#24191;&#27867;&#35270;&#20026;&#26234;&#33021;&#30340;&#26680;&#24515;&#29305;&#24449;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#33021;&#21542;&#36830;&#36143;&#22320;&#25512;&#29702;&#22240;&#26524;&#20851;&#31995;&#12290;&#29616;&#26377;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#24037;&#20316;&#20027;&#35201;&#20851;&#27880;&#35780;&#20272;LLMs&#20013;&#30340;&#24120;&#35782;&#22240;&#26524;&#25512;&#29702;&#65292;&#26410;&#33021;&#35780;&#20272;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#25353;&#29031;&#19968;&#32452;&#26126;&#30830;&#23450;&#20041;&#30340;&#24418;&#24335;&#35268;&#21017;&#25191;&#34892;&#22240;&#26524;&#25512;&#26029;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;NLP&#20219;&#21153;&#65292;&#33258;&#28982;&#35821;&#35328;&#20013;&#30340;&#22240;&#26524;&#25512;&#26029;&#65292;&#21463;&#21040;Judea Pearl&#31561;&#20154;&#25552;&#20986;&#30340;&#8220;&#22240;&#26524;&#25512;&#26029;&#24341;&#25806;&#8221;&#30340;&#21551;&#21457;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;10K&#20010;&#26679;&#26412;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;CLadder&#65292;&#36890;&#36807;&#19968;&#31181;oracle&#22240;&#26524;&#25512;&#29702;&#24341;&#25806;&#65292;&#22522;&#20110;&#19968;&#32452;&#22240;&#26524;&#22270;&#21644;&#26597;&#35810;(&#32852;&#21512;&#12289;&#24178;&#39044;&#21644;&#21453;&#20107;&#23454;)&#65292;&#24471;&#21040;&#31526;&#21495;&#38382;&#39064;&#21644;&#30495;&#23454;&#31572;&#26696;&#65292;&#24182;&#23558;&#20854;&#32763;&#35793;&#20026;&#33258;&#28982;&#35821;&#35328;&#12290;&#25105;&#20204;&#23545;&#25968;&#25454;&#38598;&#19978;&#30340;&#22810;&#20010;LLMs&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#24341;&#20837;&#21644;&#35780;&#20272;&#20102;&#19968;&#31181;&#23450;&#21046;&#30340;&#38142;&#24335;&#25512;&#29702;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to perform causal reasoning is widely considered a core feature of intelligence. In this work, we investigate whether large language models (LLMs) can coherently reason about causality. Much of the existing work in natural language processing (NLP) focuses on evaluating commonsense causal reasoning in LLMs, thus failing to assess whether a model can perform causal inference in accordance with a set of well-defined formal rules. To address this, we propose a new NLP task, causal inference in natural language, inspired by the "causal inference engine" postulated by Judea Pearl et al. We compose a large dataset, CLadder, with 10K samples: based on a collection of causal graphs and queries (associational, interventional, and counterfactual), we obtain symbolic questions and ground-truth answers, through an oracle causal inference engine. These are then translated into natural language. We evaluate multiple LLMs on our dataset, and we introduce and evaluate a bespoke chain-of-th
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#22270;&#21367;&#31215;&#26469;&#25913;&#36827;Transformer&#27169;&#22411;&#20013;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#24182;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#22810;&#20010;&#39046;&#22495;&#23637;&#31034;&#20102;&#20854;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2312.04234</link><description>&lt;p&gt;
&#22270;&#21367;&#31215;&#22312;Transformer&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#36215;&#21040;&#20102;&#25913;&#36827;&#30340;&#20316;&#29992;&#65281;&#65288;arXiv&#65306;2312.04234v2 [cs.LG]&#24050;&#26356;&#26032;&#65289;
&lt;/p&gt;
&lt;p&gt;
Graph Convolutions Enrich the Self-Attention in Transformers!. (arXiv:2312.04234v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.04234
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#22270;&#21367;&#31215;&#26469;&#25913;&#36827;Transformer&#27169;&#22411;&#20013;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#24182;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#22810;&#20010;&#39046;&#22495;&#23637;&#31034;&#20102;&#20854;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#22240;&#20854;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#32780;&#38395;&#21517;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#31561;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;Transformer&#27169;&#22411;&#38754;&#20020;&#30340;&#25361;&#25112;&#20043;&#19968;&#26159;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#65292;&#21363;&#34920;&#31034;&#22312;&#21508;&#20010;&#23618;&#20043;&#38388;&#36235;&#20110;&#26080;&#27861;&#21306;&#20998;&#30340;&#20540;&#65292;&#23548;&#33268;&#24615;&#33021;&#20005;&#37325;&#19979;&#38477;&#12290;&#25105;&#20204;&#23558;&#21407;&#22987;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#35299;&#37322;&#20026;&#19968;&#31181;&#31616;&#21333;&#30340;&#22270;&#28388;&#27874;&#22120;&#65292;&#24182;&#20174;&#22270;&#20449;&#21495;&#22788;&#29702;&#65288;GSP&#65289;&#30340;&#35282;&#24230;&#37325;&#26032;&#35774;&#35745;&#23427;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#22270;&#28388;&#27874;&#22120;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65288;GFSA&#65289;&#65292;&#20197;&#23398;&#20064;&#19968;&#31181;&#26082;&#36890;&#29992;&#21448;&#26377;&#25928;&#30340;&#26426;&#21046;&#65292;&#20854;&#22797;&#26434;&#24230;&#30053;&#39640;&#20110;&#21407;&#22987;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;GFSA&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#22270;&#27169;&#24335;&#20998;&#31867;&#12289;&#35821;&#38899;&#35782;&#21035;&#21644;&#20195;&#30721;&#20998;&#31867;&#31561;&#22810;&#20010;&#39046;&#22495;&#20013;&#25913;&#36827;&#20102;Transformer&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers, renowned for their self-attention mechanism, have achieved state-of-the-art performance across various tasks in natural language processing, computer vision, time-series modeling, etc. However, one of the challenges with deep Transformer models is the oversmoothing problem, where representations across layers converge to indistinguishable values, leading to significant performance degradation. We interpret the original self-attention as a simple graph filter and redesign it from a graph signal processing (GSP) perspective. We propose graph-filter-based self-attention (GFSA) to learn a general yet effective one, whose complexity, however, is slightly larger than that of the original self-attention mechanism. We demonstrate that GFSA improves the performance of Transformers in various fields, including computer vision, natural language processing, graph pattern classification, speech recognition, and code classification.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#20102;&#22522;&#20110;&#29289;&#29702;&#21551;&#21457;&#31070;&#32463;&#32593;&#32476;&#30340;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#25511;&#21046;&#26041;&#27861;&#22312;&#20303;&#23429;&#24314;&#31569;&#20379;&#26262;&#30340;&#38656;&#27714;&#21709;&#24212;&#20013;&#30340;&#24212;&#29992;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#28789;&#27963;&#30340;&#20248;&#21270;&#25972;&#21512;&#20102;&#22806;&#37096;&#32422;&#26463;&#26465;&#20214;&#65292;&#20026;&#23454;&#29616;&#33021;&#28304;&#28040;&#32791;&#30340;&#20248;&#21270;&#21644;&#29992;&#25143;&#28909;&#33298;&#36866;&#24230;&#30340;&#20445;&#35777;&#25552;&#20379;&#20102;&#26377;&#21069;&#26223;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2312.03365</link><description>&lt;p&gt;
&#20303;&#23429;&#24314;&#31569;&#20379;&#26262;&#30340;&#38656;&#27714;&#21709;&#24212;&#65306;&#22522;&#20110;&#29289;&#29702;&#21551;&#21457;&#31070;&#32463;&#32593;&#32476;&#30340;&#26377;&#25928;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Demand response for residential building heating: Effective Monte Carlo Tree Search control based on physics-informed neural networks. (arXiv:2312.03365v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.03365
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#20102;&#22522;&#20110;&#29289;&#29702;&#21551;&#21457;&#31070;&#32463;&#32593;&#32476;&#30340;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#25511;&#21046;&#26041;&#27861;&#22312;&#20303;&#23429;&#24314;&#31569;&#20379;&#26262;&#30340;&#38656;&#27714;&#21709;&#24212;&#20013;&#30340;&#24212;&#29992;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#28789;&#27963;&#30340;&#20248;&#21270;&#25972;&#21512;&#20102;&#22806;&#37096;&#32422;&#26463;&#26465;&#20214;&#65292;&#20026;&#23454;&#29616;&#33021;&#28304;&#28040;&#32791;&#30340;&#20248;&#21270;&#21644;&#29992;&#25143;&#28909;&#33298;&#36866;&#24230;&#30340;&#20445;&#35777;&#25552;&#20379;&#20102;&#26377;&#21069;&#26223;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25511;&#21046;&#24314;&#31569;&#29289;&#30340;&#33021;&#28304;&#28040;&#32791;&#36890;&#36807;&#38656;&#27714;&#21709;&#24212;&#24050;&#25104;&#20026;&#20943;&#23569;&#20840;&#29699;&#30899;&#25490;&#25918;&#21644;&#38480;&#21046;&#27668;&#20505;&#21464;&#21270;&#30340;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#25163;&#27573;&#12290;&#26412;&#25991;&#29305;&#21035;&#20851;&#27880;&#20303;&#23429;&#24314;&#31569;&#20379;&#26262;&#31995;&#32479;&#30340;&#25511;&#21046;&#65292;&#20197;&#20248;&#21270;&#33021;&#28304;&#28040;&#32791;&#21516;&#26102;&#20445;&#35777;&#29992;&#25143;&#30340;&#28909;&#33298;&#36866;&#24230;&#12290;&#22312;&#36825;&#20010;&#39046;&#22495;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#22522;&#20110;&#27169;&#22411;&#30340;&#25511;&#21046;&#65292;&#20363;&#22914;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65288;MPC&#65289;&#65292;&#25110;&#32773;&#22522;&#20110;&#27169;&#22411;&#26080;&#20851;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26469;&#23454;&#29616;&#23454;&#38469;&#30340;&#38656;&#27714;&#21709;&#24212;&#31639;&#27861;&#12290;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65288;MCTS&#65289;&#26159;&#19968;&#31181;&#26368;&#36817;&#22312;&#26827;&#30424;&#28216;&#25103;&#65288;&#22260;&#26827;&#12289;&#22269;&#38469;&#35937;&#26827;&#65289;&#31561;&#39046;&#22495;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#25104;&#21151;&#30340;RL&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22312;&#24314;&#31569;&#25511;&#21046;&#26041;&#38754;&#65292;MCTS&#20173;&#28982;&#34987;&#36739;&#23569;&#25506;&#32034;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#19987;&#38376;&#30740;&#31350;&#20102;MCTS&#22312;&#24314;&#31569;&#38656;&#27714;&#21709;&#24212;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#20854;&#33258;&#28982;&#30340;&#32467;&#26500;&#20801;&#35768;&#28789;&#27963;&#30340;&#20248;&#21270;&#65292;&#38544;&#24335;&#22320;&#38598;&#25104;&#22806;&#37096;&#32422;&#26463;&#26465;&#20214;&#65288;&#19982;&#20256;&#32479;&#30340;RL&#35299;&#20915;&#26041;&#26696;&#30456;&#27604;&#65289;&#65292;&#20351;MCTS&#25104;&#20026;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#20505;&#36873;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Controlling energy consumption in buildings through demand response (DR) has become increasingly important to reduce global carbon emissions and limit climate change. In this paper, we specifically focus on controlling the heating system of a residential building to optimize its energy consumption while respecting user's thermal comfort. Recent works in this area have mainly focused on either model-based control, e.g., model predictive control (MPC), or model-free reinforcement learning (RL) to implement practical DR algorithms. A specific RL method that recently has achieved impressive success in domains such as board games (go, chess) is Monte Carlo Tree Search (MCTS). Yet, for building control it has remained largely unexplored. Thus, we study MCTS specifically for building demand response. Its natural structure allows a flexible optimization that implicitly integrate exogenous constraints (as opposed, for example, to conventional RL solutions), making MCTS a promising candidate for
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23558;&#32479;&#35745;&#23398;&#30340;&#38181;&#24418;&#20998;&#24067;&#20989;&#25968;&#36716;&#21270;&#20026;&#22810;&#20934;&#21017;&#20915;&#31574;&#21046;&#23450;&#24037;&#20855;&#65292;&#24182;&#36890;&#36807;&#25193;&#23637;&#25490;&#21517;&#20989;&#25968;&#21040;&#38598;&#21512;&#65292;&#24314;&#31435;&#20102;&#38598;&#21512;&#20248;&#21270;&#26041;&#27861;&#19982;&#22522;&#20110;&#38598;&#21512;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#20855;&#26377;&#28508;&#22312;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2312.03006</link><description>&lt;p&gt;
&#22810;&#20934;&#21017;&#20915;&#31574;&#21046;&#23450;&#30340;&#22810;&#26435;&#37325;&#25490;&#21517;
&lt;/p&gt;
&lt;p&gt;
Multi-Weight Ranking for Multi-Criteria Decision Making. (arXiv:2312.03006v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.03006
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23558;&#32479;&#35745;&#23398;&#30340;&#38181;&#24418;&#20998;&#24067;&#20989;&#25968;&#36716;&#21270;&#20026;&#22810;&#20934;&#21017;&#20915;&#31574;&#21046;&#23450;&#24037;&#20855;&#65292;&#24182;&#36890;&#36807;&#25193;&#23637;&#25490;&#21517;&#20989;&#25968;&#21040;&#38598;&#21512;&#65292;&#24314;&#31435;&#20102;&#38598;&#21512;&#20248;&#21270;&#26041;&#27861;&#19982;&#22522;&#20110;&#38598;&#21512;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#20855;&#26377;&#28508;&#22312;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32479;&#35745;&#23398;&#20013;&#30340;&#38181;&#24418;&#20998;&#24067;&#20989;&#25968;&#34987;&#36716;&#21270;&#20026;&#22810;&#20934;&#21017;&#20915;&#31574;&#21046;&#23450;&#24037;&#20855;&#12290;&#36890;&#36807;&#20363;&#23376;&#23637;&#31034;&#65292;&#19982;&#32431;&#21152;&#26435;&#24635;&#21644;&#26631;&#37327;&#21270;&#30456;&#27604;&#65292;&#36825;&#31181;&#26631;&#37327;&#21270;&#26041;&#27861;&#33021;&#22815;&#25214;&#20986; Pareto &#36793;&#30028;&#30340;&#8220;&#38750;&#20984;&#8221;&#37096;&#20998;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#23558;&#25490;&#21517;&#20989;&#25968;&#25193;&#23637;&#21040;&#38598;&#21512;&#65292;&#20026;&#38598;&#21512;&#20559;&#22909;&#25552;&#20379;&#19968;&#20803;&#25351;&#26631;&#65292;&#39318;&#27425;&#24314;&#31435;&#20102;&#38598;&#21512;&#20248;&#21270;&#26041;&#27861;&#19982;&#22522;&#20110;&#38598;&#21512;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#25991;&#20013;&#36824;&#31616;&#36848;&#20102;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#28508;&#22312;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cone distribution functions from statistics are turned into Multi-Criteria Decision Making tools. It is demonstrated that this procedure can be considered as an upgrade of the weighted sum scalarization insofar as it absorbs a whole collection of weighted sum scalarizations at once instead of fixing a particular one in advance. As examples show, this type of scalarization--in contrast to a pure weighted sum scalarization-is also able to detect ``non-convex" parts of the Pareto frontier. Situations are characterized in which different types of rank reversal occur, and it is explained why this might even be useful for analyzing the ranking procedure. The ranking functions are then extended to sets providing unary indicators for set preferences which establishes, for the first time, the link between set optimization methods and set-based multi-objective optimization. A potential application in machine learning is outlined.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#36125;&#21494;&#26031;&#32593;&#32476;&#20013;Shannon&#29109;&#21644;Kullback-Leibler&#25955;&#24230;&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#19968;&#31995;&#21015;&#25968;&#20540;&#31034;&#20363;&#36827;&#34892;&#20102;&#28436;&#31034;&#12290;&#27492;&#22806;&#65292;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#39640;&#26031;&#36125;&#21494;&#26031;&#32593;&#32476;&#20013;KL&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#20174;&#31435;&#26041;&#38477;&#20302;&#21040;&#20108;&#27425;&#12290;</title><link>http://arxiv.org/abs/2312.01520</link><description>&lt;p&gt;
Bayesian&#32593;&#32476;&#30340;&#29109;&#21644;Kullback-Leibler&#25955;&#24230;&#65306;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#39640;&#25928;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
Entropy and the Kullback-Leibler Divergence for Bayesian Networks: Computational Complexity and Efficient Implementation. (arXiv:2312.01520v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.01520
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#36125;&#21494;&#26031;&#32593;&#32476;&#20013;Shannon&#29109;&#21644;Kullback-Leibler&#25955;&#24230;&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#19968;&#31995;&#21015;&#25968;&#20540;&#31034;&#20363;&#36827;&#34892;&#20102;&#28436;&#31034;&#12290;&#27492;&#22806;&#65292;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#39640;&#26031;&#36125;&#21494;&#26031;&#32593;&#32476;&#20013;KL&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#20174;&#31435;&#26041;&#38477;&#20302;&#21040;&#20108;&#27425;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#32593;&#32476;&#65288;BNs&#65289;&#26159;&#26426;&#22120;&#23398;&#20064;&#21644;&#22240;&#26524;&#25512;&#26029;&#20013;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;&#23427;&#20204;&#30340;&#22270;&#32467;&#26500;&#21487;&#20197;&#22788;&#29702;&#39640;&#32500;&#38382;&#39064;&#65292;&#24182;&#23558;&#20854;&#20998;&#20026;&#31232;&#30095;&#30340;&#19968;&#31995;&#21015;&#36739;&#23567;&#38382;&#39064;&#65292;&#36825;&#26159;Judea Pearl&#30340;&#22240;&#26524;&#24615;&#30340;&#22522;&#30784;&#65292;&#20063;&#20915;&#23450;&#20102;&#23427;&#20204;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#29702;&#35299;&#24615;&#12290;&#23613;&#31649;&#23427;&#20204;&#24456;&#21463;&#27426;&#36814;&#65292;&#20294;&#22312;&#25991;&#29486;&#20013;&#20960;&#20046;&#27809;&#26377;&#20851;&#20110;&#22914;&#20309;&#22312;&#26368;&#24120;&#35265;&#30340;&#20998;&#24067;&#20551;&#35774;&#19979;&#35745;&#31639;BNs&#30340;Shannon&#29109;&#21644;Kullback-Leibler&#65288;KL&#65289;&#25955;&#24230;&#30340;&#36164;&#28304;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;BNs&#30340;&#22270;&#32467;&#26500;&#25552;&#20379;&#20102;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#31639;&#27861;&#65292;&#24182;&#29992;&#19968;&#25972;&#22871;&#25968;&#20540;&#31034;&#20363;&#35828;&#26126;&#20102;&#23427;&#20204;&#12290;&#22312;&#27492;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#20197;&#23558;&#39640;&#26031;BNs&#30340;KL&#35745;&#31639;&#22797;&#26434;&#24230;&#20174;&#31435;&#26041;&#38477;&#20302;&#21040;&#20108;&#27425;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian networks (BNs) are a foundational model in machine learning and causal inference. Their graphical structure can handle high-dimensional problems, divide them into a sparse collection of smaller ones, underlies Judea Pearl's causality, and determines their explainability and interpretability. Despite their popularity, there are almost no resources in the literature on how to compute Shannon's entropy and the Kullback-Leibler (KL) divergence for BNs under their most common distributional assumptions. In this paper, we provide computationally efficient algorithms for both by leveraging BNs' graphical structure, and we illustrate them with a complete set of numerical examples. In the process, we show it is possible to reduce the computational complexity of KL from cubic to quadratic for Gaussian BNs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#33521;&#22269;&#22810;&#31181;&#38271;&#26399;&#30142;&#30149;&#30740;&#31350;&#32852;&#30431;&#20013;&#30340;&#21442;&#19982;&#32773;&#30340;&#35775;&#35848;&#65292;&#25506;&#35752;&#20102;&#21327;&#20316;&#25968;&#25454;&#23454;&#36341;&#30340;&#24037;&#20855;&#12289;&#27807;&#36890;&#36807;&#31243;&#21644;&#29615;&#22659;&#65292;&#20197;&#21450;&#21327;&#20316;&#24037;&#20316;&#30340;&#26465;&#20214;&#21644;&#38556;&#30861;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#20102;&#24037;&#20855;&#30340;&#36866;&#24212;&#24615;&#21644;&#20449;&#24687;&#26681;&#25454;&#21463;&#20247;&#30340;&#20010;&#24615;&#21270;&#65292;&#20197;&#21450;&#30005;&#23376;&#21307;&#30103;&#35760;&#24405;&#21644;&#25968;&#25454;&#38598;&#35775;&#38382;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2311.18424</link><description>&lt;p&gt;
&#25506;&#31350;&#21327;&#20316;&#25968;&#25454;&#23454;&#36341;&#65306;&#20197;&#21307;&#30103;&#20581;&#24247;&#30740;&#31350;&#20013;&#30340;&#20154;&#24037;&#26234;&#33021;&#20026;&#20363;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Investigating Collaborative Data Practices: a Case Study on Artificial Intelligence for Healthcare Research. (arXiv:2311.18424v2 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.18424
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#33521;&#22269;&#22810;&#31181;&#38271;&#26399;&#30142;&#30149;&#30740;&#31350;&#32852;&#30431;&#20013;&#30340;&#21442;&#19982;&#32773;&#30340;&#35775;&#35848;&#65292;&#25506;&#35752;&#20102;&#21327;&#20316;&#25968;&#25454;&#23454;&#36341;&#30340;&#24037;&#20855;&#12289;&#27807;&#36890;&#36807;&#31243;&#21644;&#29615;&#22659;&#65292;&#20197;&#21450;&#21327;&#20316;&#24037;&#20316;&#30340;&#26465;&#20214;&#21644;&#38556;&#30861;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#20102;&#24037;&#20855;&#30340;&#36866;&#24212;&#24615;&#21644;&#20449;&#24687;&#26681;&#25454;&#21463;&#20247;&#30340;&#20010;&#24615;&#21270;&#65292;&#20197;&#21450;&#30005;&#23376;&#21307;&#30103;&#35760;&#24405;&#21644;&#25968;&#25454;&#38598;&#35775;&#38382;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#21307;&#30103;&#20581;&#24247;&#39046;&#22495;&#30340;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#26159;&#19968;&#39033;&#21327;&#20316;&#24037;&#20316;&#65292;&#23558;&#25968;&#25454;&#31185;&#23398;&#23478;&#12289;&#20020;&#24202;&#21307;&#29983;&#12289;&#24739;&#32773;&#21644;&#20854;&#20182;&#23398;&#31185;&#27719;&#38598;&#22312;&#19968;&#36215;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#33521;&#22269;&#24212;&#29992;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#26469;&#29702;&#35299;&#21644;&#31649;&#29702;&#22810;&#31181;&#38271;&#26399;&#30142;&#30149;&#30340;&#30740;&#31350;&#32852;&#30431;&#20013;&#30340;&#21442;&#19982;&#32773;&#36827;&#34892;&#30340;13&#27425;&#21322;&#32467;&#26500;&#21270;&#35775;&#35848;&#30340;&#24402;&#32435;&#20027;&#39064;&#20998;&#26512;&#65292;&#25506;&#35752;&#20102;&#21327;&#20316;&#25968;&#25454;&#23454;&#36341;&#30340;&#24037;&#20855;&#12289;&#27807;&#36890;&#36807;&#31243;&#21644;&#29615;&#22659;&#65292;&#20197;&#21450;&#21327;&#20316;&#24037;&#20316;&#30340;&#26465;&#20214;&#21644;&#38556;&#30861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#20849;&#20139;&#30693;&#35782;&#25152;&#20351;&#29992;&#30340;&#24037;&#20855;&#30340;&#36866;&#24212;&#24615;&#20197;&#21450;&#26681;&#25454;&#21463;&#20247;&#65292;&#29305;&#21035;&#26159;&#20020;&#24202;&#21307;&#29983;&#25110;&#24739;&#32773;&#30340;&#35266;&#28857;&#36827;&#34892;&#20449;&#24687;&#23450;&#21046;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#20102;&#30005;&#23376;&#21307;&#30103;&#35760;&#24405;&#21644;&#25968;&#25454;&#38598;&#35775;&#38382;&#30340;&#38480;&#21046;&#38480;&#21046;&#20102;&#36825;&#31181;&#23450;&#21046;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#20250;&#35758;&#26159;&#20851;&#38190;&#30340;&#29615;&#22659;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developing artificial intelligence (AI) tools for healthcare is a collaborative effort, bringing data scientists, clinicians, patients and other disciplines together. In this paper, we explore the collaborative data practices of research consortia tasked with applying AI tools to understand and manage multiple long-term conditions in the UK. Through an inductive thematic analysis of 13 semi-structured interviews with participants of these consortia, we aimed to understand how collaboration happens based on the tools used, communication processes and settings, as well as the conditions and obstacles for collaborative work. Our findings reveal the adaptation of tools that are used for sharing knowledge and the tailoring of information based on the audience, particularly those from a clinical or patient perspective. Limitations on the ability to do this were also found to be imposed by the use of electronic healthcare records and access to datasets. We identified meetings as the key setti
&lt;/p&gt;</description></item><item><title>GraphPro&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;&#21442;&#25968;&#39640;&#25928;&#21644;&#21160;&#24577;&#22270;&#39044;&#35757;&#32451;&#19982;&#25552;&#31034;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#38271;&#26399;&#29992;&#25143;&#20559;&#22909;&#21644;&#30701;&#26399;&#34892;&#20026;&#21160;&#24577;&#65292;&#20174;&#32780;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#25512;&#33616;&#31995;&#32479;&#20013;&#25552;&#20379;&#20934;&#30830;&#21644;&#21450;&#26102;&#30340;&#25512;&#33616;&#12290;</title><link>http://arxiv.org/abs/2311.16716</link><description>&lt;p&gt;
GraphPro: &#38754;&#21521;&#25512;&#33616;&#31995;&#32479;&#30340;&#22270;&#39044;&#35757;&#32451;&#21644;&#25552;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
GraphPro: Graph Pre-training and Prompt Learning for Recommendation. (arXiv:2311.16716v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.16716
&lt;/p&gt;
&lt;p&gt;
GraphPro&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;&#21442;&#25968;&#39640;&#25928;&#21644;&#21160;&#24577;&#22270;&#39044;&#35757;&#32451;&#19982;&#25552;&#31034;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#38271;&#26399;&#29992;&#25143;&#20559;&#22909;&#21644;&#30701;&#26399;&#34892;&#20026;&#21160;&#24577;&#65292;&#20174;&#32780;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#25512;&#33616;&#31995;&#32479;&#20013;&#25552;&#20379;&#20934;&#30830;&#21644;&#21450;&#26102;&#30340;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;GNN&#30340;&#25512;&#33616;&#31995;&#32479;&#36890;&#36807;&#22810;&#27425;&#28040;&#24687;&#20256;&#36882;&#22312;&#24314;&#27169;&#22797;&#26434;&#30340;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#24573;&#35270;&#20102;&#19981;&#26029;&#21464;&#21270;&#30340;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#30340;&#21160;&#24577;&#24615;&#65292;&#36825;&#38480;&#21046;&#20102;&#20854;&#22312;&#36866;&#24212;&#29992;&#25143;&#20559;&#22909;&#21464;&#21270;&#21644;&#26032;&#21040;&#36798;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#26041;&#38754;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#21160;&#24577;&#29615;&#22659;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#24615;&#33021;&#21463;&#21040;&#20102;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GraphPro&#65292;&#36825;&#26159;&#19968;&#20010;&#23558;&#21442;&#25968;&#39640;&#25928;&#21644;&#21160;&#24577;&#22270;&#39044;&#35757;&#32451;&#19982;&#25552;&#31034;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#26694;&#26550;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#32452;&#21512;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#38271;&#26399;&#29992;&#25143;&#20559;&#22909;&#21644;&#30701;&#26399;&#34892;&#20026;&#21160;&#24577;&#65292;&#20174;&#32780;&#23454;&#29616;&#20934;&#30830;&#21644;&#21450;&#26102;&#30340;&#25512;&#33616;&#12290;&#25105;&#20204;&#30340;GraphPro&#26694;&#26550;&#36890;&#36807;&#26080;&#32541;&#38598;&#25104;&#20020;&#26102;&#25552;&#31034;&#26426;&#21046;&#21644;&#22270;&#32467;&#26500;&#25552;&#31034;&#23398;&#20064;&#26426;&#21046;&#21040;&#39044;&#35757;&#32451;&#30340;GNN&#27169;&#22411;&#20013;&#26469;&#35299;&#20915;&#29992;&#25143;&#20559;&#22909;&#19981;&#26029;&#21464;&#21270;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
GNN-based recommenders have excelled in modeling intricate user-item interactions through multi-hop message passing. However, existing methods often overlook the dynamic nature of evolving user-item interactions, which impedes the adaption to changing user preferences and distribution shifts in newly arriving data. Thus, their scalability and performances in real-world dynamic environments are limited. In this study, we propose GraphPro, a framework that incorporates parameter-efficient and dynamic graph pre-training with prompt learning. This novel combination empowers GNNs to effectively capture both long-term user preferences and short-term behavior dynamics, enabling the delivery of accurate and timely recommendations. Our GraphPro framework addresses the challenge of evolving user preferences by seamlessly integrating a temporal prompt mechanism and a graph-structural prompt learning mechanism into the pre-trained GNN model. The temporal prompt mechanism encodes time information o
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31227;&#21160;&#32593;&#26684;PDE&#30340;&#31227;&#21160;&#37319;&#26679;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;(MMPDE-Net)&#65292;&#36890;&#36807;&#35299;&#20915;&#31227;&#21160;&#32593;&#26684;PDE&#26469;&#33258;&#36866;&#24212;&#29983;&#25104;&#26032;&#30340;&#37319;&#26679;&#28857;&#65292;&#24182;&#19988;&#32467;&#21512;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#25552;&#20986;&#20102;&#31227;&#21160;&#37319;&#26679;PINN&#65288;MS-PINN&#65289;&#30340;&#26694;&#26550;&#12290;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;MS-PINN&#30456;&#23545;&#20110;PINN&#30340;&#24615;&#33021;&#25913;&#21892;&#12290;</title><link>http://arxiv.org/abs/2311.16167</link><description>&lt;p&gt;
&#22522;&#20110;&#31227;&#21160;&#32593;&#26684;PDE&#30340;&#31227;&#21160;&#37319;&#26679;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Moving Sampling Physics-informed Neural Networks induced by Moving Mesh PDE. (arXiv:2311.16167v2 [math.NA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.16167
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31227;&#21160;&#32593;&#26684;PDE&#30340;&#31227;&#21160;&#37319;&#26679;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;(MMPDE-Net)&#65292;&#36890;&#36807;&#35299;&#20915;&#31227;&#21160;&#32593;&#26684;PDE&#26469;&#33258;&#36866;&#24212;&#29983;&#25104;&#26032;&#30340;&#37319;&#26679;&#28857;&#65292;&#24182;&#19988;&#32467;&#21512;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#25552;&#20986;&#20102;&#31227;&#21160;&#37319;&#26679;PINN&#65288;MS-PINN&#65289;&#30340;&#26694;&#26550;&#12290;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;MS-PINN&#30456;&#23545;&#20110;PINN&#30340;&#24615;&#33021;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31227;&#21160;&#32593;&#26684;&#26041;&#27861;&#30340;&#31471;&#21040;&#31471;&#33258;&#36866;&#24212;&#37319;&#26679;&#31070;&#32463;&#32593;&#32476;&#65288;MMPDE-Net&#65289;&#65292;&#36890;&#36807;&#27714;&#35299;&#31227;&#21160;&#32593;&#26684;PDE&#65292;&#21487;&#20197;&#33258;&#36866;&#24212;&#29983;&#25104;&#26032;&#30340;&#37319;&#26679;&#28857;&#12290;&#35813;&#27169;&#22411;&#26088;&#22312;&#25913;&#21892;&#37319;&#26679;&#28857;&#29983;&#25104;&#30340;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22522;&#20110;MMPDE-Net&#24320;&#21457;&#20102;&#19968;&#31181;&#36845;&#20195;&#31639;&#27861;&#65292;&#20351;&#24471;&#37319;&#26679;&#28857;&#26356;&#21152;&#31934;&#30830;&#21644;&#21487;&#25511;&#12290;&#30001;&#20110;MMPDE-Net&#26159;&#29420;&#31435;&#20110;&#28145;&#24230;&#23398;&#20064;&#27714;&#35299;&#22120;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#23558;&#20854;&#19982;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#31227;&#21160;&#37319;&#26679;PINN&#65288;MS-PINN&#65289;&#65292;&#24182;&#22312;&#19968;&#20123;&#20551;&#35774;&#19979;&#36890;&#36807;&#35823;&#24046;&#20998;&#26512;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#22235;&#20010;&#20856;&#22411;&#23454;&#20363;&#30340;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;MS-PINN&#30456;&#23545;&#20110;PINN&#30340;&#24615;&#33021;&#25913;&#21892;&#65292;&#20174;&#32780;&#25968;&#20540;&#19978;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose an end-to-end adaptive sampling neural network (MMPDE-Net) based on the moving mesh method, which can adaptively generate new sampling points by solving the moving mesh PDE. This model focuses on improving the quality of sampling points generation. Moreover, we develop an iterative algorithm based on MMPDE-Net, which makes the sampling points more precise and controllable. Since MMPDE-Net is a framework independent of the deep learning solver, we combine it with physics-informed neural networks (PINN) to propose moving sampling PINN (MS-PINN) and demonstrate its effectiveness by error analysis under some assumptions. Finally, we demonstrate the performance improvement of MS-PINN compared to PINN through numerical experiments of four typical examples, which numerically verify the effectiveness of our method.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#28151;&#21512;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#20934;&#30830;&#21306;&#20998;AI&#29983;&#25104;&#25991;&#26412;&#21644;&#20154;&#31867;&#20889;&#20316;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#24212;&#29992;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#21644;&#22797;&#26434;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25104;&#21151;&#22320;&#26816;&#27979;&#21040;&#20102;AI&#29983;&#25104;&#25991;&#26412;&#21644;&#20154;&#31867;&#20889;&#20316;&#20043;&#38388;&#30340;&#24494;&#22937;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2311.15565</link><description>&lt;p&gt;
&#35780;&#20272;&#28151;&#21512;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#21306;&#20998;AI&#29983;&#25104;&#25991;&#26412;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Efficacy of Hybrid Deep Learning Models in Distinguishing AI-Generated Text. (arXiv:2311.15565v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.15565
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#28151;&#21512;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#20934;&#30830;&#21306;&#20998;AI&#29983;&#25104;&#25991;&#26412;&#21644;&#20154;&#31867;&#20889;&#20316;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#24212;&#29992;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#21644;&#22797;&#26434;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25104;&#21151;&#22320;&#26816;&#27979;&#21040;&#20102;AI&#29983;&#25104;&#25991;&#26412;&#21644;&#20154;&#31867;&#20889;&#20316;&#20043;&#38388;&#30340;&#24494;&#22937;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#30340;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#20808;&#36827;&#30340;&#28151;&#21512;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#26469;&#20934;&#30830;&#21306;&#20998;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#19982;&#20154;&#31867;&#20889;&#20316;&#12290;&#25105;&#24212;&#29992;&#20102;&#19968;&#31181;&#31283;&#20581;&#30340;&#26041;&#27861;&#35770;&#65292;&#21033;&#29992;&#20102;&#19968;&#20010;&#31934;&#24515;&#36873;&#25321;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;&#26469;&#33258;&#21508;&#31181;&#26469;&#28304;&#30340;AI&#21644;&#20154;&#31867;&#25991;&#26412;&#65292;&#27599;&#20010;&#25991;&#26412;&#37117;&#26631;&#26377;&#25351;&#31034;&#12290;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#20415;&#20110;&#23545;&#25991;&#26412;&#29305;&#24449;&#36827;&#34892;&#20998;&#26512;&#12290;&#36890;&#36807;&#32467;&#21512;&#22797;&#26434;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#36825;&#20010;&#23450;&#21046;&#27169;&#22411;&#20351;&#24471;&#23427;&#33021;&#22815;&#26816;&#27979;&#20986;AI&#21644;&#20154;&#31867;&#20869;&#23481;&#20043;&#38388;&#24494;&#22937;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
My research investigates the use of cutting-edge hybrid deep learning models to accurately differentiate between AI-generated text and human writing. I applied a robust methodology, utilising a carefully selected dataset comprising AI and human texts from various sources, each tagged with instructions. Advanced natural language processing techniques facilitated the analysis of textual features. Combining sophisticated neural networks, the custom model enabled it to detect nuanced differences between AI and human content.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#22320;&#29702;&#21644;&#22320;&#29702;&#31354;&#38388;&#39046;&#22495;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#20998;&#26512;&#20102;&#23427;&#20204;&#22312;&#21508;&#31181;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#20102;&#20854;&#21331;&#36234;&#20043;&#22788;&#20197;&#21450;&#19981;&#36275;&#20043;&#22788;&#12290;</title><link>http://arxiv.org/abs/2311.14656</link><description>&lt;p&gt;
&#25506;&#32034;&#22810;&#27169;&#24577;LLMs&#30340;&#22320;&#29702;&#21644;&#22320;&#29702;&#31354;&#38388;&#33021;&#21147;&#65306;&#24320;&#36767;&#26032;&#30340;&#39046;&#22495;
&lt;/p&gt;
&lt;p&gt;
Charting New Territories: Exploring the Geographic and Geospatial Capabilities of Multimodal LLMs. (arXiv:2311.14656v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.14656
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#22320;&#29702;&#21644;&#22320;&#29702;&#31354;&#38388;&#39046;&#22495;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#20998;&#26512;&#20102;&#23427;&#20204;&#22312;&#21508;&#31181;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#20102;&#20854;&#21331;&#36234;&#20043;&#22788;&#20197;&#21450;&#19981;&#36275;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#22312;&#24191;&#27867;&#30340;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#22320;&#29702;&#21644;&#22320;&#29702;&#31354;&#38388;&#39046;&#22495;&#30340;&#30693;&#35782;&#21644;&#33021;&#21147;&#23578;&#26410;&#34987;&#25506;&#32034;&#65292;&#23613;&#31649;&#23545;&#23548;&#33322;&#12289;&#29615;&#22659;&#30740;&#31350;&#12289;&#22478;&#24066;&#21457;&#23637;&#21644;&#28798;&#38590;&#21709;&#24212;&#31561;&#39046;&#22495;&#26377;&#28508;&#22312;&#30340;&#24191;&#27867;&#22909;&#22788;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#30340;&#23454;&#39564;&#65292;&#25506;&#32034;MLLM&#22312;&#36825;&#20123;&#39046;&#22495;&#20869;&#30340;&#21508;&#31181;&#35270;&#35273;&#33021;&#21147;&#65292;&#29305;&#21035;&#20851;&#27880;&#21069;&#27839;&#27169;&#22411;GPT-4V&#65292;&#24182;&#23558;&#20854;&#24615;&#33021;&#19982;&#24320;&#28304;&#23545;&#27604;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#25361;&#25112;&#36825;&#20123;&#27169;&#22411;&#65292;&#20351;&#29992;&#19968;&#20010;&#23567;&#35268;&#27169;&#30340;&#22320;&#29702;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;&#65292;&#27979;&#35797;&#23427;&#20204;&#22312;&#21508;&#31181;&#22797;&#26434;&#24615;&#30340;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#12290;&#20998;&#26512;&#25581;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#21738;&#20123;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#21253;&#25324;&#23427;&#20204;&#36229;&#36807;&#20154;&#31867;&#30340;&#24773;&#20917;&#65292;&#20294;&#20063;&#25581;&#31034;&#20102;&#23427;&#20204;&#20309;&#22788;&#22833;&#36133;&#65292;&#25552;&#20379;&#20102;&#23545;&#23427;&#20204;&#22312;&#22320;&#29702;&#39046;&#22495;&#33021;&#21147;&#30340;&#24179;&#34913;&#35266;&#28857;&#12290;&#20026;&#20102;&#27604;&#36739;&#21644;&#35780;&#20272;&#26410;&#26469;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#24320;&#28304;&#30340;&#23454;&#39564;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal large language models (MLLMs) have shown remarkable capabilities across a broad range of tasks but their knowledge and abilities in the geographic and geospatial domains are yet to be explored, despite potential wide-ranging benefits to navigation, environmental research, urban development, and disaster response. We conduct a series of experiments exploring various vision capabilities of MLLMs within these domains, particularly focusing on the frontier model GPT-4V, and benchmark its performance against open-source counterparts. Our methodology involves challenging these models with a small-scale geographic benchmark consisting of a suite of visual tasks, testing their abilities across a spectrum of complexity. The analysis uncovers not only where such models excel, including instances where they outperform humans, but also where they falter, providing a balanced view of their capabilities in the geographic domain. To enable the comparison and evaluation of future models, ou
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#26500;&#24314;&#21512;&#36866;&#30340;&#22522;&#20934;&#21644;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#21457;&#29616;&#65292;&#30001;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#23545;&#20110;&#25991;&#26412;-&#22270;&#20687;&#26816;&#32034;&#27169;&#22411;&#24341;&#20837;&#20102;&#19968;&#20010;&#19981;&#21487;&#35265;&#30340;&#30456;&#20851;&#20559;&#24046;&#65292;&#21363;&#20351;&#36825;&#20123;&#29983;&#25104;&#30340;&#22270;&#20687;&#22312;&#35270;&#35273;&#19978;&#19982;&#26597;&#35810;&#30340;&#30456;&#20851;&#29305;&#24449;&#30456;&#27604;&#24182;&#27809;&#26377;&#26356;&#22810;&#12290;&#36825;&#31181;&#19981;&#21487;&#35265;&#30340;&#30456;&#20851;&#20559;&#24046;&#26222;&#36941;&#23384;&#22312;&#20110;&#19981;&#21516;&#35757;&#32451;&#25968;&#25454;&#21644;&#26550;&#26500;&#30340;&#26816;&#32034;&#27169;&#22411;&#20013;&#12290;</title><link>http://arxiv.org/abs/2311.14084</link><description>&lt;p&gt;
&#30001;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#23545;&#25991;&#26412;-&#22270;&#20687;&#26816;&#32034;&#24341;&#20837;&#20102;&#19981;&#21487;&#35265;&#30340;&#30456;&#20851;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
AI-Generated Images Introduce Invisible Relevance Bias to Text-Image Retrieval. (arXiv:2311.14084v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.14084
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#26500;&#24314;&#21512;&#36866;&#30340;&#22522;&#20934;&#21644;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#21457;&#29616;&#65292;&#30001;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#23545;&#20110;&#25991;&#26412;-&#22270;&#20687;&#26816;&#32034;&#27169;&#22411;&#24341;&#20837;&#20102;&#19968;&#20010;&#19981;&#21487;&#35265;&#30340;&#30456;&#20851;&#20559;&#24046;&#65292;&#21363;&#20351;&#36825;&#20123;&#29983;&#25104;&#30340;&#22270;&#20687;&#22312;&#35270;&#35273;&#19978;&#19982;&#26597;&#35810;&#30340;&#30456;&#20851;&#29305;&#24449;&#30456;&#27604;&#24182;&#27809;&#26377;&#26356;&#22810;&#12290;&#36825;&#31181;&#19981;&#21487;&#35265;&#30340;&#30456;&#20851;&#20559;&#24046;&#26222;&#36941;&#23384;&#22312;&#20110;&#19981;&#21516;&#35757;&#32451;&#25968;&#25454;&#21644;&#26550;&#26500;&#30340;&#26816;&#32034;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29983;&#25104;&#27169;&#22411;&#30340;&#36827;&#27493;&#65292;&#30001;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#20869;&#23481;&#65288;AIGC&#65289;&#21464;&#24471;&#26356;&#21152;&#36924;&#30495;&#65292;&#28044;&#20837;&#20114;&#32852;&#32593;&#12290;&#26368;&#36817;&#30340;&#19968;&#39033;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#31181;&#29616;&#35937;&#23548;&#33268;&#20102;&#32593;&#32476;&#25628;&#32034;&#20013;&#30340;&#28304;&#20559;&#24046;&#12290;&#29305;&#21035;&#26159;&#65292;&#31070;&#32463;&#26816;&#32034;&#27169;&#22411;&#24448;&#24448;&#23558;&#29983;&#25104;&#30340;&#25991;&#26412;&#25490;&#21517;&#39640;&#20110;&#20154;&#24037;&#32534;&#20889;&#30340;&#25991;&#26412;&#12290;&#26412;&#25991;&#23558;&#36825;&#31181;&#20559;&#24046;&#30340;&#30740;&#31350;&#25193;&#23637;&#21040;&#36328;&#27169;&#24577;&#26816;&#32034;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25104;&#21151;&#26500;&#24314;&#20102;&#19968;&#20010;&#36866;&#21512;&#25506;&#32034;&#20559;&#24046;&#23384;&#22312;&#30340;&#22522;&#20934;&#12290;&#38543;&#21518;&#65292;&#22312;&#36825;&#20010;&#22522;&#20934;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#21457;&#29616;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#23545;&#20110;&#25991;&#26412;-&#22270;&#20687;&#26816;&#32034;&#27169;&#22411;&#24341;&#20837;&#20102;&#19968;&#20010;&#19981;&#21487;&#35265;&#30340;&#30456;&#20851;&#20559;&#24046;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#23613;&#31649;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#19982;&#26597;&#35810;&#30456;&#27604;&#27809;&#26377;&#26356;&#22810;&#30340;&#35270;&#35273;&#30456;&#20851;&#29305;&#24449;&#65292;&#20294;&#25991;&#26412;-&#22270;&#20687;&#26816;&#32034;&#27169;&#22411;&#24448;&#24448;&#23558;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#25490;&#21517;&#39640;&#20110;&#30495;&#23454;&#22270;&#20687;&#12290;&#36825;&#31181;&#19981;&#21487;&#35265;&#30340;&#30456;&#20851;&#20559;&#24046;&#22312;&#19981;&#21516;&#35757;&#32451;&#25968;&#25454;&#21644;&#26550;&#26500;&#30340;&#26816;&#32034;&#27169;&#22411;&#20013;&#26222;&#36941;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the advancement of generation models, AI-generated content (AIGC) is becoming more realistic, flooding the Internet. A recent study suggests that this phenomenon causes source bias in text retrieval for web search. Specifically, neural retrieval models tend to rank generated texts higher than human-written texts. In this paper, we extend the study of this bias to cross-modal retrieval. Firstly, we successfully construct a suitable benchmark to explore the existence of the bias. Subsequent extensive experiments on this benchmark reveal that AI-generated images introduce an invisible relevance bias to text-image retrieval models. Specifically, our experiments show that text-image retrieval models tend to rank the AI-generated images higher than the real images, even though the AI-generated images do not exhibit more visually relevant features to the query than real images. This invisible relevance bias is prevalent across retrieval models with varying training data and architectures
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#22312;&#22797;&#26434;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#19978;&#25506;&#32034;&#35838;&#31243;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;&#35838;&#31243;&#23398;&#20064;&#26159;&#25913;&#21892;&#22797;&#26434;&#26102;&#38388;&#24207;&#21015;&#25511;&#21046;&#20219;&#21153;&#24615;&#33021;&#30340;&#26032;&#36884;&#24452;&#65292;&#32780;&#27169;&#20223;&#23398;&#20064;&#20063;&#24212;&#35813;&#34987;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2311.13326</link><description>&lt;p&gt;
&#12298;&#22522;&#20110;&#35838;&#31243;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#30340;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#26080;&#20851;&#25511;&#21046;&#12299;
&lt;/p&gt;
&lt;p&gt;
Curriculum Learning and Imitation Learning for Model-free Control on Financial Time-series. (arXiv:2311.13326v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.13326
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#22312;&#22797;&#26434;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#19978;&#25506;&#32034;&#35838;&#31243;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;&#35838;&#31243;&#23398;&#20064;&#26159;&#25913;&#21892;&#22797;&#26434;&#26102;&#38388;&#24207;&#21015;&#25511;&#21046;&#20219;&#21153;&#24615;&#33021;&#30340;&#26032;&#36884;&#24452;&#65292;&#32780;&#27169;&#20223;&#23398;&#20064;&#20063;&#24212;&#35813;&#34987;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35838;&#31243;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#22312;&#26426;&#22120;&#20154;&#39046;&#22495;&#24050;&#34987;&#24191;&#27867;&#36816;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;&#39640;&#24230;&#38543;&#26426;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#19978;&#21033;&#29992;&#36825;&#20123;&#24819;&#27861;&#36827;&#34892;&#25511;&#21046;&#20219;&#21153;&#30340;&#30740;&#31350;&#38750;&#24120;&#26377;&#38480;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20174;&#29702;&#35770;&#21644;&#23454;&#35777;&#20004;&#20010;&#26041;&#38754;&#25506;&#35752;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#22797;&#26434;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#19978;&#30340;&#20195;&#34920;&#24615;&#25511;&#21046;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#23454;&#29616;&#20102;&#35838;&#31243;&#23398;&#20064;&#30340;&#22522;&#26412;&#24605;&#24819;&#65292;&#32780;&#36890;&#36807;&#27169;&#20223;&#23398;&#20064;&#20174;&#19987;&#23478;&#20013;&#33976;&#39311;&#20986;&#31574;&#30053;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35838;&#31243;&#23398;&#20064;&#22312;&#25913;&#36827;&#22797;&#26434;&#26102;&#38388;&#24207;&#21015;&#25511;&#21046;&#30340;&#20219;&#21153;&#24615;&#33021;&#26041;&#38754;&#24212;&#34987;&#35270;&#20026;&#19968;&#31181;&#26032;&#30340;&#26041;&#21521;&#12290;&#25105;&#20204;&#30340;&#22823;&#37327;&#38543;&#26426;&#31181;&#23376;&#22806;&#26679;&#26412;&#23454;&#35777;&#21644;&#28040;&#34701;&#30740;&#31350;&#23545;&#20110;&#26102;&#38388;&#24207;&#21015;&#25511;&#21046;&#30340;&#35838;&#31243;&#23398;&#20064;&#38750;&#24120;&#40723;&#33310;&#20154;&#24515;&#12290;&#36825;&#20123;&#21457;&#29616;&#23588;&#20854;&#40723;&#33310;&#20154;&#24515;&#65292;&#22240;&#20026;&#25105;&#20204;&#22312;&#22522;&#32447;&#19978;&#35843;&#25972;&#20102;&#25152;&#26377;&#37325;&#21472;&#30340;&#36229;&#21442;&#25968;&#65292;&#32473;&#20986;&#20102;&#22522;&#32447;&#30340;&#20248;&#21183;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#21457;&#29616;&#27169;&#20223;&#23398;&#20064;&#24212;&#35813;&#34987;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Curriculum learning and imitation learning have been leveraged extensively in the robotics domain. However, minimal research has been done on leveraging these ideas on control tasks over highly stochastic time-series data. Here, we theoretically and empirically explore these approaches in a representative control task over complex time-series data. We implement the fundamental ideas of curriculum learning via data augmentation, while imitation learning is implemented via policy distillation from an oracle. Our findings reveal that curriculum learning should be considered a novel direction in improving control-task performance over complex time-series. Our ample random-seed out-sample empirics and ablation studies are highly encouraging for curriculum learning for time-series control. These findings are especially encouraging as we tune all overlapping hyperparameters on the baseline -- giving an advantage to the baseline. On the other hand, we find that imitation learning should be use
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;$\varepsilon$-&#20998;&#25968;&#26680;&#31283;&#23450;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#20801;&#35768;&#26368;&#22810;$\varepsilon$&#27604;&#20363;&#30340;&#21487;&#33021;&#32852;&#30431;&#25104;&#20026;&#26680;&#38459;&#22622;&#32852;&#30431;&#26469;&#35299;&#20915;&#24555;&#20048;&#21338;&#24328;&#20013;&#30340;&#26680;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;&#36825;&#19968;&#25918;&#26494;&#26465;&#20214;&#21487;&#20197;&#20445;&#35777;&#23384;&#22312;&#24615;&#21644;&#22810;&#39033;&#24335;&#26102;&#38388;&#35745;&#31639;&#12290;</title><link>http://arxiv.org/abs/2311.11101</link><description>&lt;p&gt;
$\varepsilon$-&#20998;&#25968;&#26680;&#31283;&#23450;&#22312;&#24555;&#20048;&#21338;&#24328;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
$\varepsilon$-fractional Core Stability in Hedonic Games. (arXiv:2311.11101v2 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.11101
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;$\varepsilon$-&#20998;&#25968;&#26680;&#31283;&#23450;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#20801;&#35768;&#26368;&#22810;$\varepsilon$&#27604;&#20363;&#30340;&#21487;&#33021;&#32852;&#30431;&#25104;&#20026;&#26680;&#38459;&#22622;&#32852;&#30431;&#26469;&#35299;&#20915;&#24555;&#20048;&#21338;&#24328;&#20013;&#30340;&#26680;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;&#36825;&#19968;&#25918;&#26494;&#26465;&#20214;&#21487;&#20197;&#20445;&#35777;&#23384;&#22312;&#24615;&#21644;&#22810;&#39033;&#24335;&#26102;&#38388;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24555;&#20048;&#21338;&#24328;&#26159;&#19968;&#20010;&#32463;&#20856;&#30340;&#26694;&#26550;&#65292;&#29992;&#26469;&#27169;&#25311;&#25112;&#30053;&#20195;&#29702;&#20154;&#36890;&#36807;&#20010;&#20154;&#20559;&#22909;&#26469;&#24418;&#25104;&#32852;&#30431;&#12290;&#26681;&#25454;&#36825;&#20123;&#20559;&#22909;&#65292;&#29702;&#24819;&#30340;&#32852;&#30431;&#32467;&#26500;&#65288;&#21363;&#23558;&#20195;&#29702;&#20154;&#21010;&#20998;&#20026;&#21508;&#20010;&#32852;&#30431;&#65289;&#24212;&#35813;&#28385;&#36275;&#26576;&#31181;&#31283;&#23450;&#24615;&#26465;&#20214;&#12290;&#20854;&#20013;&#26368;&#20026;&#30693;&#21517;&#21644;&#33258;&#28982;&#30340;&#27010;&#24565;&#20043;&#19968;&#26159;&#26680;&#31283;&#23450;&#24615;&#12290;&#31616;&#21333;&#26469;&#35828;&#65292;&#22914;&#26524;&#27809;&#26377;&#20195;&#29702;&#20154;&#30340;&#23376;&#38598;&#24076;&#26395;&#36890;&#36807;&#37325;&#26032;&#20998;&#32452;&#25104;&#20026;&#19968;&#20010;&#25152;&#35859;&#30340;&#26680;&#38459;&#22622;&#32852;&#30431;&#26469;&#20559;&#31163;&#21407;&#32852;&#30431;&#32467;&#26500;&#65292;&#37027;&#20040;&#35813;&#21010;&#20998;&#23601;&#26159;&#26680;&#31283;&#23450;&#30340;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#26680;&#31283;&#23450;&#30340;&#21010;&#20998;&#24456;&#23569;&#23384;&#22312;&#65292;&#21363;&#20351;&#23384;&#22312;&#65292;&#23547;&#25214;&#36825;&#26679;&#30340;&#21010;&#20998;&#36890;&#24120;&#20063;&#26159;&#35745;&#31639;&#19978;&#22256;&#38590;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;$\varepsilon$-&#20998;&#25968;&#26680;&#31283;&#23450;&#30340;&#27010;&#24565;&#65292;&#21363;&#26368;&#22810;&#20801;&#35768;$\varepsilon$&#27604;&#20363;&#30340;&#21487;&#33021;&#32852;&#30431;&#25104;&#20026;&#26680;&#38459;&#22622;&#32852;&#30431;&#12290;&#20107;&#23454;&#35777;&#26126;&#65292;&#36825;&#26679;&#30340;&#25918;&#26494;&#26465;&#20214;&#21487;&#20197;&#21516;&#26102;&#20445;&#35777;&#23384;&#22312;&#24615;&#21644;&#22810;&#39033;&#24335;&#26102;&#38388;&#35745;&#31639;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#39640;&#25928;&#30340;&#31639;&#27861;&#26469;&#36820;&#22238;&#19968;&#20010;$\varepsilon$-&#20998;&#25968;&#30340;&#26680;&#31283;&#23450;&#21010;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hedonic Games (HGs) are a classical framework modeling coalition formation of strategic agents guided by their individual preferences. According to these preferences, it is desirable that a coalition structure (i.e. a partition of agents into coalitions) satisfies some form of stability. The most well-known and natural of such notions is arguably core-stability. Informally, a partition is core-stable if no subset of agents would like to deviate by regrouping in a so-called core-blocking coalition. Unfortunately, core-stable partitions seldom exist and even when they do, it is often computationally intractable to find one. To circumvent these problems, we propose the notion of $\varepsilon$-fractional core-stability, where at most an $\varepsilon$-fraction of all possible coalitions is allowed to core-block. It turns out that such a relaxation may guarantee both existence and polynomial-time computation. Specifically, we design efficient algorithms returning an $\varepsilon$-fractional 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#23398;&#20064;&#22686;&#24378;&#22411;&#29289;&#32852;&#32593;&#31995;&#32479;&#30340;&#27979;&#35797;&#36827;&#34892;&#20102;&#24418;&#24335;&#21270;&#26041;&#27861;&#30340;&#25506;&#32034;&#65292;&#20197;&#24212;&#23545;&#24403;&#21069;&#27979;&#35797;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.07377</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#23398;&#20064;&#22686;&#24378;&#22411;&#30340;&#29289;&#32852;&#32593;&#31995;&#32479;&#36827;&#34892;&#27979;&#35797;&#65306;&#19968;&#31181;&#24418;&#24335;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Testing learning-enabled cyber-physical systems with Large-Language Models: A Formal Approach. (arXiv:2311.07377v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.07377
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#23398;&#20064;&#22686;&#24378;&#22411;&#29289;&#32852;&#32593;&#31995;&#32479;&#30340;&#27979;&#35797;&#36827;&#34892;&#20102;&#24418;&#24335;&#21270;&#26041;&#27861;&#30340;&#25506;&#32034;&#65292;&#20197;&#24212;&#23545;&#24403;&#21069;&#27979;&#35797;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#25972;&#21512;&#21040;&#29289;&#32852;&#32593;&#31995;&#32479;&#65288;CPS&#65289;&#20013;&#65292;&#21487;&#20197;&#24102;&#26469;&#26174;&#33879;&#30340;&#22909;&#22788;&#65292;&#21253;&#25324;&#22686;&#24378;&#25928;&#29575;&#12289;&#39044;&#27979;&#33021;&#21147;&#12289;&#23454;&#26102;&#21709;&#24212;&#21644;&#23454;&#29616;&#33258;&#20027;&#36816;&#34892;&#12290;&#36825;&#31181;&#34701;&#21512;&#21152;&#36895;&#20102;&#19968;&#31995;&#21015;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#30340;&#24320;&#21457;&#21644;&#37096;&#32626;&#65292;&#20363;&#22914;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#12289;&#36865;&#36135;&#26080;&#20154;&#26426;&#12289;&#26381;&#21153;&#26426;&#22120;&#20154;&#21644;&#36828;&#31243;&#21307;&#30103;&#31243;&#24207;&#12290;&#28982;&#32780;&#65292;&#20154;&#24037;&#26234;&#33021;&#22686;&#24378;&#30340;CPS&#30340;&#36719;&#20214;&#24320;&#21457;&#29983;&#21629;&#21608;&#26399;&#65288;SDLC&#65289;&#19982;&#20256;&#32479;&#26041;&#27861;&#23384;&#22312;&#26126;&#26174;&#30340;&#24046;&#24322;&#65292;&#20855;&#26377;&#25968;&#25454;&#21644;&#23398;&#20064;&#20316;&#20026;&#20004;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#29616;&#26377;&#30340;&#39564;&#35777;&#21644;&#39564;&#35777;&#25216;&#26415;&#24120;&#24120;&#19981;&#36275;&#20197;&#24212;&#23545;&#36825;&#20123;&#26032;&#30340;&#33539;&#24335;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#30830;&#23450;&#30830;&#20445;&#23398;&#20064;&#22686;&#24378;&#22411;CPS&#24418;&#24335;&#21270;&#23433;&#20840;&#24615;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;&#25105;&#20204;&#39318;&#20808;&#32771;&#23519;&#20102;&#20316;&#20026;&#39564;&#35777;&#21644;&#39564;&#35777;&#26368;&#23454;&#29992;&#26041;&#27861;&#30340;&#27979;&#35797;&#65292;&#24635;&#32467;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#35770;&#12290;&#35748;&#35782;&#21040;&#24403;&#21069;&#27979;&#35797;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;
&lt;/p&gt;
&lt;p&gt;
The integration of machine learning (ML) into cyber-physical systems (CPS) offers significant benefits, including enhanced efficiency, predictive capabilities, real-time responsiveness, and the enabling of autonomous operations. This convergence has accelerated the development and deployment of a range of real-world applications, such as autonomous vehicles, delivery drones, service robots, and telemedicine procedures. However, the software development life cycle (SDLC) for AI-infused CPS diverges significantly from traditional approaches, featuring data and learning as two critical components. Existing verification and validation techniques are often inadequate for these new paradigms. In this study, we pinpoint the main challenges in ensuring formal safety for learningenabled CPS.We begin by examining testing as the most pragmatic method for verification and validation, summarizing the current state-of-the-art methodologies. Recognizing the limitations in current testing approaches t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19977;&#31181;&#22522;&#20110;&#21464;&#20998;&#37327;&#23376;&#32447;&#36335;&#30340;&#36827;&#21270;&#20248;&#21270;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#21464;&#20307;&#65292;&#24182;&#22312;Coin Game&#29615;&#22659;&#20013;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#30456;&#27604;&#20110;&#32463;&#20856;&#26041;&#27861;&#34920;&#29616;&#26174;&#33879;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2311.05546</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#37327;&#23376;&#24378;&#21270;&#23398;&#20064;&#20351;&#29992;&#36827;&#21270;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Multi-Agent Quantum Reinforcement Learning using Evolutionary Optimization. (arXiv:2311.05546v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.05546
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19977;&#31181;&#22522;&#20110;&#21464;&#20998;&#37327;&#23376;&#32447;&#36335;&#30340;&#36827;&#21270;&#20248;&#21270;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#21464;&#20307;&#65292;&#24182;&#22312;Coin Game&#29615;&#22659;&#20013;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#30456;&#27604;&#20110;&#32463;&#20856;&#26041;&#27861;&#34920;&#29616;&#26174;&#33879;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#22312;&#33258;&#21160;&#39550;&#39542;&#21644;&#20854;&#20182;&#26234;&#33021;&#20135;&#19994;&#24212;&#29992;&#26041;&#38754;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#21033;&#29992;&#37327;&#23376;&#21147;&#23398;&#30340;&#22266;&#26377;&#23646;&#24615;&#65292;&#37319;&#29992;&#26032;&#30340;&#26377;&#24076;&#26395;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#26174;&#33879;&#20943;&#23569;&#27169;&#22411;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#26799;&#24230;&#30340;&#22810;&#26234;&#33021;&#20307;&#37327;&#23376;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#24120;&#24120;&#38754;&#20020;&#36139;&#30240;&#24179;&#21488;&#38382;&#39064;&#65292;&#38459;&#30861;&#20102;&#23427;&#20204;&#19982;&#32463;&#20856;&#26041;&#27861;&#24615;&#33021;&#30340;&#21305;&#37197;&#12290;&#25105;&#20204;&#22312;&#29616;&#26377;&#30340;&#26080;&#26799;&#24230;&#37327;&#23376;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22522;&#30784;&#19978;&#26500;&#24314;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#31181;&#22522;&#20110;&#21464;&#20998;&#37327;&#23376;&#32447;&#36335;&#30340;&#36827;&#21270;&#20248;&#21270;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#21464;&#20307;&#12290;&#25105;&#20204;&#22312;Coin Game&#29615;&#22659;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#36951;&#20256;&#21464;&#31181;&#65292;&#24182;&#19982;&#32463;&#20856;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#21464;&#20998;&#37327;&#23376;&#32447;&#36335;&#26041;&#27861;&#30456;&#27604;&#20110;&#20855;&#26377;&#31867;&#20284;&#21442;&#25968;&#25968;&#37327;&#30340;&#31070;&#32463;&#32593;&#32476;&#34920;&#29616;&#26174;&#33879;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-Agent Reinforcement Learning is becoming increasingly more important in times of autonomous driving and other smart industrial applications. Simultaneously a promising new approach to Reinforcement Learning arises using the inherent properties of quantum mechanics, reducing the trainable parameters of a model significantly. However, gradient-based Multi-Agent Quantum Reinforcement Learning methods often have to struggle with barren plateaus, holding them back from matching the performance of classical approaches. We build upon an existing approach for gradient free Quantum Reinforcement Learning and propose three genetic variations with Variational Quantum Circuits for Multi-Agent Reinforcement Learning using evolutionary optimization. We evaluate our genetic variations in the Coin Game environment and also compare them to classical approaches. We showed that our Variational Quantum Circuit approaches perform significantly better compared to a neural network with a similar amount
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#26512;&#21644;&#27604;&#36739;Transformer&#27169;&#22411;&#20013;&#31867;&#20284;&#30340;&#24207;&#21015;&#32487;&#32493;&#20219;&#21153;&#30340;&#30005;&#36335;&#65292;&#30740;&#31350;&#21457;&#29616;&#20849;&#20139;&#30340;&#35745;&#31639;&#32467;&#26500;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#34892;&#20026;&#39044;&#27979;&#33021;&#21147;&#12289;&#38169;&#35823;&#35782;&#21035;&#33021;&#21147;&#21644;&#32534;&#36753;&#36807;&#31243;&#30340;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.04131</link><description>&lt;p&gt;
&#22312;Transformer&#20013;&#23450;&#20301;&#36328;&#20219;&#21153;&#24207;&#21015;&#32487;&#32493;&#30005;&#36335;
&lt;/p&gt;
&lt;p&gt;
Locating Cross-Task Sequence Continuation Circuits in Transformers. (arXiv:2311.04131v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.04131
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#26512;&#21644;&#27604;&#36739;Transformer&#27169;&#22411;&#20013;&#31867;&#20284;&#30340;&#24207;&#21015;&#32487;&#32493;&#20219;&#21153;&#30340;&#30005;&#36335;&#65292;&#30740;&#31350;&#21457;&#29616;&#20849;&#20139;&#30340;&#35745;&#31639;&#32467;&#26500;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#34892;&#20026;&#39044;&#27979;&#33021;&#21147;&#12289;&#38169;&#35823;&#35782;&#21035;&#33021;&#21147;&#21644;&#32534;&#36753;&#36807;&#31243;&#30340;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;Transformer&#27169;&#22411;&#22312;&#35821;&#35328;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#33021;&#21147;&#65292;&#20294;&#20854;&#22797;&#26434;&#30340;&#26550;&#26500;&#20351;&#20854;&#38590;&#20197;&#35299;&#37322;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#26088;&#22312;&#23558;Transformer&#27169;&#22411;&#36824;&#21407;&#20026;&#21487;&#35835;&#30340;&#30005;&#36335;&#34920;&#31034;&#65292;&#29992;&#20110;&#23454;&#29616;&#31639;&#27861;&#21151;&#33021;&#12290;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#21644;&#27604;&#36739;&#31867;&#20284;&#30340;&#24207;&#21015;&#32487;&#32493;&#20219;&#21153;&#30340;&#30005;&#36335;&#26469;&#25193;&#23637;&#36825;&#39033;&#30740;&#31350;&#65292;&#20854;&#20013;&#21253;&#25324;&#25968;&#23383;&#12289;&#25968;&#23383;&#35789;&#21644;&#26376;&#20221;&#30340;&#36882;&#22686;&#24207;&#21015;&#12290;&#36890;&#36807;&#24212;&#29992;&#30005;&#36335;&#20998;&#26512;&#25216;&#26415;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#36127;&#36131;&#26816;&#27979;&#24207;&#21015;&#25104;&#21592;&#21644;&#39044;&#27979;&#24207;&#21015;&#20013;&#19979;&#19968;&#20010;&#25104;&#21592;&#30340;&#20851;&#38190;&#23376;&#30005;&#36335;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#35821;&#20041;&#30456;&#20851;&#24207;&#21015;&#20381;&#36182;&#20110;&#20855;&#26377;&#31867;&#20284;&#20316;&#29992;&#30340;&#20849;&#20139;&#30005;&#36335;&#23376;&#22270;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#35760;&#24405;&#20849;&#20139;&#30340;&#35745;&#31639;&#32467;&#26500;&#33021;&#22815;&#26356;&#22909;&#22320;&#39044;&#27979;&#27169;&#22411;&#34892;&#20026;&#65292;&#35782;&#21035;&#38169;&#35823;&#65292;&#24182;&#36827;&#34892;&#26356;&#23433;&#20840;&#30340;&#32534;&#36753;&#36807;&#31243;&#12290;&#36825;&#31181;&#23545;Transformer&#30340;&#26426;&#26800;&#29702;&#35299;&#26159;&#26500;&#24314;&#26356;&#20581;&#22766;&#12289;&#35843;&#35797;&#21644;&#32534;&#36753;&#26356;&#23433;&#20840;&#30340;&#27169;&#22411;&#30340;&#20851;&#38190;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
While transformer models exhibit strong capabilities on linguistic tasks, their complex architectures make them difficult to interpret. Recent work has aimed to reverse engineer transformer models into human-readable representations called circuits that implement algorithmic functions. We extend this research by analyzing and comparing circuits for similar sequence continuation tasks, which include increasing sequences of digits, number words, and months. Through the application of circuit analysis techniques, we identify key sub-circuits responsible for detecting sequence members and for predicting the next member in a sequence. Our analysis reveals that semantically related sequences rely on shared circuit subgraphs with analogous roles. Overall, documenting shared computational structures enables better prediction of model behaviors, identification of errors, and safer editing procedures. This mechanistic understanding of transformers is a critical step towards building more robust,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Alympics&#65292;&#19968;&#20010;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#20154;&#36827;&#34892;&#21338;&#24328;&#35770;&#30740;&#31350;&#30340;&#31995;&#32479;&#24615;&#27169;&#25311;&#26694;&#26550;&#12290;&#36890;&#36807;&#27169;&#25311;&#20154;&#31867;&#25112;&#30053;&#20114;&#21160;&#65292;&#26694;&#26550;&#33021;&#22815;&#23450;&#24615;&#21644;&#23450;&#37327;&#22320;&#20998;&#26512;&#28216;&#25103;&#20915;&#23450;&#22240;&#32032;&#12289;&#31574;&#30053;&#21644;&#32467;&#26524;&#65292;&#24182;&#23545;&#20195;&#29702;&#20154;&#22312;&#25112;&#30053;&#20915;&#31574;&#22330;&#26223;&#20013;&#30340;&#34920;&#29616;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2311.03220</link><description>&lt;p&gt;
ALYMPICS&#65306;&#35821;&#35328;&#20195;&#29702;&#20154;&#19982;&#21338;&#24328;&#35770;&#30456;&#36935;&#8212;&#8212;&#29992;AI&#20195;&#29702;&#20154;&#25506;&#32034;&#25112;&#30053;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
ALYMPICS: Language Agents Meet Game Theory -- Exploring Strategic Decision-Making with AI Agents. (arXiv:2311.03220v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.03220
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Alympics&#65292;&#19968;&#20010;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#20154;&#36827;&#34892;&#21338;&#24328;&#35770;&#30740;&#31350;&#30340;&#31995;&#32479;&#24615;&#27169;&#25311;&#26694;&#26550;&#12290;&#36890;&#36807;&#27169;&#25311;&#20154;&#31867;&#25112;&#30053;&#20114;&#21160;&#65292;&#26694;&#26550;&#33021;&#22815;&#23450;&#24615;&#21644;&#23450;&#37327;&#22320;&#20998;&#26512;&#28216;&#25103;&#20915;&#23450;&#22240;&#32032;&#12289;&#31574;&#30053;&#21644;&#32467;&#26524;&#65292;&#24182;&#23545;&#20195;&#29702;&#20154;&#22312;&#25112;&#30053;&#20915;&#31574;&#22330;&#26223;&#20013;&#30340;&#34920;&#29616;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Alympics&#65288;&#20195;&#29702;&#20154;&#30340;&#22885;&#36816;&#20250;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20195;&#29702;&#20154;&#36827;&#34892;&#21338;&#24328;&#35770;&#30740;&#31350;&#30340;&#31995;&#32479;&#24615;&#27169;&#25311;&#26694;&#26550;&#12290;Alympics&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#21151;&#33021;&#24179;&#21488;&#65292;&#29992;&#20110;&#30740;&#31350;&#22797;&#26434;&#30340;&#21338;&#24328;&#35770;&#38382;&#39064;&#65292;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#25511;&#21046;&#29615;&#22659;&#26469;&#27169;&#25311;&#19982;LLM&#20195;&#29702;&#20154;&#36827;&#34892;&#31867;&#20284;&#20154;&#31867;&#30340;&#25112;&#30053;&#20114;&#21160;&#65292;&#24357;&#21512;&#20102;&#29702;&#35770;&#21338;&#24328;&#35770;&#21644;&#23454;&#35777;&#30740;&#31350;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#22312;&#25105;&#20204;&#30340;&#35797;&#28857;&#26696;&#20363;&#30740;&#31350;&#20013;&#65292;&#8220;&#27700;&#36164;&#28304;&#20998;&#37197;&#25361;&#25112;&#8221;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#20851;&#27880;&#31232;&#32570;&#29983;&#23384;&#36164;&#28304;&#22810;&#36718;&#25293;&#21334;&#30340;&#25361;&#25112;&#24615;&#25112;&#30053;&#28216;&#25103;&#26469;&#25506;&#32034;Alympics&#12290;&#36825;&#39033;&#30740;&#31350;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#22312;&#23450;&#24615;&#21644;&#23450;&#37327;&#20998;&#26512;&#28216;&#25103;&#20915;&#23450;&#22240;&#32032;&#12289;&#31574;&#30053;&#21644;&#32467;&#26524;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#20154;&#31867;&#35780;&#20272;&#21644;&#23545;LLM&#20195;&#29702;&#20154;&#22312;&#25112;&#30053;&#20915;&#31574;&#22330;&#26223;&#20013;&#30340;&#28145;&#20837;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#19981;&#20165;&#25193;&#23637;&#20102;&#23545;LLM&#20195;&#29702;&#20154;&#27169;&#25311;&#20154;&#31867;&#25112;&#30053;&#34892;&#20026;&#33021;&#21147;&#30340;&#29702;&#35299;&#65292;&#36824;
&lt;/p&gt;
&lt;p&gt;
This paper introduces Alympics (Olympics for Agents), a systematic simulation framework utilizing Large Language Model (LLM) agents for game theory research. Alympics creates a versatile platform for studying complex game theory problems, bridging the gap between theoretical game theory and empirical investigations by providing a controlled environment for simulating human-like strategic interactions with LLM agents. In our pilot case study, the "Water Allocation Challenge," we explore Alympics through a challenging strategic game focused on the multi-round auction on scarce survival resources. This study demonstrates the framework's ability to qualitatively and quantitatively analyze game determinants, strategies, and outcomes. Additionally, we conduct a comprehensive human assessment and an in-depth evaluation of LLM agents in strategic decision-making scenarios. Our findings not only expand the understanding of LLM agents' proficiency in emulating human strategic behavior but also h
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;&#28155;&#21152;&#26426;&#21046;&#31227;&#20301;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;SAMS-VAE&#65289;&#65292;&#29992;&#20110;&#24314;&#27169;&#32454;&#32990;&#30340;&#25200;&#21160;&#24773;&#20917;&#65292;&#24182;&#32467;&#21512;&#22797;&#21512;&#24615;&#12289;&#35299;&#32544;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#36890;&#36807;&#31232;&#30095;&#21270;&#22788;&#29702;&#20840;&#23616;&#28508;&#21464;&#37327;&#65292;SAMS-VAE&#33021;&#22815;&#35782;&#21035;&#20986;&#29305;&#23450;&#20110;&#24178;&#25200;&#30340;&#28508;&#22312;&#23376;&#31354;&#38388;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#23450;&#37327;&#21644;&#23450;&#24615;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2311.02794</link><description>&lt;p&gt;
&#29992;&#31232;&#30095;&#28155;&#21152;&#26426;&#21046;&#31227;&#20301;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#23545;&#32454;&#32990;&#25200;&#21160;&#36827;&#34892;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Modelling Cellular Perturbations with the Sparse Additive Mechanism Shift Variational Autoencoder. (arXiv:2311.02794v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.02794
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;&#28155;&#21152;&#26426;&#21046;&#31227;&#20301;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;SAMS-VAE&#65289;&#65292;&#29992;&#20110;&#24314;&#27169;&#32454;&#32990;&#30340;&#25200;&#21160;&#24773;&#20917;&#65292;&#24182;&#32467;&#21512;&#22797;&#21512;&#24615;&#12289;&#35299;&#32544;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#36890;&#36807;&#31232;&#30095;&#21270;&#22788;&#29702;&#20840;&#23616;&#28508;&#21464;&#37327;&#65292;SAMS-VAE&#33021;&#22815;&#35782;&#21035;&#20986;&#29305;&#23450;&#20110;&#24178;&#25200;&#30340;&#28508;&#22312;&#23376;&#31354;&#38388;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#23450;&#37327;&#21644;&#23450;&#24615;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#38024;&#23545;&#24178;&#39044;&#19979;&#35266;&#27979;&#25968;&#25454;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#31185;&#23398;&#39046;&#22495;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#20363;&#22914;&#65292;&#22312;&#33647;&#29289;&#21457;&#29616;&#20013;&#65292;&#38656;&#35201;&#23545;&#32454;&#32990;&#30340;&#22810;&#31181;&#24178;&#39044;&#25928;&#24212;&#36827;&#34892;&#24314;&#27169;&#65292;&#20197;&#25581;&#31034;&#26410;&#30693;&#30340;&#29983;&#29289;&#20316;&#29992;&#26426;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31232;&#30095;&#28155;&#21152;&#26426;&#21046;&#31227;&#20301;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;SAMS-VAE&#65289;&#65292;&#20197;&#32452;&#21512;&#22797;&#21512;&#24615;&#12289;&#35299;&#32544;&#21644;&#21487;&#35299;&#37322;&#24615;&#36827;&#34892;&#25200;&#21160;&#27169;&#22411;&#12290;SAMS-VAE&#23558;&#25200;&#21160;&#26679;&#26412;&#30340;&#28508;&#22312;&#29366;&#24577;&#24314;&#27169;&#20026;&#19968;&#20010;&#23616;&#37096;&#28508;&#22312;&#21464;&#37327;&#21644;&#31232;&#30095;&#20840;&#23616;&#21464;&#37327;&#20043;&#21644;&#65292;&#29992;&#20110;&#25429;&#25417;&#26679;&#26412;&#29305;&#23450;&#30340;&#21464;&#21270;&#21644;&#28508;&#22312;&#24178;&#39044;&#25928;&#24212;&#12290;&#20851;&#38190;&#26159;&#65292;SAMS-VAE&#36890;&#36807;&#23545;&#21508;&#20010;&#24178;&#39044;&#30340;&#20840;&#23616;&#28508;&#21464;&#37327;&#36827;&#34892;&#31232;&#30095;&#21270;&#22788;&#29702;&#65292;&#20174;&#32780;&#35782;&#21035;&#20986;&#35299;&#32544;&#30340;&#12289;&#24178;&#25200;&#29305;&#23450;&#30340;&#28508;&#22312;&#23376;&#31354;&#38388;&#65292;&#36825;&#20123;&#23376;&#31354;&#38388;&#20855;&#26377;&#28789;&#27963;&#30340;&#32452;&#21512;&#24615;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#27969;&#34892;&#30340;&#21333;&#32454;&#32990;&#27979;&#24207;&#25968;&#25454;&#38598;&#19978;&#23450;&#37327;&#21644;&#23450;&#24615;&#35780;&#20272;&#20102;SAMS-VAE&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative models of observations under interventions have been a vibrant topic of interest across machine learning and the sciences in recent years. For example, in drug discovery, there is a need to model the effects of diverse interventions on cells in order to characterize unknown biological mechanisms of action. We propose the Sparse Additive Mechanism Shift Variational Autoencoder, SAMS-VAE, to combine compositionality, disentanglement, and interpretability for perturbation models. SAMS-VAE models the latent state of a perturbed sample as the sum of a local latent variable capturing sample-specific variation and sparse global variables of latent intervention effects. Crucially, SAMS-VAE sparsifies these global latent variables for individual perturbations to identify disentangled, perturbation-specific latent subspaces that are flexibly composable. We evaluate SAMS-VAE both quantitatively and qualitatively on a range of tasks using two popular single cell sequencing datasets. In 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31163;&#25955;&#25193;&#25955;&#23398;&#20064;&#26080;&#30417;&#30563;&#30340;&#33258;&#21160;&#39550;&#39542;&#19990;&#30028;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;VQVAE&#23545;&#20256;&#24863;&#22120;&#35266;&#23519;&#36827;&#34892;&#26631;&#35760;&#21270;&#24182;&#36890;&#36807;&#31163;&#25955;&#25193;&#25955;&#39044;&#27979;&#26410;&#26469;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#28857;&#20113;&#35266;&#23519;&#20013;&#23454;&#29616;&#20102;&#26174;&#33879;&#25913;&#36827;&#65292;&#23558;1&#31186;&#39044;&#27979;&#30340;SOTA Chamfer&#36317;&#31163;&#38477;&#20302;&#20102;65%&#20197;&#19978;&#12290;</title><link>http://arxiv.org/abs/2311.01017</link><description>&lt;p&gt;
&#36890;&#36807;&#31163;&#25955;&#25193;&#25955;&#23398;&#20064;&#26080;&#30417;&#30563;&#30340;&#33258;&#21160;&#39550;&#39542;&#19990;&#30028;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning Unsupervised World Models for Autonomous Driving via Discrete Diffusion. (arXiv:2311.01017v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01017
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31163;&#25955;&#25193;&#25955;&#23398;&#20064;&#26080;&#30417;&#30563;&#30340;&#33258;&#21160;&#39550;&#39542;&#19990;&#30028;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;VQVAE&#23545;&#20256;&#24863;&#22120;&#35266;&#23519;&#36827;&#34892;&#26631;&#35760;&#21270;&#24182;&#36890;&#36807;&#31163;&#25955;&#25193;&#25955;&#39044;&#27979;&#26410;&#26469;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#28857;&#20113;&#35266;&#23519;&#20013;&#23454;&#29616;&#20102;&#26174;&#33879;&#25913;&#36827;&#65292;&#23558;1&#31186;&#39044;&#27979;&#30340;SOTA Chamfer&#36317;&#31163;&#38477;&#20302;&#20102;65%&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#19990;&#30028;&#27169;&#22411;&#21487;&#20197;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#25945;&#20250;&#26234;&#33021;&#20307;&#19990;&#30028;&#30340;&#36816;&#20316;&#26041;&#24335;&#12290;&#23613;&#31649;&#23427;&#21487;&#20197;&#30475;&#20316;&#26159;&#24207;&#21015;&#24314;&#27169;&#30340;&#29305;&#27530;&#24773;&#20917;&#65292;&#20294;&#22312;&#33258;&#21160;&#39550;&#39542;&#31561;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#65292;&#19982;&#20351;&#29992;&#29983;&#25104;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;&#65288;GPT&#65289;&#25193;&#23637;&#35821;&#35328;&#27169;&#22411;&#30456;&#27604;&#65292;&#25193;&#23637;&#19990;&#30028;&#27169;&#22411;&#30340;&#36827;&#23637;&#30456;&#23545;&#36739;&#24930;&#12290;&#25105;&#20204;&#25351;&#20986;&#20102;&#20004;&#20010;&#20027;&#35201;&#29942;&#39048;&#65306;&#22788;&#29702;&#22797;&#26434;&#21644;&#26080;&#32467;&#26500;&#30340;&#35266;&#23519;&#31354;&#38388;&#20197;&#21450;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19990;&#30028;&#24314;&#27169;&#26041;&#27861;&#65292;&#39318;&#20808;&#20351;&#29992;VQVAE&#23545;&#20256;&#24863;&#22120;&#35266;&#23519;&#36827;&#34892;&#26631;&#35760;&#21270;&#65292;&#28982;&#21518;&#36890;&#36807;&#31163;&#25955;&#25193;&#25955;&#39044;&#27979;&#26410;&#26469;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#24182;&#34892;&#35299;&#30721;&#21644;&#21435;&#22122;&#26631;&#35760;&#65292;&#25105;&#20204;&#23558;&#36974;&#34109;&#29983;&#25104;&#22270;&#20687;&#36716;&#25442;&#22120;&#36716;&#25442;&#20026;&#31163;&#25955;&#25193;&#25955;&#26694;&#26550;&#65292;&#24182;&#36827;&#34892;&#20102;&#19968;&#20123;&#31616;&#21333;&#30340;&#25913;&#36827;&#65292;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;&#24403;&#24212;&#29992;&#20110;&#28857;&#20113;&#35266;&#23519;&#30340;&#19990;&#30028;&#27169;&#22411;&#23398;&#20064;&#26102;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#23558;1&#31186;&#39044;&#27979;&#30340;SOTA Chamfer&#36317;&#31163;&#38477;&#20302;&#20102;65%&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning world models can teach an agent how the world works in an unsupervised manner. Even though it can be viewed as a special case of sequence modeling, progress for scaling world models on robotic applications such as autonomous driving has been somewhat less rapid than scaling language models with Generative Pre-trained Transformers (GPT). We identify two reasons as major bottlenecks: dealing with complex and unstructured observation space, and having a scalable generative model. Consequently, we propose a novel world modeling approach that first tokenizes sensor observations with VQVAE, then predicts the future via discrete diffusion. To efficiently decode and denoise tokens in parallel, we recast Masked Generative Image Transformer into the discrete diffusion framework with a few simple changes, resulting in notable improvement. When applied to learning world models on point cloud observations, our model reduces prior SOTA Chamfer distance by more than 65% for 1s prediction, an
&lt;/p&gt;</description></item><item><title>&#36817;&#26399;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#20135;&#29983;&#20102;&#19968;&#31181;&#20559;&#35265;&#65292;&#20542;&#21521;&#20110;&#23558;LLM&#29983;&#25104;&#30340;&#25991;&#26723;&#25490;&#21517;&#36739;&#39640;&#12290;&#36825;&#31181;&#8220;&#26469;&#28304;&#20559;&#35265;&#8221;&#21487;&#33021;&#23545;&#20449;&#24687;&#35775;&#38382;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.20501</link><description>&lt;p&gt;
LLM&#21487;&#33021;&#20027;&#23548;&#20449;&#24687;&#35775;&#38382;&#65306;&#31070;&#32463;&#26816;&#32034;&#22120;&#23545;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#23384;&#22312;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
LLMs may Dominate Information Access: Neural Retrievers are Biased Towards LLM-Generated Texts. (arXiv:2310.20501v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20501
&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#20135;&#29983;&#20102;&#19968;&#31181;&#20559;&#35265;&#65292;&#20542;&#21521;&#20110;&#23558;LLM&#29983;&#25104;&#30340;&#25991;&#26723;&#25490;&#21517;&#36739;&#39640;&#12290;&#36825;&#31181;&#8220;&#26469;&#28304;&#20559;&#35265;&#8221;&#21487;&#33021;&#23545;&#20449;&#24687;&#35775;&#38382;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#22312;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#24212;&#29992;&#65292;&#23588;&#20854;&#26159;&#22312;&#32593;&#32476;&#25628;&#32034;&#26041;&#38754;&#65292;&#24443;&#24213;&#25913;&#21464;&#20102;&#33539;&#24335;&#12290;&#30001;&#20110;&#20854;&#22312;&#29983;&#25104;&#31867;&#20154;&#25991;&#26412;&#26041;&#38754;&#30340;&#21331;&#36234;&#33021;&#21147;&#65292;LLMs&#22312;&#20114;&#32852;&#32593;&#19978;&#21019;&#36896;&#20102;&#22823;&#37327;&#30340;&#25991;&#26412;&#12290;&#22240;&#27492;&#65292;LLMs&#26102;&#20195;&#30340;IR&#31995;&#32479;&#38754;&#20020;&#19968;&#20010;&#26032;&#30340;&#25361;&#25112;&#65306;&#32034;&#24341;&#30340;&#25991;&#26723;&#19981;&#20165;&#26159;&#30001;&#20154;&#31867;&#25776;&#20889;&#30340;&#65292;&#32780;&#19988;&#36824;&#21253;&#25324;&#30001;LLMs&#33258;&#21160;&#29983;&#25104;&#30340;&#25991;&#26723;&#12290;&#36825;&#20123;LLM&#29983;&#25104;&#30340;&#25991;&#26723;&#22914;&#20309;&#24433;&#21709;IR&#31995;&#32479;&#26159;&#19968;&#20010;&#32039;&#36843;&#19988;&#23578;&#26410;&#25506;&#32034;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22312;&#28041;&#21450;&#20154;&#31867;&#32534;&#20889;&#21644;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#19981;&#21516;IR&#27169;&#22411;&#30340;&#22330;&#26223;&#20013;&#36827;&#34892;&#20102;&#23450;&#37327;&#35780;&#20272;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#31070;&#32463;&#26816;&#32034;&#27169;&#22411;&#20542;&#21521;&#20110;&#23558;LLM&#29983;&#25104;&#30340;&#25991;&#26723;&#25490;&#21517;&#36739;&#39640;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#31070;&#32463;&#26816;&#32034;&#27169;&#22411;&#23545;LLM&#29983;&#25104;&#25991;&#26412;&#30340;&#20559;&#35265;&#31216;&#20026;&#8220;&#26469;&#28304;&#20559;&#35265;&#8221;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#20559;&#35265;&#19981;&#20165;&#38480;&#20110;f&#26041;&#30456;&#24403;&#30340;&#24773;&#20917;&#65292;&#32780;&#19988;&#22312;&#20998;&#31867;&#20219;&#21153;&#19978;&#20063;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, the emergence of large language models (LLMs) has revolutionized the paradigm of information retrieval (IR) applications, especially in web search. With their remarkable capabilities in generating human-like texts, LLMs have created enormous texts on the Internet. As a result, IR systems in the LLMs era are facing a new challenge: the indexed documents now are not only written by human beings but also automatically generated by the LLMs. How these LLM-generated documents influence the IR systems is a pressing and still unexplored question. In this work, we conduct a quantitative evaluation of different IR models in scenarios where both human-written and LLM-generated texts are involved. Surprisingly, our findings indicate that neural retrieval models tend to rank LLM-generated documents higher. We refer to this category of biases in neural retrieval models towards the LLM-generated text as the \textbf{source bias}. Moreover, we discover that this bias is not confined to the f
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;PiNMT&#27169;&#22411;&#65292;&#23558;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#25972;&#21512;&#21040;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#65292;&#36890;&#36807;&#19977;&#20010;&#20851;&#38190;&#37096;&#20998;&#21644;&#20004;&#31181;&#35757;&#32451;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#22312;IW&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.19680</link><description>&lt;p&gt;
&#23558;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#25972;&#21512;&#21040;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;
&lt;/p&gt;
&lt;p&gt;
Integrating Pre-trained Language Model into Neural Machine Translation. (arXiv:2310.19680v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19680
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;PiNMT&#27169;&#22411;&#65292;&#23558;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#25972;&#21512;&#21040;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#65292;&#36890;&#36807;&#19977;&#20010;&#20851;&#38190;&#37096;&#20998;&#21644;&#20004;&#31181;&#35757;&#32451;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#22312;IW&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24191;&#27867;&#30340;&#30740;&#31350;&#21644;&#24320;&#21457;&#65292;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#24050;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#37325;&#35201;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#39640;&#36136;&#37327;&#30340;&#21452;&#35821;&#35821;&#35328;&#23545;&#25968;&#25454;&#30340;&#19981;&#36275;&#20173;&#28982;&#26159;&#25552;&#39640;NMT&#24615;&#33021;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#19968;&#30452;&#22312;&#25506;&#32034;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;PLM&#21644;NMT&#27169;&#22411;&#20043;&#38388;&#30340;&#19981;&#20860;&#23481;&#38382;&#39064;&#23578;&#26410;&#35299;&#20915;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;PLM&#25972;&#21512;&#30340;NMT&#65288;PiNMT&#65289;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;PiNMT&#27169;&#22411;&#30001;&#19977;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#32452;&#25104;&#65292;&#20998;&#21035;&#26159;PLM&#22810;&#23618;&#36716;&#25442;&#22120;&#65292;&#23884;&#20837;&#34701;&#21512;&#21644;&#20313;&#24358;&#23545;&#40784;&#65292;&#27599;&#20010;&#37096;&#20998;&#22312;&#21521;NMT&#25552;&#20379;&#26377;&#25928;&#30340;PLM&#20449;&#24687;&#26041;&#38754;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#20171;&#32461;&#20102;&#20004;&#31181;&#35757;&#32451;&#31574;&#30053;&#65292;&#20998;&#21035;&#26159;&#20998;&#31163;&#23398;&#20064;&#29575;&#21644;&#21452;&#27493;&#35757;&#32451;&#12290;&#36890;&#36807;&#23454;&#26045;&#25152;&#25552;&#20986;&#30340;PiNMT&#27169;&#22411;&#21644;&#35757;&#32451;&#31574;&#30053;&#65292;&#22312;IW&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Machine Translation (NMT) has become a significant technology in natural language processing through extensive research and development. However, the deficiency of high-quality bilingual language pair data still poses a major challenge to improving NMT performance. Recent studies have been exploring the use of contextual information from pre-trained language model (PLM) to address this problem. Yet, the issue of incompatibility between PLM and NMT model remains unresolved. This study proposes PLM-integrated NMT (PiNMT) model to overcome the identified problems. PiNMT model consists of three critical components, PLM Multi Layer Converter, Embedding Fusion, and Cosine Alignment, each playing a vital role in providing effective PLM information to NMT. Furthermore, two training strategies, Separate Learning Rates and Dual Step Training, are also introduced in this paper. By implementing the proposed PiNMT model and training strategy, we achieve state-of-the-art performance on the IW
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#35760;&#24518;&#25200;&#21160;&#26041;&#31243;&#65288;MPE&#65289;&#65292;&#35813;&#26041;&#31243;&#36890;&#36807;&#24212;&#29992;&#36125;&#21494;&#26031;&#21407;&#29702;&#23558;&#27169;&#22411;&#30340;&#25935;&#24863;&#24615;&#19982;&#35757;&#32451;&#25968;&#25454;&#30340;&#25200;&#21160;&#32852;&#31995;&#36215;&#26469;&#65292;&#24182;&#19988;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#27169;&#22411;&#22312;&#26410;&#35265;&#27979;&#35797;&#25968;&#25454;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.19273</link><description>&lt;p&gt;
The Memory Perturbation Equation: Understanding Model's Sensitivity to Data&#65288;&#29702;&#35299;&#27169;&#22411;&#23545;&#25968;&#25454;&#30340;&#25935;&#24863;&#24615;&#30340;&#35760;&#24518;&#25200;&#21160;&#26041;&#31243;&#65289;
&lt;/p&gt;
&lt;p&gt;
The Memory Perturbation Equation: Understanding Model's Sensitivity to Data. (arXiv:2310.19273v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19273
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#35760;&#24518;&#25200;&#21160;&#26041;&#31243;&#65288;MPE&#65289;&#65292;&#35813;&#26041;&#31243;&#36890;&#36807;&#24212;&#29992;&#36125;&#21494;&#26031;&#21407;&#29702;&#23558;&#27169;&#22411;&#30340;&#25935;&#24863;&#24615;&#19982;&#35757;&#32451;&#25968;&#25454;&#30340;&#25200;&#21160;&#32852;&#31995;&#36215;&#26469;&#65292;&#24182;&#19988;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#27169;&#22411;&#22312;&#26410;&#35265;&#27979;&#35797;&#25968;&#25454;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#27169;&#22411;&#23545;&#20854;&#35757;&#32451;&#25968;&#25454;&#30340;&#25935;&#24863;&#24615;&#23545;&#20110;&#35757;&#32451;&#36807;&#31243;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#20063;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#25104;&#26412;&#39640;&#26114;&#12290;&#20026;&#20102;&#31616;&#21270;&#36825;&#31867;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35760;&#24518;&#25200;&#21160;&#26041;&#31243;&#65288;MPE&#65289;&#65292;&#23427;&#23558;&#27169;&#22411;&#30340;&#25935;&#24863;&#24615;&#19982;&#20854;&#35757;&#32451;&#25968;&#25454;&#30340;&#25200;&#21160;&#32852;&#31995;&#36215;&#26469;&#12290;&#20351;&#29992;&#36125;&#21494;&#26031;&#21407;&#29702;&#23548;&#20986;&#30340;MPE&#23558;&#29616;&#26377;&#30340;&#25935;&#24863;&#24615;&#24230;&#37327;&#32479;&#19968;&#36215;&#26469;&#65292;&#27867;&#21270;&#21040;&#21508;&#31181;&#27169;&#22411;&#21644;&#31639;&#27861;&#65292;&#24182;&#25581;&#31034;&#20102;&#26377;&#20851;&#25935;&#24863;&#24615;&#30340;&#26377;&#29992;&#24615;&#36136;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#35757;&#32451;&#36807;&#31243;&#20013;&#33719;&#24471;&#30340;&#25935;&#24863;&#24615;&#20272;&#35745;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#22312;&#26410;&#35265;&#27979;&#35797;&#25968;&#25454;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#35813;&#25552;&#20986;&#30340;&#26041;&#31243;&#39044;&#35745;&#23558;&#23545;&#26410;&#26469;&#30340;&#40065;&#26834;&#21644;&#33258;&#36866;&#24212;&#23398;&#20064;&#30740;&#31350;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding model's sensitivity to its training data is crucial but can also be challenging and costly, especially during training. To simplify such issues, we present the Memory-Perturbation Equation (MPE) which relates model's sensitivity to perturbation in its training data. Derived using Bayesian principles, the MPE unifies existing sensitivity measures, generalizes them to a wide-variety of models and algorithms, and unravels useful properties regarding sensitivities. Our empirical results show that sensitivity estimates obtained during training can be used to faithfully predict generalization on unseen test data. The proposed equation is expected to be useful for future research on robust and adaptive learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#35835;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20687;&#30340;&#24320;&#28304;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;CXR-LLaVA&#65289;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#22270;&#20687;&#32534;&#30721;&#22120;&#21644;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#23558;&#22270;&#20687;&#19982;&#25918;&#23556;&#23398;&#24322;&#24120;&#23545;&#40784;&#65292;&#24182;&#20351;&#29992;GPT-4&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#29616;&#20102;&#38382;&#39064;&#22238;&#31572;&#30340;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.18341</link><description>&lt;p&gt;
CXR-LLaVA&#65306;&#29992;&#20110;&#35299;&#37322;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20687;&#30340;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CXR-LLaVA: Multimodal Large Language Model for Interpreting Chest X-ray Images. (arXiv:2310.18341v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18341
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#35835;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20687;&#30340;&#24320;&#28304;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;CXR-LLaVA&#65289;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#22270;&#20687;&#32534;&#30721;&#22120;&#21644;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#23558;&#22270;&#20687;&#19982;&#25918;&#23556;&#23398;&#24322;&#24120;&#23545;&#40784;&#65292;&#24182;&#20351;&#29992;GPT-4&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#29616;&#20102;&#38382;&#39064;&#22238;&#31572;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#27493;&#20197;&#22810;&#27169;&#24577;&#30340;&#26041;&#24335;&#25193;&#23637;&#20102;&#23427;&#20204;&#30340;&#33021;&#21147;&#65292;&#21487;&#33021;&#22797;&#21046;&#20154;&#31867;&#25918;&#23556;&#31185;&#21307;&#24072;&#23545;&#22270;&#20687;&#30340;&#35299;&#37322;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#29992;&#20110;&#35299;&#37322;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20687;&#30340;&#24320;&#28304;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;CXR-LLaVA&#65289;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#25552;&#31034;&#24037;&#31243;&#21644;&#27169;&#22411;&#21442;&#25968;&#65288;&#22914;&#28201;&#24230;&#21644;&#26680;&#24515;&#37319;&#26679;&#65289;&#30340;&#24433;&#21709;&#12290;&#26448;&#26009;&#21644;&#26041;&#27861;&#65306;&#25105;&#20204;&#25910;&#38598;&#20102;659,287&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20687;&#36827;&#34892;&#35757;&#32451;&#65306;417,336&#20010;&#22270;&#20687;&#24102;&#26377;&#26576;&#20123;&#25918;&#23556;&#23398;&#24322;&#24120;&#26631;&#31614;&#65288;&#25968;&#25454;&#38598;1&#65289;&#65307;241,951&#20010;&#22270;&#20687;&#24102;&#26377;&#33258;&#30001;&#25991;&#26412;&#25918;&#23556;&#23398;&#25253;&#21578;&#65288;&#25968;&#25454;&#38598;2&#65289;&#12290;&#22312;&#39044;&#35757;&#32451;Resnet50&#20316;&#20026;&#22270;&#20687;&#32534;&#30721;&#22120;&#20043;&#21518;&#65292;&#37319;&#29992;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#26469;&#23545;&#40784;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20687;&#21644;&#30456;&#24212;&#30340;&#25918;&#23556;&#23398;&#24322;&#24120;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#25968;&#25454;&#38598;2&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;Meta AI-2&#36827;&#34892;&#24494;&#35843;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#32463;&#36807;GPT-4&#30340;&#25913;&#36827;&#65292;&#29983;&#25104;&#21508;&#31181;&#38382;&#39064;&#22238;&#31572;&#24773;&#26223;&#12290;&#20195;&#30721;&#21487;&#20197;&#22312;ht&#25214;&#21040;
&lt;/p&gt;
&lt;p&gt;
Purpose: Recent advancements in large language models (LLMs) have expanded their capabilities in a multimodal fashion, potentially replicating the image interpretation of human radiologists. This study aimed to develop open-source multimodal large language model for interpreting chest X-ray images (CXR-LLaVA). We also examined the effect of prompt engineering and model parameters such as temperature and nucleus sampling.  Materials and Methods: For training, we collected 659,287 publicly available CXRs: 417,336 CXRs had labels for certain radiographic abnormalities (dataset 1); 241,951 CXRs provided free-text radiology reports (dataset 2). After pre-training the Resnet50 as an image encoder, the contrastive language-image pre-training was used to align CXRs and corresponding radiographic abnormalities. Then, the Large Language Model Meta AI-2 was fine-tuned using dataset 2, which were refined using GPT-4, with generating various question answering scenarios. The code can be found at ht
&lt;/p&gt;</description></item><item><title>Isometric Motion Manifold Primitives (IMMP) is proposed to address the degradation of Motion Manifold Primitive (MMP) performance due to geometric distortion in the latent space. IMMP preserves the geometry of the manifold in the latent coordinate space using a Riemannian metric, and experimental results show that IMMP significantly outperforms existing MMP methods.</title><link>http://arxiv.org/abs/2310.17072</link><description>&lt;p&gt;
&#31561;&#36317;&#36816;&#21160;&#27969;&#24418;&#22522;&#20803;
&lt;/p&gt;
&lt;p&gt;
Isometric Motion Manifold Primitives. (arXiv:2310.17072v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17072
&lt;/p&gt;
&lt;p&gt;
Isometric Motion Manifold Primitives (IMMP) is proposed to address the degradation of Motion Manifold Primitive (MMP) performance due to geometric distortion in the latent space. IMMP preserves the geometry of the manifold in the latent coordinate space using a Riemannian metric, and experimental results show that IMMP significantly outperforms existing MMP methods.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36816;&#21160;&#27969;&#24418;&#22522;&#20803;&#65288;MMP&#65289;&#20026;&#32473;&#23450;&#20219;&#21153;&#29983;&#25104;&#19968;&#31995;&#21015;&#36830;&#32493;&#36712;&#36857;&#27969;&#24418;&#65292;&#27599;&#19968;&#20010;&#36712;&#36857;&#27969;&#24418;&#37117;&#33021;&#25104;&#21151;&#23436;&#25104;&#20219;&#21153;&#12290;&#23427;&#30001;&#23545;&#27969;&#24418;&#36827;&#34892;&#21442;&#25968;&#21270;&#30340;&#35299;&#30721;&#20989;&#25968;&#20197;&#21450;&#28508;&#22312;&#22352;&#26631;&#31354;&#38388;&#20013;&#30340;&#27010;&#29575;&#23494;&#24230;&#32452;&#25104;&#12290;&#26412;&#25991;&#39318;&#20808;&#23637;&#31034;&#20102;&#30001;&#20110;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#20960;&#20309;&#25197;&#26354;&#65292;MMP&#30340;&#24615;&#33021;&#21487;&#33021;&#20250;&#26174;&#33879;&#38477;&#20302;--&#36890;&#36807;&#21464;&#24418;&#65292;&#25105;&#20204;&#25351;&#30340;&#26159;&#30456;&#20284;&#30340;&#36816;&#21160;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#26080;&#27861;&#30456;&#37051;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31561;&#36317;&#36816;&#21160;&#27969;&#24418;&#22522;&#20803;&#65288;IMMP&#65289;&#65292;&#20854;&#28508;&#22312;&#22352;&#26631;&#31354;&#38388;&#20445;&#25345;&#20102;&#27969;&#24418;&#30340;&#20960;&#20309;&#32467;&#26500;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24314;&#31435;&#21644;&#20351;&#29992;&#20102;&#19968;&#20010;Riemannian&#24230;&#37327;&#65292;&#29992;&#20110;&#36816;&#21160;&#31354;&#38388;&#65288;&#21363;&#65292;&#21442;&#25968;&#21270;&#26354;&#32447;&#31354;&#38388;&#65289;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;CurveGeom Riemannian&#24230;&#37327;&#12290;&#23545;&#20110;&#24179;&#38754;&#38556;&#30861;&#36991;&#35753;&#36816;&#21160;&#21644;&#25512;&#21160;&#25805;&#32437;&#20219;&#21153;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;IMMP&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;MMP&#26041;&#27861;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/Gabe-YHLee/IMMP&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Motion Manifold Primitive (MMP) produces, for a given task, a continuous manifold of trajectories each of which can successfully complete the task. It consists of the decoder function that parametrizes the manifold and the probability density in the latent coordinate space. In this paper, we first show that the MMP performance can significantly degrade due to the geometric distortion in the latent space -- by distortion, we mean that similar motions are not located nearby in the latent space. We then propose {\it Isometric Motion Manifold Primitives (IMMP)} whose latent coordinate space preserves the geometry of the manifold. For this purpose, we formulate and use a Riemannian metric for the motion space (i.e., parametric curve space), which we call a {\it CurveGeom Riemannian metric}. Experiments with planar obstacle-avoiding motions and pushing manipulation tasks show that IMMP significantly outperforms existing MMP methods. Code is available at https://github.com/Gabe-YHLee/IMMP
&lt;/p&gt;</description></item><item><title>&#20998;&#23618;&#38543;&#26426;&#24179;&#28369;&#26159;&#19968;&#31181;&#22312;&#22797;&#26434;&#25968;&#25454;&#19978;&#36827;&#34892;&#40065;&#26834;&#24615;&#35748;&#35777;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#21482;&#22312;&#19968;&#20010;&#23545;&#35937;&#30340;&#23376;&#38598;&#19978;&#28155;&#21152;&#38543;&#26426;&#22122;&#22768;&#65292;&#20197;&#26356;&#26377;&#38024;&#23545;&#24615;&#30340;&#26041;&#24335;&#25552;&#20379;&#20102;&#26356;&#24378;&#30340;&#40065;&#26834;&#24615;&#20445;&#35777;&#21644;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.16221</link><description>&lt;p&gt;
&#20998;&#23618;&#38543;&#26426;&#24179;&#28369;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Randomized Smoothing. (arXiv:2310.16221v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16221
&lt;/p&gt;
&lt;p&gt;
&#20998;&#23618;&#38543;&#26426;&#24179;&#28369;&#26159;&#19968;&#31181;&#22312;&#22797;&#26434;&#25968;&#25454;&#19978;&#36827;&#34892;&#40065;&#26834;&#24615;&#35748;&#35777;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#21482;&#22312;&#19968;&#20010;&#23545;&#35937;&#30340;&#23376;&#38598;&#19978;&#28155;&#21152;&#38543;&#26426;&#22122;&#22768;&#65292;&#20197;&#26356;&#26377;&#38024;&#23545;&#24615;&#30340;&#26041;&#24335;&#25552;&#20379;&#20102;&#26356;&#24378;&#30340;&#40065;&#26834;&#24615;&#20445;&#35777;&#21644;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#26159;&#22797;&#26434;&#30340;&#65292;&#36890;&#24120;&#30001;&#21487;&#20998;&#35299;&#20026;&#22810;&#20010;&#23454;&#20307;&#30340;&#23545;&#35937;&#32452;&#25104;&#65288;&#20363;&#22914;&#65292;&#23558;&#22270;&#20687;&#20998;&#35299;&#20026;&#20687;&#32032;&#65292;&#23558;&#22270;&#24418;&#20998;&#35299;&#20026;&#30456;&#20114;&#36830;&#25509;&#30340;&#33410;&#28857;&#65289;&#12290;&#38543;&#26426;&#24179;&#28369;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#20351;&#27169;&#22411;&#22312;&#20854;&#36755;&#20837;&#30340;&#24494;&#23567;&#21464;&#21270;&#19978;&#20855;&#26377;&#35777;&#26126;&#30340;&#40065;&#26834;&#24615;-&#36890;&#36807;&#22312;&#20998;&#31867;&#20043;&#21069;&#38543;&#26426;&#28155;&#21152;&#22122;&#22768;&#26469;&#20445;&#35777;&#22810;&#25968;&#25237;&#31080;&#30340;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;&#24403;&#23545;&#25163;&#19981;&#26159;&#20219;&#24847;&#24178;&#25200;&#25972;&#20010;&#23545;&#35937;&#65288;&#20363;&#22914;&#22270;&#20687;&#65289;&#65292;&#32780;&#26159;&#23545;&#35937;&#30340;&#26576;&#20010;&#23454;&#20307;&#30340;&#23376;&#38598;&#65288;&#20363;&#22914;&#20687;&#32032;&#65289;&#26102;&#65292;&#36890;&#36807;&#38543;&#26426;&#24179;&#28369;&#23545;&#36825;&#31181;&#22797;&#26434;&#25968;&#25454;&#36827;&#34892;&#40065;&#26834;&#24615;&#35748;&#35777;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20316;&#20026;&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20998;&#23618;&#38543;&#26426;&#24179;&#28369;&#65306;&#25105;&#20204;&#36890;&#36807;&#20165;&#22312;&#38543;&#26426;&#36873;&#25321;&#30340;&#23454;&#20307;&#23376;&#38598;&#19978;&#28155;&#21152;&#38543;&#26426;&#22122;&#22768;&#26469;&#37096;&#20998;&#24179;&#28369;&#23545;&#35937;&#12290;&#36890;&#36807;&#20197;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#26377;&#38024;&#23545;&#24615;&#30340;&#26041;&#24335;&#28155;&#21152;&#22122;&#22768;&#65292;&#25105;&#20204;&#33719;&#24471;&#26356;&#24378;&#30340;&#40065;&#26834;&#24615;&#20445;&#35777;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#19981;&#21516;&#30340;&#22122;&#22768;&#20998;&#24067;&#21021;&#22987;&#21270;&#20998;&#23618;&#24179;&#28369;&#65292;&#24471;&#21040;&#20102;&#26032;&#30340;&#40065;&#26834;&#24615;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world data is complex and often consists of objects that can be decomposed into multiple entities (e.g. images into pixels, graphs into interconnected nodes). Randomized smoothing is a powerful framework for making models provably robust against small changes to their inputs - by guaranteeing robustness of the majority vote when randomly adding noise before classification. Yet, certifying robustness on such complex data via randomized smoothing is challenging when adversaries do not arbitrarily perturb entire objects (e.g. images) but only a subset of their entities (e.g. pixels). As a solution, we introduce hierarchical randomized smoothing: We partially smooth objects by adding random noise only on a randomly selected subset of their entities. By adding noise in a more targeted manner than existing methods we obtain stronger robustness guarantees while maintaining high accuracy. We initialize hierarchical smoothing using different noising distributions, yielding novel robustness
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#28183;&#36879;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#27010;&#24565;&#65292;&#26088;&#22312;&#20351;LLMs&#33021;&#22815;&#36890;&#36807;&#29289;&#32852;&#32593;&#20256;&#24863;&#22120;&#19982;&#25191;&#34892;&#22120;&#19982;&#29289;&#29702;&#19990;&#30028;&#36827;&#34892;&#20132;&#20114;&#21644;&#25512;&#29702;&#12290;&#21021;&#27493;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;LLMs&#20855;&#26377;&#29420;&#29305;&#30340;&#33021;&#21147;&#65292;&#33021;&#22815;&#24212;&#29992;&#20869;&#23884;&#30340;&#19990;&#30028;&#30693;&#35782;&#35299;&#37322;&#29289;&#32852;&#32593;&#20256;&#24863;&#22120;&#25968;&#25454;&#24182;&#36827;&#34892;&#29289;&#29702;&#39046;&#22495;&#30340;&#25512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2310.09605</link><description>&lt;p&gt;
&#28183;&#36879;&#24335;&#20154;&#24037;&#26234;&#33021;&#65306;&#20351;LLMs&#29702;&#35299;&#29289;&#29702;&#19990;&#30028;
&lt;/p&gt;
&lt;p&gt;
Penetrative AI: Making LLMs Comprehend the Physical World. (arXiv:2310.09605v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09605
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#28183;&#36879;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#27010;&#24565;&#65292;&#26088;&#22312;&#20351;LLMs&#33021;&#22815;&#36890;&#36807;&#29289;&#32852;&#32593;&#20256;&#24863;&#22120;&#19982;&#25191;&#34892;&#22120;&#19982;&#29289;&#29702;&#19990;&#30028;&#36827;&#34892;&#20132;&#20114;&#21644;&#25512;&#29702;&#12290;&#21021;&#27493;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;LLMs&#20855;&#26377;&#29420;&#29305;&#30340;&#33021;&#21147;&#65292;&#33021;&#22815;&#24212;&#29992;&#20869;&#23884;&#30340;&#19990;&#30028;&#30693;&#35782;&#35299;&#37322;&#29289;&#32852;&#32593;&#20256;&#24863;&#22120;&#25968;&#25454;&#24182;&#36827;&#34892;&#29289;&#29702;&#39046;&#22495;&#30340;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#26174;&#33879;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;LLMs&#30340;&#24615;&#36136;&#20197;&#21450;&#23427;&#20204;&#22312;&#28041;&#21450;&#30495;&#23454;&#29289;&#29702;&#19990;&#30028;&#20449;&#24687;&#30340;&#20219;&#21153;&#20013;&#25972;&#21512;&#24120;&#35782;&#20154;&#31867;&#30693;&#35782;&#30340;&#28508;&#21147;&#20173;&#23384;&#22312;&#30097;&#38382;&#12290;&#26412;&#25991;&#36890;&#36807;&#25506;&#32034;LLMs&#22914;&#20309;&#36890;&#36807;&#29289;&#32852;&#32593;&#20256;&#24863;&#22120;&#21644;&#25191;&#34892;&#22120;&#19982;&#29289;&#29702;&#19990;&#30028;&#36827;&#34892;&#20132;&#20114;&#21644;&#25512;&#29702;&#26469;&#25506;&#35752;&#36825;&#20123;&#38382;&#39064;&#65292;&#36825;&#19968;&#27010;&#24565;&#31216;&#20026;&#8220;&#28183;&#36879;&#24335;&#20154;&#24037;&#26234;&#33021;&#8221;&#12290;&#35770;&#25991;&#22312;LLMs&#33021;&#22815;&#36879;&#36807;&#22788;&#29702;&#24863;&#30693;&#20449;&#21495;&#30340;&#20004;&#20010;&#23618;&#38754;&#19978;&#25506;&#32034;&#20102;&#36825;&#31181;&#25193;&#23637;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;LLMs&#65288;ChatGPT&#26159;&#25105;&#20204;&#30740;&#31350;&#20013;&#30340;&#20195;&#34920;&#24615;&#20363;&#23376;&#65289;&#22312;&#24212;&#29992;&#20869;&#23884;&#30340;&#19990;&#30028;&#30693;&#35782;&#35299;&#37322;&#29289;&#32852;&#32593;&#20256;&#24863;&#22120;&#25968;&#25454;&#24182;&#23545;&#29289;&#29702;&#39046;&#22495;&#30340;&#20219;&#21153;&#36827;&#34892;&#25512;&#29702;&#26041;&#38754;&#20855;&#26377;&#30456;&#24403;&#29420;&#29305;&#30340;&#33021;&#21147;&#12290;&#36825;&#19981;&#20165;&#20026;LLMs&#24320;&#36767;&#20102;&#26032;&#30340;&#24212;&#29992;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent developments in Large Language Models (LLMs) have demonstrated their remarkable capabilities across a range of tasks. Questions, however, persist about the nature of LLMs and their potential to integrate common-sense human knowledge when performing tasks involving information about the real physical world. This paper delves into these questions by exploring how LLMs can be extended to interact with and reason about the physical world through IoT sensors and actuators, a concept that we term "Penetrative AI". The paper explores such an extension at two levels of LLMs' ability to penetrate into the physical world via the processing of sensory signals. Our preliminary findings indicate that LLMs, with ChatGPT being the representative example in our exploration, have considerable and unique proficiency in employing the embedded world knowledge for interpreting IoT sensor data and reasoning over them about tasks in the physical realm. Not only this opens up new applications for LLMs 
&lt;/p&gt;</description></item><item><title>CP-KGC&#26041;&#27861;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#32422;&#26463;&#24335;&#25552;&#31034;&#26469;&#34917;&#20840;&#30693;&#35782;&#22270;&#35889;&#65292;&#25552;&#39640;&#25512;&#26029;&#25928;&#26524;&#65292;&#23637;&#31034;&#20102;&#22312;&#20302;&#36164;&#28304;&#35745;&#31639;&#26465;&#20214;&#19979;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#20043;&#21069;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.08279</link><description>&lt;p&gt;
CP-KGC: &#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32422;&#26463;&#24335;&#25552;&#31034;&#23545;&#30693;&#35782;&#22270;&#35889;&#36827;&#34892;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
CP-KGC: Constrained-Prompt Knowledge Graph Completion with Large Language Models. (arXiv:2310.08279v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08279
&lt;/p&gt;
&lt;p&gt;
CP-KGC&#26041;&#27861;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#32422;&#26463;&#24335;&#25552;&#31034;&#26469;&#34917;&#20840;&#30693;&#35782;&#22270;&#35889;&#65292;&#25552;&#39640;&#25512;&#26029;&#25928;&#26524;&#65292;&#23637;&#31034;&#20102;&#22312;&#20302;&#36164;&#28304;&#35745;&#31639;&#26465;&#20214;&#19979;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#20043;&#21069;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#26088;&#22312;&#21033;&#29992;&#29616;&#26377;&#30693;&#35782;&#25512;&#26029;&#21644;&#25512;&#27979;&#30693;&#35782;&#22270;&#35889;&#20013;&#32570;&#22833;&#30340;&#36830;&#25509;&#12290;SimKGC&#31561;&#22522;&#20110;&#25991;&#26412;&#30340;&#26041;&#27861;&#24050;&#32463;&#36229;&#36807;&#20102;&#22270;&#23884;&#20837;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#24402;&#32435;&#24335;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#25991;&#26412;&#30340;&#26041;&#27861;&#30340;&#25928;&#26524;&#21462;&#20915;&#20110;&#23454;&#20307;&#25991;&#26412;&#25551;&#36848;&#30340;&#36136;&#37327;&#12290;&#20026;&#20102;&#20943;&#36731;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#30340;&#24187;&#35273;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#32422;&#26463;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#21033;&#29992;&#23454;&#20307;&#21450;&#20854;&#25991;&#26412;&#25551;&#36848;&#20316;&#20026;&#19978;&#19979;&#25991;&#32422;&#26463;&#26469;&#25552;&#39640;&#25968;&#25454;&#36136;&#37327;&#12290;&#25105;&#20204;&#30340;&#32422;&#26463;&#24335;&#25552;&#31034;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#26041;&#27861;&#65288;CP-KGC&#65289;&#22312;&#20302;&#36164;&#28304;&#35745;&#31639;&#26465;&#20214;&#19979;&#34920;&#29616;&#20986;&#26377;&#25928;&#30340;&#25512;&#26029;&#33021;&#21147;&#65292;&#24182;&#36229;&#36807;&#20102;WN18RR&#21644;FB15K237&#25968;&#25454;&#38598;&#19978;&#30340;&#20043;&#21069;&#32467;&#26524;&#12290;&#36825;&#23637;&#31034;&#20102;LLMs&#22312;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20219;&#21153;&#20013;&#30340;&#25972;&#21512;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graph completion (KGC) aims to utilize existing knowledge to deduce and infer missing connections within knowledge graphs. Text-based approaches, like SimKGC, have outperformed graph embedding methods, showcasing the promise of inductive KGC. However, the efficacy of text-based methods hinges on the quality of entity textual descriptions. In this paper, we identify the key issue of whether large language models (LLMs) can generate effective text. To mitigate hallucination in LLM-generated text in this paper, we introduce a constraint-based prompt that utilizes the entity and its textual description as contextual constraints to enhance data quality. Our Constrained-Prompt Knowledge Graph Completion (CP-KGC) method demonstrates effective inference under low resource computing conditions and surpasses prior results on the WN18RR and FB15K237 datasets. This showcases the integration of LLMs in KGC tasks and provides new directions for future research.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RAPL&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#26426;&#22120;&#20154;&#23398;&#20064;&#20013;&#30340;&#35270;&#35273;&#34920;&#31034;&#23545;&#40784;&#38382;&#39064;&#65292;&#36890;&#36807;&#21033;&#29992;&#20154;&#31867;&#21453;&#39304;&#23558;&#26426;&#22120;&#20154;&#30340;&#35270;&#35273;&#34920;&#31034;&#19982;&#29992;&#25143;&#20559;&#22909;&#23545;&#40784;&#65292;&#24182;&#21306;&#20998;&#20219;&#21153;&#30340;&#20851;&#38190;&#35201;&#32032;&#12290;</title><link>http://arxiv.org/abs/2310.07932</link><description>&lt;p&gt;
&#20320;&#20851;&#24515;&#20160;&#20040;&#65311;&#20026;&#26426;&#22120;&#20154;&#23398;&#20064;&#23454;&#29616;&#35270;&#35273;&#34920;&#31034;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
What Matters to You? Towards Visual Representation Alignment for Robot Learning. (arXiv:2310.07932v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07932
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RAPL&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#26426;&#22120;&#20154;&#23398;&#20064;&#20013;&#30340;&#35270;&#35273;&#34920;&#31034;&#23545;&#40784;&#38382;&#39064;&#65292;&#36890;&#36807;&#21033;&#29992;&#20154;&#31867;&#21453;&#39304;&#23558;&#26426;&#22120;&#20154;&#30340;&#35270;&#35273;&#34920;&#31034;&#19982;&#29992;&#25143;&#20559;&#22909;&#23545;&#40784;&#65292;&#24182;&#21306;&#20998;&#20219;&#21153;&#30340;&#20851;&#38190;&#35201;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20026;&#20154;&#31867;&#26381;&#21153;&#26102;&#65292;&#26426;&#22120;&#20154;&#38656;&#35201;&#20248;&#21270;&#19982;&#26368;&#32456;&#29992;&#25143;&#20559;&#22909;&#19968;&#33268;&#30340;&#22870;&#21169;&#12290;&#30001;&#20110;&#26426;&#22120;&#20154;&#23558;&#20381;&#36182;&#21407;&#22987;&#24863;&#30693;&#36755;&#20837;&#22914;RGB&#22270;&#20687;&#65292;&#23427;&#20204;&#30340;&#22870;&#21169;&#23558;&#19981;&#21487;&#36991;&#20813;&#22320;&#20351;&#29992;&#35270;&#35273;&#34920;&#31034;&#12290;&#26368;&#36817;&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#35270;&#35273;&#27169;&#22411;&#30340;&#34920;&#31034;&#24341;&#21457;&#20102;&#20154;&#20204;&#30340;&#20852;&#36259;&#65292;&#20294;&#22312;&#26426;&#22120;&#20154;&#39046;&#22495;&#20351;&#20854;&#36215;&#20316;&#29992;&#30340;&#20851;&#38190;&#26159;&#24494;&#35843;&#65292;&#36890;&#24120;&#36890;&#36807;&#20195;&#29702;&#20219;&#21153;&#22914;&#21160;&#21147;&#23398;&#39044;&#27979;&#25110;&#24378;&#21046;&#26102;&#38388;&#24490;&#29615;&#19968;&#33268;&#24615;&#26469;&#23436;&#25104;&#12290;&#28982;&#32780;&#65292;&#25152;&#26377;&#36825;&#20123;&#20195;&#29702;&#20219;&#21153;&#37117;&#32469;&#36807;&#20102;&#20154;&#31867;&#23545;&#33258;&#24049;&#20851;&#24515;&#30340;&#20107;&#29289;&#30340;&#36755;&#20837;&#65292;&#21152;&#21095;&#20102;&#34394;&#20551;&#20851;&#32852;&#65292;&#24182;&#26368;&#32456;&#23548;&#33268;&#26426;&#22120;&#20154;&#30340;&#34892;&#20026;&#19982;&#29992;&#25143;&#20559;&#22909;&#19981;&#19968;&#33268;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#35758;&#26426;&#22120;&#20154;&#24212;&#35813;&#21033;&#29992;&#20154;&#31867;&#30340;&#21453;&#39304;&#26469;&#19982;&#26368;&#32456;&#29992;&#25143;&#30340;&#35270;&#35273;&#34920;&#31034;&#23545;&#40784;&#65292;&#24182;&#21306;&#20998;&#20219;&#21153;&#30340;&#20851;&#38190;&#35201;&#32032;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#35270;&#35273;&#34920;&#31034;&#23545;&#40784;&#38382;&#39064;&#21644;&#35270;&#35273;&#22870;&#21169;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#21363;&#22522;&#20110;&#20559;&#22909;&#30340;&#34920;&#31034;&#23545;&#40784;&#23398;&#20064;&#65288;RAPL&#65289;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
When operating in service of people, robots need to optimize rewards aligned with end-user preferences. Since robots will rely on raw perceptual inputs like RGB images, their rewards will inevitably use visual representations. Recently there has been excitement in using representations from pre-trained visual models, but key to making these work in robotics is fine-tuning, which is typically done via proxy tasks like dynamics prediction or enforcing temporal cycle-consistency. However, all these proxy tasks bypass the human's input on what matters to them, exacerbating spurious correlations and ultimately leading to robot behaviors that are misaligned with user preferences. In this work, we propose that robots should leverage human feedback to align their visual representations with the end-user and disentangle what matters for the task. We propose Representation-Aligned Preference-based Learning (RAPL), a method for solving the visual representation alignment problem and visual reward
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#29983;&#25104;&#24314;&#27169;&#23398;&#20064;&#20132;&#20114;&#20307;&#39564;&#30340;&#36890;&#29992;&#27169;&#25311;&#22120;&#65292;&#20197;&#27169;&#25311;&#20154;&#31867;&#12289;&#26426;&#22120;&#20154;&#21644;&#20854;&#20182;&#20132;&#20114;&#24335;&#20195;&#29702;&#20154;&#23545;&#30495;&#23454;&#19990;&#30028;&#20013;&#34892;&#20026;&#30340;&#21709;&#24212;&#12290;</title><link>http://arxiv.org/abs/2310.06114</link><description>&lt;p&gt;
&#23398;&#20064;&#20132;&#20114;&#24335;&#29616;&#23454;&#19990;&#30028;&#27169;&#25311;&#22120;
&lt;/p&gt;
&lt;p&gt;
Learning Interactive Real-World Simulators. (arXiv:2310.06114v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06114
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#29983;&#25104;&#24314;&#27169;&#23398;&#20064;&#20132;&#20114;&#20307;&#39564;&#30340;&#36890;&#29992;&#27169;&#25311;&#22120;&#65292;&#20197;&#27169;&#25311;&#20154;&#31867;&#12289;&#26426;&#22120;&#20154;&#21644;&#20854;&#20182;&#20132;&#20114;&#24335;&#20195;&#29702;&#20154;&#23545;&#30495;&#23454;&#19990;&#30028;&#20013;&#34892;&#20026;&#30340;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#22312;&#20114;&#32852;&#32593;&#25968;&#25454;&#19978;&#30340;&#29983;&#25104;&#27169;&#22411;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#35270;&#39057;&#20869;&#23481;&#30340;&#21019;&#24314;&#26041;&#24335;&#12290;&#20063;&#35768;&#29983;&#25104;&#27169;&#22411;&#30340;&#19979;&#19968;&#20010;&#37324;&#31243;&#30865;&#26159;&#22312;&#20154;&#31867;&#12289;&#26426;&#22120;&#20154;&#21644;&#20854;&#20182;&#20132;&#20114;&#24335;&#20195;&#29702;&#20154;&#37319;&#21462;&#34892;&#21160;&#26102;&#27169;&#25311;&#30495;&#23454;&#30340;&#20307;&#39564;&#12290;&#23454;&#38469;&#24212;&#29992;&#33539;&#22260;&#20174;&#28216;&#25103;&#21644;&#30005;&#24433;&#20013;&#30340;&#21487;&#25511;&#20869;&#23481;&#21019;&#24314;&#65292;&#21040;&#20165;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#35757;&#32451;&#21487;&#20197;&#30452;&#25509;&#37096;&#32626;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#20307;&#39564;&#24335;&#20195;&#29702;&#20154;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#36890;&#36807;&#29983;&#25104;&#24314;&#27169;&#26469;&#23398;&#20064;&#29616;&#23454;&#19990;&#30028;&#20132;&#20114;&#30340;&#36890;&#29992;&#27169;&#25311;&#22120;(UniSim)&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#37325;&#35201;&#22320;&#35266;&#23519;&#21040;&#65292;&#29992;&#20110;&#23398;&#20064;&#29616;&#23454;&#19990;&#30028;&#27169;&#25311;&#22120;&#30340;&#33258;&#28982;&#25968;&#25454;&#38598;&#36890;&#24120;&#22312;&#19981;&#21516;&#30340;&#26041;&#38754;&#20016;&#23500;&#22810;&#26679;&#65288;&#20363;&#22914;&#65292;&#22270;&#20687;&#25968;&#25454;&#20013;&#20016;&#23500;&#30340;&#29289;&#20307;&#65292;&#26426;&#22120;&#20154;&#25968;&#25454;&#20013;&#23494;&#38598;&#37319;&#26679;&#30340;&#21160;&#20316;&#65292;&#23548;&#33322;&#25968;&#25454;&#20013;&#22810;&#26679;&#30340;&#31227;&#21160;&#65289;&#12290;&#36890;&#36807;&#31934;&#24515;&#21327;&#35843;&#21508;&#31181;&#25968;&#25454;&#38598;&#65292;&#27599;&#20010;&#25968;&#25454;&#38598;&#37117;&#25552;&#20379;&#25972;&#20307;&#20307;&#39564;&#30340;&#19981;&#21516;&#26041;&#38754;&#65292;UniSim&#21487;&#20197;&#27169;&#25311;&#20154;&#31867;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative models trained on internet data have revolutionized how text, image, and video content can be created. Perhaps the next milestone for generative models is to simulate realistic experience in response to actions taken by humans, robots, and other interactive agents. Applications of a real-world simulator range from controllable content creation in games and movies, to training embodied agents purely in simulation that can be directly deployed in the real world. We explore the possibility of learning a universal simulator (UniSim) of real-world interaction through generative modeling. We first make the important observation that natural datasets available for learning a real-world simulator are often rich along different axes (e.g., abundant objects in image data, densely sampled actions in robotics data, and diverse movements in navigation data). With careful orchestration of diverse datasets, each providing a different aspect of the overall experience, UniSim can emulate how
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#22810;&#35821;&#35328;&#25991;&#26412;&#21040;&#25991;&#26412;&#21464;&#25442;&#22120;&#27169;&#22411;&#19978;&#24494;&#35843;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#33021;&#22815;&#33258;&#21160;&#22312;&#22810;&#35821;&#35328;&#20013;&#24635;&#32467;&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#26377;&#21161;&#20110;&#25552;&#39640;&#26410;&#26469;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#30740;&#31350;&#21644;&#21457;&#23637;&#65292;&#19988;&#33021;&#22815;&#24212;&#29992;&#20110;&#19981;&#21516;&#26063;&#35028;&#32972;&#26223;&#30340;&#24739;&#32773;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2310.00100</link><description>&lt;p&gt;
&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#22810;&#35821;&#35328;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;--&#25688;&#35201;&#26159;&#20320;&#38656;&#35201;&#30340;&#19968;&#20999;&#65281;
&lt;/p&gt;
&lt;p&gt;
Multilingual Natural Language ProcessingModel for Radiology Reports -- The Summary is all you need!. (arXiv:2310.00100v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00100
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#22810;&#35821;&#35328;&#25991;&#26412;&#21040;&#25991;&#26412;&#21464;&#25442;&#22120;&#27169;&#22411;&#19978;&#24494;&#35843;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#33021;&#22815;&#33258;&#21160;&#22312;&#22810;&#35821;&#35328;&#20013;&#24635;&#32467;&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#26377;&#21161;&#20110;&#25552;&#39640;&#26410;&#26469;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#30740;&#31350;&#21644;&#21457;&#23637;&#65292;&#19988;&#33021;&#22815;&#24212;&#29992;&#20110;&#19981;&#21516;&#26063;&#35028;&#32972;&#26223;&#30340;&#24739;&#32773;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#21360;&#35937;&#37096;&#20998;&#24635;&#32467;&#20102;&#37325;&#35201;&#30340;&#25918;&#23556;&#23398;&#21457;&#29616;&#65292;&#24182;&#22312;&#21521;&#21307;&#29983;&#20256;&#36798;&#36825;&#20123;&#21457;&#29616;&#26102;&#36215;&#21040;&#20102;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#25918;&#23556;&#31185;&#21307;&#29983;&#26469;&#35828;&#65292;&#20934;&#22791;&#36825;&#20123;&#25688;&#35201;&#26082;&#32791;&#26102;&#21448;&#23481;&#26131;&#20986;&#38169;&#12290;&#26368;&#36817;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#35768;&#22810;&#29992;&#20110;&#25918;&#23556;&#23398;&#25253;&#21578;&#25688;&#35201;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#33021;&#22815;&#22312;&#22810;&#31181;&#35821;&#35328;&#20013;&#24635;&#32467;&#36825;&#20123;&#25253;&#21578;&#30340;&#27169;&#22411;&#12290;&#36825;&#26679;&#30340;&#27169;&#22411;&#21487;&#20197;&#26497;&#22823;&#22320;&#25913;&#36827;&#26410;&#26469;&#30340;&#30740;&#31350;&#21644;&#34701;&#21512;&#26469;&#33258;&#19981;&#21516;&#26063;&#35028;&#32972;&#26223;&#30340;&#24739;&#32773;&#25968;&#25454;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21457;&#23637;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#20844;&#24320;&#21487;&#29992;&#30340;&#22522;&#20110;&#22810;&#35821;&#35328;&#25991;&#26412;&#21040;&#25991;&#26412;&#21464;&#25442;&#22120;&#30340;&#27169;&#22411;&#19978;&#24494;&#35843;&#65292;&#33258;&#21160;&#21270;&#22320;&#29983;&#25104;&#20102;&#19981;&#21516;&#35821;&#35328;&#30340;&#25918;&#23556;&#23398;&#21360;&#35937;&#65292;&#20197;&#24635;&#32467;&#33521;&#35821;&#12289;&#33889;&#33796;&#29273;&#35821;&#21644;&#24503;&#35821;&#30340;&#25918;&#23556;&#23398;&#25253;&#21578;&#20013;&#30340;&#21457;&#29616;&#12290;&#22312;&#19968;&#39033;&#30450;&#27979;&#20013;&#65292;&#20004;&#20301;&#26377;&#25191;&#19994;&#36164;&#26684;&#30340;&#25918;&#23556;&#31185;&#21307;&#29983;&#34920;&#31034;&#65292;&#23545;&#20110;&#33267;&#23569;70%&#30340;&#31995;&#32479;&#29983;&#25104;&#30340;&#25688;&#35201;&#65292;&#20854;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
The impression section of a radiology report summarizes important radiology findings and plays a critical role in communicating these findings to physicians. However, the preparation of these summaries is time-consuming and error-prone for radiologists. Recently, numerous models for radiology report summarization have been developed. Nevertheless, there is currently no model that can summarize these reports in multiple languages. Such a model could greatly improve future research and the development of Deep Learning models that incorporate data from patients with different ethnic backgrounds. In this study, the generation of radiology impressions in different languages was automated by fine-tuning a model, publicly available, based on a multilingual text-to-text Transformer to summarize findings available in English, Portuguese, and German radiology reports. In a blind test, two board-certified radiologists indicated that for at least 70% of the system-generated summaries, the quality 
&lt;/p&gt;</description></item><item><title>QAL-BP&#26159;&#19968;&#31181;&#22686;&#24191;&#25289;&#26684;&#26391;&#26085;&#37327;&#23376;&#26041;&#27861;&#65292;&#19987;&#38376;&#29992;&#20110;&#35299;&#20915;&#35013;&#31665;&#38382;&#39064;&#12290;&#23427;&#21033;&#29992;&#22686;&#24191;&#25289;&#26684;&#26391;&#26085;&#26041;&#27861;&#23558;&#35013;&#31665;&#32422;&#26463;&#21152;&#20837;&#30446;&#26631;&#20989;&#25968;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#20272;&#35745;&#21551;&#21457;&#24335;&#20056;&#25968;&#65292;&#28040;&#38500;&#20102;&#38656;&#35201;&#26681;&#25454;&#23454;&#20363;&#35745;&#31639;Lagrangian&#31995;&#25968;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2309.12678</link><description>&lt;p&gt;
QAL-BP:&#19968;&#31181;&#29992;&#20110;&#35013;&#31665;&#38382;&#39064;&#30340;&#22686;&#24191;&#25289;&#26684;&#26391;&#26085;&#37327;&#23376;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
QAL-BP: An Augmented Lagrangian Quantum Approach for Bin Packing Problem. (arXiv:2309.12678v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12678
&lt;/p&gt;
&lt;p&gt;
QAL-BP&#26159;&#19968;&#31181;&#22686;&#24191;&#25289;&#26684;&#26391;&#26085;&#37327;&#23376;&#26041;&#27861;&#65292;&#19987;&#38376;&#29992;&#20110;&#35299;&#20915;&#35013;&#31665;&#38382;&#39064;&#12290;&#23427;&#21033;&#29992;&#22686;&#24191;&#25289;&#26684;&#26391;&#26085;&#26041;&#27861;&#23558;&#35013;&#31665;&#32422;&#26463;&#21152;&#20837;&#30446;&#26631;&#20989;&#25968;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#20272;&#35745;&#21551;&#21457;&#24335;&#20056;&#25968;&#65292;&#28040;&#38500;&#20102;&#38656;&#35201;&#26681;&#25454;&#23454;&#20363;&#35745;&#31639;Lagrangian&#31995;&#25968;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35013;&#31665;&#38382;&#39064;&#26159;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#20247;&#25152;&#21608;&#30693;&#30340;NP-Hard&#38382;&#39064;&#65292;&#23547;&#25214;&#39640;&#25928;&#35299;&#20915;&#26041;&#26696;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#12290;&#30456;&#21453;&#65292;&#37327;&#23376;&#25216;&#26415;&#30340;&#26368;&#26032;&#36827;&#23637;&#26174;&#31034;&#20986;&#22312;&#26576;&#20123;&#38382;&#39064;&#31867;&#21035;&#65288;&#22914;&#32452;&#21512;&#20248;&#21270;&#65289;&#20013;&#23454;&#29616;&#22823;&#24133;&#35745;&#31639;&#21152;&#36895;&#30340;&#28508;&#21147;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;QAL-BP&#65292;&#19968;&#31181;&#19987;&#38376;&#38024;&#23545;&#35013;&#31665;&#38382;&#39064;&#35774;&#35745;&#30340;&#26032;&#22411;&#20108;&#27425;&#26080;&#32422;&#26463;&#20108;&#36827;&#21046;&#20248;&#21270;&#65288;QUBO&#65289;&#20844;&#24335;&#65292;&#36866;&#29992;&#20110;&#37327;&#23376;&#35745;&#31639;&#12290;QAL-BP&#37319;&#29992;&#22686;&#24191;&#25289;&#26684;&#26391;&#26085;&#26041;&#27861;&#23558;&#35013;&#31665;&#32422;&#26463;&#23884;&#20837;&#21040;&#30446;&#26631;&#20989;&#25968;&#20013;&#65292;&#24182;&#20415;&#20110;&#23545;&#21551;&#21457;&#24335;&#20056;&#25968;&#36827;&#34892;&#20998;&#26512;&#20272;&#35745;&#65292;&#20351;&#24471;&#27169;&#22411;&#26356;&#21152;&#28789;&#27963;&#21644;&#21487;&#25512;&#24191;&#65292;&#28040;&#38500;&#20102;&#22312;&#20854;&#20182;QUBO&#20844;&#24335;&#20013;&#24120;&#36935;&#21040;&#30340;&#38656;&#35201;&#26681;&#25454;&#23454;&#20363;&#35745;&#31639;Lagrangian&#31995;&#25968;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
The bin packing is a well-known NP-Hard problem in the domain of artificial intelligence, posing significant challenges in finding efficient solutions. Conversely, recent advancements in quantum technologies have shown promising potential for achieving substantial computational speedup, particularly in certain problem classes, such as combinatorial optimization. In this study, we introduce QAL-BP, a novel Quadratic Unconstrained Binary Optimization (QUBO) formulation designed specifically for bin packing and suitable for quantum computation. QAL-BP utilizes the augmented Lagrangian method to incorporate the bin packing constraints into the objective function while also facilitating an analytical estimation of heuristic, but empirically robust, penalty multipliers. This approach leads to a more versatile and generalizable model that eliminates the need for empirically calculating instance-dependent Lagrangian coefficients, a requirement commonly encountered in alternative QUBO formulati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20004;&#38454;&#27573;&#35757;&#32451;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#25193;&#25955;&#27169;&#22411;&#22312;&#35821;&#38899;&#22686;&#24378;&#20013;&#30340;&#38480;&#21046;&#12290;&#31532;&#19968;&#38454;&#27573;&#20351;&#29992;&#29983;&#25104;&#21435;&#22122;&#35780;&#20998;&#21305;&#37197;&#25439;&#22833;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#65292;&#31532;&#20108;&#38454;&#27573;&#36890;&#36807;&#35299;&#20915;&#21453;&#21521;&#36807;&#31243;&#26469;&#35745;&#31639;&#22686;&#24378;&#20449;&#21495;&#65292;&#24182;&#20351;&#29992;&#39044;&#27979;&#25439;&#22833;&#36827;&#34892;&#27604;&#36739;&#12290;&#36825;&#31181;&#26041;&#27861;&#21482;&#38656;&#35201;5&#20010;&#20989;&#25968;&#35780;&#20272;&#23601;&#33021;&#36798;&#21040;&#19982;&#22522;&#20934;&#27169;&#22411;&#30456;&#21516;&#30340;&#24615;&#33021;&#65292;&#32780;&#19981;&#26159;60&#20010;&#20989;&#25968;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2309.09677</link><description>&lt;p&gt;
&#21333;&#27493;&#21644;&#23569;&#27493;&#25193;&#25955;&#29992;&#20110;&#29983;&#25104;&#24335;&#35821;&#38899;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Single and Few-step Diffusion for Generative Speech Enhancement. (arXiv:2309.09677v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09677
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20004;&#38454;&#27573;&#35757;&#32451;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#25193;&#25955;&#27169;&#22411;&#22312;&#35821;&#38899;&#22686;&#24378;&#20013;&#30340;&#38480;&#21046;&#12290;&#31532;&#19968;&#38454;&#27573;&#20351;&#29992;&#29983;&#25104;&#21435;&#22122;&#35780;&#20998;&#21305;&#37197;&#25439;&#22833;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#65292;&#31532;&#20108;&#38454;&#27573;&#36890;&#36807;&#35299;&#20915;&#21453;&#21521;&#36807;&#31243;&#26469;&#35745;&#31639;&#22686;&#24378;&#20449;&#21495;&#65292;&#24182;&#20351;&#29992;&#39044;&#27979;&#25439;&#22833;&#36827;&#34892;&#27604;&#36739;&#12290;&#36825;&#31181;&#26041;&#27861;&#21482;&#38656;&#35201;5&#20010;&#20989;&#25968;&#35780;&#20272;&#23601;&#33021;&#36798;&#21040;&#19982;&#22522;&#20934;&#27169;&#22411;&#30456;&#21516;&#30340;&#24615;&#33021;&#65292;&#32780;&#19981;&#26159;60&#20010;&#20989;&#25968;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#35821;&#38899;&#22686;&#24378;&#26041;&#38754;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#21033;&#29992;&#20219;&#21153;&#36866;&#24212;&#24615;&#25193;&#25955;&#36807;&#31243;&#26469;&#29983;&#25104;&#32473;&#23450;&#22024;&#26434;&#28151;&#21512;&#22768;&#38899;&#30340;&#32431;&#20928;&#35821;&#38899;&#12290;&#28982;&#32780;&#65292;&#22312;&#27979;&#35797;&#26102;&#65292;&#29992;&#20110;&#35780;&#20998;&#20272;&#35745;&#30340;&#31070;&#32463;&#32593;&#32476;&#34987;&#22810;&#27425;&#35843;&#29992;&#20197;&#35299;&#20915;&#36845;&#20195;&#30340;&#21453;&#21521;&#36807;&#31243;&#12290;&#36825;&#23548;&#33268;&#25512;&#29702;&#36807;&#31243;&#32531;&#24930;&#65292;&#24182;&#23548;&#33268;&#22312;&#37319;&#26679;&#36712;&#36857;&#20013;&#31215;&#32047;&#31163;&#25955;&#21270;&#35823;&#24046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20004;&#38454;&#27573;&#35757;&#32451;&#26041;&#27861;&#35299;&#20915;&#20102;&#36825;&#20123;&#38480;&#21046;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#25105;&#20204;&#29992;&#29983;&#25104;&#21435;&#22122;&#35780;&#20998;&#21305;&#37197;&#25439;&#22833;&#30340;&#24120;&#35268;&#26041;&#24335;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#12290;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;&#25105;&#20204;&#36890;&#36807;&#35299;&#20915;&#21453;&#21521;&#36807;&#31243;&#26469;&#35745;&#31639;&#22686;&#24378;&#20449;&#21495;&#65292;&#24182;&#20351;&#29992;&#39044;&#27979;&#25439;&#22833;&#23558;&#32467;&#26524;&#20272;&#35745;&#19982;&#32431;&#20928;&#35821;&#38899;&#30446;&#26631;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#35777;&#26126;&#20351;&#29992;&#36825;&#20010;&#31532;&#20108;&#35757;&#32451;&#38454;&#27573;&#21482;&#38656;&#35201;5&#20010;&#20989;&#25968;&#35780;&#20272;&#65292;&#23601;&#33021;&#36798;&#21040;&#19982;&#22522;&#20934;&#27169;&#22411;&#30456;&#21516;&#30340;&#24615;&#33021;&#65292;&#32780;&#19981;&#26159;60&#20010;&#20989;&#25968;&#35780;&#20272;&#12290;&#34429;&#28982;&#22522;&#20934;&#27169;&#22411;&#30340;&#24615;&#33021;&#21487;&#33021;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have shown promising results in speech enhancement, using a task-adapted diffusion process for the conditional generation of clean speech given a noisy mixture. However, at test time, the neural network used for score estimation is called multiple times to solve the iterative reverse process. This results in a slow inference process and causes discretization errors that accumulate over the sampling trajectory. In this paper, we address these limitations through a two-stage training approach. In the first stage, we train the diffusion model the usual way using the generative denoising score matching loss. In the second stage, we compute the enhanced signal by solving the reverse process and compare the resulting estimate to the clean speech target using a predictive loss. We show that using this second training stage enables achieving the same performance as the baseline model using only 5 function evaluations instead of 60 function evaluations. While the performance of
&lt;/p&gt;</description></item><item><title>CB-Whisper&#26159;&#19968;&#31181;&#22522;&#20110;OpenAI&#30340;Whisper&#27169;&#22411;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;&#24320;&#25918;&#35789;&#27719;&#20851;&#38190;&#35789;&#26816;&#27979;&#65288;OV-KWS&#65289;&#35782;&#21035;&#32597;&#35265;&#30340;&#21629;&#21517;&#23454;&#20307;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#23454;&#20307;&#20316;&#20026;&#25552;&#31034;&#26469;&#25913;&#36827;&#35782;&#21035;&#25928;&#26524;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#25552;&#39640;&#23454;&#20307;&#21484;&#22238;&#29575;&#30340;&#21516;&#26102;&#20250;&#30053;&#24494;&#22686;&#21152;&#28151;&#28102;&#38169;&#35823;&#29575;&#65288;MER&#65289;&#12290;</title><link>http://arxiv.org/abs/2309.09552</link><description>&lt;p&gt;
CB-Whisper: &#20351;&#29992;&#24320;&#25918;&#35789;&#27719;&#20851;&#38190;&#35789;&#26816;&#27979;&#36827;&#34892;&#19978;&#19979;&#25991;&#20559;&#32622;&#30340;Whisper
&lt;/p&gt;
&lt;p&gt;
CB-Whisper: Contextual Biasing Whisper using Open-Vocabulary Keyword-Spotting. (arXiv:2309.09552v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09552
&lt;/p&gt;
&lt;p&gt;
CB-Whisper&#26159;&#19968;&#31181;&#22522;&#20110;OpenAI&#30340;Whisper&#27169;&#22411;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;&#24320;&#25918;&#35789;&#27719;&#20851;&#38190;&#35789;&#26816;&#27979;&#65288;OV-KWS&#65289;&#35782;&#21035;&#32597;&#35265;&#30340;&#21629;&#21517;&#23454;&#20307;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#23454;&#20307;&#20316;&#20026;&#25552;&#31034;&#26469;&#25913;&#36827;&#35782;&#21035;&#25928;&#26524;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#25552;&#39640;&#23454;&#20307;&#21484;&#22238;&#29575;&#30340;&#21516;&#26102;&#20250;&#30053;&#24494;&#22686;&#21152;&#28151;&#28102;&#38169;&#35823;&#29575;&#65288;MER&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#24448;&#24448;&#38590;&#20197;&#35782;&#21035;&#32597;&#35265;&#30340;&#21629;&#21517;&#23454;&#20307;&#65292;&#22914;&#20010;&#20154;&#22995;&#21517;&#12289;&#32452;&#32455;&#26426;&#26500;&#21644;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#19981;&#32463;&#24120;&#36935;&#21040;&#30340;&#19987;&#19994;&#26415;&#35821;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;OpenAI&#30340;Whisper&#27169;&#22411;&#30340;Contextual Biasing Whisper&#65288;CB-Whisper&#65289;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;Whisper&#32534;&#30721;&#22120;&#30340;&#38544;&#34255;&#29366;&#24577;&#25191;&#34892;&#24320;&#25918;&#35789;&#27719;&#20851;&#38190;&#35789;&#26816;&#27979;&#65288;OV-KWS&#65289;&#26469;&#35782;&#21035;&#29992;&#25143;&#23450;&#20041;&#30340;&#21629;&#21517;&#23454;&#20307;&#12290;&#35782;&#21035;&#20986;&#30340;&#23454;&#20307;&#34987;&#29992;&#20316;Whisper&#35299;&#30721;&#22120;&#30340;&#25552;&#31034;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;OV-KWS&#21644;ASR&#20219;&#21153;&#36827;&#34892;&#22810;&#20219;&#21153;&#35757;&#32451;&#30340;&#26041;&#27861;&#26469;&#20248;&#21270;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#21407;&#22987;Whisper&#27169;&#22411;&#30456;&#27604;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#20013;&#22269;Aishell&#28909;&#35789;&#23376;&#38598;&#21644;&#20004;&#20010;&#20869;&#37096;&#20195;&#30721;&#20999;&#25442;&#27979;&#35797;&#38598;&#19978;&#26174;&#33879;&#25552;&#39640;&#20102;&#23454;&#20307;&#21484;&#22238;&#29575;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#20869;&#37096;&#27979;&#35797;&#38598;&#19978;&#28151;&#28102;&#38169;&#35823;&#29575;&#65288;MER&#65289;&#30053;&#24494;&#22686;&#21152;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#24182;&#20351;&#29992;&#19981;&#21516;&#22823;&#23567;&#30340;Whisper&#27169;&#22411;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
End-to-end automatic speech recognition (ASR) systems often struggle to recognize rare name entities, such as personal names, organizations, and terminologies not frequently encountered in the training data. This paper presents Contextual Biasing Whisper (CB-Whisper), a novel ASR system based on OpenAI's Whisper model that can recognize user-defined name entities by performing open-vocabulary keyword-spotting (OV-KWS) using the hidden states of Whisper encoder. The recognized entities are used as prompts for the Whisper decoder. We first propose a multitask training approach with OV-KWS and ASR tasks to optimize the model. Experiments show that this approach substantially improves the entity recalls compared to the original Whisper model on Chinese Aishell hot word subsets and two internal code-switch test sets. However, we observed a slight increase in mixed-error-rate (MER) on internal test sets due to catastrophic forgetting. To address this problem and use different sizes of the Wh
&lt;/p&gt;</description></item><item><title>VoiceFlow&#20351;&#29992;&#30699;&#27491;&#27969;&#21305;&#37197;&#31639;&#27861;&#23454;&#29616;&#20102;&#39640;&#25928;&#25991;&#26412;&#36716;&#35821;&#38899;&#65292;&#24182;&#22312;&#21512;&#25104;&#36136;&#37327;&#19978;&#20248;&#20110;&#20256;&#32479;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.05027</link><description>&lt;p&gt;
VoiceFlow: &#20351;&#29992;&#30699;&#27491;&#27969;&#21305;&#37197;&#30340;&#39640;&#25928;&#25991;&#26412;&#36716;&#35821;&#38899;
&lt;/p&gt;
&lt;p&gt;
VoiceFlow: Efficient Text-to-Speech with Rectified Flow Matching. (arXiv:2309.05027v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05027
&lt;/p&gt;
&lt;p&gt;
VoiceFlow&#20351;&#29992;&#30699;&#27491;&#27969;&#21305;&#37197;&#31639;&#27861;&#23454;&#29616;&#20102;&#39640;&#25928;&#25991;&#26412;&#36716;&#35821;&#38899;&#65292;&#24182;&#22312;&#21512;&#25104;&#36136;&#37327;&#19978;&#20248;&#20110;&#20256;&#32479;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25193;&#25955;&#27169;&#22411;&#22312;&#25991;&#26412;&#36716;&#35821;&#38899;&#20013;&#22240;&#20854;&#24378;&#22823;&#30340;&#29983;&#25104;&#33021;&#21147;&#32780;&#25104;&#20026;&#19968;&#31181;&#27969;&#34892;&#36873;&#25321;&#65292;&#20294;&#20174;&#25193;&#25955;&#27169;&#22411;&#20013;&#36827;&#34892;&#37319;&#26679;&#30340;&#20869;&#22312;&#22797;&#26434;&#24615;&#25439;&#23475;&#20102;&#20854;&#25928;&#29575;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;VoiceFlow&#65292;&#19968;&#31181;&#21033;&#29992;&#30699;&#27491;&#27969;&#21305;&#37197;&#31639;&#27861;&#26469;&#23454;&#29616;&#39640;&#21512;&#25104;&#36136;&#37327;&#30340;&#22768;&#23398;&#27169;&#22411;&#65292;&#21482;&#38656;&#26377;&#38480;&#27425;&#37319;&#26679;&#27493;&#39588;&#21363;&#21487;&#23454;&#29616;&#12290;VoiceFlow&#23558;&#29983;&#25104;mel-spectrograms&#30340;&#36807;&#31243;&#36716;&#21270;&#20026;&#19968;&#20010;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#65292;&#22312;&#25991;&#26412;&#36755;&#20837;&#30340;&#26465;&#20214;&#19979;&#36827;&#34892;&#27714;&#35299;&#65292;&#24182;&#20272;&#35745;&#20986;&#20854;&#21521;&#37327;&#22330;&#12290;&#28982;&#21518;&#65292;&#30699;&#27491;&#27969;&#25216;&#26415;&#26377;&#25928;&#22320;&#20351;&#20854;&#37319;&#26679;&#36712;&#36857;&#30452;&#32447;&#21270;&#65292;&#23454;&#29616;&#39640;&#25928;&#21512;&#25104;&#12290;&#22312;&#21333;&#20010;&#21644;&#22810;&#20010;&#35828;&#35805;&#32773;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#30340;&#20027;&#35266;&#21644;&#23458;&#35266;&#35780;&#20272;&#26174;&#31034;&#65292;VoiceFlow&#30456;&#23545;&#20110;&#25193;&#25955;&#27169;&#22411;&#20855;&#26377;&#26356;&#20248;&#24322;&#30340;&#21512;&#25104;&#36136;&#37327;&#12290;&#28040;&#34701;&#30740;&#31350;&#36827;&#19968;&#27493;&#39564;&#35777;&#20102;VoiceFlow&#20013;&#30699;&#27491;&#27969;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although diffusion models in text-to-speech have become a popular choice due to their strong generative ability, the intrinsic complexity of sampling from diffusion models harms their efficiency. Alternatively, we propose VoiceFlow, an acoustic model that utilizes a rectified flow matching algorithm to achieve high synthesis quality with a limited number of sampling steps. VoiceFlow formulates the process of generating mel-spectrograms into an ordinary differential equation conditional on text inputs, whose vector field is then estimated. The rectified flow technique then effectively straightens its sampling trajectory for efficient synthesis. Subjective and objective evaluations on both single and multi-speaker corpora showed the superior synthesis quality of VoiceFlow compared to the diffusion counterpart. Ablation studies further verified the validity of the rectified flow technique in VoiceFlow.
&lt;/p&gt;</description></item><item><title>&#20256;&#32479;&#30340;&#31526;&#21495;AI&#26041;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;AI&#26041;&#27861;&#26080;&#27861;&#28385;&#36275;&#21019;&#24314;&#24378;&#22823;&#21644;&#21487;&#20449;&#36182;&#30340;AI&#30340;&#25361;&#25112;&#65292;&#28982;&#32780;&#65292;&#21457;&#23637;&#33073;&#38772;&#27861;&#36890;&#36807;&#27169;&#20223;&#20154;&#31867;&#20799;&#31461;&#30340;&#33021;&#21147;&#21457;&#23637;&#36807;&#31243;&#65292;&#20026;&#21019;&#24314;&#31283;&#20581;&#21487;&#38752;&#30340;AI&#25552;&#20379;&#20102;&#24076;&#26395;&#12290;</title><link>http://arxiv.org/abs/2308.04586</link><description>&lt;p&gt;
AIs&#30340;&#21457;&#23637;&#33073;&#38772;&#27861;
&lt;/p&gt;
&lt;p&gt;
Developmental Bootstrapping of AIs. (arXiv:2308.04586v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04586
&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#31526;&#21495;AI&#26041;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;AI&#26041;&#27861;&#26080;&#27861;&#28385;&#36275;&#21019;&#24314;&#24378;&#22823;&#21644;&#21487;&#20449;&#36182;&#30340;AI&#30340;&#25361;&#25112;&#65292;&#28982;&#32780;&#65292;&#21457;&#23637;&#33073;&#38772;&#27861;&#36890;&#36807;&#27169;&#20223;&#20154;&#31867;&#20799;&#31461;&#30340;&#33021;&#21147;&#21457;&#23637;&#36807;&#31243;&#65292;&#20026;&#21019;&#24314;&#31283;&#20581;&#21487;&#38752;&#30340;AI&#25552;&#20379;&#20102;&#24076;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24403;&#21069;&#19968;&#20123;AI&#22312;&#23553;&#38381;&#30340;&#19990;&#30028;&#65292;&#22914;&#26827;&#30424;&#28216;&#25103;&#20013;&#36229;&#36234;&#20102;&#20154;&#31867;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#28151;&#20081;&#30340;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#34920;&#29616;&#26377;&#38480;&#12290;&#23427;&#20204;&#20250;&#29359;&#22855;&#24618;&#30340;&#38169;&#35823;&#32780;&#19988;&#27809;&#26377;&#24847;&#35782;&#21040;&#12290;&#23427;&#20204;&#24456;&#38590;&#21463;&#21040;&#25351;&#23548;&#65292;&#19981;&#33021;&#36816;&#29992;&#24120;&#35782;&#65292;&#32570;&#20047;&#22909;&#22855;&#24515;&#12290;&#23427;&#20204;&#19981;&#33021;&#25104;&#20026;&#33391;&#22909;&#30340;&#21512;&#20316;&#32773;&#12290;&#20256;&#32479;&#25163;&#21160;&#26500;&#24314;&#30340;&#31526;&#21495;AI&#26041;&#27861;&#26500;&#24314;&#30340;&#31995;&#32479;&#21644;&#20351;&#29992;&#29983;&#25104;&#21644;&#28145;&#24230;&#23398;&#20064;AI&#26041;&#27861;(&#21253;&#25324;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;)&#26500;&#24314;&#30340;&#31995;&#32479;&#37117;&#26080;&#27861;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;&#23427;&#20204;&#19981;&#36866;&#21512;&#21019;&#24314;&#24378;&#22823;&#21644;&#21487;&#20449;&#36182;&#30340;AI&#12290;&#23613;&#31649;&#27492;&#26041;&#27861;&#19981;&#23646;&#20110;&#20027;&#27969;&#30340;AI&#26041;&#27861;&#65292;&#20294;&#21457;&#23637;&#33073;&#38772;&#27861;&#26174;&#31034;&#20986;&#24076;&#26395;&#12290;&#22312;&#21457;&#23637;&#33073;&#38772;&#27861;&#20013;&#65292;AI&#20687;&#20154;&#31867;&#20799;&#31461;&#19968;&#26679;&#21457;&#23637;&#33021;&#21147;&#12290;&#23427;&#20204;&#20174;&#20808;&#22825;&#33021;&#21147;&#24320;&#22987;&#12290;&#20687;&#20154;&#31867;&#19968;&#26679;&#65292;&#23427;&#20204;&#19982;&#29615;&#22659;&#20114;&#21160;&#65292;&#24182;&#20174;&#20114;&#21160;&#20013;&#23398;&#20064;&#12290;&#23427;&#20204;&#36890;&#36807;&#33258;&#25105;&#21457;&#23637;&#30340;&#33021;&#21147;&#36880;&#27493;&#25193;&#23637;&#20808;&#22825;&#33021;&#21147;&#12290;&#23427;&#20204;&#20114;&#21160;&#24182;&#36880;&#28176;&#23558;&#25152;&#23398;&#24212;&#29992;&#20110;&#23454;&#38469;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although some current AIs surpass human abilities especially in closed worlds such as board games, their performance in the messy real world is limited. They make strange mistakes and do not notice them. They cannot be instructed easily, fail to use common sense, and lack curiosity. They do not make good collaborators. Neither systems built using the traditional manually-constructed symbolic AI approach nor systems built using generative and deep learning AI approaches including large language models (LLMs) can meet the challenges. They are not well suited for creating robust and trustworthy AIs. Although it is outside of mainstream AI approaches, developmental bootstrapping shows promise. In developmental bootstrapping, AIs develop competences like human children do. They start with innate competences. Like humans, they interact with the environment and learn from their interactions. They incrementally extend their innate competences with self-developed competences. They interact and 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;LSTM&#21644;DDPM&#30456;&#32467;&#21512;&#30340;&#26041;&#26696;&#26469;&#35299;&#20915;&#26234;&#33021;&#30005;&#32593;&#31995;&#32479;&#20013;&#30340;&#33021;&#37327;&#30423;&#31363;&#26816;&#27979;&#21644;&#39044;&#27979;&#38382;&#39064;&#12290;&#36890;&#36807;&#37325;&#26500;&#21644;&#39044;&#27979;&#35823;&#24046;&#65292;&#31995;&#32479;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#33021;&#37327;&#30423;&#31363;&#30340;&#23454;&#20363;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.16149</link><description>&lt;p&gt;
&#26234;&#33021;&#30005;&#32593;&#20013;&#19968;&#31181;&#26377;&#25928;&#30340;&#29992;&#20110;&#33021;&#37327;&#30423;&#31363;&#26816;&#27979;&#21644;&#39044;&#27979;&#30340;LSTM-DDPM&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
An Effective LSTM-DDPM Scheme for Energy Theft Detection and Forecasting in Smart Grid. (arXiv:2307.16149v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16149
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;LSTM&#21644;DDPM&#30456;&#32467;&#21512;&#30340;&#26041;&#26696;&#26469;&#35299;&#20915;&#26234;&#33021;&#30005;&#32593;&#31995;&#32479;&#20013;&#30340;&#33021;&#37327;&#30423;&#31363;&#26816;&#27979;&#21644;&#39044;&#27979;&#38382;&#39064;&#12290;&#36890;&#36807;&#37325;&#26500;&#21644;&#39044;&#27979;&#35823;&#24046;&#65292;&#31995;&#32479;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#33021;&#37327;&#30423;&#31363;&#30340;&#23454;&#20363;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#37327;&#30423;&#31363;&#26816;&#27979;&#65288;ETD&#65289;&#21644;&#33021;&#37327;&#28040;&#32791;&#39044;&#27979;&#65288;ECF&#65289;&#26159;&#26234;&#33021;&#30005;&#32593;&#31995;&#32479;&#20013;&#20004;&#20010;&#30456;&#20114;&#20851;&#32852;&#30340;&#25361;&#25112;&#12290;&#20849;&#21516;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#23545;&#20110;&#30830;&#20445;&#31995;&#32479;&#23433;&#20840;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#35770;&#25991;&#35299;&#20915;&#20102;&#26234;&#33021;&#30005;&#32593;&#31995;&#32479;&#20013;&#30340;ETD&#21644;ECF&#30340;&#30456;&#20114;&#20851;&#32852;&#25361;&#25112;&#12290;&#25152;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#32467;&#21512;&#20102;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#21644;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPM&#65289;&#65292;&#29992;&#20110;&#29983;&#25104;&#36755;&#20837;&#37325;&#26500;&#21644;&#39044;&#27979;&#12290;&#36890;&#36807;&#21033;&#29992;&#37325;&#26500;&#21644;&#39044;&#27979;&#35823;&#24046;&#65292;&#31995;&#32479;&#33021;&#22815;&#35782;&#21035;&#33021;&#37327;&#30423;&#31363;&#30340;&#23454;&#20363;&#65292;&#22522;&#20110;&#37325;&#26500;&#35823;&#24046;&#21644;&#39044;&#27979;&#35823;&#24046;&#30340;&#26041;&#27861;&#30456;&#20114;&#34917;&#20805;&#65292;&#21487;&#20197;&#26816;&#27979;&#19981;&#21516;&#31867;&#22411;&#30340;&#25915;&#20987;&#12290;&#36890;&#36807;&#22312;&#30495;&#23454;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#26696;&#22312;ETD&#21644;ECF&#38382;&#39064;&#19978;&#34920;&#29616;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#12290;&#38598;&#25104;&#26041;&#27861;&#26174;&#33879;&#25552;&#21319;&#20102;ETD&#24615;&#33021;&#65292;&#33021;&#22815;&#20934;&#30830;&#26816;&#27979;&#21040;&#22522;&#20934;&#26041;&#27861;&#26410;&#33021;&#26816;&#27979;&#21040;&#30340;&#33021;&#37327;&#30423;&#31363;&#25915;&#20987;&#12290;&#35813;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#35299;&#20915;&#26234;&#33021;&#30005;&#32593;&#31995;&#32479;&#20013;ETD&#21644;ECF&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Energy theft detection (ETD) and energy consumption forecasting (ECF) are two interconnected challenges in smart grid systems. Addressing these issues collectively is crucial for ensuring system security. This paper addresses the interconnected challenges of ETD and ECF in smart grid systems. The proposed solution combines long short-term memory (LSTM) and a denoising diffusion probabilistic model (DDPM) to generate input reconstruction and forecasting. By leveraging the reconstruction and forecasting errors, the system identifies instances of energy theft, with the methods based on reconstruction error and forecasting error complementing each other in detecting different types of attacks. Through extensive experiments on real-world and synthetic datasets, the proposed scheme outperforms baseline methods in ETD and ECF problems. The ensemble method significantly enhances ETD performance, accurately detecting energy theft attacks that baseline methods fail to detect. The research offers
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35266;&#30475;&#25163;&#26415;&#35270;&#39057;&#35762;&#24231;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;SurgVLP&#65292;&#36890;&#36807;&#21033;&#29992;&#25163;&#26415;&#35270;&#39057;&#35762;&#24231;&#20013;&#30340;&#35821;&#38899;&#21644;&#35270;&#35273;&#20449;&#24687;&#36827;&#34892;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#35299;&#20915;&#20102;&#25163;&#26415;&#30456;&#20851;&#35821;&#35328;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.15220</link><description>&lt;p&gt;
&#36890;&#36807;&#35266;&#30475;&#25968;&#30334;&#20010;&#25163;&#26415;&#35270;&#39057;&#35762;&#24231;&#23398;&#20064;&#22810;&#27169;&#24577;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning Multi-modal Representations by Watching Hundreds of Surgical Video Lectures. (arXiv:2307.15220v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15220
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35266;&#30475;&#25163;&#26415;&#35270;&#39057;&#35762;&#24231;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;SurgVLP&#65292;&#36890;&#36807;&#21033;&#29992;&#25163;&#26415;&#35270;&#39057;&#35762;&#24231;&#20013;&#30340;&#35821;&#38899;&#21644;&#35270;&#35273;&#20449;&#24687;&#36827;&#34892;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#35299;&#20915;&#20102;&#25163;&#26415;&#30456;&#20851;&#35821;&#35328;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#22806;&#31185;&#35745;&#31639;&#26426;&#35270;&#35273;&#24212;&#29992;&#26041;&#38754;&#30340;&#36827;&#23637;&#20027;&#35201;&#20381;&#38752;&#23436;&#20840;&#30417;&#30563;&#26041;&#27861;&#65292;&#20027;&#35201;&#20351;&#29992;&#35270;&#35273;&#25968;&#25454;&#12290;&#36825;&#20123;&#26041;&#27861;&#20381;&#36182;&#20110;&#25163;&#21160;&#27880;&#37322;&#30340;&#25163;&#26415;&#35270;&#39057;&#26469;&#39044;&#27979;&#19968;&#32452;&#22266;&#23450;&#30340;&#23545;&#35937;&#31867;&#21035;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#26410;&#35265;&#25163;&#26415;&#31243;&#24207;&#21644;&#21518;&#32493;&#20219;&#21153;&#19978;&#30340;&#36890;&#29992;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35266;&#28857;&#65292;&#21363;&#36890;&#36807;&#24320;&#25918;&#30340;&#25163;&#26415;&#30005;&#23376;&#23398;&#20064;&#24179;&#21488;&#25552;&#20379;&#30340;&#25163;&#26415;&#35270;&#39057;&#35762;&#24231;&#21487;&#20197;&#20026;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#25552;&#20379;&#26377;&#25928;&#30340;&#30417;&#30563;&#20449;&#21495;&#65292;&#32780;&#26080;&#38656;&#20381;&#36182;&#25163;&#21160;&#27880;&#37322;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#22810;&#20010;&#20114;&#34917;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#29983;&#25104;&#25991;&#26412;&#36716;&#24405;&#26469;&#35299;&#20915;&#25163;&#26415;&#35270;&#39057;&#35762;&#24231;&#20013;&#23384;&#22312;&#30340;&#25163;&#26415;&#30456;&#20851;&#35821;&#35328;&#25361;&#25112;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;SurgVLP - &#25163;&#26415;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#65292;&#29992;&#20110;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#12290;SurgVLP&#26500;&#24314;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#65292;&#23558;&#35270;&#39057;&#21098;&#36753;&#23884;&#20837;&#19982;&#30456;&#24212;&#30340;&#25991;&#26412;&#23884;&#20837;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in surgical computer vision applications have been driven by fully-supervised methods, primarily using only visual data. These methods rely on manually annotated surgical videos to predict a fixed set of object categories, limiting their generalizability to unseen surgical procedures and downstream tasks. In this work, we put forward the idea that the surgical video lectures available through open surgical e-learning platforms can provide effective supervisory signals for multi-modal representation learning without relying on manual annotations. We address the surgery-specific linguistic challenges present in surgical video lectures by employing multiple complementary automatic speech recognition systems to generate text transcriptions. We then present a novel method, SurgVLP - Surgical Vision Language Pre-training, for multi-modal representation learning. SurgVLP constructs a new contrastive learning objective to align video clip embeddings with the corresponding m
&lt;/p&gt;</description></item><item><title>FedDRL&#26159;&#19968;&#31181;&#20998;&#38454;&#27573;&#24378;&#21270;&#23398;&#20064;&#30340;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#34701;&#21512;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#26080;&#27861;&#35299;&#20915;&#30340;&#23458;&#25143;&#31471;&#27169;&#22411;&#36136;&#37327;&#21644;&#24694;&#24847;&#27169;&#22411;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.13716</link><description>&lt;p&gt;
FedDRL: &#19968;&#31181;&#22522;&#20110;&#20998;&#38454;&#27573;&#24378;&#21270;&#23398;&#20064;&#30340;&#21487;&#20449;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#34701;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
FedDRL: A Trustworthy Federated Learning Model Fusion Method Based on Staged Reinforcement Learning. (arXiv:2307.13716v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13716
&lt;/p&gt;
&lt;p&gt;
FedDRL&#26159;&#19968;&#31181;&#20998;&#38454;&#27573;&#24378;&#21270;&#23398;&#20064;&#30340;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#34701;&#21512;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#26080;&#27861;&#35299;&#20915;&#30340;&#23458;&#25143;&#31471;&#27169;&#22411;&#36136;&#37327;&#21644;&#24694;&#24847;&#27169;&#22411;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#32852;&#37030;&#23398;&#20064;&#20351;&#29992;&#26679;&#26412;&#25968;&#37327;&#35745;&#31639;&#27599;&#20010;&#23458;&#25143;&#31471;&#27169;&#22411;&#30340;&#26435;&#37325;&#65292;&#24182;&#20351;&#29992;&#36825;&#20010;&#22266;&#23450;&#26435;&#37325;&#20540;&#26469;&#34701;&#21512;&#20840;&#23616;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#27599;&#20010;&#23458;&#25143;&#31471;&#35774;&#22791;&#21644;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#23548;&#33268;&#27599;&#20010;&#23458;&#25143;&#31471;&#27169;&#22411;&#30340;&#36136;&#37327;&#23384;&#22312;&#24046;&#24322;&#12290;&#22240;&#27492;&#65292;&#23545;&#20840;&#23616;&#27169;&#22411;&#30340;&#36129;&#29486;&#19981;&#20165;&#20165;&#21462;&#20915;&#20110;&#26679;&#26412;&#37327;&#12290;&#27492;&#22806;&#65292;&#22914;&#26524;&#23458;&#25143;&#31471;&#25925;&#24847;&#19978;&#20256;&#20302;&#36136;&#37327;&#25110;&#24694;&#24847;&#27169;&#22411;&#65292;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#32858;&#21512;&#23558;&#20005;&#37325;&#38477;&#20302;&#20840;&#23616;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#20256;&#32479;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#27809;&#26377;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedDRL&#30340;&#27169;&#22411;&#34701;&#21512;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#20004;&#20010;&#38454;&#27573;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;&#22312;&#31532;&#19968;&#20010;&#38454;&#27573;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#36807;&#28388;&#25481;&#24694;&#24847;&#27169;&#22411;&#65292;&#24182;&#36873;&#25321;&#21487;&#20449;&#30340;&#23458;&#25143;&#31471;&#27169;&#22411;&#21442;&#19982;&#27169;&#22411;&#34701;&#21512;&#12290;&#22312;&#31532;&#20108;&#20010;&#38454;&#27573;&#65292;FedDRL&#31639;&#27861;&#33258;&#36866;&#24212;&#22320;&#35843;&#25972;&#21487;&#20449;&#23458;&#25143;&#31471;&#27169;&#22411;&#30340;&#26435;&#37325;&#24182;&#32858;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional federated learning uses the number of samples to calculate the weights of each client model and uses this fixed weight value to fusion the global model. However, in practical scenarios, each client's device and data heterogeneity leads to differences in the quality of each client's model. Thus the contribution to the global model is not wholly determined by the sample size. In addition, if clients intentionally upload low-quality or malicious models, using these models for aggregation will lead to a severe decrease in global model accuracy. Traditional federated learning algorithms do not address these issues. To solve this probelm, we propose FedDRL, a model fusion approach using reinforcement learning based on a two staged approach. In the first stage, Our method could filter out malicious models and selects trusted client models to participate in the model fusion. In the second stage, the FedDRL algorithm adaptively adjusts the weights of the trusted client models and ag
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#32852;&#37030;&#21307;&#23398;&#25104;&#20687;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#23458;&#25143;&#31471;&#32423;&#24046;&#20998;&#38544;&#31169;&#30340;&#20248;&#21270;&#31574;&#30053;&#65292;&#36890;&#36807;&#23558;&#23458;&#25143;&#31471;&#25286;&#20998;&#20026;&#23376;&#23458;&#25143;&#31471;&#20316;&#20026;&#20013;&#20171;&#26426;&#26500;&#65292;&#26469;&#25913;&#21892;&#24615;&#33021;&#32780;&#19981;&#25439;&#23475;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2307.12542</link><description>&lt;p&gt;
&#23458;&#25143;&#31471;&#32423;&#24046;&#20998;&#38544;&#31169;&#36890;&#36807;&#33258;&#36866;&#24212;&#20013;&#20171;&#22312;&#32852;&#37030;&#21307;&#23398;&#25104;&#20687;&#20013;
&lt;/p&gt;
&lt;p&gt;
Client-Level Differential Privacy via Adaptive Intermediary in Federated Medical Imaging. (arXiv:2307.12542v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12542
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#32852;&#37030;&#21307;&#23398;&#25104;&#20687;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#23458;&#25143;&#31471;&#32423;&#24046;&#20998;&#38544;&#31169;&#30340;&#20248;&#21270;&#31574;&#30053;&#65292;&#36890;&#36807;&#23558;&#23458;&#25143;&#31471;&#25286;&#20998;&#20026;&#23376;&#23458;&#25143;&#31471;&#20316;&#20026;&#20013;&#20171;&#26426;&#26500;&#65292;&#26469;&#25913;&#21892;&#24615;&#33021;&#32780;&#19981;&#25439;&#23475;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26368;&#36817;&#22312;&#36890;&#36807;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#22686;&#24378;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#30340;&#38544;&#31169;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#21307;&#23398;&#22330;&#26223;&#20013;&#65292;DP&#22312;&#38544;&#31169;&#20445;&#25252;&#21644;&#24615;&#33021;&#20043;&#38388;&#30340;&#26435;&#34913;&#20173;&#26410;&#20805;&#20998;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#22312;&#23458;&#25143;&#31471;&#32423;DP&#30340;&#32972;&#26223;&#19979;&#20248;&#21270;&#36825;&#31181;&#26435;&#34913;&#65292;&#37325;&#28857;&#20851;&#27880;&#36890;&#20449;&#36807;&#31243;&#20013;&#30340;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#21307;&#23398;&#25104;&#20687;&#30340;FL&#36890;&#24120;&#28041;&#21450;&#30340;&#21442;&#19982;&#32773;&#65288;&#21307;&#38498;&#65289;&#27604;&#20854;&#20182;&#39046;&#22495;&#65288;&#22914;&#31227;&#21160;&#35774;&#22791;&#65289;&#23569;&#24471;&#22810;&#65292;&#22240;&#27492;&#30830;&#20445;&#23458;&#25143;&#31471;&#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#20013;&#20171;&#31574;&#30053;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#32780;&#19981;&#25439;&#23475;&#38544;&#31169;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#21457;&#29616;&#23558;&#23458;&#25143;&#31471;&#20998;&#20026;&#23376;&#23458;&#25143;&#31471;&#65292;&#20316;&#20026;&#21307;&#38498;&#21644;&#26381;&#21153;&#22120;&#20043;&#38388;&#30340;&#20013;&#20171;&#65292;&#21487;&#20197;&#20943;&#36731;DP&#24341;&#20837;&#30340;&#22122;&#38899;&#32780;&#19981;&#25439;&#23475;&#38544;&#31169;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20351;&#29992;&#20004;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#36827;&#34892;&#20998;&#31867;&#21644;&#20998;&#21106;&#20219;&#21153;&#30340;&#23454;&#35777;&#35780;&#20272;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite recent progress in enhancing the privacy of federated learning (FL) via differential privacy (DP), the trade-off of DP between privacy protection and performance is still underexplored for real-world medical scenario. In this paper, we propose to optimize the trade-off under the context of client-level DP, which focuses on privacy during communications. However, FL for medical imaging involves typically much fewer participants (hospitals) than other domains (e.g., mobile devices), thus ensuring clients be differentially private is much more challenging. To tackle this problem, we propose an adaptive intermediary strategy to improve performance without harming privacy. Specifically, we theoretically find splitting clients into sub-clients, which serve as intermediaries between hospitals and the server, can mitigate the noises introduced by DP without harming privacy. Our proposed approach is empirically evaluated on both classification and segmentation tasks using two public dat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22810;&#38454;&#27573;&#30005;&#32518;&#24067;&#32447;&#20219;&#21153;&#20013;&#30340;&#23618;&#27425;&#21270;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22788;&#29702;&#21487;&#21464;&#24418;&#29289;&#20307;&#12289;&#35270;&#35273;&#24863;&#30693;&#38381;&#29615;&#21644;&#25193;&#23637;&#34892;&#20026;&#30340;&#25361;&#25112;&#12290;&#25104;&#21151;&#25511;&#21046;&#22120;&#38656;&#35201;&#33021;&#22815;&#20174;&#22833;&#36133;&#20013;&#24674;&#22797;&#65292;&#24182;&#36890;&#36807;&#36873;&#25321;&#32416;&#27491;&#20302;&#32423;&#25511;&#21046;&#22120;&#30340;&#32570;&#38519;&#12290;</title><link>http://arxiv.org/abs/2307.08927</link><description>&lt;p&gt;
&#22810;&#38454;&#27573;&#30005;&#32518;&#24067;&#32447;&#30340;&#23618;&#27425;&#21270;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multi-Stage Cable Routing through Hierarchical Imitation Learning. (arXiv:2307.08927v3 [cs.RO] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08927
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22810;&#38454;&#27573;&#30005;&#32518;&#24067;&#32447;&#20219;&#21153;&#20013;&#30340;&#23618;&#27425;&#21270;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22788;&#29702;&#21487;&#21464;&#24418;&#29289;&#20307;&#12289;&#35270;&#35273;&#24863;&#30693;&#38381;&#29615;&#21644;&#25193;&#23637;&#34892;&#20026;&#30340;&#25361;&#25112;&#12290;&#25104;&#21151;&#25511;&#21046;&#22120;&#38656;&#35201;&#33021;&#22815;&#20174;&#22833;&#36133;&#20013;&#24674;&#22797;&#65292;&#24182;&#36890;&#36807;&#36873;&#25321;&#32416;&#27491;&#20302;&#32423;&#25511;&#21046;&#22120;&#30340;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#23398;&#20064;&#22914;&#20309;&#25191;&#34892;&#22810;&#38454;&#27573;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#30340;&#38382;&#39064;&#65292;&#24212;&#29992;&#20110;&#30005;&#32518;&#24067;&#32447;&#65292;&#20854;&#20013;&#26426;&#22120;&#20154;&#24517;&#39035;&#36890;&#36807;&#19968;&#31995;&#21015;&#22841;&#23376;&#26469;&#24067;&#32447;&#12290;&#36825;&#20010;&#35774;&#32622;&#20195;&#34920;&#20102;&#22797;&#26434;&#22810;&#38454;&#27573;&#26426;&#22120;&#20154;&#25805;&#20316;&#22330;&#26223;&#30340;&#25361;&#25112;&#65306;&#22788;&#29702;&#21487;&#21464;&#24418;&#29289;&#20307;&#65292;&#23545;&#35270;&#35273;&#24863;&#30693;&#38381;&#29615;&#65292;&#22788;&#29702;&#30001;&#22810;&#20010;&#27493;&#39588;&#32452;&#25104;&#30340;&#25193;&#23637;&#34892;&#20026;&#65292;&#24517;&#39035;&#25104;&#21151;&#25191;&#34892;&#25165;&#33021;&#23436;&#25104;&#25972;&#20010;&#20219;&#21153;&#12290;&#22312;&#36825;&#31181;&#35774;&#32622;&#20013;&#65292;&#20026;&#27599;&#20010;&#38454;&#27573;&#23398;&#20064;&#25104;&#21151;&#29575;&#36275;&#22815;&#39640;&#30340;&#21333;&#20010;&#22522;&#20803;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#65306;&#22914;&#26524;&#27599;&#20010;&#38454;&#27573;&#24517;&#39035;&#25104;&#21151;&#23436;&#25104;&#24182;&#19988;&#26377;&#36739;&#22823;&#30340;&#22833;&#36133;&#27010;&#29575;&#65292;&#25972;&#20010;&#20219;&#21153;&#25104;&#21151;&#23436;&#25104;&#30340;&#27010;&#29575;&#21464;&#24471;&#24494;&#19981;&#36275;&#36947;&#12290;&#22240;&#27492;&#65292;&#36825;&#26679;&#30340;&#22810;&#38454;&#27573;&#20219;&#21153;&#30340;&#25104;&#21151;&#25511;&#21046;&#22120;&#24517;&#39035;&#33021;&#22815;&#20174;&#22833;&#36133;&#20013;&#24674;&#22797;&#65292;&#24182;&#36890;&#36807;&#32874;&#26126;&#22320;&#36873;&#25321;&#20174;&#32780;&#32416;&#27491;&#20302;&#32423;&#25511;&#21046;&#22120;&#30340;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of learning to perform multi-stage robotic manipulation tasks, with applications to cable routing, where the robot must route a cable through a series of clips. This setting presents challenges representative of complex multi-stage robotic manipulation scenarios: handling deformable objects, closing the loop on visual perception, and handling extended behaviors consisting of multiple steps that must be executed successfully to complete the entire task. In such settings, learning individual primitives for each stage that succeed with a high enough rate to perform a complete temporally extended task is impractical: if each stage must be completed successfully and has a non-negligible probability of failure, the likelihood of successful completion of the entire task becomes negligible. Therefore, successful controllers for such multi-stage tasks must be able to recover from failure and compensate for imperfections in low-level controllers by smartly choosing which con
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22270;&#23398;&#20064;&#20013;&#30340;&#28508;&#21147;&#65292;&#24182;&#23581;&#35797;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#27969;&#31243;&#65306;&#23558;LLMs&#20316;&#20026;&#22686;&#24378;&#22120;&#36890;&#36807;&#28023;&#37327;&#30693;&#35782;&#26469;&#22686;&#24378;&#33410;&#28857;&#30340;&#25991;&#26412;&#23646;&#24615;&#65292;&#24182;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#29983;&#25104;&#39044;&#27979;&#65292;&#20197;&#21450;&#30452;&#25509;&#20351;&#29992;LLMs&#20316;&#20026;&#29420;&#31435;&#30340;&#39044;&#27979;&#22120;&#12290;</title><link>http://arxiv.org/abs/2307.03393</link><description>&lt;p&gt;
&#25506;&#32034;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22270;&#23398;&#20064;&#20013;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Exploring the Potential of Large Language Models (LLMs) in Learning on Graphs. (arXiv:2307.03393v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03393
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22270;&#23398;&#20064;&#20013;&#30340;&#28508;&#21147;&#65292;&#24182;&#23581;&#35797;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#27969;&#31243;&#65306;&#23558;LLMs&#20316;&#20026;&#22686;&#24378;&#22120;&#36890;&#36807;&#28023;&#37327;&#30693;&#35782;&#26469;&#22686;&#24378;&#33410;&#28857;&#30340;&#25991;&#26412;&#23646;&#24615;&#65292;&#24182;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#29983;&#25104;&#39044;&#27979;&#65292;&#20197;&#21450;&#30452;&#25509;&#20351;&#29992;LLMs&#20316;&#20026;&#29420;&#31435;&#30340;&#39044;&#27979;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#23398;&#20064;&#22240;&#20854;&#24191;&#27867;&#30340;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#32780;&#24341;&#36215;&#20102;&#26497;&#22823;&#30340;&#20851;&#27880;&#12290;&#20197;&#25991;&#26412;&#33410;&#28857;&#23646;&#24615;&#20026;&#20027;&#30340;&#22270;&#23398;&#20064;&#26368;&#27969;&#34892;&#30340;&#27969;&#31243;&#20027;&#35201;&#20381;&#36182;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#65292;&#24182;&#21033;&#29992;&#27973;&#23618;&#25991;&#26412;&#23884;&#20837;&#20316;&#20026;&#21021;&#22987;&#33410;&#28857;&#34920;&#31034;&#65292;&#20294;&#23384;&#22312;&#36890;&#29992;&#30693;&#35782;&#21644;&#28145;&#21051;&#35821;&#20041;&#29702;&#35299;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;&#36817;&#24180;&#26469;&#65292;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34987;&#35777;&#26126;&#20855;&#26377;&#24191;&#27867;&#30340;&#24120;&#35782;&#21644;&#24378;&#22823;&#30340;&#35821;&#20041;&#29702;&#35299;&#33021;&#21147;&#65292;&#24050;&#32463;&#39072;&#35206;&#20102;&#29616;&#26377;&#30340;&#22788;&#29702;&#25991;&#26412;&#25968;&#25454;&#30340;&#24037;&#20316;&#27969;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#25506;&#32034;LLMs&#22312;&#22270;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#28508;&#21147;&#65292;&#29305;&#21035;&#26159;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#30740;&#31350;&#20004;&#31181;&#21487;&#33021;&#30340;&#27969;&#31243;&#65306;LLMs&#20316;&#20026;&#22686;&#24378;&#22120;&#21644;LLMs&#20316;&#20026;&#39044;&#27979;&#22120;&#12290;&#21069;&#32773;&#21033;&#29992;LLMs&#36890;&#36807;&#20854;&#28023;&#37327;&#30693;&#35782;&#22686;&#24378;&#33410;&#28857;&#30340;&#25991;&#26412;&#23646;&#24615;&#65292;&#28982;&#21518;&#36890;&#36807;GNNs&#29983;&#25104;&#39044;&#27979;&#12290;&#21518;&#32773;&#35797;&#22270;&#30452;&#25509;&#20351;&#29992;LLMs&#20316;&#20026;&#29420;&#31435;&#30340;&#39044;&#27979;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning on Graphs has attracted immense attention due to its wide real-world applications. The most popular pipeline for learning on graphs with textual node attributes primarily relies on Graph Neural Networks (GNNs), and utilizes shallow text embedding as initial node representations, which has limitations in general knowledge and profound semantic understanding. In recent years, Large Language Models (LLMs) have been proven to possess extensive common knowledge and powerful semantic comprehension abilities that have revolutionized existing workflows to handle text data. In this paper, we aim to explore the potential of LLMs in graph machine learning, especially the node classification task, and investigate two possible pipelines: LLMs-as-Enhancers and LLMs-as-Predictors. The former leverages LLMs to enhance nodes' text attributes with their massive knowledge and then generate predictions through GNNs. The latter attempts to directly employ LLMs as standalone predictors. We conduct 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35782;&#21035;&#38750;&#32447;&#24615;&#21152;&#24615;&#22122;&#22768;&#27169;&#22411;&#20013;&#22240;&#26524;&#26426;&#21046;&#36716;&#21464;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#19987;&#27880;&#20110;&#22312;&#30456;&#20851;&#30340;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#20013;&#35782;&#21035;&#21151;&#33021;&#26426;&#21046;&#30340;&#21464;&#21270;&#65292;&#32780;&#19981;&#38656;&#35201;&#20272;&#35745;&#25972;&#20010;&#26377;&#21521;&#26080;&#29615;&#22270;(DAG)&#30340;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2306.17361</link><description>&lt;p&gt;
iSCAN&#65306;&#35782;&#21035;&#38750;&#32447;&#24615;&#21152;&#24615;&#22122;&#22768;&#27169;&#22411;&#20013;&#30340;&#22240;&#26524;&#26426;&#21046;&#36716;&#21464;
&lt;/p&gt;
&lt;p&gt;
iSCAN: Identifying Causal Mechanism Shifts among Nonlinear Additive Noise Models. (arXiv:2306.17361v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17361
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35782;&#21035;&#38750;&#32447;&#24615;&#21152;&#24615;&#22122;&#22768;&#27169;&#22411;&#20013;&#22240;&#26524;&#26426;&#21046;&#36716;&#21464;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#19987;&#27880;&#20110;&#22312;&#30456;&#20851;&#30340;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#20013;&#35782;&#21035;&#21151;&#33021;&#26426;&#21046;&#30340;&#21464;&#21270;&#65292;&#32780;&#19981;&#38656;&#35201;&#20272;&#35745;&#25972;&#20010;&#26377;&#21521;&#26080;&#29615;&#22270;(DAG)&#30340;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;(SCM)&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#20010;&#39046;&#22495;&#65292;&#20197;&#34920;&#31034;&#22797;&#26434;&#31995;&#32479;&#20013;&#21464;&#37327;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#30495;&#27491;&#30340;&#24213;&#23618;&#26377;&#21521;&#26080;&#29615;&#22270;(DAG)&#32467;&#26500;&#36890;&#24120;&#26159;&#26410;&#30693;&#30340;&#65292;&#24182;&#19988;&#20174;&#35266;&#27979;&#25968;&#25454;&#25110;&#24178;&#39044;&#25968;&#25454;&#20013;&#30830;&#23450;&#23427;&#20173;&#28982;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#30446;&#26631;&#26159;&#35782;&#21035;&#30456;&#20851;SCM&#20043;&#38388;&#30340;&#22240;&#26524;&#26426;&#21046;&#30340;&#21464;&#21270;(&#36716;&#21464;)&#32780;&#19981;&#26159;&#24674;&#22797;&#25972;&#20010;&#24213;&#23618;DAG&#32467;&#26500;&#12290;&#20363;&#23376;&#21253;&#25324;&#20998;&#26512;&#20581;&#24247;&#21644;&#30284;&#30151;&#24739;&#32773;&#20043;&#38388;&#30340;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#32467;&#26500;&#21464;&#21270;&#65292;&#25110;&#32773;&#22312;&#19981;&#21516;&#32454;&#32990;&#29615;&#22659;&#19979;&#29702;&#35299;&#29983;&#29289;&#36884;&#24452;&#30340;&#21464;&#21270;&#12290;&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#20102;&#22312;&#30456;&#21516;&#30340;&#21464;&#37327;&#38598;&#19978;&#35782;&#21035;&#20004;&#20010;&#25110;&#22810;&#20010;&#30456;&#20851;SCM&#20013;&#30340;$\textit{&#21151;&#33021;}$&#26426;&#21046;&#36716;&#21464;&#65292;&#32780;&#19981;&#38656;&#35201;&#20272;&#35745;&#27599;&#20010;SCM&#30340;&#25972;&#20010;DAG&#32467;&#26500;&#12290;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#20551;&#35774;&#20351;&#29992;&#20102;&#20855;&#26377;&#39640;&#26031;&#22122;&#22768;&#30340;&#32447;&#24615;&#27169;&#22411;&#65307;&#32780;&#26412;&#25991;&#20013;&#25105;&#20204;&#21017;&#32771;&#34385;&#20102;&#38750;&#32447;&#24615;&#21152;&#24615;&#22122;&#22768;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Structural causal models (SCMs) are widely used in various disciplines to represent causal relationships among variables in complex systems. Unfortunately, the true underlying directed acyclic graph (DAG) structure is often unknown, and determining it from observational or interventional data remains a challenging task. However, in many situations, the end goal is to identify changes (shifts) in causal mechanisms between related SCMs rather than recovering the entire underlying DAG structure. Examples include analyzing gene regulatory network structure changes between healthy and cancerous individuals or understanding variations in biological pathways under different cellular contexts. This paper focuses on identifying $\textit{functional}$ mechanism shifts in two or more related SCMs over the same set of variables -$\textit{without estimating the entire DAG structure of each SCM}$. Prior work under this setting assumed linear models with Gaussian noises; instead, in this work we ass
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#21512;&#20171;&#32461;&#20102;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#22312;&#22825;&#25991;&#23398;&#20013;&#30340;&#24212;&#29992;&#20197;&#21450;&#26395;&#36828;&#38236;&#26234;&#33021;&#21270;&#30340;&#21457;&#23637;&#21644;&#30740;&#31350;&#28909;&#28857;&#65292;&#23545;&#21508;&#31181;&#30740;&#31350;&#26041;&#21521;&#36827;&#34892;&#20102;&#32479;&#35745;&#20998;&#26512;&#65292;&#24182;&#25351;&#20986;&#20102;&#21508;&#20010;&#26395;&#36828;&#38236;&#26234;&#33021;&#21270;&#30340;&#30740;&#31350;&#36235;&#21183;&#12290;</title><link>http://arxiv.org/abs/2306.16834</link><description>&lt;p&gt;
&#22825;&#25991;&#20809;&#23398;&#26395;&#36828;&#38236;&#30340;&#26234;&#33021;&#21270;&#65306;&#29616;&#29366;&#19982;&#26410;&#26469;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Intelligence of Astronomical Optical Telescope: Present Status and Future Perspectives. (arXiv:2306.16834v1 [astro-ph.IM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16834
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#21512;&#20171;&#32461;&#20102;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#22312;&#22825;&#25991;&#23398;&#20013;&#30340;&#24212;&#29992;&#20197;&#21450;&#26395;&#36828;&#38236;&#26234;&#33021;&#21270;&#30340;&#21457;&#23637;&#21644;&#30740;&#31350;&#28909;&#28857;&#65292;&#23545;&#21508;&#31181;&#30740;&#31350;&#26041;&#21521;&#36827;&#34892;&#20102;&#32479;&#35745;&#20998;&#26512;&#65292;&#24182;&#25351;&#20986;&#20102;&#21508;&#20010;&#26395;&#36828;&#38236;&#26234;&#33021;&#21270;&#30340;&#30740;&#31350;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#22312;&#22825;&#25991;&#23398;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#65292;&#19981;&#26029;&#28044;&#29616;&#20986;&#26032;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#21644;&#24212;&#29992;&#22330;&#26223;&#12290;&#30446;&#21069;&#65292;&#26377;&#22823;&#37327;&#30340;&#35770;&#25991;&#22238;&#39038;&#20102;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#22312;&#22825;&#25991;&#23398;&#20013;&#30340;&#24212;&#29992;&#65292;&#28982;&#32780;&#24456;&#23569;&#26377;&#30456;&#20851;&#25991;&#31456;&#21333;&#29420;&#25552;&#21450;&#26395;&#36828;&#38236;&#30340;&#26234;&#33021;&#21270;&#65292;&#24182;&#19988;&#24456;&#38590;&#20174;&#36825;&#20123;&#35770;&#25991;&#20013;&#20102;&#35299;&#21040;&#26395;&#36828;&#38236;&#26234;&#33021;&#21270;&#30340;&#24403;&#21069;&#21457;&#23637;&#29366;&#20917;&#21644;&#30740;&#31350;&#28909;&#28857;&#12290;&#26412;&#25991;&#32467;&#21512;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#21457;&#23637;&#21382;&#21490;&#21644;&#26395;&#36828;&#38236;&#20851;&#38190;&#25216;&#26415;&#30340;&#22256;&#38590;&#65292;&#20840;&#38754;&#20171;&#32461;&#20102;&#26395;&#36828;&#38236;&#26234;&#33021;&#21270;&#30340;&#21457;&#23637;&#21644;&#30740;&#31350;&#28909;&#28857;&#65292;&#28982;&#21518;&#23545;&#26395;&#36828;&#38236;&#26234;&#33021;&#21270;&#30340;&#21508;&#31181;&#30740;&#31350;&#26041;&#21521;&#36827;&#34892;&#20102;&#32479;&#35745;&#20998;&#26512;&#65292;&#24182;&#23450;&#20041;&#20102;&#21508;&#20010;&#30740;&#31350;&#26041;&#21521;&#30340;&#20248;&#28857;&#12290;&#35780;&#20272;&#20102;&#21508;&#31181;&#30740;&#31350;&#26041;&#21521;&#65292;&#24182;&#25351;&#20986;&#20102;&#27599;&#20010;&#26395;&#36828;&#38236;&#26234;&#33021;&#21270;&#30340;&#30740;&#31350;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence technology has been widely used in astronomy, and new artificial intelligence technologies and application scenarios are constantly emerging. There have been a large number of papers reviewing the application of artificial intelligence technology in astronomy. However, relevant articles seldom mention telescope intelligence separately, and it is difficult to understand the current development status and research hotspots of telescope intelligence from these papers. This paper combines the development history of artificial intelligence technology and the difficulties of critical technologies of telescopes, comprehensively introduces the development and research hotspots of telescope intelligence, then conducts statistical analysis on various research directions of telescope intelligence and defines the research directions' merits. All kinds of research directions are evaluated, and the research trend of each telescope's intelligence is pointed out. Finally, accor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;DiffSketcher&#65292;&#19968;&#31181;&#36890;&#36807;&#38544;&#24335;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#25991;&#26412;&#24341;&#23548;&#30340;&#30690;&#37327;&#32032;&#25551;&#21512;&#25104;&#30340;&#21019;&#26032;&#31639;&#27861;&#12290;DiffSketcher&#36890;&#36807;&#30452;&#25509;&#20248;&#21270;&#36125;&#22622;&#23572;&#26354;&#32447;&#21644;&#25193;&#25955;&#27169;&#22411;&#25439;&#22833;&#26469;&#29983;&#25104;&#30690;&#37327;&#21270;&#30340;&#25163;&#32472;&#32032;&#25551;&#65292;&#24182;&#36890;&#36807;&#27880;&#24847;&#21147;&#22270;&#21152;&#24555;&#29983;&#25104;&#36807;&#31243;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;DiffSketcher&#30340;&#32032;&#25551;&#36136;&#37327;&#39640;&#20110;&#20043;&#21069;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.14685</link><description>&lt;p&gt;
DiffSketcher: &#36890;&#36807;&#38544;&#24335;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#25991;&#26412;&#24341;&#23548;&#30340;&#30690;&#37327;&#32032;&#25551;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
DiffSketcher: Text Guided Vector Sketch Synthesis through Latent Diffusion Models. (arXiv:2306.14685v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14685
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;DiffSketcher&#65292;&#19968;&#31181;&#36890;&#36807;&#38544;&#24335;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#25991;&#26412;&#24341;&#23548;&#30340;&#30690;&#37327;&#32032;&#25551;&#21512;&#25104;&#30340;&#21019;&#26032;&#31639;&#27861;&#12290;DiffSketcher&#36890;&#36807;&#30452;&#25509;&#20248;&#21270;&#36125;&#22622;&#23572;&#26354;&#32447;&#21644;&#25193;&#25955;&#27169;&#22411;&#25439;&#22833;&#26469;&#29983;&#25104;&#30690;&#37327;&#21270;&#30340;&#25163;&#32472;&#32032;&#25551;&#65292;&#24182;&#36890;&#36807;&#27880;&#24847;&#21147;&#22270;&#21152;&#24555;&#29983;&#25104;&#36807;&#31243;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;DiffSketcher&#30340;&#32032;&#25551;&#36136;&#37327;&#39640;&#20110;&#20043;&#21069;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20027;&#35201;&#35757;&#32451;&#20110;&#22270;&#20687;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#22312;&#24341;&#23548;&#32032;&#25551;&#21512;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;DiffSketcher&#65292;&#19968;&#31181;&#21019;&#26032;&#30340;&#31639;&#27861;&#65292;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#21019;&#24314;&#30690;&#37327;&#21270;&#30340;&#25163;&#32472;&#32032;&#25551;&#12290;DiffSketcher&#22522;&#20110;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#24320;&#21457;&#65292;&#36890;&#36807;&#20351;&#29992;&#25193;&#23637;&#29256;&#26412;&#30340;&#24471;&#20998;&#33976;&#39311;&#37319;&#26679;&#65288;SDS&#65289;&#25439;&#22833;&#30452;&#25509;&#20248;&#21270;&#19968;&#32452;&#36125;&#22622;&#23572;&#26354;&#32447;&#65292;&#20351;&#24471;&#25105;&#20204;&#21487;&#20197;&#23558;&#26629;&#26684;&#32423;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#20808;&#39564;&#26469;&#20248;&#21270;&#21442;&#25968;&#21270;&#30340;&#30690;&#37327;&#32032;&#25551;&#29983;&#25104;&#22120;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#25193;&#25955;&#27169;&#22411;&#20013;&#23884;&#20837;&#30340;&#27880;&#24847;&#21147;&#22270;&#65292;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#23454;&#29616;&#26377;&#25928;&#30340;&#31508;&#30011;&#21021;&#22987;&#21270;&#20197;&#21152;&#24555;&#36895;&#24230;&#12290;&#29983;&#25104;&#30340;&#32032;&#25551;&#23637;&#31034;&#20102;&#22810;&#23618;&#27425;&#30340;&#25277;&#35937;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#34987;&#32472;&#21046;&#20027;&#39064;&#30340;&#21487;&#35782;&#21035;&#24615;&#12289;&#22522;&#26412;&#32467;&#26500;&#21644;&#37325;&#35201;&#30340;&#35270;&#35273;&#32454;&#33410;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;DiffSketcher&#30340;&#36136;&#37327;&#20248;&#20110;&#20043;&#21069;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Even though trained mainly on images, we discover that pretrained diffusion models show impressive power in guiding sketch synthesis. In this paper, we present DiffSketcher, an innovative algorithm that creates vectorized free-hand sketches using natural language input. DiffSketcher is developed based on a pre-trained text-to-image diffusion model. It performs the task by directly optimizing a set of Bezier curves with an extended version of the score distillation sampling (SDS) loss, which allows us to use a raster-level diffusion model as a prior for optimizing a parametric vectorized sketch generator. Furthermore, we explore attention maps embedded in the diffusion model for effective stroke initialization to speed up the generation process. The generated sketches demonstrate multiple levels of abstraction while maintaining recognizability, underlying structure, and essential visual details of the subject drawn. Our experiments show that DiffSketcher achieves greater quality than pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28176;&#36827;&#33021;&#37327;&#21327;&#20316;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22810;&#22495;&#22270;&#20687;&#21040;&#22270;&#20687;&#30340;&#36716;&#25442;&#12290;&#35813;&#26694;&#26550;&#21253;&#21547;&#25551;&#36848;&#22120;&#12289;&#32763;&#35793;&#22120;&#12289;&#39118;&#26684;&#32534;&#30721;&#22120;&#21644;&#39118;&#26684;&#29983;&#25104;&#22120;&#22235;&#20010;&#32452;&#20214;&#12290;&#36890;&#36807;&#36825;&#31181;&#26694;&#26550;&#65292;&#21487;&#20197;&#23454;&#29616;&#22810;&#26679;&#21270;&#30340;&#22270;&#20687;&#29983;&#25104;&#21644;&#19968;&#23545;&#22810;&#30340;&#36716;&#25442;&#12290;</title><link>http://arxiv.org/abs/2306.14448</link><description>&lt;p&gt;
&#28176;&#36827;&#33021;&#37327;&#21327;&#20316;&#23398;&#20064;&#29992;&#20110;&#22810;&#22495;&#22270;&#20687;&#21040;&#22270;&#20687;&#30340;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
Progressive Energy-Based Cooperative Learning for Multi-Domain Image-to-Image Translation. (arXiv:2306.14448v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14448
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28176;&#36827;&#33021;&#37327;&#21327;&#20316;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22810;&#22495;&#22270;&#20687;&#21040;&#22270;&#20687;&#30340;&#36716;&#25442;&#12290;&#35813;&#26694;&#26550;&#21253;&#21547;&#25551;&#36848;&#22120;&#12289;&#32763;&#35793;&#22120;&#12289;&#39118;&#26684;&#32534;&#30721;&#22120;&#21644;&#39118;&#26684;&#29983;&#25104;&#22120;&#22235;&#20010;&#32452;&#20214;&#12290;&#36890;&#36807;&#36825;&#31181;&#26694;&#26550;&#65292;&#21487;&#20197;&#23454;&#29616;&#22810;&#26679;&#21270;&#30340;&#22270;&#20687;&#29983;&#25104;&#21644;&#19968;&#23545;&#22810;&#30340;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#33021;&#37327;&#30340;&#21512;&#20316;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22810;&#22495;&#22270;&#20687;&#21040;&#22270;&#20687;&#30340;&#36716;&#25442;&#12290;&#35813;&#26694;&#26550;&#30001;&#22235;&#20010;&#32452;&#20214;&#32452;&#25104;&#65306;&#25551;&#36848;&#22120;&#12289;&#32763;&#35793;&#22120;&#12289;&#39118;&#26684;&#32534;&#30721;&#22120;&#21644;&#39118;&#26684;&#29983;&#25104;&#22120;&#12290;&#25551;&#36848;&#22120;&#26159;&#19968;&#20010;&#22810;&#22836;&#33021;&#37327;&#27169;&#22411;&#65292;&#34920;&#31034;&#22810;&#22495;&#22270;&#20687;&#20998;&#24067;&#12290;&#32763;&#35793;&#22120;&#12289;&#39118;&#26684;&#32534;&#30721;&#22120;&#21644;&#39118;&#26684;&#29983;&#25104;&#22120;&#30340;&#32452;&#20214;&#26500;&#25104;&#20102;&#19968;&#20010;&#22810;&#26679;&#21270;&#22270;&#20687;&#29983;&#25104;&#22120;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#32473;&#23450;&#19968;&#20010;&#26469;&#33258;&#28304;&#22495;&#30340;&#36755;&#20837;&#22270;&#20687;&#65292;&#32763;&#35793;&#22120;&#26681;&#25454;&#39118;&#26684;&#20195;&#30721;&#23558;&#20854;&#36716;&#25442;&#20026;&#30446;&#26631;&#22495;&#30340;&#39118;&#26684;&#21270;&#36755;&#20986;&#22270;&#20687;&#65292;&#39118;&#26684;&#20195;&#30721;&#21487;&#20197;&#30001;&#39118;&#26684;&#32534;&#30721;&#22120;&#20174;&#21442;&#32771;&#22270;&#20687;&#25512;&#26029;&#20986;&#25110;&#30001;&#39118;&#26684;&#29983;&#25104;&#22120;&#20174;&#38543;&#26426;&#22122;&#22768;&#29983;&#25104;&#12290;&#30001;&#20110;&#39118;&#26684;&#29983;&#25104;&#22120;&#34987;&#34920;&#31034;&#20026;&#29305;&#23450;&#20110;&#22495;&#30340;&#39118;&#26684;&#20195;&#30721;&#20998;&#24067;&#65292;&#32763;&#35793;&#22120;&#21487;&#20197;&#22312;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#20043;&#38388;&#25552;&#20379;&#19968;&#23545;&#22810;&#30340;&#36716;&#25442;&#65288;&#21363;&#22810;&#26679;&#21270;&#29983;&#25104;&#65289;&#12290;&#20026;&#20102;&#35757;&#32451;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20284;&#28982;&#30340;&#22810;&#22495;&#21512;&#20316;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies a novel energy-based cooperative learning framework for multi-domain image-to-image translation. The framework consists of four components: descriptor, translator, style encoder, and style generator. The descriptor is a multi-head energy-based model that represents a multi-domain image distribution. The components of translator, style encoder, and style generator constitute a diversified image generator. Specifically, given an input image from a source domain, the translator turns it into a stylised output image of the target domain according to a style code, which can be inferred by the style encoder from a reference image or produced by the style generator from a random noise. Since the style generator is represented as an domain-specific distribution of style codes, the translator can provide a one-to-many transformation (i.e., diversified generation) between source domain and target domain. To train our framework, we propose a likelihood-based multi-domain cooper
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TMR&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#20174;&#27169;&#25311;&#25968;&#25454;&#29615;&#22659;&#20013;&#25366;&#25496;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2306.10345</link><description>&lt;p&gt;
&#20570;&#25105;&#33021;&#20570;&#30340;&#65292;&#32780;&#19981;&#26159;&#25105;&#24471;&#21040;&#30340;&#12290;(arXiv:2306.10345v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
Do as I can, not as I get. (arXiv:2306.10345v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10345
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TMR&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#20174;&#27169;&#25311;&#25968;&#25454;&#29615;&#22659;&#20013;&#25366;&#25496;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TMR&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#20174;&#27169;&#25311;&#25968;&#25454;&#29615;&#22659;&#20013;&#25366;&#25496;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#25171;&#31639;&#23436;&#25104;&#26412;&#25991;&#30340;&#25237;&#31295;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a model called TMR to mine valuable information from simulated data environments. We intend to complete the submission of this paper.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#21160;&#24577;&#25511;&#21046;&#27531;&#24046;&#30340; Q &#23398;&#20064;&#26469;&#36827;&#34892;&#31163;&#32447;&#21644;&#22312;&#32447;&#30340;&#31574;&#30053;&#23450;&#21046;&#65292;&#26080;&#38656;&#20351;&#29992;&#20215;&#20540;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2306.09526</link><description>&lt;p&gt;
&#27531;&#24046; Q &#23398;&#20064;&#65306;&#26080;&#38656;&#20215;&#20540;&#30340;&#22312;&#32447;&#21644;&#31163;&#32447;&#31574;&#30053;&#23450;&#21046;
&lt;/p&gt;
&lt;p&gt;
Residual Q-Learning: Offline and Online Policy Customization without Value. (arXiv:2306.09526v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09526
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#21160;&#24577;&#25511;&#21046;&#27531;&#24046;&#30340; Q &#23398;&#20064;&#26469;&#36827;&#34892;&#31163;&#32447;&#21644;&#22312;&#32447;&#30340;&#31574;&#30053;&#23450;&#21046;&#65292;&#26080;&#38656;&#20351;&#29992;&#20215;&#20540;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#20223;&#23398;&#20064;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#26694;&#26550;&#65292;&#36866;&#29992;&#20110;&#20174;&#28436;&#31034;&#20013;&#23398;&#20064;&#27169;&#20223;&#34892;&#20026;&#12290;&#24403;&#25163;&#24037;&#21046;&#20316;&#22870;&#21169;&#20989;&#25968;&#22256;&#38590;&#25110;&#30446;&#26631;&#26159;&#27169;&#20223;&#20154;&#31867;&#19987;&#23478;&#34892;&#20026;&#26102;&#65292;&#36825;&#31181;&#26041;&#27861;&#29305;&#21035;&#26377;&#21560;&#24341;&#21147;&#12290;&#20294;&#26159;&#65292;&#23398;&#20064;&#30340;&#27169;&#20223;&#31574;&#30053;&#21482;&#33021;&#36981;&#24490;&#28436;&#31034;&#20013;&#30340;&#34892;&#20026;&#12290;&#22312;&#24212;&#29992;&#27169;&#20223;&#31574;&#30053;&#26102;&#65292;&#25105;&#20204;&#21487;&#33021;&#38656;&#35201;&#26681;&#25454;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#35201;&#27714;&#23450;&#21046;&#31574;&#30053;&#34892;&#20026;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#20173;&#24076;&#26395;&#23450;&#21046;&#30340;&#31574;&#30053;&#20445;&#25345;&#20854;&#27169;&#20223;&#24615;&#36136;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38382;&#39064;&#35774;&#32622;&#65292;&#31216;&#20026;&#31574;&#30053;&#23450;&#21046;&#12290;&#23427;&#23558;&#23398;&#20064;&#20219;&#21153;&#23450;&#20041;&#20026;&#35757;&#32451;&#19968;&#31181;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#32487;&#25215;&#20808;&#21069;&#31574;&#30053;&#30340;&#29305;&#24615;&#65292;&#21516;&#26102;&#28385;&#36275;&#30446;&#26631;&#19979;&#28216;&#20219;&#21153;&#24378;&#21152;&#30340;&#19968;&#20123;&#38468;&#21152;&#35201;&#27714;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#21644;&#26377;&#21407;&#21017;&#30340;&#26041;&#27861;&#26469;&#35299;&#37322;&#21644;&#30830;&#23450;&#20004;&#20010;&#20219;&#21153;&#30446;&#26631;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#31181;&#21160;&#24577;&#25511;&#21046;&#27531;&#24046;&#30340; Q &#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#19981;&#20351;&#29992;&#20215;&#20540;&#20989;&#25968;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#22312;&#32447;&#21644;&#31163;&#32447;&#31574;&#30053;&#23450;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imitation Learning (IL) is a widely used framework for learning imitative behavior from demonstrations. It is especially appealing for solving complex real-world tasks where handcrafting reward function is difficult, or when the goal is to mimic human expert behavior. However, the learned imitative policy can only follow the behavior in the demonstration. When applying the imitative policy, we may need to customize the policy behavior to meet different requirements coming from diverse downstream tasks. Meanwhile, we still want the customized policy to maintain its imitative nature. To this end, we formulate a new problem setting called policy customization. It defines the learning task as training a policy that inherits the characteristics of the prior policy while satisfying some additional requirements imposed by a target downstream task. We propose a novel and principled approach to interpret and determine the trade-off between the two task objectives. Specifically, we formulate the
&lt;/p&gt;</description></item><item><title>ShiftAddViT&#36890;&#36807;&#20351;&#29992;&#20301;&#31227;&#21644;&#21152;&#27861;&#31561;&#22810;&#31181;&#20056;&#27861;&#21407;&#35821;&#23545;ViT&#36827;&#34892;&#37325;&#26032;&#21442;&#25968;&#21270;&#65292;&#23454;&#29616;&#20102;&#20943;&#23569;&#20056;&#27861;&#25805;&#20316;&#30340;&#39640;&#25928;&#35270;&#35273;&#21464;&#25442;&#22120;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;GPU&#19978;&#23454;&#29616;&#31471;&#21040;&#31471;&#30340;&#25512;&#29702;&#21152;&#36895;&#65292;&#26080;&#38656;&#20174;&#22836;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2306.06446</link><description>&lt;p&gt;
ShiftAddViT&#65306;&#22810;&#31181;&#20056;&#27861;&#21407;&#35821;&#28151;&#21512;&#23454;&#29616;&#39640;&#25928;&#30340;&#35270;&#35273;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
ShiftAddViT: Mixture of Multiplication Primitives Towards Efficient Vision Transformer. (arXiv:2306.06446v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06446
&lt;/p&gt;
&lt;p&gt;
ShiftAddViT&#36890;&#36807;&#20351;&#29992;&#20301;&#31227;&#21644;&#21152;&#27861;&#31561;&#22810;&#31181;&#20056;&#27861;&#21407;&#35821;&#23545;ViT&#36827;&#34892;&#37325;&#26032;&#21442;&#25968;&#21270;&#65292;&#23454;&#29616;&#20102;&#20943;&#23569;&#20056;&#27861;&#25805;&#20316;&#30340;&#39640;&#25928;&#35270;&#35273;&#21464;&#25442;&#22120;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;GPU&#19978;&#23454;&#29616;&#31471;&#21040;&#31471;&#30340;&#25512;&#29702;&#21152;&#36895;&#65292;&#26080;&#38656;&#20174;&#22836;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#21464;&#25442;&#22120;&#65288;ViT&#65289;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#65292;&#24182;&#25104;&#20026;&#22810;&#20010;&#35270;&#35273;&#20219;&#21153;&#30340;&#32479;&#19968;&#39592;&#24178;&#12290;&#20294;&#26159;&#65292;ViTs&#20013;&#30340;&#27880;&#24847;&#21147;&#21644;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLPs&#65289;&#30001;&#20110;&#23494;&#38598;&#30340;&#20056;&#27861;&#32780;&#19981;&#22815;&#39640;&#25928;&#65292;&#23548;&#33268;&#35757;&#32451;&#21644;&#25512;&#29702;&#20195;&#20215;&#39640;&#26114;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#39044;&#35757;&#32451;&#30340;ViT&#20197;&#22810;&#31181;&#20056;&#27861;&#21407;&#35821;&#65288;&#20363;&#22914;&#20301;&#31227;&#21644;&#21152;&#27861;&#65289;&#37325;&#26032;&#21442;&#25968;&#21270;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#20840;&#26032;&#31867;&#22411;&#30340;&#20943;&#23569;&#20056;&#27861;&#30340;&#27169;&#22411;&#65292;&#31216;&#20026;ShiftAddViT&#65292;&#26088;&#22312;&#23454;&#29616;GPU&#19978;&#30340;&#31471;&#21040;&#31471;&#25512;&#29702;&#21152;&#36895;&#65292;&#26080;&#38656;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#26597;&#35810;&#21644;&#38190;&#26144;&#23556;&#20026;&#27721;&#26126;&#31354;&#38388;&#20013;&#30340;&#20108;&#36827;&#21046;&#30721;&#20043;&#21518;&#65292;&#37319;&#29992;&#21152;&#27861;&#26680;&#23545;&#26597;&#35810;&#12289;&#38190;&#21644;&#20540;&#20043;&#38388;&#30340;MatMul&#36827;&#34892;&#37325;&#26032;&#21442;&#25968;&#21270;&#12290;&#21097;&#20313;&#30340;MLPs&#25110;&#32447;&#24615;&#23618;&#21017;&#37319;&#29992;&#20301;&#31227;&#26680;&#36827;&#34892;&#37325;&#26032;&#21442;&#25968;&#21270;&#12290;&#25105;&#20204;&#21033;&#29992;TVM&#22312;GPU&#19978;&#23454;&#26045;&#24182;&#20248;&#21270;&#36825;&#20123;&#23450;&#21046;&#26680;&#65292;&#20197;&#23454;&#29616;&#23454;&#38469;&#30828;&#20214;&#37096;&#32626;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36825;&#31181;&#37325;&#26032;&#21442;&#25968;&#21270;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25512;&#29702;&#36895;&#24230;&#65292;&#32780;&#26080;&#38656;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision Transformers (ViTs) have shown impressive performance and have become a unified backbone for multiple vision tasks. But both attention and multi-layer perceptions (MLPs) in ViTs are not efficient enough due to dense multiplications, resulting in costly training and inference. To this end, we propose to reparameterize the pre-trained ViT with a mixture of multiplication primitives, e.g., bitwise shifts and additions, towards a new type of multiplication-reduced model, dubbed $\textbf{ShiftAddViT}$, which aims for end-to-end inference speedups on GPUs without the need of training from scratch. Specifically, all $\texttt{MatMuls}$ among queries, keys, and values are reparameterized by additive kernels, after mapping queries and keys to binary codes in Hamming space. The remaining MLPs or linear layers are then reparameterized by shift kernels. We utilize TVM to implement and optimize those customized kernels for practical hardware deployment on GPUs. We find that such a reparameter
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21019;&#24314;&#20102;&#24847;&#22823;&#21033;&#31070;&#32463;&#31934;&#31070;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#24040;&#22411;&#35821;&#35328;&#27169;&#22411;&#24320;&#21457;&#20986;&#22810;&#20013;&#24515;&#35782;&#21035;&#27169;&#22411;&#65292;&#25972;&#20307; F1&#24471;&#20998;&#20026;84.77%&#12290;&#35813;&#27169;&#22411;&#23558;&#24110;&#21161;&#20020;&#24202;&#20174;&#19994;&#32773;&#20174;&#38750;&#32467;&#26500;&#21270;&#30340;&#21307;&#30103;&#35760;&#24405;&#20013;&#33258;&#21160;&#25552;&#21462;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2306.05323</link><description>&lt;p&gt;
&#24040;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24847;&#22823;&#21033;&#29983;&#29289;&#21307;&#23398;&#20449;&#24687;&#25552;&#21462;&#26041;&#38754;&#30340;&#24212;&#29992;&#65306;&#26041;&#27861;&#35770;&#30740;&#31350;&#21644;&#23454;&#38469;&#24212;&#29992;&#30340;&#22810;&#20013;&#24515;&#23454;&#36341;
&lt;/p&gt;
&lt;p&gt;
Advancing Italian Biomedical Information Extraction with Large Language Models: Methodological Insights and Multicenter Practical Application. (arXiv:2306.05323v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05323
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21019;&#24314;&#20102;&#24847;&#22823;&#21033;&#31070;&#32463;&#31934;&#31070;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#24040;&#22411;&#35821;&#35328;&#27169;&#22411;&#24320;&#21457;&#20986;&#22810;&#20013;&#24515;&#35782;&#21035;&#27169;&#22411;&#65292;&#25972;&#20307; F1&#24471;&#20998;&#20026;84.77%&#12290;&#35813;&#27169;&#22411;&#23558;&#24110;&#21161;&#20020;&#24202;&#20174;&#19994;&#32773;&#20174;&#38750;&#32467;&#26500;&#21270;&#30340;&#21307;&#30103;&#35760;&#24405;&#20013;&#33258;&#21160;&#25552;&#21462;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#38498;&#24341;&#20837;&#35745;&#31639;&#26426;&#21270;&#21307;&#30103;&#35760;&#24405;&#26377;&#21161;&#20110;&#20943;&#23569;&#25163;&#20889;&#21644;&#20449;&#24687;&#25552;&#21462;&#31561;&#32321;&#29712;&#25805;&#20316;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#21307;&#30103;&#35760;&#24405;&#20013;&#25552;&#21462;&#25968;&#25454;&#38656;&#35201;&#26102;&#38388;&#21644;&#31934;&#21147;&#65292;&#22240;&#27492;&#21307;&#30103;&#35760;&#24405;&#20013;&#21253;&#21547;&#30340;&#25968;&#25454;&#20173;&#28982;&#34987;&#20805;&#20998;&#21033;&#29992;&#31243;&#24230;&#20302;&#12290;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#23376;&#39046;&#22495;&#20449;&#24687;&#25552;&#21462;&#21487;&#20197;&#24110;&#21161;&#20020;&#24202;&#20174;&#19994;&#32773;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#20351;&#29992;&#33258;&#21160;&#21270;&#25991;&#26412;&#25366;&#25496;&#27969;&#31243;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#24847;&#22823;&#21033;&#31070;&#32463;&#31934;&#31070;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#25968;&#25454;&#38598; PsyNIT&#65292;&#24182;&#20351;&#29992;&#23427;&#26469;&#24320;&#21457;&#36825;&#19968;&#20219;&#21153;&#30340;&#24040;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#22810;&#20010;&#23454;&#39564;&#65292;&#20351;&#29992;&#19977;&#20010;&#22806;&#37096;&#29420;&#31435;&#25968;&#25454;&#38598;&#26469;&#23454;&#29616;&#26377;&#25928;&#30340;&#22810;&#20013;&#24515;&#27169;&#22411;&#65292;&#25972;&#20307; F1 &#24471;&#20998;&#20026; 84.77%&#65292;&#31934;&#30830;&#29575;&#20026; 83.16%&#65292;&#21484;&#22238;&#29575;&#20026; 86.44%&#12290;&#25105;&#20204;&#23398;&#21040;&#30340;&#32463;&#39564;&#26159;: (i) &#19968;&#33268;&#30340;&#27880;&#37322;&#36807;&#31243;&#30340;&#20851;&#38190;&#20316;&#29992;&#21644; (ii) &#32467;&#21512;&#32463;&#20856;&#26041;&#27861;&#21644;&#8220;&#23569;&#37327;&#35757;&#32451;&#8221;&#30340; fine-tuning &#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
The introduction of computerized medical records in hospitals has reduced burdensome operations like manual writing and information fetching. However, the data contained in medical records are still far underutilized, primarily because extracting them from unstructured textual medical records takes time and effort. Information Extraction, a subfield of Natural Language Processing, can help clinical practitioners overcome this limitation, using automated text-mining pipelines. In this work, we created the first Italian neuropsychiatric Named Entity Recognition dataset, PsyNIT, and used it to develop a Large Language Model for this task. Moreover, we conducted several experiments with three external independent datasets to implement an effective multicenter model, with overall F1-score 84.77%, Precision 83.16%, Recall 86.44%. The lessons learned are: (i) the crucial role of a consistent annotation process and (ii) a fine-tuning strategy that combines classical methods with a "few-shot" a
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20687;&#32032;&#32423;&#20132;&#20114;&#30340;&#22810;&#30446;&#26631;&#35270;&#39057;&#29983;&#25104;&#26041;&#27861;&#65292;&#33021;&#22815;&#29983;&#25104;&#36924;&#30495;&#30340;&#29289;&#20307;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#35270;&#39057;&#65292;&#19988;&#33021;&#20934;&#30830;&#36319;&#38543;&#29992;&#25143;&#30340;&#25511;&#21046;&#65292;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#35270;&#39057;&#29983;&#25104;&#20808;&#21069;&#24037;&#20316;&#30456;&#23218;&#32654;&#29978;&#33267;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.03988</link><description>&lt;p&gt;
&#23398;&#20064;&#25105;&#20204;&#33021;&#22815;&#25484;&#25569;&#30340;&#21147;&#37327;&#65306;&#22522;&#20110;&#20687;&#32032;&#32423;&#20132;&#20114;&#30340;&#22810;&#30446;&#26631;&#35270;&#39057;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Learn the Force We Can: Multi-Object Video Generation from Pixel-Level Interactions. (arXiv:2306.03988v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03988
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20687;&#32032;&#32423;&#20132;&#20114;&#30340;&#22810;&#30446;&#26631;&#35270;&#39057;&#29983;&#25104;&#26041;&#27861;&#65292;&#33021;&#22815;&#29983;&#25104;&#36924;&#30495;&#30340;&#29289;&#20307;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#35270;&#39057;&#65292;&#19988;&#33021;&#20934;&#30830;&#36319;&#38543;&#29992;&#25143;&#30340;&#25511;&#21046;&#65292;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#35270;&#39057;&#29983;&#25104;&#20808;&#21069;&#24037;&#20316;&#30456;&#23218;&#32654;&#29978;&#33267;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#36890;&#36807;&#21333;&#24103;&#22270;&#20687;&#19982;&#31232;&#30095;&#36816;&#21160;&#36755;&#20837;&#26469;&#33258;&#25105;&#22238;&#24402;&#29983;&#25104;&#35270;&#39057;&#12290;&#25105;&#20204;&#35757;&#32451;&#30340;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#36924;&#30495;&#30340;&#29289;&#20307;&#38388;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#22312;&#20165;&#35266;&#27979;&#21040;&#23427;&#20204;&#22312;&#30456;&#20851;&#36816;&#21160;&#27963;&#21160;&#19979;&#26102;&#20998;&#31163;&#22810;&#20010;&#29289;&#20307;&#30340;&#21160;&#24577;&#21644;&#33539;&#22260;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20851;&#38190;&#32452;&#20214;&#26159;&#38543;&#26426;&#21270;&#26465;&#20214;&#26041;&#26696;&#12289;&#36755;&#20837;&#36816;&#21160;&#25511;&#21046;&#30340;&#32534;&#30721;&#20197;&#21450;&#38543;&#26426;&#21270;&#21644;&#31232;&#30095;&#37319;&#26679;&#26469;&#25171;&#30772;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#31216;&#20043;&#20026;YODA&#30340;&#27169;&#22411;&#20855;&#26377;&#33021;&#22815;&#31227;&#21160;&#29289;&#20307;&#32780;&#26080;&#38656;&#23454;&#38469;&#35302;&#25720;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#23450;&#37327;&#21644;&#23450;&#24615;&#22320;&#23637;&#31034;&#65292;YODA&#33021;&#22815;&#20934;&#30830;&#36319;&#38543;&#29992;&#25143;&#30340;&#25511;&#21046;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21576;&#29616;&#20986;&#19982;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#35270;&#39057;&#29983;&#25104;&#20808;&#21069;&#24037;&#20316;&#30456;&#23218;&#32654;&#29978;&#33267;&#26356;&#22909;&#30340;&#35270;&#39057;&#36136;&#37327;&#12290;&#35814;&#24773;&#35831;&#21442;&#38405;&#25105;&#20204;&#30340;&#39033;&#30446;&#32593;&#31449; https://araachie.github.io/yoda&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel unsupervised method to autoregressively generate videos from a single frame and a sparse motion input. Our trained model can generate realistic object-to-object interactions and separate the dynamics and the extents of multiple objects despite only observing them under correlated motion activities. Key components in our method are the randomized conditioning scheme, the encoding of the input motion control, and the randomized and sparse sampling to break correlations. Our model, which we call YODA, has the ability to move objects without physically touching them. We show both qualitatively and quantitatively that YODA accurately follows the user control, while yielding a video quality that is on par with or better than state of the art video generation prior work on several datasets. For videos, visit our project website https://araachie.github.io/yoda.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#21160;&#24577;&#35268;&#21010;&#26041;&#27861;&#26469;&#20248;&#21270;&#20219;&#20309;&#32452;&#21512;&#30340;&#21487;&#20998;&#31163;&#30446;&#26631;&#21644;&#32422;&#26463;&#26465;&#20214;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#27604;&#36890;&#29992;&#27714;&#35299;&#22120;&#34920;&#29616;&#24471;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.19706</link><description>&lt;p&gt;
&#21487;&#20998;&#30446;&#26631;&#30340;&#26368;&#20248;&#20915;&#31574;&#26641;&#65306;&#25512;&#21160;&#21160;&#24577;&#35268;&#21010;&#30340;&#26497;&#38480;
&lt;/p&gt;
&lt;p&gt;
Optimal Decision Trees for Separable Objectives: Pushing the Limits of Dynamic Programming. (arXiv:2305.19706v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19706
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#21160;&#24577;&#35268;&#21010;&#26041;&#27861;&#26469;&#20248;&#21270;&#20219;&#20309;&#32452;&#21512;&#30340;&#21487;&#20998;&#31163;&#30446;&#26631;&#21644;&#32422;&#26463;&#26465;&#20214;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#27604;&#36890;&#29992;&#27714;&#35299;&#22120;&#34920;&#29616;&#24471;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#26641;&#30340;&#20840;&#23616;&#20248;&#21270;&#22312;&#20934;&#30830;&#24615;&#65292;&#22823;&#23567;&#21644;&#20154;&#31867;&#21487;&#29702;&#35299;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#21069;&#26223;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#26041;&#27861;&#20173;&#28982;&#20381;&#36182;&#20110;&#36890;&#29992;&#27714;&#35299;&#22120;&#65292;&#21487;&#25193;&#23637;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#38382;&#39064;&#12290;&#21160;&#24577;&#35268;&#21010;&#26041;&#27861;&#24050;&#34987;&#35777;&#26126;&#20855;&#26377;&#26356;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#36807;&#23558;&#23376;&#26641;&#20316;&#20026;&#29420;&#31435;&#30340;&#23376;&#38382;&#39064;&#35299;&#20915;&#26469;&#21033;&#29992;&#26641;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#36825;&#20165;&#36866;&#29992;&#20110;&#21487;&#20197;&#20998;&#21035;&#20248;&#21270;&#23376;&#26641;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#35814;&#32454;&#30740;&#31350;&#20102;&#36825;&#31181;&#20851;&#31995;&#65292;&#24182;&#23637;&#31034;&#20102;&#23454;&#29616;&#36825;&#31181;&#21487;&#20998;&#31163;&#32422;&#26463;&#21644;&#30446;&#26631;&#20219;&#24847;&#32452;&#21512;&#30340;&#21160;&#24577;&#35268;&#21010;&#26041;&#27861;&#12290;&#22312;&#22235;&#20010;&#24212;&#29992;&#39046;&#22495;&#30340;&#23454;&#39564;&#34920;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26222;&#36866;&#24615;&#65292;&#21516;&#26102;&#20063;&#27604;&#36890;&#29992;&#27714;&#35299;&#22120;&#20855;&#26377;&#26356;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Global optimization of decision trees has shown to be promising in terms of accuracy, size, and consequently human comprehensibility. However, many of the methods used rely on general-purpose solvers for which scalability remains an issue. Dynamic programming methods have been shown to scale much better because they exploit the tree structure by solving subtrees as independent subproblems. However, this only works when an objective can be optimized separately for subtrees. We explore this relationship in detail and show necessary and sufficient conditions for such separability and generalize previous dynamic programming approaches into a framework that can optimize any combination of separable objectives and constraints. Experiments on four application domains show the general applicability of this framework, while outperforming the scalability of general-purpose solvers by a large margin.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#35821;&#38899;&#19978;&#19979;&#25991;&#23545;&#30495;&#23454;&#35828;&#35805;&#20154;&#33080;&#29983;&#25104;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;Context-Aware Lip-Sync&#26694;&#26550;&#65288;CALS&#65289;&#65292;&#21487;&#21033;&#29992;&#35821;&#38899;&#19978;&#19979;&#25991;&#29983;&#25104;&#26356;&#21152;&#20934;&#30830;&#12289;&#31283;&#23450;&#30340;&#21767;&#37096;&#36816;&#21160;&#12290;</title><link>http://arxiv.org/abs/2305.19556</link><description>&lt;p&gt;
&#25506;&#32034;&#21767;&#37096;&#36816;&#21160;&#30340;&#35821;&#38899;&#19978;&#19979;&#25991;&#23545;&#30495;&#23454;&#35828;&#35805;&#20154;&#33080;&#29983;&#25104;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Exploring Phonetic Context in Lip Movement for Authentic Talking Face Generation. (arXiv:2305.19556v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19556
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#35821;&#38899;&#19978;&#19979;&#25991;&#23545;&#30495;&#23454;&#35828;&#35805;&#20154;&#33080;&#29983;&#25104;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;Context-Aware Lip-Sync&#26694;&#26550;&#65288;CALS&#65289;&#65292;&#21487;&#21033;&#29992;&#35821;&#38899;&#19978;&#19979;&#25991;&#29983;&#25104;&#26356;&#21152;&#20934;&#30830;&#12289;&#31283;&#23450;&#30340;&#21767;&#37096;&#36816;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35828;&#35805;&#20154;&#33080;&#29983;&#25104;&#26159;&#23558;&#33258;&#28982;&#38754;&#37096;&#19982;&#39537;&#21160;&#38899;&#39057;&#21516;&#27493;&#21512;&#25104;&#30340;&#20219;&#21153;&#12290;&#23613;&#31649;&#22312;&#35270;&#35273;&#36136;&#37327;&#12289;&#21767;&#24418;&#21516;&#27493;&#21644;&#38754;&#37096;&#21160;&#20316;&#26041;&#38754;&#21462;&#24471;&#20102;&#24456;&#22823;&#36827;&#23637;&#65292;&#20294;&#24403;&#21069;&#30340;&#30740;&#31350;&#20173;&#28982;&#38590;&#20197;&#35299;&#20915;&#31895;&#31961;&#21644;&#24322;&#27493;&#30340;&#21767;&#37096;&#36816;&#21160;&#38382;&#39064;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#31867;&#20284;&#26408;&#20598;&#21160;&#30011;&#30340;&#25928;&#26524;&#12290;&#26412;&#25991;&#21457;&#29616;&#65292;&#20197;&#24448;&#30340;&#20316;&#21697;&#36890;&#24120;&#23558;&#21767;&#37096;&#36816;&#21160;&#19982;&#38899;&#39057;&#22312;&#19981;&#21516;&#30340;&#38899;&#32032;&#32423;&#21035;&#19978;&#36827;&#34892;&#30456;&#20851;&#32852;&#65292;&#28982;&#32780;&#65292;&#30001;&#20110;&#38899;&#32032;&#20043;&#38388;&#30340;&#21327;&#21516;&#21457;&#38899;&#65288;co-articulation&#65289;&#29616;&#35937;&#65292;&#21363;&#38548;&#31163;&#30340;&#38899;&#32032;&#21463;&#21069;&#19968;&#20010;&#25110;&#19979;&#19968;&#20010;&#38899;&#32032;&#30340;&#24433;&#21709;&#65292;&#22240;&#27492;&#21516;&#19968;&#20010;&#38899;&#32032;&#30340;&#21457;&#38899;&#22240;&#38899;&#32032;&#19978;&#19979;&#25991;&#32780;&#24322;&#12290;&#22240;&#27492;&#65292;&#20351;&#29992;&#38899;&#32032;&#19978;&#19979;&#25991;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#26356;&#21152;&#31354;&#38388;&#21644;&#26102;&#38388;&#19978;&#23545;&#40784;&#12289;&#31283;&#23450;&#30340;&#21767;&#37096;&#36816;&#21160;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21767;&#37096;&#36816;&#21160;&#20013;&#30340;&#35821;&#38899;&#19978;&#19979;&#25991;&#23545;&#20110;&#30495;&#23454;&#35828;&#35805;&#20154;&#33080;&#29983;&#25104;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;Context-Aware Lip-Sync&#26694;&#26550;&#65288;CALS&#65289;&#65292;&#21033;&#29992;&#35821;&#38899;&#19978;&#19979;&#25991;&#29983;&#25104;&#26356;&#21152;&#31354;&#38388;&#21644;&#26102;&#38388;&#19978;&#23545;&#40784;&#12289;&#31283;&#23450;&#30340;&#21767;&#37096;&#36816;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Talking face generation is the task of synthesizing a natural face synchronous to driving audio. Although much progress has been made in terms of visual quality, lip synchronization, and facial motion of the talking face, current works still struggle to overcome issues of crude and asynchronous lip movement, which can result in puppetry-like animation. We identify that the prior works commonly correlate lip movement with audio at the phone level. However, due to co-articulation, where an isolated phone is influenced by the preceding or following phones, the articulation of a phone varies upon the phonetic context. Therefore, modeling lip motion with the phonetic context can generate more spatio-temporally aligned and stable lip movement. In this respect, we investigate the phonetic context in lip motion for authentic talking face generation. We propose a Context-Aware Lip-Sync framework (CALS), which leverages phonetic context to generate more spatio-temporally aligned and stable lip m
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#31526;&#21495;&#25216;&#26415;&#30340;&#21487;&#35270;&#21270;&#32534;&#31243;&#20219;&#21153;&#21512;&#25104;&#26041;&#27861;NeurTaskSyn&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#38024;&#23545;&#35268;&#33539;&#20013;&#32473;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#20195;&#30721;&#25152;&#38656;&#35201;&#30340;&#32534;&#31243;&#27010;&#24565;&#21644;&#23545;&#21487;&#35270;&#21270;&#20219;&#21153;&#30340;&#38480;&#21046;&#65292;&#33258;&#21160;&#29983;&#25104;&#32534;&#31243;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.18342</link><description>&lt;p&gt;
&#21487;&#35270;&#21270;&#32534;&#31243;&#20013;&#31070;&#32463;&#20219;&#21153;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Neural Task Synthesis for Visual Programming. (arXiv:2305.18342v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18342
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#31526;&#21495;&#25216;&#26415;&#30340;&#21487;&#35270;&#21270;&#32534;&#31243;&#20219;&#21153;&#21512;&#25104;&#26041;&#27861;NeurTaskSyn&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#38024;&#23545;&#35268;&#33539;&#20013;&#32473;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#20195;&#30721;&#25152;&#38656;&#35201;&#30340;&#32534;&#31243;&#27010;&#24565;&#21644;&#23545;&#21487;&#35270;&#21270;&#20219;&#21153;&#30340;&#38480;&#21046;&#65292;&#33258;&#21160;&#29983;&#25104;&#32534;&#31243;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21512;&#25104;&#26032;&#30340;&#20869;&#23481;&#65292;&#29983;&#25104;&#24335;&#31070;&#32463;&#27169;&#22411;&#22312;&#22686;&#24378;&#32534;&#31243;&#25945;&#32946;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#26088;&#22312;&#35774;&#35745;&#31070;&#32463;&#27169;&#22411;&#65292;&#33021;&#22815;&#26681;&#25454;&#21487;&#35270;&#21270;&#32534;&#31243;&#29615;&#22659;&#19979;&#32473;&#23450;&#30340;&#35268;&#33539;&#33258;&#21160;&#29983;&#25104;&#32534;&#31243;&#20219;&#21153;&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;&#20687; GPT-4 &#36825;&#26679;&#30340;&#22823;&#22411;&#29983;&#25104;&#27169;&#22411;&#33719;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#25105;&#20204;&#30340;&#21021;&#27493;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#21512;&#25104;&#21487;&#35270;&#21270;&#32534;&#31243;&#20219;&#21153;&#26041;&#38754;&#25928;&#26524;&#19981;&#20339;&#65292;&#24182;&#19988;&#22312;&#36923;&#36753;&#21644;&#31354;&#38388;&#25512;&#29702;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#31526;&#21495;&#25216;&#26415; NeurTaskSyn&#65292;&#35813;&#25216;&#26415;&#33021;&#22815;&#38024;&#23545;&#35268;&#33539;&#20013;&#32473;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#20195;&#30721;&#25152;&#38656;&#35201;&#30340;&#32534;&#31243;&#27010;&#24565;&#21644;&#23545;&#21487;&#35270;&#21270;&#20219;&#21153;&#30340;&#38480;&#21046;&#65292;&#21512;&#25104;&#32534;&#31243;&#20219;&#21153;&#12290;NeurTaskSyn &#30001;&#20004;&#20010;&#37096;&#20998;&#26500;&#25104;&#65306;&#31532;&#19968;&#20010;&#37096;&#20998;&#36890;&#36807;&#27169;&#20223;&#23398;&#20064;&#31243;&#24207;&#36827;&#34892;&#35757;&#32451;&#65292;&#29983;&#25104;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#20195;&#30721;&#65292;&#31532;&#20108;&#20010;&#37096;&#20998;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#31243;&#24207;&#36827;&#34892;&#35757;&#32451;&#65292;&#25351;&#23548;&#24213;&#23618;&#31526;&#21495;&#25191;&#34892;&#24341;&#25806;&#29983;&#25104;&#21487;&#35270;&#21270;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative neural models hold great promise in enhancing programming education by synthesizing new content for students. We seek to design neural models that can automatically generate programming tasks for a given specification in the context of visual programming domains. Despite the recent successes of large generative models like GPT-4, our initial results show that these models are ineffective in synthesizing visual programming tasks and struggle with logical and spatial reasoning. We propose a novel neuro-symbolic technique, NeurTaskSyn, that can synthesize programming tasks for a specification given in the form of desired programming concepts exercised by its solution code and constraints on the visual task. NeurTaskSyn has two components: the first component is trained via imitation learning procedure to generate possible solution codes, and the second component is trained via reinforcement learning procedure to guide an underlying symbolic execution engine that generates visua
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22810;&#21333;&#20301;&#25293;&#21334;&#20013;&#23398;&#20064;&#21644;&#21246;&#32467;&#30340;&#38382;&#39064;&#65292;&#20998;&#26512;&#20102;&#25293;&#21334;&#30340;&#31163;&#32447;&#21644;&#22312;&#32447;&#24615;&#36136;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#26469;&#35299;&#20915;&#31163;&#32447;&#35774;&#32622;&#20013;&#30340;&#29609;&#23478;&#20986;&#20215;&#26368;&#22823;&#21270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.17402</link><description>&lt;p&gt;
&#23398;&#20064;&#19982;&#21246;&#32467;&#22312;&#22810;&#21333;&#20301;&#25293;&#21334;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Learning and Collusion in Multi-unit Auctions. (arXiv:2305.17402v2 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17402
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22810;&#21333;&#20301;&#25293;&#21334;&#20013;&#23398;&#20064;&#21644;&#21246;&#32467;&#30340;&#38382;&#39064;&#65292;&#20998;&#26512;&#20102;&#25293;&#21334;&#30340;&#31163;&#32447;&#21644;&#22312;&#32447;&#24615;&#36136;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#26469;&#35299;&#20915;&#31163;&#32447;&#35774;&#32622;&#20013;&#30340;&#29609;&#23478;&#20986;&#20215;&#26368;&#22823;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#37325;&#22797;&#30340;&#22810;&#21333;&#20301;&#25293;&#21334;&#19982;&#32479;&#19968;&#23450;&#20215;&#65292;&#36825;&#31181;&#25293;&#21334;&#22312;&#23454;&#36341;&#20013;&#34987;&#24191;&#27867;&#29992;&#20110;&#20998;&#37197;&#35832;&#22914;&#30899;&#25490;&#25918;&#35768;&#21487;&#35777;&#20043;&#31867;&#30340;&#21830;&#21697;&#12290;&#22312;&#27599;&#19968;&#36718;&#20013;&#65292;$K$&#20010;&#30456;&#21516;&#30340;&#21830;&#21697;&#21333;&#20803;&#34987;&#21334;&#32473;&#19968;&#32452;&#20855;&#26377;&#36882;&#20943;&#36793;&#38469;&#22238;&#25253;&#20215;&#20540;&#30340;&#20080;&#23478;&#12290;&#20080;&#23478;&#20026;&#21830;&#21697;&#21333;&#20301;&#25552;&#20132;&#20986;&#20215;&#65292;&#28982;&#21518;&#26681;&#25454;&#20986;&#20215;&#30830;&#23450;&#19968;&#20010;&#21333;&#20301;&#30340;&#20215;&#26684;$p$&#65292;&#20351;&#24471;&#25152;&#26377;&#21333;&#20301;&#37117;&#33021;&#34987;&#21806;&#20986;&#12290;&#25105;&#20204;&#32771;&#34385;&#25293;&#21334;&#30340;&#20004;&#20010;&#21464;&#20307;&#65292;&#20854;&#20013;&#20215;&#26684;&#20998;&#21035;&#35774;&#20026;&#31532;$K$&#39640;&#20986;&#20215;&#21644;&#31532;$(K+1)$&#39640;&#20986;&#20215;&#12290;&#25105;&#20204;&#22312;&#31163;&#32447;&#21644;&#22312;&#32447;&#35774;&#32622;&#20013;&#20998;&#26512;&#20102;&#25293;&#21334;&#30340;&#24615;&#36136;&#12290;&#22312;&#31163;&#32447;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#19968;&#20010;&#29609;&#23478;$i$&#38754;&#20020;&#30340;&#38382;&#39064;&#65306;&#22312;&#36807;&#21435;&#30340;&#25293;&#21334;&#20013;&#65292;&#32473;&#23450;&#21253;&#21547;&#31454;&#20105;&#23545;&#25163;&#25552;&#20132;&#30340;&#20986;&#20215;&#30340;&#25968;&#25454;&#38598;&#65292;&#25214;&#21040;&#19968;&#20010;&#20986;&#20215;&#21521;&#37327;&#65292;&#20351;&#24471;&#29609;&#23478;$i$&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#30340;&#32047;&#31215;&#25928;&#29992;&#26368;&#22823;&#21270;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#20854;&#31561;&#20215;&#20110;&#22312;&#20180;&#32454;&#26500;&#36896;&#30340;&#26377;&#21521;&#22270;&#19978;&#25214;&#21040;&#26368;&#22823;&#26435;&#37325;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider repeated multi-unit auctions with uniform pricing, which are widely used in practice for allocating goods such as carbon licenses. In each round, $K$ identical units of a good are sold to a group of buyers that have valuations with diminishing marginal returns. The buyers submit bids for the units, and then a price $p$ is set per unit so that all the units are sold. We consider two variants of the auction, where the price is set to the $K$-th highest bid and $(K+1)$-st highest bid, respectively.  We analyze the properties of this auction in both the offline and online settings. In the offline setting, we consider the problem that one player $i$ is facing: given access to a data set that contains the bids submitted by competitors in past auctions, find a bid vector that maximizes player $i$'s cumulative utility on the data set. We design a polynomial time algorithm for this problem, by showing it is equivalent to finding a maximum-weight path on a carefully constructed direc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Diff-PGD&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#25509;&#36817;&#21407;&#22987;&#25968;&#25454;&#20998;&#24067;&#12289;&#36924;&#30495;&#30340;&#23545;&#25239;&#26679;&#26412;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#38544;&#34109;&#24615;&#21644;&#23545;&#25239;&#24378;&#24230;&#21487;&#35843;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.16494</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30340;&#23545;&#25239;&#26679;&#26412;&#29983;&#25104;&#20197;&#25552;&#39640;&#38544;&#34109;&#24615;&#21644;&#21487;&#25511;&#24615;
&lt;/p&gt;
&lt;p&gt;
Diffusion-Based Adversarial Sample Generation for Improved Stealthiness and Controllability. (arXiv:2305.16494v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16494
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Diff-PGD&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#25509;&#36817;&#21407;&#22987;&#25968;&#25454;&#20998;&#24067;&#12289;&#36924;&#30495;&#30340;&#23545;&#25239;&#26679;&#26412;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#38544;&#34109;&#24615;&#21644;&#23545;&#25239;&#24378;&#24230;&#21487;&#35843;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#26679;&#26412;&#30340;&#24433;&#21709;&#65306;&#36825;&#26159;&#19968;&#31181;&#29305;&#24847;&#21046;&#20316;&#30340;&#33258;&#28982;&#22270;&#29255;&#30340;&#24494;&#23567;&#21464;&#21270;&#65292;&#26088;&#22312;&#35823;&#23548;&#27169;&#22411;&#12290;&#34429;&#28982;&#36825;&#20123;&#23545;&#25239;&#26679;&#26412;&#22312;&#25968;&#23383;&#21644;&#29289;&#29702;&#22330;&#26223;&#20013;&#21487;&#20197;&#36731;&#26494;&#29983;&#25104;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#19982;&#33258;&#28982;&#22270;&#20687;&#30340;&#23454;&#38469;&#25968;&#25454;&#20998;&#24067;&#24046;&#24322;&#24456;&#22823;&#65292;&#23548;&#33268;&#24378;&#24230;&#19982;&#38544;&#34109;&#24615;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#25193;&#25955;-&#25237;&#24433;&#26799;&#24230;&#19979;&#38477;&#65288;Diff-PGD&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#36924;&#30495;&#30340;&#23545;&#25239;&#26679;&#26412;&#12290;&#36890;&#36807;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#24341;&#23548;&#30340;&#26799;&#24230;&#65292;Diff-PGD&#30830;&#20445;&#23545;&#25239;&#26679;&#26412;&#20445;&#25345;&#25509;&#36817;&#21407;&#22987;&#25968;&#25454;&#20998;&#24067;&#65292;&#21516;&#26102;&#20445;&#25345;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#36731;&#26494;&#23450;&#21046;&#29305;&#23450;&#20219;&#21153;&#65292;&#22914;&#25968;&#23383;&#25915;&#20987;&#12289;&#29289;&#29702;&#25915;&#20987;&#21644;&#22522;&#20110;&#26679;&#24335;&#30340;&#25915;&#20987;&#12290;&#19982;&#29616;&#26377;&#30340;&#29983;&#25104;&#33258;&#28982;&#39118;&#26684;&#23545;&#25239;&#26679;&#26412;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#20998;&#31163;&#20248;&#21270;&#23545;&#25239;&#24378;&#24230;&#21644;&#38544;&#34109;&#24615;&#65292;&#25552;&#20379;&#26356;&#22823;&#30340;&#28789;&#27963;&#24615;&#21644;&#23545;&#29983;&#25104;&#26679;&#26412;&#30340;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks are known to be susceptible to adversarial samples: small variations of natural examples crafted to deliberately mislead the models. While they can be easily generated using gradient-based techniques in digital and physical scenarios, they often differ greatly from the actual data distribution of natural images, resulting in a trade-off between strength and stealthiness. In this paper, we propose a novel framework dubbed Diffusion-Based Projected Gradient Descent (Diff-PGD) for generating realistic adversarial samples. By exploiting a gradient guided by a diffusion model, Diff-PGD ensures that adversarial samples remain close to the original data distribution while maintaining their effectiveness. Moreover, our framework can be easily customized for specific tasks such as digital attacks, physical-world attacks, and style-based attacks. Compared with existing methods for generating natural-style adversarial samples, our framework enables the separation of optimizing adv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#23558;&#30693;&#35782;&#22270;&#23884;&#20837;&#27169;&#22411;&#36716;&#21270;&#20026;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20801;&#35768;&#31934;&#30830;MLE&#23398;&#20064;&#12289;&#26377;&#25928;&#25277;&#26679;&#26032;&#30340;&#19977;&#20803;&#32452;&#20197;&#21450;&#20445;&#35777;&#36923;&#36753;&#32422;&#26463;&#65292;&#33719;&#24471;&#20102;&#27604;&#21407;&#22987;KGEs&#26356;&#20855;&#20280;&#32553;&#24615;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.15944</link><description>&lt;p&gt;
&#22914;&#20309;&#36890;&#36807;&#27010;&#29575;&#30005;&#36335;&#23558;&#24744;&#30340;&#30693;&#35782;&#22270;&#23884;&#20837;&#36716;&#21270;&#20026;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
How to Turn Your Knowledge Graph Embeddings into Generative Models via Probabilistic Circuits. (arXiv:2305.15944v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15944
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#23558;&#30693;&#35782;&#22270;&#23884;&#20837;&#27169;&#22411;&#36716;&#21270;&#20026;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20801;&#35768;&#31934;&#30830;MLE&#23398;&#20064;&#12289;&#26377;&#25928;&#25277;&#26679;&#26032;&#30340;&#19977;&#20803;&#32452;&#20197;&#21450;&#20445;&#35777;&#36923;&#36753;&#32422;&#26463;&#65292;&#33719;&#24471;&#20102;&#27604;&#21407;&#22987;KGEs&#26356;&#20855;&#20280;&#32553;&#24615;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20123;&#25104;&#21151;&#30340;&#30693;&#35782;&#22270;&#23884;&#20837;&#27169;&#22411;&#21487;&#29992;&#20316;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#65292;&#32780;&#36825;&#31687;&#35770;&#25991;&#35299;&#37322;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#24471;&#20998;&#20989;&#25968;&#65292;&#23558;&#20854;&#37325;&#26032;&#35299;&#37322;&#25104;&#20026;&#30005;&#36335;&#24418;&#24335;--&#36825;&#26159;&#19968;&#31181;&#20801;&#35768;&#26377;&#25928;&#36793;&#38469;&#21270;&#30340;&#32422;&#26463;&#35745;&#31639;&#22270;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#20010;&#26041;&#27861;&#26469;&#33719;&#24471;&#26377;&#25928;&#30340;&#29983;&#25104;&#30005;&#36335;&#27169;&#22411;&#65292;&#20854;&#20013;&#19968;&#20010;&#26041;&#27861;&#26159;&#23558;&#20854;&#28608;&#27963;&#38480;&#21046;&#20026;&#38750;&#36127;&#25968;&#65292;&#21478;&#19968;&#20010;&#26041;&#27861;&#26159;&#23558;&#20854;&#36755;&#20986;&#24179;&#26041;&#12290;&#25105;&#20204;&#30340;&#35299;&#37322;&#19981;&#20250;&#24433;&#21709;&#21040;&#39044;&#27979;&#33410;&#28857;&#36830;&#36793;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20294;&#30005;&#36335;&#26694;&#26550;&#20351;&#24471;MLE&#30340;&#31934;&#30830;&#23398;&#20064;&#12289;&#26032;&#19977;&#20803;&#32452;&#30340;&#26377;&#25928;&#25277;&#26679;&#20197;&#21450;&#20445;&#35777;&#36923;&#36753;&#32422;&#26463;&#24471;&#20197;&#28385;&#36275;&#25104;&#20026;&#21487;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#25317;&#26377;&#25968;&#30334;&#19975;&#20010;&#23454;&#20307;&#30340;&#22270;&#19978;&#27604;&#21407;&#22987;&#30340;KGEs&#26356;&#20855;&#20280;&#32553;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Some of the most successful knowledge graph embedding (KGE) models for link prediction -- CP, RESCAL, TuckER, ComplEx -- can be interpreted as energy-based models. Under this perspective they are not amenable for exact maximum-likelihood estimation (MLE), sampling and struggle to integrate logical constraints. This work re-interprets the score functions of these KGEs as circuits -- constrained computational graphs allowing efficient marginalisation. Then, we design two recipes to obtain efficient generative circuit models by either restricting their activations to be non-negative or squaring their outputs. Our interpretation comes with little or no loss of performance for link prediction, while the circuits framework unlocks exact learning by MLE, efficient sampling of new triples, and guarantee that logical constraints are satisfied by design. Furthermore, our models scale more gracefully than the original KGEs on graphs with millions of entities.
&lt;/p&gt;</description></item><item><title>PillarAcc&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;&#31639;&#27861;-&#30828;&#20214;&#21327;&#21516;&#35774;&#35745;&#65292;&#26377;&#25928;&#22686;&#24378;&#22522;&#20110;Pillar&#30340;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#32593;&#32476;&#65292;&#23454;&#29616;&#39640;&#25928;&#29575;&#30340;&#35745;&#31639;&#20943;&#23569;&#12290;</title><link>http://arxiv.org/abs/2305.07522</link><description>&lt;p&gt;
PillarAcc: &#31232;&#30095;&#28857;&#20113;Pillars&#21152;&#36895;&#22120;&#8212;&#8212;&#36793;&#32536;&#35774;&#22791;&#19978;&#23454;&#26102;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
PillarAcc: Sparse PointPillars Accelerator for Real-Time Point Cloud 3D Object Detection on Edge Devices. (arXiv:2305.07522v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07522
&lt;/p&gt;
&lt;p&gt;
PillarAcc&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;&#31639;&#27861;-&#30828;&#20214;&#21327;&#21516;&#35774;&#35745;&#65292;&#26377;&#25928;&#22686;&#24378;&#22522;&#20110;Pillar&#30340;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#32593;&#32476;&#65292;&#23454;&#29616;&#39640;&#25928;&#29575;&#30340;&#35745;&#31639;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#28857;&#20113;(PC)&#25968;&#25454;&#36827;&#34892;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#23545;&#20110;&#33258;&#21160;&#39550;&#39542;&#24863;&#30693;&#31649;&#36947;&#33267;&#20851;&#37325;&#35201;&#65292;&#20854;&#20013;&#39640;&#25928;&#30340;&#32534;&#30721;&#26159;&#28385;&#36275;&#20005;&#26684;&#36164;&#28304;&#21644;&#24310;&#36831;&#35201;&#27714;&#30340;&#20851;&#38190;&#12290;PointPillars&#26159;&#19968;&#31181;&#24191;&#27867;&#37319;&#29992;&#30340;&#40479;&#30640;&#22270;(BEV)&#32534;&#30721;&#26041;&#27861;&#65292;&#23558;&#19977;&#32500;&#28857;&#20113;&#25968;&#25454;&#32858;&#21512;&#21040;&#20108;&#32500;pillar&#20013;&#65292;&#23454;&#29616;&#39640;&#31934;&#24230;&#30340;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#37319;&#29992;PointPillar&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#37117;&#24573;&#35270;&#20102;pillar&#32534;&#30721;&#30340;&#22266;&#26377;&#31232;&#30095;&#24615;&#65292;&#38169;&#22833;&#20102;&#22823;&#37327;&#35745;&#31639;&#20943;&#23569;&#30340;&#26426;&#20250;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24320;&#21019;&#24615;&#30340;&#31639;&#27861;-&#30828;&#20214;&#21327;&#21516;&#35774;&#35745;&#65292;&#21152;&#36895;&#31232;&#30095;&#21367;&#31215;&#22788;&#29702;&#65292;&#24182;&#26368;&#22823;&#38480;&#24230;&#22320;&#21033;&#29992;pillar&#30340;&#31232;&#30095;&#24615;&#26469;&#36827;&#34892;&#22522;&#20110;pillar&#30340;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#32593;&#32476;&#12290;&#25105;&#20204;&#20351;&#29992;&#20808;&#36827;&#30340;pillar pruning&#26041;&#27861;&#23545;&#31232;&#30095;&#21270;&#26426;&#20250;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#23454;&#29616;&#20102;&#31934;&#30830;&#24615;&#21644;&#31232;&#30095;&#24615;&#20043;&#38388;&#30340;&#26368;&#20248;&#24179;&#34913;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;PillarAcc&#65292;&#36825;&#26159;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#31232;&#30095;&#25903;&#25345;&#26426;&#21046;&#65292;&#36890;&#36807;&#36755;&#20837;&#20855;&#26377;&#32447;&#24615;&#22797;&#26434;&#24230;&#30340;&#26041;&#27861;&#22686;&#24378;&#31232;&#30095;pillar&#21367;&#31215;&#12290;
&lt;/p&gt;
&lt;p&gt;
3D object detection using point cloud (PC) data is vital for autonomous driving perception pipelines, where efficient encoding is key to meeting stringent resource and latency requirements. PointPillars, a widely adopted bird's-eye view (BEV) encoding, aggregates 3D point cloud data into 2D pillars for high-accuracy 3D object detection. However, most state-of-the-art methods employing PointPillar overlook the inherent sparsity of pillar encoding, missing opportunities for significant computational reduction. In this study, we propose a groundbreaking algorithm-hardware co-design that accelerates sparse convolution processing and maximizes sparsity utilization in pillar-based 3D object detection networks. We investigate sparsification opportunities using an advanced pillar-pruning method, achieving an optimal balance between accuracy and sparsity. We introduce PillarAcc, a state-of-the-art sparsity support mechanism that enhances sparse pillar convolution through linear complexity input
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#23567;&#25209;&#37327;&#22823;&#23567;&#23545;&#31232;&#30095;&#21644;&#23494;&#38598;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#22312;&#20020;&#30028;&#20540;&#22788;&#20250;&#20986;&#29616;&#23574;&#38160;&#30340;&#30456;&#21464;&#65292;&#38416;&#26126;&#20102;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#30340;&#22522;&#26412;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2305.06435</link><description>&lt;p&gt;
&#31232;&#30095;&#21644;&#23494;&#38598;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#23567;&#25209;&#37327;&#22823;&#23567;&#30340;&#30456;&#21464;
&lt;/p&gt;
&lt;p&gt;
Phase transitions in the mini-batch size for sparse and dense neural networks. (arXiv:2305.06435v1 [cond-mat.dis-nn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06435
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#23567;&#25209;&#37327;&#22823;&#23567;&#23545;&#31232;&#30095;&#21644;&#23494;&#38598;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#22312;&#20020;&#30028;&#20540;&#22788;&#20250;&#20986;&#29616;&#23574;&#38160;&#30340;&#30456;&#21464;&#65292;&#38416;&#26126;&#20102;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#30340;&#22522;&#26412;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35757;&#32451;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#26102;&#65292;&#20351;&#29992;&#23567;&#25209;&#37327;&#25968;&#25454;&#29616;&#22312;&#38750;&#24120;&#26222;&#36941;&#12290;&#23613;&#31649;&#24050;&#32463;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#32570;&#23569;&#23450;&#37327;&#35299;&#37322;&#26368;&#20339;&#23567;&#25209;&#37327;&#22823;&#23567;&#24212;&#35813;&#26159;&#22810;&#22823;&#30340;&#29702;&#35770;&#12290;&#26412;&#25991;&#23581;&#35797;&#31995;&#32479;&#22320;&#29702;&#35299;&#23567;&#25209;&#37327;&#22823;&#23567;&#22312;&#35757;&#32451;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20316;&#29992;&#12290;&#22312;&#25945;&#24072;-&#23398;&#29983;&#24773;&#22659;&#19979;&#65292;&#20351;&#29992;&#31232;&#30095;&#25945;&#24072;&#65292;&#24182;&#32858;&#28966;&#20110;&#19981;&#21516;&#22797;&#26434;&#24230;&#30340;&#20219;&#21153;&#65292;&#25105;&#20204;&#37327;&#21270;&#20102;&#25913;&#21464;&#23567;&#25209;&#37327;&#22823;&#23567;m&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#23398;&#29983;&#30340;&#27867;&#21270;&#24615;&#33021;&#24378;&#28872;&#20381;&#36182;&#20110;m&#65292;&#24182;&#19988;&#21487;&#33021;&#22312;&#20020;&#30028;&#20540;mc&#22788;&#32463;&#21382;&#23574;&#38160;&#30340;&#30456;&#21464;&#65292;&#36825;&#26679;&#24403;m&lt; mc&#26102;&#65292;&#35757;&#32451;&#36807;&#31243;&#22833;&#36133;&#65292;&#32780;&#24403;m&gt; mc&#26102;&#65292;&#23398;&#29983;&#21487;&#20197;&#23436;&#32654;&#22320;&#23398;&#20064;&#25110;&#24456;&#22909;&#22320;&#27867;&#21270;&#25945;&#24072;&#12290;&#30456;&#21464;&#26159;&#30001;&#32479;&#35745;&#21147;&#23398;&#39318;&#27425;&#21457;&#29616;&#30340;&#38598;&#20307;&#29616;&#35937;&#65292;&#24182;&#22312;&#35768;&#22810;&#31185;&#23398;&#39046;&#22495;&#35266;&#23519;&#21040;&#12290;&#25214;&#21040;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#25913;&#21464;&#23567;&#25209;&#37327;&#22823;&#23567;&#30340;&#30456;&#21464;&#65292;&#21487;&#20197;&#38416;&#26126;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#30340;&#22522;&#26412;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of mini-batches of data in training artificial neural networks is nowadays very common. Despite its broad usage, theories explaining quantitatively how large or small the optimal mini-batch size should be are missing. This work presents a systematic attempt at understanding the role of the mini-batch size in training two-layer neural networks. Working in the teacher-student scenario, with a sparse teacher, and focusing on tasks of different complexity, we quantify the effects of changing the mini-batch size $m$. We find that often the generalization performances of the student strongly depend on $m$ and may undergo sharp phase transitions at a critical value $m_c$, such that for $m&lt;m_c$ the training process fails, while for $m&gt;m_c$ the student learns perfectly or generalizes very well the teacher. Phase transitions are induced by collective phenomena firstly discovered in statistical mechanics and later observed in many fields of science. Finding a phase transition varying the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;VGA&#26041;&#27861;&#30340;&#25340;&#36710;&#31995;&#32479;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#35745;&#31639;&#22823;&#35268;&#27169;MoD&#31995;&#32479;&#20013;&#30340;&#26368;&#20248;&#20056;&#23458;-&#36710;&#36742;&#20998;&#37197;&#21644;&#30456;&#24212;&#30340;&#36710;&#36742;&#36335;&#24452;&#12290;&#30740;&#31350;&#32773;&#27604;&#36739;&#20102;&#20351;&#29992;&#26368;&#20248;&#20998;&#37197;&#30340;MoD&#31995;&#32479;&#19982;&#20351;&#29992;&#26222;&#36890;&#20998;&#37197;&#30340;MoD&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.02209</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#32593;&#19978;&#25340;&#36710;&#65306;&#20219;&#21153;&#20998;&#37197;&#20248;&#21270;&#23545;&#31995;&#32479;&#24615;&#33021;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Large-scale Online Ridesharing: The Effect of Assignment Optimality on System Performance. (arXiv:2305.02209v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02209
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;VGA&#26041;&#27861;&#30340;&#25340;&#36710;&#31995;&#32479;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#35745;&#31639;&#22823;&#35268;&#27169;MoD&#31995;&#32479;&#20013;&#30340;&#26368;&#20248;&#20056;&#23458;-&#36710;&#36742;&#20998;&#37197;&#21644;&#30456;&#24212;&#30340;&#36710;&#36742;&#36335;&#24452;&#12290;&#30740;&#31350;&#32773;&#27604;&#36739;&#20102;&#20351;&#29992;&#26368;&#20248;&#20998;&#37197;&#30340;MoD&#31995;&#32479;&#19982;&#20351;&#29992;&#26222;&#36890;&#20998;&#37197;&#30340;MoD&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#21160;&#20986;&#34892;&#65288;MoD&#65289;&#31995;&#32479;&#30001;&#19968;&#32676;&#20849;&#20139;&#27773;&#36710;&#32452;&#25104;&#65292;&#21487;&#29992;&#20110;&#21333;&#31243;&#28857;&#23545;&#28857;&#30340;&#20056;&#22352;&#12290;&#36890;&#36807;&#20351;&#29992;&#25340;&#36710;&#65292;&#21363;&#23558;&#22810;&#21517;&#20056;&#23458;&#20998;&#37197;&#21040;&#19968;&#36742;&#36710;&#19978;&#65292;&#21487;&#20197;&#20943;&#23569;&#36710;&#36742;&#21644;&#36710;&#38431;&#30340;&#34892;&#39542;&#24635;&#37324;&#31243;&#12290;&#28982;&#32780;&#65292;&#22312;MoD&#31995;&#32479;&#20013;&#25214;&#21040;&#26368;&#20248;&#30340;&#20056;&#23458;-&#36710;&#36742;&#20998;&#37197;&#26159;&#19968;&#20010;&#22256;&#38590;&#30340;&#32452;&#21512;&#38382;&#39064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;VGA&#26041;&#27861;&#30340;&#26368;&#36817;&#25552;&#20986;&#30340;&#25340;&#36710;&#31995;&#32479;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#35745;&#31639;&#22823;&#35268;&#27169;MoD&#31995;&#32479;&#20013;&#30340;&#26368;&#20248;&#20056;&#23458;-&#36710;&#36742;&#20998;&#37197;&#21644;&#30456;&#24212;&#30340;&#36710;&#36742;&#36335;&#24452;&#12290;&#19982;&#29616;&#26377;&#30340;&#30740;&#31350;&#30456;&#27604;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#25152;&#26377;&#20056;&#23458;-&#36710;&#36742;&#20998;&#37197;&#38382;&#39064;&#65292;&#23450;&#26399;&#22788;&#29702;&#21253;&#21547;&#25104;&#21315;&#19978;&#19975;&#36742;&#36710;&#21644;&#20056;&#23458;&#30340;&#23454;&#20363;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#26816;&#26597;&#20351;&#29992;&#26368;&#20248;&#25340;&#36710;&#20998;&#37197;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20351;&#29992;&#26368;&#20248;&#20998;&#37197;&#30340;MoD&#31995;&#32479;&#19982;&#20351;&#29992;&#26222;&#36890;&#20998;&#37197;&#30340;MoD&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mobility-on-demand (MoD) systems consist of a fleet of shared vehicles that can be hailed for one-way point-to-point trips. The total distance driven by the vehicles and the fleet size can be reduced by employing ridesharing, i.e., by assigning multiple passengers to one vehicle. However, finding the optimal passenger-vehicle assignment in an MoD system is a hard combinatorial problem. In this work, we demonstrate how the VGA method, a recently proposed systematic method for ridesharing, can be used to compute the optimal passenger-vehicle assignments and corresponding vehicle routes in a massive-scale MoD system. In contrast to existing works, we solve all passenger-vehicle assignment problems to optimality, regularly dealing with instances containing thousands of vehicles and passengers. Moreover, to examine the impact of using optimal ridesharing assignments, we compare the performance of an MoD system that uses optimal assignments against an MoD system that uses assignments compute
&lt;/p&gt;</description></item><item><title>Pgx&#26159;&#19968;&#20010;&#29992;JAX&#32534;&#20889;&#30340;&#28216;&#25103;&#27169;&#25311;&#22120;&#38598;&#21512;&#65292;&#20855;&#26377;&#24378;&#21270;&#23398;&#20064;&#30828;&#20214;&#21152;&#36895;&#33021;&#21147;&#65292;&#25903;&#25345;&#24182;&#34892;&#25191;&#34892;&#65292;&#36895;&#24230;&#27604;&#29616;&#26377;&#30340;&#24378;&#21270;&#23398;&#20064;&#24211;&#24555;10&#20493;&#12290; &#23427;&#23454;&#29616;&#20102;Backgammon&#65292;Shogi&#21644;Go&#31561;&#22522;&#20934;&#27979;&#35797;&#28216;&#25103;&#12290;</title><link>http://arxiv.org/abs/2303.17503</link><description>&lt;p&gt;
Pgx:&#24378;&#21270;&#23398;&#20064;&#30828;&#20214;&#21152;&#36895;&#30340;&#24182;&#34892;&#28216;&#25103;&#27169;&#25311;&#22120;
&lt;/p&gt;
&lt;p&gt;
Pgx: Hardware-accelerated parallel game simulation for reinforcement learning. (arXiv:2303.17503v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17503
&lt;/p&gt;
&lt;p&gt;
Pgx&#26159;&#19968;&#20010;&#29992;JAX&#32534;&#20889;&#30340;&#28216;&#25103;&#27169;&#25311;&#22120;&#38598;&#21512;&#65292;&#20855;&#26377;&#24378;&#21270;&#23398;&#20064;&#30828;&#20214;&#21152;&#36895;&#33021;&#21147;&#65292;&#25903;&#25345;&#24182;&#34892;&#25191;&#34892;&#65292;&#36895;&#24230;&#27604;&#29616;&#26377;&#30340;&#24378;&#21270;&#23398;&#20064;&#24211;&#24555;10&#20493;&#12290; &#23427;&#23454;&#29616;&#20102;Backgammon&#65292;Shogi&#21644;Go&#31561;&#22522;&#20934;&#27979;&#35797;&#28216;&#25103;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Pgx&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;JAX&#32534;&#20889;&#30340;&#26827;&#30424;&#28216;&#25103;&#27169;&#25311;&#22120;&#38598;&#21512;&#12290;&#30001;&#20110;JAX&#30340;&#33258;&#21160;&#21521;&#37327;&#21270;&#21644;&#21363;&#26102;&#32534;&#35793;&#21151;&#33021;&#65292;Pgx&#26131;&#20110;&#22312;GPU/TPU&#21152;&#36895;&#22120;&#19978;&#36827;&#34892;&#22823;&#35268;&#27169;&#24182;&#34892;&#25191;&#34892;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#21333;&#20010;A100 GPU&#19978;&#30340;Pgx&#27169;&#25311;&#27604;&#29616;&#26377;&#30340;&#24378;&#21270;&#23398;&#20064;&#24211;&#24555;10&#20493;&#12290;Pgx&#23454;&#29616;&#20102;&#34987;&#35748;&#20026;&#26159;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#22522;&#20934;&#27979;&#35797;&#30340;&#28216;&#25103;&#65292;&#22914;Backgammon&#65292;Shogi&#21644;Go&#12290; Pgx&#21487;&#22312;https://github.com/sotetsuk/pgx&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Pgx, a collection of board game simulators written in JAX. Thanks to auto-vectorization and Just-In-Time compilation of JAX, Pgx scales easily to thousands of parallel execution on GPU/TPU accelerators. We found that the simulation of Pgx on a single A100 GPU is 10x faster than that of existing reinforcement learning libraries. Pgx implements games considered vital benchmarks in artificial intelligence research, such as Backgammon, Shogi, and Go. Pgx is available at https://github.com/sotetsuk/pgx.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#30340;&#26080;&#20154;&#26426;&#35270;&#26816;&#27979;&#26694;&#26550;&#65292;&#29992;&#20110;&#21160;&#24577;&#38567;&#36947;&#29615;&#22659;&#32780;&#26080;&#38656;&#20351;&#29992;&#20808;&#21069;&#30340;&#22320;&#22270;&#65292;&#36890;&#36807;&#20998;&#23618;&#35268;&#21010;&#26041;&#26696;&#23558;&#26816;&#27979;&#38382;&#39064;&#20998;&#35299;&#25104;&#19981;&#21516;&#23618;&#27425;&#65292;&#23454;&#29616;&#20102;&#26368;&#22823;&#21270;&#30340;&#33258;&#20027;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2301.08422</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#26410;&#30693;&#38567;&#36947;&#24314;&#35774;&#22330;&#22320;&#30340;&#22522;&#20110;&#35270;&#35273;&#30340;&#33258;&#20027;&#26080;&#20154;&#26426;&#35270;&#26816;&#27979;&#26694;&#26550;&#65288;&#26356;&#26032;&#29256;&#65289;
&lt;/p&gt;
&lt;p&gt;
A vision-based autonomous UAV inspection framework for unknown tunnel construction sites with dynamic obstacles. (arXiv:2301.08422v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08422
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#30340;&#26080;&#20154;&#26426;&#35270;&#26816;&#27979;&#26694;&#26550;&#65292;&#29992;&#20110;&#21160;&#24577;&#38567;&#36947;&#29615;&#22659;&#32780;&#26080;&#38656;&#20351;&#29992;&#20808;&#21069;&#30340;&#22320;&#22270;&#65292;&#36890;&#36807;&#20998;&#23618;&#35268;&#21010;&#26041;&#26696;&#23558;&#26816;&#27979;&#38382;&#39064;&#20998;&#35299;&#25104;&#19981;&#21516;&#23618;&#27425;&#65292;&#23454;&#29616;&#20102;&#26368;&#22823;&#21270;&#30340;&#33258;&#20027;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#38075;&#28856;&#27861;&#30340;&#38567;&#36947;&#24314;&#35774;&#38656;&#35201;&#23545;&#25366;&#25496;&#21069;&#37096;&#20301;&#36827;&#34892;&#19977;&#32500;&#27979;&#37327;&#20197;&#35780;&#20272;&#21387;&#30862;&#20301;&#32622;&#12290;&#20026;&#20102;&#32771;&#34385;&#26816;&#27979;&#21644;&#27979;&#37327;&#20219;&#21153;&#30340;&#23433;&#20840;&#12289;&#25104;&#26412;&#21644;&#25928;&#30410;&#65292;&#37096;&#32626;&#36731;&#20415;&#30340;&#33258;&#20027;&#26426;&#22120;&#20154;&#65292;&#22914;&#26080;&#20154;&#26426;&#65288;UAV&#65289;&#65292;&#21464;&#24471;&#26356;&#21152;&#24517;&#35201;&#21644;&#26222;&#36941;&#12290;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#24037;&#20316;&#37117;&#20351;&#29992;&#20808;&#21069;&#30340;&#22320;&#22270;&#36827;&#34892;&#26816;&#39564;&#35270;&#28857;&#30340;&#30830;&#23450;&#24182;&#27809;&#26377;&#32771;&#34385;&#21160;&#24577;&#38556;&#30861;&#29289;&#12290;&#20026;&#20102;&#26368;&#22823;&#38480;&#24230;&#22320;&#22686;&#21152;&#33258;&#20027;&#27700;&#24179;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#30340;&#26080;&#20154;&#26426;&#35270;&#26816;&#27979;&#26694;&#26550;&#65292;&#29992;&#20110;&#21160;&#24577;&#38567;&#36947;&#29615;&#22659;&#32780;&#26080;&#38656;&#20351;&#29992;&#20808;&#21069;&#30340;&#22320;&#22270;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20998;&#23618;&#35268;&#21010;&#26041;&#26696;&#23558;&#26816;&#27979;&#38382;&#39064;&#20998;&#35299;&#25104;&#19981;&#21516;&#23618;&#27425;&#12290;&#39640;&#23618;&#20915;&#31574;&#32773;&#39318;&#20808;&#30830;&#23450;&#26426;&#22120;&#20154;&#30340;&#20219;&#21153;&#24182;&#29983;&#25104;&#30446;&#26631;&#28857;&#12290;&#28982;&#21518;&#65292;&#20013;&#23618;&#36335;&#24452;&#35268;&#21010;&#22120;&#25214;&#21040;&#36884;&#24452;&#28857;&#36335;&#24452;&#24182;&#20248;&#21270;&#20813;&#30896;&#25758;&#38745;&#24577;&#36712;&#36857;&#12290;&#26368;&#21518;&#65292;&#38745;&#24577;&#36712;&#36857;&#23558;&#36755;&#20837;&#20302;&#23618;&#25511;&#21046;&#22120;&#20197;&#23454;&#29616;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tunnel construction using the drill-and-blast method requires the 3D measurement of the excavation front to evaluate underbreak locations. Considering the inspection and measurement task's safety, cost, and efficiency, deploying lightweight autonomous robots, such as unmanned aerial vehicles (UAV), becomes more necessary and popular. Most of the previous works use a prior map for inspection viewpoint determination and do not consider dynamic obstacles. To maximally increase the level of autonomy, this paper proposes a vision-based UAV inspection framework for dynamic tunnel environments without using a prior map. Our approach utilizes a hierarchical planning scheme, decomposing the inspection problem into different levels. The high-level decision maker first determines the task for the robot and generates the target point. Then, the mid-level path planner finds the waypoint path and optimizes the collision-free static trajectory. Finally, the static trajectory will be fed into the low-
&lt;/p&gt;</description></item><item><title>DeepSpeed Data Efficiency&#25552;&#20986;&#20102;&#20004;&#31181;&#25968;&#25454;&#25928;&#29575;&#25216;&#26415;&#65306;&#39640;&#25928;&#30340;&#25968;&#25454;&#37319;&#26679;&#21644;&#39640;&#25928;&#30340;&#25968;&#25454;&#36335;&#30001;&#65292;&#33021;&#22815;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#25928;&#29575;&#21644;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2212.03597</link><description>&lt;p&gt;
DeepSpeed&#25968;&#25454;&#25928;&#29575;&#65306;&#36890;&#36807;&#39640;&#25928;&#30340;&#25968;&#25454;&#37319;&#26679;&#21644;&#36335;&#30001;&#25913;&#21892;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36136;&#37327;&#21644;&#35757;&#32451;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
DeepSpeed Data Efficiency: Improving Deep Learning Model Quality and Training Efficiency via Efficient Data Sampling and Routing. (arXiv:2212.03597v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.03597
&lt;/p&gt;
&lt;p&gt;
DeepSpeed Data Efficiency&#25552;&#20986;&#20102;&#20004;&#31181;&#25968;&#25454;&#25928;&#29575;&#25216;&#26415;&#65306;&#39640;&#25928;&#30340;&#25968;&#25454;&#37319;&#26679;&#21644;&#39640;&#25928;&#30340;&#25968;&#25454;&#36335;&#30001;&#65292;&#33021;&#22815;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#25928;&#29575;&#21644;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#36827;&#27493;&#20197;&#24040;&#22823;&#30340;&#35757;&#32451;&#25104;&#26412;&#20026;&#20195;&#20215;&#12290;&#27169;&#22411;&#23610;&#23544;&#30340;&#22686;&#21152;&#26159;&#19968;&#20010;&#20027;&#35201;&#21407;&#22240;&#65292;&#20294;&#21478;&#19968;&#20010;&#19981;&#22826;&#34987;&#37325;&#35270;&#30340;&#20107;&#23454;&#26159;&#65292;&#25968;&#25454;&#35268;&#27169;&#23454;&#38469;&#19978;&#19982;&#27169;&#22411;&#35268;&#27169;&#20197;&#30456;&#20284;&#30340;&#36895;&#24230;&#22686;&#21152;&#65292;&#32780;&#35757;&#32451;&#25104;&#26412;&#19982;&#20004;&#32773;&#25104;&#27604;&#20363;&#12290;&#19982;&#19981;&#26029;&#21457;&#23637;&#30340;&#27169;&#22411;&#26550;&#26500;&#30456;&#27604;&#65292;&#22914;&#20309;&#39640;&#25928;&#21033;&#29992;&#35757;&#32451;&#25968;&#25454;&#65288;&#29305;&#21035;&#26159;&#23545;&#20110;&#26114;&#36149;&#30340;&#22522;&#30784;&#27169;&#22411;&#39044;&#35757;&#32451;&#65289;&#22312;&#36739;&#23569;&#34987;&#25506;&#32034;&#19988;&#38590;&#20197;&#23454;&#29616;&#65292;&#36825;&#26159;&#22240;&#20026;&#32570;&#20047;&#19968;&#20010;&#19987;&#27880;&#20110;&#25968;&#25454;&#25928;&#29575;&#33021;&#21147;&#30340;&#26041;&#20415;&#26694;&#26550;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DeepSpeed&#25968;&#25454;&#25928;&#29575;&#65292;&#19968;&#31181;&#33021;&#26356;&#22909;&#22320;&#21033;&#29992;&#25968;&#25454;&#12289;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#24182;&#25913;&#21892;&#27169;&#22411;&#36136;&#37327;&#30340;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#32467;&#21512;&#20102;&#20004;&#31181;&#25968;&#25454;&#25928;&#29575;&#25216;&#26415;&#65306;&#36890;&#36807;&#36890;&#29992;&#35838;&#31243;&#23398;&#20064;&#24211;&#23454;&#29616;&#39640;&#25928;&#25968;&#25454;&#37319;&#26679;&#65292;&#20197;&#21450;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#38543;&#26426;&#36880;&#23618;&#21024;&#38500;&#20196;&#29260;&#30340;&#25216;&#26415;&#23454;&#29616;&#39640;&#25928;&#25968;&#25454;&#36335;&#30001;&#12290;&#38024;&#23545;GPT-3 1.3B&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances on deep learning models come at the price of formidable training cost. The increasing model size is one of the root causes, but another less-emphasized fact is that data scale is actually increasing at a similar speed as model scale, and the training cost is proportional to both of them. Compared to the rapidly evolving model architecture, how to efficiently use the training data (especially for the expensive foundation model pretraining) is both less explored and difficult to realize due to the lack of a convenient framework that focuses on data efficiency capabilities. To this end, we present DeepSpeed Data Efficiency, a framework that makes better use of data, increases training efficiency, and improves model quality. Specifically, we propose and combine two data efficiency techniques: efficient data sampling via a general curriculum learning library, and efficient data routing via a novel random layerwise token dropping technique. For GPT-3 1.3B language model pretr
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#36328;&#23398;&#31185;&#30340;&#26041;&#27861;&#26469;&#25506;&#35752;NLP&#27169;&#22411;&#20559;&#35265;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#37319;&#29992;&#24515;&#29702;&#27979;&#37327;&#23398;&#30340;&#35270;&#35282;&#65292;&#29305;&#21035;&#20851;&#27880;&#26500;&#24565;&#25928;&#24230;&#21644;&#27979;&#37327;&#24037;&#20855;&#30340;&#20449;&#24230;&#65292;&#22312;&#34913;&#37327;&#27169;&#22411;&#20559;&#35265;&#30340;&#24773;&#22659;&#20013;&#22914;&#20309;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2211.13709</link><description>&lt;p&gt;
NLP&#20013;&#30340;&#19981;&#33391;&#20559;&#35265;&#65306;&#36991;&#20813;&#34913;&#37327;&#21361;&#26426;
&lt;/p&gt;
&lt;p&gt;
Undesirable biases in NLP: Averting a crisis of measurement. (arXiv:2211.13709v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13709
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#36328;&#23398;&#31185;&#30340;&#26041;&#27861;&#26469;&#25506;&#35752;NLP&#27169;&#22411;&#20559;&#35265;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#37319;&#29992;&#24515;&#29702;&#27979;&#37327;&#23398;&#30340;&#35270;&#35282;&#65292;&#29305;&#21035;&#20851;&#27880;&#26500;&#24565;&#25928;&#24230;&#21644;&#27979;&#37327;&#24037;&#20855;&#30340;&#20449;&#24230;&#65292;&#22312;&#34913;&#37327;&#27169;&#22411;&#20559;&#35265;&#30340;&#24773;&#22659;&#20013;&#22914;&#20309;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#21644;&#26222;&#21450;&#65292;&#39044;&#27979;&#20854;&#20351;&#29992;&#21487;&#33021;&#23545;&#20154;&#20204;&#36896;&#25104;&#20260;&#23475;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#36817;&#24180;&#26469;&#65292;&#19968;&#20010;&#21463;&#21040;&#20851;&#27880;&#30340;&#38382;&#39064;&#26159;&#36825;&#19968;&#25216;&#26415;&#22312;&#34892;&#20026;&#20013;&#26174;&#31034;&#20986;&#26377;&#23475;&#20559;&#35265;&#12290;&#23613;&#31649;&#24050;&#32463;&#25237;&#20837;&#20102;&#22823;&#37327;&#30340;&#21162;&#21147;&#26469;&#35780;&#20272;&#21644;&#20943;&#36731;&#36825;&#20123;&#20559;&#35265;&#65292;&#20294;&#25105;&#20204;&#34913;&#37327;NLP&#27169;&#22411;&#20559;&#35265;&#30340;&#26041;&#27861;&#23384;&#22312;&#20005;&#37325;&#38382;&#39064;&#65288;&#20363;&#22914;&#65292;&#36890;&#24120;&#19981;&#28165;&#26970;&#23427;&#20204;&#21040;&#24213;&#34913;&#37327;&#20102;&#20160;&#20040;&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#24515;&#29702;&#27979;&#37327;&#23398;&#30340;&#35270;&#35282;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#36328;&#23398;&#31185;&#30340;&#26041;&#27861;&#26469;&#35752;&#35770;NLP&#27169;&#22411;&#20559;&#35265;&#30340;&#38382;&#39064;&#65292;&#24515;&#29702;&#27979;&#37327;&#23398;&#19987;&#27880;&#20110;&#34913;&#37327;&#19981;&#30452;&#25509;&#21487;&#35266;&#23519;&#21040;&#30340;&#27010;&#24565;&#65292;&#22914;&#20559;&#35265;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#25506;&#35752;&#24515;&#29702;&#27979;&#37327;&#23398;&#30340;&#20004;&#20010;&#26680;&#24515;&#27010;&#24565;&#65292;&#21363;&#26500;&#24565;&#25928;&#24230;&#21644;&#27979;&#37327;&#24037;&#20855;&#30340;&#20449;&#24230;&#65292;&#24182;&#35752;&#35770;&#23427;&#20204;&#22312;&#34913;&#37327;&#27169;&#22411;&#20559;&#35265;&#30340;&#24773;&#22659;&#20013;&#22914;&#20309;&#24212;&#29992;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25552;&#20379;&#19968;&#20010;&#20840;&#38754;&#30340;&#35270;&#35282;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
As Large Language Models and Natural Language Processing (NLP) technology rapidly develops and spreads into daily life, it becomes crucial to anticipate how its use could harm people. One problem that has received a lot of attention in recent years is that this technology has displayed harmful biases in its behavior. Although a lot of effort has been invested in assessing and mitigating these biases, our methods of measuring the biases of NLP models have serious problems (e.g., it is often unclear what they actually measure). In this paper, we provide an interdisciplinary approach to discussing the issue of NLP model bias by adopting the lens of psychometrics -- a field specialized in the measurement of concepts like bias that are not directly observable. In particular, we will explore two central notions from psychometrics, the construct validity and the reliability of measurement tools, and discuss how they can be applied in the context of measuring model bias. Our goal is to provide
&lt;/p&gt;</description></item><item><title>&#26426;&#22120;&#23398;&#20064;&#22312;&#33258;&#36866;&#24212;&#31995;&#32479;&#20013;&#25104;&#20026;&#28909;&#38376;&#26041;&#27861;&#65292;&#20294;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#20250;&#38754;&#20020;&#36866;&#24212;&#31354;&#38388;&#20013;&#30340;&#28418;&#31227;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"&#29983;&#21629;&#21608;&#26399;&#33258;&#36866;&#24212;"&#30340;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#22788;&#29702;&#36866;&#24212;&#31354;&#38388;&#30340;&#28418;&#31227;&#12290;</title><link>http://arxiv.org/abs/2211.02658</link><description>&lt;p&gt;
&#21033;&#29992;&#29983;&#21629;&#21608;&#26399;&#33258;&#36866;&#24212;&#22788;&#29702;&#23398;&#20064;&#33258;&#36866;&#24212;&#31995;&#32479;&#20013;&#36866;&#24212;&#31354;&#38388;&#30340;&#28418;&#31227;
&lt;/p&gt;
&lt;p&gt;
Dealing with Drift of Adaptation Spaces in Learning-based Self-Adaptive Systems using Lifelong Self-Adaptation. (arXiv:2211.02658v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.02658
&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#33258;&#36866;&#24212;&#31995;&#32479;&#20013;&#25104;&#20026;&#28909;&#38376;&#26041;&#27861;&#65292;&#20294;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#20250;&#38754;&#20020;&#36866;&#24212;&#31354;&#38388;&#20013;&#30340;&#28418;&#31227;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"&#29983;&#21629;&#21608;&#26399;&#33258;&#36866;&#24212;"&#30340;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#22788;&#29702;&#36866;&#24212;&#31354;&#38388;&#30340;&#28418;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#26426;&#22120;&#23398;&#20064; (ML) &#24050;&#25104;&#20026;&#25903;&#25345;&#33258;&#36866;&#24212;&#30340;&#28909;&#38376;&#26041;&#27861;&#12290;ML &#24050;&#34987;&#29992;&#26469;&#22788;&#29702;&#33258;&#36866;&#24212;&#20013;&#30340;&#20960;&#20010;&#38382;&#39064;&#65292;&#20363;&#22914;&#22312;&#19981;&#30830;&#23450;&#24615;&#19979;&#32500;&#25252;&#26368;&#26032;&#30340;&#36816;&#34892;&#26102;&#27169;&#22411;&#21644;&#21487;&#25193;&#23637;&#30340;&#20915;&#31574;&#21046;&#23450;&#12290;&#28982;&#32780;&#65292;&#21033;&#29992; ML &#23384;&#22312;&#22266;&#26377;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30528;&#37325;&#35752;&#35770;&#38754;&#21521;&#22522;&#20110;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#31995;&#32479;&#30340;&#19968;&#20010;&#29305;&#21035;&#37325;&#35201;&#30340;&#25361;&#25112;&#65306;&#36866;&#24212;&#31354;&#38388;&#20013;&#30340;&#28418;&#31227;&#12290;&#36890;&#36807;&#36866;&#24212;&#31354;&#38388;&#65292;&#25105;&#20204;&#25351;&#30340;&#26159;&#33258;&#36866;&#24212;&#31995;&#32479;&#22312;&#26576;&#19968;&#29305;&#23450;&#26102;&#38388;&#21487;&#20197;&#36873;&#25321;&#30340;&#36866;&#24212;&#36873;&#39033;&#30340;&#38598;&#21512;&#65292;&#20197;&#26681;&#25454;&#36866;&#24212;&#36873;&#39033;&#30340;&#36136;&#37327;&#23646;&#24615;&#36827;&#34892;&#36866;&#24212;&#12290;&#36866;&#24212;&#31354;&#38388;&#30340;&#28418;&#31227;&#28304;&#20110;&#24433;&#21709;&#36866;&#24212;&#36873;&#39033;&#36136;&#37327;&#23646;&#24615;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#36825;&#31181;&#28418;&#31227;&#21487;&#33021;&#24847;&#21619;&#30528;&#26368;&#32456;&#27809;&#26377;&#36866;&#24212;&#36873;&#39033;&#33021;&#22815;&#28385;&#36275;&#26368;&#21021;&#30340;&#36866;&#24212;&#30446;&#26631;&#65292;&#20174;&#32780;&#38477;&#20302;&#31995;&#32479;&#30340;&#36136;&#37327;&#65292;&#25110;&#32773;&#21487;&#33021;&#20986;&#29616;&#20801;&#35768;&#22686;&#24378;&#36866;&#24212;&#30446;&#26631;&#30340;&#36866;&#24212;&#36873;&#39033;&#12290;&#22312; ML &#20013;&#65292;&#36825;&#31181;&#28418;&#31227;&#36890;&#24120;&#34987;&#31216;&#20026;&#27010;&#24565;&#28418;&#31227;&#25110;&#23454;&#20363;&#28418;&#31227;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#29983;&#21629;&#21608;&#26399;&#33258;&#36866;&#24212;&#8221;&#30340;&#26032;&#26041;&#27861;&#12290;&#29983;&#21629;&#21608;&#26399;&#33258;&#36866;&#24212;&#23545; ML powered self-adaptation &#36827;&#34892;&#20102;&#25193;&#23637;&#65292;&#20351;&#20854;&#33021;&#22815;&#26356;&#22909;&#22320;&#22788;&#29702;&#36866;&#24212;&#31354;&#38388;&#30340;&#28418;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, machine learning (ML) has become a popular approach to support self-adaptation. ML has been used to deal with several problems in self-adaptation, such as maintaining an up-to-date runtime model under uncertainty and scalable decision-making. Yet, exploiting ML comes with inherent challenges. In this paper, we focus on a particularly important challenge for learning-based self-adaptive systems: drift in adaptation spaces. With adaptation space we refer to the set of adaptation options a self-adaptive system can select from at a given time to adapt based on the estimated quality properties of the adaptation options. Drift of adaptation spaces originates from uncertainties, affecting the quality properties of the adaptation options. Such drift may imply that eventually no adaptation option can satisfy the initial set of the adaptation goals, deteriorating the quality of the system, or adaptation options may emerge that allow enhancing the adaptation goals. In ML, such shift cor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;LiDAR-&#30456;&#26426;&#22806;&#21442;&#33258;&#26816;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#26631;&#23450;&#30740;&#31350;&#24573;&#35270;&#26631;&#23450;&#32467;&#26524;&#27491;&#30830;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2210.10537</link><description>&lt;p&gt;
&#22312;&#32447;LiDAR-&#30456;&#26426;&#22806;&#21442;&#33258;&#26816;
&lt;/p&gt;
&lt;p&gt;
Online LiDAR-Camera Extrinsic Parameters Self-checking. (arXiv:2210.10537v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10537
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;LiDAR-&#30456;&#26426;&#22806;&#21442;&#33258;&#26816;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#26631;&#23450;&#30740;&#31350;&#24573;&#35270;&#26631;&#23450;&#32467;&#26524;&#27491;&#30830;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31070;&#32463;&#32593;&#32476;&#30340;&#21457;&#23637;&#21644;&#33258;&#21160;&#39550;&#39542;&#30340;&#26222;&#21450;&#65292;LiDAR&#21644;&#30456;&#26426;&#30340;&#26631;&#23450;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#36825;&#20010;&#26631;&#23450;&#20219;&#21153;&#26159;&#22810;&#27169;&#24577;&#30340;&#65292;&#30456;&#26426;&#25429;&#25417;&#21040;&#30340;&#20016;&#23500;&#30340;&#39068;&#33394;&#21644;&#32441;&#29702;&#20449;&#24687;&#20197;&#21450;LiDAR&#33719;&#21462;&#21040;&#30340;&#31934;&#30830;&#30340;&#19977;&#32500;&#31354;&#38388;&#20449;&#24687;&#23545;&#20110;&#19979;&#28216;&#20219;&#21153;&#38750;&#24120;&#37325;&#35201;&#12290;&#30446;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#36890;&#36807;&#20449;&#24687;&#34701;&#21512;&#33719;&#24471;&#20934;&#30830;&#30340;&#26631;&#23450;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#24456;&#23569;&#20998;&#26512;&#26631;&#23450;&#32467;&#26524;&#26159;&#21542;&#27491;&#30830;&#65292;&#36825;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#38750;&#24120;&#37325;&#35201;&#12290;&#20363;&#22914;&#65292;&#22312;&#22823;&#35268;&#27169;&#29983;&#20135;&#20013;&#65292;&#27599;&#36742;&#26234;&#33021;&#36710;&#30340;LiDAR&#21644;&#30456;&#26426;&#22312;&#31163;&#24320;&#29983;&#20135;&#32447;&#26102;&#24517;&#39035;&#33719;&#24471;&#33391;&#22909;&#30340;&#26657;&#20934;&#65292;&#32780;&#22312;&#36710;&#36742;&#20351;&#29992;&#26399;&#38388;&#65292;LiDAR&#21644;&#30456;&#26426;&#30340;&#20301;&#23039;&#20063;&#24212;&#35813;&#25345;&#32493;&#21463;&#21040;&#30417;&#30563;&#20197;&#30830;&#20445;&#23433;&#20840;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#26816;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the development of neural networks and the increasing popularity of automatic driving, the calibration of the LiDAR and the camera has attracted more and more attention. This calibration task is multi-modal, where the rich color and texture information captured by the camera and the accurate three-dimensional spatial information from the LiDAR is incredibly significant for downstream tasks. Current research interests mainly focus on obtaining accurate calibration results through information fusion. However, they seldom analyze whether the calibrated results are correct or not, which could be of significant importance in real-world applications. For example, in large-scale production, the LiDARs and the cameras of each smart car have to get well-calibrated as the car leaves the production line, while in the rest of the car life period, the poses of the LiDARs and cameras should also get continually supervised to ensure the security. To this end, this paper proposes a self-checking 
&lt;/p&gt;</description></item><item><title>NormSAGE&#26159;&#19968;&#20010;&#29992;&#20110;&#23545;&#35805;&#39537;&#21160;&#30340;&#22810;&#35821;&#35328;&#22810;&#25991;&#21270;&#35268;&#33539;&#21457;&#29616;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;GPT-3&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30693;&#35782;&#24341;&#21457;&#65292;&#24182;&#36890;&#36807;&#33258;&#39564;&#35777;&#26426;&#21046;&#30830;&#20445;&#21457;&#29616;&#30340;&#35268;&#33539;&#27491;&#30830;&#19988;&#19982;&#28304;&#23545;&#35805;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2210.08604</link><description>&lt;p&gt;
NormSAGE: &#22810;&#35821;&#35328;&#22810;&#25991;&#21270;&#23545;&#35805;&#20013;&#30340;&#35268;&#33539;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
NormSAGE: Multi-Lingual Multi-Cultural Norm Discovery from Conversations On-the-Fly. (arXiv:2210.08604v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.08604
&lt;/p&gt;
&lt;p&gt;
NormSAGE&#26159;&#19968;&#20010;&#29992;&#20110;&#23545;&#35805;&#39537;&#21160;&#30340;&#22810;&#35821;&#35328;&#22810;&#25991;&#21270;&#35268;&#33539;&#21457;&#29616;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;GPT-3&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30693;&#35782;&#24341;&#21457;&#65292;&#24182;&#36890;&#36807;&#33258;&#39564;&#35777;&#26426;&#21046;&#30830;&#20445;&#21457;&#29616;&#30340;&#35268;&#33539;&#27491;&#30830;&#19988;&#19982;&#28304;&#23545;&#35805;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35268;&#33539;&#21457;&#29616;&#23545;&#20110;&#29702;&#35299;&#21644;&#25512;&#29702;&#20154;&#31867;&#20132;&#27969;&#21644;&#20114;&#21160;&#20013;&#21487;&#25509;&#21463;&#30340;&#34892;&#20026;&#21644;&#28508;&#22312;&#36829;&#35268;&#34892;&#20026;&#38750;&#24120;&#37325;&#35201;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;NormSage&#65292;&#19968;&#20010;&#29992;&#20110;&#35299;&#20915;&#26032;&#39062;&#20219;&#21153;&#30340;&#26694;&#26550;&#65292;&#21363;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#21644;&#33258;&#39564;&#35777;&#30340;&#23545;&#35805;&#39537;&#21160;&#30340;&#22810;&#35821;&#35328;&#22810;&#25991;&#21270;&#35268;&#33539;&#21457;&#29616;&#12290;NormSAGE&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;GPT-3&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#38544;&#24335;&#30693;&#35782;&#65292;&#36890;&#36807;&#38024;&#23545;&#35268;&#33539;&#21457;&#29616;&#20219;&#21153;&#21644;&#23545;&#35805;&#32972;&#26223;&#30340;&#26377;&#21521;&#38382;&#39064;&#24341;&#21457;&#20851;&#20110;&#35268;&#33539;&#30340;&#30693;&#35782;&#12290;&#23427;&#36824;&#36890;&#36807;&#33258;&#39564;&#35777;&#26426;&#21046;&#35299;&#20915;&#35821;&#35328;&#27169;&#22411;&#34394;&#26500;&#30340;&#39118;&#38505;&#65292;&#30830;&#20445;&#21457;&#29616;&#30340;&#35268;&#33539;&#26159;&#27491;&#30830;&#30340;&#65292;&#24182;&#19988;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#19982;&#23427;&#20204;&#30340;&#28304;&#23545;&#35805;&#30456;&#20851;&#12290;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#22522;&#20934;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#23454;&#26102;&#23545;&#35805;&#20013;&#21457;&#29616;&#26356;&#22810;&#30456;&#20851;&#19988;&#26377;&#27934;&#23519;&#21147;&#30340;&#35268;&#33539;&#65288;Likert&#35780;&#20998;&#20013;&#22686;&#21152;&#20102;10%&#20197;&#19978;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Norm discovery is important for understanding and reasoning about the acceptable behaviors and potential violations in human communication and interactions. We introduce NormSage, a framework for addressing the novel task of conversation-grounded multi-lingual, multi-cultural norm discovery, based on language model prompting and self-verification. NormSAGE leverages the expressiveness and implicit knowledge of the pretrained GPT-3 language model backbone, to elicit knowledge about norms through directed questions representing the norm discovery task and conversation context. It further addresses the risk of language model hallucination with a self-verification mechanism ensuring that the norms discovered are correct and are substantially grounded to their source conversations. Evaluation results show that our approach discovers significantly more relevant and insightful norms for conversations on-the-fly compared to baselines (&gt;10+% in Likert scale rating). The norms discovered from Ch
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;RGB-D&#30456;&#26426;&#30340;&#23454;&#26102;&#21160;&#24577;&#38556;&#30861;&#29289;&#36319;&#36394;&#21644;&#24314;&#22270;&#31995;&#32479;&#65292;&#23454;&#29616;&#26080;&#20154;&#26426;&#30340;&#23548;&#33322;&#21644;&#36991;&#38556;&#12290;&#31995;&#32479;&#20351;&#29992;&#28145;&#24230;&#22270;&#20687;&#29983;&#25104;&#21160;&#24577;&#38556;&#30861;&#29289;&#21306;&#22495;&#65292;&#24182;&#21033;&#29992;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#21644;&#36830;&#32493;&#24615;&#28388;&#27874;&#22120;&#36319;&#36394;&#38556;&#30861;&#29289;&#12290;</title><link>http://arxiv.org/abs/2209.08258</link><description>&lt;p&gt;
&#26080;&#20154;&#26426;&#23548;&#33322;&#21644;&#36991;&#38556;&#30340;&#23454;&#26102;&#21160;&#24577;&#38556;&#30861;&#29289;&#36319;&#36394;&#21644;&#24314;&#22270;&#31995;&#32479;&#65288;arXiv:2209.08258v3 [cs.RO] UPDATED&#65289;
&lt;/p&gt;
&lt;p&gt;
A real-time dynamic obstacle tracking and mapping system for UAV navigation and collision avoidance with an RGB-D camera. (arXiv:2209.08258v3 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.08258
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;RGB-D&#30456;&#26426;&#30340;&#23454;&#26102;&#21160;&#24577;&#38556;&#30861;&#29289;&#36319;&#36394;&#21644;&#24314;&#22270;&#31995;&#32479;&#65292;&#23454;&#29616;&#26080;&#20154;&#26426;&#30340;&#23548;&#33322;&#21644;&#36991;&#38556;&#12290;&#31995;&#32479;&#20351;&#29992;&#28145;&#24230;&#22270;&#20687;&#29983;&#25104;&#21160;&#24577;&#38556;&#30861;&#29289;&#21306;&#22495;&#65292;&#24182;&#21033;&#29992;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#21644;&#36830;&#32493;&#24615;&#28388;&#27874;&#22120;&#36319;&#36394;&#38556;&#30861;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25317;&#25380;&#30340;&#31354;&#38388;&#20013;&#65292;&#23454;&#26102;&#21160;&#24577;&#29615;&#22659;&#24863;&#30693;&#23545;&#20110;&#33258;&#20027;&#26426;&#22120;&#20154;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#27969;&#34892;&#30340;&#22522;&#20110;&#20307;&#32032;&#30340;&#24314;&#22270;&#26041;&#27861;&#21487;&#20197;&#39640;&#25928;&#22320;&#34920;&#31034;&#20855;&#26377;&#20219;&#24847;&#22797;&#26434;&#24418;&#29366;&#30340;3D&#38556;&#30861;&#29289;&#65292;&#20294;&#23427;&#20204;&#24456;&#38590;&#21306;&#20998;&#38745;&#24577;&#21644;&#21160;&#24577;&#38556;&#30861;&#29289;&#65292;&#23548;&#33268;&#36991;&#38556;&#24615;&#33021;&#26377;&#38480;&#12290;&#34429;&#28982;&#33258;&#20027;&#39550;&#39542;&#20013;&#23384;&#22312;&#35768;&#22810;&#22797;&#26434;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#21160;&#24577;&#38556;&#30861;&#29289;&#26816;&#27979;&#31639;&#27861;&#65292;&#20294;&#22235;&#26059;&#32764;&#39134;&#34892;&#22120;&#30340;&#26377;&#38480;&#35745;&#31639;&#36164;&#28304;&#26080;&#27861;&#20351;&#29992;&#36825;&#20123;&#26041;&#27861;&#23454;&#29616;&#23454;&#26102;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;RGB-D&#30456;&#26426;&#30340;&#22235;&#26059;&#32764;&#39134;&#34892;&#22120;&#36991;&#38556;&#30340;&#23454;&#26102;&#21160;&#24577;&#38556;&#30861;&#29289;&#36319;&#36394;&#21644;&#24314;&#22270;&#31995;&#32479;&#12290;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#39318;&#20808;&#21033;&#29992;&#24102;&#26377;&#21344;&#25454;&#20307;&#32032;&#22320;&#22270;&#30340;&#28145;&#24230;&#22270;&#20687;&#29983;&#25104;&#28508;&#22312;&#30340;&#21160;&#24577;&#38556;&#30861;&#29289;&#21306;&#22495;&#20316;&#20026;&#20505;&#36873;&#12290;&#28982;&#21518;&#65292;&#21033;&#29992;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#21644;&#25105;&#20204;&#30340;&#36830;&#32493;&#24615;&#28388;&#27874;&#22120;&#26469;&#36319;&#36394;&#27599;&#20010;&#21160;&#24577;&#38556;&#30861;&#29289;&#12290;&#26368;&#21518;&#65292;&#20351;&#29992;&#29615;&#22659;&#27169;&#22411;&#21644;&#36335;&#24452;&#35268;&#21010;&#31639;&#27861;&#25552;&#20379;&#23548;&#33322;&#21644;&#36991;&#38556;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
The real-time dynamic environment perception has become vital for autonomous robots in crowded spaces. Although the popular voxel-based mapping methods can efficiently represent 3D obstacles with arbitrarily complex shapes, they can hardly distinguish between static and dynamic obstacles, leading to the limited performance of obstacle avoidance. While plenty of sophisticated learning-based dynamic obstacle detection algorithms exist in autonomous driving, the quadcopter's limited computation resources cannot achieve real-time performance using those approaches. To address these issues, we propose a real-time dynamic obstacle tracking and mapping system for quadcopter obstacle avoidance using an RGB-D camera. The proposed system first utilizes a depth image with an occupancy voxel map to generate potential dynamic obstacle regions as proposals. With the obstacle region proposals, the Kalman filter and our continuity filter are applied to track each dynamic obstacle. Finally, the environ
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;B&#26679;&#26465;&#36712;&#36857;&#20248;&#21270;&#31639;&#27861;&#65292;&#21033;&#29992;&#26426;&#22120;&#20154;&#30340;&#20869;&#32622;&#35270;&#35273;&#65292;&#23454;&#29616;&#20102;&#26080;&#20154;&#26426;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#30340;&#23548;&#33322;&#21644;&#21160;&#24577;&#36991;&#38556;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2209.07003</link><description>&lt;p&gt;
&#22522;&#20110;&#26799;&#24230;&#30340;B&#26679;&#26465;&#36712;&#36857;&#20248;&#21270;&#30340;&#35270;&#35273;&#36741;&#21161;&#26080;&#20154;&#26426;&#23548;&#33322;&#21644;&#21160;&#24577;&#36991;&#38556;
&lt;/p&gt;
&lt;p&gt;
Vision-aided UAV navigation and dynamic obstacle avoidance using gradient-based B-spline trajectory optimization. (arXiv:2209.07003v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.07003
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;B&#26679;&#26465;&#36712;&#36857;&#20248;&#21270;&#31639;&#27861;&#65292;&#21033;&#29992;&#26426;&#22120;&#20154;&#30340;&#20869;&#32622;&#35270;&#35273;&#65292;&#23454;&#29616;&#20102;&#26080;&#20154;&#26426;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#30340;&#23548;&#33322;&#21644;&#21160;&#24577;&#36991;&#38556;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#36827;&#34892;&#23548;&#33322;&#38656;&#35201;&#26426;&#22120;&#20154;&#29983;&#25104;&#26080;&#30896;&#25758;&#36712;&#36857;&#24182;&#20027;&#21160;&#36991;&#24320;&#31227;&#21160;&#38556;&#30861;&#29289;&#12290;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#30740;&#31350;&#37117;&#35774;&#35745;&#20102;&#22522;&#20110;&#21333;&#19968;&#22320;&#22270;&#34920;&#31034;&#30340;&#36335;&#24452;&#35268;&#21010;&#31639;&#27861;&#65292;&#22914;&#20960;&#20309;&#22320;&#22270;&#12289;&#21344;&#29992;&#22320;&#22270;&#25110;ESDF&#22320;&#22270;&#12290;&#23613;&#31649;&#23427;&#20204;&#22312;&#38745;&#24577;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#25104;&#21151;&#65292;&#20294;&#30001;&#20110;&#22320;&#22270;&#34920;&#31034;&#30340;&#38480;&#21046;&#65292;&#36825;&#20123;&#26041;&#27861;&#19981;&#33021;&#21487;&#38752;&#22320;&#21516;&#26102;&#22788;&#29702;&#38745;&#24577;&#21644;&#21160;&#24577;&#38556;&#30861;&#29289;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;B&#26679;&#26465;&#36712;&#36857;&#20248;&#21270;&#31639;&#27861;&#65292;&#21033;&#29992;&#26426;&#22120;&#20154;&#30340;&#20869;&#32622;&#35270;&#35273;&#12290;&#28145;&#24230;&#35270;&#35273;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#22522;&#20110;&#20307;&#32032;&#22320;&#22270;&#23545;&#21160;&#24577;&#29289;&#20307;&#36827;&#34892;&#20960;&#20309;&#36319;&#36394;&#21644;&#34920;&#31034;&#12290;&#25552;&#20986;&#30340;&#20248;&#21270;&#39318;&#20808;&#37319;&#29992;&#22522;&#20110;&#22278;&#30340;&#23548;&#21521;&#28857;&#31639;&#27861;&#26469;&#36817;&#20284;&#36991;&#24320;&#38745;&#24577;&#38556;&#30861;&#29289;&#30340;&#20195;&#20215;&#21644;&#26799;&#24230;&#12290;&#28982;&#21518;&#65292;&#21033;&#29992;&#35270;&#35273;&#26816;&#27979;&#21040;&#30340;&#31227;&#21160;&#29289;&#20307;&#65292;&#25105;&#20204;&#21516;&#26102;&#20351;&#29992;&#22238;&#36864;&#35270;&#37326;&#36317;&#31163;&#22330;&#26469;&#38450;&#27490;&#21160;&#24577;&#30896;&#25758;&#12290;&#26368;&#21518;&#65292;
&lt;/p&gt;
&lt;p&gt;
Navigating dynamic environments requires the robot to generate collision-free trajectories and actively avoid moving obstacles. Most previous works designed path planning algorithms based on one single map representation, such as the geometric, occupancy, or ESDF map. Although they have shown success in static environments, due to the limitation of map representation, those methods cannot reliably handle static and dynamic obstacles simultaneously. To address the problem, this paper proposes a gradient-based B-spline trajectory optimization algorithm utilizing the robot's onboard vision. The depth vision enables the robot to track and represent dynamic objects geometrically based on the voxel map. The proposed optimization first adopts the circle-based guide-point algorithm to approximate the costs and gradients for avoiding static obstacles. Then, with the vision-detected moving objects, our receding-horizon distance field is simultaneously used to prevent dynamic collisions. Finally,
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#23545;&#35937;&#26816;&#27979;&#26041;&#27861;&#26367;&#20195;&#20102;&#21306;&#22495;&#20998;&#21106;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#24067;&#23616;&#20998;&#26512;&#26041;&#27861;&#12290;&#36890;&#36807;&#19982;Kraken&#21644;YOLOv5&#23545;&#27604;&#65292;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;YOLOv5&#22312;&#23567;&#25968;&#25454;&#38598;&#19978;&#30340;&#20998;&#21106;&#25928;&#26524;&#26174;&#33879;&#20248;&#20110;Kraken&#12290;&#30740;&#31350;&#36824;&#21457;&#24067;&#20102;&#20004;&#20010;&#21382;&#21490;&#25991;&#26723;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#26032;&#30340;p...</title><link>http://arxiv.org/abs/2207.11230</link><description>&lt;p&gt;
&#20351;&#29992;&#23545;&#35937;&#26816;&#27979;&#26041;&#27861;&#32780;&#38750;&#21306;&#22495;&#20998;&#21106;&#22312;Kraken&#24341;&#25806;&#20013;&#36827;&#34892;&#24067;&#23616;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
You Actually Look Twice At it (YALTAi): using an object detection approach instead of region segmentation within the Kraken engine. (arXiv:2207.11230v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.11230
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#23545;&#35937;&#26816;&#27979;&#26041;&#27861;&#26367;&#20195;&#20102;&#21306;&#22495;&#20998;&#21106;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#24067;&#23616;&#20998;&#26512;&#26041;&#27861;&#12290;&#36890;&#36807;&#19982;Kraken&#21644;YOLOv5&#23545;&#27604;&#65292;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;YOLOv5&#22312;&#23567;&#25968;&#25454;&#38598;&#19978;&#30340;&#20998;&#21106;&#25928;&#26524;&#26174;&#33879;&#20248;&#20110;Kraken&#12290;&#30740;&#31350;&#36824;&#21457;&#24067;&#20102;&#20004;&#20010;&#21382;&#21490;&#25991;&#26723;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#26032;&#30340;p...
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24067;&#23616;&#20998;&#26512;&#26159;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#31561;&#20219;&#21153;&#20013;&#32447;&#20998;&#21106;&#30340;&#31532;&#19968;&#27493;&#65292;&#23427;&#26159;&#35782;&#21035;&#21306;&#22495;&#21644;&#36827;&#34892;&#20998;&#31867;&#30340;&#36807;&#31243;&#12290;&#20174;&#36793;&#32536;&#25991;&#26412;&#25110;&#26631;&#39064;&#20013;&#35782;&#21035;&#27491;&#25991;&#26159;&#25552;&#21462;&#25968;&#23383;&#21270;&#20070;&#31821;&#20840;&#25991;&#21644;&#20135;&#29983;&#22122;&#38899;&#36755;&#20986;&#20043;&#38388;&#30340;&#21306;&#21035;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22823;&#22810;&#25968;&#20998;&#21106;&#22120;&#37117;&#19987;&#27880;&#20110;&#20687;&#32032;&#20998;&#31867;&#65292;&#32780;&#19988;&#23613;&#31649;&#22312;2010&#24180;&#20195;&#21021;&#26399;&#26159;&#37325;&#28857;&#30740;&#31350;&#39046;&#22495;&#65292;&#20294;&#23558;&#27492;&#36755;&#20986;&#30340;&#22810;&#36793;&#24418;&#21270;&#20316;&#20026;&#21382;&#21490;&#25991;&#26723;&#26368;&#26032;&#31454;&#36187;&#65288;ICDAR 2017&#21450;&#20197;&#21518;&#65289;&#30340;&#30446;&#26631;&#23578;&#26410;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#12290;&#20026;&#20102;&#22312;&#25928;&#29575;&#19978;&#25913;&#36827;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#20219;&#21153;&#20174;&#22522;&#20110;&#20687;&#32032;&#20998;&#31867;&#30340;&#22810;&#36793;&#24418;&#21270;&#36716;&#21464;&#20026;&#20351;&#29992;&#31561;&#23485;&#30697;&#24418;&#30340;&#23545;&#35937;&#26816;&#27979;&#12290;&#36890;&#36807;&#23545;&#27604;Kraken&#21644;YOLOv5&#22312;&#20998;&#21106;&#26041;&#38754;&#30340;&#36755;&#20986;&#65292;&#25105;&#20204;&#21457;&#29616;&#21518;&#32773;&#22312;&#23567;&#25968;&#25454;&#38598;&#65288;1110&#20010;&#26679;&#26412;&#21450;&#20197;&#19979;&#65289;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#20004;&#20010;&#21382;&#21490;&#25991;&#26723;&#22521;&#35757;&#21644;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#19968;&#20010;&#26032;&#30340;p...
&lt;/p&gt;
&lt;p&gt;
Layout Analysis (the identification of zones and their classification) is the first step along line segmentation in Optical Character Recognition and similar tasks. The ability of identifying main body of text from marginal text or running titles makes the difference between extracting the work full text of a digitized book and noisy outputs. We show that most segmenters focus on pixel classification and that polygonization of this output has not been used as a target for the latest competition on historical document (ICDAR 2017 and onwards), despite being the focus in the early 2010s. We propose to shift, for efficiency, the task from a pixel classification-based polygonization to an object detection using isothetic rectangles. We compare the output of Kraken and YOLOv5 in terms of segmentation and show that the later severely outperforms the first on small datasets (1110 samples and below). We release two datasets for training and evaluation on historical documents as well as a new p
&lt;/p&gt;</description></item><item><title>AlphaZero-style reinforcement learning algorithms excel in various board games but face challenges with impartial games. The researchers present a concrete example of the game nim, and show that AlphaZero-style algorithms have difficulty learning these impartial games on larger board sizes. The difference between impartial games and partisan games can be explained by the vulnerability to adversarial attacks and perturbations.</title><link>http://arxiv.org/abs/2205.12787</link><description>&lt;p&gt;
&#20844;&#27491;&#28216;&#25103;&#65306;&#23545;&#24378;&#21270;&#23398;&#20064;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Impartial Games: A Challenge for Reinforcement Learning. (arXiv:2205.12787v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.12787
&lt;/p&gt;
&lt;p&gt;
AlphaZero-style reinforcement learning algorithms excel in various board games but face challenges with impartial games. The researchers present a concrete example of the game nim, and show that AlphaZero-style algorithms have difficulty learning these impartial games on larger board sizes. The difference between impartial games and partisan games can be explained by the vulnerability to adversarial attacks and perturbations.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#20284;AlphaZero&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#21508;&#31181;&#26827;&#30424;&#28216;&#25103;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#20844;&#27491;&#28216;&#25103;&#20013;&#21364;&#38754;&#20020;&#25361;&#25112;&#65292;&#36825;&#20123;&#28216;&#25103;&#20013;&#29609;&#23478;&#20849;&#20139;&#26827;&#23376;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20855;&#20307;&#30340;&#28216;&#25103;&#20363;&#23376;&#65292;&#21363;&#23567;&#23401;&#20204;&#29609;&#30340;&#23612;&#22982;&#28216;&#25103;&#65292;&#20197;&#21450;&#20854;&#20182;&#19968;&#20123;&#20844;&#27491;&#28216;&#25103;&#65292;&#36825;&#20123;&#28216;&#25103;&#20284;&#20046;&#25104;&#20026;AlphaZero&#21644;&#31867;&#20284;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#32458;&#33050;&#30707;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#19982;&#26368;&#36817;&#30340;&#30740;&#31350;&#19968;&#33268;&#65292;&#34920;&#26126;AlphaZero-style&#31639;&#27861;&#23481;&#26131;&#21463;&#21040;&#25932;&#23545;&#25915;&#20987;&#21644;&#25932;&#23545;&#25200;&#21160;&#30340;&#24433;&#21709;&#65292;&#26174;&#31034;&#20102;&#22312;&#25152;&#26377;&#21512;&#27861;&#29366;&#24577;&#19979;&#23398;&#20064;&#25484;&#25569;&#36825;&#20123;&#28216;&#25103;&#30340;&#22256;&#38590;&#12290;&#25105;&#20204;&#21457;&#29616;&#23612;&#22982;&#28216;&#25103;&#22312;&#23567;&#22411;&#26827;&#30424;&#19978;&#21487;&#20197;&#23398;&#20064;&#65292;&#20294;&#24403;&#26827;&#30424;&#23610;&#23544;&#22686;&#22823;&#26102;&#65292;AlphaZero-style&#31639;&#27861;&#30340;&#23398;&#20064;&#36895;&#24230;&#26174;&#33879;&#20943;&#24930;&#12290;&#30452;&#35266;&#19978;&#65292;&#23612;&#22982;&#31561;&#20844;&#27491;&#28216;&#25103;&#19982;&#35937;&#26827;&#21644;&#22260;&#26827;&#31561;&#20826;&#27966;&#28216;&#25103;&#20043;&#38388;&#30340;&#21306;&#21035;&#22312;&#20110;&#65292;&#22914;&#26524;&#31995;&#32479;&#20013;&#28155;&#21152;&#20102;&#24494;&#23567;&#30340;&#22122;&#38899;&#65288;&#20363;&#22914;&#65292;&#26827;&#30424;&#30340;&#19968;&#23567;&#37096;&#20998;&#34987;&#35206;&#30422;&#65289;&#65292;&#23545;&#20110;&#20844;&#27491;&#28216;&#25103;&#26469;&#35828;&#65292;&#36825;&#26159;&#19968;&#31181;&#20856;&#22411;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
AlphaZero-style reinforcement learning (RL) algorithms excel in various board games but face challenges with impartial games, where players share pieces. We present a concrete example of a game - namely the children's game of nim - and other impartial games that seem to be a stumbling block for AlphaZero-style and similar reinforcement learning algorithms.  Our findings are consistent with recent studies showing that AlphaZero-style algorithms are vulnerable to adversarial attacks and adversarial perturbations, showing the difficulty of learning to master the games in all legal states.  We show that nim can be learned on small boards, but AlphaZero-style algorithms learning dramatically slows down when the board size increases. Intuitively, the difference between impartial games like nim and partisan games like Chess and Go can be explained by the fact that if a tiny amount of noise is added to the system (e.g. if a small part of the board is covered), for impartial games, it is typica
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;TeleGraph&#65292;&#29992;&#20110;&#35780;&#20272;&#21644;&#20419;&#36827;&#38142;&#25509;&#25512;&#26029;&#25216;&#26415;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#36817;&#20046;&#26641;&#29366;&#30340;&#32593;&#32476;&#19978;&#65292;&#29616;&#26377;&#30340;&#38142;&#25509;&#39044;&#27979;&#31639;&#27861;&#22823;&#22810;&#25968;&#26080;&#27861;&#20135;&#29983;&#28385;&#24847;&#30340;&#24615;&#33021;&#65292;&#36825;&#23545;&#23454;&#36341;&#20013;&#30340;&#38142;&#25509;&#39044;&#27979;&#31639;&#27861;&#35774;&#35745;&#21644;&#37096;&#32626;&#25552;&#20986;&#20102;&#29305;&#27530;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2204.07703</link><description>&lt;p&gt;
TeleGraph: &#29992;&#20110;&#23618;&#27425;&#38142;&#25509;&#39044;&#27979;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
TeleGraph: A Benchmark Dataset for Hierarchical Link Prediction. (arXiv:2204.07703v2 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.07703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;TeleGraph&#65292;&#29992;&#20110;&#35780;&#20272;&#21644;&#20419;&#36827;&#38142;&#25509;&#25512;&#26029;&#25216;&#26415;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#36817;&#20046;&#26641;&#29366;&#30340;&#32593;&#32476;&#19978;&#65292;&#29616;&#26377;&#30340;&#38142;&#25509;&#39044;&#27979;&#31639;&#27861;&#22823;&#22810;&#25968;&#26080;&#27861;&#20135;&#29983;&#28385;&#24847;&#30340;&#24615;&#33021;&#65292;&#36825;&#23545;&#23454;&#36341;&#20013;&#30340;&#38142;&#25509;&#39044;&#27979;&#31639;&#27861;&#35774;&#35745;&#21644;&#37096;&#32626;&#25552;&#20986;&#20102;&#29305;&#27530;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38142;&#25509;&#39044;&#27979;&#26159;&#32593;&#32476;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#30001;&#20110;&#20854;&#21508;&#31181;&#24212;&#29992;&#32780;&#21560;&#24341;&#20102;&#22823;&#37327;&#30340;&#30740;&#31350;&#24037;&#20316;&#12290;&#30446;&#21069;&#30340;&#38142;&#25509;&#39044;&#27979;&#26041;&#27861;&#38024;&#23545;&#19968;&#33324;&#32593;&#32476;&#65292;&#24182;&#19988;&#36807;&#24230;&#20381;&#36182;&#20110;&#32593;&#32476;&#30340;&#23553;&#38381;&#19977;&#35282;&#24418;&#32467;&#26500;&#25110;&#33410;&#28857;&#23646;&#24615;&#12290;&#23427;&#20204;&#22312;&#31232;&#30095;&#25110;&#39640;&#24230;&#20998;&#23618;&#30340;&#32593;&#32476;&#19978;&#30340;&#24615;&#33021;&#23578;&#26410;&#24471;&#21040;&#24456;&#22909;&#22320;&#30740;&#31350;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#29616;&#26377;&#30340;&#31867;&#20284;&#26641;&#29366;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#35201;&#20040;&#26159;&#27169;&#25311;&#30340;&#65292;&#33410;&#28857;&#20449;&#24687;&#26377;&#38480;&#65292;&#35201;&#20040;&#35268;&#27169;&#36739;&#23567;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;TeleGraph&#65292;&#36825;&#26159;&#19968;&#20010;&#39640;&#24230;&#31232;&#30095;&#19988;&#23618;&#27425;&#32467;&#26500;&#30340;&#30005;&#20449;&#32593;&#32476;&#65292;&#20851;&#32852;&#30528;&#20016;&#23500;&#30340;&#33410;&#28857;&#23646;&#24615;&#65292;&#29992;&#20110;&#35780;&#20272;&#21644;&#20419;&#36827;&#38142;&#25509;&#25512;&#26029;&#25216;&#26415;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#22823;&#22810;&#25968;&#31639;&#27861;&#22312;&#36817;&#20046;&#26641;&#29366;&#30340;&#25968;&#25454;&#38598;&#19978;&#26080;&#27861;&#20135;&#29983;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#65292;&#22312;&#23454;&#36341;&#20013;&#35774;&#35745;&#25110;&#37096;&#32626;&#38142;&#25509;&#39044;&#27979;&#31639;&#27861;&#26102;&#38656;&#35201;&#29305;&#21035;&#27880;&#24847;&#12290;
&lt;/p&gt;
&lt;p&gt;
Link prediction is a key problem for network-structured data, attracting considerable research efforts owing to its diverse applications. The current link prediction methods focus on general networks and are overly dependent on either the closed triangular structure of networks or node attributes. Their performance on sparse or highly hierarchical networks has not been well studied. On the other hand, the available tree-like benchmark datasets are either simulated, with limited node information, or small in scale. To bridge this gap, we present a new benchmark dataset TeleGraph, a highly sparse and hierarchical telecommunication network associated with rich node attributes, for assessing and fostering the link inference techniques. Our empirical results suggest that most of the algorithms fail to produce a satisfactory performance on a nearly tree-like dataset, which calls for special attention when designing or deploying the link prediction algorithm in practice.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#23454;&#39564;&#21457;&#29616;&#65292;&#20154;&#20204;&#22312;&#21028;&#26029;&#25919;&#27835;&#28436;&#35762;&#30340;&#30495;&#23454;&#24615;&#26102;&#65292;&#38899;&#39057;&#21644;&#35270;&#35273;&#20449;&#24687;&#23545;&#20110;&#20934;&#30830;&#21306;&#20998;&#30495;&#23454;&#21644;&#20266;&#36896;&#30340;Deepfakes&#26356;&#20026;&#37325;&#35201;&#65292;&#32780;&#38169;&#35823;&#20449;&#24687;&#30340;&#22522;&#20934;&#24433;&#21709;&#36739;&#23567;&#12290;</title><link>http://arxiv.org/abs/2202.12883</link><description>&lt;p&gt;
&#36328;&#36716;&#24405;&#26412;&#12289;&#38899;&#39057;&#21644;&#35270;&#39057;&#30340;&#25919;&#27835;&#28436;&#35762;Deepfakes&#30340;&#20154;&#31867;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Human Detection of Political Speech Deepfakes across Transcripts, Audio, and Video. (arXiv:2202.12883v3 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.12883
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#23454;&#39564;&#21457;&#29616;&#65292;&#20154;&#20204;&#22312;&#21028;&#26029;&#25919;&#27835;&#28436;&#35762;&#30340;&#30495;&#23454;&#24615;&#26102;&#65292;&#38899;&#39057;&#21644;&#35270;&#35273;&#20449;&#24687;&#23545;&#20110;&#20934;&#30830;&#21306;&#20998;&#30495;&#23454;&#21644;&#20266;&#36896;&#30340;Deepfakes&#26356;&#20026;&#37325;&#35201;&#65292;&#32780;&#38169;&#35823;&#20449;&#24687;&#30340;&#22522;&#20934;&#24433;&#21709;&#36739;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25216;&#26415;&#30340;&#39134;&#36895;&#36827;&#27493;&#20351;&#24471;&#36229;&#36924;&#30495;&#30340;&#35270;&#35273;&#25928;&#26524;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#28145;&#24230;&#20266;&#36896;&#30340;&#25919;&#27835;&#28436;&#35762;&#35270;&#39057;&#26159;&#21542;&#24456;&#24555;&#23601;&#20250;&#19982;&#30495;&#23454;&#24405;&#20687;&#26080;&#27861;&#21306;&#20998;&#30340;&#25285;&#24551;&#12290;&#20256;&#25773;&#29702;&#35770;&#20013;&#30340;&#24120;&#35782;&#39044;&#27979;&#65292;&#24403;&#21516;&#19968;&#20010;&#25925;&#20107;&#20197;&#35270;&#39057;&#24418;&#24335;&#21644;&#25991;&#26412;&#24418;&#24335;&#23637;&#31034;&#26102;&#65292;&#20154;&#20204;&#26356;&#23481;&#26131;&#19978;&#24403;&#21463;&#39575;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;4&#20010;&#39044;&#20808;&#30331;&#35760;&#30340;&#38543;&#26426;&#23454;&#39564;&#65292;&#28041;&#21450;2015&#21517;&#21442;&#19982;&#32773;&#65292;&#20197;&#35780;&#20272;&#20154;&#31867;&#22312;&#19981;&#21516;&#30340;&#38169;&#35823;&#20449;&#24687;&#22522;&#20934;&#12289;&#38899;&#39057;&#26469;&#28304;&#21644;&#23186;&#20307;&#24418;&#24335;&#19979;&#65292;&#26159;&#21542;&#33021;&#22815;&#20934;&#30830;&#21306;&#20998;&#30495;&#23454;&#30340;&#25919;&#27835;&#28436;&#35762;&#21644;&#20266;&#36896;&#30340;&#28436;&#35762;&#12290;&#25105;&#20204;&#21457;&#29616;&#38169;&#35823;&#20449;&#24687;&#30340;&#22522;&#20934;&#23545;&#36776;&#21035;&#21028;&#26029;&#24433;&#21709;&#36739;&#23567;&#65292;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#25991;&#26412;&#36716;&#35821;&#38899;&#31639;&#27861;&#29983;&#25104;&#30340;&#24102;&#26377;&#38899;&#39057;&#30340;Deepfakes&#36739;&#20351;&#29992;&#22768;&#38899;&#28436;&#21592;&#38899;&#39057;&#30340;&#30456;&#21516;Deepfakes&#26356;&#38590;&#36776;&#21035;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#38899;&#39057;&#21644;&#35270;&#35273;&#20449;&#24687;&#27604;&#20165;&#26377;&#25991;&#26412;&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#36827;&#34892;&#36776;&#21035;&#65306;&#20154;&#31867;&#30340;&#36776;&#21035;&#26356;&#20381;&#36182;&#20110;&#20107;&#29289;&#26159;&#22914;&#20309;&#34987;&#34920;&#36798;&#30340;&#65292;&#38899;&#39057;-&#35270;&#35273;
&lt;/p&gt;
&lt;p&gt;
Recent advances in technology for hyper-realistic visual effects provoke the concern that deepfake videos of political speeches will soon be visually indistinguishable from authentic video recordings. The conventional wisdom in communication theory predicts people will fall for fake news more often when the same version of a story is presented as a video versus text. We conduct 4 pre-registered randomized experiments with 2,015 participants to evaluate how accurately humans distinguish real political speeches from fabrications across base rates of misinformation, audio sources, and media modalities. We find base rates of misinformation minimally influence discernment and deepfakes with audio produced by the state-of-the-art text-to-speech algorithms are harder to discern than the same deepfakes with voice actor audio. Moreover, we find audio and visual information enables more accurate discernment than text alone: human discernment relies more on how something is said, the audio-visual
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#31995;&#32479;&#65292;&#36890;&#36807;&#31232;&#30095;&#31361;&#21457;&#31070;&#32463;&#31361;&#35302;&#65292;&#22312;&#39046;&#22495;&#36716;&#31227;&#20013;&#33021;&#22815;&#22312;&#20808;&#21069;&#26410;&#35265;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#25345;&#32493;&#23398;&#20064;&#65292;&#20943;&#23569;&#36951;&#24536;&#12290;</title><link>http://arxiv.org/abs/2108.12056</link><description>&lt;p&gt;
&#22312;&#31232;&#30095;&#31361;&#21457;&#31070;&#32463;&#31361;&#35302;&#19979;&#30340;&#39046;&#22495;&#36716;&#31227;&#20013;&#30340;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Continual learning under domain transfer with sparse synaptic bursting. (arXiv:2108.12056v9 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.12056
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#31995;&#32479;&#65292;&#36890;&#36807;&#31232;&#30095;&#31361;&#21457;&#31070;&#32463;&#31361;&#35302;&#65292;&#22312;&#39046;&#22495;&#36716;&#31227;&#20013;&#33021;&#22815;&#22312;&#20808;&#21069;&#26410;&#35265;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#25345;&#32493;&#23398;&#20064;&#65292;&#20943;&#23569;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#26426;&#22120;&#26159;&#21151;&#33021;&#29305;&#23450;&#30340;&#24037;&#20855;&#65292;&#26131;&#20110;&#39044;&#27979;&#21644;&#25511;&#21046;&#12290;&#26126;&#22825;&#30340;&#26426;&#22120;&#21487;&#33021;&#26356;&#25509;&#36817;&#29983;&#29289;&#31995;&#32479;&#65292;&#20855;&#26377;&#21487;&#21464;&#24615;&#12289;&#38887;&#24615;&#21644;&#33258;&#20027;&#24615;&#12290;&#20294;&#26159;&#39318;&#20808;&#65292;&#23427;&#20204;&#24517;&#39035;&#33021;&#22815;&#23398;&#20064;&#21644;&#20445;&#30041;&#26032;&#20449;&#24687;&#65292;&#32780;&#19981;&#24517;&#38543;&#26426;&#25509;&#35302;&#23427;&#12290;&#36807;&#21435;&#35774;&#35745;&#36825;&#26679;&#30340;&#31995;&#32479;&#30340;&#21162;&#21147;&#26159;&#36890;&#36807;&#26500;&#24314;&#25110;&#35843;&#33410;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65292;&#20351;&#29992;&#21807;&#19968;&#25935;&#24863;&#20110;&#29305;&#23450;&#20219;&#21153;&#25110;&#36755;&#20837;&#30340;&#19981;&#30456;&#20132;&#30340;&#26435;&#37325;&#38598;&#21512;&#12290;&#28982;&#32780;&#65292;&#36825;&#23578;&#26410;&#23454;&#29616;&#22312;&#38271;&#26102;&#38388;&#30340;&#20808;&#21069;&#26410;&#35265;&#25968;&#25454;&#24207;&#21015;&#19978;&#36827;&#34892;&#25345;&#32493;&#23398;&#20064;&#32780;&#19981;&#30772;&#22351;&#29616;&#26377;&#30693;&#35782;&#30340;&#38382;&#39064;&#65292;&#36825;&#34987;&#31216;&#20026;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#31995;&#32479;&#65292;&#21487;&#20197;&#22312;&#20808;&#21069;&#26410;&#35265;&#30340;&#25968;&#25454;&#38598;&#65288;ImageNet&#65292;CIFAR-100&#65289;&#19978;&#20197;&#36739;&#23567;&#30340;&#36951;&#24536;&#36880;&#27493;&#23398;&#20064;&#12290;&#36825;&#26159;&#36890;&#36807;&#22522;&#20110;&#36755;&#20837;&#25511;&#21046;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#26435;&#37325;&#30340;&#27963;&#21160;&#65292;&#20351;&#29992;&#30001;&#31532;&#20108;&#20010;&#21069;&#39304;&#29983;&#25104;&#30340;&#33258;&#19978;&#32780;&#19979;&#35843;&#33410;&#26469;&#23454;&#29616;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing machines are functionally specific tools that were made for easy prediction and control. Tomorrow's machines may be closer to biological systems in their mutability, resilience, and autonomy. But first they must be capable of learning and retaining new information without being exposed to it arbitrarily often. Past efforts to engineer such systems have sought to build or regulate artificial neural networks using disjoint sets of weights that are uniquely sensitive to specific tasks or inputs. This has not yet enabled continual learning over long sequences of previously unseen data without corrupting existing knowledge: a problem known as catastrophic forgetting. In this paper, we introduce a system that can learn sequentially over previously unseen datasets (ImageNet, CIFAR-100) with little forgetting over time. This is done by controlling the activity of weights in a convolutional neural network on the basis of inputs using top-down regulation generated by a second feed-forwa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#25968;&#25454;&#27969;&#19978;&#36827;&#34892;&#36830;&#32493;&#26597;&#35810;&#26102;&#21487;&#33021;&#20986;&#29616;&#30340;&#38459;&#22622;&#25805;&#20316;&#21644;&#24310;&#36831;&#31572;&#26696;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#25552;&#20379;&#20551;&#35774;&#31572;&#26696;&#30340;&#35821;&#20041;&#21644;&#30456;&#24212;&#31572;&#26696;&#30340;&#22312;&#32447;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/1905.09610</link><description>&lt;p&gt;
&#22312;&#25968;&#25454;&#27969;&#19978;&#36827;&#34892;&#36830;&#32493;&#26597;&#35810;&#30340;&#20551;&#35774;&#31572;&#26696;&#25506;&#35752;
&lt;/p&gt;
&lt;p&gt;
Hypothetical answers to continuous queries over data streams. (arXiv:1905.09610v3 [cs.PL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1905.09610
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#25968;&#25454;&#27969;&#19978;&#36827;&#34892;&#36830;&#32493;&#26597;&#35810;&#26102;&#21487;&#33021;&#20986;&#29616;&#30340;&#38459;&#22622;&#25805;&#20316;&#21644;&#24310;&#36831;&#31572;&#26696;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#25552;&#20379;&#20551;&#35774;&#31572;&#26696;&#30340;&#35821;&#20041;&#21644;&#30456;&#24212;&#31572;&#26696;&#30340;&#22312;&#32447;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#25454;&#27969;&#19978;&#36827;&#34892;&#36830;&#32493;&#26597;&#35810;&#21487;&#33021;&#36935;&#21040;&#38459;&#22622;&#25805;&#20316;&#21644;/&#25110;&#26080;&#38480;&#31561;&#24453;&#30340;&#38382;&#39064;&#65292;&#36825;&#21487;&#33021;&#20250;&#24310;&#36831;&#31572;&#26696;&#30452;&#21040;&#30456;&#20851;&#36755;&#20837;&#36890;&#36807;&#25968;&#25454;&#27969;&#21040;&#36798;&#12290;&#36825;&#20123;&#24310;&#36831;&#21487;&#33021;&#23548;&#33268;&#31572;&#26696;&#22312;&#21040;&#36798;&#26102;&#23545;&#20110;&#26377;&#26102;&#19981;&#24471;&#19981;&#19981;&#20381;&#36182;&#20219;&#20309;&#24110;&#21161;&#20570;&#20986;&#20915;&#31574;&#30340;&#29992;&#25143;&#26469;&#35828;&#24050;&#32463;&#36807;&#26102;&#12290;&#22240;&#27492;&#65292;&#25552;&#20379;&#20551;&#35774;&#31572;&#26696;-&#8220;&#26681;&#25454;&#24403;&#21069;&#20449;&#24687;&#65292;&#22312;&#26102;&#38388;t&#26102;&#65292;X&#21487;&#33021;&#20250;&#21464;&#20026;&#30495;&#8221;&#32780;&#19981;&#26159;&#27809;&#26377;&#20219;&#20309;&#20449;&#24687;&#21487;&#33021;&#26159;&#26377;&#29992;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28085;&#30422;&#20102;&#36825;&#31181;&#20551;&#35774;&#31572;&#26696;&#30340;&#26597;&#35810;&#21644;&#30456;&#24212;&#31572;&#26696;&#30340;&#35821;&#20041;&#65292;&#20197;&#21450;&#19968;&#31181;&#22312;&#32447;&#31639;&#27861;&#65292;&#29992;&#20110;&#26356;&#26032;&#19982;&#24403;&#21069;&#21487;&#29992;&#20449;&#24687;&#19968;&#33268;&#30340;&#20107;&#23454;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continuous queries over data streams may suffer from blocking operations and/or unbound wait, which may delay answers until some relevant input arrives through the data stream. These delays may turn answers, when they arrive, obsolete to users who sometimes have to make decisions with no help whatsoever. Therefore, it can be useful to provide hypothetical answers - "given the current information, it is possible that X will become true at time t" instead of no information at all.  In this paper we present a semantics for queries and corresponding answers that covers such hypothetical answers, together with an online algorithm for updating the set of facts that are consistent with the currently available information.
&lt;/p&gt;</description></item><item><title>PVNet&#26159;&#19968;&#31181;&#21033;&#29992;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#25968;&#25454;&#30340;LRCN&#26550;&#26500;&#65292;&#29992;&#20110;&#39044;&#27979;24&#23567;&#26102;&#21644;48&#23567;&#26102;&#30340;&#20809;&#20239;&#21457;&#30005;&#21151;&#29575;&#12290;&#35813;&#27169;&#22411;&#20805;&#20998;&#21033;&#29992;&#20102;&#26102;&#38388;&#21644;&#31354;&#38388;&#22825;&#27668;&#25968;&#25454;&#65292;&#36890;&#36807;&#19982;&#20854;&#20182;&#27169;&#22411;&#30340;&#27604;&#36739;&#65292;&#23637;&#29616;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/1902.01453</link><description>&lt;p&gt;
PVNet: &#22522;&#20110;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#30340;&#26102;&#31354;&#20809;&#20239;&#21457;&#30005;&#21151;&#29575;&#39044;&#27979;&#30340; LRCN &#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
PVNet: A LRCN Architecture for Spatio-Temporal Photovoltaic PowerForecasting from Numerical Weather Prediction. (arXiv:1902.01453v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1902.01453
&lt;/p&gt;
&lt;p&gt;
PVNet&#26159;&#19968;&#31181;&#21033;&#29992;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#25968;&#25454;&#30340;LRCN&#26550;&#26500;&#65292;&#29992;&#20110;&#39044;&#27979;24&#23567;&#26102;&#21644;48&#23567;&#26102;&#30340;&#20809;&#20239;&#21457;&#30005;&#21151;&#29575;&#12290;&#35813;&#27169;&#22411;&#20805;&#20998;&#21033;&#29992;&#20102;&#26102;&#38388;&#21644;&#31354;&#38388;&#22825;&#27668;&#25968;&#25454;&#65292;&#36890;&#36807;&#19982;&#20854;&#20182;&#27169;&#22411;&#30340;&#27604;&#36739;&#65292;&#23637;&#29616;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20809;&#20239;&#21457;&#30005;&#26159;&#19968;&#31181;&#37325;&#35201;&#30340;&#21487;&#20877;&#29983;&#33021;&#28304;&#20043;&#19968;&#65292;&#28982;&#32780;&#20854;&#20135;&#37327;&#21463;&#22825;&#27668;&#26465;&#20214;&#65288;&#22914;&#22826;&#38451;&#36752;&#23556;&#21644;&#28201;&#24230;&#65289;&#30340;&#24433;&#21709;&#32780;&#20855;&#26377;&#39640;&#24230;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#21363;&#20351;&#26159;&#22312;24&#23567;&#26102;&#39044;&#27979;&#20013;&#65292;&#20809;&#20239;&#21457;&#30005;&#30340;&#39044;&#27979;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#23548;&#33268;&#33021;&#28304;&#20379;&#24212;&#21830;&#38656;&#35201;&#21551;&#21160;&#65288;&#24448;&#24448;&#36824;&#20250;&#25490;&#25918;&#30899;&#65289;&#30340;&#21457;&#30005;&#21378;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#65288;NWP&#65289;&#30340;&#38271;&#26399;&#24490;&#29615;&#21367;&#31215;&#32593;&#32476;&#65292;&#29992;&#20110;&#39044;&#27979;24&#23567;&#26102;&#21644;48&#23567;&#26102;&#30340;&#20809;&#20239;&#21457;&#30005;&#39044;&#27979;&#12290;&#35813;&#32593;&#32476;&#26550;&#26500;&#20805;&#20998;&#21033;&#29992;&#20102;&#25972;&#20010;&#24863;&#20852;&#36259;&#22320;&#21306;&#19978;&#37319;&#26679;&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#22825;&#27668;&#25968;&#25454;&#12290;&#25105;&#20204;&#20351;&#29992;&#32654;&#22269;&#22269;&#23478;&#28023;&#27915;&#21644;&#22823;&#27668;&#31649;&#29702;&#23616;&#65288;NOAA&#65289;&#30340;NWP&#25968;&#25454;&#38598;&#23545;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#39044;&#27979;&#24503;&#22269;&#30340;&#31354;&#38388;&#32858;&#21512;&#20809;&#20239;&#21457;&#30005;&#37327;&#65292;&#24182;&#23558;&#20854;&#24615;&#33021;&#19982;&#25345;&#32493;&#27169;&#22411;&#21644;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Photovoltaic (PV) power generation has emerged as one of the lead renewable energy sources. Yet, its production is characterized by high uncertainty, being dependent on weather conditions like solar irradiance and temperature. Predicting PV production, even in the 24-hour forecast, remains a challenge and leads energy providers to left idling - often carbon emitting - plants. In this paper, we introduce a Long-Term Recurrent Convolutional Network using Numerical Weather Predictions (NWP) to predict, in turn, PV production in the 24-hour and 48-hour forecast horizons. This network architecture fully leverages both temporal and spatial weather data, sampled over the whole geographical area of interest. We train our model on an NWP dataset from the National Oceanic and Atmospheric Administration (NOAA) to predict spatially aggregated PV production in Germany. We compare its performance to the persistence model and state-of-the-art methods.
&lt;/p&gt;</description></item></channel></rss>