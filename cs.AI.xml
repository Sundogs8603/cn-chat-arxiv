<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27169;&#22411; $\textbf{TCOW}$&#65292;&#29992;&#20110;&#22312;&#37325;&#24230;&#36974;&#25377;&#21644;&#23481;&#22120;&#20013;&#36827;&#34892;&#35270;&#35273;&#36319;&#36394;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#32452;&#28151;&#21512;&#30340;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;&#20102;&#20004;&#31181;&#26368;&#26032;&#30340;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#35270;&#39057;&#27169;&#22411;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#33021;&#22815;&#20986;&#20154;&#24847;&#26009;&#22320;&#36319;&#36394;&#30446;&#26631;&#65292;&#20294;&#20173;&#23384;&#22312;&#30456;&#24403;&#22823;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#24517;&#39035;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2305.03052</link><description>&lt;p&gt;
&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#36319;&#36394;&#21547;&#23481;&#22120;&#21644;&#36974;&#25377;&#29289;&#30340;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;
Tracking through Containers and Occluders in the Wild. (arXiv:2305.03052v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03052
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27169;&#22411; $\textbf{TCOW}$&#65292;&#29992;&#20110;&#22312;&#37325;&#24230;&#36974;&#25377;&#21644;&#23481;&#22120;&#20013;&#36827;&#34892;&#35270;&#35273;&#36319;&#36394;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#32452;&#28151;&#21512;&#30340;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;&#20102;&#20004;&#31181;&#26368;&#26032;&#30340;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#35270;&#39057;&#27169;&#22411;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#33021;&#22815;&#20986;&#20154;&#24847;&#26009;&#22320;&#36319;&#36394;&#30446;&#26631;&#65292;&#20294;&#20173;&#23384;&#22312;&#30456;&#24403;&#22823;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#24517;&#39035;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26434;&#20081;&#19988;&#21160;&#24577;&#30340;&#29615;&#22659;&#20013;&#36319;&#36394;&#20855;&#26377;&#25345;&#20037;&#24615;&#30340;&#30446;&#26631;&#20173;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#38754;&#20020;&#30340;&#38590;&#39064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27169;&#22411; $\textbf{TCOW}$&#65292;&#29992;&#20110;&#22312;&#37325;&#24230;&#36974;&#25377;&#21644;&#23481;&#22120;&#20013;&#36827;&#34892;&#35270;&#35273;&#36319;&#36394;&#12290;&#25105;&#20204;&#35774;&#23450;&#20102;&#19968;&#20010;&#20219;&#21153;&#65292;&#21363;&#22312;&#32473;&#23450;&#35270;&#39057;&#24207;&#21015;&#30340;&#24773;&#20917;&#19979;&#65292;&#20998;&#21106;&#20986;&#30446;&#26631;&#29289;&#20307;&#30340;&#25237;&#24433;&#33539;&#22260;&#20197;&#21450;&#21608;&#22260;&#30340;&#23481;&#22120;&#25110;&#36974;&#25377;&#29289;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#32452;&#28151;&#21512;&#30340;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#65292;&#20197;&#25903;&#25345;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#21464;&#21270;&#24418;&#24335;&#19979;&#30340;&#30417;&#30563;&#23398;&#20064;&#21644;&#32467;&#26500;&#21270;&#35780;&#20272;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20004;&#31181;&#26368;&#26032;&#30340;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#35270;&#39057;&#27169;&#22411;&#65292;&#24182;&#21457;&#29616;&#23613;&#31649;&#23427;&#20204;&#22312;&#26576;&#20123;&#20219;&#21153;&#21464;&#21270;&#30340;&#35774;&#32622;&#19979;&#33021;&#22815;&#20986;&#20154;&#24847;&#26009;&#22320;&#36319;&#36394;&#30446;&#26631;&#65292;&#20294;&#22312;&#25105;&#20204;&#23459;&#31216;&#19968;&#20010;&#36319;&#36394;&#27169;&#22411;&#24050;&#32463;&#33719;&#24471;&#20102;&#30495;&#27491;&#30340;&#23545;&#35937;&#24658;&#24120;&#24615;&#27010;&#24565;&#20043;&#21069;&#65292;&#20173;&#23384;&#22312;&#30456;&#24403;&#22823;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tracking objects with persistence in cluttered and dynamic environments remains a difficult challenge for computer vision systems. In this paper, we introduce $\textbf{TCOW}$, a new benchmark and model for visual tracking through heavy occlusion and containment. We set up a task where the goal is to, given a video sequence, segment both the projected extent of the target object, as well as the surrounding container or occluder whenever one exists. To study this task, we create a mixture of synthetic and annotated real datasets to support both supervised learning and structured evaluation of model performance under various forms of task variation, such as moving or nested containment. We evaluate two recent transformer-based video models and find that while they can be surprisingly capable of tracking targets under certain settings of task variation, there remains a considerable performance gap before we can claim a tracking model to have acquired a true notion of object permanence.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;SAM&#20010;&#24615;&#21270;&#26041;&#27861;PerSAM&#65292;&#21482;&#38656;&#35201;&#19968;&#24352;&#24102;&#26377;&#21442;&#32771;&#25513;&#27169;&#30340;&#21333;&#24352;&#22270;&#20687;&#21363;&#21487;&#23450;&#20301;&#21644;&#20998;&#21106;&#30446;&#26631;&#27010;&#24565;&#65292;&#36824;&#25552;&#20986;&#20102;&#39640;&#25928;&#30340;&#19968;&#27425;&#24615;&#24494;&#35843;&#21464;&#20307;PerSAM-F&#65292;&#26088;&#22312;&#35299;&#20915;&#25513;&#27169;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.03048</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#19968;&#27425;&#24615;&#20998;&#21106;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Personalize Segment Anything Model with One Shot. (arXiv:2305.03048v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03048
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;SAM&#20010;&#24615;&#21270;&#26041;&#27861;PerSAM&#65292;&#21482;&#38656;&#35201;&#19968;&#24352;&#24102;&#26377;&#21442;&#32771;&#25513;&#27169;&#30340;&#21333;&#24352;&#22270;&#20687;&#21363;&#21487;&#23450;&#20301;&#21644;&#20998;&#21106;&#30446;&#26631;&#27010;&#24565;&#65292;&#36824;&#25552;&#20986;&#20102;&#39640;&#25928;&#30340;&#19968;&#27425;&#24615;&#24494;&#35843;&#21464;&#20307;PerSAM-F&#65292;&#26088;&#22312;&#35299;&#20915;&#25513;&#27169;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#25968;&#25454;&#39044;&#35757;&#32451;&#30340;&#25512;&#21160;&#19979;&#65292;&#20998;&#21106;&#20219;&#20309;&#29289;&#20307;&#27169;&#22411;&#65288;SAM&#65289;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#20010;&#24378;&#22823;&#19988;&#39640;&#25928;&#30340;&#26694;&#26550;&#65292;&#38761;&#26032;&#20102;&#20998;&#21106;&#27169;&#22411;&#39046;&#22495;&#12290;&#23613;&#31649;SAM&#38750;&#24120;&#36890;&#29992;&#65292;&#20294;&#33258;&#21160;&#20026;&#29305;&#23450;&#35270;&#35273;&#27010;&#24565;&#23450;&#21046;SAM&#32780;&#19981;&#38656;&#35201;&#25163;&#21160;&#25552;&#31034;&#65292;&#22914;&#22312;&#19981;&#21516;&#22270;&#20687;&#20013;&#33258;&#21160;&#20998;&#21106;&#20320;&#30340;&#23456;&#29289;&#29399;&#31561;&#65292; &#36824;&#26410;&#28145;&#20837;&#30740;&#31350;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;SAM&#20010;&#24615;&#21270;&#26041;&#27861;&#65292;&#31216;&#20026;PerSAM&#12290;&#21482;&#38656;&#35201;&#19968;&#24352;&#24102;&#26377;&#21442;&#32771;&#25513;&#27169;&#30340;&#21333;&#24352;&#22270;&#20687;&#65292;PerSAM&#39318;&#20808;&#36890;&#36807;&#20301;&#32622;&#20808;&#39564;&#23450;&#20301;&#30446;&#26631;&#27010;&#24565;&#65292;&#24182;&#36890;&#36807;&#19977;&#31181;&#25216;&#26415;&#26469;&#22312;&#20854;&#20182;&#22270;&#20687;&#25110;&#35270;&#39057;&#20013;&#20998;&#21106;&#23427;&#65306;&#30446;&#26631;&#24341;&#23548;&#27880;&#24847;&#21147;&#65292;&#30446;&#26631;&#35821;&#20041;&#25552;&#31034;&#21644;&#32423;&#32852;&#21518;&#22788;&#29702;&#12290;&#36825;&#26679;&#65292;&#25105;&#20204;&#26377;&#25928;&#22320;&#36866;&#24212;&#20102;SAM&#30340;&#31169;&#20154;&#20351;&#29992;&#32780;&#26080;&#38656;&#20219;&#20309;&#35757;&#32451;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#32531;&#35299;&#25513;&#27169;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#19968;&#27425;&#24615;&#24494;&#35843;&#21464;&#20307;&#65292;&#21363;PerSAM-F&#12290;&#20923;&#32467;&#25972;&#20010;SAM&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#21487;&#23398;&#20064;&#26435;&#37325;&#29992;&#20110;&#22810;&#23610;&#24230;&#25513;&#27169;&#65292;&#20165;&#35757;&#32451;2&#20010;&#21442;&#25968;&#21363;&#21487;&#12290;
&lt;/p&gt;
&lt;p&gt;
Driven by large-data pre-training, Segment Anything Model (SAM) has been demonstrated as a powerful and promptable framework, revolutionizing the segmentation models. Despite the generality, customizing SAM for specific visual concepts without man-powered prompting is under explored, e.g., automatically segmenting your pet dog in different images. In this paper, we propose a training-free Personalization approach for SAM, termed as PerSAM. Given only a single image with a reference mask, PerSAM first localizes the target concept by a location prior, and segments it within other images or videos via three techniques: target-guided attention, target-semantic prompting, and cascaded post-refinement. In this way, we effectively adapt SAM for private use without any training. To further alleviate the mask ambiguity, we present an efficient one-shot fine-tuning variant, PerSAM-F. Freezing the entire SAM, we introduce two learnable weights for multi-scale masks, only training 2 parameters wit
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;SELF-ALIGN&#26041;&#27861;&#65292;&#20351;&#29992;&#22522;&#20110;&#21407;&#21017;&#30340;&#25512;&#29702;&#21644;LLMs&#30340;&#29983;&#25104;&#33021;&#21147;&#20197;&#26368;&#23569;&#30340;&#20154;&#31867;&#30417;&#30563;&#23454;&#29616;AI&#20195;&#29702;&#30340;&#33258;&#25105;&#23545;&#40784;&#12290;</title><link>http://arxiv.org/abs/2305.03047</link><description>&lt;p&gt;
&#21407;&#21017;&#39537;&#21160;&#33258;&#25105;&#23545;&#40784;&#30340;&#26368;&#23567;&#20154;&#21147;&#30417;&#30563;&#30340;&#35821;&#35328;&#27169;&#22411;&#20174;&#38646;&#24320;&#22987;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision. (arXiv:2305.03047v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03047
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;SELF-ALIGN&#26041;&#27861;&#65292;&#20351;&#29992;&#22522;&#20110;&#21407;&#21017;&#30340;&#25512;&#29702;&#21644;LLMs&#30340;&#29983;&#25104;&#33021;&#21147;&#20197;&#26368;&#23569;&#30340;&#20154;&#31867;&#30417;&#30563;&#23454;&#29616;AI&#20195;&#29702;&#30340;&#33258;&#25105;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;AI&#21161;&#25163;&#20195;&#29702;&#65292;&#22914;ChatGPT&#65292;&#20027;&#35201;&#20381;&#36182;&#20110;&#30417;&#30563;&#24494;&#35843;&#21644;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#26469;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#19982;&#20154;&#31867;&#24847;&#22270;&#65292;&#30830;&#20445;&#23427;&#20204;&#26159;&#26377;&#29992;&#30340;&#12289;&#36947;&#24503;&#30340;&#12289;&#21487;&#38752;&#30340;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20381;&#36182;&#24615;&#21487;&#33021;&#20250;&#26497;&#22823;&#22320;&#38480;&#21046;AI&#21161;&#25163;&#20195;&#29702;&#30340;&#30495;&#27491;&#28508;&#21147;&#65292;&#22240;&#20026;&#33719;&#24471;&#20154;&#31867;&#30417;&#30563;&#30340;&#25104;&#26412;&#24456;&#39640;&#65292;&#30456;&#20851;&#38382;&#39064;&#26377;&#36136;&#37327;&#12289;&#21487;&#38752;&#24615;&#12289;&#22810;&#26679;&#24615;&#12289;&#33258;&#19968;&#33268;&#24615;&#21644;&#19981;&#33391;&#20559;&#35265;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861; SELF-ALIGN&#65292;&#23427;&#32467;&#21512;&#20102;&#22522;&#20110;&#21407;&#21017;&#30340;&#25512;&#29702;&#21644;LLMs&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;&#20197;&#26368;&#23569;&#30340;&#20154;&#31867;&#30417;&#30563;&#23454;&#29616;AI&#20195;&#29702;&#30340;&#33258;&#25105;&#23545;&#40784;&#12290;&#26041;&#27861;&#21253;&#25324;&#22235;&#20010;&#38454;&#27573;&#65306;&#31532;&#19968;&#65292;&#25105;&#20204;&#20351;&#29992;LLM&#29983;&#25104;&#21512;&#25104;&#25552;&#31034;&#65292;&#20351;&#29992;&#20027;&#39064;&#24341;&#23548;&#26041;&#27861;&#22686;&#21152;&#25552;&#31034;&#22810;&#26679;&#24615;&#65307;&#31532;&#20108;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#23567;&#32452;&#20154;&#24037;&#32534;&#20889;&#30340;AI&#27169;&#22411;&#21407;&#21017;&#65292;&#24182;&#25351;&#23548;AI&#27169;&#22411;&#36981;&#24490;&#65307;
&lt;/p&gt;
&lt;p&gt;
Recent AI-assistant agents, such as ChatGPT, predominantly rely on supervised fine-tuning (SFT) with human annotations and reinforcement learning from human feedback (RLHF) to align the output of large language models (LLMs) with human intentions, ensuring they are helpful, ethical, and reliable. However, this dependence can significantly constrain the true potential of AI-assistant agents due to the high cost of obtaining human supervision and the related issues on quality, reliability, diversity, self-consistency, and undesirable biases. To address these challenges, we propose a novel approach called SELF-ALIGN, which combines principle-driven reasoning and the generative power of LLMs for the self-alignment of AI agents with minimal human supervision. Our approach encompasses four stages: first, we use an LLM to generate synthetic prompts, and a topic-guided method to augment the prompt diversity; second, we use a small set of human-written principles for AI models to follow, and gu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#37326;&#22806;&#35270;&#39057;&#20013;&#33258;&#21160;&#25552;&#21462;&#19977;&#32500;&#30417;&#30563;&#26469;&#25193;&#23637;&#25163;&#25345;&#29289;&#20307;&#37325;&#24314;&#27169;&#22411;&#30340;&#23398;&#20064;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#25163;&#37096;&#23039;&#21183;&#20316;&#20026;&#29289;&#20307;&#23039;&#21183;&#30340;&#20195;&#29702;&#21644;&#23398;&#20064;&#25968;&#25454;&#39537;&#21160;&#30340;&#19977;&#32500;&#24418;&#29366;&#20808;&#39564;&#30693;&#35782;&#31561;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#26410;&#30693;&#30456;&#26426;&#23039;&#21183;&#21644;&#36974;&#25377;&#31561;&#38382;&#39064;&#65292;&#20174;&#32780;&#36890;&#36807;&#20174;&#21333;&#20010;RGB&#22270;&#20687;&#39044;&#27979;&#29289;&#20307;&#19977;&#32500;&#24418;&#29366;&#30340;&#21344;&#25454;&#32593;&#32476;&#24471;&#21040;&#20102;&#20248;&#31168;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.03036</link><description>&lt;p&gt;
&#20174;&#37326;&#22806;&#35270;&#39057;&#20013;&#23398;&#20064;&#25163;&#25345;&#29289;&#20307;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
Learning Hand-Held Object Reconstruction from In-The-Wild Videos. (arXiv:2305.03036v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03036
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#37326;&#22806;&#35270;&#39057;&#20013;&#33258;&#21160;&#25552;&#21462;&#19977;&#32500;&#30417;&#30563;&#26469;&#25193;&#23637;&#25163;&#25345;&#29289;&#20307;&#37325;&#24314;&#27169;&#22411;&#30340;&#23398;&#20064;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#25163;&#37096;&#23039;&#21183;&#20316;&#20026;&#29289;&#20307;&#23039;&#21183;&#30340;&#20195;&#29702;&#21644;&#23398;&#20064;&#25968;&#25454;&#39537;&#21160;&#30340;&#19977;&#32500;&#24418;&#29366;&#20808;&#39564;&#30693;&#35782;&#31561;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#26410;&#30693;&#30456;&#26426;&#23039;&#21183;&#21644;&#36974;&#25377;&#31561;&#38382;&#39064;&#65292;&#20174;&#32780;&#36890;&#36807;&#20174;&#21333;&#20010;RGB&#22270;&#20687;&#39044;&#27979;&#29289;&#20307;&#19977;&#32500;&#24418;&#29366;&#30340;&#21344;&#25454;&#32593;&#32476;&#24471;&#21040;&#20102;&#20248;&#31168;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#21333;&#24433;&#20687;&#25163;&#25345;&#29289;&#20307;&#37325;&#24314;&#26041;&#27861;&#20381;&#36182;&#20110;&#38590;&#20197;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#35268;&#27169;&#21270;&#25910;&#38598;&#30340;&#30452;&#25509;3D&#24418;&#29366;&#30417;&#30563;&#65292;&#22240;&#27492;&#36825;&#20123;&#26041;&#27861;&#22312;&#37326;&#22806;&#29615;&#22659;&#19979;&#38754;&#23545;&#26032;&#39062;&#29289;&#20307;&#26102;&#38590;&#20197;&#25512;&#24191;&#12290;&#26412;&#25991;&#20174;&#29983;&#21160;&#30340;&#37326;&#22806;&#21407;&#22987;&#35270;&#39057;&#25968;&#25454;&#20013;&#33258;&#21160;&#25552;&#21462;&#19977;&#32500;&#30417;&#30563;&#65292;&#24182;&#36890;&#36807;&#22810;&#35270;&#35282;&#20108;&#32500;&#30417;&#30563;&#26469;&#25193;&#23637;&#25163;&#25345;&#29289;&#20307;&#37325;&#24314;&#27169;&#22411;&#30340;&#23398;&#20064;&#12290;&#36825;&#38656;&#35201;&#24212;&#23545;&#20004;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;&#26410;&#30693;&#30340;&#30456;&#26426;&#23039;&#21183;&#21644;&#36974;&#25377;&#12290;&#23545;&#20110;&#21069;&#32773;&#65292;&#25105;&#20204;&#20351;&#29992;&#25163;&#37096;&#23039;&#21183;&#20316;&#20026;&#29289;&#20307;&#23039;&#21183;&#30340;&#20195;&#29702;&#12290;&#23545;&#20110;&#21518;&#32773;&#65292;&#25105;&#20204;&#20351;&#29992;ObMan&#25968;&#25454;&#38598;&#20013;&#21512;&#25104;&#30340;&#29289;&#20307;&#26469;&#23398;&#20064;&#25968;&#25454;&#39537;&#21160;&#30340;&#19977;&#32500;&#24418;&#29366;&#20808;&#39564;&#30693;&#35782;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#38388;&#25509;&#30340;&#19977;&#32500;&#32447;&#32034;&#26469;&#35757;&#32451;&#21344;&#25454;&#32593;&#32476;&#65292;&#20174;&#21333;&#20010;RGB&#22270;&#20687;&#39044;&#27979;&#29289;&#20307;&#30340;&#19977;&#32500;&#24418;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prior works for reconstructing hand-held objects from a single image rely on direct 3D shape supervision which is challenging to gather in real world at scale. Consequently, these approaches do not generalize well when presented with novel objects in in-the-wild settings. While 3D supervision is a major bottleneck, there is an abundance of in-the-wild raw video data showing hand-object interactions. In this paper, we automatically extract 3D supervision (via multiview 2D supervision) from such raw video data to scale up the learning of models for hand-held object reconstruction. This requires tackling two key challenges: unknown camera pose and occlusion. For the former, we use hand pose (predicted from existing techniques, e.g. FrankMocap) as a proxy for object pose. For the latter, we learn data-driven 3D shape priors using synthetic objects from the ObMan dataset. We use these indirect 3D cues to train occupancy networks that predict the 3D shape of objects from a single RGB image. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#20010;&#38543;&#26426;BPE&#21464;&#20307;&#65292;&#22312;&#32763;&#35793;&#21040;&#24418;&#24577;&#20016;&#23500;&#30340;&#35821;&#35328;&#26102;&#65292;&#38543;&#26426;&#36873;&#25321;&#21512;&#24182;&#25805;&#20316;&#23545;&#19979;&#28216;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#24433;&#21709;&#24456;&#23567;&#12290;&#26631;&#20934;BPE&#34429;&#28982;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#23384;&#22312;&#30528;&#26377;&#36259;&#30340;&#28508;&#22312;&#21464;&#21270;&#23431;&#23449;&#20540;&#24471;&#25506;&#31350;&#12290;</title><link>http://arxiv.org/abs/2305.03029</link><description>&lt;p&gt;
&#38543;&#26426;&#36873;&#25321;BPE&#21512;&#24182;&#25805;&#20316;&#20250;&#24102;&#26469;&#20160;&#20040;&#21464;&#21270;&#65311;&#19981;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
What changes when you randomly choose BPE merge operations? Not much. (arXiv:2305.03029v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03029
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#20010;&#38543;&#26426;BPE&#21464;&#20307;&#65292;&#22312;&#32763;&#35793;&#21040;&#24418;&#24577;&#20016;&#23500;&#30340;&#35821;&#35328;&#26102;&#65292;&#38543;&#26426;&#36873;&#25321;&#21512;&#24182;&#25805;&#20316;&#23545;&#19979;&#28216;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#24433;&#21709;&#24456;&#23567;&#12290;&#26631;&#20934;BPE&#34429;&#28982;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#23384;&#22312;&#30528;&#26377;&#36259;&#30340;&#28508;&#22312;&#21464;&#21270;&#23431;&#23449;&#20540;&#24471;&#25506;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19977;&#20010;&#31616;&#21333;&#30340;&#38543;&#26426;BPE&#21464;&#20307;&#65292;&#24182;&#25506;&#35752;&#20102;&#38543;&#26426;&#36873;&#25321;&#21512;&#24182;&#25805;&#20316;&#26159;&#21542;&#20250;&#23545;&#19979;&#28216;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#20135;&#29983;&#23454;&#36136;&#24615;&#24433;&#21709;&#12290;&#25105;&#20204;&#30528;&#37325;&#32763;&#35793;&#21040;&#24418;&#24577;&#20016;&#23500;&#30340;&#35821;&#35328;&#65292;&#20551;&#35774;&#36825;&#20010;&#20219;&#21153;&#21487;&#33021;&#23545;&#36873;&#25321;&#23376;&#35789;&#30340;&#26041;&#27861;&#34920;&#29616;&#20986;&#25935;&#24863;&#24615;&#12290;&#20351;&#29992;&#36125;&#21494;&#26031;&#32447;&#24615;&#27169;&#22411;&#36827;&#34892;&#20998;&#26512;&#34920;&#26126;&#65292;&#20854;&#20013;&#20004;&#20010;&#21464;&#20307;&#19982;&#26631;&#20934;BPE&#30456;&#27604;&#34920;&#29616;&#20960;&#20046;&#26080;&#27861;&#21306;&#20998;&#65292;&#32780;&#21478;&#19968;&#20010;&#21464;&#20307;&#30340;&#24615;&#33021;&#19979;&#38477;&#31243;&#24230;&#27604;&#25105;&#20204;&#39044;&#26399;&#30340;&#35201;&#23567;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#23613;&#31649;&#26631;&#20934;BPE&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#23384;&#22312;&#19968;&#20010;&#26377;&#36259;&#30340;&#28508;&#22312;&#21464;&#21270;&#23431;&#23449;&#20540;&#24471;&#25506;&#31350;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312;&#20197;&#19979;&#32593;&#22336;&#25214;&#21040;&#65306;https://github.com/bltlab/random-bpe&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce three simple randomized variants of byte pair encoding (BPE) and explore whether randomizing the selection of merge operations substantially affects a downstream machine translation task. We focus on translation into morphologically rich languages, hypothesizing that this task may show sensitivity to the method of choosing subwords. Analysis using a Bayesian linear model indicates that two of the variants perform nearly indistinguishably compared to standard BPE while the other degrades performance less than we anticipated. We conclude that although standard BPE is widely used, there exists an interesting universe of potential variations on it worth investigating. Our code is available at: https://github.com/bltlab/random-bpe.
&lt;/p&gt;</description></item><item><title>&#35813;&#39033;&#30446;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#25351;&#20196;&#35843;&#25972;&#26469;&#25552;&#21319;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#25506;&#35752;&#20102;&#35757;&#32451;&#25968;&#25454;&#30340;&#22240;&#32032;&#23545;&#25351;&#20196;&#35843;&#25972;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#37327;&#21270;&#20998;&#26512;&#26469;&#20026;&#32842;&#22825;&#27169;&#22411;&#30340;&#25345;&#32493;&#21457;&#23637;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#27934;&#23519;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.03025</link><description>&lt;p&gt;
Panda LLM&#65306;&#35757;&#32451;&#25968;&#25454;&#21644;&#35780;&#20272;&#38024;&#23545;&#24320;&#28304;&#27721;&#35821;&#25351;&#20196;&#36319;&#38543;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Panda LLM: Training Data and Evaluation for Open-Sourced Chinese Instruction-Following Large Language Models. (arXiv:2305.03025v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03025
&lt;/p&gt;
&lt;p&gt;
&#35813;&#39033;&#30446;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#25351;&#20196;&#35843;&#25972;&#26469;&#25552;&#21319;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#25506;&#35752;&#20102;&#35757;&#32451;&#25968;&#25454;&#30340;&#22240;&#32032;&#23545;&#25351;&#20196;&#35843;&#25972;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#37327;&#21270;&#20998;&#26512;&#26469;&#20026;&#32842;&#22825;&#27169;&#22411;&#30340;&#25345;&#32493;&#21457;&#23637;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#27934;&#23519;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#39033;&#30446;&#30528;&#37325;&#20110;&#36890;&#36807;&#25351;&#20196;&#35843;&#25972;&#26469;&#22686;&#24378;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#23545;&#20854;&#24615;&#33021;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#21508;&#31181;&#35757;&#32451;&#25968;&#25454;&#22240;&#32032;&#65292;&#22914;&#25968;&#37327;&#12289;&#36136;&#37327;&#21644;&#35821;&#35328;&#20998;&#24067;&#65292;&#23545;&#22312;&#20844;&#24320;&#39640;&#36136;&#37327;&#25351;&#20196;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#20013;&#33521;&#21452;&#35821;&#25351;&#20196;&#35843;&#25972;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#23450;&#37327;&#20998;&#26512;&#26469;&#34917;&#20805;&#35780;&#20272;&#65292;&#20026;&#24320;&#28304;&#32842;&#22825;&#27169;&#22411;&#30340;&#25345;&#32493;&#21457;&#23637;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#27934;&#23519;&#21147;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#12289;&#25968;&#25454;&#21644;&#20195;&#30721;&#37117;&#26159;&#20844;&#24320;&#30340;&#65292;&#20379;&#20854;&#20182;&#20154;&#20351;&#29992;&#21644;&#24314;&#31435;&#12290;
&lt;/p&gt;
&lt;p&gt;
This project focuses on enhancing open-source large language models through instruction-tuning and providing comprehensive evaluations of their performance. We explore how various training data factors, such as quantity, quality, and linguistic distribution, influence the performance of instruction-tuned models trained on publicly accessible high-quality instruction datasets for both English and Chinese languages. Our goal is to supplement evaluation with quantitative analyses, providing valuable insights for the continued advancement of open-source chat models. Our model, data, and code are publicly available for others to use and build upon.
&lt;/p&gt;</description></item><item><title>FastAMI&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#25968;&#25454;&#38598;&#30340;&#32858;&#31867;&#27604;&#36739;&#30340;&#24555;&#36895;&#26041;&#27861;&#65292;&#36890;&#36807;&#33945;&#29305;&#21345;&#32599;&#27169;&#25311;&#26469;&#23454;&#29616;&#20598;&#28982;&#24615;&#35843;&#25972;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#22522;&#20110;&#25490;&#21015;&#30340;&#26041;&#27861;&#26356;&#20934;&#30830;&#12290;</title><link>http://arxiv.org/abs/2305.03022</link><description>&lt;p&gt;
FastAMI -- &#19968;&#31181;&#33945;&#29305;&#21345;&#32599;&#26041;&#27861;&#29992;&#20110;&#32858;&#31867;&#27604;&#36739;&#24230;&#37327;&#20013;&#30340;&#20598;&#28982;&#24615;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
FastAMI -- a Monte Carlo Approach to the Adjustment for Chance in Clustering Comparison Metrics. (arXiv:2305.03022v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03022
&lt;/p&gt;
&lt;p&gt;
FastAMI&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#25968;&#25454;&#38598;&#30340;&#32858;&#31867;&#27604;&#36739;&#30340;&#24555;&#36895;&#26041;&#27861;&#65292;&#36890;&#36807;&#33945;&#29305;&#21345;&#32599;&#27169;&#25311;&#26469;&#23454;&#29616;&#20598;&#28982;&#24615;&#35843;&#25972;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#22522;&#20110;&#25490;&#21015;&#30340;&#26041;&#27861;&#26356;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32858;&#31867;&#26159;&#26426;&#22120;&#23398;&#20064;&#30340;&#26680;&#24515;&#65292;&#38543;&#30528;&#25968;&#25454;&#21487;&#29992;&#24615;&#30340;&#22686;&#21152;&#65292;&#20854;&#24212;&#29992;&#26085;&#30410;&#22686;&#22810;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#25968;&#25454;&#38598;&#30340;&#22686;&#38271;&#65292;&#24102;&#26377;&#20598;&#28982;&#24615;&#35843;&#25972;&#30340;&#32858;&#31867;&#27604;&#36739;&#21464;&#24471;&#35745;&#31639;&#22256;&#38590;&#65292;&#23548;&#33268;&#27809;&#26377;&#20559;&#35265;&#30340;&#30495;&#23454;&#27604;&#36739;&#21644;&#35299;&#20915;&#26041;&#26696;&#36873;&#25321;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;FastAMI&#65292;&#19968;&#31181;&#22522;&#20110;&#33945;&#29305;&#21345;&#32599;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#22320;&#20272;&#35745;&#32463;&#36807;&#35843;&#25972;&#30340;&#20114;&#20449;&#24687;&#65288;AMI&#65289;&#24182;&#23558;&#20854;&#25193;&#23637;&#21040;&#26631;&#20934;&#21270;&#20114;&#20449;&#24687;&#65288;SMI&#65289;&#12290;&#19982;&#20934;&#30830;&#35745;&#31639;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36275;&#22815;&#24555;&#65292;&#21487;&#20197;&#20026;&#22823;&#22411;&#25968;&#25454;&#38598;&#21551;&#29992;&#36825;&#20123;&#24102;&#26377;&#35843;&#25972;&#30340;&#20449;&#24687;&#35770;&#27604;&#36739;&#65292;&#21516;&#26102;&#20445;&#25345;&#27604;&#37197;&#23545;&#26041;&#27861;&#26356;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clustering is at the very core of machine learning, and its applications proliferate with the increasing availability of data. However, as datasets grow, comparing clusterings with an adjustment for chance becomes computationally difficult, preventing unbiased ground-truth comparisons and solution selection. We propose FastAMI, a Monte Carlo-based method to efficiently approximate the Adjusted Mutual Information (AMI) and extend it to the Standardized Mutual Information (SMI). The approach is compared with the exact calculation and a recently developed variant of the AMI based on pairwise permutations, using both synthetic and real data. In contrast to the exact calculation our method is fast enough to enable these adjusted information-theoretic comparisons for large datasets while maintaining considerably more accurate results than the pairwise approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;BERT&#21644;Query-Aware LSH&#25552;&#39640;&#38750;&#27491;&#24335;&#25991;&#26723;&#20013;&#20195;&#30721;&#31034;&#20363;&#25512;&#33616;&#30340;&#36136;&#37327;&#65292;&#37325;&#28857;&#20851;&#27880;&#20110;Stack Overflow&#19978;&#30340;Java&#32534;&#31243;&#35821;&#35328;&#12290;&#30740;&#31350;&#20351;&#29992;BERT&#23558;&#20195;&#30721;&#31034;&#20363;&#36716;&#25442;&#20026;&#25968;&#20540;&#21521;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.03017</link><description>&lt;p&gt;
&#20351;&#29992;BERT&#21644;Query-Aware LSH&#25552;&#39640;&#38750;&#27491;&#24335;&#25991;&#26723;&#20013;&#20195;&#30721;&#31034;&#20363;&#25512;&#33616;&#65306;&#19968;&#39033;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Improving Code Example Recommendations on Informal Documentation Using BERT and Query-Aware LSH: A Comparative Study. (arXiv:2305.03017v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03017
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;BERT&#21644;Query-Aware LSH&#25552;&#39640;&#38750;&#27491;&#24335;&#25991;&#26723;&#20013;&#20195;&#30721;&#31034;&#20363;&#25512;&#33616;&#30340;&#36136;&#37327;&#65292;&#37325;&#28857;&#20851;&#27880;&#20110;Stack Overflow&#19978;&#30340;Java&#32534;&#31243;&#35821;&#35328;&#12290;&#30740;&#31350;&#20351;&#29992;BERT&#23558;&#20195;&#30721;&#31034;&#20363;&#36716;&#25442;&#20026;&#25968;&#20540;&#21521;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#21644;&#26368;&#36817;&#19968;&#30452;&#22312;&#36827;&#34892;&#20195;&#30721;&#31034;&#20363;&#25512;&#33616;&#30340;&#30740;&#31350;&#65292;&#20197;&#24110;&#21161;&#24320;&#21457;&#20154;&#21592;&#23436;&#25104;&#36719;&#20214;&#24320;&#21457;&#20219;&#21153;&#12290;&#30001;&#20110;&#24320;&#21457;&#20154;&#21592;&#32463;&#24120;&#33457;&#36153;&#22823;&#37327;&#26102;&#38388;&#22312;&#20114;&#32852;&#32593;&#19978;&#23547;&#25214;&#30456;&#20851;&#30340;&#20195;&#30721;&#31034;&#20363;&#65292;&#21033;&#29992;&#24320;&#28304;&#39033;&#30446;&#21644;&#38750;&#27491;&#24335;&#25991;&#26723;&#12290;&#20026;&#20102;&#25214;&#21040;&#26377;&#29992;&#30340;&#20195;&#30721;&#31034;&#20363;&#65292;&#38750;&#27491;&#24335;&#25991;&#26723;&#65288;&#22914;Stack Overflow&#35752;&#35770;&#21644;&#35770;&#22363;&#65289;&#21487;&#20197;&#38750;&#24120;&#23453;&#36149;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#37325;&#28857;&#26159;Stack Overflow&#65292;&#23427;&#26159;&#36719;&#20214;&#24320;&#21457;&#20154;&#21592;&#35752;&#35770;&#19981;&#21516;&#20027;&#39064;&#30340;&#27969;&#34892;&#36164;&#28304;&#12290;&#20026;&#20102;&#25552;&#39640;&#25512;&#33616;&#20195;&#30721;&#31034;&#20363;&#30340;&#36136;&#37327;&#65292;&#25105;&#20204;&#25910;&#38598;&#24182;&#25512;&#33616;&#20102;Java&#32534;&#31243;&#35821;&#35328;&#20013;&#26368;&#20339;&#30340;&#20195;&#30721;&#31034;&#20363;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;BERT&#26469;&#36827;&#34892;&#22788;&#29702;&#65292;&#23427;&#26159;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#20174;&#25991;&#26412;&#25968;&#25454;&#20013;&#25552;&#21462;&#35821;&#20041;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#27493;&#26159;&#20351;&#29992;BERT&#23558;&#20195;&#30721;&#31034;&#20363;&#36716;&#25442;&#20026;&#25968;&#20540;&#21521;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The study of code example recommendation has been conducted extensively in the past and recently in order to assist developers in their software development tasks. This is because developers often spend significant time searching for relevant code examples on the internet, utilizing open-source projects and informal documentation. For finding useful code examples, informal documentation, such as Stack Overflow discussions and forums, can be invaluable. We have focused our research on Stack Overflow, which is a popular resource for discussing different topics among software developers. For increasing the quality of the recommended code examples, we have collected and recommended the best code examples in the Java programming language. We have utilized BERT in our approach, which is a Large Language Model (LLM) for text representation that can effectively extract semantic information from textual data. Our first step involved using BERT to convert code examples into numerical vectors. Su
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25913;&#36827;&#20869;&#37096;&#21487;&#35299;&#37322;&#24615; ProtoPNet &#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#29983;&#25104;&#30340;&#25351;&#21521;&#24615;&#22270;&#19982;&#21518;&#32493;&#26041;&#27861;&#29983;&#25104;&#30340;&#26174;&#33879;&#24615;&#22270;&#36827;&#34892;&#27604;&#36739;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#35299;&#37322;&#24615;&#26041;&#27861;&#38388;&#23384;&#22312;&#30340;&#24046;&#24322;&#65292;&#20026;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#39046;&#22495;&#30340;&#20020;&#24202;&#24212;&#29992;&#25552;&#20379;&#20102;&#21487;&#34892;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2305.03002</link><description>&lt;p&gt;
&#36890;&#36807;&#20869;&#22312;&#30340;&#21487;&#35299;&#37322;&#24615;&#35780;&#20272;&#21518;&#32493;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating Post-hoc Interpretability with Intrinsic Interpretability. (arXiv:2305.03002v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03002
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25913;&#36827;&#20869;&#37096;&#21487;&#35299;&#37322;&#24615; ProtoPNet &#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#29983;&#25104;&#30340;&#25351;&#21521;&#24615;&#22270;&#19982;&#21518;&#32493;&#26041;&#27861;&#29983;&#25104;&#30340;&#26174;&#33879;&#24615;&#22270;&#36827;&#34892;&#27604;&#36739;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#35299;&#37322;&#24615;&#26041;&#27861;&#38388;&#23384;&#22312;&#30340;&#24046;&#24322;&#65292;&#20026;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#39046;&#22495;&#30340;&#20020;&#24202;&#24212;&#29992;&#25552;&#20379;&#20102;&#21487;&#34892;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#26576;&#20123;&#21307;&#23398;&#20219;&#21153;&#20013;&#36798;&#21040;&#20102;&#20154;&#31867;&#27700;&#24179;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#30340;&#20020;&#24202;&#24212;&#29992;&#21463;&#21040;&#20102;&#21487;&#35299;&#37322;&#24615;&#30340;&#38480;&#21046;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#20027;&#35201;&#30340;&#35299;&#37322;&#24615;&#31574;&#30053;&#65306;&#21518;&#32493;&#26041;&#27861;&#21644;&#20869;&#22312;&#26041;&#27861;&#12290;&#26412;&#25991;&#23558;&#20869;&#37096;&#21487;&#35299;&#37322;&#24615; ProtoPNet &#36827;&#34892;&#25913;&#36827;&#65292;&#36866;&#29992;&#20110;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#30340;&#32972;&#26223;&#65292;&#24182;&#23558;&#20854;&#29983;&#25104;&#30340;&#25351;&#21521;&#24615;&#22270;&#19982;&#21518;&#32493;&#26041;&#27861;&#29983;&#25104;&#30340;&#26174;&#33879;&#24615;&#22270;&#36827;&#34892;&#27604;&#36739;&#12290;&#21516;&#26102;&#65292;&#20026;&#20102;&#35780;&#20272;&#26174;&#33879;&#24615;&#22270;&#26041;&#27861;&#21644;&#25351;&#21521;&#24615;&#22270;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#26412;&#25991;&#20174;&#26174;&#33879;&#24615;&#27169;&#22411;&#25991;&#29486;&#20013;&#36873;&#21462;&#20102;10&#20010;&#26174;&#33879;&#24615;&#24230;&#37327;&#25351;&#26631;&#65292;&#24182;&#20351;&#29992;&#21253;&#21547;327,680&#20010;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#34917;&#19969;&#30340;&#20083;&#33146;&#30284;&#36716;&#31227;&#26816;&#27979;&#25968;&#25454;&#38598; PatchCamelyon &#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite Convolutional Neural Networks having reached human-level performance in some medical tasks, their clinical use has been hindered by their lack of interpretability. Two major interpretability strategies have been proposed to tackle this problem: post-hoc methods and intrinsic methods. Although there are several post-hoc methods to interpret DL models, there is significant variation between the explanations provided by each method, and it a difficult to validate them due to the lack of ground-truth. To address this challenge, we adapted the intrinsical interpretable ProtoPNet for the context of histopathology imaging and compared the attribution maps produced by it and the saliency maps made by post-hoc methods. To evaluate the similarity between saliency map methods and attribution maps we adapted 10 saliency metrics from the saliency model literature, and used the breast cancer metastases detection dataset PatchCamelyon with 327,680 patches of histopathological images of sentin
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#23545;176&#20010;&#25968;&#25454;&#38598;&#30340;&#27604;&#36739;&#20998;&#26512;&#21457;&#29616;&#65292;&#22312;&#35768;&#22810;&#25968;&#25454;&#38598;&#20013;&#65292;GBDT&#21644;NN&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#24322;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#65292;&#25110;&#32773;GBDT&#30340;&#36731;&#24494;&#36229;&#21442;&#25968;&#35843;&#25972;&#27604;&#36873;&#25321;&#26368;&#20339;&#31639;&#27861;&#26356;&#37325;&#35201;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#20154;&#21592;&#23545;965&#20010;&#20803;&#29305;&#24449;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#21457;&#29616;GBDT&#22312;&#39640;&#32500;&#31232;&#30095;&#25968;&#25454;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.02997</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20309;&#26102;&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#32988;&#36807;&#22686;&#24378;&#26641;&#65311;
&lt;/p&gt;
&lt;p&gt;
When Do Neural Nets Outperform Boosted Trees on Tabular Data?. (arXiv:2305.02997v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02997
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#23545;176&#20010;&#25968;&#25454;&#38598;&#30340;&#27604;&#36739;&#20998;&#26512;&#21457;&#29616;&#65292;&#22312;&#35768;&#22810;&#25968;&#25454;&#38598;&#20013;&#65292;GBDT&#21644;NN&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#24322;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#65292;&#25110;&#32773;GBDT&#30340;&#36731;&#24494;&#36229;&#21442;&#25968;&#35843;&#25972;&#27604;&#36873;&#25321;&#26368;&#20339;&#31639;&#27861;&#26356;&#37325;&#35201;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#20154;&#21592;&#23545;965&#20010;&#20803;&#29305;&#24449;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#21457;&#29616;GBDT&#22312;&#39640;&#32500;&#31232;&#30095;&#25968;&#25454;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#26684;&#25968;&#25454;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#26368;&#24120;&#29992;&#30340;&#25968;&#25454;&#31867;&#22411;&#20043;&#19968;&#12290;&#23613;&#31649;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#21462;&#24471;&#20102;&#26368;&#36817;&#30340;&#36827;&#23637;&#65292;&#20294;&#20154;&#20204;&#20173;&#22312;&#31215;&#26497;&#35752;&#35770;NN&#26159;&#21542;&#36890;&#24120;&#20248;&#20110;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#65288;GBDT&#65289;&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#30340;&#34920;&#29616;&#65292;&#19968;&#20123;&#26368;&#36817;&#30340;&#24037;&#20316;&#35201;&#20040;&#35748;&#20026;GBDT&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#19968;&#36143;&#20248;&#20110;NN&#65292;&#35201;&#20040;&#35748;&#20026;NN&#20248;&#20110;GBDT&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36864;&#19968;&#27493;&#38382;&#65306;'&#36825;&#37325;&#35201;&#21527;&#65311;'&#25105;&#20204;&#36890;&#36807;&#23545;176&#20010;&#25968;&#25454;&#38598;&#27604;&#36739;19&#31181;&#31639;&#27861;&#65292;&#36827;&#34892;&#20102;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;&#34920;&#26684;&#25968;&#25454;&#20998;&#26512;&#65292;&#24182;&#21457;&#29616;'NN vs. GBDT'&#20105;&#35770;&#34987;&#36807;&#20998;&#24378;&#35843;&#65306;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#22312;&#30456;&#24403;&#22810;&#30340;&#25968;&#25454;&#38598;&#20013;&#65292;GBDT&#21644;NN&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#24322;&#35201;&#20040;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#65292;&#35201;&#20040;GBDT&#30340;&#36731;&#24494;&#36229;&#21442;&#25968;&#35843;&#25972;&#27604;&#36873;&#25321;&#26368;&#20339;&#31639;&#27861;&#26356;&#37325;&#35201;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;965&#20010;&#20803;&#29305;&#24449;&#65292;&#20197;&#30830;&#23450;&#25968;&#25454;&#38598;&#30340;&#21738;&#20123;&#29305;&#24615;&#20351;NN&#25110;GBDT&#26356;&#36866;&#21512;&#34920;&#29616;&#33391;&#22909;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#21457;&#29616;GBDT&#35201;&#27604;NN&#22312;&#39640;&#32500;&#31232;&#30095;&#25968;&#25454;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tabular data is one of the most commonly used types of data in machine learning. Despite recent advances in neural nets (NNs) for tabular data, there is still an active discussion on whether or not NNs generally outperform gradient-boosted decision trees (GBDTs) on tabular data, with several recent works arguing either that GBDTs consistently outperform NNs on tabular data, or vice versa. In this work, we take a step back and ask, 'does it matter?' We conduct the largest tabular data analysis to date, by comparing 19 algorithms across 176 datasets, and we find that the 'NN vs. GBDT' debate is overemphasized: for a surprisingly high number of datasets, either the performance difference between GBDTs and NNs is negligible, or light hyperparameter tuning on a GBDT is more important than selecting the best algorithm. Next, we analyze 965 metafeatures to determine what properties of a dataset make NNs or GBDTs better-suited to perform well. For example, we find that GBDTs are much better th
&lt;/p&gt;</description></item><item><title>&#22312;&#19981;&#21516;&#25968;&#25454;&#23376;&#32676;&#20307;&#38388;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20869;&#37096;&#20934;&#30830;&#24615;&#21644;&#22806;&#37096;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#26159;&#38750;&#32447;&#24615;&#30340;&#65292;&#21576;&#29616;&#20986;&#8220;&#26376;&#20142;&#24418;&#8221;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.02995</link><description>&lt;p&gt;
&#20851;&#20110;&#25968;&#25454;&#23376;&#32676;&#20307;&#38388;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#30340;&#38750;&#32447;&#24615;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the nonlinear correlation of ML performance between data subpopulations. (arXiv:2305.02995v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02995
&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#21516;&#25968;&#25454;&#23376;&#32676;&#20307;&#38388;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20869;&#37096;&#20934;&#30830;&#24615;&#21644;&#22806;&#37096;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#26159;&#38750;&#32447;&#24615;&#30340;&#65292;&#21576;&#29616;&#20986;&#8220;&#26376;&#20142;&#24418;&#8221;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#19981;&#21516;&#25968;&#25454;&#20998;&#24067;&#19979;&#30340;&#24615;&#33021;&#23545;&#20110;&#21487;&#38752;&#30340;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#26368;&#26032;&#30340;&#32463;&#39564;&#30740;&#31350;&#35748;&#20026;&#35757;&#32451;&#25968;&#25454;&#20869;&#37096;&#30340;&#20934;&#30830;&#24615;&#21644;&#26032;&#25968;&#25454;&#22806;&#37096;&#30340;&#20934;&#30830;&#24615;&#20043;&#38388;&#23384;&#22312;&#36817;&#20046;&#23436;&#32654;&#30340;&#32447;&#24615;&#30456;&#20851;&#24615;&#65292;&#20294;&#25105;&#20204;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#21644;&#35757;&#32451;&#26102;&#26399;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#23454;&#39564;&#21644;&#20998;&#26512;&#65292;&#21457;&#29616;&#22312;&#23376;&#32676;&#20307;&#36716;&#31227;&#19979;&#65292;&#20869;&#37096;&#20934;&#30830;&#24615;&#21644;&#22806;&#37096;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#26356;&#20026;&#24494;&#22937;&#65292;&#24182;&#19988;&#22312;&#19978;&#21319;&#38454;&#27573;&#23384;&#22312;&#8220;&#26376;&#20142;&#24418;&#8221;&#30340;&#30456;&#20851;&#24615;&#65288;&#25243;&#29289;&#32447;&#19978;&#21319;&#26354;&#32447;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the performance of machine learning (ML) models across diverse data distributions is critically important for reliable applications. Despite recent empirical studies positing a near-perfect linear correlation between in-distribution (ID) and out-of-distribution (OOD) accuracies, we empirically demonstrate that this correlation is more nuanced under subpopulation shifts. Through rigorous experimentation and analysis across a variety of datasets, models, and training epochs, we demonstrate that OOD performance often has a nonlinear correlation with ID performance in subpopulation shifts. Our findings, which contrast previous studies that have posited a linear correlation in model performance during distribution shifts, reveal a "moon shape" correlation (parabolic uptrend curve) between the test performance on the majority subpopulation and the minority subpopulation. This non-trivial nonlinear correlation holds across model architectures, hyperparameters, training durations
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;SemEval 2023&#30340;&#20219;&#21153;&#19971;&#65292;&#26088;&#22312;&#36827;&#34892;&#20020;&#24202;&#35797;&#39564;&#25968;&#25454;&#30340;&#22810;&#35777;&#25454;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65292;&#35813;&#20219;&#21153;&#38590;&#24230;&#36739;&#22823;&#65292;&#35777;&#25454;&#36873;&#25321;&#20219;&#21153;&#30456;&#23545;&#20110;&#34164;&#21547;&#20219;&#21153;&#34920;&#29616;&#26356;&#20339;&#12290;</title><link>http://arxiv.org/abs/2305.02993</link><description>&lt;p&gt;
SemEval-2023&#20219;&#21153;7: &#20020;&#24202;&#35797;&#39564;&#25968;&#25454;&#30340;&#22810;&#35777;&#25454;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
SemEval-2023 Task 7: Multi-Evidence Natural Language Inference for Clinical Trial Data. (arXiv:2305.02993v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02993
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;SemEval 2023&#30340;&#20219;&#21153;&#19971;&#65292;&#26088;&#22312;&#36827;&#34892;&#20020;&#24202;&#35797;&#39564;&#25968;&#25454;&#30340;&#22810;&#35777;&#25454;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65292;&#35813;&#20219;&#21153;&#38590;&#24230;&#36739;&#22823;&#65292;&#35777;&#25454;&#36873;&#25321;&#20219;&#21153;&#30456;&#23545;&#20110;&#34164;&#21547;&#20219;&#21153;&#34920;&#29616;&#26356;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#20171;&#32461;SemEval 2023&#20219;&#21153;7&#30340;&#32467;&#26524;&#65292;&#35813;&#20219;&#21153;&#20027;&#35201;&#28041;&#21450;&#20020;&#24202;&#35797;&#39564;&#25968;&#25454;&#20013;&#30340;&#22810;&#35777;&#25454;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI4CT&#65289;&#65292;&#30001;&#20004;&#20010;&#23376;&#20219;&#21153;&#32452;&#25104;&#65306;&#19968;&#20010;&#26159;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#20219;&#21153;&#65292;&#21478;&#19968;&#20010;&#26159;&#35777;&#25454;&#36873;&#25321;&#20219;&#21153;&#12290;&#36825;&#20004;&#20010;&#20219;&#21153;&#38656;&#35201;&#36827;&#34892;&#21307;&#23398;&#21644;&#25968;&#23383;&#25512;&#29702;&#65292;&#36825;&#23545;&#20110;&#24320;&#21457;&#33021;&#22815;&#36827;&#34892;&#22823;&#35268;&#27169;&#21307;&#30103;&#35777;&#25454;&#35299;&#37322;&#21644;&#26816;&#32034;&#12289;&#25552;&#20379;&#20010;&#24615;&#21270;&#22522;&#20110;&#35777;&#25454;&#30340;&#20445;&#20581;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#31532;1&#20010;&#23376;&#20219;&#21153;&#8220;&#34164;&#21547;&#20219;&#21153;&#8221;&#25910;&#21040;&#20102;&#26469;&#33258;40&#20301;&#21442;&#36187;&#32773;&#30340;643&#20221;&#25552;&#20132;&#65292;&#31532;2&#20010;&#23376;&#20219;&#21153;&#8220;&#35777;&#25454;&#36873;&#25321;&#20219;&#21153;&#8221;&#25910;&#21040;&#20102;&#26469;&#33258;23&#20301;&#21442;&#36187;&#32773;&#30340;364&#20221;&#25552;&#20132;&#12290;&#36825;&#20004;&#20010;&#20219;&#21153;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22823;&#37096;&#20998;&#25552;&#20132;&#30340;&#31995;&#32479;&#22312;&#34164;&#21547;&#20219;&#21153;&#19978;&#26410;&#33021;&#26126;&#26174;&#20248;&#20110;&#22823;&#22810;&#25968;&#31867;&#22522;&#32447;&#65292;&#32780;&#25105;&#20204;&#35266;&#23519;&#21040;&#35777;&#25454;&#36873;&#25321;&#20219;&#21153;&#30340;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;&#34164;&#21547;&#20219;&#21153;&#12290;&#22686;&#21152;&#27169;&#22411;&#21442;&#25968;&#20250;&#23548;&#33268;&#27169;&#22411;&#22312;&#27979;&#35797;&#38598;&#19978;&#34920;&#29616;&#26356;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes the results of SemEval 2023 task 7 -- Multi-Evidence Natural Language Inference for Clinical Trial Data (NLI4CT) -- consisting of 2 tasks, a Natural Language Inference (NLI) task, and an evidence selection task on clinical trial data. The proposed challenges require multi-hop biomedical and numerical reasoning, which are of significant importance to the development of systems capable of large-scale interpretation and retrieval of medical evidence, to provide personalized evidence-based care.  Task 1, the entailment task, received 643 submissions from 40 participants, and Task 2, the evidence selection task, received 364 submissions from 23 participants. The tasks are challenging, with the majority of submitted systems failing to significantly outperform the majority class baseline on the entailment task, and we observe significantly better performance on the evidence selection task than on the entailment task. Increasing the number of model parameters leads to a di
&lt;/p&gt;</description></item><item><title>ExeKGLib&#26159;&#19968;&#20010;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;Python&#26426;&#22120;&#23398;&#20064;&#24211;&#65292;&#21487;&#24110;&#21161;&#19981;&#20855;&#22791;&#28145;&#20837;ML&#30693;&#35782;&#30340;&#29992;&#25143;&#26500;&#24314;&#21487;&#25191;&#34892;&#30340;ML&#24037;&#20316;&#27969;&#65292;&#24182;&#25552;&#39640;&#36879;&#26126;&#24230;&#21644;&#21487;&#37325;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.02966</link><description>&lt;p&gt;
ExeKGLib&#65306;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#26426;&#22120;&#23398;&#20064;&#20998;&#26512;&#24211;
&lt;/p&gt;
&lt;p&gt;
ExeKGLib: Knowledge Graphs-Empowered Machine Learning Analytics. (arXiv:2305.02966v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02966
&lt;/p&gt;
&lt;p&gt;
ExeKGLib&#26159;&#19968;&#20010;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;Python&#26426;&#22120;&#23398;&#20064;&#24211;&#65292;&#21487;&#24110;&#21161;&#19981;&#20855;&#22791;&#28145;&#20837;ML&#30693;&#35782;&#30340;&#29992;&#25143;&#26500;&#24314;&#21487;&#25191;&#34892;&#30340;ML&#24037;&#20316;&#27969;&#65292;&#24182;&#25552;&#39640;&#36879;&#26126;&#24230;&#21644;&#21487;&#37325;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#24211;&#23545;ML&#20174;&#19994;&#32773;&#24320;&#25918;&#12290;&#20856;&#22411;&#30340;ML&#27969;&#31243;&#26159;&#22797;&#26434;&#30340;&#65292;&#30001;&#19968;&#31995;&#21015;&#27493;&#39588;&#32452;&#25104;&#65292;&#27599;&#20010;&#27493;&#39588;&#37117;&#35843;&#29992;&#20102;&#20960;&#20010;ML&#24211;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;ExeKGLib&#65292;&#36825;&#26159;&#19968;&#20010;Python&#24211;&#65292;&#20801;&#35768;&#20855;&#26377;&#32534;&#30721;&#25216;&#33021;&#21644;&#26368;&#23567;ML&#30693;&#35782;&#30340;&#29992;&#25143;&#26500;&#24314;ML&#27969;&#27700;&#32447;&#12290;ExeKGLib&#20381;&#36182;&#20110;&#30693;&#35782;&#22270;&#35889;&#65292;&#20197;&#25913;&#21892;&#26500;&#24314;&#30340;ML&#24037;&#20316;&#27969;&#30340;&#36879;&#26126;&#24230;&#21644;&#21487;&#37325;&#29992;&#24615;&#65292;&#24182;&#30830;&#20445;&#20854;&#21487;&#25191;&#34892;&#24615;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;ExeKGLib&#30340;&#29992;&#27861;&#65292;&#24182;&#23558;&#20854;&#19982;&#20256;&#32479;&#30340;ML&#20195;&#30721;&#36827;&#34892;&#27604;&#36739;&#65292;&#20197;&#23637;&#31034;&#20854;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many machine learning (ML) libraries are accessible online for ML practitioners. Typical ML pipelines are complex and consist of a series of steps, each of them invoking several ML libraries. In this demo paper, we present ExeKGLib, a Python library that allows users with coding skills and minimal ML knowledge to build ML pipelines. ExeKGLib relies on knowledge graphs to improve the transparency and reusability of the built ML workflows, and to ensure that they are executable. We demonstrate the usage of ExeKGLib and compare it with conventional ML code to show its benefits.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#26435;&#37325;&#24433;&#21709;&#30340;&#35745;&#25968;&#36172;&#21338;&#26426;(WTB)&#35774;&#32622;&#65292;&#36890;&#36807;Repeated Exposure Optimality(REO)&#26469;&#30740;&#31350;&#23427;&#12290;&#20182;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#26469;&#28385;&#36275;REO&#65292;&#24182;&#25552;&#20379;&#20102;&#26368;&#20248;&#30340;&#36951;&#25022;&#36793;&#30028;&#12290;</title><link>http://arxiv.org/abs/2305.02955</link><description>&lt;p&gt;
&#21463;&#26435;&#37325;&#24433;&#21709;&#30340;&#35745;&#25968;&#36172;&#21338;&#26426;: &#36890;&#36807;&#37325;&#22797;&#26292;&#38706;&#26469;&#20811;&#26381;&#19981;&#21487;&#35299;&#24615;
&lt;/p&gt;
&lt;p&gt;
Weighted Tallying Bandits: Overcoming Intractability via Repeated Exposure Optimality. (arXiv:2305.02955v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02955
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#26435;&#37325;&#24433;&#21709;&#30340;&#35745;&#25968;&#36172;&#21338;&#26426;(WTB)&#35774;&#32622;&#65292;&#36890;&#36807;Repeated Exposure Optimality(REO)&#26469;&#30740;&#31350;&#23427;&#12290;&#20182;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#26469;&#28385;&#36275;REO&#65292;&#24182;&#25552;&#20379;&#20102;&#26368;&#20248;&#30340;&#36951;&#25022;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22312;&#32447;&#23398;&#20064;&#30340;&#25512;&#33616;&#31995;&#32479;&#25110;&#20247;&#21253;&#24212;&#29992;&#20013;&#65292;&#20154;&#31867;&#30340;&#20559;&#22909;&#25110;&#33021;&#21147;&#36890;&#24120;&#26159;&#31639;&#27861;&#26368;&#36817;&#34892;&#21160;&#30340;&#19968;&#20010;&#20989;&#25968;&#12290; &#30456;&#20851;&#24037;&#20316;&#24050;&#32463;&#24418;&#24335;&#21270;&#20102;&#35774;&#32622;&#65292;&#22312;&#36825;&#20123;&#35774;&#32622;&#20013;&#65292;&#34892;&#21160;&#30340;&#25439;&#22833;&#26159;&#26368;&#36817;$m$&#20010;&#26102;&#38388;&#27493;&#20013;&#35813;&#34892;&#21160;&#30340;&#25773;&#25918;&#27425;&#25968;&#30340;&#20989;&#25968;&#65292;&#20854;&#20013;$m$&#23545;&#24212;&#20110;&#20154;&#31867;&#35760;&#24518;&#33021;&#21147;&#30340;&#19978;&#38480;&#12290; &#20026;&#20102;&#26356;&#24544;&#23454;&#22320;&#21453;&#26144;&#20154;&#31867;&#35760;&#24518;&#38543;&#26102;&#38388;&#30340;&#34928;&#20943;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21463;&#26435;&#37325;&#24433;&#21709;&#30340;&#35745;&#25968;&#36172;&#21338;&#26426;(WTB)&#65292;&#23427;&#36890;&#36807;&#35201;&#27714;&#34892;&#21160;&#25439;&#22833;&#26159;&#26368;&#36817;$m$&#20010;&#26102;&#38388;&#27493;&#20013;&#35813;&#33218;&#34987;&#29609;&#30340;&#27425;&#25968;&#30340;&#21152;&#26435;&#24635;&#21644;&#30340;&#20989;&#25968;&#26469;&#27010;&#25324;&#36825;&#20010;&#35774;&#32622;&#12290;&#38500;&#38750;&#36827;&#19968;&#27493;&#20551;&#35774;&#65292;&#21542;&#21017;WTB&#35774;&#32622;&#26159;&#19981;&#21487;&#35299;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#22312;Repeated Exposure Optimality(REO)&#19979;&#30740;&#31350;&#20102;&#23427;&#65292;&#35813;&#26465;&#20214;&#26159;&#21463;&#20154;&#20307;&#29983;&#29702;&#23398;&#25991;&#29486;&#30340;&#21551;&#21457;&#65292;&#23427;&#35201;&#27714;&#23384;&#22312;&#19968;&#31181;&#34892;&#21160;&#65292;&#24403;&#21453;&#22797;&#25773;&#25918;&#26102;&#65292;&#26368;&#32456;&#23558;&#20135;&#29983;&#27604;&#20219;&#20309;&#20854;&#20182;&#34892;&#21160;&#26356;&#23567;&#30340;&#25439;&#22833;&#12290; &#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#28385;&#36275;WTB&#35774;&#32622;&#19979;&#30340;REO&#65292;&#24182;&#25552;&#20379;&#20102;&#26368;&#20248;&#22320;&#32553;&#25918;$m$&#21644;&#34892;&#21160;&#38598;&#22823;&#23567;&#30340;&#36951;&#25022;&#36793;&#30028;&#12290; &#25105;&#20204;&#30340;&#35777;&#26126;&#25216;&#26415;&#35201;&#27714;&#22312;&#19968;&#31181;&#35299;&#32806;&#24418;&#24335;&#19979;&#36827;&#34892;&#26032;&#39062;&#30340;&#27987;&#24230;&#32467;&#26524;&#65292;&#36825;&#21487;&#33021;&#26159;&#29420;&#31435;&#24863;&#20852;&#36259;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recommender system or crowdsourcing applications of online learning, a human's preferences or abilities are often a function of the algorithm's recent actions. Motivated by this, a significant line of work has formalized settings where an action's loss is a function of the number of times that action was recently played in the prior $m$ timesteps, where $m$ corresponds to a bound on human memory capacity. To more faithfully capture decay of human memory with time, we introduce the Weighted Tallying Bandit (WTB), which generalizes this setting by requiring that an action's loss is a function of a \emph{weighted} summation of the number of times that arm was played in the last $m$ timesteps. This WTB setting is intractable without further assumption. So we study it under Repeated Exposure Optimality (REO), a condition motivated by the literature on human physiology, which requires the existence of an action that when repetitively played will eventually yield smaller loss than any othe
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#22914;&#20309;&#22312;&#38544;&#31169;&#22686;&#24378;&#25216;&#26415;&#19979;&#65292;&#21033;&#29992;&#26799;&#24230;&#20449;&#24687;&#35782;&#21035;&#35757;&#32451;&#20013;&#24863;&#20852;&#36259;&#30340;&#25968;&#25454;&#26679;&#26412;&#36827;&#34892;&#25968;&#25454;&#36873;&#25321;&#21644;&#20272;&#20215;&#12290;</title><link>http://arxiv.org/abs/2305.02942</link><description>&lt;p&gt;
&#21033;&#29992;&#26799;&#24230;&#34913;&#37327;&#25968;&#25454;&#36873;&#25321;&#21644;&#20272;&#20215;&#22312;&#24046;&#20998;&#38544;&#31169;&#35757;&#32451;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Leveraging gradient-derived metrics for data selection and valuation in differentially private training. (arXiv:2305.02942v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02942
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#22914;&#20309;&#22312;&#38544;&#31169;&#22686;&#24378;&#25216;&#26415;&#19979;&#65292;&#21033;&#29992;&#26799;&#24230;&#20449;&#24687;&#35782;&#21035;&#35757;&#32451;&#20013;&#24863;&#20852;&#36259;&#30340;&#25968;&#25454;&#26679;&#26412;&#36827;&#34892;&#25968;&#25454;&#36873;&#25321;&#21644;&#20272;&#20215;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#30417;&#31649;&#25285;&#24551;&#21644;&#21442;&#19982;&#24230;&#30340;&#19981;&#36275;&#65292;&#20026;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#21327;&#20316;&#35757;&#32451;&#33719;&#21462;&#39640;&#36136;&#37327;&#25968;&#25454;&#21487;&#33021;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#38544;&#31169;&#22686;&#24378;&#25216;&#26415;&#65288;PET&#65289;&#26159;&#35299;&#20915;&#30417;&#31649;&#38382;&#39064;&#30340;&#19968;&#31181;&#24120;&#29992;&#26041;&#27861;&#65292;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#35757;&#32451;&#26159;&#20854;&#20013;&#26368;&#24120;&#29992;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;&#26799;&#24230;&#20449;&#24687;&#26469;&#35782;&#21035;&#38544;&#31169;&#35757;&#32451;&#20013;&#24863;&#20852;&#36259;&#30340;&#35757;&#32451;&#26679;&#26412;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#26368;&#20005;&#26684;&#30340;&#38544;&#31169;&#35774;&#32622;&#20013;&#65292;&#23384;&#22312;&#30528;&#33021;&#22815;&#20026;&#23458;&#25143;&#25552;&#20379;&#26377;&#21407;&#21017;&#30340;&#25968;&#25454;&#36873;&#25321;&#24037;&#20855;&#30340;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Obtaining high-quality data for collaborative training of machine learning models can be a challenging task due to A) the regulatory concerns and B) lack of incentive to participate. The first issue can be addressed through the use of privacy enhancing technologies (PET), one of the most frequently used one being differentially private (DP) training. The second challenge can be addressed by identifying which data points can be beneficial for model training and rewarding data owners for sharing this data. However, DP in deep learning typically adversely affects atypical (often informative) data samples, making it difficult to assess the usefulness of individual contributions. In this work we investigate how to leverage gradient information to identify training samples of interest in private training settings. We show that there exist techniques which are able to provide the clients with the tools for principled data selection even in strictest privacy settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31995;&#21015;&#38646;&#29031;&#39038;&#25552;&#31034;&#22312;&#20845;&#20010;&#26368;&#26032;&#21457;&#24067;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#38382;&#39064;&#22238;&#31572;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#33258;&#21160;&#25552;&#31034;&#21457;&#29616;&#30340;CoT&#25552;&#31034;&#21487;&#22312;&#26032;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#22312;&#24212;&#29992;&#20110;GPT-4&#27169;&#22411;&#26102;&#21462;&#24471;&#26368;&#20339;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.02897</link><description>&lt;p&gt;
&#33258;&#21160;&#21457;&#29616;&#30340;&#24605;&#32500;&#38142;&#25552;&#31034;&#21487;&#20197;&#25512;&#24191;&#21040;&#26032;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
An automatically discovered chain-of-thought prompt generalizes to novel models and datasets. (arXiv:2305.02897v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02897
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31995;&#21015;&#38646;&#29031;&#39038;&#25552;&#31034;&#22312;&#20845;&#20010;&#26368;&#26032;&#21457;&#24067;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#38382;&#39064;&#22238;&#31572;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#33258;&#21160;&#25552;&#31034;&#21457;&#29616;&#30340;CoT&#25552;&#31034;&#21487;&#22312;&#26032;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#22312;&#24212;&#29992;&#20110;GPT-4&#27169;&#22411;&#26102;&#21462;&#24471;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#20852;&#30340;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#25512;&#29702;&#33021;&#21147;&#26377;&#26395;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20808;&#21069;&#27169;&#22411;&#25152;&#21046;&#23450;&#30340;&#25552;&#31034;&#31574;&#30053;&#22914;&#20309;&#36866;&#29992;&#20110;&#26032;&#27169;&#22411;&#21644;&#19981;&#21516;&#25968;&#25454;&#38598;&#20173;&#23384;&#22312;&#19981;&#30830;&#23450;&#24615;&#12290;&#22312;&#36825;&#39033;&#23567;&#22411;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#19968;&#31995;&#21015;&#38646;&#29031;&#39038;&#25552;&#31034;&#65288;zero-shot prompts&#65289;&#30340;&#24615;&#33021;&#65292;&#20197;&#35825;&#23548;CoT&#25512;&#29702;&#65292;&#22312;6&#20010;&#26368;&#26032;&#21457;&#24067;&#30340;LLM&#65288;davinci-002&#65292;davinci-003&#65292;GPT-3.5-turbo&#65292;GPT-4&#65292;Flan-T5-xxl&#21644;Cohere command-xlarge&#65289;&#19978;&#19982;&#21253;&#25324;&#31185;&#23398;&#21644;&#21307;&#23398;&#39046;&#22495;&#30340;&#20845;&#20010;&#38382;&#31572;&#25968;&#25454;&#38598;&#28151;&#21512;&#22312;&#19968;&#36215;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36890;&#36807;&#33258;&#21160;&#25552;&#31034;&#21457;&#29616;&#30340;CoT&#25552;&#31034;&#22312;&#23454;&#39564;&#26465;&#20214;&#19979;&#34920;&#29616;&#20986;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#24212;&#29992;&#20110;&#26368;&#20808;&#36827;&#30340;GPT-4&#27169;&#22411;&#26102;&#20135;&#29983;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emergent chain-of-thought (CoT) reasoning capabilities promise to improve performance and explainability of large language models (LLMs). However, uncertainties remain about how prompting strategies formulated for previous model generations generalize to new model generations and different datasets. In this small-scale study we compare the performance of a range of zero-shot prompts for inducing CoT reasoning across six recently released LLMs (davinci-002, davinci-003, GPT-3.5-turbo, GPT-4, Flan-T5-xxl and Cohere command-xlarge) on a mixture of six question-answering datasets, including datasets from scientific and medical domains. We find that a CoT prompt that was previously discovered through automated prompt discovery shows robust performance across experimental conditions and produces best results when applied to the state-of-the-art model GPT-4.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#19968;&#31995;&#21015;&#36890;&#29992;&#23553;&#35013;&#65292;&#29992;&#20110;&#22122;&#22768;&#22686;&#24378;RL&#29615;&#22659;&#65292;&#25552;&#20379;&#20102;&#20004;&#31181;&#26032;&#30340;&#22686;&#24378;&#25216;&#26415;&#65292;&#26377;&#21161;&#20110;&#20195;&#29702;&#20154;&#25506;&#32034;&#21644;&#25913;&#21892;&#35757;&#32451;&#25968;&#25454;&#22810;&#26679;&#24615;&#12290;&#21516;&#26102;&#65292;&#20171;&#32461;&#20102;&#25511;&#21046;&#22122;&#22768;&#27880;&#20837;&#39057;&#29575;&#30340;&#36229;&#21442;&#25968;&#22122;&#22768;&#29575;&#12290;&#22312;&#23454;&#39564;&#20013;&#20351;&#29992;&#20102;&#19977;&#31181;&#27969;&#34892;&#30340;RL&#31639;&#27861;&#65292;Soft Actor-Critic&#65288;SAC&#65289;&#65292;Twin Delayed DDPG&#65288;TD3&#65289;&#21644;Proximal Policy&#65292;&#20197;&#30740;&#31350;&#36825;&#20123;&#23553;&#35013;&#23545;&#22238;&#25253;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.02882</link><description>&lt;p&gt;
&#31616;&#21333;&#30340;&#22024;&#26434;&#29615;&#22659;&#22686;&#24378;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Simple Noisy Environment Augmentation for Reinforcement Learning. (arXiv:2305.02882v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02882
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#19968;&#31995;&#21015;&#36890;&#29992;&#23553;&#35013;&#65292;&#29992;&#20110;&#22122;&#22768;&#22686;&#24378;RL&#29615;&#22659;&#65292;&#25552;&#20379;&#20102;&#20004;&#31181;&#26032;&#30340;&#22686;&#24378;&#25216;&#26415;&#65292;&#26377;&#21161;&#20110;&#20195;&#29702;&#20154;&#25506;&#32034;&#21644;&#25913;&#21892;&#35757;&#32451;&#25968;&#25454;&#22810;&#26679;&#24615;&#12290;&#21516;&#26102;&#65292;&#20171;&#32461;&#20102;&#25511;&#21046;&#22122;&#22768;&#27880;&#20837;&#39057;&#29575;&#30340;&#36229;&#21442;&#25968;&#22122;&#22768;&#29575;&#12290;&#22312;&#23454;&#39564;&#20013;&#20351;&#29992;&#20102;&#19977;&#31181;&#27969;&#34892;&#30340;RL&#31639;&#27861;&#65292;Soft Actor-Critic&#65288;SAC&#65289;&#65292;Twin Delayed DDPG&#65288;TD3&#65289;&#21644;Proximal Policy&#65292;&#20197;&#30740;&#31350;&#36825;&#20123;&#23553;&#35013;&#23545;&#22238;&#25253;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#26159;&#25552;&#21319;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#30340;&#19968;&#31181;&#24191;&#27867;&#24212;&#29992;&#30340;&#25216;&#26415;&#65292;&#23588;&#20854;&#26159;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#12290;&#26368;&#36817;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#24320;&#22987;&#24212;&#29992;&#22686;&#24378;&#25216;&#26415;&#26469;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#38382;&#39064;&#65292;&#20027;&#35201;&#19987;&#27880;&#20110;&#22522;&#20110;&#22270;&#20687;&#30340;&#22686;&#24378;&#12290;&#26412;&#25991;&#26088;&#22312;&#25506;&#35752;&#19968;&#31995;&#21015;&#36890;&#29992;&#23553;&#35013;&#65292;&#35774;&#35745;&#29992;&#20110;&#22122;&#22768;&#22686;&#24378;RL&#29615;&#22659;&#65292;&#24182;&#40723;&#21169;&#20195;&#29702;&#20154;&#25506;&#32034;&#21644;&#25913;&#21892;&#35757;&#32451;&#25968;&#25454;&#22810;&#26679;&#24615;&#65292;&#36866;&#29992;&#20110;&#24191;&#27867;&#30340;RL&#31639;&#27861;&#21644;&#29615;&#22659;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#38598;&#20013;&#20110;&#28041;&#21450;&#29366;&#24577;&#12289;&#22870;&#21169;&#21644;&#36716;&#25442;&#21160;&#24577;&#30340;&#22686;&#24378;&#65292;&#24182;&#20171;&#32461;&#20102;&#20004;&#31181;&#26032;&#30340;&#22686;&#24378;&#25216;&#26415;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#22122;&#22768;&#29575;&#36229;&#21442;&#25968;&#65292;&#20197;&#25511;&#21046;&#22122;&#22768;&#27880;&#20837;&#30340;&#39057;&#29575;&#12290;&#25105;&#20204;&#20351;&#29992;&#19977;&#31181;&#27969;&#34892;&#30340;RL&#31639;&#27861;&#65292;Soft Actor-Critic&#65288;SAC&#65289;&#65292;Twin Delayed DDPG&#65288;TD3&#65289;&#21644;Proximal Policy&#36827;&#34892;&#23454;&#39564;&#65292;&#30740;&#31350;&#36825;&#20123;&#23553;&#35013;&#23545;&#22238;&#25253;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data augmentation is a widely used technique for improving model performance in machine learning, particularly in computer vision and natural language processing. Recently, there has been increasing interest in applying augmentation techniques to reinforcement learning (RL) problems, with a focus on image-based augmentation. In this paper, we explore a set of generic wrappers designed to augment RL environments with noise and encourage agent exploration and improve training data diversity which are applicable to a broad spectrum of RL algorithms and environments. Specifically, we concentrate on augmentations concerning states, rewards, and transition dynamics and introduce two novel augmentation techniques. In addition, we introduce a noise rate hyperparameter for control over the frequency of noise injection. We present experimental results on the impact of these wrappers on return using three popular RL algorithms, Soft Actor-Critic (SAC), Twin Delayed DDPG (TD3), and Proximal Policy
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#35752;&#20102;GenZ&#23398;&#29983;&#21644;X&#19990;&#20195;&#12289;&#21315;&#31143;&#19968;&#20195;&#25945;&#24072;&#22312;&#39640;&#31561;&#25945;&#32946;&#20013;&#20351;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#30340;&#20307;&#39564;&#21644;&#24577;&#24230;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;GenZ&#23398;&#29983;&#26222;&#36941;&#30475;&#22909;GenAI&#30340;&#28508;&#22312;&#22909;&#22788;&#65292;&#32780;&#25945;&#24072;&#21017;&#26356;&#20851;&#27880;&#20854;&#22312;&#20262;&#29702;&#21644;&#25945;&#23398;&#19978;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.02878</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#20195;&#38469;&#24046;&#36317;&#65306;&#19982;X&#19990;&#20195;&#21644;&#21315;&#31143;&#19968;&#20195;&#30456;&#27604;&#65292;GenZ&#23398;&#29983;&#26356;&#24895;&#24847;&#37319;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;(&#22914;ChatGPT)&#22312;&#25945;&#23398;&#20013;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
The AI generation gap: Are Gen Z students more interested in adopting generative AI such as ChatGPT in teaching and learning than their Gen X and Millennial Generation teachers?. (arXiv:2305.02878v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02878
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#35752;&#20102;GenZ&#23398;&#29983;&#21644;X&#19990;&#20195;&#12289;&#21315;&#31143;&#19968;&#20195;&#25945;&#24072;&#22312;&#39640;&#31561;&#25945;&#32946;&#20013;&#20351;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#30340;&#20307;&#39564;&#21644;&#24577;&#24230;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;GenZ&#23398;&#29983;&#26222;&#36941;&#30475;&#22909;GenAI&#30340;&#28508;&#22312;&#22909;&#22788;&#65292;&#32780;&#25945;&#24072;&#21017;&#26356;&#20851;&#27880;&#20854;&#22312;&#20262;&#29702;&#21644;&#25945;&#23398;&#19978;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;GenZ&#23398;&#29983;&#19982;X&#19990;&#20195;&#21644;&#21315;&#31143;&#19968;&#20195;&#25945;&#24072;&#22312;&#20351;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#22312;&#39640;&#31561;&#25945;&#32946;&#20013;&#30340;&#32463;&#39564;&#12289;&#30475;&#27861;&#12289;&#30693;&#35782;&#12289;&#25285;&#24551;&#21644;&#24847;&#22270;&#12290;&#36890;&#36807;&#25307;&#21215;&#19968;&#23450;&#25968;&#37327;&#30340;&#23398;&#29983;&#21644;&#25945;&#24072;&#26469;&#36827;&#34892;&#35843;&#26597;&#65292;&#20351;&#29992;&#21253;&#25324;&#24320;&#25918;&#24335;&#21644;&#23553;&#38381;&#24335;&#38382;&#39064;&#30340;&#35843;&#26597;&#38382;&#21367;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;GenZ&#21442;&#19982;&#32773;&#26222;&#36941;&#20048;&#35266;&#22320;&#30475;&#24453;GenAI&#30340;&#28508;&#22312;&#30410;&#22788;&#65292;&#21253;&#25324;&#25552;&#39640;&#29983;&#20135;&#21147;&#12289;&#25928;&#29575;&#21644;&#20010;&#24615;&#21270;&#23398;&#20064;&#65292;&#24182;&#34920;&#36798;&#20102;&#22312;&#21508;&#31181;&#25945;&#32946;&#30446;&#30340;&#20013;&#20351;&#29992;GenAI&#30340;&#24847;&#21521;&#12290;X&#19990;&#20195;&#21644;&#21315;&#31143;&#19968;&#20195;&#25945;&#24072;&#25215;&#35748;GenAI&#30340;&#28508;&#22312;&#22909;&#22788;&#65292;&#20294;&#23545;&#20854;&#36807;&#24230;&#20381;&#36182;&#12289;&#20262;&#29702;&#21644;&#25945;&#23398;&#24433;&#21709;&#34920;&#31034;&#38271;&#26399;&#20851;&#27880;&#65292;&#24378;&#35843;&#38656;&#35201;&#36866;&#24403;&#30340;&#25351;&#21335;&#21644;&#25919;&#31574;&#26469;&#30830;&#20445;&#25216;&#26415;&#30340;&#36127;&#36131;&#20351;&#29992;&#12290;&#35813;&#30740;&#31350;&#31361;&#20986;&#20102;&#32467;&#21512;&#20256;&#32479;&#25945;&#23398;&#26041;&#27861;&#21644;&#25216;&#26415;&#65292;&#20197;&#25552;&#20379;&#26356;&#21152;&#26377;&#25928;&#30340;&#23398;&#20064;&#20307;&#39564;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study aimed to explore the experiences, perceptions, knowledge, concerns, and intentions of Gen Z students with Gen X and Gen Y teachers regarding the use of generative AI (GenAI) in higher education. A sample of students and teachers were recruited to investigate the above using a survey consisting of both open and closed questions. The findings showed that Gen Z participants were generally optimistic about the potential benefits of GenAI, including enhanced productivity, efficiency, and personalized learning, and expressed intentions to use GenAI for various educational purposes. Gen X and Gen Y teachers acknowledged the potential benefits of GenAI but expressed heightened concerns about overreliance, ethical and pedagogical implications, emphasizing the need for proper guidelines and policies to ensure responsible use of the technology. The study highlighted the importance of combining technology with traditional teaching methods to provide a more effective learning experience.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20998;&#23618;&#21487;&#25193;&#23637;&#22270;Transformer (HSGT)&#29992;&#20110;&#35299;&#20915;&#22270;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#35268;&#27169;&#38382;&#39064;&#21644;&#19978;&#19979;&#25991;&#20449;&#24687;&#25429;&#33719;&#19981;&#36275;&#38382;&#39064;&#65292;&#36890;&#36807;&#26500;&#24314;&#22810;&#23610;&#24230;&#22270;&#20998;&#23618;&#32467;&#26500;&#65292;HSGT&#23454;&#29616;&#20102;&#23545;&#22823;&#22411;&#22270;&#30340;&#24555;&#36895;&#21644;&#20869;&#23384;&#39640;&#25928;&#22788;&#29702;&#65292;&#24182;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.02866</link><description>&lt;p&gt;
&#20998;&#23618;Transformer&#29992;&#20110;&#21487;&#25193;&#23637;&#22270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Transformer for Scalable Graph Learning. (arXiv:2305.02866v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02866
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20998;&#23618;&#21487;&#25193;&#23637;&#22270;Transformer (HSGT)&#29992;&#20110;&#35299;&#20915;&#22270;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#35268;&#27169;&#38382;&#39064;&#21644;&#19978;&#19979;&#25991;&#20449;&#24687;&#25429;&#33719;&#19981;&#36275;&#38382;&#39064;&#65292;&#36890;&#36807;&#26500;&#24314;&#22810;&#23610;&#24230;&#22270;&#20998;&#23618;&#32467;&#26500;&#65292;HSGT&#23454;&#29616;&#20102;&#23545;&#22823;&#22411;&#22270;&#30340;&#24555;&#36895;&#21644;&#20869;&#23384;&#39640;&#25928;&#22788;&#29702;&#65292;&#24182;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;Transformer&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20013;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#65292;&#24182;&#22312;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#29616;&#20986;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24403;&#21069;&#23454;&#29616;&#30340;&#22270;Transformer&#20027;&#35201;&#38598;&#20013;&#22312;&#23398;&#20064;&#23567;&#35268;&#27169;&#22270;&#30340;&#34920;&#31034;&#19978;&#65292;&#20840;&#23616;&#33258;&#27880;&#24847;&#26426;&#21046;&#30340;&#20108;&#27425;&#22797;&#26434;&#24230;&#23545;&#20110;&#24212;&#29992;&#20110;&#36739;&#22823;&#35268;&#27169;&#22270;&#30340;&#20840;&#25209;&#37327;&#35757;&#32451;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#20256;&#32479;&#30340;&#22522;&#20110;&#37319;&#26679;&#30340;&#26041;&#27861;&#26080;&#27861;&#25429;&#25417;&#24517;&#35201;&#30340;&#39640;&#23618;&#27425;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#23548;&#33268;&#24615;&#33021;&#20005;&#37325;&#19979;&#38477;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20998;&#23618;&#21487;&#25193;&#23637;&#22270;Transformer (HSGT)&#20316;&#20026;&#36825;&#20123;&#25361;&#25112;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;HSGT&#25104;&#21151;&#22320;&#23558;Transformer&#26550;&#26500;&#25193;&#23637;&#21040;&#22823;&#35268;&#27169;&#22270;&#19978;&#30340;&#33410;&#28857;&#34920;&#31034;&#23398;&#20064;&#20219;&#21153;&#20013;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#24615;&#33021;&#12290;&#36890;&#36807;&#21033;&#29992;&#36890;&#36807;&#31895;&#21270;&#25216;&#26415;&#26500;&#24314;&#30340;&#22270;&#20998;&#23618;&#32467;&#26500;&#65292;HSGT&#26377;&#25928;&#22320;&#26356;&#26032;&#21644;&#23384;&#20648;&#22810;&#23610;&#24230;&#20449;&#24687;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#22823;&#22411;&#22270;&#30340;&#24555;&#36895;&#21644;&#20869;&#23384;&#39640;&#25928;&#22788;&#29702;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;HSGT&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Transformer is gaining increasing attention in the field of machine learning and has demonstrated state-of-the-art performance on benchmarks for graph representation learning. However, as current implementations of Graph Transformer primarily focus on learning representations of small-scale graphs, the quadratic complexity of the global self-attention mechanism presents a challenge for full-batch training when applied to larger graphs. Additionally, conventional sampling-based methods fail to capture necessary high-level contextual information, resulting in a significant loss of performance. In this paper, we introduce the Hierarchical Scalable Graph Transformer (HSGT) as a solution to these challenges. HSGT successfully scales the Transformer architecture to node representation learning tasks on large-scale graphs, while maintaining high performance. By utilizing graph hierarchies constructed through coarsening techniques, HSGT efficiently updates and stores multi-scale informat
&lt;/p&gt;</description></item><item><title>CausalAPM&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;NLU&#25991;&#26412;&#35299;&#32544;&#26694;&#26550;&#65292;&#23427;&#23558;&#25991;&#23383;&#21644;&#35821;&#20041;&#20449;&#24687;&#25237;&#23556;&#21040;&#29420;&#31435;&#30340;&#29305;&#24449;&#23376;&#31354;&#38388;&#20013;&#65292;&#24182;&#38480;&#21046;&#20102;&#25991;&#23383;&#20449;&#24687;&#22312;&#21518;&#32493;&#39044;&#27979;&#20013;&#30340;&#21442;&#19982;&#31243;&#24230;&#12290;&#35813;&#26694;&#26550;&#21487;&#20197;&#26377;&#25928;&#22320;&#24212;&#23545;&#25968;&#25454;&#38598;&#20559;&#32622;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#25512;&#24191;&#24615;&#33021;&#32780;&#19981;&#25439;&#22833;&#24615;&#33021;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2305.02865</link><description>&lt;p&gt;
CausalAPM: &#29992;&#20110;NLU&#21435;&#20559;&#32622;&#30340;&#36890;&#29992;&#25991;&#26412;&#35299;&#32544;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
CausalAPM: Generalizable Literal Disentanglement for NLU Debiasing. (arXiv:2305.02865v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02865
&lt;/p&gt;
&lt;p&gt;
CausalAPM&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;NLU&#25991;&#26412;&#35299;&#32544;&#26694;&#26550;&#65292;&#23427;&#23558;&#25991;&#23383;&#21644;&#35821;&#20041;&#20449;&#24687;&#25237;&#23556;&#21040;&#29420;&#31435;&#30340;&#29305;&#24449;&#23376;&#31354;&#38388;&#20013;&#65292;&#24182;&#38480;&#21046;&#20102;&#25991;&#23383;&#20449;&#24687;&#22312;&#21518;&#32493;&#39044;&#27979;&#20013;&#30340;&#21442;&#19982;&#31243;&#24230;&#12290;&#35813;&#26694;&#26550;&#21487;&#20197;&#26377;&#25928;&#22320;&#24212;&#23545;&#25968;&#25454;&#38598;&#20559;&#32622;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#25512;&#24191;&#24615;&#33021;&#32780;&#19981;&#25439;&#22833;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#38598;&#20559;&#32622;&#38382;&#39064;&#36234;&#26469;&#36234;&#24341;&#36215;&#20154;&#20204;&#23545;NLU&#27169;&#22411;&#25512;&#24191;&#33021;&#21147;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#20174;&#22240;&#26524;&#25512;&#26029;&#30340;&#35282;&#24230;&#20998;&#26512;&#20102;&#25968;&#25454;&#38598;&#20559;&#32622;&#38382;&#39064;&#30340;&#21407;&#22240;&#65292;&#25552;&#20986;&#20102;CausalAPM&#65292;&#19968;&#31181;&#36890;&#29992;&#30340;&#25991;&#26412;&#35299;&#32544;&#26694;&#26550;&#26469;&#35299;&#20915;&#20559;&#32622;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#23558;&#25991;&#23383;&#21644;&#35821;&#20041;&#20449;&#24687;&#25237;&#23556;&#21040;&#29420;&#31435;&#30340;&#29305;&#24449;&#23376;&#31354;&#38388;&#20013;&#65292;&#24182;&#38480;&#21046;&#20102;&#25991;&#23383;&#20449;&#24687;&#22312;&#21518;&#32493;&#39044;&#27979;&#20013;&#30340;&#21442;&#19982;&#31243;&#24230;&#12290;&#22312;&#19977;&#20010;NLP&#22522;&#20934;&#27979;&#35797;&#20013;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#26174;&#33879;&#25552;&#39640;&#20102;OOD&#30340;&#25512;&#24191;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;ID&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dataset bias, i.e., the over-reliance on dataset-specific literal heuristics, is getting increasing attention for its detrimental effect on the generalization ability of NLU models. Existing works focus on eliminating dataset bias by down-weighting problematic data in the training process, which induce the omission of valid feature information while mitigating bias. In this work, We analyze the causes of dataset bias from the perspective of causal inference and propose CausalAPM, a generalizable literal disentangling framework to ameliorate the bias problem from feature granularity. The proposed approach projects literal and semantic information into independent feature subspaces, and constrains the involvement of literal information in subsequent predictions. Extensive experiments on three NLP benchmarks (MNLI, FEVER, and QQP) demonstrate that our proposed framework significantly improves the OOD generalization performance while maintaining ID performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#19977;&#27493;&#39046;&#22495;&#28151;&#28102;&#26041;&#27861;&#65292;&#21253;&#25324;&#22522;&#20110;&#39057;&#29575;&#21644;&#27880;&#24847;&#21147;&#35268;&#33539;&#30340;&#36974;&#30422;&#12289;&#36974;&#30422;&#39046;&#22495;&#29305;&#23450;&#25552;&#31034;&#20449;&#24687;&#65292;&#20197;&#21450;&#25581;&#31034;&#39046;&#22495;&#36890;&#29992;&#19978;&#19979;&#25991;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#22312;&#39046;&#22495;&#36716;&#31227;&#26041;&#38754;&#26377;&#36739;&#22909;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.02858</link><description>&lt;p&gt;
ReMask&#65306;&#19968;&#31181;&#38024;&#23545;&#39046;&#22495;&#21453;&#20107;&#23454;&#29983;&#25104;&#30340;&#31283;&#20581;&#20449;&#24687;&#36974;&#30422;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ReMask: A Robust Information-Masking Approach for Domain Counterfactual Generation. (arXiv:2305.02858v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02858
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#19977;&#27493;&#39046;&#22495;&#28151;&#28102;&#26041;&#27861;&#65292;&#21253;&#25324;&#22522;&#20110;&#39057;&#29575;&#21644;&#27880;&#24847;&#21147;&#35268;&#33539;&#30340;&#36974;&#30422;&#12289;&#36974;&#30422;&#39046;&#22495;&#29305;&#23450;&#25552;&#31034;&#20449;&#24687;&#65292;&#20197;&#21450;&#25581;&#31034;&#39046;&#22495;&#36890;&#29992;&#19978;&#19979;&#25991;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#22312;&#39046;&#22495;&#36716;&#31227;&#26041;&#38754;&#26377;&#36739;&#22909;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#20559;&#31227;&#26159;NLP&#20013;&#30340;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#65292;&#22240;&#27492;&#35768;&#22810;&#26041;&#27861;&#37319;&#29992;&#23398;&#20064;&#39046;&#22495;&#19981;&#21464;&#29305;&#24449;&#26469;&#20943;&#36731;&#25512;&#29702;&#38454;&#27573;&#30340;&#39046;&#22495;&#20559;&#31227;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#26080;&#27861;&#21033;&#29992;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#39046;&#22495;&#29305;&#23450;&#32454;&#24494;&#24046;&#21035;&#12290;&#20026;&#36991;&#20813;&#36825;&#31181;&#32570;&#28857;&#65292;&#39046;&#22495;&#21453;&#20107;&#23454;&#29983;&#25104;&#26088;&#22312;&#23558;&#28304;&#22495;&#20013;&#30340;&#25991;&#26412;&#36716;&#25442;&#20026;&#32473;&#23450;&#30340;&#30446;&#26631;&#22495;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#25454;&#30340;&#26377;&#38480;&#24615;&#65292;&#36825;&#31181;&#22522;&#20110;&#39057;&#29575;&#30340;&#26041;&#27861;&#32463;&#24120;&#20250;&#38169;&#36807;&#19968;&#20123;&#26377;&#25928;&#21644;&#34394;&#20551;&#30340;&#39046;&#22495;&#26631;&#35760;&#20851;&#32852;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#19977;&#27493;&#39046;&#22495;&#28151;&#28102;&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;&#22522;&#20110;&#39057;&#29575;&#21644;&#27880;&#24847;&#21147;&#35268;&#33539;&#30340;&#36974;&#30422;&#12289;&#36974;&#30422;&#39046;&#22495;&#29305;&#23450;&#25552;&#31034;&#20449;&#24687;&#65292;&#20197;&#21450;&#25581;&#31034;&#39046;&#22495;&#36890;&#29992;&#19978;&#19979;&#25991;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#23454;&#35777;&#34920;&#26126;&#65292;&#26469;&#33258;&#25105;&#20204;&#36974;&#30422;&#25991;&#26412;&#30340;&#21453;&#20107;&#23454;&#26679;&#26412;&#22312;12&#20010;&#39046;&#22495;&#24773;&#24863;&#20998;&#31867;&#35774;&#32622;&#20013;&#26377;10&#20010;&#23454;&#29616;&#20102;&#25913;&#36827;&#30340;&#39046;&#22495;&#36716;&#31227;&#65292;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#25216;&#26415;&#24179;&#22343;&#25552;&#39640;&#20102;2%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain shift is a big challenge in NLP, thus, many approaches resort to learning domain-invariant features to mitigate the inference phase domain shift. Such methods, however, fail to leverage the domain-specific nuances relevant to the task at hand. To avoid such drawbacks, domain counterfactual generation aims to transform a text from the source domain to a given target domain. However, due to the limited availability of data, such frequency-based methods often miss and lead to some valid and spurious domain-token associations. Hence, we employ a three-step domain obfuscation approach that involves frequency and attention norm-based masking, to mask domain-specific cues, and unmasking to regain the domain generic context. Our experiments empirically show that the counterfactual samples sourced from our masked text lead to improved domain transfer on 10 out of 12 domain sentiment classification settings, with an average of 2% accuracy improvement over the state-of-the-art for unsuperv
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26368;&#22823;&#22240;&#26524;&#29109;&#36870;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#26368;&#22823;&#22240;&#26524;&#29109;&#21407;&#29702;&#26469;&#23398;&#20064;&#20195;&#29702;&#36981;&#23432;&#38480;&#21046;&#30340;&#32422;&#26463;&#26465;&#20214;&#21644;&#26368;&#20248;&#31574;&#30053;&#65292;&#20351;&#29992;&#36981;&#23432;&#36825;&#20123;&#38480;&#21046;&#30340;&#20195;&#29702;&#30340;&#23454;&#20363;&#36827;&#34892;&#23398;&#20064;&#12290;&#27492;&#26041;&#27861;&#24050;&#34987;&#35777;&#26126;&#22312;&#21508;&#31181;&#20219;&#21153;&#21644;&#29615;&#22659;&#20013;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.02857</link><description>&lt;p&gt;
&#26368;&#22823;&#22240;&#26524;&#29109;&#36870;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Maximum Causal Entropy Inverse Constrained Reinforcement Learning. (arXiv:2305.02857v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02857
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26368;&#22823;&#22240;&#26524;&#29109;&#36870;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#26368;&#22823;&#22240;&#26524;&#29109;&#21407;&#29702;&#26469;&#23398;&#20064;&#20195;&#29702;&#36981;&#23432;&#38480;&#21046;&#30340;&#32422;&#26463;&#26465;&#20214;&#21644;&#26368;&#20248;&#31574;&#30053;&#65292;&#20351;&#29992;&#36981;&#23432;&#36825;&#20123;&#38480;&#21046;&#30340;&#20195;&#29702;&#30340;&#23454;&#20363;&#36827;&#34892;&#23398;&#20064;&#12290;&#27492;&#26041;&#27861;&#24050;&#34987;&#35777;&#26126;&#22312;&#21508;&#31181;&#20219;&#21153;&#21644;&#29615;&#22659;&#20013;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#65292;&#24403;&#20154;&#20204;&#19982;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#36827;&#34892;&#20132;&#20114;&#26102;&#65292;&#20195;&#29702;&#30340;&#34892;&#20026;&#19982;&#35813;&#29615;&#22659;&#30340;&#20215;&#20540;&#35266;&#12289;&#31038;&#20250;&#35268;&#33539;&#25110;&#20854;&#20182;&#35201;&#27714;&#30456;&#19968;&#33268;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29615;&#22659;&#37117;&#26377;&#38590;&#20197;&#35268;&#23450;&#21644;&#36716;&#31227;&#32473;&#23398;&#20064;&#20195;&#29702;&#30340;&#38544;&#21547;&#38480;&#21046;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#26368;&#22823;&#22240;&#26524;&#29109;&#21407;&#29702;&#26469;&#23398;&#20064;&#20195;&#29702;&#36981;&#23432;&#38480;&#21046;&#30340;&#32422;&#26463;&#26465;&#20214;&#21644;&#26368;&#20248;&#31574;&#30053;&#65292;&#24182;&#20351;&#29992;&#36981;&#23432;&#36825;&#20123;&#38480;&#21046;&#30340;&#20195;&#29702;&#30340;&#23454;&#20363;&#36827;&#34892;&#23398;&#20064;&#12290;&#25105;&#20204;&#22312;&#34920;&#26684;&#35774;&#32622;&#20013;&#35777;&#26126;&#20102;&#25910;&#25947;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22797;&#26434;&#29615;&#22659;&#30340;&#36817;&#20284;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#35780;&#20272;&#25152;&#33719;&#24471;&#30340;&#22870;&#21169;&#21644;&#36829;&#21453;&#32422;&#26463;&#30340;&#27425;&#25968;&#26469;&#35780;&#20272;&#23398;&#20064;&#21040;&#30340;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#26681;&#25454;&#20854;&#23545;&#20854;&#20182;&#20195;&#29702;&#30340;&#21487;&#36716;&#31227;&#24615;&#26469;&#35780;&#20272;&#23398;&#20064;&#21040;&#30340;&#25104;&#26412;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24050;&#34987;&#35777;&#26126;&#22312;&#21508;&#31181;&#20219;&#21153;&#21644;&#29615;&#22659;&#20013;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
When deploying artificial agents in real-world environments where they interact with humans, it is crucial that their behavior is aligned with the values, social norms or other requirements of that environment. However, many environments have implicit constraints that are difficult to specify and transfer to a learning agent. To address this challenge, we propose a novel method that utilizes the principle of maximum causal entropy to learn constraints and an optimal policy that adheres to these constraints, using demonstrations of agents that abide by the constraints. We prove convergence in a tabular setting and provide an approximation which scales to complex environments. We evaluate the effectiveness of the learned policy by assessing the reward received and the number of constraint violations, and we evaluate the learned cost function based on its transferability to other agents. Our method has been shown to outperform state-of-the-art approaches across a variety of tasks and envi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22122;&#22768;&#25233;&#21046;&#30340;&#22810;&#27169;&#24577;&#21464;&#21387;&#22120; (NORM.TR)&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;NRGF&#25552;&#21462;&#22120;&#21644;&#21464;&#21387;&#22120;&#26469;&#25552;&#39640;&#22810;&#27169;&#24577;&#24773;&#24863;&#35782;&#21035;&#20219;&#21153;&#30340;&#40065;&#26834;&#24615;, &#35797;&#22270;&#22312;&#20854;&#27969;&#31243;&#20013;&#25552;&#21462;&#22122;&#22768;&#25233;&#21046;&#29305;&#24449;&#24182;&#24341;&#20837;&#22122;&#22768;&#24863;&#30693;&#23398;&#20064;&#26041;&#26696;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;NORM-TR&#20855;&#26377;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.02814</link><description>&lt;p&gt;
&#22122;&#22768;&#25233;&#21046;&#30340;&#22810;&#27169;&#24577;&#21464;&#21387;&#22120;&#29992;&#20110;&#24773;&#24863;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Noise-Resistant Multimodal Transformer for Emotion Recognition. (arXiv:2305.02814v1 [cs.MM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02814
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22122;&#22768;&#25233;&#21046;&#30340;&#22810;&#27169;&#24577;&#21464;&#21387;&#22120; (NORM.TR)&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;NRGF&#25552;&#21462;&#22120;&#21644;&#21464;&#21387;&#22120;&#26469;&#25552;&#39640;&#22810;&#27169;&#24577;&#24773;&#24863;&#35782;&#21035;&#20219;&#21153;&#30340;&#40065;&#26834;&#24615;, &#35797;&#22270;&#22312;&#20854;&#27969;&#31243;&#20013;&#25552;&#21462;&#22122;&#22768;&#25233;&#21046;&#29305;&#24449;&#24182;&#24341;&#20837;&#22122;&#22768;&#24863;&#30693;&#23398;&#20064;&#26041;&#26696;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;NORM-TR&#20855;&#26377;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#24773;&#24863;&#35782;&#21035;&#21487;&#20197;&#20174;&#21508;&#31181;&#25968;&#25454;&#27169;&#24577;(&#22914;&#35270;&#39057;&#12289;&#25991;&#26412;&#21644;&#38899;&#39057;)&#20013;&#35782;&#21035;&#20154;&#31867;&#30340;&#24773;&#24863;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#39033;&#20219;&#21153;&#24456;&#23481;&#26131;&#21463;&#21040;&#19981;&#21253;&#21547;&#26377;&#29992;&#35821;&#20041;&#30340;&#22122;&#22768;&#20449;&#24687;&#30340;&#24433;&#21709;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#20363;&#65292;&#35797;&#22270;&#22312;&#20854;&#27969;&#31243;&#20013;&#25552;&#21462;&#22122;&#22768;&#25233;&#21046;&#29305;&#24449;&#24182;&#24341;&#20837;&#22122;&#22768;&#24863;&#30693;&#23398;&#20064;&#26041;&#26696;&#65292;&#20197;&#26377;&#25928;&#25552;&#39640;&#22810;&#27169;&#24577;&#24773;&#24863;&#29702;&#35299;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#26032;&#27969;&#31243;&#65292;&#21363;&#22122;&#22768;&#25233;&#21046;&#30340;&#22810;&#27169;&#24577;&#21464;&#21387;&#22120;(NORM-TR)&#65292;&#20027;&#35201;&#20171;&#32461;&#20102;&#19968;&#20010;&#22122;&#22768;&#25233;&#21046;&#36890;&#29992;&#29305;&#24449;(NRGF)&#25552;&#21462;&#22120;&#21644;&#29992;&#20110;&#22810;&#27169;&#24577;&#24773;&#24863;&#35782;&#21035;&#20219;&#21153;&#30340;&#21464;&#21387;&#22120;&#12290;&#23588;&#20854;&#26159;&#65292;&#25105;&#20204;&#20351;NRGF&#25552;&#21462;&#22120;&#23398;&#20064;&#36890;&#29992;&#21644;&#19981;&#21463;&#25200;&#21160;&#30340;&#34920;&#31034;&#24418;&#24335;&#65292;&#20197;&#20415;&#33719;&#24471;&#19968;&#33268;&#19988;&#26377;&#24847;&#20041;&#30340;&#35821;&#20041;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24212;&#29992;&#21464;&#21387;&#22120;&#26469;&#32467;&#21512;&#22810;&#27169;&#24577;&#36755;&#20837;&#30340;&#22810;&#27169;&#24577;&#29305;&#24449;(MFs)&#65292;&#22522;&#20110;&#23427;&#20204;&#19982;NRGF&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#22240;&#27492;&#65292;&#21487;&#33021;&#30340;&#19981;&#25935;&#24863;&#37096;&#20998;&#21487;&#20197;&#33258;&#28982;&#22320;&#34987;&#25233;&#21046;&#65292;&#36827;&#32780;&#25552;&#39640;&#27169;&#22411;&#22312;&#22024;&#26434;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;NORM-TR&#22312;&#21508;&#31181;&#23454;&#38469;&#30340;&#22810;&#27169;&#24577;&#24773;&#24863;&#35782;&#21035;&#22522;&#20934;&#19978;&#37117;&#20855;&#26377;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#34920;&#26126;&#25105;&#20204;&#30340;&#22122;&#22768;&#25233;&#21046;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal emotion recognition identifies human emotions from various data modalities like video, text, and audio. However, we found that this task can be easily affected by noisy information that does not contain useful semantics. To this end, we present a novel paradigm that attempts to extract noise-resistant features in its pipeline and introduces a noise-aware learning scheme to effectively improve the robustness of multimodal emotion understanding. Our new pipeline, namely Noise-Resistant Multimodal Transformer (NORM-TR), mainly introduces a Noise-Resistant Generic Feature (NRGF) extractor and a Transformer for the multimodal emotion recognition task. In particular, we make the NRGF extractor learn a generic and disturbance-insensitive representation so that consistent and meaningful semantics can be obtained. Furthermore, we apply a Transformer to incorporate Multimodal Features (MFs) of multimodal inputs based on their relations to the NRGF. Therefore, the possible insensitive 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#21644;&#27880;&#24847;&#26426;&#21046;&#30340;&#35821;&#20041;&#20998;&#21106;&#26041;&#27861;MTLSegFormer&#65292;&#22312;&#31934;&#32454;&#21270;&#20892;&#19994;&#39046;&#22495;&#20013;&#30340;&#20316;&#29289;&#25490;&#26597;&#21644;&#20316;&#29289;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26368;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.02813</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#22312;&#20892;&#19994;&#31934;&#32454;&#21270;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
MTLSegFormer: Multi-task Learning with Transformers for Semantic Segmentation in Precision Agriculture. (arXiv:2305.02813v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02813
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#21644;&#27880;&#24847;&#26426;&#21046;&#30340;&#35821;&#20041;&#20998;&#21106;&#26041;&#27861;MTLSegFormer&#65292;&#22312;&#31934;&#32454;&#21270;&#20892;&#19994;&#39046;&#22495;&#20013;&#30340;&#20316;&#29289;&#25490;&#26597;&#21644;&#20316;&#29289;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26368;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;&#24050;&#34987;&#35777;&#26126;&#22312;&#25552;&#39640;&#30456;&#20851;&#20219;&#21153;&#30340;&#24615;&#33021;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26041;&#27861;&#20351;&#29992;&#20027;&#24178;&#32593;&#25552;&#21462;&#29420;&#31435;&#20219;&#21153;&#30340;&#21021;&#22987;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#23558;&#20998;&#25903;&#30340;&#29305;&#24449;&#22270;&#36830;&#25509;&#25110;&#27714;&#21644;&#26469;&#23454;&#29616;&#20998;&#25903;&#20043;&#38388;&#30340;&#20449;&#24687;&#20132;&#25442;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20449;&#24687;&#20132;&#25442;&#24182;&#27809;&#26377;&#30452;&#25509;&#32771;&#34385;&#22270;&#20687;&#30340;&#23616;&#37096;&#29305;&#24449;&#20197;&#21450;&#20219;&#21153;&#38388;&#30340;&#37325;&#35201;&#31243;&#24230;&#25110;&#30456;&#20851;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#20041;&#20998;&#21106;&#26041;&#27861;MTLSegFormer&#65292;&#23427;&#32467;&#21512;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#27880;&#24847;&#26426;&#21046;&#12290;&#22312;&#20027;&#24178;&#32593;&#29305;&#24449;&#25552;&#21462;&#21518;&#65292;&#20026;&#27599;&#20010;&#20219;&#21153;&#23398;&#20064;&#20004;&#20010;&#29305;&#24449;&#22270;&#12290;&#31532;&#19968;&#20010;&#29305;&#24449;&#22270;&#26088;&#22312;&#23398;&#20064;&#19982;&#20854;&#20219;&#21153;&#30456;&#20851;&#30340;&#29305;&#24449;&#65292;&#32780;&#31532;&#20108;&#20010;&#29305;&#24449;&#22270;&#26159;&#36890;&#36807;&#24212;&#29992;&#23398;&#20064;&#21040;&#30340;&#35270;&#35273;&#27880;&#24847;&#21147;&#26469;&#23545;&#20854;&#20182;&#20219;&#21153;&#30340;&#29305;&#24449;&#22270;&#36827;&#34892;&#23616;&#37096;&#21152;&#26435;&#24471;&#21040;&#30340;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#20026;&#20854;&#20182;&#20219;&#21153;&#30340;&#22270;&#20687;&#23616;&#37096;&#21306;&#22495;&#20998;&#37197;&#20102;&#26435;&#37325;&#65292;&#26174;&#30528;&#25552;&#39640;&#20102;&#27599;&#20010;&#20219;&#21153;&#30340;&#20998;&#21106;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#31934;&#32454;&#21270;&#20892;&#19994;&#39046;&#22495;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#20004;&#20010;&#20219;&#21153;&#65306;&#20316;&#29289;&#25490;&#26597;&#21644;&#20316;&#29289;&#20998;&#31867;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20004;&#20010;&#20219;&#21153;&#20013;&#22343;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-task learning has proven to be effective in improving the performance of correlated tasks. Most of the existing methods use a backbone to extract initial features with independent branches for each task, and the exchange of information between the branches usually occurs through the concatenation or sum of the feature maps of the branches. However, this type of information exchange does not directly consider the local characteristics of the image nor the level of importance or correlation between the tasks. In this paper, we propose a semantic segmentation method, MTLSegFormer, which combines multi-task learning and attention mechanisms. After the backbone feature extraction, two feature maps are learned for each task. The first map is proposed to learn features related to its task, while the second map is obtained by applying learned visual attention to locally re-weigh the feature maps of the other tasks. In this way, weights are assigned to local regions of the image of other 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#23384;&#22312;&#20559;&#35265;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#23376;&#27169;&#20989;&#25968;&#26469;&#20248;&#21270;&#25512;&#33616;&#31995;&#32479;&#12290;&#20808;&#21069;&#30740;&#31350;&#25351;&#20986;&#65292;&#22522;&#20110;&#20844;&#24179;&#24615;&#32422;&#26463;&#30340;&#24178;&#39044;&#21487;&#20197;&#30830;&#20445;&#27604;&#20363;&#20195;&#34920;&#24615;&#65292;&#24182;&#22312;&#23384;&#22312;&#20559;&#35265;&#26102;&#33719;&#24471;&#25509;&#36817;&#26368;&#20248;&#30340;&#25928;&#29992;&#12290;&#32780;&#26412;&#25991;&#21017;&#25506;&#35752;&#20102;&#19968;&#32452;&#33021;&#22815;&#25429;&#25417;&#36825;&#31181;&#30446;&#30340;&#30340;&#23376;&#27169;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2305.02806</link><description>&lt;p&gt;
&#22312;&#23384;&#22312;&#20559;&#35265;&#30340;&#24773;&#20917;&#19979;&#65292;&#26368;&#22823;&#21270;&#23376;&#27169;&#20989;&#25968;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Maximizing Submodular Functions for Recommendation in the Presence of Biases. (arXiv:2305.02806v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02806
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#23384;&#22312;&#20559;&#35265;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#23376;&#27169;&#20989;&#25968;&#26469;&#20248;&#21270;&#25512;&#33616;&#31995;&#32479;&#12290;&#20808;&#21069;&#30740;&#31350;&#25351;&#20986;&#65292;&#22522;&#20110;&#20844;&#24179;&#24615;&#32422;&#26463;&#30340;&#24178;&#39044;&#21487;&#20197;&#30830;&#20445;&#27604;&#20363;&#20195;&#34920;&#24615;&#65292;&#24182;&#22312;&#23384;&#22312;&#20559;&#35265;&#26102;&#33719;&#24471;&#25509;&#36817;&#26368;&#20248;&#30340;&#25928;&#29992;&#12290;&#32780;&#26412;&#25991;&#21017;&#25506;&#35752;&#20102;&#19968;&#32452;&#33021;&#22815;&#25429;&#25417;&#36825;&#31181;&#30446;&#30340;&#30340;&#23376;&#27169;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23376;&#38598;&#36873;&#25321;&#20219;&#21153;&#22312;&#25512;&#33616;&#31995;&#32479;&#21644;&#25628;&#32034;&#24341;&#25806;&#20013;&#32463;&#24120;&#20986;&#29616;&#65292;&#35201;&#27714;&#36873;&#25321;&#19968;&#20123;&#26368;&#22823;&#21270;&#29992;&#25143;&#20215;&#20540;&#30340;&#29289;&#21697;&#23376;&#38598;&#12290;&#23376;&#38598;&#30340;&#20215;&#20540;&#24448;&#24448;&#21576;&#29616;&#20986;&#36882;&#20943;&#30340;&#22238;&#25253;&#65292;&#22240;&#27492;&#65292;&#20351;&#29992;&#23376;&#27169;&#20989;&#25968;&#26469;&#24314;&#27169;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#21457;&#29616;&#36755;&#20837;&#20855;&#26377;&#31038;&#20250;&#20559;&#35265;&#65292;&#20250;&#38477;&#20302;&#36755;&#20986;&#23376;&#38598;&#30340;&#25928;&#29992;&#65292;&#22240;&#27492;&#38656;&#35201;&#24178;&#39044;&#20197;&#25552;&#39640;&#20854;&#25928;&#29992;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#32452;&#23376;&#27169;&#20989;&#25968;&#30340;&#26368;&#22823;&#21270;&#65292;&#36825;&#20123;&#20989;&#25968;&#28085;&#30422;&#20102;&#19978;&#36848;&#24212;&#29992;&#20013;&#20986;&#29616;&#30340;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Subset selection tasks, arise in recommendation systems and search engines and ask to select a subset of items that maximize the value for the user. The values of subsets often display diminishing returns, and hence, submodular functions have been used to model them. If the inputs defining the submodular function are known, then existing algorithms can be used. In many applications, however, inputs have been observed to have social biases that reduce the utility of the output subset. Hence, interventions to improve the utility are desired. Prior works focus on maximizing linear functions -- a special case of submodular functions -- and show that fairness constraint-based interventions can not only ensure proportional representation but also achieve near-optimal utility in the presence of biases. We study the maximization of a family of submodular functions that capture functions arising in the aforementioned applications. Our first result is that, unlike linear functions, constraint-ba
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26681;&#25454;&#23616;&#37096;&#26368;&#20248;&#35299;&#30456;&#20851;&#24615;&#30340;&#31639;&#23376;&#20851;&#31995;&#20998;&#26512;&#21644;&#24230;&#37327;&#26041;&#27861;&#65292;&#24182;&#26681;&#25454;&#36825;&#20010;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#31639;&#23376;&#36873;&#25321;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.02805</link><description>&lt;p&gt;
&#26412;&#22320;&#26368;&#20248;&#35299;&#30456;&#20851;&#24615;&#36741;&#21161;&#33258;&#36866;&#24212;&#31639;&#23376;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Local Optima Correlation Assisted Adaptive Operator Selection. (arXiv:2305.02805v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02805
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26681;&#25454;&#23616;&#37096;&#26368;&#20248;&#35299;&#30456;&#20851;&#24615;&#30340;&#31639;&#23376;&#20851;&#31995;&#20998;&#26512;&#21644;&#24230;&#37327;&#26041;&#27861;&#65292;&#24182;&#26681;&#25454;&#36825;&#20010;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#31639;&#23376;&#36873;&#25321;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20351;&#29992;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#35299;&#20915;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#26102;&#65292;&#19981;&#21516;&#30340;&#25628;&#32034;&#31639;&#23376;&#34987;&#24212;&#29992;&#20110;&#22312;&#32473;&#23450;&#35299;&#30340;&#37051;&#22495;&#20013;&#37319;&#26679;&#26032;&#35299;&#12290;&#20102;&#35299;&#31639;&#23376;&#20043;&#38388;&#30340;&#20851;&#31995;&#23545;&#20110;&#21508;&#31181;&#30446;&#30340;&#37117;&#24456;&#37325;&#35201;&#65292;&#20363;&#22914;&#36866;&#24212;&#24615;&#22320;&#20915;&#23450;&#20309;&#26102;&#20351;&#29992;&#21738;&#20010;&#31639;&#23376;&#20197;&#26377;&#25928;&#22320;&#25214;&#21040;&#26368;&#20248;&#35299;&#12290;&#28982;&#32780;&#65292;&#22312;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#30340;&#22797;&#26434;&#35299;&#31354;&#38388;&#20013;&#65292;&#29702;&#35770;&#20998;&#26512;&#36825;&#31181;&#20851;&#31995;&#26159;&#22256;&#38590;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26681;&#25454;&#23616;&#37096;&#26368;&#20248;&#35299;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#26469;&#32463;&#39564;&#20998;&#26512;&#31639;&#23376;&#20043;&#38388;&#20851;&#31995;&#24182;&#24230;&#37327;&#20854;&#20851;&#31995;&#30340;&#26041;&#27861;&#12290;&#22312;&#24191;&#27867;&#30340;&#24102;&#26377;&#23481;&#37327;&#38480;&#21046;&#30340;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#22522;&#20934;&#23454;&#20363;&#19978;&#36827;&#34892;&#30340;&#20840;&#38754;&#20998;&#26512;&#34920;&#26126;&#65292;&#24120;&#29992;&#31639;&#23376;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#20855;&#26377;&#19968;&#33268;&#30340;&#27169;&#24335;&#12290;&#22522;&#20110;&#26032;&#25552;&#20986;&#30340;&#23616;&#37096;&#26368;&#20248;&#35299;&#30456;&#20851;&#24230;&#37327;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#31639;&#23376;&#36873;&#25321;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
For solving combinatorial optimisation problems with metaheuristics, different search operators are applied for sampling new solutions in the neighbourhood of a given solution. It is important to understand the relationship between operators for various purposes, e.g., adaptively deciding when to use which operator to find optimal solutions efficiently. However, it is difficult to theoretically analyse this relationship, especially in the complex solution space of combinatorial optimisation problems. In this paper, we propose to empirically analyse the relationship between operators in terms of the correlation between their local optima and develop a measure for quantifying their relationship. The comprehensive analyses on a wide range of capacitated vehicle routing problem benchmark instances show that there is a consistent pattern in the correlation between commonly used operators. Based on this newly proposed local optima correlation metric, we propose a novel approach for adaptivel
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Ansible Wisdom&#30340;&#33258;&#28982;&#35821;&#35328;&#36716;Ansible-YAML&#20195;&#30721;&#30340;&#24037;&#20855;&#65292;&#21487;&#33258;&#21160;&#21270;&#29983;&#25104;Ansible&#33050;&#26412;&#65292;&#25552;&#39640;IT&#33258;&#21160;&#21270;&#29983;&#20135;&#21147;&#65292;&#24182;&#30456;&#27604;&#29616;&#26377;&#25216;&#26415;&#36798;&#21040;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2305.02783</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#20449;&#24687;&#25216;&#26415;&#20219;&#21153;&#20013;&#33258;&#21160;&#29983;&#25104;YAML&#20195;&#30721;
&lt;/p&gt;
&lt;p&gt;
Automated Code generation for Information Technology Tasks in YAML through Large Language Models. (arXiv:2305.02783v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02783
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Ansible Wisdom&#30340;&#33258;&#28982;&#35821;&#35328;&#36716;Ansible-YAML&#20195;&#30721;&#30340;&#24037;&#20855;&#65292;&#21487;&#33258;&#21160;&#21270;&#29983;&#25104;Ansible&#33050;&#26412;&#65292;&#25552;&#39640;IT&#33258;&#21160;&#21270;&#29983;&#20135;&#21147;&#65292;&#24182;&#30456;&#27604;&#29616;&#26377;&#25216;&#26415;&#36798;&#21040;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#30340;&#19981;&#26029;&#25552;&#21319;&#65292;&#22312;&#36890;&#29992;&#32534;&#31243;&#35821;&#35328;&#26041;&#38754;&#30340;&#21463;&#30410;&#26368;&#22823;&#65292;&#32780;&#38024;&#23545;IT&#33258;&#21160;&#21270;&#31561;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#30340;&#30740;&#31350;&#36739;&#23569;&#12290;&#26412;&#30740;&#31350;&#32858;&#28966;&#20110;Ansible-YAML&#30340;&#29983;&#25104;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Ansible Wisdom&#30340;&#33258;&#28982;&#35821;&#35328;&#36716;Ansible-YAML&#20195;&#30721;&#30340;&#24037;&#20855;&#65292;&#26088;&#22312;&#25552;&#39640;IT&#33258;&#21160;&#21270;&#29983;&#20135;&#21147;&#12290;&#30740;&#31350;&#37319;&#29992;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#26032;&#30340;&#21253;&#21547;Ansible-YAML&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#25193;&#23637;&#35757;&#32451;&#12290;&#21516;&#26102;&#65292;&#36824;&#24320;&#21457;&#20102;&#20004;&#20010;&#29992;&#20110;&#25429;&#25417;&#27492;&#39046;&#22495;&#29305;&#24449;&#30340;YAML&#21644;Ansible&#24615;&#33021;&#25351;&#26631;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;Ansible Wisdom&#21487;&#20197;&#31934;&#30830;&#22320;&#20174;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#20013;&#29983;&#25104;Ansible&#33050;&#26412;&#65292;&#24182;&#19988;&#20854;&#24615;&#33021;&#21487;&#19982;&#29616;&#26377;&#25216;&#26415;&#30340;&#29366;&#24577;&#30456;&#23218;&#32654;&#25110;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent improvement in code generation capabilities due to the use of large language models has mainly benefited general purpose programming languages. Domain specific languages, such as the ones used for IT Automation, have received far less attention, despite involving many active developers and being an essential component of modern cloud platforms. This work focuses on the generation of Ansible-YAML, a widely used markup language for IT Automation. We present Ansible Wisdom, a natural-language to Ansible-YAML code generation tool, aimed at improving IT automation productivity. Ansible Wisdom is a transformer-based model, extended by training with a new dataset containing Ansible-YAML. We also develop two novel performance metrics for YAML and Ansible to capture the specific characteristics of this domain. Results show that Ansible Wisdom can accurately generate Ansible script from natural language prompts with performance comparable or better than existing state of the art code 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#23398;&#20064;&#26041;&#27861;&#65292;&#21363;&#32479;&#19968;&#27169;&#22411;&#23398;&#20064;&#65292;&#21487;&#20197;&#21516;&#26102;&#36866;&#29992;&#20110;&#32763;&#35793;&#21508;&#31181;&#20219;&#21153;&#25968;&#25454;&#65292;&#24182;&#23454;&#29616;&#26234;&#33021;&#25353;&#38656;&#32763;&#35793;&#65292;&#30456;&#23545;&#29616;&#26377;&#30340;&#29305;&#23450;&#25968;&#25454;&#38598;&#27169;&#22411;&#33021;&#22815;&#24471;&#21040;&#26126;&#26174;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2305.02777</link><description>&lt;p&gt;
&#21508;&#31181;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#32479;&#19968;&#27169;&#22411;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Unified Model Learning for Various Neural Machine Translation. (arXiv:2305.02777v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02777
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#23398;&#20064;&#26041;&#27861;&#65292;&#21363;&#32479;&#19968;&#27169;&#22411;&#23398;&#20064;&#65292;&#21487;&#20197;&#21516;&#26102;&#36866;&#29992;&#20110;&#32763;&#35793;&#21508;&#31181;&#20219;&#21153;&#25968;&#25454;&#65292;&#24182;&#23454;&#29616;&#26234;&#33021;&#25353;&#38656;&#32763;&#35793;&#65292;&#30456;&#23545;&#29616;&#26377;&#30340;&#29305;&#23450;&#25968;&#25454;&#38598;&#27169;&#22411;&#33021;&#22815;&#24471;&#21040;&#26126;&#26174;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;(NMT)&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#26681;&#25454;&#26469;&#33258;&#19981;&#21516;&#20219;&#21153;(&#20363;&#22914;&#65292;&#25991;&#26723;&#32763;&#35793;&#21644;&#32842;&#22825;&#32763;&#35793;)&#30340;&#25968;&#25454;&#24320;&#21457;&#29305;&#23450;&#20110;&#25968;&#25454;&#38598;&#30340;&#27169;&#22411;&#12290;&#34429;&#28982;&#29305;&#23450;&#20110;&#25968;&#25454;&#38598;&#30340;&#27169;&#22411;&#24050;&#32463;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#24615;&#33021;&#65292;&#20294;&#27599;&#20010;&#25968;&#25454;&#38598;&#38656;&#35201;&#35774;&#35745;&#12289;&#35757;&#32451;&#21644;&#23384;&#20648;&#19968;&#20010;&#27169;&#22411;&#65292;&#36825;&#24456;&#40635;&#28902;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#23558;&#36825;&#20123;&#32763;&#35793;&#20219;&#21153;&#32479;&#19968;&#21040;&#26356;&#26222;&#36941;&#30340;&#35774;&#32622;&#20013;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#8220;&#22810;&#25165;&#22810;&#33402;&#8221;&#30340;&#27169;&#22411;&#65292;&#21363;&#36866;&#29992;&#20110;&#19981;&#21516;&#20219;&#21153;&#25968;&#25454;&#30340;&#32479;&#19968;&#27169;&#22411;&#23398;&#20064;(NMT)&#65292;&#21487;&#20197;&#21516;&#26102;&#22312;&#22810;&#31181;&#29615;&#22659;&#19979;&#36827;&#34892;&#33391;&#22909;&#30340;&#32763;&#35793;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#21487;&#20197;&#23613;&#21487;&#33021;&#22810;&#22320;&#25193;&#23637;&#12290;&#36890;&#36807;&#32479;&#19968;&#23398;&#20064;&#65292;UMLNMT&#33021;&#22815;&#36328;&#22810;&#20010;&#20219;&#21153;&#36827;&#34892;&#32852;&#21512;&#35757;&#32451;&#65292;&#23454;&#29616;&#26234;&#33021;&#25353;&#38656;&#32763;&#35793;&#12290;&#22312;&#19971;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#32763;&#35793;&#20219;&#21153;&#65292;&#21253;&#25324;&#21477;&#23376;&#32763;&#35793;&#12289;&#25991;&#26723;&#32763;&#35793;&#21644;&#32842;&#22825;&#32763;&#35793;&#20013;&#65292;&#25105;&#20204;&#30340;UMLNMT&#30456;&#23545;&#20110;&#29305;&#23450;&#25968;&#25454;&#38598;&#27169;&#22411;&#34920;&#29616;&#20986;&#20102;&#26126;&#26174;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing neural machine translation (NMT) studies mainly focus on developing dataset-specific models based on data from different tasks (e.g., document translation and chat translation). Although the dataset-specific models have achieved impressive performance, it is cumbersome as each dataset demands a model to be designed, trained, and stored. In this work, we aim to unify these translation tasks into a more general setting. Specifically, we propose a ``versatile'' model, i.e., the Unified Model Learning for NMT (UMLNMT) that works with data from different tasks, and can translate well in multiple settings simultaneously, and theoretically it can be as many as possible. Through unified learning, UMLNMT is able to jointly train across multiple tasks, implementing intelligent on-demand translation. On seven widely-used translation tasks, including sentence translation, document translation, and chat translation, our UMLNMT results in substantial improvements over dataset-specific model
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#20840;&#38754;&#27010;&#36848;&#20102;&#19981;&#21516;&#31867;&#22411;&#23545;&#35805;&#20013;&#23545;&#35805;&#20195;&#29702;&#20027;&#21160;&#24615;&#30340;&#31361;&#20986;&#38382;&#39064;&#21644;&#20808;&#36827;&#35774;&#35745;&#65292;&#35752;&#35770;&#20102;&#31526;&#21512;&#23454;&#38469;&#24212;&#29992;&#38656;&#27714;&#20294;&#38656;&#35201;&#26410;&#26469;&#26356;&#22823;&#30740;&#31350;&#37325;&#28857;&#30340;&#25361;&#25112;&#65292;&#28608;&#21457;&#26356;&#22810;&#30340;&#20250;&#35805; AI &#36827;&#23637;&#21040;&#19979;&#19968;&#32423;&#21035;&#12290;</title><link>http://arxiv.org/abs/2305.02750</link><description>&lt;p&gt;
&#20027;&#21160;&#23545;&#35805;&#31995;&#32479;&#32508;&#36848;&#65306;&#38382;&#39064;&#12289;&#26041;&#27861;&#21644;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
A Survey on Proactive Dialogue Systems: Problems, Methods, and Prospects. (arXiv:2305.02750v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02750
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#20840;&#38754;&#27010;&#36848;&#20102;&#19981;&#21516;&#31867;&#22411;&#23545;&#35805;&#20013;&#23545;&#35805;&#20195;&#29702;&#20027;&#21160;&#24615;&#30340;&#31361;&#20986;&#38382;&#39064;&#21644;&#20808;&#36827;&#35774;&#35745;&#65292;&#35752;&#35770;&#20102;&#31526;&#21512;&#23454;&#38469;&#24212;&#29992;&#38656;&#27714;&#20294;&#38656;&#35201;&#26410;&#26469;&#26356;&#22823;&#30740;&#31350;&#37325;&#28857;&#30340;&#25361;&#25112;&#65292;&#28608;&#21457;&#26356;&#22810;&#30340;&#20250;&#35805; AI &#36827;&#23637;&#21040;&#19979;&#19968;&#32423;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#23545;&#35805;&#31995;&#32479;&#19982;&#24191;&#27867;&#30340;&#29616;&#23454;&#19990;&#30028;&#23545;&#35805;&#24212;&#29992;&#30456;&#20851;&#65292;&#20351;&#23545;&#35805;&#20195;&#29702;&#33021;&#22815;&#24341;&#23548;&#23545;&#35805;&#26041;&#21521;&#65292;&#20197;&#23454;&#29616;&#39044;&#23450;&#20041;&#30340;&#30446;&#26631;&#25110;&#28385;&#36275;&#31995;&#32479;&#26041;&#38754;&#30340;&#29305;&#23450;&#30446;&#26631;&#12290;&#23427;&#36890;&#36807;&#20808;&#36827;&#25216;&#26415;&#36171;&#33021;&#20197;&#36827;&#23637;&#21040;&#38656;&#35201;&#25112;&#30053;&#24615;&#21644;&#28608;&#21169;&#24615;&#20132;&#20114;&#30340;&#26356;&#22797;&#26434;&#20219;&#21153;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#27010;&#36848;&#20102;&#19981;&#21516;&#31867;&#22411;&#23545;&#35805;&#20013;&#23545;&#35805;&#20195;&#29702;&#20027;&#21160;&#24615;&#30340;&#31361;&#20986;&#38382;&#39064;&#21644;&#20808;&#36827;&#35774;&#35745;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#31526;&#21512;&#23454;&#38469;&#24212;&#29992;&#38656;&#27714;&#20294;&#38656;&#35201;&#26410;&#26469;&#26356;&#22823;&#30740;&#31350;&#37325;&#28857;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#31687;&#20027;&#21160;&#23545;&#35805;&#31995;&#32479;&#30340;&#31532;&#19968;&#31687;&#32508;&#36848;&#21487;&#20197;&#20026;&#31038;&#21306;&#25552;&#20379;&#24555;&#36895;&#35775;&#38382;&#21644;&#25972;&#20307;&#22270;&#29255;&#65292;&#28608;&#21457;&#26356;&#22810;&#30340;&#20250;&#35805; AI &#36827;&#23637;&#21040;&#19979;&#19968;&#32423;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Proactive dialogue systems, related to a wide range of real-world conversational applications, equip the conversational agent with the capability of leading the conversation direction towards achieving pre-defined targets or fulfilling certain goals from the system side. It is empowered by advanced techniques to progress to more complicated tasks that require strategical and motivational interactions. In this survey, we provide a comprehensive overview of the prominent problems and advanced designs for conversational agent's proactivity in different types of dialogues. Furthermore, we discuss challenges that meet the real-world application needs but require a greater research focus in the future. We hope that this first survey of proactive dialogue systems can provide the community with a quick access and an overall picture to this practical problem, and stimulate more progresses on conversational AI to the next level.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26681;&#25454;&#31038;&#20250;&#31185;&#23398;&#29702;&#35770;&#24314;&#31435;&#30340;&#35745;&#31639;&#26694;&#26550;&#65292;&#20026;&#30740;&#31350;&#20154;&#31867;&#20215;&#20540;&#35266;&#22914;&#20309;&#25903;&#25345;&#35774;&#35745;&#36947;&#24503;&#20154;&#24037;&#26234;&#33021;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2305.02748</link><description>&lt;p&gt;
&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#35745;&#31639;&#26694;&#26550;&#23545;&#20110;&#36947;&#24503;&#20154;&#24037;&#26234;&#33021;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
A computational framework of human values for ethical AI. (arXiv:2305.02748v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02748
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26681;&#25454;&#31038;&#20250;&#31185;&#23398;&#29702;&#35770;&#24314;&#31435;&#30340;&#35745;&#31639;&#26694;&#26550;&#65292;&#20026;&#30740;&#31350;&#20154;&#31867;&#20215;&#20540;&#35266;&#22914;&#20309;&#25903;&#25345;&#35774;&#35745;&#36947;&#24503;&#20154;&#24037;&#26234;&#33021;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#24515;&#29702;&#23398;&#12289;&#21746;&#23398;&#21644;&#31038;&#20250;&#31185;&#23398;&#30340;&#22810;&#20803;&#21270;&#25506;&#32034;&#20013;&#65292;&#21487;&#20197;&#24471;&#20986;&#19968;&#20010;&#26126;&#30830;&#30340;&#20849;&#35782;&#65292;&#37027;&#23601;&#26159;&#20215;&#20540;&#35266;&#25351;&#23548;&#30528;&#34892;&#20026;&#12290;&#26368;&#36817;&#65292;&#20154;&#20204;&#35748;&#35782;&#21040;&#65292;&#20215;&#20540;&#35266;&#20026;&#24037;&#31243;&#21270;&#36947;&#24503;&#20154;&#24037;&#26234;&#33021;&#25552;&#20379;&#20102;&#19968;&#31181;&#26041;&#24335;&#12290;&#25105;&#20204;&#22522;&#20110;&#31038;&#20250;&#31185;&#23398;&#30340;&#27491;&#24335;&#27010;&#24565;&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#23545;&#20110;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#35745;&#31639;&#24418;&#24335;&#23450;&#20041;&#65292;&#20026;&#31995;&#32479;&#12289;&#32508;&#21512;&#21644;&#36328;&#23398;&#31185;&#22320;&#30740;&#31350;&#20154;&#31867;&#20215;&#20540;&#35266;&#22914;&#20309;&#25903;&#25345;&#35774;&#35745;&#36947;&#24503;&#20154;&#24037;&#26234;&#33021;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the diverse array of work investigating the nature of human values from psychology, philosophy and social sciences, there is a clear consensus that values guide behaviour. More recently, a recognition that values provide a means to engineer ethical AI has emerged. Indeed, Stuart Russell proposed shifting AI's focus away from simply ``intelligence'' towards intelligence ``provably aligned with human values''. This challenge -- the value alignment problem -- with others including an AI's learning of human values, aggregating individual values to groups, and designing computational mechanisms to reason over values, has energised a sustained research effort. Despite this, no formal, computational definition of values has yet been proposed. We address this through a formal conceptual framework rooted in the social sciences, that provides a foundation for the systematic, integrated and interdisciplinary investigation into how human values can support designing ethical AI.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#23545;&#35805;&#25968;&#25454;&#36827;&#34892;&#26080;&#30417;&#30563;&#23545;&#35805;&#20027;&#39064;&#20998;&#21106;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37051;&#36817;&#35805;&#35821;&#21305;&#37197;&#21644;&#20266;&#20998;&#21106;&#23398;&#20064;&#20027;&#39064;&#24863;&#30693;&#30340;&#35805;&#35821;&#34920;&#31034;&#65292;&#23454;&#39564;&#26174;&#31034;&#20854;&#26126;&#26174;&#20248;&#20110;&#24378;&#22522;&#20934;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.02747</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#23545;&#35805;&#20027;&#39064;&#20998;&#21106;&#21450;&#35805;&#35821;&#34920;&#31034;&#20013;&#20027;&#39064;&#24863;&#30693;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Dialogue Topic Segmentation with Topic-aware Utterance Representation. (arXiv:2305.02747v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02747
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#23545;&#35805;&#25968;&#25454;&#36827;&#34892;&#26080;&#30417;&#30563;&#23545;&#35805;&#20027;&#39064;&#20998;&#21106;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37051;&#36817;&#35805;&#35821;&#21305;&#37197;&#21644;&#20266;&#20998;&#21106;&#23398;&#20064;&#20027;&#39064;&#24863;&#30693;&#30340;&#35805;&#35821;&#34920;&#31034;&#65292;&#23454;&#39564;&#26174;&#31034;&#20854;&#26126;&#26174;&#20248;&#20110;&#24378;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#20027;&#39064;&#20998;&#21106;&#22312;&#21508;&#31181;&#23545;&#35805;&#24314;&#27169;&#20219;&#21153;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#29616;&#26377;&#30340;DTS&#26041;&#27861;&#35201;&#20040;&#20851;&#27880;&#35821;&#20041;&#30456;&#20284;&#24615;&#65292;&#35201;&#20040;&#20851;&#27880;&#23545;&#35805;&#36830;&#36143;&#24615;&#26469;&#35780;&#20272;&#20027;&#39064;&#30456;&#20284;&#24615;&#20197;&#36827;&#34892;&#26080;&#30417;&#30563;&#23545;&#35805;&#20998;&#21106;&#12290;&#20294;&#20027;&#39064;&#30456;&#20284;&#24615;&#19981;&#33021;&#36890;&#36807;&#35821;&#20041;&#30456;&#20284;&#24615;&#25110;&#23545;&#35805;&#36830;&#36143;&#24615;&#30340;&#26041;&#24335;&#26469;&#23436;&#20840;&#35782;&#21035;&#12290;&#27492;&#22806;&#65292;&#26410;&#26631;&#35760;&#30340;&#23545;&#35805;&#25968;&#25454;&#20013;&#21253;&#21547;&#26377;&#29992;&#30340;&#35805;&#35821;&#20851;&#31995;&#25552;&#31034;&#65292;&#20294;&#36825;&#20123;&#25552;&#31034;&#26410;&#34987;&#20805;&#20998;&#21033;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;DTS&#26694;&#26550;&#65292;&#36890;&#36807;&#37051;&#36817;&#35805;&#35821;&#21305;&#37197;&#21644;&#20266;&#20998;&#21106;&#20174;&#26410;&#26631;&#35760;&#30340;&#23545;&#35805;&#25968;&#25454;&#20013;&#23398;&#20064;&#20027;&#39064;&#24863;&#30693;&#30340;&#35805;&#35821;&#34920;&#31034;&#12290;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65288;DialSeg711&#21644;Doc2Dial&#65289;&#19978;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#20248;&#20110;&#24378;&#22522;&#20934;&#26041;&#27861;&#12290;&#20026;&#20102;&#21487;&#22797;&#29616;&#24615;&#65292;&#25105;&#20204;&#22312;https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/dial-start&#19978;&#25552;&#20379;&#20102;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dialogue Topic Segmentation (DTS) plays an essential role in a variety of dialogue modeling tasks. Previous DTS methods either focus on semantic similarity or dialogue coherence to assess topic similarity for unsupervised dialogue segmentation. However, the topic similarity cannot be fully identified via semantic similarity or dialogue coherence. In addition, the unlabeled dialogue data, which contains useful clues of utterance relationships, remains underexploited. In this paper, we propose a novel unsupervised DTS framework, which learns topic-aware utterance representations from unlabeled dialogue data through neighboring utterance matching and pseudo-segmentation. Extensive experiments on two benchmark datasets (i.e., DialSeg711 and Doc2Dial) demonstrate that our method significantly outperforms the strong baseline methods. For reproducibility, we provide our code and data at:https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/dial-start.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31038;&#20250;&#31185;&#23398;&#30340;&#24418;&#24335;&#21270;&#20215;&#20540;&#35266;&#34920;&#24449;&#65292;&#24182;&#38416;&#26126;&#20102;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#23454;&#29616;&#19982;&#20215;&#20540;&#35266;&#19968;&#33268;&#30340;&#34892;&#20026;&#30340;&#20851;&#38190;&#25361;&#25112;&#21644;&#30456;&#24212;&#30340;&#30740;&#31350;&#36335;&#32447;&#22270;&#12290;</title><link>http://arxiv.org/abs/2305.02739</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#30340;&#20154;&#31867;&#20215;&#20540;&#35266;
&lt;/p&gt;
&lt;p&gt;
Human Values in Multiagent Systems. (arXiv:2305.02739v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02739
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31038;&#20250;&#31185;&#23398;&#30340;&#24418;&#24335;&#21270;&#20215;&#20540;&#35266;&#34920;&#24449;&#65292;&#24182;&#38416;&#26126;&#20102;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#23454;&#29616;&#19982;&#20215;&#20540;&#35266;&#19968;&#33268;&#30340;&#34892;&#20026;&#30340;&#20851;&#38190;&#25361;&#25112;&#21644;&#30456;&#24212;&#30340;&#30740;&#31350;&#36335;&#32447;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20170;&#20262;&#29702;&#20154;&#24037;&#26234;&#33021;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#26159;&#24320;&#21457;&#35745;&#31639;&#31995;&#32479;&#65292;&#20854;&#25512;&#29702;&#21644;&#34892;&#20026;&#21487;&#35777;&#26126;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#19968;&#33268;&#12290;&#28982;&#32780;&#65292;&#8220;&#20154;&#31867;&#20215;&#20540;&#35266;&#8221;&#20197;&#20854;&#27495;&#20041;&#12289;&#30683;&#30462;&#21644;&#22810;&#21464;&#32780;&#33879;&#31216;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#31181;&#24046;&#36317;&#65292;&#25289;&#36817;&#25105;&#20204;&#21487;&#20197;&#27491;&#24335;&#25512;&#26029;&#23558;&#20215;&#20540;&#35266;&#23454;&#26045;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#24773;&#20917;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31038;&#20250;&#31185;&#23398;&#30340;&#20215;&#20540;&#35266;&#30340;&#27491;&#24335;&#34920;&#24449;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#31181;&#27491;&#24335;&#21576;&#29616;&#26041;&#24335;&#26469;&#38416;&#26126;&#22312;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#23454;&#29616;&#19982;&#20215;&#20540;&#35266;&#19968;&#33268;&#30340;&#34892;&#20026;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#20197;&#21450;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#30340;&#30740;&#31350;&#36335;&#32447;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the major challenges we face with ethical AI today is developing computational systems whose reasoning and behaviour are provably aligned with human values. Human values, however, are notorious for being ambiguous, contradictory and ever-changing. In order to bridge this gap, and get us closer to the situation where we can formally reason about implementing values into AI, this paper presents a formal representation of values, grounded in the social sciences. We use this formal representation to articulate the key challenges for achieving value-aligned behaviour in multiagent systems (MAS) and a research roadmap for addressing them.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#32852;&#37030;&#23398;&#20064;&#30340;&#20844;&#27491;&#24615;&#19982;&#20010;&#24615;&#21270;&#38656;&#27714;&#30340;&#20851;&#31995;&#65292;&#25552;&#20986;&#20010;&#24615;&#21270;&#20844;&#27491;&#32852;&#37030;&#23398;&#20064;&#65288;PFL&#65289;&#31639;&#27861;&#65292;&#33021;&#22815;&#35299;&#20915;&#20934;&#30830;&#24615;&#24046;&#24322;&#20197;&#21450;&#22312;FL&#27169;&#22411;&#34920;&#29616;&#19981;&#20339;&#26102;&#25552;&#20379;&#21442;&#19982;&#28608;&#21169;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.02728</link><description>&lt;p&gt;
&#20844;&#27491;&#32852;&#37030;&#23398;&#20064;&#21487;&#20197;&#20943;&#23569;&#20010;&#24615;&#21270;&#38656;&#27714;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Fair Federated Learning reduce the need for Personalisation?. (arXiv:2305.02728v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02728
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#32852;&#37030;&#23398;&#20064;&#30340;&#20844;&#27491;&#24615;&#19982;&#20010;&#24615;&#21270;&#38656;&#27714;&#30340;&#20851;&#31995;&#65292;&#25552;&#20986;&#20010;&#24615;&#21270;&#20844;&#27491;&#32852;&#37030;&#23398;&#20064;&#65288;PFL&#65289;&#31639;&#27861;&#65292;&#33021;&#22815;&#35299;&#20915;&#20934;&#30830;&#24615;&#24046;&#24322;&#20197;&#21450;&#22312;FL&#27169;&#22411;&#34920;&#29616;&#19981;&#20339;&#26102;&#25552;&#20379;&#21442;&#19982;&#28608;&#21169;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20351;&#24471;&#22312;&#19981;&#20849;&#20139;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#22312;&#36793;&#32536;&#23458;&#25143;&#31471;&#19978;&#35757;&#32451;ML&#27169;&#22411;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#32852;&#37030;&#27169;&#22411;&#22312;&#26412;&#22320;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#20250;&#26377;&#25152;&#24046;&#24322;&#65292;&#36825;&#20250;&#36896;&#25104;&#23545;&#20110;&#27809;&#26377;&#20174;FL&#20013;&#21463;&#30410;&#30340;&#23458;&#25143;&#31471;&#21442;&#19982;&#30340;&#19981;&#21033;&#24433;&#21709;&#12290;&#20844;&#27491;FL&#36890;&#36807;&#20851;&#27880;&#25439;&#22833;&#26356;&#39640;&#30340;&#23458;&#25143;&#31471;&#26469;&#20943;&#23569;&#20934;&#30830;&#24615;&#24046;&#24322;&#65292;&#32780;&#20010;&#24615;&#21270;&#35843;&#25972;&#21017;&#22312;&#26412;&#22320;&#24494;&#35843;&#27169;&#22411;&#12290;&#22312;FL&#27169;&#22411;&#30456;&#23545;&#20110;&#26412;&#22320;&#35757;&#32451;&#27169;&#22411;&#34920;&#29616;&#36739;&#24046;&#26102;&#65292;&#20010;&#24615;&#21270;&#25552;&#20379;&#20102;&#21442;&#19982;&#28608;&#21169;&#12290;&#26412;&#25991;&#35780;&#20272;&#20102;&#20004;&#31181;&#20844;&#27491;FL&#31639;&#27861;&#20316;&#20026;&#20010;&#24615;&#21270;&#30340;&#36215;&#28857;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#20844;&#27491;FL&#30340;&#26041;&#27861;&#23545;&#20110;&#35821;&#35328;&#20219;&#21153;&#24182;&#27809;&#26377;&#20135;&#29983;&#30456;&#23545;&#24615;&#33021;&#20248;&#21183;&#65292;&#24182;&#19988;&#22312;&#22270;&#20687;&#20219;&#21153;&#20013;&#21487;&#33021;&#20250;&#20351;&#34920;&#29616;&#19981;&#20339;&#30340;&#23458;&#25143;&#31471;&#25968;&#37327;&#22686;&#21152;&#19968;&#20493;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20010;&#24615;&#21270;&#20844;&#27491;&#32852;&#37030;&#23398;&#20064;&#65288;PFL&#65289;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#35821;&#35328;&#21644;&#22270;&#20687;&#20219;&#21153;&#20013;&#20943;&#23569;&#20934;&#30830;&#24615;&#24046;&#24322;&#30340;&#21516;&#26102;&#65292;&#22312;FL&#27169;&#22411;&#30456;&#23545;&#20110;&#26412;&#22320;&#27169;&#22411;&#34920;&#29616;&#36739;&#24046;&#26102;&#25552;&#20379;&#20102;&#21442;&#19982;&#28608;&#21169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) enables training ML models on edge clients without sharing data. However, the federated model's performance on local data varies, disincentivising the participation of clients who benefit little from FL. Fair FL reduces accuracy disparity by focusing on clients with higher losses while personalisation locally fine-tunes the model. Personalisation provides a participation incentive when an FL model underperforms relative to one trained locally. For situations where the federated model provides a lower accuracy than a model trained entirely locally by a client, personalisation improves the accuracy of the pre-trained federated weights to be similar to or exceed those of the local client model. This paper evaluates two Fair FL (FFL) algorithms as starting points for personalisation. Our results show that FFL provides no benefit to relative performance in a language task and may double the number of underperforming clients for an image task. Instead, we propose Pers
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24322;&#27493;&#26356;&#26032;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550; (AURL) &#26469;&#35757;&#32451;&#38754;&#21521;&#20219;&#21153;&#22411;&#23545;&#35805;&#31995;&#32479;&#65292;&#35299;&#20915;&#20102;&#19981;&#21516;&#27169;&#22359;&#20114;&#30456;&#24433;&#21709;&#30340;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#37319;&#29992;&#35838;&#31243;&#23398;&#20064;&#26469;&#35299;&#20915;&#25968;&#25454;&#20998;&#24067;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#22810;&#20010;&#29992;&#25143;&#27169;&#22411;&#26469;&#22686;&#21152;&#23545;&#35805;&#30340;&#22810;&#26679;&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;31.37%&#30340;&#23545;&#35805;&#25104;&#21151;&#29575;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2305.02718</link><description>&lt;p&gt;
&#38754;&#21521;&#20219;&#21153;&#22411;&#23545;&#35805;&#31995;&#32479;&#30340;&#24322;&#27493;&#26356;&#26032;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
An Asynchronous Updating Reinforcement Learning Framework for Task-oriented Dialog System. (arXiv:2305.02718v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02718
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24322;&#27493;&#26356;&#26032;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550; (AURL) &#26469;&#35757;&#32451;&#38754;&#21521;&#20219;&#21153;&#22411;&#23545;&#35805;&#31995;&#32479;&#65292;&#35299;&#20915;&#20102;&#19981;&#21516;&#27169;&#22359;&#20114;&#30456;&#24433;&#21709;&#30340;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#37319;&#29992;&#35838;&#31243;&#23398;&#20064;&#26469;&#35299;&#20915;&#25968;&#25454;&#20998;&#24067;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#22810;&#20010;&#29992;&#25143;&#27169;&#22411;&#26469;&#22686;&#21152;&#23545;&#35805;&#30340;&#22810;&#26679;&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;31.37%&#30340;&#23545;&#35805;&#25104;&#21151;&#29575;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#23545;&#35805;&#31995;&#32479;&#20013;&#24050;&#32463;&#24212;&#29992;&#20102;&#24378;&#21270;&#23398;&#20064;&#26469;&#36827;&#34892;&#35757;&#32451;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#23558;&#23545;&#35805;&#31995;&#32479;&#20998;&#25104;&#22810;&#20010;&#27169;&#22359;&#65292;&#21253;&#25324;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394; (DST) &#21644;&#23545;&#35805;&#31574;&#30053; (DP)&#65292;&#24182;&#21516;&#26102;&#35757;&#32451;&#36825;&#20123;&#27169;&#22359;&#12290;&#28982;&#32780;&#65292;&#19981;&#21516;&#27169;&#22359;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20250;&#30456;&#20114;&#24433;&#21709;&#12290;&#26469;&#33258; DST &#30340;&#35823;&#24046;&#21487;&#33021;&#20250;&#35823;&#23548;&#23545;&#35805;&#31574;&#30053;&#65292;&#31995;&#32479;&#25805;&#20316;&#20063;&#20250;&#32473; DST &#27169;&#22359;&#24102;&#26469;&#39069;&#22806;&#30340;&#22256;&#38590;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24322;&#27493;&#26356;&#26032;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550; (AURL)&#65292;&#22312;&#21512;&#20316;&#35774;&#32622;&#19979;&#24322;&#27493;&#22320;&#26356;&#26032; DST &#27169;&#22359;&#21644; DP &#27169;&#22359;&#12290;&#27492;&#22806;&#65292;&#37319;&#29992;&#35838;&#31243;&#23398;&#20064;&#26469;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#37319;&#26679;&#36807;&#31243;&#20013;&#30340;&#25968;&#25454;&#20998;&#24067;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#22810;&#20010;&#29992;&#25143;&#27169;&#22411;&#26469;&#22686;&#21152;&#23545;&#35805;&#30340;&#22810;&#26679;&#24615;&#12290;&#20844;&#20849; SSD-PHONE &#25968;&#25454;&#38598;&#19978;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23545;&#35805;&#25104;&#21151;&#29575;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;31.37%&#25913;&#36827;&#12290;&#20195;&#30721;&#21487;&#22312; https://github.com/thu-coai/AURL-Dialog-System &#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning has been applied to train the dialog systems in many works. Previous approaches divide the dialog system into multiple modules including DST (dialog state tracking) and DP (dialog policy), and train these modules simultaneously. However, different modules influence each other during training. The errors from DST might misguide the dialog policy, and the system action brings extra difficulties for the DST module. To alleviate this problem, we propose Asynchronous Updating Reinforcement Learning framework (AURL) that updates the DST module and the DP module asynchronously under a cooperative setting. Furthermore, curriculum learning is implemented to address the problem of unbalanced data distribution during reinforcement learning sampling, and multiple user models are introduced to increase the dialog diversity. Results on the public SSD-PHONE dataset show that our method achieves a compelling result with a 31.37% improvement on the dialog success rate. The code i
&lt;/p&gt;</description></item><item><title>DECICE&#26159;&#19968;&#20010;&#36328;&#35774;&#22791;-&#36793;&#32536;&#21644;&#20113;&#30340;&#26234;&#33021;&#21327;&#20316;&#26694;&#26550;&#65292;&#20855;&#26377;&#33258;&#36866;&#24212;&#20248;&#21270;&#21644;&#37096;&#32626;&#24212;&#29992;&#30340;&#33021;&#21147;&#65292;&#33021;&#22815;&#24212;&#29992;&#20110;&#26234;&#33021;&#20132;&#36890;&#12289;&#21307;&#23398;&#25104;&#20687;&#21644;&#32039;&#24613;&#21709;&#24212;&#31561;&#26041;&#38754;&#12290;</title><link>http://arxiv.org/abs/2305.02697</link><description>&lt;p&gt;
DECICE: &#35774;&#22791;-&#36793;&#32536;-&#20113;&#26234;&#33021;&#21327;&#20316;&#26694;&#26550;(arXiv:2305.02697v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
DECICE: Device-Edge-Cloud Intelligent Collaboration Framework. (arXiv:2305.02697v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02697
&lt;/p&gt;
&lt;p&gt;
DECICE&#26159;&#19968;&#20010;&#36328;&#35774;&#22791;-&#36793;&#32536;&#21644;&#20113;&#30340;&#26234;&#33021;&#21327;&#20316;&#26694;&#26550;&#65292;&#20855;&#26377;&#33258;&#36866;&#24212;&#20248;&#21270;&#21644;&#37096;&#32626;&#24212;&#29992;&#30340;&#33021;&#21147;&#65292;&#33021;&#22815;&#24212;&#29992;&#20110;&#26234;&#33021;&#20132;&#36890;&#12289;&#21307;&#23398;&#25104;&#20687;&#21644;&#32039;&#24613;&#21709;&#24212;&#31561;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
DECICE&#26159;&#19968;&#20010;Horizon Europe&#39033;&#30446;&#65292;&#27491;&#22312;&#24320;&#21457;&#19968;&#31181;AI-enabled&#30340;&#24320;&#25918;&#24335;&#21644;&#21487;&#31227;&#26893;&#24335;&#31649;&#29702;&#26694;&#26550;&#65292;&#29992;&#20110;&#35745;&#31639;&#36830;&#32493;&#20307;&#20013;&#24212;&#29992;&#31243;&#24207;&#30340;&#33258;&#21160;&#21644;&#33258;&#36866;&#24212;&#20248;&#21270;&#21644;&#37096;&#32626;&#65292;&#35813;&#36830;&#32493;&#20307;&#28085;&#30422;&#20102;&#20174;&#36793;&#32536;IoT&#20256;&#24863;&#22120;&#21040;&#22823;&#35268;&#27169;&#20113;/HPC&#35745;&#31639;&#22522;&#30784;&#35774;&#26045;&#30340;&#33539;&#22260;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;DECICE&#26694;&#26550;&#21644;&#26550;&#26500;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#31361;&#20986;&#20102;&#29992;&#20110;&#26694;&#26550;&#35780;&#20272;&#30340;&#29992;&#20363;&#65306;&#26234;&#33021;&#20132;&#36890;&#36335;&#21475;&#65292;&#30913;&#20849;&#25391;&#25104;&#20687;&#21644;&#32039;&#24613;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
DECICE is a Horizon Europe project that is developing an AI-enabled open and portable management framework for automatic and adaptive optimization and deployment of applications in computing continuum encompassing from IoT sensors on the Edge to large-scale Cloud / HPC computing infrastructures. In this paper, we describe the DECICE framework and architecture. Furthermore, we highlight use-cases for framework evaluation: intelligent traffic intersection, magnetic resonance imaging, and emergency response.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#35821;&#35328;&#29305;&#24322;Transformer&#23618;&#65288;LSLs&#65289;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#22686;&#21152;&#27169;&#22411;&#23481;&#37327;&#65292;&#21516;&#26102;&#20445;&#25345;&#27491;&#21521;&#20256;&#36882;&#20013;&#20351;&#29992;&#30340;&#35745;&#31639;&#37327;&#21644;&#21442;&#25968;&#25968;&#37327;&#19981;&#21464;&#65292;&#20174;&#32780;&#25552;&#39640;&#22810;&#35821;&#26426;&#22120;&#32763;&#35793;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.02665</link><description>&lt;p&gt;
&#23398;&#20064;&#22810;&#35821;&#26426;&#22120;&#32763;&#35793;&#30340;&#35821;&#35328;&#29305;&#24322;&#23618;
&lt;/p&gt;
&lt;p&gt;
Learning Language-Specific Layers for Multilingual Machine Translation. (arXiv:2305.02665v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02665
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#35821;&#35328;&#29305;&#24322;Transformer&#23618;&#65288;LSLs&#65289;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#22686;&#21152;&#27169;&#22411;&#23481;&#37327;&#65292;&#21516;&#26102;&#20445;&#25345;&#27491;&#21521;&#20256;&#36882;&#20013;&#20351;&#29992;&#30340;&#35745;&#31639;&#37327;&#21644;&#21442;&#25968;&#25968;&#37327;&#19981;&#21464;&#65292;&#20174;&#32780;&#25552;&#39640;&#22810;&#35821;&#26426;&#22120;&#32763;&#35793;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#26426;&#22120;&#32763;&#35793;&#21487;&#20197;&#25552;&#39640;&#38750;&#33521;&#35821;&#35821;&#35328;&#20043;&#38388;&#30340;&#32763;&#35793;&#36136;&#37327;&#65292;&#22312;&#35768;&#22810;&#26041;&#38754;&#37117;&#26377;&#20248;&#21183;&#65292;&#20363;&#22914;&#26356;&#20302;&#30340;&#24310;&#36831;&#65288;&#26080;&#38656;&#32763;&#35793;&#20004;&#27425;&#65289;&#21644;&#20943;&#23569;&#38169;&#35823;&#32423;&#32852;&#65288;&#20363;&#22914;&#65292;&#22312;&#36890;&#36807;&#33521;&#35821;&#36827;&#34892;&#32763;&#35793;&#26102;&#36991;&#20813;&#20002;&#22833;&#24615;&#21035;&#21644;&#31036;&#35980;&#31561;&#20449;&#24687;&#65289;&#12290;&#20294;&#26159;&#65292;&#28155;&#21152;&#26356;&#22810;&#35821;&#35328;&#20250;&#20943;&#23569;&#27599;&#31181;&#35821;&#35328;&#30340;&#27169;&#22411;&#23481;&#37327;&#65292;&#36890;&#24120;&#36890;&#36807;&#22686;&#21152;&#24635;&#20307;&#27169;&#22411;&#22823;&#23567;&#26469;&#25269;&#28040;&#65292;&#20174;&#32780;&#20351;&#35757;&#32451;&#26356;&#21152;&#22256;&#38590;&#65292;&#25512;&#29702;&#36895;&#24230;&#21464;&#24930;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#35821;&#35328;&#29305;&#24322;Transformer&#23618;&#65288;LSLs&#65289;&#65292;&#23427;&#20204;&#20351;&#25105;&#20204;&#33021;&#22815;&#22686;&#21152;&#27169;&#22411;&#23481;&#37327;&#65292;&#21516;&#26102;&#20445;&#25345;&#27491;&#21521;&#20256;&#36882;&#20013;&#20351;&#29992;&#30340;&#35745;&#31639;&#37327;&#21644;&#21442;&#25968;&#25968;&#37327;&#19981;&#21464;&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#35753;&#32534;&#30721;&#22120;&#30340;&#26576;&#20123;&#23618;&#20026;&#28304;&#35821;&#35328;&#25110;&#30446;&#26631;&#35821;&#35328;&#29305;&#24322;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#20313;&#23618;&#20849;&#20139;&#12290;&#25105;&#20204;&#20351;&#29992;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#21551;&#21457;&#24335;&#26041;&#27861;&#30740;&#31350;&#20102;&#25918;&#32622;&#36825;&#20123;&#23618;&#30340;&#26368;&#20339;&#26041;&#27861;&#65292;&#24182;&#23454;&#29616;&#20102;1.3 chrF&#65288;1.5 spBLE&#65289;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multilingual Machine Translation promises to improve translation quality between non-English languages. This is advantageous for several reasons, namely lower latency (no need to translate twice), and reduced error cascades (e.g., avoiding losing gender and formality information when translating through English). On the downside, adding more languages reduces model capacity per language, which is usually countered by increasing the overall model size, making training harder and inference slower. In this work, we introduce Language-Specific Transformer Layers (LSLs), which allow us to increase model capacity, while keeping the amount of computation and the number of parameters used in the forward pass constant. The key idea is to have some layers of the encoder be source or target language-specific, while keeping the remaining layers shared. We study the best way to place these layers using a neural architecture search inspired approach, and achieve an improvement of 1.3 chrF (1.5 spBLE
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22240;&#26524;&#24378;&#24230;&#21464;&#20998;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#20174;&#19981;&#30830;&#23450;&#25968;&#25454;&#20013;&#24674;&#22797;&#22240;&#26524;&#20851;&#31995;&#23384;&#22312;&#30340;&#20302;&#26679;&#26412;&#21033;&#29992;&#29575;&#21644;&#20998;&#24067;&#20551;&#35774;&#26080;&#33021;&#21147;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#28508;&#22312;&#28151;&#28102;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2305.02640</link><description>&lt;p&gt;
&#23398;&#20064;&#22312;&#23384;&#22312;&#38544;&#24615;&#28151;&#28102;&#22240;&#32032;&#30340;&#24773;&#20917;&#19979;&#20174;&#19981;&#30830;&#23450;&#25968;&#25454;&#20013;&#24674;&#22797;&#22240;&#26524;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Learning to Recover Causal Relationship from Indefinite Data in the Presence of Latent Confounders. (arXiv:2305.02640v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02640
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22240;&#26524;&#24378;&#24230;&#21464;&#20998;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#20174;&#19981;&#30830;&#23450;&#25968;&#25454;&#20013;&#24674;&#22797;&#22240;&#26524;&#20851;&#31995;&#23384;&#22312;&#30340;&#20302;&#26679;&#26412;&#21033;&#29992;&#29575;&#21644;&#20998;&#24067;&#20551;&#35774;&#26080;&#33021;&#21147;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#28508;&#22312;&#28151;&#28102;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20855;&#26377;&#28508;&#22312;&#21464;&#37327;&#30340;&#22240;&#26524;&#21457;&#29616;&#20013;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#20004;&#20010;&#25968;&#25454;&#33539;&#24335;&#65306;&#30830;&#23450;&#25968;&#25454;&#65306;&#20855;&#26377;&#35266;&#23519;&#33410;&#28857;&#21333;&#20540;&#30340;&#21333;&#20010;&#39592;&#26550;&#32467;&#26500;&#65292;&#21644;&#19981;&#30830;&#23450;&#25968;&#25454;&#65306;&#20855;&#26377;&#35266;&#23519;&#33410;&#28857;&#22810;&#20540;&#30340;&#19968;&#32452;&#22810;&#39592;&#26550;&#32467;&#26500;&#12290;&#22810;&#20010;&#39592;&#26550;&#24341;&#20837;&#20302;&#26679;&#26412;&#21033;&#29992;&#29575;&#65292;&#22810;&#20010;&#20540;&#24341;&#20837;&#20102;&#20998;&#24067;&#20551;&#35774;&#30340;&#26080;&#33021;&#21147;&#65292;&#36825;&#20004;&#32773;&#23548;&#33268;&#20174;&#19981;&#30830;&#23450;&#25968;&#25454;&#20013;&#24674;&#22797;&#22240;&#26524;&#20851;&#31995;&#33267;&#20170;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#22240;&#26524;&#24378;&#24230;&#21464;&#20998;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#21033;&#29992;&#22240;&#26524;&#24378;&#24230;&#32780;&#19981;&#26159;&#29420;&#31435;&#22122;&#22768;&#20316;&#20026;&#28508;&#21464;&#37327;&#26469;&#35843;&#33410;&#35777;&#25454;&#19979;&#30028;&#12290;&#36890;&#36807;&#36825;&#31181;&#35774;&#35745;&#24605;&#24819;&#65292;&#19981;&#21516;&#39592;&#26550;&#30340;&#22240;&#26524;&#24378;&#24230;&#34987;&#30475;&#20316;&#26159;&#19968;&#20010;&#20998;&#24067;&#65292;&#24182;&#21487;&#20197;&#34920;&#31034;&#20026;&#21333;&#20540;&#22240;&#26524;&#22270;&#30697;&#38453;&#12290;&#27492;&#22806;&#65292;&#32771;&#34385;&#21040;&#28508;&#22312;&#28151;&#28102;&#22240;&#32032;&#65292;&#25105;&#20204;&#23558;&#22240;&#26524;&#22270;G&#20998;&#35299;&#20026;&#20004;&#20010;&#30456;&#20851;&#23376;&#22270;O&#21644;C&#12290;O&#21253;&#21547;&#35266;&#23519;&#33410;&#28857;&#20043;&#38388;&#30340;&#32431;&#20851;&#31995;&#65292;&#32780;C&#34920;&#31034;&#28151;&#28102;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Causal Discovery with latent variables, We define two data paradigms: definite data: a single-skeleton structure with observed nodes single-value, and indefinite data: a set of multi-skeleton structures with observed nodes multi-value. Multi,skeletons induce low sample utilization and multi values induce incapability of the distribution assumption, both leading that recovering causal relations from indefinite data is, as of yet, largely unexplored. We design the causal strength variational model to settle down these two problems. Specifically, we leverage the causal strength instead of independent noise as latent variable to mediate evidence lower bound. By this design ethos, The causal strength of different skeletons is regarded as a distribution and can be expressed as a single-valued causal graph matrix. Moreover, considering the latent confounders, we disentangle the causal graph G into two relatisubgraphs O and C. O contains pure relations between observed nodes, while C repres
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#26497;&#24230;&#24369;&#30340;&#30417;&#30563;&#26041;&#27861;&#65292;&#21482;&#20381;&#36182;&#20110;&#31867;&#21035;&#21517;&#31216;&#32780;&#19981;&#26159;&#27880;&#37322;&#25968;&#25454;&#20013;&#30340;&#31867;&#21035;&#31034;&#20363;&#65292;&#35299;&#20915;&#24403;&#21069;&#20167;&#24680;&#35328;&#35770;&#35782;&#21035;&#30340;&#30740;&#31350;&#23384;&#22312;&#30340;&#25968;&#25454;&#21019;&#24314;&#31574;&#30053;&#19981;&#31995;&#32479;&#21644;&#19981;&#21516;&#27880;&#37322;&#26041;&#26696;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.02637</link><description>&lt;p&gt;
&#38754;&#21521;&#36328;&#25968;&#25454;&#38598;&#30340;&#24369;&#30417;&#30563;&#20167;&#24680;&#35328;&#35770;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Towards Weakly-Supervised Hate Speech Classification Across Datasets. (arXiv:2305.02637v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02637
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#26497;&#24230;&#24369;&#30340;&#30417;&#30563;&#26041;&#27861;&#65292;&#21482;&#20381;&#36182;&#20110;&#31867;&#21035;&#21517;&#31216;&#32780;&#19981;&#26159;&#27880;&#37322;&#25968;&#25454;&#20013;&#30340;&#31867;&#21035;&#31034;&#20363;&#65292;&#35299;&#20915;&#24403;&#21069;&#20167;&#24680;&#35328;&#35770;&#35782;&#21035;&#30340;&#30740;&#31350;&#23384;&#22312;&#30340;&#25968;&#25454;&#21019;&#24314;&#31574;&#30053;&#19981;&#31995;&#32479;&#21644;&#19981;&#21516;&#27880;&#37322;&#26041;&#26696;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#22810;&#20301;&#23398;&#32773;&#25351;&#20986;&#30340;&#37027;&#26679;&#65292;&#24403;&#21069;&#38024;&#23545;&#20167;&#24680;&#35328;&#35770;&#65288;HS&#65289;&#35782;&#21035;&#30340;&#30740;&#31350;&#29305;&#28857;&#26159;&#19981;&#31995;&#32479;&#30340;&#25968;&#25454;&#21019;&#24314;&#31574;&#30053;&#21644;&#19981;&#21516;&#30340;&#27880;&#37322;&#26041;&#26696;&#12290;&#22240;&#27492;&#65292;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#24448;&#24448;&#23545;&#23427;&#20204;&#26410;&#34987;&#35757;&#32451;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#27867;&#21270;&#24615;&#33021;&#24046;&#65292;&#24182;&#19988;&#19981;&#21516;HS&#20998;&#31867;&#27861;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#25152;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#24615;&#33021;&#26080;&#27861;&#27604;&#36739;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26497;&#24230;&#24369;&#30340;&#30417;&#30563;&#26041;&#27861;&#65292;&#21482;&#20381;&#36182;&#20110;&#31867;&#21035;&#21517;&#31216;&#32780;&#19981;&#26159;&#27880;&#37322;&#25968;&#25454;&#20013;&#30340;&#31867;&#21035;&#31034;&#20363;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#24369;&#30417;&#30563;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#20869;&#21644;&#36328;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;HS&#20998;&#31867;&#27169;&#22411;&#36890;&#29992;&#24615;&#36739;&#24046;&#30340;&#21407;&#22240;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#23450;&#37327;&#21644;&#23450;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
As pointed out by several scholars, current research on hate speech (HS) recognition is characterized by unsystematic data creation strategies and diverging annotation schemata. Subsequently, supervised-learning models tend to generalize poorly to datasets they were not trained on, and the performance of the models trained on datasets labeled using different HS taxonomies cannot be compared. To ease this problem, we propose applying extremely weak supervision that only relies on the class name rather than on class samples from the annotated data. We demonstrate the effectiveness of a state-of-the-art weakly-supervised text classification model in various in-dataset and cross-dataset settings. Furthermore, we conduct an in-depth quantitative and qualitative analysis of the source of poor generalizability of HS classification models.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#25311;&#35821;&#35328;&#29305;&#24449;&#30340;&#36890;&#20449;&#21327;&#35758;&#65292;&#29992;&#20110;&#20998;&#26512;&#20010;&#20307;&#21644;&#20849;&#20139;&#25277;&#35937;&#30340;&#24418;&#25104;&#21450;&#20854;&#23545;&#20219;&#21153;&#34920;&#29616;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#20248;&#21270;&#20449;&#24687;&#20869;&#23481;&#20197;&#26368;&#22823;&#21270;&#23398;&#29983;&#22870;&#21169;&#25913;&#21892;&#20102;&#20449;&#24687;&#32534;&#30721;&#65292;&#25552;&#39640;&#20102;&#23398;&#20064;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.02632</link><description>&lt;p&gt;
&#19968;&#31181;&#31038;&#20132;&#23398;&#20064;&#20195;&#29702;&#20013;&#35821;&#35328;&#20986;&#29616;&#19982;&#20998;&#26512;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A framework for the emergence and analysis of language in social learning agents. (arXiv:2305.02632v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02632
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#25311;&#35821;&#35328;&#29305;&#24449;&#30340;&#36890;&#20449;&#21327;&#35758;&#65292;&#29992;&#20110;&#20998;&#26512;&#20010;&#20307;&#21644;&#20849;&#20139;&#25277;&#35937;&#30340;&#24418;&#25104;&#21450;&#20854;&#23545;&#20219;&#21153;&#34920;&#29616;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#20248;&#21270;&#20449;&#24687;&#20869;&#23481;&#20197;&#26368;&#22823;&#21270;&#23398;&#29983;&#22870;&#21169;&#25913;&#21892;&#20102;&#20449;&#24687;&#32534;&#30721;&#65292;&#25552;&#39640;&#20102;&#23398;&#20064;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANNs&#65289;&#36234;&#26469;&#36234;&#34987;&#29992;&#20316;&#30740;&#31350;&#27169;&#22411;&#65292;&#20294;&#23427;&#20204;&#30340;&#26222;&#36866;&#24615;&#21644;&#34920;&#24449;&#19981;&#21464;&#24615;&#20173;&#23384;&#22312;&#38382;&#39064;&#12290;&#29983;&#29289;&#31070;&#32463;&#32593;&#32476;&#22312;&#31038;&#20132;&#32422;&#26463;&#19979;&#28436;&#21270;&#24418;&#25104;&#21487;&#20256;&#36798;&#30340;&#34920;&#24449;&#65292;&#23637;&#31034;&#20102;&#27867;&#21270;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21512;&#20316;&#20195;&#29702;&#20043;&#38388;&#30340;&#36890;&#20449;&#21327;&#35758;&#65292;&#29992;&#20110;&#20998;&#26512;&#20010;&#20307;&#21644;&#20849;&#20139;&#25277;&#35937;&#30340;&#24418;&#25104;&#21450;&#20854;&#23545;&#20219;&#21153;&#34920;&#29616;&#30340;&#24433;&#21709;&#12290;&#35813;&#36890;&#20449;&#21327;&#35758;&#26088;&#22312;&#36890;&#36807;&#20302;&#32500;&#34920;&#31034;&#32534;&#30721;&#39640;&#32500;&#20449;&#24687;&#65292;&#27169;&#25311;&#35821;&#35328;&#29305;&#24449;&#12290;&#20351;&#29992;&#32593;&#26684;&#19990;&#30028;&#36855;&#23467;&#21644;&#24378;&#21270;&#23398;&#20064;&#65292;&#25945;&#24072;ANNs&#21521;&#23398;&#29983;ANN&#20256;&#36882;&#21387;&#32553;&#28040;&#24687;&#65292;&#20197;&#20415;&#26356;&#22909;&#22320;&#23436;&#25104;&#20219;&#21153;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#23398;&#29983;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#30446;&#26631;&#21457;&#29616;&#29575;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#20219;&#21153;&#19990;&#30028;&#20013;&#25512;&#24191;&#20102;&#30446;&#26631;&#20301;&#32622;&#12290;&#36827;&#19968;&#27493;&#20248;&#21270;&#20449;&#24687;&#20869;&#23481;&#20197;&#26368;&#22823;&#21270;&#23398;&#29983;&#22870;&#21169;&#25913;&#21892;&#20102;&#20449;&#24687;&#32534;&#30721;&#65292;&#34920;&#26126;&#31934;&#30830;&#30340;&#34920;&#24449;&#21487;&#20197;&#22312;&#36890;&#20449;&#21327;&#35758;&#20013;&#24471;&#21040;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial neural networks (ANNs) are increasingly used as research models, but questions remain about their generalizability and representational invariance. Biological neural networks under social constraints evolved to enable communicable representations, demonstrating generalization capabilities. This study proposes a communication protocol between cooperative agents to analyze the formation of individual and shared abstractions and their impact on task performance. This communication protocol aims to mimic language features by encoding high-dimensional information through low-dimensional representation. Using grid-world mazes and reinforcement learning, teacher ANNs pass a compressed message to a student ANN for better task completion. Through this, the student achieves a higher goal-finding rate and generalizes the goal location across task worlds. Further optimizing message content to maximize student reward improves information encoding, suggesting that an accurate representati
&lt;/p&gt;</description></item><item><title>&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#30340;&#27969;&#34892;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#27979;&#35797;&#21644;&#20462;&#22797;LLM&#22312;&#36947;&#24503;&#20934;&#21017;&#19978;&#22833;&#24403;&#24314;&#35758;&#30340;&#26694;&#26550;&#65292;&#24182;&#24341;&#20837;ETHICSSUITE&#27979;&#35797;&#22871;&#20214;&#21644;&#24314;&#35758;-&#25209;&#21028;-&#21453;&#24605;(SCR)&#36807;&#31243;&#20197;&#33258;&#21160;&#26816;&#27979;&#21644;&#20462;&#22797;LLM&#30340;&#19981;&#36947;&#24503;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2305.02626</link><description>&lt;p&gt;
&#8220;&#21710;&#21568;&#65292;&#25105;&#21018;&#21018;&#35828;&#20102;&#20160;&#20040;&#65311;&#8221;&#29992;&#24314;&#35758;-&#25209;&#21028;-&#21453;&#24605;&#36807;&#31243;&#27979;&#35797;&#21644;&#20462;&#22797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#36947;&#24503;&#24314;&#35758;
&lt;/p&gt;
&lt;p&gt;
"Oops, Did I Just Say That?" Testing and Repairing Unethical Suggestions of Large Language Models with Suggest-Critique-Reflect Process. (arXiv:2305.02626v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02626
&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#30340;&#27969;&#34892;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#27979;&#35797;&#21644;&#20462;&#22797;LLM&#22312;&#36947;&#24503;&#20934;&#21017;&#19978;&#22833;&#24403;&#24314;&#35758;&#30340;&#26694;&#26550;&#65292;&#24182;&#24341;&#20837;ETHICSSUITE&#27979;&#35797;&#22871;&#20214;&#21644;&#24314;&#35758;-&#25209;&#21028;-&#21453;&#24605;(SCR)&#36807;&#31243;&#20197;&#33258;&#21160;&#26816;&#27979;&#21644;&#20462;&#22797;LLM&#30340;&#19981;&#36947;&#24503;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#30340;&#27969;&#34892;&#65292;&#30830;&#20445;&#23427;&#20204;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30456;&#19968;&#33268;&#24050;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#20851;&#27880;&#28857;&#12290;&#29305;&#21035;&#26159;&#65292;&#32771;&#34385;&#21040;LLM&#26377;&#26395;&#25104;&#20026;&#26085;&#24120;&#29983;&#27963;&#20013;&#36890;&#29992;&#30340;&#20154;&#24037;&#26234;&#33021;&#21161;&#25163;&#65292;&#23427;&#20204;&#24494;&#22937;&#30340;&#19981;&#36947;&#24503;&#24314;&#35758;&#25104;&#20026;&#20102;&#19968;&#20010;&#20005;&#37325;&#32780;&#29616;&#23454;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#33258;&#21160;&#21270;&#27979;&#35797;&#21644;&#20462;&#22797;&#19981;&#36947;&#24503;&#24314;&#35758;&#30340;&#25361;&#25112;&#26159;&#24456;&#22823;&#30340;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#27979;&#35797;&#21644;&#20462;&#22797;LLM&#19981;&#36947;&#24503;&#24314;&#35758;&#30340;&#26694;&#26550;&#12290;&#25105;&#39318;&#20808;&#25552;&#20986;&#20102;ETHICSSUITE&#65292;&#19968;&#20010;&#27979;&#35797;&#22871;&#20214;&#65292;&#29992;&#20110;&#27979;&#35797;LLM&#30340;&#22797;&#26434;&#12289;&#24773;&#22659;&#21270;&#21644;&#29616;&#23454;&#30340;&#36947;&#24503;&#22330;&#26223;&#12290;&#28982;&#21518;&#25105;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24314;&#35758;-&#25209;&#21028;-&#21453;&#24605;&#65288;SCR&#65289;&#36807;&#31243;&#65292;&#20316;&#20026;&#33258;&#21160;&#21270;&#27979;&#35797;&#39044;&#35328;&#65292;&#29992;&#20110;&#26816;&#27979;&#19981;&#36947;&#24503;&#24314;&#35758;&#12290;&#25105;&#20204;&#23558;&#21028;&#26029;LLM&#26159;&#21542;&#20135;&#29983;&#19981;&#36947;&#24503;&#24314;&#35758;&#65288;&#36890;&#24120;&#38656;&#35201;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#21644;&#25104;&#26412;&#36739;&#39640;&#65289;&#36716;&#21270;&#20026;&#19968;&#20010;&#21487;&#20197;&#33258;&#21160;&#26816;&#26597;&#36829;&#35268;&#30340;PCR&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;SCR&#36807;&#31243;&#36824;&#25552;&#20379;&#20102;&#20462;&#22797;&#19981;&#36947;&#24503;&#24314;&#35758;&#30340;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the popularity of large language models (LLMs) soars across various applications, ensuring their alignment with human values has become a paramount concern. In particular, given that LLMs have great potential to serve as general-purpose AI assistants in daily life, their subtly unethical suggestions become a serious and real concern. Tackling the challenge of automatically testing and repairing unethical suggestions is thus demanding.  This paper introduces the first framework for testing and repairing unethical suggestions made by LLMs. We first propose ETHICSSUITE, a test suite that presents complex, contextualized, and realistic moral scenarios to test LLMs. We then propose a novel suggest-critic-reflect (SCR) process, serving as an automated test oracle to detect unethical suggestions. We recast deciding if LLMs yield unethical suggestions (a hard problem; often requiring human expertise and costly to decide) into a PCR task that can be automatically checked for violation. Moreo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20250;&#35805;&#24773;&#24863;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#65288;CACD&#65289;&#65292;&#24182;&#36890;&#36807;&#35774;&#35745;&#20844;&#20849;&#39592;&#26550;&#21644;&#29983;&#25104;&#26367;&#20195;&#38544;&#21547;&#21407;&#22240;&#35299;&#20915;&#20102;&#22240;&#26524;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#38544;&#21547;&#21407;&#22240;&#30340;&#19981;&#21487;&#35266;&#23519;&#24615;&#30340;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#21464;&#38271;&#20250;&#35805;&#20013;&#21457;&#29616;&#22240;&#26524;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2305.02615</link><description>&lt;p&gt;
&#20250;&#35805;&#20013;&#30340;&#24773;&#24863;&#25512;&#29702;&#65306;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Affective Reasoning at Utterance Level in Conversations: A Causal Discovery Approach. (arXiv:2305.02615v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02615
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20250;&#35805;&#24773;&#24863;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#65288;CACD&#65289;&#65292;&#24182;&#36890;&#36807;&#35774;&#35745;&#20844;&#20849;&#39592;&#26550;&#21644;&#29983;&#25104;&#26367;&#20195;&#38544;&#21547;&#21407;&#22240;&#35299;&#20915;&#20102;&#22240;&#26524;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#38544;&#21547;&#21407;&#22240;&#30340;&#19981;&#21487;&#35266;&#23519;&#24615;&#30340;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#21464;&#38271;&#20250;&#35805;&#20013;&#21457;&#29616;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#25512;&#29702;&#20219;&#21153;&#26159;&#21253;&#25324;&#20250;&#35805;&#20013;&#30340;&#24773;&#24863;&#35782;&#21035;&#12289;&#24773;&#24863;-&#21407;&#22240;&#23545;&#25277;&#21462;&#21644;&#24773;&#24863;-&#21407;&#22240;&#36328;&#24230;&#35782;&#21035;&#22312;&#20869;&#30340;&#19968;&#32452;&#26032;&#20852;&#30340;&#22522;&#20110;&#24773;&#24863;&#30340;&#20219;&#21153;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#22312;&#20551;&#35774;&#34920;&#38754;&#20851;&#31995;&#26102;&#24573;&#30053;&#20102;&#22522;&#26412;&#30340;&#22240;&#26524;&#27169;&#22411;&#65292;&#22240;&#20026;&#39592;&#26550;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#38544;&#21547;&#21407;&#22240;&#30340;&#19981;&#21487;&#35266;&#23519;&#24615;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#19978;&#36848;&#20004;&#20010;&#38382;&#39064;&#65292;&#24182;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#20250;&#35805;&#24773;&#24863;&#22240;&#26524;&#21457;&#29616;&#65288;CACD&#65289;&#26041;&#27861;&#12290;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#35774;&#35745;&#20844;&#20849;&#39592;&#26550;&#21644;&#29983;&#25104;&#26367;&#20195;&#38544;&#21547;&#21407;&#22240;&#26469;&#21457;&#29616;&#20250;&#35805;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;CACD&#21253;&#21547;&#20004;&#20010;&#27493;&#39588;&#65306;&#65288;i&#65289;&#20026;&#21464;&#38271;&#20250;&#35805;&#20013;&#30340;&#25152;&#26377;&#35805;&#35821;&#24314;&#31435;&#19968;&#20010;&#20013;&#24515;&#21270;&#30340;&#21333;&#19968;&#22270;&#33410;&#28857;&#22240;&#26524;&#39592;&#26550;&#65307;&#65288;ii&#65289;&#22240;&#26524;&#33258;&#32534;&#30721;&#22120;&#65288;CAE&#65289;&#36890;&#36807;&#29983;&#25104;&#38544;&#21547;&#21407;&#22240;&#21644;&#24050;&#30693;&#26174;&#24335;&#21407;&#22240;&#26469;&#20462;&#27491;&#39592;&#26550;&#65292;&#20174;&#32780;&#20135;&#29983;&#22240;&#26524;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
The affective reasoning task is a set of emerging affect-based tasks in conversation, including Emotion Recognition in Conversation (ERC),Emotion-Cause Pair Extraction (ECPE), and Emotion-Cause Span Recognition (ECSR). Existing methods make various assumptions on the apparent relationship while neglecting the essential causal model due to the nonuniqueness of skeletons and unobservability of implicit causes. This paper settled down the above two problems and further proposed Conversational Affective Causal Discovery (CACD). It is a novel causal discovery method showing how to discover causal relationships in a conversation via designing a common skeleton and generating a substitute for implicit causes. CACD contains two steps: (i) building a common centering one graph node causal skeleton for all utterances in variable-length conversations; (ii) Causal Auto-Encoder (CAE) correcting the skeleton to yield causal representation through generated implicit causes and known explicit causes. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#22522;&#20110;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#39640;&#32500;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#29305;&#23450;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#37319;&#26679;&#12289;&#21442;&#25968;&#21270;&#37319;&#26679;&#20998;&#24067;&#30340;&#20248;&#21270;&#21450;&#21160;&#24577;&#36873;&#25321;&#26080;&#26631;&#35760;&#25968;&#25454;&#31561;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#39640;&#32500;&#36125;&#21494;&#26031;&#20248;&#21270;&#38590;&#20197;&#22788;&#29702;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.02614</link><description>&lt;p&gt;
&#22522;&#20110;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#39640;&#32500;&#36125;&#21494;&#26031;&#20248;&#21270;&#21450;&#20248;&#21270;&#26080;&#26631;&#31614;&#25968;&#25454;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
High-dimensional Bayesian Optimization via Semi-supervised Learning with Optimized Unlabeled Data Sampling. (arXiv:2305.02614v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02614
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#22522;&#20110;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#39640;&#32500;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#29305;&#23450;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#37319;&#26679;&#12289;&#21442;&#25968;&#21270;&#37319;&#26679;&#20998;&#24067;&#30340;&#20248;&#21270;&#21450;&#21160;&#24577;&#36873;&#25321;&#26080;&#26631;&#35760;&#25968;&#25454;&#31561;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#39640;&#32500;&#36125;&#21494;&#26031;&#20248;&#21270;&#38590;&#20197;&#22788;&#29702;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#26159;&#19968;&#31181;&#23547;&#25214;&#40657;&#31665;&#20989;&#25968;&#20840;&#23616;&#26368;&#20248;&#35299;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#34429;&#28982;&#40657;&#31665;&#20989;&#25968;&#30340;&#35780;&#20272;&#25104;&#26412;&#24448;&#24448;&#24456;&#39640;&#65292;&#20294;&#20943;&#23569;&#26114;&#36149;&#26631;&#35760;&#25968;&#25454;&#30340;&#20351;&#29992;&#26159;&#29702;&#24819;&#30340;&#12290;&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#31181;&#25945;&#24072;-&#23398;&#29983;&#27169;&#22411;&#65292;&#21033;&#29992;&#21322;&#30417;&#30563;&#23398;&#20064;&#22312;BO&#29615;&#22659;&#19979;&#21033;&#29992;&#22823;&#37327;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#12290;&#20854;&#20013;&#65292;&#20851;&#38190;&#22312;&#20110;&#36873;&#25321;&#39564;&#35777;&#21644;&#26410;&#26631;&#35760;&#25968;&#25454;&#20197;&#25552;&#39640;BO&#30340;&#34920;&#29616;&#12290;&#20026;&#20102;&#20248;&#21270;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#37319;&#26679;&#65292;&#25105;&#20204;&#37319;&#29992;&#40657;&#31665;&#21442;&#25968;&#21270;&#37319;&#26679;&#20998;&#24067;&#65292;&#23558;&#20854;&#20248;&#21270;&#20026;&#25152;&#37319;&#29992;&#21452;&#23618;&#20248;&#21270;&#26694;&#26550;&#30340;&#19968;&#37096;&#20998;&#12290;&#26356;&#36827;&#19968;&#27493;&#65292;&#36890;&#36807;&#20174;&#21160;&#24577;&#36866;&#24212;&#30340;&#26497;&#20540;&#20998;&#24067;&#20013;&#36873;&#25321;&#26410;&#26631;&#31614;&#25968;&#25454;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;BO&#30340;&#24615;&#33021;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#12290;&#25105;&#20204;&#30340;BO&#26041;&#27861;&#22312;&#23398;&#20064;&#21518;&#30340;&#20302;&#32500;&#28508;&#22312;&#31354;&#38388;&#20013;&#36816;&#34892;&#65292;&#20351;&#20854;&#21487;&#25193;&#23637;&#21040;&#39640;&#32500;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian optimization (BO) is a powerful tool for seeking the global optimum of black-box functions. While evaluations of the black-box functions can be highly costly, it is desirable to reduce the use of expensive labeled data. For the first time, we introduce a teacher-student model to exploit semi-supervised learning that can make use of large amounts of unlabelled data under the context of BO. Importantly, we show that the selection of the validation and unlabeled data is key to the performance of BO. To optimize the sampling of unlabeled data, we employ a black-box parameterized sampling distribution optimized as part of the employed bi-level optimization framework. Taking one step further, we demonstrate that the performance of BO can be further improved by selecting unlabeled data from a dynamically fitted extreme value distribution. Our BO method operates in a learned latent space with reduced dimensionality, making it scalable to high-dimensional problems. The proposed approac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#20851;&#38190;&#35789;&#39044;&#27979;&#30340;&#20195;&#34920;&#24615;&#30740;&#31350;&#65292;&#20998;&#26512;&#20102;&#20027;&#35201;&#27169;&#22411;&#12289;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#29305;&#21035;&#20851;&#27880;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20851;&#38190;&#35789;&#39044;&#27979;&#65292;&#24182;&#36827;&#34892;&#20102;&#23454;&#39564;&#27604;&#36739;&#20195;&#34920;&#24615;&#27169;&#22411;&#65292;&#20026;&#28145;&#20837;&#20998;&#26512;&#23427;&#20204;&#30340;&#20248;&#32570;&#28857;&#25552;&#20379;&#20102;&#21487;&#27604;&#24615;&#30340;&#25968;&#25454;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2305.02579</link><description>&lt;p&gt;
&#20174;&#32479;&#35745;&#26041;&#27861;&#21040;&#28145;&#24230;&#23398;&#20064;&#65292;&#33258;&#21160;&#20851;&#38190;&#35789;&#39044;&#27979;&#65306;&#19968;&#27425;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
From Statistical Methods to Deep Learning, Automatic Keyphrase Prediction: A Survey. (arXiv:2305.02579v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02579
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#20851;&#38190;&#35789;&#39044;&#27979;&#30340;&#20195;&#34920;&#24615;&#30740;&#31350;&#65292;&#20998;&#26512;&#20102;&#20027;&#35201;&#27169;&#22411;&#12289;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#29305;&#21035;&#20851;&#27880;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20851;&#38190;&#35789;&#39044;&#27979;&#65292;&#24182;&#36827;&#34892;&#20102;&#23454;&#39564;&#27604;&#36739;&#20195;&#34920;&#24615;&#27169;&#22411;&#65292;&#20026;&#28145;&#20837;&#20998;&#26512;&#23427;&#20204;&#30340;&#20248;&#32570;&#28857;&#25552;&#20379;&#20102;&#21487;&#27604;&#24615;&#30340;&#25968;&#25454;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#38190;&#35789;&#39044;&#27979;&#26088;&#22312;&#29983;&#25104;&#39640;&#24230;&#24635;&#32467;&#32473;&#23450;&#25991;&#26723;&#30340;&#30701;&#35821;&#65288;&#20851;&#38190;&#35789;&#65289;&#12290;&#36817;&#24180;&#26469;&#65292;&#30740;&#31350;&#20154;&#21592;&#20174;&#19981;&#21516;&#30340;&#35270;&#35282;&#23545;&#36825;&#20010;&#20219;&#21153;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#30740;&#31350;&#12290;&#26412;&#25991;&#20174;&#20027;&#35201;&#27169;&#22411;&#12289;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#30340;&#35282;&#24230;&#20840;&#38754;&#24635;&#32467;&#20102;&#20195;&#34920;&#24615;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#22810;&#36798;167&#31687;&#20808;&#21069;&#30340;&#20316;&#21697;&#65292;&#27604;&#20197;&#21069;&#30340;&#35843;&#26597;&#23454;&#29616;&#20102;&#26356;&#22823;&#33539;&#22260;&#30340;&#35206;&#30422;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#39640;&#24230;&#20851;&#27880;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20851;&#38190;&#35789;&#39044;&#27979;&#65292;&#22312;&#36817;&#24180;&#26469;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20960;&#32452;&#23454;&#39564;&#65292;&#20180;&#32454;&#27604;&#36739;&#20102;&#20195;&#34920;&#24615;&#27169;&#22411;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#26159;&#31532;&#19968;&#20010;&#20351;&#29992;&#30456;&#21516;&#24120;&#29992;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#26469;&#27604;&#36739;&#36825;&#20123;&#27169;&#22411;&#30340;&#23581;&#35797;&#65292;&#26377;&#21161;&#20110;&#28145;&#20837;&#20998;&#26512;&#23427;&#20204;&#30340;&#20248;&#32570;&#28857;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#26410;&#26469;&#35813;&#20219;&#21153;&#30340;&#21487;&#33021;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Keyphrase prediction aims to generate phrases (keyphrases) that highly summarizes a given document. Recently, researchers have conducted in-depth studies on this task from various perspectives. In this paper, we comprehensively summarize representative studies from the perspectives of dominant models, datasets and evaluation metrics. Our work analyzes up to 167 previous works, achieving greater coverage of this task than previous surveys. Particularly, we focus highly on deep learning-based keyphrase prediction, which attracts increasing attention of this task in recent years. Afterwards, we conduct several groups of experiments to carefully compare representative models. To the best of our knowledge, our work is the first attempt to compare these models using the identical commonly-used datasets and evaluation metric, facilitating in-depth analyses of their disadvantages and advantages. Finally, we discuss the possible research directions of this task in the future.
&lt;/p&gt;</description></item><item><title>ChatGPT&#21644;Bard&#31561;AI&#24037;&#20855;&#38656;&#35201;&#25345;&#32493;&#22823;&#37327;&#19988;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#26469;&#25552;&#39640;&#20854;&#24615;&#33021;&#65292;&#20294;&#29616;&#34892;&#30340;&#29256;&#26435;&#27861;&#21017;&#38480;&#21046;&#20102;&#23427;&#20204;&#23545;&#21508;&#31181;&#25968;&#25454;&#30340;&#33719;&#21462;&#12290;&#19982;&#25968;&#25454;&#25552;&#20379;&#32773;&#20998;&#20139;&#25910;&#30410;&#23558;&#26377;&#21161;&#20110;&#23558;AI&#24037;&#20855;&#19982;&#22823;&#22810;&#25968;&#29256;&#26435;&#25968;&#25454;&#25317;&#26377;&#32773;&#20043;&#38388;&#30340;&#25932;&#23545;&#20851;&#31995;&#36716;&#21464;&#20026;&#21512;&#20316;&#20851;&#31995;&#65292;&#20351;AI&#29983;&#24577;&#31995;&#32479;&#26356;&#20581;&#24247;&#12290;</title><link>http://arxiv.org/abs/2305.02555</link><description>&lt;p&gt;
ChatGPT&#21644;Bard&#26159;&#21542;&#24212;&#35813;&#19982;&#20854;&#25968;&#25454;&#25552;&#20379;&#32773;&#20998;&#20139;&#25910;&#30410;&#65311;AI&#26102;&#20195;&#30340;&#26032;&#21830;&#19994;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Should ChatGPT and Bard Share Revenue with Their Data Providers? A New Business Model for the AI Era. (arXiv:2305.02555v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02555
&lt;/p&gt;
&lt;p&gt;
ChatGPT&#21644;Bard&#31561;AI&#24037;&#20855;&#38656;&#35201;&#25345;&#32493;&#22823;&#37327;&#19988;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#26469;&#25552;&#39640;&#20854;&#24615;&#33021;&#65292;&#20294;&#29616;&#34892;&#30340;&#29256;&#26435;&#27861;&#21017;&#38480;&#21046;&#20102;&#23427;&#20204;&#23545;&#21508;&#31181;&#25968;&#25454;&#30340;&#33719;&#21462;&#12290;&#19982;&#25968;&#25454;&#25552;&#20379;&#32773;&#20998;&#20139;&#25910;&#30410;&#23558;&#26377;&#21161;&#20110;&#23558;AI&#24037;&#20855;&#19982;&#22823;&#22810;&#25968;&#29256;&#26435;&#25968;&#25454;&#25317;&#26377;&#32773;&#20043;&#38388;&#30340;&#25932;&#23545;&#20851;&#31995;&#36716;&#21464;&#20026;&#21512;&#20316;&#20851;&#31995;&#65292;&#20351;AI&#29983;&#24577;&#31995;&#32479;&#26356;&#20581;&#24247;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;ChatGPT&#31561;&#21508;&#31181;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#25105;&#20204;&#27491;&#36827;&#20837;&#30495;&#27491;&#30340;AI&#26102;&#20195;&#12290;&#25105;&#20204;&#21487;&#20197;&#39044;&#35265;&#65292;&#21331;&#36234;&#30340;AI&#24037;&#20855;&#24456;&#24555;&#23558;&#33719;&#24471;&#21487;&#35266;&#30340;&#21033;&#28070;&#12290;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#20986;&#29616;&#20102;&#65306;&#38500;&#20102;&#20256;&#32479;&#30340;&#21033;&#30410;&#30456;&#20851;&#32773;&#21644;&#32929;&#19996;&#65292;AI&#24037;&#20855;&#26159;&#21542;&#24212;&#35813;&#19982;&#23427;&#20204;&#30340;&#35757;&#32451;&#25968;&#25454;&#25552;&#20379;&#32773;&#20998;&#20139;&#25910;&#30410;&#65311;&#31572;&#26696;&#26159;&#32943;&#23450;&#30340;&#12290;&#22823;&#22411;AI&#24037;&#20855;&#65292;&#20363;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22987;&#32456;&#38656;&#35201;&#26356;&#22810;&#12289;&#26356;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#26469;&#19981;&#26029;&#25913;&#36827;&#65292;&#20294;&#24403;&#21069;&#30340;&#29256;&#26435;&#27861;&#38480;&#21046;&#20102;&#23427;&#20204;&#23545;&#21508;&#31181;&#31867;&#22411;&#25968;&#25454;&#30340;&#33719;&#21462;&#12290;&#22312;AI&#24037;&#20855;&#21644;&#25968;&#25454;&#25552;&#20379;&#32773;&#20043;&#38388;&#20998;&#20139;&#25910;&#30410;&#21487;&#20197;&#23558;&#24403;&#21069;&#25932;&#23545;&#30340;&#38646;&#21644;&#28216;&#25103;&#20851;&#31995;&#36716;&#21464;&#20026;&#19968;&#31181;&#21512;&#20316;&#21644;&#20114;&#21033;&#30340;&#20851;&#31995;&#65292;&#32780;&#36825;&#31181;&#20851;&#31995;&#23545;&#20110;&#20419;&#36827;AI&#24037;&#20855;&#12289;&#29992;&#25143;&#21644;&#25968;&#25454;&#25552;&#20379;&#32773;&#20043;&#38388;&#30340;&#33391;&#24615;&#24490;&#29615;&#21457;&#23637;&#12289;&#25512;&#21160;AI&#25216;&#26415;&#24182;&#24314;&#31435;&#20581;&#24247;&#30340;AI&#29983;&#24577;&#31995;&#32479;&#26159;&#24517;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#25910;&#30410;&#20998;&#20139;&#21830;&#19994;&#27169;&#24335;&#8230;&#8230;
&lt;/p&gt;
&lt;p&gt;
With various AI tools such as ChatGPT becoming increasingly popular, we are entering a true AI era. We can foresee that exceptional AI tools will soon reap considerable profits. A crucial question arise: should AI tools share revenue with their training data providers in additional to traditional stakeholders and shareholders? The answer is Yes. Large AI tools, such as large language models, always require more and better quality data to continuously improve, but current copyright laws limit their access to various types of data. Sharing revenue between AI tools and their data providers could transform the current hostile zero-sum game relationship between AI tools and a majority of copyrighted data owners into a collaborative and mutually beneficial one, which is necessary to facilitate the development of a virtuous cycle among AI tools, their users and data providers that drives forward AI technology and builds a healthy AI ecosystem. However, current revenue-sharing business models 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#22522;&#20110;LLMs&#27169;&#25311;&#20195;&#29702;&#30340;&#34892;&#20026;&#65292;&#31216;&#20043;&#20026;LLM Personas&#65292;&#22312;&#20998;&#37197;&#22823;&#20116;&#20154;&#26684;&#31867;&#22411;&#21644;&#24615;&#21035;&#35282;&#33394;&#26102;&#26159;&#21542;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#19968;&#33268;&#24615;&#30340;&#20010;&#24615;&#21270;&#29305;&#36136;&#30340;&#20869;&#23481;&#12290;</title><link>http://arxiv.org/abs/2305.02547</link><description>&lt;p&gt;
PersonaLLM: &#25506;&#31350;GPT-3.5&#34920;&#36798;&#20010;&#24615;&#29305;&#24449;&#21644;&#24615;&#21035;&#24046;&#24322;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
PersonaLLM: Investigating the Ability of GPT-3.5 to Express Personality Traits and Gender Differences. (arXiv:2305.02547v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02547
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#22522;&#20110;LLMs&#27169;&#25311;&#20195;&#29702;&#30340;&#34892;&#20026;&#65292;&#31216;&#20043;&#20026;LLM Personas&#65292;&#22312;&#20998;&#37197;&#22823;&#20116;&#20154;&#26684;&#31867;&#22411;&#21644;&#24615;&#21035;&#35282;&#33394;&#26102;&#26159;&#21542;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#19968;&#33268;&#24615;&#30340;&#20010;&#24615;&#21270;&#29305;&#36136;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#20010;&#34892;&#19994;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#35774;&#35745;&#20013;&#26377;&#35768;&#22810;&#29992;&#36884;&#65292;&#24182;&#19988;&#30740;&#31350;&#34920;&#26126;&#20010;&#24615;&#21270;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#28385;&#36275;&#19981;&#21516;&#20154;&#26684;&#29305;&#24449;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#65292;&#20294;&#24456;&#23569;&#26377;&#30740;&#31350;&#35780;&#20272;&#20010;&#24615;&#21270;LLM&#30340;&#34892;&#20026;&#26159;&#21542;&#33021;&#22815;&#20934;&#30830;&#12289;&#19968;&#33268;&#22320;&#21453;&#26144;&#26576;&#20123;&#20154;&#26684;&#29305;&#24449;&#12290;&#25105;&#20204;&#32771;&#34385;&#30740;&#31350;&#22522;&#20110;LLM&#30340;&#27169;&#25311;&#20195;&#29702;&#30340;&#34892;&#20026;&#65292;&#31216;&#20043;&#20026;LLM personas&#65292;&#24182;&#20351;&#29992;GPT-3.5&#65288;text-davinci-003&#65289;&#36827;&#34892;&#26696;&#20363;&#30740;&#31350;&#65292;&#20197;&#30740;&#31350;LLM&#22312;&#20998;&#37197;&#22823;&#20116;&#20154;&#26684;&#31867;&#22411;&#21644;&#24615;&#21035;&#35282;&#33394;&#26102;&#26159;&#21542;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#19968;&#33268;&#24615;&#30340;&#20010;&#24615;&#21270;&#29305;&#36136;&#30340;&#20869;&#23481;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;320&#20010;LLM personas&#65288;&#27599;&#31181;&#22823;&#20116;&#20154;&#26684;&#31867;&#22411;&#26377;5&#20010;&#22899;&#24615;&#21644;5&#20010;&#30007;&#24615;&#65289;&#65292;&#24182;&#25552;&#31034;&#20182;&#20204;&#23436;&#25104;&#32463;&#20856;&#30340;44&#39033;&#22823;&#20116;&#20154;&#26684;&#38382;&#21367;&#65288;BFI&#65289;&#65292;&#28982;&#21518;&#25776;&#20889;&#19968;&#20010;&#20851;&#20110;&#20182;&#20204;&#31461;&#24180;&#30340;800&#23383;&#25925;&#20107;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;LLM personas&#30340;&#33258;&#25105;&#25253;&#21578;&#30340;BFI&#20998;&#25968;&#19982;&#20182;&#20204;&#20998;&#37197;&#30340;&#20154;&#26684;&#31867;&#22411;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the many use cases for large language models (LLMs) in the design of chatbots in various industries and the research showing the importance of personalizing chatbots to cater to different personality traits, little work has been done to evaluate whether the behaviors of personalized LLMs can reflect certain personality traits accurately and consistently. We consider studying the behavior of LLM-based simulated agents which refer to as LLM personas and present a case study with GPT-3.5 (text-davinci-003) to investigate whether LLMs can generate content with consistent, personalized traits when assigned Big Five personality types and gender roles. We created 320 LLM personas (5 females and 5 males for each of the 32 Big Five personality types) and prompted them to complete the classic 44-item Big Five Inventory (BFI) and then write an 800-word story about their childhood. Results showed that LLM personas' self-reported BFI scores are consistent with their assigned personality typ
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#35821;&#35328;&#25552;&#31034;&#19979;&#30340;&#22870;&#21169;&#26102;&#38388;&#20559;&#22909;&#65292;&#24182;&#21457;&#29616;GPT&#22312;&#20855;&#26377;&#36739;&#24369;&#26410;&#26469;&#26102;&#24577;&#30340;&#35821;&#35328;&#19979;&#34920;&#29616;&#20986;&#26356;&#22823;&#30340;&#32784;&#24515;&#65292;&#36825;&#19982;&#20351;&#29992;&#35813;&#35821;&#35328;&#30340;&#20154;&#31867;&#30340;&#20559;&#22909;&#30456;&#20284;&#12290;</title><link>http://arxiv.org/abs/2305.02531</link><description>&lt;p&gt;
&#35821;&#35328;&#12289;&#26102;&#38388;&#20559;&#22909;&#21644;&#28040;&#36153;&#34892;&#20026;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35777;&#25454;
&lt;/p&gt;
&lt;p&gt;
Language, Time Preferences, and Consumer Behavior: Evidence from Large Language Models. (arXiv:2305.02531v1 [econ.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02531
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#35821;&#35328;&#25552;&#31034;&#19979;&#30340;&#22870;&#21169;&#26102;&#38388;&#20559;&#22909;&#65292;&#24182;&#21457;&#29616;GPT&#22312;&#20855;&#26377;&#36739;&#24369;&#26410;&#26469;&#26102;&#24577;&#30340;&#35821;&#35328;&#19979;&#34920;&#29616;&#20986;&#26356;&#22823;&#30340;&#32784;&#24515;&#65292;&#36825;&#19982;&#20351;&#29992;&#35813;&#35821;&#35328;&#30340;&#20154;&#31867;&#30340;&#20559;&#22909;&#30456;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#23545;&#25105;&#20204;&#23545;&#26102;&#38388;&#21644;&#22870;&#21169;&#30340;&#24863;&#30693;&#26377;&#24456;&#22823;&#30340;&#24433;&#21709;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#24403;&#20197;&#19981;&#21516;&#30340;&#35821;&#35328;&#35810;&#38382;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#65292;&#23427;&#20204;&#26159;&#21542;&#26174;&#31034;&#20986;&#19981;&#21516;&#30340;&#22870;&#21169;&#26102;&#38388;&#20559;&#22909;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#36873;&#25321;&#26159;&#21542;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#36873;&#25321;&#12290;&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;GPT-3.5&#65288;&#20197;&#19979;&#31616;&#31216;GPT&#65289;&#22312;&#22810;&#31181;&#35821;&#35328;&#25552;&#31034;&#19979;&#30340;&#21709;&#24212;&#65292;&#25506;&#32034;&#20102;&#36739;&#23567;&#12289;&#36739;&#26089;&#30340;&#22870;&#21169;&#21644;&#36739;&#22823;&#12289;&#36739;&#26202;&#30340;&#22870;&#21169;&#20043;&#38388;&#30340;&#20559;&#22909;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#24403;&#20197;&#35821;&#20041;&#21547;&#20041;&#36739;&#24369;&#30340;&#26410;&#26469;&#26102;&#24577;&#21442;&#32771;&#65288;FTR&#65289;&#65292;&#22914;&#24503;&#35821;&#21644;&#27721;&#35821;&#65292;&#20026;&#25552;&#31034;&#35821;&#26102;&#65292;GPT&#34920;&#29616;&#20986;&#26356;&#22823;&#30340;&#32784;&#24515;&#65292;&#30456;&#27604;&#33521;&#35821;&#21644;&#27861;&#35821;&#31561;&#20855;&#26377;&#24378;&#22823;FTR&#30340;&#35821;&#35328;&#12290;&#36825;&#20123;&#21457;&#29616;&#19982;&#29616;&#26377;&#25991;&#29486;&#19968;&#33268;&#65292;&#24182;&#34920;&#26126;&#20102;GPT&#30340;&#36873;&#25321;&#19982;&#36825;&#20123;&#35821;&#35328;&#30340;&#20351;&#29992;&#32773;&#30340;&#20559;&#22909;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#28982;&#32780;&#65292;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#36739;&#26089;&#25110;&#36739;&#26202;&#22870;&#21169;&#30340;&#20559;&#22909;&#24182;&#27809;&#26377;&#38543;&#30528;&#22870;&#21169;&#24046;&#24322;&#31995;&#32479;&#22320;&#25913;&#21464;&#65292;&#36825;&#34920;&#26126;&#20102;&#19968;&#31181;&#35789;&#20856;&#24207;&#20248;&#20808;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language has a strong influence on our perceptions of time and rewards. This raises the question of whether large language models, when asked in different languages, show different preferences for rewards over time and if their choices are similar to those of humans. In this study, we analyze the responses of GPT-3.5 (hereafter referred to as GPT) to prompts in multiple languages, exploring preferences between smaller, sooner rewards and larger, later rewards. Our results show that GPT displays greater patience when prompted in languages with weak future tense references (FTR), such as German and Mandarin, compared to languages with strong FTR, like English and French. These findings are consistent with existing literature and suggest a correlation between GPT's choices and the preferences of speakers of these languages. However, further analysis reveals that the preference for earlier or later rewards does not systematically change with reward gaps, indicating a lexicographic preferen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#24310;&#36831;&#12289;&#22797;&#21512;&#21644;&#37096;&#20998;&#21311;&#21517;&#22870;&#21169;&#21453;&#39304;&#30340;&#26080;&#38480;&#26102;&#24179;&#22343;&#22870;&#21169;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;(MDP)&#65292;&#24182;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.02527</link><description>&lt;p&gt;
&#12298;&#24310;&#36831;&#12289;&#22797;&#21512;&#21644;&#37096;&#20998;&#21311;&#21517;&#22870;&#21169;&#30340;&#24378;&#21270;&#23398;&#20064;&#12299;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning with Delayed, Composite, and Partially Anonymous Reward. (arXiv:2305.02527v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02527
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#24310;&#36831;&#12289;&#22797;&#21512;&#21644;&#37096;&#20998;&#21311;&#21517;&#22870;&#21169;&#21453;&#39304;&#30340;&#26080;&#38480;&#26102;&#24179;&#22343;&#22870;&#21169;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;(MDP)&#65292;&#24182;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#24310;&#36831;&#12289;&#22797;&#21512;&#21644;&#37096;&#20998;&#21311;&#21517;&#22870;&#21169;&#21453;&#39304;&#30340;&#26080;&#38480;&#26102;&#24179;&#22343;&#22870;&#21169;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;(MDP)&#12290;&#22870;&#21169;&#30340;&#24310;&#36831;&#21644;&#22797;&#26434;&#24615;&#24847;&#21619;&#30528;&#22312;&#32473;&#23450;&#29366;&#24577;&#19979;&#37319;&#21462;&#34892;&#21160;&#29983;&#25104;&#30340;&#22870;&#21169;&#34987;&#20998;&#35299;&#20026;&#19981;&#21516;&#30340;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#22312;&#24310;&#36831;&#30340;&#26102;&#38388;&#23454;&#20363;&#20013;&#34987;&#39034;&#24207;&#23454;&#29616;&#12290;&#37096;&#20998;&#21311;&#21517;&#23646;&#24615;&#24847;&#21619;&#30528;&#23545;&#20110;&#27599;&#20010;&#29366;&#24577;&#65292;&#23398;&#20064;&#32773;&#21482;&#35266;&#23519;&#21040;&#22312;&#35813;&#29366;&#24577;&#19979;&#37319;&#21462;&#19981;&#21516;&#34892;&#21160;&#20135;&#29983;&#30340;&#36807;&#21435;&#22870;&#21169;&#32452;&#25104;&#37096;&#20998;&#30340;&#24635;&#21644;&#65292;&#20294;&#26159;&#22312;&#35266;&#23519;&#23454;&#20363;&#20013;&#23454;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;$\mathrm{DUCRL2}$&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#33719;&#24471;&#27492;&#35774;&#32622;&#30340;&#36817;&#20284;&#26368;&#20248;&#31574;&#30053;&#65292;&#24182;&#34920;&#26126;&#23427;&#23454;&#29616;&#20102;$\tilde{\mathcal{O}}\left(DS\sqrt{AT} + d (SA)^3\right)$ &#30340;&#36951;&#25022;&#30028;&#65292;&#20854;&#20013;$S$&#21644;$A$&#20998;&#21035;&#26159;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#30340;&#22823;&#23567;&#65292;$D$&#26159;MDP&#30340;&#30452;&#24452;&#65292;$d$&#26159;&#19968;&#20010;&#30001;&#26368;&#22823;&#22870;&#21169;&#24310;&#36831;&#38480;&#21046;&#30340;&#21442;&#25968;&#65292;$T$&#34920;&#31034;&#26102;&#38388;&#30340;&#38271;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate an infinite-horizon average reward Markov Decision Process (MDP) with delayed, composite, and partially anonymous reward feedback. The delay and compositeness of rewards mean that rewards generated as a result of taking an action at a given state are fragmented into different components, and they are sequentially realized at delayed time instances. The partial anonymity attribute implies that a learner, for each state, only observes the aggregate of past reward components generated as a result of different actions taken at that state, but realized at the observation instance. We propose an algorithm named $\mathrm{DUCRL2}$ to obtain a near-optimal policy for this setting and show that it achieves a regret bound of $\tilde{\mathcal{O}}\left(DS\sqrt{AT} + d (SA)^3\right)$ where $S$ and $A$ are the sizes of the state and action spaces, respectively, $D$ is the diameter of the MDP, $d$ is a parameter upper bounded by the maximum reward delay, and $T$ denotes the time horizon
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#19968;&#31181;&#26032;&#30340;&#31038;&#20250;&#24515;&#29702;&#23398;&#35282;&#24230;&#37325;&#26032;&#23457;&#35270;&#27531;&#24046;&#32593;&#32476;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#21457;&#29616;&#20102;&#32593;&#32476;&#36129;&#29486;&#19981;&#36275;&#38382;&#39064;&#24182;&#25552;&#20986;&#35299;&#20915;&#26041;&#26696;&#21644;&#25913;&#36827;&#31574;&#30053;&#65292;&#20197;&#25552;&#39640;&#27531;&#24046;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.02507</link><description>&lt;p&gt;
Stimulative Training++&#65306;&#36229;&#36234;&#27531;&#24046;&#32593;&#32476;&#24615;&#33021;&#26497;&#38480;
&lt;/p&gt;
&lt;p&gt;
Stimulative Training++: Go Beyond The Performance Limits of Residual Networks. (arXiv:2305.02507v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02507
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#19968;&#31181;&#26032;&#30340;&#31038;&#20250;&#24515;&#29702;&#23398;&#35282;&#24230;&#37325;&#26032;&#23457;&#35270;&#27531;&#24046;&#32593;&#32476;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#21457;&#29616;&#20102;&#32593;&#32476;&#36129;&#29486;&#19981;&#36275;&#38382;&#39064;&#24182;&#25552;&#20986;&#35299;&#20915;&#26041;&#26696;&#21644;&#25913;&#36827;&#31574;&#30053;&#65292;&#20197;&#25552;&#39640;&#27531;&#24046;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27531;&#24046;&#32593;&#32476;&#22312;&#36817;&#26399;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#34920;&#29616;&#20986;&#26497;&#22823;&#30340;&#25104;&#21151;&#65292;&#24182;&#21464;&#24471;&#19981;&#21487;&#25110;&#32570;&#12290;&#26412;&#25991;&#20174;&#19968;&#31181;&#26032;&#30340;&#31038;&#20250;&#24515;&#29702;&#23398;&#35282;&#24230;&#37325;&#26032;&#32771;&#23519;&#20102;&#27531;&#24046;&#32593;&#32476;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#26696;&#20197;&#21450;&#19977;&#31181;&#25913;&#36827;&#31574;&#30053;&#65292;&#20197;&#25552;&#39640;&#27531;&#24046;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#21457;&#29616;&#19968;&#31181;&#31867;&#20284;&#31038;&#20250;&#36129;&#29486;&#30340;&#34987;&#24573;&#35270;&#30340;&#38382;&#39064;&#65292;&#21363;&#22312;&#27531;&#24046;&#32593;&#32476;&#20869;&#37096;&#65292;&#23376;&#32593;&#32476;&#22312;&#32676;&#20307;&#20013;&#24037;&#20316;&#26102;&#20542;&#21521;&#20110;&#27604;&#29420;&#33258;&#24037;&#20316;&#26102;&#20184;&#20986;&#26356;&#23569;&#30340;&#21162;&#21147;&#65292;&#36825;&#34987;&#25105;&#20204;&#23450;&#20041;&#20026;&#8220;&#32593;&#32476;&#36129;&#29486;&#19981;&#36275;&#8221;&#12290;&#19982;&#31038;&#20250;&#20013;&#34920;&#29616;&#20986;&#30340;&#20010;&#20307;&#29983;&#20135;&#21147;&#21644;&#25972;&#20307;&#32489;&#25928;&#19979;&#38477;&#31867;&#20284;&#65292;&#32593;&#32476;&#36129;&#29486;&#19981;&#36275;&#24517;&#28982;&#23548;&#33268;&#27531;&#24046;&#32593;&#32476;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;
Residual networks have shown great success and become indispensable in recent deep neural network models. In this work, we aim to re-investigate the training process of residual networks from a novel social psychology perspective of loafing, and further propose a new training scheme as well as three improved strategies for boosting residual networks beyond their performance limits. Previous research has suggested that residual networks can be considered as ensembles of shallow networks, which implies that the final performance of a residual network is influenced by a group of subnetworks. We identify a previously overlooked problem that is analogous to social loafing, where subnetworks within a residual network are prone to exert less effort when working as part of a group compared to working alone. We define this problem as \textit{network loafing}. Similar to the decreased individual productivity and overall performance as demonstrated in society, network loafing inevitably causes su
&lt;/p&gt;</description></item><item><title>AutoML-GPT &#26159;&#19968;&#31181;&#22522;&#20110; GPT &#30340;&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21160;&#24577;&#22320;&#21033;&#29992;&#21508;&#31181;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#33258;&#21160;&#21270;&#35757;&#32451;&#31649;&#36947;&#65292;&#33410;&#32422;&#20102;&#36873;&#25321;&#27169;&#22411;&#26550;&#26500;&#12289;&#20248;&#21270;&#31639;&#27861;&#21644;&#35843;&#25972;&#36229;&#21442;&#25968;&#30340;&#20154;&#21147;&#21644;&#26102;&#38388;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2305.02499</link><description>&lt;p&gt;
AutoML-GPT: &#22522;&#20110; GPT &#30340;&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
AutoML-GPT: Automatic Machine Learning with GPT. (arXiv:2305.02499v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02499
&lt;/p&gt;
&lt;p&gt;
AutoML-GPT &#26159;&#19968;&#31181;&#22522;&#20110; GPT &#30340;&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21160;&#24577;&#22320;&#21033;&#29992;&#21508;&#31181;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#33258;&#21160;&#21270;&#35757;&#32451;&#31649;&#36947;&#65292;&#33410;&#32422;&#20102;&#36873;&#25321;&#27169;&#22411;&#26550;&#26500;&#12289;&#20248;&#21270;&#31639;&#27861;&#21644;&#35843;&#25972;&#36229;&#21442;&#25968;&#30340;&#20154;&#21147;&#21644;&#26102;&#38388;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI &#20219;&#21153;&#28085;&#30422;&#20102;&#24191;&#27867;&#30340;&#39046;&#22495;&#21644;&#39046;&#22495;&#12290;&#34429;&#28982;&#20026;&#29305;&#23450;&#20219;&#21153;&#21644;&#24212;&#29992;&#31243;&#24207;&#35774;&#35745;&#20102;&#20247;&#22810; AI &#27169;&#22411;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#21147;&#25237;&#20837;&#26469;&#26597;&#25214;&#27491;&#30830;&#30340;&#27169;&#22411;&#26550;&#26500;&#12289;&#20248;&#21270;&#31639;&#27861;&#21644;&#36229;&#21442;&#25968;&#12290;&#26368;&#36817;&#65292;&#20687; ChatGPT &#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#22312;&#25512;&#29702;&#12289;&#29702;&#35299;&#21644;&#20132;&#20114;&#30340;&#21508;&#20010;&#26041;&#38754;&#23637;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24320;&#21457;&#38754;&#21521;&#20219;&#21153;&#30340;&#25552;&#31034;&#24182;&#33258;&#21160;&#21033;&#29992; LLM &#33258;&#21160;&#21270;&#35757;&#32451;&#31649;&#36947;&#30340;&#24819;&#27861;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#27010;&#24565;&#65292;&#25105;&#20204;&#25512;&#20986;&#20102; AutoML-GPT&#65292;&#23427;&#37319;&#29992; GPT &#20316;&#20026;&#36830;&#25509;&#22810;&#31181; AI &#27169;&#22411;&#30340;&#26725;&#26753;&#65292;&#24182;&#21160;&#24577;&#22320;&#20351;&#29992;&#20248;&#21270;&#36229;&#21442;&#25968;&#35757;&#32451;&#27169;&#22411;&#12290;AutoML-GPT &#20174;&#27169;&#22411;&#21644;&#25968;&#25454;&#21345;&#20013;&#21160;&#24577;&#33719;&#21462;&#29992;&#25143;&#35831;&#27714;&#65292;&#24182;&#32452;&#25104;&#30456;&#24212;&#30340;&#25552;&#31034;&#27573;&#33853;&#12290;&#26368;&#32456;&#65292;&#36890;&#36807;&#36825;&#20010;&#25552;&#31034;&#27573;&#33853;&#65292;AutoML-GPT &#23558;&#33258;&#21160;&#20174;&#25968;&#25454;&#22788;&#29702;&#21040;&#27169;&#22411;&#26550;&#26500;&#12289;&#36229;&#21442;&#25968;&#35843;&#25972;&#36827;&#34892;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
AI tasks encompass a wide range of domains and fields. While numerous AI models have been designed for specific tasks and applications, they often require considerable human efforts in finding the right model architecture, optimization algorithm, and hyperparameters. Recent advances in large language models (LLMs) like ChatGPT show remarkable capabilities in various aspects of reasoning, comprehension, and interaction. Consequently, we propose developing task-oriented prompts and automatically utilizing LLMs to automate the training pipeline. To implement this concept, we present the AutoML-GPT, which employs GPT as the bridge to diverse AI models and dynamically trains models with optimized hyperparameters. AutoML-GPT dynamically takes user requests from the model and data cards and composes the corresponding prompt paragraph. Ultimately, with this prompt paragraph, AutoML-GPT will automatically conduct the experiments from data processing to model architecture, hyperparameter tuning,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;&#30340;&#22270;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#12290;&#36890;&#36807;&#28145;&#20837;&#25506;&#35752;&#29616;&#26377;&#26041;&#27861;&#30340;&#22522;&#26412;&#26426;&#21046;&#65292;&#25552;&#20986;&#20102;&#32479;&#19968;&#30340;&#22810;GNN&#21644;&#22686;&#24378;&#22270;&#23545;&#27604;&#26694;&#26550;MAG&#65292;&#24182;&#20174;&#20013;&#25552;&#21462;&#36731;&#37327;&#32423;&#23454;&#20363;L-MAG&#21644;M-MAG&#12290;</title><link>http://arxiv.org/abs/2305.02496</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;&#30340;&#22270;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Revisiting Graph Contrastive Learning for Anomaly Detection. (arXiv:2305.02496v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02496
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;&#30340;&#22270;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#12290;&#36890;&#36807;&#28145;&#20837;&#25506;&#35752;&#29616;&#26377;&#26041;&#27861;&#30340;&#22522;&#26412;&#26426;&#21046;&#65292;&#25552;&#20986;&#20102;&#32479;&#19968;&#30340;&#22810;GNN&#21644;&#22686;&#24378;&#22270;&#23545;&#27604;&#26694;&#26550;MAG&#65292;&#24182;&#20174;&#20013;&#25552;&#21462;&#36731;&#37327;&#32423;&#23454;&#20363;L-MAG&#21644;M-MAG&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#19982;&#23545;&#27604;&#23398;&#20064;&#30456;&#32467;&#21512;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#29616;&#26377;&#30340;&#22270;&#23545;&#27604;&#24322;&#24120;&#26816;&#27979;&#65288;GCAD&#65289;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#20110;&#36890;&#36807;&#22270;&#25193;&#20805;&#21644;&#22810;&#23610;&#24230;&#23545;&#27604;&#27169;&#22359;&#25913;&#21892;&#26816;&#27979;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22359;&#30340;&#22522;&#26412;&#26426;&#21046;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#30340;&#25506;&#32034;&#12290;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;&#22810;&#23610;&#24230;&#21644;&#22270;&#25193;&#20805;&#26426;&#21046;&#65292;&#24182;&#35266;&#23519;&#21040;&#22810;&#23610;&#24230;&#23545;&#27604;&#27169;&#22359;&#24182;&#27809;&#26377;&#22686;&#24378;&#34920;&#36798;&#65292;&#32780;&#22810;GNN&#27169;&#22359;&#26159;&#38544;&#34255;&#30340;&#36129;&#29486;&#32773;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24448;&#24448;&#23558;&#22810;GNN&#24102;&#26469;&#30340;&#25910;&#30410;&#24402;&#22240;&#20110;&#22810;&#23610;&#24230;&#27169;&#22359;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#36825;&#31181;&#35823;&#35299;&#65292;&#24182;&#25552;&#20986;&#20102;&#22810;GNN&#21644;&#22686;&#24378;&#22270;&#23545;&#27604;&#26694;&#26550;MAG&#65292;&#23558;&#29616;&#26377;&#30340;GCAD&#26041;&#27861;&#32479;&#19968;&#22312;&#23545;&#27604;&#33258;&#30417;&#30563;&#30340;&#35270;&#35282;&#19979;&#12290;&#25105;&#20204;&#20174;MAG&#26694;&#26550;&#20013;&#25552;&#21462;&#20102;&#20004;&#20010;&#21464;&#20307;&#65292;L-MAG&#21644;M-MAG&#12290;L-MAG&#26159;&#36731;&#37327;&#32423;&#30340;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Combining Graph neural networks (GNNs) with contrastive learning for anomaly detection has drawn rising attention recently. Existing graph contrastive anomaly detection (GCAD) methods have primarily focused on improving detection capability through graph augmentation and multi-scale contrast modules. However, the underlying mechanisms of how these modules work have not been fully explored. We dive into the multi-scale and graph augmentation mechanism and observed that multi-scale contrast modules do not enhance the expression, while the multi-GNN modules are the hidden contributors. Previous studies have tended to attribute the benefits brought by multi-GNN to the multi-scale modules. In the paper, we delve into the misconception and propose Multi-GNN and Augmented Graph contrastive framework MAG, which unified the existing GCAD methods in the contrastive self-supervised perspective. We extracted two variants from the MAG framework, L-MAG and M-MAG. The L-MAG is the lightweight instanc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35774;&#35745;&#32852;&#21512;&#24066;&#22330;&#20197;&#24212;&#23545;&#30005;&#21147;&#34892;&#19994;&#33073;&#30899;&#65292;&#23454;&#29616;&#30005;&#21147;&#31995;&#32479;&#30340;&#23433;&#20840;&#21644;&#32463;&#27982;&#25928;&#30410;&#65292;&#24182;&#20026;&#29615;&#22659;&#20570;&#20986;&#36129;&#29486;&#12290;&#35813;&#33539;&#22411;&#29702;&#35770;&#30340;&#26694;&#26550;&#23558;&#22312;&#20004;&#37096;&#20998;&#20013;&#35814;&#32454;&#20171;&#32461;&#12290;</title><link>http://arxiv.org/abs/2305.02485</link><description>&lt;p&gt;
&#22914;&#20309;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#20419;&#36827;&#26410;&#26469;&#30005;&#21147;&#24066;&#22330;&#35774;&#35745;&#65311;&#31532;&#19968;&#37096;&#20998;&#65306;&#33539;&#22411;&#29702;&#35770;&#12290;&#65288;arXiv:2305.02485v1 [cs.AI]&#65289;
&lt;/p&gt;
&lt;p&gt;
How to Use Reinforcement Learning to Facilitate Future Electricity Market Design? Part 1: A Paradigmatic Theory. (arXiv:2305.02485v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02485
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35774;&#35745;&#32852;&#21512;&#24066;&#22330;&#20197;&#24212;&#23545;&#30005;&#21147;&#34892;&#19994;&#33073;&#30899;&#65292;&#23454;&#29616;&#30005;&#21147;&#31995;&#32479;&#30340;&#23433;&#20840;&#21644;&#32463;&#27982;&#25928;&#30410;&#65292;&#24182;&#20026;&#29615;&#22659;&#20570;&#20986;&#36129;&#29486;&#12290;&#35813;&#33539;&#22411;&#29702;&#35770;&#30340;&#26694;&#26550;&#23558;&#22312;&#20004;&#37096;&#20998;&#20013;&#35814;&#32454;&#20171;&#32461;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#23545;&#30005;&#21147;&#34892;&#19994;&#33073;&#30899;&#30340;&#36843;&#20999;&#38656;&#27714;&#65292;&#37325;&#26032;&#35774;&#35745;&#30005;&#21147;&#24066;&#22330;&#26159;&#19968;&#31181;&#23439;&#35266;&#23618;&#38754;&#30340;&#26041;&#27861;&#65292;&#20197;&#36866;&#24212;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#39640;&#28183;&#36879;&#29575;&#65292;&#24182;&#23454;&#29616;&#30005;&#21147;&#31995;&#32479;&#30340;&#25805;&#20316;&#23433;&#20840;&#12289;&#32463;&#27982;&#25928;&#29575;&#21644;&#29615;&#22659;&#21451;&#22909;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24066;&#22330;&#35774;&#35745;&#26041;&#27861;&#23398;&#23384;&#22312;&#20110;&#33021;&#28304;&#29616;&#36135;&#24066;&#22330;&#65288;ESM&#65289;&#12289;&#36741;&#21161;&#26381;&#21153;&#24066;&#22330;&#65288;ASM&#65289;&#21644;&#37329;&#34701;&#24066;&#22330;&#65288;FM&#65289;&#20043;&#38388;&#21327;&#35843;&#19981;&#36275;&#65292;&#21363;&#8220;&#32852;&#21512;&#24066;&#22330;&#8221;&#65292;&#20197;&#21450;&#32570;&#20047;&#21487;&#38752;&#30340;&#22522;&#20110;&#27169;&#25311;&#30340;&#39564;&#35777;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#32570;&#38519;&#65292;&#26412;&#25991;&#23558;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#27169;&#25311;&#65292;&#24320;&#21457;&#32852;&#21512;&#24066;&#22330;&#35774;&#35745;&#30340;&#33539;&#22411;&#29702;&#35770;&#21644;&#35814;&#32454;&#26041;&#27861;&#12290;&#31532;&#19968;&#37096;&#20998;&#25552;&#20986;&#20102;&#36825;&#31181;&#26032;&#22411;&#24066;&#22330;&#35774;&#35745;&#21746;&#23398;&#30340;&#29702;&#35770;&#21644;&#26694;&#26550;&#12290;&#39318;&#20808;&#65292;&#24635;&#32467;&#20102;&#22312;&#35774;&#35745;&#32852;&#21512;&#24066;&#22330;&#26102;&#23384;&#22312;&#30340;&#26377;&#20105;&#35758;&#30340;&#24066;&#22330;&#35774;&#35745;&#36873;&#39033;&#20316;&#20026;&#30446;&#26631;&#30740;&#31350;&#38382;&#39064;&#12290;&#20854;&#27425;&#65292;&#25552;&#20986;&#20102;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In face of the pressing need of decarbonization in the power sector, the re-design of electricity market is necessary as a Marco-level approach to accommodate the high penetration of renewable generations, and to achieve power system operation security, economic efficiency, and environmental friendliness. However, existing market design methodologies suffer from the lack of coordination among energy spot market (ESM), ancillary service market (ASM) and financial market (FM), i.e., the "joint market", and the lack of reliable simulation-based verification. To tackle these deficiencies, this two-part paper develops a paradigmatic theory and detailed methods of the joint market design using reinforcement-learning (RL)-based simulation. In Part 1, the theory and framework of this novel market design philosophy are proposed. First, the controversial market design options while designing the joint market are summarized as the targeted research questions. Second, the Markov game model is deve
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22238;&#39038;&#21644;&#35752;&#35770;&#20102;&#26469;&#33258;&#19981;&#21516;&#28304;&#30340;&#26368;&#26032;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#20083;&#33146;&#30284;&#35786;&#26029;&#20013;&#30340;&#24212;&#29992;&#65292;&#20854;&#20013;&#21253;&#25324;&#28909;&#25104;&#20687;&#12289;&#32418;&#22806;&#28909;&#25104;&#20687;&#12289;&#30005;&#38459;&#25239;&#23618;&#26512;&#25104;&#20687;&#20197;&#21450;&#34880;&#28082;&#26816;&#27979;&#20013;&#21457;&#29616;&#30340;&#29983;&#29289;&#26631;&#24535;&#29289;&#65292;&#36825;&#20123;&#25216;&#26415;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#24555;&#12289;&#26356;&#21487;&#38752;&#21644;&#26356;&#20415;&#23452;&#65292;&#24182;&#19988;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#33021;&#22815;&#25552;&#39640;&#35786;&#26029;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.02482</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#20083;&#33146;&#30284;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Breast Cancer Diagnosis Using Machine Learning Techniques. (arXiv:2305.02482v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02482
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#21644;&#35752;&#35770;&#20102;&#26469;&#33258;&#19981;&#21516;&#28304;&#30340;&#26368;&#26032;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#20083;&#33146;&#30284;&#35786;&#26029;&#20013;&#30340;&#24212;&#29992;&#65292;&#20854;&#20013;&#21253;&#25324;&#28909;&#25104;&#20687;&#12289;&#32418;&#22806;&#28909;&#25104;&#20687;&#12289;&#30005;&#38459;&#25239;&#23618;&#26512;&#25104;&#20687;&#20197;&#21450;&#34880;&#28082;&#26816;&#27979;&#20013;&#21457;&#29616;&#30340;&#29983;&#29289;&#26631;&#24535;&#29289;&#65292;&#36825;&#20123;&#25216;&#26415;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#24555;&#12289;&#26356;&#21487;&#38752;&#21644;&#26356;&#20415;&#23452;&#65292;&#24182;&#19988;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#33021;&#22815;&#25552;&#39640;&#35786;&#26029;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20083;&#33146;&#30284;&#26159;&#22899;&#24615;&#29983;&#21629;&#20013;&#26368;&#20855;&#23041;&#32961;&#30340;&#30142;&#30149;&#20043;&#19968;&#65292;&#22240;&#27492;&#26089;&#26399;&#21644;&#20934;&#30830;&#30340;&#35786;&#26029;&#22312;&#20943;&#23569;&#24739;&#32773;&#27515;&#20129;&#39118;&#38505;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#26368;&#36817;&#35745;&#31639;&#24037;&#20855;&#12289;&#32418;&#22806;&#30456;&#26426;&#20197;&#21450;&#29983;&#29289;&#38459;&#25239;&#23450;&#37327;&#35774;&#22791;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#20026;&#20854;&#20182;&#21442;&#32771;&#25216;&#26415;&#30340;&#20986;&#29616;&#25552;&#20379;&#20102;&#26426;&#20250;&#65292;&#22914;&#28909;&#25104;&#20687;&#12289;&#32418;&#22806;&#28909;&#25104;&#20687;&#12289;&#30005;&#38459;&#25239;&#23618;&#26512;&#25104;&#20687;&#20197;&#21450;&#34880;&#28082;&#26816;&#27979;&#20013;&#21457;&#29616;&#30340;&#29983;&#29289;&#26631;&#24535;&#29289;&#65292;&#22240;&#27492;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#24555;&#12289;&#26356;&#21487;&#38752;&#21644;&#26356;&#20415;&#23452;&#12290;&#22312;&#36807;&#21435;&#30340;20&#24180;&#20013;&#65292;&#19978;&#36848;&#25216;&#26415;&#19968;&#30452;&#34987;&#35748;&#20026;&#26159;&#20083;&#33146;&#30284;&#35786;&#26029;&#30340;&#24182;&#34892;&#21644;&#25193;&#23637;&#26041;&#27861;&#65292;&#35768;&#22810;&#20316;&#32773;&#24471;&#20986;&#32467;&#35770;&#65292;&#20551;&#38451;&#24615;&#21644;&#20551;&#38452;&#24615;&#29575;&#26174;&#33879;&#38477;&#20302;&#12290;&#27492;&#22806;&#65292;&#24403;&#31579;&#26597;&#26041;&#27861;&#19982;&#20020;&#24202;&#35786;&#26029;&#19968;&#36215;&#20351;&#29992;&#26102;&#65292;&#21487;&#20197;&#25552;&#39640;&#35786;&#26029;&#30340;&#20934;&#30830;&#24615;&#12290;&#26412;&#25991;&#22238;&#39038;&#21644;&#35752;&#35770;&#20102;&#26469;&#33258;&#19981;&#21516;&#28304;&#30340;&#26368;&#26032;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#20083;&#33146;&#30284;&#35786;&#26029;&#20013;&#30340;&#24212;&#29992;&#65292;&#32771;&#34385;&#23427;&#20204;&#30340;&#24615;&#33021;&#12289;&#21487;&#29992;&#24615;&#21644;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Breast cancer is one of the most threatening diseases in women's life; thus, the early and accurate diagnosis plays a key role in reducing the risk of death in a patient's life. Mammography stands as the reference technique for breast cancer screening; nevertheless, many countries still lack access to mammograms due to economic, social, and cultural issues. Latest advances in computational tools, infrared cameras and devices for bio-impedance quantification, have given a chance to emerge other reference techniques like thermography, infrared thermography, electrical impedance tomography and biomarkers found in blood tests, therefore being faster, reliable and cheaper than other methods. In the last two decades, the techniques mentioned above have been considered as parallel and extended approaches for breast cancer diagnosis, as well many authors concluded that false positives and false negatives rates are significantly reduced. Moreover, when a screening method works together with a c
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#28023;&#20107;&#39046;&#22495;&#30340;&#27010;&#29575;&#30693;&#35782;&#22270;&#35889;&#30340;&#33258;&#21160;&#26500;&#24314;&#65292;&#20197;&#21033;&#29992;&#21547;&#26377;&#20016;&#23500;&#20449;&#24687;&#30340;&#26410;&#32467;&#26500;&#21270;&#36719;&#25968;&#25454;&#65292;&#24182;&#35299;&#20915;&#20102;&#36719;&#25968;&#25454;&#25552;&#21462;&#26041;&#38754;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.02471</link><description>&lt;p&gt;
&#38754;&#21521;&#28023;&#20107;&#39046;&#22495;&#27010;&#29575;&#30693;&#35782;&#22270;&#35889;&#30340;&#33258;&#21160;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
Toward the Automated Construction of Probabilistic Knowledge Graphs for the Maritime Domain. (arXiv:2305.02471v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02471
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#28023;&#20107;&#39046;&#22495;&#30340;&#27010;&#29575;&#30693;&#35782;&#22270;&#35889;&#30340;&#33258;&#21160;&#26500;&#24314;&#65292;&#20197;&#21033;&#29992;&#21547;&#26377;&#20016;&#23500;&#20449;&#24687;&#30340;&#26410;&#32467;&#26500;&#21270;&#36719;&#25968;&#25454;&#65292;&#24182;&#35299;&#20915;&#20102;&#36719;&#25968;&#25454;&#25552;&#21462;&#26041;&#38754;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22269;&#38469;&#28023;&#19978;&#29359;&#32618;&#21464;&#24471;&#36234;&#26469;&#36234;&#22797;&#26434;&#65292;&#24182;&#24120;&#19982;&#26356;&#24191;&#27867;&#30340;&#29359;&#32618;&#32593;&#32476;&#26377;&#20851;&#12290;&#20165;&#34701;&#21512;&#19982;&#29289;&#29702;&#31227;&#21160;&#30456;&#20851;&#30340;&#25968;&#25454;&#65288;&#21363;&#30001;&#29289;&#29702;&#20256;&#24863;&#22120;&#25110;&#30828;&#25968;&#25454;&#29983;&#25104;&#30340;&#25968;&#25454;&#65289;&#26469;&#26816;&#27979;&#28023;&#19978;&#23041;&#32961;&#26159;&#19981;&#22815;&#30340;&#12290;&#36825;&#23548;&#33268;&#20102;&#30740;&#31350;&#21644;&#24320;&#21457;&#24037;&#20316;&#65292;&#26088;&#22312;&#23558;&#30828;&#25968;&#25454;&#19982;&#20854;&#20182;&#31867;&#22411;&#30340;&#25968;&#25454;&#65288;&#29305;&#21035;&#26159;&#20154;&#24037;&#29983;&#25104;&#30340;&#36719;&#25968;&#25454;&#65289;&#30456;&#32467;&#21512;&#12290;&#29616;&#26377;&#30740;&#31350;&#36890;&#24120;&#20551;&#35774;&#36755;&#20837;&#30340;&#36719;&#25968;&#25454;&#20197;&#32467;&#26500;&#21270;&#26684;&#24335;&#25552;&#20379;&#65292;&#25110;&#32773;&#38598;&#20013;&#20110;&#25552;&#21462;&#26576;&#20123;&#30456;&#20851;&#23454;&#20307;&#25110;&#27010;&#24565;&#20197;&#37197;&#21512;&#25110;&#27880;&#37322;&#30828;&#25968;&#25454;&#12290;&#23545;&#20110;&#20174;&#26410;&#32467;&#26500;&#21270;&#26684;&#24335;&#65288;&#22914;&#24773;&#25253;&#25253;&#21578;&#21644;&#26032;&#38395;&#25991;&#31456;&#65289;&#20013;&#25552;&#21462;&#19982;&#24863;&#20852;&#36259;&#24773;&#20917;&#38544;&#21547;&#30456;&#20851;&#30340;&#20016;&#23500;&#20449;&#24687;&#65292;&#20851;&#27880;&#24230;&#35201;&#23569;&#24471;&#22810;&#12290;&#20026;&#20102;&#21033;&#29992;&#36825;&#20123;&#26469;&#28304;&#20013;&#28508;&#22312;&#26377;&#29992;&#21644;&#20016;&#23500;&#30340;&#20449;&#24687;&#65292;&#38656;&#35201;&#25552;&#21462;&#19981;&#20165;&#30456;&#20851;&#30340;&#23454;&#20307;&#21644;&#27010;&#24565;&#65292;&#36824;&#38656;&#35201;&#25552;&#21462;&#38544;&#21547;&#20110;&#22823;&#37327;&#36719;&#25968;&#25454;&#20013;&#30340;&#30456;&#20851;&#24773;&#22659;&#30340;&#20016;&#23500;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
International maritime crime is becoming increasingly sophisticated, often associated with wider criminal networks. Detecting maritime threats by means of fusing data purely related to physical movement (i.e., those generated by physical sensors, or hard data) is not sufficient. This has led to research and development efforts aimed at combining hard data with other types of data (especially human-generated or soft data). Existing work often assumes that input soft data is available in a structured format, or is focused on extracting certain relevant entities or concepts to accompany or annotate hard data. Much less attention has been given to extracting the rich knowledge about the situations of interest implicitly embedded in the large amount of soft data existing in unstructured formats (such as intelligence reports and news articles). In order to exploit the potentially useful and rich information from such sources, it is necessary to extract not only the relevant entities and conc
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#25552;&#39640;&#25506;&#27979;&#34892;&#26143;&#20449;&#21495;&#20998;&#31867;&#22120;&#24615;&#33021;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#22810;&#37325;&#24615;&#22686;&#24378;&#20998;&#31867;&#22120;&#65292;&#22522;&#20110;&#29616;&#26377;&#30340;&#20998;&#31867;&#22120;&#24182;&#20351;&#29992;&#22810;&#37325;&#24615;&#20449;&#24687;&#26469;&#39564;&#35777;69&#20010;&#26032;&#30340;&#31995;&#22806;&#34892;&#26143;&#12290;</title><link>http://arxiv.org/abs/2305.02470</link><description>&lt;p&gt;
&#25506;&#27979;&#34892;&#26143;&#20449;&#21495;&#30340;&#22810;&#37325;&#24615;&#22686;&#24378;&#20998;&#31867;&#22120;&#65306;&#20351;&#29992;ExoMiner&#30340;&#22810;&#37325;&#24615;&#22686;&#24378;&#39564;&#35777;69&#20010;&#26032;&#34892;&#26143;
&lt;/p&gt;
&lt;p&gt;
Multiplicity Boost Of Transit Signal Classifiers: Validation of 69 New Exoplanets Using The Multiplicity Boost of ExoMiner. (arXiv:2305.02470v1 [astro-ph.EP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02470
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#25552;&#39640;&#25506;&#27979;&#34892;&#26143;&#20449;&#21495;&#20998;&#31867;&#22120;&#24615;&#33021;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#22810;&#37325;&#24615;&#22686;&#24378;&#20998;&#31867;&#22120;&#65292;&#22522;&#20110;&#29616;&#26377;&#30340;&#20998;&#31867;&#22120;&#24182;&#20351;&#29992;&#22810;&#37325;&#24615;&#20449;&#24687;&#26469;&#39564;&#35777;69&#20010;&#26032;&#30340;&#31995;&#22806;&#34892;&#26143;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#24050;&#30693;&#30340;&#31995;&#22806;&#34892;&#26143;&#26159;&#36890;&#36807;&#39564;&#35777;&#25216;&#26415;&#32780;&#19981;&#26159;&#36890;&#36807;&#34917;&#20805;&#35266;&#27979;&#36827;&#34892;&#30830;&#35748;&#30340;&#12290;&#36825;&#20123;&#25216;&#26415;&#29983;&#25104;&#30340;&#20998;&#25968;&#36890;&#24120;&#20195;&#34920;&#20102;&#26377;&#20851;&#20449;&#21495;&#30340;&#26576;&#20123;&#20449;&#24687;&#65288;&#29992;x&#34920;&#31034;&#65289;&#32473;&#20986;&#30340;&#25506;&#27979;&#34892;&#26143;&#20449;&#21495;&#30340;&#27010;&#29575;&#65288;y&#65288;x&#65289;=&#34892;&#26143;&#65289;&#12290;&#36825;&#39033;&#24037;&#20316;&#24341;&#20837;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#21363;&#22312;&#32473;&#23450;&#30340;&#25506;&#27979;&#34892;&#26143;&#20449;&#21495;&#30830;&#35748;&#22120;&#30340;&#22522;&#30784;&#19978;&#65292;&#21033;&#29992;&#22810;&#37325;&#24615;&#20449;&#24687;&#25913;&#21892;&#20854;&#24615;&#33021;&#12290;&#25105;&#20204;&#23558;&#27492;&#26694;&#26550;&#24212;&#29992;&#20110;&#20960;&#20010;&#29616;&#26377;&#30340;&#20998;&#31867;&#22120;&#65292;&#21253;&#25324;vespa&#65288;Morton&#31561;&#20154;2016&#65289;&#12289;Robovetter&#65288;Coughlin&#31561;&#20154;2017&#65289;&#12289;AstroNet&#65288;Shallue&#21644;Vanderburg 2018&#65289;&#12289;ExoNet&#65288;Ansdel&#31561;&#20154;2018&#65289;&#12289;GPC&#21644;RFC&#65288;Armstrong&#31561;&#20154;2020&#65289;&#20197;&#21450;ExoMiner&#65288;Valizadegan&#31561;&#20154;2022&#65289;&#65292;&#20197;&#25903;&#25345;&#25105;&#20204;&#30340;&#20998;&#31867;&#32467;&#26524;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most existing exoplanets are discovered using validation techniques rather than being confirmed by complementary observations. These techniques generate a score that is typically the probability of the transit signal being an exoplanet (y(x)=exoplanet) given some information related to that signal (represented by x). Except for the validation technique in Rowe et al. (2014) that uses multiplicity information to generate these probability scores, the existing validation techniques ignore the multiplicity boost information. In this work, we introduce a framework with the following premise: given an existing transit signal vetter (classifier), improve its performance using multiplicity information. We apply this framework to several existing classifiers, which include vespa (Morton et al. 2016), Robovetter (Coughlin et al. 2017), AstroNet (Shallue &amp; Vanderburg 2018), ExoNet (Ansdel et al. 2018), GPC and RFC (Armstrong et al. 2020), and ExoMiner (Valizadegan et al. 2022), to support our cl
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25506;&#35752;&#20102;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;AI&#31995;&#32479;&#24212;&#35813;&#26377;&#38754;&#26495;&#20197;&#25552;&#39640;&#20854;&#21487;&#29992;&#24615;&#21644;&#23433;&#20840;&#24615;&#65292;&#24182;&#19988;&#30028;&#38754;&#24212;&#35813;&#20855;&#26377;&#22522;&#20110;&#31995;&#32479;&#27169;&#22411;&#21644;&#29992;&#25143;&#27169;&#22411;&#29366;&#24577;&#30340;&#24182;&#34892;&#26174;&#31034;&#12290;</title><link>http://arxiv.org/abs/2305.02469</link><description>&lt;p&gt;
&#31995;&#32479;&#27169;&#22411;&#21644;&#29992;&#25143;&#27169;&#22411;&#65306;&#25506;&#32034;&#20154;&#24037;&#26234;&#33021;&#38754;&#26495;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
The System Model and the User Model: Exploring AI Dashboard Design. (arXiv:2305.02469v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02469
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25506;&#35752;&#20102;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;AI&#31995;&#32479;&#24212;&#35813;&#26377;&#38754;&#26495;&#20197;&#25552;&#39640;&#20854;&#21487;&#29992;&#24615;&#21644;&#23433;&#20840;&#24615;&#65292;&#24182;&#19988;&#30028;&#38754;&#24212;&#35813;&#20855;&#26377;&#22522;&#20110;&#31995;&#32479;&#27169;&#22411;&#21644;&#29992;&#25143;&#27169;&#22411;&#29366;&#24577;&#30340;&#24182;&#34892;&#26174;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#26159;&#19968;&#31687;&#20851;&#20110;&#30028;&#38754;&#35774;&#35745;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#25512;&#27979;&#24615;&#35770;&#25991;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#21253;&#25324;&#34987;&#25253;&#36947;&#30340;&#19981;&#33391;&#20132;&#20114;&#12290;&#25105;&#20204;&#35748;&#20026;&#38382;&#39064;&#30340;&#37096;&#20998;&#21407;&#22240;&#26159;&#25991;&#26412;&#24182;&#19981;&#26159;&#25152;&#26377;&#20320;&#38656;&#35201;&#30340;&#19996;&#35199;&#65306;&#22797;&#26434;&#30340;AI&#31995;&#32479;&#24212;&#35813;&#26377;&#38754;&#26495;&#65292;&#23601;&#20687;&#25152;&#26377;&#20854;&#20182;&#22797;&#26434;&#30340;&#35774;&#22791;&#19968;&#26679;&#12290;&#20551;&#35774;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;AI&#31995;&#32479;&#23558;&#21253;&#21547;&#21487;&#35299;&#37322;&#30340;&#21608;&#22260;&#19990;&#30028;&#26041;&#38754;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#35752;&#35770;&#36825;&#20123;&#38754;&#26495;&#21487;&#33021;&#26174;&#31034;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#25512;&#27979;&#65292;&#23545;&#20110;&#35768;&#22810;&#31995;&#32479;&#26469;&#35828;&#65292;&#26368;&#37325;&#35201;&#30340;&#27169;&#22411;&#23558;&#26159;&#29992;&#25143;&#27169;&#22411;&#21644;&#31995;&#32479;&#27169;&#22411;&#12290;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#31995;&#32479;&#27169;&#22411;&#8221;&#21644;&#8220;&#29992;&#25143;&#27169;&#22411;&#8221;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#20026;&#20102;&#21487;&#29992;&#24615;&#21644;&#23433;&#20840;&#24615;&#65292;&#38754;&#21521;&#22522;&#20110;&#23545;&#35805;&#30340;AI&#31995;&#32479;&#30340;&#30028;&#38754;&#24212;&#35813;&#20855;&#26377;&#22522;&#20110;&#31995;&#32479;&#27169;&#22411;&#21644;&#29992;&#25143;&#27169;&#22411;&#29366;&#24577;&#30340;&#24182;&#34892;&#26174;&#31034;&#12290;&#25214;&#21040;&#35782;&#21035;&#12289;&#35299;&#37322;&#21644;&#26174;&#31034;&#36825;&#20004;&#20010;&#27169;&#22411;&#30340;&#26041;&#27861;&#24212;&#35813;&#26159;&#30028;&#38754;&#30740;&#31350;&#30340;&#26680;&#24515;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
This is a speculative essay on interface design and artificial intelligence. Recently there has been a surge of attention to chatbots based on large language models, including widely reported unsavory interactions. We contend that part of the problem is that text is not all you need: sophisticated AI systems should have dashboards, just like all other complicated devices. Assuming the hypothesis that AI systems based on neural networks will contain interpretable models of aspects of the world around them, we discuss what data such dashboards might display. We conjecture that, for many systems, the two most important models will be of the user and of the system itself. We call these the System Model and User Model. We argue that, for usability and safety, interfaces to dialogue-based AI systems should have a parallel display based on the state of the System Model and the User Model. Finding ways to identify, interpret, and display these two models should be a core part of interface rese
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#8212;&#8212;&#35760;&#25968;&#22120;&#31034;&#20363;&#24341;&#23548;&#30340;&#31934;&#28860;&#25277;&#35937;(CEGAR)&#65292;&#29992;&#20110;&#35299;&#20915;&#24067;&#23572;&#32593;&#32476;&#30340;&#26368;&#23567;&#38519;&#38449;&#31354;&#38388;(MTSs)&#30340;&#36890;&#29992;&#23646;&#24615;&#30340;&#36923;&#36753;&#25512;&#29702;&#38382;&#39064;&#65292;&#21516;&#26102;&#21487;&#29992;&#20110;&#35782;&#21035;&#22312;&#25152;&#26377;MTSs&#19978;&#25191;&#34892;&#32473;&#23450;&#23646;&#24615;&#30340;&#24067;&#23572;&#21464;&#37327;&#30340;&#27704;&#20037;&#20923;&#32467;&#30340;&#37325;&#32534;&#31243;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.02442</link><description>&lt;p&gt;
&#22788;&#29702;&#24067;&#23572;&#32593;&#32476;&#30340;&#26368;&#23567;&#38519;&#38449;&#31354;&#38388;&#30340;&#36890;&#29992;&#23646;&#24615;
&lt;/p&gt;
&lt;p&gt;
Tackling Universal Properties of Minimal Trap Spaces of Boolean Networks. (arXiv:2305.02442v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02442
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#8212;&#8212;&#35760;&#25968;&#22120;&#31034;&#20363;&#24341;&#23548;&#30340;&#31934;&#28860;&#25277;&#35937;(CEGAR)&#65292;&#29992;&#20110;&#35299;&#20915;&#24067;&#23572;&#32593;&#32476;&#30340;&#26368;&#23567;&#38519;&#38449;&#31354;&#38388;(MTSs)&#30340;&#36890;&#29992;&#23646;&#24615;&#30340;&#36923;&#36753;&#25512;&#29702;&#38382;&#39064;&#65292;&#21516;&#26102;&#21487;&#29992;&#20110;&#35782;&#21035;&#22312;&#25152;&#26377;MTSs&#19978;&#25191;&#34892;&#32473;&#23450;&#23646;&#24615;&#30340;&#24067;&#23572;&#21464;&#37327;&#30340;&#27704;&#20037;&#20923;&#32467;&#30340;&#37325;&#32534;&#31243;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#23567;&#38519;&#38449;&#31354;&#38388;(MTSs)&#25429;&#25417;&#24067;&#23572;&#21160;&#24577;&#34987;&#22256;&#30340;&#23376;&#31354;&#38388;&#65292;&#26080;&#35770;&#26356;&#26032;&#27169;&#24335;&#22914;&#20309;&#65292;&#23427;&#20204;&#37117;&#23545;&#24212;&#20110;&#26368;&#35753;&#20154;&#28385;&#24847;&#30340;&#27169;&#24335;&#30340;&#21560;&#24341;&#23376;&#12290;&#30001;&#20110;&#20854;&#22810;&#21151;&#33021;&#24615;&#65292;&#36817;&#24180;&#26469;&#35745;&#31639;MTSs&#24050;&#32463;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#65292;&#20027;&#35201;&#26159;&#36890;&#36807;&#37325;&#28857;&#20851;&#27880;&#20854;&#26522;&#20030;&#26469;&#23454;&#29616;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;MTS&#30340;&#36890;&#29992;&#23646;&#24615;&#30340;&#36923;&#36753;&#25512;&#29702;&#65292;&#24182;&#35299;&#20915;&#20102;&#20004;&#20010;&#38382;&#39064;&#65306;&#29992;&#20110;&#35782;&#21035;&#22312;&#25152;&#26377;MTSs&#19978;&#25191;&#34892;&#32473;&#23450;&#23646;&#24615;&#30340;&#24067;&#23572;&#21464;&#37327;&#30340;&#27704;&#20037;&#20923;&#32467;&#30340;&#37325;&#32534;&#31243;&#38382;&#39064;&#65292;&#24182;&#20174;&#20854;MTSs&#30340;&#36890;&#29992;&#23646;&#24615;&#21512;&#25104;&#24067;&#23572;&#32593;&#32476;&#12290;&#36825;&#20004;&#20010;&#38382;&#39064;&#37117;&#24402;&#32467;&#20026;&#35299;&#20915;&#20855;&#26377;3&#20010;&#37327;&#21270;&#22120;($\exists\forall\exists$)&#30340;&#21629;&#39064;&#36923;&#36753;&#20844;&#24335;&#30340;&#21487;&#28385;&#36275;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#35760;&#25968;&#22120;&#31034;&#20363;&#24341;&#23548;&#30340;&#31934;&#28860;&#25277;&#35937;(CEGAR)&#26469;&#36890;&#36807;&#32806;&#21512;&#35299;&#20915;&#20004;&#20010;&#26356;&#31616;&#21333;&#30340;&#20844;&#24335;&#26469;&#26377;&#25928;&#22320;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#21407;&#22411;&#65292;&#20381;&#36182;&#20110;&#31572;&#26696;&#38598;&#32534;&#31243;&#26469;&#35828;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Minimal trap spaces (MTSs) capture subspaces in which the Boolean dynamics is trapped, whatever the update mode. They correspond to the attractors of the most permissive mode. Due to their versatility, the computation of MTSs has recently gained traction, essentially by focusing on their enumeration. In this paper, we address the logical reasoning on universal properties of MTSs in the scope of two problems: the reprogramming of Boolean networks for identifying the permanent freeze of Boolean variables that enforce a given property on all the MTSs, and the synthesis of Boolean networks from universal properties on their MTSs. Both problems reduce to solving the satisfiability of quantified propositional logic formula with 3 levels of quantifiers ($\exists\forall\exists$). In this paper, we introduce a Counter-Example Guided Refinement Abstraction (CEGAR) to efficiently solve these problems by coupling the resolution of two simpler formulas. We provide a prototype relying on Answer-Set 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26816;&#32034;&#22686;&#24378;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;Selfmem&#65292;&#36890;&#36807;&#36845;&#20195;&#29983;&#25104;&#33258;&#25105;&#35760;&#24518;&#27744;&#24182;&#37319;&#29992;&#35760;&#24518;&#36873;&#25321;&#22120;&#65292;&#20351;&#26816;&#32034;&#26356;&#21152;&#33258;&#36866;&#24212;&#65292;&#25552;&#39640;&#20102;&#25991;&#26412;&#29983;&#25104;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.02437</link><description>&lt;p&gt;
&#36816;&#29992;&#33258;&#25105;&#35760;&#24518;&#30340;&#26816;&#32034;&#22686;&#24378;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Lift Yourself Up: Retrieval-augmented Text Generation with Self Memory. (arXiv:2305.02437v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02437
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26816;&#32034;&#22686;&#24378;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;Selfmem&#65292;&#36890;&#36807;&#36845;&#20195;&#29983;&#25104;&#33258;&#25105;&#35760;&#24518;&#27744;&#24182;&#37319;&#29992;&#35760;&#24518;&#36873;&#25321;&#22120;&#65292;&#20351;&#26816;&#32034;&#26356;&#21152;&#33258;&#36866;&#24212;&#65292;&#25552;&#39640;&#20102;&#25991;&#26412;&#29983;&#25104;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#36739;&#20110;&#20256;&#32479;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#65292;&#26816;&#32034;&#22686;&#24378;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#33021;&#22815;&#30452;&#25509;&#36845;&#20195;&#20154;&#31867;&#32534;&#20889;&#30340;&#21442;&#32771;&#24211;&#65292;&#24182;&#20174;&#20013;&#26816;&#32034;&#20986;&#30456;&#24212;&#30340;&#20449;&#24687;&#65292;&#20197;&#29983;&#25104;&#26356;&#20248;&#36136;&#30340;&#25991;&#26412;&#12290;&#20294;&#24403;&#21069;&#25991;&#29486;&#23384;&#22312;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#26816;&#32034;&#21040;&#30340;&#35760;&#24518;&#26469;&#33258;&#20110;&#22266;&#23450;&#30340;&#35821;&#26009;&#24211;&#65292;&#20854;&#36136;&#37327;&#23384;&#22312;&#19968;&#23450;&#23616;&#38480;&#24615;&#65292;&#21487;&#33021;&#20250;&#38480;&#21046;&#35760;&#24518;&#22686;&#24378;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;Selfmem&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#36845;&#20195;&#22320;&#37319;&#29992;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#22120;&#33258;&#36523;&#20197;&#29983;&#25104;&#26080;&#38480;&#21046;&#30340;&#33258;&#25105;&#35760;&#24518;&#27744;&#65292;&#24182;&#20351;&#29992;&#35760;&#24518;&#36873;&#25321;&#22120;&#20026;&#19979;&#19968;&#36718;&#29983;&#25104;&#36873;&#25321;&#19968;&#20010;&#29983;&#25104;&#30340;&#35760;&#24518;&#12290;&#30456;&#32467;&#21512;&#65292;&#36825;&#20004;&#20010;&#20027;&#35201;&#38382;&#39064;&#25552;&#20986;&#20102;&#36816;&#29992;&#33258;&#25105;&#35760;&#24518;&#30340;&#26816;&#32034;&#22686;&#24378;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
With direct access to human-written reference as memory, retrieval-augmented generation has achieved much progress in a wide range of text generation tasks. Since better memory would typically prompt better generation~(we define this as primal problem), previous works mainly focus on how to retrieve better memory. However, one fundamental limitation exists for current literature: the memory is retrieved from a fixed corpus and is bounded by the quality of the corpus. Due to the finite retrieval space, bounded memory would greatly limit the potential of the memory-augmented generation model. In this paper, by exploring the duality of the primal problem: better generation also prompts better memory, we propose a framework called Selfmem, which iteratively adopts a retrieval-augmented generator itself to generate an unbounded memory pool and uses a memory selector to pick one generated memory for the next generation round. By combining the primal and dual problem, a retrieval-augmented ge
&lt;/p&gt;</description></item><item><title>PTP&#31639;&#27861;&#24341;&#20837;&#22522;&#20110;&#25200;&#21160;&#30340;&#27491;&#21017;&#21270;&#22120;&#26469;&#24179;&#28369;loss&#22270;&#20687;&#65292;&#25552;&#21319;prompt tuning&#24615;&#33021;&#21644;&#31283;&#23450;&#24615;&#65292;&#22312;&#22235;&#20010;&#27979;&#35797;&#25968;&#25454;&#38598;&#20013;&#33719;&#24471;&#20102;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.02423</link><description>&lt;p&gt;
PTP&#65306;&#21033;&#29992;&#22522;&#20110;&#25200;&#21160;&#30340;&#27491;&#21017;&#21270;&#22120;&#25552;&#21319;Prompt Tuning&#30340;&#31283;&#23450;&#24615;&#21644;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
PTP: Boosting Stability and Performance of Prompt Tuning with Perturbation-Based Regularizer. (arXiv:2305.02423v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02423
&lt;/p&gt;
&lt;p&gt;
PTP&#31639;&#27861;&#24341;&#20837;&#22522;&#20110;&#25200;&#21160;&#30340;&#27491;&#21017;&#21270;&#22120;&#26469;&#24179;&#28369;loss&#22270;&#20687;&#65292;&#25552;&#21319;prompt tuning&#24615;&#33021;&#21644;&#31283;&#23450;&#24615;&#65292;&#22312;&#22235;&#20010;&#27979;&#35797;&#25968;&#25454;&#38598;&#20013;&#33719;&#24471;&#20102;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#19979;&#28216;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#19978;&#65292;&#20351;&#29992;prompt tuning&#27604;&#24494;&#35843;&#26041;&#27861;&#26356;&#33021;&#21457;&#25381;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21147;&#37327;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;prompt tuning&#26041;&#27861;&#23384;&#22312;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#22240;&#20026;&#19981;&#21516;&#38543;&#26426;&#31181;&#23376;&#19979;&#30340;&#20998;&#25968;&#26041;&#24046;&#30456;&#24403;&#22823;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#35843;&#26597;&#24182;&#21457;&#29616;&#65292;&#26222;&#36890;&#30340;prompt tuning&#30340;&#25439;&#22833;&#20989;&#25968;&#22270;&#20687;&#22312;&#21487;&#35270;&#21270;&#26102;&#21576;&#23789;&#22721;&#29366;&#65292;&#36755;&#20837;&#25968;&#25454;&#30340;&#24494;&#23567;&#21464;&#21270;&#21487;&#20197;&#23548;&#33268;&#25439;&#22833;&#20989;&#25968;&#22270;&#20687;&#30340;&#21095;&#28872;&#27874;&#21160;&#12290;&#36825;&#26159;&#23548;&#33268;prompt tuning&#19981;&#31283;&#23450;&#24615;&#30340;&#19968;&#20010;&#37325;&#35201;&#22240;&#32032;&#12290;&#22522;&#20110;&#36825;&#20010;&#35266;&#23519;&#32467;&#26524;&#65292;&#25105;&#20204;&#23558;&#24179;&#28369;&#25439;&#22833;&#20989;&#25968;&#22270;&#20687;&#30340;&#22522;&#20110;&#25200;&#21160;&#30340;&#27491;&#21017;&#21270;&#22120;&#24341;&#20837;&#21040;prompt tuning&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PTP&#30340;&#26032;&#31639;&#27861;&#65292;&#23427;&#19981;&#20165;&#21487;&#20197;&#26174;&#33879;&#20943;&#36731;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#65292;&#36824;&#21487;&#20197;&#25552;&#39640;prompt tuning&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#31181;&#22522;&#20110;&#25200;&#21160;&#30340;&#27491;&#21017;&#21270;&#22120;&#65292;&#24182;&#22312;&#22235;&#20010;&#21463;&#27426;&#36814;&#30340;NLU&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;PTP&#22312;&#36229;&#32423;GLUE&#21644;GLUE&#19978;&#20998;&#21035;&#33719;&#24471;&#20102;&#39640;&#36798;3.9&#65285;&#21644;2.0&#65285;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#21487;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;prompt tuning&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;PTP&#36824;&#21487;&#20197;&#25552;&#39640;prompt tuning&#30340;&#40065;&#26834;&#24615;&#65292;&#20351;&#22810;&#27425;&#36816;&#34892;&#33719;&#24471;&#30340;&#24615;&#33021;&#26631;&#20934;&#20559;&#24046;&#26356;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies show that prompt tuning can better leverage the power of large language models than fine-tuning on downstream natural language understanding tasks. However, the existing prompt tuning methods have training instability issues, as the variance of scores under different random seeds is quite large. To address this critical problem, we first investigate and find that the loss landscape of vanilla prompt tuning is precipitous when it is visualized, where a slight change of input data can cause a big fluctuation in the loss landscape. This is an essential factor that leads to the instability of prompt tuning. Based on this observation, we introduce perturbation-based regularizers, which can smooth the loss landscape, into prompt tuning. We propose a new algorithm, called Prompt Tuning with Perturbation-based regularizer~(PTP), which can not only alleviate training instability dramatically but also boost the performance of prompt tuning. We design two kinds of perturbation-base
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Plan&#65292;Eliminate&#65292;&#21644;Track&#65288;PET&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24110;&#21161;&#26234;&#33021;&#20307;&#31616;&#21270;&#25511;&#21046;&#20219;&#21153;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;LLM&#30452;&#25509;&#20316;&#20026;&#26234;&#33021;&#20307;&#25152;&#38754;&#20020;&#30340;&#19968;&#20123;&#38480;&#21046;&#21644;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.02412</link><description>&lt;p&gt;
&#35745;&#21010;&#12289;&#28040;&#38500;&#21644;&#36319;&#36394;&#8212;&#8212;&#35821;&#35328;&#27169;&#22411;&#26159;&#20855;&#22791;&#20307;&#39564;&#30340;&#26234;&#33021;&#20307;&#30340;&#33391;&#24072;&#30410;&#21451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Plan, Eliminate, and Track -- Language Models are Good Teachers for Embodied Agents. (arXiv:2305.02412v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02412
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Plan&#65292;Eliminate&#65292;&#21644;Track&#65288;PET&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24110;&#21161;&#26234;&#33021;&#20307;&#31616;&#21270;&#25511;&#21046;&#20219;&#21153;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;LLM&#30452;&#25509;&#20316;&#20026;&#26234;&#33021;&#20307;&#25152;&#38754;&#20020;&#30340;&#19968;&#20123;&#38480;&#21046;&#21644;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21487;&#20197;&#25429;&#25417;&#21040;&#20851;&#20110;&#19990;&#30028;&#30340;&#31243;&#24207;&#21270;&#30693;&#35782;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#21033;&#29992;LLM&#20135;&#29983;&#30340;&#25277;&#35937;&#35745;&#21010;&#26469;&#31616;&#21270;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25511;&#21046;&#20219;&#21153;&#65292;&#36890;&#36807;&#21160;&#20316;&#25171;&#20998;&#25110;&#21160;&#20316;&#24314;&#27169;&#65288;&#24494;&#35843;&#65289;&#26469;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#21464;&#21387;&#22120;&#26550;&#26500;&#32487;&#25215;&#20102;&#20960;&#20010;&#38480;&#21046;&#65292;&#20351;&#24471;LLM&#38590;&#20197;&#30452;&#25509;&#20316;&#20026;&#26234;&#33021;&#20307;&#65306;&#20363;&#22914;&#26377;&#38480;&#30340;&#36755;&#20837;&#38271;&#24230;&#65292;&#24494;&#35843;&#30340;&#25928;&#29575;&#65292;&#39044;&#35757;&#32451;&#30340;&#20559;&#35265;&#20197;&#21450;&#19982;&#38750;&#25991;&#26412;&#29615;&#22659;&#30340;&#19981;&#20860;&#23481;&#24615;&#12290;&#20026;&#20102;&#19982;&#20302;&#32423;&#21035;&#21487;&#35757;&#32451;&#30340;&#25191;&#34892;&#22120;&#20445;&#25345;&#20860;&#23481;&#24615;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;LLMs&#20013;&#30340;&#30693;&#35782;&#26469;&#31616;&#21270;&#25511;&#21046;&#38382;&#39064;&#65292;&#32780;&#19981;&#26159;&#35299;&#20915;&#38382;&#39064;&#12290; &#25105;&#20204;&#25552;&#20986;&#20102;Plan&#65292;Eliminate&#21644;Track&#65288;PET&#65289;&#26694;&#26550;&#12290;&#35745;&#21010;&#27169;&#22359;&#23558;&#20219;&#21153;&#25551;&#36848;&#36716;&#21270;&#20026;&#39640;&#23618;&#27425;&#23376;&#20219;&#21153;&#30340;&#21015;&#34920;&#12290;&#28040;&#38500;&#27169;&#22359;&#20174;&#24403;&#21069;&#23376;&#20219;&#21153;&#30340;&#35266;&#23519;&#20013;&#23631;&#34109;&#19981;&#30456;&#20851;&#30340;&#23545;&#35937;&#21644;&#23481;&#22120;&#12290;&#26368;&#21518;&#65292;&#36319;&#36394;&#27169;&#22359;&#30830;&#23450;&#26234;&#33021;&#20307;&#26159;&#21542;&#24050;&#32463;&#23454;&#29616;&#20102;&#24403;&#21069;&#23376;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained large language models (LLMs) capture procedural knowledge about the world. Recent work has leveraged LLM's ability to generate abstract plans to simplify challenging control tasks, either by action scoring, or action modeling (fine-tuning). However, the transformer architecture inherits several constraints that make it difficult for the LLM to directly serve as the agent: e.g. limited input lengths, fine-tuning inefficiency, bias from pre-training, and incompatibility with non-text environments. To maintain compatibility with a low-level trainable actor, we propose to instead use the knowledge in LLMs to simplify the control problem, rather than solving it. We propose the Plan, Eliminate, and Track (PET) framework. The Plan module translates a task description into a list of high-level sub-tasks. The Eliminate module masks out irrelevant objects and receptacles from the observation for the current sub-task. Finally, the Track module determines whether the agent has accompli
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#19982;&#29305;&#24449;&#36873;&#25321;&#31574;&#30053;&#30456;&#32467;&#21512;&#30340;&#28151;&#21512;&#26694;&#26550;&#65292;&#20197;&#38477;&#20302;&#24694;&#24847;&#36719;&#20214;&#20998;&#31867;&#22120;&#22521;&#35757;&#26102;&#38388;&#65292;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#22312;&#27169;&#25311;&#22120;&#19978;&#21487;&#20197;&#36798;&#21040;78.91&#65285;&#30340;&#27979;&#35797;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.02396</link><description>&lt;p&gt;
&#29305;&#24449;&#24037;&#31243;&#33021;&#24110;&#21161;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Feature Engineering Help Quantum Machine Learning for Malware Detection?. (arXiv:2305.02396v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02396
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#19982;&#29305;&#24449;&#36873;&#25321;&#31574;&#30053;&#30456;&#32467;&#21512;&#30340;&#28151;&#21512;&#26694;&#26550;&#65292;&#20197;&#38477;&#20302;&#24694;&#24847;&#36719;&#20214;&#20998;&#31867;&#22120;&#22521;&#35757;&#26102;&#38388;&#65292;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#22312;&#27169;&#25311;&#22120;&#19978;&#21487;&#20197;&#36798;&#21040;78.91&#65285;&#30340;&#27979;&#35797;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#24694;&#24847;&#36719;&#20214;&#25915;&#20987;&#25968;&#37327;&#21644;&#22797;&#26434;&#24230;&#30340;&#22686;&#21152;&#65292;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#31995;&#32479;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#21516;&#26102;&#65292;&#35768;&#22810;&#29992;&#20110;&#24694;&#24847;&#36719;&#20214;&#20998;&#31867;&#30340;&#27969;&#34892;ML&#27169;&#22411;&#37117;&#26159;&#26377;&#30417;&#30563;&#23398;&#20064;&#12290;&#36825;&#20123;&#26377;&#30417;&#30563;&#20998;&#31867;&#22120;&#36890;&#24120;&#23545;&#26032;&#22411;&#24694;&#24847;&#36719;&#20214;&#30340;&#25512;&#24191;&#25928;&#26524;&#19981;&#22909;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#32463;&#24120;&#37325;&#26032;&#35757;&#32451;&#23427;&#20204;&#20197;&#26816;&#27979;&#26032;&#30340;&#24694;&#24847;&#36719;&#20214;&#26679;&#26412;&#65292;&#36825;&#21487;&#33021;&#38750;&#24120;&#32791;&#26102;&#12290;&#26412;&#25991;&#36890;&#36807;&#29702;&#35770;&#37327;&#23376;ML&#19982;&#29305;&#24449;&#36873;&#25321;&#31574;&#30053;&#30456;&#32467;&#21512;&#30340;&#28151;&#21512;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20197;&#38477;&#20302;&#25968;&#25454;&#22823;&#23567;&#21644;&#24694;&#24847;&#36719;&#20214;&#20998;&#31867;&#22120;&#22521;&#35757;&#26102;&#38388;&#12290;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;XGBoost&#36873;&#25321;&#30340;&#29305;&#24449;&#30340;VQC&#22312;&#27169;&#25311;&#22120;&#19978;&#21487;&#20197;&#33719;&#24471;78.91&#65285;&#30340;&#27979;&#35797;&#20934;&#30830;&#24615;&#12290;&#20351;&#29992;XGBoost&#36873;&#25321;&#30340;&#29305;&#24449;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;IBM 5 qubits&#26426;&#22120;&#19978;&#30340;&#24179;&#22343;&#20934;&#30830;&#24615;&#20026;74&#65285;&#65288;+-11.35&#65285;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increasing number and sophistication of malware attacks, malware detection systems based on machine learning (ML) grow in importance. At the same time, many popular ML models used in malware classification are supervised solutions. These supervised classifiers often do not generalize well to novel malware. Therefore, they need to be re-trained frequently to detect new malware specimens, which can be time-consuming. Our work addresses this problem in a hybrid framework of theoretical Quantum ML, combined with feature selection strategies to reduce the data size and malware classifier training time. The preliminary results show that VQC with XGBoost selected features can get a 78.91% test accuracy on the simulator. The average accuracy for the model trained using the features selected with XGBoost was 74% (+- 11.35%) on the IBM 5 qubits machines.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24402;&#22240;&#30340;&#31649;&#36947;AttDef&#65292;&#29992;&#20110;&#38450;&#24481;&#20004;&#31181;&#25554;&#20837;&#24335;&#27745;&#26579;&#25915;&#20987;BadNL&#21644;InSent&#65292;&#35813;&#31649;&#36947;&#21487;&#20197;&#25104;&#21151;&#32531;&#35299;&#25554;&#20837;&#24335;&#25991;&#26412;&#21518;&#38376;&#25915;&#20987;&#24182;&#22312;&#22235;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#24179;&#22343;&#25552;&#39640;&#20102;56.59%&#33267;79.97%&#21644;15.25%&#33267;48.34%&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.02394</link><description>&lt;p&gt;
&#22522;&#20110;&#24402;&#22240;&#30340;&#38450;&#24481;&#25554;&#20837;&#24335;&#25991;&#26412;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Defending against Insertion-based Textual Backdoor Attacks via Attribution. (arXiv:2305.02394v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02394
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24402;&#22240;&#30340;&#31649;&#36947;AttDef&#65292;&#29992;&#20110;&#38450;&#24481;&#20004;&#31181;&#25554;&#20837;&#24335;&#27745;&#26579;&#25915;&#20987;BadNL&#21644;InSent&#65292;&#35813;&#31649;&#36947;&#21487;&#20197;&#25104;&#21151;&#32531;&#35299;&#25554;&#20837;&#24335;&#25991;&#26412;&#21518;&#38376;&#25915;&#20987;&#24182;&#22312;&#22235;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#24179;&#22343;&#25552;&#39640;&#20102;56.59%&#33267;79.97%&#21644;15.25%&#33267;48.34%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21518;&#38376;&#25915;&#20987;&#26159;&#19968;&#31181;&#26032;&#22411;&#25915;&#20987;&#27169;&#24335;&#65292;&#24050;&#34987;&#35777;&#26126;&#22312;&#35757;&#32451;&#26399;&#38388;&#21521;&#27169;&#22411;&#28155;&#21152;&#21518;&#38376;&#26159;&#26377;&#25928;&#30340;&#12290;&#38450;&#24481;&#27492;&#31867;&#21518;&#38376;&#25915;&#20987;&#24050;&#21464;&#24471;&#32039;&#36843;&#21644;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AttDef&#30340;&#39640;&#25928;&#24402;&#22240;&#31649;&#36947;&#65292;&#29992;&#20110;&#38450;&#24481;&#20004;&#31181;&#25554;&#20837;&#24335;&#27745;&#26579;&#25915;&#20987;BadNL&#21644;InSent&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#20855;&#26377;&#36739;&#22823;&#24402;&#22240;&#20998;&#25968;&#30340;&#20196;&#29260;&#35270;&#20026;&#28508;&#22312;&#35302;&#21457;&#22120;&#65292;&#22240;&#20026;&#36739;&#22823;&#30340;&#24402;&#22240;&#35789;&#23545;&#20110;&#38169;&#35823;&#39044;&#27979;&#32467;&#26524;&#20570;&#20986;&#36739;&#22823;&#36129;&#29486;&#65292;&#22240;&#27492;&#26356;&#26377;&#21487;&#33021;&#26159;&#27745;&#26579;&#35302;&#21457;&#22120;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#21033;&#29992;&#22806;&#37096;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26469;&#21306;&#20998;&#36755;&#20837;&#26159;&#21542;&#34987;&#27745;&#26579;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#20004;&#31181;&#24120;&#35265;&#30340;&#25915;&#20987;&#22330;&#26223;&#65288;&#27745;&#26579;&#35757;&#32451;&#25968;&#25454;&#21644;&#27979;&#35797;&#25968;&#25454;&#65289;&#20013;&#20855;&#26377;&#36275;&#22815;&#30340;&#27867;&#21270;&#24615;&#65292;&#36825;&#19968;&#28857;&#25345;&#32493;&#25913;&#21892;&#20102;&#20043;&#21069;&#30340;&#26041;&#27861;&#12290;&#20363;&#22914;&#65292;AttDef&#22312;&#22235;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21487;&#20197;&#25104;&#21151;&#32531;&#35299;&#20004;&#31181;&#25915;&#20987;&#65292;&#24179;&#22343;&#20934;&#30830;&#29575;&#20026;79.97%&#65288;&#25552;&#39640;&#20102;56.59%&#65289;&#21644;48.34%&#65288;&#25552;&#39640;&#20102;15.25%&#65289;&#65292;&#35777;&#26126;&#20102;&#23427;&#22312;&#38450;&#24481;&#25554;&#20837;&#24335;&#25991;&#26412;&#21518;&#38376;&#25915;&#20987;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Textual backdoor attack, as a novel attack model, has been shown to be effective in adding a backdoor to the model during training. Defending against such backdoor attacks has become urgent and important. In this paper, we propose AttDef, an efficient attribution-based pipeline to defend against two insertion-based poisoning attacks, BadNL and InSent. Specifically, we regard the tokens with larger attribution scores as potential triggers since larger attribution words contribute more to the false prediction results and therefore are more likely to be poison triggers. Additionally, we further utilize an external pre-trained language model to distinguish whether input is poisoned or not. We show that our proposed method can generalize sufficiently well in two common attack scenarios (poisoning training data and testing data), which consistently improves previous methods. For instance, AttDef can successfully mitigate both attacks with an average accuracy of 79.97% (56.59% up) and 48.34% 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#20154;&#24037;&#26234;&#33021;&#20219;&#21153; - &#30693;&#35782;&#22270;&#35889;&#25512;&#29702;(KGR)&#65292;&#24182;&#22522;&#20110;&#25915;&#20987;&#32773;&#30340;&#30446;&#30340;&#12289;&#30693;&#35782;&#21644;&#25915;&#20987;&#21521;&#37327;&#27010;&#25324;&#20102;KGR&#30340;&#23433;&#20840;&#23041;&#32961;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#25915;&#20987;ROAR&#65292;&#39640;&#24230;&#26377;&#25928;&#22320;&#35823;&#23548;KGR&#21521;&#30446;&#26631;&#26597;&#35810;&#25552;&#20379;&#39044;&#23450;&#20041;&#31572;&#26696;&#65292;&#20294;&#23545;&#20110;&#38750;&#30446;&#26631;&#26597;&#35810;&#20960;&#20046;&#27809;&#26377;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.02383</link><description>&lt;p&gt;
&#35770;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#30340;&#23433;&#20840;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
On the Security Risks of Knowledge Graph Reasoning. (arXiv:2305.02383v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02383
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#20154;&#24037;&#26234;&#33021;&#20219;&#21153; - &#30693;&#35782;&#22270;&#35889;&#25512;&#29702;(KGR)&#65292;&#24182;&#22522;&#20110;&#25915;&#20987;&#32773;&#30340;&#30446;&#30340;&#12289;&#30693;&#35782;&#21644;&#25915;&#20987;&#21521;&#37327;&#27010;&#25324;&#20102;KGR&#30340;&#23433;&#20840;&#23041;&#32961;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#25915;&#20987;ROAR&#65292;&#39640;&#24230;&#26377;&#25928;&#22320;&#35823;&#23548;KGR&#21521;&#30446;&#26631;&#26597;&#35810;&#25552;&#20379;&#39044;&#23450;&#20041;&#31572;&#26696;&#65292;&#20294;&#23545;&#20110;&#38750;&#30446;&#26631;&#26597;&#35810;&#20960;&#20046;&#27809;&#26377;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#25512;&#29702; (KGR) - &#22238;&#31572;&#22823;&#22411;&#30693;&#35782;&#22270;&#35889;&#19978;&#22797;&#26434;&#36923;&#36753;&#26597;&#35810;&#30340;&#20219;&#21153;&#65292;&#20195;&#34920;&#20102;&#19968;&#39033;&#37325;&#35201;&#30340;&#20154;&#24037;&#26234;&#33021;&#20219;&#21153;&#65292;&#28041;&#21450;&#22810;&#31181;&#24212;&#29992;&#65288;&#20363;&#22914;&#32593;&#32476;&#23041;&#32961;&#29417;&#29454;&#65289;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#23427;&#36234;&#26469;&#36234;&#27969;&#34892;&#65292;&#20294;&#26159; KGR &#30340;&#28508;&#22312;&#23433;&#20840;&#39118;&#38505;&#36824;&#26410;&#34987;&#24191;&#27867;&#25506;&#35752;&#65292;&#36825;&#19968;&#28857;&#20196;&#20154;&#25285;&#24551;&#65292;&#22240;&#20026;&#23427;&#22312;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#24191;&#27867;&#12290;&#26412;&#25991;&#26159;&#26550;&#36215;&#29421;&#38552;&#40511;&#27807;&#30340;&#19968;&#20010;&#21487;&#38752;&#30340;&#21021;&#22987;&#27493;&#39588;&#12290;&#25105;&#20204;&#26681;&#25454;&#25915;&#20987;&#32773;&#30340;&#30446;&#30340;&#12289;&#30693;&#35782;&#21644;&#25915;&#20987;&#21521;&#37327;&#31995;&#32479;&#21270;&#22320;&#27010;&#25324;&#20102; KGR &#30340;&#23433;&#20840;&#23041;&#32961;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; ROAR&#65292;&#19968;&#31181;&#26032;&#22411;&#25915;&#20987;&#65292;&#23427;&#26159;&#23454;&#29616;&#21508;&#31181;&#27492;&#31867;&#23041;&#32961;&#30340;&#25915;&#20987;&#12290;&#36890;&#36807;&#22312;&#20195;&#34920;&#24615;&#29992;&#20363;&#65288;&#22914;&#21307;&#30103;&#20915;&#31574;&#25903;&#25345;&#12289;&#32593;&#32476;&#23041;&#32961;&#29417;&#29454;&#21644;&#24120;&#35782;&#25512;&#29702;&#65289;&#20013;&#30340;&#23454;&#35777;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126; ROAR &#23545;&#20110;&#35823;&#23548; KGR &#25552;&#20379;&#39044;&#23450;&#20041;&#31572;&#26696;&#30340;&#30446;&#26631;&#26597;&#35810;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#23545;&#38750;&#30446;&#26631;&#26597;&#35810;&#24433;&#21709;&#24494;&#20046;&#20854;&#24494;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graph reasoning (KGR) -- answering complex logical queries over large knowledge graphs -- represents an important artificial intelligence task, entailing a range of applications (e.g., cyber threat hunting). However, despite its surging popularity, the potential security risks of KGR are largely unexplored, which is concerning, given the increasing use of such capability in security-critical domains.  This work represents a solid initial step towards bridging the striking gap. We systematize the security threats to KGR according to the adversary's objectives, knowledge, and attack vectors. Further, we present ROAR, a new class of attacks that instantiate a variety of such threats. Through empirical evaluation in representative use cases (e.g., medical decision support, cyber threat hunting, and commonsense reasoning), we demonstrate that ROAR is highly effective to mislead KGR to suggest pre-defined answers for target queries, yet with negligible impact on non-target ones. Fi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24230;&#37327;&#26694;&#26550;&#21644;&#26032;&#30340;&#32858;&#21512;&#25351;&#26631;&#65292;&#29992;&#20110;&#25935;&#24863;&#24615;&#20998;&#26512;&#65292;&#21487;&#20197;&#36866;&#29992;&#20110;&#20219;&#20309;&#21487;&#24494;&#20998;&#27169;&#22411;&#12290;&#20854;&#20013;&#26032;&#30340;&#32858;&#21512;&#25351;&#26631;&#22522;&#20110;&#19977;&#31181;&#26041;&#27861;&#33719;&#21462;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#65292;&#24182;&#22312;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.02368</link><description>&lt;p&gt;
&#24212;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#25935;&#24863;&#24615;&#20998;&#26512;&#24230;&#37327;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Metric Tools for Sensitivity Analysis with Applications to Neural Networks. (arXiv:2305.02368v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02368
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24230;&#37327;&#26694;&#26550;&#21644;&#26032;&#30340;&#32858;&#21512;&#25351;&#26631;&#65292;&#29992;&#20110;&#25935;&#24863;&#24615;&#20998;&#26512;&#65292;&#21487;&#20197;&#36866;&#29992;&#20110;&#20219;&#20309;&#21487;&#24494;&#20998;&#27169;&#22411;&#12290;&#20854;&#20013;&#26032;&#30340;&#32858;&#21512;&#25351;&#26631;&#22522;&#20110;&#19977;&#31181;&#26041;&#27861;&#33719;&#21462;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#65292;&#24182;&#22312;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#34987;&#32771;&#34385;&#29992;&#20110;&#25317;&#26377;&#37325;&#22823;&#31038;&#20250;&#24433;&#21709;&#30340;&#33258;&#20027;&#20915;&#31574;&#65292;&#20102;&#35299;&#36825;&#20123;&#27169;&#22411;&#22914;&#20309;&#24037;&#20316;&#30340;&#38656;&#27714;&#36805;&#36895;&#22686;&#38271;&#12290;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;(XAI)&#26088;&#22312;&#20026;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25152;&#20570;&#30340;&#39044;&#27979;&#25552;&#20379;&#35299;&#37322;&#65292;&#20197;&#20351;&#35813;&#27169;&#22411;&#23545;&#29992;&#25143;&#26356;&#20855;&#21487;&#20449;&#24230;&#21644;&#36879;&#26126;&#24230;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#20351;&#29992;&#24230;&#37327;&#25216;&#24039;&#26469;&#30740;&#31350;&#25935;&#24863;&#24230;&#20998;&#26512;&#12290;&#20174;&#36825;&#20010;&#24230;&#37327;&#26694;&#26550;&#24320;&#22987;&#65292;&#20171;&#32461;&#20102;&#26032;&#30340;&#32858;&#21512;&#25351;&#26631;&#65292;&#20197;&#26681;&#25454;&#19977;&#31181;&#26041;&#27861;&#20174;&#20559;&#23548;&#25968;&#20013;&#33719;&#21462;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#65306;&#23616;&#37096;&#25200;&#21160;&#20998;&#26512;&#12289;&#20840;&#23616;&#25200;&#21160;&#20998;&#26512;&#21644;&#28151;&#21512;&#26041;&#27861;&#12290;&#36825;&#20123;&#25351;&#26631;&#36866;&#29992;&#20110;&#20219;&#20309;&#21487;&#24494;&#20998;&#27169;&#22411;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#24615;&#33021;&#22312;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#35828;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
As Machine Learning models are considered for autonomous decisions with significant social impact, the need for understanding how these models work rises rapidly. Explainable Artificial Intelligence (XAI) aims to provide interpretations for predictions made by Machine Learning models, in order to make the model trustworthy and more transparent for the user. For example, selecting relevant input variables for the problem directly impacts the model's ability to learn and make accurate predictions, so obtaining information about input importance play a crucial role when training the model. One of the main XAI techniques to obtain input variable importance is the sensitivity analysis based on partial derivatives. However, existing literature of this method provide no justification of the aggregation metrics used to retrieved information from the partial derivatives.  In this paper, a theoretical framework is proposed to study sensitivities of ML models using metric techniques. From this me
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#24191;&#21578;&#19982;&#26102;&#23578;&#21697;&#21619;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;Fashionpedia-Ads&#65292;&#25910;&#38598;&#24182;&#27880;&#37322;&#20102;&#24191;&#21578;&#22270;&#20687;&#30340;&#24773;&#24863;&#12289;&#35270;&#35273;&#21644;&#25991;&#26412;&#20449;&#24687;&#65292;&#20197;&#20415;&#26410;&#26469;&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2305.02360</link><description>&lt;p&gt;
Fashionpedia-Ads: &#20320;&#21916;&#27426;&#30340;&#24191;&#21578;&#33021;&#36879;&#38706;&#20320;&#30340;&#26102;&#23578;&#21697;&#21619;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Fashionpedia-Ads: Do Your Favorite Advertisements Reveal Your Fashion Taste?. (arXiv:2305.02360v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02360
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#24191;&#21578;&#19982;&#26102;&#23578;&#21697;&#21619;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;Fashionpedia-Ads&#65292;&#25910;&#38598;&#24182;&#27880;&#37322;&#20102;&#24191;&#21578;&#22270;&#20687;&#30340;&#24773;&#24863;&#12289;&#35270;&#35273;&#21644;&#25991;&#26412;&#20449;&#24687;&#65292;&#20197;&#20415;&#26410;&#26469;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20114;&#32852;&#32593;&#19978;&#65292;&#28040;&#36153;&#32773;&#26292;&#38706;&#20110;&#22810;&#20010;&#39046;&#22495;&#30340;&#24191;&#21578;&#20013;&#65292;&#22914;&#26102;&#23578;&#12289;&#32654;&#23481;&#12289;&#27773;&#36710;&#12289;&#39135;&#21697;&#31561;&#31561;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#26102;&#23578;&#26159;&#31532;&#20108;&#22823;&#30005;&#23376;&#21830;&#21153;&#36141;&#29289;&#31867;&#21035;&#12290;&#28040;&#36153;&#32773;&#22312;&#21508;&#31181;&#26102;&#23578;&#24191;&#21578;&#22270;&#29255;&#19978;&#30340;&#25968;&#23383;&#35760;&#24405;&#34892;&#20026;&#26159;&#21542;&#33021;&#36879;&#38706;&#20986;&#20182;&#20204;&#30340;&#26102;&#23578;&#21697;&#21619;&#65311;&#26469;&#33258;&#20854;&#20182;&#39046;&#22495;&#30340;&#24191;&#21578;&#26159;&#21542;&#20063;&#33021;&#25512;&#26029;&#20986;&#20182;&#20204;&#30340;&#26102;&#23578;&#21697;&#21619;&#65311;&#26412;&#25991;&#30740;&#31350;&#24191;&#21578;&#21644;&#26102;&#23578;&#21697;&#21619;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;Fashionpedia-Ads&#65292;&#35813;&#25968;&#25454;&#38598;&#35201;&#27714;&#21463;&#35797;&#32773;&#25552;&#20379;&#23545;&#24191;&#21578;&#65288;&#26102;&#23578;&#12289;&#32654;&#23481;&#12289;&#27773;&#36710;&#21644;&#29980;&#28857;&#65289;&#20197;&#21450;&#26102;&#23578;&#20135;&#21697;&#65288;&#31038;&#20132;&#32593;&#32476;&#21644;&#30005;&#23376;&#21830;&#21153;&#27454;&#24335;&#65289;&#22270;&#20687;&#30340;&#20559;&#22909;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20174;&#22810;&#20010;&#35282;&#24230;&#65288;&#25277;&#35937;&#32423;&#21035;&#12289;&#29289;&#29702;&#32423;&#21035;&#12289;&#26631;&#39064;&#21644;&#21697;&#29260;&#65289;&#20840;&#38754;&#25910;&#38598;&#24182;&#27880;&#37322;&#24191;&#21578;&#22270;&#20687;&#19978;&#30340;&#24773;&#24863;&#12289;&#35270;&#35273;&#21644;&#25991;&#26412;&#20449;&#24687;&#12290;&#25105;&#20204;&#24320;&#28304;&#20102;Fashionpedia-Ads&#65292;&#20197;&#20415;&#26410;&#26469;&#30340;&#30740;&#31350;&#65292;&#24182;&#40723;&#21169;&#26356;&#22810;&#30340;&#21487;&#35299;&#37322;&#24615;&#30740;&#31350;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Consumers are exposed to advertisements across many different domains on the internet, such as fashion, beauty, car, food, and others. On the other hand, fashion represents second highest e-commerce shopping category. Does consumer digital record behavior on various fashion ad images reveal their fashion taste? Does ads from other domains infer their fashion taste as well? In this paper, we study the correlation between advertisements and fashion taste. Towards this goal, we introduce a new dataset, Fashionpedia-Ads, which asks subjects to provide their preferences on both ad (fashion, beauty, car, and dessert) and fashion product (social network and e-commerce style) images. Furthermore, we exhaustively collect and annotate the emotional, visual and textual information on the ad images from multi-perspectives (abstractive level, physical level, captions, and brands). We open-source Fashionpedia-Ads to enable future studies and encourage more approaches to interpretability research bet
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#36890;&#36807;&#26426;&#22120;&#26500;&#24314;&#37326;&#29983;&#22320;&#28857;&#30340;&#26694;&#26550;&#65292;&#23558;&#26426;&#22120;&#29702;&#35299;&#20026;&#33021;&#22815;&#21442;&#19982;&#29983;&#20135;&#26234;&#33021;&#30340;&#27963;&#21160;&#20195;&#29702;&#12290;</title><link>http://arxiv.org/abs/2305.02328</link><description>&lt;p&gt;
&#22521;&#32946;&#37326;&#24615;&#65306;&#26426;&#22120;&#20013;&#30340;&#25216;&#26415;&#22810;&#26679;&#24615;&#19982;&#37326;&#24615;
&lt;/p&gt;
&lt;p&gt;
Cultivated Wildness: Technodiversity and Wildness in Machines. (arXiv:2305.02328v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02328
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#36890;&#36807;&#26426;&#22120;&#26500;&#24314;&#37326;&#29983;&#22320;&#28857;&#30340;&#26694;&#26550;&#65292;&#23558;&#26426;&#22120;&#29702;&#35299;&#20026;&#33021;&#22815;&#21442;&#19982;&#29983;&#20135;&#26234;&#33021;&#30340;&#27963;&#21160;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#26223;&#35266;&#35774;&#35745;&#21644;&#20154;&#24037;&#26234;&#33021;&#20132;&#21449;&#28857;&#19978;&#30340;&#22521;&#32946;&#37326;&#24615;&#30340;&#27010;&#24565;&#12290;&#26412;&#25991;&#35748;&#20026;&#65292;&#24403;&#20195;&#26223;&#35266;&#23454;&#36341;&#24212;&#35813;&#20811;&#26381;&#23545;&#37326;&#29983;&#29366;&#24577;&#21333;&#19968;&#29702;&#35299;&#30340;&#21487;&#33021;&#24615;&#65292;&#32780;&#26159;&#24212;&#35813;&#36890;&#36807;&#24403;&#20195;&#29615;&#22659;&#20154;&#25991;&#23398;&#12289;&#31185;&#23398;&#25216;&#26415;&#30740;&#31350;&#12289;&#29983;&#24577;&#23398;&#21644;&#26223;&#35266;&#24314;&#31569;&#30340;&#24605;&#24819;&#21644;&#20851;&#27880;&#28857;&#65292;&#25506;&#32034;&#22521;&#32946;&#26032;&#24418;&#24335;&#30340;&#37326;&#29983;&#22320;&#28857;&#30340;&#26223;&#35266;&#31574;&#30053;&#12290;&#26412;&#25991;&#36890;&#36807;&#29615;&#22659;&#24037;&#31243;&#12289;&#35745;&#31639;&#26426;&#31185;&#23398;&#21644;&#26223;&#35266;&#24314;&#31569;&#30740;&#31350;&#26696;&#20363;&#65292;&#25506;&#35752;&#20102;&#26500;&#24314;&#20855;&#26377;&#26234;&#33021;&#26426;&#22120;&#30340;&#37326;&#29983;&#22320;&#28857;&#30340;&#26694;&#26550;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#26426;&#22120;&#19981;&#26159;&#34987;&#29702;&#35299;&#20026;&#19968;&#23618;&#8220;&#25968;&#23383;&#22522;&#30784;&#35774;&#26045;&#8221;&#65292;&#29992;&#20110;&#25193;&#23637;&#26412;&#22320;&#21270;&#30340;&#20154;&#31867;&#26234;&#33021;&#21644;&#20195;&#29702;&#12290;&#30456;&#21453;&#65292;&#26426;&#22120;&#34987;&#27010;&#24565;&#21270;&#20026;&#33021;&#22815;&#21442;&#19982;&#20849;&#21516;&#29983;&#20135;&#26234;&#33021;&#30340;&#27963;&#21160;&#20195;&#29702;&#12290;&#36817;&#26399;&#30340;&#25511;&#21046;&#35770;&#25216;&#26415;&#21457;&#23637;&#65292;&#20363;&#22914;&#20256;&#24863;&#32593;&#32476;&#65292;&#20154;&#24037;&#26234;&#33021;&#31561;&#65292;&#24320;&#21019;&#20102;&#36825;&#31181;&#26426;&#22120;&#21442;&#19982;&#29983;&#20135;&#26234;&#33021;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the idea of cultivated wildness at the intersection of landscape design and artificial intelligence. The paper posits that contemporary landscape practices should overcome the potentially single understanding on wilderness, and instead explore landscape strategies to cultivate new forms of wild places via ideas and concerns in contemporary Environmental Humanities, Science and Technology Studies, Ecological Sciences, and Landscape Architecture. Drawing cases in environmental engineering, computer science, and landscape architecture research, this paper explores a framework to construct wild places with intelligent machines. In this framework, machines are not understood as a layer of "digital infrastructure" that is used to extend localized human intelligence and agency. Rather machines are conceptualized as active agents who can participate in the intelligence of co-production. Recent developments in cybernetic technologies such as sensing networks, artificial 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20197;&#20171;&#32461;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#28023;&#23736;&#29615;&#22659;&#20013;&#39044;&#27979;&#21464;&#37327;&#30340;&#26696;&#20363;&#20026;&#20363;&#65292;&#25552;&#20379;&#20102;&#26410;&#26469;&#26223;&#35266;&#35774;&#35745;&#20013;&#21363;&#23558;&#21040;&#26469;&#30340;&#32593;&#32476;&#29615;&#22659;&#30340;&#23454;&#35777;&#35777;&#25454;&#12290;&#35774;&#35745;&#24072;&#23558;&#19981;&#20877;&#26159;&#20316;&#23478;&#65292;&#32780;&#26159;&#26234;&#33021;&#20195;&#29702;&#20013;&#30340;&#32534;&#33310;&#32773;&#12289;&#20652;&#21270;&#21058;&#21644;&#25351;&#25381;&#32773;&#12290;&#26412;&#25991;&#20174;&#21518;&#20154;&#31867;&#20027;&#20041;&#20013;&#33719;&#24471;&#28789;&#24863;&#65292;&#35748;&#20026;&#35201;&#30495;&#27491;&#29702;&#35299;&#32593;&#32476;&#29615;&#22659;&#65292;&#25105;&#20204;&#24517;&#39035;&#37319;&#21462;&#21518;&#20154;&#31867;&#20027;&#20041;&#30340;&#20262;&#29702;&#24182;&#20811;&#26381;&#20154;&#31867;&#30340;&#20248;&#36234;&#20027;&#20041;&#12290;</title><link>http://arxiv.org/abs/2305.02327</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#22312;&#26223;&#35266;&#35774;&#35745;&#20013;&#30340;&#26410;&#26469;&#65306;&#20197;&#32654;&#22269;&#24343;&#21513;&#23612;&#20122;&#24030;&#28023;&#23736;&#20026;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
The Future of Artificial Intelligence (AI) and Machine Learning (ML) in Landscape Design: A Case Study in Coastal Virginia, USA. (arXiv:2305.02327v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02327
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20197;&#20171;&#32461;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#28023;&#23736;&#29615;&#22659;&#20013;&#39044;&#27979;&#21464;&#37327;&#30340;&#26696;&#20363;&#20026;&#20363;&#65292;&#25552;&#20379;&#20102;&#26410;&#26469;&#26223;&#35266;&#35774;&#35745;&#20013;&#21363;&#23558;&#21040;&#26469;&#30340;&#32593;&#32476;&#29615;&#22659;&#30340;&#23454;&#35777;&#35777;&#25454;&#12290;&#35774;&#35745;&#24072;&#23558;&#19981;&#20877;&#26159;&#20316;&#23478;&#65292;&#32780;&#26159;&#26234;&#33021;&#20195;&#29702;&#20013;&#30340;&#32534;&#33310;&#32773;&#12289;&#20652;&#21270;&#21058;&#21644;&#25351;&#25381;&#32773;&#12290;&#26412;&#25991;&#20174;&#21518;&#20154;&#31867;&#20027;&#20041;&#20013;&#33719;&#24471;&#28789;&#24863;&#65292;&#35748;&#20026;&#35201;&#30495;&#27491;&#29702;&#35299;&#32593;&#32476;&#29615;&#22659;&#65292;&#25105;&#20204;&#24517;&#39035;&#37319;&#21462;&#21518;&#20154;&#31867;&#20027;&#20041;&#30340;&#20262;&#29702;&#24182;&#20811;&#26381;&#20154;&#31867;&#30340;&#20248;&#36234;&#20027;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26223;&#35266;&#35774;&#35745;&#39046;&#22495;&#20013;&#65292;&#24050;&#32463;&#26377;&#19968;&#20123;&#29702;&#35770;&#24615;&#30340;&#23581;&#35797;&#30452;&#25509;&#28041;&#21450;&#21040;AI&#21644;ML&#12290;&#26412;&#25991;&#36890;&#36807;&#23637;&#31034;&#19968;&#20010;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#26469;&#39044;&#27979;&#28023;&#23736;&#29615;&#22659;&#21464;&#37327;&#30340;&#26696;&#20363;&#65292;&#25552;&#20379;&#20102;&#21363;&#23558;&#21040;&#26469;&#30340;&#32593;&#32476;&#29615;&#22659;&#30340;&#23454;&#35777;&#35777;&#25454;&#12290;&#22312;&#36825;&#20010;&#29615;&#22659;&#20013;&#65292;&#35774;&#35745;&#24072;&#19981;&#20877;&#26159;&#20316;&#23478;&#65292;&#32780;&#26159;&#35768;&#22810;&#26234;&#33021;&#20195;&#29702;&#20013;&#30340;&#32534;&#33310;&#32773;&#12289;&#20652;&#21270;&#21058;&#21644;&#25351;&#25381;&#32773;&#12290;&#26412;&#25991;&#20174;&#21518;&#20154;&#31867;&#20027;&#20041;&#30340;&#24605;&#24819;&#20013;&#33719;&#21462;&#28789;&#24863;&#65292;&#35748;&#20026;&#35201;&#30495;&#27491;&#29702;&#35299;&#32593;&#32476;&#29615;&#22659;&#65292;&#25105;&#20204;&#24517;&#39035;&#37319;&#21462;&#21518;&#20154;&#31867;&#20027;&#20041;&#30340;&#20262;&#29702;&#24182;&#20811;&#26381;&#20154;&#31867;&#30340;&#20248;&#36234;&#20027;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
There have been theory-based endeavours that directly engage with AI and ML in the landscape discipline. By presenting a case that uses machine learning techniques to predict variables in a coastal environment, this paper provides empirical evidence of the forthcoming cybernetic environment, in which designers are conceptualized not as authors but as choreographers, catalyst agents, and conductors among many other intelligent agents. Drawing ideas from posthumanism, this paper argues that, to truly understand the cybernetic environment, we have to take on posthumanist ethics and overcome human exceptionalism.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20197;&#21382;&#21490;&#30340;&#35270;&#35282;&#22238;&#39038;&#20102;&#25511;&#21046;&#35770;&#21644;&#31995;&#32479;&#24605;&#32500;&#30340;&#21457;&#23637;&#21382;&#31243;&#65292;&#24182;&#22312;&#20171;&#32461;&#26223;&#35266;&#35774;&#35745;&#23398;&#31185;&#30740;&#31350;&#30340;&#35889;&#31995;&#22522;&#30784;&#19978;&#25351;&#20986;&#26223;&#35266;&#35774;&#35745;&#24072;&#36890;&#36807;&#22522;&#20110;&#29983;&#24577;&#23398;&#30340;&#26223;&#35266;&#35774;&#35745;&#23454;&#29616;&#22522;&#20110;&#25511;&#21046;&#35770;&#21407;&#21017;&#30340;&#31995;&#32479;&#26159;&#25511;&#21046;&#35770;&#21457;&#23637;&#20013;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#26412;&#25991;&#21628;&#21505;&#24314;&#31435;&#19968;&#31181;&#26032;&#30340;&#29615;&#22659;&#21442;&#19982;&#33539;&#24335;&#65292;&#20197;&#29702;&#35299;&#35774;&#35745;&#21644;&#26426;&#22120;&#26234;&#33021;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.02326</link><description>&lt;p&gt;
&#25511;&#21046;&#35770;&#29615;&#22659;&#65306;&#20851;&#20110;&#31995;&#32479;&#12289;&#35774;&#35745;&#21644;&#26426;&#22120;&#26234;&#33021;&#30340;&#21382;&#21490;&#21453;&#24605;
&lt;/p&gt;
&lt;p&gt;
Cybernetic Environment: A Historical Reflection on System, Design, and Machine Intelligence. (arXiv:2305.02326v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02326
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20197;&#21382;&#21490;&#30340;&#35270;&#35282;&#22238;&#39038;&#20102;&#25511;&#21046;&#35770;&#21644;&#31995;&#32479;&#24605;&#32500;&#30340;&#21457;&#23637;&#21382;&#31243;&#65292;&#24182;&#22312;&#20171;&#32461;&#26223;&#35266;&#35774;&#35745;&#23398;&#31185;&#30740;&#31350;&#30340;&#35889;&#31995;&#22522;&#30784;&#19978;&#25351;&#20986;&#26223;&#35266;&#35774;&#35745;&#24072;&#36890;&#36807;&#22522;&#20110;&#29983;&#24577;&#23398;&#30340;&#26223;&#35266;&#35774;&#35745;&#23454;&#29616;&#22522;&#20110;&#25511;&#21046;&#35770;&#21407;&#21017;&#30340;&#31995;&#32479;&#26159;&#25511;&#21046;&#35770;&#21457;&#23637;&#20013;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#26412;&#25991;&#21628;&#21505;&#24314;&#31435;&#19968;&#31181;&#26032;&#30340;&#29615;&#22659;&#21442;&#19982;&#33539;&#24335;&#65292;&#20197;&#29702;&#35299;&#35774;&#35745;&#21644;&#26426;&#22120;&#26234;&#33021;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20197;&#21382;&#21490;&#30340;&#35270;&#35282;&#22238;&#39038;&#20102;&#25511;&#21046;&#35770;&#21644;&#31995;&#32479;&#24605;&#32500;&#30340;&#21457;&#23637;&#21382;&#31243;&#65292;&#36861;&#28335;&#21040;20&#19990;&#32426;50&#24180;&#20195;&#65292;&#19968;&#32676;&#36328;&#23398;&#31185;&#23398;&#32773;&#22522;&#20110;&#26426;&#22120;&#21644;&#31995;&#32479;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#29702;&#35770;&#27169;&#22411;&#26469;&#29702;&#35299;&#24847;&#20041;&#12289;&#20449;&#24687;&#12289;&#24847;&#35782;&#21644;&#29983;&#21629;&#31561;&#38382;&#39064;&#12290;&#36890;&#36807;&#20171;&#32461;&#26223;&#35266;&#35774;&#35745;&#23398;&#31185;&#30740;&#31350;&#30340;&#35889;&#31995;&#65292;&#26412;&#25991;&#35748;&#20026;&#26223;&#35266;&#35774;&#35745;&#24072;&#36890;&#36807;&#22522;&#20110;&#29983;&#24577;&#23398;&#30340;&#26223;&#35266;&#35774;&#35745;&#22312;&#29615;&#22659;&#20013;&#23454;&#29616;&#22522;&#20110;&#25511;&#21046;&#35770;&#21407;&#21017;&#30340;&#31995;&#32479;&#65292;&#26159;&#25511;&#21046;&#35770;&#21457;&#23637;&#20013;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#26223;&#35266;&#23398;&#31185;&#24050;&#32463;&#21457;&#23637;&#20986;&#20102;&#19968;&#20010;&#35774;&#35745;&#26694;&#26550;&#65292;&#25552;&#20379;&#20102;&#28145;&#20837;&#29702;&#35299;&#26426;&#22120;&#26234;&#33021;&#30340;&#36716;&#21270;&#24615;&#27934;&#35265;&#12290;&#26412;&#25991;&#21628;&#21505;&#24314;&#31435;&#19968;&#31181;&#26032;&#30340;&#29615;&#22659;&#21442;&#19982;&#33539;&#24335;&#65292;&#20197;&#29702;&#35299;&#35774;&#35745;&#21644;&#26426;&#22120;&#26234;&#33021;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Taking on a historical lens, this paper traces the development of cybernetics and systems thinking back to the 1950s, when a group of interdisciplinary scholars converged to create a new theoretical model based on machines and systems for understanding matters of meaning, information, consciousness, and life. By presenting a genealogy of research in the landscape architecture discipline, the paper argues that landscape architects have been an important part of the development of cybernetics by materializing systems based on cybernetic principles in the environment through ecologically based landscape design. The landscape discipline has developed a design framework that provides transformative insights into understanding machine intelligence. The paper calls for a new paradigm of environmental engagement to understand matters of design and machine intelligence.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27979;&#35797;&#20102;ChatGPT&#22312;&#20837;&#38376;&#32423;&#20989;&#25968;&#24335;&#35821;&#35328;&#32534;&#31243;&#35838;&#31243;&#20013;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#26174;&#31034;&#23427;&#21487;&#20197;&#33719;&#24471;B-&#30340;&#25104;&#32489;&#24182;&#19988;&#33021;&#22815;&#32473;&#23398;&#29983;&#21644;&#35762;&#24072;&#24102;&#26469;&#28508;&#22312;&#22909;&#22788;&#12290;</title><link>http://arxiv.org/abs/2305.02230</link><description>&lt;p&gt;
ChatGPT&#33021;&#36890;&#36807;&#20837;&#38376;&#32423;&#20989;&#25968;&#24335;&#35821;&#35328;&#32534;&#31243;&#35838;&#31243;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can ChatGPT Pass An Introductory Level Functional Language Programming Course?. (arXiv:2305.02230v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02230
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27979;&#35797;&#20102;ChatGPT&#22312;&#20837;&#38376;&#32423;&#20989;&#25968;&#24335;&#35821;&#35328;&#32534;&#31243;&#35838;&#31243;&#20013;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#26174;&#31034;&#23427;&#21487;&#20197;&#33719;&#24471;B-&#30340;&#25104;&#32489;&#24182;&#19988;&#33021;&#22815;&#32473;&#23398;&#29983;&#21644;&#35762;&#24072;&#24102;&#26469;&#28508;&#22312;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#30340;&#24341;&#20837;&#24341;&#36215;&#20102;&#24037;&#19994;&#30028;&#21644;&#23398;&#26415;&#30028;&#30340;&#24191;&#27867;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#22312;&#35299;&#20915;&#21508;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#35821;&#35328;&#32763;&#35793;&#12289;&#25991;&#26412;&#25688;&#35201;&#21644;&#35745;&#31639;&#26426;&#32534;&#31243;&#26041;&#38754;&#20855;&#26377;&#21331;&#36234;&#30340;&#33021;&#21147;&#12290;&#23427;&#32534;&#20889;&#12289;&#20462;&#25913;&#29978;&#33267;&#32416;&#27491;&#20195;&#30721;&#30340;&#33021;&#21147;&#21152;&#19978;&#20854;&#26131;&#20110;&#20351;&#29992;&#21644;&#35775;&#38382;&#24050;&#32463;&#23545;&#35745;&#31639;&#26426;&#31185;&#23398;&#25945;&#32946;&#20135;&#29983;&#20102;&#24040;&#22823;&#24433;&#21709;&#12290;&#26412;&#25991;&#26088;&#22312;&#25506;&#35752;ChatGPT&#22312;&#20837;&#38376;&#32423;&#20989;&#25968;&#24335;&#35821;&#35328;&#32534;&#31243;&#35838;&#31243;&#20013;&#30340;&#34920;&#29616;&#12290;&#22312;&#25105;&#20204;&#30340;&#31995;&#32479;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#25226;ChatGPT&#35270;&#20026;&#25105;&#20204;&#30340;&#19968;&#21517;&#23398;&#29983;&#65292;&#24182;&#35777;&#26126;&#23427;&#21487;&#20197;&#33719;&#24471;B-&#30340;&#25104;&#32489;&#65292;&#25490;&#21517;&#22312;314&#21517;&#23398;&#29983;&#20013;&#30340;&#31532;155&#20301;&#12290;&#25105;&#20204;&#30340;&#32508;&#21512;&#35780;&#20272;&#20174;&#23398;&#29983;&#21644;&#35762;&#24072;&#30340;&#35282;&#24230;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30830;&#23450;&#20102;ChatGPT&#21487;&#20197;&#20026;&#36825;&#20004;&#20010;&#32676;&#20307;&#25552;&#20379;&#30340;&#20960;&#20010;&#28508;&#22312;&#22909;&#22788;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35748;&#20026;&#26412;&#30740;&#31350;&#26126;&#30830;&#20102;ChatGPT&#22312;&#35745;&#31639;&#26426;&#31185;&#23398;&#25945;&#32946;&#20013;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent introduction of ChatGPT has drawn significant attention from both industry and academia due to its impressive capabilities in solving a diverse range of tasks, including language translation, text summarization, and computer programming. Its capability for writing, modifying, and even correcting code together with its ease of use and access is already dramatically impacting computer science education. This paper aims to explore how well ChatGPT can perform in an introductory-level functional language programming course. In our systematic evaluation, we treated ChatGPT as one of our students and demonstrated that it can achieve a grade B- and its rank in the class is 155 out of 314 students overall. Our comprehensive evaluation provides valuable insights into ChatGPT's impact from both student and instructor perspectives. Additionally, we identify several potential benefits that ChatGPT can offer to both groups. Overall, we believe that this study significantly clarifies and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; Doc2SoarGraph &#26694;&#26550;&#65292;&#21033;&#29992;&#35821;&#20041;&#23548;&#21521;&#20998;&#23618;&#22270;&#32467;&#26500;&#20013;&#20803;&#32032;&#20043;&#38388;&#30340;&#24046;&#24322;&#21644;&#30456;&#20851;&#24615;&#65292;&#22312;&#23500;&#21547;&#35270;&#35273;&#34920;&#26684;&#25991;&#26412;&#30340;TAT-DQA&#38382;&#39064;&#19979;&#23454;&#29616;&#20102;&#31163;&#25955;&#25512;&#29702;&#65292;&#34920;&#29616;&#20986;&#20102;&#26368;&#20339;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.01938</link><description>&lt;p&gt;
Doc2SoarGraph&#65306;&#22522;&#20110;&#35821;&#20041;&#23548;&#21521;&#20998;&#23618;&#22270;&#30340;&#23500;&#21547;&#35270;&#35273;&#34920;&#26684;&#25991;&#26723;&#30340;&#31163;&#25955;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Doc2SoarGraph: Discrete Reasoning over Visually-Rich Table-Text Documents with Semantic-Oriented Hierarchical Graphs. (arXiv:2305.01938v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01938
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; Doc2SoarGraph &#26694;&#26550;&#65292;&#21033;&#29992;&#35821;&#20041;&#23548;&#21521;&#20998;&#23618;&#22270;&#32467;&#26500;&#20013;&#20803;&#32032;&#20043;&#38388;&#30340;&#24046;&#24322;&#21644;&#30456;&#20851;&#24615;&#65292;&#22312;&#23500;&#21547;&#35270;&#35273;&#34920;&#26684;&#25991;&#26412;&#30340;TAT-DQA&#38382;&#39064;&#19979;&#23454;&#29616;&#20102;&#31163;&#25955;&#25512;&#29702;&#65292;&#34920;&#29616;&#20986;&#20102;&#26368;&#20339;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#20004;&#24180;&#26469;&#65292;&#23545;&#20110;&#34920;&#26684;&#25991;&#26412;&#25991;&#26723;&#65288;&#20363;&#22914;&#36130;&#21153;&#25253;&#21578;&#65289;&#30340;&#31163;&#25955;&#25512;&#29702;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#22823;&#22810;&#36890;&#36807;&#25163;&#21160;&#36873;&#25321;&#21644;&#36716;&#25442;&#25991;&#26723;&#39029;&#38754;&#21040;&#32467;&#26500;&#21270;&#30340;&#34920;&#26684;&#21644;&#27573;&#33853;&#26469;&#31616;&#21270;&#36825;&#19968;&#25361;&#25112;&#65292;&#20174;&#32780;&#38459;&#30861;&#20854;&#23454;&#38469;&#24212;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#31350;&#20102;&#19968;&#31181;&#26356;&#20026;&#29616;&#23454;&#30340;&#38382;&#39064;&#35774;&#32622;&#65292;&#21363;&#20197; TAT-DQA &#30340;&#24418;&#24335;&#22238;&#31572;&#23500;&#21547;&#35270;&#35273;&#34920;&#26684;&#25991;&#26412;&#30340;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340; Doc2SoarGraph &#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#35821;&#20041;&#23548;&#21521;&#20998;&#23618;&#22270;&#32467;&#26500;&#20013;&#19981;&#21516;&#20803;&#32032;&#20043;&#38388;&#30340;&#24046;&#24322;&#21644;&#30456;&#20851;&#24615;&#65292;&#25552;&#39640;&#20102;&#20854;&#31163;&#25955;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#23545; TAT-DQA &#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#27979;&#35797;&#38598;&#19978;&#30340;&#31934;&#30830;&#21305;&#37197;&#65288;EM&#65289;&#21644; F1 &#24471;&#20998;&#26041;&#38754;&#20998;&#21035;&#27604;&#26368;&#20339;&#22522;&#32447;&#27169;&#22411;&#20998;&#21035;&#25552;&#39640;&#20102; 17.73% &#21644; 16.91%&#65292;&#23454;&#29616;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discrete reasoning over table-text documents (e.g., financial reports) gains increasing attention in recent two years. Existing works mostly simplify this challenge by manually selecting and transforming document pages to structured tables and paragraphs, hindering their practical application. In this work, we explore a more realistic problem setting in the form of TAT-DQA, i.e. to answer the question over a visually-rich table-text document. Specifically, we propose a novel Doc2SoarGraph framework with enhanced discrete reasoning capability by harnessing the differences and correlations among different elements (e.g., quantities, dates) of the given question and document with Semantic-oriented hierarchical Graph structures. We conduct extensive experiments on TAT-DQA dataset, and the results show that our proposed framework outperforms the best baseline model by 17.73% and 16.91% in terms of Exact Match (EM) and F1 score respectively on the test set, achieving the new state-of-the-art
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#21453;&#39304;&#25913;&#36827;&#21477;&#23376;&#23884;&#20837;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#30340;&#26041;&#24335;&#65292;&#21487;&#20197;&#25552;&#39640;&#23545;&#27604;&#23398;&#20064;&#26679;&#26412;&#23545;&#30340;&#36136;&#37327;&#65292;&#24182;&#32467;&#21512;&#20154;&#31867;&#21453;&#39304;&#26469;&#25552;&#20379;&#26356;&#22909;&#30340;&#30417;&#30563;&#20449;&#21495;&#12290;</title><link>http://arxiv.org/abs/2305.01918</link><description>&lt;p&gt;
&#25913;&#36827;&#21477;&#23376;&#23884;&#20837;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
Improving Contrastive Learning of Sentence Embeddings from AI Feedback. (arXiv:2305.01918v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01918
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#21453;&#39304;&#25913;&#36827;&#21477;&#23376;&#23884;&#20837;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#30340;&#26041;&#24335;&#65292;&#21487;&#20197;&#25552;&#39640;&#23545;&#27604;&#23398;&#20064;&#26679;&#26412;&#23545;&#30340;&#36136;&#37327;&#65292;&#24182;&#32467;&#21512;&#20154;&#31867;&#21453;&#39304;&#26469;&#25552;&#20379;&#26356;&#22909;&#30340;&#30417;&#30563;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#24050;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#21477;&#23376;&#23884;&#20837;&#23398;&#20064;&#20013;&#30340;&#27969;&#34892;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#33258;&#28982;&#35821;&#35328;&#30340;&#31163;&#25955;&#24615;&#20351;&#24471;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#29983;&#25104;&#30340;&#27491;&#36127;&#26679;&#26412;&#23545;&#30340;&#36136;&#37327;&#38590;&#20197;&#20445;&#35777;&#12290;&#34429;&#28982;&#26377;&#30417;&#30563;&#30340;&#23545;&#27604;&#23398;&#20064;&#21487;&#20197;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#26631;&#31614;&#29983;&#25104;&#26356;&#20934;&#30830;&#30340;&#26679;&#26412;&#23545;&#65292;&#20294;&#20173;&#32570;&#20047;&#32454;&#31890;&#24230;&#30340;&#35757;&#32451;&#20449;&#21495;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;AI&#21453;&#39304;&#26469;&#25913;&#36827;&#21477;&#23376;&#23884;&#20837;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#27861;&#65288;CLAIF&#65289;&#65292;&#21033;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411; (LLMs) &#30340;AI&#21453;&#39304;&#26500;&#24314;&#24102;&#26377;&#32454;&#31890;&#24230;&#26679;&#26412;&#30456;&#20284;&#24230;&#20998;&#25968;&#30340;&#26679;&#26412;&#23545;&#65292;&#20197;&#25913;&#36827;&#23545;&#27604;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#32467;&#21512;&#20154;&#24037;&#21453;&#39304;&#21644;AI&#21453;&#39304;&#20026;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#21477;&#23376;&#23884;&#20837;&#25552;&#20379;&#26356;&#22909;&#30340;&#30417;&#30563;&#20449;&#21495;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive learning has become a popular approach in natural language processing, particularly for the learning of sentence embeddings. However, the discrete nature of natural language makes it difficult to ensure the quality of positive and negative sample pairs generated through data augmentation methods. Although supervised contrastive learning can produce more accurate sample pairs with human feedback labels, it still lacks fine-grained training signals. In this paper, we propose to improve \textbf{C}ontrastive \textbf{L}earning of sentence embeddings from \textbf{AI} \textbf{F}eedback \textbf{(CLAIF)}. Our method utilizes AI feedback from large pre-trained language models (LLMs) to construct sample pairs with fine-grained sample similarity scores to improve contrastive learning. Besides, we combine human feedback and AI feedback to provide better supervision signals for supervised contrastive learning of sentence embeddings. Experimental results show that our method achieves stat
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#24863;&#30693;&#30340;&#30693;&#35782;&#24341;&#23548;&#25552;&#31034;&#26041;&#27861;&#65292;&#23558;&#20854;&#20316;&#20026;&#24178;&#39044;&#22120;&#35013;&#22791;&#21040;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#21477;&#23376;&#25552;&#21462;&#22120;&#20013;&#65292;&#20197;&#32531;&#35299;&#27010;&#24565;&#20559;&#24046;&#12290;&#22312;&#20195;&#34920;&#24615;&#30340;&#22810;&#35821;&#35328;KG&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.01876</link><description>&lt;p&gt;
&#22522;&#20110;&#22240;&#26524;&#24863;&#30693;&#30340;&#30693;&#35782;&#24341;&#23548;&#21477;&#23376;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Causality-aware Concept Extraction based on Knowledge-guided Prompting. (arXiv:2305.01876v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01876
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#24863;&#30693;&#30340;&#30693;&#35782;&#24341;&#23548;&#25552;&#31034;&#26041;&#27861;&#65292;&#23558;&#20854;&#20316;&#20026;&#24178;&#39044;&#22120;&#35013;&#22791;&#21040;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#21477;&#23376;&#25552;&#21462;&#22120;&#20013;&#65292;&#20197;&#32531;&#35299;&#27010;&#24565;&#20559;&#24046;&#12290;&#22312;&#20195;&#34920;&#24615;&#30340;&#22810;&#35821;&#35328;KG&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#24565;&#26377;&#21161;&#20110;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65292;&#20294;&#29616;&#26377;&#30340;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#20013;&#36828;&#26410;&#23436;&#21892;&#12290;&#26368;&#36817;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#24050;&#34987;&#24191;&#27867;&#29992;&#20110;&#22522;&#20110;&#25991;&#26412;&#30340;&#27010;&#24565;&#25552;&#21462;&#65288;CE&#65289;&#12290;&#28982;&#32780;&#65292;PLM&#24448;&#24448;&#20174;&#22823;&#37327;&#35821;&#26009;&#24211;&#30340;&#20849;&#29616;&#20851;&#32852;&#20013;&#36827;&#34892;&#39044;&#35757;&#32451;&#30693;&#35782;&#25366;&#25496;&#65292;&#32780;&#38750;Token&#20043;&#38388;&#30340;&#30495;&#23454;&#22240;&#26524;&#20851;&#31995;&#12290;&#22240;&#27492;&#65292;&#39044;&#35757;&#32451;&#30693;&#35782;&#28151;&#28102;&#20102;PLM&#65292;&#23548;&#33268;&#25552;&#21462;&#22522;&#20110;&#34394;&#20551;&#20849;&#29616;&#30456;&#20851;&#24615;&#30340;&#26377;&#20559;&#27010;&#24565;&#65292;&#19981;&#21487;&#36991;&#20813;&#22320;&#23548;&#33268;&#20302;&#31934;&#24230;&#12290;&#26412;&#25991;&#36890;&#36807;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65288;SCM&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#24341;&#23548;&#25552;&#31034;&#26041;&#27861;&#65292;&#23558;&#20854;&#20316;&#20026;&#24178;&#39044;&#22120;&#35013;&#22791;&#21040;&#22522;&#20110;PLM&#30340;&#25552;&#21462;&#22120;&#20013;&#65292;&#20197;&#20943;&#36731;&#27010;&#24565;&#20559;&#24046;&#12290;&#25552;&#31034;&#37319;&#29992;&#29616;&#26377;KG&#20013;&#30340;&#32473;&#23450;&#23454;&#20307;&#20027;&#39064;&#26469;&#32531;&#35299;&#23454;&#20307;&#21644;&#26377;&#20559;&#27010;&#24565;&#20043;&#38388;&#30340;&#34394;&#20551;&#20849;&#29616;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#22312;&#20195;&#34920;&#24615;&#30340;&#22810;&#35821;&#35328;KG&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#25552;&#31034;&#26174;&#33879;&#25913;&#36827;&#20102;&#25552;&#21462;&#24615;&#33021;&#65292;&#24182;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Concepts benefit natural language understanding but are far from complete in existing knowledge graphs (KGs). Recently, pre-trained language models (PLMs) have been widely used in text-based concept extraction (CE). However, PLMs tend to mine the co-occurrence associations from massive corpus as pre-trained knowledge rather than the real causal effect between tokens.As a result, the pre-trained knowledge confounds PLMs to extract biased concepts based on spurious co-occurrence correlations, inevitably resulting in low precision. In this paper, through the lens of a Structural Causal Model (SCM), we propose equipping the PLM-based extractor with a knowledge-guided prompt as an intervention to alleviate concept bias. The prompt adopts the topic of the given entity from the existing knowledge in KGs to mitigate the spurious co-occurrence correlations between entities and biased concepts. Our extensive experiments on representative multilingual KG datasets justify that our proposed prompt 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;KB-BINDER&#26694;&#26550;&#65292;&#36890;&#36807;&#23569;&#37327;&#30340;&#19978;&#19979;&#25991;&#28436;&#31034;&#23454;&#29616;&#20102;&#22312;&#22810;&#20010;&#30693;&#35782;&#24211;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#30340;&#32972;&#26223;&#23398;&#20064;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;KBQA&#38382;&#39064;&#30340;&#21487;&#35299;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.01750</link><description>&lt;p&gt;
&#22522;&#20110;&#23569;&#26679;&#26412;&#32972;&#26223;&#23398;&#20064;&#30340;&#30693;&#35782;&#24211;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Few-shot In-context Learning for Knowledge Base Question Answering. (arXiv:2305.01750v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01750
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;KB-BINDER&#26694;&#26550;&#65292;&#36890;&#36807;&#23569;&#37327;&#30340;&#19978;&#19979;&#25991;&#28436;&#31034;&#23454;&#29616;&#20102;&#22312;&#22810;&#20010;&#30693;&#35782;&#24211;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#30340;&#32972;&#26223;&#23398;&#20064;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;KBQA&#38382;&#39064;&#30340;&#21487;&#35299;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#24211;&#38382;&#31572;&#34987;&#35748;&#20026;&#26159;&#19968;&#20010;&#38590;&#20197;&#35299;&#20915;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#38656;&#35201;&#24212;&#23545;&#21508;&#31181;&#21487;&#33021;&#30340;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#19981;&#21516;&#30693;&#35782;&#24211;&#26550;&#26500;&#39033;&#20043;&#38388;&#30340;&#24322;&#26500;&#24615;&#36890;&#24120;&#38656;&#35201;&#38024;&#23545;&#19981;&#21516;&#30340;&#30693;&#35782;&#24211;&#38382;&#31572;&#65288;KBQA&#65289;&#25968;&#25454;&#38598;&#36827;&#34892;&#19987;&#38376;&#30340;&#35757;&#32451;&#12290;&#20026;&#20102;&#22788;&#29702;&#22810;&#31181;KBQA&#25968;&#25454;&#38598;&#19978;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;KB-BINDER&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#36827;&#34892;&#23569;&#37327;&#26679;&#26412;&#30340;&#32972;&#26223;&#23398;&#20064;&#65292;&#24182;&#23558;&#19981;&#21516;&#30340;KBQA&#25968;&#25454;&#38598;&#32479;&#19968;&#12290;&#39318;&#20808;&#65292;KB-BINDER&#21033;&#29992;&#20687;Codex&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#27169;&#20223;&#23569;&#37327;&#28436;&#31034;&#26469;&#29983;&#25104;&#29305;&#23450;&#38382;&#39064;&#30340;&#36923;&#36753;&#24418;&#24335;&#20316;&#20026;&#33609;&#31295;&#12290;&#20854;&#27425;&#65292;KB-BINDER&#22522;&#20110;&#30693;&#35782;&#24211;&#26469;&#32465;&#23450;&#29983;&#25104;&#30340;&#33609;&#31295;&#33267;&#21487;&#25191;&#34892;&#24418;&#24335;&#65292;&#36890;&#36807;BM25&#20998;&#25968;&#21305;&#37197;&#12290;&#22312;&#22235;&#20010;&#20844;&#24320;&#30340;&#24322;&#26500;KBQA&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;KB-BINDER&#21487;&#20197;&#22312;&#23569;&#37327;&#19978;&#19979;&#25991;&#28436;&#31034;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Question answering over knowledge bases is considered a difficult problem due to the challenge of generalizing to a wide variety of possible natural language questions. Additionally, the heterogeneity of knowledge base schema items between different knowledge bases often necessitates specialized training for different knowledge base question-answering (KBQA) datasets. To handle questions over diverse KBQA datasets with a unified training-free framework, we propose KB-BINDER, which for the first time enables few-shot in-context learning over KBQA tasks. Firstly, KB-BINDER leverages large language models like Codex to generate logical forms as the draft for a specific question by imitating a few demonstrations. Secondly, KB-BINDER grounds on the knowledge base to bind the generated draft to an executable one with BM25 score matching. The experimental results on four public heterogeneous KBQA datasets show that KB-BINDER can achieve a strong performance with only a few in-context demonstr
&lt;/p&gt;</description></item><item><title>&#35813;&#26041;&#27861;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#39044;&#35757;&#32451;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#23398;&#20064;&#20808;&#39564;&#30340;&#25968;&#25454;&#38598;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#23558;&#22823;&#37327;&#22270;&#20687;&#33976;&#39311;&#20026;&#23569;&#37327;&#20013;&#38388;&#29305;&#24449;&#21521;&#37327;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#25152;&#26377;&#35774;&#32622;&#20013;&#30340;&#36328;&#20307;&#31995;&#32467;&#26500;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.01649</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#29983;&#25104;&#20808;&#39564;&#30340;&#25968;&#25454;&#38598;&#33976;&#39311;&#26041;&#27861;&#30340;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Generalizing Dataset Distillation via Deep Generative Prior. (arXiv:2305.01649v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01649
&lt;/p&gt;
&lt;p&gt;
&#35813;&#26041;&#27861;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#39044;&#35757;&#32451;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#23398;&#20064;&#20808;&#39564;&#30340;&#25968;&#25454;&#38598;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#23558;&#22823;&#37327;&#22270;&#20687;&#33976;&#39311;&#20026;&#23569;&#37327;&#20013;&#38388;&#29305;&#24449;&#21521;&#37327;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#25152;&#26377;&#35774;&#32622;&#20013;&#30340;&#36328;&#20307;&#31995;&#32467;&#26500;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#38598;&#33976;&#39311;&#26088;&#22312;&#23558;&#25972;&#20010;&#25968;&#25454;&#38598;&#30340;&#30693;&#35782;&#33976;&#39311;&#21040;&#20960;&#20010;&#21512;&#25104;&#22270;&#20687;&#20013;&#12290;&#20854;&#24605;&#24819;&#26159;&#21512;&#25104;&#23569;&#37327;&#30340;&#21512;&#25104;&#25968;&#25454;&#28857;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#35757;&#32451;&#25968;&#25454;&#25552;&#20379;&#32473;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#24471;&#21040;&#19968;&#20010;&#36924;&#36817;&#21407;&#22987;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;&#23613;&#31649;&#35813;&#39046;&#22495;&#26368;&#36817;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#29616;&#26377;&#30340;&#25968;&#25454;&#38598;&#33976;&#39311;&#26041;&#27861;&#26080;&#27861;&#25512;&#24191;&#21040;&#26032;&#30340;&#20307;&#31995;&#32467;&#26500;&#24182;&#25193;&#23637;&#21040;&#39640;&#20998;&#36776;&#29575;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#20811;&#26381;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#39044;&#35757;&#32451;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#23398;&#20064;&#20808;&#39564;&#26469;&#21512;&#25104;&#33976;&#39311;&#30340;&#25968;&#25454;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#30340;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#22312;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#23558;&#22823;&#37327;&#22270;&#20687;&#33976;&#39311;&#20026;&#23569;&#37327;&#20013;&#38388;&#29305;&#24449;&#21521;&#37327;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22686;&#24378;&#20102;&#29616;&#26377;&#25216;&#26415;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#25152;&#26377;&#35774;&#32622;&#20013;&#30340;&#36328;&#20307;&#31995;&#32467;&#26500;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dataset Distillation aims to distill an entire dataset's knowledge into a few synthetic images. The idea is to synthesize a small number of synthetic data points that, when given to a learning algorithm as training data, result in a model approximating one trained on the original data. Despite recent progress in the field, existing dataset distillation methods fail to generalize to new architectures and scale to high-resolution datasets. To overcome the above issues, we propose to use the learned prior from pre-trained deep generative models to synthesize the distilled data. To achieve this, we present a new optimization algorithm that distills a large number of images into a few intermediate feature vectors in the generative model's latent space. Our method augments existing techniques, significantly improving cross-architecture generalization in all settings.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20851;&#27880;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#33021;&#22815;&#33258;&#20027;&#31649;&#29702;&#25506;&#32034;&#31574;&#30053;&#30340;&#22810;&#27169;&#24335;&#26234;&#33021;&#20307;&#38750;&#21333;&#20307;&#25506;&#32034;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.01322</link><description>&lt;p&gt;
&#22522;&#20110;Option&#26694;&#26550;&#30340;&#22810;&#27169;&#24335;&#25506;&#32034;&#33258;&#20027;&#38750;&#21333;&#20307;&#26234;&#33021;&#20307;
&lt;/p&gt;
&lt;p&gt;
An Autonomous Non-monolithic Agent with Multi-mode Exploration based on Options Framework. (arXiv:2305.01322v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01322
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20851;&#27880;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#33021;&#22815;&#33258;&#20027;&#31649;&#29702;&#25506;&#32034;&#31574;&#30053;&#30340;&#22810;&#27169;&#24335;&#26234;&#33021;&#20307;&#38750;&#21333;&#20307;&#25506;&#32034;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#30340;&#25506;&#32034;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#8220;&#22914;&#20309;&#25506;&#32034;&#8221;&#30340;&#25506;&#32034;&#26041;&#24335;&#65292;&#32780;&#8220;&#20309;&#26102;&#25506;&#32034;&#8221;&#30340;&#25506;&#32034;&#30740;&#31350;&#19968;&#30452;&#27809;&#26377;&#25104;&#20026;&#37325;&#28857;&#12290;&#20856;&#22411;&#30340;&#25506;&#32034;&#34892;&#20026;&#36890;&#24120;&#23558;&#25506;&#32034;&#34892;&#20026;&#19982;&#26234;&#33021;&#20307;&#30340;&#24320;&#21457;&#21033;&#29992;&#34892;&#20026;&#32465;&#23450;&#22312;&#19968;&#36215;&#12290;&#26368;&#36817;&#20986;&#29616;&#20102;&#38750;&#21333;&#20307;&#25506;&#32034;&#34892;&#20026;&#30340;&#30740;&#31350;&#65292;&#20197;&#30740;&#31350;&#20154;&#31867;&#21644;&#21160;&#29289;&#30340;&#27169;&#24335;&#20999;&#25442;&#34892;&#20026;&#12290;&#26412;&#30740;&#31350;&#30340;&#26368;&#32456;&#30446;&#30340;&#26159;&#20351;&#26234;&#33021;&#20307;&#33021;&#22815;&#33258;&#20027;&#20915;&#23450;&#20309;&#26102;&#25506;&#32034;&#25110;&#21033;&#29992;&#12290;&#25105;&#20204;&#22312;Option&#26694;&#26550;&#20013;&#25551;&#36848;&#20102;&#33258;&#20027;&#22810;&#27169;&#24335;&#25506;&#32034;&#30340;&#21021;&#22987;&#30740;&#31350;&#12290;&#36890;&#36807;&#27604;&#36739;&#23454;&#39564;&#32467;&#26524;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;&#29616;&#26377;&#30340;&#38750;&#21333;&#20307;&#25506;&#32034;&#26041;&#27861;&#30340;&#26356;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most exploration research on reinforcement learning (RL) has paid attention to `the way of exploration', which is `how to explore'. The other exploration research, `when to explore', has not been the main focus of RL exploration research. \textcolor{black}{The issue of `when' of a monolithic exploration in the usual RL exploration behaviour binds an exploratory action to an exploitational action of an agent. Recently, a non-monolithic exploration research has emerged to examine the mode-switching exploration behaviour of humans and animals.} The ultimate purpose of our research is to enable an agent to decide when to explore or exploit autonomously. We describe the initial research of an autonomous multi-mode exploration of non-monolithic behaviour in an options framework. The higher performance of our method is shown against the existing non-monolithic exploration method through comparative experimental results.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#19987;&#19994;&#30693;&#35782;&#26641;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#38598;&#20307;&#20915;&#31574;&#20013;&#19987;&#19994;&#30693;&#35782;&#27700;&#24179;&#19981;&#21516;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.01063</link><description>&lt;p&gt;
&#19987;&#19994;&#30693;&#35782;&#26641;&#22312;&#38598;&#20307;&#20915;&#31574;&#20013;&#35299;&#20915;&#30693;&#35782;&#23616;&#38480;&#24615;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Expertise Trees Resolve Knowledge Limitations in Collective Decision-Making. (arXiv:2305.01063v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01063
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#19987;&#19994;&#30693;&#35782;&#26641;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#38598;&#20307;&#20915;&#31574;&#20013;&#19987;&#19994;&#30693;&#35782;&#27700;&#24179;&#19981;&#21516;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21521;&#20915;&#31574;&#32773;&#25552;&#20379;&#24314;&#35758;&#30340;&#19987;&#23478;&#24448;&#24448;&#20250;&#26174;&#31034;&#20986;&#38543;&#38382;&#39064;&#23454;&#20363;&#21464;&#21270;&#32780;&#21464;&#21270;&#30340;&#19987;&#19994;&#30693;&#35782;&#27700;&#24179;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#38024;&#23545;&#23569;&#25968;&#24773;&#20917;&#30340;&#27425;&#20248;&#25110;&#27495;&#35270;&#24615;&#20915;&#31574;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#31181;&#30693;&#35782;&#28145;&#24230;&#21644;&#24191;&#24230;&#30340;&#21464;&#21270;&#24314;&#27169;&#20026;&#23558;&#38382;&#39064;&#31354;&#38388;&#21010;&#20998;&#20026;&#19981;&#21516;&#19987;&#19994;&#30693;&#35782;&#21306;&#22495;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20123;&#26032;&#31639;&#27861;&#65292;&#23427;&#20204;&#26126;&#30830;&#32771;&#34385;&#24182;&#36866;&#24212;&#38382;&#39064;&#23454;&#20363;&#19982;&#19987;&#23478;&#30693;&#35782;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#24182;&#24378;&#35843;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#36817;&#37051;&#26597;&#35810;&#30340;&#22825;&#30495;&#26041;&#27861;&#30340;&#32570;&#28857;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#8212;&#8212;&#19987;&#19994;&#30693;&#35782;&#26641;&#65292;&#23427;&#26500;&#24314;&#20915;&#31574;&#26641;&#65292;&#20351;&#23398;&#20064;&#32773;&#33021;&#22815;&#36873;&#25321;&#36866;&#24403;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#35265;&#35299;&#65292;&#24182;&#22312;&#19968;&#31995;&#21015;&#29616;&#26377;&#26041;&#27861;&#34987;&#35777;&#26126;&#19981;&#36275;&#20197;&#35299;&#20915;&#38382;&#39064;&#30340;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#32463;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#26032;&#26041;&#27861;&#30340;&#25913;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Experts advising decision-makers are likely to display expertise which varies as a function of the problem instance. In practice, this may lead to sub-optimal or discriminatory decisions against minority cases. In this work we model such changes in depth and breadth of knowledge as a partitioning of the problem space into regions of differing expertise. We provide here new algorithms that explicitly consider and adapt to the relationship between problem instances and experts' knowledge. We first propose and highlight the drawbacks of a naive approach based on nearest neighbor queries. To address these drawbacks we then introduce a novel algorithm - expertise trees - that constructs decision trees enabling the learner to select appropriate models. We provide theoretical insights and empirically validate the improved performance of our novel approach on a range of problems for which existing methods proved to be inadequate.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;SHAP&#26694;&#26550;&#21644;&#35270;&#35273;&#20808;&#39564;&#30693;&#35782;&#29983;&#25104;&#20840;&#38754;&#12289;&#26377;&#24847;&#20041;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#26356;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#12289;&#26356;&#39640;&#30340;&#35299;&#37322;&#34920;&#29616;&#21147;&#65292;&#24182;&#21487;&#20197;&#25512;&#24191;&#21040;&#20854;&#20182;&#27169;&#22411;&#19978;&#12290;</title><link>http://arxiv.org/abs/2304.14986</link><description>&lt;p&gt;
&#21033;&#29992;&#35821;&#20041;&#35270;&#35273;&#20808;&#39564;&#35299;&#37322;&#35270;&#35273;&#21644;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Interpreting Vision and Language Generative Models with Semantic Visual Priors. (arXiv:2304.14986v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14986
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;SHAP&#26694;&#26550;&#21644;&#35270;&#35273;&#20808;&#39564;&#30693;&#35782;&#29983;&#25104;&#20840;&#38754;&#12289;&#26377;&#24847;&#20041;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#26356;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#12289;&#26356;&#39640;&#30340;&#35299;&#37322;&#34920;&#29616;&#21147;&#65292;&#24182;&#21487;&#20197;&#25512;&#24191;&#21040;&#20854;&#20182;&#27169;&#22411;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24212;&#29992;&#20110;&#22270;&#20687;&#21040;&#25991;&#26412;&#27169;&#22411;&#26102;&#65292;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#36890;&#24120;&#25552;&#20379;&#36880;&#20010;&#26631;&#35760;&#30340;&#35299;&#37322;&#65292;&#21363;&#20026;&#25152;&#29983;&#25104;&#30340;&#24207;&#21015;&#20013;&#30340;&#27599;&#20010;&#26631;&#35760;&#35745;&#31639;&#35270;&#35273;&#35299;&#37322;&#12290;&#36825;&#20123;&#35299;&#37322;&#35745;&#31639;&#25104;&#26412;&#39640;&#65292;&#26080;&#27861;&#20840;&#38754;&#35299;&#37322;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#26576;&#31181;&#36817;&#20284;&#26041;&#27861;&#65292;&#26368;&#32456;&#20250;&#23548;&#33268;&#35823;&#23548;&#24615;&#30340;&#35299;&#37322;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;SHAP&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20801;&#35768;&#21033;&#29992;&#36755;&#20986;&#24207;&#21015;&#30340;&#21547;&#20041;&#34920;&#31034;&#29983;&#25104;&#20840;&#38754;&#12289;&#26377;&#24847;&#20041;&#30340;&#35299;&#37322;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#21033;&#29992;&#35270;&#35273;&#20027;&#24178;&#32593;&#32476;&#20013;&#30340;&#35821;&#20041;&#20808;&#39564;&#30693;&#35782;&#65292;&#25105;&#20204;&#25552;&#21462;&#20102;&#20219;&#24847;&#25968;&#37327;&#30340;&#29305;&#24449;&#65292;&#24182;&#33021;&#22815;&#22312;&#22823;&#35268;&#27169;&#27169;&#22411;&#19978;&#39640;&#25928;&#35745;&#31639;Shapley&#20540;&#65292;&#21516;&#26102;&#29983;&#25104;&#39640;&#24230;&#26126;&#30830;&#30340;&#35270;&#35273;&#35299;&#37322;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26356;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#19979;&#29983;&#25104;&#35821;&#20041;&#19978;&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;&#35299;&#37322;&#65292;&#24182;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#20854;&#20182;&#27169;&#22411;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
When applied to Image-to-text models, interpretability methods often provide token-by-token explanations namely, they compute a visual explanation for each token of the generated sequence. Those explanations are expensive to compute and unable to comprehensively explain the model's output. Therefore, these models often require some sort of approximation that eventually leads to misleading explanations. We develop a framework based on SHAP, that allows for generating comprehensive, meaningful explanations leveraging the meaning representation of the output sequence as a whole. Moreover, by exploiting semantic priors in the visual backbone, we extract an arbitrary number of features that allows the efficient computation of Shapley values on large-scale models, generating at the same time highly meaningful visual explanations. We demonstrate that our method generates semantically more expressive explanations than traditional methods at a lower compute cost and that it can be generalized o
&lt;/p&gt;</description></item><item><title>FineEHR&#21033;&#29992;&#24230;&#37327;&#23398;&#20064;&#21644;&#24494;&#35843;&#25216;&#26415;&#26469;&#20248;&#21270;&#20020;&#24202;&#35760;&#24405;&#23884;&#20837;&#65292;&#20197;&#35299;&#20915;&#21407;&#22987;&#20020;&#24202;&#25968;&#25454;&#30340;&#22797;&#26434;&#32467;&#26500;&#21644;&#22122;&#22768;&#23545;&#39044;&#27979;&#31639;&#27861;&#20934;&#30830;&#24615;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2304.11794</link><description>&lt;p&gt;
FineEHR&#65306;&#20248;&#21270;&#20020;&#24202;&#35760;&#24405;&#34920;&#31034;&#20197;&#25552;&#39640;&#27515;&#20129;&#39044;&#27979;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
FineEHR: Refine Clinical Note Representations to Improve Mortality Prediction. (arXiv:2304.11794v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11794
&lt;/p&gt;
&lt;p&gt;
FineEHR&#21033;&#29992;&#24230;&#37327;&#23398;&#20064;&#21644;&#24494;&#35843;&#25216;&#26415;&#26469;&#20248;&#21270;&#20020;&#24202;&#35760;&#24405;&#23884;&#20837;&#65292;&#20197;&#35299;&#20915;&#21407;&#22987;&#20020;&#24202;&#25968;&#25454;&#30340;&#22797;&#26434;&#32467;&#26500;&#21644;&#22122;&#22768;&#23545;&#39044;&#27979;&#31639;&#27861;&#20934;&#30830;&#24615;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30417;&#27979;&#37325;&#30151;&#30417;&#25252;&#23460;(ICU)&#24739;&#32773;&#20581;&#24247;&#29366;&#20917;&#26159;&#25552;&#20379;&#20248;&#36136;&#21307;&#30103;&#25252;&#29702;&#21644;&#27835;&#30103;&#30340;&#20851;&#38190;&#12290;&#22823;&#35268;&#27169;&#30005;&#23376;&#30149;&#21382;(EHR)&#30340;&#20986;&#29616;&#20026;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#20020;&#24202;&#25991;&#26412;&#21644;&#29983;&#21629;&#20307;&#24449;&#25968;&#25454;&#65292;&#20351;&#23427;&#20204;&#33021;&#22815;&#36827;&#34892;&#39640;&#24230;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;&#23613;&#31649;&#29616;&#22312;&#26377;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#31639;&#27861;&#29992;&#20110;&#20020;&#24202;&#35760;&#24405;&#20998;&#26512;&#65292;&#20294;&#21407;&#22987;&#20020;&#24202;&#25968;&#25454;&#20013;&#30340;&#22797;&#26434;&#25991;&#26412;&#32467;&#26500;&#21644;&#22122;&#22768;&#20173;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#27809;&#26377;&#36827;&#34892;&#29305;&#23450;&#39046;&#22495;&#25913;&#36827;&#30340;&#31895;&#30053;&#23884;&#20837;&#26041;&#27861;&#38480;&#21046;&#20102;&#36825;&#20123;&#31639;&#27861;&#30340;&#31934;&#30830;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FINEEHR&#65292;&#36825;&#26159;&#19968;&#20010;&#31995;&#32479;&#65292;&#21033;&#29992;&#20004;&#31181;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#65292;&#21363;&#24230;&#37327;&#23398;&#20064;&#21644;&#24494;&#35843;&#65292;&#26469;&#20248;&#21270;&#20020;&#24202;&#35760;&#24405;&#23884;&#20837;&#65292;&#21516;&#26102;&#21033;&#29992;&#19981;&#21516;&#20581;&#24247;&#29366;&#24577;&#21644;&#35760;&#24405;&#31867;&#21035;&#20043;&#38388;&#30340;&#20869;&#22312;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#20004;&#20010;&#25351;&#26631;n...
&lt;/p&gt;
&lt;p&gt;
Monitoring the health status of patients in the Intensive Care Unit (ICU) is a critical aspect of providing superior care and treatment. The availability of large-scale electronic health records (EHR) provides machine learning models with an abundance of clinical text and vital sign data, enabling them to make highly accurate predictions. Despite the emergence of advanced Natural Language Processing (NLP) algorithms for clinical note analysis, the complex textual structure and noise present in raw clinical data have posed significant challenges. Coarse embedding approaches without domain-specific refinement have limited the accuracy of these algorithms. To address this issue, we propose FINEEHR, a system that utilizes two representation learning techniques, namely metric learning and fine-tuning, to refine clinical note embeddings, while leveraging the intrinsic correlations among different health statuses and note categories. We evaluate the performance of FINEEHR using two metrics, n
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#30495;&#23454;&#20154;&#33080;&#33258;&#25200;&#21160;&#29983;&#25104;&#20266;&#23545;&#25239;&#24615;&#20154;&#33080;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#36825;&#31181;&#26041;&#27861;&#35757;&#32451;&#30340;&#23545;&#25239;&#24615;&#20154;&#33080;&#26816;&#27979;&#22120;&#19981;&#38656;&#25915;&#20987;&#25968;&#25454;&#21363;&#21487;&#26816;&#27979;&#26032;&#22411;&#26410;&#30693;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2304.11359</link><description>&lt;p&gt;
&#20165;&#20351;&#29992;&#30495;&#23454;&#20154;&#33080;&#33258;&#25200;&#21160;&#26816;&#27979;&#23545;&#25239;&#24615;&#20154;&#33080;
&lt;/p&gt;
&lt;p&gt;
Detecting Adversarial Faces Using Only Real Face Self-Perturbations. (arXiv:2304.11359v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11359
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#30495;&#23454;&#20154;&#33080;&#33258;&#25200;&#21160;&#29983;&#25104;&#20266;&#23545;&#25239;&#24615;&#20154;&#33080;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#36825;&#31181;&#26041;&#27861;&#35757;&#32451;&#30340;&#23545;&#25239;&#24615;&#20154;&#33080;&#26816;&#27979;&#22120;&#19981;&#38656;&#25915;&#20987;&#25968;&#25454;&#21363;&#21487;&#26816;&#27979;&#26032;&#22411;&#26410;&#30693;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#25915;&#20987;&#26088;&#22312;&#36890;&#36807;&#21521;&#36755;&#20837;&#26679;&#26412;&#28155;&#21152;&#29305;&#23450;&#22122;&#22768;&#26469;&#25200;&#20081;&#30446;&#26631;&#31995;&#32479;&#30340;&#21151;&#33021;&#65292;&#24403;&#24212;&#29992;&#20110;&#20154;&#33080;&#35782;&#21035;&#31995;&#32479;&#26102;&#65292;&#23545;&#23433;&#20840;&#24615;&#21644;&#31283;&#20581;&#24615;&#24102;&#26469;&#28508;&#22312;&#23041;&#32961;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#38450;&#24481;&#25216;&#26415;&#22312;&#26816;&#27979;&#26576;&#20123;&#29305;&#23450;&#30340;&#23545;&#25239;&#24615;&#20154;&#33080;&#65288;adv-faces&#65289;&#26041;&#38754;&#21462;&#24471;&#20102;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#20855;&#26377;&#23436;&#20840;&#19981;&#21516;&#22122;&#22768;&#27169;&#24335;&#30340;&#26032;&#25915;&#20987;&#26041;&#27861;&#23588;&#20854;&#26159;&#22522;&#20110; GAN &#30340;&#25915;&#20987;&#21017;&#32469;&#36807;&#23427;&#20204;&#24182;&#36798;&#21040;&#26356;&#39640;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;&#26356;&#31967;&#31957;&#30340;&#26159;&#65292;&#29616;&#26377;&#25216;&#26415;&#38656;&#35201;&#25915;&#20987;&#25968;&#25454;&#25165;&#33021;&#23454;&#29616;&#38450;&#24481;&#65292;&#20351;&#24471;&#38450;&#24481;&#32773;&#26080;&#27861;&#38450;&#24481;&#26410;&#34987;&#21457;&#29616;&#30340;&#26032;&#20852;&#25915;&#20987;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;adv-faces&#30340;&#20869;&#22312;&#26222;&#36941;&#24615;&#65292;&#36890;&#36807;&#20351;&#29992;&#19977;&#31181;&#21551;&#21457;&#24335;&#35774;&#35745;&#30340;&#22122;&#22768;&#27169;&#24335;&#25200;&#21160;&#30495;&#23454;&#20154;&#33080;&#26469;&#29983;&#25104;&#20266;&#23545;&#25239;&#24615;&#20154;&#33080;&#12290;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#20165;&#20351;&#29992;&#30495;&#23454;&#20154;&#33080;&#21450;&#20854;&#33258;&#25200;&#21160;&#35757;&#32451;&#23545;&#25239;&#24615;&#20154;&#33080;&#26816;&#27979;&#22120;&#30340;&#30740;&#31350;&#65292;&#19981;&#21463;&#21463;&#23475;&#32773;&#20154;&#33080;&#35782;&#21035;&#31995;&#32479;&#24433;&#21709;&#65292;&#20063;&#19981;&#21463;&#26410;&#30693;&#25915;&#20987;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial attacks aim to disturb the functionality of a target system by adding specific noise to the input samples, bringing potential threats to security and robustness when applied to facial recognition systems. Although existing defense techniques achieve high accuracy in detecting some specific adversarial faces (adv-faces), new attack methods especially GAN-based attacks with completely different noise patterns circumvent them and reach a higher attack success rate. Even worse, existing techniques require attack data before implementing the defense, making it impractical to defend newly emerging attacks that are unseen to defenders. In this paper, we investigate the intrinsic generality of adv-faces and propose to generate pseudo adv-faces by perturbing real faces with three heuristically designed noise patterns. We are the first to train an adv-face detector using only real faces and their self-perturbations, agnostic to victim facial recognition systems, and agnostic to unsee
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22823;&#35268;&#27169;&#21387;&#32553;&#38754;&#37096;&#35270;&#39057;&#36136;&#37327;&#35780;&#20272;&#65288;CFVQA&#65289;&#25968;&#25454;&#24211;&#65292;&#29992;&#20110;&#31995;&#32479;&#22320;&#20102;&#35299;&#38754;&#37096;&#35270;&#39057;&#24863;&#30693;&#36136;&#37327;&#21644;&#22810;&#26679;&#21270;&#21387;&#32553;&#22833;&#30495;&#12290;&#29983;&#25104;&#24335;&#32534;&#30721;&#26041;&#27861;&#34987;&#30830;&#23450;&#20026;&#20855;&#26377;&#21512;&#29702;&#30340;&#24863;&#30693;&#30721;&#29575;&#22833;&#30495;&#25240;&#34935;&#30340;&#26377;&#21069;&#36884;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#21033;&#29992;&#38754;&#37096;&#35270;&#39057;&#30340;&#32479;&#35745;&#20808;&#39564;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2304.07056</link><description>&lt;p&gt;
&#38754;&#37096;&#35270;&#39057;&#21387;&#32553;&#30340;&#24863;&#30693;&#36136;&#37327;&#35780;&#20272;&#65306;&#22522;&#20934;&#21644;&#26377;&#25928;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Perceptual Quality Assessment of Face Video Compression: A Benchmark and An Effective Method. (arXiv:2304.07056v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07056
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22823;&#35268;&#27169;&#21387;&#32553;&#38754;&#37096;&#35270;&#39057;&#36136;&#37327;&#35780;&#20272;&#65288;CFVQA&#65289;&#25968;&#25454;&#24211;&#65292;&#29992;&#20110;&#31995;&#32479;&#22320;&#20102;&#35299;&#38754;&#37096;&#35270;&#39057;&#24863;&#30693;&#36136;&#37327;&#21644;&#22810;&#26679;&#21270;&#21387;&#32553;&#22833;&#30495;&#12290;&#29983;&#25104;&#24335;&#32534;&#30721;&#26041;&#27861;&#34987;&#30830;&#23450;&#20026;&#20855;&#26377;&#21512;&#29702;&#30340;&#24863;&#30693;&#30721;&#29575;&#22833;&#30495;&#25240;&#34935;&#30340;&#26377;&#21069;&#36884;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#21033;&#29992;&#38754;&#37096;&#35270;&#39057;&#30340;&#32479;&#35745;&#20808;&#39564;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23545;&#38754;&#37096;&#35270;&#39057;&#21387;&#32553;&#30340;&#38656;&#27714;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#20154;&#24037;&#26234;&#33021;&#30340;&#25104;&#21151;&#20351;&#24471;&#36229;&#20986;&#20102;&#20256;&#32479;&#30340;&#28151;&#21512;&#35270;&#39057;&#32534;&#30721;&#33539;&#22260;&#12290;&#29983;&#25104;&#24335;&#32534;&#30721;&#26041;&#27861;&#34987;&#30830;&#23450;&#20026;&#20855;&#26377;&#21512;&#29702;&#30340;&#24863;&#30693;&#30721;&#29575;&#22833;&#30495;&#25240;&#34935;&#30340;&#26377;&#21069;&#36884;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#21033;&#29992;&#38754;&#37096;&#35270;&#39057;&#30340;&#32479;&#35745;&#20808;&#39564;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#31354;&#38388;&#21644;&#26102;&#38388;&#22495;&#20013;&#25197;&#26354;&#31867;&#22411;&#30340;&#26497;&#22823;&#22810;&#26679;&#24615;&#65292;&#20174;&#20256;&#32479;&#30340;&#28151;&#21512;&#32534;&#30721;&#26694;&#26550;&#21040;&#29983;&#25104;&#27169;&#22411;&#65292;&#32473;&#21387;&#32553;&#38754;&#37096;&#35270;&#39057;&#36136;&#37327;&#35780;&#20272;&#65288;VQA&#65289;&#24102;&#26469;&#20102;&#24040;&#22823;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#22823;&#35268;&#27169;&#21387;&#32553;&#38754;&#37096;&#35270;&#39057;&#36136;&#37327;&#35780;&#20272;&#65288;CFVQA&#65289;&#25968;&#25454;&#24211;&#65292;&#36825;&#26159;&#31995;&#32479;&#22320;&#20102;&#35299;&#38754;&#37096;&#35270;&#39057;&#24863;&#30693;&#36136;&#37327;&#21644;&#22810;&#26679;&#21270;&#21387;&#32553;&#22833;&#30495;&#30340;&#31532;&#19968;&#27425;&#23581;&#35797;&#12290;&#35813;&#25968;&#25454;&#24211;&#21253;&#21547; 3,240 &#20010;&#21387;&#32553;&#30340;&#38754;&#37096;&#35270;&#39057;&#29255;&#27573;&#65292;&#28085;&#30422;&#22810;&#20010;&#21387;&#32553;&#32423;&#21035;&#65292;&#36825;&#20123;&#29255;&#27573;&#26469;&#33258; 135 &#20010;&#28304;&#35270;&#39057;&#65292;&#20855;&#26377;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed an exponential increase in the demand for face video compression, and the success of artificial intelligence has expanded the boundaries beyond traditional hybrid video coding. Generative coding approaches have been identified as promising alternatives with reasonable perceptual rate-distortion trade-offs, leveraging the statistical priors of face videos. However, the great diversity of distortion types in spatial and temporal domains, ranging from the traditional hybrid coding frameworks to generative models, present grand challenges in compressed face video quality assessment (VQA). In this paper, we introduce the large-scale Compressed Face Video Quality Assessment (CFVQA) database, which is the first attempt to systematically understand the perceptual quality and diversified compression distortions in face videos. The database contains 3,240 compressed face video clips in multiple compression levels, which are derived from 135 source videos with diversif
&lt;/p&gt;</description></item><item><title>&#21069;&#20154;&#30340;&#20154;&#33080;&#38450;&#20266;&#65288;FAS&#65289;&#25216;&#26415;&#24212;&#29992;&#20110;&#23454;&#38469;&#22330;&#26223;&#20173;&#28982;&#23384;&#22312;&#38480;&#21046;&#65292;&#22240;&#20026;&#24403;&#21069;&#20844;&#24320;&#30340;FAS&#25968;&#25454;&#38598;&#25968;&#37327;&#21644;&#22810;&#26679;&#24615;&#19981;&#36275;&#65292;&#24341;&#36215;&#36807;&#25311;&#21512;&#21644;&#22330;&#26223;&#35823;&#21028;&#12290;&#36890;&#36807;&#24341;&#20837;&#37326;&#22806;&#20154;&#33080;&#38450;&#20266;&#65288;WFAS&#65289;&#25968;&#25454;&#38598;&#65292;&#35813;&#30740;&#31350;&#25552;&#39640;&#20102;&#25968;&#25454;&#35268;&#27169;&#21644;&#22810;&#26679;&#21270;&#31243;&#24230;&#65292;&#20419;&#36827;&#20102;FAS&#25216;&#26415;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2304.05753</link><description>&lt;p&gt;
&#37326;&#22806;&#20154;&#33080;&#38450;&#20266;&#25361;&#25112;&#36187;2023&#65306;&#22522;&#20934;&#21644;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
Wild Face Anti-Spoofing Challenge 2023: Benchmark and Results. (arXiv:2304.05753v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05753
&lt;/p&gt;
&lt;p&gt;
&#21069;&#20154;&#30340;&#20154;&#33080;&#38450;&#20266;&#65288;FAS&#65289;&#25216;&#26415;&#24212;&#29992;&#20110;&#23454;&#38469;&#22330;&#26223;&#20173;&#28982;&#23384;&#22312;&#38480;&#21046;&#65292;&#22240;&#20026;&#24403;&#21069;&#20844;&#24320;&#30340;FAS&#25968;&#25454;&#38598;&#25968;&#37327;&#21644;&#22810;&#26679;&#24615;&#19981;&#36275;&#65292;&#24341;&#36215;&#36807;&#25311;&#21512;&#21644;&#22330;&#26223;&#35823;&#21028;&#12290;&#36890;&#36807;&#24341;&#20837;&#37326;&#22806;&#20154;&#33080;&#38450;&#20266;&#65288;WFAS&#65289;&#25968;&#25454;&#38598;&#65292;&#35813;&#30740;&#31350;&#25552;&#39640;&#20102;&#25968;&#25454;&#35268;&#27169;&#21644;&#22810;&#26679;&#21270;&#31243;&#24230;&#65292;&#20419;&#36827;&#20102;FAS&#25216;&#26415;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#33080;&#38450;&#20266;&#65288;FAS&#65289;&#26159;&#20445;&#25252;&#33258;&#21160;&#20154;&#33080;&#35782;&#21035;&#31995;&#32479;&#23436;&#25972;&#24615;&#30340;&#37325;&#35201;&#26426;&#21046;&#12290;&#23613;&#31649;&#26377;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#23558;&#29616;&#26377;&#26041;&#27861;&#25512;&#24191;&#21040;&#23454;&#38469;&#24212;&#29992;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#36825;&#31181;&#38480;&#21046;&#21487;&#20197;&#24402;&#22240;&#20110;&#20844;&#24320;&#21487;&#29992;&#30340;FAS&#25968;&#25454;&#38598;&#30340;&#31232;&#32570;&#24615;&#21644;&#32570;&#20047;&#22810;&#26679;&#24615;&#65292;&#36825;&#32463;&#24120;&#23548;&#33268;&#35757;&#32451;&#26399;&#38388;&#36807;&#25311;&#21512;&#25110;&#27979;&#35797;&#26399;&#38388;&#39281;&#21644;&#12290;&#23601;&#25968;&#37327;&#32780;&#35328;&#65292;&#27450;&#35784;&#20027;&#20307;&#30340;&#25968;&#37327;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#20915;&#23450;&#22240;&#32032;&#12290;&#22823;&#22810;&#25968;&#25968;&#25454;&#38598;&#20165;&#21253;&#25324;&#23569;&#20110;2,000&#20010;&#21463;&#35797;&#32773;&#12290;&#23601;&#22810;&#26679;&#24615;&#32780;&#35328;&#65292;&#22823;&#22810;&#25968;&#25968;&#25454;&#38598;&#30001;&#22312;&#21463;&#25511;&#29615;&#22659;&#20013;&#20351;&#29992;&#37325;&#22797;&#26426;&#26800;&#21270;&#36807;&#31243;&#25910;&#38598;&#30340;&#27450;&#35784;&#26679;&#26412;&#32452;&#25104;&#12290;&#36825;&#31181;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#23548;&#33268;&#21516;&#36136;&#21270;&#26679;&#26412;&#21644;&#22330;&#26223;&#22810;&#26679;&#24615;&#30340;&#21294;&#20047;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#32570;&#28857;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#37326;&#22806;&#20154;&#33080;&#38450;&#20266;&#65288;WFAS&#65289;&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#12289;&#22810;&#26679;&#21270;&#30340;FAS&#25968;&#25454;&#38598;&#65292;&#21487;&#22312;&#19981;&#21463;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#25910;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Face anti-spoofing (FAS) is an essential mechanism for safeguarding the integrity of automated face recognition systems. Despite substantial advancements, the generalization of existing approaches to real-world applications remains challenging. This limitation can be attributed to the scarcity and lack of diversity in publicly available FAS datasets, which often leads to overfitting during training or saturation during testing. In terms of quantity, the number of spoof subjects is a critical determinant. Most datasets comprise fewer than 2,000 subjects. With regard to diversity, the majority of datasets consist of spoof samples collected in controlled environments using repetitive, mechanical processes. This data collection methodology results in homogenized samples and a dearth of scenario diversity. To address these shortcomings, we introduce the Wild Face Anti-Spoofing (WFAS) dataset, a large-scale, diverse FAS dataset collected in unconstrained settings. Our dataset encompasses 853
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#29983;&#25104;AI&#21512;&#25104;&#35270;&#39057;&#26469;&#21019;&#24314;&#22312;&#32447;&#25945;&#32946;&#20869;&#23481;&#30340;&#25928;&#29992;&#65292;&#20351;&#29992;&#28151;&#21512;&#26041;&#27861;&#21644;&#25104;&#24180;&#23398;&#20064;&#32773;&#36827;&#34892;&#23454;&#39564;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#21512;&#25104;&#23398;&#20064;&#35270;&#39057;&#23545;&#23398;&#20064;&#20869;&#23481;&#33719;&#21462;&#21644;&#23398;&#20064;&#20307;&#39564;&#26377;&#30528;&#31215;&#26497;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2304.03784</link><description>&lt;p&gt;
&#29983;&#25104;AI&#29992;&#20110;&#23398;&#20064;&#65306;&#30740;&#31350;&#21512;&#25104;&#23398;&#20064;&#35270;&#39057;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative AI for learning: Investigating the potential of synthetic learning videos. (arXiv:2304.03784v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03784
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#29983;&#25104;AI&#21512;&#25104;&#35270;&#39057;&#26469;&#21019;&#24314;&#22312;&#32447;&#25945;&#32946;&#20869;&#23481;&#30340;&#25928;&#29992;&#65292;&#20351;&#29992;&#28151;&#21512;&#26041;&#27861;&#21644;&#25104;&#24180;&#23398;&#20064;&#32773;&#36827;&#34892;&#23454;&#39564;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#21512;&#25104;&#23398;&#20064;&#35270;&#39057;&#23545;&#23398;&#20064;&#20869;&#23481;&#33719;&#21462;&#21644;&#23398;&#20064;&#20307;&#39564;&#26377;&#30528;&#31215;&#26497;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#24050;&#32463;&#24341;&#36215;&#20102;&#20840;&#29699;&#30340;&#20851;&#27880;&#12290;&#20687;Dalle-2&#21644;ChatGPT&#36825;&#26679;&#30340;&#24037;&#20855;&#34920;&#26126;&#65292;&#20197;&#21069;&#34987;&#35748;&#20026;&#36229;&#20986;&#20102;AI&#33021;&#21147;&#30340;&#20219;&#21153;&#29616;&#22312;&#21487;&#20197;&#20197;&#21508;&#31181;&#26032;&#26041;&#24335;&#22686;&#21152;&#21019;&#24847;&#23186;&#20307;&#30340;&#29983;&#20135;&#21147;&#65292;&#21253;&#25324;&#29983;&#25104;&#21512;&#25104;&#35270;&#39057;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#29983;&#25104;AI&#21512;&#25104;&#35270;&#39057;&#26469;&#21019;&#24314;&#22312;&#22312;&#32447;&#25945;&#32946;&#29615;&#22659;&#19979;&#21487;&#34892;&#30340;&#25945;&#32946;&#20869;&#23481;&#30340;&#25928;&#29992;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#36824;&#27809;&#26377;&#30740;&#31350;&#35843;&#26597;AI&#29983;&#25104;&#30340;&#21512;&#25104;&#23186;&#20307;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#25945;&#32946;&#20215;&#20540;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#37319;&#29992;&#28151;&#21512;&#26041;&#27861;&#38543;&#26426;&#23558;&#25104;&#24180;&#23398;&#20064;&#32773;&#65288;n = 83&#65289;&#20998;&#37197;&#21040;&#20004;&#31181;&#24494;&#22411;&#23398;&#20064;&#26465;&#20214;&#20043;&#19968;&#65292;&#25910;&#38598;&#21069;&#21518;&#23398;&#20064;&#35780;&#20272;&#65292;&#24182;&#35843;&#26597;&#21442;&#19982;&#32773;&#30340;&#23398;&#20064;&#20307;&#39564;&#12290;&#25511;&#21046;&#32452;
&lt;/p&gt;
&lt;p&gt;
Recent advances in generative artificial intelligence (AI) have captured worldwide attention. Tools such as Dalle-2 and ChatGPT suggest that tasks previously thought to be beyond the capabilities of AI may now augment the productivity of creative media in various new ways, including through the generation of synthetic video. This research paper explores the utility of using AI-generated synthetic video to create viable educational content for online educational settings. To date, there is limited research investigating the real-world educational value of AI-generated synthetic media. To address this gap, we examined the impact of using AI-generated synthetic video in an online learning platform on both learners content acquisition and learning experience. We took a mixed-method approach, randomly assigning adult learners (n=83) into one of two micro-learning conditions, collecting pre- and post-learning assessments, and surveying participants on their learning experience. The control c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35266;&#23519;&#21040;&#22522;&#20110;DPR&#30340;&#26368;&#36817;&#30340;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#32463;&#24120;&#23558;&#26080;&#27861;&#22238;&#31572;&#30340;&#21453;&#20107;&#23454;&#24773;&#26223;&#25490;&#21517;&#39640;&#20110;&#21487;&#22238;&#31572;&#30340;&#21407;&#22987;&#24773;&#26223;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#27573;&#33853;&#26816;&#32034;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;PiCL&#12290;</title><link>http://arxiv.org/abs/2304.03031</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#24102;&#26377;&#26080;&#27861;&#22238;&#31572;&#30340;&#21453;&#20107;&#23454;&#24773;&#26223;&#30340;&#23494;&#38598;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Revisiting Dense Retrieval with Unanswerable Counterfactuals. (arXiv:2304.03031v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03031
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35266;&#23519;&#21040;&#22522;&#20110;DPR&#30340;&#26368;&#36817;&#30340;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#32463;&#24120;&#23558;&#26080;&#27861;&#22238;&#31572;&#30340;&#21453;&#20107;&#23454;&#24773;&#26223;&#25490;&#21517;&#39640;&#20110;&#21487;&#22238;&#31572;&#30340;&#21407;&#22987;&#24773;&#26223;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#27573;&#33853;&#26816;&#32034;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;PiCL&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#65288;ODQA&#65289;&#20013;&#65292;&#26816;&#32034;&#22120;-&#38405;&#35835;&#22120;&#26694;&#26550;&#24456;&#21463;&#27426;&#36814;&#65292;&#20854;&#20013;&#26816;&#32034;&#22120;&#20174;&#22823;&#22411;&#35821;&#26009;&#24211;&#20013;&#20026;&#38405;&#35835;&#22120;&#25277;&#21462;&#19968;&#32452;&#30456;&#20851;&#30340;&#20505;&#36873;&#27573;&#33853;&#12290;&#36825;&#31181;&#26041;&#27861;&#32972;&#21518;&#30340;&#19968;&#20010;&#20851;&#38190;&#20551;&#35774;&#26159;&#65292;&#20174;&#26816;&#32034;&#22120;&#24471;&#21040;&#30340;&#39640;&#30456;&#20851;&#24615;&#20998;&#25968;&#21487;&#33021;&#34920;&#26126;&#20174;&#38405;&#35835;&#22120;&#33719;&#21462;&#31572;&#26696;&#30340;&#21487;&#33021;&#24615;&#24456;&#39640;&#65292;&#36825;&#24847;&#21619;&#30528;&#26816;&#32034;&#21040;&#30340;&#27573;&#33853;&#24456;&#21487;&#33021;&#21253;&#21547;&#32473;&#23450;&#38382;&#39064;&#30340;&#31572;&#26696;&#12290;&#25105;&#20204;&#22312;&#26412;&#30740;&#31350;&#20013;&#23454;&#35777;&#39539;&#26021;&#20102;&#36825;&#31181;&#35266;&#28857;&#65292;&#24182;&#35266;&#23519;&#21040;&#22522;&#20110;DPR&#30340;&#26368;&#36817;&#30340;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#32463;&#24120;&#23558;&#26080;&#27861;&#22238;&#31572;&#30340;&#21453;&#20107;&#23454;&#24773;&#26223;&#25490;&#21517;&#39640;&#20110;&#21487;&#22238;&#31572;&#30340;&#21407;&#22987;&#24773;&#26223;&#12290;&#20026;&#20102;&#35299;&#20915;&#23494;&#38598;&#26816;&#32034;&#20013;&#36825;&#31181;&#23545;&#31572;&#26696;&#26080;&#24863;&#30693;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#23547;&#27714;&#20351;&#29992;&#21453;&#20107;&#23454;&#26679;&#26412;&#20316;&#20026;&#39069;&#22806;&#30340;&#35757;&#32451;&#36164;&#28304;&#65292;&#20197;&#26356;&#22909;&#22320;&#21516;&#27493;DPR&#30340;&#30456;&#20851;&#24615;&#27979;&#37327;&#21644;&#38382;&#39064;-&#27573;&#33853;&#23545;&#30340;&#21487;&#31572;&#24615;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21453;&#20107;&#23454;Pivoting&#23545;&#27604;&#23398;&#20064;&#65288;PiCL&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#27573;&#33853;&#26816;&#32034;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The retriever-reader framework is popular for open-domain question answering (ODQA), where a retriever samples for the reader a set of relevant candidate passages from a large corpus. A key assumption behind this method is that high relevance scores from the retriever likely indicate high answerability from the reader, which implies a high probability that the retrieved passages contain answers to a given question. In this work, we empirically dispel this belief and observe that recent dense retrieval models based on DPR often rank unanswerable counterfactual passages higher than their answerable original passages. To address such answer-unawareness in dense retrievers, we seek to use counterfactual samples as additional training resources to better synchronize the relevance measurement of DPR with the answerability of question-passage pairs. Specifically, we present counterfactually-Pivoting Contrastive Learning (PiCL), a novel representation learning approach for passage retrieval th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;&#21382;&#31243;&#20197;&#21450;&#26368;&#36817;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLMs)&#65292;&#24182;&#24378;&#35843;&#27169;&#22411;&#25193;&#23637;&#23558;&#24102;&#26469;&#24615;&#33021;&#25913;&#36827;&#21644;&#29305;&#27530;&#33021;&#21147;&#30340;&#21457;&#25496;&#12290;</title><link>http://arxiv.org/abs/2303.18223</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey of Large Language Models. (arXiv:2303.18223v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18223
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;&#21382;&#31243;&#20197;&#21450;&#26368;&#36817;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLMs)&#65292;&#24182;&#24378;&#35843;&#27169;&#22411;&#25193;&#23637;&#23558;&#24102;&#26469;&#24615;&#33021;&#25913;&#36827;&#21644;&#29305;&#27530;&#33021;&#21147;&#30340;&#21457;&#25496;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#26412;&#36136;&#19978;&#26159;&#19968;&#20010;&#30001;&#35821;&#27861;&#35268;&#21017;&#25511;&#21046;&#30340;&#22797;&#26434;&#31934;&#32454;&#30340;&#20154;&#31867;&#34920;&#36798;&#31995;&#32479;&#65292;&#23545;&#20110;&#24320;&#21457;&#29702;&#35299;&#21644;&#25484;&#25569;&#35821;&#35328;&#30340;&#33021;&#21147;&#30340;AI&#31639;&#27861;&#26469;&#35828;&#26159;&#19968;&#39033;&#37325;&#22823;&#25361;&#25112;&#12290;&#20316;&#20026;&#20027;&#35201;&#26041;&#27861;&#20043;&#19968;&#65292;&#35821;&#35328;&#24314;&#27169;&#22312;&#36807;&#21435;&#20108;&#21313;&#24180;&#37324;&#24191;&#27867;&#30740;&#31350;&#29992;&#20110;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#65292;&#20174;&#32479;&#35745;&#35821;&#35328;&#27169;&#22411;&#28436;&#21270;&#20026;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#12290;&#26368;&#36817;&#65292;&#36890;&#36807;&#22312;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#19978;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#65292;&#22312;&#35299;&#20915;&#21508;&#31181;NLP&#20219;&#21153;&#26041;&#38754;&#26174;&#31034;&#20986;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;&#30001;&#20110;&#30740;&#31350;&#20154;&#21592;&#21457;&#29616;&#27169;&#22411;&#32553;&#25918;&#21487;&#20197;&#23548;&#33268;&#24615;&#33021;&#25913;&#36827;&#65292;&#20182;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#22686;&#21152;&#27169;&#22411;&#35268;&#27169;&#26469;&#30740;&#31350;&#32553;&#25918;&#25928;&#24212;&#65292;&#26377;&#36259;&#30340;&#26159;&#65292;&#24403;&#21442;&#25968;&#35268;&#27169;&#36229;&#36807;&#19968;&#23450;&#27700;&#24179;&#26102;&#65292;&#36825;&#20123;&#25193;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#19981;&#20165;&#21487;&#20197;&#23454;&#29616;&#26174;&#30528;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#32780;&#19988;&#36824;&#26174;&#31034;&#20986;&#19968;&#20123;&#23567;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#25152;&#27809;&#26377;&#30340;&#29305;&#27530;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale langu
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#35821;&#35328;&#22270;&#20687;&#39044;&#35757;&#32451;&#30340;&#25104;&#23545;Sigmoid&#25439;&#22833;&#20989;&#25968;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#39640;&#35757;&#32451;&#25209;&#37327;&#22823;&#23567;&#65292;&#21516;&#26102;&#19981;&#38656;&#35201;&#20840;&#23616;&#26597;&#30475;&#37197;&#23545;&#30456;&#20284;&#24615;&#36827;&#34892;&#24402;&#19968;&#21270;&#65292;&#20854;&#35757;&#32451;&#20986;&#26469;&#30340;&#27169;&#22411;&#22312;ImageNet&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2303.15343</link><description>&lt;p&gt;
Sigmoid Loss&#29992;&#20110;&#35821;&#35328;&#22270;&#20687;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Sigmoid Loss for Language Image Pre-Training. (arXiv:2303.15343v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15343
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#35821;&#35328;&#22270;&#20687;&#39044;&#35757;&#32451;&#30340;&#25104;&#23545;Sigmoid&#25439;&#22833;&#20989;&#25968;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#39640;&#35757;&#32451;&#25209;&#37327;&#22823;&#23567;&#65292;&#21516;&#26102;&#19981;&#38656;&#35201;&#20840;&#23616;&#26597;&#30475;&#37197;&#23545;&#30456;&#20284;&#24615;&#36827;&#34892;&#24402;&#19968;&#21270;&#65292;&#20854;&#35757;&#32451;&#20986;&#26469;&#30340;&#27169;&#22411;&#22312;ImageNet&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#25104;&#23545;Sigmoid&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#22270;&#20687;-&#25991;&#26412;&#39044;&#35757;&#32451;&#12290;&#19982;&#26631;&#20934;&#30340;&#20855;&#26377;softmax&#24402;&#19968;&#21270;&#30340;&#23545;&#27604;&#23398;&#20064;&#19981;&#21516;&#65292;Sigmoid&#25439;&#22833;&#21482;&#25805;&#20316;&#22270;&#20687;-&#25991;&#26412;&#23545;&#65292;&#19981;&#38656;&#35201;&#20840;&#23616;&#26597;&#30475;&#37197;&#23545;&#30456;&#20284;&#24615;&#20197;&#36827;&#34892;&#24402;&#19968;&#21270;&#12290;Sigmoid&#25439;&#22833;&#21516;&#26102;&#20351;&#25209;&#37327;&#22823;&#23567;&#36827;&#19968;&#27493;&#22686;&#21152;&#65292;&#24182;&#21487;&#22312;&#36739;&#23567;&#30340;&#25209;&#37327;&#22823;&#23567;&#19979;&#34920;&#29616;&#26356;&#22909;&#12290;&#20165;&#20351;&#29992;&#22235;&#20010;TPUv4&#33455;&#29255;&#65292;&#25105;&#20204;&#23601;&#33021;&#22312;4k&#25209;&#37327;&#22823;&#23567;&#19979;&#35757;&#32451;&#20986;&#19968;&#20010;Base CLIP&#27169;&#22411;&#21644;&#22312;20k&#25209;&#37327;&#22823;&#23567;&#19979;&#35757;&#32451;&#20986;&#19968;&#20010;&#22823;&#35268;&#27169;LiT&#27169;&#22411;&#65292;&#21518;&#32773;&#22312;&#20004;&#22825;&#20869;&#23454;&#29616;&#20102;84.5%&#30340;ImageNet&#38646;&#26679;&#26412;&#20934;&#30830;&#29575;&#12290;&#36825;&#31181;&#25439;&#22833;&#20989;&#25968;&#23558;&#25209;&#37327;&#22823;&#23567;&#19982;&#25439;&#22833;&#20989;&#25968;&#20998;&#31163;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#30740;&#31350;&#31034;&#20363;&#19982;&#23545;&#20043;&#38388;&#12289;&#36127;-&#27491;&#20363;&#27604;&#29575;&#30340;&#24433;&#21709;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#25209;&#37327;&#22823;&#23567;&#25512;&#21040;&#26497;&#38480;&#65292;&#39640;&#36798;&#19968;&#30334;&#19975;&#65292;&#21457;&#29616;&#25193;&#22823;&#25209;&#37327;&#22823;&#23567;&#30340;&#22909;&#22788;&#24456;&#24555;&#23601;&#20250;&#20943;&#24369;&#65292;32k&#25209;&#37327;&#22823;&#23567;&#24050;&#32463;&#36275;&#22815;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#30740;&#31350;&#33021;&#22815;&#28608;&#21457;&#36827;&#19968;&#27493;&#25506;&#32034;&#22914;&#20309;&#25552;&#39640;&#36136;&#37327;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a simple pairwise sigmoid loss for image-text pre-training. Unlike standard contrastive learning with softmax normalization, the sigmoid loss operates solely on image-text pairs and does not require a global view of the pairwise similarities for normalization. The sigmoid loss simultaneously allows further scaling up the batch size, while also performing better at smaller batch sizes. With only four TPUv4 chips, we can train a Base CLIP model at 4k batch size and a Large LiT model at 20k batch size, the latter achieves 84.5% ImageNet zero-shot accuracy in two days. This disentanglement of the batch size from the loss further allows us to study the impact of examples vs pairs and negative to positive ratio. Finally, we push the batch size to the extreme, up to one million, and find that the benefits of growing batch size quickly diminish, with a more reasonable batch size of 32k being sufficient. We hope our research motivates further explorations in improving the quality and
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26679;&#21270;&#21644;&#36924;&#30495;&#30340;&#22686;&#24378;&#26041;&#27861;&#65292;&#21487;&#20197;&#21019;&#24314;&#25972;&#20307;&#23545;&#35937;&#24182;&#28789;&#27963;&#22320;&#23450;&#20301;&#21644;&#26059;&#36716;&#23545;&#35937;&#65292;&#24182;&#30456;&#24212;&#22320;&#24212;&#29992;&#33258;&#36974;&#25377;&#21644;&#22806;&#36974;&#25377;&#12290;&#36890;&#36807;&#36845;&#20195;&#26500;&#24314;&#22810;&#20010;&#23545;&#35937;&#26469;&#25552;&#39640;&#25972;&#20307;&#23545;&#35937;&#26500;&#36896;&#30340;&#22810;&#26679;&#24615;&#65292;&#26500;&#36896;&#30340;&#23545;&#35937;&#21487;&#20197;&#22312;&#35757;&#32451;&#24103;&#20013;&#38543;&#26426;&#25918;&#32622;&#21644;&#26059;&#36716;&#12290;</title><link>http://arxiv.org/abs/2303.12743</link><description>&lt;p&gt;
DR.CPO&#65306;&#36890;&#36807;&#36845;&#20195;&#26500;&#24314;&#12289;&#38543;&#26426;&#25918;&#32622;&#21644; HPR &#36974;&#34109;&#23454;&#29616;&#30340;&#22810;&#26679;&#21270;&#21644;&#36924;&#30495;&#30340;&#19977;&#32500;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
DR.CPO: Diversified and Realistic 3D Augmentation via Iterative Construction, Random Placement, and HPR Occlusion. (arXiv:2303.12743v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12743
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26679;&#21270;&#21644;&#36924;&#30495;&#30340;&#22686;&#24378;&#26041;&#27861;&#65292;&#21487;&#20197;&#21019;&#24314;&#25972;&#20307;&#23545;&#35937;&#24182;&#28789;&#27963;&#22320;&#23450;&#20301;&#21644;&#26059;&#36716;&#23545;&#35937;&#65292;&#24182;&#30456;&#24212;&#22320;&#24212;&#29992;&#33258;&#36974;&#25377;&#21644;&#22806;&#36974;&#25377;&#12290;&#36890;&#36807;&#36845;&#20195;&#26500;&#24314;&#22810;&#20010;&#23545;&#35937;&#26469;&#25552;&#39640;&#25972;&#20307;&#23545;&#35937;&#26500;&#36896;&#30340;&#22810;&#26679;&#24615;&#65292;&#26500;&#36896;&#30340;&#23545;&#35937;&#21487;&#20197;&#22312;&#35757;&#32451;&#24103;&#20013;&#38543;&#26426;&#25918;&#32622;&#21644;&#26059;&#36716;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#65292;&#25968;&#25454;&#22686;&#24378;&#24120;&#29992;&#20110;&#25913;&#36827;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#12290;&#26368;&#22522;&#26412;&#30340;&#26041;&#27861;&#21253;&#25324;&#25554;&#20837;&#22797;&#21046;&#23545;&#35937;&#21644;&#26059;&#36716;&#21644;&#32553;&#25918;&#25972;&#20010;&#35757;&#32451;&#24103;&#12290;&#20063;&#24050;&#32463;&#24320;&#21457;&#20102;&#35768;&#22810;&#21464;&#20307;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#19982;&#29616;&#23454;&#19990;&#30028;&#30340;&#21487;&#33021;&#24615;&#30456;&#27604;&#30456;&#24403;&#26377;&#38480;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22810;&#26679;&#21270;&#21644;&#36924;&#30495;&#22686;&#24378;&#26041;&#27861;&#65292;&#21487;&#20197;&#28789;&#27963;&#22320;&#26500;&#36896;&#25972;&#20307;&#23545;&#35937;&#65292;&#33258;&#30001;&#22320;&#23450;&#20301;&#21644;&#26059;&#36716;&#23545;&#35937;&#65292;&#24182;&#30456;&#24212;&#22320;&#24212;&#29992;&#33258;&#36974;&#25377;&#21644;&#22806;&#36974;&#25377;&#12290;&#20026;&#20102;&#25552;&#39640;&#25972;&#20307;&#23545;&#35937;&#26500;&#36896;&#30340;&#22810;&#26679;&#24615;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#36845;&#20195;&#26041;&#27861;&#65292;&#23558;&#20174;&#29616;&#23454;&#19990;&#30028;&#35266;&#23519;&#21040;&#30340;&#22810;&#20010;&#23545;&#35937;&#38543;&#26426;&#32452;&#21512;&#25104;&#21333;&#20010;&#23545;&#35937;&#12290;&#19982;&#29616;&#26377;&#22686;&#24378;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#26500;&#36896;&#30340;&#23545;&#35937;&#21487;&#20197;&#38543;&#26426;&#25918;&#32622;&#21644;&#26059;&#36716;&#22312;&#35757;&#32451;&#24103;&#20013;&#65292;&#22240;&#20026;&#36866;&#24403;&#30340;&#36974;&#25377;&#21487;&#20197;&#21453;&#26144;&#22312;&#26368;&#32456;&#25972;&#20307;&#23545;&#35937;&#20013;&#12290;&#26368;&#21518;&#65292;&#20026;&#20102;&#38450;&#27490;&#36807;&#24230;&#22686;&#24378;&#23548;&#33268;&#36807;&#25311;&#21512;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#20998;&#23618;&#36974;&#25377;&#27010;&#29575;&#35774;&#32622;&#65292;&#36890;&#36807;&#23545;&#35937;&#30340;&#20301;&#32622;&#21644;&#22823;&#23567;&#35843;&#25972;&#36974;&#25377;&#24378;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
In autonomous driving, data augmentation is commonly used for improving 3D object detection. The most basic methods include insertion of copied objects and rotation and scaling of the entire training frame. Numerous variants have been developed as well. The existing methods, however, are considerably limited when compared to the variety of the real world possibilities. In this work, we develop a diversified and realistic augmentation method that can flexibly construct a whole-body object, freely locate and rotate the object, and apply self-occlusion and external-occlusion accordingly. To improve the diversity of the whole-body object construction, we develop an iterative method that stochastically combines multiple objects observed from the real world into a single object. Unlike the existing augmentation methods, the constructed objects can be randomly located and rotated in the training frame because proper occlusions can be reflected to the whole-body objects in the final step. Fina
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38754;&#21521;&#39046;&#22495;&#30340;&#39044;&#35757;&#32451;&#23545;&#25972;&#24352;&#20999;&#29255;&#22270;&#20687;&#20998;&#31867;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;&#20351;&#29992;&#38754;&#21521;&#39046;&#22495;&#30340;&#39044;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#22810;&#23454;&#20363;&#23398;&#20064;&#27169;&#22411;&#22312;&#20999;&#29255;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.09833</link><description>&lt;p&gt;
&#38754;&#21521;&#39046;&#22495;&#30340;&#39044;&#35757;&#32451;&#25552;&#39640;&#20102;&#25972;&#24352;&#20999;&#29255;&#22270;&#20687;&#20998;&#31867;&#30340;&#20449;&#24515;
&lt;/p&gt;
&lt;p&gt;
Domain-Specific Pre-training Improves Confidence in Whole Slide Image Classification. (arXiv:2302.09833v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09833
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38754;&#21521;&#39046;&#22495;&#30340;&#39044;&#35757;&#32451;&#23545;&#25972;&#24352;&#20999;&#29255;&#22270;&#20687;&#20998;&#31867;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;&#20351;&#29992;&#38754;&#21521;&#39046;&#22495;&#30340;&#39044;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#22810;&#23454;&#20363;&#23398;&#20064;&#27169;&#22411;&#22312;&#20999;&#29255;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25972;&#24352;&#20999;&#29255;&#22270;&#20687;&#25110;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#22312;&#25968;&#23383;&#30149;&#29702;&#23398;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#30001;&#20110;&#20854;&#22823;&#23567;&#21644;&#32570;&#20047;&#20687;&#32032;&#32423;&#27880;&#37322;&#65292;&#25972;&#24352;&#20999;&#29255;&#22270;&#20687;&#23545;&#20110;&#20020;&#24202;&#35786;&#26029;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26500;&#25104;&#20102;&#24040;&#22823;&#25361;&#25112;&#12290;&#26368;&#36817;&#22312;&#35745;&#31639;&#30149;&#29702;&#23398;&#39046;&#22495;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#22810;&#23454;&#20363;&#23398;&#20064;&#30340;&#26032;&#27169;&#22411;&#12290;&#22810;&#23454;&#20363;&#23398;&#20064;&#38656;&#35201;&#21019;&#24314;&#34917;&#19969;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#34917;&#19969;&#30340;&#32534;&#30721;&#36827;&#34892;&#35786;&#26029;&#12290;&#36825;&#20123;&#27169;&#22411;&#20351;&#29992;&#36890;&#29992;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;&#22312;ImageNet&#19978;&#39044;&#35757;&#32451;&#30340;ResNet-50&#65289;&#36827;&#34892;&#34917;&#19969;&#32534;&#30721;&#12290;&#26368;&#36817;&#25552;&#20986;&#30340;KimiaNet&#26159;&#19968;&#31181;&#22522;&#20110;DenseNet121&#30340;&#38754;&#21521;&#39046;&#22495;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#26159;&#22312;TCGA&#20999;&#29255;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#38754;&#21521;&#39046;&#22495;&#30340;&#39044;&#35757;&#32451;&#23545;&#25972;&#24352;&#20999;&#29255;&#22270;&#20687;&#20998;&#31867;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#35843;&#26597;&#38754;&#21521;&#39046;&#22495;&#30340;&#39044;&#35757;&#32451;&#30340;&#25928;&#26524;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#22810;&#23454;&#20363;&#23398;&#20064;&#27169;&#22411;&#65306;1&#65289;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;CLAM&#27169;&#22411;&#21644;2&#65289;&#33258;&#27880;&#24847;&#30340;TransMIL&#27169;&#22411;&#65292;&#24182;&#35780;&#20272;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Whole Slide Images (WSIs) or histopathology images are used in digital pathology. WSIs pose great challenges to deep learning models for clinical diagnosis, owing to their size and lack of pixel-level annotations. With the recent advancements in computational pathology, newer multiple-instance learning-based models have been proposed. Multiple-instance learning for WSIs necessitates creating patches and uses the encoding of these patches for diagnosis. These models use generic pre-trained models (ResNet-50 pre-trained on ImageNet) for patch encoding. The recently proposed KimiaNet, a DenseNet121 model pre-trained on TCGA slides, is a domain-specific pre-trained model. This paper shows the effect of domain-specific pre-training on WSI classification. To investigate the effect of domain-specific pre-training, we considered the current state-of-the-art multiple-instance learning models, 1) CLAM, an attention-based model, and 2) TransMIL, a self-attention-based model, and evaluated the mod
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#29992;&#20110;&#32467;&#32928;&#29983;&#38271;&#35786;&#26029;&#30340;&#27785;&#28024;&#24335;&#34394;&#25311;&#32467;&#32928;&#38236;&#26597;&#30475;&#22120;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#30830;&#23450;&#20102;&#20854;&#35206;&#30422;&#33539;&#22260;&#12289;&#25345;&#32493;&#26102;&#38388;&#21644;&#35786;&#26029;&#20934;&#30830;&#24615;&#31561;&#26041;&#38754;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2302.02946</link><description>&lt;p&gt;
&#24320;&#21457;&#29992;&#20110;&#32467;&#32928;&#29983;&#38271;&#35786;&#26029;&#30340;&#27785;&#28024;&#24335;&#34394;&#25311;&#32467;&#32928;&#38236;&#26597;&#30475;&#22120;
&lt;/p&gt;
&lt;p&gt;
Development of an Immersive Virtual Colonoscopy Viewer for Colon Growths Diagnosis. (arXiv:2302.02946v2 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02946
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#29992;&#20110;&#32467;&#32928;&#29983;&#38271;&#35786;&#26029;&#30340;&#27785;&#28024;&#24335;&#34394;&#25311;&#32467;&#32928;&#38236;&#26597;&#30475;&#22120;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#30830;&#23450;&#20102;&#20854;&#35206;&#30422;&#33539;&#22260;&#12289;&#25345;&#32493;&#26102;&#38388;&#21644;&#35786;&#26029;&#20934;&#30830;&#24615;&#31561;&#26041;&#38754;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26700;&#38754;&#34394;&#25311;&#32467;&#32928;&#38236;&#24050;&#34987;&#35777;&#26126;&#22312;&#21457;&#29616;&#32467;&#32928;&#24322;&#24120;&#26041;&#38754;&#26159;&#19968;&#31181;&#26377;&#29992;&#30340;&#24037;&#20855;&#12290;&#34429;&#28982;&#36825;&#19968;&#36807;&#31243;&#20934;&#30830;&#65292;&#20294;&#32791;&#26102;&#36739;&#38271;&#12290;&#27785;&#28024;&#24335;&#30028;&#38754;&#22312;&#34394;&#25311;&#32467;&#32928;&#38236;&#20013;&#30340;&#24212;&#29992;&#23578;&#22788;&#20110;&#21021;&#27493;&#38454;&#27573;&#65292;&#23578;&#26410;&#34987;&#20805;&#20998;&#29702;&#35299;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35774;&#35745;&#65292;&#25506;&#32034;&#34394;&#25311;&#29616;&#23454;&#33539;&#24335;&#20803;&#32032;&#65292;&#20197;&#20351;&#27785;&#28024;&#24335;&#20998;&#26512;&#26356;&#21152;&#39640;&#25928;&#32780;&#20173;&#28982;&#26377;&#25928;&#12290;&#25105;&#20204;&#36824;&#35745;&#21010;&#36827;&#34892;&#19987;&#23478;&#23454;&#39564;&#65292;&#35780;&#20272;&#35206;&#30422;&#33539;&#22260;&#12289;&#25345;&#32493;&#26102;&#38388;&#21644;&#35786;&#26029;&#20934;&#30830;&#24615;&#31561;&#22810;&#20010;&#22240;&#32032;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Desktop-based virtual colonoscopy has been proven to be an asset in the identification of colon anomalies. The process is accurate, although time-consuming. The use of immersive interfaces for virtual colonoscopy is incipient and not yet understood. In this work, we present a new design exploring elements of the VR paradigm to make the immersive analysis more efficient while still effective. We also plan the conduction of experiments with experts to assess the multi-factor influences of coverage, duration, and diagnostic accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26159;&#23545;&#39640;&#25928;&#35757;&#32451;Transformer&#39046;&#22495;&#30340;&#31995;&#32479;&#32508;&#36848;&#65292;&#20998;&#26512;&#21644;&#27604;&#36739;&#20102;&#22312;&#35757;&#32451;&#20013;&#20026;&#20013;&#38388;&#24352;&#37327;&#33410;&#30465;&#35745;&#31639;&#21644;&#20869;&#23384;&#25104;&#26412;&#30340;&#26041;&#27861;&#19982;&#30828;&#20214;/&#31639;&#27861;&#20849;&#21516;&#35774;&#35745;&#30340;&#25216;&#26415;&#65292;&#24182;&#25506;&#35752;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#25361;&#25112;&#21644;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2302.01107</link><description>&lt;p&gt;
Transformers&#35757;&#32451;&#30340;&#39640;&#25928;&#26041;&#27861;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Efficient Training of Transformers. (arXiv:2302.01107v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01107
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26159;&#23545;&#39640;&#25928;&#35757;&#32451;Transformer&#39046;&#22495;&#30340;&#31995;&#32479;&#32508;&#36848;&#65292;&#20998;&#26512;&#21644;&#27604;&#36739;&#20102;&#22312;&#35757;&#32451;&#20013;&#20026;&#20013;&#38388;&#24352;&#37327;&#33410;&#30465;&#35745;&#31639;&#21644;&#20869;&#23384;&#25104;&#26412;&#30340;&#26041;&#27861;&#19982;&#30828;&#20214;/&#31639;&#27861;&#20849;&#21516;&#35774;&#35745;&#30340;&#25216;&#26415;&#65292;&#24182;&#25506;&#35752;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#25361;&#25112;&#21644;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;Transformers&#30340;&#21457;&#23637;&#20026;&#35745;&#31639;&#36164;&#28304;&#25552;&#20986;&#20102;&#24040;&#22823;&#35201;&#27714;&#65292;&#24378;&#35843;&#20102;&#24320;&#21457;&#39640;&#25928;&#35757;&#32451;&#25216;&#26415;&#20197;&#36890;&#36807;&#26377;&#25928;&#21033;&#29992;&#35745;&#31639;&#21644;&#20869;&#23384;&#36164;&#28304;&#20351;Transformer&#30340;&#35757;&#32451;&#26356;&#24555;&#12289;&#26356;&#20302;&#25104;&#26412;&#19988;&#26356;&#39640;&#31934;&#24230;&#30340;&#37325;&#35201;&#24615;&#12290;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#39640;&#25928;&#35757;&#32451;Transformer&#30340;&#39318;&#20010;&#31995;&#32479;&#32508;&#36848;&#65292;&#35206;&#30422;&#20102;&#21152;&#36895;&#31639;&#26415;&#21644;&#30828;&#20214;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#37325;&#28857;&#20851;&#27880;&#21069;&#32773;&#12290;&#25105;&#20204;&#20998;&#26512;&#21644;&#27604;&#36739;&#20102;&#22312;&#35757;&#32451;&#26399;&#38388;&#20026;&#20013;&#38388;&#24352;&#37327;&#33410;&#30465;&#35745;&#31639;&#21644;&#20869;&#23384;&#25104;&#26412;&#30340;&#26041;&#27861;&#65292;&#20197;&#21450;&#30828;&#20214;/&#31639;&#27861;&#20849;&#21516;&#35774;&#35745;&#30340;&#25216;&#26415;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#25361;&#25112;&#21644;&#26377;&#21069;&#36884;&#30340;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in Transformers have come with a huge requirement on computing resources, highlighting the importance of developing efficient training techniques to make Transformer training faster, at lower cost, and to higher accuracy by the efficient use of computation and memory resources. This survey provides the first systematic overview of the efficient training of Transformers, covering the recent progress in acceleration arithmetic and hardware, with a focus on the former. We analyze and compare methods that save computation and memory costs for intermediate tensors during training, together with techniques on hardware/algorithm co-design. We finally discuss challenges and promising areas for future research.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RTD-AE&#30340;&#26041;&#27861;&#29992;&#20110;&#23398;&#20064;&#20445;&#25345;&#25968;&#25454;&#25299;&#25169;&#32467;&#26500;&#30340;&#38477;&#32500;&#34920;&#31034;,&#22312;&#20445;&#30041;&#20840;&#23616;&#32467;&#26500;&#21644;&#25299;&#25169;&#24615;&#26041;&#38754;&#65292;&#20854;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#31454;&#20105;&#23545;&#25163;&#12290;</title><link>http://arxiv.org/abs/2302.00136</link><description>&lt;p&gt;
&#23398;&#20064;&#20445;&#25345;&#25299;&#25169;&#32467;&#26500;&#30340;&#25968;&#25454;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning Topology-Preserving Data Representations. (arXiv:2302.00136v2 [cs.LG] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00136
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RTD-AE&#30340;&#26041;&#27861;&#29992;&#20110;&#23398;&#20064;&#20445;&#25345;&#25968;&#25454;&#25299;&#25169;&#32467;&#26500;&#30340;&#38477;&#32500;&#34920;&#31034;,&#22312;&#20445;&#30041;&#20840;&#23616;&#32467;&#26500;&#21644;&#25299;&#25169;&#24615;&#26041;&#38754;&#65292;&#20854;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#31454;&#20105;&#23545;&#25163;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23398;&#20064;&#25299;&#25169;&#20445;&#25345;&#25968;&#25454;&#34920;&#31034;&#65288;&#38477;&#32500;&#65289;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#24378;&#21046;&#25299;&#25169;&#29305;&#24449;&#65288;&#32858;&#31867;&#12289;&#29615;&#12289;2D&#31354;&#27934;&#31561;&#65289;&#21450;&#20854;&#26412;&#22320;&#21270;&#30340;&#30456;&#20284;&#24615;&#25552;&#20379;&#25968;&#25454;&#27969;&#24418;&#21644;&#20854;&#28508;&#22312;&#34920;&#31034;&#20043;&#38388;&#30340;&#25299;&#25169;&#30456;&#20284;&#24615;&#12290;&#35813;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#22312;&#21407;&#22987;&#39640;&#32500;&#25968;&#25454;&#21644;&#20302;&#32500;&#34920;&#31034;&#20043;&#38388;&#26368;&#23567;&#21270;&#34920;&#31034;&#25299;&#25169;&#25955;&#24230;&#65288;RTD&#65289;&#12290;RTD&#26368;&#23567;&#21270;&#25552;&#20379;&#20102;&#24378;&#26377;&#21147;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#25299;&#25169;&#29305;&#24449;&#30340;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;RTD&#20998;&#21270;&#26041;&#26696;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20026;&#33258;&#32534;&#30721;&#22120;&#25439;&#22833;&#39033;&#12290; RTD-AE&#26041;&#27861;&#19982;&#29616;&#26377;&#26368;&#20808;&#36827;&#31454;&#20105;&#23545;&#25163;&#30456;&#27604;&#65292;&#36890;&#36807;&#32447;&#24615;&#30456;&#20851;&#12289;&#19977;&#37325;&#36317;&#31163;&#25490;&#21517;&#20934;&#30830;&#24615;&#20197;&#21450;&#25345;&#20037;&#26465;&#24418;&#30721;&#20043;&#38388;&#30340;Wasserstein&#36317;&#31163;&#31561;&#27979;&#37327;&#65292;&#26356;&#22909;&#22320;&#20445;&#30041;&#20102;&#25968;&#25454;&#27969;&#24418;&#30340;&#20840;&#23616;&#32467;&#26500;&#21644;&#25299;&#25169;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a method for learning topology-preserving data representations (dimensionality reduction). The method aims to provide topological similarity between the data manifold and its latent representation via enforcing the similarity in topological features (clusters, loops, 2D voids, etc.) and their localization. The core of the method is the minimization of the Representation Topology Divergence (RTD) between original high-dimensional data and low-dimensional representation in latent space. RTD minimization provides closeness in topological features with strong theoretical guarantees. We develop a scheme for RTD differentiation and apply it as a loss term for the autoencoder. The proposed method "RTD-AE" better preserves the global structure and topology of the data manifold than state-of-the-art competitors as measured by linear correlation, triplet distance ranking accuracy, and Wasserstein distance between persistence barcodes.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#34987;&#35270;&#20026;&#38544;&#24335;&#30340;&#20027;&#39064;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#20174;&#27880;&#37322;&#25968;&#25454;&#20013;&#36873;&#25321;&#26368;&#20339;&#31034;&#33539;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2301.11916</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#34987;&#35270;&#20026;&#38544;&#21547;&#30340;&#20027;&#39064;&#27169;&#22411;&#65306;&#35299;&#37322;&#21644;&#23547;&#25214;&#22909;&#30340;&#31034;&#33539;&#20197;&#23454;&#29616;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Are Implicitly Topic Models: Explaining and Finding Good Demonstrations for In-Context Learning. (arXiv:2301.11916v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#34987;&#35270;&#20026;&#38544;&#24335;&#30340;&#20027;&#39064;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#20174;&#27880;&#37322;&#25968;&#25454;&#20013;&#36873;&#25321;&#26368;&#20339;&#31034;&#33539;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#20102;&#22312;&#25512;&#29702;&#26102;&#23454;&#29616;&#23569;&#37327;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;&#30340;&#26174;&#33879;&#25928;&#29575;&#65292;&#34987;&#31216;&#20026;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290; &#28982;&#32780;&#65292;&#29616;&#26377;&#25991;&#29486;&#24378;&#35843;&#36825;&#31181;&#33021;&#21147;&#23545;&#23569;&#37327;&#26679;&#26412;&#31034;&#33539;&#30340;&#36873;&#25321;&#24456;&#25935;&#24863;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#36125;&#21494;&#26031;&#35270;&#35282;&#30740;&#31350;&#19978;&#19979;&#25991;&#23398;&#20064;&#29616;&#35937;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35270;&#20026;&#20174;&#31034;&#33539;&#20013;&#38544;&#21547;&#22320;&#25512;&#26029;&#20986;&#30456;&#20851;&#20449;&#24687;&#30340;&#20027;&#39064;&#27169;&#22411;&#12290;&#22312;&#27492;&#21069;&#25552;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#29992;&#20110;&#20174;&#19968;&#32452;&#27880;&#37322;&#25968;&#25454;&#20013;&#36873;&#25321;&#26368;&#20339;&#31034;&#33539;&#65292;&#24182;&#35777;&#26126;&#30456;&#23545;&#20110;&#38543;&#26426;&#36873;&#25321;&#22522;&#32447;&#30340;&#24179;&#22343;&#20540;&#65292;&#22312;&#20843;&#20010;&#19981;&#21516;&#30340;&#30495;&#23454;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#24179;&#22343;&#27599;&#20010; GPT2 &#21644; GPT3 &#27169;&#22411;&#26377;&#26174;&#30528;&#30340; 12.5% &#30340;&#25552;&#21319;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#21457;&#29616;&#25903;&#25345;&#25105;&#20204;&#30340;&#20551;&#35774;&#65292;&#21363;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#34987;&#35270;&#20026;&#38544;&#21547;&#30340;&#20027;&#39064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, pre-trained large language models have demonstrated remarkable efficiency in achieving an inference-time few-shot learning capability known as in-context learning. However, existing literature has highlighted the sensitivity of this capability to the selection of few-shot demonstrations. The underlying mechanisms by which this capability arises from regular language model pretraining objectives remain poorly understood. In this study, we aim to examine the in-context learning phenomenon through a Bayesian lens, viewing large language models as topic models that implicitly infer task-related information from demonstrations. On this premise, we propose an algorithm for selecting optimal demonstrations from a set of annotated data and demonstrate a significant 12.5% improvement relative to the random selection baseline, averaged over eight GPT2 and GPT3 models on eight different real-world text classification datasets. Our empirical findings support our hypothesis that la
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;POMDP&#27169;&#22411;&#21644;&#20154;&#31867;&#19990;&#30028;&#32467;&#26500;&#20197;&#21450;&#20154;&#26426;&#20132;&#20114;&#30340;&#32467;&#26500;&#65292;&#21487;&#20197;&#23454;&#29616;&#23454;&#38469;&#26377;&#25928;&#30340;&#24191;&#20041;&#30446;&#26631;&#25628;&#32034;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2301.10121</link><description>&lt;p&gt;
&#24191;&#20041;&#30446;&#26631;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Generalized Object Search. (arXiv:2301.10121v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10121
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;POMDP&#27169;&#22411;&#21644;&#20154;&#31867;&#19990;&#30028;&#32467;&#26500;&#20197;&#21450;&#20154;&#26426;&#20132;&#20114;&#30340;&#32467;&#26500;&#65292;&#21487;&#20197;&#23454;&#29616;&#23454;&#38469;&#26377;&#25928;&#30340;&#24191;&#20041;&#30446;&#26631;&#25628;&#32034;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26410;&#26469;&#30340;&#21327;&#20316;&#26426;&#22120;&#20154;&#24517;&#39035;&#20855;&#22791;&#23547;&#25214;&#29289;&#20307;&#30340;&#33021;&#21147;&#12290;&#20316;&#20026;&#19968;&#39033;&#22522;&#26412;&#25216;&#33021;&#65292;&#25105;&#20204;&#39044;&#35745;&#30446;&#26631;&#25628;&#32034;&#26368;&#32456;&#20250;&#25104;&#20026;&#20219;&#20309;&#26426;&#22120;&#20154;&#30340;&#29616;&#25104;&#33021;&#21147;&#65292;&#31867;&#20284;&#20110;&#29289;&#20307;&#26816;&#27979;&#12289;SLAM&#21644;&#36816;&#21160;&#35268;&#21010;&#31561;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#35201;&#20040;&#20570;&#20986;&#19981;&#20999;&#23454;&#38469;&#30340;&#22949;&#21327;&#65288;&#20363;&#22914;&#23558;&#38382;&#39064;&#20174;3D&#32553;&#20943;&#21040;2D&#65289;&#65292;&#35201;&#20040;&#37319;&#29992;&#19987;&#38376;&#30340;&#36138;&#23146;&#25628;&#32034;&#31574;&#30053;&#65292;&#25110;&#32773;&#35797;&#22270;&#22312;&#27169;&#25311;&#20013;&#23398;&#20064;&#31471;&#21040;&#31471;&#31574;&#30053;&#65292;&#36825;&#20123;&#31574;&#30053;&#23578;&#26410;&#22312;&#30495;&#23454;&#26426;&#22120;&#20154;&#21644;&#29615;&#22659;&#20013;&#24471;&#21040;&#25512;&#24191;&#12290;&#35813;&#35770;&#25991;&#35748;&#20026;&#65292;&#36890;&#36807;&#21033;&#29992;&#37096;&#20998;&#21487;&#35266;&#27979;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDP&#65289;&#26469;&#24314;&#27169;&#30446;&#26631;&#25628;&#32034;&#65292;&#21516;&#26102;&#21033;&#29992;&#20154;&#31867;&#19990;&#30028;&#65288;&#20363;&#22914;&#20843;&#21449;&#26641;&#12289;&#30456;&#20851;&#24615;&#65289;&#21644;&#20154;&#26426;&#20132;&#20114;&#20013;&#30340;&#32467;&#26500;&#65288;&#20363;&#22914;&#31354;&#38388;&#35821;&#35328;&#65289;&#65292;&#21487;&#20197;&#23454;&#29616;&#19968;&#31181;&#23454;&#29992;&#26377;&#25928;&#30340;&#24191;&#20041;&#30446;&#26631;&#25628;&#32034;&#31995;&#32479;&#12290;&#20026;&#20102;&#25903;&#25345;&#36825;&#19968;&#35770;&#28857;&#65292;&#25105;&#24320;&#21457;&#20102;&#22312;&#26377;&#38480;&#35270;&#37326;&#12289;&#36974;&#25377;&#21644;&#22122;&#22768;&#23384;&#22312;&#30340;&#19981;&#30830;&#23450;&#24615;&#19979;&#65292;&#22312;3D&#29615;&#22659;&#20013;&#36827;&#34892;&#65288;&#22810;&#20010;&#65289;&#30446;&#26631;&#25628;&#32034;&#30340;&#26041;&#27861;&#21644;&#31995;&#32479;&#65292;&#24182;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#22330;&#26223;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Future collaborative robots must be capable of finding objects. As such a fundamental skill, we expect object search to eventually become an off-the-shelf capability for any robot, similar to e.g., object detection, SLAM, and motion planning. However, existing approaches either make unrealistic compromises (e.g., reduce the problem from 3D to 2D), resort to ad-hoc, greedy search strategies, or attempt to learn end-to-end policies in simulation that are yet to generalize across real robots and environments. This thesis argues that through using Partially Observable Markov Decision Processes (POMDPs) to model object search while exploiting structures in the human world (e.g., octrees, correlations) and in human-robot interaction (e.g., spatial language), a practical and effective system for generalized object search can be achieved. In support of this argument, I develop methods and systems for (multi-)object search in 3D environments under uncertainty due to limited field of view, occlu
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#22352;&#26631;&#30340;MLPs&#30340;&#35889;&#20559;&#32622;&#23545;&#39640;&#39057;&#32452;&#20214;&#25910;&#25947;&#30340;&#38459;&#30861;&#65292;&#24182;&#25552;&#20986;&#20351;&#29992;&#39640;&#39057;&#27491;&#24358;&#27874;&#32534;&#30721;&#36755;&#20837;&#26469;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2301.05816</link><description>&lt;p&gt;
&#36890;&#36807;&#35757;&#32451;&#21160;&#24577;&#29702;&#35299;&#22522;&#20110;&#22352;&#26631;&#30340;MLPs&#30340;&#35889;&#20559;&#32622;
&lt;/p&gt;
&lt;p&gt;
Understanding the Spectral Bias of Coordinate Based MLPs Via Training Dynamics. (arXiv:2301.05816v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.05816
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#22352;&#26631;&#30340;MLPs&#30340;&#35889;&#20559;&#32622;&#23545;&#39640;&#39057;&#32452;&#20214;&#25910;&#25947;&#30340;&#38459;&#30861;&#65292;&#24182;&#25552;&#20986;&#20351;&#29992;&#39640;&#39057;&#27491;&#24358;&#27874;&#32534;&#30721;&#36755;&#20837;&#26469;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35889;&#20559;&#32622;&#26159;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#37325;&#35201;&#35266;&#23519;&#32467;&#26524;&#65292;&#23427;&#34920;&#31034;&#32593;&#32476;&#22312;&#25910;&#25947;&#21040;&#26356;&#39640;&#39057;&#29575;&#32452;&#20214;&#21069;&#65292;&#20250;&#23398;&#20064;&#30446;&#26631;&#20989;&#25968;&#30340;&#20302;&#39057;&#34920;&#31034;&#12290;&#36825;&#19968;&#23646;&#24615;&#19982;&#36229;&#21442;&#25968;&#32593;&#32476;&#30340;&#33391;&#22909;&#27867;&#21270;&#33021;&#21147;&#26377;&#20851;&#65292;&#20294;&#22312;&#24212;&#29992;&#20110;&#22330;&#26223;&#28210;&#26579;&#26102;&#65292;&#37319;&#29992;&#20855;&#26377;ReLU&#28608;&#27963;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;(MLPs)&#21033;&#29992;&#23494;&#38598;&#30340;&#20302;&#32500;&#22352;&#26631;&#36755;&#20837;&#20250;&#23548;&#33268;&#20005;&#37325;&#30340;&#35889;&#20559;&#24046;&#65292;&#23436;&#20840;&#38459;&#30861;&#20102;&#25910;&#25947;&#21040;&#39640;&#39057;&#32452;&#20214;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#21487;&#20197;&#20351;&#29992;&#39640;&#39057;&#27491;&#24358;&#27874;&#32534;&#30721;&#36755;&#20837;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#35797;&#22270;&#20351;&#29992;&#31070;&#32463;&#20999;&#21521;&#26680;(NTK)&#21644;&#20613;&#37324;&#21494;&#20998;&#26512;&#26469;&#35299;&#37322;&#22352;&#26631;&#31995;&#20013;&#30340;&#35889;&#20559;&#24046;&#21450;&#20854;&#20005;&#37325;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#23384;&#22312;&#21508;&#31181;&#38480;&#21046;&#65292;&#22240;&#20026;NTK&#19981;&#33021;&#25429;&#25417;&#21040;&#30495;&#27491;&#30340;&#32593;&#32476;&#21160;&#24577;&#65292;&#32780;&#20613;&#37324;&#21494;&#20998;&#26512;&#21482;&#33021;&#25552;&#20379;&#23545;&#39057;&#29575;&#32452;&#20214;&#30340;&#20840;&#23616;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spectral bias is an important observation of neural network training, stating that the network will learn a low frequency representation of the target function before converging to higher frequency components. This property is interesting due to its link to good generalization in over-parameterized networks. However, in applications to scene rendering, where multi-layer perceptrons (MLPs) with ReLU activations utilize dense, low dimensional coordinate based inputs, a severe spectral bias occurs that obstructs convergence to high freqeuncy components entirely. In order to overcome this limitation, one can encode the inputs using high frequency sinusoids. Previous works attempted to explain both spectral bias and its severity in the coordinate based regime using Neural Tangent Kernel (NTK) and Fourier analysis. However, such methods come with various limitations, since NTK does not capture real network dynamics, and Fourier analysis only offers a global perspective on the frequency compo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#30340;&#21069;&#27839;&#30740;&#31350;&#32508;&#21512;&#35843;&#26597;&#12290;&#35752;&#35770;&#20102;&#26032;&#20852;&#25512;&#29702;&#33021;&#21147;&#20986;&#29616;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#24182;&#25552;&#20379;&#31995;&#32479;&#36164;&#28304;&#24110;&#21161;&#21021;&#23398;&#32773;&#12290;</title><link>http://arxiv.org/abs/2212.09597</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Reasoning with Language Model Prompting: A Survey. (arXiv:2212.09597v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09597
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#30340;&#21069;&#27839;&#30740;&#31350;&#32508;&#21512;&#35843;&#26597;&#12290;&#35752;&#35770;&#20102;&#26032;&#20852;&#25512;&#29702;&#33021;&#21147;&#20986;&#29616;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#24182;&#25552;&#20379;&#31995;&#32479;&#36164;&#28304;&#24110;&#21161;&#21021;&#23398;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#29702;&#20316;&#20026;&#22797;&#26434;&#38382;&#39064;&#35299;&#20915;&#30340;&#37325;&#35201;&#33021;&#21147;&#65292;&#21487;&#20197;&#20026;&#21307;&#30103;&#35786;&#26029;&#12289;&#35848;&#21028;&#31561;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#25552;&#20379;&#21518;&#31471;&#25903;&#25345;&#12290;&#26412;&#25991;&#23545;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#30340;&#21069;&#27839;&#30740;&#31350;&#36827;&#34892;&#20102;&#32508;&#21512;&#35843;&#26597;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#30740;&#31350;&#25104;&#26524;&#30340;&#27604;&#36739;&#21644;&#24635;&#32467;&#65292;&#24182;&#25552;&#20379;&#20102;&#31995;&#32479;&#36164;&#28304;&#20197;&#24110;&#21161;&#21021;&#23398;&#32773;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#26032;&#20852;&#25512;&#29702;&#33021;&#21147;&#20986;&#29616;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#24182;&#31361;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#36164;&#28304;&#21487;&#22312; https://github.com/zjunlp/Prompt4ReasoningPapers &#19978;&#33719;&#21462;&#65288;&#23450;&#26399;&#26356;&#26032;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reasoning, as an essential ability for complex problem-solving, can provide back-end support for various real-world applications, such as medical diagnosis, negotiation, etc. This paper provides a comprehensive survey of cutting-edge research on reasoning with language model prompting. We introduce research works with comparisons and summaries and provide systematic resources to help beginners. We also discuss the potential reasons for emerging such reasoning abilities and highlight future research directions. Resources are available at https://github.com/zjunlp/Prompt4ReasoningPapers (updated periodically).
&lt;/p&gt;</description></item><item><title>xTrimoABFold&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#25239;&#20307;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#22411;&#25239;&#20307;&#32467;&#26500;&#39044;&#27979;&#26041;&#27861;&#65292;&#26080;&#38656;&#22810;&#24207;&#21015;&#27604;&#23545;&#65292;&#26377;&#26395;&#20419;&#36827;&#39640;&#36890;&#37327;&#33647;&#29289;&#35774;&#35745;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2212.00735</link><description>&lt;p&gt;
xTrimoABFold&#65306;&#26080;&#22810;&#24207;&#21015;&#27604;&#23545;&#30340;&#26032;&#22411;&#25239;&#20307;&#32467;&#26500;&#39044;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
xTrimoABFold: De novo Antibody Structure Prediction without MSA. (arXiv:2212.00735v2 [q-bio.QM] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.00735
&lt;/p&gt;
&lt;p&gt;
xTrimoABFold&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#25239;&#20307;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#22411;&#25239;&#20307;&#32467;&#26500;&#39044;&#27979;&#26041;&#27861;&#65292;&#26080;&#38656;&#22810;&#24207;&#21015;&#27604;&#23545;&#65292;&#26377;&#26395;&#20419;&#36827;&#39640;&#36890;&#37327;&#33647;&#29289;&#35774;&#35745;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25239;&#20307;&#24037;&#31243;&#39046;&#22495;&#65292;&#35774;&#35745;&#19968;&#20010;&#26032;&#22411;&#25239;&#20307;&#20197;&#27491;&#30830;&#22320;&#32467;&#21512;&#29305;&#23450;&#25239;&#21407;&#30340;&#34920;&#20301;&#26159;&#19968;&#39033;&#37325;&#35201;&#30340;&#20219;&#21153;&#12290;&#20102;&#35299;&#25239;&#20307;&#32467;&#26500;&#21644;&#20854;&#34920;&#20301;&#21487;&#20197;&#20419;&#36827;&#23545;&#20854;&#21151;&#33021;&#30340;&#26426;&#21046;&#29702;&#35299;&#12290;&#22240;&#27492;&#65292;&#20174;&#20854;&#24207;&#21015;&#39044;&#27979;&#25239;&#20307;&#32467;&#26500;&#19968;&#30452;&#26159;&#19968;&#39033;&#39640;&#24230;&#26377;&#20215;&#20540;&#30340;&#20219;&#21153;&#65292;&#32780;AlphaFold2&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;&#34507;&#30333;&#36136;&#24207;&#21015;&#39044;&#27979;&#34507;&#30333;&#36136;&#32467;&#26500;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#23545;&#20110;&#25239;&#20307;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#25239;&#20307;&#30340;&#20114;&#34917;&#20915;&#23450;&#21306;&#65288;CDRs&#65289;&#65292;&#20854;&#39044;&#27979;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#26377;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of antibody engineering, an essential task is to design a novel antibody whose paratopes bind to a specific antigen with correct epitopes. Understanding antibody structure and its paratope can facilitate a mechanistic understanding of its function. Therefore, antibody structure prediction from its sequence alone has always been a highly valuable problem for de novo antibody design. AlphaFold2, a breakthrough in the field of structural biology, provides a solution to predict protein structure based on protein sequences and computationally expensive coevolutionary multiple sequence alignments (MSAs). However, the computational efficiency and undesirable prediction accuracy of antibodies, especially on the complementarity-determining regions (CDRs) of antibodies limit their applications in the industrially high-throughput drug design. To learn an informative representation of antibodies, we employed a deep antibody language model (ALM) on curated sequences from the observed a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#21160;&#24577;&#32593;&#32476;&#65288;HDNet&#65289;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#22270;&#20687;&#21327;&#35843;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#26412;&#22320;&#21160;&#24577;&#65288;LD&#65289;&#27169;&#22359;&#21644;&#22522;&#20110;&#25513;&#30721;&#30340;&#20840;&#23616;&#21160;&#24577;&#65288;MGD&#65289;&#27169;&#22359;&#65292;LD&#20445;&#30041;&#20102;&#35814;&#32454;&#30340;&#26412;&#22320;&#35270;&#35273;&#19968;&#33268;&#24615;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;HDNet&#22312;&#23450;&#37327;&#21644;&#23450;&#24615;&#35780;&#20272;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#21327;&#35843;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2211.08639</link><description>&lt;p&gt;
&#20998;&#23618;&#21160;&#24577;&#22270;&#20687;&#21327;&#35843;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Dynamic Image Harmonization. (arXiv:2211.08639v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08639
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#21160;&#24577;&#32593;&#32476;&#65288;HDNet&#65289;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#22270;&#20687;&#21327;&#35843;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#26412;&#22320;&#21160;&#24577;&#65288;LD&#65289;&#27169;&#22359;&#21644;&#22522;&#20110;&#25513;&#30721;&#30340;&#20840;&#23616;&#21160;&#24577;&#65288;MGD&#65289;&#27169;&#22359;&#65292;LD&#20445;&#30041;&#20102;&#35814;&#32454;&#30340;&#26412;&#22320;&#35270;&#35273;&#19968;&#33268;&#24615;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;HDNet&#22312;&#23450;&#37327;&#21644;&#23450;&#24615;&#35780;&#20272;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#21327;&#35843;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#21327;&#35843;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#20219;&#21153;&#65292;&#26088;&#22312;&#35843;&#25972;&#21069;&#26223;&#20197;&#20351;&#20854;&#19982;&#32972;&#26223;&#20860;&#23481;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#20027;&#35201;&#20851;&#27880;&#20351;&#29992;&#20840;&#23616;&#21464;&#25442;&#65288;&#21363;&#24402;&#19968;&#21270;&#21644;&#33394;&#24425;&#26354;&#32447;&#28210;&#26579;&#65289;&#26469;&#23454;&#29616;&#35270;&#35273;&#19968;&#33268;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#24573;&#30053;&#20102;&#26412;&#22320;&#35270;&#35273;&#19968;&#33268;&#24615;&#65292;&#24182;&#19988;&#23427;&#20204;&#24040;&#22823;&#30340;&#27169;&#22411;&#22823;&#23567;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#35843;&#21644;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#21160;&#24577;&#32593;&#32476;&#65288;HDNet&#65289;&#65292;&#20197;&#36866;&#24212;&#29305;&#24449;&#20174;&#23616;&#37096;&#21040;&#25972;&#20307;&#30340;&#35270;&#35282;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#22270;&#20687;&#21327;&#35843;&#12290;&#22312;&#21508;&#31181;&#21160;&#24577;&#27169;&#22411;&#21462;&#24471;&#25104;&#21151;&#30340;&#22522;&#30784;&#19978;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#26412;&#22320;&#21160;&#24577;&#65288;LD&#65289;&#27169;&#22359;&#21644;&#22522;&#20110;&#25513;&#30721;&#30340;&#20840;&#23616;&#21160;&#24577;&#65288;MGD&#65289;&#27169;&#22359;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;LD&#22522;&#20110;&#35821;&#20041;&#30456;&#20284;&#24615;&#21305;&#37197;&#21069;&#26223;&#21644;&#32972;&#26223;&#21306;&#22495;&#20043;&#38388;&#30340;&#26412;&#22320;&#34920;&#31034;&#65292;&#28982;&#21518;&#26681;&#25454;&#20854;$K$&#20010;&#26368;&#36817;&#37051;&#32972;&#26223;&#21306;&#22495;&#30340;&#22806;&#35266;&#33258;&#36866;&#24212;&#22320;&#35843;&#25972;&#27599;&#20010;&#21069;&#26223;&#26412;&#22320;&#34920;&#31034;&#12290;&#36825;&#26679;&#65292;LD&#20445;&#30041;&#20102;&#35814;&#32454;&#30340;&#26412;&#22320;&#35270;&#35273;&#19968;&#33268;&#24615;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;LD&#30340;MGD&#36890;&#36807;&#23558;&#21069;&#26223;&#25513;&#30721;&#24341;&#20837;&#27169;&#22411;&#65292;&#22686;&#24378;&#20102;&#20840;&#23616;&#21464;&#25442;&#30340;&#24615;&#33021;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;HDNet&#22312;&#23450;&#37327;&#21644;&#23450;&#24615;&#35780;&#20272;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#21327;&#35843;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image harmonization is a critical task in computer vision, which aims to adjust the foreground to make it compatible with the background. Recent works mainly focus on using global transformations (i.e., normalization and color curve rendering) to achieve visual consistency. However, these models ignore local visual consistency and their huge model sizes limit their harmonization ability on edge devices. In this paper, we propose a hierarchical dynamic network (HDNet) to adapt features from local to global view for better feature transformation in efficient image harmonization. Inspired by the success of various dynamic models, local dynamic (LD) module and mask-aware global dynamic (MGD) module are proposed in this paper. Specifically, LD matches local representations between the foreground and background regions based on semantic similarities, then adaptively adjust every foreground local representation according to the appearance of its $K$-nearest neighbor background regions. In thi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#32447;&#24615;&#35268;&#21010;&#30340;&#22522;&#26412;&#21407;&#29702;&#21644;&#23454;&#36341;&#65292;&#24182;&#36890;&#36807;&#20379;&#24212;&#38142;&#31649;&#29702;&#21644;&#36816;&#36755;&#29289;&#27969;&#20013;&#30340;&#31354;&#38388;&#20998;&#26512;&#20248;&#21270;&#38382;&#39064;&#28436;&#31034;&#22914;&#20309;&#35299;&#20915;&#32463;&#20856;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.07345</link><description>&lt;p&gt;
&#32447;&#24615;&#35268;&#21010;&#20013;&#30340;&#25945;&#31243;&#19982;&#23454;&#36341;&#65306;&#20379;&#24212;&#38142;&#19982;&#36816;&#36755;&#29289;&#27969;&#20013;&#30340;&#20248;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Tutorial and Practice in Linear Programming: Optimization Problems in Supply Chain and Transport Logistics. (arXiv:2211.07345v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.07345
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#32447;&#24615;&#35268;&#21010;&#30340;&#22522;&#26412;&#21407;&#29702;&#21644;&#23454;&#36341;&#65292;&#24182;&#36890;&#36807;&#20379;&#24212;&#38142;&#31649;&#29702;&#21644;&#36816;&#36755;&#29289;&#27969;&#20013;&#30340;&#31354;&#38388;&#20998;&#26512;&#20248;&#21270;&#38382;&#39064;&#28436;&#31034;&#22914;&#20309;&#35299;&#20915;&#32463;&#20856;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#26159;&#19968;&#31687;&#38754;&#21521;&#23398;&#29983;&#21644;&#23454;&#36341;&#32773;&#30340;&#25104;&#20154;&#25945;&#32946;&#25351;&#21335;&#65292;&#26088;&#22312;&#24110;&#21161;&#20182;&#20204;&#29702;&#35299;&#32447;&#24615;&#35268;&#21010;&#30340;&#22522;&#26412;&#21407;&#29702;&#21644;&#23454;&#36341;&#12290;&#26412;&#25991;&#36890;&#36807;&#37325;&#28857;&#20171;&#32461;&#20379;&#24212;&#38142;&#31649;&#29702;&#21644;&#36816;&#36755;&#29289;&#27969;&#20013;&#30340;&#31354;&#38388;&#20998;&#26512;&#20248;&#21270;&#38382;&#39064;&#26469;&#28436;&#31034;&#22914;&#20309;&#35299;&#20915;&#32463;&#20856;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#25152;&#26377;&#30340;&#20064;&#39064;&#37117;&#23637;&#31034;&#20102;&#29992;&#20110;&#35299;&#20915;&#23427;&#20204;&#30340;Python&#31243;&#24207;&#21644;&#20248;&#21270;&#24211;&#12290;&#31532;&#19968;&#31456;&#20171;&#32461;&#20102;&#32447;&#24615;&#35268;&#21010;&#30340;&#20851;&#38190;&#27010;&#24565;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35748;&#30693;&#26694;&#26550;&#65292;&#20197;&#24110;&#21161;&#23398;&#29983;&#21644;&#23454;&#36341;&#32773;&#35774;&#32622;&#27599;&#20010;&#20248;&#21270;&#38382;&#39064;&#12290;&#35813;&#35748;&#30693;&#26694;&#26550;&#23558;&#20915;&#31574;&#21464;&#37327;&#12289;&#32422;&#26463;&#12289;&#30446;&#26631;&#20989;&#25968;&#21644;&#21464;&#37327;&#33539;&#22260;&#32452;&#32455;&#25104;&#19968;&#20010;&#26684;&#24335;&#65292;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#20248;&#21270;&#36719;&#20214;&#12290;&#31532;&#20108;&#31456;&#20171;&#32461;&#20102;&#20004;&#31181;&#31867;&#22411;&#30340;&#31227;&#21160;&#20248;&#21270;&#38382;&#39064;&#65288;&#32593;&#32476;&#19978;&#30340;&#26368;&#30701;&#36335;&#24452;&#21644;&#26368;&#23567;&#25104;&#26412;&#24033;&#22238;&#65289;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#20132;&#20184;&#21644;&#26381;&#21153;&#35745;&#21010;&#29289;&#27969;&#12290;&#31532;&#19977;&#31456;&#20171;&#32461;&#20102;&#22235;&#31181;&#31867;&#22411;&#30340;&#31354;&#38388;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This tutorial is an andragogical guide for students and practitioners seeking to understand the fundamentals and practice of linear programming. The exercises demonstrate how to solve classical optimization problems with an emphasis on spatial analysis in supply chain management and transport logistics. All exercises display the Python programs and optimization libraries used to solve them. The first chapter introduces key concepts in linear programming and contributes a new cognitive framework to help students and practitioners set up each optimization problem. The cognitive framework organizes the decision variables, constraints, the objective function, and variable bounds in a format for direct application to optimization software. The second chapter introduces two types of mobility optimization problems (shortest path in a network and minimum cost tour) in the context of delivery and service planning logistics. The third chapter introduces four types of spatial optimization problem
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20135;&#29983;&#21551;&#21457;&#24335;&#26469;&#25351;&#23548;&#26377;&#21521;&#25511;&#21046;&#22120;&#21512;&#25104;&#31639;&#27861;&#30340;&#22686;&#37327;&#25506;&#32034;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2210.05393</link><description>&lt;p&gt;
&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#21363;&#20852;&#25511;&#21046;&#22120;&#21512;&#25104;&#31574;&#30053;&#25506;&#32034;&#31639;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Exploration Policies for On-the-Fly Controller Synthesis: A Reinforcement Learning Approach. (arXiv:2210.05393v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.05393
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20135;&#29983;&#21551;&#21457;&#24335;&#26469;&#25351;&#23548;&#26377;&#21521;&#25511;&#21046;&#22120;&#21512;&#25104;&#31639;&#27861;&#30340;&#22686;&#37327;&#25506;&#32034;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25511;&#21046;&#22120;&#21512;&#25104;&#26159;&#22522;&#20110;&#27169;&#22411;&#30340;&#35268;&#21010;&#30340;&#19968;&#31181;&#65292;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#33719;&#24471;&#21551;&#21457;&#24335;&#26469;&#36827;&#34892;&#26377;&#21521;&#25511;&#21046;&#22120;&#21512;&#25104;&#30340;&#31639;&#27861;&#30740;&#31350;&#12290;&#27492;&#31639;&#27861;&#30340;&#22686;&#37327;&#25506;&#32034;&#30001;&#19982;&#39046;&#22495;&#26080;&#20851;&#30340;&#20154;&#24037;&#35774;&#35745;&#24341;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Controller synthesis is in essence a case of model-based planning for non-deterministic environments in which plans (actually ''strategies'') are meant to preserve system goals indefinitely. In the case of supervisory control environments are specified as the parallel composition of state machines and valid strategies are required to be ''non-blocking'' (i.e., always enabling the environment to reach certain marked states) in addition to safe (i.e., keep the system within a safe zone). Recently, On-the-fly Directed Controller Synthesis techniques were proposed to avoid the exploration of the entire -and exponentially large-environment space, at the cost of non-maximal permissiveness, to either find a strategy or conclude that there is none. The incremental exploration of the plant is currently guided by a domain-independent human-designed heuristic. In this work, we propose a new method for obtaining heuristics based on Reinforcement Learning (RL). The synthesis algorithm is thus frame
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21327;&#21516;&#25163;&#21183;&#21512;&#25104;&#26041;&#27861;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#22522;&#20110;&#33410;&#22863;&#30340;&#20998;&#21106;&#27969;&#31243;&#65292;&#24182;&#26377;&#25928;&#22320;&#21306;&#20998;&#20102;&#22522;&#20110;&#35821;&#35328;&#23398;&#29702;&#35770;&#30340;&#35821;&#38899;&#21644;&#21160;&#20316;&#30340;&#20302;&#32423;&#21644;&#39640;&#32423;&#31070;&#32463;&#23884;&#20837;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22312;&#35821;&#38899;&#21644;&#25163;&#21183;&#20043;&#38388;&#30340;&#30495;&#27491;&#24773;&#24863;&#20114;&#21160;&#12290;</title><link>http://arxiv.org/abs/2210.01448</link><description>&lt;p&gt;
&#33410;&#22863;&#24615;&#25163;&#21183;&#29983;&#25104;&#22120;&#65306;&#24102;&#26377;&#20998;&#23618;&#31070;&#32463;&#23884;&#20837;&#30340;&#33410;&#22863;&#24863;&#30693;&#35821;&#38899;&#21327;&#21516;&#25163;&#21183;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Rhythmic Gesticulator: Rhythm-Aware Co-Speech Gesture Synthesis with Hierarchical Neural Embeddings. (arXiv:2210.01448v3 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.01448
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21327;&#21516;&#25163;&#21183;&#21512;&#25104;&#26041;&#27861;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#22522;&#20110;&#33410;&#22863;&#30340;&#20998;&#21106;&#27969;&#31243;&#65292;&#24182;&#26377;&#25928;&#22320;&#21306;&#20998;&#20102;&#22522;&#20110;&#35821;&#35328;&#23398;&#29702;&#35770;&#30340;&#35821;&#38899;&#21644;&#21160;&#20316;&#30340;&#20302;&#32423;&#21644;&#39640;&#32423;&#31070;&#32463;&#23884;&#20837;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22312;&#35821;&#38899;&#21644;&#25163;&#21183;&#20043;&#38388;&#30340;&#30495;&#27491;&#24773;&#24863;&#20114;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#36896;&#26234;&#33021;&#20307;&#30340;&#21019;&#20316;&#20013;&#65292;&#33258;&#21160;&#29983;&#25104;&#36924;&#30495;&#30340;&#21327;&#21516;&#25163;&#21183;&#25104;&#20026;&#20102;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#12289;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#36807;&#21435;&#30340;&#31995;&#32479;&#20027;&#35201;&#38598;&#20013;&#22312;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#29983;&#25104;&#25163;&#21183;&#65292;&#36825;&#23548;&#33268;&#20102;&#22312;&#35821;&#38899;&#21644;&#25163;&#21183;&#20043;&#38388;&#30340;&#22797;&#26434;&#20294;&#24494;&#22937;&#30340;&#21644;&#35856;&#20851;&#31995;&#20013;&#25366;&#25496;&#28165;&#26224;&#30340;&#33410;&#22863;&#21644;&#35821;&#20041;&#30340;&#22256;&#38590;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21327;&#21516;&#25163;&#21183;&#21512;&#25104;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#33410;&#22863;&#21644;&#35821;&#20041;&#26041;&#38754;&#37117;&#23454;&#29616;&#20196;&#20154;&#20449;&#26381;&#30340;&#32467;&#26524;&#12290;&#23545;&#20110;&#33410;&#22863;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#21253;&#21547;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#22522;&#20110;&#33410;&#22863;&#30340;&#20998;&#21106;&#27969;&#31243;&#65292;&#20197;&#30830;&#20445;&#35821;&#38899;&#21644;&#25163;&#21183;&#20043;&#38388;&#30340;&#26102;&#38388;&#21327;&#35843;&#26126;&#30830;&#21487;&#35265;&#12290;&#23545;&#20110;&#25163;&#21183;&#30340;&#35821;&#20041;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26426;&#21046;&#26469;&#26377;&#25928;&#22320;&#21306;&#20998;&#22522;&#20110;&#35821;&#35328;&#23398;&#29702;&#35770;&#30340;&#35821;&#38899;&#21644;&#21160;&#20316;&#30340;&#20302;&#32423;&#21644;&#39640;&#32423;&#31070;&#32463;&#23884;&#20837;&#12290;&#39640;&#32423;&#23884;&#20837;&#23545;&#24212;&#35821;&#20041;&#65292;&#32780;&#20302;&#32423;&#23884;&#20837;&#21017;&#28041;&#21450;&#24494;&#23567;&#30340;&#21464;&#21270;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#35821;&#38899;&#21644;&#21160;&#20316;&#30340;&#20998;&#23618;&#23884;&#20837;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#65292;&#20351;&#24471;&#25105;&#20204;&#30340;&#31995;&#32479;&#21487;&#20197;&#22312;&#35821;&#38899;&#21644;&#25163;&#21183;&#20043;&#38388;&#23454;&#29616;&#30495;&#27491;&#30340;&#24773;&#24863;&#20114;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic synthesis of realistic co-speech gestures is an increasingly important yet challenging task in artificial embodied agent creation. Previous systems mainly focus on generating gestures in an end-to-end manner, which leads to difficulties in mining the clear rhythm and semantics due to the complex yet subtle harmony between speech and gestures. We present a novel co-speech gesture synthesis method that achieves convincing results both on the rhythm and semantics. For the rhythm, our system contains a robust rhythm-based segmentation pipeline to ensure the temporal coherence between the vocalization and gestures explicitly. For the gesture semantics, we devise a mechanism to effectively disentangle both low- and high-level neural embeddings of speech and motion based on linguistic theory. The high-level embedding corresponds to semantics, while the low-level embedding relates to subtle variations. Lastly, we build correspondence between the hierarchical embeddings of the speech 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#25991;&#26723;&#35270;&#35273;&#38382;&#31572;&#25968;&#25454;&#38598;TAT-DQA&#65292;&#24182;&#36890;&#36807;&#35813;&#25968;&#25454;&#38598;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#22411;&#27169;&#22411;MHST&#12290;MHST&#27169;&#22411;&#32771;&#34385;&#20102;&#22810;&#31181;&#27169;&#24577;&#30340;&#20449;&#24687;&#26469;&#26234;&#33021;&#22320;&#22238;&#31572;&#19981;&#21516;&#31867;&#22411;&#30340;&#38382;&#39064;&#65292;&#23454;&#39564;&#34920;&#26126;&#20854;&#22312;TAT-DQA&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2207.11871</link><description>&lt;p&gt;
&#36890;&#36807;&#31163;&#25955;&#25512;&#29702;&#23454;&#29616;&#22797;&#26434;&#25991;&#26723;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Towards Complex Document Understanding By Discrete Reasoning. (arXiv:2207.11871v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.11871
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#25991;&#26723;&#35270;&#35273;&#38382;&#31572;&#25968;&#25454;&#38598;TAT-DQA&#65292;&#24182;&#36890;&#36807;&#35813;&#25968;&#25454;&#38598;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#22411;&#27169;&#22411;MHST&#12290;MHST&#27169;&#22411;&#32771;&#34385;&#20102;&#22810;&#31181;&#27169;&#24577;&#30340;&#20449;&#24687;&#26469;&#26234;&#33021;&#22320;&#22238;&#31572;&#19981;&#21516;&#31867;&#22411;&#30340;&#38382;&#39064;&#65292;&#23454;&#39564;&#34920;&#26126;&#20854;&#22312;TAT-DQA&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26723;&#35270;&#35273;&#38382;&#31572;&#26088;&#22312;&#29702;&#35299;&#35270;&#35273;&#20016;&#23500;&#30340;&#25991;&#26723;&#65292;&#20197;&#22238;&#31572;&#33258;&#28982;&#35821;&#35328;&#20013;&#30340;&#38382;&#39064;&#65292;&#36825;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#19968;&#20010;&#26032;&#20852;&#30740;&#31350;&#35838;&#39064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;TAT-DQA&#30340;&#26032;&#30340;&#25991;&#26723;&#35270;&#35273;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#23427;&#30001;3067&#20010;&#21322;&#32467;&#26500;&#21270;&#34920;&#26684;&#21644;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#39029;&#38754;&#20197;&#21450;16558&#20010;&#38382;&#39064;-&#31572;&#26696;&#23545;&#32452;&#25104;&#65292;&#26159;&#36890;&#36807;&#25193;&#23637;TAT-QA&#25968;&#25454;&#38598;&#24471;&#21040;&#30340;&#12290;&#36825;&#20123;&#25991;&#26723;&#26469;&#33258;&#20110;&#29616;&#23454;&#19990;&#30028;&#30340;&#36130;&#21153;&#25253;&#21578;&#65292;&#21253;&#21547;&#22823;&#37327;&#25968;&#23383;&#65292;&#22240;&#27492;&#38656;&#35201;&#20855;&#26377;&#31163;&#25955;&#25512;&#29702;&#33021;&#21147;&#26469;&#22238;&#31572;&#36825;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#38382;&#39064;&#12290;&#22522;&#20110;TAT-DQA&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;MHST&#30340;&#26032;&#22411;&#27169;&#22411;&#65292;&#23427;&#32771;&#34385;&#20102;&#22810;&#31181;&#27169;&#24577;&#30340;&#20449;&#24687;&#65292;&#21253;&#25324;&#25991;&#26412;&#12289;&#24067;&#23616;&#21644;&#35270;&#35273;&#22270;&#20687;&#65292;&#20197;&#26234;&#33021;&#22320;&#38024;&#23545;&#19981;&#21516;&#31867;&#22411;&#30340;&#38382;&#39064;&#37319;&#29992;&#30456;&#24212;&#30340;&#31574;&#30053;&#65292;&#21363;&#25277;&#21462;&#25110;&#25512;&#29702;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;MHST&#27169;&#22411;&#22312;TAT-DQA&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#21644;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#25991;&#26723;&#35270;&#35273;&#38382;&#31572;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Document Visual Question Answering (VQA) aims to understand visually-rich documents to answer questions in natural language, which is an emerging research topic for both Natural Language Processing and Computer Vision. In this work, we introduce a new Document VQA dataset, named TAT-DQA, which consists of 3,067 document pages comprising semi-structured table(s) and unstructured text as well as 16,558 question-answer pairs by extending the TAT-QA dataset. These documents are sampled from real-world financial reports and contain lots of numbers, which means discrete reasoning capability is demanded to answer questions on this dataset. Based on TAT-DQA, we further develop a novel model named MHST that takes into account the information in multi-modalities, including text, layout and visual image, to intelligently address different types of questions with corresponding strategies, i.e., extraction or reasoning. Extensive experiments show that the MHST model significantly outperforms the ba
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#26694;&#26550;AutoOpt&#65292;&#29992;&#20110;&#33258;&#21160;&#35774;&#35745;&#20855;&#26377;&#22810;&#26679;&#21270;&#32467;&#26500;&#30340;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#36890;&#36807;&#21457;&#24067;&#19968;&#20010;&#36890;&#29992;&#30340;&#31639;&#27861;&#21407;&#22411;&#65292;&#23427;&#21487;&#20197;&#26356;&#22909;&#22320;&#21457;&#25496;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#23478;&#26063;&#30340;&#28508;&#21147;&#21644;&#26032;&#39062;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#35774;&#35745;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2204.00998</link><description>&lt;p&gt;
AutoOpt&#65306;&#19968;&#20010;&#33258;&#21160;&#35774;&#35745;&#22810;&#26679;&#21270;&#32467;&#26500;&#20803;&#21551;&#21457;&#24335;&#20248;&#21270;&#31639;&#27861;&#30340;&#36890;&#29992;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
AutoOpt: A General Framework for Automatically Designing Metaheuristic Optimization Algorithms with Diverse Structures. (arXiv:2204.00998v6 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.00998
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#26694;&#26550;AutoOpt&#65292;&#29992;&#20110;&#33258;&#21160;&#35774;&#35745;&#20855;&#26377;&#22810;&#26679;&#21270;&#32467;&#26500;&#30340;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#36890;&#36807;&#21457;&#24067;&#19968;&#20010;&#36890;&#29992;&#30340;&#31639;&#27861;&#21407;&#22411;&#65292;&#23427;&#21487;&#20197;&#26356;&#22909;&#22320;&#21457;&#25496;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#23478;&#26063;&#30340;&#28508;&#21147;&#21644;&#26032;&#39062;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#35774;&#35745;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#34987;&#24191;&#27867;&#35748;&#21487;&#20026;&#35299;&#20915;&#19981;&#28385;&#36275;&#24120;&#35268;&#27714;&#35299;&#22120;&#20005;&#26684;&#25968;&#23398;&#20551;&#35774;&#30340;&#38590;&#39064;&#30340;&#26080;&#26799;&#24230;&#27714;&#35299;&#22120;&#12290;&#33258;&#21160;&#35774;&#35745;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#25552;&#20379;&#20102;&#19968;&#26465;&#21560;&#24341;&#20154;&#30340;&#36335;&#24452;&#65292;&#21487;&#20197;&#20943;&#36731;&#25163;&#24037;&#35774;&#35745;&#30340;&#24037;&#20316;&#37327;&#65292;&#24182;&#33719;&#24471;&#36229;&#36234;&#20154;&#36896;&#31639;&#27861;&#30340;&#22686;&#24378;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#33258;&#21160;&#35774;&#35745;&#27969;&#27700;&#32447;&#20013;&#20855;&#20307;&#30340;&#31639;&#27861;&#21407;&#22411;&#21644;&#32447;&#24615;&#31639;&#27861;&#34920;&#31034;&#23558;&#35774;&#35745;&#38480;&#21046;&#22312;&#22266;&#23450;&#30340;&#31639;&#27861;&#32467;&#26500;&#20869;&#65292;&#36825;&#38459;&#30861;&#20102;&#21457;&#29616;&#20803;&#21551;&#21457;&#24335;&#23478;&#26063;&#20013;&#30340;&#26032;&#39062;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#35774;&#35745;&#22810;&#26679;&#21270;&#32467;&#26500;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#30340;&#36890;&#29992;&#26694;&#26550;&#8212;&#8212;AutoOpt&#12290;AutoOpt&#21253;&#21547;&#19977;&#20010;&#21019;&#26032;&#28857;&#65306;&#65288;1&#65289;&#19968;&#20010;&#36890;&#29992;&#30340;&#31639;&#27861;&#21407;&#22411;&#65292;&#19987;&#38376;&#28085;&#30422;&#20102;&#23613;&#21487;&#33021;&#24191;&#27867;&#30340;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#23478;&#26063;&#12290;&#23427;&#36890;&#36807;&#23436;&#20840;&#21457;&#25496;&#28508;&#21147;&#21644;&#26032;&#39062;&#24615;&#65292;&#22312;&#19981;&#21516;&#30340;&#38382;&#39064;&#19978;&#20419;&#36827;&#39640;&#36136;&#37327;&#30340;&#33258;&#21160;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Metaheuristics are widely recognized gradient-free solvers to hard problems that do not meet the rigorous mathematical assumptions of conventional solvers. The automated design of metaheuristic algorithms provides an attractive path to relieve manual design effort and gain enhanced performance beyond human-made algorithms. However, the specific algorithm prototype and linear algorithm representation in the current automated design pipeline restrict the design within a fixed algorithm structure, which hinders discovering novelties and diversity across the metaheuristic family. To address this challenge, this paper proposes a general framework, AutoOpt, for automatically designing metaheuristic algorithms with diverse structures. AutoOpt contains three innovations: (i) A general algorithm prototype dedicated to covering the metaheuristic family as widely as possible. It promotes high-quality automated design on different problems by fully discovering potentials and novelties across the f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#29992;&#21338;&#24328;&#30340;&#31354;&#38388;&#29366;&#24577;&#21160;&#20316;&#29305;&#24449;&#30340;&#35774;&#35745;&#21644;&#39640;&#25928;&#23454;&#29616;&#26041;&#27861;&#65292;&#21487;&#20197;&#25903;&#25345;&#19981;&#21516;&#26827;&#30424;&#21644;&#22270;&#24418;&#30340;&#22810;&#31181;&#19981;&#21516;&#28216;&#25103;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#23454;&#26102;&#35780;&#20272;&#27963;&#36291;&#29305;&#24449;&#30340;&#39640;&#25928;&#26041;&#27861;&#65292;&#24182;&#22312;&#22810;&#20010;&#28216;&#25103;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2201.06401</link><description>&lt;p&gt;
&#36890;&#29992;&#21338;&#24328;&#30340;&#31354;&#38388;&#29366;&#24577;&#21160;&#20316;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Spatial State-Action Features for General Games. (arXiv:2201.06401v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.06401
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#29992;&#21338;&#24328;&#30340;&#31354;&#38388;&#29366;&#24577;&#21160;&#20316;&#29305;&#24449;&#30340;&#35774;&#35745;&#21644;&#39640;&#25928;&#23454;&#29616;&#26041;&#27861;&#65292;&#21487;&#20197;&#25903;&#25345;&#19981;&#21516;&#26827;&#30424;&#21644;&#22270;&#24418;&#30340;&#22810;&#31181;&#19981;&#21516;&#28216;&#25103;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#23454;&#26102;&#35780;&#20272;&#27963;&#36291;&#29305;&#24449;&#30340;&#39640;&#25928;&#26041;&#27861;&#65292;&#24182;&#22312;&#22810;&#20010;&#28216;&#25103;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#26827;&#30424;&#28216;&#25103;&#21644;&#20854;&#20182;&#25277;&#35937;&#28216;&#25103;&#20013;&#65292;&#20351;&#29992;&#22270;&#26696;&#20316;&#20026;&#29305;&#24449;&#21487;&#20197;&#25351;&#23548;&#33258;&#21160;&#21270;&#28216;&#25103;&#20195;&#29702;&#30340;&#25805;&#20316;&#65292;&#32780;&#36825;&#20123;&#29305;&#24449;&#36890;&#24120;&#20195;&#34920;&#30528;&#23545;&#20110;&#28216;&#25103;&#30340;&#31574;&#30053;&#32780;&#35328;&#38750;&#24120;&#37325;&#35201;&#30340;&#26827;&#23376;&#12289;&#31354;&#20301;&#31561;&#29305;&#23450;&#37197;&#32622;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35774;&#35745;&#21644;&#39640;&#25928;&#23454;&#29616;&#36890;&#29992;&#21338;&#24328;&#30340;&#31354;&#38388;&#29366;&#24577;&#21160;&#20316;&#29305;&#24449;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;&#29305;&#24449;&#26159;&#21487;&#20197;&#26681;&#25454;&#25152;&#22788;&#21306;&#22495;&#30340;&#29366;&#24577;&#21464;&#37327;&#21305;&#37197;&#24773;&#20917;&#26469;&#40723;&#21169;&#25110;&#32773;&#25490;&#26021;&#21160;&#20316;&#30340;&#22270;&#26696;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#33509;&#24178;&#35774;&#35745;&#21644;&#23454;&#29616;&#36873;&#25321;&#30340;&#35814;&#32454;&#32454;&#33410;&#65292;&#24182;&#30528;&#37325;&#23454;&#29616;&#20102;&#39640;&#24230;&#36890;&#29992;&#24615;&#20197;&#25903;&#25345;&#19981;&#21516;&#26827;&#30424;&#20960;&#20309;&#22270;&#24418;&#25110;&#20854;&#20182;&#22270;&#24418;&#30340;&#21508;&#31181;&#19981;&#21516;&#28216;&#25103;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#22312;&#28216;&#25103;&#36807;&#31243;&#20013;&#23454;&#26102;&#35780;&#20272;&#27963;&#36291;&#29305;&#24449;&#65292;&#22522;&#20110;&#19968;&#31181;&#23558;&#27963;&#36291;&#29305;&#24449;&#38598;&#21512;&#34920;&#31034;&#20026;&#31232;&#30095;&#20108;&#20540;&#21521;&#37327;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#23427;&#25903;&#25345;&#19982;&#26435;&#37325;&#21521;&#37327;&#30340;&#26377;&#25928;&#28857;&#31215;&#35745;&#31639;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#28216;&#25103;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#21253;&#25324;&#22260;&#26827;&#12289;&#20845;&#35282;&#12289;&#20116;&#23376;&#26827;&#31561;&#28216;&#25103;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#23569;&#37327;&#27963;&#36291;&#35201;&#32032;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many board games and other abstract games, patterns have been used as features that can guide automated game-playing agents. Such patterns or features often represent particular configurations of pieces, empty positions, etc., which may be relevant for a game's strategies. Their use has been particularly prevalent in the game of Go, but also many other games used as benchmarks for AI research. In this paper, we formulate a design and efficient implementation of spatial state-action features for general games. These are patterns that can be trained to incentivise or disincentivise actions based on whether or not they match variables of the state in a local area around action variables. We provide extensive details on several design and implementation choices, with a primary focus on achieving a high degree of generality to support a wide variety of different games using different board geometries or other graphs. Secondly, we propose an efficient approach for evaluating active featur
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#22312;&#20195;&#25968;ZX&#28436;&#31639;&#20013;&#23545;&#25152;&#26377; $2^m\times 2^n$ &#22823;&#23567;&#30340;&#22522;&#26412;&#30697;&#38453;&#36827;&#34892;&#22270;&#31034;&#34920;&#31034;&#65292;&#20026;&#37327;&#23376;&#35745;&#31639;&#31561;&#39046;&#22495;&#20013;&#30340;ZX&#28436;&#31639;&#24212;&#29992;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2110.06898</link><description>&lt;p&gt;
&#29992;&#20195;&#25968;ZX&#28436;&#31639;&#34920;&#31034;&#21644;&#23454;&#29616;&#30697;&#38453;
&lt;/p&gt;
&lt;p&gt;
Representing and Implementing Matrices Using Algebraic ZX-calculus. (arXiv:2110.06898v4 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.06898
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#22312;&#20195;&#25968;ZX&#28436;&#31639;&#20013;&#23545;&#25152;&#26377; $2^m\times 2^n$ &#22823;&#23567;&#30340;&#22522;&#26412;&#30697;&#38453;&#36827;&#34892;&#22270;&#31034;&#34920;&#31034;&#65292;&#20026;&#37327;&#23376;&#35745;&#31639;&#31561;&#39046;&#22495;&#20013;&#30340;ZX&#28436;&#31639;&#24212;&#29992;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#24615;&#20195;&#25968;&#24212;&#29992;&#20013;&#65292;&#22522;&#26412;&#30697;&#38453;&#25198;&#28436;&#30528;&#37325;&#35201;&#30340;&#35282;&#33394;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#20195;&#25968;ZX&#28436;&#31639;&#20013;&#23545;&#25152;&#26377; $2^m\times 2^n$ &#22823;&#23567;&#30340;&#22522;&#26412;&#30697;&#38453;&#36827;&#34892;&#22270;&#31034;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#30340;&#36870;&#21644;&#36716;&#32622;&#31561;&#29305;&#24615;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#21033;&#29992;&#36825;&#31181;&#34920;&#31034;&#26041;&#27861;&#22312;&#20195;&#25968;ZX&#28436;&#31639;&#20013;&#23637;&#31034;&#20102;Jozsa-style matchgate&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#23454;&#38469;&#24212;&#29992;&#65292;&#25105;&#20204;&#22312;\texttt{discopy}&#20013;&#23454;&#29616;&#20102;&#36825;&#31181;&#34920;&#31034;&#26041;&#27861;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#36825;&#39033;&#24037;&#20316;&#20026;&#22914;&#22312;&#37327;&#23376;&#35745;&#31639;&#20013;&#21512;&#25104;&#21463;&#25511;&#30697;&#38453;[arXiv:2212.04462]&#31561;&#26356;&#22810;&#24212;&#29992;&#39046;&#22495;&#20013;&#30340;ZX&#28436;&#31639;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
In linear algebra applications, elementary matrices hold a significant role. This paper presents a diagrammatic representation of all $2^m\times 2^n$-sized elementary matrices in algebraic ZX-calculus, showcasing their properties on inverses and transpose through diagrammatic rewriting. Additionally, the paper uses this representation to depict the Jozsa-style matchgate in algebraic ZX-calculus. To further enhance practical use, we have implemented this representation in \texttt{discopy}. Overall, this work sets the groundwork for more applications of ZX-calculus such as synthesising controlled matrices [arXiv:2212.04462] in quantum computing.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24314;&#31435;&#20102;&#37327;&#23376;&#31163;&#25955;&#20107;&#20214;&#31995;&#32479;&#30340;&#30417;&#30563;&#25511;&#21046;&#26694;&#26550;&#65292;&#20351;&#29992;&#37327;&#23376;&#26377;&#38480;&#29366;&#24577;&#33258;&#21160;&#26426;&#20316;&#20026;&#24314;&#27169;&#24418;&#24335;&#65292;&#24182;&#25552;&#20986;&#24182;&#35777;&#26126;&#20102;&#20854;&#30417;&#30563;&#25511;&#21046;&#23450;&#29702;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#20915;&#23450;QDES&#29366;&#24577;&#21487;&#36798;&#24615;&#38382;&#39064;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#65292;&#24182;&#19988;&#35813;&#26041;&#27861;&#21487;&#20197;&#30452;&#25509;&#25193;&#23637;&#21040;DES&#20013;&#12290;</title><link>http://arxiv.org/abs/2104.09753</link><description>&lt;p&gt;
&#37327;&#23376;&#31163;&#25955;&#20107;&#20214;&#31995;&#32479;&#30340;&#30417;&#30563;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Supervisory Control of Quantum Discrete Event Systems. (arXiv:2104.09753v3 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2104.09753
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24314;&#31435;&#20102;&#37327;&#23376;&#31163;&#25955;&#20107;&#20214;&#31995;&#32479;&#30340;&#30417;&#30563;&#25511;&#21046;&#26694;&#26550;&#65292;&#20351;&#29992;&#37327;&#23376;&#26377;&#38480;&#29366;&#24577;&#33258;&#21160;&#26426;&#20316;&#20026;&#24314;&#27169;&#24418;&#24335;&#65292;&#24182;&#25552;&#20986;&#24182;&#35777;&#26126;&#20102;&#20854;&#30417;&#30563;&#25511;&#21046;&#23450;&#29702;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#20915;&#23450;QDES&#29366;&#24577;&#21487;&#36798;&#24615;&#38382;&#39064;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#65292;&#24182;&#19988;&#35813;&#26041;&#27861;&#21487;&#20197;&#30452;&#25509;&#25193;&#23637;&#21040;DES&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#25955;&#20107;&#20214;&#31995;&#32479;&#65288;DES&#65289;&#24050;&#22312;&#23454;&#36341;&#20013;&#24471;&#21040;&#28145;&#20837;&#21457;&#23637;&#21644;&#24212;&#29992;&#65292;&#20294;DES&#20013;&#30340;&#29366;&#24577;&#22797;&#26434;&#24230;&#20173;&#28982;&#26159;&#38656;&#35201;&#21019;&#26032;&#26041;&#27861;&#26356;&#22909;&#22320;&#35299;&#20915;&#30340;&#37325;&#35201;&#38382;&#39064;&#12290;&#38543;&#30528;&#37327;&#23376;&#35745;&#31639;&#21644;&#37327;&#23376;&#25511;&#21046;&#30340;&#21457;&#23637;&#65292;&#19968;&#20010;&#33258;&#28982;&#30340;&#38382;&#39064;&#26159;&#36890;&#36807;&#37327;&#23376;&#35745;&#31639;&#27169;&#22411;&#27169;&#25311;DES&#65292;&#24182;&#24314;&#31435;&#8220;&#37327;&#23376;&#31163;&#25955;&#20107;&#20214;&#31995;&#32479;&#8221;&#65288;QDES&#65289;&#12290;&#20854;&#21160;&#26426;&#26159;&#21452;&#37325;&#30340;&#65306;&#19968;&#26041;&#38754;&#65292;&#22312;&#21033;&#29992;&#37327;&#23376;&#35745;&#31639;&#26426;&#27169;&#25311;&#21644;&#22788;&#29702;DES&#26102;&#65292;QDES&#20855;&#26377;&#28508;&#22312;&#30340;&#24212;&#29992;&#20215;&#20540;&#65292;&#20854;&#20013;&#37327;&#23376;&#31995;&#32479;&#34987;&#29992;&#20110;&#27169;&#25311;&#30001;&#31163;&#25955;&#20107;&#20214;&#39537;&#21160;&#30340;&#29366;&#24577;&#28436;&#21270;&#65307;&#21478;&#19968;&#26041;&#38754;&#65292;&#23545;&#20110;&#27169;&#25311;&#26576;&#20123;&#23454;&#38469;&#38382;&#39064;&#65292;QDES&#21487;&#33021;&#27604;DES&#20855;&#26377;&#26356;&#37325;&#35201;&#30340;&#20248;&#21183;&#65292;&#23588;&#20854;&#26159;&#22312;&#29366;&#24577;&#22797;&#26434;&#24230;&#26041;&#38754;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#26088;&#22312;&#21033;&#29992;&#8220;&#37327;&#23376;&#26377;&#38480;&#29366;&#24577;&#33258;&#21160;&#26426;&#8221;&#65288;QFA&#65289;&#20316;&#20026;&#24314;&#27169;&#24418;&#24335;&#65292;&#24314;&#31435;QDES&#30340;&#22522;&#26412;&#26694;&#26550;&#65292;&#24182;&#24314;&#31435;&#24182;&#35777;&#26126;&#20102;QDES&#30340;&#30417;&#30563;&#25511;&#21046;&#23450;&#29702;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#20915;&#23450;QDES&#29366;&#24577;&#21487;&#36798;&#24615;&#38382;&#39064;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;DES&#20013;&#20351;&#29992;&#30340;&#26041;&#27861;&#21487;&#20197;&#30452;&#25509;&#25193;&#23637;&#21040;QDES&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20123;&#26696;&#20363;&#30740;&#31350;&#20197;&#35828;&#26126;&#25152;&#25552;&#20986;&#30340;QDES&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#21644;&#28508;&#22312;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discrete event systems (DES) have been deeply developed and applied in practice, but state complexity in DES still is an important problem to be better solved with innovative methods. With the development of quantum computing and quantum control, a natural problem is to simulate DES by means of quantum computing models and to establish {\it quantum DES} (QDES). The motivation is twofold: on the one hand, QDES have potential applications when DES are simulated and processed by quantum computers, where quantum systems are employed to simulate the evolution of states driven by discrete events, and on the other hand, QDES may have essential advantages over DES concerning state complexity for imitating some practical problems. So, the goal of this paper is to establish a basic framework of QDES by using {\it quantum finite automata} (QFA) as the modelling formalisms, and the supervisory control theorems of QDES are established and proved. Then we present a polynomial-time algorithm to decid
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#22024;&#26434;&#30340;&#20013;&#38388;&#35268;&#27169;&#37327;&#23376;&#35745;&#31639;&#26426;&#19978;&#36827;&#34892;&#30340;&#39318;&#20010;&#22823;&#20110;100&#20010;&#21477;&#23376;&#25968;&#25454;&#38598;&#30340;NLP&#23454;&#39564;&#32467;&#26524;&#65292;&#25104;&#21151;&#22320;&#35757;&#32451;&#20102;&#35299;&#20915;&#31616;&#21333;&#21477;&#23376;&#20998;&#31867;&#20219;&#21153;&#30340;NLP&#27169;&#22411;&#65292;&#35777;&#26126;&#20102;&#32452;&#21512;&#27169;&#22411;&#30340;&#21547;&#20041;&#19982;&#37327;&#23376;&#29702;&#35770;&#20855;&#26377;&#24418;&#24335;&#30456;&#20284;&#24615;&#12290;</title><link>http://arxiv.org/abs/2102.12846</link><description>&lt;p&gt;
&#23454;&#36341;&#20013;&#30340;QNLP&#65306;&#22312;&#37327;&#23376;&#35745;&#31639;&#26426;&#19978;&#36816;&#34892;&#32452;&#21512;&#27169;&#22411;&#30340;&#21547;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
QNLP in Practice: Running Compositional Models of Meaning on a Quantum Computer. (arXiv:2102.12846v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2102.12846
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#22024;&#26434;&#30340;&#20013;&#38388;&#35268;&#27169;&#37327;&#23376;&#35745;&#31639;&#26426;&#19978;&#36827;&#34892;&#30340;&#39318;&#20010;&#22823;&#20110;100&#20010;&#21477;&#23376;&#25968;&#25454;&#38598;&#30340;NLP&#23454;&#39564;&#32467;&#26524;&#65292;&#25104;&#21151;&#22320;&#35757;&#32451;&#20102;&#35299;&#20915;&#31616;&#21333;&#21477;&#23376;&#20998;&#31867;&#20219;&#21153;&#30340;NLP&#27169;&#22411;&#65292;&#35777;&#26126;&#20102;&#32452;&#21512;&#27169;&#22411;&#30340;&#21547;&#20041;&#19982;&#37327;&#23376;&#29702;&#35770;&#20855;&#26377;&#24418;&#24335;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;QNLP&#65289;&#28041;&#21450;&#35774;&#35745;&#21644;&#23454;&#29616;&#26088;&#22312;&#22312;&#37327;&#23376;&#30828;&#20214;&#19978;&#36816;&#34892;&#30340;NLP&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#22312;&#22024;&#26434;&#30340;&#20013;&#38388;&#35268;&#27169;&#37327;&#23376;&#65288;NISQ&#65289;&#35745;&#31639;&#26426;&#19978;&#36827;&#34892;&#30340;&#39318;&#20010;&#22823;&#20110;100&#20010;&#21477;&#23376;&#25968;&#25454;&#38598;&#30340;NLP&#23454;&#39564;&#32467;&#26524;&#12290;&#21033;&#29992;&#30001;Coecke&#12289;Sadrzadeh&#21644;Clark&#65288;2010&#65289;&#25552;&#20986;&#30340;&#21547;&#20041;&#32452;&#21512;&#27169;&#22411;&#19982;&#37327;&#23376;&#29702;&#35770;&#30340;&#24418;&#24335;&#30456;&#20284;&#24615;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#20855;&#26377;&#33258;&#28982;&#26144;&#23556;&#21040;&#37327;&#23376;&#30005;&#36335;&#30340;&#21477;&#23376;&#34920;&#31034;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#34920;&#31034;&#26469;&#23454;&#29616;&#24182;&#25104;&#21151;&#35757;&#32451;&#22312;&#37327;&#23376;&#30828;&#20214;&#19978;&#35299;&#20915;&#31616;&#21333;&#21477;&#23376;&#20998;&#31867;&#20219;&#21153;&#30340;NLP&#27169;&#22411;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#37327;&#23376;&#27169;&#25311;&#65292;&#27604;&#36739;&#20102;Coecke&#31561;&#20154;&#30340;&#35821;&#27861;&#25935;&#24863;&#27169;&#22411;&#19982;&#20351;&#29992;&#36739;&#23569;&#25110;&#26080;&#35821;&#27861;&#30340;&#20004;&#20010;&#22522;&#32447;&#65292;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#8220;&#35789;&#34955;&#8221;&#27169;&#22411;&#30340;&#37327;&#23376;&#27169;&#25311;&#65292;&#20854;&#20013;&#26681;&#26412;&#19981;&#32771;&#34385;&#35821;&#27861;&#65292;&#20197;&#21450;&#21333;&#35789;&#24207;&#21015;&#27169;&#22411;&#30340;&#37327;&#23376;&#27169;&#25311;&#65292;&#20165;&#23562;&#37325;&#21333;&#35789;&#39034;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum Natural Language Processing (QNLP) deals with the design and implementation of NLP models intended to be run on quantum hardware. In this paper, we present results on the first NLP experiments conducted on Noisy Intermediate-Scale Quantum (NISQ) computers for datasets of size greater than 100 sentences. Exploiting the formal similarity of the compositional model of meaning by Coecke, Sadrzadeh and Clark (2010) with quantum theory, we create representations for sentences that have a natural mapping to quantum circuits. We use these representations to implement and successfully train NLP models that solve simple sentence classification tasks on quantum hardware. We conduct quantum simulations that compare the syntax-sensitive model of Coecke et al. with two baselines that use less or no syntax; specifically, we implement the quantum analogues of a "bag-of-words" model, where syntax is not taken into account at all, and of a word-sequence model, where only word order is respected.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;ISP&#31934;&#39311;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#23558;RAW&#22270;&#20687;&#30340;&#39044;&#27979;&#19982;&#32463;&#36807;&#22788;&#29702;&#30340;RGB&#22270;&#20687;&#30340;&#39044;&#27979;&#30456;&#21305;&#37197;&#65292;&#25104;&#21151;&#22320;&#25552;&#39640;&#20102;&#30452;&#25509;&#23558;&#26426;&#22120;&#35270;&#35273;&#27169;&#22411;&#24212;&#29992;&#20110;RAW&#22270;&#20687;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2101.10203</link><description>&lt;p&gt;
ISP&#31934;&#39311;
&lt;/p&gt;
&lt;p&gt;
ISP Distillation. (arXiv:2101.10203v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2101.10203
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;ISP&#31934;&#39311;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#23558;RAW&#22270;&#20687;&#30340;&#39044;&#27979;&#19982;&#32463;&#36807;&#22788;&#29702;&#30340;RGB&#22270;&#20687;&#30340;&#39044;&#27979;&#30456;&#21305;&#37197;&#65292;&#25104;&#21151;&#22320;&#25552;&#39640;&#20102;&#30452;&#25509;&#23558;&#26426;&#22120;&#35270;&#35273;&#27169;&#22411;&#24212;&#29992;&#20110;RAW&#22270;&#20687;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#35768;&#22810;&#22270;&#20687;&#21482;&#33021;&#30001;&#26426;&#22120;&#32780;&#19981;&#26159;&#20154;&#31867;&#8220;&#35266;&#23519;&#8221;&#65292;&#20363;&#22914;&#22312;&#33258;&#21160;&#31995;&#32479;&#20013;&#12290;&#39640;&#32423;&#26426;&#22120;&#35270;&#35273;&#27169;&#22411;&#65288;&#22914;&#23545;&#35937;&#35782;&#21035;&#25110;&#35821;&#20041;&#20998;&#21106;&#65289;&#20551;&#23450;&#30456;&#26426;&#23558;&#22270;&#20687;&#36716;&#25442;&#20026;&#26576;&#20010;&#35268;&#33539;&#21270;&#30340;&#22270;&#20687;&#31354;&#38388;&#65292;&#21363;&#22270;&#20687;&#20449;&#21495;&#22788;&#29702;&#22120;&#65288;ISP&#65289;&#12290;&#28982;&#32780;&#65292;&#30456;&#26426;ISP&#20248;&#21270;&#30340;&#26159;&#20026;&#20154;&#31867;&#35266;&#23519;&#32773;&#29983;&#25104;&#35270;&#35273;&#19978;&#20196;&#20154;&#24841;&#24742;&#30340;&#22270;&#20687;&#65292;&#32780;&#19981;&#26159;&#26426;&#22120;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21487;&#33021;&#20250;&#30465;&#30053;ISP&#30340;&#35745;&#31639;&#26102;&#38388;&#65292;&#30452;&#25509;&#23558;&#35270;&#35273;&#27169;&#22411;&#24212;&#29992;&#20110;RAW&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#22312;RAW&#22270;&#20687;&#19978;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#20250;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#31181;&#19979;&#38477;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;RAW&#21644;RGB&#22270;&#20687;&#23545;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21487;&#20197;&#36731;&#26494;&#33719;&#21462;&#19988;&#26080;&#38656;&#20154;&#24037;&#26631;&#35760;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#26469;&#35757;&#32451;&#19968;&#20010;&#30452;&#25509;&#24212;&#29992;&#20110;RAW&#25968;&#25454;&#30340;&#27169;&#22411;&#65292;&#20351;&#24471;&#35813;&#27169;&#22411;&#23545;&#20110;RAW&#22270;&#20687;&#30340;&#39044;&#27979;&#19982;&#24050;&#22788;&#29702;&#30340;RGB&#22270;&#20687;&#30340;&#39044;&#27979;&#30456;&#21305;&#37197;&#12290;&#25105;&#20204;&#22312;&#35821;&#20041;&#20998;&#21106;&#21644;&#30446;&#26631;&#26816;&#27979;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;ISP&#31934;&#39311;&#26041;&#27861;&#21487;&#20197;&#25345;&#32493;&#32780;&#26174;&#33879;&#22320;&#25552;&#39640;&#30452;&#25509;&#24212;&#29992;&#26426;&#22120;&#35270;&#35273;&#27169;&#22411;&#20110;RAW&#22270;&#20687;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nowadays, many of the images captured are `observed' by machines only and not by humans, e.g., in autonomous systems. High-level machine vision models, such as object recognition or semantic segmentation, assume images are transformed into some canonical image space by the camera \ans{Image Signal Processor (ISP)}. However, the camera ISP is optimized for producing visually pleasing images for human observers and not for machines. Therefore, one may spare the ISP compute time and apply vision models directly to RAW images. Yet, it has been shown that training such models directly on RAW images results in a performance drop. To mitigate this drop, we use a RAW and RGB image pairs dataset, which can be easily acquired with no human labeling. We then train a model that is applied directly to the RAW data by using knowledge distillation such that the model predictions for RAW images will be aligned with the predictions of an off-the-shelf pre-trained model for processed RGB images. Our exp
&lt;/p&gt;</description></item></channel></rss>