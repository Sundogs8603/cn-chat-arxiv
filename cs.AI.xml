<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#36890;&#36807;&#28145;&#20837;&#30740;&#31350;Transformer-based&#35821;&#35328;&#27169;&#22411;&#22312;&#20107;&#23454;&#22238;&#24518;&#20219;&#21153;&#20013;&#30340;&#26426;&#21046;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#38646;/&#23569;&#27425;&#26679;&#26412;&#24773;&#20917;&#19979;&#30340;&#29305;&#23450;&#20219;&#21153;&#22836;&#12289;MLP&#23618;&#21644;&#27531;&#24046;&#27969;&#30340;&#21151;&#33021;&#65292;&#20197;&#21450;&#25239;&#36807;&#24230;&#33258;&#20449;&#26426;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.19521</link><description>&lt;p&gt;
&#35299;&#37322;&#22522;&#20110;Transformer&#27169;&#22411;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#20107;&#23454;&#22238;&#24518;&#20013;&#30340;&#20851;&#38190;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Interpreting Key Mechanisms of Factual Recall in Transformer-Based Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19521
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28145;&#20837;&#30740;&#31350;Transformer-based&#35821;&#35328;&#27169;&#22411;&#22312;&#20107;&#23454;&#22238;&#24518;&#20219;&#21153;&#20013;&#30340;&#26426;&#21046;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#38646;/&#23569;&#27425;&#26679;&#26412;&#24773;&#20917;&#19979;&#30340;&#29305;&#23450;&#20219;&#21153;&#22836;&#12289;MLP&#23618;&#21644;&#27531;&#24046;&#27969;&#30340;&#21151;&#33021;&#65292;&#20197;&#21450;&#25239;&#36807;&#24230;&#33258;&#20449;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;Transformer-based&#35821;&#35328;&#27169;&#22411;&#22312;&#20107;&#23454;&#22238;&#24518;&#20219;&#21153;&#20013;&#25152;&#37319;&#29992;&#30340;&#26426;&#21046;&#12290;&#22312;&#38646;&#27425;&#26679;&#26412;&#24773;&#20917;&#19979;&#65292;&#32473;&#23450;&#31867;&#20284;&#8220;&#27861;&#22269;&#30340;&#39318;&#37117;&#26159;&#8221;&#30340;&#25552;&#31034;&#65292;&#29305;&#23450;&#20219;&#21153;&#30340;&#27880;&#24847;&#21147;&#22836;&#20250;&#20174;&#19978;&#19979;&#25991;&#20013;&#25552;&#21462;&#20027;&#39064;&#23454;&#20307;&#65292;&#22914;&#8220;&#27861;&#22269;&#8221;&#65292;&#24182;&#23558;&#20854;&#20256;&#36882;&#32473;&#21518;&#32493;&#30340;MLP&#20197;&#22238;&#24518;&#25152;&#38656;&#30340;&#31572;&#26696;&#65292;&#22914;&#8220;&#24052;&#40654;&#8221;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#26088;&#22312;&#23558;MLP&#30340;&#36755;&#20986;&#20998;&#35299;&#20026;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#32452;&#20214;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#37327;&#21270;&#20102;&#36319;&#38543;&#36825;&#20123;&#29305;&#23450;&#20219;&#21153;&#22836;&#30340;MLP&#23618;&#30340;&#21151;&#33021;&#12290;&#22312;&#27531;&#24046;&#27969;&#20013;&#65292;&#23427;&#20250;&#25830;&#38500;&#25110;&#25918;&#22823;&#26469;&#33258;&#21508;&#20010;&#22836;&#30340;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#23427;&#20250;&#29983;&#25104;&#19968;&#20010;&#32452;&#20214;&#65292;&#23558;&#27531;&#24046;&#27969;&#37325;&#26032;&#23450;&#21521;&#21040;&#39044;&#26399;&#31572;&#26696;&#30340;&#26041;&#21521;&#12290;&#36825;&#20123;&#38646;&#27425;&#26426;&#21046;&#20063;&#36866;&#29992;&#20110;&#23569;&#27425;&#26679;&#26412;&#24773;&#20917;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#19968;&#31181;&#24191;&#27867;&#23384;&#22312;&#30340;&#25239;&#36807;&#24230;&#33258;&#20449;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19521v1 Announce Type: cross  Abstract: In this paper, we deeply explore the mechanisms employed by Transformer-based language models in factual recall tasks. In zero-shot scenarios, given a prompt like "The capital of France is," task-specific attention heads extract the topic entity, such as "France," from the context and pass it to subsequent MLPs to recall the required answer such as "Paris." We introduce a novel analysis method aimed at decomposing the outputs of the MLP into components understandable by humans. Through this method, we quantify the function of the MLP layer following these task-specific heads. In the residual stream, it either erases or amplifies the information originating from individual heads. Moreover, it generates a component that redirects the residual stream towards the direction of its expected answer. These zero-shot mechanisms are also employed in few-shot scenarios. Additionally, we observed a widely existent anti-overconfidence mechanism in 
&lt;/p&gt;</description></item><item><title>&#20026;&#20102;&#22686;&#21152;&#27979;&#35797;&#26399;&#38388;&#27491;&#30830;&#39044;&#27979;&#30340;&#26426;&#20250;&#65292;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#22270;&#20687;&#21040;&#22270;&#20687;&#30340;&#36716;&#25442;&#32416;&#27491;FSL&#27169;&#22411;&#30340;&#27979;&#35797;&#36755;&#20837;&#65292;&#29983;&#25104;&#34987;&#27979;&#35797;&#31867;&#21035;&#30340;&#26032;&#26679;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.18292</link><description>&lt;p&gt;
FSL&#27169;&#22411;&#21487;&#20197;&#22240;&#20026;&#20854;&#20248;&#36234;&#24615;&#24471;&#20998;&#26356;&#39640;
&lt;/p&gt;
&lt;p&gt;
FSL Model can Score Higher as It Is
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18292
&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22686;&#21152;&#27979;&#35797;&#26399;&#38388;&#27491;&#30830;&#39044;&#27979;&#30340;&#26426;&#20250;&#65292;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#22270;&#20687;&#21040;&#22270;&#20687;&#30340;&#36716;&#25442;&#32416;&#27491;FSL&#27169;&#22411;&#30340;&#27979;&#35797;&#36755;&#20837;&#65292;&#29983;&#25104;&#34987;&#27979;&#35797;&#31867;&#21035;&#30340;&#26032;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#65292;&#20026;&#20102;&#22686;&#21152;&#34987;&#27491;&#30830;&#35782;&#21035;&#30340;&#26426;&#20250;&#65292;&#25105;&#20204;&#20542;&#21521;&#20110;&#38754;&#23545;&#38754;&#22320;&#30452;&#35270;&#38754;&#37096;&#35782;&#21035;&#26426;&#65292;&#32780;&#19981;&#26159;&#20391;&#30528;&#38754;&#23545;&#12290;&#23569;&#26679;&#26412;&#23398;&#20064;&#65288;FSL&#65289;&#20998;&#31867;&#26412;&#36523;&#23601;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#27169;&#22411;&#24517;&#39035;&#35782;&#21035;&#23646;&#20110;&#35757;&#32451;&#26102;&#26410;&#35265;&#30340;&#31867;&#21035;&#30340;&#22270;&#20687;&#12290;&#22240;&#27492;&#65292;&#22312;&#27979;&#35797;&#26399;&#38388;&#23545;&#25197;&#26354;&#21644;&#38750;&#20856;&#22411;&#30340;&#26597;&#35810;&#25110;&#25903;&#25345;&#22270;&#20687;&#20250;&#35753;&#27169;&#22411;&#26356;&#38590;&#27491;&#30830;&#39044;&#27979;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#20026;&#20102;&#22686;&#21152;&#27979;&#35797;&#26399;&#38388;&#27491;&#30830;&#39044;&#27979;&#30340;&#26426;&#20250;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#22270;&#20687;&#21040;&#22270;&#20687;&#30340;&#36716;&#25442;&#32416;&#27491;&#35757;&#32451;&#36807;&#30340;FSL&#27169;&#22411;&#30340;&#27979;&#35797;&#36755;&#20837;&#65292;&#29983;&#25104;&#34987;&#27979;&#35797;&#31867;&#21035;&#30340;&#26032;&#26679;&#26412;&#12290;FSL&#27169;&#22411;&#36890;&#24120;&#26159;&#22312;&#20855;&#26377;&#36275;&#22815;&#26679;&#26412;&#30340;&#31867;&#21035;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#20855;&#26377;&#23569;&#26679;&#26412;&#26679;&#26412;&#30340;&#31867;&#21035;&#19978;&#36827;&#34892;&#27979;&#35797;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#39318;&#20808;&#25429;&#25417;&#27979;&#35797;&#22270;&#20687;&#30340;&#39118;&#26684;&#25110;&#24418;&#29366;&#65292;&#28982;&#21518;&#35782;&#21035;&#19968;&#20010;&#36866;&#24403;&#30340;&#35757;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18292v1 Announce Type: cross  Abstract: In daily life, we tend to present the front of our faces by staring squarely at a facial recognition machine, instead of facing it sideways, in order to increase the chance of being correctly recognised. Few-shot-learning (FSL) classification is challenging in itself because a model has to identify images that belong to classes previously unseen during training. Therefore, a warped and non-typical query or support image during testing can make it even more challenging for a model to predict correctly. In our work, to increase the chance of correct prediction during testing, we aim to rectify the test input of a trained FSL model by generating new samples of the tested classes through image-to-image translation. An FSL model is usually trained on classes with sufficient samples, and then tested on classes with few-shot samples. Our proposed method first captures the style or shape of the test image, and then identifies a suitable traine
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;LLMs&#36755;&#20986;&#20998;&#24067;&#36827;&#34892;&#27745;&#26579;&#26816;&#27979;&#30340;&#26041;&#27861;CDD&#65292;&#20197;&#21450;&#19968;&#31181;&#22522;&#20110;LLMs&#36755;&#20986;&#20462;&#27491;&#30340;&#21487;&#20449;&#35780;&#20272;&#26041;&#27861;TED&#65292;&#20197;&#24212;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#25454;&#27745;&#26579;&#21644;&#21487;&#20449;&#35780;&#20272;&#26041;&#38754;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.15938</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#25110;&#35760;&#24518;&#65306;&#25968;&#25454;&#27745;&#26579;&#19982;&#21487;&#20449;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Generalization or Memorization: Data Contamination and Trustworthy Evaluation for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15938
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;LLMs&#36755;&#20986;&#20998;&#24067;&#36827;&#34892;&#27745;&#26579;&#26816;&#27979;&#30340;&#26041;&#27861;CDD&#65292;&#20197;&#21450;&#19968;&#31181;&#22522;&#20110;LLMs&#36755;&#20986;&#20462;&#27491;&#30340;&#21487;&#20449;&#35780;&#20272;&#26041;&#27861;TED&#65292;&#20197;&#24212;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#25454;&#27745;&#26579;&#21644;&#21487;&#20449;&#35780;&#20272;&#26041;&#38754;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20851;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#33021;&#21147;&#30340;&#35828;&#27861;&#36890;&#24120;&#26159;&#36890;&#36807;&#22312;&#24320;&#25918;&#33719;&#21462;&#30340;&#22522;&#20934;&#19978;&#36827;&#34892;&#35780;&#20272;&#26469;&#25903;&#25345;&#30340;&#12290;&#32771;&#34385;&#21040;LLMs&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#24222;&#22823;&#35268;&#27169;&#21644;&#24191;&#27867;&#26469;&#28304;&#65292;&#23427;&#21487;&#33021;&#26126;&#30830;&#25110;&#38544;&#21547;&#22320;&#21253;&#21547;&#27979;&#35797;&#25968;&#25454;&#65292;&#23548;&#33268;LLMs&#26356;&#23481;&#26131;&#21463;&#21040;&#25968;&#25454;&#27745;&#26579;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35757;&#32451;&#25968;&#25454;&#30340;&#19981;&#36879;&#26126;&#24615;&#12289;&#27169;&#22411;&#30340;&#40657;&#30418;&#35775;&#38382;&#20197;&#21450;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#30340;&#24555;&#36895;&#22686;&#38271;&#65292;&#23545;&#20110;LLMs&#26469;&#35828;&#26816;&#27979;&#21644;&#20943;&#36731;&#25968;&#25454;&#27745;&#26579;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CDD&#65292;&#21363;&#36890;&#36807;LLMs&#36755;&#20986;&#20998;&#24067;&#36827;&#34892;&#27745;&#26579;&#26816;&#27979;&#30340;CDD&#12290;CDD&#20165;&#38656;&#35201;&#37319;&#26679;&#25991;&#26412;&#26469;&#26816;&#27979;&#25968;&#25454;&#27745;&#26579;&#65292;&#36890;&#36807;&#35782;&#21035;LLMs&#36755;&#20986;&#20998;&#24067;&#30340;&#23792;&#20540;&#26469;&#36827;&#34892;&#26816;&#27979;&#12290;&#20026;&#20102;&#20943;&#36731;&#35780;&#20272;&#20013;&#25968;&#25454;&#27745;&#26579;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;TED&#65306;&#22522;&#20110;LLMs&#36755;&#20986;&#20462;&#27491;&#30340;&#21487;&#20449;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15938v1 Announce Type: cross  Abstract: Recent statements about the impressive capabilities of large language models (LLMs) are usually supported by evaluating on open-access benchmarks. Considering the vast size and wide-ranging sources of LLMs' training data, it could explicitly or implicitly include test data, leading to LLMs being more susceptible to data contamination. However, due to the opacity of training data, the black-box access of models, and the rapid growth of synthetic training data, detecting and mitigating data contamination for LLMs faces significant challenges. In this paper, we propose CDD, which stands for Contamination Detection via output Distribution for LLMs. CDD necessitates only the sampled texts to detect data contamination, by identifying the peakedness of LLM's output distribution. To mitigate the impact of data contamination in evaluation, we also present TED: Trustworthy Evaluation via output Distribution, based on the correction of LLM's outp
&lt;/p&gt;</description></item><item><title>GenTranslate&#26159;&#19968;&#20010;&#26032;&#30340;&#32763;&#35793;&#20219;&#21153;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20016;&#23500;&#35821;&#35328;&#30693;&#35782;&#21644;&#24378;&#22823;&#25512;&#29702;&#33021;&#21147;&#65292;&#21487;&#20197;&#20174;N-best&#21015;&#34920;&#20013;&#29983;&#25104;&#26356;&#39640;&#36136;&#37327;&#30340;&#32763;&#35793;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.06894</link><description>&lt;p&gt;
GenTranslate: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#29983;&#25104;&#30340;&#22810;&#35821;&#35328;&#35821;&#38899;&#21644;&#26426;&#22120;&#32763;&#35793;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
GenTranslate: Large Language Models are Generative Multilingual Speech and Machine Translators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06894
&lt;/p&gt;
&lt;p&gt;
GenTranslate&#26159;&#19968;&#20010;&#26032;&#30340;&#32763;&#35793;&#20219;&#21153;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20016;&#23500;&#35821;&#35328;&#30693;&#35782;&#21644;&#24378;&#22823;&#25512;&#29702;&#33021;&#21147;&#65292;&#21487;&#20197;&#20174;N-best&#21015;&#34920;&#20013;&#29983;&#25104;&#26356;&#39640;&#36136;&#37327;&#30340;&#32763;&#35793;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#36890;&#36807;&#20943;&#23569;&#34920;&#31034;&#35823;&#24046;&#21644;&#24341;&#20837;&#22806;&#37096;&#30693;&#35782;&#65292;&#25512;&#21160;&#20102;&#22810;&#35821;&#35328;&#35821;&#38899;&#21644;&#26426;&#22120;&#32763;&#35793;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#32763;&#35793;&#20219;&#21153;&#36890;&#24120;&#20351;&#29992;&#26463;&#25628;&#32034;&#35299;&#30721;&#21644;&#21069;k&#20010;&#20551;&#35774;&#36873;&#25321;&#36827;&#34892;&#25512;&#29702;&#12290;&#36825;&#20123;&#25216;&#26415;&#24448;&#24448;&#19981;&#33021;&#20805;&#20998;&#21033;&#29992;&#22810;&#26679;&#21270;&#30340;N-best&#20551;&#35774;&#20013;&#30340;&#20016;&#23500;&#20449;&#24687;&#65292;&#20351;&#24471;&#23427;&#20204;&#22312;&#38656;&#35201;&#21333;&#20010;&#39640;&#36136;&#37327;&#36755;&#20986;&#24207;&#21015;&#30340;&#32763;&#35793;&#20219;&#21153;&#20013;&#25928;&#26524;&#19981;&#20339;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#32763;&#35793;&#20219;&#21153;&#29983;&#25104;&#27169;&#22411;&#65292;&#21363;&#8220;GenTranslate&#8221;&#65292;&#23427;&#22522;&#20110;LLMs&#26469;&#20174;N-best&#21015;&#34920;&#20013;&#29983;&#25104;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#21033;&#29992;LLMs&#20016;&#23500;&#30340;&#35821;&#35328;&#30693;&#35782;&#21644;&#24378;&#22823;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#25105;&#20204;&#30340;&#26032;&#27169;&#22411;&#21487;&#20197;&#23558;N-best&#20505;&#36873;&#20154;&#20013;&#30340;&#20016;&#23500;&#20449;&#24687;&#25972;&#21512;&#36215;&#26469;&#65292;&#29983;&#25104;&#26356;&#39640;&#36136;&#37327;&#30340;&#32763;&#35793;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#25903;&#25345;LLM&#30340;&#24494;&#35843;&#65292;&#25105;&#20204;&#26500;&#24314;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;HypoTransla&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in large language models (LLMs) have stepped forward the development of multilingual speech and machine translation by its reduced representation errors and incorporated external knowledge. However, both translation tasks typically utilize beam search decoding and top-1 hypothesis selection for inference. These techniques struggle to fully exploit the rich information in the diverse N-best hypotheses, making them less optimal for translation tasks that require a single, high-quality output sequence. In this paper, we propose a new generative paradigm for translation tasks, namely "GenTranslate", which builds upon LLMs to generate better results from the diverse translation versions in N-best list. Leveraging the rich linguistic knowledge and strong reasoning abilities of LLMs, our new paradigm can integrate the rich information in N-best candidates to generate a higher-quality translation result. Furthermore, to support LLM finetuning, we build and release a HypoTransla
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;ConsiStory&#65292;&#36890;&#36807;&#20849;&#20139;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20869;&#37096;&#28608;&#27963;&#65292;&#23454;&#29616;&#20102;&#19968;&#33268;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#12290;&#24341;&#20837;&#20102;&#20027;&#39064;&#39537;&#21160;&#30340;&#20849;&#20139;&#27880;&#24847;&#21147;&#22359;&#21644;&#22522;&#20110;&#23545;&#24212;&#30340;&#29305;&#24449;&#27880;&#20837;&#65292;&#20419;&#36827;&#20102;&#22270;&#20687;&#20043;&#38388;&#30340;&#20027;&#39064;&#19968;&#33268;&#24615;&#65292;&#24182;&#37319;&#29992;&#20102;&#31574;&#30053;&#26469;&#20445;&#25345;&#24067;&#23616;&#22810;&#26679;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03286</link><description>&lt;p&gt;
&#26080;&#38656;&#35757;&#32451;&#30340;&#19968;&#33268;&#24615;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Training-Free Consistent Text-to-Image Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03286
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;ConsiStory&#65292;&#36890;&#36807;&#20849;&#20139;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20869;&#37096;&#28608;&#27963;&#65292;&#23454;&#29616;&#20102;&#19968;&#33268;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#12290;&#24341;&#20837;&#20102;&#20027;&#39064;&#39537;&#21160;&#30340;&#20849;&#20139;&#27880;&#24847;&#21147;&#22359;&#21644;&#22522;&#20110;&#23545;&#24212;&#30340;&#29305;&#24449;&#27880;&#20837;&#65292;&#20419;&#36827;&#20102;&#22270;&#20687;&#20043;&#38388;&#30340;&#20027;&#39064;&#19968;&#33268;&#24615;&#65292;&#24182;&#37319;&#29992;&#20102;&#31574;&#30053;&#26469;&#20445;&#25345;&#24067;&#23616;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#24341;&#23548;&#22270;&#20687;&#29983;&#25104;&#36807;&#31243;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#21019;&#36896;&#24615;&#28789;&#27963;&#24615;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#22312;&#22810;&#26679;&#21270;&#30340;&#25552;&#31034;&#19979;&#19968;&#33268;&#22320;&#25551;&#32472;&#30456;&#21516;&#30340;&#20027;&#39064;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#20248;&#21270;&#27169;&#22411;&#26469;&#25945;&#25480;&#23427;&#25551;&#36848;&#29305;&#23450;&#29992;&#25143;&#25552;&#20379;&#20027;&#39064;&#30340;&#26032;&#35789;&#27719;&#25110;&#32773;&#20026;&#27169;&#22411;&#28155;&#21152;&#22270;&#20687;&#26465;&#20214;&#12290;&#36825;&#20123;&#26041;&#27861;&#35201;&#27714;&#38024;&#23545;&#27599;&#20010;&#20027;&#39064;&#36827;&#34892;&#28459;&#38271;&#30340;&#20248;&#21270;&#25110;&#36827;&#34892;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#22312;&#23558;&#29983;&#25104;&#30340;&#22270;&#20687;&#19982;&#25991;&#26412;&#25552;&#31034;&#23545;&#40784;&#21644;&#25551;&#32472;&#22810;&#20010;&#20027;&#39064;&#26041;&#38754;&#36935;&#21040;&#22256;&#38590;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26080;&#35757;&#32451;&#26041;&#27861;ConsiStory&#65292;&#36890;&#36807;&#20849;&#20139;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20869;&#37096;&#28608;&#27963;&#26469;&#23454;&#29616;&#19968;&#33268;&#30340;&#20027;&#39064;&#29983;&#25104;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20027;&#39064;&#39537;&#21160;&#20849;&#20139;&#27880;&#24847;&#21147;&#22359;&#21644;&#22522;&#20110;&#23545;&#24212;&#30340;&#29305;&#24449;&#27880;&#20837;&#65292;&#20197;&#20419;&#36827;&#22270;&#20687;&#20043;&#38388;&#30340;&#20027;&#39064;&#19968;&#33268;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#31574;&#30053;&#20197;&#40723;&#21169;&#24067;&#23616;&#22810;&#26679;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#20027;&#39064;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image models offer a new level of creative flexibility by allowing users to guide the image generation process through natural language. However, using these models to consistently portray the same subject across diverse prompts remains challenging. Existing approaches fine-tune the model to teach it new words that describe specific user-provided subjects or add image conditioning to the model. These methods require lengthy per-subject optimization or large-scale pre-training. Moreover, they struggle to align generated images with text prompts and face difficulties in portraying multiple subjects. Here, we present ConsiStory, a training-free approach that enables consistent subject generation by sharing the internal activations of the pretrained model. We introduce a subject-driven shared attention block and correspondence-based feature injection to promote subject consistency between images. Additionally, we develop strategies to encourage layout diversity while maintaining su
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Lookbehind-SAM&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#27425;&#19978;&#21319;&#27493;&#39588;&#21644;&#32447;&#24615;&#25554;&#20540;&#26469;&#22686;&#24378;&#26368;&#22823;&#21270;&#21644;&#26368;&#23567;&#21270;&#36807;&#31243;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#25439;&#22833;&#38160;&#24230;&#25240;&#34935;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#37117;&#26377;&#22810;&#31181;&#20248;&#28857;&#65292;&#21253;&#25324;&#25552;&#39640;&#30340;&#27867;&#21270;&#24615;&#33021;&#12289;&#26356;&#39640;&#30340;&#40065;&#26834;&#24615;&#21644;&#25913;&#36827;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2307.16704</link><description>&lt;p&gt;
Lookbehind-SAM: k&#27493;&#22238;&#26395;&#65292;1&#27493;&#21069;&#36827;
&lt;/p&gt;
&lt;p&gt;
Lookbehind-SAM: k steps back, 1 step forward
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2307.16704
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Lookbehind-SAM&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#27425;&#19978;&#21319;&#27493;&#39588;&#21644;&#32447;&#24615;&#25554;&#20540;&#26469;&#22686;&#24378;&#26368;&#22823;&#21270;&#21644;&#26368;&#23567;&#21270;&#36807;&#31243;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#25439;&#22833;&#38160;&#24230;&#25240;&#34935;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#37117;&#26377;&#22810;&#31181;&#20248;&#28857;&#65292;&#21253;&#25324;&#25552;&#39640;&#30340;&#27867;&#21270;&#24615;&#33021;&#12289;&#26356;&#39640;&#30340;&#40065;&#26834;&#24615;&#21644;&#25913;&#36827;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38160;&#24230;&#24863;&#30693;&#20248;&#21270;&#65288;SAM&#65289;&#26041;&#27861;&#36890;&#36807;&#23558;&#26368;&#23567;&#21270;&#25439;&#22833;&#20540;&#21644;&#25439;&#22833;&#38160;&#24230;&#38382;&#39064;&#34920;&#36848;&#20026;&#26497;&#23567;&#26497;&#22823;&#22411;&#30446;&#26631;&#65292;&#24471;&#21040;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#22686;&#21152;&#20102;SAM&#30446;&#26631;&#20013;&#26368;&#22823;&#21270;&#21644;&#26368;&#23567;&#21270;&#37096;&#20998;&#30340;&#25928;&#29575;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#25439;&#22833;&#38160;&#24230;&#25240;&#34935;&#12290;&#21463;Lookahead&#20248;&#21270;&#22120;&#30340;&#21551;&#21457;&#65292;&#35813;&#20248;&#21270;&#22120;&#20351;&#29992;&#22810;&#20010;&#21521;&#21069;&#30340;&#19979;&#38477;&#27493;&#39588;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Lookbehind&#65292;&#23427;&#22312;&#21518;&#38754;&#25191;&#34892;&#22810;&#20010;&#19978;&#21319;&#27493;&#39588;&#65292;&#22686;&#24378;&#20102;SAM&#30340;&#26368;&#22823;&#21270;&#27493;&#39588;&#65292;&#24182;&#25214;&#21040;&#20102;&#19968;&#20010;&#20855;&#26377;&#26356;&#39640;&#25439;&#22833;&#30340;&#26368;&#22351;&#24773;&#20917;&#25200;&#21160;&#12290;&#28982;&#21518;&#65292;&#20026;&#20102;&#20943;&#23567;&#30001;&#20110;&#25910;&#38598;&#21040;&#30340;&#22810;&#20010;&#19978;&#21319;&#27493;&#39588;&#30340;&#26799;&#24230;&#25152;&#24341;&#36215;&#30340;&#19979;&#38477;&#27493;&#39588;&#30340;&#26041;&#24046;&#65292;&#25105;&#20204;&#37319;&#29992;&#32447;&#24615;&#25554;&#20540;&#26469;&#25913;&#36827;&#26368;&#23567;&#21270;&#36807;&#31243;&#12290;Lookbehind&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#24102;&#26469;&#20102;&#35768;&#22810;&#22909;&#22788;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25552;&#39640;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#23545;&#22122;&#22768;&#26435;&#37325;&#30340;&#26356;&#39640;&#40065;&#26834;&#24615;&#65292;&#20197;&#21450;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#25913;&#36827;&#30340;&#25928;&#26524;&#21644;&#36739;&#23569;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sharpness-aware minimization (SAM) methods have gained increasing popularity by formulating the problem of minimizing both loss value and loss sharpness as a minimax objective. In this work, we increase the efficiency of the maximization and minimization parts of SAM's objective to achieve a better loss-sharpness trade-off. By taking inspiration from the Lookahead optimizer, which uses multiple descent steps ahead, we propose Lookbehind, which performs multiple ascent steps behind to enhance the maximization step of SAM and find a worst-case perturbation with higher loss. Then, to mitigate the variance in the descent step arising from the gathered gradients across the multiple ascent steps, we employ linear interpolation to refine the minimization step. Lookbehind leads to a myriad of benefits across a variety of tasks. Particularly, we show increased generalization performance, greater robustness against noisy weights, as well as improved learning and less catastrophic forgetting in l
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30450;&#28857;&#22312;&#20110;&#20854;&#23545;&#36229;&#21465;&#20107;&#35821;&#35328;&#20449;&#24687;&#30340;&#24573;&#35270;&#65292;&#30740;&#31350;&#25552;&#20986;&#32771;&#34385;&#27169;&#22411;&#22914;&#20309;&#24863;&#30693;&#35821;&#35328;&#20449;&#24687;&#26377;&#21161;&#20110;&#28145;&#20837;&#20102;&#35299;&#20854;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2306.06794</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30450;&#28857;&#65306;&#36229;&#21465;&#20107;&#35821;&#35328;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
A blind spot for large language models: Supradiegetic linguistic information
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.06794
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30450;&#28857;&#22312;&#20110;&#20854;&#23545;&#36229;&#21465;&#20107;&#35821;&#35328;&#20449;&#24687;&#30340;&#24573;&#35270;&#65292;&#30740;&#31350;&#25552;&#20986;&#32771;&#34385;&#27169;&#22411;&#22914;&#20309;&#24863;&#30693;&#35821;&#35328;&#20449;&#24687;&#26377;&#21161;&#20110;&#28145;&#20837;&#20102;&#35299;&#20854;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20687;ChatGPT&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21453;&#26144;&#20102;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#28145;&#21051;&#21464;&#38761;&#65292;&#23454;&#29616;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#29978;&#33267;&#20196;&#20154;&#38663;&#24778;&#30340;&#31867;&#20154;&#35821;&#35328;&#27969;&#21033;&#24230;&#12290;&#23427;&#20204;&#30446;&#21069;&#21644;&#28508;&#22312;&#30340;&#33021;&#21147;&#33539;&#22260;&#26159;&#19968;&#20010;&#31215;&#26497;&#25506;&#35752;&#30340;&#39046;&#22495;&#65292;&#32477;&#38750;&#20165;&#38480;&#20110;&#31185;&#30740;&#20154;&#21592;&#12290;&#20154;&#20204;&#36890;&#24120;&#23558;LLMs&#30340;&#35757;&#32451;&#25968;&#25454;&#26694;&#23450;&#20026;&#8220;&#25991;&#26412;&#8221;&#29978;&#33267;&#8220;&#35821;&#35328;&#8221;&#12290;&#25105;&#20204;&#20351;&#29992;&#26469;&#33258;&#35821;&#35328;&#23398;&#12289;&#20307;&#29616;&#35748;&#30693;&#12289;&#35748;&#30693;&#31185;&#23398;&#12289;&#25968;&#23398;&#21644;&#21382;&#21490;&#31561;&#39046;&#22495;&#30340;&#24605;&#24819;&#65292;&#20180;&#32454;&#23457;&#35270;&#36825;&#19968;&#26694;&#26550;&#30340;&#32454;&#33410;&#12290;&#25105;&#20204;&#25552;&#20986;&#65292;&#32771;&#34385;&#20687;ChatGPT&#36825;&#26679;&#30340;LLM&#26159;&#20160;&#20040;&#24863;&#35273;&#65292;&#27491;&#22914;&#32435;&#26684;&#23572;&#21487;&#33021;&#20250;&#35828;&#30340;&#37027;&#26679;&#65292;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#28145;&#20837;&#20102;&#35299;&#20854;&#25972;&#20307;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#65292;&#20854;&#25509;&#21463;&#30340;&#35821;&#35328;&#35757;&#32451;&#25968;&#25454;&#21487;&#20197;&#34987;&#26377;&#30410;&#22320;&#37325;&#26032;&#26500;&#24605;&#20026;&#23545;&#35821;&#35328;&#20013;&#32534;&#30721;&#30340;&#21465;&#20107;&#20449;&#24687;&#30340;&#25509;&#35302;&#65292;&#20854;&#32570;&#38519;&#21487;&#20197;&#34987;&#37325;&#26032;&#26500;&#24605;&#20026;&#23545;&#36825;&#20123;&#20449;&#24687;&#30340;&#26080;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2306.06794v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) like ChatGPT reflect profound changes in the field of Artificial Intelligence, achieving a linguistic fluency that is impressively, even shockingly, human-like. The extent of their current and potential capabilities is an active area of investigation by no means limited to scientific researchers. It is common for people to frame the training data for LLMs as "text" or even "language". We examine the details of this framing using ideas from several areas, including linguistics, embodied cognition, cognitive science, mathematics, and history. We propose that considering what it is like to be an LLM like ChatGPT, as Nagel might have put it, can help us gain insight into its capabilities in general, and in particular, that its exposure to linguistic training data can be productively reframed as exposure to the diegetic information encoded in language, and its deficits can be reframed as ignorance of ext
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#32508;&#36848;&#20171;&#32461;&#20102;&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#22312;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#26041;&#38754;&#30340;&#30740;&#31350;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#26368;&#36817;&#30340;&#26041;&#27861;&#35797;&#22270;&#23558;&#31526;&#21495;&#25512;&#29702;&#21644;&#28145;&#24230;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#20197;&#29983;&#25104;&#20855;&#26377;&#35299;&#37322;&#24615;&#12289;&#31454;&#20105;&#24615;&#33021;&#21147;&#24182;&#38598;&#25104;&#19987;&#23478;&#30693;&#35782;&#30340;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2302.07200</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#30340;&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.07200
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#32508;&#36848;&#20171;&#32461;&#20102;&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#22312;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#26041;&#38754;&#30340;&#30740;&#31350;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#26368;&#36817;&#30340;&#26041;&#27861;&#35797;&#22270;&#23558;&#31526;&#21495;&#25512;&#29702;&#21644;&#28145;&#24230;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#20197;&#29983;&#25104;&#20855;&#26377;&#35299;&#37322;&#24615;&#12289;&#31454;&#20105;&#24615;&#33021;&#21147;&#24182;&#38598;&#25104;&#19987;&#23478;&#30693;&#35782;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#26159;&#19968;&#20010;&#26085;&#30410;&#27963;&#36291;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#23427;&#23558;&#31526;&#21495;&#25512;&#29702;&#26041;&#27861;&#19982;&#28145;&#24230;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#20197;&#21033;&#29992;&#23427;&#20204;&#30340;&#20114;&#34917;&#20248;&#21183;&#12290;&#38543;&#30528;&#30693;&#35782;&#22270;&#35889;&#25104;&#20026;&#34920;&#31034;&#24322;&#26500;&#21644;&#22810;&#20851;&#31995;&#25968;&#25454;&#30340;&#19968;&#31181;&#27969;&#34892;&#26041;&#24335;&#65292;&#23545;&#22270;&#32467;&#26500;&#36827;&#34892;&#25512;&#29702;&#30340;&#26041;&#27861;&#24320;&#22987;&#36981;&#24490;&#36825;&#31181;&#31070;&#32463;&#31526;&#21495;&#33539;&#24335;&#12290;&#20256;&#32479;&#19978;&#65292;&#36825;&#20123;&#26041;&#27861;&#35201;&#20040;&#21033;&#29992;&#22522;&#20110;&#35268;&#21017;&#30340;&#25512;&#29702;&#65292;&#35201;&#20040;&#29983;&#25104;&#20195;&#34920;&#24615;&#30340;&#25968;&#20540;&#23884;&#20837;&#65292;&#20174;&#20013;&#21487;&#20197;&#25552;&#21462;&#20986;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#19968;&#20123;&#30740;&#31350;&#23581;&#35797;&#24357;&#21512;&#36825;&#31181;&#20108;&#20803;&#23545;&#31435;&#65292;&#25552;&#20986;&#20102;&#33021;&#22815;&#20419;&#36827;&#21487;&#35299;&#37322;&#24615;&#12289;&#20445;&#25345;&#31454;&#20105;&#24615;&#33021;&#21147;&#24182;&#38598;&#25104;&#19987;&#23478;&#30693;&#35782;&#30340;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#22312;&#30693;&#35782;&#22270;&#35889;&#19978;&#25191;&#34892;&#31070;&#32463;&#31526;&#21495;&#25512;&#29702;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#31867;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#20010;&#20027;&#35201;&#31867;&#21035;&#65306;&#65288;1&#65289;&#36923;&#36753;&#20449;&#24687;&#23884;&#20837;&#26041;&#27861;&#65292;&#65288;2&#65289;&#22522;&#20110;&#23884;&#20837;&#30340;&#26041;&#27861;&#19982;&#36923;&#36753;&#19968;&#33268;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Neurosymbolic AI is an increasingly active area of research that combines symbolic reasoning methods with deep learning to leverage their complementary benefits. As knowledge graphs are becoming a popular way to represent heterogeneous and multi-relational data, methods for reasoning on graph structures have attempted to follow this neurosymbolic paradigm. Traditionally, such approaches have utilized either rule-based inference or generated representative numerical embeddings from which patterns could be extracted. However, several recent studies have attempted to bridge this dichotomy to generate models that facilitate interpretability, maintain competitive performance, and integrate expert knowledge. Therefore, we survey methods that perform neurosymbolic reasoning tasks on knowledge graphs and propose a novel taxonomy by which we can classify them. Specifically, we propose three major categories: (1) logically-informed embedding approaches, (2) embedding approaches with logical cons
&lt;/p&gt;</description></item><item><title>HyperSense&#26159;&#19968;&#20010;&#21327;&#21516;&#35774;&#35745;&#30340;&#30828;&#20214;&#21644;&#36719;&#20214;&#31995;&#32479;&#65292;&#33021;&#22815;&#26681;&#25454;&#20256;&#24863;&#22120;&#25968;&#25454;&#20013;&#30340;&#29289;&#20307;&#23384;&#22312;&#39044;&#27979;&#26377;&#25928;&#22320;&#25511;&#21046;&#25968;&#25454;&#29983;&#25104;&#36895;&#29575;&#65292;&#36890;&#36807;&#20351;&#29992;&#20302;&#31934;&#24230;ADC&#20943;&#23569;&#20887;&#20313;&#25968;&#25454;&#38477;&#20302;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#25104;&#26412;&#65292;&#21033;&#29992;&#36229;&#32500;&#24230;&#35745;&#31639;&#30340;&#29305;&#28857;&#20998;&#26512;&#23454;&#26102;&#30340;&#20302;&#31934;&#24230;&#20256;&#24863;&#22120;&#25968;&#25454;&#65292;&#22312;&#22788;&#29702;&#22122;&#22768;&#12289;&#20197;&#20869;&#23384;&#20026;&#20013;&#24515;&#21644;&#23454;&#26102;&#23398;&#20064;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;&#35813;&#31995;&#32479;&#32467;&#21512;&#20102;&#39640;&#24615;&#33021;&#30340;&#29289;&#20307;&#26816;&#27979;&#36719;&#20214;&#21644;&#23454;&#26102;&#30340;&#30828;&#20214;&#39044;&#27979;&#65292;&#24341;&#20837;&#20102;&#26234;&#33021;&#20256;&#24863;&#22120;&#25511;&#21046;&#30340;&#26032;&#27010;&#24565;&#12290;&#22312;&#36719;&#20214;&#21644;&#30828;&#20214;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.10267</link><description>&lt;p&gt;
HyperSense: &#21152;&#36895;&#36229;&#32500;&#24230;&#35745;&#31639;&#20197;&#29992;&#20110;&#26234;&#33021;&#20256;&#24863;&#22120;&#25968;&#25454;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
HyperSense: Accelerating Hyper-Dimensional Computing for Intelligent Sensor Data Processing. (arXiv:2401.10267v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10267
&lt;/p&gt;
&lt;p&gt;
HyperSense&#26159;&#19968;&#20010;&#21327;&#21516;&#35774;&#35745;&#30340;&#30828;&#20214;&#21644;&#36719;&#20214;&#31995;&#32479;&#65292;&#33021;&#22815;&#26681;&#25454;&#20256;&#24863;&#22120;&#25968;&#25454;&#20013;&#30340;&#29289;&#20307;&#23384;&#22312;&#39044;&#27979;&#26377;&#25928;&#22320;&#25511;&#21046;&#25968;&#25454;&#29983;&#25104;&#36895;&#29575;&#65292;&#36890;&#36807;&#20351;&#29992;&#20302;&#31934;&#24230;ADC&#20943;&#23569;&#20887;&#20313;&#25968;&#25454;&#38477;&#20302;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#25104;&#26412;&#65292;&#21033;&#29992;&#36229;&#32500;&#24230;&#35745;&#31639;&#30340;&#29305;&#28857;&#20998;&#26512;&#23454;&#26102;&#30340;&#20302;&#31934;&#24230;&#20256;&#24863;&#22120;&#25968;&#25454;&#65292;&#22312;&#22788;&#29702;&#22122;&#22768;&#12289;&#20197;&#20869;&#23384;&#20026;&#20013;&#24515;&#21644;&#23454;&#26102;&#23398;&#20064;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;&#35813;&#31995;&#32479;&#32467;&#21512;&#20102;&#39640;&#24615;&#33021;&#30340;&#29289;&#20307;&#26816;&#27979;&#36719;&#20214;&#21644;&#23454;&#26102;&#30340;&#30828;&#20214;&#39044;&#27979;&#65292;&#24341;&#20837;&#20102;&#26234;&#33021;&#20256;&#24863;&#22120;&#25511;&#21046;&#30340;&#26032;&#27010;&#24565;&#12290;&#22312;&#36719;&#20214;&#21644;&#30828;&#20214;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;HyperSense&#65292;&#25105;&#20204;&#21327;&#21516;&#35774;&#35745;&#30340;&#30828;&#20214;&#21644;&#36719;&#20214;&#31995;&#32479;&#26681;&#25454;&#20256;&#24863;&#22120;&#25968;&#25454;&#20013;&#30340;&#29289;&#20307;&#23384;&#22312;&#39044;&#27979;&#26377;&#25928;&#22320;&#25511;&#21046;&#27169;&#25311;&#21040;&#25968;&#23383;&#36716;&#25442;&#22120;&#65288;ADC&#65289;&#27169;&#22359;&#30340;&#25968;&#25454;&#29983;&#25104;&#36895;&#29575;&#12290;&#38024;&#23545;&#19981;&#26029;&#22686;&#21152;&#30340;&#20256;&#24863;&#22120;&#25968;&#37327;&#21644;&#25968;&#25454;&#36895;&#29575;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;HyperSense&#20351;&#29992;&#39640;&#25928;&#30340;&#20302;&#31934;&#24230;ADC&#20943;&#23569;&#20887;&#20313;&#30340;&#25968;&#23383;&#25968;&#25454;&#65292;&#38477;&#20302;&#20102;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#25104;&#26412;&#12290;&#21033;&#29992;&#31070;&#32463;&#21551;&#21457;&#30340;&#36229;&#32500;&#24230;&#35745;&#31639;&#65288;HDC&#65289;&#65292;HyperSense&#20998;&#26512;&#23454;&#26102;&#30340;&#21407;&#22987;&#20302;&#31934;&#24230;&#20256;&#24863;&#22120;&#25968;&#25454;&#65292;&#22312;&#22788;&#29702;&#22122;&#22768;&#12289;&#20197;&#20869;&#23384;&#20026;&#20013;&#24515;&#21644;&#23454;&#26102;&#23398;&#20064;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;HyperSense&#27169;&#22411;&#23558;&#39640;&#24615;&#33021;&#30340;&#29289;&#20307;&#26816;&#27979;&#36719;&#20214;&#19982;&#23454;&#26102;&#30340;&#30828;&#20214;&#39044;&#27979;&#32467;&#21512;&#36215;&#26469;&#65292;&#24341;&#20837;&#20102;&#26234;&#33021;&#20256;&#24863;&#22120;&#25511;&#21046;&#30340;&#26032;&#27010;&#24565;&#12290;&#20840;&#38754;&#30340;&#36719;&#20214;&#21644;&#30828;&#20214;&#35780;&#20272;&#23637;&#31034;&#20102;&#25105;&#20204;&#35299;&#20915;&#26041;&#26696;&#30340;&#21331;&#36234;&#24615;&#33021;&#65292;&#36890;&#36807;&#26368;&#39640;&#30340;&#26354;&#32447;&#19979;&#38754;&#31215;&#65288;AUC&#65289;&#21644;&#26368;&#38497;&#30340;&#25509;&#25910;&#22120;&#25805;&#20316;&#29305;&#24615;&#65288;ROC&#65289;&#26354;&#32447;&#35777;&#26126;&#20102;&#36825;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Introducing HyperSense, our co-designed hardware and software system efficiently controls Analog-to-Digital Converter (ADC) modules' data generation rate based on object presence predictions in sensor data. Addressing challenges posed by escalating sensor quantities and data rates, HyperSense reduces redundant digital data using energy-efficient low-precision ADC, diminishing machine learning system costs. Leveraging neurally-inspired HyperDimensional Computing (HDC), HyperSense analyzes real-time raw low-precision sensor data, offering advantages in handling noise, memory-centricity, and real-time learning.  Our proposed HyperSense model combines high-performance software for object detection with real-time hardware prediction, introducing the novel concept of Intelligent Sensor Control. Comprehensive software and hardware evaluations demonstrate our solution's superior performance, evidenced by the highest Area Under the Curve (AUC) and sharpest Receiver Operating Characteristic (ROC
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#65292;&#21363;&#26368;&#23567;&#19981;&#19968;&#33268;&#24230;&#37327;&#65288;LDM&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#22797;&#26434;&#20915;&#31574;&#36793;&#30028;&#24773;&#20917;&#19979;&#30340;&#20027;&#21160;&#23398;&#20064;&#38382;&#39064;&#12290;&#36890;&#36807;&#26597;&#35810;&#20855;&#26377;&#26368;&#23567;LDM&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#65292;&#21487;&#20197;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.09787</link><description>&lt;p&gt;
&#26597;&#35810;&#26131;&#20110;&#32763;&#36716;&#26679;&#26412;&#30340;&#28145;&#24230;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Querying Easily Flip-flopped Samples for Deep Active Learning. (arXiv:2401.09787v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09787
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#65292;&#21363;&#26368;&#23567;&#19981;&#19968;&#33268;&#24230;&#37327;&#65288;LDM&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#22797;&#26434;&#20915;&#31574;&#36793;&#30028;&#24773;&#20917;&#19979;&#30340;&#20027;&#21160;&#23398;&#20064;&#38382;&#39064;&#12290;&#36890;&#36807;&#26597;&#35810;&#20855;&#26377;&#26368;&#23567;LDM&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#65292;&#21487;&#20197;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#23398;&#20064;&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#26088;&#22312;&#36890;&#36807;&#36873;&#25321;&#21644;&#26597;&#35810;&#26410;&#26631;&#35760;&#25968;&#25454;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#19968;&#31181;&#26377;&#25928;&#30340;&#36873;&#25321;&#31574;&#30053;&#26159;&#22522;&#20110;&#27169;&#22411;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#65292;&#36825;&#21487;&#20197;&#35299;&#37322;&#20026;&#26679;&#26412;&#30340;&#20449;&#24687;&#37327;&#24230;&#37327;&#12290;&#26679;&#26412;&#21040;&#20915;&#31574;&#36793;&#30028;&#30340;&#36317;&#31163;&#26159;&#19968;&#31181;&#33258;&#28982;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#65292;&#20294;&#36890;&#24120;&#38590;&#20197;&#35745;&#31639;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#22810;&#31867;&#20998;&#31867;&#20219;&#21153;&#20013;&#24418;&#25104;&#30340;&#22797;&#26434;&#20915;&#31574;&#36793;&#30028;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#8220;&#26368;&#23567;&#19981;&#19968;&#33268;&#24230;&#37327;&#8221;&#65288;LDM&#65289;&#65292;&#23450;&#20041;&#20026;&#39044;&#27979;&#26631;&#31614;&#19981;&#19968;&#33268;&#30340;&#26368;&#23567;&#27010;&#29575;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;LDM&#30340;&#20272;&#35745;&#22120;&#22312;&#28201;&#21644;&#20551;&#35774;&#19979;&#26159;&#28176;&#36817;&#19968;&#33268;&#30340;&#12290;&#35813;&#20272;&#35745;&#22120;&#35745;&#31639;&#25928;&#29575;&#39640;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#21442;&#25968;&#25200;&#21160;&#36731;&#26494;&#23454;&#29616;&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#20351;&#29992;&#12290;&#22522;&#20110;LDM&#30340;&#20027;&#21160;&#23398;&#20064;&#36890;&#36807;&#26597;&#35810;&#20855;&#26377;&#26368;&#23567;LDM&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#26469;&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Active learning is a machine learning paradigm that aims to improve the performance of a model by strategically selecting and querying unlabeled data. One effective selection strategy is to base it on the model's predictive uncertainty, which can be interpreted as a measure of how informative a sample is. The sample's distance to the decision boundary is a natural measure of predictive uncertainty, but it is often intractable to compute, especially for complex decision boundaries formed in multiclass classification tasks. To address this issue, this paper proposes the {\it least disagree metric} (LDM), defined as the smallest probability of disagreement of the predicted label, and an estimator for LDM proven to be asymptotically consistent under mild assumptions. The estimator is computationally efficient and can be easily implemented for deep learning models using parameter perturbation. The LDM-based active learning is performed by querying unlabeled data with the smallest LDM. Exper
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20197;&#23562;&#37325;&#20026;&#35270;&#35282;&#35780;&#20272;&#19982;&#35821;&#35328;&#20195;&#29702;&#30340;&#20132;&#20114;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#21152;&#20851;&#27880;&#20851;&#31995;&#21644;&#24773;&#22659;&#22240;&#32032;&#30340;&#20262;&#29702;&#26041;&#27861;&#65292;&#26088;&#22312;&#24110;&#21161;LLM&#25216;&#26415;&#34920;&#29616;&#24471;&#8220;&#22909;&#8221;</title><link>http://arxiv.org/abs/2401.09082</link><description>&lt;p&gt;
&#20160;&#20040;&#26159;&#8220;&#22909;&#8221;&#30340;&#31038;&#20132;&#34892;&#20026;&#32773;&#65311;&#20197;&#23562;&#37325;&#20026;&#35270;&#35282;&#35780;&#20272;&#19982;&#35821;&#35328;&#20195;&#29702;&#30340;&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;
What makes for a 'good' social actor? Using respect as a lens to evaluate interactions with language agents. (arXiv:2401.09082v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09082
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20197;&#23562;&#37325;&#20026;&#35270;&#35282;&#35780;&#20272;&#19982;&#35821;&#35328;&#20195;&#29702;&#30340;&#20132;&#20114;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#21152;&#20851;&#27880;&#20851;&#31995;&#21644;&#24773;&#22659;&#22240;&#32032;&#30340;&#20262;&#29702;&#26041;&#27861;&#65292;&#26088;&#22312;&#24110;&#21161;LLM&#25216;&#26415;&#34920;&#29616;&#24471;&#8220;&#22909;&#8221;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#23545;&#35805;&#20195;&#29702;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#22914;&#20309;&#30830;&#20445;&#23427;&#20204;&#30340;&#34892;&#20026;&#36947;&#24503;&#21644;&#36866;&#24403;&#24615;&#24050;&#32463;&#24341;&#36215;&#20102;&#32039;&#24613;&#20851;&#27880;&#12290;&#20174;&#8220;HHH&#8221;&#26631;&#20934;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#20027;&#35201;&#20307;&#29616;&#22312;&#35753;&#36755;&#20986;&#26356;&#26377;&#24110;&#21161;&#21644;&#35802;&#23454;&#65292;&#24182;&#36991;&#20813;&#26377;&#23475;&#65288;&#26377;&#20559;&#35265;&#12289;&#26377;&#27602;&#25110;&#19981;&#20934;&#30830;&#65289;&#30340;&#38472;&#36848;&#12290;&#34429;&#28982;&#36825;&#31181;&#35821;&#20041;&#28966;&#28857;&#23545;&#20110;&#23558;LLM&#20195;&#29702;&#35270;&#20026;&#32431;&#31929;&#30340;&#20449;&#24687;&#23186;&#20171;&#26159;&#26377;&#29992;&#30340;&#65292;&#20294;&#23427;&#26410;&#33021;&#32771;&#34385;&#21040;&#22312;&#19981;&#21516;&#31038;&#20132;&#24773;&#22659;&#20013;&#65292;&#21516;&#26679;&#30340;&#35805;&#35821;&#21487;&#33021;&#20250;&#26174;&#24471;&#26356;&#25110;&#32773;&#26356;&#23569;&#20882;&#29359;&#25110;&#19981;&#24471;&#20307;&#30340;&#23454;&#38469;&#22240;&#32032;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#21152;&#20851;&#27880;&#20851;&#31995;&#21644;&#24773;&#22659;&#22240;&#32032;&#30340;&#20262;&#29702;&#26041;&#27861;&#65292;&#25506;&#35752;&#20316;&#20026;&#31038;&#20132;&#34892;&#20026;&#32773;&#30340;&#31995;&#32479;&#22914;&#20309;&#22312;&#20132;&#20114;&#20013;&#20197;&#23562;&#37325;&#30340;&#26041;&#24335;&#23545;&#24453;&#20010;&#20307;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#39044;&#35265;&#20102;&#22312;&#24773;&#22659;&#20132;&#20114;&#23618;&#38754;&#19978;&#19968;&#31995;&#21015;&#23578;&#26410;&#34987;&#25506;&#32034;&#30340;&#39118;&#38505;&#65292;&#24182;&#25552;&#20379;&#20102;&#23454;&#29992;&#24314;&#35758;&#65292;&#20197;&#24110;&#21161;LLM&#25216;&#26415;&#34920;&#29616;&#24471;&#8220;&#22909;&#8221;
&lt;/p&gt;
&lt;p&gt;
With the growing popularity of dialogue agents based on large language models (LLMs), urgent attention has been drawn to finding ways to ensure their behaviour is ethical and appropriate. These are largely interpreted in terms of the 'HHH' criteria: making outputs more helpful and honest, and avoiding harmful (biased, toxic, or inaccurate) statements. Whilst this semantic focus is useful from the perspective of viewing LLM agents as mere mediums for information, it fails to account for pragmatic factors that can make the same utterance seem more or less offensive or tactless in different social situations. We propose an approach to ethics that is more centred on relational and situational factors, exploring what it means for a system, as a social actor, to treat an individual respectfully in a (series of) interaction(s). Our work anticipates a set of largely unexplored risks at the level of situated interaction, and offers practical suggestions to help LLM technologies behave as 'good'
&lt;/p&gt;</description></item><item><title>NID-SLAM&#26159;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#38544;&#24335;&#34920;&#31034;&#30340;RGB-D SLAM&#31639;&#27861;&#65292;&#23427;&#36890;&#36807;&#25552;&#21319;&#35821;&#20041;&#25513;&#33180;&#20013;&#30340;&#19981;&#20934;&#30830;&#21306;&#22495;&#21644;&#24341;&#20837;&#20851;&#38190;&#24103;&#36873;&#25321;&#31574;&#30053;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#22312;&#21160;&#24577;&#29615;&#22659;&#19979;&#30340;&#24615;&#33021;&#65292;&#25552;&#39640;&#20102;&#36319;&#36394;&#31934;&#24230;&#21644;&#24314;&#22270;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2401.01189</link><description>&lt;p&gt;
NID-SLAM: &#22522;&#20110;&#31070;&#32463;&#38544;&#24335;&#34920;&#31034;&#30340;&#21160;&#24577;&#29615;&#22659;RGB-D SLAM
&lt;/p&gt;
&lt;p&gt;
NID-SLAM: Neural Implicit Representation-based RGB-D SLAM in dynamic environments. (arXiv:2401.01189v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01189
&lt;/p&gt;
&lt;p&gt;
NID-SLAM&#26159;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#38544;&#24335;&#34920;&#31034;&#30340;RGB-D SLAM&#31639;&#27861;&#65292;&#23427;&#36890;&#36807;&#25552;&#21319;&#35821;&#20041;&#25513;&#33180;&#20013;&#30340;&#19981;&#20934;&#30830;&#21306;&#22495;&#21644;&#24341;&#20837;&#20851;&#38190;&#24103;&#36873;&#25321;&#31574;&#30053;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#22312;&#21160;&#24577;&#29615;&#22659;&#19979;&#30340;&#24615;&#33021;&#65292;&#25552;&#39640;&#20102;&#36319;&#36394;&#31934;&#24230;&#21644;&#24314;&#22270;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#38544;&#24335;&#34920;&#31034;&#24050;&#34987;&#29992;&#20110;&#22686;&#24378;&#35270;&#35273;SLAM&#31639;&#27861;&#65292;&#29305;&#21035;&#26159;&#25552;&#20379;&#39640;&#20445;&#30495;&#23494;&#38598;&#22320;&#22270;&#26041;&#38754;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#22312;&#38745;&#24577;&#22330;&#26223;&#19979;&#36816;&#20316;&#33391;&#22909;&#65292;&#20294;&#22312;&#31227;&#21160;&#29289;&#20307;&#36896;&#25104;&#30340;&#24178;&#25200;&#19979;&#22256;&#38590;&#37325;&#37325;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;NID-SLAM&#65292;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#26174;&#33879;&#25913;&#21892;&#20102;&#31070;&#32463;SLAM&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#35821;&#20041;&#25513;&#33180;&#20013;&#30340;&#19981;&#20934;&#30830;&#21306;&#22495;&#65292;&#29305;&#21035;&#26159;&#36793;&#32536;&#21306;&#22495;&#12290;&#21033;&#29992;&#28145;&#24230;&#22270;&#20687;&#20013;&#30340;&#20960;&#20309;&#20449;&#24687;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#31934;&#30830;&#22320;&#21435;&#38500;&#21160;&#24577;&#29289;&#20307;&#65292;&#20174;&#32780;&#38477;&#20302;&#30456;&#26426;&#28418;&#31227;&#30340;&#27010;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#21160;&#24577;&#22330;&#26223;&#30340;&#20851;&#38190;&#24103;&#36873;&#25321;&#31574;&#30053;&#65292;&#22686;&#24378;&#20102;&#30456;&#26426;&#36319;&#36394;&#23545;&#22823;&#23610;&#24230;&#29289;&#20307;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#25552;&#39640;&#20102;&#24314;&#22270;&#30340;&#25928;&#29575;&#12290;&#22312;&#20844;&#24320;&#21487;&#29992;&#30340;RGB-D&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#36319;&#36394;&#31934;&#24230;&#21644;&#24314;&#22270;&#36136;&#37327;&#19978;&#20248;&#20110;&#31454;&#20105;&#24615;&#30340;&#31070;&#32463;SLAM&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural implicit representations have been explored to enhance visual SLAM algorithms, especially in providing high-fidelity dense map. Existing methods operate robustly in static scenes but struggle with the disruption caused by moving objects. In this paper we present NID-SLAM, which significantly improves the performance of neural SLAM in dynamic environments. We propose a new approach to enhance inaccurate regions in semantic masks, particularly in marginal areas. Utilizing the geometric information present in depth images, this method enables accurate removal of dynamic objects, thereby reducing the probability of camera drift. Additionally, we introduce a keyframe selection strategy for dynamic scenes, which enhances camera tracking robustness against large-scale objects and improves the efficiency of mapping. Experiments on publicly available RGB-D datasets demonstrate that our method outperforms competitive neural SLAM approaches in tracking accuracy and mapping quality in dynam
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#23398;&#20064;&#22686;&#24378;&#22411;&#29289;&#32852;&#32593;&#31995;&#32479;&#30340;&#27979;&#35797;&#36827;&#34892;&#20102;&#24418;&#24335;&#21270;&#26041;&#27861;&#30340;&#25506;&#32034;&#65292;&#20197;&#24212;&#23545;&#24403;&#21069;&#27979;&#35797;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.07377</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#23398;&#20064;&#22686;&#24378;&#22411;&#30340;&#29289;&#32852;&#32593;&#31995;&#32479;&#36827;&#34892;&#27979;&#35797;&#65306;&#19968;&#31181;&#24418;&#24335;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Testing learning-enabled cyber-physical systems with Large-Language Models: A Formal Approach. (arXiv:2311.07377v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.07377
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#23398;&#20064;&#22686;&#24378;&#22411;&#29289;&#32852;&#32593;&#31995;&#32479;&#30340;&#27979;&#35797;&#36827;&#34892;&#20102;&#24418;&#24335;&#21270;&#26041;&#27861;&#30340;&#25506;&#32034;&#65292;&#20197;&#24212;&#23545;&#24403;&#21069;&#27979;&#35797;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#25972;&#21512;&#21040;&#29289;&#32852;&#32593;&#31995;&#32479;&#65288;CPS&#65289;&#20013;&#65292;&#21487;&#20197;&#24102;&#26469;&#26174;&#33879;&#30340;&#22909;&#22788;&#65292;&#21253;&#25324;&#22686;&#24378;&#25928;&#29575;&#12289;&#39044;&#27979;&#33021;&#21147;&#12289;&#23454;&#26102;&#21709;&#24212;&#21644;&#23454;&#29616;&#33258;&#20027;&#36816;&#34892;&#12290;&#36825;&#31181;&#34701;&#21512;&#21152;&#36895;&#20102;&#19968;&#31995;&#21015;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#30340;&#24320;&#21457;&#21644;&#37096;&#32626;&#65292;&#20363;&#22914;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#12289;&#36865;&#36135;&#26080;&#20154;&#26426;&#12289;&#26381;&#21153;&#26426;&#22120;&#20154;&#21644;&#36828;&#31243;&#21307;&#30103;&#31243;&#24207;&#12290;&#28982;&#32780;&#65292;&#20154;&#24037;&#26234;&#33021;&#22686;&#24378;&#30340;CPS&#30340;&#36719;&#20214;&#24320;&#21457;&#29983;&#21629;&#21608;&#26399;&#65288;SDLC&#65289;&#19982;&#20256;&#32479;&#26041;&#27861;&#23384;&#22312;&#26126;&#26174;&#30340;&#24046;&#24322;&#65292;&#20855;&#26377;&#25968;&#25454;&#21644;&#23398;&#20064;&#20316;&#20026;&#20004;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#29616;&#26377;&#30340;&#39564;&#35777;&#21644;&#39564;&#35777;&#25216;&#26415;&#24120;&#24120;&#19981;&#36275;&#20197;&#24212;&#23545;&#36825;&#20123;&#26032;&#30340;&#33539;&#24335;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#30830;&#23450;&#30830;&#20445;&#23398;&#20064;&#22686;&#24378;&#22411;CPS&#24418;&#24335;&#21270;&#23433;&#20840;&#24615;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;&#25105;&#20204;&#39318;&#20808;&#32771;&#23519;&#20102;&#20316;&#20026;&#39564;&#35777;&#21644;&#39564;&#35777;&#26368;&#23454;&#29992;&#26041;&#27861;&#30340;&#27979;&#35797;&#65292;&#24635;&#32467;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#35770;&#12290;&#35748;&#35782;&#21040;&#24403;&#21069;&#27979;&#35797;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;
&lt;/p&gt;
&lt;p&gt;
The integration of machine learning (ML) into cyber-physical systems (CPS) offers significant benefits, including enhanced efficiency, predictive capabilities, real-time responsiveness, and the enabling of autonomous operations. This convergence has accelerated the development and deployment of a range of real-world applications, such as autonomous vehicles, delivery drones, service robots, and telemedicine procedures. However, the software development life cycle (SDLC) for AI-infused CPS diverges significantly from traditional approaches, featuring data and learning as two critical components. Existing verification and validation techniques are often inadequate for these new paradigms. In this study, we pinpoint the main challenges in ensuring formal safety for learningenabled CPS.We begin by examining testing as the most pragmatic method for verification and validation, summarizing the current state-of-the-art methodologies. Recognizing the limitations in current testing approaches t
&lt;/p&gt;</description></item><item><title>GOPlan&#26159;&#19968;&#20010;&#20351;&#29992;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#35745;&#21010;&#30340;&#30446;&#26631;&#26465;&#20214;&#19979;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#20808;&#39564;&#31574;&#30053;&#21644;&#20351;&#29992;&#37325;&#26032;&#20998;&#26512;&#26041;&#27861;&#29983;&#25104;&#34394;&#26500;&#36712;&#36857;&#65292;&#29992;&#20197;&#25552;&#39640;&#24615;&#33021;&#21644;&#22788;&#29702;&#26377;&#38480;&#25968;&#25454;&#39044;&#31639;&#21644;&#26410;&#35265;&#30446;&#26631;&#27867;&#21270;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.20025</link><description>&lt;p&gt;
GOPlan:&#36890;&#36807;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#35745;&#21010;&#30340;&#30446;&#26631;&#26465;&#20214;&#19979;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
GOPlan: Goal-conditioned Offline Reinforcement Learning by Planning with Learned Models. (arXiv:2310.20025v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20025
&lt;/p&gt;
&lt;p&gt;
GOPlan&#26159;&#19968;&#20010;&#20351;&#29992;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#35745;&#21010;&#30340;&#30446;&#26631;&#26465;&#20214;&#19979;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#20808;&#39564;&#31574;&#30053;&#21644;&#20351;&#29992;&#37325;&#26032;&#20998;&#26512;&#26041;&#27861;&#29983;&#25104;&#34394;&#26500;&#36712;&#36857;&#65292;&#29992;&#20197;&#25552;&#39640;&#24615;&#33021;&#21644;&#22788;&#29702;&#26377;&#38480;&#25968;&#25454;&#39044;&#31639;&#21644;&#26410;&#35265;&#30446;&#26631;&#27867;&#21270;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#30446;&#26631;&#26465;&#20214;&#19979;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;GCRL&#65289;&#20026;&#20174;&#22810;&#26679;&#21270;&#21644;&#22810;&#20219;&#21153;&#30340;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#36890;&#29992;&#31574;&#30053;&#25552;&#20379;&#20102;&#21487;&#34892;&#30340;&#33539;&#20363;&#12290;&#23613;&#31649;&#36817;&#26399;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#20027;&#23548;&#30340;&#31163;&#32447;GCRL&#26041;&#27861;&#20173;&#28982;&#21463;&#38480;&#20110;&#26080;&#27169;&#22411;&#26041;&#27861;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#24212;&#23545;&#26377;&#38480;&#25968;&#25454;&#39044;&#31639;&#21644;&#26410;&#35265;&#30446;&#26631;&#27867;&#21270;&#30340;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20004;&#38454;&#27573;&#27169;&#22411;&#20026;&#22522;&#30784;&#30340;&#26694;&#26550;&#65292;Goal-conditioned Offline Planning&#65288;GOPlan&#65289;&#65292;&#21253;&#25324;&#65288;1&#65289;&#39044;&#35757;&#32451;&#19968;&#20010;&#33021;&#22815;&#25429;&#25417;&#22810;&#30446;&#26631;&#25968;&#25454;&#38598;&#20013;&#22810;&#27169;&#24577;&#21160;&#20316;&#20998;&#24067;&#30340;&#20808;&#39564;&#31574;&#30053;&#65307;&#65288;2&#65289;&#21033;&#29992;&#35268;&#21010;&#30340;&#37325;&#26032;&#20998;&#26512;&#26041;&#27861;&#20026;&#24494;&#35843;&#31574;&#30053;&#29983;&#25104;&#34394;&#26500;&#36712;&#36857;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20808;&#39564;&#31574;&#30053;&#22522;&#20110;&#19968;&#20010;&#20855;&#26377;&#26126;&#26174;&#27169;&#24335;&#20998;&#31163;&#30340;&#24102;&#20248;&#21183;&#26435;&#37325;&#30340;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#20197;&#20811;&#26381;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#21160;&#20316;&#30340;&#32570;&#28857;&#12290;&#20026;&#36827;&#19968;&#27493;&#20248;&#21270;&#31574;&#30053;&#65292;&#37325;&#26032;&#20998;&#26512;&#26041;&#27861;&#36890;&#36807;&#35268;&#21010;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#34394;&#26500;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline goal-conditioned RL (GCRL) offers a feasible paradigm to learn general-purpose policies from diverse and multi-task offline datasets. Despite notable recent progress, the predominant offline GCRL methods have been restricted to model-free approaches, constraining their capacity to tackle limited data budgets and unseen goal generalization. In this work, we propose a novel two-stage model-based framework, Goal-conditioned Offline Planning (GOPlan), including (1) pretraining a prior policy capable of capturing multi-modal action distribution within the multi-goal dataset; (2) employing the reanalysis method with planning to generate imagined trajectories for funetuning policies. Specifically, the prior policy is based on an advantage-weighted Conditioned Generative Adversarial Networks that exhibits distinct mode separation to overcome the pitfalls of out-of-distribution (OOD) actions. For further policy optimization, the reanalysis method generates high-quality imaginary data by
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20174;&#33258;&#28982;&#20114;&#21160;&#20013;&#23454;&#29616;&#22797;&#26434;&#34892;&#20026;&#22686;&#37327;&#23398;&#20064;&#30340;&#31995;&#32479;&#65292;&#24182;&#28436;&#31034;&#20102;&#22312;&#19968;&#20010;&#20154;&#24418;&#26426;&#22120;&#20154;&#19978;&#30340;&#24212;&#29992;&#12290;&#35813;&#31995;&#32479;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#26426;&#22120;&#20154;&#30340;&#34892;&#20026;&#36827;&#34892;&#39640;&#32423;&#21327;&#35843;&#65292;&#36890;&#36807;&#20132;&#20114;&#24335;&#25511;&#21046;&#21488;&#29983;&#25104;Python&#35821;&#21477;&#26469;&#35843;&#29992;&#26426;&#22120;&#20154;&#30340;&#24863;&#30693;&#21644;&#21160;&#20316;&#65292;&#24182;&#36890;&#36807;&#23558;&#20154;&#31867;&#25351;&#20196;&#12289;&#29615;&#22659;&#35266;&#27979;&#21644;&#25191;&#34892;&#32467;&#26524;&#21453;&#39304;&#32473;&#35821;&#35328;&#27169;&#22411;&#26469;&#23454;&#29616;&#24490;&#29615;&#20132;&#20114;&#12290;</title><link>http://arxiv.org/abs/2309.04316</link><description>&lt;p&gt;
&#20174;&#33258;&#28982;&#20114;&#21160;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#22686;&#37327;&#23398;&#20064;&#20154;&#24418;&#26426;&#22120;&#20154;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Incremental Learning of Humanoid Robot Behavior from Natural Interaction and Large Language Models. (arXiv:2309.04316v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04316
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20174;&#33258;&#28982;&#20114;&#21160;&#20013;&#23454;&#29616;&#22797;&#26434;&#34892;&#20026;&#22686;&#37327;&#23398;&#20064;&#30340;&#31995;&#32479;&#65292;&#24182;&#28436;&#31034;&#20102;&#22312;&#19968;&#20010;&#20154;&#24418;&#26426;&#22120;&#20154;&#19978;&#30340;&#24212;&#29992;&#12290;&#35813;&#31995;&#32479;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#26426;&#22120;&#20154;&#30340;&#34892;&#20026;&#36827;&#34892;&#39640;&#32423;&#21327;&#35843;&#65292;&#36890;&#36807;&#20132;&#20114;&#24335;&#25511;&#21046;&#21488;&#29983;&#25104;Python&#35821;&#21477;&#26469;&#35843;&#29992;&#26426;&#22120;&#20154;&#30340;&#24863;&#30693;&#21644;&#21160;&#20316;&#65292;&#24182;&#36890;&#36807;&#23558;&#20154;&#31867;&#25351;&#20196;&#12289;&#29615;&#22659;&#35266;&#27979;&#21644;&#25191;&#34892;&#32467;&#26524;&#21453;&#39304;&#32473;&#35821;&#35328;&#27169;&#22411;&#26469;&#23454;&#29616;&#24490;&#29615;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#23545;&#35805;&#23545;&#20110;&#30452;&#35266;&#30340;&#20154;&#26426;&#20132;&#20114;&#33267;&#20851;&#37325;&#35201;&#12290;&#23427;&#19981;&#20165;&#21487;&#20197;&#29992;&#26469;&#34920;&#36798;&#20154;&#31867;&#30340;&#24847;&#22270;&#65292;&#32780;&#19988;&#36824;&#21487;&#20197;&#29992;&#26469;&#20256;&#36798;&#25351;&#20196;&#20197;&#25913;&#36827;&#26426;&#22120;&#20154;&#23545;&#21629;&#20196;&#30340;&#29702;&#35299;&#12290;&#38750;&#24120;&#37325;&#35201;&#30340;&#26159;&#35201;&#36171;&#20104;&#26426;&#22120;&#20154;&#20174;&#36825;&#31181;&#20132;&#20114;&#32463;&#39564;&#20013;&#22686;&#37327;&#23398;&#20064;&#30340;&#33021;&#21147;&#65292;&#20197;&#20351;&#23427;&#20204;&#33021;&#22815;&#25913;&#36827;&#33258;&#24049;&#30340;&#34892;&#20026;&#25110;&#36991;&#20813;&#26410;&#26469;&#30340;&#38169;&#35823;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20174;&#33258;&#28982;&#20114;&#21160;&#20013;&#23454;&#29616;&#22797;&#26434;&#34892;&#20026;&#22686;&#37327;&#23398;&#20064;&#30340;&#31995;&#32479;&#65292;&#24182;&#22312;&#19968;&#20010;&#20154;&#24418;&#26426;&#22120;&#20154;&#19978;&#28436;&#31034;&#20854;&#23454;&#29616;&#12290;&#22522;&#20110;&#26368;&#26032;&#30340;&#36827;&#23637;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;Large Language Models&#65292;LLMs&#65289;&#36827;&#34892;&#26426;&#22120;&#20154;&#34892;&#20026;&#30340;&#39640;&#23618;&#21327;&#35843;&#30340;&#31995;&#32479;&#65292;&#36825;&#20010;&#31995;&#32479;&#30340;&#24605;&#24819;&#26159;&#35753;LLM&#22312;&#20132;&#20114;&#24335;&#25511;&#21046;&#21488;&#20013;&#29983;&#25104;Python&#35821;&#21477;&#26469;&#35843;&#29992;&#26426;&#22120;&#20154;&#30340;&#24863;&#30693;&#21644;&#21160;&#20316;&#12290;&#36890;&#36807;&#23558;&#20154;&#31867;&#25351;&#20196;&#12289;&#29615;&#22659;&#35266;&#27979;&#21644;&#25191;&#34892;&#32467;&#26524;&#21453;&#39304;&#32473;LLM&#26469;&#20851;&#38381;&#20132;&#20114;&#24490;&#29615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural-language dialog is key for intuitive human-robot interaction. It can be used not only to express humans' intents, but also to communicate instructions for improvement if a robot does not understand a command correctly. Of great importance is to endow robots with the ability to learn from such interaction experience in an incremental way to allow them to improve their behaviors or avoid mistakes in the future. In this paper, we propose a system to achieve incremental learning of complex behavior from natural interaction, and demonstrate its implementation on a humanoid robot. Building on recent advances, we present a system that deploys Large Language Models (LLMs) for high-level orchestration of the robot's behavior, based on the idea of enabling the LLM to generate Python statements in an interactive console to invoke both robot perception and action. The interaction loop is closed by feeding back human instructions, environment observations, and execution results to the LLM, 
&lt;/p&gt;</description></item><item><title>MIMIC&#26159;&#19968;&#31181;&#22522;&#20110;&#22270;&#20687;&#23545;&#24212;&#20851;&#31995;&#30340;&#36974;&#34109;&#22270;&#20687;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#25366;&#25496;&#19981;&#38656;&#35201;&#20219;&#20309;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;&#22810;&#20010;&#33258;&#30417;&#30563;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#36798;&#21040;&#20102;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#19978;&#20248;&#20110;&#20351;&#29992;&#27880;&#37322;&#25366;&#25496;&#30340;&#34920;&#31034;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.15128</link><description>&lt;p&gt;
MIMIC: &#22522;&#20110;&#22270;&#20687;&#23545;&#24212;&#20851;&#31995;&#30340;&#36974;&#34109;&#22270;&#20687;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
MIMIC: Masked Image Modeling with Image Correspondences. (arXiv:2306.15128v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15128
&lt;/p&gt;
&lt;p&gt;
MIMIC&#26159;&#19968;&#31181;&#22522;&#20110;&#22270;&#20687;&#23545;&#24212;&#20851;&#31995;&#30340;&#36974;&#34109;&#22270;&#20687;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#25366;&#25496;&#19981;&#38656;&#35201;&#20219;&#20309;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;&#22810;&#20010;&#33258;&#30417;&#30563;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#36798;&#21040;&#20102;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#19978;&#20248;&#20110;&#20351;&#29992;&#27880;&#37322;&#25366;&#25496;&#30340;&#34920;&#31034;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#20687;&#32032;&#32423;&#30340;&#23494;&#38598;&#39044;&#27979;&#20219;&#21153;&#8212;&#8212;&#22914;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#28145;&#24230;&#20272;&#35745;&#21644;&#35821;&#20041;&#20998;&#21106;&#8212;&#8212;&#22914;&#20170;&#20381;&#36182;&#20110;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#34920;&#31034;&#12290;&#22240;&#27492;&#65292;&#31579;&#36873;&#26377;&#25928;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#33267;&#20851;&#37325;&#35201;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#26377;&#25928;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#20165;&#36890;&#36807;&#27169;&#25311;&#29615;&#22659;&#20013;&#30340;&#24102;&#26377;&#27880;&#37322;&#30340;3D&#32593;&#26684;&#12289;&#28857;&#20113;&#21644;&#30456;&#26426;&#21442;&#25968;&#31579;&#36873;&#32780;&#26469;&#65292;&#24182;&#19981;&#20855;&#22791;&#22810;&#35270;&#35282;&#22330;&#26223;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#20219;&#20309;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#31579;&#36873;&#26426;&#21046;&#12290;&#25105;&#20204;&#20174;&#24320;&#28304;&#35270;&#39057;&#25968;&#25454;&#38598;&#21644;&#21512;&#25104;&#30340;3D&#29615;&#22659;&#20013;&#25366;&#25496;&#20102;&#20004;&#20010;&#25968;&#25454;&#38598;&#65306;MIMIC-1M(&#21253;&#21547;1.3M&#20010;&#22810;&#35270;&#35282;&#22270;&#20687;&#23545;)&#21644;MIMIC-3M(&#21253;&#21547;3.1M&#20010;&#22810;&#35270;&#35282;&#22270;&#20687;&#23545;)&#12290;&#25105;&#20204;&#20351;&#29992;&#22810;&#20010;&#33258;&#30417;&#30563;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#37319;&#29992;&#19981;&#21516;&#30340;&#36974;&#34109;&#22270;&#20687;&#24314;&#27169;&#30446;&#26631;&#65292;&#23637;&#31034;&#20102;&#20197;&#19979;&#21457;&#29616;&#65306;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#65292;&#22522;&#20110;MIMIC-3M&#35757;&#32451;&#30340;&#34920;&#31034;&#20248;&#20110;&#20351;&#29992;&#27880;&#37322;&#25366;&#25496;&#30340;&#34920;&#31034;&#65292;&#21253;&#25324;&#28145;&#24230;&#20272;&#35745;&#12289;&#35821;&#20041;&#20998;&#21106;&#12289;&#34920;&#38754;&#27861;&#32447;&#21644;&#23039;&#24577;&#20272;&#35745;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many pixelwise dense prediction tasks-depth estimation and semantic segmentation in computer vision today rely on pretrained image representations. Therefore, curating effective pretraining datasets is vital. Unfortunately, the effective pretraining datasets are those with multi-view scenes and have only been curated using annotated 3D meshes, point clouds, and camera parameters from simulated environments. We propose a dataset-curation mechanism that does not require any annotations. We mine two datasets: MIMIC-1M with 1.3M and MIMIC-3M with 3.1M multi-view image pairs from open-sourced video datasets and from synthetic 3D environments. We train multiple self-supervised models with different masked image modeling objectives to showcase the following findings: Representations trained on MIMIC-3M outperform those mined using annotations on multiple downstream tasks, including depth estimation, semantic segmentation, surface normals, and pose estimation. They also outperform representati
&lt;/p&gt;</description></item><item><title>MagicBrush&#26159;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#25163;&#21160;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#25351;&#23548;&#30495;&#23454;&#22270;&#20687;&#30340;&#32534;&#36753;&#12290;&#23427;&#21253;&#25324;&#36229;&#36807;10K&#20010;&#25163;&#21160;&#26631;&#27880;&#30340;&#19977;&#20803;&#32452;&#65292;&#25903;&#25345;&#22823;&#35268;&#27169;&#30340;&#25991;&#26412;&#25351;&#23548;&#22270;&#20687;&#32534;&#36753;&#27169;&#22411;&#35757;&#32451;&#12290;&#22312;&#27492;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;InstructPix2Pix&#21487;&#20197;&#26681;&#25454;&#20154;&#31867;&#35780;&#20272;&#25552;&#20379;&#26356;&#22909;&#30340;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2306.10012</link><description>&lt;p&gt;
MagicBrush: &#20154;&#24037;&#26631;&#27880;&#30340;&#29992;&#20110;&#25351;&#23548;&#22270;&#20687;&#32534;&#36753;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
MagicBrush: A Manually Annotated Dataset for Instruction-Guided Image Editing. (arXiv:2306.10012v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10012
&lt;/p&gt;
&lt;p&gt;
MagicBrush&#26159;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#25163;&#21160;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#25351;&#23548;&#30495;&#23454;&#22270;&#20687;&#30340;&#32534;&#36753;&#12290;&#23427;&#21253;&#25324;&#36229;&#36807;10K&#20010;&#25163;&#21160;&#26631;&#27880;&#30340;&#19977;&#20803;&#32452;&#65292;&#25903;&#25345;&#22823;&#35268;&#27169;&#30340;&#25991;&#26412;&#25351;&#23548;&#22270;&#20687;&#32534;&#36753;&#27169;&#22411;&#35757;&#32451;&#12290;&#22312;&#27492;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;InstructPix2Pix&#21487;&#20197;&#26681;&#25454;&#20154;&#31867;&#35780;&#20272;&#25552;&#20379;&#26356;&#22909;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#25351;&#23548;&#30340;&#22270;&#20687;&#32534;&#36753;&#20174;&#20010;&#20154;&#20351;&#29992;&#21040;&#19987;&#19994;&#24212;&#29992;&#65288;&#22914;Photoshop&#65289;&#24191;&#27867;&#38656;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#35201;&#20040;&#26159;&#38646;&#26679;&#26412;&#65292;&#35201;&#20040;&#26159;&#22312;&#33258;&#21160;&#21512;&#25104;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20854;&#20013;&#21547;&#26377;&#22823;&#37327;&#30340;&#22122;&#22768;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#22312;&#23454;&#36341;&#20013;&#20173;&#38656;&#35201;&#22823;&#37327;&#30340;&#25163;&#21160;&#35843;&#25972;&#25165;&#33021;&#20135;&#29983;&#29702;&#24819;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MagicBrush&#65292;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#25163;&#21160;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#25351;&#23548;&#30495;&#23454;&#22270;&#20687;&#30340;&#32534;&#36753;&#65292;&#21253;&#25324;&#21333;&#20010;&#25805;&#20316;&#12289;&#22810;&#20010;&#25805;&#20316;&#12289;&#25552;&#20379;&#25513;&#30721;&#21644;&#19981;&#25552;&#20379;&#25513;&#30721;&#31561;&#19981;&#21516;&#22330;&#26223;&#12290;MagicBrush&#21253;&#25324;&#36229;&#36807;10K&#20010;&#25163;&#21160;&#26631;&#27880;&#30340;&#19977;&#20803;&#32452;&#65288;&#28304;&#22270;&#20687;&#65292;&#25351;&#20196;&#65292;&#30446;&#26631;&#22270;&#20687;&#65289;&#65292;&#25903;&#25345;&#22823;&#35268;&#27169;&#30340;&#25991;&#26412;&#25351;&#23548;&#22270;&#20687;&#32534;&#36753;&#27169;&#22411;&#35757;&#32451;&#12290;&#25105;&#20204;&#22312;MagicBrush&#19978;&#24494;&#35843;InstructPix2Pix&#65292;&#24182;&#23637;&#31034;&#20102;&#26032;&#27169;&#22411;&#21487;&#20197;&#26681;&#25454;&#20154;&#31867;&#35780;&#20272;&#25552;&#20379;&#26356;&#22909;&#30340;&#22270;&#20687;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#35780;&#20272;&#65292;&#20197;&#35780;&#20272;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#20351;&#29992;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-guided image editing is widely needed in daily life, ranging from personal use to professional applications such as Photoshop. However, existing methods are either zero-shot or trained on an automatically synthesized dataset, which contains a high volume of noise. Thus, they still require lots of manual tuning to produce desirable outcomes in practice. To address this issue, we introduce MagicBrush (https://osu-nlp-group.github.io/MagicBrush/), the first large-scale, manually annotated dataset for instruction-guided real image editing that covers diverse scenarios: single-turn, multi-turn, mask-provided, and mask-free editing. MagicBrush comprises over 10K manually annotated triples (source image, instruction, target image), which supports trainining large-scale text-guided image editing models. We fine-tune InstructPix2Pix on MagicBrush and show that the new model can produce much better images according to human evaluation. We further conduct extensive experiments to evaluate cu
&lt;/p&gt;</description></item></channel></rss>