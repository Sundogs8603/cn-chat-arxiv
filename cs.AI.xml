<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>AWEQ&#26159;&#19968;&#31181;&#21518;&#35757;&#32451;&#37327;&#21270;&#21644;&#28608;&#27963;&#26435;&#37325;&#22343;&#34913;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#36229;&#20302;&#20301;&#37327;&#21270;&#21644;8-bit&#26435;&#37325;&#21644;&#28608;&#27963;&#37327;&#21270;&#65292;&#24182;&#36890;&#36807;&#25913;&#36827;&#30340;&#22343;&#34913;&#26041;&#27861;&#20943;&#23567;&#37327;&#21270;&#20559;&#24046;&#35823;&#24046;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.01305</link><description>&lt;p&gt;
AWEQ&#65306;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21518;&#35757;&#32451;&#37327;&#21270;&#21644;&#28608;&#27963;&#26435;&#37325;&#22343;&#34913;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
AWEQ: Post-Training Quantization with Activation-Weight Equalization for Large Language Models. (arXiv:2311.01305v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01305
&lt;/p&gt;
&lt;p&gt;
AWEQ&#26159;&#19968;&#31181;&#21518;&#35757;&#32451;&#37327;&#21270;&#21644;&#28608;&#27963;&#26435;&#37325;&#22343;&#34913;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#36229;&#20302;&#20301;&#37327;&#21270;&#21644;8-bit&#26435;&#37325;&#21644;&#28608;&#27963;&#37327;&#21270;&#65292;&#24182;&#36890;&#36807;&#25913;&#36827;&#30340;&#22343;&#34913;&#26041;&#27861;&#20943;&#23567;&#37327;&#21270;&#20559;&#24046;&#35823;&#24046;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20854;&#35745;&#31639;&#21644;&#23384;&#20648;&#25104;&#26412;&#20063;&#30456;&#23545;&#36739;&#39640;&#12290;&#37327;&#21270;&#36825;&#20123;&#27169;&#22411;&#26159;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#24456;&#38590;&#22312;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#30828;&#20214;&#25928;&#29575;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;AWEQ&#65292;&#19968;&#31181;&#21518;&#35757;&#32451;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#24320;&#38144;&#12290;AWEQ&#22312;&#36229;&#20302;&#20301;&#37327;&#21270;&#21644;8-bit&#26435;&#37325;&#21644;&#28608;&#27963;(W8A8)&#37327;&#21270;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#35266;&#23519;&#21040;&#26435;&#37325;&#37327;&#21270;&#27604;&#28608;&#27963;&#37327;&#21270;&#26356;&#23481;&#26131;&#12290;AWEQ&#36890;&#36807;&#36890;&#36947;&#22343;&#34913;&#23558;&#28608;&#27963;&#37327;&#21270;&#30340;&#38590;&#24230;&#36716;&#31227;&#21040;&#26435;&#37325;&#19978;&#65292;&#23454;&#29616;&#20102;&#20004;&#32773;&#37327;&#21270;&#22256;&#38590;&#30340;&#24179;&#34913;&#65292;&#20174;&#32780;&#26368;&#22823;&#21270;&#20102;&#24615;&#33021;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25913;&#36827;&#20102;&#22343;&#34913;&#26041;&#27861;&#65292;&#20943;&#23567;&#20102;&#37327;&#21270;&#20559;&#24046;&#35823;&#24046;&#65292;&#30830;&#20445;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;&#20687;LLaMA&#36825;&#26679;&#30340;&#27969;&#34892;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models(LLMs) exhibit excellent performance across a variety of tasks, but they come with significant computational and storage costs. Quantizing these models is an effective way to alleviate this issue. However, existing methods struggle to strike a balance between model accuracy and hardware efficiency. This is where we introduce AWEQ, a post-training method that requires no additional training overhead. AWEQ excels in both ultra-low-bit quantization and 8-bit weight and activation (W8A8) quantization. There is an observation that weight quantization is less challenging than activation quantization. AWEQ transfers the difficulty of activation quantization to weights using channel equalization, achieving a balance between the quantization difficulties of both, and thereby maximizing performance. We have further refined the equalization method to mitigate quantization bias error, ensuring the robustness of the model. Extensive experiments on popular models such as LLaMA a
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#32654;&#22269;&#32456;&#32467;HIV&#27969;&#34892;&#35745;&#21010;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#36827;&#34892;&#29305;&#23450;&#22320;&#21306;&#30340;&#20915;&#31574;&#20998;&#26512;&#65292;&#24182;&#32771;&#34385;&#21040;&#22320;&#21306;&#20043;&#38388;&#30340;&#27969;&#34892;&#30149;&#23398;&#30456;&#20114;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2311.00855</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#32654;&#22269;&#32456;&#32467;HIV&#27969;&#34892;&#35745;&#21010;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Multi-Agent Reinforcement Learning Framework for Evaluating the U.S. Ending the HIV Epidemic Plan. (arXiv:2311.00855v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00855
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#32654;&#22269;&#32456;&#32467;HIV&#27969;&#34892;&#35745;&#21010;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#36827;&#34892;&#29305;&#23450;&#22320;&#21306;&#30340;&#20915;&#31574;&#20998;&#26512;&#65292;&#24182;&#32771;&#34385;&#21040;&#22320;&#21306;&#20043;&#38388;&#30340;&#27969;&#34892;&#30149;&#23398;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#20813;&#30123;&#32570;&#38519;&#30149;&#27602;&#65288;HIV&#65289;&#26159;&#32654;&#22269;&#30340;&#20027;&#35201;&#20844;&#20849;&#21355;&#29983;&#38382;&#39064;&#65292;&#27599;&#24180;&#26377;&#32422;1.2&#19975;&#20154;&#24863;&#26579;HIV&#65292;&#20854;&#20013;&#26377;3.5&#19975;&#20154;&#26159;&#26032;&#24863;&#26579;&#32773;&#12290;&#32654;&#22269;&#30340;HIV&#36127;&#25285;&#21644;&#25252;&#29702;&#25509;&#35302;&#23384;&#22312;&#30528;&#22320;&#29702;&#24046;&#24322;&#12290;2019&#24180;&#30340;&#32456;&#32467;HIV&#27969;&#34892;&#35745;&#21010;&#26088;&#22312;&#21040;2030&#24180;&#23558;&#26032;&#24863;&#26579;&#20154;&#25968;&#20943;&#23569;90%&#65292;&#36890;&#36807;&#25552;&#39640;&#35786;&#26029;&#12289;&#27835;&#30103;&#21644;&#39044;&#38450;&#24178;&#39044;&#25514;&#26045;&#30340;&#35206;&#30422;&#29575;&#65292;&#24182;&#20248;&#20808;&#32771;&#34385;HIV&#39640;&#27969;&#34892;&#22320;&#21306;&#12290;&#30830;&#23450;&#26368;&#20339;&#24178;&#39044;&#25514;&#26045;&#30340;&#35268;&#27169;&#25193;&#22823;&#23558;&#26377;&#21161;&#20110;&#36164;&#28304;&#20998;&#37197;&#30340;&#20915;&#31574;&#12290;&#29616;&#26377;&#30340;HIV&#20915;&#31574;&#27169;&#22411;&#35201;&#20040;&#21482;&#35780;&#20272;&#29305;&#23450;&#22478;&#24066;&#65292;&#35201;&#20040;&#35780;&#20272;&#25972;&#20010;&#22269;&#23478;&#20154;&#21475;&#65292;&#24573;&#35270;&#22320;&#26041;&#30340;&#30456;&#20114;&#20316;&#29992;&#25110;&#24046;&#24322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#36827;&#34892;&#29305;&#23450;&#22320;&#21306;&#30340;&#20915;&#31574;&#20998;&#26512;&#65292;&#21516;&#26102;&#32771;&#34385;&#36328;&#22320;&#21306;&#30340;&#27969;&#34892;&#30149;&#20114;&#21160;&#12290;&#22312;&#23454;&#39564;&#20998;&#26512;&#20013;&#65292;
&lt;/p&gt;
&lt;p&gt;
Human immunodeficiency virus (HIV) is a major public health concern in the United States, with about 1.2 million people living with HIV and 35,000 newly infected each year. There are considerable geographical disparities in HIV burden and care access across the U.S. The 2019 Ending the HIV Epidemic (EHE) initiative aims to reduce new infections by 90% by 2030, by improving coverage of diagnoses, treatment, and prevention interventions and prioritizing jurisdictions with high HIV prevalence. Identifying optimal scale-up of intervention combinations will help inform resource allocation. Existing HIV decision analytic models either evaluate specific cities or the overall national population, thus overlooking jurisdictional interactions or differences. In this paper, we propose a multi-agent reinforcement learning (MARL) model, that enables jurisdiction-specific decision analyses but in an environment with cross-jurisdictional epidemiological interactions. In experimental analyses, conduct
&lt;/p&gt;</description></item><item><title>ZEETAD&#26159;&#19968;&#20010;&#38646;&#26679;&#26412;&#31471;&#21040;&#31471;&#26102;&#38388;&#21160;&#20316;&#26816;&#27979;&#27169;&#22411;&#65292;&#20854;&#20013;&#21253;&#25324;&#21452;&#23450;&#20301;&#21644;&#38646;&#26679;&#26412;&#25552;&#35758;&#20998;&#31867;&#20004;&#20010;&#27169;&#22359;&#12290;&#21069;&#32773;&#26159;&#22522;&#20110;Transformer&#30340;&#27169;&#22359;&#65292;&#29992;&#20110;&#26816;&#27979;&#21160;&#20316;&#20107;&#20214;&#24182;&#25910;&#38598;&#20851;&#38190;&#30340;&#35821;&#20041;&#23884;&#20837;&#65292;&#21518;&#32773;&#26159;&#22522;&#20110;CLIP&#30340;&#27169;&#22359;&#65292;&#29992;&#20110;&#29983;&#25104;&#25991;&#26412;&#21644;&#24103;&#36755;&#20837;&#30340;&#35821;&#20041;&#23884;&#20837;&#12290;</title><link>http://arxiv.org/abs/2311.00729</link><description>&lt;p&gt;
ZEETAD: &#20026;&#38646;&#26679;&#26412;&#31471;&#21040;&#31471;&#26102;&#38388;&#21160;&#20316;&#26816;&#27979;&#25913;&#36827;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ZEETAD: Adapting Pretrained Vision-Language Model for Zero-Shot End-to-End Temporal Action Detection. (arXiv:2311.00729v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00729
&lt;/p&gt;
&lt;p&gt;
ZEETAD&#26159;&#19968;&#20010;&#38646;&#26679;&#26412;&#31471;&#21040;&#31471;&#26102;&#38388;&#21160;&#20316;&#26816;&#27979;&#27169;&#22411;&#65292;&#20854;&#20013;&#21253;&#25324;&#21452;&#23450;&#20301;&#21644;&#38646;&#26679;&#26412;&#25552;&#35758;&#20998;&#31867;&#20004;&#20010;&#27169;&#22359;&#12290;&#21069;&#32773;&#26159;&#22522;&#20110;Transformer&#30340;&#27169;&#22359;&#65292;&#29992;&#20110;&#26816;&#27979;&#21160;&#20316;&#20107;&#20214;&#24182;&#25910;&#38598;&#20851;&#38190;&#30340;&#35821;&#20041;&#23884;&#20837;&#65292;&#21518;&#32773;&#26159;&#22522;&#20110;CLIP&#30340;&#27169;&#22359;&#65292;&#29992;&#20110;&#29983;&#25104;&#25991;&#26412;&#21644;&#24103;&#36755;&#20837;&#30340;&#35821;&#20041;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#21160;&#20316;&#26816;&#27979;&#65288;TAD&#65289;&#28041;&#21450;&#22312;&#38750;&#21098;&#36753;&#35270;&#39057;&#20013;&#23450;&#20301;&#21644;&#20998;&#31867;&#21160;&#20316;&#23454;&#20363;&#12290;&#23613;&#31649;&#20256;&#32479;TAD&#22312;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#19978;&#37319;&#29992;&#23436;&#20840;&#30417;&#30563;&#23398;&#20064;&#21644;&#23553;&#38381;&#38598;&#35774;&#32622;&#65292;&#20294;&#26368;&#36817;&#30340;&#38646;&#26679;&#26412;TAD&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#22823;&#35268;&#27169;&#23545;&#27604;&#30340;&#35270;&#35273;-&#35821;&#35328;&#65288;ViL&#65289;&#39044;&#35757;&#32451;&#27169;&#22411;&#23637;&#31034;&#20102;&#24320;&#25918;&#38598;&#35774;&#32622;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#38646;&#26679;&#26412;TAD&#26041;&#27861;&#22312;&#22914;&#20309;&#36866;&#24403;&#26500;&#24314;&#23450;&#20301;&#21644;&#20998;&#31867;&#36825;&#20004;&#20010;&#30456;&#20114;&#20381;&#36182;&#30340;&#20219;&#21153;&#20043;&#38388;&#30340;&#24378;&#20851;&#31995;&#20197;&#21450;&#23558;ViL&#27169;&#22411;&#36866;&#24212;&#20110;&#35270;&#39057;&#29702;&#35299;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ZEETAD&#65292;&#21253;&#21547;&#20004;&#20010;&#27169;&#22359;&#65306;&#21452;&#23450;&#20301;&#21644;&#38646;&#26679;&#26412;&#25552;&#35758;&#20998;&#31867;&#12290;&#21069;&#32773;&#26159;&#22522;&#20110;Transformer&#30340;&#27169;&#22359;&#65292;&#29992;&#20110;&#26816;&#27979;&#21160;&#20316;&#20107;&#20214;&#65292;&#24182;&#36873;&#25321;&#24615;&#22320;&#25910;&#38598;&#20851;&#38190;&#30340;&#35821;&#20041;&#23884;&#20837;&#20197;&#20379;&#21518;&#32493;&#35782;&#21035;&#12290;&#21518;&#32773;&#26159;&#22522;&#20110;CLIP&#30340;&#27169;&#22359;&#65292;&#29992;&#20110;&#20026;&#27599;&#20010;&#26102;&#38388;&#21333;&#20301;&#20174;&#25991;&#26412;&#21644;&#24103;&#36755;&#20837;&#29983;&#25104;&#35821;&#20041;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal action detection (TAD) involves the localization and classification of action instances within untrimmed videos. While standard TAD follows fully supervised learning with closed-set setting on large training data, recent zero-shot TAD methods showcase the promising of open-set setting by leveraging large-scale contrastive visual-language (ViL) pretrained models. However, existing zero-shot TAD methods have limitations on how to properly construct the strong relationships between two interdependent tasks of localization and classification and adapt ViL model to video understanding. In this work, we present ZEETAD, featuring two modules: dual-localization and zero-shot proposal classification. The former is a Transformer-based module that detects action events while selectively collecting crucial semantic embeddings for later recognition. The latter one, CLIP-based module, generates semantic embeddings from text and frame inputs for each temporal unit. Additionally, we enhance d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#32858;&#31867;&#30340;&#35282;&#24230;&#35299;&#37322;&#20102;&#22522;&#20110;&#29109;&#30340;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#65288;EBTTA&#65289;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36845;&#20195;&#31639;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#23545;&#20110;EBTTA&#26041;&#27861;&#26469;&#35828;&#65292;&#29109;&#25439;&#22833;&#20250;&#36827;&#19968;&#27493;&#22686;&#21152;&#26368;&#22823;&#30340;&#27010;&#29575;&#65292;&#20174;&#32780;&#20026;&#20854;&#22312;&#32858;&#31867;&#20219;&#21153;&#19978;&#30340;&#36739;&#22909;&#24615;&#33021;&#25552;&#20379;&#20102;&#19968;&#20010;&#26367;&#20195;&#24615;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2310.20327</link><description>&lt;p&gt;
&#20174;&#32858;&#31867;&#35270;&#35282;&#25913;&#36827;&#22522;&#20110;&#29109;&#30340;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Improving Entropy-Based Test-Time Adaptation from a Clustering View. (arXiv:2310.20327v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20327
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#32858;&#31867;&#30340;&#35282;&#24230;&#35299;&#37322;&#20102;&#22522;&#20110;&#29109;&#30340;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#65288;EBTTA&#65289;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36845;&#20195;&#31639;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#23545;&#20110;EBTTA&#26041;&#27861;&#26469;&#35828;&#65292;&#29109;&#25439;&#22833;&#20250;&#36827;&#19968;&#27493;&#22686;&#21152;&#26368;&#22823;&#30340;&#27010;&#29575;&#65292;&#20174;&#32780;&#20026;&#20854;&#22312;&#32858;&#31867;&#20219;&#21153;&#19978;&#30340;&#36739;&#22909;&#24615;&#33021;&#25552;&#20379;&#20102;&#19968;&#20010;&#26367;&#20195;&#24615;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#39046;&#22495;&#20559;&#31227;&#26159;&#19968;&#20010;&#24120;&#35265;&#30340;&#38382;&#39064;&#65292;&#35757;&#32451;&#25968;&#25454;&#21644;&#27979;&#35797;&#25968;&#25454;&#36981;&#24490;&#19981;&#21516;&#30340;&#25968;&#25454;&#20998;&#24067;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#23436;&#20840;&#30340;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#65288;TTA&#65289;&#21033;&#29992;&#27979;&#35797;&#26102;&#38388;&#36935;&#21040;&#30340;&#26080;&#26631;&#31614;&#25968;&#25454;&#26469;&#36866;&#24212;&#27169;&#22411;&#12290;&#29305;&#21035;&#26159;&#22522;&#20110;&#29109;&#30340;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#65288;EBTTA&#65289;&#26041;&#27861;&#65292;&#22312;&#27979;&#35797;&#26679;&#26412;&#19978;&#26368;&#23567;&#21270;&#39044;&#27979;&#30340;&#29109;&#65292;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#25104;&#21151;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#32858;&#31867;&#30340;&#35282;&#24230;&#20171;&#32461;&#20102;EBTTA&#30340;&#26032;&#35270;&#35282;&#21644;&#35299;&#37322;&#12290;&#36825;&#26159;&#19968;&#20010;&#36845;&#20195;&#31639;&#27861;&#65306;1&#65289;&#22312;&#20998;&#37197;&#27493;&#39588;&#20013;&#65292;EBTTA&#27169;&#22411;&#30340;&#21069;&#21521;&#36807;&#31243;&#26159;&#20026;&#36825;&#20123;&#27979;&#35797;&#26679;&#26412;&#20998;&#37197;&#26631;&#31614;&#65307;2&#65289;&#22312;&#26356;&#26032;&#27493;&#39588;&#20013;&#65292;&#21453;&#21521;&#36807;&#31243;&#26159;&#36890;&#36807;&#24050;&#20998;&#37197;&#30340;&#26679;&#26412;&#26469;&#26356;&#26032;&#27169;&#22411;&#12290;&#26681;&#25454;&#36825;&#31181;&#35299;&#37322;&#65292;&#25105;&#20204;&#21487;&#20197;&#26356;&#28145;&#20837;&#22320;&#29702;&#35299;EBTTA&#65292;&#20854;&#20013;&#25105;&#20204;&#23637;&#31034;&#20102;&#29109;&#25439;&#22833;&#20250;&#36827;&#19968;&#27493;&#22686;&#21152;&#26368;&#22823;&#30340;&#27010;&#29575;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26367;&#20195;&#24615;&#35299;&#37322;&#65292;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#29616;&#26377;&#30340;EBTTA&#26041;&#27861;&#22312;&#32858;&#31867;&#20219;&#21153;&#19978;&#27604;&#22312;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain shift is a common problem in the realistic world, where training data and test data follow different data distributions. To deal with this problem, fully test-time adaptation (TTA) leverages the unlabeled data encountered during test time to adapt the model. In particular, Entropy-Based TTA (EBTTA) methods, which minimize the prediction's entropy on test samples, have shown great success. In this paper, we introduce a new perspective on the EBTTA, which interprets these methods from a view of clustering. It is an iterative algorithm: 1) in the assignment step, the forward process of the EBTTA models is the assignment of labels for these test samples, and 2) in the updating step, the backward process is the update of the model via the assigned samples. Based on the interpretation, we can gain a deeper understanding of EBTTA, where we show that the entropy loss would further increase the largest probability. Accordingly, we offer an alternative explanation that why existing EBTTA 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#25506;&#32034;&#24182;&#35757;&#32451;&#20102;&#24378;&#22823;&#30340;&#22810;&#35821;&#35328;&#25968;&#23398;&#25512;&#29702;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#32763;&#35793;&#26500;&#24314;&#20102;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#21508;&#31181;&#35757;&#32451;&#31574;&#30053;&#26469;&#26500;&#24314;&#24378;&#22823;&#30340;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#23454;&#21457;&#29616;&#22312;&#22810;&#35821;&#35328;&#35757;&#32451;&#20013;&#65292;&#23558;&#30446;&#26631;&#35821;&#35328;&#30340;&#32763;&#35793;&#19982;&#21407;&#22987;&#35821;&#35328;&#30340;&#34920;&#31034;&#32467;&#21512;&#36215;&#26469;&#20197;&#21450;&#20132;&#26367;&#35757;&#32451;&#21644;&#22810;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#20030;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#27169;&#22411;&#22312;&#22788;&#29702;&#20302;&#39057;&#35789;&#21644;&#38271;&#21477;&#23376;&#26041;&#38754;&#20173;&#38754;&#20020;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.20246</link><description>&lt;p&gt;
&#22312;&#22810;&#35821;&#35328;&#25968;&#23398;&#25512;&#29702;&#20013;&#25171;&#30772;&#35821;&#35328;&#38556;&#30861;&#65306;&#35265;&#35299;&#19982;&#35266;&#23519;
&lt;/p&gt;
&lt;p&gt;
Breaking Language Barriers in Multilingual Mathematical Reasoning: Insights and Observations. (arXiv:2310.20246v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20246
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#25506;&#32034;&#24182;&#35757;&#32451;&#20102;&#24378;&#22823;&#30340;&#22810;&#35821;&#35328;&#25968;&#23398;&#25512;&#29702;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#32763;&#35793;&#26500;&#24314;&#20102;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#21508;&#31181;&#35757;&#32451;&#31574;&#30053;&#26469;&#26500;&#24314;&#24378;&#22823;&#30340;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#23454;&#21457;&#29616;&#22312;&#22810;&#35821;&#35328;&#35757;&#32451;&#20013;&#65292;&#23558;&#30446;&#26631;&#35821;&#35328;&#30340;&#32763;&#35793;&#19982;&#21407;&#22987;&#35821;&#35328;&#30340;&#34920;&#31034;&#32467;&#21512;&#36215;&#26469;&#20197;&#21450;&#20132;&#26367;&#35757;&#32451;&#21644;&#22810;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#20030;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#27169;&#22411;&#22312;&#22788;&#29702;&#20302;&#39057;&#35789;&#21644;&#38271;&#21477;&#23376;&#26041;&#38754;&#20173;&#38754;&#20020;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#24320;&#21457;&#36866;&#29992;&#20110;&#21333;&#35821;&#35328;&#20013;&#30340;&#25968;&#23398;&#25512;&#29702;&#30340;&#24378;&#22823;&#35821;&#35328;&#23398;&#20064;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#19979;&#20445;&#25345;&#25928;&#26524;&#30340;&#30740;&#31350;&#24456;&#23569;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#26412;&#25991;&#39318;&#27425;&#25506;&#32034;&#21644;&#35757;&#32451;&#24378;&#22823;&#30340;&#22810;&#35821;&#35328;&#25968;&#23398;&#25512;&#29702;&#65288;xMR&#65289;LLM&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#21033;&#29992;&#32763;&#35793;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#31532;&#19968;&#20010;&#21253;&#21547;&#21313;&#31181;&#19981;&#21516;&#35821;&#35328;&#30340;&#22810;&#35821;&#35328;&#25968;&#23398;&#25512;&#29702;&#25351;&#23548;&#25968;&#25454;&#38598;MGSM8KInstruct&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;xMR&#20219;&#21153;&#20013;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#30340;&#38382;&#39064;&#12290;&#26681;&#25454;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#35757;&#32451;&#31574;&#30053;&#26469;&#26500;&#24314;&#24378;&#22823;&#30340;xMR LLMs&#65292;&#34987;&#21629;&#21517;&#20026;MathOctopus&#65292;&#22312;&#20960;&#27425;&#35757;&#32451;&#20013;&#34920;&#29616;&#20986;&#20248;&#20110;&#20256;&#32479;&#24320;&#28304;LLMs&#21644;ChatGPT&#30340;&#33021;&#21147;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;MathOctopus-13B&#22312;MGSM&#27979;&#35797;&#38598;&#19978;&#36798;&#21040;&#20102;47.6%&#30340;&#20934;&#30830;&#29575;&#65292;&#36229;&#36807;&#20102;ChatGPT&#30340;46.3%&#12290;&#38500;&#20102;&#26174;&#33879;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#36824;&#20174;&#22823;&#37327;&#30340;&#23454;&#39564;&#35777;&#23454;&#20013;&#21457;&#29616;&#20102;&#19968;&#20123;&#37325;&#35201;&#30340;&#35266;&#23519;&#21644;&#35265;&#35299;&#65306;&#65288;1&#65289;&#22312;&#22810;&#35821;&#35328;&#19978;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#26368;&#22909;&#23558;&#30446;&#26631;&#35821;&#35328;&#30340;&#32763;&#35793;&#19982;&#21407;&#22987;&#35821;&#35328;&#30340;&#34920;&#31034;&#32467;&#21512;&#36215;&#26469;&#12290; &#65288;2&#65289;&#20132;&#26367;&#35757;&#32451;&#21644;&#22810;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#20030;&#26377;&#21161;&#20110;&#25552;&#39640;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290; &#65288;3&#65289;&#27169;&#22411;&#23545;&#20110;&#20302;&#39057;&#35789;&#21644;&#38271;&#21477;&#23376;&#30340;&#22788;&#29702;&#26159;&#25361;&#25112;&#30340;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing research predominantly focuses on developing powerful language learning models (LLMs) for mathematical reasoning within monolingual languages, with few explorations in preserving efficacy in a multilingual context. To bridge this gap, this paper pioneers exploring and training powerful Multilingual Math Reasoning (xMR) LLMs. Firstly, by utilizing translation, we construct the first multilingual math reasoning instruction dataset, MGSM8KInstruct, encompassing ten distinct languages, thus addressing the issue of training data scarcity in xMR tasks. Based on the collected dataset, we propose different training strategies to build powerful xMR LLMs, named MathOctopus, notably outperform conventional open-source LLMs and exhibit superiority over ChatGPT in few-shot scenarios. Notably, MathOctopus-13B reaches 47.6% accuracy which exceeds ChatGPT 46.3% on MGSM testset. Beyond remarkable results, we unearth several pivotal observations and insights from extensive experiments: (1) When
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;Safety-Gymnasium&#30340;&#29615;&#22659;&#22871;&#20214;&#65292;&#20854;&#20013;&#21253;&#25324;&#21333;&#20010;&#21644;&#22810;&#20010;Agent&#22330;&#26223;&#20013;&#30340;&#23433;&#20840;&#20851;&#38190;&#20219;&#21153;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#21253;&#21547;16&#31181;&#26368;&#20808;&#36827;&#30340;SafeRL&#31639;&#27861;&#30340;&#31639;&#27861;&#24211;&#12290;&#36825;&#20010;&#22522;&#20934;&#26088;&#22312;&#20419;&#36827;&#23545;&#23433;&#20840;&#24615;&#33021;&#30340;&#35780;&#20272;&#21644;&#27604;&#36739;&#65292;&#25512;&#21160;&#24378;&#21270;&#23398;&#20064;&#22312;&#23433;&#20840;&#24615;&#33021;&#19978;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2310.12567</link><description>&lt;p&gt;
Safety-Gymnasion&#65306;&#19968;&#20010;&#32479;&#19968;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Safety-Gymnasium: A Unified Safe Reinforcement Learning Benchmark. (arXiv:2310.12567v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12567
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;Safety-Gymnasium&#30340;&#29615;&#22659;&#22871;&#20214;&#65292;&#20854;&#20013;&#21253;&#25324;&#21333;&#20010;&#21644;&#22810;&#20010;Agent&#22330;&#26223;&#20013;&#30340;&#23433;&#20840;&#20851;&#38190;&#20219;&#21153;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#21253;&#21547;16&#31181;&#26368;&#20808;&#36827;&#30340;SafeRL&#31639;&#27861;&#30340;&#31639;&#27861;&#24211;&#12290;&#36825;&#20010;&#22522;&#20934;&#26088;&#22312;&#20419;&#36827;&#23545;&#23433;&#20840;&#24615;&#33021;&#30340;&#35780;&#20272;&#21644;&#27604;&#36739;&#65292;&#25512;&#21160;&#24378;&#21270;&#23398;&#20064;&#22312;&#23433;&#20840;&#24615;&#33021;&#19978;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#25317;&#26377;&#25512;&#21160;&#31038;&#20250;&#36827;&#27493;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#37096;&#32626;&#32463;&#24120;&#38754;&#20020;&#20005;&#37325;&#30340;&#23433;&#20840;&#38382;&#39064;&#12290;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;(SafeRL)&#20316;&#20026;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#20986;&#29616;&#65292;&#21487;&#20197;&#22312;&#21516;&#26102;&#36981;&#23432;&#22810;&#20010;&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#20248;&#21270;&#31574;&#30053;&#65292;&#20174;&#32780;&#35299;&#20915;&#38598;&#25104;&#24378;&#21270;&#23398;&#20064;&#22312;&#23433;&#20840;&#20851;&#38190;&#22330;&#26223;&#20013;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;Safety-Gymnasium&#30340;&#29615;&#22659;&#22871;&#20214;&#65292;&#21253;&#25324;&#21333;&#20010;&#21644;&#22810;&#20010;Agent&#22330;&#26223;&#20013;&#30340;&#23433;&#20840;&#20851;&#38190;&#20219;&#21153;&#65292;&#24182;&#25509;&#21463;&#21521;&#37327;&#21644;&#20165;&#35270;&#35273;&#36755;&#20837;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#21517;&#20026;Safe Policy Optimization&#65288;SafePO&#65289;&#30340;&#31639;&#27861;&#24211;&#65292;&#21253;&#21547;16&#31181;&#26368;&#20808;&#36827;&#30340;SafeRL&#31639;&#27861;&#12290;&#36825;&#20010;&#32508;&#21512;&#24615;&#24211;&#21487;&#20197;&#20316;&#20026;&#30740;&#31350;&#31038;&#21306;&#30340;&#39564;&#35777;&#24037;&#20855;&#12290;&#36890;&#36807;&#24341;&#20837;&#36825;&#20010;&#22522;&#20934;&#65292;&#25105;&#20204;&#26088;&#22312;&#20419;&#36827;&#23545;&#23433;&#20840;&#24615;&#33021;&#30340;&#35780;&#20272;&#21644;&#27604;&#36739;&#65292;&#20174;&#32780;&#25512;&#21160;&#24378;&#21270;&#23398;&#20064;&#22312;&#23433;&#20840;&#24615;&#33021;&#19978;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI) systems possess significant potential to drive societal progress. However, their deployment often faces obstacles due to substantial safety concerns. Safe reinforcement learning (SafeRL) emerges as a solution to optimize policies while simultaneously adhering to multiple constraints, thereby addressing the challenge of integrating reinforcement learning in safety-critical scenarios. In this paper, we present an environment suite called Safety-Gymnasium, which encompasses safety-critical tasks in both single and multi-agent scenarios, accepting vector and vision-only input. Additionally, we offer a library of algorithms named Safe Policy Optimization (SafePO), comprising 16 state-of-the-art SafeRL algorithms. This comprehensive library can serve as a validation tool for the research community. By introducing this benchmark, we aim to facilitate the evaluation and comparison of safety performance, thus fostering the development of reinforcement learning for s
&lt;/p&gt;</description></item><item><title>&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#19979;&#65292;&#33258;&#21160;&#20462;&#22797;&#22768;&#26126;&#24335;&#36719;&#20214;&#35268;&#33539;&#30340;&#26377;&#25928;&#25216;&#26415;&#38656;&#27714;&#24840;&#21457;&#31361;&#20986;&#12290;&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#21033;&#29992;ChatGPT&#20462;&#22797;Alloy&#22768;&#26126;&#24335;&#35821;&#35328;&#35268;&#33539;&#30340;&#25928;&#26524;&#65292;&#24182;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27492;&#33258;&#21160;&#20462;&#22797;&#36807;&#31243;&#20013;&#30340;&#26426;&#20250;&#12290;</title><link>http://arxiv.org/abs/2310.12425</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#19979;&#30340;&#22768;&#26126;&#24335;&#36719;&#20214;&#35268;&#33539;&#33258;&#21160;&#20462;&#22797;
&lt;/p&gt;
&lt;p&gt;
Automated Repair of Declarative Software Specifications in the Era of Large Language Models. (arXiv:2310.12425v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12425
&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#19979;&#65292;&#33258;&#21160;&#20462;&#22797;&#22768;&#26126;&#24335;&#36719;&#20214;&#35268;&#33539;&#30340;&#26377;&#25928;&#25216;&#26415;&#38656;&#27714;&#24840;&#21457;&#31361;&#20986;&#12290;&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#21033;&#29992;ChatGPT&#20462;&#22797;Alloy&#22768;&#26126;&#24335;&#35821;&#35328;&#35268;&#33539;&#30340;&#25928;&#26524;&#65292;&#24182;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27492;&#33258;&#21160;&#20462;&#22797;&#36807;&#31243;&#20013;&#30340;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22768;&#26126;&#24335;&#36719;&#20214;&#35268;&#33539;&#35821;&#35328;&#30340;&#24191;&#27867;&#37319;&#29992;&#20197;&#21450;&#20854;&#22312;&#35843;&#35797;&#26041;&#38754;&#30340;&#22256;&#38590;&#24615;&#65292;&#20984;&#26174;&#20102;&#23545;&#36866;&#29992;&#20110;&#27492;&#31867;&#35821;&#35328;&#30340;&#26377;&#25928;&#33258;&#21160;&#21270;&#20462;&#22797;&#25216;&#26415;&#30340;&#38656;&#27714;&#12290;&#30740;&#31350;&#20154;&#21592;&#26368;&#36817;&#25506;&#32034;&#20102;&#21508;&#31181;&#26041;&#27861;&#26469;&#33258;&#21160;&#20462;&#22797;&#22768;&#26126;&#24335;&#36719;&#20214;&#35268;&#33539;&#65292;&#22914;&#22522;&#20110;&#27169;&#26495;&#30340;&#20462;&#22797;&#12289;&#21453;&#39304;&#39537;&#21160;&#30340;&#36845;&#20195;&#20462;&#22797;&#21644;&#26377;&#30028;&#31351;&#20030;&#26041;&#27861;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#26032;&#21457;&#23637;&#20026;&#33258;&#21160;&#20462;&#22797;&#22768;&#26126;&#24335;&#35268;&#33539;&#25552;&#20379;&#20102;&#26032;&#26426;&#20250;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#21033;&#29992;OpenAI&#30340;ChatGPT&#20462;&#22797;&#29992;Alloy&#22768;&#26126;&#24335;&#35821;&#35328;&#32534;&#20889;&#30340;&#36719;&#20214;&#35268;&#33539;&#30340;&#25928;&#26524;&#12290;&#19982;&#21629;&#20196;&#24335;&#35821;&#35328;&#19981;&#21516;&#65292;Alloy&#20013;&#30340;&#35268;&#33539;&#19981;&#20250;&#34987;&#25191;&#34892;&#65292;&#32780;&#26159;&#34987;&#36716;&#25442;&#20026;&#36923;&#36753;&#20844;&#24335;&#65292;&#24182;&#20351;&#29992;&#21518;&#31471;&#32422;&#26463;&#27714;&#35299;&#22120;&#36827;&#34892;&#35780;&#20272;&#65292;&#20197;&#35782;&#21035;&#35268;&#33539;&#23454;&#20363;&#21644;&#26029;&#35328;&#30340;&#21453;&#20363;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#37325;&#28857;&#26159;ChatGPT&#22312;&#25913;&#36827;&#22768;&#26126;&#24335;&#35268;&#33539;&#20462;&#22797;&#33021;&#21147;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The growing adoption of declarative software specification languages, coupled with their inherent difficulty in debugging, has underscored the need for effective and automated repair techniques applicable to such languages. Researchers have recently explored various methods to automatically repair declarative software specifications, such as template-based repair, feedback-driven iterative repair, and bounded exhaustive approaches. The latest developments in large language models provide new opportunities for the automatic repair of declarative specifications. In this study, we assess the effectiveness of utilizing OpenAI's ChatGPT to repair software specifications written in the Alloy declarative language. Unlike imperative languages, specifications in Alloy are not executed but rather translated into logical formulas and evaluated using backend constraint solvers to identify specification instances and counterexamples to assertions. Our evaluation focuses on ChatGPT's ability to impr
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#22810;&#20219;&#21153;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#21644;&#21457;&#29616;&#37327;&#23376;&#24615;&#36136;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#21516;&#26102;&#39044;&#27979;&#21644;&#21457;&#29616;&#22810;&#20010;&#37327;&#23376;&#24615;&#36136;&#65292;&#24182;&#19988;&#33021;&#22815;&#25512;&#26029;&#22810;&#20307;&#37327;&#23376;&#31995;&#32479;&#30340;&#20840;&#23616;&#24615;&#36136;&#21644;&#21457;&#29616;&#19981;&#21516;&#30456;&#20043;&#38388;&#30340;&#26410;&#30693;&#36793;&#30028;&#12290;</title><link>http://arxiv.org/abs/2310.11807</link><description>&lt;p&gt;
&#29992;&#22810;&#20219;&#21153;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#21644;&#21457;&#29616;&#37327;&#23376;&#24615;&#36136;
&lt;/p&gt;
&lt;p&gt;
Learning and Discovering Quantum Properties with Multi-Task Neural Networks. (arXiv:2310.11807v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11807
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#22810;&#20219;&#21153;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#21644;&#21457;&#29616;&#37327;&#23376;&#24615;&#36136;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#21516;&#26102;&#39044;&#27979;&#21644;&#21457;&#29616;&#22810;&#20010;&#37327;&#23376;&#24615;&#36136;&#65292;&#24182;&#19988;&#33021;&#22815;&#25512;&#26029;&#22810;&#20307;&#37327;&#23376;&#31995;&#32479;&#30340;&#20840;&#23616;&#24615;&#36136;&#21644;&#21457;&#29616;&#19981;&#21516;&#30456;&#20043;&#38388;&#30340;&#26410;&#30693;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26159;&#19968;&#31181;&#20174;&#26377;&#38480;&#27979;&#37327;&#25968;&#25454;&#20013;&#39044;&#27979;&#37327;&#23376;&#24577;&#24615;&#36136;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#32593;&#32476;&#27169;&#22411;&#65292;&#21487;&#20197;&#21516;&#26102;&#39044;&#27979;&#22810;&#20010;&#37327;&#23376;&#24615;&#36136;&#65292;&#21253;&#25324;&#37327;&#23376;&#21487;&#35266;&#27979;&#37327;&#30340;&#26399;&#26395;&#20540;&#20197;&#21450;&#37327;&#23376;&#29366;&#24577;&#30340;&#19968;&#33324;&#38750;&#32447;&#24615;&#20989;&#25968;&#65292;&#22914;&#32416;&#32544;&#29109;&#21644;&#22810;&#20307;&#25299;&#25169;&#19981;&#21464;&#37327;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#19968;&#20010;&#22312;&#32473;&#23450;&#24615;&#36136;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#20063;&#21487;&#20197;&#21457;&#29616;&#36229;&#20986;&#35813;&#38598;&#21512;&#30340;&#26032;&#24615;&#36136;&#12290;&#22810;&#21151;&#33021;&#35757;&#32451;&#36824;&#20351;&#27169;&#22411;&#33021;&#22815;&#20174;&#23616;&#37096;&#27979;&#37327;&#20013;&#25512;&#26029;&#20986;&#22810;&#20307;&#37327;&#23376;&#31995;&#32479;&#30340;&#20840;&#23616;&#24615;&#36136;&#65292;&#20998;&#31867;&#20445;&#25252;&#23545;&#31216;&#30340;&#25299;&#25169;&#29289;&#36136;&#30456;&#65292;&#24182;&#21457;&#29616;&#19981;&#21516;&#30456;&#20043;&#38388;&#30340;&#26410;&#30693;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks are a powerful tool for predicting properties of quantum states from limited measurement data. Here we develop a network model that can simultaneously predict multiple quantum properties, including not only expectation values of quantum observables, but also general nonlinear functions of the quantum state, like entanglement entropies and many-body topological invariants. Remarkably, we find that a model trained on a given set of properties can also discover new properties outside that set. Multi-purpose training also enables the model to infer global properties of many-body quantum systems from local measurements, to classify symmetry protected topological phases of matter, and to discover unknown boundaries between different phases.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#22686;&#24378;&#23398;&#20064;&#26041;&#27861;&#22312;&#35299;&#20915;&#32452;&#21512;&#38382;&#39064;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#25552;&#20379;&#20102;&#32943;&#23450;&#30340;&#31572;&#26696;&#12290;&#36825;&#20010;&#26694;&#26550;&#23545;&#20110;&#35299;&#20915;&#21253;&#25324;&#26368;&#22823;&#21106;&#21644;&#26368;&#23567;&#21106;&#12289;&#26368;&#22823;$k$&#32422;&#26463;&#38382;&#39064;&#12289;&#26368;&#22823;&#26435;&#37325;&#20108;&#20998;&#22270;&#21305;&#37197;&#21644;&#26053;&#34892;&#21830;&#38382;&#39064;&#22312;&#20869;&#30340;&#24191;&#27867;&#30340;&#32452;&#21512;&#38382;&#39064;&#26377;&#37325;&#35201;&#30340;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2310.05309</link><description>&lt;p&gt;
&#20248;&#21270;&#32452;&#21512;&#38382;&#39064;&#30340;&#35299;&#37319;&#26679;&#22120;&#65306;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#30340;&#26799;&#24230;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Optimizing Solution-Samplers for Combinatorial Problems: The Landscape of Policy-Gradient Methods. (arXiv:2310.05309v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05309
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#22686;&#24378;&#23398;&#20064;&#26041;&#27861;&#22312;&#35299;&#20915;&#32452;&#21512;&#38382;&#39064;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#25552;&#20379;&#20102;&#32943;&#23450;&#30340;&#31572;&#26696;&#12290;&#36825;&#20010;&#26694;&#26550;&#23545;&#20110;&#35299;&#20915;&#21253;&#25324;&#26368;&#22823;&#21106;&#21644;&#26368;&#23567;&#21106;&#12289;&#26368;&#22823;$k$&#32422;&#26463;&#38382;&#39064;&#12289;&#26368;&#22823;&#26435;&#37325;&#20108;&#20998;&#22270;&#21305;&#37197;&#21644;&#26053;&#34892;&#21830;&#38382;&#39064;&#22312;&#20869;&#30340;&#24191;&#27867;&#30340;&#32452;&#21512;&#38382;&#39064;&#26377;&#37325;&#35201;&#30340;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#22686;&#24378;&#23398;&#20064;&#26041;&#27861;&#22312;&#35299;&#20915;&#22797;&#26434;&#30340;&#32452;&#21512;&#38382;&#39064;&#26041;&#38754;&#20855;&#26377;&#24456;&#39640;&#30340;&#23454;&#29992;&#20215;&#20540;&#12290;&#22312;&#36825;&#20123;&#26041;&#27861;&#20013;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#34987;&#29992;&#20316;&#35299;&#20915;&#26041;&#26696;&#29983;&#25104;&#22120;&#65292;&#28982;&#21518;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#31561;&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#36880;&#27493;&#33719;&#24471;&#26356;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#20998;&#24067;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29702;&#35770;&#26694;&#26550;&#26469;&#20998;&#26512;&#36825;&#20123;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#23545;&#36825;&#20010;&#38382;&#39064;&#30340;&#31215;&#26497;&#22238;&#31572;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#36866;&#29992;&#20110;&#21253;&#25324;&#26368;&#22823;&#21106;&#21644;&#26368;&#23567;&#21106;&#12289;&#26368;&#22823;$k$&#32422;&#26463;&#38382;&#39064;&#12289;&#26368;&#22823;&#26435;&#37325;&#20108;&#20998;&#22270;&#21305;&#37197;&#21644;&#26053;&#34892;&#21830;&#38382;&#39064;&#22312;&#20869;&#30340;&#24191;&#27867;&#30340;&#32452;&#21512;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks and Reinforcement Learning methods have empirically shown great promise in tackling challenging combinatorial problems. In those methods a deep neural network is used as a solution generator which is then trained by gradient-based methods (e.g., policy gradient) to successively obtain better solution distributions. In this work we introduce a novel theoretical framework for analyzing the effectiveness of such methods. We ask whether there exist generative models that (i) are expressive enough to generate approximately optimal solutions; (ii) have a tractable, i.e, polynomial in the size of the input, number of parameters; (iii) their optimization landscape is benign in the sense that it does not contain sub-optimal stationary points. Our main contribution is a positive answer to this question. Our result holds for a broad class of combinatorial problems including Max- and Min-Cut, Max-$k$-CSP, Maximum-Weight-Bipartite-Matching, and the Traveling Salesman Problem. A
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24377;&#24615;&#19981;&#21464;&#31070;&#32463;&#32593;&#32476;&#65288;DINN&#65289;&#29992;&#20110;&#22788;&#29702;&#21463;&#21040;&#20960;&#20309;&#24418;&#21464;&#24433;&#21709;&#30340;&#22270;&#20687;&#30340;&#22270;&#20687;&#22788;&#29702;&#20219;&#21153;&#12290;DINN&#36890;&#36807;&#34701;&#20837;&#25311;&#20445;&#24418;&#21464;&#25442;&#32593;&#32476;&#65288;QCTN&#65289;&#26469;&#36755;&#20986;&#19968;&#33268;&#30340;&#28508;&#22312;&#29305;&#24449;&#65292;&#20351;&#24471;&#20855;&#26377;&#30456;&#21516;&#21407;&#22987;&#23545;&#35937;&#25110;&#22330;&#26223;&#30340;&#20960;&#20309;&#24418;&#21464;&#22270;&#20687;&#33021;&#22815;&#26356;&#25509;&#36817;&#33258;&#28982;&#25110;&#33391;&#22909;&#22270;&#20687;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2310.02641</link><description>&lt;p&gt;
&#24377;&#24615;&#19981;&#21464;&#31070;&#32463;&#32593;&#32476;&#21450;&#20854;&#22312;&#21464;&#24418;&#22270;&#20687;&#24674;&#22797;&#21644;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Deformation-Invariant Neural Network and Its Applications in Distorted Image Restoration and Analysis. (arXiv:2310.02641v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02641
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24377;&#24615;&#19981;&#21464;&#31070;&#32463;&#32593;&#32476;&#65288;DINN&#65289;&#29992;&#20110;&#22788;&#29702;&#21463;&#21040;&#20960;&#20309;&#24418;&#21464;&#24433;&#21709;&#30340;&#22270;&#20687;&#30340;&#22270;&#20687;&#22788;&#29702;&#20219;&#21153;&#12290;DINN&#36890;&#36807;&#34701;&#20837;&#25311;&#20445;&#24418;&#21464;&#25442;&#32593;&#32476;&#65288;QCTN&#65289;&#26469;&#36755;&#20986;&#19968;&#33268;&#30340;&#28508;&#22312;&#29305;&#24449;&#65292;&#20351;&#24471;&#20855;&#26377;&#30456;&#21516;&#21407;&#22987;&#23545;&#35937;&#25110;&#22330;&#26223;&#30340;&#20960;&#20309;&#24418;&#21464;&#22270;&#20687;&#33021;&#22815;&#26356;&#25509;&#36817;&#33258;&#28982;&#25110;&#33391;&#22909;&#22270;&#20687;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#20960;&#20309;&#24418;&#21464;&#24433;&#21709;&#30340;&#22270;&#20687;&#23545;&#20110;&#30446;&#26631;&#35782;&#21035;&#31561;&#22270;&#20687;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#26469;&#35828;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#12290;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22270;&#20687;&#27169;&#22411;&#36890;&#24120;&#26080;&#27861;&#23545;&#20960;&#20309;&#24418;&#21464;&#22270;&#20687;&#32473;&#20986;&#20934;&#30830;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24377;&#24615;&#19981;&#21464;&#31070;&#32463;&#32593;&#32476;&#65288;DINN&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#20960;&#20309;&#24418;&#21464;&#22270;&#20687;&#30340;&#22270;&#20687;&#22788;&#29702;&#20219;&#21153;&#12290;DINN&#20026;&#20960;&#20309;&#24418;&#21464;&#22270;&#20687;&#36755;&#20986;&#19968;&#33268;&#30340;&#28508;&#22312;&#29305;&#24449;&#65292;&#36825;&#20123;&#22270;&#20687;&#20855;&#26377;&#30456;&#21516;&#30340;&#21407;&#22987;&#23545;&#35937;&#25110;&#22330;&#26223;&#12290;DINN&#30340;&#24605;&#24819;&#26159;&#23558;&#19968;&#20010;&#31616;&#21333;&#30340;&#32452;&#20214;&#65292;&#31216;&#20026;&#25311;&#20445;&#24418;&#21464;&#25442;&#32593;&#32476;&#65288;QCTN&#65289;&#65292;&#34701;&#20837;&#21040;&#20854;&#20182;&#29616;&#26377;&#30340;&#28145;&#24230;&#32593;&#32476;&#20013;&#36827;&#34892;&#22270;&#20687;&#22788;&#29702;&#20219;&#21153;&#12290;QCTN&#26159;&#19968;&#20010;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#23427;&#36755;&#20986;&#19968;&#20010;&#25311;&#20445;&#24418;&#26144;&#23556;&#65292;&#21487;&#20197;&#23558;&#20960;&#20309;&#24418;&#21464;&#30340;&#22270;&#20687;&#36716;&#25442;&#20026;&#26356;&#25509;&#36817;&#33258;&#28982;&#25110;&#33391;&#22909;&#22270;&#20687;&#20998;&#24067;&#30340;&#25913;&#36827;&#29256;&#26412;&#12290;&#23427;&#39318;&#20808;&#36755;&#20986;&#19968;&#20010;&#36125;&#23572;&#29305;&#25289;&#23494;&#31995;&#25968;&#65292;&#29992;&#20110;&#34913;&#37327;&#25311;&#20445;&#24418;&#26144;&#23556;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Images degraded by geometric distortions pose a significant challenge to imaging and computer vision tasks such as object recognition. Deep learning-based imaging models usually fail to give accurate performance for geometrically distorted images. In this paper, we propose the deformation-invariant neural network (DINN), a framework to address the problem of imaging tasks for geometrically distorted images. The DINN outputs consistent latent features for images that are geometrically distorted but represent the same underlying object or scene. The idea of DINN is to incorporate a simple component, called the quasiconformal transformer network (QCTN), into other existing deep networks for imaging tasks. The QCTN is a deep neural network that outputs a quasiconformal map, which can be used to transform a geometrically distorted image into an improved version that is closer to the distribution of natural or good images. It first outputs a Beltrami coefficient, which measures the quasiconf
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#31579;&#36873;&#33258;&#28982;&#23545;&#31435;&#31034;&#20363;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#35780;&#20272;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#36890;&#36807;&#33258;&#21160;&#24369;&#30417;&#30563;&#26631;&#27880;&#33719;&#24471;&#30340;&#27010;&#29575;&#26631;&#31614;&#26469;&#23454;&#29616;&#36825;&#19968;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.00543</link><description>&lt;p&gt;
&#20026;&#20102;&#26500;&#24314;&#21487;&#20449;&#36182;&#30340;&#21307;&#30103;AI&#31995;&#32479;&#65292;&#31579;&#36873;&#22825;&#28982;&#23545;&#31435;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Curating Naturally Adversarial Datasets for Trustworthy AI in Healthcare. (arXiv:2309.00543v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00543
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#31579;&#36873;&#33258;&#28982;&#23545;&#31435;&#31034;&#20363;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#35780;&#20272;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#36890;&#36807;&#33258;&#21160;&#24369;&#30417;&#30563;&#26631;&#27880;&#33719;&#24471;&#30340;&#27010;&#29575;&#26631;&#31614;&#26469;&#23454;&#29616;&#36825;&#19968;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#30340;&#21307;&#30103;&#24212;&#29992;&#20013;&#23637;&#31034;&#20986;&#20102;&#26377;&#24076;&#26395;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#30830;&#20445;&#36825;&#20123;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#23545;&#20110;&#26500;&#24314;&#21487;&#20449;&#36182;&#30340;AI&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#20110;&#23545;&#21512;&#25104;&#23545;&#31435;&#31034;&#20363;&#30340;&#40065;&#26834;&#24615;&#65292;&#36825;&#20123;&#31034;&#20363;&#26159;&#36890;&#36807;&#21521;&#28165;&#27905;&#36755;&#20837;&#25968;&#25454;&#28155;&#21152;&#38590;&#20197;&#23519;&#35273;&#30340;&#25200;&#21160;&#32780;&#21046;&#20316;&#20986;&#26469;&#30340;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#21512;&#25104;&#23545;&#31435;&#31034;&#20363;&#24182;&#19981;&#33021;&#20934;&#30830;&#21453;&#26144;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#29616;&#23454;&#22330;&#26223;&#65292;&#29305;&#21035;&#26159;&#22312;&#21307;&#30103;&#25968;&#25454;&#30340;&#32972;&#26223;&#19979;&#12290;&#22240;&#27492;&#65292;&#23545;&#21512;&#25104;&#23545;&#31435;&#31034;&#20363;&#30340;&#40065;&#26834;&#24615;&#26410;&#24517;&#33021;&#22815;&#36716;&#21270;&#20026;&#23545;&#33258;&#28982;&#20135;&#29983;&#30340;&#23545;&#31435;&#31034;&#20363;&#30340;&#40065;&#26834;&#24615;&#65292;&#32780;&#36825;&#23545;&#20110;&#21487;&#20449;&#36182;&#30340;AI&#32780;&#35328;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31579;&#36873;&#30001;&#33258;&#28982;&#23545;&#31435;&#31034;&#20363;&#32452;&#25104;&#30340;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20381;&#36182;&#20110;&#36890;&#36807;&#33258;&#21160;&#24369;&#30417;&#30563;&#26631;&#27880;&#33719;&#24471;&#30340;&#27010;&#29575;&#26631;&#31614;&#65292;&#36825;&#31181;&#26631;&#31614;&#32467;&#21512;&#20102;&#22024;&#26434;&#19988;&#26131;&#33719;&#24471;&#30340;&#26631;&#27880;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning models have shown promising predictive accuracy for time-series healthcare applications. However, ensuring the robustness of these models is vital for building trustworthy AI systems. Existing research predominantly focuses on robustness to synthetic adversarial examples, crafted by adding imperceptible perturbations to clean input data. However, these synthetic adversarial examples do not accurately reflect the most challenging real-world scenarios, especially in the context of healthcare data. Consequently, robustness to synthetic adversarial examples may not necessarily translate to robustness against naturally occurring adversarial examples, which is highly desirable for trustworthy AI. We propose a method to curate datasets comprised of natural adversarial examples to evaluate model robustness. The method relies on probabilistic labels obtained from automated weakly-supervised labeling that combines noisy and cheap-to-obtain labeling heuristics. Based on these labels
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25991;&#26412;&#30340;&#32500;&#22522;&#30334;&#31185;&#25991;&#31456;&#38142;&#25509;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#35789;&#24615;&#26631;&#27880;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#23545;&#20004;&#20010;&#33410;&#28857;&#26159;&#21542;&#26377;&#38142;&#25509;&#30340;&#20998;&#31867;&#39044;&#27979;&#65292;&#24182;&#22312;&#27604;&#36187;&#20013;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#25104;&#32489;&#12290;</title><link>http://arxiv.org/abs/2309.00317</link><description>&lt;p&gt;
&#22522;&#20110;&#25991;&#26412;&#30340;&#32500;&#22522;&#30334;&#31185;&#25991;&#31456;&#38142;&#25509;&#39044;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Text-based Approach For Link Prediction on Wikipedia Articles. (arXiv:2309.00317v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00317
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25991;&#26412;&#30340;&#32500;&#22522;&#30334;&#31185;&#25991;&#31456;&#38142;&#25509;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#35789;&#24615;&#26631;&#27880;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#23545;&#20004;&#20010;&#33410;&#28857;&#26159;&#21542;&#26377;&#38142;&#25509;&#30340;&#20998;&#31867;&#39044;&#27979;&#65292;&#24182;&#22312;&#27604;&#36187;&#20013;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#25104;&#32489;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#22312;DSAA 2023&#25361;&#25112;&#20013;&#20851;&#20110;&#32500;&#22522;&#30334;&#31185;&#25991;&#31456;&#38142;&#25509;&#39044;&#27979;&#30340;&#24037;&#20316;&#12290;&#25105;&#20204;&#21033;&#29992;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20197;&#21450;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#30340;&#35789;&#24615;&#26631;&#27880;&#29305;&#24449;&#26469;&#35757;&#32451;&#20998;&#31867;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#20004;&#20010;&#33410;&#28857;&#26159;&#21542;&#26377;&#38142;&#25509;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#29305;&#24449;&#22312;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#19978;&#36827;&#34892;&#27979;&#35797;&#12290;&#25105;&#20204;&#36890;&#36807;F1&#24471;&#20998;&#20026;0.99999&#33719;&#24471;&#20102;&#31532;7&#21517;&#12290;&#25105;&#20204;&#30340;&#28304;&#20195;&#30721;&#21487;&#20197;&#22312;&#20197;&#19979;&#38142;&#25509;&#20013;&#20844;&#24320;&#33719;&#21462;&#65306;https://github.com/Tam1032/DSAA2023-Challenge-Link-prediction-DS-UIT_SAT
&lt;/p&gt;
&lt;p&gt;
This paper present our work in the DSAA 2023 Challenge about Link Prediction for Wikipedia Articles. We use traditional machine learning models with POS tags (part-of-speech tags) features extracted from text to train the classification model for predicting whether two nodes has the link. Then, we use these tags to test on various machine learning models. We obtained the results by F1 score at 0.99999 and got 7th place in the competition. Our source code is publicly available at this link: https://github.com/Tam1032/DSAA2023-Challenge-Link-prediction-DS-UIT_SAT
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31227;&#21160;&#25968;&#25454;&#36827;&#34892;&#23454;&#26102;&#38656;&#27714;&#21709;&#24212;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26469;&#32531;&#35299;&#36710;&#31449;&#25317;&#25380;&#30340;&#38081;&#36335;&#37325;&#26032;&#35843;&#24230;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25512;&#26029;&#30495;&#23454;&#19990;&#30028;&#30340;&#20056;&#23458;&#27969;&#21160;&#24615;&#26469;&#23454;&#29616;&#23454;&#26102;&#35843;&#24230;&#65292;&#24182;&#30528;&#37325;&#32771;&#34385;&#39640;&#38656;&#27714;&#36710;&#31449;&#30340;&#37325;&#26032;&#23433;&#25490;&#12290;</title><link>http://arxiv.org/abs/2308.11849</link><description>&lt;p&gt;
&#19968;&#31181;&#21033;&#29992;&#31227;&#21160;&#25968;&#25454;&#36827;&#34892;&#23454;&#26102;&#38656;&#27714;&#21709;&#24212;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26469;&#32531;&#35299;&#36710;&#31449;&#25317;&#25380;&#30340;&#38081;&#36335;&#37325;&#26032;&#35843;&#24230;
&lt;/p&gt;
&lt;p&gt;
A deep reinforcement learning approach for real-time demand-responsive railway rescheduling to mitigate station overcrowding using mobile data. (arXiv:2308.11849v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11849
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31227;&#21160;&#25968;&#25454;&#36827;&#34892;&#23454;&#26102;&#38656;&#27714;&#21709;&#24212;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26469;&#32531;&#35299;&#36710;&#31449;&#25317;&#25380;&#30340;&#38081;&#36335;&#37325;&#26032;&#35843;&#24230;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25512;&#26029;&#30495;&#23454;&#19990;&#30028;&#30340;&#20056;&#23458;&#27969;&#21160;&#24615;&#26469;&#23454;&#29616;&#23454;&#26102;&#35843;&#24230;&#65292;&#24182;&#30528;&#37325;&#32771;&#34385;&#39640;&#38656;&#27714;&#36710;&#31449;&#30340;&#37325;&#26032;&#23433;&#25490;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#26102;&#38081;&#36335;&#37325;&#26032;&#35843;&#24230;&#26159;&#19968;&#31181;&#21450;&#26102;&#28789;&#27963;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#26681;&#25454;&#26102;&#21464;&#26465;&#20214;&#33258;&#21160;&#25913;&#21464;&#36816;&#33829;&#35745;&#21010;&#12290;&#30446;&#21069;&#30340;&#30740;&#31350;&#32570;&#20047;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#25429;&#25417;&#38081;&#36335;&#20013;&#20056;&#23458;&#22312;&#32039;&#24613;&#24773;&#20917;&#19979;&#30340;&#23454;&#26102;&#27969;&#21160;&#24615;&#65292;&#20027;&#35201;&#20381;&#36182;&#22522;&#20110;OD&#30340;&#25968;&#25454;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#21015;&#36710;&#30340;&#38656;&#27714;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#29616;&#26377;&#30340;&#38271;&#26399;&#32039;&#24613;&#24773;&#20917;&#19979;&#30340;&#35843;&#24230;&#26356;&#26032;&#21407;&#21017;&#24573;&#35270;&#20102;&#38656;&#27714;&#22312;&#26102;&#38388;&#19978;&#30340;&#19981;&#22343;&#21248;&#20998;&#24067;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38656;&#27714;&#21709;&#24212;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#31227;&#21160;&#25968;&#25454;&#20013;&#25512;&#26029;&#20986;&#30495;&#23454;&#19990;&#30028;&#30340;&#20056;&#23458;&#27969;&#21160;&#24615;&#26469;&#20419;&#36827;&#23454;&#26102;&#35843;&#24230;&#12290;&#19982;&#32593;&#32476;&#23618;&#38754;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#26412;&#25991;&#30528;&#37325;&#20851;&#27880;&#32039;&#24613;&#21306;&#22495;&#19978;&#28216;&#30340;&#39640;&#38656;&#27714;&#36710;&#31449;&#12290;&#30446;&#26631;&#26159;&#37325;&#26032;&#23433;&#25490;&#36890;&#36807;&#35813;&#30446;&#26631;&#31449;&#30340;&#22810;&#26465;&#32447;&#36335;&#19978;&#21463;&#21040;&#20005;&#37325;&#31361;&#21457;&#20107;&#20214;&#65288;&#22914;&#33258;&#28982;&#28798;&#23475;&#65289;&#24433;&#21709;&#30340;&#25152;&#26377;&#21015;&#36710;&#12290;&#38656;&#35201;&#29305;&#21035;&#27880;&#24847;&#36991;&#20813;&#31215;&#32047;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-time railway rescheduling is a timely and flexible technique to automatically alter the operation schedule in response to time-varying conditions. Current research lacks data-driven approaches that capture real-time passenger mobility during railway disruptions, relying mostly on OD-based data and model-based methods for estimating demands of trains. Meanwhile, the schedule-updating principles for a long-term disruption overlook the uneven distribution of demand over time. To fill this gap, this paper proposes a demand-responsive approach by inferring real-world passenger mobility from mobile data (MD) to facilitate real-time rescheduling. Unlike network-level approaches, this paper focuses on a heavy-demand station upstream of the disrupted area. The objective is to reschedule all trains on multiple routes passing through this target station, which have been affected by a severe emergency event such as a natural disaster. Particular attention should be given to avoiding the accum
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;AI&#20195;&#29702;&#29992;&#20110;&#20219;&#21153;&#35268;&#21010;&#21644;&#24037;&#20855;&#20351;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#26500;&#26694;&#26550;&#65292;&#35774;&#35745;&#20102;&#20004;&#31181;&#20195;&#29702;&#26469;&#25191;&#34892;&#25512;&#29702;&#36807;&#31243;&#65292;&#23454;&#20363;&#21270;&#20102;&#26694;&#26550;&#65292;&#24182;&#35780;&#20272;&#20102;&#23427;&#20204;&#30340;&#20219;&#21153;&#35268;&#21010;&#21644;&#24037;&#20855;&#20351;&#29992;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.03427</link><description>&lt;p&gt;
TPTU: &#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;AI&#20195;&#29702;&#29992;&#20110;&#20219;&#21153;&#35268;&#21010;&#21644;&#24037;&#20855;&#20351;&#29992;
&lt;/p&gt;
&lt;p&gt;
TPTU: Large Language Model-based AI Agents for Task Planning and Tool Usage. (arXiv:2308.03427v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03427
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;AI&#20195;&#29702;&#29992;&#20110;&#20219;&#21153;&#35268;&#21010;&#21644;&#24037;&#20855;&#20351;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#26500;&#26694;&#26550;&#65292;&#35774;&#35745;&#20102;&#20004;&#31181;&#20195;&#29702;&#26469;&#25191;&#34892;&#25512;&#29702;&#36807;&#31243;&#65292;&#23454;&#20363;&#21270;&#20102;&#26694;&#26550;&#65292;&#24182;&#35780;&#20272;&#20102;&#23427;&#20204;&#30340;&#20219;&#21153;&#35268;&#21010;&#21644;&#24037;&#20855;&#20351;&#29992;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#25104;&#20026;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#23613;&#31649;&#23427;&#20204;&#38750;&#24120;&#24378;&#22823;&#65292;&#20294;&#26159;LLMs&#30340;&#20869;&#22312;&#29983;&#25104;&#33021;&#21147;&#21487;&#33021;&#19981;&#36275;&#20197;&#22788;&#29702;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#36825;&#20123;&#20219;&#21153;&#38656;&#35201;&#32467;&#21512;&#20219;&#21153;&#35268;&#21010;&#21644;&#22806;&#37096;&#24037;&#20855;&#30340;&#20351;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#38376;&#20026;LLM-based AI Agents&#37327;&#36523;&#23450;&#21046;&#30340;&#32467;&#26500;&#26694;&#26550;&#65292;&#24182;&#35752;&#35770;&#20102;&#22788;&#29702;&#22797;&#26434;&#38382;&#39064;&#25152;&#24517;&#38656;&#30340;&#20851;&#38190;&#33021;&#21147;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20869;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#20195;&#29702;&#65288;&#21363;&#19968;&#27493;&#20195;&#29702;&#21644;&#36830;&#32493;&#20195;&#29702;&#65289;&#26469;&#25191;&#34892;&#25512;&#29702;&#36807;&#31243;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#21508;&#31181;LLMs&#23454;&#20363;&#21270;&#20102;&#36825;&#20010;&#26694;&#26550;&#65292;&#24182;&#35780;&#20272;&#20102;&#23427;&#20204;&#22312;&#20856;&#22411;&#20219;&#21153;&#20013;&#30340;&#20219;&#21153;&#35268;&#21010;&#21644;&#24037;&#20855;&#20351;&#29992;&#33021;&#21147;&#12290;&#36890;&#36807;&#24378;&#35843;&#20851;&#38190;&#21457;&#29616;&#21644;&#25361;&#25112;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#25552;&#20379;&#19968;&#20010;&#26377;&#21161;&#20110;&#22312;&#20182;&#20204;&#30340;AI&#24212;&#29992;&#20013;&#21457;&#25381;LLMs&#33021;&#21147;&#30340;&#26377;&#29992;&#36164;&#28304;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102;
&lt;/p&gt;
&lt;p&gt;
With recent advancements in natural language processing, Large Language Models (LLMs) have emerged as powerful tools for various real-world applications. Despite their prowess, the intrinsic generative abilities of LLMs may prove insufficient for handling complex tasks which necessitate a combination of task planning and the usage of external tools. In this paper, we first propose a structured framework tailored for LLM-based AI Agents and discuss the crucial capabilities necessary for tackling intricate problems. Within this framework, we design two distinct types of agents (i.e., one-step agent and sequential agent) to execute the inference process. Subsequently, we instantiate the framework using various LLMs and evaluate their Task Planning and Tool Usage (TPTU) abilities on typical tasks. By highlighting key findings and challenges, our goal is to provide a helpful resource for researchers and practitioners to leverage the power of LLMs in their AI applications. Our study emphasiz
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#22312;&#23384;&#22312;&#27169;&#22411;EMA&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20248;&#21270;&#30340;&#32553;&#25918;&#35268;&#21017;&#65292;&#20197;&#20445;&#25345;&#35757;&#32451;&#21160;&#24577;&#30340;&#19968;&#33268;&#24615;&#12290;&#36825;&#23545;&#20110;&#23454;&#38469;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#26435;&#34913;&#25209;&#37327;&#22823;&#23567;&#21644;&#22681;&#38047;&#26102;&#38388;&#38750;&#24120;&#37325;&#35201;&#12290;&#27169;&#22411;EMA&#33021;&#22815;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#20197;&#21450;&#31283;&#23450;&#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#20026;&#33258;&#30417;&#30563;&#23398;&#20064;&#25552;&#20379;&#23398;&#20064;&#20449;&#21495;&#12290;</title><link>http://arxiv.org/abs/2307.13813</link><description>&lt;p&gt;
&#22914;&#20309;&#25193;&#23637;&#24744;&#30340;EMA&#65288;arXiv:2307.13813v1 [stat.ML]&#65289;
&lt;/p&gt;
&lt;p&gt;
How to Scale Your EMA. (arXiv:2307.13813v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13813
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#22312;&#23384;&#22312;&#27169;&#22411;EMA&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20248;&#21270;&#30340;&#32553;&#25918;&#35268;&#21017;&#65292;&#20197;&#20445;&#25345;&#35757;&#32451;&#21160;&#24577;&#30340;&#19968;&#33268;&#24615;&#12290;&#36825;&#23545;&#20110;&#23454;&#38469;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#26435;&#34913;&#25209;&#37327;&#22823;&#23567;&#21644;&#22681;&#38047;&#26102;&#38388;&#38750;&#24120;&#37325;&#35201;&#12290;&#27169;&#22411;EMA&#33021;&#22815;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#20197;&#21450;&#31283;&#23450;&#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#20026;&#33258;&#30417;&#30563;&#23398;&#20064;&#25552;&#20379;&#23398;&#20064;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#20445;&#25345;&#35757;&#32451;&#21160;&#24577;&#22312;&#25209;&#37327;&#22823;&#23567;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#26159;&#19968;&#31181;&#37325;&#35201;&#24037;&#20855;&#65292;&#23427;&#33021;&#22815;&#22312;&#25209;&#37327;&#22823;&#23567;&#21644;&#22681;&#38047;&#26102;&#38388;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#12290;&#36825;&#31181;&#26435;&#34913;&#36890;&#24120;&#36890;&#36807;&#19968;&#20010;&#32553;&#25918;&#35268;&#21017;&#26469;&#23454;&#29616;&#65292;&#20363;&#22914;&#65292;&#22312;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#20013;&#65292;&#24212;&#35813;&#23558;&#23398;&#20064;&#29575;&#19982;&#25209;&#37327;&#22823;&#23567;&#21576;&#32447;&#24615;&#20851;&#31995;&#12290;&#21478;&#19968;&#20010;&#23454;&#38469;&#26426;&#22120;&#23398;&#20064;&#30340;&#37325;&#35201;&#24037;&#20855;&#26159;&#27169;&#22411;&#25351;&#25968;&#31227;&#21160;&#24179;&#22343;&#65288;EMA&#65289;&#65292;&#23427;&#26159;&#19968;&#20010;&#19981;&#25509;&#25910;&#26799;&#24230;&#20449;&#24687;&#30340;&#27169;&#22411;&#21103;&#26412;&#65292;&#32780;&#26159;&#20197;&#19968;&#23450;&#30340;&#21160;&#37327;&#36319;&#38543;&#20854;&#30446;&#26631;&#27169;&#22411;&#12290;&#36825;&#20010;&#27169;&#22411;EMA&#21487;&#20197;&#25552;&#39640;&#30417;&#30563;&#23398;&#20064;&#30340;&#31283;&#20581;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;&#65292;&#31283;&#23450;&#20266;&#26631;&#35760;&#65292;&#20026;&#33258;&#30417;&#30563;&#23398;&#20064;&#25552;&#20379;&#23398;&#20064;&#20449;&#21495;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#23558;&#27169;&#22411;EMA&#19982;&#20248;&#21270;&#20998;&#24320;&#22788;&#29702;&#65292;&#23548;&#33268;&#25209;&#37327;&#22823;&#23567;&#20043;&#38388;&#23384;&#22312;&#19981;&#21516;&#30340;&#35757;&#32451;&#21160;&#24577;&#21644;&#36739;&#20302;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22312;&#23384;&#22312;&#27169;&#22411;EMA&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20248;&#21270;&#30340;&#32553;&#25918;&#35268;&#21017;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Preserving training dynamics across batch sizes is an important tool for practical machine learning as it enables the trade-off between batch size and wall-clock time. This trade-off is typically enabled by a scaling rule, for example, in stochastic gradient descent, one should scale the learning rate linearly with the batch size. Another important tool for practical machine learning is the model Exponential Moving Average (EMA), which is a model copy that does not receive gradient information, but instead follows its target model with some momentum. This model EMA can improve the robustness and generalization properties of supervised learning, stabilize pseudo-labeling, and provide a learning signal for Self-Supervised Learning (SSL). Prior works have treated the model EMA separately from optimization, leading to different training dynamics across batch sizes and lower model performance. In this work, we provide a scaling rule for optimization in the presence of model EMAs and demonst
&lt;/p&gt;</description></item><item><title>FedMEKT&#26159;&#19968;&#31181;&#22522;&#20110;&#33976;&#39311;&#30340;&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#21033;&#29992;&#19981;&#21516;&#27169;&#24577;&#30340;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#26381;&#21153;&#22120;&#21644;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#32852;&#21512;&#30693;&#35782;&#20256;&#36755;&#12290;</title><link>http://arxiv.org/abs/2307.13214</link><description>&lt;p&gt;
FedMEKT: &#22522;&#20110;&#33976;&#39311;&#30340;&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#23884;&#20837;&#30693;&#35782;&#20256;&#36755;
&lt;/p&gt;
&lt;p&gt;
FedMEKT: Distillation-based Embedding Knowledge Transfer for Multimodal Federated Learning. (arXiv:2307.13214v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13214
&lt;/p&gt;
&lt;p&gt;
FedMEKT&#26159;&#19968;&#31181;&#22522;&#20110;&#33976;&#39311;&#30340;&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#21033;&#29992;&#19981;&#21516;&#27169;&#24577;&#30340;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#26381;&#21153;&#22120;&#21644;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#32852;&#21512;&#30693;&#35782;&#20256;&#36755;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20351;&#22810;&#20010;&#23458;&#25143;&#31471;&#33021;&#22815;&#22312;&#19981;&#20849;&#20139;&#31169;&#26377;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21327;&#21516;&#35757;&#32451;&#19968;&#20010;&#24191;&#20041;&#20840;&#23616;&#27169;&#22411;&#65292;&#20174;&#32780;&#23454;&#29616;&#20998;&#25955;&#24335;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#12290;&#29616;&#26377;&#30340;&#22823;&#37096;&#20998;&#24037;&#20316;&#21482;&#26159;&#38024;&#23545;&#21333;&#27169;&#24577;&#25968;&#25454;&#25552;&#20986;&#20102;&#20856;&#22411;&#30340;FL&#31995;&#32479;&#65292;&#22240;&#27492;&#38480;&#21046;&#20102;&#23427;&#23545;&#20110;&#21033;&#29992;&#23453;&#36149;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#36827;&#34892;&#26410;&#26469;&#20010;&#24615;&#21270;&#24212;&#29992;&#30340;&#28508;&#21147;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;FL&#26041;&#27861;&#20173;&#28982;&#20381;&#36182;&#20110;&#23458;&#25143;&#31471;&#30340;&#26631;&#35760;&#25968;&#25454;&#65292;&#30001;&#20110;&#29992;&#25143;&#26080;&#27861;&#36827;&#34892;&#33258;&#27880;&#37322;&#65292;&#36825;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#26159;&#26377;&#38480;&#30340;&#12290;&#37492;&#20110;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22810;&#27169;&#24577;FL&#26694;&#26550;&#65292;&#37319;&#29992;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#21033;&#29992;&#19981;&#21516;&#27169;&#24577;&#30340;&#34920;&#31034;&#12290;&#23558;&#36825;&#20010;&#27010;&#24565;&#24341;&#20837;&#19968;&#20010;&#31995;&#32479;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#33976;&#39311;&#30340;&#22810;&#27169;&#24577;&#23884;&#20837;&#30693;&#35782;&#20256;&#36755;&#26426;&#21046;&#65292;&#31216;&#20026;FedMEKT&#65292;&#23427;&#20801;&#35768;&#26381;&#21153;&#22120;&#21644;&#23458;&#25143;&#31471;&#20132;&#25442;&#20174;&#23567;&#22411;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#30340;&#23398;&#20064;&#27169;&#22411;&#30340;&#32852;&#21512;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) enables a decentralized machine learning paradigm for multiple clients to collaboratively train a generalized global model without sharing their private data. Most existing works simply propose typical FL systems for single-modal data, thus limiting its potential on exploiting valuable multimodal data for future personalized applications. Furthermore, the majority of FL approaches still rely on the labeled data at the client side, which is limited in real-world applications due to the inability of self-annotation from users. In light of these limitations, we propose a novel multimodal FL framework that employs a semi-supervised learning approach to leverage the representations from different modalities. Bringing this concept into a system, we develop a distillation-based multimodal embedding knowledge transfer mechanism, namely FedMEKT, which allows the server and clients to exchange the joint knowledge of their learning models extracted from a small multimodal 
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34987;&#35748;&#20026;&#26159;&#20855;&#26377;&#20010;&#24615;&#25110;&#19968;&#22871;&#20215;&#20540;&#35266;&#30340;&#65292;&#20294;&#23454;&#38469;&#19978;&#23427;&#21487;&#20197;&#30475;&#20316;&#26159;&#20855;&#26377;&#19981;&#21516;&#20215;&#20540;&#35266;&#21644;&#20010;&#24615;&#29305;&#24449;&#30340;&#35282;&#24230;&#30340;&#21472;&#21152;&#12290;&#36890;&#36807;&#35282;&#24230;&#21487;&#25511;&#24615;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#35282;&#24230;&#19979;&#23637;&#31034;&#30340;&#20215;&#20540;&#35266;&#21644;&#20010;&#24615;&#29305;&#24449;&#30340;&#21464;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#26126;&#26174;&#25552;&#31034;&#30340;&#24773;&#20917;&#19979;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20063;&#20250;&#34920;&#36798;&#20986;&#19981;&#21516;&#30340;&#20215;&#20540;&#35266;&#12290;</title><link>http://arxiv.org/abs/2307.07870</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25991;&#21270;&#35282;&#24230;&#30340;&#21472;&#21152;
&lt;/p&gt;
&lt;p&gt;
Large Language Models as Superpositions of Cultural Perspectives. (arXiv:2307.07870v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07870
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34987;&#35748;&#20026;&#26159;&#20855;&#26377;&#20010;&#24615;&#25110;&#19968;&#22871;&#20215;&#20540;&#35266;&#30340;&#65292;&#20294;&#23454;&#38469;&#19978;&#23427;&#21487;&#20197;&#30475;&#20316;&#26159;&#20855;&#26377;&#19981;&#21516;&#20215;&#20540;&#35266;&#21644;&#20010;&#24615;&#29305;&#24449;&#30340;&#35282;&#24230;&#30340;&#21472;&#21152;&#12290;&#36890;&#36807;&#35282;&#24230;&#21487;&#25511;&#24615;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#35282;&#24230;&#19979;&#23637;&#31034;&#30340;&#20215;&#20540;&#35266;&#21644;&#20010;&#24615;&#29305;&#24449;&#30340;&#21464;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#26126;&#26174;&#25552;&#31034;&#30340;&#24773;&#20917;&#19979;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20063;&#20250;&#34920;&#36798;&#20986;&#19981;&#21516;&#30340;&#20215;&#20540;&#35266;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24120;&#24120;&#34987;&#38169;&#35823;&#22320;&#35748;&#20026;&#20855;&#26377;&#20010;&#24615;&#25110;&#19968;&#22871;&#20215;&#20540;&#35266;&#12290;&#25105;&#20204;&#35748;&#20026;LLMs&#21487;&#20197;&#30475;&#20316;&#26159;&#20855;&#26377;&#19981;&#21516;&#20215;&#20540;&#35266;&#21644;&#20010;&#24615;&#29305;&#24449;&#30340;&#35282;&#24230;&#21472;&#21152;&#12290;LLMs&#34920;&#29616;&#20986;&#20381;&#36182;&#20110;&#19978;&#19979;&#25991;&#30340;&#20215;&#20540;&#35266;&#21644;&#20010;&#24615;&#29305;&#24449;&#65292;&#36825;&#20123;&#29305;&#24449;&#22522;&#20110;&#20135;&#29983;&#30340;&#35282;&#24230;&#32780;&#25913;&#21464;&#65288;&#19982;&#20154;&#31867;&#30456;&#21453;&#65292;&#20154;&#31867;&#22312;&#19981;&#21516;&#24773;&#22659;&#19979;&#36890;&#24120;&#20855;&#26377;&#26356;&#19968;&#33268;&#30340;&#20215;&#20540;&#35266;&#21644;&#20010;&#24615;&#29305;&#24449;&#65289;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;&#35282;&#24230;&#21487;&#25511;&#24615;&#8221;&#30340;&#27010;&#24565;&#65292;&#25351;&#30340;&#26159;&#27169;&#22411;&#37319;&#29992;&#19981;&#21516;&#20855;&#26377;&#19981;&#21516;&#20215;&#20540;&#35266;&#21644;&#20010;&#24615;&#29305;&#24449;&#30340;&#35282;&#24230;&#30340;&#33021;&#21147;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#24515;&#29702;&#23398;&#38382;&#21367;&#65288;PVQ&#12289;VSM&#12289;IPIP&#65289;&#26469;&#30740;&#31350;&#23637;&#31034;&#30340;&#20215;&#20540;&#35266;&#21644;&#20010;&#24615;&#29305;&#24449;&#22914;&#20309;&#22522;&#20110;&#19981;&#21516;&#35282;&#24230;&#32780;&#25913;&#21464;&#12290;&#36890;&#36807;&#23450;&#24615;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#25552;&#31034;&#20013;&#65288;&#38544;&#24335;&#25110;&#26174;&#24335;&#65289;&#26263;&#31034;&#20102;&#26576;&#20123;&#20215;&#20540;&#35266;&#26102;&#65292;LLMs&#34920;&#36798;&#20986;&#19981;&#21516;&#30340;&#20215;&#20540;&#35266;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#26126;&#26174;&#26263;&#31034;&#30340;&#24773;&#20917;&#19979;&#65292;LLMs&#20063;&#20250;&#34920;&#36798;&#20986;&#19981;&#21516;&#30340;&#20215;&#20540;&#35266;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are often misleadingly recognized as having a personality or a set of values. We argue that an LLM can be seen as a superposition of perspectives with different values and personality traits. LLMs exhibit context-dependent values and personality traits that change based on the induced perspective (as opposed to humans, who tend to have more coherent values and personality traits across contexts). We introduce the concept of perspective controllability, which refers to a model's affordance to adopt various perspectives with differing values and personality traits. In our experiments, we use questionnaires from psychology (PVQ, VSM, IPIP) to study how exhibited values and personality traits change based on different perspectives. Through qualitative experiments, we show that LLMs express different values when those are (implicitly or explicitly) implied in the prompt, and that LLMs express different values even when those are not obviously implied (demonstrat
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FAIRO&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#20154;&#26426;&#20132;&#20114;&#29615;&#22659;&#20013;&#23454;&#29616;&#20844;&#24179;&#30340;&#39034;&#24207;&#20915;&#31574;&#12290;&#35813;&#31639;&#27861;&#23558;&#20154;&#30340;&#34892;&#20026;&#21487;&#21464;&#24615;&#21644;&#20559;&#22909;&#21464;&#21270;&#32771;&#34385;&#22312;&#20869;&#65292;&#36890;&#36807;&#21033;&#29992;&#36873;&#39033;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#23558;&#22797;&#26434;&#30340;&#20844;&#24179;&#20219;&#21153;&#20998;&#35299;&#20026;&#33258;&#36866;&#24212;&#23376;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2307.05857</link><description>&lt;p&gt;
FAIRO: &#38754;&#21521;&#20154;&#26426;&#20132;&#20114;&#31995;&#32479;&#20013;&#39034;&#24207;&#20915;&#31574;&#30340;&#20844;&#24179;&#36866;&#24212;&#24615;
&lt;/p&gt;
&lt;p&gt;
FAIRO: Fairness-aware Adaptation in Sequential-Decision Making for Human-in-the-Loop Systems. (arXiv:2307.05857v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05857
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FAIRO&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#20154;&#26426;&#20132;&#20114;&#29615;&#22659;&#20013;&#23454;&#29616;&#20844;&#24179;&#30340;&#39034;&#24207;&#20915;&#31574;&#12290;&#35813;&#31639;&#27861;&#23558;&#20154;&#30340;&#34892;&#20026;&#21487;&#21464;&#24615;&#21644;&#20559;&#22909;&#21464;&#21270;&#32771;&#34385;&#22312;&#20869;&#65292;&#36890;&#36807;&#21033;&#29992;&#36873;&#39033;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#23558;&#22797;&#26434;&#30340;&#20844;&#24179;&#20219;&#21153;&#20998;&#35299;&#20026;&#33258;&#36866;&#24212;&#23376;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#26426;&#20132;&#20114;&#29615;&#22659;&#20013;&#23454;&#29616;&#39034;&#24207;&#20915;&#31574;&#31995;&#32479;&#30340;&#20844;&#24179;&#24615;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#24403;&#22810;&#20010;&#20855;&#26377;&#19981;&#21516;&#34892;&#20026;&#21644;&#26399;&#26395;&#30340;&#20154;&#21463;&#21040;&#31995;&#32479;&#20013;&#30456;&#21516;&#36866;&#24212;&#20915;&#31574;&#30340;&#24433;&#21709;&#26102;&#12290;&#20154;&#30340;&#21487;&#21464;&#24615;&#22240;&#32032;&#22686;&#21152;&#20102;&#22797;&#26434;&#24615;&#65292;&#22240;&#20026;&#22312;&#26576;&#19968;&#26102;&#38388;&#28857;&#34987;&#35748;&#20026;&#26159;&#20844;&#24179;&#30340;&#25919;&#31574;&#21487;&#33021;&#20250;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#30001;&#20110;&#20154;&#30340;&#20559;&#22909;&#21464;&#21270;&#32780;&#25104;&#20026;&#27495;&#35270;&#24615;&#25919;&#31574;&#12290;&#26412;&#25991;&#20174;&#20844;&#24179;&#24615;&#35270;&#35282;&#32771;&#34385;&#20154;&#30340;&#34892;&#20026;&#21487;&#21464;&#24615;&#21644;&#20154;&#30340;&#20559;&#22909;&#21464;&#21270;&#65292;&#35299;&#20915;&#20102;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;FAIRO&#65292;&#29992;&#20110;&#38754;&#21521;&#20154;&#26426;&#20132;&#20114;&#36866;&#24212;&#30340;&#20844;&#24179;&#39034;&#24207;&#20915;&#31574;&#65292;&#23427;&#23558;&#36825;&#20123;&#27010;&#24565;&#32435;&#20837;&#20915;&#31574;&#36807;&#31243;&#20013;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;FAIRO&#23558;&#36825;&#20010;&#22797;&#26434;&#30340;&#20844;&#24179;&#20219;&#21153;&#22522;&#20110;&#20010;&#20307;&#20154;&#30340;&#20559;&#22909;&#36890;&#36807;&#21033;&#29992;&#36873;&#39033;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#20998;&#35299;&#20026;&#33258;&#36866;&#24212;&#23376;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Achieving fairness in sequential-decision making systems within Human-in-the-Loop (HITL) environments is a critical concern, especially when multiple humans with different behavior and expectations are affected by the same adaptation decisions in the system. This human variability factor adds more complexity since policies deemed fair at one point in time may become discriminatory over time due to variations in human preferences resulting from inter- and intra-human variability. This paper addresses the fairness problem from an equity lens, considering human behavior variability, and the changes in human preferences over time. We propose FAIRO, a novel algorithm for fairness-aware sequential-decision making in HITL adaptation, which incorporates these notions into the decision-making process. In particular, FAIRO decomposes this complex fairness task into adaptive sub-tasks based on individual human preferences through leveraging the Options reinforcement learning framework. We design 
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#20351;&#29992;&#32479;&#35745;&#29289;&#29702;&#23398;&#30340;&#27010;&#24565;&#65292;&#30740;&#31350;&#20102;&#26102;&#38388;&#24046;&#20998;&#23398;&#20064;&#22312;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#22120;&#19979;&#30340;&#20856;&#22411;&#23398;&#20064;&#26354;&#32447;&#12290;&#25105;&#20204;&#21457;&#29616;&#30001;&#20110;&#23376;&#37319;&#26679;&#21487;&#33021;&#30340;&#36712;&#36857;&#31354;&#38388;&#32780;&#20135;&#29983;&#30340;&#38543;&#26426;&#21322;&#26799;&#24230;&#22122;&#22768;&#20250;&#23548;&#33268;&#20540;&#35823;&#24046;&#20986;&#29616;&#26174;&#33879;&#30340;&#24179;&#21488;&#12290;</title><link>http://arxiv.org/abs/2307.04841</link><description>&lt;p&gt;
&#26102;&#38388;&#24046;&#20998;&#24378;&#21270;&#23398;&#20064;&#30340;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Dynamics of Temporal Difference Reinforcement Learning. (arXiv:2307.04841v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04841
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;&#32479;&#35745;&#29289;&#29702;&#23398;&#30340;&#27010;&#24565;&#65292;&#30740;&#31350;&#20102;&#26102;&#38388;&#24046;&#20998;&#23398;&#20064;&#22312;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#22120;&#19979;&#30340;&#20856;&#22411;&#23398;&#20064;&#26354;&#32447;&#12290;&#25105;&#20204;&#21457;&#29616;&#30001;&#20110;&#23376;&#37319;&#26679;&#21487;&#33021;&#30340;&#36712;&#36857;&#31354;&#38388;&#32780;&#20135;&#29983;&#30340;&#38543;&#26426;&#21322;&#26799;&#24230;&#22122;&#22768;&#20250;&#23548;&#33268;&#20540;&#35823;&#24046;&#20986;&#29616;&#26174;&#33879;&#30340;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#38656;&#35201;&#23398;&#20064;&#22312;&#21453;&#39304;&#26377;&#38480;&#30340;&#29615;&#22659;&#20013;&#34892;&#21160;&#30340;&#22810;&#20010;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#26377;&#36825;&#31181;&#32463;&#39564;&#19978;&#30340;&#25104;&#21151;&#65292;&#20173;&#28982;&#27809;&#26377;&#23545;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#30340;&#21442;&#25968;&#21644;&#29992;&#20110;&#34920;&#31034;&#29366;&#24577;&#30340;&#29305;&#24449;&#22914;&#20309;&#30456;&#20114;&#20316;&#29992;&#25511;&#21046;&#23398;&#20064;&#21160;&#24577;&#30340;&#29702;&#35770;&#29702;&#35299;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#32479;&#35745;&#29289;&#29702;&#23398;&#30340;&#27010;&#24565;&#65292;&#30740;&#31350;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#22120;&#19979;&#26102;&#38388;&#24046;&#20998;&#23398;&#20064;&#20215;&#20540;&#20989;&#25968;&#30340;&#20856;&#22411;&#23398;&#20064;&#26354;&#32447;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#26159;&#22312;&#19968;&#20010;&#39640;&#26031;&#31561;&#25928;&#20551;&#35774;&#19979;&#25512;&#23548;&#20986;&#26469;&#30340;&#65292;&#20854;&#20013;&#23545;&#38543;&#26426;&#36712;&#36857;&#30340;&#24179;&#22343;&#20540;&#34987;&#26367;&#25442;&#20026;&#26102;&#24577;&#30456;&#20851;&#30340;&#39640;&#26031;&#29305;&#24449;&#24179;&#22343;&#20540;&#65292;&#24182;&#19988;&#25105;&#20204;&#22312;&#23567;&#35268;&#27169;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#30001;&#20110;&#23545;&#21487;&#33021;&#30340;&#36712;&#36857;&#31354;&#38388;&#36827;&#34892;&#23376;&#37319;&#26679;&#32780;&#20135;&#29983;&#30340;&#38543;&#26426;&#21322;&#26799;&#24230;&#22122;&#22768;&#23548;&#33268;&#20540;&#35823;&#24046;&#20986;&#29616;&#26174;&#33879;&#30340;&#24179;&#21488;&#65292;&#36825;&#19982;&#20256;&#32479;&#30340;&#26799;&#24230;&#19979;&#38477;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning has been successful across several applications in which agents have to learn to act in environments with sparse feedback. However, despite this empirical success there is still a lack of theoretical understanding of how the parameters of reinforcement learning models and the features used to represent states interact to control the dynamics of learning. In this work, we use concepts from statistical physics, to study the typical case learning curves for temporal difference learning of a value function with linear function approximators. Our theory is derived under a Gaussian equivalence hypothesis where averages over the random trajectories are replaced with temporally correlated Gaussian feature averages and we validate our assumptions on small scale Markov Decision Processes. We find that the stochastic semi-gradient noise due to subsampling the space of possible episodes leads to significant plateaus in the value error, unlike in traditional gradient descent 
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36870;&#21521;&#34507;&#30333;&#36136;&#25240;&#21472;&#30340;&#22270;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#25193;&#25955;&#36807;&#31243;&#29983;&#25104;&#20102;&#19968;&#32452;&#22810;&#26679;&#30340;&#20505;&#36873;&#24207;&#21015;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#24456;&#22909;&#22320;&#27010;&#25324;&#22810;&#26679;&#30340;&#21487;&#34892;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2306.16819</link><description>&lt;p&gt;
&#36870;&#21521;&#34507;&#30333;&#36136;&#25240;&#21472;&#30340;&#22270;&#21435;&#22122;&#25193;&#25955;
&lt;/p&gt;
&lt;p&gt;
Graph Denoising Diffusion for Inverse Protein Folding. (arXiv:2306.16819v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16819
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36870;&#21521;&#34507;&#30333;&#36136;&#25240;&#21472;&#30340;&#22270;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#25193;&#25955;&#36807;&#31243;&#29983;&#25104;&#20102;&#19968;&#32452;&#22810;&#26679;&#30340;&#20505;&#36873;&#24207;&#21015;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#24456;&#22909;&#22320;&#27010;&#25324;&#22810;&#26679;&#30340;&#21487;&#34892;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36870;&#21521;&#34507;&#30333;&#36136;&#25240;&#21472;&#20855;&#26377;&#19968;&#23545;&#22810;&#30340;&#26144;&#23556;&#29305;&#24615;&#65292;&#36825;&#20351;&#24471;&#23427;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#35768;&#22810;&#21487;&#33021;&#30340;&#27688;&#22522;&#37240;&#24207;&#21015;&#21487;&#20197;&#25240;&#21472;&#25104;&#19968;&#20010;&#30456;&#21516;&#30340;&#34507;&#30333;&#36136;&#39592;&#26550;&#12290;&#36825;&#20010;&#20219;&#21153;&#19981;&#20165;&#28041;&#21450;&#21040;&#35782;&#21035;&#21487;&#34892;&#30340;&#24207;&#21015;&#65292;&#36824;&#35201;&#34920;&#31034;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#30340;&#22810;&#26679;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#37492;&#21035;&#27169;&#22411;&#65292;&#22914;&#22522;&#20110;Transformer&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#24456;&#38590;&#27010;&#25324;&#21508;&#31181;&#21487;&#34892;&#35299;&#30340;&#22810;&#26679;&#24615;&#12290;&#30456;&#21453;&#65292;&#20316;&#20026;&#19968;&#31181;&#26032;&#20852;&#30340;&#29983;&#25104;&#26041;&#27861;&#65292;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#25552;&#20379;&#20102;&#29983;&#25104;&#19968;&#32452;&#22810;&#26679;&#24207;&#21015;&#20505;&#36873;&#30340;&#28508;&#21147;&#65292;&#29992;&#20110;&#30830;&#23450;&#34507;&#30333;&#36136;&#39592;&#26550;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36870;&#21521;&#34507;&#30333;&#36136;&#25240;&#21472;&#30340;&#22270;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65292;&#20854;&#20013;&#32473;&#23450;&#30340;&#34507;&#30333;&#36136;&#39592;&#26550;&#25351;&#23548;&#23545;&#24212;&#27688;&#22522;&#37240;&#27531;&#22522;&#31867;&#22411;&#30340;&#25193;&#25955;&#36807;&#31243;&#12290;&#35813;&#27169;&#22411;&#25512;&#26029;&#20102;&#20197;&#33410;&#28857;&#30340;&#29289;&#29702;&#21270;&#23398;&#23646;&#24615;&#21644;&#23616;&#37096;&#29615;&#22659;&#20026;&#26465;&#20214;&#30340;&#27688;&#22522;&#37240;&#30340;&#32852;&#21512;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inverse protein folding is challenging due to its inherent one-to-many mapping characteristic, where numerous possible amino acid sequences can fold into a single, identical protein backbone. This task involves not only identifying viable sequences but also representing the sheer diversity of potential solutions. However, existing discriminative models, such as transformer-based auto-regressive models, struggle to encapsulate the diverse range of plausible solutions. In contrast, diffusion probabilistic models, as an emerging genre of generative approaches, offer the potential to generate a diverse set of sequence candidates for determined protein backbones. We propose a novel graph denoising diffusion model for inverse protein folding, where a given protein backbone guides the diffusion process on the corresponding amino acid residue types. The model infers the joint distribution of amino acids conditioned on the nodes' physiochemical properties and local environment. Moreover, we uti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#22240;&#26524;&#26694;&#26550;&#20013;&#30340;&#28508;&#22312;&#32467;&#26524;&#27169;&#22411;(RCM)&#21644;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;(SCM)&#65292;&#24182;&#38416;&#26126;&#20102;RCM&#25104;&#20026;SCM&#21487;&#34920;&#36798;&#30340;&#26465;&#20214;&#65292;&#20197;&#21450;&#27599;&#20010;RCM&#20316;&#20026;&#26576;&#20123;&#21487;&#34920;&#36798;&#30340;RCM&#30340;&#25277;&#35937;&#12290;&#20316;&#32773;&#20171;&#32461;&#20102;SCM&#21407;&#21017;&#22312;RCM&#32463;&#20856;&#24212;&#29992;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#30001;&#22270;&#34920;&#31034;&#30340;&#20195;&#25968;&#32422;&#26463;&#30340;&#29305;&#24449;&#65292;&#26377;&#21161;&#20110;&#36827;&#19968;&#27493;&#27604;&#36739;&#20004;&#20010;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2306.14351</link><description>&lt;p&gt;
&#27604;&#36739;&#22240;&#26524;&#26694;&#26550;&#65306;&#28508;&#22312;&#32467;&#26524;&#12289;&#32467;&#26500;&#27169;&#22411;&#12289;&#22270;&#21644;&#25277;&#35937;
&lt;/p&gt;
&lt;p&gt;
Comparing Causal Frameworks: Potential Outcomes, Structural Models, Graphs, and Abstractions. (arXiv:2306.14351v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14351
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#22240;&#26524;&#26694;&#26550;&#20013;&#30340;&#28508;&#22312;&#32467;&#26524;&#27169;&#22411;(RCM)&#21644;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;(SCM)&#65292;&#24182;&#38416;&#26126;&#20102;RCM&#25104;&#20026;SCM&#21487;&#34920;&#36798;&#30340;&#26465;&#20214;&#65292;&#20197;&#21450;&#27599;&#20010;RCM&#20316;&#20026;&#26576;&#20123;&#21487;&#34920;&#36798;&#30340;RCM&#30340;&#25277;&#35937;&#12290;&#20316;&#32773;&#20171;&#32461;&#20102;SCM&#21407;&#21017;&#22312;RCM&#32463;&#20856;&#24212;&#29992;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#30001;&#22270;&#34920;&#31034;&#30340;&#20195;&#25968;&#32422;&#26463;&#30340;&#29305;&#24449;&#65292;&#26377;&#21161;&#20110;&#36827;&#19968;&#27493;&#27604;&#36739;&#20004;&#20010;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#38416;&#26126;&#28508;&#22312;&#32467;&#26524;&#27169;&#22411;&#65288;RCM&#65289;&#19982;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65288;SCM&#65289;&#26694;&#26550;&#22312;&#22240;&#26524;&#25512;&#26029;&#20013;&#30340;&#20851;&#31995;&#12290;&#37319;&#29992;&#20013;&#31435;&#30340;&#36923;&#36753;&#35270;&#35282;&#65292;&#20511;&#37492;&#20197;&#21069;&#30340;&#30740;&#31350;&#25104;&#26524;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;RCM&#25104;&#20026;SCM&#21487;&#34920;&#36798;&#30340;&#26465;&#20214;&#12290;&#19968;&#20010;&#20851;&#38190;&#32467;&#26524;&#26174;&#31034;&#65292;&#27599;&#20010;RCM -- &#21253;&#25324;&#37027;&#20123;&#36829;&#21453;SCM&#26694;&#26550;&#20013;&#26263;&#31034;&#30340;&#20195;&#25968;&#21407;&#21017;&#30340;RCM -- &#20316;&#20026;&#26576;&#20123;&#21487;&#34920;&#36798;&#30340;RCM&#30340;&#25277;&#35937;&#32780;&#20986;&#29616;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#20934;&#30830;&#23450;&#20301;SCM&#21407;&#21017;&#22312;RCM&#32463;&#20856;&#24212;&#29992;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;&#65292;&#38416;&#26126;&#20102;&#36825;&#31181;&#25913;&#36827;&#24615;&#35270;&#35282;&#30340;&#20248;&#21183;&#65307;&#21453;&#20043;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#30001;&#22270;&#34920;&#31034;&#30340;&#20195;&#25968;&#32422;&#26463;&#30340;&#29305;&#24449;&#65292;&#26377;&#21161;&#20110;&#36827;&#19968;&#27493;&#27604;&#36739;&#20004;&#20010;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
The aim of this paper is to make clear and precise the relationship between the Rubin causal model (RCM) and structural causal model (SCM) frameworks for causal inference. Adopting a neutral logical perspective, and drawing on previous work, we show what is required for an RCM to be representable by an SCM. A key result then shows that every RCM -- including those that violate algebraic principles implied by the SCM framework -- emerges as an abstraction of some representable RCM. Finally, we illustrate the power of this ameliorative perspective by pinpointing an important role for SCM principles in classic applications of RCMs; conversely, we offer a characterization of the algebraic constraints implied by a graph, helping to substantiate further comparisons between the two frameworks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#21442;&#25968;$g_k(G)$&#29992;&#20110;&#26368;&#22823;$k$-plex&#38382;&#39064;&#65292;&#38024;&#23545;&#20854;&#35774;&#35745;&#20102;&#19968;&#20010;&#26681;&#25454;$g_k(G)$&#21442;&#25968;&#21270;&#30340;&#31934;&#30830;&#31639;&#27861;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#65292;&#20294;&#21487;&#20197;&#22312;&#23454;&#38469;&#22270;&#20013;&#24471;&#21040;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2306.13258</link><description>&lt;p&gt;
&#26681;&#25454;&#36864;&#21270;&#38388;&#38553;&#21442;&#25968;&#21270;&#30340;&#24555;&#36895;$k$-Plex&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Fast Maximum $k$-Plex Algorithm Parameterized by the Degeneracy Gap. (arXiv:2306.13258v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13258
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#21442;&#25968;$g_k(G)$&#29992;&#20110;&#26368;&#22823;$k$-plex&#38382;&#39064;&#65292;&#38024;&#23545;&#20854;&#35774;&#35745;&#20102;&#19968;&#20010;&#26681;&#25454;$g_k(G)$&#21442;&#25968;&#21270;&#30340;&#31934;&#30830;&#31639;&#27861;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#65292;&#20294;&#21487;&#20197;&#22312;&#23454;&#38469;&#22270;&#20013;&#24471;&#21040;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32473;&#23450;&#19968;&#20010;&#22270;&#65292;$k$-plex&#26159;&#19968;&#20010;&#39030;&#28857;&#38598;&#65292;&#20854;&#20013;&#27599;&#20010;&#39030;&#28857;&#19982;&#35813;&#38598;&#21512;&#20013;&#26368;&#22810;$k-1$&#20010;&#20854;&#20182;&#39030;&#28857;&#19981;&#30456;&#37051;&#12290;&#26368;&#22823;$k$-plex&#38382;&#39064;&#26159;&#20174;&#32473;&#23450;&#30340;&#22270;&#20013;&#23547;&#25214;&#26368;&#22823;$k$-plex&#65292;&#26159;&#22270;&#25628;&#32034;&#21644;&#31038;&#21306;&#26816;&#27979;&#31561;&#24212;&#29992;&#20013;&#37325;&#35201;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35745;&#31639;&#38382;&#39064;&#12290;&#30446;&#21069;&#65292;&#23384;&#22312;&#35768;&#22810;&#32463;&#39564;&#31639;&#27861;&#65292;&#22312;&#25928;&#29575;&#26041;&#38754;&#27809;&#26377;&#36275;&#22815;&#30340;&#29702;&#35770;&#35299;&#37322;&#12290;&#25105;&#20204;&#36890;&#36807;&#23450;&#20041;&#36755;&#20837;&#23454;&#20363;&#30340;&#19968;&#20010;&#26032;&#21442;&#25968;$g_k(G)$&#65292;&#26368;&#22823;$k$-plex&#30340;&#36864;&#21270;&#36793;&#30028;&#21644;&#22823;&#23567;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26681;&#25454;$g_k(G)$&#21442;&#25968;&#21270;&#30340;&#31934;&#30830;&#31639;&#27861;&#65292;&#26469;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31639;&#27861;&#65292;&#20854;&#36816;&#34892;&#26102;&#38388;&#22810;&#39033;&#24335;&#22797;&#26434;&#24230;&#19982;&#36755;&#20837;&#22270;&#30340;&#22823;&#23567;&#25104;&#27491;&#27604;&#65292;&#25351;&#25968;&#22797;&#26434;&#24230;&#19982;$g_k(G)$&#25104;&#27491;&#27604;&#65292;&#20854;&#20013;$k$&#26159;&#19968;&#20010;&#24120;&#25968;&#12290;&#36890;&#24120;&#65292;&#23454;&#38469;&#22270;&#30340;$g_k(G)$&#24456;&#23567;&#65292;&#34987;$O(\log{(|V|)})$&#38480;&#21046;&#65292;&#36825;&#34920;&#26126;&#35813;&#31639;&#27861;&#30340;&#36816;&#34892;&#26102;&#38388;&#22810;&#39033;&#24335;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given a graph, the $k$-plex is a vertex set in which each vertex is not adjacent to at most $k-1$ other vertices in the set. The maximum $k$-plex problem, which asks for the largest $k$-plex from a given graph, is an important but computationally challenging problem in applications like graph search and community detection. So far, there is a number of empirical algorithms without sufficient theoretical explanations on the efficiency. We try to bridge this gap by defining a novel parameter of the input instance, $g_k(G)$, the gap between the degeneracy bound and the size of maximum $k$-plex in the given graph, and presenting an exact algorithm parameterized by $g_k(G)$. In other words, we design an algorithm with running time polynomial in the size of input graph and exponential in $g_k(G)$ where $k$ is a constant. Usually, $g_k(G)$ is small and bounded by $O(\log{(|V|)})$ in real-world graphs, indicating that the algorithm runs in polynomial time. We also carry out massive experiments
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; MusicGen&#65292;&#19968;&#20010;&#21333;&#19968;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#26465;&#20214;&#25551;&#36848;&#25110;&#26059;&#24459;&#29305;&#24449;&#25511;&#21046;&#19979;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#65292;&#24182;&#19988;&#22312;&#26631;&#20934;&#30340;&#25991;&#26412;&#21040;&#38899;&#20048;&#22522;&#20934;&#19978;&#30340;&#23454;&#35777;&#30740;&#31350;&#20013;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.05284</link><description>&lt;p&gt;
&#31616;&#21333;&#19988;&#21487;&#25511;&#30340;&#38899;&#20048;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Simple and Controllable Music Generation. (arXiv:2306.05284v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05284
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; MusicGen&#65292;&#19968;&#20010;&#21333;&#19968;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#26465;&#20214;&#25551;&#36848;&#25110;&#26059;&#24459;&#29305;&#24449;&#25511;&#21046;&#19979;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#65292;&#24182;&#19988;&#22312;&#26631;&#20934;&#30340;&#25991;&#26412;&#21040;&#38899;&#20048;&#22522;&#20934;&#19978;&#30340;&#23454;&#35777;&#30740;&#31350;&#20013;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#26465;&#20214;&#38899;&#20048;&#29983;&#25104;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;MusicGen&#65292;&#23427;&#26159;&#19968;&#20010;&#21333;&#19968;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#25805;&#20316;&#22810;&#20010;&#21387;&#32553;&#31163;&#25955;&#38899;&#20048;&#34920;&#31034;&#27969;&#65292;&#21363;&#20196;&#29260;&#12290;&#19982;&#20197;&#24448;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;MusicGen&#30001;&#19968;&#20010;&#21333;&#19968;&#38454;&#27573;&#30340;Transformer LM&#21644;&#39640;&#25928;&#30340;&#20196;&#29260;&#20132;&#38169;&#27169;&#24335;&#32452;&#25104;&#65292;&#28040;&#38500;&#20102;&#32423;&#32852;&#22810;&#20010;&#27169;&#22411;&#30340;&#38656;&#35201;&#65292;&#20363;&#22914;&#20998;&#23618;&#25110;&#19978;&#37319;&#26679;&#12290;&#37319;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;MusicGen&#22914;&#20309;&#22312;&#26465;&#20214;&#25551;&#36848;&#25110;&#26059;&#24459;&#29305;&#24449;&#30340;&#25511;&#21046;&#19979;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#35780;&#20272;&#65292;&#32771;&#34385;&#20102;&#33258;&#21160;&#21644;&#20154;&#20026;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#26631;&#20934;&#25991;&#26412;&#21040;&#38899;&#20048;&#22522;&#20934;&#19978;&#35780;&#20272;&#30340;&#22522;&#32447;&#12290;&#36890;&#36807;&#28040;&#34701;&#30740;&#31350;&#65292;&#25105;&#20204;&#38416;&#26126;&#20102;MusicGen&#25152;&#21253;&#21547;&#32452;&#20214;&#30340;&#37325;&#35201;&#24615;&#12290;&#38899;&#20048;&#26679;&#26412;&#12289;&#20195;&#30721;&#21644;&#27169;&#22411;&#21487;&#20197;&#22312;https://github.com/fac&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
We tackle the task of conditional music generation. We introduce MusicGen, a single Language Model (LM) that operates over several streams of compressed discrete music representation, i.e., tokens. Unlike prior work, MusicGen is comprised of a single-stage transformer LM together with efficient token interleaving patterns, which eliminates the need for cascading several models, e.g., hierarchically or upsampling. Following this approach, we demonstrate how MusicGen can generate high-quality samples, while being conditioned on textual description or melodic features, allowing better controls over the generated output. We conduct extensive empirical evaluation, considering both automatic and human studies, showing the proposed approach is superior to the evaluated baselines on a standard text-to-music benchmark. Through ablation studies, we shed light over the importance of each of the components comprising MusicGen. Music samples, code, and models are available at https://github.com/fac
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24378;&#20581;&#30340;&#20559;&#22909;&#35843;&#26597;&#31639;&#27861;&#65292;&#26088;&#22312;&#20248;&#20808;&#20026;COVID-19&#24739;&#32773;&#25552;&#20379;&#21307;&#38498;&#36164;&#28304;&#65292;&#36890;&#36807;&#25307;&#21215;&#20122;&#39532;&#36874;&#26426;&#26800;&#22303;&#32819;&#20854;&#24037;&#20316;&#32773;&#23545;&#35813;&#31639;&#27861;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2306.04061</link><description>&lt;p&gt;
&#37096;&#32626;&#19968;&#20010;&#24378;&#20581;&#30340;&#31215;&#26497;&#20559;&#22909;&#35843;&#26597;&#31639;&#27861;&#65306;COVID-19&#24739;&#32773;&#20248;&#20808;&#25490;&#24207;&#30340;&#23454;&#39564;&#35774;&#35745;&#12289;&#30028;&#38754;&#21644;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Deploying a Robust Active Preference Elicitation Algorithm: Experiment Design, Interface, and Evaluation for COVID-19 Patient Prioritization. (arXiv:2306.04061v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04061
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24378;&#20581;&#30340;&#20559;&#22909;&#35843;&#26597;&#31639;&#27861;&#65292;&#26088;&#22312;&#20248;&#20808;&#20026;COVID-19&#24739;&#32773;&#25552;&#20379;&#21307;&#38498;&#36164;&#28304;&#65292;&#36890;&#36807;&#25307;&#21215;&#20122;&#39532;&#36874;&#26426;&#26800;&#22303;&#32819;&#20854;&#24037;&#20316;&#32773;&#23545;&#35813;&#31639;&#27861;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20559;&#22909;&#35843;&#26597;&#21033;&#29992;AI&#25110;&#20248;&#21270;&#26469;&#23398;&#20064;&#20174;&#24066;&#22330;&#21040;&#20844;&#20849;&#25919;&#31574;&#31561;&#21508;&#31181;&#24773;&#20917;&#19979;&#30340;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#20559;&#22909;&#12290;arXiv:2003.01899&#30340;&#22312;&#32447;&#20581;&#22766;&#20559;&#22909;&#35843;&#26597;&#27969;&#31243;&#22312;&#27169;&#25311;&#20013;&#34920;&#29616;&#20986;&#20248;&#20110;&#20854;&#20182;&#20559;&#22909;&#35843;&#26597;&#27969;&#31243;&#30340;&#25928;&#26524;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#20010;&#20307;&#30340;&#30495;&#23454;&#25928;&#29992;&#12290;&#28982;&#32780;&#65292;&#19982;&#20219;&#20309;&#27169;&#25311;&#19968;&#26679;&#65292;&#35813;&#26041;&#27861;&#20570;&#20986;&#20102;&#19968;&#31995;&#21015;&#26080;&#27861;&#36731;&#26131;&#39564;&#35777;&#26159;&#21542;&#22312;&#27169;&#25311;&#20197;&#22806;&#25104;&#31435;&#30340;&#20551;&#35774;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24314;&#35758;&#39564;&#35777;&#36825;&#31181;&#20581;&#22766;&#26041;&#27861;&#22312;&#37096;&#32626;&#20013;&#30340;&#34920;&#29616;&#65292;&#37325;&#28857;&#20851;&#27880;&#36873;&#25321;COVID-19&#24739;&#32773;&#30340;&#25919;&#31574;&#65292;&#20197;&#20248;&#20808;&#20026;&#30123;&#24773;&#26399;&#38388;&#30340;&#21307;&#38498;&#36164;&#28304;&#25552;&#20379;&#26381;&#21153;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22312;&#32447;&#20559;&#22909;&#35843;&#26597;&#24179;&#21488;&#65292;&#29992;&#25143;&#36890;&#36807;&#29305;&#23450;&#30340;&#20559;&#22909;&#35843;&#26597;&#36807;&#31243;&#25253;&#21578;&#20182;&#20204;&#22312;&#23569;&#37327;&#25104;&#23545;&#27604;&#36739;&#20013;&#30340;&#20559;&#22909;&#12290;&#25105;&#20204;&#25307;&#21215;&#20102;193&#20301;Amazon Mechanical Turk&#24037;&#20316;&#32773;&#26469;&#25253;&#21578;&#20182;&#20204;&#30340;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Preference elicitation leverages AI or optimization to learn stakeholder preferences in settings ranging from marketing to public policy. The online robust preference elicitation procedure of arXiv:2003.01899 has been shown in simulation to outperform various other elicitation procedures in terms of effectively learning individuals' true utilities. However, as with any simulation, the method makes a series of assumptions that cannot easily be verified to hold true beyond simulation. Thus, we propose to validate the robust method's performance in deployment, focused on the particular challenge of selecting policies for prioritizing COVID-19 patients for scarce hospital resources during the pandemic. To this end, we develop an online platform for preference elicitation where users report their preferences between alternatives over a moderate number of pairwise comparisons chosen by a particular elicitation procedure. We recruit Amazon Mechanical Turk workers ($n$ = 193) to report their p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350; Transformer &#27169;&#22411;&#20013;&#20301;&#32622;&#32534;&#30721;&#23545;&#20110;&#38271;&#24230;&#25512;&#24191;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#34920;&#26126;&#24120;&#29992;&#30340;&#20301;&#32622;&#32534;&#30721;&#26041;&#27861;&#24182;&#19981;&#36866;&#21512;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#38271;&#24230;&#25512;&#24191;&#65292;&#24182;&#19988;&#20351;&#29992;&#20301;&#32622;&#32534;&#30721;&#29978;&#33267;&#21487;&#33021;&#20250;&#25439;&#23475;&#38271;&#24230;&#25512;&#24191;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.19466</link><description>&lt;p&gt;
&#20301;&#32622;&#32534;&#30721;&#23545; Transformer &#27169;&#22411;&#38271;&#24230;&#25512;&#24191;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The Impact of Positional Encoding on Length Generalization in Transformers. (arXiv:2305.19466v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19466
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350; Transformer &#27169;&#22411;&#20013;&#20301;&#32622;&#32534;&#30721;&#23545;&#20110;&#38271;&#24230;&#25512;&#24191;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#34920;&#26126;&#24120;&#29992;&#30340;&#20301;&#32622;&#32534;&#30721;&#26041;&#27861;&#24182;&#19981;&#36866;&#21512;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#38271;&#24230;&#25512;&#24191;&#65292;&#24182;&#19988;&#20351;&#29992;&#20301;&#32622;&#32534;&#30721;&#29978;&#33267;&#21487;&#33021;&#20250;&#25439;&#23475;&#38271;&#24230;&#25512;&#24191;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer-based &#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#21457;&#20013;&#65292;&#38271;&#24230;&#25512;&#24191;&#26159;&#19968;&#20010;&#20851;&#38190;&#30340;&#25361;&#25112;&#65292;&#23427;&#26159;&#25351;&#20174;&#23567;&#30340;&#35757;&#32451;&#25991;&#26412;&#33539;&#22260;&#21040;&#26356;&#22823;&#33539;&#22260;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20301;&#32622;&#32534;&#30721;&#65288;PE&#65289;&#34987;&#21457;&#29616;&#26159;&#24433;&#21709;&#38271;&#24230;&#25512;&#24191;&#30340;&#20027;&#35201;&#22240;&#32032;&#20043;&#19968;&#65292;&#20294;&#19981;&#21516;&#30340; PE &#26041;&#26696;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#22806;&#25512;&#24433;&#21709;&#36824;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#27604;&#35780;&#20272;&#20116;&#31181;&#19981;&#21516;&#20301;&#32622;&#32534;&#30721;&#26041;&#27861;&#65288;&#21253;&#25324;&#32477;&#23545;&#20301;&#32622;&#23884;&#20837;&#12289;T5 &#30340;&#30456;&#23545; PE&#12289;ALiBi&#12289;Rotary &#21644;&#26080;&#20301;&#32622;&#32534;&#30721;&#65289;&#30340;&#35299;&#30721;&#22120; Transformer &#30340;&#38271;&#24230;&#25512;&#24191;&#33021;&#21147;&#65292;&#23545;&#25512;&#29702;&#21644;&#25968;&#23398;&#20219;&#21153;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#23454;&#35777;&#30740;&#31350;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#24120;&#29992;&#30340;&#20301;&#32622;&#32534;&#30721;&#26041;&#27861;&#65292;&#22914; ALiBi&#12289;Rotary &#21644; APE&#65292;&#24182;&#19981;&#36866;&#21512;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#38271;&#24230;&#25512;&#24191;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#26080; PE &#30340; Transformer &#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26174;&#24335; PE &#26041;&#27861;&#65292;&#36825;&#24847;&#21619;&#30528;&#20351;&#29992;&#20301;&#32622;&#32534;&#30721;&#23454;&#38469;&#19978;&#21487;&#33021;&#20250;&#25439;&#23475;&#38271;&#24230;&#25512;&#24191;&#30340;&#33021;&#21147;&#12290;&#36825;&#20123;&#21457;&#29616;&#25581;&#31034;&#20102;&#20301;&#32622;&#32534;&#30721;&#22312;&#26377;&#25928; Transformer &#27169;&#22411;&#24320;&#21457;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Length generalization, the ability to generalize from small training context sizes to larger ones, is a critical challenge in the development of Transformer-based language models. Positional encoding (PE) has been identified as a major factor influencing length generalization, but the exact impact of different PE schemes on extrapolation in downstream tasks remains unclear. In this paper, we conduct a systematic empirical study comparing the length generalization performance of decoder-only Transformers with five different position encoding approaches including Absolute Position Embedding (APE), T5's Relative PE, ALiBi, and Rotary, in addition to Transformers without positional encoding (NoPE). Our evaluation encompasses a battery of reasoning and mathematical tasks. Our findings reveal that the most commonly used positional encoding methods, such as ALiBi, Rotary, and APE, are not well suited for length generalization in downstream tasks. More importantly, NoPE outperforms other expli
&lt;/p&gt;</description></item><item><title>CongFu&#26159;&#19968;&#31181;&#29992;&#20110;&#33647;&#29289;&#21327;&#21516;&#39044;&#27979;&#30340;&#26032;&#22411;&#26465;&#20214;&#22270;&#34701;&#21512;&#23618;&#65292;&#37319;&#29992;&#27880;&#24847;&#26426;&#21046;&#21644;&#29942;&#39048;&#25216;&#26415;&#25552;&#21462;&#26412;&#22320;&#22270;&#19978;&#19979;&#25991;&#65292;&#24182;&#22312;&#20840;&#23616;&#19978;&#19979;&#25991;&#20013;&#26377;&#26465;&#20214;&#22320;&#34701;&#21512;&#22270;&#25968;&#25454;&#12290;&#20854;&#27169;&#22359;&#21270;&#26550;&#26500;&#20351;&#24471;&#21487;&#20197;&#26356;&#25442;&#22270;&#23618;&#27169;&#22359;&#65292;&#21253;&#25324;&#35835;&#20986;&#21644;&#22270;&#32534;&#30721;&#22120;&#65292;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#24212;&#29992;&#22330;&#26223;&#65292;&#24182;&#22312;12&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#30340;11&#20010;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;&#20854;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.14517</link><description>&lt;p&gt;
CongFu: &#29992;&#20110;&#33647;&#29289;&#21327;&#21516;&#39044;&#27979;&#30340;&#26465;&#20214;&#22270;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
CongFu: Conditional Graph Fusion for Drug Synergy Prediction. (arXiv:2305.14517v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14517
&lt;/p&gt;
&lt;p&gt;
CongFu&#26159;&#19968;&#31181;&#29992;&#20110;&#33647;&#29289;&#21327;&#21516;&#39044;&#27979;&#30340;&#26032;&#22411;&#26465;&#20214;&#22270;&#34701;&#21512;&#23618;&#65292;&#37319;&#29992;&#27880;&#24847;&#26426;&#21046;&#21644;&#29942;&#39048;&#25216;&#26415;&#25552;&#21462;&#26412;&#22320;&#22270;&#19978;&#19979;&#25991;&#65292;&#24182;&#22312;&#20840;&#23616;&#19978;&#19979;&#25991;&#20013;&#26377;&#26465;&#20214;&#22320;&#34701;&#21512;&#22270;&#25968;&#25454;&#12290;&#20854;&#27169;&#22359;&#21270;&#26550;&#26500;&#20351;&#24471;&#21487;&#20197;&#26356;&#25442;&#22270;&#23618;&#27169;&#22359;&#65292;&#21253;&#25324;&#35835;&#20986;&#21644;&#22270;&#32534;&#30721;&#22120;&#65292;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#24212;&#29992;&#22330;&#26223;&#65292;&#24182;&#22312;12&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#30340;11&#20010;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;&#20854;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33647;&#29289;&#21327;&#21516;&#26159;&#25351;&#22810;&#31181;&#33647;&#29289;&#32852;&#21512;&#20316;&#29992;&#25152;&#20135;&#29983;&#30340;&#21512;&#25104;&#25928;&#24212;&#65292;&#23545;&#20110;&#20248;&#21270;&#27835;&#30103;&#32467;&#26524;&#38750;&#24120;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#21487;&#33021;&#30340;&#33647;&#29289;&#32452;&#21512;&#25968;&#37327;&#24040;&#22823;&#65292;&#35745;&#31639;&#25104;&#26412;&#39640;&#65292;&#23548;&#33268;&#33647;&#29289;&#21327;&#21516;&#25968;&#25454;&#26377;&#38480;&#65292;&#38656;&#35201;&#39044;&#27979;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26465;&#20214;&#22270;&#34701;&#21512;&#23618;&#8212;&#8212;CongFu&#65292;&#29992;&#20110;&#39044;&#27979;&#33647;&#29289;&#21327;&#21516;&#20316;&#29992;&#12290;CongFu&#37319;&#29992;&#27880;&#24847;&#26426;&#21046;&#21644;&#29942;&#39048;&#25216;&#26415;&#26469;&#25552;&#21462;&#26412;&#22320;&#22270;&#19978;&#19979;&#25991;&#24182;&#22312;&#20840;&#23616;&#19978;&#19979;&#25991;&#20013;&#26377;&#26465;&#20214;&#22320;&#34701;&#21512;&#22270;&#25968;&#25454;&#12290;&#20854;&#27169;&#22359;&#21270;&#26550;&#26500;&#20351;&#24471;&#21487;&#20197;&#26356;&#25442;&#22270;&#23618;&#27169;&#22359;&#65292;&#21253;&#25324;&#35835;&#20986;&#21644;&#22270;&#32534;&#30721;&#22120;&#65292;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#24212;&#29992;&#22330;&#26223;&#12290;&#20026;&#20102;&#35780;&#20272;CongFu&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#23545;&#22235;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#28085;&#30422;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#33647;&#29289;&#21327;&#21516;&#39044;&#27979;&#35774;&#32622;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;CongFu&#22312;12&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#30340;11&#20010;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;&#20854;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Drug synergy, characterized by the amplified combined effect of multiple drugs, presents a critical phenomenon for optimizing therapeutic outcomes. However, limited data on drug synergy, arising from the vast number of possible drug combinations and computational costs, motivate the need for predictive methods. In this work, we introduce CongFu, a novel Conditional Graph Fusion Layer, designed to predict drug synergy. CongFu employs an attention mechanism and a bottleneck to extract local graph contexts and conditionally fuse graph data within a global context. Its modular architecture enables flexible replacement of layer modules, including readouts and graph encoders, facilitating customization for diverse applications. To evaluate the performance of CongFu, we conduct comprehensive experiments on four datasets, encompassing three distinct setups for drug synergy prediction. Remarkably, CongFu achieves state-of-the-art results on 11 out of 12 benchmark datasets, demonstrating its abi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#21160;&#24577;&#35748;&#30693;&#36923;&#36753;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#20013;&#25506;&#35752;&#20102;&#29702;&#35299;&#24515;&#26234;&#29702;&#35770;&#30340;&#26041;&#27861;&#12290;&#34429;&#28982;GPT-4&#34920;&#29616;&#20986;&#25913;&#36827;&#30340;&#33021;&#21147;&#65292;&#20294;&#38656;&#35201;&#36827;&#19968;&#27493;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2305.03353</link><description>&lt;p&gt;
MindGames&#65306;&#21033;&#29992;&#21160;&#24577;&#35748;&#30693;&#27169;&#24577;&#36923;&#36753;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#38024;&#23545;&#24515;&#26234;&#29702;&#35770;&#36827;&#34892;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
MindGames: Targeting Theory of Mind in Large Language Models with Dynamic Epistemic Modal Logic. (arXiv:2305.03353v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03353
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#21160;&#24577;&#35748;&#30693;&#36923;&#36753;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#20013;&#25506;&#35752;&#20102;&#29702;&#35299;&#24515;&#26234;&#29702;&#35770;&#30340;&#26041;&#27861;&#12290;&#34429;&#28982;GPT-4&#34920;&#29616;&#20986;&#25913;&#36827;&#30340;&#33021;&#21147;&#65292;&#20294;&#38656;&#35201;&#36827;&#19968;&#27493;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#26234;&#29702;&#35770;(ToM)&#26159;&#26234;&#33021;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#20294;&#20934;&#30830;&#24230;&#37327;&#23427;&#20173;&#28982;&#26159;&#19968;&#20010;&#20105;&#35758;&#35805;&#39064;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#23581;&#35797;&#23558;&#20154;&#31867;ToM&#35780;&#20272;&#24212;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#65292;&#20351;&#29992;&#20154;&#31867;&#21019;&#24314;&#30340;&#26631;&#20934;&#21270;&#27979;&#35797;&#25110;&#22522;&#20110;&#35268;&#21017;&#30340;&#27169;&#26495;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#31616;&#21333;&#30340;&#25512;&#29702;&#19978;&#65292;&#24182;&#38656;&#35201;&#36827;&#19968;&#27493;&#39564;&#35777;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#20855;&#26377;&#19982;ToM&#37325;&#21472;&#30340;&#21160;&#24577;&#35748;&#30693;&#36923;&#36753;&#26469;&#29983;&#25104;&#26356;&#22797;&#26434;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#26032;&#30340;&#35821;&#35328;&#25216;&#24039;&#26469;&#29992;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#29305;&#23450;&#30340;&#35821;&#35328;&#27169;&#22411;&#32553;&#25918;&#65288;&#20174;70M&#21040;6B&#21644;350M&#21040;174B&#65289;&#24182;&#19981;&#19968;&#33268;&#22320;&#20135;&#29983;&#27604;&#38543;&#26426;&#32467;&#26524;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#34429;&#28982;GPT-4&#23637;&#31034;&#20102;&#25913;&#36827;&#30340;&#35748;&#30693;&#25512;&#29702;&#33021;&#21147;&#65292;&#20294;&#20173;&#26377;&#25552;&#21319;&#31354;&#38388;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#21487;&#22312;&#20197;&#19979;&#38142;&#25509;&#20844;&#24320;&#33719;&#21462;&#65306;https://github.com/antoinelrnld/modlog https://huggingface.co/datasets/sileo
&lt;/p&gt;
&lt;p&gt;
Theory of Mind (ToM) is a critical component of intelligence, yet accurately measuring it continues to be a subject of debate. Prior research has attempted to apply human ToM assessments to natural language processing models using either human-created standardized tests or rule-based templates. However, these methods primarily focus on simplistic reasoning and require further validation. In this study, we utilize dynamic epistemic logic, which has established overlaps with ToM, to generate more intricate problems. We also introduce novel verbalization techniques to express these problems using natural language. Our findings indicate that certain language model scaling (from 70M to 6B and 350M to 174B) does not consistently yield results better than random chance. While GPT-4 demonstrates improved epistemic reasoning capabilities, there is still room for enhancement. Our code and datasets are publicly available https://github.com/antoinelrnld/modlog https://huggingface.co/datasets/sileo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;WiFi&#20449;&#21495;&#36827;&#34892;&#20154;&#21592;&#23384;&#22312;&#26816;&#27979;&#30340;&#26032;&#31995;&#32479;&#65292;&#37319;&#29992;&#20102;&#20851;&#27880;&#26426;&#21046;&#21644;&#21452;&#21521;LSTM&#32593;&#32476;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#31283;&#20581;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.13105</link><description>&lt;p&gt;
&#21033;&#29992;&#23460;&#20869;WiFi&#31995;&#32479;&#36827;&#34892;&#26080;&#35774;&#22791;&#31359;&#22681;&#23384;&#22312;&#26816;&#27979;&#30340;&#27880;&#24847;&#22686;&#24378;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Attention-Enhanced Deep Learning for Device-Free Through-the-Wall Presence Detection Using Indoor WiFi System. (arXiv:2304.13105v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13105
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;WiFi&#20449;&#21495;&#36827;&#34892;&#20154;&#21592;&#23384;&#22312;&#26816;&#27979;&#30340;&#26032;&#31995;&#32479;&#65292;&#37319;&#29992;&#20102;&#20851;&#27880;&#26426;&#21046;&#21644;&#21452;&#21521;LSTM&#32593;&#32476;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23460;&#20869;&#29615;&#22659;&#20013;&#20934;&#30830;&#26816;&#27979;&#20154;&#21592;&#23384;&#22312;&#23545;&#20110;&#21508;&#31181;&#24212;&#29992;&#38750;&#24120;&#37325;&#35201;&#65292;&#20363;&#22914;&#33021;&#28304;&#31649;&#29702;&#21644;&#23433;&#20840;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;WiFi&#20449;&#21495;&#30340;&#36890;&#36947;&#29366;&#24577;&#20449;&#24687;&#65288;CSI&#65289;&#36827;&#34892;&#20154;&#21592;&#23384;&#22312;&#26816;&#27979;&#30340;&#26032;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#21517;&#20026;&#27880;&#24847;&#21147;&#22686;&#24378;&#28145;&#24230;&#23398;&#20064;&#65288;ALPD&#65289;&#65292;&#37319;&#29992;&#20851;&#27880;&#26426;&#21046;&#20174;CSI&#25968;&#25454;&#20013;&#33258;&#21160;&#36873;&#25321;&#26377;&#20449;&#24687;&#37327;&#30340;&#23376;&#36733;&#27874;&#65292;&#24182;&#37319;&#29992;&#21452;&#21521;&#38271;&#30701;&#26102;&#35760;&#24518;&#65288;LSTM&#65289;&#32593;&#32476;&#25429;&#25417;CSI&#20013;&#30340;&#26102;&#38388;&#20381;&#36182;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#20010;&#38745;&#24577;&#29305;&#24449;&#26469;&#25552;&#39640;&#38745;&#24577;&#29366;&#24577;&#19979;&#20154;&#21592;&#23384;&#22312;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#37096;&#32626;&#19968;&#23545;WiFi&#25509;&#20837;&#28857;&#65288;AP&#65289;&#26469;&#25910;&#38598;CSI&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#25152;&#25552;&#20986;&#30340;ALPD&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#36827;&#19968;&#27493;&#19982;&#20960;&#20010;&#22522;&#20934;&#36827;&#34892;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;ALPD&#31995;&#32479;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#22522;&#20934;&#65292;&#29305;&#21035;&#26159;&#22312;&#23384;&#22312;&#24178;&#25200;&#30340;&#24773;&#20917;&#19979;&#12290;&#27492;&#22806;&#65292;&#21452;&#21521;&#20256;&#36755;&#25968;&#25454;&#19981;&#20250;&#24433;&#21709;&#25105;&#20204;&#31995;&#32479;&#30340;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate detection of human presence in indoor environments is important for various applications, such as energy management and security. In this paper, we propose a novel system for human presence detection using the channel state information (CSI) of WiFi signals. Our system named attention-enhanced deep learning for presence detection (ALPD) employs an attention mechanism to automatically select informative subcarriers from the CSI data and a bidirectional long short-term memory (LSTM) network to capture temporal dependencies in CSI. Additionally, we utilize a static feature to improve the accuracy of human presence detection in static states. We evaluate the proposed ALPD system by deploying a pair of WiFi access points (APs) for collecting CSI dataset, which is further compared with several benchmarks. The results demonstrate that our ALPD system outperforms the benchmarks in terms of accuracy, especially in the presence of interference. Moreover, bidirectional transmission data 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31354;&#38388;-&#35821;&#35328;&#27880;&#24847;&#21147;&#31574;&#30053;(SLAP)&#65292;&#20351;&#29992;&#19977;&#32500;&#26631;&#35760;&#20316;&#20026;&#36755;&#20837;&#34920;&#31034;&#65292;&#20197;&#35757;&#32451;&#21333;&#19968;&#22810;&#20219;&#21153;&#21644;&#35821;&#35328;&#26465;&#20214;&#21270;&#30340;&#21160;&#20316;&#39044;&#27979;&#31574;&#30053;&#65292;&#33021;&#22815;&#22312;&#24341;&#20837;&#26410;&#35265;&#36807;&#30340;&#24178;&#25200;&#29289;&#21644;&#29289;&#20307;&#37197;&#32622;&#26102;&#36798;&#21040;47.5%&#30340;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.11235</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#26426;&#22120;&#20154;&#23398;&#20064;&#20013;&#30340;&#31354;&#38388;-&#35821;&#35328;&#27880;&#24847;&#21147;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Spatial-Language Attention Policies for Efficient Robot Learning. (arXiv:2304.11235v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11235
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31354;&#38388;-&#35821;&#35328;&#27880;&#24847;&#21147;&#31574;&#30053;(SLAP)&#65292;&#20351;&#29992;&#19977;&#32500;&#26631;&#35760;&#20316;&#20026;&#36755;&#20837;&#34920;&#31034;&#65292;&#20197;&#35757;&#32451;&#21333;&#19968;&#22810;&#20219;&#21153;&#21644;&#35821;&#35328;&#26465;&#20214;&#21270;&#30340;&#21160;&#20316;&#39044;&#27979;&#31574;&#30053;&#65292;&#33021;&#22815;&#22312;&#24341;&#20837;&#26410;&#35265;&#36807;&#30340;&#24178;&#25200;&#29289;&#21644;&#29289;&#20307;&#37197;&#32622;&#26102;&#36798;&#21040;47.5%&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;Transformer&#24314;&#31435;&#21644;&#35757;&#32451;&#26426;&#22120;&#20154;&#20915;&#31574;&#21046;&#23450;&#30340;&#31354;&#38388;&#34920;&#31034;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#20026;&#20102;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#22312;&#21508;&#31181;&#29615;&#22659;&#20013;&#36816;&#34892;&#65292;&#25105;&#20204;&#24517;&#39035;&#33021;&#22815;&#24555;&#36895;&#35757;&#32451;&#25110;&#24494;&#35843;&#26426;&#22120;&#20154;&#24863;&#30693;&#21644;&#21160;&#20316;&#31574;&#30053;&#65292;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#24773;&#20917;&#24182;&#20855;&#26377;&#25968;&#25454;&#25928;&#29575;&#21644;&#40065;&#26834;&#24615;&#12290;&#20316;&#20026;&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31354;&#38388;-&#35821;&#35328;&#27880;&#24847;&#21147;&#31574;&#30053;&#65288;SLAP&#65289;&#12290;SLAP&#20351;&#29992;&#19977;&#32500;&#26631;&#35760;&#20316;&#20026;&#36755;&#20837;&#34920;&#31034;&#65292;&#20197;&#35757;&#32451;&#21333;&#19968;&#22810;&#20219;&#21153;&#12289;&#35821;&#35328;&#26465;&#20214;&#21270;&#30340;&#21160;&#20316;&#39044;&#27979;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#23637;&#31034;&#20102;80%&#30340;&#25104;&#21151;&#29575;&#65292;&#36328;&#36234;&#20102;8&#39033;&#20219;&#21153;&#24182;&#20165;&#20351;&#29992;&#21333;&#19968;&#27169;&#22411;&#65292;&#22312;&#24341;&#20837;&#26410;&#35265;&#36807;&#30340;&#24178;&#25200;&#29289;&#21644;&#29289;&#20307;&#37197;&#32622;&#26102;&#20173;&#20445;&#25345;&#20102;47.5%&#30340;&#25104;&#21151;&#29575;&#65292;&#21363;&#20351;&#27599;&#20010;&#20219;&#21153;&#20165;&#20351;&#29992;&#23569;&#25968;&#31034;&#20363;&#12290;&#30456;&#23545;&#20110;&#20808;&#21069;&#30340;&#24037;&#20316;&#65288;&#20165;&#20351;&#29992;&#26410;&#35265;&#24178;&#25200;&#29289;&#21644;&#37197;&#32622;&#30340;&#24773;&#20917;&#19979;&#65292;&#25104;&#21151;&#29575;&#20026;20%&#65289;&#65292;&#36825;&#34920;&#31034;&#20102;30%&#30340;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate how to build and train spatial representations for robot decision making with Transformers. In particular, for robots to operate in a range of environments, we must be able to quickly train or fine-tune robot sensorimotor policies that are robust to clutter, data efficient, and generalize well to different circumstances. As a solution, we propose Spatial Language Attention Policies (SLAP). SLAP uses three-dimensional tokens as the input representation to train a single multi-task, language-conditioned action prediction policy. Our method shows 80% success rate in the real world across eight tasks with a single model, and a 47.5% success rate when unseen clutter and unseen object configurations are introduced, even with only a handful of examples per task. This represents an improvement of 30% over prior work (20% given unseen distractors and configurations).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;--&#22810;&#23610;&#24230;&#36827;&#21270;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65292;&#29992;&#20110;&#33258;&#21160;&#35774;&#35745;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65292;&#21516;&#26102;&#32771;&#34385;&#24494;&#35266;&#12289;&#20013;&#35266;&#21644;&#23439;&#35266;&#23610;&#24230;&#30340;&#33041;&#25299;&#25169;&#32467;&#26500;&#12290;MSE-NAS&#21487;&#20197;&#24110;&#21161;SNN&#23454;&#29616;&#22810;&#30005;&#36335;&#27169;&#24335;&#30340;&#33258;&#32452;&#32455;&#38598;&#25104;&#65292;&#24182;&#36890;&#36807;&#20840;&#23616;&#24615;&#30340;&#36328;&#27169;&#24335;&#36830;&#25509;&#26469;&#20248;&#21270;&#32593;&#32476;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.10749</link><description>&lt;p&gt;
&#22810;&#23610;&#24230;&#36827;&#21270;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#29992;&#20110;&#28145;&#24230;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Multi-scale Evolutionary Neural Architecture Search for Deep Spiking Neural Networks. (arXiv:2304.10749v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10749
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;--&#22810;&#23610;&#24230;&#36827;&#21270;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65292;&#29992;&#20110;&#33258;&#21160;&#35774;&#35745;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65292;&#21516;&#26102;&#32771;&#34385;&#24494;&#35266;&#12289;&#20013;&#35266;&#21644;&#23439;&#35266;&#23610;&#24230;&#30340;&#33041;&#25299;&#25169;&#32467;&#26500;&#12290;MSE-NAS&#21487;&#20197;&#24110;&#21161;SNN&#23454;&#29616;&#22810;&#30005;&#36335;&#27169;&#24335;&#30340;&#33258;&#32452;&#32455;&#38598;&#25104;&#65292;&#24182;&#36890;&#36807;&#20840;&#23616;&#24615;&#30340;&#36328;&#27169;&#24335;&#36830;&#25509;&#26469;&#20248;&#21270;&#32593;&#32476;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#19981;&#20165;&#22240;&#20854;&#31163;&#25955;&#20449;&#21495;&#22788;&#29702;&#30340;&#33021;&#28304;&#25928;&#29575;&#21331;&#36234;&#65292;&#32780;&#19988;&#22240;&#20854;&#22825;&#28982;&#36866;&#21512;&#20110;&#38598;&#25104;&#22810;&#23610;&#24230;&#29983;&#29289;&#21487;&#22609;&#24615;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;SNN&#30452;&#25509;&#37319;&#29992;&#25104;&#29087;&#30340;DNN&#32467;&#26500;&#65292;&#24456;&#23569;&#33258;&#21160;&#35774;&#35745;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#29992;&#20110;SNN&#12290;&#20154;&#31867;&#22823;&#33041;&#31070;&#32463;&#27169;&#24335;&#30340;&#25299;&#25169;&#32467;&#26500;&#65292;&#27169;&#22359;&#21270;&#30340;&#21306;&#22495;&#32467;&#26500;&#21644;&#20840;&#23616;&#24615;&#30340;&#36328;&#33041;&#21306;&#36830;&#25509;&#26159;&#33258;&#28982;&#36827;&#21270;&#30340;&#20135;&#29289;&#65292;&#21487;&#20197;&#20316;&#20026;&#35774;&#35745;&#22522;&#20110;&#33041;&#30340;SNN&#26550;&#26500;&#30340;&#23436;&#32654;&#21442;&#32771;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#36827;&#21270;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;MSE-NAS&#65289;&#65292;&#21516;&#26102;&#32771;&#34385;&#24494;&#35266;&#12289;&#20013;&#35266;&#21644;&#23439;&#35266;&#23610;&#24230;&#30340;&#33041;&#25299;&#25169;&#20316;&#20026;&#36827;&#21270;&#25628;&#32034;&#31354;&#38388;&#12290; MSE-NAS&#36890;&#36807;&#22522;&#20110;&#22823;&#33041;&#21551;&#21457;&#30340;&#38388;&#25509;&#26041;&#24335;&#65292;&#36827;&#21270;&#21333;&#20010;&#31070;&#32463;&#20803;&#25805;&#20316;&#65292;&#22810;&#20010;&#30005;&#36335;&#27169;&#24335;&#30340;&#33258;&#32452;&#32455;&#38598;&#25104;&#20197;&#21450;&#36328;&#27169;&#24335;&#30340;&#20840;&#23616;&#36830;&#36890;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spiking Neural Networks (SNNs) have received considerable attention not only for their superiority in energy efficient with discrete signal processing, but also for their natural suitability to integrate multi-scale biological plasticity. However, most SNNs directly adopt the structure of the well-established DNN, rarely automatically design Neural Architecture Search (NAS) for SNNs. The neural motifs topology, modular regional structure and global cross-brain region connection of the human brain are the product of natural evolution and can serve as a perfect reference for designing brain-inspired SNN architecture. In this paper, we propose a Multi-Scale Evolutionary Neural Architecture Search (MSE-NAS) for SNN, simultaneously considering micro-, meso- and macro-scale brain topologies as the evolutionary search space. MSE-NAS evolves individual neuron operation, self-organized integration of multiple circuit motifs, and global connectivity across motifs through a brain-inspired indirec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;&#21551;&#21457;&#24335;&#31639;&#27861;&#26500;&#36896;&#35206;&#30422;&#32534;&#30721;&#26144;&#23556;&#65292;&#23558;&#22823;&#22411;&#25628;&#32034;&#31354;&#38388;&#20013;&#30340;&#36807;&#31243;&#26144;&#23556;&#21040;&#21407;&#22987;&#25628;&#32034;&#31354;&#38388;&#30340;&#23376;&#38598;&#20013;&#65292;&#24182;&#20351;&#29992;&#37325;&#25972;&#21270;&#32676;&#30340;&#35270;&#35282;&#26469;&#22788;&#29702;&#31163;&#25955;&#20248;&#21270;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#30456;&#23545;&#20110;&#21333;&#29420;&#30340;&#23616;&#37096;&#25628;&#32034;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.16258</link><description>&lt;p&gt;
&#36890;&#36807;&#32534;&#30721;&#23454;&#29616;&#20248;&#21270;&#65306;&#37325;&#25972;&#21270;&#32676;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Optimisation via encodings: a renormalisation group perspective. (arXiv:2303.16258v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16258
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;&#21551;&#21457;&#24335;&#31639;&#27861;&#26500;&#36896;&#35206;&#30422;&#32534;&#30721;&#26144;&#23556;&#65292;&#23558;&#22823;&#22411;&#25628;&#32034;&#31354;&#38388;&#20013;&#30340;&#36807;&#31243;&#26144;&#23556;&#21040;&#21407;&#22987;&#25628;&#32034;&#31354;&#38388;&#30340;&#23376;&#38598;&#20013;&#65292;&#24182;&#20351;&#29992;&#37325;&#25972;&#21270;&#32676;&#30340;&#35270;&#35282;&#26469;&#22788;&#29702;&#31163;&#25955;&#20248;&#21270;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#30456;&#23545;&#20110;&#21333;&#29420;&#30340;&#23616;&#37096;&#25628;&#32034;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#31163;&#25955;&#20248;&#21270;&#38382;&#39064;&#30340;&#20256;&#32479;&#26041;&#27861;&#26159;&#20351;&#29992;&#36866;&#24403;&#23450;&#20041;&#30340;&#25104;&#26412;&#25110;&#36866;&#24212;&#24615;&#26223;&#35266;&#19978;&#30340;&#23616;&#37096;&#25628;&#32034;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#21463;&#21040;&#20856;&#22411;&#23822;&#23702;&#26223;&#35266;&#20013;&#30340;&#23616;&#37096;&#26368;&#23567;&#20540;&#21046;&#32422;&#65292;&#20196;&#25628;&#32034;&#36807;&#31243;&#21464;&#24930;&#12290;&#21478;&#19968;&#31181;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#30340;&#26041;&#27861;&#26159;&#36890;&#36807;&#20351;&#29992;&#21551;&#21457;&#24335;&#36924;&#36817;&#20272;&#35745;&#20840;&#23616;&#26368;&#23567;&#20540;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#36825;&#20004;&#31181;&#26041;&#27861;&#30340;&#32452;&#21512;&#65292;&#20351;&#29992;&#35206;&#30422;&#32534;&#30721;&#26144;&#23556;&#23558;&#22823;&#22411;&#25628;&#32034;&#31354;&#38388;&#20013;&#30340;&#36807;&#31243;&#26144;&#23556;&#21040;&#21407;&#22987;&#25628;&#32034;&#31354;&#38388;&#30340;&#23376;&#38598;&#20013;&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#21033;&#29992;&#36866;&#24403;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#26500;&#36896;&#35206;&#30422;&#32534;&#30721;&#26144;&#23556;&#65292;&#22312;&#22823;&#22411;&#25628;&#32034;&#31354;&#38388;&#19978;&#29983;&#25104;&#19981;&#20877;&#26174;&#31034;&#38519;&#38449;&#23616;&#37096;&#26368;&#23567;&#20540;&#30340;&#26223;&#35266;&#12290;&#36890;&#24120;&#37319;&#29992;&#30340;&#36807;&#31243;&#28041;&#21450;&#26576;&#31181;&#24418;&#24335;&#30340;&#31895;&#31890;&#21270;&#65292;&#25105;&#20204;&#35748;&#20026;&#23427;&#20204;&#21487;&#20197;&#34987;&#35270;&#20026;&#32479;&#35745;&#21147;&#23398;&#20013;&#37325;&#25972;&#21270;&#32676;&#30340;&#21270;&#36523;&#12290;&#25105;&#20204;&#20351;&#29992;&#32452;&#21512;&#20248;&#21270;&#39046;&#22495;&#30340;&#27979;&#35797;&#38382;&#39064;&#26469;&#35828;&#26126;&#36825;&#31181;&#26041;&#27861;&#65292;&#21457;&#29616;&#35206;&#30422;&#32534;&#30721;&#26144;&#23556;&#19982;&#23616;&#37096;&#25628;&#32034;&#30340;&#32452;&#21512;&#22987;&#32456;&#20248;&#20110;&#21333;&#29420;&#30340;&#23616;&#37096;&#25628;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
The traditional way of tackling discrete optimization problems is by using local search on suitably defined cost or fitness landscapes. Such approaches are however limited by the slowing down that occurs when local minima, that are a feature of the typically rugged landscapes encountered, arrest the progress of the search process. Another way of tackling optimization problems is by the use of heuristic approximations to estimate a global cost minimum. Here we present a combination of these two approaches by using cover-encoding maps which map processes from a larger search space to subsets of the original search space. The key idea is to construct cover-encoding maps with the help of suitable heuristics that single out near-optimal solutions and result in landscapes on the larger search space that no longer exhibit trapping local minima. The processes that are typically employed involve some form of coarse-graining, and we suggest here that they can be viewed as avatars of renormalisat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31995;&#32479;&#65292;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22810;&#27493;&#36923;&#36753;&#25512;&#29702;&#65292;&#37319;&#29992;&#20102;&#26174;&#24335;&#35268;&#21010;&#26469;&#24110;&#21161;&#20570;&#20986;&#26356;&#26126;&#26234;&#30340;&#20915;&#31574;&#65292;&#27604;&#20854;&#20182;&#31454;&#20105;&#31995;&#32479;&#34920;&#29616;&#26356;&#22909;&#65292;&#26174;&#24335;&#35268;&#21010;&#22312;&#31995;&#32479;&#24615;&#33021;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2303.15714</link><description>&lt;p&gt;
&#26174;&#24335;&#35268;&#21010;&#26377;&#21161;&#20110;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#36923;&#36753;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Explicit Planning Helps Language Models in Logical Reasoning. (arXiv:2303.15714v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15714
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31995;&#32479;&#65292;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22810;&#27493;&#36923;&#36753;&#25512;&#29702;&#65292;&#37319;&#29992;&#20102;&#26174;&#24335;&#35268;&#21010;&#26469;&#24110;&#21161;&#20570;&#20986;&#26356;&#26126;&#26234;&#30340;&#20915;&#31574;&#65292;&#27604;&#20854;&#20182;&#31454;&#20105;&#31995;&#32479;&#34920;&#29616;&#26356;&#22909;&#65292;&#26174;&#24335;&#35268;&#21010;&#22312;&#31995;&#32479;&#24615;&#33021;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#31995;&#32479;&#65292;&#37319;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22810;&#27493;&#36923;&#36753;&#25512;&#29702;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#23558;&#26174;&#24335;&#35268;&#21010;&#32435;&#20837;&#21040;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#22240;&#27492;&#21487;&#20197;&#36890;&#36807;&#23637;&#26395;&#26410;&#26469;&#30340;&#25928;&#26524;&#26469;&#20570;&#20986;&#26356;&#26126;&#26234;&#30340;&#20915;&#31574;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#20840;&#22871;&#31995;&#32479;&#22312;&#22810;&#39033;&#36873;&#25321;&#39064;&#31572;&#39064;&#20219;&#21153;&#20013;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#31454;&#20105;&#31995;&#32479;&#65292;&#23613;&#31649;&#21482;&#26377;&#32422;15&#20159;&#20010;&#21442;&#25968;&#65292;&#20294;&#19982;GPT-3-davinci&#34920;&#29616;&#30456;&#24403;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22810;&#20010;&#28040;&#34701;&#30740;&#31350;&#20197;&#35777;&#26126;&#26174;&#24335;&#35268;&#21010;&#22312;&#31995;&#32479;&#24615;&#33021;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models have been shown to perform remarkably well on a wide range of natural language processing tasks. In this paper, we propose a novel system that uses language models to perform multi-step logical reasoning. Our system incorporates explicit planning into its inference procedure, thus able to make more informed reasoning decisions at each step by looking ahead into their future effects. In our experiments, our full system significantly outperforms other competing systems. On a multiple-choice question answering task, our system performs competitively compared to GPT-3-davinci despite having only around 1.5B parameters. We conduct several ablation studies to demonstrate that explicit planning plays a crucial role in the system's performance.
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25506;&#35752;&#20102;AI&#21327;&#21516;&#21512;&#20316;&#30340;&#21382;&#21490;&#21644;&#35201;&#27714;&#65292;&#26159;&#21327;&#21516;AI&#30740;&#31350;&#30340;&#21160;&#26426;&#21644;&#32972;&#26223;&#12290;</title><link>http://arxiv.org/abs/2303.12040</link><description>&lt;p&gt;
&#21327;&#21516;&#20154;&#24037;&#26234;&#33021;&#30340;&#26681;&#28304;&#21644;&#35201;&#27714;
&lt;/p&gt;
&lt;p&gt;
Roots and Requirements for Collaborative AI. (arXiv:2303.12040v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12040
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25506;&#35752;&#20102;AI&#21327;&#21516;&#21512;&#20316;&#30340;&#21382;&#21490;&#21644;&#35201;&#27714;&#65292;&#26159;&#21327;&#21516;AI&#30740;&#31350;&#30340;&#21160;&#26426;&#21644;&#32972;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#21327;&#20316;&#32773;&#30340;&#24895;&#26223;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#26159;&#31185;&#24187;&#23567;&#35828;&#30340;&#32463;&#20856;&#32032;&#26448;&#65292;&#20854;&#20013;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#29702;&#35299;&#21327;&#20316;&#21644;&#20154;&#31867;&#27807;&#36890;&#30340;&#24494;&#22937;&#24046;&#21035;&#12290;&#23427;&#20204;&#36890;&#36807;&#36129;&#29486;&#29305;&#27530;&#30340;&#25165;&#33021;&#32473;&#20182;&#20204;&#30340;&#20154;&#31867;&#21512;&#20316;&#32773;&#21644;&#22242;&#38431;&#24102;&#26469;&#20248;&#21183;&#12290;&#22810;&#24180;&#26469;&#65292;&#25919;&#24220;&#21672;&#35810;&#22242;&#20307;&#21644;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#39046;&#34966;&#19968;&#30452;&#20513;&#23548;AIs&#24212;&#35813;&#20855;&#26377;&#20154;&#31867;&#20860;&#23481;&#24615;&#21644;&#26377;&#25928;&#21327;&#20316;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20855;&#22791;&#20687;&#25165;&#21326;&#27178;&#28322;&#30340;&#20154;&#37027;&#26679;&#21327;&#20316;&#33021;&#21147;&#30340;&#24378;&#22823;&#30340;AI&#20173;&#28982;&#36965;&#19981;&#21487;&#21450;&#12290;&#36825;&#31687;&#35770;&#25991;&#20381;&#25454;&#23545;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#20195;&#29702;&#26377;&#25928;&#21644;&#24378;&#22823;&#21327;&#20316;&#25152;&#38656;&#35748;&#30693;&#30340;&#20998;&#26512;&#65292;&#27010;&#36848;&#20102;&#20844;&#20247;&#21644;AI&#24895;&#26223;&#20013;&#20851;&#20110;&#20154;&#24037;&#21327;&#20316;&#32773;&#30340;&#21382;&#21490;&#65292;&#24320;&#22987;&#20110;&#26089;&#26399;&#26234;&#33021;&#22686;&#24378;(IA)&#21644;&#20154;&#24037;&#26234;&#33021;(AI)&#30340;&#24895;&#26223;&#12290;&#36825;&#31687;&#35770;&#25991;&#26088;&#22312;&#25104;&#20026;&#21327;&#21516;AI&#30340;&#31532;&#20108;&#20010;&#31435;&#22330;&#25991;&#20214;(Stefik &amp; Price, 2023)&#30340;&#21160;&#26426;&#21644;&#32972;&#26223;&#12290;&#31532;&#20108;&#31687;&#35770;&#25991;&#22238;&#39038;&#20102;&#22810;&#23398;&#31185;&#30340;&#29616;&#29366;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;AI&#21327;&#20316;&#30740;&#31350;&#30340;&#36335;&#32447;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
The vision of AI collaborators has long been a staple of science fiction, where artificial agents understand nuances of collaboration and human communication. They bring advantages to their human collaborators and teams by contributing their special talents. Government advisory groups and leaders in AI have advocated for years that AIs should be human compatible and be capable of effective collaboration. Nonetheless, robust AIs that can collaborate like talented people remain out of reach. This position paper draws on a cognitive analysis of what effective and robust collaboration requires of human and artificial agents. It sketches a history of public and AI visions for artificial collaborators, starting with early visions of intelligence augmentation (IA) and artificial intelligence (AI). It is intended as motivation and context for a second position paper on collaborative AI (Stefik &amp; Price, 2023). The second paper reviews the multi-disciplinary state-of-the-art and proposes a roadm
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#19978;&#19979;&#25991;&#22312;&#30446;&#26631;&#26816;&#27979;&#21306;&#22495;-&#35789;&#23545;&#40784;&#20013;&#20316;&#29992;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29305;&#23450;&#30340;&#36127;&#37319;&#26679;&#26041;&#27861;&#25552;&#39640;&#20102;&#23646;&#24615;&#30340;&#20316;&#29992;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#30446;&#26631;&#26816;&#27979;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.10093</link><description>&lt;p&gt;
&#25552;&#39640;&#19978;&#19979;&#25991;&#22312;&#30446;&#26631;&#26816;&#27979;&#21306;&#22495;-&#35789;&#23545;&#40784;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Enhancing the Role of Context in Region-Word Alignment for Object Detection. (arXiv:2303.10093v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10093
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#19978;&#19979;&#25991;&#22312;&#30446;&#26631;&#26816;&#27979;&#21306;&#22495;-&#35789;&#23545;&#40784;&#20013;&#20316;&#29992;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29305;&#23450;&#30340;&#36127;&#37319;&#26679;&#26041;&#27861;&#25552;&#39640;&#20102;&#23646;&#24615;&#30340;&#20316;&#29992;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#30446;&#26631;&#26816;&#27979;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#23398;&#20064;&#22270;&#20687;-&#26631;&#27880;&#37197;&#23545;&#20043;&#38388;&#30340;&#32454;&#31890;&#24230;&#21306;&#22495;-&#35789;&#23545;&#40784;&#65292;&#25512;&#21160;&#20102;&#24320;&#25918;&#35789;&#27719;&#30446;&#26631;&#26816;&#27979;&#30340;&#36827;&#23637;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#21306;&#22495;-&#35789;&#23545;&#40784;&#26041;&#27861;&#36890;&#24120;&#20165;&#38024;&#23545;&#30446;&#26631;&#21517;&#35789;&#22312;&#26816;&#27979;&#20013;&#20351;&#29992;&#65292;&#20854;&#20182;&#19978;&#19979;&#25991;&#65292;&#20363;&#22914;&#23646;&#24615;&#65292;&#23545;&#26816;&#27979;&#30340;&#24433;&#21709;&#19981;&#26126;&#30830;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#35821;&#35328;&#19978;&#19979;&#25991;&#22914;&#20309;&#24433;&#21709;&#19979;&#28216;&#30446;&#26631;&#26816;&#27979;&#65292;&#24182;&#25552;&#35758;&#22686;&#24378;&#19978;&#19979;&#25991;&#30340;&#20316;&#29992;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#31574;&#30053;&#24615;&#22320;&#23558;&#25509;&#22320;&#39044;&#35757;&#32451;&#30446;&#26631;&#24773;&#22659;&#21270;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#23545;&#40784;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#23646;&#24615;&#20316;&#20026;&#29305;&#21035;&#26377;&#29992;&#30340;&#30446;&#26631;&#19978;&#19979;&#25991;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#24418;&#23481;&#35789;&#21644;&#21517;&#35789;&#30340;&#36127;&#37319;&#26679;&#31574;&#30053;&#65292;&#20197;&#22686;&#21152;&#23545;&#23427;&#20204;&#30340;&#23545;&#27604;&#23398;&#20064;&#30340;&#20851;&#27880;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#19982;&#21306;&#22495;-&#35789;&#39044;&#35757;&#32451;&#30340;&#26368;&#26032;&#25216;&#26415;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#21319;&#20102;&#30446;&#26631;&#26816;&#27979;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#25991;&#26412;-&#21306;&#22495;&#21487;&#35270;&#21270;&#26174;&#31034;&#23646;&#24615;&#25935;&#24863;&#27169;&#22411;&#30340;&#32454;&#31890;&#24230;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-language pretraining to learn a fine-grained, region-word alignment between image-caption pairs has propelled progress in open-vocabulary object detection. We observe that region-word alignment methods are typically used in detection with respect to only object nouns, and the impact of other rich context in captions, such as attributes, is unclear. In this study, we explore how language context affects downstream object detection and propose to enhance the role of context. In particular, we show how to strategically contextualize the grounding pretraining objective for improved alignment. We further hone in on attributes as especially useful object context and propose a novel adjective and noun-based negative sampling strategy for increasing their focus in contrastive learning. Overall, our methods enhance object detection when compared to the state-of-the-art in region-word pretraining. We also highlight the fine-grained utility of an attribute-sensitive model through text-regi
&lt;/p&gt;</description></item><item><title>&#22269;&#23478;&#30284;&#30151;&#30740;&#31350;&#25152;&#24433;&#20687;&#25968;&#25454;&#20849;&#20139;&#24179;&#21488; (IDC) &#26088;&#22312;&#20419;&#36827;&#35745;&#31639;&#30149;&#29702;&#23398;&#39046;&#22495;&#30340;&#30740;&#31350;&#21487;&#37325;&#22797;&#24615;&#65292;&#23454;&#29616;&#20102; FAIR &#21407;&#21017;&#65292;&#25552;&#20379;&#20844;&#20849;&#24211;&#21644;&#20113;&#31471;&#25216;&#26415;&#25903;&#25345;&#65292;&#26041;&#20415;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#30284;&#30151;&#32452;&#32455;&#20998;&#31867;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2303.09354</link><description>&lt;p&gt;
&#22269;&#23478;&#30284;&#30151;&#30740;&#31350;&#25152;&#24433;&#20687;&#25968;&#25454;&#20849;&#20139;&#24179;&#21488;&#65306;&#35745;&#31639;&#30149;&#29702;&#23398;&#21487;&#37325;&#22797;&#30740;&#31350;&#30340;&#22522;&#30784;
&lt;/p&gt;
&lt;p&gt;
The NCI Imaging Data Commons as a platform for reproducible research in computational pathology. (arXiv:2303.09354v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09354
&lt;/p&gt;
&lt;p&gt;
&#22269;&#23478;&#30284;&#30151;&#30740;&#31350;&#25152;&#24433;&#20687;&#25968;&#25454;&#20849;&#20139;&#24179;&#21488; (IDC) &#26088;&#22312;&#20419;&#36827;&#35745;&#31639;&#30149;&#29702;&#23398;&#39046;&#22495;&#30340;&#30740;&#31350;&#21487;&#37325;&#22797;&#24615;&#65292;&#23454;&#29616;&#20102; FAIR &#21407;&#21017;&#65292;&#25552;&#20379;&#20844;&#20849;&#24211;&#21644;&#20113;&#31471;&#25216;&#26415;&#25903;&#25345;&#65292;&#26041;&#20415;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#30284;&#30151;&#32452;&#32455;&#20998;&#31867;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;&#21487;&#37325;&#22797;&#24615;&#23545;&#20110;&#23558;&#35745;&#31639;&#30149;&#29702;&#23398;&#65288;CompPath&#65289;&#20013;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#35299;&#20915;&#26041;&#26696;&#36716;&#21270;&#20026;&#23454;&#36341;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#25253;&#21578;&#38590;&#20197;&#37325;&#22797; ML &#32467;&#26524;&#30340;&#22256;&#38590;&#12290;&#22269;&#23478;&#30284;&#30151;&#30740;&#31350;&#25152;&#24433;&#20687;&#25968;&#25454;&#20849;&#20139;&#24179;&#21488;&#65288;IDC&#65289;&#26159;&#19968;&#20010;&#20844;&#20849;&#24211;&#65292;&#21253;&#21547; &gt;120 &#20010;&#30284;&#30151;&#22270;&#20687;&#25910;&#38598;&#65292;&#21253;&#25324; &gt;38,000 &#24352;&#20840;&#20999;&#29255;&#22270;&#20687;&#65288;WSIs&#65289;&#65292;&#26088;&#22312;&#19982;&#20113;&#31471; ML &#26381;&#21153;&#19968;&#36215;&#20351;&#29992;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102; IDC &#20419;&#36827; CompPath &#30740;&#31350;&#21487;&#37325;&#22797;&#24615;&#30340;&#28508;&#21147;&#12290; &#26448;&#26009;&#21644;&#26041;&#27861;&#65306;IDC &#23454;&#29616;&#20102; FAIR &#21407;&#21017;&#65306;&#25152;&#26377;&#22270;&#20687;&#37117;&#26681;&#25454; DICOM &#26631;&#20934;&#36827;&#34892;&#32534;&#30721;&#65292;&#20855;&#26377;&#25345;&#20037;&#21270;&#26631;&#35782;&#31526;&#12289;&#21487;&#36890;&#36807;&#20016;&#23500;&#30340;&#20803;&#25968;&#25454;&#36827;&#34892;&#21457;&#29616;&#65292;&#24182;&#21487;&#36890;&#36807;&#24320;&#25918;&#24335;&#24037;&#20855;&#35775;&#38382;&#12290;&#20511;&#27492;&#20248;&#21183;&#65292;&#25105;&#20204;&#22312; IDC &#30340;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#20004;&#20010;&#23454;&#39564;&#65292;&#38024;&#23545;&#32954;&#30284;&#32452;&#32455;&#20998;&#31867;&#30340;&#19968;&#31181;&#20195;&#34920;&#24615;&#22522;&#20110; ML &#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#35757;&#32451;&#21644;/&#25110;&#35780;&#20272;&#12290;&#20026;&#35780;&#20272;&#21487;&#37325;&#22797;&#24615;&#65292;&#23454;&#39564;&#34987;&#22810;&#27425;&#36816;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Objective: Reproducibility is critical for translating machine learning-based (ML) solutions in computational pathology (CompPath) into practice. However, an increasing number of studies report difficulties in reproducing ML results. The NCI Imaging Data Commons (IDC) is a public repository of &gt;120 cancer image collections, including &gt;38,000 whole-slide images (WSIs), that is designed to be used with cloud-based ML services. Here, we explore the potential of the IDC to facilitate reproducibility of CompPath research.  Materials and Methods: The IDC realizes the FAIR principles: All images are encoded according to the DICOM standard, persistently identified, discoverable via rich metadata, and accessible via open tools. Taking advantage of this, we implemented two experiments in which a representative ML-based method for classifying lung tumor tissue was trained and/or evaluated on different datasets from the IDC. To assess reproducibility, the experiments were run multiple times with i
&lt;/p&gt;</description></item><item><title>DynGFN&#26159;&#19968;&#31181;&#20511;&#21161;RNA&#36895;&#24230;&#25216;&#26415;&#36827;&#34892;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#25429;&#25417;&#32593;&#32476;&#32467;&#26500;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#22312;&#20934;&#30830;&#24230;&#19978;&#36229;&#36807;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.04178</link><description>&lt;p&gt;
DynGFN: &#20511;&#21161;RNA&#36895;&#24230;&#25216;&#26415;&#36827;&#34892;&#36125;&#21494;&#26031;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#25512;&#26029;&#30340;GFlowNets&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DynGFN: Towards Bayesian Inference of Gene Regulatory Networks with GFlowNets. (arXiv:2302.04178v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04178
&lt;/p&gt;
&lt;p&gt;
DynGFN&#26159;&#19968;&#31181;&#20511;&#21161;RNA&#36895;&#24230;&#25216;&#26415;&#36827;&#34892;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#25429;&#25417;&#32593;&#32476;&#32467;&#26500;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#22312;&#20934;&#30830;&#24230;&#19978;&#36229;&#36807;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32454;&#32990;&#29983;&#29289;&#23398;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#26159;&#25512;&#26029;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#65288;GRN&#65289;&#65292;&#35813;&#32593;&#32476;&#25551;&#36848;&#20102;&#25511;&#21046;&#22522;&#22240;&#34920;&#36798;&#21644;&#32454;&#32990;&#21151;&#33021;&#30340;&#22522;&#22240;&#21450;&#20854;&#20135;&#29289;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#26412;&#25991;&#20511;&#21161;RNA&#36895;&#24230;&#25216;&#26415;&#24320;&#21457;&#20102;&#19968;&#31181;&#26041;&#27861;DynGFN&#65292;&#35813;&#26041;&#27861;&#35757;&#32451;&#29983;&#25104;&#27969;&#32593;&#32476;&#65292;&#20351;&#29992;RNA&#36895;&#24230;&#25968;&#25454;&#25191;&#34892;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#65292;&#24182;&#25429;&#25417;&#32593;&#32476;&#32467;&#26500;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the grand challenges of cell biology is inferring the gene regulatory network (GRN) which describes interactions between genes and their products that control gene expression and cellular function. We can treat this as a causal discovery problem but with two non-standard challenges: (1) regulatory networks are inherently cyclic so we should not model a GRN as a directed acyclic graph (DAG), and (2) observations have significant measurement noise, so for typical sample sizes there will always be a large equivalence class of graphs that are likely given the data, and we want methods that capture this uncertainty. Existing methods either focus on challenge (1), identifying cyclic structure from dynamics, or on challenge (2) learning complex Bayesian posteriors over DAGs, but not both. In this paper we leverage the fact that it is possible to estimate the "velocity" of gene expression with RNA velocity techniques to develop an approach that addresses both challenges. Because we have
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#25285;&#20445;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#31526;&#21512;&#33258;&#28982;&#31185;&#23398;&#24050;&#26377;&#30693;&#35782;&#24182;&#26368;&#20248;&#36924;&#36817;&#31995;&#32479;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2212.01346</link><description>&lt;p&gt;
&#31070;&#32463;&#31526;&#21495;&#27169;&#22411;&#30340;&#33258;&#28982;&#32422;&#26463;&#25285;&#20445;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
Guaranteed Conformance of Neurosymbolic Models to Natural Constraints. (arXiv:2212.01346v7 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01346
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#25285;&#20445;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#31526;&#21512;&#33258;&#28982;&#31185;&#23398;&#24050;&#26377;&#30693;&#35782;&#24182;&#26368;&#20248;&#36924;&#36817;&#31995;&#32479;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#25104;&#20026;&#22823;&#37096;&#20998;&#26426;&#22120;&#20154;&#21644;&#25511;&#21046;&#24212;&#29992;&#30340;&#20027;&#35201;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#20316;&#20026;&#21160;&#24577;&#31995;&#32479;&#27169;&#22411;&#12290;&#36825;&#31867;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#21448;&#21487;&#20197;&#29992;&#20110;&#35774;&#35745;&#21644;&#39564;&#35777;&#33258;&#20027;&#31995;&#32479;&#12290;&#22312;&#21307;&#30103;&#31995;&#32479;&#24314;&#27169;&#26041;&#38754;&#23588;&#20854;&#26377;&#29992;&#65292;&#22240;&#20026;&#25968;&#25454;&#33021;&#22815;&#34987;&#29992;&#20110;&#20010;&#20307;&#21270;&#27835;&#30103;&#12290;&#22312;&#23433;&#20840;&#20851;&#38190;&#30340;&#24212;&#29992;&#20013;&#65292;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#23545;&#26469;&#33258;&#33258;&#28982;&#31185;&#23398;&#30340;&#24050;&#26377;&#30693;&#35782;&#30340;&#31526;&#21512;&#24615;&#26174;&#24471;&#23588;&#20026;&#37325;&#35201;&#12290;&#36825;&#20123;&#30693;&#35782;&#36890;&#24120;&#26159;&#21487;&#29992;&#30340;&#65292;&#25110;&#32773;&#21487;&#20197;&#34987;&#27987;&#32553;&#25104;&#65288;&#21487;&#33021;&#26159;&#40657;&#30418;&#30340;&#65289;&#27169;&#22411;&#12290;&#20363;&#22914;&#65292;F1&#36187;&#36710;&#24212;&#31526;&#21512;&#29275;&#39039;&#23450;&#24459;&#65288;&#36825;&#34987;&#32534;&#30721;&#22312;&#19968;&#20010;&#21333;&#36718;&#27169;&#22411;&#20013;&#65289;&#12290;&#37492;&#20110;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#32771;&#34385;&#20197;&#19979;&#38382;&#39064;&#8212;&#8212;&#32473;&#23450;&#19968;&#20010;&#27169;&#22411;M&#21644;&#19968;&#20010;&#29366;&#24577;&#36716;&#31227;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#24076;&#26395;&#22312;&#36317;&#31163;M&#30340;&#33539;&#22260;&#20869;&#26368;&#22909;&#22320;&#36817;&#20284;&#31995;&#32479;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#26469;&#25285;&#20445;&#36825;&#31181;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#27493;&#26159;&#23558;&#25968;&#25454;&#38598;&#27987;&#32553;&#25104;&#20960;&#20010;&#20195;&#34920;&#24615;&#30340;&#26679;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks have emerged as the workhorse for a large section of robotics and control applications, especially as models for dynamical systems. Such data-driven models are in turn used for designing and verifying autonomous systems. They are particularly useful in modeling medical systems where data can be leveraged to individualize treatment. In safety-critical applications, it is important that the data-driven model is conformant to established knowledge from the natural sciences. Such knowledge is often available or can often be distilled into a (possibly black-box) model. For instance, an F1 racing car should conform to Newton's laws (which are encoded within a unicycle model). In this light, we consider the following problem - given a model $M$ and a state transition dataset, we wish to best approximate the system model while being a bounded distance away from $M$. We propose a method to guarantee this conformance. Our first step is to distill the dataset into a few repre
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#36328;&#23398;&#31185;&#30340;&#26041;&#27861;&#26469;&#25506;&#35752;NLP&#27169;&#22411;&#20559;&#35265;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#37319;&#29992;&#24515;&#29702;&#27979;&#37327;&#23398;&#30340;&#35270;&#35282;&#65292;&#29305;&#21035;&#20851;&#27880;&#26500;&#24565;&#25928;&#24230;&#21644;&#27979;&#37327;&#24037;&#20855;&#30340;&#20449;&#24230;&#65292;&#22312;&#34913;&#37327;&#27169;&#22411;&#20559;&#35265;&#30340;&#24773;&#22659;&#20013;&#22914;&#20309;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2211.13709</link><description>&lt;p&gt;
NLP&#20013;&#30340;&#19981;&#33391;&#20559;&#35265;&#65306;&#36991;&#20813;&#34913;&#37327;&#21361;&#26426;
&lt;/p&gt;
&lt;p&gt;
Undesirable biases in NLP: Averting a crisis of measurement. (arXiv:2211.13709v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13709
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#36328;&#23398;&#31185;&#30340;&#26041;&#27861;&#26469;&#25506;&#35752;NLP&#27169;&#22411;&#20559;&#35265;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#37319;&#29992;&#24515;&#29702;&#27979;&#37327;&#23398;&#30340;&#35270;&#35282;&#65292;&#29305;&#21035;&#20851;&#27880;&#26500;&#24565;&#25928;&#24230;&#21644;&#27979;&#37327;&#24037;&#20855;&#30340;&#20449;&#24230;&#65292;&#22312;&#34913;&#37327;&#27169;&#22411;&#20559;&#35265;&#30340;&#24773;&#22659;&#20013;&#22914;&#20309;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#21644;&#26222;&#21450;&#65292;&#39044;&#27979;&#20854;&#20351;&#29992;&#21487;&#33021;&#23545;&#20154;&#20204;&#36896;&#25104;&#20260;&#23475;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#36817;&#24180;&#26469;&#65292;&#19968;&#20010;&#21463;&#21040;&#20851;&#27880;&#30340;&#38382;&#39064;&#26159;&#36825;&#19968;&#25216;&#26415;&#22312;&#34892;&#20026;&#20013;&#26174;&#31034;&#20986;&#26377;&#23475;&#20559;&#35265;&#12290;&#23613;&#31649;&#24050;&#32463;&#25237;&#20837;&#20102;&#22823;&#37327;&#30340;&#21162;&#21147;&#26469;&#35780;&#20272;&#21644;&#20943;&#36731;&#36825;&#20123;&#20559;&#35265;&#65292;&#20294;&#25105;&#20204;&#34913;&#37327;NLP&#27169;&#22411;&#20559;&#35265;&#30340;&#26041;&#27861;&#23384;&#22312;&#20005;&#37325;&#38382;&#39064;&#65288;&#20363;&#22914;&#65292;&#36890;&#24120;&#19981;&#28165;&#26970;&#23427;&#20204;&#21040;&#24213;&#34913;&#37327;&#20102;&#20160;&#20040;&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#24515;&#29702;&#27979;&#37327;&#23398;&#30340;&#35270;&#35282;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#36328;&#23398;&#31185;&#30340;&#26041;&#27861;&#26469;&#35752;&#35770;NLP&#27169;&#22411;&#20559;&#35265;&#30340;&#38382;&#39064;&#65292;&#24515;&#29702;&#27979;&#37327;&#23398;&#19987;&#27880;&#20110;&#34913;&#37327;&#19981;&#30452;&#25509;&#21487;&#35266;&#23519;&#21040;&#30340;&#27010;&#24565;&#65292;&#22914;&#20559;&#35265;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#25506;&#35752;&#24515;&#29702;&#27979;&#37327;&#23398;&#30340;&#20004;&#20010;&#26680;&#24515;&#27010;&#24565;&#65292;&#21363;&#26500;&#24565;&#25928;&#24230;&#21644;&#27979;&#37327;&#24037;&#20855;&#30340;&#20449;&#24230;&#65292;&#24182;&#35752;&#35770;&#23427;&#20204;&#22312;&#34913;&#37327;&#27169;&#22411;&#20559;&#35265;&#30340;&#24773;&#22659;&#20013;&#22914;&#20309;&#24212;&#29992;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25552;&#20379;&#19968;&#20010;&#20840;&#38754;&#30340;&#35270;&#35282;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
As Large Language Models and Natural Language Processing (NLP) technology rapidly develops and spreads into daily life, it becomes crucial to anticipate how its use could harm people. One problem that has received a lot of attention in recent years is that this technology has displayed harmful biases in its behavior. Although a lot of effort has been invested in assessing and mitigating these biases, our methods of measuring the biases of NLP models have serious problems (e.g., it is often unclear what they actually measure). In this paper, we provide an interdisciplinary approach to discussing the issue of NLP model bias by adopting the lens of psychometrics -- a field specialized in the measurement of concepts like bias that are not directly observable. In particular, we will explore two central notions from psychometrics, the construct validity and the reliability of measurement tools, and discuss how they can be applied in the context of measuring model bias. Our goal is to provide
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Failed goal Aware HER (FAHER)&#30340;&#26032;&#26041;&#27861;&#26469;&#22686;&#24378;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#37319;&#26679;&#25928;&#29575;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#23454;&#29616;&#30446;&#26631;&#19982;&#26410;&#23454;&#29616;&#30446;&#26631;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#32858;&#31867;&#27169;&#22411;&#23545;&#24207;&#21015;&#36827;&#34892;&#32858;&#31867;&#21644;&#37319;&#26679;&#12290;</title><link>http://arxiv.org/abs/2208.14741</link><description>&lt;p&gt;
&#22833;&#36133;&#30446;&#26631;&#24863;&#30693;&#30340;&#20107;&#21518;&#32463;&#39564;&#37325;&#25773;
&lt;/p&gt;
&lt;p&gt;
Failed Goal Aware Hindsight Experience Replay. (arXiv:2208.14741v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.14741
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Failed goal Aware HER (FAHER)&#30340;&#26032;&#26041;&#27861;&#26469;&#22686;&#24378;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#37319;&#26679;&#25928;&#29575;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#23454;&#29616;&#30446;&#26631;&#19982;&#26410;&#23454;&#29616;&#30446;&#26631;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#32858;&#31867;&#27169;&#22411;&#23545;&#24207;&#21015;&#36827;&#34892;&#32858;&#31867;&#21644;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32473;&#23450;&#29615;&#22659;&#20013;&#30340;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#20195;&#29702;&#36890;&#36807;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#33719;&#24471;&#30340;&#32463;&#39564;&#26469;&#23398;&#20064;&#23454;&#29616;&#22810;&#20010;&#30446;&#26631;&#30340;&#31574;&#30053;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#20351;&#29992;&#31232;&#30095;&#20108;&#20803;&#22870;&#21169;&#35757;&#32451;&#20195;&#29702;&#65292;&#30001;&#20110;&#32570;&#20047;&#25104;&#21151;&#30340;&#32463;&#39564;&#65292;&#36825;&#21487;&#33021;&#20250;&#24456;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#20107;&#21518;&#32463;&#39564;&#37325;&#25773;&#65288;HER&#65289;&#20174;&#19981;&#25104;&#21151;&#30340;&#32463;&#39564;&#20013;&#29983;&#25104;&#25104;&#21151;&#30340;&#32463;&#39564;&#12290;&#28982;&#32780;&#65292;&#20174;&#22343;&#21248;&#25277;&#26679;&#30340;&#32463;&#39564;&#20013;&#29983;&#25104;&#25104;&#21151;&#30340;&#32463;&#39564;&#30340;&#36807;&#31243;&#21487;&#33021;&#25928;&#29575;&#20302;&#19979;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Failed goal Aware HER (FAHER)&#30340;&#26032;&#26041;&#27861;&#26469;&#22686;&#24378;&#37319;&#26679;&#25928;&#29575;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#23454;&#29616;&#30446;&#26631;&#19982;&#26410;&#23454;&#29616;&#30446;&#26631;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#20351;&#29992;&#32858;&#31867;&#27169;&#22411;&#23545;&#20855;&#26377;&#19981;&#21516;&#23454;&#29616;&#30446;&#26631;&#30340;&#24207;&#21015;&#36827;&#34892;&#32858;&#31867;&#65292;&#24182;&#22312;HER&#30340;&#26041;&#24335;&#19979;&#23545;&#32463;&#39564;&#36827;&#34892;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;
In multi-goal reinforcement learning for a given environment, agents learn policies to achieve multiple goals by using experiences gained from interactions with the environment. One of the key challenges in this setting is training agents using sparse binary rewards, which can be difficult due to a lack of successful experiences. To address this challenge, hindsight experience replay (HER) generates successful experiences from unsuccessful experiences. However, the process of generating successful experiences from uniformly sampled ones can be inefficient. In this paper, a novel approach called Failed goal Aware HER (FAHER) is proposed to enhance the sampling efficiency. The approach exploits the property of achieved goals in relation to failed goals that are defined as the original goals not achieved. The proposed method involves clustering episodes with different achieved goals using a cluster model and subsequently sampling experiences in the manner of HER. The cluster model is gene
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SSIVD-Net&#30340;&#26032;&#25216;&#26415;&#65292;&#29992;&#20110;&#26292;&#21147;&#35782;&#21035;&#20219;&#21153;&#12290;&#36890;&#36807;&#20351;&#29992;&#26174;&#33879;-&#36229;&#32423;&#22270;&#20687;&#34920;&#31034;&#20943;&#23569;&#20102;3D&#35270;&#39057;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#65292;&#25552;&#39640;&#20102;&#25512;&#26029;&#12289;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26550;&#26500;&#8220;Salient-Classifier&#8221;&#65292;&#23558;&#26680;&#26041;&#27861;&#21644;&#27531;&#24046;&#23398;&#20064;&#31574;&#30053;&#30456;&#32467;&#21512;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2207.12850</link><description>&lt;p&gt;
SSIVD-Net&#65306;&#19968;&#31181;&#26032;&#30340;&#27494;&#22120;&#21270;&#26292;&#21147;&#26174;&#33879;&#36229;&#32423;&#22270;&#20687;&#20998;&#31867;&#21644;&#26816;&#27979;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
SSIVD-Net: A Novel Salient Super Image Classification &amp; Detection Technique for Weaponized Violence. (arXiv:2207.12850v6 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.12850
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SSIVD-Net&#30340;&#26032;&#25216;&#26415;&#65292;&#29992;&#20110;&#26292;&#21147;&#35782;&#21035;&#20219;&#21153;&#12290;&#36890;&#36807;&#20351;&#29992;&#26174;&#33879;-&#36229;&#32423;&#22270;&#20687;&#34920;&#31034;&#20943;&#23569;&#20102;3D&#35270;&#39057;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#65292;&#25552;&#39640;&#20102;&#25512;&#26029;&#12289;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26550;&#26500;&#8220;Salient-Classifier&#8221;&#65292;&#23558;&#26680;&#26041;&#27861;&#21644;&#27531;&#24046;&#23398;&#20064;&#31574;&#30053;&#30456;&#32467;&#21512;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38381;&#36335;&#30005;&#35270;&#65288;CCTV&#65289;&#30417;&#25511;&#24405;&#20687;&#20013;&#26816;&#27979;&#26292;&#21147;&#21644;&#27494;&#22120;&#21270;&#26292;&#21147;&#38656;&#35201;&#19968;&#20010;&#20840;&#38754;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#8220;&#26234;&#24935;&#22478;&#24066;CCTV&#26292;&#21147;&#26816;&#27979;&#65288;SCVD&#65289;&#8221;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#20419;&#36827;&#23545;&#30417;&#25511;&#35270;&#39057;&#20013;&#27494;&#22120;&#20998;&#24067;&#30340;&#23398;&#20064;&#12290;&#20026;&#20102;&#35299;&#20915;&#20998;&#26512;3D&#30417;&#25511;&#35270;&#39057;&#36827;&#34892;&#26292;&#21147;&#35782;&#21035;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#25216;&#26415;&#65292;&#31216;&#20026;SSIVD-Net&#65288;&#29992;&#20110;&#26292;&#21147;&#26816;&#27979;&#30340;&#26174;&#33879;-&#36229;&#32423;-&#22270;&#20687;&#65289;&#65292;&#36890;&#36807;&#20351;&#29992;&#26174;&#33879;-&#36229;&#32423;&#22270;&#20687;&#34920;&#31034;&#20943;&#23569;3D&#35270;&#39057;&#25968;&#25454;&#22797;&#26434;&#24615;&#12289;&#38477;&#32500;&#21644;&#20449;&#24687;&#25439;&#22833;&#65292;&#21516;&#26102;&#25552;&#39640;&#25512;&#26029;&#12289;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#32771;&#34385;&#21040;&#26410;&#26469;&#26234;&#24935;&#22478;&#24066;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#21487;&#25345;&#32493;&#24615;&#35201;&#27714;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26550;&#26500;&#8220;Salient-Classifier&#8221;&#65292;&#23558;&#26680;&#26041;&#27861;&#21644;&#27531;&#24046;&#23398;&#20064;&#31574;&#30053;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;SSIVD-Net&#22312;SCVD&#12289;Hockey Fight&#12289;Moviescope&#20197;&#21450;Large-Scale Fight Detection&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#19982;&#29616;&#26377;&#31639;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detection of violence and weaponized violence in closed-circuit television (CCTV) footage requires a comprehensive approach. In this work, we introduce the \emph{Smart-City CCTV Violence Detection (SCVD)} dataset, specifically designed to facilitate the learning of weapon distribution in surveillance videos. To tackle the complexities of analyzing 3D surveillance video for violence recognition tasks, we propose a novel technique called, \emph{SSIVD-Net} (\textbf{S}alient-\textbf{S}uper-\textbf{I}mage for \textbf{V}iolence \textbf{D}etection). Our method reduces 3D video data complexity, dimensionality, and information loss while improving inference, performance, and explainability through the use of Salient-Super-Image representations. Considering the scalability and sustainability requirements of futuristic smart cities, the authors introduce the \emph{Salient-Classifier}, a novel architecture combining a kernelized approach with a residual learning strategy. We evaluate variations of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#24182;&#37325;&#26032;&#21019;&#24314;&#20102;&#31526;&#21495;&#22238;&#24402;&#25968;&#25454;&#38598;&#65292;&#20197;&#35780;&#20272;&#31526;&#21495;&#22238;&#24402;&#22312;&#31185;&#23398;&#21457;&#29616;&#20013;&#30340;&#28508;&#21147;&#21644;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20351;&#29992;&#24402;&#19968;&#21270;&#32534;&#36753;&#36317;&#31163;&#36827;&#34892;&#24230;&#37327;&#12290;</title><link>http://arxiv.org/abs/2206.10540</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#31185;&#23398;&#21457;&#29616;&#20013;&#31526;&#21495;&#22238;&#24402;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Rethinking Symbolic Regression Datasets and Benchmarks for Scientific Discovery. (arXiv:2206.10540v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.10540
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#24182;&#37325;&#26032;&#21019;&#24314;&#20102;&#31526;&#21495;&#22238;&#24402;&#25968;&#25454;&#38598;&#65292;&#20197;&#35780;&#20272;&#31526;&#21495;&#22238;&#24402;&#22312;&#31185;&#23398;&#21457;&#29616;&#20013;&#30340;&#28508;&#21147;&#21644;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20351;&#29992;&#24402;&#19968;&#21270;&#32534;&#36753;&#36317;&#31163;&#36827;&#34892;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#31526;&#21495;&#22238;&#24402;&#65288;SR&#65289;&#30340;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#26631;&#20934;&#65292;&#29305;&#21035;&#20851;&#27880;&#20854;&#22312;&#31185;&#23398;&#21457;&#29616;&#20013;&#30340;&#28508;&#21147;&#12290;&#38024;&#23545;&#29616;&#26377;&#25968;&#25454;&#38598;&#20013;&#22522;&#20110;&#36153;&#26364;&#29289;&#29702;&#35762;&#20041;&#30340;&#19968;&#32452;&#20844;&#24335;&#65292;&#25105;&#20204;&#37325;&#26032;&#21019;&#24314;&#20102;120&#20010;&#25968;&#25454;&#38598;&#65292;&#35752;&#35770;&#31526;&#21495;&#22238;&#24402;&#22312;&#31185;&#23398;&#21457;&#29616;&#20013;&#30340;&#24615;&#33021;&#65288;SRSD&#65289;&#12290;&#23545;&#20110;&#36825;120&#20010;SRSD&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#20180;&#32454;&#23457;&#26597;&#20102;&#20844;&#24335;&#21450;&#20854;&#21464;&#37327;&#30340;&#23646;&#24615;&#65292;&#35774;&#35745;&#20102;&#21512;&#29702;&#30340;&#23454;&#20540;&#33539;&#22260;&#26469;&#37319;&#26679;&#20540;&#65292;&#20197;&#20415;&#25105;&#20204;&#30340;&#26032;SRSD&#25968;&#25454;&#38598;&#21487;&#29992;&#20110;&#35780;&#20272;SRSD&#30340;&#28508;&#21147;&#65292;&#22914;SR&#26041;&#27861;&#26159;&#21542;&#33021;&#20174;&#36825;&#26679;&#30340;&#25968;&#25454;&#38598;&#20013;&#65288;&#37325;&#26032;&#65289;&#21457;&#29616;&#29289;&#29702;&#23450;&#24459;&#12290;&#25105;&#20204;&#36824;&#21019;&#24314;&#20102;&#21478;&#22806;120&#20010;&#21253;&#21547;&#34394;&#25311;&#21464;&#37327;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#26816;&#39564;SR&#26041;&#27861;&#26159;&#21542;&#33021;&#22815;&#20165;&#36873;&#25321;&#24517;&#35201;&#21464;&#37327;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#39044;&#27979;&#26041;&#31243;&#19982;&#30495;&#23454;&#26041;&#31243;&#26641;&#20043;&#38388;&#30340;&#24402;&#19968;&#21270;&#32534;&#36753;&#36317;&#31163;&#65288;NED&#65289;&#26469;&#35299;&#20915;&#29616;&#26377;SR&#24230;&#37327;&#23384;&#22312;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#21363;&#20108;&#20803;&#24230;&#37327;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper revisits datasets and evaluation criteria for Symbolic Regression (SR), specifically focused on its potential for scientific discovery. Focused on a set of formulas used in the existing datasets based on Feynman Lectures on Physics, we recreate 120 datasets to discuss the performance of symbolic regression for scientific discovery (SRSD). For each of the 120 SRSD datasets, we carefully review the properties of the formula and its variables to design reasonably realistic sampling ranges of values so that our new SRSD datasets can be used for evaluating the potential of SRSD such as whether or not an SR method can (re)discover physical laws from such datasets. We also create another 120 datasets that contain dummy variables to examine whether SR methods can choose necessary variables only. Besides, we propose to use normalized edit distances (NED) between a predicted equation and the true equation trees for addressing a critical issue that existing SR metrics are either binary
&lt;/p&gt;</description></item><item><title>FAIR4Cov&#26159;&#19968;&#31181;&#38024;&#23545;COVID-19&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#23427;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#36523;&#20307;&#22768;&#38899;&#30340;&#27874;&#24418;&#21644;&#35889;&#22270;&#34920;&#31034;&#30340;&#20851;&#33410;&#29305;&#24449;&#21521;&#37327;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#26816;&#27979;COVID-19&#24739;&#32773;&#65292;&#32988;&#36807;&#20102;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2204.10581</link><description>&lt;p&gt;
FAIR4Cov&#65306;&#29992;&#20110; COVID-19 &#26816;&#27979;&#30340;&#34701;&#21512;&#38899;&#39057;&#23454;&#20363;&#21644;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
FAIR4Cov: Fused Audio Instance and Representation for COVID-19 Detection. (arXiv:2204.10581v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.10581
&lt;/p&gt;
&lt;p&gt;
FAIR4Cov&#26159;&#19968;&#31181;&#38024;&#23545;COVID-19&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#23427;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#36523;&#20307;&#22768;&#38899;&#30340;&#27874;&#24418;&#21644;&#35889;&#22270;&#34920;&#31034;&#30340;&#20851;&#33410;&#29305;&#24449;&#21521;&#37327;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#26816;&#27979;COVID-19&#24739;&#32773;&#65292;&#32988;&#36807;&#20102;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#36523;&#20307;&#22768;&#38899;&#30340;&#20998;&#31867;&#25216;&#26415;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#34987;&#30740;&#31350;&#29992;&#20110;&#25903;&#25345;&#35786;&#26029;&#20915;&#31574;&#65292;&#29305;&#21035;&#26159;&#22312;&#32954;&#37096;&#30142;&#30149;&#26041;&#38754;&#12290;&#38024;&#23545; COVID-19 &#30123;&#24773;&#30340;&#32039;&#36843;&#24615;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#27169;&#22411;&#34987;&#24320;&#21457;&#26469;&#22522;&#20110;&#22768;&#23398;&#36755;&#20837;&#35782;&#21035; COVID-19 &#24739;&#32773;&#12290;&#22823;&#22810;&#25968;&#27169;&#22411;&#20391;&#37325;&#20110;&#21683;&#22013;&#65292;&#22240;&#20026;&#24178;&#21683;&#26159; COVID-19 &#26368;&#20026;&#20154;&#25152;&#30693;&#30340;&#30151;&#29366;&#12290;&#28982;&#32780;&#65292;&#21628;&#21560;&#21644;&#35328;&#35821;&#31561;&#20854;&#20182;&#36523;&#20307;&#22768;&#38899;&#20063;&#34987;&#21457;&#29616;&#19982; COVID-19 &#30456;&#20851;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; FAIR4Cov&#65292;&#23427;&#19981;&#20381;&#36182;&#20110;&#29305;&#23450;&#30340;&#36523;&#20307;&#22768;&#38899;&#65292;&#32780;&#26159;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#36523;&#20307;&#22768;&#38899;&#30340;&#27874;&#24418;&#21644;&#35889;&#22270;&#34920;&#31034;&#30340;&#20851;&#33410;&#29305;&#24449;&#21521;&#37327;&#12290;FAIR4Cov &#30340;&#26680;&#24515;&#32452;&#20214;&#26159;&#19968;&#20010;&#33258;&#27880;&#24847;&#34701;&#21512;&#21333;&#20803;&#65292;&#23427;&#30340;&#35757;&#32451;&#30446;&#30340;&#26159;&#24314;&#31435;&#22810;&#20010;&#36523;&#20307;&#22768;&#38899;&#21644;&#38899;&#39057;&#34920;&#31034;&#30340;&#20851;&#31995;&#24182;&#23558;&#20854;&#38598;&#25104;&#21040;&#19968;&#20010;&#32039;&#20945;&#30340;&#29305;&#24449;&#21521;&#37327;&#20013;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#35774;&#32622;&#20102;&#23454;&#39564;&#65292;&#24182;&#22312;&#19981;&#21516;&#22330;&#26223;&#19979;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#25552;&#35758;&#26041;&#27861;&#65292;&#21253;&#25324;&#36328;&#25968;&#25454;&#38598;&#35780;&#20272;&#21644;&#26089;&#26399;&#26816;&#27979;&#35774;&#32622;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;FAIR4Cov &#32988;&#36807;&#20102;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#21033;&#29992;&#21508;&#31181;&#36523;&#20307;&#22768;&#38899;&#26816;&#27979; COVID-19 &#24739;&#32773;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Audio-based classification techniques on body sounds have long been studied to support diagnostic decisions, particularly in pulmonary diseases. In response to the urgency of the COVID-19 pandemic, a growing number of models are developed to identify COVID-19 patients based on acoustic input. Most models focus on cough because the dry cough is the best-known symptom of COVID-19. However, other body sounds, such as breath and speech, have also been revealed to correlate with COVID-19 as well. In this work, rather than relying on a specific body sound, we propose Fused Audio Instance and Representation for COVID-19 Detection (FAIR4Cov). It relies on constructing a joint feature vector obtained from a plurality of body sounds in waveform and spectrogram representation. The core component of FAIR4Cov is a self-attention fusion unit that is trained to establish the relation of multiple body sounds and audio representations and integrate it into a compact feature vector. We set up our experi
&lt;/p&gt;</description></item></channel></rss>