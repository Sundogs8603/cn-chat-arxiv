<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;LLaMA-Adapter V2&#65292;&#36825;&#26159;&#19968;&#20010;&#21442;&#25968;&#39640;&#25928;&#30340;&#35270;&#35273;&#25351;&#20196;&#27169;&#22411;&#65292;&#36890;&#36807;&#35299;&#38145;&#26356;&#22810;&#21487;&#23398;&#20064;&#30340;&#21442;&#25968;&#65292;&#26089;&#26399;&#34701;&#21512;&#31574;&#30053;&#21644;&#32852;&#21512;&#35757;&#32451;&#31574;&#30053;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#22788;&#29702;&#35270;&#35273;&#36755;&#20837;&#21644;&#31934;&#30830;&#22320;&#25191;&#34892;&#24320;&#25918;&#24335;&#35270;&#35273;&#25351;&#20196;&#12290;</title><link>http://arxiv.org/abs/2304.15010</link><description>&lt;p&gt;
LLaMA-Adapter V2: &#21442;&#25968;&#39640;&#25928;&#30340;&#35270;&#35273;&#25351;&#20196;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model. (arXiv:2304.15010v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.15010
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;LLaMA-Adapter V2&#65292;&#36825;&#26159;&#19968;&#20010;&#21442;&#25968;&#39640;&#25928;&#30340;&#35270;&#35273;&#25351;&#20196;&#27169;&#22411;&#65292;&#36890;&#36807;&#35299;&#38145;&#26356;&#22810;&#21487;&#23398;&#20064;&#30340;&#21442;&#25968;&#65292;&#26089;&#26399;&#34701;&#21512;&#31574;&#30053;&#21644;&#32852;&#21512;&#35757;&#32451;&#31574;&#30053;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#22788;&#29702;&#35270;&#35273;&#36755;&#20837;&#21644;&#31934;&#30830;&#22320;&#25191;&#34892;&#24320;&#25918;&#24335;&#35270;&#35273;&#25351;&#20196;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#30740;&#31350;&#26041;&#21521;&#26159;&#22914;&#20309;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39640;&#25928;&#22320;&#36716;&#21270;&#20026;&#25351;&#20196;&#36319;&#38543;&#32773;&#65292;&#32780;&#20026;&#22810;&#27169;&#24577;&#25512;&#29702;&#35757;&#32451;LLM&#30340;&#30740;&#31350;&#20173;&#28982;&#36739;&#23569;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;LLaMA-Adapter&#35777;&#26126;&#20102;&#29992;LLM&#22788;&#29702;&#35270;&#35273;&#36755;&#20837;&#30340;&#28508;&#21147;&#65292;&#20294;&#23427;&#20173;&#28982;&#19981;&#33021;&#24456;&#22909;&#22320;&#25512;&#24191;&#21040;&#24320;&#25918;&#24335;&#35270;&#35273;&#25351;&#20196;&#65292;&#24182;&#19988;&#33853;&#21518;&#20110;GPT-4&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;LLaMA-Adapter V2&#65292;&#36825;&#26159;&#19968;&#20010;&#21442;&#25968;&#39640;&#25928;&#30340;&#35270;&#35273;&#25351;&#20196;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
How to efficiently transform large language models (LLMs) into instruction followers is recently a popular research direction, while training LLM for multi-modal reasoning remains less explored. Although the recent LLaMA-Adapter demonstrates the potential to handle visual inputs with LLMs, it still cannot generalize well to open-ended visual instructions and lags behind GPT-4. In this paper, we present LLaMA-Adapter V2, a parameter-efficient visual instruction model. Specifically, we first augment LLaMA-Adapter by unlocking more learnable parameters (e.g., norm, bias and scale), which distribute the instruction-following ability across the entire LLaMA model besides adapters. Secondly, we propose an early fusion strategy to feed visual tokens only into the early LLM layers, contributing to better visual knowledge incorporation. Thirdly, a joint training paradigm of image-text pairs and instruction-following data is introduced by optimizing disjoint groups of learnable parameters. This 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25351;&#20986;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25152;&#35859;&#30340;&#26032;&#20852;&#25216;&#33021;&#26159;&#30740;&#31350;&#32773;&#20998;&#26512;&#30340;&#20135;&#29289;&#65292;&#19981;&#26159;&#27169;&#22411;&#34892;&#20026;&#30340;&#22522;&#26412;&#21464;&#21270;&#12290;&#30740;&#31350;&#36824;&#23637;&#31034;&#20102;&#24230;&#37327;&#26631;&#20934;&#36873;&#25321;&#21644;&#21487;&#33021;&#30740;&#31350;&#20154;&#21592;&#30340;&#20559;&#35265;&#65292;&#21487;&#33021;&#23548;&#33268;&#36825;&#31181;&#26032;&#20852;&#25216;&#33021;&#30340;&#20986;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.15004</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25152;&#34920;&#29616;&#30340;&#26032;&#20852;&#25216;&#33021;&#26159;&#21542;&#20026;&#24187;&#35273;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Emergent Abilities of Large Language Models a Mirage?. (arXiv:2304.15004v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.15004
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25351;&#20986;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25152;&#35859;&#30340;&#26032;&#20852;&#25216;&#33021;&#26159;&#30740;&#31350;&#32773;&#20998;&#26512;&#30340;&#20135;&#29289;&#65292;&#19981;&#26159;&#27169;&#22411;&#34892;&#20026;&#30340;&#22522;&#26412;&#21464;&#21270;&#12290;&#30740;&#31350;&#36824;&#23637;&#31034;&#20102;&#24230;&#37327;&#26631;&#20934;&#36873;&#25321;&#21644;&#21487;&#33021;&#30740;&#31350;&#20154;&#21592;&#30340;&#20559;&#35265;&#65292;&#21487;&#33021;&#23548;&#33268;&#36825;&#31181;&#26032;&#20852;&#25216;&#33021;&#30340;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#22768;&#31216;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20102;&#26032;&#20852;&#25216;&#33021;&#65292;&#36825;&#20123;&#25216;&#33021;&#22312;&#26356;&#23567;&#35268;&#27169;&#30340;&#27169;&#22411;&#20013;&#19981;&#23384;&#22312;&#65292;&#20294;&#22312;&#26356;&#22823;&#35268;&#27169;&#30340;&#27169;&#22411;&#20013;&#23384;&#22312;&#12290;&#26032;&#20852;&#25216;&#33021;&#35753;&#20154;&#24863;&#21040;&#22256;&#24785;&#30340;&#26159;&#20004;&#26041;&#38754;&#65306;&#23427;&#20204;&#30340;&#28165;&#26224;&#24230;&#65292;&#20284;&#20046;&#30636;&#38388;&#20174;&#19981;&#23384;&#22312;&#21040;&#23384;&#22312;&#65292;&#20197;&#21450;&#23427;&#20204;&#30340;&#19981;&#21487;&#39044;&#27979;&#24615;&#65292;&#20284;&#20046;&#22312;&#19981;&#21487;&#39044;&#35265;&#30340;&#27169;&#22411;&#35268;&#27169;&#19979;&#20986;&#29616;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#26032;&#20852;&#25216;&#33021;&#30340;&#21478;&#19968;&#31181;&#35299;&#37322;&#65292;&#21363;&#23545;&#20110;&#29305;&#23450;&#20219;&#21153;&#21644;&#27169;&#22411;&#26063;&#65292;&#24403;&#20998;&#26512;&#22266;&#23450;&#30340;&#27169;&#22411;&#36755;&#20986;&#26102;&#65292;&#21487;&#20197;&#36873;&#25321;&#23548;&#33268;&#25512;&#26029;&#20986;&#26032;&#20852;&#25216;&#33021;&#25110;&#19981;&#23548;&#33268;&#25512;&#26029;&#20986;&#26032;&#20852;&#25216;&#33021;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#35299;&#37322;&#34920;&#26126;&#65292;&#29616;&#26377;&#30340;&#26032;&#20852;&#25216;&#33021;&#22768;&#26126;&#26159;&#30740;&#31350;&#20154;&#21592;&#20998;&#26512;&#30340;&#20135;&#29289;&#65292;&#32780;&#19981;&#26159;&#29305;&#23450;&#20219;&#21153;&#20013;&#27169;&#22411;&#34892;&#20026;&#30340;&#22522;&#26412;&#21464;&#21270;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#31616;&#21333;&#30340;&#25968;&#23398;&#27169;&#22411;&#20013;&#25552;&#20986;&#20102;&#25105;&#20204;&#30340;&#35299;&#37322;&#65292;&#28982;&#21518;&#36890;&#36807;&#19977;&#31181;&#20114;&#34917;&#30340;&#26041;&#24335;&#36827;&#34892;&#20102;&#27979;&#35797;&#65306;&#25105;&#20204;(1)&#21046;&#20316;&#12289;&#27979;&#35797;&#24182;&#39564;&#35777;&#20102;&#20851;&#20110;&#25253;&#21578;&#30340;&#26032;&#20852;&#25216;&#33021;&#30340;&#24230;&#37327;&#36873;&#25321;&#30340;&#19977;&#20010;&#39044;&#27979;&#25928;&#24212;&#65307;(2)&#23637;&#31034;&#20102;&#27169;&#22411;&#26550;&#26500;&#21644;&#35757;&#32451;&#31243;&#24207;&#30340;&#31616;&#21333;&#21464;&#21270;&#20250;&#22312;&#19968;&#20010;&#24050;&#32463;&#30830;&#23450;&#30340;&#20219;&#21153;&#20013;&#20135;&#29983;&#22823;&#30340;&#26032;&#20852;&#33021;&#21147;&#24046;&#24322;&#65307;(3)&#23637;&#31034;&#25152;&#35859;&#30340;&#26032;&#20852;&#25216;&#33021;&#21487;&#20197;&#36890;&#36807;&#26377;&#24847;&#20248;&#21270;&#25152;&#36873;&#25321;&#30340;&#35780;&#20272;&#25351;&#26631;&#26469;&#23454;&#29616;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#35748;&#20026;&#30446;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26032;&#20852;&#33021;&#21147;&#30340;&#22768;&#26126;&#24456;&#21487;&#33021;&#24182;&#19981;&#26159;&#30495;&#23454;&#23384;&#22312;&#30340;&#65292;&#32780;&#26159;&#24230;&#37327;&#26631;&#20934;&#20219;&#24847;&#36873;&#25321;&#21644;&#21487;&#33021;&#30340;&#30740;&#31350;&#20154;&#21592;&#20559;&#35265;&#30340;&#20135;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work claims that large language models display emergent abilities, abilities not present in smaller-scale models that are present in larger-scale models. What makes emergent abilities intriguing is two-fold: their sharpness, transitioning seemingly instantaneously from not present to present, and their unpredictability, appearing at seemingly unforeseeable model scales. Here, we present an alternative explanation for emergent abilities: that for a particular task and model family, when analyzing fixed model outputs, one can choose a metric which leads to the inference of an emergent ability or another metric which does not. Thus, our alternative suggests that existing claims of emergent abilities are creations of the researcher's analyses, not fundamental changes in model behavior on specific tasks with scale. We present our explanation in a simple mathematical model, then test it in three complementary ways: we (1) make, test and confirm three predictions on the effect of metri
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23454;&#35777;&#20998;&#26512;&#20102;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#25216;&#26415;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#65292;&#25552;&#20379;&#20102;&#36873;&#25321;&#20248;&#21270;&#24494;&#35843;&#25216;&#26415;&#30340;&#26694;&#26550;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#22312;&#20302;&#25968;&#25454;&#22330;&#26223;&#19979;PEFT&#25216;&#26415;&#25910;&#25947;&#36895;&#24230;&#36739;&#24930;&#30340;&#20107;&#23454;&#12290;</title><link>http://arxiv.org/abs/2304.14999</link><description>&lt;p&gt;
LLM&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#25216;&#26415;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#30340;&#23454;&#35777;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Empirical Analysis of the Strengths and Weaknesses of PEFT Techniques for LLMs. (arXiv:2304.14999v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14999
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23454;&#35777;&#20998;&#26512;&#20102;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#25216;&#26415;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#65292;&#25552;&#20379;&#20102;&#36873;&#25321;&#20248;&#21270;&#24494;&#35843;&#25216;&#26415;&#30340;&#26694;&#26550;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#22312;&#20302;&#25968;&#25454;&#22330;&#26223;&#19979;PEFT&#25216;&#26415;&#25910;&#25947;&#36895;&#24230;&#36739;&#24930;&#30340;&#20107;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22411;&#35268;&#27169;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#39640;&#25928;&#30340;&#36866;&#24212;&#26041;&#27861;&#21464;&#24471;&#36234;&#26469;&#36234;&#20851;&#38190;&#12290;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26159;&#30446;&#21069;&#26368;&#27969;&#34892;&#30340;&#36866;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26041;&#27861;&#20043;&#19968;&#65292;&#21482;&#38656;&#35201;&#20462;&#25913;&#27169;&#22411;&#21442;&#25968;&#30340;&#19968;&#23567;&#37096;&#20998;&#65292;&#26368;&#36817;&#25552;&#20986;&#20102;&#20960;&#31181;&#20855;&#26377;&#19981;&#21516;&#26435;&#34913;&#30340;PEFT&#25216;&#26415;&#12290;&#25105;&#20204;&#22312;&#20195;&#34920;&#24615;&#30340;LLM FLAN-T5&#27169;&#22411;&#19978;&#25552;&#20379;&#21508;&#31181;PEFT&#25216;&#26415;&#30340;&#20840;&#38754;&#21644;&#32479;&#19968;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#35780;&#20272;&#22312;&#20998;&#31867;&#21644;&#29983;&#25104;&#25968;&#25454;&#38598;&#30340;&#19981;&#21516;&#25968;&#25454;&#35268;&#27169;&#19979;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#26681;&#25454;&#20219;&#21153;&#31867;&#22411;&#21644;&#25968;&#25454;&#21487;&#29992;&#24615;&#36873;&#25321;&#26368;&#20339;&#30340;&#24494;&#35843;&#25216;&#26415;&#12290;&#19982;&#20256;&#32479;&#35266;&#24565;&#30456;&#21453;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#35777;&#26126;PEFT&#25216;&#26415;&#22312;&#20302;&#25968;&#25454;&#22330;&#26223;&#19979;&#25910;&#25947;&#36895;&#24230;&#27604;&#23436;&#20840;&#24494;&#35843;&#24930;&#65292;&#24182;&#25552;&#20986;&#20102;PEFT&#26041;&#27861;&#38656;&#35201;&#34920;&#29616;&#33391;&#22909;&#21644;&#26377;&#25928;&#25910;&#25947;&#25152;&#38656;&#35201;&#30340;&#25968;&#25454;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
As foundation models continue to exponentially scale in size, efficient methods of adaptation become increasingly critical. Parameter-efficient fine-tuning (PEFT), a recent class of techniques that require only modifying a small percentage of the model parameters, is currently the most popular method for adapting large language models (LLMs). Several PEFT techniques have recently been proposed with varying tradeoffs. We provide a comprehensive and uniform benchmark of various PEFT techniques across a representative LLM, the FLAN-T5 model, and evaluate model performance across different data scales of classification and generation datasets. Based on this, we provide a framework for choosing the optimal fine-tuning techniques given the task type and data availability. Contrary to popular belief, we also empirically prove that PEFT techniques converge slower than full tuning in low data scenarios, and posit the amount of data required for PEFT methods to both perform well and converge eff
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;ChatGPT&#22312;&#22238;&#31572;&#26412;&#31185;&#35745;&#31639;&#26426;&#31185;&#23398;&#38382;&#39064;&#19978;&#30340;&#19981;&#21487;&#38752;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#22312;&#23398;&#26415;&#30028;&#20351;&#29992;ChatGPT&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2304.14993</link><description>&lt;p&gt;
ChatGPT - &#23545;&#20110;&#26412;&#31185;&#35745;&#31639;&#26426;&#31185;&#23398;&#23398;&#29983;&#21644;&#25945;&#24072;&#26159;&#31119;&#26159;&#31096;?
&lt;/p&gt;
&lt;p&gt;
ChatGPT -- a Blessing or a Curse for Undergraduate Computer Science Students and Instructors?. (arXiv:2304.14993v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14993
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;ChatGPT&#22312;&#22238;&#31572;&#26412;&#31185;&#35745;&#31639;&#26426;&#31185;&#23398;&#38382;&#39064;&#19978;&#30340;&#19981;&#21487;&#38752;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#22312;&#23398;&#26415;&#30028;&#20351;&#29992;ChatGPT&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;&#30001;OpenAI&#24320;&#21457;&#30340;AI&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#29702;&#35299;&#21644;&#29983;&#25104;&#31867;&#20154;&#25991;&#26412;&#12290;&#23427;&#21487;&#29992;&#20110;&#35821;&#35328;&#29983;&#25104;&#12289;&#38382;&#31572;&#12289;&#25991;&#26412;&#25688;&#35201;&#12289;&#32842;&#22825;&#26426;&#22120;&#20154;&#24320;&#21457;&#12289;&#35821;&#35328;&#32763;&#35793;&#12289;&#24773;&#24863;&#20998;&#26512;&#12289;&#20869;&#23481;&#21019;&#20316;&#12289;&#20010;&#24615;&#21270;&#12289;&#25991;&#26412;&#23436;&#25104;&#21644;&#25925;&#20107;&#21465;&#36848;&#31561;&#22810;&#31181;&#29992;&#36884;&#12290;&#34429;&#28982;ChatGPT&#21463;&#21040;&#20102;&#30456;&#24403;&#31215;&#26497;&#30340;&#20851;&#27880;&#65292;&#20294;&#22312;&#23398;&#26415;&#30028;&#20063;&#24341;&#36215;&#20102;&#19968;&#31181;&#25285;&#24551;&#21644;&#19981;&#30830;&#23450;&#24863;&#12290;&#23384;&#22312;&#25285;&#24551;&#23398;&#29983;&#21487;&#33021;&#20250;&#21033;&#29992;ChatGPT&#23436;&#25104;&#35838;&#22806;&#20316;&#19994;&#21644;&#32771;&#35797;&#65292;&#24182;&#33719;&#24471;&#26377;&#21033;&#30340;&#25104;&#32489;&#65292;&#32780;&#19981;&#30495;&#27491;&#33719;&#24471;&#30693;&#35782;&#12290;&#26412;&#25991;&#37319;&#29992;&#23450;&#37327;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;ChatGPT&#22312;&#22238;&#31572;&#26412;&#31185;&#35745;&#31639;&#26426;&#31185;&#23398;&#33539;&#22260;&#20869;&#30340;&#21508;&#31181;&#38382;&#39064;&#19978;&#20855;&#26377;&#39640;&#24230;&#30340;&#19981;&#21487;&#38752;&#24615;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#23398;&#29983;&#30450;&#30446;&#20381;&#36182;ChatGPT&#23436;&#25104;&#20316;&#19994;&#21644;&#32771;&#35797;&#21487;&#33021;&#20250;&#33258;&#27585;&#21069;&#31243;&#12290;&#25105;&#20204;&#22312;&#36825;&#20010;&#20998;&#26512;&#22522;&#30784;&#19978;&#25552;&#20986;&#20102;&#25945;&#24072;&#21644;&#23398;&#29983;&#22914;&#20309;&#22312;&#23398;&#26415;&#30028;&#20351;&#29992;ChatGPT&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT is an AI language model developed by OpenAI that can understand and generate human-like text. It can be used for a variety of use cases such as language generation, question answering, text summarization, chatbot development, language translation, sentiment analysis, content creation, personalization, text completion, and storytelling. While ChatGPT has garnered significant positive attention, it has also generated a sense of apprehension and uncertainty in academic circles. There is concern that students may leverage ChatGPT to complete take-home assignments and exams and obtain favorable grades without genuinely acquiring knowledge. This paper adopts a quantitative approach to demonstrate ChatGPT's high degree of unreliability in answering a diverse range of questions pertaining to topics in undergraduate computer science. Our analysis shows that students may risk self-sabotage by blindly depending on ChatGPT to complete assignments and exams. We build upon this analysis to p
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;SHAP&#26694;&#26550;&#21644;&#35270;&#35273;&#20808;&#39564;&#30693;&#35782;&#29983;&#25104;&#20840;&#38754;&#12289;&#26377;&#24847;&#20041;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#26356;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#12289;&#26356;&#39640;&#30340;&#35299;&#37322;&#34920;&#29616;&#21147;&#65292;&#24182;&#21487;&#20197;&#25512;&#24191;&#21040;&#20854;&#20182;&#27169;&#22411;&#19978;&#12290;</title><link>http://arxiv.org/abs/2304.14986</link><description>&lt;p&gt;
&#21033;&#29992;&#35821;&#20041;&#35270;&#35273;&#20808;&#39564;&#35299;&#37322;&#35270;&#35273;&#21644;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Interpreting Vision and Language Generative Models with Semantic Visual Priors. (arXiv:2304.14986v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14986
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;SHAP&#26694;&#26550;&#21644;&#35270;&#35273;&#20808;&#39564;&#30693;&#35782;&#29983;&#25104;&#20840;&#38754;&#12289;&#26377;&#24847;&#20041;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#26356;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#12289;&#26356;&#39640;&#30340;&#35299;&#37322;&#34920;&#29616;&#21147;&#65292;&#24182;&#21487;&#20197;&#25512;&#24191;&#21040;&#20854;&#20182;&#27169;&#22411;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24212;&#29992;&#20110;&#22270;&#20687;&#21040;&#25991;&#26412;&#27169;&#22411;&#26102;&#65292;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#36890;&#24120;&#25552;&#20379;&#36880;&#20010;&#26631;&#35760;&#30340;&#35299;&#37322;&#65292;&#21363;&#20026;&#25152;&#29983;&#25104;&#30340;&#24207;&#21015;&#20013;&#30340;&#27599;&#20010;&#26631;&#35760;&#35745;&#31639;&#35270;&#35273;&#35299;&#37322;&#12290;&#36825;&#20123;&#35299;&#37322;&#35745;&#31639;&#25104;&#26412;&#39640;&#65292;&#26080;&#27861;&#20840;&#38754;&#35299;&#37322;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#26576;&#31181;&#36817;&#20284;&#26041;&#27861;&#65292;&#26368;&#32456;&#20250;&#23548;&#33268;&#35823;&#23548;&#24615;&#30340;&#35299;&#37322;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;SHAP&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20801;&#35768;&#21033;&#29992;&#36755;&#20986;&#24207;&#21015;&#30340;&#21547;&#20041;&#34920;&#31034;&#29983;&#25104;&#20840;&#38754;&#12289;&#26377;&#24847;&#20041;&#30340;&#35299;&#37322;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#21033;&#29992;&#35270;&#35273;&#20027;&#24178;&#32593;&#32476;&#20013;&#30340;&#35821;&#20041;&#20808;&#39564;&#30693;&#35782;&#65292;&#25105;&#20204;&#25552;&#21462;&#20102;&#20219;&#24847;&#25968;&#37327;&#30340;&#29305;&#24449;&#65292;&#24182;&#33021;&#22815;&#22312;&#22823;&#35268;&#27169;&#27169;&#22411;&#19978;&#39640;&#25928;&#35745;&#31639;Shapley&#20540;&#65292;&#21516;&#26102;&#29983;&#25104;&#39640;&#24230;&#26126;&#30830;&#30340;&#35270;&#35273;&#35299;&#37322;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26356;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#19979;&#29983;&#25104;&#35821;&#20041;&#19978;&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;&#35299;&#37322;&#65292;&#24182;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#20854;&#20182;&#27169;&#22411;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
When applied to Image-to-text models, interpretability methods often provide token-by-token explanations namely, they compute a visual explanation for each token of the generated sequence. Those explanations are expensive to compute and unable to comprehensively explain the model's output. Therefore, these models often require some sort of approximation that eventually leads to misleading explanations. We develop a framework based on SHAP, that allows for generating comprehensive, meaningful explanations leveraging the meaning representation of the output sequence as a whole. Moreover, by exploiting semantic priors in the visual backbone, we extract an arbitrary number of features that allows the efficient computation of Shapley values on large-scale models, generating at the same time highly meaningful visual explanations. We demonstrate that our method generates semantically more expressive explanations than traditional methods at a lower compute cost and that it can be generalized o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38598;&#21512;Kemeny&#25237;&#31080;&#26426;&#21046;&#65292;&#25552;&#20986;&#20102;&#26368;&#20248;&#22810;&#25968;&#35268;&#21017;&#21644;&#25968;&#37327;&#21270;&#21338;&#24503;&#38647;&#24471;&#20934;&#21017;&#65292;&#24182;&#27604;&#36739;&#20102;&#20256;&#32479;Kemeny&#35268;&#21017;&#21644;3-&#26041;&#24335;Kemeny&#25237;&#31080;&#26041;&#26696;&#30340;&#20248;&#21155;&#12290;</title><link>http://arxiv.org/abs/2304.14980</link><description>&lt;p&gt;
&#38598;&#21512;Kemeny&#25237;&#31080;&#26426;&#21046;&#30340;&#26368;&#20248;&#22810;&#25968;&#35268;&#21017;&#21644;&#25968;&#37327;&#21270;&#21338;&#24503;&#38647;&#24471;&#20934;&#21017;
&lt;/p&gt;
&lt;p&gt;
Optimal majority rules and quantitative Condorcet properties of setwise Kemeny voting schemes. (arXiv:2304.14980v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14980
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38598;&#21512;Kemeny&#25237;&#31080;&#26426;&#21046;&#65292;&#25552;&#20986;&#20102;&#26368;&#20248;&#22810;&#25968;&#35268;&#21017;&#21644;&#25968;&#37327;&#21270;&#21338;&#24503;&#38647;&#24471;&#20934;&#21017;&#65292;&#24182;&#27604;&#36739;&#20102;&#20256;&#32479;Kemeny&#35268;&#21017;&#21644;3-&#26041;&#24335;Kemeny&#25237;&#31080;&#26041;&#26696;&#30340;&#20248;&#21155;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Kemeny&#38382;&#39064;&#26159;&#35745;&#31639;&#36873;&#20030;&#30340;&#20013;&#20301;&#25968;&#20849;&#35782;&#25490;&#21517;&#30340;&#37325;&#35201;&#38382;&#39064;&#65292;&#20855;&#26377;&#29983;&#29289;&#23398;&#21644;&#35745;&#31639;&#31038;&#20250;&#36873;&#25321;&#31561;&#37325;&#35201;&#24212;&#29992;&#65292;&#24182;&#36890;&#36807;Gilbert&#31561;&#20154;&#30340;&#26377;&#36259;&#30340;&#38598;&#21512;&#26041;&#27861;&#26368;&#36817;&#24471;&#21040;&#20102;&#25512;&#24191;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#32452;&#32467;&#26524;&#30830;&#23450;&#20102;&#20256;&#32479;Kemeny&#20013;&#20301;&#25968;&#38382;&#39064;&#30340;&#19968;&#33268;&#24615;&#23646;&#24615;&#21644;&#33879;&#21517;&#30340;Betzler&#31561;&#20154;&#30340;3/4&#22810;&#25968;&#35268;&#21017;&#30340;&#26368;&#20248;&#37327;&#21270;&#25193;&#23637;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#35814;&#32454;&#21015;&#20986;&#25968;&#37327;&#21270;&#20844;&#29702;&#23646;&#24615;&#65288;&#22914;Condorcet&#21644;Smith&#20934;&#21017;&#65292;5/6&#22810;&#25968;&#35268;&#21017;&#31561;&#65289;&#30340;&#35814;&#23613;&#28165;&#21333;&#65292;&#25105;&#20204;&#21457;&#29616;&#23545;&#20110;3&#20010;&#20505;&#36873;&#20154;&#30340;&#23376;&#38598;&#30340;&#20248;&#32988;&#32773;&#20043;&#38388;&#19981;&#20165;&#32771;&#34385;&#25104;&#23545;&#27604;&#36739;&#36824;&#35201;&#32771;&#34385;&#19981;&#19968;&#33268;&#24615;&#65292;&#30001;3-&#26041;&#24335;Kemeny&#35268;&#21017;&#24341;&#20986;&#30340;3-&#26041;&#24335;Kendall-tau&#36317;&#31163;&#35825;&#23548;&#30340;3-&#26041;&#24335;Kemeny&#25237;&#31080;&#26041;&#26696;&#19982;&#20256;&#32479;Kemeny&#35268;&#21017;&#30456;&#27604;&#20855;&#26377;&#26377;&#36259;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
The important Kemeny problem, which consists of computing median consensus rankings of an election with respect to the Kemeny voting rule, admits important applications in biology and computational social choice and was generalized recently via an interesting setwise approach by Gilbert et. al. Our first results establish optimal quantitative extensions of the Unanimity property and the well-known $3/4$-majority rule of Betzler et al. for the classical Kemeny median problem. Moreover, by elaborating an exhaustive list of quantified axiomatic properties (such as the Condorcet and Smith criteria, the $5/6$-majority rule, etc.) of the $3$-wise Kemeny rule where not only pairwise comparisons but also the discordance between the winners of subsets of three candidates are also taken into account, we come to the conclusion that the $3$-wise Kemeny voting scheme induced by the $3$-wise Kendall-tau distance presents interesting advantages in comparison with the classical Kemeny rule. For exampl
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37327;&#23376;Dempster&#32452;&#21512;&#35268;&#21017;&#65288;QDRC&#65289;&#65292;&#21033;&#29992;Toffoli&#38376;&#23454;&#29616;&#65292;&#33021;&#22815;&#24212;&#23545;Dempster&#32452;&#21512;&#35268;&#21017;&#35745;&#31639;&#22797;&#26434;&#24230;&#25351;&#25968;&#32423;&#22686;&#38271;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.14966</link><description>&lt;p&gt;
&#19968;&#31181;&#26032;&#30340;&#37327;&#23376;Dempster&#32452;&#21512;&#35268;&#21017;
&lt;/p&gt;
&lt;p&gt;
A New Quantum Dempster Rule of Combination. (arXiv:2304.14966v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14966
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37327;&#23376;Dempster&#32452;&#21512;&#35268;&#21017;&#65288;QDRC&#65289;&#65292;&#21033;&#29992;Toffoli&#38376;&#23454;&#29616;&#65292;&#33021;&#22815;&#24212;&#23545;Dempster&#32452;&#21512;&#35268;&#21017;&#35745;&#31639;&#22797;&#26434;&#24230;&#25351;&#25968;&#32423;&#22686;&#38271;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Dempster&#32452;&#21512;&#35268;&#21017;&#65288;DRC&#65289;&#34987;&#24191;&#27867;&#29992;&#20110;&#26234;&#33021;&#20449;&#24687;&#31995;&#32479;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#25512;&#29702;&#65292;&#26368;&#36817;&#20063;&#34987;&#25512;&#24191;&#21040;&#20102;&#22797;&#26434;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#35782;&#21035;&#26694;&#26550;&#20803;&#32032;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;Dempster&#32452;&#21512;&#35268;&#21017;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#20250;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36890;&#36807;Toffoli&#38376;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37327;&#23376;Dempster&#32452;&#21512;&#35268;&#21017;&#65288;QDRC&#65289;&#12290;QDRC&#32452;&#21512;&#36807;&#31243;&#23436;&#20840;&#20351;&#29992;&#37327;&#23376;&#30005;&#36335;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dempster rule of combination (DRC) is widely used for uncertainty reasoning in intelligent information system, which is generalized to complex domain recently. However, as the increase of identification framework elements, the computational complexity of Dempster Rule of Combination increases exponentially. To address this issue, we propose a novel quantum Dempster rule of combination (QDRC) by means of Toffoli gate. The QDRC combination process is completely implemented using quantum circuits.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#34701;&#21512;&#22312;&#19981;&#21516;&#27169;&#24577;&#19978;&#35757;&#32451;&#30340;transformer&#36827;&#34892;&#22810;&#27169;&#24577;&#27169;&#22411;&#34701;&#21512;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#21442;&#25968;&#26377;&#25928;&#30340;&#27169;&#24577;&#19981;&#21487;&#30693;&#26550;&#26500;&#65292;&#24418;&#25104;&#26377;&#25928;&#30340;&#35757;&#32451;&#37197;&#26041;&#12290;</title><link>http://arxiv.org/abs/2304.14933</link><description>&lt;p&gt;
&#19968;&#39033;&#22810;&#27169;&#24577;&#27169;&#22411;&#34701;&#21512;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Empirical Study of Multimodal Model Merging. (arXiv:2304.14933v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14933
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#34701;&#21512;&#22312;&#19981;&#21516;&#27169;&#24577;&#19978;&#35757;&#32451;&#30340;transformer&#36827;&#34892;&#22810;&#27169;&#24577;&#27169;&#22411;&#34701;&#21512;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#21442;&#25968;&#26377;&#25928;&#30340;&#27169;&#24577;&#19981;&#21487;&#30693;&#26550;&#26500;&#65292;&#24418;&#25104;&#26377;&#25928;&#30340;&#35757;&#32451;&#37197;&#26041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#34701;&#21512;&#65288;&#20363;&#22914;&#25554;&#20540;&#25110;&#20219;&#21153;&#31639;&#26415;&#65289;&#23558;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#35757;&#32451;&#30340;&#22810;&#20010;&#27169;&#22411;&#21512;&#24182;&#20197;&#29983;&#25104;&#22810;&#20219;&#21153;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#25216;&#26415;&#22312;&#20808;&#21069;&#30340;&#30740;&#31350;&#20013;&#24050;&#32463;&#34987;&#35777;&#26126;&#25104;&#21151;&#65292;&#20854;&#20013;&#27169;&#22411;&#26159;&#22312;&#30456;&#20284;&#30340;&#20219;&#21153;&#21644;&#30456;&#21516;&#30340;&#21021;&#22987;&#21270;&#19979;&#35757;&#32451;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#22312;&#19981;&#21516;&#27169;&#24577;&#19978;&#35757;&#32451;&#30340;transformer&#36827;&#34892;&#34701;&#21512;&#65292;&#23558;&#27492;&#27010;&#24565;&#25193;&#23637;&#21040;&#22810;&#27169;&#24577;&#35774;&#32622;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#38024;&#23545;&#19968;&#20010;&#26032;&#39062;&#30340;&#30446;&#26631;&#36827;&#34892;&#30740;&#31350;&#65292;&#22312;&#35813;&#30446;&#26631;&#20013;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#35270;&#35273;&#12289;&#35821;&#35328;&#21644;&#36328;&#27169;&#24577;&#30340;transformer&#21512;&#24182;&#21040;&#29305;&#23450;&#27169;&#24577;&#30340;&#26550;&#26500;&#20013;&#65292;&#20197;&#21019;&#24314;&#19968;&#20010;&#21442;&#25968;&#26377;&#25928;&#30340;&#27169;&#24577;&#19981;&#21487;&#30693;&#26550;&#26500;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#24433;&#21709;&#27169;&#22411;&#34701;&#21512;&#21518;&#24615;&#33021;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#21253;&#25324;&#21021;&#22987;&#21270;&#12289;&#34701;&#21512;&#26426;&#21046;&#21644;&#27169;&#22411;&#26550;&#26500;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#24471;&#20986;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#35757;&#32451;&#37197;&#26041;&#65292;&#21487;&#20197;&#36890;&#36807;&#27169;&#22411;&#34701;&#21512;&#26469;&#21305;&#37197;&#27169;&#24577;&#19981;&#21487;&#30693;&#22522;&#32447;&#30340;&#24615;&#33021;&#65288;&#21363;&#20174;&#22836;&#24320;&#22987;&#39044;&#35757;&#32451;&#65289;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#20379;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model merging (e.g., via interpolation or task arithmetic) fuses multiple models trained on different tasks to generate a multi-task solution. The technique has been proven successful in previous studies, where the models are trained on similar tasks and with the same initialization. In this paper, we expand on this concept to a multimodal setup by merging transformers trained on different modalities. Furthermore, we conduct our study for a novel goal where we can merge vision, language, and cross-modal transformers of a modality-specific architecture to create a parameter-efficient modality-agnostic architecture. Through comprehensive experiments, we systematically investigate the key factors impacting model performance after merging, including initialization, merging mechanisms, and model architectures. Our analysis leads to an effective training recipe for matching the performance of the modality-agnostic baseline (i.e. pre-trained from scratch) via model merging. Our code is availa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#32771;&#34385;&#30456;&#20284;&#26679;&#26412;&#19982;&#25935;&#24863;&#24615;&#24863;&#30693;&#65292;&#26377;&#25928;&#22320;&#35745;&#31639;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.14925</link><description>&lt;p&gt;
&#22522;&#20110;&#30456;&#20284;&#24615;&#21644;&#25935;&#24863;&#24615;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Uncertainty Aware Neural Network from Similarity and Sensitivity. (arXiv:2304.14925v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14925
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#32771;&#34385;&#30456;&#20284;&#26679;&#26412;&#19982;&#25935;&#24863;&#24615;&#24863;&#30693;&#65292;&#26377;&#25928;&#22320;&#35745;&#31639;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#19981;&#30830;&#23450;&#24615;&#30340;&#30740;&#31350;&#24050;&#26377;&#22810;&#31181;&#26041;&#27861;&#34987;&#25552;&#20986;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#30340;&#26041;&#27861;&#37117;&#26159;&#22522;&#20110;&#24378;&#20551;&#35774;&#30340;&#65292;&#24182;&#19988;&#22312;&#26576;&#20123;&#36755;&#20837;&#39046;&#22495;&#20013;&#34920;&#29616;&#19981;&#20339;&#65292;&#32780;&#34920;&#29616;&#19981;&#20339;&#30340;&#21407;&#22240;&#23578;&#19981;&#26126;&#30830;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#30456;&#20284;&#26679;&#26412;&#19982;&#25935;&#24863;&#24615;&#24863;&#30693;&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#26041;&#27861;&#12290;&#22312;&#36825;&#31181;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#35757;&#32451;&#19968;&#20010;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#28857;&#39044;&#27979;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35745;&#31639;&#39044;&#27979;&#20540;&#21644;&#30446;&#26631;&#20540;&#20043;&#38388;&#30340;&#32477;&#23545;&#24046;&#20540;&#65292;&#24182;&#35757;&#32451;&#21478;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#26469;&#39044;&#27979;&#36825;&#20123;&#32477;&#23545;&#24046;&#20540;&#25110;&#32773;&#32477;&#23545;&#35823;&#24046;&#12290;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#36739;&#39640;&#30340;&#39046;&#22495;&#34920;&#31034;&#39640;&#24230;&#19981;&#30830;&#23450;&#24615;&#12290;&#22312;&#25509;&#19979;&#26469;&#30340;&#27493;&#39588;&#20013;&#65292;&#25105;&#20204;&#36880;&#20010;&#36873;&#25321;&#35757;&#32451;&#38598;&#20013;&#30340;&#27599;&#20010;&#26679;&#26412;&#65292;&#24182;&#35745;&#31639;&#39044;&#27979;&#21644;&#35823;&#24046;&#30340;&#25935;&#24863;&#24615;&#12290;&#28982;&#21518;&#25105;&#20204;&#36873;&#25321;&#25935;&#24863;&#24615;&#32771;&#34385;&#30340;&#30456;&#20284;&#26679;&#26412;&#24182;&#20445;&#23384;&#30456;&#20284;&#26679;&#26412;&#30340;&#32034;&#24341;&#12290;
&lt;/p&gt;
&lt;p&gt;
Researchers have proposed several approaches for neural network (NN) based uncertainty quantification (UQ). However, most of the approaches are developed considering strong assumptions. Uncertainty quantification algorithms often perform poorly in an input domain and the reason for poor performance remains unknown. Therefore, we present a neural network training method that considers similar samples with sensitivity awareness in this paper. In the proposed NN training method for UQ, first, we train a shallow NN for the point prediction. Then, we compute the absolute differences between prediction and targets and train another NN for predicting those absolute differences or absolute errors. Domains with high average absolute errors represent a high uncertainty. In the next step, we select each sample in the training set one by one and compute both prediction and error sensitivities. Then we select similar samples with sensitivity consideration and save indexes of similar samples. The ra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;&#24615;&#24341;&#23548;&#30340;&#36890;&#36947;&#36873;&#25321;&#26694;&#26550;&#65288;ICS&#65289;&#29992;&#20110;&#21496;&#26426;&#30130;&#21171;&#26816;&#27979;&#20219;&#21153;&#65292;&#36890;&#36807;&#36880;&#27493;&#36873;&#25321;&#20851;&#38190;&#30340;&#36129;&#29486;&#36890;&#36947;&#65292;&#22312;&#25552;&#39640;&#21496;&#26426;&#30130;&#21171;&#26816;&#27979;&#20934;&#30830;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.14920</link><description>&lt;p&gt;
&#19968;&#31181;&#36890;&#36807;&#35299;&#37322;&#24615;&#24341;&#23548;&#36827;&#34892;&#21496;&#26426;&#30130;&#21171;&#26816;&#27979;&#30340;&#33041;&#30005;&#20449;&#21495;&#36890;&#36947;&#36873;&#25321;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
An EEG Channel Selection Framework for Driver Drowsiness Detection via Interpretability Guidance. (arXiv:2304.14920v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14920
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;&#24615;&#24341;&#23548;&#30340;&#36890;&#36947;&#36873;&#25321;&#26694;&#26550;&#65288;ICS&#65289;&#29992;&#20110;&#21496;&#26426;&#30130;&#21171;&#26816;&#27979;&#20219;&#21153;&#65292;&#36890;&#36807;&#36880;&#27493;&#36873;&#25321;&#20851;&#38190;&#30340;&#36129;&#29486;&#36890;&#36947;&#65292;&#22312;&#25552;&#39640;&#21496;&#26426;&#30130;&#21171;&#26816;&#27979;&#20934;&#30830;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30130;&#21171;&#39550;&#39542;&#23545;&#34892;&#36710;&#23433;&#20840;&#26377;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#24433;&#21709;&#65292;&#20419;&#20351;&#20154;&#20204;&#36843;&#20999;&#38656;&#35201;&#36827;&#34892;&#21496;&#26426;&#30130;&#21171;&#26816;&#27979;&#12290;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#20449;&#21495;&#33021;&#22815;&#20934;&#30830;&#21453;&#26144;&#31934;&#31070;&#30130;&#21171;&#29366;&#24577;&#65292;&#22240;&#27492;&#22312;&#30130;&#21171;&#30417;&#27979;&#26041;&#38754;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#21407;&#22987;&#30340;EEG&#25968;&#25454;&#26412;&#36136;&#19978;&#26159;&#22024;&#26434;&#19988;&#20887;&#20313;&#30340;&#65292;&#36825;&#34987;&#29616;&#26377;&#30340;&#30740;&#31350;&#24573;&#35270;&#20102;&#65292;&#36825;&#20123;&#30740;&#31350;&#20165;&#20351;&#29992;&#21333;&#36890;&#36947;EEG&#25968;&#25454;&#25110;&#20840;&#22836;&#36890;&#36947;EEG&#25968;&#25454;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#65292;&#23548;&#33268;&#21496;&#26426;&#30130;&#21171;&#26816;&#27979;&#24615;&#33021;&#26377;&#38480;&#12290;&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;&#24615;&#24341;&#23548;&#30340;&#36890;&#36947;&#36873;&#25321;&#26694;&#26550;&#65288;ICS&#65289;&#29992;&#20110;&#21496;&#26426;&#30130;&#21171;&#26816;&#27979;&#20219;&#21153;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#36890;&#36807;&#35299;&#37322;&#24615;&#24341;&#23548;&#36880;&#27493;&#36873;&#25321;&#20851;&#38190;&#30340;&#36129;&#29486;&#36890;&#36947;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;&#31532;&#19968;&#38454;&#27573;&#20351;&#29992;&#20840;&#22836;&#36890;&#36947;EEG&#25968;&#25454;&#35757;&#32451;&#19968;&#20010;&#25945;&#24072;&#32593;&#32476;&#65292;&#28982;&#21518;&#23545;&#35757;&#32451;&#22909;&#30340;&#25945;&#24072;&#27169;&#22411;&#24212;&#29992;&#31867;&#28608;&#27963;&#26144;&#23556;&#65288;CAM&#65289;&#26469;&#31361;&#20986;&#26174;&#31034;&#23545;&#20110;&#30130;&#21171;&#26816;&#27979;&#20219;&#21153;&#26377;&#39640;&#36129;&#29486;&#30340;&#36890;&#36947;&#12290;&#22522;&#20110;&#36873;&#25321;&#30340;&#20851;&#38190;&#36890;&#36947;&#65292;&#25105;&#20204;&#22312;&#31532;&#20108;&#38454;&#27573;&#35757;&#32451;&#19968;&#20010;&#26356;&#36731;&#37327;&#32423;&#30340;&#23398;&#29983;&#32593;&#32476;&#65292;&#23427;&#20351;&#29992;&#30340;&#36890;&#36947;&#25968;&#37327;&#23569;&#24471;&#22810;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#22312;&#20004;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;ICS&#26694;&#26550;&#22312;&#25552;&#39640;&#21496;&#26426;&#30130;&#21171;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Drowsy driving has a crucial influence on driving safety, creating an urgent demand for driver drowsiness detection. Electroencephalogram (EEG) signal can accurately reflect the mental fatigue state and thus has been widely studied in drowsiness monitoring. However, the raw EEG data is inherently noisy and redundant, which is neglected by existing works that just use single-channel EEG data or full-head channel EEG data for model training, resulting in limited performance of driver drowsiness detection. In this paper, we are the first to propose an Interpretability-guided Channel Selection (ICS) framework for the driver drowsiness detection task. Specifically, we design a two-stage training strategy to progressively select the key contributing channels with the guidance of interpretability. We first train a teacher network in the first stage using full-head channel EEG data. Then we apply the class activation mapping (CAM) to the trained teacher model to highlight the high-contributing
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#25913;&#21464;&#36755;&#20837;&#34920;&#31034;&#21644;&#20215;&#20540;&#25439;&#22833;&#65292;&#20351;&#26827;&#30424;&#28216;&#25103;&#24615;&#33021;&#33719;&#24471;&#20102;&#39640;&#36798;180 Elo&#28857;&#30340;&#22686;&#24378;</title><link>http://arxiv.org/abs/2304.14918</link><description>&lt;p&gt;
&#34920;&#31034;&#24456;&#37325;&#35201;&#65306;&#26827;&#30424;&#28216;&#25103;&#23545;&#35270;&#35273;Transformer&#25552;&#20986;&#20102;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Representation Matters: The Game of Chess Poses a Challenge to Vision Transformers. (arXiv:2304.14918v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14918
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#25913;&#21464;&#36755;&#20837;&#34920;&#31034;&#21644;&#20215;&#20540;&#25439;&#22833;&#65292;&#20351;&#26827;&#30424;&#28216;&#25103;&#24615;&#33021;&#33719;&#24471;&#20102;&#39640;&#36798;180 Elo&#28857;&#30340;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;Transformer&#24050;&#32463;&#25104;&#20026;&#20102;&#8220;AI&#30340;&#29790;&#22763;&#20891;&#20992;&#8221;&#65292;&#20294;&#27809;&#26377;&#20154;&#25361;&#25112;&#23427;&#20204;&#21435;&#25484;&#25569;&#35937;&#26827;&#36825;&#20010;&#32463;&#20856;&#30340;AI&#22522;&#20934;&#12290;&#20294;&#20165;&#20165;&#20351;&#29992;&#35270;&#35273;Transformer&#65288;ViT&#65289;&#22312;AlphaZero&#20013;&#26080;&#27861;&#25484;&#25569;&#35937;&#26827;&#65292;&#20027;&#35201;&#26159;&#22240;&#20026;ViT&#22826;&#24930;&#20102;&#12290;&#21363;&#20351;&#20351;&#29992;MobileNet&#21644;NextViT&#30340;&#32452;&#21512;&#20351;&#23427;&#20204;&#26356;&#26377;&#25928;&#65292;&#20063;&#26080;&#27861;&#20987;&#36133;&#23454;&#38469;&#19978;&#26356;&#37325;&#35201;&#30340;&#19996;&#35199;&#65306;&#31616;&#21333;&#25913;&#21464;&#36755;&#20837;&#34920;&#31034;&#21644;&#20215;&#20540;&#25439;&#22833;&#65292;&#20174;&#32780;&#33719;&#24471;&#39640;&#36798;180 Elo&#28857;&#30340;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;
While transformers have gained the reputation as the "Swiss army knife of AI", no one has challenged them to master the game of chess, one of the classical AI benchmarks. Simply using vision transformers (ViTs) within AlphaZero does not master the game of chess, mainly because ViTs are too slow. Even making them more efficient using a combination of MobileNet and NextViT does not beat what actually matters: a simple change of the input representation and value loss, resulting in a greater boost of up to 180 Elo points over AlphaZero.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#33033;&#25615;&#27874;&#20998;&#26512;&#39044;&#27979;&#34880;&#21387;&#30340;&#20219;&#21153;&#65292;&#21457;&#29616;&#35768;&#22810;&#35770;&#25991;&#24120;&#24120;&#20986;&#29616;&#25968;&#25454;&#27844;&#28431;&#21644;&#23545;&#20219;&#21153;&#21450;&#39044;&#22788;&#29702;&#27493;&#39588;&#30340;&#19981;&#20999;&#23454;&#38469;&#38480;&#21046;&#12290;&#25552;&#20986;&#20102;&#26032;&#30340;&#24037;&#20855;&#26469;&#30830;&#23450;&#36755;&#20837;&#20449;&#21495;&#65288;&#22914;PPG&#65289;&#26159;&#21542;&#33021;&#39044;&#27979;&#25152;&#38656;&#30340;&#37327;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.14916</link><description>&lt;p&gt;
&#36890;&#36807;&#33033;&#25615;&#27874;&#20998;&#26512;&#20272;&#35745;&#34880;&#21387;&#30340;&#25361;&#25112;&#65306;&#33021;&#21542;&#25215;&#21463;&#65311;
&lt;/p&gt;
&lt;p&gt;
"Can't Take the Pressure?": Examining the Challenges of Blood Pressure Estimation via Pulse Wave Analysis. (arXiv:2304.14916v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#33033;&#25615;&#27874;&#20998;&#26512;&#39044;&#27979;&#34880;&#21387;&#30340;&#20219;&#21153;&#65292;&#21457;&#29616;&#35768;&#22810;&#35770;&#25991;&#24120;&#24120;&#20986;&#29616;&#25968;&#25454;&#27844;&#28431;&#21644;&#23545;&#20219;&#21153;&#21450;&#39044;&#22788;&#29702;&#27493;&#39588;&#30340;&#19981;&#20999;&#23454;&#38469;&#38480;&#21046;&#12290;&#25552;&#20986;&#20102;&#26032;&#30340;&#24037;&#20855;&#26469;&#30830;&#23450;&#36755;&#20837;&#20449;&#21495;&#65288;&#22914;PPG&#65289;&#26159;&#21542;&#33021;&#39044;&#27979;&#25152;&#38656;&#30340;&#37327;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#21487;&#31359;&#25140;&#20256;&#24863;&#22120;&#25968;&#25454;&#65288;&#22914;&#20809;&#30005;&#23481;&#31215;&#27874;&#24418;&#22270;[PPG]&#65289;&#25512;&#23548;&#20581;&#24247;&#37327;&#24230;&#65288;&#22914;&#33889;&#33796;&#31958;&#27700;&#24179;&#25110;&#34880;&#21387;&#65289;&#26159;&#30446;&#21069;&#38750;&#24120;&#28909;&#38376;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#35813;&#25216;&#26415;&#21487;&#20197;&#23545;&#20581;&#24247;&#31579;&#26597;&#12289;&#24930;&#24615;&#30149;&#31649;&#29702;&#21644;&#36828;&#31243;&#30417;&#27979;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#20174;PPG&#33033;&#25615;&#27874;&#20998;&#26512;&#20013;&#39044;&#27979;&#34880;&#21387;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#35768;&#22810;&#35770;&#25991;&#23481;&#26131;&#20986;&#29616;&#25968;&#25454;&#27844;&#28431;&#20197;&#21450;&#23545;&#20219;&#21153;&#21644;&#39044;&#22788;&#29702;&#27493;&#39588;&#30340;&#19981;&#20999;&#23454;&#38469;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#22871;&#24037;&#20855;&#26469;&#30830;&#23450;&#36755;&#20837;&#20449;&#21495;&#65288;&#22914;PPG&#65289;&#26159;&#21542;&#33021;&#22815;&#33391;&#22909;&#22320;&#39044;&#27979;&#25152;&#38656;&#30340;&#37327;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of observed wearable sensor data (e.g., photoplethysmograms [PPG]) to infer health measures (e.g., glucose level or blood pressure) is a very active area of research. Such technology can have a significant impact on health screening, chronic disease management and remote monitoring. A common approach is to collect sensor data and corresponding labels from a clinical grade device (e.g., blood pressure cuff), and train deep learning models to map one to the other. Although well intentioned, this approach often ignores a principled analysis of whether the input sensor data has enough information to predict the desired metric. We analyze the task of predicting blood pressure from PPG pulse wave analysis. Our review of the prior work reveals that many papers fall prey data leakage, and unrealistic constraints on the task and the preprocessing steps. We propose a set of tools to help determine if the input signal in question (e.g., PPG) is indeed a good predictor of the desired label
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#33258;&#30417;&#30563;&#34920;&#31034;&#27861;&#35782;&#21035;&#20154;&#31867;&#27963;&#21160;&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#22823;&#37327;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#30340;&#22522;&#30784;&#19978;&#33719;&#24471;&#20102;&#36739;&#24378;&#24615;&#33021;&#65292;&#24182;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.14912</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#30417;&#30563;&#34920;&#31034;&#27861;&#35782;&#21035;&#31359;&#25140;&#25968;&#25454;&#30340;&#20154;&#31867;&#27963;&#21160;
&lt;/p&gt;
&lt;p&gt;
Human Activity Recognition Using Self-Supervised Representations of Wearable Data. (arXiv:2304.14912v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14912
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#33258;&#30417;&#30563;&#34920;&#31034;&#27861;&#35782;&#21035;&#20154;&#31867;&#27963;&#21160;&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#22823;&#37327;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#30340;&#22522;&#30784;&#19978;&#33719;&#24471;&#20102;&#36739;&#24378;&#24615;&#33021;&#65292;&#24182;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#20329;&#25140;&#20256;&#24863;&#22120;&#36827;&#34892;&#33258;&#21160;&#21270;&#21644;&#20934;&#30830;&#30340;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#65288;HAR&#65289;&#21487;&#20197;&#23454;&#29616;&#23454;&#29992;&#21644;&#32463;&#27982;&#25928;&#30410;&#30340;&#26085;&#24120;&#29983;&#27963;&#27963;&#21160;&#65288;ADL&#65289;&#30340;&#36828;&#31243;&#30417;&#27979;&#65292;&#36825;&#20123;&#27963;&#21160;&#24050;&#34987;&#35777;&#26126;&#21487;&#25552;&#20379;&#36328;&#22810;&#20010;&#27835;&#30103;&#39046;&#22495;&#30340;&#20020;&#24202;&#27934;&#35265;&#12290;&#20934;&#30830;&#22320;&#35782;&#21035;&#20154;&#31867;&#27963;&#21160;&#65288;HAR&#65289;&#30340;&#31639;&#27861;&#30340;&#21457;&#23637;&#21463;&#21040;&#32570;&#20047;&#22823;&#22411;&#30495;&#23454;&#19990;&#30028;&#26631;&#35760;&#25968;&#25454;&#38598;&#30340;&#38459;&#30861;&#12290;&#27492;&#22806;&#65292;&#31639;&#27861;&#24456;&#23569;&#33021;&#22815;&#36229;&#36234;&#23427;&#20204;&#21407;&#22411;&#19978;&#30340;&#29305;&#23450;&#20256;&#24863;&#22120;&#65292;&#24341;&#21457;&#20102;&#26377;&#20851;&#26159;&#21542;&#21487;&#33021;&#22522;&#20110;&#21152;&#36895;&#24230;&#35745;&#30340;HAR&#30340;&#20105;&#35758;[Tong&#31561;&#20154;&#65292;2020]&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20845;&#31867;HAR&#27169;&#22411;&#65292;&#24403;&#22312;&#35757;&#32451;&#26399;&#38388;&#26410;&#35265;&#36807;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#26102;&#65292;&#20855;&#26377;&#36739;&#24378;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22522;&#20110;&#22312;&#22823;&#22411;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#19978;&#23398;&#20064;&#30340;&#20923;&#32467;&#33258;&#30417;&#30563;&#34920;&#31034;&#27861;&#65292;&#32467;&#21512;&#20855;&#26377;&#26102;&#38388;&#24179;&#28369;&#30340;&#27973;&#23618;&#22810;&#23618;&#24863;&#30693;&#22120;&#12290;&#35813;&#27169;&#22411;&#22312;Capture24&#25968;&#25454;&#38598;[$\kappa$=0.86]&#20869;&#37096;&#25968;&#25454;&#38598;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#20998;&#24067;&#22806;&#65288;OOD&#65289;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Automated and accurate human activity recognition (HAR) using body-worn sensors enables practical and cost efficient remote monitoring of Activity of DailyLiving (ADL), which are shown to provide clinical insights across multiple therapeutic areas. Development of accurate algorithms for human activity recognition(HAR) is hindered by the lack of large real-world labeled datasets. Furthermore, algorithms seldom work beyond the specific sensor on which they are prototyped, prompting debate about whether accelerometer-based HAR is even possible [Tong et al., 2020]. Here we develop a 6-class HAR model with strong performance when evaluated on real-world datasets not seen during training. Our model is based on a frozen self-supervised representation learned on a large unlabeled dataset, combined with a shallow multi-layer perceptron with temporal smoothing. The model obtains in-dataset state-of-the art performance on the Capture24 dataset ($\kappa= 0.86$). Out-of-distribution (OOD) performan
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#35821;&#20041;&#27010;&#24565;&#34920;&#31034;&#22312;CNN&#20013;&#31283;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#20026;&#23454;&#29616;&#24378;&#22823;&#30340;&#21487;&#35299;&#37322;&#24615;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;&#26412;&#25991;&#20851;&#27880;&#35745;&#31639;&#26426;&#35270;&#35273;CNN&#20013;&#27010;&#24565;&#34920;&#31034;&#30340;&#31283;&#23450;&#24615;&#65306;&#27010;&#24565;&#26816;&#32034;&#31283;&#23450;&#24615;&#21644;&#27010;&#24565;&#24402;&#23646;&#31283;&#23450;&#24615;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#20197;&#35299;&#20915;&#27010;&#24565;&#26816;&#32034;&#31283;&#23450;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.14864</link><description>&lt;p&gt;
&#35780;&#20272;CNN&#20013;&#35821;&#20041;&#27010;&#24565;&#34920;&#31034;&#30340;&#31283;&#23450;&#24615;&#65292;&#20197;&#23454;&#29616;&#24378;&#22823;&#30340;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Stability of Semantic Concept Representations in CNNs for Robust Explainability. (arXiv:2304.14864v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14864
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#35821;&#20041;&#27010;&#24565;&#34920;&#31034;&#22312;CNN&#20013;&#31283;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#20026;&#23454;&#29616;&#24378;&#22823;&#30340;&#21487;&#35299;&#37322;&#24615;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;&#26412;&#25991;&#20851;&#27880;&#35745;&#31639;&#26426;&#35270;&#35273;CNN&#20013;&#27010;&#24565;&#34920;&#31034;&#30340;&#31283;&#23450;&#24615;&#65306;&#27010;&#24565;&#26816;&#32034;&#31283;&#23450;&#24615;&#21644;&#27010;&#24565;&#24402;&#23646;&#31283;&#23450;&#24615;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#20197;&#35299;&#20915;&#27010;&#24565;&#26816;&#32034;&#31283;&#23450;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#20013;&#65292;&#20998;&#26512;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#20013;&#35821;&#20041;&#27010;&#24565;&#30340;&#34920;&#31034;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#26041;&#27861;&#12290;&#20854;&#21160;&#26426;&#26159;&#22240;&#20026;&#21508;&#20010;&#39046;&#22495;&#22914;&#33258;&#21160;&#39550;&#39542;&#31561;&#23433;&#20840;&#20851;&#38190;&#30340;&#22522;&#20110;AI&#30340;&#31995;&#32479;&#38656;&#35201;&#36879;&#26126;&#24230;&#12290;&#28982;&#32780;&#65292;&#35201;&#23558;&#36825;&#20123;&#27010;&#24565;&#34920;&#31034;&#29992;&#20110;&#23433;&#20840;&#30456;&#20851;&#30446;&#30340;&#65292;&#20363;&#22914;&#26816;&#26597;&#25110;&#38169;&#35823;&#26816;&#32034;&#65292;&#36825;&#20123;&#34920;&#31034;&#24517;&#39035;&#20855;&#26377;&#39640;&#36136;&#37327;&#65292;&#29305;&#21035;&#26159;&#31283;&#23450;&#24615;&#12290;&#26412;&#25991;&#20851;&#27880;&#35745;&#31639;&#26426;&#35270;&#35273;CNN&#20013;&#27010;&#24565;&#34920;&#31034;&#30340;&#31283;&#23450;&#24615;&#65306;&#27010;&#24565;&#26816;&#32034;&#31283;&#23450;&#24615;&#21644;&#27010;&#24565;&#24402;&#23646;&#31283;&#23450;&#24615;&#12290;&#20197;&#30446;&#26631;&#26816;&#27979;&#65288;OD&#65289;CNN&#30340;&#20107;&#21518;&#21487;&#35299;&#37322;&#24615;&#26694;&#26550;&#20026;&#25351;&#23548;&#30446;&#26631;&#65292;&#25104;&#21151;&#22320;&#23558;&#29616;&#26377;&#30340;&#27010;&#24565;&#20998;&#26512;&#65288;CA&#65289;&#26041;&#27861;&#24212;&#29992;&#20110;&#20854;&#19978;&#12290;&#20026;&#20102;&#35299;&#20915;&#27010;&#24565;&#26816;&#32034;&#30340;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#32771;&#34385;&#27010;&#24565;&#20998;&#31163;&#21644;&#19968;&#33268;&#24615;&#65292;&#19982;&#23618;&#21644;&#27010;&#24565;&#34920;&#31034;&#26080;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
Analysis of how semantic concepts are represented within Convolutional Neural Networks (CNNs) is a widely used approach in Explainable Artificial Intelligence (XAI) for interpreting CNNs. A motivation is the need for transparency in safety-critical AI-based systems, as mandated in various domains like automated driving. However, to use the concept representations for safety-relevant purposes, like inspection or error retrieval, these must be of high quality and, in particular, stable. This paper focuses on two stability goals when working with concept representations in computer vision CNNs: stability of concept retrieval and of concept attribution. The guiding use-case is a post-hoc explainability framework for object detection (OD) CNNs, towards which existing concept analysis (CA) methods are successfully adapted. To address concept retrieval stability, we propose a novel metric that considers both concept separation and consistency, and is agnostic to layer and concept representati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#22825;&#27668;&#26465;&#20214;&#22797;&#26434;&#20849;&#29616;&#20381;&#36182;&#20851;&#31995;&#30340;&#22810;&#26631;&#31614;&#35782;&#21035;&#27169;&#22411;MASK-CNN-Transformer&#65292;&#35813;&#27169;&#22411;&#32467;&#21512;&#20102;CNN&#21644;Transformer&#65292;&#24182;&#21033;&#29992;MASK&#26426;&#21046;&#20197;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.14857</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#23454;&#26102;&#22810;&#26631;&#31614;&#22825;&#27668;&#35782;&#21035;&#30340;MASK-CNN-Transformer&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MASK-CNN-Transformer For Real-Time Multi-Label Weather Recognition. (arXiv:2304.14857v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14857
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#22825;&#27668;&#26465;&#20214;&#22797;&#26434;&#20849;&#29616;&#20381;&#36182;&#20851;&#31995;&#30340;&#22810;&#26631;&#31614;&#35782;&#21035;&#27169;&#22411;MASK-CNN-Transformer&#65292;&#35813;&#27169;&#22411;&#32467;&#21512;&#20102;CNN&#21644;Transformer&#65292;&#24182;&#21033;&#29992;MASK&#26426;&#21046;&#20197;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22825;&#27668;&#35782;&#21035;&#23545;&#20110;&#35768;&#22810;&#23454;&#38469;&#29983;&#27963;&#24212;&#29992;&#65292;&#21253;&#25324;&#20132;&#36890;&#23433;&#20840;&#12289;&#29615;&#22659;&#21644;&#27668;&#35937;&#26041;&#38754;&#37117;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#25903;&#25345;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29616;&#26377;&#30340;&#30456;&#20851;&#24037;&#20316;&#30001;&#20110;&#22797;&#26434;&#30340;&#20849;&#29616;&#20381;&#36182;&#20851;&#31995;&#32780;&#26080;&#27861;&#20840;&#38754;&#25551;&#36848;&#22825;&#27668;&#26465;&#20214;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#36825;&#20123;&#20381;&#36182;&#20851;&#31995;&#30340;&#26032;&#22411;&#22810;&#26631;&#31614;&#22825;&#27668;&#35782;&#21035;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#21517;&#20026;MASK-CNN-Transformer (MASK-CT)&#65292;&#22522;&#20110;Transformer&#12289;&#21367;&#31215;&#36807;&#31243;&#21644;MASK&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Weather recognition is an essential support for many practical life applications, including traffic safety, environment, and meteorology. However, many existing related works cannot comprehensively describe weather conditions due to their complex co-occurrence dependencies. This paper proposes a novel multi-label weather recognition model considering these dependencies. The proposed model called MASK-Convolutional Neural Network-Transformer (MASK-CT) is based on the Transformer, the convolutional process, and the MASK mechanism. The model employs multiple convolutional layers to extract features from weather images and a Transformer encoder to calculate the probability of each weather condition based on the extracted features. To improve the generalization ability of MASK-CT, a MASK mechanism is used during the training phase. The effect of the MASK mechanism is explored and discussed. The Mask mechanism randomly withholds some information from one-pair training instances (one image an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#38899;&#20048;&#20154;&#22768;&#20998;&#31163;&#30340;&#24863;&#30693;&#20219;&#21153;&#24314;&#27169;&#20026;&#22810;&#36712;&#36857;&#36319;&#36394;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#27979;&#38899;&#31526;&#20043;&#38388;&#30340;&#38142;&#25509;&#23558;&#19981;&#21516;&#30340;&#20154;&#22768;&#36827;&#34892;&#20998;&#31163;&#65292;&#20174;&#32780;&#40723;&#21169;&#21333;&#22768;&#37096;&#65288;&#20154;&#22768;&#65289;&#36712;&#36857;&#30340;&#20135;&#29983;&#12290;</title><link>http://arxiv.org/abs/2304.14848</link><description>&lt;p&gt;
&#23558;&#38899;&#20048;&#20154;&#22768;&#20998;&#31163;&#24314;&#27169;&#20026;&#38142;&#36335;&#39044;&#27979;&#65306;&#23558;&#38899;&#20048;&#24863;&#30693;&#20219;&#21153;&#24314;&#27169;&#20026;&#22810;&#36712;&#36857;&#36319;&#36394;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Musical Voice Separation as Link Prediction: Modeling a Musical Perception Task as a Multi-Trajectory Tracking Problem. (arXiv:2304.14848v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14848
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#38899;&#20048;&#20154;&#22768;&#20998;&#31163;&#30340;&#24863;&#30693;&#20219;&#21153;&#24314;&#27169;&#20026;&#22810;&#36712;&#36857;&#36319;&#36394;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#27979;&#38899;&#31526;&#20043;&#38388;&#30340;&#38142;&#25509;&#23558;&#19981;&#21516;&#30340;&#20154;&#22768;&#36827;&#34892;&#20998;&#31163;&#65292;&#20174;&#32780;&#40723;&#21169;&#21333;&#22768;&#37096;&#65288;&#20154;&#22768;&#65289;&#36712;&#36857;&#30340;&#20135;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#35299;&#20915;&#22312;&#22810;&#22768;&#37096;&#38899;&#20048;&#20013;&#20998;&#31163;&#19981;&#21516;&#20154;&#22768;&#65288;&#21363;&#21333;&#22768;&#37096;&#26059;&#24459;&#27969;&#65289;&#30340;&#24863;&#30693;&#38382;&#39064;&#12290;&#25105;&#20204;&#38024;&#23545;&#31526;&#21495;&#38899;&#20048;&#65292;&#21363;&#26126;&#30830;&#32534;&#30721;&#38899;&#31526;&#65292;&#23558;&#27492;&#20219;&#21153;&#24314;&#27169;&#20026;&#20174;&#31163;&#25955;&#35266;&#27979;&#65288;&#21363;&#38899;&#39640;-&#26102;&#38388;&#31354;&#38388;&#20013;&#30340;&#38899;&#31526;&#65289;&#20013;&#30340;&#22810;&#36712;&#36857;&#36319;&#36394;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#20026;&#27599;&#20010;&#38899;&#31526;&#21019;&#24314;&#19968;&#20010;&#33410;&#28857;&#26469;&#26500;&#24314;&#38899;&#20048;&#29255;&#27573;&#30340;&#22270;&#24418;&#65292;&#24182;&#36890;&#36807;&#39044;&#27979;&#20004;&#20010;&#38899;&#31526;&#20043;&#38388;&#30340;&#38142;&#25509;&#26469;&#20998;&#31163;&#26059;&#24459;&#36712;&#36857;&#65292;&#22914;&#26524;&#23427;&#20204;&#22312;&#21516;&#19968;&#22768;&#37096;/&#27969;&#20013;&#36830;&#32493;&#12290;&#36825;&#31181;&#23616;&#37096;&#65292;&#36138;&#24515;&#30340;&#39044;&#27979;&#26159;&#30001;&#24322;&#36136;&#22270;&#31070;&#32463;&#32593;&#32476;&#21019;&#24314;&#30340;&#33410;&#28857;&#23884;&#20837;&#25152;&#23454;&#29616;&#30340;&#65292;&#35813;&#33410;&#28857;&#23884;&#20837;&#21487;&#20197;&#25429;&#25417;&#36712;&#36857;&#20043;&#38388;&#21644;&#36712;&#36857;&#20869;&#30340;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27491;&#21017;&#21270;&#25439;&#22833;&#65292;&#40723;&#21169;&#36755;&#20986;&#36981;&#23432;&#27599;&#20010;&#33410;&#28857;&#26368;&#22810;&#26377;&#19968;&#20010;&#20837;&#21475;&#21644;&#19968;&#20010;&#20986;&#21475;&#38142;&#25509;&#30340;&#22810;&#36712;&#36857;&#36319;&#36394;&#21069;&#25552;&#65292;&#25903;&#25345;&#21333;&#22768;&#37096;&#65288;&#20154;&#22768;&#65289;&#36712;&#36857;&#65307;&#36825;&#31181;&#25439;&#22833;&#20989;&#25968;&#22312;&#20854;&#20182;&#29983;&#25104;&#24207;&#21015;&#35774;&#32622;&#20013;&#20063;&#21487;&#33021;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper targets the perceptual task of separating the different interacting voices, i.e., monophonic melodic streams, in a polyphonic musical piece. We target symbolic music, where notes are explicitly encoded, and model this task as a Multi-Trajectory Tracking (MTT) problem from discrete observations, i.e., notes in a pitch-time space. Our approach builds a graph from a musical piece, by creating one node for every note, and separates the melodic trajectories by predicting a link between two notes if they are consecutive in the same voice/stream. This kind of local, greedy prediction is made possible by node embeddings created by a heterogeneous graph neural network that can capture inter- and intra-trajectory information. Furthermore, we propose a new regularization loss that encourages the output to respect the MTT premise of at most one incoming and one outgoing link for every node, favouring monophonic (voice) trajectories; this loss function might also be useful in other gener
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;HE&#21451;&#22909;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#65292;&#25104;&#21151;&#28436;&#31034;&#20102;&#22312;ResNet&#21644;ConvNeXt&#31561;&#32463;&#20856;&#21644;&#29616;&#20195;CNN&#19978;&#36816;&#34892;&#21152;&#23494;&#26679;&#26412;&#65292;&#24182;&#20197;&#21069;&#25152;&#26410;&#26377;&#30340;&#26041;&#24335;&#28436;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;CLIP GPT-2&#27169;&#22411;&#36827;&#34892;&#38646;&#30693;&#35782;&#23433;&#20840;&#39044;&#27979;&#65292;&#26159;&#19968;&#31181;&#21487;&#34892;&#30340;&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2304.14836</link><description>&lt;p&gt;
&#20351;&#29992;&#21516;&#24577;&#21152;&#23494;&#23545;&#22823;&#35268;&#27169;CNN&#36827;&#34892;&#25935;&#24863;&#35843;&#25972;&#20197;&#36827;&#34892;&#31471;&#21040;&#31471;&#23433;&#20840;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Sensitive Tuning of Large Scale CNNs for E2E Secure Prediction using Homomorphic Encryption. (arXiv:2304.14836v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14836
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;HE&#21451;&#22909;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#65292;&#25104;&#21151;&#28436;&#31034;&#20102;&#22312;ResNet&#21644;ConvNeXt&#31561;&#32463;&#20856;&#21644;&#29616;&#20195;CNN&#19978;&#36816;&#34892;&#21152;&#23494;&#26679;&#26412;&#65292;&#24182;&#20197;&#21069;&#25152;&#26410;&#26377;&#30340;&#26041;&#24335;&#28436;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;CLIP GPT-2&#27169;&#22411;&#36827;&#34892;&#38646;&#30693;&#35782;&#23433;&#20840;&#39044;&#27979;&#65292;&#26159;&#19968;&#31181;&#21487;&#34892;&#30340;&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#31169;&#20445;&#25252;&#30340;&#26426;&#22120;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#36817;&#26469;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#20854;&#20013;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#36235;&#21183;&#26159;&#20351;&#29992;&#21516;&#24577;&#21152;&#23494;&#65288;HE&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#21152;&#23494;&#25968;&#25454;&#19978;&#25191;&#34892;&#35745;&#31639;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#35757;&#32451;&#36866;&#29992;&#20110;HE&#30340;&#21152;&#23494;&#25110;&#26410;&#21152;&#23494;&#30340;&#28145;&#23618;CNN&#65292;&#20197;&#23454;&#29616;&#33391;&#22909;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;HE&#21451;&#22909;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#65292;&#24182;&#22312;&#22522;&#26412;&#21644;&#29616;&#20195;CNN&#19978;&#36827;&#34892;&#20102;&#28436;&#31034;&#65292;&#20363;&#22914;ResNet&#21644;ConvNeXt&#12290;&#35757;&#32451;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;HELayers SDK&#36816;&#34892;&#21152;&#23494;&#26679;&#26412;&#26469;&#35780;&#20272;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#24182;&#35777;&#26126;&#23427;&#20204;&#20135;&#29983;&#20102;&#25152;&#38656;&#30340;&#32467;&#26524;&#12290;&#22312;ImageNet&#25968;&#25454;&#38598;&#19978;&#36816;&#34892;&#26102;&#65292;&#25105;&#20204;&#30340;ResNet-18/50/101&#23454;&#29616;&#20165;&#38656;&#35201;7&#12289;31&#21644;57&#20998;&#38047;&#65292;&#36825;&#34920;&#26126;&#36825;&#20010;&#35299;&#20915;&#26041;&#26696;&#26159;&#23454;&#29992;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20123;&#20851;&#20110;&#22312;HE&#19979;&#22788;&#29702;&#28608;&#27963;&#20989;&#25968;&#21644;&#36339;&#36291;&#36830;&#25509;&#30340;&#35265;&#35299;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20197;&#21069;&#25152;&#26410;&#26377;&#30340;&#26041;&#24335;&#28436;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;CLIP GPT-2&#27169;&#22411;&#36827;&#34892;&#38646;&#30693;&#35782;&#23433;&#20840;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Privacy-preserving machine learning solutions have recently gained significant attention. One promising research trend is using Homomorphic Encryption (HE), a method for performing computation over encrypted data. One major challenge in this approach is training HE-friendly, encrypted or unencrypted, deep CNNs with decent accuracy. We propose a novel training method for HE-friendly models, and demonstrate it on fundamental and modern CNNs, such as ResNet and ConvNeXt. After training, we evaluate our models by running encrypted samples using HELayers SDK and proving that they yield the desired results. When running on a GPU over the ImageNet dataset, our ResNet-18/50/101 implementations take only 7, 31 and 57 minutes, respectively, which shows that this solution is practical. Furthermore, we present several insights on handling the activation functions and skip-connections under HE. Finally, we demonstrate in an unprecedented way how to perform secure zero-shot prediction using a CLIP m
&lt;/p&gt;</description></item><item><title>ViziQuer&#26159;&#19968;&#31181;&#21487;&#35270;&#21270;&#26597;&#35810;&#31526;&#21495;&#21644;&#24037;&#20855;&#65292;&#25552;&#20379;&#20102;&#21487;&#35270;&#21270;&#22270;&#24418;&#25163;&#27573;&#26469;&#25551;&#36848;&#20016;&#23500;&#30340;&#25968;&#25454;&#26597;&#35810;&#12290;&#23427;&#30340;&#21019;&#26032;&#28857;&#22312;&#20110;&#25552;&#20379;&#19968;&#31181;&#21487;&#35270;&#21270;&#30340;&#26041;&#24335;&#26469;&#36827;&#34892;&#25968;&#25454;&#26597;&#35810;&#65292;&#20351;&#24471;&#38750;&#25216;&#26415;&#19987;&#23478;&#20063;&#33021;&#21442;&#19982;&#21040;&#25968;&#25454;&#26597;&#35810;&#30340;&#36807;&#31243;&#20013;&#12290;</title><link>http://arxiv.org/abs/2304.14825</link><description>&lt;p&gt;
ViziQuer&#20013;&#30340;&#21487;&#35270;&#21270;&#22270;&#24418;&#26597;&#35810;: &#27010;&#36848;&#21644;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
Visual Diagrammatic Queries in ViziQuer: Overview and Implementation. (arXiv:2304.14825v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14825
&lt;/p&gt;
&lt;p&gt;
ViziQuer&#26159;&#19968;&#31181;&#21487;&#35270;&#21270;&#26597;&#35810;&#31526;&#21495;&#21644;&#24037;&#20855;&#65292;&#25552;&#20379;&#20102;&#21487;&#35270;&#21270;&#22270;&#24418;&#25163;&#27573;&#26469;&#25551;&#36848;&#20016;&#23500;&#30340;&#25968;&#25454;&#26597;&#35810;&#12290;&#23427;&#30340;&#21019;&#26032;&#28857;&#22312;&#20110;&#25552;&#20379;&#19968;&#31181;&#21487;&#35270;&#21270;&#30340;&#26041;&#24335;&#26469;&#36827;&#34892;&#25968;&#25454;&#26597;&#35810;&#65292;&#20351;&#24471;&#38750;&#25216;&#26415;&#19987;&#23478;&#20063;&#33021;&#21442;&#19982;&#21040;&#25968;&#25454;&#26597;&#35810;&#30340;&#36807;&#31243;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#24050;&#25104;&#20026;&#19968;&#31181;&#37325;&#35201;&#30340;&#25968;&#25454;&#32452;&#32455;&#33539;&#24335;&#12290;SPARQL&#31561;&#21487;&#29992;&#30340;&#25991;&#26412;&#26597;&#35810;&#35821;&#35328;&#29992;&#20110;&#20174;&#30693;&#35782;&#22270;&#35889;&#20013;&#26816;&#32034;&#20449;&#24687;&#65292;&#20294;&#19981;&#25552;&#20379;&#21442;&#19982;&#38750;&#25216;&#26415;&#19987;&#23478;&#30340;&#25968;&#25454;&#35775;&#38382;&#36807;&#31243;&#30340;&#26041;&#27861;&#12290;&#19982;&#22522;&#20110;&#34920;&#21333;&#21644;&#33258;&#28982;&#35821;&#35328;&#30340;&#26597;&#35810;&#19968;&#36215;&#65292;&#21487;&#35270;&#21270;&#26597;&#35810;&#24418;&#24335;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21270;&#29992;&#25143;&#21442;&#19982;&#25968;&#25454;&#26597;&#35810;&#36807;&#31243;&#30340;&#26041;&#27861;&#12290;ViziQuer&#26159;&#19968;&#31181;&#21487;&#35270;&#21270;&#26597;&#35810;&#31526;&#21495;&#21644;&#24037;&#20855;&#65292;&#25552;&#20379;&#20102;&#21487;&#35270;&#21270;&#22270;&#24418;&#25163;&#27573;&#26469;&#25551;&#36848;&#20016;&#23500;&#30340;&#25968;&#25454;&#26597;&#35810;&#65292;&#21253;&#25324;&#21487;&#36873;&#21644;&#21542;&#23450;&#26500;&#36896;&#65292;&#20197;&#21450;&#32858;&#21512;&#21644;&#23376;&#26597;&#35810;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#26368;&#32456;&#29992;&#25143;&#30340;&#35282;&#24230;&#22238;&#39038;&#20102;&#21487;&#35270;&#21270;ViziQuer&#31526;&#21495;&#65292;&#24182;&#25551;&#36848;&#20102;&#27010;&#24565;&#21644;&#25216;&#26415;&#35299;&#20915;&#26041;&#26696;&#65288;&#21253;&#25324;&#25277;&#35937;&#35821;&#27861;&#27169;&#22411;&#65292;&#20197;&#21450;&#25991;&#26412;&#26597;&#35810;&#29983;&#25104;&#27169;&#22411;&#65289;&#65292;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#20801;&#35768;&#23558;&#21487;&#35270;&#21270;&#22270;&#24418;&#26597;&#35810;&#31526;&#21495;&#26144;&#23556;&#21040;&#25991;&#26412;SPARQL&#35821;&#35328;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23454;&#38469;&#25968;&#25454;&#19978;&#30340;&#20016;&#23500;&#21487;&#35270;&#21270;&#26597;&#35810;&#30340;&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graphs (KG) have become an important data organization paradigm. The available textual query languages for information retrieval from KGs, as SPARQL for RDF-structured data, do not provide means for involving non-technical experts in the data access process. Visual query formalisms, alongside form-based and natural language-based ones, offer means for easing user involvement in the data querying process. ViziQuer is a visual query notation and tool offering visual diagrammatic means for describing rich data queries, involving optional and negation constructs, as well as aggregation and subqueries. In this paper we review the visual ViziQuer notation from the end-user point of view and describe the conceptual and technical solutions (including abstract syntax model, followed by a generation model for textual queries) that allow mapping of the visual diagrammatic query notation into the textual SPARQL language, thus enabling the execution of rich visual queries over the actual 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#36741;&#21161;&#24494;&#27874;&#31561;&#31163;&#23376;&#20307;&#30456;&#20114;&#20316;&#29992;&#25216;&#26415;&#30340;&#31561;&#31163;&#23376;&#20307;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#27979;&#37327;&#24494;&#27874;&#25955;&#23556;&#24341;&#36215;&#30340;&#30005;&#22330;&#27169;&#24335;&#26469;&#20272;&#35745;&#23494;&#24230;&#21078;&#38754;&#12290;</title><link>http://arxiv.org/abs/2304.14807</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#36741;&#21161;&#24494;&#27874;&#31561;&#31163;&#23376;&#20307;&#30456;&#20114;&#20316;&#29992;&#25216;&#26415;&#30340;&#31561;&#31163;&#23376;&#20307;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Deep Learning assisted microwave-plasma interaction based technique for plasma density estimation. (arXiv:2304.14807v1 [physics.plasm-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14807
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#36741;&#21161;&#24494;&#27874;&#31561;&#31163;&#23376;&#20307;&#30456;&#20114;&#20316;&#29992;&#25216;&#26415;&#30340;&#31561;&#31163;&#23376;&#20307;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#27979;&#37327;&#24494;&#27874;&#25955;&#23556;&#24341;&#36215;&#30340;&#30005;&#22330;&#27169;&#24335;&#26469;&#20272;&#35745;&#23494;&#24230;&#21078;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#23494;&#24230;&#26159;&#34920;&#24449;&#20219;&#20309;&#31561;&#31163;&#23376;&#20307;&#30340;&#20851;&#38190;&#21442;&#25968;&#12290;&#20302;&#28201;&#31561;&#31163;&#23376;&#20307;&#39046;&#22495;&#30340;&#22823;&#37096;&#20998;&#24212;&#29992;&#21644;&#30740;&#31350;&#37117;&#22522;&#20110;&#31561;&#31163;&#23376;&#20307;&#23494;&#24230;&#21644;&#31561;&#31163;&#23376;&#20307;&#28201;&#24230;&#12290;&#20256;&#32479;&#30340;&#30005;&#23376;&#23494;&#24230;&#27979;&#37327;&#26041;&#27861;&#38024;&#23545;&#32473;&#23450;&#32447;&#24615;&#20302;&#28201;&#31561;&#31163;&#23376;&#20307;&#35774;&#22791;&#25552;&#20379;&#36724;&#21521;&#21644;&#24452;&#21521;&#21078;&#38754;&#12290;&#36825;&#20123;&#26041;&#27861;&#23384;&#22312;&#25805;&#20316;&#33539;&#22260;&#36739;&#23567;&#12289;&#20202;&#22120;&#27785;&#37325;&#20197;&#21450;&#25968;&#25454;&#20998;&#26512;&#36807;&#31243;&#22797;&#26434;&#31561;&#20027;&#35201;&#32570;&#28857;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#23454;&#38469;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#36741;&#21161;&#24494;&#27874;&#31561;&#31163;&#23376;&#20307;&#30456;&#20114;&#20316;&#29992;&#30340;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#33021;&#22815;&#30830;&#23450;&#31561;&#31163;&#23376;&#20307;&#20869;&#30005;&#23376;&#23494;&#24230;&#21078;&#38754;&#12290;&#36890;&#36807;&#27979;&#37327;&#24494;&#27874;&#25955;&#23556;&#24341;&#36215;&#30340;&#30005;&#22330;&#27169;&#24335;&#26469;&#20272;&#35745;&#23494;&#24230;&#21078;&#38754;&#12290;&#35813;&#31574;&#30053;&#38024;&#23545;&#19968;&#20010;&#27169;&#25311;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#27010;&#24565;&#39564;&#35777;&#65292;&#20854;&#20013;&#21253;&#25324;&#20302;&#28201;&#12289;&#38750;&#30913;&#21270;&#21644;&#30896;&#25758;&#24615;&#31561;&#31163;&#23376;&#20307;&#30340;&#19981;&#21516;&#31867;&#22411;&#30340;&#39640;&#26031;&#24418;&#29366;&#23494;&#24230;&#21078;&#38754;&#33539;&#22260;&#20869;&#12290;
&lt;/p&gt;
&lt;p&gt;
The electron density is a key parameter to characterize any plasma. Most of the plasma applications and research in the area of low-temperature plasmas (LTPs) is based on plasma density and plasma temperature. The conventional methods for electron density measurements offer axial and radial profiles for any given linear LTP device. These methods have major disadvantages of operational range (not very wide), cumbersome instrumentation, and complicated data analysis procedures. To address such practical concerns, the article proposes a novel machine learning (ML) assisted microwave-plasma interaction based strategy which is capable enough to determine the electron density profile within the plasma. The electric field pattern due to microwave scattering is measured to estimate the density profile. The proof of concept is tested for a simulated training data set comprising a low-temperature, unmagnetized, collisional plasma. Different types of Gaussian-shaped density profiles, in the range
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22810;&#23545;&#21333;&#30693;&#35782;&#33976;&#39311;&#30340;&#27169;&#22411;&#26469;&#25552;&#39640;&#28857;&#20113;&#35821;&#20041;&#20998;&#21106;&#20013;&#30340;&#20998;&#31867;&#34920;&#29616;&#65292;&#35813;&#27169;&#22411;&#38024;&#23545;&#38590;&#20998;&#31867;&#31867;&#21035;&#21482;&#34701;&#21512;&#23427;&#20204;&#30340;&#23454;&#20363;&#65292;&#24182;&#20351;&#29992;&#22810;&#32423;&#33976;&#39311;&#26694;&#26550;&#21644;&#23454;&#20363;&#24863;&#30693;&#26500;&#24418;&#33976;&#39311;&#31639;&#27861;&#20174;&#22810;&#20010;&#25195;&#25551;&#20013;&#33976;&#39311;&#26377;&#20215;&#20540;&#30340;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2304.14800</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#23545;&#21333;&#30340;&#30693;&#35782;&#33976;&#39311;&#30340;&#28857;&#20113;&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Multi-to-Single Knowledge Distillation for Point Cloud Semantic Segmentation. (arXiv:2304.14800v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14800
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22810;&#23545;&#21333;&#30693;&#35782;&#33976;&#39311;&#30340;&#27169;&#22411;&#26469;&#25552;&#39640;&#28857;&#20113;&#35821;&#20041;&#20998;&#21106;&#20013;&#30340;&#20998;&#31867;&#34920;&#29616;&#65292;&#35813;&#27169;&#22411;&#38024;&#23545;&#38590;&#20998;&#31867;&#31867;&#21035;&#21482;&#34701;&#21512;&#23427;&#20204;&#30340;&#23454;&#20363;&#65292;&#24182;&#20351;&#29992;&#22810;&#32423;&#33976;&#39311;&#26694;&#26550;&#21644;&#23454;&#20363;&#24863;&#30693;&#26500;&#24418;&#33976;&#39311;&#31639;&#27861;&#20174;&#22810;&#20010;&#25195;&#25551;&#20013;&#33976;&#39311;&#26377;&#20215;&#20540;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19977;&#32500;&#28857;&#20113;&#35821;&#20041;&#20998;&#21106;&#26159;&#29615;&#22659;&#29702;&#35299;&#30340;&#22522;&#26412;&#20219;&#21153;&#20043;&#19968;&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#26159;&#23545;&#20110;&#26679;&#26412;&#36739;&#23569;&#25110;&#28857;&#25968;&#36739;&#23569;&#30340;&#31867;&#30340;&#24615;&#33021;&#20173;&#28982;&#19981;&#22815;&#28385;&#24847;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#22810;&#23545;&#21333;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#65292;&#26469;&#25552;&#39640;&#37027;&#20123;&#38590;&#20998;&#31867;&#30340;&#31867;&#30340;&#24615;&#33021;&#12290;&#19981;&#21516;&#20110;&#30452;&#25509;&#34701;&#21512;&#22810;&#20010;&#25195;&#25551;&#30340;&#25152;&#26377;&#28857;&#65292;&#21482;&#34701;&#21512;&#23646;&#20110;&#20043;&#21069;&#23450;&#20041;&#30340;&#38590;&#20998;&#31867;&#31867;&#21035;&#30340;&#23454;&#20363;&#12290;&#20026;&#20102;&#26377;&#25928;&#21644;&#20805;&#20998;&#22320;&#20174;&#22810;&#20010;&#25195;&#25551;&#20013;&#33976;&#39311;&#26377;&#20215;&#20540;&#30340;&#30693;&#35782;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#22810;&#32423;&#33976;&#39311;&#26694;&#26550;&#65292;&#21363;&#29305;&#24449;&#34920;&#24449;&#33976;&#39311;&#12289;logit&#33976;&#39311;&#21644;&#26500;&#24418;&#33976;&#39311;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#23454;&#20363;&#24863;&#30693;&#26500;&#24418;&#33976;&#39311;&#31639;&#27861;&#65292;&#29992;&#20110;&#25429;&#25417;&#39640;&#23618;&#27425;&#30340;&#32467;&#26500;&#30693;&#35782;&#65292;&#20197;&#22686;&#24378;&#38590;&#20998;&#31867;&#30340;&#33976;&#39311;&#25928;&#33021;&#12290;&#26368;&#21518;&#65292;&#22312;&#23448;&#26041;&#25968;&#25454;&#38598;&#19978;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
3D point cloud semantic segmentation is one of the fundamental tasks for environmental understanding. Although significant progress has been made in recent years, the performance of classes with few examples or few points is still far from satisfactory. In this paper, we propose a novel multi-to-single knowledge distillation framework for the 3D point cloud semantic segmentation task to boost the performance of those hard classes. Instead of fusing all the points of multi-scans directly, only the instances that belong to the previously defined hard classes are fused. To effectively and sufficiently distill valuable knowledge from multi-scans, we leverage a multilevel distillation framework, i.e., feature representation distillation, logit distillation, and affinity distillation. We further develop a novel instance-aware affinity distillation algorithm for capturing high-level structural knowledge to enhance the distillation efficacy for hard classes. Finally, we conduct experiments on 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#31934;&#30830;&#21387;&#32553;&#26469;&#20943;&#23569;&#22312;&#22823;&#22411;&#22270;&#19978;&#23398;&#20064;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20869;&#23384;&#38656;&#27714;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#35777;&#26126;&#31561;&#25928;&#30340;&#21387;&#32553;&#22270;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.14793</link><description>&lt;p&gt;
&#21033;&#29992;&#31934;&#30830;&#21387;&#32553;&#23398;&#20064;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Learning Graph Neural Networks using Exact Compression. (arXiv:2304.14793v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14793
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#31934;&#30830;&#21387;&#32553;&#26469;&#20943;&#23569;&#22312;&#22823;&#22411;&#22270;&#19978;&#23398;&#20064;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20869;&#23384;&#38656;&#27714;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#35777;&#26126;&#31561;&#25928;&#30340;&#21387;&#32553;&#22270;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#26159;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#21487;&#20197;&#23545;&#22270;&#32467;&#26500;&#25968;&#25454;&#36827;&#34892;&#24191;&#27867;&#30340;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#12290;&#23398;&#20064;&#36825;&#31181;&#32593;&#32476;&#65292;&#28982;&#32780;&#65292;&#23545;&#20110;&#20869;&#23384;&#21463;&#38480;&#30340;&#35774;&#22791;&#65288;&#22914;GPU&#65289;&#26469;&#35828;&#26159;&#19968;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#31934;&#30830;&#21387;&#32553;&#26469;&#20943;&#23569;&#22312;&#22823;&#22411;&#22270;&#19978;&#23398;&#20064;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20869;&#23384;&#38656;&#27714;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#24418;&#24335;&#21270;&#21387;&#32553;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#38382;&#39064;&#36716;&#21270;&#20026;&#35777;&#26126;&#31561;&#25928;&#30340;&#21387;&#32553;&#22270;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#38382;&#39064;&#12290;&#22312;&#21021;&#27493;&#30340;&#23454;&#39564;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#27934;&#23519;&#20102;&#30495;&#23454;&#19990;&#30028;&#22270;&#19978;&#21487;&#20197;&#33719;&#24471;&#30340;&#21387;&#32553;&#27604;&#65292;&#24182;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#29616;&#26377;&#30340;&#19968;&#20010;&#22270;&#31070;&#32463;&#32593;&#32476;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) are a form of deep learning that enable a wide range of machine learning applications on graph-structured data. The learning of GNNs, however, is known to pose challenges for memory-constrained devices such as GPUs. In this paper, we study exact compression as a way to reduce the memory requirements of learning GNNs on large graphs. In particular, we adopt a formal approach to compression and propose a methodology that transforms GNN learning problems into provably equivalent compressed GNN learning problems. In a preliminary experimental evaluation, we give insights into the compression ratios that can be obtained on real-world graphs and apply our methodology to an existing GNN benchmark.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;GPT-SW3&#30340;&#22810;&#35821;&#35328;&#20998;&#35789;&#22120;&#65292;&#36890;&#36807;&#20351;&#29992;SentencePiece&#24211;&#21644;BPE&#31639;&#27861;&#22312;Nordic Pile&#19978;&#35757;&#32451;&#65292;&#35780;&#20272;&#20102;&#20854;&#23545;&#20110;&#19981;&#21516;&#35821;&#35328;&#30340;&#34920;&#29616;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.14780</link><description>&lt;p&gt;
GPT-SW3&#22810;&#35821;&#35328;&#20998;&#35789;&#22120;&#30340;&#35757;&#32451;&#19982;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Training and Evaluation of a Multilingual Tokenizer for GPT-SW3. (arXiv:2304.14780v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;GPT-SW3&#30340;&#22810;&#35821;&#35328;&#20998;&#35789;&#22120;&#65292;&#36890;&#36807;&#20351;&#29992;SentencePiece&#24211;&#21644;BPE&#31639;&#27861;&#22312;Nordic Pile&#19978;&#35757;&#32451;&#65292;&#35780;&#20272;&#20102;&#20854;&#23545;&#20110;&#19981;&#21516;&#35821;&#35328;&#30340;&#34920;&#29616;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;GPT-SW3&#25152;&#20351;&#29992;&#30340;&#22810;&#35821;&#35328;&#20998;&#35789;&#22120;&#12290;&#35813;&#20998;&#35789;&#22120;&#20351;&#29992;SentencePiece&#24211;&#21644;BPE&#31639;&#27861;&#22312;Nordic Pile&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#27010;&#36848;&#20102;&#20998;&#35789;&#22120;&#30340;&#26368;&#37325;&#35201;&#29305;&#28857;&#65292;&#24182;&#20998;&#20139;&#20102;&#20854;&#23398;&#20064;&#35789;&#27719;&#30340;&#32454;&#33410;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#20998;&#26512;&#20102;&#20998;&#35789;&#22120;&#22312;&#25968;&#25454;&#20013;&#19981;&#21516;&#35821;&#35328;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#24182;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper provides a detailed discussion of the multilingual tokenizer used for GPT-SW3. It was trained on the Nordic Pile using the SentencePiece library and the BPE algorithm. We outline the tokenizer's most important features and share details on its learned vocabulary. In addition, we systematically analyze the properties and evaluate the performance of the tokenizer with regard to the different languages present in the data.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22522;&#20110;&#24230;&#37327;&#30340;&#32447;&#24615;&#26102;&#38388;&#24179;&#34913;&#36923;&#36753;&#26469;&#22788;&#29702;&#28041;&#21450;&#26102;&#38388;&#32422;&#26463;&#30340;&#21160;&#24577;&#31995;&#32479;&#38382;&#39064;&#65292;&#24182;&#32473;&#20986;&#20102;&#36890;&#36807;&#23558;&#24230;&#37327;&#20844;&#24335;&#36716;&#21270;&#20026;&#21333;&#19968;&#19968;&#38454;&#20844;&#24335;&#23454;&#29616;&#27169;&#22411;&#26816;&#26597;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.14778</link><description>&lt;p&gt;
&#22522;&#20110;&#26102;&#38388;&#36712;&#36857;&#30340;&#24230;&#37327;&#26102;&#38388;&#24179;&#34913;&#36923;&#36753;
&lt;/p&gt;
&lt;p&gt;
Metric Temporal Equilibrium Logic over Timed Traces. (arXiv:2304.14778v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14778
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22522;&#20110;&#24230;&#37327;&#30340;&#32447;&#24615;&#26102;&#38388;&#24179;&#34913;&#36923;&#36753;&#26469;&#22788;&#29702;&#28041;&#21450;&#26102;&#38388;&#32422;&#26463;&#30340;&#21160;&#24577;&#31995;&#32479;&#38382;&#39064;&#65292;&#24182;&#32473;&#20986;&#20102;&#36890;&#36807;&#23558;&#24230;&#37327;&#20844;&#24335;&#36716;&#21270;&#20026;&#21333;&#19968;&#19968;&#38454;&#20844;&#24335;&#23454;&#29616;&#27169;&#22411;&#26816;&#26597;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#20110;&#32447;&#24615;&#26102;&#38388;&#30340;Answer Set Programming (ASP)&#30340;&#26102;&#38388;&#25193;&#23637;&#20013;&#65292;&#21160;&#24577;&#31995;&#32479;&#30340;&#34892;&#20026;&#36890;&#36807;&#29366;&#24577;&#24207;&#21015;&#26469;&#25429;&#33719;&#12290;&#34429;&#28982;&#27492;&#34920;&#31034;&#21453;&#26144;&#20102;&#23427;&#20204;&#30340;&#30456;&#23545;&#39034;&#24207;&#65292;&#20294;&#23427;&#25277;&#35937;&#25481;&#20102;&#19982;&#27599;&#20010;&#29366;&#24577;&#20851;&#32852;&#30340;&#20855;&#20307;&#26102;&#38388;&#12290;&#28982;&#32780;&#65292;&#26102;&#38388;&#32422;&#26463;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#26159;&#37325;&#35201;&#30340;&#65292;&#27604;&#22914;&#24403;&#35745;&#21010;&#21644;&#35843;&#24230;&#30456;&#20114;&#37197;&#21512;&#26102;&#12290;&#25105;&#20204;&#36890;&#36807;&#24320;&#21457;&#22522;&#20110;&#24230;&#37327;&#30340;&#32447;&#24615;&#26102;&#38388;&#24179;&#34913;&#36923;&#36753;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20854;&#20013;&#26102;&#38388;&#36816;&#31639;&#31526;&#21463;&#33258;&#28982;&#25968;&#21306;&#38388;&#30340;&#32422;&#26463;&#12290;&#30001;&#27492;&#24471;&#21040;&#30340;&#24230;&#37327;&#24179;&#34913;&#36923;&#36753;&#20026;&#25351;&#23450;&#23450;&#24615;&#21644;&#23450;&#37327;&#21160;&#24577;&#32422;&#26463;&#30340;&#22522;&#20110;ASP&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23558;&#24230;&#37327;&#20844;&#24335;&#36716;&#21270;&#20026;&#21333;&#35843;&#19968;&#38454;&#20844;&#24335;&#65292;&#24182;&#20998;&#21035;&#32473;&#20986;&#22312;Metric Equilibrium Logic&#21644;Monadic Quantified Equilibrium Logic&#20013;&#30340;&#27169;&#22411;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#32763;&#35793;&#25552;&#20379;&#20102;Metric Equilibrium Logic&#30340;&#23454;&#29616;&#27169;&#22411;&#26816;&#26597;&#30340;&#34013;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
In temporal extensions of Answer Set Programming (ASP) based on linear-time, the behavior of dynamic systems is captured by sequences of states. While this representation reflects their relative order, it abstracts away the specific times associated with each state. However, timing constraints are important in many applications like, for instance, when planning and scheduling go hand in hand. We address this by developing a metric extension of linear-time temporal equilibrium logic, in which temporal operators are constrained by intervals over natural numbers. The resulting Metric Equilibrium Logic provides the foundation of an ASP-based approach for specifying qualitative and quantitative dynamic constraints. To this end, we define a translation of metric formulas into monadic first-order formulas and give a correspondence between their models in Metric Equilibrium Logic and Monadic Quantified Equilibrium Logic, respectively. Interestingly, our translation provides a blue print for im
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#32467;&#21512;&#36801;&#31227;&#23398;&#20064;&#21644;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#35299;&#20915;&#25968;&#23383;&#21270;&#32472;&#30011;&#20013;&#30340;&#30011;&#23478;&#35782;&#21035;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#32422; 10,000 &#24352;&#22270;&#29255;&#21644; 146 &#20301;&#30011;&#23478;&#30340;&#26032;&#22411;&#25968;&#25454;&#38598; MultiPaint-10K&#12290;</title><link>http://arxiv.org/abs/2304.14773</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21327;&#21516;&#20316;&#29992;&#22312;&#22810;&#30011;&#23478;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Synergy of Machine and Deep Learning Models for Multi-Painter Recognition. (arXiv:2304.14773v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14773
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#32467;&#21512;&#36801;&#31227;&#23398;&#20064;&#21644;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#35299;&#20915;&#25968;&#23383;&#21270;&#32472;&#30011;&#20013;&#30340;&#30011;&#23478;&#35782;&#21035;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#32422; 10,000 &#24352;&#22270;&#29255;&#21644; 146 &#20301;&#30011;&#23478;&#30340;&#26032;&#22411;&#25968;&#25454;&#38598; MultiPaint-10K&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#21270;&#33402;&#26415;&#25910;&#34255;&#21697;&#30340;&#22686;&#22810;&#24102;&#26469;&#20102;&#22823;&#37327;&#28041;&#21450;&#25277;&#35937;&#27010;&#24565;&#30340;&#25968;&#25454;&#31649;&#29702;&#12289;&#20998;&#26512;&#21644;&#20998;&#31867;&#30340;&#38656;&#27714;&#65292;&#36825;&#26159;&#35745;&#31639;&#26426;&#31185;&#23398;&#20013;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#24182;&#24341;&#21457;&#20102;&#26032;&#30340;&#30740;&#31350;&#35270;&#35282;&#12290;&#20154;&#24037;&#26234;&#33021;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#36827;&#27493;&#25552;&#20379;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#30340;&#33391;&#22909;&#24037;&#20855;&#12290;&#26412;&#25991;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#25552;&#21462;&#36866;&#24403;&#29305;&#24449;&#21644;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#35780;&#20272;&#65292;&#22312;&#26469;&#33258; WikiArt &#24211;&#30340;&#19968;&#32452;&#25968;&#23383;&#21270;&#32472;&#30011;&#20013;&#22788;&#29702;&#30011;&#23478;&#35782;&#21035;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#27979;&#35797;&#21508;&#31181;&#27169;&#22411;&#21644;&#24494;&#35843;&#23427;&#20204;&#65292;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65306;RegNet &#22312;&#25552;&#21462;&#29305;&#24449;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#65292;&#32780; SVM &#22522;&#20110;&#30011;&#23478;&#30340;&#22270;&#29255;&#20998;&#31867;&#26368;&#20339;&#65292;&#24615;&#33021;&#39640;&#36798; 85%&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026; MultiPaint-10K &#30340;&#26032;&#22823;&#22411;&#32472;&#30011;&#20316;&#21697;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;&#26469;&#33258; 146 &#20301;&#30011;&#23478;&#30340;&#32422; 10,000 &#24352;&#22270;&#29255;&#12290;
&lt;/p&gt;
&lt;p&gt;
The growing availability of digitized art collections has created the need to manage, analyze and categorize large amounts of data related to abstract concepts, highlighting a demanding problem of computer science and leading to new research perspectives. Advances in artificial intelligence and neural networks provide the right tools for this challenge. The analysis of artworks to extract features useful in certain works is at the heart of the era. In the present work, we approach the problem of painter recognition in a set of digitized paintings, derived from the WikiArt repository, using transfer learning to extract the appropriate features and classical machine learning methods to evaluate the result. Through the testing of various models and their fine tuning we came to the conclusion that RegNet performs better in exporting features, while SVM makes the best classification of images based on the painter with a performance of up to 85%. Also, we introduced a new large dataset for p
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LostPaw&#30340;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#24212;&#29992;&#31243;&#24207;&#65292;&#21033;&#29992;&#23545;&#27604;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20934;&#30830;&#21306;&#20998;&#23456;&#29289;&#22270;&#20687;&#65292;&#21487;&#29992;&#20110;&#31934;&#20934;&#25628;&#32034;&#22833;&#36394;&#30340;&#23456;&#29289;&#12290;&#35813;&#27169;&#22411;&#36798;&#21040;&#20102;90%&#30340;&#27979;&#35797;&#20934;&#30830;&#29575;&#65292;&#24182;&#20026;&#28508;&#22312;&#30340; Web &#24212;&#29992;&#31243;&#24207;&#25552;&#20379;&#20102;&#22522;&#30784;&#65292;&#29992;&#25143;&#33021;&#22815;&#19978;&#20256;&#20002;&#22833;&#23456;&#29289;&#30340;&#22270;&#20687;&#24182;&#22312;&#25968;&#25454;&#24211;&#20013;&#25214;&#21040;&#21305;&#37197;&#22270;&#20687;&#26102;&#25509;&#25910;&#36890;&#30693;&#12290;</title><link>http://arxiv.org/abs/2304.14765</link><description>&lt;p&gt;
LostPaw: &#20351;&#29992;&#24102;&#35270;&#35273;&#36755;&#20837;&#30340;&#23545;&#27604;&#23398;&#20064; Transformer &#25214;&#21040;&#22833;&#36394;&#30340;&#23456;&#29289;
&lt;/p&gt;
&lt;p&gt;
LostPaw: Finding Lost Pets using a Contrastive Learning-based Transformer with Visual Input. (arXiv:2304.14765v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14765
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LostPaw&#30340;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#24212;&#29992;&#31243;&#24207;&#65292;&#21033;&#29992;&#23545;&#27604;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20934;&#30830;&#21306;&#20998;&#23456;&#29289;&#22270;&#20687;&#65292;&#21487;&#29992;&#20110;&#31934;&#20934;&#25628;&#32034;&#22833;&#36394;&#30340;&#23456;&#29289;&#12290;&#35813;&#27169;&#22411;&#36798;&#21040;&#20102;90%&#30340;&#27979;&#35797;&#20934;&#30830;&#29575;&#65292;&#24182;&#20026;&#28508;&#22312;&#30340; Web &#24212;&#29992;&#31243;&#24207;&#25552;&#20379;&#20102;&#22522;&#30784;&#65292;&#29992;&#25143;&#33021;&#22815;&#19978;&#20256;&#20002;&#22833;&#23456;&#29289;&#30340;&#22270;&#20687;&#24182;&#22312;&#25968;&#25454;&#24211;&#20013;&#25214;&#21040;&#21305;&#37197;&#22270;&#20687;&#26102;&#25509;&#25910;&#36890;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22833;&#21435;&#23456;&#29289;&#21487;&#33021;&#20250;&#35753;&#23456;&#29289;&#20027;&#20154;&#20493;&#24863;&#30171;&#33510;&#65292;&#32780;&#25214;&#21040;&#22833;&#36394;&#30340;&#23456;&#29289;&#36890;&#24120;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#32791;&#26102;&#30340;&#12290;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#24212;&#29992;&#31243;&#24207;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#23547;&#25214;&#20002;&#22833;&#23456;&#29289;&#30340;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#20415;&#20110;&#36825;&#26679;&#30340;&#24212;&#29992;&#31243;&#24207;&#30340;&#23454;&#29616;&#65292;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#23545;&#27604;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#33021;&#22815;&#20934;&#30830;&#22320;&#21306;&#20998;&#19981;&#21516;&#23456;&#29289;&#30340;&#22270;&#20687;&#12290;&#35813;&#27169;&#22411;&#22312;&#22823;&#37327;&#30340;&#29399;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807; 3 &#25240;&#20132;&#21449;&#39564;&#35777;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#22312; 350 &#20010;&#35757;&#32451;&#21608;&#26399;&#21518;&#65292;&#27169;&#22411;&#21462;&#24471;&#20102;90%&#30340;&#27979;&#35797;&#20934;&#30830;&#24230;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#27979;&#35797;&#20934;&#30830;&#24615;&#25509;&#36817;&#35757;&#32451;&#20934;&#30830;&#24615;&#65292;&#36991;&#20813;&#20102;&#36807;&#24230;&#25311;&#21512;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23545;&#27604;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20316;&#20026;&#23450;&#20301;&#22833;&#36394;&#23456;&#29289;&#30340;&#24037;&#20855;&#20855;&#26377;&#28508;&#21147;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#28508;&#22312;&#30340; Web &#24212;&#29992;&#31243;&#24207;&#30340;&#22522;&#30784;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#19978;&#20256;&#20854;&#20002;&#22833;&#23456;&#29289;&#30340;&#22270;&#20687;&#65292;&#24182;&#22312;&#24212;&#29992;&#31243;&#24207;&#30340;&#22270;&#20687;&#25968;&#25454;&#24211;&#20013;&#25214;&#21040;&#21305;&#37197;&#22270;&#20687;&#26102;&#25509;&#25910;&#36890;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;
Losing pets can be highly distressing for pet owners, and finding a lost pet is often challenging and time-consuming. An artificial intelligence-based application can significantly improve the speed and accuracy of finding lost pets. In order to facilitate such an application, this study introduces a contrastive neural network model capable of accurately distinguishing between images of pets. The model was trained on a large dataset of dog images and evaluated through 3-fold cross-validation. Following 350 epochs of training, the model achieved a test accuracy of 90%. Furthermore, overfitting was avoided, as the test accuracy closely matched the training accuracy. Our findings suggest that contrastive neural network models hold promise as a tool for locating lost pets. This paper provides the foundation for a potential web application that allows users to upload images of their missing pets, receiving notifications when matching images are found in the application's image database. Thi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#20855;&#26377;&#38750;&#20108;&#20803;&#29305;&#24449;&#30340;&#20998;&#31867;&#22120;&#30340;&#26032;&#22411;&#35299;&#37322;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#20379;&#26356;&#22810;&#20851;&#20110;&#20915;&#31574;&#21644;&#22522;&#30784;&#20998;&#31867;&#22120;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2304.14760</link><description>&lt;p&gt;
&#20855;&#26377;&#38750;&#20108;&#20803;&#29305;&#24449;&#30340;&#20998;&#31867;&#22120;&#30340;&#26032;&#22411;&#35299;&#37322;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A New Class of Explanations for Classifiers with Non-Binary Features. (arXiv:2304.14760v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14760
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#20855;&#26377;&#38750;&#20108;&#20803;&#29305;&#24449;&#30340;&#20998;&#31867;&#22120;&#30340;&#26032;&#22411;&#35299;&#37322;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#20379;&#26356;&#22810;&#20851;&#20110;&#20915;&#31574;&#21644;&#22522;&#30784;&#20998;&#31867;&#22120;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26469;&#65292;&#24403;&#20998;&#26512;&#20998;&#31867;&#22120;&#20915;&#31574;&#26102;&#65292;&#24050;&#32463;&#26377;&#20004;&#31181;&#31867;&#22411;&#30340;&#35299;&#37322;&#21463;&#21040;&#20102;&#25991;&#29486;&#20013;&#30340;&#37325;&#35270;&#12290;&#31532;&#19968;&#31181;&#35299;&#37322;&#26159;&#20026;&#20915;&#31574;&#25552;&#20379;&#20805;&#20998;&#29702;&#30001;&#30340;&#35299;&#37322;&#65292;&#21363;&#32553;&#20889;&#20026;PI&#35299;&#37322;&#30340;&#35825;&#23548;&#24335;&#35299;&#37322;&#65307;&#31532;&#20108;&#31181;&#35299;&#37322;&#26159;&#20026;&#20309;&#19981;&#20570;&#20986;&#20854;&#20182;&#20915;&#31574;&#30340;&#35299;&#37322;&#65292;&#21363;&#23545;&#29031;&#24335;&#25110;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#24517;&#35201;&#29702;&#30001;&#12290;&#36825;&#20123;&#35299;&#37322;&#26159;&#20026;&#20108;&#20803;&#12289;&#31163;&#25955;&#21644;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#20026;&#36830;&#32493;&#29305;&#24449;&#30340;&#20998;&#31867;&#22120;&#23450;&#20041;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#23384;&#22312;&#38750;&#20108;&#20803;&#29305;&#24449;&#26102;&#65292;&#36825;&#20123;&#35299;&#37322;&#21487;&#20197;&#24471;&#21040;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#20174;&#32780;&#23548;&#33268;&#20102;&#19968;&#31867;&#26032;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#20379;&#26356;&#22810;&#20851;&#20110;&#20915;&#31574;&#21644;&#22522;&#30784;&#20998;&#31867;&#22120;&#30340;&#20449;&#24687;&#12290;&#24517;&#35201;&#21644;&#20805;&#20998;&#21407;&#22240;&#20063;&#34987;&#35777;&#26126;&#26159;&#23436;&#25972;&#21407;&#22240;&#30340;&#20027;&#35201;&#34164;&#21547;&#39033;&#21644;&#34987;&#34164;&#21547;&#39033;&#65292;&#21487;&#20197;&#20351;&#29992;&#37327;&#21270;&#31639;&#23376;&#33719;&#24471;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25913;&#36827;&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#21407;&#22240;&#30340;&#27010;&#24565;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#22320;&#36866;&#29992;&#20110;&#20855;&#26377;&#38750;&#20108;&#20803;&#29305;&#24449;&#30340;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Two types of explanations have received significant attention in the literature recently when analyzing the decisions made by classifiers. The first type explains why a decision was made and is known as a sufficient reason for the decision, also an abductive or PI-explanation. The second type explains why some other decision was not made and is known as a necessary reason for the decision, also a contrastive or counterfactual explanation. These explanations were defined for classifiers with binary, discrete and, in some cases, continuous features. We show that these explanations can be significantly improved in the presence of non-binary features, leading to a new class of explanations that relay more information about decisions and the underlying classifiers. Necessary and sufficient reasons were also shown to be the prime implicates and implicants of the complete reason for a decision, which can be obtained using a quantification operator. We show that our improved notions of necessa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31639;&#27861;&#20379;&#24212;&#38142;&#21450;&#20854;&#23545;&#31639;&#27861;&#31995;&#32479;&#27835;&#29702;&#21644;&#36131;&#20219;&#25152;&#24102;&#26469;&#30340;&#22256;&#38590;&#24433;&#21709;&#65292;&#35748;&#20026;&#31639;&#27861;&#36131;&#20219;&#30340;&#35752;&#35770;&#24517;&#39035;&#32771;&#34385;&#21040;&#20379;&#24212;&#38142;&#12290;</title><link>http://arxiv.org/abs/2304.14749</link><description>&lt;p&gt;
&#29702;&#35299;&#31639;&#27861;&#20379;&#24212;&#38142;&#20013;&#30340;&#36131;&#20219;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Understanding accountability in algorithmic supply chains. (arXiv:2304.14749v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14749
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31639;&#27861;&#20379;&#24212;&#38142;&#21450;&#20854;&#23545;&#31639;&#27861;&#31995;&#32479;&#27835;&#29702;&#21644;&#36131;&#20219;&#25152;&#24102;&#26469;&#30340;&#22256;&#38590;&#24433;&#21709;&#65292;&#35748;&#20026;&#31639;&#27861;&#36131;&#20219;&#30340;&#35752;&#35770;&#24517;&#39035;&#32771;&#34385;&#21040;&#20379;&#24212;&#38142;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#26415;&#21644;&#25919;&#31574;&#19978;&#23545;&#31639;&#27861;&#36131;&#20219;&#30340;&#25552;&#35758;&#24120;&#24120;&#35797;&#22270;&#22312;&#31038;&#20250;&#25216;&#26415;&#32972;&#26223;&#19979;&#29702;&#35299;&#31639;&#27861;&#31995;&#32479;&#65292;&#35748;&#35782;&#21040;&#23427;&#20204;&#30001;&#8220;&#22810;&#26041;&#8221;&#20849;&#21516;&#21046;&#36896;&#12290;&#28982;&#32780;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#31639;&#27861;&#31995;&#32479;&#20063;&#26159;&#30001;&#22810;&#20010;&#21442;&#19982;&#32773;&#32452;&#25104;&#30340;&#20379;&#24212;&#38142;&#29983;&#20135;&#12289;&#37096;&#32626;&#21644;&#20351;&#29992;&#30340;&#65292;&#23427;&#20204;&#20043;&#38388;&#36890;&#36807;&#25968;&#25454;&#27969;&#32852;&#31995;&#22312;&#19968;&#36215;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#31639;&#27861;&#20379;&#24212;&#38142;&#20013;&#19981;&#21516;&#21442;&#19982;&#32773;&#20043;&#38388;&#30340;&#21327;&#20316;&#26159;&#25512;&#21160;&#31995;&#32479;&#24182;&#20135;&#29983;&#29305;&#23450;&#32467;&#26524;&#30340;&#20027;&#35201;&#22240;&#32032;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#31639;&#27861;&#36131;&#20219;&#30340;&#35752;&#35770;&#24517;&#39035;&#32771;&#34385;&#21040;&#20379;&#24212;&#38142;&#20197;&#21450;&#23427;&#20204;&#23545;&#31639;&#27861;&#31995;&#32479;&#27835;&#29702;&#21644;&#36131;&#20219;&#25152;&#24102;&#26469;&#30340;&#22256;&#38590;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#30740;&#31350;&#31639;&#27861;&#20379;&#24212;&#38142;&#65292;&#23558;&#20854;&#23450;&#20301;&#20110;&#24191;&#27867;&#30340;&#25216;&#26415;&#21644;&#25919;&#27835;&#32463;&#27982;&#32972;&#26223;&#20013;&#65292;&#24182;&#30830;&#23450;&#20102;&#19968;&#20123;&#20851;&#38190;&#29305;&#24449;&#65292;&#24212;&#22312;&#26410;&#26469;&#30340;&#31639;&#27861;&#27835;&#29702;&#21644;&#36131;&#20219;&#30740;&#31350;&#20013;&#21152;&#20197;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Academic and policy proposals on algorithmic accountability often seek to understand algorithmic systems in their socio-technical context, recognising that they are produced by 'many hands'. Increasingly, however, algorithmic systems are also produced, deployed, and used within a supply chain comprising multiple actors tied together by flows of data between them. In such cases, it is the working together of an algorithmic supply chain of different actors who contribute to the production, deployment, use, and functionality that drives systems and produces particular outcomes. We argue that algorithmic accountability discussions must consider supply chains and the difficult implications they raise for the governance and accountability of algorithmic systems. In doing so, we explore algorithmic supply chains, locating them in their broader technical and political economic context and identifying some key features that should be understood in future work on algorithmic governance and accou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25506;&#32034;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#23398;&#20064;&#36710;&#36742;&#32500;&#20462;&#39046;&#22495;&#32452;&#20214;&#30340;&#29305;&#23450;&#26448;&#26009;&#65292;&#25104;&#21151;&#20811;&#26381;&#20102;&#25968;&#25454;&#31232;&#30095;&#24615;&#38382;&#39064;&#21644;&#32570;&#20047;&#27880;&#37322;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.14745</link><description>&lt;p&gt;
&#30001;&#20160;&#20040;&#26500;&#25104;&#65311;&#23398;&#20064;&#20462;&#36710;&#39046;&#22495;&#32452;&#20214;&#30340;&#21487;&#20449;&#26448;&#26009;
&lt;/p&gt;
&lt;p&gt;
Made of Steel? Learning Plausible Materials for Components in the Vehicle Repair Domain. (arXiv:2304.14745v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14745
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25506;&#32034;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#23398;&#20064;&#36710;&#36742;&#32500;&#20462;&#39046;&#22495;&#32452;&#20214;&#30340;&#29305;&#23450;&#26448;&#26009;&#65292;&#25104;&#21151;&#20811;&#26381;&#20102;&#25968;&#25454;&#31232;&#30095;&#24615;&#38382;&#39064;&#21644;&#32570;&#20047;&#27880;&#37322;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25506;&#32034;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#20013;&#30340;cloze&#20219;&#21153;&#26679;&#24335;&#35774;&#32622;&#26469;&#23398;&#20064;&#36710;&#36742;&#32500;&#20462;&#39046;&#22495;&#32452;&#20214;&#30340;&#29305;&#23450;&#26448;&#26009;&#65292;&#20197;&#20811;&#26381;&#32570;&#20047;&#27880;&#37322;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#32858;&#21512;&#20102;&#19968;&#32452;cloze&#26597;&#35810;&#27169;&#26495;&#30340;&#26174;&#33879;&#39044;&#27979;&#65292;&#24182;&#34920;&#26126;&#20351;&#29992;&#23567;&#22411;&#39640;&#36136;&#37327;&#25110;&#23450;&#21046;&#30340;&#32500;&#22522;&#30334;&#31185;&#35821;&#26009;&#24211;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#24403;&#25506;&#32034;&#36164;&#28304;&#32039;&#32570;&#30340;&#26367;&#20195;&#26041;&#26696;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#31934;&#31616;&#30340;PLM&#26126;&#26174;&#20248;&#20110;&#32463;&#20856;&#30340;&#22522;&#20110;&#27169;&#24335;&#30340;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#32771;&#34385;&#21040;&#25105;&#20204;&#39046;&#22495;&#29305;&#23450;&#32452;&#20214;&#30340;98&#65285;&#37117;&#26159;&#22810;&#35789;&#34920;&#36798;&#24335;&#65292;&#25105;&#20204;&#25104;&#21151;&#22320;&#21033;&#29992;&#32452;&#25104;&#24615;&#20551;&#35774;&#26469;&#35299;&#20915;&#25968;&#25454;&#31232;&#30095;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel approach to learn domain-specific plausible materials for components in the vehicle repair domain by probing Pretrained Language Models (PLMs) in a cloze task style setting to overcome the lack of annotated datasets. We devise a new method to aggregate salient predictions from a set of cloze query templates and show that domain-adaptation using either a small, high-quality or a customized Wikipedia corpus boosts performance. When exploring resource-lean alternatives, we find a distilled PLM clearly outperforming a classic pattern-based algorithm. Further, given that 98% of our domain-specific components are multiword expressions, we successfully exploit the compositionality assumption as a way to address data sparsity.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;LitCQD&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#19981;&#23436;&#25972;&#30693;&#35782;&#22270;&#20013;&#22238;&#31572;&#24102;&#26377;&#25968;&#23383;&#23383;&#38754;&#37327;&#30340;&#22797;&#26434;&#22810;&#36339;&#26597;&#35810;&#65292;&#24182;&#22312;&#21253;&#21547;&#25968;&#23383;&#23383;&#38754;&#37327;&#30340;&#22810;&#36339;&#26597;&#35810;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2304.14742</link><description>&lt;p&gt;
LitCQD&#65306;&#24102;&#26377;&#25968;&#20540;&#23383;&#38754;&#37327;&#30340;&#19981;&#23436;&#25972;&#30693;&#35782;&#22270;&#20013;&#30340;&#22810;&#36339;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
LitCQD: Multi-Hop Reasoning in Incomplete Knowledge Graphs with Numeric Literals. (arXiv:2304.14742v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14742
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;LitCQD&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#19981;&#23436;&#25972;&#30693;&#35782;&#22270;&#20013;&#22238;&#31572;&#24102;&#26377;&#25968;&#23383;&#23383;&#38754;&#37327;&#30340;&#22797;&#26434;&#22810;&#36339;&#26597;&#35810;&#65292;&#24182;&#22312;&#21253;&#21547;&#25968;&#23383;&#23383;&#38754;&#37327;&#30340;&#22810;&#36339;&#26597;&#35810;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#30693;&#35782;&#22270;&#65292;&#21253;&#25324;Wikidata&#65292;DBpedia&#21644;Yago&#22312;&#20869;&#65292;&#37117;&#26159;&#19981;&#23436;&#25972;&#30340;&#12290;&#38024;&#23545;&#36825;&#20123;&#19981;&#23436;&#25972;&#30340;&#30693;&#35782;&#22270;&#22238;&#31572;&#26597;&#35810;&#26159;&#19968;&#20010;&#37325;&#35201;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#19968;&#20123;&#26041;&#27861;&#65292;&#21253;&#25324;&#22797;&#26434;&#30340;&#26597;&#35810;&#20998;&#35299;&#65288;CQD&#65289;&#65292;&#20197;&#22238;&#31572;&#24102;&#26377;&#21512;&#21462;&#21644;&#26512;&#21462;&#30340;&#22797;&#26434;&#22810;&#36339;&#26597;&#35810;&#12290;&#28982;&#32780;&#65292;&#25152;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#37117;&#21482;&#32771;&#34385;&#30001;&#23454;&#20307;&#21644;&#20851;&#31995;&#32452;&#25104;&#30340;&#22270;&#65292;&#32780;&#24573;&#30053;&#20102;&#23383;&#38754;&#37327;&#20540;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LitCQD&#65292;&#19968;&#31181;&#29992;&#20110;&#22238;&#31572;&#21253;&#21547;&#25968;&#23383;&#23383;&#38754;&#37327;&#30340;&#22797;&#26434;&#22810;&#36339;&#26597;&#35810;&#30340;&#26041;&#27861;&#65306;LitCQD&#33021;&#22815;&#22238;&#31572;&#20855;&#26377;&#25968;&#23383;&#31572;&#26696;&#25110;&#28385;&#36275;&#25968;&#23383;&#32422;&#26463;&#30340;&#23454;&#20307;&#31572;&#26696;&#30340;&#26597;&#35810;&#12290;&#20363;&#22914;&#65292;&#23427;&#20801;&#35768;&#26597;&#35810;&#65288;1&#65289;&#20303;&#22312;&#32445;&#32422;&#24182;&#19988;&#24180;&#40836;&#22312;&#26576;&#20010;&#33539;&#22260;&#20869;&#30340;&#20154;&#65292;&#20197;&#21450;&#65288;2&#65289;&#20303;&#22312;&#32445;&#32422;&#30340;&#20154;&#30340;&#24179;&#22343;&#24180;&#40836;&#12290;&#25105;&#20204;&#22312;&#20855;&#26377;&#21644;&#19981;&#20855;&#26377;&#23383;&#38754;&#37327;&#20540;&#30340;&#26597;&#35810;&#31867;&#22411;&#19978;&#35780;&#20272;&#20102;LitCQD&#12290;&#25105;&#20204;&#29983;&#25104;&#20102;&#19968;&#20010;&#21253;&#21547;&#25968;&#23383;&#23383;&#38754;&#37327;&#30340;&#22810;&#36339;&#26597;&#35810;&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#38024;&#23545;Wikidata&#30340;&#19968;&#20010;&#23376;&#38598;&#23545;LitCQD&#36827;&#34892;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LitCQD&#22312;&#27492;&#31867;&#26597;&#35810;&#19978;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most real-world knowledge graphs, including Wikidata, DBpedia, and Yago are incomplete. Answering queries on such incomplete graphs is an important, but challenging problem. Recently, a number of approaches, including complex query decomposition (CQD), have been proposed to answer complex, multi-hop queries with conjunctions and disjunctions on such graphs. However, all state-of-the-art approaches only consider graphs consisting of entities and relations, neglecting literal values. In this paper, we propose LitCQD -- an approach to answer complex, multi-hop queries where both the query and the knowledge graph can contain numeric literal values: LitCQD can answer queries having numerical answers or having entity answers satisfying numerical constraints. For example, it allows to query (1)~persons living in New York having a certain age, and (2)~the average age of persons living in New York. We evaluate LitCQD on query types with and without literal values. To evaluate LitCQD, we generat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#23558;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#19982;&#20225;&#19994;&#39046;&#22495;&#30693;&#35782;&#30456;&#32467;&#21512;&#65292;&#20197;&#26367;&#20195;&#25163;&#21160;&#21019;&#24314;&#30340;ML&#31649;&#36947;&#65292;&#20026;&#20013;&#23567;&#22411;&#20225;&#19994;&#23454;&#29616;&#33258;&#21160;&#21270;&#20215;&#26684;&#39044;&#27979;&#25552;&#20379;&#21487;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.14735</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#20215;&#26684;&#39044;&#27979;&#24212;&#29992;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Automated Machine Learning Methods for Price Forecasting Applications. (arXiv:2304.14735v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14735
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#23558;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#19982;&#20225;&#19994;&#39046;&#22495;&#30693;&#35782;&#30456;&#32467;&#21512;&#65292;&#20197;&#26367;&#20195;&#25163;&#21160;&#21019;&#24314;&#30340;ML&#31649;&#36947;&#65292;&#20026;&#20013;&#23567;&#22411;&#20225;&#19994;&#23454;&#29616;&#33258;&#21160;&#21270;&#20215;&#26684;&#39044;&#27979;&#25552;&#20379;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#31354;&#38388;&#21644;&#26102;&#38388;&#30340;&#20215;&#26684;&#27874;&#21160;&#65292;&#22312;&#20108;&#25163;&#24314;&#31569;&#35774;&#22791;&#30340;&#20215;&#26684;&#39044;&#27979;&#26041;&#38754;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#22522;&#20110;&#24403;&#21069;&#24066;&#22330;&#25968;&#25454;&#33258;&#21160;&#21270;&#39044;&#27979;&#36807;&#31243;&#20855;&#26377;&#26497;&#39640;&#30340;&#20852;&#36259;&#12290;&#21363;&#20351;&#23558;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#24212;&#29992;&#20110;&#36825;&#20123;&#25968;&#25454;&#26159;&#39044;&#27979;&#26576;&#20123;&#24037;&#20855;&#27531;&#20540;&#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#20294;&#30001;&#20110;&#20013;&#23567;&#22411;&#20225;&#19994;&#30340;ML&#19987;&#19994;&#30693;&#35782;&#19981;&#36275;&#65292;&#22240;&#27492;&#24456;&#38590;&#23454;&#29616;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#29992;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#65288;AutoML&#65289;&#35299;&#20915;&#26041;&#26696;&#20195;&#26367;&#25163;&#21160;&#21019;&#24314;&#30340;ML&#31649;&#36947;&#30340;&#21487;&#33021;&#24615;&#65292;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#33258;&#21160;&#29983;&#25104;&#24213;&#23618;&#31649;&#36947;&#12290;&#25105;&#20204;&#23558;AutoML&#26041;&#27861;&#19982;&#20844;&#21496;&#30340;&#39046;&#22495;&#30693;&#35782;&#30456;&#32467;&#21512;&#12290;&#22522;&#20110;CRISP-DM&#36807;&#31243;&#65292;&#25105;&#20204;&#23558;&#25163;&#21160;ML&#31649;&#36947;&#20998;&#20026;&#26426;&#22120;&#23398;&#20064;&#37096;&#20998;&#21644;&#38750;&#26426;&#22120;&#23398;&#20064;&#37096;&#20998;&#12290;&#20026;&#20102;&#32771;&#34385;&#25152;&#26377;&#22797;&#26434;&#30340;&#24037;&#19994;&#35201;&#27714;&#24182;&#23637;&#31034;&#25105;&#20204;&#26032;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#21517;&#20026;&#8220;&#26041;&#27861;&#35780;&#20272;&#8221;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Price forecasting for used construction equipment is a challenging task due to spatial and temporal price fluctuations. It is thus of high interest to automate the forecasting process based on current market data. Even though applying machine learning (ML) to these data represents a promising approach to predict the residual value of certain tools, it is hard to implement for small and medium-sized enterprises due to their insufficient ML expertise. To this end, we demonstrate the possibility of substituting manually created ML pipelines with automated machine learning (AutoML) solutions, which automatically generate the underlying pipelines. We combine AutoML methods with the domain knowledge of the companies. Based on the CRISP-DM process, we split the manual ML pipeline into a machine learning and non-machine learning part. To take all complex industrial requirements into account and to demonstrate the applicability of our new approach, we designed a novel metric named method evalua
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#30495;&#23454;&#19990;&#30028;&#35013;&#31665;&#38382;&#39064;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#23454;&#20363;&#29983;&#25104;&#22120;&#65292;&#21487;&#20197;&#29992;&#26469;&#27604;&#36739;&#19981;&#21516;&#35013;&#31665;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.14712</link><description>&lt;p&gt;
&#30495;&#23454;&#19990;&#30028;&#19977;&#32500;&#35013;&#31665;&#38382;&#39064;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#23454;&#20363;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
Benchmark dataset and instance generator for Real-World Three-Dimensional Bin Packing Problems. (arXiv:2304.14712v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#30495;&#23454;&#19990;&#30028;&#35013;&#31665;&#38382;&#39064;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#23454;&#20363;&#29983;&#25104;&#22120;&#65292;&#21487;&#20197;&#29992;&#26469;&#27604;&#36739;&#19981;&#21516;&#35013;&#31665;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#30495;&#23454;&#19990;&#30028;&#35013;&#31665;&#38382;&#39064;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#30001;12&#20010;&#23454;&#20363;&#32452;&#25104;&#65292;&#28085;&#30422;&#20102;&#19981;&#21516;&#22823;&#23567;&#21644;&#29992;&#25143;&#38656;&#27714;&#30340;&#38382;&#39064;&#22797;&#26434;&#24230;&#27700;&#24179;&#65288;&#21253;&#21547;&#20174;38&#21040;53&#20010;&#21253;&#35065;&#30340;&#25968;&#37327;&#65289;&#12290;&#23454;&#38469;&#19978;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20960;&#20010;&#38754;&#21521;&#30495;&#23454;&#19990;&#30028;&#30340;&#38480;&#21046;&#26465;&#20214;&#26469;&#26500;&#24314;&#36825;&#20123;&#23454;&#20363;&#65306;i)&#29289;&#21697;&#21644;&#31665;&#23376;&#23610;&#23544;&#65292;ii)&#37325;&#37327;&#38480;&#21046;&#65292;iii)&#21253;&#31867;&#21035;&#20043;&#38388;&#30340;&#20146;&#21644;&#24615;&#65292;iv)&#21253;&#35013;&#39034;&#24207;&#30340;&#20559;&#22909;&#21644;v)&#36127;&#36733;&#24179;&#34913;&#12290;&#38500;&#20102;&#25968;&#25454;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#33258;&#20027;&#24320;&#21457;&#30340;Python&#33050;&#26412;&#29992;&#20110;&#25968;&#25454;&#38598;&#29983;&#25104;&#65292;&#31216;&#20026;Q4RealBPP-DataGen&#12290;&#35813;&#22522;&#20934;&#39318;&#20808;&#34987;&#35774;&#35745;&#29992;&#20110;&#35780;&#20272;&#37327;&#23376;&#27714;&#35299;&#22120;&#65292;&#22240;&#27492;&#36825;&#32452;&#23454;&#20363;&#30340;&#29305;&#24449;&#26159;&#25353;&#29031;&#37327;&#23376;&#35774;&#22791;&#30340;&#24403;&#21069;&#38480;&#21046;&#35774;&#35745;&#30340;&#12290;&#27492;&#22806;&#65292;&#25968;&#25454;&#38598;&#29983;&#25104;&#22120;&#21253;&#21547;&#22312;&#20869;&#65292;&#20801;&#35768;&#26500;&#24314;&#36890;&#29992;&#22522;&#20934;&#12290;&#26412;&#25991;&#20171;&#32461;&#30340;&#25968;&#25454;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20934;&#65292;&#21487;&#20197;&#29992;&#26469;&#27604;&#36739;&#19981;&#21516;&#26041;&#27861;&#30340;&#24615;&#33021;&#21644;&#23545;&#27604;&#31639;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, a benchmark for real-world bin packing problems is proposed. This dataset is composed of 12 instances comprehending different levels of problem complexity regarding size (with the number of packages ranging from 38 to 53) and user-defined requirements. In fact, several real-world oriented restrictions have been considered for building these instances: i) items and bins dimensions, ii) weight restrictions, iii) affinities among packages categories iv) preferences for package ordering and v) load balancing. Besides the data, we also provide an own-developed Python script for the dataset generation, coined as Q4RealBPP-DataGen. The benchmark was firstly proposed to evaluate quantum solvers, therefore the characteristic of this set of instances were designed according to the current limitations of quantum devices. Additionally, the dataset generator is included to allow the construction of general-purpose benchmarks. The data introduced on this paper provides a baseline that
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#21360;&#24230;&#25163;&#35821;&#30340;&#25163;&#35821;&#35782;&#21035;&#31995;&#32479;&#65292;&#26088;&#22312;&#23454;&#26102;&#23558;&#25163;&#35821;&#32763;&#35793;&#25104;&#25991;&#26412;&#20197;&#35299;&#20915;&#32843;&#21713;&#20154;&#21644;&#21548;&#38556;&#20154;&#22763;&#20132;&#27969;&#38556;&#30861;&#12290;</title><link>http://arxiv.org/abs/2304.14710</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#20687;&#30340;&#21360;&#24230;&#25163;&#35821;&#35782;&#21035;&#65306;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#23454;&#29992;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Image-based Indian Sign Language Recognition: A Practical Review using Deep Neural Networks. (arXiv:2304.14710v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14710
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#21360;&#24230;&#25163;&#35821;&#30340;&#25163;&#35821;&#35782;&#21035;&#31995;&#32479;&#65292;&#26088;&#22312;&#23454;&#26102;&#23558;&#25163;&#35821;&#32763;&#35793;&#25104;&#25991;&#26412;&#20197;&#35299;&#20915;&#32843;&#21713;&#20154;&#21644;&#21548;&#38556;&#20154;&#22763;&#20132;&#27969;&#38556;&#30861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32843;&#21713;&#20154;&#21644;&#21548;&#38556;&#20154;&#22763;&#20351;&#29992;&#25163;&#35821;&#26469;&#34920;&#36798;&#33258;&#24049;&#30340;&#24847;&#24605;&#12290;&#34429;&#28982;&#25163;&#35821;&#26159;&#35299;&#20915;&#21548;&#38556;&#20154;&#22763;&#20132;&#27969;&#22256;&#38590;&#30340;&#19968;&#31181;&#26041;&#24335;&#65292;&#20294;&#22823;&#22810;&#25968;&#20154;&#20173;&#28982;&#26080;&#27861;&#29702;&#35299;&#36825;&#31181;&#35821;&#35328;&#65292;&#36896;&#25104;&#20132;&#27969;&#38556;&#30861;&#12290;&#25163;&#35821;&#35782;&#21035;&#31995;&#32479;&#26159;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#24517;&#35201;&#25163;&#27573;&#12290;&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#30340;&#26159;&#24320;&#21457;&#19968;&#31181;&#33021;&#22815;&#23454;&#26102;&#32763;&#35793;&#25163;&#35821;&#20026;&#25991;&#26412;&#30340;&#21333;&#35789;&#32423;&#25163;&#35821;&#35782;&#21035;&#31995;&#32479;&#12290;&#38024;&#23545;&#21360;&#24230;&#32843;&#21713;&#20154;&#21644;&#21548;&#38556;&#20154;&#22763;&#30340;&#38656;&#27714;&#65292;&#25105;&#20204;&#24320;&#23637;&#20102;&#20197;&#21360;&#24230;&#25163;&#35821;&#20026;&#22522;&#30784;&#30340;&#25163;&#35821;&#35782;&#21035;&#31995;&#32479;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
People with vocal and hearing disabilities use sign language to express themselves using visual gestures and signs. Although sign language is a solution for communication difficulties faced by deaf people, there are still problems as most of the general population cannot understand this language, creating a communication barrier, especially in places such as banks, airports, supermarkets, etc. [1]. A sign language recognition(SLR) system is a must to solve this problem. The main focus of this model is to develop a real-time word-level sign language recognition system that would translate sign language to text. Much research has been done on ASL(American sign language). Thus, we have worked on ISL(Indian sign language) to cater to the needs of the deaf and hard-of-hearing community of India[2]. In this research, we provide an Indian Sign Language-based Sign Language recognition system. For this analysis, the user must be able to take pictures of hand movements using a web camera, and th
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;X-RLflow&#65292;&#29992;&#20110;&#26367;&#25442;&#31070;&#32463;&#32593;&#32476;&#30340;&#23376;&#22270;&#65292;&#20197;&#27714;&#24471;&#26356;&#20248;&#30340;&#35745;&#31639;&#22270;&#32467;&#26500;&#65292;&#21487;&#22312;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36234;&#29616;&#26377;&#25216;&#26415;&#30340;&#36229;&#20248;&#21270;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2304.14698</link><description>&lt;p&gt;
X-RLflow&#65306;&#38754;&#21521;&#31070;&#32463;&#32593;&#32476;&#23376;&#22270;&#36716;&#25442;&#30340;&#22270;&#24418;&#22686;&#24378;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
X-RLflow: Graph Reinforcement Learning for Neural Network Subgraphs Transformation. (arXiv:2304.14698v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14698
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;X-RLflow&#65292;&#29992;&#20110;&#26367;&#25442;&#31070;&#32463;&#32593;&#32476;&#30340;&#23376;&#22270;&#65292;&#20197;&#27714;&#24471;&#26356;&#20248;&#30340;&#35745;&#31639;&#22270;&#32467;&#26500;&#65292;&#21487;&#22312;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36234;&#29616;&#26377;&#25216;&#26415;&#30340;&#36229;&#20248;&#21270;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24352;&#37327;&#22270;&#36229;&#20248;&#21270;&#31995;&#32479;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#30340;&#19968;&#31995;&#21015;&#23376;&#22270;&#26367;&#25442;&#26469;&#25214;&#21040;&#26368;&#20248;&#30340;&#35745;&#31639;&#22270;&#32467;&#26500;&#12290;&#36825;&#20010;&#22270;&#36716;&#25442;&#36807;&#31243;&#33258;&#28982;&#32780;&#28982;&#22320;&#33853;&#20837;&#20102;&#24207;&#21015;&#20915;&#31574;&#26694;&#26550;&#20013;, &#29616;&#26377;&#31995;&#32479;&#36890;&#24120;&#37319;&#29992;&#36138;&#24515;&#25628;&#32034;&#26041;&#27861;&#65292;&#26080;&#27861;&#25506;&#32034;&#25972;&#20010;&#25628;&#32034;&#31354;&#38388;&#65292;&#22240;&#20026;&#23427;&#19981;&#33021;&#23481;&#24525;&#20020;&#26102;&#30340;&#24615;&#33021;&#25439;&#22833;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064; (RL) &#30340;&#26367;&#20195;&#25628;&#32034;&#26041;&#27861;&#26469;&#35299;&#20915;&#24352;&#37327;&#22270;&#36229;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;X-RLflow&#65292;&#21487;&#20197;&#23398;&#20064;&#25191;&#34892;&#31070;&#32463;&#32593;&#32476;&#25968;&#25454;&#27969;&#22270;&#37325;&#20889;&#65292;&#19968;&#27425;&#26367;&#25442;&#19968;&#20010;&#23376;&#22270;&#12290;X-RLflow &#22522;&#20110;&#19968;&#31181;&#26080;&#27169;&#22411; RL &#20195;&#29702;&#65292;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476; (GNN) &#23545;&#30446;&#26631;&#35745;&#31639;&#22270;&#36827;&#34892;&#32534;&#30721;&#65292;&#24182;&#36845;&#20195;&#36755;&#20986;&#36716;&#25442;&#21518;&#30340;&#35745;&#31639;&#22270;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36234;&#29616;&#26377;&#25216;&#26415;&#30340;&#36229;&#20248;&#21270;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tensor graph superoptimisation systems perform a sequence of subgraph substitution to neural networks, to find the optimal computation graph structure. Such a graph transformation process naturally falls into the framework of sequential decision-making, and existing systems typically employ a greedy search approach, which cannot explore the whole search space as it cannot tolerate a temporary loss of performance. In this paper, we address the tensor graph superoptimisation problem by exploring an alternative search approach, reinforcement learning (RL). Our proposed approach, X-RLflow, can learn to perform neural network dataflow graph rewriting, which substitutes a subgraph one at a time. X-RLflow is based on a model-free RL agent that uses a graph neural network (GNN) to encode the target computation graph and outputs a transformed computation graph iteratively. We show that our approach can outperform state-of-the-art superoptimisation systems over a range of deep learning models an
&lt;/p&gt;</description></item><item><title>NeuralKG-ind&#26159;&#19968;&#31181;Python&#24211;&#65292;&#29992;&#20110;&#24402;&#32435;&#24335;&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;&#23398;&#20064;&#12290;&#23427;&#21253;&#25324;&#26631;&#20934;&#21270;&#30340;&#27969;&#31243;&#12289;&#20016;&#23500;&#30340;&#29616;&#26377;&#26041;&#27861;&#12289;&#35299;&#32806;&#30340;&#27169;&#22359;&#21644;&#20840;&#38754;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;&#36825;&#20010;&#24211;&#21487;&#20197;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#21644;&#24037;&#31243;&#24072;&#36731;&#26494;&#27604;&#36739;&#19981;&#21516;&#30340;&#24402;&#32435;&#24335;KGRL&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.14678</link><description>&lt;p&gt;
NeuralKG-ind: &#19968;&#31181;Python&#24211;&#65292;&#29992;&#20110;&#24402;&#32435;&#24335;&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
NeuralKG-ind: A Python Library for Inductive Knowledge Graph Representation Learning. (arXiv:2304.14678v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14678
&lt;/p&gt;
&lt;p&gt;
NeuralKG-ind&#26159;&#19968;&#31181;Python&#24211;&#65292;&#29992;&#20110;&#24402;&#32435;&#24335;&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;&#23398;&#20064;&#12290;&#23427;&#21253;&#25324;&#26631;&#20934;&#21270;&#30340;&#27969;&#31243;&#12289;&#20016;&#23500;&#30340;&#29616;&#26377;&#26041;&#27861;&#12289;&#35299;&#32806;&#30340;&#27169;&#22359;&#21644;&#20840;&#38754;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;&#36825;&#20010;&#24211;&#21487;&#20197;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#21644;&#24037;&#31243;&#24072;&#36731;&#26494;&#27604;&#36739;&#19981;&#21516;&#30340;&#24402;&#32435;&#24335;KGRL&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#21160;&#24577;&#29305;&#24615;&#65292;&#36817;&#24180;&#26469;&#25552;&#20986;&#20102;&#35768;&#22810;&#24402;&#32435;&#24335;&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;&#23398;&#20064;&#65288;KGRL&#65289;&#26041;&#27861;&#65292;&#19987;&#27880;&#20110;&#23454;&#29616;&#23545;&#26032;&#23454;&#20307;&#30340;&#39044;&#27979;&#12290;NeuralKG-ind&#26159;&#20316;&#20026;NeuralKG&#24211;&#30340;&#37325;&#35201;&#26356;&#26032;&#30340;&#24402;&#32435;&#24335;KGRL&#30340;&#31532;&#19968;&#20010;&#24211;&#12290;&#23427;&#21253;&#25324;&#26631;&#20934;&#21270;&#30340;&#27969;&#31243;&#12289;&#20016;&#23500;&#30340;&#29616;&#26377;&#26041;&#27861;&#12289;&#35299;&#32806;&#30340;&#27169;&#22359;&#21644;&#20840;&#38754;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;&#20351;&#29992;NeuralKG-ind&#65292;&#30740;&#31350;&#20154;&#21592;&#21644;&#24037;&#31243;&#24072;&#21487;&#20197;&#36731;&#26494;&#22320;&#22797;&#29616;&#12289;&#37325;&#26032;&#24320;&#21457;&#21644;&#27604;&#36739;&#24402;&#32435;&#24335;KGRL&#26041;&#27861;&#12290;NeuralKG-ind&#30340;&#24211;&#12289;&#23454;&#39564;&#26041;&#27861;&#21644;&#27169;&#22411;&#37325;&#26032;&#23454;&#29616;&#30340;&#32467;&#26524;&#37117;&#22312; https://github.com/zjukg/NeuralKG/tree/ind &#19978;&#20844;&#24320;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since the dynamic characteristics of knowledge graphs, many inductive knowledge graph representation learning (KGRL) works have been proposed in recent years, focusing on enabling prediction over new entities. NeuralKG-ind is the first library of inductive KGRL as an important update of NeuralKG library. It includes standardized processes, rich existing methods, decoupled modules, and comprehensive evaluation metrics. With NeuralKG-ind, it is easy for researchers and engineers to reproduce, redevelop, and compare inductive KGRL methods. The library, experimental methodologies, and model re-implementing results of NeuralKG-ind are all publicly released at https://github.com/zjukg/NeuralKG/tree/ind .
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#21307;&#30103;&#20445;&#20581;NLP&#39046;&#22495;&#20013;&#30340;&#25552;&#31034;&#24037;&#31243;&#26368;&#26032;&#36827;&#23637;&#65292;&#24378;&#35843;&#20854;&#22312;&#38382;&#31572;&#31995;&#32479;&#12289;&#25991;&#26412;&#25688;&#35201;&#21644;&#26426;&#22120;&#32763;&#35793;&#31561;&#24212;&#29992;&#20013;&#30340;&#36129;&#29486;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#26377;&#29992;&#36164;&#28304;&#21644;&#26725;&#26753;&#65292;&#20197;&#26356;&#22909;&#22320;&#25506;&#32034;&#25552;&#31034;&#24037;&#31243;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2304.14670</link><description>&lt;p&gt;
&#21307;&#30103;&#20445;&#20581;&#30340;&#25552;&#31034;&#24037;&#31243;:  &#26041;&#27861;&#21644;&#24212;&#29992;. (arXiv:2304.14670v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
Prompt Engineering for Healthcare: Methodologies and Applications. (arXiv:2304.14670v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14670
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#21307;&#30103;&#20445;&#20581;NLP&#39046;&#22495;&#20013;&#30340;&#25552;&#31034;&#24037;&#31243;&#26368;&#26032;&#36827;&#23637;&#65292;&#24378;&#35843;&#20854;&#22312;&#38382;&#31572;&#31995;&#32479;&#12289;&#25991;&#26412;&#25688;&#35201;&#21644;&#26426;&#22120;&#32763;&#35793;&#31561;&#24212;&#29992;&#20013;&#30340;&#36129;&#29486;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#26377;&#29992;&#36164;&#28304;&#21644;&#26725;&#26753;&#65292;&#20197;&#26356;&#22909;&#22320;&#25506;&#32034;&#25552;&#31034;&#24037;&#31243;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#20171;&#32461;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#20869;&#21307;&#30103;&#20445;&#20581;&#25552;&#31034;&#24037;&#31243;&#26368;&#26032;&#30340;&#36827;&#23637;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#25552;&#20379;&#19968;&#20010;&#31616;&#35201;&#30340;&#25552;&#31034;&#24037;&#31243;&#21457;&#23637;&#27010;&#36848;&#65292;&#24182;&#24378;&#35843;&#20854;&#23545;&#21307;&#30103;&#20445;&#20581;NLP&#24212;&#29992;&#22914;&#38382;&#31572;&#31995;&#32479;&#12289;&#25991;&#26412;&#25688;&#35201;&#21644;&#26426;&#22120;&#32763;&#35793;&#30340;&#37325;&#35201;&#36129;&#29486;&#12290;&#38543;&#30528;&#36890;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#26029;&#25913;&#36827;&#65292;&#25552;&#31034;&#24037;&#31243;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#37325;&#35201;&#24615;&#36234;&#26469;&#36234;&#31361;&#20986;&#12290;&#26412;&#25991;&#30340;&#30446;&#30340;&#26159;&#20026;&#21307;&#30103;&#20445;&#20581;NLP&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#26377;&#29992;&#30340;&#36164;&#28304;&#21644;&#26725;&#26753;&#65292;&#26356;&#22909;&#22320;&#25506;&#32034;&#25552;&#31034;&#24037;&#31243;&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#24076;&#26395;&#26412;&#25991;&#21487;&#20197;&#25552;&#20379;&#26032;&#30340;&#24605;&#36335;&#65292;&#28608;&#21457;&#21307;&#30103;NLP&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#30340;&#20805;&#20998;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This review will introduce the latest advances in prompt engineering in the field of natural language processing (NLP) for the medical domain. First, we will provide a brief overview of the development of prompt engineering and emphasize its significant contributions to healthcare NLP applications such as question-answering systems, text summarization, and machine translation. With the continuous improvement of general large language models, the importance of prompt engineering in the healthcare domain is becoming increasingly prominent. The aim of this article is to provide useful resources and bridges for healthcare NLP researchers to better explore the application of prompt engineering in this field. We hope that this review can provide new ideas and inspire ample possibilities for research and application in medical NLP.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;ICU&#20303;&#38498;&#26102;&#38388;&#39044;&#27979;&#30340;&#32852;&#37030;&#23398;&#20064;&#23458;&#25143;&#31471;&#25307;&#21215;&#31574;&#30053;&#65292;&#36890;&#36807;&#20934;&#21017;&#12289;&#30456;&#20284;&#24615;&#21644;&#32858;&#31867;&#65292;&#21487;&#20197;&#38477;&#20302;&#36890;&#20449;&#24320;&#38144;&#21644;&#35757;&#32451;&#25104;&#26412;&#65292;&#21516;&#26102;&#20445;&#25345;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.14663</link><description>&lt;p&gt;
&#22522;&#20110;ICU&#20303;&#38498;&#26102;&#38388;&#39044;&#27979;&#30340;&#32852;&#37030;&#23398;&#20064;&#23458;&#25143;&#31471;&#25307;&#21215;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Client Recruitment for Federated Learning in ICU Length of Stay Prediction. (arXiv:2304.14663v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14663
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;ICU&#20303;&#38498;&#26102;&#38388;&#39044;&#27979;&#30340;&#32852;&#37030;&#23398;&#20064;&#23458;&#25143;&#31471;&#25307;&#21215;&#31574;&#30053;&#65292;&#36890;&#36807;&#20934;&#21017;&#12289;&#30456;&#20284;&#24615;&#21644;&#32858;&#31867;&#65292;&#21487;&#20197;&#38477;&#20302;&#36890;&#20449;&#24320;&#38144;&#21644;&#35757;&#32451;&#25104;&#26412;&#65292;&#21516;&#26102;&#20445;&#25345;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#21307;&#30103;&#21644;&#20445;&#20581;&#39046;&#22495;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#21644;&#25913;&#36827;&#12290;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#21307;&#30103;&#39046;&#22495;&#20013;&#26377;&#22823;&#37327;&#30340;&#36825;&#26679;&#30340;&#25968;&#25454;&#65292;&#20294;&#26159;&#36825;&#20123;&#25968;&#25454;&#26159;&#20998;&#25955;&#30340;&#12290;&#32852;&#37030;&#23398;&#20064;&#25216;&#26415;&#38750;&#24120;&#36866;&#21512;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;&#26412;&#25991;&#38024;&#23545;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#23458;&#25143;&#31471;&#36873;&#25321;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;ICU&#20303;&#38498;&#26102;&#38388;&#39044;&#27979;&#30340;&#32852;&#37030;&#23398;&#20064;&#23458;&#25143;&#31471;&#25307;&#21215;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#31574;&#30053;&#21253;&#25324;&#20960;&#20010;&#38454;&#27573;&#65306;&#65288;i&#65289;&#22522;&#20110;&#20934;&#21017;&#30340;&#23458;&#25143;&#31471;&#39044;&#36873;&#65307;&#65288;ii&#65289;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#23458;&#25143;&#31471;&#36873;&#25321;&#65307;&#21644;&#65288;iii&#65289;&#23458;&#25143;&#31471;&#32858;&#31867;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31574;&#30053;&#21487;&#20197;&#38477;&#20302;&#36890;&#20449;&#24320;&#38144;&#21644;&#35757;&#32451;&#25104;&#26412;&#65292;&#21516;&#26102;&#20445;&#25345;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine and deep learning methods for medical and healthcare applications have shown significant progress and performance improvement in recent years. These methods require vast amounts of training data which are available in the medical sector, albeit decentralized. Medical institutions generate vast amounts of data for which sharing and centralizing remains a challenge as the result of data and privacy regulations. The federated learning technique is well-suited to tackle these challenges. However, federated learning comes with a new set of open problems related to communication overhead, efficient parameter aggregation, client selection strategies and more. In this work, we address the step prior to the initiation of a federated network for model training, client recruitment. By intelligently recruiting clients, communication overhead and overall cost of training can be reduced without sacrificing predictive performance. Client recruitment aims at pre-excluding potential clients fro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35843;&#22522;&#20934;&#27979;&#35797;&#29983;&#25104;&#22120;&#21644;&#19987;&#29992;&#27714;&#35299;&#22120;&#29992;&#20110;&#24050;&#30693;&#24085;&#32047;&#25176;&#21069;&#27839;&#30340;&#22810;&#30446;&#26631;&#35268;&#21010;&#38382;&#39064;&#65292;&#24182;&#19988;&#36890;&#36807;&#23454;&#20363;&#30340;&#27714;&#35299;&#23637;&#31034;&#20102;&#35813;&#27714;&#35299;&#22120;&#30340;&#21487;&#34892;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.14659</link><description>&lt;p&gt;
MultiZenoTravel&#65306;&#24050;&#30693;&#24085;&#32047;&#25176;&#21069;&#27839;&#30340;&#22810;&#30446;&#26631;&#35268;&#21010;&#21487;&#35843;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
MultiZenoTravel: a Tunable Benchmark for Multi-Objective Planning with Known Pareto Front. (arXiv:2304.14659v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14659
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35843;&#22522;&#20934;&#27979;&#35797;&#29983;&#25104;&#22120;&#21644;&#19987;&#29992;&#27714;&#35299;&#22120;&#29992;&#20110;&#24050;&#30693;&#24085;&#32047;&#25176;&#21069;&#27839;&#30340;&#22810;&#30446;&#26631;&#35268;&#21010;&#38382;&#39064;&#65292;&#24182;&#19988;&#36890;&#36807;&#23454;&#20363;&#30340;&#27714;&#35299;&#23637;&#31034;&#20102;&#35813;&#27714;&#35299;&#22120;&#30340;&#21487;&#34892;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#30446;&#26631;&#20154;&#24037;&#26234;&#33021;&#35268;&#21010;&#32570;&#20047;&#23637;&#31034;&#24050;&#30693;&#24085;&#32047;&#25176;&#21069;&#27839;&#22522;&#20934;&#27979;&#35797;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35843;&#22522;&#20934;&#27979;&#35797;&#29983;&#25104;&#22120;&#21644;&#19968;&#20010;&#19987;&#29992;&#27714;&#35299;&#22120;&#65292;&#21487;&#35777;&#26126;&#35745;&#31639;&#20986;&#25152;&#24471;&#23454;&#20363;&#30340;&#30495;&#23454;&#24085;&#32047;&#25176;&#21069;&#27839;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20010;&#21629;&#39064;&#65292;&#25551;&#36848;&#20102;&#35813;&#38382;&#39064;&#30340;&#21463;&#38480;&#29256;&#26412;&#30340;&#26368;&#20339;&#35745;&#21010;&#29305;&#24449;&#65292;&#28982;&#21518;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#19968;&#33324;&#38382;&#39064;&#31616;&#21270;&#20026;&#21463;&#38480;&#38382;&#39064;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26500;&#36896;&#24615;&#26041;&#27861;&#26469;&#25214;&#21040;&#25152;&#26377;&#24085;&#32047;&#25176;&#26368;&#20248;&#35745;&#21010;&#24182;&#35752;&#35770;&#20102;&#31639;&#27861;&#30340;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#23454;&#29616;&#65292;&#20351;&#27714;&#35299;&#22120;&#21487;&#20197;&#22312;&#21512;&#29702;&#30340;&#26102;&#38388;&#20869;&#22788;&#29702;&#29616;&#23454;&#24773;&#20917;&#30340;&#23454;&#20363;&#12290;&#26368;&#21518;&#65292;&#20316;&#20026;&#23454;&#38469;&#28436;&#31034;&#65292;&#25105;&#20204;&#20351;&#29992;&#27492;&#27714;&#35299;&#22120;&#22312;&#32771;&#34385;50&#20010;&#26368;&#22823;&#26426;&#22330;&#20043;&#38388;&#36335;&#32447;&#12289;&#26426;&#22330;&#38388;&#30340;&#29699;&#38754;&#36317;&#31163;&#21644;&#19968;&#20010;&#34394;&#26500;&#30340;&#39118;&#38505;&#30340;&#24773;&#20917;&#19979;&#65292;&#25214;&#21040;&#20102;&#19990;&#30028;&#19978;&#26368;&#22823;&#30340;&#20004;&#20010;&#26426;&#22330;&#20043;&#38388;&#25152;&#26377;&#24085;&#32047;&#25176;&#26368;&#20248;&#35745;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-objective AI planning suffers from a lack of benchmarks exhibiting known Pareto Fronts. In this work, we propose a tunable benchmark generator, together with a dedicated solver that provably computes the true Pareto front of the resulting instances. First, we prove a proposition allowing us to characterize the optimal plans for a constrained version of the problem, and then show how to reduce the general problem to the constrained one. Second, we provide a constructive way to find all the Pareto-optimal plans and discuss the complexity of the algorithm. We provide an implementation that allows the solver to handle realistic instances in a reasonable time. Finally, as a practical demonstration, we used this solver to find all Pareto-optimal plans between the two largest airports in the world, considering the routes between the 50 largest airports, spherical distances between airports and a made-up risk.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#26174;&#24615;&#27807;&#36890;&#21040;&#38544;&#24615;&#21512;&#20316;&#30340;&#26032;&#21327;&#20316;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#33539;&#24335;&#65292;&#36890;&#36807;&#22312;&#20195;&#29702;&#20043;&#38388;&#20998;&#20139;&#20449;&#24687;&#21644;&#20351;&#29992;&#26412;&#22320;&#36712;&#36857;&#37325;&#24314;&#20449;&#24687;&#26469;&#20419;&#36827;&#21327;&#20316;&#65292;&#24182;&#36880;&#28176;&#20943;&#23569;&#26174;&#24335;&#20256;&#36798;&#20449;&#24687;&#30340;&#27604;&#20363;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#33539;&#24335;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#26223;&#19979;&#27604;&#20256;&#32479;&#30340; CTDE &#33539;&#24335;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2304.14656</link><description>&lt;p&gt;
&#20174;&#26174;&#24615;&#27807;&#36890;&#21040;&#38544;&#24615;&#21512;&#20316;&#65306;&#19968;&#31181;&#26032;&#30340;&#21327;&#20316;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
From Explicit Communication to Tacit Cooperation:A Novel Paradigm for Cooperative MARL. (arXiv:2304.14656v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14656
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#26174;&#24615;&#27807;&#36890;&#21040;&#38544;&#24615;&#21512;&#20316;&#30340;&#26032;&#21327;&#20316;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#33539;&#24335;&#65292;&#36890;&#36807;&#22312;&#20195;&#29702;&#20043;&#38388;&#20998;&#20139;&#20449;&#24687;&#21644;&#20351;&#29992;&#26412;&#22320;&#36712;&#36857;&#37325;&#24314;&#20449;&#24687;&#26469;&#20419;&#36827;&#21327;&#20316;&#65292;&#24182;&#36880;&#28176;&#20943;&#23569;&#26174;&#24335;&#20256;&#36798;&#20449;&#24687;&#30340;&#27604;&#20363;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#33539;&#24335;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#26223;&#19979;&#27604;&#20256;&#32479;&#30340; CTDE &#33539;&#24335;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38598;&#20013;&#22521;&#35757;&#19982;&#20998;&#25955;&#25191;&#34892;&#65288;CTDE&#65289;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#22312;&#22797;&#26434;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#30528;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#23616;&#37096;&#21487;&#35266;&#23519;&#24615;&#38382;&#39064;&#21644;&#20195;&#29702;&#20043;&#38388;&#32570;&#20047;&#26377;&#25928;&#20849;&#20139;&#20449;&#21495;&#30340;&#23384;&#22312;&#32463;&#24120;&#38480;&#21046;&#20102;&#23427;&#22312;&#20419;&#36827;&#21327;&#20316;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#34429;&#28982;&#36890;&#20449;&#21487;&#20197;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#20294;&#21516;&#26102;&#20063;&#38477;&#20302;&#20102;&#31639;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;&#21463;&#20154;&#31867;&#22242;&#38431;&#21512;&#20316;&#23398;&#20064;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#20197;&#20419;&#36827;&#20174;&#26174;&#24615;&#36890;&#20449;&#21040;&#38544;&#24615;&#21512;&#20316;&#30340;&#36880;&#28176;&#36716;&#21464;&#12290;&#22312;&#21021;&#22987;&#35757;&#32451;&#38454;&#27573;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#20195;&#29702;&#20043;&#38388;&#20998;&#20139;&#30456;&#20851;&#20449;&#24687;&#21644;&#21516;&#26102;&#20351;&#29992;&#27599;&#20010;&#20195;&#29702;&#30340;&#26412;&#22320;&#36712;&#36857;&#37325;&#24314;&#35813;&#20449;&#24687;&#26469;&#20419;&#36827;&#21327;&#20316;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#26174;&#24335;&#20256;&#36798;&#30340;&#20449;&#24687;&#19982;&#37325;&#24314;&#30340;&#20449;&#24687;&#30456;&#32467;&#21512;&#65292;&#24471;&#21040;&#28151;&#21512;&#20449;&#24687;&#12290;&#22312;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23398;&#20064;&#21040;&#30340;&#38544;&#21547;&#21512;&#20316;&#36880;&#28176;&#20943;&#23569;&#26174;&#24335;&#20256;&#36798;&#20449;&#24687;&#30340;&#27604;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Centralized training with decentralized execution (CTDE) is a widely-used learning paradigm that has achieved significant success in complex tasks. However, partial observability issues and the absence of effectively shared signals between agents often limit its effectiveness in fostering cooperation. While communication can address this challenge, it simultaneously reduces the algorithm's practicality. Drawing inspiration from human team cooperative learning, we propose a novel paradigm that facilitates a gradual shift from explicit communication to tacit cooperation. In the initial training stage, we promote cooperation by sharing relevant information among agents and concurrently reconstructing this information using each agent's local trajectory. We then combine the explicitly communicated information with the reconstructed information to obtain mixed information. Throughout the training process, we progressively reduce the proportion of explicitly communicated information, facilit
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;GraphSANN&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#21516;&#26500;&#21644;&#24322;&#26500;&#22270;&#30340;&#19981;&#24179;&#34913;&#33410;&#28857;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2304.14635</link><description>&lt;p&gt;
&#36229;&#36234;&#21516;&#26500;&#20551;&#35774;&#30340;&#19981;&#24179;&#34913;&#33410;&#28857;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Imbalanced Node Classification Beyond Homophilic Assumption. (arXiv:2304.14635v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14635
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;GraphSANN&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#21516;&#26500;&#21644;&#24322;&#26500;&#22270;&#30340;&#19981;&#24179;&#34913;&#33410;&#28857;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#24179;&#34913;&#33410;&#28857;&#20998;&#31867;&#24191;&#27867;&#23384;&#22312;&#20110;&#29616;&#23454;&#19990;&#30028;&#32593;&#32476;&#20013;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#20542;&#21521;&#20110;&#22810;&#25968;&#31867;&#24182;&#19988;&#22312;&#20998;&#31867;&#23569;&#25968;&#31867;&#33410;&#28857;&#26102;&#24615;&#33021;&#20005;&#37325;&#19979;&#38477;&#12290;&#26368;&#36817;&#25552;&#20986;&#20102;&#21508;&#31181;&#19981;&#24179;&#34913;&#33410;&#28857;&#20998;&#31867;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#26500;&#24314;&#21512;&#25104;&#33410;&#28857;&#21644;&#36793;&#20197;&#24179;&#34913;&#26631;&#31614;&#21644;&#25299;&#25169;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#37117;&#22522;&#20110;&#21516;&#26500;&#20551;&#35774;&#65292;&#21363;&#30456;&#21516;&#26631;&#31614;&#30340;&#33410;&#28857;&#20542;&#21521;&#20110;&#36830;&#25509;&#65292;&#23613;&#31649;&#22312;&#29616;&#23454;&#19990;&#30028;&#22270;&#20013;&#24191;&#27867;&#23384;&#22312;&#24322;&#26500;&#36793;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#32479;&#19968;&#32858;&#21512;&#26469;&#33258;&#21516;&#26500;&#21644;&#24322;&#26500;&#37051;&#23621;&#30340;&#29305;&#24449;&#65292;&#24182;&#20381;&#36182;&#20110;&#29305;&#24449;&#30456;&#20284;&#24615;&#29983;&#25104;&#21512;&#25104;&#36793;&#65292;&#26080;&#27861;&#24212;&#29992;&#20110;&#39640;&#24322;&#36136;&#24615;&#30340;&#19981;&#24179;&#34913;&#22270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;GraphSANN&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#21516;&#26500;&#21644;&#24322;&#26500;&#22270;&#30340;&#19981;&#24179;&#34913;&#33410;&#28857;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imbalanced node classification widely exists in real-world networks where graph neural networks (GNNs) are usually highly inclined to majority classes and suffer from severe performance degradation on classifying minority class nodes. Various imbalanced node classification methods have been proposed recently which construct synthetic nodes and edges w.r.t. minority classes to balance the label and topology distribution. However, they are all based on the homophilic assumption that nodes of the same label tend to connect despite the wide existence of heterophilic edges in real-world graphs. Thus, they uniformly aggregate features from both homophilic and heterophilic neighbors and rely on feature similarity to generate synthetic edges, which cannot be applied to imbalanced graphs in high heterophily. To address this problem, we propose a novel GraphSANN for imbalanced node classification on both homophilic and heterophilic graphs. Firstly, we propose a unified feature mixer to generate 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#22242;&#38431;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#20215;&#20307;&#30340;3D&#31070;&#32463;&#37325;&#24314;&#26694;&#26550;CVRecon&#65292;&#21033;&#29992;&#20016;&#23500;&#30340;&#20960;&#20309;&#23884;&#20837;&#26469;&#20419;&#36827;3D&#20960;&#20309;&#29305;&#24449;&#23398;&#20064;&#12290;&#36890;&#36807;&#24341;&#20837;&#23556;&#32447;&#19978;&#19979;&#25991;&#34917;&#20607;&#20195;&#20215;&#20307;&#65288;RCCV&#65289;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#35270;&#35282;&#30456;&#20851;&#20449;&#24687;&#30340;&#23436;&#25972;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#21508;&#31181;&#24230;&#37327;&#26041;&#38754;&#26174;&#30528;&#25552;&#39640;&#20102;&#37325;&#24314;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2304.14633</link><description>&lt;p&gt;
CVRecon: &#37325;&#26032;&#24605;&#32771;&#31070;&#32463;&#37325;&#24314;&#30340;3D&#20960;&#20309;&#29305;&#24449;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
CVRecon: Rethinking 3D Geometric Feature Learning For Neural Reconstruction. (arXiv:2304.14633v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14633
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#22242;&#38431;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#20215;&#20307;&#30340;3D&#31070;&#32463;&#37325;&#24314;&#26694;&#26550;CVRecon&#65292;&#21033;&#29992;&#20016;&#23500;&#30340;&#20960;&#20309;&#23884;&#20837;&#26469;&#20419;&#36827;3D&#20960;&#20309;&#29305;&#24449;&#23398;&#20064;&#12290;&#36890;&#36807;&#24341;&#20837;&#23556;&#32447;&#19978;&#19979;&#25991;&#34917;&#20607;&#20195;&#20215;&#20307;&#65288;RCCV&#65289;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#35270;&#35282;&#30456;&#20851;&#20449;&#24687;&#30340;&#23436;&#25972;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#21508;&#31181;&#24230;&#37327;&#26041;&#38754;&#26174;&#30528;&#25552;&#39640;&#20102;&#37325;&#24314;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20351;&#29992;&#22270;&#20687;&#24207;&#21015;&#36827;&#34892;&#31070;&#32463;&#37325;&#24314;&#30340;&#36827;&#23637;&#21462;&#24471;&#20102;&#26174;&#30528;&#36827;&#23637;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#32570;&#20047;&#28145;&#24230;&#20449;&#24687;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#20307;&#31215;&#30340;&#25216;&#26415;&#20165;&#27839;&#25972;&#20010;&#30456;&#26426;&#20809;&#32447;&#22797;&#21046;&#23545;&#35937;&#34920;&#38754;&#30340;2D&#22270;&#20687;&#29305;&#24449;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#31181;&#22797;&#21046;&#20250;&#22312;&#31354;&#27934;&#21644;&#36974;&#25377;&#31354;&#38388;&#20013;&#24341;&#20837;&#22122;&#22768;&#65292;&#20174;&#32780;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;3D&#20960;&#20309;&#20307;&#25104;&#24418;&#26041;&#38754;&#20135;&#29983;&#25361;&#25112;&#12290;&#21463;&#20256;&#32479;&#22810;&#35270;&#35282;&#31435;&#20307;&#26041;&#27861;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;3D&#31070;&#32463;&#37325;&#24314;&#26694;&#26550;CVRecon&#65292;&#26088;&#22312;&#21033;&#29992;&#20195;&#20215;&#20307;&#20013;&#20016;&#23500;&#30340;&#20960;&#20309;&#23884;&#20837;&#26469;&#20419;&#36827;3D&#20960;&#20309;&#29305;&#24449;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;3D&#20960;&#20309;&#29305;&#24449;&#34920;&#31034;&#27861;&#8212;&#8212;&#23556;&#32447;&#19978;&#19979;&#25991;&#34917;&#20607;&#20195;&#20215;&#20307;&#65288;RCCV&#65289;&#65292;&#23427;&#20855;&#26377;&#26356;&#22909;&#30340;&#23436;&#25972;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#21487;&#20197;&#32534;&#30721;&#35270;&#35282;&#30456;&#20851;&#20449;&#24687;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#24230;&#37327;&#26041;&#38754;&#26174;&#30528;&#25552;&#39640;&#20102;&#37325;&#24314;&#36136;&#37327;&#65292;&#24182;&#24674;&#22797;&#20102;&#28165;&#26224;&#30340;
&lt;/p&gt;
&lt;p&gt;
Recent advances in neural reconstruction using posed image sequences have made remarkable progress. However, due to the lack of depth information, existing volumetric-based techniques simply duplicate 2D image features of the object surface along the entire camera ray. We contend this duplication introduces noise in empty and occluded spaces, posing challenges for producing high-quality 3D geometry. Drawing inspiration from traditional multi-view stereo methods, we propose an end-to-end 3D neural reconstruction framework CVRecon, designed to exploit the rich geometric embedding in the cost volumes to facilitate 3D geometric feature learning. Furthermore, we present Ray-contextual Compensated Cost Volume (RCCV), a novel 3D geometric feature representation that encodes view-dependent information with improved integrity and robustness. Through comprehensive experiments, we demonstrate that our approach significantly improves the reconstruction quality in various metrics and recovers clear
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31995;&#32479;ChartSpark&#65292;&#21033;&#29992;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#23558;&#35821;&#20041;&#19978;&#19979;&#25991;&#23884;&#20837;&#21040;&#22270;&#34920;&#20013;&#65292;&#20197;&#29983;&#25104;&#20855;&#26377;&#39640;&#36136;&#37327;&#35821;&#20041;&#19978;&#19979;&#25991;&#30340;&#22270;&#31034;&#21487;&#35270;&#21270;&#12290;</title><link>http://arxiv.org/abs/2304.14630</link><description>&lt;p&gt;
&#35753;&#22270;&#34920;&#38378;&#20142;&#36215;&#26469;&#65306;&#21033;&#29992;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#23558;&#35821;&#20041;&#19978;&#19979;&#25991;&#23884;&#20837;&#21040;&#22270;&#34920;&#20013;
&lt;/p&gt;
&lt;p&gt;
Let the Chart Spark: Embedding Semantic Context into Chart with Text-to-Image Generative Model. (arXiv:2304.14630v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14630
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31995;&#32479;ChartSpark&#65292;&#21033;&#29992;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#23558;&#35821;&#20041;&#19978;&#19979;&#25991;&#23884;&#20837;&#21040;&#22270;&#34920;&#20013;&#65292;&#20197;&#29983;&#25104;&#20855;&#26377;&#39640;&#36136;&#37327;&#35821;&#20041;&#19978;&#19979;&#25991;&#30340;&#22270;&#31034;&#21487;&#35270;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31034;&#21487;&#35270;&#21270;&#23558;&#25968;&#25454;&#21644;&#35821;&#20041;&#19978;&#19979;&#25991;&#33391;&#22909;&#22320;&#25972;&#21512;&#21040;&#35270;&#35273;&#34920;&#29616;&#20013;&#65292;&#20197;&#19968;&#31181;&#26082;&#24341;&#20154;&#21448;&#20449;&#24687;&#37327;&#22823;&#30340;&#26041;&#24335;&#20256;&#36798;&#22797;&#26434;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#31995;&#32479;ChartSpark&#65292;&#23427;&#22522;&#20110;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#23558;&#35821;&#20041;&#19978;&#19979;&#25991;&#23884;&#20837;&#22312;&#22270;&#34920;&#20013;&#29983;&#25104;&#22270;&#31034;&#21270;&#30340;&#21487;&#35270;&#21270;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#21069;&#26223;&#21644;&#32972;&#26223;&#30340;&#22270;&#31034;&#29983;&#25104;&#65292;&#26088;&#22312;&#21019;&#36896;&#20855;&#26377;&#35821;&#20041;&#19978;&#19979;&#25991;&#30340;&#39640;&#36136;&#37327;&#22270;&#31034;&#21487;&#35270;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pictorial visualization seamlessly integrates data and semantic context into visual representation, conveying complex information in a manner that is both engaging and informative. Extensive studies have been devoted to developing authoring tools to simplify the creation of pictorial visualizations. However, mainstream works mostly follow a retrieving-and-editing pipeline that heavily relies on retrieved visual elements from a dedicated corpus, which often compromise the data integrity. Text-guided generation methods are emerging, but may have limited applicability due to its predefined recognized entities. In this work, we propose ChartSpark, a novel system that embeds semantic context into chart based on text-to-image generative model. ChartSpark generates pictorial visualizations conditioned on both semantic context conveyed in textual inputs and data information embedded in plain charts. The method is generic for both foreground and background pictorial generation, satisfying the d
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#20171;&#32461;&#20102;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26102;&#25152;&#38754;&#20020;&#30340;&#30693;&#35782;&#20135;&#26435;&#20445;&#25252;&#38382;&#39064;&#65292;&#20197;&#21450;&#36817;&#24180;&#26469;&#38450;&#27490;&#21644;&#21457;&#29616;&#27169;&#22411;&#31363;&#21462;&#21644;&#26410;&#32463;&#25480;&#26435;&#37325;&#26032;&#20998;&#21457;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.14613</link><description>&lt;p&gt;
&#28145;&#24230;&#30693;&#35782;&#20135;&#26435;: &#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Deep Intellectual Property: A Survey. (arXiv:2304.14613v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14613
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#20171;&#32461;&#20102;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26102;&#25152;&#38754;&#20020;&#30340;&#30693;&#35782;&#20135;&#26435;&#20445;&#25252;&#38382;&#39064;&#65292;&#20197;&#21450;&#36817;&#24180;&#26469;&#38450;&#27490;&#21644;&#21457;&#29616;&#27169;&#22411;&#31363;&#21462;&#21644;&#26410;&#32463;&#25480;&#26435;&#37325;&#26032;&#20998;&#21457;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#24037;&#19994;&#21046;&#36896;&#21644;&#21830;&#19994;&#26381;&#21153;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#32463;&#36807;&#20805;&#20998;&#35757;&#32451;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#30001;&#20110;&#24222;&#22823;&#30340;&#35757;&#32451;&#25104;&#26412;&#21644;&#20248;&#31168;&#30340;&#27867;&#21270;&#24615;&#33021;&#21464;&#24471;&#36234;&#26469;&#36234;&#26377;&#20215;&#20540;&#21644;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#20123;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#21487;&#20197;&#34987;&#29992;&#25143;&#21033;&#29992;&#65292;&#32780;&#26080;&#38656;&#20102;&#35299;&#22826;&#22810;&#19987;&#19994;&#30693;&#35782;&#65292;&#36825;&#24471;&#30410;&#20110;&#26032;&#20852;&#30340;&#8220;&#26426;&#22120;&#23398;&#20064;&#21363;&#26381;&#21153;&#8221;(MLaaS)&#33539;&#24335;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#33539;&#24335;&#20063;&#20351;&#24471;&#26114;&#36149;&#30340;&#27169;&#22411;&#38754;&#20020;&#35768;&#22810;&#28508;&#22312;&#23041;&#32961;&#65292;&#20363;&#22914;&#27169;&#22411;&#31363;&#21462;&#21644;&#28389;&#29992;&#12290;&#20026;&#20102;&#25269;&#24481;&#36825;&#20123;&#23041;&#32961;&#30340;&#36843;&#20999;&#38656;&#27714;&#65292;&#28145;&#24230;&#30693;&#35782;&#20135;&#26435;&#65288;Deep Intellectual Property&#65292;DeepIP&#65289;&#25104;&#20026;&#20102;&#19994;&#30028;&#21644;&#23398;&#26415;&#30028;&#30340;&#20849;&#35782;&#65292;&#20197;&#20445;&#25252;&#31169;&#26377;&#35757;&#32451;&#25968;&#25454;&#12289;&#36153;&#23613;&#24515;&#24605;&#35843;&#25972;&#30340;&#36229;&#21442;&#25968;&#25110;&#26114;&#36149;&#23398;&#20064;&#30340;&#27169;&#22411;&#26435;&#37325;&#12290;&#20026;&#27492;&#65292;&#36817;&#24180;&#26469;&#25552;&#20986;&#20102;&#35768;&#22810;&#26041;&#27861;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#29305;&#21035;&#26159;&#38450;&#27490;&#25110;&#21457;&#29616;&#27169;&#22411;&#31363;&#21462;&#21644;&#26410;&#32463;&#25480;&#26435;&#30340;&#37325;&#26032;&#20998;&#21457;&#12290;&#37492;&#20110;&#36825;&#19968;&#24555;&#36895;&#28436;&#21464;&#30340;&#26102;&#26399;&#65292;
&lt;/p&gt;
&lt;p&gt;
With the widespread application in industrial manufacturing and commercial services, well-trained deep neural networks (DNNs) are becoming increasingly valuable and crucial assets due to the tremendous training cost and excellent generalization performance. These trained models can be utilized by users without much expert knowledge benefiting from the emerging ''Machine Learning as a Service'' (MLaaS) paradigm. However, this paradigm also exposes the expensive models to various potential threats like model stealing and abuse. As an urgent requirement to defend against these threats, Deep Intellectual Property (DeepIP), to protect private training data, painstakingly-tuned hyperparameters, or costly learned model weights, has been the consensus of both industry and academia. To this end, numerous approaches have been proposed to achieve this goal in recent years, especially to prevent or discover model stealing and unauthorized redistribution. Given this period of rapid evolution, the g
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Temporal Adversarial Augmentation&#65288;TA&#65289;&#65292;&#19968;&#31181;&#21033;&#29992;&#26102;&#38388;&#27880;&#24847;&#21147;&#30340;&#35270;&#39057;&#22686;&#24378;&#25216;&#26415;&#65292;&#21487;&#20197;&#36890;&#36807;&#26368;&#22823;&#21270;&#26102;&#38388;&#30456;&#20851;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#25913;&#21464;&#31070;&#32463;&#32593;&#32476;&#23545;&#35270;&#39057;&#29255;&#27573;&#30340;&#27880;&#24847;&#20998;&#24067;&#12290;&#21033;&#29992;TA&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Temporal Video Adversarial Fine-tuning&#65288;TAF&#65289;&#26694;&#26550;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25913;&#21892;&#35270;&#39057;&#34920;&#31034;&#24182;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.14601</link><description>&lt;p&gt;
&#29992;&#26102;&#24207;&#23545;&#25239;&#22686;&#24378;&#25216;&#26415;&#25913;&#36827;&#35270;&#39057;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Improve Video Representation with Temporal Adversarial Augmentation. (arXiv:2304.14601v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14601
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Temporal Adversarial Augmentation&#65288;TA&#65289;&#65292;&#19968;&#31181;&#21033;&#29992;&#26102;&#38388;&#27880;&#24847;&#21147;&#30340;&#35270;&#39057;&#22686;&#24378;&#25216;&#26415;&#65292;&#21487;&#20197;&#36890;&#36807;&#26368;&#22823;&#21270;&#26102;&#38388;&#30456;&#20851;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#25913;&#21464;&#31070;&#32463;&#32593;&#32476;&#23545;&#35270;&#39057;&#29255;&#27573;&#30340;&#27880;&#24847;&#20998;&#24067;&#12290;&#21033;&#29992;TA&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Temporal Video Adversarial Fine-tuning&#65288;TAF&#65289;&#26694;&#26550;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25913;&#21892;&#35270;&#39057;&#34920;&#31034;&#24182;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22914;&#26524;&#20197;&#36866;&#24403;&#30340;&#26041;&#24335;&#20351;&#29992;&#65292;&#23545;&#25239;&#22686;&#24378;&#26377;&#21161;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26102;&#38388;&#27880;&#24847;&#21147;&#30340;&#26032;&#22411;&#35270;&#39057;&#22686;&#24378;&#25216;&#26415;&#8212;&#8212;Temporal Adversarial Augmentation (TA)&#12290;&#19982;&#20256;&#32479;&#30340;&#23545;&#25239;&#22686;&#24378;&#19981;&#21516;&#65292;TA&#19987;&#20026;&#36890;&#36807;&#26368;&#22823;&#21270;&#26102;&#38388;&#30456;&#20851;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#25913;&#21464;&#31070;&#32463;&#32593;&#32476;&#23545;&#35270;&#39057;&#29255;&#27573;&#30340;&#27880;&#24847;&#20998;&#24067;&#32780;&#35774;&#35745;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;TA&#23558;&#33719;&#24471;&#22810;&#26679;&#21270;&#30340;&#26102;&#38388;&#35270;&#35282;&#65292;&#36825;&#26174;&#33879;&#24433;&#21709;&#31070;&#32463;&#32593;&#32476;&#30340;&#28966;&#28857;&#12290;&#20351;&#29992;&#36825;&#20123;&#31034;&#20363;&#36827;&#34892;&#35757;&#32451;&#20462;&#22797;&#20102;&#19981;&#24179;&#34913;&#30340;&#26102;&#38388;&#20449;&#24687;&#24863;&#30693;&#32570;&#38519;&#65292;&#24182;&#22686;&#24378;&#20102;&#25269;&#24481;&#26102;&#38388;&#20559;&#31227;&#30340;&#33021;&#21147;&#65292;&#26368;&#32456;&#23548;&#33268;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#20026;&#20102;&#21033;&#29992;TA&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Temporal Video Adversarial Fine-tuning (TAF)&#26694;&#26550;&#26469;&#25913;&#36827;&#35270;&#39057;&#34920;&#31034;&#12290;TAF&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#27169;&#22411;&#26080;&#20851;&#12289;&#21487;&#35299;&#37322;&#24615;&#21451;&#22909;&#30340;&#35757;&#32451;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works reveal that adversarial augmentation benefits the generalization of neural networks (NNs) if used in an appropriate manner. In this paper, we introduce Temporal Adversarial Augmentation (TA), a novel video augmentation technique that utilizes temporal attention. Unlike conventional adversarial augmentation, TA is specifically designed to shift the attention distributions of neural networks with respect to video clips by maximizing a temporal-related loss function. We demonstrate that TA will obtain diverse temporal views, which significantly affect the focus of neural networks. Training with these examples remedies the flaw of unbalanced temporal information perception and enhances the ability to defend against temporal shifts, ultimately leading to better generalization. To leverage TA, we propose Temporal Video Adversarial Fine-tuning (TAF) framework for improving video representations. TAF is a model-agnostic, generic, and interpretability-friendly training strategy. We
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#19981;&#30830;&#23450;&#24615;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30340;&#26426;&#22120;&#20154;&#25163;&#26415;&#25216;&#33021;&#35780;&#20272;&#25216;&#26415;&#65292;&#36890;&#36807;&#23558;&#26631;&#35760;&#25968;&#25454;&#39046;&#22495;&#30693;&#35782;&#36716;&#31227;&#21040;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#36827;&#34892;&#35757;&#32451;&#65292;&#26368;&#32456;&#22312;&#34394;&#25311;&#29616;&#23454;&#27169;&#25311;&#30340;&#35757;&#32451;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.14589</link><description>&lt;p&gt;
&#26426;&#22120;&#20154;&#36741;&#21161;&#25163;&#26415;&#20132;&#21449;&#39046;&#22495;&#25216;&#33021;&#35780;&#20272;&#30340;&#19981;&#30830;&#23450;&#24615;&#33258;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-aware Self-supervised Learning for Cross-domain Technical Skill Assessment in Robot-assisted Surgery. (arXiv:2304.14589v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14589
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#19981;&#30830;&#23450;&#24615;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30340;&#26426;&#22120;&#20154;&#25163;&#26415;&#25216;&#33021;&#35780;&#20272;&#25216;&#26415;&#65292;&#36890;&#36807;&#23558;&#26631;&#35760;&#25968;&#25454;&#39046;&#22495;&#30693;&#35782;&#36716;&#31227;&#21040;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#36827;&#34892;&#35757;&#32451;&#65292;&#26368;&#32456;&#22312;&#34394;&#25311;&#29616;&#23454;&#27169;&#25311;&#30340;&#35757;&#32451;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#25216;&#33021;&#35780;&#20272;&#23545;&#26032;&#21307;&#29983;&#36827;&#34892;&#26426;&#22120;&#20154;&#36741;&#21161;&#25163;&#26415;&#30340;&#26377;&#25928;&#22521;&#35757;&#33267;&#20851;&#37325;&#35201;&#12290;&#38543;&#30528;&#29289;&#29702;&#21644;&#34394;&#25311;&#29615;&#22659;&#19979;&#30340;&#25163;&#26415;&#22521;&#35757;&#35745;&#21010;&#30340;&#36827;&#23637;&#65292;&#24320;&#21457;&#33258;&#21160;&#35780;&#20272;&#25216;&#33021;&#30340;&#36890;&#29992;&#26041;&#27861;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#26631;&#35760;&#36816;&#21160;&#25968;&#25454;&#30340;&#39046;&#22495;&#30693;&#35782;&#36716;&#31227;&#33267;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#36827;&#34892;&#25216;&#33021;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#26469;&#33258;&#24120;&#35265;&#25163;&#26415;&#35757;&#32451;&#20219;&#21153;&#65288;&#22914;&#32541;&#21512;&#12289;&#31359;&#38024;&#21644;&#25171;&#32467;&#65289;&#30340;&#26631;&#35760;&#25968;&#25454;&#26469;&#32852;&#21512;&#35757;&#32451;&#20855;&#26377;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#27169;&#22411;&#12290;&#36890;&#36807;&#36845;&#20195;&#26041;&#24335;&#29983;&#25104;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#20266;&#26631;&#31614;&#65292;&#20854;&#20013;&#21253;&#21547;&#19981;&#30830;&#23450;&#24230;&#20272;&#35745;&#20197;&#30830;&#20445;&#20934;&#30830;&#26631;&#35760;&#12290;&#25105;&#20204;&#20351;&#29992;&#26469;&#33258;da Vinci Research Kit&#65288;dVRK&#65289;&#30340;&#25968;&#25454;&#22312;&#34394;&#25311;&#29616;&#23454;&#27169;&#25311;&#30340;&#35757;&#32451;&#20219;&#21153;&#65288;Ring Transfer&#65289;&#19978;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#26426;&#22120;&#20154;&#36741;&#21161;&#19979;&#36827;&#34892;&#22521;&#35757;&#30340;&#23454;&#20064;&#29983;&#20855;&#26377;&#26174;&#33879;&#26356;&#39640;&#30340;&#19987;&#23478;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Objective technical skill assessment is crucial for effective training of new surgeons in robot-assisted surgery. With advancements in surgical training programs in both physical and virtual environments, it is imperative to develop generalizable methods for automatically assessing skills. In this paper, we propose a novel approach for skill assessment by transferring domain knowledge from labeled kinematic data to unlabeled data. Our approach leverages labeled data from common surgical training tasks such as Suturing, Needle Passing, and Knot Tying to jointly train a model with both labeled and unlabeled data. Pseudo labels are generated for the unlabeled data through an iterative manner that incorporates uncertainty estimation to ensure accurate labeling. We evaluate our method on a virtual reality simulated training task (Ring Transfer) using data from the da Vinci Research Kit (dVRK). The results show that trainees with robotic assistance have significantly higher expert probabilit
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;ANS&#20316;&#20026;&#19968;&#31181;&#25968;&#23383;&#27835;&#30103;&#31574;&#30053;&#65292;&#27835;&#30103;&#38590;&#27835;&#24615;&#30315;&#30187;&#30340;&#28508;&#21147;&#65292;ANS&#20351;&#29992;&#38381;&#29615;&#31995;&#32479;&#65292;&#21487;&#20197;&#38477;&#20302;&#30315;&#30187;&#30340;&#21457;&#20316;&#39057;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.14583</link><description>&lt;p&gt;
Epilexie&#65306;&#19968;&#31181;&#36879;&#36807;&#39034;&#24212;&#24615;&#31070;&#32463;&#21050;&#28608;&#25968;&#23383;&#27835;&#30103;&#26041;&#27861;&#27835;&#30103;&#38590;&#27835;&#24615;&#30315;&#30187;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
\'Epilexie: A digital therapeutic approach for treating intractable epilepsy via Amenable Neurostimulation. (arXiv:2304.14583v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14583
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;ANS&#20316;&#20026;&#19968;&#31181;&#25968;&#23383;&#27835;&#30103;&#31574;&#30053;&#65292;&#27835;&#30103;&#38590;&#27835;&#24615;&#30315;&#30187;&#30340;&#28508;&#21147;&#65292;ANS&#20351;&#29992;&#38381;&#29615;&#31995;&#32479;&#65292;&#21487;&#20197;&#38477;&#20302;&#30315;&#30187;&#30340;&#21457;&#20316;&#39057;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30315;&#30187;&#26159;&#19968;&#31181;&#31070;&#32463;&#30142;&#30149;&#65292;&#34920;&#29616;&#20026;&#25345;&#32493;&#30340;&#25277;&#25616;&#65292;&#26377;&#26102;&#20063;&#31216;&#20026;&#24778;&#21413;&#12290;&#23613;&#31649;&#26377;&#33647;&#29289;&#21644;&#25163;&#26415;&#31561;&#26377;&#25928;&#27835;&#30103;&#26041;&#27861;&#65292;&#20294;&#20173;&#26377;&#19968;&#20123;&#24739;&#32773;&#23384;&#22312;&#38590;&#27835;&#24615;&#30315;&#30187;&#65292;&#26080;&#27861;&#23545;&#26631;&#20934;&#26041;&#27861;&#20570;&#20986;&#21453;&#24212;&#12290;&#38590;&#27835;&#24615;&#30315;&#30187;&#26159;&#19968;&#31181;&#20005;&#37325;&#30340;&#31070;&#32463;&#30142;&#30149;&#65292;&#24433;&#21709;&#30528;&#20840;&#29699;&#25968;&#30334;&#19975;&#20154;&#12290;&#36817;&#24180;&#26469;&#65292;&#22823;&#33041;&#21487;&#32534;&#31243;&#30005;&#21050;&#28608;&#25216;&#26415;&#30340;&#24212;&#29992;&#24050;&#26174;&#31034;&#20986;&#38477;&#20302;&#38590;&#27835;&#24615;&#30315;&#30187;&#30340;&#21457;&#20316;&#39057;&#29575;&#30340;&#25968;&#23383;&#27835;&#30103;&#31574;&#30053;&#30340;&#28508;&#21147;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#23558;&#39034;&#24212;&#24615;&#31070;&#32463;&#21050;&#28608;&#65288;ANS&#65289;&#20316;&#20026;&#25968;&#23383;&#27835;&#30103;&#31574;&#30053;&#30340;&#19968;&#37096;&#20998;&#29992;&#20110;&#38590;&#27835;&#24615;&#30315;&#30187;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Epilepsy is a neurological illness that is characterised by continuous spasms of shaking, sometimes known as convulsions. Although there are effective treatments for epilepsy, such as drugs and surgery, there is still a group of individuals who have intractable epilepsy that fails to respond to standard methods. Intractable epilepsy is a severe neurological illness that ripples across the globe and impacts millions of individuals. It is extremely difficult to control intractable epilepsy, which is defined as the lack of response to two or more standard antiepileptic medication treatments. In recent years, the use of programmable electrical stimulation of the brain has shown promise as a digital treatment strategy for lowering seizure frequency in individuals with intractable epilepsy. In this research, the use of Amenable Neurostimulation (ANS) as part of a digital treatment strategy to intractable epilepsy is investigated. When applied to the brain, ANS uses a closed-loop system to se
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#29992;&#25143;&#30740;&#31350;&#21457;&#29616;&#65292;&#34429;&#28982;&#26032;&#25163;&#29992;&#25143;&#21019;&#24314;Deepfakes&#21487;&#33021;&#19981;&#23481;&#26131;&#65292;&#20294;&#20063;&#19981;&#26159;&#23436;&#20840;&#22256;&#38590;&#65292;&#24182;&#19988;&#38656;&#35201;&#24341;&#36215;&#30740;&#31350;&#30028;&#21644;&#25919;&#31574;&#21046;&#23450;&#32773;&#30340;&#20851;&#27880;&#12290;</title><link>http://arxiv.org/abs/2304.14576</link><description>&lt;p&gt;
&#26032;&#25163;&#29992;&#25143;&#26159;&#21542;&#33021;&#21019;&#24314;deepfakes&#65311;
&lt;/p&gt;
&lt;p&gt;
Can deepfakes be created by novice users?. (arXiv:2304.14576v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14576
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#29992;&#25143;&#30740;&#31350;&#21457;&#29616;&#65292;&#34429;&#28982;&#26032;&#25163;&#29992;&#25143;&#21019;&#24314;Deepfakes&#21487;&#33021;&#19981;&#23481;&#26131;&#65292;&#20294;&#20063;&#19981;&#26159;&#23436;&#20840;&#22256;&#38590;&#65292;&#24182;&#19988;&#38656;&#35201;&#24341;&#36215;&#30740;&#31350;&#30028;&#21644;&#25919;&#31574;&#21046;&#23450;&#32773;&#30340;&#20851;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#26368;&#26032;&#36827;&#23637;&#24050;&#32463;&#23548;&#33268;&#20102;Deepfakes&#30340;&#27867;&#28389;&#12290;&#38543;&#30528;&#25216;&#26415;&#30340;&#27665;&#20027;&#21270;&#65292;&#36234;&#26469;&#36234;&#25285;&#24515;&#26032;&#25163;&#29992;&#25143;&#21487;&#20197;&#21019;&#24314;Deepfakes&#65292;&#20197;&#25171;&#20987;&#20182;&#20154;&#24182;&#30772;&#22351;&#20844;&#20849;&#35805;&#35821;&#12290;&#26412;&#25991;&#36890;&#36807;&#29992;&#25143;&#30740;&#31350;&#26469;&#20102;&#35299;&#20855;&#26377;&#20808;&#36827;&#35745;&#31639;&#26426;&#25216;&#33021;&#21644;&#19981;&#21516;&#35745;&#31639;&#26426;&#31185;&#23398;&#19987;&#19994;&#27700;&#24179;&#30340;&#21442;&#19982;&#32773;&#26159;&#21542;&#21487;&#20197;&#20351;&#29992;&#26377;&#38480;&#30340;&#23186;&#20307;&#25991;&#20214;&#21019;&#24314;Deepfakes&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20004;&#39033;&#30740;&#31350;&#65292;&#22312;&#31532;&#19968;&#39033;&#30740;&#31350;&#20013;&#65288;n = 39&#65289;&#65292;&#21442;&#19982;&#32773;&#23581;&#35797;&#22312;&#32422;&#26463;&#26102;&#38388;&#20869;&#20351;&#29992;&#20219;&#20309;&#24037;&#20855;&#21019;&#24314;&#30446;&#26631;Deepfake&#12290;&#22312;&#31532;&#20108;&#39033;&#30740;&#31350;&#20013;&#65288;n = 29&#65289;&#65292;&#21442;&#19982;&#32773;&#20351;&#29992;&#39044;&#20808;&#25351;&#23450;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#24037;&#20855;&#26469;&#21019;&#24314;&#30456;&#21516;&#30340;Deepfake&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#31532;&#19968;&#39033;&#30740;&#31350;&#20013;&#65292;23.1%&#30340;&#21442;&#19982;&#32773;&#25104;&#21151;&#22320;&#21019;&#24314;&#20102;&#20855;&#26377;&#38899;&#39057;&#21644;&#35270;&#39057;&#30340;&#23436;&#25972;Deepfakes&#65292;&#32780;&#22312;&#31532;&#20108;&#39033;&#29992;&#25143;&#30740;&#31350;&#20013;&#65292;58.6%&#30340;&#21442;&#19982;&#32773;&#25104;&#21151;&#22320;&#23558;&#30446;&#26631;&#28436;&#35762;&#32541;&#21512;&#21040;&#28304;&#35270;&#39057;&#20013;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#34429;&#28982;&#23545;&#20110;&#26032;&#25163;&#29992;&#25143;&#32780;&#35328;&#21019;&#24314;Deepfakes&#21487;&#33021;&#24182;&#19981;&#23481;&#26131;&#65292;&#20294;&#20063;&#19981;&#26159;&#23436;&#20840;&#22256;&#38590;&#65292;&#22240;&#27492;&#38656;&#35201;&#30740;&#31350;&#30028;&#21644;&#25919;&#31574;&#21046;&#23450;&#32773;&#30340;&#20851;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in machine learning and computer vision have led to the proliferation of Deepfakes. As technology democratizes over time, there is an increasing fear that novice users can create Deepfakes, to discredit others and undermine public discourse. In this paper, we conduct user studies to understand whether participants with advanced computer skills and varying levels of computer science expertise can create Deepfakes of a person saying a target statement using limited media files. We conduct two studies; in the first study (n = 39) participants try creating a target Deepfake in a constrained time frame using any tool they desire. In the second study (n = 29) participants use pre-specified deep learning-based tools to create the same Deepfake. We find that for the first study, 23.1% of the participants successfully created complete Deepfakes with audio and video, whereas, for the second user study, 58.6% of the participants were successful in stitching target speech to th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;SceneGenie&#27169;&#22411;&#65292;&#21033;&#29992;&#36793;&#30028;&#26694;&#21644;&#20998;&#21106;&#22320;&#22270;&#20449;&#24687;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#65292;&#22788;&#29702;&#22797;&#26434;&#30340;&#25991;&#26412;&#25552;&#31034;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;CLIP&#23884;&#20837;&#30340;&#35821;&#20041;&#29305;&#24449;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#20445;&#30495;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.14573</link><description>&lt;p&gt;
SceneGenie: &#22330;&#26223;&#22270;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#22270;&#20687;&#21512;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
SceneGenie: Scene Graph Guided Diffusion Models for Image Synthesis. (arXiv:2304.14573v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14573
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;SceneGenie&#27169;&#22411;&#65292;&#21033;&#29992;&#36793;&#30028;&#26694;&#21644;&#20998;&#21106;&#22320;&#22270;&#20449;&#24687;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#65292;&#22788;&#29702;&#22797;&#26434;&#30340;&#25991;&#26412;&#25552;&#31034;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;CLIP&#23884;&#20837;&#30340;&#35821;&#20041;&#29305;&#24449;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#20445;&#30495;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#30340;&#25216;&#26415;&#22312;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#26368;&#26032;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#25903;&#25345;&#19979;&#21462;&#24471;&#20102;&#26174;&#30528;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#20197;&#25991;&#26412;&#25552;&#31034;&#20026;&#26465;&#20214;&#30340;&#25193;&#25955;&#27169;&#22411;&#22312;&#20135;&#29983;&#21360;&#35937;&#28145;&#21051;&#30340;&#39640;&#36136;&#37327;&#22270;&#20687;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#20934;&#30830;&#34920;&#31034;&#29305;&#23450;&#23545;&#35937;&#30340;&#23454;&#20363;&#25968;&#37327;&#31561;&#22797;&#26434;&#25991;&#26412;&#25552;&#31034;&#26041;&#38754;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25193;&#25955;&#27169;&#22411;&#37319;&#26679;&#36807;&#31243;&#25351;&#23548;&#26041;&#27861;&#65292;&#22312;&#25512;&#29702;&#26102;&#21033;&#29992;&#36793;&#30028;&#26694;&#21644;&#20998;&#21106;&#22320;&#22270;&#20449;&#24687;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#36890;&#36807;&#37319;&#26679;&#36807;&#31243;&#20013;&#30340;&#26032;&#25439;&#22833;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#26469;&#33258;CLIP&#23884;&#20837;&#30340;&#35821;&#20041;&#29305;&#24449;&#24341;&#23548;&#27169;&#22411;&#65292;&#24182;&#24378;&#21046;&#25191;&#34892;&#20960;&#20309;&#32422;&#26463;&#65292;&#20174;&#32780;&#29983;&#25104;&#20934;&#30830;&#34920;&#31034;&#22330;&#26223;&#30340;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#12290;&#20026;&#20102;&#33719;&#24471;&#36793;&#30028;&#26694;&#21644;&#20998;&#21106;&#22320;&#22270;&#20449;&#24687;&#65292;&#25105;&#20204;&#23558;&#25991;&#26412;&#25552;&#31034;&#32467;&#26500;&#21270;&#20026;&#19968;&#20010;&#22330;&#26223;&#22270;&#65292;&#24182;&#20351;&#29992;CLIP&#23884;&#20837;&#20016;&#23500;&#33410;&#28857;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#20174;&#22797;&#26434;&#25991;&#26412;&#25552;&#31034;&#29983;&#25104;&#22270;&#20687;&#26041;&#38754;&#23454;&#29616;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#20445;&#30495;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-conditioned image generation has made significant progress in recent years with generative adversarial networks and more recently, diffusion models. While diffusion models conditioned on text prompts have produced impressive and high-quality images, accurately representing complex text prompts such as the number of instances of a specific object remains challenging.  To address this limitation, we propose a novel guidance approach for the sampling process in the diffusion model that leverages bounding box and segmentation map information at inference time without additional training data. Through a novel loss in the sampling process, our approach guides the model with semantic features from CLIP embeddings and enforces geometric constraints, leading to high-resolution images that accurately represent the scene. To obtain bounding box and segmentation map information, we structure the text prompt as a scene graph and enrich the nodes with CLIP embeddings. Our proposed model achieve
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#24418;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#24378;&#21046;&#25191;&#34892;&#35299;&#21078;&#23398;&#24418;&#29366;&#30340;&#36830;&#32493;&#24615;&#21644;&#36830;&#36890;&#24615;&#65292;&#20197;&#25913;&#36827;&#21307;&#23398;&#35786;&#26029;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.14572</link><description>&lt;p&gt;
SCOPE&#65306;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#32467;&#26500;&#36830;&#32493;&#24615;&#20445;&#25345;
&lt;/p&gt;
&lt;p&gt;
SCOPE: Structural Continuity Preservation for Medical Image Segmentation. (arXiv:2304.14572v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14572
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#24418;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#24378;&#21046;&#25191;&#34892;&#35299;&#21078;&#23398;&#24418;&#29366;&#30340;&#36830;&#32493;&#24615;&#21644;&#36830;&#36890;&#24615;&#65292;&#20197;&#25913;&#36827;&#21307;&#23398;&#35786;&#26029;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#20445;&#25345;&#24418;&#29366;&#36830;&#32493;&#24615;&#21644;&#29983;&#29702;&#35299;&#21078;&#23398;&#30340;&#20551;&#35774;&#26159;&#33258;&#28982;&#30340;&#65292;&#20294;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#24573;&#35270;&#36825;&#19968;&#28857;&#65292;&#23427;&#20204;&#20027;&#35201;&#26088;&#22312;&#32479;&#35745;&#24314;&#27169;&#36755;&#20837;&#25968;&#25454;&#20316;&#20026;&#20687;&#32032;&#65292;&#32780;&#19981;&#26159;&#30456;&#20114;&#36830;&#25509;&#30340;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#22312;&#29983;&#29289;&#32467;&#26500;&#20013;&#65292;&#22120;&#23448;&#24182;&#19981;&#26159;&#29420;&#31435;&#30340;&#23454;&#20307;&#65307;&#20363;&#22914;&#65292;&#22312;&#29616;&#23454;&#20013;&#65292;&#26029;&#24320;&#30340;&#34880;&#31649;&#26159;&#28508;&#22312;&#38382;&#39064;&#30340;&#25351;&#26631;&#65292;&#20294;&#20256;&#32479;&#20998;&#21106;&#27169;&#22411;&#24182;&#27809;&#26377;&#34987;&#35774;&#35745;&#20026;&#20005;&#26684;&#25191;&#34892;&#35299;&#21078;&#36830;&#32493;&#24615;&#65292;&#21487;&#33021;&#23548;&#33268;&#19981;&#20934;&#30830;&#30340;&#21307;&#23398;&#35786;&#26029;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#24418;&#30340;&#26041;&#27861;&#65292;&#22312;&#21307;&#23398;&#22270;&#20687;&#20013;&#24378;&#21046;&#25191;&#34892;&#35299;&#21078;&#25299;&#25169;&#30340;&#36830;&#32493;&#24615;&#21644;&#36830;&#36890;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#24418;&#29366;&#30340;&#36830;&#32493;&#24615;&#32534;&#30721;&#20026;&#22270;&#24418;&#32422;&#26463;&#65292;&#30830;&#20445;&#32593;&#32476;&#30340;&#39044;&#27979;&#20445;&#25345;&#36825;&#31181;&#36830;&#32493;&#24615;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20844;&#20849;&#22522;&#20934;&#27979;&#35797;&#20013;&#23545;&#35270;&#32593;&#33180;&#34880;&#31649;&#20998;&#21106;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#26174;&#31034;&#20986;&#22312;&#36830;&#25509;&#24615;&#21644;&#36830;&#32493;&#24615;&#20445;&#25345;&#26041;&#38754;&#30340;&#26174;&#30528;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although the preservation of shape continuity and physiological anatomy is a natural assumption in the segmentation of medical images, it is often neglected by deep learning methods that mostly aim for the statistical modeling of input data as pixels rather than interconnected structures. In biological structures, however, organs are not separate entities; for example, in reality, a severed vessel is an indication of an underlying problem, but traditional segmentation models are not designed to strictly enforce the continuity of anatomy, potentially leading to inaccurate medical diagnoses. To address this issue, we propose a graph-based approach that enforces the continuity and connectivity of anatomical topology in medical images. Our method encodes the continuity of shapes as a graph constraint, ensuring that the network's predictions maintain this continuity. We evaluate our method on two public benchmarks on retinal vessel segmentation, showing significant improvements in connectiv
&lt;/p&gt;</description></item><item><title>&#22312;chatbot&#30340;&#20351;&#29992;&#20013;&#65292;&#24212;&#35813;&#20381;&#25454;&#36866;&#24403;&#24615;&#21407;&#21017;&#32780;&#38750;&#32431;&#31929;&#30340;&#23433;&#20840;&#24615;&#21407;&#21017;&#26469;&#36827;&#34892;&#35780;&#20272;&#65292;&#20197;&#36991;&#20813;&#20854;&#21463;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2304.14553</link><description>&lt;p&gt;
&#36866;&#24403;&#24615;&#26159;&#20320;&#25152;&#38656;&#35201;&#30340;&#19968;&#20999;&#65281;
&lt;/p&gt;
&lt;p&gt;
Appropriateness is all you need!. (arXiv:2304.14553v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14553
&lt;/p&gt;
&lt;p&gt;
&#22312;chatbot&#30340;&#20351;&#29992;&#20013;&#65292;&#24212;&#35813;&#20381;&#25454;&#36866;&#24403;&#24615;&#21407;&#21017;&#32780;&#38750;&#32431;&#31929;&#30340;&#23433;&#20840;&#24615;&#21407;&#21017;&#26469;&#36827;&#34892;&#35780;&#20272;&#65292;&#20197;&#36991;&#20813;&#20854;&#21463;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20445;&#38556;AI&#24212;&#29992;&#31243;&#24207;&#30340;&#8220;&#23433;&#20840;&#24615;&#8221;&#24050;&#25104;&#20026;&#23427;&#20204;&#20801;&#35768;&#20351;&#29992;&#30340;&#20027;&#35201;&#35268;&#33539;&#35201;&#27714;&#65292;&#29978;&#33267;&#26159;&#21807;&#19968;&#35268;&#33539;&#35201;&#27714;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#8220;&#23433;&#20840;&#24615;&#35268;&#33539;&#24615;&#8221;&#26041;&#27861;&#24050;&#32463;&#26174;&#31034;&#20986;&#22312;&#35299;&#20915;chatGPT&#21644;&#20854;&#20182;chatbot&#24341;&#21457;&#30340;&#38382;&#39064;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38480;&#21046;chatbot&#35805;&#39064;&#33539;&#22260;&#30340;&#8220;&#36866;&#24403;&#24615;&#35268;&#33539;&#24615;&#8221;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#35805;&#35821;&#30340;&#19977;&#31181;&#36866;&#24403;&#24615;&#65288;&#25216;&#26415;&#20132;&#38469;&#12289;&#31038;&#20250;&#12289;&#36947;&#24503;&#65289;&#36827;&#34892;&#35780;&#20272;&#26469;&#35268;&#23450;chatbot&#30340;&#35821;&#35328;&#34920;&#36798;&#35201;&#27714;&#65292;&#20197;&#36991;&#20813;&#20854;&#21463;&#32422;&#26463;&#30340;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;
The strive to make AI applications "safe" has led to the development of safety-measures as the main or even sole normative requirement of their permissible use. Similar can be attested to the latest version of chatbots, such as chatGPT. In this view, if they are "safe", they are supposed to be permissible to deploy. This approach, which we call "safety-normativity", is rather limited in solving the emerging issues that chatGPT and other chatbots have caused thus far. In answering this limitation, in this paper we argue for limiting chatbots in the range of topics they can chat about according to the normative concept of appropriateness. We argue that rather than looking for "safety" in a chatbot's utterances to determine what they may and may not say, we ought to assess those utterances according to three forms of appropriateness: technical-discursive, social, and moral. We then spell out what requirements for chatbots follow from these forms of appropriateness to avoid the limits of p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;&#8220;&#20849;&#21333;&#35843;&#29420;&#31435;&#20998;&#31867;&#22120;&#8221;(CIBer)&#30340;&#26032;&#25216;&#26415;&#65292;&#19987;&#27880;&#20110;&#29305;&#24449;&#30340;&#26368;&#20248;&#20998;&#21306;&#65292;&#26088;&#22312;&#20811;&#26381;&#26420;&#32032;&#36125;&#21494;&#26031;&#26041;&#27861;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#24182;&#19988;&#35777;&#26126;&#35813;&#25216;&#26415;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#29575;&#21644;&#26356;&#20302;&#30340;&#38169;&#35823;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.14537</link><description>&lt;p&gt;
&#22522;&#20110;&#36125;&#21494;&#26031;&#20998;&#31867;&#22120;&#30340;&#29305;&#24449;&#26368;&#20248;&#20998;&#21306;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Optimal partition of feature using Bayesian classifier. (arXiv:2304.14537v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14537
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;&#8220;&#20849;&#21333;&#35843;&#29420;&#31435;&#20998;&#31867;&#22120;&#8221;(CIBer)&#30340;&#26032;&#25216;&#26415;&#65292;&#19987;&#27880;&#20110;&#29305;&#24449;&#30340;&#26368;&#20248;&#20998;&#21306;&#65292;&#26088;&#22312;&#20811;&#26381;&#26420;&#32032;&#36125;&#21494;&#26031;&#26041;&#27861;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#24182;&#19988;&#35777;&#26126;&#35813;&#25216;&#26415;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#29575;&#21644;&#26356;&#20302;&#30340;&#38169;&#35823;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26420;&#32032;&#36125;&#21494;&#26031;&#20998;&#31867;&#22120;&#26159;&#19968;&#31181;&#24212;&#29992;&#36125;&#21494;&#26031;&#21407;&#29702;&#30340;&#27969;&#34892;&#20998;&#31867;&#26041;&#27861;&#65292;&#23613;&#31649;&#36755;&#20837;&#21464;&#37327;&#20043;&#38388;&#30340;&#26465;&#20214;&#20381;&#36182;&#20851;&#31995;&#21548;&#36215;&#26469;&#24456;&#22909;&#65292;&#20294;&#23454;&#38469;&#19978;&#20250;&#23548;&#33268;&#22823;&#22810;&#25968;&#25237;&#31080;&#39118;&#26684;&#30340;&#34892;&#20026;&#12290;&#26420;&#32032;&#36125;&#21494;&#26031;&#31639;&#27861;&#20013;&#30340;&#26576;&#20123;&#29305;&#24449;&#34987;&#31216;&#20026;&#29420;&#31435;&#29305;&#24449;&#65292;&#22240;&#20026;&#22312;&#39044;&#27979;&#20998;&#31867;&#26102;&#23427;&#20204;&#27809;&#26377;&#26465;&#20214;&#30456;&#20851;&#24615;&#25110;&#20381;&#36182;&#24615;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;&#8220;&#20849;&#21333;&#35843;&#29420;&#31435;&#20998;&#31867;&#22120;&#8221;(CIBer)&#30340;&#26032;&#25216;&#26415;&#65292;&#19987;&#27880;&#20110;&#29305;&#24449;&#30340;&#26368;&#20248;&#20998;&#21306;&#65292;&#26088;&#22312;&#20811;&#26381;&#26420;&#32032;&#36125;&#21494;&#26031;&#26041;&#27861;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#26126;&#30830;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#65292;&#22312;&#38169;&#35823;&#29575;&#26356;&#20302;&#12289;&#20934;&#30830;&#29575;&#26356;&#39640;&#25110;&#30456;&#24403;&#30340;&#24773;&#20917;&#19979;&#65292;&#19982;&#38543;&#26426;&#26862;&#26519;&#21644;XGBoost&#31561;&#27169;&#22411;&#30456;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Naive Bayesian classifier is a popular classification method employing the Bayesian paradigm. The concept of having conditional dependence among input variables sounds good in theory but can lead to a majority vote style behaviour. Achieving conditional independence is often difficult, and they introduce decision biases in the estimates. In Naive Bayes, certain features are called independent features as they have no conditional correlation or dependency when predicting a classification. In this paper, we focus on the optimal partition of features by proposing a novel technique called the Comonotone-Independence Classifier (CIBer) which is able to overcome the challenges posed by the Naive Bayes method. For different datasets, we clearly demonstrate the efficacy of our technique, where we achieve lower error rates and higher or equivalent accuracy compared to models such as Random Forests and XGBoost.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20854;&#20013;&#19968;&#20010;&#25200;&#21160;&#32593;&#32476;&#36890;&#36807;&#26368;&#22823;&#21270;&#26234;&#33021;&#20307;&#25191;&#34892;&#19981;&#21516;&#21160;&#20316;&#30340;&#27010;&#29575;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#29366;&#24577;&#30340;&#25197;&#26354;&#65292;&#20197;&#20943;&#36731;&#25968;&#25454;&#36807;&#25311;&#21512;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2304.14533</link><description>&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#23545;&#25239;&#31574;&#30053;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Adversarial Policy Optimization in Deep Reinforcement Learning. (arXiv:2304.14533v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14533
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20854;&#20013;&#19968;&#20010;&#25200;&#21160;&#32593;&#32476;&#36890;&#36807;&#26368;&#22823;&#21270;&#26234;&#33021;&#20307;&#25191;&#34892;&#19981;&#21516;&#21160;&#20316;&#30340;&#27010;&#29575;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#29366;&#24577;&#30340;&#25197;&#26354;&#65292;&#20197;&#20943;&#36731;&#25968;&#25454;&#36807;&#25311;&#21512;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#30340;&#31574;&#30053;&#21487;&#20197;&#36807;&#24230;&#25311;&#21512;&#35266;&#27979;&#20013;&#30340;&#34920;&#38754;&#29305;&#24449;&#65292;&#36825;&#20250;&#22952;&#30861;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#23398;&#20064;&#26377;&#25928;&#30340;&#31574;&#30053;&#12290;&#22312;&#39640;&#32500;&#29366;&#24577;&#19979;&#65292;&#36825;&#20010;&#38382;&#39064;&#21464;&#24471;&#26356;&#21152;&#20005;&#37325;&#65292;&#26234;&#33021;&#20307;&#38590;&#20197;&#23398;&#20064;&#26377;&#29992;&#30340;&#31574;&#30053;&#12290;&#25968;&#25454;&#22686;&#24378;&#21487;&#20197;&#36890;&#36807;&#20943;&#36731;&#36807;&#25311;&#21512;&#30340;&#24433;&#21709;&#26469;&#25552;&#20379;&#24615;&#33021;&#25552;&#21319;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#25968;&#25454;&#22686;&#24378;&#26159;&#19968;&#31181;&#20808;&#39564;&#30693;&#35782;&#65292;&#22914;&#26524;&#22312;&#29615;&#22659;&#20013;&#31616;&#21333;&#22320;&#24212;&#29992;&#23427;&#20204;&#21487;&#33021;&#20250;&#38477;&#20302;&#26234;&#33021;&#20307;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#20943;&#36731;&#19978;&#36848;&#38382;&#39064;&#24182;&#25552;&#39640;&#23398;&#20064;&#31574;&#30053;&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#19968;&#20010;&#21338;&#24328;&#29702;&#35770;&#30446;&#26631;&#65292;&#22312;&#36825;&#20010;&#30446;&#26631;&#20013;&#65292;&#25200;&#21160;&#32593;&#32476;&#20462;&#25913;&#29366;&#24577;&#65292;&#20197;&#26368;&#22823;&#21270;&#26234;&#33021;&#20307;&#25191;&#34892;&#19981;&#21516;&#21160;&#20316;&#30340;&#27010;&#29575;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#29366;&#24577;&#30340;&#25197;&#26354;&#12290;&#30456;&#21453;&#65292;&#31574;&#30053;&#32593;&#32476;&#26356;&#26032;&#20854;&#21442;&#25968;&#65292;&#20197;&#26368;&#23567;&#21270;&#25200;&#21160;&#25928;&#26524;&#65292;&#21516;&#26102;&#26368;&#22823;&#21270;&#26410;&#26469;&#22870;&#21169;&#30340;&#26399;&#26395;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
The policy represented by the deep neural network can overfit the spurious features in observations, which hamper a reinforcement learning agent from learning effective policy. This issue becomes severe in high-dimensional state, where the agent struggles to learn a useful policy. Data augmentation can provide a performance boost to RL agents by mitigating the effect of overfitting. However, such data augmentation is a form of prior knowledge, and naively applying them in environments might worsen an agent's performance. In this paper, we propose a novel RL algorithm to mitigate the above issue and improve the efficiency of the learned policy. Our approach consists of a max-min game theoretic objective where a perturber network modifies the state to maximize the agent's probability of taking a different action while minimizing the distortion in the state. In contrast, the policy network updates its parameters to minimize the effect of perturbation while maximizing the expected future r
&lt;/p&gt;</description></item><item><title>pyBibX&#26159;&#19968;&#31181;Python&#24211;&#65292;&#26088;&#22312;&#36890;&#36807;Scopus&#12289;Web of Science&#21644;PubMed&#30340;&#21407;&#22987;&#25968;&#25454;&#25991;&#20214;&#36827;&#34892;&#20840;&#38754;&#30340;&#25991;&#29486;&#35745;&#37327;&#21644;&#31185;&#23398;&#35745;&#37327;&#20998;&#26512;&#65292;&#24182;&#23558;&#20808;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#33021;&#21147;&#34701;&#20837;&#21040;&#20854;&#26680;&#24515;&#21151;&#33021;&#20013;&#12290;</title><link>http://arxiv.org/abs/2304.14516</link><description>&lt;p&gt;
pyBibX--&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#30340;&#29992;&#20110;&#25991;&#29486;&#35745;&#37327;&#21644;&#31185;&#23398;&#35745;&#37327;&#20998;&#26512;&#30340;Python&#24211;
&lt;/p&gt;
&lt;p&gt;
pyBibX -- A Python Library for Bibliometric and Scientometric Analysis Powered with Artificial Intelligence Tools. (arXiv:2304.14516v1 [cs.DL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14516
&lt;/p&gt;
&lt;p&gt;
pyBibX&#26159;&#19968;&#31181;Python&#24211;&#65292;&#26088;&#22312;&#36890;&#36807;Scopus&#12289;Web of Science&#21644;PubMed&#30340;&#21407;&#22987;&#25968;&#25454;&#25991;&#20214;&#36827;&#34892;&#20840;&#38754;&#30340;&#25991;&#29486;&#35745;&#37327;&#21644;&#31185;&#23398;&#35745;&#37327;&#20998;&#26512;&#65292;&#24182;&#23558;&#20808;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#33021;&#21147;&#34701;&#20837;&#21040;&#20854;&#26680;&#24515;&#21151;&#33021;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#29486;&#35745;&#37327;&#21644;&#31185;&#23398;&#35745;&#37327;&#20998;&#26512;&#25552;&#20379;&#20102;&#23545;&#36328;&#23398;&#31185;&#30340;&#22797;&#26434;&#30740;&#31350;&#39046;&#22495;&#21644;&#21327;&#20316;&#21160;&#24577;&#30340;&#23453;&#36149;&#27934;&#23519;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102; pyBibX&#65292;&#36825;&#26159;&#19968;&#20010;Python&#24211;&#65292;&#26088;&#22312;&#22312;&#26469;&#33258;Scopus&#12289;Web of Science&#21644;PubMed&#30340;&#21407;&#22987;&#25968;&#25454;&#25991;&#20214;&#19978;&#36827;&#34892;&#20840;&#38754;&#30340;&#25991;&#29486;&#35745;&#37327;&#21644;&#31185;&#23398;&#35745;&#37327;&#20998;&#26512;&#65292;&#24182;&#23558;&#26368;&#20808;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#33021;&#21147;&#26080;&#32541;&#38598;&#25104;&#21040;&#20854;&#26680;&#24515;&#21151;&#33021;&#20013;&#12290;&#35813;&#24211;&#25191;&#34892;&#20840;&#38754;&#30340;EDA&#65292;&#36890;&#36807;&#35270;&#35273;&#21560;&#24341;&#20154;&#30340;&#22270;&#24418;&#35828;&#26126;&#32467;&#26524;&#12290;&#32593;&#32476;&#21151;&#33021;&#24050;&#32463;&#24039;&#22937;&#22320;&#38598;&#25104;&#65292;&#21253;&#25324;&#24341;&#25991;&#12289;&#21327;&#20316;&#21644;&#30456;&#20284;&#24615;&#20998;&#26512;&#12290;&#27492;&#22806;&#65292;&#35813;&#24211;&#36824;&#21253;&#21547;&#20102;&#20154;&#24037;&#26234;&#33021;&#33021;&#21147;&#65292;&#21253;&#25324;&#23884;&#20837;&#21521;&#37327;&#12289;&#19987;&#39064;&#24314;&#27169;&#12289;&#25991;&#26412;&#25688;&#35201;&#21644;&#20854;&#20182;&#19968;&#33324;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#20351;&#29992;&#27169;&#22411;&#22914;&#21477;&#23376;BERT&#12289;BerTopic&#12289;BERT&#12289;chatGPT&#21644;PEGASUS&#12290;&#20316;&#20026;&#31034;&#33539;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#19982;&#22810;&#26631;&#20934;&#20915;&#31574;&#30456;&#20851;&#30340;184&#20221;&#25991;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bibliometric and Scientometric analyses offer invaluable perspectives on the complex research terrain and collaborative dynamics spanning diverse academic disciplines. This paper presents pyBibX, a python library devised to conduct comprehensive bibliometric and scientometric analyses on raw data files sourced from Scopus, Web of Science, and PubMed, seamlessly integrating state of the art AI capabilities into its core functionality. The library executes a comprehensive EDA, presenting outcomes via visually appealing graphical illustrations. Network capabilities have been deftly integrated, encompassing Citation, Collaboration, and Similarity Analysis. Furthermore, the library incorporates AI capabilities, including Embedding vectors, Topic Modeling, Text Summarization, and other general Natural Language Processing tasks, employing models such as Sentence-BERT, BerTopic, BERT, chatGPT, and PEGASUS. As a demonstration, we have analyzed 184 documents associated with multiple-criteria dec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#35299;&#20915;&#38754;&#37096;&#21464;&#24418;&#25915;&#20987;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#37319;&#29992;&#22810;&#20010;&#28145;&#24230;&#31070;&#32463;&#21367;&#31215;&#26550;&#26500;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#38590;&#20197;&#29702;&#35299;&#21644;&#20998;&#26512;&#65292;&#32473;&#29983;&#29289;&#27979;&#37327;&#23398;&#30028;&#24102;&#26469;&#20102;&#22256;&#25200;&#12290;</title><link>http://arxiv.org/abs/2304.14509</link><description>&lt;p&gt;
&#19968;&#31181;&#39640;&#25928;&#30340;&#38598;&#25104;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26041;&#27861;&#29992;&#20110;&#21464;&#24418;&#20154;&#33080;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
An Efficient Ensemble Explainable AI (XAI) Approach for Morphed Face Detection. (arXiv:2304.14509v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14509
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#35299;&#20915;&#38754;&#37096;&#21464;&#24418;&#25915;&#20987;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#37319;&#29992;&#22810;&#20010;&#28145;&#24230;&#31070;&#32463;&#21367;&#31215;&#26550;&#26500;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#38590;&#20197;&#29702;&#35299;&#21644;&#20998;&#26512;&#65292;&#32473;&#29983;&#29289;&#27979;&#37327;&#23398;&#30028;&#24102;&#26469;&#20102;&#22256;&#25200;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#35782;&#21035;&#35748;&#35777;&#31995;&#32479;&#30340;&#24191;&#27867;&#20351;&#29992;&#23548;&#33268;&#25915;&#20987;&#32773;/&#20882;&#20805;&#32773;&#36890;&#36807;&#20266;&#36896;&#22270;&#20687;&#26469;&#20266;&#36896;&#29992;&#25143;&#36523;&#20221;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#38754;&#37096;&#21464;&#24418;&#25915;&#20987;&#26816;&#27979;&#65288;MAD&#65289;&#30340;&#22810;&#20010;&#28145;&#24230;&#31070;&#32463;&#21367;&#31215;&#26550;&#26500;&#65292;&#20197;&#39044;&#38450;&#36825;&#31181;&#25915;&#20987;&#24182;&#20943;&#23569;&#19982;&#20854;&#30456;&#20851;&#30340;&#39118;&#38505;&#12290;&#34429;&#28982;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20248;&#32467;&#26524;&#65292;&#20294;&#30001;&#20110;&#20854;&#40657;&#30418;/&#19981;&#36879;&#26126;&#30340;&#29305;&#24615;&#65292;&#38590;&#20197;&#29702;&#35299;&#21644;&#20998;&#26512;&#36825;&#20123;&#32593;&#32476;&#12290;&#22240;&#27492;&#65292;&#21487;&#33021;&#20250;&#20570;&#20986;&#38169;&#35823;&#30340;&#21028;&#26029;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#32570;&#20047;&#26377;&#20851;&#40657;&#21283;&#23376;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#20915;&#31574;&#26041;&#27861;&#65292;&#36825;&#20123;&#27169;&#22411;&#29992;&#20110;&#29983;&#29289;&#27979;&#37327;&#23398;&#34920;&#31034;&#25915;&#20987;&#26816;&#27979;&#65288;PAD&#65289;&#25110;MAD&#21487;&#24110;&#21161;&#29983;&#29289;&#27979;&#37327;&#23398;&#30028;&#23545;&#20110;&#20351;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#29983;&#29289;&#27979;&#37327;&#23398;&#31995;&#32479;&#36827;&#34892;&#36523;&#20221;&#39564;&#35777;&#21644;&#35748;&#35777;&#24863;&#21040;&#20449;&#20219;&#12290;
&lt;/p&gt;
&lt;p&gt;
The extensive utilization of biometric authentication systems have emanated attackers / imposters to forge user identity based on morphed images. In this attack, a synthetic image is produced and merged with genuine. Next, the resultant image is user for authentication. Numerous deep neural convolutional architectures have been proposed in literature for face Morphing Attack Detection (MADs) to prevent such attacks and lessen the risks associated with them. Although, deep learning models achieved optimal results in terms of performance, it is difficult to understand and analyse these networks since they are black box/opaque in nature. As a consequence, incorrect judgments may be made. There is, however, a dearth of literature that explains decision-making methods of black box deep learning models for biometric Presentation Attack Detection (PADs) or MADs that can aid the biometric community to have trust in deep learning-based biometric systems for identification and authentication in 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#19977;&#31181;&#26032;&#26041;&#27861;&#26469;&#21019;&#24314;&#20154;&#31867;&#36816;&#21160;&#30340;&#21487;&#35299;&#37322;&#34920;&#31034;&#65292;&#20197;&#35299;&#20915;&#24314;&#27169;&#20154;&#20307;&#36816;&#21160;&#30340;&#31185;&#23398;&#25361;&#25112;&#65292;&#24182;&#19988;&#35813;&#26041;&#27861;&#33021;&#20934;&#30830;&#39044;&#27979;&#20154;&#20307;&#36816;&#21160;&#24182;&#29983;&#25104;&#26032;&#30340;&#36816;&#21160;&#12290;</title><link>http://arxiv.org/abs/2304.14502</link><description>&lt;p&gt;
&#29992;&#20110;&#21487;&#35299;&#37322;&#30340;&#23039;&#21183;&#34920;&#36798;&#12289;&#20998;&#26512;&#21644;&#29983;&#25104;&#30340;&#28145;&#23618;&#29366;&#24577;&#31354;&#38388;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Deep state-space modeling for explainable representation, analysis, and generation of professional human poses. (arXiv:2304.14502v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14502
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#19977;&#31181;&#26032;&#26041;&#27861;&#26469;&#21019;&#24314;&#20154;&#31867;&#36816;&#21160;&#30340;&#21487;&#35299;&#37322;&#34920;&#31034;&#65292;&#20197;&#35299;&#20915;&#24314;&#27169;&#20154;&#20307;&#36816;&#21160;&#30340;&#31185;&#23398;&#25361;&#25112;&#65292;&#24182;&#19988;&#35813;&#26041;&#27861;&#33021;&#20934;&#30830;&#39044;&#27979;&#20154;&#20307;&#36816;&#21160;&#24182;&#29983;&#25104;&#26032;&#30340;&#36816;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#24191;&#27867;&#30340;&#23454;&#38469;&#24212;&#29992;&#65292;&#20154;&#20307;&#21160;&#20316;&#30340;&#20998;&#26512;&#24050;&#34987;&#24191;&#27867;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#21069;&#27839;&#20173;&#28982;&#38754;&#20020;&#24314;&#27169;&#20154;&#20307;&#36816;&#21160;&#30340;&#31185;&#23398;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#38656;&#35201;&#26032;&#30340;&#27169;&#22411;&#26469;&#32771;&#34385;&#20154;&#31867;&#36816;&#21160;&#30340;&#38543;&#26426;&#24615;&#21644;&#20154;&#20307;&#30340;&#29289;&#29702;&#32467;&#26500;&#65292;&#20197;&#20934;&#30830;&#39044;&#27979;&#20840;&#36523;&#36816;&#21160;&#25551;&#36848;&#31526;&#38543;&#26102;&#38388;&#30340;&#28436;&#21464;&#12290;&#20854;&#27425;&#65292;&#29616;&#26377;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#22312;&#29983;&#25104;&#20154;&#31867;&#36816;&#21160;&#26102;&#65292;&#23545;&#20110;&#20854;&#36523;&#20307;&#23039;&#21183;&#39044;&#27979;&#30340;&#21487;&#35299;&#37322;&#24615;&#20173;&#38656;&#25913;&#36827;&#65292;&#22240;&#20026;&#23427;&#20204;&#32570;&#20047;&#20154;&#31867;&#36816;&#21160;&#30340;&#21487;&#29702;&#35299;&#34920;&#31034;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19977;&#31181;&#26032;&#26041;&#27861;&#26469;&#21019;&#24314;&#20154;&#31867;&#36816;&#21160;&#30340;&#21487;&#35299;&#37322;&#34920;&#31034;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#20840;&#36523;&#36816;&#21160;&#34987;&#20844;&#24335;&#21270;&#20026;&#21160;&#24577;&#31995;&#32479;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;&#20854;&#21442;&#25968;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#32479;&#35745;&#31639;&#27861;&#36827;&#34892;&#20272;&#35745;&#12290;&#36825;&#20123;&#34920;&#31034;&#36981;&#24490;&#25903;&#25345;&#20154;&#31867;&#36816;&#21160;&#30340;&#32467;&#26500;&#21270;&#21407;&#21017;&#65292;&#24182;&#19988;&#30001;&#29992;&#25143;&#21487;&#20197;&#29702;&#35299;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#20154;&#20307;&#36816;&#21160;&#24182;&#29983;&#25104;&#26032;&#30340;&#36816;&#21160;&#65292;&#21516;&#26102;&#25552;&#20379;&#26377;&#24847;&#20041;&#30340;&#36523;&#20307;&#23039;&#21183;&#39044;&#27979;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
The analysis of human movements has been extensively studied due to its wide variety of practical applications. Nevertheless, the state-of-the-art still faces scientific challenges while modeling human movements. Firstly, new models that account for the stochasticity of human movement and the physical structure of the human body are required to accurately predict the evolution of full-body motion descriptors over time. Secondly, the explainability of existing deep learning algorithms regarding their body posture predictions while generating human movements still needs to be improved as they lack comprehensible representations of human movement. This paper addresses these challenges by introducing three novel approaches for creating explainable representations of human movement. In this work, full-body movement is formulated as a state-space model of a dynamic system whose parameters are estimated using deep learning and statistical algorithms. The representations adhere to the structur
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#35270;&#39057;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#20154;&#31867;&#20449;&#24565;&#39044;&#27979;&#65292;&#20197;&#20419;&#36827;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#25512;&#26029;&#20154;&#31867;&#20449;&#24565;&#30340;&#21457;&#23637;&#21644;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2304.14501</link><description>&lt;p&gt;
&#35835;&#25026;&#25105;&#30340;&#24819;&#27861;&#65306;&#19968;&#20010;&#29992;&#20110;&#20154;&#31867;&#20449;&#24565;&#39044;&#27979;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Read My Mind: A Multi-Modal Dataset for Human Belief Prediction. (arXiv:2304.14501v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14501
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#35270;&#39057;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#20154;&#31867;&#20449;&#24565;&#39044;&#27979;&#65292;&#20197;&#20419;&#36827;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#25512;&#26029;&#20154;&#31867;&#20449;&#24565;&#30340;&#21457;&#23637;&#21644;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#20154;&#31867;&#24847;&#22270;&#23545;&#20110;&#23454;&#29616;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#20154;&#26426;&#20132;&#20114;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#20351;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#25512;&#26029;&#20154;&#31867;&#20449;&#24565;&#30340;&#33021;&#21147;&#24471;&#21040;&#21457;&#23637;&#21644;&#35780;&#20272;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#29289;&#20307;&#29615;&#22659;&#20851;&#31995;&#30340;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#35270;&#39057;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#24847;&#22270;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding human intentions is key to enabling effective and efficient human-robot interaction (HRI) in collaborative settings. To enable developments and evaluation of the ability of artificial intelligence (AI) systems to infer human beliefs, we introduce a large-scale multi-modal video dataset for intent prediction based on object-context relations.
&lt;/p&gt;</description></item><item><title>MWaste&#26159;&#19968;&#27454;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#31227;&#21160;&#24212;&#29992;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#23558;&#22403;&#22334;&#26448;&#26009;&#20998;&#31867;&#20026;&#22403;&#22334;&#12289;&#22609;&#26009;&#12289;&#32440;&#24352;&#12289;&#37329;&#23646;&#12289;&#29627;&#29827;&#25110;&#30828;&#32440;&#26495;&#65292;&#21487;&#24110;&#21161;&#24212;&#23545;&#27668;&#20505;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2304.14498</link><description>&lt;p&gt;
MWaste&#65306;&#31649;&#29702;&#23478;&#24237;&#22403;&#22334;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MWaste: A Deep Learning Approach to Manage Household Waste. (arXiv:2304.14498v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14498
&lt;/p&gt;
&lt;p&gt;
MWaste&#26159;&#19968;&#27454;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#31227;&#21160;&#24212;&#29992;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#23558;&#22403;&#22334;&#26448;&#26009;&#20998;&#31867;&#20026;&#22403;&#22334;&#12289;&#22609;&#26009;&#12289;&#32440;&#24352;&#12289;&#37329;&#23646;&#12289;&#29627;&#29827;&#25110;&#30828;&#32440;&#26495;&#65292;&#21487;&#24110;&#21161;&#24212;&#23545;&#27668;&#20505;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#35270;&#35273;&#26041;&#27861;&#24050;&#34987;&#35777;&#26126;&#22312;&#20998;&#31867;&#22403;&#22334;&#22788;&#29702;&#30340;&#22238;&#25910;&#20998;&#31867;&#26041;&#38754;&#24456;&#26377;&#25928;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#25104;&#26412;&#39640;&#12289;&#19981;&#31934;&#30830;&#19988;&#19981;&#28165;&#26224;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;MWaste&#65292;&#19968;&#27454;&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#65292;&#21033;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#23558;&#22403;&#22334;&#26448;&#26009;&#20998;&#31867;&#20026;&#22403;&#22334;&#12289;&#22609;&#26009;&#12289;&#32440;&#24352;&#12289;&#37329;&#23646;&#12289;&#29627;&#29827;&#25110;&#30828;&#32440;&#26495;&#12290;&#20854;&#26377;&#25928;&#24615;&#24050;&#22312;&#21508;&#31181;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#21644;&#30495;&#23454;&#19990;&#30028;&#22270;&#20687;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#22312;&#27979;&#35797;&#38598;&#19978;&#36798;&#21040;&#20102;92&#65285;&#30340;&#24179;&#22343;&#31934;&#24230;&#12290;&#35813;&#24212;&#29992;&#31243;&#24207;&#21487;&#20197;&#36890;&#36807;&#20351;&#22403;&#22334;&#22788;&#29702;&#26356;&#26377;&#25928;&#24182;&#20943;&#23569;&#22240;&#19981;&#27491;&#30830;&#30340;&#22403;&#22334;&#22788;&#29702;&#32780;&#24341;&#36215;&#30340;&#28201;&#23460;&#27668;&#20307;&#25490;&#25918;&#26469;&#24110;&#21161;&#24212;&#23545;&#27668;&#20505;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computer vision methods have shown to be effective in classifying garbage into recycling categories for waste processing, existing methods are costly, imprecise, and unclear. To tackle this issue, we introduce MWaste, a mobile application that uses computer vision and deep learning techniques to classify waste materials as trash, plastic, paper, metal, glass or cardboard. Its effectiveness was tested on various neural network architectures and real-world images, achieving an average precision of 92\% on the test set. This app can help combat climate change by enabling efficient waste processing and reducing the generation of greenhouse gases caused by incorrect waste disposal.
&lt;/p&gt;</description></item><item><title>&#26080;&#38656;&#25509;&#35302;&#24335;&#20256;&#24863;&#22120;&#65292;&#20351;&#29992;&#26234;&#33021;&#25163;&#26426;&#25668;&#20687;&#22836;&#27979;&#37327;SpO2&#30340;&#25361;&#25112;&#24615;&#20351;&#24471;&#38656;&#35201;&#25552;&#21462;&#38754;&#37096;&#20852;&#36259;&#21306;&#22495;&#65292;&#20174;&#20013;&#33719;&#21462;&#20809;&#30005;&#23481;&#25239;&#20449;&#21495;&#24182;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20272;&#35745;SpO2&#12290;</title><link>http://arxiv.org/abs/2304.14495</link><description>&lt;p&gt;
&#29983;&#29702;&#21644;&#21307;&#30103;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Model Explainability in Physiological and Healthcare-based Neural Networks. (arXiv:2304.14495v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14495
&lt;/p&gt;
&lt;p&gt;
&#26080;&#38656;&#25509;&#35302;&#24335;&#20256;&#24863;&#22120;&#65292;&#20351;&#29992;&#26234;&#33021;&#25163;&#26426;&#25668;&#20687;&#22836;&#27979;&#37327;SpO2&#30340;&#25361;&#25112;&#24615;&#20351;&#24471;&#38656;&#35201;&#25552;&#21462;&#38754;&#37096;&#20852;&#36259;&#21306;&#22495;&#65292;&#20174;&#20013;&#33719;&#21462;&#20809;&#30005;&#23481;&#25239;&#20449;&#21495;&#24182;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20272;&#35745;SpO2&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#21628;&#21560;&#21151;&#33021;&#21644;&#27835;&#30103;&#24930;&#24615;&#32954;&#37096;&#30142;&#30149;&#20851;&#38190;&#22312;&#20110;&#20272;&#35745;&#21644;&#30417;&#27979;SpO2&#12290;COVID-19&#22823;&#27969;&#34892;&#20984;&#26174;&#20102;&#22312;&#26080;&#30151;&#29366;&#30149;&#20154;&#20013;&#21450;&#26089;&#26816;&#27979;SpO2&#21464;&#21270;&#30340;&#37325;&#35201;&#24615;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;SpO2&#27979;&#37327;&#26041;&#27861;&#20381;&#36182;&#20110;&#25509;&#35302;&#24335;&#20256;&#24863;&#65292;&#23384;&#22312;&#20132;&#21449;&#24863;&#26579;&#21644;&#24739;&#26377;&#32930;&#20307;&#28748;&#27969;&#38556;&#30861;&#30340;&#24739;&#32773;&#24182;&#21457;&#30151;&#30340;&#39118;&#38505;&#12290;&#27492;&#22806;&#65292;&#33033;&#25615;&#34880;&#27687;&#20202;&#21487;&#33021;&#22312;&#36793;&#32536;&#21270;&#31038;&#21306;&#21644;&#27424;&#21457;&#36798;&#22269;&#23478;&#19981;&#21487;&#29992;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#35752;&#20351;&#29992;&#35270;&#39057;&#27979;&#37327;SpO2&#20197;&#35299;&#20915;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#20294;&#20351;&#29992;&#26234;&#33021;&#25163;&#26426;&#25668;&#20687;&#22836;&#26080;&#25509;&#35302;&#22320;&#27979;&#37327;SpO2&#65292;&#29305;&#21035;&#26159;&#30001;&#20110;&#26234;&#33021;&#25163;&#26426;&#25668;&#20687;&#22836;&#20256;&#24863;&#22120;&#30340;&#29983;&#29702;&#20449;&#21495;&#36739;&#24494;&#24369;&#19988;&#20809;&#23398;&#36873;&#25321;&#24615;&#36739;&#20302;&#65292;&#22240;&#27492;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#35813;&#31995;&#32479;&#21253;&#25324;&#19977;&#20010;&#20027;&#35201;&#27493;&#39588;&#65306;1&#65289;&#25552;&#21462;&#38754;&#37096;&#21306;&#22495;&#30340;&#20852;&#36259;&#21306;&#22495;&#65288;ROI&#65289;&#65292;2&#65289;&#20174;ROI&#33719;&#21462;&#20809;&#30005;&#23481;&#25239;&#65288;PPG&#65289;&#20449;&#21495;&#65292;3&#65289;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20174;&#33719;&#21462;&#30340;PPG&#20449;&#21495;&#20272;&#35745;SpO2&#12290;
&lt;/p&gt;
&lt;p&gt;
The estimation and monitoring of SpO2 are crucial for assessing lung function and treating chronic pulmonary diseases. The COVID-19 pandemic has highlighted the importance of early detection of changes in SpO2, particularly in asymptomatic patients with clinical deterioration. However, conventional SpO2 measurement methods rely on contact-based sensing, presenting the risk of cross-contamination and complications in patients with impaired limb perfusion. Additionally, pulse oximeters may not be available in marginalized communities and undeveloped countries. To address these limitations and provide a more comfortable and unobtrusive way to monitor SpO2, recent studies have investigated SpO2 measurement using videos. However, measuring SpO2 using cameras in a contactless way, particularly from smartphones, is challenging due to weaker physiological signals and lower optical selectivity of smartphone camera sensors. The system includes three main steps: 1) extraction of the region of int
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#20027;&#21160;&#25512;&#29702;&#19979;&#30340;&#29289;&#20307;&#20013;&#24515;&#34920;&#31034;&#20013;&#30340;&#23545;&#31216;&#24615;&#65292;&#20197;&#29983;&#25104;&#26368;&#31616;&#27905;&#32780;&#20934;&#30830;&#30340;&#27169;&#22411;&#65292;&#20174;&#32780;&#23398;&#20064;&#21644;&#39044;&#27979;&#26032;&#30340;&#29289;&#20307;&#35270;&#22270;&#12290;</title><link>http://arxiv.org/abs/2304.14493</link><description>&lt;p&gt;
&#23545;&#35937;&#20013;&#24515;&#30340;&#28145;&#24230;&#20027;&#21160;&#25512;&#29702;&#27169;&#22411;&#20013;&#30340;&#23545;&#31216;&#24615;&#19982;&#22797;&#26434;&#24615;
&lt;/p&gt;
&lt;p&gt;
Symmetry and Complexity in Object-Centric Deep Active Inference Models. (arXiv:2304.14493v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14493
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#20027;&#21160;&#25512;&#29702;&#19979;&#30340;&#29289;&#20307;&#20013;&#24515;&#34920;&#31034;&#20013;&#30340;&#23545;&#31216;&#24615;&#65292;&#20197;&#29983;&#25104;&#26368;&#31616;&#27905;&#32780;&#20934;&#30830;&#30340;&#27169;&#22411;&#65292;&#20174;&#32780;&#23398;&#20064;&#21644;&#39044;&#27979;&#26032;&#30340;&#29289;&#20307;&#35270;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#27599;&#22825;&#35201;&#24863;&#30693;&#21644;&#20114;&#21160;&#19978;&#30334;&#20010;&#29289;&#20307;&#12290;&#20026;&#27492;&#65292;&#20182;&#20204;&#38656;&#35201;&#20351;&#29992;&#36825;&#20123;&#29289;&#20307;&#30340;&#24515;&#29702;&#27169;&#22411;&#65292;&#24182;&#32463;&#24120;&#21033;&#29992;&#29289;&#20307;&#24418;&#29366;&#21644;&#22806;&#35266;&#30340;&#23545;&#31216;&#24615;&#26469;&#23398;&#20064;&#36890;&#29992;&#21644;&#21487;&#36716;&#31227;&#30340;&#25216;&#33021;&#12290;&#20027;&#21160;&#25512;&#29702;&#26159;&#29702;&#35299;&#21644;&#24314;&#27169;&#26377;&#24863;&#30693;&#33021;&#21147;&#30340;&#20195;&#29702;&#30340;&#19968;&#31181;&#22522;&#26412;&#26041;&#27861;&#12290;&#23427;&#35748;&#20026;&#20195;&#29702;&#20154;&#22312;&#29615;&#22659;&#20013;&#20135;&#29983;&#27169;&#22411;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#19978;&#38480;&#30340;&#24778;&#22855;&#65288;&#21363;&#33258;&#30001;&#33021;&#65289;&#26469;&#23398;&#20064;&#21644;&#34892;&#21160;&#12290;&#33258;&#30001;&#33021;&#20998;&#35299;&#20026;&#20934;&#30830;&#24615;&#21644;&#22797;&#26434;&#24615;&#39033;&#65292;&#36825;&#24847;&#21619;&#30528;&#20195;&#29702;&#20542;&#21521;&#20110;&#36873;&#25321;&#26368;&#31616;&#21333;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#20934;&#30830;&#22320;&#35299;&#37322;&#20182;&#20204;&#30340;&#24863;&#35273;&#35266;&#23519;&#32467;&#26524;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#29305;&#23450;&#29289;&#20307;&#22825;&#29983;&#23545;&#31216;&#24615;&#22312;&#28145;&#24230;&#20027;&#21160;&#25512;&#29702;&#19979;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#29366;&#24577;&#31354;&#38388;&#20013;&#20063;&#34920;&#29616;&#20026;&#23545;&#31216;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20851;&#27880;&#23545;&#35937;&#20013;&#24515;&#30340;&#34920;&#31034;&#65292;&#20854;&#20174;&#20687;&#32032;&#20013;&#35757;&#32451;&#20197;&#39044;&#27979;&#26032;&#30340;&#29289;&#20307;&#35270;&#22270;&#32780;&#24180;&#40836;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans perceive and interact with hundreds of objects every day. In doing so, they need to employ mental models of these objects and often exploit symmetries in the object's shape and appearance in order to learn generalizable and transferable skills. Active inference is a first principles approach to understanding and modeling sentient agents. It states that agents entertain a generative model of their environment, and learn and act by minimizing an upper bound on their surprisal, i.e. their Free Energy. The Free Energy decomposes into an accuracy and complexity term, meaning that agents favor the least complex model, that can accurately explain their sensory observations. In this paper, we investigate how inherent symmetries of particular objects also emerge as symmetries in the latent state space of the generative model learnt under deep active inference. In particular, we focus on object-centric representations, which are trained from pixels to predict novel object views as the age
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38450;&#24481;&#24615;&#26694;&#26550;&#65292;&#38024;&#23545;&#23545;&#25239;&#24615;&#21518;&#38376;&#25915;&#20987;&#65292;&#21033;&#29992;&#21487;&#24863;&#30693;&#27169;&#24335;&#21387;&#20498;&#25915;&#20987;&#32773;&#30340;&#19981;&#21487;&#24863;&#30693;&#27169;&#24335;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.14483</link><description>&lt;p&gt;
&#23545;&#25239;&#24863;&#30693;&#30340;&#36845;&#20195;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Adversary Aware Continual Learning. (arXiv:2304.14483v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14483
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38450;&#24481;&#24615;&#26694;&#26550;&#65292;&#38024;&#23545;&#23545;&#25239;&#24615;&#21518;&#38376;&#25915;&#20987;&#65292;&#21033;&#29992;&#21487;&#24863;&#30693;&#27169;&#24335;&#21387;&#20498;&#25915;&#20987;&#32773;&#30340;&#19981;&#21487;&#24863;&#30693;&#27169;&#24335;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;&#26041;&#27861;&#38750;&#24120;&#26377;&#29992;&#65292;&#22240;&#20026;&#23427;&#20204;&#24110;&#21161;&#27169;&#22411;&#25353;&#39034;&#24207;&#23398;&#20064;&#26032;&#20449;&#24687;&#65288;&#31867;&#21035;&#65289;&#65292;&#21516;&#26102;&#20445;&#30041;&#20043;&#21069;&#33719;&#24471;&#30340;&#20449;&#24687;&#65288;&#31867;&#21035;&#65289;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#26041;&#27861;&#26497;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#21518;&#38376;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#22312;&#35757;&#32451;&#26399;&#38388;&#65292;&#26234;&#33021;&#23545;&#25163;&#21487;&#20197;&#36890;&#36807;&#24341;&#20837;&#23569;&#37327;&#30340;&#20449;&#24687;&#35823;&#23548;&#27169;&#22411;&#65292;&#20174;&#32780;&#22312;&#27979;&#35797;&#26102;&#25925;&#24847;&#24536;&#35760;&#29305;&#23450;&#30340;&#20219;&#21153;&#25110;&#31867;&#21035;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38450;&#24481;&#24615;&#26694;&#26550;&#26469;&#21453;&#20987;&#36825;&#31181;&#28508;&#22312;&#25915;&#20987;&#12290;&#25105;&#20204;&#21033;&#29992;&#25915;&#20987;&#32773;&#30340;&#20027;&#35201;&#20248;&#21183;--&#20351;&#21518;&#38376;&#27169;&#24335;&#23545;&#20154;&#19981;&#21487;&#24863;&#30693;--&#24182;&#25552;&#35758;&#22312;&#35757;&#32451;&#26399;&#38388;&#23398;&#20064;&#19968;&#20010;&#21487;&#20197;&#21387;&#20498;&#25915;&#20987;&#32773;&#30340;&#21487;&#24863;&#30693;&#27169;&#24335;&#20197;&#25269;&#28040;&#23545;&#25239;&#24615;&#25915;&#20987;&#32773;&#30340;&#27169;&#24335;&#12290;&#36890;&#36807;&#21508;&#31181;&#24120;&#29992;&#30340;Replay-based&#65288;&#20004;&#32773;&#37117;
&lt;/p&gt;
&lt;p&gt;
Class incremental learning approaches are useful as they help the model to learn new information (classes) sequentially, while also retaining the previously acquired information (classes). However, it has been shown that such approaches are extremely vulnerable to the adversarial backdoor attacks, where an intelligent adversary can introduce small amount of misinformation to the model in the form of imperceptible backdoor pattern during training to cause deliberate forgetting of a specific task or class at test time. In this work, we propose a novel defensive framework to counter such an insidious attack where, we use the attacker's primary strength-hiding the backdoor pattern by making it imperceptible to humans-against it, and propose to learn a perceptible (stronger) pattern (also during the training) that can overpower the attacker's imperceptible (weaker) pattern. We demonstrate the effectiveness of the proposed defensive mechanism through various commonly used Replay-based (both 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#32534;&#30721;&#22312;&#35268;&#21017;&#32593;&#26684;&#19978;&#30340;NeRFs&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#37319;&#26679;&#20986;&#36924;&#30495;&#30340;NeRFs&#65292;&#24182;&#20801;&#35768;&#26377;&#26465;&#20214;&#30340;&#29983;&#25104;&#65292;&#32473;&#23450;&#26576;&#20010;&#35266;&#23519;&#20316;&#20026;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2304.14473</link><description>&lt;p&gt;
&#23398;&#20064;&#25193;&#25955;&#20808;&#39564;&#29992;&#20110;NeRFs
&lt;/p&gt;
&lt;p&gt;
Learning a Diffusion Prior for NeRFs. (arXiv:2304.14473v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14473
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#32534;&#30721;&#22312;&#35268;&#21017;&#32593;&#26684;&#19978;&#30340;NeRFs&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#37319;&#26679;&#20986;&#36924;&#30495;&#30340;NeRFs&#65292;&#24182;&#20801;&#35768;&#26377;&#26465;&#20214;&#30340;&#29983;&#25104;&#65292;&#32473;&#23450;&#26576;&#20010;&#35266;&#23519;&#20316;&#20026;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRFs&#65289;&#24050;&#25104;&#20026;&#20174;2D&#25968;&#25454;&#27966;&#29983;&#20986;&#30340;&#29289;&#20307;&#21644;&#22330;&#26223;&#30340;&#24378;&#22823;&#31070;&#32463;3D&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#29983;&#25104;NeRFs&#20173;&#28982;&#26159;&#22256;&#38590;&#30340;&#12290;&#20363;&#22914;&#65292;&#21482;&#20351;&#29992;&#23569;&#37327;&#35270;&#22270;&#20316;&#20026;&#30417;&#30563;&#35757;&#32451;NeRFs&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#26159;&#19968;&#20010;&#27424;&#21442;&#25968;&#38382;&#39064;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#38656;&#35201;&#19968;&#20123;&#24402;&#32435;&#20808;&#39564;&#26469;&#36807;&#28388;&#19981;&#33391;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;&#24341;&#20837;&#36825;&#26679;&#30340;&#24402;&#32435;&#20808;&#39564;&#30340;&#19968;&#31181;&#26041;&#27861;&#26159;&#23398;&#20064;NeRFs&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#24314;&#27169;&#26576;&#31867;&#22330;&#26223;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#32534;&#30721;&#22312;&#35268;&#21017;&#32593;&#26684;&#19978;&#30340;NeRFs&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#37319;&#26679;&#20986;&#36924;&#30495;&#30340;NeRFs&#65292;&#24182;&#21516;&#26102;&#20801;&#35768;&#26377;&#26465;&#20214;&#30340;&#29983;&#25104;&#65292;&#32473;&#23450;&#26576;&#20010;&#35266;&#23519;&#20316;&#20026;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Radiance Fields (NeRFs) have emerged as a powerful neural 3D representation for objects and scenes derived from 2D data. Generating NeRFs, however, remains difficult in many scenarios. For instance, training a NeRF with only a small number of views as supervision remains challenging since it is an under-constrained problem. In such settings, it calls for some inductive prior to filter out bad local minima. One way to introduce such inductive priors is to learn a generative model for NeRFs modeling a certain class of scenes. In this paper, we propose to use a diffusion model to generate NeRFs encoded on a regularized grid. We show that our model can sample realistic NeRFs, while at the same time allowing conditional generations, given a certain observation as guidance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#20855;&#26377;&#35821;&#20041;&#24863;&#30693;&#20808;&#39564;&#30340;&#21487;&#25511;&#21333;&#27425;&#20154;&#33080;&#35270;&#39057;&#21512;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#21487;&#38752;&#30340;&#38754;&#37096;&#20998;&#21106;&#21644;&#26032;&#39062;&#30340;&#35821;&#20041;&#24863;&#30693;&#36816;&#21160;&#25197;&#26354;&#26041;&#26696;&#65292;&#22312;&#20445;&#35777;&#35821;&#20041;&#20934;&#30830;&#21644;&#23454;&#29616;&#30495;&#23454;&#36816;&#21160;&#30340;&#21516;&#26102;&#65292;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#38754;&#37096;&#35270;&#39057;&#12290;</title><link>http://arxiv.org/abs/2304.14471</link><description>&lt;p&gt;
&#20855;&#26377;&#35821;&#20041;&#24863;&#30693;&#20808;&#39564;&#30340;&#21487;&#25511;&#21333;&#27425;&#20154;&#33080;&#35270;&#39057;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Controllable One-Shot Face Video Synthesis With Semantic Aware Prior. (arXiv:2304.14471v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14471
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#20855;&#26377;&#35821;&#20041;&#24863;&#30693;&#20808;&#39564;&#30340;&#21487;&#25511;&#21333;&#27425;&#20154;&#33080;&#35270;&#39057;&#21512;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#21487;&#38752;&#30340;&#38754;&#37096;&#20998;&#21106;&#21644;&#26032;&#39062;&#30340;&#35821;&#20041;&#24863;&#30693;&#36816;&#21160;&#25197;&#26354;&#26041;&#26696;&#65292;&#22312;&#20445;&#35777;&#35821;&#20041;&#20934;&#30830;&#21644;&#23454;&#29616;&#30495;&#23454;&#36816;&#21160;&#30340;&#21516;&#26102;&#65292;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#38754;&#37096;&#35270;&#39057;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#27425;&#22836;&#20687;&#21512;&#25104;&#20219;&#21153;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#20174;&#39550;&#39542;&#24103;&#20013;&#23398;&#20064;&#30340;&#31232;&#30095;&#20851;&#38190;&#28857;&#20272;&#35745;&#30340;&#36816;&#21160;&#22330;&#26469;&#25197;&#26354;&#20174;&#28304;&#25552;&#21462;&#30340;&#22806;&#35266;&#29305;&#24449;&#65292;&#20174;&#32780;&#23558;&#28304;&#22270;&#20687;&#21160;&#30011;&#21270;&#20026;&#21478;&#19968;&#31181;&#23039;&#21183;&#21644;&#34920;&#24773;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#38480;&#21046;&#65306;1&#65289;&#22312;&#22836;&#37096;&#23039;&#21183;&#36739;&#22823;&#26102;&#21512;&#25104;&#36136;&#37327;&#19981;&#20339;&#65292;&#28304;&#22270;&#20687;&#21644;&#39550;&#39542;&#35270;&#39057;&#20013;&#30340;&#31532;&#19968;&#24103;&#20043;&#38388;&#23384;&#22312;&#21487;&#35266;&#30340;&#23039;&#21183;&#38169;&#37197;&#65307;2&#65289;&#30001;&#20110;&#32570;&#20047;&#35821;&#20041;&#29702;&#35299;&#21644;&#36866;&#24403;&#30340;&#33080;&#37096;&#20960;&#20309;&#27491;&#21017;&#21270;&#65292;&#22240;&#27492;&#26080;&#27861;&#25429;&#25417;&#31934;&#32454;&#20294;&#20851;&#38190;&#30340;&#38754;&#37096;&#36816;&#21160;&#32454;&#33410;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#32570;&#38519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20016;&#23500;&#30340;&#38754;&#37096;&#20808;&#39564;&#20449;&#24687;&#65292;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#25913;&#36827;&#35821;&#20041;&#19968;&#33268;&#24615;&#21644;&#36924;&#30495;&#36816;&#21160;&#32454;&#33410;&#30340;&#38754;&#37096;&#35270;&#39057;&#12290;
&lt;/p&gt;
&lt;p&gt;
The one-shot talking-head synthesis task aims to animate a source image to another pose and expression, which is dictated by a driving frame. Recent methods rely on warping the appearance feature extracted from the source, by using motion fields estimated from the sparse keypoints, that are learned in an unsupervised manner. Due to their lightweight formulation, they are suitable for video conferencing with reduced bandwidth. However, based on our study, current methods suffer from two major limitations: 1) unsatisfactory generation quality in the case of large head poses and the existence of observable pose misalignment between the source and the first frame in driving videos. 2) fail to capture fine yet critical face motion details due to the lack of semantic understanding and appropriate face geometry regularization. To address these shortcomings, we propose a novel method that leverages the rich face prior information, the proposed model can generate face videos with improved seman
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Moccasin&#30340;&#26032;&#22411;&#32422;&#26463;&#32534;&#31243;&#24418;&#24335;&#65292;&#29992;&#20110;&#23454;&#29616;&#22312;&#20869;&#23384;&#39044;&#31639;&#19979;&#26368;&#23567;&#21270;&#35745;&#31639;&#22270;&#30340;&#25191;&#34892;&#26102;&#38388;&#65292;&#30456;&#36739;&#20110;&#26368;&#36817;&#30340;&#30740;&#31350;&#65292;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#25928;&#29575;&#65292;&#24182;&#25104;&#21151;&#24212;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#39640;&#25928;&#24352;&#37327;&#37325;&#31639;&#12290;</title><link>http://arxiv.org/abs/2304.14463</link><description>&lt;p&gt;
Moccasin&#65306;&#31070;&#32463;&#32593;&#32476;&#30340;&#39640;&#25928;&#24352;&#37327;&#37325;&#31639;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Moccasin: Efficient Tensor Rematerialization for Neural Networks. (arXiv:2304.14463v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14463
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Moccasin&#30340;&#26032;&#22411;&#32422;&#26463;&#32534;&#31243;&#24418;&#24335;&#65292;&#29992;&#20110;&#23454;&#29616;&#22312;&#20869;&#23384;&#39044;&#31639;&#19979;&#26368;&#23567;&#21270;&#35745;&#31639;&#22270;&#30340;&#25191;&#34892;&#26102;&#38388;&#65292;&#30456;&#36739;&#20110;&#26368;&#36817;&#30340;&#30740;&#31350;&#65292;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#25928;&#29575;&#65292;&#24182;&#25104;&#21151;&#24212;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#39640;&#25928;&#24352;&#37327;&#37325;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36793;&#32536;&#35745;&#31639;&#35774;&#22791;&#19978;&#37096;&#32626;&#21644;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#38754;&#20020;&#35768;&#22810;&#25361;&#25112;&#65292;&#20854;&#20013;&#36739;&#20302;&#30340;&#20869;&#23384;&#26159;&#37096;&#32626;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#26102;&#32463;&#24120;&#36935;&#21040;&#30340;&#26368;&#22823;&#38480;&#21046;&#22240;&#32032;&#20043;&#19968;&#12290;&#24352;&#37327;&#37325;&#31639;&#26159;&#35299;&#20915;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#21644;&#25512;&#29702;&#25152;&#38656;&#39640;&#20869;&#23384;&#38656;&#27714;&#30340;&#19968;&#31181;&#26041;&#24335;&#12290;&#26412;&#25991;&#32771;&#34385;&#22312;&#20869;&#23384;&#39044;&#31639;&#19979;&#26368;&#23567;&#21270;&#35745;&#31639;&#22270;&#30340;&#25191;&#34892;&#26102;&#38388;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#32422;&#26463;&#32534;&#31243;&#24418;&#24335;&#65292;&#31216;&#20026;Moccasin&#65292;&#20854;&#20013;&#21482;&#26377;$O(n)$&#20010;&#25972;&#25968;&#21464;&#37327;&#65292;$n$&#26159;&#35745;&#31639;&#22270;&#20013;&#33410;&#28857;&#30340;&#25968;&#37327;&#12290;&#36825;&#30456;&#23545;&#20110;&#26368;&#36817;&#25991;&#29486;&#20013;&#25552;&#20986;&#30340;&#20855;&#26377;$O(n^2)$&#24067;&#23572;&#21464;&#37327;&#30340;&#20844;&#24335;&#25552;&#20986;&#20102;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25968;&#20540;&#30740;&#31350;&#32467;&#26524;&#65292;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22823;&#35268;&#27169;&#22270;&#19978;&#27604;&#26368;&#36817;&#30340;&#24037;&#20316;&#24555;&#19968;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
The deployment and training of neural networks on edge computing devices pose many challenges. The low memory nature of edge devices is often one of the biggest limiting factors encountered in the deployment of large neural network models. Tensor rematerialization or recompute is a way to address high memory requirements for neural network training and inference. In this paper we consider the problem of execution time minimization of compute graphs subject to a memory budget. In particular, we develop a new constraint programming formulation called \textsc{Moccasin} with only $O(n)$ integer variables, where $n$ is the number of nodes in the compute graph. This is a significant improvement over the works in the recent literature that propose formulations with $O(n^2)$ Boolean variables. We present numerical studies that show that our approach is up to an order of magnitude faster than recent work especially for large-scale graphs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#30740;&#20102;&#20351;&#29992;&#32463;&#20856;&#21644;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#38450;&#24481;XSS&#21644;CSRF&#30340;&#30740;&#31350;&#65292;&#24182;&#24635;&#32467;&#20986;&#20851;&#38190;&#35201;&#28857;&#65292;&#20026;&#25506;&#35752;&#35813;&#30740;&#31350;&#26041;&#21521;&#25552;&#20379;&#20102;&#21442;&#32771;&#12290;</title><link>http://arxiv.org/abs/2304.14451</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#29992;&#20110;&#26816;&#27979;&#21644;&#32531;&#35299;Web&#28431;&#27934;&#21644;Web&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Machine Learning for Detection and Mitigation of Web Vulnerabilities and Web Attacks. (arXiv:2304.14451v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14451
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#30740;&#20102;&#20351;&#29992;&#32463;&#20856;&#21644;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#38450;&#24481;XSS&#21644;CSRF&#30340;&#30740;&#31350;&#65292;&#24182;&#24635;&#32467;&#20986;&#20851;&#38190;&#35201;&#28857;&#65292;&#20026;&#25506;&#35752;&#35813;&#30740;&#31350;&#26041;&#21521;&#25552;&#20379;&#20102;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;Web&#23433;&#20840;&#39046;&#22495;&#20013;&#65292;&#26816;&#27979;&#21644;&#32531;&#35299;&#36328;&#31449;&#33050;&#26412;&#65288;XSS&#65289;&#21644;&#36328;&#31449;&#35831;&#27714;&#20266;&#36896;&#65288;CSRF&#65289;&#31561;&#20851;&#38190;Web&#28431;&#27934;&#21644;&#25915;&#20987;&#19968;&#30452;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#12290;&#36825;&#20123;Web&#25915;&#20987;&#19981;&#26029;&#28436;&#21464;&#65292;&#36234;&#26469;&#36234;&#38590;&#20197;&#26816;&#27979;&#12290;&#36817;&#24180;&#26469;&#65292;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#24320;&#22987;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#26469;&#38450;&#24481;XSS&#21644;CSRF&#65292;&#30001;&#20110;&#21462;&#24471;&#30340;&#31215;&#26497;&#32467;&#26524;&#65292;&#21487;&#20197;&#24471;&#20986;&#32467;&#35770;&#65292;&#36825;&#26159;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#31616;&#35201;&#20171;&#32461;&#24050;&#32463;&#21457;&#34920;&#30340;&#30740;&#31350;&#24037;&#20316;&#65292;&#36825;&#20123;&#30740;&#31350;&#24037;&#20316;&#37319;&#29992;&#32463;&#20856;&#21644;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#26469;&#35782;&#21035;&#21644;&#39044;&#38450;XSS&#21644;CSRF&#12290;&#25552;&#20379;&#36825;&#20221;&#35843;&#26597;&#30340;&#30446;&#30340;&#26159;&#20026;&#20102;&#25506;&#35752;&#24050;&#32463;&#23454;&#26045;&#30340;&#19981;&#21516;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20102;&#35299;&#20854;&#20013;&#30340;&#20851;&#38190;&#35201;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detection and mitigation of critical web vulnerabilities and attacks like cross-site scripting (XSS), and cross-site request forgery (CSRF) have been a great concern in the field of web security. Such web attacks are evolving and becoming more challenging to detect. Several ideas from different perspectives have been put forth that can be used to improve the performance of detecting these web vulnerabilities and preventing the attacks from happening. Machine learning techniques have lately been used by researchers to defend against XSS and CSRF, and given the positive findings, it can be concluded that it is a promising research direction. The objective of this paper is to briefly report on the research works that have been published in this direction of applying classical and advanced machine learning to identify and prevent XSS and CSRF. The purpose of providing this survey is to address different machine learning approaches that have been implemented, understand the key takeaway of 
&lt;/p&gt;</description></item><item><title>&#35813;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#26080;&#30417;&#30563;&#26041;&#24335;&#36827;&#34892;&#40065;&#26834;&#30340;&#19977;&#32500;&#24418;&#29366;&#21305;&#37197;&#65292;&#21487;&#20197;&#33719;&#24471;&#19981;&#21516;&#22330;&#26223;&#19979;&#30340;&#20934;&#30830;&#23545;&#24212;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2304.14419</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#23398;&#20064;&#40065;&#26834;&#24615;&#35889;&#24418;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Learning of Robust Spectral Shape Matching. (arXiv:2304.14419v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14419
&lt;/p&gt;
&lt;p&gt;
&#35813;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#26080;&#30417;&#30563;&#26041;&#24335;&#36827;&#34892;&#40065;&#26834;&#30340;&#19977;&#32500;&#24418;&#29366;&#21305;&#37197;&#65292;&#21487;&#20197;&#33719;&#24471;&#19981;&#21516;&#22330;&#26223;&#19979;&#30340;&#20934;&#30830;&#23545;&#24212;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#26041;&#27861;&#26469;&#36827;&#34892;&#40065;&#26834;&#30340;&#19977;&#32500;&#24418;&#29366;&#21305;&#37197;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#28145;&#24230;&#20989;&#25968;&#26144;&#23556;&#65292;&#24182;&#21487;&#20197;&#23436;&#20840;&#26080;&#30417;&#30563;&#22320;&#36827;&#34892;&#35757;&#32451;&#12290;&#20043;&#21069;&#30340;&#28145;&#24230;&#20989;&#25968;&#26144;&#23556;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#39044;&#27979;&#26368;&#20248;&#21270;&#20989;&#25968;&#26144;&#23556;&#19978;&#65292;&#28982;&#21518;&#20381;&#38752;&#29616;&#25104;&#30340;&#21518;&#26399;&#22788;&#29702;&#26469;&#33719;&#24471;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#20934;&#30830;&#30340;&#28857;&#26144;&#23556;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#20004;&#38454;&#27573;&#36807;&#31243;&#36890;&#24120;&#20250;&#20135;&#29983;&#27425;&#20248;&#24615;&#33021;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#25439;&#22833;&#65292;&#21033;&#29992;&#20989;&#25968;&#26144;&#23556;&#21644;&#28857;&#26144;&#23556;&#20043;&#38388;&#20851;&#31995;&#30340;&#26368;&#26032;&#35265;&#35299;&#65292;&#20174;&#32780;&#30452;&#25509;&#33719;&#24471;&#28857;&#26144;&#23556;&#32780;&#26080;&#38656;&#20219;&#20309;&#21518;&#26399;&#22788;&#29702;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#21487;&#20197;&#33719;&#24471;&#36817;&#31561;&#36317;&#24418;&#29366;&#30340;&#20934;&#30830;&#23545;&#24212;&#20851;&#31995;&#65292;&#36824;&#21487;&#20197;&#33719;&#24471;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#38750;&#31561;&#36317;&#24418;&#29366;&#21644;&#37096;&#20998;&#24418;&#29366;&#65292;&#20197;&#21450;&#23384;&#22312;&#19981;&#21516;&#31163;&#25955;&#21270;&#25110;&#25299;&#25169;&#22122;&#22768;&#30340;&#24418;&#29366;&#30340;&#20934;&#30830;&#23545;&#24212;&#20851;&#31995;&#12290;&#20351;&#29992;&#24635;&#20849;&#20061;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19981;&#21516;&#22330;&#26223;&#19979;&#24471;&#21040;&#20102;&#24456;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel learning-based approach for robust 3D shape matching. Our method builds upon deep functional maps and can be trained in a fully unsupervised manner. Previous deep functional map methods mainly focus on predicting optimised functional maps alone, and then rely on off-the-shelf post-processing to obtain accurate point-wise maps during inference. However, this two-stage procedure for obtaining point-wise maps often yields sub-optimal performance. In contrast, building upon recent insights about the relation between functional maps and point-wise maps, we propose a novel unsupervised loss to couple the functional maps and point-wise maps, and thereby directly obtain point-wise maps without any post-processing. Our approach obtains accurate correspondences not only for near-isometric shapes, but also for more challenging non-isometric shapes and partial shapes, as well as shapes with different discretisation or topological noise. Using a total of nine diverse datasets, we
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#23398;&#26415;&#30028;&#20351;&#29992;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;ChatGPT&#30340;&#24433;&#21709;&#65292;&#26088;&#22312;&#20102;&#35299;&#20854;&#22914;&#20309;&#38761;&#26032;&#24037;&#31243;&#25945;&#32946;&#24182;&#25913;&#21464;&#25216;&#26415;&#12289;&#25945;&#32844;&#24037;&#19982;&#23398;&#29983;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#35843;&#26597;&#21487;&#20379;&#20854;&#20182;&#22823;&#23398;&#21644;&#26426;&#26500;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2304.14415</link><description>&lt;p&gt;
&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#30693;&#35273;&#65306;&#20851;&#20110;&#23398;&#26415;&#30028;&#20351;&#29992;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#30340;&#25945;&#32844;&#24037;&#21644;&#23398;&#29983;&#30475;&#27861;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Generative AI Perceptions: A Survey to Measure the Perceptions of Faculty, Staff, and Students on Generative AI Tools in Academia. (arXiv:2304.14415v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14415
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#23398;&#26415;&#30028;&#20351;&#29992;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;ChatGPT&#30340;&#24433;&#21709;&#65292;&#26088;&#22312;&#20102;&#35299;&#20854;&#22914;&#20309;&#38761;&#26032;&#24037;&#31243;&#25945;&#32946;&#24182;&#25913;&#21464;&#25216;&#26415;&#12289;&#25945;&#32844;&#24037;&#19982;&#23398;&#29983;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#35843;&#26597;&#21487;&#20379;&#20854;&#20182;&#22823;&#23398;&#21644;&#26426;&#26500;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;&#19968;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24037;&#20855;&#65292;&#21487;&#20197;&#36827;&#34892;&#31867;&#20284;&#20154;&#31867;&#23545;&#35805;&#30340;&#20132;&#20114;&#65292;&#24182;&#33021;&#22815;&#26681;&#25454;&#19981;&#21516;&#30340;&#25552;&#31034;&#29983;&#25104;&#36830;&#36143;&#19988;&#30456;&#20851;&#30340;&#22238;&#22797;&#12290;&#35813;&#24037;&#20855;&#33021;&#22815;&#29702;&#35299;&#29992;&#25143;&#36755;&#20837;&#30340;&#33258;&#28982;&#25991;&#26412;&#65292;&#24182;&#20197;&#22810;&#31181;&#24418;&#24335;&#29983;&#25104;&#36866;&#24403;&#30340;&#21709;&#24212;&#12290;&#26412;&#25991;&#37325;&#28857;&#25506;&#35752;&#20102;ChatGPT&#22914;&#20309;&#38761;&#26032;&#24037;&#31243;&#25945;&#32946;&#65292;&#20197;&#21450;&#25216;&#26415;&#12289;&#23398;&#29983;&#12289;&#25945;&#32844;&#24037;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#22240;&#20026;&#35813;&#24037;&#20855;&#24555;&#36895;&#21464;&#21270;&#21644;&#19981;&#26029;&#25913;&#36827;&#65292;&#22240;&#27492;&#29616;&#22312;&#26159;&#25910;&#38598;&#30456;&#20851;&#25968;&#25454;&#30340;&#20851;&#38190;&#26102;&#26399;&#12290;&#20026;&#27492;&#65292;&#35774;&#35745;&#20102;&#19968;&#39033;&#35843;&#26597;&#65292;&#20197;&#34913;&#37327;ChatGPT&#23545;&#25945;&#32844;&#24037;&#21644;&#23398;&#29983;&#30340;&#24433;&#21709;&#65292;&#24182;&#23558;&#35813;&#35843;&#26597;&#20316;&#20026;&#24503;&#20811;&#33832;&#26031;A&amp;M&#22823;&#23398;&#25216;&#26415;&#25253;&#21578;&#20998;&#20139;&#65292;&#20197;&#20415;&#20854;&#20182;&#22823;&#23398;&#21644;&#26426;&#26500;&#20351;&#29992;&#35813;&#35843;&#26597;&#65292;&#24182;&#22312;&#20854;&#20182;&#22320;&#26041;&#36827;&#34892;&#24433;&#21709;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT is a natural language processing tool that can engage in human-like conversations and generate coherent and contextually relevant responses to various prompts. ChatGPT is capable of understanding natural text that is input by a user and generating appropriate responses in various forms. This tool represents a major step in how humans are interacting with technology. This paper specifically focuses on how ChatGPT is revolutionizing the realm of engineering education and the relationship between technology, students, and faculty and staff. Because this tool is quickly changing and improving with the potential for even greater future capability, it is a critical time to collect pertinent data. A survey was created to measure the effects of ChatGPT on students, faculty, and staff. This survey is shared as a Texas A&amp;M University technical report to allow other universities and entities to use this survey and measure the effects elsewhere.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#27627;&#31859;&#27874;&#38647;&#36798;&#31232;&#30095;&#28857;&#20113;&#19978;&#36827;&#34892;&#20154;&#31867;&#35821;&#20041;&#20998;&#21106;&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20248;&#20110;&#30456;&#26426;&#21644;&#28608;&#20809;&#38647;&#36798;&#30340;&#38544;&#31169;&#20445;&#25252;&#21644;&#25239;&#24178;&#25200;&#33021;&#21147;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#24341;&#20837;&#22270;&#32467;&#26500;&#21644;&#25299;&#25169;&#29305;&#24449;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#21547;&#26377;&#20840;&#23616;&#29305;&#24449;&#27169;&#22359;&#21644;&#39034;&#24207;&#29305;&#24449;&#27169;&#22359;&#30340;&#35821;&#20041;&#20998;&#21106;&#26694;&#26550;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#31232;&#30095;&#21644;&#26102;&#38388;&#25299;&#25169;&#29305;&#24449;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.14132</link><description>&lt;p&gt;
&#20351;&#29992;&#27627;&#31859;&#27874;&#38647;&#36798;&#31232;&#30095;&#28857;&#20113;&#36827;&#34892;&#20154;&#31867;&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Human Semantic Segmentation using Millimeter-Wave Radar Sparse Point Clouds. (arXiv:2304.14132v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14132
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#27627;&#31859;&#27874;&#38647;&#36798;&#31232;&#30095;&#28857;&#20113;&#19978;&#36827;&#34892;&#20154;&#31867;&#35821;&#20041;&#20998;&#21106;&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20248;&#20110;&#30456;&#26426;&#21644;&#28608;&#20809;&#38647;&#36798;&#30340;&#38544;&#31169;&#20445;&#25252;&#21644;&#25239;&#24178;&#25200;&#33021;&#21147;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#24341;&#20837;&#22270;&#32467;&#26500;&#21644;&#25299;&#25169;&#29305;&#24449;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#21547;&#26377;&#20840;&#23616;&#29305;&#24449;&#27169;&#22359;&#21644;&#39034;&#24207;&#29305;&#24449;&#27169;&#22359;&#30340;&#35821;&#20041;&#20998;&#21106;&#26694;&#26550;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#31232;&#30095;&#21644;&#26102;&#38388;&#25299;&#25169;&#29305;&#24449;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#27627;&#31859;&#27874;&#38647;&#36798;&#31232;&#30095;&#26102;&#24207;&#28857;&#20113;&#19978;&#36827;&#34892;&#35821;&#20041;&#20998;&#21106;&#30340;&#26694;&#26550;&#12290;&#30456;&#27604;&#30456;&#26426;&#21644;&#28608;&#20809;&#38647;&#36798;&#65292;&#27627;&#31859;&#27874;&#38647;&#36798;&#20855;&#26377;&#19981;&#27844;&#38706;&#38544;&#31169;&#12289;&#24378;&#25239;&#24178;&#25200;&#33021;&#21147;&#21644;&#38271;&#26816;&#27979;&#36317;&#31163;&#30340;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#27627;&#31859;&#27874;&#25968;&#25454;&#30340;&#31232;&#30095;&#21644;&#26102;&#38388;&#25299;&#25169;&#29305;&#24449;&#30340;&#25429;&#33719;&#20173;&#28982;&#26159;&#19968;&#20010;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#20154;&#31867;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#20013;&#65292;&#20197;&#21069;&#30340;&#20808;&#36827;&#20998;&#21106;&#26041;&#27861; (&#22914;PointNet&#12289;PointCNN&#12289;Point Transformer) &#27809;&#26377;&#34987;&#20805;&#20998;&#24212;&#29992;&#20110;&#23454;&#38469;&#22330;&#26223;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a framework for semantic segmentation on sparse sequential point clouds of millimeter-wave radar. Compared with cameras and lidars, millimeter-wave radars have the advantage of not revealing privacy, having a strong anti-interference ability, and having long detection distance. The sparsity and capturing temporal-topological features of mmWave data is still a problem. However, the issue of capturing the temporal-topological coupling features under the human semantic segmentation task prevents previous advanced segmentation methods (e.g PointNet, PointCNN, Point Transformer) from being well utilized in practical scenarios. To address the challenge caused by the sparsity and temporal-topological feature of the data, we (i) introduce graph structure and topological features to the point cloud, (ii) propose a semantic segmentation framework including a global feature-extracting module and a sequential feature-extracting module. In addition, we design an efficient and mo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#20351;&#29992;GPT-3.5&#21644;GPT-4&#35299;&#20915;&#21307;&#23398;&#38382;&#39064;&#30340;&#23433;&#20840;&#24615;&#20197;&#21450;&#19982;&#20449;&#24687;&#25216;&#26415;&#21672;&#35810;&#26381;&#21153;&#25253;&#21578;&#30340;&#19968;&#33268;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20004;&#20010;LLMs&#37117;&#21487;&#20197;&#20197;&#23433;&#20840;&#21644;&#19968;&#33268;&#30340;&#26041;&#24335;&#28385;&#36275;&#21307;&#29983;&#30340;&#20449;&#24687;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2304.13714</link><description>&lt;p&gt;
&#35780;&#20272;GPT-3.5&#21644;GPT-4&#22312;&#25903;&#25345;&#21307;&#30103;&#20445;&#20581;&#20449;&#24687;&#38656;&#27714;&#26041;&#38754;&#30340;&#23454;&#38469;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Evaluation of GPT-3.5 and GPT-4 for supporting real-world information needs in healthcare delivery. (arXiv:2304.13714v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13714
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#20351;&#29992;GPT-3.5&#21644;GPT-4&#35299;&#20915;&#21307;&#23398;&#38382;&#39064;&#30340;&#23433;&#20840;&#24615;&#20197;&#21450;&#19982;&#20449;&#24687;&#25216;&#26415;&#21672;&#35810;&#26381;&#21153;&#25253;&#21578;&#30340;&#19968;&#33268;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20004;&#20010;LLMs&#37117;&#21487;&#20197;&#20197;&#23433;&#20840;&#21644;&#19968;&#33268;&#30340;&#26041;&#24335;&#28385;&#36275;&#21307;&#29983;&#30340;&#20449;&#24687;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#65292;&#20294;&#24403;&#21069;&#30340;&#25506;&#32034;&#24182;&#26410;&#35780;&#20272;LLMs&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#30340;&#23454;&#29992;&#24615;&#21644;&#23433;&#20840;&#24615;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#30830;&#23450;&#20004;&#20010;LLM&#26159;&#21542;&#21487;&#20197;&#20197;&#23433;&#20840;&#21644;&#19968;&#33268;&#30340;&#26041;&#24335;&#28385;&#36275;&#30001;&#21307;&#29983;&#25552;&#20132;&#30340;&#20449;&#24687;&#38656;&#27714;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;66&#20010;&#26469;&#33258;&#20449;&#24687;&#25216;&#26415;&#21672;&#35810;&#26381;&#21153;&#30340;&#38382;&#39064;&#36890;&#36807;&#31616;&#21333;&#30340;&#25552;&#31034;&#25552;&#20132;&#32473;GPT-3.5&#21644;GPT-4&#12290;12&#21517;&#21307;&#29983;&#35780;&#20272;&#20102;LLM&#21709;&#24212;&#23545;&#24739;&#32773;&#36896;&#25104;&#20260;&#23475;&#30340;&#21487;&#33021;&#24615;&#20197;&#21450;&#19982;&#20449;&#24687;&#25216;&#26415;&#21672;&#35810;&#26381;&#21153;&#30340;&#29616;&#26377;&#25253;&#21578;&#30340;&#19968;&#33268;&#24615;&#12290;&#21307;&#29983;&#30340;&#35780;&#20272;&#22522;&#20110;&#22810;&#25968;&#31080;&#27719;&#24635;&#12290;&#23545;&#20110;&#27809;&#26377;&#20219;&#20309;&#38382;&#39064;&#65292;&#22823;&#22810;&#25968;&#21307;&#29983;&#35748;&#20026;&#20219;&#20309;&#19968;&#20010;LLM&#21709;&#24212;&#37117;&#19981;&#20250;&#36896;&#25104;&#20260;&#23475;&#12290;&#23545;&#20110;GPT-3.5&#65292;8&#20010;&#38382;&#39064;&#30340;&#21709;&#24212;&#19982;&#20449;&#24687;&#25216;&#26415;&#21672;&#35810;&#25253;&#21578;&#19968;&#33268;&#65292;20&#20010;&#19981;&#19968;&#33268;&#65292;9&#20010;&#26080;&#27861;&#35780;&#20272;&#12290;&#26377;29&#20010;&#21709;&#24212;&#27809;&#26377;&#22810;&#25968;&#31080;&#34920;&#31034;&#8220;&#21516;&#24847;&#8221;&#12289;&#8220;&#19981;&#21516;&#24847;&#8221;&#21644;&#8220;&#26080;&#27861;&#35780;&#20272;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite growing interest in using large language models (LLMs) in healthcare, current explorations do not assess the real-world utility and safety of LLMs in clinical settings. Our objective was to determine whether two LLMs can serve information needs submitted by physicians as questions to an informatics consultation service in a safe and concordant manner. Sixty six questions from an informatics consult service were submitted to GPT-3.5 and GPT-4 via simple prompts. 12 physicians assessed the LLM responses' possibility of patient harm and concordance with existing reports from an informatics consultation service. Physician assessments were summarized based on majority vote. For no questions did a majority of physicians deem either LLM response as harmful. For GPT-3.5, responses to 8 questions were concordant with the informatics consult report, 20 discordant, and 9 were unable to be assessed. There were 29 responses with no majority on "Agree", "Disagree", and "Unable to assess". Fo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CaLiCa&#30340;&#31471;&#21040;&#31471;&#28145;&#24230;&#33258;&#26631;&#23450;&#32593;&#32476;&#65292;&#29992;&#20110;&#32852;&#21512;&#33258;&#21160;&#26657;&#20934;&#38024;&#23380;&#30456;&#26426;&#21644;&#28608;&#20809;&#38647;&#36798;&#30340;&#22266;&#26377;&#21644;&#22806;&#21442;&#21442;&#25968;&#20197;&#30830;&#20445;&#36710;&#36742;&#22810;&#27169;&#24335;&#24863;&#30693;&#20256;&#24863;&#22120;&#30340;&#26657;&#20934;&#36136;&#37327;&#65292;&#21516;&#26102;&#37319;&#29992;&#23402;&#29983;&#32467;&#26500;&#20197;&#36798;&#21040;&#39046;&#22495;&#20849;&#20139;&#29305;&#24449;&#30340;&#30446;&#30340;&#12290;</title><link>http://arxiv.org/abs/2304.12412</link><description>&lt;p&gt;
&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#28608;&#20809;&#38647;&#36798;-&#30456;&#26426;&#31471;&#21040;&#31471;&#33258;&#26631;&#23450;
&lt;/p&gt;
&lt;p&gt;
End-to-End Lidar-Camera Self-Calibration for Autonomous Vehicles. (arXiv:2304.12412v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12412
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CaLiCa&#30340;&#31471;&#21040;&#31471;&#28145;&#24230;&#33258;&#26631;&#23450;&#32593;&#32476;&#65292;&#29992;&#20110;&#32852;&#21512;&#33258;&#21160;&#26657;&#20934;&#38024;&#23380;&#30456;&#26426;&#21644;&#28608;&#20809;&#38647;&#36798;&#30340;&#22266;&#26377;&#21644;&#22806;&#21442;&#21442;&#25968;&#20197;&#30830;&#20445;&#36710;&#36742;&#22810;&#27169;&#24335;&#24863;&#30693;&#20256;&#24863;&#22120;&#30340;&#26657;&#20934;&#36136;&#37327;&#65292;&#21516;&#26102;&#37319;&#29992;&#23402;&#29983;&#32467;&#26500;&#20197;&#36798;&#21040;&#39046;&#22495;&#20849;&#20139;&#29305;&#24449;&#30340;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#37197;&#22791;&#20102;&#22810;&#27169;&#24335;&#24863;&#30693;&#20256;&#24863;&#22120;&#65292;&#20197;&#30830;&#20445;&#27773;&#36710;&#23433;&#20840;&#34892;&#39542;&#12290;&#20294;&#26159;&#22914;&#20309;&#22312;&#27773;&#36710;&#36816;&#34892;&#26399;&#38388;&#20445;&#25345;&#20256;&#24863;&#22120;&#30340;&#26657;&#20934;&#36136;&#37327;&#25104;&#20026;&#19968;&#20010;&#26377;&#36259;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#22914;&#20309;&#32852;&#21512;&#26657;&#20934;&#22810;&#20010;&#20256;&#24863;&#22120;&#20197;&#30830;&#20445;&#31995;&#32479;&#35823;&#24046;&#19981;&#20250;&#20256;&#25773;&#20063;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;CaLiCa&#30340;&#31471;&#21040;&#31471;&#28145;&#24230;&#33258;&#26631;&#23450;&#32593;&#32476;&#65292;&#38024;&#23545;&#38024;&#23380;&#30456;&#26426;&#21644;&#28608;&#20809;&#38647;&#36798;&#30340;&#33258;&#21160;&#26657;&#20934;&#38382;&#39064;&#20570;&#20986;&#20102;&#25913;&#36827;&#12290;&#25105;&#20204;&#36890;&#36807;&#22238;&#24402;&#30456;&#26426;&#22270;&#20687;&#21644;&#28608;&#20809;&#28857;&#20113;&#20043;&#38388;&#30340;&#29305;&#24449;&#30456;&#20851;&#24615;&#65292;&#32852;&#21512;&#39044;&#27979;&#30456;&#26426;&#22266;&#26377;&#21442;&#25968;(&#28966;&#36317;&#21644;&#30072;&#21464;)&#20197;&#21450;&#28608;&#20809;&#38647;&#36798;-&#30456;&#26426;&#22806;&#21442;&#21442;&#25968;(&#26059;&#36716;&#21644;&#24179;&#31227;)&#12290;&#32593;&#32476;&#37319;&#29992;&#23402;&#29983;&#32467;&#26500;&#23433;&#25490;&#20197;&#23558;&#32593;&#32476;&#29305;&#24449;&#23398;&#20064;&#32422;&#26463;&#22312;&#28857;&#20113;&#21644;&#30456;&#26426;&#22270;&#20687;&#39046;&#22495;&#30340;&#20849;&#20139;&#29305;&#24449;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous vehicles are equipped with a multi-modal sensor setup to enable the car to drive safely. The initial calibration of such perception sensors is a highly matured topic and is routinely done in an automated factory environment. However, an intriguing question arises on how to maintain the calibration quality throughout the vehicle's operating duration. Another challenge is to calibrate multiple sensors jointly to ensure no propagation of systemic errors. In this paper, we propose CaLiCa, an end-to-end deep self-calibration network which addresses the automatic calibration problem for pinhole camera and Lidar. We jointly predict the camera intrinsic parameters (focal length and distortion) as well as Lidar-Camera extrinsic parameters (rotation and translation), by regressing feature correlation between the camera image and the Lidar point cloud. The network is arranged in a Siamese-twin structure to constrain the network features learning to a mutually shared feature in both poi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#23436;&#20840;&#21512;&#20316;&#35774;&#32622;&#19979;&#65292;&#35780;&#20272;&#20195;&#29702;&#20043;&#38388;&#22266;&#25191;&#31243;&#24230;&#30340;&#29615;&#22659;Stubborn&#65292;&#36890;&#36807;&#19968;&#20010;&#33021;&#22815;&#20307;&#29616;&#20154;&#31867;&#31038;&#20132;&#34892;&#20026;&#30340;&#25351;&#26631;&#26469;&#20419;&#36827;&#30740;&#31350;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#31038;&#20132;&#21160;&#24577;&#21644;&#22266;&#25191;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2304.12280</link><description>&lt;p&gt;
Stubborn&#65306;&#29992;&#20110;&#35780;&#20272;&#20855;&#26377;&#19968;&#33268;&#28608;&#21169;&#30340;&#20195;&#29702;&#20043;&#38388;&#30340;&#39037;&#22266;&#24230;&#30340;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
Stubborn: An Environment for Evaluating Stubbornness between Agents with Aligned Incentives. (arXiv:2304.12280v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12280
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#23436;&#20840;&#21512;&#20316;&#35774;&#32622;&#19979;&#65292;&#35780;&#20272;&#20195;&#29702;&#20043;&#38388;&#22266;&#25191;&#31243;&#24230;&#30340;&#29615;&#22659;Stubborn&#65292;&#36890;&#36807;&#19968;&#20010;&#33021;&#22815;&#20307;&#29616;&#20154;&#31867;&#31038;&#20132;&#34892;&#20026;&#30340;&#25351;&#26631;&#26469;&#20419;&#36827;&#30740;&#31350;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#31038;&#20132;&#21160;&#24577;&#21644;&#22266;&#25191;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22312;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#39046;&#22495;&#30340;&#30740;&#31350;&#24050;&#32463;&#22312;&#23398;&#20064;&#31038;&#20132;&#34892;&#20026;&#21644;&#21512;&#20316;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#28151;&#21512;&#21644;&#21338;&#24328;&#35774;&#32622;&#19979;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#31038;&#20132;&#22256;&#22659;&#24050;&#32463;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#65292;&#20294;&#22312;&#23436;&#20840;&#21512;&#20316;&#30340;&#35774;&#32622;&#19979;&#65292;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#31038;&#20132;&#22256;&#22659;&#30740;&#31350;&#36739;&#23569;&#12290;&#34429;&#28982;&#23436;&#20840;&#19968;&#33268;&#30340;&#21033;&#30410;&#26377;&#21161;&#20110;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#21512;&#20316;&#65292;&#24182;&#19988;&#21487;&#20197;&#24102;&#26469;&#22870;&#21169;&#65292;&#20294;&#24182;&#19981;&#20445;&#35777;&#21512;&#20316;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#34913;&#37327;&#26234;&#33021;&#20307;&#20043;&#38388;&#8220;&#22266;&#25191;&#8221;&#31243;&#24230;&#30340;&#25351;&#26631;&#65292;&#26088;&#22312;&#20307;&#29616;&#20854;&#21517;&#31216;&#25152;&#25551;&#36848;&#30340;&#20154;&#31867;&#31038;&#20132;&#34892;&#20026;&#65306;&#19968;&#20010;&#24930;&#24930;&#21319;&#32423;&#21644;&#28508;&#22312;&#28798;&#38590;&#24615;&#30340;&#20998;&#27495;&#12290;&#25105;&#20204;&#24076;&#26395;&#20419;&#36827;&#30740;&#31350;&#20851;&#20110;&#26234;&#33021;&#20307;&#22266;&#25191;&#30340;&#20542;&#21521;&#65292;&#23545;&#24212;&#26234;&#33021;&#20307;&#30340;&#21453;&#24212;&#20197;&#21450;&#30001;&#27492;&#20135;&#29983;&#30340;&#31038;&#20132;&#21160;&#24577;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Stubborn&#30340;&#29615;&#22659;&#65292;&#29992;&#20110;&#35780;&#20272;&#20855;&#26377;&#23436;&#20840;&#19968;&#33268;&#28608;&#21169;&#30340;&#20195;&#29702;&#20043;&#38388;&#30340;&#22266;&#25191;&#31243;&#24230;&#12290;&#22312;&#25105;&#20204;&#30340;&#21021;&#27493;&#32467;&#26524;&#20013;&#65292;&#26234;&#33021;&#20307;&#23398;&#20250;&#20102;&#20351;&#29992;&#20854;&#34892;&#20026;&#26469;&#31649;&#29702;&#20914;&#31361;&#65292;&#32780;&#19981;&#26159;&#36873;&#25321;&#20197;&#29306;&#29298;&#33258;&#24049;&#30340;&#26368;&#20339;&#21033;&#30410;&#26469;&#36798;&#25104;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research in multi-agent reinforcement learning (MARL) has shown success in learning social behavior and cooperation. Social dilemmas between agents in mixed-sum settings have been studied extensively, but there is little research into social dilemmas in fullycooperative settings, where agents have no prospect of gaining reward at another agent's expense.  While fully-aligned interests are conducive to cooperation between agents, they do not guarantee it. We propose a measure of "stubbornness" between agents that aims to capture the human social behavior from which it takes its name: a disagreement that is gradually escalating and potentially disastrous. We would like to promote research into the tendency of agents to be stubborn, the reactions of counterpart agents, and the resulting social dynamics.  In this paper we present Stubborn, an environment for evaluating stubbornness between agents with fully-aligned incentives. In our preliminary results, the agents learn to use thei
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#33258;&#25105;&#20013;&#24515;&#35760;&#24518;&#21644;&#25511;&#21046;&#30340;&#26694;&#26550;LLM-Brain&#65292;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#26426;&#22120;&#20154;&#22823;&#33041;&#36827;&#34892;&#38646;-shot&#23398;&#20064;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#23553;&#38381;&#24335;&#22810;&#36718;&#23545;&#35805;&#65292;&#35206;&#30422;&#20102;&#24863;&#30693;&#12289;&#35268;&#21010;&#12289;&#25511;&#21046;&#21644;&#35760;&#24518;&#65292;&#20855;&#26377;&#24456;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#36866;&#29992;&#20110;&#22810;&#20010;&#26426;&#22120;&#20154;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2304.09349</link><description>&lt;p&gt;
LLM&#20316;&#20026;&#26426;&#22120;&#20154;&#30340;&#22823;&#33041;&#65306;&#32479;&#19968;&#33258;&#25105;&#20013;&#24515;&#35760;&#24518;&#19982;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
LLM as A Robotic Brain: Unifying Egocentric Memory and Control. (arXiv:2304.09349v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09349
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#33258;&#25105;&#20013;&#24515;&#35760;&#24518;&#21644;&#25511;&#21046;&#30340;&#26694;&#26550;LLM-Brain&#65292;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#26426;&#22120;&#20154;&#22823;&#33041;&#36827;&#34892;&#38646;-shot&#23398;&#20064;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#23553;&#38381;&#24335;&#22810;&#36718;&#23545;&#35805;&#65292;&#35206;&#30422;&#20102;&#24863;&#30693;&#12289;&#35268;&#21010;&#12289;&#25511;&#21046;&#21644;&#35760;&#24518;&#65292;&#20855;&#26377;&#24456;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#36866;&#29992;&#20110;&#22810;&#20010;&#26426;&#22120;&#20154;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20307;&#24863;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#21644;&#24320;&#21457;&#20855;&#22791;&#29289;&#29702;&#25110;&#34394;&#25311;&#23454;&#20307;&#65288;&#21363;&#26426;&#22120;&#20154;&#65289;&#24182;&#33021;&#22815;&#19982;&#29615;&#22659;&#21160;&#24577;&#20132;&#20114;&#30340;&#26234;&#33021;&#31995;&#32479;&#12290;&#35760;&#24518;&#21644;&#25511;&#21046;&#26159;&#20307;&#24863;&#31995;&#32479;&#30340;&#20004;&#20010;&#22522;&#26412;&#37096;&#20998;&#65292;&#36890;&#24120;&#38656;&#35201;&#20998;&#21035;&#20351;&#29992;&#26694;&#26550;&#36827;&#34892;&#24314;&#27169;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#12289;&#21487;&#25512;&#24191;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;LLM-Brain&#65306;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#26426;&#22120;&#20154;&#22823;&#33041;&#65292;&#32479;&#19968;&#33258;&#25105;&#20013;&#24515;&#35760;&#24518;&#21644;&#25511;&#21046;&#12290;LLM-Brain&#26694;&#26550;&#38598;&#25104;&#20102;&#22810;&#20010;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#26426;&#22120;&#20154;&#20219;&#21153;&#65292;&#21033;&#29992;&#38646;-shot&#23398;&#20064;&#26041;&#27861;&#12290;LLM-Brain&#20013;&#30340;&#25152;&#26377;&#32452;&#20214;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#36827;&#34892;&#23553;&#38381;&#24335;&#22810;&#36718;&#23545;&#35805;&#65292;&#21253;&#25324;&#24863;&#30693;&#12289;&#35268;&#21010;&#12289;&#25511;&#21046;&#21644;&#35760;&#24518;&#12290;&#31995;&#32479;&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#20855;&#22791;&#33258;&#25105;&#20013;&#24515;&#35760;&#24518;&#21644;&#25511;&#21046;&#26426;&#22120;&#20154;&#30340;&#23454;&#20307;LLM&#12290;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#20004;&#20010;&#19979;&#28216;&#20219;&#21153;&#65306;&#20027;&#21160;&#25506;&#32034;&#21644;&#23454;&#20307;&#38382;&#31572;&#26469;&#28436;&#31034;LLM-Brain&#12290;
&lt;/p&gt;
&lt;p&gt;
Embodied AI focuses on the study and development of intelligent systems that possess a physical or virtual embodiment (i.e. robots) and are able to dynamically interact with their environment. Memory and control are the two essential parts of an embodied system and usually require separate frameworks to model each of them. In this paper, we propose a novel and generalizable framework called LLM-Brain: using Large-scale Language Model as a robotic brain to unify egocentric memory and control. The LLM-Brain framework integrates multiple multimodal language models for robotic tasks, utilizing a zero-shot learning approach. All components within LLM-Brain communicate using natural language in closed-loop multi-round dialogues that encompass perception, planning, control, and memory. The core of the system is an embodied LLM to maintain egocentric memory and control the robot. We demonstrate LLM-Brain by examining two downstream tasks: active exploration and embodied question answering. The
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#35270;&#35273;&#36741;&#21161;&#35745;&#21010;&#65288;VPA&#65289;&#30340;&#20219;&#21153;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#24207;&#21015;&#27169;&#22411;&#65292;&#22312;&#35270;&#39057;&#34892;&#21160;&#20998;&#21106;&#21644;&#39044;&#27979;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#65292;&#26469;&#23454;&#29616;&#22810;&#27169;&#24577;AI&#21161;&#25163;&#25351;&#23548;&#29992;&#25143;&#23436;&#25104;&#22797;&#26434;&#22810;&#27493;&#39588;&#30446;&#26631;&#30340;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2304.09179</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20154;&#31867;&#36741;&#21161;&#35270;&#35273;&#35745;&#21010;&#32773;
&lt;/p&gt;
&lt;p&gt;
Pretrained Language Models as Visual Planners for Human Assistance. (arXiv:2304.09179v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09179
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#35270;&#35273;&#36741;&#21161;&#35745;&#21010;&#65288;VPA&#65289;&#30340;&#20219;&#21153;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#24207;&#21015;&#27169;&#22411;&#65292;&#22312;&#35270;&#39057;&#34892;&#21160;&#20998;&#21106;&#21644;&#39044;&#27979;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#65292;&#26469;&#23454;&#29616;&#22810;&#27169;&#24577;AI&#21161;&#25163;&#25351;&#23548;&#29992;&#25143;&#23436;&#25104;&#22797;&#26434;&#22810;&#27493;&#39588;&#30446;&#26631;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#23454;&#29616;&#22810;&#27169;&#24577;AI&#21161;&#25163;&#25351;&#23548;&#29992;&#25143;&#23436;&#25104;&#22797;&#26434;&#22810;&#27493;&#39588;&#30446;&#26631;&#30340;&#36827;&#23637;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#35270;&#35273;&#36741;&#21161;&#35745;&#21010;&#65288;VPA&#65289;&#30340;&#20219;&#21153;&#12290;&#32473;&#23450;&#33258;&#28982;&#35821;&#35328;&#31616;&#35201;&#25551;&#36848;&#30340;&#30446;&#26631;&#65292;&#20363;&#22914;&#8220;&#21046;&#20316;&#20070;&#26550;&#8221;&#65292;&#20197;&#21450;&#29992;&#25143;&#36804;&#20170;&#20026;&#27490;&#30340;&#35270;&#39057;&#36827;&#23637;&#65292;VPA&#30340;&#30446;&#26631;&#26159;&#33719;&#24471;&#19968;&#20010;&#35745;&#21010;&#65292;&#21363;&#19968;&#31995;&#21015;&#34892;&#21160;&#65292;&#22914;&#8220;&#30722;&#20809;&#20070;&#26550;&#8221;&#12289;&#8220;&#28034;&#28422;&#20070;&#26550;&#8221;&#31561;&#65292;&#20197;&#23454;&#29616;&#30446;&#26631;&#12290;&#36825;&#38656;&#35201;&#35780;&#20272;&#29992;&#25143;&#22312;&#26410;&#32463;&#20462;&#21098;&#30340;&#35270;&#39057;&#20013;&#30340;&#36827;&#23637;&#65292;&#24182;&#19982;&#24213;&#23618;&#30446;&#26631;&#30340;&#35201;&#27714;&#30456;&#20851;&#32852;&#65292;&#21363;&#34892;&#21160;&#30340;&#30456;&#20851;&#24615;&#21644;&#20854;&#20013;&#30340;&#25490;&#24207;&#20381;&#36182;&#20851;&#31995;&#12290;&#22240;&#27492;&#65292;&#36825;&#38656;&#35201;&#22788;&#29702;&#38271;&#26102;&#38388;&#30340;&#35270;&#39057;&#21382;&#21490;&#35760;&#24405;&#21644;&#20219;&#24847;&#22797;&#26434;&#30340;&#34892;&#21160;&#20381;&#36182;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;VPA&#20998;&#35299;&#20026;&#35270;&#39057;&#34892;&#21160;&#20998;&#21106;&#21644;&#39044;&#27979;&#12290;&#25105;&#20204;&#23558;&#39044;&#27979;&#27493;&#39588;&#20844;&#24335;&#21270;&#20026;&#22810;&#27169;&#24577;&#24207;&#21015;&#24314;&#27169;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#35745;&#21010;&#32773;&#65288;VLaMP&#65289;&#65292;&#20854;&#20013;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;LMs&#20316;&#20026;&#24207;&#21015;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#65288;Epic Kitchen&#21644;Charades-Ego&#65289;&#19978;&#23637;&#31034;&#20102;VLaMP&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;VLaMP&#22312;&#20934;&#30830;&#24615;&#12289;&#25928;&#29575;&#21644;&#27867;&#21270;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
To make progress towards multi-modal AI assistants which can guide users to achieve complex multi-step goals, we propose the task of Visual Planning for Assistance (VPA). Given a goal briefly described in natural language, e.g., "make a shelf", and a video of the user's progress so far, the aim of VPA is to obtain a plan, i.e., a sequence of actions such as "sand shelf", "paint shelf", etc., to achieve the goal. This requires assessing the user's progress from the untrimmed video, and relating it to the requirements of underlying goal, i.e., relevance of actions and ordering dependencies amongst them. Consequently, this requires handling long video history, and arbitrarily complex action dependencies. To address these challenges, we decompose VPA into video action segmentation and forecasting. We formulate the forecasting step as a multi-modal sequence modeling problem and present Visual Language Model based Planner (VLaMP), which leverages pre-trained LMs as the sequence model. We dem
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#27169;&#22411;&#25152;&#26377;&#26435;&#35299;&#20915;&#26041;&#26696;&#20013;&#23545;&#25239;&#24694;&#24847;&#21407;&#21578;&#30340;&#40065;&#26834;&#24615;&#38382;&#39064;&#65292;&#23637;&#31034;&#20102;&#24120;&#35265;&#30340;MOR&#26041;&#26696;&#21487;&#20197;&#34987;&#24694;&#24847;&#21407;&#21578;&#38024;&#23545;&#26410;&#34987;&#30423;&#29992;&#30340;&#29420;&#31435;&#27169;&#22411;&#25552;&#20986;&#34394;&#20551;&#25351;&#25511;&#12290;</title><link>http://arxiv.org/abs/2304.06607</link><description>&lt;p&gt;
&#27169;&#22411;&#25152;&#26377;&#26435;&#20105;&#35758;&#20013;&#30340;&#34394;&#20551;&#25351;&#25511;
&lt;/p&gt;
&lt;p&gt;
False Claims against Model Ownership Resolution. (arXiv:2304.06607v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06607
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#27169;&#22411;&#25152;&#26377;&#26435;&#35299;&#20915;&#26041;&#26696;&#20013;&#23545;&#25239;&#24694;&#24847;&#21407;&#21578;&#30340;&#40065;&#26834;&#24615;&#38382;&#39064;&#65292;&#23637;&#31034;&#20102;&#24120;&#35265;&#30340;MOR&#26041;&#26696;&#21487;&#20197;&#34987;&#24694;&#24847;&#21407;&#21578;&#38024;&#23545;&#26410;&#34987;&#30423;&#29992;&#30340;&#29420;&#31435;&#27169;&#22411;&#25552;&#20986;&#34394;&#20551;&#25351;&#25511;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#26159;&#27169;&#22411;&#25152;&#26377;&#32773;&#30340;&#26377;&#20215;&#20540;&#30693;&#35782;&#20135;&#26435;&#65292;&#26500;&#25104;&#20102;&#31454;&#20105;&#20248;&#21183;&#12290;&#22240;&#27492;&#65292;&#24320;&#21457;&#20445;&#25252;&#27169;&#22411;&#19981;&#34987;&#30423;&#29992;&#30340;&#25216;&#26415;&#33267;&#20851;&#37325;&#35201;&#12290;&#27169;&#22411;&#25152;&#26377;&#26435;&#35299;&#20915;&#26041;&#26696;&#65288;MOR&#65289;&#26159;&#19968;&#31867;&#21487;&#20197;&#38450;&#27490;&#27169;&#22411;&#34987;&#30423;&#30340;&#25216;&#26415;&#12290;MOR&#26041;&#26696;&#20351;&#24471;&#21407;&#21578;&#26041;&#21487;&#20197;&#36890;&#36807;&#25552;&#20379;&#35777;&#25454;&#65288;&#22914;&#27700;&#21360;&#25110;&#25351;&#32441;&#65289;&#26469;&#26029;&#35328;&#23545;&#28041;&#23244;&#30423;&#29992;&#27169;&#22411;&#30340;&#34987;&#21578;&#26041;&#22768;&#31216;&#25152;&#26377;&#26435;&#65292;&#35777;&#26126;&#28041;&#23244;&#27169;&#22411;&#26159;&#34987;&#30423;&#25110;&#32773;&#28304;&#33258;&#20110;&#21407;&#21578;&#26041;&#25317;&#26377;&#30340;&#28304;&#27169;&#22411;&#12290;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968; MOR &#26041;&#26696;&#37325;&#28857;&#25918;&#22312;&#38450;&#33539;&#24694;&#24847;&#28041;&#23244;&#26041;&#26041;&#38754;&#65292;&#30830;&#20445;&#22914;&#26524;&#28041;&#23244;&#27169;&#22411;&#30830;&#23454;&#26159;&#34987;&#30423;&#29256;&#65292;&#21017;&#21407;&#21578;&#26041;&#23558;&#33719;&#32988;&#12290;&#20294;&#26159;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#29616;&#26377;&#25991;&#29486;&#20013;&#30340;&#24120;&#35265; MOR &#26041;&#26696;&#23384;&#22312;&#30528;&#21478;&#19968;&#20010;&#21516;&#31561;&#37325;&#35201;&#20294;&#23578;&#26410;&#34987;&#20805;&#20998;&#25506;&#35752;&#30340;&#40065;&#26834;&#24615;&#38382;&#39064;&#65306;&#24694;&#24847;&#21407;&#21578;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#25104;&#21151;&#22320;&#38024;&#23545;&#26410;&#34987;&#30423;&#29992;&#30340;&#29420;&#31435;&#27169;&#22411;&#25552;&#20986;&#34394;&#20551;&#25351;&#25511;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural network (DNN) models are valuable intellectual property of model owners, constituting a competitive advantage. Therefore, it is crucial to develop techniques to protect against model theft. Model ownership resolution (MOR) is a class of techniques that can deter model theft. A MOR scheme enables an accuser to assert an ownership claim for a suspect model by presenting evidence, such as a watermark or fingerprint, to show that the suspect model was stolen or derived from a source model owned by the accuser. Most of the existing MOR schemes prioritize robustness against malicious suspects, ensuring that the accuser will win if the suspect model is indeed a stolen model.  In this paper, we show that common MOR schemes in the literature are vulnerable to a different, equally important but insufficiently explored, robustness concern: a malicious accuser. We show how malicious accusers can successfully make false claims against independent suspect models that were not stolen. Our
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#35843;&#26597;&#20102;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#36136;&#37327;&#22833;&#35823;&#21450;&#20854;&#24212;&#29992;&#20110;&#26816;&#27979;Deepfakes&#65292;&#35782;&#21035;&#20102;&#20116;&#31181;&#23450;&#24615;&#32570;&#38519;&#12290;&#36825;&#20123;&#21457;&#29616;&#26377;&#21161;&#20110;&#25913;&#36827;&#27169;&#22411;&#24182;&#21046;&#23450;&#26816;&#27979;Deepfakes&#30340;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2304.06470</link><description>&lt;p&gt;
&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#23450;&#24615;&#22833;&#36133;&#21450;&#20854;&#22312;&#26816;&#27979;Deepfakes&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Qualitative Failures of Image Generation Models and Their Application in Detecting Deepfakes. (arXiv:2304.06470v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06470
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#35843;&#26597;&#20102;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#36136;&#37327;&#22833;&#35823;&#21450;&#20854;&#24212;&#29992;&#20110;&#26816;&#27979;Deepfakes&#65292;&#35782;&#21035;&#20102;&#20116;&#31181;&#23450;&#24615;&#32570;&#38519;&#12290;&#36825;&#20123;&#21457;&#29616;&#26377;&#21161;&#20110;&#25913;&#36827;&#27169;&#22411;&#24182;&#21046;&#23450;&#26816;&#27979;Deepfakes&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#21644;&#35270;&#39057;&#29983;&#25104;&#27169;&#22411;&#21019;&#36896;&#20986;&#36924;&#30495;&#30340;&#24433;&#20687;&#30340;&#33021;&#21147;&#24050;&#32463;&#36798;&#21040;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#39640;&#24230;&#65292;&#36825;&#20351;&#24471;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#24456;&#38590;&#21306;&#20998;&#30495;&#23454;&#21644;&#20266;&#36896;&#30340;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#29983;&#25104;&#22270;&#20687;&#30340;&#36136;&#37327;&#21644;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#22270;&#20687;&#20043;&#38388;&#20173;&#23384;&#22312;&#24046;&#36317;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#22823;&#37327;&#23398;&#26415;&#35770;&#25991;&#21644;&#31038;&#20132;&#23186;&#20307;&#20869;&#23481;&#65292;&#20197;&#30830;&#23450;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#23450;&#24615;&#32570;&#38519;&#65292;&#24182;&#23558;&#20854;&#20998;&#31867;&#20026;&#20116;&#31867;&#12290;&#36890;&#36807;&#20102;&#35299;&#36825;&#20123;&#22833;&#36133;&#65292;&#25105;&#20204;&#21487;&#20197;&#30830;&#23450;&#36825;&#20123;&#27169;&#22411;&#38656;&#35201;&#25913;&#36827;&#30340;&#39046;&#22495;&#65292;&#24182;&#21046;&#23450;&#26816;&#27979;Deepfakes&#30340;&#31574;&#30053;&#12290;&#20170;&#22825;&#31038;&#20250;&#20013;Deepfakes&#30340;&#26222;&#36941;&#23384;&#22312;&#26159;&#19968;&#20010;&#20005;&#37325;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#21487;&#20197;&#24110;&#21161;&#20943;&#36731;&#23427;&#20204;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability of image and video generation models to create photorealistic images has reached unprecedented heights, making it difficult to distinguish between real and fake images in many cases. However, despite this progress, a gap remains between the quality of generated images and those found in the real world. To address this, we have reviewed a vast body of literature from both academic publications and social media to identify qualitative shortcomings in image generation models, which we have classified into five categories. By understanding these failures, we can identify areas where these models need improvement, as well as develop strategies for detecting deep fakes. The prevalence of deep fakes in today's society is a serious concern, and our findings can help mitigate their negative impact.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26679;&#21270;&#21644;&#36924;&#30495;&#30340;&#22686;&#24378;&#26041;&#27861;&#65292;&#21487;&#20197;&#21019;&#24314;&#25972;&#20307;&#23545;&#35937;&#24182;&#28789;&#27963;&#22320;&#23450;&#20301;&#21644;&#26059;&#36716;&#23545;&#35937;&#65292;&#24182;&#30456;&#24212;&#22320;&#24212;&#29992;&#33258;&#36974;&#25377;&#21644;&#22806;&#36974;&#25377;&#12290;&#36890;&#36807;&#36845;&#20195;&#26500;&#24314;&#22810;&#20010;&#23545;&#35937;&#26469;&#25552;&#39640;&#25972;&#20307;&#23545;&#35937;&#26500;&#36896;&#30340;&#22810;&#26679;&#24615;&#65292;&#26500;&#36896;&#30340;&#23545;&#35937;&#21487;&#20197;&#22312;&#35757;&#32451;&#24103;&#20013;&#38543;&#26426;&#25918;&#32622;&#21644;&#26059;&#36716;&#12290;</title><link>http://arxiv.org/abs/2303.12743</link><description>&lt;p&gt;
DR.CPO&#65306;&#36890;&#36807;&#36845;&#20195;&#26500;&#24314;&#12289;&#38543;&#26426;&#25918;&#32622;&#21644; HPR &#36974;&#34109;&#23454;&#29616;&#30340;&#22810;&#26679;&#21270;&#21644;&#36924;&#30495;&#30340;&#19977;&#32500;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
DR.CPO: Diversified and Realistic 3D Augmentation via Iterative Construction, Random Placement, and HPR Occlusion. (arXiv:2303.12743v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12743
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26679;&#21270;&#21644;&#36924;&#30495;&#30340;&#22686;&#24378;&#26041;&#27861;&#65292;&#21487;&#20197;&#21019;&#24314;&#25972;&#20307;&#23545;&#35937;&#24182;&#28789;&#27963;&#22320;&#23450;&#20301;&#21644;&#26059;&#36716;&#23545;&#35937;&#65292;&#24182;&#30456;&#24212;&#22320;&#24212;&#29992;&#33258;&#36974;&#25377;&#21644;&#22806;&#36974;&#25377;&#12290;&#36890;&#36807;&#36845;&#20195;&#26500;&#24314;&#22810;&#20010;&#23545;&#35937;&#26469;&#25552;&#39640;&#25972;&#20307;&#23545;&#35937;&#26500;&#36896;&#30340;&#22810;&#26679;&#24615;&#65292;&#26500;&#36896;&#30340;&#23545;&#35937;&#21487;&#20197;&#22312;&#35757;&#32451;&#24103;&#20013;&#38543;&#26426;&#25918;&#32622;&#21644;&#26059;&#36716;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#65292;&#25968;&#25454;&#22686;&#24378;&#24120;&#29992;&#20110;&#25913;&#36827;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#12290;&#26368;&#22522;&#26412;&#30340;&#26041;&#27861;&#21253;&#25324;&#25554;&#20837;&#22797;&#21046;&#23545;&#35937;&#21644;&#26059;&#36716;&#21644;&#32553;&#25918;&#25972;&#20010;&#35757;&#32451;&#24103;&#12290;&#20063;&#24050;&#32463;&#24320;&#21457;&#20102;&#35768;&#22810;&#21464;&#20307;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#19982;&#29616;&#23454;&#19990;&#30028;&#30340;&#21487;&#33021;&#24615;&#30456;&#27604;&#30456;&#24403;&#26377;&#38480;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22810;&#26679;&#21270;&#21644;&#36924;&#30495;&#22686;&#24378;&#26041;&#27861;&#65292;&#21487;&#20197;&#28789;&#27963;&#22320;&#26500;&#36896;&#25972;&#20307;&#23545;&#35937;&#65292;&#33258;&#30001;&#22320;&#23450;&#20301;&#21644;&#26059;&#36716;&#23545;&#35937;&#65292;&#24182;&#30456;&#24212;&#22320;&#24212;&#29992;&#33258;&#36974;&#25377;&#21644;&#22806;&#36974;&#25377;&#12290;&#20026;&#20102;&#25552;&#39640;&#25972;&#20307;&#23545;&#35937;&#26500;&#36896;&#30340;&#22810;&#26679;&#24615;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#36845;&#20195;&#26041;&#27861;&#65292;&#23558;&#20174;&#29616;&#23454;&#19990;&#30028;&#35266;&#23519;&#21040;&#30340;&#22810;&#20010;&#23545;&#35937;&#38543;&#26426;&#32452;&#21512;&#25104;&#21333;&#20010;&#23545;&#35937;&#12290;&#19982;&#29616;&#26377;&#22686;&#24378;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#26500;&#36896;&#30340;&#23545;&#35937;&#21487;&#20197;&#38543;&#26426;&#25918;&#32622;&#21644;&#26059;&#36716;&#22312;&#35757;&#32451;&#24103;&#20013;&#65292;&#22240;&#20026;&#36866;&#24403;&#30340;&#36974;&#25377;&#21487;&#20197;&#21453;&#26144;&#22312;&#26368;&#32456;&#25972;&#20307;&#23545;&#35937;&#20013;&#12290;&#26368;&#21518;&#65292;&#20026;&#20102;&#38450;&#27490;&#36807;&#24230;&#22686;&#24378;&#23548;&#33268;&#36807;&#25311;&#21512;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#20998;&#23618;&#36974;&#25377;&#27010;&#29575;&#35774;&#32622;&#65292;&#36890;&#36807;&#23545;&#35937;&#30340;&#20301;&#32622;&#21644;&#22823;&#23567;&#35843;&#25972;&#36974;&#25377;&#24378;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
In autonomous driving, data augmentation is commonly used for improving 3D object detection. The most basic methods include insertion of copied objects and rotation and scaling of the entire training frame. Numerous variants have been developed as well. The existing methods, however, are considerably limited when compared to the variety of the real world possibilities. In this work, we develop a diversified and realistic augmentation method that can flexibly construct a whole-body object, freely locate and rotate the object, and apply self-occlusion and external-occlusion accordingly. To improve the diversity of the whole-body object construction, we develop an iterative method that stochastically combines multiple objects observed from the real world into a single object. Unlike the existing augmentation methods, the constructed objects can be randomly located and rotated in the training frame because proper occlusions can be reflected to the whole-body objects in the final step. Fina
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#19978;&#19979;&#25991;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#36816;&#29992;&#20998;&#20301;&#25968;&#22238;&#24402;&#26862;&#26519;&#26469;&#27169;&#25311;&#29305;&#24449;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#33021;&#22815;&#26356;&#20934;&#30830;&#21644;&#21487;&#35299;&#37322;&#22320;&#35782;&#21035;&#20559;&#31163;&#31867;&#20284;&#23545;&#35937;&#19978;&#19979;&#25991;&#30340;&#20854;&#20182;&#23545;&#35937;&#12290;</title><link>http://arxiv.org/abs/2302.11239</link><description>&lt;p&gt;
&#20351;&#29992;&#20998;&#20301;&#25968;&#22238;&#24402;&#26862;&#26519;&#30340;&#21487;&#35299;&#37322;&#19978;&#19979;&#25991;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Explainable Contextual Anomaly Detection using Quantile Regression Forests. (arXiv:2302.11239v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11239
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#19978;&#19979;&#25991;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#36816;&#29992;&#20998;&#20301;&#25968;&#22238;&#24402;&#26862;&#26519;&#26469;&#27169;&#25311;&#29305;&#24449;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#33021;&#22815;&#26356;&#20934;&#30830;&#21644;&#21487;&#35299;&#37322;&#22320;&#35782;&#21035;&#20559;&#31163;&#31867;&#20284;&#23545;&#35937;&#19978;&#19979;&#25991;&#30340;&#20854;&#20182;&#23545;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#36890;&#36807;&#24179;&#31561;&#23545;&#24453;&#25152;&#26377;&#29305;&#24449;&#26469;&#35782;&#21035;&#20559;&#31163;&#22823;&#22810;&#25968;&#20854;&#20182;&#23545;&#35937;&#30340;&#23545;&#35937;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#19978;&#19979;&#25991;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#36890;&#36807;&#23558;&#29305;&#24449;&#21010;&#20998;&#20026;&#19978;&#19979;&#25991;&#29305;&#24449;&#21644;&#34892;&#20026;&#29305;&#24449;&#65292;&#26088;&#22312;&#26816;&#27979;&#20559;&#31163;&#31867;&#20284;&#23545;&#35937;&#19978;&#19979;&#25991;&#30340;&#20854;&#20182;&#23545;&#35937;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#20381;&#36182;&#20110;&#20256;&#32479;&#30340;&#22522;&#20110;&#20381;&#36182;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#21644;&#19978;&#19979;&#25991;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#22522;&#20110;&#30001;&#27492;&#33719;&#24471;&#30340;&#35265;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#37319;&#29992;&#20998;&#20301;&#25968;&#22238;&#24402;&#26862;&#26519;&#26469;&#27169;&#25311;&#29305;&#24449;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#23454;&#29616;&#20869;&#22312;&#30340;&#21487;&#35299;&#37322;&#19978;&#19979;&#25991;&#24322;&#24120;&#26816;&#27979;&#12290;&#21508;&#31181;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35782;&#21035;&#19978;&#19979;&#25991;&#24322;&#24120;&#26041;&#38754;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#29366;&#24577;-of-art&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional anomaly detection methods aim to identify objects that deviate from most other objects by treating all features equally. In contrast, contextual anomaly detection methods aim to detect objects that deviate from other objects within a context of similar objects by dividing the features into contextual features and behavioral features. In this paper, we develop connections between dependency-based traditional anomaly detection methods and contextual anomaly detection methods. Based on resulting insights, we propose a novel approach to inherently interpretable contextual anomaly detection that uses Quantile Regression Forests to model dependencies between features. Extensive experiments on various synthetic and real-world datasets demonstrate that our method outperforms state-of-the-art anomaly detection methods in identifying contextual anomalies in terms of accuracy and interpretability.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#27714;&#35299;&#22120;PGPSNet&#65292;&#36890;&#36807;&#23558;&#22270;&#34920;&#36716;&#21270;&#20026;&#22522;&#26412;&#25991;&#26412;&#23376;&#21477;&#20197;&#26377;&#25928;&#25551;&#36848;&#22270;&#34920;&#29305;&#24449;&#65292;&#24182;&#32467;&#21512;&#32467;&#26500;&#21644;&#35821;&#20041;&#39044;&#35757;&#32451;&#12289;&#25968;&#25454;&#22686;&#24378;&#21644;&#33258;&#25105;&#38480;&#21046;&#35299;&#30721;&#31561;&#25216;&#26415;&#65292;PGPSNet&#25317;&#26377;&#20016;&#23500;&#30340;&#20960;&#20309;&#23450;&#29702;&#21644;&#20960;&#20309;&#34920;&#31034;&#30693;&#35782;&#65292;&#20174;&#32780;&#22312;GPS&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.11097</link><description>&lt;p&gt;
&#20174;&#22270;&#34920;&#20013;&#35299;&#26512;&#25991;&#26412;&#23376;&#21477;&#30340;&#22810;&#27169;&#24577;&#31070;&#32463;&#20960;&#20309;&#27714;&#35299;&#22120;
&lt;/p&gt;
&lt;p&gt;
A Multi-Modal Neural Geometric Solver with Textual Clauses Parsed from Diagram. (arXiv:2302.11097v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11097
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#27714;&#35299;&#22120;PGPSNet&#65292;&#36890;&#36807;&#23558;&#22270;&#34920;&#36716;&#21270;&#20026;&#22522;&#26412;&#25991;&#26412;&#23376;&#21477;&#20197;&#26377;&#25928;&#25551;&#36848;&#22270;&#34920;&#29305;&#24449;&#65292;&#24182;&#32467;&#21512;&#32467;&#26500;&#21644;&#35821;&#20041;&#39044;&#35757;&#32451;&#12289;&#25968;&#25454;&#22686;&#24378;&#21644;&#33258;&#25105;&#38480;&#21046;&#35299;&#30721;&#31561;&#25216;&#26415;&#65292;PGPSNet&#25317;&#26377;&#20016;&#23500;&#30340;&#20960;&#20309;&#23450;&#29702;&#21644;&#20960;&#20309;&#34920;&#31034;&#30693;&#35782;&#65292;&#20174;&#32780;&#22312;GPS&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20960;&#20309;&#38382;&#39064;&#27714;&#35299;(GPS)&#26159;&#39640;&#32423;&#25968;&#23398;&#25512;&#29702;&#65292;&#38656;&#35201;&#22810;&#27169;&#24577;&#34701;&#21512;&#21644;&#20960;&#20309;&#30693;&#35782;&#24212;&#29992;&#30340;&#33021;&#21147;&#12290;&#26368;&#36817;&#65292;&#31070;&#32463;&#27714;&#35299;&#22120;&#22312;GPS&#26041;&#38754;&#34920;&#29616;&#20986;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#20173;&#28982;&#22312;&#22270;&#34920;&#34920;&#29616;&#21644;&#27169;&#24577;&#34701;&#21512;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#22270;&#34920;&#36716;&#21270;&#20026;&#22522;&#26412;&#25991;&#26412;&#23376;&#21477;&#65292;&#20197;&#26377;&#25928;&#25551;&#36848;&#22270;&#34920;&#29305;&#24449;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#27714;&#35299;&#22120;PGPSNet&#65292;&#20197;&#39640;&#25928;&#22320;&#34701;&#21512;&#22810;&#27169;&#24577;&#20449;&#24687;&#12290;&#32467;&#21512;&#32467;&#26500;&#21644;&#35821;&#20041;&#39044;&#35757;&#32451;&#12289;&#25968;&#25454;&#22686;&#24378;&#21644;&#33258;&#25105;&#38480;&#21046;&#35299;&#30721;&#65292;PGPSNet&#25317;&#26377;&#20016;&#23500;&#30340;&#20960;&#20309;&#23450;&#29702;&#21644;&#20960;&#20309;&#34920;&#31034;&#30693;&#35782;&#65292;&#20174;&#32780;&#20419;&#36827;&#20102;&#20960;&#20309;&#29702;&#35299;&#21644;&#25512;&#29702;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20419;&#36827;GPS&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#22823;&#35268;&#27169;&#31934;&#32454;&#27880;&#37322;&#30340;GPS&#25968;&#25454;&#38598;&#65292;&#21517;&#20026;PGPS9K&#65292;&#26631;&#26377;&#31934;&#32454;&#30340;&#22270;&#34920;&#27880;&#37322;&#21644;&#21487;&#35299;&#37322;&#30340;&#35299;&#20915;&#26041;&#26696;&#31243;&#24207;&#12290;&#22312;PGPS9K&#21644;&#29616;&#26377;&#25968;&#25454;&#38598;Geometry3K&#19978;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;PGPSNet&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Geometry problem solving (GPS) is a high-level mathematical reasoning requiring the capacities of multi-modal fusion and geometric knowledge application. Recently, neural solvers have shown great potential in GPS but still be short in diagram presentation and modal fusion. In this work, we convert diagrams into basic textual clauses to describe diagram features effectively, and propose a new neural solver called PGPSNet to fuse multi-modal information efficiently. Combining structural and semantic pre-training, data augmentation and self-limited decoding, PGPSNet is endowed with rich knowledge of geometry theorems and geometric representation, and therefore promotes geometric understanding and reasoning. In addition, to facilitate the research of GPS, we build a new large-scale and fine-annotated GPS dataset named PGPS9K, labeled with both fine-grained diagram annotation and interpretable solution program. Experiments on PGPS9K and an existing dataset Geometry3K validate the superiorit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#38480;&#21046;&#20351;&#24471;&#23454;&#26102;&#24490;&#29615;&#23398;&#20064;&#31639;&#27861;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#65292;&#20998;&#21035;&#26159;&#23558;&#32593;&#32476;&#20998;&#35299;&#20026;&#29420;&#31435;&#27169;&#22359;&#25110;&#36880;&#27493;&#23398;&#20064;&#32593;&#32476;&#12290;&#19982;&#20854;&#20182;&#21487;&#25193;&#23637;&#31639;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#36825;&#20123;&#31639;&#27861;&#19981;&#20250;&#21521;&#26799;&#24230;&#20272;&#35745;&#28155;&#21152;&#22122;&#22768;&#25110;&#20559;&#24046;&#65292;&#32780;&#26159;&#36890;&#36807;&#26435;&#34913;&#32593;&#32476;&#30340;&#21151;&#33021;&#33021;&#21147;&#20197;&#23454;&#29616;&#21487;&#25193;&#23637;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2302.05326</link><description>&lt;p&gt;
&#20351;&#29992;&#31232;&#30095;&#36830;&#25509;&#21644;&#36873;&#25321;&#24615;&#23398;&#20064;&#30340;&#21487;&#25193;&#23637;&#23454;&#26102;&#24490;&#29615;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Scalable Real-Time Recurrent Learning Using Sparse Connections and Selective Learning. (arXiv:2302.05326v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05326
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#38480;&#21046;&#20351;&#24471;&#23454;&#26102;&#24490;&#29615;&#23398;&#20064;&#31639;&#27861;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#65292;&#20998;&#21035;&#26159;&#23558;&#32593;&#32476;&#20998;&#35299;&#20026;&#29420;&#31435;&#27169;&#22359;&#25110;&#36880;&#27493;&#23398;&#20064;&#32593;&#32476;&#12290;&#19982;&#20854;&#20182;&#21487;&#25193;&#23637;&#31639;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#36825;&#20123;&#31639;&#27861;&#19981;&#20250;&#21521;&#26799;&#24230;&#20272;&#35745;&#28155;&#21152;&#22122;&#22768;&#25110;&#20559;&#24046;&#65292;&#32780;&#26159;&#36890;&#36807;&#26435;&#34913;&#32593;&#32476;&#30340;&#21151;&#33021;&#33021;&#21147;&#20197;&#23454;&#29616;&#21487;&#25193;&#23637;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#24863;&#30693;&#35266;&#23519;&#20013;&#26500;&#24314;&#29366;&#24577;&#26159;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#19968;&#31181;&#29992;&#20110;&#29366;&#24577;&#26500;&#24314;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#20351;&#29992;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#12290; BPTT&#21644;&#23454;&#26102;&#24490;&#29615;&#23398;&#20064;&#65288;RTRL&#65289;&#26159;&#20004;&#31181;&#27969;&#34892;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#24490;&#29615;&#23398;&#20064;&#26041;&#27861;&#12290; BPTT&#22312;&#35745;&#31639;&#26799;&#24230;&#20043;&#21069;&#38656;&#35201;&#23436;&#25972;&#30340;&#35266;&#23519;&#24207;&#21015;&#65292;&#19981;&#36866;&#21512;&#22312;&#32447;&#23454;&#26102;&#26356;&#26032;&#12290; RTRL&#21487;&#20197;&#36827;&#34892;&#22312;&#32447;&#26356;&#26032;&#65292;&#20294;&#19981;&#36866;&#29992;&#20110;&#22823;&#22411;&#32593;&#32476;&#12290; &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#38480;&#21046;&#65292;&#20351;RTRL&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36890;&#36807;&#23558;&#32593;&#32476;&#20998;&#35299;&#20026;&#29420;&#31435;&#27169;&#22359;&#25110;&#36880;&#27493;&#23398;&#20064;&#32593;&#32476;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;RTRL&#19982;&#21442;&#25968;&#25968;&#37327;&#21576;&#32447;&#24615;&#27604;&#20363;&#20851;&#31995;&#12290;&#19982;&#20808;&#21069;&#30340;&#21487;&#25193;&#23637;&#26799;&#24230;&#20272;&#35745;&#31639;&#27861;&#65288;&#20363;&#22914;UORO&#21644;Truncated-BPTT&#65289;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#19981;&#20250;&#21521;&#26799;&#24230;&#20272;&#35745;&#28155;&#21152;&#22122;&#22768;&#25110;&#20559;&#24046;&#12290;&#30456;&#21453;&#65292;&#23427;&#20204;&#26435;&#34913;&#20102;&#32593;&#32476;&#30340;&#21151;&#33021;&#33021;&#21147;&#20197;&#23454;&#29616;&#21487;&#25193;&#23637;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
State construction from sensory observations is an important component of a reinforcement learning agent. One solution for state construction is to use recurrent neural networks. Back-propagation through time (BPTT), and real-time recurrent learning (RTRL) are two popular gradient-based methods for recurrent learning. BPTT requires the complete sequence of observations before computing gradients and is unsuitable for online real-time updates. RTRL can do online updates but scales poorly to large networks. In this paper, we propose two constraints that make RTRL scalable. We show that by either decomposing the network into independent modules, or learning the network incrementally, we can make RTRL scale linearly with the number of parameters. Unlike prior scalable gradient estimation algorithms, such as UORO and Truncated-BPTT, our algorithms do not add noise or bias to the gradient estimate. Instead, they trade-off the functional capacity of the network to achieve scalable learning. W
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26641;&#29366;Parzen&#20272;&#35745;&#22120;&#65288;TPE&#65289;&#21644;&#20998;&#35299;&#31639;&#27861;&#30340;&#26032;&#26694;&#26550;&#65288;TPE-VMD-TFT&#65289;&#65292;&#29992;&#20110;24&#23567;&#26102;&#21644;48&#23567;&#26102;&#20043;&#21069;&#30340;&#39118;&#30005;&#21151;&#29575;&#39044;&#27979;&#12290;&#22312;&#27861;&#22269;&#30005;&#21147;&#20844;&#21496;Engie&#30340;&#39118;&#33021;&#25968;&#25454;&#38598;&#19978;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2302.01222</link><description>&lt;p&gt;
&#22522;&#20110;&#26102;&#38388;&#27880;&#24847;&#26426;&#21046;&#30340;&#20013;&#26399;&#39118;&#30005;&#21151;&#29575;&#39044;&#27979;&#26032;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A novel framework for medium-term wind power prediction based on temporal attention mechanisms. (arXiv:2302.01222v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01222
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26641;&#29366;Parzen&#20272;&#35745;&#22120;&#65288;TPE&#65289;&#21644;&#20998;&#35299;&#31639;&#27861;&#30340;&#26032;&#26694;&#26550;&#65288;TPE-VMD-TFT&#65289;&#65292;&#29992;&#20110;24&#23567;&#26102;&#21644;48&#23567;&#26102;&#20043;&#21069;&#30340;&#39118;&#30005;&#21151;&#29575;&#39044;&#27979;&#12290;&#22312;&#27861;&#22269;&#30005;&#21147;&#20844;&#21496;Engie&#30340;&#39118;&#33021;&#25968;&#25454;&#38598;&#19978;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39118;&#33021;&#26159;&#19968;&#31181;&#24191;&#27867;&#20998;&#24067;&#12289;&#21487;&#20877;&#29983;&#21644;&#29615;&#20445;&#30340;&#33021;&#28304;&#65292;&#23545;&#32531;&#35299;&#20840;&#29699;&#21464;&#26262;&#21644;&#33021;&#28304;&#30701;&#32570;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#19981;&#30830;&#23450;&#24615;&#21644;&#27874;&#21160;&#24615;&#65292;&#22823;&#35268;&#27169;&#39118;&#30005;&#31995;&#32479;&#30340;&#32593;&#26684;&#38598;&#25104;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20013;&#26399;&#39118;&#30005;&#21151;&#29575;&#39044;&#27979;&#21487;&#20197;&#20026;&#33021;&#37327;&#35843;&#24230;&#25552;&#20379;&#22522;&#26412;&#20381;&#25454;&#65292;&#22240;&#27492;&#31934;&#30830;&#30340;&#39118;&#30005;&#21151;&#29575;&#39044;&#27979;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26641;&#29366;Parzen&#20272;&#35745;&#22120;&#65288;TPE&#65289;&#21644;&#20998;&#35299;&#31639;&#27861;&#30340;&#26032;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#22522;&#20110;&#21464;&#20998;&#27169;&#24335;&#20998;&#35299;&#65288;VMD&#65289;&#21644;&#26102;&#38388;&#34701;&#21512;&#21464;&#21387;&#22120;&#65288;TFT&#65289;&#23450;&#20041;&#20102;24&#23567;&#26102;&#21644;48&#23567;&#26102;&#20043;&#21069;&#30340;&#39118;&#30005;&#21151;&#29575;&#39044;&#27979;&#30340;TPE-VMD-TFT&#26041;&#27861;&#12290;&#22312;&#27861;&#22269;&#30005;&#21147;&#20844;&#21496;Engie&#30340;&#39118;&#33021;&#25968;&#25454;&#38598;&#19978;&#65292;&#32467;&#26524;&#34920;&#26126;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Wind energy is a widely distributed, recyclable and environmentally friendly energy source that plays an important role in mitigating global warming and energy shortages. Wind energy's uncertainty and fluctuating nature makes grid integration of large-scale wind energy systems challenging. Medium-term wind power forecasts can provide an essential basis for energy dispatch, so accurate wind power forecasts are essential. Much research has yielded excellent results in recent years. However, many of them require additional experimentation and analysis when applied to other data. In this paper, we propose a novel short-term forecasting framework by tree-structured parzen estimator (TPE) and decomposition algorithms. This framework defines the TPE-VMD-TFT method for 24-h and 48-h ahead wind power forecasting based on variational mode decomposition (VMD) and time fusion transformer (TFT). In the Engie wind dataset from the electricity company in France, the results show that the proposed met
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20687;&#32032;&#25551;&#36848;&#31526;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#39640;&#20998;&#36776;&#29575;&#36965;&#24863;&#22270;&#20687;&#20013;&#24322;&#24120;&#27169;&#24335;&#20998;&#21106;&#38382;&#39064;&#12290;&#27169;&#22411;&#36890;&#36807;&#25968;&#25454;&#22686;&#24191;&#29983;&#25104;&#34394;&#25311;&#24322;&#24120;&#26631;&#26412;&#65292;&#24182;&#20351;&#29992;&#20855;&#26377;&#21306;&#20998;&#24615;&#30340;&#20687;&#32032;&#25551;&#36848;&#31526;&#36827;&#34892;&#28145;&#24230;&#21333;&#31867;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2301.13422</link><description>&lt;p&gt;
&#22522;&#20110;&#20687;&#32032;&#25551;&#36848;&#31526;&#30340;&#39640;&#20998;&#36776;&#29575;&#36965;&#24863;&#22270;&#20687;&#24322;&#24120;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Anomaly Segmentation for High-Resolution Remote Sensing Images Based on Pixel Descriptors. (arXiv:2301.13422v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13422
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20687;&#32032;&#25551;&#36848;&#31526;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#39640;&#20998;&#36776;&#29575;&#36965;&#24863;&#22270;&#20687;&#20013;&#24322;&#24120;&#27169;&#24335;&#20998;&#21106;&#38382;&#39064;&#12290;&#27169;&#22411;&#36890;&#36807;&#25968;&#25454;&#22686;&#24191;&#29983;&#25104;&#34394;&#25311;&#24322;&#24120;&#26631;&#26412;&#65292;&#24182;&#20351;&#29992;&#20855;&#26377;&#21306;&#20998;&#24615;&#30340;&#20687;&#32032;&#25551;&#36848;&#31526;&#36827;&#34892;&#28145;&#24230;&#21333;&#31867;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#39640;&#31354;&#38388;&#20998;&#36776;&#29575;&#36965;&#24863;&#22270;&#20687;&#20013;&#24322;&#24120;&#27169;&#24335;&#30340;&#20998;&#21106;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20687;&#32032;&#25551;&#36848;&#31526;&#30340;&#24322;&#24120;&#20998;&#21106;&#27169;&#22411;(ASD)&#12290;&#35813;&#27169;&#22411;&#20351;&#29992;&#20855;&#26377;&#21306;&#20998;&#24615;&#30340;&#20687;&#32032;&#25551;&#36848;&#31526;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#36827;&#34892;&#28145;&#24230;&#21333;&#31867;&#20998;&#31867;&#65292;&#36890;&#36807;&#25968;&#25454;&#22686;&#24191;&#29983;&#25104;&#34394;&#25311;&#24322;&#24120;&#26631;&#26412;&#65292;&#20197;&#20351;&#24471;&#20687;&#32032;&#25551;&#36848;&#31526;&#32039;&#20945;&#32780;&#19981;&#20250;&#24573;&#30053;&#27491;&#24120;&#25968;&#25454;&#65292;&#24182;&#22312;&#35757;&#32451;&#26102;&#36991;&#20813;&#27169;&#22411;&#23849;&#28291;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;ASD&#27169;&#22411;&#36824;&#24341;&#20837;&#20102;&#22810;&#32423;&#21644;&#22810;&#23610;&#24230;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly segmentation in high spatial resolution (HSR) remote sensing imagery is aimed at segmenting anomaly patterns of the earth deviating from normal patterns, which plays an important role in various Earth vision applications. However, it is a challenging task due to the complex distribution and the irregular shapes of objects, and the lack of abnormal samples. To tackle these problems, an anomaly segmentation model based on pixel descriptors (ASD) is proposed for anomaly segmentation in HSR imagery. Specifically, deep one-class classification is introduced for anomaly segmentation in the feature space with discriminative pixel descriptors. The ASD model incorporates the data argument for generating virtual ab-normal samples, which can force the pixel descriptors to be compact for normal data and meanwhile to be diverse to avoid the model collapse problems when only positive samples participated in the training. In addition, the ASD introduced a multi-level and multi-scale feature e
&lt;/p&gt;</description></item><item><title>&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#19981;&#21487;&#38752;&#65292;&#24212;&#35813;&#35880;&#24910;&#20351;&#29992;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#21487;&#20197;&#35782;&#21035;&#21040;&#30340;&#30830;&#20999;&#25104;&#21592;&#26679;&#26412;&#30340;&#23376;&#31181;&#32676;&#19978;&#20855;&#26377;&#39640;&#35823;&#25253;&#29575;&#12290;</title><link>http://arxiv.org/abs/2212.02701</link><description>&lt;p&gt;
&#35770;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#30340;&#19981;&#21487;&#38752;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Discredibility of Membership Inference Attacks. (arXiv:2212.02701v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02701
&lt;/p&gt;
&lt;p&gt;
&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#19981;&#21487;&#38752;&#65292;&#24212;&#35813;&#35880;&#24910;&#20351;&#29992;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#21487;&#20197;&#35782;&#21035;&#21040;&#30340;&#30830;&#20999;&#25104;&#21592;&#26679;&#26412;&#30340;&#23376;&#31181;&#32676;&#19978;&#20855;&#26377;&#39640;&#35823;&#25253;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#30740;&#31350;&#22312;&#25935;&#24863;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#21487;&#33021;&#27844;&#28431;&#25968;&#25454;&#30340;&#28508;&#22312;&#38382;&#39064;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#21508;&#31181;&#25104;&#21592;&#25512;&#29702; (MI) &#25915;&#20987;&#26469;&#30830;&#23450;&#26679;&#26412;&#26159;&#21542;&#23646;&#20110;&#35757;&#32451;&#38598;&#12290;&#38382;&#39064;&#22312;&#20110;&#36825;&#20123;&#25915;&#20987;&#26159;&#21542;&#21487;&#38752;&#22320;&#24212;&#29992;&#20110;&#23454;&#36341;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;MI&#27169;&#22411;&#32463;&#24120;&#23558;&#38468;&#36817;&#30340;&#38750;&#25104;&#21592;&#26679;&#26412;&#35823;&#20998;&#31867;&#20026;&#25104;&#21592;&#65292;&#24182;&#22312;&#21487;&#20197;&#35782;&#21035;&#21040;&#30340;&#30830;&#20999;&#25104;&#21592;&#26679;&#26412;&#30340;&#23376;&#31181;&#32676;&#19978;&#20855;&#26377;&#39640;&#35823;&#25253;&#29575;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;MI&#25915;&#20987;&#30340;&#23454;&#38469;&#24212;&#29992;&#65292;&#20854;&#20013;&#36825;&#20010;&#38382;&#39064;&#23545;&#30495;&#23454;&#19990;&#30028;&#20135;&#29983;&#20102;&#24433;&#21709;&#12290;&#22312;&#36825;&#37324;&#65292;MI&#25915;&#20987;&#34987;&#22806;&#37096;&#23457;&#35745;&#24072;&#65288;&#35843;&#26597;&#21592;&#65289;&#29992;&#26469;&#21521;&#27861;&#23448;/&#38506;&#23457;&#22242;&#23637;&#31034;&#34987;&#23457;&#35745;&#20154;&#38750;&#27861;&#20351;&#29992;&#25935;&#24863;&#25968;&#25454;&#12290;&#30001;&#20110;MI&#25915;&#20987;&#22312;&#25104;&#21592;&#30340;&#23376;&#31181;&#32676;&#19978;&#20855;&#26377;&#39640;&#20551;&#38451;&#24615;&#29575;&#65292;&#34987;&#23457;&#35745;&#20154;&#36890;&#36807;&#25581;&#31034;MI&#25915;&#20987;&#30340;&#24615;&#33021;&#26469;&#25361;&#25112;&#23457;&#35745;&#24072;&#30340;&#21487;&#20449;&#24230;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;MI&#25915;&#20987;&#19981;&#21487;&#38752;&#65292;&#24212;&#35880;&#24910;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the wide-spread application of machine learning models, it has become critical to study the potential data leakage of models trained on sensitive data. Recently, various membership inference (MI) attacks are proposed to determine if a sample was part of the training set or not. The question is whether these attacks can be reliably used in practice. We show that MI models frequently misclassify neighboring nonmember samples of a member sample as members. In other words, they have a high false positive rate on the subpopulations of the exact member samples that they can identify. We then showcase a practical application of MI attacks where this issue has a real-world repercussion. Here, MI attacks are used by an external auditor (investigator) to show to a judge/jury that an auditee unlawfully used sensitive data. Due to the high false positive rate of MI attacks on member's subpopulations, auditee challenges the credibility of the auditor by revealing the performance of the MI atta
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31163;&#25955;&#20248;&#21270;&#38382;&#39064;-&#21018;&#24615;&#25240;&#32440;&#28216;&#25103;&#65292;&#35813;&#28216;&#25103;&#21487;&#20197;&#25193;&#23637;&#21018;&#24615;&#25240;&#32440;&#30340;&#28508;&#21147;&#65292;&#20351;&#20854;&#24471;&#21040;&#38024;&#23545;&#24212;&#29992;&#29305;&#23450;&#30340;&#25240;&#30165;&#22270;&#26696;&#65292;&#20174;&#32780;&#21487;&#20197;&#24471;&#21040;&#26085;&#24120;&#29289;&#21697;&#30340;&#26032;&#39062;&#12289;&#21487;&#25240;&#21472;&#21644;&#23454;&#29992;&#30340;&#35774;&#35745;&#12290;</title><link>http://arxiv.org/abs/2211.13219</link><description>&lt;p&gt;
&#33258;&#21160;&#21270;&#21018;&#24615;&#25240;&#32440;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Automating Rigid Origami Design. (arXiv:2211.13219v2 [cs.GR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13219
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31163;&#25955;&#20248;&#21270;&#38382;&#39064;-&#21018;&#24615;&#25240;&#32440;&#28216;&#25103;&#65292;&#35813;&#28216;&#25103;&#21487;&#20197;&#25193;&#23637;&#21018;&#24615;&#25240;&#32440;&#30340;&#28508;&#21147;&#65292;&#20351;&#20854;&#24471;&#21040;&#38024;&#23545;&#24212;&#29992;&#29305;&#23450;&#30340;&#25240;&#30165;&#22270;&#26696;&#65292;&#20174;&#32780;&#21487;&#20197;&#24471;&#21040;&#26085;&#24120;&#29289;&#21697;&#30340;&#26032;&#39062;&#12289;&#21487;&#25240;&#21472;&#21644;&#23454;&#29992;&#30340;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21018;&#24615;&#25240;&#32440;&#22312;&#24456;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#23637;&#29616;&#20986;&#20102;&#20854;&#28508;&#21147;&#65292;&#28982;&#32780;&#30446;&#21069;&#30340;&#21018;&#24615;&#25240;&#32440;&#35126;&#30385;&#22270;&#26696;&#35774;&#35745;&#20027;&#35201;&#20381;&#36182;&#20110;&#24050;&#30693;&#30340;&#25340;&#36148;&#12290;&#36825;&#20005;&#37325;&#38480;&#21046;&#20102;&#21487;&#20197;&#21019;&#24314;&#30340;&#22270;&#26696;&#30340;&#22810;&#26679;&#24615;&#21644;&#26032;&#39062;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22312;&#26368;&#36817;&#24320;&#21457;&#30340;&#19977;&#21333;&#20803;&#21407;&#21017;&#30340;&#22522;&#30784;&#19978;&#65292;&#23558;&#21018;&#24615;&#25240;&#32440;&#35774;&#35745;&#25104;&#19968;&#31181;&#31163;&#25955;&#20248;&#21270;&#38382;&#39064;&#65292;&#21363;&#21018;&#24615;&#25240;&#32440;&#28216;&#25103;&#12290;&#25105;&#20204;&#30340;&#23454;&#29616;&#20801;&#35768;&#31616;&#21333;&#23450;&#20041;&#22810;&#26679;&#30340;&#30446;&#26631;&#65292;&#20174;&#32780;&#36827;&#19968;&#27493;&#25193;&#23637;&#21018;&#24615;&#25240;&#32440;&#30340;&#28508;&#21147;&#65292;&#24471;&#21040;&#38024;&#23545;&#24212;&#29992;&#29305;&#23450;&#30340;&#25240;&#30165;&#22270;&#26696;&#12290;&#25105;&#20204;&#36890;&#36807;&#22810;&#31181;&#25628;&#32034;&#26041;&#27861;&#22312;&#20960;&#20010;&#26696;&#20363;&#30740;&#31350;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#20844;&#24335;&#30340;&#28789;&#27963;&#24615;&#12290;&#25105;&#20204;&#19981;&#20165;&#33021;&#22815;&#26500;&#36896;&#20986;&#36817;&#20284;&#32473;&#23450;&#30446;&#26631;&#24418;&#29366;&#30340;&#21508;&#31181;&#22270;&#26696;&#65292;&#32780;&#19988;&#36824;&#33021;&#22815;&#25351;&#23450;&#22522;&#20110;&#20989;&#25968;&#30340;&#25277;&#35937;&#22870;&#21169;&#65292;&#20174;&#32780;&#24471;&#21040;&#26085;&#24120;&#29289;&#21697;&#30340;&#26032;&#39062;&#12289;&#21487;&#25240;&#21472;&#21644;&#23454;&#29992;&#30340;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rigid origami has shown potential in large diversity of practical applications. However, current rigid origami crease pattern design mostly relies on known tessellations. This strongly limits the diversity and novelty of patterns that can be created. In this work, we build upon the recently developed principle of three units method to formulate rigid origami design as a discrete optimization problem, the rigid origami game. Our implementation allows for a simple definition of diverse objectives and thereby expands the potential of rigid origami further to optimized, application-specific crease patterns. We showcase the flexibility of our formulation through use of a diverse set of search methods in several illustrative case studies. We are not only able to construct various patterns that approximate given target shapes, but to also specify abstract, function-based rewards which result in novel, foldable and functional designs for everyday objects.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#27169;&#22411;&#37325;&#26032;&#32534;&#31243;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#38024;&#23545;&#22797;&#26434;&#36755;&#20837;&#25968;&#25454;&#25552;&#20986;&#20102;&#36755;&#20837;&#20381;&#36182;NMR&#33539;&#24335;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#36827;&#34892;&#38899;&#20048;&#39118;&#26684;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2211.01317</link><description>&lt;p&gt;
&#22522;&#20110;&#36328;&#27169;&#24577;&#31070;&#32463;&#27169;&#22411;&#37325;&#26032;&#32534;&#31243;&#30340;&#20302;&#36164;&#28304;&#38899;&#20048;&#39118;&#26684;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Low-Resource Music Genre Classification with Cross-Modal Neural Model Reprogramming. (arXiv:2211.01317v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01317
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#27169;&#22411;&#37325;&#26032;&#32534;&#31243;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#38024;&#23545;&#22797;&#26434;&#36755;&#20837;&#25968;&#25454;&#25552;&#20986;&#20102;&#36755;&#20837;&#20381;&#36182;NMR&#33539;&#24335;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#36827;&#34892;&#38899;&#20048;&#39118;&#26684;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#22312;&#22788;&#29702;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#30340;&#20219;&#21153;&#26102;&#23637;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#22788;&#29702;&#30446;&#26631;&#39046;&#22495;&#25968;&#25454;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#36164;&#28304;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#27169;&#22411;&#37325;&#26032;&#32534;&#31243; (NMR) &#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#20302;&#36164;&#28304;&#38899;&#20048;&#20998;&#31867;&#12290;NMR&#26088;&#22312;&#36890;&#36807;&#20462;&#25913;&#20923;&#32467;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#36755;&#20837;&#65292;&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#20174;&#28304;&#22495;&#37325;&#26032;&#35843;&#25972;&#29992;&#20110;&#30446;&#26631;&#22495;&#12290;&#38500;&#20102;&#24050;&#30693;&#30340;&#19982;&#36755;&#20837;&#26080;&#20851;&#30340;&#37325;&#26032;&#32534;&#31243;&#26041;&#27861;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#20808;&#36827;&#30340;&#37325;&#26032;&#32534;&#31243;&#33539;&#24335;&#65306;&#36755;&#20837;&#20381;&#36182;NMR&#65292;&#20197;&#22686;&#21152;&#23545;&#22797;&#26434;&#36755;&#20837;&#25968;&#25454;&#65288;&#22914;&#38899;&#39057;&#65289;&#30340;&#36866;&#24212;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#36825;&#31181;&#37325;&#26032;&#32534;&#31243;&#26041;&#27861;&#65292;&#22522;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#39044;&#35757;&#32451;&#30340;&#31070;&#32463;&#27169;&#22411;&#25104;&#21151;&#22320;&#36827;&#34892;&#38899;&#20048;&#39118;&#26684;&#20998;&#31867;&#12290;&#25152;&#25552;&#20986;&#30340;&#20004;&#31181;&#36755;&#20837;&#30456;&#20851;&#30340;NMR&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transfer learning (TL) approaches have shown promising results when handling tasks with limited training data. However, considerable memory and computational resources are often required for fine-tuning pre-trained neural networks with target domain data. In this work, we introduce a novel method for leveraging pre-trained models for low-resource (music) classification based on the concept of Neural Model Reprogramming (NMR). NMR aims at re-purposing a pre-trained model from a source domain to a target domain by modifying the input of a frozen pre-trained model. In addition to the known, input-independent, reprogramming method, we propose an advanced reprogramming paradigm: Input-dependent NMR, to increase adaptability to complex input data such as musical audio. Experimental results suggest that a neural model pre-trained on large-scale datasets can successfully perform music genre classification by using this reprogramming method. The two proposed Input-dependent NMR TL methods outpe
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#30417;&#30563;&#27169;&#22411;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#33258;&#21160;&#35780;&#20272;&#21457;&#38899;&#38556;&#30861;&#20005;&#37325;&#31243;&#24230;&#30340;&#26041;&#27861;&#65292;&#22312;&#36739;&#23569;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#21521;&#20256;&#32479;&#26041;&#27861;&#30340;&#20248;&#21270;&#65292;&#24182;&#19988;&#30456;&#23545;&#25552;&#39640;&#20102;1.25%&#30340;F1-score&#12290;</title><link>http://arxiv.org/abs/2210.15387</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#27169;&#22411;&#19982;&#22810;&#20219;&#21153;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#21457;&#38899;&#38556;&#30861;&#33258;&#21160;&#20005;&#37325;&#31243;&#24230;&#35780;&#20272;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Automatic Severity Assessment of Dysarthric speech by using Self-supervised Model with Multi-task Learning. (arXiv:2210.15387v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.15387
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#30417;&#30563;&#27169;&#22411;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#33258;&#21160;&#35780;&#20272;&#21457;&#38899;&#38556;&#30861;&#20005;&#37325;&#31243;&#24230;&#30340;&#26041;&#27861;&#65292;&#22312;&#36739;&#23569;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#21521;&#20256;&#32479;&#26041;&#27861;&#30340;&#20248;&#21270;&#65292;&#24182;&#19988;&#30456;&#23545;&#25552;&#39640;&#20102;1.25%&#30340;F1-score&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#35780;&#20272;&#21457;&#38899;&#38556;&#30861;&#30340;&#20005;&#37325;&#31243;&#24230;&#23545;&#20110;&#25345;&#32493;&#27835;&#30103;&#21644;&#24247;&#22797;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#33719;&#21462;&#38750;&#20856;&#22411;&#21457;&#38899;&#30340;&#38590;&#24230;&#36739;&#22823;&#65292;&#24448;&#24448;&#20250;&#23548;&#33268;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#21160;&#35780;&#20272;&#21457;&#38899;&#38556;&#30861;&#20005;&#37325;&#31243;&#24230;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#33258;&#30417;&#30563;&#27169;&#22411;&#19982;&#22810;&#20219;&#21153;&#23398;&#20064;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#32852;&#21512;&#35757;&#32451;Wav2vec 2.0 XLS-R&#36827;&#34892;&#20004;&#20010;&#19981;&#21516;&#30340;&#20219;&#21153;&#65306;&#20005;&#37325;&#31243;&#24230;&#20998;&#31867;&#21644;&#36741;&#21161;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#12290;&#23545;&#20110;&#22522;&#20934;&#23454;&#39564;&#65292;&#25105;&#20204;&#37319;&#29992;&#25163;&#24037;&#21046;&#20316;&#30340;&#22768;&#23398;&#29305;&#24449;&#21644;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#65292;&#22914;SVM&#12289;MLP&#21644;XGBoost&#12290;&#22312;&#38889;&#22269;&#21457;&#38899;&#38556;&#30861;QoLT&#25968;&#25454;&#24211;&#19978;&#36827;&#34892;&#25506;&#31350;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20248;&#20110;&#20256;&#32479;&#30340;&#22522;&#20934;&#26041;&#27861;&#65292;F1-score&#30456;&#23545;&#25552;&#39640;1.25%&#12290;&#27492;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#36229;&#36807;&#20102;&#27809;&#26377;ASR&#22836;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;10.61%&#30340;&#30456;&#23545;&#30334;&#20998;&#27604;&#25552;&#39640;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#22914;&#20309;&#24433;&#21709;&#38556;&#30861;&#20005;&#37325;&#31243;&#24230;&#30340;&#35780;&#20272;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic assessment of dysarthric speech is essential for sustained treatments and rehabilitation. However, obtaining atypical speech is challenging, often leading to data scarcity issues. To tackle the problem, we propose a novel automatic severity assessment method for dysarthric speech, using the self-supervised model in conjunction with multi-task learning. Wav2vec 2.0 XLS-R is jointly trained for two different tasks: severity classification and auxiliary automatic speech recognition (ASR). For the baseline experiments, we employ hand-crafted acoustic features and machine learning classifiers such as SVM, MLP, and XGBoost. Explored on the Korean dysarthric speech QoLT database, our model outperforms the traditional baseline methods, with a relative percentage increase of 1.25% for F1-score. In addition, the proposed model surpasses the model trained without ASR head, achieving 10.61% relative percentage improvements. Furthermore, we present how multi-task learning affects the seve
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#28082;&#20307;&#21160;&#21147;&#23398;&#27169;&#25311;&#21644;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#23545;&#20302;&#36136;&#37327;&#26197;&#30340;SZ&#36890;&#37327;&#36136;&#37327;&#65288;$Y$-$M$&#65289;&#20851;&#31995;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#65292;&#21457;&#29616;&#36890;&#36807;&#31616;&#21333;&#22320;&#23558;$Y \rightarrow Y(1 + M_*/M_\mathrm{gas})$&#21487;&#20197;&#20351;&#35813;&#20851;&#31995;&#26174;&#33879;&#33258;&#30456;&#20284;&#65292;&#36825;&#23545;&#20110;&#20302;&#36136;&#37327;&#26143;&#22242;&#21644;&#26143;&#31995;&#32676;&#26159;&#19968;&#31181;&#31283;&#20581;&#30340;&#22810;&#27874;&#38271;&#36136;&#37327;&#20195;&#29702;&#12290;</title><link>http://arxiv.org/abs/2209.02075</link><description>&lt;p&gt;
&#22312;&#20302;&#36136;&#37327;&#26197;&#30340;&#24773;&#20917;&#19979;&#65292;&#31526;&#21495;&#22238;&#24402;&#21644;&#24378;&#32422;&#26463;&#23545;SZ&#36890;&#37327;&#36136;&#37327;&#65288;$Y$-$M$&#65289;&#20851;&#31995;&#30340;&#25913;&#36827;&#65306;&#23545;&#37325;&#23376;&#21453;&#39304;&#30340;&#20840;&#38754;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
The SZ flux-mass ($Y$-$M$) relation at low halo masses: improvements with symbolic regression and strong constraints on baryonic feedback. (arXiv:2209.02075v2 [astro-ph.CO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.02075
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#28082;&#20307;&#21160;&#21147;&#23398;&#27169;&#25311;&#21644;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#23545;&#20302;&#36136;&#37327;&#26197;&#30340;SZ&#36890;&#37327;&#36136;&#37327;&#65288;$Y$-$M$&#65289;&#20851;&#31995;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#65292;&#21457;&#29616;&#36890;&#36807;&#31616;&#21333;&#22320;&#23558;$Y \rightarrow Y(1 + M_*/M_\mathrm{gas})$&#21487;&#20197;&#20351;&#35813;&#20851;&#31995;&#26174;&#33879;&#33258;&#30456;&#20284;&#65292;&#36825;&#23545;&#20110;&#20302;&#36136;&#37327;&#26143;&#22242;&#21644;&#26143;&#31995;&#32676;&#26159;&#19968;&#31181;&#31283;&#20581;&#30340;&#22810;&#27874;&#38271;&#36136;&#37327;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26469;&#33258;&#27963;&#21160;&#26143;&#31995;&#26680;&#21644;&#36229;&#26032;&#26143;&#30340;&#21453;&#39304;&#20250;&#24433;&#21709;CMB&#35843;&#26597;&#20013;&#26197;&#30340;&#38598;&#25104;SZ&#36890;&#37327;&#65288;$Y_\mathrm{SZ}$&#65289;&#30340;&#27979;&#37327;&#32467;&#26524;&#65292;&#24182;&#23548;&#33268;&#20854;&#19982;&#26197;&#36136;&#37327;&#65288;$Y_\mathrm{SZ}-M$&#65289;&#30340;&#20851;&#31995;&#20559;&#31163;&#32500;&#37324;&#23450;&#29702;&#30340;&#33258;&#30456;&#20284;&#24130;&#24459;&#39044;&#27979;&#12290;&#25105;&#20204;&#20351;&#29992;&#20855;&#26377;&#24191;&#27867;&#21453;&#39304;&#26041;&#26696;&#21464;&#21270;&#30340;&#28082;&#20307;&#21160;&#21147;&#23398;&#27169;&#25311;&#22871;&#25151;CAMELS&#23545;&#36825;&#31181;&#20559;&#31163;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#12290;&#25105;&#20204;&#20351;&#29992;&#20004;&#31181;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#65288;&#38543;&#26426;&#26862;&#26519;&#21644;&#31526;&#21495;&#22238;&#24402;&#65289;&#30340;&#32452;&#21512;&#26469;&#25628;&#32034;$Y-M$&#20851;&#31995;&#30340;&#31867;&#27604;&#29289;&#65292;&#36825;&#20123;&#31867;&#27604;&#29289;&#23545;&#20302;&#36136;&#37327;&#65288;$M \lesssim 10 ^ {14} \&#65292;h ^ {-1} \&#65292;M_\odot$&#65289;&#30340;&#21453;&#39304;&#36807;&#31243;&#26356;&#21152;&#31283;&#20581;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#20851;&#31995;&#20013;&#31616;&#21333;&#22320;&#23558;$Y \rightarrow Y(1 + M_*/M_\mathrm{gas})$&#21487;&#20351;&#20854;&#26174;&#33879;&#33258;&#30456;&#20284;&#65292;&#36825;&#21487;&#20197;&#29992;&#20316;&#20302;&#36136;&#37327;&#26143;&#22242;&#21644;&#26143;&#31995;&#32676;&#30340;&#31283;&#20581;&#22810;&#27874;&#38271;&#36136;&#37327;&#20195;&#29702;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#35770;&#36824;&#21487;&#20197;&#26222;&#36941;&#29992;&#20110;&#25913;&#36827;&#20854;&#20182;&#22825;&#20307;&#29289;&#29702;&#30340;&#26377;&#25928;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;
Feedback from active galactic nuclei (AGN) and supernovae can affect measurements of integrated SZ flux of halos ($Y_\mathrm{SZ}$) from CMB surveys, and cause its relation with the halo mass ($Y_\mathrm{SZ}-M$) to deviate from the self-similar power-law prediction of the virial theorem. We perform a comprehensive study of such deviations using CAMELS, a suite of hydrodynamic simulations with extensive variations in feedback prescriptions. We use a combination of two machine learning tools (random forest and symbolic regression) to search for analogues of the $Y-M$ relation which are more robust to feedback processes for low masses ($M\lesssim 10^{14}\, h^{-1} \, M_\odot$); we find that simply replacing $Y\rightarrow Y(1+M_*/M_\mathrm{gas})$ in the relation makes it remarkably self-similar. This could serve as a robust multiwavelength mass proxy for low-mass clusters and galaxy groups. Our methodology can also be generally useful to improve the domain of validity of other astrophysical 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25991;&#26412;&#21040;&#38899;&#39057;&#29983;&#25104;&#26694;&#26550;&#65292;&#20854;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#31163;&#25955;&#25193;&#25955;&#35299;&#30721;&#22120;&#26469;&#22686;&#24378;&#29983;&#25104;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2207.09983</link><description>&lt;p&gt;
Diffsound&#65306;&#31163;&#25955;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#25991;&#26412;&#21040;&#38899;&#39057;
&lt;/p&gt;
&lt;p&gt;
Diffsound: Discrete Diffusion Model for Text-to-sound Generation. (arXiv:2207.09983v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.09983
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25991;&#26412;&#21040;&#38899;&#39057;&#29983;&#25104;&#26694;&#26550;&#65292;&#20854;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#31163;&#25955;&#25193;&#25955;&#35299;&#30721;&#22120;&#26469;&#22686;&#24378;&#29983;&#25104;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20204;&#23545;&#20110;&#25152;&#38656;&#22768;&#25928;&#30340;&#29983;&#25104;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#25991;&#26412;&#21040;&#38899;&#39057;&#29983;&#25104;&#26694;&#26550;&#65292;&#21253;&#25324;&#25991;&#26412;&#32534;&#30721;&#22120;&#65292;&#30690;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;(VQ-VAE)&#65292;&#35299;&#30721;&#22120;&#21644;&#35821;&#38899;&#32534;&#30721;&#22120;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#35774;&#35745;&#35299;&#30721;&#22120;&#26041;&#38754;&#65292;&#20256;&#32479;&#30340;&#33258;&#22238;&#24402;&#35299;&#30721;&#22120;&#39044;&#27979;&#30340;&#23454;&#26102;&#39057;&#35889;&#23384;&#22312;&#19981;&#24179;&#28369;&#24773;&#20917;&#65292;&#19981;&#36866;&#29992;&#20110;&#35813;&#20219;&#21153;&#65292;&#22240;&#27492;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31163;&#25955;&#25193;&#25955;&#35299;&#30721;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating sound effects that humans want is an important topic. However, there are few studies in this area for sound generation. In this study, we investigate generating sound conditioned on a text prompt and propose a novel text-to-sound generation framework that consists of a text encoder, a Vector Quantized Variational Autoencoder (VQ-VAE), a decoder, and a vocoder. The framework first uses the decoder to transfer the text features extracted from the text encoder to a mel-spectrogram with the help of VQ-VAE, and then the vocoder is used to transform the generated mel-spectrogram into a waveform. We found that the decoder significantly influences the generation performance. Thus, we focus on designing a good decoder in this study. We begin with the traditional autoregressive decoder, which has been proved as a state-of-the-art method in previous sound generation works. However, the AR decoder always predicts the mel-spectrogram tokens one by one in order, which introduces the unidi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20851;&#20110;&#25351;&#31034;&#26465;&#20214;&#21477;&#30340;&#19977;&#20803;&#35821;&#20041;&#23398;&#21644;&#27010;&#29575;&#65292;&#26500;&#24314;&#20102;&#20004;&#31181;&#26465;&#20214;&#25512;&#29702;&#36923;&#36753;&#65306;&#20174;&#30830;&#23450;&#21069;&#25552;&#36827;&#34892;&#25512;&#29702;&#30340;&#36923;&#36753;C&#21644;&#20174;&#19981;&#30830;&#23450;&#21069;&#25552;&#36827;&#34892;&#25512;&#29702;&#30340;&#36923;&#36753;U&#12290;&#23427;&#20204;&#37117;&#33021;&#29992;&#20110;&#20998;&#26512;&#26465;&#20214;&#25512;&#29702;&#30340;&#26377;&#25928;&#24615;&#65292;&#34429;&#28982;U&#19981;&#36981;&#23432;&#20551;&#35328;&#25512;&#29702;&#35268;&#21017;&#12290;</title><link>http://arxiv.org/abs/2207.08276</link><description>&lt;p&gt;
&#25351;&#31034;&#26465;&#20214;&#21477;&#19979;&#30340;&#30830;&#23450;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Certain and Uncertain Inference with Indicative Conditionals. (arXiv:2207.08276v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.08276
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20851;&#20110;&#25351;&#31034;&#26465;&#20214;&#21477;&#30340;&#19977;&#20803;&#35821;&#20041;&#23398;&#21644;&#27010;&#29575;&#65292;&#26500;&#24314;&#20102;&#20004;&#31181;&#26465;&#20214;&#25512;&#29702;&#36923;&#36753;&#65306;&#20174;&#30830;&#23450;&#21069;&#25552;&#36827;&#34892;&#25512;&#29702;&#30340;&#36923;&#36753;C&#21644;&#20174;&#19981;&#30830;&#23450;&#21069;&#25552;&#36827;&#34892;&#25512;&#29702;&#30340;&#36923;&#36753;U&#12290;&#23427;&#20204;&#37117;&#33021;&#29992;&#20110;&#20998;&#26512;&#26465;&#20214;&#25512;&#29702;&#30340;&#26377;&#25928;&#24615;&#65292;&#34429;&#28982;U&#19981;&#36981;&#23432;&#20551;&#35328;&#25512;&#29702;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#33258;&#28982;&#35821;&#35328;&#20013;&#30340;&#25351;&#31034;&#26465;&#20214;&#21477;&#65292;&#25552;&#20986;&#20102;&#19977;&#20803;&#35821;&#20041;&#23398;&#30340;&#30495;&#20540;&#26465;&#20214;&#21644;&#27010;&#29575;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#22522;&#20110;W.Cooper&#26368;&#20808;&#25552;&#20986;&#30340;&#19977;&#20803;&#30495;&#20540;&#26465;&#20214;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#26465;&#20214;&#25512;&#29702;&#36923;&#36753;&#65306;&#65288;i&#65289;&#20174;&#30830;&#23450;&#21069;&#25552;&#36827;&#34892;&#25512;&#29702;&#30340;&#36923;&#36753;C&#65307;&#65288;ii&#65289;&#20174;&#19981;&#30830;&#23450;&#21069;&#25552;&#36827;&#34892;&#25512;&#29702;&#30340;&#36923;&#36753;U&#12290;&#28982;&#32780;&#65292;C&#23545;&#20110;&#26465;&#20214;&#21477;&#26159;&#21333;&#35843;&#30340;&#65292;&#32780;U&#21017;&#19981;&#26159;&#65292;&#32780;&#19988;C&#36981;&#23432;&#20551;&#35328;&#25512;&#29702;&#35268;&#21017;&#65292;U&#21482;&#26377;&#22312;&#19968;&#23450;&#38480;&#21046;&#19979;&#25165;&#33021;&#36981;&#23432;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#36825;&#20004;&#31181;&#26694;&#26550;&#20013;&#36827;&#34892;&#25512;&#29702;&#30340;&#19977;&#20803;&#21644;&#27010;&#29575;&#34920;&#36798;&#20043;&#38388;&#30340;&#31995;&#32479;&#23545;&#24212;&#20851;&#31995;&#65292;&#24182;&#21033;&#29992;&#36825;&#20004;&#20010;&#31995;&#32479;&#30340;&#21306;&#21035;&#65292;&#29305;&#21035;&#26159;&#23545;McGee&#20851;&#20110;&#20551;&#35328;&#25512;&#29702;&#35268;&#21017;&#30340;&#35868;&#39064;&#25552;&#20986;&#26032;&#30340;&#35299;&#37322;&#12290;&#32467;&#26524;&#26159;&#19968;&#20010;&#20851;&#20110;&#25351;&#31034;&#26465;&#20214;&#21477;&#30340;&#35821;&#20041;&#21644;&#35748;&#35782;&#35770;&#30340;&#32479;&#19968;&#35299;&#37322;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#24212;&#29992;&#20110;&#20998;&#26512;&#26465;&#20214;&#25512;&#29702;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper develops a trivalent semantics for the truth conditions and the probability of the natural language indicative conditional. Our framework rests on trivalent truth conditions first proposed by W. Cooper and yields two logics of conditional reasoning: (i) a logic C of inference from certain premises; and (ii) a logic U of inference from uncertain premises. But whereas C is monotonic for the conditional, U is not, and whereas C obeys Modus Ponens, U does not without restrictions. We show systematic correspondences between trivalent and probabilistic representations of inferences in either framework, and we use the distinction between the two systems to cast light, in particular, on McGee's puzzle about Modus Ponens. The result is a unified account of the semantics and epistemology of indicative conditionals that can be fruitfully applied to analyzing the validity of conditional inferences.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#21487;&#35299;&#37322;&#24615;&#26234;&#33021;&#23398;&#20064;&#35786;&#26029;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#28145;&#24230;&#23398;&#20064;&#30340;&#34920;&#31034;&#23398;&#20064;&#33021;&#21147;&#21644;&#22522;&#20110;&#24515;&#29702;&#27979;&#37327;&#30340;&#26041;&#27861;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#20351;&#24471;&#22312;&#26234;&#24935;&#25945;&#32946;&#20013;&#33021;&#22815;&#24179;&#34913;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2207.03122</link><description>&lt;p&gt;
&#26234;&#33021;&#21270;&#23398;&#20064;&#35786;&#26029;&#32479;&#19968;&#21487;&#35299;&#37322;&#24615;&#26694;&#26550;&#22312;&#26234;&#24935;&#25945;&#32946;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
A Unified Interpretable Intelligent Learning Diagnosis Framework for Smart Education. (arXiv:2207.03122v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.03122
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#21487;&#35299;&#37322;&#24615;&#26234;&#33021;&#23398;&#20064;&#35786;&#26029;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#28145;&#24230;&#23398;&#20064;&#30340;&#34920;&#31034;&#23398;&#20064;&#33021;&#21147;&#21644;&#22522;&#20110;&#24515;&#29702;&#27979;&#37327;&#30340;&#26041;&#27861;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#20351;&#24471;&#22312;&#26234;&#24935;&#25945;&#32946;&#20013;&#33021;&#22815;&#24179;&#34913;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#23398;&#20064;&#35786;&#26029;&#26159;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#30340;&#37325;&#35201;&#24341;&#25806;&#65292;&#26088;&#22312;&#20272;&#35745;&#23398;&#20064;&#32773;&#24403;&#21069;&#30340;&#30693;&#35782;&#25484;&#25569;&#29366;&#20917;&#24182;&#39044;&#27979;&#20854;&#26410;&#26469;&#30340;&#23398;&#20064;&#34920;&#29616;&#12290;&#20256;&#32479;&#30340;&#23398;&#20064;&#35786;&#26029;&#26041;&#27861;&#22312;&#35786;&#26029;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#20043;&#38388;&#38590;&#20197;&#24179;&#34913;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#22522;&#20110;&#24515;&#29702;&#27979;&#37327;&#30340;&#23398;&#20064;&#35786;&#26029;&#26041;&#27861;&#36890;&#36807;&#35748;&#30693;&#21442;&#25968;&#25552;&#20379;&#20102;&#19968;&#23450;&#39046;&#22495;&#35299;&#37322;&#65292;&#20294;&#23427;&#20204;&#23545;&#20110;&#22823;&#35268;&#27169;&#23398;&#20064;&#25968;&#25454;&#30340;&#24314;&#27169;&#33021;&#21147;&#19981;&#36275;&#12290;&#32780;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#23398;&#20064;&#35786;&#26029;&#26041;&#27861;&#34429;&#28982;&#25552;&#39640;&#20102;&#23398;&#20064;&#34920;&#29616;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#20854;&#20869;&#22312;&#30340;&#40657;&#30418;&#29305;&#24615;&#23548;&#33268;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#65292;&#20351;&#24471;&#22312;&#25945;&#32946;&#24212;&#29992;&#20013;&#26080;&#27861;&#20449;&#20219;&#20854;&#32467;&#26524;&#12290;&#20026;&#35299;&#20915;&#20197;&#19978;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#32479;&#19968;&#21487;&#35299;&#37322;&#26234;&#33021;&#23398;&#20064;&#35786;&#26029;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#32467;&#21512;&#20102;&#28145;&#24230;&#23398;&#20064;&#30340;&#24378;&#22823;&#34920;&#31034;&#23398;&#20064;&#33021;&#21147;&#21644;&#22522;&#20110;&#24515;&#29702;&#27979;&#37327;&#30340;&#26041;&#27861;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#35813;&#26694;&#26550;&#30001;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35786;&#26029;&#27169;&#22359;&#21644;&#35748;&#30693;&#21442;&#25968;&#35299;&#37322;&#27169;&#22359;&#20004;&#37096;&#20998;&#32452;&#25104;&#65292;&#20004;&#20010;&#27169;&#22359;&#30456;&#20114;&#34917;&#20805;&#65292;&#20351;&#24471;&#26694;&#26550;&#22312;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#20043;&#38388;&#36798;&#21040;&#24179;&#34913;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#23398;&#20064;&#35786;&#26029;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intelligent learning diagnosis is a critical engine of intelligent tutoring systems, which aims to estimate learners' current knowledge mastery status and predict their future learning performance. The significant challenge with traditional learning diagnosis methods is the inability to balance diagnostic accuracy and interpretability. Although the existing psychometric-based learning diagnosis methods provide some domain interpretation through cognitive parameters, they have insufficient modeling capability with a shallow structure for large-scale learning data. While the deep learning-based learning diagnosis methods have improved the accuracy of learning performance prediction, their inherent black-box properties lead to a lack of interpretability, making their results untrustworthy for educational applications. To settle the above problem, the proposed unified interpretable intelligent learning diagnosis framework, which benefits from the powerful representation learning ability of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#25968;&#25454;&#39537;&#21160;&#31639;&#23376;&#23398;&#20064;&#26694;&#26550;OFormer&#65292;&#23427;&#21487;&#20197;&#24191;&#27867;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#65292;&#24182;&#19988;&#20855;&#26377;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#24403;&#30340;&#20934;&#30830;&#24615;&#65292;&#29978;&#33267;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#20855;&#26377;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2205.13671</link><description>&lt;p&gt;
&#29992;Transformer&#36827;&#34892;&#20559;&#24494;&#20998;&#26041;&#31243;&#31639;&#23376;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Transformer for Partial Differential Equations' Operator Learning. (arXiv:2205.13671v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.13671
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#25968;&#25454;&#39537;&#21160;&#31639;&#23376;&#23398;&#20064;&#26694;&#26550;OFormer&#65292;&#23427;&#21487;&#20197;&#24191;&#27867;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#65292;&#24182;&#19988;&#20855;&#26377;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#24403;&#30340;&#20934;&#30830;&#24615;&#65292;&#29978;&#33267;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#20855;&#26377;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#35299;&#31639;&#23376;&#23398;&#20064;&#36817;&#24180;&#26469;&#24050;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#33539;&#24335;&#65292;&#29992;&#20110;&#36817;&#20284;&#22522;&#30784;&#35299;&#12290;&#35299;&#31639;&#23376;&#36890;&#24120;&#30001;&#22522;&#20110;&#38382;&#39064;&#29305;&#23450;&#24402;&#32435;&#20559;&#35265;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21442;&#25968;&#21270;&#12290;&#20363;&#22914;&#65292;&#21367;&#31215;&#25110;&#22270;&#31070;&#32463;&#32593;&#32476;&#21033;&#29992;&#20102;&#20989;&#25968;&#20540;&#34987;&#37319;&#26679;&#30340;&#26412;&#22320;&#32593;&#26684;&#32467;&#26500;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#27880;&#24847;&#21147;&#26426;&#21046;&#25552;&#20379;&#20102;&#19968;&#31181;&#28789;&#27963;&#30340;&#26041;&#24335;&#26469;&#38544;&#24335;&#21033;&#29992;&#36755;&#20837;&#20013;&#30340;&#27169;&#24335;&#65292;&#20197;&#21450;&#20219;&#24847;&#26597;&#35810;&#20301;&#32622;&#21644;&#36755;&#20837;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#25968;&#25454;&#39537;&#21160;&#31639;&#23376;&#23398;&#20064;&#26694;&#26550;&#65292;&#31216;&#20026;Operator Transformer (OFormer)&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#24314;&#31435;&#22312;&#33258;&#27880;&#24847;&#12289;&#20132;&#21449;&#27880;&#24847;&#21644;&#19968;&#32452;&#36880;&#28857;&#22810;&#23618;&#24863;&#30693;&#26426;(MLP)&#20043;&#19978;&#65292;&#22240;&#27492;&#22312;&#36755;&#20837;&#20989;&#25968;&#25110;&#26597;&#35810;&#20301;&#32622;&#30340;&#37319;&#26679;&#27169;&#24335;&#19978;&#20570;&#20986;&#20102;&#24456;&#23569;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#33021;&#22815;&#24191;&#27867;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#65292;&#24182;&#19988;&#20855;&#26377;&#19982;&#20808;&#21069;&#22522;&#20110;&#21367;&#31215;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#30456;&#24403;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#20855;&#26377;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-driven learning of partial differential equations' solution operators has recently emerged as a promising paradigm for approximating the underlying solutions. The solution operators are usually parameterized by deep learning models that are built upon problem-specific inductive biases. An example is a convolutional or a graph neural network that exploits the local grid structure where functions' values are sampled. The attention mechanism, on the other hand, provides a flexible way to implicitly exploit the patterns within inputs, and furthermore, relationship between arbitrary query locations and inputs. In this work, we present an attention-based framework for data-driven operator learning, which we term Operator Transformer (OFormer). Our framework is built upon self-attention, cross-attention, and a set of point-wise multilayer perceptrons (MLPs), and thus it makes few assumptions on the sampling pattern of the input function or query locations. We show that the proposed frame
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CLNet&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#22312;&#22823;&#35268;&#27169;MIMO CSI&#21453;&#39304;&#20013;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20943;&#23569;&#35745;&#31639;&#24320;&#38144;&#65292;&#21516;&#26102;&#36981;&#24490;CSI&#30340;&#20869;&#22312;&#23646;&#24615;&#12290;&#22312;&#23460;&#20869;&#22806;&#22330;&#26223;&#20013;&#65292;CLNet&#24179;&#22343;&#20934;&#30830;&#24230;&#25552;&#39640;&#20102;5.41&#65285;&#65292;&#35745;&#31639;&#24320;&#38144;&#20943;&#23569;&#20102;24.1&#65285;&#12290;</title><link>http://arxiv.org/abs/2102.07507</link><description>&lt;p&gt;
CLNet&#65306;&#29992;&#20110;&#22823;&#35268;&#27169;MIMO CSI&#21453;&#39304;&#30340;&#22797;&#26434;&#36755;&#20837;&#36731;&#37327;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
CLNet: Complex Input Lightweight Neural Network designed for Massive MIMO CSI Feedback. (arXiv:2102.07507v3 [cs.IT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2102.07507
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CLNet&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#22312;&#22823;&#35268;&#27169;MIMO CSI&#21453;&#39304;&#20013;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20943;&#23569;&#35745;&#31639;&#24320;&#38144;&#65292;&#21516;&#26102;&#36981;&#24490;CSI&#30340;&#20869;&#22312;&#23646;&#24615;&#12290;&#22312;&#23460;&#20869;&#22806;&#22330;&#26223;&#20013;&#65292;CLNet&#24179;&#22343;&#20934;&#30830;&#24230;&#25552;&#39640;&#20102;5.41&#65285;&#65292;&#35745;&#31639;&#24320;&#38144;&#20943;&#23569;&#20102;24.1&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#36890;&#36807;&#20943;&#23569;CSI&#21453;&#39304;&#30340;&#24320;&#38144;&#26469;&#21457;&#25381;&#22823;&#35268;&#27169;MIMO&#22312;FDD&#27169;&#24335;&#19979;&#30340;&#20840;&#37096;&#28508;&#21147;&#24050;&#32463;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#12290;&#35768;&#22810;&#29992;&#20110;&#22823;&#35268;&#27169;MIMO CSI&#21453;&#39304;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#24050;&#32463;&#23637;&#31034;&#20102;&#23427;&#20204;&#30340;&#25928;&#29575;&#21644;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#65292;&#20294;&#35745;&#31639;&#22797;&#26434;&#24230;&#20063;&#38543;&#20043;&#22686;&#21152;&#65292;&#32780;&#19988;&#38543;&#30528;CSI&#21387;&#32553;&#29575;&#30340;&#22686;&#21152;&#65292;&#20934;&#30830;&#24615;&#26174;&#33879;&#38477;&#20302;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;CLNet&#65292;&#38024;&#23545;CSI&#21453;&#39304;&#38382;&#39064;&#65292;&#22522;&#20110;CSI&#30340;&#20869;&#22312;&#23646;&#24615;&#65292;CLNet&#25552;&#20986;&#20102;&#19968;&#20010;&#38203;&#36896;&#30340;&#22797;&#26434;&#20540;&#36755;&#20837;&#23618;&#26469;&#22788;&#29702;&#20449;&#21495;&#65292;&#24182;&#21033;&#29992;&#27880;&#24847;&#26426;&#21046;&#26469;&#25552;&#39640;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;CLNet&#22312;&#23460;&#20869;&#22806;&#22330;&#26223;&#20013;&#30340;&#24179;&#22343;&#20934;&#30830;&#24230;&#25552;&#39640;&#20102;5.41&#65285;&#65292;&#35745;&#31639;&#24320;&#38144;&#24179;&#22343;&#20943;&#23569;&#20102;24.1&#65285;&#65292;&#20248;&#20110;&#30446;&#21069;&#30340;&#26368;&#26032;&#26041;&#27861;&#12290;&#28145;&#24230;&#23398;&#20064;CSI&#21453;&#39304;CLNet&#30340;&#20195;&#30721;&#21487;&#22312;GitHub&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unleashing the full potential of massive MIMO in FDD mode by reducing the overhead of CSI feedback has recently garnered attention. Numerous deep learning for massive MIMO CSI feedback approaches have demonstrated their efficiency and potential. However, most existing methods improve accuracy at the cost of computational complexity and the accuracy decreases significantly as the CSI compression rate increases. This paper presents a novel neural network CLNet tailored for CSI feedback problem based on the intrinsic properties of CSI. CLNet proposes a forge complex-valued input layer to process signals and utilizes attention mechanism to enhance the performance of the network. The experiment result shows that CLNet outperforms the state-of-the-art method by average accuracy improvement of 5.41\% in both outdoor and indoor scenarios with average 24.1\% less computational overhead. Codes for deep learning-based CSI feedback CLNet are available at GitHub.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#21033;&#29992;&#35268;&#33539;&#31354;&#38388;&#35777;&#26126;&#23398;&#20064;&#22823;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#30340;&#30446;&#26631;&#20989;&#25968;&#22312;&#25910;&#25947;&#21040;&#20840;&#23616;&#26368;&#23567;&#20540;&#26102;&#26159;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;&#20351;&#29992;&#28857;&#32447;&#24615;&#36716;&#25442;&#30340;&#26041;&#27861;&#24314;&#31435;&#21407;&#22987;NN&#27169;&#22411;&#31354;&#38388;&#21644;&#35268;&#33539;&#31354;&#38388;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#35777;&#26126;&#20102;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65292;&#21482;&#35201;&#24046;&#24322;&#30697;&#38453;&#20445;&#25345;&#23436;&#25972;&#31209;&#65292;&#23601;&#19968;&#23450;&#33021;&#25910;&#25947;&#21040;&#38646;&#25439;&#22833;&#30340;&#20840;&#23616;&#26368;&#23567;&#20540;&#12290;&#22823;&#35268;&#27169;NN&#20855;&#26377;&#22855;&#24322;&#30340;&#24046;&#24322;&#30697;&#38453;&#30340;&#27010;&#29575;&#38750;&#24120;&#23567;&#12290;</title><link>http://arxiv.org/abs/1903.02140</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#20026;&#20160;&#20040;&#34892;&#20026;&#31867;&#20284;&#20110;&#20984;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Why Learning of Large-Scale Neural Networks Behaves Like Convex Optimization. (arXiv:1903.02140v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1903.02140
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#21033;&#29992;&#35268;&#33539;&#31354;&#38388;&#35777;&#26126;&#23398;&#20064;&#22823;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#30340;&#30446;&#26631;&#20989;&#25968;&#22312;&#25910;&#25947;&#21040;&#20840;&#23616;&#26368;&#23567;&#20540;&#26102;&#26159;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;&#20351;&#29992;&#28857;&#32447;&#24615;&#36716;&#25442;&#30340;&#26041;&#27861;&#24314;&#31435;&#21407;&#22987;NN&#27169;&#22411;&#31354;&#38388;&#21644;&#35268;&#33539;&#31354;&#38388;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#35777;&#26126;&#20102;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65292;&#21482;&#35201;&#24046;&#24322;&#30697;&#38453;&#20445;&#25345;&#23436;&#25972;&#31209;&#65292;&#23601;&#19968;&#23450;&#33021;&#25910;&#25947;&#21040;&#38646;&#25439;&#22833;&#30340;&#20840;&#23616;&#26368;&#23567;&#20540;&#12290;&#22823;&#35268;&#27169;NN&#20855;&#26377;&#22855;&#24322;&#30340;&#24046;&#24322;&#30697;&#38453;&#30340;&#27010;&#29575;&#38750;&#24120;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20123;&#29702;&#35770;&#24037;&#20316;&#65292;&#20197;&#35299;&#37322;&#20026;&#20160;&#20040;&#31616;&#21333;&#30340;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#22312;&#35299;&#20915;&#23398;&#20064;&#22823;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#30340;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#20013;&#22914;&#27492;&#25104;&#21151;&#12290;&#22312;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#35268;&#33539;&#31354;&#38388;&#30340;&#25968;&#23398;&#24037;&#20855;&#20043;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35268;&#33539;&#27169;&#22411;&#31354;&#38388;&#20013;&#30340;&#23398;&#20064;NN&#30446;&#26631;&#20989;&#25968;&#26159;&#20984;&#30340;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#38416;&#26126;&#20102;&#21407;&#22987;NN&#27169;&#22411;&#31354;&#38388;&#21644;&#35268;&#33539;&#31354;&#38388;&#20043;&#38388;&#30340;&#26799;&#24230;&#20043;&#38388;&#30340;&#20851;&#31995;&#26159;&#36890;&#36807;&#25152;&#35859;&#30340;&#24046;&#24322;&#30697;&#38453;&#34920;&#31034;&#30340;&#36880;&#28857;&#32447;&#24615;&#21464;&#25442;&#30456;&#20851;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24050;&#32463;&#35777;&#26126;&#65292;&#22914;&#26524;&#24046;&#24322;&#30697;&#38453;&#20445;&#25345;&#23436;&#25972;&#31209;&#65292;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#19968;&#23450;&#20250;&#25910;&#25947;&#21040;&#38646;&#25439;&#22833;&#30340;&#20840;&#23616;&#26368;&#23567;&#20540;&#12290;&#22914;&#26524;&#36825;&#20010;&#23436;&#25972;&#31209;&#26465;&#20214;&#25104;&#31435;&#65292;&#22312;NN&#30340;&#23398;&#20064;&#20013;&#30340;&#34892;&#20026;&#19982;&#27491;&#24120;&#30340;&#20984;&#20248;&#21270;&#30456;&#21516;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#65292;&#22823;&#35268;&#27169;NN&#20855;&#26377;&#22855;&#24322;&#30340;&#24046;&#24322;&#30697;&#38453;&#30340;&#27010;&#29575;&#38750;&#24120;&#23567;&#12290;&#29305;&#21035;&#26159;&#65292;&#24403;&#36229;&#21442;&#25968;&#21270;&#30340;NN&#26159;...
&lt;/p&gt;
&lt;p&gt;
In this paper, we present some theoretical work to explain why simple gradient descent methods are so successful in solving non-convex optimization problems in learning large-scale neural networks (NN). After introducing a mathematical tool called canonical space, we have proved that the objective functions in learning NNs are convex in the canonical model space. We further elucidate that the gradients between the original NN model space and the canonical space are related by a pointwise linear transformation, which is represented by the so-called disparity matrix. Furthermore, we have proved that gradient descent methods surely converge to a global minimum of zero loss provided that the disparity matrices maintain full rank. If this full-rank condition holds, the learning of NNs behaves in the same way as normal convex optimization. At last, we have shown that the chance to have singular disparity matrices is extremely slim in large NNs. In particular, when over-parameterized NNs are 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Jiminy&#30340;&#20262;&#29702;&#24314;&#35758;&#32452;&#20214;&#65292;&#23427;&#20351;&#29992;&#35268;&#33539;&#31995;&#32479;&#21644;&#24418;&#24335;&#35770;&#35777;&#25216;&#26415;&#65292;&#20197;&#22312;&#21033;&#30410;&#30456;&#20851;&#32773;&#20043;&#38388;&#36798;&#25104;&#36947;&#24503;&#21327;&#35758;&#12290; Jiminy&#36890;&#36807;&#21033;&#29992;&#35268;&#33539;&#31995;&#32479;&#20195;&#34920;&#27599;&#20010;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#20262;&#29702;&#35266;&#28857;&#65292;&#24182;&#36890;&#36807;&#35770;&#35777;&#26469;&#35299;&#20915;&#28041;&#21450;&#21033;&#30410;&#30456;&#20851;&#32773;&#24847;&#35265;&#30340;&#36947;&#24503;&#22256;&#22659;&#12290;</title><link>http://arxiv.org/abs/1812.04741</link><description>&lt;p&gt;
Jiminy&#39038;&#38382;&#65306;&#22522;&#20110;&#35268;&#33539;&#21644;&#35770;&#35777;&#30340;&#21033;&#30410;&#30456;&#20851;&#32773;&#36947;&#24503;&#21327;&#35758;
&lt;/p&gt;
&lt;p&gt;
The Jiminy Advisor: Moral Agreements Among Stakeholders Based on Norms and Argumentation. (arXiv:1812.04741v4 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1812.04741
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Jiminy&#30340;&#20262;&#29702;&#24314;&#35758;&#32452;&#20214;&#65292;&#23427;&#20351;&#29992;&#35268;&#33539;&#31995;&#32479;&#21644;&#24418;&#24335;&#35770;&#35777;&#25216;&#26415;&#65292;&#20197;&#22312;&#21033;&#30410;&#30456;&#20851;&#32773;&#20043;&#38388;&#36798;&#25104;&#36947;&#24503;&#21327;&#35758;&#12290; Jiminy&#36890;&#36807;&#21033;&#29992;&#35268;&#33539;&#31995;&#32479;&#20195;&#34920;&#27599;&#20010;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#20262;&#29702;&#35266;&#28857;&#65292;&#24182;&#36890;&#36807;&#35770;&#35777;&#26469;&#35299;&#20915;&#28041;&#21450;&#21033;&#30410;&#30456;&#20851;&#32773;&#24847;&#35265;&#30340;&#36947;&#24503;&#22256;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#30001;&#21046;&#36896;&#21830;&#26500;&#24314;&#30340;&#33258;&#20027;&#31995;&#32479;&#65292;&#22312;&#21463;&#21046;&#20110;&#35268;&#33539;&#21644;&#27861;&#24459;&#30340;&#31038;&#20250;&#20013;&#36816;&#20316;&#65292;&#24182;&#19982;&#26368;&#32456;&#29992;&#25143;&#20114;&#21160;&#12290;&#25152;&#26377;&#36825;&#20123;&#21442;&#19982;&#32773;&#37117;&#26159;&#21463;&#21040;&#33258;&#20027;&#31995;&#32479;&#34892;&#20026;&#24433;&#21709;&#30340;&#21033;&#30410;&#30456;&#20851;&#32773;&#12290;&#25105;&#20204;&#35299;&#20915;&#20102;&#22914;&#20309;&#23558;&#36825;&#20123;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#20262;&#29702;&#35266;&#28857;&#38598;&#25104;&#21040;&#33258;&#20027;&#31995;&#32479;&#34892;&#20026;&#20013;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Jiminy&#30340;&#20262;&#29702;&#24314;&#35758;&#32452;&#20214;&#65292;&#23427;&#20351;&#29992;&#35268;&#33539;&#31995;&#32479;&#21644;&#24418;&#24335;&#35770;&#35777;&#25216;&#26415;&#65292;&#20197;&#22312;&#21033;&#30410;&#30456;&#20851;&#32773;&#20043;&#38388;&#36798;&#25104;&#36947;&#24503;&#21327;&#35758;&#12290;Jiminy&#36890;&#36807;&#20351;&#29992;&#35268;&#33539;&#31995;&#32479;&#26469;&#20195;&#34920;&#27599;&#20010;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#20262;&#29702;&#35266;&#28857;&#65292;&#24182;&#20855;&#26377;&#19977;&#31181;&#35299;&#20915;&#28041;&#21450;&#21033;&#30410;&#30456;&#20851;&#32773;&#24847;&#35265;&#30340;&#36947;&#24503;&#22256;&#22659;&#30340;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;Jiminy&#32771;&#34385;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#35770;&#35777;&#22914;&#20309;&#20851;&#32852;&#24444;&#27492;&#65292;&#36825;&#21487;&#33021;&#24050;&#32463;&#35299;&#20915;&#20102;&#36947;&#24503;&#22256;&#22659;&#12290;&#20854;&#27425;&#65292;Jiminy&#23558;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#35268;&#33539;&#31995;&#32479;&#21512;&#24182;&#65292;&#20197;&#20415;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#21512;&#24182;&#19987;&#19994;&#30693;&#35782;&#21487;&#20197;&#35299;&#20915;&#36947;&#24503;&#22256;&#22659;&#12290;&#31532;&#19977;&#65292;&#24182;&#19988;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;Jiminy&#21487;&#20197;&#35753;&#21033;&#30410;&#30456;&#20851;&#32773;&#36827;&#34892;&#35770;&#35777;&#65292;&#20197;&#35299;&#20915;&#36947;&#24503;&#22256;&#22659;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#33258;&#20027;&#36710;&#36742;&#30340;&#26696;&#20363;&#30740;&#31350;&#20013;&#23637;&#31034;&#20102;Jiminy&#65292;&#24182;&#28436;&#31034;&#20102;&#22914;&#20309;&#23558;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#20262;&#29702;&#35266;&#28857;&#38598;&#25104;&#21040;&#33258;&#20027;&#31995;&#32479;&#30340;&#34892;&#20026;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
An autonomous system is constructed by a manufacturer, operates in a society subject to norms and laws, and interacts with end users. All of these actors are stakeholders affected by the behavior of the autonomous system. We address the challenge of how the ethical views of such stakeholders can be integrated in the behavior of an autonomous system. We propose an ethical recommendation component called Jiminy which uses techniques from normative systems and formal argumentation to reach moral agreements among stakeholders. A Jiminy represents the ethical views of each stakeholder by using normative systems, and has three ways of resolving moral dilemmas that involve the opinions of the stakeholders. First, the Jiminy considers how the arguments of the stakeholders relate to one another, which may already resolve the dilemma. Secondly, the Jiminy combines the normative systems of the stakeholders such that the combined expertise of the stakeholders may resolve the dilemma. Thirdly, and 
&lt;/p&gt;</description></item></channel></rss>