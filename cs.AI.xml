<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20108;&#32500;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;&#24930;-&#24555;&#23545;&#27604;&#34701;&#21512;&#23454;&#29616;&#19977;&#32500;&#29289;&#20307;&#23454;&#20363;&#20998;&#21106;&#30340;&#26041;&#27861;&#65292;&#22312;&#22823;&#37327;&#29289;&#20307;&#30340;&#22330;&#26223;&#20013;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#65292;&#21516;&#26102;&#19981;&#38656;&#35201;&#29289;&#20307;&#25968;&#37327;&#30340;&#19978;&#30028;&#38480;&#21046;&#65292;&#36890;&#36807;&#38480;&#21046;&#22810;&#35270;&#35282;&#19968;&#33268;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#21322;&#30495;&#23454;&#25968;&#25454;&#38598;(Messy Rooms)&#12290;</title><link>http://arxiv.org/abs/2306.04633</link><description>&lt;p&gt;
&#23545;&#27604;&#25552;&#21319;&#65306;&#36890;&#36807;&#24930;-&#24555;&#23545;&#27604;&#34701;&#21512;&#23454;&#29616;&#19977;&#32500;&#29289;&#20307;&#23454;&#20363;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Contrastive Lift: 3D Object Instance Segmentation by Slow-Fast Contrastive Fusion. (arXiv:2306.04633v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04633
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20108;&#32500;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;&#24930;-&#24555;&#23545;&#27604;&#34701;&#21512;&#23454;&#29616;&#19977;&#32500;&#29289;&#20307;&#23454;&#20363;&#20998;&#21106;&#30340;&#26041;&#27861;&#65292;&#22312;&#22823;&#37327;&#29289;&#20307;&#30340;&#22330;&#26223;&#20013;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#65292;&#21516;&#26102;&#19981;&#38656;&#35201;&#29289;&#20307;&#25968;&#37327;&#30340;&#19978;&#30028;&#38480;&#21046;&#65292;&#36890;&#36807;&#38480;&#21046;&#22810;&#35270;&#35282;&#19968;&#33268;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#21322;&#30495;&#23454;&#25968;&#25454;&#38598;(Messy Rooms)&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#32570;&#20047;&#22823;&#35268;&#27169;&#27880;&#37322;&#30340;&#19977;&#32500;&#25968;&#25454;&#38598;&#65292;&#19977;&#32500;&#29289;&#20307;&#23454;&#20363;&#20998;&#21106;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#20108;&#32500;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#26377;&#25928;&#35299;&#20915;&#35813;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31070;&#32463;&#22330;&#34920;&#31034;&#23558;2D&#20998;&#27573;&#21521;&#19978;&#25552;&#21319;&#21040;3D&#65292;&#24182;&#23558;&#23427;&#20204;&#36890;&#36807;&#24930;-&#24555;&#23545;&#27604;&#34701;&#21512;&#12290;&#36825;&#31181;&#26041;&#27861;&#40723;&#21169;&#36328;&#24103;&#30340;&#22810;&#35270;&#35282;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#12289;&#36866;&#29992;&#20110;&#20855;&#26377;&#22823;&#37327;&#29289;&#20307;&#30340;&#22330;&#26223;&#30340;&#24930;-&#24555;&#32858;&#31867;&#30446;&#26631;&#20989;&#25968;&#12290;&#19982;&#20808;&#21069;&#30340;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#23545;&#29289;&#20307;&#25968;&#37327;&#25110;&#36328;&#24103;&#29289;&#20307;&#36319;&#36394;&#36827;&#34892;&#35774;&#32622;&#19978;&#30028;&#12290;&#20026;&#20102;&#23637;&#31034;&#24930;-&#24555;&#32858;&#31867;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;Messy Rooms&#30340;&#26032;&#30340;&#21322;&#30495;&#23454;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#22330;&#26223;&#20013;&#26368;&#22810;&#26377;500&#20010;&#29289;&#20307;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;ScanNet&#12289;Hypersim&#21644;Replica&#25968;&#25454;&#38598;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22330;&#26223;&#20013;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instance segmentation in 3D is a challenging task due to the lack of large-scale annotated datasets. In this paper, we show that this task can be addressed effectively by leveraging instead 2D pre-trained models for instance segmentation. We propose a novel approach to lift 2D segments to 3D and fuse them by means of a neural field representation, which encourages multi-view consistency across frames. The core of our approach is a slow-fast clustering objective function, which is scalable and well-suited for scenes with a large number of objects. Unlike previous approaches, our method does not require an upper bound on the number of objects or object tracking across frames. To demonstrate the scalability of the slow-fast clustering, we create a new semi-realistic dataset called the Messy Rooms dataset, which features scenes with up to 500 objects per scene. Our approach outperforms the state-of-the-art on challenging scenes from the ScanNet, Hypersim, and Replica datasets, as well as o
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#24320;&#28304;&#22522;&#20934;&#27979;&#35797;&#8212;&#8212;&#8220;&#20004;&#20010;&#35789;&#27979;&#35797;&#8221;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#20041;&#33021;&#21147;&#12290;&#27979;&#35797;&#38656;&#35201;&#23545;1768&#20010;&#21517;&#35789;&#32452;&#21512;&#36827;&#34892;&#24847;&#20041;&#24615;&#21028;&#26029;&#65292;&#24182;&#21487;&#29992;&#20110;&#35780;&#20272;0-4&#37327;&#34920;&#19978;&#30340;&#26377;&#24847;&#20041;&#35780;&#20998;&#21644;&#20108;&#36827;&#21046;&#21028;&#26029;&#12290;</title><link>http://arxiv.org/abs/2306.04610</link><description>&lt;p&gt;
&#19968;&#20010;&#35821;&#20041;&#22522;&#20934;&#27979;&#35797;&#65306;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20004;&#20010;&#35789;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
The Two Word Test: A Semantic Benchmark for Large Language Models. (arXiv:2306.04610v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04610
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#24320;&#28304;&#22522;&#20934;&#27979;&#35797;&#8212;&#8212;&#8220;&#20004;&#20010;&#35789;&#27979;&#35797;&#8221;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#20041;&#33021;&#21147;&#12290;&#27979;&#35797;&#38656;&#35201;&#23545;1768&#20010;&#21517;&#35789;&#32452;&#21512;&#36827;&#34892;&#24847;&#20041;&#24615;&#21028;&#26029;&#65292;&#24182;&#21487;&#29992;&#20110;&#35780;&#20272;0-4&#37327;&#34920;&#19978;&#30340;&#26377;&#24847;&#20041;&#35780;&#20998;&#21644;&#20108;&#36827;&#21046;&#21028;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36817;&#26469;&#34920;&#29616;&#20986;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#36890;&#36807;&#39640;&#32423;&#19987;&#19994;&#32771;&#35797;&#21644;&#33499;&#21051;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#36825;&#31181;&#24615;&#33021;&#20351;&#35768;&#22810;&#20154;&#35748;&#20026;&#23427;&#20204;&#25509;&#36817;&#20110;&#23454;&#29616;&#20154;&#31867;&#25110;&#8220;&#30495;&#27491;&#30340;&#8221;&#35821;&#35328;&#29702;&#35299;&#65292;&#29978;&#33267;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#24320;&#28304;&#22522;&#20934;&#27979;&#35797;&#65292;&#21487;&#20197;&#20351;&#29992;&#20004;&#20010;&#21333;&#35789;&#30701;&#35821;&#35780;&#20272;LLMs&#30340;&#35821;&#20041;&#33021;&#21147;&#65292;&#35813;&#20219;&#21153;&#21487;&#20197;&#30456;&#23545;&#23481;&#26131;&#22320;&#30001;&#27809;&#26377;&#39640;&#32423;&#22521;&#35757;&#30340;&#20154;&#31867;&#23436;&#25104;&#12290;&#23558;&#22810;&#20010;&#35789;&#32452;&#21512;&#25104;&#19968;&#20010;&#27010;&#24565;&#26159;&#20154;&#31867;&#35821;&#35328;&#21644;&#26234;&#33021;&#30340;&#22522;&#26412;&#26041;&#38754;&#12290;&#35813;&#27979;&#35797;&#38656;&#35201;&#23545;1768&#20010;&#35780;&#20026;&#26377;&#24847;&#20041;&#65288;&#20363;&#22914;baby boy&#65289;&#25110;&#19981;&#20855;&#26377;&#24847;&#20041;&#65288;&#20363;&#22914;goat sky&#65289;&#30340;&#21517;&#35789;&#32452;&#21512;&#36827;&#34892;&#26377;&#24847;&#20041;&#24615;&#21028;&#26029;&#65292;&#30001;150&#20010;&#20154;&#31867;&#35780;&#23450;&#32773;&#36827;&#34892;&#35780;&#23450;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#22312;0-4&#37327;&#34920;&#19978;&#25506;&#27979;&#26377;&#24847;&#20041;&#35780;&#20998;&#20197;&#21450;&#20108;&#36827;&#21046;&#21028;&#26029;&#30340;&#20219;&#21153;&#29256;&#26412;&#12290;&#25105;&#20204;&#20351;&#29992;TWT&#22312;GPT-4&#12289;GPT-3.5&#21644;Bard&#19978;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown remarkable abilities recently, including passing advanced professional exams and demanding benchmark tests. This performance has led many to suggest that they are close to achieving humanlike or 'true' understanding of language, and even Artificial General Intelligence (AGI). Here, we provide a new open-source benchmark that can assess semantic abilities of LLMs using two-word phrases using a task that can be performed relatively easily by humans without advanced training. Combining multiple words into a single concept is a fundamental aspect of human language and intelligence. The test requires meaningfulness judgments of 1768 noun-noun combinations that have been rated as meaningful (e.g., baby boy) or not meaningful (e.g., goat sky). by 150 human raters. We provide versions of the task that probe meaningfulness ratings on a 0-4 scale as well as binary judgments. We conducted a series of experiments using the TWT on GPT-4, GPT-3.5, and Bard, wi
&lt;/p&gt;</description></item><item><title>GeoDiffusion&#20351;&#29992;&#25991;&#26412;&#25552;&#31034;&#23558;&#21508;&#31181;&#20960;&#20309;&#26465;&#20214;&#36716;&#21270;&#20026;&#22270;&#20687;&#65292;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#26816;&#27979;&#25968;&#25454;&#65292;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.04607</link><description>&lt;p&gt;
&#23558;&#20960;&#20309;&#25511;&#21046;&#38598;&#25104;&#21040;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#20197;&#36890;&#36807;&#25991;&#26412;&#25552;&#31034;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#26816;&#27979;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Integrating Geometric Control into Text-to-Image Diffusion Models for High-Quality Detection Data Generation via Text Prompt. (arXiv:2306.04607v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04607
&lt;/p&gt;
&lt;p&gt;
GeoDiffusion&#20351;&#29992;&#25991;&#26412;&#25552;&#31034;&#23558;&#21508;&#31181;&#20960;&#20309;&#26465;&#20214;&#36716;&#21270;&#20026;&#22270;&#20687;&#65292;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#26816;&#27979;&#25968;&#25454;&#65292;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22240;&#20854;&#22312;&#21019;&#24314;&#20869;&#23481;&#21644;&#29983;&#25104;&#25968;&#25454;&#26041;&#38754;&#30340;&#26174;&#30528;&#33021;&#21147;&#32780;&#21463;&#21040;&#37325;&#35270;&#65292;&#20363;&#22914;&#22270;&#20687;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#29289;&#20307;&#26816;&#27979;&#25968;&#25454;&#20173;&#28982;&#26159;&#19968;&#20010;&#19981;&#34987;&#20805;&#20998;&#25506;&#32034;&#30340;&#39046;&#22495;&#65292;&#20854;&#20013;&#19981;&#20165;&#22270;&#20687;&#27700;&#24179;&#30340;&#24863;&#30693;&#36136;&#37327;&#65292;&#32780;&#19988;&#36793;&#30028;&#26694;&#21644;&#30456;&#26426;&#35270;&#22270;&#31561;&#20960;&#20309;&#26465;&#20214;&#20063;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#21069;&#26399;&#30740;&#31350;&#20351;&#29992;&#27169;&#22359;&#32534;&#30721;&#35821;&#20041;&#24067;&#23616;&#26469;&#23454;&#29616;&#22797;&#21046;&#31896;&#36148;&#21512;&#25104;&#25110;&#24067;&#23616;&#21040;&#22270;&#20687;(L2I)&#29983;&#25104;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;GeoDiffusion&#65292;&#19968;&#31181;&#31616;&#21333;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#28789;&#27963;&#22320;&#23558;&#21508;&#31181;&#20960;&#20309;&#26465;&#20214;&#36716;&#21270;&#20026;&#25991;&#26412;&#25552;&#31034;&#65292;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;(T2I)&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#26816;&#27979;&#25968;&#25454;&#12290;&#19982;&#20197;&#24448;&#30340;L2I&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;GeoDiffusion&#19981;&#20165;&#33021;&#22815;&#32534;&#30721;&#36793;&#30028;&#26694;&#65292;&#36824;&#33021;&#22815;&#32534;&#30721;&#33258;&#39550;&#22330;&#26223;&#20013;&#30340;&#39069;&#22806;&#20960;&#20309;&#26465;&#20214;&#65292;&#22914;&#25668;&#20687;&#22836;&#35270;&#22270;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;GeoDiffusion&#22312;&#29289;&#20307;&#26816;&#27979;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#24182;&#38024;&#23545;&#21508;&#31181;&#20960;&#20309;&#26465;&#20214;&#29983;&#25104;&#20855;&#26377;&#26356;&#39640;&#24863;&#30693;&#36136;&#37327;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have attracted significant attention due to their remarkable ability to create content and generate data for tasks such as image classification. However, the usage of diffusion models to generate high-quality object detection data remains an underexplored area, where not only the image-level perceptual quality but also geometric conditions such as bounding boxes and camera views are essential. Previous studies have utilized either copy-paste synthesis or layout-to-image (L2I) generation with specifically designed modules to encode semantic layouts. In this paper, we propose GeoDiffusion, a simple framework that can flexibly translate various geometric conditions into text prompts and empower the pre-trained text-to-image (T2I) diffusion models for high-quality detection data generation. Unlike previous L2I methods, our GeoDiffusion is able to encode not only bounding boxes but also extra geometric conditions such as camera views in self-driving scenes. Extensive experi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25991;&#29486;&#32508;&#36848;&#65292;&#24635;&#32467;&#20102;&#29983;&#25104;&#24335; AI &#23545;&#36719;&#20214;&#20135;&#21697;&#31649;&#29702;&#39046;&#22495;&#30340;&#28508;&#22312;&#24212;&#29992;&#12289;&#20248;&#21183;&#21644;&#38480;&#21046;&#12290;&#35813;&#25216;&#26415;&#21487;&#20197;&#22312;&#21019;&#24847;&#29983;&#25104;&#12289;&#24066;&#22330;&#30740;&#31350;&#12289;&#23458;&#25143;&#27934;&#23519;&#12289;&#20135;&#21697;&#38656;&#27714;&#24037;&#31243;&#21644;&#20135;&#21697;&#24320;&#21457;&#31561;&#26041;&#38754;&#36741;&#21161;&#65292;&#24110;&#21161;&#20943;&#23569;&#24320;&#21457;&#26102;&#38388;&#21644;&#25104;&#26412;&#12290;&#26368;&#32456;&#65292;&#23427;&#21487;&#20197;&#25512;&#21160;&#36719;&#20214;&#20135;&#21697;&#31649;&#29702;&#27963;&#21160;&#30340;&#25928;&#29575;&#25552;&#21319;&#21644;&#36164;&#28304;&#26356;&#21152;&#39640;&#25928;&#30340;&#21033;&#29992;&#65292;&#33719;&#24471;&#26356;&#22909;&#30340;&#20135;&#21697;&#32467;&#26524;&#21644;&#25913;&#36827;&#30340;&#32456;&#31471;&#29992;&#25143;&#20307;&#39564;&#12290;</title><link>http://arxiv.org/abs/2306.04605</link><description>&lt;p&gt;
&#25480;&#26435; Business Transformation&#65306;&#29983;&#25104;&#24335; AI &#23545;&#36719;&#20214;&#20135;&#21697;&#31649;&#29702;&#30340;&#31215;&#26497;&#24433;&#21709;&#21644;&#20262;&#29702;&#32771;&#34385; &#8212;&#8212; &#19968;&#39033;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Empowering Business Transformation: The Positive Impact and Ethical Considerations of Generative AI in Software Product Management -- A Systematic Literature Review. (arXiv:2306.04605v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04605
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25991;&#29486;&#32508;&#36848;&#65292;&#24635;&#32467;&#20102;&#29983;&#25104;&#24335; AI &#23545;&#36719;&#20214;&#20135;&#21697;&#31649;&#29702;&#39046;&#22495;&#30340;&#28508;&#22312;&#24212;&#29992;&#12289;&#20248;&#21183;&#21644;&#38480;&#21046;&#12290;&#35813;&#25216;&#26415;&#21487;&#20197;&#22312;&#21019;&#24847;&#29983;&#25104;&#12289;&#24066;&#22330;&#30740;&#31350;&#12289;&#23458;&#25143;&#27934;&#23519;&#12289;&#20135;&#21697;&#38656;&#27714;&#24037;&#31243;&#21644;&#20135;&#21697;&#24320;&#21457;&#31561;&#26041;&#38754;&#36741;&#21161;&#65292;&#24110;&#21161;&#20943;&#23569;&#24320;&#21457;&#26102;&#38388;&#21644;&#25104;&#26412;&#12290;&#26368;&#32456;&#65292;&#23427;&#21487;&#20197;&#25512;&#21160;&#36719;&#20214;&#20135;&#21697;&#31649;&#29702;&#27963;&#21160;&#30340;&#25928;&#29575;&#25552;&#21319;&#21644;&#36164;&#28304;&#26356;&#21152;&#39640;&#25928;&#30340;&#21033;&#29992;&#65292;&#33719;&#24471;&#26356;&#22909;&#30340;&#20135;&#21697;&#32467;&#26524;&#21644;&#25913;&#36827;&#30340;&#32456;&#31471;&#29992;&#25143;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;GAI&#65289;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#65292;&#22312;&#36719;&#20214;&#20135;&#21697;&#31649;&#29702;&#26041;&#38754;&#20135;&#29983;&#20102;&#24456;&#22823;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#36890;&#36807;&#28085;&#30422;2016&#24180;&#33267;2023&#24180;&#30340;&#30456;&#20851;&#25991;&#31456;&#65292;&#25581;&#31034;&#20102;&#29983;&#25104;&#24335; AI &#22312;&#27492;&#39046;&#22495;&#20013;&#30340;&#28508;&#22312;&#24212;&#29992;&#12289;&#20248;&#21183;&#21644;&#38480;&#21046;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#35813;&#25216;&#26415;&#21487;&#20197;&#36741;&#21161;&#21019;&#24847;&#29983;&#25104;&#12289;&#24066;&#22330;&#30740;&#31350;&#12289;&#23458;&#25143;&#27934;&#23519;&#12289;&#20135;&#21697;&#38656;&#27714;&#24037;&#31243;&#21644;&#20135;&#21697;&#24320;&#21457;&#31561;&#26041;&#38754;&#12290;&#36890;&#36807;&#33258;&#21160;&#20195;&#30721;&#29983;&#25104;&#12289;&#23458;&#25143;&#21453;&#39304;&#20998;&#26512;&#31561;&#26041;&#24335;&#65292;&#23427;&#21487;&#20197;&#24110;&#21161;&#20943;&#23569;&#24320;&#21457;&#26102;&#38388;&#21644;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#35813;&#25216;&#26415;&#30340;&#31934;&#24230;&#12289;&#21487;&#38752;&#24615;&#21644;&#20262;&#29702;&#38382;&#39064;&#20173;&#28982;&#23384;&#22312;&#12290;&#26368;&#32456;&#65292;&#29983;&#25104;&#24335; AI &#30340;&#23454;&#38469;&#24212;&#29992;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#36719;&#20214;&#20135;&#21697;&#31649;&#29702;&#27963;&#21160;&#30340;&#25928;&#29575;&#65292;&#23454;&#29616;&#36164;&#28304;&#26356;&#21152;&#39640;&#25928;&#30340;&#21033;&#29992;&#65292;&#33719;&#24471;&#26356;&#22909;&#30340;&#20135;&#21697;&#32467;&#26524;&#21644;&#25913;&#36827;&#30340;&#32456;&#31471;&#29992;&#25143;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Artificial Intelligence (GAI) has made outstanding strides in recent years, with a good-sized impact on software product management. Drawing on pertinent articles from 2016 to 2023, this systematic literature evaluation reveals generative AI's potential applications, benefits, and constraints in this area. The study shows that technology can assist in idea generation, market research, customer insights, product requirements engineering, and product development. It can help reduce development time and costs through automatic code generation, customer feedback analysis, and more. However, the technology's accuracy, reliability, and ethical consideration persist. Ultimately, generative AI's practical application can significantly improve software product management activities, leading to more efficient use of resources, better product outcomes, and improved end-user experiences.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#35266;&#27979;&#36716;&#31227;&#27867;&#21270;&#38382;&#39064;&#65292;&#22312;&#22522;&#20110;&#20223;&#30495;&#22120;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#23398;&#20064;&#21040;&#21487;&#20197;&#36866;&#24212;&#19981;&#21516;&#35266;&#27979;&#36716;&#31227;&#30340;&#34920;&#24449;&#31354;&#38388;&#65292;&#23454;&#29616;&#20102;&#23545;&#20110;&#26410;&#30693;&#29615;&#22659;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.04595</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#35266;&#27979;&#36716;&#31227;&#27867;&#21270;&#38382;&#39064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Generalization Across Observation Shifts in Reinforcement Learning. (arXiv:2306.04595v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04595
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#35266;&#27979;&#36716;&#31227;&#27867;&#21270;&#38382;&#39064;&#65292;&#22312;&#22522;&#20110;&#20223;&#30495;&#22120;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#23398;&#20064;&#21040;&#21487;&#20197;&#36866;&#24212;&#19981;&#21516;&#35266;&#27979;&#36716;&#31227;&#30340;&#34920;&#24449;&#31354;&#38388;&#65292;&#23454;&#29616;&#20102;&#23545;&#20110;&#26410;&#30693;&#29615;&#22659;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#21040;&#30340;&#31574;&#30053;&#23545;&#20110;&#29615;&#22659;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#23545;&#20110;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#22312;&#23454;&#38469;&#19990;&#30028;&#30340;&#24212;&#29992;&#21644;&#36328;&#29615;&#22659;&#36716;&#31227;&#23398;&#20064;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20851;&#27880;&#20110;&#22522;&#20110;&#20223;&#30495;&#22120;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#30340;&#35266;&#27979;&#21464;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31561;&#20215;&#24230;&#37327;&#30340;&#26032;&#22411;&#30446;&#26631;&#20989;&#25968;&#65292;&#36890;&#36807;&#23398;&#20064;&#21040;&#21487;&#20197;&#36866;&#24212;&#19981;&#21516;&#35266;&#27979;&#36716;&#31227;&#30340;&#34920;&#24449;&#31354;&#38388;&#65292;&#23454;&#29616;&#20102;&#23545;&#20110;&#26410;&#30693;&#29615;&#22659;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning policies which are robust to changes in the environment are critical for real world deployment of Reinforcement Learning agents. They are also necessary for achieving good generalization across environment shifts. We focus on bisimulation metrics, which provide a powerful means for abstracting task relevant components of the observation and learning a succinct representation space for training the agent using reinforcement learning. In this work, we extend the bisimulation framework to also account for context dependent observation shifts. Specifically, we focus on the simulator based learning setting and use alternate observations to learn a representation space which is invariant to observation shifts using a novel bisimulation based objective. This allows us to deploy the agent to varying observation settings during test time and generalize to unseen scenarios. We further provide novel theoretical bounds for simulator fidelity and performance transfer guarantees for using a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26657;&#20934;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#36807;&#31243;&#20013;&#20302;&#25509;&#36817;&#24230;&#25968;&#25454;&#21644;&#39640;&#25509;&#36817;&#24230;&#25968;&#25454;&#20043;&#38388;&#19981;&#19968;&#33268;&#30340;&#35823;&#26657;&#20934;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.04590</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#30456;&#20284;&#24615;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Proximity-Informed Calibration for Deep Neural Networks. (arXiv:2306.04590v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04590
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26657;&#20934;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#36807;&#31243;&#20013;&#20302;&#25509;&#36817;&#24230;&#25968;&#25454;&#21644;&#39640;&#25509;&#36817;&#24230;&#25968;&#25454;&#20043;&#38388;&#19981;&#19968;&#33268;&#30340;&#35823;&#26657;&#20934;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#29702;&#32467;&#26524;&#30340;&#21487;&#20449;&#24230;&#26657;&#20934;&#23545;&#20110;&#25552;&#20379;&#20934;&#30830;&#21644;&#21487;&#35299;&#37322;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#33267;&#20851;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#22312;&#23433;&#20840;&#20851;&#38190;&#22330;&#26223;&#19979;&#12290;&#24050;&#26377;&#30340;&#26657;&#20934;&#26041;&#27861;&#36890;&#24120;&#24573;&#30053;&#20102;&#25509;&#36817;&#24230;&#20559;&#24046;&#30340;&#38382;&#39064;&#65292;&#21363;&#27169;&#22411;&#22312;&#20302;&#25509;&#36817;&#24615;&#25968;&#25454;&#65288;&#21363;&#20998;&#24067;&#30340;&#31232;&#30095;&#21306;&#22495;&#65289;&#20013;&#20542;&#21521;&#20110;&#26356;&#33258;&#20449;&#65292;&#32780;&#22312;&#39640;&#25509;&#36817;&#24615;&#26679;&#26412;&#20013;&#34920;&#29616;&#20986;&#19981;&#19968;&#33268;&#30340;&#35823;&#26657;&#20934;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#19968;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;ImageNet&#39044;&#35757;&#32451;&#27169;&#22411;&#19978;&#30740;&#31350;&#20102;&#36825;&#19968;&#38382;&#39064;&#65292;&#24182;&#35266;&#23519;&#21040;&#65306;1&#65289;&#25509;&#36817;&#24230;&#20559;&#24046;&#23384;&#22312;&#20110;&#21508;&#31181;&#27169;&#22411;&#26550;&#26500;&#21644;&#22823;&#23567;&#20043;&#38388;&#65307;2&#65289;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#27604;&#22522;&#20110;CNN&#30340;&#27169;&#22411;&#26356;&#23481;&#26131;&#21463;&#21040;&#25509;&#36817;&#24230;&#20559;&#24046;&#30340;&#24433;&#21709;&#65307;3&#65289;&#21363;&#20351;&#37319;&#29992;&#27969;&#34892;&#30340;&#26657;&#20934;&#31639;&#27861;&#22914;&#28201;&#24230;&#32553;&#25918;&#65292;&#25509;&#36817;&#24230;&#20559;&#24046;&#20063;&#20250;&#25345;&#32493;&#23384;&#22312;&#65307;4&#65289;&#27169;&#22411;&#22312;&#20302;&#25509;&#36817;&#24615;&#26679;&#26412;&#19978;&#30340;&#36807;&#25311;&#21512;&#31243;&#24230;&#27604;&#39640;&#25509;&#36817;&#24615;&#26679;&#26412;&#26356;&#20005;&#37325;&#12290;&#22312;&#36825;&#20123;&#23454;&#35777;&#21457;&#29616;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ProCal&#12290;
&lt;/p&gt;
&lt;p&gt;
Confidence calibration is central to providing accurate and interpretable uncertainty estimates, especially under safety-critical scenarios. However, we find that existing calibration algorithms often overlook the issue of proximity bias, a phenomenon where models tend to be more overconfident in low proximity data (i.e., lying in the sparse region of the data distribution) compared to high proximity samples, and thus suffer from inconsistent miscalibration across different proximity samples. We examine the problem over pretrained ImageNet models and observe that: 1) Proximity bias exists across a wide variety of model architectures and sizes; 2) Transformer-based models are more susceptible to proximity bias than CNN-based models; 3) Proximity bias persists even after performing popular calibration algorithms like temperature scaling; 4) Models tend to overfit more heavily on low proximity samples than on high proximity samples. Motivated by the empirical findings, we propose ProCal, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36873;&#39033;&#25913;&#36827;&#27169;&#20223;&#23398;&#20064;&#23545;&#25239;&#24615;&#31034;&#33539;&#30340;&#24615;&#33021;&#34920;&#29616;&#30340;&#26032;&#25216;&#26415;&#65292;&#21487;&#20197;&#35782;&#21035;&#26410;&#34987;&#23545;&#25163;&#26174;&#30528;&#20462;&#25913;&#30340;&#28436;&#31034;&#36712;&#36857;&#24182;&#20165;&#20174;&#20013;&#36827;&#34892;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2306.04581</link><description>&lt;p&gt;
&#21033;&#29992;&#36873;&#39033;&#25913;&#36827;&#27169;&#20223;&#23398;&#20064;&#23545;&#25239;&#24615;&#31034;&#33539;&#30340;&#24615;&#33021;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Divide and Repair: Using Options to Improve Performance of Imitation Learning Against Adversarial Demonstrations. (arXiv:2306.04581v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04581
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36873;&#39033;&#25913;&#36827;&#27169;&#20223;&#23398;&#20064;&#23545;&#25239;&#24615;&#31034;&#33539;&#30340;&#24615;&#33021;&#34920;&#29616;&#30340;&#26032;&#25216;&#26415;&#65292;&#21487;&#20197;&#35782;&#21035;&#26410;&#34987;&#23545;&#25163;&#26174;&#30528;&#20462;&#25913;&#30340;&#28436;&#31034;&#36712;&#36857;&#24182;&#20165;&#20174;&#20013;&#36827;&#34892;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20174;&#25945;&#24072;&#25110;&#19987;&#23478;&#30340;&#28436;&#31034;&#20013;&#23398;&#20064;&#25191;&#34892;&#20219;&#21153;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#35782;&#21035;&#26410;&#34987;&#23545;&#25163;&#26174;&#30528;&#20462;&#25913;&#30340;&#28436;&#31034;&#36712;&#36857;&#30340;&#37096;&#20998;&#65292;&#24182;&#20351;&#29992;&#26102;&#38388;&#19978;&#25193;&#23637;&#30340;&#31574;&#30053;&#25110;&#36873;&#39033;&#36827;&#34892;&#23398;&#20064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#31181;&#22522;&#20110;&#28436;&#31034;&#36712;&#36857;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#29305;&#24449;&#30340;&#36712;&#36857;&#20998;&#27495;&#24230;&#37327;&#65292;&#20197;&#26816;&#27979;&#21644;&#20002;&#24323;&#24050;&#34987;&#23545;&#25163;&#26174;&#30528;&#20462;&#25913;&#30340;&#36712;&#36857;&#37096;&#20998;&#65292;&#24182;&#21487;&#33021;&#38477;&#20302;&#23398;&#20064;&#32773;&#30340;&#24615;&#33021;&#65292;&#22914;&#26524;&#29992;&#20110;&#23398;&#20064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#36873;&#39033;&#30340;&#31639;&#27861;&#26469;&#20998;&#21106;&#36712;&#36857;&#65292;&#24182;&#21482;&#20174;&#24050;&#30830;&#23450;&#20026;&#21487;&#25509;&#21463;&#30340;&#36712;&#36857;&#37096;&#20998;&#20013;&#36827;&#34892;&#23398;&#20064;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#25105;&#20204;&#25216;&#26415;&#30340;&#29702;&#35770;&#32467;&#26524;&#65292;&#20197;&#34920;&#26126;&#20462;&#22797;&#37096;&#20998;&#36712;&#36857;&#21487;&#20197;&#25913;&#21892;&#23398;&#20064;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of learning to perform a task from demonstrations given by teachers or experts, when some of the experts' demonstrations might be adversarial and demonstrate an incorrect way to perform the task. We propose a novel technique that can identify parts of demonstrated trajectories that have not been significantly modified by the adversary and utilize them for learning, using temporally extended policies or options. We first define a trajectory divergence measure based on the spatial and temporal features of demonstrated trajectories to detect and discard parts of the trajectories that have been significantly modified by an adversarial expert, and, could degrade the learner's performance, if used for learning, We then use an options-based algorithm that partitions trajectories and learns only from the parts of trajectories that have been determined as admissible. We provide theoretical results of our technique to show that repairing partial trajectories improves the 
&lt;/p&gt;</description></item><item><title>&#32508;&#36848;&#20102;&#26426;&#22120;&#23398;&#20064;&#12289;&#36965;&#24863;&#21644;&#29289;&#32852;&#32593;&#26041;&#27861;&#22312;&#20892;&#19994;&#20013;&#30340;&#24212;&#29992;&#65292;&#36825;&#20123;&#25216;&#26415;&#30340;&#25972;&#21512;&#20026;&#20892;&#19994;&#25552;&#20379;&#20102;&#27934;&#23519;&#21644;&#39044;&#27979;&#65292;&#25552;&#39640;&#20102;&#20892;&#19994;&#29983;&#20135;&#25928;&#29575;&#21644;&#21487;&#25345;&#32493;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.04566</link><description>&lt;p&gt;
&#20892;&#19994;&#20013;&#26426;&#22120;&#23398;&#20064;&#12289;&#36965;&#24863;&#21644;&#29289;&#32852;&#32593;&#26041;&#27861;&#22312;&#20135;&#37327;&#39044;&#27979;&#26041;&#38754;&#30340;&#24212;&#29992;&#65306;&#19968;&#27425;&#37325;&#35201;&#30340;&#22238;&#39038;
&lt;/p&gt;
&lt;p&gt;
Recent applications of machine learning, remote sensing, and iot approaches in yield prediction: a critical review. (arXiv:2306.04566v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04566
&lt;/p&gt;
&lt;p&gt;
&#32508;&#36848;&#20102;&#26426;&#22120;&#23398;&#20064;&#12289;&#36965;&#24863;&#21644;&#29289;&#32852;&#32593;&#26041;&#27861;&#22312;&#20892;&#19994;&#20013;&#30340;&#24212;&#29992;&#65292;&#36825;&#20123;&#25216;&#26415;&#30340;&#25972;&#21512;&#20026;&#20892;&#19994;&#25552;&#20379;&#20102;&#27934;&#23519;&#21644;&#39044;&#27979;&#65292;&#25552;&#39640;&#20102;&#20892;&#19994;&#29983;&#20135;&#25928;&#29575;&#21644;&#21487;&#25345;&#32493;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36965;&#24863;&#21644;&#26426;&#22120;&#23398;&#20064;&#22312;&#20892;&#19994;&#20013;&#30340;&#25972;&#21512;&#36890;&#36807;&#25968;&#25454;&#20998;&#26512;&#25552;&#20379;&#20102;&#27934;&#23519;&#21644;&#39044;&#27979;&#65292;&#27491;&#22312;&#25913;&#21464;&#34892;&#19994;&#12290;&#36825;&#31181;&#32467;&#21512;&#23548;&#33268;&#20102;&#25913;&#21892;&#20135;&#37327;&#39044;&#27979;&#21644;&#27700;&#31649;&#29702;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#25552;&#39640;&#25928;&#29575;&#12289;&#33719;&#24471;&#26356;&#22909;&#30340;&#20135;&#37327;&#21644;&#26356;&#21487;&#25345;&#32493;&#30340;&#20892;&#19994;&#23454;&#36341;&#12290;&#36890;&#36807;&#25972;&#21512;&#36825;&#20123;&#25216;&#26415;&#65292;&#21487;&#20197;&#24320;&#21457;&#19968;&#20010;&#24378;&#22823;&#30340;&#20892;&#19994;&#31227;&#21160;&#25110;Web&#24212;&#29992;&#31243;&#24207;&#65292;&#20026;&#20892;&#27665;&#21644;&#20915;&#31574;&#32773;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#21644;&#24037;&#20855;&#65292;&#20197;&#25913;&#21892;&#20316;&#29289;&#31649;&#29702;&#21644;&#25552;&#39640;&#25928;&#29575;&#12290;&#26412;&#25991;&#23545;&#26426;&#22120;&#23398;&#20064;&#12289;&#36965;&#24863;&#21644;&#29289;&#32852;&#32593;&#26041;&#27861;&#22312;&#20135;&#37327;&#39044;&#27979;&#26041;&#38754;&#30340;&#26368;&#26032;&#24212;&#29992;&#36827;&#34892;&#20102;&#37325;&#35201;&#30340;&#22238;&#39038;&#12290;
&lt;/p&gt;
&lt;p&gt;
The integration of remote sensing and machine learning in agriculture is transforming the industry by providing insights and predictions through data analysis. This combination leads to improved yield prediction and water management, resulting in increased efficiency, better yields, and more sustainable agricultural practices. Achieving the United Nations' Sustainable Development Goals, especially "zero hunger," requires the investigation of crop yield and precipitation gaps, which can be accomplished through, the usage of artificial intelligence (AI), machine learning (ML), remote sensing (RS), and the internet of things (IoT). By integrating these technologies, a robust agricultural mobile or web application can be developed, providing farmers and decision-makers with valuable information and tools for improving crop management and increasing efficiency. Several studies have investigated these new technologies and their potential for diverse tasks such as crop monitoring, yield predi
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;OpenAI&#30340;ChatGPT&#27169;&#22411;&#22312;&#24189;&#40664;&#35782;&#21035;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#27169;&#22411;&#30340;&#24189;&#40664;&#24182;&#19981;&#26159;&#30828;&#32534;&#30721;&#30340;&#65292;&#20294;&#22823;&#37096;&#20998;&#29983;&#25104;&#30340;&#31505;&#35805;&#37117;&#19981;&#26159;&#26032;&#30340;&#65292;&#20960;&#20046;&#26159;&#23569;&#37327;&#20960;&#32452;&#37325;&#22797;&#30340;&#12290;</title><link>http://arxiv.org/abs/2306.04563</link><description>&lt;p&gt;
ChatGPT&#24456;&#26377;&#36259;&#65292;&#20294;&#24182;&#19981;&#22909;&#31505;&#65281;&#24189;&#40664;&#23545;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#20381;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT is fun, but it is not funny! Humor is still challenging Large Language Models. (arXiv:2306.04563v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04563
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;OpenAI&#30340;ChatGPT&#27169;&#22411;&#22312;&#24189;&#40664;&#35782;&#21035;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#27169;&#22411;&#30340;&#24189;&#40664;&#24182;&#19981;&#26159;&#30828;&#32534;&#30721;&#30340;&#65292;&#20294;&#22823;&#37096;&#20998;&#29983;&#25104;&#30340;&#31505;&#35805;&#37117;&#19981;&#26159;&#26032;&#30340;&#65292;&#20960;&#20046;&#26159;&#23569;&#37327;&#20960;&#32452;&#37325;&#22797;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24189;&#40664;&#26159;&#20154;&#31867;&#20132;&#27969;&#20013;&#38750;&#24120;&#37325;&#35201;&#30340;&#19968;&#20010;&#26041;&#38754;&#65292;&#36804;&#20170;&#20026;&#27490;&#23578;&#26410;&#34987;&#35299;&#20915;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36234;&#26469;&#36234;&#33021;&#22815;&#25429;&#25417;&#38544;&#21547;&#21644;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#29305;&#21035;&#26159;&#65292;OpenAI&#30340;ChatGPT&#26368;&#36817;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20844;&#20247;&#20851;&#27880;&#12290;&#22522;&#20110;GPT3&#30340;&#27169;&#22411;&#20960;&#20046;&#21487;&#20197;&#36798;&#21040;&#19982;&#20154;&#31867;&#20132;&#27969;&#30340;&#27700;&#24179;&#65292;&#29978;&#33267;&#33021;&#22815;&#35762;&#31505;&#35805;&#12290;&#20294;&#26159;&#65292;ChatGPT&#30495;&#30340;&#24456;&#26377;&#36259;&#21527;&#65311;&#20316;&#32773;&#38024;&#23545;&#27169;&#22411;&#30340;&#24189;&#40664;&#24863;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#21253;&#25324;&#29983;&#25104;&#12289;&#35299;&#37322;&#21644;&#26816;&#27979;&#31561;&#29615;&#33410;&#65292;&#35797;&#22270;&#20102;&#35299;ChatGPT&#29702;&#35299;&#24182;&#20877;&#29616;&#20154;&#31867;&#24189;&#40664;&#30340;&#33021;&#21147;&#12290;&#30001;&#20110;&#27169;&#22411;&#26412;&#36523;&#19981;&#21487;&#35775;&#38382;&#65292;&#20316;&#32773;&#37319;&#29992;&#20102;&#22522;&#20110;&#25552;&#31034;&#30340;&#23454;&#39564;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#25454;&#34920;&#26126;&#65292;&#22823;&#37096;&#20998;&#29983;&#25104;&#30340;&#31505;&#35805;&#24182;&#19981;&#26159;&#30001;&#35813;&#27169;&#22411;&#26032;&#29983;&#25104;&#30340;&#65292;&#32780;&#26159;&#37325;&#22797;&#20102;&#23569;&#37327;&#30340;&#20960;&#32452;&#31505;&#35805;&#12290;&#34429;&#28982;&#31995;&#32479;&#21487;&#20197;&#20934;&#30830;&#22320;&#35299;&#37322;&#26377;&#25928;&#30340;&#31505;&#35805;&#65292;&#20294;&#20063;&#20250;&#25552;&#20986;&#34394;&#26500;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humor is a central aspect of human communication that has not been solved for artificial agents so far. Large language models (LLMs) are increasingly able to capture implicit and contextual information. Especially, OpenAI's ChatGPT recently gained immense public attention. The GPT3-based model almost seems to communicate on a human level and can even tell jokes. Humor is an essential component of human communication. But is ChatGPT really funny? We put ChatGPT's sense of humor to the test. In a series of exploratory experiments around jokes, i.e., generation, explanation, and detection, we seek to understand ChatGPT's capability to grasp and reproduce human humor. Since the model itself is not accessible, we applied prompt-based experiments. Our empirical evidence indicates that jokes are not hard-coded but mostly also not newly generated by the model. Over 90% of 1008 generated jokes were the same 25 Jokes. The system accurately explains valid jokes but also comes up with fictional ex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#22312;&#21608;&#32536;&#21270;&#25551;&#36848;&#36923;&#36753;&#30693;&#35782;&#24211;&#19978;&#35780;&#20272;&#36830;&#35789;&#26597;&#35810;&#21644;&#20854;&#24182;&#38598;&#30340;&#21487;&#21028;&#23450;&#24615;&#65292;&#24182;&#19988;&#24471;&#21040;&#20102;DL-Lite&#30340;&#25968;&#25454;&#22797;&#26434;&#24615;&#21644;&#21512;&#24182;&#22797;&#26434;&#24615;&#30340;&#23436;&#25972;&#25551;&#36848;&#12290;</title><link>http://arxiv.org/abs/2306.04546</link><description>&lt;p&gt;
&#26597;&#35810;&#21608;&#32536;&#21270;&#25551;&#36848;&#36923;&#36753;&#30693;&#35782;&#24211;
&lt;/p&gt;
&lt;p&gt;
Querying Circumscribed Description Logic Knowledge Bases. (arXiv:2306.04546v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04546
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#22312;&#21608;&#32536;&#21270;&#25551;&#36848;&#36923;&#36753;&#30693;&#35782;&#24211;&#19978;&#35780;&#20272;&#36830;&#35789;&#26597;&#35810;&#21644;&#20854;&#24182;&#38598;&#30340;&#21487;&#21028;&#23450;&#24615;&#65292;&#24182;&#19988;&#24471;&#21040;&#20102;DL-Lite&#30340;&#25968;&#25454;&#22797;&#26434;&#24615;&#21644;&#21512;&#24182;&#22797;&#26434;&#24615;&#30340;&#23436;&#25972;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21608;&#32536;&#21270;&#26159;&#23450;&#20041;&#38750;&#21333;&#35843;&#25551;&#36848;&#36923;&#36753;&#30340;&#20027;&#35201;&#26041;&#27861;&#20043;&#19968;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;&#22312;&#21608;&#32536;&#21270;&#25551;&#36848;&#36923;&#36753;&#30693;&#35782;&#24211;&#19978;&#35780;&#20272;&#36830;&#35789;&#26597;&#35810;&#21644;&#20854;&#24182;&#38598;&#30340;&#21487;&#21028;&#23450;&#24615;&#65292;&#24182;&#19988;&#24471;&#21040;&#20102;ALCHIO&#12289;EL&#21040;&#21508;&#31181;&#29256;&#26412;&#30340;DL-Lite&#30340;&#25968;&#25454;&#22797;&#26434;&#24615;&#21644;&#21512;&#24182;&#22797;&#26434;&#24615;&#30340;&#23436;&#25972;&#25551;&#36848;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#31616;&#21333;&#30340;&#21407;&#23376;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;
Circumscription is one of the main approaches for defining non-monotonic description logics (DLs). While the decidability and complexity of traditional reasoning tasks such as satisfiability of circumscribed DL knowledge bases (KBs) is well understood, for evaluating conjunctive queries (CQs) and unions thereof (UCQs), not even decidability had been established. In this paper, we prove decidability of (U)CQ evaluation on circumscribed DL KBs and obtain a rather complete picture of both the combined complexity and the data complexity, for DLs ranging from ALCHIO via EL to various versions of DL-Lite. We also study the much simpler atomic queries (AQs).
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#32858;&#31867;&#33258;&#20030;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#21482;&#20351;&#29992;&#31895;&#30053;&#27880;&#37322;&#21644;&#26144;&#23556;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#39640;&#25991;&#26412;&#20998;&#31867;&#30340;&#32454;&#21270;&#31243;&#24230;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.04544</link><description>&lt;p&gt;
&#23545;&#27604;&#33258;&#20030;&#29992;&#20110;&#26631;&#31614;&#32454;&#21270;
&lt;/p&gt;
&lt;p&gt;
Contrastive Bootstrapping for Label Refinement. (arXiv:2306.04544v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04544
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#32858;&#31867;&#33258;&#20030;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#21482;&#20351;&#29992;&#31895;&#30053;&#27880;&#37322;&#21644;&#26144;&#23556;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#39640;&#25991;&#26412;&#20998;&#31867;&#30340;&#32454;&#21270;&#31243;&#24230;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#25991;&#26412;&#20998;&#31867;&#36890;&#24120;&#23558;&#25991;&#26412;&#20998;&#31867;&#21040;&#39044;&#23450;&#20041;&#30340;&#31895;&#31890;&#24230;&#31867;&#21035;&#20013;&#65292;&#30001;&#27492;&#29983;&#25104;&#30340;&#27169;&#22411;&#26080;&#27861;&#22788;&#29702;&#30495;&#23454;&#19990;&#30028;&#20013;&#23450;&#26399;&#20986;&#29616;&#30340;&#26356;&#31934;&#32454;&#30340;&#31867;&#21035;&#20197;&#25552;&#20379;&#20934;&#30830;&#30340;&#26381;&#21153;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#20165;&#20351;&#29992;&#31895;&#31890;&#24230;&#31867;&#21035;&#30340;&#27880;&#37322;&#21644;&#20174;&#31895;&#21040;&#32454;&#30340;&#26144;&#23556;&#36827;&#34892;&#32454;&#31890;&#24230;&#20998;&#31867;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#23545;&#27604;&#32858;&#31867;&#33258;&#20030;&#26041;&#27861;&#65292;&#26469;&#36845;&#20195;&#22320;&#20248;&#21270;&#27573;&#33853;&#30340;&#26631;&#31614;&#12290;&#22312;&#32858;&#31867;&#36807;&#31243;&#20013;&#65292;&#23427;&#20174;&#20840;&#23616;&#21644;&#26412;&#22320;&#30340;&#35282;&#24230;&#19979;&#38754;&#26397;&#36127;&#26679;&#26412;-&#21407;&#22411;&#32452;&#20043;&#38388;&#20445;&#25345;&#30456;&#23545;&#36317;&#31163;&#12290;&#22312; NYT &#21644; 20News &#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional text classification typically categorizes texts into pre-defined coarse-grained classes, from which the produced models cannot handle the real-world scenario where finer categories emerge periodically for accurate services. In this work, we investigate the setting where fine-grained classification is done only using the annotation of coarse-grained categories and the coarse-to-fine mapping. We propose a lightweight contrastive clustering-based bootstrapping method to iteratively refine the labels of passages. During clustering, it pulls away negative passage-prototype pairs under the guidance of the mapping from both global and local perspectives. Experiments on NYT and 20News show that our method outperforms the state-of-the-art methods by a large margin.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#35774;&#35745;&#22522;&#30784;&#65292;&#21363;&#20854;&#19977;&#20010;&#20851;&#38190;&#32452;&#20214;&#65306;&#27491;&#21521;&#36807;&#31243;&#12289;&#36870;&#21521;&#36807;&#31243;&#21644;&#37319;&#26679;&#36807;&#31243;&#65292;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#30410;&#30340;&#32454;&#31890;&#24230;&#36879;&#35270;&#12290;</title><link>http://arxiv.org/abs/2306.04542</link><description>&lt;p&gt;
&#20851;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#35774;&#35745;&#22522;&#30784;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
On the Design Fundamentals of Diffusion Models: A Survey. (arXiv:2306.04542v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04542
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#35774;&#35745;&#22522;&#30784;&#65292;&#21363;&#20854;&#19977;&#20010;&#20851;&#38190;&#32452;&#20214;&#65306;&#27491;&#21521;&#36807;&#31243;&#12289;&#36870;&#21521;&#36807;&#31243;&#21644;&#37319;&#26679;&#36807;&#31243;&#65292;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#30410;&#30340;&#32454;&#31890;&#24230;&#36879;&#35270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26159;&#19968;&#31181;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#36880;&#28176;&#28155;&#21152;&#21644;&#21024;&#38500;&#22122;&#22768;&#26469;&#23398;&#20064;&#35757;&#32451;&#25968;&#25454;&#30340;&#28508;&#22312;&#20998;&#24067;&#20197;&#29983;&#25104;&#25968;&#25454;&#12290;&#25193;&#25955;&#27169;&#22411;&#30340;&#32452;&#25104;&#37096;&#20998;&#24050;&#32463;&#21463;&#21040;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#65292;&#35768;&#22810;&#35774;&#35745;&#36873;&#25321;&#34987;&#25552;&#20986;&#12290;&#29616;&#26377;&#30340;&#35780;&#35770;&#20027;&#35201;&#20851;&#27880;&#39640;&#23618;&#27425;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23545;&#32452;&#20214;&#30340;&#35774;&#35745;&#22522;&#30784;&#35206;&#30422;&#36739;&#23569;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#20840;&#38754;&#32780;&#36830;&#36143;&#30340;&#32508;&#36848;&#65292;&#38024;&#23545;&#25193;&#25955;&#27169;&#22411;&#30340;&#32452;&#20214;&#35774;&#35745;&#36873;&#25321;&#36827;&#34892;&#20998;&#26512;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#32508;&#36848;&#25353;&#29031;&#19977;&#20010;&#20851;&#38190;&#32452;&#20214;&#36827;&#34892;&#32452;&#32455;&#65292;&#21363;&#27491;&#21521;&#36807;&#31243;&#12289;&#36870;&#21521;&#36807;&#31243;&#21644;&#37319;&#26679;&#36807;&#31243;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#21487;&#20197;&#25552;&#20379;&#25193;&#25955;&#27169;&#22411;&#30340;&#32454;&#31890;&#24230;&#36879;&#35270;&#65292;&#26377;&#21161;&#20110;&#26410;&#26469;&#30740;&#31350;&#20998;&#26512;&#20010;&#20307;&#32452;&#20214;&#12289;&#35774;&#35745;&#36873;&#25321;&#30340;&#36866;&#29992;&#24615;&#20197;&#21450;&#25193;&#25955;&#27169;&#22411;&#30340;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models are generative models, which gradually add and remove noise to learn the underlying distribution of training data for data generation. The components of diffusion models have gained significant attention with many design choices proposed. Existing reviews have primarily focused on higher-level solutions, thereby covering less on the design fundamentals of components. This study seeks to address this gap by providing a comprehensive and coherent review on component-wise design choices in diffusion models. Specifically, we organize this review according to their three key components, namely the forward process, the reverse process, and the sampling procedure. This allows us to provide a fine-grained perspective of diffusion models, benefiting future studies in the analysis of individual components, the applicability of design choices, and the implementation of diffusion models.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#35745;&#25968;&#27169;&#29702;&#35770;&#65288;#SMT&#65289;&#30340;&#32534;&#35793;&#31574;&#30053;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20840;&#38754;&#30340;DPLL&#65288;T&#65289;&#25628;&#32034;&#30165;&#36857;&#30340;&#33258;&#19978;&#32780;&#19979;&#30340;&#32534;&#35793;&#22120;&#12290;</title><link>http://arxiv.org/abs/2306.04541</link><description>&lt;p&gt;
&#33258;&#39030;&#21521;&#19979;&#30340;&#30693;&#35782;&#32534;&#35793;&#29992;&#20110;&#35745;&#25968;&#27169;&#29702;&#35770;&#65288;Counting Modulo Theories&#65289;
&lt;/p&gt;
&lt;p&gt;
Top-Down Knowledge Compilation for Counting Modulo Theories. (arXiv:2306.04541v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04541
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#35745;&#25968;&#27169;&#29702;&#35770;&#65288;#SMT&#65289;&#30340;&#32534;&#35793;&#31574;&#30053;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20840;&#38754;&#30340;DPLL&#65288;T&#65289;&#25628;&#32034;&#30165;&#36857;&#30340;&#33258;&#19978;&#32780;&#19979;&#30340;&#32534;&#35793;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#36755;&#20837;&#20844;&#24335;&#26159;&#30830;&#23450;&#24615;&#21487;&#20998;&#35299;&#21542;&#23450;&#33539;&#24335;&#65288;d-DNNF&#65289;&#26102;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#21629;&#39064;&#27169;&#22411;&#35745;&#25968;&#38382;&#39064;&#65288;&#65283;SAT&#65289;&#12290;&#23558;&#20219;&#24847;&#20844;&#24335;&#36716;&#25442;&#20026;&#20801;&#35768;&#25191;&#34892;&#25512;&#29702;&#20219;&#21153;&#65288;&#22914;&#35745;&#25968;&#65289;&#30340;&#34920;&#31034;&#24418;&#24335;&#31216;&#20026;&#30693;&#35782;&#32534;&#35793;&#12290;&#33258;&#39030;&#21521;&#19979;&#30340;&#30693;&#35782;&#32534;&#35793;&#26159;&#35299;&#20915;&#65283;SAT&#38382;&#39064;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#65292;&#23427;&#21033;&#29992;&#20840;&#38754;&#30340;DPLL&#25628;&#32034;&#30340;&#30165;&#36857;&#26469;&#33719;&#24471;d-DNNF&#34920;&#31034;&#12290;&#34429;&#28982;&#30693;&#35782;&#32534;&#35793;&#22312;&#21629;&#39064;&#26041;&#27861;&#26041;&#38754;&#24471;&#21040;&#20102;&#24456;&#22909;&#30340;&#30740;&#31350;&#65292;&#20294;&#23545;&#20110;&#65288;&#26080;&#37327;&#21270;&#65289;&#35745;&#25968;&#27169;&#29702;&#35770;&#35774;&#32622;&#65288;&#65283;SMT&#65289;&#30340;&#30693;&#35782;&#32534;&#35793;&#30740;&#31350;&#35201;&#23569;&#24471;&#22810;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#65283;SMT&#30340;&#32534;&#35793;&#31574;&#30053;&#12290;&#25105;&#20204;&#29305;&#21035;&#20513;&#23548;&#19968;&#31181;&#22522;&#20110;&#20840;&#38754;&#30340;DPLL&#65288;T&#65289;&#25628;&#32034;&#30165;&#36857;&#30340;&#33258;&#19978;&#32780;&#19979;&#30340;&#32534;&#35793;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Propositional model counting (#SAT) can be solved efficiently when the input formula is in deterministic decomposable negation normal form (d-DNNF). Translating an arbitrary formula into a representation that allows inference tasks, such as counting, to be performed efficiently, is called knowledge compilation. Top-down knowledge compilation is a state-of-the-art technique for solving #SAT problems that leverages the traces of exhaustive DPLL search to obtain d-DNNF representations. While knowledge compilation is well studied for propositional approaches, knowledge compilation for the (quantifier free) counting modulo theory setting (#SMT) has been studied to a much lesser degree. In this paper, we discuss compilation strategies for #SMT. We specifically advocate for a top-down compiler based on the traces of exhaustive DPLL(T) search.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#36741;&#21161;&#20219;&#21153;&#19979;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#26679;&#26412;&#32423;&#21152;&#26435;&#31639;&#27861;SLGrad&#65292;&#36890;&#36807;&#26679;&#26412;&#29305;&#23450;&#30340;&#20219;&#21153;&#26435;&#37325;&#65292;&#28040;&#38500;&#26377;&#23475;&#30340;&#36741;&#21161;&#20449;&#21495;&#24182;&#22686;&#24378;&#26377;&#29992;&#30340;&#20219;&#21153;&#20449;&#21495;&#65292;&#23454;&#29616;&#20102;&#27867;&#21270;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2306.04519</link><description>&lt;p&gt;
&#36741;&#21161;&#20219;&#21153;&#19979;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#26679;&#26412;&#32423;&#21152;&#26435;
&lt;/p&gt;
&lt;p&gt;
Sample-Level Weighting for Multi-Task Learning with Auxiliary Tasks. (arXiv:2306.04519v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04519
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#36741;&#21161;&#20219;&#21153;&#19979;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#26679;&#26412;&#32423;&#21152;&#26435;&#31639;&#27861;SLGrad&#65292;&#36890;&#36807;&#26679;&#26412;&#29305;&#23450;&#30340;&#20219;&#21153;&#26435;&#37325;&#65292;&#28040;&#38500;&#26377;&#23475;&#30340;&#36741;&#21161;&#20449;&#21495;&#24182;&#22686;&#24378;&#26377;&#29992;&#30340;&#20219;&#21153;&#20449;&#21495;&#65292;&#23454;&#29616;&#20102;&#27867;&#21270;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;(MTL)&#21487;&#20197;&#36890;&#36807;&#19982;&#30456;&#20851;&#20219;&#21153;&#20849;&#20139;&#34920;&#31034;&#26469;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;MTL&#20063;&#21487;&#33021;&#36890;&#36807;&#20219;&#21153;&#20043;&#38388;&#30340;&#26377;&#23475;&#24178;&#25200;&#32780;&#38477;&#20302;&#24615;&#33021;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#37319;&#29992;&#20219;&#21153;&#29305;&#23450;&#30340;&#25439;&#22833;&#21152;&#26435;&#20316;&#20026;&#35299;&#20915;&#24178;&#25200;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#31639;&#27861;&#23558;&#20219;&#21153;&#35270;&#20026;&#21407;&#23376;&#24615;&#65292;&#32570;&#20047;&#23558;&#26377;&#23475;&#21644;&#26377;&#29992;&#20449;&#21495;&#26126;&#30830;&#20998;&#31163;&#30340;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SLGrad&#65292;&#19968;&#31181;&#29992;&#20110;&#36741;&#21161;&#20219;&#21153;&#19979;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#26679;&#26412;&#32423;&#21152;&#26435;&#31639;&#27861;&#12290;&#36890;&#36807;&#26679;&#26412;&#29305;&#23450;&#30340;&#20219;&#21153;&#26435;&#37325;&#65292;SLGrad&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#37325;&#26032;&#22609;&#36896;&#20219;&#21153;&#20998;&#24067;&#65292;&#28040;&#38500;&#26377;&#23475;&#30340;&#36741;&#21161;&#20449;&#21495;&#24182;&#22686;&#24378;&#26377;&#29992;&#30340;&#20219;&#21153;&#20449;&#21495;&#12290;&#22312;(&#21322;)&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#24120;&#35265;&#30340;&#30417;&#30563;&#22810;&#20219;&#21153;&#38382;&#39064;&#19978;&#35266;&#23519;&#21040;&#20102;&#26174;&#33879;&#30340;&#27867;&#21270;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-task learning (MTL) can improve the generalization performance of neural networks by sharing representations with related tasks. Nonetheless, MTL can also degrade performance through harmful interference between tasks. Recent work has pursued task-specific loss weighting as a solution for this interference. However, existing algorithms treat tasks as atomic, lacking the ability to explicitly separate harmful and helpful signals beyond the task level. To this end, we propose SLGrad, a sample-level weighting algorithm for multi-task learning with auxiliary tasks. Through sample-specific task weights, SLGrad reshapes the task distributions during training to eliminate harmful auxiliary signals and augment useful task signals. Substantial generalization performance gains are observed on (semi-) synthetic datasets and common supervised multi-task problems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31572;&#26696;&#21453;&#39304;&#26469;&#22686;&#24378;&#22810;&#36328;&#24230;&#38382;&#31572;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#20256;&#32479;&#30340;&#28436;&#31034;&#31034;&#20363;&#25193;&#23637;&#20102;&#21453;&#39304;&#20449;&#24687;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#39640;&#27169;&#22411;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.04508</link><description>&lt;p&gt;
&#36890;&#36807;&#31572;&#26696;&#21453;&#39304;&#22686;&#24378;&#22810;&#36328;&#24230;&#38382;&#31572;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Enhancing In-Context Learning with Answer Feedback for Multi-Span Question Answering. (arXiv:2306.04508v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04508
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31572;&#26696;&#21453;&#39304;&#26469;&#22686;&#24378;&#22810;&#36328;&#24230;&#38382;&#31572;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#20256;&#32479;&#30340;&#28436;&#31034;&#31034;&#20363;&#25193;&#23637;&#20102;&#21453;&#39304;&#20449;&#24687;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#39640;&#27169;&#22411;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;ChatGPT&#30340;&#20986;&#29616;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#36890;&#29992;&#24615;&#33021;&#21147;&#65292;&#20294;&#22312;&#29305;&#23450;&#20219;&#21153;&#65288;&#22914;&#22810;&#36328;&#24230;&#38382;&#31572;&#65289;&#19978;&#20173;&#23384;&#22312;&#36739;&#22823;&#24046;&#36317;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#20351;&#29992;&#23569;&#37327;&#20219;&#21153;&#30456;&#20851;&#30340;&#26631;&#35760;&#25968;&#25454;&#20316;&#20026;&#28436;&#31034;&#31034;&#20363;&#26500;&#24314;&#23569;&#37327;&#36861;&#38382;&#25552;&#31034;&#65292;&#21487;&#20197;&#21033;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#26377;&#25928;&#22320;&#21033;&#29992;LLM&#12290;&#19968;&#31181;&#27969;&#34892;&#30340;&#23454;&#29616;&#26041;&#24335;&#26159;&#36890;&#36807;&#31616;&#21333;&#30340;&#27169;&#26495;&#23558;&#20960;&#20010;&#38382;&#39064;&#21450;&#20854;&#27491;&#30830;&#31572;&#26696;&#36827;&#34892;&#36830;&#25509;&#65292;&#36890;&#30693;LLM&#25152;&#38656;&#30340;&#36755;&#20986;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21033;&#29992;&#26631;&#35760;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#20351;&#20043;&#20063;&#25552;&#20379;&#23545;LLM&#19968;&#20123;&#19981;&#26399;&#26395;&#30340;&#36755;&#20986;&#30340;&#25552;&#31034;&#65292;&#36890;&#36807;&#25193;&#23637;&#28436;&#31034;&#31034;&#20363;&#20197;&#21453;&#39304;&#39044;&#27979;&#27169;&#22411;&#39044;&#27979;&#30340;&#31572;&#26696;&#65292;&#20363;&#22914;&#65292;&#27491;&#30830;&#12289;&#19981;&#27491;&#30830;&#25110;&#19981;&#23436;&#25972;&#12290;&#22312;&#19977;&#20010;&#22810;&#36328;&#24230;&#38382;&#31572;&#25968;&#25454;&#38598;&#20197;&#21450;&#19968;&#20010;&#20851;&#38190;&#35789;&#25552;&#21462;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26032;&#25552;&#31034;&#31574;&#30053;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Whereas the recent emergence of large language models (LLMs) like ChatGPT has exhibited impressive general performance, it still has a large gap with fully-supervised models on specific tasks such as multi-span question answering. Previous researches found that in-context learning is an effective approach to exploiting LLM, by using a few task-related labeled data as demonstration examples to construct a few-shot prompt for answering new questions. A popular implementation is to concatenate a few questions and their correct answers through simple templates, informing LLM of the desired output. In this paper, we propose a novel way of employing labeled data such that it also informs LLM of some undesired output, by extending demonstration examples with feedback about answers predicted by an off-the-shelf model, e.g., correct, incorrect, or incomplete. Experiments on three multi-span question answering datasets as well as a keyphrase extraction dataset show that our new prompting strateg
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#21033;&#29992;AFC&#22312;&#39640;&#20445;&#30495;&#24230;&#21644;&#23436;&#22791;&#24615;&#30340;&#35774;&#23450;&#19979;&#65292;&#36873;&#25321;&#26080;&#20449;&#24687;&#35777;&#20070;&#30340;&#20219;&#21153;&#26159; NP-hard &#30340;&#12290;</title><link>http://arxiv.org/abs/2306.04505</link><description>&lt;p&gt;
&#20266;&#35777;&#20070;&#36873;&#25321;&#30340;&#38590;&#24230;
&lt;/p&gt;
&lt;p&gt;
Hardness of Deceptive Certificate Selection. (arXiv:2306.04505v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04505
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#21033;&#29992;AFC&#22312;&#39640;&#20445;&#30495;&#24230;&#21644;&#23436;&#22791;&#24615;&#30340;&#35774;&#23450;&#19979;&#65292;&#36873;&#25321;&#26080;&#20449;&#24687;&#35777;&#20070;&#30340;&#20219;&#21153;&#26159; NP-hard &#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#20132;&#20114;&#24335;&#35777;&#26126;&#31995;&#32479;&#30340;&#20998;&#31867;&#22120;&#22312;&#23454;&#29616;AI&#29702;&#35770;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#12290;&#35777;&#26126;&#32773;&#20174;&#25968;&#25454;&#28857;&#20013;&#36873;&#25321;&#35777;&#20070;&#24182;&#23558;&#20854;&#21457;&#36865;&#32473;&#39564;&#35777;&#32773;&#65292;&#21518;&#32773;&#20915;&#23450;&#20998;&#31867;&#12290;&#22312;&#26426;&#22120;&#23398;&#20064;&#30340;&#32972;&#26223;&#19979;&#65292;&#36825;&#26679;&#30340;&#35777;&#20070;&#21487;&#20197;&#26159;&#20449;&#24687;&#24615;&#30340;&#29305;&#24449;&#12290;&#23545;&#20110;&#39640;&#24230;&#20445;&#30495;&#19982;&#23436;&#22791;&#24615;&#30340;&#35774;&#23450;&#65292;&#20132;&#25442;&#30340;&#35777;&#20070;&#24517;&#39035;&#19982;&#25968;&#25454;&#28857;&#30340;&#30495;&#23454;&#20998;&#31867;&#26377;&#39640;&#30340;&#30456;&#20114;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20445;&#35777;&#20381;&#36182;&#20110;&#25968;&#25454;&#38598;&#30340;&#38750;&#23545;&#31216;&#29305;&#24449;&#30456;&#20851;&#24615;&#30340;&#30028;&#38480;&#65292;&#36825;&#26159;&#36804;&#20170;&#20026;&#27490;&#38590;&#20197;&#20272;&#35745;&#39640;&#32500;&#25968;&#25454;&#30340;&#23646;&#24615;&#12290;W\"aldchen&#31561;&#20154;&#29468;&#27979;&#65292;&#21033;&#29992;AFC&#26159;&#35745;&#31639;&#19978;&#22256;&#38590;&#30340;&#65292;&#25105;&#20204;&#22312;&#36825;&#37324;&#35777;&#26126;&#20102;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#32771;&#34385;&#24694;&#24847;&#30340;&#35777;&#26126;&#32773;-&#39564;&#35777;&#32773;&#20108;&#20803;&#32452;&#65292;&#26088;&#22312;&#21033;&#29992;AFC&#26469;&#23454;&#29616;&#39640;&#30340;&#23436;&#22791;&#24615;&#21644;&#20445;&#30495;&#24615;&#65292;&#21516;&#26102;&#20351;&#29992;&#27809;&#26377;&#20449;&#24687;&#30340;&#35777;&#20070;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20010;&#20219;&#21153;&#26159;$\mathsf{NP}$&#38590;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent progress towards theoretical interpretability guarantees for AI has been made with classifiers that are based on interactive proof systems. A prover selects a certificate from the datapoint and sends it to a verifier who decides the class. In the context of machine learning, such a certificate can be a feature that is informative of the class. For a setup with high soundness and completeness, the exchanged certificates must have a high mutual information with the true class of the datapoint. However, this guarantee relies on a bound on the Asymmetric Feature Correlation of the dataset, a property that so far is difficult to estimate for high-dimensional data. It was conjectured in W\"aldchen et al. that it is computationally hard to exploit the AFC, which is what we prove here.  We consider a malicious prover-verifier duo that aims to exploit the AFC to achieve high completeness and soundness while using uninformative certificates. We show that this task is $\mathsf{NP}$-hard an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AGRA&#30340;&#33258;&#36866;&#24212;&#26799;&#24230;&#24322;&#24120;&#20540;&#21435;&#38500;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#21160;&#24577;&#35843;&#25972;&#25968;&#25454;&#38598;&#20174;&#32780;&#26377;&#25928;&#25552;&#39640;&#27169;&#22411;&#23398;&#20064;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.04502</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#22522;&#20110;&#26799;&#24230;&#30340;&#24322;&#24120;&#20540;&#21435;&#38500;&#30340;&#22024;&#26434;&#26631;&#31614;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning with Noisy Labels by Adaptive Gradient-Based Outlier Removal. (arXiv:2306.04502v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04502
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AGRA&#30340;&#33258;&#36866;&#24212;&#26799;&#24230;&#24322;&#24120;&#20540;&#21435;&#38500;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#21160;&#24577;&#35843;&#25972;&#25968;&#25454;&#38598;&#20174;&#32780;&#26377;&#25928;&#25552;&#39640;&#27169;&#22411;&#23398;&#20064;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#21487;&#38752;&#21644;&#39640;&#24615;&#33021;&#27169;&#22411;&#38656;&#35201;&#20934;&#30830;&#21644;&#20016;&#23500;&#30340;&#25968;&#25454;&#38598;&#65292;&#20294;&#21363;&#20415;&#26159;&#20154;&#24037;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#20063;&#20250;&#21253;&#21547;&#38169;&#35823;&#65292;&#26356;&#19981;&#29992;&#35828;&#33258;&#21160;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#20102;&#12290;&#29616;&#26377;&#30340;&#19968;&#20123;&#25968;&#25454;&#21435;&#22122;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#20110;&#26816;&#27979;&#24322;&#24120;&#20540;&#24182;&#36827;&#34892;&#27704;&#20037;&#24615;&#21435;&#38500;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#24456;&#23481;&#26131;&#36807;&#24230;&#25110;&#32773;&#27424;&#24230;&#36807;&#28388;&#25968;&#25454;&#38598;&#12290;&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;&#26799;&#24230;&#24322;&#24120;&#20540;&#21435;&#38500;&#26041;&#27861;&#65288;AGRA&#65289;&#65292;&#19981;&#21516;&#20110;&#22312;&#27169;&#22411;&#35757;&#32451;&#20043;&#21069;&#28165;&#27927;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21160;&#24577;&#35843;&#25972;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#27604;&#36739;&#19968;&#32452;&#26679;&#26412;&#30340;&#32047;&#31215;&#26799;&#24230;&#21644;&#21333;&#20010;&#26679;&#26412;&#30340;&#26799;&#24230;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20915;&#23450;&#26159;&#21542;&#22312;&#24403;&#21069;&#26356;&#26032;&#26102;&#20445;&#30041;&#23545;&#24212;&#30340;&#26679;&#26412;&#65292;&#20197;&#27492;&#26469;&#30830;&#23450;&#23427;&#26159;&#21542;&#26377;&#21161;&#20110;&#27169;&#22411;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#24191;&#27867;&#35780;&#20272;&#34920;&#26126;&#65292;AGRA&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#19988;&#20840;&#38754;&#30340;&#32467;&#26524;&#20998;&#26512;&#35777;&#23454;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#29702;&#35770;&#21644;&#23454;&#36341;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
An accurate and substantial dataset is necessary to train a reliable and well-performing model. However, even manually labeled datasets contain errors, not to mention automatically labeled ones. The problem of data denoising was addressed in different existing research, most of which focuses on the detection of outliers and their permanent removal - a process that is likely to over- or underfilter the dataset. In this work, we propose AGRA: a new method for Adaptive GRAdient-based outlier removal. Instead of cleaning the dataset prior to model training, the dataset is adjusted during the training process. By comparing the aggregated gradient of a batch of samples and an individual example gradient, our method dynamically decides whether a corresponding example is helpful for the model at this point or is counter-productive and should be left out for the current update. Extensive evaluation on several datasets demonstrates the AGRA effectiveness, while comprehensive results analysis sup
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; rewarded soup &#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#22810;&#31181;&#20195;&#29702;&#22870;&#21169;&#65292;&#23454;&#29616;&#24494;&#35843;&#26435;&#37325;&#25554;&#20540;&#65292;&#20174;&#32780;&#22312;&#25972;&#20010;&#20559;&#22909;&#31354;&#38388;&#20013;&#23454;&#29616;&#24085;&#32047;&#25176;&#26368;&#20248;&#24191;&#20041;&#21270;&#12290;&#35813;&#26041;&#27861;&#22312;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#19978;&#20855;&#26377;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.04488</link><description>&lt;p&gt;
&#22522;&#20110;&#32467;&#21512;&#22810;&#26679;&#21270;&#22870;&#21169;&#24494;&#35843;&#26435;&#37325;&#25554;&#20540;&#30340;&#24085;&#32047;&#25176;&#26368;&#20248;&#23545;&#40784;&#30340;&#22870;&#21169;&#27748;
&lt;/p&gt;
&lt;p&gt;
Rewarded soups: towards Pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards. (arXiv:2306.04488v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04488
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; rewarded soup &#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#22810;&#31181;&#20195;&#29702;&#22870;&#21169;&#65292;&#23454;&#29616;&#24494;&#35843;&#26435;&#37325;&#25554;&#20540;&#65292;&#20174;&#32780;&#22312;&#25972;&#20010;&#20559;&#22909;&#31354;&#38388;&#20013;&#23454;&#29616;&#24085;&#32047;&#25176;&#26368;&#20248;&#24191;&#20041;&#21270;&#12290;&#35813;&#26041;&#27861;&#22312;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#19978;&#20855;&#26377;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#39318;&#20808;&#22312;&#22823;&#37327;&#26080;&#21442;&#32771;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#26377;&#26631;&#27880;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#24378;&#21270;&#23398;&#20064;&#65292;&#29305;&#21035;&#26159;&#26469;&#33258;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;(RLHF)&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#20351;&#32593;&#32476;&#19982;&#39044;&#26399;&#30340;&#20351;&#29992;&#30456;&#21305;&#37197;&#12290;&#28982;&#32780;&#65292;&#20195;&#29702;&#22870;&#21169;&#30340;&#32570;&#38519;&#21487;&#33021;&#20250;&#22952;&#30861;&#35757;&#32451;&#65292;&#23548;&#33268;&#27425;&#20248;&#32467;&#26524;&#65307;&#29616;&#23454;&#20219;&#21153;&#21644;&#20154;&#31867;&#24847;&#35265;&#30340;&#22810;&#26679;&#24615;&#21152;&#21095;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#37319;&#29992;&#22810;&#31574;&#30053;&#26041;&#27861;&#26469;&#25317;&#25265;&#22810;&#26679;&#21270;&#22870;&#21169;&#30340;&#24322;&#36136;&#24615;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#19981;&#26159;&#19987;&#27880;&#20110;&#21333;&#19968;&#30340;&#20808;&#39564;&#22870;&#21169;&#65292;&#32780;&#26159;&#22312;&#25972;&#20010;&#20559;&#22909;&#31354;&#38388;&#20013;&#23454;&#29616;&#24085;&#32047;&#25176;&#26368;&#20248;&#24191;&#20041;&#21270;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; rewarded soup&#65292;&#39318;&#20808;&#29420;&#31435;&#22320;&#19987;&#38376;&#21270;&#22810;&#20010;&#32593;&#32476;(&#27599;&#20010;&#20195;&#29702;&#22870;&#21169;&#19968;&#20010;)&#65292;&#28982;&#21518;&#22312;&#23427;&#20204;&#30340;&#26435;&#37325;&#20043;&#38388;&#36827;&#34892;&#32447;&#24615;&#25554;&#20540;&#12290;&#36890;&#36807;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#26159;&#25104;&#21151;&#30340;&#65292;&#22240;&#20026;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#22810;&#26679;&#21270;&#22870;&#21169;&#26469;&#33258;&#20849;&#20139;&#30340;&#39044;&#35757;&#32451;&#21021;&#22987;&#21270;&#26102;&#65292;&#26435;&#37325;&#20173;&#28982;&#20445;&#25345;&#32447;&#24615;&#36830;&#25509;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation models are first pre-trained on vast unsupervised datasets and then fine-tuned on labeled data. Reinforcement learning, notably from human feedback (RLHF), can further align the network with the intended usage. Yet the imperfections in the proxy reward may hinder the training and lead to suboptimal results; the diversity of objectives in real-world tasks and human opinions exacerbate the issue. This paper proposes embracing the heterogeneity of diverse rewards by following a multi-policy strategy. Rather than focusing on a single a priori reward, we aim for Pareto-optimal generalization across the entire space of preferences. To this end, we propose rewarded soup, first specializing multiple networks independently (one for each proxy reward) and then interpolating their weights linearly. This succeeds empirically because we show that the weights remain linearly connected when fine-tuned on diverse rewards from a shared pre-trained initialization. We demonstrate the effective
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#27169;&#31946;&#28388;&#38236;&#31561;&#22806;&#35266;&#25913;&#21464;&#25216;&#26415;&#65292;&#20154;&#24037;&#26234;&#33021;&#21487;&#20197;&#23548;&#33268;&#20010;&#20307;&#23545;&#27169;&#31946;&#20182;&#20154;&#30340;&#33258;&#31169;&#34892;&#20026;&#22686;&#21152;&#65292;&#36825;&#34920;&#26126;&#20854;&#21487;&#20197;&#36890;&#36807;&#38750;&#20010;&#24615;&#21270;&#20419;&#36827;&#36947;&#24503;&#32553;&#20943;&#65292;&#38656;&#35201;&#26356;&#24191;&#27867;&#30340;&#20262;&#29702;&#35752;&#35770;&#12290;</title><link>http://arxiv.org/abs/2306.04484</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#21487;&#20197;&#36890;&#36807;&#20462;&#25913;&#20114;&#21160;&#20249;&#20276;&#30340;&#22806;&#35980;&#26469;&#20419;&#36827;&#33258;&#31169;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence can facilitate selfish decisions by altering the appearance of interaction partners. (arXiv:2306.04484v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04484
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#27169;&#31946;&#28388;&#38236;&#31561;&#22806;&#35266;&#25913;&#21464;&#25216;&#26415;&#65292;&#20154;&#24037;&#26234;&#33021;&#21487;&#20197;&#23548;&#33268;&#20010;&#20307;&#23545;&#27169;&#31946;&#20182;&#20154;&#30340;&#33258;&#31169;&#34892;&#20026;&#22686;&#21152;&#65292;&#36825;&#34920;&#26126;&#20854;&#21487;&#20197;&#36890;&#36807;&#38750;&#20010;&#24615;&#21270;&#20419;&#36827;&#36947;&#24503;&#32553;&#20943;&#65292;&#38656;&#35201;&#26356;&#24191;&#27867;&#30340;&#20262;&#29702;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#21644;&#35270;&#39057;&#20250;&#35758;&#25216;&#26415;&#20013;&#36234;&#26469;&#36234;&#26222;&#36941;&#30340;&#22270;&#20687;&#25913;&#21464;&#28388;&#38236;&#24341;&#36215;&#20102;&#20851;&#20110;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#25805;&#32437;&#25105;&#20204;&#23545;&#20182;&#20154;&#30340;&#35748;&#30693;&#30340;&#20262;&#29702;&#21644;&#24515;&#29702;&#24433;&#21709;&#30340;&#25285;&#24551;&#12290;&#26412;&#30740;&#31350;&#37325;&#28857;&#30740;&#31350;&#20102;&#27169;&#31946;&#28388;&#38236;&#8212;&#8212;&#19968;&#31181;&#22806;&#35266;&#25913;&#21464;&#25216;&#26415;&#8212;&#8212;&#23545;&#20010;&#20307;&#23545;&#24453;&#20182;&#20154;&#34892;&#20026;&#30340;&#28508;&#22312;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#19968;&#33268;&#34920;&#26126;&#65292;&#23545;&#37027;&#20123;&#22806;&#35980;&#34987;&#27169;&#31946;&#30340;&#20154;&#26174;&#31034;&#20986;&#26356;&#22810;&#30340;&#33258;&#31169;&#34892;&#20026;&#65292;&#36825;&#34920;&#26126;&#27169;&#31946;&#28388;&#38236;&#21487;&#20197;&#36890;&#36807;&#38750;&#20010;&#24615;&#21270;&#20419;&#36827;&#36947;&#24503;&#32553;&#20943;&#12290;&#36825;&#20123;&#32467;&#26524;&#24378;&#35843;&#26377;&#20851;&#20462;&#25913;&#25105;&#20204;&#23545;&#20182;&#20154;&#30340;&#35748;&#30693;&#30340;AI&#25216;&#26415;&#30340;&#26356;&#24191;&#27867;&#30340;&#20262;&#29702;&#35752;&#35770;&#30340;&#24517;&#35201;&#24615;&#65292;&#21253;&#25324;&#36879;&#26126;&#24230;&#12289;&#21516;&#24847;&#21644;&#24847;&#35782;&#21040;&#34987;&#20182;&#20154;&#30340;&#22806;&#35980;&#25805;&#20316;&#25152;&#24433;&#21709;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#36824;&#24378;&#35843;&#20808;&#21457;&#24615;&#30340;&#23454;&#39564;&#22312;&#24341;&#23548;&#36127;&#36131;&#20219;&#30340;Gui&#24320;&#21457;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing prevalence of image-altering filters on social media and video conferencing technologies has raised concerns about the ethical and psychological implications of using Artificial Intelligence (AI) to manipulate our perception of others. In this study, we specifically investigate the potential impact of blur filters, a type of appearance-altering technology, on individuals' behavior towards others. Our findings consistently demonstrate a significant increase in selfish behavior directed towards individuals whose appearance is blurred, suggesting that blur filters can facilitate moral disengagement through depersonalization. These results emphasize the need for broader ethical discussions surrounding AI technologies that modify our perception of others, including issues of transparency, consent, and the awareness of being subject to appearance manipulation by others. We also emphasize the importance of anticipatory experiments in informing the development of responsible gui
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26399;&#26395;&#26041;&#24046;&#19982;&#39640;&#26031;&#36807;&#31243;&#65288;EV-GP&#65289;&#26631;&#20934;&#29992;&#20110;&#31070;&#32463;&#20027;&#21160;&#23398;&#20064;&#65292;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#23545;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#19988;&#20445;&#35777;&#22312;&#25968;&#25454;&#36873;&#25321;&#26102;NN&#20855;&#26377;&#33391;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#21644;&#21021;&#22987;&#21270;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.04454</link><description>&lt;p&gt;
&#26080;&#38656;&#35757;&#32451;&#30340;&#31070;&#32463;&#20027;&#21160;&#23398;&#20064;&#19982;&#21021;&#22987;&#21270;&#40065;&#26834;&#24615;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Training-Free Neural Active Learning with Initialization-Robustness Guarantees. (arXiv:2306.04454v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04454
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26399;&#26395;&#26041;&#24046;&#19982;&#39640;&#26031;&#36807;&#31243;&#65288;EV-GP&#65289;&#26631;&#20934;&#29992;&#20110;&#31070;&#32463;&#20027;&#21160;&#23398;&#20064;&#65292;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#23545;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#19988;&#20445;&#35777;&#22312;&#25968;&#25454;&#36873;&#25321;&#26102;NN&#20855;&#26377;&#33391;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#21644;&#21021;&#22987;&#21270;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#31070;&#32463;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#26088;&#22312;&#36890;&#36807;&#36873;&#25321;&#25968;&#25454;&#36827;&#34892;&#26631;&#35760;&#26469;&#20248;&#21270;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#38500;&#20102;&#33391;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#22806;&#65292;&#23545;&#20110;&#38543;&#26426;&#21442;&#25968;&#21021;&#22987;&#21270;&#30340;&#40065;&#26834;&#24615;&#20063;&#26159;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#30340;&#37325;&#35201;&#35201;&#27714;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26399;&#26395;&#26041;&#24046;&#19982;&#39640;&#26031;&#36807;&#31243;&#65288;EV-GP&#65289;&#26631;&#20934;&#26469;&#36827;&#34892;&#31070;&#32463;&#20027;&#21160;&#23398;&#20064;&#65292;&#35813;&#26631;&#20934;&#22312;&#29702;&#35770;&#19978;&#20445;&#35777;&#36873;&#25321;&#25968;&#25454;&#28857;&#21487;&#20197;&#23548;&#33268;&#35757;&#32451;&#30340; NN &#20855;&#26377;&#33391;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#21644;&#21021;&#22987;&#21270;&#40065;&#26834;&#24615;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340; EV-GP &#26631;&#20934;&#26159;&#26080;&#38656;&#35757;&#32451;&#30340;&#65292;&#21363;&#22312;&#25968;&#25454;&#36873;&#25321;&#36807;&#31243;&#20013;&#19981;&#38656;&#35201;&#23545; NN &#36827;&#34892;&#20219;&#20309;&#35757;&#32451;&#65292;&#36825;&#20351;&#20854;&#22312;&#35745;&#31639;&#19978;&#26356;&#21152;&#39640;&#25928;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340; EV-GP &#26631;&#20934;&#19982;&#21021;&#22987;&#21270;&#40065;&#26834;&#24615;&#21644;&#27010;&#25324;&#24615;&#33021;&#39640;&#24230;&#30456;&#20851;&#65292;&#24182;&#19988;&#34920;&#26126;&#23427;&#22312;&#20004;&#20010;&#26399;&#26395;&#26041;&#38754;&#30340;&#34920;&#29616;&#22343;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#65292;&#23588;&#20854;&#26159;&#21021;&#22987;&#21270;&#40065;&#26834;&#24615;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing neural active learning algorithms have aimed to optimize the predictive performance of neural networks (NNs) by selecting data for labelling. However, other than a good predictive performance, being robust against random parameter initializations is also a crucial requirement in safety-critical applications. To this end, we introduce our expected variance with Gaussian processes (EV-GP) criterion for neural active learning, which is theoretically guaranteed to select data points which lead to trained NNs with both (a) good predictive performances and (b) initialization robustness. Importantly, our EV-GP criterion is training-free, i.e., it does not require any training of the NN during data selection, which makes it computationally efficient. We empirically demonstrate that our EV-GP criterion is highly correlated with both initialization robustness and generalization performance, and show that it consistently outperforms baseline methods in terms of both desiderata, especiall
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#31350;&#20102;&#20351;&#29992;&#31934;&#31616;&#31574;&#30053;&#32593;&#32476;&#20316;&#20026;&#33258;&#25105;&#27169;&#22411;&#30340;&#20248;&#32570;&#28857;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#32463;&#36807;&#31934;&#31616;&#30340;&#31574;&#30053;&#32593;&#32476;&#20316;&#20026;&#33258;&#25105;&#27169;&#22411;&#21487;&#20197;&#31283;&#23450;&#35757;&#32451;&#65292;&#24182;&#19988;&#26377;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.04440</link><description>&lt;p&gt;
&#20197;&#21452;&#31574;&#30053;&#20026;&#33258;&#27169;&#22411;&#30340;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Dual policy as self-model for planning. (arXiv:2306.04440v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04440
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#31350;&#20102;&#20351;&#29992;&#31934;&#31616;&#31574;&#30053;&#32593;&#32476;&#20316;&#20026;&#33258;&#25105;&#27169;&#22411;&#30340;&#20248;&#32570;&#28857;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#32463;&#36807;&#31934;&#31616;&#30340;&#31574;&#30053;&#32593;&#32476;&#20316;&#20026;&#33258;&#25105;&#27169;&#22411;&#21487;&#20197;&#31283;&#23450;&#35757;&#32451;&#65292;&#24182;&#19988;&#26377;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35268;&#21010;&#26159;&#19968;&#31181;&#25968;&#25454;&#26377;&#25928;&#30340;&#20915;&#31574;&#31574;&#30053;&#65292;&#20195;&#29702;&#36890;&#36807;&#25506;&#32034;&#21487;&#33021;&#30340;&#26410;&#26469;&#29366;&#24577;&#26469;&#36873;&#25321;&#20505;&#36873;&#21160;&#20316;&#12290;&#24403;&#23384;&#22312;&#39640;&#32500;&#34892;&#21160;&#31354;&#38388;&#26102;&#65292;&#20026;&#20102;&#27169;&#25311;&#26410;&#26469;&#29366;&#24577;&#65292;&#24517;&#39035;&#20351;&#29992;&#33258;&#24049;&#30340;&#20915;&#31574;&#31574;&#30053;&#26469;&#38480;&#21046;&#25152;&#38656;&#25506;&#32034;&#30340;&#21160;&#20316;&#25968;&#37327;&#12290;&#25105;&#20204;&#23558;&#29992;&#20110;&#27169;&#25311;&#33258;&#24049;&#20915;&#31574;&#30340;&#27169;&#22411;&#31216;&#20026;&#20195;&#29702;&#30340;&#33258;&#25105;&#27169;&#22411;&#12290;&#23613;&#31649;&#22312;&#35268;&#21010;&#34892;&#21160;&#26102;&#65292;&#19990;&#30028;&#27169;&#22411;&#36890;&#24120;&#19982;&#33258;&#25105;&#27169;&#22411;&#19968;&#36215;&#38544;&#21547;&#22320;&#20351;&#29992;&#65292;&#20294;&#22914;&#20309;&#35774;&#35745;&#33258;&#25105;&#27169;&#22411;&#20173;&#19981;&#28165;&#26970;&#12290;&#21463;&#24403;&#21069;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#21644;&#31070;&#32463;&#31185;&#23398;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20351;&#29992;&#31934;&#31616;&#31574;&#30053;&#32593;&#32476;&#20316;&#20026;&#33258;&#25105;&#27169;&#22411;&#30340;&#20248;&#32570;&#28857;&#12290;&#22312;&#36825;&#26679;&#30340;&#21452;&#31574;&#30053;&#20195;&#29702;&#20013;&#65292;&#19968;&#20010;&#26080;&#27169;&#22411;&#31574;&#30053;&#21644;&#19968;&#20010;&#32463;&#36807;&#31934;&#31616;&#30340;&#31574;&#30053;&#20998;&#21035;&#29992;&#20110;&#26080;&#27169;&#22411;&#21160;&#20316;&#21644;&#35745;&#21010;&#21160;&#20316;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#29983;&#24577;&#30456;&#20851;&#30340;&#21442;&#25968;&#29615;&#22659;&#19978;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#32463;&#36807;&#31934;&#31616;&#30340;&#31574;&#30053;&#32593;&#32476;&#20316;&#20026;&#33258;&#25105;&#27169;&#22411;&#21487;&#20197;&#31283;&#23450;&#35757;&#32451;&#65292;&#24182;&#19988;&#26377;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Planning is a data efficient decision-making strategy where an agent selects candidate actions by exploring possible future states. To simulate future states when there is a high-dimensional action space, the knowledge of one's decision making strategy must be used to limit the number of actions to be explored. We refer to the model used to simulate one's decisions as the agent's self-model. While self-models are implicitly used widely in conjunction with world models to plan actions, it remains unclear how self-models should be designed. Inspired by current reinforcement learning approaches and neuroscience, we explore the benefits and limitations of using a distilled policy network as the self-model. In such dual-policy agents, a model-free policy and a distilled policy are used for model-free actions and planned actions, respectively. Our results on a ecologically relevant, parametric environment indicate that distilled policy network for self-model stabilizes training, has faster i
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#26080;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#35299;&#20915;&#20102;&#36328;&#39046;&#22495;ECG&#20449;&#21495;&#20998;&#31867;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36328;&#22495;&#29305;&#24449;&#24046;&#24322;&#20248;&#21270;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#28145;&#24230;&#32593;&#32476;&#65292;&#22788;&#29702;&#20102;&#26368;&#22351;&#24773;&#20917;&#35757;&#32451;&#25439;&#22833;&#30340;&#28040;&#22833;&#65292;&#35299;&#20915;&#20102;ECG&#20449;&#21495;&#20998;&#31867;&#30340;&#38480;&#21046;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.04433</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#29992;&#20110;&#36328;&#25968;&#25454;&#24211;&#21644;&#36328;&#36890;&#36947;&#30340;&#24515;&#30005;&#20449;&#21495;&#24515;&#24459;&#22833;&#24120;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Cross-Database and Cross-Channel ECG Arrhythmia Heartbeat Classification Based on Unsupervised Domain Adaptation. (arXiv:2306.04433v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04433
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#26080;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#35299;&#20915;&#20102;&#36328;&#39046;&#22495;ECG&#20449;&#21495;&#20998;&#31867;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36328;&#22495;&#29305;&#24449;&#24046;&#24322;&#20248;&#21270;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#28145;&#24230;&#32593;&#32476;&#65292;&#22788;&#29702;&#20102;&#26368;&#22351;&#24773;&#20917;&#35757;&#32451;&#25439;&#22833;&#30340;&#28040;&#22833;&#65292;&#35299;&#20915;&#20102;ECG&#20449;&#21495;&#20998;&#31867;&#30340;&#38480;&#21046;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#30005;&#22270;(ECG)&#30340;&#20998;&#31867;&#22312;&#33258;&#21160;&#24515;&#34880;&#31649;&#35786;&#26029;&#31995;&#32479;&#30340;&#21457;&#23637;&#20013;&#25198;&#28436;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#35282;&#33394;&#12290;&#20294;&#26159;&#65292;&#19981;&#21516;&#20010;&#20307;&#20043;&#38388;ECG&#20449;&#21495;&#30340;&#30456;&#24403;&#24046;&#24322;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#25968;&#25454;&#20998;&#24067;&#30340;&#21464;&#21270;&#38480;&#21046;&#20102;&#27169;&#22411;&#30340;&#36328;&#39046;&#22495;&#21033;&#29992;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#21033;&#29992;&#20174;&#26631;&#35760;&#28304;&#22495;&#33719;&#21462;&#30340;&#30693;&#35782;&#26469;&#23545;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#20013;&#30340;ECG&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36328;&#22495;&#29305;&#24449;&#24046;&#24322;&#20248;&#21270;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#28145;&#24230;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#19977;&#20010;&#38454;&#27573;&#65306;&#39044;&#35757;&#32451;&#12289;&#32858;&#31867;&#20013;&#24515;&#35745;&#31639;&#21644;&#33258;&#36866;&#24212;&#12290;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#65292;&#25105;&#20204;&#37319;&#29992;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;(DRO)&#25216;&#26415;&#26469;&#22788;&#29702;&#26368;&#22351;&#24773;&#20917;&#35757;&#32451;&#25439;&#22833;&#30340;&#28040;&#22833;&#12290;&#20026;&#20102;&#22686;&#24378;&#29305;&#24449;&#30340;&#20016;&#23500;&#24615;&#65292;&#25105;&#20204;&#23558;&#19977;&#20010;&#26102;&#38388;&#29305;&#24449;&#19982;&#28145;&#24230;&#23398;&#20064;&#29305;&#24449;&#36830;&#25509;&#36215;&#26469;&#12290;&#32858;&#31867;&#35745;&#31639;&#38454;&#27573;&#28041;&#21450;&#35745;&#31639;&#28304;&#32858;&#31867;&#30340;&#26126;&#26174;&#21487;&#20998;&#31163;&#31751;&#30340;&#36136;&#24515;&#65292;
&lt;/p&gt;
&lt;p&gt;
The classification of electrocardiogram (ECG) plays a crucial role in the development of an automatic cardiovascular diagnostic system. However, considerable variances in ECG signals between individuals is a significant challenge. Changes in data distribution limit cross-domain utilization of a model. In this study, we propose a solution to classify ECG in an unlabeled dataset by leveraging knowledge obtained from labeled source domain. We present a domain-adaptive deep network based on cross-domain feature discrepancy optimization. Our method comprises three stages: pre-training, cluster-centroid computing, and adaptation. In pre-training, we employ a Distributionally Robust Optimization (DRO) technique to deal with the vanishing worst-case training loss. To enhance the richness of the features, we concatenate three temporal features with the deep learning features. The cluster computing stage involves computing centroids of distinctly separable clusters for the source using true labe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31283;&#23450;&#24179;&#34913;&#28857;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#25552;&#39640;&#25506;&#32034;&#24615;&#25968;&#25454;&#20998;&#26512;&#30340;&#25928;&#29575;&#21644;&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20026;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#32858;&#31867;&#21644;&#25968;&#25454;&#21487;&#35270;&#21270;&#12290;</title><link>http://arxiv.org/abs/2306.04425</link><description>&lt;p&gt;
&#21033;&#29992;&#31283;&#23450;&#24179;&#34913;&#28857;&#25512;&#36827;&#39640;&#24615;&#33021;&#25506;&#32034;&#24615;&#25968;&#25454;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Towards High-Performance Exploratory Data Analysis (EDA) Via Stable Equilibrium Point. (arXiv:2306.04425v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04425
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31283;&#23450;&#24179;&#34913;&#28857;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#25552;&#39640;&#25506;&#32034;&#24615;&#25968;&#25454;&#20998;&#26512;&#30340;&#25928;&#29575;&#21644;&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20026;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#32858;&#31867;&#21644;&#25968;&#25454;&#21487;&#35270;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#24615;&#25968;&#25454;&#20998;&#26512;&#65288;EDA&#65289;&#26159;&#25968;&#25454;&#31185;&#23398;&#39033;&#30446;&#20013;&#30340;&#37325;&#35201;&#36807;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31283;&#23450;&#24179;&#34913;&#28857;&#65288;SEP&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#25552;&#39640;EDA&#30340;&#25928;&#29575;&#21644;&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#12290;&#36890;&#36807;&#21033;&#29992;SEP&#20316;&#20026;&#20195;&#34920;&#28857;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#20026;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#32858;&#31867;&#21644;&#25968;&#25454;&#21487;&#35270;&#21270;&#12290;&#35813;&#26041;&#27861;&#30340;&#19968;&#20010;&#38750;&#24120;&#29420;&#29305;&#30340;&#23646;&#24615;&#26159;&#65292;SEP&#23558;&#30452;&#25509;&#32534;&#30721;&#25968;&#25454;&#38598;&#30340;&#32858;&#31867;&#23646;&#24615;&#12290;&#19982;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#30340;&#32858;&#31867;&#21644;&#25968;&#25454;&#21487;&#35270;&#21270;&#26041;&#27861;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20801;&#35768;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#20998;&#26512;&#20219;&#21153;&#20013;&#26174;&#30528;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#21644;&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exploratory data analysis (EDA) is a vital procedure for data science projects. In this work, we introduce a stable equilibrium point (SEP) - based framework for improving the efficiency and solution quality of EDA. By exploiting the SEPs to be the representative points, our approach aims to generate high-quality clustering and data visualization for large-scale data sets. A very unique property of the proposed method is that the SEPs will directly encode the clustering properties of data sets. Compared with prior state-of-the-art clustering and data visualization methods, the proposed methods allow substantially improving computing efficiency and solution quality for large-scale data analysis tasks.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#19982;&#20154;&#31867;&#24418;&#24577;&#26426;&#22120;&#20154;&#30340;&#28216;&#25103;&#21270;&#20250;&#35805;&#65292;&#21487;&#20197;&#25552;&#39640;&#20799;&#31461;&#21742;&#21912;&#24739;&#32773;&#30340;&#27835;&#30103;&#20381;&#20174;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.04422</link><description>&lt;p&gt;
&#31038;&#20132;&#26426;&#22120;&#20154;&#22312;&#20799;&#31461;&#21742;&#21912;&#27835;&#30103;&#20381;&#20174;&#24615;&#26041;&#38754;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Social robots to improve therapeutic adherence in pediatric asthma. (arXiv:2306.04422v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04422
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#19982;&#20154;&#31867;&#24418;&#24577;&#26426;&#22120;&#20154;&#30340;&#28216;&#25103;&#21270;&#20250;&#35805;&#65292;&#21487;&#20197;&#25552;&#39640;&#20799;&#31461;&#21742;&#21912;&#24739;&#32773;&#30340;&#27835;&#30103;&#20381;&#20174;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24930;&#24615;&#30142;&#30149;&#20013;&#65292;&#27491;&#30830;&#35786;&#26029;&#21644;&#25552;&#20379;&#26368;&#36866;&#24403;&#30340;&#27835;&#30103;&#36890;&#24120;&#24182;&#19981;&#36275;&#20197;&#20445;&#35777;&#24739;&#32773;&#20020;&#24202;&#29366;&#20917;&#30340;&#25913;&#21892;&#12290;&#20302;&#27835;&#30103;&#20381;&#20174;&#24615;&#26159;&#38459;&#30861;&#36798;&#21040;&#27835;&#30103;&#30446;&#26631;&#30340;&#20027;&#35201;&#21407;&#22240;&#20043;&#19968;&#65292;&#23588;&#20854;&#26159;&#22312;&#26576;&#20123;&#30142;&#30149;&#21644;&#29305;&#23450;&#30446;&#26631;&#24739;&#32773;&#65288;&#22914;&#20799;&#31461;&#65289;&#20013;&#23588;&#20026;&#24120;&#35265;&#12290;&#19968;&#31181;&#26377;&#36259;&#30340;&#25216;&#26415;&#21487;&#20197;&#29992;&#20110;&#25903;&#25345;&#20020;&#24202;&#23454;&#36341;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#20581;&#24247;&#32467;&#26524;&#12290;&#25105;&#20204;&#20551;&#35774;&#19982;&#20256;&#32479;&#30340;&#27835;&#30103;&#25945;&#32946;&#26041;&#27861;&#30456;&#27604;&#65292;&#19982;&#20154;&#31867;&#24418;&#24577;&#26426;&#22120;&#20154;&#30340;&#28216;&#25103;&#21270;&#20250;&#35805;&#21487;&#20197;&#26356;&#21152;&#26377;&#25928;&#22320;&#25945;&#25480;&#20799;&#31461;&#21742;&#21912;&#24739;&#32773;&#27491;&#30830;&#30340;&#21560;&#20837;&#26041;&#27861;&#12290;&#22312;&#36825;&#20010;&#35282;&#24230;&#19978;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#22312;Pepper&#26426;&#22120;&#20154;&#24179;&#21488;&#19978;&#23454;&#29616;&#30340;&#20132;&#20114;&#24335;&#27169;&#22359;&#20197;&#21450;&#35745;&#21010;&#20110;2020&#24180;&#22312;Palermo&#30340;CNR&#32954;&#37096;&#36807;&#25935;&#20818;&#31461;&#35786;&#25152;&#24320;&#23637;&#30340;&#19968;&#39033;&#30740;&#31350;&#12290;&#30001;&#20110;COVID-19&#32039;&#24613;&#24773;&#20917;&#30340;&#21407;&#22240;&#65292;&#36825;&#39033;&#30740;&#31350;&#34987;&#21462;&#28040;&#65292;&#20294;&#25105;&#20204;&#25552;&#20379;&#21021;&#27493;&#32467;&#26524;&#65292;&#20197;&#35780;&#20272;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#21644;&#21516;&#40836;&#20154;&#30340;&#20852;&#36259;&#12290;&#36890;&#36807;&#20998;&#26512;&#22521;&#35757;&#21069;&#21518;&#30340;&#21560;&#20837;&#20064;&#24815;&#30340;&#27604;&#36739;&#21644;&#32473;&#21442;&#19982;&#32773;&#30340;&#38382;&#21367;&#35843;&#26597;&#32467;&#26524;&#26469;&#35780;&#20272;&#25945;&#32946;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In chronic diseases, obtaining a correct diagnosis and providing the most appropriate treatments often is not enough to guarantee an improvement of the clinical condition of a patient. Poor adherence to medical prescriptions constitutes one of the main causes preventing achievement of therapeutic goals. This is generally true especially for certain diseases and specific target patients, such as children. An engaging and entertaining technology can be exploited in support of clinical practices to achieve better health outcomes. Our assumption is that a gamified session with a humanoid robot, compared to the usual methodologies for therapeutic education, can be more incisive in learning the correct inhalation procedure in children affected by asthma. In this perspective, we describe an interactive module implemented on the Pepper robotic platform and the setting of a study that was planned in 2020 to be held at the Pneumoallergology Pediatric clinic of CNR in Palermo. The study was cance
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#31639;&#27861;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#29983;&#25104;&#27801;&#31890;&#32452;&#35013;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28857;&#20113;&#33258;&#32534;&#30721;&#22120;&#25226;&#27801;&#31890;&#30340;&#19977;&#32500;&#28857;&#20113;&#32467;&#26500;&#32534;&#30721;&#21040;&#36739;&#20302;&#32500;&#24230;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#65292;&#29983;&#25104;&#30340;&#27801;&#31890;&#26679;&#26412;&#31526;&#21512;&#21407;&#22987;&#25968;&#25454;&#20998;&#24067;&#65292;&#24182;&#21487;&#29992;&#20110;&#22320;&#36136;&#24037;&#31243;&#12289;&#35745;&#31639;&#26426;&#21160;&#30011;&#12289;&#30707;&#27833;&#24037;&#31243;&#31561;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2306.04411</link><description>&lt;p&gt;
&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#25216;&#26415;&#21512;&#25104;&#36924;&#30495;&#30340;&#27801;&#31890;&#32452;&#35013;
&lt;/p&gt;
&lt;p&gt;
Synthesizing realistic sand assemblies with denoising diffusion in latent space. (arXiv:2306.04411v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04411
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#31639;&#27861;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#29983;&#25104;&#27801;&#31890;&#32452;&#35013;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28857;&#20113;&#33258;&#32534;&#30721;&#22120;&#25226;&#27801;&#31890;&#30340;&#19977;&#32500;&#28857;&#20113;&#32467;&#26500;&#32534;&#30721;&#21040;&#36739;&#20302;&#32500;&#24230;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#65292;&#29983;&#25104;&#30340;&#27801;&#31890;&#26679;&#26412;&#31526;&#21512;&#21407;&#22987;&#25968;&#25454;&#20998;&#24067;&#65292;&#24182;&#21487;&#29992;&#20110;&#22320;&#36136;&#24037;&#31243;&#12289;&#35745;&#31639;&#26426;&#21160;&#30011;&#12289;&#30707;&#27833;&#24037;&#31243;&#31561;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27801;&#31890;&#32452;&#35013;&#20013;&#30340;&#24418;&#24577;&#21644;&#24418;&#24577;&#29305;&#24449;&#22312;&#35768;&#22810;&#24037;&#31243;&#24212;&#29992;&#20013;&#65292;&#22914;&#22320;&#36136;&#24037;&#31243;&#12289;&#35745;&#31639;&#26426;&#21160;&#30011;&#12289;&#30707;&#27833;&#24037;&#31243;&#21644;&#27987;&#32553;&#22826;&#38451;&#33021;&#26041;&#38754;&#20855;&#26377;&#28145;&#36828;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21435;&#22122;&#25193;&#25955;&#31639;&#27861;&#65292;&#21033;&#29992;&#20174;&#21333;&#20010;&#27801;&#31890;&#34920;&#38754;&#25910;&#38598;&#30340;&#19968;&#32452;&#28857;&#20113;&#29983;&#25104;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#27801;&#31890;&#12290;&#36890;&#36807;&#20351;&#29992;&#28857;&#20113;&#33258;&#32534;&#30721;&#22120;&#65292;&#25226;&#27801;&#31890;&#30340;&#19977;&#32500;&#28857;&#20113;&#32467;&#26500;&#39318;&#20808;&#32534;&#30721;&#21040;&#36739;&#20302;&#32500;&#24230;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#12290;&#35757;&#32451;&#20102;&#19968;&#31181;&#29983;&#25104;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65292;&#29992;&#26469;&#29983;&#25104;&#26368;&#22823;&#21270;&#29983;&#25104;&#26679;&#26412;&#23646;&#20110;&#21407;&#22987;&#25968;&#25454;&#20998;&#24067;&#30340;&#23545;&#25968;&#20284;&#28982;&#30340;&#21512;&#25104;&#27801;&#31890;&#65292;&#36825;&#36890;&#36807;Kullback-Leibler&#25955;&#24230;&#26469;&#27979;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The shapes and morphological features of grains in sand assemblies have far-reaching implications in many engineering applications, such as geotechnical engineering, computer animations, petroleum engineering, and concentrated solar power. Yet, our understanding of the influence of grain geometries on macroscopic response is often only qualitative, due to the limited availability of high-quality 3D grain geometry data. In this paper, we introduce a denoising diffusion algorithm that uses a set of point clouds collected from the surface of individual sand grains to generate grains in the latent space. By employing a point cloud autoencoder, the three-dimensional point cloud structures of sand grains are first encoded into a lower-dimensional latent space. A generative denoising diffusion probabilistic model is trained to produce synthetic sand that maximizes the log-likelihood of the generated samples belonging to the original data distribution measured by a Kullback-Leibler divergence.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#21040;&#28023;&#39532;&#21644;&#21069;&#39069;&#21494;&#30382;&#36136;&#21551;&#21457;&#30340;&#29983;&#29289;&#21487;&#34892;&#20803;&#23398;&#20064;&#27169;&#22411;&#65292;&#20351;&#29992;&#24102;&#26377;&#22870;&#21169;&#30340;&#23398;&#20064;&#31995;&#32479;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65292;&#20855;&#26377;&#22312;&#26368;&#23567;&#25968;&#25454;&#19979;&#36866;&#24212;&#26032;&#20219;&#21153;&#21644;&#24555;&#36895;&#23398;&#20064;&#36830;&#32493;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#36991;&#20813;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#29616;&#35937;&#65292;&#24182;&#19988;&#20248;&#20110;&#20854;&#20182;&#29983;&#29289;&#21487;&#34892;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.04410</link><description>&lt;p&gt;
&#20197;&#22870;&#21169;&#35843;&#21046;STDP&#20026;&#29305;&#24449;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#20803;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Meta-Learning in Spiking Neural Networks with Reward-Modulated STDP. (arXiv:2306.04410v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04410
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#21040;&#28023;&#39532;&#21644;&#21069;&#39069;&#21494;&#30382;&#36136;&#21551;&#21457;&#30340;&#29983;&#29289;&#21487;&#34892;&#20803;&#23398;&#20064;&#27169;&#22411;&#65292;&#20351;&#29992;&#24102;&#26377;&#22870;&#21169;&#30340;&#23398;&#20064;&#31995;&#32479;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65292;&#20855;&#26377;&#22312;&#26368;&#23567;&#25968;&#25454;&#19979;&#36866;&#24212;&#26032;&#20219;&#21153;&#21644;&#24555;&#36895;&#23398;&#20064;&#36830;&#32493;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#36991;&#20813;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#29616;&#35937;&#65292;&#24182;&#19988;&#20248;&#20110;&#20854;&#20182;&#29983;&#29289;&#21487;&#34892;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#22823;&#33041;&#36890;&#36807;&#25226;&#33719;&#24471;&#30340;&#30693;&#35782;&#21644;&#32463;&#39564;&#25972;&#21512;&#21040;&#35760;&#24518;&#20013;&#65292;&#19981;&#26029;&#23398;&#20064;&#24182;&#36805;&#36895;&#36866;&#24212;&#26032;&#29615;&#22659;&#12290;&#22312;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#65292;&#24320;&#21457;&#36825;&#31181;&#33021;&#21147;&#34987;&#35270;&#20026;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#30340;&#37325;&#35201;&#30446;&#26631;&#65292;&#22240;&#20026;&#24403;&#25968;&#25454;&#26377;&#38480;&#25110;&#32773;&#38656;&#35201;&#24555;&#36895;&#36866;&#24212;&#26032;&#30340;&#26410;&#30693;&#20219;&#21153;&#26102;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#34920;&#29616;&#19981;&#20339;&#12290;&#36817;&#26399;&#25552;&#20986;&#20102;&#19968;&#20123;&#22312;&#39640;&#24615;&#33021;&#27700;&#24179;&#19978;&#34920;&#29616;&#20986;&#20302;&#25968;&#25454;&#23398;&#20064;&#29305;&#24615;&#30340;&#20803;&#23398;&#20064;&#27169;&#22411;&#65292;&#20294;&#23427;&#20204;&#19981;&#31526;&#21512;&#29983;&#29289;&#23398;&#30340;&#23454;&#29616;&#26041;&#24335;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#21040;&#28023;&#39532;&#21644;&#21069;&#39069;&#21494;&#30382;&#36136;&#21551;&#21457;&#30340;&#29983;&#29289;&#21487;&#34892;&#20803;&#23398;&#20064;&#27169;&#22411;&#65292;&#20351;&#29992;&#24102;&#26377;&#22870;&#21169;&#30340;&#23398;&#20064;&#31995;&#32479;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21253;&#25324;&#19968;&#20010;&#26088;&#22312;&#38450;&#27490;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#35760;&#24518;&#65292;&#36825;&#31181;&#29616;&#35937;&#22312;&#20803;&#23398;&#20064;&#27169;&#22411;&#22312;&#26032;&#20219;&#21153;&#24320;&#22987;&#26102;&#20250;&#24536;&#35760;&#23427;&#20204;&#25152;&#23398;&#30340;&#20869;&#23481;&#65292;&#21516;&#26102;&#25105;&#20204;&#30340;&#26032;&#27169;&#22411;&#20855;&#26377;&#22312;&#26368;&#23567;&#25968;&#25454;&#19979;&#36866;&#24212;&#26032;&#20219;&#21153;&#21644;&#24555;&#36895;&#23398;&#20064;&#36830;&#32493;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#25552;&#20986;&#30340;&#27169;&#22411;&#20248;&#20110;&#20854;&#20182;&#29983;&#29289;&#21487;&#34892;&#27169;&#22411;&#65292;&#24182;&#19982;&#31070;&#32463;&#29983;&#29289;&#23398;&#21457;&#29616;&#30456;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
The human brain constantly learns and rapidly adapts to new situations by integrating acquired knowledge and experiences into memory. Developing this capability in machine learning models is considered an important goal of AI research since deep neural networks perform poorly when there is limited data or when they need to adapt quickly to new unseen tasks. Meta-learning models are proposed to facilitate quick learning in low-data regimes by employing absorbed information from the past. Although some models have recently been introduced that reached high-performance levels, they are not biologically plausible. We have proposed a bio-plausible meta-learning model inspired by the hippocampus and the prefrontal cortex using spiking neural networks with a reward-based learning system. Our proposed model includes a memory designed to prevent catastrophic forgetting, a phenomenon that occurs when meta-learning models forget what they have learned as soon as the new task begins. Also, our new
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#20844;&#24179;&#20998;&#31867;&#22120;&#65292;&#21033;&#29992;&#19977;&#20803;&#32452;&#25439;&#22833;&#30340;&#36793;&#38469;&#38480;&#21046;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25152;&#36896;&#25104;&#30340;&#20559;&#35265;&#65292;&#24182;&#37319;&#29992;&#19977;&#20803;&#32452;&#25240;&#21472;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.04400</link><description>&lt;p&gt;
&#19968;&#31181;&#37319;&#29992;&#19977;&#20803;&#32452;&#25240;&#21472;&#30340;&#20844;&#24179;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
A Fair Classifier Embracing Triplet Collapse. (arXiv:2306.04400v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04400
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#20844;&#24179;&#20998;&#31867;&#22120;&#65292;&#21033;&#29992;&#19977;&#20803;&#32452;&#25439;&#22833;&#30340;&#36793;&#38469;&#38480;&#21046;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25152;&#36896;&#25104;&#30340;&#20559;&#35265;&#65292;&#24182;&#37319;&#29992;&#19977;&#20803;&#32452;&#25240;&#21472;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19977;&#20803;&#32452;&#25439;&#22833;&#30340;&#34892;&#20026;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22914;&#20309;&#34987;&#21033;&#29992;&#26469;&#38480;&#21046;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25152;&#20135;&#29983;&#30340;&#20559;&#35265;&#12290;&#25105;&#20204;&#30340;&#20844;&#24179;&#20998;&#31867;&#22120;&#22312;&#20351;&#29992;&#38543;&#26426;&#19977;&#20803;&#32452;&#36873;&#25321;&#26102;&#65292;&#22312;&#19977;&#20803;&#32452;&#25439;&#22833;&#30340;&#36793;&#38469;&#22823;&#20110;&#28508;&#22312;&#31354;&#38388;&#20013;&#20219;&#24847;&#20004;&#28857;&#20043;&#38388;&#30340;&#26368;&#22823;&#36317;&#31163;&#26102;&#37319;&#29992;&#19977;&#20803;&#32452;&#25240;&#21472;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study the behaviour of the triplet loss and show that it can be exploited to limit the biases created and perpetuated by machine learning models. Our fair classifier uses the collapse of the triplet loss when its margin is greater than the maximum distance between two points in the latent space, in the case of stochastic triplet selection.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38750;&#23545;&#31216;&#26799;&#24230;&#24341;&#23548;&#26469;&#25351;&#23548;&#25193;&#25955;&#37319;&#26679;&#30340;&#21453;&#21521;&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#20197;&#25913;&#21892;&#25193;&#25955;&#22270;&#20687;&#32763;&#35793;&#30340;&#39118;&#26684;&#36716;&#25442;&#21644;&#20869;&#23481;&#20445;&#30041;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2306.04396</link><description>&lt;p&gt;
&#20351;&#29992;&#38750;&#23545;&#31216;&#26799;&#24230;&#24341;&#23548;&#26469;&#25913;&#36827;&#25193;&#25955;&#22270;&#20687;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Improving Diffusion-based Image Translation using Asymmetric Gradient Guidance. (arXiv:2306.04396v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04396
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38750;&#23545;&#31216;&#26799;&#24230;&#24341;&#23548;&#26469;&#25351;&#23548;&#25193;&#25955;&#37319;&#26679;&#30340;&#21453;&#21521;&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#20197;&#25913;&#21892;&#25193;&#25955;&#22270;&#20687;&#32763;&#35793;&#30340;&#39118;&#26684;&#36716;&#25442;&#21644;&#20869;&#23481;&#20445;&#30041;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25193;&#25955;&#27169;&#22411;&#22312;&#22270;&#20687;&#32763;&#35793;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#30528;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#38543;&#26426;&#24615;&#65292;&#36890;&#24120;&#23384;&#22312;&#30528;&#39118;&#26684;&#36716;&#25442;&#21644;&#20869;&#23481;&#20445;&#30041;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38750;&#23545;&#31216;&#26799;&#24230;&#24341;&#23548;&#26469;&#25351;&#23548;&#25193;&#25955;&#37319;&#26679;&#30340;&#21453;&#21521;&#36807;&#31243;&#30340;&#26041;&#27861;&#12290;&#36825;&#23548;&#33268;&#20102;&#26356;&#24555;&#21644;&#26356;&#31283;&#23450;&#30340;&#22270;&#20687;&#25805;&#20316;&#65292;&#36866;&#29992;&#20110;&#22522;&#20110;&#25991;&#26412;&#21644;&#22270;&#29255;&#30340;&#22270;&#20687;&#32763;&#35793;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have shown significant progress in image translation tasks recently. However, due to their stochastic nature, there's often a trade-off between style transformation and content preservation. Current strategies aim to disentangle style and content, preserving the source image's structure while successfully transitioning from a source to a target domain under text or one-shot image conditions. Yet, these methods often require computationally intense fine-tuning of diffusion models or additional neural networks. To address these challenges, here we present an approach that guides the reverse process of diffusion sampling by applying asymmetric gradient guidance. This results in quicker and more stable image manipulation for both text-guided and image-guided image translation. Our model's adaptability allows it to be implemented with both image- and latent-diffusion models. Experiments show that our method outperforms various state-of-the-art models in image translation ta
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#32763;&#35793;&#21644;&#36328;&#35821;&#35328;&#36801;&#31227;&#20004;&#31181;&#26041;&#27861;&#26469;&#25191;&#34892;&#20020;&#24202;&#39046;&#22495;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#65292;&#24182;&#35777;&#26126;&#36328;&#35821;&#35328;&#36801;&#31227;&#27604;&#36825;&#20004;&#31181;&#32763;&#35793;&#26041;&#27861;&#22312;&#27861;&#35821;&#21644;&#24503;&#35821;&#20013;&#37117;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.04384</link><description>&lt;p&gt;
&#22810;&#35821;&#35328;&#20020;&#24202;&#23454;&#20307;&#35782;&#21035;&#65306;&#32763;&#35793;&#36824;&#26159;&#36328;&#35821;&#35328;&#36801;&#31227;&#65311;
&lt;/p&gt;
&lt;p&gt;
Multilingual Clinical NER: Translation or Cross-lingual Transfer?. (arXiv:2306.04384v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04384
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#32763;&#35793;&#21644;&#36328;&#35821;&#35328;&#36801;&#31227;&#20004;&#31181;&#26041;&#27861;&#26469;&#25191;&#34892;&#20020;&#24202;&#39046;&#22495;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#65292;&#24182;&#35777;&#26126;&#36328;&#35821;&#35328;&#36801;&#31227;&#27604;&#36825;&#20004;&#31181;&#32763;&#35793;&#26041;&#27861;&#22312;&#27861;&#35821;&#21644;&#24503;&#35821;&#20013;&#37117;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#39046;&#22495;&#38750;&#33521;&#35821;&#25991;&#26412;&#20013;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#31561;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#21487;&#33021;&#38750;&#24120;&#32791;&#26102;&#21644;&#26114;&#36149;&#65292;&#22240;&#20026;&#32570;&#20047;&#26631;&#27880;&#25968;&#25454;&#12290;&#36328;&#35821;&#35328;&#36801;&#31227;&#65288;CLT&#65289;&#26159;&#19968;&#31181;&#32469;&#36807;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#22312;&#19968;&#20010;&#35821;&#35328;&#19978;&#23545;&#29305;&#23450;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#22312;&#21478;&#19968;&#20010;&#35821;&#35328;&#19978;&#25552;&#20379;&#39640;&#31934;&#24230;&#12290;&#28982;&#32780;&#65292;&#36824;&#26377;&#20854;&#20182;&#21033;&#29992;&#32763;&#35793;&#27169;&#22411;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#30446;&#26631;&#35821;&#35328;&#20013;&#25191;&#34892;NER&#65292;&#32780;&#26080;&#38656;&#26631;&#27880;&#25968;&#25454;&#65292;&#21487;&#20197;&#36890;&#36807;&#32763;&#35793;&#35757;&#32451;&#38598;&#25110;&#27979;&#35797;&#38598;&#12290;&#26412;&#25991;&#27604;&#36739;&#20102;&#36328;&#35821;&#35328;&#36801;&#31227;&#19982;&#36825;&#20004;&#31181;&#26367;&#20195;&#26041;&#27861;&#65292;&#20197;&#22312;&#27861;&#35821;&#21644;&#24503;&#35821;&#20013;&#25191;&#34892;&#20020;&#24202;NER&#65292;&#32780;&#19981;&#38656;&#35201;&#36825;&#20123;&#35821;&#35328;&#30340;&#20219;&#20309;&#35757;&#32451;&#25968;&#25454;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;MedNERF&#65292;&#36825;&#26159;&#20174;&#27861;&#22269;&#33647;&#29289;&#22788;&#26041;&#20013;&#25552;&#21462;&#30340;&#21307;&#23398;NER&#27979;&#35797;&#38598;&#65292;&#24182;&#20351;&#29992;&#19982;&#33521;&#35821;&#25968;&#25454;&#38598;&#30456;&#21516;&#30340;&#25351;&#21335;&#36827;&#34892;&#20102;&#27880;&#37322;&#12290;&#36890;&#36807;&#23545;&#36825;&#20010;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#24503;&#35821;&#21307;&#23398;&#35821;&#26009;&#24211;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#36328;&#35821;&#35328;&#36801;&#31227;&#27604;&#20004;&#31181;&#32763;&#35793;&#26041;&#27861;&#22312;&#20004;&#31181;&#30446;&#26631;&#35821;&#35328;&#20013;&#37117;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural language tasks like Named Entity Recognition (NER) in the clinical domain on non-English texts can be very time-consuming and expensive due to the lack of annotated data. Cross-lingual transfer (CLT) is a way to circumvent this issue thanks to the ability of multilingual large language models to be fine-tuned on a specific task in one language and to provide high accuracy for the same task in another language. However, other methods leveraging translation models can be used to perform NER without annotated data in the target language, by either translating the training set or test set. This paper compares cross-lingual transfer with these two alternative methods, to perform clinical NER in French and in German without any training data in those languages. To this end, we release MedNERF a medical NER test set extracted from French drug prescriptions and annotated with the same guidelines as an English dataset. Through extensive experiments on this dataset and on a German medica
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;DFM&#26694;&#26550;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#37327;&#21270;&#26631;&#31614;&#20559;&#31227;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#24615;&#33021;&#19978;&#38480;&#21644;&#40065;&#26834;&#24615;&#12290;&#20351;&#29992;&#22522;&#20110;&#26680;&#30340;DFM&#29256;&#26412;&#21487;&#20197;&#25552;&#39640;&#25928;&#29575;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.04376</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#24067;&#29305;&#24449;&#21305;&#37197;&#30340;&#26631;&#31614;&#20559;&#31227;&#37327;&#37327;&#21270;&#21450;&#20854;&#40065;&#26834;&#24615;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Label Shift Quantification with Robustness Guarantees via Distribution Feature Matching. (arXiv:2306.04376v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04376
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;DFM&#26694;&#26550;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#37327;&#21270;&#26631;&#31614;&#20559;&#31227;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#24615;&#33021;&#19978;&#38480;&#21644;&#40065;&#26834;&#24615;&#12290;&#20351;&#29992;&#22522;&#20110;&#26680;&#30340;DFM&#29256;&#26412;&#21487;&#20197;&#25552;&#39640;&#25928;&#29575;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#23398;&#20064;&#22788;&#29702;&#22312;&#26631;&#31614;&#20559;&#31227;&#19979;&#20272;&#35745;&#30446;&#26631;&#26631;&#31614;&#20998;&#24067;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#20998;&#24067;&#29305;&#24449;&#21305;&#37197;&#65288;DFM&#65289;&#65292;&#23558;&#20808;&#21069;&#25991;&#29486;&#20013;&#24341;&#20837;&#30340;&#21508;&#31181;&#20272;&#35745;&#22120;&#24674;&#22797;&#20026;&#29305;&#23450;&#23454;&#20363;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;DFM&#31243;&#24207;&#30340;&#19968;&#33324;&#24615;&#33021;&#30028;&#65292;&#25913;&#36827;&#20102;&#20808;&#21069;&#22312;&#29305;&#23450;&#24773;&#20917;&#19979;&#25512;&#23548;&#30340;&#30028;&#38480;&#30340;&#33509;&#24178;&#20851;&#38190;&#26041;&#38754;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#36825;&#19968;&#20998;&#26512;&#25193;&#23637;&#21040;&#30740;&#31350;DFM&#31243;&#24207;&#22312;&#26410;&#31934;&#30830;&#20551;&#35774;&#26631;&#31614;&#20559;&#31227;&#37327;&#30340;&#24773;&#20917;&#19979;&#30340;&#40065;&#26834;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#30446;&#26631;&#21463;&#21040;&#26410;&#30693;&#20998;&#24067;&#27745;&#26579;&#30340;&#24773;&#20917;&#19979;&#12290;&#36825;&#20123;&#29702;&#35770;&#21457;&#29616;&#22312;&#27169;&#25311;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#20102;&#35814;&#32454;&#30340;&#25968;&#23383;&#30740;&#31350;&#30830;&#35748;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#38543;&#26426;&#20613;&#37324;&#21494;&#29305;&#24449;&#21407;&#29702;&#20171;&#32461;&#20102;&#19968;&#31181;&#39640;&#25928;&#65292;&#21487;&#25193;&#23637;&#19988;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#22522;&#20110;&#26680;&#30340;DFM&#29256;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantification learning deals with the task of estimating the target label distribution under label shift. In this paper, we first present a unifying framework, distribution feature matching (DFM), that recovers as particular instances various estimators introduced in previous literature. We derive a general performance bound for DFM procedures, improving in several key aspects upon previous bounds derived in particular cases. We then extend this analysis to study robustness of DFM procedures in the misspecified setting under departure from the exact label shift hypothesis, in particular in the case of contamination of the target by an unknown distribution. These theoretical findings are confirmed by a detailed numerical study on simulated and real-world datasets. We also introduce an efficient, scalable and robust version of kernel-based DFM using the Random Fourier Feature principle.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GCN&#21487;&#20449;&#24230;&#39044;&#27979;&#30340;&#21327;&#21516;&#31227;&#21160;&#32676;&#24863;&#30693;&#30340;&#39640;&#25928;&#25307;&#21215;&#31574;&#30053;&#65292;&#36890;&#36807;&#25429;&#33719;&#24037;&#20154;&#20043;&#38388;&#30340;&#38750;&#23545;&#31216;&#20449;&#20219;&#20851;&#31995;&#21644;&#24037;&#20154;&#33021;&#21147;&#26469;&#23454;&#29616;&#26377;&#25928;&#30340;&#20219;&#21153;&#20998;&#37197;&#65292;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.04366</link><description>&lt;p&gt;
&#22522;&#20110;GCN&#21487;&#20449;&#24230;&#39044;&#27979;&#30340;&#21327;&#21516;&#31227;&#21160;&#32676;&#24863;&#30693;&#30340;&#39640;&#25928;&#25307;&#21215;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Efficient Recruitment Strategy for Collaborative Mobile Crowd Sensing Based on GCN Trustworthiness Prediction. (arXiv:2306.04366v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04366
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GCN&#21487;&#20449;&#24230;&#39044;&#27979;&#30340;&#21327;&#21516;&#31227;&#21160;&#32676;&#24863;&#30693;&#30340;&#39640;&#25928;&#25307;&#21215;&#31574;&#30053;&#65292;&#36890;&#36807;&#25429;&#33719;&#24037;&#20154;&#20043;&#38388;&#30340;&#38750;&#23545;&#31216;&#20449;&#20219;&#20851;&#31995;&#21644;&#24037;&#20154;&#33021;&#21147;&#26469;&#23454;&#29616;&#26377;&#25928;&#30340;&#20219;&#21153;&#20998;&#37197;&#65292;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21327;&#21516;&#31227;&#21160;&#32676;&#24863;&#30693;&#21487;&#20197;&#36890;&#36807;&#20419;&#36827;&#20219;&#21153;&#24863;&#30693;&#30340;&#22242;&#38431;&#21512;&#20316;&#26469;&#25552;&#39640;&#25968;&#25454;&#36136;&#37327;&#21644;&#35206;&#30422;&#33539;&#22260;&#65292;&#32780;&#24037;&#20154;&#25307;&#21215;&#21017;&#20195;&#34920;&#30528;&#19968;&#20010;&#22797;&#26434;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#12290;&#29616;&#26377;&#31574;&#30053;&#20027;&#35201;&#20851;&#27880;&#24037;&#20154;&#26412;&#36523;&#30340;&#29305;&#24449;&#65292;&#24573;&#30053;&#20102;&#24037;&#20154;&#20043;&#38388;&#30340;&#38750;&#23545;&#31216;&#20449;&#20219;&#20851;&#31995;&#65292;&#20174;&#32780;&#24433;&#21709;&#20102;&#20219;&#21153;&#25928;&#29992;&#35780;&#20272;&#30340;&#21512;&#29702;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#39318;&#20808;&#20351;&#29992;Mini-Batch K-Means&#32858;&#31867;&#31639;&#27861;&#21644;&#36793;&#32536;&#26381;&#21153;&#22120;&#26469;&#23454;&#29616;&#39640;&#25928;&#30340;&#20998;&#24067;&#24335;&#24037;&#20154;&#25307;&#21215;&#12290;&#21033;&#29992;&#21382;&#21490;&#25968;&#25454;&#21644;&#20219;&#21153;&#35201;&#27714;&#33719;&#24471;&#24037;&#20154;&#30340;&#33021;&#21147;&#31867;&#22411;&#21644;&#36317;&#31163;&#12290;&#20351;&#29992;&#24037;&#20154;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#20449;&#20219;&#23548;&#21521;&#22270;&#36755;&#20837;&#33267;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#26694;&#26550;&#36827;&#34892;&#35757;&#32451;&#65292;&#25429;&#33719;&#24037;&#20154;&#20043;&#38388;&#30340;&#38750;&#23545;&#31216;&#20449;&#20219;&#20851;&#31995;&#12290;&#36890;&#36807;&#24037;&#20154;&#20043;&#38388;&#30340;&#39640;&#20449;&#20219;&#20540;&#65292;&#38450;&#27490;CMCS&#22330;&#26223;&#19979;&#30340;&#38544;&#31169;&#27844;&#38706;&#12290;&#26368;&#32456;&#65292;&#21033;&#29992;&#39044;&#27979;&#30340;&#20449;&#20219;&#21644;&#24037;&#20154;&#33021;&#21147;&#26500;&#24314;&#20102;&#19968;&#20010;&#26080;&#21521;&#25307;&#21215;&#22270;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#20219;&#21153;&#20998;&#37197;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#36825;&#31181;&#25307;&#21215;&#26041;&#27861;&#22312;&#25307;&#21215;&#20934;&#30830;&#24230;&#12289;&#20219;&#21153;&#23436;&#25104;&#26102;&#38388;&#21644;&#33021;&#37327;&#28040;&#32791;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Collaborative Mobile Crowd Sensing (CMCS) enhances data quality and coverage by promoting teamwork in task sensing, with worker recruitment representing a complex multi-objective optimization problem. Existing strategies mainly focus on the characteristics of workers themselves, neglecting the asymmetric trust relationships between them, which affects the rationality of task utility evaluation. To address this, this paper first employs the Mini-Batch K-Means clustering algorithm and deploys edge servers to enable efficient distributed worker recruitment. Historical data and task requirements are utilized to obtain workers' ability types and distances. A trust-directed graph in the worker's social network is input into the Graph Convolutional Network (GCN) framework for training, capturing asymmetric trustworthiness between worker pairs. Privacy leakage is prevented in CMCS scenarios through high trust values between workers. Ultimately, an undirected recruitment graph is constructed us
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#22522;&#24102;&#20449;&#21495;&#26469;&#24555;&#36895;&#20934;&#30830;&#22320;&#35786;&#26029;APA&#20013;&#30340;&#25925;&#38556;&#20803;&#20214;&#21644;&#37096;&#20214;&#65292;&#24182;&#35777;&#26126;&#20854;&#20855;&#26377;&#39640;98%&#30340;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.04360</link><description>&lt;p&gt;
&#22522;&#20110;&#22522;&#24102;&#20449;&#21495;&#30340;&#27627;&#31859;&#27874;&#26377;&#28304;&#30456;&#25511;&#38453;&#22825;&#32447;&#40065;&#26834;&#39640;&#25928;&#25925;&#38556;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Robust and Efficient Fault Diagnosis of mm-Wave Active Phased Arrays using Baseband Signal. (arXiv:2306.04360v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04360
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#22522;&#24102;&#20449;&#21495;&#26469;&#24555;&#36895;&#20934;&#30830;&#22320;&#35786;&#26029;APA&#20013;&#30340;&#25925;&#38556;&#20803;&#20214;&#21644;&#37096;&#20214;&#65292;&#24182;&#35777;&#26126;&#20854;&#20855;&#26377;&#39640;98%&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
5G&#21644;6G&#26080;&#32447;&#30005;&#20013;&#19968;&#20010;&#20851;&#38190;&#30340;&#36890;&#20449;&#27169;&#22359;&#26159;&#26377;&#28304;&#30456;&#25511;&#38453;&#22825;&#32447;(APA)&#65292;&#20026;&#30830;&#20445;&#21487;&#38752;&#25805;&#20316;&#65292;&#29616;&#22330;&#39640;&#25928;&#21450;&#26102;&#30340;apa&#25925;&#38556;&#35786;&#26029;&#33267;&#20851;&#37325;&#35201;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#25925;&#38556;&#35786;&#26029;&#19968;&#30452;&#20381;&#38752;&#20351;&#29992;&#26114;&#36149;&#30340;&#35774;&#22791;&#21644;&#22810;&#20010;&#20005;&#26684;&#25511;&#21046;&#30340;&#27979;&#37327;&#25506;&#38024;&#27979;&#37327;&#39057;&#22495;&#36752;&#23556;&#22270;&#26696;&#65292;&#36825;&#20123;&#26041;&#27861;&#32791;&#26102;&#12289;&#22797;&#26434;&#65292;&#22240;&#27492;&#22312;&#29616;&#22330;&#37096;&#32626;&#20013;&#19981;&#21487;&#34892;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#26469;&#25552;&#21462;&#22522;&#24102;&#20013;&#38544;&#34255;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#21306;&#20998;&#19981;&#21516;&#30340;&#25925;&#38556;&#65292;&#21482;&#38656;&#22312;&#19968;&#20010;&#27979;&#37327;&#28857;&#20351;&#29992;&#19968;&#20010;&#25506;&#38024;&#21363;&#21487;&#24555;&#36895;&#20934;&#30830;&#22320;&#35786;&#26029;apa&#20013;&#30340;&#25925;&#38556;&#20803;&#20214;&#21644;&#37096;&#20214;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#29992;&#21830;&#29992;28 GHz APA&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#38024;&#23545;&#21333;&#20803;&#32032;&#21644;&#22810;&#20803;&#32032;&#25925;&#38556;&#26816;&#27979;&#65292;&#20998;&#21035;&#23637;&#31034;&#20102;99%&#21644;80%&#30340;&#20934;&#30830;&#29575;&#12290;&#30740;&#31350;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#27979;&#35797;&#22330;&#26223;&#65306;on
&lt;/p&gt;
&lt;p&gt;
One key communication block in 5G and 6G radios is the active phased array (APA). To ensure reliable operation, efficient and timely fault diagnosis of APAs on-site is crucial. To date, fault diagnosis has relied on measurement of frequency domain radiation patterns using costly equipment and multiple strictly controlled measurement probes, which are time-consuming, complex, and therefore infeasible for on-site deployment. This paper proposes a novel method exploiting a Deep Neural Network (DNN) tailored to extract the features hidden in the baseband in-phase and quadrature signals for classifying the different faults. It requires only a single probe in one measurement point for fast and accurate diagnosis of the faulty elements and components in APAs.  Validation of the proposed method is done using a commercial 28 GHz APA. Accuracies of 99% and 80% have been demonstrated for single- and multi-element failure detection, respectively. Three different test scenarios are investigated: on
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#23545;&#35805;&#21709;&#24212;&#36873;&#25321;&#30340;&#21518;&#35757;&#32451;&#25216;&#26415;Dial-MAE&#65292;&#21033;&#29992;&#29983;&#25104;&#26041;&#27861;&#26356;&#22909;&#22320;&#21387;&#32553;&#23545;&#35805;&#35821;&#20041;&#33267;&#23494;&#38598;&#21521;&#37327;&#65292;&#24182;&#25552;&#39640;&#23545;&#35805;&#21709;&#24212;&#36873;&#25321;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.04357</link><description>&lt;p&gt;
&#29992;&#20110;&#22522;&#20110;&#26816;&#32034;&#30340;&#23545;&#35805;&#31995;&#32479;&#30340;&#19978;&#19979;&#25991;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
ConTextual Masked Auto-Encoder for Retrieval-based Dialogue Systems. (arXiv:2306.04357v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04357
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#23545;&#35805;&#21709;&#24212;&#36873;&#25321;&#30340;&#21518;&#35757;&#32451;&#25216;&#26415;Dial-MAE&#65292;&#21033;&#29992;&#29983;&#25104;&#26041;&#27861;&#26356;&#22909;&#22320;&#21387;&#32553;&#23545;&#35805;&#35821;&#20041;&#33267;&#23494;&#38598;&#21521;&#37327;&#65292;&#24182;&#25552;&#39640;&#23545;&#35805;&#21709;&#24212;&#36873;&#25321;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#21709;&#24212;&#36873;&#25321;&#26088;&#22312;&#26681;&#25454;&#32473;&#23450;&#30340;&#29992;&#25143;&#21644;&#31995;&#32479;&#35805;&#35821;&#21382;&#21490;&#35760;&#24405;&#20174;&#20960;&#20010;&#20505;&#36873;&#21709;&#24212;&#20013;&#36873;&#25321;&#36866;&#24403;&#30340;&#21709;&#24212;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#36890;&#36807;&#21518;&#35757;&#32451;&#22823;&#22810;&#20381;&#36182;&#20110;&#21333;&#32431;&#30340;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#26041;&#27861;&#26469;&#25552;&#39640;&#23545;&#35805;&#21709;&#24212;&#36873;&#25321;&#30340;&#20934;&#30830;&#24615;&#12290;&#20294;&#26159;&#65292;&#26368;&#36817;&#24320;&#21457;&#30340;&#29983;&#25104;&#26041;&#27861;&#22312;IR&#31038;&#21306;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#25991;&#26412;&#34920;&#31034;&#33021;&#21147;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#26356;&#22909;&#30340;&#23545;&#35805;&#35821;&#20041;&#24314;&#27169;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986; Dial-MAE&#65288;&#23545;&#35805;&#19978;&#19979;&#25991;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#38024;&#23545;&#23545;&#35805;&#21709;&#24212;&#36873;&#25321;&#30340;&#21518;&#35757;&#32451;&#25216;&#26415;&#12290; Dial-MAE&#20351;&#29992;&#19968;&#20010;&#19981;&#23545;&#31216;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#23398;&#20064;&#23558;&#23545;&#35805;&#30340;&#35821;&#20041;&#26356;&#22909;&#22320;&#21387;&#32553;&#21040;&#23494;&#38598;&#21521;&#37327;&#20013;&#12290; Dial-MAE&#30340;&#36807;&#31243;&#21253;&#25324;&#30001;&#28145;&#24230;&#32534;&#30721;&#22120;&#21019;&#24314;&#24102;&#26377;&#25513;&#30721;&#23545;&#35805;&#19978;&#19979;&#25991;&#30340;&#23545;&#35805;&#23884;&#20837;&#65292;&#28982;&#21518;&#26159;&#27973;&#35299;&#30721;&#22120;&#65292;&#35813;&#35299;&#30721;&#22120;&#20351;&#29992;&#27492;&#23884;&#20837;&#20197;&#21450;&#19978;&#19979;&#25991;&#21521;&#37327;&#26469;&#29983;&#25104;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dialogue response selection aims to select an appropriate response from several candidates based on a given user and system utterance history. Recent studies have been improving the accuracy of dialogue response selection through post-training, mostly relying on naive masked language modeling methods. However, the recently developed generative methods have shown promising text representation capabilities in IR community, which could potentially lead to better dialogue semantics modeling. Thus, in this paper, we propose Dial-MAE (Dialogue Contextual Masking Auto-encoder), a straightforward yet effective post-training technique tailored for dialogue response selection. Dial-MAE uses an asymmetric encoder-decoder architecture that learns to better compress the semantics of the dialogue into dialogue-dense vectors. The process of Dial-MAE involves a deep encoder creating a dialogue embedding with the masked dialogue context, followed by a shallow decoder that uses this embedding along with
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38754;&#21521;&#24773;&#24863;-&#21407;&#22240;&#23545;&#25552;&#21462;&#30340;&#21327;&#21516;&#28436;&#21270;&#22270;&#25512;&#29702;&#32593;&#32476;&#65292;&#36890;&#36807;&#24314;&#31435;&#24322;&#26500;&#22270;&#25429;&#25417;&#22240;&#26524;&#20851;&#31995;&#21644;&#26174;&#24335;&#20381;&#36182;&#20851;&#31995;&#65292;&#23454;&#29616;&#20102;&#24773;&#24863;-&#21407;&#22240;&#23545;&#25552;&#21462;&#30340;&#26032;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.04340</link><description>&lt;p&gt;
&#38754;&#21521;&#24773;&#24863;-&#21407;&#22240;&#23545;&#25552;&#21462;&#30340;&#21327;&#21516;&#28436;&#21270;&#22270;&#25512;&#29702;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Co-evolving Graph Reasoning Network for Emotion-Cause Pair Extraction. (arXiv:2306.04340v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04340
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38754;&#21521;&#24773;&#24863;-&#21407;&#22240;&#23545;&#25552;&#21462;&#30340;&#21327;&#21516;&#28436;&#21270;&#22270;&#25512;&#29702;&#32593;&#32476;&#65292;&#36890;&#36807;&#24314;&#31435;&#24322;&#26500;&#22270;&#25429;&#25417;&#22240;&#26524;&#20851;&#31995;&#21644;&#26174;&#24335;&#20381;&#36182;&#20851;&#31995;&#65292;&#23454;&#29616;&#20102;&#24773;&#24863;-&#21407;&#22240;&#23545;&#25552;&#21462;&#30340;&#26032;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;-&#21407;&#22240;&#23545;&#25552;&#21462;&#65288;ECPE&#65289;&#26088;&#22312;&#20174;&#25991;&#26723;&#20013;&#25552;&#21462;&#25152;&#26377;&#24773;&#24863;&#23376;&#21477;&#21450;&#20854;&#30456;&#24212;&#30340;&#21407;&#22240;&#23376;&#21477;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#19968;&#20219;&#21153;&#65292;&#22312;&#35813;&#26694;&#26550;&#20013;&#65292;&#36825;&#20004;&#20010;&#23376;&#20219;&#21153;&#20026;ECPE&#25552;&#20379;&#20102;&#25351;&#31034;&#24615;&#30340;&#32447;&#32034;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#30340;MTL&#26694;&#26550;&#20165;&#32771;&#34385;&#20102;&#19968;&#36718;&#22810;&#20219;&#21153;&#25512;&#29702;&#65292;&#24573;&#30053;&#20102;&#20174;ECPE&#21040;&#23376;&#20219;&#21153;&#30340;&#21453;&#39304;&#12290;&#27492;&#22806;&#65292;&#20854;&#22810;&#20219;&#21153;&#25512;&#29702;&#20165;&#20381;&#36182;&#20110;&#35821;&#20041;&#27700;&#24179;&#30340;&#20132;&#20114;&#65292;&#26080;&#27861;&#25429;&#25417;&#26174;&#24335;&#20381;&#36182;&#20851;&#31995;&#65292;&#32780;&#32534;&#30721;&#22120;&#20849;&#20139;&#21644;&#22810;&#20219;&#21153;&#38544;&#34255;&#29366;&#24577;&#32423;&#32852;&#20063;&#38590;&#20197;&#25429;&#25417;&#22240;&#26524;&#20851;&#31995;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21327;&#21516;&#28436;&#21270;&#25512;&#29702;&#30340;&#26032;MTL&#26694;&#26550;&#12290;&#23427;&#65288;1&#65289;&#27169;&#25311;&#20102;ECPE&#19982;&#20854;&#23376;&#20219;&#21153;&#20043;&#38388;&#30340;&#21452;&#21521;&#21453;&#39304;&#65307;&#65288;2&#65289;&#20801;&#35768;&#19977;&#20010;&#20219;&#21153;&#19968;&#36215;&#28436;&#21270;&#65292;&#24182;&#30456;&#20114;&#24341;&#23548;&#65307;&#65288;3&#65289;&#38598;&#25104;&#39044;&#27979;&#32423;&#21035;&#30340;&#20132;&#20114;&#26469;&#25429;&#25417;&#26174;&#24335;&#20381;&#36182;&#20851;&#31995;&#12290;&#28982;&#21518;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;Co-evolving Graph Reasoning Network&#65288;CGRN&#65289;&#65292;&#26469;&#23454;&#29616;Co-evolving Reasoning MTL&#26694;&#26550;&#12290;CGRN&#22312;&#19977;&#20010;&#20219;&#21153;&#21450;&#20854;&#36755;&#20837;&#20043;&#38388;&#26500;&#24314;&#20102;&#19968;&#20010;&#24322;&#26500;&#22270;&#65292;&#36890;&#36807;&#22270;&#25512;&#29702;&#36807;&#31243;&#33258;&#28982;&#22320;&#25429;&#25417;&#20102;&#22240;&#26524;&#20851;&#31995;&#21644;&#26174;&#24335;&#20381;&#36182;&#20851;&#31995;&#12290;&#22312;&#20004;&#20010;ECPE&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;CGRN&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emotion-Cause Pair Extraction (ECPE) aims to extract all emotion clauses and their corresponding cause clauses from a document. Existing approaches tackle this task through multi-task learning (MTL) framework in which the two subtasks provide indicative clues for ECPE. However, the previous MTL framework considers only one round of multi-task reasoning and ignores the reverse feedbacks from ECPE to the subtasks. Besides, its multi-task reasoning only relies on semantics-level interactions, which cannot capture the explicit dependencies, and both the encoder sharing and multi-task hidden states concatenations can hardly capture the causalities. To solve these issues, we first put forward a new MTL framework based on Co-evolving Reasoning. It (1) models the bidirectional feedbacks between ECPE and its subtasks; (2) allows the three tasks to evolve together and prompt each other recurrently; (3) integrates prediction-level interactions to capture explicit dependencies. Then we propose a n
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#37197;&#23545;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#29289;&#29702;&#39537;&#21160;&#30340;CycleGAN&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#37197;&#23545;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20934;&#30830;&#22320;&#20272;&#35745;&#21160;&#24577;&#22686;&#24378;&#30913;&#20849;&#25391;&#25104;&#20687;&#33647;&#20195;&#21160;&#21147;&#23398;&#21442;&#25968;&#21644;&#21160;&#33033;&#36755;&#20837;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2306.04339</link><description>&lt;p&gt;
&#22522;&#20110;&#38750;&#37197;&#23545;&#28145;&#24230;&#23398;&#20064;&#30340;&#21160;&#24577;&#22686;&#24378;&#30913;&#20849;&#25391;&#25104;&#20687;&#33647;&#20195;&#21160;&#21147;&#23398;&#21442;&#25968;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Unpaired Deep Learning for Pharmacokinetic Parameter Estimation from Dynamic Contrast-Enhanced MRI. (arXiv:2306.04339v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04339
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#37197;&#23545;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#29289;&#29702;&#39537;&#21160;&#30340;CycleGAN&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#37197;&#23545;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20934;&#30830;&#22320;&#20272;&#35745;&#21160;&#24577;&#22686;&#24378;&#30913;&#20849;&#25391;&#25104;&#20687;&#33647;&#20195;&#21160;&#21147;&#23398;&#21442;&#25968;&#21644;&#21160;&#33033;&#36755;&#20837;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#22686;&#24378;&#30913;&#20849;&#25391;&#25104;&#20687; (DCE-MRI) &#21487;&#20197;&#25552;&#20379;&#20851;&#20110;&#34880;&#31649;&#36890;&#36879;&#24615;&#21644;&#32452;&#32455;&#28748;&#27880;&#30340;&#33647;&#20195;&#21160;&#21147;&#23398;&#21442;&#25968;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#33647;&#20195;&#21160;&#21147;&#23398;&#21442;&#25968;&#20272;&#35745;&#26041;&#27861;&#28041;&#21450;&#25311;&#21512;&#31034;&#36394;&#21058;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#30001;&#20110;&#22122;&#22768;&#21160;&#33033;&#36755;&#20837;&#20989;&#25968; (AIF) &#30340;&#27979;&#37327;&#24120;&#24120;&#23548;&#33268;&#35745;&#31639;&#22797;&#26434;&#24615;&#21644;&#20302;&#20934;&#30830;&#24615;&#12290;&#34429;&#28982;&#19968;&#20123;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#24050;&#34987;&#25552;&#20986;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#20294;&#22823;&#37096;&#20998;&#29616;&#26377;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#26377;&#26631;&#31614;&#30340; DCE-MRI &#21644;&#24050;&#26631;&#27880;&#30340;&#33647;&#20195;&#21160;&#21147;&#23398;&#21442;&#25968;&#22270;&#12290;&#36825;&#20381;&#36182;&#20110;&#26631;&#31614;&#25968;&#25454;&#20250;&#23548;&#33268;&#26174;&#33879;&#30340;&#26102;&#38388;&#21644;&#36164;&#28304;&#38480;&#21046;&#65292;&#21487;&#33021;&#20250;&#24341;&#20837;&#26631;&#31614;&#22122;&#22768;&#65292;&#20351;&#24471;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#32463;&#24120;&#19981;&#23454;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#22312;&#36825;&#37324;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38750;&#37197;&#23545;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#29289;&#29702;&#39537;&#21160;&#30340; CycleGAN &#26041;&#27861;&#20272;&#35745;&#33647;&#20195;&#21160;&#21147;&#23398;&#21442;&#25968;&#21644; AIF&#12290;&#25105;&#20204;&#25552;&#20986;&#30340; CycleGAN &#26694;&#26550;&#22522;&#20110;&#33258;&#23545;&#25239;&#23398;&#20064;&#65292;&#36890;&#36807;&#23398;&#20064;&#20004;&#20010;&#19981;&#21516;&#20998;&#24067;&#20043;&#38388;&#30340;&#26144;&#23556;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#37197;&#23545;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23545;&#33647;&#20195;&#21160;&#21147;&#23398;&#21442;&#25968;&#36827;&#34892;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
DCE-MRI provides information about vascular permeability and tissue perfusion through the acquisition of pharmacokinetic parameters. However, traditional methods for estimating these pharmacokinetic parameters involve fitting tracer kinetic models, which often suffer from computational complexity and low accuracy due to noisy arterial input function (AIF) measurements. Although some deep learning approaches have been proposed to tackle these challenges, most existing methods rely on supervised learning that requires paired input DCE-MRI and labeled pharmacokinetic parameter maps. This dependency on labeled data introduces significant time and resource constraints, as well as potential noise in the labels, making supervised learning methods often impractical. To address these limitations, here we present a novel unpaired deep learning method for estimating both pharmacokinetic parameters and the AIF using a physics-driven CycleGAN approach. Our proposed CycleGAN framework is designed ba
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;&#20256;&#24863;&#22120;&#20010;&#20154;&#20581;&#24247;&#30417;&#27979;&#31995;&#32479;&#20013;&#35821;&#20041;&#25216;&#26415;&#30340;&#24212;&#29992;&#29616;&#29366;&#65292;&#21457;&#29616;&#27492;&#31867;&#31995;&#32479;&#24517;&#39035;&#20811;&#26381;&#30340;&#20851;&#38190;&#25361;&#25112;&#20026;&#20114;&#25805;&#20316;&#24615;&#12289;&#19978;&#19979;&#25991;&#24863;&#30693;&#12289;&#24773;&#22659;&#26816;&#27979;&#12289;&#24773;&#22659;&#39044;&#27979;&#12289;&#20915;&#31574;&#25903;&#25345;&#21644;&#30693;&#35782;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2306.04335</link><description>&lt;p&gt;
&#20256;&#24863;&#22120;&#20010;&#20154;&#20581;&#24247;&#30417;&#27979;&#31995;&#32479;&#20013;&#30340;&#35821;&#20041;&#25216;&#26415;&#65306;&#19968;&#39033;&#31995;&#32479;&#24615;&#26144;&#23556;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Semantic Technologies in Sensor-Based Personal Health Monitoring Systems: A Systematic Mapping Study. (arXiv:2306.04335v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04335
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;&#20256;&#24863;&#22120;&#20010;&#20154;&#20581;&#24247;&#30417;&#27979;&#31995;&#32479;&#20013;&#35821;&#20041;&#25216;&#26415;&#30340;&#24212;&#29992;&#29616;&#29366;&#65292;&#21457;&#29616;&#27492;&#31867;&#31995;&#32479;&#24517;&#39035;&#20811;&#26381;&#30340;&#20851;&#38190;&#25361;&#25112;&#20026;&#20114;&#25805;&#20316;&#24615;&#12289;&#19978;&#19979;&#25991;&#24863;&#30693;&#12289;&#24773;&#22659;&#26816;&#27979;&#12289;&#24773;&#22659;&#39044;&#27979;&#12289;&#20915;&#31574;&#25903;&#25345;&#21644;&#30693;&#35782;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20154;&#20204;&#23545;&#20110;&#30142;&#30149;&#30340;&#26089;&#26399;&#26816;&#27979;&#12289;&#39044;&#38450;&#21644;&#39044;&#27979;&#36234;&#26469;&#36234;&#37325;&#35270;&#12290;&#27492;&#22806;&#65292;&#20256;&#24863;&#22120;&#25216;&#26415;&#21644;&#29289;&#32852;&#32593;&#25216;&#26415;&#30340;&#19981;&#26029;&#36827;&#27493;&#20063;&#25512;&#21160;&#20102;&#20010;&#20154;&#20581;&#24247;&#30417;&#27979;&#31995;&#32479;&#30340;&#21457;&#23637;&#12290;&#35821;&#20041;&#25216;&#26415;&#20316;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#19981;&#20165;&#21487;&#20197;&#22788;&#29702;&#24322;&#26500;&#20581;&#24247;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#20114;&#25805;&#20316;&#24615;&#38382;&#39064;&#65292;&#36824;&#21487;&#20197;&#34920;&#31034;&#19987;&#23478;&#20581;&#24247;&#30693;&#35782;&#20197;&#25903;&#25345;&#20915;&#31574;&#25152;&#38656;&#30340;&#22797;&#26434;&#25512;&#29702;&#12290;&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#20256;&#24863;&#22120;&#20010;&#20154;&#20581;&#24247;&#30417;&#27979;&#31995;&#32479;&#20013;&#35821;&#20041;&#25216;&#26415;&#30340;&#24212;&#29992;&#29616;&#29366;&#12290;&#20351;&#29992;&#31995;&#32479;&#26041;&#27861;&#23545;40&#20010;&#20195;&#34920;&#35813;&#39046;&#22495;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#30340;&#31995;&#32479;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#36890;&#36807;&#36825;&#39033;&#20998;&#26512;&#65292;&#30830;&#23450;&#20102;&#27492;&#31867;&#31995;&#32479;&#24517;&#39035;&#20811;&#26381;&#30340;&#20845;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;&#20114;&#25805;&#20316;&#24615;&#12289;&#19978;&#19979;&#25991;&#24863;&#30693;&#12289;&#24773;&#22659;&#26816;&#27979;&#12289;&#24773;&#22659;&#39044;&#27979;&#12289;&#20915;&#31574;&#25903;&#25345;&#21644;&#30693;&#35782;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, there has been an increased focus on early detection, prevention, and prediction of diseases. This, together with advances in sensor technology and the Internet of Things, has led to accelerated efforts in the development of personal health monitoring systems. Semantic technologies have emerged as an effective way to not only deal with the issue of interoperability associated with heterogeneous health sensor data, but also to represent expert health knowledge to support complex reasoning required for decision-making. This study evaluates the state of the art in the use of semantic technologies in sensor-based personal health monitoring systems. Using a systematic approach, a total of 40 systems representing the state of the art in the field are analysed. Through this analysis, six key challenges that such systems must overcome for optimal and effective health monitoring are identified: interoperability, context awareness, situation detection, situation prediction, deci
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#25442;&#22120;&#21644;&#22270;&#21367;&#31215;&#30340;&#26053;&#34892;&#26102;&#38388;&#20272;&#35745;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#19981;&#21516;&#25968;&#25454;&#27169;&#24577;&#25552;&#21462;&#36755;&#20837;&#36335;&#24452;&#30340;&#19981;&#21516;&#23646;&#24615;&#26469;&#25552;&#39640;&#20272;&#35745;&#31934;&#24230;&#65292;&#24182;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#36229;&#36807;&#20102;&#29616;&#26377;&#25216;&#26415;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.04324</link><description>&lt;p&gt;
GCT-TTE: &#22522;&#20110;&#22270;&#21367;&#31215;&#21464;&#25442;&#22120;&#30340;&#26053;&#34892;&#26102;&#38388;&#20272;&#35745;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
GCT-TTE: Graph Convolutional Transformer for Travel Time Estimation. (arXiv:2306.04324v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04324
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#25442;&#22120;&#21644;&#22270;&#21367;&#31215;&#30340;&#26053;&#34892;&#26102;&#38388;&#20272;&#35745;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#19981;&#21516;&#25968;&#25454;&#27169;&#24577;&#25552;&#21462;&#36755;&#20837;&#36335;&#24452;&#30340;&#19981;&#21516;&#23646;&#24615;&#26469;&#25552;&#39640;&#20272;&#35745;&#31934;&#24230;&#65292;&#24182;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#36229;&#36807;&#20102;&#29616;&#26377;&#25216;&#26415;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#21464;&#25442;&#22120;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#26053;&#34892;&#26102;&#38388;&#20272;&#35745;&#38382;&#39064;&#12290;&#25152;&#25552;&#20986;&#30340;GCT-TTE&#26550;&#26500;&#30340;&#20851;&#38190;&#29305;&#24449;&#26159;&#21033;&#29992;&#25429;&#25417;&#36755;&#20837;&#36335;&#24452;&#19981;&#21516;&#23646;&#24615;&#30340;&#19981;&#21516;&#25968;&#25454;&#27169;&#24577;&#12290;&#38500;&#20102;&#28041;&#21450;&#27169;&#22411;&#37197;&#32622;&#30340;&#24191;&#27867;&#30740;&#31350;&#22806;&#65292;&#25105;&#20204;&#36824;&#23454;&#29616;&#24182;&#35780;&#20272;&#20102;&#36275;&#22815;&#25968;&#37327;&#30340;&#23454;&#38469;&#22522;&#32447;&#65292;&#29992;&#20110;&#36335;&#24452;&#24863;&#30693;&#21644;&#36335;&#24452;&#30450;&#35774;&#32622;&#12290;&#25152;&#36827;&#34892;&#30340;&#35745;&#31639;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#27969;&#31243;&#30340;&#21487;&#34892;&#24615;&#65292;&#35813;&#27969;&#31243;&#22312;&#20004;&#20010;&#32771;&#34385;&#30340;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;GCT-TTE&#24050;&#37096;&#32626;&#20026;Web&#26381;&#21153;&#65292;&#21487;&#29992;&#20110;&#36827;&#19968;&#27493;&#23454;&#39564;&#19982;&#29992;&#25143;&#23450;&#20041;&#36335;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a new transformer-based model for the problem of travel time estimation. The key feature of the proposed GCT-TTE architecture is the utilization of different data modalities capturing different properties of an input path. Along with the extensive study regarding the model configuration, we implemented and evaluated a sufficient number of actual baselines for path-aware and path-blind settings. The conducted computational experiments have confirmed the viability of our pipeline, which outperformed state-of-the-art models on both considered datasets. Additionally, GCT-TTE was deployed as a web service accessible for further experiments with user-defined routes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#29983;&#25104;&#25193;&#25955;&#24341;&#23548;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#22312;&#21512;&#25104;&#22810;&#23186;&#20307;&#20869;&#23481;&#21516;&#26102;&#20445;&#30041;&#35821;&#20041;&#29305;&#24449;&#26041;&#38754;&#30340;&#20248;&#21183;&#65292;&#20197;&#21457;&#36865;&#39640;&#24230;&#21387;&#32553;&#30340;&#35821;&#20041;&#20449;&#24687;&#26469;&#38477;&#20302;&#24102;&#23485;&#20351;&#29992;&#65292;&#20174;&#32780;&#36229;&#36234;&#29616;&#26377;&#26041;&#27861;&#22312;&#37325;&#24314;&#36136;&#37327;&#21644;&#35821;&#20041;&#20445;&#30041;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2306.04321</link><description>&lt;p&gt;
&#29983;&#25104;&#35821;&#20041;&#36890;&#35759;&#65306;&#36229;&#36234;&#27604;&#29305;&#24674;&#22797;&#30340;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Generative Semantic Communication: Diffusion Models Beyond Bit Recovery. (arXiv:2306.04321v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04321
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#29983;&#25104;&#25193;&#25955;&#24341;&#23548;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#22312;&#21512;&#25104;&#22810;&#23186;&#20307;&#20869;&#23481;&#21516;&#26102;&#20445;&#30041;&#35821;&#20041;&#29305;&#24449;&#26041;&#38754;&#30340;&#20248;&#21183;&#65292;&#20197;&#21457;&#36865;&#39640;&#24230;&#21387;&#32553;&#30340;&#35821;&#20041;&#20449;&#24687;&#26469;&#38477;&#20302;&#24102;&#23485;&#20351;&#29992;&#65292;&#20174;&#32780;&#36229;&#36234;&#29616;&#26377;&#26041;&#27861;&#22312;&#37325;&#24314;&#36136;&#37327;&#21644;&#35821;&#20041;&#20445;&#30041;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#36890;&#35759;&#34987;&#35748;&#20026;&#26159;&#19979;&#19968;&#20195;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#36890;&#35759;&#20013;&#30340;&#26680;&#24515;&#20043;&#19968;&#12290;&#35821;&#20041;&#36890;&#35759;&#30340;&#19968;&#20010;&#21487;&#33021;&#24615;&#26159;&#65292;&#22312;&#19981;&#24517;&#24674;&#22797;&#20256;&#36755;&#27604;&#29305;&#24207;&#21015;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#30446;&#26631;&#31471;&#37325;&#24314;&#19982;&#20256;&#36755;&#30340;&#22270;&#20687;&#25110;&#35270;&#39057;&#35821;&#20041;&#31561;&#25928;&#30340;&#21103;&#26412;&#12290;&#24403;&#21069;&#30340;&#35299;&#20915;&#26041;&#26696;&#20173;&#28982;&#32570;&#20047;&#20174;&#25509;&#25910;&#21040;&#30340;&#37096;&#20998;&#20449;&#24687;&#20013;&#26500;&#24314;&#22797;&#26434;&#22330;&#26223;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#25193;&#25955;&#24341;&#23548;&#26694;&#26550;&#26469;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#21512;&#25104;&#22810;&#23186;&#20307;&#20869;&#23481;&#21516;&#26102;&#20445;&#30041;&#35821;&#20041;&#29305;&#24449;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#36890;&#36807;&#20165;&#21457;&#36865;&#39640;&#24230;&#21387;&#32553;&#30340;&#35821;&#20041;&#20449;&#24687;&#26469;&#38477;&#20302;&#24102;&#23485;&#20351;&#29992;&#12290;&#28982;&#21518;&#25509;&#25910;&#31471;&#30340;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22810;&#23186;&#20307;&#20869;&#23481;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#37325;&#24314;&#36136;&#37327;&#21644;&#35821;&#20041;&#20445;&#30041;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic communication is expected to be one of the cores of next-generation AI-based communications. One of the possibilities offered by semantic communication is the capability to regenerate, at the destination side, images or videos semantically equivalent to the transmitted ones, without necessarily recovering the transmitted sequence of bits. The current solutions still lack the ability to build complex scenes from the received partial information. Clearly, there is an unmet need to balance the effectiveness of generation methods and the complexity of the transmitted information, possibly taking into account the goal of communication. In this paper, we aim to bridge this gap by proposing a novel generative diffusion-guided framework for semantic communication that leverages the strong abilities of diffusion models in synthesizing multimedia content while preserving semantic features. We reduce bandwidth usage by sending highly-compressed semantic information only. Then, the diffus
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;GPT-3 Davinci-003&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#20154;&#26684;&#29305;&#36136;&#65292;&#21457;&#29616;&#20854;&#20855;&#26377;&#33391;&#22909;&#30340;&#31038;&#20132;&#28212;&#26395;&#21644;&#20146;&#31038;&#20250;&#29305;&#36136;&#65292;&#20294;&#22312;&#19981;&#21516;&#26102;&#38388;&#30340;&#19968;&#33268;&#24615;&#23384;&#22312;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2306.04308</link><description>&lt;p&gt;
GPT-3&#30340;&#20154;&#26684;&#27979;&#35797;&#65306;&#26102;&#38388;&#21487;&#38752;&#24615;&#26377;&#38480;&#65292;&#20294;&#20984;&#26174;&#20102;&#31038;&#20132;&#28212;&#26395;&#30340;&#20154;&#26684;&#24037;&#20855;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personality testing of GPT-3: Limited temporal reliability, but highlighted social desirability of GPT-3's personality instruments results. (arXiv:2306.04308v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04308
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;GPT-3 Davinci-003&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#20154;&#26684;&#29305;&#36136;&#65292;&#21457;&#29616;&#20854;&#20855;&#26377;&#33391;&#22909;&#30340;&#31038;&#20132;&#28212;&#26395;&#21644;&#20146;&#31038;&#20250;&#29305;&#36136;&#65292;&#20294;&#22312;&#19981;&#21516;&#26102;&#38388;&#30340;&#19968;&#33268;&#24615;&#23384;&#22312;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35780;&#20272;&#32842;&#22825;&#26426;&#22120;&#20154;GPT-3 Davinci-003&#30340;&#28508;&#22312;&#24212;&#29992;&#21644;&#38480;&#21046;&#65292;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#24212;&#29992;&#20110;&#32842;&#22825;&#26426;&#22120;&#20154;&#21450;&#20854;&#20010;&#24615;&#21270;&#36164;&#26009;&#30340;&#20154;&#26684;&#38382;&#21367;&#30340;&#26102;&#38388;&#21487;&#38752;&#24615;&#12290;&#22312;&#20004;&#20010;&#19981;&#21516;&#30340;&#22330;&#21512;&#65292;&#24515;&#29702;&#38382;&#21367;&#34987;&#24212;&#29992;&#20110;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#28982;&#21518;&#23558;&#22238;&#31572;&#19982;&#20154;&#31867;&#22522;&#20934;&#25968;&#25454;&#36827;&#34892;&#27604;&#36739;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#22238;&#31572;&#26377;&#19981;&#21516;&#31243;&#24230;&#30340;&#19968;&#33268;&#24615;&#65292;&#26377;&#20123;&#37327;&#34920;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#19968;&#33268;&#24615;&#65292;&#32780;&#26377;&#20123;&#21017;&#34920;&#29616;&#20986;&#36739;&#24046;&#30340;&#19968;&#33268;&#24615;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;Davinci-003&#26174;&#31034;&#20986;&#19968;&#20010;&#31038;&#20132;&#28212;&#26395;&#21644;&#20146;&#31038;&#20250;&#30340;&#20154;&#26684;&#29305;&#36136;&#65292;&#23588;&#20854;&#26159;&#22312;&#20146;&#21644;&#21147;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#32842;&#22825;&#26426;&#22120;&#20154;&#22238;&#31572;&#30340;&#22522;&#30784;&#65292;&#26080;&#35770;&#26159;&#30001;&#20027;&#35266;&#33258;&#25105;&#21453;&#24605;&#36824;&#26159;&#39044;&#23450;&#31639;&#27861;&#39537;&#21160;&#65292;&#23578;&#19981;&#30830;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
To assess the potential applications and limitations of chatbot GPT-3 Davinci-003, this study explored the temporal reliability of personality questionnaires applied to the chatbot and its personality profile. Psychological questionnaires were administered to the chatbot on two separate occasions, followed by a comparison of the responses to human normative data. The findings revealed varying levels of agreement in the chatbot's responses over time, with some scales displaying excellent while others demonstrated poor agreement. Overall, Davinci-003 displayed a socially desirable and pro-social personality profile, particularly in the domain of communion. However, the underlying basis of the chatbot's responses, whether driven by conscious self-reflection or predetermined algorithms, remains uncertain.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#23454;&#39564;&#65292;&#27604;&#36739;&#20102;&#22240;&#26524;&#25512;&#26029;&#21644;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#23450;&#26102;&#36807;&#31243;&#24178;&#39044;&#26041;&#38754;&#30340;&#20248;&#21155;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#24378;&#21270;&#23398;&#20064;&#22312;&#25928;&#26524;&#21644;&#31283;&#23450;&#24615;&#26041;&#38754;&#22343;&#20248;&#20110;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.04299</link><description>&lt;p&gt;
&#29992;&#22240;&#26524;&#25512;&#26029;&#21644;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#26102;&#38388;&#36807;&#31243;&#24178;&#39044;&#12290;
&lt;/p&gt;
&lt;p&gt;
Timing Process Interventions with Causal Inference and Reinforcement Learning. (arXiv:2306.04299v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04299
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#23454;&#39564;&#65292;&#27604;&#36739;&#20102;&#22240;&#26524;&#25512;&#26029;&#21644;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#23450;&#26102;&#36807;&#31243;&#24178;&#39044;&#26041;&#38754;&#30340;&#20248;&#21155;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#24378;&#21270;&#23398;&#20064;&#22312;&#25928;&#26524;&#21644;&#31283;&#23450;&#24615;&#26041;&#38754;&#22343;&#20248;&#20110;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#29702;&#35299;&#21644;&#39044;&#27979;&#36807;&#31243;&#21040;&#20854;&#20248;&#21270;&#65292;&#36825;&#22330;&#36716;&#21464;&#20026;&#20225;&#19994;&#21644;&#20854;&#20182;&#32452;&#32455;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#21033;&#30410;&#12290;&#31934;&#30830;&#35745;&#26102;&#30340;&#36807;&#31243;&#24178;&#39044;&#26159;&#26377;&#25928;&#20248;&#21270;&#30340;&#22522;&#30707;&#12290;&#22788;&#26041;&#24335;&#36807;&#31243;&#30417;&#25511;(PresPM)&#26159;&#36807;&#31243;&#25366;&#25496;&#30340;&#23376;&#39046;&#22495;&#65292;&#19987;&#27880;&#20110;&#36807;&#31243;&#20248;&#21270;&#12290;&#26032;&#20852;&#30340;PresPM&#25991;&#29486;&#30830;&#23450;&#20102;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#22240;&#26524;&#25512;&#26029;(CI)&#21644;&#24378;&#21270;&#23398;&#20064;(RL)&#65292;&#20294;&#27809;&#26377;&#36827;&#34892;&#23450;&#37327;&#27604;&#36739;&#12290;&#22823;&#22810;&#25968;&#23454;&#39564;&#26159;&#20351;&#29992;&#21382;&#21490;&#25968;&#25454;&#36827;&#34892;&#30340;&#65292;&#23548;&#33268;&#35780;&#20272;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#38382;&#39064;&#65292;&#24182;&#38459;&#27490;&#20102;&#22312;&#32447;RL&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26159;&#36890;&#36807;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#23545;&#23450;&#26102;&#36807;&#31243;&#24178;&#39044;&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#65292;&#20351;&#24471;&#30495;&#27491;&#30340;&#22312;&#32447;RL&#21644;&#19982;CI&#30340;&#27604;&#36739;&#25104;&#20026;&#21487;&#33021;&#65292;&#24182;&#20801;&#35768;&#23545;&#32467;&#26524;&#36827;&#34892;&#20934;&#30830;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;RL&#30340;&#31574;&#30053;&#32988;&#36807;CI&#30340;&#31574;&#30053;&#65292;&#24182;&#19988;&#21516;&#26102;&#26356;&#21152;&#31283;&#20581;&#12290;&#20107;&#23454;&#19978;&#65292;RL&#31574;&#30053;&#30340;&#32467;&#26524;&#27604;CI&#26356;&#21487;&#38752;&#12290;
&lt;/p&gt;
&lt;p&gt;
The shift from the understanding and prediction of processes to their optimization offers great benefits to businesses and other organizations. Precisely timed process interventions are the cornerstones of effective optimization. Prescriptive process monitoring (PresPM) is the sub-field of process mining that concentrates on process optimization. The emerging PresPM literature identifies state-of-the-art methods, causal inference (CI) and reinforcement learning (RL), without presenting a quantitative comparison. Most experiments are carried out using historical data, causing problems with the accuracy of the methods' evaluations and preempting online RL. Our contribution consists of experiments on timed process interventions with synthetic data that renders genuine online RL and the comparison to CI possible, and allows for an accurate evaluation of the results. Our experiments reveal that RL's policies outperform those from CI and are more robust at the same time. Indeed, the RL polic
&lt;/p&gt;</description></item><item><title>&#24403;&#21069;XAI&#30740;&#31350;&#20013;&#23384;&#22312;&#22522;&#26412;&#35823;&#35299;&#65292;&#20363;&#22914;&#26410;&#26126;&#30830;&#35299;&#37322;&#25216;&#26415;&#30340;&#30446;&#30340;&#65292;&#20381;&#36182;&#20110;&#20851;&#20110;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#25152;&#23398;&#8220;&#27010;&#24565;&#8221;&#30340;&#24378;&#28872;&#20551;&#35774;&#31561;&#12290;&#25105;&#20204;&#38656;&#35201;&#37319;&#21462;&#25514;&#26045;&#20351;XAI&#25104;&#20026;&#26356;&#23454;&#36136;&#24615;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2306.04292</link><description>&lt;p&gt;
&#20146;&#29233;&#30340;XAI&#31038;&#21306;&#65292;&#25105;&#20204;&#38656;&#35201;&#35848;&#35848;&#65281;&#20851;&#20110;&#24403;&#21069;XAI&#30740;&#31350;&#20013;&#23384;&#22312;&#30340;&#22522;&#26412;&#35823;&#35299;
&lt;/p&gt;
&lt;p&gt;
Dear XAI Community, We Need to Talk! Fundamental Misconceptions in Current XAI Research. (arXiv:2306.04292v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04292
&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;XAI&#30740;&#31350;&#20013;&#23384;&#22312;&#22522;&#26412;&#35823;&#35299;&#65292;&#20363;&#22914;&#26410;&#26126;&#30830;&#35299;&#37322;&#25216;&#26415;&#30340;&#30446;&#30340;&#65292;&#20381;&#36182;&#20110;&#20851;&#20110;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#25152;&#23398;&#8220;&#27010;&#24565;&#8221;&#30340;&#24378;&#28872;&#20551;&#35774;&#31561;&#12290;&#25105;&#20204;&#38656;&#35201;&#37319;&#21462;&#25514;&#26045;&#20351;XAI&#25104;&#20026;&#26356;&#23454;&#36136;&#24615;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#35813;&#39046;&#22495;&#24050;&#32463;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#30446;&#21069;XAI&#30740;&#31350;&#30340;&#37325;&#35201;&#37096;&#20998;&#20173;&#26410;&#24314;&#31435;&#22312;&#22362;&#23454;&#30340;&#27010;&#24565;&#12289;&#20262;&#29702;&#25110;&#26041;&#27861;&#35770;&#22522;&#30784;&#19978;&#12290;&#20196;&#20154;&#36951;&#25022;&#30340;&#26159;&#65292;&#36825;&#20123;&#22522;&#30784;&#34180;&#24369;&#30340;&#37096;&#20998;&#24182;&#27809;&#26377;&#20943;&#23569;&#65292;&#32780;&#26159;&#19981;&#26029;&#22686;&#38271;&#12290;&#35768;&#22810;&#35299;&#37322;&#25216;&#26415;&#20173;&#28982;&#27809;&#26377;&#28548;&#28165;&#20854;&#30446;&#30340;&#65292;&#32780;&#26159;&#29992;&#36234;&#26469;&#36234;&#33457;&#21736;&#30340;&#28909;&#28857;&#22270;&#25110;&#30475;&#20284;&#30456;&#20851;&#30340;&#22522;&#20934;&#26469;&#23459;&#20256;&#12290;&#27492;&#22806;&#65292;&#35299;&#37322;&#25216;&#26415;&#30340;&#21160;&#26426;&#23384;&#22312;&#38382;&#39064;&#65292;&#20363;&#22914;&#24314;&#31435;&#20449;&#20219;&#65292;&#25110;&#20381;&#36182;&#20110;&#20851;&#20110;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#25152;&#23398;&#8220;&#27010;&#24565;&#8221;&#30340;&#24378;&#28872;&#20551;&#35774;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#31361;&#20986;&#24182;&#35752;&#35770;&#20102;&#24403;&#21069;XAI&#30740;&#31350;&#20013;&#30340;&#36825;&#20123;&#21644;&#20854;&#20182;&#35823;&#35299;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#20351;XAI&#25104;&#20026;&#26356;&#23454;&#36136;&#24615;&#30740;&#31350;&#39046;&#22495;&#30340;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite progress in the field, significant parts of current XAI research are still not on solid conceptual, ethical, or methodological grounds. Unfortunately, these unfounded parts are not on the decline but continue to grow. Many explanation techniques are still proposed without clarifying their purpose. Instead, they are advertised with ever more fancy-looking heatmaps or only seemingly relevant benchmarks. Moreover, explanation techniques are motivated with questionable goals, such as building trust, or rely on strong assumptions about the 'concepts' that deep learning algorithms learn. In this paper, we highlight and discuss these and other misconceptions in current XAI research. We also suggest steps to make XAI a more substantive area of research.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22312;&#40657;&#26495;&#26550;&#26500;&#20013;&#22686;&#21152;&#23481;&#22120;&#21644;&#38142;&#25509;&#30340;&#20316;&#29992;&#65292;&#36825;&#20123;&#23545;&#35937;&#21487;&#20197;&#26356;&#22909;&#22320;&#24314;&#27169;&#32452;&#32455;&#12289;&#29289;&#29702;&#12289;&#31354;&#38388;&#31561;&#20851;&#31995;&#65292;&#20351;&#40657;&#26495;&#26550;&#26500;&#33021;&#22815;&#26356;&#22909;&#22320;&#24212;&#29992;&#20110;&#22797;&#26434;&#38382;&#39064;&#20013;&#12290;</title><link>http://arxiv.org/abs/2306.04289</link><description>&lt;p&gt;
&#40657;&#26495;&#26550;&#26500;&#20013;&#28155;&#21152;&#38142;&#25509;&#21644;&#23481;&#22120;&#30340;&#20171;&#32461;&#19982;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Introduction and Assessment of the Addition of Links and Containers to the Blackboard Architecture. (arXiv:2306.04289v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04289
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22312;&#40657;&#26495;&#26550;&#26500;&#20013;&#22686;&#21152;&#23481;&#22120;&#21644;&#38142;&#25509;&#30340;&#20316;&#29992;&#65292;&#36825;&#20123;&#23545;&#35937;&#21487;&#20197;&#26356;&#22909;&#22320;&#24314;&#27169;&#32452;&#32455;&#12289;&#29289;&#29702;&#12289;&#31354;&#38388;&#31561;&#20851;&#31995;&#65292;&#20351;&#40657;&#26495;&#26550;&#26500;&#33021;&#22815;&#26356;&#22909;&#22320;&#24212;&#29992;&#20110;&#22797;&#26434;&#38382;&#39064;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40657;&#26495;&#26550;&#26500;&#25552;&#20379;&#20102;&#19968;&#31181;&#26426;&#21046;&#65292;&#29992;&#20110;&#23384;&#20648;&#25968;&#25454;&#21644;&#36923;&#36753;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#20570;&#20986;&#24433;&#21709;&#40657;&#26495;&#26550;&#26500;&#32593;&#32476;&#24314;&#27169;&#30340;&#24212;&#29992;&#29615;&#22659;&#30340;&#20915;&#31574;&#12290;&#34429;&#28982;&#35268;&#21017;&#20107;&#23454;&#21160;&#20316;&#32593;&#32476;&#21487;&#20197;&#34920;&#31034;&#22810;&#31181;&#31867;&#22411;&#30340;&#25968;&#25454;&#65292;&#20294;&#21487;&#20197;&#36731;&#26494;&#24314;&#27169;&#30340;&#20851;&#31995;&#21463;&#21040;&#35268;&#21017;&#20107;&#23454;&#32593;&#32476;&#32467;&#26500;&#21629;&#39064;&#36923;&#36753;&#24615;&#36136;&#30340;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#22312;&#40657;&#26495;&#26550;&#26500;&#20013;&#22686;&#21152;&#23481;&#22120;&#21644;&#38142;&#25509;&#12290;&#36825;&#20123;&#23545;&#35937;&#26088;&#22312;&#20801;&#35768;&#23427;&#20204;&#24314;&#27169;&#32452;&#32455;&#12289;&#29289;&#29702;&#12289;&#31354;&#38388;&#21644;&#20854;&#20182;&#20851;&#31995;&#65292;&#36825;&#20123;&#20851;&#31995;&#19981;&#33021;&#20316;&#20026;&#24067;&#23572;&#36923;&#36753;&#35268;&#21017;&#36731;&#26494;&#25110;&#26377;&#25928;&#22320;&#23454;&#29616;&#12290;&#23481;&#22120;&#23558;&#30456;&#20851;&#25968;&#25454;&#32452;&#21512;&#22312;&#19968;&#36215;&#65292;&#24182;&#21487;&#23884;&#22871;&#20197;&#23454;&#29616;&#22797;&#26434;&#20851;&#31995;&#12290;&#38142;&#25509;&#30456;&#20114;&#36830;&#25509;&#20855;&#26377;&#30456;&#20851;&#32452;&#32455;&#30446;&#30340;&#30340;&#23481;&#22120;&#12290;&#36825;&#20004;&#31181;&#23545;&#35937;&#20849;&#21516;&#20419;&#36827;&#20102;&#20351;&#29992;&#40657;&#26495;&#26550;&#26500;&#30340;&#26032;&#26041;&#24335;&#65292;&#24182;&#20351;&#20854;&#33021;&#22815;&#31616;&#21270;&#24212;&#29992;&#20110;&#22797;&#26434;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Blackboard Architecture provides a mechanism for storing data and logic and using it to make decisions that impact the application environment that the Blackboard Architecture network models. While rule-fact-action networks can represent numerous types of data, the relationships that can be easily modeled are limited by the propositional logic nature of the rule-fact network structure. This paper proposes and evaluates the inclusion of containers and links in the Blackboard Architecture. These objects are designed to allow them to model organizational, physical, spatial and other relationships that cannot be readily or efficiently implemented as Boolean logic rules. Containers group related facts together and can be nested to implement complex relationships. Links interconnect containers that have a relationship that is relevant to their organizational purpose. Both objects, together, facilitate new ways of using the Blackboard Architecture and enable or simply its use for complex 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25193;&#23637;&#40657;&#26495;&#26550;&#26500;&#30340;&#26041;&#27861;&#65292;&#22686;&#21152;&#20102;&#23545;&#20849;&#21516;&#23646;&#24615;&#21644;&#36890;&#29992;&#35268;&#21017;&#30340;&#25903;&#25345;&#65292;&#20197;&#23454;&#29616;&#23545;&#32452;&#32455;&#12289;&#31354;&#38388;&#21644;&#20854;&#20182;&#20851;&#31995;&#30340;&#24314;&#27169;&#65292;&#24182;&#20419;&#36827;&#23545;&#36923;&#36753;&#32467;&#26500;&#30340;&#37325;&#29992;&#12290;</title><link>http://arxiv.org/abs/2306.04287</link><description>&lt;p&gt;
&#25193;&#23637;&#40657;&#26495;&#26550;&#26500;&#30340;&#26041;&#27861;&#65306;&#20849;&#21516;&#23646;&#24615;&#21644;&#36890;&#29992;&#35268;&#21017;&#30340;&#25903;&#25345;
&lt;/p&gt;
&lt;p&gt;
Extension of the Blackboard Architecture with Common Properties and Generic Rules. (arXiv:2306.04287v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04287
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25193;&#23637;&#40657;&#26495;&#26550;&#26500;&#30340;&#26041;&#27861;&#65292;&#22686;&#21152;&#20102;&#23545;&#20849;&#21516;&#23646;&#24615;&#21644;&#36890;&#29992;&#35268;&#21017;&#30340;&#25903;&#25345;&#65292;&#20197;&#23454;&#29616;&#23545;&#32452;&#32455;&#12289;&#31354;&#38388;&#21644;&#20854;&#20182;&#20851;&#31995;&#30340;&#24314;&#27169;&#65292;&#24182;&#20419;&#36827;&#23545;&#36923;&#36753;&#32467;&#26500;&#30340;&#37325;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40657;&#26495;&#26550;&#26500;&#25552;&#20379;&#20102;&#19968;&#31181;&#23558;&#25968;&#25454;&#12289;&#20915;&#31574;&#21644;&#25191;&#34892;&#20307;&#29616;&#22312;&#19968;&#36215;&#30340;&#26426;&#21046;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#20351;&#29992;&#23481;&#22120;&#23545;&#35937;&#21644;&#38142;&#25509;&#30340;&#26426;&#21046;&#26469;&#24314;&#27169;&#32452;&#32455;&#21644;&#20854;&#20182;&#20851;&#31995;&#65292;&#20294;&#21463;&#21040;&#25163;&#21160;&#23450;&#20041;&#30340;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#40657;&#26495;&#26550;&#26500;&#30340;&#26041;&#27861;&#65292;&#22686;&#21152;&#20102;&#23545;&#20849;&#21516;&#23646;&#24615;&#21644;&#36890;&#29992;&#35268;&#21017;&#30340;&#25903;&#25345;&#65292;&#20197;&#23454;&#29616;&#23545;&#32452;&#32455;&#12289;&#31354;&#38388;&#21644;&#20854;&#20182;&#20851;&#31995;&#30340;&#24314;&#27169;&#65292;&#24182;&#20419;&#36827;&#23545;&#36923;&#36753;&#32467;&#26500;&#30340;&#37325;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Blackboard Architecture provides a mechanism for embodying data, decision making and actuation. Its versatility has been demonstrated across a wide number of application areas. However, it lacks the capability to directly model organizational, spatial and other relationships which may be useful in decision-making, in addition to the propositional logic embodied in the rule-fact-action network. Previous work has proposed the use of container objects and links as a mechanism to simultaneously model these organizational and other relationships, while leaving the operational logic modeled in the rules, facts and actions. While containers facilitate this modeling, their utility is limited by the need to manually define them. For systems which may have multiple instances of a particular type of object and which may build their network autonomously, based on sensing, the reuse of logical structures facilitates operations and reduces storage and processing needs. This paper, thus, presents
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#25513;&#27169;&#30340;&#21333;&#22768;&#36947;&#35821;&#38899;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;MFNet&#65292;&#35813;&#32593;&#32476;&#19981;&#20165;&#21487;&#20197;&#26144;&#23556;&#35821;&#38899;&#65292;&#36824;&#21487;&#20197;&#26144;&#23556;&#21453;&#22122;&#22768;&#65292;&#22312;&#24378;&#22122;&#22768;&#29615;&#22659;&#19979;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#21462;&#24471;&#20102;2020&#24180;DNS&#25361;&#25112;&#27979;&#35797;&#38598;&#30340;&#26368;&#20339;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.04286</link><description>&lt;p&gt;
&#19968;&#31181;&#26080;&#38656;&#25513;&#27169;&#30340;&#21333;&#22768;&#36947;&#35821;&#38899;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
A Mask Free Neural Network for Monaural Speech Enhancement. (arXiv:2306.04286v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04286
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#25513;&#27169;&#30340;&#21333;&#22768;&#36947;&#35821;&#38899;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;MFNet&#65292;&#35813;&#32593;&#32476;&#19981;&#20165;&#21487;&#20197;&#26144;&#23556;&#35821;&#38899;&#65292;&#36824;&#21487;&#20197;&#26144;&#23556;&#21453;&#22122;&#22768;&#65292;&#22312;&#24378;&#22122;&#22768;&#29615;&#22659;&#19979;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#21462;&#24471;&#20102;2020&#24180;DNS&#25361;&#25112;&#27979;&#35797;&#38598;&#30340;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35821;&#38899;&#22686;&#24378;&#20013;&#65292;&#30446;&#26631;&#35821;&#38899;&#30456;&#20301;&#32570;&#20047;&#26126;&#26174;&#30340;&#32467;&#26500;&#29305;&#24449;&#65292;&#38656;&#35201;&#20351;&#29992;&#20445;&#23432;&#32780;&#32321;&#29712;&#30340;&#32593;&#32476;&#26694;&#26550;&#12290;&#20351;&#29992;&#30452;&#25509;&#26041;&#27861;&#21644;&#31616;&#21333;&#30340;&#32593;&#32476;&#26550;&#26500;&#20284;&#20046;&#24456;&#38590;&#21462;&#24471;&#31454;&#20105;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MFNet&#65292;&#19968;&#31181;&#30452;&#25509;&#32780;&#31616;&#21333;&#30340;&#32593;&#32476;&#65292;&#19981;&#20165;&#21487;&#20197;&#26144;&#23556;&#35821;&#38899;&#65292;&#36824;&#21487;&#20197;&#26144;&#23556;&#21453;&#22122;&#22768;&#12290;&#36825;&#20010;&#32593;&#32476;&#26159;&#36890;&#36807;&#22534;&#21472;&#20840;&#23616;&#26412;&#22320;&#21069;&#31471;&#22359;&#65288;GLFBs&#65289;&#26500;&#24314;&#30340;&#65292;&#23427;&#32467;&#21512;&#20102;Mobileblock&#30340;&#20840;&#23616;&#22788;&#29702;&#20248;&#21183;&#21644;Metaformer&#26550;&#26500;&#30340;&#26412;&#22320;&#20132;&#20114;&#20248;&#21183;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#20351;&#29992;&#26144;&#23556;&#26041;&#27861;&#30340;&#32593;&#32476;&#20248;&#20110;&#25513;&#27169;&#26041;&#27861;&#65292;&#24182;&#19988;&#30452;&#25509;&#26144;&#23556;&#21453;&#22122;&#22768;&#26159;&#24378;&#22122;&#22768;&#29615;&#22659;&#19979;&#30340;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;2020&#24180;Deep Noise Suppression&#65288;DNS&#65289;&#25361;&#25112;&#27979;&#35797;&#38598;&#19978;&#30340;&#27178;&#21521;&#27604;&#36739;&#20013;&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;MFNet&#26159;&#24403;&#21069;&#29366;&#24577;-of-the-art&#65288;SOTA&#65289;&#30340;&#26144;&#23556;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In speech enhancement, the lack of clear structural characteristics in the target speech phase requires the use of conservative and cumbersome network frameworks. It seems difficult to achieve competitive performance using direct methods and simple network architectures. However, we propose the MFNet, a direct and simple network that can not only map speech but also map reverse noise. This network is constructed by stacking global local former blocks (GLFBs), which combine the advantages of Mobileblock for global processing and Metaformer architecture for local interaction. Our experimental results demonstrate that our network using mapping method outperforms masking methods, and direct mapping of reverse noise is the optimal solution in strong noise environments. In a horizontal comparison on the 2020 Deep Noise Suppression (DNS) challenge test set without reverberation, to the best of our knowledge, MFNet is the current state-of-the-art (SOTA) mapping model.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#22522;&#20110;&#21435;&#20013;&#24515;&#21270;&#25216;&#26415;&#30340;&#20154;&#24037;&#26234;&#33021;&#26530;&#32445;&#30340;&#28508;&#21147;&#65292;&#20197;&#35299;&#20915;&#20256;&#32479;&#26530;&#32445;&#30340;&#19968;&#20123;&#38382;&#39064;&#65292;&#22914;&#39640;&#25104;&#26412;&#12289;&#32570;&#20047;&#36135;&#24065;&#21270;&#21644;&#22870;&#21169;&#12289;&#32570;&#20047;&#25511;&#21046;&#21644;&#37325;&#29616;&#30340;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2306.04274</link><description>&lt;p&gt;
&#22522;&#20110;&#21435;&#20013;&#24515;&#21270;&#25216;&#26415;&#30340;&#20154;&#24037;&#26234;&#33021;&#26530;&#32445;
&lt;/p&gt;
&lt;p&gt;
Decentralized Technologies for AI Hubs. (arXiv:2306.04274v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04274
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#22522;&#20110;&#21435;&#20013;&#24515;&#21270;&#25216;&#26415;&#30340;&#20154;&#24037;&#26234;&#33021;&#26530;&#32445;&#30340;&#28508;&#21147;&#65292;&#20197;&#35299;&#20915;&#20256;&#32479;&#26530;&#32445;&#30340;&#19968;&#20123;&#38382;&#39064;&#65292;&#22914;&#39640;&#25104;&#26412;&#12289;&#32570;&#20047;&#36135;&#24065;&#21270;&#21644;&#22870;&#21169;&#12289;&#32570;&#20047;&#25511;&#21046;&#21644;&#37325;&#29616;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#38656;&#35201;&#22823;&#37327;&#30340;&#23384;&#20648;&#21644;&#35745;&#31639;&#65292;&#36825;&#20123;&#36164;&#28304;&#36890;&#24120;&#23384;&#20648;&#22312;&#20154;&#24037;&#26234;&#33021;&#26530;&#32445;&#20013;&#12290;&#20154;&#24037;&#26234;&#33021;&#26530;&#32445;&#22312;&#26222;&#21450;&#20154;&#24037;&#26234;&#33021;&#26041;&#38754;&#20570;&#20986;&#20102;&#37325;&#22823;&#36129;&#29486;&#65292;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#23454;&#29616;&#19982;&#22522;&#30784;&#35774;&#26045;&#21644;&#27835;&#29702;&#31995;&#32479;&#30456;&#20851;&#30340;&#26576;&#20123;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#26377;&#20851;&#12290;&#36825;&#20123;&#23616;&#38480;&#24615;&#21253;&#25324;&#39640;&#25104;&#26412;&#12289;&#32570;&#20047;&#36135;&#24065;&#21270;&#21644;&#22870;&#21169;&#12289;&#32570;&#20047;&#25511;&#21046;&#21644;&#37325;&#29616;&#30340;&#22256;&#38590;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#21435;&#20013;&#24515;&#21270;&#25216;&#26415;&#65288;&#22914;Web3&#38065;&#21253;&#12289;&#28857;&#23545;&#28857;&#24066;&#22330;&#12289;&#23384;&#20648;&#21644;&#35745;&#31639;&#20197;&#21450;DAO&#65289;&#30340;&#28508;&#21147;&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#24314;&#35758;&#36825;&#20123;&#22522;&#30784;&#35774;&#26045;&#32452;&#20214;&#21487;&#20197;&#32467;&#21512;&#20351;&#29992;&#22312;&#21435;&#20013;&#24515;&#21270;&#20154;&#24037;&#26234;&#33021;&#26530;&#32445;&#30340;&#35774;&#35745;&#21644;&#26500;&#24314;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
AI requires heavy amounts of storage and compute with assets that are commonly stored in AI Hubs. AI Hubs have contributed significantly to the democratization of AI. However, existing implementations are associated with certain benefits and limitations that stem from the underlying infrastructure and governance systems with which they are built. These limitations include high costs, lack of monetization and reward, lack of control and difficulty of reproducibility. In the current work, we explore the potential of decentralized technologies - such as Web3 wallets, peer-to-peer marketplaces, storage and compute, and DAOs - to address some of these issues. We suggest that these infrastructural components can be used in combination in the design and construction of decentralized AI Hubs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#24322;&#36136;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#26032;&#22411;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;PEGFAN&#65292;&#23427;&#20351;&#29992;&#32622;&#25442;&#31561;&#21464;&#22270;&#26694;&#26550;&#23454;&#29616;&#20102;&#22810;&#23610;&#24230;&#29305;&#24449;&#25552;&#21462;&#65292;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#22312;&#30456;&#23545;&#36739;&#22823;&#21644;&#23494;&#38598;&#36830;&#25509;&#30340;&#25968;&#25454;&#38598;&#20013;&#12290;</title><link>http://arxiv.org/abs/2306.04265</link><description>&lt;p&gt;
&#32622;&#25442;&#31561;&#21464;&#22270;&#26694;&#26550;&#22312;&#24322;&#36136;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Permutaion Equivariant Graph Framelets for Heterophilous Semi-supervised Learning. (arXiv:2306.04265v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04265
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#24322;&#36136;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#26032;&#22411;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;PEGFAN&#65292;&#23427;&#20351;&#29992;&#32622;&#25442;&#31561;&#21464;&#22270;&#26694;&#26550;&#23454;&#29616;&#20102;&#22810;&#23610;&#24230;&#29305;&#24449;&#25552;&#21462;&#65292;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#22312;&#30456;&#23545;&#36739;&#22823;&#21644;&#23494;&#38598;&#36830;&#25509;&#30340;&#25968;&#25454;&#38598;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#36136;&#22270;&#30340;&#26412;&#36136;&#19982;&#21516;&#36136;&#22270;&#26174;&#33879;&#19981;&#21516;&#65292;&#36825;&#34920;&#26126;1-hop&#20197;&#22806;&#30340;&#32858;&#21512;&#26041;&#24335;&#24182;&#24341;&#36215;&#26089;&#26399;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#22256;&#38590;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#23610;&#24230;&#25552;&#21462;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#20855;&#26377;&#32622;&#25442;&#31561;&#21464;&#24615;&#65292;&#39640;&#25928;&#24615;&#21644;&#31232;&#30095;&#24615;&#30340;Haar-type&#22270;&#26694;&#26550;&#65292;&#22312;&#22270;&#19978;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#20013;&#23454;&#29616;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20351;&#29992;&#25105;&#20204;&#26500;&#24314;&#30340;&#22270;&#26694;&#26550;&#35774;&#35745;&#20102;&#22270;&#26694;&#26550;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;PEGFAN&#12290;&#23454;&#39564;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;9&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#65292;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36827;&#34892;&#24615;&#33021;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#26576;&#20123;&#24322;&#36136;&#22270;&#25968;&#25454;&#38598;&#65288;&#21253;&#25324;&#30456;&#23545;&#36739;&#22823;&#21644;&#26356;&#23494;&#38598;&#30340;&#36830;&#25509;&#30340;&#22823;&#37096;&#20998;&#24322;&#36136;&#25968;&#25454;&#38598;&#65289;&#19978;&#21487;&#20197;&#36798;&#21040;&#26368;&#20339;&#24615;&#33021;&#65292;&#24182;&#22312;&#20854;&#20313;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The nature of heterophilous graphs is significantly different with that of homophilous graphs, which suggests aggregations beyond 1-hop neighborhood and causes difficulties in early graph neural network models. In this paper, we develop a new way to implement multi-scale extraction via constructing Haar-type graph framelets with desired properties of permutation equivariance, efficiency, and sparsity, for deep learning tasks on graphs. We further deisgn a graph framelet neural network model PEGFAN using our constructed graph framelets. The experiments are conducted on a synthetic dataset and 9 benchmark datasets to compare performance with other state-of-the-art models. The result shows that our model can achieve best performance on certain datasets of heterophilous graphs (including the majority of heterophilous datasets with relatively larger sizes and denser connections) and competitive performance on the remaining.
&lt;/p&gt;</description></item><item><title>SGD&#22312;&#35757;&#32451;&#36807;&#24230;&#34920;&#36798;&#30340;&#32593;&#32476;&#26102;&#65292;&#20250;&#38543;&#26426;&#22320;&#23558;&#21160;&#24577;&#21560;&#24341;&#21040;&#26356;&#31616;&#21333;&#30340;&#23376;&#32593;&#32476;&#65292;&#36825;&#31181;&#38543;&#26426;&#21560;&#24341;&#24615;&#33021;&#22815;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.04251</link><description>&lt;p&gt;
&#38543;&#26426;&#22349;&#32553;&#65306;&#22914;&#20309;&#21033;&#29992;&#26799;&#24230;&#22122;&#22768;&#20351;SGD&#21160;&#24577;&#36235;&#21521;&#26356;&#31616;&#21333;&#30340;&#23376;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Stochastic Collapse: How Gradient Noise Attracts SGD Dynamics Towards Simpler Subnetworks. (arXiv:2306.04251v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04251
&lt;/p&gt;
&lt;p&gt;
SGD&#22312;&#35757;&#32451;&#36807;&#24230;&#34920;&#36798;&#30340;&#32593;&#32476;&#26102;&#65292;&#20250;&#38543;&#26426;&#22320;&#23558;&#21160;&#24577;&#21560;&#24341;&#21040;&#26356;&#31616;&#21333;&#30340;&#23376;&#32593;&#32476;&#65292;&#36825;&#31181;&#38543;&#26426;&#21560;&#24341;&#24615;&#33021;&#22815;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#30340;&#19968;&#20010;&#24378;&#28872;&#38544;&#24335;&#20559;&#22909;&#65292;&#23427;&#23558;&#36807;&#24230;&#34920;&#36798;&#30340;&#32593;&#32476;&#39537;&#21160;&#21040;&#26356;&#31616;&#21333;&#30340;&#23376;&#32593;&#32476;&#65292;&#20174;&#32780;&#22823;&#22823;&#20943;&#23569;&#20102;&#29420;&#31435;&#21442;&#25968;&#30340;&#25968;&#37327;&#65292;&#24182;&#25552;&#39640;&#20102;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#25581;&#31034;&#36825;&#20010;&#20559;&#22909;&#65292;&#25105;&#20204;&#35782;&#21035;&#20102;&#19981;&#21464;&#38598;&#65292;&#25110;&#32773;&#35828;&#26159;SGD&#26410;&#20462;&#25913;&#30340;&#21442;&#25968;&#31354;&#38388;&#30340;&#23376;&#38598;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#20004;&#31867;&#19981;&#21464;&#38598;&#65292;&#23427;&#20204;&#23545;&#24212;&#20110;&#29616;&#20195;&#26550;&#26500;&#20013;&#24120;&#35265;&#30340;&#26356;&#31616;&#21333;&#30340;&#23376;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;SGD&#22312;&#36825;&#20123;&#31616;&#21333;&#19981;&#21464;&#38598;&#26041;&#38754;&#20855;&#26377;&#38543;&#26426;&#21560;&#24341;&#24615;&#30340;&#29305;&#24615;&#12290;&#25105;&#20204;&#26681;&#25454;&#25439;&#22833;&#26223;&#35266;&#22312;&#19981;&#21464;&#38598;&#21608;&#22260;&#30340;&#26354;&#29575;&#21644;&#38543;&#26426;&#26799;&#24230;&#24341;&#20837;&#30340;&#22122;&#22768;&#20043;&#38388;&#30340;&#31454;&#20105;&#24314;&#31435;&#20102;&#19968;&#31181;&#38543;&#26426;&#21560;&#24341;&#24615;&#30340;&#20805;&#20998;&#26465;&#20214;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#22686;&#21152;&#22122;&#22768;&#27700;&#24179;&#20250;&#22686;&#24378;&#21560;&#24341;&#21147;&#65292;&#23548;&#33268;&#19982;&#38797;&#28857;&#25110;&#35757;&#32451;&#25439;&#22833;&#30340;&#23616;&#37096;&#26497;&#22823;&#20540;&#30456;&#20851;&#30340;&#21560;&#24341;&#19981;&#21464;&#38598;&#30340;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we reveal a strong implicit bias of stochastic gradient descent (SGD) that drives overly expressive networks to much simpler subnetworks, thereby dramatically reducing the number of independent parameters, and improving generalization. To reveal this bias, we identify invariant sets, or subsets of parameter space that remain unmodified by SGD. We focus on two classes of invariant sets that correspond to simpler subnetworks and commonly appear in modern architectures. Our analysis uncovers that SGD exhibits a property of stochastic attractivity towards these simpler invariant sets. We establish a sufficient condition for stochastic attractivity based on a competition between the loss landscape's curvature around the invariant set and the noise introduced by stochastic gradients. Remarkably, we find that an increased level of noise strengthens attractivity, leading to the emergence of attractive invariant sets associated with saddle-points or local maxima of the train loss.
&lt;/p&gt;</description></item><item><title>MobileNMT&#26159;&#19968;&#20010;&#21487;&#20197;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#23454;&#29616;15MB&#21644;30ms&#32763;&#35793;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#19968;&#31995;&#21015;&#27169;&#22411;&#21387;&#32553;&#21407;&#21017;&#21644;&#26379;&#21451;INT8&#21644;&#35299;&#30721;&#30340;&#24341;&#25806;&#30340;&#21327;&#21516;&#35774;&#35745;&#65292;&#23427;&#25104;&#21151;&#35299;&#20915;&#20102;NMT&#27169;&#22411;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#23384;&#20648;&#12289;&#20869;&#23384;&#12289;&#35745;&#31639;&#21644;&#21151;&#32791;&#31561;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#19988;&#20854;&#36895;&#24230;&#25552;&#21319;&#20102;47.0&#20493;&#65292;&#33410;&#30465;&#20102;99.5%&#30340;&#20869;&#23384;&#65292;&#20294;&#20165;&#25439;&#22833;&#20102;11.6%&#30340;BLEU&#12290;</title><link>http://arxiv.org/abs/2306.04235</link><description>&lt;p&gt;
MobileNMT&#65306;&#22312;15MB&#21644;30ms&#20869;&#23454;&#29616;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
MobileNMT: Enabling Translation in 15MB and 30ms. (arXiv:2306.04235v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04235
&lt;/p&gt;
&lt;p&gt;
MobileNMT&#26159;&#19968;&#20010;&#21487;&#20197;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#23454;&#29616;15MB&#21644;30ms&#32763;&#35793;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#19968;&#31995;&#21015;&#27169;&#22411;&#21387;&#32553;&#21407;&#21017;&#21644;&#26379;&#21451;INT8&#21644;&#35299;&#30721;&#30340;&#24341;&#25806;&#30340;&#21327;&#21516;&#35774;&#35745;&#65292;&#23427;&#25104;&#21151;&#35299;&#20915;&#20102;NMT&#27169;&#22411;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#23384;&#20648;&#12289;&#20869;&#23384;&#12289;&#35745;&#31639;&#21644;&#21151;&#32791;&#31561;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#19988;&#20854;&#36895;&#24230;&#25552;&#21319;&#20102;47.0&#20493;&#65292;&#33410;&#30465;&#20102;99.5%&#30340;&#20869;&#23384;&#65292;&#20294;&#20165;&#25439;&#22833;&#20102;11.6%&#30340;BLEU&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38544;&#31169;&#12289;&#20302;&#24310;&#36831;&#21644;&#31163;&#32447;&#22330;&#26223;&#19979;&#65292;&#23558;NMT&#27169;&#22411;&#37096;&#32626;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;NMT&#27169;&#22411;&#30340;&#23481;&#37327;&#36739;&#22823;&#65292;&#22312;&#35774;&#22791;&#19978;&#36816;&#34892;&#36825;&#20123;&#27169;&#22411;&#38754;&#20020;&#30528;&#23384;&#20648;&#12289;&#20869;&#23384;&#12289;&#35745;&#31639;&#21644;&#21151;&#32791;&#31561;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MobileNMT&#30340;&#31995;&#32479;&#65292;&#23427;&#21487;&#20197;&#22312;&#35774;&#22791;&#19978;&#23454;&#29616;15MB&#21644;30ms&#30340;&#32763;&#35793;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#27169;&#22411;&#21387;&#32553;&#30340;&#21407;&#21017;&#65292;&#24182;&#19982;&#37327;&#21270;&#30456;&#32467;&#21512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26379;&#21451;INT8&#21644;&#35299;&#30721;&#30340;&#24341;&#25806;&#12290;&#36890;&#36807;&#27169;&#22411;&#21644;&#24341;&#25806;&#30340;&#21327;&#21516;&#35774;&#35745;&#65292;&#19982;&#29616;&#26377;&#31995;&#32479;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#21152;&#36895;&#20102;47.0&#20493;&#65292;&#33410;&#30465;&#20102;99.5%&#30340;&#20869;&#23384;&#65292;&#20165;&#25439;&#22833;&#20102;11.6%&#30340;BLEU&#12290;&#20195;&#30721;&#21487;&#22312; https://github.com/zjersey/Lightseq-ARM &#19978;&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deploying NMT models on mobile devices is essential for privacy, low latency, and offline scenarios. For high model capacity, NMT models are rather large. Running these models on devices is challenging with limited storage, memory, computation, and power consumption. Existing work either only focuses on a single metric such as FLOPs or general engine which is not good at auto-regressive decoding. In this paper, we present MobileNMT, a system that can translate in 15MB and 30ms on devices. We propose a series of principles for model compression when combined with quantization. Further, we implement an engine that is friendly to INT8 and decoding. With the co-design of model and engine, compared with the existing system, we speed up 47.0x and save 99.5% of memory with only 11.6% loss of BLEU. The code is publicly available at https://github.com/zjersey/Lightseq-ARM.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#21452;&#37325;/&#26080;&#20559;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30740;&#31350;&#20102;&#20809;&#30005;&#21322;&#23548;&#20307;&#21046;&#36896;&#20013;&#30340;&#36820;&#24037;&#27493;&#39588;&#65292;&#20026;&#38646;&#20214;&#36820;&#24037;&#21046;&#23450;&#31574;&#30053;&#24182;&#20174;&#32463;&#39564;&#19978;&#20272;&#35745;&#23427;&#20204;&#30340;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2306.04223</link><description>&lt;p&gt;
&#23398;&#20064;&#21046;&#23450;&#26368;&#20339;&#36820;&#24037;&#31574;&#30053;&#30340;&#22240;&#26524;&#20851;&#31995;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Causally Learning an Optimal Rework Policy. (arXiv:2306.04223v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04223
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#21452;&#37325;/&#26080;&#20559;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30740;&#31350;&#20102;&#20809;&#30005;&#21322;&#23548;&#20307;&#21046;&#36896;&#20013;&#30340;&#36820;&#24037;&#27493;&#39588;&#65292;&#20026;&#38646;&#20214;&#36820;&#24037;&#21046;&#23450;&#31574;&#30053;&#24182;&#20174;&#32463;&#39564;&#19978;&#20272;&#35745;&#23427;&#20204;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21046;&#36896;&#19994;&#20013;&#65292;&#36820;&#24037;&#26159;&#19968;&#31181;&#26088;&#22312;&#28040;&#38500;&#38169;&#35823;&#25110;&#32416;&#27491;&#19981;&#31526;&#21512;&#25152;&#38656;&#36136;&#37327;&#26631;&#20934;&#30340;&#20135;&#21697;&#30340;&#21487;&#36873;&#29983;&#20135;&#27493;&#39588;&#12290;&#37325;&#26032;&#21152;&#24037;&#29983;&#20135;&#25209;&#27425;&#28041;&#21450;&#37325;&#22797;&#20197;&#21069;&#30340;&#29983;&#20135;&#38454;&#27573;&#65292;&#24182;&#36827;&#34892;&#35843;&#25972;&#20197;&#30830;&#20445;&#26368;&#32456;&#20135;&#21697;&#31526;&#21512;&#25152;&#38656;&#35268;&#26684;&#12290;&#34429;&#28982;&#25552;&#20379;&#20102;&#25913;&#21892;&#20135;&#37327;&#20174;&#32780;&#22686;&#21152;&#29983;&#20135;&#25209;&#27425;&#25910;&#20837;&#30340;&#26426;&#20250;&#65292;&#20294;&#36820;&#24037;&#27493;&#39588;&#20063;&#20250;&#20135;&#29983;&#39069;&#22806;&#30340;&#25104;&#26412;&#12290;&#27492;&#22806;&#65292;&#37325;&#26032;&#21152;&#24037;&#24050;&#28385;&#36275;&#30446;&#26631;&#35268;&#26684;&#30340;&#38646;&#20214;&#21487;&#33021;&#20250;&#25439;&#22351;&#23427;&#20204;&#24182;&#38477;&#20302;&#20135;&#37327;&#12290;&#26412;&#25991;&#24212;&#29992;&#21452;&#37325;/&#26080;&#20559;&#26426;&#22120;&#23398;&#20064;&#65288;DML&#65289;&#26469;&#20272;&#35745;&#20809;&#30005;&#21322;&#23548;&#20307;&#21046;&#36896;&#20013;&#39068;&#33394;&#36716;&#25442;&#36807;&#31243;&#20013;&#19968;&#27425;&#36820;&#24037;&#27493;&#39588;&#23545;&#26368;&#32456;&#20135;&#21697;&#20135;&#37327;&#30340;&#26465;&#20214;&#22788;&#29702;&#25928;&#24212;&#12290; &#25105;&#20204;&#21033;&#29992;DoubleML&#23454;&#29616;&#21046;&#23450;&#38646;&#20214;&#36820;&#24037;&#31574;&#30053;&#24182;&#20174;&#32463;&#39564;&#19978;&#20272;&#35745;&#23427;&#20204;&#30340;&#20215;&#20540;&#12290;&#20174;&#25105;&#20204;&#30340;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#20998;&#26512;&#20013;
&lt;/p&gt;
&lt;p&gt;
In manufacturing, rework refers to an optional step of a production process which aims to eliminate errors or remedy products that do not meet the desired quality standards. Reworking a production lot involves repeating a previous production stage with adjustments to ensure that the final product meets the required specifications. While offering the chance to improve the yield and thus increase the revenue of a production lot, a rework step also incurs additional costs. Additionally, the rework of parts that already meet the target specifications may damage them and decrease the yield. In this paper, we apply double/debiased machine learning (DML) to estimate the conditional treatment effect of a rework step during the color conversion process in opto-electronic semiconductor manufacturing on the final product yield. We utilize the implementation DoubleML to develop policies for the rework of components and estimate their value empirically. From our causal machine learning analysis we 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;TDM&#65292;&#21033;&#29992;&#31995;&#32479;&#21160;&#21147;&#23398;&#30340;&#22522;&#26412;&#23545;&#31216;&#24615;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#23567;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2306.04220</link><description>&lt;p&gt;
&#22312;&#34920;&#38754;&#20043;&#19979;&#23547;&#25214;&#65306;&#21033;&#29992;&#22522;&#26412;&#23545;&#31216;&#24615;&#23454;&#29616;&#39640;&#25928;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Look Beneath the Surface: Exploiting Fundamental Symmetry for Sample-Efficient Offline RL. (arXiv:2306.04220v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04220
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;TDM&#65292;&#21033;&#29992;&#31995;&#32479;&#21160;&#21147;&#23398;&#30340;&#22522;&#26412;&#23545;&#31216;&#24615;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#23567;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#36890;&#36807;&#20174;&#39044;&#20808;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#31574;&#30053;&#26469;&#35299;&#20915;&#19982;&#29615;&#22659;&#20132;&#20114;&#30340;&#23454;&#38469;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#20110;&#25968;&#25454;&#38598;&#30340;&#35268;&#27169;&#21644;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#35206;&#30422;&#33539;&#22260;&#12290;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#25910;&#38598;&#36890;&#24120;&#26159;&#26114;&#36149;&#21644;&#38590;&#20197;&#25511;&#21046;&#30340;&#65292;&#23548;&#33268;&#25968;&#25454;&#38598;&#23567;&#19988;&#35206;&#30422;&#33539;&#22260;&#29421;&#31364;&#65292;&#20174;&#32780;&#23545;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#23454;&#38469;&#37096;&#32626;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#35265;&#35299;&#65292;&#21363;&#21033;&#29992;&#31995;&#32479;&#21160;&#21147;&#23398;&#30340;&#22522;&#26412;&#23545;&#31216;&#24615;&#21487;&#20197;&#22312;&#23567;&#25968;&#25454;&#38598;&#19979;&#26174;&#33879;&#25552;&#39640;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26102;&#38388;&#21453;&#28436;&#23545;&#31216;(T-symmetry)&#24378;&#21046;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;(TDM)&#65292;&#24314;&#31435;&#20102;&#19968;&#23545;&#27491;&#21521;&#21644;&#21453;&#21521;&#28508;&#22312;&#21160;&#21147;&#23398;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;TDM&#20026;&#23567;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#33391;&#22909;&#30340;&#34920;&#31034;&#65292;&#24182;&#22522;&#20110;T-symmetry&#30340;&#31526;&#21512;&#24615;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;OOD&#26679;&#26412;&#30340;&#21487;&#38752;&#24615;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (RL) offers an appealing approach to real-world tasks by learning policies from pre-collected datasets without interacting with the environment. However, the performance of existing offline RL algorithms heavily depends on the scale and state-action space coverage of datasets. Real-world data collection is often expensive and uncontrollable, leading to small and narrowly covered datasets and posing significant challenges for practical deployments of offline RL. In this paper, we provide a new insight that leveraging the fundamental symmetry of system dynamics can substantially enhance offline RL performance under small datasets. Specifically, we propose a Time-reversal symmetry (T-symmetry) enforced Dynamics Model (TDM), which establishes consistency between a pair of forward and reverse latent dynamics. TDM provides both well-behaved representations for small datasets and a new reliability measure for OOD samples based on compliance with the T-symmetry. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#19968;&#38454;&#27169;&#22411;&#35745;&#25968;&#38382;&#39064;&#30340;&#31639;&#27861;&#65292;&#23427;&#21487;&#20197;&#34920;&#36798;&#35768;&#22810;&#31867;&#22411;&#36882;&#24402;&#35745;&#31639;&#12290;&#25913;&#36827;&#21518;&#30340;&#31639;&#27861;&#21487;&#20197;&#25214;&#21040;&#37027;&#20123;&#20043;&#21069;&#26080;&#27861;&#26377;&#25928;&#35299;&#20915;&#30340;&#35745;&#25968;&#38382;&#39064;&#30340;&#39640;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2306.04189</link><description>&lt;p&gt;
&#38754;&#21521;&#19968;&#38454;&#27169;&#22411;&#35745;&#25968;&#30340;&#36882;&#24402;&#20989;&#25968;&#21512;&#25104;&#65306;&#25361;&#25112;&#12289;&#36827;&#23637;&#21644;&#29468;&#24819;
&lt;/p&gt;
&lt;p&gt;
Synthesising Recursive Functions for First-Order Model Counting: Challenges, Progress, and Conjectures. (arXiv:2306.04189v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04189
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#19968;&#38454;&#27169;&#22411;&#35745;&#25968;&#38382;&#39064;&#30340;&#31639;&#27861;&#65292;&#23427;&#21487;&#20197;&#34920;&#36798;&#35768;&#22810;&#31867;&#22411;&#36882;&#24402;&#35745;&#31639;&#12290;&#25913;&#36827;&#21518;&#30340;&#31639;&#27861;&#21487;&#20197;&#25214;&#21040;&#37027;&#20123;&#20043;&#21069;&#26080;&#27861;&#26377;&#25928;&#35299;&#20915;&#30340;&#35745;&#25968;&#38382;&#39064;&#30340;&#39640;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#38454;&#27169;&#22411;&#35745;&#25968;(FOMC)&#26159;&#19968;&#20010;&#35745;&#31639;&#38382;&#39064;&#65292;&#23427;&#35201;&#27714;&#22312;&#26377;&#38480;&#22495;&#19968;&#38454;&#36923;&#36753;&#20013;&#35745;&#31639;&#20986;&#19968;&#20010;&#21477;&#23376;&#30340;&#27169;&#22411;&#25968;&#37327;&#12290;&#26412;&#25991;&#35748;&#20026;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;FOMC&#31639;&#27861;&#30340;&#33021;&#21147;&#21463;&#21040;&#20102;&#23427;&#20204;&#34920;&#36798;&#35768;&#22810;&#31867;&#22411;&#36882;&#24402;&#35745;&#31639;&#30340;&#33021;&#21147;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20123;&#35745;&#31639;&#65292;&#25105;&#20204;&#25918;&#23485;&#20102;&#36890;&#24120;&#20276;&#38543;&#22495;&#36882;&#24402;&#30340;&#38480;&#21046;&#65292;&#24182;&#23558;&#29992;&#20110;&#34920;&#36798;FOMC&#38382;&#39064;&#35299;&#30340;&#30005;&#36335;&#36890;&#29992;&#21270;&#20026;&#21487;&#33021;&#21253;&#21547;&#24490;&#29615;&#30340;&#26377;&#21521;&#22270;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23558;&#26368;&#20855;&#20195;&#34920;&#24615;&#30340;FOMC&#31639;&#27861;ForcLift(&#21152;&#26435;)&#36866;&#24212;&#20026;&#33021;&#22815;&#19982;&#36825;&#26679;&#30340;&#22270;&#19968;&#36215;&#20351;&#29992;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#30340;&#32534;&#35793;&#35268;&#21017;&#65292;&#21487;&#20197;&#21019;&#24314;&#32534;&#30721;&#36882;&#24402;&#20989;&#25968;&#35843;&#29992;&#30340;&#24490;&#29615;&#24341;&#23548;&#36793;&#32536;&#12290;&#36825;&#20123;&#25913;&#36827;&#20351;&#31639;&#27861;&#33021;&#22815;&#25214;&#21040;&#23545;&#20197;&#21069;&#26080;&#27861;&#21040;&#36798;&#30340;&#35745;&#25968;&#38382;&#39064;&#30340;&#39640;&#25928;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#37027;&#20123;&#19981;&#33021;&#34987;&#20219;&#20309;&#20854;&#20182;&#31934;&#30830;FOMC&#31639;&#27861;&#26377;&#25928;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#19968;&#20123;&#20851;&#20110;&#21738;&#20123;&#24773;&#20917;&#31867;&#36882;&#24402;&#36816;&#31639;&#21487;&#20197;&#25903;&#25345;&#30340;&#29468;&#24819;&#12290;
&lt;/p&gt;
&lt;p&gt;
First-order model counting (FOMC) is a computational problem that asks to count the models of a sentence in finite-domain first-order logic. In this paper, we argue that the capabilities of FOMC algorithms to date are limited by their inability to express many types of recursive computations. To enable such computations, we relax the restrictions that typically accompany domain recursion and generalise the circuits used to express a solution to an FOMC problem to directed graphs that may contain cycles. To this end, we adapt the most well-established (weighted) FOMC algorithm ForcLift to work with such graphs and introduce new compilation rules that can create cycle-inducing edges that encode recursive function calls. These improvements allow the algorithm to find efficient solutions to counting problems that were previously beyond its reach, including those that cannot be solved efficiently by any other exact FOMC algorithm. We end with a few conjectures on what classes of instances c
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#20915;&#23450;&#22312;&#20309;&#26102;&#20351;&#29992;&#25991;&#26723;&#25110;QA-pair&#22238;&#31572;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.04176</link><description>&lt;p&gt;
&#20309;&#26102;&#26597;&#38405;&#25991;&#26723;&#25110;QA&#21382;&#21490;&#35760;&#24405;&#65306;&#32479;&#19968;&#21644;&#26377;&#36873;&#25321;&#30340;&#24320;&#25918;&#39046;&#22495;QA&#38382;&#39064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
When to Read Documents or QA History: On Unified and Selective Open-domain QA. (arXiv:2306.04176v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04176
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#20915;&#23450;&#22312;&#20309;&#26102;&#20351;&#29992;&#25991;&#26723;&#25110;QA-pair&#22238;&#31572;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#30340;&#38382;&#39064;&#65292;&#33268;&#21147;&#20110;&#21033;&#29992;&#30693;&#35782;&#36164;&#28304;&#22238;&#31572;&#21508;&#31181;&#21508;&#26679;&#30340;&#38382;&#39064;&#12290;&#25991;&#26723;&#35821;&#26009;&#24211;&#21644;QA-pair&#26159;&#20004;&#31181;&#24120;&#29992;&#30340;&#20449;&#24687;&#28304;&#65292;&#21069;&#32773;&#22312;&#22788;&#29702;&#24050;&#30693;&#38382;&#39064;&#26102;&#38750;&#24120;&#20934;&#30830;&#65292;&#32780;&#21518;&#32773;&#22312;&#22788;&#29702;&#26410;&#30693;&#38382;&#39064;&#26102;&#26356;&#20855;&#24191;&#27867;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#39044;&#27979;&#31572;&#26696;&#32622;&#20449;&#24230;&#36827;&#34892;&#20998;&#26512;&#30340;&#26041;&#26696;&#65292;&#20197;&#20915;&#23450;&#20309;&#26102;&#20351;&#29992;&#25991;&#26723;&#25110;QA-pair&#12290;&#35813;&#26041;&#27861;&#22312;Natural Questions&#21644;TriviaQA&#31561;&#24191;&#27867;&#37319;&#29992;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the problem of open-domain question answering, with the aim of answering a diverse range of questions leveraging knowledge resources. Two types of sources, QA-pair and document corpora, have been actively leveraged with the following complementary strength. The former is highly precise when the paraphrase of given question $q$ was seen and answered during training, often posed as a retrieval problem, while the latter generalizes better for unseen questions. A natural follow-up is thus leveraging both models, while a naive pipelining or integration approaches have failed to bring additional gains over either model alone. Our distinction is interpreting the problem as calibration, which estimates the confidence of predicted answers as an indicator to decide when to use a document or QA-pair corpus. The effectiveness of our method was validated on widely adopted benchmarks such as Natural Questions and TriviaQA.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#22788;&#29702;&#24212;&#29992;&#31243;&#24207;&#23631;&#24149;&#20687;&#32032;&#32423;&#21035;&#34394;&#25311;&#21161;&#25163;&#65292;&#21487;&#20197;&#29702;&#35299;&#23454;&#20363;&#32423;&#29992;&#25143;&#24847;&#22270;&#24182;&#39044;&#27979;&#20854;&#30446;&#26631;&#25805;&#20316;&#21306;&#22495;&#65292;&#19981;&#38656;&#35201;&#24212;&#29992;&#31243;&#24207;&#20803;&#25968;&#25454;&#25193;&#23637;&#12290;</title><link>http://arxiv.org/abs/2306.04163</link><description>&lt;p&gt;
&#25552;&#21319;&#34394;&#25311;&#21161;&#25163;&#26234;&#33021;&#65306;&#38024;&#23545;&#20803;&#25968;&#25454;&#20197;&#22806;&#30340;&#23454;&#20363;&#32423;&#29992;&#25143;&#24847;&#22270;&#31934;&#20934;&#21306;&#22495;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Enhancing Virtual Assistant Intelligence: Precise Area Targeting for Instance-level User Intents beyond Metadata. (arXiv:2306.04163v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04163
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#22788;&#29702;&#24212;&#29992;&#31243;&#24207;&#23631;&#24149;&#20687;&#32032;&#32423;&#21035;&#34394;&#25311;&#21161;&#25163;&#65292;&#21487;&#20197;&#29702;&#35299;&#23454;&#20363;&#32423;&#29992;&#25143;&#24847;&#22270;&#24182;&#39044;&#27979;&#20854;&#30446;&#26631;&#25805;&#20316;&#21306;&#22495;&#65292;&#19981;&#38656;&#35201;&#24212;&#29992;&#31243;&#24207;&#20803;&#25968;&#25454;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#34394;&#25311;&#21161;&#25163;&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#31227;&#21160;&#30005;&#35805;&#29992;&#25143;&#20013;&#12290;&#34394;&#25311;&#21161;&#25163;&#22312;&#22788;&#29702;&#29992;&#25143;&#24847;&#22270;&#26041;&#38754;&#30340;&#33021;&#21147;&#24050;&#32463;&#36805;&#36895;&#21457;&#23637;&#65292;&#20294;&#26159;&#65292;&#22312;&#22823;&#22810;&#25968;&#24179;&#21488;&#19978;&#65292;&#34394;&#25311;&#21161;&#25163;&#21482;&#33021;&#22788;&#29702;&#30001;&#24320;&#21457;&#20154;&#21592;&#39069;&#22806;&#25163;&#21160;&#21162;&#21147;&#25903;&#25345;&#30340;&#39044;&#23450;&#20041;&#39640;&#32423;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#21253;&#21547;&#26356;&#35814;&#32454;&#30446;&#26631;&#21644;&#22797;&#26434;&#23454;&#38469;&#24773;&#20917;&#30340;&#23454;&#20363;&#32423;&#29992;&#25143;&#24847;&#22270;&#30446;&#21069;&#21364;&#40092;&#26377;&#30740;&#31350;&#12290;&#26412;&#25991;&#26088;&#22312;&#25506;&#32034;&#33021;&#22815;&#22522;&#20110;&#24212;&#29992;&#31243;&#24207;&#23631;&#24149;&#20687;&#32032;&#22788;&#29702;&#23454;&#20363;&#32423;&#29992;&#25143;&#24847;&#22270;&#30340;&#34394;&#25311;&#21161;&#25163;&#65292;&#32780;&#19981;&#38656;&#35201;&#22312;&#24212;&#29992;&#31243;&#24207;&#31471;&#36827;&#34892;&#39069;&#22806;&#25193;&#23637;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36328;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#29702;&#35299;&#35821;&#38899;&#25110;&#25991;&#26412;&#23454;&#20363;&#32423;&#29992;&#25143;&#24847;&#22270;&#65292;&#39044;&#27979;&#30446;&#26631;&#25805;&#20316;&#21306;&#22495;&#65292;&#24182;&#22312;&#27809;&#26377;&#24212;&#29992;&#31243;&#24207;&#20803;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#26816;&#27979;&#23631;&#24149;&#19978;&#30340;&#32477;&#23545;&#25353;&#38062;&#21306;&#22495;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#29992;&#25143;&#30740;&#31350;&#65292;&#25910;&#38598;&#20102;&#19968;&#20010;&#27979;&#35797;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;10&#20010;&#21442;&#19982;&#32773;&#30340;&#23454;&#20363;&#32423;&#29992;&#25143;&#24847;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Virtual assistants have been widely used by mobile phone users in recent years. Although their capabilities of processing user intents have been developed rapidly, virtual assistants in most platforms are only capable of handling pre-defined high-level tasks supported by extra manual efforts of developers. However, instance-level user intents containing more detailed objectives with complex practical situations, are yet rarely studied so far. In this paper, we explore virtual assistants capable of processing instance-level user intents based on pixels of application screens, without the requirements of extra extensions on the application side. We propose a novel cross-modal deep learning pipeline, which understands the input vocal or textual instance-level user intents, predicts the targeting operational area, and detects the absolute button area on screens without any metadata of applications. We conducted a user study with 10 participants to collect a testing dataset with instance-le
&lt;/p&gt;</description></item><item><title>&#21452;&#21521; GaitNet&#26159;&#19968;&#31181;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#27169;&#25311;&#25968;&#25454;&#21644;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#26469;&#39044;&#27979;&#20154;&#31867;&#30340;&#27493;&#24577;&#21644;&#36523;&#20307;&#29366;&#20917;&#65292;&#33021;&#24212;&#29992;&#20110;&#20581;&#24247;&#21644;&#21463;&#25439;&#27493;&#24577;&#30340;&#39044;&#27979;&#21644;&#20272;&#31639;&#12290;</title><link>http://arxiv.org/abs/2306.04161</link><description>&lt;p&gt;
&#21452;&#21521; GaitNet&#65306;&#19968;&#31181;&#20154;&#31867;&#27493;&#24577;&#21644;&#35299;&#21078;&#24773;&#20917;&#30340;&#21452;&#21521;&#39044;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Bidirectional GaitNet: A Bidirectional Prediction Model of Human Gait and Anatomical Conditions. (arXiv:2306.04161v1 [cs.GR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04161
&lt;/p&gt;
&lt;p&gt;
&#21452;&#21521; GaitNet&#26159;&#19968;&#31181;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#27169;&#25311;&#25968;&#25454;&#21644;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#26469;&#39044;&#27979;&#20154;&#31867;&#30340;&#27493;&#24577;&#21644;&#36523;&#20307;&#29366;&#20917;&#65292;&#33021;&#24212;&#29992;&#20110;&#20581;&#24247;&#21644;&#21463;&#25439;&#27493;&#24577;&#30340;&#39044;&#27979;&#21644;&#20272;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#31216;&#20026;&#21452;&#21521; GaitNet&#65292;&#23427;&#23398;&#20064;&#20102;&#20154;&#31867;&#35299;&#21078;&#32467;&#26500;&#21644;&#27493;&#24577;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#20154;&#20307;&#35299;&#21078;&#30340;&#27169;&#25311;&#27169;&#22411;&#26159;&#19968;&#20010;&#20840;&#36523;&#30340;&#12289;&#21253;&#21547;304&#20010;Hill&#31867;&#22411;&#32908;&#33137;&#21333;&#20301;&#30340;&#32908;&#32905;&#39592;&#39612;&#27169;&#22411;&#12290;&#21452;&#21521; GaitNet&#30001;&#21069;&#21521;&#21644;&#21518;&#21521;&#27169;&#22411;&#32452;&#25104;&#12290;&#21069;&#21521;&#27169;&#22411;&#39044;&#27979;&#20855;&#26377;&#29305;&#23450;&#36523;&#20307;&#29366;&#20917;&#30340;&#20154;&#30340;&#27493;&#24577;&#27169;&#24335;&#65292;&#32780;&#21518;&#21521;&#27169;&#22411;&#20272;&#31639;&#20154;&#30340;&#36523;&#20307;&#29366;&#20917;&#24403;&#20854;&#27493;&#24577;&#27169;&#24335;&#34987;&#25552;&#20379;&#12290;&#25105;&#20204;&#22522;&#20110;&#27169;&#25311;&#26041;&#27861;&#39318;&#20808;&#36890;&#36807;&#25552;&#21462;&#30001;&#26368;&#20808;&#36827;&#30340;&#39044;&#27979;&#27493;&#24577;&#27169;&#25311;&#22120;&#29983;&#25104;&#30340;&#27169;&#25311;&#25968;&#25454;&#26469;&#23398;&#20064;&#21069;&#21521;&#27169;&#22411;&#65292;&#28982;&#21518;&#20351;&#29992;&#23398;&#20064;&#30340;&#21069;&#21521;&#27169;&#22411;&#26500;&#24314;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#12290;&#19968;&#26086;&#23427;&#34987;&#23398;&#20064;&#65292;&#23427;&#30340;&#32534;&#30721;&#22120;&#23601;&#20316;&#20026;&#21518;&#21521;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#22810;&#31181;&#20581;&#24247;/&#21463;&#25439;&#27493;&#24577;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#19982;&#30495;&#23454;&#24739;&#32773;&#30340;&#20307;&#26816;&#25968;&#25454;&#36827;&#34892;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel generative model, called Bidirectional GaitNet, that learns the relationship between human anatomy and its gait. The simulation model of human anatomy is a comprehensive, full-body, simulation-ready, musculoskeletal model with 304 Hill-type musculotendon units. The Bidirectional GaitNet consists of forward and backward models. The forward model predicts a gait pattern of a person with specific physical conditions, while the backward model estimates the physical conditions of a person when his/her gait pattern is provided. Our simulation-based approach first learns the forward model by distilling the simulation data generated by a state-of-the-art predictive gait simulator and then constructs a Variational Autoencoder (VAE) with the learned forward model as its decoder. Once it is learned its encoder serves as the backward model. We demonstrate our model on a variety of healthy/impaired gaits and validate it in comparison with physical examination data of real patient
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#35299;&#20915;&#26041;&#26696; One-ASQP&#65292;&#20197;&#26816;&#27979;&#26041;&#38754;&#31867;&#21035;&#24182;&#21516;&#26102;&#35782;&#21035;&#26041;&#38754;-&#35266;&#28857;-&#24773;&#24863;&#65288;AOS&#65289;&#19977;&#20803;&#32452;&#65292;&#36890;&#36807;&#24341;&#20837;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#20998;&#23618;&#27880;&#24847;&#26426;&#21046;&#65292;&#25552;&#39640;&#20102;&#39044;&#27979;&#20934;&#30830;&#24615;&#24182;&#20855;&#26377;&#39640;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.04152</link><description>&lt;p&gt;
&#19968;&#31181;&#32479;&#19968;&#30340;&#35299;&#20915;&#26041;&#26696;&#29992;&#20110;&#26041;&#38754;&#24773;&#24863;&#22235;&#20803;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
A Unified One-Step Solution for Aspect Sentiment Quad Prediction. (arXiv:2306.04152v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04152
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#35299;&#20915;&#26041;&#26696; One-ASQP&#65292;&#20197;&#26816;&#27979;&#26041;&#38754;&#31867;&#21035;&#24182;&#21516;&#26102;&#35782;&#21035;&#26041;&#38754;-&#35266;&#28857;-&#24773;&#24863;&#65288;AOS&#65289;&#19977;&#20803;&#32452;&#65292;&#36890;&#36807;&#24341;&#20837;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#20998;&#23618;&#27880;&#24847;&#26426;&#21046;&#65292;&#25552;&#39640;&#20102;&#39044;&#27979;&#20934;&#30830;&#24615;&#24182;&#20855;&#26377;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26041;&#38754;&#24773;&#24863;&#22235;&#20803;&#39044;&#27979;&#65288;ASQP&#65289;&#26159;&#26041;&#38754;&#24773;&#24863;&#20998;&#26512;&#20013;&#20855;&#26377;&#25361;&#25112;&#24615;&#20294;&#26497;&#20855;&#24847;&#20041;&#30340;&#23376;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#25552;&#20379;&#20102;&#23436;&#25972;&#30340;&#26041;&#38754;&#32423;&#24773;&#24863;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;ASQP&#25968;&#25454;&#38598;&#36890;&#24120;&#24456;&#23567;&#19988;&#23494;&#24230;&#36739;&#20302;&#65292;&#38459;&#30861;&#20102;&#25216;&#26415;&#30340;&#36827;&#27493;&#12290;&#20026;&#20102;&#25193;&#23637;&#33021;&#21147;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;ASQP&#25968;&#25454;&#38598;&#65292;&#20855;&#26377;&#20197;&#19979;&#29305;&#24449;&#65306;&#26356;&#22823;&#30340;&#22823;&#23567;&#65292;&#26356;&#22810;&#30340;&#21333;&#35789;&#25968;&#37327;&#21644;&#26356;&#39640;&#30340;&#23494;&#24230;&#12290;&#36890;&#36807;&#36825;&#26679;&#30340;&#25968;&#25454;&#38598;&#65292;&#25581;&#31034;&#20102;&#29616;&#26377;&#24378;ASQP&#22522;&#32447;&#30340;&#32570;&#38519;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;ASQP&#19968;&#27493;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;One-ASQP&#65292;&#20197;&#21516;&#26102;&#26816;&#27979;&#26041;&#38754;&#31867;&#21035;&#24182;&#35782;&#21035;&#26041;&#38754;-&#35266;&#28857;-&#24773;&#24863;&#65288;AOS&#65289;&#19977;&#20803;&#32452;&#12290; One-ASQP&#20855;&#26377;&#20197;&#19979;&#20960;&#28857;&#29420;&#29305;&#30340;&#20248;&#21183;&#65306;&#65288;1&#65289;&#36890;&#36807;&#23558;ASQP&#20998;&#25104;&#20004;&#20010;&#23376;&#20219;&#21153;&#24182;&#29420;&#31435;&#21516;&#26102;&#35299;&#20915;&#23427;&#20204;&#65292;&#25105;&#20204;&#21487;&#20197;&#36991;&#20813;&#22522;&#20110;&#31649;&#36947;&#30340;&#26041;&#27861;&#20013;&#30340;&#38169;&#35823;&#20256;&#25773;&#65292;&#24182;&#20811;&#26381;&#29983;&#25104;&#24615;&#26041;&#27861;&#20013;&#30340;&#24930;&#36895;&#35757;&#32451;&#21644;&#25512;&#29702;;&#65288;2&#65289;&#36890;&#36807;&#24341;&#20837;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#20998;&#23618;&#27880;&#24847;&#26426;&#21046;&#65292;&#25105;&#20204;&#21487;&#20197;&#25552;&#39640;&#26041;&#38754;&#31867;&#21035;&#21644;AOS&#19977;&#20803;&#32452;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#22522;&#20110;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;One-ASQP&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aspect sentiment quad prediction (ASQP) is a challenging yet significant subtask in aspect-based sentiment analysis as it provides a complete aspect-level sentiment structure. However, existing ASQP datasets are usually small and low-density, hindering technical advancement. To expand the capacity, in this paper, we release two new datasets for ASQP, which contain the following characteristics: larger size, more words per sample, and higher density. With such datasets, we unveil the shortcomings of existing strong ASQP baselines and therefore propose a unified one-step solution for ASQP, namely One-ASQP, to detect the aspect categories and to identify the aspect-opinion-sentiment (AOS) triplets simultaneously. Our One-ASQP holds several unique advantages: (1) by separating ASQP into two subtasks and solving them independently and simultaneously, we can avoid error propagation in pipeline-based methods and overcome slow training and inference in generation-based methods; (2) by introduc
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;AI&#26159;&#19968;&#31181;&#26032;&#30340;&#33402;&#26415;&#23186;&#20171;&#65292;&#20855;&#26377;&#25913;&#21464;&#21019;&#36896;&#36807;&#31243;&#21644;&#31038;&#20250;&#26500;&#24819;&#30340;&#28508;&#21147;&#65292;&#21487;&#20197;&#20174;&#32654;&#23398;&#12289;&#27861;&#24459;&#12289;&#21019;&#20316;&#26410;&#26469;&#21644;&#23186;&#20307;&#29983;&#24577;&#31561;&#26041;&#38754;&#24433;&#21709;&#21019;&#20316;&#32773;&#21644;&#31038;&#20250;&#12290;</title><link>http://arxiv.org/abs/2306.04141</link><description>&lt;p&gt;
&#29983;&#25104;AI&#30340;&#33402;&#26415;&#19982;&#31185;&#23398;&#65306;&#26356;&#28145;&#20837;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Art and the science of generative AI: A deeper dive. (arXiv:2306.04141v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04141
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;AI&#26159;&#19968;&#31181;&#26032;&#30340;&#33402;&#26415;&#23186;&#20171;&#65292;&#20855;&#26377;&#25913;&#21464;&#21019;&#36896;&#36807;&#31243;&#21644;&#31038;&#20250;&#26500;&#24819;&#30340;&#28508;&#21147;&#65292;&#21487;&#20197;&#20174;&#32654;&#23398;&#12289;&#27861;&#24459;&#12289;&#21019;&#20316;&#26410;&#26469;&#21644;&#23186;&#20307;&#29983;&#24577;&#31561;&#26041;&#38754;&#24433;&#21709;&#21019;&#20316;&#32773;&#21644;&#31038;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#31867;&#26032;&#24037;&#20855;&#34987;&#20439;&#31216;&#20026;&#29983;&#25104;AI&#65292;&#21487;&#20197;&#20026;&#35270;&#35273;&#33402;&#26415;&#12289;&#27010;&#24565;&#33402;&#26415;&#12289;&#38899;&#20048;&#12289;&#23567;&#35828;&#12289;&#25991;&#23398;&#12289;&#35270;&#39057;&#21644;&#21160;&#30011;&#21019;&#36896;&#39640;&#36136;&#37327;&#30340;&#33402;&#26415;&#23186;&#20171;&#12290;&#36825;&#20123;&#24037;&#20855;&#30340;&#29983;&#25104;&#33021;&#21147;&#21487;&#33021;&#20250;&#20174;&#26681;&#26412;&#19978;&#25913;&#21464;&#21019;&#36896;&#32773;&#21046;&#23450;&#24819;&#27861;&#24182;&#23558;&#20854;&#25237;&#20837;&#29983;&#20135;&#30340;&#21019;&#36896;&#36807;&#31243;&#12290;&#38543;&#30528;&#21019;&#36896;&#21147;&#30340;&#37325;&#26032;&#26500;&#24819;&#65292;&#31038;&#20250;&#30340;&#35768;&#22810;&#39046;&#22495;&#20063;&#21487;&#33021;&#34987;&#37325;&#26032;&#26500;&#24819;&#12290;&#29702;&#35299;&#29983;&#25104;AI&#30340;&#24433;&#21709;&#24182;&#21046;&#23450;&#25919;&#31574;&#20915;&#31574; - &#38656;&#35201;&#26032;&#30340;&#36328;&#23398;&#31185;&#30340;&#31185;&#23398;&#25506;&#31350;&#25991;&#21270;&#12289;&#32463;&#27982;&#12289;&#27861;&#24459;&#12289;&#31639;&#27861;&#20197;&#21450;&#25216;&#26415;&#21644;&#21019;&#36896;&#21147;&#30340;&#20132;&#20114;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#29983;&#25104;AI&#24182;&#19981;&#26159;&#33402;&#26415;&#28781;&#20129;&#30340;&#20808;&#20806;&#65292;&#32780;&#26159;&#19968;&#31181;&#20855;&#26377;&#33258;&#24049;&#29420;&#29305;&#20248;&#21183;&#30340;&#26032;&#23186;&#20171;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#36825;&#31181;&#26032;&#23186;&#20171;&#23545;&#22235;&#20010;&#20027;&#39064;&#20013;&#30340;&#21019;&#20316;&#32773;&#30340;&#24433;&#21709;&#65306;&#32654;&#23398;&#21644;&#25991;&#21270;&#65292;&#25152;&#26377;&#26435;&#21644;&#20449;&#29992;&#30340;&#27861;&#24459;&#38382;&#39064;&#65292;&#21019;&#36896;&#24615;&#24037;&#20316;&#30340;&#26410;&#26469;&#20197;&#21450;&#23545;&#24403;&#20195;&#23186;&#20307;&#29983;&#24577;&#31995;&#32479;&#30340;&#24433;&#21709;&#12290;&#22312;&#36825;&#20123;&#20027;&#39064;&#20013;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#29983;&#25104;AI&#20013;&#33402;&#26415;&#21644;&#31185;&#23398;&#30340;&#20132;&#21449;&#23545;&#21019;&#36896;&#21147;&#21644;&#31038;&#20250;&#30340;&#36716;&#21270;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
A new class of tools, colloquially called generative AI, can produce high-quality artistic media for visual arts, concept art, music, fiction, literature, video, and animation. The generative capabilities of these tools are likely to fundamentally alter the creative processes by which creators formulate ideas and put them into production. As creativity is reimagined, so too may be many sectors of society. Understanding the impact of generative AI and making policy decisions around it - requires new interdisciplinary scientific inquiry into culture, economics, law, algorithms, and the interaction of technology and creativity. We argue that generative AI is not the harbinger of art's demise, but rather is a new medium with its own distinct affordances. In this vein, we consider the impacts of this new medium on creators across four themes: aesthetics and culture, legal questions of ownership and credit, the future of creative work, and impacts on the contemporary media ecosystem. Acros
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20840;&#38754;&#32508;&#36848;&#20102;&#22312;&#32467;&#26500;&#21270;&#25968;&#25454;&#39046;&#22495;&#20013;&#26368;&#36817;&#25552;&#20986;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#20171;&#32461;&#20102;&#20854;&#29702;&#35770;&#22522;&#30784;&#21644;&#24212;&#29992;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2306.04139</link><description>&lt;p&gt;
&#32467;&#26500;&#21270;&#25968;&#25454;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Generative Diffusion Models for Structured Data. (arXiv:2306.04139v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04139
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#32508;&#36848;&#20102;&#22312;&#32467;&#26500;&#21270;&#25968;&#25454;&#39046;&#22495;&#20013;&#26368;&#36817;&#25552;&#20986;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#20171;&#32461;&#20102;&#20854;&#29702;&#35770;&#22522;&#30784;&#21644;&#24212;&#29992;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#65288;generative diffusion models&#65289;&#22312;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#39046;&#22495;&#21462;&#24471;&#20102;&#31361;&#30772;&#24615;&#25104;&#26524;&#65292;&#23637;&#29616;&#20102;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#30340;&#20986;&#33394;&#34920;&#29616;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#32467;&#26500;&#21270;&#25968;&#25454;&#65288;&#21253;&#25324;&#34920;&#26684;&#21644;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65289;&#22312;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;&#30028;&#20013;&#21463;&#21040;&#30340;&#20851;&#27880;&#30456;&#23545;&#36739;&#23569;&#65292;&#23613;&#31649;&#20854;&#26080;&#22788;&#19981;&#22312;&#19988;&#24212;&#29992;&#24191;&#27867;&#12290;&#22240;&#27492;&#65292;&#19982;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#20854;&#20182;&#25968;&#25454;&#24418;&#24335;&#30456;&#27604;&#65292;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#23545;&#32467;&#26500;&#21270;&#25968;&#25454;&#24314;&#27169;&#30340;&#25991;&#29486;&#21450;&#20854;&#32508;&#36848;&#20173;&#28982;&#32570;&#20047;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#32467;&#26500;&#21270;&#25968;&#25454;&#39046;&#22495;&#20013;&#26368;&#36817;&#25552;&#20986;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#20840;&#38754;&#32508;&#36848;&#12290;&#39318;&#20808;&#65292;&#26412;&#32508;&#36848;&#25552;&#20379;&#20102;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#29702;&#35770;&#30340;&#31616;&#35201;&#27010;&#36848;&#65292;&#38543;&#21518;&#21448;&#35814;&#32454;&#25551;&#36848;&#20102;&#22312;&#25968;&#25454;&#39537;&#21160;&#30340;&#36890;&#29992;&#20219;&#21153;&#21644;&#29305;&#23450;&#39046;&#22495;&#24212;&#29992;&#20013;&#20351;&#29992;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#22823;&#37096;&#20998;&#24320;&#21019;&#24615;&#24037;&#20316;&#30340;&#25216;&#26415;&#25551;&#36848;&#12290;&#26368;&#21518;&#65292;
&lt;/p&gt;
&lt;p&gt;
In recent years, generative diffusion models have achieved a rapid paradigm shift in deep generative models by showing groundbreaking performance across various applications. Meanwhile, structured data, encompassing tabular and time series data, has been received comparatively limited attention from the deep learning research community, despite its omnipresence and extensive applications. Thus, there is still a lack of literature and its review on structured data modelling via diffusion models, compared to other data modalities such as computer vision and natural language processing. Hence, in this paper, we present a comprehensive review of recently proposed diffusion models in the field of structured data. First, this survey provides a concise overview of the score-based diffusion model theory, subsequently proceeding to the technical descriptions of the majority of pioneering works using structured data in both data-driven general tasks and domain-specific applications. Thereafter, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;RetroKNN&#65292;&#23427;&#20351;&#29992;&#22522;&#20110;&#26412;&#22320;&#21453;&#24212;&#27169;&#26495;&#30340;k&#26368;&#36817;&#37051;&#26816;&#32034;&#32467;&#21512;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#26469;&#25552;&#39640;&#36870;&#21521;&#21512;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.04123</link><description>&lt;p&gt;
&#22522;&#20110;&#26412;&#22320;&#27169;&#26495;&#26816;&#32034;&#30340;&#36870;&#21521;&#21512;&#25104;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Retrosynthesis Prediction with Local Template Retrieval. (arXiv:2306.04123v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04123
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;RetroKNN&#65292;&#23427;&#20351;&#29992;&#22522;&#20110;&#26412;&#22320;&#21453;&#24212;&#27169;&#26495;&#30340;k&#26368;&#36817;&#37051;&#26816;&#32034;&#32467;&#21512;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#26469;&#25552;&#39640;&#36870;&#21521;&#21512;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36870;&#21521;&#21512;&#25104;&#26159;&#33647;&#29289;&#25506;&#32034;&#20013;&#39044;&#27979;&#32473;&#23450;&#30446;&#26631;&#20998;&#23376;&#21453;&#24212;&#29289;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#36870;&#21521;&#21512;&#25104;&#26041;&#27861;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;RetroKNN&#65292;&#19968;&#31181;&#22522;&#20110;&#26412;&#22320;&#21453;&#24212;&#27169;&#26495;&#26816;&#32034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38750;&#21442;&#25968;&#26816;&#32034;&#36827;&#19968;&#27493;&#25552;&#39640;&#22522;&#20110;&#27169;&#26495;&#30340;&#31995;&#32479;&#24615;&#33021;&#12290;&#25105;&#20204;&#39318;&#20808;&#26500;&#24314;&#20102;&#19968;&#20010;&#21407;&#23376;&#27169;&#26495;&#23384;&#20648;&#24211;&#21644;&#19968;&#20010;&#38190;&#27169;&#26495;&#23384;&#20648;&#24211;&#65292;&#20854;&#20013;&#21253;&#21547;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#26412;&#22320;&#27169;&#26495;&#65292;&#28982;&#21518;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#20351;&#29992;k&#26368;&#36817;&#37051;&#65288;KNN&#65289;&#25628;&#32034;&#20174;&#36825;&#20123;&#27169;&#26495;&#20013;&#26816;&#32034;&#12290;&#26816;&#32034;&#21040;&#30340;&#27169;&#26495;&#19982;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#30456;&#32467;&#21512;&#20316;&#20026;&#26368;&#32456;&#36755;&#20986;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#36866;&#37197;&#22120;&#65292;&#20197;&#35843;&#25972;&#22522;&#20110;&#38544;&#34255;&#34920;&#31034;&#21644;&#26816;&#32034;&#27169;&#26495;&#30340;&#31070;&#32463;&#32593;&#32476;&#21644;KNN&#39044;&#27979;&#30340;&#26435;&#37325;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#22522;&#20934;&#27979;&#35797;USPTO-50K&#21644;USPTO-MIT&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#12290;&#23588;&#20854;&#26159;&#22312;top-1&#31934;&#24230;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Retrosynthesis, which predicts the reactants of a given target molecule, is an essential task for drug discovery. In recent years, the machine learing based retrosynthesis methods have achieved promising results. In this work, we introduce RetroKNN, a local reaction template retrieval method to further boost the performance of template-based systems with non-parametric retrieval. We first build an atom-template store and a bond-template store that contain the local templates in the training data, then retrieve from these templates with a k-nearest-neighbor (KNN) search during inference. The retrieved templates are combined with neural network predictions as the final output. Furthermore, we propose a lightweight adapter to adjust the weights when combing neural network and KNN predictions conditioned on the hidden representation and the retrieved templates. We conduct comprehensive experiments on two widely used benchmarks, the USPTO-50K and USPTO-MIT. Especially for the top-1 accuracy
&lt;/p&gt;</description></item><item><title>MESSY&#20272;&#35745;&#26041;&#27861;&#26159;&#19968;&#31181;&#22522;&#20110;&#26368;&#22823;&#29109;&#30340;&#38543;&#26426;&#21644;&#31526;&#21495;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#22522;&#20110;&#26799;&#24230;&#30340;&#28418;&#31227;&#25193;&#25955;&#36807;&#31243;&#26469;&#39640;&#25928;&#22320;&#25214;&#21040;&#26368;&#22823;&#29109;&#20998;&#24067;&#30340;&#21442;&#25968;&#65292;&#25903;&#25345;&#39640;&#32500;&#38382;&#39064;&#65292;&#24182;&#20855;&#26377;&#20248;&#20110;&#29616;&#26377;&#26368;&#26032;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#26222;&#36866;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.04120</link><description>&lt;p&gt;
MESSY&#20272;&#35745;&#65306;&#22522;&#20110;&#26368;&#22823;&#29109;&#30340;&#38543;&#26426;&#21644;&#31526;&#21495;&#23494;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
MESSY Estimation: Maximum-Entropy based Stochastic and Symbolic densitY Estimation. (arXiv:2306.04120v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04120
&lt;/p&gt;
&lt;p&gt;
MESSY&#20272;&#35745;&#26041;&#27861;&#26159;&#19968;&#31181;&#22522;&#20110;&#26368;&#22823;&#29109;&#30340;&#38543;&#26426;&#21644;&#31526;&#21495;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#22522;&#20110;&#26799;&#24230;&#30340;&#28418;&#31227;&#25193;&#25955;&#36807;&#31243;&#26469;&#39640;&#25928;&#22320;&#25214;&#21040;&#26368;&#22823;&#29109;&#20998;&#24067;&#30340;&#21442;&#25968;&#65292;&#25903;&#25345;&#39640;&#32500;&#38382;&#39064;&#65292;&#24182;&#20855;&#26377;&#20248;&#20110;&#29616;&#26377;&#26368;&#26032;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#26368;&#22823;&#29109;&#30340;&#38543;&#26426;&#21644;&#31526;&#21495;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;MESSY&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#29992;&#26799;&#24230;&#27969;&#30340;&#30697;&#23558;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#20174;&#26679;&#26412;&#20013;&#24674;&#22797;&#20026;&#31526;&#21495;&#34920;&#36798;&#24335;&#65292;&#24182;&#23558;ansatz&#20316;&#20026;&#39537;&#21160;&#21147;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;&#26799;&#24230;&#30340;&#28418;&#31227;&#25193;&#25955;&#36807;&#31243;&#65292;&#23558;&#26410;&#30693;&#20998;&#24067;&#20989;&#25968;&#30340;&#26679;&#26412;&#19982;&#29468;&#27979;&#30340;&#31526;&#21495;&#34920;&#36798;&#24335;&#30456;&#36830;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20986;&#24403;&#29468;&#27979;&#20998;&#24067;&#20855;&#26377;&#26368;&#22823;&#29109;&#24418;&#24335;&#26102;&#65292;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#25552;&#20379;&#30340;&#26679;&#26412;&#30340;&#30697;&#26500;&#24314;&#30340;&#32447;&#24615;&#26041;&#31243;&#32452;&#39640;&#25928;&#22320;&#25214;&#21040;&#35813;&#20998;&#24067;&#30340;&#21442;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#31526;&#21495;&#22238;&#24402;&#26469;&#25506;&#32034;&#24179;&#28369;&#20989;&#25968;&#30340;&#31354;&#38388;&#65292;&#24182;&#25214;&#21040;&#23548;&#33268;&#26368;&#22823;&#29109;&#27867;&#20989;&#25351;&#25968;&#30340;&#26368;&#20248;&#22522;&#20989;&#25968;&#65292;&#20197;&#33719;&#24471;&#33391;&#22909;&#26465;&#20214;&#12290;&#35813;&#26041;&#27861;&#22312;&#38543;&#26426;&#25628;&#32034;&#30340;&#27599;&#27425;&#36845;&#20195;&#20013;&#30340;&#25104;&#26412;&#19982;&#26679;&#26412;&#25968;&#37327;&#21576;&#32447;&#24615;&#20851;&#31995;&#65292;&#19982;&#21464;&#37327;&#25968;&#37327;&#21576;&#20108;&#27425;&#20851;&#31995;&#65292;&#20351;&#20854;&#21487;&#25193;&#23637;&#21040;&#39640;&#32500;&#38382;&#39064;&#12290;&#25968;&#20540;&#23454;&#39564;&#26174;&#31034;&#20986;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#26222;&#36866;&#24615;&#65292;&#19982;&#29616;&#26377;&#30340;&#26368;&#26032;&#26041;&#27861;&#30456;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce MESSY estimation, a Maximum-Entropy based Stochastic and Symbolic densitY estimation method. The proposed approach recovers probability density functions symbolically from samples using moments of a Gradient flow in which the ansatz serves as the driving force. In particular, we construct a gradient-based drift-diffusion process that connects samples of the unknown distribution function to a guess symbolic expression. We then show that when the guess distribution has the maximum entropy form, the parameters of this distribution can be found efficiently by solving a linear system of equations constructed using the moments of the provided samples. Furthermore, we use Symbolic regression to explore the space of smooth functions and find optimal basis functions for the exponent of the maximum entropy functional leading to good conditioning. The cost of the proposed method in each iteration of the random search is linear with the number of samples and quadratic with the number 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;M$^3$Fair&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22810;&#20010;&#25968;&#25454;&#23618;&#38754;&#21644;&#22810;&#20010;&#25935;&#24863;&#23646;&#24615;&#30340;&#24773;&#20917;&#19979;&#24179;&#34913;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#65292;&#32531;&#35299;&#21307;&#30103;&#25968;&#25454;&#20013;&#30340;&#20559;&#35265;&#21644;&#19981;&#20844;&#24179;&#29616;&#35937;&#65292;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#33021;&#21644;&#20934;&#30830;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.04118</link><description>&lt;p&gt;
M$^3$Fair&#65306;&#22810;&#23618;&#27425;&#22810;&#25935;&#24863;&#23646;&#24615;&#21152;&#26435;&#26041;&#27861;&#20943;&#36731;&#21307;&#30103;&#25968;&#25454;&#20013;&#30340;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
M$^3$Fair: Mitigating Bias in Healthcare Data through Multi-Level and Multi-Sensitive-Attribute Reweighting Method. (arXiv:2306.04118v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04118
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;M$^3$Fair&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22810;&#20010;&#25968;&#25454;&#23618;&#38754;&#21644;&#22810;&#20010;&#25935;&#24863;&#23646;&#24615;&#30340;&#24773;&#20917;&#19979;&#24179;&#34913;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#65292;&#32531;&#35299;&#21307;&#30103;&#25968;&#25454;&#20013;&#30340;&#20559;&#35265;&#21644;&#19981;&#20844;&#24179;&#29616;&#35937;&#65292;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#33021;&#21644;&#20934;&#30830;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#25454;&#39537;&#21160;&#30340;&#20154;&#24037;&#26234;&#33021;&#33539;&#24335;&#20013;&#65292;&#27169;&#22411;&#20005;&#37325;&#20381;&#36182;&#20110;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#26679;&#26412;&#20998;&#24067;&#22833;&#34913;&#31561;&#22240;&#32032;&#21487;&#33021;&#23548;&#33268;&#21307;&#30103;&#25968;&#25454;&#20013;&#30340;&#20559;&#35265;&#21644;&#19981;&#20844;&#24179;&#38382;&#39064;&#12290;&#22312;&#21307;&#30103;&#20154;&#24037;&#26234;&#33021;&#20013;&#65292;&#22914;&#31181;&#26063;&#12289;&#24615;&#21035;&#12289;&#24180;&#40836;&#21644;&#21307;&#30103;&#29366;&#20917;&#31561;&#25935;&#24863;&#23646;&#24615;&#36890;&#24120;&#19982;&#27495;&#35270;&#25110;&#20559;&#35265;&#26377;&#20851;&#12290;&#36825;&#20123;&#23646;&#24615;&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#21487;&#33021;&#23545;&#20010;&#20154;&#33719;&#24471;&#30340;&#25252;&#29702;&#36136;&#37327;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#26816;&#27979;&#21644;&#20943;&#36731;&#25968;&#25454;&#20013;&#30340;&#20559;&#24046;&#23545;&#20110;&#25552;&#39640;&#20581;&#24247;&#20844;&#24179;&#33267;&#20851;&#37325;&#35201;&#12290;&#20559;&#24046;&#32531;&#35299;&#26041;&#27861;&#21253;&#25324;&#21069;&#22788;&#29702;&#12289;&#20013;&#22788;&#29702;&#21644;&#21518;&#22788;&#29702;&#12290;&#20854;&#20013;&#65292;&#21152;&#26435;&#65288;RW&#65289;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#21069;&#22788;&#29702;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#24179;&#34913;&#26426;&#22120;&#23398;&#20064;&#24615;&#33021;&#21644;&#20844;&#24179;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#12290;RW&#35843;&#25972;&#29305;&#23450;&#25935;&#24863;&#23646;&#24615;&#30340;&#26679;&#26412;&#26435;&#37325;&#20197;&#35299;&#20915;&#20559;&#24046;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;RW&#26041;&#27861;&#22312;&#24179;&#34913;&#22810;&#20010;&#25935;&#24863;&#23646;&#24615;&#21644;&#22788;&#29702;&#22810;&#23618;&#27425;&#25968;&#25454;&#26041;&#38754;&#25928;&#26524;&#26377;&#38480;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102; M$^3$Fair&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#23618;&#27425;&#22810;&#25935;&#24863;&#23646;&#24615;&#21152;&#26435;&#26041;&#27861;&#65292;&#21487;&#20197;&#24179;&#34913;&#19981;&#21516;&#25968;&#25454;&#23618;&#27425;&#19978;&#22810;&#20010;&#25935;&#24863;&#23646;&#24615;&#30340;&#20998;&#24067;&#12290;M$^3$Fair&#22312;&#32500;&#25345;&#26426;&#22120;&#23398;&#20064;&#24615;&#33021;&#30340;&#21516;&#26102;&#26377;&#25928;&#24179;&#34913;&#20102;&#19981;&#21516;&#25935;&#24863;&#23646;&#24615;&#30340;&#20844;&#24179;&#24615;&#33021;&#12290;&#22312;&#22522;&#20934;&#21307;&#30103;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;M$^3$Fair&#22312;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the data-driven artificial intelligence paradigm, models heavily rely on large amounts of training data. However, factors like sampling distribution imbalance can lead to issues of bias and unfairness in healthcare data. Sensitive attributes, such as race, gender, age, and medical condition, are characteristics of individuals that are commonly associated with discrimination or bias. In healthcare AI, these attributes can play a significant role in determining the quality of care that individuals receive. For example, minority groups often receive fewer procedures and poorer-quality medical care than white individuals in US. Therefore, detecting and mitigating bias in data is crucial to enhancing health equity. Bias mitigation methods include pre-processing, in-processing, and post-processing. Among them, Reweighting (RW) is a widely used pre-processing method that performs well in balancing machine learning performance and fairness performance. RW adjusts the weights for samples wit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#30340;&#28040;&#24687;&#20256;&#36882;&#26041;&#27861;&#65292;&#31216;&#20026;BeMap&#65292;&#26088;&#22312;&#35299;&#20915;&#28040;&#24687;&#20256;&#36882;&#20013;&#30340;&#20559;&#24046;&#25918;&#22823;&#38382;&#39064;&#65292;&#36890;&#36807;&#24179;&#34913;&#24863;&#30693;&#30340;&#37319;&#26679;&#31574;&#30053;&#26469;&#24179;&#34913;&#19981;&#21516;&#20154;&#21475;&#32676;&#20307;&#30340;1-hop&#37051;&#23621;&#30340;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.04107</link><description>&lt;p&gt;
BeMap&#65306;&#24179;&#34913;&#30340;&#28040;&#24687;&#20256;&#36882;&#26041;&#27861;&#29992;&#20110;&#20844;&#24179;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
BeMap: Balanced Message Passing for Fair Graph Neural Network. (arXiv:2306.04107v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04107
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#30340;&#28040;&#24687;&#20256;&#36882;&#26041;&#27861;&#65292;&#31216;&#20026;BeMap&#65292;&#26088;&#22312;&#35299;&#20915;&#28040;&#24687;&#20256;&#36882;&#20013;&#30340;&#20559;&#24046;&#25918;&#22823;&#38382;&#39064;&#65292;&#36890;&#36807;&#24179;&#34913;&#24863;&#30693;&#30340;&#37319;&#26679;&#31574;&#30053;&#26469;&#24179;&#34913;&#19981;&#21516;&#20154;&#21475;&#32676;&#20307;&#30340;1-hop&#37051;&#23621;&#30340;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#36890;&#36807;&#36845;&#20195;&#22320;&#32858;&#21512;&#27599;&#20010;&#33410;&#28857;&#30340;&#23616;&#37096;&#37051;&#22495;&#20449;&#24687;&#26469;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#23454;&#35777;&#24615;&#33021;&#65292;&#21363;&#28040;&#24687;&#20256;&#36882;&#12290;&#28982;&#32780;&#65292;&#20855;&#20307;&#35777;&#25454;&#26174;&#31034;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#21487;&#33021;&#23545;&#26576;&#20123;&#20154;&#21475;&#32676;&#20307;&#23384;&#22312;&#20559;&#35265;&#65292;&#36825;&#35201;&#27714;&#32771;&#34385;&#31639;&#27861;&#30340;&#20844;&#27491;&#24615;&#12290;&#23613;&#31649;&#36234;&#26469;&#36234;&#22810;&#30340;&#21162;&#21147;&#22312;&#20445;&#35777;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#31639;&#27861;&#20844;&#24179;&#24615;&#65292;&#20294;&#22312;&#35757;&#32451;&#26399;&#38388;&#24448;&#24448;&#24182;&#19981;&#26126;&#30830;&#32771;&#34385;&#28040;&#24687;&#20256;&#36882;&#22312;GNN&#20013;&#24341;&#36215;&#30340;&#20559;&#24046;&#12290;&#26412;&#25991;&#39318;&#20808;&#30740;&#31350;&#20102;&#28040;&#24687;&#20256;&#36882;&#20013;&#30340;&#20559;&#24046;&#25918;&#22823;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#32463;&#39564;&#35777;&#25454;&#21644;&#29702;&#35770;&#35777;&#26126;&#65292;&#24403;&#26469;&#33258;&#19981;&#21516;&#20154;&#21475;&#32676;&#20307;&#30340;1-hop&#37051;&#23621;&#19981;&#24179;&#34913;&#26102;&#65292;&#28040;&#24687;&#20256;&#36882;&#21487;&#33021;&#20250;&#25918;&#22823;&#20559;&#24046;&#12290;&#22312;&#36825;&#20123;&#20998;&#26512;&#30340;&#25351;&#23548;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BeMap&#65292;&#19968;&#31181;&#20844;&#24179;&#30340;&#28040;&#24687;&#20256;&#36882;&#26041;&#27861;&#65292;&#21033;&#29992;&#24179;&#34913;&#24863;&#30693;&#30340;&#37319;&#26679;&#31574;&#30053;&#26469;&#24179;&#34913;&#27599;&#20010;&#33410;&#28857;&#30340;1-hop&#37051;&#23621;&#30340;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Network (GNN) has shown strong empirical performance in many downstream tasks by iteratively aggregating information from the local neighborhood of each node, i.e., message passing. However, concrete evidence has revealed that a graph neural network could be biased against certain demographic groups, which calls for the consideration of algorithmic fairness. Despite the increasing efforts in ensuring algorithmic fairness on graph neural networks, they often do not explicitly consider the induced bias caused by message passing in GNN during training. In this paper, we first investigate the problem of bias amplification in message passing. We empirically and theoretically demonstrate that message passing could amplify the bias when the 1-hop neighbors from different demographic groups are unbalanced. Guided by such analyses, we propose BeMap, a fair message passing method, that leverages a balance-aware sampling strategy to balance the number of the 1-hop neighbors of each n
&lt;/p&gt;</description></item><item><title>Gotta&#26159;&#19968;&#20010;&#22522;&#20110;&#22635;&#31354;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#25552;&#31034;&#24335;&#22635;&#31354;&#20219;&#21153;&#19982;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#30456;&#32467;&#21512;&#65292;&#25552;&#39640;&#20102;&#23569;&#26679;&#26412;&#38382;&#39064;&#22238;&#31572;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.04101</link><description>&lt;p&gt;
Gotta: &#22522;&#20110;&#25552;&#31034;&#24335;&#22635;&#31354;&#25968;&#25454;&#22686;&#24378;&#30340;&#29983;&#25104;&#24335;&#23569;&#26679;&#26412;&#38382;&#39064;&#22238;&#31572;
&lt;/p&gt;
&lt;p&gt;
Gotta: Generative Few-shot Question Answering by Prompt-based Cloze Data Augmentation. (arXiv:2306.04101v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04101
&lt;/p&gt;
&lt;p&gt;
Gotta&#26159;&#19968;&#20010;&#22522;&#20110;&#22635;&#31354;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#25552;&#31034;&#24335;&#22635;&#31354;&#20219;&#21153;&#19982;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#30456;&#32467;&#21512;&#65292;&#25552;&#39640;&#20102;&#23569;&#26679;&#26412;&#38382;&#39064;&#22238;&#31572;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23569;&#26679;&#26412;&#38382;&#39064;&#22238;&#31572;&#26088;&#22312;&#20165;&#20351;&#29992;&#23569;&#37327;&#35757;&#32451;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#20174;&#19978;&#19979;&#25991;&#27573;&#33853;&#20013;&#31934;&#30830;&#22320;&#25214;&#21040;&#19968;&#32452;&#38382;&#39064;&#30340;&#31572;&#26696;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#30740;&#31350;&#24050;&#32463;&#21462;&#24471;&#20102;&#19968;&#23450;&#30340;&#36827;&#23637;&#24182;&#19988;&#36890;&#24120;&#33021;&#22815;&#21462;&#24471;&#33391;&#22909;&#30340;&#32467;&#26524;&#65292;&#20294;&#26159;&#23427;&#20204;&#22312;&#29702;&#35299;&#28145;&#23618;&#35821;&#20041;&#30340;&#33021;&#21147;&#26041;&#38754;&#20173;&#28982;&#23384;&#22312;&#22256;&#38590;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Gotta&#65292;&#19968;&#20010;&#22522;&#20110;&#29983;&#25104;&#24335;&#25552;&#31034;&#24335;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;&#65292;&#20197;&#32531;&#35299;&#19978;&#36848;&#25361;&#25112;&#12290;&#21463;&#21040;&#20154;&#31867;&#25512;&#29702;&#36807;&#31243;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;&#25552;&#31034;&#24335;&#22635;&#31354;&#20219;&#21153;&#19982;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#30456;&#32467;&#21512;&#26469;&#22686;&#24378;&#23569;&#26679;&#26412;&#38382;&#39064;&#22238;&#31572;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;&#22312;&#26368;&#36817;&#25552;&#31034;&#35843;&#25972;&#30340;&#25104;&#21151;&#20043;&#21518;&#65292;&#25105;&#20204;&#20197;&#19982;&#20027;&#35201;&#30340;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#30456;&#21516;&#30340;&#26684;&#24335;&#25552;&#20986;&#20102;&#22635;&#31354;&#20219;&#21153;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#26080;&#32541;&#22320;&#23398;&#20064;&#20004;&#20010;&#20219;&#21153;&#65292;&#20805;&#20998;&#21033;&#29992;&#25552;&#31034;&#35843;&#25972;&#30340;&#33021;&#21147;&#12290;&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;Gotta&#22987;&#32456;&#20248;&#20110;&#31454;&#20105;&#22522;&#32447;&#65292;&#39564;&#35777;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#22522;&#20110;&#25552;&#31034;&#35843;&#25972;&#30340;&#22635;&#31354;&#20219;&#21153;&#30340;&#26377;&#25928;&#24615;&#65292;&#23427;&#19981;&#20165;&#25552;&#39640;&#20102;&#24615;&#33021;&#36824;&#22686;&#24378;&#20102;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few-shot question answering (QA) aims at precisely discovering answers to a set of questions from context passages while only a few training samples are available. Although existing studies have made some progress and can usually achieve proper results, they suffer from understanding deep semantics for reasoning out the questions. In this paper, we develop Gotta, a Generative prOmpT-based daTa Augmentation framework to mitigate the challenge above. Inspired by the human reasoning process, we propose to integrate the cloze task to enhance few-shot QA learning. Following the recent success of prompt-tuning, we present the cloze task in the same format as the main QA task, allowing the model to learn both tasks seamlessly together to fully take advantage of the power of prompt-tuning. Extensive experiments on widely used benchmarks demonstrate that Gotta consistently outperforms competitive baselines, validating the effectiveness of our proposed prompt-tuning-based cloze task, which not o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20013;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PLAYBEST&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#20174;&#31726;&#29699;&#27604;&#36187;&#21382;&#21490;&#25968;&#25454;&#20013;&#23398;&#20064;&#31574;&#30053;&#65292;&#29983;&#25104;&#26356;&#21152;&#30495;&#23454;&#21644;&#22810;&#26679;&#21270;&#30340;&#29699;&#21592;&#34892;&#20026;&#65292;&#20174;&#32780;&#25552;&#39640;&#29699;&#21592;&#30340;&#20915;&#31574;&#21046;&#23450;&#25928;&#26524;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2306.04090</link><description>&lt;p&gt;
&#36890;&#36807;&#25193;&#25955;&#35268;&#21010;&#36827;&#34892;&#32844;&#19994;&#31726;&#29699;&#36816;&#21160;&#21592;&#34892;&#20026;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Professional Basketball Player Behavior Synthesis via Planning with Diffusion. (arXiv:2306.04090v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04090
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20013;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PLAYBEST&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#20174;&#31726;&#29699;&#27604;&#36187;&#21382;&#21490;&#25968;&#25454;&#20013;&#23398;&#20064;&#31574;&#30053;&#65292;&#29983;&#25104;&#26356;&#21152;&#30495;&#23454;&#21644;&#22810;&#26679;&#21270;&#30340;&#29699;&#21592;&#34892;&#20026;&#65292;&#20174;&#32780;&#25552;&#39640;&#29699;&#21592;&#30340;&#20915;&#31574;&#21046;&#23450;&#25928;&#26524;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#21160;&#24577;&#35268;&#21010;&#24050;&#32463;&#34987;&#24212;&#29992;&#20110;&#25913;&#21892;&#21508;&#31181;&#39046;&#22495;&#30340;&#20915;&#31574;&#21046;&#23450;&#12290;&#32844;&#19994;&#31726;&#29699;&#20316;&#20026;&#19968;&#20010;&#21253;&#21547;&#38544;&#34109;&#24615;&#25112;&#30053;&#31574;&#30053;&#21644;&#20915;&#31574;&#21046;&#23450;&#30340;&#21160;&#24577;&#26102;&#31354;&#21338;&#24328;&#30340;&#24341;&#20154;&#27880;&#30446;&#30340;&#20363;&#23376;&#12290;&#28982;&#32780;&#65292;&#22788;&#29702;&#22810;&#26679;&#30340;&#22330;&#19978;&#20449;&#21495;&#21644;&#23548;&#33322;&#28508;&#22312;&#21160;&#20316;&#21644;&#32467;&#26524;&#30340;&#24191;&#38420;&#31354;&#38388;&#20351;&#24471;&#29616;&#26377;&#26041;&#27861;&#24456;&#38590;&#36805;&#36895;&#35782;&#21035;&#21709;&#24212;&#19981;&#26029;&#21464;&#21270;&#30340;&#24773;&#20917;&#19979;&#30340;&#26368;&#20339;&#31574;&#30053;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#24207;&#21015;&#20915;&#31574;&#21046;&#23450;&#36807;&#31243;&#23450;&#20041;&#20026;&#26465;&#20214;&#36712;&#36857;&#29983;&#25104;&#36807;&#31243;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24341;&#20837;PLAYBEST&#65288;PLAYER BEhavior SynThesis&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#25552;&#39640;&#29699;&#21592;&#20915;&#31574;&#21046;&#23450;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;&#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#27169;&#22411;&#8212;&#8212;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65292;&#20197;&#20174;&#21382;&#21490;&#30340;&#32654;&#22269;&#32844;&#19994;&#31726;&#29699;&#32852;&#36187;(NBA)&#29699;&#21592;&#36816;&#21160;&#36319;&#36394;&#25968;&#25454;&#20013;&#23398;&#20064;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#21160;&#24577;&#12290;&#20026;&#20102;&#34701;&#21512;&#25968;&#25454;&#39537;&#21160;&#30340;&#31574;&#30053;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#36741;&#21161;&#21464;&#37327;&#21040;PLAYBEST&#20013;&#65292;&#20197;&#36866;&#24212;&#22806;&#37096;&#36755;&#20837;&#24182;&#29983;&#25104;&#30495;&#23454;&#21644;&#22810;&#26679;&#21270;&#30340;&#29699;&#21592;&#36712;&#36857;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;PLAYBEST&#21487;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#29699;&#21592;&#34892;&#20026;&#65292;&#24182;&#22312;&#21508;&#31181;&#35780;&#20272;&#22330;&#26223;&#20013;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamically planning in multi-agent systems has been explored to improve decision-making in various domains. Professional basketball serves as a compelling example of a dynamic spatio-temporal game, encompassing both concealed strategic policies and decision-making. However, processing the diverse on-court signals and navigating the vast space of potential actions and outcomes makes it difficult for existing approaches to swiftly identify optimal strategies in response to evolving circumstances. In this study, we first formulate the sequential decision-making process as a conditional trajectory generation process. We further introduce PLAYBEST (PLAYer BEhavior SynThesis), a method for enhancing player decision-making. We extend the state-of-the-art generative model, diffusion probabilistic model, to learn challenging multi-agent environmental dynamics from historical National Basketball Association (NBA) player motion tracking data. To incorporate data-driven strategies, an auxiliary v
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24378;&#20581;&#30340;&#20559;&#22909;&#35843;&#26597;&#31639;&#27861;&#65292;&#26088;&#22312;&#20248;&#20808;&#20026;COVID-19&#24739;&#32773;&#25552;&#20379;&#21307;&#38498;&#36164;&#28304;&#65292;&#36890;&#36807;&#25307;&#21215;&#20122;&#39532;&#36874;&#26426;&#26800;&#22303;&#32819;&#20854;&#24037;&#20316;&#32773;&#23545;&#35813;&#31639;&#27861;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2306.04061</link><description>&lt;p&gt;
&#37096;&#32626;&#19968;&#20010;&#24378;&#20581;&#30340;&#31215;&#26497;&#20559;&#22909;&#35843;&#26597;&#31639;&#27861;&#65306;COVID-19&#24739;&#32773;&#20248;&#20808;&#25490;&#24207;&#30340;&#23454;&#39564;&#35774;&#35745;&#12289;&#30028;&#38754;&#21644;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Deploying a Robust Active Preference Elicitation Algorithm: Experiment Design, Interface, and Evaluation for COVID-19 Patient Prioritization. (arXiv:2306.04061v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04061
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24378;&#20581;&#30340;&#20559;&#22909;&#35843;&#26597;&#31639;&#27861;&#65292;&#26088;&#22312;&#20248;&#20808;&#20026;COVID-19&#24739;&#32773;&#25552;&#20379;&#21307;&#38498;&#36164;&#28304;&#65292;&#36890;&#36807;&#25307;&#21215;&#20122;&#39532;&#36874;&#26426;&#26800;&#22303;&#32819;&#20854;&#24037;&#20316;&#32773;&#23545;&#35813;&#31639;&#27861;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20559;&#22909;&#35843;&#26597;&#21033;&#29992;AI&#25110;&#20248;&#21270;&#26469;&#23398;&#20064;&#20174;&#24066;&#22330;&#21040;&#20844;&#20849;&#25919;&#31574;&#31561;&#21508;&#31181;&#24773;&#20917;&#19979;&#30340;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#20559;&#22909;&#12290;arXiv:2003.01899&#30340;&#22312;&#32447;&#20581;&#22766;&#20559;&#22909;&#35843;&#26597;&#27969;&#31243;&#22312;&#27169;&#25311;&#20013;&#34920;&#29616;&#20986;&#20248;&#20110;&#20854;&#20182;&#20559;&#22909;&#35843;&#26597;&#27969;&#31243;&#30340;&#25928;&#26524;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#20010;&#20307;&#30340;&#30495;&#23454;&#25928;&#29992;&#12290;&#28982;&#32780;&#65292;&#19982;&#20219;&#20309;&#27169;&#25311;&#19968;&#26679;&#65292;&#35813;&#26041;&#27861;&#20570;&#20986;&#20102;&#19968;&#31995;&#21015;&#26080;&#27861;&#36731;&#26131;&#39564;&#35777;&#26159;&#21542;&#22312;&#27169;&#25311;&#20197;&#22806;&#25104;&#31435;&#30340;&#20551;&#35774;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24314;&#35758;&#39564;&#35777;&#36825;&#31181;&#20581;&#22766;&#26041;&#27861;&#22312;&#37096;&#32626;&#20013;&#30340;&#34920;&#29616;&#65292;&#37325;&#28857;&#20851;&#27880;&#36873;&#25321;COVID-19&#24739;&#32773;&#30340;&#25919;&#31574;&#65292;&#20197;&#20248;&#20808;&#20026;&#30123;&#24773;&#26399;&#38388;&#30340;&#21307;&#38498;&#36164;&#28304;&#25552;&#20379;&#26381;&#21153;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22312;&#32447;&#20559;&#22909;&#35843;&#26597;&#24179;&#21488;&#65292;&#29992;&#25143;&#36890;&#36807;&#29305;&#23450;&#30340;&#20559;&#22909;&#35843;&#26597;&#36807;&#31243;&#25253;&#21578;&#20182;&#20204;&#22312;&#23569;&#37327;&#25104;&#23545;&#27604;&#36739;&#20013;&#30340;&#20559;&#22909;&#12290;&#25105;&#20204;&#25307;&#21215;&#20102;193&#20301;Amazon Mechanical Turk&#24037;&#20316;&#32773;&#26469;&#25253;&#21578;&#20182;&#20204;&#30340;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Preference elicitation leverages AI or optimization to learn stakeholder preferences in settings ranging from marketing to public policy. The online robust preference elicitation procedure of arXiv:2003.01899 has been shown in simulation to outperform various other elicitation procedures in terms of effectively learning individuals' true utilities. However, as with any simulation, the method makes a series of assumptions that cannot easily be verified to hold true beyond simulation. Thus, we propose to validate the robust method's performance in deployment, focused on the particular challenge of selecting policies for prioritizing COVID-19 patients for scarce hospital resources during the pandemic. To this end, we develop an online platform for preference elicitation where users report their preferences between alternatives over a moderate number of pairwise comparisons chosen by a particular elicitation procedure. We recruit Amazon Mechanical Turk workers ($n$ = 193) to report their p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;CAVEN - &#19968;&#31181;&#20855;&#26377;&#23545;&#35805;&#21151;&#33021;&#30340;&#38899;&#39057;&#35270;&#35273;&#23548;&#33322;&#20195;&#29702;&#65292;&#33021;&#22815;&#21521;&#20154;&#31867;/&#31070;&#35861;&#25552;&#20986;&#23548;&#33322;&#38382;&#39064;&#24182;&#22788;&#29702;&#31070;&#35861;&#22238;&#31572;&#20197;&#21327;&#21161;&#33258;&#20027;&#23548;&#33322;&#12290;&#35813;&#31995;&#32479;&#22522;&#20110;&#22810;&#27169;&#24577;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#19977;&#20010;&#20302;&#32423;&#31574;&#30053;&#36827;&#34892;&#24341;&#23548;&#12290;</title><link>http://arxiv.org/abs/2306.04047</link><description>&lt;p&gt;
&#29992;&#20110;&#25913;&#36827;&#35270;&#21548;&#34701;&#21512;&#23548;&#33322;&#30340;&#20027;&#21160;&#31232;&#30095;&#23545;&#35805;
&lt;/p&gt;
&lt;p&gt;
Active Sparse Conversations for Improved Audio-Visual Embodied Navigation. (arXiv:2306.04047v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04047
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;CAVEN - &#19968;&#31181;&#20855;&#26377;&#23545;&#35805;&#21151;&#33021;&#30340;&#38899;&#39057;&#35270;&#35273;&#23548;&#33322;&#20195;&#29702;&#65292;&#33021;&#22815;&#21521;&#20154;&#31867;/&#31070;&#35861;&#25552;&#20986;&#23548;&#33322;&#38382;&#39064;&#24182;&#22788;&#29702;&#31070;&#35861;&#22238;&#31572;&#20197;&#21327;&#21161;&#33258;&#20027;&#23548;&#33322;&#12290;&#35813;&#31995;&#32479;&#22522;&#20110;&#22810;&#27169;&#24577;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#19977;&#20010;&#20302;&#32423;&#31574;&#30053;&#36827;&#34892;&#24341;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#39640;&#25928;&#22320;&#23548;&#33322;&#21040;&#19968;&#20010;&#21548;&#35273;&#30446;&#26631;&#65292;&#19968;&#20010;&#20855;&#26377;&#22266;&#23450;&#33258;&#20027;&#26435;&#30340;&#23454;&#20307;&#24517;&#39035;&#19981;&#20165;&#35201;&#26377;&#33021;&#21147;&#26377;&#25928;&#22320;&#20351;&#29992;&#35270;&#21548;&#32447;&#32034;, &#32780;&#19988;&#36824;&#35201;&#26377;&#33021;&#21147;&#22312;&#19981;&#29306;&#29298;&#33258;&#20027;&#24615;&#30340;&#24773;&#20917;&#19979;&#20027;&#21160;&#23547;&#27714;&#20154;&#31867;/&#31070;&#35861;&#30340;&#24110;&#21161;&#65292;&#20363;&#22914;&#65292;&#24403;&#19981;&#30830;&#23450;&#23548;&#33322;&#21040;&#21738;&#37324;&#23547;&#25214;&#22024;&#26434;&#25110;&#38388;&#27463;&#24615;&#21548;&#35273;&#30446;&#26631;&#26102;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CAVEN-&#19968;&#31181;&#20855;&#26377;&#23545;&#35805;&#21151;&#33021;&#30340;&#38899;&#39057;&#35270;&#35273;&#23548;&#33322;&#20195;&#29702;&#65292;&#33021;&#22815;&#21521;&#20154;&#31867;/&#31070;&#35861;&#25552;&#20986;&#23548;&#33322;&#38382;&#39064;&#24182;&#22788;&#29702;&#31070;&#35861;&#30340;&#33258;&#30001;&#24418;&#24335;&#33258;&#28982;&#35821;&#35328;&#22238;&#31572;&#12290;&#22312;CAVEN&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;(RL)&#35774;&#32622;&#65292;&#23427;&#37197;&#22791;&#20102;&#19968;&#20010;&#39640;&#32423;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#32463;&#36807;&#35757;&#32451;&#65292;&#21487;&#20197;&#22312;&#27599;&#19968;&#27493;&#20174;&#19977;&#20010;&#20302;&#32423;&#31574;&#30053;&#20013;&#36873;&#25321;&#19968;&#20010;&#65292;&#21363;&#65306;(i)&#20351;&#29992;&#35270;&#21548;&#32447;&#32034;&#36827;&#34892;&#23548;&#33322;&#65292;&#25110;(ii)&#21521;&#31070;&#35861;&#25552;&#20986;&#38382;&#39064;&#24182;&#25509;&#25910;&#30701;&#25110;&#35814;&#32454;&#30340;&#22238;&#31572;&#65292;&#25110;(iii)&#25552;&#38382;&#26222;&#36941;&#38382;&#39064;(&#24403;&#19981;&#30830;&#23450;&#35813;&#38382;&#20160;&#20040;&#26102;)&#24182;&#33719;&#24471;&#25351;&#23548;
&lt;/p&gt;
&lt;p&gt;
Efficient navigation towards an audio-goal necessitates an embodied agent to not only possess the ability to use audio-visual cues effectively, but also be equipped to actively (but occasionally) seek human/oracle assistance without sacrificing autonomy, e.g., when it is uncertain of where to navigate towards locating a noisy or sporadic audio goal. To this end, we present CAVEN -- a conversational audio-visual embodied navigation agent that is capable of posing navigation questions to a human/oracle and processing the oracle responses; both in free-form natural language. At the core of CAVEN is a multimodal hierarchical reinforcement learning (RL) setup that is equipped with a high-level policy that is trained to choose from one of three low-level policies (at every step), namely: (i) to navigate using audio-visual cues, or (ii) to frame a question to the oracle and receive a short or detailed response, or (iii) ask generic questions (when unsure of what to ask) and receive instructio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;FedVal&#26041;&#27861;&#65292;&#23427;&#26159;&#19968;&#20010;&#19981;&#38656;&#35201;&#20174;&#23458;&#25143;&#31471;&#33719;&#21462;&#20219;&#20309;&#38468;&#21152;&#20449;&#24687;&#30340;&#20840;&#26032;&#26041;&#27861;&#65292;&#21487;&#21516;&#26102;&#20855;&#26377;&#31283;&#20581;&#21644;&#20844;&#24179;&#24615;&#65292;&#24182;&#36890;&#36807;&#35780;&#20998;&#20989;&#25968;&#22312;&#26381;&#21153;&#22120;&#31471;&#39564;&#35777;&#23458;&#25143;&#31471;&#26356;&#26032;&#65292;&#20197;&#30830;&#23450;&#26412;&#22320;&#35757;&#32451;&#27169;&#22411;&#20043;&#38388;&#30340;&#26368;&#20339;&#32858;&#21512;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2306.04040</link><description>&lt;p&gt;
FedVal&#65306;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#19981;&#21516;&#22909;&#22351;
&lt;/p&gt;
&lt;p&gt;
FedVal: Different good or different bad in federated learning. (arXiv:2306.04040v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04040
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;FedVal&#26041;&#27861;&#65292;&#23427;&#26159;&#19968;&#20010;&#19981;&#38656;&#35201;&#20174;&#23458;&#25143;&#31471;&#33719;&#21462;&#20219;&#20309;&#38468;&#21152;&#20449;&#24687;&#30340;&#20840;&#26032;&#26041;&#27861;&#65292;&#21487;&#21516;&#26102;&#20855;&#26377;&#31283;&#20581;&#21644;&#20844;&#24179;&#24615;&#65292;&#24182;&#36890;&#36807;&#35780;&#20998;&#20989;&#25968;&#22312;&#26381;&#21153;&#22120;&#31471;&#39564;&#35777;&#23458;&#25143;&#31471;&#26356;&#26032;&#65292;&#20197;&#30830;&#23450;&#26412;&#22320;&#35757;&#32451;&#27169;&#22411;&#20043;&#38388;&#30340;&#26368;&#20339;&#32858;&#21512;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#23481;&#26131;&#21463;&#21040;&#24694;&#24847;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#25915;&#20987;&#32773;&#21487;&#33021;&#20250;&#36890;&#36807;&#21508;&#31181;&#27602;&#21270;&#25915;&#20987;&#26469;&#30772;&#22351;&#35757;&#32451;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;FL&#22312;&#35299;&#20915;&#22242;&#20307;&#20559;&#35265;&#26041;&#38754;&#20063;&#38754;&#20020;&#26032;&#30340;&#25361;&#25112;&#65292;&#20363;&#22914;&#30830;&#20445;&#19981;&#21516;&#20154;&#21475;&#32676;&#20307;&#30340;&#20844;&#24179;&#24615;&#33021;&#12290;&#20256;&#32479;&#26041;&#27861;&#38656;&#35201;&#23545;&#25968;&#25454;&#36827;&#34892;&#38598;&#20013;&#22788;&#29702;&#65292;&#32780;FL&#31995;&#32479;&#24182;&#27809;&#26377;&#36825;&#20010;&#21151;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;FedVal&#30340;&#20840;&#26032;&#26041;&#27861;&#65292;&#26082;&#20855;&#26377;&#31283;&#20581;&#24615;&#21448;&#20855;&#26377;&#20844;&#24179;&#24615;&#65292;&#20854;&#19981;&#38656;&#35201;&#20174;&#23458;&#25143;&#31471;&#33719;&#21462;&#20219;&#20309;&#21487;&#33021;&#24341;&#21457;&#38544;&#31169;&#38382;&#39064;&#24182;&#21361;&#21450;FL&#31995;&#32479;&#23436;&#25972;&#24615;&#30340;&#38468;&#21152;&#20449;&#24687;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26381;&#21153;&#22120;&#31471;&#39564;&#35777;&#26041;&#27861;&#30340;&#21019;&#26032;&#35780;&#20998;&#20989;&#25968;&#65292;&#35813;&#26041;&#27861;&#35780;&#20272;&#23458;&#25143;&#31471;&#26356;&#26032;&#24182;&#30830;&#23450;&#26412;&#22320;&#35757;&#32451;&#27169;&#22411;&#20043;&#38388;&#30340;&#26368;&#20339;&#32858;&#21512;&#24179;&#34913;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#33021;&#22815;&#26377;&#25928;&#20445;&#25252;&#27169;&#22411;&#20813;&#21463;&#27602;&#21270;&#25915;&#20987;&#65292;&#32780;&#19988;&#36824;&#21487;&#29992;&#20110;&#20943;&#23569;&#32676;&#20307;&#20559;&#35265;&#21644;&#38543;&#21518;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) systems are susceptible to attacks from malicious actors who might attempt to corrupt the training model through various poisoning attacks. FL also poses new challenges in addressing group bias, such as ensuring fair performance for different demographic groups. Traditional methods used to address such biases require centralized access to the data, which FL systems do not have. In this paper, we present a novel approach FedVal for both robustness and fairness that does not require any additional information from clients that could raise privacy concerns and consequently compromise the integrity of the FL system. To this end, we propose an innovative score function based on a server-side validation method that assesses client updates and determines the optimal aggregation balance between locally-trained models. Our research shows that this approach not only provides solid protection against poisoning attacks but can also be used to reduce group bias and subsequen
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23450;&#37327;&#20998;&#26512;&#20102;&#29992;&#20110;&#36965;&#24863;&#22270;&#20687;&#20998;&#31867;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#25506;&#31350;&#20102;&#19981;&#21516;&#23646;&#24615;&#30340;XAI&#26041;&#27861;&#65292;&#25552;&#20379;&#36873;&#21462;&#21512;&#36866;&#26041;&#27861;&#20197;&#28145;&#20837;&#20102;&#35299;&#27169;&#22411;&#20915;&#31574;&#30340;&#35265;&#35299;&#21644;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2306.04037</link><description>&lt;p&gt;
&#36965;&#24863;&#22270;&#20687;&#20998;&#31867;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#30340;&#20027;&#35201;&#36129;&#29486;&#30340;&#23450;&#37327;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Quantitative Analysis of Primary Attribution Explainable Artificial Intelligence Methods for Remote Sensing Image Classification. (arXiv:2306.04037v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04037
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23450;&#37327;&#20998;&#26512;&#20102;&#29992;&#20110;&#36965;&#24863;&#22270;&#20687;&#20998;&#31867;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#25506;&#31350;&#20102;&#19981;&#21516;&#23646;&#24615;&#30340;XAI&#26041;&#27861;&#65292;&#25552;&#20379;&#36873;&#21462;&#21512;&#36866;&#26041;&#27861;&#20197;&#28145;&#20837;&#20102;&#35299;&#27169;&#22411;&#20915;&#31574;&#30340;&#35265;&#35299;&#21644;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#20998;&#26512;&#23450;&#37327;&#35780;&#20272;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#25216;&#26415;&#29992;&#20110;&#36965;&#24863;&#22270;&#20687;&#20998;&#31867;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#22810;&#31181;&#27169;&#24577;&#19979;&#25191;&#34892;&#36965;&#24863;&#22270;&#20687;&#20998;&#31867;&#12290;&#25105;&#20204;&#36890;&#36807;XAI&#26041;&#27861;&#23450;&#24615;&#22320;&#30740;&#31350;&#20102;&#27169;&#22411;&#30340;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#25152;&#38656;&#23646;&#24615;&#30340;&#21508;&#31181;&#31867;&#21035;&#26469;&#23450;&#37327;&#27604;&#36739;XAI&#26041;&#27861;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;XAI&#26041;&#27861;&#20197;&#21152;&#28145;&#23545;&#27169;&#22411;&#20915;&#31574;&#36807;&#31243;&#29702;&#35299;&#30340;&#35265;&#35299;&#21644;&#24314;&#35758;&#12290;&#27492;&#24037;&#20316;&#30340;&#20195;&#30721;&#26159;&#20844;&#24320;&#21487;&#29992;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a comprehensive analysis of quantitatively evaluating explainable artificial intelligence (XAI) techniques for remote sensing image classification. Our approach leverages state-of-the-art machine learning approaches to perform remote sensing image classification across multiple modalities. We investigate the results of the models qualitatively through XAI methods. Additionally, we compare the XAI methods quantitatively through various categories of desired properties. Through our analysis, we offer insights and recommendations for selecting the most appropriate XAI method(s) to gain a deeper understanding of the models' decision-making processes. The code for this work is publicly available.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#25351;&#21335;&#8221;&#30340;&#35821;&#35328;&#27169;&#22411;&#24037;&#20855;&#31867;&#65292;&#23427;&#20351;&#29992;&#29366;&#24577;&#21644;&#22686;&#37327;&#32422;&#26463;&#26469;&#25351;&#23548;&#29983;&#25104;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.04031</link><description>&lt;p&gt;
&#24102;&#26377;&#8220;&#25351;&#21335;&#8221;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#21512;&#35268;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Certified Reasoning with Language Models. (arXiv:2306.04031v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04031
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#25351;&#21335;&#8221;&#30340;&#35821;&#35328;&#27169;&#22411;&#24037;&#20855;&#31867;&#65292;&#23427;&#20351;&#29992;&#29366;&#24577;&#21644;&#22686;&#37327;&#32422;&#26463;&#26469;&#25351;&#23548;&#29983;&#25104;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#65292;&#35821;&#35328;&#27169;&#22411;&#24448;&#24448;&#36890;&#36807;&#36880;&#27493;&#25512;&#29702;&#23454;&#29616;&#26356;&#39640;&#30340;&#31934;&#24230;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#25512;&#29702;&#21487;&#20197;&#26159;&#19981;&#23436;&#22791;&#30340;&#12289;&#19981;&#19968;&#33268;&#30340;&#25110;&#32773;&#20381;&#36182;&#20110;&#19981;&#33391;&#30340;&#20248;&#20808;&#20551;&#35774;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#25351;&#21335;&#8221;&#30340;&#35821;&#35328;&#27169;&#22411;&#24037;&#20855;&#31867;&#65292;&#21033;&#29992;&#29366;&#24577;&#21644;&#36882;&#22686;&#32422;&#26463;&#26469;&#25351;&#23548;&#29983;&#25104;&#12290;&#27169;&#22411;&#21487;&#20197;&#35843;&#29992;&#25351;&#21335;&#65292;&#23558;&#20854;&#33258;&#24049;&#30340;&#29983;&#25104;&#38480;&#21046;&#22312;&#19968;&#32452;&#24037;&#20855;&#32473;&#20986;&#30340;&#26377;&#25928;&#38472;&#36848;&#20043;&#20869;&#12290;&#21453;&#36807;&#26469;&#65292;&#27169;&#22411;&#30340;&#36873;&#25321;&#21487;&#20197;&#25913;&#21464;&#25351;&#21335;&#30340;&#29366;&#24577;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#31995;&#32479;&#26469;&#36827;&#34892;&#36923;&#36753;&#25512;&#29702;&#65292;&#21487;&#20197;&#34987;&#29992;&#20316;&#25351;&#21335;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;LogicGuide&#12290;&#32473;&#23450;&#33258;&#28982;&#35821;&#35328;&#30340;&#25512;&#29702;&#38382;&#39064;&#65292;&#27169;&#22411;&#21487;&#20197;&#20026;LogicGuide&#24418;&#24335;&#21270;&#23427;&#30340;&#20551;&#35774;&#65292;&#20174;&#32780;&#20445;&#35777;&#20854;&#25512;&#29702;&#27493;&#39588;&#26159;&#23436;&#22791;&#30340;&#12290;&#22312;PrOntoQA&#21644;ProofWriter&#25512;&#29702;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#20013;&#65292;LogicGuide&#26174;&#33879;&#25552;&#39640;&#20102;GPT-3&#12289;GPT-3.5 Turbo&#21644;LLaMA&#30340;&#24615;&#33021;&#65288;&#31934;&#24230;&#25552;&#39640;&#20102;35%&#65289;&#12290;LogicGuide&#20063;&#22823;&#22823;&#20943;&#23569;&#20102;&#20869;&#23481;&#25928;&#26524;&#30340;&#27874;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models often achieve higher accuracy when reasoning step-by-step in complex tasks. However, their reasoning can be unsound, inconsistent, or rely on undesirable prior assumptions. To tackle these issues, we introduce a class of tools for language models called guides that use state and incremental constraints to guide generation. A guide can be invoked by the model to constrain its own generation to a set of valid statements given by the tool. In turn, the model's choices can change the guide's state. We show how a general system for logical reasoning can be used as a guide, which we call LogicGuide. Given a reasoning problem in natural language, a model can formalize its assumptions for LogicGuide and then guarantee that its reasoning steps are sound. In experiments with the PrOntoQA and ProofWriter reasoning datasets, LogicGuide significantly improves the performance of GPT-3, GPT-3.5 Turbo and LLaMA (accuracy gains up to 35%). LogicGuide also drastically reduces content eff
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#23376;&#22270;&#27169;&#22411;&#30340;&#8220;&#24178;&#39044;&#22240;&#23376;&#27169;&#22411;&#8221;(IFM)&#26041;&#27861;&#65292;&#20165;&#22522;&#20110;&#23545;&#25805;&#32437;&#31995;&#32479;&#20998;&#24067;&#30340;&#22240;&#23376;&#20998;&#35299;&#30340;&#26368;&#23567;&#20551;&#35774;&#65292;&#20197;&#23454;&#29616;&#20174;&#36807;&#21435;&#30340;&#23454;&#39564;&#21040;&#26032;&#30340;&#26465;&#20214;&#30340;&#36291;&#36801;&#12290;</title><link>http://arxiv.org/abs/2306.04027</link><description>&lt;p&gt;
&#22240;&#23376;&#22270;&#27169;&#22411;&#35270;&#35282;&#19979;&#30340;&#24178;&#39044;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Intervention Generalization: A View from Factor Graph Models. (arXiv:2306.04027v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04027
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#23376;&#22270;&#27169;&#22411;&#30340;&#8220;&#24178;&#39044;&#22240;&#23376;&#27169;&#22411;&#8221;(IFM)&#26041;&#27861;&#65292;&#20165;&#22522;&#20110;&#23545;&#25805;&#32437;&#31995;&#32479;&#20998;&#24067;&#30340;&#22240;&#23376;&#20998;&#35299;&#30340;&#26368;&#23567;&#20551;&#35774;&#65292;&#20197;&#23454;&#29616;&#20174;&#36807;&#21435;&#30340;&#23454;&#39564;&#21040;&#26032;&#30340;&#26465;&#20214;&#30340;&#36291;&#36801;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#25512;&#26029;&#30340;&#19968;&#20010;&#30446;&#26631;&#26159;&#20174;&#36807;&#21435;&#30340;&#23454;&#39564;&#21644;&#35266;&#23519;&#25968;&#25454;&#25512;&#24191;&#21040;&#26032;&#30340;&#26465;&#20214;&#12290;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#25552;&#20379;&#36275;&#22815;&#22810;&#30340;&#23454;&#39564;&#30340;&#24773;&#20917;&#19979;&#65292;&#29702;&#35770;&#19978;&#21487;&#33021;&#26368;&#32456;&#23398;&#20064;&#20174;&#26032;&#30340;&#23454;&#39564;&#26465;&#20214;&#21040;&#24863;&#20852;&#36259;&#30340;&#32467;&#26524;&#30340;&#26144;&#23556;&#65292;&#20294;&#26159;&#22788;&#29702;&#22823;&#37327;&#21487;&#33021;&#30340;&#24178;&#39044;&#32452;&#21512;&#31354;&#38388;&#24456;&#22256;&#38590;&#12290;&#22312;&#20856;&#22411;&#30340;&#31232;&#30095;&#23454;&#39564;&#35774;&#35745;&#19979;&#65292;&#22914;&#26524;&#19981;&#20381;&#36182;&#20110;&#37325;&#30340;&#35268;&#21017;&#21270;&#25110;&#20808;&#39564;&#20998;&#24067;&#65292;&#36825;&#31181;&#26144;&#23556;&#26159;&#19981;&#36866;&#24403;&#30340;&#12290;&#36825;&#26679;&#30340;&#20551;&#35774;&#21487;&#33021;&#26159;&#21487;&#38752;&#30340;&#65292;&#20063;&#21487;&#33021;&#26159;&#19981;&#21487;&#38752;&#30340;&#65292;&#24456;&#38590;&#36777;&#25252;&#25110;&#27979;&#35797;&#12290;&#26412;&#25991;&#20174;&#22240;&#23376;&#22270;&#27169;&#22411;&#30340;&#35821;&#35328;&#35282;&#24230;&#28145;&#20837;&#25506;&#35752;&#22914;&#20309;&#20445;&#35777;&#20174;&#36807;&#21435;&#30340;&#23454;&#39564;&#21040;&#26032;&#30340;&#26465;&#20214;&#30340;&#36291;&#36801;&#65292;&#20165;&#22522;&#20110;&#23545;&#25805;&#32437;&#31995;&#32479;&#20998;&#24067;&#30340;&#22240;&#23376;&#20998;&#35299;&#30340;&#26368;&#23567;&#20551;&#35774;&#12290;&#20551;&#35774;&#30340;&#8220;&#24178;&#39044;&#22240;&#23376;&#27169;&#22411;&#8221;&#21487;&#33021;&#24182;&#19981;&#24635;&#26159;&#26377;&#29992;&#30340;&#65292;&#20294;&#26159;&#23427;&#24456;&#26041;&#20415;&#22320;&#22788;&#29702;&#20102;&#22823;&#37327;&#21487;&#33021;&#30340;&#24178;&#39044;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the goals of causal inference is to generalize from past experiments and observational data to novel conditions. While it is in principle possible to eventually learn a mapping from a novel experimental condition to an outcome of interest, provided a sufficient variety of experiments is available in the training data, coping with a large combinatorial space of possible interventions is hard. Under a typical sparse experimental design, this mapping is ill-posed without relying on heavy regularization or prior distributions. Such assumptions may or may not be reliable, and can be hard to defend or test. In this paper, we take a close look at how to warrant a leap from past experiments to novel conditions based on minimal assumptions about the factorization of the distribution of the manipulated system, communicated in the well-understood language of factor graph models. A postulated $\textit{interventional factor model}$ (IFM) may not always be informative, but it conveniently abs
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#25511;&#21046;&#29702;&#35770;&#20013;&#30340;&#39564;&#35777;&#26041;&#27861;&#24212;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20215;&#20540;&#20989;&#25968;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#20197;&#39564;&#35777;&#23433;&#20840;&#25511;&#21046;&#20219;&#21153;&#20013;&#30340;&#20215;&#20540;&#20989;&#25968;&#65292;&#24182;&#20195;&#34920;&#20102;&#36890;&#29992;&#12289;&#21487;&#20280;&#32553;&#21644;&#21487;&#39564;&#35777;&#30340;&#25511;&#21046;&#31995;&#32479;&#35774;&#35745;&#26694;&#26550;&#30340;&#31532;&#19968;&#27493;&#12290;</title><link>http://arxiv.org/abs/2306.04026</link><description>&lt;p&gt;
&#20215;&#20540;&#20989;&#25968;&#21363;&#25511;&#21046;&#38556;&#30861;&#20989;&#25968;&#65306;&#20351;&#29992;&#25511;&#21046;&#29702;&#35770;&#39564;&#35777;&#23398;&#20064;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Your Value Function is a Control Barrier Function: Verification of Learned Policies using Control Theory. (arXiv:2306.04026v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04026
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#25511;&#21046;&#29702;&#35770;&#20013;&#30340;&#39564;&#35777;&#26041;&#27861;&#24212;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20215;&#20540;&#20989;&#25968;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#20197;&#39564;&#35777;&#23433;&#20840;&#25511;&#21046;&#20219;&#21153;&#20013;&#30340;&#20215;&#20540;&#20989;&#25968;&#65292;&#24182;&#20195;&#34920;&#20102;&#36890;&#29992;&#12289;&#21487;&#20280;&#32553;&#21644;&#21487;&#39564;&#35777;&#30340;&#25511;&#21046;&#31995;&#32479;&#35774;&#35745;&#26694;&#26550;&#30340;&#31532;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24378;&#21270;&#23398;&#20064;&#20855;&#26377;&#39640;&#24230;&#30340;&#36890;&#29992;&#24615;&#21644;&#21487;&#20280;&#32553;&#24615;&#65292;&#20294;&#39564;&#35777;&#31574;&#30053;&#34892;&#20026;&#30340;&#38590;&#24230;&#23545;&#20110;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#31243;&#24207;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24314;&#35758;&#23558;&#25511;&#21046;&#29702;&#35770;&#20013;&#20351;&#29992;&#30340;&#39564;&#35777;&#26041;&#27861;&#24212;&#29992;&#20110;&#23398;&#20064;&#30340;&#20215;&#20540;&#20989;&#25968;&#12290;&#36890;&#36807;&#20998;&#26512;&#23433;&#20840;&#32500;&#25252;&#30340;&#31616;&#21333;&#20219;&#21153;&#32467;&#26500;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#23558;&#20540;&#20989;&#25968;&#19982;&#25511;&#21046;&#38556;&#30861;&#20989;&#25968;&#30456;&#32852;&#31995;&#30340;&#21407;&#22987;&#23450;&#29702;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#20197;&#39564;&#35777;&#23433;&#20840;&#25511;&#21046;&#20219;&#21153;&#20013;&#30340;&#20215;&#20540;&#20989;&#25968;&#65292;&#24182;&#25552;&#20986;&#20102;&#25913;&#21892;&#23398;&#20064;&#30340;&#23454;&#38469;&#23454;&#26045;&#32454;&#33410;&#12290;&#38500;&#20102;&#25552;&#20986;&#35777;&#20070;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#22806;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;RL&#31574;&#30053;&#35299;&#38145;&#20102;&#20016;&#23500;&#30340;&#25511;&#21046;&#29702;&#35770;&#39564;&#35777;&#26041;&#27861;&#65292;&#24182;&#20195;&#34920;&#20102;&#36890;&#29992;&#12289;&#21487;&#20280;&#32553;&#21644;&#21487;&#39564;&#35777;&#30340;&#25511;&#21046;&#31995;&#32479;&#35774;&#35745;&#26694;&#26550;&#30340;&#31532;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although RL is highly general and scalable, the difficulty of verifying policy behaviours poses challenges for safety-critical applications. To remedy this, we propose to apply verification methods used in control theory to learned value functions. By analyzing a simple task structure for safety preservation, we derive original theorems linking value functions to control barrier functions. Inspired by this, we propose novel metrics for verification of value functions in safe control tasks, and practical implementation details that improve learning. Besides proposing a novel method for certificate learning, our work unlocks a wealth of verification methods in control theory for RL policies, and represents a first step towards a framework for general, scalable, and verifiable design of control systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20027;&#21160;&#25512;&#29702;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#26550;&#26500;&#65292;&#21487;&#20197;&#36861;&#36394;&#20915;&#31574;&#30340;&#22240;&#32032;&#24182;&#29983;&#25104;&#26131;&#20110;&#29702;&#35299;&#30340;&#27169;&#22411;&#65292;&#20174;&#32780;&#23454;&#29616;&#36879;&#26126;&#21270;&#20869;&#30465;&#21644;&#20915;&#31574;&#21046;&#23450;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#30340;&#24605;&#36335;&#12290;</title><link>http://arxiv.org/abs/2306.04025</link><description>&lt;p&gt;
&#20351;&#29992;&#20027;&#21160;&#25512;&#29702;&#35774;&#35745;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65306;&#36879;&#26126;&#20869;&#30465;&#19982;&#20915;&#31574;&#21046;&#23450;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Designing explainable artificial intelligence with active inference: A framework for transparent introspection and decision-making. (arXiv:2306.04025v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04025
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20027;&#21160;&#25512;&#29702;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#26550;&#26500;&#65292;&#21487;&#20197;&#36861;&#36394;&#20915;&#31574;&#30340;&#22240;&#32032;&#24182;&#29983;&#25104;&#26131;&#20110;&#29702;&#35299;&#30340;&#27169;&#22411;&#65292;&#20174;&#32780;&#23454;&#29616;&#36879;&#26126;&#21270;&#20869;&#30465;&#21644;&#20915;&#31574;&#21046;&#23450;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#30340;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#20027;&#21160;&#25512;&#29702;&#21644;&#33258;&#30001;&#33021;&#21407;&#29702;&#21457;&#23637;&#20154;&#21487;&#35299;&#35835;&#30340;&#12289;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21069;&#26223;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20379;&#20102;&#26377;&#20851;&#20027;&#21160;&#25512;&#29702;&#30340;&#31616;&#35201;&#27010;&#36848;&#65292;&#29305;&#21035;&#26159;&#22914;&#20309;&#24212;&#29992;&#20110;&#20915;&#31574;&#21046;&#23450;&#12289;&#20869;&#30465;&#20197;&#21450;&#20135;&#29983;&#20844;&#24320;&#21644;&#38544;&#34109;&#25805;&#20316;&#30340;&#24314;&#27169;&#12290;&#28982;&#21518;&#25105;&#20204;&#35752;&#35770;&#20102;&#22914;&#20309;&#21033;&#29992;&#20027;&#21160;&#25512;&#29702;&#26469;&#35774;&#35745;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#21363;&#36890;&#36807;&#20801;&#35768;&#25105;&#20204;&#23545;&#8220;&#20869;&#30465;&#8221;&#36807;&#31243;&#30340;&#26680;&#24515;&#21151;&#33021;&#36827;&#34892;&#24314;&#27169;&#24182;&#29983;&#25104;&#26377;&#29992;&#30340;&#12289;&#21487;&#35299;&#35835;&#30340;&#20915;&#31574;&#21046;&#23450;&#36807;&#31243;&#30340;&#20154;&#21487;&#29702;&#35299;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#20027;&#21160;&#25512;&#29702;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#26550;&#26500;&#12290;&#35813;&#26550;&#26500;&#24378;&#35843;&#20102;&#26174;&#24335;&#20998;&#23618;&#29983;&#25104;&#27169;&#22411;&#30340;&#20316;&#29992;&#65292;&#20854;&#25805;&#20316;&#21487;&#20197;&#20351;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#36319;&#36394;&#21644;&#35299;&#37322;&#26377;&#21161;&#20110;&#20854;&#20915;&#31574;&#30340;&#22240;&#32032;&#65292;&#24182;&#19988;&#20854;&#32467;&#26500;&#35774;&#35745;&#25104;&#21487;&#20197;&#34987;&#20154;&#31867;&#29992;&#25143;&#35299;&#35835;&#21644;&#23457;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the prospect of developing human-interpretable, explainable artificial intelligence (AI) systems based on active inference and the free energy principle. We first provide a brief overview of active inference, and in particular, of how it applies to the modeling of decision-making, introspection, as well as the generation of overt and covert actions. We then discuss how active inference can be leveraged to design explainable AI systems, namely, by allowing us to model core features of ``introspective'' processes and by generating useful, human-interpretable models of the processes involved in decision-making. We propose an architecture for explainable AI systems using active inference. This architecture foregrounds the role of an explicit hierarchical generative model, the operation of which enables the AI system to track and explain the factors that contribute to its own decisions, and whose structure is designed to be interpretable and auditable by human users.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33021;&#37327;&#27169;&#22411;&#21644;&#21367;&#31215;&#21464;&#25442;&#30340;&#36328;&#27169;&#24577;&#23450;&#20301;&#26041;&#27861;&#65292;&#21033;&#29992;&#21355;&#26143;&#22270;&#20687;&#36827;&#34892;&#22320;&#22270;&#26500;&#24314;&#65292;&#21487;&#22312;&#27809;&#26377;GPS&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#31934;&#30830;&#30340;&#24230;&#37327;&#32423;&#21035;&#23450;&#20301;&#65292;&#24182;&#22312;KITTI&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26356;&#39640;&#30340;&#23450;&#20301;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.04021</link><description>&lt;p&gt;
&#21033;&#29992;&#33021;&#37327;&#27169;&#22411;&#36827;&#34892;&#36328;&#27169;&#24577;&#23450;&#20301;&#30340;&#21367;&#31215;&#21464;&#25442;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Energy-Based Models for Cross-Modal Localization using Convolutional Transformers. (arXiv:2306.04021v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04021
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33021;&#37327;&#27169;&#22411;&#21644;&#21367;&#31215;&#21464;&#25442;&#30340;&#36328;&#27169;&#24577;&#23450;&#20301;&#26041;&#27861;&#65292;&#21033;&#29992;&#21355;&#26143;&#22270;&#20687;&#36827;&#34892;&#22320;&#22270;&#26500;&#24314;&#65292;&#21487;&#22312;&#27809;&#26377;GPS&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#31934;&#30830;&#30340;&#24230;&#37327;&#32423;&#21035;&#23450;&#20301;&#65292;&#24182;&#22312;KITTI&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26356;&#39640;&#30340;&#23450;&#20301;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;&#33021;&#37327;&#27169;&#22411;&#65288;EBMs&#65289;&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#27809;&#26377;GPS&#30340;&#24773;&#20917;&#19979;&#23450;&#20301;&#19968;&#20010;&#25645;&#36733;&#26377;&#27979;&#36317;&#20256;&#24863;&#22120;&#30340;&#22320;&#38754;&#36710;&#36742;&#30456;&#23545;&#20110;&#21355;&#26143;&#22270;&#20687;&#30340;&#20301;&#32622;&#12290;&#26412;&#26041;&#27861;&#21033;&#29992;&#21355;&#26143;&#22270;&#20687;&#36827;&#34892;&#22320;&#22270;&#26500;&#24314;&#65292;&#36825;&#31181;&#22320;&#22270;&#26159;&#24191;&#27867;&#21487;&#29992;&#21644;&#26131;&#20110;&#33719;&#21462;&#30340;&#65292;&#32780;&#19988;&#20855;&#26377;&#20840;&#38754;&#30340;&#35206;&#30422;&#38754;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21367;&#31215;&#21464;&#25442;&#30340;&#26041;&#27861;&#65292;&#22312;&#36328;&#27169;&#24577;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#31934;&#30830;&#30340;&#24230;&#37327;&#32423;&#21035;&#23450;&#20301;&#65292;&#36825;&#26159;&#30001;&#20110;&#31232;&#30095;&#30340;&#27979;&#36317;&#20256;&#24863;&#22120;&#35835;&#25968;&#21644;&#20016;&#23500;&#30340;&#21355;&#26143;&#22270;&#20687;&#20043;&#38388;&#22806;&#35266;&#24046;&#24322;&#30340;&#26497;&#24230;&#20043;&#22823;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#26159;&#31471;&#21040;&#31471;&#35757;&#32451;&#30340;&#65292;&#24182;&#19988;&#22312;KITTI&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#34920;&#26126;&#65292;&#30456;&#27604;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#26412;&#25991;&#30340;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#26356;&#39640;&#30340;&#23450;&#20301;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel framework using Energy-Based Models (EBMs) for localizing a ground vehicle mounted with a range sensor against satellite imagery in the absence of GPS. Lidar sensors have become ubiquitous on autonomous vehicles for describing its surrounding environment. Map priors are typically built using the same sensor modality for localization purposes. However, these map building endeavors using range sensors are often expensive and time-consuming. Alternatively, we leverage the use of satellite images as map priors, which are widely available, easily accessible, and provide comprehensive coverage. We propose a method using convolutional transformers that performs accurate metric-level localization in a cross-modal manner, which is challenging due to the drastic difference in appearance between the sparse range sensor readings and the rich satellite imagery. We train our model end-to-end and demonstrate our approach achieving higher accuracy than the state-of-the-art on KITTI,
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#29305;&#23450;&#25628;&#32034;&#31354;&#38388;&#21551;&#21457;&#24335;&#26041;&#27861;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#20026;&#32463;&#20856;&#35745;&#21010;&#25552;&#20379;&#36317;&#31163;&#30446;&#26631;&#20272;&#35745;&#65292;&#26377;&#26102;&#33021;&#22815;&#19982;&#39046;&#22495;&#26080;&#20851;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#31454;&#20105;&#12290;</title><link>http://arxiv.org/abs/2306.04019</link><description>&lt;p&gt;
&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#25628;&#32034;&#31354;&#38388;&#29305;&#23450;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning Search-Space Specific Heuristics Using Neural Networks. (arXiv:2306.04019v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04019
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#29305;&#23450;&#25628;&#32034;&#31354;&#38388;&#21551;&#21457;&#24335;&#26041;&#27861;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#20026;&#32463;&#20856;&#35745;&#21010;&#25552;&#20379;&#36317;&#31163;&#30446;&#26631;&#20272;&#35745;&#65292;&#26377;&#26102;&#33021;&#22815;&#19982;&#39046;&#22495;&#26080;&#20851;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#19968;&#20010;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#20026;&#22522;&#20110;&#21069;&#21521;&#25628;&#32034;&#30340;&#28385;&#36275;&#32463;&#20856;&#35745;&#21010;&#25552;&#20379;&#20102;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#21551;&#21457;&#24335;&#20989;&#25968;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#20174;&#22836;&#24320;&#22987;&#23398;&#20064;&#36317;&#31163;&#30446;&#26631;&#20272;&#35745;&#22120;&#65292;&#20165;&#32473;&#23450;&#19968;&#20010;PDDL&#35757;&#32451;&#23454;&#20363;&#12290;&#22521;&#35757;&#25968;&#25454;&#26159;&#36890;&#36807;&#21518;&#21521;&#22238;&#24402;&#25628;&#32034;&#25110;&#20174;&#32473;&#23450;&#25110;&#29468;&#27979;&#30340;&#30446;&#26631;&#29366;&#24577;&#36827;&#34892;&#30340;&#21518;&#21521;&#25628;&#32034;&#20135;&#29983;&#30340;&#12290;&#22312;&#20687;24&#25340;&#22270;&#36825;&#26679;&#25152;&#26377;&#23454;&#20363;&#20849;&#20139;&#21516;&#19968;&#25628;&#32034;&#31354;&#38388;&#30340;&#22495;&#20013;&#65292;&#36825;&#31181;&#21551;&#21457;&#24335;&#26041;&#27861;&#20063;&#21487;&#20197;&#22312;&#35813;&#39046;&#22495;&#30340;&#25152;&#26377;&#23454;&#20363;&#20013;&#37325;&#22797;&#20351;&#29992;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20010;&#30456;&#23545;&#31616;&#21333;&#30340;&#31995;&#32479;&#21487;&#20197;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#24615;&#33021;&#65292;&#26377;&#26102;&#21487;&#20197;&#19982;&#20247;&#25152;&#21608;&#30693;&#30340;&#39046;&#22495;&#29420;&#31435;&#21551;&#21457;&#24335;&#26041;&#27861;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose and evaluate a system which learns a neuralnetwork heuristic function for forward search-based, satisficing classical planning. Our system learns distance-to-goal estimators from scratch, given a single PDDL training instance. Training data is generated by backward regression search or by backward search from given or guessed goal states. In domains such as the 24-puzzle where all instances share the same search space, such heuristics can also be reused across all instances in the domain. We show that this relatively simple system can perform surprisingly well, sometimes competitive with well-known domain-independent heuristics.
&lt;/p&gt;</description></item><item><title>PyTrial&#26159;&#19968;&#20010;&#23454;&#29616;&#22810;&#31181;AI&#31639;&#27861;&#25903;&#25345;&#30340;&#20020;&#24202;&#35797;&#39564;&#20219;&#21153;&#12289;&#21487;&#38598;&#25104;&#33258;&#24049;AI&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#30340;Python&#36719;&#20214;&#21253;&#65292;&#24182;&#22312;&#29616;&#23454;&#20020;&#24202;&#35797;&#39564;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2306.04018</link><description>&lt;p&gt;
PyTrial&#65306;&#33647;&#29289;&#30740;&#21457;&#20154;&#24037;&#26234;&#33021;&#30340;&#20840;&#38754;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
PyTrial: A Comprehensive Platform for Artificial Intelligence for Drug Development. (arXiv:2306.04018v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04018
&lt;/p&gt;
&lt;p&gt;
PyTrial&#26159;&#19968;&#20010;&#23454;&#29616;&#22810;&#31181;AI&#31639;&#27861;&#25903;&#25345;&#30340;&#20020;&#24202;&#35797;&#39564;&#20219;&#21153;&#12289;&#21487;&#38598;&#25104;&#33258;&#24049;AI&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#30340;Python&#36719;&#20214;&#21253;&#65292;&#24182;&#22312;&#29616;&#23454;&#20020;&#24202;&#35797;&#39564;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33647;&#29289;&#30740;&#21457;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#36807;&#31243;&#65292;&#26088;&#22312;&#36890;&#36807;&#20020;&#24202;&#35797;&#39564;&#27979;&#35797;&#20505;&#36873;&#33647;&#29289;&#22312;&#20154;&#20307;&#20869;&#30340;&#30103;&#25928;&#21644;&#23433;&#20840;&#24615;&#20197;&#33719;&#24471;&#30417;&#31649;&#25209;&#20934;&#12290;&#26368;&#36817;&#65292;&#26426;&#22120;&#23398;&#20064;&#20316;&#20026;&#33647;&#29289;&#30740;&#21457;&#30340;&#37325;&#35201;&#24037;&#20855;&#20986;&#29616;&#20102;&#65292;&#20026;&#25552;&#39640;&#35813;&#36807;&#31243;&#30340;&#25928;&#29575;&#21644;&#25104;&#21151;&#29575;&#25552;&#20379;&#20102;&#26032;&#26426;&#20250;&#12290;&#20026;&#20102;&#20419;&#36827;&#33647;&#29289;&#30740;&#21457;&#20154;&#24037;&#26234;&#33021;&#30340;&#30740;&#31350;&#21644;&#24320;&#21457;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;Python&#36719;&#20214;&#21253;&#65292;&#21517;&#20026;PyTrial&#65292;&#35813;&#36719;&#20214;&#21253;&#23454;&#29616;&#20102;&#22810;&#31181;&#34987;AI&#31639;&#27861;&#25903;&#25345;&#30340;&#20020;&#24202;&#35797;&#39564;&#20219;&#21153;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;PyTrial&#23454;&#29616;&#20102;6&#20010;&#20851;&#38190;&#30340;&#33647;&#29289;&#30740;&#21457;&#20219;&#21153;&#65292;&#21253;&#25324;&#24739;&#32773;&#32467;&#26524;&#39044;&#27979;&#12289;&#35797;&#39564;&#22320;&#28857;&#36873;&#25321;&#12289;&#35797;&#39564;&#32467;&#26524;&#39044;&#27979;&#12289;&#24739;&#32773;-&#35797;&#39564;&#21305;&#37197;&#12289;&#35797;&#39564;&#30456;&#20284;&#24615;&#25628;&#32034;&#21644;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#12290;&#22312;PyTrial&#20013;&#65292;&#25152;&#26377;&#20219;&#21153;&#37117;&#30001;&#22235;&#20010;&#27493;&#39588;&#23450;&#20041;&#65306;&#21152;&#36733;&#25968;&#25454;&#12289;&#27169;&#22411;&#23450;&#20041;&#12289;&#27169;&#22411;&#35757;&#32451;&#21644;&#27169;&#22411;&#35780;&#20272;&#65292;&#36825;&#21487;&#20197;&#29992;&#20960;&#34892;&#20195;&#30721;&#23436;&#25104;&#12290;&#27492;&#22806;&#65292;&#27169;&#22359;&#21270;&#30340;API&#35774;&#35745;&#20801;&#35768;&#20174;&#19994;&#32773;&#38598;&#25104;&#33258;&#24049;&#30340;AI&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#12290;PyTrial&#24050;&#22312;&#29616;&#23454;&#20020;&#24202;&#35797;&#39564;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#27979;&#35797;&#65292;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#20854;&#22312;&#21508;&#31181;&#21463;AI&#25903;&#25345;&#30340;&#20020;&#24202;&#35797;&#39564;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Drug development is a complex process that aims to test the efficacy and safety of candidate drugs in the human body for regulatory approval via clinical trials. Recently, machine learning has emerged as a vital tool for drug development, offering new opportunities to improve the efficiency and success rates of the process. To facilitate the research and development of artificial intelligence (AI) for drug development, we developed a Python package, namely PyTrial, that implements various clinical trial tasks supported by AI algorithms.  To be specific, PyTrial implements 6 essential drug development tasks, including patient outcome prediction, trial site selection, trial outcome prediction, patient-trial matching, trial similarity search, and synthetic data generation. In PyTrial, all tasks are defined by four steps: load data, model definition, model training, and model evaluation, which can be done with a couple of lines of code. In addition, the modular API design allows practition
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#36719;&#25552;&#31034;&#21644;&#38543;&#26426;&#28216;&#36208;&#30340;&#26041;&#27861;&#65292;&#20197;&#20415;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22810;&#36339;&#25512;&#29702;&#30340;&#38382;&#31572;&#20219;&#21153;&#65292;&#21462;&#24471;&#20102;&#27604;&#26631;&#20934;&#35843;&#25972;&#26041;&#27861;&#26356;&#22823;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2306.04009</link><description>&lt;p&gt;
&#20351;&#29992;&#36719;&#25552;&#31034;&#21644;&#38543;&#26426;&#28216;&#36208;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#35302;&#21457;&#22810;&#36339;&#25512;&#29702;&#36827;&#34892;&#38382;&#39064;&#22238;&#31572;
&lt;/p&gt;
&lt;p&gt;
Triggering Multi-Hop Reasoning for Question Answering in Language Models using Soft Prompts and Random Walks. (arXiv:2306.04009v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04009
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#36719;&#25552;&#31034;&#21644;&#38543;&#26426;&#28216;&#36208;&#30340;&#26041;&#27861;&#65292;&#20197;&#20415;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22810;&#36339;&#25512;&#29702;&#30340;&#38382;&#31572;&#20219;&#21153;&#65292;&#21462;&#24471;&#20102;&#27604;&#26631;&#20934;&#35843;&#25972;&#26041;&#27861;&#26356;&#22823;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#21487;&#20197;&#36731;&#26494;&#22320;&#35760;&#24518;&#26377;&#20851;&#23454;&#20307;&#30340;&#19990;&#30028;&#30693;&#35782;&#65292;&#20294;&#39044;&#20808;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#24448;&#24448;&#22312;&#32452;&#21512;&#20004;&#20010;&#25110;&#22810;&#20010;&#20107;&#23454;&#20197;&#25191;&#34892;&#22810;&#36339;&#25512;&#29702;&#30340;&#38382;&#31572;&#20219;&#21153;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#25216;&#26415;&#26469;&#25913;&#21892;&#36825;&#20010;&#38480;&#21046;&#65292;&#36825;&#20123;&#25216;&#26415;&#20381;&#38752;&#32467;&#26500;&#21270;&#30693;&#35782;&#22270;&#19978;&#30340;&#38543;&#26426;&#28216;&#36208;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#36719;&#25552;&#31034;&#26469;&#24341;&#23548;LM&#65292;&#36890;&#36807;&#23398;&#20064;&#23558;&#22810;&#36339;&#38382;&#39064;&#26144;&#23556;&#21040;&#36890;&#21521;&#31572;&#26696;&#30340;&#38543;&#26426;&#28216;&#36208;&#36335;&#24452;&#26469;&#38142;&#24335;&#32534;&#30721;&#23427;&#20204;&#30340;&#30693;&#35782;&#12290;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#20004;&#20010;T5 LM&#19978;&#65292;&#22312;&#22238;&#31572;&#38656;&#35201;2&#36339;&#25512;&#29702;&#30340;&#38382;&#39064;&#26041;&#38754;&#65292;&#34920;&#29616;&#20986;&#20102;&#27604;&#26631;&#20934;&#35843;&#25972;&#26041;&#27861;&#26356;&#22823;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite readily memorizing world knowledge about entities, pre-trained language models (LMs) struggle to compose together two or more facts to perform multi-hop reasoning in question-answering tasks. In this work, we propose techniques that improve upon this limitation by relying on random walks over structured knowledge graphs. Specifically, we use soft prompts to guide LMs to chain together their encoded knowledge by learning to map multi-hop questions to random walk paths that lead to the answer. Applying our methods on two T5 LMs shows substantial improvements over standard tuning approaches in answering questions that require 2-hop reasoning.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#20026;&#22522;&#30784;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#19968;&#32500;&#28145;&#24230;&#22270;&#20687;&#20808;&#39564;&#25311;&#21512;&#30005;&#30913;&#27714;&#35299;&#22120;&#30340;S&#21442;&#25968;&#12290;&#36890;&#36807;&#19982;&#20844;&#24320;&#21487;&#29992;&#21644;&#19987;&#26377;&#30340;&#34892;&#19994;&#26631;&#20934;&#25311;&#21512;&#26041;&#27861;&#30340;&#27604;&#36739;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#37325;&#24314;&#36136;&#37327;&#21644;&#35745;&#31639;&#26102;&#38388;&#26041;&#38754;&#22343;&#26377;&#26174;&#30528;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2306.04001</link><description>&lt;p&gt;
&#29992;&#19968;&#32500;&#28145;&#24230;&#22270;&#20687;&#20808;&#39564;&#36827;&#34892;&#30005;&#30913;&#27714;&#35299;&#22120;S&#21442;&#25968;&#26354;&#32447;&#25311;&#21512;
&lt;/p&gt;
&lt;p&gt;
One-Dimensional Deep Image Prior for Curve Fitting of S-Parameters from Electromagnetic Solvers. (arXiv:2306.04001v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04001
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#20026;&#22522;&#30784;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#19968;&#32500;&#28145;&#24230;&#22270;&#20687;&#20808;&#39564;&#25311;&#21512;&#30005;&#30913;&#27714;&#35299;&#22120;&#30340;S&#21442;&#25968;&#12290;&#36890;&#36807;&#19982;&#20844;&#24320;&#21487;&#29992;&#21644;&#19987;&#26377;&#30340;&#34892;&#19994;&#26631;&#20934;&#25311;&#21512;&#26041;&#27861;&#30340;&#27604;&#36739;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#37325;&#24314;&#36136;&#37327;&#21644;&#35745;&#31639;&#26102;&#38388;&#26041;&#38754;&#22343;&#26377;&#26174;&#30528;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38598;&#25104;&#30005;&#36335;&#23553;&#35013;&#20013;&#24314;&#27169;&#20449;&#21495;&#23436;&#25972;&#24615;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#26159;&#38656;&#35201;&#22312;&#25152;&#38656;&#39057;&#24102;&#20869;&#36827;&#34892;&#22810;&#20010;S&#21442;&#25968;&#27979;&#37327;&#20197;&#33719;&#24471;&#36275;&#22815;&#30340;&#20998;&#36776;&#29575;&#12290;&#20351;&#29992;&#30005;&#30913;&#22330;&#27714;&#35299;&#22120;&#33719;&#24471;&#36825;&#20123;&#26679;&#26412;&#36890;&#24120;&#26159;&#35745;&#31639;&#26114;&#36149;&#30340;&#12290;&#22240;&#27492;&#65292;&#24120;&#35265;&#26041;&#27861;&#26159;&#36873;&#25321;&#25152;&#38656;&#26679;&#26412;&#30340;&#19968;&#23567;&#37096;&#20998;&#65292;&#24182;&#20351;&#29992;&#36866;&#24403;&#30340;&#25311;&#21512;&#26426;&#21046;&#37325;&#26032;&#21019;&#24314;&#23494;&#38598;&#37319;&#26679;&#30340;&#23485;&#24102;&#34920;&#31034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#26469;&#20351;&#29992;&#19968;&#32500;&#28145;&#24230;&#22270;&#20687;&#20808;&#39564;&#65288;DIP&#65289;&#25311;&#21512;&#26469;&#33258;EM&#27714;&#35299;&#22120;&#30340;S&#21442;&#25968;&#12290;DIP&#26159;&#19968;&#31181;&#25216;&#26415;&#65292;&#23427;&#20248;&#21270;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#20197;&#36866;&#24212;&#20174;&#22122;&#22768;&#25110;&#27424;&#23450;&#27979;&#37327;&#20013;&#30340;&#20449;&#21495;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#33258;&#23450;&#20041;&#26550;&#26500;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#28789;&#24863;&#26469;&#33258;&#24179;&#28369;&#26679;&#26465;&#65292;&#20197;&#24809;&#32602;&#19981;&#36830;&#32493;&#30340;&#36339;&#36291;&#12290;&#25105;&#20204;&#22312;&#20004;&#31181;&#31867;&#22411;&#30340;&#26080;&#28304;&#28388;&#27874;&#22120;&#19978;&#23454;&#39564;&#27604;&#36739;&#20102;DIP&#19982;&#20844;&#24320;&#21487;&#29992;&#30340;&#21644;&#19987;&#26377;&#30340;&#34892;&#19994;&#26631;&#20934;&#25311;&#21512;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#37325;&#24314;&#36136;&#37327;&#21644;&#35745;&#31639;&#26102;&#38388;&#30340;&#26174;&#30528;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
A key problem when modeling signal integrity for passive filters and interconnects in IC packages is the need for multiple S-parameter measurements within a desired frequency band to obtain adequate resolution. These samples are often computationally expensive to obtain using electromagnetic (EM) field solvers. Therefore, a common approach is to select a small subset of the necessary samples and use an appropriate fitting mechanism to recreate a densely-sampled broadband representation. We present the first deep generative model-based approach to fit S-parameters from EM solvers using one-dimensional Deep Image Prior (DIP). DIP is a technique that optimizes the weights of a randomly-initialized convolutional neural network to fit a signal from noisy or under-determined measurements. We design a custom architecture and propose a novel regularization inspired by smoothing splines that penalizes discontinuous jumps. We experimentally compare DIP to publicly available and proprietary indus
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23454;&#26102;&#22312;&#32447;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#35774;&#32622;&#65288;R$^2$OUDA&#65289;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22810;&#25668;&#20687;&#22836;&#31995;&#32479;R$^2$MMT&#12290;&#36890;&#36807;R$^2$OUDA&#65292;&#26412;&#25991;&#35299;&#20915;&#20102;&#29616;&#23454;&#24212;&#29992;&#20013;&#34987;&#24573;&#30053;&#30340;&#22235;&#20010;&#20027;&#35201;&#38480;&#21046;&#65292;&#20197;&#23454;&#29616;&#30495;&#27491;&#30340;&#23454;&#26102;&#22312;&#32447;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#12290;</title><link>http://arxiv.org/abs/2306.03993</link><description>&lt;p&gt;
&#23454;&#26102;&#22312;&#32447;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#20154;&#21592;&#20877;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Real-Time Online Unsupervised Domain Adaptation for Real-World Person Re-identification. (arXiv:2306.03993v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03993
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23454;&#26102;&#22312;&#32447;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#35774;&#32622;&#65288;R$^2$OUDA&#65289;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22810;&#25668;&#20687;&#22836;&#31995;&#32479;R$^2$MMT&#12290;&#36890;&#36807;R$^2$OUDA&#65292;&#26412;&#25991;&#35299;&#20915;&#20102;&#29616;&#23454;&#24212;&#29992;&#20013;&#34987;&#24573;&#30053;&#30340;&#22235;&#20010;&#20027;&#35201;&#38480;&#21046;&#65292;&#20197;&#23454;&#29616;&#30495;&#27491;&#30340;&#23454;&#26102;&#22312;&#32447;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;UDA&#65289;&#22312;&#20154;&#21592;&#20877;&#35782;&#21035;&#20013;&#30340;&#27969;&#34892;&#65292;&#26368;&#36817;&#25552;&#20986;&#30340;&#22312;&#32447;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;OUDA&#65289;&#23581;&#35797;&#36890;&#36807;&#24341;&#20837;&#25968;&#25454;&#27969;&#30340;&#32771;&#34385;&#26469;&#24357;&#21512;&#23454;&#38469;&#24212;&#29992;&#30340;&#24046;&#36317;&#12290;&#28982;&#32780;&#65292;&#36825;&#20173;&#28982;&#26080;&#27861;&#30495;&#27491;&#20195;&#34920;&#30495;&#23454;&#30340;&#29616;&#23454;&#24212;&#29992;&#12290;&#26412;&#25991;&#23450;&#20041;&#20102;&#29616;&#23454;&#19990;&#30028;&#23454;&#26102;&#22312;&#32447;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;R$^2$OUDA&#65289;&#12290;R$^2$OUDA&#35774;&#32622;&#20102;&#30495;&#27491;&#30340;&#29616;&#23454;&#19990;&#30028;&#23454;&#26102;OUDA&#30340;&#33310;&#21488;&#65292;&#25581;&#31034;&#20102;&#24403;&#21069;&#30740;&#31350;&#20013;&#32463;&#24120;&#34987;&#24573;&#30053;&#30340;&#22235;&#20010;&#20027;&#35201;&#38480;&#21046;&#65306;&#31995;&#32479;&#29983;&#25104;&#30340;&#20154;&#21592;&#22270;&#20687;&#12289;&#23376;&#38598;&#20998;&#24067;&#36873;&#25321;&#12289;&#22522;&#20110;&#26102;&#38388;&#30340;&#25968;&#25454;&#27969;&#20998;&#21106;&#21644;&#22522;&#20110;&#20998;&#27573;&#30340;&#26102;&#38388;&#32422;&#26463;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#26032;&#30340;R$^2$OUDA&#35774;&#32622;&#30340;&#25152;&#26377;&#26041;&#38754;&#65292;&#26412;&#25991;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#29616;&#23454;&#19990;&#30028;&#23454;&#26102;&#22312;&#32447;&#27969;&#24335;&#20114;&#30456;&#24179;&#22343;&#25945;&#23398;&#65288;R$^2$MMT&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#25668;&#20687;&#22836;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Following the popularity of Unsupervised Domain Adaptation (UDA) in person re-identification, the recently proposed setting of Online Unsupervised Domain Adaptation (OUDA) attempts to bridge the gap towards practical applications by introducing a consideration of streaming data. However, this still falls short of truly representing real-world applications. This paper defines the setting of Real-world Real-time Online Unsupervised Domain Adaptation (R$^2$OUDA) for Person Re-identification. The R$^2$OUDA setting sets the stage for true real-world real-time OUDA, bringing to light four major limitations found in real-world applications that are often neglected in current research: system generated person images, subset distribution selection, time-based data stream segmentation, and a segment-based time constraint. To address all aspects of this new R$^2$OUDA setting, this paper further proposes Real-World Real-Time Online Streaming Mutual Mean-Teaching (R$^2$MMT), a novel multi-camera sy
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20687;&#32032;&#32423;&#20132;&#20114;&#30340;&#22810;&#30446;&#26631;&#35270;&#39057;&#29983;&#25104;&#26041;&#27861;&#65292;&#33021;&#22815;&#29983;&#25104;&#36924;&#30495;&#30340;&#29289;&#20307;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#35270;&#39057;&#65292;&#19988;&#33021;&#20934;&#30830;&#36319;&#38543;&#29992;&#25143;&#30340;&#25511;&#21046;&#65292;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#35270;&#39057;&#29983;&#25104;&#20808;&#21069;&#24037;&#20316;&#30456;&#23218;&#32654;&#29978;&#33267;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.03988</link><description>&lt;p&gt;
&#23398;&#20064;&#25105;&#20204;&#33021;&#22815;&#25484;&#25569;&#30340;&#21147;&#37327;&#65306;&#22522;&#20110;&#20687;&#32032;&#32423;&#20132;&#20114;&#30340;&#22810;&#30446;&#26631;&#35270;&#39057;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Learn the Force We Can: Multi-Object Video Generation from Pixel-Level Interactions. (arXiv:2306.03988v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03988
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20687;&#32032;&#32423;&#20132;&#20114;&#30340;&#22810;&#30446;&#26631;&#35270;&#39057;&#29983;&#25104;&#26041;&#27861;&#65292;&#33021;&#22815;&#29983;&#25104;&#36924;&#30495;&#30340;&#29289;&#20307;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#35270;&#39057;&#65292;&#19988;&#33021;&#20934;&#30830;&#36319;&#38543;&#29992;&#25143;&#30340;&#25511;&#21046;&#65292;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#35270;&#39057;&#29983;&#25104;&#20808;&#21069;&#24037;&#20316;&#30456;&#23218;&#32654;&#29978;&#33267;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#36890;&#36807;&#21333;&#24103;&#22270;&#20687;&#19982;&#31232;&#30095;&#36816;&#21160;&#36755;&#20837;&#26469;&#33258;&#25105;&#22238;&#24402;&#29983;&#25104;&#35270;&#39057;&#12290;&#25105;&#20204;&#35757;&#32451;&#30340;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#36924;&#30495;&#30340;&#29289;&#20307;&#38388;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#22312;&#20165;&#35266;&#27979;&#21040;&#23427;&#20204;&#22312;&#30456;&#20851;&#36816;&#21160;&#27963;&#21160;&#19979;&#26102;&#20998;&#31163;&#22810;&#20010;&#29289;&#20307;&#30340;&#21160;&#24577;&#21644;&#33539;&#22260;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20851;&#38190;&#32452;&#20214;&#26159;&#38543;&#26426;&#21270;&#26465;&#20214;&#26041;&#26696;&#12289;&#36755;&#20837;&#36816;&#21160;&#25511;&#21046;&#30340;&#32534;&#30721;&#20197;&#21450;&#38543;&#26426;&#21270;&#21644;&#31232;&#30095;&#37319;&#26679;&#26469;&#25171;&#30772;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#31216;&#20043;&#20026;YODA&#30340;&#27169;&#22411;&#20855;&#26377;&#33021;&#22815;&#31227;&#21160;&#29289;&#20307;&#32780;&#26080;&#38656;&#23454;&#38469;&#35302;&#25720;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#23450;&#37327;&#21644;&#23450;&#24615;&#22320;&#23637;&#31034;&#65292;YODA&#33021;&#22815;&#20934;&#30830;&#36319;&#38543;&#29992;&#25143;&#30340;&#25511;&#21046;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21576;&#29616;&#20986;&#19982;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#35270;&#39057;&#29983;&#25104;&#20808;&#21069;&#24037;&#20316;&#30456;&#23218;&#32654;&#29978;&#33267;&#26356;&#22909;&#30340;&#35270;&#39057;&#36136;&#37327;&#12290;&#35814;&#24773;&#35831;&#21442;&#38405;&#25105;&#20204;&#30340;&#39033;&#30446;&#32593;&#31449; https://araachie.github.io/yoda&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel unsupervised method to autoregressively generate videos from a single frame and a sparse motion input. Our trained model can generate realistic object-to-object interactions and separate the dynamics and the extents of multiple objects despite only observing them under correlated motion activities. Key components in our method are the randomized conditioning scheme, the encoding of the input motion control, and the randomized and sparse sampling to break correlations. Our model, which we call YODA, has the ability to move objects without physically touching them. We show both qualitatively and quantitatively that YODA accurately follows the user control, while yielding a video quality that is on par with or better than state of the art video generation prior work on several datasets. For videos, visit our project website https://araachie.github.io/yoda.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#23383;&#34920;&#22411;&#21644;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#65292;&#36890;&#36807;&#39044;&#27979;&#12289;&#26816;&#27979;&#21644;&#22240;&#26524;&#35299;&#37322;&#24739;&#26377;&#31934;&#31070;&#20998;&#35010;&#30151;&#30151;&#29366;&#21464;&#21270;&#30340;&#20010;&#20307;&#24773;&#20917;&#65292;&#21487;&#33021;&#24110;&#21161;&#25913;&#36827;&#31934;&#31070;&#30149;&#23398;&#20020;&#24202;&#20915;&#31574;&#65292;&#31995;&#32479;&#35823;&#24046;&#29575;&#20302;&#20110;10%&#12290;</title><link>http://arxiv.org/abs/2306.03980</link><description>&lt;p&gt;
&#25968;&#23383;&#34920;&#22411;&#22312;&#31934;&#31070;&#30149;&#23398;&#20020;&#24202;&#20915;&#31574;&#20013;&#30340;&#24212;&#29992;&#8212;&#8212;&#22522;&#20110;&#23545;&#30149;&#20363;&#30340;&#39044;&#27979;&#21644;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Explanations and Predictive Models to Enhance Clinical Decision-Making in Schizophrenia using Digital Phenotyping. (arXiv:2306.03980v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03980
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#23383;&#34920;&#22411;&#21644;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#65292;&#36890;&#36807;&#39044;&#27979;&#12289;&#26816;&#27979;&#21644;&#22240;&#26524;&#35299;&#37322;&#24739;&#26377;&#31934;&#31070;&#20998;&#35010;&#30151;&#30151;&#29366;&#21464;&#21270;&#30340;&#20010;&#20307;&#24773;&#20917;&#65292;&#21487;&#33021;&#24110;&#21161;&#25913;&#36827;&#31934;&#31070;&#30149;&#23398;&#20020;&#24202;&#20915;&#31574;&#65292;&#31995;&#32479;&#35823;&#24046;&#29575;&#20302;&#20110;10%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#31070;&#30149;&#23398;&#30340;&#20020;&#24202;&#23454;&#36341;&#24120;&#38754;&#20020;&#21355;&#29983;&#26381;&#21153;&#38656;&#27714;&#30340;&#22686;&#21152;&#21644;&#21307;&#30103;&#36164;&#28304;&#30340;&#31232;&#32570;&#12290;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#26032;&#22411;&#20581;&#24247;&#25968;&#25454;&#33539;&#20363;&#21487;&#33021;&#20250;&#25171;&#24320;&#31934;&#31070;&#30149;&#23398;&#20020;&#24202;&#35780;&#20272;&#21644;&#27835;&#30103;&#20851;&#38190;&#38454;&#27573;&#30340;&#24037;&#20316;&#27969;&#25913;&#36827;&#30340;&#21487;&#33021;&#24615;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#65292;&#21033;&#29992;&#34892;&#20026;&#25968;&#23383;&#34920;&#22411;&#25968;&#25454;&#39044;&#27979;&#12289;&#26816;&#27979;&#21644;&#35299;&#37322;&#24739;&#26377;&#31934;&#31070;&#20998;&#35010;&#30151;&#30151;&#29366;&#21464;&#21270;&#30340;&#20010;&#20307;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#35823;&#24046;&#29575;&#20302;&#20110;10%&#12290;&#31995;&#32479;&#21033;&#29992;&#21464;&#28857;&#26816;&#27979;&#31639;&#27861;&#26816;&#27979;&#30151;&#29366;&#19979;&#38477;&#65292;&#24182;&#22312;&#27169;&#25311;&#30340;&#36830;&#32493;&#30417;&#25511;&#22330;&#26223;&#20013;&#20351;&#29992;&#22240;&#26524;&#35299;&#37322;&#21151;&#33021;&#20316;&#20026;&#19968;&#31181;&#34917;&#25937;&#25514;&#26045;&#12290;&#36825;&#39033;&#30740;&#31350;&#20174;&#27169;&#25311;&#20020;&#24202;&#27969;&#31243;&#30340;&#35282;&#24230;&#25552;&#20379;&#20102;&#26377;&#20851;&#22240;&#26524;&#35299;&#37322;&#12289;&#39044;&#27979;&#27169;&#22411;&#21644;&#21464;&#28857;&#26816;&#27979;&#24615;&#33021;&#21644;&#28508;&#21147;&#30340;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;&#26412;&#30740;&#31350;&#24471;&#20986;&#20197;&#19979;&#32467;&#35770;&#65306;&#25968;&#23383;&#34920;&#22411;&#19982;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#21487;&#20197;&#24110;&#21161;&#25913;&#36827;&#31934;&#31070;&#30149;&#23398;&#20020;&#24202;&#20915;&#31574;&#65307;&#25152;&#25552;&#20986;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#21487;&#20197;&#39640;&#31934;&#24230;&#22320;&#39044;&#27979;&#21644;&#26816;&#27979;&#30149;&#20363;&#20010;&#20307;&#30151;&#29366;&#21464;&#21270;&#65307;&#22240;&#26524;&#35299;&#37322;&#22312;&#27169;&#25311;&#30340;&#36830;&#32493;&#30417;&#25511;&#22330;&#26223;&#20013;&#26159;&#19968;&#31181;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clinical practice in psychiatry is burdened with the increased demand for healthcare services and the scarce resources available. New paradigms of health data powered with machine learning techniques could open the possibility to improve clinical workflow in critical stages of clinical assessment and treatment in psychiatry. In this work, we propose a machine learning system capable of predicting, detecting, and explaining individual changes in symptoms of patients with Schizophrenia by using behavioral digital phenotyping data. We forecast symptoms of patients with an error rate below 10%. The system detects decreases in symptoms using changepoint algorithms and uses counterfactual explanations as a recourse in a simulated continuous monitoring scenario in healthcare. Overall, this study offers valuable insights into the performance and potential of counterfactual explanations, predictive models, and change-point detection within a simulated clinical workflow. These findings lay the f
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#34920;&#36798;&#24335;&#24067;&#23572;&#20844;&#24335;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65292;&#21033;&#29992;&#26412;&#22320;&#20248;&#21270;&#25216;&#26415;&#35757;&#32451;&#20998;&#31867;&#27169;&#22411;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#20449;&#29992;&#35780;&#20998;&#21644;&#21307;&#30103;&#30149;&#20917;&#30340;&#35786;&#26029;&#65292;&#24182;&#20855;&#26377;&#26410;&#26469;&#24212;&#29992;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.03976</link><description>&lt;p&gt;
&#20351;&#29992;&#34920;&#36798;&#24335;&#24067;&#23572;&#20844;&#24335;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Explainable AI using expressive Boolean formulas. (arXiv:2306.03976v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03976
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#34920;&#36798;&#24335;&#24067;&#23572;&#20844;&#24335;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65292;&#21033;&#29992;&#26412;&#22320;&#20248;&#21270;&#25216;&#26415;&#35757;&#32451;&#20998;&#31867;&#27169;&#22411;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#20449;&#29992;&#35780;&#20998;&#21644;&#21307;&#30103;&#30149;&#20917;&#30340;&#35786;&#26029;&#65292;&#24182;&#20855;&#26377;&#26410;&#26469;&#24212;&#29992;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#24182;&#23454;&#29616;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#27169;&#22411;&#65292;&#22522;&#20110;&#34920;&#36798;&#24335;&#24067;&#23572;&#20844;&#24335;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#12290;&#28508;&#22312;&#30340;&#24212;&#29992;&#21253;&#25324;&#20449;&#29992;&#35780;&#20998;&#21644;&#21307;&#30103;&#30149;&#20917;&#30340;&#35786;&#26029;&#12290;&#24067;&#23572;&#20844;&#24335;&#23450;&#20041;&#20102;&#19968;&#20010;&#35268;&#21017;&#65292;&#26681;&#25454;&#35813;&#35268;&#21017;&#23558;&#36755;&#20837;&#25968;&#25454;&#20998;&#31867;&#20026;&#21487;&#35843;&#25972;&#22797;&#26434;&#24230;&#65288;&#25110;&#21487;&#35299;&#37322;&#24615;&#65289;&#12290;&#36825;&#26679;&#30340;&#20844;&#24335;&#21487;&#20197;&#21253;&#21547;&#24212;&#29992;&#20110;&#19968;&#20010;&#25110;&#22810;&#20010;&#24067;&#23572;&#21464;&#37327;&#30340;&#20219;&#20309;&#36816;&#31639;&#31526;&#65292;&#22240;&#27492;&#20855;&#26377;&#27604;&#26356;&#20005;&#26684;&#30340;&#22522;&#20110;&#35268;&#21017;&#21644;&#22522;&#20110;&#26641;&#30340;&#26041;&#27861;&#26356;&#39640;&#30340;&#34920;&#36798;&#24615;&#12290;&#20998;&#31867;&#22120;&#20351;&#29992;&#26412;&#22320;&#20248;&#21270;&#25216;&#26415;&#36827;&#34892;&#35757;&#32451;&#65292;&#26377;&#25928;&#22320;&#25628;&#32034;&#21487;&#34892;&#20844;&#24335;&#30340;&#31354;&#38388;&#12290;&#27973;&#30340;&#35268;&#21017;&#21487;&#20197;&#36890;&#36807;&#24555;&#36895;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#65288;ILP&#65289;&#25110;&#20108;&#27425;&#26080;&#32422;&#26463;&#20108;&#36827;&#21046;&#20248;&#21270;&#65288;QUBO&#65289;&#27714;&#35299;&#22120;&#30830;&#23450;&#65292;&#21487;&#33021;&#30001;&#19987;&#29992;&#30828;&#20214;&#25110;&#37327;&#23376;&#35774;&#22791;&#25552;&#20379;&#21160;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#25191;&#34892;&#26412;&#22320;&#20248;&#21270;&#22120;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#25928;&#29575;&#19982;&#36825;&#20123;&#35774;&#22791;&#30340;&#24555;&#36895;&#25805;&#20316;&#30456;&#32467;&#21512;&#65292;&#20026;&#26410;&#26469;&#30340;AI&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose and implement an interpretable machine learning classification model for Explainable AI (XAI) based on expressive Boolean formulas. Potential applications include credit scoring and diagnosis of medical conditions. The Boolean formula defines a rule with tunable complexity (or interpretability), according to which input data are classified. Such a formula can include any operator that can be applied to one or more Boolean variables, thus providing higher expressivity compared to more rigid rule-based and tree-based approaches. The classifier is trained using native local optimization techniques, efficiently searching the space of feasible formulas. Shallow rules can be determined by fast Integer Linear Programming (ILP) or Quadratic Unconstrained Binary Optimization (QUBO) solvers, potentially powered by special purpose hardware or quantum devices. We combine the expressivity and efficiency of the native local optimizer with the fast operation of these devices by executing n
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#31639;&#27861; PILLAR&#65292;&#21487;&#20197;&#22312;&#21322;&#30417;&#30563;&#21322;&#31169;&#26377;&#65288;SP&#65289;&#23398;&#20064;&#20013;&#26126;&#26174;&#38477;&#20302;&#31169;&#26377;&#26631;&#35760;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#24182;&#21487;&#20197;&#22312;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#39640;&#25928;&#36816;&#34892;&#65292;&#21487;&#20197;&#21033;&#29992;&#22312;&#20844;&#20849;&#25968;&#25454;&#19978;&#39044;&#35757;&#32451;&#30340;&#32593;&#32476;&#25552;&#21462;&#30340;&#29305;&#24449;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26174;&#33879;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.03962</link><description>&lt;p&gt;
PILLAR&#65306;&#22914;&#20309;&#20351;&#21322;&#31169;&#26377;&#23398;&#20064;&#26356;&#26377;&#25928;
&lt;/p&gt;
&lt;p&gt;
PILLAR: How to make semi-private learning more effective. (arXiv:2306.03962v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03962
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#31639;&#27861; PILLAR&#65292;&#21487;&#20197;&#22312;&#21322;&#30417;&#30563;&#21322;&#31169;&#26377;&#65288;SP&#65289;&#23398;&#20064;&#20013;&#26126;&#26174;&#38477;&#20302;&#31169;&#26377;&#26631;&#35760;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#24182;&#21487;&#20197;&#22312;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#39640;&#25928;&#36816;&#34892;&#65292;&#21487;&#20197;&#21033;&#29992;&#22312;&#20844;&#20849;&#25968;&#25454;&#19978;&#39044;&#35757;&#32451;&#30340;&#32593;&#32476;&#25552;&#21462;&#30340;&#29305;&#24449;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26174;&#33879;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21322;&#30417;&#30563;&#21322;&#31169;&#26377;&#65288;SP&#65289;&#23398;&#20064;&#20013;&#65292;&#23398;&#20064;&#32773;&#21487;&#20197;&#35775;&#38382;&#20844;&#20849;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#21644;&#31169;&#26377;&#30340;&#26631;&#35760;&#25968;&#25454;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#31639;&#27861;&#65292;&#20551;&#35774;&#25968;&#25454;&#31526;&#21512;&#19968;&#23450;&#26465;&#20214;&#65292;&#21487;&#20197;&#26126;&#26174;&#38477;&#20302;&#31169;&#26377;&#26631;&#35760;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#24182;&#21487;&#20197;&#22312;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#39640;&#25928;&#36816;&#34892;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;&#22312;&#20844;&#20849;&#25968;&#25454;&#65288;&#26631;&#35760;&#25110;&#26410;&#26631;&#35760;&#65289;&#19978;&#39044;&#35757;&#32451;&#30340;&#32593;&#32476;&#25552;&#21462;&#30340;&#29305;&#24449;&#65292;&#36825;&#20123;&#29305;&#24449;&#30340;&#20998;&#24067;&#21487;&#33021;&#19982;&#36827;&#34892;SP&#23398;&#20064;&#30340;&#20998;&#24067;&#26174;&#33879;&#19981;&#21516;&#12290;&#20026;&#20102;&#39564;&#35777;&#20854;&#23454;&#35777;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#31181;&#22312;&#20005;&#26684;&#30340;&#38544;&#31169;&#32422;&#26463;&#65288;\(\epsilon=0.1\))&#21644;&#20302;&#25968;&#25454;&#37327;&#24773;&#20917;&#19979;&#30340;&#23454;&#39564;&#12290;&#22312;&#25152;&#26377;&#36825;&#20123;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#34920;&#29616;&#20986;&#26174;&#33879;&#20248;&#20110;&#20351;&#29992;&#31867;&#20284;&#25968;&#37327;&#30340;&#20844;&#20849;&#25968;&#25454;&#30340;&#29616;&#26377;&#22522;&#32447;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Semi-Supervised Semi-Private (SP) learning, the learner has access to both public unlabelled and private labelled data. We propose a computationally efficient algorithm that, under mild assumptions on the data, provably achieves significantly lower private labelled sample complexity and can be efficiently run on real-world datasets. For this purpose, we leverage the features extracted by networks pre-trained on public (labelled or unlabelled) data, whose distribution can significantly differ from the one on which SP learning is performed. To validate its empirical effectiveness, we propose a wide variety of experiments under tight privacy constraints (\(\epsilon=0.1\)) and with a focus on low-data regimes. In all of these settings, our algorithm exhibits significantly improved performance over available baselines that use similar amounts of public data.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#21363;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#38598;&#21512;&#26469;&#39640;&#25928;&#22320;&#35782;&#21035;&#25163;&#20889;&#27721;&#23383;&#65292;&#33021;&#22815;&#23454;&#29616;&#39640;&#36798;99.4%&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.03954</link><description>&lt;p&gt;
&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#38598;&#21512;&#35782;&#21035;&#25163;&#20889;&#26085;&#35821;&#23383;&#31526;
&lt;/p&gt;
&lt;p&gt;
Recognition of Handwritten Japanese Characters Using Ensemble of Convolutional Neural Networks. (arXiv:2306.03954v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03954
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#21363;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#38598;&#21512;&#26469;&#39640;&#25928;&#22320;&#35782;&#21035;&#25163;&#20889;&#27721;&#23383;&#65292;&#33021;&#22815;&#23454;&#29616;&#39640;&#36798;99.4%&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26085;&#26412;&#20070;&#20889;&#31995;&#32479;&#22797;&#26434;&#65292;&#21253;&#25324;&#24179;&#20551;&#21517;&#12289;&#29255;&#20551;&#21517;&#21644;&#27721;&#23383;&#19977;&#31181;&#23383;&#31526;&#31867;&#22411;&#12290;&#27721;&#23383;&#21253;&#25324;&#25968;&#21315;&#20010;&#19981;&#21516;&#30340;&#23383;&#31526;&#65292;&#36827;&#19968;&#27493;&#22686;&#21152;&#20102;&#23545;&#23383;&#31526;&#30340;&#35782;&#21035;&#21644;&#29702;&#35299;&#38590;&#24230;&#12290;&#23558;&#25163;&#20889;&#26085;&#35821;&#23383;&#31526;&#36716;&#25442;&#20026;&#25968;&#23383;&#25991;&#26412;&#23545;&#20110;&#25968;&#25454;&#20998;&#26512;&#12289;&#32763;&#35793;&#12289;&#23398;&#20064;&#21644;&#25991;&#21270;&#20445;&#25252;&#37117;&#24456;&#26377;&#29992;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#20998;&#26512;&#21644;&#35782;&#21035;&#25163;&#20889;&#26085;&#35821;&#23383;&#31526;&#65288;&#27721;&#23383;&#65289;&#12290;&#30740;&#31350;&#20351;&#29992;&#20102;&#19977;&#20010;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#38598;&#21512;&#26469;&#35782;&#21035;&#25163;&#20889;&#26085;&#35821;&#27721;&#23383;&#65292;&#24182;&#21033;&#29992; MNIST&#12289;K-MNIST&#12289;Kuzushiji-49&#65288;K49&#65289;&#20197;&#21450; Kuzushiji-Kanji&#65288;K-Kanji&#65289;&#20013;&#21069;150&#20010;&#37325;&#35201;&#31867;&#21035;&#30340;&#22235;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#24615;&#33021;&#35780;&#20272;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#25552;&#20986;&#30340;CNN-ensemble&#26550;&#26500;&#35782;&#21035;&#25163;&#20889;&#23383;&#31526;&#20855;&#26377;&#21487;&#34892;&#24615;&#65292;&#22312;MNIST&#19978;&#23454;&#29616;&#20102;99.4&#65285;&#12289;96.4&#65285;&#12289;95.0&#65285;&#21644;96.4&#65285;&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Japanese writing system is complex, with three character types of Hiragana, Katakana, and Kanji. Kanji consists of thousands of unique characters, further adding to the complexity of character identification and literature understanding. Being able to translate handwritten Japanese characters into digital text is useful for data analysis, translation, learning and cultural preservation. In this study, a machine learning approach to analyzing and recognizing handwritten Japanese characters (Kanji) is proposed. The study used an ensemble of three convolutional neural networks (CNNs) for recognizing handwritten Kanji characters and utilized four datasets of MNIST, K-MNIST, Kuzushiji-49 (K49) and the top 150 represented classes in the Kuzushiji-Kanji (K-Kanji) dataset for its performance evaluation. The results indicate feasibility of using proposed CNN-ensemble architecture for recognizing handwritten characters, achieving 99.4%, 96.4%, 95.0% and 96.4% classification accuracy on MNIST
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#19968;&#32452;&#29420;&#31435;&#26426;&#21046;&#30340;&#21453;&#21521;&#25805;&#20316;&#65292;&#20197;&#21457;&#29616;&#21644;&#20998;&#31163;&#19968;&#32452;&#29420;&#31435;&#30340;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2306.03938</link><description>&lt;p&gt;
&#36890;&#36807;&#27491;&#20132;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#22240;&#26524;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Learning Causal Mechanisms through Orthogonal Neural Networks. (arXiv:2306.03938v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03938
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#19968;&#32452;&#29420;&#31435;&#26426;&#21046;&#30340;&#21453;&#21521;&#25805;&#20316;&#65292;&#20197;&#21457;&#29616;&#21644;&#20998;&#31163;&#19968;&#32452;&#29420;&#31435;&#30340;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#26234;&#33021;&#30340;&#37325;&#35201;&#29305;&#24449;&#20043;&#19968;&#26159;&#33021;&#22815;&#20174;&#20302;&#32423;&#24863;&#23448;&#25968;&#25454;&#20013;&#25512;&#23548;&#20986;&#39640;&#32423;&#25277;&#35937;&#12290;&#36825;&#31181;&#25512;&#29702;&#30340;&#19968;&#20010;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#26159;&#21457;&#29616;&#27169;&#22359;&#21270;&#30340;&#29983;&#25104;&#26426;&#21046;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#22914;&#20309;&#22312;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#20174;&#22833;&#30495;&#30340;&#25968;&#25454;&#28857;&#20013;&#23398;&#20064;&#19968;&#32452;&#29420;&#31435;&#26426;&#21046;&#30340;&#21453;&#21521;&#25805;&#20316;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#36890;&#36807;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#65292;&#29616;&#26377;&#26426;&#22120;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#30340;&#19968;&#20010;&#37325;&#35201;&#24369;&#28857;&#22312;&#20110;&#36328;&#27169;&#22359;&#22810;&#26679;&#21270;&#19981;&#36275;&#12290;&#35299;&#20915;&#20154;&#24037;&#26234;&#33021;&#19982;&#26426;&#22120;&#26234;&#33021;&#20043;&#38388;&#30340;&#36825;&#19968;&#37325;&#35201;&#24046;&#36317;&#26159;&#27169;&#24335;&#35782;&#21035;&#31995;&#32479;&#30340;&#37325;&#35201;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#21487;&#20197;&#21457;&#29616;&#21644;&#20998;&#31163;&#19968;&#32452;&#29420;&#31435;&#30340;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
A fundamental feature of human intelligence is the ability to infer high-level abstractions from low-level sensory data. An essential component of such inference is the ability to discover modularized generative mechanisms. Despite many efforts to use statistical learning and pattern recognition for finding disentangled factors, arguably human intelligence remains unmatched in this area.  In this paper, we investigate a problem of learning, in a fully unsupervised manner, the inverse of a set of independent mechanisms from distorted data points. We postulate, and justify this claim with experimental results, that an important weakness of existing machine learning solutions lies in the insufficiency of cross-module diversification. Addressing this crucial discrepancy between human and machine intelligence is an important challenge for pattern recognition systems.  To this end, our work proposes an unsupervised method that discovers and disentangles a set of independent mechanisms from u
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#24341;&#23548;&#26368;&#21518;&#19968;&#23618;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#26368;&#36817;&#31867;&#22343;&#20540;(NCM)&#31934;&#30830;&#19988;&#39640;&#25928;&#22320;&#25311;&#21512;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#65292;&#24182;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.03937</link><description>&lt;p&gt;
&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#24341;&#23548;&#26368;&#21518;&#19968;&#23618;
&lt;/p&gt;
&lt;p&gt;
Guiding The Last Layer in Federated Learning with Pre-Trained Models. (arXiv:2306.03937v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03937
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#24341;&#23548;&#26368;&#21518;&#19968;&#23618;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#26368;&#36817;&#31867;&#22343;&#20540;(NCM)&#31934;&#30830;&#19988;&#39640;&#25928;&#22320;&#25311;&#21512;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#65292;&#24182;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;(Fl)&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#33539;&#24335;&#65292;&#20801;&#35768;&#22312;&#19981;&#20849;&#20139;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36328;&#22810;&#20010;&#21442;&#19982;&#32773;&#35757;&#32451;&#27169;&#22411;&#12290;&#26368;&#36817;&#30340;&#19968;&#20123;&#24037;&#20316;&#24320;&#22987;&#32771;&#34385;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#20316;&#20026;&#29616;&#26377;FL&#31639;&#27861;&#30340;&#21021;&#22987;&#21270;&#28857;&#30340;&#24433;&#21709;; &#20294;&#26159;&#65292;&#36825;&#20123;&#26041;&#27861;&#24573;&#30053;&#20102;&#38598;&#20013;&#24335;&#23398;&#20064;&#35774;&#32622;&#20013;&#22823;&#37327;&#26377;&#25928;&#30340;&#36801;&#31227;&#23398;&#20064;&#25991;&#29486;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#20808;&#21069;&#24037;&#20316;&#20013;&#32771;&#34385;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;FL&#38382;&#39064;&#65292;&#24182;&#23558;&#20854;&#25193;&#23637;&#21040;&#19968;&#32452;&#35745;&#31639;&#26426;&#35270;&#35273;&#36801;&#31227;&#23398;&#20064;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#35266;&#23519;&#21040;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#20165;&#25311;&#21512;&#32447;&#24615;&#20998;&#31867;&#22836;&#26159;&#39640;&#25928;&#19988;&#26377;&#25928;&#30340;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;FL&#35774;&#32622;&#20013;&#65292;&#20351;&#29992;&#26368;&#36817;&#31867;&#22343;&#20540;(NCM)&#25311;&#21512;&#20998;&#31867;&#22120;&#21487;&#20197;&#31934;&#30830;&#22320;&#19988;&#27604;&#29616;&#26377;&#25552;&#35758;&#39640;&#25928;&#22320;&#23436;&#25104;&#65292;&#21516;&#26102;&#33719;&#24471;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20351;&#29992;&#20004;&#38454;&#27573;&#26041;&#27861;&#33719;&#24471;&#20998;&#31867;&#22120;&#65292;&#28982;&#21518;&#24494;&#35843;&#27169;&#22411;&#21487;&#20197;&#20135;&#29983;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is an emerging paradigm that allows a model to be trained across a number of participants without sharing data. Recent works have begun to consider the effects of using pre-trained models as an initialization point for existing FL algorithms; however, these approaches ignore the vast body of efficient transfer learning literature from the centralized learning setting. Here we revisit the problem of FL from a pre-trained model considered in prior work and expand it to a set of computer vision transfer learning problems. We first observe that simply fitting a linear classification head can be efficient and effective in many cases. We then show that in the FL setting, fitting a classifier using the Nearest Class Means (NCM) can be done exactly and orders of magnitude more efficiently than existing proposals, while obtaining strong performance. Finally, we demonstrate that using a two-phase approach of obtaining the classifier and then fine-tuning the model can yiel
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#32622;&#25442;&#19981;&#21464;&#30340;&#39640;&#32500;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#21518;&#23558;&#20854;&#29992;&#20110;&#39640;&#33021;&#29289;&#29702;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#35782;&#21035;&#20986;&#22312;&#20165;&#20855;&#22791;&#32972;&#26223;&#20551;&#35774;&#19979;&#25490;&#38500;&#24322;&#24120;&#30340;&#21943;&#27880;&#12290;</title><link>http://arxiv.org/abs/2306.03933</link><description>&lt;p&gt;
&#39640;&#32500;&#21644;&#32622;&#25442;&#19981;&#21464;&#24322;&#24120;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-dimensional and Permutation Invariant Anomaly Detection. (arXiv:2306.03933v1 [hep-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03933
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#32622;&#25442;&#19981;&#21464;&#30340;&#39640;&#32500;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#21518;&#23558;&#20854;&#29992;&#20110;&#39640;&#33021;&#29289;&#29702;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#35782;&#21035;&#20986;&#22312;&#20165;&#20855;&#22791;&#32972;&#26223;&#20551;&#35774;&#19979;&#25490;&#38500;&#24322;&#24120;&#30340;&#21943;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#23398;&#20064;&#39640;&#32500;&#27010;&#29575;&#23494;&#24230;&#30340;&#22256;&#38590;&#65292;&#26032;&#29289;&#29702;&#36807;&#31243;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#36890;&#24120;&#23616;&#38480;&#20110;&#20302;&#32500;&#31354;&#38388;&#12290;&#29305;&#21035;&#26159;&#22312;&#25104;&#20998;&#32423;&#21035;&#19978;&#65292;&#23558;&#32622;&#25442;&#19981;&#21464;&#24615;&#21644;&#21487;&#21464;&#38271;&#24230;&#30340;&#36755;&#20837;&#31561;&#33391;&#22909;&#24615;&#36136;&#21512;&#24182;&#21040;&#27969;&#34892;&#30340;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;&#20013;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#31890;&#23376;&#29289;&#29702;&#25968;&#25454;&#32622;&#25442;&#19981;&#21464;&#23494;&#24230;&#20272;&#35745;&#22120;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#22788;&#29702;&#21487;&#21464;&#38271;&#24230;&#30340;&#36755;&#20837;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#23398;&#20064;&#21040;&#30340;&#23494;&#24230;&#29992;&#20316;&#32622;&#25442;&#19981;&#21464;&#30340;&#24322;&#24120;&#26816;&#27979;&#35780;&#20998;&#26469;&#23637;&#31034;&#25105;&#20204;&#26041;&#27861;&#30340;&#21151;&#25928;&#65292;&#26377;&#25928;&#22320;&#35782;&#21035;&#20986;&#22312;&#20165;&#20855;&#22791;&#32972;&#26223;&#20551;&#35774;&#19979;&#30340;&#21487;&#33021;&#24615;&#36739;&#20302;&#30340;&#21943;&#27880;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#30340;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23398;&#20064;&#21040;&#30340;&#23494;&#24230;&#27604;&#19982;&#34987;&#30417;&#30563;&#20998;&#31867;&#31639;&#27861;&#33719;&#24471;&#30340;&#23494;&#24230;&#20043;&#38388;&#30340;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Methods for anomaly detection of new physics processes are often limited to low-dimensional spaces due to the difficulty of learning high-dimensional probability densities. Particularly at the constituent level, incorporating desirable properties such as permutation invariance and variable-length inputs becomes difficult within popular density estimation methods. In this work, we introduce a permutation-invariant density estimator for particle physics data based on diffusion models, specifically designed to handle variable-length inputs. We demonstrate the efficacy of our methodology by utilizing the learned density as a permutation-invariant anomaly detection score, effectively identifying jets with low likelihood under the background-only hypothesis. To validate our density estimation method, we investigate the ratio of learned densities and compare to those obtained by a supervised classification algorithm.
&lt;/p&gt;</description></item><item><title>ChatDB&#39033;&#30446;&#23558;SQL&#25968;&#25454;&#24211;&#20316;&#20026;&#31526;&#21495;&#20869;&#23384;&#65292;&#22686;&#24378;LLMs&#30340;&#22797;&#26434;&#22810;&#36339;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.03901</link><description>&lt;p&gt;
ChatDB: &#23558;&#25968;&#25454;&#24211;&#20316;&#20026;&#31526;&#21495;&#20869;&#23384;&#22686;&#24378;LLMs
&lt;/p&gt;
&lt;p&gt;
ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory. (arXiv:2306.03901v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03901
&lt;/p&gt;
&lt;p&gt;
ChatDB&#39033;&#30446;&#23558;SQL&#25968;&#25454;&#24211;&#20316;&#20026;&#31526;&#21495;&#20869;&#23384;&#65292;&#22686;&#24378;LLMs&#30340;&#22797;&#26434;&#22810;&#36339;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20869;&#23384;&#26159;&#35745;&#31639;&#36890;&#29992;&#30340;&#12290;&#28982;&#32780;&#65292;&#20027;&#27969;LLMs&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#20869;&#23384;&#65292;&#24182;&#19988;&#35774;&#35745;&#21463;&#21040;&#29983;&#29289;&#22823;&#33041;&#30340;&#20005;&#37325;&#24433;&#21709;&#12290;&#30001;&#20110;&#20854;&#36817;&#20284;&#24615;&#36136;&#21644;&#38169;&#35823;&#32047;&#31215;&#20542;&#21521;&#65292;&#20256;&#32479;&#31070;&#32463;&#20869;&#23384;&#26426;&#21046;&#19981;&#33021;&#25903;&#25345;LLMs&#27169;&#25311;&#22797;&#26434;&#25512;&#29702;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#29616;&#20195;&#35745;&#31639;&#26426;&#26550;&#26500;&#20013;&#23547;&#27714;&#28789;&#24863;&#65292;&#20026;&#22797;&#26434;&#30340;&#22810;&#36339;&#25512;&#29702;&#22686;&#24378;LLMs&#31526;&#21495;&#20869;&#23384;&#12290;&#36825;&#26679;&#30340;&#31526;&#21495;&#20869;&#23384;&#26694;&#26550;&#34987;&#23454;&#20363;&#21270;&#20026;&#19968;&#20010;LLM&#21644;&#19968;&#32452;SQL&#25968;&#25454;&#24211;&#65292;&#20854;&#20013;LLM&#29983;&#25104;SQL&#25351;&#20196;&#20197;&#25805;&#20316;SQL&#25968;&#25454;&#24211;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#38656;&#35201;&#22797;&#26434;&#25512;&#29702;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#30340;&#20869;&#23384;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290; &#39033;&#30446;&#32593;&#31449;&#20301;&#20110;https://chatdatabase.github.io/&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) with memory are computationally universal. However, mainstream LLMs are not taking full advantage of memory, and the designs are heavily influenced by biological brains. Due to their approximate nature and proneness to the accumulation of errors, conventional neural memory mechanisms cannot support LLMs to simulate complex reasoning. In this paper, we seek inspiration from modern computer architectures to augment LLMs with symbolic memory for complex multi-hop reasoning. Such a symbolic memory framework is instantiated as an LLM and a set of SQL databases, where the LLM generates SQL instructions to manipulate the SQL databases. We validate the effectiveness of the proposed memory framework on a synthetic dataset requiring complex reasoning. The project website is available at https://chatdatabase.github.io/ .
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#24212;&#29992;&#28436;&#32462;&#39564;&#35777;&#25216;&#26415;&#65292;&#20351;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36827;&#34892;&#26126;&#30830;&#32780;&#20005;&#35880;&#30340;&#28436;&#32462;&#25512;&#29702;&#65292;&#20197;&#30830;&#20445;&#20854;&#25512;&#29702;&#36807;&#31243;&#30340;&#21487;&#20449;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.03872</link><description>&lt;p&gt;
&#24212;&#29992;&#28436;&#32462;&#39564;&#35777;&#25216;&#26415;&#39564;&#35777;&#24605;&#32500;&#38142;&#30340;&#25512;&#29702;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Deductive Verification of Chain-of-Thought Reasoning. (arXiv:2306.03872v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03872
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#24212;&#29992;&#28436;&#32462;&#39564;&#35777;&#25216;&#26415;&#65292;&#20351;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36827;&#34892;&#26126;&#30830;&#32780;&#20005;&#35880;&#30340;&#28436;&#32462;&#25512;&#29702;&#65292;&#20197;&#30830;&#20445;&#20854;&#25512;&#29702;&#36807;&#31243;&#30340;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#25512;&#29702;&#20219;&#21153;&#20013;&#21463;&#30410;&#21290;&#27973;&#65292;&#29305;&#21035;&#26159;&#24212;&#29992;&#24605;&#32500;&#38142;&#25552;&#31034;&#21487;&#20197;&#20351;&#27169;&#22411;&#20135;&#29983;&#26356;&#20840;&#38754;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#24605;&#32500;&#38142;&#30340;&#24378;&#35843;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#21487;&#33021;&#20250;&#19981;&#24910;&#23548;&#33268;&#20135;&#29983;&#24187;&#35273;&#21644;&#32047;&#31215;&#38169;&#35823;&#65292;&#20174;&#32780;&#38480;&#21046;&#27169;&#22411;&#35299;&#20915;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#28789;&#24863;&#26469;&#33258;&#20110;&#20154;&#31867;&#22914;&#20309;&#36827;&#34892;&#32454;&#33268;&#30340;&#28436;&#32462;&#36923;&#36753;&#25512;&#29702;&#36807;&#31243;&#26469;&#35299;&#20915;&#20219;&#21153;&#65292;&#25105;&#20204;&#26088;&#22312;&#20351;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36827;&#34892;&#26126;&#30830;&#32780;&#20005;&#35880;&#30340;&#28436;&#32462;&#25512;&#29702;&#65292;&#24182;&#36890;&#36807;&#33258;&#25105;&#39564;&#35777;&#30830;&#20445;&#25512;&#29702;&#36807;&#31243;&#30340;&#21487;&#20449;&#24230;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26159;&#20687;ChatGPT&#36825;&#26679;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#30452;&#25509;&#39564;&#35777;&#25972;&#20010;&#28436;&#32462;&#25512;&#29702;&#36807;&#31243;&#30340;&#26377;&#25928;&#24615;&#20063;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#25512;&#29702;&#39564;&#35777;&#36807;&#31243;&#20998;&#35299;&#20026;&#19968;&#31995;&#21015;&#36880;&#27493;&#30340;&#23376;&#36807;&#31243;&#65292;&#27599;&#20010;&#36807;&#31243;&#21482;&#25509;&#25910;&#20854;&#24517;&#35201;&#30340;&#19978;&#19979;&#25991;&#21644;&#21069;&#25552;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) significantly benefit from Chain-of-Thought (CoT) prompting in performing various reasoning tasks. While CoT allows models to produce more comprehensive reasoning processes, its emphasis on intermediate reasoning steps can inadvertently introduce hallucinations and accumulated errors, thereby limiting models' ability to solve complex reasoning tasks. Inspired by how humans engage in careful and meticulous deductive logical reasoning processes to solve tasks, we seek to enable language models to perform explicit and rigorous deductive reasoning, and also ensure the trustworthiness of their reasoning process through self-verification. However, directly verifying the validity of an entire deductive reasoning process is challenging, even with advanced models like ChatGPT. In light of this, we propose to decompose a reasoning verification process into a series of step-by-step subprocesses, each only receiving their necessary context and premises. To facilitate t
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;ChatGPT&#25216;&#26415;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#20174;&#36130;&#32463;&#26032;&#38395;&#20013;&#25552;&#21462;&#20986;&#19981;&#26029;&#21464;&#21270;&#30340;&#32593;&#32476;&#32467;&#26500;&#65292;&#24182;&#29992;&#20110;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;&#65292;&#33719;&#24471;&#20102;&#36229;&#36807;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#22522;&#20934;&#30340;&#34920;&#29616;&#65292;&#25552;&#31034;&#20102;ChatGPT&#22312;&#25991;&#26412;&#25512;&#26029;&#21644;&#37329;&#34701;&#39044;&#27979;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.03763</link><description>&lt;p&gt;
ChatGPT&#20449;&#24687;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
ChatGPT Informed Graph Neural Network for Stock Movement Prediction. (arXiv:2306.03763v1 [q-fin.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03763
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;ChatGPT&#25216;&#26415;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#20174;&#36130;&#32463;&#26032;&#38395;&#20013;&#25552;&#21462;&#20986;&#19981;&#26029;&#21464;&#21270;&#30340;&#32593;&#32476;&#32467;&#26500;&#65292;&#24182;&#29992;&#20110;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;&#65292;&#33719;&#24471;&#20102;&#36229;&#36807;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#22522;&#20934;&#30340;&#34920;&#29616;&#65292;&#25552;&#31034;&#20102;ChatGPT&#22312;&#25991;&#26412;&#25512;&#26029;&#21644;&#37329;&#34701;&#39044;&#27979;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#24050;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20174;&#26102;&#38388;&#25991;&#26412;&#25968;&#25454;&#65288;&#23588;&#20854;&#26159;&#36130;&#32463;&#26032;&#38395;&#65289;&#25512;&#26029;&#21160;&#24577;&#32593;&#32476;&#32467;&#26500;&#30340;&#28508;&#21147;&#20173;&#26159;&#19968;&#20010;&#26410;&#24320;&#21457;&#30340;&#39046;&#22495;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;ChatGPT&#30340;&#22270;&#25512;&#26029;&#33021;&#21147;&#26469;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#24039;&#22937;&#22320;&#20174;&#25991;&#26412;&#25968;&#25454;&#20013;&#25552;&#21462;&#20986;&#19981;&#26029;&#21464;&#21270;&#30340;&#32593;&#32476;&#32467;&#26500;&#65292;&#24182;&#23558;&#36825;&#20123;&#32593;&#32476;&#32467;&#26500;&#34701;&#21512;&#21040;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#36827;&#34892;&#21518;&#32493;&#30340;&#39044;&#27979;&#20219;&#21153;&#12290;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22987;&#32456;&#20248;&#20110;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#22522;&#20934;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#25105;&#20204;&#27169;&#22411;&#30340;&#20135;&#20986;&#26500;&#24314;&#30340;&#32452;&#21512;&#23637;&#31034;&#20986;&#26356;&#39640;&#30340;&#24180;&#21270;&#32047;&#35745;&#22238;&#25253;&#12289;&#26356;&#20302;&#30340;&#27874;&#21160;&#24615;&#21644;&#26368;&#22823;&#22238;&#25764;&#12290;&#36825;&#31181;&#21331;&#36234;&#34920;&#29616;&#31361;&#26174;&#20102;ChatGPT&#29992;&#20110;&#22522;&#20110;&#25991;&#26412;&#30340;&#32593;&#32476;&#25512;&#26029;&#21644;&#37329;&#34701;&#39044;&#27979;&#24212;&#29992;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT has demonstrated remarkable capabilities across various natural language processing (NLP) tasks. However, its potential for inferring dynamic network structures from temporal textual data, specifically financial news, remains an unexplored frontier. In this research, we introduce a novel framework that leverages ChatGPT's graph inference capabilities to enhance Graph Neural Networks (GNN). Our framework adeptly extracts evolving network structures from textual data, and incorporates these networks into graph neural networks for subsequent predictive tasks. The experimental results from stock movement forecasting indicate our model has consistently outperformed the state-of-the-art Deep Learning-based benchmarks. Furthermore, the portfolios constructed based on our model's outputs demonstrate higher annualized cumulative returns, alongside reduced volatility and maximum drawdown. This superior performance highlights the potential of ChatGPT for text-based network inferences and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#38024;&#23545;&#32469;&#21475;&#20196;&#29983;&#25104;&#30340;&#22522;&#20110;&#38899;&#38901;&#23398;&#30340;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#65292;&#25552;&#20379;&#20102;TwistList&#25968;&#25454;&#38598;&#21644;TwisterMisters&#22522;&#20934;&#31995;&#32479;&#65292;&#24182;&#39564;&#35777;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#27809;&#26377;&#20219;&#21153;&#29305;&#23450;&#25968;&#25454;&#21644;&#26174;&#24335;&#38899;&#38901;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#30340;&#33391;&#22909;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.03457</link><description>&lt;p&gt;
&#22522;&#20110;&#38899;&#38901;&#23398;&#30340;&#35821;&#35328;&#29983;&#25104;&#65306;&#20197;&#32469;&#21475;&#20196;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Phonetically-Grounded Language Generation: The Case of Tongue Twisters. (arXiv:2306.03457v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03457
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#38024;&#23545;&#32469;&#21475;&#20196;&#29983;&#25104;&#30340;&#22522;&#20110;&#38899;&#38901;&#23398;&#30340;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#65292;&#25552;&#20379;&#20102;TwistList&#25968;&#25454;&#38598;&#21644;TwisterMisters&#22522;&#20934;&#31995;&#32479;&#65292;&#24182;&#39564;&#35777;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#27809;&#26377;&#20219;&#21153;&#29305;&#23450;&#25968;&#25454;&#21644;&#26174;&#24335;&#38899;&#38901;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#30340;&#33391;&#22909;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#38899;&#38901;&#23398;&#35821;&#35328;&#29983;&#25104;&#20027;&#35201;&#38598;&#20013;&#22312;&#35789;&#27468;&#21644;&#35799;&#27468;&#31561;&#39046;&#22495;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#22260;&#32469;&#32469;&#21475;&#20196;&#29983;&#25104;&#23637;&#24320;&#30340;&#24037;&#20316;&#65292;&#32469;&#21475;&#20196;&#38656;&#35201;&#22312;&#20445;&#25345;&#35821;&#20041;&#27491;&#30830;&#24615;&#30340;&#21516;&#26102;&#65292;&#26368;&#22823;&#21270;&#38899;&#39057;&#37325;&#21472;&#24182;&#20445;&#25345;&#35821;&#27861;&#27491;&#30830;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;TwistList&#65292;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;2.1K&#20154;&#24037;&#32534;&#20889;&#30340;&#32469;&#21475;&#20196;&#30340;&#22823;&#22411;&#27880;&#37322;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#38024;&#23545;&#32469;&#21475;&#20196;&#29983;&#25104;&#25552;&#20986;&#20102;&#19968;&#20123;&#22522;&#20934;&#31995;&#32479;(TwisterMisters)&#65292;&#21253;&#25324;&#38656;&#35201;&#21644;&#19981;&#38656;&#35201;&#22312;&#22495;&#20869;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;&#33258;&#21160;&#21644;&#20154;&#24037;&#35780;&#20272;&#30340;&#32467;&#26524;&#26469;&#35777;&#26126;&#29616;&#26377;&#20027;&#27969;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#27492;&#20219;&#21153;&#20013;&#24615;&#33021;&#20248;&#33391;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#20219;&#21153;&#29305;&#23450;&#35757;&#32451;&#25968;&#25454;&#21644;&#26174;&#24335;&#38899;&#38901;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#32469;&#21475;&#20196;&#29983;&#25104;&#30340;&#20219;&#21153;&#26159;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Previous work in phonetically-grounded language generation has mainly focused on domains such as lyrics and poetry. In this paper, we present work on the generation of tongue twisters - a form of language that is required to be phonetically conditioned to maximise sound overlap, whilst maintaining semantic consistency with an input topic, and still being grammatically correct. We present \textbf{TwistList}, a large annotated dataset of tongue twisters, consisting of 2.1K+ human-authored examples. We additionally present several benchmark systems (referred to as TwisterMisters) for the proposed task of tongue twister generation, including models that both do and do not require training on in-domain data. We present the results of automatic and human evaluation to demonstrate the performance of existing mainstream pre-trained models in this task with limited (or no) task specific training and data, and no explicit phonetic knowledge. We find that the task of tongue twister generation is 
&lt;/p&gt;</description></item><item><title>Vid2Act&#26159;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#19990;&#30028;&#27169;&#22411;&#26469;&#20256;&#36755;&#39046;&#22495;&#30456;&#20851;&#30340;&#21160;&#24577;&#21644;&#31574;&#30053;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#26679;&#26412;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.03360</link><description>&lt;p&gt;
Vid2Act&#65306;&#20026;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#28608;&#27963;&#31163;&#32447;&#35270;&#39057;
&lt;/p&gt;
&lt;p&gt;
Vid2Act: Activate Offline Videos for Visual RL. (arXiv:2306.03360v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03360
&lt;/p&gt;
&lt;p&gt;
Vid2Act&#26159;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#19990;&#30028;&#27169;&#22411;&#26469;&#20256;&#36755;&#39046;&#22495;&#30456;&#20851;&#30340;&#21160;&#24577;&#21644;&#31574;&#30053;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31163;&#32447;&#35270;&#39057;&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#26159;&#25552;&#39640;&#20854;&#22312;&#32447;&#20219;&#21153;&#25928;&#29575;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#20294;&#30001;&#20110;&#36328;&#22495;&#20013;&#20219;&#21153;&#12289;&#21160;&#24577;&#21644;&#34892;&#20026;&#30340;&#22266;&#26377;&#19981;&#21305;&#37197;&#24615;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26368;&#36817;&#65292;&#19968;&#31181;&#21517;&#20026;APV&#30340;&#27169;&#22411;&#36991;&#20813;&#20102;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#30340;&#20276;&#38543;&#21160;&#20316;&#35760;&#24405;&#65292;&#32780;&#26159;&#19987;&#27880;&#20110;&#22312;&#28304;&#22495;&#20869;&#39044;&#35757;&#32451;&#19982;&#20219;&#21153;&#26080;&#20851;&#30340;&#12289;&#19981;&#28041;&#21450;&#25805;&#20316;&#30340;&#19990;&#30028;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Vid2Act&#65292;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#23398;&#20064;&#20174;&#31163;&#32447;&#21040;&#22312;&#32447;&#29615;&#22659;&#20013;&#20256;&#36755;&#26377;&#20215;&#20540;&#30340;&#21160;&#20316;&#26465;&#20214;&#21160;&#24577;&#21644;&#28508;&#22312;&#26377;&#29992;&#30340;&#21160;&#20316;&#28436;&#31034;&#12290;&#20854;&#20027;&#35201;&#24605;&#24819;&#26159;&#19981;&#20165;&#23558;&#19990;&#30028;&#27169;&#22411;&#29992;&#20316;&#34892;&#20026;&#23398;&#20064;&#30340;&#27169;&#25311;&#22120;&#65292;&#36824;&#23558;&#20854;&#29992;&#20316;&#27979;&#37327;&#39046;&#22495;&#30456;&#20851;&#24615;&#30340;&#24037;&#20855;&#65292;&#20197;&#20415;&#36827;&#34892;&#21160;&#24577;&#34920;&#31034;&#20256;&#36755;&#21644;&#31574;&#30053;&#20256;&#36755;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#36890;&#36807;&#22495;&#36873;&#25321;&#30693;&#35782;&#33976;&#39311;&#25439;&#22833;&#35757;&#32451;&#19990;&#30028;&#27169;&#22411;&#29983;&#25104;&#19968;&#32452;&#26102;&#38388;&#21464;&#21270;&#30340;&#20219;&#21153;&#30456;&#20284;&#24230;&#12290;&#36825;&#20123;&#30456;&#20284;&#24230;&#26377;&#20004;&#20010;&#30446;&#30340;&#65306;&#65288;i&#65289;&#33258;&#36866;&#24212;&#22320;&#23558;&#26368;&#30456;&#20851;&#30340;&#39046;&#22495;&#30340;&#21160;&#24577;&#20256;&#36755;&#21040;&#22312;&#32447;&#29615;&#22659;&#65292;&#21644;&#65288;ii&#65289;&#22312;&#22312;&#32447;&#29615;&#22659;&#20013;&#25351;&#23548;&#20195;&#29702;&#38598;&#20013;&#25191;&#34892;&#20219;&#21153;&#30456;&#20851;&#30340;&#21160;&#20316;&#12290;&#22312;Atari&#21644;DMControl&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20854;&#22312;&#26679;&#26412;&#25928;&#29575;&#26041;&#38754;&#22823;&#22823;&#20248;&#20110;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretraining RL models on offline video datasets is a promising way to improve their training efficiency in online tasks, but challenging due to the inherent mismatch in tasks, dynamics, and behaviors across domains. A recent model, APV, sidesteps the accompanied action records in offline datasets and instead focuses on pretraining a task-irrelevant, action-free world model within the source domains. We present Vid2Act, a model-based RL method that learns to transfer valuable action-conditioned dynamics and potentially useful action demonstrations from offline to online settings. The main idea is to use the world models not only as simulators for behavior learning but also as tools to measure the domain relevance for both dynamics representation transfer and policy transfer. Specifically, we train the world models to generate a set of time-varying task similarities using a domain-selective knowledge distillation loss. These similarities serve two purposes: (i) adaptively transferring th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#25512;&#29702;&#26102;&#38388;&#24178;&#39044;&#65288;ITI&#65289;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#36328;&#36234;&#26377;&#38480;&#25968;&#37327;&#30340;&#27880;&#24847;&#21147;&#22836;&#65292;&#26174;&#30528;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#12290;&#22312;TruthfulQA&#22522;&#20934;&#19978;&#65292;ITI&#20351;LLaMA&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#20174;32.5%&#25552;&#39640;&#21040;65.1%&#12290;ITI&#26159;&#19968;&#31181;&#26368;&#23567;&#31243;&#24230;&#30340;&#24178;&#25200;&#65292;&#35745;&#31639;&#24265;&#20215;&#65292;&#19988;&#25968;&#25454;&#25928;&#29575;&#39640;&#12290;</title><link>http://arxiv.org/abs/2306.03341</link><description>&lt;p&gt;
&#25512;&#29702;&#26102;&#38388;&#24178;&#39044;&#65306;&#20174;&#35821;&#35328;&#27169;&#22411;&#20013;&#24341;&#23548;&#20986;&#30495;&#23454;&#30340;&#31572;&#26696;
&lt;/p&gt;
&lt;p&gt;
Inference-Time Intervention: Eliciting Truthful Answers from a Language Model. (arXiv:2306.03341v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03341
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#25512;&#29702;&#26102;&#38388;&#24178;&#39044;&#65288;ITI&#65289;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#36328;&#36234;&#26377;&#38480;&#25968;&#37327;&#30340;&#27880;&#24847;&#21147;&#22836;&#65292;&#26174;&#30528;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#12290;&#22312;TruthfulQA&#22522;&#20934;&#19978;&#65292;ITI&#20351;LLaMA&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#20174;32.5%&#25552;&#39640;&#21040;65.1%&#12290;ITI&#26159;&#19968;&#31181;&#26368;&#23567;&#31243;&#24230;&#30340;&#24178;&#25200;&#65292;&#35745;&#31639;&#24265;&#20215;&#65292;&#19988;&#25968;&#25454;&#25928;&#29575;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#25512;&#29702;&#26102;&#38388;&#24178;&#39044;&#65288;ITI&#65289;&#25216;&#26415;&#65292;&#26088;&#22312;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#30495;&#23454;&#24615;&#12290;ITI&#36890;&#36807;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#27839;&#30528;&#19968;&#32452;&#26041;&#21521;&#31227;&#21160;&#27169;&#22411;&#28608;&#27963;&#65292;&#36328;&#36234;&#26377;&#38480;&#25968;&#37327;&#30340;&#27880;&#24847;&#21147;&#22836;&#12290;&#36825;&#31181;&#24178;&#39044;&#26174;&#30528;&#25552;&#39640;&#20102;LLaMA&#27169;&#22411;&#22312;TruthfulQA&#22522;&#20934;&#19978;&#30340;&#34920;&#29616;&#12290;&#22312;&#25351;&#20196;&#24494;&#35843;&#30340;LLaMA Alpaca&#19978;&#65292;ITI&#23558;&#20854;&#30495;&#23454;&#24615;&#20174;32.5&#65285;&#25552;&#39640;&#21040;65.1&#65285;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#30495;&#23454;&#24615;&#21644;&#21487;&#29992;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24182;&#28436;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#35843;&#25972;&#24178;&#39044;&#24378;&#24230;&#26469;&#24179;&#34913;&#23427;&#12290;ITI &#21462;&#24471;&#20102;&#26368;&#20302;&#31243;&#24230;&#30340;&#24178;&#25200;&#19988;&#35745;&#31639;&#24265;&#20215;&#12290;&#27492;&#22806;&#65292;&#35813;&#25216;&#26415;&#22312;&#25968;&#25454;&#25928;&#29575;&#19978;&#34920;&#29616;&#20248;&#24322;&#65306;&#34429;&#28982;&#20687;RLHF&#36825;&#26679;&#30340;&#26041;&#27861;&#38656;&#35201;&#24191;&#27867;&#27880;&#37322;&#65292;&#20294;&#26159;ITI&#20165;&#20351;&#29992;&#20102;&#20960;&#30334;&#20010;&#20363;&#23376;&#23601;&#33021;&#23450;&#20301;&#30495;&#23454;&#30340;&#26041;&#21521;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;LLMs&#21487;&#33021;&#20855;&#26377;&#26576;&#31181;&#20869;&#37096;&#34920;&#31034;&#26041;&#27861;&#26469;&#34920;&#31034;&#26576;&#20107;&#26159;&#30495;&#23454;&#30340;&#21487;&#33021;&#24615;&#65292;&#21363;&#20351;&#23427;&#20204;&#22312;&#34920;&#38754;&#19978;&#20135;&#29983;&#20102;&#34394;&#20551;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Inference-Time Intervention (ITI), a technique designed to enhance the truthfulness of large language models (LLMs). ITI operates by shifting model activations during inference, following a set of directions across a limited number of attention heads. This intervention significantly improves the performance of LLaMA models on the TruthfulQA benchmark. On an instruction-finetuned LLaMA called Alpaca, ITI improves its truthfulness from 32.5% to 65.1%. We identify a tradeoff between truthfulness and helpfulness and demonstrate how to balance it by tuning the intervention strength. ITI is minimally invasive and computationally inexpensive. Moreover, the technique is data efficient: while approaches like RLHF require extensive annotations, ITI locates truthful directions using only few hundred examples. Our findings suggest that LLMs may have an internal representation of the likelihood of something being true, even as they produce falsehoods on the surface.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;APA&#65292;&#20854;&#37319;&#29992;&#20248;&#21183;&#35825;&#23548;&#31574;&#30053;&#23545;&#40784;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#35821;&#35328;&#27169;&#22411;&#12290;&#30456;&#23545;&#20110;&#20256;&#32479;&#26041;&#27861;&#65288;PPO&#65289;&#65292;APA&#22312;&#35821;&#35328;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#22909;&#65292;&#36991;&#20813;&#20102;&#27169;&#22411;&#30340;&#23849;&#28291;&#19982;&#19981;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.02231</link><description>&lt;p&gt;
&#37319;&#29992;&#20248;&#21183;&#35825;&#23548;&#31574;&#30053;&#23545;&#40784;&#30340;Fine-Tuning&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Fine-Tuning Language Models with Advantage-Induced Policy Alignment. (arXiv:2306.02231v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02231
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;APA&#65292;&#20854;&#37319;&#29992;&#20248;&#21183;&#35825;&#23548;&#31574;&#30053;&#23545;&#40784;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#35821;&#35328;&#27169;&#22411;&#12290;&#30456;&#23545;&#20110;&#20256;&#32479;&#26041;&#27861;&#65288;PPO&#65289;&#65292;APA&#22312;&#35821;&#35328;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#22909;&#65292;&#36991;&#20813;&#20102;&#27169;&#22411;&#30340;&#23849;&#28291;&#19982;&#19981;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#24050;&#32463;&#25104;&#20026;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#30340;&#21487;&#38752;&#26041;&#27861;&#12290;&#22312;&#20247;&#22810;RLHF&#25216;&#26415;&#20013;&#65292;&#25509;&#36817;&#31574;&#30053;&#20248;&#21270;&#65288;PPO&#65289;&#26159;&#26368;&#24120;&#29992;&#30340;&#26041;&#27861;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;PPO&#24456;&#27969;&#34892;&#65292;&#20294;&#23427;&#21487;&#33021;&#20250;&#36973;&#21463;&#27169;&#24335;&#23849;&#28291;&#12289;&#19981;&#31283;&#23450;&#21644;&#25928;&#29575;&#20302;&#19979;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;--&#22522;&#20110;&#20272;&#35745;&#20248;&#21183;&#30340;&#24179;&#26041;&#35823;&#24046;&#25439;&#22833;&#20989;&#25968;&#30340;&#20248;&#21183;&#35825;&#23548;&#31574;&#30053;&#23545;&#40784;&#65288;APA&#65289;&#65292;&#21487;&#20197;&#20943;&#36731;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#24403;&#20351;&#29992;&#21333;&#29420;&#30340;&#22870;&#21169;&#27169;&#22411;&#20316;&#20026;&#35780;&#20272;&#22120;&#26102;&#65292;APA&#22312;&#35821;&#35328;&#20219;&#21153;&#20013;&#22987;&#32456;&#27604;PPO&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#19982;PPO&#30456;&#27604;&#65292;APA&#21487;&#20197;&#26356;&#31283;&#23450;&#22320;&#25511;&#21046;&#27169;&#22411;&#19982;&#21021;&#22987;&#31574;&#30053;&#30340;&#20559;&#24046;&#65292;&#30830;&#20445;&#27169;&#22411;&#25552;&#39640;&#24615;&#33021;&#32780;&#19981;&#20250;&#23849;&#28291;&#20026;&#30830;&#23450;&#24615;&#36755;&#20986;&#12290;&#38500;&#20102;&#32463;&#39564;&#32467;&#26524;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;APA&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning from human feedback (RLHF) has emerged as a reliable approach to aligning large language models (LLMs) to human preferences. Among the plethora of RLHF techniques, proximal policy optimization (PPO) is of the most widely used methods. Despite its popularity, however, PPO may suffer from mode collapse, instability, and poor sample efficiency. We show that these issues can be alleviated by a novel algorithm that we refer to as Advantage-Induced Policy Alignment (APA), which leverages a squared error loss function based on the estimated advantages. We demonstrate empirically that APA consistently outperforms PPO in language tasks by a large margin, when a separate reward model is employed as the evaluator. In addition, compared with PPO, APA offers a more stable form of control over the deviation from the model's initial policy, ensuring that the model improves its performance without collapsing to deterministic output. In addition to empirical results, we also prov
&lt;/p&gt;</description></item><item><title>SGEM&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#21033;&#29992;&#27874;&#26463;&#25628;&#32034;&#21644;&#24191;&#20041;&#29109;&#26368;&#23567;&#21270;&#35843;&#25972;&#39044;&#35757;&#32451;ASR&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#19977;&#20010;&#20027;&#27969;ASR&#27169;&#22411;&#22312;&#19981;&#21516;&#39046;&#22495;&#36716;&#21464;&#26102;&#30340;&#26368;&#26032;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.01981</link><description>&lt;p&gt;
SGEM&#65306;&#36890;&#36807;&#24207;&#21015;&#32423;&#24191;&#20041;&#29109;&#26368;&#23567;&#21270;&#23454;&#29616;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30340;&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
SGEM: Test-Time Adaptation for Automatic Speech Recognition via Sequential-Level Generalized Entropy Minimization. (arXiv:2306.01981v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01981
&lt;/p&gt;
&lt;p&gt;
SGEM&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#21033;&#29992;&#27874;&#26463;&#25628;&#32034;&#21644;&#24191;&#20041;&#29109;&#26368;&#23567;&#21270;&#35843;&#25972;&#39044;&#35757;&#32451;ASR&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#19977;&#20010;&#20027;&#27969;ASR&#27169;&#22411;&#22312;&#19981;&#21516;&#39046;&#22495;&#36716;&#21464;&#26102;&#30340;&#26368;&#26032;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#23454;&#38469;&#24773;&#20917;&#19979;&#65292;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#27169;&#22411;&#32463;&#24120;&#26292;&#38706;&#20110;&#25968;&#25454;&#20998;&#24067;&#30340;&#21464;&#21270;&#65292;&#23548;&#33268;&#38169;&#35823;&#30340;&#39044;&#27979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26368;&#36817;&#25552;&#20986;&#20102;&#19968;&#31181;&#29616;&#26377;&#30340;&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;&#65288;TTA&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#28304;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#35843;&#25972;&#39044;&#35757;&#32451;&#30340;ASR&#27169;&#22411;&#20197;&#36866;&#24212;&#26410;&#26631;&#35760;&#30340;&#27979;&#35797;&#23454;&#20363;&#12290;&#23613;&#31649;&#26377;&#20102;&#19981;&#38169;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#20294;&#36825;&#39033;&#24037;&#20316;&#20165;&#20381;&#36182;&#20110;&#31616;&#21333;&#30340;&#36138;&#24515;&#35299;&#30721;&#65292;&#24182;&#22312;&#24103;&#32423;&#21035;&#19978;&#36328;&#36234;&#26102;&#38388;&#27493;&#38271;&#36827;&#34892;&#35843;&#25972;&#65292;&#36825;&#22312;&#27169;&#22411;&#36755;&#20986;&#30340;&#24207;&#21015;&#24615;&#36136;&#19979;&#21487;&#33021;&#19981;&#26159;&#26368;&#20248;&#30340;&#12290;&#20986;&#20110;&#36825;&#20010;&#21160;&#26426;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;TTA&#26694;&#26550;&#65292;&#31216;&#20026;SGEM&#65292;&#29992;&#20110;&#19968;&#33324;ASR&#27169;&#22411;&#12290;&#20026;&#20102;&#22788;&#29702;&#24207;&#21015;&#36755;&#20986;&#65292;SGEM&#39318;&#20808;&#21033;&#29992;&#27874;&#26463;&#25628;&#32034;&#26469;&#25506;&#32034;&#20505;&#36873;&#36755;&#20986;&#26631;&#24535;&#65292;&#24182;&#36873;&#25321;&#26368;&#21487;&#20449;&#30340;&#26631;&#24535;&#12290;&#28982;&#21518;&#65292;&#23427;&#21033;&#29992;&#24191;&#20041;&#29109;&#26368;&#23567;&#21270;&#21644;&#36127;&#25277;&#26679;&#20316;&#20026;&#26080;&#30417;&#30563;&#30446;&#26631;&#26469;&#36866;&#24212;&#27169;&#22411;&#12290;&#22312;&#21508;&#31181;&#39046;&#22495;&#30340;&#36716;&#21464;&#19979;&#65292;SGEM&#23454;&#29616;&#20102;&#19977;&#31181;&#20027;&#27969;ASR&#27169;&#22411;&#30340;&#26368;&#26032;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic speech recognition (ASR) models are frequently exposed to data distribution shifts in many real-world scenarios, leading to erroneous predictions. To tackle this issue, an existing test-time adaptation (TTA) method has recently been proposed to adapt the pre-trained ASR model on unlabeled test instances without source data. Despite decent performance gain, this work relies solely on naive greedy decoding and performs adaptation across timesteps at a frame level, which may not be optimal given the sequential nature of the model output. Motivated by this, we propose a novel TTA framework, dubbed SGEM, for general ASR models. To treat the sequential output, SGEM first exploits beam search to explore candidate output logits and selects the most plausible one. Then, it utilizes generalized entropy minimization and negative sampling as unsupervised objectives to adapt the model. SGEM achieves state-of-the-art performance for three mainstream ASR models under various domain shifts.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;Text-to-SQL&#27169;&#22411;SQL-PaLM&#65292;&#20351;&#29992;&#20102;&#38754;&#21521;Text-to-SQL&#30340;&#22522;&#20110;&#25191;&#34892;&#30340;&#33258;&#19968;&#33268;&#25552;&#31034;&#26041;&#27861;&#65292;&#22312;Spider&#19978;&#23454;&#29616;&#20102;77.3%&#30340;&#27979;&#35797;&#22871;&#20214;&#20934;&#30830;&#24230;&#65292;&#24182;&#26174;&#30528;&#36229;&#36234;&#20197;&#21069;&#30340;&#26368;&#26032;&#25216;&#26415;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.00739</link><description>&lt;p&gt;
SQL-PaLM&#65306;&#38024;&#23545;Text-to-SQL&#30340;&#25913;&#36827;&#22823;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;&#24615;
&lt;/p&gt;
&lt;p&gt;
SQL-PaLM: Improved Large Language Model Adaptation for Text-to-SQL. (arXiv:2306.00739v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00739
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;Text-to-SQL&#27169;&#22411;SQL-PaLM&#65292;&#20351;&#29992;&#20102;&#38754;&#21521;Text-to-SQL&#30340;&#22522;&#20110;&#25191;&#34892;&#30340;&#33258;&#19968;&#33268;&#25552;&#31034;&#26041;&#27861;&#65292;&#22312;Spider&#19978;&#23454;&#29616;&#20102;77.3%&#30340;&#27979;&#35797;&#22871;&#20214;&#20934;&#30830;&#24230;&#65292;&#24182;&#26174;&#30528;&#36229;&#36234;&#20197;&#21069;&#30340;&#26368;&#26032;&#25216;&#26415;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#19968;&#20010;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#26032;&#20852;&#21151;&#33021;&#26159;&#29983;&#25104;&#20195;&#30721;&#65292;&#21253;&#25324;&#29992;&#20110;&#25968;&#25454;&#24211;&#30340;&#32467;&#26500;&#21270;&#26597;&#35810;&#35821;&#35328;&#65288;SQL&#65289;&#12290;&#23545;&#20110;&#23558;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#36716;&#25442;&#20026;SQL&#26597;&#35810;&#30340;&#20219;&#21153;&#65292;&#21363;Text-to-SQL&#65292;LLMs&#30340;&#36866;&#24212;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#20855;&#20307;&#21462;&#20915;&#20110;&#20351;&#29992;&#30340;&#36866;&#24212;&#24615;&#25968;&#25454;&#37327;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;Text-to-SQL&#27169;&#22411;SQL-PaLM&#65292;&#21033;&#29992;&#20102;PaLM-2&#65292;&#25512;&#21160;&#20102;&#20004;&#31181;&#35774;&#32622;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;Few-shot SQL-PaLM&#22522;&#20110;&#38754;&#21521;Text-to-SQL&#30340;&#22522;&#20110;&#25191;&#34892;&#30340;&#33258;&#19968;&#33268;&#25552;&#31034;&#26041;&#27861;&#65292;&#21487;&#22312;Spider&#19978;&#23454;&#29616;77.3%&#30340;&#27979;&#35797;&#22871;&#20214;&#20934;&#30830;&#24230;&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#36890;&#36807;&#26174;&#30528;&#36739;&#22823;&#30340;&#24494;&#35843;&#36229;&#36234;&#20197;&#21069;&#30340;&#26368;&#26032;&#25216;&#26415;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#32463;&#36807;&#31934;&#32454;&#35843;&#25972;&#30340;SQL-PALM&#21487;&#36827;&#19968;&#27493;&#25552;&#39640;1%&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#23558;SQL-PaLM&#24212;&#29992;&#20110;&#23454;&#38469;&#22330;&#26223;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35780;&#20272;&#20102;&#20854;&#23545;&#20854;&#20182;&#25361;&#25112;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
One impressive emergent capability of large language models (LLMs) is generation of code, including Structured Query Language (SQL) for databases. For the task of converting natural language text to SQL queries, Text-to-SQL, adaptation of LLMs is of paramount importance, both in in-context learning and fine-tuning settings, depending on the amount of adaptation data used. In this paper, we propose an LLM-based Text-to-SQL model SQL-PaLM, leveraging on PaLM-2, that pushes the state-of-the-art in both settings. Few-shot SQL-PaLM is based on an execution-based self-consistency prompting approach designed for Text-to-SQL, and achieves 77.3% in test-suite accuracy on Spider, which to our best knowledge is the first to outperform previous state-of-the-art with fine-tuning by a significant margin, 4%. Furthermore, we demonstrate that the fine-tuned SQL-PALM outperforms it further by another 1%. Towards applying SQL-PaLM to real-world scenarios we further evaluate its robustness on other chall
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SuperNorm&#30340;&#19987;&#29992;&#24402;&#19968;&#21270;&#26041;&#26696;&#65292;&#36890;&#36807;&#23884;&#20837;&#23376;&#22270;&#29305;&#23450;&#22240;&#23376;&#21644;&#32435;&#20837;&#22270;&#23454;&#20363;&#29305;&#23450;&#32479;&#35745;&#25968;&#25454;&#26469;&#21152;&#24378;GNN&#30340;&#20195;&#34920;&#24615;&#33021;&#21147;&#65292;&#23454;&#29616;&#23545;&#33410;&#28857;&#24863;&#24212;&#23376;&#22270;&#20013;&#20869;&#37096;&#36830;&#25509;&#20449;&#24687;&#30340;&#26126;&#30830;&#32771;&#34385;&#65292;&#20174;&#32780;&#25913;&#21892;GNN&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.19903</link><description>&lt;p&gt;
&#20351;&#29992;&#23376;&#22270;&#29305;&#23450;&#22240;&#23376;&#23884;&#20837;&#24402;&#19968;&#21270;&#25913;&#21892;GNN&#30340;&#34920;&#36798;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improving Expressivity of GNNs with Subgraph-specific Factor Embedded Normalization. (arXiv:2305.19903v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19903
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SuperNorm&#30340;&#19987;&#29992;&#24402;&#19968;&#21270;&#26041;&#26696;&#65292;&#36890;&#36807;&#23884;&#20837;&#23376;&#22270;&#29305;&#23450;&#22240;&#23376;&#21644;&#32435;&#20837;&#22270;&#23454;&#20363;&#29305;&#23450;&#32479;&#35745;&#25968;&#25454;&#26469;&#21152;&#24378;GNN&#30340;&#20195;&#34920;&#24615;&#33021;&#21147;&#65292;&#23454;&#29616;&#23545;&#33410;&#28857;&#24863;&#24212;&#23376;&#22270;&#20013;&#20869;&#37096;&#36830;&#25509;&#20449;&#24687;&#30340;&#26126;&#30830;&#32771;&#34385;&#65292;&#20174;&#32780;&#25913;&#21892;GNN&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31867;&#22788;&#29702;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#24378;&#22823;&#23398;&#20064;&#26550;&#26500;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;GNN&#36890;&#24120;&#24573;&#30053;&#20102;&#33410;&#28857;&#24863;&#24212;&#23376;&#22270;&#20013;&#30340;&#37325;&#35201;&#32467;&#26500;&#29305;&#24449;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#19987;&#29992;&#30340;&#21363;&#25554;&#21363;&#29992;&#24402;&#19968;&#21270;&#26041;&#26696;&#8212;&#8212;SUbgraph-sPEcific FactoR Embedded Normalization&#65288;SuperNorm&#65289;&#26469;&#21152;&#24378;GNN&#30340;&#20195;&#34920;&#24615;&#33021;&#21147;&#65292;&#35813;&#26041;&#26696;&#26126;&#30830;&#32771;&#34385;&#20102;&#27599;&#20010;&#33410;&#28857;&#24863;&#24212;&#23376;&#22270;&#20869;&#37096;&#36830;&#25509;&#30340;&#20449;&#24687;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#22312;&#26631;&#20934;&#30340;BatchNorm&#24320;&#22987;&#21644;&#32467;&#26463;&#26102;&#23884;&#20837;&#20102;&#23376;&#22270;&#29305;&#23450;&#22240;&#23376;&#65292;&#24182;&#32435;&#20837;&#22270;&#23454;&#20363;&#29305;&#23450;&#32479;&#35745;&#25968;&#25454;&#20197;&#25552;&#39640;&#21306;&#20998;&#33021;&#21147;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#25903;&#25345;&#65292;&#25351;&#20986;&#36890;&#36807;&#25913;&#21892;&#30340;SuperNorm&#65292;&#20219;&#24847;GNN&#33267;&#23569;&#19982;1-WL&#27979;&#35797;&#19968;&#26679;&#33021;&#22815;&#21306;&#20998;&#38750;&#21516;&#26500;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks~(GNNs) have emerged as a powerful category of learning architecture for handling graph-structured data. However, existing GNNs typically ignore crucial structural characteristics in node-induced subgraphs, which thus limits their expressiveness for various downstream tasks. In this paper, we strive to strengthen the representative capabilities of GNNs by devising a dedicated plug-and-play normalization scheme, termed as SUbgraph-sPEcific FactoR Embedded Normalization (SuperNorm), that explicitly considers the intra-connection information within each node-induced subgraph. To this end, we embed the subgraph-specific factor at the beginning and the end of the standard BatchNorm, as well as incorporate graph instance-specific statistics for improved distinguishable capabilities. In the meantime, we provide theoretical analysis to support that, with the elaborated SuperNorm, an arbitrary GNN is at least as powerful as the 1-WL test in distinguishing non-isomorphism gr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;GAN&#26469;&#35757;&#32451;Learnable-MPC&#31574;&#30053;&#65292;&#20197;&#20415;&#22312;&#28436;&#31034;&#32773;&#21644;&#27169;&#20223;&#32773;&#20195;&#29702;&#19981;&#33021;&#30456;&#21516;&#26102;&#36827;&#34892;&#21442;&#25968;&#21270;&#30340;&#25104;&#26412;&#20989;&#25968;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2305.19111</link><description>&lt;p&gt;
GAN-MPC:&#20351;&#29992;&#38750;&#30456;&#21516;&#19987;&#23478;&#30340;&#28436;&#31034;&#35757;&#32451;&#20855;&#26377;&#21442;&#25968;&#21270;&#25104;&#26412;&#20989;&#25968;&#30340;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#22120;
&lt;/p&gt;
&lt;p&gt;
GAN-MPC: Training Model Predictive Controllers with Parameterized Cost Functions using Demonstrations from Non-identical Experts. (arXiv:2305.19111v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19111
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;GAN&#26469;&#35757;&#32451;Learnable-MPC&#31574;&#30053;&#65292;&#20197;&#20415;&#22312;&#28436;&#31034;&#32773;&#21644;&#27169;&#20223;&#32773;&#20195;&#29702;&#19981;&#33021;&#30456;&#21516;&#26102;&#36827;&#34892;&#21442;&#25968;&#21270;&#30340;&#25104;&#26412;&#20989;&#25968;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65288;MPC&#65289;&#26159;&#26426;&#22120;&#20154;&#23454;&#38469;&#24212;&#29992;&#20013;&#36712;&#36857;&#20248;&#21270;&#30340;&#27969;&#34892;&#26041;&#27861;&#12290;MPC&#31574;&#30053;&#21487;&#20197;&#22312;&#36816;&#21160;&#21160;&#21147;&#23398;&#21644;&#23433;&#20840;&#24615;&#32422;&#26463;&#19979;&#20248;&#21270;&#36712;&#36857;&#21442;&#25968;&#65292;&#24182;&#22312;&#23433;&#20840;&#20445;&#38556;&#12289;&#26368;&#20248;&#24615;&#12289;&#27867;&#21270;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#35828;&#26126;&#24615;&#26041;&#38754;&#25552;&#20379;&#20445;&#35777;&#12290;&#28982;&#32780;&#65292;&#26377;&#20123;&#34892;&#20026;&#26159;&#22797;&#26434;&#30340;&#65292;&#25163;&#24037;&#21046;&#20316;MPC&#30446;&#26631;&#20989;&#25968;&#26159;&#22256;&#38590;&#30340;&#12290;&#19968;&#31181;&#21517;&#20026;&#21487;&#23398;&#20064;MPC&#30340;&#29305;&#27530;&#31867;&#21035;&#30340;MPC&#31574;&#30053;&#21033;&#29992;&#26469;&#33258;&#19987;&#23478;&#28436;&#31034;&#30340;&#27169;&#20223;&#23398;&#20064;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#35201;&#27714;&#28436;&#31034;&#32773;&#21644;&#27169;&#20223;&#32773;&#20195;&#29702;&#30456;&#21516;&#65292;&#36825;&#22312;&#35768;&#22810;&#26426;&#22120;&#20154;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#24456;&#38590;&#28385;&#36275;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#24403;&#28436;&#31034;&#32773;&#21644;&#27169;&#20223;&#32773;&#27809;&#26377;&#20849;&#20139;&#30456;&#21516;&#21160;&#21147;&#23398;&#19988;&#29366;&#24577;&#31354;&#38388;&#21487;&#33021;&#37096;&#20998;&#37325;&#21472;&#26102;&#35757;&#32451;&#21487;&#23398;&#20064;MPC&#31574;&#30053;&#30340;&#23454;&#38469;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#26469;&#26368;&#23567;&#21270;Jensen-Shannon&#36317;&#31163;&#26469;&#36827;&#34892;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model predictive control (MPC) is a popular approach for trajectory optimization in practical robotics applications. MPC policies can optimize trajectory parameters under kinodynamic and safety constraints and provide guarantees on safety, optimality, generalizability, interpretability, and explainability. However, some behaviors are complex and it is difficult to hand-craft an MPC objective function. A special class of MPC policies called Learnable-MPC addresses this difficulty using imitation learning from expert demonstrations. However, they require the demonstrator and the imitator agents to be identical which is hard to satisfy in many real world applications of robotics. In this paper, we address the practical problem of training Learnable-MPC policies when the demonstrator and the imitator do not share the same dynamics and their state spaces may have a partial overlap. We propose a novel approach that uses a generative adversarial network (GAN) to minimize the Jensen-Shannon di
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20803;&#25968;&#25454;&#30340; AI &#29983;&#25104;&#30340;&#22823;&#23398;&#29983;&#20316;&#19994;&#26816;&#27979;&#26041;&#27861; HowkGPT&#65292;&#36890;&#36807;&#35745;&#31639;&#22256;&#24785;&#24230;&#24471;&#20998;&#26469;&#21306;&#20998;&#23398;&#29983;&#25552;&#20132;&#21644; ChatGPT &#29983;&#25104;&#30340;&#20316;&#19994;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20998;&#26512;&#30340;&#31934;&#24230;&#65292;&#20197;&#24110;&#21161;&#32500;&#25252;&#23398;&#26415;&#35802;&#20449;&#21644;&#38450;&#27490;&#20316;&#24330;&#12290;</title><link>http://arxiv.org/abs/2305.18226</link><description>&lt;p&gt;
HowkGPT: &#22522;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;&#22256;&#24785;&#24230;&#20998;&#26512;&#30340; ChatGPT &#29983;&#25104;&#30340;&#22823;&#23398;&#29983;&#20316;&#19994;&#26816;&#27979;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
HowkGPT: Investigating the Detection of ChatGPT-generated University Student Homework through Context-Aware Perplexity Analysis. (arXiv:2305.18226v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18226
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20803;&#25968;&#25454;&#30340; AI &#29983;&#25104;&#30340;&#22823;&#23398;&#29983;&#20316;&#19994;&#26816;&#27979;&#26041;&#27861; HowkGPT&#65292;&#36890;&#36807;&#35745;&#31639;&#22256;&#24785;&#24230;&#24471;&#20998;&#26469;&#21306;&#20998;&#23398;&#29983;&#25552;&#20132;&#21644; ChatGPT &#29983;&#25104;&#30340;&#20316;&#19994;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20998;&#26512;&#30340;&#31934;&#24230;&#65292;&#20197;&#24110;&#21161;&#32500;&#25252;&#23398;&#26415;&#35802;&#20449;&#21644;&#38450;&#27490;&#20316;&#24330;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#20351;&#29992;&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#20154;&#20204;&#25285;&#24515;&#23427;&#20204;&#21487;&#33021;&#20250;&#21361;&#21450;&#23398;&#26415;&#35802;&#20449;&#12290;&#25945;&#32946;&#37096;&#38376;&#30446;&#21069;&#27491;&#22312;&#21162;&#21147;&#21306;&#20998;&#23398;&#29983;&#25552;&#20132;&#30340;&#23478;&#24237;&#20316;&#19994;&#21644;AI&#29983;&#25104;&#30340;&#20316;&#19994;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837; HowkGPT &#26631;&#35782;&#30001; AI &#29983;&#25104;&#30340;&#20316;&#19994;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;HowkGPT &#22522;&#20110;&#19968;&#32452;&#23398;&#26415;&#20316;&#19994;&#21644;&#30456;&#24212;&#20803;&#25968;&#25454;&#26500;&#24314;&#65292;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340; LLM &#35745;&#31639;&#23398;&#29983;&#25552;&#20132;&#21644; ChatGPT &#29983;&#25104;&#30340;&#22238;&#31572;&#30340;&#22256;&#24785;&#24230;&#24471;&#20998;&#12290;&#28982;&#21518;&#65292;&#36825;&#20123;&#24471;&#20998;&#26377;&#21161;&#20110;&#24314;&#31435;&#21306;&#20998;&#25552;&#20132;&#20316;&#19994;&#26469;&#28304;&#30340;&#38408;&#20540;&#12290;&#37492;&#20110;&#23398;&#26415;&#24037;&#20316;&#30340;&#29305;&#27530;&#24615;&#21644;&#19978;&#19979;&#25991;&#24615;&#36136;&#65292;HowkGPT &#36824;&#36890;&#36807;&#23450;&#20041;&#20174;&#20803;&#25968;&#25454;&#20013;&#23548;&#20986;&#30340;&#31867;&#21035;&#29305;&#23450;&#30340;&#38408;&#20540;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640;&#20998;&#26512;&#30340;&#31934;&#24230;&#12290;&#26412;&#30740;&#31350;&#24378;&#35843;&#20102;&#22312; LLM &#25991;&#26412;&#29983;&#25104;&#26102;&#26399;&#32500;&#25252;&#23398;&#26415;&#35802;&#20449;&#21644;&#38450;&#27490;&#20316;&#24330;&#30340;&#26377;&#25928;&#31574;&#30053;&#30340;&#20851;&#38190;&#24615;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the use of Large Language Models (LLMs) in text generation tasks proliferates, concerns arise over their potential to compromise academic integrity. The education sector currently tussles with distinguishing student-authored homework assignments from AI-generated ones. This paper addresses the challenge by introducing HowkGPT, designed to identify homework assignments generated by AI. HowkGPT is built upon a dataset of academic assignments and accompanying metadata [17] and employs a pretrained LLM to compute perplexity scores for student-authored and ChatGPT-generated responses. These scores then assist in establishing a threshold for discerning the origin of a submitted assignment. Given the specificity and contextual nature of academic work, HowkGPT further refines its analysis by defining category-specific thresholds derived from the metadata, enhancing the precision of the detection. This study emphasizes the critical need for effective strategies to uphold academic integrity a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25968;&#25454;&#21464;&#25442;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#29616;&#33021;&#21147;&#65292;&#35777;&#26126;&#20102;&#21333;&#35843;&#26368;&#22823;&#21644;&#30340;GNN&#19982;Datalog&#20043;&#38388;&#30340;&#20851;&#31995;&#32039;&#23494;&#65292;&#21363;&#20219;&#20309;&#21333;&#35843;&#26368;&#22823;&#21644;&#30340;GNN&#37117;&#21487;&#20197;&#34920;&#31034;&#20026;&#19968;&#20010;Datalog&#31243;&#24207;&#65292;&#21453;&#20043;&#20134;&#28982;&#12290;</title><link>http://arxiv.org/abs/2305.18015</link><description>&lt;p&gt;
&#22522;&#20110;&#21333;&#35843;&#26368;&#22823;&#21644;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#19982;Datalog&#30340;&#23545;&#24212;&#20851;&#31995;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Correspondence Between Monotonic Max-Sum GNNs and Datalog. (arXiv:2305.18015v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18015
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25968;&#25454;&#21464;&#25442;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#29616;&#33021;&#21147;&#65292;&#35777;&#26126;&#20102;&#21333;&#35843;&#26368;&#22823;&#21644;&#30340;GNN&#19982;Datalog&#20043;&#38388;&#30340;&#20851;&#31995;&#32039;&#23494;&#65292;&#21363;&#20219;&#20309;&#21333;&#35843;&#26368;&#22823;&#21644;&#30340;GNN&#37117;&#21487;&#20197;&#34920;&#31034;&#20026;&#19968;&#20010;Datalog&#31243;&#24207;&#65292;&#21453;&#20043;&#20134;&#28982;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#21040;&#32467;&#26500;&#21270;&#25968;&#25454;&#19978;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#65292;&#20294;&#36825;&#20123;&#25216;&#26415;&#30340;&#34920;&#36798;&#33021;&#21147;&#65288;&#21363;&#21487;&#20197;&#23398;&#20064;&#21040;&#20160;&#20040;&#65289;&#20173;&#28982;&#19981;&#22826;&#28165;&#26970;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#25454;&#21464;&#25442;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#27880;&#24847;&#21040;&#22914;&#20309;&#23558;&#25968;&#25454;&#38598;&#32534;&#30721;&#25104;GNN&#21487;&#22788;&#29702;&#30340;&#25968;&#23383;&#24418;&#24335;&#21487;&#20197;&#27169;&#31946;&#27169;&#22411;&#34920;&#36798;&#33021;&#21147;&#30340;&#25551;&#36848;&#65292;&#25105;&#20204;&#35748;&#20026;&#19968;&#31181;&#35268;&#33539;&#32534;&#30721;&#25552;&#20379;&#20102;&#36866;&#24403;&#30340;&#22522;&#30784;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21333;&#35843;&#26368;&#22823;&#21644;&#30340;GNN&#30340;&#34920;&#29616;&#33021;&#21147;&#65292;&#23427;&#20204;&#28085;&#30422;&#20102;&#20855;&#26377;max&#21644;sum&#32858;&#21512;&#20989;&#25968;&#30340;GNN&#30340;&#19968;&#20010;&#23376;&#31867;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#27599;&#20010;&#36825;&#26679;&#30340;GNN&#65292;&#21487;&#20197;&#35745;&#31639;&#20986;&#19968;&#20010;Datalog&#31243;&#24207;&#65292;&#20351;&#24471;&#23558;GNN&#24212;&#29992;&#20110;&#20219;&#20309;&#25968;&#25454;&#38598;&#37117;&#20250;&#20135;&#29983;&#19982;&#23558;&#31243;&#24207;&#35268;&#21017;&#24212;&#29992;&#20110;&#25968;&#25454;&#38598;&#30340;&#21333;&#20010;&#24490;&#29615;&#30456;&#21516;&#30340;&#20107;&#23454;&#12290;&#21333;&#35843;&#26368;&#22823;&#21644;&#30340;GNN&#33021;&#22815;&#23545;&#26080;&#38480;&#25968;&#37327;&#30340;&#29305;&#24449;&#21521;&#37327;&#27714;&#21644;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#29305;&#24449;&#20540;&#20219;&#24847;&#22686;&#22823;&#65292;&#32780;&#22522;&#20110;&#35268;&#21017;&#30340;&#31995;&#32479;&#65288;&#20363;&#22914;Datalog&#65289;&#20855;&#26377;&#20445;&#35777;&#31995;&#32479;&#22987;&#32456;&#20250;&#36235;&#20110;&#31283;&#23450;&#29366;&#24577;&#30340;&#21333;&#35843;&#24615;&#36136;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#21333;&#35843;&#26368;&#22823;&#21644;&#30340;GNN&#19982;Datalog&#20043;&#38388;&#30340;&#36825;&#31181;&#20851;&#31995;&#26159;&#32039;&#23494;&#30340;&#65292;&#21363;&#20219;&#20309;&#21333;&#35843;&#26368;&#22823;&#21644;&#30340;GNN&#37117;&#21487;&#20197;&#34920;&#31034;&#20026;&#19968;&#20010;Datalog&#31243;&#24207;&#65292;&#21453;&#20043;&#20134;&#28982;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although there has been significant interest in applying machine learning techniques to structured data, the expressivity (i.e., a description of what can be learned) of such techniques is still poorly understood. In this paper, we study data transformations based on graph neural networks (GNNs). First, we note that the choice of how a dataset is encoded into a numeric form processable by a GNN can obscure the characterisation of a model's expressivity, and we argue that a canonical encoding provides an appropriate basis. Second, we study the expressivity of monotonic max-sum GNNs, which cover a subclass of GNNs with max and sum aggregation functions. We show that, for each such GNN, one can compute a Datalog program such that applying the GNN to any dataset produces the same facts as a single round of application of the program's rules to the dataset. Monotonic max-sum GNNs can sum an unbounded number of feature vectors which can result in arbitrarily large feature values, whereas rul
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31616;&#35201;&#27010;&#36848;&#20102;&#38271;&#25991;&#26412;&#30340;&#31070;&#32463;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#29616;&#29366;&#65292;&#20027;&#35201;&#21253;&#25324;&#25991;&#26723;&#20998;&#31867;&#21644;&#25688;&#35201;&#65292;&#28085;&#30422;&#20102;&#24773;&#24863;&#20998;&#26512;&#65292;&#21516;&#26102;&#36824;&#25506;&#35752;&#20102;&#38271;&#25991;&#26412;NLP&#30340;&#20027;&#35201;&#25361;&#25112;&#12289;&#38382;&#39064;&#21644;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2305.16259</link><description>&lt;p&gt;
&#38271;&#25991;&#26412;&#30340;&#31070;&#32463;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65306;&#29616;&#29366;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Neural Natural Language Processing for Long Texts: A Survey of the State-of-the-Art. (arXiv:2305.16259v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16259
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31616;&#35201;&#27010;&#36848;&#20102;&#38271;&#25991;&#26412;&#30340;&#31070;&#32463;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#29616;&#29366;&#65292;&#20027;&#35201;&#21253;&#25324;&#25991;&#26723;&#20998;&#31867;&#21644;&#25688;&#35201;&#65292;&#28085;&#30422;&#20102;&#24773;&#24863;&#20998;&#26512;&#65292;&#21516;&#26102;&#36824;&#25506;&#35752;&#20102;&#38271;&#25991;&#26412;NLP&#30340;&#20027;&#35201;&#25361;&#25112;&#12289;&#38382;&#39064;&#21644;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#37319;&#29992;&#26497;&#22823;&#22320;&#20419;&#36827;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#38271;&#25991;&#26412;&#20998;&#26512;&#30340;&#38656;&#27714;&#19982;&#30701;&#25991;&#26412;&#26377;&#24456;&#22823;&#19981;&#21516;&#65292;&#32780;&#32593;&#32476;&#19978;&#20256;&#36755;&#30340;&#25991;&#26723;&#22823;&#23567;&#19981;&#26029;&#22686;&#21152;&#65292;&#20351;&#38271;&#25991;&#26412;&#30340;&#33258;&#21160;&#29702;&#35299;&#25104;&#20026;&#19968;&#39033;&#20851;&#38190;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#26412;&#25991;&#30340;&#20004;&#20010;&#30446;&#26631;&#26159;&#65306;a&#65289;&#27010;&#36848;&#30456;&#20851;&#30340;&#31070;&#32463;&#26500;&#24314;&#27169;&#22359;&#65292;&#20316;&#20026;&#30701;&#25945;&#31243;&#65307;b&#65289;&#24635;&#32467;&#38271;&#25991;&#26412;NLP&#30340;&#29616;&#29366;&#65292;&#20027;&#35201;&#20851;&#27880;&#20004;&#20010;&#26680;&#24515;&#20219;&#21153;&#65306;&#25991;&#26723;&#20998;&#31867;&#21644;&#25991;&#26723;&#25688;&#35201;&#12290;&#24773;&#24863;&#20998;&#26512;&#20063;&#28085;&#30422;&#22312;&#20869;&#65292;&#22240;&#20026;&#23427;&#36890;&#24120;&#34987;&#35270;&#20026;&#25991;&#26723;&#20998;&#31867;&#30340;&#29305;&#20363;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#35752;&#35770;&#20102;&#38271;&#25991;&#26412;NLP&#30456;&#20851;&#30340;&#20027;&#35201;&#25361;&#25112;&#12289;&#38382;&#39064;&#21644;&#35299;&#20915;&#26041;&#26696;&#12290;&#26368;&#21518;&#65292;&#20171;&#32461;&#20102;&#30456;&#20851;&#30340;&#20844;&#24320;&#30340;&#27880;&#37322;&#25968;&#25454;&#38598;&#65292;&#20197;&#20415;&#20419;&#36827;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
The adoption of Deep Neural Networks (DNNs) has greatly benefited Natural Language Processing (NLP) during the past decade. However, the demands of long document analysis are quite different from those of shorter texts, while the ever increasing size of documents uploaded on-line renders automated understanding of long texts a critical area of research. This article has two goals: a) it overviews the relevant neural building blocks, thus serving as a short tutorial, and b) it surveys the state-of-the-art in long document NLP, mainly focusing on two central tasks: document classification and document summarization. Sentiment analysis for long texts is also covered, since it is typically treated as a particular case of document classification. Additionally, this article discusses the main challenges, issues and current solutions related to long document NLP. Finally, the relevant, publicly available, annotated datasets are presented, in order to facilitate further research.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#23558;spotPython&#36229;&#21442;&#25968;&#35843;&#35856;&#22120;&#38598;&#25104;&#21040;PyTorch&#35757;&#32451;&#24037;&#20316;&#27969;&#20013;&#65292;&#20197;&#25552;&#39640;&#26426;&#22120;&#25110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20197;CIFAR10&#22270;&#20687;&#20998;&#31867;&#22120;&#20026;&#20363;&#12290;</title><link>http://arxiv.org/abs/2305.11930</link><description>&lt;p&gt;
PyTorch&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#8212;&#8212;&#38754;&#21521;spotPython&#30340;&#25945;&#31243;
&lt;/p&gt;
&lt;p&gt;
PyTorch Hyperparameter Tuning -- A Tutorial for spotPython. (arXiv:2305.11930v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11930
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#23558;spotPython&#36229;&#21442;&#25968;&#35843;&#35856;&#22120;&#38598;&#25104;&#21040;PyTorch&#35757;&#32451;&#24037;&#20316;&#27969;&#20013;&#65292;&#20197;&#25552;&#39640;&#26426;&#22120;&#25110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20197;CIFAR10&#22270;&#20687;&#20998;&#31867;&#22120;&#20026;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#21442;&#25968;&#35843;&#25972;&#65288;&#25110;&#36229;&#21442;&#25968;&#20248;&#21270;&#65289;&#30340;&#30446;&#26631;&#26159;&#20248;&#21270;&#36229;&#21442;&#25968;&#20197;&#25552;&#39640;&#26426;&#22120;&#25110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;spotPython&#26159;&#30693;&#21517;&#36229;&#21442;&#25968;&#35843;&#35856;&#22120;SPOT&#30340;Python&#29256;&#26412;&#65292;SPOT&#24050;&#32463;&#22312;R&#32534;&#31243;&#29615;&#22659;&#20013;&#20026;&#32479;&#35745;&#20998;&#26512;&#24320;&#21457;&#20102;&#21313;&#24180;&#20197;&#19978;&#12290;PyTorch&#26159;&#19968;&#31181;&#22522;&#20110;GPU&#21644;CPU&#30340;&#28145;&#24230;&#23398;&#20064;&#20248;&#21270;&#24352;&#37327;&#24211;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;spotPython&#36229;&#21442;&#25968;&#35843;&#35856;&#22120;&#38598;&#25104;&#21040;PyTorch&#35757;&#32451;&#24037;&#20316;&#27969;&#20013;&#12290;&#20197;CIFAR10&#22270;&#20687;&#20998;&#31867;&#22120;&#20026;&#20363;&#65292;&#20171;&#32461;&#20102;spotPython&#20197;&#21450;&#19982;Ray Tune&#30340;&#31616;&#30701;&#27604;&#36739;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#20004;&#31181;&#26041;&#27861;&#30340;&#20248;&#32570;&#28857;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;spotPython&#30340;&#20351;&#29992;&#32463;&#39564;&#65292;&#20197;&#21450;&#22914;&#20309;&#20351;&#29992;hook&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#33258;&#21160;&#35843;&#25972;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of hyperparameter tuning (or hyperparameter optimization) is to optimize the hyperparameters to improve the performance of the machine or deep learning model. spotPython (``Sequential Parameter Optimization Toolbox in Python'') is the Python version of the well-known hyperparameter tuner SPOT, which has been developed in the R programming environment for statistical analysis for over a decade. PyTorch is an optimized tensor library for deep learning using GPUs and CPUs. This document shows how to integrate the spotPython hyperparameter tuner into the PyTorch training workflow. As an example, the results of the CIFAR10 image classifier are used. In addition to an introduction to spotPython, this tutorial also includes a brief comparison with Ray Tune, a Python library for running experiments and tuning hyperparameters. This comparison is based on the PyTorch hyperparameter tuning tutorial. The advantages and disadvantages of both approaches are discussed. We show that spotPytho
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;HOI&#26816;&#27979;&#25968;&#25454;&#25286;&#20998;&#65292;&#26088;&#22312;&#35780;&#20272;&#31995;&#32479;&#24615;&#27867;&#21270;&#12290;&#22312;&#26032;&#30340;&#25968;&#25454;&#25286;&#20998;&#19978;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#65292;HOI&#26816;&#27979;&#27169;&#22411;&#23545;&#20110;&#26410;&#35265;&#36807;&#30340;&#23545;&#35937;&#21644;&#20132;&#20114;&#32452;&#21512;&#30340;&#27867;&#21270;&#21313;&#20998;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2305.09948</link><description>&lt;p&gt;
HICO-DET-SG&#21644;V-COCO-SG&#65306;&#26032;&#30340;&#25968;&#25454;&#25286;&#20998;&#29992;&#20110;&#35780;&#20272;&#20154;-&#29289;&#20132;&#20114;&#26816;&#27979;&#20013;&#30340;&#31995;&#32479;&#24615;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
HICO-DET-SG and V-COCO-SG: New Data Splits to Evaluate Systematic Generalization in Human-Object Interaction Detection. (arXiv:2305.09948v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09948
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;HOI&#26816;&#27979;&#25968;&#25454;&#25286;&#20998;&#65292;&#26088;&#22312;&#35780;&#20272;&#31995;&#32479;&#24615;&#27867;&#21270;&#12290;&#22312;&#26032;&#30340;&#25968;&#25454;&#25286;&#20998;&#19978;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#65292;HOI&#26816;&#27979;&#27169;&#22411;&#23545;&#20110;&#26410;&#35265;&#36807;&#30340;&#23545;&#35937;&#21644;&#20132;&#20114;&#32452;&#21512;&#30340;&#27867;&#21270;&#21313;&#20998;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;-&#29289;&#20132;&#20114;&#26816;&#27979;&#26159;&#19968;&#31181;&#39044;&#27979;&#22270;&#20687;&#20013;&#20154;&#19982;&#29289;&#21697;&#20043;&#38388;&#20132;&#20114;&#30340;&#20219;&#21153;&#12290;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#38656;&#35201;&#23545;HOI&#26816;&#27979;&#27169;&#22411;&#36827;&#34892;&#31995;&#32479;&#24615;&#30340;&#27867;&#21270;&#65292;&#21363;&#27867;&#21270;&#21040;&#26032;&#30340;&#23545;&#35937;&#21644;&#20132;&#20114;&#32452;&#21512;&#19978;&#65292;&#22240;&#20026;&#35757;&#32451;&#25968;&#25454;&#20165;&#21487;&#33021;&#28085;&#30422;&#25152;&#26377;&#21487;&#33021;&#32452;&#21512;&#30340;&#19968;&#23567;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#27809;&#26377;&#24320;&#25918;&#30340;&#22522;&#20934;&#27979;&#35797;&#25110;&#29616;&#26377;&#24037;&#20316;&#35780;&#20272;HOI&#26816;&#27979;&#20013;&#30340;&#31995;&#32479;&#24615;&#27867;&#21270;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22522;&#20110;HICO-DET&#21644;V-COCO&#25968;&#25454;&#38598;&#21019;&#24314;&#20102;&#20004;&#20010;&#21517;&#20026;HICO-DET-SG&#21644;V-COCO-SG&#30340;&#26032;&#30340;HOI&#26816;&#27979;&#25968;&#25454;&#25286;&#20998;&#12290;&#25105;&#20204;&#22312;&#26032;&#30340;&#25968;&#25454;&#25286;&#20998;&#19978;&#35780;&#20272;&#20102;&#20195;&#34920;&#24615;&#30340;HOI&#26816;&#27979;&#27169;&#22411;&#65292;&#24182;&#35266;&#23519;&#21040;&#19982;&#21407;&#22987;&#25968;&#25454;&#38598;&#19978;&#30456;&#27604;&#27979;&#35797;&#24615;&#33021;&#26377;&#24456;&#22823;&#30340;&#38477;&#20302;&#12290;&#36825;&#20010;&#32467;&#26524;&#34920;&#26126;&#31995;&#32479;&#24615;&#27867;&#21270;&#26159;HOI&#26816;&#27979;&#20013;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#30446;&#26631;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#26032;&#25968;&#25454;&#25286;&#20998;&#33021;&#22815;&#40723;&#21169;&#26356;&#22810;&#30340;&#30740;&#31350;&#26397;&#30528;&#36825;&#20010;&#30446;&#26631;&#21162;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human-Object Interaction (HOI) detection is a task to predict interactions between humans and objects in an image. In real-world scenarios, HOI detection models are required systematic generalization, i.e., generalization to novel combinations of objects and interactions, because it is highly probable that the train data only cover a limited portion of all possible combinations. However, to our knowledge, no open benchmark or existing work evaluates the systematic generalization in HOI detection. To address this issue, we created two new sets of HOI detection data splits named HICO-DET-SG and V-COCO-SG based on HICO-DET and V-COCO datasets. We evaluated representative HOI detection models on the new data splits and observed large degradation in the test performances compared to those on the original datasets. This result shows that systematic generalization is a challenging goal in HOI detection. We hope our new data splits encourage more research toward this goal.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;PALR&#30340;&#26694;&#26550;&#65292;&#23558;&#29992;&#25143;&#30340;&#21382;&#21490;&#34892;&#20026;&#19982;LLMs&#30456;&#32467;&#21512;&#65292;&#29983;&#25104;&#29992;&#25143;&#21916;&#27426;&#30340;&#29289;&#21697;&#30340;&#25512;&#33616;&#12290;&#19982;&#29616;&#26377;&#30340;&#25512;&#33616;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;PALR&#26694;&#26550;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.07622</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#24863;&#30693;&#30340;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;LMMs&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PALR: Personalization Aware LLMs for Recommendation. (arXiv:2305.07622v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07622
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;PALR&#30340;&#26694;&#26550;&#65292;&#23558;&#29992;&#25143;&#30340;&#21382;&#21490;&#34892;&#20026;&#19982;LLMs&#30456;&#32467;&#21512;&#65292;&#29983;&#25104;&#29992;&#25143;&#21916;&#27426;&#30340;&#29289;&#21697;&#30340;&#25512;&#33616;&#12290;&#19982;&#29616;&#26377;&#30340;&#25512;&#33616;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;PALR&#26694;&#26550;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30001;&#20110;&#20854;&#20986;&#33394;&#30340;&#24615;&#33021;&#32780;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;PALR&#65292;&#23558;&#29992;&#25143;&#30340;&#21382;&#21490;&#34892;&#20026;&#19982;LLMs&#30456;&#32467;&#21512;&#65292;&#20197;&#29983;&#25104;&#29992;&#25143;&#21916;&#27426;&#30340;&#29289;&#21697;&#30340;&#25512;&#33616;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#29992;&#25143;/&#29289;&#21697;&#20114;&#21160;&#20316;&#20026;&#20505;&#36873;&#26816;&#32034;&#30340;&#25351;&#23548;&#65292;&#28982;&#21518;&#37319;&#29992;&#22522;&#20110;LLMs&#30340;&#25490;&#24207;&#27169;&#22411;&#29983;&#25104;&#25512;&#33616;&#29289;&#21697;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#30340;&#25512;&#33616;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;PALR&#26694;&#26550;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have recently received significant attention for their exceptional capabilities. Despite extensive efforts in developing general-purpose LLMs that can be utilized in various natural language processing (NLP) tasks, there has been less research exploring their potential in recommender systems. In this paper, we propose a novel framework, named PALR, which aiming to combine user history behaviors (such as clicks, purchases, ratings, etc.) with LLMs to generate user preferred items. Specifically, we first use user/item interactions as guidance for candidate retrieval. Then we adopt a LLM-based ranking model to generate recommended items. Unlike existing approaches that typically adopt general-purpose LLMs for zero/few-shot recommendation testing or training on small-sized language models (with less than 1 billion parameters), which cannot fully elicit LLMs' reasoning abilities and leverage rich item side parametric knowledge, we fine-tune a 7 billion parameter
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#29289;&#29702;&#25915;&#20987;&#26041;&#27861;&#8212;&#8212;&#23545;&#25239;&#24615;&#32418;&#22806;&#22359;&#65288;AdvIB&#65289;&#65292;&#21487;&#20197;&#20174;&#22810;&#20010;&#35282;&#24230;&#23545;&#28909;&#25104;&#20687;&#31995;&#32479;&#25191;&#34892;&#38544;&#34109;&#30340;&#40657;&#30418;&#25915;&#20987;&#65292;&#25104;&#21151;&#35825;&#23548;&#30446;&#26631;&#32418;&#22806;&#26816;&#27979;&#22120;&#22312;&#29289;&#29702;&#22330;&#26223;&#20013;&#23545;&#23545;&#35937;&#25110;&#20154;&#31867;&#36827;&#34892;&#35823;&#20998;&#31867;&#65292;&#24182;&#19988;&#19981;&#34987;&#20154;&#31867;&#23519;&#35273;&#12290;</title><link>http://arxiv.org/abs/2304.10712</link><description>&lt;p&gt;
&#29289;&#29702;&#19990;&#30028;&#20013;&#24858;&#24324;&#28909;&#32418;&#22806;&#25506;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
Fooling Thermal Infrared Detectors in Physical World. (arXiv:2304.10712v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#29289;&#29702;&#25915;&#20987;&#26041;&#27861;&#8212;&#8212;&#23545;&#25239;&#24615;&#32418;&#22806;&#22359;&#65288;AdvIB&#65289;&#65292;&#21487;&#20197;&#20174;&#22810;&#20010;&#35282;&#24230;&#23545;&#28909;&#25104;&#20687;&#31995;&#32479;&#25191;&#34892;&#38544;&#34109;&#30340;&#40657;&#30418;&#25915;&#20987;&#65292;&#25104;&#21151;&#35825;&#23548;&#30446;&#26631;&#32418;&#22806;&#26816;&#27979;&#22120;&#22312;&#29289;&#29702;&#22330;&#26223;&#20013;&#23545;&#23545;&#35937;&#25110;&#20154;&#31867;&#36827;&#34892;&#35823;&#20998;&#31867;&#65292;&#24182;&#19988;&#19981;&#34987;&#20154;&#31867;&#23519;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32418;&#22806;&#25104;&#20687;&#31995;&#32479;&#22312;&#34892;&#20154;&#26816;&#27979;&#21644;&#33258;&#21160;&#39550;&#39542;&#31561;&#26041;&#38754;&#26377;&#30528;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#23433;&#20840;&#24615;&#33021;&#22791;&#21463;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#30740;&#31350;&#25506;&#32034;&#20102;&#32418;&#22806;&#25104;&#20687;&#31995;&#32479;&#22312;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#19979;&#30340;&#23433;&#20840;&#24615;&#12290;&#36807;&#21435;&#30340;&#30740;&#31350;&#20351;&#29992;&#29289;&#29702;&#24178;&#25200;&#65292;&#22914;&#23567;&#28783;&#27873;&#21644;&#28909;&#8220;QR&#20195;&#30721;&#8221;&#26469;&#25915;&#20987;&#32418;&#22806;&#25104;&#20687;&#25506;&#27979;&#22120;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#24456;&#23481;&#26131;&#34987;&#23519;&#35273;&#65292;&#32570;&#20047;&#38544;&#31192;&#24615;&#12290;&#20854;&#20182;&#30740;&#31350;&#20154;&#21592;&#20351;&#29992;&#28909;&#21644;&#20919;&#22359;&#26469;&#27450;&#39575;&#32418;&#22806;&#25104;&#20687;&#25506;&#27979;&#22120;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#22312;&#20174;&#22810;&#20010;&#35282;&#24230;&#25191;&#34892;&#25915;&#20987;&#26041;&#38754;&#30340;&#33021;&#21147;&#26377;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#32570;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29289;&#29702;&#25915;&#20987;&#26041;&#27861;&#65292;&#31216;&#20026;&#23545;&#25239;&#24615;&#32418;&#22806;&#22359;&#65288;AdvIB&#65289;&#12290;&#36890;&#36807;&#20248;&#21270;&#23545;&#25239;&#24615;&#32418;&#22806;&#22359;&#30340;&#29289;&#29702;&#21442;&#25968;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20174;&#22810;&#20010;&#35282;&#24230;&#23545;&#28909;&#25104;&#20687;&#31995;&#32479;&#25191;&#34892;&#38544;&#34109;&#30340;&#40657;&#30418;&#25915;&#20987;&#12290;&#25105;&#20204;&#26681;&#25454;&#20854;&#26377;&#25928;&#24615;&#12289;&#38544;&#31192;&#24615;&#21644;&#31283;&#20581;&#24615;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;AdvIB&#21487;&#20197;&#25104;&#21151;&#35825;&#23548;&#30446;&#26631;&#32418;&#22806;&#26816;&#27979;&#22120;&#22312;&#29289;&#29702;&#22330;&#26223;&#20013;&#23545;&#23545;&#35937;&#25110;&#20154;&#31867;&#36827;&#34892;&#35823;&#20998;&#31867;&#65292;&#24182;&#19988;&#19981;&#34987;&#20154;&#31867;&#23519;&#35273;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24378;&#35843;&#20102;&#22312;&#32418;&#22806;&#25104;&#20687;&#31995;&#32479;&#20013;&#25552;&#39640;&#23433;&#20840;&#25514;&#26045;&#30340;&#24517;&#35201;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Infrared imaging systems have a vast array of potential applications in pedestrian detection and autonomous driving, and their safety performance is of great concern. However, few studies have explored the safety of infrared imaging systems in real-world settings. Previous research has used physical perturbations such as small bulbs and thermal "QR codes" to attack infrared imaging detectors, but such methods are highly visible and lack stealthiness. Other researchers have used hot and cold blocks to deceive infrared imaging detectors, but this method is limited in its ability to execute attacks from various angles. To address these shortcomings, we propose a novel physical attack called adversarial infrared blocks (AdvIB). By optimizing the physical parameters of the adversarial infrared blocks, this method can execute a stealthy black-box attack on thermal imaging system from various angles. We evaluate the proposed method based on its effectiveness, stealthiness, and robustness. Our
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#33258;&#22238;&#24402;&#25991;&#26412;&#29983;&#25104;&#20013;&#20351;&#29992;&#21487;&#25805;&#20316;&#27010;&#29575;&#27169;&#22411;&#26469;&#24378;&#21046;&#23454;&#26045;&#38480;&#21046;&#30340;&#25511;&#21046;&#26041;&#27861;GeLaTo&#65292;&#24182;&#21462;&#24471;&#20102;&#22312;&#24120;&#35265;&#30340;&#32422;&#26463;&#25991;&#26412;&#29983;&#25104;&#27979;&#35797;&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.07438</link><description>&lt;p&gt;
&#21487;&#25805;&#20316;&#30340;&#33258;&#22238;&#24402;&#35821;&#35328;&#29983;&#25104;&#25511;&#21046;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Tractable Control for Autoregressive Language Generation. (arXiv:2304.07438v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07438
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#33258;&#22238;&#24402;&#25991;&#26412;&#29983;&#25104;&#20013;&#20351;&#29992;&#21487;&#25805;&#20316;&#27010;&#29575;&#27169;&#22411;&#26469;&#24378;&#21046;&#23454;&#26045;&#38480;&#21046;&#30340;&#25511;&#21046;&#26041;&#27861;GeLaTo&#65292;&#24182;&#21462;&#24471;&#20102;&#22312;&#24120;&#35265;&#30340;&#32422;&#26463;&#25991;&#26412;&#29983;&#25104;&#27979;&#35797;&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#33258;&#22238;&#24402;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#29983;&#25104;&#28385;&#36275;&#22797;&#26434;&#38480;&#21046;&#30340;&#25991;&#26412;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#65306;&#21363;&#20351;&#26159;&#26368;&#31616;&#21333;&#30340;&#35789;&#27719;&#38480;&#21046;&#20063;&#20351;&#26465;&#20214;&#20998;&#24067;$\Pr(\text{text} | \alpha)$&#30340;&#37319;&#26679;&#21464;&#24471;&#19981;&#21487;&#35745;&#31639;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#21487;&#25805;&#20316;&#30340;&#27010;&#29575;&#27169;&#22411;&#23558;&#35789;&#27719;&#38480;&#21046;&#24378;&#21152;&#20110;&#33258;&#22238;&#24402;&#25991;&#26412;&#29983;&#25104;&#20013;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026; GeLaTo&#12290;&#20026;&#20102;&#35777;&#26126;&#36825;&#20010;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#31934;&#31616;&#30340;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#26469;&#25511;&#21046;&#20174;GPT2&#21040;&#33258;&#22238;&#24402;&#30340;&#29983;&#25104;&#12290;GeLaTo&#22312;&#32422;&#26463;&#25991;&#26412;&#29983;&#25104;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;CommonGen&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#22823;&#24133;&#20987;&#36133;&#20102;&#21508;&#31181;&#24378;&#22522;&#32447;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#19981;&#20165;&#20026;&#25511;&#21046;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24320;&#36767;&#20102;&#26032;&#30340;&#36884;&#24452;&#65292;&#36824;&#28608;&#21169;&#20154;&#20204;&#24320;&#21457;&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;&#21487;&#25805;&#20316;&#27010;&#29575;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the success of autoregressive large language models in text generation, it remains a major challenge to generate text that satisfies complex constraints: sampling from the conditional distribution $\Pr(\text{text} | \alpha)$ is intractable for even the simplest lexical constraints $\alpha$. To overcome this challenge, we propose to use tractable probabilistic models to impose lexical constraints in autoregressive text generation, which we refer to as GeLaTo. To demonstrate the effectiveness of this framework, we use distilled hidden Markov models to control autoregressive generation from GPT2. GeLaTo achieves state-of-the-art performance on CommonGen, a challenging benchmark for constrained text generation, beating a wide range of strong baselines by a large margin. Our work not only opens up new avenues for controlling large language models but also motivates the development of more expressive tractable probabilistic models.
&lt;/p&gt;</description></item><item><title>Open-TransMind&#26159;&#26234;&#33021;&#20132;&#36890;&#39046;&#22495;&#31532;&#19968;&#20010;&#22522;&#30784;&#27169;&#22411;&#25361;&#25112;&#36187;&#30340;&#26032;&#22522;&#20934;&#65292;&#26088;&#22312;&#35299;&#20915;&#25968;&#25454;&#37327;&#23569;&#12289;&#27867;&#21270;&#33021;&#21147;&#24046;&#20197;&#21450;&#32570;&#20047;&#22810;&#27169;&#24577;&#25216;&#26415;&#31561;&#20856;&#22411;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2304.06051</link><description>&lt;p&gt;
&#24320;&#25918;&#24335;&#26234;&#33021;&#20132;&#36890;&#22522;&#30784;&#27169;&#22411;&#25361;&#25112;&#36187;&#30340;&#26032;&#22522;&#20934;&#19982;&#22522;&#20934;&#25968;&#25454;&#38598; - Open-TransMind
&lt;/p&gt;
&lt;p&gt;
Open-TransMind: A New Baseline and Benchmark for 1st Foundation Model Challenge of Intelligent Transportation. (arXiv:2304.06051v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06051
&lt;/p&gt;
&lt;p&gt;
Open-TransMind&#26159;&#26234;&#33021;&#20132;&#36890;&#39046;&#22495;&#31532;&#19968;&#20010;&#22522;&#30784;&#27169;&#22411;&#25361;&#25112;&#36187;&#30340;&#26032;&#22522;&#20934;&#65292;&#26088;&#22312;&#35299;&#20915;&#25968;&#25454;&#37327;&#23569;&#12289;&#27867;&#21270;&#33021;&#21147;&#24046;&#20197;&#21450;&#32570;&#20047;&#22810;&#27169;&#24577;&#25216;&#26415;&#31561;&#20856;&#22411;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36817;&#24180;&#26469;&#35745;&#31639;&#33021;&#21147;&#21644;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#19981;&#26029;&#25552;&#21319;&#65292;&#22522;&#30784;&#27169;&#22411;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#30001;&#20110;&#20854;&#24378;&#22823;&#30340;&#33021;&#21147;&#21644;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#36825;&#31181;&#25216;&#26415;&#34987;&#36234;&#26469;&#36234;&#22810;&#30340;&#34892;&#19994;&#37319;&#29992;&#21644;&#24212;&#29992;&#12290;&#22312;&#26234;&#33021;&#20132;&#36890;&#34892;&#19994;&#20013;&#65292;&#20154;&#24037;&#26234;&#33021;&#38754;&#20020;&#30528;&#20197;&#19979;&#20856;&#22411;&#25361;&#25112;&#65306;&#25968;&#25454;&#37327;&#23569;&#12289;&#27867;&#21270;&#33021;&#21147;&#24046;&#20197;&#21450;&#32570;&#20047;&#22810;&#27169;&#24577;&#25216;&#26415;&#12290;&#22522;&#30784;&#27169;&#22411;&#25216;&#26415;&#21487;&#20197;&#26174;&#33879;&#32531;&#35299;&#19978;&#36848;&#38382;&#39064;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#31532;&#19968;&#20010;&#22522;&#30784;&#27169;&#22411;&#25361;&#25112;&#65292;&#26088;&#22312;&#22686;&#21152;&#22522;&#30784;&#27169;&#22411;&#25216;&#26415;&#22312;&#20132;&#36890;&#22330;&#26223;&#20013;&#30340;&#26222;&#21450;&#24230;&#65292;&#24182;&#20419;&#36827;&#26234;&#33021;&#20132;&#36890;&#34892;&#19994;&#30340;&#24555;&#36895;&#21457;&#23637;&#12290;&#35813;&#25361;&#25112;&#20998;&#20026;&#20004;&#20010;&#36187;&#36947;&#65306;&#20840;&#33021;&#22411;&#21644;&#36328;&#27169;&#24577;&#22270;&#20687;&#26816;&#32034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20026;&#36825;&#20004;&#20010;&#36187;&#36947;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#32447;&#21644;&#22522;&#20934;&#25968;&#25454;&#65292;&#31216;&#20026;Open-TransMind&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#26234;&#33021;&#20132;&#36890;&#39046;&#22495;&#30340;&#31532;&#19968;&#20010;&#22522;&#30784;&#27169;&#22411;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the continuous improvement of computing power and deep learning algorithms in recent years, the foundation model has grown in popularity. Because of its powerful capabilities and excellent performance, this technology is being adopted and applied by an increasing number of industries. In the intelligent transportation industry, artificial intelligence faces the following typical challenges: few shots, poor generalization, and a lack of multi-modal techniques. Foundation model technology can significantly alleviate the aforementioned issues. To address these, we designed the 1st Foundation Model Challenge, with the goal of increasing the popularity of foundation model technology in traffic scenarios and promoting the rapid development of the intelligent transportation industry. The challenge is divided into two tracks: all-in-one and cross-modal image retrieval. Furthermore, we provide a new baseline and benchmark for the two tracks, called Open-TransMind. According to our knowledg
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#39046;&#22495;&#30693;&#35782;&#36827;&#34892;&#20010;&#24615;&#21270;&#25968;&#23383;&#20581;&#24247;&#34892;&#20026;&#21464;&#38761;&#24178;&#39044;&#30340;&#31995;&#32479;&#65292;&#20854;&#21033;&#29992;&#21453;&#20107;&#23454;&#20363;&#23376;&#36827;&#34892;&#29305;&#24449;&#25511;&#21046;&#20197;&#39044;&#27979;&#24178;&#39044;&#25928;&#26524;&#24182;&#20248;&#21270;&#24178;&#39044;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.03392</link><description>&lt;p&gt;
&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#39046;&#22495;&#30693;&#35782;&#20010;&#24615;&#21270;&#25968;&#23383;&#20581;&#24247;&#34892;&#20026;&#21464;&#38761;&#24178;&#39044;
&lt;/p&gt;
&lt;p&gt;
Personalizing Digital Health Behavior Change Interventions using Machine Learning and Domain Knowledge. (arXiv:2304.03392v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03392
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#39046;&#22495;&#30693;&#35782;&#36827;&#34892;&#20010;&#24615;&#21270;&#25968;&#23383;&#20581;&#24247;&#34892;&#20026;&#21464;&#38761;&#24178;&#39044;&#30340;&#31995;&#32479;&#65292;&#20854;&#21033;&#29992;&#21453;&#20107;&#23454;&#20363;&#23376;&#36827;&#34892;&#29305;&#24449;&#25511;&#21046;&#20197;&#39044;&#27979;&#24178;&#39044;&#25928;&#26524;&#24182;&#20248;&#21270;&#24178;&#39044;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#27491;&#22312;&#24320;&#21457;&#19968;&#31181;&#34394;&#25311;&#25945;&#32451;&#31995;&#32479;&#65292;&#24110;&#21161;&#24739;&#32773;&#22362;&#25345;&#34892;&#20026;&#21464;&#38761;&#24178;&#39044;&#65288;BCI&#65289;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#39044;&#27979;&#24739;&#32773;&#26159;&#21542;&#20250;&#25191;&#34892;&#30446;&#26631;&#34892;&#20026;&#65292;&#24182;&#20351;&#29992;&#21453;&#20107;&#23454;&#20363;&#23376;&#36827;&#34892;&#29305;&#24449;&#25511;&#21046;&#65292;&#20197;&#25351;&#23548;&#20010;&#24615;&#21270;BCI&#12290;&#25105;&#20204;&#20351;&#29992;&#20855;&#26377;&#19981;&#21516;&#21709;&#24212;&#27700;&#24179;&#30340;&#27169;&#25311;&#24739;&#32773;&#25968;&#25454;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We are developing a virtual coaching system that helps patients adhere to behavior change interventions (BCI). Our proposed system predicts whether a patient will perform the targeted behavior and uses counterfactual examples with feature control to guide personalizsation of BCI. We evaluated our prediction model using simulated patient data with varying levels of receptivity to intervention.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#21487;&#20197;&#36890;&#36807;&#19968;&#20010;&#31616;&#21333;&#30340;&#25552;&#31034;&#26041;&#26696;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25191;&#34892;&#35745;&#31639;&#26426;&#20219;&#21153;&#65292;&#35813;&#26041;&#27861;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#24182;&#22312;MiniWoB++&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36234;&#20102;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.17491</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#35299;&#20915;&#35745;&#31639;&#26426;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Language Models can Solve Computer Tasks. (arXiv:2303.17491v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17491
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#21487;&#20197;&#36890;&#36807;&#19968;&#20010;&#31616;&#21333;&#30340;&#25552;&#31034;&#26041;&#26696;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25191;&#34892;&#35745;&#31639;&#26426;&#20219;&#21153;&#65292;&#35813;&#26041;&#27861;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#24182;&#22312;MiniWoB++&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36234;&#20102;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#22815;&#22312;&#35745;&#31639;&#26426;&#19978;&#25191;&#34892;&#36890;&#29992;&#20219;&#21153;&#30340;&#20195;&#29702;&#21487;&#20197;&#36890;&#36807;&#33258;&#21160;&#21270;&#37325;&#22797;&#20219;&#21153;&#21644;&#21327;&#21161;&#22797;&#26434;&#38382;&#39064;&#30340;&#35299;&#20915;&#26469;&#25552;&#39640;&#25928;&#29575;&#21644;&#29983;&#20135;&#21147;&#12290;&#29702;&#24819;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#20195;&#29702;&#24212;&#35813;&#33021;&#22815;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#21629;&#20196;&#35299;&#20915;&#26032;&#30340;&#35745;&#31639;&#26426;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#19987;&#23478;&#31034;&#33539;&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#36825;&#20004;&#32773;&#23545;&#20110;&#26032;&#20219;&#21153;&#26469;&#35828;&#37117;&#19981;&#20999;&#23454;&#38469;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#39044;&#20808;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20195;&#29702;&#21487;&#20197;&#20351;&#29992;&#19968;&#20010;&#31616;&#21333;&#30340;&#25552;&#31034;&#26041;&#26696;&#65288;RCI&#65289;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25351;&#23548;&#25191;&#34892;&#35745;&#31639;&#26426;&#20219;&#21153;&#65292;&#24182;&#22312;&#25209;&#35780;&#21644;&#25913;&#36827;&#36755;&#20986;&#30340;&#36807;&#31243;&#20013;&#21462;&#24471;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;RCI&#26041;&#27861;&#22312;&#33258;&#21160;&#21270;&#35745;&#31639;&#26426;&#20219;&#21153;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;LLM&#26041;&#27861;&#65292;&#24182;&#22312;MiniWoB++&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36234;&#20102;&#30417;&#30563;&#23398;&#20064;&#65288;SL&#65289;&#21644;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#27861;&#12290;RCI&#26041;&#27861;&#20351;&#29992;&#27599;&#20010;&#20219;&#21153;&#20165;&#26377;&#30340;&#23569;&#25968;&#31034;&#33539;&#65292;&#19982;&#26368;&#26032;&#30340;SL+RL&#26041;&#27861;&#30456;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;
Agents capable of carrying out general tasks on a computer can improve efficiency and productivity by automating repetitive tasks and assisting in complex problem-solving. Ideally, such agents should be able to solve new computer tasks presented to them through natural language commands. However, previous approaches to this problem require large amounts of expert demonstrations and task-specific reward functions, both of which are impractical for new tasks. In this work, we show that a pre-trained large language model (LLM) agent can execute computer tasks guided by natural language using a simple prompting scheme where the agent recursively criticizes and improves its output (RCI). The RCI approach significantly outperforms existing LLM methods for automating computer tasks and surpasses supervised learning (SL) and reinforcement learning (RL) approaches on the MiniWoB++ benchmark. RCI is competitive with the state-of-the-art SL+RL method, using only a handful of demonstrations per ta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#37319;&#29992;&#37327;&#23376;&#29289;&#29702;&#23398;&#30340;&#29702;&#35770;&#24037;&#20855;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21028;&#23450;&#25968;&#25454;&#36866;&#21512;&#20110;&#23616;&#37096;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#24517;&#35201;&#19988;&#20805;&#20998;&#26465;&#20214;&#65292;&#24182;&#23548;&#20986;&#20102;&#19968;&#31181;&#30456;&#24212;&#30340;&#39044;&#22788;&#29702;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.11249</link><description>&lt;p&gt;
&#20160;&#20040;&#35753;&#25968;&#25454;&#36866;&#21512;&#20110;&#23616;&#37096;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#65311;&#19968;&#31181;&#22522;&#20110;&#37327;&#23376;&#32416;&#32544;&#30340;&#24517;&#35201;&#19988;&#20805;&#20998;&#26465;&#20214;
&lt;/p&gt;
&lt;p&gt;
What Makes Data Suitable for a Locally Connected Neural Network? A Necessary and Sufficient Condition Based on Quantum Entanglement. (arXiv:2303.11249v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11249
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#37319;&#29992;&#37327;&#23376;&#29289;&#29702;&#23398;&#30340;&#29702;&#35770;&#24037;&#20855;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21028;&#23450;&#25968;&#25454;&#36866;&#21512;&#20110;&#23616;&#37096;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#24517;&#35201;&#19988;&#20805;&#20998;&#26465;&#20214;&#65292;&#24182;&#23548;&#20986;&#20102;&#19968;&#31181;&#30456;&#24212;&#30340;&#39044;&#22788;&#29702;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#20110;&#25968;&#25454;&#20998;&#24067;&#36866;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38382;&#39064;&#26159;&#19968;&#20010;&#22522;&#26412;&#30340;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;&#26412;&#25991;&#37319;&#29992;&#26469;&#33258;&#37327;&#23376;&#29289;&#29702;&#23398;&#30340;&#29702;&#35770;&#24037;&#20855;&#65292;&#38024;&#23545;&#21253;&#25324;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12289;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21644;&#23616;&#37096;&#33258;&#27880;&#24847;&#21147;&#27169;&#22411;&#22312;&#20869;&#30340;&#24191;&#27867;&#30340;&#23616;&#37096;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#65292;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#29702;&#35770;&#32467;&#26524;&#26159;&#65292;&#22312;&#26576;&#20123;&#29305;&#24449;&#30340;&#35268;&#33539;&#21010;&#20998;&#19979;&#65292;&#24403;&#25968;&#25454;&#20998;&#24067;&#25509;&#21463;&#20302;&#37327;&#23376;&#32416;&#32544;&#26102;&#65292;&#29305;&#23450;&#30340;&#23616;&#37096;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#25165;&#33021;&#22815;&#20934;&#30830;&#22320;&#39044;&#27979;&#35813;&#25968;&#25454;&#20998;&#24067;&#12290;&#20316;&#20026;&#26412;&#32467;&#26524;&#30340;&#23454;&#38469;&#24212;&#29992;&#65292;&#25105;&#20204;&#23548;&#20986;&#20102;&#19968;&#31181;&#39044;&#22788;&#29702;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#25968;&#25454;&#20998;&#24067;&#36866;&#21512;&#23616;&#37096;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#23545;&#24191;&#27867;&#30340;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#21457;&#29616;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#20351;&#29992;&#37327;&#23376;&#32416;&#32544;&#23558;&#40723;&#21169;&#24418;&#24335;&#25512;&#29702;&#30340;&#29289;&#29702;&#24037;&#20855;&#26469;&#36827;&#19968;&#27493;&#37319;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The question of what makes a data distribution suitable for deep learning is a fundamental open problem. Focusing on locally connected neural networks (a prevalent family of architectures that includes convolutional and recurrent neural networks as well as local self-attention models), we address this problem by adopting theoretical tools from quantum physics. Our main theoretical result states that a certain locally connected neural network is capable of accurate prediction over a data distribution if and only if the data distribution admits low quantum entanglement under certain canonical partitions of features. As a practical application of this result, we derive a preprocessing method for enhancing the suitability of a data distribution to locally connected neural networks. Experiments with widespread models over various datasets demonstrate our findings. We hope that our use of quantum entanglement will encourage further adoption of tools from physics for formally reasoning about 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26500;&#24314;&#20102;&#20013;&#22269;40&#20010;&#20027;&#35201;&#22478;&#24066;&#27004;&#23431;&#24314;&#31569;&#38754;&#31215;&#25968;&#25454;&#38598;&#24182;&#21033;&#29992;&#22810;&#20219;&#21153;&#23545;&#35937;&#20998;&#21106;&#22120;&#26041;&#27861;&#23398;&#20064;&#20102;&#24314;&#31569;&#29289;&#21344;&#22320;&#38754;&#31215;&#21644;&#39640;&#24230;&#65292;&#20026;&#22478;&#24066;&#35268;&#21010;&#25552;&#20379;&#25968;&#25454;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2303.02230</link><description>&lt;p&gt;
&#20013;&#22269;&#27004;&#23431;&#24314;&#31569;&#38754;&#31215;&#25968;&#25454;&#38598;&#19982;&#23398;&#20064;&#27969;&#31243;&#30340;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
Building Floorspace in China: A Dataset and Learning Pipeline. (arXiv:2303.02230v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02230
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26500;&#24314;&#20102;&#20013;&#22269;40&#20010;&#20027;&#35201;&#22478;&#24066;&#27004;&#23431;&#24314;&#31569;&#38754;&#31215;&#25968;&#25454;&#38598;&#24182;&#21033;&#29992;&#22810;&#20219;&#21153;&#23545;&#35937;&#20998;&#21106;&#22120;&#26041;&#27861;&#23398;&#20064;&#20102;&#24314;&#31569;&#29289;&#21344;&#22320;&#38754;&#31215;&#21644;&#39640;&#24230;&#65292;&#20026;&#22478;&#24066;&#35268;&#21010;&#25552;&#20379;&#25968;&#25454;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#20013;&#22269;40&#20010;&#20027;&#35201;&#22478;&#24066;&#27004;&#23431;&#24314;&#31569;&#38754;&#31215;&#65288;&#24314;&#31569;&#29289;&#21344;&#22320;&#38754;&#31215;&#21644;&#39640;&#24230;&#65289;&#27979;&#37327;&#30340;&#31532;&#19968;&#20010;&#37324;&#31243;&#30865;&#12290;&#25105;&#20204;&#21033;&#29992;&#22810;&#20219;&#21153;&#23545;&#35937;&#20998;&#21106;&#22120;&#26041;&#27861;&#22312;&#21516;&#19968;&#26694;&#26550;&#20013;&#23398;&#20064;&#24314;&#31569;&#29289;&#21344;&#22320;&#38754;&#31215;&#21644;&#39640;&#24230;&#65292;&#21033;&#29992;Sentinel-1&#21644;-2&#21355;&#26143;&#22270;&#20687;&#20316;&#20026;&#20027;&#35201;&#25968;&#25454;&#28304;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#35814;&#32454;&#30340;&#25968;&#25454;&#38598;&#26500;&#24314;&#21644;&#23398;&#20064;&#27969;&#31243;&#35828;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper provides a first milestone in measuring the floorspace of buildings (that is, building footprint and height) for 40 major Chinese cities. The intent is to maximize city coverage and, eventually provide longitudinal data. Doing so requires building on imagery that is of a medium-fine-grained granularity, as larger cross sections of cities and longer time series for them are only available in such format. We use a multi-task object segmenter approach to learn the building footprint and height in the same framework in parallel: (1) we determine the surface area is covered by any buildings (the square footage of occupied land); (2) we determine floorspace from multi-image representations of buildings from various angles to determine the height of buildings. We use Sentinel-1 and -2 satellite images as our main data source. The benefits of these data are their large cross-sectional and longitudinal scope plus their unrestricted accessibility. We provide a detailed description of 
&lt;/p&gt;</description></item><item><title>Alexa Arena&#26159;&#19968;&#20010;&#29992;&#25143;&#20013;&#24515;&#30340;&#27169;&#25311;&#24179;&#21488;&#65292;&#20855;&#26377;&#22810;&#20010;&#25151;&#38388;&#24067;&#23616;&#21644;&#21487;&#20132;&#20114;&#23545;&#35937;&#65292;&#29992;&#20110;&#21019;&#24314;&#20154;&#26426;&#20132;&#20114;&#20219;&#21153;&#65292;&#20026;&#23454;&#20307;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#25552;&#20379;&#20102;&#39640;&#25928;&#30340;&#25968;&#25454;&#25910;&#38598;&#21644;&#31995;&#32479;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2303.01586</link><description>&lt;p&gt;
Alexa Arena: &#19968;&#20010;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#20132;&#20114;&#24335;&#24179;&#21488;&#65292;&#29992;&#20110;&#30740;&#31350;&#23454;&#20307;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Alexa Arena: A User-Centric Interactive Platform for Embodied AI. (arXiv:2303.01586v2 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01586
&lt;/p&gt;
&lt;p&gt;
Alexa Arena&#26159;&#19968;&#20010;&#29992;&#25143;&#20013;&#24515;&#30340;&#27169;&#25311;&#24179;&#21488;&#65292;&#20855;&#26377;&#22810;&#20010;&#25151;&#38388;&#24067;&#23616;&#21644;&#21487;&#20132;&#20114;&#23545;&#35937;&#65292;&#29992;&#20110;&#21019;&#24314;&#20154;&#26426;&#20132;&#20114;&#20219;&#21153;&#65292;&#20026;&#23454;&#20307;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#25552;&#20379;&#20102;&#39640;&#25928;&#30340;&#25968;&#25454;&#25910;&#38598;&#21644;&#31995;&#32479;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102; Alexa Arena&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#25143;&#20013;&#24515;&#30340;&#27169;&#25311;&#24179;&#21488;&#65292;&#29992;&#20110;&#30740;&#31350;&#23454;&#20307;&#20154;&#24037;&#26234;&#33021;&#12290;Alexa Arena &#25552;&#20379;&#20102;&#21508;&#31181;&#22810;&#25151;&#38388;&#24067;&#23616;&#21644;&#21487;&#20132;&#20114;&#23545;&#35937;&#65292;&#20197;&#21019;&#24314;&#20154;&#26426;&#20132;&#20114;&#20219;&#21153;&#12290;Alexa Arena &#25317;&#26377;&#29992;&#25143;&#21451;&#22909;&#30340;&#22270;&#24418;&#21644;&#25511;&#21046;&#26426;&#21046;&#65292;&#25903;&#25345;&#24320;&#21457;&#36866;&#29992;&#20110;&#26222;&#36890;&#20154;&#31867;&#29992;&#25143;&#30340;&#28216;&#25103;&#21270;&#26426;&#22120;&#20154;&#20219;&#21153;&#65292;&#20174;&#32780;&#20026;&#39640;&#25928;&#30340;&#20154;&#26426;&#20132;&#20114;&#25968;&#25454;&#25910;&#38598;&#21644; EAI &#31995;&#32479;&#35780;&#20272;&#24320;&#36767;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#23545;&#35805;&#21551;&#29992;&#30340;&#25351;&#20196;&#36319;&#38543;&#22522;&#20934;&#65292;&#24182;&#20026;&#20854;&#25552;&#20379;&#20102;&#22522;&#32447;&#32467;&#26524;&#12290;&#25105;&#20204;&#21521;&#20844;&#20247;&#25552;&#20379; Alexa Arena&#65292;&#20197;&#20419;&#36827;&#26500;&#24314;&#20855;&#26377;&#26222;&#36866;&#24615;&#21644;&#36741;&#21161;&#24615;&#30340;&#23454;&#20307;&#20195;&#29702;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Alexa Arena, a user-centric simulation platform for Embodied AI (EAI) research. Alexa Arena provides a variety of multi-room layouts and interactable objects, for the creation of human-robot interaction (HRI) missions. With user-friendly graphics and control mechanisms, Alexa Arena supports the development of gamified robotic tasks readily accessible to general human users, thus opening a new venue for high-efficiency HRI data collection and EAI system evaluation. Along with the platform, we introduce a dialog-enabled instruction-following benchmark and provide baseline results for it. We make Alexa Arena publicly available to facilitate research in building generalizable and assistive embodied agents.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;SQRL&#65292;&#21487;&#20197;&#26080;&#30417;&#30563;&#22320;&#20174;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#20013;&#25512;&#26029;&#20986;&#32479;&#35745;&#35268;&#21017;&#65307;&#22312;&#20998;&#31867;&#12289;&#30446;&#26631;&#26816;&#27979;&#21644;&#25968;&#25454;&#22635;&#20805;&#31561;&#20219;&#21153;&#20013;&#65292;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36890;&#24120;&#20250;&#36829;&#21453;&#36825;&#20123;&#35268;&#21017;&#65292;&#20294;&#26159;&#36890;&#36807;&#22312;&#27979;&#35797;&#26102;&#38388;&#20869;&#23545;&#27169;&#22411;&#36827;&#34892;&#36866;&#24212;&#65292;&#36829;&#35268;&#34892;&#20026;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#24182;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.01433</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26159;&#21542;&#33021;&#23398;&#20064;&#20174;&#25968;&#25454;&#20013;&#25512;&#26029;&#20986;&#30340;&#32479;&#35745;&#35268;&#21017;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do Machine Learning Models Learn Statistical Rules Inferred from Data?. (arXiv:2303.01433v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01433
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;SQRL&#65292;&#21487;&#20197;&#26080;&#30417;&#30563;&#22320;&#20174;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#20013;&#25512;&#26029;&#20986;&#32479;&#35745;&#35268;&#21017;&#65307;&#22312;&#20998;&#31867;&#12289;&#30446;&#26631;&#26816;&#27979;&#21644;&#25968;&#25454;&#22635;&#20805;&#31561;&#20219;&#21153;&#20013;&#65292;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36890;&#24120;&#20250;&#36829;&#21453;&#36825;&#20123;&#35268;&#21017;&#65292;&#20294;&#26159;&#36890;&#36807;&#22312;&#27979;&#35797;&#26102;&#38388;&#20869;&#23545;&#27169;&#22411;&#36827;&#34892;&#36866;&#24212;&#65292;&#36829;&#35268;&#34892;&#20026;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#24182;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#33021;&#20250;&#22312;&#22823;&#37327;&#25968;&#25454;&#20013;&#38544;&#34255;&#19968;&#20123;&#37325;&#35201;&#30340;&#38169;&#35823;&#65292;&#32780;&#36825;&#20123;&#38169;&#35823;&#36890;&#24120;&#36829;&#21453;&#20102;&#20154;&#31867;&#30452;&#35273;&#30340;&#35268;&#21017;&#12290;&#28982;&#32780;&#65292;&#20197;&#20154;&#31867;&#30693;&#35782;&#20026;&#22522;&#30784;&#30340;&#35268;&#21017;&#24448;&#24448;&#19981;&#26131;&#25193;&#23637;&#25110;&#27491;&#24335;&#21270;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;SQRL&#65292;&#23427;&#23558;&#22522;&#20110;&#36923;&#36753;&#30340;&#26041;&#27861;&#19982;&#32479;&#35745;&#25512;&#26029;&#30456;&#32467;&#21512;&#65292;&#26080;&#38656;&#30417;&#30563;&#21363;&#21487;&#20174;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#20013;&#25512;&#23548;&#20986;&#36825;&#20123;&#35268;&#21017;&#65292;&#24182;&#37327;&#21270;&#27169;&#22411;&#24050;&#32463;&#23398;&#20064;&#21040;&#36825;&#20123;&#35268;&#21017;&#30340;&#31243;&#24230;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#27979;&#35797;&#26102;&#38388;&#20869;&#35843;&#25972;&#27169;&#22411;&#20197;&#20943;&#23569;&#35268;&#21017;&#36829;&#35268;&#24182;&#20135;&#29983;&#26356;&#36830;&#36143;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;SQRL&#21487;&#20197;&#22312;&#35270;&#35273;&#12289;&#34920;&#26684;&#21644;&#35821;&#35328;&#22330;&#26223;&#19979;&#30340;&#25968;&#25454;&#38598;&#20013;&#29983;&#25104;&#22810;&#36798;30&#19975;&#26465;&#35268;&#21017;&#12290;&#25105;&#20204;&#21457;&#29616;&#26368;&#20808;&#36827;&#30340;&#20998;&#31867;&#12289;&#30446;&#26631;&#26816;&#27979;&#21644;&#25968;&#25454;&#22635;&#20805;&#27169;&#22411;&#36829;&#21453;&#20102;&#36825;&#20123;&#35268;&#21017;&#39640;&#36798;158K&#27425;&#12290;&#32780;&#27979;&#35797;&#26102;&#38388;&#30340;&#36866;&#24212;&#21487;&#20197;&#23558;&#36825;&#20123;&#36829;&#35268;&#34892;&#20026;&#20943;&#23569;&#39640;&#36798;68.7%&#65292;&#24182;&#25552;&#39640;&#30456;&#23545;&#24615;&#33021;&#39640;&#36798;32%&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models can make critical errors that are easily hidden within vast amounts of data. Such errors often run counter to rules based on human intuition. However, rules based on human knowledge are challenging to scale or to even formalize. We thereby seek to infer statistical rules from the data and quantify the extent to which a model has learned them. We propose a framework SQRL that integrates logic-based methods with statistical inference to derive these rules from a model's training data without supervision. We further show how to adapt models at test time to reduce rule violations and produce more coherent predictions. SQRL generates up to 300K rules over datasets from vision, tabular, and language settings. We uncover up to 158K violations of those rules by state-of-the-art models for classification, object detection, and data imputation. Test-time adaptation reduces these violations by up to 68.7% with relative performance improvement up to 32%. SQRL is available a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#36801;&#31227;&#23398;&#20064;&#22312;&#20302;&#25968;&#25454;&#29366;&#24577;&#19979;&#30340;&#25968;&#25454;&#32553;&#25918;&#65292;&#21457;&#29616;&#20102;&#19968;&#31181;&#31216;&#20026;&#24748;&#23830;&#23398;&#20064;&#30340;&#29616;&#35937;&#65292;&#23427;&#21453;&#26144;&#20102;&#23398;&#20064;&#31639;&#27861;&#30340;&#20808;&#39564;&#30693;&#35782;&#19982;&#20219;&#21153;&#20043;&#38388;&#30340;&#20860;&#23481;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2302.07348</link><description>&lt;p&gt;
&#24748;&#23830;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Cliff-Learning. (arXiv:2302.07348v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07348
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#36801;&#31227;&#23398;&#20064;&#22312;&#20302;&#25968;&#25454;&#29366;&#24577;&#19979;&#30340;&#25968;&#25454;&#32553;&#25918;&#65292;&#21457;&#29616;&#20102;&#19968;&#31181;&#31216;&#20026;&#24748;&#23830;&#23398;&#20064;&#30340;&#29616;&#35937;&#65292;&#23427;&#21453;&#26144;&#20102;&#23398;&#20064;&#31639;&#27861;&#30340;&#20808;&#39564;&#30693;&#35782;&#19982;&#20219;&#21153;&#20043;&#38388;&#30340;&#20860;&#23481;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#22312;&#20302;&#19979;&#28216;&#25968;&#25454;&#29366;&#24577;&#19979;&#30340;&#25968;&#25454;&#32553;&#25918;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#29616;&#35937;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#24748;&#23830;&#23398;&#20064;&#12290;&#24748;&#23830;&#23398;&#20064;&#26159;&#25351;&#22312;&#25968;&#25454;&#32553;&#25918;&#27861;&#21017;&#30340;&#26576;&#20123;&#21306;&#22495;&#20013;&#65292;&#24615;&#33021;&#30340;&#25552;&#21319;&#36895;&#24230;&#24555;&#20110;&#24130;&#24459;&#36895;&#24230;&#30340;&#29616;&#35937;&#65288;&#21363;&#22312;&#23545;&#25968;&#32553;&#25918;&#22270;&#19978;&#30340;&#20985;&#24418;&#21306;&#22495;&#65289;&#12290;&#25105;&#20204;&#23545;&#22522;&#30784;&#27169;&#22411;&#30340;&#24748;&#23830;&#23398;&#20064;&#36827;&#34892;&#20102;&#28145;&#20837;&#35843;&#26597;&#24182;&#30740;&#31350;&#20102;&#36825;&#19968;&#29616;&#35937;&#30340;&#29609;&#20855;&#27169;&#22411;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#24748;&#23830;&#23398;&#20064;&#30340;&#31243;&#24230;&#21453;&#26144;&#20102;&#23398;&#20064;&#31639;&#27861;&#30340;&#20808;&#39564;&#30693;&#35782;&#21644;&#25152;&#23398;&#20219;&#21153;&#20043;&#38388;&#30340;&#20860;&#23481;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the data-scaling of transfer learning from foundation models in the low-downstream-data regime. We observe an intriguing phenomenon which we call cliff-learning. Cliff-learning refers to regions of data-scaling laws where performance improves at a faster than power law rate (i.e. regions of concavity on a log-log scaling plot). We conduct an in-depth investigation of foundation-model cliff-learning and study toy models of the phenomenon. We observe that the degree of cliff-learning reflects the degree of compatibility between the priors of a learning algorithm and the task being learned.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#37327;&#23376;&#22810;&#26234;&#33021;&#20307;&#28436;&#21592;-&#35780;&#35770;&#32593;&#32476;&#30340;&#26032;&#31639;&#27861;&#65292;&#29992;&#20110;&#33258;&#20027;&#26500;&#24314;&#19968;&#20010;&#24378;&#22823;&#30340;&#31227;&#21160;&#35775;&#38382;&#31995;&#32479;&#65292;&#21033;&#29992;&#22810;&#20010;&#26080;&#20154;&#26426;&#65292;&#24182;&#21033;&#29992;&#37327;&#23376;&#35745;&#31639;&#30340;&#21407;&#21017;&#20197;&#25552;&#39640;&#20854;&#35757;&#32451;&#36807;&#31243;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2302.04445</link><description>&lt;p&gt;
&#22810;&#26080;&#20154;&#26426;&#31995;&#32479;&#20013;&#30340;&#21512;&#20316;&#31227;&#21160;&#35775;&#38382;&#30340;&#37327;&#23376;&#22810;&#26234;&#33021;&#20307;&#28436;&#21592;-&#35780;&#35770;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Quantum Multi-Agent Actor-Critic Networks for Cooperative Mobile Access in Multi-UAV Systems. (arXiv:2302.04445v2 [cs.MA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04445
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#37327;&#23376;&#22810;&#26234;&#33021;&#20307;&#28436;&#21592;-&#35780;&#35770;&#32593;&#32476;&#30340;&#26032;&#31639;&#27861;&#65292;&#29992;&#20110;&#33258;&#20027;&#26500;&#24314;&#19968;&#20010;&#24378;&#22823;&#30340;&#31227;&#21160;&#35775;&#38382;&#31995;&#32479;&#65292;&#21033;&#29992;&#22810;&#20010;&#26080;&#20154;&#26426;&#65292;&#24182;&#21033;&#29992;&#37327;&#23376;&#35745;&#31639;&#30340;&#21407;&#21017;&#20197;&#25552;&#39640;&#20854;&#35757;&#32451;&#36807;&#31243;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#37327;&#23376;&#22810;&#26234;&#33021;&#20307;&#28436;&#21592;-&#35780;&#35770;&#32593;&#32476;&#65288;QMACN&#65289;&#30340;&#26032;&#31639;&#27861;&#65292;&#29992;&#20110;&#33258;&#20027;&#26500;&#24314;&#19968;&#20010;&#24378;&#22823;&#30340;&#31227;&#21160;&#35775;&#38382;&#31995;&#32479;&#65292;&#21033;&#29992;&#22810;&#20010;&#26080;&#20154;&#26426;&#65288;UAV&#65289;&#12290;&#22312;&#20419;&#36827;&#22810;&#20010;&#26080;&#20154;&#26426;&#20043;&#38388;&#30340;&#21327;&#20316;&#26041;&#38754;&#65292;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#25216;&#26415;&#30340;&#24212;&#29992;&#34987;&#35748;&#20026;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#20351;&#26080;&#20154;&#26426;&#33021;&#22815;&#38598;&#20307;&#23398;&#20064;&#65292;&#22312;&#20849;&#20139;&#29615;&#22659;&#20013;&#20248;&#21270;&#23427;&#20204;&#30340;&#34892;&#21160;&#65292;&#26368;&#32456;&#23548;&#33268;&#26356;&#26377;&#25928;&#30340;&#21512;&#20316;&#34892;&#20026;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#36816;&#29992;&#20102;&#37327;&#23376;&#35745;&#31639;&#65288;QC&#65289;&#30340;&#21407;&#21017;&#65292;&#20197;&#22686;&#24378;&#28041;&#21450;&#30340;&#26080;&#20154;&#26426;&#30340;&#35757;&#32451;&#36807;&#31243;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#21033;&#29992;&#37327;&#23376;&#35745;&#31639;&#30340;&#29420;&#29305;&#35745;&#31639;&#20248;&#21183;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#25552;&#39640;&#26080;&#20154;&#26426;&#31995;&#32479;&#30340;&#25972;&#20307;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;QC&#24341;&#20837;&#21487;&#25193;&#23637;&#24615;&#25361;&#25112;&#65292;&#22240;&#20026;&#36817;&#20013;&#38388;&#35268;&#27169;&#37327;&#23376;&#65288;NISQ&#65289;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel algorithm, named quantum multi-agent actor-critic networks (QMACN) for autonomously constructing a robust mobile access system employing multiple unmanned aerial vehicles (UAVs). In the context of facilitating collaboration among multiple unmanned aerial vehicles (UAVs), the application of multi-agent reinforcement learning (MARL) techniques is regarded as a promising approach. These methods enable UAVs to learn collectively, optimizing their actions within a shared environment, ultimately leading to more efficient cooperative behavior. Furthermore, the principles of a quantum computing (QC) are employed in our study to enhance the training process and inference capabilities of the UAVs involved. By leveraging the unique computational advantages of quantum computing, our approach aims to boost the overall effectiveness of the UAV system. However, employing a QC introduces scalability challenges due to the near intermediate-scale quantum (NISQ) limitation ass
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20851;&#31995;Weisfeiler-Leman&#31639;&#27861;&#30340;&#29702;&#35770;&#65292;&#20026;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#20102;&#29702;&#35770;&#35299;&#37322;&#65292;&#24182;&#23545;&#21508;&#31181;&#27169;&#22411;&#30340;&#34920;&#36798;&#33021;&#21147;&#36827;&#34892;&#20102;&#34920;&#24449;&#65292;&#24182;&#35299;&#37322;&#20102;&#19968;&#20123;&#24191;&#27867;&#37319;&#29992;&#30340;&#23454;&#38469;&#35774;&#35745;&#36873;&#25321;&#30340;&#20248;&#28857;&#12290;</title><link>http://arxiv.org/abs/2302.02209</link><description>&lt;p&gt;
&#22522;&#20110;&#20851;&#31995;Weisfeiler-Leman&#30340;&#38142;&#36335;&#39044;&#27979;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
A Theory of Link Prediction via Relational Weisfeiler-Leman. (arXiv:2302.02209v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02209
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20851;&#31995;Weisfeiler-Leman&#31639;&#27861;&#30340;&#29702;&#35770;&#65292;&#20026;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#20102;&#29702;&#35770;&#35299;&#37322;&#65292;&#24182;&#23545;&#21508;&#31181;&#27169;&#22411;&#30340;&#34920;&#36798;&#33021;&#21147;&#36827;&#34892;&#20102;&#34920;&#24449;&#65292;&#24182;&#35299;&#37322;&#20102;&#19968;&#20123;&#24191;&#27867;&#37319;&#29992;&#30340;&#23454;&#38469;&#35774;&#35745;&#36873;&#25321;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#26159;&#29992;&#20110;&#22270;&#32467;&#26500;&#25968;&#25454;&#34920;&#31034;&#23398;&#20064;&#30340;&#37325;&#35201;&#27169;&#22411;&#12290;&#23613;&#31649;&#25105;&#20204;&#24050;&#32463;&#24456;&#22909;&#22320;&#29702;&#35299;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#31616;&#21333;&#22270;&#19978;&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#65292;&#20294;&#23545;&#20110;&#30693;&#35782;&#22270;&#35889;&#65292;&#25105;&#20204;&#30340;&#29702;&#35299;&#20173;&#28982;&#19981;&#23436;&#25972;&#12290;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#20026;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#31995;&#32479;&#24615;&#30340;&#29702;&#35299;&#65292;&#20197;&#35299;&#20915;&#38142;&#36335;&#39044;&#27979;&#31561;&#37325;&#35201;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#28041;&#21450;&#19968;&#31181;&#32479;&#19968;&#30340;&#35270;&#35282;&#12289;&#30475;&#20284;&#19981;&#30456;&#20851;&#30340;&#27169;&#22411;&#65292;&#24182;&#35299;&#38145;&#20102;&#19968;&#31995;&#21015;&#20854;&#20182;&#27169;&#22411;&#12290;&#36890;&#36807;&#30456;&#24212;&#30340;&#20851;&#31995;Weisfeiler-Leman&#31639;&#27861;&#65292;&#34920;&#24449;&#20102;&#21508;&#31181;&#27169;&#22411;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#27492;&#20998;&#26512;&#34987;&#25193;&#23637;&#20197;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#31867;&#21035;&#25429;&#25417;&#30340;&#20989;&#25968;&#31867;&#36827;&#34892;&#31934;&#30830;&#36923;&#36753;&#25551;&#36848;&#12290;&#25552;&#20986;&#30340;&#29702;&#35770;&#21457;&#29616;&#35299;&#37322;&#20102;&#19968;&#20123;&#24191;&#27867;&#37319;&#29992;&#30340;&#23454;&#38469;&#35774;&#35745;&#36873;&#25321;&#30340;&#20248;&#28857;&#65292;&#24182;&#24471;&#21040;&#20102;&#32463;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks are prominent models for representation learning over graph-structured data. While the capabilities and limitations of these models are well-understood for simple graphs, our understanding remains incomplete in the context of knowledge graphs. Our goal is to provide a systematic understanding of the landscape of graph neural networks for knowledge graphs pertaining to the prominent task of link prediction. Our analysis entails a unifying perspective on seemingly unrelated models and unlocks a series of other models. The expressive power of various models is characterized via a corresponding relational Weisfeiler-Leman algorithm. This analysis is extended to provide a precise logical characterization of the class of functions captured by a class of graph neural networks. The theoretical findings presented in this paper explain the benefits of some widely employed practical design choices, which are validated empirically.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#23457;&#35270; Bellman Errors&#65292;&#21457;&#29616;&#20043;&#21069;&#30340;Bellman Errors &#26041;&#27861;&#38656;&#35201;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#25165;&#33021;&#34920;&#29616;&#33391;&#22909;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#26356;&#20934;&#30830;&#30340; MSBE &#20272;&#35745;&#22120;&#65292;&#22312;&#31163;&#25955;&#25511;&#21046;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2302.00141</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270; Bellman Errors &#29992;&#20110;&#31163;&#32447;&#27169;&#22411;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Revisiting Bellman Errors for Offline Model Selection. (arXiv:2302.00141v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00141
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270; Bellman Errors&#65292;&#21457;&#29616;&#20043;&#21069;&#30340;Bellman Errors &#26041;&#27861;&#38656;&#35201;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#25165;&#33021;&#34920;&#29616;&#33391;&#22909;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#26356;&#20934;&#30830;&#30340; MSBE &#20272;&#35745;&#22120;&#65292;&#22312;&#31163;&#25955;&#25511;&#21046;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#27169;&#22411;&#36873;&#25321;&#65288;OMS&#65289;&#21363;&#22312;&#21482;&#26377;&#24050;&#35760;&#24405;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20174;&#20247;&#22810;&#31574;&#30053;&#20013;&#36873;&#25321;&#26368;&#20339;&#31574;&#30053;&#65292;&#23545;&#20110;&#22312;&#23454;&#38469;&#29615;&#22659;&#19979;&#24212;&#29992;&#31163;&#32447;RL&#33267;&#20851;&#37325;&#35201;&#12290;&#19968;&#20010;&#32463;&#36807;&#24191;&#27867;&#25506;&#35752;&#30340;&#24819;&#27861;&#26159;&#26681;&#25454;&#30456;&#20851;Q&#20989;&#25968;&#30340;&#22343;&#26041;Bellman&#35823;&#24046;&#65288;MSBE&#65289;&#36873;&#25321;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#20043;&#21069;&#30340;&#30740;&#31350;&#19968;&#30452;&#22312;&#20351;&#29992;Bellman&#35823;&#24046;&#26102;&#26080;&#27861;&#33719;&#24471;&#36275;&#22815;&#30340;OMS&#24615;&#33021;&#65292;&#23548;&#33268;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#25918;&#24323;&#27492;&#24819;&#27861;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#38416;&#36848;&#20102;&#20026;&#20160;&#20040;&#20043;&#21069;&#30340;&#32467;&#26524;&#20351;&#29992;Bellman&#35823;&#24046;&#26102;&#20250;&#30475;&#21040;&#24754;&#35266;&#30340;&#32467;&#26524;&#65292;&#24182;&#30830;&#23450;&#20102;&#22522;&#20110;Bellman&#35823;&#24046;&#30340;OMS&#31639;&#27861;&#23558;&#34920;&#29616;&#33391;&#22909;&#30340;&#26465;&#20214;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#27604;&#20043;&#21069;&#26041;&#27861;&#26356;&#20934;&#30830;&#30340;MSBE&#30340;&#26032;&#30340;&#20272;&#35745;&#22120;&#12290;&#25105;&#20204;&#30340;&#20272;&#35745;&#22120;&#22312;&#19981;&#21516;&#30340;&#31163;&#25955;&#25511;&#21046;&#20219;&#21153;&#65288;&#21253;&#25324; Atari &#28216;&#25103;&#65289;&#19978;&#33719;&#24471;&#20102;&#20986;&#33394;&#30340;OMS&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline model selection (OMS), that is, choosing the best policy from a set of many policies given only logged data, is crucial for applying offline RL in real-world settings. One idea that has been extensively explored is to select policies based on the mean squared Bellman error (MSBE) of the associated Q-functions. However, previous work has struggled to obtain adequate OMS performance with Bellman errors, leading many researchers to abandon the idea. To this end, we elucidate why previous work has seen pessimistic results with Bellman errors and identify conditions under which OMS algorithms based on Bellman errors will perform well. Moreover, we develop a new estimator of the MSBE that is more accurate than prior methods. Our estimator obtains impressive OMS performance on diverse discrete control tasks, including Atari games.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#24179;&#31283;&#20004;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#30340;&#31574;&#30053;&#65292;&#33021;&#22815;&#22788;&#29702;&#24179;&#28369;&#21464;&#21270;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#31574;&#30053;&#22312;&#20108;&#27425;Lipschitz&#36830;&#32493;&#30340;&#24773;&#20917;&#19979;&#30340;&#36951;&#25022;&#20026; $\tilde O(T^{3/5})$&#12290;</title><link>http://arxiv.org/abs/2301.12366</link><description>&lt;p&gt;
&#24179;&#28369;&#30340;&#38750;&#24179;&#31283;&#36830;&#32493;&#36172;&#21338;&#26426;
&lt;/p&gt;
&lt;p&gt;
Smooth Non-Stationary Bandits. (arXiv:2301.12366v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12366
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#24179;&#31283;&#20004;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#30340;&#31574;&#30053;&#65292;&#33021;&#22815;&#22788;&#29702;&#24179;&#28369;&#21464;&#21270;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#31574;&#30053;&#22312;&#20108;&#27425;Lipschitz&#36830;&#32493;&#30340;&#24773;&#20917;&#19979;&#30340;&#36951;&#25022;&#20026; $\tilde O(T^{3/5})$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#22312;&#32447;&#20915;&#31574;&#24212;&#29992;&#20013;&#65292;&#29615;&#22659;&#37117;&#26159;&#38750;&#24179;&#31283;&#30340;&#65292;&#22240;&#27492;&#20351;&#29992;&#33021;&#22815;&#22788;&#29702;&#21464;&#21270;&#30340;&#36172;&#21338;&#31639;&#27861;&#33267;&#20851;&#37325;&#35201;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#26159;&#20026;&#20102;&#20445;&#25252;&#38750;&#24179;&#28369;&#21464;&#21270;&#32780;&#35774;&#35745;&#30340;&#65292;&#20165;&#21463;&#21040;&#24635;&#21464;&#24046;&#25110;&#26102;&#38388;&#19978;&#30340;Lipschitz&#24615;&#30340;&#38480;&#21046;&#65292;&#20854;&#20013;&#23427;&#20204;&#20445;&#35777;$\tilde \Theta(T^{2/3})$&#30340;&#36951;&#25022;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#29615;&#22659;&#32463;&#24120;&#20197;&#24179;&#31283;&#30340;&#26041;&#24335;&#25913;&#21464;&#65292;&#22240;&#27492;&#36825;&#31181;&#31639;&#27861;&#21487;&#33021;&#20250;&#22312;&#36825;&#20123;&#35774;&#32622;&#20013;&#20135;&#29983;&#27604;&#24517;&#35201;&#26356;&#39640;&#30340;&#36951;&#25022;&#65292;&#24182;&#19988;&#19981;&#21033;&#29992;&#21464;&#21270;&#29575;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#38750;&#24179;&#31283;&#30340;&#20004;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#20551;&#35774;&#33218;&#30340;&#24179;&#22343;&#22238;&#25253;&#26159;&#19968;&#20010;$\beta$-H\''older&#20989;&#25968;&#65292;&#21363;&#23427;&#26159;$(\beta-1)$&#27425;Lipschitz&#36830;&#32493;&#21487;&#24494;&#20998;&#30340;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#31574;&#30053;&#65292;&#23545;&#20110;$\beta=2$&#65292;&#23427;&#30340;&#36951;&#25022;&#20026;$\tilde O(T^{3/5})$&#65292;&#20174;&#32780;&#39318;&#27425;&#22312;&#24179;&#28369;&#21644;&#38750;&#24179;&#28369;&#20043;&#38388;&#36827;&#34892;&#20102;&#21306;&#20998;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#20219;&#24847;$\Omg(T^{(\beta+1)/(2\beta+1)})$&#30340;&#19979;&#30028;&#26469;&#34917;&#20805;&#36825;&#20010;&#32467;&#26524;&#65292;&#35828;&#26126;&#20102;&#36825;&#20010;&#38382;&#39064;&#30340;&#22256;&#38590;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many applications of online decision making, the environment is non-stationary and it is therefore crucial to use bandit algorithms that handle changes. Most existing approaches are designed to protect against non-smooth changes, constrained only by total variation or Lipschitzness over time, where they guarantee $\tilde \Theta(T^{2/3})$ regret. However, in practice environments are often changing {\bf smoothly}, so such algorithms may incur higher-than-necessary regret in these settings and do not leverage information on the rate of change. We study a non-stationary two-armed bandits problem where we assume that an arm's mean reward is a $\beta$-H\"older function over (normalized) time, meaning it is $(\beta-1)$-times Lipschitz-continuously differentiable. We show the first separation between the smooth and non-smooth regimes by presenting a policy with $\tilde O(T^{3/5})$ regret for $\beta=2$. We complement this result by an $\Omg(T^{(\beta+1)/(2\beta+1)})$ lower bound for any int
&lt;/p&gt;</description></item><item><title>Tracr&#26159;&#19968;&#20010;&#32534;&#35793;&#22120;&#65292;&#23558;&#21487;&#35835;&#24615;&#24378;&#30340;&#31243;&#24207;&#32534;&#35793;&#25104;&#26631;&#20934;&#30340;&#20165;&#35299;&#30721;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#35813;&#32534;&#35793;&#27169;&#22411;&#30340;&#24050;&#30693;&#32467;&#26500;&#21487;&#20197;&#29992;&#20110;&#35774;&#35745;&#23454;&#39564;&#21644;&#35780;&#20272;&#21487;&#35299;&#37322;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2301.05062</link><description>&lt;p&gt;
Tracr: &#32534;&#35793;&#21464;&#21387;&#22120;&#27169;&#22411;&#20316;&#20026;&#21487;&#35299;&#37322;&#24615;&#23454;&#39564;&#23460;
&lt;/p&gt;
&lt;p&gt;
Tracr: Compiled Transformers as a Laboratory for Interpretability. (arXiv:2301.05062v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.05062
&lt;/p&gt;
&lt;p&gt;
Tracr&#26159;&#19968;&#20010;&#32534;&#35793;&#22120;&#65292;&#23558;&#21487;&#35835;&#24615;&#24378;&#30340;&#31243;&#24207;&#32534;&#35793;&#25104;&#26631;&#20934;&#30340;&#20165;&#35299;&#30721;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#35813;&#32534;&#35793;&#27169;&#22411;&#30340;&#24050;&#30693;&#32467;&#26500;&#21487;&#20197;&#29992;&#20110;&#35774;&#35745;&#23454;&#39564;&#21644;&#35780;&#20272;&#21487;&#35299;&#37322;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#21487;&#35835;&#24615;&#24378;&#30340;&#31243;&#24207;&#32534;&#35793;&#25104;&#26631;&#20934;&#30340;&#20165;&#35299;&#30721;&#21464;&#21387;&#22120;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32534;&#35793;&#22120;Tracr&#29983;&#25104;&#20855;&#26377;&#24050;&#30693;&#32467;&#26500;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#29992;&#20110;&#35774;&#35745;&#23454;&#39564;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#20351;&#29992;&#23427;&#26469;&#30740;&#31350;&#25191;&#34892;&#22810;&#27493;&#31639;&#27861;&#30340;&#21464;&#21387;&#22120;&#20013;&#30340;&#8220;&#21472;&#21152;&#8221;&#12290;&#27492;&#22806;&#65292;Tracr&#32534;&#35793;&#27169;&#22411;&#30340;&#24050;&#30693;&#32467;&#26500;&#21487;&#20197;&#20316;&#20026;&#35780;&#20272;&#21487;&#35299;&#37322;&#26041;&#27861;&#30340;&#30495;&#23454;&#22522;&#20934;&#12290;&#36890;&#24120;&#65292;&#30001;&#20110;&#21464;&#21387;&#22120;&#23398;&#20064;&#30340;&#8220;&#31243;&#24207;&#8221;&#26159;&#26410;&#30693;&#30340;&#65292;&#22240;&#27492;&#19981;&#28165;&#26970;&#35299;&#37322;&#26159;&#21542;&#25104;&#21151;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#29616;&#21644;&#26816;&#26597;&#21253;&#25324;&#35745;&#31639;&#20196;&#29260;&#39057;&#29575;&#12289;&#25490;&#24207;&#21644;&#25324;&#21495;&#26816;&#26597;&#22312;&#20869;&#30340;&#31243;&#24207;&#26469;&#28436;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;https://github.com/deepmind/tracr&#25552;&#20379;&#20102;Tracr&#30340;&#24320;&#28304;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show how to "compile" human-readable programs into standard decoder-only transformer models. Our compiler, Tracr, generates models with known structure. This structure can be used to design experiments. For example, we use it to study "superposition" in transformers that execute multi-step algorithms. Additionally, the known structure of Tracr-compiled models can serve as ground-truth for evaluating interpretability methods. Commonly, because the "programs" learned by transformers are unknown it is unclear whether an interpretation succeeded. We demonstrate our approach by implementing and examining programs including computing token frequencies, sorting, and parenthesis checking. We provide an open-source implementation of Tracr at https://github.com/deepmind/tracr.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102; QTO&#65288;&#26597;&#35810;&#35745;&#31639;&#26641;&#20248;&#21270;&#65289;&#20197;&#26377;&#25928;&#22320;&#22238;&#31572;&#22797;&#26434;&#36923;&#36753;&#26597;&#35810;&#65292;&#36890;&#36807;&#26597;&#35810;&#35745;&#31639;&#26641;&#19978;&#30340;&#27491;&#21453;&#21521;&#20256;&#25773;&#25214;&#21040;&#20102;&#31934;&#30830;&#30340;&#26368;&#20248;&#35299;&#65292;&#24182;&#21033;&#29992;&#20102;&#26597;&#35810;&#35745;&#31639;&#26641;&#20013;&#30340;&#29420;&#31435;&#24615;&#26469;&#20943;&#23569;&#25628;&#32034;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2212.09567</link><description>&lt;p&gt;
&#36890;&#36807;&#26597;&#35810;&#35745;&#31639;&#26641;&#20248;&#21270;&#22312;&#30693;&#35782;&#22270;&#35889;&#19978;&#22238;&#31572;&#22797;&#26434;&#36923;&#36753;&#26597;&#35810;
&lt;/p&gt;
&lt;p&gt;
Answering Complex Logical Queries on Knowledge Graphs via Query Computation Tree Optimization. (arXiv:2212.09567v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09567
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102; QTO&#65288;&#26597;&#35810;&#35745;&#31639;&#26641;&#20248;&#21270;&#65289;&#20197;&#26377;&#25928;&#22320;&#22238;&#31572;&#22797;&#26434;&#36923;&#36753;&#26597;&#35810;&#65292;&#36890;&#36807;&#26597;&#35810;&#35745;&#31639;&#26641;&#19978;&#30340;&#27491;&#21453;&#21521;&#20256;&#25773;&#25214;&#21040;&#20102;&#31934;&#30830;&#30340;&#26368;&#20248;&#35299;&#65292;&#24182;&#21033;&#29992;&#20102;&#26597;&#35810;&#35745;&#31639;&#26641;&#20013;&#30340;&#29420;&#31435;&#24615;&#26469;&#20943;&#23569;&#25628;&#32034;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#23436;&#25972;&#30340;&#30693;&#35782;&#22270;&#35889;&#19978;&#22238;&#31572;&#22797;&#26434;&#36923;&#36753;&#26597;&#35810;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#24182;&#24050;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#12290;&#23884;&#20837;&#24335;&#26041;&#27861;&#38656;&#35201;&#35757;&#32451;&#22797;&#26434;&#26597;&#35810;&#65292;&#24182;&#19988;&#19981;&#33021;&#24456;&#22909;&#22320;&#27867;&#21270;&#21040;&#20998;&#24067;&#26597;&#35810;&#32467;&#26500;&#20043;&#22806;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#23558;&#27492;&#20219;&#21153;&#20316;&#20026;&#31471;&#21040;&#31471;&#20248;&#21270;&#38382;&#39064;&#36827;&#34892;&#20102;&#26694;&#26550;&#21270;&#65292;&#21482;&#38656;&#35201;&#39044;&#35757;&#32451;&#30340;&#38142;&#25509;&#39044;&#27979;&#22120;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25351;&#25968;&#32423;&#30340;&#32452;&#21512;&#25628;&#32034;&#31354;&#38388;&#65292;&#26368;&#20248;&#35299;&#21482;&#33021;&#34987;&#36817;&#20284;&#65292;&#38480;&#21046;&#20102;&#26368;&#32456;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; QTO&#65288;&#26597;&#35810;&#35745;&#31639;&#26641;&#20248;&#21270;&#65289;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25214;&#21040;&#31934;&#30830;&#30340;&#26368;&#20248;&#35299;&#12290;QTO&#36890;&#36807;&#22312;&#26641;&#29366;&#35745;&#31639;&#22270;&#65288;&#21363;&#26597;&#35810;&#35745;&#31639;&#26641;&#65289;&#19978;&#36827;&#34892;&#27491;&#21453;&#21521;&#20256;&#25773;&#26469;&#25214;&#21040;&#26368;&#20248;&#35299;&#12290;&#29305;&#21035;&#22320;&#65292;QTO&#21033;&#29992;&#20102;&#26597;&#35810;&#35745;&#31639;&#26641;&#20013;&#32534;&#30721;&#30340;&#29420;&#31435;&#24615;&#26469;&#20943;&#23569;&#25628;&#32034;&#31354;&#38388;&#65292;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#20165;&#28041;&#21450;&#23616;&#37096;&#35745;&#31639;&#12290;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;QTO&#33719;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Answering complex logical queries on incomplete knowledge graphs is a challenging task, and has been widely studied. Embedding-based methods require training on complex queries, and cannot generalize well to out-of-distribution query structures. Recent work frames this task as an end-to-end optimization problem, and it only requires a pretrained link predictor. However, due to the exponentially large combinatorial search space, the optimal solution can only be approximated, limiting the final accuracy. In this work, we propose QTO (Query Computation Tree Optimization) that can efficiently find the exact optimal solution. QTO finds the optimal solution by a forward-backward propagation on the tree-like computation graph, i.e., query computation tree. In particular, QTO utilizes the independence encoded in the query computation tree to reduce the search space, where only local computations are involved during the optimization procedure. Experiments on 3 datasets show that QTO obtains sta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37197;&#23545;&#20114;&#34917;&#26102;&#38388;&#24490;&#29615;&#19968;&#33268;&#23545;&#25239;&#32593;&#32476;&#30340;&#38647;&#36798;&#38477;&#27700;&#39044;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#29983;&#25104;&#22120;&#32593;&#32476;&#21644;&#24490;&#29615;&#19968;&#33268;&#24615;&#25439;&#22833;&#21644;&#23545;&#25239;&#24615;&#25439;&#22833;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#25512;&#24191;&#33021;&#21147;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#25216;&#26415;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2211.15046</link><description>&lt;p&gt;
&#22522;&#20110;&#37197;&#23545;&#20114;&#34917;&#26102;&#38388;&#24490;&#29615;&#19968;&#33268;&#23545;&#25239;&#32593;&#32476;&#30340;&#38647;&#36798;&#38477;&#27700;&#39044;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
PCT-CycleGAN: Paired Complementary Temporal Cycle-Consistent Adversarial Networks for Radar-Based Precipitation Nowcasting. (arXiv:2211.15046v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15046
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37197;&#23545;&#20114;&#34917;&#26102;&#38388;&#24490;&#29615;&#19968;&#33268;&#23545;&#25239;&#32593;&#32476;&#30340;&#38647;&#36798;&#38477;&#27700;&#39044;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#29983;&#25104;&#22120;&#32593;&#32476;&#21644;&#24490;&#29615;&#19968;&#33268;&#24615;&#25439;&#22833;&#21644;&#23545;&#25239;&#24615;&#25439;&#22833;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#25512;&#24191;&#33021;&#21147;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#25216;&#26415;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38477;&#27700;&#39044;&#27979;&#26041;&#27861;&#24050;&#32463;&#22312;&#36807;&#21435;&#20960;&#20010;&#19990;&#32426;&#37324;&#24471;&#21040;&#20102;&#24456;&#22909;&#30340;&#21457;&#23637;&#65292;&#22240;&#20026;&#38632;&#27700;&#23545;&#20154;&#31867;&#29983;&#27963;&#26377;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#24433;&#21709;&#12290;&#29616;&#26377;&#30340;&#38477;&#27700;&#39044;&#27979;&#27169;&#22411;&#21253;&#25324;&#23450;&#37327;&#38477;&#27700;&#39044;&#27979; (QPF) &#27169;&#22411;&#12289;&#21367;&#31215;&#38271;&#30701;&#26399;&#35760;&#24518; (ConvLSTM) &#27169;&#22411;&#20197;&#21450;&#26368;&#26032;&#30340; MetNet-2 &#31561;&#22810;&#31181;&#22797;&#26434;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#37197;&#23545;&#20114;&#34917;&#26102;&#38388;&#24490;&#29615;&#19968;&#33268;&#23545;&#25239;&#32593;&#32476; (PCT-CycleGAN) &#30340;&#38647;&#36798;&#38477;&#27700;&#39044;&#27979;&#26041;&#27861;&#65292;&#21463;&#23545;&#25239;&#29983;&#25104;&#32593;&#32476; (CycleGAN) &#24378;&#22823;&#30340;&#22270;&#20687;&#36716;&#25442;&#24615;&#33021;&#21551;&#21457;&#12290;PCT-CycleGAN &#20351;&#29992;&#20004;&#20010;&#20855;&#26377;&#21521;&#21069;&#21644;&#21521;&#21518;&#26102;&#38388;&#21160;&#24577;&#30340;&#29983;&#25104;&#22120;&#32593;&#32476;&#29983;&#25104;&#26102;&#24207;&#24615;&#65292;&#27599;&#20010;&#29983;&#25104;&#22120;&#32593;&#32476;&#23398;&#20064;&#19968;&#20010;&#24222;&#22823;&#30340;&#19968;&#23545;&#19968;&#26144;&#23556;&#65292;&#20197;&#36924;&#36817;&#34920;&#31034;&#27599;&#20010;&#26041;&#21521;&#19978;&#30340;&#26102;&#38388;&#21160;&#24577;&#30340;&#26144;&#23556;&#20989;&#25968;&#12290;&#20026;&#20102;&#21019;&#24314;&#37197;&#23545;&#20114;&#34917;&#24490;&#29615;&#20043;&#38388;&#30340;&#24378;&#20581;&#26102;&#38388;&#22240;&#26524;&#20851;&#31995;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;&#24490;&#29615;&#19968;&#33268;&#24615;&#25439;&#22833;&#21644;&#23545;&#25239;&#24615;&#25439;&#22833;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;PCT-CycleGAN &#22312;&#20934;&#30830;&#24615;&#21644;&#25512;&#24191;&#33021;&#21147;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#25216;&#26415;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The precipitation nowcasting methods have been elaborated over the centuries because rain has a crucial impact on human life. Not only quantitative precipitation forecast (QPF) models and convolutional long short-term memory (ConvLSTM), but also various sophisticated methods such as the latest MetNet-2 are emerging. In this paper, we propose a paired complementary temporal cycle-consistent adversarial networks (PCT-CycleGAN) for radar-based precipitation nowcasting, inspired by cycle-consistent adversarial networks (CycleGAN), which shows strong performance in image-to-image translation. PCT-CycleGAN generates temporal causality using two generator networks with forward and backward temporal dynamics in paired complementary cycles. Each generator network learns a huge number of one-to-one mappings about time-dependent radar-based precipitation data to approximate a mapping function representing the temporal dynamics in each direction. To create robust temporal causality between paired 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31579;&#36873;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#22810;&#37325;&#25512;&#24191;&#27493;&#39588;&#26469;&#23398;&#20064;&#19982;&#35748;&#30693;&#22270;&#19968;&#33268;&#30340;&#21512;&#29702;&#35268;&#21017;&#65292;&#33021;&#22815;&#23398;&#20064;&#26356;&#24191;&#27867;&#30340;&#21512;&#29702;&#35268;&#21017;&#20197;&#21453;&#26144;&#35748;&#30693;&#22270;&#20013;&#30340;&#21512;&#29702;&#24615;&#32771;&#34385;&#65292;&#23454;&#39564;&#21457;&#29616;&#20854;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2211.02918</link><description>&lt;p&gt;
&#22522;&#20110;&#31579;&#36873;&#30340;&#19968;&#33324;&#26041;&#27861;&#26469;&#23398;&#20064;&#35748;&#30693;&#22270;&#30340;&#21512;&#29702;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
A Filtering-based General Approach to Learning Rational Constraints of Epistemic Graphs. (arXiv:2211.02918v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.02918
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31579;&#36873;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#22810;&#37325;&#25512;&#24191;&#27493;&#39588;&#26469;&#23398;&#20064;&#19982;&#35748;&#30693;&#22270;&#19968;&#33268;&#30340;&#21512;&#29702;&#35268;&#21017;&#65292;&#33021;&#22815;&#23398;&#20064;&#26356;&#24191;&#27867;&#30340;&#21512;&#29702;&#35268;&#21017;&#20197;&#21453;&#26144;&#35748;&#30693;&#22270;&#20013;&#30340;&#21512;&#29702;&#24615;&#32771;&#34385;&#65292;&#23454;&#39564;&#21457;&#29616;&#20854;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35748;&#30693;&#22270;&#26159;&#27010;&#29575;&#35770;&#35777;&#30340;&#35748;&#30693;&#26041;&#27861;&#30340;&#19968;&#31181;&#25512;&#24191;&#12290;Hunter&#25552;&#20986;&#20102;&#19968;&#20010;&#20108;&#27425;&#25512;&#24191;&#26694;&#26550;&#26469;&#20174;&#20247;&#21253;&#25968;&#25454;&#20013;&#23398;&#20064;&#35748;&#30693;&#32422;&#26463;&#12290;&#28982;&#32780;&#65292;&#23398;&#20064;&#21040;&#30340;&#35748;&#30693;&#32422;&#26463;&#21482;&#21453;&#26144;&#20102;&#29992;&#25143;&#20174;&#25968;&#25454;&#20013;&#30340;&#20449;&#24565;&#65292;&#32780;&#27809;&#26377;&#32771;&#34385;&#21040;&#35748;&#30693;&#22270;&#20013;&#32534;&#30721;&#30340;&#21512;&#29702;&#24615;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#24403;&#21069;&#26694;&#26550;&#21482;&#33021;&#29983;&#25104;&#21453;&#26144;&#20195;&#29702;&#20154;&#20449;&#8203;&#8203;&#20219;&#31243;&#24230;&#32780;&#19981;&#26159;&#26159;&#21542;&#30456;&#20449;&#19968;&#20010;&#35770;&#28857;&#30340;&#35748;&#30693;&#32422;&#26463;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31579;&#36873;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#22810;&#37325;&#25512;&#24191;&#27493;&#39588;&#20174;&#25968;&#25454;&#38598;&#20013;&#29983;&#25104;&#19968;&#32452;&#19982;&#20854;&#35748;&#30693;&#22270;&#19968;&#33268;&#30340;&#21512;&#29702;&#35268;&#21017;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#23398;&#20064;&#26356;&#24191;&#27867;&#30340;&#21512;&#29702;&#35268;&#21017;&#65292;&#20197;&#21453;&#26144;&#35748;&#30693;&#22270;&#20013;&#30340;&#21512;&#29702;&#24615;&#32771;&#34385;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23398;&#20064;&#21512;&#29702;&#35268;&#21017;&#30340;&#31934;&#24230;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Epistemic graphs are a generalization of the epistemic approach to probabilistic argumentation. Hunter proposed a 2-way generalization framework to learn epistemic constraints from crowd-sourcing data. However, the learnt epistemic constraints only reflect users' beliefs from data, without considering the rationality encoded in epistemic graphs. Meanwhile, the current framework can only generate epistemic constraints that reflect whether an agent believes an argument, but not the degree to which it believes in it. The major challenge to achieving this effect is that the computational complexity will increase sharply when expanding the variety of constraints, which may lead to unacceptable time performance. To address these problems, we propose a filtering-based approach using a multiple-way generalization step to generate a set of rational rules which are consistent with their epistemic graphs from a dataset. This approach is able to learn a wider variety of rational rules that reflect
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#24182;&#22312;&#22810;&#31181;&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;&#21313;&#31181;&#35299;&#37322;&#22120;&#30340;&#34920;&#29616;&#65292;&#25552;&#20379;&#20102;&#19981;&#21516;GNN&#20307;&#31995;&#32467;&#26500;&#26131;&#35299;&#37322;&#24615;&#30340;&#20851;&#38190;&#27934;&#23519;&#12290;</title><link>http://arxiv.org/abs/2210.15304</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#35299;&#37322;&#26041;&#27861;&#65306;&#19968;&#39033;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Explaining the Explainers in Graph Neural Networks: a Comparative Study. (arXiv:2210.15304v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.15304
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#24182;&#22312;&#22810;&#31181;&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;&#21313;&#31181;&#35299;&#37322;&#22120;&#30340;&#34920;&#29616;&#65292;&#25552;&#20379;&#20102;&#19981;&#21516;GNN&#20307;&#31995;&#32467;&#26500;&#26131;&#35299;&#37322;&#24615;&#30340;&#20851;&#38190;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24555;&#36895;&#21457;&#23637;&#21518;&#65292;GNN&#24050;&#32463;&#22312;&#35768;&#22810;&#31185;&#23398;&#21644;&#24037;&#31243;&#39046;&#22495;&#24212;&#29992;&#24191;&#27867;&#65292;&#36825;&#20419;&#20351;&#38656;&#35201;&#26041;&#27861;&#26469;&#29702;&#35299;&#23427;&#20204;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#26368;&#36817;&#20960;&#24180;&#65292;GNN&#35299;&#37322;&#22120;&#24320;&#22987;&#20986;&#29616;&#65292;&#26377;&#22810;&#31181;&#26041;&#27861;&#65292;&#19968;&#20123;&#26159;&#26032;&#39062;&#30340;&#65292;&#19968;&#20123;&#26159;&#20174;&#20854;&#20182;&#39046;&#22495;&#25913;&#32534;&#32780;&#26469;&#30340;&#12290;&#20026;&#20102;&#25972;&#29702;&#36825;&#31181;&#28023;&#37327;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#19968;&#20123;&#30740;&#31350;&#22312;&#21508;&#31181;&#21487;&#35299;&#37322;&#24615;&#25351;&#26631;&#26041;&#38754;&#23545;&#19981;&#21516;&#30340;&#35299;&#37322;&#22120;&#24615;&#33021;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26089;&#26399;&#30340;&#24037;&#20316;&#27809;&#26377;&#23581;&#35797;&#25552;&#20379;&#20851;&#20110;&#19981;&#21516;&#30340;GNN&#20307;&#31995;&#32467;&#26500;&#26356;&#25110;&#19981;&#26131;&#35299;&#37322;&#30340;&#27934;&#23519;&#65292;&#20063;&#27809;&#26377;&#35828;&#26126;&#22312;&#32473;&#23450;&#29615;&#22659;&#20013;&#24212;&#35813;&#36873;&#25321;&#21738;&#31181;&#35299;&#37322;&#22120;&#12290;&#22312;&#26412;&#27425;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#35774;&#35745;&#31995;&#32479;&#24615;&#23454;&#39564;&#30740;&#31350;&#65292;&#23545;&#20843;&#20010;&#20195;&#34920;&#24615;&#20307;&#31995;&#32467;&#26500;&#19978;&#35757;&#32451;&#30340;&#21313;&#31181;&#35299;&#37322;&#22120;&#22312;&#20845;&#20010;&#31934;&#24515;&#35774;&#35745;&#30340;&#22270;&#21644;&#33410;&#28857;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#22635;&#34917;&#20102;&#36825;&#20123;&#31354;&#30333;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#38190;&#30340;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Following a fast initial breakthrough in graph based learning, Graph Neural Networks (GNNs) have reached a widespread application in many science and engineering fields, prompting the need for methods to understand their decision process.  GNN explainers have started to emerge in recent years, with a multitude of methods both novel or adapted from other domains. To sort out this plethora of alternative approaches, several studies have benchmarked the performance of different explainers in terms of various explainability metrics. However, these earlier works make no attempts at providing insights into why different GNN architectures are more or less explainable, or which explainer should be preferred in a given setting.  In this survey, we fill these gaps by devising a systematic experimental study, which tests ten explainers on eight representative architectures trained on six carefully designed graph and node classification datasets. With our results we provide key insights on the cho
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#19968;&#31995;&#21015;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#26469;&#35782;&#21035;&#20020;&#24202;&#35760;&#24405;&#20013;&#30340;&#30284;&#30151;&#65292;&#37319;&#29992;&#29942;&#39048;&#36866;&#37197;&#22120;&#21644;&#25552;&#31034;&#24494;&#35843;&#30340;&#26041;&#27861;&#20248;&#20110;&#20854;&#23427;&#26041;&#27861;&#65292;&#21487;&#22312;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2210.09440</link><description>&lt;p&gt;
&#21033;&#29992;&#29942;&#39048;&#36866;&#37197;&#22120;&#22312;&#20302;&#36164;&#28304;&#38480;&#21046;&#19979;&#35782;&#21035;&#20020;&#24202;&#35760;&#24405;&#20013;&#30340;&#30284;&#30151;
&lt;/p&gt;
&lt;p&gt;
Using Bottleneck Adapters to Identify Cancer in Clinical Notes under Low-Resource Constraints. (arXiv:2210.09440v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.09440
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#19968;&#31995;&#21015;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#26469;&#35782;&#21035;&#20020;&#24202;&#35760;&#24405;&#20013;&#30340;&#30284;&#30151;&#65292;&#37319;&#29992;&#29942;&#39048;&#36866;&#37197;&#22120;&#21644;&#25552;&#31034;&#24494;&#35843;&#30340;&#26041;&#27861;&#20248;&#20110;&#20854;&#23427;&#26041;&#27861;&#65292;&#21487;&#22312;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22788;&#29702;&#23384;&#20648;&#22312;&#20020;&#24202;&#20581;&#24247;&#35760;&#24405;&#20013;&#30340;&#20449;&#24687;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#26159;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#19968;&#20010;&#27963;&#36291;&#30740;&#31350;&#39046;&#22495;&#12290;&#26412;&#25991;&#22312;&#19968;&#20010;&#21547;&#26377;&#20020;&#24202;&#35760;&#24405;&#30340;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#19968;&#31995;&#21015;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#20174;&#31616;&#21333;&#30340;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#21040;&#19987;&#19994;&#30340;&#36716;&#25442;&#22120;&#65292;&#20363;&#22914; BioBERT&#65292;&#24182;&#38468;&#26377;&#25351;&#31034;&#26679;&#26412;&#26159;&#21542;&#19982;&#30284;&#30151;&#30456;&#20851;&#30340;&#19968;&#32452;&#27880;&#37322;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#29305;&#21035;&#37319;&#29992;&#20102;&#26469;&#33258;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#65292;&#21363;&#29942;&#39048;&#36866;&#37197;&#22120;&#21644;&#25552;&#31034;&#35843;&#25972;&#65292;&#20197;&#36866;&#24212;&#25105;&#20204;&#30340;&#19987;&#19994;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#20110;&#33258;&#28982;&#35821;&#35328;&#30340;&#20923;&#32467;&#30340;&#22522;&#20110;BERT&#30340;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#29942;&#39048;&#36866;&#37197;&#22120;&#24494;&#35843;&#65292;&#20248;&#20110;&#25152;&#26377;&#20854;&#20182;&#31574;&#30053;&#65292;&#21253;&#25324;&#20840;&#38754;&#24494;&#35843;&#19987;&#29992;&#30340;BioBERT&#27169;&#22411;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#25105;&#20204;&#24314;&#35758;&#22312;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#20351;&#29992;&#29942;&#39048;&#36866;&#37197;&#22120;&#65292;&#29305;&#21035;&#26159;&#22312;&#26377;&#38480;&#30340;&#26631;&#35760;&#25968;&#25454;&#25110;&#22788;&#29702;&#33021;&#21147;&#26102;&#65292;&#21487;&#33021;&#26159;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#25366;&#25496;&#30340;&#21487;&#34892;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Processing information locked within clinical health records is a challenging task that remains an active area of research in biomedical NLP. In this work, we evaluate a broad set of machine learning techniques ranging from simple RNNs to specialised transformers such as BioBERT on a dataset containing clinical notes along with a set of annotations indicating whether a sample is cancer-related or not.  Furthermore, we specifically employ efficient fine-tuning methods from NLP, namely, bottleneck adapters and prompt tuning, to adapt the models to our specialised task. Our evaluations suggest that fine-tuning a frozen BERT model pre-trained on natural language and with bottleneck adapters outperforms all other strategies, including full fine-tuning of the specialised BioBERT model. Based on our findings, we suggest that using bottleneck adapters in low-resource situations with limited access to labelled data or processing capacity could be a viable strategy in biomedical text mining. The
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#26032;&#30340;&#23398;&#20064;&#20219;&#21153;&#65306;&#27169;&#22411;&#38142;&#25509;&#65292;&#26088;&#22312;&#36890;&#36807;&#23398;&#20064;&#19981;&#21516;&#40657;&#30418;&#27169;&#22411;&#36755;&#20986;&#31354;&#38388;&#20043;&#38388;&#26144;&#23556;&#30340;&#27169;&#22411;&#38142;&#25509;&#65292;&#25226;&#23427;&#20204;&#36830;&#25509;&#36215;&#26469;&#12290;&#25552;&#20986;&#20102;&#25903;&#25345;&#38142;&#25509;&#19981;&#21516;&#40657;&#30418;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35774;&#35745;&#65292;&#35299;&#20915;&#20102;&#20998;&#24067;&#24046;&#24322;&#25361;&#25112;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#35843;&#24230;&#31639;&#27861;&#65292;&#24182;&#22312;&#22810;&#20010;&#23454;&#39564;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#25512;&#29702;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2209.13883</link><description>&lt;p&gt;
MLink&#65306;&#22810;&#20010;&#39046;&#22495;&#30340;&#40657;&#30418;&#27169;&#22411;&#38142;&#25509;&#23454;&#29616;&#21327;&#21516;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
MLink: Linking Black-Box Models from Multiple Domains for Collaborative Inference. (arXiv:2209.13883v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.13883
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26032;&#30340;&#23398;&#20064;&#20219;&#21153;&#65306;&#27169;&#22411;&#38142;&#25509;&#65292;&#26088;&#22312;&#36890;&#36807;&#23398;&#20064;&#19981;&#21516;&#40657;&#30418;&#27169;&#22411;&#36755;&#20986;&#31354;&#38388;&#20043;&#38388;&#26144;&#23556;&#30340;&#27169;&#22411;&#38142;&#25509;&#65292;&#25226;&#23427;&#20204;&#36830;&#25509;&#36215;&#26469;&#12290;&#25552;&#20986;&#20102;&#25903;&#25345;&#38142;&#25509;&#19981;&#21516;&#40657;&#30418;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35774;&#35745;&#65292;&#35299;&#20915;&#20102;&#20998;&#24067;&#24046;&#24322;&#25361;&#25112;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#35843;&#24230;&#31639;&#27861;&#65292;&#24182;&#22312;&#22810;&#20010;&#23454;&#39564;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#25512;&#29702;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#27169;&#22411;&#25512;&#29702;&#30340;&#25104;&#26412;&#25928;&#30410;&#23545;&#20110;&#26102;&#24310;&#25935;&#24863;&#30340;&#20219;&#21153;&#21644;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#33267;&#20851;&#37325;&#35201;&#12290;&#19968;&#20010;&#20856;&#22411;&#30340;&#22256;&#22659;&#26159;&#65306;&#20026;&#20102;&#25552;&#20379;&#22797;&#26434;&#30340;&#26234;&#33021;&#26381;&#21153;&#65288;&#22914;&#26234;&#33021;&#22478;&#24066;&#65289;&#65292;&#25105;&#20204;&#38656;&#35201;&#22810;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25512;&#29702;&#32467;&#26524;&#65292;&#20294;&#25104;&#26412;&#39044;&#31639;&#65288;&#22914;GPU&#20869;&#23384;&#65289;&#19981;&#36275;&#20197;&#36816;&#34892;&#25152;&#26377;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#40657;&#30418;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20043;&#38388;&#30340;&#22522;&#30784;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#20219;&#21153;&#65306;&#27169;&#22411;&#38142;&#25509;&#65292;&#26088;&#22312;&#36890;&#36807;&#23398;&#20064;&#23427;&#20204;&#36755;&#20986;&#31354;&#38388;&#20043;&#38388;&#30340;&#26144;&#23556;&#65288;&#31216;&#20026;&#27169;&#22411;&#38142;&#25509;&#65289;&#26469;&#36830;&#25509;&#19981;&#21516;&#40657;&#30418;&#27169;&#22411;&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#25903;&#25345;&#38142;&#25509;&#24322;&#26500;&#40657;&#30418;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#27169;&#22411;&#38142;&#25509;&#35774;&#35745;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#35299;&#20915;&#20998;&#24067;&#24046;&#24322;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#27169;&#22411;&#38142;&#25509;&#30340;&#36866;&#24212;&#21644;&#32858;&#21512;&#26041;&#27861;&#12290;&#22522;&#20110;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#38142;&#25509;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#35843;&#24230;&#31639;&#27861;&#65292;&#21517;&#20026;MLink&#12290;&#36890;&#36807;&#21551;&#29992;&#21327;&#20316;&#22810;&#27169;&#22411;&#25512;&#29702;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#22810;&#20010;&#23454;&#39564;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#25512;&#29702;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The cost efficiency of model inference is critical to real-world machine learning (ML) applications, especially for delay-sensitive tasks and resource-limited devices. A typical dilemma is: in order to provide complex intelligent services (e.g. smart city), we need inference results of multiple ML models, but the cost budget (e.g. GPU memory) is not enough to run all of them. In this work, we study underlying relationships among black-box ML models and propose a novel learning task: model linking, which aims to bridge the knowledge of different black-box models by learning mappings (dubbed model links) between their output spaces. We propose the design of model links which supports linking heterogeneous black-box ML models. Also, in order to address the distribution discrepancy challenge, we present adaptation and aggregation methods of model links. Based on our proposed model links, we developed a scheduling algorithm, named MLink. Through collaborative multi-model inference enabled b
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#21487;&#23398;&#20064;&#30340;&#36755;&#20837;&#36807;&#28388;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#25512;&#29702;&#27169;&#22411;&#21644;&#36755;&#20837;&#36807;&#28388;&#22120;&#30340;&#20551;&#35774;&#22797;&#26434;&#24615;&#36827;&#34892;&#29702;&#35770;&#27604;&#36739;&#65292;&#20174;&#32780;&#20102;&#35299;&#20248;&#21270;&#28508;&#21147;&#12290;&#35813;&#26694;&#26550;&#20943;&#23569;&#20887;&#20313;&#65292;&#38477;&#20302;&#25512;&#29702;&#25104;&#26412;&#65292;&#24182;&#22312;f&#20540;&#12289;&#25512;&#29702;&#36895;&#24230;&#21644;&#20869;&#23384;&#21344;&#29992;&#26041;&#38754;&#36229;&#36234;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2209.13873</link><description>&lt;p&gt;
InFi&#65306;&#31227;&#21160;&#31471;&#25512;&#29702;&#30340;&#36164;&#28304;&#39640;&#25928;&#24615;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#31471;&#21040;&#31471;&#36755;&#20837;&#36807;&#28388;
&lt;/p&gt;
&lt;p&gt;
InFi: End-to-End Learning to Filter Input for Resource-Efficiency in Mobile-Centric Inference. (arXiv:2209.13873v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.13873
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#21487;&#23398;&#20064;&#30340;&#36755;&#20837;&#36807;&#28388;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#25512;&#29702;&#27169;&#22411;&#21644;&#36755;&#20837;&#36807;&#28388;&#22120;&#30340;&#20551;&#35774;&#22797;&#26434;&#24615;&#36827;&#34892;&#29702;&#35770;&#27604;&#36739;&#65292;&#20174;&#32780;&#20102;&#35299;&#20248;&#21270;&#28508;&#21147;&#12290;&#35813;&#26694;&#26550;&#20943;&#23569;&#20887;&#20313;&#65292;&#38477;&#20302;&#25512;&#29702;&#25104;&#26412;&#65292;&#24182;&#22312;f&#20540;&#12289;&#25512;&#29702;&#36895;&#24230;&#21644;&#20869;&#23384;&#21344;&#29992;&#26041;&#38754;&#36229;&#36234;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31227;&#21160;&#31471;AI&#24212;&#29992;&#23545;&#27169;&#22411;&#25512;&#29702;&#30340;&#36164;&#28304;&#39640;&#25928;&#24615;&#26377;&#24456;&#39640;&#30340;&#35201;&#27714;&#12290;&#36755;&#20837;&#36807;&#28388;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#28040;&#38500;&#20887;&#20313;&#65292;&#20174;&#32780;&#38477;&#20302;&#25512;&#29702;&#25104;&#26412;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#24050;&#32463;&#20026;&#35768;&#22810;&#24212;&#29992;&#31243;&#24207;&#37327;&#36523;&#23450;&#21046;&#20102;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#30041;&#19979;&#20102;&#20004;&#20010;&#22522;&#26412;&#38382;&#39064;&#26410;&#35299;&#31572;&#65306;&#65288;1&#65289;&#25512;&#29702;&#24037;&#20316;&#37327;&#30340;&#29702;&#35770;&#21487;&#36807;&#28388;&#24615;&#65292;&#20197;&#25351;&#23548;&#36755;&#20837;&#36807;&#28388;&#25216;&#26415;&#30340;&#24212;&#29992;&#65292;&#20174;&#32780;&#36991;&#20813;&#36164;&#28304;&#21463;&#38480;&#30340;&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#30340;&#35797;&#38169;&#25104;&#26412;&#65307;&#65288;2&#65289;&#29305;&#24449;&#23884;&#20837;&#30340;&#40065;&#26834;&#24615;&#21306;&#20998;&#24230;&#65292;&#20197;&#20351;&#36755;&#20837;&#36807;&#28388;&#23545;&#22810;&#26679;&#21270;&#25512;&#29702;&#20219;&#21153;&#21644;&#36755;&#20837;&#20869;&#23481;&#26222;&#36941;&#26377;&#25928;&#12290;&#20026;&#20102;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#24418;&#24335;&#21270;&#36755;&#20837;&#36807;&#28388;&#38382;&#39064;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#27604;&#36739;&#25512;&#29702;&#27169;&#22411;&#21644;&#36755;&#20837;&#36807;&#28388;&#22120;&#30340;&#20551;&#35774;&#22797;&#26434;&#24615;&#65292;&#20197;&#20102;&#35299;&#20248;&#21270;&#28508;&#21147;&#12290;&#28982;&#21518;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#31471;&#21040;&#31471;&#21487;&#23398;&#20064;&#30340;&#36755;&#20837;&#36807;&#28388;&#26694;&#26550;&#65292;&#28085;&#30422;&#20102;&#22823;&#22810;&#25968;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;f&#20540;&#12289;&#25512;&#29702;&#36895;&#24230;&#21644;&#20869;&#23384;&#21344;&#29992;&#26041;&#38754;&#36229;&#36234;&#20102;&#23427;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mobile-centric AI applications have high requirements for resource-efficiency of model inference. Input filtering is a promising approach to eliminate the redundancy so as to reduce the cost of inference. Previous efforts have tailored effective solutions for many applications, but left two essential questions unanswered: (1) theoretical filterability of an inference workload to guide the application of input filtering techniques, thereby avoiding the trial-and-error cost for resource-constrained mobile applications; (2) robust discriminability of feature embedding to allow input filtering to be widely effective for diverse inference tasks and input content. To answer them, we first formalize the input filtering problem and theoretically compare the hypothesis complexity of inference models and input filters to understand the optimization potential. Then we propose the first end-to-end learnable input filtering framework that covers most state-of-the-art methods and surpasses them in f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#30450;&#30446;&#26816;&#27979;&#36755;&#20837;&#38899;&#39057;&#30340;&#30495;&#23454;&#24615;&#65292;&#21487;&#20197;&#26377;&#25928;&#24212;&#23545;&#38899;&#39057;&#27450;&#35784;&#38382;&#39064;&#30340;&#20998;&#31867;&#22120;&#12290;&#32780;&#36825;&#31181;&#20998;&#31867;&#22120;&#19981;&#38656;&#35201;&#20219;&#20309;&#21442;&#32771;&#65292;&#33021;&#22815;&#22312;&#27809;&#26377;&#30495;&#23454;&#26469;&#28304;&#30340;&#24773;&#20917;&#19979;&#26816;&#27979;&#20986;&#27169;&#20223;&#38899;&#39057;&#12290;</title><link>http://arxiv.org/abs/2209.12573</link><description>&lt;p&gt;
&#25968;&#23383;&#38899;&#39057;&#21462;&#35777;&#65306;&#30450;&#30446;&#26816;&#27979;&#20154;&#31867;&#35821;&#38899;&#27169;&#20223;
&lt;/p&gt;
&lt;p&gt;
Digital Audio Forensics: Blind Human Voice Mimicry Detection. (arXiv:2209.12573v4 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.12573
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#30450;&#30446;&#26816;&#27979;&#36755;&#20837;&#38899;&#39057;&#30340;&#30495;&#23454;&#24615;&#65292;&#21487;&#20197;&#26377;&#25928;&#24212;&#23545;&#38899;&#39057;&#27450;&#35784;&#38382;&#39064;&#30340;&#20998;&#31867;&#22120;&#12290;&#32780;&#36825;&#31181;&#20998;&#31867;&#22120;&#19981;&#38656;&#35201;&#20219;&#20309;&#21442;&#32771;&#65292;&#33021;&#22815;&#22312;&#27809;&#26377;&#30495;&#23454;&#26469;&#28304;&#30340;&#24773;&#20917;&#19979;&#26816;&#27979;&#20986;&#27169;&#20223;&#38899;&#39057;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#39057;&#26159;&#20154;&#31867;&#20132;&#27969;&#20013;&#20351;&#29992;&#26368;&#24191;&#27867;&#30340;&#26041;&#24335;&#20043;&#19968;&#65292;&#20294;&#21516;&#26102;&#20063;&#24456;&#23481;&#26131;&#34987;&#35823;&#29992;&#26469;&#27450;&#39575;&#20154;&#20204;&#12290;&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#30340;&#38761;&#21629;&#65292;&#30456;&#20851;&#25216;&#26415;&#29616;&#22312;&#23545;&#20960;&#20046;&#25152;&#26377;&#20154;&#37117;&#21487;&#29992;&#65292;&#36825;&#20351;&#24471;&#29359;&#32618;&#21644;&#20266;&#36896;&#21464;&#24471;&#26356;&#21152;&#31616;&#21333;&#12290;&#26412;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#20998;&#31867;&#22120;&#65292;&#21487;&#20197;&#30450;&#30446;&#20998;&#31867;&#36755;&#20837;&#38899;&#39057;&#20026;&#30495;&#23454;&#25110;&#32773;&#27169;&#20223;&#65307;&#8220;&#30450;&#30446;&#8221;&#25351;&#30340;&#26159;&#33021;&#22815;&#22312;&#27809;&#26377;&#21442;&#32771;&#25110;&#30495;&#23454;&#26469;&#28304;&#30340;&#24773;&#20917;&#19979;&#26816;&#27979;&#20223;&#21046;&#38899;&#39057;&#30340;&#33021;&#21147;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#26159;&#22312;&#19968;&#20010;&#22823;&#22411;&#38899;&#39057;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#30340;&#19968;&#32452;&#37325;&#35201;&#29305;&#24449;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#20197;&#24471;&#21040;&#19968;&#20010;&#20998;&#31867;&#22120;&#65292;&#35813;&#20998;&#31867;&#22120;&#34987;&#29992;&#20110;&#27979;&#35797;&#19981;&#21516;&#38899;&#39057;&#30340;&#30456;&#21516;&#29305;&#24449;&#38598;&#12290;&#25968;&#25454;&#25552;&#21462;&#33258;&#20004;&#20010;&#21407;&#22987;&#25968;&#25454;&#38598;&#65292;&#29305;&#21035;&#20026;&#36825;&#39033;&#24037;&#20316;&#32780;&#32534;&#20889;;&#19968;&#20010;&#20840;&#33521;&#25991;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#28151;&#21512;&#25968;&#25454;&#38598;&#65288;&#38463;&#25289;&#20271;&#35821;&#21152;&#33521;&#35821;&#65289;&#12290;&#36825;&#20123;&#25968;&#25454;&#38598;&#24050;&#36890;&#36807;GitHub&#20197;&#21407;&#22987;&#24418;&#24335;&#25552;&#20379;&#32473;&#30740;&#31350;&#31038;&#21306;&#65292;&#32593;&#22336;&#20026;https://github.com/SaSs7/Datas
&lt;/p&gt;
&lt;p&gt;
Audio is one of the most used ways of human communication, but at the same time it can be easily misused to trick people. With the revolution of AI, the related technologies are now accessible to almost everyone thus making it simple for the criminals to commit crimes and forgeries. In this work, we introduce a deep learning method to develop a classifier that will blindly classify an input audio as real or mimicked; the word 'blindly' refers to the ability to detect mimicked audio without references or real sources. The proposed model was trained on a set of important features extracted from a large dataset of audios to get a classifier that was tested on the same set of features from different audios. The data was extracted from two raw datasets, especially composed for this work; an all English dataset and a mixed dataset (Arabic plus English). These datasets have been made available, in raw form, through GitHub for the use of the research community at https://github.com/SaSs7/Datas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;Z-Code++&#65292;&#23427;&#20351;&#29992;&#20004;&#31181;&#39044;&#35757;&#32451;&#38454;&#27573;&#21644;&#19977;&#31181;&#25216;&#26415;&#36827;&#34892;&#20248;&#21270;&#65292;&#20854;&#20013;&#21253;&#25324;&#35299;&#32806;&#30340;&#27880;&#24847;&#21147;&#23618;&#21644;&#34701;&#21512;&#32534;&#30721;&#26041;&#27861;&#12290;&#35813;&#27169;&#22411;&#22312;&#25277;&#35937;&#25991;&#26412;&#25688;&#35201;&#20219;&#21153;&#19978;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#65292;&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#21442;&#25968;&#21270;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2208.09770</link><description>&lt;p&gt;
Z-Code++&#65306;&#19968;&#31181;&#38024;&#23545;&#25277;&#35937;&#25991;&#26412;&#25688;&#35201;&#20248;&#21270;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Z-Code++: A Pre-trained Language Model Optimized for Abstractive Summarization. (arXiv:2208.09770v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.09770
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;Z-Code++&#65292;&#23427;&#20351;&#29992;&#20004;&#31181;&#39044;&#35757;&#32451;&#38454;&#27573;&#21644;&#19977;&#31181;&#25216;&#26415;&#36827;&#34892;&#20248;&#21270;&#65292;&#20854;&#20013;&#21253;&#25324;&#35299;&#32806;&#30340;&#27880;&#24847;&#21147;&#23618;&#21644;&#34701;&#21512;&#32534;&#30721;&#26041;&#27861;&#12290;&#35813;&#27169;&#22411;&#22312;&#25277;&#35937;&#25991;&#26412;&#25688;&#35201;&#20219;&#21153;&#19978;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#65292;&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#21442;&#25968;&#21270;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Z-Code++&#65292;&#19968;&#31181;&#26032;&#30340;&#38024;&#23545;&#25277;&#35937;&#25991;&#26412;&#25688;&#35201;&#20248;&#21270;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#25193;&#23637;&#20102;&#26368;&#20808;&#36827;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#65292;&#36816;&#29992;&#20102;&#19977;&#31181;&#25216;&#26415;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#20004;&#38454;&#27573;&#30340;&#39044;&#35757;&#32451;&#36807;&#31243;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#22312;&#20302;&#36164;&#28304;&#25688;&#35201;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#35813;&#27169;&#22411;&#39318;&#20808;&#20351;&#29992;&#25991;&#26412;&#35821;&#26009;&#24211;&#36827;&#34892;&#35821;&#35328;&#29702;&#35299;&#30340;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#25688;&#35201;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#36830;&#32493;&#30340;&#39044;&#35757;&#32451;&#20197;&#25552;&#39640;&#20854;&#22522;&#20110;&#25991;&#26412;&#29983;&#25104;&#30340;&#33021;&#21147;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#29992;&#35299;&#32806;&#30340;&#27880;&#24847;&#21147;&#23618;&#21462;&#20195;&#32534;&#30721;&#22120;&#20013;&#30340;&#33258;&#27880;&#24847;&#21147;&#23618;&#65292;&#20854;&#20013;&#27599;&#20010;&#21333;&#35789;&#20998;&#21035;&#20351;&#29992;&#20004;&#20010;&#21521;&#37327;&#26469;&#34920;&#31034;&#20854;&#20869;&#23481;&#21644;&#20301;&#32622;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#20351;&#29992;&#32534;&#30721;&#22120;&#20013;&#30340;&#34701;&#21512;&#32534;&#30721;&#26041;&#27861;&#65292;&#20197;&#19968;&#31181;&#20998;&#23618;&#30340;&#26041;&#24335;&#23545;&#38271;&#24207;&#21015;&#36827;&#34892;&#32534;&#30721;&#12290;Z-Code++&#22312;5&#31181;&#35821;&#35328;&#30340;13&#20010;&#25991;&#26412;&#25688;&#35201;&#20219;&#21153;&#20013;&#26377;9&#20010;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#26368;&#20248;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#21442;&#25968;&#25928;&#29575;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#22312;XSum&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#36229;&#36807;&#20102;&#27604;&#20854;&#22823;600&#20493;&#30340;PaLM-540B&#65292;&#20197;&#21450;&#27604;&#20854;&#22823;200&#20493;&#30340;FeBERT&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents Z-Code++, a new pre-trained language model optimized for abstractive text summarization. The model extends the state of the art encoder-decoder model using three techniques. First, we use a two-phase pre-training process to improve model's performance on low-resource summarization tasks. The model is first pre-trained using text corpora for language understanding, and then is continually pre-trained on summarization corpora for grounded text generation. Second, we replace self-attention layers in the encoder with disentangled attention layers, where each word is represented using two vectors that encode its content and position, respectively. Third, we use fusion-in-encoder, a simple yet effective method of encoding long sequences in a hierarchical manner. Z-Code++ creates new state of the art on 9 out of 13 text summarization tasks across 5 languages. Our model is parameter-efficient in that it outperforms the 600x larger PaLM-540B on XSum, and the finetuned 200x l
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SSIVD-Net&#30340;&#26032;&#25216;&#26415;&#65292;&#29992;&#20110;&#26292;&#21147;&#35782;&#21035;&#20219;&#21153;&#12290;&#36890;&#36807;&#20351;&#29992;&#26174;&#33879;-&#36229;&#32423;&#22270;&#20687;&#34920;&#31034;&#20943;&#23569;&#20102;3D&#35270;&#39057;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#65292;&#25552;&#39640;&#20102;&#25512;&#26029;&#12289;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26550;&#26500;&#8220;Salient-Classifier&#8221;&#65292;&#23558;&#26680;&#26041;&#27861;&#21644;&#27531;&#24046;&#23398;&#20064;&#31574;&#30053;&#30456;&#32467;&#21512;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2207.12850</link><description>&lt;p&gt;
SSIVD-Net&#65306;&#19968;&#31181;&#26032;&#30340;&#27494;&#22120;&#21270;&#26292;&#21147;&#26174;&#33879;&#36229;&#32423;&#22270;&#20687;&#20998;&#31867;&#21644;&#26816;&#27979;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
SSIVD-Net: A Novel Salient Super Image Classification &amp; Detection Technique for Weaponized Violence. (arXiv:2207.12850v6 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.12850
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SSIVD-Net&#30340;&#26032;&#25216;&#26415;&#65292;&#29992;&#20110;&#26292;&#21147;&#35782;&#21035;&#20219;&#21153;&#12290;&#36890;&#36807;&#20351;&#29992;&#26174;&#33879;-&#36229;&#32423;&#22270;&#20687;&#34920;&#31034;&#20943;&#23569;&#20102;3D&#35270;&#39057;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#65292;&#25552;&#39640;&#20102;&#25512;&#26029;&#12289;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26550;&#26500;&#8220;Salient-Classifier&#8221;&#65292;&#23558;&#26680;&#26041;&#27861;&#21644;&#27531;&#24046;&#23398;&#20064;&#31574;&#30053;&#30456;&#32467;&#21512;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38381;&#36335;&#30005;&#35270;&#65288;CCTV&#65289;&#30417;&#25511;&#24405;&#20687;&#20013;&#26816;&#27979;&#26292;&#21147;&#21644;&#27494;&#22120;&#21270;&#26292;&#21147;&#38656;&#35201;&#19968;&#20010;&#20840;&#38754;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#8220;&#26234;&#24935;&#22478;&#24066;CCTV&#26292;&#21147;&#26816;&#27979;&#65288;SCVD&#65289;&#8221;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#20419;&#36827;&#23545;&#30417;&#25511;&#35270;&#39057;&#20013;&#27494;&#22120;&#20998;&#24067;&#30340;&#23398;&#20064;&#12290;&#20026;&#20102;&#35299;&#20915;&#20998;&#26512;3D&#30417;&#25511;&#35270;&#39057;&#36827;&#34892;&#26292;&#21147;&#35782;&#21035;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#25216;&#26415;&#65292;&#31216;&#20026;SSIVD-Net&#65288;&#29992;&#20110;&#26292;&#21147;&#26816;&#27979;&#30340;&#26174;&#33879;-&#36229;&#32423;-&#22270;&#20687;&#65289;&#65292;&#36890;&#36807;&#20351;&#29992;&#26174;&#33879;-&#36229;&#32423;&#22270;&#20687;&#34920;&#31034;&#20943;&#23569;3D&#35270;&#39057;&#25968;&#25454;&#22797;&#26434;&#24615;&#12289;&#38477;&#32500;&#21644;&#20449;&#24687;&#25439;&#22833;&#65292;&#21516;&#26102;&#25552;&#39640;&#25512;&#26029;&#12289;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#32771;&#34385;&#21040;&#26410;&#26469;&#26234;&#24935;&#22478;&#24066;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#21487;&#25345;&#32493;&#24615;&#35201;&#27714;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26550;&#26500;&#8220;Salient-Classifier&#8221;&#65292;&#23558;&#26680;&#26041;&#27861;&#21644;&#27531;&#24046;&#23398;&#20064;&#31574;&#30053;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;SSIVD-Net&#22312;SCVD&#12289;Hockey Fight&#12289;Moviescope&#20197;&#21450;Large-Scale Fight Detection&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#19982;&#29616;&#26377;&#31639;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detection of violence and weaponized violence in closed-circuit television (CCTV) footage requires a comprehensive approach. In this work, we introduce the \emph{Smart-City CCTV Violence Detection (SCVD)} dataset, specifically designed to facilitate the learning of weapon distribution in surveillance videos. To tackle the complexities of analyzing 3D surveillance video for violence recognition tasks, we propose a novel technique called, \emph{SSIVD-Net} (\textbf{S}alient-\textbf{S}uper-\textbf{I}mage for \textbf{V}iolence \textbf{D}etection). Our method reduces 3D video data complexity, dimensionality, and information loss while improving inference, performance, and explainability through the use of Salient-Super-Image representations. Considering the scalability and sustainability requirements of futuristic smart cities, the authors introduce the \emph{Salient-Classifier}, a novel architecture combining a kernelized approach with a residual learning strategy. We evaluate variations of
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#22411;&#30340;&#20107;&#20214;&#32423;&#35270;&#35273;&#38382;&#31572;&#26694;&#26550;&#8212;&#8212;&#36328;&#27169;&#24577;&#22240;&#26524;&#20851;&#31995;&#25512;&#29702;&#65288;CMCIR&#65289;&#65292;&#36890;&#36807;&#24341;&#20837;&#22240;&#26524;&#24178;&#39044;&#26041;&#27861;&#65292;&#21457;&#29616;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#24577;&#30340;&#30495;&#27491;&#22240;&#26524;&#32467;&#26500;&#65292;&#23454;&#29616;&#24378;&#20581;&#30340;&#22240;&#26524;&#24863;&#30693;&#35270;&#35273;&#35821;&#35328;&#38382;&#31572;&#12290;</title><link>http://arxiv.org/abs/2207.12647</link><description>&lt;p&gt;
&#36328;&#27169;&#24577;&#22240;&#26524;&#20851;&#31995;&#25512;&#29702;&#22312;&#20107;&#20214;&#32423;&#35270;&#35273;&#38382;&#31572;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Cross-Modal Causal Relational Reasoning for Event-Level Visual Question Answering. (arXiv:2207.12647v5 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.12647
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#22411;&#30340;&#20107;&#20214;&#32423;&#35270;&#35273;&#38382;&#31572;&#26694;&#26550;&#8212;&#8212;&#36328;&#27169;&#24577;&#22240;&#26524;&#20851;&#31995;&#25512;&#29702;&#65288;CMCIR&#65289;&#65292;&#36890;&#36807;&#24341;&#20837;&#22240;&#26524;&#24178;&#39044;&#26041;&#27861;&#65292;&#21457;&#29616;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#24577;&#30340;&#30495;&#27491;&#22240;&#26524;&#32467;&#26500;&#65292;&#23454;&#29616;&#24378;&#20581;&#30340;&#22240;&#26524;&#24863;&#30693;&#35270;&#35273;&#35821;&#35328;&#38382;&#31572;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#35270;&#35273;&#38382;&#31572;&#26041;&#27861;&#24448;&#24448;&#25429;&#25417;&#36328;&#27169;&#24577;&#30340;&#20266;&#30456;&#20851;&#24615;&#65292;&#32780;&#26410;&#33021;&#21457;&#29616;&#30495;&#27491;&#30340;&#22240;&#26524;&#26426;&#21046;&#65292;&#20197;&#30495;&#23454;&#22320;&#22522;&#20110;&#20027;&#23548;&#35270;&#35273;&#35777;&#25454;&#21644;&#38382;&#39064;&#24847;&#22270;&#36827;&#34892;&#25512;&#29702;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#24573;&#30053;&#20102;&#36328;&#27169;&#24577;&#20107;&#20214;&#32423;&#29702;&#35299;&#65292;&#38656;&#35201;&#32852;&#21512;&#24314;&#27169;&#20107;&#20214;&#30340;&#26102;&#38388;&#24615;&#12289;&#22240;&#26524;&#24615;&#21644;&#21160;&#24577;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#26032;&#30340;&#35282;&#24230;&#65292;&#21363;&#36328;&#27169;&#24577;&#22240;&#26524;&#20851;&#31995;&#25512;&#29702;&#65292;&#32858;&#28966;&#20110;&#20107;&#20214;&#32423;&#35270;&#35273;&#38382;&#31572;&#65292;&#24341;&#20837;&#22240;&#26524;&#24178;&#39044;&#26041;&#27861;&#26469;&#21457;&#29616;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#24577;&#30340;&#30495;&#27491;&#22240;&#26524;&#32467;&#26500;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#36328;&#27169;&#24577;&#22240;&#26524;&#20851;&#31995;&#25512;&#29702;&#65288;CMCIR&#65289;&#30340;&#26032;&#22411;&#20107;&#20214;&#32423;&#35270;&#35273;&#38382;&#31572;&#26694;&#26550;&#65292;&#20197;&#23454;&#29616;&#24378;&#20581;&#30340;&#22240;&#26524;&#24863;&#30693;&#35270;&#35273;&#35821;&#35328;&#38382;&#31572;&#12290;&#20026;&#20102;&#21457;&#29616;&#36328;&#27169;&#24577;&#22240;&#26524;&#32467;&#26500;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22240;&#26524;&#24863;&#30693;&#35270;&#35273;&#35821;&#35328;&#25512;&#29702;&#65288;CVLR&#65289;&#27169;&#22359;&#65292;&#29992;&#20110;&#20849;&#21516;&#23545;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#24577;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing visual question answering methods tend to capture the cross-modal spurious correlations and fail to discover the true causal mechanism that facilitates reasoning truthfully based on the dominant visual evidence and the question intention. Additionally, the existing methods usually ignore the cross-modal event-level understanding that requires to jointly model event temporality, causality, and dynamics. In this work, we focus on event-level visual question answering from a new perspective, i.e., cross-modal causal relational reasoning, by introducing causal intervention methods to discover the true causal structures for visual and linguistic modalities. Specifically, we propose a novel event-level visual question answering framework named Cross-Modal Causal RelatIonal Reasoning (CMCIR), to achieve robust causality-aware visual-linguistic question answering. To discover cross-modal causal structures, the Causality-aware Visual-Linguistic Reasoning (CVLR) module is proposed to co
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#19978;&#19979;&#25991;&#25935;&#24863;&#30340;&#21333;&#35789;&#23884;&#20837;&#26041;&#27861;&#29992;&#20110;&#33258;&#21160;&#26816;&#27979; troll &#25512;&#25991;&#65292;&#32467;&#26524;&#34920;&#26126;&#37319;&#29992;ELMo&#21644;BERT&#23884;&#20837;&#26041;&#27861;&#30340;&#24615;&#33021;&#26356;&#22909;&#65292;&#26368;&#20339;&#34920;&#29616;&#26041;&#27861;&#20026;&#22522;&#20110;ELMo&#30340;&#26550;&#26500;&#65292;&#37319;&#29992;&#20102;&#19968;&#20010;GRU&#20998;&#31867;&#22120;&#65292;&#20855;&#26377;0.929&#30340;AUC&#24471;&#20998;&#12290;</title><link>http://arxiv.org/abs/2207.08230</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#19978;&#19979;&#25991;&#25935;&#24863;&#30340;&#21333;&#35789;&#23884;&#20837;&#26041;&#27861;&#29992;&#20110;&#26816;&#27979;&#24694;&#24847;&#25512;&#25991;
&lt;/p&gt;
&lt;p&gt;
A Context-Sensitive Word Embedding Approach for The Detection of Troll Tweets. (arXiv:2207.08230v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.08230
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#19978;&#19979;&#25991;&#25935;&#24863;&#30340;&#21333;&#35789;&#23884;&#20837;&#26041;&#27861;&#29992;&#20110;&#33258;&#21160;&#26816;&#27979; troll &#25512;&#25991;&#65292;&#32467;&#26524;&#34920;&#26126;&#37319;&#29992;ELMo&#21644;BERT&#23884;&#20837;&#26041;&#27861;&#30340;&#24615;&#33021;&#26356;&#22909;&#65292;&#26368;&#20339;&#34920;&#29616;&#26041;&#27861;&#20026;&#22522;&#20110;ELMo&#30340;&#26550;&#26500;&#65292;&#37319;&#29992;&#20102;&#19968;&#20010;GRU&#20998;&#31867;&#22120;&#65292;&#20855;&#26377;0.929&#30340;AUC&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#24320;&#21457;&#21644;&#35780;&#20272;&#19968;&#32452;&#27169;&#22411;&#26550;&#26500;&#26469;&#33258;&#21160;&#26816;&#27979; troll &#25512;&#25991;&#65292;&#20174;&#32780;&#35299;&#20915;&#31038;&#20132;&#23186;&#20307;&#19978;&#28363;&#25200;&#34892;&#20026;&#26085;&#36235;&#20005;&#37325;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#21644;&#39044;&#35757;&#32451;&#30340;&#21333;&#35789;&#23884;&#20837;&#26041;&#27861;&#65292;&#22914;BERT&#12289;ELMo&#21644;GloVe&#65292;&#20351;&#29992;&#20998;&#31867;&#20934;&#30830;&#24230;&#12289;F1&#24471;&#20998;&#12289;AUC&#21644;&#31934;&#30830;&#24230;&#31561;&#25351;&#26631;&#35780;&#20272;&#20102;&#27599;&#20010;&#26550;&#26500;&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;BERT&#21644;ELMo&#23884;&#20837;&#26041;&#27861;&#34920;&#29616;&#20248;&#20110;GloVe&#26041;&#27861;&#65292;&#21487;&#33021;&#26159;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#25552;&#20379;&#26356;&#22909;&#22320;&#25429;&#25417;&#22312;&#32447;&#31038;&#20132;&#23186;&#20307;&#35821;&#35328;&#20351;&#29992;&#32454;&#24494;&#24046;&#21035;&#30340;&#19978;&#19979;&#25991;&#21270;&#21333;&#35789;&#23884;&#20837;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;CNN&#21644;GRU&#32534;&#30721;&#22120;&#22312;F1&#20998;&#25968;&#21644;AUC&#26041;&#38754;&#34920;&#29616;&#30456;&#20284;&#65292;&#34920;&#26126;&#23427;&#20204;&#22312;&#20174;&#36755;&#20837;&#25991;&#26412;&#20013;&#25552;&#21462;&#30456;&#20851;&#20449;&#24687;&#26041;&#38754;&#20855;&#26377;&#26377;&#25928;&#24615;&#12290;&#26368;&#20339;&#34920;&#29616;&#26041;&#27861;&#26159;&#22522;&#20110;ELMo&#30340;&#26550;&#26500;&#65292;&#37319;&#29992;&#20102;&#19968;&#20010;GRU&#20998;&#31867;&#22120;&#65292;&#20855;&#26377;0.929&#30340;AUC&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we aimed to address the growing concern of trolling behavior on social media by developing and evaluating a set of model architectures for the automatic detection of troll tweets. Utilizing deep learning techniques and pre-trained word embedding methods such as BERT, ELMo, and GloVe, we evaluated the performance of each architecture using metrics such as classification accuracy, F1 score, AUC, and precision. Our results indicate that BERT and ELMo embedding methods performed better than the GloVe method, likely due to their ability to provide contextualized word embeddings that better capture the nuances and subtleties of language use in online social media. Additionally, we found that CNN and GRU encoders performed similarly in terms of F1 score and AUC, suggesting their effectiveness in extracting relevant information from input text. The best-performing method was found to be an ELMo-based architecture that employed a GRU classifier, with an AUC score of 0.929. This r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#28508;&#22312;&#30446;&#26631;&#35299;&#30721;&#65288;LTD&#65289;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#20943;&#23569;&#39044;&#27979;&#29305;&#24449;&#25233;&#21046;&#65292;&#20174;&#32780;&#20026;&#23545;&#27604;&#22270;&#20687;-&#23383;&#24149;&#26816;&#32034;&#65288;ICR&#65289;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2204.13382</link><description>&lt;p&gt;
&#20943;&#23569;&#36164;&#28304;&#21463;&#38480;&#23545;&#27604;&#22270;&#20687;-&#23383;&#24149;&#26816;&#32034;&#20013;&#30340;&#39044;&#27979;&#29305;&#24449;&#25233;&#21046;
&lt;/p&gt;
&lt;p&gt;
Reducing Predictive Feature Suppression in Resource-Constrained Contrastive Image-Caption Retrieval. (arXiv:2204.13382v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.13382
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#28508;&#22312;&#30446;&#26631;&#35299;&#30721;&#65288;LTD&#65289;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#20943;&#23569;&#39044;&#27979;&#29305;&#24449;&#25233;&#21046;&#65292;&#20174;&#32780;&#20026;&#23545;&#27604;&#22270;&#20687;-&#23383;&#24149;&#26816;&#32034;&#65288;ICR&#65289;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#35757;&#32451;&#22270;&#20687;-&#23383;&#24149;&#26816;&#32034;&#65288;ICR&#65289;&#26041;&#27861;&#65292;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#26159;&#20248;&#21270;&#20989;&#25968;&#30340;&#24120;&#35265;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#23545;&#27604;ICR&#26041;&#27861;&#23481;&#26131;&#21463;&#21040;&#39044;&#27979;&#29305;&#24449;&#25233;&#21046;&#30340;&#24433;&#21709;&#12290;&#39044;&#27979;&#29305;&#24449;&#26159;&#27491;&#30830;&#25351;&#31034;&#26597;&#35810;&#21644;&#20505;&#36873;&#39033;&#20043;&#38388;&#30456;&#20284;&#24615;&#30340;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23384;&#22312;&#22810;&#20010;&#39044;&#27979;&#29305;&#24449;&#26102;&#65292;&#32534;&#30721;&#22120;&#27169;&#22411;&#24448;&#24448;&#20250;&#25233;&#21046;&#20887;&#20313;&#30340;&#39044;&#27979;&#29305;&#24449;&#65292;&#22240;&#20026;&#36825;&#20123;&#29305;&#24449;&#19981;&#38656;&#35201;&#23398;&#20064;&#21306;&#20998;&#27491;&#38754;&#21644;&#36127;&#38754;&#23545;&#12290;&#34429;&#28982;&#26377;&#20123;&#39044;&#27979;&#29305;&#24449;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26159;&#20887;&#20313;&#30340;&#65292;&#20294;&#22312;&#35780;&#20272;&#36807;&#31243;&#20013;&#21487;&#33021;&#26159;&#26377;&#29992;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20943;&#23569;&#36164;&#28304;&#21463;&#38480;ICR&#26041;&#27861;&#20013;&#39044;&#27979;&#29305;&#24449;&#25233;&#21046;&#30340;&#26041;&#27861;&#65306;&#28508;&#22312;&#30446;&#26631;&#35299;&#30721;&#65288;LTD&#65289;&#12290;&#25105;&#20204;&#22312;&#23545;&#27604;ICR&#26694;&#26550;&#20013;&#28155;&#21152;&#20102;&#19968;&#20010;&#39069;&#22806;&#30340;&#35299;&#30721;&#22120;&#65292;&#20197;&#22312;&#36890;&#29992;&#21477;&#23376;&#32534;&#30721;&#22120;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#37325;&#24314;&#36755;&#20837;&#23383;&#24149;&#65292;&#20174;&#32780;&#38450;&#27490;&#22270;&#20687;&#21644;&#23383;&#24149;&#32534;&#30721;&#22120;&#22312;&#19981;&#21305;&#37197;&#30340;&#36127;&#38754;&#23545;&#20013;&#25233;&#21046;&#39044;&#27979;&#29305;&#24449;&#12290;&#25105;&#20204;&#22312;Flikr30k&#21644;MS COCO&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;LTD&#65292;&#24182;&#34920;&#26126;&#23427;&#27604;&#36164;&#28304;&#21463;&#38480;&#22330;&#26223;&#20013;&#30340;&#22522;&#32447;&#26041;&#27861;&#25913;&#36827;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
To train image-caption retrieval (ICR) methods, contrastive loss functions are a common choice for optimization functions. Unfortunately, contrastive ICR methods are vulnerable to predictive feature suppression. Predictive features are features that correctly indicate the similarity between a query and a candidate item. However, in the presence of multiple predictive features during training, encoder models tend to suppress redundant predictive features, since these features are not needed to learn to discriminate between positive and negative pairs. While some predictive features are redundant during training, these features might be relevant during evaluation. We introduce an approach to reduce predictive feature suppression for resource-constrained ICR methods: latent target decoding (LTD). We add an additional decoder to the contrastive ICR framework, to reconstruct the input caption in a latent space of a general-purpose sentence encoder, which prevents the image and caption encod
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20223;&#30495;&#38598;&#25104;&#30340;&#29983;&#29289;&#21551;&#21457;&#24335;&#25628;&#32034;&#27979;&#35797;&#26041;&#27861;Deeper&#65292;&#29992;&#20110;&#29983;&#25104;&#29992;&#20110;&#27979;&#35797;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#36710;&#36947;&#20445;&#25345;&#31995;&#32479;&#30340;&#25925;&#38556;&#21457;&#29616;&#27979;&#35797;&#22330;&#26223;&#65292;&#36890;&#36807;&#23454;&#35777;&#35780;&#20272;&#21644;&#19982;&#31454;&#36187;&#20013;&#30340;&#20854;&#20182;&#24037;&#20855;&#30340;&#27604;&#36739;&#23637;&#31034;&#20102;&#20854;&#24615;&#33021;&#30340;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2203.12026</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#20223;&#30495;&#38598;&#25104;&#30340;&#29983;&#29289;&#21551;&#21457;&#24335;&#25628;&#32034;&#27979;&#35797;&#26041;&#27861;&#22312;ADAS&#26696;&#20363;&#30740;&#31350;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Machine Learning Testing in an ADAS Case Study Using Simulation-Integrated Bio-Inspired Search-Based Testing. (arXiv:2203.12026v3 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.12026
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20223;&#30495;&#38598;&#25104;&#30340;&#29983;&#29289;&#21551;&#21457;&#24335;&#25628;&#32034;&#27979;&#35797;&#26041;&#27861;Deeper&#65292;&#29992;&#20110;&#29983;&#25104;&#29992;&#20110;&#27979;&#35797;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#36710;&#36947;&#20445;&#25345;&#31995;&#32479;&#30340;&#25925;&#38556;&#21457;&#29616;&#27979;&#35797;&#22330;&#26223;&#65292;&#36890;&#36807;&#23454;&#35777;&#35780;&#20272;&#21644;&#19982;&#31454;&#36187;&#20013;&#30340;&#20854;&#20182;&#24037;&#20855;&#30340;&#27604;&#36739;&#23637;&#31034;&#20102;&#20854;&#24615;&#33021;&#30340;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;Deeper&#30340;&#25193;&#23637;&#29256;&#26412;&#65292;&#23427;&#26159;&#19968;&#31181;&#22522;&#20110;&#25628;&#32034;&#23454;&#29616;&#30340;&#20223;&#30495;&#38598;&#25104;&#27979;&#35797;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#29983;&#25104;&#29992;&#20110;&#27979;&#35797;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#36710;&#36947;&#20445;&#25345;&#31995;&#32479;&#30340;&#25925;&#38556;&#21457;&#29616;&#27979;&#35797;&#22330;&#26223;&#12290;&#22312;&#26032;&#29256;&#26412;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#19968;&#32452;&#26032;&#30340;&#29983;&#29289;&#21551;&#21457;&#24335;&#25628;&#32034;&#31639;&#27861;-&#36951;&#20256;&#31639;&#27861;&#65288;GA&#65289;&#12289;&#65288;&#956;+&#955;&#65289;&#21644;&#65288;&#956;&#65292;&#955;&#65289;&#36827;&#21270;&#31574;&#30053;&#65288;ES&#65289;&#20197;&#21450;&#31890;&#23376;&#32676;&#20248;&#21270;&#65288;PSO&#65289;&#65292;&#36825;&#20123;&#31639;&#27861;&#21033;&#29992;&#36136;&#37327;&#31181;&#23376;&#31181;&#32676;&#20197;&#21450;&#20026;&#24314;&#27169;&#27979;&#35797;&#22330;&#26223;&#20351;&#29992;&#30340;&#29305;&#23450;&#39046;&#22495;&#20132;&#21449;&#21644;&#31361;&#21464;&#25805;&#20316;&#12290;&#20026;&#20102;&#23637;&#31034;Deeper&#20013;&#26032;&#27979;&#35797;&#29983;&#25104;&#22120;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#65292;&#24182;&#19982;SBST 2021&#30340;&#20116;&#20010;&#21442;&#36187;&#24037;&#20855;&#30340;&#32467;&#26524;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#26032;&#29256;&#26412;&#20013;&#65292;Deeper&#20013;&#30340;&#26032;&#27979;&#35797;&#29983;&#25104;&#22120;&#19981;&#20165;&#22312;&#20197;&#21069;&#30340;&#29256;&#26412;&#19978;&#26377;&#20102;&#24456;&#22823;&#25552;&#21319;&#65292;&#32780;&#19988;...
&lt;/p&gt;
&lt;p&gt;
This paper presents an extended version of Deeper, a search-based simulation-integrated test solution that generates failure-revealing test scenarios for testing a deep neural network-based lane-keeping system. In the newly proposed version, we utilize a new set of bio-inspired search algorithms, genetic algorithm (GA), $({\mu}+{\lambda})$ and $({\mu},{\lambda})$ evolution strategies (ES), and particle swarm optimization (PSO), that leverage a quality population seed and domain-specific cross-over and mutation operations tailored for the presentation model used for modeling the test scenarios. In order to demonstrate the capabilities of the new test generators within Deeper, we carry out an empirical evaluation and comparison with regard to the results of five participating tools in the cyber-physical systems testing competition at SBST 2021. Our evaluation shows the newly proposed test generators in Deeper not only represent a considerable improvement on the previous version but also 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;36&#31687;&#39030;&#32423;&#20250;&#35758;&#19978;&#21457;&#34920;&#30340;&#23454;&#20307;&#23545;&#40784;&#23884;&#20837;&#26041;&#27861;&#36827;&#34892;&#20102;&#32479;&#35745;&#21644;&#23454;&#39564;&#24615;&#20998;&#26512;&#65292;&#20026;&#23454;&#20307;&#23545;&#40784;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#26041;&#27861;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#32508;&#36848;&#65292;&#21253;&#25324;&#36825;&#20123;&#26041;&#27861;&#30340;&#20248;&#28857;&#12289;&#32570;&#28857;&#21644;&#36866;&#29992;&#24615;&#24182;&#30830;&#23450;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#25361;&#25112;&#21644;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2203.09280</link><description>&lt;p&gt;
&#23454;&#20307;&#23545;&#40784;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#26041;&#27861;&#65306;&#19968;&#20010;&#23454;&#39564;&#24615;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph Embedding Methods for Entity Alignment: An Experimental Review. (arXiv:2203.09280v2 [cs.DB] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.09280
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;36&#31687;&#39030;&#32423;&#20250;&#35758;&#19978;&#21457;&#34920;&#30340;&#23454;&#20307;&#23545;&#40784;&#23884;&#20837;&#26041;&#27861;&#36827;&#34892;&#20102;&#32479;&#35745;&#21644;&#23454;&#39564;&#24615;&#20998;&#26512;&#65292;&#20026;&#23454;&#20307;&#23545;&#40784;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#26041;&#27861;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#32508;&#36848;&#65292;&#21253;&#25324;&#36825;&#20123;&#26041;&#27861;&#30340;&#20248;&#28857;&#12289;&#32570;&#28857;&#21644;&#36866;&#29992;&#24615;&#24182;&#30830;&#23450;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#25361;&#25112;&#21644;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#25105;&#20204;&#30446;&#30585;&#20102;&#30693;&#35782;&#22270;&#35889;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#26088;&#22312;&#25903;&#25345;&#38382;&#31572;&#12289;&#25512;&#33616;&#31561;&#24212;&#29992;&#12290;&#23558;&#19981;&#21516;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#30693;&#35782;&#25972;&#21512;&#30340;&#19968;&#20010;&#24120;&#35265;&#20219;&#21153;&#26159;&#25214;&#21040;&#21738;&#20123;&#23376;&#22270;&#24341;&#29992;&#20102;&#21516;&#19968;&#20010;&#29616;&#23454;&#19990;&#30028;&#30340;&#23454;&#20307;&#12290;&#26368;&#36817;&#65292;&#23884;&#20837;&#26041;&#27861;&#24050;&#34987;&#29992;&#20110;&#23454;&#20307;&#23545;&#40784;&#20219;&#21153;&#65292;&#23398;&#20064;&#29992;&#20110;&#20445;&#30041;&#21407;&#22987;&#30693;&#35782;&#22270;&#35889;&#20013;&#23454;&#20307;&#30456;&#20284;&#24230;&#30340;&#21521;&#37327;&#31354;&#38388;&#34920;&#31034;&#12290;&#20154;&#20204;&#25552;&#20986;&#20102;&#21508;&#31181;&#21463;&#30417;&#30563;&#12289;&#26080;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#21033;&#29992;&#20102;&#30693;&#35782;&#22270;&#35889;&#20013;&#23454;&#20307;&#30340;&#20107;&#23454;&#65288;&#22522;&#20110;&#23646;&#24615;&#65289;&#21644;&#32467;&#26500;&#20449;&#24687;&#65288;&#22522;&#20110;&#20851;&#31995;&#65289;&#12290;&#28982;&#32780;&#65292;&#26681;&#25454;&#19981;&#21516;&#24615;&#33021;&#25351;&#26631;&#21644;&#30693;&#35782;&#22270;&#35889;&#29305;&#24449;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#30693;&#35782;&#22270;&#35889;&#20013;&#23545;&#23427;&#20204;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#36827;&#34892;&#23450;&#37327;&#35780;&#20272;&#30340;&#25991;&#29486;&#23578;&#32570;&#22833;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#39318;&#27425;&#23545;&#27969;&#34892;&#30340;&#23454;&#20307;&#23545;&#40784;&#23884;&#20837;&#26041;&#27861;&#36827;&#34892;&#30340;&#20803;&#20998;&#26512;&#65292;&#36825;&#22522;&#20110;&#20102;36&#31687;&#21457;&#34920;&#22312;&#39030;&#32423;&#20250;&#35758;&#19978;&#30340;&#35770;&#25991;&#65292;&#24182;&#36827;&#34892;&#20102;&#32479;&#35745;&#21644;&#23454;&#39564;&#24615;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25552;&#20379;&#20102;&#23545;&#20110;&#23454;&#20307;&#23545;&#40784;&#23884;&#20837;&#26041;&#27861;&#30340;&#29616;&#29366;&#32508;&#36848;&#65292;&#20197;&#21450;&#23427;&#20204;&#22312;&#19981;&#21516;&#30693;&#35782;&#22270;&#35889;&#22330;&#26223;&#19979;&#30340;&#20248;&#28857;&#12289;&#32570;&#28857;&#21644;&#36866;&#29992;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30830;&#23450;&#24182;&#20998;&#26512;&#20102;&#26410;&#26469;&#30740;&#31350;&#20013;&#30340;&#25361;&#25112;&#21644;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, we have witnessed the proliferation of knowledge graphs (KG) in various domains, aiming to support applications like question answering, recommendations, etc. A frequent task when integrating knowledge from different KGs is to find which subgraphs refer to the same real-world entity. Recently, embedding methods have been used for entity alignment tasks, that learn a vector-space representation of entities which preserves their similarity in the original KGs. A wide variety of supervised, unsupervised, and semi-supervised methods have been proposed that exploit both factual (attribute based) and structural information (relation based) of entities in the KGs. Still, a quantitative assessment of their strengths and weaknesses in real-world KGs according to different performance metrics and KG characteristics is missing from the literature. In this work, we conduct the first meta-level analysis of popular embedding methods for entity alignment, based on a statistically sou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22870;&#21169;&#23398;&#20064;&#20013;&#22870;&#21169;&#20989;&#25968;&#30340;&#37096;&#20998;&#21487;&#35782;&#21035;&#24615;&#65292;&#24182;&#20998;&#26512;&#20102;&#36825;&#31181;&#37096;&#20998;&#21487;&#35782;&#21035;&#24615;&#23545;&#25919;&#31574;&#20248;&#21270;&#31561;&#19979;&#28216;&#20219;&#21153;&#30340;&#24433;&#21709;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23545;&#27604;&#22870;&#21169;&#23398;&#20064;&#30340;&#25968;&#25454;&#28304;&#21644;&#19979;&#28216;&#20219;&#21153;&#65292;&#20197;&#20854;&#19981;&#21464;&#24615;&#20026;&#20381;&#25454;&#65292;&#23545;&#22870;&#21169;&#23398;&#20064;&#30340;&#25968;&#25454;&#28304;&#30340;&#35774;&#35745;&#21644;&#36873;&#25321;&#20135;&#29983;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2203.07475</link><description>&lt;p&gt;
&#25919;&#31574;&#20248;&#21270;&#20013;&#30340;&#19981;&#21464;&#24615;&#21450;&#22870;&#21169;&#23398;&#20064;&#20013;&#30340;&#37096;&#20998;&#21487;&#35782;&#21035;&#24615;
&lt;/p&gt;
&lt;p&gt;
Invariance in Policy Optimisation and Partial Identifiability in Reward Learning. (arXiv:2203.07475v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.07475
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22870;&#21169;&#23398;&#20064;&#20013;&#22870;&#21169;&#20989;&#25968;&#30340;&#37096;&#20998;&#21487;&#35782;&#21035;&#24615;&#65292;&#24182;&#20998;&#26512;&#20102;&#36825;&#31181;&#37096;&#20998;&#21487;&#35782;&#21035;&#24615;&#23545;&#25919;&#31574;&#20248;&#21270;&#31561;&#19979;&#28216;&#20219;&#21153;&#30340;&#24433;&#21709;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23545;&#27604;&#22870;&#21169;&#23398;&#20064;&#30340;&#25968;&#25454;&#28304;&#21644;&#19979;&#28216;&#20219;&#21153;&#65292;&#20197;&#20854;&#19981;&#21464;&#24615;&#20026;&#20381;&#25454;&#65292;&#23545;&#22870;&#21169;&#23398;&#20064;&#30340;&#25968;&#25454;&#28304;&#30340;&#35774;&#35745;&#21644;&#36873;&#25321;&#20135;&#29983;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22797;&#26434;&#30340;&#29616;&#23454;&#20219;&#21153;&#65292;&#25163;&#21160;&#35774;&#35745;&#22870;&#21169;&#20989;&#25968;&#36890;&#24120;&#26159;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21487;&#20197;&#20351;&#29992;&#22870;&#21169;&#23398;&#20064;&#20174;&#25968;&#25454;&#20013;&#25512;&#26029;&#22870;&#21169;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#22312;&#26080;&#38480;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#24120;&#20063;&#20250;&#26377;&#22810;&#20010;&#22870;&#21169;&#20989;&#25968;&#21487;&#20197;&#24456;&#22909;&#22320;&#25311;&#21512;&#25968;&#25454;&#12290;&#36825;&#24847;&#21619;&#30528;&#22870;&#21169;&#20989;&#25968;&#21482;&#33021;&#34987;&#37096;&#20998;&#22320;&#35782;&#21035;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#27491;&#24335;&#25551;&#36848;&#20102;&#22312;&#20960;&#31181;&#27969;&#34892;&#30340;&#22870;&#21169;&#23398;&#20064;&#25968;&#25454;&#28304;&#65288;&#21253;&#25324;&#19987;&#23478;&#28436;&#31034;&#21644;&#36712;&#36857;&#27604;&#36739;&#65289;&#19979;&#22870;&#21169;&#20989;&#25968;&#30340;&#37096;&#20998;&#21487;&#35782;&#21035;&#24615;&#12290;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#36825;&#31181;&#37096;&#20998;&#21487;&#35782;&#21035;&#24615;&#23545;&#20110;&#20960;&#39033;&#19979;&#28216;&#20219;&#21153;&#65288;&#20363;&#22914;&#25919;&#31574;&#20248;&#21270;&#65289;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#26694;&#26550;&#20013;&#32479;&#19968;&#20102;&#25105;&#20204;&#30340;&#32467;&#26524;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#20854;&#19981;&#21464;&#24615;&#23545;&#27604;&#25968;&#25454;&#28304;&#21644;&#19979;&#28216;&#20219;&#21153;&#65292;&#24182;&#23545;&#22870;&#21169;&#23398;&#20064;&#30340;&#25968;&#25454;&#28304;&#30340;&#35774;&#35745;&#21644;&#36873;&#25321;&#20135;&#29983;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is often very challenging to manually design reward functions for complex, real-world tasks. To solve this, one can instead use reward learning to infer a reward function from data. However, there are often multiple reward functions that fit the data equally well, even in the infinite-data limit. This means that the reward function is only partially identifiable. In this work, we formally characterise the partial identifiability of the reward function given several popular reward learning data sources, including expert demonstrations and trajectory comparisons. We also analyse the impact of this partial identifiability for several downstream tasks, such as policy optimisation. We unify our results in a framework for comparing data sources and downstream tasks by their invariances, with implications for the design and selection of data sources for reward learning.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#36830;&#32493;&#26102;&#38388;&#31574;&#30053;&#35780;&#20272;&#38382;&#39064;&#30340;&#26102;&#38388;&#24046;&#20998;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;&#29702;&#35770;&#25910;&#25947;&#36895;&#24230;&#12290;&#21516;&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#36824;&#21487;&#20197;&#35299;&#37322;&#20026;&#35299;&#20915;&#32447;&#24615;PDE&#25110;&#32447;&#24615;BSDE&#30340;&#26032;&#22411;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2202.07960</link><description>&lt;p&gt;
&#36830;&#32493;&#26102;&#38388;&#21644;&#29366;&#24577;&#19979;&#30340;&#26102;&#38388;&#24046;&#20998;&#23398;&#20064;&#65288;&#38543;&#26426;&#22330;&#26223;&#20013;&#65289;
&lt;/p&gt;
&lt;p&gt;
Temporal Difference Learning with Continuous Time and State in the Stochastic Setting. (arXiv:2202.07960v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.07960
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#36830;&#32493;&#26102;&#38388;&#31574;&#30053;&#35780;&#20272;&#38382;&#39064;&#30340;&#26102;&#38388;&#24046;&#20998;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;&#29702;&#35770;&#25910;&#25947;&#36895;&#24230;&#12290;&#21516;&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#36824;&#21487;&#20197;&#35299;&#37322;&#20026;&#35299;&#20915;&#32447;&#24615;PDE&#25110;&#32447;&#24615;BSDE&#30340;&#26032;&#22411;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#36830;&#32493;&#26102;&#38388;&#31574;&#30053;&#35780;&#20272;&#30340;&#38382;&#39064;&#12290;&#36825;&#24847;&#21619;&#30528;&#36890;&#36807;&#35266;&#23519;&#26469;&#23398;&#20064;&#19982;&#26410;&#21463;&#25511;&#21046;&#30340;&#36830;&#32493;&#26102;&#38388;&#38543;&#26426;&#21160;&#24577;&#21644;&#22870;&#21169;&#20989;&#25968;&#30456;&#20851;&#32852;&#30340;&#20215;&#20540;&#20989;&#25968;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#20351;&#29992;&#36880;&#28176;&#20943;&#23569;&#30340;&#26102;&#38388;&#27493;&#38271;&#30340;&#33879;&#21517;TD&#65288;0&#65289;&#26041;&#27861;&#30340;&#21407;&#22987;&#21464;&#20307;&#12290;&#19968;&#31181;&#26159;&#26080;&#27169;&#22411;&#30340;&#65292;&#21478;&#19968;&#31181;&#26159;&#22522;&#20110;&#27169;&#22411;&#30340;&#12290;&#23545;&#20110;&#20004;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#29702;&#35770;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#36890;&#36807;&#25968;&#20540;&#27169;&#25311;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#25110;&#32773;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#35299;&#37322;&#20026;&#36817;&#20284;&#35299;&#20915;&#32447;&#24615;PDE&#65288;&#20559;&#24494;&#20998;&#26041;&#31243;&#65289;&#25110;&#32447;&#24615;BSDE&#65288;&#21453;&#21521;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65289;&#30340;&#26032;&#22411;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of continuous-time policy evaluation. This consists in learning through observations the value function associated with an uncontrolled continuous-time stochastic dynamic and a reward function. We propose two original variants of the well-known TD(0) method using vanishing time steps. One is model-free and the other is model-based. For both methods, we prove theoretical convergence rates that we subsequently verify through numerical simulations. Alternatively, those methods can be interpreted as novel reinforcement learning approaches for approximating solutions of linear PDEs (partial differential equations) or linear BSDEs (backward stochastic differential equations).
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;Paddle-HeterPS&#30340;&#20998;&#24067;&#24335;&#26694;&#26550;&#65292;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#35843;&#24230;&#26041;&#27861;&#21487;&#20197;&#39640;&#25928;&#22320;&#21033;&#29992;&#22810;&#31181;&#31867;&#22411;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#35299;&#20915;&#20102;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#20013;&#22810;&#23618;&#27425;&#20998;&#37197;&#35745;&#31639;&#36164;&#28304;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2111.10635</link><description>&lt;p&gt;
HeterPS&#65306;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#35843;&#24230;&#30340;&#24322;&#26500;&#29615;&#22659;&#19979;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
HeterPS: Distributed Deep Learning With Reinforcement Learning Based Scheduling in Heterogeneous Environments. (arXiv:2111.10635v3 [cs.DC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.10635
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;Paddle-HeterPS&#30340;&#20998;&#24067;&#24335;&#26694;&#26550;&#65292;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#35843;&#24230;&#26041;&#27861;&#21487;&#20197;&#39640;&#25928;&#22320;&#21033;&#29992;&#22810;&#31181;&#31867;&#22411;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#35299;&#20915;&#20102;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#20013;&#22810;&#23618;&#27425;&#20998;&#37197;&#35745;&#31639;&#36164;&#28304;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21033;&#29992;&#35768;&#22810;&#23618;&#21644;&#22823;&#37327;&#21442;&#25968;&#23454;&#29616;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;DNN&#27169;&#22411;&#30340;&#35757;&#32451;&#36807;&#31243;&#36890;&#24120;&#22788;&#29702;&#20855;&#26377;&#35768;&#22810;&#31232;&#30095;&#29305;&#24449;&#30340;&#22823;&#35268;&#27169;&#36755;&#20837;&#25968;&#25454;&#65292;&#36825;&#20250;&#20135;&#29983;&#39640;&#24310;&#36831;&#21644;I/O&#25104;&#26412;&#65292;&#32780;&#26576;&#20123;&#23618;&#30340;&#35745;&#31639;&#25104;&#26412;&#24456;&#39640;&#12290;&#35757;&#32451;&#36807;&#31243;&#36890;&#24120;&#21033;&#29992;&#20998;&#24067;&#24335;&#35745;&#31639;&#36164;&#28304;&#26469;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#12290;&#27492;&#22806;&#65292;&#22810;&#31181;&#31867;&#22411;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#22914;CPU&#21644;GPU&#31561;&#65292;&#20063;&#21487;&#29992;&#20110;&#20998;&#24067;&#24335;&#35757;&#32451;&#36807;&#31243;&#12290;&#22240;&#27492;&#65292;&#22810;&#23618;&#27425;&#22320;&#20998;&#37197;&#35745;&#31639;&#36164;&#28304;&#23545;&#35757;&#32451;&#36807;&#31243;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#36890;&#36807;&#24322;&#26500;&#35745;&#31639;&#36164;&#28304;&#39640;&#25928;&#22320;&#35757;&#32451;DNN&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#26694;&#26550;Paddle-Heterogeneous Parameter Server&#65288;Paddle-HeterPS&#65289;&#65292;&#30001;&#20998;&#24067;&#24335;&#26550;&#26500;&#21644;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#35843;&#24230;&#26041;&#27861;&#32452;&#25104;&#12290;&#19982;&#29616;&#26377;&#26694;&#26550;&#30456;&#27604;&#65292;Paddle-HeterPS&#30340;&#20248;&#28857;&#26377;&#19977;&#20010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) exploit many layers and a large number of parameters to achieve excellent performance. The training process of DNN models generally handles large-scale input data with many sparse features, which incurs high Input/Output (IO) cost, while some layers are compute-intensive. The training process generally exploits distributed computing resources to reduce training time. In addition, heterogeneous computing resources, e.g., CPUs, GPUs of multiple types, are available for the distributed training process. Thus, the scheduling of multiple layers to diverse computing resources is critical for the training process. To efficiently train a DNN model using the heterogeneous computing resources, we propose a distributed framework, i.e., Paddle-Heterogeneous Parameter Server (Paddle-HeterPS), composed of a distributed architecture and a Reinforcement Learning (RL)-based scheduling method. The advantages of Paddle-HeterPS are three-fold compared with existing frameworks. 
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#24635;&#32467;&#20102;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#30340;&#30456;&#20851;&#25968;&#25454;&#38598;&#12289;&#26041;&#27861;&#12289;&#21019;&#26032;&#21644;&#25361;&#25112;&#65292;&#24182;&#25506;&#35752;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2111.10056</link><description>&lt;p&gt;
&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Medical Visual Question Answering: A Survey. (arXiv:2111.10056v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.10056
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#24635;&#32467;&#20102;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#30340;&#30456;&#20851;&#25968;&#25454;&#38598;&#12289;&#26041;&#27861;&#12289;&#21019;&#26032;&#21644;&#25361;&#25112;&#65292;&#24182;&#25506;&#35752;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#26159;&#21307;&#23398;&#20154;&#24037;&#26234;&#33021;&#21644;&#27969;&#34892;VQA&#25361;&#25112;&#30456;&#32467;&#21512;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;&#32473;&#23450;&#19968;&#24352;&#21307;&#23398;&#22270;&#20687;&#21644;&#19968;&#20010;&#33258;&#28982;&#35821;&#35328;&#30456;&#20851;&#30340;&#20020;&#24202;&#38382;&#39064;&#65292;&#21307;&#23398;VQA&#31995;&#32479;&#24212;&#35813;&#39044;&#27979;&#19968;&#20010;&#21512;&#29702;&#19988;&#26377;&#35828;&#26381;&#21147;&#30340;&#31572;&#26696;&#12290;&#34429;&#28982;&#24191;&#27867;&#30740;&#31350;&#20102;&#19968;&#33324;&#39046;&#22495;&#30340;VQA&#65292;&#20294;&#30001;&#20110;&#20854;&#20219;&#21153;&#29305;&#24615;&#65292;&#21307;&#23398;VQA&#20173;&#38656;&#35201;&#20855;&#26377;&#29305;&#23450;&#30340;&#35843;&#26597;&#21644;&#25506;&#32034;&#12290;&#22312;&#26412;&#32508;&#36848;&#30340;&#31532;&#19968;&#37096;&#20998;&#20013;&#65292;&#25105;&#20204;&#27719;&#24635;&#24182;&#35752;&#35770;&#20102;&#36804;&#20170;&#20026;&#27490;&#20844;&#24320;&#21487;&#29992;&#30340;&#21307;&#23398;VQA&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#25968;&#25454;&#28304;&#12289;&#25968;&#25454;&#25968;&#37327;&#21644;&#20219;&#21153;&#29305;&#24449;&#12290;&#22312;&#31532;&#20108;&#37096;&#20998;&#20013;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#29992;&#20110;&#21307;&#23398;VQA&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#24635;&#32467;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#30340;&#25216;&#26415;&#12289;&#21019;&#26032;&#21644;&#28508;&#22312;&#25913;&#36827;&#12290;&#22312;&#26368;&#21518;&#19968;&#37096;&#20998;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#35813;&#39046;&#22495;&#20013;&#30340;&#19968;&#20123;&#21307;&#23398;&#29305;&#23450;&#25361;&#25112;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20026;&#23545;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#39046;&#22495;&#24863;&#20852;&#36259;&#30340;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20840;&#38754;&#26377;&#29992;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical Visual Question Answering~(VQA) is a combination of medical artificial intelligence and popular VQA challenges. Given a medical image and a clinically relevant question in natural language, the medical VQA system is expected to predict a plausible and convincing answer. Although the general-domain VQA has been extensively studied, the medical VQA still needs specific investigation and exploration due to its task features. In the first part of this survey, we collect and discuss the publicly available medical VQA datasets up-to-date about the data source, data quantity, and task feature. In the second part, we review the approaches used in medical VQA tasks. We summarize and discuss their techniques, innovations, and potential improvements. In the last part, we analyze some medical-specific challenges for the field and discuss future research directions. Our goal is to provide comprehensive and helpful information for researchers interested in the medical visual question answeri
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#22870;&#21169;&#21644;&#24809;&#32602;&#26041;&#27861;&#19979;&#65292;&#20154;&#20204;&#23545;&#20110;&#23398;&#20064;&#32773;&#30340;&#26399;&#26395;&#20551;&#35774;&#65292;&#21457;&#29616;&#20154;&#20204;&#20551;&#35774;&#23398;&#20064;&#32773;&#20855;&#26377;&#39640;&#30340;&#25240;&#25187;&#29575;&#21644;&#39640;&#24230;&#37325;&#35270;&#25506;&#32034;&#65292;&#24182;&#26681;&#25454;&#23398;&#20064;&#32773;&#36827;&#23637;&#35843;&#25972;&#25945;&#23398;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2009.02476</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#25945;&#23398;&#30740;&#31350;&#25945;&#25480;&#24378;&#21270;&#23398;&#20064;&#32773;&#26102;&#20154;&#31867;&#30340;&#20551;&#35774;
&lt;/p&gt;
&lt;p&gt;
Using Machine Teaching to Investigate Human Assumptions when Teaching Reinforcement Learners. (arXiv:2009.02476v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2009.02476
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#22870;&#21169;&#21644;&#24809;&#32602;&#26041;&#27861;&#19979;&#65292;&#20154;&#20204;&#23545;&#20110;&#23398;&#20064;&#32773;&#30340;&#26399;&#26395;&#20551;&#35774;&#65292;&#21457;&#29616;&#20154;&#20204;&#20551;&#35774;&#23398;&#20064;&#32773;&#20855;&#26377;&#39640;&#30340;&#25240;&#25187;&#29575;&#21644;&#39640;&#24230;&#37325;&#35270;&#25506;&#32034;&#65292;&#24182;&#26681;&#25454;&#23398;&#20064;&#32773;&#36827;&#23637;&#35843;&#25972;&#25945;&#23398;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#25104;&#21151;&#25945;&#23398;&#65292;&#38656;&#35201;&#23545;&#23398;&#20064;&#32773;&#23398;&#20064;&#26041;&#24335;&#36827;&#34892;&#20551;&#35774;&#65292;&#21363;&#23398;&#20064;&#32773;&#22914;&#20309;&#20351;&#29992;&#26469;&#33258;&#19990;&#30028;&#30340;&#32463;&#39564;&#26469;&#26356;&#26032;&#20854;&#20869;&#37096;&#29366;&#24577;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#22870;&#21169;&#21644;&#24809;&#32602;&#26041;&#27861;&#19979;&#65292;&#20154;&#20204;&#23545;&#20110;&#23398;&#20064;&#32773;&#30340;&#26399;&#26395;&#20551;&#35774;&#12290;&#30740;&#31350;&#37325;&#28857;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861; Q-learning&#65292;&#36890;&#36807;&#34892;&#20026;&#23454;&#39564;&#32771;&#23519;&#20154;&#20204;&#30340;&#20551;&#35774;&#12290;&#20026;&#20102;&#36798;&#21040;&#27492;&#30446;&#30340;&#65292;&#25105;&#20204;&#39318;&#20808;&#24314;&#31435;&#20102;&#19968;&#20010;&#35268;&#33539;&#26631;&#20934;&#65292;&#23558;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#26426;&#22120;&#25945;&#23398;&#20248;&#21270;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#26426;&#22120;&#25945;&#23398;&#20248;&#21270;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#36924;&#36817;&#26041;&#27861;&#26469;&#27169;&#25311;&#23398;&#20064;&#32773;&#22312;&#29615;&#22659;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#23398;&#20064;&#39044;&#27979;&#21453;&#39304;&#22914;&#20309;&#24433;&#21709;&#23398;&#20064;&#32773;&#30340;&#20869;&#37096;&#29366;&#24577;&#12290;&#22312;&#25945;&#25480;&#29702;&#24819;&#21270;&#30340;&#25506;&#32034;&#21033;&#29992;&#20219;&#21153;&#26102;&#65292;&#20154;&#20204;&#23545;&#23398;&#20064;&#32773;&#30340;&#23398;&#20064;&#21644;&#25240;&#25187;&#29575;&#26377;&#21738;&#20123;&#20551;&#35774;&#65311;&#22312;&#34892;&#20026;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20154;&#20204;&#21487;&#20197;&#30456;&#23545;&#39640;&#25928;&#21644;&#20934;&#30830;&#22320;&#25945;&#23548; Q-&#23398;&#20064;&#32773;&#36825;&#39033;&#20219;&#21153;&#12290;&#20154;&#20204;&#20542;&#21521;&#20110;&#20551;&#35774;&#23398;&#20064;&#32773;&#20855;&#26377;&#39640;&#30340;&#25240;&#25187;&#29575;&#65292;&#24182;&#39640;&#24230;&#37325;&#35270;&#25506;&#32034;&#12290;&#27492;&#22806;&#65292;&#20154;&#20204;&#20250;&#26681;&#25454;&#23398;&#20064;&#32773;&#30340;&#36827;&#23637;&#35843;&#25972;&#33258;&#24049;&#30340;&#25945;&#23398;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Successful teaching requires an assumption of how the learner learns - how the learner uses experiences from the world to update their internal states. We investigate what expectations people have about a learner when they teach them in an online manner using rewards and punishment. We focus on a common reinforcement learning method, Q-learning, and examine what assumptions people have using a behavioral experiment. To do so, we first establish a normative standard, by formulating the problem as a machine teaching optimization problem. To solve the machine teaching optimization problem, we use a deep learning approximation method which simulates learners in the environment and learns to predict how feedback affects the learner's internal states. What do people assume about a learner's learning and discount rates when they teach them an idealized exploration-exploitation task? In a behavioral experiment, we find that people can teach the task to Q-learners in a relatively efficient and 
&lt;/p&gt;</description></item></channel></rss>