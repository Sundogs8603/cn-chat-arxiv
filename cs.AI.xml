<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#35813;&#35770;&#25991;&#23545;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#28145;&#20837;&#35752;&#35770;&#20102;&#20851;&#38190;&#38382;&#39064;&#65292;&#21253;&#25324;&#22810;&#35821;&#35328;&#35821;&#26009;&#24211;&#12289;&#23545;&#40784;&#21644;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2404.00929</link><description>&lt;p&gt;
&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#35821;&#26009;&#24211;&#12289;&#23545;&#40784;&#21644;&#20559;&#35265;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Multilingual Large Language Models: Corpora, Alignment, and Bias
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00929
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23545;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#28145;&#20837;&#35752;&#35770;&#20102;&#20851;&#38190;&#38382;&#39064;&#65292;&#21253;&#25324;&#22810;&#35821;&#35328;&#35821;&#26009;&#24211;&#12289;&#23545;&#40784;&#21644;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#22522;&#30784;&#19978;&#65292;&#21457;&#23637;&#20102;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#26469;&#35299;&#20915;&#22810;&#35821;&#35328;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#25361;&#25112;&#65292;&#24076;&#26395;&#23454;&#29616;&#20174;&#39640;&#36164;&#28304;&#21040;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#30693;&#35782;&#36716;&#31227;&#12290;&#28982;&#32780;&#65292;&#20173;&#28982;&#23384;&#22312;&#37325;&#35201;&#38480;&#21046;&#21644;&#25361;&#25112;&#65292;&#27604;&#22914;&#35821;&#35328;&#19981;&#24179;&#34913;&#12289;&#22810;&#35821;&#35328;&#23545;&#40784;&#21644;&#22266;&#26377;&#20559;&#35265;&#12290;&#26412;&#25991;&#26088;&#22312;&#23545;MLLMs&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#65292;&#28145;&#20837;&#35752;&#35770;&#22260;&#32469;&#36825;&#20123;&#20851;&#38190;&#38382;&#39064;&#30340;&#35758;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00929v1 Announce Type: cross  Abstract: Based on the foundation of Large Language Models (LLMs), Multilingual Large Language Models (MLLMs) have been developed to address the challenges of multilingual natural language processing tasks, hoping to achieve knowledge transfer from high-resource to low-resource languages. However, significant limitations and challenges still exist, such as language imbalance, multilingual alignment, and inherent bias. In this paper, we aim to provide a comprehensive analysis of MLLMs, delving deeply into discussions surrounding these critical issues. First of all, we start by presenting an overview of MLLMs, covering their evolution, key techniques, and multilingual capacities. Secondly, we explore widely utilized multilingual corpora for MLLMs' training and multilingual datasets oriented for downstream tasks that are crucial for enhancing the cross-lingual capability of MLLMs. Thirdly, we survey the existing studies on multilingual representati
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#35770;&#25454;&#24863;&#30693;&#26041;&#27861;&#25913;&#36827;&#20107;&#20214;&#38142;&#25509;&#27169;&#22411;&#65292;&#33021;&#26356;&#22909;&#22320;&#35782;&#21035;&#21644;&#20998;&#31867;&#19981;&#22312;&#30693;&#35782;&#24211;&#20013;&#30340;&#20107;&#20214;&#25552;&#21450;&#65292;&#24357;&#34917;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;</title><link>https://arxiv.org/abs/2403.15097</link><description>&lt;p&gt;
&#35770;&#25454;&#24863;&#30693;&#20107;&#20214;&#38142;&#25509;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Argument-Aware Approach To Event Linking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15097
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#35770;&#25454;&#24863;&#30693;&#26041;&#27861;&#25913;&#36827;&#20107;&#20214;&#38142;&#25509;&#27169;&#22411;&#65292;&#33021;&#26356;&#22909;&#22320;&#35782;&#21035;&#21644;&#20998;&#31867;&#19981;&#22312;&#30693;&#35782;&#24211;&#20013;&#30340;&#20107;&#20214;&#25552;&#21450;&#65292;&#24357;&#34917;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15097v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;: &#20107;&#20214;&#38142;&#25509;&#23558;&#25991;&#26412;&#20013;&#30340;&#20107;&#20214;&#25552;&#21450;&#19982;&#30693;&#35782;&#24211;&#65288;KB&#65289;&#20013;&#30456;&#20851;&#33410;&#28857;&#36830;&#25509;&#36215;&#26469;&#12290;&#20808;&#21069;&#22312;&#20107;&#20214;&#38142;&#25509;&#26041;&#38754;&#30340;&#30740;&#31350;&#20027;&#35201;&#20511;&#37492;&#20102;&#23454;&#20307;&#38142;&#25509;&#30340;&#26041;&#27861;&#65292;&#24573;&#30053;&#20102;&#20107;&#20214;&#30340;&#29420;&#29305;&#29305;&#24449;&#12290;&#19982;&#24191;&#27867;&#25506;&#35752;&#30340;&#23454;&#20307;&#38142;&#25509;&#20219;&#21153;&#30456;&#27604;&#65292;&#20107;&#20214;&#20855;&#26377;&#26356;&#21152;&#22797;&#26434;&#30340;&#32467;&#26500;&#65292;&#21487;&#20197;&#36890;&#36807;&#26816;&#26597;&#20854;&#20851;&#32852;&#30340;&#35770;&#25454;&#26356;&#26377;&#25928;&#22320;&#21152;&#20197;&#21306;&#20998;&#12290;&#27492;&#22806;&#65292;&#20107;&#20214;&#30340;&#20449;&#24687;&#20016;&#23500;&#24615;&#23548;&#33268;&#20107;&#20214;&#30693;&#35782;&#24211;&#30340;&#31232;&#32570;&#24615;&#12290;&#36825;&#24378;&#35843;&#20102;&#20107;&#20214;&#38142;&#25509;&#27169;&#22411;&#38656;&#35201;&#35782;&#21035;&#21644;&#20998;&#31867;&#19981;&#22312;&#30693;&#35782;&#24211;&#20013;&#30340;&#20107;&#20214;&#25552;&#21450;&#20316;&#20026;&#8220;&#36229;&#20986;&#30693;&#35782;&#24211;&#8221;&#30340;&#37325;&#35201;&#24615;&#65292;&#32780;&#36825;&#19968;&#39046;&#22495;&#21463;&#21040;&#20102;&#26377;&#38480;&#20851;&#27880;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#35770;&#25454;&#24863;&#30693;&#26041;&#27861;&#26469;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#26631;&#35760;&#20107;&#20214;&#35770;&#25454;&#20449;&#24687;&#26469;&#25913;&#36827;&#20107;&#20214;&#38142;&#25509;&#27169;&#22411;&#65292;&#26377;&#21161;&#20110;&#35782;&#21035;&#26377;&#20851;&#20107;&#20214;&#25552;&#21450;&#30340;&#20851;&#38190;&#20449;&#24687;&#12290;&#38543;&#21518;&#65292;&#20026;&#20102;&#24110;&#21161;&#27169;&#22411;&#22788;&#29702;&#8220;&#36229;&#20986;&#30693;&#35782;&#24211;&#8221;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15097v1 Announce Type: cross  Abstract: Event linking connects event mentions in text with relevant nodes in a knowledge base (KB). Prior research in event linking has mainly borrowed methods from entity linking, overlooking the distinct features of events. Compared to the extensively explored entity linking task, events have more complex structures and can be more effectively distinguished by examining their associated arguments. Moreover, the information-rich nature of events leads to the scarcity of event KBs. This emphasizes the need for event linking models to identify and classify event mentions not in the KB as ``out-of-KB,'' an area that has received limited attention. In this work, we tackle these challenges by introducing an argument-aware approach. First, we improve event linking models by augmenting input text with tagged event argument information, facilitating the recognition of key information about event mentions. Subsequently, to help the model handle ``out-
&lt;/p&gt;</description></item><item><title>ThermoHands&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;ThermoHands&#65292;&#26088;&#22312;&#35299;&#20915;&#28909;&#22270;&#20013;&#20027;&#35266;&#35270;&#35282;3D&#25163;&#37096;&#23039;&#21183;&#20272;&#35745;&#30340;&#25361;&#25112;&#65292;&#20171;&#32461;&#20102;&#19968;&#20010;&#20855;&#26377;&#21452;transformer&#27169;&#22359;&#30340;&#23450;&#21046;&#22522;&#32447;&#26041;&#27861;TheFormer&#65292;&#34920;&#26126;&#28909;&#25104;&#20687;&#22312;&#24694;&#21155;&#26465;&#20214;&#19979;&#23454;&#29616;&#31283;&#20581;&#30340;3D&#25163;&#37096;&#23039;&#21183;&#20272;&#35745;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.09871</link><description>&lt;p&gt;
ThermoHands&#65306;&#19968;&#31181;&#29992;&#20110;&#20174;&#20027;&#35266;&#35270;&#35282;&#28909;&#22270;&#20013;&#20272;&#35745;3D&#25163;&#37096;&#23039;&#21183;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
ThermoHands: A Benchmark for 3D Hand Pose Estimation from Egocentric Thermal Image
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09871
&lt;/p&gt;
&lt;p&gt;
ThermoHands&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;ThermoHands&#65292;&#26088;&#22312;&#35299;&#20915;&#28909;&#22270;&#20013;&#20027;&#35266;&#35270;&#35282;3D&#25163;&#37096;&#23039;&#21183;&#20272;&#35745;&#30340;&#25361;&#25112;&#65292;&#20171;&#32461;&#20102;&#19968;&#20010;&#20855;&#26377;&#21452;transformer&#27169;&#22359;&#30340;&#23450;&#21046;&#22522;&#32447;&#26041;&#27861;TheFormer&#65292;&#34920;&#26126;&#28909;&#25104;&#20687;&#22312;&#24694;&#21155;&#26465;&#20214;&#19979;&#23454;&#29616;&#31283;&#20581;&#30340;3D&#25163;&#37096;&#23039;&#21183;&#20272;&#35745;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ThermoHands&#65292;&#36825;&#26159;&#19968;&#20010;&#38024;&#23545;&#22522;&#20110;&#28909;&#22270;&#30340;&#20027;&#35266;&#35270;&#35282;3D&#25163;&#37096;&#23039;&#21183;&#20272;&#35745;&#30340;&#26032;&#22522;&#20934;&#65292;&#26088;&#22312;&#20811;&#26381;&#35832;&#22914;&#20809;&#29031;&#21464;&#21270;&#21644;&#36974;&#25377;&#65288;&#20363;&#22914;&#25163;&#37096;&#31359;&#25140;&#29289;&#65289;&#31561;&#25361;&#25112;&#12290;&#35813;&#22522;&#20934;&#21253;&#25324;&#26469;&#33258;28&#21517;&#20027;&#20307;&#36827;&#34892;&#25163;-&#29289;&#20307;&#21644;&#25163;-&#34394;&#25311;&#20132;&#20114;&#30340;&#22810;&#26679;&#25968;&#25454;&#38598;&#65292;&#32463;&#36807;&#33258;&#21160;&#21270;&#36807;&#31243;&#20934;&#30830;&#26631;&#27880;&#20102;3D&#25163;&#37096;&#23039;&#21183;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#23450;&#21046;&#30340;&#22522;&#32447;&#26041;&#27861;TheFormer&#65292;&#21033;&#29992;&#21452;transformer&#27169;&#22359;&#22312;&#28909;&#22270;&#20013;&#23454;&#29616;&#26377;&#25928;&#30340;&#20027;&#35266;&#35270;&#35282;3D&#25163;&#37096;&#23039;&#21183;&#20272;&#35745;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#31361;&#26174;&#20102;TheFormer&#30340;&#39046;&#20808;&#24615;&#33021;&#65292;&#24182;&#30830;&#35748;&#20102;&#28909;&#25104;&#20687;&#22312;&#23454;&#29616;&#24694;&#21155;&#26465;&#20214;&#19979;&#31283;&#20581;&#30340;3D&#25163;&#37096;&#23039;&#21183;&#20272;&#35745;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09871v1 Announce Type: cross  Abstract: In this work, we present ThermoHands, a new benchmark for thermal image-based egocentric 3D hand pose estimation, aimed at overcoming challenges like varying lighting and obstructions (e.g., handwear). The benchmark includes a diverse dataset from 28 subjects performing hand-object and hand-virtual interactions, accurately annotated with 3D hand poses through an automated process. We introduce a bespoken baseline method, TheFormer, utilizing dual transformer modules for effective egocentric 3D hand pose estimation in thermal imagery. Our experimental results highlight TheFormer's leading performance and affirm thermal imaging's effectiveness in enabling robust 3D hand pose estimation in adverse conditions.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36845;&#20195;&#33258;&#21453;&#39304;&#30340;&#26816;&#32034;&#22686;&#24378;&#26041;&#27861;&#22312;&#25351;&#23450;&#20219;&#21153;&#30340;&#29305;&#23450;&#22330;&#26223;&#20013;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#20248;&#20110;&#29616;&#26377;&#22522;&#20934;&#27169;&#22411;&#65292;&#26174;&#33879;&#22686;&#24378;&#20102;&#20107;&#23454;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.06840</link><description>&lt;p&gt;
RA-ISF: &#36890;&#36807;&#36845;&#20195;&#33258;&#21453;&#39304;&#23398;&#20064;&#26816;&#32034;&#22686;&#24378;&#20197;&#22238;&#31572;&#21644;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
RA-ISF: Learning to Answer and Understand from Retrieval Augmentation via Iterative Self-Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06840
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36845;&#20195;&#33258;&#21453;&#39304;&#30340;&#26816;&#32034;&#22686;&#24378;&#26041;&#27861;&#22312;&#25351;&#23450;&#20219;&#21153;&#30340;&#29305;&#23450;&#22330;&#26223;&#20013;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#20248;&#20110;&#29616;&#26377;&#22522;&#20934;&#27169;&#22411;&#65292;&#26174;&#33879;&#22686;&#24378;&#20102;&#20107;&#23454;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20173;&#28982;&#20005;&#37325;&#20381;&#36182;&#23384;&#20648;&#22312;&#20854;&#21442;&#25968;&#20013;&#30340;&#30693;&#35782;&#12290;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;(RAG)&#26041;&#27861;&#36890;&#36807;&#25972;&#21512;&#22806;&#37096;&#30693;&#35782;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#26816;&#32034;&#19982;&#26597;&#35810;&#30456;&#20851;&#30340;&#30693;&#35782;&#26469;&#22238;&#31572;&#20197;&#21069;&#26080;&#27861;&#22238;&#31572;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#26816;&#32034;&#22686;&#24378;&#36845;&#20195;&#33258;&#21453;&#39304;(RA-ISF)&#26694;&#26550;&#65292;&#36890;&#36807;&#19977;&#20010;&#23376;&#27169;&#22359;&#36845;&#20195;&#20998;&#35299;&#20219;&#21153;&#24182;&#22788;&#29702;&#23427;&#20204;&#65292;&#20197;&#22686;&#24378;&#27169;&#22411;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#22522;&#20934;&#65292;&#22312;&#35832;&#22914;GPT3.5&#12289;Llama2&#20043;&#31867;&#30340;&#27169;&#22411;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#26174;&#33879;&#22686;&#24378;&#20102;&#20107;&#23454;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06840v1 Announce Type: cross  Abstract: Large language models (LLMs) demonstrate exceptional performance in numerous tasks but still heavily rely on knowledge stored in their parameters. Moreover, updating this knowledge incurs high training costs. Retrieval-augmented generation (RAG) methods address this issue by integrating external knowledge. The model can answer questions it couldn't previously by retrieving knowledge relevant to the query. This approach improves performance in certain scenarios for specific tasks. However, if irrelevant texts are retrieved, it may impair model performance. In this paper, we propose Retrieval Augmented Iterative Self-Feedback (RA-ISF), a framework that iteratively decomposes tasks and processes them in three submodules to enhance the model's problem-solving capabilities. Experiments show that our method outperforms existing benchmarks, performing well on models like GPT3.5, Llama2, significantly enhancing factual reasoning capabilities a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;LaGTran&#26694;&#26550;&#65292;&#21033;&#29992;&#25991;&#26412;&#25551;&#36848;&#26469;&#24341;&#23548;&#30693;&#35782;&#36716;&#31227;&#65292;&#22312;&#22788;&#29702;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.05535</link><description>&lt;p&gt;
&#35762;&#36848;&#65292;&#32780;&#19981;&#26159;&#23637;&#31034;&#65281;&#65306;&#35821;&#35328;&#25351;&#23548;&#26377;&#21161;&#20110;&#22312;&#22270;&#20687;&#21644;&#35270;&#39057;&#39046;&#22495;&#20043;&#38388;&#36827;&#34892;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
Tell, Don't Show!: Language Guidance Eases Transfer Across Domains in Images and Videos
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05535
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;LaGTran&#26694;&#26550;&#65292;&#21033;&#29992;&#25991;&#26412;&#25551;&#36848;&#26469;&#24341;&#23548;&#30693;&#35782;&#36716;&#31227;&#65292;&#22312;&#22788;&#29702;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;LaGTran&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#21363;&#21487;&#33719;&#24471;&#25110;&#26131;&#20110;&#33719;&#21462;&#30340;&#25991;&#26412;&#25551;&#36848;&#65292;&#24341;&#23548;&#20174;&#24102;&#26631;&#31614;&#30340;&#28304;&#25968;&#25454;&#21040;&#20855;&#26377;&#22495;&#20559;&#31227;&#30340;&#26080;&#26631;&#31614;&#30446;&#26631;&#25968;&#25454;&#30340;&#40065;&#26834;&#24615;&#30693;&#35782;&#36716;&#31227;&#12290;&#21463;&#21040;&#25105;&#20204;&#35266;&#23519;&#21040;&#26356;&#23500;&#35821;&#20041;&#30340;&#25991;&#26412;&#27169;&#24577;&#20855;&#26377;&#26356;&#26377;&#21033;&#30340;&#36716;&#31227;&#29305;&#24615;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#36716;&#31227;&#26426;&#21046;&#65292;&#20351;&#29992;&#28304;&#35757;&#32451;&#30340;&#25991;&#26412;&#20998;&#31867;&#22120;&#22312;&#30446;&#26631;&#25991;&#26412;&#25551;&#36848;&#19978;&#29983;&#25104;&#39044;&#27979;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#39044;&#27979;&#20316;&#20026;&#30456;&#24212;&#22270;&#20687;&#30340;&#30417;&#30563;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20197;&#35821;&#35328;&#25351;&#23548;&#20026;&#39537;&#21160;&#65292;&#20986;&#22855;&#22320;&#31616;&#21333;&#26131;&#34892;&#65292;&#21364;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#22914;GeoNet&#21644;DomainNet&#19978;&#26174;&#33879;&#20248;&#20110;&#20197;&#24448;&#25152;&#26377;&#26041;&#27861;&#65292;&#39564;&#35777;&#20102;&#20854;&#26497;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05535v1 Announce Type: cross  Abstract: We introduce LaGTran, a novel framework that utilizes readily available or easily acquired text descriptions to guide robust transfer of discriminative knowledge from labeled source to unlabeled target data with domain shifts. While unsupervised adaptation methods have been established to address this problem, they show limitations in handling challenging domain shifts due to their exclusive operation within the pixel-space. Motivated by our observation that semantically richer text modality has more favorable transfer properties, we devise a transfer mechanism to use a source-trained text-classifier to generate predictions on the target text descriptions, and utilize these predictions as supervision for the corresponding images. Our approach driven by language guidance is surprisingly easy and simple, yet significantly outperforms all prior approaches on challenging datasets like GeoNet and DomainNet, validating its extreme effectiven
&lt;/p&gt;</description></item><item><title>&#32852;&#37030;&#23398;&#20064;&#24341;&#20837;&#20102;&#26032;&#30340;&#38544;&#31169;&#35201;&#27714;&#65292;&#20419;&#20351;&#30740;&#31350;&#24320;&#22987;&#20851;&#27880;&#36866;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#29615;&#22659;&#30340;&#21453;&#23398;&#20064;&#26426;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.02437</link><description>&lt;p&gt;
SoK: &#32852;&#37030;&#21453;&#23398;&#20064;&#20013;&#30340;&#25361;&#25112;&#19982;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
SoK: Challenges and Opportunities in Federated Unlearning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02437
&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#24341;&#20837;&#20102;&#26032;&#30340;&#38544;&#31169;&#35201;&#27714;&#65292;&#20419;&#20351;&#30740;&#31350;&#24320;&#22987;&#20851;&#27880;&#36866;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#29615;&#22659;&#30340;&#21453;&#23398;&#20064;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20110;2017&#24180;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20419;&#36827;&#20102;&#19981;&#20449;&#20219;&#26041;&#20043;&#38388;&#30340;&#21512;&#20316;&#23398;&#20064;&#65292;&#26080;&#38656;&#21508;&#26041;&#26126;&#30830;&#20849;&#20139;&#20854;&#25968;&#25454;&#12290;&#36825;&#20801;&#35768;&#22312;&#23562;&#37325;GDPR&#21644;CPRA&#31561;&#38544;&#31169;&#35268;&#23450;&#30340;&#21516;&#26102;&#65292;&#22312;&#29992;&#25143;&#25968;&#25454;&#19978;&#35757;&#32451;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#26032;&#20852;&#30340;&#38544;&#31169;&#35201;&#27714;&#21487;&#33021;&#35201;&#27714;&#27169;&#22411;&#25152;&#26377;&#32773;&#33021;&#22815;&#8220;&#36951;&#24536;&#8221;&#19968;&#20123;&#24050;&#23398;&#20064;&#30340;&#25968;&#25454;&#65292;&#20363;&#22914;&#24403;&#25968;&#25454;&#25152;&#26377;&#32773;&#25110;&#25191;&#27861;&#26426;&#26500;&#35201;&#27714;&#26102;&#12290;&#36825;&#20652;&#29983;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;&#26426;&#22120;&#21453;&#23398;&#20064;&#8221;&#30340;&#27963;&#36291;&#30740;&#31350;&#39046;&#22495;&#12290;&#22312;FL&#30340;&#32972;&#26223;&#19979;&#65292;&#35768;&#22810;&#20026;&#38598;&#20013;&#24335;&#29615;&#22659;&#24320;&#21457;&#30340;&#21453;&#23398;&#20064;&#25216;&#26415;&#24182;&#19981;&#23481;&#26131;&#24212;&#29992;&#65281;&#36825;&#26159;&#30001;&#20110;FL&#20013;&#38598;&#20013;&#24335;&#21644;&#20998;&#24067;&#24335;&#23398;&#20064;&#20043;&#38388;&#30340;&#29420;&#29305;&#24046;&#24322;&#65292;&#29305;&#21035;&#26159;&#20114;&#21160;&#24615;&#12289;&#38543;&#26426;&#24615;&#12289;&#24322;&#26500;&#24615;&#21644;&#26377;&#38480;&#21487;&#35775;&#38382;&#24615;&#12290;&#20026;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#26368;&#36817;&#30340;&#19968;&#31995;&#21015;&#30740;&#31350;&#24037;&#20316;&#32858;&#28966;&#20110;&#24320;&#21457;&#36866;&#29992;&#20110;FL&#30340;&#21453;&#23398;&#20064;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02437v1 Announce Type: cross  Abstract: Federated learning (FL), introduced in 2017, facilitates collaborative learning between non-trusting parties with no need for the parties to explicitly share their data among themselves. This allows training models on user data while respecting privacy regulations such as GDPR and CPRA. However, emerging privacy requirements may mandate model owners to be able to \emph{forget} some learned data, e.g., when requested by data owners or law enforcement. This has given birth to an active field of research called \emph{machine unlearning}. In the context of FL, many techniques developed for unlearning in centralized settings are not trivially applicable! This is due to the unique differences between centralized and distributed learning, in particular, interactivity, stochasticity, heterogeneity, and limited accessibility in FL. In response, a recent line of work has focused on developing unlearning mechanisms tailored to FL.   This SoK pape
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#26102;&#31354;&#22330;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#27169;&#22411;&#21644;&#37329;&#23383;&#22612;&#25512;&#26029;&#26694;&#26550;&#65292;&#22312;&#31354;&#27668;&#36136;&#37327;&#25512;&#26029;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.02354</link><description>&lt;p&gt;
&#22522;&#20110;&#26102;&#31354;&#22330;&#31070;&#32463;&#32593;&#32476;&#30340;&#31354;&#27668;&#36136;&#37327;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Spatio-Temporal Field Neural Networks for Air Quality Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02354
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#26102;&#31354;&#22330;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#27169;&#22411;&#21644;&#37329;&#23383;&#22612;&#25512;&#26029;&#26694;&#26550;&#65292;&#22312;&#31354;&#27668;&#36136;&#37327;&#25512;&#26029;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#27668;&#36136;&#37327;&#25512;&#26029;&#38382;&#39064;&#26088;&#22312;&#21033;&#29992;&#26469;&#33258;&#26377;&#38480;&#35266;&#27979;&#31449;&#30340;&#21382;&#21490;&#25968;&#25454;&#25512;&#26029;&#26410;&#30693;&#20301;&#32622;&#30340;&#31354;&#27668;&#36136;&#37327;&#25351;&#25968;&#12290;&#32771;&#34385;&#21040;&#35266;&#27979;&#31449;&#39640;&#26114;&#30340;&#32500;&#25252;&#25104;&#26412;&#23548;&#33268;&#25968;&#25454;&#31232;&#30095;&#24615;&#65292;&#33391;&#22909;&#30340;&#25512;&#26029;&#31639;&#27861;&#21487;&#20197;&#26377;&#25928;&#33410;&#32422;&#25104;&#26412;&#24182;&#32454;&#21270;&#25968;&#25454;&#31890;&#24230;&#12290;&#23613;&#31649;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#36825;&#20010;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#23545;&#29616;&#23454;&#30340;&#38750;&#27431;&#20960;&#37324;&#24471;&#21644;&#31163;&#25955;&#25968;&#25454;&#32467;&#26500;&#24314;&#27169;&#38480;&#21046;&#20102;&#28508;&#21147;&#12290;&#26412;&#25991;&#39318;&#27425;&#23581;&#35797;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#26032;&#27169;&#22411;&#65292;&#21363;&#26102;&#31354;&#22330;&#31070;&#32463;&#32593;&#32476;&#65292;&#21450;&#20854;&#23545;&#24212;&#30340;&#26032;&#26694;&#26550;&#65292;&#37329;&#23383;&#22612;&#25512;&#26029;&#65292;&#23558;&#20004;&#31181;&#19981;&#21516;&#30340;&#26102;&#31354;&#35266;&#28857;&#65292;&#22330;&#21644;&#22270;&#65292;&#30456;&#32467;&#21512;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#20013;&#22269;&#22823;&#38470;&#20840;&#22269;&#33539;&#22260;&#20869;&#30340;&#31354;&#27668;&#36136;&#37327;&#25512;&#26029;&#20013;&#23454;&#29616;&#20102;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02354v1 Announce Type: cross  Abstract: The air quality inference problem aims to utilize historical data from a limited number of observation sites to infer the air quality index at an unknown location. Considering the sparsity of data due to the high maintenance cost of the stations, good inference algorithms can effectively save the cost and refine the data granularity. While spatio-temporal graph neural networks have made excellent progress on this problem, their non-Euclidean and discrete data structure modeling of reality limits its potential. In this work, we make the first attempt to combine two different spatio-temporal perspectives, fields and graphs, by proposing a new model, Spatio-Temporal Field Neural Network, and its corresponding new framework, Pyramidal Inference. Extensive experiments validate that our model achieves state-of-the-art performance in nationwide air quality inference in the Chinese Mainland, demonstrating the superiority of our proposed model 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#21464;&#37327;&#22240;&#26524;&#25512;&#26029;&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#21435;&#20559;&#26041;&#38754;&#32423;&#24773;&#24863;&#20998;&#26512;&#65292;&#20174;&#32780;&#35299;&#20915;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#23398;&#20064;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.01166</link><description>&lt;p&gt;
DINER&#65306;&#20351;&#29992;&#22810;&#21464;&#37327;&#22240;&#26524;&#25512;&#26029;&#26469;&#21435;&#20559;&#26041;&#38754;&#32423;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
DINER: Debiasing Aspect-based Sentiment Analysis with Multi-variable Causal Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01166
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#21464;&#37327;&#22240;&#26524;&#25512;&#26029;&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#21435;&#20559;&#26041;&#38754;&#32423;&#24773;&#24863;&#20998;&#26512;&#65292;&#20174;&#32780;&#35299;&#20915;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#23398;&#20064;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#38754;&#32423;&#24773;&#24863;&#20998;&#26512;&#65288;ABSA&#65289;&#27169;&#22411;&#23481;&#26131;&#20174;&#27880;&#37322;&#20559;&#35265;&#20013;&#23398;&#20064;&#21040;&#34394;&#20551;&#30456;&#20851;&#24615;&#65292;&#23548;&#33268;&#22312;&#23545;&#25239;&#24615;&#25968;&#25454;&#36716;&#25442;&#19978;&#40065;&#26834;&#24615;&#36739;&#24046;&#12290;&#22312;&#21435;&#20559;&#35299;&#20915;&#26041;&#26696;&#20013;&#65292;&#22522;&#20110;&#22240;&#26524;&#25512;&#26029;&#30340;&#26041;&#27861;&#24341;&#36215;&#20102;&#35768;&#22810;&#30740;&#31350;&#20851;&#27880;&#65292;&#20027;&#35201;&#21487;&#20998;&#20026;&#22240;&#26524;&#24178;&#39044;&#26041;&#27861;&#21644;&#21453;&#20107;&#23454;&#25512;&#29702;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#22823;&#22810;&#25968;&#21435;&#20559;&#26041;&#27861;&#37117;&#38598;&#20013;&#22312;&#21333;&#21464;&#37327;&#22240;&#26524;&#25512;&#26029;&#19978;&#65292;&#36825;&#23545;&#20110;&#20855;&#26377;&#20004;&#20010;&#36755;&#20837;&#21464;&#37327;&#65288;&#30446;&#26631;&#26041;&#38754;&#21644;&#35780;&#35770;&#65289;&#30340;ABSA&#24182;&#19981;&#36866;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22810;&#21464;&#37327;&#22240;&#26524;&#25512;&#26029;&#30340;&#26032;&#26694;&#26550;&#29992;&#20110;&#21435;&#20559;ABSA&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#19981;&#21516;&#31867;&#22411;&#30340;&#20559;&#35265;&#22522;&#20110;&#19981;&#21516;&#30340;&#22240;&#26524;&#24178;&#39044;&#26041;&#27861;&#24471;&#21040;&#22788;&#29702;&#12290;&#23545;&#20110;&#35780;&#35770;&#20998;&#25903;&#65292;&#20559;&#35265;&#34987;&#24314;&#27169;&#20026;&#26469;&#33258;&#19978;&#19979;&#25991;&#30340;&#38388;&#25509;&#28151;&#26434;&#65292;&#20854;&#20013;&#23454;&#26045;&#21453;&#21521;&#35843;&#25972;&#24178;&#39044;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01166v1 Announce Type: cross  Abstract: Though notable progress has been made, neural-based aspect-based sentiment analysis (ABSA) models are prone to learn spurious correlations from annotation biases, resulting in poor robustness on adversarial data transformations. Among the debiasing solutions, causal inference-based methods have attracted much research attention, which can be mainly categorized into causal intervention methods and counterfactual reasoning methods. However, most of the present debiasing methods focus on single-variable causal inference, which is not suitable for ABSA with two input variables (the target aspect and the review). In this paper, we propose a novel framework based on multi-variable causal inference for debiasing ABSA. In this framework, different types of biases are tackled based on different causal intervention methods. For the review branch, the bias is modeled as indirect confounding from context, where backdoor adjustment intervention is 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#23558;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#20027;&#21160;&#23398;&#20064;&#21644;LoRA&#30456;&#32467;&#21512;&#65292;&#20197;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25968;&#25454;&#39640;&#25928;&#24494;&#35843;&#20013;&#36935;&#21040;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.01165</link><description>&lt;p&gt;
STAR: &#20351;&#29992;&#21160;&#24577;&#20027;&#21160;&#23398;&#20064;&#32422;&#26463;LoRA&#65292;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25968;&#25454;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
STAR: Constraint LoRA with Dynamic Active Learning for Data-Efficient Fine-Tuning of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01165
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#23558;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#20027;&#21160;&#23398;&#20064;&#21644;LoRA&#30456;&#32467;&#21512;&#65292;&#20197;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25968;&#25454;&#39640;&#25928;&#24494;&#35843;&#20013;&#36935;&#21040;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36890;&#36807;&#25552;&#31034;&#26041;&#27861;&#23637;&#31034;&#20102;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#24378;&#22823;&#33021;&#21147;&#65292;&#20294;&#23545;&#20110;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20173;&#38656;&#30417;&#30563;&#35757;&#32451;&#12290;&#38024;&#23545;LLMs&#30340;&#21442;&#25968;&#20247;&#22810;&#21644;&#20869;&#23384;&#28040;&#32791;&#22823;&#38382;&#39064;&#65292;&#20998;&#21035;&#25552;&#20986;&#20102;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;(PEFT)&#26041;&#27861;&#21644;&#20869;&#23384;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#39640;&#25928;&#24494;&#35843;&#26088;&#22312;&#35299;&#20915;&#22823;&#37327;&#27880;&#37322;&#25968;&#25454;&#28040;&#32791;&#30340;&#38382;&#39064;&#65292;&#21364;&#40092;&#26377;&#30740;&#31350;&#12290;&#19968;&#31181;&#26126;&#26174;&#30340;&#26041;&#24335;&#26159;&#23558;PEFT&#26041;&#27861;&#19982;&#20027;&#21160;&#23398;&#20064;&#30456;&#32467;&#21512;&#12290;&#28982;&#32780;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#31181;&#32452;&#21512;&#24182;&#38750;&#31616;&#21333;&#65292;&#24182;&#20135;&#29983;&#36739;&#24046;&#30340;&#32467;&#26524;&#12290;&#36890;&#36807;&#25506;&#38024;&#23454;&#39564;&#65292;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#21487;&#33021;&#30001;&#20004;&#20010;&#20027;&#35201;&#21407;&#22240;&#35299;&#37322;&#65306;&#19981;&#30830;&#23450;&#24615;&#24046;&#36317;&#21644;&#27169;&#22411;&#26657;&#20934;&#19981;&#20339;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#23558;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#20027;&#21160;&#23398;&#20064;&#21644;LoRA&#36827;&#34892;&#25972;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01165v1 Announce Type: cross  Abstract: Though Large Language Models (LLMs) have demonstrated the powerful capabilities of few-shot learning through prompting methods, supervised training is still necessary for complex reasoning tasks. Because of their extensive parameters and memory consumption, both Parameter-Efficient Fine-Tuning (PEFT) methods and Memory-Efficient Fine-Tuning methods have been proposed for LLMs. Nevertheless, the issue of large annotated data consumption, the aim of Data-Efficient Fine-Tuning, remains unexplored. One obvious way is to combine the PEFT method with active learning. However, the experimental results show that such a combination is not trivial and yields inferior results. Through probe experiments, such observation might be explained by two main reasons: uncertainty gap and poor model calibration. Therefore, in this paper, we propose a novel approach to effectively integrate uncertainty-based active learning and LoRA. Specifically, for the u
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#20840;&#38754;&#35780;&#20272;&#21518;&#35757;&#32451;&#37327;&#21270;&#23545;&#26435;&#37325;&#12289;&#28608;&#27963;&#21644;KV&#32531;&#23384;&#30340;&#24433;&#21709;&#65292;&#20197;&#25351;&#23548;&#36873;&#25321;&#37327;&#21270;&#26041;&#27861;&#65292;&#24182;&#23545;11&#31181;&#27169;&#22411;&#31995;&#21015;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#35206;&#30422;&#20102;&#22810;&#31181;&#20219;&#21153;&#31867;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.18158</link><description>&lt;p&gt;
&#35780;&#20272;&#37327;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Evaluating Quantized Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18158
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#20840;&#38754;&#35780;&#20272;&#21518;&#35757;&#32451;&#37327;&#21270;&#23545;&#26435;&#37325;&#12289;&#28608;&#27963;&#21644;KV&#32531;&#23384;&#30340;&#24433;&#21709;&#65292;&#20197;&#25351;&#23548;&#36873;&#25321;&#37327;&#21270;&#26041;&#27861;&#65292;&#24182;&#23545;11&#31181;&#27169;&#22411;&#31995;&#21015;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#35206;&#30422;&#20102;&#22810;&#31181;&#20219;&#21153;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#35757;&#32451;&#37327;&#21270;&#65288;PTQ&#65289;&#24050;&#32463;&#25104;&#20026;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25104;&#26412;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#25216;&#26415;&#65292;&#20855;&#20307;&#22320;&#65292;PTQ&#21487;&#20197;&#26377;&#25928;&#22320;&#20943;&#36731;LLMs&#20013;&#30340;&#20869;&#23384;&#28040;&#32791;&#24182;&#38477;&#20302;&#35745;&#31639;&#24320;&#38144;&#12290;&#20026;&#20102;&#28385;&#36275;&#21508;&#31181;&#22330;&#26223;&#19979;&#39640;&#25928;&#29575;&#21644;&#24615;&#33021;&#30340;&#35201;&#27714;&#65292;&#23545;&#37327;&#21270;LLMs&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#26159;&#24517;&#35201;&#30340;&#65292;&#20197;&#25351;&#23548;&#37327;&#21270;&#26041;&#27861;&#30340;&#36873;&#25321;&#12290;&#26412;&#25991;&#36890;&#36807;&#35780;&#20272;PTQ&#23545;11&#20010;&#27169;&#22411;&#31995;&#21015;&#65288;&#21253;&#25324;OPT&#12289;LLaMA2&#12289;Falcon&#12289;Bloomz&#12289;Mistral&#12289;ChatGLM&#12289;Vicuna&#12289;LongChat&#12289;StableLM&#12289;Gemma&#21644;Mamba&#65289;&#30340;&#26435;&#37325;&#12289;&#28608;&#27963;&#21644;KV&#32531;&#23384;&#30340;&#24433;&#21709;&#65292;&#33539;&#22260;&#20174;125M&#21040;180B&#65292;&#20840;&#38754;&#35780;&#20272;&#20102;&#36825;&#20123;&#22240;&#32032;&#12290;&#35780;&#20272;&#28085;&#30422;&#20102;&#20116;&#31181;&#31867;&#22411;&#30340;&#20219;&#21153;&#65306;&#22522;&#30784;NLP&#12289;&#31361;&#28982;&#20986;&#29616;&#30340;&#33021;&#21147;&#12289;&#21487;&#38752;&#24615;&#12289;&#23545;&#35805;&#21644;&#38271;&#19978;&#19979;&#25991;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#26368;&#20808;&#36827;&#30340;&#37327;&#21270;&#26041;&#27861;&#65292;&#20197;&#23637;&#31034;&#23427;&#20204;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18158v1 Announce Type: cross  Abstract: Post-training quantization (PTQ) has emerged as a promising technique to reduce the cost of large language models (LLMs). Specifically, PTQ can effectively mitigate memory consumption and reduce computational overhead in LLMs. To meet the requirements of both high efficiency and performance across diverse scenarios, a comprehensive evaluation of quantized LLMs is essential to guide the selection of quantization methods. This paper presents a thorough evaluation of these factors by evaluating the effect of PTQ on Weight, Activation, and KV Cache on 11 model families, including OPT, LLaMA2, Falcon, Bloomz, Mistral, ChatGLM, Vicuna, LongChat, StableLM, Gemma, and Mamba, with parameters ranging from 125M to 180B. The evaluation encompasses five types of tasks: basic NLP, emergent ability, trustworthiness, dialogue, and long-context tasks. Moreover, we also evaluate the state-of-the-art (SOTA) quantization methods to demonstrate their appli
&lt;/p&gt;</description></item><item><title>&#21464;&#20998;&#23398;&#20064;&#22312;&#22823;&#22411;&#28145;&#24230;&#32593;&#32476;&#20013;&#23637;&#29616;&#20986;&#38750;&#24120;&#22909;&#30340;&#25928;&#26524;&#65292;IVON&#20248;&#21270;&#22120;&#22312;&#35757;&#32451;&#22823;&#22411;&#32593;&#32476;&#26102;&#20960;&#20046;&#33021;&#19982;Adam&#30456;&#23218;&#32654;&#29978;&#33267;&#32988;&#36807;&#23427;&#65292;&#19988;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#26356;&#20934;&#30830;&#65292;&#23545;&#27169;&#22411;&#24494;&#35843;&#12289;&#27867;&#21270;&#35823;&#24046;&#39044;&#27979;&#21644;&#25968;&#25454;&#25935;&#24863;&#24615;&#20272;&#35745;&#22343;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.17641</link><description>&lt;p&gt;
&#21464;&#20998;&#23398;&#20064;&#23545;&#22823;&#22411;&#28145;&#24230;&#32593;&#32476;&#26377;&#25928;
&lt;/p&gt;
&lt;p&gt;
Variational Learning is Effective for Large Deep Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17641
&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#23398;&#20064;&#22312;&#22823;&#22411;&#28145;&#24230;&#32593;&#32476;&#20013;&#23637;&#29616;&#20986;&#38750;&#24120;&#22909;&#30340;&#25928;&#26524;&#65292;IVON&#20248;&#21270;&#22120;&#22312;&#35757;&#32451;&#22823;&#22411;&#32593;&#32476;&#26102;&#20960;&#20046;&#33021;&#19982;Adam&#30456;&#23218;&#32654;&#29978;&#33267;&#32988;&#36807;&#23427;&#65292;&#19988;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#26356;&#20934;&#30830;&#65292;&#23545;&#27169;&#22411;&#24494;&#35843;&#12289;&#27867;&#21270;&#35823;&#24046;&#39044;&#27979;&#21644;&#25968;&#25454;&#25935;&#24863;&#24615;&#20272;&#35745;&#22343;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;&#22823;&#37327;&#23454;&#35777;&#35777;&#25454;&#65292;&#21453;&#39539;&#20102;&#21464;&#20998;&#23398;&#20064;&#23545;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#26080;&#25928;&#30340;&#26222;&#36941;&#30475;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#21517;&#20026;Improved Variational Online Newton (IVON)&#30340;&#20248;&#21270;&#22120;&#65292;&#22312;&#35757;&#32451;&#22823;&#22411;&#32593;&#32476;&#65288;&#22914;GPT-2&#21644;ResNets&#65289;&#26102;&#22987;&#32456;&#33021;&#22815;&#19982;Adam&#30456;&#21305;&#37197;&#25110;&#32988;&#36807;&#23427;&#12290;IVON&#30340;&#35745;&#31639;&#25104;&#26412;&#20960;&#20046;&#19982;Adam&#30456;&#21516;&#65292;&#20294;&#20854;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#26356;&#22909;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;IVON&#30340;&#20960;&#31181;&#26032;&#29992;&#20363;&#65292;&#20854;&#20013;&#25105;&#20204;&#25913;&#36827;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#21644;&#27169;&#22411;&#21512;&#24182;&#65292;&#22312;&#20934;&#30830;&#39044;&#27979;&#27867;&#21270;&#35823;&#24046;&#21644;&#24544;&#23454;&#20272;&#35745;&#23545;&#25968;&#25454;&#30340;&#25935;&#24863;&#24615;&#26041;&#38754;&#12290;&#25105;&#20204;&#25214;&#21040;&#20102;&#22823;&#37327;&#25903;&#25345;&#21464;&#20998;&#23398;&#20064;&#26377;&#25928;&#24615;&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17641v1 Announce Type: cross  Abstract: We give extensive empirical evidence against the common belief that variational learning is ineffective for large neural networks. We show that an optimizer called Improved Variational Online Newton (IVON) consistently matches or outperforms Adam for training large networks such as GPT-2 and ResNets from scratch. IVON's computational costs are nearly identical to Adam but its predictive uncertainty is better. We show several new use cases of IVON where we improve fine-tuning and model merging in Large Language Models, accurately predict generalization error, and faithfully estimate sensitivity to data. We find overwhelming evidence in support of effectiveness of variational learning.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#65292;&#38024;&#23545;&#39135;&#35889;&#25991;&#26412;&#24320;&#21457;&#20102;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;&#65292;&#36890;&#36807;&#31995;&#32479;&#30340;&#25968;&#25454;&#22788;&#29702;&#21644;&#20998;&#26512;&#65292;&#26500;&#24314;&#20102;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#26032;&#39135;&#35889;&#30340;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2402.17447</link><description>&lt;p&gt;
&#39135;&#35889;&#30340;&#28145;&#24230;&#23398;&#20064;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Deep Learning Based Named Entity Recognition Models for Recipes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17447
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#65292;&#38024;&#23545;&#39135;&#35889;&#25991;&#26412;&#24320;&#21457;&#20102;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;&#65292;&#36890;&#36807;&#31995;&#32479;&#30340;&#25968;&#25454;&#22788;&#29702;&#21644;&#20998;&#26512;&#65292;&#26500;&#24314;&#20102;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#26032;&#39135;&#35889;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39135;&#29289;&#36890;&#36807;&#21508;&#31181;&#21162;&#21147;&#26041;&#24335;&#24433;&#21709;&#30528;&#25105;&#20204;&#30340;&#29983;&#27963;&#65292;&#21253;&#25324;&#21475;&#21619;&#12289;&#33829;&#20859;&#12289;&#20581;&#24247;&#21644;&#21487;&#25345;&#32493;&#24615;&#12290;&#39135;&#35889;&#26159;&#36890;&#36807;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20195;&#20195;&#30456;&#20256;&#30340;&#25991;&#21270;&#33014;&#22218;&#12290;&#33258;&#21160;&#35782;&#21035;&#21629;&#21517;&#23454;&#20307;&#30340;&#21327;&#35758;&#65292;&#21363;&#39135;&#35889;&#25991;&#26412;&#30340;&#22522;&#26412;&#32452;&#25104;&#37096;&#20998;&#65292;&#23545;&#20110;&#21508;&#31181;&#24212;&#29992;&#26469;&#35828;&#37117;&#20855;&#26377;&#24040;&#22823;&#20215;&#20540;&#65292;&#20174;&#20449;&#24687;&#25552;&#21462;&#21040;&#26032;&#39062;&#39135;&#35889;&#29983;&#25104;&#12290;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26159;&#19968;&#31181;&#20174;&#24050;&#30693;&#26631;&#31614;&#30340;&#38750;&#32467;&#26500;&#21270;&#25110;&#21322;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#25552;&#21462;&#20449;&#24687;&#30340;&#25216;&#26415;&#12290;&#25105;&#20204;&#20174;&#25163;&#21160;&#27880;&#37322;&#30340;6,611&#20010;&#25104;&#20998;&#30701;&#35821;&#30340;&#25968;&#25454;&#24320;&#22987;&#65292;&#32047;&#31215;&#21019;&#24314;&#20102;26,445&#20010;&#30701;&#35821;&#30340;&#22686;&#24378;&#25968;&#25454;&#38598;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#28165;&#29702;&#21644;&#20998;&#26512;&#20102;&#26469;&#33258;RecipeDB&#30340;&#25104;&#20998;&#30701;&#35821;&#65292;&#36825;&#26159;&#40644;&#37329;&#26631;&#20934;&#30340;&#39135;&#35889;&#25968;&#25454;&#23384;&#20648;&#24211;&#65292;&#24182;&#20351;&#29992;Stanford NER&#36827;&#34892;&#20102;&#26631;&#27880;&#12290;&#22522;&#20110;&#20998;&#26512;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#32858;&#31867;&#30340;&#26041;&#27861;&#23545;88,526&#20010;&#30701;&#35821;&#30340;&#23376;&#38598;&#36827;&#34892;&#20102;&#21462;&#26679;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17447v1 Announce Type: cross  Abstract: Food touches our lives through various endeavors, including flavor, nourishment, health, and sustainability. Recipes are cultural capsules transmitted across generations via unstructured text. Automated protocols for recognizing named entities, the building blocks of recipe text, are of immense value for various applications ranging from information extraction to novel recipe generation. Named entity recognition is a technique for extracting information from unstructured or semi-structured data with known labels. Starting with manually-annotated data of 6,611 ingredient phrases, we created an augmented dataset of 26,445 phrases cumulatively. Simultaneously, we systematically cleaned and analyzed ingredient phrases from RecipeDB, the gold-standard recipe data repository, and annotated them using the Stanford NER. Based on the analysis, we sampled a subset of 88,526 phrases using a clustering-based approach while preserving the diversity
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#26500;&#21270;&#35780;&#20272;&#26694;&#26550;&#65292;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#37327;&#21270;&#31574;&#30053;&#23637;&#24320;&#20840;&#38754;&#35780;&#20272;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#30740;&#31350;&#20013;&#30340;&#31354;&#30333;&#12290;</title><link>https://arxiv.org/abs/2402.16775</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#37327;&#21270;&#31574;&#30053;&#30340;&#20840;&#38754;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Evaluation of Quantization Strategies for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16775
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#26500;&#21270;&#35780;&#20272;&#26694;&#26550;&#65292;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#37327;&#21270;&#31574;&#30053;&#23637;&#24320;&#20840;&#38754;&#35780;&#20272;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#30740;&#31350;&#20013;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22686;&#21152;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#21442;&#25968;&#25968;&#37327;&#36890;&#24120;&#20250;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#25552;&#39640;&#24615;&#33021;&#65292;&#20294;&#20250;&#22686;&#21152;&#35745;&#31639;&#21644;&#23384;&#20648;&#25104;&#26412;&#65292;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#37096;&#32626;&#21464;&#24471;&#22256;&#38590;&#12290;&#37327;&#21270;&#25216;&#26415;&#36890;&#36807;&#20943;&#23569;&#27169;&#22411;&#26435;&#37325;&#25110;&#28608;&#27963;&#25152;&#38656;&#30340;&#20301;&#25968;&#65292;&#24182;&#26368;&#23567;&#21270;&#24615;&#33021;&#25439;&#22833;&#65292;&#24050;&#32463;&#22240;LLMs&#30340;&#20852;&#36215;&#32780;&#21464;&#24471;&#27969;&#34892;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#37327;&#21270;&#30740;&#31350;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;LLMs&#65292;&#37327;&#21270;&#23545;&#35843;&#25972;&#36807;&#25351;&#20196;&#30340;LLMs&#30340;&#24433;&#21709;&#20197;&#21450;&#37327;&#21270;LLMs&#30340;&#22256;&#24785;&#24230;&#19982;&#22522;&#20934;&#24615;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#23578;&#19981;&#26126;&#30830;&#12290;&#23545;&#37327;&#21270;LLMs&#30340;&#35780;&#20272;&#36890;&#24120;&#20165;&#38480;&#20110;&#35821;&#35328;&#24314;&#27169;&#21644;&#23569;&#25968;&#20998;&#31867;&#20219;&#21153;&#65292;&#20854;&#22312;&#20854;&#20182;&#22522;&#20934;&#19978;&#30340;&#24615;&#33021;&#23578;&#19981;&#28165;&#26970;&#12290;&#20026;&#22635;&#34917;&#36825;&#20123;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#26500;&#21270;&#35780;&#20272;&#26694;&#26550;&#65292;&#21253;&#25324;&#19977;&#20010;&#20851;&#38190;&#32500;&#24230;&#65306;&#65288;1&#65289;&#30693;&#35782;&#21644;&#23481;&#37327;&#65292;&#65288;2&#65289;&#23545;&#40784;&#24615;&#21644;&#65288;3&#65289;&#25928;&#29575;&#65292;&#24182;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16775v1 Announce Type: new  Abstract: Increasing the number of parameters in large language models (LLMs) usually improves performance in downstream tasks but raises compute and memory costs, making deployment difficult in resource-limited settings. Quantization techniques, which reduce the bits needed for model weights or activations with minimal performance loss, have become popular due to the rise of LLMs. However, most quantization studies use pre-trained LLMs, and the impact of quantization on instruction-tuned LLMs and the relationship between perplexity and benchmark performance of quantized LLMs are not well understood. Evaluation of quantized LLMs is often limited to language modeling and a few classification tasks, leaving their performance on other benchmarks unclear. To address these gaps, we propose a structured evaluation framework consisting of three critical dimensions: (1) knowledge \&amp; capacity, (2) alignment, and (3) efficiency, and conduct extensive experi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#20110;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#30340;&#20195;&#25968;&#29702;&#35770;&#65292;&#24212;&#29992;&#33539;&#30068;&#35770;&#26500;&#24314;&#20102;&#19968;&#20010;&#26725;&#26753;&#65292;&#26377;&#25928;&#22320;&#28085;&#30422;&#20102;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#30340;&#19981;&#21516;&#39118;&#26684;&#65292;&#21516;&#26102;&#33258;&#28982;&#22320;&#32534;&#30721;&#20102;&#35745;&#31639;&#26426;&#31185;&#23398;&#21644;&#33258;&#21160;&#26426;&#29702;&#35770;&#20013;&#30340;&#35768;&#22810;&#26631;&#20934;&#32467;&#26500;&#12290;</title><link>https://arxiv.org/abs/2402.15332</link><description>&lt;p&gt;
&#20998;&#31867;&#28145;&#24230;&#23398;&#20064;&#65306;&#19968;&#31181;&#20851;&#20110;&#26550;&#26500;&#30340;&#20195;&#25968;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Categorical Deep Learning: An Algebraic Theory of Architectures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15332
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#20110;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#30340;&#20195;&#25968;&#29702;&#35770;&#65292;&#24212;&#29992;&#33539;&#30068;&#35770;&#26500;&#24314;&#20102;&#19968;&#20010;&#26725;&#26753;&#65292;&#26377;&#25928;&#22320;&#28085;&#30422;&#20102;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#30340;&#19981;&#21516;&#39118;&#26684;&#65292;&#21516;&#26102;&#33258;&#28982;&#22320;&#32534;&#30721;&#20102;&#35745;&#31639;&#26426;&#31185;&#23398;&#21644;&#33258;&#21160;&#26426;&#29702;&#35770;&#20013;&#30340;&#35768;&#22810;&#26631;&#20934;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#25351;&#23450;&#21644;&#30740;&#31350;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#30340;&#36890;&#29992;&#26694;&#26550;&#30340;&#31435;&#22330;&#12290;&#25105;&#20204;&#35748;&#20026;&#21040;&#30446;&#21069;&#20026;&#27490;&#20851;&#20110;&#36825;&#19968;&#39046;&#22495;&#30340;&#20851;&#38190;&#23581;&#35797;&#32570;&#20047;&#19968;&#31181;&#19968;&#33268;&#30340;&#26725;&#26753;&#65292;&#33021;&#22815;&#25351;&#23450;&#27169;&#22411;&#24517;&#39035;&#28385;&#36275;&#30340;&#32422;&#26463;&#24182;&#35268;&#23450;&#23427;&#20204;&#30340;&#23454;&#29616;&#26041;&#24335;&#12290;&#19987;&#27880;&#20110;&#26500;&#24314;&#36825;&#26679;&#19968;&#20010;&#26725;&#26753;&#65292;&#25105;&#20204;&#24314;&#35758;&#24212;&#29992;&#33539;&#30068;&#35770;&#8212;&#8212;&#20934;&#30830;&#22320;&#35828;&#65292;&#21333;&#23376;&#20540;&#20110;&#21442;&#25968;&#26144;&#23556;&#30340;&#20108;&#33539;&#30068;&#30340;&#36890;&#29992;&#20195;&#25968;&#8212;&#8212;&#20316;&#20026;&#19968;&#31181;&#21333;&#19968;&#29702;&#35770;&#65292;&#20248;&#38597;&#22320;&#21253;&#21547;&#20102;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#30340;&#36825;&#20004;&#31181;&#39118;&#26684;&#12290;&#20026;&#20102;&#25903;&#25345;&#25105;&#20204;&#30340;&#35266;&#28857;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#19968;&#29702;&#35770;&#22914;&#20309;&#24674;&#22797;&#30001;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#23548;&#33268;&#30340;&#32422;&#26463;&#65292;&#20197;&#21450;&#20174;&#31070;&#32463;&#32593;&#32476;&#19981;&#21516;&#39046;&#22495;&#30340;&#22810;&#31181;&#26550;&#26500;&#65288;&#22914;RNNs&#65289;&#30340;&#23454;&#29616;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#36825;&#19968;&#29702;&#35770;&#22914;&#20309;&#33258;&#28982;&#22320;&#32534;&#30721;&#20102;&#35745;&#31639;&#26426;&#31185;&#23398;&#21644;&#33258;&#21160;&#26426;&#29702;&#35770;&#20013;&#30340;&#35768;&#22810;&#26631;&#20934;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15332v1 Announce Type: cross  Abstract: We present our position on the elusive quest for a general-purpose framework for specifying and studying deep learning architectures. Our opinion is that the key attempts made so far lack a coherent bridge between specifying constraints which models must satisfy and specifying their implementations. Focusing on building a such a bridge, we propose to apply category theory -- precisely, the universal algebra of monads valued in a 2-category of parametric maps -- as a single theory elegantly subsuming both of these flavours of neural network design. To defend our position, we show how this theory recovers constraints induced by geometric deep learning, as well as implementations of many architectures drawn from the diverse landscape of neural networks, such as RNNs. We also illustrate how the theory naturally encodes many standard constructs in computer science and automata theory.
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#21487;&#21464;&#25442;&#39640;&#26031;&#22870;&#21169;&#20989;&#25968;(TGRF)&#26469;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#31038;&#20132;&#24863;&#30693;&#23548;&#33322;&#22870;&#21169;&#35774;&#35745;&#22797;&#26434;&#12289;&#36229;&#21442;&#25968;&#20887;&#20313;&#21644;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2402.14569</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#31038;&#20132;&#24863;&#30693;&#23548;&#33322;&#30340;&#21487;&#21464;&#25442;&#39640;&#26031;&#22870;&#21169;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Transformable Gaussian Reward Function for Socially-Aware Navigation with Deep Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14569
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#21487;&#21464;&#25442;&#39640;&#26031;&#22870;&#21169;&#20989;&#25968;(TGRF)&#26469;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#31038;&#20132;&#24863;&#30693;&#23548;&#33322;&#22870;&#21169;&#35774;&#35745;&#22797;&#26434;&#12289;&#36229;&#21442;&#25968;&#20887;&#20313;&#21644;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#23548;&#33322;&#24050;&#32463;&#20174;&#20248;&#20808;&#32771;&#34385;&#36991;&#38556;&#36716;&#21464;&#20026;&#37319;&#29992;&#31038;&#20132;&#24863;&#30693;&#23548;&#33322;&#31574;&#30053;&#26469;&#36866;&#24212;&#20154;&#31867;&#23384;&#22312;&#12290;&#22240;&#27492;&#65292;&#22312;&#21160;&#24577;&#20154;&#31867;&#20013;&#24515;&#29615;&#22659;&#20013;&#30340;&#31038;&#20132;&#24863;&#30693;&#23548;&#33322;&#30340;&#35748;&#30693;&#22312;&#26426;&#22120;&#20154;&#39046;&#22495;&#20013;&#21464;&#24471;&#37325;&#35201;&#12290;&#34429;&#28982;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#25512;&#21160;&#20102;&#31038;&#20132;&#24863;&#30693;&#23548;&#33322;&#30340;&#36827;&#27493;&#65292;&#20294;&#22312;&#25317;&#25380;&#29615;&#22659;&#20013;&#23450;&#20041;&#36866;&#24403;&#30340;&#22870;&#21169;&#20989;&#25968;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#36825;&#20123;&#22870;&#21169;&#22312;&#24341;&#23548;&#26426;&#22120;&#20154;&#34892;&#20026;&#26102;&#33267;&#20851;&#37325;&#35201;&#65292;&#30001;&#20110;&#20854;&#22797;&#26434;&#24615;&#21644;&#26080;&#27861;&#33258;&#21160;&#35774;&#32622;&#30340;&#29305;&#24615;&#65292;&#38656;&#35201;&#31934;&#24515;&#35774;&#35745;&#12290;&#22823;&#37327;&#25163;&#24037;&#35774;&#35745;&#30340;&#22870;&#21169;&#24102;&#26469;&#20102;&#36229;&#21442;&#25968;&#20887;&#20313;&#12289;&#19981;&#24179;&#34913;&#20197;&#21450;&#26080;&#27861;&#20805;&#20998;&#34920;&#31034;&#29420;&#29305;&#23545;&#35937;&#29305;&#24449;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21487;&#21464;&#25442;&#39640;&#26031;&#22870;&#21169;&#20989;&#25968;&#65288;TGRF&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14569v1 Announce Type: cross  Abstract: Robot navigation has transitioned from prioritizing obstacle avoidance to adopting socially aware navigation strategies that accommodate human presence. As a result, the recognition of socially aware navigation within dynamic human-centric environments has gained prominence in the field of robotics. Although reinforcement learning technique has fostered the advancement of socially aware navigation, defining appropriate reward functions, especially in congested environments, has posed a significant challenge. These rewards, crucial in guiding robot actions, demand intricate human-crafted design due to their complex nature and inability to be automatically set. The multitude of manually designed rewards poses issues with hyperparameter redundancy, imbalance, and inadequate representation of unique object characteristics. To address these challenges, we introduce a transformable gaussian reward function (TGRF). The TGRF significantly redu
&lt;/p&gt;</description></item><item><title>FanOutQA &#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#22810;&#36339;&#12289;&#22810;&#25991;&#26723;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#26434;&#25512;&#29702;&#33021;&#21147;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;&#24403;&#20195;&#27169;&#22411;&#22312;&#38271;&#31687;&#19978;&#19979;&#25991;&#20013;&#20173;&#26377;&#25913;&#36827;&#20132;&#21449;&#25991;&#26723;&#20381;&#36182;&#25512;&#29702;&#30340;&#31354;&#38388;&#12290;</title><link>https://arxiv.org/abs/2402.14116</link><description>&lt;p&gt;
FanOutQA&#65306;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#36339;&#12289;&#22810;&#25991;&#26723;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
FanOutQA: Multi-Hop, Multi-Document Question Answering for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14116
&lt;/p&gt;
&lt;p&gt;
FanOutQA &#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#22810;&#36339;&#12289;&#22810;&#25991;&#26723;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#26434;&#25512;&#29702;&#33021;&#21147;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;&#24403;&#20195;&#27169;&#22411;&#22312;&#38271;&#31687;&#19978;&#19979;&#25991;&#20013;&#20173;&#26377;&#25913;&#36827;&#20132;&#21449;&#25991;&#26723;&#20381;&#36182;&#25512;&#29702;&#30340;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#24120;&#35265;&#20110;&#26085;&#24120;&#22330;&#26223;&#20013;&#30340;&#38382;&#39064;&#31867;&#22411;&#26159;&#8220;fan-out&#8221;&#38382;&#39064;&#65292;&#21363;&#22797;&#26434;&#30340;&#22810;&#36339;&#12289;&#22810;&#25991;&#26723;&#25512;&#29702;&#38382;&#39064;&#65292;&#38656;&#35201;&#25214;&#21040;&#22823;&#37327;&#23454;&#20307;&#30340;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#24456;&#23569;&#26377;&#36164;&#28304;&#21487;&#20197;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#31181;&#38382;&#39064;&#22238;&#31572;&#33021;&#21147;&#19978;&#30340;&#34920;&#29616;&#12290;&#20026;&#20102;&#26356;&#20840;&#38754;&#22320;&#35780;&#20272;LLMs&#20013;&#30340;&#22797;&#26434;&#25512;&#29702;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FanOutQA&#65292;&#36825;&#26159;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;fan-out&#38382;&#39064;-&#31572;&#26696;&#23545;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#33521;&#25991;&#32500;&#22522;&#30334;&#31185;&#20316;&#20026;&#30693;&#35782;&#24211;&#30340;&#20154;&#24037;&#27880;&#37322;&#20998;&#35299;&#12290;&#25105;&#20204;&#22312;&#25968;&#25454;&#38598;&#19978;&#21046;&#23450;&#20102;&#19977;&#31181;&#22522;&#20934;&#35774;&#32622;&#65292;&#24182;&#23545;7&#20010;LLMs&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;GPT-4&#12289;LLaMA 2&#12289;Claude-2.1&#21644;Mixtral-8x7B&#65292;&#21457;&#29616;&#24403;&#20195;&#27169;&#22411;&#22312;&#38271;&#31687;&#19978;&#19979;&#25991;&#20013;&#20173;&#26377;&#25913;&#36827;&#25512;&#29702;&#36328;&#25991;&#26723;&#20381;&#36182;&#30340;&#31354;&#38388;&#12290;&#25105;&#20204;&#25552;&#20379;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21644;&#24320;&#28304;&#24037;&#20855;&#26469;&#36816;&#34892;&#27169;&#22411;&#65292;&#20197;&#40723;&#21169;&#22312;https://fanoutqa.com&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14116v1 Announce Type: cross  Abstract: One type of question that is commonly found in day-to-day scenarios is ``fan-out'' questions, complex multi-hop, multi-document reasoning questions that require finding information about a large number of entities. However, there exist few resources to evaluate this type of question-answering capability among large language models. To evaluate complex reasoning in LLMs more fully, we present FanOutQA, a high-quality dataset of fan-out question-answer pairs and human-annotated decompositions with English Wikipedia as the knowledge base. We formulate three benchmark settings across our dataset and benchmark 7 LLMs, including GPT-4, LLaMA 2, Claude-2.1, and Mixtral-8x7B, finding that contemporary models still have room to improve reasoning over inter-document dependencies in a long context. We provide our dataset and open-source tools to run models to encourage evaluation at https://fanoutqa.com
&lt;/p&gt;</description></item><item><title>Soft Self-Consistency (Soft-SC)&#36890;&#36807;&#36719;&#21270;&#35780;&#20998;&#26631;&#20934;&#26367;&#20195;&#33258;&#19968;&#33268;&#24615;&#30340;&#22810;&#25968;&#25237;&#31080;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#22312;&#28041;&#21450;&#29983;&#25104;&#22810;&#20010;&#21160;&#20316;&#30340;&#38271;&#26399;&#20114;&#21160;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#21644;&#25928;&#29575;</title><link>https://arxiv.org/abs/2402.13212</link><description>&lt;p&gt;
&#36719;&#33258;&#19968;&#33268;&#24615;&#25913;&#21892;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Soft Self-Consistency Improves Language Model Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13212
&lt;/p&gt;
&lt;p&gt;
Soft Self-Consistency (Soft-SC)&#36890;&#36807;&#36719;&#21270;&#35780;&#20998;&#26631;&#20934;&#26367;&#20195;&#33258;&#19968;&#33268;&#24615;&#30340;&#22810;&#25968;&#25237;&#31080;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#22312;&#28041;&#21450;&#29983;&#25104;&#22810;&#20010;&#21160;&#20316;&#30340;&#38271;&#26399;&#20114;&#21160;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#21644;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#21487;&#20197;&#36890;&#36807;&#23545;&#22810;&#20010;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#25277;&#26679;&#21644;&#35780;&#20998;&#26469;&#25913;&#36827;&#65292;&#20197;&#36873;&#25321;&#26368;&#32456;&#31572;&#26696;&#12290;&#24403;&#21069;&#30340;&#8220;&#25277;&#26679;&#21644;&#36873;&#25321;&#8221;&#26041;&#27861;&#22914;&#33258;&#19968;&#33268;&#24615;&#65288;SC&#65289;&#20381;&#36182;&#20110;&#22810;&#25968;&#25237;&#31080;&#26469;&#35780;&#20998;&#31572;&#26696;&#12290;&#28982;&#32780;&#65292;&#24403;&#20219;&#21153;&#26377;&#35768;&#22810;&#19981;&#21516;&#19988;&#26377;&#25928;&#30340;&#31572;&#26696;&#26102;&#65292;&#36890;&#36807;&#25237;&#31080;&#36827;&#34892;&#36873;&#25321;&#38656;&#35201;&#22823;&#37327;&#26679;&#26412;&#12290;&#36825;&#20351;&#24471;SC&#22312;&#28041;&#21450;&#39034;&#24207;&#29983;&#25104;&#22810;&#20010;&#21160;&#20316;&#65288;&#31572;&#26696;&#65289;&#30340;&#20114;&#21160;&#20219;&#21153;&#26102;&#25104;&#26412;&#36807;&#39640;&#12290;&#22312;&#30830;&#23450;&#22823;&#22810;&#25968;&#25237;&#31080;&#26410;&#33021;&#20026;&#27492;&#31867;&#20219;&#21153;&#25552;&#20379;&#19968;&#33268;&#30340;&#25910;&#30410;&#20043;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#36719;&#21270;&#35780;&#20998;&#26631;&#20934;&#26469;&#25552;&#39640;&#25104;&#21151;&#29575;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#36719;&#33258;&#19968;&#33268;&#24615;&#65288;Soft-SC&#65289;&#65292;&#23427;&#29992;&#27169;&#22411;&#21487;&#33021;&#24615;&#35745;&#31639;&#36830;&#32493;&#20998;&#25968;&#26469;&#21462;&#20195;SC&#30340;&#19981;&#36830;&#32493;&#35780;&#20998;&#65292;&#21363;&#20351;&#21160;&#20316;&#20998;&#24067;&#31232;&#30095;&#65292;&#20063;&#20801;&#35768;&#36873;&#25321;&#12290;&#36719;&#33258;&#19968;&#33268;&#24615;&#22312;&#38271;&#26399;&#20114;&#21160;&#20219;&#21153;&#19978;&#25552;&#39640;&#20102;&#24615;&#33021;&#21644;&#25928;&#29575;&#65292;&#38656;&#35201;&#36739;&#23569;&#30340;&#26679;&#26412;&#21644;&#25237;&#31080;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13212v1 Announce Type: cross  Abstract: Generations from large language models (LLMs) can be improved by sampling and scoring multiple solutions to select a final answer. Current "sample and select" methods such as self-consistency (SC) rely on majority voting to score answers. However, when tasks have many distinct and valid answers, selection by voting requires a large number of samples. This makes SC prohibitively expensive for interactive tasks that involve generating multiple actions (answers) sequentially. After establishing that majority voting fails to provide consistent gains on such tasks, we demonstrate how to increase success rates by softening the scoring criterion. We introduce Soft Self-Consistency (Soft-SC), which replaces SC's discontinuous scoring with a continuous score computed from model likelihoods, allowing for selection even when actions are sparsely distributed. Soft-SC improves both performance and efficiency on long-horizon interactive tasks, requi
&lt;/p&gt;</description></item><item><title>TRAP&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Targeted Random Adversarial Prompt (TRAP)&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#29305;&#23450;LLM&#30340;&#20351;&#29992;&#65292;&#24182;&#22312;&#21333;&#27425;&#20132;&#20114;&#21518;&#20197;&#36229;&#36807;95%&#30340;&#30495;&#38451;&#24615;&#29575;&#21644;&#20302;&#20110;0.2%&#30340;&#35823;&#25253;&#29575;&#26816;&#27979;&#30446;&#26631;LLMs&#12290;</title><link>https://arxiv.org/abs/2402.12991</link><description>&lt;p&gt;
TRAP: &#38754;&#21521;&#40657;&#30418;&#36523;&#20221;&#39564;&#35777;&#30340;&#26377;&#38024;&#23545;&#24615;&#38543;&#26426;&#23545;&#25239;&#25552;&#31034;&#35825;&#39285;
&lt;/p&gt;
&lt;p&gt;
TRAP: Targeted Random Adversarial Prompt Honeypot for Black-Box Identification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12991
&lt;/p&gt;
&lt;p&gt;
TRAP&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Targeted Random Adversarial Prompt (TRAP)&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#29305;&#23450;LLM&#30340;&#20351;&#29992;&#65292;&#24182;&#22312;&#21333;&#27425;&#20132;&#20114;&#21518;&#20197;&#36229;&#36807;95%&#30340;&#30495;&#38451;&#24615;&#29575;&#21644;&#20302;&#20110;0.2%&#30340;&#35823;&#25253;&#29575;&#26816;&#27979;&#30446;&#26631;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26381;&#21153;&#21644;&#27169;&#22411;&#36890;&#24120;&#20276;&#38543;&#30528;&#20851;&#20110;&#35841;&#21487;&#20197;&#20351;&#29992;&#23427;&#20204;&#20197;&#21450;&#20182;&#20204;&#24517;&#39035;&#22914;&#20309;&#20351;&#29992;&#23427;&#20204;&#30340;&#27861;&#24459;&#35268;&#23450;&#12290;&#35780;&#20272;&#21457;&#24067;&#30340;LLMs&#30340;&#21512;&#35268;&#24615;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#22240;&#20026;&#36825;&#20123;&#35268;&#23450;&#20445;&#25252;&#20102;LLM&#36129;&#29486;&#32773;&#30340;&#21033;&#30410;&#24182;&#38450;&#27490;&#20102;&#28389;&#29992;&#12290;&#22312;&#36825;&#31181;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#40657;&#30418;&#36523;&#20221;&#39564;&#35777;&#65288;BBIV&#65289;&#30340;&#26032;&#38382;&#39064;&#12290;&#20854;&#30446;&#26631;&#26159;&#30830;&#23450;&#31532;&#19977;&#26041;&#24212;&#29992;&#26159;&#21542;&#36890;&#36807;&#20854;&#32842;&#22825;&#21151;&#33021;&#20351;&#29992;&#26576;&#20010;&#29305;&#23450;&#30340;LLM&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#30446;&#26631;&#38543;&#26426;&#23545;&#25239;&#25552;&#31034;&#65288;TRAP&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#27491;&#22312;&#20351;&#29992;&#30340;&#20855;&#20307;LLM&#12290;&#25105;&#20204;&#37325;&#26032;&#21033;&#29992;&#20102;&#26368;&#21021;&#29992;&#20110;&#36234;&#29425;&#30340;&#23545;&#25239;&#24615;&#21518;&#32512;&#65292;&#20197;&#20174;&#30446;&#26631;LLM&#33719;&#24471;&#39044;&#23450;&#20041;&#30340;&#31572;&#26696;&#65292;&#32780;&#20854;&#20182;&#27169;&#22411;&#21017;&#32473;&#20986;&#38543;&#26426;&#31572;&#26696;&#12290;TRAP&#21487;&#20197;&#22312;&#21333;&#27425;&#20132;&#20114;&#21518;&#20197;&#36229;&#36807;95%&#30340;&#30495;&#38451;&#24615;&#29575;&#21644;&#20302;&#20110;0.2%&#30340;&#35823;&#25253;&#29575;&#26816;&#27979;&#30446;&#26631;LLMs&#12290;&#21363;&#20351;LLM&#26377;&#19981;&#20250;&#26174;&#33879;&#25913;&#21464;&#30340;&#32454;&#24494;&#21464;&#21270;&#65292;TRAP&#20173;&#28982;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12991v1 Announce Type: cross  Abstract: Large Language Model (LLM) services and models often come with legal rules on who can use them and how they must use them. Assessing the compliance of the released LLMs is crucial, as these rules protect the interests of the LLM contributor and prevent misuse. In this context, we describe the novel problem of Black-box Identity Verification (BBIV). The goal is to determine whether a third-party application uses a certain LLM through its chat function. We propose a method called Targeted Random Adversarial Prompt (TRAP) that identifies the specific LLM in use. We repurpose adversarial suffixes, originally proposed for jailbreaking, to get a pre-defined answer from the target LLM, while other models give random answers. TRAP detects the target LLMs with over 95% true positive rate at under 0.2% false positive rate even after a single interaction. TRAP remains effective even if the LLM has minor changes that do not significantly alter the
&lt;/p&gt;</description></item><item><title>&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26080;&#32541;&#25972;&#21512;&#35270;&#35273;&#21644;&#25991;&#26412;&#27169;&#24577;&#65292;&#20026;&#29983;&#25104;&#26234;&#33021;&#25552;&#20379;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.12451</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38761;&#21629;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
The (R)Evolution of Multimodal Large Language Models: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12451
&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26080;&#32541;&#25972;&#21512;&#35270;&#35273;&#21644;&#25991;&#26412;&#27169;&#24577;&#65292;&#20026;&#29983;&#25104;&#26234;&#33021;&#25552;&#20379;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#25509;&#25991;&#26412;&#21644;&#35270;&#35273;&#27169;&#24577;&#22312;&#29983;&#25104;&#26234;&#33021;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#12290;&#21463;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#30446;&#21069;&#24050;&#26377;&#22823;&#37327;&#30740;&#31350;&#24037;&#20316;&#33268;&#21147;&#20110;&#24320;&#21457;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#12290;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#26080;&#32541;&#22320;&#25972;&#21512;&#35270;&#35273;&#21644;&#25991;&#26412;&#27169;&#24577;&#65292;&#21516;&#26102;&#20316;&#20026;&#36755;&#20837;&#21644;&#36755;&#20986;&#65292;&#25552;&#20379;&#22522;&#20110;&#23545;&#35805;&#30340;&#25509;&#21475;&#21644;&#36981;&#24490;&#25351;&#20196;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#20840;&#38754;&#23457;&#26597;&#20102;&#26368;&#36817;&#22522;&#20110;&#35270;&#35273;&#30340;MLLMs&#65292;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#26550;&#26500;&#36873;&#25321;&#12289;&#22810;&#27169;&#24577;&#23545;&#40784;&#31574;&#30053;&#21644;&#35757;&#32451;&#25216;&#26415;&#12290;&#25105;&#20204;&#36824;&#23545;&#36825;&#20123;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65292;&#21253;&#25324;&#35270;&#35273;&#23450;&#20301;&#12289;&#22270;&#20687;&#29983;&#25104;&#21644;&#32534;&#36753;&#12289;&#35270;&#35273;&#29702;&#35299;&#20197;&#21450;&#29305;&#23450;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#32534;&#21046;&#24182;&#25551;&#36848;&#20102;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#22522;&#20934;&#65292;&#23545;&#29616;&#26377;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12451v1 Announce Type: cross  Abstract: Connecting text and visual modalities plays an essential role in generative intelligence. For this reason, inspired by the success of large language models, significant research efforts are being devoted to the development of Multimodal Large Language Models (MLLMs). These models can seamlessly integrate visual and textual modalities, both as input and output, while providing a dialogue-based interface and instruction-following capabilities. In this paper, we provide a comprehensive review of recent visual-based MLLMs, analyzing their architectural choices, multimodal alignment strategies, and training techniques. We also conduct a detailed analysis of these models across a wide range of tasks, including visual grounding, image generation and editing, visual understanding, and domain-specific applications. Additionally, we compile and describe training datasets and evaluation benchmarks, conducting comparisons among existing models in 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;LLM&#22312;&#35299;&#37322;&#34920;&#26684;&#25968;&#25454;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#27604;&#36739;&#20102;&#25991;&#26412;&#21644;&#22270;&#20687;&#34920;&#26684;&#34920;&#31034;&#23545;LLM&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#20026;&#22312;&#34920;&#26684;&#30456;&#20851;&#20219;&#21153;&#19978;&#26377;&#25928;&#20351;&#29992;LLM&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.12424</link><description>&lt;p&gt;
&#34920;&#26684;&#20316;&#20026;&#22270;&#29255;&#65311;&#25506;&#35752;LLM&#22312;&#22810;&#27169;&#24577;&#34920;&#26684;&#25968;&#25454;&#34920;&#31034;&#19978;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;
&lt;/p&gt;
&lt;p&gt;
Tables as Images? Exploring the Strengths and Limitations of LLMs on Multimodal Representations of Tabular Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12424
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;LLM&#22312;&#35299;&#37322;&#34920;&#26684;&#25968;&#25454;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#27604;&#36739;&#20102;&#25991;&#26412;&#21644;&#22270;&#20687;&#34920;&#26684;&#34920;&#31034;&#23545;LLM&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#20026;&#22312;&#34920;&#26684;&#30456;&#20851;&#20219;&#21153;&#19978;&#26377;&#25928;&#20351;&#29992;LLM&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#19981;&#21516;&#30340;&#25552;&#31034;&#31574;&#30053;&#21644;&#25968;&#25454;&#26684;&#24335;&#30740;&#31350;&#20102;&#21508;&#31181;LLM&#22312;&#35299;&#37322;&#34920;&#26684;&#25968;&#25454;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#28085;&#30422;&#20102;&#20845;&#20010;&#38024;&#23545;&#19982;&#34920;&#26684;&#30456;&#20851;&#20219;&#21153;&#30340;&#22522;&#20934;&#65292;&#22914;&#38382;&#31572;&#21644;&#20107;&#23454;&#26680;&#26597;&#12290;&#25105;&#20204;&#39318;&#27425;&#20171;&#32461;&#20102;LLM&#22312;&#22522;&#20110;&#22270;&#20687;&#30340;&#34920;&#26684;&#34920;&#31034;&#19978;&#30340;&#34920;&#29616;&#35780;&#20272;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20116;&#31181;&#22522;&#20110;&#25991;&#26412;&#21644;&#19977;&#31181;&#22522;&#20110;&#22270;&#20687;&#30340;&#34920;&#26684;&#34920;&#31034;&#65292;&#23637;&#31034;&#20102;&#34920;&#31034;&#21644;&#25552;&#31034;&#23545;LLM&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#22312;&#34920;&#26684;&#30456;&#20851;&#20219;&#21153;&#19978;&#26377;&#25928;&#20351;&#29992;LLM&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12424v1 Announce Type: cross  Abstract: In this paper, we investigate the effectiveness of various LLMs in interpreting tabular data through different prompting strategies and data formats. Our analysis extends across six benchmarks for table-related tasks such as question-answering and fact-checking. We introduce for the first time the assessment of LLMs' performance on image-based table representations. Specifically, we compare five text-based and three image-based table representations, demonstrating the influence of representation and prompting on LLM performance. Our study provides insights into the effective use of LLMs on table-related tasks.
&lt;/p&gt;</description></item><item><title>&#23433;&#20840;&#23545;&#40784;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#20250;&#36890;&#36807;&#27169;&#25311;&#22833;&#35843;&#26694;&#26550;&#65292;&#22312;&#23545;&#25239;&#24615;&#25805;&#32437;&#19979;&#20135;&#29983;&#21361;&#38505;&#32467;&#26524;&#65292;&#23545;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#21452;&#20493;&#26377;&#23475;&#24615;&#65292;&#39640;&#20110;&#24378;&#22522;&#32447;&#65292;&#24378;&#35843;&#20102;&#21363;&#20351;&#22312;&#23433;&#20840;&#23545;&#40784;&#21518;&#20063;&#38656;&#35201;&#37325;&#26032;&#35780;&#20272;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.12343</link><description>&lt;p&gt;
&#27169;&#25311;&#22833;&#35843;: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#23545;&#40784;&#21487;&#33021;&#20250;&#36866;&#24471;&#20854;&#21453;&#65281;
&lt;/p&gt;
&lt;p&gt;
Emulated Disalignment: Safety Alignment for Large Language Models May Backfire!
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12343
&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#23545;&#40784;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#20250;&#36890;&#36807;&#27169;&#25311;&#22833;&#35843;&#26694;&#26550;&#65292;&#22312;&#23545;&#25239;&#24615;&#25805;&#32437;&#19979;&#20135;&#29983;&#21361;&#38505;&#32467;&#26524;&#65292;&#23545;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#21452;&#20493;&#26377;&#23475;&#24615;&#65292;&#39640;&#20110;&#24378;&#22522;&#32447;&#65292;&#24378;&#35843;&#20102;&#21363;&#20351;&#22312;&#23433;&#20840;&#23545;&#40784;&#21518;&#20063;&#38656;&#35201;&#37325;&#26032;&#35780;&#20272;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38656;&#35201;&#36827;&#34892;&#23433;&#20840;&#23545;&#40784;&#65292;&#20197;&#30830;&#20445;&#19982;&#20154;&#31867;&#36827;&#34892;&#23433;&#20840;&#30340;&#23545;&#35805;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#25512;&#29702;&#26102;&#25915;&#20987;&#26694;&#26550;&#65292;&#34920;&#26126;&#23433;&#20840;&#23545;&#40784;&#20063;&#21487;&#33021;&#22312;&#23545;&#25239;&#24615;&#25805;&#32437;&#19979;&#26080;&#24847;&#20013;&#20419;&#25104;&#26377;&#23475;&#32467;&#26524;&#12290;&#36825;&#20010;&#26694;&#26550;&#34987;&#21629;&#21517;&#20026;&#27169;&#25311;&#22833;&#35843;&#65288;ED&#65289;&#65292;&#22312;&#36755;&#20986;&#31354;&#38388;&#20013;&#19981;&#33391;&#22320;&#32452;&#21512;&#20102;&#19968;&#23545;&#24320;&#28304;&#39044;&#35757;&#32451;&#21644;&#23433;&#20840;&#23545;&#40784;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20135;&#29983;&#20102;&#19968;&#20010;&#26377;&#23475;&#30340;&#35821;&#35328;&#27169;&#22411;&#32780;&#26080;&#38656;&#20219;&#20309;&#35757;&#32451;&#12290;&#25105;&#20204;&#23545;ED&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#21644;&#22235;&#20010;&#27169;&#22411;&#31995;&#21015;&#65288;Llama-1&#12289;Llama-2&#12289;Mistral&#21644;Alpaca&#65289;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;ED&#20351;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26377;&#23475;&#24615;&#22686;&#21152;&#20102;&#19968;&#20493;&#65292;&#24182;&#32988;&#36807;&#24378;&#22522;&#32447;&#65292;&#20197;&#36739;&#22823;&#20248;&#21183;&#22312;48&#20010;&#35780;&#20272;&#23376;&#38598;&#20013;&#30340;43&#20010;&#20013;&#23454;&#29616;&#20102;&#26368;&#39640;&#30340;&#26377;&#23475;&#29575;&#12290;&#33267;&#20851;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#20984;&#26174;&#20102;&#21363;&#20351;&#22312;&#23433;&#20840;&#23545;&#40784;&#21518;&#65292;&#37325;&#26032;&#35780;&#20272;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#23454;&#36341;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12343v1 Announce Type: new  Abstract: Large language models (LLMs) need to undergo safety alignment to ensure safe conversations with humans. However, in this work, we introduce an inference-time attack framework, demonstrating that safety alignment can also unintentionally facilitate harmful outcomes under adversarial manipulation. This framework, named Emulated Disalignment (ED), adversely combines a pair of open-source pre-trained and safety-aligned language models in the output space to produce a harmful language model without any training. Our experiments with ED across three datasets and four model families (Llama-1, Llama-2, Mistral, and Alpaca) show that ED doubles the harmfulness of pre-trained models and outperforms strong baselines, achieving the highest harmful rate in 43 out of 48 evaluation subsets by a large margin. Crucially, our findings highlight the importance of reevaluating the practice of open-sourcing language models even after safety alignment.
&lt;/p&gt;</description></item><item><title>LEIA&#26159;&#19968;&#31181;&#35821;&#35328;&#36866;&#24212;&#35843;&#25972;&#26041;&#27861;&#65292;&#21033;&#29992;&#32500;&#22522;&#30334;&#31185;&#23454;&#20307;&#21517;&#31216;&#36328;&#35821;&#35328;&#22686;&#24378;&#30446;&#26631;&#35821;&#35328;&#35821;&#26009;&#24211;&#65292;&#36890;&#36807;&#24038;&#21040;&#21491;&#30340;&#35821;&#35328;&#24314;&#27169;&#35757;&#32451;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#21508;&#31181;&#38750;&#33521;&#35821;&#35821;&#35328;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.11485</link><description>&lt;p&gt;
LEIA: &#21033;&#29992;&#22522;&#20110;&#23454;&#20307;&#30340;&#25968;&#25454;&#22686;&#24378;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#20419;&#36827;&#36328;&#35821;&#35328;&#30693;&#35782;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
LEIA: Facilitating Cross-Lingual Knowledge Transfer in Language Models with Entity-based Data Augmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11485
&lt;/p&gt;
&lt;p&gt;
LEIA&#26159;&#19968;&#31181;&#35821;&#35328;&#36866;&#24212;&#35843;&#25972;&#26041;&#27861;&#65292;&#21033;&#29992;&#32500;&#22522;&#30334;&#31185;&#23454;&#20307;&#21517;&#31216;&#36328;&#35821;&#35328;&#22686;&#24378;&#30446;&#26631;&#35821;&#35328;&#35821;&#26009;&#24211;&#65292;&#36890;&#36807;&#24038;&#21040;&#21491;&#30340;&#35821;&#35328;&#24314;&#27169;&#35757;&#32451;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#21508;&#31181;&#38750;&#33521;&#35821;&#35821;&#35328;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22522;&#20110;&#33521;&#35821;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36866;&#24212;&#20854;&#20182;&#35821;&#35328;&#30340;&#25805;&#20316;&#30001;&#20110;&#36328;&#35821;&#35328;&#36716;&#31227;&#30340;&#25928;&#29575;&#21644;&#28508;&#21147;&#32780;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#35821;&#35328;&#36866;&#24212;&#26041;&#27861;&#24120;&#24120;&#24573;&#35270;&#36328;&#35821;&#35328;&#30417;&#30563;&#30340;&#22909;&#22788;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;LEIA&#65292;&#19968;&#31181;&#21033;&#29992;&#36328;&#35821;&#35328;&#23545;&#40784;&#30340;&#32500;&#22522;&#30334;&#31185;&#23454;&#20307;&#21517;&#31216;&#30340;&#35821;&#35328;&#36866;&#24212;&#35843;&#25972;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#28041;&#21450;&#20351;&#29992;&#33521;&#35821;&#23454;&#20307;&#21517;&#31216;&#22686;&#24378;&#30446;&#26631;&#35821;&#35328;&#35821;&#26009;&#24211;&#65292;&#24182;&#20351;&#29992;&#20174;&#24038;&#21040;&#21491;&#30340;&#35821;&#35328;&#24314;&#27169;&#35757;&#32451;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#22810;&#26679;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;LEIA&#65292;&#20351;&#29992;7B&#21442;&#25968;&#30340;LLMs&#65292;&#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#38750;&#33521;&#35821;&#35821;&#35328;&#19978;&#30340;&#26174;&#33879;&#24615;&#33021;&#22686;&#30410;&#12290;&#28304;&#20195;&#30721;&#21487;&#22312;https://github.com/studio-ousia/leia&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11485v1 Announce Type: cross  Abstract: Adapting English-based large language models (LLMs) to other languages has become increasingly popular due to the efficiency and potential of cross-lingual transfer. However, existing language adaptation methods often overlook the benefits of cross-lingual supervision. In this study, we introduce LEIA, a language adaptation tuning method that utilizes Wikipedia entity names aligned across languages. This method involves augmenting the target language corpus with English entity names and training the model using left-to-right language modeling. We assess LEIA on diverse question answering datasets using 7B-parameter LLMs, demonstrating significant performance gains across various non-English languages. The source code is available at https://github.com/studio-ousia/leia.
&lt;/p&gt;</description></item><item><title>Attraos&#27169;&#22411;&#22522;&#20110;&#28151;&#27788;&#29702;&#35770;&#65292;&#22312;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#21033;&#29992;&#22810;&#23610;&#24230;&#21160;&#24577;&#35760;&#24518;&#21333;&#20803;&#21644;&#23616;&#37096;&#28436;&#21270;&#31574;&#30053;&#65292;&#34920;&#29616;&#20248;&#24322;&#20110;&#20854;&#20182;LTSF&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.11463</link><description>&lt;p&gt;
&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#21560;&#24341;&#23376;&#35760;&#24518;&#65306;&#28151;&#27788;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Attractor Memory for Long-Term Time Series Forecasting: A Chaos Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11463
&lt;/p&gt;
&lt;p&gt;
Attraos&#27169;&#22411;&#22522;&#20110;&#28151;&#27788;&#29702;&#35770;&#65292;&#22312;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#21033;&#29992;&#22810;&#23610;&#24230;&#21160;&#24577;&#35760;&#24518;&#21333;&#20803;&#21644;&#23616;&#37096;&#28436;&#21270;&#31574;&#30053;&#65292;&#34920;&#29616;&#20248;&#24322;&#20110;&#20854;&#20182;LTSF&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65288;LTSF&#65289;&#20219;&#21153;&#20013;&#65292;&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24573;&#35270;&#20102;&#31163;&#25955;&#26102;&#38388;&#24207;&#21015;&#28304;&#33258;&#28508;&#22312;&#36830;&#32493;&#21160;&#24577;&#31995;&#32479;&#30340;&#20851;&#38190;&#29305;&#24449;&#65292;&#23548;&#33268;&#32570;&#20047;&#22806;&#25512;&#21644;&#28436;&#21270;&#33021;&#21147;&#12290; &#37492;&#21035;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#28151;&#27788;&#24615;&#36136;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;\textbf{\textit{Attraos}}&#23558;&#28151;&#27788;&#29702;&#35770;&#34701;&#20837;&#21040;LTSF&#20013;&#65292;&#23558;&#23454;&#38469;&#26102;&#38388;&#24207;&#21015;&#35270;&#20026;&#26410;&#30693;&#39640;&#32500;&#28151;&#27788;&#21160;&#24577;&#31995;&#32479;&#30340;&#35266;&#27979;&#12290; &#22312;&#21560;&#24341;&#23376;&#19981;&#21464;&#24615;&#30340;&#27010;&#24565;&#19979;&#65292;Attraos&#21033;&#29992;&#25552;&#20986;&#30340;&#22810;&#23610;&#24230;&#21160;&#24577;&#35760;&#24518;&#21333;&#20803;&#26469;&#35760;&#24518;&#21382;&#21490;&#21160;&#24577;&#32467;&#26500;&#65292;&#24182;&#36890;&#36807;&#39057;&#29575;&#22686;&#24378;&#30340;&#23616;&#37096;&#28436;&#21270;&#31574;&#30053;&#36827;&#34892;&#39044;&#27979;&#12290; &#35814;&#32454;&#30340;&#29702;&#35770;&#20998;&#26512;&#21644;&#20016;&#23500;&#30340;&#32463;&#39564;&#35777;&#25454;&#19968;&#33268;&#34920;&#26126;&#65292;Attraos&#22312;&#20027;&#27969;LTSF&#25968;&#25454;&#38598;&#21644;&#28151;&#27788;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#21508;&#31181;LTSF&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11463v1 Announce Type: cross  Abstract: In long-term time series forecasting (LTSF) tasks, existing deep learning models overlook the crucial characteristic that discrete time series originate from underlying continuous dynamic systems, resulting in a lack of extrapolation and evolution capabilities. Recognizing the chaotic nature of real-world data, our model, \textbf{\textit{Attraos}}, incorporates chaos theory into LTSF, perceiving real-world time series as observations from unknown high-dimensional chaotic dynamic systems. Under the concept of attractor invariance, Attraos utilizes the proposed multi-scale dynamic memory unit to memorize historical dynamics structure and predicts by a frequency-enhanced local evolution strategy. Detailed theoretical analysis and abundant empirical evidence consistently show that Attraos outperforms various LTSF methods on mainstream LTSF datasets and chaotic datasets.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27809;&#26377;&#23398;&#20064;&#21040;&#35821;&#35328;&#30340;&#35270;&#21548;&#29305;&#24615;&#65292;&#22312;&#26032;&#30340;&#20219;&#21153;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#36739;&#24046;&#65292;&#26292;&#38706;&#20102;&#20154;&#31867;&#35821;&#35328;&#29702;&#35299;&#19982;&#35821;&#35328;&#27169;&#22411;&#24863;&#23448;&#22788;&#29702;&#33021;&#21147;&#20043;&#38388;&#30340;&#26681;&#26412;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2402.11349</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#26410;&#23398;&#20064;&#30340;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Tasks That Language Models Don't Learn
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11349
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27809;&#26377;&#23398;&#20064;&#21040;&#35821;&#35328;&#30340;&#35270;&#21548;&#29305;&#24615;&#65292;&#22312;&#26032;&#30340;&#20219;&#21153;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#36739;&#24046;&#65292;&#26292;&#38706;&#20102;&#20154;&#31867;&#35821;&#35328;&#29702;&#35299;&#19982;&#35821;&#35328;&#27169;&#22411;&#24863;&#23448;&#22788;&#29702;&#33021;&#21147;&#20043;&#38388;&#30340;&#26681;&#26412;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35748;&#20026;&#65292;&#25105;&#20204;&#30446;&#21069;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#27809;&#26377;&#23398;&#20064;&#21040;&#35821;&#35328;&#30340;&#26576;&#20123;&#29305;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#31995;&#21015;&#20219;&#21153;&#65288;&#31216;&#20026;H-TEST&#65289;&#23545;&#35821;&#35328;&#30340;&#35270;&#21548;&#29305;&#24615;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#12290;&#36825;&#19968;&#22522;&#20934;&#27979;&#35797;&#31361;&#26174;&#20102;&#20154;&#31867;&#35821;&#35328;&#29702;&#35299;&#19982;LLMs&#30340;&#24863;&#23448;&#21463;&#38480;&#22788;&#29702;&#33021;&#21147;&#20043;&#38388;&#30340;&#26681;&#26412;&#24046;&#36317;&#12290;&#25903;&#25345;&#25105;&#20204;&#30340;&#20551;&#35774;&#65292;1. &#25925;&#24847;&#25512;&#29702;&#65288;&#24605;&#32500;&#38142;&#65289;&#65292;2. &#23569;&#37327;&#26696;&#20363;&#65292;&#25110;3. &#21516;&#19968;&#27169;&#22411;&#31995;&#21015;&#30340;&#26356;&#24378;&#22823;LLM&#65288;LLaMA 2 13B-&gt;LLaMA 2 70B&#65289;&#24182;&#19981;&#33021;&#31616;&#21333;&#22320;&#24102;&#26469;H-TEST&#24615;&#33021;&#30340;&#25913;&#21892;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#29305;&#21035;&#23558;&#20854;&#19982;&#29595;&#20029;&#30340;&#21746;&#23398;&#26696;&#20363;&#32852;&#31995;&#36215;&#26469;&#65292;&#22905;&#22312;&#24863;&#23448;&#21463;&#38480;&#29615;&#22659;&#20013;&#20102;&#35299;&#19990;&#30028;&#65288;Jackson&#65292;1986&#65289;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#19968;&#20123;&#26368;&#24378;&#22823;&#30340;&#19987;&#26377;LLMs&#30340;&#34920;&#29616;&#25509;&#36817;&#20110;&#38543;&#26426;&#22522;&#20934;&#20934;&#30830;&#29575;50&#65285;&#65292;&#31361;&#26174;&#20102;&#26497;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11349v1 Announce Type: cross  Abstract: We argue that there are certain properties of language that our current large language models (LLMs) don't learn. We present an empirical investigation of visual-auditory properties of language through a series of tasks, termed H-TEST. This benchmark highlights a fundamental gap between human linguistic comprehension, which naturally integrates sensory experiences, and the sensory-deprived processing capabilities of LLMs. In support of our hypothesis, 1. deliberate reasoning (Chain-of-Thought), 2. few-shot examples, or 3. stronger LLM from the same model family (LLaMA 2 13B -&gt; LLaMA 2 70B) do not trivially bring improvements in H-TEST performance. Therefore, we make a particular connection to the philosophical case of Mary, who learns about the world in a sensory-deprived environment (Jackson, 1986). Our experiments show that some of the strongest proprietary LLMs stay near random chance baseline accuracy of 50%, highlighting the limit
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#23545;&#27604;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#30456;&#20284;&#24615;&#26469;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#26410;&#30693;&#20219;&#21153;&#25351;&#20196;&#30340;&#31283;&#20581;&#24615;</title><link>https://arxiv.org/abs/2402.11138</link><description>&lt;p&gt;
&#23545;&#27604;&#25351;&#20196;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Contrastive Instruction Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11138
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#23545;&#27604;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#30456;&#20284;&#24615;&#26469;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#26410;&#30693;&#20219;&#21153;&#25351;&#20196;&#30340;&#31283;&#20581;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#25972;&#19968;&#30452;&#34987;&#29992;&#20316;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#26410;&#30693;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;LLMs&#22312;&#38754;&#20020;&#26410;&#30693;&#25351;&#20196;&#26102;&#34920;&#29616;&#20986;&#26377;&#38480;&#30340;&#31283;&#20581;&#24615;&#65292;&#24403;&#30456;&#21516;&#30340;&#25351;&#20196;&#20197;&#31245;&#24494;&#21464;&#21270;&#30340;&#24418;&#24335;&#25110;&#35821;&#35328;&#39118;&#26684;&#25552;&#20986;&#26102;&#20250;&#20135;&#29983;&#19981;&#19968;&#33268;&#30340;&#36755;&#20986;&#12290;&#36825;&#31181;&#34892;&#20026;&#34920;&#26126;LLMs&#23545;&#25991;&#26412;&#21464;&#21270;&#30340;&#31283;&#20581;&#24615;&#21644;&#23545;&#26410;&#30693;&#25351;&#20196;&#30340;&#27867;&#21270;&#33021;&#21147;&#19981;&#36275;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#21487;&#20449;&#24230;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#27604;&#25351;&#20196;&#35843;&#25972;&#65292;&#35813;&#26041;&#27861;&#22312;&#26368;&#22823;&#21270;&#35821;&#20041;&#19978;&#31561;&#20215;&#30340;&#25351;&#20196;-&#23454;&#20363;&#23545;&#30340;&#38544;&#34255;&#34920;&#31034;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#30340;&#21516;&#26102;&#65292;&#26368;&#23567;&#21270;&#35821;&#20041;&#19978;&#19981;&#21516;&#30340;&#23545;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#20026;&#20102;&#20419;&#36827;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#36890;&#36807;&#37322;&#20041;&#20219;&#21153;&#25351;&#20196;&#65292;&#25193;&#20805;&#29616;&#26377;&#30340;FLAN&#38598;&#21512;&#12290;&#22312;PromptBench&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#23545;&#27604;&#25351;&#20196;&#35843;&#25972;&#65288;CoIN&#65289;&#19968;&#30452;&#25552;&#39640;&#20102;LLMs&#23545;&#26410;&#30693;&#25351;&#20196;&#30340;&#31283;&#20581;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11138v1 Announce Type: cross  Abstract: Instruction tuning has been used as a promising approach to improve the performance of large language models (LLMs) on unseen tasks. However, current LLMs exhibit limited robustness to unseen instructions, generating inconsistent outputs when the same instruction is phrased with slightly varied forms or language styles. This behavior indicates LLMs' lack of robustness to textual variations and generalizability to unseen instructions, potentially leading to trustworthiness issues. Accordingly, we propose Contrastive Instruction Tuning, which maximizes the similarity between the hidden representations of semantically equivalent instruction-instance pairs while minimizing the similarity between semantically different ones. To facilitate this approach, we augment the existing FLAN collection by paraphrasing task instructions. Experiments on the PromptBench benchmark show that CoIN consistently improves LLMs' robustness to unseen instructio
&lt;/p&gt;</description></item><item><title>&#24403;&#21069;&#30740;&#31350;&#36890;&#36807;&#23454;&#39564;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#27493;&#38382;&#39064;&#27714;&#35299;&#20013;&#20351;&#29992;&#26641;&#25628;&#32034;&#30340;&#21487;&#34892;&#24615;&#65292;&#25351;&#20986;&#39640;&#32423;&#35268;&#21010;&#26041;&#27861;&#38656;&#35201;&#37492;&#21035;&#22120;&#33267;&#23569;90%&#20934;&#30830;&#24615;&#25165;&#33021;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.10890</link><description>&lt;p&gt;
LLM&#35268;&#21010;&#20013;&#26641;&#25628;&#32034;&#20309;&#26102;&#26377;&#29992;&#65311;&#21462;&#20915;&#20110;&#37492;&#21035;&#22120;
&lt;/p&gt;
&lt;p&gt;
When is Tree Search Useful for LLM Planning? It Depends on the Discriminator
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10890
&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30740;&#31350;&#36890;&#36807;&#23454;&#39564;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#27493;&#38382;&#39064;&#27714;&#35299;&#20013;&#20351;&#29992;&#26641;&#25628;&#32034;&#30340;&#21487;&#34892;&#24615;&#65292;&#25351;&#20986;&#39640;&#32423;&#35268;&#21010;&#26041;&#27861;&#38656;&#35201;&#37492;&#21035;&#22120;&#33267;&#23569;90%&#20934;&#30830;&#24615;&#25165;&#33021;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#35821;&#35328;&#20195;&#29702;&#26694;&#26550;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;&#20309;&#22312;&#22810;&#27493;&#38382;&#39064;&#19979;&#35299;&#20915;&#38382;&#39064;&#65292;&#35813;&#26694;&#26550;&#21253;&#25324;&#29983;&#25104;&#22120;&#12289;&#37492;&#21035;&#22120;&#21644;&#35268;&#21010;&#26041;&#27861;&#19977;&#20010;&#37096;&#20998;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#31181;&#20808;&#36827;&#35268;&#21010;&#26041;&#27861;&#65292;&#36845;&#20195;&#26657;&#27491;&#21644;&#26641;&#25628;&#32034;&#30340;&#23454;&#38469;&#25928;&#29992;&#12290;&#25105;&#20204;&#20840;&#38754;&#20998;&#26512;&#20102;&#37492;&#21035;&#20934;&#30830;&#24615;&#22914;&#20309;&#24433;&#21709;&#20195;&#29702;&#22312;&#20351;&#29992;&#36825;&#20004;&#31181;&#26041;&#27861;&#25110;&#26356;&#31616;&#21333;&#30340;&#37325;&#26032;&#25490;&#24207;&#26041;&#27861;&#26102;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;&#22312;&#20004;&#39033;&#20219;&#21153;&#65292;&#25991;&#26412;&#21040;SQL&#35299;&#26512;&#21644;&#25968;&#23398;&#25512;&#29702;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65306;&#65288;1&#65289;&#39640;&#32423;&#35268;&#21010;&#26041;&#27861;&#38656;&#35201;&#33267;&#23569;90%&#20934;&#30830;&#24615;&#30340;&#37492;&#21035;&#22120;&#25165;&#33021;&#23454;&#29616;&#26174;&#33879;&#25913;&#36827;&#65307;&#65288;2&#65289;&#24403;&#21069;LLMs&#30340;&#37492;&#21035;&#33021;&#21147;&#23578;&#26410;&#28385;&#36275;&#39640;&#32423;&#35268;&#21010;&#26041;&#27861;&#23454;&#29616;&#36825;&#31181;&#25913;&#36827;&#30340;&#38656;&#27714;&#65307;&#65288;3&#65289;&#37319;&#29992;&#22522;&#20110;LLM&#30340;&#37492;&#21035;&#22120;&#26102;&#65292;&#39640;&#32423;&#35268;&#21010;&#26041;&#27861;&#21487;&#33021;&#26080;&#27861;&#20805;&#20998;&#24179;&#34913;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10890v1 Announce Type: cross  Abstract: In this paper, we examine how large language models (LLMs) solve multi-step problems under a language agent framework with three components: a generator, a discriminator, and a planning method. We investigate the practical utility of two advanced planning methods, iterative correction and tree search. We present a comprehensive analysis of how discrimination accuracy affects the overall performance of agents when using these two methods or a simpler method, re-ranking. Experiments on two tasks, text-to-SQL parsing and mathematical reasoning, show that: (1) advanced planning methods demand discriminators with at least 90% accuracy to achieve significant improvements over re-ranking; (2) current LLMs' discrimination abilities have not met the needs of advanced planning methods to achieve such improvements; (3) with LLM-based discriminators, advanced planning methods may not adequately balance accuracy and efficiency. For example, compare
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#26041;&#27861;&#65292;&#21363;&#20855;&#26377;&#20559;&#32622;&#30340;DPO&#65288;ODPO&#65289;&#65292;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#19981;&#21516;&#23545;&#24453;&#27599;&#20010;&#20559;&#22909;&#23545;&#12290;</title><link>https://arxiv.org/abs/2402.10571</link><description>&lt;p&gt;
&#20855;&#26377;&#20559;&#32622;&#30340;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Direct Preference Optimization with an Offset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10571
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#26041;&#27861;&#65292;&#21363;&#20855;&#26377;&#20559;&#32622;&#30340;DPO&#65288;ODPO&#65289;&#65292;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#19981;&#21516;&#23545;&#24453;&#27599;&#20010;&#20559;&#22909;&#23545;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#26159;&#19968;&#31181;&#25104;&#21151;&#30340;&#24494;&#35843;&#31574;&#30053;&#65292;&#29992;&#20110;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#20445;&#25345;&#19968;&#33268;&#65292;&#32780;&#26080;&#38656;&#35757;&#32451;&#22870;&#21169;&#27169;&#22411;&#25110;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;DPO&#30340;&#27867;&#21270;&#24418;&#24335;&#65292;&#31216;&#20026;&#20855;&#26377;&#20559;&#32622;&#30340;DPO&#65288;ODPO&#65289;&#65292;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#19981;&#23558;&#27599;&#20010;&#20559;&#22909;&#23545;&#35270;&#20026;&#30456;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10571v1 Announce Type: cross  Abstract: Direct preference optimization (DPO) is a successful fine-tuning strategy for aligning large language models with human preferences without the need to train a reward model or employ reinforcement learning. DPO, as originally formulated, relies on binary preference data and fine-tunes a language model to increase the likelihood of a preferred response over a dispreferred response. However, not all preference pairs are equal: while in some cases the preferred response is only slightly better than the dispreferred response, there can be a stronger preference for one response when, for example, the other response includes harmful or toxic content. In this paper, we propose a generalization of DPO, termed DPO with an offset (ODPO), that does not treat every preference pair equally during fine-tuning. Intuitively, ODPO requires the difference between the likelihood of the preferred and dispreferred response to be greater than an offset valu
&lt;/p&gt;</description></item><item><title>DUDF&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#26080;&#31526;&#21495;&#36317;&#31163;&#22330;&#30340;&#21452;&#26354;&#32553;&#25918;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#32593;&#32476;&#38598;&#25104;&#65292;&#35299;&#20915;&#20102;&#24320;&#25918;&#34920;&#38754;&#34920;&#31034;&#30340;&#25361;&#25112;&#65292;&#24182;&#22312;&#37325;&#24314;&#36136;&#37327;&#21644;&#35757;&#32451;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.08876</link><description>&lt;p&gt;
DUDF: &#20855;&#26377;&#21452;&#26354;&#32553;&#25918;&#30340;&#21487;&#24494;&#26080;&#31526;&#21495;&#36317;&#31163;&#22330;
&lt;/p&gt;
&lt;p&gt;
DUDF: Differentiable Unsigned Distance Fields with Hyperbolic Scaling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08876
&lt;/p&gt;
&lt;p&gt;
DUDF&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#26080;&#31526;&#21495;&#36317;&#31163;&#22330;&#30340;&#21452;&#26354;&#32553;&#25918;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#32593;&#32476;&#38598;&#25104;&#65292;&#35299;&#20915;&#20102;&#24320;&#25918;&#34920;&#38754;&#34920;&#31034;&#30340;&#25361;&#25112;&#65292;&#24182;&#22312;&#37325;&#24314;&#36136;&#37327;&#21644;&#35757;&#32451;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#20154;&#20204;&#23545;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#20197;&#36924;&#36817;&#26080;&#31526;&#21495;&#36317;&#31163;&#22330;&#65288;UDF&#65289;&#29992;&#20110;&#19977;&#32500;&#37325;&#24314;&#20013;&#34920;&#31034;&#24320;&#25918;&#34920;&#38754;&#30340;&#26041;&#27861;&#36234;&#21457;&#24863;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;UDF&#22312;&#38646;&#32423;&#38598;&#22788;&#26159;&#38750;&#21487;&#24494;&#30340;&#65292;&#36825;&#20250;&#23548;&#33268;&#36317;&#31163;&#21644;&#26799;&#24230;&#30340;&#26174;&#33879;&#35823;&#24046;&#65292;&#36890;&#24120;&#23548;&#33268;&#34920;&#38754;&#30340;&#30862;&#29255;&#21270;&#21644;&#19981;&#36830;&#32493;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23398;&#20064;&#26080;&#31526;&#21495;&#36317;&#31163;&#22330;&#30340;&#21452;&#26354;&#32553;&#25918;&#65292;&#36825;&#23450;&#20041;&#20102;&#19968;&#20010;&#20855;&#26377;&#19981;&#21516;&#36793;&#30028;&#26465;&#20214;&#30340;&#26032;&#35843;&#21644;&#38382;&#39064;&#12290;&#36825;&#20351;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#26080;&#32541;&#22320;&#38598;&#25104;&#21040;&#26368;&#20808;&#36827;&#30340;&#36830;&#32493;&#21487;&#24494;&#30340;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#32593;&#32476;&#20013;&#65292;&#22312;&#25991;&#29486;&#20013;&#24191;&#27867;&#24212;&#29992;&#20110;&#34920;&#31034;&#26377;&#31526;&#21495;&#36317;&#31163;&#22330;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#33021;&#22815;&#35299;&#20915;&#24320;&#25918;&#34920;&#38754;&#34920;&#31034;&#30340;&#25361;&#25112;&#65292;&#36824;&#22312;&#37325;&#24314;&#36136;&#37327;&#21644;&#35757;&#32451;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20986;&#26174;&#33879;&#25913;&#36827;&#12290;&#21478;&#22806;&#65292;&#35813;&#26041;&#27861;&#35299;&#38145;&#20102;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08876v1 Announce Type: cross Abstract: In recent years, there has been a growing interest in training Neural Networks to approximate Unsigned Distance Fields (UDFs) for representing open surfaces in the context of 3D reconstruction. However, UDFs are non-differentiable at the zero level set which leads to significant errors in distances and gradients, generally resulting in fragmented and discontinuous surfaces. In this paper, we propose to learn a hyperbolic scaling of the unsigned distance field, which defines a new Eikonal problem with distinct boundary conditions. This allows our formulation to integrate seamlessly with state-of-the-art continuously differentiable implicit neural representation networks, largely applied in the literature to represent signed distance fields. Our approach not only addresses the challenge of open surface representation but also demonstrates significant improvement in reconstruction quality and training performance. Moreover, the unlocked fi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25511;&#30340;&#22810;&#27169;&#24577;&#21453;&#39304;&#21512;&#25104;&#31995;&#32479;&#65292;&#33021;&#22815;&#26681;&#25454;&#25991;&#26412;&#21644;&#22270;&#20687;&#36755;&#20837;&#29983;&#25104;&#20855;&#26377;&#29305;&#23450;&#24773;&#24863;&#65288;&#31215;&#26497;&#25110;&#28040;&#26497;&#65289;&#30340;&#21453;&#39304;&#65292;&#26377;&#30528;&#24191;&#27867;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>https://arxiv.org/abs/2402.07640</link><description>&lt;p&gt;
&#21512;&#25104;&#23545;&#22810;&#27169;&#24577;&#25991;&#26412;&#21644;&#22270;&#29255;&#25968;&#25454;&#30340;&#24773;&#24863;&#25511;&#21046;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
Synthesizing Sentiment-Controlled Feedback For Multimodal Text and Image Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07640
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25511;&#30340;&#22810;&#27169;&#24577;&#21453;&#39304;&#21512;&#25104;&#31995;&#32479;&#65292;&#33021;&#22815;&#26681;&#25454;&#25991;&#26412;&#21644;&#22270;&#20687;&#36755;&#20837;&#29983;&#25104;&#20855;&#26377;&#29305;&#23450;&#24773;&#24863;&#65288;&#31215;&#26497;&#25110;&#28040;&#26497;&#65289;&#30340;&#21453;&#39304;&#65292;&#26377;&#30528;&#24191;&#27867;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#22810;&#27169;&#24577;&#36755;&#20837;&#65288;&#21253;&#25324;&#25991;&#26412;&#21644;&#22270;&#29255;&#65289;&#30340;&#24773;&#24863;&#25511;&#21046;&#21453;&#39304;&#33021;&#22815;&#24357;&#34917;&#20154;&#26426;&#20132;&#20114;&#39046;&#22495;&#30340;&#19968;&#20010;&#20851;&#38190;&#24046;&#36317;&#65292;&#20351;&#31995;&#32479;&#33021;&#22815;&#25552;&#20379;&#20855;&#26377;&#21516;&#29702;&#24515;&#12289;&#20934;&#30830;&#24615;&#21644;&#24341;&#20154;&#20837;&#32988;&#30340;&#22238;&#24212;&#12290;&#36825;&#31181;&#33021;&#21147;&#22312;&#21307;&#30103;&#12289;&#33829;&#38144;&#21644;&#25945;&#32946;&#31561;&#39046;&#22495;&#26377;&#30528;&#28145;&#36828;&#30340;&#24212;&#29992;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#21487;&#25511;&#22810;&#27169;&#24577;&#21453;&#39304;&#21512;&#25104;&#65288;CMFeed&#65289;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25511;&#30340;&#21453;&#39304;&#21512;&#25104;&#31995;&#32479;&#12290;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#21253;&#25324;&#19968;&#20010;&#32534;&#30721;&#22120;&#12289;&#35299;&#30721;&#22120;&#21644;&#25511;&#21046;&#24615;&#27169;&#22359;&#65292;&#29992;&#20110;&#22788;&#29702;&#25991;&#26412;&#21644;&#35270;&#35273;&#36755;&#20837;&#12290;&#23427;&#20351;&#29992;Transformer&#21644;Faster R-CNN&#32593;&#32476;&#25552;&#21462;&#25991;&#26412;&#21644;&#35270;&#35273;&#29305;&#24449;&#65292;&#24182;&#23558;&#23427;&#20204;&#32467;&#21512;&#36215;&#26469;&#29983;&#25104;&#21453;&#39304;&#12290;CMFeed&#25968;&#25454;&#38598;&#21253;&#21547;&#22270;&#29255;&#12289;&#25991;&#26412;&#12289;&#23545;&#24086;&#23376;&#30340;&#21453;&#24212;&#12289;&#24102;&#26377;&#30456;&#20851;&#24615;&#35780;&#20998;&#30340;&#20154;&#31867;&#35780;&#35770;&#20197;&#21450;&#23545;&#35780;&#35770;&#30340;&#21453;&#24212;&#12290;&#23545;&#24086;&#23376;&#21644;&#35780;&#35770;&#30340;&#21453;&#24212;&#34987;&#29992;&#26469;&#35757;&#32451;&#25552;&#20986;&#30340;&#27169;&#22411;&#20197;&#20135;&#29983;&#20855;&#26377;&#29305;&#23450;&#65288;&#31215;&#26497;&#25110;&#28040;&#26497;&#65289;&#24773;&#24863;&#30340;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to generate sentiment-controlled feedback in response to multimodal inputs, comprising both text and images, addresses a critical gap in human-computer interaction by enabling systems to provide empathetic, accurate, and engaging responses. This capability has profound applications in healthcare, marketing, and education. To this end, we construct a large-scale Controllable Multimodal Feedback Synthesis (CMFeed) dataset and propose a controllable feedback synthesis system. The proposed system includes an encoder, decoder, and controllability block for textual and visual inputs. It extracts textual and visual features using a transformer and Faster R-CNN networks and combines them to generate feedback. The CMFeed dataset encompasses images, text, reactions to the post, human comments with relevance scores, and reactions to the comments. The reactions to the post and comments are utilized to train the proposed model to produce feedback with a particular (positive or negative)
&lt;/p&gt;</description></item><item><title>T-RAG&#26159;&#19968;&#20010;&#22522;&#20110;LLM&#30340;&#24212;&#29992;&#31243;&#24207;&#65292;&#29992;&#20110;&#31169;&#20154;&#20225;&#19994;&#25991;&#20214;&#38382;&#31572;&#65292;&#23427;&#32467;&#21512;&#20102;RAG&#26694;&#26550;&#21644;&#32463;&#36807;&#24494;&#35843;&#30340;&#24320;&#28304;LLM&#65292;&#24182;&#20998;&#20139;&#20102;&#26500;&#24314;&#21644;&#37096;&#32626;&#36807;&#31243;&#20013;&#30340;&#32463;&#39564;&#25945;&#35757;&#12290;</title><link>https://arxiv.org/abs/2402.07483</link><description>&lt;p&gt;
T-RAG: &#26469;&#33258;LLM&#25112;&#22330;&#30340;&#32463;&#39564;&#25945;&#35757;
&lt;/p&gt;
&lt;p&gt;
T-RAG: Lessons from the LLM Trenches
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07483
&lt;/p&gt;
&lt;p&gt;
T-RAG&#26159;&#19968;&#20010;&#22522;&#20110;LLM&#30340;&#24212;&#29992;&#31243;&#24207;&#65292;&#29992;&#20110;&#31169;&#20154;&#20225;&#19994;&#25991;&#20214;&#38382;&#31572;&#65292;&#23427;&#32467;&#21512;&#20102;RAG&#26694;&#26550;&#21644;&#32463;&#36807;&#24494;&#35843;&#30340;&#24320;&#28304;LLM&#65292;&#24182;&#20998;&#20139;&#20102;&#26500;&#24314;&#21644;&#37096;&#32626;&#36807;&#31243;&#20013;&#30340;&#32463;&#39564;&#25945;&#35757;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#24778;&#20154;&#30340;&#35821;&#35328;&#33021;&#21147;&#65292;&#25512;&#21160;&#20102;&#23558;&#23427;&#20204;&#25972;&#21512;&#21040;&#21508;&#20010;&#39046;&#22495;&#30340;&#24212;&#29992;&#30340;&#23581;&#35797;&#12290;&#19968;&#20010;&#37325;&#35201;&#30340;&#24212;&#29992;&#39046;&#22495;&#26159;&#23545;&#31169;&#20154;&#20225;&#19994;&#25991;&#20214;&#36827;&#34892;&#38382;&#31572;&#65292;&#20854;&#20013;&#20027;&#35201;&#32771;&#34385;&#22240;&#32032;&#26159;&#25968;&#25454;&#23433;&#20840;&#65292;&#38656;&#35201;&#33021;&#22815;&#22312;&#26412;&#22320;&#37096;&#32626;&#30340;&#24212;&#29992;&#31243;&#24207;&#65292;&#26377;&#38480;&#30340;&#35745;&#31639;&#36164;&#28304;&#21644;&#23545;&#26597;&#35810;&#27491;&#30830;&#21709;&#24212;&#30340;&#20581;&#22766;&#24212;&#29992;&#30340;&#38656;&#27714;&#12290;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#24050;&#25104;&#20026;&#26500;&#24314;&#22522;&#20110;LLM&#30340;&#24212;&#29992;&#31243;&#24207;&#30340;&#26368;&#37325;&#35201;&#30340;&#26694;&#26550;&#12290;&#34429;&#28982;&#26500;&#24314;RAG&#30456;&#23545;&#31616;&#21333;&#65292;&#20294;&#35201;&#20351;&#20854;&#20581;&#22766;&#21644;&#21487;&#38752;&#30340;&#24212;&#29992;&#31243;&#24207;&#38656;&#35201;&#24191;&#27867;&#30340;&#23450;&#21046;&#21270;&#21644;&#30456;&#23545;&#28145;&#20837;&#30340;&#24212;&#29992;&#39046;&#22495;&#30693;&#35782;&#12290;&#25105;&#20204;&#20998;&#20139;&#20102;&#26500;&#24314;&#21644;&#37096;&#32626;&#19968;&#20010;&#22522;&#20110;LLM&#30340;&#31169;&#20154;&#32452;&#32455;&#25991;&#20214;&#38382;&#31572;&#24212;&#29992;&#30340;&#32463;&#39564;&#12290;&#25105;&#20204;&#30340;&#24212;&#29992;&#32467;&#21512;&#20102;RAG&#30340;&#20351;&#29992;&#21644;&#32463;&#36807;&#24494;&#35843;&#30340;&#24320;&#28304;LLM&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#36824;&#20855;&#26377; ...
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLM) have shown remarkable language capabilities fueling attempts to integrate them into applications across a wide range of domains. An important application area is question answering over private enterprise documents where the main considerations are data security, which necessitates applications that can be deployed on-prem, limited computational resources and the need for a robust application that correctly responds to queries. Retrieval-Augmented Generation (RAG) has emerged as the most prominent framework for building LLM-based applications. While building a RAG is relatively straightforward, making it robust and a reliable application requires extensive customization and relatively deep knowledge of the application domain. We share our experiences building and deploying an LLM application for question answering over private organizational documents. Our application combines the use of RAG with a finetuned open-source LLM. Additionally, our system, which w
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#22312;&#25552;&#20379;&#20219;&#21153;&#29305;&#23450;&#25351;&#20196;&#30340;&#24773;&#20917;&#19979;&#26159;&#21542;&#38656;&#35201;&#20248;&#21270;&#19978;&#19979;&#25991;&#31034;&#20363;&#65292;&#25105;&#20204;&#25361;&#25112;&#20102;&#23545;&#20110;&#25351;&#23548;&#24615;LLMs&#30340;&#20849;&#35782;&#65292;&#24182;&#21457;&#29616;&#22312;&#26576;&#20123;&#20219;&#21153;&#20013;&#65292;&#19981;&#21516;&#30340;&#20248;&#21270;&#19978;&#19979;&#25991;&#31034;&#20363;&#26041;&#27861;&#20250;&#20135;&#29983;&#36882;&#20943;&#30340;&#22238;&#25253;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;"&#24230;&#37327;&#26631;&#20934;"&#65292;&#29992;&#20110;&#34913;&#37327;&#20174;&#32473;&#23450;&#25351;&#20196;&#20013;&#23398;&#20064;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#24110;&#21161;&#20915;&#23450;&#26159;&#21542;&#20248;&#21270;&#25351;&#20196;&#36824;&#26159;ICE&#29992;&#20110;&#20219;&#20309;&#26032;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2402.06733</link><description>&lt;p&gt;
NICE: &#20248;&#21270;&#19978;&#19979;&#25991;&#31034;&#20363;&#36824;&#26159;&#19981;&#20248;&#21270;&#65311;
&lt;/p&gt;
&lt;p&gt;
NICE: To Optimize In-Context Examples or Not?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06733
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#22312;&#25552;&#20379;&#20219;&#21153;&#29305;&#23450;&#25351;&#20196;&#30340;&#24773;&#20917;&#19979;&#26159;&#21542;&#38656;&#35201;&#20248;&#21270;&#19978;&#19979;&#25991;&#31034;&#20363;&#65292;&#25105;&#20204;&#25361;&#25112;&#20102;&#23545;&#20110;&#25351;&#23548;&#24615;LLMs&#30340;&#20849;&#35782;&#65292;&#24182;&#21457;&#29616;&#22312;&#26576;&#20123;&#20219;&#21153;&#20013;&#65292;&#19981;&#21516;&#30340;&#20248;&#21270;&#19978;&#19979;&#25991;&#31034;&#20363;&#26041;&#27861;&#20250;&#20135;&#29983;&#36882;&#20943;&#30340;&#22238;&#25253;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;"&#24230;&#37327;&#26631;&#20934;"&#65292;&#29992;&#20110;&#34913;&#37327;&#20174;&#32473;&#23450;&#25351;&#20196;&#20013;&#23398;&#20064;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#24110;&#21161;&#20915;&#23450;&#26159;&#21542;&#20248;&#21270;&#25351;&#20196;&#36824;&#26159;ICE&#29992;&#20110;&#20219;&#20309;&#26032;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#20248;&#21270;&#19978;&#19979;&#25991;&#31034;&#20363;&#65288;ICE&#65289;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#20551;&#35774;&#22312;&#25552;&#31034;&#20449;&#24687;&#20013;&#35201;&#20040;&#26159;&#22266;&#23450;&#30340;&#65292;&#35201;&#20040;&#27809;&#26377;&#25552;&#20379;&#25351;&#20196;&#65292;&#23548;&#33268;&#20102;&#19968;&#20010;&#34920;&#38754;&#19978;&#30340;&#20849;&#35782;&#65306;&#20248;&#21270;&#19978;&#19979;&#25991;&#31034;&#20363;&#23545;&#20110;&#25552;&#39640;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#38024;&#23545;&#32463;&#36807;&#25351;&#23548;&#30340;LLMs&#25361;&#25112;&#36825;&#19968;&#20849;&#35782;&#65292;&#30740;&#31350;&#22312;&#25552;&#20379;&#20102;&#20219;&#21153;&#29305;&#23450;&#25351;&#20196;&#30340;&#24773;&#20917;&#19979;&#20248;&#21270;&#19978;&#19979;&#25991;&#31034;&#20363;&#26159;&#21542;&#24517;&#35201;&#65292;&#24182;&#21457;&#29616;&#26377;&#19968;&#20123;&#20219;&#21153;&#23545;&#20110;&#19981;&#21516;&#30340;&#20248;&#21270;&#19978;&#19979;&#25991;&#31034;&#20363;&#26041;&#27861;&#20135;&#29983;&#36882;&#20943;&#30340;&#22238;&#25253;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20219;&#21153;&#29305;&#23450;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#31216;&#20026;"&#24230;&#37327;&#26631;&#20934;"&#65288;Metric&#65289;&#65292;&#29992;&#20110;&#37327;&#21270;&#20174;&#32473;&#23450;&#25351;&#20196;&#20013;&#23398;&#20064;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#24110;&#21161;&#20915;&#23450;&#26159;&#21542;&#20248;&#21270;&#25351;&#20196;&#36824;&#26159;ICE&#29992;&#20110;&#20219;&#20309;&#26032;&#20219;&#21153;&#12290;&#36890;&#36807;&#23545;&#21508;&#31181;&#20219;&#21153;&#21644;&#36880;&#27493;&#22686;&#21152;&#30340;&#25351;&#20196;&#38598;&#30340;&#31995;&#32479;&#24615;&#30740;&#31350;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#35813;&#21551;&#21457;&#24335;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works have shown that large language models (LLMs) work remarkably well on a wide range of tasks through in-context learning and optimization of in-context examples (ICE). However, most of these studies assume either a fixed or no instruction provided in the prompt, leading to the apparent consensus that the optimization of in-context examples is critical for better performance. We challenge this consensus for instruction-tuned LLMs by investigating the necessity of optimizing in-context examples when task-specific instructions are provided, and find that there are tasks for which various ways of optimizing in-context examples yield diminishing returns. We introduce a task-specific metric called \metriclong{} (\metric) that quantifies the learnability of tasks from a given instruction, and provides a heuristic that helps decide whether to optimize for instructions or ICE for any new task. On a wide range of tasks and a systematically created instruction set with gradually added 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29109;&#27491;&#21017;&#21270;&#30340;&#20196;&#29260;&#32423;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#65288;ETPO&#65289;&#65292;&#29992;&#20110;&#20248;&#21270;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#36890;&#36807;&#30452;&#25509;&#19982;&#20219;&#21153;&#29305;&#23450;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#65292;&#24182;&#35299;&#20915;&#22312;&#22914;&#20309;&#20998;&#37197;&#20196;&#29260;&#32423;&#23398;&#20998;&#21644;&#26368;&#22823;&#21270;&#22870;&#21169;&#20043;&#38388;&#30340;&#20914;&#31361;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.06700</link><description>&lt;p&gt;
&#29109;&#27491;&#21017;&#21270;&#30340;&#20196;&#29260;&#32423;&#31574;&#30053;&#20248;&#21270;&#29992;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Entropy-Regularized Token-Level Policy Optimization for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06700
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29109;&#27491;&#21017;&#21270;&#30340;&#20196;&#29260;&#32423;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#65288;ETPO&#65289;&#65292;&#29992;&#20110;&#20248;&#21270;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#36890;&#36807;&#30452;&#25509;&#19982;&#20219;&#21153;&#29305;&#23450;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#65292;&#24182;&#35299;&#20915;&#22312;&#22914;&#20309;&#20998;&#37197;&#20196;&#29260;&#32423;&#23398;&#20998;&#21644;&#26368;&#22823;&#21270;&#22870;&#21169;&#20043;&#38388;&#30340;&#20914;&#31361;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20132;&#20114;&#24335;&#20915;&#31574;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#26234;&#33021;&#20195;&#29702;&#30340;&#28508;&#21147;&#12290;&#20256;&#32479;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#31934;&#24515;&#35774;&#35745;&#30340;&#25552;&#31034;&#12289;&#39640;&#36136;&#37327;&#30340;&#31034;&#20363;&#25110;&#39069;&#22806;&#30340;&#22870;&#21169;&#27169;&#22411;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#30417;&#30563;&#24494;&#35843;&#25110;RLHF&#12290;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#21160;&#24577;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;LLMs&#33021;&#22815;&#36890;&#36807;&#30452;&#25509;&#19982;&#20219;&#21153;&#29305;&#23450;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#26469;&#20811;&#26381;&#36825;&#20123;&#20381;&#36182;&#20851;&#31995;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#23427;&#38754;&#20020;&#30528;&#37325;&#37325;&#22256;&#38590;&#65306;1&#65289;&#30001;&#20110;&#24040;&#22823;&#30340;&#21160;&#20316;&#31354;&#38388;&#38656;&#35201;&#25506;&#32034;&#32780;&#20135;&#29983;&#30340;&#19981;&#31283;&#23450;&#24615;&#65307;2&#65289;&#22522;&#20110;&#21160;&#20316;&#32423;&#22870;&#21169;&#20449;&#21495;&#20998;&#37197;&#20196;&#29260;&#32423;&#23398;&#20998;&#30340;&#25361;&#25112;&#65292;&#23548;&#33268;&#26368;&#22823;&#21270;&#22870;&#21169;&#21644;&#20934;&#30830;&#24314;&#27169;&#35821;&#26009;&#24211;&#25968;&#25454;&#20043;&#38388;&#30340;&#20914;&#31361;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#29109;&#27491;&#21017;&#21270;&#30340;&#20196;&#29260;&#32423;&#31574;&#30053;&#20248;&#21270;&#65288;ETPO&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#19987;&#20026;&#22312;&#20196;&#29260;&#32423;&#20248;&#21270;LLMs&#32780;&#35774;&#35745;&#30340;&#29109;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;ETPO&#30340;&#26680;&#24515;&#26159;&#25105;&#20204;&#30340;&#19968;&#31181;&#26032;&#39062;&#30340;&#36880;&#20196;&#29260;&#36719;Bellman&#26356;&#26032;&#31639;&#27861;&#65292;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown promise as intelligent agents in interactive decision-making tasks. Traditional approaches often depend on meticulously designed prompts, high-quality examples, or additional reward models for in-context learning, supervised fine-tuning, or RLHF. Reinforcement learning (RL) presents a dynamic alternative for LLMs to overcome these dependencies by engaging directly with task-specific environments. Nonetheless, it faces significant hurdles: 1) instability stemming from the exponentially vast action space requiring exploration; 2) challenges in assigning token-level credit based on action-level reward signals, resulting in discord between maximizing rewards and accurately modeling corpus data. In response to these challenges, we introduce Entropy-Regularized Token-level Policy Optimization (ETPO), an entropy-augmented RL method tailored for optimizing LLMs at the token level. At the heart of ETPO is our novel per-token soft Bellman update, designed 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;MLLM&#20316;&#20026;&#27861;&#23448;&#30340;&#26032;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;MLLM&#22312;&#36741;&#21161;&#27861;&#23448;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;MLLM&#22312;&#23545;&#27604;&#35780;&#20272;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20102;&#38750;&#24120;&#31867;&#20154;&#30340;&#36776;&#21035;&#33021;&#21147;&#65292;&#20294;&#22312;&#35780;&#20998;&#35780;&#20272;&#21644;&#25209;&#37327;&#25490;&#24207;&#20219;&#21153;&#20013;&#19982;&#20154;&#31867;&#20559;&#22909;&#23384;&#22312;&#26174;&#33879;&#30340;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;&#23545;&#20110;&#20808;&#36827;&#27169;&#22411;&#22914;GPT-4V&#65292;MLLM&#20173;&#28982;&#38754;&#20020;&#30528;&#20559;&#35265;&#12289;&#24187;&#35273;&#24335;&#22238;&#31572;&#21644;&#19981;&#19968;&#33268;&#24615;&#31561;&#21028;&#26029;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.04788</link><description>&lt;p&gt;
MLLM&#20316;&#20026;&#27861;&#23448;&#65306;&#20351;&#29992;&#35270;&#35273;&#35821;&#35328;&#22522;&#20934;&#35780;&#20272;&#22810;&#27169;&#24577;MLLM&#20316;&#20026;&#27861;&#23448;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04788
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;MLLM&#20316;&#20026;&#27861;&#23448;&#30340;&#26032;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;MLLM&#22312;&#36741;&#21161;&#27861;&#23448;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;MLLM&#22312;&#23545;&#27604;&#35780;&#20272;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20102;&#38750;&#24120;&#31867;&#20154;&#30340;&#36776;&#21035;&#33021;&#21147;&#65292;&#20294;&#22312;&#35780;&#20998;&#35780;&#20272;&#21644;&#25209;&#37327;&#25490;&#24207;&#20219;&#21153;&#20013;&#19982;&#20154;&#31867;&#20559;&#22909;&#23384;&#22312;&#26174;&#33879;&#30340;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;&#23545;&#20110;&#20808;&#36827;&#27169;&#22411;&#22914;GPT-4V&#65292;MLLM&#20173;&#28982;&#38754;&#20020;&#30528;&#20559;&#35265;&#12289;&#24187;&#35273;&#24335;&#22238;&#31572;&#21644;&#19981;&#19968;&#33268;&#24615;&#31561;&#21028;&#26029;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#36817;&#26469;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#23637;&#29616;&#20986;&#20154;&#24037;&#26234;&#33021;&#26041;&#38754;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#35780;&#20272;MLLM&#30340;&#23454;&#29992;&#24615;&#23384;&#22312;&#30528;&#30456;&#24403;&#22823;&#30340;&#25361;&#25112;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#32570;&#20047;&#19982;&#20154;&#31867;&#20559;&#22909;&#30456;&#31526;&#30340;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#12290;&#26412;&#25991;&#21463;&#21040;LLM&#27169;&#22411;&#20013;LLM&#20316;&#20026;&#27861;&#23448;&#30340;&#21551;&#21457;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#34987;&#31216;&#20026;MLLM&#20316;&#20026;&#27861;&#23448;&#65292;&#29992;&#20110;&#35780;&#20272;MLLM&#22312;&#21327;&#21161;&#27861;&#23448;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#19977;&#20010;&#19981;&#21516;&#30340;&#20219;&#21153;&#65306;&#35780;&#20998;&#35780;&#20272;&#12289;&#23545;&#27604;&#35780;&#20272;&#21644;&#25209;&#37327;&#25490;&#24207;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#34429;&#28982;MLLM&#22312;&#23545;&#27604;&#35780;&#20272;&#26041;&#38754;&#23637;&#31034;&#20986;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#31867;&#20154;&#36776;&#21035;&#33021;&#21147;&#65292;&#20294;&#22312;&#35780;&#20998;&#35780;&#20272;&#21644;&#25209;&#37327;&#25490;&#24207;&#20219;&#21153;&#20013;&#19982;&#20154;&#31867;&#20559;&#22909;&#23384;&#22312;&#26174;&#33879;&#30340;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;&#23545;&#20110;&#20687;GPT-4V&#36825;&#26679;&#30340;&#20808;&#36827;&#27169;&#22411;&#65292;MLLM&#20173;&#28982;&#38754;&#20020;&#30528;&#21028;&#26029;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#22810;&#26679;&#30340;&#20559;&#35265;&#12289;&#24187;&#35273;&#24335;&#30340;&#22238;&#31572;&#21644;&#19981;&#19968;&#33268;&#24615;&#12290;&#36825;&#20123;&#21457;&#29616;&#24378;&#35843;&#20102;&#23545;MLLM&#30340;&#25913;&#36827;&#21644;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#36843;&#20999;&#38656;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal Large Language Models (MLLMs) have gained significant attention recently, showing remarkable potential in artificial general intelligence. However, assessing the utility of MLLMs presents considerable challenges, primarily due to the absence multimodal benchmarks that align with human preferences. Inspired by LLM-as-a-Judge in LLMs, this paper introduces a novel benchmark, termed MLLM-as-a-Judge, to assess the ability of MLLMs in assisting judges including three distinct tasks: Scoring Evaluation, Pair Comparison, and Batch Ranking. Our study reveals that, while MLLMs demonstrate remarkable human-like discernment in Pair Comparisons, there is a significant divergence from human preferences in Scoring Evaluation and Batch Ranking tasks. Furthermore, MLLMs still face challenges in judgment, including diverse biases, hallucinatory responses, and inconsistencies, even for advanced models such as GPT-4V. These findings emphasize the pressing need for enhancements and further rese
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Auxiliary-Delayed Reinforcement Learning (AD-RL)&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#36741;&#21161;&#30340;&#30701;&#26102;&#24310;&#20219;&#21153;&#26469;&#21152;&#36895;&#38271;&#26102;&#24310;&#20219;&#21153;&#30340;&#23398;&#20064;&#36807;&#31243;&#65292;&#21516;&#26102;&#22312;&#38543;&#26426;&#29615;&#22659;&#20013;&#20445;&#25345;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#33021;&#26174;&#33879;&#38477;&#20302;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#24182;&#22312;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.03141</link><description>&lt;p&gt;
&#20351;&#29992;&#36741;&#21161;&#30701;&#26102;&#24310;&#20219;&#21153;&#25552;&#21319;&#38271;&#26102;&#24310;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Boosting Long-Delayed Reinforcement Learning with Auxiliary Short-Delayed Task
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03141
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Auxiliary-Delayed Reinforcement Learning (AD-RL)&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#36741;&#21161;&#30340;&#30701;&#26102;&#24310;&#20219;&#21153;&#26469;&#21152;&#36895;&#38271;&#26102;&#24310;&#20219;&#21153;&#30340;&#23398;&#20064;&#36807;&#31243;&#65292;&#21516;&#26102;&#22312;&#38543;&#26426;&#29615;&#22659;&#20013;&#20445;&#25345;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#33021;&#26174;&#33879;&#38477;&#20302;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#24182;&#22312;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24310;&#36831;&#24773;&#26223;&#19979;&#30340;&#24378;&#21270;&#23398;&#20064;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#24310;&#36831;&#24773;&#26223;&#26159;&#25351;&#35266;&#23519;&#21644;&#20132;&#20114;&#23384;&#22312;&#24310;&#36831;&#30340;&#24120;&#35265;&#23454;&#38469;&#24773;&#20917;&#12290;&#29616;&#26377;&#25216;&#26415;&#20013;&#65292;&#29366;&#24577;&#22686;&#24378;&#25216;&#26415;&#22312;&#24310;&#36831;&#27493;&#39588;&#20013;&#21487;&#33021;&#20250;&#20986;&#29616;&#29366;&#24577;&#31354;&#38388;&#25193;&#22823;&#25110;&#22312;&#38543;&#26426;&#29615;&#22659;&#20013;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Auxiliary-Delayed Reinforcement Learning&#65288;AD-RL&#65289;&#65292;&#21033;&#29992;&#19968;&#20010;&#36741;&#21161;&#30340;&#30701;&#26102;&#24310;&#20219;&#21153;&#26469;&#21152;&#36895;&#38271;&#26102;&#24310;&#20219;&#21153;&#30340;&#23398;&#20064;&#65292;&#21516;&#26102;&#19981;&#25439;&#23475;&#22312;&#38543;&#26426;&#29615;&#22659;&#20013;&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;AD-RL&#22312;&#30701;&#26102;&#24310;&#20219;&#21153;&#20013;&#23398;&#20064;&#20540;&#20989;&#25968;&#65292;&#28982;&#21518;&#23558;&#20854;&#19982;&#38271;&#26102;&#24310;&#20219;&#21153;&#20013;&#30340;&#33258;&#20030;&#21644;&#31574;&#30053;&#25913;&#36827;&#25216;&#26415;&#32467;&#21512;&#36215;&#26469;&#12290;&#25105;&#20204;&#29702;&#35770;&#19978;&#35777;&#26126;&#65292;&#19982;&#30452;&#25509;&#22312;&#21407;&#22987;&#38271;&#26102;&#24310;&#20219;&#21153;&#19978;&#23398;&#20064;&#30456;&#27604;&#65292;&#36825;&#26679;&#20570;&#21487;&#20197;&#22823;&#22823;&#20943;&#23567;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#22312;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26679;&#26412;&#25928;&#29575;&#21644;&#24615;&#33021;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning is challenging in delayed scenarios, a common real-world situation where observations and interactions occur with delays. State-of-the-art (SOTA) state-augmentation techniques either suffer from the state-space explosion along with the delayed steps, or performance degeneration in stochastic environments. To address these challenges, our novel Auxiliary-Delayed Reinforcement Learning (AD-RL) leverages an auxiliary short-delayed task to accelerate the learning on a long-delayed task without compromising the performance in stochastic environments. Specifically, AD-RL learns the value function in the short-delayed task and then employs it with the bootstrapping and policy improvement techniques in the long-delayed task. We theoretically show that this can greatly reduce the sample complexity compared to directly learning on the original long-delayed task. On deterministic and stochastic benchmarks, our method remarkably outperforms the SOTAs in both sample efficienc
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24191;&#27867;&#23454;&#39564;&#21457;&#29616;&#22522;&#20110;&#28857;&#20113;&#30340;&#26041;&#27861;&#22312;&#26426;&#22120;&#20154;&#23398;&#20064;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#21508;&#31181;&#39044;&#35757;&#32451;&#21644;&#27867;&#21270;&#20219;&#21153;&#20013;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#28857;&#20113;&#35266;&#27979;&#27169;&#24577;&#23545;&#20110;&#22797;&#26434;&#26426;&#22120;&#20154;&#20219;&#21153;&#26159;&#26377;&#20215;&#20540;&#30340;&#12290;</title><link>https://arxiv.org/abs/2402.02500</link><description>&lt;p&gt;
&#28857;&#20113;&#38382;&#39064;:&#37325;&#26032;&#24605;&#32771;&#19981;&#21516;&#35266;&#27979;&#31354;&#38388;&#23545;&#26426;&#22120;&#20154;&#23398;&#20064;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Point Cloud Matters: Rethinking the Impact of Different Observation Spaces on Robot Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02500
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24191;&#27867;&#23454;&#39564;&#21457;&#29616;&#22522;&#20110;&#28857;&#20113;&#30340;&#26041;&#27861;&#22312;&#26426;&#22120;&#20154;&#23398;&#20064;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#21508;&#31181;&#39044;&#35757;&#32451;&#21644;&#27867;&#21270;&#20219;&#21153;&#20013;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#28857;&#20113;&#35266;&#27979;&#27169;&#24577;&#23545;&#20110;&#22797;&#26434;&#26426;&#22120;&#20154;&#20219;&#21153;&#26159;&#26377;&#20215;&#20540;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19981;&#21516;&#35266;&#27979;&#31354;&#38388;&#23545;&#26426;&#22120;&#20154;&#23398;&#20064;&#30340;&#24433;&#21709;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#19977;&#31181;&#20027;&#35201;&#27169;&#24577;&#65306;RGB&#65292;RGB-D&#21644;&#28857;&#20113;&#12290;&#36890;&#36807;&#22312;&#36229;&#36807;17&#20010;&#19981;&#21516;&#25509;&#35302;&#20016;&#23500;&#30340;&#25805;&#20316;&#20219;&#21153;&#19978;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#28041;&#21450;&#20004;&#20010;&#22522;&#20934;&#21644;&#20223;&#30495;&#22120;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20102;&#19968;&#20010;&#26174;&#33879;&#30340;&#36235;&#21183;&#65306;&#22522;&#20110;&#28857;&#20113;&#30340;&#26041;&#27861;&#65292;&#21363;&#20351;&#26159;&#26368;&#31616;&#21333;&#30340;&#35774;&#35745;&#65292;&#36890;&#24120;&#22312;&#24615;&#33021;&#19978;&#36229;&#36807;&#20102;&#20854;RGB&#21644;RGB-D&#30340;&#23545;&#24212;&#29289;&#12290;&#36825;&#22312;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#21644;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#20004;&#31181;&#24773;&#20917;&#19979;&#37117;&#26159;&#19968;&#33268;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#28857;&#20113;&#35266;&#27979;&#22312;&#30456;&#26426;&#35270;&#35282;&#12289;&#29031;&#26126;&#26465;&#20214;&#12289;&#22122;&#22768;&#27700;&#24179;&#21644;&#32972;&#26223;&#22806;&#35266;&#31561;&#21508;&#31181;&#20960;&#20309;&#21644;&#35270;&#35273;&#32447;&#32034;&#26041;&#38754;&#65292;&#37117;&#33021;&#25552;&#39640;&#31574;&#30053;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#19977;&#32500;&#28857;&#20113;&#26159;&#22797;&#26434;&#26426;&#22120;&#20154;&#20219;&#21153;&#20013;&#26377;&#20215;&#20540;&#30340;&#35266;&#27979;&#27169;&#24577;&#12290;&#25105;&#20204;&#23558;&#20844;&#24320;&#25152;&#26377;&#30340;&#20195;&#30721;&#21644;&#26816;&#26597;&#28857;&#65292;&#24076;&#26395;&#25105;&#20204;&#30340;&#35266;&#28857;&#33021;&#24110;&#21161;&#35299;&#20915;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we explore the influence of different observation spaces on robot learning, focusing on three predominant modalities: RGB, RGB-D, and point cloud. Through extensive experimentation on over 17 varied contact-rich manipulation tasks, conducted across two benchmarks and simulators, we have observed a notable trend: point cloud-based methods, even those with the simplest designs, frequently surpass their RGB and RGB-D counterparts in performance. This remains consistent in both scenarios: training from scratch and utilizing pretraining. Furthermore, our findings indicate that point cloud observations lead to improved policy zero-shot generalization in relation to various geometry and visual clues, including camera viewpoints, lighting conditions, noise levels and background appearance. The outcomes suggest that 3D point cloud is a valuable observation modality for intricate robotic tasks. We will open-source all our codes and checkpoints, hoping that our insights can help de
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#23500;&#26377;&#34920;&#29616;&#21147;&#21644;&#21487;&#22788;&#29702;&#30340;&#27010;&#29575;&#29983;&#25104;&#24314;&#27169;&#39046;&#22495;&#30340;&#36827;&#23637;&#21644;&#25216;&#26415;&#65292;&#24182;&#37325;&#28857;&#20851;&#27880;&#20102;&#27010;&#29575;&#30005;&#36335;&#12290;&#25991;&#31456;&#25552;&#20379;&#20102;&#20851;&#20110;&#34920;&#36798;&#33021;&#21147;&#21644;&#21487;&#22788;&#29702;&#24615;&#20043;&#38388;&#26435;&#34913;&#30340;&#32479;&#19968;&#35270;&#35282;&#65292;&#24182;&#35828;&#26126;&#20102;&#35774;&#35745;&#21407;&#21017;&#21644;&#31639;&#27861;&#25193;&#23637;&#65292;&#25104;&#21151;&#22320;&#26500;&#24314;&#20102;&#23500;&#26377;&#34920;&#29616;&#21147;&#21644;&#39640;&#25928;&#30340;&#27010;&#29575;&#30005;&#36335;&#12290;&#27492;&#22806;&#65292;&#25991;&#31456;&#36824;&#35752;&#35770;&#20102;&#26368;&#26032;&#30340;&#28145;&#24230;&#21644;&#28151;&#21512;&#27010;&#29575;&#30005;&#36335;&#30740;&#31350;&#65292;&#24182;&#27010;&#36848;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#25361;&#25112;&#21644;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.00759</link><description>&lt;p&gt;
&#26500;&#24314;&#23500;&#26377;&#34920;&#29616;&#21147;&#21644;&#21487;&#22788;&#29702;&#30340;&#27010;&#29575;&#29983;&#25104;&#27169;&#22411;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Building Expressive and Tractable Probabilistic Generative Models: A Review
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00759
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#23500;&#26377;&#34920;&#29616;&#21147;&#21644;&#21487;&#22788;&#29702;&#30340;&#27010;&#29575;&#29983;&#25104;&#24314;&#27169;&#39046;&#22495;&#30340;&#36827;&#23637;&#21644;&#25216;&#26415;&#65292;&#24182;&#37325;&#28857;&#20851;&#27880;&#20102;&#27010;&#29575;&#30005;&#36335;&#12290;&#25991;&#31456;&#25552;&#20379;&#20102;&#20851;&#20110;&#34920;&#36798;&#33021;&#21147;&#21644;&#21487;&#22788;&#29702;&#24615;&#20043;&#38388;&#26435;&#34913;&#30340;&#32479;&#19968;&#35270;&#35282;&#65292;&#24182;&#35828;&#26126;&#20102;&#35774;&#35745;&#21407;&#21017;&#21644;&#31639;&#27861;&#25193;&#23637;&#65292;&#25104;&#21151;&#22320;&#26500;&#24314;&#20102;&#23500;&#26377;&#34920;&#29616;&#21147;&#21644;&#39640;&#25928;&#30340;&#27010;&#29575;&#30005;&#36335;&#12290;&#27492;&#22806;&#65292;&#25991;&#31456;&#36824;&#35752;&#35770;&#20102;&#26368;&#26032;&#30340;&#28145;&#24230;&#21644;&#28151;&#21512;&#27010;&#29575;&#30005;&#36335;&#30740;&#31350;&#65292;&#24182;&#27010;&#36848;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#25361;&#25112;&#21644;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#21487;&#22788;&#29702;&#30340;&#27010;&#29575;&#29983;&#25104;&#24314;&#27169;&#39046;&#22495;&#20013;&#30340;&#36827;&#23637;&#21644;&#25216;&#26415;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35843;&#26597;&#65292;&#37325;&#28857;&#20851;&#27880;&#27010;&#29575;&#30005;&#36335;&#65288;PCs&#65289;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#34920;&#36798;&#33021;&#21147;&#21644;&#21487;&#22788;&#29702;&#24615;&#20043;&#38388;&#22266;&#26377;&#26435;&#34913;&#30340;&#32479;&#19968;&#35270;&#35282;&#65292;&#31361;&#20986;&#20102;&#20351;PCs&#23500;&#26377;&#34920;&#29616;&#21147;&#21644;&#39640;&#25928;&#30340;&#35774;&#35745;&#21407;&#21017;&#21644;&#31639;&#27861;&#25193;&#23637;&#65292;&#24182;&#25552;&#20379;&#20102;&#35813;&#39046;&#22495;&#30340;&#20998;&#31867;&#27861;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#26368;&#36817;&#36890;&#36807;&#34701;&#21512;&#28145;&#24230;&#31070;&#32463;&#27169;&#22411;&#27010;&#24565;&#26469;&#26500;&#24314;&#28145;&#24230;&#21644;&#28151;&#21512;PCs&#30340;&#21162;&#21147;&#65292;&#24182;&#27010;&#36848;&#20102;&#25351;&#23548;&#26410;&#26469;&#30740;&#31350;&#30340;&#25361;&#25112;&#21644;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a comprehensive survey of the advancements and techniques in the field of tractable probabilistic generative modeling, primarily focusing on Probabilistic Circuits (PCs). We provide a unified perspective on the inherent trade-offs between expressivity and the tractability, highlighting the design principles and algorithmic extensions that have enabled building expressive and efficient PCs, and provide a taxonomy of the field. We also discuss recent efforts to build deep and hybrid PCs by fusing notions from deep neural models, and outline the challenges and open questions that can guide future research in this evolving field.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AudioSeal&#30340;&#38899;&#39057;&#27700;&#21360;&#25216;&#26415;&#65292;&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#22120;/&#26816;&#27979;&#22120;&#26550;&#26500;&#21644;&#26412;&#22320;&#21270;&#25439;&#22833;&#35757;&#32451;&#26469;&#23454;&#29616;&#26412;&#22320;&#21270;&#27700;&#21360;&#26816;&#27979;&#65292;&#21516;&#26102;&#24341;&#29992;&#20102;&#21463;&#21548;&#35273;&#23631;&#34109;&#21551;&#21457;&#30340;&#24863;&#30693;&#25439;&#22833;&#65292;&#20197;&#25552;&#39640;&#19981;&#21487;&#23519;&#35273;&#24615;&#12290;&#27492;&#25216;&#26415;&#22312;&#33258;&#21160;&#21644;&#20154;&#24037;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#20855;&#26377;&#24555;&#36895;&#30340;&#21333;&#36890;&#36947;&#26816;&#27979;&#22120;&#65292;&#21487;&#22312;&#22823;&#35268;&#27169;&#21644;&#23454;&#26102;&#24212;&#29992;&#20013;&#23454;&#29616;&#26356;&#24555;&#30340;&#26816;&#27979;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2401.17264</link><description>&lt;p&gt;
&#38024;&#23545;&#26412;&#22320;&#21270;&#27700;&#21360;&#25216;&#26415;&#30340;&#22768;&#38899;&#20811;&#38534;&#20027;&#21160;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Proactive Detection of Voice Cloning with Localized Watermarking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17264
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AudioSeal&#30340;&#38899;&#39057;&#27700;&#21360;&#25216;&#26415;&#65292;&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#22120;/&#26816;&#27979;&#22120;&#26550;&#26500;&#21644;&#26412;&#22320;&#21270;&#25439;&#22833;&#35757;&#32451;&#26469;&#23454;&#29616;&#26412;&#22320;&#21270;&#27700;&#21360;&#26816;&#27979;&#65292;&#21516;&#26102;&#24341;&#29992;&#20102;&#21463;&#21548;&#35273;&#23631;&#34109;&#21551;&#21457;&#30340;&#24863;&#30693;&#25439;&#22833;&#65292;&#20197;&#25552;&#39640;&#19981;&#21487;&#23519;&#35273;&#24615;&#12290;&#27492;&#25216;&#26415;&#22312;&#33258;&#21160;&#21644;&#20154;&#24037;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#20855;&#26377;&#24555;&#36895;&#30340;&#21333;&#36890;&#36947;&#26816;&#27979;&#22120;&#65292;&#21487;&#22312;&#22823;&#35268;&#27169;&#21644;&#23454;&#26102;&#24212;&#29992;&#20013;&#23454;&#29616;&#26356;&#24555;&#30340;&#26816;&#27979;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35821;&#38899;&#29983;&#25104;&#27169;&#22411;&#24555;&#36895;&#21457;&#23637;&#30340;&#39046;&#22495;&#20013;&#65292;&#23545;&#22768;&#38899;&#20811;&#38534;&#39118;&#38505;&#36827;&#34892;&#22768;&#38899;&#30495;&#23454;&#24615;&#20445;&#35777;&#21464;&#24471;&#36843;&#20999;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;AudioSeal&#65292;&#36825;&#26159;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#26412;&#22320;&#21270;&#26816;&#27979;AI&#29983;&#25104;&#35821;&#38899;&#30340;&#38899;&#39057;&#27700;&#21360;&#25216;&#26415;&#12290;AudioSeal&#37319;&#29992;&#20102;&#29983;&#25104;&#22120;/&#26816;&#27979;&#22120;&#26550;&#26500;&#65292;&#19982;&#26412;&#22320;&#21270;&#25439;&#22833;&#19968;&#36215;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#23454;&#29616;&#37319;&#26679;&#32423;&#21035;&#30340;&#26412;&#22320;&#21270;&#27700;&#21360;&#26816;&#27979;&#65292;&#24182;&#37319;&#29992;&#20102;&#21463;&#21548;&#35273;&#23631;&#34109;&#21551;&#21457;&#30340;&#26032;&#22411;&#24863;&#30693;&#25439;&#22833;&#65292;&#20351;AudioSeal&#33021;&#22815;&#26356;&#22909;&#22320;&#23454;&#29616;&#19981;&#21487;&#23519;&#35273;&#24615;&#12290;&#22312;&#33258;&#21160;&#21644;&#20154;&#24037;&#35780;&#20272;&#25351;&#26631;&#19978;&#65292;AudioSeal&#22312;&#30495;&#23454;&#29983;&#27963;&#38899;&#39057;&#22788;&#29702;&#21644;&#19981;&#21487;&#23519;&#35273;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;AudioSeal&#37319;&#29992;&#20102;&#24555;&#36895;&#30340;&#21333;&#36890;&#36947;&#26816;&#27979;&#22120;&#65292;&#22823;&#22823;&#36229;&#36807;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#36895;&#24230;&#65292;&#23454;&#29616;&#20102;&#20004;&#20010;&#25968;&#37327;&#32423;&#30340;&#26356;&#24555;&#26816;&#27979;&#65292;&#20351;&#20854;&#25104;&#20026;&#22823;&#35268;&#27169;&#21644;&#23454;&#26102;&#24212;&#29992;&#30340;&#29702;&#24819;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the rapidly evolving field of speech generative models, there is a pressing need to ensure audio authenticity against the risks of voice cloning. We present AudioSeal, the first audio watermarking technique designed specifically for localized detection of AI-generated speech. AudioSeal employs a generator/detector architecture trained jointly with a localization loss to enable localized watermark detection up to the sample level, and a novel perceptual loss inspired by auditory masking, that enables AudioSeal to achieve better imperceptibility. AudioSeal achieves state-of-the-art performance in terms of robustness to real life audio manipulations and imperceptibility based on automatic and human evaluation metrics. Additionally, AudioSeal is designed with a fast, single-pass detector, that significantly surpasses existing models in speed - achieving detection up to two orders of magnitude faster, making it ideal for large-scale and real-time applications.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#25552;&#31034;&#20248;&#21270;&#31639;&#27861;&#65288;RPO&#65289;&#29992;&#20110;&#23545;&#25239;&#35821;&#35328;&#27169;&#22411;&#30340;&#30772;&#35299;&#25915;&#20987;&#65292;&#36890;&#36807;&#26799;&#24230;&#20248;&#21270;&#26469;&#30830;&#20445;&#36755;&#20986;&#30340;&#26080;&#23475;&#24615;&#65292;&#24182;&#25104;&#21151;&#38477;&#20302;&#20102;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;</title><link>https://arxiv.org/abs/2401.17263</link><description>&lt;p&gt;
&#40065;&#26834;&#30340;&#25552;&#31034;&#20248;&#21270;&#29992;&#20110;&#23545;&#25239;&#35821;&#35328;&#27169;&#22411;&#30340;&#30772;&#35299;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Robust Prompt Optimization for Defending Language Models Against Jailbreaking Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17263
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#25552;&#31034;&#20248;&#21270;&#31639;&#27861;&#65288;RPO&#65289;&#29992;&#20110;&#23545;&#25239;&#35821;&#35328;&#27169;&#22411;&#30340;&#30772;&#35299;&#25915;&#20987;&#65292;&#36890;&#36807;&#26799;&#24230;&#20248;&#21270;&#26469;&#30830;&#20445;&#36755;&#20986;&#30340;&#26080;&#23475;&#24615;&#65292;&#24182;&#25104;&#21151;&#38477;&#20302;&#20102;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#20154;&#24037;&#26234;&#33021;&#23545;&#40784;&#26041;&#38754;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#25110;&#30772;&#35299;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#20854;&#20013;&#23545;&#25163;&#20462;&#25913;&#36755;&#20837;&#25552;&#31034;&#20197;&#35825;&#23548;&#26377;&#23475;&#34892;&#20026;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#38450;&#24481;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#20165;&#20851;&#27880;&#29421;&#31364;&#30340;&#23041;&#32961;&#27169;&#22411;&#65292;&#24182;&#19981;&#33021;&#25552;&#20379;&#24378;&#22823;&#30340;&#38450;&#24481;&#12290;&#20026;&#20102;&#23454;&#29616;&#24378;&#22823;&#30340;&#38450;&#24481;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#29992;&#20110;&#23545;&#25239;&#30772;&#35299;&#25915;&#20987;&#30340;&#23545;&#25239;&#30446;&#26631;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#40065;&#26834;&#25552;&#31034;&#20248;&#21270;&#65288;RPO&#65289;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#22522;&#20110;&#26799;&#24230;&#30340;&#20196;&#29260;&#20248;&#21270;&#26469;&#30830;&#20445;&#36755;&#20986;&#30340;&#26080;&#23475;&#24615;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#19968;&#20010;&#26131;&#20110;&#35775;&#38382;&#30340;&#21518;&#32512;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#23545;&#30772;&#35299;&#25915;&#20987;&#30340;&#24378;&#38887;&#24615;&#65292;&#21253;&#25324;&#20248;&#21270;&#36807;&#31243;&#20013;&#20986;&#29616;&#30340;&#30772;&#35299;&#25915;&#20987;&#20197;&#21450;&#26410;&#30693;&#30340;&#30772;&#35299;&#25915;&#20987;&#65292;&#23558;&#25915;&#20987;&#25104;&#21151;&#29575;&#20174;84%&#38477;&#20302;&#21040;8.66%&#65292;&#22312;20&#20010;&#30772;&#35299;&#25915;&#20987;&#20013;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;RPO&#23545;&#27491;&#24120;LM&#20351;&#29992;&#30340;&#24433;&#21709;&#36739;&#23567;&#65292;&#22312;&#36866;&#24212;&#24615;&#25915;&#20987;&#19979;&#20173;&#28982;&#26377;&#25928;&#65292;&#24182;&#19988;&#21487;&#20197;&#36801;&#31227;&#21040;&#40657;&#30418;&#27169;&#22411;&#20013;&#65292;&#38477;&#20302;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite advances in AI alignment, language models (LM) remain vulnerable to adversarial attacks or jailbreaking, in which adversaries modify input prompts to induce harmful behavior. While some defenses have been proposed, they focus on narrow threat models and fall short of a strong defense, which we posit should be effective, universal, and practical. To achieve this, we propose the first adversarial objective for defending LMs against jailbreaking attacks and an algorithm, robust prompt optimization (RPO), that uses gradient-based token optimization to enforce harmless outputs. This results in an easily accessible suffix that significantly improves robustness to both jailbreaks seen during optimization and unknown, held-out jailbreaks, reducing the attack success rate on Starling-7B from 84% to 8.66% across 20 jailbreaks. In addition, we find that RPO has a minor effect on normal LM use, is successful under adaptive attacks, and can transfer to black-box models, reducing the success
&lt;/p&gt;</description></item><item><title>MELA&#26159;&#31532;&#19968;&#20010;&#35206;&#30422;10&#31181;&#35821;&#35328;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#21487;&#25509;&#21463;&#24615;&#22522;&#20934;&#65292;&#36890;&#36807;&#20998;&#26512;XLM-R&#30340;&#24494;&#35843;&#26435;&#37325;&#65292;&#25506;&#35752;&#20102;&#36328;&#35821;&#35328;&#36801;&#31227;&#22256;&#38590;&#24615;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#19978;&#19979;&#25991;&#31034;&#20363;&#26041;&#38754;ChatGPT&#34920;&#29616;&#33391;&#22909;&#20294;&#20173;&#33853;&#21518;&#20110;&#32463;&#36807;&#24494;&#35843;&#30340;XLM-R&#12290;</title><link>https://arxiv.org/abs/2311.09033</link><description>&lt;p&gt;
MELA&#65306;&#22810;&#35821;&#35328;&#35821;&#35328;&#21487;&#25509;&#21463;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
MELA: Multilingual Evaluation of Linguistic Acceptability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09033
&lt;/p&gt;
&lt;p&gt;
MELA&#26159;&#31532;&#19968;&#20010;&#35206;&#30422;10&#31181;&#35821;&#35328;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#21487;&#25509;&#21463;&#24615;&#22522;&#20934;&#65292;&#36890;&#36807;&#20998;&#26512;XLM-R&#30340;&#24494;&#35843;&#26435;&#37325;&#65292;&#25506;&#35752;&#20102;&#36328;&#35821;&#35328;&#36801;&#31227;&#22256;&#38590;&#24615;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#19978;&#19979;&#25991;&#31034;&#20363;&#26041;&#38754;ChatGPT&#34920;&#29616;&#33391;&#22909;&#20294;&#20173;&#33853;&#21518;&#20110;&#32463;&#36807;&#24494;&#35843;&#30340;XLM-R&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#22522;&#20934;&#20027;&#35201;&#38598;&#20013;&#22312;&#24212;&#29992;&#39537;&#21160;&#30340;&#20219;&#21153;&#65292;&#22914;&#22797;&#26434;&#25512;&#29702;&#21644;&#20195;&#30721;&#29983;&#25104;&#19978;&#65292;&#23548;&#33268;LLMs&#30340;&#32431;&#35821;&#35328;&#35780;&#20272;&#20005;&#37325;&#19981;&#36275;&#12290;&#38024;&#23545;&#36825;&#19968;&#32972;&#26223;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Multilingual Evaluation of Linguistic Acceptability&#65288;MELA&#65289;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#28085;&#30422;&#26469;&#33258;&#22810;&#20010;&#35821;&#35328;&#23478;&#26063;&#30340;10&#31181;&#35821;&#35328;&#12289;&#20849;48K&#20010;&#26679;&#26412;&#30340;&#35821;&#35328;&#21487;&#25509;&#21463;&#24615;&#22810;&#35821;&#35328;&#22522;&#20934;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#24120;&#29992;LLMs&#21644;&#30417;&#30563;&#27169;&#22411;&#30340;&#22522;&#32447;&#65292;&#20351;&#29992;XLM-R&#36827;&#34892;&#36328;&#35821;&#35328;&#36801;&#31227;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#23454;&#39564;&#12290;&#20026;&#20102;&#23454;&#29616;&#22810;&#35821;&#35328;&#21487;&#35299;&#37322;&#24615;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#24494;&#35843;&#21518;&#30340;XLM-R&#30340;&#26435;&#37325;&#65292;&#25506;&#35752;&#20102;&#35782;&#21035;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#36801;&#31227;&#22256;&#38590;&#24615;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;ChatGPT&#20174;&#19978;&#19979;&#25991;&#31034;&#20363;&#20013;&#21463;&#30410;&#33391;&#22810;&#65292;&#20294;&#20173;&#33853;&#21518;&#20110;&#32463;&#36807;&#24494;&#35843;&#30340;XLM-R&#65292;&#32780;GPT-4&#30340;&#24615;&#33021;&#19982;&#20043;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09033v2 Announce Type: replace-cross  Abstract: Recent benchmarks for Large Language Models (LLMs) have mostly focused on application-driven tasks such as complex reasoning and code generation, and this has led to a scarcity in purely linguistic evaluation of LLMs. Against this background, we introduce Multilingual Evaluation of Linguistic Acceptability -- MELA, the first multilingual benchmark on linguistic acceptability with 48K samples covering 10 languages from a diverse set of language families. We establish baselines of commonly used LLMs along with supervised models, and conduct cross-lingual transfer and multi-task learning experiments with XLM-R. In pursuit of multilingual interpretability, we analyze the weights of fine-tuned XLM-R to explore the possibility of identifying transfer difficulty between languages. Our results show that ChatGPT benefits much from in-context examples but still lags behind fine-tuned XLM-R, while the performance of GPT-4 is on par with f
&lt;/p&gt;</description></item><item><title>ReGAL&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21457;&#29616;&#36890;&#29992;&#25277;&#35937;&#30340;&#31243;&#24207;&#37325;&#26500;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#37325;&#26500;&#20195;&#30721;&#23398;&#20064;&#21487;&#37325;&#29992;&#30340;&#20989;&#25968;&#24211;&#65292;&#21033;&#29992;&#36825;&#20123;&#20849;&#20139;&#20989;&#25968;&#24211;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#31243;&#24207;&#12290;</title><link>http://arxiv.org/abs/2401.16467</link><description>&lt;p&gt;
ReGAL: &#29992;&#20110;&#21457;&#29616;&#36890;&#29992;&#25277;&#35937;&#30340;&#31243;&#24207;&#37325;&#26500;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ReGAL: Refactoring Programs to Discover Generalizable Abstractions. (arXiv:2401.16467v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16467
&lt;/p&gt;
&lt;p&gt;
ReGAL&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21457;&#29616;&#36890;&#29992;&#25277;&#35937;&#30340;&#31243;&#24207;&#37325;&#26500;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#37325;&#26500;&#20195;&#30721;&#23398;&#20064;&#21487;&#37325;&#29992;&#30340;&#20989;&#25968;&#24211;&#65292;&#21033;&#29992;&#36825;&#20123;&#20849;&#20139;&#20989;&#25968;&#24211;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#29992;&#20110;&#31243;&#24207;&#21512;&#25104;&#65292;&#20294;&#23427;&#20204;&#32570;&#20047;&#24320;&#21457;&#26377;&#29992;&#25277;&#35937;&#25152;&#38656;&#30340;&#20840;&#23616;&#35270;&#35282;&#65307;&#23427;&#20204;&#36890;&#24120;&#19968;&#27425;&#39044;&#27979;&#19968;&#20010;&#31243;&#24207;&#65292;&#32463;&#24120;&#37325;&#22797;&#30456;&#21516;&#30340;&#21151;&#33021;&#12290;&#20174;&#22836;&#24320;&#22987;&#29983;&#25104;&#20887;&#20313;&#20195;&#30721;&#26082;&#20302;&#25928;&#21448;&#23481;&#26131;&#20986;&#38169;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#36890;&#29992;&#25277;&#35937;&#23398;&#20064;&#30340;&#37325;&#26500;&#26041;&#27861;&#65288;ReGAL&#65289;&#65292;&#36890;&#36807;&#20195;&#30721;&#37325;&#26500;&#26469;&#23398;&#20064;&#21487;&#37325;&#29992;&#20989;&#25968;&#24211;&#65292;&#21363;&#22312;&#19981;&#25913;&#21464;&#20195;&#30721;&#25191;&#34892;&#36755;&#20986;&#30340;&#24773;&#20917;&#19979;&#37325;&#32452;&#20195;&#30721;&#12290;ReGAL&#20174;&#19968;&#23567;&#32452;&#29616;&#26377;&#31243;&#24207;&#20013;&#23398;&#20064;&#65292;&#36890;&#36807;&#25191;&#34892;&#39564;&#35777;&#21644;&#32454;&#21270;&#25277;&#35937;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;ReGAL&#21457;&#29616;&#30340;&#20849;&#20139;&#20989;&#25968;&#24211;&#20351;&#24471;&#22312;&#19981;&#21516;&#39046;&#22495;&#39044;&#27979;&#31243;&#24207;&#21464;&#24471;&#26356;&#21152;&#23481;&#26131;&#12290;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#65288;LOGO&#22270;&#24418;&#29983;&#25104;&#12289;&#26085;&#26399;&#25512;&#29702;&#21644;&#22522;&#20110;Minecraft&#30340;&#25991;&#23383;&#28216;&#25103;TextCraft&#65289;&#19978;&#65292;&#24320;&#28304;&#21644;&#19987;&#26377;&#30340;LLMs&#22312;&#20351;&#29992;ReGAL&#20989;&#25968;&#24211;&#39044;&#27979;&#31243;&#24207;&#26102;&#20934;&#30830;&#24615;&#24471;&#21040;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
While large language models (LLMs) are increasingly being used for program synthesis, they lack the global view needed to develop useful abstractions; they generally predict programs one at a time, often repeating the same functionality. Generating redundant code from scratch is both inefficient and error-prone. To address this, we propose Refactoring for Generalizable Abstraction Learning (ReGAL), a gradient-free method for learning a library of reusable functions via code refactorization, i.e. restructuring code without changing its execution output. ReGAL learns from a small set of existing programs, iteratively verifying and refining its abstractions via execution. We find that the shared function libraries discovered by ReGAL make programs easier to predict across diverse domains. On three datasets (LOGO graphics generation, Date reasoning, and TextCraft, a Minecraft-based text game), both open-source and proprietary LLMs improve in accuracy when predicting programs with ReGAL fun
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;&#20309;&#21033;&#29992;&#28304;&#35821;&#35328;&#21644;&#21442;&#32771;&#20449;&#24687;&#35780;&#20272;&#26426;&#22120;&#32763;&#35793;&#30340;&#36136;&#37327;&#65292;&#24182;&#21457;&#29616;&#21442;&#32771;&#20449;&#24687;&#26174;&#33879;&#25552;&#39640;&#20102;&#35780;&#20272;&#20934;&#30830;&#24615;&#65292;&#32780;&#28304;&#35821;&#35328;&#20449;&#24687;&#26377;&#26102;&#20250;&#36866;&#24471;&#20854;&#21453;&#65292;&#34920;&#26126;&#22312;&#20351;&#29992;LLMs&#35780;&#20272;&#32763;&#35793;&#26102;&#23384;&#22312;&#36328;&#35821;&#35328;&#33021;&#21147;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.06568</link><description>&lt;p&gt;
&#22312;&#28304;&#35821;&#35328;&#20013;&#36855;&#22833;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#35780;&#20272;&#26426;&#22120;&#32763;&#35793;&#30340;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
Lost in the Source Language: How Large Language Models Evaluate the Quality of Machine Translation. (arXiv:2401.06568v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06568
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;&#20309;&#21033;&#29992;&#28304;&#35821;&#35328;&#21644;&#21442;&#32771;&#20449;&#24687;&#35780;&#20272;&#26426;&#22120;&#32763;&#35793;&#30340;&#36136;&#37327;&#65292;&#24182;&#21457;&#29616;&#21442;&#32771;&#20449;&#24687;&#26174;&#33879;&#25552;&#39640;&#20102;&#35780;&#20272;&#20934;&#30830;&#24615;&#65292;&#32780;&#28304;&#35821;&#35328;&#20449;&#24687;&#26377;&#26102;&#20250;&#36866;&#24471;&#20854;&#21453;&#65292;&#34920;&#26126;&#22312;&#20351;&#29992;LLMs&#35780;&#20272;&#32763;&#35793;&#26102;&#23384;&#22312;&#36328;&#35821;&#35328;&#33021;&#21147;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#26426;&#22120;&#32763;&#35793;&#35780;&#20272;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#65292;&#20294;&#23545;&#23427;&#20204;&#22914;&#20309;&#21033;&#29992;&#25552;&#20379;&#30340;&#25968;&#25454;&#36827;&#34892;&#35780;&#20272;&#20173;&#23384;&#22312;&#30693;&#35782;&#24046;&#36317;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;LLMs&#22914;&#20309;&#21033;&#29992;&#28304;&#35821;&#35328;&#21644;&#21442;&#32771;&#20449;&#24687;&#35780;&#20272;&#32763;&#35793;&#65292;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;LLMs&#30340;&#24037;&#20316;&#26426;&#21046;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#28085;&#30422;&#21508;&#31181;&#36755;&#20837;&#27169;&#24335;&#21644;&#27169;&#22411;&#31867;&#22411;&#30340;&#21463;&#25511;&#23454;&#39564;&#65292;&#24182;&#37319;&#29992;&#31895;&#31890;&#24230;&#21644;&#32454;&#31890;&#24230;&#30340;&#25552;&#31034;&#26469;&#21306;&#20998;&#28304;&#35821;&#35328;&#21644;&#21442;&#32771;&#20449;&#24687;&#30340;&#23454;&#29992;&#24615;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#21442;&#32771;&#20449;&#24687;&#26174;&#33879;&#25552;&#39640;&#20102;&#35780;&#20272;&#20934;&#30830;&#24615;&#65292;&#32780;&#28304;&#35821;&#35328;&#20449;&#24687;&#26377;&#26102;&#20250;&#36866;&#24471;&#20854;&#21453;&#65292;&#34920;&#26126;&#22312;&#20351;&#29992;LLMs&#35780;&#20272;&#32763;&#35793;&#26102;&#23384;&#22312;&#36328;&#35821;&#35328;&#33021;&#21147;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#36824;&#23545;LLMs&#36827;&#34892;&#20102;&#32763;&#35793;&#38169;&#35823;&#26816;&#27979;&#30340;&#20803;&#35780;&#20272;&#65292;&#35266;&#23519;&#21040;&#20102;&#31867;&#20284;&#30340;&#29616;&#35937;&#12290;&#36825;&#20123;&#21457;&#29616;&#36824;&#26263;&#31034;&#20102;&#19968;&#31181;&#28508;&#22312;&#30340;&#26041;&#27861;&#65292;&#21363;&#21033;&#29992;&#21442;&#32771;&#20449;&#24687;&#26469;&#25913;&#21892;&#26426;&#22120;&#32763;&#35793;&#30340;&#36136;&#37327;&#35780;&#20272;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have achieved remarkable results in the machine translation evaluation task, yet there remains a gap in knowledge regarding how they utilize the provided data to conduct evaluations. This study aims to explore how LLMs leverage source and reference information in evaluating translations, with the ultimate goal of better understanding the working mechanism of LLMs. To this end, we design the controlled experiments across various input modes and model types, and employ both coarse-grained and fine-grained prompts to discern the utility of source versus reference information. Surprisingly, we find that reference information significantly enhances the evaluation accuracy, while source information sometimes is counterproductive, indicating a lack of cross-lingual capability when using LLMs to evaluate translations. We further conduct a meta-evaluation for translation error detection of LLMs, observing a similar phenomenon. These findings also suggest a potential
&lt;/p&gt;</description></item><item><title>&#20114;&#32852;&#32593;&#19978;&#30340;&#22823;&#37327;&#20869;&#23481;&#26159;&#36890;&#36807;&#26426;&#22120;&#32763;&#35793;&#22810;&#21521;&#32763;&#35793;&#30340;&#65292;&#20854;&#20302;&#36136;&#37327;&#21487;&#33021;&#20250;&#23545;&#20351;&#29992;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#20135;&#29983;&#20005;&#37325;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.05749</link><description>&lt;p&gt;
&#20114;&#32852;&#32593;&#19978;&#30340;&#22823;&#37327;&#20869;&#23481;&#37117;&#26159;&#26426;&#22120;&#32763;&#35793;&#30340;&#65306;&#26469;&#33258;&#22810;&#21521;&#24182;&#34892;&#24615;&#30340;&#27934;&#23519;
&lt;/p&gt;
&lt;p&gt;
A Shocking Amount of the Web is Machine Translated: Insights from Multi-Way Parallelism. (arXiv:2401.05749v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05749
&lt;/p&gt;
&lt;p&gt;
&#20114;&#32852;&#32593;&#19978;&#30340;&#22823;&#37327;&#20869;&#23481;&#26159;&#36890;&#36807;&#26426;&#22120;&#32763;&#35793;&#22810;&#21521;&#32763;&#35793;&#30340;&#65292;&#20854;&#20302;&#36136;&#37327;&#21487;&#33021;&#20250;&#23545;&#20351;&#29992;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#20135;&#29983;&#20005;&#37325;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#20114;&#32852;&#32593;&#19978;&#30340;&#20869;&#23481;&#32463;&#24120;&#34987;&#32763;&#35793;&#25104;&#22810;&#31181;&#35821;&#35328;&#65292;&#24182;&#19988;&#36825;&#20123;&#22810;&#21521;&#32763;&#35793;&#30340;&#20302;&#36136;&#37327;&#34920;&#26126;&#23427;&#20204;&#24456;&#21487;&#33021;&#26159;&#20351;&#29992;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#21019;&#24314;&#30340;&#12290;&#22810;&#21521;&#24182;&#34892;&#30340;&#26426;&#22120;&#29983;&#25104;&#20869;&#23481;&#19981;&#20165;&#22312;&#36164;&#28304;&#36739;&#23569;&#30340;&#35821;&#35328;&#20013;&#21344;&#20027;&#23548;&#22320;&#20301;&#65292;&#32780;&#19988;&#26500;&#25104;&#35813;&#35821;&#35328;&#20013;&#24635;&#20307;&#32593;&#39029;&#20869;&#23481;&#30340;&#24456;&#22823;&#19968;&#37096;&#20998;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#35777;&#25454;&#34920;&#26126;&#65292;&#34987;&#32763;&#35793;&#25104;&#22810;&#31181;&#35821;&#35328;&#30340;&#20869;&#23481;&#23384;&#22312;&#36873;&#25321;&#24615;&#20559;&#24046;&#65292;&#19982;&#23558;&#20302;&#36136;&#37327;&#33521;&#25991;&#20869;&#23481;&#36890;&#36807;&#26426;&#22120;&#32763;&#35793;&#22823;&#35268;&#27169;&#32763;&#35793;&#25104;&#35768;&#22810;&#36164;&#28304;&#36739;&#23569;&#30340;&#35821;&#35328;&#19968;&#33268;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#23545;&#20110;&#22312;&#32593;&#32476;&#19978;&#20174;&#21333;&#35821;&#21644;&#21452;&#35821;&#25968;&#25454;&#35757;&#32451;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31561;&#27169;&#22411;&#25552;&#20986;&#20102;&#20005;&#37325;&#30340;&#25285;&#24551;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show that content on the web is often translated into many languages, and the low quality of these multi-way translations indicates they were likely created using Machine Translation (MT). Multi-way parallel, machine generated content not only dominates the translations in lower resource languages; it also constitutes a large fraction of the total web content in those languages. We also find evidence of a selection bias in the type of content which is translated into many languages, consistent with low quality English content being translated en masse into many lower resource languages, via MT. Our work raises serious concerns about training models such as multilingual large language models on both monolingual and bilingual data scraped from the web.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;DebugBench&#30340;LLM&#35843;&#35797;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35843;&#35797;&#33021;&#21147;&#12290;&#30740;&#31350;&#21457;&#29616;&#38381;&#28304;&#27169;&#22411;&#19982;&#20154;&#31867;&#30456;&#27604;&#20855;&#26377;&#36739;&#20302;&#30340;&#35843;&#35797;&#24615;&#33021;&#65292;&#32780;&#24320;&#28304;&#27169;&#22411;&#26410;&#33021;&#36798;&#21040;&#21512;&#26684;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.04621</link><description>&lt;p&gt;
DebugBench: &#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35843;&#35797;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
DebugBench: Evaluating Debugging Capability of Large Language Models. (arXiv:2401.04621v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04621
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;DebugBench&#30340;LLM&#35843;&#35797;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35843;&#35797;&#33021;&#21147;&#12290;&#30740;&#31350;&#21457;&#29616;&#38381;&#28304;&#27169;&#22411;&#19982;&#20154;&#31867;&#30456;&#27604;&#20855;&#26377;&#36739;&#20302;&#30340;&#35843;&#35797;&#24615;&#33021;&#65292;&#32780;&#24320;&#28304;&#27169;&#22411;&#26410;&#33021;&#36798;&#21040;&#21512;&#26684;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20986;&#20102;&#20986;&#33394;&#30340;&#32534;&#30721;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20316;&#20026;&#32534;&#31243;&#33021;&#21147;&#30340;&#21478;&#19968;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;LLMs&#30340;&#35843;&#35797;&#33021;&#21147;&#20173;&#28982;&#30456;&#23545;&#26410;&#34987;&#25506;&#32034;&#12290;&#20043;&#21069;&#23545;LLMs&#30340;&#35843;&#35797;&#33021;&#21147;&#35780;&#20272;&#21463;&#21040;&#25968;&#25454;&#27844;&#38706;&#39118;&#38505;&#12289;&#25968;&#25454;&#38598;&#35268;&#27169;&#21644;&#27979;&#35797;&#28431;&#27934;&#31181;&#31867;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#19981;&#36275;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;DebugBench&#8221;&#30340;LLM&#35843;&#35797;&#22522;&#20934;&#65292;&#21253;&#21547;4253&#20010;&#23454;&#20363;&#12290;&#23427;&#28085;&#30422;&#20102;C ++&#65292;Java&#21644;Python&#20013;&#22235;&#20010;&#20027;&#35201;&#30340;&#38169;&#35823;&#31867;&#21035;&#21644;18&#20010;&#27425;&#35201;&#31867;&#22411;&#12290;&#20026;&#20102;&#26500;&#24314;DebugBench&#65292;&#25105;&#20204;&#20174;LeetCode&#31038;&#21306;&#25910;&#38598;&#20102;&#20195;&#30721;&#29255;&#27573;&#65292;&#20351;&#29992;GPT-4&#21521;&#28304;&#25968;&#25454;&#20013;&#27880;&#20837;&#38169;&#35823;&#65292;&#24182;&#36827;&#34892;&#20005;&#26684;&#30340;&#36136;&#37327;&#26816;&#26597;&#12290;&#25105;&#20204;&#22312;&#38646;&#26679;&#20363;&#24773;&#20917;&#19979;&#35780;&#20272;&#20102;&#20004;&#20010;&#21830;&#19994;&#27169;&#22411;&#21644;&#19977;&#20010;&#24320;&#28304;&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#65288;1&#65289;&#19982;&#20154;&#31867;&#30456;&#27604;&#65292;&#38381;&#28304;&#27169;&#22411;&#22914;GPT-4&#34920;&#29616;&#20986;&#36739;&#20302;&#30340;&#35843;&#35797;&#24615;&#33021;&#65292;&#32780;&#24320;&#28304;&#27169;&#22411;&#22914;Code Llama&#26080;&#27861;&#36798;&#21040;&#20219;&#20309;&#21512;&#26684;&#29575;&#65307;&#65288;2&#65289;t
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated exceptional coding capability. However, as another critical component of programming proficiency, the debugging capability of LLMs remains relatively unexplored. Previous evaluations of LLMs' debugging ability are significantly limited by the risk of data leakage, the scale of the dataset, and the variety of tested bugs. To overcome these deficiencies, we introduce `DebugBench', an LLM debugging benchmark consisting of 4,253 instances. It covers four major bug categories and 18 minor types in C++, Java, and Python. To construct DebugBench, we collect code snippets from the LeetCode community, implant bugs into source data with GPT-4, and assure rigorous quality checks. We evaluate two commercial and three open-source models in a zero-shot scenario. We find that (1) while closed-source models like GPT-4 exhibit inferior debugging performance compared to humans, open-source models such as Code Llama fail to attain any pass rate scores; (2) t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#23545;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#27169;&#22411;&#21450;&#20854;&#21069;&#39537;&#36827;&#34892;&#20998;&#31867;&#12290;&#35813;&#26694;&#26550;&#24341;&#20837;&#20102;&#19981;&#21516;&#23618;&#27425;&#30340;AGI&#24615;&#33021;&#12289;&#24191;&#27867;&#24615;&#21644;&#33258;&#20027;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#20849;&#21516;&#30340;&#35821;&#35328;&#29992;&#20110;&#27604;&#36739;&#27169;&#22411;&#12289;&#35780;&#20272;&#39118;&#38505;&#65292;&#24182;&#34913;&#37327;&#22312;AGI&#36335;&#24452;&#19978;&#30340;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2311.02462</link><description>&lt;p&gt;
AGI&#30340;&#23618;&#27425;&#65306;&#23558;AGI&#36335;&#24452;&#19978;&#30340;&#36827;&#23637;&#21487;&#25805;&#20316;&#21270;
&lt;/p&gt;
&lt;p&gt;
Levels of AGI: Operationalizing Progress on the Path to AGI. (arXiv:2311.02462v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.02462
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#23545;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#27169;&#22411;&#21450;&#20854;&#21069;&#39537;&#36827;&#34892;&#20998;&#31867;&#12290;&#35813;&#26694;&#26550;&#24341;&#20837;&#20102;&#19981;&#21516;&#23618;&#27425;&#30340;AGI&#24615;&#33021;&#12289;&#24191;&#27867;&#24615;&#21644;&#33258;&#20027;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#20849;&#21516;&#30340;&#35821;&#35328;&#29992;&#20110;&#27604;&#36739;&#27169;&#22411;&#12289;&#35780;&#20272;&#39118;&#38505;&#65292;&#24182;&#34913;&#37327;&#22312;AGI&#36335;&#24452;&#19978;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#27169;&#22411;&#21450;&#20854;&#21069;&#39537;&#30340;&#33021;&#21147;&#21644;&#34892;&#20026;&#36827;&#34892;&#20998;&#31867;&#12290;&#35813;&#26694;&#26550;&#24341;&#20837;&#20102;AGI&#24615;&#33021;&#12289;&#24191;&#27867;&#24615;&#21644;&#33258;&#20027;&#24615;&#30340;&#23618;&#27425;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#20010;&#26694;&#26550;&#33021;&#22815;&#20687;&#33258;&#21160;&#39550;&#39542;&#30340;&#23618;&#27425;&#19968;&#26679;&#65292;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#20849;&#21516;&#30340;&#35821;&#35328;&#26469;&#27604;&#36739;&#27169;&#22411;&#12289;&#35780;&#20272;&#39118;&#38505;&#65292;&#24182;&#34913;&#37327;&#22312;AGI&#36335;&#24452;&#19978;&#30340;&#36827;&#23637;&#12290;&#20026;&#20102;&#24320;&#21457;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#29616;&#26377;&#30340;AGI&#23450;&#20041;&#65292;&#24182;&#25552;&#21462;&#20986;&#20102;&#19968;&#20010;&#26377;&#29992;&#30340;AGI&#26412;&#20307;&#35770;&#24212;&#28385;&#36275;&#30340;&#20845;&#20010;&#21407;&#21017;&#12290;&#36825;&#20123;&#21407;&#21017;&#21253;&#25324;&#20851;&#27880;&#33021;&#21147;&#32780;&#19981;&#26159;&#26426;&#21046;&#65307;&#20998;&#21035;&#35780;&#20272;&#24191;&#27867;&#24615;&#21644;&#24615;&#33021;&#65307;&#23450;&#20041;AGI&#36335;&#24452;&#19978;&#30340;&#38454;&#27573;&#65292;&#32780;&#19981;&#26159;&#19987;&#27880;&#20110;&#32456;&#28857;&#12290;&#22522;&#20110;&#36825;&#20123;&#21407;&#21017;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;AGI&#30340;&#23618;&#27425;&#8221;&#65292;&#26681;&#25454;&#33021;&#21147;&#30340;&#28145;&#24230;&#65288;&#24615;&#33021;&#65289;&#21644;&#24191;&#24230;&#65288;&#24191;&#27867;&#24615;&#65289;&#65292;&#24182;&#24605;&#32771;&#24403;&#21069;&#31995;&#32479;&#22914;&#20309;&#31526;&#21512;&#36825;&#20010;&#26412;&#20307;&#35770;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#23545;&#23454;&#29616;AGI&#25152;&#25552;&#20986;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a framework for classifying the capabilities and behavior of Artificial General Intelligence (AGI) models and their precursors. This framework introduces levels of AGI performance, generality, and autonomy. It is our hope that this framework will be useful in an analogous way to the levels of autonomous driving, by providing a common language to compare models, assess risks, and measure progress along the path to AGI. To develop our framework, we analyze existing definitions of AGI, and distill six principles that a useful ontology for AGI should satisfy. These principles include focusing on capabilities rather than mechanisms; separately evaluating generality and performance; and defining stages along the path toward AGI, rather than focusing on the endpoint. With these principles in mind, we propose 'Levels of AGI' based on depth (performance) and breadth (generality) of capabilities, and reflect on how current systems fit into this ontology. We discuss the challenging req
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;LLM&#28040;&#38500;&#26041;&#27861;&#65292;&#31216;&#20026;&#8220;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#28040;&#38500;&#8221;&#65292;&#23427;&#25552;&#20379;&#20102;&#19978;&#19979;&#25991;&#30340;&#36755;&#20837;&#19988;&#26080;&#38656;&#26356;&#26032;&#27169;&#22411;&#21442;&#25968;&#12290;&#36825;&#31181;&#26041;&#27861;&#35299;&#20915;&#20102;&#28040;&#38500;&#23545;&#20110;&#24456;&#22823;&#27169;&#22411;&#26469;&#35828;&#22312;&#35745;&#31639;&#19978;&#30340;&#22256;&#38590;&#65292;&#24182;&#22312;&#23454;&#36341;&#20013;&#20855;&#26377;&#26356;&#39640;&#30340;&#21487;&#34892;&#24615;&#21644;&#20415;&#25463;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.07579</link><description>&lt;p&gt;
In-Context Unlearning: &#22522;&#20110;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#28040;&#38500;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
In-Context Unlearning: Language Models as Few Shot Unlearners. (arXiv:2310.07579v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07579
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;LLM&#28040;&#38500;&#26041;&#27861;&#65292;&#31216;&#20026;&#8220;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#28040;&#38500;&#8221;&#65292;&#23427;&#25552;&#20379;&#20102;&#19978;&#19979;&#25991;&#30340;&#36755;&#20837;&#19988;&#26080;&#38656;&#26356;&#26032;&#27169;&#22411;&#21442;&#25968;&#12290;&#36825;&#31181;&#26041;&#27861;&#35299;&#20915;&#20102;&#28040;&#38500;&#23545;&#20110;&#24456;&#22823;&#27169;&#22411;&#26469;&#35828;&#22312;&#35745;&#31639;&#19978;&#30340;&#22256;&#38590;&#65292;&#24182;&#22312;&#23454;&#36341;&#20013;&#20855;&#26377;&#26356;&#39640;&#30340;&#21487;&#34892;&#24615;&#21644;&#20415;&#25463;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#28040;&#38500;&#23398;&#20064;&#26159;&#30740;&#31350;&#22914;&#20309;&#39640;&#25928;&#22320;&#21435;&#38500;&#29305;&#23450;&#35757;&#32451;&#25968;&#25454;&#23545;&#35757;&#32451;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#36817;&#26469;&#24341;&#36215;&#20102;&#26356;&#22810;&#30340;&#20851;&#27880;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#38656;&#35201;&#36981;&#23432;&#35832;&#22914;&#34987;&#36951;&#24536;&#26435;&#31561;&#38544;&#31169;&#27861;&#35268;&#30340;&#38656;&#27714;&#12290;&#23613;&#31649;&#22312;&#29256;&#26435;&#38382;&#39064;&#19978;LLM&#65288;&#35821;&#35328;&#27169;&#22411;&#65289;&#23588;&#20854;&#30456;&#20851;&#65292;&#20294;&#22312;&#38750;&#24120;&#22823;&#30340;&#27169;&#22411;&#19978;&#23454;&#29616;&#31934;&#30830;&#28040;&#38500;&#26159;&#35745;&#31639;&#19978;&#19981;&#21487;&#34892;&#30340;&#12290;&#20026;&#27492;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#20960;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#36817;&#20284;&#28040;&#38500;&#35757;&#32451;&#25968;&#25454;&#12290;&#36825;&#20123;&#31639;&#27861;&#20851;&#38190;&#20381;&#36182;&#20110;&#23545;&#27169;&#22411;&#21442;&#25968;&#30340;&#35775;&#38382;&#26469;&#26356;&#26032;&#23427;&#20204;&#65292;&#20294;&#22312;&#23454;&#36341;&#20013;&#21487;&#33021;&#30001;&#20110;&#35745;&#31639;&#32422;&#26463;&#25110;&#36890;&#36807;API&#35775;&#38382;LLM&#32780;&#26080;&#27861;&#28385;&#36275;&#36825;&#31181;&#20551;&#35774;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;LLM&#28040;&#38500;&#26041;&#27861;&#65292;&#31216;&#20026;&#8220;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#28040;&#38500;&#8221;&#65292;&#23427;&#25552;&#20379;&#20102;&#19978;&#19979;&#25991;&#30340;&#36755;&#20837;&#19988;&#26080;&#38656;&#26356;&#26032;&#27169;&#22411;&#21442;&#25968;&#12290;&#20026;&#20102;&#28040;&#38500;&#29305;&#23450;&#30340;&#35757;&#32451;&#23454;&#20363;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;i
&lt;/p&gt;
&lt;p&gt;
Machine unlearning, the study of efficiently removing the impact of specific training points on the trained model, has garnered increased attention of late, driven by the need to comply with privacy regulations like the Right to be Forgotten. Although unlearning is particularly relevant for LLMs in light of the copyright issues they raise, achieving precise unlearning is computationally infeasible for very large models. To this end, recent work has proposed several algorithms which approximate the removal of training data without retraining the model. These algorithms crucially rely on access to the model parameters in order to update them, an assumption that may not hold in practice due to computational constraints or when the LLM is accessed via API. In this work, we propose a new class of unlearning methods for LLMs we call ''In-Context Unlearning'', providing inputs in context and without having to update model parameters. To unlearn a particular training instance, we provide the i
&lt;/p&gt;</description></item><item><title>&#35821;&#35328;&#20195;&#29702;&#26641;&#25628;&#32034;&#65288;LATS&#65289;&#26159;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#22312;&#35268;&#21010;&#12289;&#34892;&#21160;&#21644;&#25512;&#29702;&#26041;&#38754;&#30456;&#20114;&#21327;&#21516;&#65292;&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#22806;&#37096;&#21453;&#39304;&#30340;&#29615;&#22659;&#65292;&#23454;&#29616;&#26356;&#21152;&#28145;&#24605;&#29087;&#34385;&#21644;&#36866;&#24212;&#24615;&#30340;&#38382;&#39064;&#35299;&#20915;&#26426;&#21046;&#12290;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;LATS&#22312;&#22810;&#20010;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#24615;&#65292;&#29305;&#21035;&#22312;&#32534;&#31243;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;94.4%&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.04406</link><description>&lt;p&gt;
&#35821;&#35328;&#20195;&#29702;&#26641;&#25628;&#32034;&#32479;&#19968;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25512;&#29702;&#12289;&#34892;&#21160;&#21644;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Language Agent Tree Search Unifies Reasoning Acting and Planning in Language Models. (arXiv:2310.04406v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04406
&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#20195;&#29702;&#26641;&#25628;&#32034;&#65288;LATS&#65289;&#26159;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#22312;&#35268;&#21010;&#12289;&#34892;&#21160;&#21644;&#25512;&#29702;&#26041;&#38754;&#30456;&#20114;&#21327;&#21516;&#65292;&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#22806;&#37096;&#21453;&#39304;&#30340;&#29615;&#22659;&#65292;&#23454;&#29616;&#26356;&#21152;&#28145;&#24605;&#29087;&#34385;&#21644;&#36866;&#24212;&#24615;&#30340;&#38382;&#39064;&#35299;&#20915;&#26426;&#21046;&#12290;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;LATS&#22312;&#22810;&#20010;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#24615;&#65292;&#29305;&#21035;&#22312;&#32534;&#31243;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;94.4%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#19968;&#31995;&#21015;&#20915;&#31574;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#20381;&#36182;&#20110;&#31616;&#21333;&#30340;&#34892;&#21160;&#36807;&#31243;&#65292;&#24182;&#26410;&#33021;&#24191;&#27867;&#37096;&#32626;&#20316;&#20026;&#33258;&#20027;&#20195;&#29702;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;LATS&#65288;&#35821;&#35328;&#20195;&#29702;&#26641;&#25628;&#32034;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#23558;LLMs&#22312;&#35268;&#21010;&#12289;&#34892;&#21160;&#21644;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#30456;&#20114;&#21327;&#21516;&#12290;LATS&#20511;&#37492;&#20102;&#27169;&#22411;&#23548;&#21521;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#30340;&#24605;&#24819;&#65292;&#23558;LLMs&#29992;&#20316;&#20195;&#29702;&#12289;&#20215;&#20540;&#20989;&#25968;&#21644;&#20248;&#21270;&#22120;&#65292;&#37325;&#26032;&#21033;&#29992;&#20854;&#28508;&#22312;&#30340;&#20248;&#21183;&#20197;&#25552;&#21319;&#20915;&#31574;&#33021;&#21147;&#12290;&#20851;&#38190;&#30340;&#19968;&#28857;&#26159;LATS&#20351;&#29992;&#19968;&#20010;&#20855;&#26377;&#22806;&#37096;&#21453;&#39304;&#30340;&#29615;&#22659;&#65292;&#36825;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#21152;&#28145;&#24605;&#29087;&#34385;&#21644;&#36866;&#24212;&#24615;&#30340;&#38382;&#39064;&#35299;&#20915;&#26426;&#21046;&#65292;&#36229;&#36234;&#20102;&#29616;&#26377;&#25216;&#26415;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#22312;&#32534;&#31243;&#12289;HotPotQA&#21644;WebShop&#31561;&#22810;&#20010;&#39046;&#22495;&#36827;&#34892;&#20102;&#23454;&#39564;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;LATS&#22312;&#25512;&#29702;&#21644;&#34892;&#21160;&#26041;&#38754;&#30340;&#36866;&#29992;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;&#32534;&#31243;&#26041;&#38754;&#65292;LATS&#23454;&#29616;&#20102;94.4%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
While large language models (LLMs) have demonstrated impressive performance on a range of decision-making tasks, they rely on simple acting processes and fall short of broad deployment as autonomous agents. We introduce LATS (Language Agent Tree Search), a general framework that synergizes the capabilities of LLMs in planning, acting, and reasoning. Drawing inspiration from Monte Carlo tree search in model-based reinforcement learning, LATS employs LLMs as agents, value functions, and optimizers, repurposing their latent strengths for enhanced decision-making. What is crucial in this method is the use of an environment for external feedback, which offers a more deliberate and adaptive problem-solving mechanism that moves beyond the limitations of existing techniques. Our experimental evaluation across diverse domains, such as programming, HotPotQA, and WebShop, illustrates the applicability of LATS for both reasoning and acting. In particular, LATS achieves 94.4\% for programming on Hu
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#28436;&#32462;&#25512;&#29702;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#26126;&#26377;&#24207;&#30340;&#26041;&#27861;&#65292;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#23376;&#20219;&#21153;&#24182;&#19988;&#20154;&#31867;&#21270;&#22320;&#32452;&#32455;&#24605;&#32500;&#65292;&#20197;&#25552;&#39640;&#28436;&#32462;&#25512;&#29702;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.03309</link><description>&lt;p&gt;
&#31616;&#26126;&#26377;&#24207;&#30340;&#24863;&#30693;&#26377;&#21161;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#28436;&#32462;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Concise and Organized Perception Facilitates Large Language Models for Deductive Reasoning. (arXiv:2310.03309v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03309
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#28436;&#32462;&#25512;&#29702;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#26126;&#26377;&#24207;&#30340;&#26041;&#27861;&#65292;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#23376;&#20219;&#21153;&#24182;&#19988;&#20154;&#31867;&#21270;&#22320;&#32452;&#32455;&#24605;&#32500;&#65292;&#20197;&#25552;&#39640;&#28436;&#32462;&#25512;&#29702;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#35299;&#20915;&#28436;&#32462;&#25512;&#29702;&#38382;&#39064;&#24050;&#32463;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#22312;&#22797;&#26434;&#30340;&#28436;&#32462;&#38382;&#39064;&#20013;&#20173;&#28982;&#24456;&#38590;&#21462;&#24471;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#65292;&#36825;&#31867;&#38382;&#39064;&#20855;&#26377;&#22823;&#37327;&#21069;&#25552;&#65288;&#21363;&#20107;&#23454;&#25110;&#35268;&#21017;&#65289;&#65292;&#20854;&#20013;&#28041;&#21450;&#23454;&#20307;&#20043;&#38388;&#38169;&#32508;&#22797;&#26434;&#30340;&#20851;&#31995;&#65292;&#38656;&#35201;&#36827;&#34892;&#22810;&#36339;&#25512;&#29702;&#12290;&#19968;&#31181;&#30452;&#35266;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#23558;&#21407;&#22987;&#20219;&#21153;&#20998;&#35299;&#20026;&#36739;&#23567;&#30340;&#23376;&#20219;&#21153;&#65292;&#28982;&#21518;&#20197;&#21069;&#21521;&#65288;&#20363;&#22914;&#36873;&#25321;-&#25512;&#29702;&#65289;&#25110;&#21453;&#21521;&#65288;&#20363;&#22914;LAMBADA&#65289;&#26041;&#24335;&#23558;&#22810;&#20010;&#22240;&#26524;&#25512;&#29702;&#27493;&#39588;&#36830;&#25509;&#22312;&#19968;&#36215;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25216;&#26415;&#19981;&#21487;&#36991;&#20813;&#22320;&#38656;&#35201;&#22823;&#37327;&#30340;&#24635;&#20307;&#38454;&#27573;&#65292;&#23548;&#33268;&#35745;&#31639;&#24320;&#38144;&#22823;&#65292;&#24182;&#19988;&#26377;&#26356;&#39640;&#30340;&#21487;&#33021;&#24615;&#20135;&#29983;&#35823;&#23548;&#24615;&#30340;&#27493;&#39588;&#12290;&#38500;&#20102;&#36880;&#38454;&#27573;&#20998;&#35299;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#20174;&#20154;&#31867;&#38382;&#39064;&#35299;&#20915;&#30340;&#21478;&#19968;&#20010;&#26041;&#38754;&#33719;&#24471;&#20102;&#21551;&#21457;&#12290;&#20154;&#31867;&#20542;&#21521;&#20110;&#25552;&#28860;&#20986;&#26368;&#30456;&#20851;&#30340;&#20449;&#24687;&#24182;&#26377;&#24207;&#22320;&#32452;&#32455;&#24605;&#32500;&#65288;&#20363;&#22914;&#21019;&#24314;&#24605;&#32500;&#23548;&#22270;&#65289;&#65292;&#36825;&#26377;&#21161;&#20110;&#20182;&#20204;&#23545;&#38382;&#39064;&#36827;&#34892;&#26377;&#25928;&#30340;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exploiting large language models (LLMs) to tackle deductive reasoning has garnered growing attention. It still remains highly challenging to achieve satisfactory results in complex deductive problems, characterized by plenty of premises (i.e., facts or rules) entailing intricate relationships among entities and requiring multi-hop reasoning. One intuitive solution is to decompose the original task into smaller sub-tasks, and then chain the multiple casual reasoning steps together in a forward (e.g., Selection-Inference) or backward (e.g., LAMBADA) direction. However, these techniques inevitably necessitate a large number of overall stages, leading to computationally expensive operations and a higher possibility of making misleading steps. In addition to stage-by-stage decomposition, we draw inspiration from another aspect of human problem-solving. Humans tend to distill the most relevant information and organize their thoughts systematically (e.g., creating mind maps), which assists th
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#29305;&#21270;&#65292;&#36890;&#36807;&#20351;&#29992;&#19987;&#19994;&#39046;&#22495;&#30340;&#25968;&#25454;&#21644;&#23569;&#37327;&#26631;&#35760;&#31181;&#23376;&#36827;&#34892;&#33258;&#25105;&#23545;&#40784;&#65292;&#25552;&#39640;&#20102;&#22312;&#30446;&#26631;&#39046;&#22495;&#30340;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.00160</link><description>&lt;p&gt;
&#33258;&#25105;&#29305;&#21270;&#65306;&#25581;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#28508;&#22312;&#19987;&#19994;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Self-Specialization: Uncovering Latent Expertise within Large Language Models. (arXiv:2310.00160v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00160
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#29305;&#21270;&#65292;&#36890;&#36807;&#20351;&#29992;&#19987;&#19994;&#39046;&#22495;&#30340;&#25968;&#25454;&#21644;&#23569;&#37327;&#26631;&#35760;&#31181;&#23376;&#36827;&#34892;&#33258;&#25105;&#23545;&#40784;&#65292;&#25552;&#39640;&#20102;&#22312;&#30446;&#26631;&#39046;&#22495;&#30340;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#33258;&#25105;&#35843;&#25972;&#30340;&#26377;&#25928;&#24615;&#65292;&#21363;&#36890;&#36807;&#20351;&#29992;&#23569;&#37327;&#20154;&#31867;&#32534;&#20889;&#30340;&#31181;&#23376;&#25968;&#25454;&#33258;&#21160;&#29983;&#25104;&#25945;&#23398;&#25968;&#25454;&#65292;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#23545;&#40784;&#20197;&#36981;&#24490;&#19968;&#33324;&#25351;&#31034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#19981;&#20877;&#20851;&#27880;&#19968;&#33324;&#23545;&#40784;&#65292;&#32780;&#26159;&#19987;&#27880;&#20110;&#19987;&#23478;&#39046;&#22495;&#29305;&#21270;&#30340;&#33258;&#25105;&#23545;&#40784;&#65288;&#20363;&#22914;&#65292;&#29983;&#29289;&#21307;&#23398;&#65289;&#65292;&#21457;&#29616;&#23427;&#23545;&#20110;&#25552;&#39640;&#30446;&#26631;&#39046;&#22495;&#30340;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#24615;&#33021;&#38750;&#24120;&#26377;&#25928;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#29616;&#26377;&#23545;&#40784;&#27169;&#22411;&#22312;&#19987;&#19994;&#39046;&#22495;&#20869;&#30340;&#22522;&#20934;&#32467;&#26524;&#65292;&#25581;&#31034;&#20102;&#8220;&#36890;&#29992;&#8221;&#25351;&#31034;&#36319;&#38543;&#35757;&#32451;&#23545;&#19979;&#28216;&#19987;&#23478;&#39046;&#22495;&#24615;&#33021;&#30340;&#36793;&#38469;&#25928;&#24212;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#33258;&#25105;&#29305;&#21270;&#65292;&#21033;&#29992;&#39046;&#22495;&#29305;&#23450;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#21644;&#23569;&#37327;&#26631;&#35760;&#31181;&#23376;&#36827;&#34892;&#33258;&#25105;&#23545;&#40784;&#36807;&#31243;&#12290;&#24403;&#36890;&#36807;&#26816;&#32034;&#26469;&#20943;&#23569;&#20135;&#29983;&#24187;&#35273;&#24182;&#25552;&#39640;&#23545;&#40784;&#30340;&#24182;&#21457;&#24615;&#21518;&#65292;&#33258;&#25105;&#29305;&#21270;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works have demonstrated the effectiveness of self-alignment in which a large language model is, by itself, aligned to follow general instructions through the automatic generation of instructional data using a handful of human-written seeds. Instead of general alignment, in this work, we focus on self-alignment for expert domain specialization (e.g., biomedicine), discovering it to be very effective for improving zero-shot and few-shot performance in target domains of interest. As a preliminary, we first present the benchmark results of existing aligned models within a specialized domain, which reveals the marginal effect that "generic" instruction-following training has on downstream expert domains' performance. To remedy this, we explore self-specialization that leverages domain-specific unlabelled data and a few labeled seeds for the self-alignment process. When augmented with retrieval to reduce hallucination and enhance concurrency of the alignment, self-specialization offer
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#20840;&#38754;&#35843;&#26597;&#20102;&#24605;&#32500;&#38142;&#25512;&#29702;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#28085;&#30422;&#20102;&#26500;&#24314;&#12289;&#32467;&#26500;&#21464;&#20307;&#21644;&#22686;&#24378;&#25216;&#26415;&#31561;&#26041;&#27861;&#20998;&#31867;&#65292;&#20197;&#21450;&#35268;&#21010;&#12289;&#24037;&#20855;&#20351;&#29992;&#21644;&#25552;&#28860;&#31561;&#21069;&#27839;&#24212;&#29992;&#12290;&#21516;&#26102;&#35752;&#35770;&#20102;&#25361;&#25112;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;&#36825;&#20221;&#35843;&#26597;&#25253;&#21578;&#23545;&#20110;&#22312;&#24605;&#32500;&#38142;&#25512;&#29702;&#39046;&#22495;&#23547;&#27714;&#21019;&#26032;&#30340;&#30740;&#31350;&#20154;&#21592;&#26469;&#35828;&#26159;&#19968;&#20010;&#26377;&#20215;&#20540;&#30340;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2309.15402</link><description>&lt;p&gt;
&#20851;&#20110;&#24605;&#32500;&#38142;&#25512;&#29702;&#65306;&#36827;&#23637;&#12289;&#21069;&#27839;&#21644;&#26410;&#26469;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future. (arXiv:2309.15402v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15402
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#20840;&#38754;&#35843;&#26597;&#20102;&#24605;&#32500;&#38142;&#25512;&#29702;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#28085;&#30422;&#20102;&#26500;&#24314;&#12289;&#32467;&#26500;&#21464;&#20307;&#21644;&#22686;&#24378;&#25216;&#26415;&#31561;&#26041;&#27861;&#20998;&#31867;&#65292;&#20197;&#21450;&#35268;&#21010;&#12289;&#24037;&#20855;&#20351;&#29992;&#21644;&#25552;&#28860;&#31561;&#21069;&#27839;&#24212;&#29992;&#12290;&#21516;&#26102;&#35752;&#35770;&#20102;&#25361;&#25112;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;&#36825;&#20221;&#35843;&#26597;&#25253;&#21578;&#23545;&#20110;&#22312;&#24605;&#32500;&#38142;&#25512;&#29702;&#39046;&#22495;&#23547;&#27714;&#21019;&#26032;&#30340;&#30740;&#31350;&#20154;&#21592;&#26469;&#35828;&#26159;&#19968;&#20010;&#26377;&#20215;&#20540;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24605;&#32500;&#38142;&#25512;&#29702;&#26159;&#20154;&#31867;&#26234;&#33021;&#30340;&#22522;&#26412;&#35748;&#30693;&#36807;&#31243;&#65292;&#22312;&#20154;&#24037;&#26234;&#33021;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20173;&#32570;&#20047;&#19968;&#20221;&#20840;&#38754;&#30340;&#35843;&#26597;&#25253;&#21578;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#36808;&#20986;&#20102;&#31532;&#19968;&#27493;&#65292;&#20180;&#32454;&#24191;&#27867;&#22320;&#27010;&#36848;&#20102;&#36825;&#20010;&#30740;&#31350;&#39046;&#22495;&#12290;&#25105;&#20204;&#29992;&#8220;X-of-Thought&#8221;&#26469;&#25351;&#20195;&#24191;&#20041;&#19978;&#30340;&#24605;&#32500;&#38142;&#25512;&#29702;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26681;&#25454;&#26041;&#27861;&#30340;&#20998;&#31867;&#20307;&#31995;&#23545;&#24403;&#21069;&#30340;&#30740;&#31350;&#36827;&#34892;&#20102;&#31995;&#32479;&#32452;&#32455;&#65292;&#21253;&#25324;&#24605;&#32500;&#38142;&#30340;&#26500;&#24314;&#12289;&#32467;&#26500;&#21464;&#20307;&#21644;&#22686;&#24378;&#25216;&#26415;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#24605;&#32500;&#38142;&#22312;&#35268;&#21010;&#12289;&#24037;&#20855;&#20351;&#29992;&#21644;&#25552;&#28860;&#31561;&#39046;&#22495;&#30340;&#21069;&#27839;&#24212;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#19968;&#20123;&#25361;&#25112;&#21644;&#26410;&#26469;&#30340;&#26041;&#21521;&#65292;&#21253;&#25324;&#24544;&#23454;&#24230;&#12289;&#22810;&#27169;&#24577;&#21644;&#29702;&#35770;&#31561;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#20221;&#35843;&#26597;&#25253;&#21578;&#33021;&#25104;&#20026;&#23547;&#27714;&#22312;&#24605;&#32500;&#38142;&#25512;&#29702;&#39046;&#22495;&#21019;&#26032;&#30340;&#30740;&#31350;&#20154;&#21592;&#30340;&#23453;&#36149;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chain-of-thought reasoning, a cognitive process fundamental to human intelligence, has garnered significant attention in the realm of artificial intelligence and natural language processing. However, there still remains a lack of a comprehensive survey for this arena. To this end, we take the first step and present a thorough survey of this research field carefully and widely. We use X-of-Thought to refer to Chain-of-Thought in a broad sense. In detail, we systematically organize the current research according to the taxonomies of methods, including XoT construction, XoT structure variants, and enhanced XoT. Additionally, we describe XoT with frontier applications, covering planning, tool use, and distillation. Furthermore, we address challenges and discuss some future directions, including faithfulness, multi-modal, and theory. We hope this survey serves as a valuable resource for researchers seeking to innovate within the domain of chain-of-thought reasoning.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20026;&#26448;&#26009;&#31185;&#23398;&#39046;&#22495;&#26500;&#24314;&#20102;&#19968;&#20010;&#30693;&#35782;&#22270;&#38382;&#31572;&#22522;&#20934;&#25968;&#25454;&#38598;&#65288;KGQA4MAT&#65289;&#65292;&#37325;&#28857;&#20851;&#27880;&#37329;&#23646;&#26377;&#26426;&#26694;&#26550;&#65288;MOF&#65289;&#12290;&#36890;&#36807;&#24320;&#21457;&#33258;&#28982;&#35821;&#35328;&#25509;&#21475;&#21644;&#21033;&#29992;ChatGPT&#23558;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#36716;&#21270;&#20026;&#24418;&#24335;&#21270;&#30340;KG&#26597;&#35810;&#65292;&#35813;&#35770;&#25991;&#23637;&#31034;&#20102;ChatGPT&#22312;&#35299;&#20915;KGQA&#38382;&#39064;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.11361</link><description>&lt;p&gt;
&#26448;&#26009;&#31185;&#23398;&#20013;&#30340;&#30693;&#35782;&#22270;&#38382;&#31572;&#65288;KGQA4MAT&#65289;&#65306;&#20026;&#37329;&#23646;&#26377;&#26426;&#26694;&#26550;&#30693;&#35782;&#22270;&#65288;MOF-KG&#65289;&#24320;&#21457;&#33258;&#28982;&#35821;&#35328;&#25509;&#21475;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph Question Answering for Materials Science (KGQA4MAT): Developing Natural Language Interface for Metal-Organic Frameworks Knowledge Graph (MOF-KG). (arXiv:2309.11361v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11361
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20026;&#26448;&#26009;&#31185;&#23398;&#39046;&#22495;&#26500;&#24314;&#20102;&#19968;&#20010;&#30693;&#35782;&#22270;&#38382;&#31572;&#22522;&#20934;&#25968;&#25454;&#38598;&#65288;KGQA4MAT&#65289;&#65292;&#37325;&#28857;&#20851;&#27880;&#37329;&#23646;&#26377;&#26426;&#26694;&#26550;&#65288;MOF&#65289;&#12290;&#36890;&#36807;&#24320;&#21457;&#33258;&#28982;&#35821;&#35328;&#25509;&#21475;&#21644;&#21033;&#29992;ChatGPT&#23558;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#36716;&#21270;&#20026;&#24418;&#24335;&#21270;&#30340;KG&#26597;&#35810;&#65292;&#35813;&#35770;&#25991;&#23637;&#31034;&#20102;ChatGPT&#22312;&#35299;&#20915;KGQA&#38382;&#39064;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#26448;&#26009;&#31185;&#23398;&#20013;&#30340;&#30693;&#35782;&#22270;&#38382;&#31572;&#22522;&#20934;&#25968;&#25454;&#38598;&#65288;KGQA4MAT&#65289;&#65292;&#37325;&#28857;&#20851;&#27880;&#37329;&#23646;&#26377;&#26426;&#26694;&#26550;&#65288;MOF&#65289;&#12290;&#36890;&#36807;&#25972;&#21512;&#32467;&#26500;&#21270;&#25968;&#25454;&#24211;&#21644;&#20174;&#25991;&#29486;&#20013;&#25552;&#21462;&#30340;&#30693;&#35782;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#37329;&#23646;&#26377;&#26426;&#26694;&#26550;&#30693;&#35782;&#22270;&#65288;MOF-KG&#65289;&#12290;&#20026;&#20102;&#22686;&#24378;&#39046;&#22495;&#19987;&#23478;&#35775;&#38382;MOF-KG&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#33258;&#28982;&#35821;&#35328;&#25509;&#21475;&#26469;&#26597;&#35810;&#30693;&#35782;&#22270;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20934;&#65292;&#21253;&#25324;161&#20010;&#28041;&#21450;&#27604;&#36739;&#12289;&#32858;&#21512;&#21644;&#22797;&#26434;&#22270;&#32467;&#26500;&#30340;&#22797;&#26434;&#38382;&#39064;&#12290;&#27599;&#20010;&#38382;&#39064;&#37117;&#26377;&#19977;&#31181;&#19981;&#21516;&#30340;&#25913;&#20889;&#24418;&#24335;&#65292;&#20849;&#35745;644&#20010;&#38382;&#39064;&#21644;161&#20010;KG&#26597;&#35810;&#12290;&#20026;&#20102;&#35780;&#20272;&#22522;&#20934;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;ChatGPT&#23558;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#36716;&#21270;&#20026;&#24418;&#24335;&#21270;&#30340;KG&#26597;&#35810;&#12290;&#25105;&#20204;&#36824;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#33879;&#21517;&#30340;QALD-9&#25968;&#25454;&#38598;&#65292;&#23637;&#31034;&#20102;ChatGPT&#22312;&#35299;&#20915;&#19981;&#21516;&#24179;&#21488;&#21644;&#38382;&#39064;&#19979;KGQA&#38382;&#39064;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a comprehensive benchmark dataset for Knowledge Graph Question Answering in Materials Science (KGQA4MAT), with a focus on metal-organic frameworks (MOFs). A knowledge graph for metal-organic frameworks (MOF-KG) has been constructed by integrating structured databases and knowledge extracted from the literature. To enhance MOF-KG accessibility for domain experts, we aim to develop a natural language interface for querying the knowledge graph. We have developed a benchmark comprised of 161 complex questions involving comparison, aggregation, and complicated graph structures. Each question is rephrased in three additional variations, resulting in 644 questions and 161 KG queries. To evaluate the benchmark, we have developed a systematic approach for utilizing ChatGPT to translate natural language questions into formal KG queries. We also apply the approach to the well-known QALD-9 dataset, demonstrating ChatGPT's potential in addressing KGQA issues for different platforms and q
&lt;/p&gt;</description></item><item><title>CB-Whisper&#26159;&#19968;&#31181;&#22522;&#20110;OpenAI&#30340;Whisper&#27169;&#22411;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;&#24320;&#25918;&#35789;&#27719;&#20851;&#38190;&#35789;&#26816;&#27979;&#65288;OV-KWS&#65289;&#35782;&#21035;&#32597;&#35265;&#30340;&#21629;&#21517;&#23454;&#20307;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#23454;&#20307;&#20316;&#20026;&#25552;&#31034;&#26469;&#25913;&#36827;&#35782;&#21035;&#25928;&#26524;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#25552;&#39640;&#23454;&#20307;&#21484;&#22238;&#29575;&#30340;&#21516;&#26102;&#20250;&#30053;&#24494;&#22686;&#21152;&#28151;&#28102;&#38169;&#35823;&#29575;&#65288;MER&#65289;&#12290;</title><link>http://arxiv.org/abs/2309.09552</link><description>&lt;p&gt;
CB-Whisper: &#20351;&#29992;&#24320;&#25918;&#35789;&#27719;&#20851;&#38190;&#35789;&#26816;&#27979;&#36827;&#34892;&#19978;&#19979;&#25991;&#20559;&#32622;&#30340;Whisper
&lt;/p&gt;
&lt;p&gt;
CB-Whisper: Contextual Biasing Whisper using Open-Vocabulary Keyword-Spotting. (arXiv:2309.09552v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09552
&lt;/p&gt;
&lt;p&gt;
CB-Whisper&#26159;&#19968;&#31181;&#22522;&#20110;OpenAI&#30340;Whisper&#27169;&#22411;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;&#24320;&#25918;&#35789;&#27719;&#20851;&#38190;&#35789;&#26816;&#27979;&#65288;OV-KWS&#65289;&#35782;&#21035;&#32597;&#35265;&#30340;&#21629;&#21517;&#23454;&#20307;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#23454;&#20307;&#20316;&#20026;&#25552;&#31034;&#26469;&#25913;&#36827;&#35782;&#21035;&#25928;&#26524;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#25552;&#39640;&#23454;&#20307;&#21484;&#22238;&#29575;&#30340;&#21516;&#26102;&#20250;&#30053;&#24494;&#22686;&#21152;&#28151;&#28102;&#38169;&#35823;&#29575;&#65288;MER&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#24448;&#24448;&#38590;&#20197;&#35782;&#21035;&#32597;&#35265;&#30340;&#21629;&#21517;&#23454;&#20307;&#65292;&#22914;&#20010;&#20154;&#22995;&#21517;&#12289;&#32452;&#32455;&#26426;&#26500;&#21644;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#19981;&#32463;&#24120;&#36935;&#21040;&#30340;&#19987;&#19994;&#26415;&#35821;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;OpenAI&#30340;Whisper&#27169;&#22411;&#30340;Contextual Biasing Whisper&#65288;CB-Whisper&#65289;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;Whisper&#32534;&#30721;&#22120;&#30340;&#38544;&#34255;&#29366;&#24577;&#25191;&#34892;&#24320;&#25918;&#35789;&#27719;&#20851;&#38190;&#35789;&#26816;&#27979;&#65288;OV-KWS&#65289;&#26469;&#35782;&#21035;&#29992;&#25143;&#23450;&#20041;&#30340;&#21629;&#21517;&#23454;&#20307;&#12290;&#35782;&#21035;&#20986;&#30340;&#23454;&#20307;&#34987;&#29992;&#20316;Whisper&#35299;&#30721;&#22120;&#30340;&#25552;&#31034;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;OV-KWS&#21644;ASR&#20219;&#21153;&#36827;&#34892;&#22810;&#20219;&#21153;&#35757;&#32451;&#30340;&#26041;&#27861;&#26469;&#20248;&#21270;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#21407;&#22987;Whisper&#27169;&#22411;&#30456;&#27604;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#20013;&#22269;Aishell&#28909;&#35789;&#23376;&#38598;&#21644;&#20004;&#20010;&#20869;&#37096;&#20195;&#30721;&#20999;&#25442;&#27979;&#35797;&#38598;&#19978;&#26174;&#33879;&#25552;&#39640;&#20102;&#23454;&#20307;&#21484;&#22238;&#29575;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#20869;&#37096;&#27979;&#35797;&#38598;&#19978;&#28151;&#28102;&#38169;&#35823;&#29575;&#65288;MER&#65289;&#30053;&#24494;&#22686;&#21152;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#24182;&#20351;&#29992;&#19981;&#21516;&#22823;&#23567;&#30340;Whisper&#27169;&#22411;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
End-to-end automatic speech recognition (ASR) systems often struggle to recognize rare name entities, such as personal names, organizations, and terminologies not frequently encountered in the training data. This paper presents Contextual Biasing Whisper (CB-Whisper), a novel ASR system based on OpenAI's Whisper model that can recognize user-defined name entities by performing open-vocabulary keyword-spotting (OV-KWS) using the hidden states of Whisper encoder. The recognized entities are used as prompts for the Whisper decoder. We first propose a multitask training approach with OV-KWS and ASR tasks to optimize the model. Experiments show that this approach substantially improves the entity recalls compared to the original Whisper model on Chinese Aishell hot word subsets and two internal code-switch test sets. However, we observed a slight increase in mixed-error-rate (MER) on internal test sets due to catastrophic forgetting. To address this problem and use different sizes of the Wh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#33258;&#30001;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#38646;&#21644;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#20013;&#23454;&#29616;&#19982;&#27169;&#22411;&#20026;&#22522;&#30784;&#31639;&#27861;&#30456;&#21516;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#39318;&#27425;&#35777;&#26126;&#20102;&#27169;&#22411;&#33258;&#30001;&#31639;&#27861;&#21487;&#20197;&#22312;&#26102;&#38388;&#27573;&#20381;&#36182;&#24615;&#26041;&#38754;&#36798;&#21040;&#21516;&#26679;&#30340;&#20248;&#21270;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.08858</link><description>&lt;p&gt;
&#27809;&#26377;&#27169;&#22411;&#30340;&#31639;&#27861;&#22312;&#38646;&#21644;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#20013;&#25552;&#39640;&#20102;&#26679;&#26412;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Model-Free Algorithm with Improved Sample Efficiency for Zero-Sum Markov Games. (arXiv:2308.08858v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08858
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#33258;&#30001;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#38646;&#21644;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#20013;&#23454;&#29616;&#19982;&#27169;&#22411;&#20026;&#22522;&#30784;&#31639;&#27861;&#30456;&#21516;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#39318;&#27425;&#35777;&#26126;&#20102;&#27169;&#22411;&#33258;&#30001;&#31639;&#27861;&#21487;&#20197;&#22312;&#26102;&#38388;&#27573;&#20381;&#36182;&#24615;&#26041;&#38754;&#36798;&#21040;&#21516;&#26679;&#30340;&#20248;&#21270;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20004;&#20154;&#38646;&#21644;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#38382;&#39064;&#22312;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#29702;&#35770;&#30740;&#31350;&#20013;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20852;&#36259;&#12290;&#29305;&#21035;&#26159;&#23545;&#20110;&#26377;&#38480;&#26102;&#38388;&#27573;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#24050;&#32463;&#35777;&#26126;&#20102;&#27169;&#22411;&#20026;&#22522;&#30784;&#30340;&#31639;&#27861;&#21487;&#20197;&#36890;&#36807;&#26679;&#26412;&#22797;&#26434;&#24230;&#20026;$O(H^3SAB/\epsilon^2)$&#25214;&#21040;$\epsilon$-&#26368;&#20248;&#30340;&#32435;&#20160;&#22343;&#34913;&#65288;NE&#65289;&#65292;&#20854;&#20013;$H$&#26159;&#26102;&#38388;&#27573;&#65292;$S$&#26159;&#29366;&#24577;&#25968;&#37327;&#65288;$A$&#21644;$B$&#20998;&#21035;&#34920;&#31034;&#20004;&#20010;&#29609;&#23478;&#30340;&#21160;&#20316;&#25968;&#37327;&#65289;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#27809;&#26377;&#19968;&#31181;&#29616;&#26377;&#30340;&#27169;&#22411;&#33258;&#30001;&#31639;&#27861;&#21487;&#20197;&#36798;&#21040;&#36825;&#26679;&#30340;&#20248;&#21270;&#25928;&#26524;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#33258;&#30001;&#30340;&#38454;&#27573;&#24615;Q&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#23637;&#31034;&#23427;&#23454;&#29616;&#20102;&#19982;&#26368;&#20339;&#27169;&#22411;&#20026;&#22522;&#30784;&#31639;&#27861;&#30456;&#21516;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#22240;&#27492;&#39318;&#27425;&#35777;&#26126;&#20102;&#27169;&#22411;&#33258;&#30001;&#31639;&#27861;&#21487;&#20197;&#22312;&#26102;&#38388;&#27573;&#20381;&#36182;&#24615;&#26041;&#38754;&#20139;&#21463;&#19982;&#27169;&#22411;&#20026;&#22522;&#30784;&#31639;&#27861;&#30456;&#21516;&#30340;&#20248;&#21270;&#25928;&#26524;&#12290;&#23545;&#20110;$H$&#30340;&#20381;&#36182;&#24615;&#30340;&#20027;&#35201;&#25913;&#36827;&#26469;&#28304;&#20110;...
&lt;/p&gt;
&lt;p&gt;
The problem of two-player zero-sum Markov games has recently attracted increasing interests in theoretical studies of multi-agent reinforcement learning (RL). In particular, for finite-horizon episodic Markov decision processes (MDPs), it has been shown that model-based algorithms can find an $\epsilon$-optimal Nash Equilibrium (NE) with the sample complexity of $O(H^3SAB/\epsilon^2)$, which is optimal in the dependence of the horizon $H$ and the number of states $S$ (where $A$ and $B$ denote the number of actions of the two players, respectively). However, none of the existing model-free algorithms can achieve such an optimality. In this work, we propose a model-free stage-based Q-learning algorithm and show that it achieves the same sample complexity as the best model-based algorithm, and hence for the first time demonstrate that model-free algorithms can enjoy the same optimality in the $H$ dependence as model-based algorithms. The main improvement of the dependency on $H$ arises by
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#21033;&#29992;&#24050;&#24314;&#31435;&#30340;&#27880;&#37322;&#32534;&#30721;&#26412;&#30340;&#30693;&#35782;&#65292;&#25506;&#32034;&#38646;&#26679;&#26412;&#26041;&#27861;&#29992;&#20110;&#25919;&#27835;&#20107;&#20214;&#26412;&#20307;&#20851;&#31995;&#20998;&#31867;&#65292;&#24182;&#20171;&#32461;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;ZSP&#12290;ZSP&#37319;&#29992;&#20102;&#19968;&#31181;&#26641;&#26597;&#35810;&#26694;&#26550;&#65292;&#25552;&#39640;&#20102;&#35299;&#37322;&#24615;&#12289;&#25928;&#29575;&#21644;&#23545;&#27169;&#24335;&#26356;&#25913;&#30340;&#36866;&#24212;&#24615;&#12290;&#22312;&#32454;&#31890;&#24230;&#26681;&#20195;&#30721;&#20998;&#31867;&#19978;&#65292;ZSP&#30340;&#24615;&#33021;&#26126;&#26174;&#20248;&#20110;ChatGPT&#65292;F1&#24471;&#20998;&#25552;&#39640;&#20102;40%&#12290;</title><link>http://arxiv.org/abs/2308.07876</link><description>&lt;p&gt;
&#36890;&#36807;&#32534;&#30721;&#26412;&#30693;&#35782;&#12289;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#21644;ChatGPT&#26469;&#21512;&#25104;&#25919;&#27835;&#38646;&#26679;&#26412;&#20851;&#31995;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Synthesizing Political Zero-Shot Relation Classification via Codebook Knowledge, NLI, and ChatGPT. (arXiv:2308.07876v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07876
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#21033;&#29992;&#24050;&#24314;&#31435;&#30340;&#27880;&#37322;&#32534;&#30721;&#26412;&#30340;&#30693;&#35782;&#65292;&#25506;&#32034;&#38646;&#26679;&#26412;&#26041;&#27861;&#29992;&#20110;&#25919;&#27835;&#20107;&#20214;&#26412;&#20307;&#20851;&#31995;&#20998;&#31867;&#65292;&#24182;&#20171;&#32461;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;ZSP&#12290;ZSP&#37319;&#29992;&#20102;&#19968;&#31181;&#26641;&#26597;&#35810;&#26694;&#26550;&#65292;&#25552;&#39640;&#20102;&#35299;&#37322;&#24615;&#12289;&#25928;&#29575;&#21644;&#23545;&#27169;&#24335;&#26356;&#25913;&#30340;&#36866;&#24212;&#24615;&#12290;&#22312;&#32454;&#31890;&#24230;&#26681;&#20195;&#30721;&#20998;&#31867;&#19978;&#65292;ZSP&#30340;&#24615;&#33021;&#26126;&#26174;&#20248;&#20110;ChatGPT&#65292;F1&#24471;&#20998;&#25552;&#39640;&#20102;40%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#20107;&#20214;&#32534;&#30721;&#30340;&#30417;&#30563;&#27169;&#22411;&#22312;&#24615;&#33021;&#26041;&#38754;&#36828;&#36828;&#36229;&#36807;&#27169;&#24335;&#21305;&#37197;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20165;&#20165;&#20381;&#36182;&#20110;&#26032;&#30340;&#27880;&#37322;&#65292;&#24573;&#35270;&#20102;&#19987;&#23478;&#25968;&#25454;&#24211;&#20013;&#30340;&#22823;&#37327;&#30693;&#35782;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#32454;&#31890;&#24230;&#20998;&#31867;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#24050;&#24314;&#31435;&#30340;&#27880;&#37322;&#32534;&#30721;&#26412;&#30340;&#30693;&#35782;&#65292;&#25506;&#32034;&#38646;&#26679;&#26412;&#26041;&#27861;&#29992;&#20110;&#25919;&#27835;&#20107;&#20214;&#26412;&#20307;&#20851;&#31995;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#28085;&#30422;&#20102;ChatGPT&#21644;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;ZSP&#12290;ZSP&#37319;&#29992;&#20102;&#19968;&#31181;&#26641;&#26597;&#35810;&#26694;&#26550;&#65292;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#19978;&#19979;&#25991;&#12289;&#35821;&#24577;&#21644;&#31867;&#21035;&#28040;&#27495;&#30340;&#19981;&#21516;&#23618;&#27425;&#12290;&#35813;&#26694;&#26550;&#25552;&#39640;&#20102;&#35299;&#37322;&#24615;&#12289;&#25928;&#29575;&#21644;&#23545;&#27169;&#24335;&#26356;&#25913;&#30340;&#36866;&#24212;&#24615;&#12290;&#36890;&#36807;&#22312;&#25105;&#20204;&#26032;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#25351;&#20986;&#20102;ChatGPT&#20013;&#30340;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#31361;&#20986;&#20102;ZSP&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;ZSP&#22312;&#32454;&#31890;&#24230;&#26681;&#20195;&#30721;&#20998;&#31867;&#30340;F1&#24471;&#20998;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25552;&#39640;40%&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent supervised models for event coding vastly outperform pattern-matching methods. However, their reliance solely on new annotations disregards the vast knowledge within expert databases, hindering their applicability to fine-grained classification. To address these limitations, we explore zero-shot approaches for political event ontology relation classification, by leveraging knowledge from established annotation codebooks. Our study encompasses both ChatGPT and a novel natural language inference (NLI) based approach named ZSP. ZSP adopts a tree-query framework that deconstructs the task into context, modality, and class disambiguation levels. This framework improves interpretability, efficiency, and adaptability to schema changes. By conducting extensive experiments on our newly curated datasets, we pinpoint the instability issues within ChatGPT and highlight the superior performance of ZSP. ZSP achieves an impressive 40% improvement in F1 score for fine-grained Rootcode classific
&lt;/p&gt;</description></item><item><title>STAR&#26694;&#26550;&#36890;&#36807;&#35774;&#35745;&#22810;&#31181;&#26102;&#31354;&#22270;&#26469;&#25429;&#25417;&#20301;&#32622;&#30340;&#21160;&#24577;&#26102;&#31354;&#25928;&#24212;&#65292;&#20026;&#20154;&#31867;&#31227;&#21160;&#27169;&#25311;&#20219;&#21153;&#25552;&#20379;&#26032;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.09381</link><description>&lt;p&gt;
&#22522;&#20110;&#26102;&#31354;&#25193;&#23637;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20154;&#31867;&#31227;&#21160;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Spatiotemporal-Augmented Graph Neural Networks for Human Mobility Simulation. (arXiv:2306.09381v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09381
&lt;/p&gt;
&lt;p&gt;
STAR&#26694;&#26550;&#36890;&#36807;&#35774;&#35745;&#22810;&#31181;&#26102;&#31354;&#22270;&#26469;&#25429;&#25417;&#20301;&#32622;&#30340;&#21160;&#24577;&#26102;&#31354;&#25928;&#24212;&#65292;&#20026;&#20154;&#31867;&#31227;&#21160;&#27169;&#25311;&#20219;&#21153;&#25552;&#20379;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#31227;&#21160;&#27169;&#24335;&#22312;&#25919;&#31574;&#20915;&#31574;&#21644;&#32463;&#27982;&#34892;&#20026;&#30740;&#31350;&#20013;&#26377;&#30528;&#37325;&#35201;&#30340;&#24212;&#29992;&#12290;&#20154;&#31867;&#31227;&#21160;&#27169;&#25311;&#20219;&#21153;&#26088;&#22312;&#32473;&#23450;&#19968;&#23567;&#32452;&#36712;&#36857;&#25968;&#25454;&#29983;&#25104;&#20154;&#31867;&#31227;&#21160;&#36712;&#36857;&#65292;&#20294;&#30001;&#20110;&#20154;&#31867;&#31227;&#21160;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#21644;&#31232;&#30095;&#24615;&#65292;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#29616;&#26377;&#26041;&#27861;&#22823;&#22810;&#20381;&#36182;&#20110;&#22320;&#28857;&#20043;&#38388;&#30340;&#38745;&#24577;&#20851;&#31995;&#65292;&#32780;&#24456;&#22823;&#31243;&#24230;&#19978;&#24573;&#30053;&#20102;&#20301;&#32622;&#30340;&#21160;&#24577;&#26102;&#31354;&#25928;&#24212;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21363;SpatioTemporal-Augmented gRaph&#31070;&#32463;&#32593;&#32476;&#65288;STAR&#65289;&#65292;&#26469;&#27169;&#25311;&#20301;&#32622;&#30340;&#21160;&#24577;&#26102;&#31354;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human mobility patterns have shown significant applications in policy-decision scenarios and economic behavior researches. The human mobility simulation task aims to generate human mobility trajectories given a small set of trajectory data, which have aroused much concern due to the scarcity and sparsity of human mobility data. Existing methods mostly rely on the static relationships of locations, while largely neglect the dynamic spatiotemporal effects of locations. On the one hand, spatiotemporal correspondences of visit distributions reveal the spatial proximity and the functionality similarity of locations. On the other hand, the varying durations in different locations hinder the iterative generation process of the mobility trajectory. Therefore, we propose a novel framework to model the dynamic spatiotemporal effects of locations, namely SpatioTemporal-Augmented gRaph neural networks (STAR). The STAR framework designs various spatiotemporal graphs to capture the spatiotemporal co
&lt;/p&gt;</description></item><item><title>&#20026;&#30740;&#31350;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20132;&#20114;&#65292;&#30740;&#31350;&#32773;&#21019;&#24314;&#20102;ArtWhisperer&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#32447;&#28216;&#25103;&#65292;&#20154;&#20204;&#36890;&#36807;&#21453;&#22797;&#23581;&#35797;&#19981;&#21516;&#30340;&#25552;&#31034;&#35789;&#65292;&#26469;&#29983;&#25104;&#21644;&#30446;&#26631;&#22270;&#20687;&#31867;&#20284;&#30340;&#22270;&#20687;&#65292;&#24182;&#35760;&#24405;&#20102;50,000&#22810;&#20010;&#20132;&#20114;&#35760;&#24405;&#12290;&#22312;&#21021;&#27493;&#20998;&#26512;&#20013;&#65292;&#30740;&#31350;&#32773;&#21457;&#29616;&#20154;&#20204;&#25552;&#20132;&#20102;&#21508;&#31181;&#21508;&#26679;&#30340;&#25552;&#31034;&#35789;&#65292;&#24182;&#33021;&#22815;&#21457;&#29616;&#29983;&#25104;&#21508;&#31181;&#25991;&#26412;&#25551;&#36848;&#30340;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2306.08141</link><description>&lt;p&gt;
ArtWhisperer&#65306;&#19968;&#20010;&#29992;&#20110;&#25551;&#36848;&#33402;&#26415;&#21019;&#20316;&#20013;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20132;&#20114;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
ArtWhisperer: A Dataset for Characterizing Human-AI Interactions in Artistic Creations. (arXiv:2306.08141v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08141
&lt;/p&gt;
&lt;p&gt;
&#20026;&#30740;&#31350;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20132;&#20114;&#65292;&#30740;&#31350;&#32773;&#21019;&#24314;&#20102;ArtWhisperer&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#32447;&#28216;&#25103;&#65292;&#20154;&#20204;&#36890;&#36807;&#21453;&#22797;&#23581;&#35797;&#19981;&#21516;&#30340;&#25552;&#31034;&#35789;&#65292;&#26469;&#29983;&#25104;&#21644;&#30446;&#26631;&#22270;&#20687;&#31867;&#20284;&#30340;&#22270;&#20687;&#65292;&#24182;&#35760;&#24405;&#20102;50,000&#22810;&#20010;&#20132;&#20114;&#35760;&#24405;&#12290;&#22312;&#21021;&#27493;&#20998;&#26512;&#20013;&#65292;&#30740;&#31350;&#32773;&#21457;&#29616;&#20154;&#20204;&#25552;&#20132;&#20102;&#21508;&#31181;&#21508;&#26679;&#30340;&#25552;&#31034;&#35789;&#65292;&#24182;&#33021;&#22815;&#21457;&#29616;&#29983;&#25104;&#21508;&#31181;&#25991;&#26412;&#25551;&#36848;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#36234;&#26469;&#36234;&#26222;&#21450;&#65292;&#30740;&#31350;&#20154;&#31867;&#29992;&#25143;&#22914;&#20309;&#19982;&#36825;&#20123;&#27169;&#22411;&#20132;&#20114;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20154;&#20204;&#22914;&#20309;&#20351;&#29992;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#27169;&#22411;&#29983;&#25104;&#25152;&#38656;&#30340;&#30446;&#26631;&#22270;&#20687;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#31181;&#20132;&#20114;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;ArtWhisperer&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#32447;&#28216;&#25103;&#65292;&#29992;&#25143;&#20250;&#24471;&#21040;&#19968;&#20010;&#30446;&#26631;&#22270;&#20687;&#65292;&#24182;&#38656;&#35201;&#21453;&#22797;&#23581;&#35797;&#19981;&#21516;&#30340;&#25552;&#31034;&#35789;&#65292;&#20197;&#20415;&#29983;&#25104;&#31867;&#20284;&#30446;&#26631;&#22270;&#20687;&#30340;&#22270;&#20687;&#12290;&#36890;&#36807;&#36825;&#20010;&#28216;&#25103;&#65292;&#25105;&#20204;&#35760;&#24405;&#20102;50,000&#22810;&#20010;&#20154;&#24037;&#26234;&#33021;-&#20154;&#31867;&#20132;&#20114;&#30340;&#35760;&#24405;&#65307;&#27599;&#20010;&#20132;&#20114;&#37117;&#23545;&#24212;&#30528;&#29992;&#25143;&#21019;&#24314;&#30340;&#19968;&#20010;&#25552;&#31034;&#35789;&#21644;&#30456;&#24212;&#29983;&#25104;&#30340;&#22270;&#20687;&#12290;&#22823;&#22810;&#25968;&#35760;&#24405;&#37117;&#26159;&#37325;&#22797;&#30340;&#20132;&#20114;&#65292;&#29992;&#25143;&#36890;&#36807;&#21453;&#22797;&#23581;&#35797;&#25214;&#21040;&#26368;&#20339;&#30340;&#25552;&#31034;&#35789;&#20197;&#29983;&#25104;&#30446;&#26631;&#22270;&#20687;&#65292;&#36825;&#20351;&#24471;&#36825;&#20010;&#25968;&#25454;&#38598;&#25104;&#20026;&#30740;&#31350;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21327;&#20316;&#30340;&#29420;&#29305;&#36830;&#32493;&#25968;&#25454;&#38598;&#12290;&#22312;&#23545;&#36825;&#20010;&#25968;&#25454;&#38598;&#30340;&#21021;&#27493;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20123;&#25552;&#31034;&#35789;&#20132;&#20114;&#21644;&#29992;&#25143;&#31574;&#30053;&#30340;&#29305;&#24449;&#12290;&#20154;&#20204;&#25552;&#20132;&#20102;&#21508;&#31181;&#21508;&#26679;&#30340;&#25552;&#31034;&#35789;&#65292;&#24182;&#33021;&#22815;&#21457;&#29616;&#29983;&#25104;&#21508;&#31181;&#25991;&#26412;&#25551;&#36848;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
As generative AI becomes more prevalent, it is important to study how human users interact with such models. In this work, we investigate how people use text-to-image models to generate desired target images. To study this interaction, we created ArtWhisperer, an online game where users are given a target image and are tasked with iteratively finding a prompt that creates a similar-looking image as the target. Through this game, we recorded over 50,000 human-AI interactions; each interaction corresponds to one text prompt created by a user and the corresponding generated image. The majority of these are repeated interactions where a user iterates to find the best prompt for their target image, making this a unique sequential dataset for studying human-AI collaborations. In an initial analysis of this dataset, we identify several characteristics of prompt interactions and user strategies. People submit diverse prompts and are able to discover a variety of text descriptions that generate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;"&#22240;&#26524;&#31354;&#38388;"&#30340;&#27010;&#24565;&#65292;&#26088;&#22312;&#20197;&#26607;&#23572;&#33707;&#25096;&#32599;&#22827;&#30340;&#27010;&#29575;&#27979;&#24230;&#20844;&#29702;&#21270;&#20026;&#36215;&#28857;&#65292;&#23454;&#29616;&#23545;&#22240;&#26524;&#20851;&#31995;&#30340;&#20844;&#29702;&#21270;&#65292;&#24182;&#25104;&#21151;&#22320;&#35299;&#20915;&#20102;&#29616;&#26377;&#26694;&#26550;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2305.17139</link><description>&lt;p&gt;
&#22240;&#26524;&#20851;&#31995;&#30340;&#27979;&#24230;&#35770;&#20844;&#29702;&#21270;
&lt;/p&gt;
&lt;p&gt;
A Measure-Theoretic Axiomatisation of Causality. (arXiv:2305.17139v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17139
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;"&#22240;&#26524;&#31354;&#38388;"&#30340;&#27010;&#24565;&#65292;&#26088;&#22312;&#20197;&#26607;&#23572;&#33707;&#25096;&#32599;&#22827;&#30340;&#27010;&#29575;&#27979;&#24230;&#20844;&#29702;&#21270;&#20026;&#36215;&#28857;&#65292;&#23454;&#29616;&#23545;&#22240;&#26524;&#20851;&#31995;&#30340;&#20844;&#29702;&#21270;&#65292;&#24182;&#25104;&#21151;&#22320;&#35299;&#20915;&#20102;&#29616;&#26377;&#26694;&#26550;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#20851;&#31995;&#26159;&#35768;&#22810;&#30740;&#31350;&#39046;&#22495;&#20013;&#30340;&#26680;&#24515;&#27010;&#24565;&#65292;&#20294;&#20173;&#28982;&#27809;&#26377;&#26222;&#36941;&#35748;&#21487;&#30340;&#22240;&#26524;&#20851;&#31995;&#20844;&#29702;&#21270;&#12290;&#25105;&#20204;&#23558;&#22240;&#26524;&#20851;&#31995;&#35270;&#20026;&#27010;&#29575;&#29702;&#35770;&#30340;&#25193;&#23637;&#65292;&#24182;&#20316;&#20026;&#30740;&#31350;&#22312;&#31995;&#32479;&#19978;&#24178;&#39044;&#26102;&#20250;&#21457;&#29983;&#20160;&#20040;&#30340;&#30740;&#31350;&#65292;&#24182;&#25552;&#35758;&#20197;&#26607;&#23572;&#33707;&#25096;&#32599;&#22827;&#30340;&#27010;&#29575;&#27979;&#24230;&#20844;&#29702;&#21270;&#20316;&#20026;&#22240;&#26524;&#20851;&#31995;&#20844;&#29702;&#21270;&#30340;&#36215;&#28857;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;"&#22240;&#26524;&#31354;&#38388;"&#30340;&#27010;&#24565;&#65292;&#21253;&#25324;&#19968;&#20010;&#27010;&#29575;&#31354;&#38388;&#21644;&#31216;&#20026;"&#22240;&#26524;&#26680;"&#30340;&#36716;&#31227;&#27010;&#29575;&#26680;&#30340;&#38598;&#21512;&#65292;&#29992;&#26469;&#32534;&#30721;&#35813;&#31354;&#38388;&#30340;&#22240;&#26524;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#19981;&#20165;&#22312;&#27979;&#24230;&#35770;&#19978;&#20005;&#26684;&#22320;&#22522;&#20110;&#65292;&#36824;&#25581;&#31034;&#20102;&#29616;&#26377;&#26694;&#26550;&#30340;&#38271;&#26399;&#38480;&#21046;&#65292;&#20363;&#22914;&#65292;&#24490;&#29615;&#12289;&#28508;&#22312;&#21464;&#37327;&#21644;&#38543;&#26426;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causality is a central concept in a wide range of research areas, yet there is still no universally agreed axiomatisation of causality. We view causality both as an extension of probability theory and as a study of \textit{what happens when one intervenes on a system}, and argue in favour of taking Kolmogorov's measure-theoretic axiomatisation of probability as the starting point towards an axiomatisation of causality. To that end, we propose the notion of a \textit{causal space}, consisting of a probability space along with a collection of transition probability kernels, called \textit{causal kernels}, that encode the causal information of the space. Our proposed framework is not only rigorously grounded in measure theory, but it also sheds light on long-standing limitations of existing frameworks including, for example, cycles, latent variables and stochastic processes.
&lt;/p&gt;</description></item><item><title>C-MCTS &#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26377;&#32422;&#26463;&#30340;&#20915;&#31574;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#23433;&#20840;&#35780;&#21028;&#22120;&#36827;&#34892;&#25104;&#26412;&#20272;&#35745;&#65292;&#24182;&#22312;&#37096;&#32626;&#26399;&#38388;&#36890;&#36807;&#21098;&#26525;&#19981;&#23433;&#20840;&#36712;&#36857;&#26469;&#38480;&#21046;&#25506;&#32034;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#22870;&#21169;&#21644;&#26356;&#39640;&#25928;&#30340;&#35268;&#21010;&#27493;&#39588;&#12290;</title><link>http://arxiv.org/abs/2305.16209</link><description>&lt;p&gt;
C-MCTS: &#23433;&#20840;&#35268;&#21010;&#19982;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
C-MCTS: Safe Planning with Monte Carlo Tree Search. (arXiv:2305.16209v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16209
&lt;/p&gt;
&lt;p&gt;
C-MCTS &#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26377;&#32422;&#26463;&#30340;&#20915;&#31574;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#23433;&#20840;&#35780;&#21028;&#22120;&#36827;&#34892;&#25104;&#26412;&#20272;&#35745;&#65292;&#24182;&#22312;&#37096;&#32626;&#26399;&#38388;&#36890;&#36807;&#21098;&#26525;&#19981;&#23433;&#20840;&#36712;&#36857;&#26469;&#38480;&#21046;&#25506;&#32034;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#22870;&#21169;&#21644;&#26356;&#39640;&#25928;&#30340;&#35268;&#21010;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#32422;&#26463;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;CMDP&#65289;&#21487;&#20197;&#35299;&#20915;&#21463;&#32422;&#26463;&#30340;&#23433;&#20840;&#20915;&#31574;&#38382;&#39064;&#12290;&#23613;&#31649;CMDP&#22312;&#24378;&#21270;&#23398;&#20064;&#30340;&#25991;&#29486;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#23545;&#20110;&#20351;&#29992;MCTS&#31561;&#22522;&#20110;&#37319;&#26679;&#30340;&#35268;&#21010;&#31639;&#27861;&#26469;&#35299;&#20915;CMDP&#30340;&#30740;&#31350;&#21364;&#24456;&#23569;&#12290;&#20197;&#24448;&#30340;&#26041;&#27861;&#22312;&#25104;&#26412;&#26041;&#38754;&#20445;&#23432;&#34892;&#20107;&#65292;&#36890;&#36807;&#20351;&#29992;&#33945;&#29305;&#21345;&#27931;&#25104;&#26412;&#20272;&#35745;&#26469;&#36991;&#20813;&#36829;&#21453;&#32422;&#26463;&#65292;&#20294;&#36825;&#31181;&#20272;&#35745;&#23384;&#22312;&#39640;&#26041;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#32422;&#26463;MCTS&#65288;C-MCTS&#65289;&#65292;&#23427;&#20351;&#29992;&#20808;&#21069;&#22312;&#20195;&#29702;&#37096;&#32626;&#20043;&#21069;&#36890;&#36807;&#26102;&#38388;&#24046;&#20998;&#23398;&#20064;&#35757;&#32451;&#30340;&#23433;&#20840;&#35780;&#21028;&#22120;&#26469;&#20272;&#35745;&#25104;&#26412;&#12290;&#22312;&#37096;&#32626;&#26399;&#38388;&#65292;&#35780;&#21028;&#22120;&#36890;&#36807;&#21098;&#26525;&#19981;&#23433;&#20840;&#36712;&#36857;&#26469;&#38480;&#21046;&#25506;&#32034;&#12290;C-MCTS&#28385;&#36275;&#25104;&#26412;&#32422;&#26463;&#65292;&#20294;&#25805;&#20316;&#25509;&#36817;&#32422;&#26463;&#36793;&#30028;&#65292;&#27604;&#20197;&#24448;&#30340;&#24037;&#20316;&#33719;&#24471;&#26356;&#39640;&#30340;&#22870;&#21169;&#12290;&#20316;&#20026;&#19968;&#20010;&#24456;&#22909;&#30340;&#21103;&#20135;&#21697;&#65292;&#36825;&#20010;&#35268;&#21010;&#22120;&#22312;&#35268;&#21010;&#27493;&#39588;&#26041;&#38754;&#26356;&#21152;&#39640;&#25928;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#22312;&#27169;&#22411;&#19979;&#65292;
&lt;/p&gt;
&lt;p&gt;
The Constrained Markov Decision Process (CMDP) formulation allows to solve safety-critical decision making tasks that are subject to constraints. While CMDPs have been extensively studied in the Reinforcement Learning literature, little attention has been given to sampling-based planning algorithms such as MCTS for solving them. Previous approaches perform conservatively with respect to costs as they avoid constraint violations by using Monte Carlo cost estimates that suffer from high variance. We propose Constrained MCTS (C-MCTS), which estimates cost using a safety critic that is trained with Temporal Difference learning in an offline phase prior to agent deployment. The critic limits exploration by pruning unsafe trajectories within MCTS during deployment. C-MCTS satisfies cost constraints but operates closer to the constraint boundary, achieving higher rewards than previous work. As a nice byproduct, the planner is more efficient w.r.t. planning steps. Most importantly, under model
&lt;/p&gt;</description></item><item><title>&#25968;&#25454;&#32593;&#26684;&#26159;&#19968;&#31181;&#20419;&#36827;&#25968;&#25454;&#27665;&#20027;&#21270;&#30340;&#31038;&#20250;&#25216;&#26415;&#27010;&#24565;&#65292;&#20854;&#21160;&#26426;&#22240;&#32032;&#21253;&#25324;&#21162;&#21147;&#25104;&#20026;&#26356;&#20855;&#25968;&#25454;&#39537;&#21160;&#24615;&#65292;&#25361;&#25112;&#21253;&#25324;&#21521;&#32852;&#37030;&#27835;&#29702;&#30340;&#36716;&#21464;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#38656;&#35201;&#26368;&#20339;&#23454;&#36341;&#30340;&#25351;&#23548;&#65292;&#20197;&#23454;&#29616;&#20854;&#28508;&#22312;&#30340;&#19994;&#21153;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2302.01713</link><description>&lt;p&gt;
&#25968;&#25454;&#32593;&#26684;&#65306;&#21160;&#26426;&#22240;&#32032;&#12289;&#25361;&#25112;&#21644;&#26368;&#20339;&#23454;&#36341;
&lt;/p&gt;
&lt;p&gt;
Data Mesh: Motivational Factors, Challenges, and Best Practices. (arXiv:2302.01713v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01713
&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#32593;&#26684;&#26159;&#19968;&#31181;&#20419;&#36827;&#25968;&#25454;&#27665;&#20027;&#21270;&#30340;&#31038;&#20250;&#25216;&#26415;&#27010;&#24565;&#65292;&#20854;&#21160;&#26426;&#22240;&#32032;&#21253;&#25324;&#21162;&#21147;&#25104;&#20026;&#26356;&#20855;&#25968;&#25454;&#39537;&#21160;&#24615;&#65292;&#25361;&#25112;&#21253;&#25324;&#21521;&#32852;&#37030;&#27835;&#29702;&#30340;&#36716;&#21464;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#38656;&#35201;&#26368;&#20339;&#23454;&#36341;&#30340;&#25351;&#23548;&#65292;&#20197;&#23454;&#29616;&#20854;&#28508;&#22312;&#30340;&#19994;&#21153;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25968;&#25454;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#26085;&#30410;&#37325;&#35201;&#65292;&#32452;&#32455;&#21162;&#21147;&#25104;&#20026;&#26356;&#20855;&#25968;&#25454;&#39537;&#21160;&#24615;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#25968;&#25454;&#26550;&#26500;&#24182;&#19981;&#19968;&#23450;&#35774;&#35745;&#29992;&#20110;&#24212;&#23545;&#25968;&#25454;&#21644;&#20998;&#26512;&#29992;&#20363;&#30340;&#35268;&#27169;&#21644;&#33539;&#22260;&#12290;&#20107;&#23454;&#19978;&#65292;&#29616;&#26377;&#26550;&#26500;&#24120;&#24120;&#26080;&#27861;&#23454;&#29616;&#23427;&#20204;&#25152;&#25215;&#35834;&#30340;&#20215;&#20540;&#12290;&#25968;&#25454;&#32593;&#26684;&#26159;&#19968;&#20010;&#31038;&#20250;&#25216;&#26415;&#27010;&#24565;&#65292;&#20854;&#20013;&#21253;&#21547;&#26550;&#26500;&#26041;&#38754;&#30340;&#20869;&#23481;&#65292;&#20197;&#20419;&#36827;&#25968;&#25454;&#27665;&#20027;&#21270;&#65292;&#24182;&#20351;&#32452;&#32455;&#30495;&#27491;&#25104;&#20026;&#25968;&#25454;&#39537;&#21160;&#22411;&#12290;&#30001;&#20110;&#25968;&#25454;&#32593;&#26684;&#30340;&#27010;&#24565;&#20173;&#28982;&#26159;&#26032;&#39062;&#30340;&#65292;&#22240;&#27492;&#32570;&#20047;&#26469;&#33258;&#23454;&#22320;&#30340;&#32463;&#39564;&#35777;&#23454;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#32570;&#23569;&#20102;&#35299;&#24341;&#20837;&#25968;&#25454;&#32593;&#26684;&#30340;&#21160;&#26426;&#22240;&#32032;&#12289;&#30456;&#20851;&#25361;&#25112;&#12289;&#26368;&#20339;&#23454;&#36341;&#12289;&#20854;&#19994;&#21153;&#24433;&#21709;&#21644;&#28508;&#22312;&#21407;&#22411;&#30340;&#29702;&#35299;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#23545;15&#20301;&#34892;&#19994;&#19987;&#23478;&#36827;&#34892;&#20102;&#21322;&#32467;&#26500;&#21270;&#35775;&#35848;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#34892;&#19994;&#19987;&#23478;&#22312;&#21521;&#32852;&#37030;&#27835;&#29702;&#30340;&#36716;&#21464;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increasing importance of data and artificial intelligence, organizations strive to become more data-driven. However, current data architectures are not necessarily designed to keep up with the scale and scope of data and analytics use cases. In fact, existing architectures often fail to deliver the promised value associated with them. Data mesh is a socio-technical concept that includes architectural aspects to promote data democratization and enables organizations to become truly data-driven. As the concept of data mesh is still novel, it lacks empirical insights from the field. Specifically, an understanding of the motivational factors for introducing data mesh, the associated challenges, best practices, its business impact, and potential archetypes, is missing. To address this gap, we conduct 15 semi-structured interviews with industry experts. Our results show, among other insights, that industry experts have difficulties with the transition toward federated governance ass
&lt;/p&gt;</description></item></channel></rss>