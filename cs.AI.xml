<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20026;&#20195;&#30721;&#29255;&#27573;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#25688;&#35201;&#30340;&#20219;&#21153;&#65292;&#21457;&#29616;&#20195;&#30721;LLMs&#20248;&#20110;&#36890;&#29992;&#23545;&#24212;&#27169;&#22411;&#65292;&#22312;&#22788;&#29702;&#20855;&#26377;&#19981;&#30456;&#20284;&#20998;&#24067;&#30340;&#25968;&#25454;&#38598;&#26102;&#65292;&#38646;&#26679;&#26412;&#26041;&#27861;&#21487;&#20197;&#24471;&#21040;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.16673</link><description>&lt;p&gt;
&#25506;&#32034;&#29992;&#20110;&#20195;&#30721;&#35299;&#37322;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Exploring Large Language Models for Code Explanation. (arXiv:2310.16673v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16673
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20026;&#20195;&#30721;&#29255;&#27573;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#25688;&#35201;&#30340;&#20219;&#21153;&#65292;&#21457;&#29616;&#20195;&#30721;LLMs&#20248;&#20110;&#36890;&#29992;&#23545;&#24212;&#27169;&#22411;&#65292;&#22312;&#22788;&#29702;&#20855;&#26377;&#19981;&#30456;&#20284;&#20998;&#24067;&#30340;&#25968;&#25454;&#38598;&#26102;&#65292;&#38646;&#26679;&#26412;&#26041;&#27861;&#21487;&#20197;&#24471;&#21040;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#20195;&#30721;&#25991;&#26723;&#36890;&#36807;&#35299;&#37322;&#24615;&#25991;&#26412;&#22312;&#20195;&#30721;&#29702;&#35299;&#26041;&#38754;&#21487;&#33021;&#38750;&#24120;&#26377;&#30410;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#29305;&#21035;&#26159;&#22312;&#36719;&#20214;&#24037;&#31243;&#20219;&#21153;&#20013;&#65292;&#22914;&#20195;&#30721;&#29983;&#25104;&#21644;&#20195;&#30721;&#25688;&#35201;&#12290;&#26412;&#30740;&#31350;&#20855;&#20307;&#30740;&#31350;&#20102;&#20351;&#29992;&#21508;&#31181;LLMs&#20026;&#20195;&#30721;&#29255;&#27573;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#25688;&#35201;&#30340;&#20219;&#21153;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20195;&#30721;LLMs&#20248;&#20110;&#20854;&#36890;&#29992;&#23545;&#24212;&#27169;&#22411;&#65292;&#24182;&#19988;&#24403;&#22788;&#29702;&#20855;&#26377;&#35757;&#32451;&#38598;&#21644;&#27979;&#35797;&#38598;&#20043;&#38388;&#20998;&#24067;&#19981;&#30456;&#20284;&#30340;&#25968;&#25454;&#38598;&#26102;&#65292;&#38646;&#26679;&#26412;&#26041;&#27861;&#20135;&#29983;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automating code documentation through explanatory text can prove highly beneficial in code understanding. Large Language Models (LLMs) have made remarkable strides in Natural Language Processing, especially within software engineering tasks such as code generation and code summarization. This study specifically delves into the task of generating natural-language summaries for code snippets, using various LLMs. The findings indicate that Code LLMs outperform their generic counterparts, and zero-shot methods yield superior results when dealing with datasets with dissimilar distributions between training and testing sets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21407;&#21017;&#24615;&#37325;&#20889;&#26041;&#27861;&#26469;&#25913;&#21892;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#25928;&#26524;&#65292;&#36890;&#36807;&#20351;&#29992;&#19987;&#38376;&#30340;&#33258;&#21160;&#23383;&#24149;&#27169;&#22411;&#37325;&#26032;&#26631;&#27880;&#35821;&#26009;&#24211;&#65292;&#24182;&#22312;&#37325;&#20889;&#21518;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#65292;&#27169;&#22411;&#22312;&#25972;&#20307;&#22270;&#20687;&#36136;&#37327;&#26041;&#38754;&#24471;&#21040;&#20102;&#26174;&#33879;&#25913;&#21892;&#12290;</title><link>http://arxiv.org/abs/2310.16656</link><description>&lt;p&gt;
&#19968;&#24352;&#22270;&#29255;&#32988;&#36807;&#21315;&#35328;&#19975;&#35821;&#65306;&#21407;&#21017;&#24615;&#37325;&#20889;&#25913;&#21892;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
A Picture is Worth a Thousand Words: Principled Recaptioning Improves Image Generation. (arXiv:2310.16656v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16656
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21407;&#21017;&#24615;&#37325;&#20889;&#26041;&#27861;&#26469;&#25913;&#21892;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#25928;&#26524;&#65292;&#36890;&#36807;&#20351;&#29992;&#19987;&#38376;&#30340;&#33258;&#21160;&#23383;&#24149;&#27169;&#22411;&#37325;&#26032;&#26631;&#27880;&#35821;&#26009;&#24211;&#65292;&#24182;&#22312;&#37325;&#20889;&#21518;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#65292;&#27169;&#22411;&#22312;&#25972;&#20307;&#22270;&#20687;&#36136;&#37327;&#26041;&#38754;&#24471;&#21040;&#20102;&#26174;&#33879;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#31361;&#30772;&#65292;&#20351;&#24471;&#20174;&#25991;&#26412;&#25552;&#31034;&#20013;&#39640;&#36136;&#37327;&#19988;&#22810;&#26679;&#21270;&#22320;&#21512;&#25104;&#22270;&#20687;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26159;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#20063;&#24120;&#24120;&#38590;&#20197;&#20934;&#30830;&#22320;&#36981;&#24490;&#20854;&#25552;&#31034;&#20013;&#30340;&#25152;&#26377;&#25351;&#20196;&#12290;&#36825;&#20123;&#27169;&#22411;&#20013;&#32477;&#22823;&#37096;&#20998;&#26159;&#22312;&#30001;&#65288;&#22270;&#20687;&#65292;&#23383;&#24149;&#65289;&#23545;&#32452;&#25104;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#20854;&#20013;&#22270;&#20687;&#36890;&#24120;&#26469;&#33258;&#32593;&#32476;&#65292;&#32780;&#23383;&#24149;&#21017;&#26159;&#23427;&#20204;&#30340;HTML&#26367;&#20195;&#25991;&#26412;&#12290;&#19968;&#20010;&#26174;&#33879;&#30340;&#20363;&#23376;&#26159;LAION&#25968;&#25454;&#38598;&#65292;&#34987;Stable Diffusion&#21644;&#20854;&#20182;&#27169;&#22411;&#20351;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#36825;&#20123;&#23383;&#24149;&#36890;&#24120;&#36136;&#37327;&#36739;&#20302;&#65292;&#24182;&#35748;&#20026;&#36825;&#26174;&#33879;&#24433;&#21709;&#20102;&#27169;&#22411;&#29702;&#35299;&#25991;&#26412;&#25552;&#31034;&#20013;&#24494;&#22937;&#35821;&#20041;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#23637;&#31034;&#36890;&#36807;&#20351;&#29992;&#19987;&#38376;&#30340;&#33258;&#21160;&#23383;&#24149;&#27169;&#22411;&#37325;&#26032;&#26631;&#27880;&#35821;&#26009;&#24211;&#65292;&#24182;&#22312;&#37325;&#20889;&#21518;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#65292;&#27169;&#22411;&#22312;&#21508;&#20010;&#26041;&#38754;&#37117;&#20250;&#24471;&#21040;&#22823;&#24133;&#24230;&#30340;&#25913;&#21892;&#12290;&#39318;&#20808;&#65292;&#22312;&#25972;&#20307;&#22270;&#20687;&#36136;&#37327;&#26041;&#38754;&#65306;&#20363;&#22914;FID 14.84 vs. t
&lt;/p&gt;
&lt;p&gt;
Text-to-image diffusion models achieved a remarkable leap in capabilities over the last few years, enabling high-quality and diverse synthesis of images from a textual prompt. However, even the most advanced models often struggle to precisely follow all of the directions in their prompts. The vast majority of these models are trained on datasets consisting of (image, caption) pairs where the images often come from the web, and the captions are their HTML alternate text. A notable example is the LAION dataset, used by Stable Diffusion and other models. In this work we observe that these captions are often of low quality, and argue that this significantly affects the model's capability to understand nuanced semantics in the textual prompts. We show that by relabeling the corpus with a specialized automatic captioning model and training a text-to-image model on the recaptioned dataset, the model benefits substantially across the board. First, in overall image quality: e.g. FID 14.84 vs. t
&lt;/p&gt;</description></item><item><title>ArTST&#26159;&#19968;&#31181;&#29992;&#20110;&#25903;&#25345;&#38463;&#25289;&#20271;&#35821;&#24320;&#28304;&#35821;&#38899;&#25216;&#26415;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;&#20174;&#22836;&#24320;&#22987;&#30340;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#65292;&#23427;&#22312;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#12289;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#21644;&#21475;&#35821;&#26041;&#35328;&#35782;&#21035;&#20219;&#21153;&#20013;&#36798;&#21040;&#20102;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#25216;&#26415;&#30456;&#24403;&#29978;&#33267;&#36229;&#36807;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.16621</link><description>&lt;p&gt;
ArTST: &#38463;&#25289;&#20271;&#25991;&#26412;&#21644;&#35821;&#38899;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
ArTST: Arabic Text and Speech Transformer. (arXiv:2310.16621v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16621
&lt;/p&gt;
&lt;p&gt;
ArTST&#26159;&#19968;&#31181;&#29992;&#20110;&#25903;&#25345;&#38463;&#25289;&#20271;&#35821;&#24320;&#28304;&#35821;&#38899;&#25216;&#26415;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;&#20174;&#22836;&#24320;&#22987;&#30340;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#65292;&#23427;&#22312;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#12289;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#21644;&#21475;&#35821;&#26041;&#35328;&#35782;&#21035;&#20219;&#21153;&#20013;&#36798;&#21040;&#20102;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#25216;&#26415;&#30456;&#24403;&#29978;&#33267;&#36229;&#36807;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25903;&#25345;&#38463;&#25289;&#20271;&#35821;&#24320;&#28304;&#35821;&#38899;&#25216;&#26415;&#30340;&#39044;&#35757;&#32451;&#38463;&#25289;&#20271;&#25991;&#26412;&#21644;&#35821;&#38899;&#21464;&#25442;&#22120;ArTST&#12290;&#35813;&#27169;&#22411;&#26550;&#26500;&#36981;&#24490;&#26368;&#36817;&#21457;&#24067;&#30340;&#33521;&#25991;&#32479;&#19968;&#27169;&#24577;&#26694;&#26550;SpeechT5&#65292;&#24182;&#19988;&#19987;&#27880;&#20110;&#29616;&#20195;&#26631;&#20934;&#38463;&#25289;&#20271;&#35821;&#65288;MSA&#65289;&#65292;&#35745;&#21010;&#23558;&#27169;&#22411;&#25193;&#23637;&#21040;&#26410;&#26469;&#29256;&#26412;&#30340;&#26041;&#35328;&#21644;&#28151;&#21512;&#38463;&#25289;&#20271;&#35821;&#12290;&#25105;&#20204;&#20174;&#22836;&#24320;&#22987;&#22312;MSA&#35821;&#38899;&#21644;&#25991;&#26412;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#65292;&#24182;&#23545;&#20197;&#19979;&#20219;&#21153;&#36827;&#34892;&#20102;&#24494;&#35843;&#65306;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#12289;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#65288;TTS&#65289;&#21644;&#21475;&#35821;&#26041;&#35328;&#35782;&#21035;&#12290;&#36890;&#36807;&#19982;SpeechT5&#20197;&#21450;&#20808;&#21069;&#25253;&#36947;&#30340;&#36825;&#20123;&#20219;&#21153;&#30340;&#32467;&#26524;&#36827;&#34892;&#27604;&#36739;&#65292;ArTST&#22312;&#25152;&#26377;&#19977;&#20010;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#25345;&#24179;&#25110;&#36229;&#36807;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#39044;&#35757;&#32451;&#26377;&#21161;&#20110;&#27867;&#21270;&#65292;&#36825;&#22312;&#20302;&#36164;&#28304;TTS&#20219;&#21153;&#20013;&#29305;&#21035;&#26126;&#26174;&#12290;&#39044;&#35757;&#32451;&#27169;&#22411;&#20197;&#21450;&#24494;&#35843;&#30340;ASR&#21644;TTS&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
We present ArTST, a pre-trained Arabic text and speech transformer for supporting open-source speech technologies for the Arabic language. The model architecture follows the unified-modal framework, SpeechT5, that was recently released for English, and is focused on Modern Standard Arabic (MSA), with plans to extend the model for dialectal and code-switched Arabic in future editions. We pre-trained the model from scratch on MSA speech and text data, and fine-tuned it for the following tasks: Automatic Speech Recognition (ASR), Text-To-Speech synthesis (TTS), and spoken dialect identification. In our experiments comparing ArTST with SpeechT5, as well as with previously reported results in these tasks, ArTST performs on a par with or exceeding the current state-of-the-art in all three tasks. Moreover, we find that our pre-training is conducive for generalization, which is particularly evident in the low-resource TTS task. The pre-trained model as well as the fine-tuned ASR and TTS models
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21453;&#36716;&#24405;&#27861;&#21644;&#38169;&#35823;&#20998;&#31867;&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#35821;&#38899;&#35782;&#21035;&#38169;&#35823;&#23545;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#34920;&#26126;&#21512;&#25104;&#35821;&#38899;&#19982;&#38899;&#39057;&#24405;&#21046;&#23545;&#20110;&#35813;&#26041;&#27861;&#30340;&#32467;&#26524;&#27809;&#26377;&#26174;&#33879;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2310.16609</link><description>&lt;p&gt;
&#20316;&#20026;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#27169;&#22411;&#23545;&#35821;&#38899;&#35782;&#21035;&#38169;&#35823;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#30340;&#21453;&#36716;&#24405;&#27861;
&lt;/p&gt;
&lt;p&gt;
Back Transcription as a Method for Evaluating Robustness of Natural Language Understanding Models to Speech Recognition Errors. (arXiv:2310.16609v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16609
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21453;&#36716;&#24405;&#27861;&#21644;&#38169;&#35823;&#20998;&#31867;&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#35821;&#38899;&#35782;&#21035;&#38169;&#35823;&#23545;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#34920;&#26126;&#21512;&#25104;&#35821;&#38899;&#19982;&#38899;&#39057;&#24405;&#21046;&#23545;&#20110;&#35813;&#26041;&#27861;&#30340;&#32467;&#26524;&#27809;&#26377;&#26174;&#33879;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21475;&#35821;&#23545;&#35805;&#31995;&#32479;&#20013;&#65292;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#27169;&#22411;&#20043;&#21069;&#26159;&#19968;&#20010;&#21487;&#33021;&#24433;&#21709;&#20854;&#24615;&#33021;&#30340;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#30740;&#31350;&#35821;&#38899;&#35782;&#21035;&#38169;&#35823;&#23545;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#27169;&#22411;&#24615;&#33021;&#24433;&#21709;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#23558;&#21453;&#36716;&#24405;&#31243;&#24207;&#19982;&#32454;&#31890;&#24230;&#30340;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#23545;&#24433;&#21709;NLU&#27169;&#22411;&#24615;&#33021;&#30340;&#38169;&#35823;&#36827;&#34892;&#20998;&#31867;&#12290;&#35813;&#26041;&#27861;&#20381;&#36182;&#20110;&#20351;&#29992;&#21512;&#25104;&#35821;&#38899;&#36827;&#34892;NLU&#35780;&#20272;&#12290;&#25105;&#20204;&#35777;&#26126;&#22312;&#37325;&#35201;&#31243;&#24230;&#19978;&#65292;&#23558;&#21512;&#25104;&#35821;&#38899;&#29992;&#20110;&#38899;&#39057;&#24405;&#21046;&#30340;&#26367;&#20195;&#26041;&#27861;&#19981;&#20250;&#26174;&#33879;&#25913;&#21464;&#25152;&#25552;&#20986;&#25216;&#26415;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In a spoken dialogue system, an NLU model is preceded by a speech recognition system that can deteriorate the performance of natural language understanding. This paper proposes a method for investigating the impact of speech recognition errors on the performance of natural language understanding models. The proposed method combines the back transcription procedure with a fine-grained technique for categorizing the errors that affect the performance of NLU models. The method relies on the usage of synthesized speech for NLU evaluation. We show that the use of synthesized speech in place of audio recording does not change the outcomes of the presented technique in a significant way.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32452;&#21512;&#29420;&#31435;&#26174;&#33879;&#24615;&#26816;&#39564;&#26102;&#24179;&#34913;&#20013;&#24515;&#21644;&#36793;&#32536;&#25298;&#32477;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#27979;&#37327;&#20004;&#32773;&#24179;&#34913;&#30340;&#32452;&#21512;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2310.16600</link><description>&lt;p&gt;
&#22312;&#32452;&#21512;&#29420;&#31435;&#26174;&#33879;&#24615;&#26816;&#39564;&#26102;&#24179;&#34913;&#20013;&#24515;&#21644;&#36793;&#32536;&#25298;&#32477;
&lt;/p&gt;
&lt;p&gt;
Balancing central and marginal rejection when combining independent significance tests. (arXiv:2310.16600v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16600
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32452;&#21512;&#29420;&#31435;&#26174;&#33879;&#24615;&#26816;&#39564;&#26102;&#24179;&#34913;&#20013;&#24515;&#21644;&#36793;&#32536;&#25298;&#32477;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#27979;&#37327;&#20004;&#32773;&#24179;&#34913;&#30340;&#32452;&#21512;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21407;&#22987;&#25968;&#25454;&#19981;&#21487;&#29992;&#26102;&#65292;&#35780;&#20272;&#19968;&#32452;p&#20540;&#30340;&#26174;&#33879;&#24615;&#30340;&#24120;&#35265;&#26041;&#27861;&#26159;&#23558;&#23427;&#20204;&#19982;&#27719;&#38598;&#20989;&#25968;&#36827;&#34892;&#32452;&#21512;&#12290;&#36825;&#20123;&#27719;&#38598;&#30340;p&#20540;&#23558;p&#20540;&#26679;&#26412;&#36716;&#21270;&#20026;&#19968;&#20010;&#34920;&#29616;&#31867;&#20284;&#20110;&#21333;&#21464;&#37327;p&#20540;&#30340;&#21333;&#19968;&#25968;&#20540;&#12290;&#20026;&#20102;&#26126;&#30830;&#35752;&#35770;&#36825;&#20123;&#20989;&#25968;&#65292;&#24341;&#20837;&#20102;&#19968;&#31995;&#21015;&#20132;&#21449;&#20551;&#35774;&#65292;&#20197;&#20256;&#36798;p&#20540;&#20013;&#38750;&#38646;&#35777;&#25454;&#30340;&#24378;&#24230;&#21644;&#26222;&#36941;&#24615;&#65292;&#28982;&#21518;&#35752;&#35770;&#20102;&#24120;&#35268;&#27719;&#38598;&#20844;&#24335;&#12290;&#22312;&#29305;&#23450;&#20132;&#21449;&#20551;&#35774;&#30340;UMP&#27719;&#38598;p&#20540;&#20013;&#35266;&#23519;&#21040;&#30340;&#27169;&#24335;&#25512;&#21160;&#20102;&#23545;&#20110;&#20013;&#24515;&#21644;&#36793;&#32536;&#25298;&#32477;&#27700;&#24179;&#22312;&#945;&#22788;&#30340;&#23450;&#20041;&#21644;&#35752;&#35770;&#12290;&#35777;&#26126;&#20102;&#20013;&#24515;&#25298;&#32477;&#24635;&#26159;&#22823;&#20110;&#31561;&#20110;&#36793;&#32536;&#25298;&#32477;&#65292;&#20174;&#32780;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#27979;&#37327;&#20004;&#32773;&#22312;&#27719;&#38598;&#30340;p&#20540;&#20013;&#24179;&#34913;&#30340;&#21830;&#12290;&#22522;&#20110;&#967;&#178;_&#954;&#20998;&#20301;&#25968;&#21464;&#25442;&#30340;&#32452;&#21512;&#20989;&#25968;&#34987;&#25552;&#20986;&#20197;&#25511;&#21046;&#36825;&#20010;&#21830;&#65292;&#24182;&#19988;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
A common approach to evaluating the significance of a collection of $p$-values combines them with a pooling function, in particular when the original data are not available. These pooled $p$-values convert a sample of $p$-values into a single number which behaves like a univariate $p$-value. To clarify discussion of these functions, a telescoping series of alternative hypotheses are introduced that communicate the strength and prevalence of non-null evidence in the $p$-values before general pooling formulae are discussed. A pattern noticed in the UMP pooled $p$-value for a particular alternative motivates the definition and discussion of central and marginal rejection levels at $\alpha$. It is proven that central rejection is always greater than or equal to marginal rejection, motivating a quotient to measure the balance between the two for pooled $p$-values. A combining function based on the $\chi^2_{\kappa}$ quantile transformation is proposed to control this quotient and shown to be
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#29305;&#24449;&#34920;&#31034;&#30340;&#32479;&#35745;&#29305;&#24615;&#65292;&#22312;&#28508;&#22312;&#34920;&#31034;&#20013;&#20351;&#29992;&#25968;&#25454;&#33258;&#36866;&#24212;&#30340;&#39640;&#32500;&#20551;&#35774;&#26816;&#39564;&#26469;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#36825;&#31181;&#26041;&#27861;&#20811;&#26381;&#20102;&#29616;&#26377;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#23545;&#20302;&#32500;&#20998;&#24067;&#20551;&#35774;&#30340;&#20381;&#36182;&#21644;&#25968;&#25454;&#22495;&#38480;&#21046;&#30340;&#38382;&#39064;&#65292;&#20026;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.16587</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#39640;&#32500;&#26816;&#39564;&#22312;&#28508;&#22312;&#34920;&#31034;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Adaptive Uncertainty Estimation via High-Dimensional Testing on Latent Representations. (arXiv:2310.16587v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16587
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#29305;&#24449;&#34920;&#31034;&#30340;&#32479;&#35745;&#29305;&#24615;&#65292;&#22312;&#28508;&#22312;&#34920;&#31034;&#20013;&#20351;&#29992;&#25968;&#25454;&#33258;&#36866;&#24212;&#30340;&#39640;&#32500;&#20551;&#35774;&#26816;&#39564;&#26469;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#36825;&#31181;&#26041;&#27861;&#20811;&#26381;&#20102;&#29616;&#26377;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#23545;&#20302;&#32500;&#20998;&#24067;&#20551;&#35774;&#30340;&#20381;&#36182;&#21644;&#25968;&#25454;&#22495;&#38480;&#21046;&#30340;&#38382;&#39064;&#65292;&#20026;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26088;&#22312;&#35780;&#20272;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32622;&#20449;&#24230;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#20381;&#36182;&#20110;&#20302;&#32500;&#20998;&#24067;&#20551;&#35774;&#65292;&#22240;&#27492;&#21463;&#21040;&#28508;&#22312;&#29305;&#24449;&#30340;&#39640;&#32500;&#38382;&#39064;&#30340;&#38480;&#21046;&#12290;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#20851;&#27880;&#20110;&#31163;&#25955;&#20998;&#31867;&#27010;&#29575;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#36825;&#23548;&#33268;&#23545;&#20110;&#20854;&#20182;&#20219;&#21153;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#27867;&#21270;&#33021;&#21147;&#36739;&#24046;&#12290;&#27492;&#22806;&#65292;&#22823;&#37096;&#20998;&#25991;&#29486;&#35201;&#27714;&#22312;&#35757;&#32451;&#26102;&#35201;&#30475;&#21040;&#22806;&#22495;&#65288;OOD&#65289;&#25968;&#25454;&#20197;&#26356;&#22909;&#22320;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#65292;&#36825;&#38480;&#21046;&#20102;&#23454;&#38469;&#20013;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#24615;&#33021;&#65292;&#22240;&#20026;&#22806;&#22495;&#25968;&#25454;&#36890;&#24120;&#26159;&#26410;&#35265;&#36807;&#30340;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#25968;&#25454;&#33258;&#36866;&#24212;&#30340;&#39640;&#32500;&#20551;&#35774;&#27979;&#35797;&#26469;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#36825;&#21033;&#29992;&#20102;&#29305;&#24449;&#34920;&#31034;&#30340;&#32479;&#35745;&#29305;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30452;&#25509;&#22312;&#28508;&#22312;&#34920;&#31034;&#19978;&#25805;&#20316;&#65292;&#22240;&#27492;&#19981;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#29305;&#24449;&#32534;&#30721;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Uncertainty estimation aims to evaluate the confidence of a trained deep neural network. However, existing uncertainty estimation approaches rely on low-dimensional distributional assumptions and thus suffer from the high dimensionality of latent features. Existing approaches tend to focus on uncertainty on discrete classification probabilities, which leads to poor generalizability to uncertainty estimation for other tasks. Moreover, most of the literature requires seeing the out-of-distribution (OOD) data in the training for better estimation of uncertainty, which limits the uncertainty estimation performance in practice because the OOD data are typically unseen. To overcome these limitations, we propose a new framework using data-adaptive high-dimensional hypothesis testing for uncertainty estimation, which leverages the statistical properties of the feature representations. Our method directly operates on latent representations and thus does not require retraining the feature encode
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26368;&#23567;&#26368;&#22823;-&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#31639;&#27861;&#21644;&#38590;&#24230;&#35843;&#25972;&#30340;&#36890;&#29992;&#28216;&#25103;&#23545;&#25112;&#31574;&#30053;&#65292;&#21487;&#20197;&#22312;&#38646;&#21644;&#21338;&#24328;&#20013;&#23454;&#29616;&#20855;&#26377;&#19981;&#21516;&#38590;&#24230;&#32423;&#21035;&#30340;&#20154;&#24037;&#26234;&#33021;&#23545;&#25163;&#12290;</title><link>http://arxiv.org/abs/2310.16581</link><description>&lt;p&gt;
&#28151;&#21512;&#26368;&#23567;&#26368;&#22823;-&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#21644;&#38590;&#24230;&#35843;&#25972;&#30340;&#36890;&#29992;&#28216;&#25103;&#23545;&#25112;
&lt;/p&gt;
&lt;p&gt;
Hybrid Minimax-MCTS and Difficulty Adjustment for General Game Playing. (arXiv:2310.16581v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16581
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26368;&#23567;&#26368;&#22823;-&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#31639;&#27861;&#21644;&#38590;&#24230;&#35843;&#25972;&#30340;&#36890;&#29992;&#28216;&#25103;&#23545;&#25112;&#31574;&#30053;&#65292;&#21487;&#20197;&#22312;&#38646;&#21644;&#21338;&#24328;&#20013;&#23454;&#29616;&#20855;&#26377;&#19981;&#21516;&#38590;&#24230;&#32423;&#21035;&#30340;&#20154;&#24037;&#26234;&#33021;&#23545;&#25163;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26827;&#30424;&#28216;&#25103;&#26159;&#19968;&#31181;&#36866;&#21512;&#21508;&#20010;&#24180;&#40836;&#27573;&#30340;&#23089;&#20048;&#26041;&#24335;&#65292;&#23427;&#21019;&#36896;&#20102;&#31454;&#20105;&#21644;&#21560;&#24341;&#20154;&#30340;&#29615;&#22659;&#65292;&#21516;&#26102;&#20063;&#20419;&#36827;&#20102;&#23398;&#20064;&#21644;&#25112;&#30053;&#24605;&#32771;&#12290;&#23545;&#20110;&#25968;&#23383;&#29256;&#26412;&#30340;&#26827;&#30424;&#28216;&#25103;&#65288;&#20197;&#21450;&#20854;&#20182;&#31867;&#22411;&#30340;&#25968;&#23383;&#28216;&#25103;&#65289;&#65292;&#36890;&#24120;&#21487;&#20197;&#36873;&#25321;&#28216;&#25103;&#30340;&#38590;&#24230;&#12290;&#36890;&#24120;&#36890;&#36807;&#33258;&#23450;&#20041;AI&#31639;&#27861;&#30340;&#25628;&#32034;&#21442;&#25968;&#26469;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#26080;&#27861;&#25193;&#23637;&#21040;&#36890;&#29992;&#28216;&#25103;&#23545;&#25112;&#20195;&#29702;&#31243;&#24207;&#65292;&#22240;&#20026;&#19981;&#21516;&#30340;&#28216;&#25103;&#21487;&#33021;&#38656;&#35201;&#19981;&#21516;&#30340;&#21442;&#25968;&#35774;&#32622;&#26469;&#36866;&#24212;&#21508;&#20010;&#38590;&#24230;&#32423;&#21035;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#26469;&#23454;&#29616;&#20855;&#26377;&#38590;&#24230;&#32423;&#21035;&#30340;&#38646;&#21644;&#21338;&#24328;&#30340;&#20154;&#24037;&#26234;&#33021;&#23545;&#25163;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26368;&#23567;&#26368;&#22823;-&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#28151;&#21512;&#31639;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#26368;&#23567;&#26368;&#22823;&#25628;&#32034;&#36807;&#31243;&#21644;GGP&#65288;&#36890;&#29992;&#28216;&#25103;&#23545;&#25112;&#65289;&#20013;&#30340;MCTS&#65288;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65289;&#30340;&#29305;&#28857;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#25105;&#20204;&#30340;&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;LoBoGames&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#35813;&#24212;&#29992;&#31243;&#24207;&#26159;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#26827;&#30424;&#28216;&#25103;&#24179;&#21488;&#65292;&#26088;&#22312;&#25317;&#26377;&#24191;&#27867;&#30340;&#28216;&#25103;&#30446;&#24405;&#65292;&#24182;&#27880;&#37325;&#21487;&#35775;&#38382;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Board games are a great source of entertainment for all ages, as they create a competitive and engaging environment, as well as stimulating learning and strategic thinking. It is common for digital versions of board games, as any other type of digital games, to offer the option to select the difficulty of the game. This is usually done by customizing the search parameters of the AI algorithm. However, this approach cannot be extended to General Game Playing agents, as different games might require different parametrization for each difficulty level. In this paper, we present a general approach to implement an artificial intelligence opponent with difficulty levels for zero-sum games, together with a propose of a Minimax-MCTS hybrid algorithm, which combines the minimax search process with GGP aspects of MCTS. This approach was tested in our mobile application LoBoGames, an extensible board games platform, that is intended to have an broad catalog of games, with an emphasis on accessibi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#25991;&#26412;&#29983;&#25104;&#22270;&#29255;&#25193;&#25955;&#27169;&#22411;&#36866;&#24212;&#24615;&#22320;&#35843;&#25972;&#20219;&#20309;&#36328;&#22495;&#21644;&#36328;&#31867;&#21035;&#30340;&#22270;&#20687;&#20998;&#31867;&#22120;&#12290;&#20316;&#32773;&#36890;&#36807;&#20351;&#29992;&#21512;&#25104;&#30340;&#39640;&#20445;&#30495;&#24230;&#22270;&#29255;&#20316;&#20026;&#28304;&#22495;&#25968;&#25454;&#30340;&#20195;&#29702;&#65292;&#24182;&#23558;&#23884;&#20837;&#22312;&#25991;&#26412;&#29983;&#25104;&#22270;&#29255;&#27169;&#22411;&#20013;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#30446;&#26631;&#22495;&#65292;&#20174;&#32780;&#30465;&#21435;&#20102;&#25163;&#21160;&#25910;&#38598;&#21644;&#27880;&#37322;&#28304;&#22495;&#25968;&#25454;&#30340;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2310.16573</link><description>&lt;p&gt;
&#36890;&#36807;&#25991;&#26412;&#29983;&#25104;&#22270;&#29255;&#25193;&#25955;&#27169;&#22411;&#65292;&#36866;&#24212;&#24615;&#22320;&#35843;&#25972;&#20219;&#20309;&#22270;&#20687;&#20998;&#31867;&#22120;&#36328;&#22495;&#21644;&#31867;&#21035;
&lt;/p&gt;
&lt;p&gt;
Adapt Anything: Tailor Any Image Classifiers across Domains And Categories Using Text-to-Image Diffusion Models. (arXiv:2310.16573v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16573
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#25991;&#26412;&#29983;&#25104;&#22270;&#29255;&#25193;&#25955;&#27169;&#22411;&#36866;&#24212;&#24615;&#22320;&#35843;&#25972;&#20219;&#20309;&#36328;&#22495;&#21644;&#36328;&#31867;&#21035;&#30340;&#22270;&#20687;&#20998;&#31867;&#22120;&#12290;&#20316;&#32773;&#36890;&#36807;&#20351;&#29992;&#21512;&#25104;&#30340;&#39640;&#20445;&#30495;&#24230;&#22270;&#29255;&#20316;&#20026;&#28304;&#22495;&#25968;&#25454;&#30340;&#20195;&#29702;&#65292;&#24182;&#23558;&#23884;&#20837;&#22312;&#25991;&#26412;&#29983;&#25104;&#22270;&#29255;&#27169;&#22411;&#20013;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#30446;&#26631;&#22495;&#65292;&#20174;&#32780;&#30465;&#21435;&#20102;&#25163;&#21160;&#25910;&#38598;&#21644;&#27880;&#37322;&#28304;&#22495;&#25968;&#25454;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#30340;&#19981;&#26159;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#32780;&#26159;&#30740;&#31350;&#20102;&#29616;&#20195;&#25991;&#26412;&#29983;&#25104;&#22270;&#29255;&#25193;&#25955;&#27169;&#22411;&#33021;&#21542;&#36866;&#24212;&#24615;&#22320;&#35843;&#25972;&#20219;&#20309;&#36328;&#22495;&#21644;&#36328;&#31867;&#21035;&#30340;&#22270;&#20687;&#20998;&#31867;&#22120;&#12290;&#29616;&#26377;&#30340;&#39046;&#22495;&#36866;&#24212;&#24615;&#22270;&#20687;&#20998;&#31867;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#25968;&#25454;&#36827;&#34892;&#22495;&#23545;&#40784;&#65292;&#20174;&#32780;&#23558;&#20174;&#26377;&#26631;&#31614;&#28304;&#22495;&#25968;&#25454;&#20013;&#23398;&#21040;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#26080;&#26631;&#31614;&#30446;&#26631;&#22495;&#25968;&#25454;&#20013;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#25991;&#26412;&#29983;&#25104;&#22270;&#29255;&#25193;&#25955;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#25105;&#20204;&#24819;&#30693;&#36947;&#20174;&#25991;&#26412;&#29983;&#25104;&#30340;&#39640;&#20445;&#30495;&#24230;&#21512;&#25104;&#25968;&#25454;&#26159;&#21542;&#21487;&#20197;&#20316;&#20026;&#23454;&#38469;&#22330;&#26223;&#20013;&#28304;&#22495;&#25968;&#25454;&#30340;&#20195;&#29702;&#12290;&#36825;&#26679;&#65292;&#25105;&#20204;&#23601;&#19981;&#38656;&#35201;&#20197;&#19968;&#23545;&#19968;&#30340;&#26041;&#24335;&#25910;&#38598;&#21644;&#27880;&#37322;&#27599;&#20010;&#39046;&#22495;&#36866;&#24212;&#30340;&#20219;&#21153;&#20013;&#30340;&#28304;&#22495;&#25968;&#25454;&#12290;&#32780;&#26159;&#21482;&#20351;&#29992;&#19968;&#20010;&#29616;&#25104;&#30340;&#25991;&#26412;&#29983;&#25104;&#22270;&#29255;&#27169;&#22411;&#65292;&#29983;&#25104;&#24102;&#26377;&#30456;&#24212;&#25991;&#26412;&#25552;&#31034;&#27966;&#29983;&#30340;&#31867;&#21035;&#26631;&#31614;&#30340;&#22270;&#29255;&#65292;&#28982;&#21518;&#21033;&#29992;&#36825;&#20123;&#21512;&#25104;&#25968;&#25454;&#20316;&#20026;&#26725;&#26753;&#65292;&#23558;&#23884;&#20837;&#22312;&#20219;&#21153;&#26080;&#20851;&#30340;&#25991;&#26412;&#29983;&#25104;&#22270;&#29255;&#27169;&#22411;&#20013;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#30446;&#26631;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
We do not pursue a novel method in this paper, but aim to study if a modern text-to-image diffusion model can tailor any task-adaptive image classifier across domains and categories. Existing domain adaptive image classification works exploit both source and target data for domain alignment so as to transfer the knowledge learned from the labeled source data to the unlabeled target data. However, as the development of the text-to-image diffusion model, we wonder if the high-fidelity synthetic data from the text-to-image generator can serve as a surrogate of the source data in real world. In this way, we do not need to collect and annotate the source data for each domain adaptation task in a one-for-one manner. Instead, we utilize only one off-the-shelf text-to-image model to synthesize images with category labels derived from the corresponding text prompts, and then leverage the surrogate data as a bridge to transfer the knowledge embedded in the task-agnostic text-to-image generator t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22270;&#20013;&#30340;&#26631;&#31614;&#22122;&#22768;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26631;&#31614;&#20256;&#25773;&#30340;&#31639;&#27861;&#26469;&#22788;&#29702;&#20219;&#24847;&#24322;&#36136;&#24615;&#30340;&#22270;&#26631;&#31614;&#22122;&#22768;&#65292;&#20197;&#32416;&#27491;&#22122;&#22768;&#26631;&#31614;&#24182;&#20026;&#26410;&#26631;&#35760;&#30340;&#33410;&#28857;&#20998;&#37197;&#26631;&#31614;&#12290;</title><link>http://arxiv.org/abs/2310.16560</link><description>&lt;p&gt;
&#22270;&#26631;&#31614;&#20256;&#25773;&#31639;&#27861;&#24212;&#23545;&#22270;&#26631;&#31614;&#22122;&#22768;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Label Propagation for Graph Label Noise. (arXiv:2310.16560v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16560
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22270;&#20013;&#30340;&#26631;&#31614;&#22122;&#22768;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26631;&#31614;&#20256;&#25773;&#30340;&#31639;&#27861;&#26469;&#22788;&#29702;&#20219;&#24847;&#24322;&#36136;&#24615;&#30340;&#22270;&#26631;&#31614;&#22122;&#22768;&#65292;&#20197;&#32416;&#27491;&#22122;&#22768;&#26631;&#31614;&#24182;&#20026;&#26410;&#26631;&#35760;&#30340;&#33410;&#28857;&#20998;&#37197;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#31614;&#22122;&#22768;&#26159;&#22823;&#22411;&#25968;&#25454;&#38598;&#20013;&#24120;&#35265;&#30340;&#25361;&#25112;&#65292;&#23427;&#20250;&#26174;&#33879;&#38477;&#20302;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22823;&#37096;&#20998;&#29616;&#26377;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#22122;&#22768;&#26631;&#31614;&#65292;&#28982;&#32780;&#65292;&#22270;&#27169;&#22411;&#23558;&#33410;&#28857;&#29305;&#24449;&#21644;&#22270;&#25299;&#25169;&#32467;&#26500;&#20316;&#20026;&#36755;&#20837;&#65292;&#36890;&#36807;&#28040;&#24687;&#20256;&#36882;&#26426;&#21046;&#26356;&#23481;&#26131;&#21463;&#21040;&#26631;&#31614;&#22122;&#22768;&#30340;&#24433;&#21709;&#12290;&#36817;&#26399;&#65292;&#21482;&#26377;&#23569;&#25968;&#20960;&#31687;&#25991;&#31456;&#25552;&#20986;&#20102;&#35299;&#20915;&#22270;&#20013;&#26631;&#31614;&#22122;&#22768;&#30340;&#26041;&#27861;&#12290;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#38480;&#21046;&#26159;&#23427;&#20204;&#20551;&#35774;&#22270;&#26159;&#21516;&#26500;&#30340;&#65292;&#24182;&#19988;&#26631;&#31614;&#26159;&#24179;&#28369;&#20998;&#24067;&#30340;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#22270;&#21487;&#33021;&#21253;&#21547;&#19981;&#21516;&#31243;&#24230;&#30340;&#24322;&#36136;&#24615;&#29978;&#33267;&#26159;&#24322;&#36136;&#24615;&#30340;&#20027;&#23548;&#65292;&#23548;&#33268;&#24403;&#21069;&#26041;&#27861;&#30340;&#19981;&#36275;&#12290;&#26412;&#25991;&#30740;&#31350;&#20219;&#24847;&#24322;&#36136;&#24615;&#26465;&#20214;&#19979;&#30340;&#22270;&#26631;&#31614;&#22122;&#22768;&#38382;&#39064;&#65292;&#26088;&#22312;&#32416;&#27491;&#22122;&#22768;&#26631;&#31614;&#24182;&#20026;&#20043;&#21069;&#26410;&#26631;&#35760;&#30340;&#33410;&#28857;&#20998;&#37197;&#26631;&#31614;&#12290;&#25105;&#20204;&#39318;&#20808;&#36827;&#34892;&#20102;&#20004;&#20010;&#23454;&#35777;&#20998;&#26512;&#65292;&#25506;&#35752;&#22270;&#21516;&#36136;&#24615;&#23545;&#22270;&#26631;&#31614;&#22122;&#22768;&#30340;&#24433;&#21709;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26631;&#31614;&#20256;&#25773;&#30340;&#31639;&#27861;&#26469;&#22788;&#29702;&#20219;&#24847;&#24322;&#36136;&#24615;&#30340;&#22270;&#26631;&#31614;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;
Label noise is a common challenge in large datasets, as it can significantly degrade the generalization ability of deep neural networks. Most existing studies focus on noisy labels in computer vision; however, graph models encompass both node features and graph topology as input, and become more susceptible to label noise through message-passing mechanisms. Recently, only a few works have been proposed to tackle the label noise on graphs. One major limitation is that they assume the graph is homophilous and the labels are smoothly distributed. Nevertheless, real-world graphs may contain varying degrees of heterophily or even be heterophily-dominated, leading to the inadequacy of current methods. In this paper, we study graph label noise in the context of arbitrary heterophily, with the aim of rectifying noisy labels and assigning labels to previously unlabeled nodes. We begin by conducting two empirical analyses to explore the impact of graph homophily on graph label noise. Following o
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#38543;&#26426;&#21270;&#39118;&#38505;&#26631;&#20934;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#36991;&#20813;&#22312;&#39118;&#38505;&#19978;&#30340;&#20559;&#21521;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#25910;&#25947;&#24615;&#21644;&#26368;&#20248;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#21253;&#25324;Atari 55&#28216;&#25103;&#22312;&#20869;&#30340;&#21508;&#31181;&#29615;&#22659;&#20013;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#20998;&#24067;&#24335;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.16546</link><description>&lt;p&gt;
&#20048;&#35266;&#20027;&#20041;&#30340;&#38519;&#38449;&#65306;&#36890;&#36807;&#38543;&#26426;&#21270;&#39118;&#38505;&#26631;&#20934;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Pitfall of Optimism: Distributional Reinforcement Learning by Randomizing Risk Criterion. (arXiv:2310.16546v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16546
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#38543;&#26426;&#21270;&#39118;&#38505;&#26631;&#20934;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#36991;&#20813;&#22312;&#39118;&#38505;&#19978;&#30340;&#20559;&#21521;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#25910;&#25947;&#24615;&#21644;&#26368;&#20248;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#21253;&#25324;Atari 55&#28216;&#25103;&#22312;&#20869;&#30340;&#21508;&#31181;&#29615;&#22659;&#20013;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#20998;&#24067;&#24335;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#35797;&#22270;&#21033;&#29992;&#20272;&#35745;&#30340;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#25506;&#32034;&#65292;&#22914;&#22312;&#38754;&#23545;&#19981;&#30830;&#23450;&#24615;&#26102;&#30340;&#20048;&#35266;&#20027;&#20041;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#20272;&#35745;&#30340;&#26041;&#24046;&#36827;&#34892;&#20048;&#35266;&#25506;&#32034;&#21487;&#33021;&#23548;&#33268;&#25968;&#25454;&#25910;&#38598;&#30340;&#20559;&#24046;&#65292;&#38459;&#30861;&#25910;&#25947;&#25110;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#38543;&#26426;&#21270;&#39118;&#38505;&#26631;&#20934;&#26469;&#36873;&#25321;&#21160;&#20316;&#65292;&#36991;&#20813;&#22312;&#39118;&#38505;&#19978;&#30340;&#21333;&#21521;&#20542;&#21521;&#12290;&#25105;&#20204;&#36890;&#36807;&#25197;&#26354;&#39118;&#38505;&#24230;&#37327;&#25552;&#20379;&#20102;&#19968;&#20010;&#25200;&#21160;&#30340;&#20998;&#24067;&#36125;&#23572;&#26364;&#26368;&#20248;&#24615;&#31639;&#23376;&#65292;&#24182;&#35777;&#26126;&#20102;&#25152;&#25552;&#26041;&#27861;&#20855;&#26377;&#36739;&#24369;&#30340;&#25910;&#32553;&#24615;&#36136;&#30340;&#25910;&#25947;&#24615;&#21644;&#26368;&#20248;&#24615;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#25903;&#25345;&#65292;&#25152;&#25552;&#26041;&#27861;&#19981;&#20250;&#38519;&#20837;&#20559;&#21521;&#24615;&#30340;&#25506;&#32034;&#65292;&#24182;&#30830;&#20445;&#25910;&#25947;&#21040;&#26368;&#20248;&#22238;&#25253;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#21253;&#25324;Atari 55&#28216;&#25103;&#22312;&#20869;&#30340;&#21508;&#31181;&#29615;&#22659;&#20013;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#29616;&#26377;&#30340;&#22522;&#20110;&#20998;&#24067;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributional reinforcement learning algorithms have attempted to utilize estimated uncertainty for exploration, such as optimism in the face of uncertainty. However, using the estimated variance for optimistic exploration may cause biased data collection and hinder convergence or performance. In this paper, we present a novel distributional reinforcement learning algorithm that selects actions by randomizing risk criterion to avoid one-sided tendency on risk. We provide a perturbed distributional Bellman optimality operator by distorting the risk measure and prove the convergence and optimality of the proposed method with the weaker contraction property. Our theoretical results support that the proposed method does not fall into biased exploration and is guaranteed to converge to an optimal return. Finally, we empirically show that our method outperforms other existing distribution-based algorithms in various environments including Atari 55 games.
&lt;/p&gt;</description></item><item><title>FedTherapist&#26159;&#19968;&#31181;&#20351;&#29992;&#32852;&#37030;&#23398;&#20064;&#22312;&#26234;&#33021;&#25163;&#26426;&#19978;&#36827;&#34892;&#29992;&#25143;&#29983;&#25104;&#30340;&#35821;&#35328;&#34920;&#36798;&#30340;&#31934;&#31070;&#20581;&#24247;&#30417;&#27979;&#30340;&#31995;&#32479;&#12290;&#23427;&#26377;&#25928;&#22320;&#21033;&#29992;&#20102;&#25345;&#32493;&#35821;&#38899;&#21644;&#38190;&#30424;&#36755;&#20837;&#65292;&#24182;&#36890;&#36807;&#19978;&#19979;&#25991;&#24863;&#30693;&#35821;&#35328;&#23398;&#20064;&#26041;&#27861;&#26469;&#25552;&#39640;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#35780;&#20272;&#20013;&#65292;&#27604;&#36739;&#20102;&#38750;&#35821;&#35328;&#29305;&#24449;&#65292;&#32467;&#26524;&#26174;&#31034;FedTherapist&#22312;&#39044;&#27979;&#25233;&#37057;&#12289;&#21387;&#21147;&#12289;&#28966;&#34385;&#21644;&#24515;&#24773;&#26041;&#38754;&#30340;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2310.16538</link><description>&lt;p&gt;
FedTherapist&#65306;&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#22312;&#26234;&#33021;&#25163;&#26426;&#19978;&#20351;&#29992;&#29992;&#25143;&#29983;&#25104;&#30340;&#35821;&#35328;&#34920;&#36798;&#36827;&#34892;&#31934;&#31070;&#20581;&#24247;&#30417;&#27979;
&lt;/p&gt;
&lt;p&gt;
FedTherapist: Mental Health Monitoring with User-Generated Linguistic Expressions on Smartphones via Federated Learning. (arXiv:2310.16538v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16538
&lt;/p&gt;
&lt;p&gt;
FedTherapist&#26159;&#19968;&#31181;&#20351;&#29992;&#32852;&#37030;&#23398;&#20064;&#22312;&#26234;&#33021;&#25163;&#26426;&#19978;&#36827;&#34892;&#29992;&#25143;&#29983;&#25104;&#30340;&#35821;&#35328;&#34920;&#36798;&#30340;&#31934;&#31070;&#20581;&#24247;&#30417;&#27979;&#30340;&#31995;&#32479;&#12290;&#23427;&#26377;&#25928;&#22320;&#21033;&#29992;&#20102;&#25345;&#32493;&#35821;&#38899;&#21644;&#38190;&#30424;&#36755;&#20837;&#65292;&#24182;&#36890;&#36807;&#19978;&#19979;&#25991;&#24863;&#30693;&#35821;&#35328;&#23398;&#20064;&#26041;&#27861;&#26469;&#25552;&#39640;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#35780;&#20272;&#20013;&#65292;&#27604;&#36739;&#20102;&#38750;&#35821;&#35328;&#29305;&#24449;&#65292;&#32467;&#26524;&#26174;&#31034;FedTherapist&#22312;&#39044;&#27979;&#25233;&#37057;&#12289;&#21387;&#21147;&#12289;&#28966;&#34385;&#21644;&#24515;&#24773;&#26041;&#38754;&#30340;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#31070;&#31185;&#21307;&#29983;&#36890;&#36807;&#24739;&#32773;&#30340;&#35821;&#35328;&#20351;&#29992;&#26469;&#35786;&#26029;&#31934;&#31070;&#30142;&#30149;&#12290;&#20294;&#30001;&#20110;&#25968;&#25454;&#38544;&#31169;&#38382;&#39064;&#65292;&#29616;&#26377;&#30340;&#34987;&#21160;&#31934;&#31070;&#20581;&#24247;&#30417;&#27979;&#31995;&#32479;&#20351;&#29992;&#25163;&#26426;&#35774;&#22791;&#30340;&#27963;&#21160;&#12289;&#24212;&#29992;&#20351;&#29992;&#21644;&#20301;&#32622;&#31561;&#26367;&#20195;&#29305;&#24449;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;FedTherapist&#65292;&#19968;&#31181;&#21033;&#29992;&#32852;&#37030;&#23398;&#20064;&#20197;&#38544;&#31169;&#20445;&#25252;&#26041;&#24335;&#20351;&#29992;&#25345;&#32493;&#35821;&#38899;&#21644;&#38190;&#30424;&#36755;&#20837;&#30340;&#31227;&#21160;&#31934;&#31070;&#20581;&#24247;&#30417;&#27979;&#31995;&#32479;&#12290;&#25105;&#20204;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#27169;&#22411;&#35774;&#35745;&#30340;&#24615;&#33021;&#21644;&#24320;&#38144;&#26469;&#20811;&#26381;&#22312;&#26234;&#33021;&#25163;&#26426;&#19978;&#36827;&#34892;&#35774;&#22791;&#20869;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#30340;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#24863;&#30693;&#35821;&#35328;&#23398;&#20064;&#65288;CALL&#65289;&#26041;&#27861;&#65292;&#20197;&#26377;&#25928;&#21033;&#29992;&#26234;&#33021;&#25163;&#26426;&#30340;&#22823;&#35268;&#27169;&#21644;&#22024;&#26434;&#25991;&#26412;&#36827;&#34892;&#31934;&#31070;&#20581;&#24247;&#20449;&#21495;&#24863;&#30693;&#12290;&#25105;&#20204;&#22312;46&#21517;&#21442;&#19982;&#32773;&#20013;&#36827;&#34892;&#20102;&#32463;IRB&#25209;&#20934;&#30340;&#33258;&#25105;&#25253;&#21578;&#25233;&#37057;&#12289;&#21387;&#21147;&#12289;&#28966;&#34385;&#21644;&#24515;&#24773;&#30340;&#39044;&#27979;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;FedTherapist&#30456;&#27604;&#20110;&#38750;&#35821;&#35328;&#29305;&#24449;&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;0.15 AUROC&#12290;
&lt;/p&gt;
&lt;p&gt;
Psychiatrists diagnose mental disorders via the linguistic use of patients. Still, due to data privacy, existing passive mental health monitoring systems use alternative features such as activity, app usage, and location via mobile devices. We propose FedTherapist, a mobile mental health monitoring system that utilizes continuous speech and keyboard input in a privacy-preserving way via federated learning. We explore multiple model designs by comparing their performance and overhead for FedTherapist to overcome the complex nature of on-device language model training on smartphones. We further propose a Context-Aware Language Learning (CALL) methodology to effectively utilize smartphones' large and noisy text for mental health signal sensing. Our IRB-approved evaluation of the prediction of self-reported depression, stress, anxiety, and mood from 46 participants shows higher accuracy of FedTherapist compared with the performance with non-language features, achieving 0.15 AUROC improveme
&lt;/p&gt;</description></item><item><title>R$^3$&#25552;&#31034;&#26159;&#19968;&#31181;&#29992;&#20110;&#22312;&#22122;&#22768;&#32972;&#26223;&#19979;&#36827;&#34892;CoT&#25512;&#29702;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22797;&#23457;&#12289;&#25913;&#20889;&#21644;&#35299;&#20915;&#30340;&#24605;&#32771;&#36807;&#31243;&#65292;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20132;&#20114;&#20197;&#36827;&#34892;&#20851;&#38190;&#21477;&#25552;&#21462;&#12289;&#21464;&#37327;&#22768;&#26126;&#21644;&#31572;&#26696;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2310.16535</link><description>&lt;p&gt;
R$^3$ Prompting&#65306;&#26080;&#22122;&#22768;&#32972;&#26223;&#19979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38142;&#24335;&#24605;&#32500;&#25512;&#29702;&#30340;&#35780;&#23457;&#12289;&#25913;&#20889;&#21644;&#35299;&#20915;
&lt;/p&gt;
&lt;p&gt;
R$^3$ Prompting: Review, Rephrase and Resolve for Chain-of-Thought Reasoning in Large Language Models under Noisy Context. (arXiv:2310.16535v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16535
&lt;/p&gt;
&lt;p&gt;
R$^3$&#25552;&#31034;&#26159;&#19968;&#31181;&#29992;&#20110;&#22312;&#22122;&#22768;&#32972;&#26223;&#19979;&#36827;&#34892;CoT&#25512;&#29702;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22797;&#23457;&#12289;&#25913;&#20889;&#21644;&#35299;&#20915;&#30340;&#24605;&#32771;&#36807;&#31243;&#65292;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20132;&#20114;&#20197;&#36827;&#34892;&#20851;&#38190;&#21477;&#25552;&#21462;&#12289;&#21464;&#37327;&#22768;&#26126;&#21644;&#31572;&#26696;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38142;&#24335;&#24605;&#32500;&#65288;CoT&#65289;&#25552;&#31034;&#30340;&#24110;&#21161;&#19979;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#25512;&#29702;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#37117;&#22312;&#26080;&#22122;&#22768;&#32972;&#26223;&#19979;&#36827;&#34892;&#35780;&#20272;&#65292;LLMs&#22312;&#22122;&#22768;&#32972;&#26223;&#19979;&#20135;&#29983;&#19981;&#20934;&#30830;&#32467;&#26524;&#30340;&#22256;&#22659;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#35843;&#26597;&#12290;&#29616;&#26377;&#30740;&#31350;&#21033;&#29992;&#35302;&#21457;&#21477;&#23376;&#40723;&#21169;LLMs&#38598;&#20013;&#20110;&#30456;&#20851;&#20449;&#24687;&#65292;&#20294;&#35302;&#21457;&#23545;&#26368;&#32456;&#31572;&#26696;&#39044;&#27979;&#30340;&#24433;&#21709;&#26377;&#38480;&#12290;&#21463;&#20132;&#20114;&#24335;CoT&#26041;&#27861;&#30340;&#21551;&#21457;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#29992;&#25143;&#21644;LLMs&#20043;&#38388;&#22810;&#36718;&#20114;&#21160;&#20419;&#36827;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#21363;R$^3$&#25552;&#31034;&#65292;&#29992;&#20110;&#22312;&#22122;&#22768;&#32972;&#26223;&#19979;&#36827;&#34892;CoT&#24605;&#32500;&#25512;&#29702;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;R$^3$&#25552;&#31034;&#19982;LLMs&#36827;&#34892;&#20851;&#38190;&#21477;&#25552;&#21462;&#12289;&#21464;&#37327;&#22768;&#26126;&#21644;&#31572;&#26696;&#39044;&#27979;&#30340;&#20132;&#20114;&#65292;&#23545;&#24212;&#20110;&#22797;&#23457;&#12289;&#25913;&#20889;&#21644;&#35299;&#20915;&#30340;&#24605;&#32771;&#36807;&#31243;&#12290;&#26368;&#21518;&#19968;&#27425;&#20114;&#21160;&#20013;&#29983;&#25104;&#30340;&#21709;&#24212;&#23558;&#25191;&#34892;
&lt;/p&gt;
&lt;p&gt;
With the help of Chain-of-Thought (CoT) prompting, Large Language Models (LLMs) have achieved remarkable performance on various reasoning tasks. However, most of them have been evaluated under noise-free context and the dilemma for LLMs to produce inaccurate results under the noisy context has not been fully investigated. Existing studies utilize trigger sentences to encourage LLMs to concentrate on the relevant information but the trigger has limited effect on final answer prediction. Inspired by interactive CoT method, where intermediate reasoning steps are promoted by multiple rounds of interaction between users and LLMs, we propose a novel prompting method, namely R$^3$ prompting, for CoT reasoning under noisy context. Specifically, R$^3$ prompting interacts with LLMs to perform key sentence extraction, variable declaration and answer prediction, which corresponds to a thought process of reviewing, rephrasing and resolving. The responses generated at the last interaction will perfo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20154;&#21475;&#22810;&#26679;&#24615;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#25216;&#26415;CCSV&#65292;&#36890;&#36807;&#21033;&#29992;&#27169;&#22411;&#30340;&#22810;&#26679;&#24615;&#25512;&#29702;&#33021;&#21147;&#26469;&#25913;&#21892;&#20154;&#21475;&#22810;&#26679;&#24615;&#65292;&#32780;&#26080;&#38656;&#20381;&#36182;&#25163;&#24037;&#21046;&#20316;&#30340;&#31034;&#20363;&#25110;&#25552;&#31034;&#35843;&#25972;&#12290;</title><link>http://arxiv.org/abs/2310.16523</link><description>&lt;p&gt;
&#36890;&#36807;&#38598;&#20307;&#25209;&#35780;&#21644;&#33258;&#25105;&#25237;&#31080;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20154;&#21475;&#22810;&#26679;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Diversity of Demographic Representation in Large Language Models via Collective-Critiques and Self-Voting. (arXiv:2310.16523v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16523
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20154;&#21475;&#22810;&#26679;&#24615;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#25216;&#26415;CCSV&#65292;&#36890;&#36807;&#21033;&#29992;&#27169;&#22411;&#30340;&#22810;&#26679;&#24615;&#25512;&#29702;&#33021;&#21147;&#26469;&#25913;&#21892;&#20154;&#21475;&#22810;&#26679;&#24615;&#65292;&#32780;&#26080;&#38656;&#20381;&#36182;&#25163;&#24037;&#21046;&#20316;&#30340;&#31034;&#20363;&#25110;&#25552;&#31034;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#35828;&#65292;&#22810;&#26679;&#24615;&#26159;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#65306;&#24403;&#29992;&#25143;&#30340;&#25552;&#31034;&#19981;&#26126;&#30830;&#26102;&#65292;&#27169;&#22411;&#21487;&#33021;&#20250;&#22312;&#29983;&#25104;&#21709;&#24212;&#26102;&#36981;&#24490;&#38544;&#21547;&#20551;&#35774;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#21709;&#24212;&#30340;&#21516;&#36136;&#21270;&#65292;&#20197;&#21450;&#26576;&#20123;&#20154;&#21475;&#32676;&#20307;&#30340;&#20195;&#34920;&#24615;&#19981;&#36275;&#29978;&#33267;&#28040;&#22833;&#22312;&#29983;&#25104;&#30340;&#21709;&#24212;&#20013;&#12290;&#26412;&#25991;&#35268;&#33539;&#20102;&#29983;&#25104;&#24335;LLMs&#20013;&#30340;&#22810;&#26679;&#24615;&#34920;&#31034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#34913;&#37327;&#22312;&#20154;&#21644;&#25991;&#21270;&#26041;&#21521;&#19978;&#29983;&#25104;&#21709;&#24212;&#22810;&#26679;&#24615;&#30340;&#24230;&#37327;&#25351;&#26631;&#12290;&#25105;&#20204;&#21457;&#29616;LLMs&#29702;&#35299;&#22810;&#26679;&#24615;&#30340;&#27010;&#24565;&#65292;&#24182;&#19988;&#23427;&#20204;&#21487;&#20197;&#23545;&#33258;&#24049;&#30340;&#21709;&#24212;&#36827;&#34892;&#25512;&#29702;&#21644;&#25209;&#35780;&#20197;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#12290;&#36825;&#19968;&#21457;&#29616;&#28608;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;&#38598;&#20307;&#25209;&#35780;&#21644;&#33258;&#25105;&#25237;&#31080;(CCSC)&#30340;&#26032;&#25552;&#31034;&#25216;&#26415;&#65292;&#36890;&#36807;&#21033;&#29992;&#23427;&#30340;&#22810;&#26679;&#24615;&#25512;&#29702;&#33021;&#21147;&#26469;&#25552;&#39640;LLMs&#30340;&#20154;&#21475;&#22810;&#26679;&#24615;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#25163;&#24037;&#21046;&#20316;&#30340;&#31034;&#20363;&#25110;&#25552;&#31034;&#35843;&#25972;&#12290;&#36890;&#36807;&#20154;&#31867;&#21644;&#33258;&#21160;&#21270;&#35780;&#20272;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
A crucial challenge for generative large language models (LLMs) is diversity: when a user's prompt is under-specified, models may follow implicit assumptions while generating a response, which may result in homogenization of the responses, as well as certain demographic groups being under-represented or even erased from the generated responses. In this paper, we formalize diversity of representation in generative LLMs. We present evaluation datasets and propose metrics to measure diversity in generated responses along people and culture axes. We find that LLMs understand the notion of diversity, and that they can reason and critique their own responses for that goal. This finding motivated a new prompting technique called collective-critique and self-voting (CCSV) to self-improve people diversity of LLMs by tapping into its diversity reasoning capabilities, without relying on handcrafted examples or prompt tuning. Extensive empirical experiments with both human and automated evaluation
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35770;&#35777;&#30340;&#26041;&#27861;&#26469;&#30830;&#23450;&#20026;&#20160;&#20040;&#19968;&#20010;&#20010;&#20307;&#34987;&#20998;&#31867;&#19982;&#30456;&#20284;&#20010;&#20307;&#19981;&#21516;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#23450;&#37327;&#35770;&#35777;&#26694;&#26550;&#26469;&#34920;&#31034;&#20010;&#20307;&#21644;&#19982;&#20854;&#30456;&#20284;&#20010;&#20307;&#30340;&#23646;&#24615;-&#20540;&#23545;&#65292;&#24182;&#20351;&#29992;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#35821;&#20041;&#26469;&#30830;&#23450;&#23545;&#20010;&#20307;&#20998;&#31867;&#20135;&#29983;&#26368;&#22823;&#36129;&#29486;&#30340;&#23646;&#24615;-&#20540;&#23545;&#12290;</title><link>http://arxiv.org/abs/2310.16506</link><description>&lt;p&gt;
&#35782;&#21035;&#20559;&#35265;&#30340;&#21407;&#22240;&#65306;&#19968;&#31181;&#22522;&#20110;&#35770;&#35777;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Identifying Reasons for Bias: An Argumentation-Based Approach. (arXiv:2310.16506v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16506
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35770;&#35777;&#30340;&#26041;&#27861;&#26469;&#30830;&#23450;&#20026;&#20160;&#20040;&#19968;&#20010;&#20010;&#20307;&#34987;&#20998;&#31867;&#19982;&#30456;&#20284;&#20010;&#20307;&#19981;&#21516;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#23450;&#37327;&#35770;&#35777;&#26694;&#26550;&#26469;&#34920;&#31034;&#20010;&#20307;&#21644;&#19982;&#20854;&#30456;&#20284;&#20010;&#20307;&#30340;&#23646;&#24615;-&#20540;&#23545;&#65292;&#24182;&#20351;&#29992;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#35821;&#20041;&#26469;&#30830;&#23450;&#23545;&#20010;&#20307;&#20998;&#31867;&#20135;&#29983;&#26368;&#22823;&#36129;&#29486;&#30340;&#23646;&#24615;-&#20540;&#23545;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31639;&#27861;&#20915;&#31574;&#31995;&#32479;&#22312;&#31038;&#20250;&#20013;&#30340;&#26222;&#21450;&#65292;&#30830;&#20445;&#36825;&#20123;&#31995;&#32479;&#30340;&#20844;&#24179;&#24615;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#34429;&#28982;&#22312;&#26500;&#24314;&#20844;&#24179;&#31639;&#27861;&#20915;&#31574;&#31995;&#32479;&#26041;&#38754;&#24050;&#32463;&#36827;&#34892;&#20102;&#22823;&#37327;&#30740;&#31350;&#65292;&#20294;&#20854;&#20013;&#22823;&#37096;&#20998;&#26041;&#27861;&#38656;&#35201;&#35775;&#38382;&#35757;&#32451;&#25968;&#25454;&#65292;&#21253;&#25324;&#20010;&#20154;&#29305;&#24449;&#65292;&#24182;&#19988;&#23545;&#20110;&#21738;&#20123;&#20010;&#20307;&#34987;&#19981;&#20844;&#24179;&#22320;&#20998;&#31867;&#27809;&#26377;&#36879;&#26126;&#24230;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#22522;&#20110;&#35770;&#35777;&#30340;&#26041;&#27861;&#65292;&#20197;&#30830;&#23450;&#20026;&#20160;&#20040;&#19968;&#20010;&#20010;&#20307;&#34987;&#20998;&#31867;&#19982;&#30456;&#20284;&#20010;&#20307;&#19981;&#21516;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#23450;&#37327;&#35770;&#35777;&#26694;&#26550;&#26469;&#34920;&#31034;&#20010;&#20307;&#21644;&#19982;&#20854;&#30456;&#20284;&#20010;&#20307;&#30340;&#23646;&#24615;-&#20540;&#23545;&#65292;&#24182;&#20351;&#29992;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#35821;&#20041;&#26469;&#30830;&#23450;&#23545;&#20010;&#20307;&#20998;&#31867;&#20135;&#29983;&#26368;&#22823;&#36129;&#29486;&#30340;&#23646;&#24615;-&#20540;&#23545;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#22312;&#20844;&#24179;&#39046;&#22495;&#24120;&#29992;&#30340;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#35782;&#21035;&#24046;&#24322;&#20998;&#31867;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As algorithmic decision-making systems become more prevalent in society, ensuring the fairness of these systems is becoming increasingly important. Whilst there has been substantial research in building fair algorithmic decision-making systems, the majority of these methods require access to the training data, including personal characteristics, and are not transparent regarding which individuals are classified unfairly. In this paper, we propose a novel model-agnostic argumentation-based method to determine why an individual is classified differently in comparison to similar individuals. Our method uses a quantitative argumentation framework to represent attribute-value pairs of an individual and of those similar to them, and uses a well-known semantics to identify the attribute-value pairs in the individual contributing most to their different classification. We evaluate our method on two datasets commonly used in the fairness literature and illustrate its effectiveness in the identi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25991;&#26412;&#24322;&#24120;&#26333;&#20809;&#22312;&#35270;&#35273;&#22806;&#30028;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#20351;&#29992;&#25991;&#26412;&#24322;&#24120;&#20540;&#20195;&#26367;&#22270;&#20687;&#20013;&#30340;&#24322;&#24120;&#20540;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#20351;&#29992;&#25991;&#26412;&#24322;&#24120;&#20540;&#30340;&#22909;&#22788;&#65292;&#24182;&#25552;&#20986;&#20102;&#22810;&#31181;&#29983;&#25104;&#25991;&#26412;&#24322;&#24120;&#20540;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.16492</link><description>&lt;p&gt;
&#20851;&#20110;&#25991;&#26412;&#24322;&#24120;&#26333;&#20809;&#22312;&#35270;&#35273;&#22806;&#30028;&#26816;&#27979;&#20013;&#30340;&#25928;&#21147;
&lt;/p&gt;
&lt;p&gt;
On the Powerfulness of Textual Outlier Exposure for Visual OoD Detection. (arXiv:2310.16492v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16492
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25991;&#26412;&#24322;&#24120;&#26333;&#20809;&#22312;&#35270;&#35273;&#22806;&#30028;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#20351;&#29992;&#25991;&#26412;&#24322;&#24120;&#20540;&#20195;&#26367;&#22270;&#20687;&#20013;&#30340;&#24322;&#24120;&#20540;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#20351;&#29992;&#25991;&#26412;&#24322;&#24120;&#20540;&#30340;&#22909;&#22788;&#65292;&#24182;&#25552;&#20986;&#20102;&#22810;&#31181;&#29983;&#25104;&#25991;&#26412;&#24322;&#24120;&#20540;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25104;&#21151;&#26816;&#27979;&#36234;&#30028;&#25968;&#25454;&#23545;&#20110;&#30830;&#20445;&#31070;&#32463;&#32593;&#32476;&#30340;&#23433;&#20840;&#37096;&#32626;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#36234;&#30028;&#26816;&#27979;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#26159;&#31070;&#32463;&#32593;&#32476;&#22312;&#36234;&#30028;&#25968;&#25454;&#19978;&#36755;&#20986;&#36807;&#24230;&#33258;&#20449;&#30340;&#39044;&#27979;&#65292;&#36825;&#20351;&#24471;&#20165;&#26681;&#25454;&#39044;&#27979;&#26469;&#30830;&#23450;&#25968;&#25454;&#30340;&#36234;&#30028;&#24615;&#21464;&#24471;&#22256;&#38590;&#12290;&#24322;&#24120;&#26333;&#20809;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24341;&#20837;&#39069;&#22806;&#30340;&#25439;&#22833;&#65292;&#40723;&#21169;&#23545;&#36234;&#30028;&#25968;&#25454;&#36827;&#34892;&#20302;&#32622;&#20449;&#24230;&#39044;&#27979;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#23613;&#31649;&#24322;&#24120;&#26333;&#20809;&#22312;&#25552;&#39640;&#36234;&#30028;&#26816;&#27979;&#24615;&#33021;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#24456;&#22823;&#28508;&#21147;&#65292;&#20294;&#26159;&#20043;&#21069;&#25152;&#26377;&#20851;&#20110;&#24322;&#24120;&#26333;&#20809;&#30340;&#30740;&#31350;&#37117;&#38480;&#20110;&#20351;&#29992;&#35270;&#35273;&#24322;&#24120;&#12290;&#21463;&#21040;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#30340;&#26368;&#26032;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#26412;&#25991;&#39318;&#27425;&#25506;&#32034;&#20102;&#25991;&#26412;&#24322;&#24120;&#26333;&#20809;&#22312;&#23578;&#26410;&#24320;&#25299;&#30340;&#39046;&#22495;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#29992;&#25991;&#26412;&#31561;&#20215;&#29289;&#26367;&#25442;&#22270;&#20687;&#22495;&#20013;&#30340;&#30495;&#23454;&#25110;&#34394;&#25311;&#24322;&#24120;&#20540;&#26469;&#25581;&#31034;&#20351;&#29992;&#25991;&#26412;&#24322;&#24120;&#20540;&#30340;&#22909;&#22788;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#31181;&#29983;&#25104;&#25991;&#26412;&#24322;&#24120;&#20540;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Successful detection of Out-of-Distribution (OoD) data is becoming increasingly important to ensure safe deployment of neural networks. One of the main challenges in OoD detection is that neural networks output overconfident predictions on OoD data, make it difficult to determine OoD-ness of data solely based on their predictions. Outlier exposure addresses this issue by introducing an additional loss that encourages low-confidence predictions on OoD data during training. While outlier exposure has shown promising potential in improving OoD detection performance, all previous studies on outlier exposure have been limited to utilizing visual outliers. Drawing inspiration from the recent advancements in vision-language pre-training, this paper venture out to the uncharted territory of textual outlier exposure. First, we uncover the benefits of using textual outliers by replacing real or virtual outliers in the image-domain with textual equivalents. Then, we propose various ways of genera
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#25551;&#36848;&#36923;&#36753;&#20013;&#20351;&#29992;&#21322;&#29615;&#28335;&#28304;&#30340;&#26694;&#26550;&#65292;&#24182;&#23450;&#20041;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#36731;&#37327;&#32423;&#25551;&#36848;&#36923;&#36753;&#30340;&#28335;&#28304;&#35821;&#20041;&#12290;&#35770;&#25991;&#35777;&#26126;&#20102;&#22312;&#21322;&#29615;&#26045;&#21152;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#65292;&#35821;&#20041;&#28385;&#36275;&#19968;&#20123;&#37325;&#35201;&#30340;&#29305;&#24615;&#65292;&#24182;&#23545;why&#28335;&#28304;&#26041;&#27861;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2310.16472</link><description>&lt;p&gt;
&#36866;&#29992;&#20110;&#36731;&#37327;&#32423;&#25551;&#36848;&#36923;&#36753;&#30340;&#21322;&#29615;&#28335;&#28304;
&lt;/p&gt;
&lt;p&gt;
Semiring Provenance for Lightweight Description Logics. (arXiv:2310.16472v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16472
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#25551;&#36848;&#36923;&#36753;&#20013;&#20351;&#29992;&#21322;&#29615;&#28335;&#28304;&#30340;&#26694;&#26550;&#65292;&#24182;&#23450;&#20041;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#36731;&#37327;&#32423;&#25551;&#36848;&#36923;&#36753;&#30340;&#28335;&#28304;&#35821;&#20041;&#12290;&#35770;&#25991;&#35777;&#26126;&#20102;&#22312;&#21322;&#29615;&#26045;&#21152;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#65292;&#35821;&#20041;&#28385;&#36275;&#19968;&#20123;&#37325;&#35201;&#30340;&#29305;&#24615;&#65292;&#24182;&#23545;why&#28335;&#28304;&#26041;&#27861;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#21322;&#29615;&#28335;&#28304;&#8212;&#8212;&#19968;&#31181;&#26368;&#21021;&#22312;&#20851;&#31995;&#25968;&#25454;&#24211;&#29615;&#22659;&#20013;&#23450;&#20041;&#30340;&#25104;&#21151;&#26694;&#26550;&#65292;&#29992;&#20110;&#25551;&#36848;&#36923;&#36753;&#12290;&#22312;&#27492;&#19978;&#19979;&#25991;&#20013;&#65292;&#26412;&#20307;&#20844;&#29702;&#34987;&#29992;&#20132;&#25442;&#21322;&#29615;&#30340;&#20803;&#32032;&#36827;&#34892;&#27880;&#37322;&#65292;&#24182;&#19988;&#36825;&#20123;&#27880;&#37322;&#26681;&#25454;&#23427;&#20204;&#30340;&#25512;&#23548;&#26041;&#24335;&#20256;&#25773;&#21040;&#26412;&#20307;&#30340;&#32467;&#26524;&#20013;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#31181;&#28335;&#28304;&#35821;&#20041;&#65292;&#36866;&#29992;&#20110;&#21253;&#25324;&#20960;&#31181;&#36731;&#37327;&#32423;&#25551;&#36848;&#36923;&#36753;&#30340;&#35821;&#35328;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#19982;&#20026;&#24102;&#26377;&#29305;&#23450;&#31867;&#22411;&#27880;&#37322;&#65288;&#22914;&#27169;&#31946;&#24230;&#65289;&#30340;&#26412;&#20307;&#23450;&#20041;&#30340;&#20854;&#20182;&#35821;&#20041;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#19968;&#20123;&#23545;&#21322;&#29615;&#26045;&#21152;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#65292;&#35821;&#20041;&#28385;&#36275;&#19968;&#20123;&#26399;&#26395;&#30340;&#29305;&#24615;&#65288;&#22914;&#25193;&#23637;&#20102;&#25968;&#25454;&#24211;&#20013;&#23450;&#20041;&#30340;&#21322;&#29615;&#28335;&#28304;&#65289;&#12290;&#28982;&#21518;&#25105;&#20204;&#19987;&#27880;&#20110;&#33879;&#21517;&#30340;why&#28335;&#28304;&#26041;&#27861;&#65292;&#23427;&#20801;&#35768;&#35745;&#31639;&#27599;&#20010;&#21152;&#27861;&#24130;&#31561;&#21644;&#20056;&#27861;&#24130;&#31561;&#30340;&#20132;&#25442;&#21322;&#29615;&#30340;&#21322;&#29615;&#28335;&#28304;&#65292;&#24182;&#30740;&#31350;&#20102;&#19982;&#36825;&#31181;&#28335;&#28304;&#26041;&#27861;&#30456;&#20851;&#30340;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate semiring provenance--a successful framework originally defined in the relational database setting--for description logics. In this context, the ontology axioms are annotated with elements of a commutative semiring and these annotations are propagated to the ontology consequences in a way that reflects how they are derived. We define a provenance semantics for a language that encompasses several lightweight description logics and show its relationships with semantics that have been defined for ontologies annotated with a specific kind of annotation (such as fuzzy degrees). We show that under some restrictions on the semiring, the semantics satisfies desirable properties (such as extending the semiring provenance defined for databases). We then focus on the well-known why-provenance, which allows to compute the semiring provenance for every additively and multiplicatively idempotent commutative semiring, and for which we study the complexity of problems related to the prov
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21333;&#30524;&#28145;&#24230;&#20272;&#35745;&#26041;&#27861;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#20154;&#31867;&#24863;&#30693;&#28145;&#24230;&#30340;&#19968;&#20010;&#26368;&#37325;&#35201;&#30340;&#35270;&#35273;&#32447;&#32034;&#8212;&#8212;&#30456;&#23545;&#23610;&#23544;&#12290;&#36890;&#36807;&#27169;&#25311;&#20154;&#31867;&#23454;&#39564;&#21644;&#27979;&#35797;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#23454;&#29616;&#20102;&#24179;&#22343;&#20934;&#30830;&#29575;&#32422;&#20026;77%&#30340;&#32467;&#26524;&#65292;&#38388;&#25509;&#25581;&#31034;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.16457</link><description>&lt;p&gt;
&#12298;&#21333;&#30524;&#28145;&#24230;&#20272;&#35745;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#12299;
&lt;/p&gt;
&lt;p&gt;
Towards Explainability in Monocular Depth Estimation. (arXiv:2310.16457v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16457
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21333;&#30524;&#28145;&#24230;&#20272;&#35745;&#26041;&#27861;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#20154;&#31867;&#24863;&#30693;&#28145;&#24230;&#30340;&#19968;&#20010;&#26368;&#37325;&#35201;&#30340;&#35270;&#35273;&#32447;&#32034;&#8212;&#8212;&#30456;&#23545;&#23610;&#23544;&#12290;&#36890;&#36807;&#27169;&#25311;&#20154;&#31867;&#23454;&#39564;&#21644;&#27979;&#35797;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#23454;&#29616;&#20102;&#24179;&#22343;&#20934;&#30830;&#29575;&#32422;&#20026;77%&#30340;&#32467;&#26524;&#65292;&#38388;&#25509;&#25581;&#31034;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20108;&#32500;&#22270;&#20687;&#28145;&#24230;&#20272;&#35745;&#19968;&#30452;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#20855;&#26377;&#25361;&#25112;&#24615;&#19988;&#24191;&#27867;&#30740;&#31350;&#30340;&#35838;&#39064;&#12290;&#36817;&#24180;&#26469;&#65292;&#38543;&#30528;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#30340;&#20986;&#29616;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#24182;&#19988;&#21462;&#24471;&#20102;&#26497;&#39640;&#30340;&#25104;&#21151;&#29575;&#12290;&#26412;&#25991;&#20851;&#27880;&#21333;&#30524;&#28145;&#24230;&#20272;&#35745;&#26041;&#27861;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#21363;&#20154;&#31867;&#22914;&#20309;&#24863;&#30693;&#28145;&#24230;&#12290;&#36825;&#39033;&#21021;&#27493;&#30740;&#31350;&#24378;&#35843;&#20102;&#26368;&#26174;&#33879;&#30340;&#35270;&#35273;&#32447;&#32034;&#20043;&#19968;&#65292;&#21363;&#30456;&#23545;&#23610;&#23544;&#65292;&#22312;&#20960;&#20046;&#25152;&#26377;&#35266;&#23519;&#30340;&#22270;&#20687;&#20013;&#37117;&#38750;&#24120;&#31361;&#20986;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#29305;&#23450;&#30340;&#23454;&#39564;&#26469;&#27169;&#25311;&#20154;&#31867;&#23454;&#39564;&#65292;&#24182;&#27979;&#35797;&#20102;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20197;&#38388;&#25509;&#35780;&#20272;&#22312;&#25152;&#23450;&#20041;&#30340;&#19978;&#19979;&#25991;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#27979;&#37327;&#20934;&#30830;&#24615;&#38656;&#35201;&#36827;&#19968;&#27493;&#20851;&#27880;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#27530;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#26041;&#27861;&#20013;&#65292;&#24179;&#22343;&#20934;&#30830;&#29575;&#36798;&#21040;&#20102;&#32422;77%&#65292;&#20854;&#20013;&#19968;&#20123;&#26041;&#27861;&#34920;&#29616;&#20986;&#33394;&#65292;&#20174;&#32780;&#38388;&#25509;&#25581;&#31034;&#20102;&#23427;&#20204;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The estimation of depth in two-dimensional images has long been a challenging and extensively studied subject in computer vision. Recently, significant progress has been made with the emergence of Deep Learning-based approaches, which have proven highly successful. This paper focuses on the explainability in monocular depth estimation methods, in terms of how humans perceive depth. This preliminary study emphasizes on one of the most significant visual cues, the relative size, which is prominent in almost all viewed images. We designed a specific experiment to mimic the experiments in humans and have tested state-of-the-art methods to indirectly assess the explainability in the context defined. In addition, we observed that measuring the accuracy required further attention and a particular approach is proposed to this end. The results show that a mean accuracy of around 77% across methods is achieved, with some of the methods performing markedly better, thus, indirectly revealing their
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PEARLM&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35821;&#35328;&#24314;&#27169;&#24320;&#23637;&#22522;&#20110;&#36335;&#24452;&#30340;&#30693;&#35782;&#22270;&#35889;&#25512;&#33616;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#23545;&#39044;&#35757;&#32451;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#20381;&#36182;&#20197;&#21450;&#26410;&#20805;&#20998;&#21033;&#29992;&#23454;&#20307;&#21644;&#20851;&#31995;&#20043;&#38388;&#30456;&#20114;&#20381;&#36182;&#24615;&#30340;&#38382;&#39064;&#65292;&#36824;&#36991;&#20813;&#20102;&#29983;&#25104;&#19981;&#20934;&#30830;&#30340;&#35299;&#37322;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25928;&#26524;&#26174;&#33879;&#12290;</title><link>http://arxiv.org/abs/2310.16452</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;&#36335;&#24452;&#30340;&#30693;&#35782;&#22270;&#25512;&#33616;&#20013;&#30340;&#24544;&#23454;&#36335;&#24452;&#35821;&#35328;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Faithful Path Language Modelling for Explainable Recommendation over Knowledge Graph. (arXiv:2310.16452v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16452
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PEARLM&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35821;&#35328;&#24314;&#27169;&#24320;&#23637;&#22522;&#20110;&#36335;&#24452;&#30340;&#30693;&#35782;&#22270;&#35889;&#25512;&#33616;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#23545;&#39044;&#35757;&#32451;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#20381;&#36182;&#20197;&#21450;&#26410;&#20805;&#20998;&#21033;&#29992;&#23454;&#20307;&#21644;&#20851;&#31995;&#20043;&#38388;&#30456;&#20114;&#20381;&#36182;&#24615;&#30340;&#38382;&#39064;&#65292;&#36824;&#36991;&#20813;&#20102;&#29983;&#25104;&#19981;&#20934;&#30830;&#30340;&#35299;&#37322;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25928;&#26524;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#36335;&#24452;&#25512;&#29702;&#26041;&#27861;&#22312;&#25552;&#39640;&#25512;&#33616;&#31995;&#32479;&#36879;&#26126;&#24230;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PEARLM&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#35821;&#35328;&#24314;&#27169;&#26377;&#25928;&#25429;&#33719;&#29992;&#25143;&#34892;&#20026;&#21644;&#20135;&#21697;&#31471;&#30693;&#35782;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#30452;&#25509;&#20174;&#30693;&#35782;&#22270;&#35889;&#19978;&#30340;&#36335;&#24452;&#20013;&#23398;&#20064;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65292;&#24182;&#23558;&#23454;&#20307;&#21644;&#20851;&#31995;&#32479;&#19968;&#22312;&#21516;&#19968;&#20248;&#21270;&#31354;&#38388;&#20013;&#12290;&#24207;&#21015;&#35299;&#30721;&#30340;&#32422;&#26463;&#20445;&#35777;&#20102;&#36335;&#24452;&#23545;&#30693;&#35782;&#22270;&#35889;&#30340;&#24544;&#23454;&#24615;&#12290;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#19982;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Path reasoning methods over knowledge graphs have gained popularity for their potential to improve transparency in recommender systems. However, the resulting models still rely on pre-trained knowledge graph embeddings, fail to fully exploit the interdependence between entities and relations in the KG for recommendation, and may generate inaccurate explanations. In this paper, we introduce PEARLM, a novel approach that efficiently captures user behaviour and product-side knowledge through language modelling. With our approach, knowledge graph embeddings are directly learned from paths over the KG by the language model, which also unifies entities and relations in the same optimisation space. Constraints on the sequence decoding additionally guarantee path faithfulness with respect to the KG. Experiments on two datasets show the effectiveness of our approach compared to state-of-the-art baselines. Source code and datasets: AVAILABLE AFTER GETTING ACCEPTED.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22810;&#38382;&#39064;&#29983;&#25104;&#27169;&#22411;&#65288;mQG&#65289;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#20851;&#27880;&#19978;&#19979;&#25991;&#21644;&#38382;&#39064;&#26469;&#29983;&#25104;&#22810;&#26679;&#21270;&#19988;&#21487;&#22238;&#31572;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#23545;FairytaleQA&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#21644;&#35780;&#20272;&#65292;&#20197;&#21450;&#22312;TellMeWhy&#21644;SQuAD1.1&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#38646;-shot&#36866;&#24212;&#65292;mQG&#22312;&#21508;&#31181;&#35780;&#20272;&#25351;&#26631;&#19978;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#36825;&#39033;&#30740;&#31350;&#23558;&#38382;&#39064;&#29983;&#25104;&#30340;&#22810;&#26679;&#24615;&#24341;&#20837;&#25925;&#20107;&#20070;&#39046;&#22495;&#65292;&#20026;&#25552;&#39640;&#29702;&#35299;&#21644;&#21442;&#19982;&#24230;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.16446</link><description>&lt;p&gt;
&#25925;&#20107;&#20070;&#30340;&#22810;&#26679;&#24615;&#22686;&#24378;&#21465;&#20107;&#38382;&#39064;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Diversity Enhanced Narrative Question Generation for Storybooks. (arXiv:2310.16446v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16446
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22810;&#38382;&#39064;&#29983;&#25104;&#27169;&#22411;&#65288;mQG&#65289;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#20851;&#27880;&#19978;&#19979;&#25991;&#21644;&#38382;&#39064;&#26469;&#29983;&#25104;&#22810;&#26679;&#21270;&#19988;&#21487;&#22238;&#31572;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#23545;FairytaleQA&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#21644;&#35780;&#20272;&#65292;&#20197;&#21450;&#22312;TellMeWhy&#21644;SQuAD1.1&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#38646;-shot&#36866;&#24212;&#65292;mQG&#22312;&#21508;&#31181;&#35780;&#20272;&#25351;&#26631;&#19978;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#36825;&#39033;&#30740;&#31350;&#23558;&#38382;&#39064;&#29983;&#25104;&#30340;&#22810;&#26679;&#24615;&#24341;&#20837;&#25925;&#20107;&#20070;&#39046;&#22495;&#65292;&#20026;&#25552;&#39640;&#29702;&#35299;&#21644;&#21442;&#19982;&#24230;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#32473;&#23450;&#30340;&#19978;&#19979;&#25991;&#29983;&#25104;&#38382;&#39064;&#21487;&#20197;&#22686;&#24378;&#29702;&#35299;&#12289;&#21442;&#19982;&#24230;&#12289;&#35780;&#20272;&#21644;&#23398;&#20064;&#25110;&#23545;&#35805;&#29615;&#22659;&#30340;&#25972;&#20307;&#25928;&#21147;&#12290;&#23613;&#31649;&#38382;&#39064;&#29983;&#25104;&#39046;&#22495;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#25552;&#39640;&#25110;&#34913;&#37327;&#29983;&#25104;&#38382;&#39064;&#30340;&#22810;&#26679;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22810;&#38382;&#39064;&#29983;&#25104;&#27169;&#22411;&#65288;mQG&#65289;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#20851;&#27880;&#19978;&#19979;&#25991;&#21644;&#38382;&#39064;&#26469;&#29983;&#25104;&#22810;&#26679;&#21270;&#19988;&#21487;&#22238;&#31572;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#39564;&#35777;&#29983;&#25104;&#38382;&#39064;&#30340;&#21487;&#22238;&#31572;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#32463;&#36807;SQuAD2.0&#24494;&#35843;&#30340;&#38382;&#31572;&#27169;&#22411;&#65292;&#23558;&#38382;&#39064;&#20998;&#31867;&#20026;&#21487;&#22238;&#31572;&#25110;&#19981;&#21487;&#22238;&#31572;&#12290;&#25105;&#20204;&#22312;FairytaleQA&#25968;&#25454;&#38598;&#19978;&#23545;mQG&#36827;&#34892;&#35757;&#32451;&#21644;&#35780;&#20272;&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#22522;&#20110;&#25925;&#20107;&#20070;&#30340;&#32467;&#26500;&#21270;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#21465;&#20107;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#36824;&#23545;TellMeWhy&#21644;SQuAD1.1&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#38646;-shot&#36866;&#24212;&#12290;mQG&#22312;&#21508;&#31181;&#35780;&#20272;&#25351;&#26631;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#36229;&#36807;&#20102;&#24378;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Question generation (QG) from a given context can enhance comprehension, engagement, assessment, and overall efficacy in learning or conversational environments. Despite recent advancements in QG, the challenge of enhancing or measuring the diversity of generated questions often remains unaddressed. In this paper, we introduce a multi-question generation model (mQG), which is capable of generating multiple, diverse, and answerable questions by focusing on context and questions. To validate the answerability of the generated questions, we employ a SQuAD2.0 fine-tuned question answering model, classifying the questions as answerable or not. We train and evaluate mQG on the FairytaleQA dataset, a well-structured QA dataset based on storybooks, with narrative questions. We further apply a zero-shot adaptation on the TellMeWhy and SQuAD1.1 datasets. mQG shows promising results across various evaluation metrics, among strong baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#21512;&#20102;XGBoost&#21644;xDeepFM&#31639;&#27861;&#30340;&#38598;&#25104;&#27169;&#22411;&#65292;&#26088;&#22312;&#25913;&#36827;&#21330;&#20013;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#20005;&#26684;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#36890;&#36807;&#19982;&#20854;&#20182;&#27169;&#22411;&#30340;&#23545;&#27604;&#24471;&#20986;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#65292;&#23545;&#21330;&#20013;&#39044;&#27979;&#39046;&#22495;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#36827;&#23637;&#20570;&#20986;&#20102;&#37325;&#35201;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2310.16430</link><description>&lt;p&gt;
&#19968;&#20010;&#32508;&#21512;&#24615;&#33539;&#24335;&#29992;&#20110;&#22686;&#24378;&#21330;&#20013;&#39044;&#27979;&#65306;&#32467;&#21512;XGBoost&#21644;xDeepFM&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Integrative Paradigm for Enhanced Stroke Prediction: Synergizing XGBoost and xDeepFM Algorithms. (arXiv:2310.16430v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16430
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#21512;&#20102;XGBoost&#21644;xDeepFM&#31639;&#27861;&#30340;&#38598;&#25104;&#27169;&#22411;&#65292;&#26088;&#22312;&#25913;&#36827;&#21330;&#20013;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#20005;&#26684;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#36890;&#36807;&#19982;&#20854;&#20182;&#27169;&#22411;&#30340;&#23545;&#27604;&#24471;&#20986;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#65292;&#23545;&#21330;&#20013;&#39044;&#27979;&#39046;&#22495;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#36827;&#23637;&#20570;&#20986;&#20102;&#37325;&#35201;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21330;&#20013;&#39044;&#27979;&#22312;&#39044;&#38450;&#21644;&#31649;&#29702;&#36825;&#31181;&#33268;&#27531;&#26465;&#20214;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#22871;&#32508;&#21512;&#25968;&#25454;&#38598;&#26469;&#24212;&#23545;&#21330;&#20013;&#39044;&#27979;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;XGBoost&#21644;xDeepFM&#31639;&#27861;&#21147;&#37327;&#30340;&#38598;&#25104;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26088;&#22312;&#36890;&#36807;&#23454;&#29616;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#26469;&#25913;&#36827;&#29616;&#26377;&#30340;&#21330;&#20013;&#39044;&#27979;&#27169;&#22411;&#12290;&#36890;&#36807;&#20005;&#26684;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#21033;&#29992;AUC&#25351;&#26631;&#39564;&#35777;&#20102;&#38598;&#25104;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#23558;&#25105;&#20204;&#30340;&#21457;&#29616;&#19982;&#35813;&#39046;&#22495;&#20854;&#20182;&#27169;&#22411;&#30340;&#21457;&#29616;&#36827;&#34892;&#23545;&#27604;&#65292;&#25105;&#20204;&#23545;&#21508;&#31181;&#26041;&#27861;&#30340;&#20248;&#32570;&#28857;&#26377;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;&#36825;&#21453;&#36807;&#26469;&#23545;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#21330;&#20013;&#39044;&#27979;&#39046;&#22495;&#30340;&#36827;&#23637;&#20570;&#20986;&#20102;&#37325;&#35201;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stroke prediction plays a crucial role in preventing and managing this debilitating condition. In this study, we address the challenge of stroke prediction using a comprehensive dataset, and propose an ensemble model that combines the power of XGBoost and xDeepFM algorithms. Our work aims to improve upon existing stroke prediction models by achieving higher accuracy and robustness. Through rigorous experimentation, we validate the effectiveness of our ensemble model using the AUC metric. Through comparing our findings with those of other models in the field, we gain valuable insights into the merits and drawbacks of various approaches. This, in turn, contributes significantly to the progress of machine learning and deep learning techniques specifically in the domain of stroke prediction.
&lt;/p&gt;</description></item><item><title>&#22270;&#20687;&#26234;&#33021;&#20307;&#65288;GA&#65289;&#26159;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#22270;&#24418;&#23884;&#20837;&#26041;&#27861;&#21644;&#31526;&#21495;&#25512;&#29702;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23454;&#29616;&#20102;&#23545;&#22797;&#26434;&#22270;&#24418;&#25512;&#29702;&#20219;&#21153;&#30340;&#35299;&#37322;&#24615;&#25512;&#29702;&#65292;&#24182;&#22312;&#33410;&#28857;&#20998;&#31867;&#21644;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26377;&#25928;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.16421</link><description>&lt;p&gt;
&#22270;&#20687;&#24863;&#30693;&#26234;&#33021;&#20307;: &#38024;&#23545;&#22270;&#20687;&#30340;&#26174;&#24615;&#25512;&#29702;&#26234;&#33021;&#20307;
&lt;/p&gt;
&lt;p&gt;
Graph Agent: Explicit Reasoning Agent for Graphs. (arXiv:2310.16421v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16421
&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#26234;&#33021;&#20307;&#65288;GA&#65289;&#26159;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#22270;&#24418;&#23884;&#20837;&#26041;&#27861;&#21644;&#31526;&#21495;&#25512;&#29702;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23454;&#29616;&#20102;&#23545;&#22797;&#26434;&#22270;&#24418;&#25512;&#29702;&#20219;&#21153;&#30340;&#35299;&#37322;&#24615;&#25512;&#29702;&#65292;&#24182;&#22312;&#33410;&#28857;&#20998;&#31867;&#21644;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26377;&#25928;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#24418;&#23884;&#20837;&#26041;&#27861;&#65292;&#22914;&#22270;&#24418;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#21644;&#22270;&#24418;&#21464;&#25442;&#22120;&#65288;Graph Transformers&#65289;&#65292;&#23545;&#20110;&#30693;&#35782;&#22270;&#35889;&#19978;&#30340;&#21508;&#31181;&#20219;&#21153;&#30340;&#22270;&#24418;&#25512;&#29702;&#31639;&#27861;&#30340;&#21457;&#23637;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;&#28982;&#32780;&#65292;&#22270;&#24418;&#23884;&#20837;&#26041;&#27861;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#19981;&#36275;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#38656;&#35201;&#26174;&#24615;&#25512;&#29702;&#30340;&#22330;&#26223;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22270;&#20687;&#26234;&#33021;&#20307;&#65288;GA&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12289;&#24402;&#32435;-&#28436;&#32462;&#25512;&#29702;&#27169;&#22359;&#21644;&#38271;&#26399;&#35760;&#24518;&#36827;&#34892;&#30693;&#35782;&#22270;&#25512;&#29702;&#20219;&#21153;&#30340;&#26234;&#33021;&#20307;&#26041;&#27861;&#12290;GA&#25972;&#21512;&#20102;&#31526;&#21495;&#25512;&#29702;&#21644;&#29616;&#26377;&#30340;&#22270;&#24418;&#23884;&#20837;&#26041;&#27861;&#65292;&#20026;&#22797;&#26434;&#30340;&#22270;&#24418;&#25512;&#29702;&#20219;&#21153;&#25552;&#20379;&#20102;&#21019;&#26032;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#22270;&#24418;&#32467;&#26500;&#36716;&#21270;&#20026;&#25991;&#26412;&#25968;&#25454;&#65292;GA&#20351;LLMs&#33021;&#22815;&#22788;&#29702;&#12289;&#25512;&#29702;&#24182;&#25552;&#20379;&#20154;&#31867;&#21487;&#35299;&#37322;&#30340;&#35299;&#37322;&#12290;&#36890;&#36807;&#33410;&#28857;&#20998;&#31867;&#21644;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#35780;&#20272;&#20102;GA&#30340;&#25928;&#26524;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;GA&#36798;&#21040;&#20102;
&lt;/p&gt;
&lt;p&gt;
Graph embedding methods such as Graph Neural Networks (GNNs) and Graph Transformers have contributed to the development of graph reasoning algorithms for various tasks on knowledge graphs. However, the lack of interpretability and explainability of graph embedding methods has limited their applicability in scenarios requiring explicit reasoning. In this paper, we introduce the Graph Agent (GA), an intelligent agent methodology of leveraging large language models (LLMs), inductive-deductive reasoning modules, and long-term memory for knowledge graph reasoning tasks. GA integrates aspects of symbolic reasoning and existing graph embedding methods to provide an innovative approach for complex graph reasoning tasks. By converting graph structures into textual data, GA enables LLMs to process, reason, and provide predictions alongside human-interpretable explanations. The effectiveness of the GA was evaluated on node classification and link prediction tasks. Results showed that GA reached s
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MulCanon&#30340;&#22810;&#20219;&#21153;&#21462;&#28040;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#24320;&#25918;&#30693;&#35782;&#24211;&#35268;&#33539;&#21270;&#20013;&#30340;&#26426;&#22120;&#21462;&#28040;&#23398;&#20064;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.16419</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#21462;&#28040;&#23398;&#20064;&#22312;&#24320;&#25918;&#30693;&#35782;&#24211;&#35268;&#33539;&#21270;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Open Knowledge Base Canonicalization with Multi-task Unlearning. (arXiv:2310.16419v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16419
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MulCanon&#30340;&#22810;&#20219;&#21153;&#21462;&#28040;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#24320;&#25918;&#30693;&#35782;&#24211;&#35268;&#33539;&#21270;&#20013;&#30340;&#26426;&#22120;&#21462;&#28040;&#23398;&#20064;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#24320;&#25918;&#30693;&#35782;&#24211;&#65288;OKB&#65289;&#30340;&#26500;&#24314;&#23545;&#31227;&#21160;&#35745;&#31639;&#39046;&#22495;&#20013;&#30340;&#35768;&#22810;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;OKB&#20013;&#30340;&#21517;&#35789;&#30701;&#35821;&#21644;&#20851;&#31995;&#30701;&#35821;&#32463;&#24120;&#23384;&#22312;&#20887;&#20313;&#21644;&#27495;&#20041;&#65292;&#36825;&#23601;&#38656;&#35201;&#23545;OKB&#36827;&#34892;&#35268;&#33539;&#21270;&#30340;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#28385;&#36275;&#19968;&#20123;&#38544;&#31169;&#20445;&#25252;&#35268;&#23450;&#30340;&#35201;&#27714;&#65292;&#24182;&#30830;&#20445;&#25968;&#25454;&#30340;&#21450;&#26102;&#24615;&#65292;&#35268;&#33539;&#21270;&#21518;&#30340;OKB&#36890;&#24120;&#38656;&#35201;&#21024;&#38500;&#19968;&#20123;&#25935;&#24863;&#20449;&#24687;&#25110;&#36807;&#26102;&#25968;&#25454;&#12290;OKB&#35268;&#33539;&#21270;&#20013;&#30340;&#26426;&#22120;&#21462;&#28040;&#23398;&#20064;&#26159;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#30340;&#19968;&#20010;&#24456;&#22909;&#30340;&#26041;&#27861;&#12290;&#30446;&#21069;&#30340;&#35299;&#20915;&#26041;&#26696;&#36890;&#36807;&#35774;&#35745;&#20808;&#36827;&#30340;&#32858;&#31867;&#31639;&#27861;&#21644;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65288;KGE&#65289;&#36827;&#19968;&#27493;&#20419;&#36827;&#35268;&#33539;&#21270;&#36807;&#31243;&#12290;&#36843;&#20999;&#38656;&#35201;&#26377;&#25928;&#30340;&#26041;&#26696;&#20805;&#20998;&#21327;&#21516;&#26426;&#22120;&#21462;&#28040;&#23398;&#20064;&#19982;&#32858;&#31867;&#21644;KGE&#23398;&#20064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#20219;&#21153;&#21462;&#28040;&#23398;&#20064;&#26694;&#26550;&#65292;&#21517;&#20026;MulCanon&#65292;&#26469;&#35299;&#20915;OKB&#35268;&#33539;&#21270;&#20013;&#30340;&#26426;&#22120;&#21462;&#28040;&#23398;&#20064;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The construction of large open knowledge bases (OKBs) is integral to many applications in the field of mobile computing. Noun phrases and relational phrases in OKBs often suffer from redundancy and ambiguity, which calls for the investigation on OKB canonicalization. However, in order to meet the requirements of some privacy protection regulations and to ensure the timeliness of the data, the canonicalized OKB often needs to remove some sensitive information or outdated data. The machine unlearning in OKB canonicalization is an excellent solution to the above problem. Current solutions address OKB canonicalization by devising advanced clustering algorithms and using knowledge graph embedding (KGE) to further facilitate the canonicalization process. Effective schemes are urgently needed to fully synergise machine unlearning with clustering and KGE learning. To this end, we put forward a multi-task unlearning framework, namely MulCanon, to tackle machine unlearning problem in OKB canonic
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;AlphaZero&#20013;&#25552;&#21462;&#26032;&#30340;&#22269;&#38469;&#35937;&#26827;&#27010;&#24565;&#65292;&#24182;&#21457;&#29616;&#36825;&#20123;&#27010;&#24565;&#21487;&#20197;&#34987;&#39030;&#32423;&#22269;&#38469;&#35937;&#26827;&#22823;&#24072;&#25152;&#23398;&#20064;&#21644;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.16410</link><description>&lt;p&gt;
&#24357;&#21512;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#30693;&#35782;&#30340;&#24046;&#36317;&#65306;&#22312;AlphaZero&#20013;&#36827;&#34892;&#27010;&#24565;&#21457;&#29616;&#21644;&#20256;&#36882;
&lt;/p&gt;
&lt;p&gt;
Bridging the Human-AI Knowledge Gap: Concept Discovery and Transfer in AlphaZero. (arXiv:2310.16410v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16410
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;AlphaZero&#20013;&#25552;&#21462;&#26032;&#30340;&#22269;&#38469;&#35937;&#26827;&#27010;&#24565;&#65292;&#24182;&#21457;&#29616;&#36825;&#20123;&#27010;&#24565;&#21487;&#20197;&#34987;&#39030;&#32423;&#22269;&#38469;&#35937;&#26827;&#22823;&#24072;&#25152;&#23398;&#20064;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#36229;&#20154;&#31867;&#27700;&#24179;&#30340;&#34920;&#29616;&#65292;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#36827;&#19968;&#27493;&#25552;&#21319;&#20154;&#31867;&#30693;&#35782;&#21644;&#25552;&#39640;&#20154;&#31867;&#19987;&#23478;&#34920;&#29616;&#30340;&#26426;&#20250;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#39640;&#25928;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#25152;&#21253;&#21547;&#30340;&#30693;&#35782;&#24448;&#24448;&#38590;&#20197;&#25552;&#21462;&#65292;&#20063;&#21487;&#33021;&#38590;&#20197;&#29702;&#35299;&#25110;&#23398;&#20064;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;AlphaZero&#20013;&#25552;&#21462;&#26032;&#30340;&#22269;&#38469;&#35937;&#26827;&#27010;&#24565;&#65292;AlphaZero&#26159;&#19968;&#20010;&#36890;&#36807;&#33258;&#25105;&#23545;&#24328;&#32780;&#25484;&#25569;&#22269;&#38469;&#35937;&#26827;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;AlphaZero&#21487;&#33021;&#32534;&#30721;&#20102;&#36229;&#36234;&#29616;&#26377;&#20154;&#31867;&#30693;&#35782;&#30340;&#30693;&#35782;&#65292;&#20294;&#36825;&#20123;&#30693;&#35782;&#26368;&#32456;&#24182;&#19981;&#36229;&#20986;&#20154;&#31867;&#30340;&#29702;&#35299;&#33539;&#22260;&#65292;&#24182;&#19988;&#21487;&#20197;&#25104;&#21151;&#22320;&#23398;&#20064;&#12290;&#22312;&#20154;&#31867;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#27010;&#24565;&#26159;&#21487;&#20197;&#34987;&#39030;&#32423;&#22269;&#38469;&#35937;&#26827;&#22823;&#24072;&#25152;&#23398;&#20064;&#30340;&#65292;&#22240;&#20026;&#22235;&#21517;&#39030;&#32423;&#22269;&#38469;&#35937;&#26827;&#22823;&#24072;&#22312;&#35299;&#20915;&#25152;&#21576;&#29616;&#30340;&#27010;&#24565;&#21407;&#22411;&#20301;&#32622;&#26102;&#26174;&#31034;&#20986;&#20102;&#36827;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence (AI) systems have made remarkable progress, attaining super-human performance across various domains. This presents us with an opportunity to further human knowledge and improve human expert performance by leveraging the hidden knowledge encoded within these highly performant AI systems. Yet, this knowledge is often hard to extract, and may be hard to understand or learn from. Here, we show that this is possible by proposing a new method that allows us to extract new chess concepts in AlphaZero, an AI system that mastered the game of chess via self-play without human supervision. Our analysis indicates that AlphaZero may encode knowledge that extends beyond the existing human knowledge, but knowledge that is ultimately not beyond human grasp, and can be successfully learned from. In a human study, we show that these concepts are learnable by top human experts, as four top chess grandmasters show improvements in solving the presented concept prototype positions. 
&lt;/p&gt;</description></item><item><title>&#23556;&#39057;&#25351;&#32441;&#35782;&#21035;&#25216;&#26415;&#36890;&#36807;&#21033;&#29992;&#30828;&#20214;&#32570;&#38519;&#22312;&#29289;&#29702;&#23618;&#39564;&#35777;&#26080;&#32447;&#35774;&#22791;&#30340;&#36523;&#20221;&#65292;&#28982;&#32780;&#19982;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#25972;&#21512;&#21644;&#23454;&#38469;&#22330;&#26223;&#24212;&#29992;&#24102;&#26469;&#20102;&#35768;&#22810;&#25361;&#25112;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#36825;&#20123;&#25361;&#25112;&#24182;&#25351;&#20986;&#24403;&#21069;&#38459;&#30861;&#23556;&#39057;&#25351;&#32441;&#35782;&#21035;&#23454;&#38469;&#37096;&#32626;&#30340;&#38382;&#39064;&#65292;&#24182;&#25506;&#35752;&#20102;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2310.16406</link><description>&lt;p&gt;
&#23556;&#39057;&#25351;&#32441;&#35782;&#21035;&#30340;&#25361;&#25112;&#65306;&#20174;&#25968;&#25454;&#25910;&#38598;&#21040;&#37096;&#32626;
&lt;/p&gt;
&lt;p&gt;
Challenges of Radio Frequency Fingerprinting: From Data Collection to Deployment. (arXiv:2310.16406v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16406
&lt;/p&gt;
&lt;p&gt;
&#23556;&#39057;&#25351;&#32441;&#35782;&#21035;&#25216;&#26415;&#36890;&#36807;&#21033;&#29992;&#30828;&#20214;&#32570;&#38519;&#22312;&#29289;&#29702;&#23618;&#39564;&#35777;&#26080;&#32447;&#35774;&#22791;&#30340;&#36523;&#20221;&#65292;&#28982;&#32780;&#19982;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#25972;&#21512;&#21644;&#23454;&#38469;&#22330;&#26223;&#24212;&#29992;&#24102;&#26469;&#20102;&#35768;&#22810;&#25361;&#25112;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#36825;&#20123;&#25361;&#25112;&#24182;&#25351;&#20986;&#24403;&#21069;&#38459;&#30861;&#23556;&#39057;&#25351;&#32441;&#35782;&#21035;&#23454;&#38469;&#37096;&#32626;&#30340;&#38382;&#39064;&#65292;&#24182;&#25506;&#35752;&#20102;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23556;&#39057;&#25351;&#32441;&#35782;&#21035;&#65288;RFF&#65289;&#25216;&#26415;&#25215;&#35834;&#22522;&#20110;&#21046;&#36896;&#36807;&#31243;&#20013;&#24341;&#20837;&#30340;&#22266;&#26377;&#30828;&#20214;&#32570;&#38519;&#26469;&#22312;&#29289;&#29702;&#23618;&#23545;&#26080;&#32447;&#35774;&#22791;&#36827;&#34892;&#36523;&#20221;&#39564;&#35777;&#12290;&#36825;&#20123;&#23556;&#39057;&#21457;&#23556;&#22120;&#30340;&#32570;&#38519;&#21453;&#26144;&#22312;&#31354;&#20013;&#20449;&#21495;&#20013;&#65292;&#20351;&#25509;&#25910;&#22120;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#23556;&#39057;&#21457;&#23556;&#28304;&#12290;&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#65292;&#29305;&#21035;&#26159;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#30340;&#36827;&#27493;&#65292;&#25552;&#39640;&#20102;RFF&#31995;&#32479;&#25552;&#21462;&#21644;&#23398;&#20064;&#26500;&#25104;&#35774;&#22791;&#29305;&#23450;&#25351;&#32441;&#30340;&#22797;&#26434;&#29305;&#24449;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23558;DL&#25216;&#26415;&#19982;RFF&#38598;&#25104;&#24182;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#36816;&#34892;&#31995;&#32479;&#38754;&#20020;&#35768;&#22810;&#25361;&#25112;&#12290;&#26412;&#25991;&#22312;&#32771;&#34385;DL-based RFF&#31995;&#32479;&#30340;&#19977;&#20010;&#21442;&#32771;&#38454;&#27573;&#65288;&#25968;&#25454;&#25910;&#38598;&#21644;&#39044;&#22788;&#29702;&#65292;&#35757;&#32451;&#65292;&#26368;&#21518;&#37096;&#32626;&#65289;&#30340;&#22522;&#30784;&#19978;&#65292;&#35782;&#21035;&#21644;&#20998;&#26512;&#20102;&#36825;&#20123;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#25351;&#20986;&#20102;&#30446;&#21069;&#38459;&#30861;RFF&#23454;&#38469;&#37096;&#32626;&#30340;&#29616;&#26377;&#38382;&#39064;&#65292;&#24182;&#35752;&#35770;&#20102;&#26377;&#21069;&#36884;&#30340;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Radio Frequency Fingerprinting (RFF) techniques promise to authenticate wireless devices at the physical layer based on inherent hardware imperfections introduced during manufacturing. Such RF transmitter imperfections are reflected into over-the-air signals, allowing receivers to accurately identify the RF transmitting source. Recent advances in Machine Learning, particularly in Deep Learning (DL), have improved the ability of RFF systems to extract and learn complex features that make up the device-specific fingerprint. However, integrating DL techniques with RFF and operating the system in real-world scenarios presents numerous challenges. This article identifies and analyzes these challenges while considering the three reference phases of any DL-based RFF system: (i) data collection and preprocessing, (ii) training, and finally, (iii) deployment. Our investigation points out the current open problems that prevent real deployment of RFF while discussing promising future directions, 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FLDM&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#34701;&#21512;&#22270;&#20687; Latent Diffusion Model&#65288;LDM&#65289;&#21644;&#35270;&#39057; LDM&#65292;&#22312;&#35270;&#39057;&#32534;&#36753;&#36807;&#31243;&#20013;&#23454;&#29616;&#20102;&#25991;&#26412;&#24341;&#23548;&#30340;&#35270;&#39057;&#32534;&#36753;&#12290;&#36825;&#19968;&#26041;&#27861;&#26082;&#20445;&#25345;&#20102;&#35270;&#39057;&#30340;&#26102;&#38388;&#19968;&#33268;&#24615;&#65292;&#21448;&#21033;&#29992;&#20102;&#22270;&#20687; LDM &#30340;&#39640;&#20445;&#30495;&#24230;&#65292;&#24182;&#19988;&#20855;&#26377;&#28789;&#27963;&#24615;&#19982;&#21487;&#26367;&#25442;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.16400</link><description>&lt;p&gt;
&#34701;&#21512;&#28508;&#21464;&#25193;&#25955;&#27169;&#22411;&#30340;&#35270;&#39057;&#32534;&#36753;&#65306;&#22810;&#28304;&#28508;&#21464;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Fuse Your Latents: Video Editing with Multi-source Latent Diffusion Models. (arXiv:2310.16400v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16400
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FLDM&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#34701;&#21512;&#22270;&#20687; Latent Diffusion Model&#65288;LDM&#65289;&#21644;&#35270;&#39057; LDM&#65292;&#22312;&#35270;&#39057;&#32534;&#36753;&#36807;&#31243;&#20013;&#23454;&#29616;&#20102;&#25991;&#26412;&#24341;&#23548;&#30340;&#35270;&#39057;&#32534;&#36753;&#12290;&#36825;&#19968;&#26041;&#27861;&#26082;&#20445;&#25345;&#20102;&#35270;&#39057;&#30340;&#26102;&#38388;&#19968;&#33268;&#24615;&#65292;&#21448;&#21033;&#29992;&#20102;&#22270;&#20687; LDM &#30340;&#39640;&#20445;&#30495;&#24230;&#65292;&#24182;&#19988;&#20855;&#26377;&#28789;&#27963;&#24615;&#19982;&#21487;&#26367;&#25442;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28508;&#21464;&#25193;&#25955;&#27169;&#22411;&#65288;LDM&#65289;&#20197;&#20854;&#22312;&#22270;&#20687;&#21644;&#35270;&#39057;&#21512;&#25104;&#26041;&#38754;&#30340;&#24378;&#22823;&#33021;&#21147;&#32780;&#38395;&#21517;&#12290;&#28982;&#32780;&#65292;&#35270;&#39057;&#32534;&#36753;&#26041;&#27861;&#23384;&#22312;&#30528;&#39044;&#35757;&#32451;&#25968;&#25454;&#19981;&#36275;&#25110;&#35270;&#39057;&#36880;&#24103;&#37325;&#26032;&#35757;&#32451;&#25104;&#26412;&#39640;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FLDM&#65288;&#34701;&#21512;&#28508;&#21464;&#25193;&#25955;&#27169;&#22411;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26080;&#38656;&#35757;&#32451;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#35270;&#39057;LDM&#20013;&#24212;&#29992;&#29616;&#25104;&#30340;&#22270;&#20687;&#32534;&#36753;&#26041;&#27861;&#26469;&#23454;&#29616;&#22522;&#20110;&#25991;&#26412;&#30340;&#35270;&#39057;&#32534;&#36753;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;FLDM&#22312;&#21435;&#22122;&#36807;&#31243;&#20013;&#34701;&#21512;&#20102;&#22270;&#20687;LDM&#21644;&#35270;&#39057;LDM&#30340;&#28508;&#21464;&#12290;&#36825;&#26679;&#65292;&#21487;&#20197;&#20445;&#25345;&#35270;&#39057;LDM&#30340;&#26102;&#38388;&#19968;&#33268;&#24615;&#65292;&#21516;&#26102;&#20063;&#21487;&#20197;&#21033;&#29992;&#22270;&#20687;LDM&#30340;&#39640;&#20445;&#30495;&#24230;&#12290;&#21516;&#26102;&#65292;&#30001;&#20110;&#22270;&#20687;LDM&#21644;&#35270;&#39057;LDM&#37117;&#21487;&#20197;&#26367;&#25442;&#65292;&#25152;&#20197;FLDM&#20855;&#26377;&#24456;&#39640;&#30340;&#28789;&#27963;&#24615;&#65292;&#21487;&#20197;&#21033;&#29992;&#39640;&#32423;&#22270;&#20687;&#32534;&#36753;&#26041;&#27861;&#65292;&#22914;InstructPix2Pix&#21644;ControlNet&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;FLDM&#26159;&#31532;&#19968;&#31181;&#23558;&#29616;&#25104;&#30340;&#22270;&#20687;&#32534;&#36753;&#26041;&#27861;&#24212;&#29992;&#20110;&#35270;&#39057;LDM&#36827;&#34892;&#35270;&#39057;&#32534;&#36753;&#30340;&#26041;&#27861;&#12290;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23450;&#37327;&#21644;&#23450;&#24615;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Latent Diffusion Models (LDMs) are renowned for their powerful capabilities in image and video synthesis. Yet, video editing methods suffer from insufficient pre-training data or video-by-video re-training cost. In addressing this gap, we propose FLDM (Fused Latent Diffusion Model), a training-free framework to achieve text-guided video editing by applying off-the-shelf image editing methods in video LDMs. Specifically, FLDM fuses latents from an image LDM and an video LDM during the denoising process. In this way, temporal consistency can be kept with video LDM while high-fidelity from the image LDM can also be exploited. Meanwhile, FLDM possesses high flexibility since both image LDM and video LDM can be replaced so advanced image editing methods such as InstructPix2Pix and ControlNet can be exploited. To the best of our knowledge, FLDM is the first method to adapt off-the-shelf image editing methods into video LDMs for video editing. Extensive quantitative and qualitative experiment
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#23558;&#24515;&#29702;&#27979;&#37327;&#23398;&#25918;&#22312;&#35780;&#20272;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#26680;&#24515;&#20301;&#32622;&#65292;&#20197;&#35299;&#20915;&#20256;&#32479;&#22522;&#20934;&#30340;&#19968;&#20123;&#25361;&#25112;&#21644;&#32570;&#28857;&#12290;</title><link>http://arxiv.org/abs/2310.16379</link><description>&lt;p&gt;
&#29992;&#24515;&#29702;&#27979;&#37327;&#23398;&#35780;&#20272;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Evaluating General-Purpose AI with Psychometrics. (arXiv:2310.16379v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16379
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#23558;&#24515;&#29702;&#27979;&#37327;&#23398;&#25918;&#22312;&#35780;&#20272;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#26680;&#24515;&#20301;&#32622;&#65292;&#20197;&#35299;&#20915;&#20256;&#32479;&#22522;&#20934;&#30340;&#19968;&#20123;&#25361;&#25112;&#21644;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#24050;&#32463;&#20174;&#29305;&#23450;&#20219;&#21153;&#21521;&#36890;&#29992;&#31995;&#32479;&#30340;&#21457;&#23637;&#65292;&#36235;&#21521;&#20110;&#20154;&#31867;&#30340;&#22810;&#21151;&#33021;&#24615;&#12290;&#38543;&#30528;AI&#31995;&#32479;&#24320;&#22987;&#22312;&#31038;&#20250;&#20013;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#65292;&#30830;&#20445;&#23545;&#20854;&#36827;&#34892;&#20805;&#20998;&#35780;&#20272;&#21464;&#24471;&#24456;&#37325;&#35201;&#12290;&#30446;&#21069;&#30340;AI&#22522;&#20934;&#36890;&#24120;&#22312;&#29305;&#23450;&#20219;&#21153;&#38598;&#21512;&#19978;&#35780;&#20272;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#35780;&#20272;&#36890;&#29992;AI&#31995;&#32479;&#26469;&#35828;&#65292;&#36825;&#26377;&#19968;&#20123;&#32570;&#28857;&#12290;&#39318;&#20808;&#65292;&#24456;&#38590;&#39044;&#27979;AI&#31995;&#32479;&#26159;&#21542;&#33021;&#23436;&#25104;&#19968;&#39033;&#23427;&#20174;&#26410;&#35265;&#36807;&#25110;&#20043;&#21069;&#19981;&#23384;&#22312;&#30340;&#26032;&#20219;&#21153;&#12290;&#20854;&#27425;&#65292;&#36825;&#20123;&#22522;&#20934;&#24120;&#24120;&#20851;&#27880;&#25972;&#20307;&#24615;&#33021;&#25351;&#26631;&#65292;&#21487;&#33021;&#24573;&#35270;&#20102;&#23545;&#20570;&#20986;&#26126;&#26234;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#30340;&#32454;&#33410;&#12290;&#26368;&#21518;&#65292;&#23545;&#29616;&#26377;&#22522;&#20934;&#30340;&#21487;&#38752;&#24615;&#23384;&#22312;&#36234;&#26469;&#36234;&#22810;&#30340;&#25285;&#24551;&#65292;&#24182;&#23545;&#27491;&#22312;&#36827;&#34892;&#30340;&#27979;&#37327;&#25552;&#20986;&#20102;&#30097;&#38382;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#24314;&#35758;&#23558;&#24515;&#29702;&#27979;&#37327;&#23398;&#65292;&#21363;&#24515;&#29702;&#27979;&#37327;&#30340;&#31185;&#23398;&#65292;&#25918;&#22312;&#35780;&#20272;&#36890;&#29992;AI&#30340;&#26680;&#24515;&#20301;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI) has witnessed an evolution from task-specific to general-purpose systems that trend toward human versatility. As AI systems begin to play pivotal roles in society, it is important to ensure that they are adequately evaluated. Current AI benchmarks typically assess performance on collections of specific tasks. This has drawbacks when used for assessing general-purpose AI systems. First, it is difficult to predict whether AI systems could complete a new task it has never seen or that did not previously exist. Second, these benchmarks often focus on overall performance metrics, potentially overlooking the finer details crucial for making informed decisions. Lastly, there are growing concerns about the reliability of existing benchmarks and questions about what is being measured. To solve these challenges, this paper suggests that psychometrics, the science of psychological measurement, should be placed at the core of evaluating general-purpose AI. Psychometric
&lt;/p&gt;</description></item><item><title>GADY&#26159;&#19968;&#31181;&#22312;&#21160;&#24577;&#22270;&#19978;&#36827;&#34892;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20986;&#36830;&#32493;&#21160;&#24577;&#22270;&#27169;&#22411;&#21644;&#24341;&#20837;&#36127;&#37319;&#26679;&#27169;&#22359;&#26469;&#35299;&#20915;&#20102;&#21160;&#24577;&#32467;&#26500;&#26500;&#24314;&#25361;&#25112;&#21644;&#36127;&#37319;&#26679;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.16376</link><description>&lt;p&gt;
GADY: &#21160;&#24577;&#22270;&#19978;&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
GADY: Unsupervised Anomaly Detection on Dynamic Graphs. (arXiv:2310.16376v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16376
&lt;/p&gt;
&lt;p&gt;
GADY&#26159;&#19968;&#31181;&#22312;&#21160;&#24577;&#22270;&#19978;&#36827;&#34892;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20986;&#36830;&#32493;&#21160;&#24577;&#22270;&#27169;&#22411;&#21644;&#24341;&#20837;&#36127;&#37319;&#26679;&#27169;&#22359;&#26469;&#35299;&#20915;&#20102;&#21160;&#24577;&#32467;&#26500;&#26500;&#24314;&#25361;&#25112;&#21644;&#36127;&#37319;&#26679;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#22270;&#19978;&#30340;&#24322;&#24120;&#26816;&#27979;&#26159;&#25351;&#26816;&#27979;&#34892;&#20026;&#26126;&#26174;&#20559;&#31163;&#22270;&#20013;&#35266;&#23519;&#21040;&#30340;&#35268;&#33539;&#34892;&#20026;&#30340;&#23454;&#20307;&#21644;&#23427;&#20204;&#30340;&#26102;&#38388;&#20449;&#24687;&#12290;&#36825;&#20010;&#39046;&#22495;&#22240;&#20854;&#22312;&#37329;&#34701;&#12289;&#32593;&#32476;&#23433;&#20840;&#12289;&#31038;&#20132;&#32593;&#32476;&#31561;&#26041;&#38754;&#30340;&#24212;&#29992;&#32780;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#38754;&#20020;&#20004;&#20010;&#25361;&#25112;&#65306;&#21160;&#24577;&#32467;&#26500;&#26500;&#24314;&#25361;&#25112;-&#38590;&#20197;&#25429;&#25417;&#24102;&#26377;&#22797;&#26434;&#26102;&#38388;&#20449;&#24687;&#30340;&#22270;&#32467;&#26500;&#65292;&#20197;&#21450;&#36127;&#37319;&#26679;&#25361;&#25112;-&#26080;&#27861;&#26500;&#24314;&#20248;&#31168;&#30340;&#36127;&#37319;&#26679;&#36827;&#34892;&#26080;&#30417;&#30563;&#23398;&#20064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#21160;&#24577;&#22270;&#19978;&#30340;&#26080;&#30417;&#30563;&#29983;&#25104;&#24322;&#24120;&#26816;&#27979;&#65288;GADY&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#31532;&#19968;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36830;&#32493;&#30340;&#21160;&#24577;&#22270;&#27169;&#22411;&#26469;&#25429;&#25417;&#32454;&#31890;&#24230;&#20449;&#24687;&#65292;&#31361;&#30772;&#20102;&#29616;&#26377;&#31163;&#25955;&#26041;&#27861;&#30340;&#38480;&#21046;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#20010;&#28040;&#24687;&#20256;&#36882;&#26694;&#26550;&#32467;&#21512;&#20301;&#32622;&#29305;&#24449;&#26469;&#33719;&#21462;&#36793;&#30340;&#23884;&#20837;&#65292;&#36890;&#36807;&#35299;&#30721;&#26469;&#35782;&#21035;&#24322;&#24120;&#12290;&#23545;&#20110;&#31532;&#20108;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#36127;&#37319;&#26679;&#27169;&#22359;&#26469;&#26500;&#36896;&#20248;&#31168;&#30340;&#36127;&#26679;&#26412;&#65292;&#20174;&#32780;&#25552;&#39640;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection on dynamic graphs refers to detecting entities whose behaviors obviously deviate from the norms observed within graphs and their temporal information. This field has drawn increasing attention due to its application in finance, network security, social networks, and more. However, existing methods face two challenges: dynamic structure constructing challenge - difficulties in capturing graph structure with complex time information and negative sampling challenge - unable to construct excellent negative samples for unsupervised learning. To address these challenges, we propose Unsupervised Generative Anomaly Detection on Dynamic Graphs (GADY). To tackle the first challenge, we propose a continuous dynamic graph model to capture the fine-grained information, which breaks the limit of existing discrete methods. Specifically, we employ a message-passing framework combined with positional features to get edge embeddings, which are decoded to identify anomalies. For the sec
&lt;/p&gt;</description></item><item><title>InstructPTS&#26159;&#19968;&#31181;&#29992;&#20110;&#20135;&#21697;&#26631;&#39064;&#25688;&#35201;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25351;&#23548;&#35843;&#33410;LLMs&#21487;&#26681;&#25454;&#22810;&#31181;&#26631;&#20934;&#29983;&#25104;&#26356;&#20934;&#30830;&#30340;&#25688;&#35201;&#65292;&#25552;&#39640;&#20102;&#20135;&#21697;&#21517;&#31216;&#25688;&#35201;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.16361</link><description>&lt;p&gt;
InstructPTS: &#29992;&#20110;&#20135;&#21697;&#26631;&#39064;&#25688;&#35201;&#30340;&#25351;&#23548;&#35843;&#33410;LLMs&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
InstructPTS: Instruction-Tuning LLMs for Product Title Summarization. (arXiv:2310.16361v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16361
&lt;/p&gt;
&lt;p&gt;
InstructPTS&#26159;&#19968;&#31181;&#29992;&#20110;&#20135;&#21697;&#26631;&#39064;&#25688;&#35201;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25351;&#23548;&#35843;&#33410;LLMs&#21487;&#26681;&#25454;&#22810;&#31181;&#26631;&#20934;&#29983;&#25104;&#26356;&#20934;&#30830;&#30340;&#25688;&#35201;&#65292;&#25552;&#39640;&#20102;&#20135;&#21697;&#21517;&#31216;&#25688;&#35201;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#21830;&#21153;&#20135;&#21697;&#30446;&#24405;&#20013;&#21253;&#21547;&#25968;&#21313;&#20159;&#20010;&#21830;&#21697;&#12290;&#22823;&#22810;&#25968;&#20135;&#21697;&#26631;&#39064;&#24456;&#38271;&#65292;&#21334;&#23478;&#20351;&#29992;&#20135;&#21697;&#23646;&#24615;&#26469;&#25913;&#36827;&#21830;&#21697;&#26816;&#32034;&#24182;&#31361;&#20986;&#20851;&#38190;&#20135;&#21697;&#26041;&#38754;&#12290;&#36825;&#23548;&#33268;&#20102;&#19981;&#33258;&#28982;&#30340;&#20135;&#21697;&#26631;&#39064;&#19982;&#23458;&#25143;&#23545;&#20854;&#30340;&#31216;&#21628;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#23427;&#20063;&#38480;&#21046;&#20102;&#30005;&#23376;&#21830;&#21153;&#21830;&#24215;&#20351;&#29992;&#36825;&#20123;&#21334;&#23478;&#25552;&#20379;&#30340;&#26631;&#39064;&#36827;&#34892;&#25512;&#33616;&#12289;&#38382;&#31572;&#25110;&#35780;&#35770;&#25688;&#35201;&#30340;&#33021;&#21147;&#12290;&#21463;&#26368;&#36817;&#26377;&#20851;&#25351;&#23548;&#35843;&#33410;LLMs&#30340;&#30740;&#31350;&#30340;&#21551;&#31034;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;InstructPTS&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#20135;&#21697;&#26631;&#39064;&#25688;&#35201;&#20219;&#21153;&#30340;&#21487;&#25511;&#26041;&#27861;&#12290;&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;&#25351;&#23548;&#24494;&#35843;&#31574;&#30053;&#36827;&#34892;&#35757;&#32451;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#26681;&#25454;&#21508;&#31181;&#26631;&#20934;&#65288;&#20363;&#22914;&#25688;&#35201;&#20013;&#30340;&#21333;&#35789;&#25968;&#37327;&#12289;&#21253;&#21547;&#29305;&#23450;&#30701;&#35821;&#31561;&#65289;&#24635;&#32467;&#20135;&#21697;&#26631;&#39064;&#12290;&#23545;&#23454;&#38469;&#30005;&#23376;&#21830;&#21153;&#30446;&#24405;&#36827;&#34892;&#24191;&#27867;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#19982;&#31616;&#21333;&#24494;&#35843;LLMs&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#26356;&#20934;&#30830;&#30340;&#20135;&#21697;&#21517;&#31216;&#25688;&#35201;&#65292;BLEU&#21644;ROUG&#25552;&#39640;&#20102;&#36229;&#36807;14&#21644;8&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
E-commerce product catalogs contain billions of items. Most products have lengthy titles, as sellers pack them with product attributes to improve retrieval, and highlight key product aspects. This results in a gap between such unnatural products titles, and how customers refer to them. It also limits how e-commerce stores can use these seller-provided titles for recommendation, QA, or review summarization.  Inspired by recent work on instruction-tuned LLMs, we present InstructPTS, a controllable approach for the task of Product Title Summarization (PTS). Trained using a novel instruction fine-tuning strategy, our approach is able to summarize product titles according to various criteria (e.g. number of words in a summary, inclusion of specific phrases, etc.). Extensive evaluation on a real-world e-commerce catalog shows that compared to simple fine-tuning of LLMs, our proposed approach can generate more accurate product name summaries, with an improvement of over 14 and 8 BLEU and ROUG
&lt;/p&gt;</description></item><item><title>AI&#25216;&#26415;&#19982;&#26080;&#20154;&#26426;&#30340;&#32467;&#21512;&#27491;&#24341;&#39046;&#30528;&#20892;&#19994;&#12289;&#30417;&#35270;&#23454;&#36341;&#21644;&#28798;&#23475;&#31649;&#29702;&#31574;&#30053;&#31561;&#39046;&#22495;&#30340;&#38761;&#21629;&#65292;&#23454;&#29616;&#20102;&#23548;&#33322;&#12289;&#25506;&#27979;&#12289;&#30417;&#27979;&#21644;&#36890;&#20449;&#31561;&#26041;&#38754;&#30340;&#31361;&#30772;&#12290;</title><link>http://arxiv.org/abs/2310.16360</link><description>&lt;p&gt;
AI&#25216;&#26415;&#19982;&#26080;&#20154;&#26426;&#30340;&#20840;&#38754;&#32508;&#36848;&#65306;&#36235;&#21183;&#12289;&#24895;&#26223;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Review of AI-enabled Unmanned Aerial Vehicle: Trends, Vision , and Challenges. (arXiv:2310.16360v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16360
&lt;/p&gt;
&lt;p&gt;
AI&#25216;&#26415;&#19982;&#26080;&#20154;&#26426;&#30340;&#32467;&#21512;&#27491;&#24341;&#39046;&#30528;&#20892;&#19994;&#12289;&#30417;&#35270;&#23454;&#36341;&#21644;&#28798;&#23475;&#31649;&#29702;&#31574;&#30053;&#31561;&#39046;&#22495;&#30340;&#38761;&#21629;&#65292;&#23454;&#29616;&#20102;&#23548;&#33322;&#12289;&#25506;&#27979;&#12289;&#30417;&#27979;&#21644;&#36890;&#20449;&#31561;&#26041;&#38754;&#30340;&#31361;&#30772;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#19982;&#26080;&#20154;&#26426;&#65288;UAV&#65289;&#30340;&#32467;&#21512;&#22312;&#21508;&#20010;&#39046;&#22495;&#24102;&#26469;&#20102;&#36827;&#23637;&#12290;&#26412;&#32508;&#21512;&#20998;&#26512;&#25506;&#35752;&#20102;AI&#39537;&#21160;&#30340;&#26080;&#20154;&#26426;&#21644;&#21451;&#22909;&#35745;&#31639;&#22312;&#24212;&#29992;&#20013;&#25152;&#24102;&#26469;&#30340;&#21464;&#38761;&#12290;&#23427;&#28085;&#30422;&#20102;&#26032;&#20852;&#36235;&#21183;&#12289;&#26410;&#26469;&#24895;&#26223;&#20197;&#21450;&#36825;&#31181;&#20851;&#31995;&#25152;&#22266;&#26377;&#30340;&#25361;&#25112;&#12290;&#35813;&#30740;&#31350;&#32771;&#23519;&#20102;AI&#22312;&#23548;&#33322;&#12289;&#25506;&#27979;&#21644;&#36319;&#36394;&#29289;&#20307;&#12289;&#30417;&#27979;&#37326;&#29983;&#21160;&#26893;&#29289;&#12289;&#25552;&#21319;&#31934;&#20934;&#20892;&#19994;&#12289;&#26041;&#20415;&#25937;&#25588;&#34892;&#21160;&#12289;&#36827;&#34892;&#30417;&#35270;&#27963;&#21160;&#20197;&#21450;&#20351;&#29992;&#29615;&#20445;&#24847;&#35782;&#35745;&#31639;&#25216;&#26415;&#22312;&#26080;&#20154;&#26426;&#20043;&#38388;&#24314;&#31435;&#36890;&#20449;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;&#36890;&#36807;&#28145;&#20837;&#30740;&#31350;AI&#19982;&#26080;&#20154;&#26426;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#26412;&#20998;&#26512;&#24378;&#35843;&#20102;&#36825;&#20123;&#25216;&#26415;&#22312;&#20892;&#19994;&#12289;&#30417;&#35270;&#23454;&#36341;&#12289;&#28798;&#23475;&#31649;&#29702;&#31574;&#30053;&#31561;&#34892;&#19994;&#38761;&#21629;&#30340;&#28508;&#21147;&#12290;&#22312;&#26500;&#24819;&#21487;&#33021;&#24615;&#30340;&#21516;&#26102;&#65292;&#23427;&#36824;&#20851;&#27880;&#20102;&#36947;&#24503;&#32771;&#34385;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the combination of artificial intelligence (AI) and unmanned aerial vehicles (UAVs) has brought about advancements in various areas. This comprehensive analysis explores the changing landscape of AI-powered UAVs and friendly computing in their applications. It covers emerging trends, futuristic visions, and the inherent challenges that come with this relationship. The study examines how AI plays a role in enabling navigation, detecting and tracking objects, monitoring wildlife, enhancing precision agriculture, facilitating rescue operations, conducting surveillance activities, and establishing communication among UAVs using environmentally conscious computing techniques. By delving into the interaction between AI and UAVs, this analysis highlights the potential for these technologies to revolutionise industries such as agriculture, surveillance practices, disaster management strategies, and more. While envisioning possibilities, it also takes a look at ethical consider
&lt;/p&gt;</description></item><item><title>AccoMontage-3&#26159;&#19968;&#31181;&#36890;&#36807;&#39034;&#24207;&#39118;&#26684;&#36716;&#25442;&#21644;&#22810;&#36712;&#36947;&#21151;&#33021;&#20808;&#39564;&#23454;&#29616;&#20840;&#38899;&#20048;&#20276;&#22863;&#32534;&#25490;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#26681;&#25454;&#20027;&#26059;&#24459;&#19982;&#21644;&#24358;&#30340;&#36755;&#20837;&#29983;&#25104;&#22810;&#38899;&#36712;&#30340;&#20276;&#22863;&#12290;</title><link>http://arxiv.org/abs/2310.16334</link><description>&lt;p&gt;
AccoMontage-3: &#36890;&#36807;&#39034;&#24207;&#39118;&#26684;&#36716;&#25442;&#21644;&#22810;&#36712;&#36947;&#21151;&#33021;&#20808;&#39564;&#23454;&#29616;&#20840;&#38899;&#20048;&#20276;&#22863;&#32534;&#25490;
&lt;/p&gt;
&lt;p&gt;
AccoMontage-3: Full-Band Accompaniment Arrangement via Sequential Style Transfer and Multi-Track Function Prior. (arXiv:2310.16334v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16334
&lt;/p&gt;
&lt;p&gt;
AccoMontage-3&#26159;&#19968;&#31181;&#36890;&#36807;&#39034;&#24207;&#39118;&#26684;&#36716;&#25442;&#21644;&#22810;&#36712;&#36947;&#21151;&#33021;&#20808;&#39564;&#23454;&#29616;&#20840;&#38899;&#20048;&#20276;&#22863;&#32534;&#25490;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#26681;&#25454;&#20027;&#26059;&#24459;&#19982;&#21644;&#24358;&#30340;&#36755;&#20837;&#29983;&#25104;&#22810;&#38899;&#36712;&#30340;&#20276;&#22863;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;AccoMontage-3&#65292;&#36825;&#26159;&#19968;&#20010;&#31526;&#21495;&#38899;&#20048;&#33258;&#21160;&#21270;&#31995;&#32479;&#65292;&#21487;&#20197;&#26681;&#25454;&#20027;&#26059;&#24459;&#19982;&#21644;&#24358;&#30340;&#36755;&#20837;&#65288;&#21363;&#24341;&#23548;&#20048;&#35889;&#65289;&#65292;&#29983;&#25104;&#22810;&#38899;&#36712;&#30340;&#20840;&#38899;&#20048;&#20276;&#22863;&#12290;&#35813;&#31995;&#32479;&#21253;&#21547;&#19977;&#20010;&#27169;&#22359;&#21270;&#32452;&#20214;&#65292;&#27599;&#20010;&#32452;&#20214;&#27169;&#25311;&#20840;&#38899;&#20048;&#20316;&#26354;&#30340;&#37325;&#35201;&#26041;&#38754;&#12290;&#31532;&#19968;&#20010;&#32452;&#20214;&#26159;&#38050;&#29748;&#32534;&#26354;&#24072;&#65292;&#36890;&#36807;&#23558;&#32441;&#29702;&#39118;&#26684;&#36716;&#25442;&#20026;&#21644;&#24358;&#65292;&#20351;&#29992;&#28508;&#22312;&#30340;&#21644;&#24358;-&#32441;&#29702;&#20998;&#31163;&#21644;&#21551;&#21457;&#24335;&#32441;&#29702;&#20379;&#24212;&#32773;&#26816;&#32034;&#65292;&#29983;&#25104;&#38050;&#29748;&#20276;&#22863;&#12290;&#31532;&#20108;&#20010;&#32452;&#20214;&#26681;&#25454;&#20010;&#21035;&#38899;&#36712;&#21151;&#33021;&#32534;&#30721;&#30340;&#31649;&#24358;&#20048;&#39118;&#26684;&#65292;&#23558;&#38050;&#29748;&#20276;&#22863;&#20048;&#35889;&#32534;&#25490;&#25104;&#20840;&#38899;&#20048;&#20276;&#22863;&#12290;&#23558;&#21069;&#20004;&#20010;&#32452;&#20214;&#36830;&#25509;&#36215;&#26469;&#30340;&#31532;&#19977;&#20010;&#32452;&#20214;&#26159;&#19968;&#20010;&#20808;&#39564;&#27169;&#22411;&#65292;&#29992;&#20110;&#25551;&#36848;&#25972;&#39318;&#38899;&#20048;&#20316;&#21697;&#19978;&#30340;&#32534;&#26354;&#39118;&#26684;&#30340;&#20840;&#23616;&#32467;&#26500;&#12290;&#25972;&#20010;&#31995;&#32479;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#33258;&#25105;&#30417;&#30563;&#22320;&#23398;&#20064;&#29983;&#25104;&#20840;&#38899;&#20048;&#20276;&#22863;&#65292;&#23558;&#39118;&#26684;&#36716;&#25442;&#24212;&#29992;&#20110;&#20004;&#20010;&#23618;&#38754;&#30340;&#22810;&#22768;&#37096;&#21327;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose AccoMontage-3, a symbolic music automation system capable of generating multi-track, full-band accompaniment based on the input of a lead melody with chords (i.e., a lead sheet). The system contains three modular components, each modelling a vital aspect of full-band composition. The first component is a piano arranger that generates piano accompaniment for the lead sheet by transferring texture styles to the chords using latent chord-texture disentanglement and heuristic retrieval of texture donors. The second component orchestrates the piano accompaniment score into full-band arrangement according to the orchestration style encoded by individual track functions. The third component, which connects the previous two, is a prior model characterizing the global structure of orchestration style over the whole piece of music. From end to end, the system learns to generate full-band accompaniment in a self-supervised fashion, applying style transfer at two levels of polyphonic co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CoheSentia&#30340;&#26032;&#22411;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#33258;&#21160;&#29983;&#25104;&#25991;&#26412;&#30340;&#20154;&#31867;&#24863;&#30693;&#36830;&#36143;&#24615;&#12290;&#25105;&#20204;&#30340;&#27880;&#37322;&#21327;&#35758;&#21253;&#25324;&#25972;&#20307;&#35780;&#20998;&#21644;&#36880;&#21477;&#35780;&#20998;&#20004;&#20010;&#35282;&#24230;&#12290;&#36890;&#36807;&#27492;&#22522;&#20934;&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#35780;&#20272;&#29983;&#25104;&#25991;&#26412;&#30340;&#36830;&#36143;&#24615;&#24182;&#20998;&#26512;&#20854;&#30456;&#20851;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2310.16329</link><description>&lt;p&gt;
CoheSentia: &#19968;&#20010;&#23545;&#29983;&#25104;&#25991;&#26412;&#36830;&#36143;&#24615;&#36827;&#34892;&#22686;&#37327;&#19982;&#25972;&#20307;&#35780;&#20272;&#30340;&#26032;&#22411;&#22522;&#20934;&#12290;&#65288;arXiv:2310.16329v1 [cs.CL]&#65289;
&lt;/p&gt;
&lt;p&gt;
CoheSentia: A Novel Benchmark of Incremental versus Holistic Assessment of Coherence in Generated Texts. (arXiv:2310.16329v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16329
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CoheSentia&#30340;&#26032;&#22411;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#33258;&#21160;&#29983;&#25104;&#25991;&#26412;&#30340;&#20154;&#31867;&#24863;&#30693;&#36830;&#36143;&#24615;&#12290;&#25105;&#20204;&#30340;&#27880;&#37322;&#21327;&#35758;&#21253;&#25324;&#25972;&#20307;&#35780;&#20998;&#21644;&#36880;&#21477;&#35780;&#20998;&#20004;&#20010;&#35282;&#24230;&#12290;&#36890;&#36807;&#27492;&#22522;&#20934;&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#35780;&#20272;&#29983;&#25104;&#25991;&#26412;&#30340;&#36830;&#36143;&#24615;&#24182;&#20998;&#26512;&#20854;&#30456;&#20851;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#36143;&#24615;&#26159;&#19968;&#20010;&#35821;&#35328;&#23398;&#26415;&#35821;&#65292;&#25351;&#30340;&#26159;&#25991;&#26412;&#21333;&#20803;&#65288;&#21477;&#23376;&#12289;&#21629;&#39064;&#65289;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20351;&#25991;&#26412;&#22312;&#36923;&#36753;&#19978;&#19968;&#33268;&#24182;&#23545;&#35835;&#32773;&#26377;&#24847;&#20041;&#12290;&#38543;&#30528;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#29983;&#25104;&#27169;&#22411;&#30340;&#36827;&#23637;&#65292;&#36843;&#20999;&#38656;&#35201;&#33258;&#21160;&#35780;&#20272;&#33258;&#21160;&#29983;&#25104;&#25991;&#26412;&#30340;&#20154;&#31867;&#24863;&#30693;&#36830;&#36143;&#24615;&#12290;&#30446;&#21069;&#20026;&#27490;&#65292;&#20851;&#20110;&#26126;&#30830;&#35780;&#20272;&#29983;&#25104;&#25991;&#26412;&#30340;&#36830;&#36143;&#24615;&#24182;&#20998;&#26512;&#65288;&#19981;&#65289;&#36830;&#36143;&#24615;&#22240;&#32032;&#30340;&#24037;&#20316;&#24456;&#23569;&#12290;&#20197;&#24448;&#20851;&#20110;&#35813;&#20027;&#39064;&#30340;&#30740;&#31350;&#20351;&#29992;&#20854;&#20182;&#20219;&#21153;&#65288;&#22914;&#21477;&#23376;&#37325;&#25490;&#65289;&#20316;&#20026;&#36830;&#36143;&#24615;&#30340;&#26367;&#20195;&#25351;&#26631;&#65292;&#32780;&#19981;&#26159;&#30452;&#25509;&#36827;&#34892;&#36830;&#36143;&#24615;&#26816;&#27979;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;CoheSentia&#30340;&#26032;&#22411;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#33258;&#21160;&#29983;&#25104;&#25991;&#26412;&#30340;&#20154;&#31867;&#24863;&#30693;&#36830;&#36143;&#24615;&#12290;&#25105;&#20204;&#30340;&#27880;&#37322;&#21327;&#35758;&#21453;&#26144;&#20102;&#20004;&#20010;&#35282;&#24230;&#65306;&#19968;&#20010;&#26159;&#20840;&#23616;&#35282;&#24230;&#65292;&#32473;&#20986;&#19968;&#20010;&#24635;&#20307;&#36830;&#36143;&#24615;&#20998;&#25968;&#65307;&#21478;&#19968;&#20010;&#26159;&#36880;&#21477;&#35780;&#20998;&#30340;&#22686;&#37327;&#35282;&#24230;&#12290;&#22686;&#37327;&#26041;&#27861;&#20135;&#29983;&#20102;&#19968;&#20010;&#65288;&#19981;&#65289;&#36830;&#36143;&#24615;&#35780;&#20272;&#20197;&#21450;&#30456;&#24212;&#30340;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Coherence is a linguistic term that refers to the relations between small textual units (sentences, propositions), which make the text logically consistent and meaningful to the reader. With the advances of generative foundational models in NLP, there is a pressing need to automatically assess the human-perceived coherence of automatically generated texts. Up until now, little work has been done on explicitly assessing the coherence of generated texts and analyzing the factors contributing to (in)coherence. Previous work on the topic used other tasks, e.g., sentence reordering, as proxies of coherence, rather than approaching coherence detection heads on. In this paper, we introduce {\sc CoheSentia}, a novel benchmark of human-perceived coherence of automatically generated texts. Our annotation protocol reflects two perspectives; one is global, assigning a single coherence score, and the other is incremental, scoring sentence by sentence. The incremental method produces an (in)coherenc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19982;&#35821;&#35328;&#24418;&#24335;&#26080;&#20851;&#30340;&#20803;&#23398;&#20064;&#33945;&#29256;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;MetaMAE&#65292;&#36890;&#36807;&#23558;&#33945;&#29256;&#33258;&#32534;&#30721;&#22120;&#65288;MAE&#65289;&#30340;&#33945;&#29256;&#37325;&#26500;&#35270;&#20026;&#20803;&#23398;&#20064;&#20219;&#21153;&#65292;&#24182;&#37319;&#29992;&#36716;&#25442;&#22120;&#20803;&#23398;&#20064;&#25216;&#26415;&#26469;&#25913;&#36827;MAE&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#19981;&#21516;&#35821;&#35328;&#24418;&#24335;&#19978;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.16318</link><description>&lt;p&gt;
&#19968;&#31181;&#19982;&#35821;&#35328;&#24418;&#24335;&#26080;&#20851;&#30340;&#20803;&#23398;&#20064;&#33945;&#29256;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Modality-Agnostic Self-Supervised Learning with Meta-Learned Masked Auto-Encoder. (arXiv:2310.16318v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16318
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19982;&#35821;&#35328;&#24418;&#24335;&#26080;&#20851;&#30340;&#20803;&#23398;&#20064;&#33945;&#29256;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;MetaMAE&#65292;&#36890;&#36807;&#23558;&#33945;&#29256;&#33258;&#32534;&#30721;&#22120;&#65288;MAE&#65289;&#30340;&#33945;&#29256;&#37325;&#26500;&#35270;&#20026;&#20803;&#23398;&#20064;&#20219;&#21153;&#65292;&#24182;&#37319;&#29992;&#36716;&#25442;&#22120;&#20803;&#23398;&#20064;&#25216;&#26415;&#26469;&#25913;&#36827;MAE&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#19981;&#21516;&#35821;&#35328;&#24418;&#24335;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#21508;&#31181;&#35821;&#35328;&#24418;&#24335;&#20013;&#20855;&#26377;&#23454;&#38469;&#37325;&#35201;&#24615;&#65292;&#20294;&#36817;&#26399;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#23569;&#25968;&#32463;&#36807;&#31934;&#36873;&#30340;&#39046;&#22495;&#65292;&#22914;&#35270;&#35273;&#21644;&#35821;&#35328;&#65292;&#24182;&#19988;&#24120;&#24120;&#20381;&#36182;&#20110;&#29305;&#23450;&#39046;&#22495;&#30340;&#30693;&#35782;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#33945;&#29256;&#33258;&#32534;&#30721;&#22120;&#65288;MAE&#65289;&#20316;&#20026;&#19968;&#20010;&#32479;&#19968;&#30340;&#12289;&#19982;&#35821;&#35328;&#24418;&#24335;&#26080;&#20851;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#36827;&#34892;&#20102;&#25299;&#23637;&#12290;&#25105;&#20204;&#35748;&#20026;&#20803;&#23398;&#20064;&#26159;&#35299;&#37322;MAE&#20316;&#20026;&#19982;&#35821;&#35328;&#24418;&#24335;&#26080;&#20851;&#23398;&#20064;&#22120;&#30340;&#20851;&#38190;&#65292;&#24182;&#20174;&#20849;&#21516;&#25552;&#39640;&#20854;&#22312;&#22810;&#31181;&#35821;&#35328;&#24418;&#24335;&#19978;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#33021;&#21147;&#30340;&#21160;&#26426;&#20986;&#21457;&#65292;&#25552;&#20986;&#20102;MetaMAE&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#23558;MAE&#30340;&#33945;&#29256;&#37325;&#26500;&#35270;&#20026;&#19968;&#20010;&#20803;&#23398;&#20064;&#20219;&#21153;&#65306;&#36890;&#36807;&#23545;&#26410;&#33945;&#29256;&#26631;&#35760;&#36827;&#34892;&#33258;&#36866;&#24212;&#26469;&#39044;&#27979;&#33945;&#29256;&#26631;&#35760;&#65292;&#20174;&#32780;&#36890;&#36807;&#36716;&#25442;&#22120;&#20803;&#23398;&#20064;&#23454;&#29616;&#23545;&#20854;&#36827;&#34892;&#24635;&#35823;&#24046;&#20943;&#23567;&#12290;&#22522;&#20110;&#36825;&#19968;&#26032;&#30340;&#35299;&#37322;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;&#20004;&#31181;&#39640;&#32423;&#20803;&#23398;&#20064;&#25216;&#26415;&#32467;&#21512;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite its practical importance across a wide range of modalities, recent advances in self-supervised learning (SSL) have been primarily focused on a few well-curated domains, e.g., vision and language, often relying on their domain-specific knowledge. For example, Masked Auto-Encoder (MAE) has become one of the popular architectures in these domains, but less has explored its potential in other modalities. In this paper, we develop MAE as a unified, modality-agnostic SSL framework. In turn, we argue meta-learning as a key to interpreting MAE as a modality-agnostic learner, and propose enhancements to MAE from the motivation to jointly improve its SSL across diverse modalities, coined MetaMAE as a result. Our key idea is to view the mask reconstruction of MAE as a meta-learning task: masked tokens are predicted by adapting the Transformer meta-learner through the amortization of unmasked tokens. Based on this novel interpretation, we propose to integrate two advanced meta-learning tec
&lt;/p&gt;</description></item><item><title>Sum-of-Parts&#27169;&#22411;&#36890;&#36807;&#26500;&#36896;&#20445;&#35777;&#29305;&#24449;&#32452;&#24402;&#22240;&#30340;&#24544;&#23454;&#24615;&#65292;&#23558;&#39044;&#27979;&#20998;&#35299;&#20026;&#21487;&#35299;&#37322;&#30340;&#20998;&#25968;&#20043;&#21644;&#65292;&#24110;&#21161;&#22825;&#20307;&#29289;&#29702;&#23398;&#23478;&#21457;&#29616;&#20102;&#20851;&#20110;&#26143;&#31995;&#24418;&#25104;&#30340;&#26032;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2310.16316</link><description>&lt;p&gt;
Sum-of-Parts&#27169;&#22411;&#65306;&#23545;&#29305;&#24449;&#32452;&#30340;&#24544;&#23454;&#24402;&#22240;
&lt;/p&gt;
&lt;p&gt;
Sum-of-Parts Models: Faithful Attributions for Groups of Features. (arXiv:2310.16316v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16316
&lt;/p&gt;
&lt;p&gt;
Sum-of-Parts&#27169;&#22411;&#36890;&#36807;&#26500;&#36896;&#20445;&#35777;&#29305;&#24449;&#32452;&#24402;&#22240;&#30340;&#24544;&#23454;&#24615;&#65292;&#23558;&#39044;&#27979;&#20998;&#35299;&#20026;&#21487;&#35299;&#37322;&#30340;&#20998;&#25968;&#20043;&#21644;&#65292;&#24110;&#21161;&#22825;&#20307;&#29289;&#29702;&#23398;&#23478;&#21457;&#29616;&#20102;&#20851;&#20110;&#26143;&#31995;&#24418;&#25104;&#30340;&#26032;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#26524;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35299;&#37322;&#20934;&#30830;&#21453;&#26144;&#20102;&#20854;&#20915;&#31574;&#36807;&#31243;&#65292;&#21017;&#34987;&#35748;&#20026;&#26159;&#8220;&#24544;&#23454;&#8221;&#30340;&#35299;&#37322;&#12290;&#28982;&#32780;&#65292;&#20363;&#22914;&#28145;&#24230;&#23398;&#20064;&#30340;&#29305;&#24449;&#24402;&#22240;&#31561;&#35299;&#37322;&#24182;&#19981;&#33021;&#20445;&#35777;&#24544;&#23454;&#65292;&#26377;&#21487;&#33021;&#20135;&#29983;&#20855;&#26377;&#35823;&#23548;&#24615;&#30340;&#35299;&#37322;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;Sum-of-Parts&#65288;SOP&#65289;&#27169;&#22411;&#65292;&#23427;&#26159;&#19968;&#31867;&#27169;&#22411;&#65292;&#20854;&#39044;&#27979;&#20855;&#26377;&#36890;&#36807;&#26500;&#36896;&#20445;&#35777;&#24544;&#23454;&#30340;&#29305;&#24449;&#32452;&#24402;&#22240;&#12290;&#35813;&#27169;&#22411;&#23558;&#39044;&#27979;&#20998;&#35299;&#20026;&#21487;&#35299;&#37322;&#30340;&#20998;&#25968;&#20043;&#21644;&#65292;&#27599;&#20010;&#20998;&#25968;&#30452;&#25509;&#24402;&#22240;&#20110;&#19968;&#32452;&#31232;&#30095;&#29305;&#24449;&#12290;&#25105;&#20204;&#20351;&#29992;&#26631;&#20934;&#21487;&#35299;&#37322;&#24615;&#25351;&#26631;&#23545;SOP&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#22312;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#20013;&#65292;&#21033;&#29992;SOP&#25552;&#20379;&#30340;&#24544;&#23454;&#35299;&#37322;&#24110;&#21161;&#22825;&#20307;&#29289;&#29702;&#23398;&#23478;&#21457;&#29616;&#20102;&#20851;&#20110;&#26143;&#31995;&#24418;&#25104;&#30340;&#26032;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
An explanation of a machine learning model is considered "faithful" if it accurately reflects the model's decision-making process. However, explanations such as feature attributions for deep learning are not guaranteed to be faithful, and can produce potentially misleading interpretations. In this work, we develop Sum-of-Parts (SOP), a class of models whose predictions come with grouped feature attributions that are faithful-by-construction. This model decomposes a prediction into an interpretable sum of scores, each of which is directly attributable to a sparse group of features. We evaluate SOP on benchmarks with standard interpretability metrics, and in a case study, we use the faithful explanations from SOP to help astrophysicists discover new knowledge about galaxy formation.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#20363;&#21270;&#32447;&#24615;&#21270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#36890;&#36807;&#32473;&#27169;&#22411;&#20869;&#37096;&#30340;&#27599;&#20010;&#36755;&#20837;&#29305;&#24449;&#20998;&#37197;&#37325;&#35201;&#24471;&#20998;&#65292;&#25581;&#31034;&#20102;&#27169;&#22411;&#22914;&#20309;&#20351;&#29992;&#29305;&#24449;&#20570;&#20986;&#20915;&#31574;&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#21161;&#20110;&#35299;&#20915;&#24403;&#21069;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#20013;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#39640;&#20102;&#27169;&#22411;&#35299;&#37322;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.16295</link><description>&lt;p&gt;
&#23545;&#20110;&#27169;&#22411;&#35299;&#37322;&#30340;&#31070;&#32463;&#32593;&#32476;&#23454;&#20363;&#21270;&#32447;&#24615;&#21270;
&lt;/p&gt;
&lt;p&gt;
Instance-wise Linearization of Neural Network for Model Interpretation. (arXiv:2310.16295v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16295
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#20363;&#21270;&#32447;&#24615;&#21270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#36890;&#36807;&#32473;&#27169;&#22411;&#20869;&#37096;&#30340;&#27599;&#20010;&#36755;&#20837;&#29305;&#24449;&#20998;&#37197;&#37325;&#35201;&#24471;&#20998;&#65292;&#25581;&#31034;&#20102;&#27169;&#22411;&#22914;&#20309;&#20351;&#29992;&#29305;&#24449;&#20570;&#20986;&#20915;&#31574;&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#21161;&#20110;&#35299;&#20915;&#24403;&#21069;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#20013;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#39640;&#20102;&#27169;&#22411;&#35299;&#37322;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#22312;&#35768;&#22810;&#31185;&#23398;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#20173;&#28982;&#26159;&#23558;&#36825;&#31181;&#25216;&#26415;&#24212;&#29992;&#20110;&#25105;&#20204;&#26085;&#24120;&#29983;&#27963;&#30340;&#20027;&#35201;&#29942;&#39048;&#12290;&#25361;&#25112;&#22312;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#38750;&#32447;&#24615;&#34892;&#20026;&#65292;&#36825;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#38190;&#24615;&#38382;&#39064;&#65292;&#21363;&#27169;&#22411;&#22914;&#20309;&#20351;&#29992;&#36755;&#20837;&#29305;&#24449;&#36827;&#34892;&#20915;&#31574;&#12290;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#30340;&#32463;&#20856;&#26041;&#27861;&#26159;&#29305;&#24449;&#24402;&#22240;&#65292;&#23427;&#20026;&#27599;&#20010;&#36755;&#20837;&#29305;&#24449;&#20998;&#37197;&#19968;&#20010;&#37325;&#35201;&#24471;&#20998;&#65292;&#24182;&#25581;&#31034;&#20854;&#23545;&#24403;&#21069;&#39044;&#27979;&#30340;&#37325;&#35201;&#24615;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#32463;&#24120;&#25351;&#31034;&#27599;&#20010;&#36755;&#20837;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#65292;&#32780;&#27809;&#26377;&#35814;&#32454;&#35828;&#26126;&#23427;&#20204;&#22312;&#27169;&#22411;&#20869;&#37096;&#23454;&#38469;&#19978;&#26159;&#22914;&#20309;&#22788;&#29702;&#30340;&#12290;&#36825;&#20123;&#24402;&#22240;&#26041;&#27861;&#24120;&#24120;&#24341;&#21457;&#19968;&#20010;&#20851;&#27880;&#28857;&#65292;&#21363;&#23427;&#20204;&#26159;&#21542;&#27491;&#30830;&#22320;&#24378;&#35843;&#20102;&#27169;&#22411;&#39044;&#27979;&#30340;&#29305;&#24449;&#12290;&#23545;&#20110;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#38750;&#32447;&#24615;&#34892;&#20026;&#36890;&#24120;&#26159;&#30001;&#27169;&#22411;&#30340;&#38750;&#32447;&#24615;&#28608;&#27963;&#21333;&#20803;&#24341;&#36215;&#30340;&#12290;&#28982;&#32780;&#65292;&#39044;&#27979;&#30340;&#35745;&#31639;&#34892;&#20026;&#24448;&#24448;&#26159;&#22797;&#26434;&#30340;&#65292;&#36825;&#20351;&#24471;&#35299;&#37322;&#21644;&#29702;&#35299;&#27169;&#22411;&#30340;&#20915;&#31574;&#21464;&#24471;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural network have achieved remarkable successes in many scientific fields. However, the interpretability of the neural network model is still a major bottlenecks to deploy such technique into our daily life. The challenge can dive into the non-linear behavior of the neural network, which rises a critical question that how a model use input feature to make a decision. The classical approach to address this challenge is feature attribution, which assigns an important score to each input feature and reveal its importance of current prediction. However, current feature attribution approaches often indicate the importance of each input feature without detail of how they are actually processed by a model internally. These attribution approaches often raise a concern that whether they highlight correct features for a model prediction.  For a neural network model, the non-linear behavior is often caused by non-linear activation units of a model. However, the computation behavior of a predict
&lt;/p&gt;</description></item><item><title>XFEVER &#25968;&#25454;&#38598;&#26159;&#20026;&#20102;&#22312;&#19981;&#21516;&#35821;&#35328;&#20013;&#23545;&#20107;&#23454;&#39564;&#35777;&#27169;&#22411;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#32780;&#35774;&#35745;&#30340;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#26500;&#24314;&#19981;&#21516;&#35821;&#35328;&#30340;&#20107;&#23454;&#39564;&#35777;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.16278</link><description>&lt;p&gt;
XFEVER: &#36328;&#35821;&#35328;&#20107;&#23454;&#39564;&#35777;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
XFEVER: Exploring Fact Verification across Languages. (arXiv:2310.16278v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16278
&lt;/p&gt;
&lt;p&gt;
XFEVER &#25968;&#25454;&#38598;&#26159;&#20026;&#20102;&#22312;&#19981;&#21516;&#35821;&#35328;&#20013;&#23545;&#20107;&#23454;&#39564;&#35777;&#27169;&#22411;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#32780;&#35774;&#35745;&#30340;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#26500;&#24314;&#19981;&#21516;&#35821;&#35328;&#30340;&#20107;&#23454;&#39564;&#35777;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#36866;&#29992;&#20110;&#36328;&#35821;&#35328;&#20107;&#23454;&#39564;&#35777;&#27169;&#22411;&#22522;&#20934;&#27979;&#35797;&#30340;Cross-lingual Fact Extraction and VERification (XFEVER)&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;Fact Extraction and VERification (FEVER)&#25968;&#25454;&#38598;&#30340;&#20027;&#24352;&#21644;&#35777;&#25454;&#25991;&#26412;&#32763;&#35793;&#25104;&#20845;&#31181;&#35821;&#35328;&#26469;&#26500;&#24314;&#23427;&#12290;&#35757;&#32451;&#38598;&#21644;&#24320;&#21457;&#38598;&#20351;&#29992;&#26426;&#22120;&#32763;&#35793;&#36827;&#34892;&#32763;&#35793;&#65292;&#32780;&#27979;&#35797;&#38598;&#21253;&#25324;&#30001;&#19987;&#19994;&#32763;&#35793;&#20154;&#21592;&#21644;&#26426;&#22120;&#32763;&#35793;&#29983;&#25104;&#30340;&#25991;&#26412;&#12290;&#20351;&#29992;XFEVER&#25968;&#25454;&#38598;&#65292;&#26412;&#25991;&#23450;&#20041;&#20102;&#20004;&#31181;&#36328;&#35821;&#35328;&#20107;&#23454;&#39564;&#35777;&#22330;&#26223;&#65292;&#21363;&#38646;&#26679;&#26412;&#23398;&#20064;&#21644;&#32763;&#35793;&#35757;&#32451;&#23398;&#20064;&#65292;&#24182;&#38024;&#23545;&#27599;&#20010;&#22330;&#26223;&#25552;&#20986;&#20102;&#22522;&#20934;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#26500;&#24314;&#19981;&#21516;&#35821;&#35328;&#30340;&#20107;&#23454;&#39564;&#35777;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#24615;&#33021;&#22312;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#26377;&#24046;&#24322;&#65292;&#24182;&#19988;&#22312;&#33521;&#35821;&#24773;&#20917;&#19979;&#31245;&#36874;&#19968;&#31609;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#25105;&#20204;&#21487;&#20197;&#26377;&#25928;&#22320;&#20943;&#36731;&#27169;&#22411;&#30340;&#35823;&#24046;&#26657;&#20934;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces the Cross-lingual Fact Extraction and VERification (XFEVER) dataset designed for benchmarking the fact verification models across different languages. We constructed it by translating the claim and evidence texts of the Fact Extraction and VERification (FEVER) dataset into six languages. The training and development sets were translated using machine translation, whereas the test set includes texts translated by professional translators and machine-translated texts. Using the XFEVER dataset, two cross-lingual fact verification scenarios, zero-shot learning and translate-train learning, are defined, and baseline models for each scenario are also proposed in this paper. Experimental results show that the multilingual language model can be used to build fact verification models in different languages efficiently. However, the performance varies by language and is somewhat inferior to the English case. We also found that we can effectively mitigate model miscalibratio
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23398;&#20064;&#32593;&#32476;&#21442;&#25968;&#30340;&#39046;&#22495;&#19981;&#21464;&#21518;&#39564;&#20998;&#24067;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PosTerior Generalization&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#27867;&#21270;&#21040;&#26410;&#35265;&#36807;&#30340;&#30446;&#26631;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2310.16277</link><description>&lt;p&gt;
Bayesian&#39046;&#22495;&#19981;&#21464;&#23398;&#20064;&#36890;&#36807;&#21442;&#25968;&#20998;&#24067;&#30340;&#21518;&#39564;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Bayesian Domain Invariant Learning via Posterior Generalization of Parameter Distributions. (arXiv:2310.16277v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16277
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23398;&#20064;&#32593;&#32476;&#21442;&#25968;&#30340;&#39046;&#22495;&#19981;&#21464;&#21518;&#39564;&#20998;&#24067;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PosTerior Generalization&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#27867;&#21270;&#21040;&#26410;&#35265;&#36807;&#30340;&#30446;&#26631;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#19981;&#21464;&#23398;&#20064;&#26088;&#22312;&#23398;&#20064;&#33021;&#22815;&#25552;&#21462;&#21508;&#31181;&#35757;&#32451;&#39046;&#22495;&#20013;&#19981;&#21464;&#29305;&#24449;&#30340;&#27169;&#22411;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#27867;&#21270;&#21040;&#26410;&#35265;&#36807;&#30340;&#30446;&#26631;&#39046;&#22495;&#12290;&#26368;&#36817;&#65292;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#22312;&#39046;&#22495;&#19981;&#21464;&#23398;&#20064;&#26041;&#38754;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#65292;&#20294;&#22823;&#22810;&#25968;&#30740;&#31350;&#38598;&#20013;&#22312;&#23545;&#40784;&#29305;&#24449;&#20998;&#24067;&#32780;&#19981;&#26159;&#21442;&#25968;&#20998;&#24067;&#12290;&#21463;&#21040;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#21407;&#29702;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#35797;&#22270;&#30452;&#25509;&#23398;&#20064;&#32593;&#32476;&#21442;&#25968;&#30340;&#39046;&#22495;&#19981;&#21464;&#21518;&#39564;&#20998;&#24067;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23450;&#29702;&#65292;&#34920;&#26126;&#21487;&#20197;&#36890;&#36807;&#32858;&#21512;&#19981;&#21516;&#35757;&#32451;&#39046;&#22495;&#19978;&#30340;&#21518;&#39564;&#26469;&#38544;&#24335;&#25512;&#26029;&#21442;&#25968;&#30340;&#19981;&#21464;&#21518;&#39564;&#12290;&#25105;&#20204;&#30340;&#20551;&#35774;&#26356;&#20855;&#23485;&#26494;&#24615;&#65292;&#21487;&#20197;&#25552;&#21462;&#26356;&#22810;&#30340;&#39046;&#22495;&#19981;&#21464;&#20449;&#24687;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"PosTerior Generalization (PTG)"&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#19981;&#21464;&#30340;&#21442;&#25968;&#20998;&#24067;&#12290;PTG&#20805;&#20998;&#21033;&#29992;&#20102;&#21464;&#20998;&#25512;&#26029;&#26469;&#36817;&#20284;&#21442;&#25968;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain invariant learning aims to learn models that extract invariant features over various training domains, resulting in better generalization to unseen target domains. Recently, Bayesian Neural Networks have achieved promising results in domain invariant learning, but most works concentrate on aligning features distributions rather than parameter distributions. Inspired by the principle of Bayesian Neural Network, we attempt to directly learn the domain invariant posterior distribution of network parameters. We first propose a theorem to show that the invariant posterior of parameters can be implicitly inferred by aggregating posteriors on different training domains. Our assumption is more relaxed and allows us to extract more domain invariant information. We also propose a simple yet effective method, named PosTerior Generalization (PTG), that can be used to estimate the invariant parameter distribution. PTG fully exploits variational inference to approximate parameter distribution
&lt;/p&gt;</description></item><item><title>CycleAlign&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#28860;&#23545;&#40784;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#36845;&#20195;&#25552;&#28860;&#23454;&#29616;&#23545;&#40657;&#30418;&#27169;&#22411;&#21040;&#30333;&#30418;&#27169;&#22411;&#30340;&#36716;&#21464;&#65292;&#35299;&#20915;&#20102;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20215;&#20540;&#23545;&#40784;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.16271</link><description>&lt;p&gt;
CycleAlign: &#20174;&#40657;&#30418;&#35821;&#35328;&#27169;&#22411;&#21040;&#30333;&#30418;&#27169;&#22411;&#36827;&#34892;&#36845;&#20195;&#25552;&#28860;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#20154;&#31867;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
CycleAlign: Iterative Distillation from Black-box LLM to White-box Models for Better Human Alignment. (arXiv:2310.16271v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16271
&lt;/p&gt;
&lt;p&gt;
CycleAlign&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#28860;&#23545;&#40784;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#36845;&#20195;&#25552;&#28860;&#23454;&#29616;&#23545;&#40657;&#30418;&#27169;&#22411;&#21040;&#30333;&#30418;&#27169;&#22411;&#30340;&#36716;&#21464;&#65292;&#35299;&#20915;&#20102;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20215;&#20540;&#23545;&#40784;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#20250;&#29983;&#25104;&#26377;&#23475;&#12289;&#26377;&#27602;&#25110;&#19982;&#20154;&#31867;&#20559;&#22909;&#30456;&#24726;&#30340;&#20869;&#23481;&#65292;&#20351;&#24471;&#20854;&#19982;&#20154;&#31867;&#20215;&#20540;&#30340;&#23545;&#40784;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#23545;&#40784;&#30340;&#26041;&#27861;&#65288;&#22914;PPO&#65289;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#26041;&#27861;&#65292;&#20294;&#24448;&#24448;&#22797;&#26434;&#12289;&#19981;&#31283;&#23450;&#19988;&#36164;&#28304;&#23494;&#38598;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#25490;&#21517;&#30340;&#23545;&#40784;&#26041;&#27861;&#24050;&#32463;&#20986;&#29616;&#65292;&#36890;&#36807;&#29992;&#30417;&#30563;&#24494;&#35843;&#26367;&#25442;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#25552;&#20379;&#31283;&#23450;&#24615;&#21644;&#26377;&#25928;&#24615;&#65292;&#20294;&#30001;&#20110;&#38656;&#35201;&#24102;&#27880;&#37322;&#30340;&#25968;&#25454;&#65292;&#23427;&#20204;&#30340;&#25104;&#26412;&#36739;&#39640;&#12290;&#32771;&#34385;&#21040;&#29616;&#26377;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;ChatGPT&#65289;&#24050;&#32463;&#30456;&#23545;&#36739;&#22909;&#22320;&#23545;&#40784;&#24182;&#19988;&#25104;&#26412;&#36739;&#20302;&#65292;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#24320;&#22987;&#20174;AI&#21453;&#39304;&#20013;&#23545;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#36827;&#34892;&#23545;&#40784;&#12290;&#29616;&#26377;&#30340;&#24120;&#35268;&#23454;&#36341;&#20165;&#20165;&#20174;LLMs&#25552;&#28860;&#20986;&#36981;&#24490;&#25351;&#20196;&#30340;&#21709;&#24212;&#65292;&#21463;&#21040;&#20102;&#29942;&#39048;&#30340;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;CycleAlign&#26469;&#20174;&#21442;&#25968;&#38750;&#21487;&#35265;&#30340;&#27169;&#22411;&#20013;&#25552;&#28860;&#23545;&#40784;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models trained on large-scale corpus often generate content that is harmful, toxic, or contrary to human preferences, making their alignment with human values a critical concern. Reinforcement learning from human feedback (RLHF) with algorithms like PPO is a prevalent approach for alignment but is often complex, unstable, and resource-intensive. Recently, ranking-based alignment methods have emerged, offering stability and effectiveness by replacing the RL framework with supervised fine-tuning, but they are costly due to the need for annotated data. Considering that existing large language models (LLMs) like ChatGPT are already relatively well-aligned and cost-friendly, researchers have begun to align the language model with human preference from AI feedback. The common practices, which unidirectionally distill the instruction-following responses from LLMs, are constrained by their bottleneck. Thus we introduce CycleAlign to distill alignment capabilities from parameter-invisi
&lt;/p&gt;</description></item><item><title>Attention Lens&#26159;&#19968;&#31181;&#24037;&#20855;&#65292;&#23427;&#33021;&#22815;&#36890;&#36807;&#23398;&#20064;&#30340;&#27880;&#24847;&#21147;&#22836;&#29305;&#23450;&#36716;&#25442;&#23558;&#27880;&#24847;&#21147;&#22836;&#30340;&#36755;&#20986;&#32763;&#35793;&#20026;&#35789;&#27719;&#26631;&#35760;&#12290;&#20351;&#29992;Attention Lens&#65292;&#25105;&#20204;&#21487;&#20197;&#35299;&#37322;&#27880;&#24847;&#21147;&#22836;&#22312;&#29983;&#25104;&#26368;&#32456;&#26631;&#35760;&#39044;&#27979;&#20013;&#30340;&#20316;&#29992;&#12290;&#27880;&#24847;&#21147;&#22836;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#25198;&#28436;&#30528;&#39640;&#24230;&#19987;&#38376;&#21270;&#30340;&#35282;&#33394;&#12290;</title><link>http://arxiv.org/abs/2310.16270</link><description>&lt;p&gt;
&#27880;&#24847;&#21147;&#38236;&#22836;&#65306;&#19968;&#31181;&#35299;&#37322;&#27880;&#24847;&#21147;&#22836;&#20449;&#24687;&#26816;&#32034;&#26426;&#21046;&#30340;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Attention Lens: A Tool for Mechanistically Interpreting the Attention Head Information Retrieval Mechanism. (arXiv:2310.16270v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16270
&lt;/p&gt;
&lt;p&gt;
Attention Lens&#26159;&#19968;&#31181;&#24037;&#20855;&#65292;&#23427;&#33021;&#22815;&#36890;&#36807;&#23398;&#20064;&#30340;&#27880;&#24847;&#21147;&#22836;&#29305;&#23450;&#36716;&#25442;&#23558;&#27880;&#24847;&#21147;&#22836;&#30340;&#36755;&#20986;&#32763;&#35793;&#20026;&#35789;&#27719;&#26631;&#35760;&#12290;&#20351;&#29992;Attention Lens&#65292;&#25105;&#20204;&#21487;&#20197;&#35299;&#37322;&#27880;&#24847;&#21147;&#22836;&#22312;&#29983;&#25104;&#26368;&#32456;&#26631;&#35760;&#39044;&#27979;&#20013;&#30340;&#20316;&#29992;&#12290;&#27880;&#24847;&#21147;&#22836;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#25198;&#28436;&#30528;&#39640;&#24230;&#19987;&#38376;&#21270;&#30340;&#35282;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26159;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#23581;&#35797;&#36890;&#36807;&#36870;&#21521;&#24037;&#31243;&#32447;&#24615;&#23618;&#30340;&#20316;&#29992;&#65292;&#35299;&#30721;LLMs&#20026;&#25991;&#26412;&#23436;&#25104;&#20219;&#21153;&#20570;&#20986;&#26368;&#32456;&#39044;&#27979;&#30340;&#20869;&#37096;&#26426;&#21046;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#27880;&#24847;&#21147;&#22836;&#22312;&#29983;&#25104;&#26368;&#32456;&#26631;&#35760;&#39044;&#27979;&#20013;&#30340;&#20855;&#20307;&#20316;&#29992;&#36824;&#30693;&#20043;&#29978;&#23569;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Attention Lens&#65292;&#19968;&#20010;&#24037;&#20855;&#65292;&#21487;&#20197;&#36890;&#36807;&#23398;&#20064;&#30340;&#27880;&#24847;&#21147;&#22836;&#29305;&#23450;&#36716;&#25442;(&#31216;&#20026;&#38236;&#22836;)&#23558;&#27880;&#24847;&#21147;&#22836;&#30340;&#36755;&#20986;&#32763;&#35793;&#20026;&#35789;&#27719;&#26631;&#35760;&#12290;&#25105;&#20204;&#35757;&#32451;&#30340;&#38236;&#22836;&#30340;&#21021;&#27493;&#21457;&#29616;&#34920;&#26126;&#65292;&#27880;&#24847;&#21147;&#22836;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#25198;&#28436;&#30528;&#39640;&#24230;&#19987;&#38376;&#21270;&#30340;&#35282;&#33394;&#12290;Attention Lens&#30340;&#20195;&#30721;&#21487;&#22312;github.com/msakarvadia/AttentionLens&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based Large Language Models (LLMs) are the state-of-the-art for natural language tasks. Recent work has attempted to decode, by reverse engineering the role of linear layers, the internal mechanisms by which LLMs arrive at their final predictions for text completion tasks. Yet little is known about the specific role of attention heads in producing the final token prediction. We propose Attention Lens, a tool that enables researchers to translate the outputs of attention heads into vocabulary tokens via learned attention-head-specific transformations called lenses. Preliminary findings from our trained lenses indicate that attention heads play highly specialized roles in language models. The code for Attention Lens is available at github.com/msakarvadia/AttentionLens.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#30495;&#23454;&#26032;&#38395;&#26426;&#26500;&#30340;&#35780;&#32423;&#26469;&#21019;&#24314;&#19968;&#20010;&#22810;&#35821;&#35328;&#26032;&#38395;&#35821;&#26009;&#24211;&#65292;&#20854;&#20013;&#21253;&#25324;&#31895;&#30053;&#30340;&#31435;&#22330;&#27880;&#37322;&#21644;&#33258;&#21160;&#25552;&#21462;&#30340;&#20027;&#39064;&#27880;&#37322;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#27492;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#33021;&#22815;&#30830;&#23450;&#22823;&#22810;&#25968;&#26410;&#35265;&#25253;&#32440;&#30340;&#31038;&#35770;&#31435;&#22330;&#12290;</title><link>http://arxiv.org/abs/2310.16269</link><description>&lt;p&gt;
&#23186;&#20307;&#30340;&#22810;&#35821;&#35328;&#31895;&#30053;&#25919;&#27835;&#31435;&#22330;&#20998;&#31867;&#65306;ChatGPT&#21644;Bard&#25253;&#32440;&#30340;&#31038;&#35770;&#31435;&#22330;
&lt;/p&gt;
&lt;p&gt;
Multilingual Coarse Political Stance Classification of Media. The Editorial Line of a ChatGPT and Bard Newspaper. (arXiv:2310.16269v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16269
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#30495;&#23454;&#26032;&#38395;&#26426;&#26500;&#30340;&#35780;&#32423;&#26469;&#21019;&#24314;&#19968;&#20010;&#22810;&#35821;&#35328;&#26032;&#38395;&#35821;&#26009;&#24211;&#65292;&#20854;&#20013;&#21253;&#25324;&#31895;&#30053;&#30340;&#31435;&#22330;&#27880;&#37322;&#21644;&#33258;&#21160;&#25552;&#21462;&#30340;&#20027;&#39064;&#27880;&#37322;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#27492;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#33021;&#22815;&#30830;&#23450;&#22823;&#22810;&#25968;&#26410;&#35265;&#25253;&#32440;&#30340;&#31038;&#35770;&#31435;&#22330;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20013;&#31435;&#24615;&#22312;&#25919;&#27835;&#20013;&#24456;&#38590;&#23454;&#29616;&#65292;&#20063;&#26159;&#20027;&#35266;&#30340;&#12290;&#20256;&#32479;&#23186;&#20307;&#36890;&#24120;&#37319;&#21462;&#19968;&#31181;&#31038;&#35770;&#31435;&#22330;&#65292;&#36825;&#21487;&#20197;&#20316;&#20026;&#28508;&#22312;&#35835;&#32773;&#29992;&#26469;&#21028;&#26029;&#23186;&#20307;&#20559;&#35265;&#30340;&#25351;&#26631;&#12290;&#30446;&#21069;&#26377;&#20960;&#20010;&#24179;&#21488;&#26681;&#25454;&#25919;&#27835;&#20559;&#35265;&#23545;&#26032;&#38395;&#26426;&#26500;&#36827;&#34892;&#35780;&#32423;&#12290;&#31038;&#35770;&#31435;&#22330;&#21644;&#35780;&#32423;&#26377;&#21161;&#20110;&#35835;&#32773;&#33719;&#24471;&#19968;&#31181;&#24179;&#34913;&#30340;&#26032;&#38395;&#35266;&#28857;&#12290;&#20294;&#26159;&#38543;&#30528;&#25351;&#20196;&#36981;&#24490;&#22411;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#22914;&#20309;&#22312;&#19981;&#26045;&#21152;&#20559;&#35265;&#30340;&#24773;&#20917;&#19979;&#65292;&#20154;&#24037;&#26234;&#33021;&#26032;&#38395;&#26426;&#26500;&#20250;&#20301;&#20110;&#20309;&#22788;&#30340;&#20559;&#35265;&#35780;&#32423;&#65311;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#30495;&#23454;&#26032;&#38395;&#26426;&#26500;&#30340;&#35780;&#32423;&#26469;&#21019;&#24314;&#19968;&#20010;&#21253;&#21547;&#31895;&#30053;&#31435;&#22330;&#27880;&#37322;&#65288;&#24038;&#32764;&#21644;&#21491;&#32764;&#65289;&#20197;&#21450;&#33258;&#21160;&#25552;&#21462;&#30340;&#20027;&#39064;&#27880;&#37322;&#30340;&#22810;&#35821;&#35328;&#26032;&#38395;&#35821;&#26009;&#24211;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#27492;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#33021;&#22815;&#35782;&#21035;&#20986;&#33521;&#35821;&#12289;&#24503;&#35821;&#12289;&#35199;&#29677;&#29273;&#35821;&#21644;&#21152;&#27888;&#32599;&#23612;&#20122;&#35821;&#30340;&#22823;&#22810;&#25968;&#26410;&#35265;&#25253;&#32440;&#30340;&#31038;&#35770;&#31435;&#22330;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#20998;&#31867;&#22120;&#24212;&#29992;&#20110;101&#20221;&#31867;&#20284;&#25253;&#32440;&#30340;&#26032;&#38395;&#21002;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neutrality is difficult to achieve and, in politics, subjective. Traditional media typically adopt an editorial line that can be used by their potential readers as an indicator of the media bias. Several platforms currently rate news outlets according to their political bias. The editorial line and the ratings help readers in gathering a balanced view of news. But in the advent of instruction-following language models, tasks such as writing a newspaper article can be delegated to computers. Without imposing a biased persona, where would an AI-based news outlet lie within the bias ratings? In this work, we use the ratings of authentic news outlets to create a multilingual corpus of news with coarse stance annotations (Left and Right) along with automatically extracted topic annotations. We show that classifiers trained on this data are able to identify the editorial line of most unseen newspapers in English, German, Spanish and Catalan. We then apply the classifiers to 101 newspaper-lik
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#38024;&#23545;&#23433;&#20840;&#20195;&#30721;&#29983;&#25104;&#30340;&#32508;&#21512;&#30740;&#31350;&#65292;&#36890;&#36807;&#20351;&#29992;&#32463;&#36807;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#35780;&#20272;&#21644;&#22686;&#24378;&#20195;&#30721;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#20351;&#29992;&#26410;&#32463;&#28040;&#27602;&#30340;&#24320;&#28304;&#25968;&#25454;&#35757;&#32451;&#27169;&#22411;&#24341;&#20837;&#23433;&#20840;&#28431;&#27934;&#30340;&#39118;&#38505;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#29616;&#26377;&#27169;&#22411;&#22312;&#23433;&#20840;&#26041;&#38754;&#24120;&#24120;&#34987;&#24573;&#35270;&#12290;</title><link>http://arxiv.org/abs/2310.16263</link><description>&lt;p&gt;
&#25552;&#21319;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#23433;&#20840;&#20195;&#30721;&#29983;&#25104;&#65306;&#22522;&#20110;&#25968;&#25454;&#38598;&#39537;&#21160;&#30340;&#28431;&#27934;&#32531;&#35299;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Enhancing Large Language Models for Secure Code Generation: A Dataset-driven Study on Vulnerability Mitigation. (arXiv:2310.16263v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16263
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#38024;&#23545;&#23433;&#20840;&#20195;&#30721;&#29983;&#25104;&#30340;&#32508;&#21512;&#30740;&#31350;&#65292;&#36890;&#36807;&#20351;&#29992;&#32463;&#36807;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#35780;&#20272;&#21644;&#22686;&#24378;&#20195;&#30721;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#20351;&#29992;&#26410;&#32463;&#28040;&#27602;&#30340;&#24320;&#28304;&#25968;&#25454;&#35757;&#32451;&#27169;&#22411;&#24341;&#20837;&#23433;&#20840;&#28431;&#27934;&#30340;&#39118;&#38505;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#29616;&#26377;&#27169;&#22411;&#22312;&#23433;&#20840;&#26041;&#38754;&#24120;&#24120;&#34987;&#24573;&#35270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#20195;&#30721;&#29983;&#25104;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#26082;&#26377;&#21033;&#20110;&#26032;&#25163;&#24320;&#21457;&#20154;&#21592;&#65292;&#20063;&#26377;&#21033;&#20110;&#32463;&#39564;&#20016;&#23500;&#30340;&#24320;&#21457;&#20154;&#21592;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20351;&#29992;&#26469;&#33258;&#24320;&#28304;&#20179;&#24211;&#65288;&#22914;GitHub&#65289;&#30340;&#26410;&#32463;&#28040;&#27602;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#20250;&#24341;&#20837;&#24847;&#22806;&#20256;&#25773;&#23433;&#20840;&#28431;&#27934;&#30340;&#39118;&#38505;&#12290;&#20026;&#20102;&#26377;&#25928;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#20174;&#36719;&#20214;&#23433;&#20840;&#30340;&#35282;&#24230;&#36827;&#34892;&#20102;&#19968;&#39033;&#32508;&#21512;&#30740;&#31350;&#65292;&#26088;&#22312;&#35780;&#20272;&#21644;&#22686;&#24378;&#20195;&#30721;LLMs&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;SecuCoGen&#65292;&#19968;&#20010;&#31934;&#24515;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#65292;&#38024;&#23545;21&#31181;&#20851;&#38190;&#28431;&#27934;&#31867;&#22411;&#12290;SecuCoGen&#21253;&#21547;180&#20010;&#26679;&#26412;&#65292;&#24182;&#20316;&#20026;&#36827;&#34892;&#19977;&#20010;&#20851;&#38190;&#30340;&#19982;&#20195;&#30721;&#30456;&#20851;&#20219;&#21153;&#30340;&#23454;&#39564;&#30340;&#22522;&#30784;&#65306;&#20195;&#30721;&#29983;&#25104;&#12289;&#20195;&#30721;&#20462;&#22797;&#21644;&#28431;&#27934;&#20998;&#31867;&#65292;&#20854;&#20013;&#23433;&#20840;&#24615;&#24471;&#21040;&#20102;&#26497;&#22823;&#30340;&#24378;&#35843;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#29616;&#26377;&#27169;&#22411;&#22312;&#22788;&#29702;&#23433;&#20840;&#38382;&#39064;&#26102;&#32463;&#24120;&#34987;&#24573;&#35270;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have brought significant advancements to code generation, benefiting both novice and experienced developers. However, their training using unsanitized data from open-source repositories, like GitHub, introduces the risk of inadvertently propagating security vulnerabilities. To effectively mitigate this concern, this paper presents a comprehensive study focused on evaluating and enhancing code LLMs from a software security perspective. We introduce SecuCoGen\footnote{SecuCoGen has been uploaded as supplemental material and will be made publicly available after publication.}, a meticulously curated dataset targeting 21 critical vulnerability types. SecuCoGen comprises 180 samples and serves as the foundation for conducting experiments on three crucial code-related tasks: code generation, code repair and vulnerability classification, with a strong emphasis on security. Our experimental results reveal that existing models often overlook security concerns during
&lt;/p&gt;</description></item><item><title>rTisane&#26159;&#19968;&#31181;&#20351;&#29992;DSL&#22806;&#37096;&#21270;&#27010;&#24565;&#27169;&#22411;&#30340;&#24037;&#20855;&#65292;&#33021;&#22815;&#24110;&#21161;&#20998;&#26512;&#24072;&#26356;&#28145;&#20837;&#22320;&#21442;&#19982;&#24182;&#20934;&#30830;&#22320;&#34920;&#36798;&#20182;&#20204;&#30340;&#20551;&#35774;&#65292;&#20174;&#32780;&#25552;&#39640;&#32479;&#35745;&#27169;&#22411;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.16262</link><description>&lt;p&gt;
rTisane: &#22806;&#37096;&#21270;&#27010;&#24565;&#27169;&#22411;&#25552;&#39640;&#19982;&#39046;&#22495;&#30693;&#35782;&#30340;&#20114;&#21160;&#24182;&#25552;&#39640;&#32479;&#35745;&#27169;&#22411;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
rTisane: Externalizing conceptual models for data analysis increases engagement with domain knowledge and improves statistical model quality. (arXiv:2310.16262v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16262
&lt;/p&gt;
&lt;p&gt;
rTisane&#26159;&#19968;&#31181;&#20351;&#29992;DSL&#22806;&#37096;&#21270;&#27010;&#24565;&#27169;&#22411;&#30340;&#24037;&#20855;&#65292;&#33021;&#22815;&#24110;&#21161;&#20998;&#26512;&#24072;&#26356;&#28145;&#20837;&#22320;&#21442;&#19982;&#24182;&#20934;&#30830;&#22320;&#34920;&#36798;&#20182;&#20204;&#30340;&#20551;&#35774;&#65292;&#20174;&#32780;&#25552;&#39640;&#32479;&#35745;&#27169;&#22411;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32479;&#35745;&#27169;&#22411;&#24212;&#20934;&#30830;&#21453;&#26144;&#20998;&#26512;&#24072;&#23545;&#21464;&#37327;&#21450;&#20854;&#20851;&#31995;&#30340;&#39046;&#22495;&#30693;&#35782;&#12290;&#23613;&#31649;&#36817;&#26399;&#24037;&#20855;&#20801;&#35768;&#20998;&#26512;&#24072;&#34920;&#36798;&#36825;&#20123;&#20551;&#35774;&#24182;&#20351;&#29992;&#23427;&#20204;&#20135;&#29983;&#32467;&#26524;&#30340;&#32479;&#35745;&#27169;&#22411;&#65292;&#20294;&#26159;&#30446;&#21069;&#19981;&#28165;&#26970;&#20998;&#26512;&#24072;&#24819;&#35201;&#34920;&#36798;&#20160;&#20040;&#20197;&#21450;&#22806;&#37096;&#21270;&#22914;&#20309;&#24433;&#21709;&#32479;&#35745;&#27169;&#22411;&#36136;&#37327;&#12290;&#26412;&#25991;&#22635;&#34917;&#20102;&#36825;&#20123;&#31354;&#30333;&#12290;&#25105;&#20204;&#39318;&#20808;&#36827;&#34892;&#20102;&#19968;&#39033;&#25506;&#32034;&#24615;&#30740;&#31350;&#65292;&#30740;&#31350;&#20998;&#26512;&#24072;&#20351;&#29992;&#29305;&#23450;&#39046;&#22495;&#35821;&#35328;&#65288;DSL&#65289;&#26469;&#34920;&#36798;&#27010;&#24565;&#27169;&#22411;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#20998;&#26512;&#24072;&#26356;&#20542;&#21521;&#20110;&#35814;&#32454;&#25551;&#36848;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#24076;&#26395;&#33021;&#22815;&#20801;&#35768;&#24182;&#31245;&#21518;&#35299;&#20915;&#27010;&#24565;&#27169;&#22411;&#20013;&#30340;&#27495;&#20041;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#21457;&#29616;&#24320;&#21457;&#20102;rTisane&#65292;&#19968;&#31181;&#29992;&#20110;&#34920;&#36798;&#27010;&#24565;&#27169;&#22411;&#30340;DSL&#65292;&#24182;&#37197;&#21512;&#19968;&#20010;&#20132;&#20114;&#24335;&#28040;&#27495;&#36807;&#31243;&#12290;&#22312;&#19968;&#39033;&#21463;&#25511;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;rTisane&#30340;DSL&#33021;&#24110;&#21161;&#20998;&#26512;&#24072;&#26356;&#28145;&#20837;&#22320;&#21442;&#19982;&#24182;&#20934;&#30830;&#22320;&#22806;&#37096;&#21270;&#20182;&#20204;&#30340;&#20551;&#35774;&#12290;rTisane&#36824;&#33021;&#20135;&#29983;&#31526;&#21512;&#20998;&#26512;&#24072;&#20551;&#35774;&#30340;&#32479;&#35745;&#27169;&#22411;&#65292;&#20445;&#25345;&#20998;&#26512;&#24072;&#20551;&#35774;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Statistical models should accurately reflect analysts' domain knowledge about variables and their relationships. While recent tools let analysts express these assumptions and use them to produce a resulting statistical model, it remains unclear what analysts want to express and how externalization impacts statistical model quality. This paper addresses these gaps. We first conduct an exploratory study of analysts using a domain-specific language (DSL) to express conceptual models. We observe a preference for detailing how variables relate and a desire to allow, and then later resolve, ambiguity in their conceptual models. We leverage these findings to develop rTisane, a DSL for expressing conceptual models augmented with an interactive disambiguation process. In a controlled evaluation, we find that rTisane's DSL helps analysts engage more deeply with and accurately externalize their assumptions. rTisane also leads to statistical models that match analysts' assumptions, maintain analys
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22240;&#26524;&#35299;&#32544;&#31163;&#25955;&#22810;&#31890;&#24230;&#22270;&#20998;&#31867;&#26041;&#27861;&#65288;CDM-GNN&#65289;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#35299;&#20915;&#22270;&#25968;&#25454;&#30340;&#22810;&#31890;&#24230;&#29305;&#24615;&#65292;&#23454;&#29616;&#20102;&#23545;&#22270;&#20013;&#37325;&#35201;&#23376;&#32467;&#26500;&#21644;&#20559;&#24046;&#37096;&#20998;&#30340;&#35299;&#26512;&#65292;&#24182;&#29992;&#20110;&#22270;&#20998;&#31867;&#20219;&#21153;&#20013;&#12290;</title><link>http://arxiv.org/abs/2310.16256</link><description>&lt;p&gt;
&#19968;&#31181;&#22240;&#26524;&#35299;&#32544;&#31163;&#25955;&#22810;&#31890;&#24230;&#22270;&#20998;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Causal Disentangled Multi-Granularity Graph Classification Method. (arXiv:2310.16256v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16256
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22240;&#26524;&#35299;&#32544;&#31163;&#25955;&#22810;&#31890;&#24230;&#22270;&#20998;&#31867;&#26041;&#27861;&#65288;CDM-GNN&#65289;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#35299;&#20915;&#22270;&#25968;&#25454;&#30340;&#22810;&#31890;&#24230;&#29305;&#24615;&#65292;&#23454;&#29616;&#20102;&#23545;&#22270;&#20013;&#37325;&#35201;&#23376;&#32467;&#26500;&#21644;&#20559;&#24046;&#37096;&#20998;&#30340;&#35299;&#26512;&#65292;&#24182;&#29992;&#20110;&#22270;&#20998;&#31867;&#20219;&#21153;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#29983;&#27963;&#20013;&#24191;&#27867;&#23384;&#22312;&#22823;&#37327;&#25968;&#25454;&#21644;&#22797;&#26434;&#32467;&#26500;&#30340;&#22270;&#25968;&#25454;&#12290;&#23558;&#22270;&#25968;&#25454;&#26144;&#23556;&#21040;&#20302;&#32500;&#23884;&#20837;&#31354;&#38388;&#26159;&#24517;&#35201;&#30340;&#12290;&#22270;&#20998;&#31867;&#26159;&#19968;&#39033;&#20851;&#38190;&#30340;&#22270;&#20219;&#21153;&#65292;&#20027;&#35201;&#20381;&#36182;&#20110;&#35782;&#21035;&#22270;&#20013;&#30340;&#37325;&#35201;&#23376;&#32467;&#26500;&#12290;&#30446;&#21069;&#65292;&#19968;&#20123;&#22270;&#20998;&#31867;&#26041;&#27861;&#27809;&#26377;&#32467;&#21512;&#22270;&#25968;&#25454;&#30340;&#22810;&#31890;&#24230;&#29305;&#24615;&#12290;&#24314;&#27169;&#20013;&#32570;&#20047;&#31890;&#24230;&#21306;&#20998;&#23548;&#33268;&#27169;&#22411;&#20013;&#20851;&#38190;&#20449;&#24687;&#21644;&#34394;&#20551;&#30456;&#20851;&#24615;&#28151;&#28102;&#12290;&#22240;&#27492;&#65292;&#23454;&#29616;&#21487;&#20449;&#21644;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#30340;&#30446;&#26631;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22240;&#26524;&#35299;&#32544;&#31163;&#25955;&#22810;&#31890;&#24230;&#22270;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65288;CDM-GNN&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#12290;CDM-GNN&#27169;&#22411;&#20174;&#22810;&#31890;&#24230;&#30340;&#35282;&#24230;&#23545;&#22270;&#20013;&#30340;&#37325;&#35201;&#23376;&#32467;&#26500;&#21644;&#20559;&#24046;&#37096;&#20998;&#36827;&#34892;&#35299;&#32544;&#31163;&#12290;CDM-GNN&#27169;&#22411;&#30340;&#35299;&#32544;&#31163;&#25581;&#31034;&#20102;&#37325;&#35201;&#21644;&#20559;&#24046;&#37096;&#20998;&#65292;&#20026;&#20854;&#20998;&#31867;&#20219;&#21153;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph data widely exists in real life, with large amounts of data and complex structures. It is necessary to map graph data to low-dimensional embedding. Graph classification, a critical graph task, mainly relies on identifying the important substructures within the graph. At present, some graph classification methods do not combine the multi-granularity characteristics of graph data. This lack of granularity distinction in modeling leads to a conflation of key information and false correlations within the model. So, achieving the desired goal of a credible and interpretable model becomes challenging. This paper proposes a causal disentangled multi-granularity graph representation learning method (CDM-GNN) to solve this challenge. The CDM-GNN model disentangles the important substructures and bias parts within the graph from a multi-granularity perspective. The disentanglement of the CDM-GNN model reveals important and bias parts, forming the foundation for its classification task, spe
&lt;/p&gt;</description></item><item><title>ConDefects&#26159;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#19987;&#38376;&#35299;&#20915;&#22522;&#20110;LLM&#30340;&#25925;&#38556;&#23450;&#20301;&#21644;&#31243;&#24207;&#20462;&#22797;&#20013;&#30340;&#25968;&#25454;&#27844;&#28431;&#38382;&#39064;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;1,254&#20010;Java&#26377;&#38169;&#35823;&#30340;&#31243;&#24207;&#21644;1,625&#20010;Python&#26377;&#38169;&#35823;&#30340;&#31243;&#24207;&#65292;&#26469;&#33258;&#20110;&#32447;&#19978;&#31454;&#36187;&#24179;&#21488;AtCoder&#12290;&#27599;&#19968;&#20010;&#38169;&#35823;&#37117;&#37197;&#26377;&#38169;&#35823;&#20301;&#32622;&#21644;&#30456;&#24212;&#30340;&#20462;&#22797;&#20195;&#30721;&#29256;&#26412;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#25925;&#38556;&#23450;&#20301;&#21644;&#31243;&#24207;&#20462;&#22797;&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2310.16253</link><description>&lt;p&gt;
ConDefects&#65306;&#35299;&#20915;&#22522;&#20110;LLM&#30340;&#25925;&#38556;&#23450;&#20301;&#21644;&#31243;&#24207;&#20462;&#22797;&#20013;&#30340;&#25968;&#25454;&#27844;&#28431;&#38382;&#39064;&#30340;&#26032;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
ConDefects: A New Dataset to Address the Data Leakage Concern for LLM-based Fault Localization and Program Repair. (arXiv:2310.16253v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16253
&lt;/p&gt;
&lt;p&gt;
ConDefects&#26159;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#19987;&#38376;&#35299;&#20915;&#22522;&#20110;LLM&#30340;&#25925;&#38556;&#23450;&#20301;&#21644;&#31243;&#24207;&#20462;&#22797;&#20013;&#30340;&#25968;&#25454;&#27844;&#28431;&#38382;&#39064;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;1,254&#20010;Java&#26377;&#38169;&#35823;&#30340;&#31243;&#24207;&#21644;1,625&#20010;Python&#26377;&#38169;&#35823;&#30340;&#31243;&#24207;&#65292;&#26469;&#33258;&#20110;&#32447;&#19978;&#31454;&#36187;&#24179;&#21488;AtCoder&#12290;&#27599;&#19968;&#20010;&#38169;&#35823;&#37117;&#37197;&#26377;&#38169;&#35823;&#20301;&#32622;&#21644;&#30456;&#24212;&#30340;&#20462;&#22797;&#20195;&#30721;&#29256;&#26412;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#25925;&#38556;&#23450;&#20301;&#21644;&#31243;&#24207;&#20462;&#22797;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25925;&#38556;&#23450;&#20301;&#21644;&#31243;&#24207;&#20462;&#22797;&#20013;&#30340;&#20852;&#36259;&#22686;&#21152;&#65292;&#30830;&#20445;LLM&#26041;&#27861;&#30340;&#23436;&#25972;&#24615;&#21644;&#26222;&#36866;&#24615;&#21464;&#24471;&#38750;&#24120;&#37325;&#35201;&#12290;&#29616;&#26377;&#24191;&#27867;&#37319;&#29992;&#30340;&#36825;&#20123;&#20219;&#21153;&#30340;&#22522;&#20934;&#20195;&#30721;&#26159;&#22312;LLMs&#30340;&#20852;&#36215;&#20043;&#21069;&#32534;&#20889;&#30340;&#65292;&#21487;&#33021;&#21253;&#21547;&#22312;&#29616;&#26377;&#27969;&#34892;&#30340;LLMs&#30340;&#35757;&#32451;&#25968;&#25454;&#20013;&#65292;&#20174;&#32780;&#38754;&#20020;&#25968;&#25454;&#27844;&#28431;&#30340;&#23041;&#32961;&#65292;&#23548;&#33268;&#35823;&#23548;&#24615;&#20048;&#35266;&#30340;&#24615;&#33021;&#25351;&#26631;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;ConDefects&#8221;&#36825;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;&#65292;&#31934;&#24515;&#31574;&#21010;&#20197;&#28040;&#38500;&#36825;&#31181;&#37325;&#21472;&#12290;ConDefects&#21253;&#21547;1,254&#20010;Java&#26377;&#38169;&#35823;&#30340;&#31243;&#24207;&#21644;1,625&#20010;Python&#26377;&#38169;&#35823;&#30340;&#31243;&#24207;&#12290;&#25152;&#26377;&#36825;&#20123;&#31243;&#24207;&#37117;&#26469;&#33258;&#32447;&#19978;&#31454;&#36187;&#24179;&#21488;AtCoder&#65292;&#24182;&#22312;2021&#24180;10&#26376;&#33267;2023&#24180;9&#26376;&#20043;&#38388;&#20135;&#29983;&#12290;&#25105;&#20204;&#23558;&#27599;&#20010;&#38169;&#35823;&#19982;&#38169;&#35823;&#20301;&#32622;&#21644;&#30456;&#24212;&#30340;&#20462;&#22797;&#20195;&#30721;&#29256;&#26412;&#37197;&#23545;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#25925;&#38556;&#23450;&#20301;&#21644;&#31243;&#24207;&#20462;&#22797;&#30456;&#20851;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;...
&lt;/p&gt;
&lt;p&gt;
With the growing interest on Large Language Models (LLMs) for fault localization and program repair, ensuring the integrity and generalizability of the LLM-based methods becomes paramount. The code in existing widely-adopted benchmarks for these tasks was written before the the bloom of LLMs and may be included in the training data of existing popular LLMs, thereby suffering from the threat of data leakage, leading to misleadingly optimistic performance metrics. To address this issue, we introduce "ConDefects", a novel dataset of real faults meticulously curated to eliminate such overlap. ConDefects contains 1,254 Java faulty programs and 1,625 Python faulty programs. All these programs are sourced from the online competition platform AtCoder and were produced between October 2021 and September 2023. We pair each fault with fault locations and the corresponding repaired code versions, making it tailored for in fault localization and program repair related research. We also provide inte
&lt;/p&gt;</description></item><item><title>Speakerly&#26159;&#19968;&#31181;&#22522;&#20110;&#35821;&#38899;&#30340;&#25991;&#26412;&#21019;&#20316;&#36741;&#21161;&#24037;&#20855;&#65292;&#29992;&#25143;&#21487;&#36890;&#36807;&#25351;&#20196;&#25110;&#21475;&#36848;&#19982;&#31995;&#32479;&#36827;&#34892;&#20132;&#20114;&#65292;&#31995;&#32479;&#29983;&#25104;&#26684;&#24335;&#33391;&#22909;&#12289;&#36830;&#36143;&#30340;&#25991;&#26723;&#12290;&#35813;&#31995;&#32479;&#20351;&#29992;&#23567;&#22411;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#21644;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#24555;&#36895;&#26377;&#25928;&#30340;&#25991;&#26412;&#21019;&#20316;&#65292;&#24182;&#25903;&#25345;&#21508;&#31181;&#36755;&#20837;&#27169;&#24335;&#20197;&#25552;&#39640;&#21487;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.16251</link><description>&lt;p&gt;
Speakerly&#65306;&#19968;&#31181;&#22522;&#20110;&#35821;&#38899;&#30340;&#25991;&#26412;&#21019;&#20316;&#36741;&#21161;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Speakerly: A Voice-based Writing Assistant for Text Composition. (arXiv:2310.16251v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16251
&lt;/p&gt;
&lt;p&gt;
Speakerly&#26159;&#19968;&#31181;&#22522;&#20110;&#35821;&#38899;&#30340;&#25991;&#26412;&#21019;&#20316;&#36741;&#21161;&#24037;&#20855;&#65292;&#29992;&#25143;&#21487;&#36890;&#36807;&#25351;&#20196;&#25110;&#21475;&#36848;&#19982;&#31995;&#32479;&#36827;&#34892;&#20132;&#20114;&#65292;&#31995;&#32479;&#29983;&#25104;&#26684;&#24335;&#33391;&#22909;&#12289;&#36830;&#36143;&#30340;&#25991;&#26723;&#12290;&#35813;&#31995;&#32479;&#20351;&#29992;&#23567;&#22411;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#21644;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#24555;&#36895;&#26377;&#25928;&#30340;&#25991;&#26412;&#21019;&#20316;&#65292;&#24182;&#25903;&#25345;&#21508;&#31181;&#36755;&#20837;&#27169;&#24335;&#20197;&#25552;&#39640;&#21487;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Speakerly&#65292;&#19968;&#31181;&#26032;&#30340;&#23454;&#26102;&#35821;&#38899;&#25991;&#26412;&#21019;&#20316;&#36741;&#21161;&#31995;&#32479;&#65292;&#21487;&#24110;&#21161;&#29992;&#25143;&#22312;&#21508;&#31181;&#29992;&#20363;&#20013;&#36827;&#34892;&#25991;&#26412;&#21019;&#20316;&#65292;&#22914;&#30005;&#23376;&#37038;&#20214;&#12289;&#21363;&#26102;&#36890;&#35759;&#21644;&#31508;&#35760;&#12290;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#25351;&#20196;&#25110;&#21475;&#36848;&#19982;&#31995;&#32479;&#36827;&#34892;&#20132;&#20114;&#65292;&#31995;&#32479;&#29983;&#25104;&#26684;&#24335;&#33391;&#22909;&#12289;&#36830;&#36143;&#30340;&#25991;&#26723;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#31995;&#32479;&#26550;&#26500;&#65292;&#24182;&#35814;&#32454;&#20171;&#32461;&#20102;&#22312;&#26500;&#24314;&#21644;&#37096;&#32626;&#22914;&#27492;&#35268;&#27169;&#30340;&#31995;&#32479;&#26102;&#22914;&#20309;&#35299;&#20915;&#21508;&#31181;&#25361;&#25112;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#20351;&#29992;&#20102;&#19968;&#32452;&#23567;&#22411;&#12289;&#20219;&#21153;&#29305;&#23450;&#30340;&#27169;&#22411;&#20197;&#21450;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#24555;&#36895;&#26377;&#25928;&#30340;&#25991;&#26412;&#21019;&#20316;&#65292;&#21516;&#26102;&#25903;&#25345;&#21508;&#31181;&#36755;&#20837;&#27169;&#24335;&#20197;&#25552;&#39640;&#21487;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Speakerly, a new real-time voice-based writing assistance system that helps users with text composition across various use cases such as emails, instant messages, and notes. The user can interact with the system through instructions or dictation, and the system generates a well-formatted and coherent document. We describe the system architecture and detail how we address the various challenges while building and deploying such a system at scale. More specifically, our system uses a combination of small, task-specific models as well as pre-trained language models for fast and effective text composition while supporting a variety of input modes for better usability.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#37051;&#25509;&#30697;&#38453;&#29305;&#24449;&#21521;&#37327;&#30340;&#32858;&#31867;&#24037;&#20855;&#65292;&#29992;&#20110;&#35843;&#35797;&#26377;&#38480;&#20803;&#27169;&#22411;&#20013;&#30340;&#38169;&#35823;&#65292;&#24182;&#22312;&#21830;&#19994;&#32467;&#26500;FE&#22871;&#20214;&#20013;&#25104;&#21151;&#37096;&#32626;&#65292;&#24182;&#19988;&#24050;&#34987;&#29992;&#25143;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#30340;FE&#27169;&#22411;&#35843;&#35797;&#12290;</title><link>http://arxiv.org/abs/2310.16249</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#37051;&#25509;&#30697;&#38453;&#30340;&#29305;&#24449;&#21521;&#37327;&#30340;&#32858;&#31867;&#24037;&#20855;&#29992;&#20110;&#26377;&#38480;&#20803;&#27169;&#22411;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A clustering tool for interrogating finite element models based on eigenvectors of graph adjacency. (arXiv:2310.16249v1 [cs.CE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16249
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#37051;&#25509;&#30697;&#38453;&#29305;&#24449;&#21521;&#37327;&#30340;&#32858;&#31867;&#24037;&#20855;&#65292;&#29992;&#20110;&#35843;&#35797;&#26377;&#38480;&#20803;&#27169;&#22411;&#20013;&#30340;&#38169;&#35823;&#65292;&#24182;&#22312;&#21830;&#19994;&#32467;&#26500;FE&#22871;&#20214;&#20013;&#25104;&#21151;&#37096;&#32626;&#65292;&#24182;&#19988;&#24050;&#34987;&#29992;&#25143;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#30340;FE&#27169;&#22411;&#35843;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#35843;&#35797;&#26377;&#38480;&#20803;&#65288;FE&#65289;&#20223;&#30495;&#27169;&#22411;&#20013;&#30340;&#38169;&#35823;&#65292;&#24182;&#35814;&#32454;&#35828;&#26126;&#20102;&#35813;&#31639;&#27861;&#30340;&#29983;&#20135;&#21270;&#36807;&#31243;&#12290;&#35813;&#31639;&#27861;&#20351;&#29992;&#21018;&#24230;&#30697;&#38453;&#30340;&#37051;&#25509;&#24615;&#36136;&#23545;FE&#27169;&#22411;&#30340;&#33258;&#30001;&#24230;&#36827;&#34892;&#32858;&#31867;&#12290;&#35813;&#31639;&#27861;&#24050;&#32463;&#20316;&#20026;&#19968;&#31181;&#21517;&#20026;&#8220;&#27169;&#22411;&#31283;&#23450;&#24615;&#20998;&#26512;&#8221;&#24037;&#20855;&#37096;&#32626;&#22312;&#21830;&#19994;&#32467;&#26500;FE&#22871;&#20214;Oasys GSA&#65288;www.oasys-software.com/gsa&#65289;&#20013;&#12290;&#23427;&#24050;&#32463;&#25104;&#21151;&#22320;&#34987;&#26368;&#32456;&#29992;&#25143;&#29992;&#20110;&#35843;&#35797;&#30495;&#23454;&#19990;&#30028;&#30340;FE&#27169;&#22411;&#65292;&#24182;&#19988;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#24037;&#20855;&#30340;&#23454;&#38469;&#24212;&#29992;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
This note introduces an unsupervised learning algorithm to debug errors in finite element (FE) simulation models and details how it was productionised. The algorithm clusters degrees of freedom in the FE model using numerical properties of the adjacency of its stiffness matrix. The algorithm has been deployed as a tool called `Model Stability Analysis' tool within the commercial structural FE suite Oasys GSA (www.oasys-software.com/gsa). It has been used successfully by end-users for debugging real world FE models and we present examples of the tool in action.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#30417;&#30563;&#30340;&#20687;&#32032;&#32423;&#32858;&#31867;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#22270;&#20687;&#20998;&#21106;&#25104;&#21306;&#22495;&#65292;&#32780;&#19981;&#38656;&#35201;&#20351;&#29992;&#22320;&#38754;&#30495;&#20540;&#27880;&#37322;&#12290;&#36890;&#36807;&#29305;&#24449;&#23884;&#20837;&#12289;&#29305;&#24449;&#32479;&#35745;&#35745;&#31639;&#12289;&#22270;&#20687;&#37325;&#26500;&#21644;&#36229;&#20687;&#32032;&#20998;&#21106;&#31561;&#27169;&#22359;&#65292;&#20197;&#21450;&#21033;&#29992;&#36229;&#20687;&#32032;&#20869;&#37096;&#19968;&#33268;&#24615;&#21644;&#37051;&#36817;&#36229;&#20687;&#32032;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;/&#19981;&#30456;&#20284;&#24615;&#36827;&#34892;&#35757;&#32451;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;&#26080;&#30417;&#30563;&#20998;&#21106;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#21518;&#22788;&#29702;&#26041;&#27861;&#26469;&#36991;&#20813;&#36807;&#20998;&#20998;&#21106;&#12290;</title><link>http://arxiv.org/abs/2310.16234</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#22270;&#20687;&#20998;&#21106;&#30340;&#20687;&#32032;&#32423;&#32858;&#31867;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Pixel-Level Clustering Network for Unsupervised Image Segmentation. (arXiv:2310.16234v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16234
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#30417;&#30563;&#30340;&#20687;&#32032;&#32423;&#32858;&#31867;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#22270;&#20687;&#20998;&#21106;&#25104;&#21306;&#22495;&#65292;&#32780;&#19981;&#38656;&#35201;&#20351;&#29992;&#22320;&#38754;&#30495;&#20540;&#27880;&#37322;&#12290;&#36890;&#36807;&#29305;&#24449;&#23884;&#20837;&#12289;&#29305;&#24449;&#32479;&#35745;&#35745;&#31639;&#12289;&#22270;&#20687;&#37325;&#26500;&#21644;&#36229;&#20687;&#32032;&#20998;&#21106;&#31561;&#27169;&#22359;&#65292;&#20197;&#21450;&#21033;&#29992;&#36229;&#20687;&#32032;&#20869;&#37096;&#19968;&#33268;&#24615;&#21644;&#37051;&#36817;&#36229;&#20687;&#32032;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;/&#19981;&#30456;&#20284;&#24615;&#36827;&#34892;&#35757;&#32451;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;&#26080;&#30417;&#30563;&#20998;&#21106;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#21518;&#22788;&#29702;&#26041;&#27861;&#26469;&#36991;&#20813;&#36807;&#20998;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22270;&#20687;&#20998;&#21106;&#22312;&#21508;&#31181;&#35745;&#31639;&#26426;&#35270;&#35273;&#24212;&#29992;&#20013;&#37117;&#33267;&#20851;&#37325;&#35201;&#65292;&#22914;&#33258;&#21160;&#39550;&#39542;&#12289;&#25235;&#21462;&#21644;&#26426;&#22120;&#20154;&#23548;&#33322;&#65292;&#20294;&#20026;&#35757;&#32451;&#25152;&#26377;&#20687;&#32032;&#32423;&#21035;&#19978;&#30340;&#23545;&#35937;&#36827;&#34892;&#27880;&#37322;&#20960;&#20046;&#26159;&#19981;&#21487;&#33021;&#30340;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#26080;&#30417;&#30563;&#22270;&#20687;&#20998;&#21106;&#26041;&#27861;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20687;&#32032;&#32423;&#32858;&#31867;&#26694;&#26550;&#65292;&#23558;&#22270;&#20687;&#20998;&#21106;&#25104;&#21306;&#22495;&#65292;&#32780;&#19981;&#20351;&#29992;&#22320;&#38754;&#30495;&#20540;&#27880;&#37322;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#21253;&#25324;&#20855;&#26377;&#27880;&#24847;&#26426;&#21046;&#30340;&#29305;&#24449;&#23884;&#20837;&#27169;&#22359;&#12289;&#29305;&#24449;&#32479;&#35745;&#35745;&#31639;&#27169;&#22359;&#12289;&#22270;&#20687;&#37325;&#26500;&#21644;&#36229;&#20687;&#32032;&#20998;&#21106;&#65292;&#20197;&#23454;&#29616;&#20934;&#30830;&#30340;&#26080;&#30417;&#30563;&#20998;&#21106;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35757;&#32451;&#31574;&#30053;&#65292;&#21033;&#29992;&#27599;&#20010;&#36229;&#20687;&#32032;&#20869;&#37096;&#19968;&#33268;&#24615;&#12289;&#37051;&#36817;&#36229;&#20687;&#32032;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;/&#19981;&#30456;&#20284;&#24615;&#20197;&#21450;&#22270;&#20687;&#30340;&#32467;&#26500;&#30456;&#20284;&#24615;&#12290;&#20026;&#20102;&#36991;&#20813;&#36229;&#20687;&#32032;&#25439;&#22833;&#24341;&#36215;&#30340;&#36807;&#20998;&#20998;&#21106;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21518;&#22788;&#29702;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20215;&#25351;&#26631;&#65292;&#29992;&#20110;&#35780;&#20272;&#20998;&#21106;&#24615;&#33021;&#24182;&#19982;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
While image segmentation is crucial in various computer vision applications, such as autonomous driving, grasping, and robot navigation, annotating all objects at the pixel-level for training is nearly impossible. Therefore, the study of unsupervised image segmentation methods is essential. In this paper, we present a pixel-level clustering framework for segmenting images into regions without using ground truth annotations. The proposed framework includes feature embedding modules with an attention mechanism, a feature statistics computing module, image reconstruction, and superpixel segmentation to achieve accurate unsupervised segmentation. Additionally, we propose a training strategy that utilizes intra-consistency within each superpixel, inter-similarity/dissimilarity between neighboring superpixels, and structural similarity between images. To avoid potential over-segmentation caused by superpixel-based losses, we also propose a post-processing method. Furthermore, we present an e
&lt;/p&gt;</description></item><item><title>CleanCoNLL&#26159;&#19968;&#31181;&#20960;&#20046;&#26080;&#22122;&#22768;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#20840;&#38754;&#37325;&#26631;&#35760;&#21644;&#33258;&#21160;&#19968;&#33268;&#24615;&#26816;&#26597;&#26469;&#32416;&#27491;CoNLL-03&#20013;&#30340;&#27880;&#37322;&#38169;&#35823;&#65292;&#25552;&#39640;&#20102;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;F1&#20998;&#25968;&#65292;&#24182;&#20943;&#23569;&#20102;&#22240;&#27880;&#37322;&#32570;&#22833;&#32780;&#35823;&#21028;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2310.16225</link><description>&lt;p&gt;
CleanCoNLL: &#19968;&#31181;&#20960;&#20046;&#26080;&#22122;&#22768;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
CleanCoNLL: A Nearly Noise-Free Named Entity Recognition Dataset. (arXiv:2310.16225v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16225
&lt;/p&gt;
&lt;p&gt;
CleanCoNLL&#26159;&#19968;&#31181;&#20960;&#20046;&#26080;&#22122;&#22768;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#20840;&#38754;&#37325;&#26631;&#35760;&#21644;&#33258;&#21160;&#19968;&#33268;&#24615;&#26816;&#26597;&#26469;&#32416;&#27491;CoNLL-03&#20013;&#30340;&#27880;&#37322;&#38169;&#35823;&#65292;&#25552;&#39640;&#20102;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;F1&#20998;&#25968;&#65292;&#24182;&#20943;&#23569;&#20102;&#22240;&#27880;&#37322;&#32570;&#22833;&#32780;&#35823;&#21028;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
CoNLL-03&#35821;&#26009;&#24211;&#34987;&#35748;&#20026;&#26159;&#26368;&#33879;&#21517;&#21644;&#24191;&#27867;&#20351;&#29992;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#20043;&#21069;&#30340;&#30740;&#31350;&#21457;&#29616;&#20102;&#22823;&#37327;&#30340;&#27880;&#37322;&#38169;&#35823;&#12289;&#19981;&#23436;&#25972;&#24615;&#21644;&#25968;&#25454;&#19981;&#19968;&#33268;&#24615;&#12290;&#36825;&#32473;&#23458;&#35266;&#27604;&#36739;NER&#26041;&#27861;&#21644;&#20998;&#26512;&#20854;&#38169;&#35823;&#24102;&#26469;&#20102;&#25361;&#25112;&#65292;&#22240;&#20026;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#22312;CoNLL-03&#20013;&#36798;&#21040;&#30340;F1&#20998;&#25968;&#19982;&#20272;&#35745;&#30340;&#22122;&#22768;&#27700;&#24179;&#30456;&#24403;&#29978;&#33267;&#26356;&#39640;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36890;&#36807;&#33258;&#21160;&#19968;&#33268;&#24615;&#26816;&#26597;&#36741;&#21161;&#30340;&#20840;&#38754;&#37325;&#26631;&#35760;&#24037;&#20316;&#26469;&#32416;&#27491;&#33521;&#25991;CoNLL-03&#20013;&#25152;&#26377;&#26631;&#31614;&#30340;7.0&#65285;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#36824;&#20026;&#20102;&#26356;&#22909;&#22320;&#35299;&#37322;NER&#26631;&#31614;&#21644;&#20316;&#20026;&#38468;&#21152;&#20445;&#35777;&#27880;&#37322;&#36136;&#37327;&#32780;&#28155;&#21152;&#20102;&#19968;&#20010;&#23454;&#20307;&#38142;&#25509;&#27880;&#37322;&#23618;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#19981;&#20165;&#22312;&#25105;&#20204;&#30340;&#25968;&#25454;&#19978;&#36798;&#21040;&#20102;&#26174;&#33879;&#26356;&#39640;&#30340;F1&#20998;&#25968;&#65288;97.1&#65285;&#65289;&#65292;&#32780;&#19988;&#20851;&#38190;&#26159;&#27491;&#30830;&#39044;&#27979;&#34987;&#38169;&#35823;&#22320;&#35745;&#31639;&#20026;&#38169;&#35823;&#30340;&#27604;&#20363;&#30001;&#20110;&#27880;&#37322;&#30340;&#32570;&#22833;&#24471;&#21040;&#20102;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
The CoNLL-03 corpus is arguably the most well-known and utilized benchmark dataset for named entity recognition (NER). However, prior works found significant numbers of annotation errors, incompleteness, and inconsistencies in the data. This poses challenges to objectively comparing NER approaches and analyzing their errors, as current state-of-the-art models achieve F1-scores that are comparable to or even exceed the estimated noise level in CoNLL-03. To address this issue, we present a comprehensive relabeling effort assisted by automatic consistency checking that corrects 7.0% of all labels in the English CoNLL-03. Our effort adds a layer of entity linking annotation both for better explainability of NER labels and as additional safeguard of annotation quality. Our experimental evaluation finds not only that state-of-the-art approaches reach significantly higher F1-scores (97.1%) on our data, but crucially that the share of correct predictions falsely counted as errors due to annota
&lt;/p&gt;</description></item><item><title>&#20998;&#23618;&#38543;&#26426;&#24179;&#28369;&#26159;&#19968;&#31181;&#22312;&#22797;&#26434;&#25968;&#25454;&#19978;&#36827;&#34892;&#40065;&#26834;&#24615;&#35748;&#35777;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#21482;&#22312;&#19968;&#20010;&#23545;&#35937;&#30340;&#23376;&#38598;&#19978;&#28155;&#21152;&#38543;&#26426;&#22122;&#22768;&#65292;&#20197;&#26356;&#26377;&#38024;&#23545;&#24615;&#30340;&#26041;&#24335;&#25552;&#20379;&#20102;&#26356;&#24378;&#30340;&#40065;&#26834;&#24615;&#20445;&#35777;&#21644;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.16221</link><description>&lt;p&gt;
&#20998;&#23618;&#38543;&#26426;&#24179;&#28369;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Randomized Smoothing. (arXiv:2310.16221v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16221
&lt;/p&gt;
&lt;p&gt;
&#20998;&#23618;&#38543;&#26426;&#24179;&#28369;&#26159;&#19968;&#31181;&#22312;&#22797;&#26434;&#25968;&#25454;&#19978;&#36827;&#34892;&#40065;&#26834;&#24615;&#35748;&#35777;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#21482;&#22312;&#19968;&#20010;&#23545;&#35937;&#30340;&#23376;&#38598;&#19978;&#28155;&#21152;&#38543;&#26426;&#22122;&#22768;&#65292;&#20197;&#26356;&#26377;&#38024;&#23545;&#24615;&#30340;&#26041;&#24335;&#25552;&#20379;&#20102;&#26356;&#24378;&#30340;&#40065;&#26834;&#24615;&#20445;&#35777;&#21644;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#26159;&#22797;&#26434;&#30340;&#65292;&#36890;&#24120;&#30001;&#21487;&#20998;&#35299;&#20026;&#22810;&#20010;&#23454;&#20307;&#30340;&#23545;&#35937;&#32452;&#25104;&#65288;&#20363;&#22914;&#65292;&#23558;&#22270;&#20687;&#20998;&#35299;&#20026;&#20687;&#32032;&#65292;&#23558;&#22270;&#24418;&#20998;&#35299;&#20026;&#30456;&#20114;&#36830;&#25509;&#30340;&#33410;&#28857;&#65289;&#12290;&#38543;&#26426;&#24179;&#28369;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#20351;&#27169;&#22411;&#22312;&#20854;&#36755;&#20837;&#30340;&#24494;&#23567;&#21464;&#21270;&#19978;&#20855;&#26377;&#35777;&#26126;&#30340;&#40065;&#26834;&#24615;-&#36890;&#36807;&#22312;&#20998;&#31867;&#20043;&#21069;&#38543;&#26426;&#28155;&#21152;&#22122;&#22768;&#26469;&#20445;&#35777;&#22810;&#25968;&#25237;&#31080;&#30340;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;&#24403;&#23545;&#25163;&#19981;&#26159;&#20219;&#24847;&#24178;&#25200;&#25972;&#20010;&#23545;&#35937;&#65288;&#20363;&#22914;&#22270;&#20687;&#65289;&#65292;&#32780;&#26159;&#23545;&#35937;&#30340;&#26576;&#20010;&#23454;&#20307;&#30340;&#23376;&#38598;&#65288;&#20363;&#22914;&#20687;&#32032;&#65289;&#26102;&#65292;&#36890;&#36807;&#38543;&#26426;&#24179;&#28369;&#23545;&#36825;&#31181;&#22797;&#26434;&#25968;&#25454;&#36827;&#34892;&#40065;&#26834;&#24615;&#35748;&#35777;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20316;&#20026;&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20998;&#23618;&#38543;&#26426;&#24179;&#28369;&#65306;&#25105;&#20204;&#36890;&#36807;&#20165;&#22312;&#38543;&#26426;&#36873;&#25321;&#30340;&#23454;&#20307;&#23376;&#38598;&#19978;&#28155;&#21152;&#38543;&#26426;&#22122;&#22768;&#26469;&#37096;&#20998;&#24179;&#28369;&#23545;&#35937;&#12290;&#36890;&#36807;&#20197;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#26377;&#38024;&#23545;&#24615;&#30340;&#26041;&#24335;&#28155;&#21152;&#22122;&#22768;&#65292;&#25105;&#20204;&#33719;&#24471;&#26356;&#24378;&#30340;&#40065;&#26834;&#24615;&#20445;&#35777;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#19981;&#21516;&#30340;&#22122;&#22768;&#20998;&#24067;&#21021;&#22987;&#21270;&#20998;&#23618;&#24179;&#28369;&#65292;&#24471;&#21040;&#20102;&#26032;&#30340;&#40065;&#26834;&#24615;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world data is complex and often consists of objects that can be decomposed into multiple entities (e.g. images into pixels, graphs into interconnected nodes). Randomized smoothing is a powerful framework for making models provably robust against small changes to their inputs - by guaranteeing robustness of the majority vote when randomly adding noise before classification. Yet, certifying robustness on such complex data via randomized smoothing is challenging when adversaries do not arbitrarily perturb entire objects (e.g. images) but only a subset of their entities (e.g. pixels). As a solution, we introduce hierarchical randomized smoothing: We partially smooth objects by adding random noise only on a randomly selected subset of their entities. By adding noise in a more targeted manner than existing methods we obtain stronger robustness guarantees while maintaining high accuracy. We initialize hierarchical smoothing using different noising distributions, yielding novel robustness
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#23398;&#26415;&#21644;&#24037;&#19994;&#39046;&#22495;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;LLMs&#30340;&#30693;&#35782;&#32534;&#36753;&#38382;&#39064;&#65292;&#24378;&#35843;&#20102;&#38656;&#35201;&#24320;&#21457;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#25216;&#26415;&#26469;&#26356;&#26032;&#39044;&#35757;&#32451;LLMs&#20197;&#32435;&#20837;&#26032;&#30693;&#35782;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.16218</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#32534;&#36753;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Knowledge Editing for Large Language Models: A Survey. (arXiv:2310.16218v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16218
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#23398;&#26415;&#21644;&#24037;&#19994;&#39046;&#22495;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;LLMs&#30340;&#30693;&#35782;&#32534;&#36753;&#38382;&#39064;&#65292;&#24378;&#35843;&#20102;&#38656;&#35201;&#24320;&#21457;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#25216;&#26415;&#26469;&#26356;&#26032;&#39044;&#35757;&#32451;LLMs&#20197;&#32435;&#20837;&#26032;&#30693;&#35782;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36817;&#26399;&#20197;&#20854;&#20986;&#33394;&#30340;&#29702;&#35299;&#12289;&#20998;&#26512;&#21644;&#29983;&#25104;&#25991;&#26412;&#30340;&#33021;&#21147;&#65292;&#26681;&#25454;&#20854;&#24191;&#21338;&#30340;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#25913;&#21464;&#20102;&#23398;&#26415;&#21644;&#24037;&#19994;&#39046;&#22495;&#30340;&#26684;&#23616;&#12290;&#28982;&#32780;&#65292;LLMs&#30340;&#19968;&#20010;&#20027;&#35201;&#32570;&#28857;&#26159;&#23427;&#20204;&#22312;&#39044;&#35757;&#32451;&#26102;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#65292;&#22240;&#20026;&#20854;&#21442;&#25968;&#25968;&#37327;&#21069;&#25152;&#26410;&#26377;&#12290;&#24403;&#38656;&#35201;&#39057;&#32321;&#24341;&#20837;&#26032;&#30693;&#35782;&#21040;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#26102;&#65292;&#36825;&#20010;&#32570;&#28857;&#26356;&#21152;&#26174;&#33879;&#12290;&#22240;&#27492;&#65292;&#24320;&#21457;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#25216;&#26415;&#26469;&#26356;&#26032;&#39044;&#35757;&#32451;LLMs&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#12290;&#20256;&#32479;&#26041;&#27861;&#26159;&#36890;&#36807;&#30452;&#25509;&#24494;&#35843;&#23558;&#26032;&#30693;&#35782;&#32534;&#30721;&#21040;&#39044;&#35757;&#32451;LLMs&#20013;&#12290;&#28982;&#32780;&#65292;&#31616;&#21333;&#22320;&#37325;&#26032;&#35757;&#32451;LLMs&#21487;&#33021;&#35745;&#31639;&#36164;&#28304;&#23494;&#38598;&#65292;&#24182;&#19988;&#23384;&#22312;&#23558;&#19982;&#27169;&#22411;&#26356;&#26032;&#26080;&#20851;&#30340;&#26377;&#20215;&#20540;&#30340;&#39044;&#35757;&#32451;&#30693;&#35782;&#36864;&#21270;&#30340;&#39118;&#38505;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#30693;&#35782;&#30340;&#27169;&#22411;&#32534;&#36753;(KME)&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#26088;&#22312;&#31934;&#30830;&#20462;&#25913;LLMs&#20197;&#32435;&#20837;&#29305;&#23450;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have recently transformed both the academic and industrial landscapes due to their remarkable capacity to understand, analyze, and generate texts based on their vast knowledge and reasoning ability. Nevertheless, one major drawback of LLMs is their substantial computational cost for pre-training due to their unprecedented amounts of parameters. The disadvantage is exacerbated when new knowledge frequently needs to be introduced into the pre-trained model. Therefore, it is imperative to develop effective and efficient techniques to update pre-trained LLMs. Traditional methods encode new knowledge in pre-trained LLMs through direct fine-tuning. However, naively re-training LLMs can be computationally intensive and risks degenerating valuable pre-trained knowledge irrelevant to the update in the model. Recently, Knowledge-based Model Editing (KME) has attracted increasing attention, which aims to precisely modify the LLMs to incorporate specific knowledge, wit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#27169;&#22411;&#22312;&#38271;&#24230;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20165;&#20381;&#36182;&#20110;&#25991;&#26723;&#38271;&#24230;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#24310;&#38271;&#25991;&#26723;&#30340;&#38271;&#24230;&#20250;&#21152; intensify &#36798;&#21040;&#30340;&#39640;&#20869;&#37096;&#30456;&#20284;&#24615;&#65292;&#24182;&#19988;&#36825;&#31181;&#31561;&#21521;&#24615;&#30340;&#34920;&#29616;&#39640;&#24230;&#20381;&#36182;&#20110;&#25991;&#26412;&#38271;&#24230;&#33539;&#22260;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#36890;&#29992;&#30340;&#25991;&#26723;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#29616;&#35821;&#20041;&#40065;&#26834;&#30340;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2310.16193</link><description>&lt;p&gt;
&#38271;&#24230;&#23545;&#20110;&#25991;&#26723;&#32423;&#35821;&#20041;&#32780;&#35328;&#26082;&#26159;&#35781;&#21650;&#20063;&#26159;&#31119;&#38899;
&lt;/p&gt;
&lt;p&gt;
Length is a Curse and a Blessing for Document-level Semantics. (arXiv:2310.16193v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16193
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#27169;&#22411;&#22312;&#38271;&#24230;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20165;&#20381;&#36182;&#20110;&#25991;&#26723;&#38271;&#24230;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#24310;&#38271;&#25991;&#26723;&#30340;&#38271;&#24230;&#20250;&#21152; intensify &#36798;&#21040;&#30340;&#39640;&#20869;&#37096;&#30456;&#20284;&#24615;&#65292;&#24182;&#19988;&#36825;&#31181;&#31561;&#21521;&#24615;&#30340;&#34920;&#29616;&#39640;&#24230;&#20381;&#36182;&#20110;&#25991;&#26412;&#38271;&#24230;&#33539;&#22260;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#36890;&#29992;&#30340;&#25991;&#26723;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#29616;&#35821;&#20041;&#40065;&#26834;&#30340;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#23545;&#27604;&#23398;&#20064;&#65288;CL&#65289;&#24050;&#32463;&#24191;&#27867;&#24212;&#29992;&#20110;&#20174;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#24674;&#22797;&#21477;&#23376;&#21644;&#25991;&#26723;&#32423;&#21035;&#30340;&#32534;&#30721;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36136;&#30097;&#22522;&#20110;CL&#30340;&#27169;&#22411;&#30340;&#38271;&#24230;&#27867;&#21270;&#33021;&#21147;&#65292;&#21363;&#23427;&#20204;&#23545;&#20110;&#38271;&#24230;&#24341;&#36215;&#30340;&#35821;&#20041;&#21464;&#21270;&#30340;&#26131;&#21463;&#25915;&#20987;&#30340;&#31243;&#24230;&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;&#38271;&#24230;&#26131;&#21463;&#25915;&#20987;&#26159;&#19968;&#20010;&#37325;&#35201;&#20294;&#34987;&#24573;&#35270;&#30340;&#30740;&#31350;&#31354;&#30333;&#65292;&#24182;&#19988;&#25105;&#20204;&#21487;&#20197;&#35774;&#35745;&#20165;&#20381;&#36182;&#20110;&#25991;&#26723;&#38271;&#24230;&#25552;&#20379;&#30340;&#35821;&#20041;&#20449;&#21495;&#30340;&#26080;&#30417;&#30563;CL&#26041;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#25512;&#23548;&#20102;&#38271;&#24230;&#25915;&#20987;&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#34920;&#26126;&#24310;&#38271;&#25991;&#26723;&#20250;&#21152; intensify &#24050;&#32463;&#30001;CL&#24102;&#26469;&#30340;&#39640;&#20869;&#37096;&#25991;&#26723;&#30456;&#20284;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;CL&#25215;&#35834;&#30340;&#31561;&#21521;&#24615;&#39640;&#24230;&#20381;&#36182;&#20110;&#35757;&#32451;&#20013;&#26292;&#38706;&#30340;&#25991;&#26412;&#38271;&#24230;&#33539;&#22260;&#12290;&#21463;&#21040;&#36825;&#20123;&#21457;&#29616;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#36890;&#29992;&#30340;&#25991;&#26723;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;LA(SER)$^{3}$: &#38271;&#24230;&#19981;&#21463;&#38480;&#30340;&#33258;&#25105;&#21442;&#29031;&#29992;&#20110;&#35821;&#20041;&#40065;&#26834;&#30340;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
In recent years, contrastive learning (CL) has been extensively utilized to recover sentence and document-level encoding capability from pre-trained language models. In this work, we question the length generalizability of CL-based models, i.e., their vulnerability towards length-induced semantic shift. We verify not only that length vulnerability is a significant yet overlooked research gap, but we can devise unsupervised CL methods solely depending on the semantic signal provided by document length. We first derive the theoretical foundations underlying length attacks, showing that elongating a document would intensify the high intra-document similarity that is already brought by CL. Moreover, we found that isotropy promised by CL is highly dependent on the length range of text exposed in training. Inspired by these findings, we introduce a simple yet universal document representation learning framework, LA(SER)$^{3}$: length-agnostic self-reference for semantically robust sentence r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25216;&#26415;&#65292;CoBa&#65292;&#29992;&#20110;&#20943;&#23569;&#25688;&#35201;&#20013;&#30340;&#24187;&#35273;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#27979;&#37327;&#26465;&#20214;&#35789;&#27010;&#29575;&#21644;&#19978;&#19979;&#25991;&#35789;&#36317;&#31163;&#30340;&#32479;&#35745;&#20449;&#24687;&#36827;&#34892;&#24187;&#35273;&#26816;&#27979;&#65292;&#24182;&#36890;&#36807;&#30452;&#35266;&#30340;&#22238;&#28335;&#27861;&#36827;&#34892;&#20943;&#36731;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;CoBa&#22312;&#20943;&#23569;&#25688;&#35201;&#24187;&#35273;&#26041;&#38754;&#26159;&#26377;&#25928;&#19988;&#39640;&#25928;&#30340;&#12290;</title><link>http://arxiv.org/abs/2310.16176</link><description>&lt;p&gt;
&#36890;&#36807;&#22238;&#28335;&#27861;&#32416;&#27491;&#65292;&#20943;&#23569;&#25688;&#35201;&#20013;&#30340;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Correction with Backtracking Reduces Hallucination in Summarization. (arXiv:2310.16176v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25216;&#26415;&#65292;CoBa&#65292;&#29992;&#20110;&#20943;&#23569;&#25688;&#35201;&#20013;&#30340;&#24187;&#35273;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#27979;&#37327;&#26465;&#20214;&#35789;&#27010;&#29575;&#21644;&#19978;&#19979;&#25991;&#35789;&#36317;&#31163;&#30340;&#32479;&#35745;&#20449;&#24687;&#36827;&#34892;&#24187;&#35273;&#26816;&#27979;&#65292;&#24182;&#36890;&#36807;&#30452;&#35266;&#30340;&#22238;&#28335;&#27861;&#36827;&#34892;&#20943;&#36731;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;CoBa&#22312;&#20943;&#23569;&#25688;&#35201;&#24187;&#35273;&#26041;&#38754;&#26159;&#26377;&#25928;&#19988;&#39640;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25688;&#35201;&#29983;&#25104;&#26088;&#22312;&#29983;&#25104;&#28304;&#25991;&#20214;&#30340;&#33258;&#28982;&#35821;&#35328;&#25688;&#35201;&#65292;&#26082;&#31616;&#27905;&#21448;&#20445;&#30041;&#37325;&#35201;&#20803;&#32032;&#12290;&#23613;&#31649;&#26368;&#36817;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#31070;&#32463;&#25991;&#26412;&#25688;&#35201;&#27169;&#22411;&#23481;&#26131;&#20135;&#29983;&#24187;&#35273;&#65288;&#25110;&#26356;&#20934;&#30830;&#22320;&#35828;&#26159;&#28151;&#28102;&#65289;&#65292;&#21363;&#29983;&#25104;&#30340;&#25688;&#35201;&#21253;&#21547;&#28304;&#25991;&#20214;&#20013;&#27809;&#26377;&#26681;&#25454;&#30340;&#32454;&#33410;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25216;&#26415;&#65292;CoBa&#65292;&#29992;&#20110;&#20943;&#23569;&#25688;&#35201;&#20013;&#30340;&#24187;&#35273;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#20004;&#20010;&#27493;&#39588;&#65306;&#24187;&#35273;&#26816;&#27979;&#21644;&#20943;&#36731;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#27979;&#37327;&#26377;&#20851;&#26465;&#20214;&#35789;&#27010;&#29575;&#21644;&#19978;&#19979;&#25991;&#35789;&#36317;&#31163;&#30340;&#31616;&#21333;&#32479;&#35745;&#20449;&#24687;&#21487;&#20197;&#23454;&#29616;&#21069;&#32773;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#30452;&#35266;&#30340;&#22238;&#28335;&#27861;&#22312;&#20943;&#36731;&#24187;&#35273;&#26041;&#38754;&#30340;&#24778;&#20154;&#25928;&#26524;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#25991;&#26412;&#25688;&#35201;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23545;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;CoBa&#22312;&#20943;&#23569;&#25688;&#35201;&#24187;&#35273;&#26041;&#38754;&#26159;&#26377;&#25928;&#19988;&#39640;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Abstractive summarization aims at generating natural language summaries of a source document that are succinct while preserving the important elements. Despite recent advances, neural text summarization models are known to be susceptible to hallucinating (or more correctly confabulating), that is to produce summaries with details that are not grounded in the source document. In this paper, we introduce a simple yet efficient technique, CoBa, to reduce hallucination in abstractive summarization. The approach is based on two steps: hallucination detection and mitigation. We show that the former can be achieved through measuring simple statistics about conditional word probabilities and distance to context words. Further, we demonstrate that straight-forward backtracking is surprisingly effective at mitigation. We thoroughly evaluate the proposed method with prior art on three benchmark datasets for text summarization. The results show that CoBa is effective and efficient in reducing hall
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35770;&#35777;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#21644;&#25968;&#25454;&#20998;&#26512;&#20013;&#29305;&#24449;&#24402;&#22240;&#30340;&#25361;&#25112;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#24191;&#20041;&#21487;&#21152;&#27169;&#22411;&#21644;&#26799;&#24230;&#26041;&#27861;&#19982;&#26367;&#20195;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#21516;&#26102;&#32771;&#34385;&#29992;&#25143;&#30340;&#32972;&#26223;&#20449;&#24687;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#24402;&#22240;&#30340;&#20934;&#30830;&#24615;&#21644;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.16157</link><description>&lt;p&gt;
&#22522;&#20110;&#35770;&#35777;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#29305;&#24449;&#24402;&#22240;
&lt;/p&gt;
&lt;p&gt;
Context-aware feature attribution through argumentation. (arXiv:2310.16157v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16157
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35770;&#35777;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#21644;&#25968;&#25454;&#20998;&#26512;&#20013;&#29305;&#24449;&#24402;&#22240;&#30340;&#25361;&#25112;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#24191;&#20041;&#21487;&#21152;&#27169;&#22411;&#21644;&#26799;&#24230;&#26041;&#27861;&#19982;&#26367;&#20195;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#21516;&#26102;&#32771;&#34385;&#29992;&#25143;&#30340;&#32972;&#26223;&#20449;&#24687;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#24402;&#22240;&#30340;&#20934;&#30830;&#24615;&#21644;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#24402;&#22240;&#26159;&#26426;&#22120;&#23398;&#20064;&#21644;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#22522;&#26412;&#20219;&#21153;&#65292;&#28041;&#21450;&#30830;&#23450;&#20010;&#21035;&#29305;&#24449;&#25110;&#21464;&#37327;&#23545;&#27169;&#22411;&#36755;&#20986;&#30340;&#36129;&#29486;&#12290;&#36825;&#20010;&#36807;&#31243;&#26377;&#21161;&#20110;&#30830;&#23450;&#39044;&#27979;&#32467;&#26524;&#26368;&#37325;&#35201;&#30340;&#29305;&#24449;&#12290;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#30340;&#21382;&#21490;&#21487;&#20197;&#36861;&#28335;&#21040;&#24191;&#20041;&#21487;&#21152;&#27169;&#22411; (GAMs)&#65292;&#23427;&#36890;&#36807;&#23558;&#22240;&#21464;&#37327;&#21644;&#33258;&#21464;&#37327;&#20043;&#38388;&#30340;&#38750;&#32447;&#24615;&#20851;&#31995;&#32435;&#20837;&#27169;&#22411;&#65292;&#25193;&#23637;&#20102;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#12290;&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#21644;&#26367;&#20195;&#27169;&#22411;&#24050;&#32463;&#34987;&#24212;&#29992;&#20110;&#25581;&#31034;&#22797;&#26434;&#30340;&#20154;&#24037;&#26234;&#33021; (AI) &#31995;&#32479;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#23384;&#22312;&#19968;&#20123;&#23616;&#38480;&#24615;&#12290;GAMs &#24448;&#24448;&#33021;&#22815;&#36798;&#21040;&#36739;&#20302;&#30340;&#20934;&#30830;&#24615;&#65292;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#24456;&#38590;&#35299;&#37322;&#65292;&#26367;&#20195;&#27169;&#22411;&#36890;&#24120;&#23384;&#22312;&#31283;&#23450;&#24615;&#21644;&#20445;&#30495;&#24230;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#22823;&#37096;&#20998;&#29616;&#26377;&#26041;&#27861;&#37117;&#27809;&#26377;&#32771;&#34385;&#29992;&#25143;&#30340;&#32972;&#26223;&#65292;&#32780;&#29992;&#25143;&#30340;&#32972;&#26223;&#21487;&#33021;&#20250;&#23545;&#20182;&#20204;&#30340;&#20559;&#22909;&#20135;&#29983;&#37325;&#35201;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#24182;&#25512;&#36827;&#24403;&#21069;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Feature attribution is a fundamental task in both machine learning and data analysis, which involves determining the contribution of individual features or variables to a model's output. This process helps identify the most important features for predicting an outcome. The history of feature attribution methods can be traced back to General Additive Models (GAMs), which extend linear regression models by incorporating non-linear relationships between dependent and independent variables. In recent years, gradient-based methods and surrogate models have been applied to unravel complex Artificial Intelligence (AI) systems, but these methods have limitations. GAMs tend to achieve lower accuracy, gradient-based methods can be difficult to interpret, and surrogate models often suffer from stability and fidelity issues. Furthermore, most existing methods do not consider users' contexts, which can significantly influence their preferences. To address these limitations and advance the current s
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Yin Yang&#21367;&#31215;&#32593;&#32476;&#65292;&#36890;&#36807;&#23545;&#31435;&#20998;&#26512;&#25552;&#21462;&#22270;&#20687;&#27969;&#24418;&#65292;&#22312;CIFAR-10&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;State-of-the-Art&#30340;&#25928;&#33021;&#65292;&#24182;&#19988;&#30456;&#23545;&#20110;&#20043;&#21069;&#30340;SOTA&#27169;&#22411;&#65292;&#21442;&#25968;&#20943;&#23569;&#20102;150k&#12290;</title><link>http://arxiv.org/abs/2310.16148</link><description>&lt;p&gt;
Yin Yang&#21367;&#31215;&#32593;&#32476;&#65306;&#36890;&#36807;&#23545;&#31435;&#20998;&#26512;&#25552;&#21462;&#22270;&#20687;&#27969;&#24418;
&lt;/p&gt;
&lt;p&gt;
Yin Yang Convolutional Nets: Image Manifold Extraction by the Analysis of Opposites. (arXiv:2310.16148v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16148
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Yin Yang&#21367;&#31215;&#32593;&#32476;&#65292;&#36890;&#36807;&#23545;&#31435;&#20998;&#26512;&#25552;&#21462;&#22270;&#20687;&#27969;&#24418;&#65292;&#22312;CIFAR-10&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;State-of-the-Art&#30340;&#25928;&#33021;&#65292;&#24182;&#19988;&#30456;&#23545;&#20110;&#20043;&#21069;&#30340;SOTA&#27169;&#22411;&#65292;&#21442;&#25968;&#20943;&#23569;&#20102;150k&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#35270;&#35273;&#22312;&#35757;&#32451;&#20248;&#21270;&#12289;&#26032;&#30340;&#26550;&#26500;&#65288;&#32431;&#27880;&#24847;&#21147;&#12289;&#39640;&#25928;&#22359;&#12289;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#12289;&#29983;&#25104;&#27169;&#22411;&#31561;&#65289;&#31561;&#26041;&#38754;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#27493;&#12290;&#36825;&#20123;&#36827;&#27493;&#25913;&#21892;&#20102;&#20998;&#31867;&#31561;&#22810;&#20010;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#36825;&#20123;&#27169;&#22411;&#37117;&#38598;&#20013;&#20110;&#19982;&#22823;&#33041;&#30456;&#20851;&#30340;&#29616;&#23454;&#31070;&#32463;&#31185;&#23398;&#26041;&#27861;&#30340;&#20462;&#25913;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#26356;&#20855;&#29983;&#29289;&#21551;&#21457;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;Yin Yang&#21367;&#31215;&#32593;&#32476;&#65292;&#36825;&#26159;&#19968;&#31181;&#25552;&#21462;&#35270;&#35273;&#27969;&#24418;&#30340;&#26550;&#26500;&#65292;&#23427;&#30340;&#22359;&#26088;&#22312;&#22312;&#21021;&#22987;&#23618;&#20998;&#31163;&#39068;&#33394;&#21644;&#24418;&#29366;&#30340;&#20998;&#26512;&#65292;&#27169;&#25311;&#26517;&#21494;&#30340;&#25805;&#20316;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;CIFAR-10&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;&#26550;&#26500;&#22312;&#21442;&#25968;&#36739;&#20302;&#30340;&#26550;&#26500;&#20013;&#25552;&#20379;&#20102;&#26368;&#20808;&#36827;&#30340;&#25928;&#33021;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#20010;&#27169;&#22411;&#36798;&#21040;&#20102;93.32&#65285;&#30340;&#27979;&#35797;&#20934;&#30830;&#29575;&#65292;&#27604;&#35813;&#31867;&#21035;&#20013;&#26356;&#26089;&#30340;&#26368;&#20339;&#32467;&#26524;&#39640;&#20986;0.8&#65285;&#65292;&#21516;&#26102;&#21442;&#25968;&#20943;&#23569;&#20102;15&#19975;&#20010;&#65288;&#24635;&#20849;726k&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computer vision in general presented several advances such as training optimizations, new architectures (pure attention, efficient block, vision language models, generative models, among others). This have improved performance in several tasks such as classification, and others. However, the majority of these models focus on modifications that are taking distance from realistic neuroscientific approaches related to the brain. In this work, we adopt a more bio-inspired approach and present the Yin Yang Convolutional Network, an architecture that extracts visual manifold, its blocks are intended to separate analysis of colors and forms at its initial layers, simulating occipital lobe's operations. Our results shows that our architecture provides State-of-the-Art efficiency among low parameter architectures in the dataset CIFAR-10. Our first model reached 93.32\% test accuracy, 0.8\% more than the older SOTA in this category, while having 150k less parameters (726k in total). Our second m
&lt;/p&gt;</description></item><item><title>PreWoMe&#26159;&#19968;&#31181;&#22788;&#29702;&#38271;&#31687;&#38382;&#31572;&#20013;&#20449;&#24687;&#26816;&#32034;&#38382;&#39064;&#30340;&#32479;&#19968;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;&#38382;&#39064;&#20013;&#30340;&#39044;&#35774;&#24182;&#21033;&#29992;&#20854;&#20316;&#20026;&#24037;&#20316;&#35760;&#24518;&#26469;&#29983;&#25104;&#21453;&#39304;&#21644;&#34892;&#21160;&#65292;&#19981;&#20165;&#33021;&#26377;&#25928;&#35299;&#20915;&#35823;&#23548;&#24615;&#38382;&#39064;&#65292;&#32780;&#19988;&#36866;&#29992;&#20110;&#22788;&#29702;&#27491;&#24120;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#22312;&#23454;&#38469;&#38382;&#31572;&#22330;&#26223;&#20013;&#21033;&#29992;&#39044;&#35774;&#12289;&#21453;&#39304;&#21644;&#34892;&#21160;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.16147</link><description>&lt;p&gt;
PreWoMe:&#21033;&#29992;&#39044;&#35774;&#20026;&#38271;&#31687;&#38382;&#31572;&#20013;&#30340;&#24037;&#20316;&#35760;&#24518;&#36827;&#34892;&#38382;&#39064;&#22238;&#31572;
&lt;/p&gt;
&lt;p&gt;
PreWoMe: Exploiting Presuppositions as Working Memory for Long Form Question Answering. (arXiv:2310.16147v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16147
&lt;/p&gt;
&lt;p&gt;
PreWoMe&#26159;&#19968;&#31181;&#22788;&#29702;&#38271;&#31687;&#38382;&#31572;&#20013;&#20449;&#24687;&#26816;&#32034;&#38382;&#39064;&#30340;&#32479;&#19968;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;&#38382;&#39064;&#20013;&#30340;&#39044;&#35774;&#24182;&#21033;&#29992;&#20854;&#20316;&#20026;&#24037;&#20316;&#35760;&#24518;&#26469;&#29983;&#25104;&#21453;&#39304;&#21644;&#34892;&#21160;&#65292;&#19981;&#20165;&#33021;&#26377;&#25928;&#35299;&#20915;&#35823;&#23548;&#24615;&#38382;&#39064;&#65292;&#32780;&#19988;&#36866;&#29992;&#20110;&#22788;&#29702;&#27491;&#24120;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#22312;&#23454;&#38469;&#38382;&#31572;&#22330;&#26223;&#20013;&#21033;&#29992;&#39044;&#35774;&#12289;&#21453;&#39304;&#21644;&#34892;&#21160;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#31687;&#38382;&#31572;&#20013;&#30340;&#20449;&#24687;&#26816;&#32034;&#38382;&#39064;&#24120;&#24120;&#30001;&#20110;&#38382;&#39064;&#20013;&#30340;&#27169;&#31946;&#25110;&#38169;&#35823;&#39044;&#35774;&#32780;&#35823;&#23548;&#12290;&#34429;&#28982;&#35768;&#22810;&#29616;&#26377;&#26041;&#27861;&#21487;&#20197;&#22788;&#29702;&#35823;&#23548;&#24615;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#38024;&#23545;&#30340;&#26159;&#26377;&#38480;&#30340;&#38382;&#39064;&#65292;&#24456;&#38590;&#36866;&#24212;&#23454;&#38469;&#24773;&#20917;&#20013;&#19981;&#21487;&#39044;&#27979;&#30340;&#36755;&#20837;&#29305;&#24449;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PreWoMe&#65292;&#36825;&#26159;&#19968;&#31181;&#32479;&#19968;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22788;&#29702;&#20219;&#20309;&#31867;&#22411;&#30340;&#20449;&#24687;&#26816;&#32034;&#38382;&#39064;&#12290;PreWoMe&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#25552;&#21462;&#38382;&#39064;&#20013;&#30340;&#39044;&#35774;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#24037;&#20316;&#35760;&#24518;&#26469;&#29983;&#25104;&#23545;&#38382;&#39064;&#30340;&#21453;&#39304;&#21644;&#34892;&#21160;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;PreWoMe&#19981;&#20165;&#22312;&#35299;&#20915;&#35823;&#23548;&#24615;&#38382;&#39064;&#26041;&#38754;&#26377;&#25928;&#65292;&#32780;&#19988;&#22312;&#22788;&#29702;&#27491;&#24120;&#38382;&#39064;&#26041;&#38754;&#20063;&#24456;&#26377;&#25928;&#65292;&#20174;&#32780;&#35777;&#26126;&#20102;&#22312;&#23454;&#38469;&#38382;&#31572;&#22330;&#26223;&#20013;&#21033;&#29992;&#39044;&#35774;&#12289;&#21453;&#39304;&#21644;&#34892;&#21160;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Information-seeking questions in long-form question answering (LFQA) often prove misleading due to ambiguity or false presupposition in the question. While many existing approaches handle misleading questions, they are tailored to limited questions, which are insufficient in a real-world setting with unpredictable input characteristics. In this work, we propose PreWoMe, a unified approach capable of handling any type of information-seeking question. The key idea of PreWoMe involves extracting presuppositions in the question and exploiting them as working memory to generate feedback and action about the question. Our experiment shows that PreWoMe is effective not only in tackling misleading questions but also in handling normal ones, thereby demonstrating the effectiveness of leveraging presuppositions, feedback, and action for real-world QA settings.
&lt;/p&gt;</description></item><item><title>Clinfo.ai&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#31995;&#32479;&#65292;&#20351;&#29992;&#31185;&#23398;&#25991;&#29486;&#22238;&#31572;&#21307;&#23398;&#38382;&#39064;&#12290;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#19968;&#20010;&#20449;&#24687;&#26816;&#32034;&#21644;&#25277;&#35937;&#27010;&#25324;&#20219;&#21153;&#65292;&#21457;&#24067;&#20102;&#30456;&#24212;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2310.16146</link><description>&lt;p&gt;
Clinfo.ai:&#29992;&#31185;&#23398;&#25991;&#29486;&#22238;&#31572;&#21307;&#23398;&#38382;&#39064;&#30340;&#24320;&#28304;&#26816;&#32034;&#22686;&#24378;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Clinfo.ai: An Open-Source Retrieval-Augmented Large Language Model System for Answering Medical Questions using Scientific Literature. (arXiv:2310.16146v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16146
&lt;/p&gt;
&lt;p&gt;
Clinfo.ai&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#31995;&#32479;&#65292;&#20351;&#29992;&#31185;&#23398;&#25991;&#29486;&#22238;&#31572;&#21307;&#23398;&#38382;&#39064;&#12290;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#19968;&#20010;&#20449;&#24687;&#26816;&#32034;&#21644;&#25277;&#35937;&#27010;&#25324;&#20219;&#21153;&#65292;&#21457;&#24067;&#20102;&#30456;&#24212;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21307;&#23398;&#25991;&#29486;&#30340;&#24555;&#36895;&#22686;&#38271;&#65292;&#21307;&#29983;&#21644;&#30740;&#31350;&#20154;&#21592;&#24456;&#38590;&#21450;&#26102;&#36319;&#19978;&#24182;&#24635;&#32467;&#26368;&#36817;&#30340;&#30456;&#20851;&#21457;&#29616;&#12290;&#34429;&#28982;&#29616;&#22312;&#23384;&#22312;&#20960;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#38381;&#28304;&#25688;&#35201;&#24037;&#20855;&#65292;&#20294;&#20854;&#36755;&#20986;&#32467;&#26524;&#32570;&#20047;&#20005;&#26684;&#21644;&#31995;&#32479;&#30340;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#32570;&#20047;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#21644;&#36866;&#24403;&#30340;&#22522;&#20934;&#20219;&#21153;&#26469;&#35780;&#20272;&#36825;&#20123;&#24037;&#20855;&#12290;&#25105;&#20204;&#36890;&#36807;&#22235;&#20010;&#36129;&#29486;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65306;&#25105;&#20204;&#21457;&#24067;&#20102;&#21517;&#20026;Clinfo.ai&#30340;&#24320;&#28304;WebApp&#65292;&#23427;&#22522;&#20110;&#21160;&#24577;&#26816;&#32034;&#30340;&#31185;&#23398;&#25991;&#29486;&#22238;&#31572;&#20020;&#24202;&#38382;&#39064;&#65307;&#25105;&#20204;&#25351;&#23450;&#20102;&#19968;&#20010;&#20449;&#24687;&#26816;&#32034;&#21644;&#25277;&#35937;&#27010;&#25324;&#20219;&#21153;&#65292;&#20197;&#35780;&#20272;&#36825;&#31181;&#26816;&#32034;&#22686;&#24378;&#22411;LLM&#31995;&#32479;&#30340;&#24615;&#33021;&#65307;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#20010;&#21253;&#21547;200&#20010;&#38382;&#39064;&#21450;&#20854;&#23545;&#24212;&#31572;&#26696;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#23558;&#20854;&#21629;&#21517;&#20026;PubMed&#26816;&#32034;&#21644;&#32508;&#36848;&#65288;PubMedRS-200&#65289;&#65307;&#24182;&#25253;&#21578;&#20102;Cli&#30340;&#22522;&#20934;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The quickly-expanding nature of published medical literature makes it challenging for clinicians and researchers to keep up with and summarize recent, relevant findings in a timely manner. While several closed-source summarization tools based on large language models (LLMs) now exist, rigorous and systematic evaluations of their outputs are lacking. Furthermore, there is a paucity of high-quality datasets and appropriate benchmark tasks with which to evaluate these tools. We address these issues with four contributions: we release Clinfo.ai, an open-source WebApp that answers clinical questions based on dynamically retrieved scientific literature; we specify an information retrieval and abstractive summarization task to evaluate the performance of such retrieval-augmented LLM systems; we release a dataset of 200 questions and corresponding answers derived from published systematic reviews, which we name PubMed Retrieval and Synthesis (PubMedRS-200); and report benchmark results for Cli
&lt;/p&gt;</description></item><item><title>&#24320;&#21457;&#20102;&#19968;&#20010;&#24490;&#29615;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#21333;&#20010;&#33258;&#25105;&#27880;&#24847;&#22836;&#32039;&#23494;&#27169;&#25311;&#20102;&#35748;&#30693;&#29702;&#35770;&#20013;&#20551;&#35774;&#30340;&#35760;&#24518;&#31995;&#32479;&#65292;&#24182;&#25429;&#25417;&#21040;&#20154;&#31867;&#21477;&#23376;&#22788;&#29702;&#20013;&#30340;&#24178;&#25200;&#12290;</title><link>http://arxiv.org/abs/2310.16142</link><description>&lt;p&gt;
&#26377;&#38480;&#35760;&#24518;&#23481;&#37327;&#30340;&#35821;&#35328;&#27169;&#22411;&#25429;&#25417;&#20154;&#31867;&#21477;&#23376;&#22788;&#29702;&#20013;&#30340;&#24178;&#25200;
&lt;/p&gt;
&lt;p&gt;
A Language Model with Limited Memory Capacity Captures Interference in Human Sentence Processing. (arXiv:2310.16142v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16142
&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20102;&#19968;&#20010;&#24490;&#29615;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#21333;&#20010;&#33258;&#25105;&#27880;&#24847;&#22836;&#32039;&#23494;&#27169;&#25311;&#20102;&#35748;&#30693;&#29702;&#35770;&#20013;&#20551;&#35774;&#30340;&#35760;&#24518;&#31995;&#32479;&#65292;&#24182;&#25429;&#25417;&#21040;&#20154;&#31867;&#21477;&#23376;&#22788;&#29702;&#20013;&#30340;&#24178;&#25200;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21477;&#23376;&#22788;&#29702;&#22256;&#38590;&#30340;&#20004;&#20010;&#26680;&#24515;&#22240;&#32032;&#34987;&#35748;&#20026;&#26159;&#26399;&#26395;&#21644;&#26469;&#33258;&#24037;&#20316;&#35760;&#24518;&#30340;&#26816;&#32034;&#12290;&#26368;&#36817;&#30340;&#19968;&#20010;&#23581;&#35797;&#65292;&#26088;&#22312;&#21019;&#24314;&#19968;&#20010;&#32508;&#21512;&#35748;&#30693;&#27169;&#22411;&#65292;&#23558;&#36825;&#20004;&#20010;&#22240;&#32032;&#25972;&#21512;&#22312;&#19968;&#36215;&#65292;&#20381;&#36182;&#20110;transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#21644;&#20154;&#31867;&#21477;&#23376;&#22788;&#29702;&#20013;&#22522;&#20110;&#26263;&#31034;&#30340;&#24037;&#20316;&#35760;&#24518;&#26816;&#32034;&#29702;&#35770;&#20043;&#38388;&#30340;&#30456;&#20284;&#20043;&#22788;&#12290;&#65288;Ryu and Lewis, 2021&#65289;.&#34429;&#28982;Ryu&#21644;Lewis&#23637;&#31034;&#20102;GPT-2&#30340;&#29305;&#27530;&#33258;&#27880;&#24847;&#22836;&#20013;&#30340;&#27880;&#24847;&#27169;&#24335;&#19982;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#24178;&#25200;&#30340;&#20851;&#38190;&#39044;&#27979;&#19968;&#33268;&#65292;&#36825;&#26159;&#22522;&#20110;&#26263;&#31034;&#30340;&#26816;&#32034;&#27169;&#22411;&#65292;&#20294;&#20182;&#20204;&#30340;&#26041;&#27861;&#38656;&#35201;&#35782;&#21035;&#20986;&#21477;&#27861;&#29305;&#21270;&#30340;&#33258;&#27880;&#24847;&#22836;&#65292;&#24182;&#20570;&#20986;&#35748;&#30693;&#19978;&#19981;&#21512;&#29702;&#30340;&#20551;&#35774;&#65292;&#21363;&#25968;&#30334;&#27425;&#30340;&#20869;&#23384;&#26816;&#32034;&#25805;&#20316;&#26159;&#24182;&#34892;&#36827;&#34892;&#30340;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20855;&#26377;&#21333;&#20010;&#33258;&#25105;&#27880;&#24847;&#22836;&#30340;&#24490;&#29615;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#65292;&#26356;&#36148;&#36817;&#35748;&#30693;&#29702;&#35770;&#25152;&#20551;&#35774;&#30340;&#35760;&#24518;&#31995;&#32479;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#27169;&#22411;&#30340;...
&lt;/p&gt;
&lt;p&gt;
Two of the central factors believed to underpin human sentence processing difficulty are expectations and retrieval from working memory. A recent attempt to create a unified cognitive model integrating these two factors relied on the parallels between the self-attention mechanism of transformer language models and cue-based retrieval theories of working memory in human sentence processing (Ryu and Lewis 2021). While Ryu and Lewis show that attention patterns in specialized attention heads of GPT-2 are consistent with similarity-based interference, a key prediction of cue-based retrieval models, their method requires identifying syntactically specialized attention heads, and makes the cognitively implausible assumption that hundreds of memory retrieval operations take place in parallel. In the present work, we develop a recurrent neural language model with a single self-attention head, which more closely parallels the memory system assumed by cognitive theories. We show that our model's
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;CA-KGCN&#65292;&#19968;&#20010;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#25512;&#33616;&#31995;&#32479;&#26694;&#26550;&#65292;&#33021;&#22815;&#23558;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#35821;&#20041;&#20851;&#31995;&#32435;&#20837;&#24314;&#27169;&#65292;&#25552;&#39640;&#25512;&#33616;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.16141</link><description>&lt;p&gt;
&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#21487;&#35299;&#37322;&#30693;&#35782;&#22270;&#35889;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Context-aware explainable recommendations over knowledge graphs. (arXiv:2310.16141v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16141
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;CA-KGCN&#65292;&#19968;&#20010;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#25512;&#33616;&#31995;&#32479;&#26694;&#26550;&#65292;&#33021;&#22815;&#23558;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#35821;&#20041;&#20851;&#31995;&#32435;&#20837;&#24314;&#27169;&#65292;&#25552;&#39640;&#25512;&#33616;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#21253;&#21547;&#19982;&#39033;&#30446;&#30456;&#20851;&#30340;&#20016;&#23500;&#35821;&#20041;&#20851;&#31995;&#65292;&#23558;&#36825;&#20123;&#35821;&#20041;&#20851;&#31995;&#32435;&#20837;&#25512;&#33616;&#31995;&#32479;&#26377;&#21161;&#20110;&#25506;&#32034;&#39033;&#30446;&#30340;&#28508;&#22312;&#36830;&#25509;&#65292;&#20174;&#32780;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#24182;&#22686;&#24378;&#25512;&#33616;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#21487;&#35299;&#37322;&#24615;&#19981;&#36866;&#24212;&#29992;&#25143;&#30340;&#24773;&#22659;&#65292;&#32780;&#24773;&#22659;&#21487;&#20197;&#26174;&#33879;&#24433;&#21709;&#20854;&#20559;&#22909;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;CA-KGCN&#65288;&#19978;&#19979;&#25991;&#24863;&#30693;&#30693;&#35782;&#22270;&#35889;&#21367;&#31215;&#32593;&#32476;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#26681;&#25454;&#29992;&#25143;&#30340;&#24773;&#22659;&#26469;&#24314;&#27169;&#20854;&#20559;&#22909;&#65292;&#24182;&#23558;&#19982;&#39033;&#30446;&#30456;&#20851;&#30340;&#20016;&#23500;&#35821;&#20041;&#20851;&#31995;&#32435;&#20837;&#30693;&#35782;&#22270;&#35889;&#20013;&#12290;&#35813;&#26694;&#26550;&#25429;&#25417;&#29992;&#25143;&#23545;&#19981;&#21516;&#22240;&#32032;&#30340;&#27880;&#24847;&#21147;&#65306;&#19978;&#19979;&#25991;&#21644;&#39033;&#30446;&#29305;&#24449;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#26681;&#25454;&#29305;&#23450;&#24773;&#22659;&#24314;&#27169;&#29992;&#25143;&#30340;&#20559;&#22909;&#24182;&#25552;&#20379;&#36866;&#24212;&#32473;&#23450;&#24773;&#22659;&#30340;&#35299;&#37322;&#12290;&#22312;&#19977;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graphs contain rich semantic relationships related to items and incorporating such semantic relationships into recommender systems helps to explore the latent connections of items, thus improving the accuracy of prediction and enhancing the explainability of recommendations. However, such explainability is not adapted to users' contexts, which can significantly influence their preferences. In this work, we propose CA-KGCN (Context-Aware Knowledge Graph Convolutional Network), an end-to-end framework that can model users' preferences adapted to their contexts and can incorporate rich semantic relationships in the knowledge graph related to items. This framework captures users' attention to different factors: contexts and features of items. More specifically, the framework can model users' preferences adapted to their contexts and provide explanations adapted to the given context. Experiments on three real-world datasets show the effectiveness of our framework: modeling users' 
&lt;/p&gt;</description></item><item><title>Alquist 5.0&#26159;&#19968;&#31181;&#26032;&#30340;SocialBot&#31995;&#32479;&#65292;&#36890;&#36807;&#23558;&#23545;&#35805;&#26641;&#21644;&#29983;&#25104;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#20197;&#21450;&#24341;&#20837;NRG Barista&#21644;&#25903;&#25345;&#22810;&#27169;&#24335;&#35774;&#22791;&#65292;&#25552;&#39640;&#20102;&#29992;&#25143;&#23545;&#35805;&#20307;&#39564;&#65292;&#24182;&#20445;&#25345;&#20102;&#20849;&#24773;&#21644;&#30693;&#35782;&#22411;&#23545;&#35805;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.16119</link><description>&lt;p&gt;
Alquist 5.0&#65306;&#23545;&#35805;&#26641;&#19982;&#29983;&#25104;&#27169;&#22411;&#30456;&#32467;&#21512;&#12290;&#22686;&#24378;SocialBot&#23545;&#35805;&#30340;&#19968;&#31181;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Alquist 5.0: Dialogue Trees Meet Generative Models. A Novel Approach for Enhancing SocialBot Conversations. (arXiv:2310.16119v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16119
&lt;/p&gt;
&lt;p&gt;
Alquist 5.0&#26159;&#19968;&#31181;&#26032;&#30340;SocialBot&#31995;&#32479;&#65292;&#36890;&#36807;&#23558;&#23545;&#35805;&#26641;&#21644;&#29983;&#25104;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#20197;&#21450;&#24341;&#20837;NRG Barista&#21644;&#25903;&#25345;&#22810;&#27169;&#24335;&#35774;&#22791;&#65292;&#25552;&#39640;&#20102;&#29992;&#25143;&#23545;&#35805;&#20307;&#39564;&#65292;&#24182;&#20445;&#25345;&#20102;&#20849;&#24773;&#21644;&#30693;&#35782;&#22411;&#23545;&#35805;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#25105;&#20204;&#30340;SocialBot- Alquist 5.0-&#65292;&#35813;&#31995;&#32479;&#26159;&#20026;Alexa Prize SocialBot&#22823;&#25361;&#25112;5&#24320;&#21457;&#30340;&#12290;&#22312;&#25105;&#20204;&#31995;&#32479;&#30340;&#21069;&#20960;&#20010;&#29256;&#26412;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;NRG Barista&#65292;&#24182;&#27010;&#36848;&#20102;&#23558;Barista&#25972;&#21512;&#21040;&#25105;&#20204;&#30340;SocialBot&#20013;&#30340;&#20960;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#25972;&#20307;&#30340;&#23545;&#35805;&#20307;&#39564;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25193;&#23637;&#20102;&#25105;&#20204;&#30340;SocialBot&#20197;&#25903;&#25345;&#22810;&#27169;&#24335;&#35774;&#22791;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;Alquist 5.0&#24320;&#21457;&#30340;&#35265;&#35299;&#65292;&#35813;&#31995;&#32479;&#22312;&#28385;&#36275;&#29992;&#25143;&#19981;&#26029;&#21464;&#21270;&#30340;&#26399;&#26395;&#30340;&#21516;&#26102;&#65292;&#20445;&#25345;&#20102;&#23545;&#21508;&#31181;&#20027;&#39064;&#30340;&#20849;&#24773;&#21644;&#30693;&#35782;&#22411;&#23545;&#35805;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present our SocialBot -- Alquist~5.0 -- developed for the Alexa Prize SocialBot Grand Challenge~5. Building upon previous versions of our system, we introduce the NRG Barista and outline several innovative approaches for integrating Barista into our SocialBot, improving the overall conversational experience. Additionally, we extend our SocialBot to support multimodal devices. This paper offers insights into the development of Alquist~5.0, which meets evolving user expectations while maintaining empathetic and knowledgeable conversational abilities across diverse topics.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#21078;&#23398;&#24863;&#30693;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20998;&#21106;&#25513;&#27169;&#20013;&#30340;&#20840;&#23616;&#20449;&#24687;&#26469;&#20272;&#35745;&#21322;&#30417;&#30563;&#22270;&#20687;&#20998;&#21106;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#35813;&#26041;&#27861;&#20811;&#26381;&#20102;&#20256;&#32479;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#30340;&#39640;&#35745;&#31639;&#22797;&#26434;&#24615;&#21644;&#23545;&#20687;&#32032;&#32423;&#24046;&#24322;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2310.16099</link><description>&lt;p&gt;
&#35299;&#21078;&#23398;&#24863;&#30693;&#30340;&#21322;&#30417;&#30563;&#22270;&#20687;&#20998;&#21106;&#30340;&#19981;&#30830;&#23450;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Anatomically-aware Uncertainty for Semi-supervised Image Segmentation. (arXiv:2310.16099v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16099
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#21078;&#23398;&#24863;&#30693;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20998;&#21106;&#25513;&#27169;&#20013;&#30340;&#20840;&#23616;&#20449;&#24687;&#26469;&#20272;&#35745;&#21322;&#30417;&#30563;&#22270;&#20687;&#20998;&#21106;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#35813;&#26041;&#27861;&#20811;&#26381;&#20102;&#20256;&#32479;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#30340;&#39640;&#35745;&#31639;&#22797;&#26434;&#24615;&#21644;&#23545;&#20687;&#32032;&#32423;&#24046;&#24322;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#30417;&#30563;&#23398;&#20064;&#36890;&#36807;&#21033;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#26469;&#25918;&#23485;&#23545;&#22270;&#20687;&#20998;&#21106;&#22823;&#35268;&#27169;&#20687;&#32032;&#32423;&#26631;&#27880;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;&#21033;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#19968;&#20010;&#37325;&#35201;&#26041;&#27861;&#26159;&#23545;&#27169;&#22411;&#39044;&#27979;&#36827;&#34892;&#35268;&#33539;&#21270;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#39044;&#27979;&#21487;&#33021;&#19981;&#21487;&#38752;&#65292;&#36890;&#24120;&#38656;&#35201;&#20511;&#21161;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#26041;&#27861;&#36880;&#28176;&#23398;&#20064;&#26377;&#24847;&#20041;&#21644;&#21487;&#38752;&#30340;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#20381;&#36182;&#20110;&#27169;&#22411;&#39044;&#27979;&#30340;&#22810;&#27425;&#25512;&#26029;&#65292;&#27599;&#20010;&#35757;&#32451;&#27493;&#39588;&#37117;&#38656;&#35201;&#35745;&#31639;&#65292;&#36825;&#22312;&#35745;&#31639;&#19978;&#26159;&#26114;&#36149;&#30340;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#19981;&#30830;&#23450;&#24615;&#22270;&#20687;&#25429;&#25417;&#20687;&#32032;&#32423;&#24046;&#24322;&#65292;&#24182;&#19981;&#33021;&#32771;&#34385;&#20840;&#23616;&#20449;&#24687;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20998;&#21106;&#25513;&#27169;&#20013;&#30340;&#20840;&#23616;&#20449;&#24687;&#26469;&#20272;&#35745;&#20998;&#21106;&#19981;&#30830;&#23450;&#24615;&#12290;&#26356;&#30830;&#20999;&#22320;&#35828;&#65292;&#39318;&#20808;&#23398;&#20064;&#35299;&#21078;&#23398;&#24863;&#30693;&#30340;&#34920;&#31034;&#26469;&#24314;&#27169;&#21487;&#29992;&#30340;&#20998;&#21106;&#25513;&#27169;&#12290;&#28982;&#21518;&#65292;&#35813;&#23398;&#20064;&#34920;&#31034;&#23558;&#26032;&#30340;&#20998;&#21106;&#39044;&#27979;&#26144;&#23556;&#21040;&#19968;&#20010;&#35299;&#21078;&#32467;&#26500;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised learning relaxes the need of large pixel-wise labeled datasets for image segmentation by leveraging unlabeled data. A prominent way to exploit unlabeled data is to regularize model predictions. Since the predictions of unlabeled data can be unreliable, uncertainty-aware schemes are typically employed to gradually learn from meaningful and reliable predictions. Uncertainty estimation methods, however, rely on multiple inferences from the model predictions that must be computed for each training step, which is computationally expensive. Moreover, these uncertainty maps capture pixel-wise disparities and do not consider global information. This work proposes a novel method to estimate segmentation uncertainty by leveraging global information from the segmentation masks. More precisely, an anatomically-aware representation is first learnt to model the available segmentation masks. The learnt representation thereupon maps the prediction of a new segmentation into an anatomic
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#24314;&#31435;&#31283;&#20581;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#30005;&#32593;&#39057;&#29575;&#12290;&#20010;&#20307;&#21270;&#30340;&#21367;&#31215;LSTM&#27169;&#22411;&#21487;&#20197;&#29420;&#31435;&#22320;&#20026;&#22823;&#23398;&#26657;&#22253;&#20869;&#30340;&#24314;&#31569;&#36827;&#34892;&#35757;&#32451;&#21644;&#35780;&#20272;&#65292;&#24182;&#19988;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.16071</link><description>&lt;p&gt;
&#20351;&#29992;&#21367;&#31215;LSTM&#22312;&#22823;&#23398;&#26657;&#22253;&#20013;&#39044;&#27979;&#30005;&#32593;&#39057;&#29575;
&lt;/p&gt;
&lt;p&gt;
Grid Frequency Forecasting in University Campuses using Convolutional LSTM. (arXiv:2310.16071v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16071
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#24314;&#31435;&#31283;&#20581;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#30005;&#32593;&#39057;&#29575;&#12290;&#20010;&#20307;&#21270;&#30340;&#21367;&#31215;LSTM&#27169;&#22411;&#21487;&#20197;&#29420;&#31435;&#22320;&#20026;&#22823;&#23398;&#26657;&#22253;&#20869;&#30340;&#24314;&#31569;&#36827;&#34892;&#35757;&#32451;&#21644;&#35780;&#20272;&#65292;&#24182;&#19988;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#30005;&#32593;&#38754;&#20020;&#30528;&#36234;&#26469;&#36234;&#22797;&#26434;&#30340;&#38382;&#39064;&#65292;&#20027;&#35201;&#28304;&#20110;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#25972;&#21512;&#21644;&#28040;&#36153;&#27169;&#24335;&#30340;&#28436;&#21464;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#32593;&#32476;&#24314;&#31435;&#31283;&#20581;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#65292;&#29992;&#20110;&#30005;&#32593;&#39057;&#29575;&#12290;&#36825;&#20123;&#27169;&#22411;&#26377;&#25928;&#22320;&#25429;&#25417;&#20102;&#30005;&#32593;&#39057;&#29575;&#25968;&#25454;&#20013;&#22266;&#26377;&#30340;&#26102;&#31354;&#22797;&#26434;&#24615;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#22686;&#24378;&#20102;&#30005;&#32593;&#30340;&#21487;&#38752;&#24615;&#12290;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#23398;&#26657;&#22253;&#20869;&#20010;&#20307;&#21270;&#30340;&#21367;&#31215;LSTM&#65288;ConvLSTM&#65289;&#27169;&#22411;&#30340;&#28508;&#21147;&#21644;&#21457;&#23637;&#65292;&#20351;&#23427;&#20204;&#33021;&#22815;&#29420;&#31435;&#22320;&#38024;&#23545;&#27599;&#19968;&#26635;&#24314;&#31569;&#36827;&#34892;&#35757;&#32451;&#21644;&#35780;&#20272;&#12290;&#20010;&#20307;ConvLSTM&#27169;&#22411;&#22522;&#20110;&#27599;&#26635;&#26657;&#22253;&#24314;&#31569;&#30340;&#29992;&#30005;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#26681;&#25454;&#21382;&#21490;&#36235;&#21183;&#39044;&#27979;&#30005;&#32593;&#39057;&#29575;&#12290;&#32467;&#26524;&#26377;&#21147;&#22320;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#27169;&#22411;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The modern power grid is facing increasing complexities, primarily stemming from the integration of renewable energy sources and evolving consumption patterns. This paper introduces an innovative methodology that harnesses Convolutional Neural Networks (CNN) and Long Short-Term Memory (LSTM) networks to establish robust time series forecasting models for grid frequency. These models effectively capture the spatiotemporal intricacies inherent in grid frequency data, significantly enhancing prediction accuracy and bolstering power grid reliability. The research explores the potential and development of individualized Convolutional LSTM (ConvLSTM) models for buildings within a university campus, enabling them to be independently trained and evaluated for each building. Individual ConvLSTM models are trained on power consumption data for each campus building and forecast the grid frequency based on historical trends. The results convincingly demonstrate the superiority of the proposed mode
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#31215;&#20998;&#21464;&#25442;-&#36229;&#32500;&#21464;&#25442;&#65292;&#23427;&#23558;&#20989;&#25968;&#36716;&#25442;&#20026;&#22122;&#22768;&#40065;&#26834;&#12289;&#20840;&#24687;&#12289;&#39640;&#32500;&#34920;&#31034;&#30340;&#36229;&#32500;&#21521;&#37327;&#65292;&#19982;&#20854;&#20182;&#31215;&#20998;&#21464;&#25442;&#32039;&#23494;&#30456;&#20851;&#65292;&#24182;&#20026;&#36229;&#32500;&#39046;&#22495;&#25552;&#20379;&#20102;&#29702;&#35770;&#22522;&#30784;&#21644;&#26032;&#30340;&#27934;&#23519;&#12290;</title><link>http://arxiv.org/abs/2310.16065</link><description>&lt;p&gt;
&#36229;&#32500;&#21464;&#25442;&#65306;&#20989;&#25968;&#30340;&#20840;&#24687;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
The Hyperdimensional Transform: a Holographic Representation of Functions. (arXiv:2310.16065v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16065
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#31215;&#20998;&#21464;&#25442;-&#36229;&#32500;&#21464;&#25442;&#65292;&#23427;&#23558;&#20989;&#25968;&#36716;&#25442;&#20026;&#22122;&#22768;&#40065;&#26834;&#12289;&#20840;&#24687;&#12289;&#39640;&#32500;&#34920;&#31034;&#30340;&#36229;&#32500;&#21521;&#37327;&#65292;&#19982;&#20854;&#20182;&#31215;&#20998;&#21464;&#25442;&#32039;&#23494;&#30456;&#20851;&#65292;&#24182;&#20026;&#36229;&#32500;&#39046;&#22495;&#25552;&#20379;&#20102;&#29702;&#35770;&#22522;&#30784;&#21644;&#26032;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31215;&#20998;&#21464;&#25442;&#26159;&#23558;&#20989;&#25968;&#26144;&#23556;&#21040;&#26356;&#23481;&#26131;&#34920;&#24449;&#30340;&#31354;&#38388;&#20013;&#30340;&#23453;&#36149;&#25968;&#23398;&#24037;&#20855;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#36229;&#32500;&#21464;&#25442;&#20316;&#20026;&#19968;&#31181;&#26032;&#22411;&#30340;&#31215;&#20998;&#21464;&#25442;&#12290;&#23427;&#23558;&#21487;&#31215;&#20989;&#25968;&#36716;&#25442;&#20026;&#22122;&#22768;&#40065;&#26834;&#12289;&#20840;&#24687;&#12289;&#39640;&#32500;&#34920;&#31034;&#65292;&#31216;&#20026;&#36229;&#32500;&#21521;&#37327;&#12290;&#20854;&#26680;&#24515;&#24605;&#24819;&#26159;&#29992;&#38543;&#26426;&#20989;&#25968;&#30340;&#32447;&#24615;&#32452;&#21512;&#26469;&#36924;&#36817;&#19968;&#20010;&#20989;&#25968;&#12290;&#25105;&#20204;&#27491;&#24335;&#24341;&#20837;&#20102;&#19968;&#32452;&#38543;&#26426;&#30340;&#27491;&#20132;&#22522;&#20989;&#25968;&#65292;&#24182;&#23450;&#20041;&#20102;&#36229;&#32500;&#21464;&#25442;&#21450;&#20854;&#36870;&#21464;&#25442;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#19968;&#33324;&#21464;&#25442;&#30456;&#20851;&#30340;&#24615;&#36136;&#65292;&#22914;&#20854;&#21807;&#19968;&#24615;&#12289;&#36870;&#21464;&#25442;&#30340;&#36924;&#36817;&#24615;&#36136;&#20197;&#21450;&#31215;&#20998;&#21644;&#23548;&#25968;&#30340;&#34920;&#31034;&#12290;&#36229;&#32500;&#21464;&#25442;&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#32780;&#28789;&#27963;&#30340;&#26694;&#26550;&#65292;&#19982;&#20613;&#37324;&#21494;&#12289;&#25289;&#26222;&#25289;&#26031;&#21644;&#27169;&#31946;&#21464;&#25442;&#31561;&#20854;&#20182;&#31215;&#20998;&#21464;&#25442;&#23494;&#20999;&#30456;&#20851;&#12290;&#27492;&#22806;&#65292;&#23427;&#20026;&#36229;&#32500;&#39046;&#22495;&#25552;&#20379;&#20102;&#29702;&#35770;&#22522;&#30784;&#21644;&#26032;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
Integral transforms are invaluable mathematical tools to map functions into spaces where they are easier to characterize. We introduce the hyperdimensional transform as a new kind of integral transform. It converts square-integrable functions into noise-robust, holographic, high-dimensional representations called hyperdimensional vectors. The central idea is to approximate a function by a linear combination of random functions. We formally introduce a set of stochastic, orthogonal basis functions and define the hyperdimensional transform and its inverse. We discuss general transform-related properties such as its uniqueness, approximation properties of the inverse transform, and the representation of integrals and derivatives. The hyperdimensional transform offers a powerful, flexible framework that connects closely with other integral transforms, such as the Fourier, Laplace, and fuzzy transforms. Moreover, it provides theoretical foundations and new insights for the field of hyperdim
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#39046;&#22495;&#36866;&#24212;&#19982;&#28151;&#28102;&#24179;&#34913;&#65288;ADA-CBF&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#39044;&#35757;&#32451;&#22823;&#27169;&#22411;&#24494;&#35843;&#20013;&#32771;&#34385;&#28151;&#28102;&#22240;&#32032;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#39044;&#35757;&#32451;&#22823;&#27169;&#22411;&#12289;&#39046;&#22495;&#20998;&#31867;&#22120;&#21644;&#28151;&#28102;&#20998;&#31867;&#22120;&#65292;&#24182;&#20351;&#29992;&#23545;&#25239;&#25439;&#22833;&#26469;&#25913;&#21892;&#39046;&#22495;&#19981;&#21464;&#30340;&#34920;&#31034;&#23398;&#20064;&#21644;&#24179;&#34913;&#28151;&#28102;&#22240;&#23376;&#30340;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2310.16062</link><description>&lt;p&gt;
&#22312;&#39044;&#35757;&#32451;&#22823;&#27169;&#22411;&#24494;&#35843;&#20013;&#30340;&#23545;&#25239;&#39046;&#22495;&#36866;&#24212;&#20013;&#36827;&#34892;&#28151;&#28102;&#24179;&#34913;
&lt;/p&gt;
&lt;p&gt;
Confounder Balancing in Adversarial Domain Adaptation for Pre-Trained Large Models Fine-Tuning. (arXiv:2310.16062v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16062
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#39046;&#22495;&#36866;&#24212;&#19982;&#28151;&#28102;&#24179;&#34913;&#65288;ADA-CBF&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#39044;&#35757;&#32451;&#22823;&#27169;&#22411;&#24494;&#35843;&#20013;&#32771;&#34385;&#28151;&#28102;&#22240;&#32032;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#39044;&#35757;&#32451;&#22823;&#27169;&#22411;&#12289;&#39046;&#22495;&#20998;&#31867;&#22120;&#21644;&#28151;&#28102;&#20998;&#31867;&#22120;&#65292;&#24182;&#20351;&#29992;&#23545;&#25239;&#25439;&#22833;&#26469;&#25913;&#21892;&#39046;&#22495;&#19981;&#21464;&#30340;&#34920;&#31034;&#23398;&#20064;&#21644;&#24179;&#34913;&#28151;&#28102;&#22240;&#23376;&#30340;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#22823;&#27169;&#22411;&#65288;PLMs&#65289;&#20855;&#26377;&#20986;&#33394;&#30340;&#27867;&#21270;&#12289;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#20986;&#29616;&#33021;&#21147;&#65292;&#21487;&#20197;&#22788;&#29702;&#27809;&#26377;&#30452;&#25509;&#35757;&#32451;&#25968;&#25454;&#30340;&#29305;&#23450;&#20219;&#21153;&#65292;&#20351;&#23427;&#20204;&#25104;&#20026;&#23545;&#25239;&#39046;&#22495;&#36866;&#24212;&#65288;ADA&#65289;&#26041;&#27861;&#20013;&#23558;&#20174;&#28304;&#39046;&#22495;&#23398;&#20064;&#30340;&#30693;&#35782;&#36716;&#31227;&#33267;&#30446;&#26631;&#39046;&#22495;&#30340;&#26356;&#22909;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;ADA&#26041;&#27861;&#26410;&#33021;&#27491;&#30830;&#32771;&#34385;&#28151;&#28102;&#22240;&#32032;&#65292;&#28151;&#28102;&#22240;&#32032;&#26159;&#23548;&#33268;&#28304;&#25968;&#25454;&#20998;&#24067;&#19982;&#30446;&#26631;&#39046;&#22495;&#19981;&#21516;&#30340;&#26681;&#26412;&#21407;&#22240;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;PLMs&#24494;&#35843;&#30340;&#23545;&#25239;&#39046;&#22495;&#36866;&#24212;&#19982;&#28151;&#28102;&#24179;&#34913;&#65288;ADA-CBF&#65289;&#26041;&#27861;&#12290;ADA-CBF&#21253;&#25324;&#19968;&#20010;PLM&#20316;&#20026;&#29305;&#24449;&#25552;&#21462;&#22120;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#20197;&#21450;&#19968;&#20010;&#39046;&#22495;&#20998;&#31867;&#22120;&#21644;&#19968;&#20010;&#28151;&#28102;&#20998;&#31867;&#22120;&#65292;&#23427;&#20204;&#36890;&#36807;&#23545;&#25239;&#25439;&#22833;&#36827;&#34892;&#32852;&#21512;&#35757;&#32451;&#12290;&#36825;&#20010;&#25439;&#22833;&#26088;&#22312;&#36890;&#36807;&#20943;&#24369;&#39046;&#22495;&#20998;&#31867;&#22120;&#20013;&#30340;&#27495;&#35270;&#26469;&#25913;&#36827;&#39046;&#22495;&#19981;&#21464;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;&#21516;&#26102;&#65292;&#23545;&#25239;&#25439;&#22833;&#20063;&#24179;&#34913;&#28151;&#28102;&#22240;&#23376;&#30340;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
The excellent generalization, contextual learning, and emergence abilities in the pre-trained large models (PLMs) handle specific tasks without direct training data, making them the better foundation models in the adversarial domain adaptation (ADA) methods to transfer knowledge learned from the source domain to target domains. However, existing ADA methods fail to account for the confounder properly, which is the root cause of the source data distribution that differs from the target domains. This study proposes an adversarial domain adaptation with confounder balancing for PLMs fine-tuning (ADA-CBF). The ADA-CBF includes a PLM as the foundation model for a feature extractor, a domain classifier and a confounder classifier, and they are jointly trained with an adversarial loss. This loss is designed to improve the domain-invariant representation learning by diluting the discrimination in the domain classifier. At the same time, the adversarial loss also balances the confounder distrib
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#33258;&#21160;&#25191;&#34892;Web&#36719;&#20214;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27493;&#39588;&#24615;&#29983;&#25104;&#23567;&#22411;&#31243;&#24207;&#26469;&#23454;&#29616;&#23545;&#28857;&#20987;&#12289;&#28378;&#21160;&#21644;&#25991;&#26412;&#36755;&#20837;&#25805;&#20316;&#30340;&#25511;&#21046;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;MiniWob++&#22522;&#20934;&#27979;&#35797;&#20013;&#36890;&#36807;&#19968;&#20010;&#19978;&#19979;&#25991;&#31034;&#20363;&#23601;&#33021;&#36798;&#21040;&#30456;&#20284;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.16042</link><description>&lt;p&gt;
WebWISE&#65306;&#20855;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;Web&#30028;&#38754;&#25511;&#21046;&#21644;&#39034;&#24207;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
WebWISE: Web Interface Control and Sequential Exploration with Large Language Models. (arXiv:2310.16042v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16042
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#33258;&#21160;&#25191;&#34892;Web&#36719;&#20214;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27493;&#39588;&#24615;&#29983;&#25104;&#23567;&#22411;&#31243;&#24207;&#26469;&#23454;&#29616;&#23545;&#28857;&#20987;&#12289;&#28378;&#21160;&#21644;&#25991;&#26412;&#36755;&#20837;&#25805;&#20316;&#30340;&#25511;&#21046;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;MiniWob++&#22522;&#20934;&#27979;&#35797;&#20013;&#36890;&#36807;&#19968;&#20010;&#19978;&#19979;&#25991;&#31034;&#20363;&#23601;&#33021;&#36798;&#21040;&#30456;&#20284;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#33258;&#21160;&#25191;&#34892;&#28857;&#20987;&#12289;&#28378;&#21160;&#21644;&#25991;&#26412;&#36755;&#20837;&#25805;&#20316;&#30340;Web&#36719;&#20214;&#20219;&#21153;&#12290;&#20043;&#21069;&#30340;&#26041;&#27861;&#65292;&#22914;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#25110;&#27169;&#20223;&#23398;&#20064;&#65292;&#35757;&#32451;&#25928;&#29575;&#20302;&#19988;&#29305;&#23450;&#20110;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#31579;&#36873;&#21518;&#30340;&#25991;&#26723;&#23545;&#35937;&#27169;&#22411;&#65288;DOM&#65289;&#20803;&#32032;&#20316;&#20026;&#35266;&#23519;&#65292;&#36880;&#27493;&#25191;&#34892;&#20219;&#21153;&#65292;&#26681;&#25454;&#24403;&#21069;&#35266;&#23519;&#29983;&#25104;&#23567;&#22411;&#31243;&#24207;&#12290;&#25105;&#20204;&#20351;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#21487;&#20197;&#20174;&#21333;&#20010;&#25163;&#21160;&#25552;&#20379;&#30340;&#31034;&#20363;&#20013;&#33719;&#30410;&#65292;&#25110;&#32773;&#20174;&#25104;&#21151;&#30340;&#38646;&#26679;&#20363;&#35797;&#39564;&#20013;&#29983;&#25104;&#33258;&#21160;&#31034;&#20363;&#12290;&#25105;&#20204;&#22312;MiniWob++&#22522;&#20934;&#27979;&#35797;&#19978;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;&#21482;&#26377;&#19968;&#20010;&#19978;&#19979;&#25991;&#31034;&#20363;&#65292;&#25105;&#20204;&#30340;WebWISE&#26041;&#27861;&#30340;&#24615;&#33021;&#19982;&#20854;&#20182;&#38656;&#35201;&#35768;&#22810;&#28436;&#31034;&#25110;&#35797;&#39564;&#30340;&#26041;&#27861;&#30456;&#24403;&#25110;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper investigates using a Large Language Model (LLM) to automatically perform web software tasks using click, scroll, and text input operations. Previous approaches, such as reinforcement learning (RL) or imitation learning, are inefficient to train and task-specific. Our method uses filtered Document Object Model (DOM) elements as observations and performs tasks step-by-step, sequentially generating small programs based on the current observations. We use in-context learning, either benefiting from a single manually provided example, or an automatically generated example based on a successful zero-shot trial. We evaluate the proposed method on the MiniWob++ benchmark. With only one in-context example, our WebWISE method achieves similar or better performance than other methods that require many demonstrations or trials.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#22522;&#20110;&#22810;&#36890;&#36947;&#32418;&#22806;&#21355;&#26143;&#35266;&#27979;&#25968;&#25454;&#30340;&#21487;&#35299;&#37322;&#24615;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#22823;&#27668;&#23545;&#27969;&#36215;&#22987;&#12290;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#65292;&#35813;&#27169;&#22411;&#34920;&#29616;&#20986;&#23545;&#20113;&#23618;&#21644;&#28287;&#24230;&#29305;&#24615;&#30340;&#20381;&#36182;&#24615;&#65292;&#24182;&#22312;&#35823;&#25253;&#29575;&#19978;&#26174;&#33879;&#20248;&#20110;&#32463;&#20856;&#30340;&#36923;&#36753;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.16015</link><description>&lt;p&gt;
&#20351;&#29992;GOES-16&#21355;&#26143;&#35266;&#27979;&#25968;&#25454;&#30340;&#21487;&#35299;&#37322;&#24615;&#28145;&#24230;&#23398;&#20064;&#23545;&#22823;&#27668;&#23545;&#27969;&#36215;&#22987;&#36827;&#34892;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Physically Explainable Deep Learning for Convective Initiation Nowcasting Using GOES-16 Satellite Observations. (arXiv:2310.16015v1 [physics.ao-ph] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16015
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#22522;&#20110;&#22810;&#36890;&#36947;&#32418;&#22806;&#21355;&#26143;&#35266;&#27979;&#25968;&#25454;&#30340;&#21487;&#35299;&#37322;&#24615;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#22823;&#27668;&#23545;&#27969;&#36215;&#22987;&#12290;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#65292;&#35813;&#27169;&#22411;&#34920;&#29616;&#20986;&#23545;&#20113;&#23618;&#21644;&#28287;&#24230;&#29305;&#24615;&#30340;&#20381;&#36182;&#24615;&#65292;&#24182;&#22312;&#35823;&#25253;&#29575;&#19978;&#26174;&#33879;&#20248;&#20110;&#32463;&#20856;&#30340;&#36923;&#36753;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#27668;&#23545;&#27969;&#36215;&#22987;&#65288;CI&#65289;&#30340;&#39044;&#27979;&#22312;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#21644;&#29616;&#26377;&#30340;&#39044;&#27979;&#31639;&#27861;&#20013;&#20173;&#28982;&#26159;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#22522;&#20110;&#22810;&#36890;&#36947;&#32418;&#22806;GOES-R&#21355;&#26143;&#35266;&#27979;&#25968;&#25454;&#30340;&#22522;&#20110;&#23545;&#35937;&#30340;&#27010;&#29575;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;CI&#12290;&#25968;&#25454;&#26469;&#33258;&#20110;2020&#24180;6&#26376;&#21644;7&#26376;&#20197;&#21450;2021&#24180;6&#26376;&#22312;&#32654;&#22269;&#22823;&#24179;&#21407;&#22320;&#21306;&#22810;&#38647;&#36798;&#22810;&#20256;&#24863;&#22120;&#22810;&#26222;&#21202;&#22825;&#27668;&#38647;&#36798;&#20135;&#21697;&#20013;&#35782;&#21035;&#20986;&#30340;&#28508;&#22312;CI&#20107;&#20214;&#21608;&#22260;&#30340;&#34917;&#19969;&#12290;&#20351;&#29992;&#23458;&#35266;&#30340;&#22522;&#20110;&#38647;&#36798;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#36825;&#20123;&#20107;&#20214;&#12290;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#25552;&#21069;&#26102;&#38388;&#20026;1&#23567;&#26102;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#20248;&#20110;&#32463;&#20856;&#30340;&#36923;&#36753;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#22312;&#35823;&#25253;&#29575;&#19978;&#12290;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23637;&#31034;&#20102;&#23545;&#20113;&#23618;&#21644;&#22810;&#23618;&#27425;&#28287;&#24230;&#29305;&#24615;&#30340;&#20381;&#36182;&#24615;&#12290;&#27169;&#22411;&#35299;&#37322;&#36827;&#19968;&#27493;&#25581;&#31034;&#20102;&#27169;&#22411;&#22312;&#19981;&#21516;&#22522;&#32447;&#19979;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#35299;&#37322;&#32467;&#26524;&#31361;&#20986;&#20102;&#28287;&#24230;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Convection initiation (CI) nowcasting remains a challenging problem for both numerical weather prediction models and existing nowcasting algorithms. In this study, object-based probabilistic deep learning models are developed to predict CI based on multichannel infrared GOES-R satellite observations. The data come from patches surrounding potential CI events identified in Multi-Radar Multi-Sensor Doppler weather radar products over the Great Plains region from June and July 2020 and June 2021. An objective radar-based approach is used to identify these events. The deep learning models significantly outperform the classical logistic model at lead times up to 1 hour, especially on the false alarm ratio. Through case studies, the deep learning model exhibits the dependence on the characteristics of clouds and moisture at multiple levels. Model explanation further reveals the model's decision-making process with different baselines. The explanation results highlight the importance of moist
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20855;&#26377;&#19987;&#38376;&#21475;&#38899;&#20195;&#30721;&#26412;&#30340;&#21475;&#38899;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#20132;&#21449;&#27880;&#24847;&#21147;&#21644;&#21487;&#35757;&#32451;&#20195;&#30721;&#26412;&#65292;&#29992;&#20110;&#31471;&#21040;&#31471;ASR&#31995;&#32479;&#12290;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#24050;&#35265;&#21644;&#26410;&#35265;&#30340;&#21475;&#38899;&#19978;&#37117;&#33021;&#33719;&#24471;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2310.15970</link><description>&lt;p&gt;
&#20351;&#29992;&#20855;&#26377;&#19987;&#38376;&#21475;&#38899;&#20195;&#30721;&#26412;&#30340;&#21475;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Accented Speech Recognition With Accent-specific Codebooks. (arXiv:2310.15970v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15970
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20855;&#26377;&#19987;&#38376;&#21475;&#38899;&#20195;&#30721;&#26412;&#30340;&#21475;&#38899;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#20132;&#21449;&#27880;&#24847;&#21147;&#21644;&#21487;&#35757;&#32451;&#20195;&#30721;&#26412;&#65292;&#29992;&#20110;&#31471;&#21040;&#31471;ASR&#31995;&#32479;&#12290;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#24050;&#35265;&#21644;&#26410;&#35265;&#30340;&#21475;&#38899;&#19978;&#37117;&#33021;&#33719;&#24471;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#21475;&#38899;&#23545;&#20110;&#29616;&#26377;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#26500;&#25104;&#20102;&#37325;&#35201;&#25361;&#25112;&#12290;&#22312;&#20195;&#34920;&#24615;&#19981;&#36275;&#30340;&#21475;&#38899;&#20013;&#30340;&#24615;&#33021;&#19979;&#38477;&#20005;&#37325;&#38459;&#30861;&#20102;ASR&#30340;&#26222;&#21450;&#24212;&#29992;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21475;&#38899;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#20132;&#21449;&#27880;&#24847;&#21147;&#21644;&#21487;&#35757;&#32451;&#20195;&#30721;&#26412;&#65292;&#29992;&#20110;&#31471;&#21040;&#31471;ASR&#31995;&#32479;&#12290;&#36825;&#20123;&#21487;&#23398;&#20064;&#30340;&#20195;&#30721;&#26412;&#25429;&#25417;&#20102;&#21475;&#38899;&#29305;&#23450;&#20449;&#24687;&#65292;&#24182;&#34987;&#25972;&#21512;&#21040;ASR&#32534;&#30721;&#22120;&#23618;&#20013;&#12290;&#27169;&#22411;&#22312;&#24102;&#21475;&#38899;&#30340;&#33521;&#35821;&#35821;&#38899;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#27979;&#35797;&#25968;&#25454;&#20013;&#20063;&#21253;&#21547;&#20102;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26410;&#35265;&#36807;&#30340;&#21475;&#38899;&#12290;&#22312;Mozilla Common Voice&#22810;&#21475;&#38899;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#19981;&#20165;&#22312;&#24050;&#35265;&#30340;&#33521;&#35821;&#21475;&#38899;&#20013;&#33719;&#24471;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65288;&#21333;&#35789;&#38169;&#35823;&#29575;&#30456;&#23545;&#25552;&#21319;&#39640;&#36798;37%&#65289;&#65292;&#32780;&#19988;&#22312;&#26410;&#35265;&#30340;&#21475;&#38899;&#19978;&#20063;&#33719;&#24471;&#20102;5%&#30340;&#30456;&#23545;&#25552;&#21319;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22312;L2Artic&#25968;&#25454;&#38598;&#19978;&#30340;&#38646;&#26679;&#26412;&#36801;&#31227;&#35774;&#32622;&#30340;&#22909;&#22788;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#23545;&#27604;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech accents pose a significant challenge to state-of-the-art automatic speech recognition (ASR) systems. Degradation in performance across underrepresented accents is a severe deterrent to the inclusive adoption of ASR. In this work, we propose a novel accent adaptation approach for end-to-end ASR systems using cross-attention with a trainable set of codebooks. These learnable codebooks capture accent-specific information and are integrated within the ASR encoder layers. The model is trained on accented English speech, while the test data also contained accents which were not seen during training. On the Mozilla Common Voice multi-accented dataset, we show that our proposed approach yields significant performance gains not only on the seen English accents (up to $37\%$ relative improvement in word error rate) but also on the unseen accents (up to $5\%$ relative improvement in WER). Further, we illustrate benefits for a zero-shot transfer setup on the L2Artic dataset. We also compare
&lt;/p&gt;</description></item><item><title>FANToM&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#29992;&#20110;&#36890;&#36807;&#38382;&#31572;&#22312;&#20449;&#24687;&#19981;&#23545;&#31216;&#30340;&#23545;&#35805;&#29615;&#22659;&#20013;&#21387;&#21147;&#27979;&#35797;&#26426;&#22120;&#30340;&#24515;&#26234;&#29702;&#35770;&#12290;&#36825;&#20010;&#22522;&#20934;&#23545;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#35828;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#21363;&#20351;&#26159;&#20855;&#26377;&#24605;&#32500;&#38142;&#25512;&#29702;&#21644;&#24494;&#35843;&#30340;&#27169;&#22411;&#20063;&#27604;&#20154;&#31867;&#34920;&#29616;&#24471;&#24046;&#12290;</title><link>http://arxiv.org/abs/2310.15421</link><description>&lt;p&gt;
FANToM: &#22312;&#20132;&#20114;&#20013;&#23545;&#26426;&#22120;&#24515;&#26234;&#29702;&#35770;&#36827;&#34892;&#21387;&#21147;&#27979;&#35797;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions. (arXiv:2310.15421v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15421
&lt;/p&gt;
&lt;p&gt;
FANToM&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#29992;&#20110;&#36890;&#36807;&#38382;&#31572;&#22312;&#20449;&#24687;&#19981;&#23545;&#31216;&#30340;&#23545;&#35805;&#29615;&#22659;&#20013;&#21387;&#21147;&#27979;&#35797;&#26426;&#22120;&#30340;&#24515;&#26234;&#29702;&#35770;&#12290;&#36825;&#20010;&#22522;&#20934;&#23545;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#35828;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#21363;&#20351;&#26159;&#20855;&#26377;&#24605;&#32500;&#38142;&#25512;&#29702;&#21644;&#24494;&#35843;&#30340;&#27169;&#22411;&#20063;&#27604;&#20154;&#31867;&#34920;&#29616;&#24471;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#20851;&#20110;&#24515;&#26234;&#29702;&#35770;&#65288;ToM&#65289;&#30340;&#35780;&#20272;&#20027;&#35201;&#38598;&#20013;&#22312;&#20351;&#29992;&#32570;&#20047;&#20114;&#21160;&#24615;&#30340;&#34987;&#21160;&#25925;&#20107;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;FANToM&#65292;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#36890;&#36807;&#38382;&#31572;&#22312;&#20449;&#24687;&#19981;&#23545;&#31216;&#30340;&#23545;&#35805;&#29615;&#22659;&#20013;&#36827;&#34892;&#24515;&#26234;&#29702;&#35770;&#30340;&#21387;&#21147;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#32467;&#21512;&#20102;&#24515;&#29702;&#23398;&#20013;&#30340;&#37325;&#35201;&#29702;&#35770;&#35201;&#27714;&#21644;&#23545;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26102;&#24517;&#35201;&#30340;&#32463;&#39564;&#32771;&#34385;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#22810;&#31181;&#31867;&#22411;&#30340;&#38382;&#39064;&#65292;&#35201;&#27714;&#30456;&#21516;&#30340;&#22522;&#26412;&#25512;&#29702;&#26469;&#35782;&#21035;LLM&#20013;&#19981;&#23384;&#22312;&#25110;&#34394;&#20551;&#30340;&#24515;&#26234;&#29702;&#35770;&#33021;&#21147;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;FANToM&#23545;&#26368;&#20808;&#36827;&#30340;LLM&#26469;&#35828;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#21363;&#20351;&#26159;&#20855;&#26377;&#24605;&#32500;&#38142;&#25512;&#29702;&#21644;&#24494;&#35843;&#30340;LLM&#20063;&#34920;&#29616;&#27604;&#20154;&#31867;&#24046;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
Theory of mind (ToM) evaluations currently focus on testing models using passive narratives that inherently lack interactivity. We introduce FANToM, a new benchmark designed to stress-test ToM within information-asymmetric conversational contexts via question answering. Our benchmark draws upon important theoretical requisites from psychology and necessary empirical considerations when evaluating large language models (LLMs). In particular, we formulate multiple types of questions that demand the same underlying reasoning to identify illusory or false sense of ToM capabilities in LLMs. We show that FANToM is challenging for state-of-the-art LLMs, which perform significantly worse than humans even with chain-of-thought reasoning or fine-tuning.
&lt;/p&gt;</description></item><item><title>TaskDiff&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#35805;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#30340;&#23545;&#35805;&#32452;&#25104;&#37096;&#20998;&#26469;&#35745;&#31639;&#30456;&#20284;&#24230;&#65292;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.15298</link><description>&lt;p&gt;
&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
TaskDiff: A Similarity Metric for Task-Oriented Conversations. (arXiv:2310.15298v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15298
&lt;/p&gt;
&lt;p&gt;
TaskDiff&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#35805;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#30340;&#23545;&#35805;&#32452;&#25104;&#37096;&#20998;&#26469;&#35745;&#31639;&#30456;&#20284;&#24230;&#65292;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20250;&#35805;&#24335;&#25968;&#23383;&#21161;&#25163;&#30340;&#26222;&#21450;&#23548;&#33268;&#20102;&#22823;&#37327;&#20250;&#35805;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#65292;&#36825;&#21487;&#20197;&#29992;&#20110;&#25913;&#21892;&#29992;&#25143;&#20307;&#39564;&#21644;&#20010;&#24615;&#21270;&#21709;&#24212;&#29983;&#25104;&#12290;&#20351;&#29992;&#20687;ChatGPT&#36825;&#26679;&#30340;&#27969;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#36825;&#20123;&#21161;&#25163;&#36824;&#38656;&#35201;&#39069;&#22806;&#24378;&#35843;&#25552;&#31034;&#24037;&#31243;&#21644;&#35780;&#20272;&#26041;&#27861;&#12290;&#25991;&#26412;&#30456;&#20284;&#24230;&#24230;&#37327;&#26159;&#36825;&#31181;&#20998;&#26512;&#21644;&#35780;&#20272;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#34429;&#28982;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#35768;&#22810;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#22312;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#26041;&#38754;&#24182;&#19981;&#26377;&#25928;&#65292;&#22240;&#20026;&#23427;&#20204;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#29420;&#29305;&#30340;&#23545;&#35805;&#29305;&#24449;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#35805;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;TaskDiff&#65292;&#23427;&#21033;&#29992;&#23545;&#35805;&#30340;&#19981;&#21516;&#32452;&#25104;&#37096;&#20998;&#65288;&#35805;&#35821;&#12289;&#24847;&#22270;&#21644;&#27133;&#65289;&#21450;&#20854;&#20998;&#24067;&#26469;&#35745;&#31639;&#30456;&#20284;&#24230;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#35777;&#26126;&#20102;TaskDiff&#22312;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#30340;&#20248;&#36234;&#34920;&#29616;&#65292;&#36229;&#36807;&#20102;&#20854;&#20182;&#30456;&#20851;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The popularity of conversational digital assistants has resulted in the availability of large amounts of conversational data which can be utilized for improved user experience and personalized response generation. Building these assistants using popular large language models like ChatGPT also require additional emphasis on prompt engineering and evaluation methods. Textual similarity metrics are a key ingredient for such analysis and evaluations. While many similarity metrics have been proposed in the literature, they have not proven effective for task-oriented conversations as they do not take advantage of unique conversational features. To address this gap, we present TaskDiff, a novel conversational similarity metric that utilizes different dialogue components (utterances, intents, and slots) and their distributions to compute similarity. Extensive experimental evaluation of TaskDiff on a benchmark dataset demonstrates its superior performance and improved robustness over other rela
&lt;/p&gt;</description></item><item><title>MGAS&#26159;&#19968;&#20010;&#22810;&#31890;&#24230;&#26550;&#26500;&#25628;&#32034;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#29305;&#23450;&#31890;&#24230;&#32423;&#21035;&#30340;&#31163;&#25955;&#21270;&#20989;&#25968;&#65292;&#33258;&#36866;&#24212;&#22320;&#30830;&#23450;&#21097;&#20313;&#27604;&#20363;&#65292;&#20174;&#32780;&#23454;&#29616;&#21516;&#26102;&#20248;&#21270;&#27169;&#22411;&#22823;&#23567;&#21644;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.15074</link><description>&lt;p&gt;
MGAS: &#22810;&#31890;&#24230;&#26550;&#26500;&#25628;&#32034;&#20197;&#23454;&#29616;&#39640;&#25928;&#19988;&#26377;&#25928;&#30340;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
MGAS: Multi-Granularity Architecture Search for Effective and Efficient Neural Networks. (arXiv:2310.15074v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15074
&lt;/p&gt;
&lt;p&gt;
MGAS&#26159;&#19968;&#20010;&#22810;&#31890;&#24230;&#26550;&#26500;&#25628;&#32034;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#29305;&#23450;&#31890;&#24230;&#32423;&#21035;&#30340;&#31163;&#25955;&#21270;&#20989;&#25968;&#65292;&#33258;&#36866;&#24212;&#22320;&#30830;&#23450;&#21097;&#20313;&#27604;&#20363;&#65292;&#20174;&#32780;&#23454;&#29616;&#21516;&#26102;&#20248;&#21270;&#27169;&#22411;&#22823;&#23567;&#21644;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#24494;&#20998;&#26550;&#26500;&#25628;&#32034;(DAS)&#36890;&#36807;&#26102;&#38388;&#39640;&#25928;&#30340;&#33258;&#21160;&#21270;&#25913;&#21464;&#20102;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;(NAS)&#30340;&#26041;&#24335;&#65292;&#20174;&#31163;&#25955;&#20505;&#36873;&#37319;&#26679;&#21644;&#35780;&#20272;&#36716;&#21464;&#20026;&#21487;&#24494;&#20998;&#36229;&#32593;&#32476;&#20248;&#21270;&#21644;&#31163;&#25955;&#21270;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;DAS&#26041;&#27861;&#35201;&#20040;&#21482;&#36827;&#34892;&#31895;&#31890;&#24230;&#30340;&#25805;&#20316;&#32423;&#25628;&#32034;&#65292;&#35201;&#20040;&#25163;&#21160;&#23450;&#20041;&#21097;&#20313;&#30340;&#32454;&#31890;&#24230;&#30340;&#26680;&#32423;&#21644;&#26435;&#37325;&#32423;&#21333;&#20301;&#30340;&#27604;&#20363;&#65292;&#20174;&#32780;&#26080;&#27861;&#21516;&#26102;&#20248;&#21270;&#27169;&#22411;&#22823;&#23567;&#21644;&#27169;&#22411;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#20026;&#20102;&#20943;&#23569;&#20869;&#23384;&#28040;&#32791;&#32780;&#29306;&#29298;&#20102;&#25628;&#32034;&#36136;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22810;&#31890;&#24230;&#26550;&#26500;&#25628;&#32034;(MGAS)&#65292;&#36825;&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#20840;&#38754;&#32780;&#20869;&#23384;&#39640;&#25928;&#22320;&#25506;&#32034;&#22810;&#31890;&#24230;&#25628;&#32034;&#31354;&#38388;&#65292;&#21457;&#29616;&#26082;&#26377;&#25928;&#21448;&#39640;&#25928;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23398;&#20064;&#20102;&#38024;&#23545;&#27599;&#20010;&#31890;&#24230;&#32423;&#21035;&#30340;&#31163;&#25955;&#21270;&#20989;&#25968;&#65292;&#26681;&#25454;&#19981;&#26029;&#28436;&#21270;&#30340;&#26550;&#26500;&#33258;&#36866;&#24212;&#22320;&#30830;&#23450;&#21097;&#20313;&#30340;&#27604;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Differentiable architecture search (DAS) revolutionizes neural architecture search (NAS) with time-efficient automation, transitioning from discrete candidate sampling and evaluation to differentiable super-net optimization and discretization. However, existing DAS methods either only conduct coarse-grained operation-level search or manually define the remaining ratios for fine-grained kernel-level and weight-level units, which fail to simultaneously optimize model size and model performance. Furthermore, these methods compromise search quality to reduce memory consumption. To tackle these issues, we introduce multi-granularity architecture search (MGAS), a unified framework which aims to comprehensively and memory-efficiently explore the multi-granularity search space to discover both effective and efficient neural networks. Specifically, we learn discretization functions specific to each granularity level to adaptively determine the remaining ratios according to the evolving architec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#20462;&#21098;&#26041;&#27861;MoSo&#65292;&#23427;&#36890;&#36807;&#35780;&#20272;&#26679;&#26412;&#23545;&#26368;&#20248;&#32463;&#39564;&#39118;&#38505;&#30340;&#24433;&#21709;&#26469;&#30830;&#23450;&#27599;&#20010;&#26679;&#26412;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#19968;&#38454;&#36817;&#20284;&#22120;&#26469;&#35745;&#31639;&#26679;&#26412;&#30340;&#37325;&#35201;&#24615;&#65292;&#35813;&#36817;&#20284;&#22120;&#21482;&#38656;&#35201;&#26799;&#24230;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2310.14664</link><description>&lt;p&gt;
&#36890;&#36807;&#31227;&#38500;&#21333;&#20010;&#26679;&#26412;&#36827;&#34892;&#25968;&#25454;&#20462;&#21098;
&lt;/p&gt;
&lt;p&gt;
Data Pruning via Moving-one-Sample-out. (arXiv:2310.14664v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14664
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#20462;&#21098;&#26041;&#27861;MoSo&#65292;&#23427;&#36890;&#36807;&#35780;&#20272;&#26679;&#26412;&#23545;&#26368;&#20248;&#32463;&#39564;&#39118;&#38505;&#30340;&#24433;&#21709;&#26469;&#30830;&#23450;&#27599;&#20010;&#26679;&#26412;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#19968;&#38454;&#36817;&#20284;&#22120;&#26469;&#35745;&#31639;&#26679;&#26412;&#30340;&#37325;&#35201;&#24615;&#65292;&#35813;&#36817;&#20284;&#22120;&#21482;&#38656;&#35201;&#26799;&#24230;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#20462;&#21098;&#26041;&#27861;&#31216;&#20026;&#31227;&#38500;&#21333;&#20010;&#26679;&#26412;(MoSo)&#65292;&#26088;&#22312;&#20174;&#35757;&#32451;&#38598;&#20013;&#35782;&#21035;&#24182;&#31227;&#38500;&#26368;&#19981;&#30456;&#20851;&#30340;&#26679;&#26412;&#12290;MoSo&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#36890;&#36807;&#35780;&#20272;&#26679;&#26412;&#23545;&#26368;&#20248;&#32463;&#39564;&#39118;&#38505;&#30340;&#24433;&#21709;&#26469;&#30830;&#23450;&#27599;&#20010;&#26679;&#26412;&#30340;&#37325;&#35201;&#24615;&#12290;&#36825;&#36890;&#36807;&#34913;&#37327;&#20174;&#35757;&#32451;&#38598;&#20013;&#25490;&#38500;&#19968;&#20010;&#29305;&#23450;&#26679;&#26412;&#26102;&#65292;&#32463;&#39564;&#39118;&#38505;&#30340;&#21464;&#21270;&#31243;&#24230;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#19968;&#38454;&#36817;&#20284;&#22120;&#65292;&#23427;&#20165;&#38656;&#35201;&#26469;&#33258;&#19981;&#21516;&#35757;&#32451;&#38454;&#27573;&#30340;&#26799;&#24230;&#20449;&#24687;&#65292;&#32780;&#19981;&#26159;&#20351;&#29992;&#35745;&#31639;&#19978;&#26114;&#36149;&#30340;&#36880;&#20010;&#26679;&#26412;&#37325;&#26032;&#35757;&#32451;&#30340;&#36807;&#31243;&#12290;&#25105;&#20204;&#36817;&#20284;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#65292;&#26799;&#24230;&#19982;&#35757;&#32451;&#38598;&#30340;&#24179;&#22343;&#26799;&#24230;&#19968;&#33268;&#30340;&#26679;&#26412;&#26356;&#20855;&#20449;&#24687;&#37327;&#65292;&#24182;&#19988;&#24212;&#35813;&#33719;&#24471;&#26356;&#39640;&#30340;&#20998;&#25968;&#65292;&#21487;&#20197;&#30452;&#35266;&#22320;&#29702;&#35299;&#20026;&#65306;&#22914;&#26524;&#26469;&#33258;&#29305;&#23450;&#26679;&#26412;&#30340;&#26799;&#24230;&#19982;&#24179;&#22343;&#26799;&#24230;&#21521;&#37327;&#19968;&#33268;&#65292;&#21017;&#24847;&#21619;&#30528;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a novel data-pruning approach called moving-one-sample-out (MoSo), which aims to identify and remove the least informative samples from the training set. The core insight behind MoSo is to determine the importance of each sample by assessing its impact on the optimal empirical risk. This is achieved by measuring the extent to which the empirical risk changes when a particular sample is excluded from the training set. Instead of using the computationally expensive leaving-one-out-retraining procedure, we propose an efficient first-order approximator that only requires gradient information from different training stages. The key idea behind our approximation is that samples with gradients that are consistently aligned with the average gradient of the training set are more informative and should receive higher scores, which could be intuitively understood as follows: if the gradient from a specific sample is consistent with the average gradient vector, it implies
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38754;&#21521;&#20808;&#36827;&#20154;&#24037;&#26234;&#33021;&#30340;&#31038;&#20250;&#35268;&#27169;&#39118;&#38505;&#35780;&#20272;&#30340;&#22269;&#38469;&#21512;&#20316;&#26426;&#26500;&#35299;&#20915;&#26041;&#26696;&#65292;&#25552;&#35758;&#22312;&#20154;&#24037;&#26234;&#33021;&#24320;&#21457;&#32773;&#21644;&#31532;&#19977;&#26041;&#35780;&#20272;&#20154;&#21592;&#20043;&#38388;&#24314;&#31435;&#32852;&#21512;&#20307;&#65292;&#20197;&#35299;&#20915;&#35780;&#20272;&#20154;&#21592;&#22810;&#26679;&#24615;&#26377;&#38480;&#12289;&#21162;&#21147;&#20998;&#37197;&#19981;&#29702;&#24819;&#21644;&#28608;&#21169;&#26426;&#21046;&#39072;&#20498;&#31561;&#21327;&#35843;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.14455</link><description>&lt;p&gt;
&#19968;&#20010;&#22269;&#38469;&#21512;&#20316;&#26426;&#26500;&#35780;&#20272;&#38754;&#21521;&#20808;&#36827;&#20154;&#24037;&#26234;&#33021;&#30340;&#31038;&#20250;&#35268;&#27169;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
An International Consortium for Evaluations of Societal-Scale Risks from Advanced AI. (arXiv:2310.14455v2 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14455
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38754;&#21521;&#20808;&#36827;&#20154;&#24037;&#26234;&#33021;&#30340;&#31038;&#20250;&#35268;&#27169;&#39118;&#38505;&#35780;&#20272;&#30340;&#22269;&#38469;&#21512;&#20316;&#26426;&#26500;&#35299;&#20915;&#26041;&#26696;&#65292;&#25552;&#35758;&#22312;&#20154;&#24037;&#26234;&#33021;&#24320;&#21457;&#32773;&#21644;&#31532;&#19977;&#26041;&#35780;&#20272;&#20154;&#21592;&#20043;&#38388;&#24314;&#31435;&#32852;&#21512;&#20307;&#65292;&#20197;&#35299;&#20915;&#35780;&#20272;&#20154;&#21592;&#22810;&#26679;&#24615;&#26377;&#38480;&#12289;&#21162;&#21147;&#20998;&#37197;&#19981;&#29702;&#24819;&#21644;&#28608;&#21169;&#26426;&#21046;&#39072;&#20498;&#31561;&#21327;&#35843;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#20808;&#36827;&#20154;&#24037;&#26234;&#33021;&#30340;&#24555;&#36895;&#21457;&#23637;&#21644;&#21069;&#27839;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#39118;&#38505;&#65292;&#20154;&#24037;&#26234;&#33021;&#27835;&#29702;&#21644;&#30417;&#31649;&#26041;&#26696;&#30340;&#21019;&#24314;&#21644;&#23454;&#26045;&#24212;&#35813;&#20248;&#20808;&#32771;&#34385;&#24182;&#36827;&#34892;&#22823;&#37327;&#25237;&#36164;&#12290;&#28982;&#32780;&#65292;&#29616;&#29366;&#26159;&#38590;&#20197;&#32500;&#25345;&#30340;&#65292;&#32780;&#19988;&#21361;&#38505;&#12290;&#30417;&#31649;&#32570;&#21475;&#20351;&#24471;&#20154;&#24037;&#26234;&#33021;&#23454;&#39564;&#23460;&#21487;&#20197;&#22312;&#24456;&#23569;&#30417;&#30563;&#19979;&#36827;&#34892;&#30740;&#31350;&#12289;&#24320;&#21457;&#21644;&#37096;&#32626;&#27963;&#21160;&#12290;&#20316;&#20026;&#22238;&#24212;&#65292;&#25552;&#20986;&#20102;&#21069;&#27839;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#35780;&#20272;&#20316;&#20026;&#35780;&#20272;&#21069;&#27839;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#39118;&#38505;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#26032;&#20852;&#30340;&#20154;&#24037;&#26234;&#33021;&#39118;&#38505;&#35780;&#20272;&#29983;&#24577;&#31995;&#32479;&#38754;&#20020;&#30528;&#37325;&#22823;&#30340;&#21327;&#35843;&#25361;&#25112;&#65292;&#20363;&#22914;&#35780;&#20272;&#20154;&#21592;&#30340;&#22810;&#26679;&#24615;&#26377;&#38480;&#12289;&#21162;&#21147;&#20998;&#37197;&#19981;&#29702;&#24819;&#21644;&#28608;&#21169;&#26426;&#21046;&#39072;&#20498;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#36890;&#36807;&#19968;&#20010;&#30001;&#20154;&#24037;&#26234;&#33021;&#24320;&#21457;&#32773;&#21644;&#31532;&#19977;&#26041;&#39118;&#38505;&#35780;&#20272;&#20154;&#21592;&#32452;&#25104;&#30340;&#22269;&#38469;&#32852;&#21512;&#20307;&#26469;&#36827;&#34892;&#20154;&#24037;&#26234;&#33021;&#39118;&#38505;&#35780;&#20272;&#12290;&#36825;&#26679;&#30340;&#32852;&#21512;&#20307;&#21487;&#20197;&#21457;&#25381;&#20915;&#23450;&#24615;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given rapid progress toward advanced AI and risks from frontier AI systems (advanced AI systems pushing the boundaries of the AI capabilities frontier), the creation and implementation of AI governance and regulatory schemes deserves prioritization and substantial investment. However, the status quo is untenable and, frankly, dangerous. A regulatory gap has permitted AI labs to conduct research, development, and deployment activities with minimal oversight. In response, frontier AI system evaluations have been proposed as a way of assessing risks from the development and deployment of frontier AI systems. Yet, the budding AI risk evaluation ecosystem faces significant coordination challenges, such as a limited diversity of evaluators, suboptimal allocation of effort, and perverse incentives. This paper proposes a solution in the form of an international consortium for AI risk evaluations, comprising both AI developers and third-party AI risk evaluators. Such a consortium could play a c
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#36229;&#36234;&#20256;&#32479;&#24037;&#20855;&#30340;&#39640;&#32423;&#33521;&#35793;&#38463;&#25289;&#20271;&#35821;&#32763;&#35793;&#22120;&#65292;&#20351;&#29992;&#36203;&#23572;&#36763;&#22522;&#21464;&#21387;&#22120;&#21644;&#32431;&#25991;&#23398;&#38463;&#25289;&#20271;&#35821;&#25968;&#25454;&#38598;&#65292;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#24378;&#35843;&#20102;&#20854;&#22312;&#25991;&#21270;&#25935;&#24863;&#24615;&#21644;&#35821;&#22659;&#20934;&#30830;&#24615;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2310.13613</link><description>&lt;p&gt;
Hunayn&#65306;&#36229;&#36234;&#23383;&#38754;&#24847;&#20041;&#30340;&#32763;&#35793;&#36827;&#27493;
&lt;/p&gt;
&lt;p&gt;
Hunayn: Elevating Translation Beyond the Literal. (arXiv:2310.13613v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13613
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#36229;&#36234;&#20256;&#32479;&#24037;&#20855;&#30340;&#39640;&#32423;&#33521;&#35793;&#38463;&#25289;&#20271;&#35821;&#32763;&#35793;&#22120;&#65292;&#20351;&#29992;&#36203;&#23572;&#36763;&#22522;&#21464;&#21387;&#22120;&#21644;&#32431;&#25991;&#23398;&#38463;&#25289;&#20271;&#35821;&#25968;&#25454;&#38598;&#65292;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#24378;&#35843;&#20102;&#20854;&#22312;&#25991;&#21270;&#25935;&#24863;&#24615;&#21644;&#35821;&#22659;&#20934;&#30830;&#24615;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#39033;&#30446;&#20171;&#32461;&#20102;&#19968;&#31181;&#36229;&#36234;&#20256;&#32479;&#24037;&#20855;&#30340;&#39640;&#32423;&#33521;&#35793;&#38463;&#25289;&#20271;&#35821;&#32763;&#35793;&#22120;&#12290;&#21033;&#29992;&#36203;&#23572;&#36763;&#22522;&#21464;&#21387;&#22120;&#65288;MarianMT&#65289;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#22312;&#19968;&#20010;&#33258;&#21160;&#29983;&#25104;&#30340;&#12289;&#32431;&#25991;&#23398;&#38463;&#25289;&#20271;&#35821;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#19982;&#35895;&#27468;&#32763;&#35793;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#22312;&#23450;&#24615;&#35780;&#20272;&#20013;&#22987;&#32456;&#34920;&#29616;&#20986;&#33394;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#23427;&#22312;&#25991;&#21270;&#25935;&#24863;&#24615;&#21644;&#35821;&#22659;&#20934;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#36825;&#39033;&#30740;&#31350;&#24378;&#35843;&#20102;&#36203;&#23572;&#36763;&#22522;&#21464;&#21387;&#22120;&#22312;&#20351;&#29992;Fusha&#25968;&#25454;&#38598;&#30340;&#33521;&#38463;&#32763;&#35793;&#20013;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
This project introduces an advanced English-to-Arabic translator surpassing conventional tools. Leveraging the Helsinki transformer (MarianMT), our approach involves fine-tuning on a self-scraped, purely literary Arabic dataset. Evaluations against Google Translate show consistent outperformance in qualitative assessments. Notably, it excels in cultural sensitivity and context accuracy. This research underscores the Helsinki transformer's superiority for English-to-Arabic translation using a Fusha dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#36229;&#20687;&#32032;&#32467;&#26500;&#24046;&#24322;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;MDGCN&#65289;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#34920;&#24449;&#65292;&#36890;&#36807;&#32858;&#31867;&#24863;&#30693;&#30456;&#20284;&#20687;&#32032;&#65292;&#20943;&#23569;&#20102;&#21518;&#32493;&#22788;&#29702;&#30340;&#35270;&#35273;&#22522;&#20803;&#25968;&#37327;&#65292;&#24182;&#25366;&#25496;&#20102;&#26356;&#31934;&#30830;&#30340;&#25299;&#25169;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2310.13447</link><description>&lt;p&gt;
&#22810;&#23610;&#24230;&#36229;&#20687;&#32032;&#32467;&#26500;&#24046;&#24322;&#22270;&#21367;&#31215;&#32593;&#32476;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#34920;&#24449;
&lt;/p&gt;
&lt;p&gt;
Multiscale Superpixel Structured Difference Graph Convolutional Network for VL Representation. (arXiv:2310.13447v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13447
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#36229;&#20687;&#32032;&#32467;&#26500;&#24046;&#24322;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;MDGCN&#65289;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#34920;&#24449;&#65292;&#36890;&#36807;&#32858;&#31867;&#24863;&#30693;&#30456;&#20284;&#20687;&#32032;&#65292;&#20943;&#23569;&#20102;&#21518;&#32493;&#22788;&#29702;&#30340;&#35270;&#35273;&#22522;&#20803;&#25968;&#37327;&#65292;&#24182;&#25366;&#25496;&#20102;&#26356;&#31934;&#30830;&#30340;&#25299;&#25169;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#27169;&#24577;&#39046;&#22495;&#20013;&#65292;&#25972;&#21512;&#35270;&#35273;&#21644;&#35821;&#35328;&#30340;&#20851;&#38190;&#22312;&#20110;&#24314;&#31435;&#19968;&#20010;&#33391;&#22909;&#30340;&#23545;&#40784;&#31574;&#30053;&#12290;&#26368;&#36817;&#65292;&#21463;&#21040;&#33258;&#30417;&#30563;&#23398;&#20064;&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#22522;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#30340;&#22810;&#27169;&#24577;&#35821;&#20041;&#34920;&#24449;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#35270;&#35273;&#35821;&#20041;&#34920;&#24449;&#20173;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;&#24403;&#21069;&#22522;&#20110;&#20687;&#32032;&#25110;&#22359;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#25552;&#21462;&#22797;&#26434;&#22330;&#26223;&#36793;&#30028;&#26041;&#38754;&#23384;&#22312;&#31354;&#38388;&#35821;&#20041;&#36830;&#36143;&#24615;&#19981;&#36275;&#21644;&#23545;&#22122;&#22768;&#30340;&#33030;&#24369;&#24615;&#30340;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#23558;&#36229;&#20687;&#32032;&#20316;&#20026;&#21487;&#23398;&#20064;&#22270;&#20687;&#25968;&#25454;&#30340;&#32508;&#21512;&#32039;&#20945;&#34920;&#24449;&#65292;&#36890;&#36807;&#23545;&#24863;&#30693;&#30456;&#20284;&#20687;&#32032;&#36827;&#34892;&#32858;&#31867;&#65292;&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;&#21518;&#32493;&#22788;&#29702;&#30340;&#35270;&#35273;&#22522;&#20803;&#25968;&#37327;&#12290;&#20026;&#20102;&#25366;&#25496;&#26356;&#31934;&#30830;&#30340;&#25299;&#25169;&#20851;&#31995;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#24046;&#24322;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;MDGCN&#65289;&#12290;&#23427;&#23558;&#25972;&#20010;&#22270;&#20687;&#35299;&#26512;&#20026;&#32454;&#21040;&#31895;&#30340;&#23618;&#27425;&#32467;&#26500;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#25972;&#20010;&#22270;&#20687;&#30340;&#35299;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Within the multimodal field, the key to integrating vision and language lies in establishing a good alignment strategy. Recently, benefiting from the success of self-supervised learning, significant progress has been made in multimodal semantic representation based on pre-trained models for vision and language. However, there is still room for improvement in visual semantic representation. The lack of spatial semantic coherence and vulnerability to noise makes it challenging for current pixel or patch-based methods to accurately extract complex scene boundaries. To this end, this paper develops superpixel as a comprehensive compact representation of learnable image data, which effectively reduces the number of visual primitives for subsequent processing by clustering perceptually similar pixels. To mine more precise topological relations, we propose a Multiscale Difference Graph Convolutional Network (MDGCN). It parses the entire image as a fine-to-coarse hierarchical structure of cons
&lt;/p&gt;</description></item><item><title>CLAIR&#26159;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#26426;&#22120;&#29983;&#25104;&#30340;&#22270;&#20687;&#26631;&#39064;&#12290;&#30456;&#23545;&#20110;&#29616;&#26377;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;CLAIR&#22312;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#30456;&#20851;&#24615;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#38024;&#23545;&#20855;&#20307;&#25968;&#25454;&#38598;&#21462;&#24471;&#20102;&#36739;&#22823;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2310.12971</link><description>&lt;p&gt;
CLAIR: &#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#22270;&#20687;&#26631;&#39064;
&lt;/p&gt;
&lt;p&gt;
CLAIR: Evaluating Image Captions with Large Language Models. (arXiv:2310.12971v1 [cs.CV] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12971
&lt;/p&gt;
&lt;p&gt;
CLAIR&#26159;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#26426;&#22120;&#29983;&#25104;&#30340;&#22270;&#20687;&#26631;&#39064;&#12290;&#30456;&#23545;&#20110;&#29616;&#26377;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;CLAIR&#22312;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#30456;&#20851;&#24615;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#38024;&#23545;&#20855;&#20307;&#25968;&#25454;&#38598;&#21462;&#24471;&#20102;&#36739;&#22823;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#29983;&#25104;&#22270;&#20687;&#26631;&#39064;&#30340;&#35780;&#20272;&#26159;&#19968;&#20010;&#26377;&#36259;&#20294;&#25345;&#20037;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#26377;&#25928;&#30340;&#35780;&#20272;&#25351;&#26631;&#24517;&#39035;&#32771;&#34385;&#22810;&#20010;&#30456;&#20284;&#24615;&#32500;&#24230;&#65292;&#21253;&#25324;&#35821;&#20041;&#30456;&#20851;&#24615;&#12289;&#35270;&#35273;&#32467;&#26500;&#12289;&#29289;&#20307;&#20132;&#20114;&#12289;&#26631;&#39064;&#22810;&#26679;&#24615;&#21644;&#29305;&#23450;&#24615;&#12290;&#29616;&#26377;&#30340;&#39640;&#24230;&#24037;&#31243;&#21270;&#30340;&#35780;&#20272;&#26041;&#27861;&#35797;&#22270;&#25429;&#25417;&#29305;&#23450;&#26041;&#38754;&#65292;&#20294;&#22312;&#25552;&#20379;&#19982;&#20154;&#31867;&#21028;&#26029;&#23494;&#20999;&#19968;&#33268;&#30340;&#25972;&#20307;&#35780;&#20998;&#26041;&#38754;&#20173;&#26377;&#19981;&#36275;&#20043;&#22788;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CLAIR&#65292;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#38646;&#23556;&#35821;&#35328;&#24314;&#27169;&#33021;&#21147;&#26469;&#35780;&#20272;&#20505;&#36873;&#26631;&#39064;&#30340;&#26032;&#26041;&#27861;&#12290;&#22312;&#25105;&#20204;&#30340;&#35780;&#20272;&#20013;&#65292;CLAIR&#30456;&#23545;&#20110;&#29616;&#26377;&#25351;&#26631;&#26356;&#33021;&#19982;&#20154;&#31867;&#23545;&#26631;&#39064;&#36136;&#37327;&#30340;&#21028;&#26029;&#30456;&#20851;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;Flickr8K-Expert&#19978;&#65292;CLAIR&#22312;&#19982;SPICE&#30456;&#27604;&#30340;&#30456;&#20851;&#25913;&#36827;&#26041;&#38754;&#25552;&#39640;&#20102;39.6&#65285;&#65292;&#22312;&#19982;RefCLIP-S&#31561;&#22270;&#20687;&#22686;&#24378;&#26041;&#27861;&#30456;&#27604;&#30340;&#30456;&#20851;&#25913;&#36827;&#26041;&#38754;&#25552;&#39640;&#20102;18.3&#65285;&#12290;&#27492;&#22806;&#65292;CLAIR&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#24615;&#32467;&#26524;&#65292;&#20801;&#35768;&#35821;&#35328;&#27169;&#22411;&#35782;&#21035;u
&lt;/p&gt;
&lt;p&gt;
The evaluation of machine-generated image captions poses an interesting yet persistent challenge. Effective evaluation measures must consider numerous dimensions of similarity, including semantic relevance, visual structure, object interactions, caption diversity, and specificity. Existing highly-engineered measures attempt to capture specific aspects, but fall short in providing a holistic score that aligns closely with human judgments. Here, we propose CLAIR, a novel method that leverages the zero-shot language modeling capabilities of large language models (LLMs) to evaluate candidate captions. In our evaluations, CLAIR demonstrates a stronger correlation with human judgments of caption quality compared to existing measures. Notably, on Flickr8K-Expert, CLAIR achieves relative correlation improvements over SPICE of 39.6% and over image-augmented methods such as RefCLIP-S of 18.3%. Moreover, CLAIR provides noisily interpretable results by allowing the language model to identify the u
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35843;&#26597;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#35774;&#35745;MOEA&#25805;&#20316;&#31526;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#36866;&#24403;&#30340;&#25552;&#31034;&#24037;&#31243;&#65292;&#25104;&#21151;&#23558;&#36890;&#29992;&#30340;LLM&#20197;&#38646;-shot&#26041;&#24335;&#20316;&#20026;MOEA/D&#30340;&#40657;&#30418;&#25628;&#32034;&#25805;&#20316;&#31526;&#65292;&#24182;&#36890;&#36807;&#20174;LLM&#34892;&#20026;&#20013;&#23398;&#20064;&#35774;&#35745;&#20102;&#19968;&#20010;&#26174;&#24615;&#30340;&#30333;&#30418;&#25805;&#20316;&#31526;&#12290;</title><link>http://arxiv.org/abs/2310.12541</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#22810;&#30446;&#26631;&#36827;&#21270;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Large Language Model for Multi-objective Evolutionary Optimization. (arXiv:2310.12541v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12541
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35843;&#26597;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#35774;&#35745;MOEA&#25805;&#20316;&#31526;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#36866;&#24403;&#30340;&#25552;&#31034;&#24037;&#31243;&#65292;&#25104;&#21151;&#23558;&#36890;&#29992;&#30340;LLM&#20197;&#38646;-shot&#26041;&#24335;&#20316;&#20026;MOEA/D&#30340;&#40657;&#30418;&#25628;&#32034;&#25805;&#20316;&#31526;&#65292;&#24182;&#36890;&#36807;&#20174;LLM&#34892;&#20026;&#20013;&#23398;&#20064;&#35774;&#35745;&#20102;&#19968;&#20010;&#26174;&#24615;&#30340;&#30333;&#30418;&#25805;&#20316;&#31526;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#30446;&#26631;&#36827;&#21270;&#31639;&#27861;&#65288;MOEAs&#65289;&#26159;&#35299;&#20915;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65288;MOPs&#65289;&#30340;&#20027;&#35201;&#26041;&#27861;&#12290;&#22312;&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#65292;&#25552;&#20986;&#20102;&#35768;&#22810;MOEAs&#65292;&#20854;&#25805;&#20316;&#31526;&#38656;&#35201;&#36890;&#36807;&#39046;&#22495;&#30693;&#35782;&#36827;&#34892;&#31934;&#24515;&#35774;&#35745;&#12290;&#26368;&#36817;&#65292;&#19968;&#20123;&#23581;&#35797;&#23558;MOEAs&#20013;&#25163;&#21160;&#35774;&#35745;&#30340;&#25805;&#20316;&#31526;&#26367;&#25442;&#20026;&#22522;&#20110;&#23398;&#20064;&#30340;&#25805;&#20316;&#31526;&#65288;&#22914;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65289;&#24050;&#32463;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#35774;&#35745;&#21644;&#35757;&#32451;&#36825;&#26679;&#30340;&#27169;&#22411;&#20173;&#28982;&#38656;&#35201;&#22823;&#37327;&#30340;&#24037;&#20316;&#65292;&#24182;&#19988;&#23398;&#20064;&#21040;&#30340;&#25805;&#20316;&#31526;&#21487;&#33021;&#19981;&#33021;&#24456;&#22909;&#22320;&#25512;&#24191;&#21040;&#35299;&#20915;&#26032;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#21033;&#29992;&#24378;&#22823;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#35774;&#35745;MOEA&#25805;&#20316;&#31526;&#30340;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#36866;&#24403;&#30340;&#25552;&#31034;&#24037;&#31243;&#65292;&#25105;&#20204;&#25104;&#21151;&#22320;&#35753;&#19968;&#20010;&#36890;&#29992;&#30340;LLM&#20197;&#38646;-shot&#30340;&#26041;&#24335;&#20316;&#20026;&#20998;&#35299;&#22411;MOEA&#65288;MOEA/D&#65289;&#30340;&#40657;&#30418;&#25628;&#32034;&#25805;&#20316;&#31526;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#20174;LLM&#34892;&#20026;&#20013;&#23398;&#20064;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35774;&#35745;&#20102;&#19968;&#20010;&#26174;&#24615;&#30340;&#30333;&#30418;&#25805;&#20316;&#31526;&#65292;&#24182;&#25552;&#20986;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Multiobjective evolutionary algorithms (MOEAs) are major methods for solving multiobjective optimization problems (MOPs). Many MOEAs have been proposed in the past decades, of which the operators need carefully handcrafted design with domain knowledge. Recently, some attempts have been made to replace the manually designed operators in MOEAs with learning-based operators (e.g., neural network models). However, much effort is still required for designing and training such models, and the learned operators might not generalize well to solve new problems. To tackle the above challenges, this work investigates a novel approach that leverages the powerful large language model (LLM) to design MOEA operators. With proper prompt engineering, we successfully let a general LLM serve as a black-box search operator for decomposition-based MOEA (MOEA/D) in a zero-shot manner. In addition, by learning from the LLM behavior, we further design an explicit white-box operator with randomness and propose
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#20154;&#28216;&#25103;&#20013;&#33258;&#25105;&#23545;&#25239;&#30340;&#20445;&#35777;&#38382;&#39064;&#65292;&#36890;&#36807;&#22810;&#30697;&#38453;&#21487;&#20998;&#35299;&#24615;&#65292;&#22312;&#28385;&#36275;&#19968;&#23450;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#33258;&#25105;&#23545;&#25239;&#23398;&#20064;&#30340;&#31639;&#27861;&#33021;&#22815;&#20135;&#29983;&#26377;&#30028;&#33030;&#24369;&#24615;&#30340;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2310.11518</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#30697;&#38453;&#21487;&#20998;&#35299;&#24615;&#22312;&#22810;&#20154;&#28216;&#25103;&#20013;&#23545;&#33258;&#25105;&#23545;&#25239;&#30340;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Guarantees for Self-Play in Multiplayer Games via Polymatrix Decomposability. (arXiv:2310.11518v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11518
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#20154;&#28216;&#25103;&#20013;&#33258;&#25105;&#23545;&#25239;&#30340;&#20445;&#35777;&#38382;&#39064;&#65292;&#36890;&#36807;&#22810;&#30697;&#38453;&#21487;&#20998;&#35299;&#24615;&#65292;&#22312;&#28385;&#36275;&#19968;&#23450;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#33258;&#25105;&#23545;&#25239;&#23398;&#20064;&#30340;&#31639;&#27861;&#33021;&#22815;&#20135;&#29983;&#26377;&#30028;&#33030;&#24369;&#24615;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#25105;&#23545;&#25239;&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#22312;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#30340;&#25216;&#26415;&#65292;&#20854;&#20013;&#23398;&#20064;&#31639;&#27861;&#36890;&#36807;&#19982;&#33258;&#36523;&#30340;&#21103;&#26412;&#20132;&#20114;&#26469;&#23398;&#20064;&#12290;&#33258;&#25105;&#23545;&#25239;&#23545;&#20110;&#29983;&#25104;&#22823;&#37327;&#30340;&#23398;&#20064;&#25968;&#25454;&#24456;&#26377;&#29992;&#65292;&#20294;&#23427;&#30340;&#32570;&#28857;&#26159;&#35757;&#32451;&#21518;&#23398;&#20064;&#32773;&#23558;&#38754;&#23545;&#30340;&#26234;&#33021;&#20307;&#21487;&#33021;&#19982;&#36890;&#36807;&#19982;&#33258;&#36523;&#20132;&#20114;&#26102;&#25152;&#26399;&#26395;&#30340;&#26234;&#33021;&#20307;&#34892;&#20026;&#25130;&#28982;&#19981;&#21516;&#12290;&#23545;&#20110;&#20004;&#20154;&#24120;&#21644;&#28216;&#25103;&#30340;&#29305;&#27530;&#24773;&#20917;&#65292;&#36798;&#21040;&#32435;&#20160;&#22343;&#34913;&#30340;&#33258;&#25105;&#23545;&#25239;&#33021;&#22815;&#20445;&#35777;&#20135;&#29983;&#23545;&#20219;&#20309;&#35757;&#32451;&#21518;&#23545;&#25163;&#34920;&#29616;&#33391;&#22909;&#30340;&#31574;&#30053;&#65307;&#28982;&#32780;&#65292;&#23545;&#20110;&#22810;&#20154;&#28216;&#25103;&#26469;&#35828;&#27809;&#26377;&#36825;&#26679;&#30340;&#20445;&#35777;&#23384;&#22312;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#36817;&#20284;&#20998;&#35299;&#20026;&#19968;&#32452;&#20004;&#20154;&#24120;&#21644;&#28216;&#25103;&#65288;&#31216;&#20026;&#22810;&#30697;&#38453;&#28216;&#25103;&#65289;&#30340;&#28216;&#25103;&#20013;&#65292;&#20854;&#20013;&#20840;&#23616; $\epsilon$-&#32435;&#20160;&#22343;&#34913;&#22312;&#27599;&#20010;&#23376;&#28216;&#25103;&#20013;&#37117;&#19982;&#32435;&#20160;&#22343;&#34913;&#26377;&#26377;&#30028;&#36317;&#31163;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#33258;&#25105;&#23545;&#25239;&#23398;&#20064;&#30340;&#26080;&#22806;&#37096;&#36951;&#25022;&#31639;&#27861;&#23558;&#20135;&#29983;&#19968;&#20010;&#26377;&#30028;&#33030;&#24369;&#24615;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#39318;&#27425;&#30830;&#23450;&#20102;&#8230;&#8230;
&lt;/p&gt;
&lt;p&gt;
Self-play is a technique for machine learning in multi-agent systems where a learning algorithm learns by interacting with copies of itself. Self-play is useful for generating large quantities of data for learning, but has the drawback that the agents the learner will face post-training may have dramatically different behavior than the learner came to expect by interacting with itself. For the special case of two-player constant-sum games, self-play that reaches Nash equilibrium is guaranteed to produce strategies that perform well against any post-training opponent; however, no such guarantee exists for multi-player games. We show that in games that approximately decompose into a set of two-player constant-sum games (called polymatrix games) where global $\epsilon$-Nash equilibria are boundedly far from Nash-equilibria in each subgame, any no-external-regret algorithm that learns by self-play will produce a strategy with bounded vulnerability. For the first time, our results identify 
&lt;/p&gt;</description></item><item><title>ACES&#26159;&#19968;&#31181;&#20351;&#29992;&#33258;&#25105;&#30446;&#26631;&#35821;&#35328;&#27169;&#22411;&#21644;&#35821;&#20041;&#25551;&#36848;&#31526;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#32534;&#31243;&#38590;&#39064;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20248;&#21270;&#26377;&#36259;&#30340;&#22810;&#26679;&#24615;&#21644;&#23569;&#26679;&#26412;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2310.10692</link><description>&lt;p&gt;
ACES: &#20351;&#29992;&#33258;&#25105;&#30446;&#26631;&#35821;&#35328;&#27169;&#22411;&#21644;&#35821;&#20041;&#25551;&#36848;&#31526;&#29983;&#25104;&#22810;&#26679;&#30340;&#32534;&#31243;&#38590;&#39064;
&lt;/p&gt;
&lt;p&gt;
ACES: generating diverse programming puzzles with autotelic language models and semantic descriptors. (arXiv:2310.10692v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10692
&lt;/p&gt;
&lt;p&gt;
ACES&#26159;&#19968;&#31181;&#20351;&#29992;&#33258;&#25105;&#30446;&#26631;&#35821;&#35328;&#27169;&#22411;&#21644;&#35821;&#20041;&#25551;&#36848;&#31526;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#32534;&#31243;&#38590;&#39064;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20248;&#21270;&#26377;&#36259;&#30340;&#22810;&#26679;&#24615;&#21644;&#23569;&#26679;&#26412;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23547;&#25214;&#21644;&#36873;&#25321;&#26032;&#39062;&#26377;&#36259;&#30340;&#38382;&#39064;&#26159;&#22909;&#22855;&#24515;&#12289;&#31185;&#23398;&#21644;&#21019;&#26032;&#30340;&#26680;&#24515;&#12290;&#22312;Python&#32534;&#31243;&#38590;&#39064;&#30340;&#26080;&#38480;&#31354;&#38388;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#33258;&#21160;&#38382;&#39064;&#29983;&#25104;&#12290;&#29616;&#26377;&#30340;&#29983;&#25104;&#27169;&#22411;&#36890;&#24120;&#26088;&#22312;&#24314;&#27169;&#21442;&#32771;&#20998;&#24067;&#65292;&#27809;&#26377;&#26126;&#30830;&#30340;&#22810;&#26679;&#24615;&#20248;&#21270;&#12290;&#20854;&#20182;&#26041;&#27861;&#22312;&#26377;&#38480;&#30340;&#25163;&#24037;&#32534;&#30721;&#34920;&#31034;&#31354;&#38388;&#25110;&#19981;&#21487;&#35299;&#37322;&#30340;&#23398;&#20064;&#23884;&#20837;&#31354;&#38388;&#20013;&#26126;&#30830;&#20248;&#21270;&#22810;&#26679;&#24615;&#65292;&#36825;&#20123;&#23884;&#20837;&#31354;&#38388;&#21487;&#33021;&#19982;&#20154;&#31867;&#23545;&#26377;&#36259;&#21464;&#21270;&#30340;&#24863;&#30693;&#19981;&#31526;&#12290;&#36890;&#36807;ACES&#65288;&#33258;&#25105;&#30446;&#26631;&#20195;&#30721;&#25506;&#32034;&#19982;&#35821;&#20041;&#25551;&#36848;&#31526;&#65289;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#25105;&#30446;&#26631;&#29983;&#25104;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#29983;&#25104;&#35821;&#20041;&#25551;&#36848;&#31526;&#65292;&#30452;&#25509;&#20248;&#21270;&#26377;&#36259;&#30340;&#22810;&#26679;&#24615;&#65292;&#20197;&#21450;&#23569;&#26679;&#26412;&#29983;&#25104;&#12290;&#27599;&#20010;&#38590;&#39064;&#37117;&#26631;&#35760;&#26377;10&#20010;&#32500;&#24230;&#65292;&#27599;&#20010;&#32500;&#24230;&#25429;&#25417;&#20102;&#35299;&#20915;&#23427;&#25152;&#38656;&#30340;&#32534;&#31243;&#25216;&#33021;&#12290;ACES&#29983;&#25104;&#24182;&#36861;&#27714;&#26032;&#39062;&#21487;&#34892;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finding and selecting new and interesting problems to solve is at the heart of curiosity, science and innovation. We here study automated problem generation in the context of the open-ended space of python programming puzzles. Existing generative models often aim at modeling a reference distribution without any explicit diversity optimization. Other methods explicitly optimizing for diversity do so either in limited hand-coded representation spaces or in uninterpretable learned embedding spaces that may not align with human perceptions of interesting variations. With ACES (Autotelic Code Exploration via Semantic descriptors), we introduce a new autotelic generation method that leverages semantic descriptors produced by a large language model (LLM) to directly optimize for interesting diversity, as well as few-shot-based generation. Each puzzle is labeled along 10 dimensions, each capturing a programming skill required to solve it. ACES generates and pursues novel and feasible goals to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25913;&#36827;&#25688;&#35201;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#20154;&#24037;&#32534;&#36753;&#30340;&#21453;&#39304;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#24207;&#21015;&#23545;&#40784;&#65288;&#19981;&#65289;&#20284;&#28982;&#35757;&#32451;(SALT)&#25216;&#26415;&#23558;&#20154;&#24037;&#32534;&#36753;&#25968;&#25454;&#19982;&#27169;&#22411;&#29983;&#25104;&#25968;&#25454;&#32467;&#21512;&#36215;&#26469;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#21307;&#23398;&#39046;&#22495;&#25688;&#35201;&#29983;&#25104;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.05857</link><description>&lt;p&gt;
&#20351;&#29992;&#20154;&#24037;&#32534;&#36753;&#25913;&#36827;&#25688;&#35201;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Improving Summarization with Human Edits. (arXiv:2310.05857v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05857
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25913;&#36827;&#25688;&#35201;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#20154;&#24037;&#32534;&#36753;&#30340;&#21453;&#39304;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#24207;&#21015;&#23545;&#40784;&#65288;&#19981;&#65289;&#20284;&#28982;&#35757;&#32451;(SALT)&#25216;&#26415;&#23558;&#20154;&#24037;&#32534;&#36753;&#25968;&#25454;&#19982;&#27169;&#22411;&#29983;&#25104;&#25968;&#25454;&#32467;&#21512;&#36215;&#26469;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#21307;&#23398;&#39046;&#22495;&#25688;&#35201;&#29983;&#25104;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#33539;&#24335;&#23398;&#20064;&#21487;&#20197;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#22312;&#36890;&#29992;&#39046;&#22495;&#25277;&#35937;&#21270;&#25688;&#35201;&#29983;&#25104;&#20013;&#20351;&#29992;&#20154;&#31867;&#21453;&#39304;&#26469;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#65292;&#24182;&#33719;&#24471;&#20102;&#36229;&#36234;&#20256;&#32479;&#20284;&#28982;&#35757;&#32451;&#30340;&#25688;&#35201;&#36136;&#37327;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#19968;&#31181;&#36739;&#23569;&#25506;&#32034;&#30340;&#20154;&#31867;&#21453;&#39304;&#24418;&#24335;&#8212;&#8212;&#20154;&#24037;&#32534;&#36753;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#8212;&#8212;&#24207;&#21015;&#23545;&#40784;&#65288;&#19981;&#65289;&#20284;&#28982;&#35757;&#32451;(SALT)&#65292;&#22312;&#35757;&#32451;&#24490;&#29615;&#20013;&#21516;&#26102;&#20351;&#29992;&#20154;&#24037;&#32534;&#36753;&#21644;&#27169;&#22411;&#29983;&#25104;&#30340;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#20351;&#29992;&#29616;&#26377;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#22522;&#20934;&#25688;&#35201;&#26469;&#27169;&#25311;&#20154;&#24037;&#32534;&#36753;&#65292;&#20197;&#21450;&#22312;&#35757;&#32451;&#21518;&#33719;&#21462;&#30340;&#27169;&#22411;&#29983;&#25104;&#25688;&#35201;&#65292;&#20197;&#20943;&#23569;&#23545;&#26114;&#36149;&#30340;&#20154;&#24037;&#32534;&#36753;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23558;&#20154;&#31867;&#21453;&#39304;&#30340;&#25506;&#32034;&#20174;&#36890;&#29992;&#39046;&#22495;&#25688;&#35201;&#29983;&#25104;&#25193;&#23637;&#21040;&#21307;&#23398;&#39046;&#22495;&#25688;&#35201;&#29983;&#25104;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;SALT&#22312;&#25913;&#36827;&#25688;&#35201;&#29983;&#25104;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work has shown the promise of learning with human feedback paradigms to produce human-determined high-quality text. Existing works use human feedback to train large language models (LLMs) in general domain abstractive summarization and have obtained summary quality exceeding traditional likelihood training. In this paper, we focus on a less explored form of human feedback -- Human Edits. We propose Sequence Alignment (un)Likelihood Training (SALT), a novel technique to use both the human-edited and model-generated data together in the training loop. In addition, we demonstrate simulating Human Edits with ground truth summaries coming from existing training data -Imitation edits, along with the model-generated summaries obtained after the training, to reduce the need for expensive human-edit data. In our experiments, we extend human feedback exploration from general domain summarization to medical domain summarization. Our results demonstrate the effectiveness of SALT in improv
&lt;/p&gt;</description></item><item><title>CodeTransOcean&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#22810;&#35821;&#35328;&#20195;&#30721;&#32763;&#35793;&#22522;&#20934;&#65292;&#21253;&#21547;&#22810;&#20010;&#21019;&#26032;&#30340;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#24182;&#28385;&#36275;&#20102;&#29616;&#23454;&#24212;&#29992;&#30340;&#22810;&#26679;&#21270;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2310.04951</link><description>&lt;p&gt;
CodeTransOcean: &#19968;&#20010;&#35206;&#30422;&#22810;&#31181;&#35821;&#35328;&#30340;&#20840;&#38754;&#20195;&#30721;&#32763;&#35793;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
CodeTransOcean: A Comprehensive Multilingual Benchmark for Code Translation. (arXiv:2310.04951v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04951
&lt;/p&gt;
&lt;p&gt;
CodeTransOcean&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#22810;&#35821;&#35328;&#20195;&#30721;&#32763;&#35793;&#22522;&#20934;&#65292;&#21253;&#21547;&#22810;&#20010;&#21019;&#26032;&#30340;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#24182;&#28385;&#36275;&#20102;&#29616;&#23454;&#24212;&#29992;&#30340;&#22810;&#26679;&#21270;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#20195;&#30721;&#32763;&#35793;&#25216;&#26415;&#21033;&#29992;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#23558;&#28304;&#20195;&#30721;&#20174;&#19968;&#31181;&#32534;&#31243;&#35821;&#35328;&#32763;&#35793;&#25104;&#21478;&#19968;&#31181;&#65292;&#20197;&#28385;&#36275;&#29983;&#20135;&#20860;&#23481;&#24615;&#25110;&#25552;&#39640;&#20195;&#30721;&#24211;&#32500;&#25252;&#25928;&#29575;&#30340;&#38656;&#27714;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#20195;&#30721;&#32763;&#35793;&#25968;&#25454;&#38598;&#21482;&#20851;&#27880;&#19968;&#23545;&#27969;&#34892;&#32534;&#31243;&#35821;&#35328;&#12290;&#20026;&#20102;&#25512;&#36827;&#20195;&#30721;&#32763;&#35793;&#30340;&#30740;&#31350;&#24182;&#28385;&#36275;&#29616;&#23454;&#24212;&#29992;&#30340;&#22810;&#26679;&#21270;&#38656;&#27714;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;CodeTransOcean&#65292;&#36825;&#26159;&#19968;&#20010;&#25903;&#25345;&#26368;&#22810;&#31181;&#32534;&#31243;&#35821;&#35328;&#30340;&#22823;&#35268;&#27169;&#32508;&#21512;&#22522;&#20934;&#12290;CodeTransOcean&#21253;&#25324;&#19977;&#20010;&#21019;&#26032;&#30340;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#20998;&#21035;&#26159;&#25903;&#25345;&#22810;&#31181;&#27969;&#34892;&#32534;&#31243;&#35821;&#35328;&#20043;&#38388;&#32763;&#35793;&#30340;MultilingualTrans&#65292;&#29992;&#20110;&#22312;&#23567;&#20247;&#32534;&#31243;&#35821;&#35328;&#21644;&#27969;&#34892;&#32534;&#31243;&#35821;&#35328;&#20043;&#38388;&#32763;&#35793;&#30340;NicheTrans&#65292;&#20197;&#21450;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#32763;&#35793;&#20195;&#30721;&#30340;&#21487;&#25191;&#34892;&#24615;&#30340;LLMTrans&#12290;CodeTransOcean&#36824;&#21253;&#25324;&#19968;&#20010;&#26032;&#39062;&#30340;&#36328;&#26694;&#26550;&#25968;&#25454;&#38598;DLTrans&#65292;&#29992;&#20110;&#32763;&#35793;...
&lt;/p&gt;
&lt;p&gt;
Recent code translation techniques exploit neural machine translation models to translate source code from one programming language to another to satisfy production compatibility or to improve efficiency of codebase maintenance. Most existing code translation datasets only focus on a single pair of popular programming languages. To advance research on code translation and meet diverse requirements of real-world applications, we construct CodeTransOcean, a large-scale comprehensive benchmark that supports the largest variety of programming languages for code translation. CodeTransOcean consists of three novel multilingual datasets, namely, MultilingualTrans supporting translations between multiple popular programming languages, NicheTrans for translating between niche programming languages and popular ones, and LLMTrans for evaluating executability of translated code by large language models (LLMs). CodeTransOcean also includes a novel cross-framework dataset, DLTrans, for translating d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#30452;&#25509;&#21033;&#29992;&#37197;&#23545;&#30340;OSM&#25968;&#25454;&#21644;&#20809;&#23398;&#22270;&#20687;&#36827;&#34892;&#22303;&#22320;&#35206;&#30422;&#21464;&#21270;&#26816;&#27979;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#35937;&#24341;&#23548;&#30340;Transformer&#26550;&#26500;&#65292;&#20174;&#32780;&#25299;&#23485;&#20102;&#21464;&#21270;&#26816;&#27979;&#20219;&#21153;&#30340;&#33539;&#22260;&#65292;&#24182;&#26174;&#33879;&#20943;&#23569;&#20102;&#35745;&#31639;&#24320;&#38144;&#21644;&#20869;&#23384;&#36127;&#25285;&#12290;</title><link>http://arxiv.org/abs/2310.02674</link><description>&lt;p&gt;
&#21033;&#29992;&#37197;&#23545;&#30340;OpenStreetMap&#25968;&#25454;&#21644;&#20809;&#23398;&#39640;&#20998;&#36776;&#29575;&#24433;&#20687;&#36827;&#34892;&#22303;&#22320;&#35206;&#30422;&#21464;&#21270;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Land-cover change detection using paired OpenStreetMap data and optical high-resolution imagery via object-guided Transformer. (arXiv:2310.02674v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02674
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#30452;&#25509;&#21033;&#29992;&#37197;&#23545;&#30340;OSM&#25968;&#25454;&#21644;&#20809;&#23398;&#22270;&#20687;&#36827;&#34892;&#22303;&#22320;&#35206;&#30422;&#21464;&#21270;&#26816;&#27979;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#35937;&#24341;&#23548;&#30340;Transformer&#26550;&#26500;&#65292;&#20174;&#32780;&#25299;&#23485;&#20102;&#21464;&#21270;&#26816;&#27979;&#20219;&#21153;&#30340;&#33539;&#22260;&#65292;&#24182;&#26174;&#33879;&#20943;&#23569;&#20102;&#35745;&#31639;&#24320;&#38144;&#21644;&#20869;&#23384;&#36127;&#25285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20809;&#23398;&#39640;&#20998;&#36776;&#29575;&#24433;&#20687;&#21644;OpenStreetMap&#65288;OSM&#65289;&#25968;&#25454;&#26159;&#22303;&#22320;&#35206;&#30422;&#21464;&#21270;&#26816;&#27979;&#30340;&#20004;&#20010;&#37325;&#35201;&#25968;&#25454;&#28304;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#21033;&#29992;OSM&#25968;&#25454;&#26469;&#36741;&#21161;&#22810;&#26102;&#26399;&#20809;&#23398;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#30340;&#21464;&#21270;&#26816;&#27979;&#12290;&#26412;&#25991;&#36890;&#36807;&#30452;&#25509;&#21033;&#29992;&#37197;&#23545;&#30340;OSM&#25968;&#25454;&#21644;&#20809;&#23398;&#22270;&#20687;&#36827;&#34892;&#22303;&#22320;&#35206;&#30422;&#21464;&#21270;&#26816;&#27979;&#65292;&#25299;&#23485;&#20102;&#21464;&#21270;&#26816;&#27979;&#20219;&#21153;&#30340;&#33539;&#22260;&#65292;&#28085;&#30422;&#26356;&#22810;&#21160;&#24577;&#22320;&#29699;&#35266;&#27979;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#35937;&#24341;&#23548;&#30340;Transformer&#65288;ObjFormer&#65289;&#26550;&#26500;&#65292;&#23558;&#27969;&#34892;&#30340;&#22522;&#20110;&#23545;&#35937;&#30340;&#22270;&#20687;&#20998;&#26512;&#65288;OBIA&#65289;&#25216;&#26415;&#19982;&#20808;&#36827;&#30340;&#35270;&#35273;Transformer&#26550;&#26500;&#33258;&#28982;&#22320;&#32467;&#21512;&#36215;&#26469;&#12290;&#24341;&#20837;OBIA&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#20013;&#30340;&#35745;&#31639;&#24320;&#38144;&#21644;&#20869;&#23384;&#36127;&#25285;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25152;&#25552;&#20986;&#30340;ObjFormer&#20855;&#26377;&#23618;&#27425;&#20266;&#23402;&#29983;&#32534;&#30721;&#22120;&#65292;&#21253;&#21547;&#23545;&#35937;&#24341;&#23548;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#29992;&#20110;&#25552;&#21462;&#20195;&#34920;&#24615;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optical high-resolution imagery and OpenStreetMap (OSM) data are two important data sources for land-cover change detection. Previous studies in these two data sources focus on utilizing the information in OSM data to aid the change detection on multi-temporal optical high-resolution images. This paper pioneers the direct detection of land-cover changes utilizing paired OSM data and optical imagery, thereby broadening the horizons of change detection tasks to encompass more dynamic earth observations. To this end, we propose an object-guided Transformer (ObjFormer) architecture by naturally combining the prevalent object-based image analysis (OBIA) technique with the advanced vision Transformer architecture. The introduction of OBIA can significantly reduce the computational overhead and memory burden in the self-attention module. Specifically, the proposed ObjFormer has a hierarchical pseudo-siamese encoder consisting of object-guided self-attention modules that extract representative
&lt;/p&gt;</description></item><item><title>OceanGPT&#26159;&#39318;&#20010;&#19987;&#20026;&#28023;&#27915;&#31185;&#23398;&#20219;&#21153;&#35774;&#35745;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;DoInstruct&#26694;&#26550;&#23454;&#29616;&#33258;&#21160;&#33719;&#21462;&#28023;&#27915;&#39046;&#22495;&#25351;&#23548;&#25968;&#25454;&#12290;&#36825;&#19968;&#27169;&#22411;&#30340;&#24341;&#20837;&#22635;&#34917;&#20102;&#28023;&#27915;&#31185;&#23398;&#39046;&#22495;&#20013;&#23545;LLM&#30340;&#38656;&#27714;&#32570;&#21475;&#65292;&#24182;&#20026;&#28023;&#27915;&#31185;&#23398;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#24037;&#20855;&#21644;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.02031</link><description>&lt;p&gt;
OceanGPT&#65306;&#29992;&#20110;&#28023;&#27915;&#31185;&#23398;&#20219;&#21153;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
OceanGPT: A Large Language Model for Ocean Science Tasks. (arXiv:2310.02031v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02031
&lt;/p&gt;
&lt;p&gt;
OceanGPT&#26159;&#39318;&#20010;&#19987;&#20026;&#28023;&#27915;&#31185;&#23398;&#20219;&#21153;&#35774;&#35745;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;DoInstruct&#26694;&#26550;&#23454;&#29616;&#33258;&#21160;&#33719;&#21462;&#28023;&#27915;&#39046;&#22495;&#25351;&#23548;&#25968;&#25454;&#12290;&#36825;&#19968;&#27169;&#22411;&#30340;&#24341;&#20837;&#22635;&#34917;&#20102;&#28023;&#27915;&#31185;&#23398;&#39046;&#22495;&#20013;&#23545;LLM&#30340;&#38656;&#27714;&#32570;&#21475;&#65292;&#24182;&#20026;&#28023;&#27915;&#31185;&#23398;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#24037;&#20855;&#21644;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28023;&#27915;&#31185;&#23398;&#26159;&#25506;&#32034;&#20805;&#28385;&#29983;&#21629;&#21644;&#29983;&#29289;&#22810;&#26679;&#24615;&#30340;&#28023;&#27915;&#30340;&#31185;&#23398;&#65292;&#32771;&#34385;&#21040;&#28023;&#27915;&#35206;&#30422;&#20102;&#22320;&#29699;&#34920;&#38754;&#30340;70&#65285;&#20197;&#19978;&#65292;&#36825;&#19968;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36827;&#23637;&#25913;&#21464;&#20102;&#31185;&#23398;&#30340;&#33539;&#24335;&#12290;&#23613;&#31649;&#22312;&#20854;&#20182;&#39046;&#22495;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#29616;&#26377;&#30340;LLM&#36890;&#24120;&#26080;&#27861;&#28385;&#36275;&#28023;&#27915;&#23398;&#23478;&#31561;&#39046;&#22495;&#19987;&#23478;&#30340;&#38656;&#27714;&#65292;&#21516;&#26102;&#23545;LLM&#22312;&#28023;&#27915;&#31185;&#23398;&#20013;&#30340;&#28508;&#21147;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#36825;&#20854;&#20013;&#30340;&#26681;&#26412;&#21407;&#22240;&#21487;&#33021;&#26159;&#28023;&#27915;&#25968;&#25454;&#30340;&#24222;&#22823;&#32780;&#22797;&#26434;&#30340;&#24615;&#36136;&#65292;&#20197;&#21450;&#23545;&#26356;&#39640;&#30340;&#31890;&#24230;&#21644;&#20016;&#23500;&#30340;&#30693;&#35782;&#30340;&#38656;&#27714;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25512;&#20986;&#20102;&#39318;&#20010;&#28023;&#27915;&#39046;&#22495;&#30340;LLM&#8212;&#8212;OceanGPT&#65292;&#35813;&#27169;&#22411;&#25797;&#38271;&#21508;&#31181;&#28023;&#27915;&#31185;&#23398;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;DoInstruct&#65292;&#29992;&#20110;&#33258;&#21160;&#33719;&#21462;&#22823;&#37327;&#30340;&#28023;&#27915;&#39046;&#22495;&#25351;&#23548;&#25968;&#25454;&#65292;&#23427;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#30340;&#21327;&#20316;&#29983;&#25104;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ocean science, which delves into the oceans that are reservoirs of life and biodiversity, is of great significance given that oceans cover over 70% of our planet's surface. Recently, advances in Large Language Models (LLMs) have transformed the paradigm in science. Despite the success in other domains, current LLMs often fall short in catering to the needs of domain experts like oceanographers, and the potential of LLMs for ocean science is under-explored. The intrinsic reason may be the immense and intricate nature of ocean data as well as the necessity for higher granularity and richness in knowledge. To alleviate these issues, we introduce OceanGPT, the first-ever LLM in the ocean domain, which is expert in various ocean science tasks. We propose DoInstruct, a novel framework to automatically obtain a large volume of ocean domain instruction data, which generates instructions based on multi-agent collaboration. Additionally, we construct the first oceanography benchmark, OceanBench,
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24178;&#39044;&#24863;&#30693;&#30340;&#27010;&#24565;&#23884;&#20837;&#27169;&#22411;&#65292;&#29992;&#20110;&#25552;&#39640;&#31070;&#32463;&#26550;&#26500;&#23545;&#27010;&#24565;&#24178;&#39044;&#30340;&#21709;&#24212;&#24615;&#65292;&#24182;&#35299;&#20915;&#20102;&#27010;&#24565;&#24178;&#39044;&#39034;&#24207;&#21644;&#27169;&#22411;&#26550;&#26500;&#30340;&#20381;&#36182;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.16928</link><description>&lt;p&gt;
&#23398;&#20064;&#25509;&#21463;&#24110;&#21161;&#65306;&#24178;&#39044;&#24863;&#30693;&#30340;&#27010;&#24565;&#23884;&#20837;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning to Receive Help: Intervention-Aware Concept Embedding Models. (arXiv:2309.16928v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16928
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24178;&#39044;&#24863;&#30693;&#30340;&#27010;&#24565;&#23884;&#20837;&#27169;&#22411;&#65292;&#29992;&#20110;&#25552;&#39640;&#31070;&#32463;&#26550;&#26500;&#23545;&#27010;&#24565;&#24178;&#39044;&#30340;&#21709;&#24212;&#24615;&#65292;&#24182;&#35299;&#20915;&#20102;&#27010;&#24565;&#24178;&#39044;&#39034;&#24207;&#21644;&#27169;&#22411;&#26550;&#26500;&#30340;&#20381;&#36182;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65288;CBMs&#65289;&#36890;&#36807;&#20351;&#29992;&#19968;&#32452;&#39640;&#32423;&#27010;&#24565;&#26500;&#24314;&#21644;&#35299;&#37322;&#31070;&#32463;&#26550;&#26500;&#30340;&#39044;&#27979;&#65292;&#20197;&#35299;&#20915;&#20854;&#19981;&#36879;&#26126;&#24615;&#30340;&#38382;&#39064;&#12290;&#36825;&#20123;&#27169;&#22411;&#30340;&#19968;&#20010;&#29305;&#27530;&#23646;&#24615;&#26159;&#23427;&#20204;&#20801;&#35768;&#27010;&#24565;&#24178;&#39044;&#65292;&#29992;&#25143;&#21487;&#20197;&#32416;&#27491;&#34987;&#38169;&#35823;&#39044;&#27979;&#30340;&#27010;&#24565;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#24178;&#39044;&#26377;&#25928;&#24615;&#21487;&#33021;&#20005;&#37325;&#20381;&#36182;&#20110;&#24178;&#39044;&#27010;&#24565;&#30340;&#39034;&#24207;&#20197;&#21450;&#27169;&#22411;&#30340;&#26550;&#26500;&#21644;&#35757;&#32451;&#36229;&#21442;&#25968;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36825;&#28304;&#20110;CBM&#22312;&#35757;&#32451;&#26102;&#32570;&#20047;&#27169;&#22411;&#36866;&#24212;&#27010;&#24565;&#24178;&#39044;&#30340;&#28608;&#21169;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24178;&#39044;&#24863;&#30693;&#30340;&#27010;&#24565;&#23884;&#20837;&#27169;&#22411;&#65288;IntCEMs&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;CBM&#30340;&#26032;&#22411;&#26550;&#26500;&#21644;&#35757;&#32451;&#33539;&#24335;&#65292;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#23545;&#27979;&#35797;&#26102;&#24178;&#39044;&#30340;&#21709;&#24212;&#24615;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#23398;&#20064;&#20102;&#19968;&#20010;&#27010;&#24565;&#24178;&#39044;&#31574;&#30053;&#65292;&#20174;&#20013;&#21487;&#20197;&#37319;&#26679;&#26377;&#24847;&#20041;&#30340;&#24178;&#39044;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Concept Bottleneck Models (CBMs) tackle the opacity of neural architectures by constructing and explaining their predictions using a set of high-level concepts. A special property of these models is that they permit concept interventions, wherein users can correct mispredicted concepts and thus improve the model's performance. Recent work, however, has shown that intervention efficacy can be highly dependent on the order in which concepts are intervened on and on the model's architecture and training hyperparameters. We argue that this is rooted in a CBM's lack of train-time incentives for the model to be appropriately receptive to concept interventions. To address this, we propose Intervention-aware Concept Embedding models (IntCEMs), a novel CBM-based architecture and training paradigm that improves a model's receptiveness to test-time interventions. Our model learns a concept intervention policy in an end-to-end fashion from where it can sample meaningful intervention trajectories a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#36820;&#22238;&#26465;&#20214;&#31574;&#30053;&#65288;ARP&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#25551;&#36848;&#21644;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#32534;&#30721;&#22120;&#26469;&#25552;&#21319;&#26234;&#33021;&#20307;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#23884;&#20837;&#31354;&#38388;&#20013;&#35745;&#31639;&#35270;&#35273;&#35266;&#23519;&#21644;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#65292;&#24182;&#23558;&#20854;&#29992;&#20316;&#22870;&#21169;&#20449;&#21495;&#65292;ARP&#26377;&#25928;&#32531;&#35299;&#20102;&#30446;&#26631;&#35823;&#27867;&#21270;&#38382;&#39064;&#65292;&#24182;&#22312;&#38754;&#23545;&#26410;&#30693;&#30340;&#25991;&#26412;&#25351;&#20196;&#26102;&#23637;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.10790</link><description>&lt;p&gt;
&#29992;&#33258;&#36866;&#24212;&#22810;&#27169;&#24577;&#22870;&#21169;&#24341;&#23548;&#20320;&#30340;&#26234;&#33021;&#20307;
&lt;/p&gt;
&lt;p&gt;
Guide Your Agent with Adaptive Multimodal Rewards. (arXiv:2309.10790v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10790
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#36820;&#22238;&#26465;&#20214;&#31574;&#30053;&#65288;ARP&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#25551;&#36848;&#21644;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#32534;&#30721;&#22120;&#26469;&#25552;&#21319;&#26234;&#33021;&#20307;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#23884;&#20837;&#31354;&#38388;&#20013;&#35745;&#31639;&#35270;&#35273;&#35266;&#23519;&#21644;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#65292;&#24182;&#23558;&#20854;&#29992;&#20316;&#22870;&#21169;&#20449;&#21495;&#65292;ARP&#26377;&#25928;&#32531;&#35299;&#20102;&#30446;&#26631;&#35823;&#27867;&#21270;&#38382;&#39064;&#65292;&#24182;&#22312;&#38754;&#23545;&#26410;&#30693;&#30340;&#25991;&#26412;&#25351;&#20196;&#26102;&#23637;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27169;&#20223;&#23398;&#20064;&#20013;&#65292;&#24320;&#21457;&#19968;&#20010;&#33021;&#22815;&#36866;&#24212;&#26410;&#30693;&#29615;&#22659;&#30340;&#26234;&#33021;&#20307;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33258;&#36866;&#24212;&#36820;&#22238;&#26465;&#20214;&#31574;&#30053;(ARP)&#30340;&#39640;&#25928;&#26694;&#26550;&#65292;&#29992;&#20110;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#25551;&#36848;&#21644;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#32534;&#30721;&#22120;&#26469;&#25552;&#21319;&#26234;&#33021;&#20307;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#22312;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#23884;&#20837;&#31354;&#38388;(&#20363;&#22914;CLIP)&#20013;&#35745;&#31639;&#35270;&#35273;&#35266;&#27979;&#21644;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#22870;&#21169;&#20449;&#21495;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#29992;&#22810;&#27169;&#24577;&#22870;&#21169;&#26631;&#35760;&#30340;&#19987;&#23478;&#28436;&#31034;&#26469;&#35757;&#32451;&#19968;&#20010;&#36820;&#22238;&#26465;&#20214;&#31574;&#30053;&#12290;&#30001;&#20110;&#22810;&#27169;&#24577;&#22870;&#21169;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#25552;&#20379;&#33258;&#36866;&#24212;&#20449;&#21495;&#65292;&#25105;&#20204;&#30340;ARP&#26377;&#25928;&#22320;&#32531;&#35299;&#20102;&#30446;&#26631;&#35823;&#27867;&#21270;&#38382;&#39064;&#12290;&#19982;&#29616;&#26377;&#30340;&#25991;&#26412;&#26465;&#20214;&#31574;&#30053;&#30456;&#27604;&#65292;&#21363;&#20351;&#38754;&#23545;&#26410;&#30693;&#30340;&#25991;&#26412;&#25351;&#20196;&#65292;&#25105;&#20204;&#30340;ARP&#22312;&#27867;&#21270;&#24615;&#33021;&#26041;&#38754;&#20063;&#34920;&#29616;&#20986;&#20247;&#12290;&#20026;&#20102;&#25552;&#39640;&#22870;&#21169;&#30340;&#36136;&#37327;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#39044;&#35757;&#32451;&#24494;&#35843;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developing an agent capable of adapting to unseen environments remains a difficult challenge in imitation learning. This work presents Adaptive Return-conditioned Policy (ARP), an efficient framework designed to enhance the agent's generalization ability using natural language task descriptions and pre-trained multimodal encoders. Our key idea is to calculate a similarity between visual observations and natural language instructions in the pre-trained multimodal embedding space (such as CLIP) and use it as a reward signal. We then train a return-conditioned policy using expert demonstrations labeled with multimodal rewards. Because the multimodal rewards provide adaptive signals at each timestep, our ARP effectively mitigates the goal misgeneralization. This results in superior generalization performances even when faced with unseen text instructions, compared to existing text-conditioned policies. To improve the quality of rewards, we also introduce a fine-tuning method for pre-traine
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21487;&#37197;&#32622;&#30340;&#24211;&#65292;&#29992;&#20110;&#29983;&#25104;&#21644;&#22788;&#29702;&#36855;&#23467;&#25968;&#25454;&#38598;&#65292;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#36890;&#36807;&#35813;&#24211;&#29983;&#25104;&#19981;&#21516;&#20998;&#24067;&#30340;&#36855;&#23467;&#25968;&#25454;&#38598;&#65292;&#24182;&#23545;&#29983;&#25104;&#21442;&#25968;&#21644;&#29983;&#25104;&#35268;&#21017;&#36827;&#34892;&#33258;&#23450;&#20041;&#25511;&#21046;&#12290;&#21487;&#20197;&#25903;&#25345;&#22810;&#31181;&#36755;&#20986;&#26684;&#24335;&#65292;&#36866;&#29992;&#20110;&#19981;&#21516;&#31867;&#22411;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.10498</link><description>&lt;p&gt;
&#19968;&#20010;&#21487;&#37197;&#32622;&#30340;&#24211;&#29992;&#20110;&#29983;&#25104;&#21644;&#25805;&#20316;&#36855;&#23467;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
A Configurable Library for Generating and Manipulating Maze Datasets. (arXiv:2309.10498v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10498
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21487;&#37197;&#32622;&#30340;&#24211;&#65292;&#29992;&#20110;&#29983;&#25104;&#21644;&#22788;&#29702;&#36855;&#23467;&#25968;&#25454;&#38598;&#65292;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#36890;&#36807;&#35813;&#24211;&#29983;&#25104;&#19981;&#21516;&#20998;&#24067;&#30340;&#36855;&#23467;&#25968;&#25454;&#38598;&#65292;&#24182;&#23545;&#29983;&#25104;&#21442;&#25968;&#21644;&#29983;&#25104;&#35268;&#21017;&#36827;&#34892;&#33258;&#23450;&#20041;&#25511;&#21046;&#12290;&#21487;&#20197;&#25903;&#25345;&#22810;&#31181;&#36755;&#20986;&#26684;&#24335;&#65292;&#36866;&#29992;&#20110;&#19981;&#21516;&#31867;&#22411;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;&#20998;&#24067;&#20559;&#31227;&#30340;&#21709;&#24212;&#26041;&#24335;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#25361;&#25112;&#12290;&#30001;&#20110;&#19981;&#21516;&#30340;&#29983;&#25104;&#31639;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#32454;&#33268;&#30340;&#24179;&#21488;&#26469;&#27169;&#25311;&#24494;&#22937;&#21644;&#26174;&#33879;&#30340;&#20998;&#24067;&#20559;&#31227;&#65292;&#36855;&#23467;&#20316;&#20026;&#19968;&#20010;&#20248;&#31168;&#30340;&#27979;&#35797;&#22522;&#20934;&#12290;&#20026;&#20102;&#25903;&#25345;&#23545;&#27169;&#22411;&#22312;&#20998;&#24067;&#20559;&#31163;&#25968;&#25454;&#19978;&#34892;&#20026;&#30340;&#31995;&#32479;&#24615;&#30740;&#31350;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;maze-dataset&#8221;&#65292;&#19968;&#20010;&#21253;&#21547;&#36855;&#23467;&#27714;&#35299;&#20219;&#21153;&#30340;&#29983;&#25104;&#12289;&#22788;&#29702;&#21644;&#21487;&#35270;&#21270;&#25968;&#25454;&#38598;&#30340;&#32508;&#21512;&#24211;&#12290;&#20511;&#21161;&#36825;&#20010;&#24211;&#65292;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#36731;&#26494;&#21019;&#24314;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#23545;&#20351;&#29992;&#30340;&#29983;&#25104;&#31639;&#27861;&#12289;&#20256;&#36882;&#32473;&#36873;&#25321;&#31639;&#27861;&#30340;&#21442;&#25968;&#21644;&#29983;&#25104;&#30340;&#36855;&#23467;&#24517;&#39035;&#28385;&#36275;&#30340;&#31579;&#36873;&#22120;&#36827;&#34892;&#24191;&#27867;&#30340;&#25511;&#21046;&#12290;&#27492;&#22806;&#65292;&#23427;&#25903;&#25345;&#22810;&#31181;&#36755;&#20986;&#26684;&#24335;&#65292;&#21253;&#25324;&#26629;&#26684;&#21270;&#21644;&#22522;&#20110;&#25991;&#26412;&#30340;&#26684;&#24335;&#65292;&#36866;&#29992;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#33258;&#22238;&#24402;&#21464;&#25442;&#27169;&#22411;&#12290;&#36825;&#20123;&#26684;&#24335;&#20197;&#21450;&#29992;&#20110;&#21487;&#35270;&#21270;&#21644;&#36716;&#25442;&#30340;&#24037;&#20855;&#30830;&#20445;&#20102;&#28789;&#27963;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding how machine learning models respond to distributional shifts is a key research challenge. Mazes serve as an excellent testbed due to varied generation algorithms offering a nuanced platform to simulate both subtle and pronounced distributional shifts. To enable systematic investigations of model behavior on out-of-distribution data, we present $\texttt{maze-dataset}$, a comprehensive library for generating, processing, and visualizing datasets consisting of maze-solving tasks. With this library, researchers can easily create datasets, having extensive control over the generation algorithm used, the parameters fed to the algorithm of choice, and the filters that generated mazes must satisfy. Furthermore, it supports multiple output formats, including rasterized and text-based, catering to convolutional neural networks and autoregressive transformer models. These formats, along with tools for visualizing and converting between them, ensure versatility and adaptability in re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#22914;&#20309;&#22788;&#29702;&#25968;&#25454;&#39532;&#25289;&#26494;&#20013;&#30340;&#25968;&#25454;&#30340;&#25351;&#23548;&#26041;&#38024;&#21644;&#24314;&#35758;&#65292;&#36890;&#36807;10&#20010;&#26696;&#20363;&#30740;&#31350;&#39564;&#35777;&#20102;&#25552;&#20986;&#30340;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.09770</link><description>&lt;p&gt;
&#22914;&#20309;&#22312;&#25968;&#25454;&#39532;&#25289;&#26494;&#20013;&#22788;&#29702;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
How to Data in Datathons. (arXiv:2309.09770v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09770
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#22914;&#20309;&#22788;&#29702;&#25968;&#25454;&#39532;&#25289;&#26494;&#20013;&#30340;&#25968;&#25454;&#30340;&#25351;&#23548;&#26041;&#38024;&#21644;&#24314;&#35758;&#65292;&#36890;&#36807;10&#20010;&#26696;&#20363;&#30740;&#31350;&#39564;&#35777;&#20102;&#25552;&#20986;&#30340;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#39532;&#25289;&#26494;&#30340;&#20852;&#36215;&#25552;&#20379;&#20102;&#19968;&#20010;&#22312;&#30701;&#26102;&#38388;&#20869;&#21512;&#20316;&#12289;&#23398;&#20064;&#21644;&#21019;&#26032;&#30340;&#24179;&#21488;&#12290;&#23613;&#31649;&#23427;&#20204;&#20855;&#26377;&#37325;&#35201;&#30340;&#28508;&#22312;&#22909;&#22788;&#65292;&#20294;&#32452;&#32455;&#24448;&#24448;&#22240;&#32570;&#20047;&#26126;&#30830;&#30340;&#25351;&#23548;&#26041;&#38024;&#21644;&#26368;&#20339;&#23454;&#36341;&#32780;&#38590;&#20197;&#26377;&#25928;&#22788;&#29702;&#25968;&#25454;&#12290;&#26681;&#25454;&#25105;&#20204;&#33258;&#24049;&#30340;&#32463;&#39564;&#20197;&#21450;&#33258;2016&#24180;&#20197;&#26469;&#32452;&#32455;&#20102;&#36229;&#36807;80&#20010;&#25968;&#25454;&#39532;&#25289;&#26494;&#25361;&#25112;&#36187;&#19982;60&#20010;&#21512;&#20316;&#20249;&#20276;&#32452;&#32455;&#30340;&#35265;&#35299;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#25351;&#23548;&#26041;&#38024;&#21644;&#24314;&#35758;&#65292;&#20316;&#20026;&#32452;&#32455;&#32773;&#22312;&#22788;&#29702;&#25968;&#25454;&#30456;&#20851;&#22797;&#26434;&#24615;&#26102;&#30340;&#36164;&#28304;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#24212;&#29992;&#20110;10&#20010;&#26696;&#20363;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rise of datathons, also known as data or data science hackathons, has provided a platform to collaborate, learn, and innovate in a short timeframe. Despite their significant potential benefits, organizations often struggle to effectively work with data due to a lack of clear guidelines and best practices for potential issues that might arise. Drawing on our own experiences and insights from organizing &gt;80 datathon challenges with &gt;60 partnership organizations since 2016, we provide guidelines and recommendations that serve as a resource for organizers to navigate the data-related complexities of datathons. We apply our proposed framework to 10 case studies.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#20419;&#36827;&#24739;&#32773;&#21644;&#21307;&#29983;&#20043;&#38388;&#30340;&#24322;&#27493;&#36890;&#20449;&#65292;&#36890;&#36807;&#35775;&#35848;&#30740;&#31350;&#20102;&#35299;&#20102;&#20182;&#20204;&#23545;LLMs&#30340;&#38656;&#27714;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;Talk2Care&#30340;LLM&#39537;&#21160;&#30340;&#36890;&#20449;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2309.09357</link><description>&lt;p&gt;
Talk2Care: &#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20419;&#36827;&#24322;&#27493;&#24739;&#32773;-&#21307;&#29983;&#36890;&#20449;
&lt;/p&gt;
&lt;p&gt;
Talk2Care: Facilitating Asynchronous Patient-Provider Communication with Large-Language-Model. (arXiv:2309.09357v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09357
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#20419;&#36827;&#24739;&#32773;&#21644;&#21307;&#29983;&#20043;&#38388;&#30340;&#24322;&#27493;&#36890;&#20449;&#65292;&#36890;&#36807;&#35775;&#35848;&#30740;&#31350;&#20102;&#35299;&#20102;&#20182;&#20204;&#23545;LLMs&#30340;&#38656;&#27714;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;Talk2Care&#30340;LLM&#39537;&#21160;&#30340;&#36890;&#20449;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26377;&#22823;&#37327;&#30340;&#36828;&#31243;&#21307;&#30103;&#24212;&#29992;&#31243;&#24207;&#26469;&#24110;&#21161;&#23478;&#24237;&#20013;&#30340;&#32769;&#24180;&#20154;&#21644;&#21307;&#30103;&#25552;&#20379;&#32773;&#65292;&#20294;&#22522;&#26412;&#30340;&#28040;&#24687;&#21644;&#30005;&#35805;&#20173;&#28982;&#26159;&#26368;&#24120;&#35265;&#30340;&#36890;&#20449;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#23384;&#22312;&#26377;&#38480;&#30340;&#21487;&#29992;&#24615;&#12289;&#20449;&#24687;&#20002;&#22833;&#21644;&#27969;&#31243;&#25928;&#29575;&#20302;&#19979;&#30340;&#38382;&#39064;&#12290;&#20419;&#36827;&#24739;&#32773;-&#21307;&#29983;&#36890;&#20449;&#30340;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21450;&#20854;&#24378;&#22823;&#30340;&#33258;&#28982;&#23545;&#35805;&#21644;&#25688;&#35201;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;LLMs&#22312;&#36890;&#20449;&#36807;&#31243;&#20013;&#30340;&#20316;&#29992;&#36824;&#23384;&#22312;&#26377;&#38480;&#30340;&#29702;&#35299;&#12290;&#25105;&#20204;&#39318;&#20808;&#36827;&#34892;&#20102;&#20004;&#39033;&#35775;&#35848;&#30740;&#31350;&#65292;&#20998;&#21035;&#19982;&#32769;&#24180;&#20154;(N=10)&#21644;&#21307;&#30103;&#25552;&#20379;&#32773;(N=9)&#36827;&#34892;&#20102;&#20132;&#27969;&#65292;&#20197;&#20102;&#35299;&#20182;&#20204;&#22312;&#24739;&#32773;-&#21307;&#29983;&#24322;&#27493;&#36890;&#20449;&#20013;&#23545;LLMs&#30340;&#38656;&#27714;&#21644;&#26426;&#20250;&#12290;&#22522;&#20110;&#36825;&#20123;&#35265;&#35299;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;LLM&#39537;&#21160;&#30340;&#36890;&#20449;&#31995;&#32479;Talk2Care&#65292;&#24182;&#20026;&#20004;&#20010;&#32676;&#20307;&#35774;&#35745;&#20102;&#20132;&#20114;&#32452;&#20214;: (1) &#23545;&#20110;&#32769;&#24180;&#20154;&#65292;&#25105;&#20204;&#21033;&#29992;&#35821;&#38899;&#21161;&#25163;&#30340;&#20415;&#21033;&#24615;&#21644;&#26131;&#20110;&#33719;&#21462;&#24615;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;LLM&#39537;&#21160;&#30340;&#35821;&#38899;&#21161;&#25163;
&lt;/p&gt;
&lt;p&gt;
Despite the plethora of telehealth applications to assist home-based older adults and healthcare providers, basic messaging and phone calls are still the most common communication methods, which suffer from limited availability, information loss, and process inefficiencies. One promising solution to facilitate patient-provider communication is to leverage large language models (LLMs) with their powerful natural conversation and summarization capability. However, there is a limited understanding of LLMs' role during the communication. We first conducted two interview studies with both older adults (N=10) and healthcare providers (N=9) to understand their needs and opportunities for LLMs in patient-provider asynchronous communication. Based on the insights, we built an LLM-powered communication system, Talk2Care, and designed interactive components for both groups: (1) For older adults, we leveraged the convenience and accessibility of voice assistants (VAs) and built an LLM-powered VA i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26681;&#25454;&#31185;&#23398;&#25688;&#35201;&#25991;&#26412;&#30340;&#33021;&#21147;&#65292;&#26469;&#36776;&#21035;&#25903;&#25345;&#25110;&#21453;&#39539;&#29305;&#23450;&#20551;&#35774;&#30340;&#35777;&#25454;&#12290;&#36890;&#36807;&#31038;&#21306;&#39537;&#21160;&#30340;&#27880;&#37322;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#38024;&#23545;&#31038;&#20250;&#31185;&#23398;&#20013;&#30340;&#31185;&#23398;&#20551;&#35774;&#35777;&#25454;&#20219;&#21153;&#12290;&#19982;&#20854;&#20182;&#22522;&#20934;&#36827;&#34892;&#20102;&#24615;&#33021;&#27604;&#36739;&#65292;&#24182;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;</title><link>http://arxiv.org/abs/2309.06578</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#36776;&#21035;&#31185;&#23398;&#20551;&#35774;&#30340;&#35777;&#25454;&#65311;&#31038;&#20250;&#31185;&#23398;&#26696;&#20363;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models Discern Evidence for Scientific Hypotheses? Case Studies in the Social Sciences. (arXiv:2309.06578v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06578
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26681;&#25454;&#31185;&#23398;&#25688;&#35201;&#25991;&#26412;&#30340;&#33021;&#21147;&#65292;&#26469;&#36776;&#21035;&#25903;&#25345;&#25110;&#21453;&#39539;&#29305;&#23450;&#20551;&#35774;&#30340;&#35777;&#25454;&#12290;&#36890;&#36807;&#31038;&#21306;&#39537;&#21160;&#30340;&#27880;&#37322;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#38024;&#23545;&#31038;&#20250;&#31185;&#23398;&#20013;&#30340;&#31185;&#23398;&#20551;&#35774;&#35777;&#25454;&#20219;&#21153;&#12290;&#19982;&#20854;&#20182;&#22522;&#20934;&#36827;&#34892;&#20102;&#24615;&#33021;&#27604;&#36739;&#65292;&#24182;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20551;&#35774;&#30340;&#21046;&#23450;&#21644;&#27979;&#35797;&#26159;&#32463;&#39564;&#24615;&#30740;&#31350;&#30340;&#26680;&#24515;&#12290;&#19968;&#20010;&#24378;&#26377;&#21147;&#30340;&#20551;&#35774;&#26159;&#22522;&#20110;&#29616;&#26377;&#35777;&#25454;&#30340;&#26368;&#20339;&#29468;&#27979;&#65292;&#24182;&#19988;&#26159;&#22522;&#20110;&#30456;&#20851;&#25991;&#29486;&#30340;&#20840;&#38754;&#35270;&#22270;&#36827;&#34892;&#21551;&#21457;&#30340;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#27599;&#24180;&#31185;&#23398;&#25991;&#31456;&#25968;&#37327;&#30340;&#25351;&#25968;&#22686;&#38271;&#65292;&#23545;&#20110;&#32473;&#23450;&#20551;&#35774;&#30456;&#20851;&#35777;&#25454;&#30340;&#25163;&#21160;&#27719;&#24635;&#21644;&#32508;&#21512;&#26159;&#19968;&#39033;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25506;&#32034;&#20102;&#24403;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26681;&#25454;&#31185;&#23398;&#25688;&#35201;&#25991;&#26412;&#20013;&#30340;&#35777;&#25454;&#65292;&#33021;&#21542;&#36776;&#21035;&#25903;&#25345;&#25110;&#21453;&#39539;&#29305;&#23450;&#20551;&#35774;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#20849;&#20139;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#31038;&#20250;&#31185;&#23398;&#20013;&#20351;&#29992;&#31038;&#21306;&#39537;&#21160;&#30340;&#30740;&#31350;&#27880;&#37322;&#30340;&#31185;&#23398;&#20551;&#35774;&#35777;&#25454;&#20219;&#21153;&#12290;&#25105;&#20204;&#23558;LLMs&#30340;&#24615;&#33021;&#19982;&#20960;&#20010;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#25351;&#20986;&#26410;&#26469;&#30740;&#31350;&#30340;&#26426;&#20250;&#12290;&#35813;&#25968;&#25454;&#38598;&#21487;&#22312;https://github.com/Sai90000/ScientificHypothesisEvidencing.git&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hypothesis formulation and testing are central to empirical research. A strong hypothesis is a best guess based on existing evidence and informed by a comprehensive view of relevant literature. However, with exponential increase in the number of scientific articles published annually, manual aggregation and synthesis of evidence related to a given hypothesis is a challenge. Our work explores the ability of current large language models (LLMs) to discern evidence in support or refute of specific hypotheses based on the text of scientific abstracts. We share a novel dataset for the task of scientific hypothesis evidencing using community-driven annotations of studies in the social sciences. We compare the performance of LLMs to several state-of-the-art benchmarks and highlight opportunities for future research in this area. The dataset is available at https://github.com/Sai90000/ScientificHypothesisEvidencing.git
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#25105;&#20013;&#24515;&#25668;&#20687;&#22836;&#36827;&#34892;&#25684;&#20498;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#21548;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#36831;&#20915;&#31574;&#34701;&#21512;&#23558;&#38899;&#39057;&#21644;&#35270;&#35273;&#20449;&#24687;&#30456;&#32467;&#21512;&#21487;&#20197;&#25552;&#39640;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.04579</link><description>&lt;p&gt;
EGOFALLS:&#19968;&#31181;&#20351;&#29992;&#33258;&#25105;&#20013;&#24515;&#25668;&#20687;&#22836;&#36827;&#34892;&#25684;&#20498;&#26816;&#27979;&#30340;&#35270;&#21548;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#65288;arXiv:2309.04579v1 [cs.CV]&#65289;
&lt;/p&gt;
&lt;p&gt;
EGOFALLS: A visual-audio dataset and benchmark for fall detection using egocentric cameras. (arXiv:2309.04579v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04579
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#25105;&#20013;&#24515;&#25668;&#20687;&#22836;&#36827;&#34892;&#25684;&#20498;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#21548;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#36831;&#20915;&#31574;&#34701;&#21512;&#23558;&#38899;&#39057;&#21644;&#35270;&#35273;&#20449;&#24687;&#30456;&#32467;&#21512;&#21487;&#20197;&#25552;&#39640;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#33030;&#24369;&#20154;&#32676;&#65292;&#22914;&#32769;&#24180;&#20154;&#65292;&#25684;&#20498;&#24448;&#24448;&#26159;&#20005;&#37325;&#19988;&#24120;&#23548;&#33268;&#27515;&#20129;&#30340;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#36890;&#36807;&#20381;&#36182;&#21333;&#20010;&#20256;&#24863;&#22120;&#65288;&#22270;&#20687;&#25110;&#21152;&#36895;&#24230;&#35745;&#65289;&#25429;&#25417;&#25968;&#25454;&#26469;&#35299;&#20915;&#25684;&#20498;&#30340;&#26816;&#27979;&#38382;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20381;&#36182;&#20110;&#20174;&#33258;&#25105;&#20013;&#24515;&#25668;&#20687;&#22836;&#25429;&#25417;&#30340;&#35270;&#39057;&#20013;&#25552;&#21462;&#30340;&#22810;&#27169;&#24577;&#25551;&#36848;&#31526;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21253;&#25324;&#19968;&#20010;&#22312;&#25552;&#21462;&#30340;&#25551;&#36848;&#31526;&#20043;&#19978;&#26500;&#24314;&#30340;&#36831;&#20915;&#31574;&#34701;&#21512;&#23618;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25910;&#38598;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;&#36825;&#26159;&#25105;&#20204;&#35748;&#20026;&#30340;&#31532;&#19968;&#20010;&#20844;&#20849;&#21516;&#31867;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;14&#20010;&#21463;&#35797;&#32773;&#30340;10,948&#20010;&#35270;&#39057;&#26679;&#26412;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#28040;&#34701;&#23454;&#39564;&#20197;&#35780;&#20272;&#21333;&#20010;&#29305;&#24449;&#25552;&#21462;&#22120;&#30340;&#24615;&#33021;&#65292;&#35270;&#35273;&#20449;&#24687;&#34701;&#21512;&#20197;&#21450;&#35270;&#35273;&#21644;&#38899;&#39057;&#20449;&#24687;&#30340;&#34701;&#21512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#20869;&#37096;&#21644;&#22806;&#37096;&#20132;&#21449;&#39564;&#35777;&#30340;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#36831;&#20915;&#31574;&#34701;&#21512;&#23558;&#38899;&#39057;&#21644;&#35270;&#35273;&#20449;&#24687;&#30456;&#32467;&#21512;&#21487;&#20197;&#25552;&#39640;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Falls are significant and often fatal for vulnerable populations such as the elderly. Previous works have addressed the detection of falls by relying on data capture by a single sensor, images or accelerometers. In this work, we rely on multimodal descriptors extracted from videos captured by egocentric cameras. Our proposed method includes a late decision fusion layer that builds on top of the extracted descriptors. Furthermore, we collect a new dataset on which we assess our proposed approach. We believe this is the first public dataset of its kind. The dataset comprises 10,948 video samples by 14 subjects. We conducted ablation experiments to assess the performance of individual feature extractors, fusion of visual information, and fusion of both visual and audio information. Moreover, we experimented with internal and external cross-validation. Our results demonstrate that the fusion of audio and visual information through late decision fusion improves detection performance, making
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;RePo&#31639;&#27861;&#65292;&#36890;&#36807;&#27491;&#21017;&#21270;&#21518;&#39564;&#21487;&#39044;&#27979;&#24615;&#30340;&#26041;&#24335;&#65292;&#22686;&#24378;&#20102;&#35270;&#35273;&#27169;&#22411;&#22522;&#30784;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#24377;&#24615;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#23545;&#20887;&#20313;&#21644;&#20266;&#21464;&#21270;&#20855;&#26377;&#24377;&#24615;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#25552;&#39640;&#20102;&#26041;&#27861;&#23545;&#35270;&#35273;&#24178;&#25200;&#30340;&#40065;&#26834;&#24615;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#36816;&#34892;&#12290;</title><link>http://arxiv.org/abs/2309.00082</link><description>&lt;p&gt;
RePo: &#36890;&#36807;&#27491;&#21017;&#21270;&#21518;&#39564;&#21487;&#39044;&#27979;&#24615;&#22686;&#24378;&#24377;&#24615;&#27169;&#22411;&#22522;&#30784;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
RePo: Resilient Model-Based Reinforcement Learning by Regularizing Posterior Predictability. (arXiv:2309.00082v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00082
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;RePo&#31639;&#27861;&#65292;&#36890;&#36807;&#27491;&#21017;&#21270;&#21518;&#39564;&#21487;&#39044;&#27979;&#24615;&#30340;&#26041;&#24335;&#65292;&#22686;&#24378;&#20102;&#35270;&#35273;&#27169;&#22411;&#22522;&#30784;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#24377;&#24615;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#23545;&#20887;&#20313;&#21644;&#20266;&#21464;&#21270;&#20855;&#26377;&#24377;&#24615;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#25552;&#39640;&#20102;&#26041;&#27861;&#23545;&#35270;&#35273;&#24178;&#25200;&#30340;&#40065;&#26834;&#24615;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#36816;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#27169;&#22411;&#22522;&#30784;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#23558;&#22270;&#20687;&#35266;&#27979;&#32534;&#30721;&#20026;&#20302;&#32500;&#34920;&#31034;&#26041;&#24335;&#65292;&#36825;&#31181;&#26041;&#24335;&#26410;&#33021;&#28040;&#38500;&#20887;&#20313;&#20449;&#24687;&#12290;&#36825;&#20351;&#24471;&#36825;&#20123;&#26041;&#27861;&#23481;&#26131;&#21463;&#21040;&#20266;&#21464;&#21270;&#30340;&#24433;&#21709;&#65292;&#21363;&#19982;&#20219;&#21153;&#26080;&#20851;&#30340;&#32452;&#25104;&#37096;&#20998;&#30340;&#21464;&#21270;&#65292;&#22914;&#32972;&#26223;&#24178;&#25200;&#22240;&#32032;&#25110;&#20809;&#29031;&#26465;&#20214;&#30340;&#21464;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35270;&#35273;&#27169;&#22411;&#22522;&#30784;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23398;&#20064;&#21040;&#20102;&#19968;&#31181;&#23545;&#36825;&#31181;&#20266;&#21464;&#21270;&#20855;&#26377;&#24377;&#24615;&#30340;&#28508;&#22312;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#35757;&#32451;&#30446;&#26631;&#40723;&#21169;&#35813;&#34920;&#31034;&#22312;&#21160;&#21147;&#23398;&#21644;&#22870;&#21169;&#39044;&#27979;&#26041;&#38754;&#20855;&#26377;&#26368;&#22823;&#30340;&#39044;&#27979;&#24615;&#65292;&#21516;&#26102;&#38480;&#21046;&#20102;&#35266;&#27979;&#21040;&#28508;&#22312;&#34920;&#31034;&#30340;&#20449;&#24687;&#27969;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#19968;&#30446;&#26631;&#26497;&#22823;&#22686;&#24378;&#20102;&#35270;&#35273;&#27169;&#22411;&#22522;&#30784;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#23545;&#35270;&#35273;&#24178;&#25200;&#30340;&#24377;&#24615;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#36816;&#34892;&#12290;&#28982;&#21518;&#25105;&#20204;&#23637;&#31034;&#20102;&#34429;&#28982;&#23398;&#20064;&#21040;&#30340;&#32534;&#30721;&#22120;&#23545;&#20266;&#21464;&#21270;&#20855;&#26377;&#24377;&#24615;&#65292;&#20294;&#22312;&#26174;&#33879;&#20998;&#24067;&#21464;&#21270;&#19979;&#24182;&#27809;&#26377;&#19981;&#21464;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#22870;&#21169;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual model-based RL methods typically encode image observations into low-dimensional representations in a manner that does not eliminate redundant information. This leaves them susceptible to spurious variations -- changes in task-irrelevant components such as background distractors or lighting conditions. In this paper, we propose a visual model-based RL method that learns a latent representation resilient to such spurious variations. Our training objective encourages the representation to be maximally predictive of dynamics and reward, while constraining the information flow from the observation to the latent representation. We demonstrate that this objective significantly bolsters the resilience of visual model-based RL methods to visual distractors, allowing them to operate in dynamic environments. We then show that while the learned encoder is resilient to spirious variations, it is not invariant under significant distribution shift. To address this, we propose a simple reward-f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35838;&#31243;&#23398;&#20064;&#30340;&#36974;&#32617;&#33258;&#32534;&#30721;&#22120;&#65288;CL-MAE&#65289;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21487;&#23398;&#20064;&#30340;&#36974;&#32617;&#27169;&#22359;&#65292;&#36890;&#36807;&#26356;&#26032;&#36974;&#32617;&#31574;&#30053;&#26469;&#22686;&#21152;&#33258;&#30417;&#30563;&#37325;&#26500;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#12290;&#36890;&#36807;&#36880;&#28176;&#22686;&#21152;&#20219;&#21153;&#22797;&#26434;&#24615;&#65292;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#26356;&#22797;&#26434;&#21644;&#21487;&#36801;&#31227;&#30340;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2308.16572</link><description>&lt;p&gt;
CL-MAE: &#35838;&#31243;&#23398;&#20064;&#30340;&#36974;&#32617;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
CL-MAE: Curriculum-Learned Masked Autoencoders. (arXiv:2308.16572v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16572
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35838;&#31243;&#23398;&#20064;&#30340;&#36974;&#32617;&#33258;&#32534;&#30721;&#22120;&#65288;CL-MAE&#65289;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21487;&#23398;&#20064;&#30340;&#36974;&#32617;&#27169;&#22359;&#65292;&#36890;&#36807;&#26356;&#26032;&#36974;&#32617;&#31574;&#30053;&#26469;&#22686;&#21152;&#33258;&#30417;&#30563;&#37325;&#26500;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#12290;&#36890;&#36807;&#36880;&#28176;&#22686;&#21152;&#20219;&#21153;&#22797;&#26434;&#24615;&#65292;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#26356;&#22797;&#26434;&#21644;&#21487;&#36801;&#31227;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36974;&#32617;&#22270;&#20687;&#24314;&#27169;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#39044;&#25991;&#26412;&#20219;&#21153;&#65292;&#29992;&#20110;&#29983;&#25104;&#33021;&#22815;&#26377;&#25928;&#27867;&#21270;&#21040;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#30340;&#40065;&#26834;&#34920;&#31034;&#12290;&#36890;&#24120;&#65292;&#36825;&#31181;&#26041;&#27861;&#28041;&#21450;&#22312;&#36755;&#20837;&#22270;&#20687;&#20013;&#38543;&#26426;&#36974;&#32617;&#34917;&#19969;&#65288;&#26631;&#35760;&#65289;&#65292;&#24182;&#19988;&#36974;&#32617;&#31574;&#30053;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20445;&#25345;&#19981;&#21464;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26356;&#26032;&#36974;&#32617;&#31574;&#30053;&#20197;&#25345;&#32493;&#22686;&#21152;&#33258;&#30417;&#30563;&#37325;&#26500;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#25512;&#27979;&#65292;&#36890;&#36807;&#36880;&#28176;&#22686;&#21152;&#20219;&#21153;&#22797;&#26434;&#24615;&#65292;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#26356;&#22797;&#26434;&#21644;&#21487;&#36801;&#31227;&#30340;&#34920;&#31034;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#23398;&#20064;&#36974;&#32617;&#27169;&#22359;&#65292;&#20855;&#26377;&#29983;&#25104;&#19981;&#21516;&#22797;&#26434;&#24230;&#36974;&#32617;&#30340;&#33021;&#21147;&#65292;&#24182;&#23558;&#35813;&#27169;&#22359;&#19982;&#36974;&#32617;&#33258;&#32534;&#30721;&#22120;&#65288;MAE&#65289;&#38598;&#25104;&#12290;&#25105;&#20204;&#30340;&#27169;&#22359;&#19982;MAE&#19968;&#21516;&#35757;&#32451;&#65292;&#21516;&#26102;&#35843;&#25972;&#20854;&#34892;&#20026;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20174;MAE&#30340;&#21442;&#19982;&#32773;&#36807;&#28193;&#21040;MAE&#65288;&#20248;&#21270;&#30456;&#21516;&#30340;&#37325;&#26500;&#30446;&#26631;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Masked image modeling has been demonstrated as a powerful pretext task for generating robust representations that can be effectively generalized across multiple downstream tasks. Typically, this approach involves randomly masking patches (tokens) in input images, with the masking strategy remaining unchanged during training. In this paper, we propose a curriculum learning approach that updates the masking strategy to continually increase the complexity of the self-supervised reconstruction task. We conjecture that, by gradually increasing the task complexity, the model can learn more sophisticated and transferable representations. To facilitate this, we introduce a novel learnable masking module that possesses the capability to generate masks of different complexities, and integrate the proposed module into masked autoencoders (MAE). Our module is jointly trained with the MAE, while adjusting its behavior during training, transitioning from a partner to the MAE (optimizing the same rec
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;QMLS&#30340;&#26032;&#27010;&#24565;&#65292;&#36890;&#36807;&#32467;&#21512;&#26426;&#22120;&#23398;&#20064;&#21644;&#37327;&#23376;&#27169;&#25311;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#32553;&#30701;&#33647;&#29289;&#30740;&#21457;&#30340;&#26102;&#38388;&#21644;&#38477;&#20302;&#25104;&#26412;&#12290;&#36890;&#36807;&#29983;&#25104;&#21629;&#20013;&#29289;&#21644;&#20248;&#21270;&#20998;&#23376;&#30340;&#36807;&#31243;&#65292;&#21487;&#20197;&#22823;&#22823;&#25552;&#39640;&#33647;&#29289;&#21457;&#29616;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.08561</link><description>&lt;p&gt;
&#26410;&#26469;&#33647;&#29289;&#21457;&#29616;&#30340;&#23454;&#26045;&#65306;&#22522;&#20110;&#37327;&#23376;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#25311;(QMLS)&#12290;
&lt;/p&gt;
&lt;p&gt;
Implementation of The Future of Drug Discovery: QuantumBased Machine Learning Simulation (QMLS). (arXiv:2308.08561v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08561
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;QMLS&#30340;&#26032;&#27010;&#24565;&#65292;&#36890;&#36807;&#32467;&#21512;&#26426;&#22120;&#23398;&#20064;&#21644;&#37327;&#23376;&#27169;&#25311;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#32553;&#30701;&#33647;&#29289;&#30740;&#21457;&#30340;&#26102;&#38388;&#21644;&#38477;&#20302;&#25104;&#26412;&#12290;&#36890;&#36807;&#29983;&#25104;&#21629;&#20013;&#29289;&#21644;&#20248;&#21270;&#20998;&#23376;&#30340;&#36807;&#31243;&#65292;&#21487;&#20197;&#22823;&#22823;&#25552;&#39640;&#33647;&#29289;&#21457;&#29616;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33647;&#29289;&#30740;&#21457;&#30340;&#30740;&#31350;&#19982;&#24320;&#21457;(R&amp;D)&#38454;&#27573;&#26159;&#19968;&#20010;&#28459;&#38271;&#32780;&#26114;&#36149;&#30340;&#36807;&#31243;&#12290;&#20026;&#20102;&#25913;&#38761;&#36825;&#20010;&#36807;&#31243;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26032;&#27010;&#24565;QMLS&#65292;&#23558;&#25972;&#20010;R&amp;D&#38454;&#27573;&#32553;&#30701;&#21040;&#19977;&#21040;&#20845;&#20010;&#26376;&#65292;&#25104;&#26412;&#20165;&#20026;&#20116;&#21040;&#20843;&#19975;&#32654;&#20803;&#12290;&#23545;&#20110;&#21629;&#20013;&#20135;&#29983;&#65292;&#26426;&#22120;&#23398;&#20064;&#20998;&#23376;&#29983;&#25104;(MLMG)&#26681;&#25454;&#30446;&#26631;&#34507;&#30333;&#30340;&#20998;&#23376;&#32467;&#26500;&#29983;&#25104;&#21487;&#33021;&#30340;&#21629;&#20013;&#29289;&#65292;&#32780;&#37327;&#23376;&#27169;&#25311;(QS)&#26681;&#25454;&#19982;&#30446;&#26631;&#34507;&#30333;&#30340;&#21453;&#24212;&#21644;&#32467;&#21512;&#25928;&#26524;&#36807;&#28388;&#21407;&#22987;&#23454;&#39564;&#20013;&#30340;&#20998;&#23376;&#12290;&#28982;&#21518;&#65292;&#23545;&#20110;&#38085;&#20248;&#21270;&#65292;&#20174;MLMG&#21644;QS&#29983;&#25104;&#21644;&#36807;&#28388;&#30340;&#32467;&#26524;&#20998;&#23376;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#20998;&#23376;&#21464;&#24322;(MLMV)&#23558;&#37027;&#20123;&#20986;&#29616;&#22312;&#20004;&#20010;&#36807;&#31243;&#20013;&#30340;&#20998;&#23376;&#21046;&#25104;&#25968;&#21313;&#31181;&#20998;&#23376;&#21464;&#20307;&#65292;&#32780;&#20854;&#20182;&#20998;&#23376;&#21482;&#21046;&#25104;&#20960;&#31181;&#21464;&#20307;&#12290;&#26368;&#21518;&#65292;&#25152;&#26377;&#20248;&#21270;&#30340;&#20998;&#23376;&#23558;&#32463;&#36807;&#22810;&#36718;&#39640;&#26631;&#20934;&#30340;QS&#36807;&#28388;&#65292;&#20197;&#30830;&#20445;&#21453;&#24212;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Research &amp; Development (R&amp;D) phase of drug development is a lengthy and costly process. To revolutionize this process, we introduce our new concept QMLS to shorten the whole R&amp;D phase to three to six months and decrease the cost to merely fifty to eighty thousand USD. For Hit Generation, Machine Learning Molecule Generation (MLMG) generates possible hits according to the molecular structure of the target protein while the Quantum Simulation (QS) filters molecules from the primary essay based on the reaction and binding effectiveness with the target protein. Then, For Lead Optimization, the resultant molecules generated and filtered from MLMG and QS are compared, and molecules that appear as a result of both processes will be made into dozens of molecular variations through Machine Learning Molecule Variation (MLMV), while others will only be made into a few variations. Lastly, all optimized molecules would undergo multiple rounds of QS filtering with a high standard for reaction ef
&lt;/p&gt;</description></item><item><title>&#20256;&#32479;&#30340;&#31526;&#21495;AI&#26041;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;AI&#26041;&#27861;&#26080;&#27861;&#28385;&#36275;&#21019;&#24314;&#24378;&#22823;&#21644;&#21487;&#20449;&#36182;&#30340;AI&#30340;&#25361;&#25112;&#65292;&#28982;&#32780;&#65292;&#21457;&#23637;&#33073;&#38772;&#27861;&#36890;&#36807;&#27169;&#20223;&#20154;&#31867;&#20799;&#31461;&#30340;&#33021;&#21147;&#21457;&#23637;&#36807;&#31243;&#65292;&#20026;&#21019;&#24314;&#31283;&#20581;&#21487;&#38752;&#30340;AI&#25552;&#20379;&#20102;&#24076;&#26395;&#12290;</title><link>http://arxiv.org/abs/2308.04586</link><description>&lt;p&gt;
AIs&#30340;&#21457;&#23637;&#33073;&#38772;&#27861;
&lt;/p&gt;
&lt;p&gt;
Developmental Bootstrapping of AIs. (arXiv:2308.04586v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04586
&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#31526;&#21495;AI&#26041;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;AI&#26041;&#27861;&#26080;&#27861;&#28385;&#36275;&#21019;&#24314;&#24378;&#22823;&#21644;&#21487;&#20449;&#36182;&#30340;AI&#30340;&#25361;&#25112;&#65292;&#28982;&#32780;&#65292;&#21457;&#23637;&#33073;&#38772;&#27861;&#36890;&#36807;&#27169;&#20223;&#20154;&#31867;&#20799;&#31461;&#30340;&#33021;&#21147;&#21457;&#23637;&#36807;&#31243;&#65292;&#20026;&#21019;&#24314;&#31283;&#20581;&#21487;&#38752;&#30340;AI&#25552;&#20379;&#20102;&#24076;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24403;&#21069;&#19968;&#20123;AI&#22312;&#23553;&#38381;&#30340;&#19990;&#30028;&#65292;&#22914;&#26827;&#30424;&#28216;&#25103;&#20013;&#36229;&#36234;&#20102;&#20154;&#31867;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#28151;&#20081;&#30340;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#34920;&#29616;&#26377;&#38480;&#12290;&#23427;&#20204;&#20250;&#29359;&#22855;&#24618;&#30340;&#38169;&#35823;&#32780;&#19988;&#27809;&#26377;&#24847;&#35782;&#21040;&#12290;&#23427;&#20204;&#24456;&#38590;&#21463;&#21040;&#25351;&#23548;&#65292;&#19981;&#33021;&#36816;&#29992;&#24120;&#35782;&#65292;&#32570;&#20047;&#22909;&#22855;&#24515;&#12290;&#23427;&#20204;&#19981;&#33021;&#25104;&#20026;&#33391;&#22909;&#30340;&#21512;&#20316;&#32773;&#12290;&#20256;&#32479;&#25163;&#21160;&#26500;&#24314;&#30340;&#31526;&#21495;AI&#26041;&#27861;&#26500;&#24314;&#30340;&#31995;&#32479;&#21644;&#20351;&#29992;&#29983;&#25104;&#21644;&#28145;&#24230;&#23398;&#20064;AI&#26041;&#27861;(&#21253;&#25324;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;)&#26500;&#24314;&#30340;&#31995;&#32479;&#37117;&#26080;&#27861;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;&#23427;&#20204;&#19981;&#36866;&#21512;&#21019;&#24314;&#24378;&#22823;&#21644;&#21487;&#20449;&#36182;&#30340;AI&#12290;&#23613;&#31649;&#27492;&#26041;&#27861;&#19981;&#23646;&#20110;&#20027;&#27969;&#30340;AI&#26041;&#27861;&#65292;&#20294;&#21457;&#23637;&#33073;&#38772;&#27861;&#26174;&#31034;&#20986;&#24076;&#26395;&#12290;&#22312;&#21457;&#23637;&#33073;&#38772;&#27861;&#20013;&#65292;AI&#20687;&#20154;&#31867;&#20799;&#31461;&#19968;&#26679;&#21457;&#23637;&#33021;&#21147;&#12290;&#23427;&#20204;&#20174;&#20808;&#22825;&#33021;&#21147;&#24320;&#22987;&#12290;&#20687;&#20154;&#31867;&#19968;&#26679;&#65292;&#23427;&#20204;&#19982;&#29615;&#22659;&#20114;&#21160;&#65292;&#24182;&#20174;&#20114;&#21160;&#20013;&#23398;&#20064;&#12290;&#23427;&#20204;&#36890;&#36807;&#33258;&#25105;&#21457;&#23637;&#30340;&#33021;&#21147;&#36880;&#27493;&#25193;&#23637;&#20808;&#22825;&#33021;&#21147;&#12290;&#23427;&#20204;&#20114;&#21160;&#24182;&#36880;&#28176;&#23558;&#25152;&#23398;&#24212;&#29992;&#20110;&#23454;&#38469;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although some current AIs surpass human abilities especially in closed worlds such as board games, their performance in the messy real world is limited. They make strange mistakes and do not notice them. They cannot be instructed easily, fail to use common sense, and lack curiosity. They do not make good collaborators. Neither systems built using the traditional manually-constructed symbolic AI approach nor systems built using generative and deep learning AI approaches including large language models (LLMs) can meet the challenges. They are not well suited for creating robust and trustworthy AIs. Although it is outside of mainstream AI approaches, developmental bootstrapping shows promise. In developmental bootstrapping, AIs develop competences like human children do. They start with innate competences. Like humans, they interact with the environment and learn from their interactions. They incrementally extend their innate competences with self-developed competences. They interact and 
&lt;/p&gt;</description></item><item><title>AgentBench&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLMs&#20316;&#20026;&#20195;&#29702;&#20154;&#30340;&#22810;&#32500;&#24230;&#22522;&#20934;&#65292;&#21457;&#29616;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#65292;&#21830;&#19994;LLMs&#22312;&#20805;&#24403;&#20195;&#29702;&#20154;&#26041;&#38754;&#34920;&#29616;&#24378;&#21170;&#65292;&#20294;&#19982;&#24320;&#28304;&#31454;&#20105;&#23545;&#25163;&#30456;&#27604;&#65292;&#23384;&#22312;&#26174;&#33879;&#24615;&#33021;&#24046;&#36317;&#12290;&#35813;&#30740;&#31350;&#25581;&#31034;&#20102;LLMs&#22312;&#38271;&#26399;&#25512;&#29702;&#12289;&#20915;&#31574;&#21644;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#19978;&#30340;&#29942;&#39048;&#12290;</title><link>http://arxiv.org/abs/2308.03688</link><description>&lt;p&gt;
AgentBench: &#35780;&#20272;LLMs&#20316;&#20026;&#20195;&#29702;&#20154;
&lt;/p&gt;
&lt;p&gt;
AgentBench: Evaluating LLMs as Agents. (arXiv:2308.03688v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03688
&lt;/p&gt;
&lt;p&gt;
AgentBench&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLMs&#20316;&#20026;&#20195;&#29702;&#20154;&#30340;&#22810;&#32500;&#24230;&#22522;&#20934;&#65292;&#21457;&#29616;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#65292;&#21830;&#19994;LLMs&#22312;&#20805;&#24403;&#20195;&#29702;&#20154;&#26041;&#38754;&#34920;&#29616;&#24378;&#21170;&#65292;&#20294;&#19982;&#24320;&#28304;&#31454;&#20105;&#23545;&#25163;&#30456;&#27604;&#65292;&#23384;&#22312;&#26174;&#33879;&#24615;&#33021;&#24046;&#36317;&#12290;&#35813;&#30740;&#31350;&#25581;&#31034;&#20102;LLMs&#22312;&#38271;&#26399;&#25512;&#29702;&#12289;&#20915;&#31574;&#21644;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#19978;&#30340;&#29942;&#39048;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21464;&#24471;&#36234;&#26469;&#36234;&#26234;&#33021;&#21644;&#33258;&#20027;&#65292;&#38024;&#23545;&#20256;&#32479;&#30340;NLP&#20219;&#21153;&#20043;&#22806;&#30340;&#29616;&#23454;&#19990;&#30028;&#23454;&#38469;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#36843;&#20999;&#38656;&#35201;&#22312;&#20114;&#21160;&#29615;&#22659;&#20013;&#35780;&#20272;LLMs&#20316;&#20026;&#20195;&#29702;&#20154;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#19978;&#30340;&#25512;&#29702;&#21644;&#20915;&#31574;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;AgentBench&#65292;&#19968;&#20010;&#22810;&#32500;&#24230;&#28436;&#21464;&#30340;&#22522;&#20934;&#65292;&#30446;&#21069;&#21253;&#25324;8&#20010;&#19981;&#21516;&#30340;&#29615;&#22659;&#65292;&#20197;&#35780;&#20272;LLM&#20316;&#20026;&#20195;&#29702;&#20154;&#22312;&#22810;&#36718;&#24320;&#25918;&#24335;&#29983;&#25104;&#35774;&#32622;&#20013;&#30340;&#25512;&#29702;&#21644;&#20915;&#31574;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;27&#20010;&#22522;&#20110;API&#21644;&#24320;&#28304;&#30340;LLM&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#27979;&#35797;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;&#39030;&#32423;&#21830;&#19994;LLM&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#20195;&#29702;&#20154;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#19982;&#24320;&#28304;&#31454;&#20105;&#23545;&#25163;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#24456;&#22823;&#12290;&#25105;&#20204;&#25214;&#20986;&#20102;&#29615;&#22659;&#21644;LLM&#20013;&#22833;&#36133;&#30340;&#20856;&#22411;&#21407;&#22240;&#65292;&#34920;&#26126;&#38271;&#26399;&#25512;&#29702;&#12289;&#20915;&#31574;&#21644;&#36981;&#24490;&#25351;&#31034;&#33021;&#21147;&#19981;&#20339;&#26159;&#24320;&#21457;&#21487;&#29992;LLM&#20195;&#29702;&#20154;&#30340;&#20027;&#35201;&#38556;&#30861;&#12290;&#36890;&#36807;&#23545;&#20195;&#30721;&#21644;&#39640;&#36136;&#37327;&#36827;&#34892;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are becoming increasingly smart and autonomous, targeting real-world pragmatic missions beyond traditional NLP tasks. As a result, there has been an urgent need to evaluate LLMs as agents on challenging tasks in interactive environments. We present AgentBench, a multi-dimensional evolving benchmark that currently consists of 8 distinct environments to assess LLM-as-Agent's reasoning and decision-making abilities in a multi-turn open-ended generation setting. Our extensive test over 27 API-based and open-sourced (OSS) LLMs shows that, while top commercial LLMs present a strong ability of acting as agents in complex environments, there is a significant disparity in performance between them and OSS competitors. We identify the typical reasons of failures in environments and LLMs, showing that poor long-term reasoning, decision-making, and instruction following abilities are the main obstacles for developing usable LLM agents. Training on code and high quality 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36873;&#25321;&#21644;&#22686;&#24378;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;&#25991;&#26412;&#22686;&#24378;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65292;&#36890;&#36807;&#22810;&#20219;&#21153;&#26694;&#26550;&#36873;&#25321;&#30456;&#20851;&#30340;&#25991;&#26412;&#25551;&#36848;&#65292;&#24182;&#23545;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#36827;&#34892;&#23545;&#40784;&#25110;&#22686;&#24378;&#12290;</title><link>http://arxiv.org/abs/2307.15776</link><description>&lt;p&gt;
&#36873;&#25321;&#21644;&#22686;&#24378;&#65306;&#22686;&#24378;&#31264;&#23494;&#26816;&#32034;&#30693;&#35782;&#22270;&#35889;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Select and Augment: Enhanced Dense Retrieval Knowledge Graph Augmentation. (arXiv:2307.15776v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15776
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36873;&#25321;&#21644;&#22686;&#24378;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;&#25991;&#26412;&#22686;&#24378;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65292;&#36890;&#36807;&#22810;&#20219;&#21153;&#26694;&#26550;&#36873;&#25321;&#30456;&#20851;&#30340;&#25991;&#26412;&#25551;&#36848;&#65292;&#24182;&#23545;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#36827;&#34892;&#23545;&#40784;&#25110;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31038;&#21306;&#20013;&#65292;&#23558;&#25991;&#26412;&#20449;&#24687;&#27880;&#20837;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#23454;&#20307;&#34920;&#31034;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#20540;&#24471;&#25506;&#32034;&#30340;&#39046;&#22495;&#65292;&#20197;&#25552;&#39640;KG&#30456;&#20851;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#24120;&#29992;&#30340;&#22806;&#37096;&#30693;&#35782;&#22686;&#24378;KG&#23884;&#20837;&#30340;&#26041;&#27861;&#21253;&#25324;&#35821;&#20041;&#20016;&#23500;&#30340;&#20381;&#36182;&#35299;&#26512;&#29305;&#24449;&#12289;&#19968;&#32452;&#30456;&#20851;&#20851;&#38190;&#35789;&#65292;&#20197;&#21450;&#26469;&#33258;&#22806;&#37096;&#35821;&#26009;&#24211;&#65288;&#22914;&#32500;&#22522;&#30334;&#31185;&#65289;&#30340;&#23436;&#25972;&#25991;&#26412;&#25551;&#36848;&#12290;&#23613;&#31649;&#36825;&#31181;&#21019;&#26032;&#65288;&#25991;&#26412;&#22686;&#24378;&#30340;KG&#23884;&#20837;&#65289;&#21462;&#24471;&#20102;&#19968;&#23450;&#30340;&#36827;&#23637;&#65292;&#20294;&#26412;&#25991;&#25552;&#20986;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;&#25105;&#20204;&#19981;&#20877;&#20351;&#29992;&#21333;&#19968;&#25991;&#26412;&#25551;&#36848;&#65288;&#22240;&#20026;&#25991;&#26412;&#30340;&#22266;&#26377;&#35821;&#20041;&#27495;&#20041;&#26080;&#27861;&#20805;&#20998;&#34920;&#31034;&#19968;&#20010;&#23454;&#20307;&#65289;&#65292;&#32780;&#26159;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#20219;&#21153;&#26694;&#26550;&#65292;&#26082;&#33021;&#36873;&#25321;&#19982;KG&#23454;&#20307;&#30456;&#20851;&#30340;&#19968;&#32452;&#25991;&#26412;&#25551;&#36848;&#65292;&#21448;&#33021;&#23558;KG&#23884;&#20837;&#19982;&#25991;&#26412;&#25551;&#36848;&#36827;&#34892;&#23545;&#40784;&#25110;&#22686;&#24378;&#12290;&#19982;&#20043;&#21069;&#23558;&#24418;&#24335;&#21270;&#23454;&#20307;&#25551;&#36848;&#25554;&#20837;&#30693;&#35782;&#24211;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#36825;&#19968;&#26041;&#27861;&#26159;&#25552;&#20379;&#20102;&#23545;KG&#23884;&#20837;&#36827;&#34892;&#22686;&#24378;&#21644;&#23545;&#40784;&#30340;&#26032;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Injecting textual information into knowledge graph (KG) entity representations has been a worthwhile expedition in terms of improving performance in KG oriented tasks within the NLP community. External knowledge often adopted to enhance KG embeddings ranges from semantically rich lexical dependency parsed features to a set of relevant key words to entire text descriptions supplied from an external corpus such as wikipedia and many more. Despite the gains this innovation (Text-enhanced KG embeddings) has made, the proposal in this work suggests that it can be improved even further. Instead of using a single text description (which would not sufficiently represent an entity because of the inherent lexical ambiguity of text), we propose a multi-task framework that jointly selects a set of text descriptions relevant to KG entities as well as align or augment KG embeddings with text descriptions. Different from prior work that plugs formal entity descriptions declared in knowledge bases, th
&lt;/p&gt;</description></item><item><title>WebArena&#26159;&#19968;&#20010;&#29992;&#20110;&#26500;&#24314;&#33258;&#20027;&#26234;&#33021;&#20307;&#30340;&#30495;&#23454;&#32593;&#32476;&#29615;&#22659;&#65292;&#23427;&#21253;&#21547;&#20102;&#23436;&#20840;&#21151;&#33021;&#30340;&#32593;&#31449;&#65292;&#24182;&#19988;&#36890;&#36807;&#24341;&#20837;&#24037;&#20855;&#21644;&#22806;&#37096;&#30693;&#35782;&#24211;&#26469;&#40723;&#21169;&#26234;&#33021;&#20307;&#20687;&#20154;&#31867;&#19968;&#26679;&#35299;&#20915;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;WebArena&#36824;&#21457;&#24067;&#20102;&#19968;&#32452;&#29992;&#20110;&#35780;&#20272;&#20219;&#21153;&#23436;&#25104;&#21151;&#33021;&#27491;&#30830;&#24615;&#30340;&#22522;&#20934;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2307.13854</link><description>&lt;p&gt;
WebArena: &#19968;&#20010;&#29992;&#20110;&#26500;&#24314;&#33258;&#20027;&#26234;&#33021;&#20307;&#30340;&#30495;&#23454;&#32593;&#32476;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
WebArena: A Realistic Web Environment for Building Autonomous Agents. (arXiv:2307.13854v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13854
&lt;/p&gt;
&lt;p&gt;
WebArena&#26159;&#19968;&#20010;&#29992;&#20110;&#26500;&#24314;&#33258;&#20027;&#26234;&#33021;&#20307;&#30340;&#30495;&#23454;&#32593;&#32476;&#29615;&#22659;&#65292;&#23427;&#21253;&#21547;&#20102;&#23436;&#20840;&#21151;&#33021;&#30340;&#32593;&#31449;&#65292;&#24182;&#19988;&#36890;&#36807;&#24341;&#20837;&#24037;&#20855;&#21644;&#22806;&#37096;&#30693;&#35782;&#24211;&#26469;&#40723;&#21169;&#26234;&#33021;&#20307;&#20687;&#20154;&#31867;&#19968;&#26679;&#35299;&#20915;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;WebArena&#36824;&#21457;&#24067;&#20102;&#19968;&#32452;&#29992;&#20110;&#35780;&#20272;&#20219;&#21153;&#23436;&#25104;&#21151;&#33021;&#27491;&#30830;&#24615;&#30340;&#22522;&#20934;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#23637;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#36827;&#34892;&#26085;&#24120;&#20219;&#21153;&#30340;&#33258;&#20027;&#26234;&#33021;&#20307;&#30340;&#28508;&#21147;&#36880;&#28176;&#26174;&#29616;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26234;&#33021;&#20307;&#20027;&#35201;&#26159;&#22312;&#31616;&#21270;&#30340;&#21512;&#25104;&#29615;&#22659;&#20013;&#21019;&#24314;&#21644;&#27979;&#35797;&#30340;&#65292;&#20005;&#37325;&#38480;&#21046;&#20102;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#39640;&#24230;&#36924;&#30495;&#19988;&#21487;&#22797;&#29616;&#30340;&#26234;&#33021;&#20307;&#25351;&#20196;&#21644;&#25511;&#21046;&#29615;&#22659;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20851;&#27880;&#22312;&#32593;&#31449;&#19978;&#25191;&#34892;&#20219;&#21153;&#30340;&#26234;&#33021;&#20307;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#26469;&#33258;&#22235;&#20010;&#24120;&#35265;&#39046;&#22495;&#30340;&#23436;&#20840;&#21151;&#33021;&#32593;&#31449;&#30340;&#29615;&#22659;&#65292;&#20998;&#21035;&#26159;&#30005;&#23376;&#21830;&#21153;&#12289;&#31038;&#20132;&#35770;&#22363;&#35752;&#35770;&#12289;&#21327;&#21516;&#36719;&#20214;&#24320;&#21457;&#21644;&#20869;&#23481;&#31649;&#29702;&#12290;&#25105;&#20204;&#30340;&#29615;&#22659;&#20351;&#29992;&#24037;&#20855;&#65288;&#22914;&#22320;&#22270;&#65289;&#21644;&#22806;&#37096;&#30693;&#35782;&#24211;&#65288;&#22914;&#29992;&#25143;&#25163;&#20876;&#65289;&#26469;&#40723;&#21169;&#20687;&#20154;&#31867;&#19968;&#26679;&#35299;&#20915;&#20219;&#21153;&#12290;&#22312;&#25105;&#20204;&#30340;&#29615;&#22659;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#32452;&#37325;&#28857;&#35780;&#20272;&#20219;&#21153;&#23436;&#25104;&#21151;&#33021;&#27491;&#30830;&#24615;&#30340;&#22522;&#20934;&#20219;&#21153;&#12290;&#25105;&#20204;&#22522;&#20934;&#20219;&#21153;&#20855;&#26377;&#22810;&#26679;&#24615;&#21644;&#38271;&#36828;&#30340;&#35270;&#37326;&#65292;&#24182;&#19988;&#34987;&#35774;&#35745;&#20026;&#40723;&#21169;&#26234;&#33021;&#20307;&#36827;&#34892;&#26356;&#28145;&#23618;&#27425;&#30340;&#20219;&#21153;&#29702;&#35299;&#21644;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;
With generative AI advances, the exciting potential for autonomous agents to manage daily tasks via natural language commands has emerged. However, cur rent agents are primarily created and tested in simplified synthetic environments, substantially limiting real-world scenario representation. In this paper, we build an environment for agent command and control that is highly realistic and reproducible. Specifically, we focus on agents that perform tasks on websites, and we create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management. Our environment is enriched with tools (e.g., a map) and external knowledge bases (e.g., user manuals) to encourage human-like task-solving. Building upon our environment, we release a set of benchmark tasks focusing on evaluating the functional correctness of task completions. The tasks in our benchmark are diverse, long-horizon, and are desi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#20010;&#24615;&#21270;&#33647;&#29289;&#21327;&#21516;&#20316;&#29992;&#24182;&#36827;&#34892;&#33647;&#29289;&#35774;&#35745;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#23567;&#22411;&#30340;&#20010;&#24615;&#21270;&#25968;&#25454;&#38598;&#65292;&#19981;&#20381;&#36182;&#20110;&#25991;&#26412;&#35821;&#26009;&#24211;&#12289;&#20998;&#23376;&#25351;&#32441;&#25110;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#30340;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#65292;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.11694</link><description>&lt;p&gt;
SynerGPT:&#19978;&#19979;&#25991;&#23398;&#20064;&#29992;&#20110;&#20010;&#24615;&#21270;&#33647;&#29289;&#21327;&#21516;&#20316;&#29992;&#39044;&#27979;&#21644;&#33647;&#29289;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
SynerGPT: In-Context Learning for Personalized Drug Synergy Prediction and Drug Design. (arXiv:2307.11694v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11694
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#20010;&#24615;&#21270;&#33647;&#29289;&#21327;&#21516;&#20316;&#29992;&#24182;&#36827;&#34892;&#33647;&#29289;&#35774;&#35745;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#23567;&#22411;&#30340;&#20010;&#24615;&#21270;&#25968;&#25454;&#38598;&#65292;&#19981;&#20381;&#36182;&#20110;&#25991;&#26412;&#35821;&#26009;&#24211;&#12289;&#20998;&#23376;&#25351;&#32441;&#25110;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#30340;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#65292;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#33647;&#29289;&#30340;&#21327;&#21516;&#32452;&#21512;&#21487;&#20197;&#21152;&#36895;&#30284;&#30151;&#27835;&#30103;&#30340;&#21457;&#29616;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#27963;&#26816;&#32454;&#32990;&#20010;&#24615;&#21270;&#30340;&#27835;&#30103;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35774;&#32622;&#21644;&#27169;&#22411;&#29992;&#20110;&#19978;&#19979;&#25991;&#20013;&#30340;&#33647;&#29289;&#21327;&#21516;&#23398;&#20064;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#19968;&#20010;&#23567;&#30340;&#8220;&#20010;&#24615;&#21270;&#25968;&#25454;&#38598;&#8221;&#65292;&#20854;&#20013;&#21253;&#21547;&#29305;&#23450;&#30284;&#30151;&#38774;&#32454;&#32990;&#19978;&#19979;&#25991;&#20013;&#30340;10-20&#20010;&#33647;&#29289;&#21327;&#21516;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#39044;&#27979;&#35813;&#19978;&#19979;&#25991;&#20013;&#30340;&#39069;&#22806;&#33647;&#29289;&#21327;&#21516;&#20851;&#31995;&#12290;&#21463;&#26368;&#36817;&#24037;&#20316;&#30340;&#21551;&#21457;&#65292;&#35813;&#24037;&#20316;&#36890;&#36807;&#39044;&#35757;&#32451;GPT&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#26469;&#8220;&#19978;&#19979;&#25991;&#23398;&#20064;&#8221;&#24120;&#35265;&#30340;&#21151;&#33021;&#31867;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181; &#26032;&#30340;&#39044;&#35757;&#32451;&#26041;&#26696;&#65292;&#20351;GPT&#27169;&#22411;&#33021;&#22815;&#19978;&#19979;&#25991;&#23398;&#20064;&#8220;&#33647;&#29289;&#21327;&#21516;&#21151;&#33021;&#8221;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411; - &#19981;&#20351;&#29992;&#20219;&#20309;&#25991;&#26412;&#35821;&#26009;&#24211;&#65292;&#20998;&#23376;&#25351;&#32441;&#65292;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#25110;&#20219;&#20309;&#20854;&#20182;&#39046;&#22495;&#29305;&#23450;&#30340;&#30693;&#35782; - &#33021;&#22815;&#21462;&#24471;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;&#25105;&#20204;&#30340;&#19978;&#19979;&#25991;&#26041;&#27861;&#19982;&#36951;&#20256;&#31639;&#27861;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#20248;&#21270;&#27169;&#22411;&#25552;&#31034;&#24182;&#36873;&#25321;&#21327;&#21516;&#20505;&#36873;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting synergistic drug combinations can help accelerate discovery of cancer treatments, particularly therapies personalized to a patient's specific tumor via biopsied cells. In this paper, we propose a novel setting and models for in-context drug synergy learning. We are given a small "personalized dataset" of 10-20 drug synergy relationships in the context of specific cancer cell targets. Our goal is to predict additional drug synergy relationships in that context. Inspired by recent work that pre-trains a GPT language model (LM) to "in-context learn" common function classes, we devise novel pre-training schemes that enable a GPT model to in-context learn "drug synergy functions". Our model -- which does not use any textual corpora, molecular fingerprints, protein interaction or any other domain-specific knowledge -- is able to achieve competitive results. We further integrate our in-context approach with a genetic algorithm to optimize model prompts and select synergy candidates
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22823;&#22411;&#25163;&#24037;&#26631;&#35760;&#26118;&#34411;&#22270;&#20687;&#25968;&#25454;&#38598;BIOSCAN-Insect&#65292;&#29992;&#20110;&#23545;&#26118;&#34411;&#29983;&#29289;&#22810;&#26679;&#24615;&#36827;&#34892;&#32534;&#30446;&#12290;&#35813;&#25968;&#25454;&#38598;&#36824;&#20855;&#26377;&#24341;&#20154;&#27880;&#30446;&#30340;&#29305;&#24449;&#65292;&#23545;&#24191;&#27867;&#30340;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#20063;&#20855;&#26377;&#30740;&#31350;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2307.10455</link><description>&lt;p&gt;
&#26397;&#30528;&#20840;&#29699;&#29983;&#29289;&#22810;&#26679;&#24615;&#35780;&#20272;&#36808;&#20986;&#30340;&#19968;&#27493;&#65306;BIOSCAN-1M&#26118;&#34411;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
A Step Towards Worldwide Biodiversity Assessment: The BIOSCAN-1M Insect Dataset. (arXiv:2307.10455v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10455
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22823;&#22411;&#25163;&#24037;&#26631;&#35760;&#26118;&#34411;&#22270;&#20687;&#25968;&#25454;&#38598;BIOSCAN-Insect&#65292;&#29992;&#20110;&#23545;&#26118;&#34411;&#29983;&#29289;&#22810;&#26679;&#24615;&#36827;&#34892;&#32534;&#30446;&#12290;&#35813;&#25968;&#25454;&#38598;&#36824;&#20855;&#26377;&#24341;&#20154;&#27880;&#30446;&#30340;&#29305;&#24449;&#65292;&#23545;&#24191;&#27867;&#30340;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#20063;&#20855;&#26377;&#30740;&#31350;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#23545;&#26118;&#34411;&#29983;&#29289;&#22810;&#26679;&#24615;&#36827;&#34892;&#32534;&#30446;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22823;&#22411;&#25163;&#24037;&#26631;&#35760;&#26118;&#34411;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#21363;BIOSCAN-Insect&#25968;&#25454;&#38598;&#12290;&#27599;&#20010;&#35760;&#24405;&#37117;&#30001;&#19987;&#23478;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#19988;&#20855;&#26377;&#30456;&#20851;&#30340;&#36951;&#20256;&#20449;&#24687;&#65292;&#21253;&#25324;&#21407;&#22987;&#26680;&#33527;&#37240;&#26465;&#24418;&#30721;&#24207;&#21015;&#21644;&#20998;&#37197;&#30340;&#26465;&#24418;&#30721;&#32034;&#24341;&#21495;&#65292;&#36825;&#20123;&#26159;&#22522;&#20110;&#36951;&#20256;&#30340;&#29289;&#31181;&#20998;&#31867;&#30340;&#20195;&#29702;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#31934;&#36873;&#30340;&#30334;&#19975;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#20027;&#35201;&#29992;&#20110;&#35757;&#32451;&#33021;&#22815;&#25552;&#20379;&#22522;&#20110;&#22270;&#20687;&#30340;&#20998;&#31867;&#35780;&#20272;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#65292;&#20294;&#35813;&#25968;&#25454;&#38598;&#36824;&#20855;&#26377;&#24341;&#20154;&#27880;&#30446;&#30340;&#29305;&#24449;&#65292;&#23545;&#24191;&#27867;&#30340;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#20063;&#20855;&#26377;&#30740;&#31350;&#20215;&#20540;&#12290;&#30001;&#20110;&#25968;&#25454;&#38598;&#22266;&#26377;&#30340;&#29983;&#29289;&#24615;&#36136;&#65292;&#23637;&#29616;&#20986;&#20102;&#20855;&#26377;&#38271;&#23614;&#31867;&#21035;&#19981;&#24179;&#34913;&#20998;&#24067;&#30340;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#20998;&#31867;&#26631;&#31614;&#26159;&#19968;&#20010;&#20998;&#23618;&#20998;&#31867;&#26041;&#26696;&#65292;&#22312;&#36739;&#20302;&#32423;&#21035;&#19978;&#21576;&#29616;&#20986;&#39640;&#24230;&#32454;&#31890;&#24230;&#30340;&#20998;&#31867;&#38382;&#39064;&#12290;&#38500;&#20102;&#28608;&#21457;&#23545;&#29983;&#29289;&#22810;&#26679;&#24615;&#30740;&#31350;&#30340;&#20852;&#36259;&#22806;&#65292;&#35813;&#25968;&#25454;&#38598;&#36824;&#20419;&#36827;&#20102;&#23545;&#26426;&#22120;&#23398;&#20064;&#30340;&#28145;&#20837;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
In an effort to catalog insect biodiversity, we propose a new large dataset of hand-labelled insect images, the BIOSCAN-Insect Dataset. Each record is taxonomically classified by an expert, and also has associated genetic information including raw nucleotide barcode sequences and assigned barcode index numbers, which are genetically-based proxies for species classification. This paper presents a curated million-image dataset, primarily to train computer-vision models capable of providing image-based taxonomic assessment, however, the dataset also presents compelling characteristics, the study of which would be of interest to the broader machine learning community. Driven by the biological nature inherent to the dataset, a characteristic long-tailed class-imbalance distribution is exhibited. Furthermore, taxonomic labelling is a hierarchical classification scheme, presenting a highly fine-grained classification problem at lower levels. Beyond spurring interest in biodiversity research w
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#31995;&#32479;&#27604;&#36739;&#20102;&#24037;&#19994;&#24212;&#29992;&#20013;&#30340;&#36719;&#20214;&#20195;&#29702;&#21644;&#25968;&#23383;&#23402;&#29983;&#12290;&#30740;&#31350;&#26088;&#22312;&#30830;&#23450;&#36825;&#20004;&#31181;&#33539;&#24335;&#20043;&#38388;&#30340;&#24046;&#24322;&#12289;&#30456;&#20284;&#20043;&#22788;&#21644;&#28508;&#22312;&#30340;&#21327;&#21516;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.08421</link><description>&lt;p&gt;
&#36719;&#20214;&#20195;&#29702;&#21644;&#25968;&#23383;&#23402;&#29983;&#30340;&#31995;&#32479;&#27604;&#36739;&#65306;&#22312;&#24037;&#19994;&#29983;&#20135;&#20013;&#30340;&#24046;&#24322;&#12289;&#30456;&#20284;&#20043;&#22788;&#21644;&#21327;&#21516;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Systematic Comparison of Software Agents and Digital Twins: Differences, Similarities, and Synergies in Industrial Production. (arXiv:2307.08421v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08421
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#31995;&#32479;&#27604;&#36739;&#20102;&#24037;&#19994;&#24212;&#29992;&#20013;&#30340;&#36719;&#20214;&#20195;&#29702;&#21644;&#25968;&#23383;&#23402;&#29983;&#12290;&#30740;&#31350;&#26088;&#22312;&#30830;&#23450;&#36825;&#20004;&#31181;&#33539;&#24335;&#20043;&#38388;&#30340;&#24046;&#24322;&#12289;&#30456;&#20284;&#20043;&#22788;&#21644;&#28508;&#22312;&#30340;&#21327;&#21516;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#23454;&#29616;&#39640;&#24230;&#28789;&#27963;&#21644;&#21487;&#21464;&#30340;&#29983;&#20135;&#65292;&#24037;&#19994;&#29983;&#20135;&#31995;&#32479;&#36880;&#28176;&#21464;&#24471;&#26356;&#21152;&#20998;&#25955;&#12289;&#20114;&#32852;&#21644;&#26234;&#33021;&#21270;&#12290;&#22312;&#36825;&#20010;&#24895;&#26223;&#20013;&#65292;&#29983;&#20135;&#36164;&#20135;&#24444;&#27492;&#21512;&#20316;&#65292;&#23637;&#31034;&#39640;&#24230;&#30340;&#33258;&#20027;&#24615;&#12290;&#27492;&#22806;&#65292;&#20010;&#20307;&#29983;&#20135;&#36164;&#20135;&#30340;&#30693;&#35782;&#22312;&#23427;&#20204;&#25972;&#20010;&#29983;&#21629;&#21608;&#26399;&#20013;&#26159;&#38543;&#26102;&#21487;&#29992;&#30340;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#24895;&#26223;&#65292;&#38656;&#35201;&#20805;&#20998;&#21033;&#29992;&#20449;&#24687;&#25216;&#26415;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#20004;&#31181;&#24120;&#29992;&#30340;&#36719;&#20214;&#33539;&#24335;&#26159;&#36719;&#20214;&#20195;&#29702;&#65288;&#31616;&#31216;&#20195;&#29702;&#65289;&#21644;&#25968;&#23383;&#23402;&#29983;&#65288;DT&#65289;&#12290;&#26412;&#30740;&#31350;&#23545;&#24037;&#19994;&#24212;&#29992;&#20013;&#30340;&#20195;&#29702;&#21644;&#25968;&#23383;&#23402;&#29983;&#36827;&#34892;&#20102;&#31995;&#32479;&#27604;&#36739;&#12290;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#30830;&#23450;&#36825;&#20004;&#31181;&#33539;&#24335;&#20043;&#38388;&#30340;&#24046;&#24322;&#12289;&#30456;&#20284;&#20043;&#22788;&#21644;&#28508;&#22312;&#30340;&#21327;&#21516;&#20316;&#29992;&#12290;&#27604;&#36739;&#22522;&#20110;&#20195;&#29702;&#21644;&#25968;&#23383;&#23402;&#29983;&#30340;&#24212;&#29992;&#30446;&#30340;&#12289;&#36825;&#20123;&#36719;&#20214;&#33539;&#24335;&#23637;&#31034;&#30340;&#23646;&#24615;&#21644;&#33021;&#21147;&#65292;&#20197;&#21450;&#23427;&#20204;&#22914;&#20309;&#22312;&#21442;&#32771;&#26694;&#26550;&#20013;&#20998;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
To achieve a highly agile and flexible production, it is envisioned that industrial production systems gradually become more decentralized, interconnected, and intelligent. Within this vision, production assets collaborate with each other, exhibiting a high degree of autonomy. Furthermore, knowledge about individual production assets is readily available throughout their entire life-cycles. To realize this vision, adequate use of information technology is required. Two commonly applied software paradigms in this context are Software Agents (referred to as Agents) and Digital Twins (DTs). This work presents a systematic comparison of Agents and DTs in industrial applications. The goal of the study is to determine the differences, similarities, and potential synergies between the two paradigms. The comparison is based on the purposes for which Agents and DTs are applied, the properties and capabilities exhibited by these software paradigms, and how they can be allocated within the Refere
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#22522;&#20110;&#20248;&#21270;&#21644;&#22522;&#20110;&#23398;&#20064;&#26041;&#27861;&#30340;&#38646;&#26679;&#26412;&#25193;&#25955;&#20248;&#21270;&#65288;ZeDO&#65289;&#31649;&#36947;&#65292;&#29992;&#20110;&#35299;&#20915;3D&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#20013;&#30340;&#36328;&#39046;&#22495;&#21644;&#37326;&#22806;&#25361;&#25112;&#65292;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.03833</link><description>&lt;p&gt;
&#22238;&#24402;&#20248;&#21270;&#65306;&#22522;&#20110;&#25193;&#25955;&#30340;&#38646;&#26679;&#26412;3D&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Back to Optimization: Diffusion-based Zero-Shot 3D Human Pose Estimation. (arXiv:2307.03833v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03833
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#22522;&#20110;&#20248;&#21270;&#21644;&#22522;&#20110;&#23398;&#20064;&#26041;&#27861;&#30340;&#38646;&#26679;&#26412;&#25193;&#25955;&#20248;&#21270;&#65288;ZeDO&#65289;&#31649;&#36947;&#65292;&#29992;&#20110;&#35299;&#20915;3D&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#20013;&#30340;&#36328;&#39046;&#22495;&#21644;&#37326;&#22806;&#25361;&#25112;&#65292;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#22312;&#22823;&#22810;&#25968;&#22522;&#20934;&#27979;&#35797;&#20013;&#27604;&#20256;&#32479;&#30340;&#22522;&#20110;&#20248;&#21270;&#30340;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#65292;&#23427;&#20204;&#20027;&#23548;&#20102;3D&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22312;&#37326;&#22806;&#30340;3D&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#20173;&#28982;&#26159;&#22522;&#20110;&#23398;&#20064;&#30340;&#27169;&#22411;&#38754;&#20020;&#30340;&#26368;&#22823;&#25361;&#25112;&#65292;&#26080;&#35770;&#26159;2D-3D&#25552;&#21319;&#65292;&#22270;&#20687;&#21040;3D&#36824;&#26159;&#22522;&#20110;&#25193;&#25955;&#30340;&#26041;&#27861;&#65292;&#22240;&#20026;&#35757;&#32451;&#30340;&#32593;&#32476;&#38544;&#21547;&#22320;&#23398;&#20064;&#20102;&#30456;&#26426;&#20869;&#21442;&#21644;&#22522;&#20110;&#39046;&#22495;&#30340;3D&#20154;&#20307;&#23039;&#21183;&#20998;&#24067;&#65292;&#24182;&#36890;&#36807;&#32479;&#35745;&#24179;&#22343;&#26469;&#20272;&#35745;&#23039;&#21183;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22522;&#20110;&#20248;&#21270;&#30340;&#26041;&#27861;&#21487;&#20197;&#36880;&#26696;&#20363;&#20272;&#35745;&#32467;&#26524;&#65292;&#33021;&#22815;&#22312;&#37326;&#22806;&#39044;&#27979;&#26356;&#22810;&#26679;&#21270;&#21644;&#22797;&#26434;&#30340;&#20154;&#20307;&#23039;&#21183;&#12290;&#36890;&#36807;&#32467;&#21512;&#22522;&#20110;&#20248;&#21270;&#21644;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#30340;&#20248;&#21183;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#25193;&#25955;&#20248;&#21270;&#65288;ZeDO&#65289;&#31649;&#36947;&#29992;&#20110;&#35299;&#20915;&#36328;&#39046;&#22495;&#21644;&#37326;&#22806;3D&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#22810;&#20551;&#35774;ZeDO&#22312;Human3.6M&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65288;minMPJPE 51.4mm&#65289;&#65292;&#24182;&#19988;&#26080;&#38656;&#23545;&#20854;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning-based methods have dominated the 3D human pose estimation (HPE) tasks with significantly better performance in most benchmarks than traditional optimization-based methods. Nonetheless, 3D HPE in the wild is still the biggest challenge of learning-based models, whether with 2D-3D lifting, image-to-3D, or diffusion-based methods, since the trained networks implicitly learn camera intrinsic parameters and domain-based 3D human pose distributions and estimate poses by statistical average. On the other hand, the optimization-based methods estimate results case-by-case, which can predict more diverse and sophisticated human poses in the wild. By combining the advantages of optimization-based and learning-based methods, we propose the Zero-shot Diffusion-based Optimization (ZeDO) pipeline for 3D HPE to solve the problem of cross-domain and in-the-wild 3D HPE. Our multi-hypothesis ZeDO achieves state-of-the-art (SOTA) performance on Human3.6M as minMPJPE $51.4$mm without training with
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#20351;&#29992;&#21069;softmax&#20998;&#25968;&#30340;&#24402;&#23646;&#26041;&#27861;&#30340;&#19968;&#20010;&#28431;&#27934;&#65292;&#35813;&#26041;&#27861;&#29992;&#20110;&#35299;&#37322;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#36755;&#20986;&#12290;&#19982;&#23545;&#25239;&#24615;&#25915;&#20987;&#19981;&#21516;&#65292;&#20316;&#32773;&#20851;&#27880;&#30340;&#26159;&#23545;&#24402;&#23646;&#26041;&#27861;&#36827;&#34892;&#23567;&#20462;&#25913;&#21487;&#33021;&#23548;&#33268;&#30340;&#24433;&#21709;&#65292;&#32780;&#19981;&#20250;&#25913;&#21464;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;</title><link>http://arxiv.org/abs/2307.03305</link><description>&lt;p&gt;
&#20351;&#29992;&#21069;softmax&#20998;&#25968;&#30340;&#24402;&#23646;&#26041;&#27861;&#30340;&#19968;&#20010;&#28431;&#27934;
&lt;/p&gt;
&lt;p&gt;
A Vulnerability of Attribution Methods Using Pre-Softmax Scores. (arXiv:2307.03305v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03305
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#20351;&#29992;&#21069;softmax&#20998;&#25968;&#30340;&#24402;&#23646;&#26041;&#27861;&#30340;&#19968;&#20010;&#28431;&#27934;&#65292;&#35813;&#26041;&#27861;&#29992;&#20110;&#35299;&#37322;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#36755;&#20986;&#12290;&#19982;&#23545;&#25239;&#24615;&#25915;&#20987;&#19981;&#21516;&#65292;&#20316;&#32773;&#20851;&#27880;&#30340;&#26159;&#23545;&#24402;&#23646;&#26041;&#27861;&#36827;&#34892;&#23567;&#20462;&#25913;&#21487;&#33021;&#23548;&#33268;&#30340;&#24433;&#21709;&#65292;&#32780;&#19981;&#20250;&#25913;&#21464;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35752;&#35770;&#20102;&#19968;&#31867;&#29992;&#20110;&#35299;&#37322;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#36755;&#20986;&#30340;&#24402;&#23646;&#26041;&#27861;&#30340;&#19968;&#20010;&#28431;&#27934;&#12290;&#24050;&#30693;&#36825;&#31181;&#31867;&#22411;&#30340;&#32593;&#32476;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#21363;&#36755;&#20837;&#30340;&#24494;&#23567;&#25200;&#21160;&#21487;&#33021;&#20250;&#25913;&#21464;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;&#19982;&#27492;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#20851;&#27880;&#30340;&#26159;&#23545;&#24402;&#23646;&#26041;&#27861;&#36827;&#34892;&#23567;&#20462;&#25913;&#21487;&#33021;&#23548;&#33268;&#30340;&#24433;&#21709;&#65292;&#32780;&#19981;&#20250;&#25913;&#21464;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
We discuss a vulnerability involving a category of attribution methods used to provide explanations for the outputs of convolutional neural networks working as classifiers. It is known that this type of networks are vulnerable to adversarial attacks, in which imperceptible perturbations of the input may alter the outputs of the model. In contrast, here we focus on effects that small modifications in the model may cause on the attribution method without altering the model outputs.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#35774;&#35745;&#29992;&#20110;&#39640;&#25928;&#23398;&#20064;&#24179;&#38754;&#22270;&#23436;&#22791;&#19981;&#21464;&#37327;&#30340;&#26550;&#26500;&#12290;</title><link>http://arxiv.org/abs/2307.01180</link><description>&lt;p&gt;
PlanE: &#24179;&#38754;&#22270;&#30340;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
PlanE: Representation Learning over Planar Graphs. (arXiv:2307.01180v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01180
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#35774;&#35745;&#29992;&#20110;&#39640;&#25928;&#23398;&#20064;&#24179;&#38754;&#22270;&#23436;&#22791;&#19981;&#21464;&#37327;&#30340;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#26159;&#29992;&#20110;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#26480;&#20986;&#27169;&#22411;&#65292;&#20854;&#24605;&#24819;&#26159;&#36890;&#36807;&#19968;&#31995;&#21015;&#21464;&#25442;&#26469;&#36845;&#20195;&#35745;&#31639;&#36755;&#20837;&#22270;&#20013;&#33410;&#28857;&#30340;&#34920;&#31034;&#65292;&#20174;&#32780;&#23398;&#20064;&#21040;&#30340;&#22270;&#20989;&#25968;&#22312;&#22270;&#21516;&#26500;&#26102;&#26159;&#19981;&#21464;&#30340;&#65292;&#20174;&#32780;&#20351;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#25104;&#20026;&#22270;&#19981;&#21464;&#37327;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20247;&#25152;&#21608;&#30693;&#65292;&#36825;&#31867;&#27169;&#22411;&#23398;&#20064;&#21040;&#30340;&#22270;&#19981;&#21464;&#37327;&#26159;&#19981;&#23436;&#22791;&#30340;&#65306;&#23384;&#22312;&#19968;&#20123;&#38750;&#21516;&#26500;&#30340;&#22270;&#23545;&#65292;&#26631;&#20934;&#22270;&#31070;&#32463;&#32593;&#32476;&#26080;&#27861;&#21306;&#20998;&#23427;&#20204;&#12290;&#36825;&#22312;&#23545;&#19968;&#33324;&#22270;&#36827;&#34892;&#21516;&#26500;&#24615;&#27979;&#35797;&#30340;&#35745;&#31639;&#22256;&#38590;&#24615;&#30340;&#24773;&#20917;&#19979;&#24182;&#19981;&#20196;&#20154;&#24778;&#35766;&#65292;&#20294;&#23545;&#20110;&#19968;&#20123;&#29305;&#27530;&#30340;&#22270;&#31867;&#26469;&#35828;&#65292;&#24773;&#20917;&#21487;&#33021;&#26377;&#25152;&#19981;&#21516;&#65292;&#20363;&#22914;&#24179;&#38754;&#22270;&#65292;&#23545;&#20110;&#36825;&#20123;&#22270;&#65292;&#24050;&#30693;&#23384;&#22312;&#39640;&#25928;&#30340;&#22270;&#21516;&#26500;&#27979;&#35797;&#31639;&#27861;&#12290;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#35774;&#35745;&#29992;&#20110;&#39640;&#25928;&#23398;&#20064;&#24179;&#38754;&#22270;&#23436;&#22791;&#19981;&#21464;&#37327;&#30340;&#26550;&#26500;&#12290;&#21463;Hopcroft&#21644;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks are prominent models for representation learning over graphs, where the idea is to iteratively compute representations of nodes of an input graph through a series of transformations in such a way that the learned graph function is isomorphism invariant on graphs, which makes the learned representations graph invariants. On the other hand, it is well-known that graph invariants learned by these class of models are incomplete: there are pairs of non-isomorphic graphs which cannot be distinguished by standard graph neural networks. This is unsurprising given the computational difficulty of graph isomorphism testing on general graphs, but the situation begs to differ for special graph classes, for which efficient graph isomorphism testing algorithms are known, such as planar graphs. The goal of this work is to design architectures for efficiently learning complete invariants of planar graphs. Inspired by the classical planar graph isomorphism algorithm of Hopcroft and
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20998;&#31163;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;SPINN&#65289;&#65292;&#36890;&#36807;&#36880;&#20010;&#22788;&#29702;&#36724;&#26469;&#26174;&#33879;&#20943;&#23569;&#20102;&#22810;&#32500; PDE &#20013;&#30340;&#32593;&#32476;&#20256;&#25773;&#25968;&#37327;&#65292;&#24182;&#20351;&#29992;&#27491;&#21521;&#27169;&#24335;&#33258;&#21160;&#24494;&#20998;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#65292;&#20351;&#24471;&#21487;&#20197;&#22312;&#21333;&#20010;&#26222;&#36890; GPU &#19978;&#20351;&#29992;&#22823;&#37327;&#30340;&#37197;&#28857;&#12290;</title><link>http://arxiv.org/abs/2306.15969</link><description>&lt;p&gt;
&#21487;&#20998;&#31163;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Separable Physics-Informed Neural Networks. (arXiv:2306.15969v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15969
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20998;&#31163;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;SPINN&#65289;&#65292;&#36890;&#36807;&#36880;&#20010;&#22788;&#29702;&#36724;&#26469;&#26174;&#33879;&#20943;&#23569;&#20102;&#22810;&#32500; PDE &#20013;&#30340;&#32593;&#32476;&#20256;&#25773;&#25968;&#37327;&#65292;&#24182;&#20351;&#29992;&#27491;&#21521;&#27169;&#24335;&#33258;&#21160;&#24494;&#20998;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#65292;&#20351;&#24471;&#21487;&#20197;&#22312;&#21333;&#20010;&#26222;&#36890; GPU &#19978;&#20351;&#29992;&#22823;&#37327;&#30340;&#37197;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;(PINNs)&#26368;&#36817;&#24050;&#32463;&#25104;&#20026;&#26377;&#24076;&#26395;&#30340;&#22522;&#20110;&#25968;&#25454;&#30340;PDE&#27714;&#35299;&#22120;&#65292;&#22312;&#21508;&#31181;PDE&#19978;&#26174;&#31034;&#20986;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;PINNs&#26469;&#35299;&#20915;&#22810;&#32500;PDE&#21644;&#36924;&#36817;&#39640;&#24230;&#22797;&#26434;&#35299;&#20989;&#25968;&#23384;&#22312;&#26681;&#26412;&#38480;&#21046;&#12290;&#22312;&#36825;&#20123;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;PDE&#19978;&#25152;&#38656;&#30340;&#35757;&#32451;&#28857;&#25968;&#37327;(&#37197;&#28857;)&#22823;&#22823;&#22686;&#21152;&#65292;&#20294;&#30001;&#20110;&#26114;&#36149;&#30340;&#35745;&#31639;&#25104;&#26412;&#21644;&#24222;&#22823;&#30340;&#20869;&#23384;&#24320;&#38144;&#65292;&#20854;&#21463;&#21040;&#20005;&#37325;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;PINNs&#30340;&#32593;&#32476;&#26550;&#26500;&#21644;&#35757;&#32451;&#31639;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#21487;&#20998;&#31163;&#30340;PINN (SPINN)&#65292;&#22312;&#22810;&#32500;PDE&#20013;&#25353;&#36724;&#36880;&#20010;&#22788;&#29702;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#20102;&#32593;&#32476;&#20256;&#25773;&#30340;&#25968;&#37327;&#65292;&#19981;&#21516;&#20110;&#20256;&#32479;PINNs&#20013;&#30340;&#36880;&#28857;&#22788;&#29702;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20351;&#29992;&#27491;&#21521;&#27169;&#24335;&#33258;&#21160;&#24494;&#20998;&#26469;&#38477;&#20302;&#35745;&#31639;PDE&#27531;&#24046;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#20174;&#32780;&#22312;&#21333;&#20010;&#26222;&#36890;GPU&#19978;&#21487;&#20197;&#20351;&#29992;&#22823;&#37327;&#30340;&#37197;&#28857;(&gt;10^7)&#12290;
&lt;/p&gt;
&lt;p&gt;
Physics-informed neural networks (PINNs) have recently emerged as promising data-driven PDE solvers showing encouraging results on various PDEs. However, there is a fundamental limitation of training PINNs to solve multi-dimensional PDEs and approximate highly complex solution functions. The number of training points (collocation points) required on these challenging PDEs grows substantially, but it is severely limited due to the expensive computational costs and heavy memory overhead. To overcome this issue, we propose a network architecture and training algorithm for PINNs. The proposed method, separable PINN (SPINN), operates on a per-axis basis to significantly reduce the number of network propagations in multi-dimensional PDEs unlike point-wise processing in conventional PINNs. We also propose using forward-mode automatic differentiation to reduce the computational cost of computing PDE residuals, enabling a large number of collocation points (&gt;10^7) on a single commodity GPU. The
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26080;&#30417;&#30563;&#30340;&#21095;&#38598;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#20803;&#23398;&#20064;&#35299;&#20915;&#27809;&#26377;&#26631;&#31614;&#30340;&#23569;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;&#38382;&#39064;&#12290;&#23427;&#20204;&#20805;&#20998;&#21033;&#29992;&#25152;&#26377;&#33410;&#28857;&#20449;&#24687;&#65292;&#24182;&#19988;&#36890;&#36807;&#27867;&#21270;&#33021;&#21147;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.15217</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#30340;&#21095;&#38598;&#29983;&#25104;&#26041;&#27861;&#29992;&#20110;&#22270;&#20803;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Episode Generation for Graph Meta-learning. (arXiv:2306.15217v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15217
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26080;&#30417;&#30563;&#30340;&#21095;&#38598;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#20803;&#23398;&#20064;&#35299;&#20915;&#27809;&#26377;&#26631;&#31614;&#30340;&#23569;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;&#38382;&#39064;&#12290;&#23427;&#20204;&#20805;&#20998;&#21033;&#29992;&#25152;&#26377;&#33410;&#28857;&#20449;&#24687;&#65292;&#24182;&#19988;&#36890;&#36807;&#27867;&#21270;&#33021;&#21147;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26080;&#30417;&#30563;&#30340;&#21095;&#38598;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#20803;&#23398;&#20064;&#26469;&#35299;&#20915;&#27809;&#26377;&#26631;&#31614;&#30340;&#23569;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;&#38382;&#39064;&#12290;&#20027;&#27969;&#30340;&#23569;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#26159;&#22312;&#23384;&#22312;&#22823;&#37327;&#26377;&#26631;&#31614;&#33410;&#28857;&#29992;&#20110;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#24320;&#21457;&#30340;&#65292;&#28982;&#32780;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#21487;&#33021;&#26080;&#27861;&#33719;&#24471;&#36825;&#26679;&#30340;&#25968;&#25454;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#35299;&#20915;&#26631;&#31614;&#31232;&#32570;&#24615;&#38382;&#39064;&#30340;&#30740;&#31350;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#20381;&#36182;&#20110;&#26377;&#38480;&#25968;&#37327;&#30340;&#26377;&#26631;&#31614;&#25968;&#25454;&#65292;&#36825;&#38480;&#21046;&#20102;&#23545;&#22270;&#20013;&#25152;&#26377;&#33410;&#28857;&#20449;&#24687;&#30340;&#20805;&#20998;&#21033;&#29992;&#12290;&#23613;&#31649;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#22312;&#27809;&#26377;&#26631;&#31614;&#30340;&#33410;&#28857;&#20998;&#31867;&#38382;&#39064;&#19978;&#24456;&#26377;&#25928;&#65292;&#20294;&#23427;&#20204;&#20027;&#35201;&#23398;&#20064;&#36890;&#29992;&#30340;&#33410;&#28857;&#23884;&#20837;&#65292;&#27809;&#26377;&#32771;&#34385;&#35201;&#35299;&#20915;&#30340;&#19979;&#28216;&#20219;&#21153;&#65292;&#36825;&#21487;&#33021;&#38480;&#21046;&#20102;&#20854;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26080;&#30417;&#30563;&#30340;&#21095;&#38598;&#29983;&#25104;&#26041;&#27861;&#65292;&#20197;&#21033;&#29992;&#23427;&#20204;&#22312;&#23569;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21516;&#26102;&#35299;&#20915;&#26631;&#31614;&#31232;&#32570;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22270;&#22686;&#24378;&#26041;&#27861;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate Unsupervised Episode Generation methods to solve Few-Shot Node-Classification (FSNC) problem via Meta-learning without labels. Dominant meta-learning methodologies for FSNC were developed under the existence of abundant labeled nodes for training, which however may not be possible to obtain in the real-world. Although few studies have been proposed to tackle the label-scarcity problem, they still rely on a limited amount of labeled data, which hinders the full utilization of the information of all nodes in a graph. Despite the effectiveness of Self-Supervised Learning (SSL) approaches on FSNC without labels, they mainly learn generic node embeddings without consideration on the downstream task to be solved, which may limit its performance. In this work, we propose unsupervised episode generation methods to benefit from their generalization ability for FSNC tasks while resolving label-scarcity problem. We first propose a method that utilizes graph augmentat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27979;&#35797;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#23545;&#20110;&#35757;&#32451;&#20808;&#39564;&#30340;&#20381;&#36182;&#31243;&#24230;&#65292;&#21457;&#29616;&#27169;&#22411;&#33021;&#22815;&#26356;&#22909;&#22320;&#29983;&#25104;&#19982;&#35757;&#32451;&#25968;&#25454;&#20013;&#20986;&#29616;&#39057;&#29575;&#26356;&#39640;&#30340;&#19977;&#20803;&#32452;&#23545;&#40784;&#30340;&#22270;&#20687;&#65292;&#20294;&#36825;&#20063;&#20250;&#38477;&#20302;&#20854;&#29983;&#25104;&#20197;&#32763;&#36716;&#19977;&#20803;&#32452;&#20026;&#22522;&#30784;&#30340;&#22270;&#20687;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.01755</link><description>&lt;p&gt;
&#35757;&#32451;&#20808;&#39564;&#24433;&#21709;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Training Priors Predict Text-To-Image Model Performance. (arXiv:2306.01755v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01755
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27979;&#35797;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#23545;&#20110;&#35757;&#32451;&#20808;&#39564;&#30340;&#20381;&#36182;&#31243;&#24230;&#65292;&#21457;&#29616;&#27169;&#22411;&#33021;&#22815;&#26356;&#22909;&#22320;&#29983;&#25104;&#19982;&#35757;&#32451;&#25968;&#25454;&#20013;&#20986;&#29616;&#39057;&#29575;&#26356;&#39640;&#30340;&#19977;&#20803;&#32452;&#23545;&#40784;&#30340;&#22270;&#20687;&#65292;&#20294;&#36825;&#20063;&#20250;&#38477;&#20302;&#20854;&#29983;&#25104;&#20197;&#32763;&#36716;&#19977;&#20803;&#32452;&#20026;&#22522;&#30784;&#30340;&#22270;&#20687;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#19968;&#20123;&#20851;&#31995;&#65292;&#27604;&#22914;&#8220;&#23431;&#33322;&#21592;&#39569;&#39532;&#8221;&#65292;&#20294;&#21364;&#19981;&#33021;&#29983;&#25104;&#30001;&#30456;&#21516;&#22522;&#26412;&#37096;&#20998;&#32452;&#25104;&#30340;&#20854;&#20182;&#20851;&#31995;&#65292;&#27604;&#22914;&#8220;&#39532;&#39569;&#23431;&#33322;&#21592;&#8221;&#12290;&#36825;&#20123;&#22833;&#36133;&#36890;&#24120;&#34987;&#35270;&#20026;&#27169;&#22411;&#20381;&#36182;&#35757;&#32451;&#20808;&#39564;&#32780;&#19981;&#26159;&#26500;&#24314;&#26032;&#39062;&#30340;&#22270;&#20687;&#32452;&#21512;&#30340;&#35777;&#25454;&#12290;&#26412;&#25991;&#30452;&#25509;&#22312;&#31283;&#23450;&#25193;&#25955;2.1&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#36890;&#36807;&#35266;&#23519;&#32452;&#25104;&#36825;&#20123;&#25552;&#31034;&#30340;&#20027;&#35821;-&#35859;&#35821;-&#23486;&#35821; (SVO) &#19977;&#20803;&#32452;&#65288;&#20363;&#22914;&#65292;&#8220;&#23431;&#33322;&#21592;&#8221;&#65292;&#8220;&#39569;&#8221;&#65292;&#8220;&#39532;&#8221;&#65289;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;SVO&#19977;&#20803;&#32452;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#20986;&#29616;&#30340;&#27425;&#25968;&#36234;&#22810;&#65292;&#35813;&#27169;&#22411;&#23601;&#33021;&#29983;&#25104;&#19982;&#35813;&#19977;&#20803;&#32452;&#23545;&#40784;&#30340;&#22270;&#20687;&#23601;&#36234;&#22909;&#12290;&#22312;&#36825;&#37324;&#65292;&#36890;&#36807;&#23545;&#40784;&#65292;&#25105;&#20204;&#30340;&#24847;&#24605;&#26159;&#27599;&#20010;&#26415;&#35821;&#22312;&#29983;&#25104;&#30340;&#22270;&#20687;&#20013;&#20197;&#27491;&#30830;&#30340;&#20851;&#31995;&#20986;&#29616;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#22686;&#21152;&#30340;&#39057;&#29575;&#20063;&#20250;&#20943;&#23569;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#19982;&#32763;&#36716;&#19977;&#20803;&#32452;&#23545;&#40784;&#30340;&#22270;&#20687;&#30340;&#33021;&#21147;&#12290;&#20363;&#22914;&#65292;&#22914;&#26524;&#8220;&#23431;&#33322;&#21592;&#39569;&#39532;&#8221;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#39057;&#32321;&#20986;&#29616;&#65292;&#37027;&#20040;&#8220;&#39532;&#39569;&#23431;&#33322;&#21592;&#8221;&#30340;&#23545;&#40784;&#36136;&#37327;&#23601;&#20250;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image models can often generate some relations, i.e., "astronaut riding horse", but fail to generate other relations composed of the same basic parts, i.e., "horse riding astronaut". These failures are often taken as evidence that the models rely on training priors rather than constructing novel images compositionally. This paper tests this intuition directly on the stablediffusion 2.1 text-to-image model. By looking at the subject-verb-object (SVO) triads that form the backbone of these prompts (e.g., "astronaut", "ride", "horse"), we find that the more often an SVO triad appears in the training data, the better the model can generate an image aligned with that triad. Here, by aligned we mean that each of the terms appears in the generated image in the proper relation to each other. However, this increased frequency also diminishes how well the model can generate an image aligned with the flipped triad. For example, if "astronaut riding horse" appears frequently in the trainin
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;NUDGE&#30340;&#31574;&#30053;&#65292;&#21033;&#29992;&#35757;&#32451;&#22909;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20195;&#29702;&#26469;&#24341;&#23548;&#36923;&#36753;&#35268;&#21017;&#30340;&#25628;&#32034;&#65292;&#23454;&#29616;&#20102;&#21487;&#35299;&#37322;&#21644;&#21487;&#35299;&#37322;&#30340;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2306.01439</link><description>&lt;p&gt;
&#36890;&#36807;&#31070;&#32463;&#24341;&#23548;&#31526;&#21495;&#25277;&#35937;&#23454;&#29616;&#21487;&#35299;&#37322;&#21644;&#21487;&#35299;&#37322;&#36923;&#36753;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Interpretable and Explainable Logical Policies via Neurally Guided Symbolic Abstraction. (arXiv:2306.01439v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01439
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;NUDGE&#30340;&#31574;&#30053;&#65292;&#21033;&#29992;&#35757;&#32451;&#22909;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20195;&#29702;&#26469;&#24341;&#23548;&#36923;&#36753;&#35268;&#21017;&#30340;&#25628;&#32034;&#65292;&#23454;&#29616;&#20102;&#21487;&#35299;&#37322;&#21644;&#21487;&#35299;&#37322;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#25152;&#38656;&#35201;&#30340;&#26377;&#38480;&#20808;&#39564;&#20351;&#20854;&#25104;&#20026;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#32534;&#30721;&#21644;&#23398;&#20064;&#31574;&#30053;&#30340;&#20027;&#35201;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20063;&#26159;&#40657;&#21283;&#23376;&#65292;&#22312;&#24037;&#20316;&#22312;&#22270;&#20687;&#32423;&#21035;&#26102;&#38590;&#20197;&#29702;&#35299;&#20195;&#29702;&#34892;&#20026;&#12290;&#22240;&#27492;&#65292;&#31070;&#32463;&#31526;&#21495;RL&#26088;&#22312;&#39318;&#20808;&#21019;&#24314;&#21487;&#35299;&#37322;&#30340;&#31574;&#30053;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#21487;&#35299;&#37322;&#24615;&#19981;&#24847;&#21619;&#30528;&#21487;&#35299;&#37322;&#24615;&#12290;&#20026;&#20102;&#23454;&#29616;&#35299;&#37322;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31070;&#32463;&#24341;&#23548;&#21487;&#24494;&#20998;&#36923;&#36753;&#31574;&#30053;&#65288;NUDGE&#65289;&#12290;NUDGE&#21033;&#29992;&#35757;&#32451;&#22909;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20195;&#29702;&#26469;&#24341;&#23548;&#20505;&#36873;&#21152;&#26435;&#36923;&#36753;&#35268;&#21017;&#30340;&#25628;&#32034;&#65292;&#28982;&#21518;&#20351;&#29992;&#21487;&#24494;&#20998;&#30340;&#36923;&#36753;&#26469;&#35757;&#32451;&#36923;&#36753;&#20195;&#29702;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;NUDGE&#20195;&#29702;&#21487;&#20197;&#20135;&#29983;&#21487;&#35299;&#37322;&#21644;&#21487;&#35299;&#37322;&#30340;&#31574;&#30053;&#65292;&#21516;&#26102;&#32988;&#36807;&#32431;&#31070;&#32463;&#20195;&#29702;&#65292;&#24182;&#23637;&#29616;&#20986;&#33391;&#22909;&#30340;&#28789;&#27963;&#24615;&#65292;&#20197;&#36866;&#24212;&#19981;&#21516;&#21021;&#22987;&#29366;&#24577;&#21644;&#38382;&#39064;&#22823;&#23567;&#30340;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
The limited priors required by neural networks make them the dominating choice to encode and learn policies using reinforcement learning (RL). However, they are also black-boxes, making it hard to understand the agent's behaviour, especially when working on the image level. Therefore, neuro-symbolic RL aims at creating policies that are interpretable in the first place. Unfortunately, interpretability is not explainability. To achieve both, we introduce Neurally gUided Differentiable loGic policiEs (NUDGE). NUDGE exploits trained neural network-based agents to guide the search of candidate-weighted logic rules, then uses differentiable logic to train the logic agents. Our experimental evaluation demonstrates that NUDGE agents can induce interpretable and explainable policies while outperforming purely neural ones and showing good flexibility to environments of different initial states and problem sizes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#36870;&#38382;&#39064;&#30340;&#30452;&#25509;&#25193;&#25955;&#38142;&#26725;&#31639;&#27861;&#65292;&#25552;&#39640;&#20102;&#36870;&#38382;&#39064;&#27714;&#35299;&#22120;&#30340;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#25968;&#25454;&#19968;&#33268;&#24615;&#35299;&#20915;&#20102;&#24403;&#21069;DDB&#26694;&#26550;&#23384;&#22312;&#30340;&#20851;&#38190;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2305.19809</link><description>&lt;p&gt;
&#20351;&#29992;&#25968;&#25454;&#19968;&#33268;&#24615;&#30340;&#30452;&#25509;&#25193;&#25955;&#38142;&#26725;&#35299;&#20915;&#36870;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Direct Diffusion Bridge using Data Consistency for Inverse Problems. (arXiv:2305.19809v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19809
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#36870;&#38382;&#39064;&#30340;&#30452;&#25509;&#25193;&#25955;&#38142;&#26725;&#31639;&#27861;&#65292;&#25552;&#39640;&#20102;&#36870;&#38382;&#39064;&#27714;&#35299;&#22120;&#30340;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#25968;&#25454;&#19968;&#33268;&#24615;&#35299;&#20915;&#20102;&#24403;&#21069;DDB&#26694;&#26550;&#23384;&#22312;&#30340;&#20851;&#38190;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#36870;&#38382;&#39064;&#27714;&#35299;&#22120;&#34920;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#65292;&#20294;&#36895;&#24230;&#21463;&#38480;&#65292;&#20027;&#35201;&#26159;&#22240;&#20026;&#38656;&#35201;&#20174;&#22122;&#22768;&#24320;&#22987;&#36827;&#34892;&#21453;&#21521;&#25193;&#25955;&#37319;&#26679;&#12290;&#36817;&#26399;&#30340;&#19968;&#20123;&#24037;&#20316;&#23581;&#35797;&#36890;&#36807;&#26500;&#24314;&#25193;&#25955;&#36807;&#31243;&#26469;&#30452;&#25509;&#26725;&#25509;&#29305;&#23450;&#36870;&#38382;&#39064;&#30340;&#28165;&#27905;&#21644;&#27745;&#26579;&#25968;&#25454;&#20197;&#20943;&#36731;&#36825;&#20010;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#36825;&#20123;&#29616;&#26377;&#24037;&#20316;&#32479;&#19968;&#21629;&#21517;&#20026;&#30452;&#25509;&#25193;&#25955;&#38142;&#26725;&#65288;DDB&#65289;&#65292;&#35777;&#26126;&#23613;&#31649;&#21463;&#19981;&#21516;&#29702;&#35770;&#30340;&#21551;&#21457;&#65292;&#20294;&#30001;&#27492;&#20135;&#29983;&#30340;&#31639;&#27861;&#22312;&#21442;&#25968;&#36873;&#25321;&#19978;&#30340;&#19981;&#21516;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24378;&#35843;&#24403;&#21069;DDB&#26694;&#26550;&#30340;&#19968;&#20010;&#20851;&#38190;&#38480;&#21046;&#65292;&#21363;&#23427;&#19981;&#33021;&#20445;&#35777;&#25968;&#25454;&#19968;&#33268;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#25913;&#30340;&#25512;&#26029;&#31243;&#24207;&#65292;&#23427;&#22312;&#19981;&#38656;&#35201;&#31934;&#32454;&#35843;&#25972;&#30340;&#24773;&#20917;&#19979;&#24378;&#21046;&#25968;&#25454;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#23558;&#24471;&#21040;&#30340;&#26041;&#27861;&#31216;&#20026;&#25968;&#25454;&#19968;&#33268;&#30340;DDB&#65288;CDDB&#65289;&#65292;&#23427;&#22312;&#24863;&#30693;&#21644;&#22833;&#30495;&#25351;&#26631;&#26041;&#38754;&#37117;&#20248;&#20110;&#19981;&#19968;&#33268;&#30340;&#23545;&#24212;&#29289;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#25512;&#21160;&#20102;&#36870;&#38382;&#39064;&#27714;&#35299;&#22120;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion model-based inverse problem solvers have shown impressive performance, but are limited in speed, mostly as they require reverse diffusion sampling starting from noise. Several recent works have tried to alleviate this problem by building a diffusion process, directly bridging the clean and the corrupted for specific inverse problems. In this paper, we first unify these existing works under the name Direct Diffusion Bridges (DDB), showing that while motivated by different theories, the resulting algorithms only differ in the choice of parameters. Then, we highlight a critical limitation of the current DDB framework, namely that it does not ensure data consistency. To address this problem, we propose a modified inference procedure that imposes data consistency without the need for fine-tuning. We term the resulting method data Consistent DDB (CDDB), which outperforms its inconsistent counterpart in terms of both perception and distortion metrics, thereby effectively pushing the
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#21160;&#24577;&#35268;&#21010;&#26041;&#27861;&#26469;&#20248;&#21270;&#20219;&#20309;&#32452;&#21512;&#30340;&#21487;&#20998;&#31163;&#30446;&#26631;&#21644;&#32422;&#26463;&#26465;&#20214;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#27604;&#36890;&#29992;&#27714;&#35299;&#22120;&#34920;&#29616;&#24471;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.19706</link><description>&lt;p&gt;
&#21487;&#20998;&#30446;&#26631;&#30340;&#26368;&#20248;&#20915;&#31574;&#26641;&#65306;&#25512;&#21160;&#21160;&#24577;&#35268;&#21010;&#30340;&#26497;&#38480;
&lt;/p&gt;
&lt;p&gt;
Optimal Decision Trees for Separable Objectives: Pushing the Limits of Dynamic Programming. (arXiv:2305.19706v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19706
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#21160;&#24577;&#35268;&#21010;&#26041;&#27861;&#26469;&#20248;&#21270;&#20219;&#20309;&#32452;&#21512;&#30340;&#21487;&#20998;&#31163;&#30446;&#26631;&#21644;&#32422;&#26463;&#26465;&#20214;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#27604;&#36890;&#29992;&#27714;&#35299;&#22120;&#34920;&#29616;&#24471;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#26641;&#30340;&#20840;&#23616;&#20248;&#21270;&#22312;&#20934;&#30830;&#24615;&#65292;&#22823;&#23567;&#21644;&#20154;&#31867;&#21487;&#29702;&#35299;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#21069;&#26223;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#26041;&#27861;&#20173;&#28982;&#20381;&#36182;&#20110;&#36890;&#29992;&#27714;&#35299;&#22120;&#65292;&#21487;&#25193;&#23637;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#38382;&#39064;&#12290;&#21160;&#24577;&#35268;&#21010;&#26041;&#27861;&#24050;&#34987;&#35777;&#26126;&#20855;&#26377;&#26356;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#36807;&#23558;&#23376;&#26641;&#20316;&#20026;&#29420;&#31435;&#30340;&#23376;&#38382;&#39064;&#35299;&#20915;&#26469;&#21033;&#29992;&#26641;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#36825;&#20165;&#36866;&#29992;&#20110;&#21487;&#20197;&#20998;&#21035;&#20248;&#21270;&#23376;&#26641;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#35814;&#32454;&#30740;&#31350;&#20102;&#36825;&#31181;&#20851;&#31995;&#65292;&#24182;&#23637;&#31034;&#20102;&#23454;&#29616;&#36825;&#31181;&#21487;&#20998;&#31163;&#32422;&#26463;&#21644;&#30446;&#26631;&#20219;&#24847;&#32452;&#21512;&#30340;&#21160;&#24577;&#35268;&#21010;&#26041;&#27861;&#12290;&#22312;&#22235;&#20010;&#24212;&#29992;&#39046;&#22495;&#30340;&#23454;&#39564;&#34920;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26222;&#36866;&#24615;&#65292;&#21516;&#26102;&#20063;&#27604;&#36890;&#29992;&#27714;&#35299;&#22120;&#20855;&#26377;&#26356;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Global optimization of decision trees has shown to be promising in terms of accuracy, size, and consequently human comprehensibility. However, many of the methods used rely on general-purpose solvers for which scalability remains an issue. Dynamic programming methods have been shown to scale much better because they exploit the tree structure by solving subtrees as independent subproblems. However, this only works when an objective can be optimized separately for subtrees. We explore this relationship in detail and show necessary and sufficient conditions for such separability and generalize previous dynamic programming approaches into a framework that can optimize any combination of separable objectives and constraints. Experiments on four application domains show the general applicability of this framework, while outperforming the scalability of general-purpose solvers by a large margin.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;Maximize to Explore (MEX)&#65292;&#21482;&#38656;&#20248;&#21270;&#19968;&#20010;&#26080;&#32422;&#26463;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#33258;&#21160;&#24179;&#34913;&#25506;&#32034;&#21644;&#21033;&#29992;&#65292;&#23454;&#29616;&#27425;&#32447;&#24615;&#36951;&#25022;&#12290;</title><link>http://arxiv.org/abs/2305.18258</link><description>&lt;p&gt;
&#19968;&#31181;&#34701;&#21512;&#20272;&#35745;&#21644;&#35268;&#21010;&#23454;&#29616;&#25506;&#32034;&#30340;&#26368;&#22823;&#21270;&#30446;&#26631;&#20989;&#25968;&#30340;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
One Objective to Rule Them All: A Maximization Objective Fusing Estimation and Planning for Exploration. (arXiv:2305.18258v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18258
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;Maximize to Explore (MEX)&#65292;&#21482;&#38656;&#20248;&#21270;&#19968;&#20010;&#26080;&#32422;&#26463;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#33258;&#21160;&#24179;&#34913;&#25506;&#32034;&#21644;&#21033;&#29992;&#65292;&#23454;&#29616;&#27425;&#32447;&#24615;&#36951;&#25022;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#24179;&#34913;&#25506;&#32034;&#21644;&#21033;&#29992;&#23545;&#20110;&#20197;&#26377;&#25928;&#30340;&#26041;&#24335;&#25214;&#21040;&#26368;&#20248;&#31574;&#30053;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#29616;&#26377;&#30340;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36890;&#24120;&#21253;&#25324;&#19977;&#20010;&#32452;&#25104;&#37096;&#20998;&#65306;&#20272;&#35745;&#12289;&#35268;&#21010;&#21644;&#25506;&#32034;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#24212;&#23545;&#36890;&#29992;&#20989;&#25968;&#36924;&#36817;&#22120;&#65292;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#37117;&#38656;&#35201;&#20351;&#29992;&#19981;&#20999;&#23454;&#38469;&#30340;&#31639;&#27861;&#32452;&#20214;&#26469;&#28608;&#21169;&#25506;&#32034;&#65292;&#20363;&#22914;&#25968;&#25454;&#30456;&#20851;&#30340;&#32423;&#21035;&#38598;&#20869;&#20248;&#21270;&#25110;&#32321;&#29712;&#30340;&#37319;&#26679;&#36807;&#31243;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26131;&#20110;&#23454;&#29616;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#31216;&#20026;Maximize to Explore (MEX) &#65292;&#23427;&#21482;&#38656;&#35201;&#26080;&#32422;&#26463;&#22320;&#20248;&#21270;&#19968;&#20010;&#38598;&#25104;&#20102;&#20272;&#35745;&#21644;&#35268;&#21010;&#32452;&#20214;&#30340;&#21333;&#19968;&#30446;&#26631;&#20989;&#25968;&#65292;&#21516;&#26102;&#33258;&#21160;&#24179;&#34913;&#25506;&#32034;&#21644;&#21033;&#29992;&#12290;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#36890;&#29992;&#20989;&#25968;&#36924;&#36817;&#65292;MEX&#23454;&#29616;&#20102;&#19968;&#20010;&#27425;&#32447;&#24615;&#30340;&#36951;&#25022;&#65292;&#36827;&#19968;&#27493;&#65306;
&lt;/p&gt;
&lt;p&gt;
In online reinforcement learning (online RL), balancing exploration and exploitation is crucial for finding an optimal policy in a sample-efficient way. To achieve this, existing sample-efficient online RL algorithms typically consist of three components: estimation, planning, and exploration. However, in order to cope with general function approximators, most of them involve impractical algorithmic components to incentivize exploration, such as optimization within data-dependent level-sets or complicated sampling procedures. To address this challenge, we propose an easy-to-implement RL framework called \textit{Maximize to Explore} (\texttt{MEX}), which only needs to optimize \emph{unconstrainedly} a single objective that integrates the estimation and planning components while balancing exploration and exploitation automatically. Theoretically, we prove that \texttt{MEX} achieves a sublinear regret with general function approximations for Markov decision processes (MDP) and is further 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31070;&#32463;&#31526;&#21495;&#27010;&#24565;&#23398;&#20064;&#30340;&#22270;&#20687;&#25805;&#20316;&#31995;&#32479;NeuroSIM&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#22810;&#36339;&#25351;&#20196;&#22312;&#22810;&#29289;&#20307;&#22330;&#26223;&#20013;&#25191;&#34892;&#22797;&#26434;&#30340;&#25512;&#29702;&#65292;&#21482;&#38656;&#35201;&#24369;&#30417;&#30563;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#12290;&#35813;&#31995;&#32479;&#20855;&#26377;&#24456;&#39640;&#30340;&#31454;&#20105;&#21147;&#25110;&#36229;&#36807;SOTA&#22522;&#32447;&#12290;</title><link>http://arxiv.org/abs/2305.14410</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#36339;&#25351;&#20196;&#36827;&#34892;&#22270;&#20687;&#25805;&#20316;&#8212;&#8212;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#20110;&#24369;&#30417;&#30563;&#30340;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Image Manipulation via Multi-Hop Instructions -- A New Dataset and Weakly-Supervised Neuro-Symbolic Approach. (arXiv:2305.14410v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14410
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31070;&#32463;&#31526;&#21495;&#27010;&#24565;&#23398;&#20064;&#30340;&#22270;&#20687;&#25805;&#20316;&#31995;&#32479;NeuroSIM&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#22810;&#36339;&#25351;&#20196;&#22312;&#22810;&#29289;&#20307;&#22330;&#26223;&#20013;&#25191;&#34892;&#22797;&#26434;&#30340;&#25512;&#29702;&#65292;&#21482;&#38656;&#35201;&#24369;&#30417;&#30563;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#12290;&#35813;&#31995;&#32479;&#20855;&#26377;&#24456;&#39640;&#30340;&#31454;&#20105;&#21147;&#25110;&#36229;&#36807;SOTA&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#36827;&#34892;&#22270;&#20687;&#25805;&#20316;&#24863;&#20852;&#36259;&#65292;&#36825;&#26159;&#22810;&#20010;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#31243;&#24207;&#20013;&#26377;&#29992;&#30340;&#20219;&#21153;&#65292;&#20294;&#38656;&#35201;&#23545;&#22810;&#27169;&#24577;&#31354;&#38388;&#36827;&#34892;&#22797;&#26434;&#30340;&#25512;&#29702;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;&#26368;&#36817;&#25552;&#20986;&#30340;&#31070;&#32463;&#31526;&#21495;&#27010;&#24565;&#23398;&#20064;(NSCL)&#65292;&#35813;&#26041;&#27861;&#22312;&#35270;&#35273;&#38382;&#31572;(VQA)&#20219;&#21153;&#19978;&#38750;&#24120;&#26377;&#25928;&#65292;&#25193;&#23637;&#20854;&#29992;&#20110;&#22270;&#20687;&#25805;&#20316;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#31216;&#20026;NeuroSIM&#65292;&#21487;&#20197;&#22312;&#22810;&#29289;&#20307;&#22330;&#26223;&#19978;&#25191;&#34892;&#22797;&#26434;&#30340;&#22810;&#36339;&#25512;&#29702;&#65292;&#21482;&#38656;&#35201;&#20197;VQA&#30340;&#27880;&#37322;&#25968;&#25454;&#24418;&#24335;&#25552;&#20379;&#24369;&#30417;&#30563;&#12290;NeuroSIM&#23558;&#25351;&#20196;&#35299;&#26512;&#25104;&#31526;&#21495;&#31243;&#24207;&#65292;&#22522;&#20110;&#30001;&#23545;&#35937;&#23646;&#24615;&#21644;&#25805;&#20316;&#32452;&#25104;&#30340;&#19987;&#19994;&#39046;&#22495;&#35821;&#35328;(DSL)&#65292;&#25351;&#23548;&#20854;&#25191;&#34892;&#12290;&#25105;&#20204;&#20026;&#36825;&#20010;&#20219;&#21153;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;NeuroSIM&#19982;&#20351;&#29992;&#30417;&#30563;&#25968;&#25454;&#36827;&#34892;&#25805;&#20316;&#30340;SOTA&#22522;&#32447;&#30456;&#27604;&#20855;&#26377;&#24456;&#39640;&#30340;&#31454;&#20105;&#21147;&#25110;&#36229;&#36807;SOTA&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
We are interested in image manipulation via natural language text -- a task that is useful for multiple AI applications but requires complex reasoning over multi-modal spaces. We extend recently proposed Neuro Symbolic Concept Learning (NSCL), which has been quite effective for the task of Visual Question Answering (VQA), for the task of image manipulation. Our system referred to as NeuroSIM can perform complex multi-hop reasoning over multi-object scenes and only requires weak supervision in the form of annotated data for VQA. NeuroSIM parses an instruction into a symbolic program, based on a Domain Specific Language (DSL) comprising of object attributes and manipulation operations, that guides its execution. We create a new dataset for the task, and extensive experiments demonstrate that NeuroSIM is highly competitive with or beats SOTA baselines that make use of supervised data for manipulation.
&lt;/p&gt;</description></item><item><title>&#21521;&#37327;&#33258;&#22238;&#24402;&#28436;&#21270;(VARE)&#36890;&#36807;&#20351;&#29992;&#21521;&#37327;&#33258;&#22238;&#24402;(VAR)&#27169;&#22411;&#21644;&#29615;&#22659;&#24863;&#30693;&#36229;&#31361;&#21464;(EAH)&#31574;&#30053;&#65292;&#26377;&#25928;&#22788;&#29702;&#21160;&#24577;&#22810;&#30446;&#26631;&#20248;&#21270;(DMO)&#20013;&#30340;&#29615;&#22659;&#21464;&#21270;&#65292;&#24182;&#25552;&#39640;&#31181;&#32676;&#30340;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.12752</link><description>&lt;p&gt;
&#21160;&#24577;&#22810;&#30446;&#26631;&#20248;&#21270;&#30340;&#21521;&#37327;&#33258;&#22238;&#24402;&#28436;&#21270;
&lt;/p&gt;
&lt;p&gt;
Vector Autoregressive Evolution for Dynamic Multi-Objective Optimisation. (arXiv:2305.12752v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12752
&lt;/p&gt;
&lt;p&gt;
&#21521;&#37327;&#33258;&#22238;&#24402;&#28436;&#21270;(VARE)&#36890;&#36807;&#20351;&#29992;&#21521;&#37327;&#33258;&#22238;&#24402;(VAR)&#27169;&#22411;&#21644;&#29615;&#22659;&#24863;&#30693;&#36229;&#31361;&#21464;(EAH)&#31574;&#30053;&#65292;&#26377;&#25928;&#22788;&#29702;&#21160;&#24577;&#22810;&#30446;&#26631;&#20248;&#21270;(DMO)&#20013;&#30340;&#29615;&#22659;&#21464;&#21270;&#65292;&#24182;&#25552;&#39640;&#31181;&#32676;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#22810;&#30446;&#26631;&#20248;&#21270;(DMO)&#22788;&#29702;&#22312;&#19981;&#26029;&#21464;&#21270;&#30340;&#29615;&#22659;&#20013;&#20855;&#26377;&#22810;&#20010;&#65288;&#36890;&#24120;&#20914;&#31361;&#30340;&#65289;&#30446;&#26631;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#36825;&#20123;&#38382;&#39064;&#23545;&#36827;&#21270;&#31639;&#27861;&#25552;&#20986;&#20102;&#21508;&#31181;&#25361;&#25112;&#65292;&#36827;&#21270;&#31639;&#27861;&#36890;&#24120;&#34987;&#29992;&#26469;&#35299;&#20915;&#22797;&#26434;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#30001;&#20110;&#20854;&#21160;&#24577;&#24615;&#21644;&#22312;&#21464;&#21270;&#29615;&#22659;&#20013;&#30340;&#36164;&#28304;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#21521;&#37327;&#33258;&#22238;&#24402;&#28436;&#21270;(VARE)&#65292;&#23427;&#30001;&#21521;&#37327;&#33258;&#22238;&#24402;(VAR)&#21644;&#29615;&#22659;&#24863;&#30693;&#36229;&#31361;&#21464;&#26500;&#25104;&#65292;&#20197;&#24212;&#23545;DMO&#20013;&#30340;&#29615;&#22659;&#21464;&#21270;&#12290;VARE&#26500;&#24314;&#20102;&#19968;&#31181;&#32771;&#34385;&#20915;&#31574;&#21464;&#37327;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#30340;VAR&#27169;&#22411;&#65292;&#20197;&#26377;&#25928;&#39044;&#27979;&#21160;&#24577;&#29615;&#22659;&#20013;&#30340;&#21464;&#21270;&#35299;&#12290;&#27492;&#22806;&#65292;VARE&#24341;&#20837;&#20102;&#29615;&#22659;&#24863;&#30693;&#36229;&#31361;&#21464;(EAH)&#26469;&#35299;&#20915;&#29616;&#26377;&#36229;&#31361;&#21464;&#31574;&#30053;&#22312;&#22686;&#21152;&#21160;&#24577;&#22330;&#26223;&#19979;&#30340;&#31181;&#32676;&#22810;&#26679;&#24615;&#26102;&#30340;&#30450;&#30446;&#24615;&#65292;&#22240;&#20026;&#39044;&#27979;&#26041;&#27861;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#19981;&#36866;&#29992;&#12290;VAR&#21644;EAH&#30340;&#26080;&#32541;&#38598;&#25104;&#20197;&#36866;&#24212;&#29615;&#22659;&#30340;&#26041;&#24335;&#20351;VARE&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#24191;&#27867;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamic multi-objective optimisation (DMO) handles optimisation problems with multiple (often conflicting) objectives in varying environments. Such problems pose various challenges to evolutionary algorithms, which have popularly been used to solve complex optimisation problems, due to their dynamic nature and resource restrictions in changing environments. This paper proposes vector autoregressive evolution (VARE) consisting of vector autoregression (VAR) and environment-aware hypermutation to address environmental changes in DMO. VARE builds a VAR model that considers mutual relationship between decision variables to effectively predict the moving solutions in dynamic environments. Additionally, VARE introduces EAH to address the blindness of existing hypermutation strategies in increasing population diversity in dynamic scenarios where predictive approaches are unsuitable. A seamless integration of VAR and EAH in an environment-adaptive manner makes VARE effective to handle a wide r
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#20154;&#31867;&#21644;&#21160;&#29289;&#22914;&#20309;&#25512;&#26029;&#29289;&#29702;&#19990;&#30028;&#30340;&#22522;&#26412;&#21160;&#24577;&#36712;&#36857;&#20197;&#21450;&#22914;&#20309;&#39044;&#27979;&#26410;&#26469;&#21487;&#33021;&#20986;&#29616;&#30340;&#29366;&#24577;&#65292;&#24182;&#35780;&#20272;&#20102;&#20960;&#31867;&#24863;&#30693;-&#35748;&#30693;&#32593;&#32476;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#21457;&#29616;&#22312;&#25928;&#29575;&#12289;&#26222;&#36941;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#38388;&#23384;&#22312;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2305.11772</link><description>&lt;p&gt;
&#31070;&#32463;&#22522;&#30784;&#20013;&#30340;&#24515;&#29702;&#27169;&#25311;&#65306;&#39044;&#27979;&#21160;&#24577;&#22330;&#26223;&#20013;&#30340;&#28508;&#22312;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Neural Foundations of Mental Simulation: Future Prediction of Latent Representations on Dynamic Scenes. (arXiv:2305.11772v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11772
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#20154;&#31867;&#21644;&#21160;&#29289;&#22914;&#20309;&#25512;&#26029;&#29289;&#29702;&#19990;&#30028;&#30340;&#22522;&#26412;&#21160;&#24577;&#36712;&#36857;&#20197;&#21450;&#22914;&#20309;&#39044;&#27979;&#26410;&#26469;&#21487;&#33021;&#20986;&#29616;&#30340;&#29366;&#24577;&#65292;&#24182;&#35780;&#20272;&#20102;&#20960;&#31867;&#24863;&#30693;-&#35748;&#30693;&#32593;&#32476;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#21457;&#29616;&#22312;&#25928;&#29575;&#12289;&#26222;&#36941;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#38388;&#23384;&#22312;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#21644;&#21160;&#29289;&#23545;&#29289;&#29702;&#19990;&#30028;&#26377;&#30528;&#20016;&#23500;&#32780;&#28789;&#27963;&#30340;&#29702;&#35299;&#65292;&#33021;&#22815;&#25512;&#26029;&#20986;&#20107;&#20214;&#30340;&#22522;&#26412;&#21160;&#24577;&#36712;&#36857;&#65292;&#39044;&#27979;&#26410;&#26469;&#21487;&#33021;&#20986;&#29616;&#30340;&#29366;&#24577;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#20449;&#24687;&#35268;&#21010;&#21644;&#39044;&#27979;&#34892;&#20026;&#30340;&#21518;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35745;&#31639;&#32972;&#21518;&#30340;&#31070;&#32463;&#26426;&#21046;&#23578;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#37319;&#29992;&#30446;&#26631;&#39537;&#21160;&#30340;&#24314;&#27169;&#26041;&#27861;&#65292;&#32467;&#21512;&#23494;&#38598;&#30340;&#31070;&#32463;&#29983;&#29702;&#23398;&#25968;&#25454;&#21644;&#39640;&#36890;&#37327;&#30340;&#20154;&#31867;&#34892;&#20026;&#36755;&#20986;&#26469;&#25506;&#31350;&#36825;&#20010;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#26500;&#24314;&#21644;&#35780;&#20272;&#20102;&#20960;&#31867;&#24863;&#30693;-&#35748;&#30693;&#32593;&#32476;&#26469;&#39044;&#27979;&#20016;&#23500;&#12289;&#20855;&#26377;&#34892;&#20026;&#23398;&#24847;&#20041;&#30340;&#29615;&#22659;&#30340;&#26410;&#26469;&#29366;&#24577;&#65292;&#20174;&#20687;&#32032;&#25110;&#38754;&#21521;&#23545;&#35937;&#30446;&#26631;&#30340;&#33258;&#20027;&#30417;&#30563;&#31471;&#21040;&#31471;&#27169;&#22411;&#65292;&#21040;&#23558;&#32431;&#38745;&#24577;&#22522;&#20110;&#22270;&#20687;&#25110;&#21160;&#24577;&#35270;&#39057;&#30340;&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#36827;&#34892;&#26410;&#26469;&#39044;&#27979;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#31867;&#21035;&#22312;&#20854;&#39044;&#27979;&#31070;&#32463;&#21644;&#34892;&#20026;&#25968;&#25454;&#30340;&#33021;&#21147;&#19978;&#26377;&#24456;&#24378;&#30340;&#24046;&#24322;&#65292;&#26080;&#35770;&#22312;&#20854;&#22521;&#35757;&#39046;&#22495;&#20869;&#25110;&#22806;&#65292;&#36825;&#31181;&#24046;&#24322;&#21453;&#26144;&#20102;&#25928;&#29575;&#12289;&#26222;&#36941;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#20043;&#38388;&#30340;&#22522;&#26412;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans and animals have a rich and flexible understanding of the physical world, which enables them to infer the underlying dynamical trajectories of objects and events, plausible future states, and use that to plan and anticipate the consequences of actions. However, the neural mechanisms underlying these computations are unclear. We combine a goal-driven modeling approach with dense neurophysiological data and high-throughput human behavioral readouts to directly impinge on this question. Specifically, we construct and evaluate several classes of sensory-cognitive networks to predict the future state of rich, ethologically-relevant environments, ranging from self-supervised end-to-end models with pixel-wise or object-centric objectives, to models that future predict in the latent space of purely static image-based or dynamic video-based pretrained foundation models. We find strong differentiation across these model classes in their ability to predict neural and behavioral data both w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#20998;&#31867;&#27861;&#65292;&#21487;&#20197;&#29992;&#26469;&#35774;&#35745;&#20855;&#26377;&#29305;&#23450;&#23646;&#24615;&#30340;&#25552;&#31034;&#26469;&#25191;&#34892;&#21508;&#31181;&#22797;&#26434;&#20219;&#21153;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;LLM&#22312;&#25191;&#34892;&#22797;&#26434;&#20219;&#21153;&#26041;&#38754;&#30340;&#24615;&#33021;&#21464;&#24322;&#24040;&#22823;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.11430</link><description>&lt;p&gt;
TELeR&#65306;&#29992;&#20110;&#22522;&#20934;&#27979;&#35797;&#22797;&#26434;&#20219;&#21153;&#30340;LLM&#25552;&#31034;&#30340;&#36890;&#29992;&#20998;&#31867;&#27861;
&lt;/p&gt;
&lt;p&gt;
TELeR: A General Taxonomy of LLM Prompts for Benchmarking Complex Tasks. (arXiv:2305.11430v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11430
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#20998;&#31867;&#27861;&#65292;&#21487;&#20197;&#29992;&#26469;&#35774;&#35745;&#20855;&#26377;&#29305;&#23450;&#23646;&#24615;&#30340;&#25552;&#31034;&#26469;&#25191;&#34892;&#21508;&#31181;&#22797;&#26434;&#20219;&#21153;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;LLM&#22312;&#25191;&#34892;&#22797;&#26434;&#20219;&#21153;&#26041;&#38754;&#30340;&#24615;&#33021;&#21464;&#24322;&#24040;&#22823;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;LLM&#22312;&#20256;&#32479;&#23545;&#35805;&#29615;&#22659;&#20013;&#29702;&#35299;&#21644;&#29983;&#25104;&#25991;&#26412;&#26102;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#22312;&#25191;&#34892;&#19981;&#26126;&#30830;&#30340;&#22797;&#26434;&#20219;&#21153;&#26041;&#38754;&#30340;&#28508;&#21147;&#20173;&#28982;&#21463;&#21040;&#24456;&#23569;&#30340;&#30740;&#31350;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#20998;&#31867;&#27861;&#65292;&#21487;&#20197;&#29992;&#26469;&#35774;&#35745;&#20855;&#26377;&#29305;&#23450;&#23646;&#24615;&#30340;&#25552;&#31034;&#65292;&#20197;&#25191;&#34892;&#21508;&#31181;&#22797;&#26434;&#20219;&#21153;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#20351;&#29992;&#19981;&#21516;&#25552;&#31034;&#31867;&#22411;/&#39118;&#26684;&#21644;&#25552;&#31034;&#25552;&#20379;&#30340;&#19981;&#21516;&#35814;&#32454;&#31243;&#24230;&#26102;LLM&#24615;&#33021;&#21464;&#21270;&#24040;&#22823;&#30340;&#38382;&#39064;&#12290;&#36825;&#20010;&#20998;&#31867;&#27861;&#23558;&#20351;&#26410;&#26469;&#30340;&#22522;&#20934;&#27979;&#35797;&#30740;&#31350;&#33021;&#22815;&#25253;&#21578;&#30740;&#31350;&#20013;&#20351;&#29992;&#30340;&#29305;&#23450;&#25552;&#31034;&#31867;&#21035;&#65292;&#20174;&#32780;&#23454;&#29616;&#36328;&#19981;&#21516;&#30740;&#31350;&#30340;&#26377;&#24847;&#20041;&#30340;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
While LLMs have shown great success in understanding and generating text in traditional conversational settings, their potential for performing ill-defined complex tasks is largely under-studied. Indeed, we are yet to conduct comprehensive benchmarking studies with multiple LLMs that are exclusively focused on a complex task. However, conducting such benchmarking studies is challenging because of the large variations in LLMs' performance when different prompt types/styles are used and different degrees of detail are provided in the prompts. To address this issue, the paper proposes a general taxonomy that can be used to design prompts with specific properties in order to perform a wide range of complex tasks. This taxonomy will allow future benchmarking studies to report the specific categories of prompts used as part of the study, enabling meaningful comparisons across different studies. Also, by establishing a common standard through this taxonomy, researchers will be able to draw mo
&lt;/p&gt;</description></item><item><title>API-Bank&#26159;&#19968;&#20010;&#38024;&#23545;&#24037;&#20855;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#36890;&#36807;&#35299;&#20915;&#19977;&#20010;&#20851;&#38190;&#38382;&#39064;&#26469;&#35780;&#20272;LLMs&#30340;&#33021;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;GPT-3.5&#30340;&#25913;&#36827;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.08244</link><description>&lt;p&gt;
API-Bank: &#19968;&#31181;&#38024;&#23545;&#24037;&#20855;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#38754;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
API-Bank: A Comprehensive Benchmark for Tool-Augmented LLMs. (arXiv:2304.08244v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08244
&lt;/p&gt;
&lt;p&gt;
API-Bank&#26159;&#19968;&#20010;&#38024;&#23545;&#24037;&#20855;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#36890;&#36807;&#35299;&#20915;&#19977;&#20010;&#20851;&#38190;&#38382;&#39064;&#26469;&#35780;&#20272;LLMs&#30340;&#33021;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;GPT-3.5&#30340;&#25913;&#36827;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#22806;&#37096;&#24037;&#20855;&#26469;&#22686;&#24378;&#20854;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20173;&#28982;&#23384;&#22312;&#19977;&#20010;&#20851;&#38190;&#38382;&#39064;&#23578;&#26410;&#35299;&#31572;&#65306;&#65288;1&#65289;&#30446;&#21069;&#30340;LLMs&#22312;&#21033;&#29992;&#24037;&#20855;&#26041;&#38754;&#30340;&#25928;&#26524;&#22914;&#20309;&#65311;&#65288;2&#65289;&#22914;&#20309;&#22686;&#24378;LLMs&#21033;&#29992;&#24037;&#20855;&#30340;&#33021;&#21147;&#65311;&#65288;3&#65289;&#22914;&#20309;&#20811;&#26381;&#21033;&#29992;&#24037;&#20855;&#25152;&#38754;&#20020;&#30340;&#38556;&#30861;&#65311;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;API-Bank&#65292;&#19968;&#20010;&#20855;&#26377;&#31361;&#30772;&#24615;&#24847;&#20041;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#19987;&#38376;&#20026;&#24037;&#20855;&#22686;&#24378;&#30340;LLMs&#35774;&#35745;&#12290;&#38024;&#23545;&#31532;&#19968;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21487;&#36816;&#34892;&#30340;&#35780;&#20272;&#31995;&#32479;&#65292;&#21253;&#21547;73&#20010;API&#24037;&#20855;&#12290;&#25105;&#20204;&#20351;&#29992;753&#20010;API&#35843;&#29992;&#27880;&#37322;&#20102;314&#20010;&#24037;&#20855;&#20351;&#29992;&#23545;&#35805;&#65292;&#20197;&#35780;&#20272;&#29616;&#26377;LLMs&#22312;&#35268;&#21010;&#12289;&#26816;&#32034;&#21644;&#35843;&#29992;API&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#38024;&#23545;&#31532;&#20108;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#26469;&#33258;1,000&#20010;&#19981;&#21516;&#39046;&#22495;&#30340;2,138&#20010;API&#30340;1,888&#20010;&#24037;&#20855;&#20351;&#29992;&#23545;&#35805;&#30340;&#20840;&#38754;&#35757;&#32451;&#38598;&#12290;&#20351;&#29992;&#36825;&#20010;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;Lynx&#65292;&#36825;&#26159;&#19968;&#20010;&#20174;Alpaca&#21021;&#22987;&#21270;&#30340;&#24037;&#20855;&#22686;&#24378;LLM&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;GPT-3.5&#26174;&#31034;&#20986;&#20102;&#25913;&#36827;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research has demonstrated that Large Language Models (LLMs) can enhance their capabilities by utilizing external tools. However, three pivotal questions remain unanswered: (1) How effective are current LLMs in utilizing tools? (2) How can we enhance LLMs' ability to utilize tools? (3) What obstacles need to be overcome to leverage tools? To address these questions, we introduce API-Bank, a groundbreaking benchmark, specifically designed for tool-augmented LLMs. For the first question, we develop a runnable evaluation system consisting of 73 API tools. We annotate 314 tool-use dialogues with 753 API calls to assess the existing LLMs' capabilities in planning, retrieving, and calling APIs. For the second question, we construct a comprehensive training set containing 1,888 tool-use dialogues from 2,138 APIs spanning 1,000 distinct domains. Using this dataset, we train Lynx, a tool-augmented LLM initialized from Alpaca. Experimental results demonstrate that GPT-3.5 exhibits improved
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25581;&#31034;&#20197;&#21450;&#25552;&#20986;&#20102;&#35299;&#20915;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#24040;&#22823;&#27700;&#36275;&#36857;&#30340;&#26041;&#27861;&#65292;&#22240;&#20026;&#20854;&#28129;&#27700;&#28040;&#32791;&#24050;&#32463;&#24341;&#36215;&#22269;&#38469;&#31038;&#20250;&#30340;&#37325;&#35270;&#65292;&#24182;&#19988;AI&#27169;&#22411;&#24212;&#35813;&#25215;&#25285;&#31038;&#20250;&#36131;&#20219;&#65292;&#20570;&#20986;&#38754;&#23545;&#27700;&#21361;&#26426;&#30340;&#34920;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.03271</link><description>&lt;p&gt;
&#20351;AI&#8220;&#21475;&#28212;&#8221;&#20943;&#23569;&#30340;&#26041;&#27861;&#65306;&#25581;&#31034;&#21644;&#35299;&#20915;AI&#27169;&#22411;&#30340;&#31192;&#23494;&#27700;&#28040;&#32791;
&lt;/p&gt;
&lt;p&gt;
Making AI Less "Thirsty": Uncovering and Addressing the Secret Water Footprint of AI Models. (arXiv:2304.03271v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03271
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25581;&#31034;&#20197;&#21450;&#25552;&#20986;&#20102;&#35299;&#20915;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#24040;&#22823;&#27700;&#36275;&#36857;&#30340;&#26041;&#27861;&#65292;&#22240;&#20026;&#20854;&#28129;&#27700;&#28040;&#32791;&#24050;&#32463;&#24341;&#36215;&#22269;&#38469;&#31038;&#20250;&#30340;&#37325;&#35270;&#65292;&#24182;&#19988;AI&#27169;&#22411;&#24212;&#35813;&#25215;&#25285;&#31038;&#20250;&#36131;&#20219;&#65292;&#20570;&#20986;&#38754;&#23545;&#27700;&#21361;&#26426;&#30340;&#34920;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27169;&#22411;&#30340;&#30899;&#36275;&#36857;&#19981;&#26029;&#22686;&#38271;&#65292;&#29305;&#21035;&#26159;&#20687;GPT-3&#21644;GPT-4&#36825;&#26679;&#30340;&#22823;&#22411;&#27169;&#22411;&#65292;&#24050;&#32463;&#21463;&#21040;&#20844;&#20247;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#21516;&#31561;&#37325;&#35201;&#19988;&#24040;&#22823;&#30340;AI&#27169;&#22411;&#27700;&#21360;&#23578;&#26410;&#24341;&#36215;&#20154;&#20204;&#30340;&#27880;&#24847;&#12290;&#20363;&#22914;&#65292;&#22312;&#24494;&#36719;&#26368;&#20808;&#36827;&#30340;&#32654;&#22269;&#25968;&#25454;&#20013;&#24515;&#20013;&#35757;&#32451;GPT-3&#21487;&#20197;&#30452;&#25509;&#28040;&#32791;70&#19975;&#21319;&#28165;&#27905;&#28129;&#27700;&#65288;&#30456;&#24403;&#20110;&#29983;&#20135;370&#36742;&#23453;&#39532;&#27773;&#36710;&#25110;320&#36742;&#29305;&#26031;&#25289;&#30005;&#21160;&#27773;&#36710;&#65289;&#65292;&#22914;&#26524;&#22312;&#24494;&#36719;&#30340;&#20122;&#27954;&#25968;&#25454;&#20013;&#24515;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#20010;&#27700;&#28040;&#32791;&#37327;&#23558;&#22686;&#21152;&#19977;&#20493;&#65292;&#20294;&#36825;&#26679;&#30340;&#20449;&#24687;&#19968;&#30452;&#34987;&#20445;&#23494;&#12290;&#36825;&#26497;&#20854;&#20196;&#20154;&#25285;&#24551;&#65292;&#22240;&#20026;&#28129;&#27700;&#30701;&#32570;&#24050;&#25104;&#20026;&#22312;&#20154;&#21475;&#36805;&#36895;&#22686;&#38271;&#12289;&#27700;&#36164;&#28304;&#20943;&#23569;&#21644;&#32769;&#21270;&#30340;&#27700;&#22522;&#30784;&#35774;&#26045;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25152;&#26377;&#20154;&#38754;&#20020;&#30340;&#26368;&#32039;&#36843;&#30340;&#25361;&#25112;&#20043;&#19968;&#12290;&#20026;&#20102;&#24212;&#23545;&#20840;&#29699;&#27700;&#36164;&#28304;&#30340;&#25361;&#25112;&#65292;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#21487;&#20197;&#65292;&#32780;&#19988;&#24212;&#35813;&#65292;&#25215;&#25285;&#31038;&#20250;&#36131;&#20219;&#65292;&#20197;&#36523;&#20316;&#21017;&#35299;&#20915;&#33258;&#24049;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The growing carbon footprint of artificial intelligence (AI) models, especially large ones such as GPT-3 and GPT-4, has been undergoing public scrutiny. Unfortunately, however, the equally important and enormous water footprint of AI models has remained under the radar. For example, training GPT-3 in Microsoft's state-of-the-art U.S. data centers can directly consume 700,000 liters of clean freshwater (enough for producing 370 BMW cars or 320 Tesla electric vehicles) and the water consumption would have been tripled if training were done in Microsoft's Asian data centers, but such information has been kept as a secret. This is extremely concerning, as freshwater scarcity has become one of the most pressing challenges shared by all of us in the wake of the rapidly growing population, depleting water resources, and aging water infrastructures. To respond to the global water challenges, AI models can, and also should, take social responsibility and lead by example by addressing their own 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;SQL&#26597;&#35810;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;LLMs&#20013;&#30340;&#20449;&#24687;&#65292;&#21487;&#20197;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#25552;&#21462;&#25968;&#25454;&#24182;&#36827;&#34892;&#26597;&#35810;&#12290;&#36890;&#36807;Galois&#21407;&#22411;&#23454;&#29616;&#20102;&#26597;&#35810;LLMs&#30340;&#26032;&#29289;&#29702;&#36816;&#31639;&#31526;&#65292;&#24182;&#21462;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.00472</link><description>&lt;p&gt;
&#20351;&#29992;SQL&#26597;&#35810;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Querying Large Language Models with SQL. (arXiv:2304.00472v2 [cs.DB] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00472
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;SQL&#26597;&#35810;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;LLMs&#20013;&#30340;&#20449;&#24687;&#65292;&#21487;&#20197;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#25552;&#21462;&#25968;&#25454;&#24182;&#36827;&#34892;&#26597;&#35810;&#12290;&#36890;&#36807;Galois&#21407;&#22411;&#23454;&#29616;&#20102;&#26597;&#35810;LLMs&#30340;&#26032;&#29289;&#29702;&#36816;&#31639;&#31526;&#65292;&#24182;&#21462;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#20351;&#29992;&#22330;&#26223;&#20013;&#65292;&#20449;&#24687;&#23384;&#20648;&#22312;&#25991;&#26412;&#20013;&#65292;&#20294;&#26080;&#27861;&#22312;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#33719;&#21462;&#12290;&#28982;&#32780;&#65292;&#20174;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#20013;&#25552;&#21462;&#25968;&#25454;&#20197;&#31934;&#30830;&#36866;&#37197;&#27169;&#24335;&#65292;&#24182;&#23454;&#29616;&#26597;&#35810;&#65292;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#38543;&#30528;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20852;&#36215;&#65292;&#29616;&#22312;&#26377;&#20102;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#23384;&#20648;&#21644;&#20351;&#29992;&#20174;&#22823;&#35268;&#27169;&#25991;&#26412;&#25991;&#26723;&#20013;&#25552;&#21462;&#30340;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35774;&#24819;&#20351;&#29992;SQL&#26597;&#35810;&#26469;&#28085;&#30422;&#20256;&#32479;&#25968;&#25454;&#24211;&#26080;&#27861;&#25552;&#21462;&#30340;&#24191;&#27867;&#25968;&#25454;&#65292;&#36890;&#36807;&#21033;&#29992;LLMs&#20013;&#30340;&#20449;&#24687;&#12290;&#20026;&#20102;&#25903;&#25745;&#36825;&#20010;&#24895;&#26223;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#20256;&#32479;&#25968;&#25454;&#24211;&#20307;&#31995;&#32467;&#26500;&#30340;Galois&#21407;&#22411;&#65292;&#20294;&#20855;&#26377;&#29992;&#20110;&#26597;&#35810;&#24213;&#23618;LLM&#30340;&#26032;&#29289;&#29702;&#31639;&#23376;&#12290;&#20027;&#35201;&#24605;&#24819;&#26159;&#20351;&#29992;&#25552;&#31034;&#31526;&#25191;&#34892;&#26597;&#35810;&#35745;&#21010;&#20013;&#30340;&#26576;&#20123;&#25805;&#20316;&#31526;&#65292;&#20174;LLM&#20013;&#26816;&#32034;&#25968;&#25454;&#12290;&#23545;&#20110;&#22823;&#31867;&#21035;&#30340;SQL&#26597;&#35810;&#65292;&#26597;&#35810;LLMs&#36820;&#22238;&#32467;&#26500;&#33391;&#22909;&#30340;&#20851;&#31995;&#65292;&#21462;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#23450;&#24615;&#32467;&#26524;&#12290;&#21021;&#27493;&#23454;&#39564;&#32467;&#26524;&#20351;&#39044;&#35757;&#32451;&#30340;LLMs&#25104;&#20026;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many use-cases, information is stored in text but not available in structured data. However, extracting data from natural language text to precisely fit a schema, and thus enable querying, is a challenging task. With the rise of pre-trained Large Language Models (LLMs), there is now an effective solution to store and use information extracted from massive corpora of text documents. Thus, we envision the use of SQL queries to cover a broad range of data that is not captured by traditional databases by tapping the information in LLMs. To ground this vision, we present Galois, a prototype based on a traditional database architecture, but with new physical operators for querying the underlying LLM. The main idea is to execute some operators of the the query plan with prompts that retrieve data from the LLM. For a large class of SQL queries, querying LLMs returns well structured relations, with encouraging qualitative results. Preliminary experimental results make pre-trained LLMs a prom
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;D5&#65292;&#36890;&#36807;&#30446;&#26631;&#39537;&#21160;&#30340;&#26041;&#24335;&#33258;&#21160;&#21457;&#29616;&#20004;&#20010;&#22823;&#22411;&#35821;&#26009;&#24211;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#20316;&#32773;&#26500;&#24314;&#20102;&#19968;&#20010;D5&#31995;&#32479;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#22871;&#32479;&#19968;&#30340;&#35780;&#20272;&#25351;&#26631;&#26469;&#34913;&#37327;&#20854;&#24615;&#33021;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#20351;&#29992;&#30446;&#26631;&#39537;&#21160;&#30340;&#26041;&#27861;&#26469;&#21457;&#29616;&#35821;&#26009;&#24211;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2302.14233</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#30446;&#26631;&#30340;&#35821;&#35328;&#25551;&#36848;&#21457;&#29616;&#20998;&#24067;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
Goal Driven Discovery of Distributional Differences via Language Descriptions. (arXiv:2302.14233v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14233
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;D5&#65292;&#36890;&#36807;&#30446;&#26631;&#39537;&#21160;&#30340;&#26041;&#24335;&#33258;&#21160;&#21457;&#29616;&#20004;&#20010;&#22823;&#22411;&#35821;&#26009;&#24211;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#20316;&#32773;&#26500;&#24314;&#20102;&#19968;&#20010;D5&#31995;&#32479;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#22871;&#32479;&#19968;&#30340;&#35780;&#20272;&#25351;&#26631;&#26469;&#34913;&#37327;&#20854;&#24615;&#33021;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#20351;&#29992;&#30446;&#26631;&#39537;&#21160;&#30340;&#26041;&#27861;&#26469;&#21457;&#29616;&#35821;&#26009;&#24211;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25366;&#25496;&#22823;&#22411;&#35821;&#26009;&#24211;&#21487;&#20197;&#20135;&#29983;&#26377;&#29992;&#30340;&#21457;&#29616;&#65292;&#20294;&#23545;&#20154;&#31867;&#26469;&#35828;&#32791;&#26102;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;D5&#65292;&#23427;&#20197;&#30446;&#26631;&#39537;&#21160;&#30340;&#26041;&#24335;&#33258;&#21160;&#21457;&#29616;&#20004;&#20010;&#22823;&#22411;&#35821;&#26009;&#24211;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#20219;&#21153;&#36755;&#20837;&#26159;&#19968;&#20010;&#38382;&#39064;&#65292;&#21253;&#25324;&#19968;&#20010;&#30740;&#31350;&#30446;&#26631;&#8220;&#27604;&#36739;&#33647;&#29289;A&#21644;&#33647;&#29289;B&#30340;&#21103;&#20316;&#29992;&#8221;&#65292;&#20197;&#21450;&#19968;&#20010;&#35821;&#26009;&#24211;&#23545;&#65288;&#20004;&#20010;&#22823;&#22411;&#24739;&#32773;&#33258;&#25253;&#21453;&#24212;&#30340;&#38598;&#21512;&#65289;&#12290;&#36755;&#20986;&#26159;&#23545;&#36825;&#20123;&#35821;&#26009;&#24211;&#24046;&#24322;&#30340;&#35821;&#35328;&#25551;&#36848;&#65288;&#21457;&#29616;&#65289;&#65288;&#20351;&#29992;&#33647;&#29289;A&#21518;&#65292;&#24739;&#32773;&#26356;&#32463;&#24120;&#25552;&#21040;&#8220;&#20559;&#25191;&#24863;&#8221;&#65289;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;D5&#31995;&#32479;&#65292;&#24182;&#20026;&#20102;&#23450;&#37327;&#34913;&#37327;&#20854;&#24615;&#33021;&#65292;&#25105;&#20204;&#36129;&#29486;&#20102;&#19968;&#20010;&#20803;&#25968;&#25454;&#38598;OpenD5&#65292;&#32858;&#21512;&#20102;675&#20010;&#24320;&#25918;&#38382;&#39064;&#65292;&#28085;&#30422;&#20102;&#21830;&#19994;&#12289;&#31038;&#20250;&#31185;&#23398;&#12289;&#20154;&#25991;&#23398;&#31185;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#20581;&#24247;&#31561;&#39046;&#22495;&#65292;&#21516;&#26102;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#22871;&#32479;&#19968;&#30340;&#35780;&#20272;&#25351;&#26631;&#65306;&#26377;&#25928;&#24615;&#12289;&#30456;&#20851;&#24615;&#12289;&#26032;&#39062;&#24615;&#21644;&#26174;&#33879;&#24615;&#12290;&#36890;&#36807;&#25968;&#25454;&#38598;&#21644;&#32479;&#19968;&#25351;&#26631;&#65292;&#25105;&#20204;&#30830;&#35748;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#20351;&#29992;&#30446;&#26631;&#39537;&#21160;&#30340;&#26041;&#27861;&#26469;&#21457;&#29616;&#35821;&#26009;&#24211;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mining large corpora can generate useful discoveries but is time-consuming for humans. We formulate a new task, D5, that automatically discovers differences between two large corpora in a goal-driven way. The task input is a problem comprising a research goal "$\textit{comparing the side effects of drug A and drug B}$" and a corpus pair (two large collections of patients' self-reported reactions after taking each drug). The output is a language description (discovery) of how these corpora differ (patients taking drug A "$\textit{mention feelings of paranoia}$" more often). We build a D5 system, and to quantitatively measure its performance, we 1) contribute a meta-dataset, OpenD5, aggregating 675 open-ended problems ranging across business, social sciences, humanities, machine learning, and health, and 2) propose a set of unified evaluation metrics: validity, relevance, novelty, and significance. With the dataset and the unified metrics, we confirm that language models can use the goal
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;&#24191;&#20041;&#30340;Tsallis KL&#25955;&#24230;&#65292;&#25193;&#23637;&#20102;Munchausen&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#23558;KL&#27491;&#21017;&#21270;&#32435;&#20837;&#23454;&#38469;&#31639;&#27861;&#30340;&#26041;&#27861;&#12290;&#23545;&#20110;Tsallis KL&#65292;&#24403;$q &gt; 1$&#26102;&#65292;&#21487;&#20197;&#33719;&#24471;&#26032;&#30340;&#31574;&#30053;&#20248;&#21270;&#36873;&#39033;&#12290;</title><link>http://arxiv.org/abs/2301.11476</link><description>&lt;p&gt;
&#20351;&#29992;Tsallis KL&#25955;&#24230;&#30340;&#24191;&#20041;Munchausen&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Generalized Munchausen Reinforcement Learning using Tsallis KL Divergence. (arXiv:2301.11476v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11476
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;&#24191;&#20041;&#30340;Tsallis KL&#25955;&#24230;&#65292;&#25193;&#23637;&#20102;Munchausen&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#23558;KL&#27491;&#21017;&#21270;&#32435;&#20837;&#23454;&#38469;&#31639;&#27861;&#30340;&#26041;&#27861;&#12290;&#23545;&#20110;Tsallis KL&#65292;&#24403;$q &gt; 1$&#26102;&#65292;&#21487;&#20197;&#33719;&#24471;&#26032;&#30340;&#31574;&#30053;&#20248;&#21270;&#36873;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#37117;&#37319;&#29992;Kullback-Leibler&#65288;KL&#65289;&#25955;&#24230;&#21040;&#19978;&#19968;&#20010;&#31574;&#30053;&#65292;&#20197;&#38450;&#27490;&#31574;&#30053;&#21464;&#21270;&#36807;&#24555;&#12290;&#36825;&#20010;&#24819;&#27861;&#26368;&#21021;&#26159;&#22312;Conservative Policy Iteration&#30340;&#19968;&#31687;&#37325;&#35201;&#35770;&#25991;&#20013;&#25552;&#20986;&#30340;&#65292;&#36817;&#20284;&#31639;&#27861;&#22914;TRPO&#21644;Munchausen Value Iteration&#65288;MVI&#65289;&#32473;&#20986;&#20102;&#26377;&#38480;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#19968;&#31181;&#24191;&#20041;&#30340;KL&#25955;&#24230; - &#31216;&#20026;Tsallis KL&#25955;&#24230; - &#26469;&#32487;&#32493;&#36825;&#19968;&#24037;&#20316;&#65292;&#23427;&#22312;&#23450;&#20041;&#20013;&#20351;&#29992;&#20102;$q$-&#23545;&#25968;&#12290;&#36825;&#31181;&#26041;&#27861;&#26159;&#19968;&#31181;&#20005;&#26684;&#30340;&#25512;&#24191;&#65292;&#22240;&#20026;$q = 1$&#23545;&#24212;&#20110;&#26631;&#20934;&#30340;KL&#25955;&#24230;&#65307;$q &gt; 1$&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#26032;&#30340;&#36873;&#39033;&#12290;&#25105;&#20204;&#23545;&#22312;Tsallis KL&#19979;&#23398;&#20064;&#30340;&#31574;&#30053;&#31867;&#22411;&#36827;&#34892;&#20102;&#34920;&#24449;&#65292;&#24182;&#38416;&#36848;&#20102;&#20309;&#26102;$ q &gt; 1 $&#21487;&#33021;&#26159;&#26377;&#30410;&#30340;&#12290;&#20026;&#20102;&#33719;&#24471;&#19968;&#20010;&#23558;Tsallis KL&#27491;&#21017;&#21270;&#32435;&#20837;&#23454;&#38469;&#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;MVI&#65292;&#23427;&#26159;&#19968;&#31181;&#26368;&#31616;&#21333;&#30340;&#21253;&#21547;KL&#27491;&#21017;&#21270;&#30340;&#26041;&#27861;&#20043;&#19968;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#24191;&#20041;MVI&#65288;$q$&#65289;&#33719;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many policy optimization approaches in reinforcement learning incorporate a Kullback-Leilbler (KL) divergence to the previous policy, to prevent the policy from changing too quickly. This idea was initially proposed in a seminal paper on Conservative Policy Iteration, with approximations given by algorithms like TRPO and Munchausen Value Iteration (MVI). We continue this line of work by investigating a generalized KL divergence -- called the Tsallis KL divergence -- which use the $q$-logarithm in the definition. The approach is a strict generalization, as $q = 1$ corresponds to the standard KL divergence; $q &gt; 1$ provides a range of new options. We characterize the types of policies learned under the Tsallis KL, and motivate when $q &gt;1$ could be beneficial. To obtain a practical algorithm that incorporates Tsallis KL regularization, we extend MVI, which is one of the simplest approaches to incorporate KL regularization. We show that this generalized MVI($q$) obtains significant improve
&lt;/p&gt;</description></item><item><title>AtMan&#26159;&#19968;&#31181;&#36890;&#36807;&#22312;&#29983;&#25104;&#24335;Transformer&#27169;&#22411;&#20013;&#25805;&#32437;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#35299;&#37322;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#20960;&#20046;&#19981;&#21344;&#29992;&#39069;&#22806;&#20869;&#23384;&#65292;&#21487;&#22312;&#29983;&#20135;&#29615;&#22659;&#20013;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2301.08110</link><description>&lt;p&gt;
AtMan:&#36890;&#36807;&#33410;&#32422;&#20869;&#23384;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#29702;&#35299;Transformer&#30340;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
AtMan: Understanding Transformer Predictions Through Memory Efficient Attention Manipulation. (arXiv:2301.08110v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08110
&lt;/p&gt;
&lt;p&gt;
AtMan&#26159;&#19968;&#31181;&#36890;&#36807;&#22312;&#29983;&#25104;&#24335;Transformer&#27169;&#22411;&#20013;&#25805;&#32437;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#35299;&#37322;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#20960;&#20046;&#19981;&#21344;&#29992;&#39069;&#22806;&#20869;&#23384;&#65292;&#21487;&#22312;&#29983;&#20135;&#29615;&#22659;&#20013;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#30340;Transformer&#27169;&#22411;&#36234;&#26469;&#36234;&#22797;&#26434;&#65292;&#21442;&#25968;&#25968;&#37327;&#22823;&#19988;&#20855;&#22791;&#22788;&#29702;&#22810;&#36755;&#20837;&#27169;&#24577;&#30340;&#33021;&#21147;&#12290;&#30446;&#21069;&#35299;&#37322;&#23427;&#20204;&#30340;&#39044;&#27979;&#30340;&#26041;&#27861;&#36164;&#28304;&#23494;&#38598;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#23427;&#20204;&#38656;&#35201;&#36807;&#22810;&#30340;&#39069;&#22806;&#20869;&#23384;&#65292;&#22240;&#20026;&#23427;&#20204;&#20381;&#36182;&#21453;&#21521;&#20256;&#25773;&#65292;&#32780;&#21453;&#21521;&#20256;&#25773;&#20250;&#20998;&#37197;&#30340;GPU&#20869;&#23384;&#20960;&#20046;&#26159;&#21069;&#21521;&#20256;&#25773;&#30340;&#20004;&#20493;&#12290;&#36825;&#20351;&#24471;&#22312;&#29983;&#20135;&#29615;&#22659;&#20013;&#20351;&#29992;&#23427;&#20204;&#38750;&#24120;&#22256;&#38590;&#65292;&#29978;&#33267;&#19981;&#21487;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;AtMan&#65292;&#23427;&#20960;&#20046;&#19981;&#20250;&#20135;&#29983;&#39069;&#22806;&#30340;&#25104;&#26412;&#65292;&#29992;&#20110;&#35299;&#37322;&#29983;&#25104;&#24335;Transformer&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;AtMan&#26159;&#19968;&#31181;&#27169;&#24577;&#26080;&#20851;&#30340;&#25200;&#21160;&#26041;&#27861;&#65292;&#36890;&#36807;&#25805;&#32437;Transformer&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#29983;&#25104;&#19982;&#36755;&#20986;&#39044;&#27979;&#30456;&#20851;&#24615;&#30340;&#37325;&#35201;&#24615;&#22270;&#12290;AtMan&#19981;&#20351;&#29992;&#21453;&#21521;&#20256;&#25773;&#65292;&#32780;&#26159;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#24212;&#29992;&#19968;&#31181;&#22522;&#20110;&#20313;&#24358;&#30456;&#20284;&#24230;&#37051;&#36817;&#24615;&#30340;&#21487;&#24182;&#34892;&#21270;&#22522;&#20110;&#35760;&#21495;&#30340;&#25628;&#32034;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#25991;&#26412;&#21644;&#22270;&#20687;-&#25991;&#26412;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#35814;&#23613;&#30340;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
Generative transformer models have become increasingly complex, with large numbers of parameters and the ability to process multiple input modalities. Current methods for explaining their predictions are resource-intensive. Most crucially, they require prohibitively large amounts of extra memory, since they rely on backpropagation which allocates almost twice as much GPU memory as the forward pass. This makes it difficult, if not impossible, to use them in production. We present AtMan that provides explanations of generative transformer models at almost no extra cost. Specifically, AtMan is a modality-agnostic perturbation method that manipulates the attention mechanisms of transformers to produce relevance maps for the input with respect to the output prediction. Instead of using backpropagation, AtMan applies a parallelizable token-based search method based on cosine similarity neighborhood in the embedding space. Our exhaustive experiments on text and image-text benchmarks demonstra
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#38024;&#23545;"&#24320;&#25918;&#39046;&#22495;"&#30340;&#22810;&#25991;&#26723;&#25688;&#35201;&#20219;&#21153;&#36827;&#34892;&#30740;&#31350;&#65292;&#21457;&#29616;&#26368;&#20808;&#36827;&#30340;&#25688;&#35201;&#22120;&#22312;&#27492;&#24773;&#22659;&#19979;&#24615;&#33021;&#19979;&#38477;&#20005;&#37325;&#65292;&#20294;&#36827;&#34892;&#39069;&#22806;&#30340;&#24320;&#25918;&#39046;&#22495;&#30340;&#35757;&#32451;&#21487;&#20197;&#38477;&#20302;&#23545;&#19981;&#23436;&#32654;&#26816;&#32034;&#30340;&#25935;&#24863;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.10526</link><description>&lt;p&gt;
&#38754;&#21521;&#24320;&#25918;&#39046;&#22495;&#30340;&#22810;&#25991;&#26723;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Towards multi-document summarization in the open-domain. (arXiv:2212.10526v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10526
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#38024;&#23545;"&#24320;&#25918;&#39046;&#22495;"&#30340;&#22810;&#25991;&#26723;&#25688;&#35201;&#20219;&#21153;&#36827;&#34892;&#30740;&#31350;&#65292;&#21457;&#29616;&#26368;&#20808;&#36827;&#30340;&#25688;&#35201;&#22120;&#22312;&#27492;&#24773;&#22659;&#19979;&#24615;&#33021;&#19979;&#38477;&#20005;&#37325;&#65292;&#20294;&#36827;&#34892;&#39069;&#22806;&#30340;&#24320;&#25918;&#39046;&#22495;&#30340;&#35757;&#32451;&#21487;&#20197;&#38477;&#20302;&#23545;&#19981;&#23436;&#32654;&#26816;&#32034;&#30340;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#25991;&#26723;&#25688;&#35201;(MDS)&#36890;&#24120;&#20551;&#35774;&#25552;&#20379;&#19968;&#32452;&#20027;&#39064;&#30456;&#20851;&#30340;&#25991;&#26723;&#12290;&#20294;&#26159;&#65292;&#36825;&#20010;&#25991;&#26723;&#38598;&#36890;&#24120;&#26159;&#25968;&#25454;&#38598;&#31574;&#21010;&#36807;&#31243;&#30340;&#20135;&#29289;&#65307;&#22312;&#23454;&#36341;&#20013;&#65292;&#23427;&#19981;&#19968;&#23450;&#21487;&#29992;&#65292;&#38656;&#35201;&#26681;&#25454;&#20449;&#24687;&#38656;&#27714;&#65292;&#21363;&#38382;&#39064;&#25110;&#20027;&#39064;&#38472;&#36848;&#36827;&#34892;&#26816;&#32034;&#12290;&#25105;&#20204;&#36890;&#36807;&#24418;&#24335;&#21270;&#20219;&#21153;&#24182;&#20351;&#29992;&#29616;&#26377;&#25968;&#25454;&#38598;&#65292;&#26816;&#32034;&#22120;&#21644;&#25688;&#35201;&#22120;&#26469;&#24341;&#23548;&#36825;&#20010;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#8220;&#24320;&#25918;&#39046;&#22495;&#8221;&#35774;&#32622;&#30340;&#30740;&#31350;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#30830;&#23450;&#65306;(1)&#21363;&#20351;&#26816;&#32034;&#24615;&#33021;&#36739;&#39640;&#65292;&#26368;&#20808;&#36827;&#30340;&#25688;&#35201;&#22120;&#22312;&#24212;&#29992;&#20110;&#24320;&#25918;&#39046;&#22495;&#26102;&#20063;&#20250;&#22823;&#24133;&#38477;&#20302;&#24615;&#33021;;(2)&#22312;&#24320;&#25918;&#39046;&#22495;&#30340;&#35774;&#32622;&#20013;&#36827;&#34892;&#39069;&#22806;&#30340;&#35757;&#32451;&#21487;&#20197;&#38477;&#20302;&#23545;&#19981;&#23436;&#32654;&#26816;&#32034;&#30340;&#25935;&#24863;&#24615;&#65292;(3)&#25688;&#35201;&#22120;&#23545;&#26816;&#32034;&#37325;&#22797;&#25991;&#26723;&#21644;&#26816;&#32034;&#25991;&#26723;&#30340;&#39034;&#24207;&#19981;&#25935;&#24863;&#65292;&#20294;&#23545;&#20854;&#20182;&#38169;&#35823;&#65292;&#22914;&#26816;&#32034;&#26080;&#20851;&#25991;&#26723;&#30340;&#25935;&#24863;&#24615;&#36739;&#39640;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;
&lt;/p&gt;
&lt;p&gt;
Multi-document summarization (MDS) traditionally assumes a set of topic-related documents are provided. However, this document set is often an artifact of the dataset curation process; in practice, it is not necessarily available and would need to be retrieved given an information need, i.e. a question or topic statement. We study this more challenging "open-domain" setting by formalizing the task and bootstrapping it using existing datasets, retrievers and summarizers. Via extensive experimentation, we determine that: (1) state-of-the-art summarizers suffer large reductions in performance when applied to the open-domain, even when retrieval performance is high, (2) additional training in the open-domain setting can reduce this sensitivity to imperfect retrieval, and (3) summarizers are insensitive to the retrieval of duplicate documents and the order of retrieved documents, but highly sensitive to other errors, like the retrieval of irrelevant documents. Based on our results, we provi
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CTCO&#30340;&#26694;&#26550;&#65292;&#20854;&#20013;&#20195;&#29702;&#36873;&#25321;&#36873;&#39033;&#20316;&#20026;&#21487;&#21464;&#25345;&#32493;&#26102;&#38388;&#30340;&#23376;&#31574;&#30053;&#12290;&#36825;&#20010;&#26694;&#26550;&#21487;&#20197;&#20197;&#20219;&#20309;&#25152;&#38656;&#39057;&#29575;&#19982;&#31995;&#32479;&#20132;&#20114;&#65292;&#20174;&#32780;&#25552;&#20379;&#24179;&#28369;&#30340;&#21160;&#20316;&#21464;&#21270;&#65292;&#30456;&#27604;&#20256;&#32479;RL&#21644;&#26102;&#38388;&#25277;&#35937;RL&#26041;&#27861;&#65292;&#20854;&#24615;&#33021;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2212.04407</link><description>&lt;p&gt;
&#21487;&#21464;&#21270;&#20915;&#31574;&#39057;&#29575;&#30340;&#36873;&#39033;&#35780;&#35770;&#32773;
&lt;/p&gt;
&lt;p&gt;
Variable Decision-Frequency Option Critic. (arXiv:2212.04407v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.04407
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CTCO&#30340;&#26694;&#26550;&#65292;&#20854;&#20013;&#20195;&#29702;&#36873;&#25321;&#36873;&#39033;&#20316;&#20026;&#21487;&#21464;&#25345;&#32493;&#26102;&#38388;&#30340;&#23376;&#31574;&#30053;&#12290;&#36825;&#20010;&#26694;&#26550;&#21487;&#20197;&#20197;&#20219;&#20309;&#25152;&#38656;&#39057;&#29575;&#19982;&#31995;&#32479;&#20132;&#20114;&#65292;&#20174;&#32780;&#25552;&#20379;&#24179;&#28369;&#30340;&#21160;&#20316;&#21464;&#21270;&#65292;&#30456;&#27604;&#20256;&#32479;RL&#21644;&#26102;&#38388;&#25277;&#35937;RL&#26041;&#27861;&#65292;&#20854;&#24615;&#33021;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#65292;&#20195;&#29702;&#22312;&#31163;&#25955;&#21644;&#22266;&#23450;&#30340;&#26102;&#38388;&#38388;&#38548;&#20869;&#20570;&#20986;&#20915;&#31574;&#12290;&#20915;&#31574;&#20043;&#38388;&#30340;&#25345;&#32493;&#26102;&#38388;&#21464;&#25104;&#20102;&#19968;&#20010;&#20851;&#38190;&#30340;&#36229;&#21442;&#25968;&#65292;&#22240;&#20026;&#35774;&#32622;&#24471;&#22826;&#30701;&#21487;&#33021;&#20250;&#22686;&#21152;&#38382;&#39064;&#30340;&#38590;&#24230;&#65292;&#38656;&#35201;&#20195;&#29702;&#36827;&#34892;&#22810;&#27425;&#20915;&#31574;&#25165;&#33021;&#23454;&#29616;&#20854;&#30446;&#26631;&#65292;&#32780;&#35774;&#32622;&#24471;&#22826;&#38271;&#20250;&#23548;&#33268;&#20195;&#29702;&#22833;&#21435;&#23545;&#31995;&#32479;&#30340;&#25511;&#21046;&#12290;&#28982;&#32780;&#65292;&#29289;&#29702;&#31995;&#32479;&#19981;&#19968;&#23450;&#38656;&#35201;&#24658;&#23450;&#30340;&#25511;&#21046;&#39057;&#29575;&#65292;&#23545;&#20110;&#23398;&#20064;&#20195;&#29702;&#26469;&#35828;&#65292;&#19968;&#33324;&#24773;&#20917;&#19979;&#65292;&#24403;&#38656;&#35201;&#26102;&#20197;&#39640;&#39057;&#29575;&#36816;&#34892;&#65292;&#32780;&#22312;&#21487;&#33021;&#26102;&#20197;&#20302;&#39057;&#29575;&#36816;&#34892;&#26356;&#22909;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#36830;&#32493;&#26102;&#38388;&#36830;&#32493;&#36873;&#39033; (CTCO) &#30340;&#26694;&#26550;&#65292;&#20854;&#20013;&#20195;&#29702;&#36873;&#25321;&#36873;&#39033;&#20316;&#20026;&#21487;&#21464;&#25345;&#32493;&#26102;&#38388;&#30340;&#23376;&#31574;&#30053;&#12290;&#36825;&#20123;&#36873;&#39033;&#26159;&#26102;&#38388;&#36830;&#32493;&#30340;&#65292;&#21487;&#20197;&#20197;&#20219;&#20309;&#25152;&#38656;&#39057;&#29575;&#19982;&#31995;&#32479;&#20132;&#20114;&#65292;&#20174;&#32780;&#25552;&#20379;&#24179;&#28369;&#30340;&#21160;&#20316;&#21464;&#21270;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#20854;&#24615;&#33021;&#19982;&#20256;&#32479; RL &#21644;&#26102;&#38388;&#25277;&#35937; RL &#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#23637;&#31034;&#20102; CTCO &#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In classic reinforcement learning algorithms, agents make decisions at discrete and fixed time intervals. The duration between decisions becomes a crucial hyperparameter, as setting it too short may increase the difficulty of the problem by requiring the agent to make numerous decisions to achieve its goal, while setting it too long can result in the agent losing control over the system. However, physical systems do not necessarily require a constant control frequency, and for learning agents, it is often preferable to operate with a low frequency when possible and a high frequency when necessary. We propose a framework called Continuous-Time Continuous-Options (CTCO), where the agent chooses options as sub-policies of variable durations. These options are time-continuous and can interact with the system at any desired frequency providing a smooth change of actions. We demonstrate the effectiveness of CTCO by comparing its performance to classical RL and temporal-abstraction RL methods
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#31119;&#21033;&#21644;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38750;&#32447;&#24615;&#31119;&#21033;&#20989;&#25968;&#30340;Q-learning&#31639;&#27861;&#65292;&#36890;&#36807;&#38750;&#32447;&#24615;&#26631;&#37327;&#21270;&#23398;&#20064;&#26356;&#26032;&#21644;&#38750;&#31283;&#24577;&#21160;&#20316;&#36873;&#25321;&#26469;&#20248;&#21270;&#31574;&#30053;&#12290;&#31639;&#27861;&#34987;&#35777;&#26126;&#26159;&#21487;&#25910;&#25947;&#30340;&#12290;</title><link>http://arxiv.org/abs/2212.01382</link><description>&lt;p&gt;
&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#31119;&#21033;&#21644;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Welfare and Fairness in Multi-objective Reinforcement Learning. (arXiv:2212.01382v4 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01382
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#31119;&#21033;&#21644;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38750;&#32447;&#24615;&#31119;&#21033;&#20989;&#25968;&#30340;Q-learning&#31639;&#27861;&#65292;&#36890;&#36807;&#38750;&#32447;&#24615;&#26631;&#37327;&#21270;&#23398;&#20064;&#26356;&#26032;&#21644;&#38750;&#31283;&#24577;&#21160;&#20316;&#36873;&#25321;&#26469;&#20248;&#21270;&#31574;&#30053;&#12290;&#31639;&#27861;&#34987;&#35777;&#26126;&#26159;&#21487;&#25910;&#25947;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20844;&#24179;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#65292;&#20854;&#20013;&#19968;&#20010;&#20195;&#29702;&#24517;&#39035;&#23398;&#20064;&#19968;&#20010;&#33021;&#22815;&#22312;&#22810;&#20010;&#32500;&#24230;&#30340;&#21521;&#37327;&#20540;&#22870;&#21169;&#19978;&#21516;&#26102;&#33719;&#24471;&#39640;&#22238;&#25253;&#30340;&#31574;&#30053;&#12290;&#21463;&#20844;&#24179;&#36164;&#28304;&#20998;&#37197;&#25991;&#29486;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;&#20854;&#24314;&#27169;&#20026;&#26399;&#26395;&#31119;&#21033;&#26368;&#22823;&#21270;&#38382;&#39064;&#65292;&#38024;&#23545;&#21521;&#37327;&#30340;&#38271;&#26399;&#32047;&#31215;&#22870;&#21169;&#30340;&#38750;&#32447;&#24615;&#20844;&#24179;&#31119;&#21033;&#20989;&#25968;&#12290;&#20854;&#20013;&#19968;&#20010;&#32463;&#20856;&#30340;&#20363;&#23376;&#26159;&#32435;&#20160;&#31038;&#20250;&#31119;&#21033;&#20989;&#25968;&#65292;&#25110;&#32773;&#20960;&#20309;&#24179;&#22343;&#25968;&#65292;&#20854;&#23545;&#25968;&#21464;&#25442;&#20063;&#34987;&#31216;&#20026;&#27604;&#20363;&#20844;&#24179;&#30446;&#26631;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#21363;&#20351;&#22312;&#34920;&#26684;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#26399;&#26395;&#32435;&#20160;&#31038;&#20250;&#31119;&#21033;&#36827;&#34892;&#36817;&#20284;&#26368;&#20248;&#21270;&#20063;&#26159;&#35745;&#31639;&#19978;&#38590;&#20197;&#22788;&#29702;&#30340;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;Q-learning&#25913;&#36827;&#26041;&#27861;&#65292;&#32467;&#21512;&#38750;&#32447;&#24615;&#26631;&#37327;&#21270;&#23398;&#20064;&#26356;&#26032;&#21644;&#38750;&#31283;&#24577;&#21160;&#20316;&#36873;&#25321;&#65292;&#20197;&#23398;&#20064;&#26377;&#25928;&#30340;&#20248;&#21270;&#38750;&#32447;&#24615;&#31119;&#21033;&#20989;&#25968;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#26159;&#21487;&#25910;&#25947;&#30340;&#65292;&#24182;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study fair multi-objective reinforcement learning in which an agent must learn a policy that simultaneously achieves high reward on multiple dimensions of a vector-valued reward. Motivated by the fair resource allocation literature, we model this as an expected welfare maximization problem, for some non-linear fair welfare function of the vector of long-term cumulative rewards. One canonical example of such a function is the Nash Social Welfare, or geometric mean, the log transform of which is also known as the Proportional Fairness objective. We show that even approximately optimal optimization of the expected Nash Social Welfare is computationally intractable even in the tabular case. Nevertheless, we provide a novel adaptation of Q-learning that combines non-linear scalarized learning updates and non-stationary action selection to learn effective policies for optimizing nonlinear welfare functions. We show that our algorithm is provably convergent, and we demonstrate experimental
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#30740;&#31350;&#20102;&#22270;&#24418;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#27010;&#24565;&#12289;&#26041;&#27861;&#12289;&#35780;&#20272;&#21450;&#20854;&#24212;&#29992;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24773;&#20917;&#65292;&#25552;&#20379;&#20102;&#20998;&#31867;&#27861;&#12289;&#32479;&#19968;&#30340;&#31526;&#21495;&#34920;&#31034;&#12289;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#24182;&#23545;&#21313;&#22235;&#31181;&#26041;&#27861;&#12289;&#20108;&#21313;&#20108;&#20010;&#25968;&#25454;&#38598;&#21644;&#21313;&#20061;&#20010;&#25351;&#26631;&#36827;&#34892;&#20102;&#35752;&#35770;&#21644;&#25972;&#21512;&#12290;&#26410;&#26469;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#35299;&#20915;&#24320;&#25918;&#30340;&#25361;&#25112;&#19978;&#12290;</title><link>http://arxiv.org/abs/2210.12089</link><description>&lt;p&gt;
&#22270;&#24418;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#32508;&#36848;: &#23450;&#20041;, &#26041;&#27861;, &#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Survey on Graph Counterfactual Explanations: Definitions, Methods, Evaluation. (arXiv:2210.12089v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.12089
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#30740;&#31350;&#20102;&#22270;&#24418;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#27010;&#24565;&#12289;&#26041;&#27861;&#12289;&#35780;&#20272;&#21450;&#20854;&#24212;&#29992;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24773;&#20917;&#65292;&#25552;&#20379;&#20102;&#20998;&#31867;&#27861;&#12289;&#32479;&#19968;&#30340;&#31526;&#21495;&#34920;&#31034;&#12289;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#24182;&#23545;&#21313;&#22235;&#31181;&#26041;&#27861;&#12289;&#20108;&#21313;&#20108;&#20010;&#25968;&#25454;&#38598;&#21644;&#21313;&#20061;&#20010;&#25351;&#26631;&#36827;&#34892;&#20102;&#35752;&#35770;&#21644;&#25972;&#21512;&#12290;&#26410;&#26469;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#35299;&#20915;&#24320;&#25918;&#30340;&#25361;&#25112;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476; (GNNs) &#22312;&#31038;&#21306;&#26816;&#27979;&#21644;&#20998;&#23376;&#20998;&#31867;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#21453;&#20107;&#23454;&#35299;&#37322; (CE) &#25552;&#20379;&#21453;&#20363;&#26469;&#20811;&#26381;&#40657;&#30418;&#27169;&#22411;&#30340;&#36879;&#26126;&#24230;&#38480;&#21046;&#12290;&#30001;&#20110;&#23545;&#22270;&#23398;&#20064;&#30340;&#20851;&#27880;&#19981;&#26029;&#22686;&#38271;&#65292;&#25105;&#20204;&#23558;&#37325;&#28857;&#20851;&#27880; GNNs &#30340; CE &#27010;&#24565;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#38750;&#24120;&#35268;&#30340;&#25163;&#27573;&#65292;&#25552;&#20379;&#20102;&#20998;&#31867;&#27861;&#65292;&#32479;&#19968;&#30340;&#31526;&#21495;&#34920;&#31034;&#65292;&#20197;&#21450;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#21313;&#22235;&#31181;&#26041;&#27861;&#65292;&#23427;&#20204;&#30340;&#35780;&#20272;&#21327;&#35758;&#65292;&#20108;&#21313;&#20108;&#20010;&#25968;&#25454;&#38598;&#21644;&#21313;&#20061;&#20010;&#25351;&#26631;&#12290;&#25105;&#20204;&#25972;&#21512;&#20102;&#22823;&#22810;&#25968;&#26041;&#27861;&#21040; GRETEL &#24211;&#20013;&#65292;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#65292;&#20197;&#20102;&#35299;&#23427;&#20204;&#30340;&#20248;&#21183;&#21644;&#32570;&#28857;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;&#24320;&#25918;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) perform well in community detection and molecule classification. Counterfactual Explanations (CE) provide counter-examples to overcome the transparency limitations of black-box models. Due to the growing attention in graph learning, we focus on the concepts of CE for GNNs. We analysed the SoA to provide a taxonomy, a uniform notation, and the benchmarking datasets and evaluation metrics. We discuss fourteen methods, their evaluation protocols, twenty-two datasets, and nineteen metrics. We integrated the majority of methods into the GRETEL library to conduct an empirical evaluation to understand their strengths and pitfalls. We highlight open challenges and future work.
&lt;/p&gt;</description></item><item><title>GLM-130B&#26159;&#19968;&#20010;&#20855;&#26377;1300&#20159;&#21442;&#25968;&#30340;&#24320;&#28304;&#21452;&#35821;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#33021;&#22815;&#36229;&#36234;GPT-3&#21644;&#26368;&#22823;&#30340;&#20013;&#25991;&#35821;&#35328;&#27169;&#22411;ERNIE TITAN 3.0 260B&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2210.02414</link><description>&lt;p&gt;
GLM-130B: &#19968;&#20010;&#24320;&#28304;&#30340;&#21452;&#35821;&#39044;&#35757;&#32451;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
GLM-130B: An Open Bilingual Pre-trained Model. (arXiv:2210.02414v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.02414
&lt;/p&gt;
&lt;p&gt;
GLM-130B&#26159;&#19968;&#20010;&#20855;&#26377;1300&#20159;&#21442;&#25968;&#30340;&#24320;&#28304;&#21452;&#35821;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#33021;&#22815;&#36229;&#36234;GPT-3&#21644;&#26368;&#22823;&#30340;&#20013;&#25991;&#35821;&#35328;&#27169;&#22411;ERNIE TITAN 3.0 260B&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;GLM-130B&#65292;&#19968;&#20010;&#20855;&#26377;1300&#20159;&#21442;&#25968;&#30340;&#21452;&#35821;&#65288;&#33521;&#35821;&#21644;&#20013;&#25991;&#65289;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#12290;&#23427;&#26159;&#20026;&#20102;&#25171;&#24320;1000&#20159;&#35268;&#27169;&#30340;&#27169;&#22411;&#65292;&#33267;&#23569;&#19982;GPT-3&#65288;davinci&#65289;&#19968;&#26679;&#22909;&#65292;&#24182;&#25581;&#31034;&#22914;&#20309;&#25104;&#21151;&#22320;&#36827;&#34892;&#22914;&#27492;&#35268;&#27169;&#30340;&#39044;&#35757;&#32451;&#12290;&#22312;&#36825;&#20010;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#36935;&#21040;&#20102;&#35768;&#22810;&#24847;&#22806;&#30340;&#25216;&#26415;&#21644;&#24037;&#31243;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#25439;&#22833;&#23792;&#21644;&#21457;&#25955;&#26041;&#38754;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;GLM-130B&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#21253;&#25324;&#35774;&#35745;&#36873;&#25321;&#12289;&#25552;&#39640;&#25928;&#29575;&#21644;&#31283;&#23450;&#24615;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#20197;&#21450;&#24037;&#31243;&#21162;&#21147;&#12290;GLM-130B&#27169;&#22411;&#22312;&#24191;&#27867;&#30340;&#33521;&#35821;&#22522;&#20934;&#27979;&#35797;&#20013;&#26126;&#26174;&#20248;&#20110;GPT-3 175B&#65288;davinci&#65289;&#65292;&#20294;&#22312;OPT-175B&#21644;BLOOM-176B&#20013;&#27809;&#26377;&#35266;&#23519;&#21040;&#24615;&#33021;&#20248;&#21183;&#12290;&#23427;&#36824;&#22312;&#30456;&#20851;&#22522;&#20934;&#27979;&#35797;&#20013;&#22987;&#32456;&#19988;&#26174;&#33879;&#20248;&#20110;&#26368;&#22823;&#30340;&#20013;&#25991;&#35821;&#35328;&#27169;&#22411;ERNIE TITAN 3.0 260B&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;GLM-130B&#30340;&#29420;&#29305;&#32553;&#25918;&#24615;&#33021;&#36827;&#34892;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce GLM-130B, a bilingual (English and Chinese) pre-trained language model with 130 billion parameters. It is an attempt to open-source a 100B-scale model at least as good as GPT-3 (davinci) and unveil how models of such a scale can be successfully pre-trained. Over the course of this effort, we face numerous unexpected technical and engineering challenges, particularly on loss spikes and divergence. In this paper, we introduce the training process of GLM-130B including its design choices, training strategies for both efficiency and stability, and engineering efforts. The resultant GLM-130B model offers significant outperformance over GPT-3 175B (davinci) on a wide range of popular English benchmarks while the performance advantage is not observed in OPT-175B and BLOOM-176B. It also consistently and significantly outperforms ERNIE TITAN 3.0 260B -- the largest Chinese language model -- across related benchmarks. Finally, we leverage a unique scaling property of GLM-130B to rea
&lt;/p&gt;</description></item><item><title>&#38544;&#24335;&#21452;&#22612;&#31574;&#30053;&#65288;ITT&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#32467;&#26500;&#21270;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#20307;&#31995;&#65292;&#36890;&#36807;&#22312;&#31574;&#30053;&#22534;&#26632;&#20013;&#26174;&#24335;&#21306;&#20998;&#21160;&#20316;&#21644;&#29366;&#24577;&#22788;&#29702;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#35745;&#31639;&#25928;&#30410;&#21644;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#22312;&#40657;&#30418;/&#36827;&#21270;&#20248;&#21270;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2208.01191</link><description>&lt;p&gt;
&#38544;&#24335;&#21452;&#22612;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Implicit Two-Tower Policies. (arXiv:2208.01191v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.01191
&lt;/p&gt;
&lt;p&gt;
&#38544;&#24335;&#21452;&#22612;&#31574;&#30053;&#65288;ITT&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#32467;&#26500;&#21270;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#20307;&#31995;&#65292;&#36890;&#36807;&#22312;&#31574;&#30053;&#22534;&#26632;&#20013;&#26174;&#24335;&#21306;&#20998;&#21160;&#20316;&#21644;&#29366;&#24577;&#22788;&#29702;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#35745;&#31639;&#25928;&#30410;&#21644;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#22312;&#40657;&#30418;/&#36827;&#21270;&#20248;&#21270;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32467;&#26500;&#21270;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#20307;&#31995;&#65292;&#21363;&#38544;&#24335;&#21452;&#22612;&#65288;ITT&#65289;&#31574;&#30053;&#65292;&#20854;&#20013;&#21160;&#20316;&#22522;&#20110;&#20854;&#21487;&#23398;&#20064;&#30340;&#28508;&#22312;&#34920;&#31034;&#19982;&#36755;&#20837;&#29366;&#24577;&#30340;&#27880;&#24847;&#21147;&#20998;&#25968;&#36827;&#34892;&#36873;&#25321;&#12290;&#36890;&#36807;&#22312;&#31574;&#30053;&#22534;&#26632;&#20013;&#26174;&#24335;&#21306;&#20998;&#21160;&#20316;&#21644;&#29366;&#24577;&#22788;&#29702;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#20004;&#20010;&#20027;&#35201;&#30446;&#26631;&#65306;&#26174;&#33879;&#30340;&#35745;&#31639;&#25928;&#30410;&#21644;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26550;&#26500;&#36866;&#29992;&#20110;&#31163;&#25955;&#21644;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#12290;&#36890;&#36807;&#22312;OpenAI Gym&#21644;DeepMind Control Suite&#30340;15&#20010;&#29615;&#22659;&#19978;&#36827;&#34892;&#27979;&#35797;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;ITT&#26550;&#26500;&#29305;&#21035;&#36866;&#29992;&#20110;&#40657;&#30418;/&#36827;&#21270;&#20248;&#21270;&#65292;&#30456;&#24212;&#30340;&#31574;&#30053;&#35757;&#32451;&#31639;&#27861;&#20248;&#20110;&#20854;&#33609;&#29575;&#30340;&#38544;&#24335;&#23545;&#24212;&#29289;&#20197;&#21450;&#24120;&#29992;&#30340;&#26174;&#24335;&#31574;&#30053;&#12290;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#22914;&#20309;&#24212;&#29992;&#21704;&#24076;&#21644;&#24816;&#24615;&#22612;&#26356;&#26032;&#31561;&#25216;&#26415;&#65292;&#20851;&#38190;&#20381;&#36182;&#20110;ITT&#30340;&#21452;&#22612;&#32467;&#26500;&#65292;&#26469;&#34917;&#20805;&#25105;&#20204;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new class of structured reinforcement learning policy-architectures, Implicit Two-Tower (ITT) policies, where the actions are chosen based on the attention scores of their learnable latent representations with those of the input states. By explicitly disentangling action from state processing in the policy stack, we achieve two main goals: substantial computational gains and better performance. Our architectures are compatible with both: discrete and continuous action spaces. By conducting tests on 15 environments from OpenAI Gym and DeepMind Control Suite, we show that ITT-architectures are particularly suited for blackbox/evolutionary optimization and the corresponding policy training algorithms outperform their vanilla unstructured implicit counterparts as well as commonly used explicit policies. We complement our analysis by showing how techniques such as hashing and lazy tower updates, critically relying on the two-tower structure of ITTs, can be applied to obtain add
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#22788;&#29702;&#35884;&#35823;&#30340;&#20027;&#39064;-&#26041;&#38754;-&#35770;&#35777;&#27169;&#22411;&#65292;&#36890;&#36807;&#24418;&#24335;&#32422;&#26463;&#26469;&#34920;&#24449;&#35884;&#35823;&#65292;&#25552;&#20986;&#20102;&#21487;&#35299;&#37322;&#30340;&#35884;&#35823;&#35782;&#21035;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#33021;&#36827;&#34892;&#20462;&#36766;&#24314;&#27169;&#21644;&#28145;&#23618;&#35821;&#20041;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2205.15141</link><description>&lt;p&gt;
&#22788;&#29702;&#35884;&#35823;&#30340;&#20027;&#39064;-&#26041;&#38754;-&#35770;&#35777;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Theme Aspect Argumentation Model for Handling Fallacies. (arXiv:2205.15141v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.15141
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#22788;&#29702;&#35884;&#35823;&#30340;&#20027;&#39064;-&#26041;&#38754;-&#35770;&#35777;&#27169;&#22411;&#65292;&#36890;&#36807;&#24418;&#24335;&#32422;&#26463;&#26469;&#34920;&#24449;&#35884;&#35823;&#65292;&#25552;&#20986;&#20102;&#21487;&#35299;&#37322;&#30340;&#35884;&#35823;&#35782;&#21035;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#33021;&#36827;&#34892;&#20462;&#36766;&#24314;&#27169;&#21644;&#28145;&#23618;&#35821;&#20041;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#26085;&#24120;&#35752;&#35770;&#21040;&#33829;&#38144;&#24191;&#21578;&#20877;&#21040;&#25919;&#27835;&#35328;&#35770;&#65292;&#20449;&#24687;&#25805;&#20316;&#26080;&#22788;&#19981;&#22312;&#12290;&#22240;&#27492;&#65292;&#25317;&#26377;&#19968;&#22871;&#27491;&#30830;&#30340;&#24037;&#20855;&#26469;&#25269;&#24481;&#25805;&#32437;&#24615;&#35328;&#36766;&#25110;&#35884;&#35823;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#33258;&#21160;&#35782;&#21035;&#35884;&#35823;&#30340;&#25216;&#26415;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#20013;&#27491;&#22312;&#34987;&#25506;&#32034;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#19978;&#19979;&#25991;&#20013;&#30340;&#35884;&#35823;&#22312;&#21478;&#19968;&#20010;&#19978;&#19979;&#25991;&#20013;&#21487;&#33021;&#19981;&#26159;&#35884;&#35823;&#65292;&#22240;&#27492;&#26377;&#24517;&#35201;&#35299;&#37322;&#23427;&#34987;&#21028;&#26029;&#20026;&#35884;&#35823;&#30340;&#26041;&#24335;&#21644;&#21407;&#22240;&#12290;&#20026;&#20102;&#21487;&#35299;&#37322;&#30340;&#35884;&#35823;&#35782;&#21035;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24418;&#24335;&#32422;&#26463;&#26469;&#34920;&#24449;&#35884;&#35823;&#30340;&#26032;&#26041;&#27861;&#65292;&#20316;&#20026;&#20256;&#32479;&#35884;&#35823;&#20998;&#31867;&#30340;&#21487;&#34892;&#26367;&#20195;&#26041;&#27861;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#35770;&#35777;&#27169;&#22411;&#65292;&#21363;&#20027;&#39064;-&#26041;&#38754;-&#35770;&#35777;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#23454;&#29616;&#20004;&#20010;&#21151;&#33021;&#65306;&#23545;&#32473;&#23450;&#30340;&#35770;&#35777;&#36827;&#34892;&#24314;&#27169;&#65288;&#20462;&#36766;&#24314;&#27169;&#65289;&#20197;&#21450;&#26356;&#28145;&#20837;&#30340;&#35821;&#20041;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
From daily discussions to marketing ads to political statements, information manipulation is rife. It is increasingly more important that we have the right set of tools to defend ourselves from manipulative rhetoric, or fallacies. Suitable techniques to automatically identify fallacies are being investigated in natural language processing research. However, a fallacy in one context may not be a fallacy in another context, so there is also a need to explain how and why it has come to be judged a fallacy. For the explainable fallacy identification, we present a novel approach to characterising fallacies through formal constraints, as a viable alternative to more traditional fallacy classifications by informal criteria. To achieve this objective, we introduce a novel context-aware argumentation model, the theme aspect argumentation model, which can do both: the modelling of a given argumentation as it is expressed (rhetorical modelling); and a deeper semantic analysis of the rhetorical ar
&lt;/p&gt;</description></item><item><title>AlphaZero-style reinforcement learning algorithms excel in various board games but face challenges with impartial games. The researchers present a concrete example of the game nim, and show that AlphaZero-style algorithms have difficulty learning these impartial games on larger board sizes. The difference between impartial games and partisan games can be explained by the vulnerability to adversarial attacks and perturbations.</title><link>http://arxiv.org/abs/2205.12787</link><description>&lt;p&gt;
&#20844;&#27491;&#28216;&#25103;&#65306;&#23545;&#24378;&#21270;&#23398;&#20064;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Impartial Games: A Challenge for Reinforcement Learning. (arXiv:2205.12787v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.12787
&lt;/p&gt;
&lt;p&gt;
AlphaZero-style reinforcement learning algorithms excel in various board games but face challenges with impartial games. The researchers present a concrete example of the game nim, and show that AlphaZero-style algorithms have difficulty learning these impartial games on larger board sizes. The difference between impartial games and partisan games can be explained by the vulnerability to adversarial attacks and perturbations.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#20284;AlphaZero&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#21508;&#31181;&#26827;&#30424;&#28216;&#25103;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#20844;&#27491;&#28216;&#25103;&#20013;&#21364;&#38754;&#20020;&#25361;&#25112;&#65292;&#36825;&#20123;&#28216;&#25103;&#20013;&#29609;&#23478;&#20849;&#20139;&#26827;&#23376;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20855;&#20307;&#30340;&#28216;&#25103;&#20363;&#23376;&#65292;&#21363;&#23567;&#23401;&#20204;&#29609;&#30340;&#23612;&#22982;&#28216;&#25103;&#65292;&#20197;&#21450;&#20854;&#20182;&#19968;&#20123;&#20844;&#27491;&#28216;&#25103;&#65292;&#36825;&#20123;&#28216;&#25103;&#20284;&#20046;&#25104;&#20026;AlphaZero&#21644;&#31867;&#20284;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#32458;&#33050;&#30707;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#19982;&#26368;&#36817;&#30340;&#30740;&#31350;&#19968;&#33268;&#65292;&#34920;&#26126;AlphaZero-style&#31639;&#27861;&#23481;&#26131;&#21463;&#21040;&#25932;&#23545;&#25915;&#20987;&#21644;&#25932;&#23545;&#25200;&#21160;&#30340;&#24433;&#21709;&#65292;&#26174;&#31034;&#20102;&#22312;&#25152;&#26377;&#21512;&#27861;&#29366;&#24577;&#19979;&#23398;&#20064;&#25484;&#25569;&#36825;&#20123;&#28216;&#25103;&#30340;&#22256;&#38590;&#12290;&#25105;&#20204;&#21457;&#29616;&#23612;&#22982;&#28216;&#25103;&#22312;&#23567;&#22411;&#26827;&#30424;&#19978;&#21487;&#20197;&#23398;&#20064;&#65292;&#20294;&#24403;&#26827;&#30424;&#23610;&#23544;&#22686;&#22823;&#26102;&#65292;AlphaZero-style&#31639;&#27861;&#30340;&#23398;&#20064;&#36895;&#24230;&#26174;&#33879;&#20943;&#24930;&#12290;&#30452;&#35266;&#19978;&#65292;&#23612;&#22982;&#31561;&#20844;&#27491;&#28216;&#25103;&#19982;&#35937;&#26827;&#21644;&#22260;&#26827;&#31561;&#20826;&#27966;&#28216;&#25103;&#20043;&#38388;&#30340;&#21306;&#21035;&#22312;&#20110;&#65292;&#22914;&#26524;&#31995;&#32479;&#20013;&#28155;&#21152;&#20102;&#24494;&#23567;&#30340;&#22122;&#38899;&#65288;&#20363;&#22914;&#65292;&#26827;&#30424;&#30340;&#19968;&#23567;&#37096;&#20998;&#34987;&#35206;&#30422;&#65289;&#65292;&#23545;&#20110;&#20844;&#27491;&#28216;&#25103;&#26469;&#35828;&#65292;&#36825;&#26159;&#19968;&#31181;&#20856;&#22411;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
AlphaZero-style reinforcement learning (RL) algorithms excel in various board games but face challenges with impartial games, where players share pieces. We present a concrete example of a game - namely the children's game of nim - and other impartial games that seem to be a stumbling block for AlphaZero-style and similar reinforcement learning algorithms.  Our findings are consistent with recent studies showing that AlphaZero-style algorithms are vulnerable to adversarial attacks and adversarial perturbations, showing the difficulty of learning to master the games in all legal states.  We show that nim can be learned on small boards, but AlphaZero-style algorithms learning dramatically slows down when the board size increases. Intuitively, the difference between impartial games like nim and partisan games like Chess and Go can be explained by the fact that if a tiny amount of noise is added to the system (e.g. if a small part of the board is covered), for impartial games, it is typica
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#25968;&#23383;&#30828;&#20214;&#19978;&#35299;&#20915;&#21453;&#38382;&#39064;&#30340;&#38480;&#21046;&#65292;&#24182;&#35777;&#26126;&#20102;&#23545;&#20110;&#23567;&#30340;&#26494;&#24347;&#21442;&#25968;&#65292;&#26377;&#38480;&#32500;&#21453;&#38382;&#39064;&#26080;&#27861;&#36890;&#36807;&#35745;&#31639;&#26041;&#27861;&#35299;&#20915;&#12290;&#36825;&#20123;&#32467;&#26524;&#36824;&#24341;&#20837;&#20102;&#31639;&#27861;&#21487;&#33719;&#24471;&#20934;&#30830;&#24230;&#30340;&#19979;&#38480;&#12290;</title><link>http://arxiv.org/abs/2202.13490</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#25968;&#23383;&#30828;&#20214;&#19978;&#30340;&#21453;&#38382;&#39064;&#23384;&#22312;&#30340;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Limitations of Deep Learning for Inverse Problems on Digital Hardware. (arXiv:2202.13490v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.13490
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#25968;&#23383;&#30828;&#20214;&#19978;&#35299;&#20915;&#21453;&#38382;&#39064;&#30340;&#38480;&#21046;&#65292;&#24182;&#35777;&#26126;&#20102;&#23545;&#20110;&#23567;&#30340;&#26494;&#24347;&#21442;&#25968;&#65292;&#26377;&#38480;&#32500;&#21453;&#38382;&#39064;&#26080;&#27861;&#36890;&#36807;&#35745;&#31639;&#26041;&#27861;&#35299;&#20915;&#12290;&#36825;&#20123;&#32467;&#26524;&#36824;&#24341;&#20837;&#20102;&#31639;&#27861;&#21487;&#33719;&#24471;&#20934;&#30830;&#24230;&#30340;&#19979;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#20960;&#24180;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#12290;&#30001;&#20110;&#35757;&#32451;&#26159;&#22312;&#25968;&#23383;&#30828;&#20214;&#19978;&#36827;&#34892;&#30340;&#65292;&#26412;&#25991;&#20998;&#26512;&#20102;&#22312;&#24403;&#21069;&#27169;&#25311;&#20026;&#22270;&#28789;&#26426;&#30340;&#30828;&#20214;&#24179;&#21488;&#19978;&#23454;&#38469;&#21487;&#20197;&#35745;&#31639;&#30340;&#20869;&#23481;&#65292;&#36825;&#20250;&#23548;&#33268;&#28145;&#24230;&#23398;&#20064;&#30340;&#22266;&#26377;&#38480;&#21046;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#37325;&#28857;&#30740;&#31350;&#20102;&#21453;&#38382;&#39064;&#31867;&#65292;&#29305;&#21035;&#26159;&#28085;&#30422;&#20102;&#20174;&#27979;&#37327;&#20013;&#37325;&#26500;&#25968;&#25454;&#30340;&#20219;&#20309;&#20219;&#21153;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#23545;&#20110;&#23567;&#30340;&#26494;&#24347;&#21442;&#25968;&#65292;&#26377;&#38480;&#32500;&#21453;&#38382;&#39064;&#26080;&#27861;&#36890;&#36807;&#24052;&#25343;&#36203;-&#39532;&#20857;&#23572;&#35745;&#31639;&#26041;&#27861;&#35299;&#20915;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#24341;&#20837;&#20102;&#22312;&#31639;&#27861;&#19978;&#21487;&#20197;&#33719;&#24471;&#30340;&#20934;&#30830;&#24230;&#30340;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks have seen tremendous success over the last years. Since the training is performed on digital hardware, in this paper, we analyze what actually can be computed on current hardware platforms modeled as Turing machines, which would lead to inherent restrictions of deep learning. For this, we focus on the class of inverse problems, which, in particular, encompasses any task to reconstruct data from measurements. We prove that finite-dimensional inverse problems are not Banach-Mazur computable for small relaxation parameters. Even more, our results introduce a lower bound on the accuracy that can be obtained algorithmically.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#35774;&#35745;&#36817;&#20284;&#20056;&#27861;&#22120;&#65292;&#24182;&#26681;&#25454;&#25805;&#20316;&#25968;&#20998;&#24067;&#26469;&#26368;&#23567;&#21270;&#24179;&#22343;&#35823;&#24046;&#12290;&#25152;&#25552;&#20056;&#27861;&#22120;&#22312;DNN&#20013;&#36798;&#21040;&#20102;&#27604;&#26368;&#20339;&#22797;&#21046;&#30340;&#36817;&#20284;&#20056;&#27861;&#22120;&#39640;&#36798;50.24%&#30340;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#20855;&#26377;&#36739;&#23567;&#30340;&#38754;&#31215;&#12289;&#21151;&#32791;&#21644;&#24310;&#36831;&#12290;</title><link>http://arxiv.org/abs/2201.08022</link><description>&lt;p&gt;
HEAM: &#39640;&#25928;&#36817;&#20284;&#20056;&#27861;&#22120;&#20248;&#21270;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
HEAM: High-Efficiency Approximate Multiplier Optimization for Deep Neural Networks. (arXiv:2201.08022v4 [cs.AR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.08022
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#35774;&#35745;&#36817;&#20284;&#20056;&#27861;&#22120;&#65292;&#24182;&#26681;&#25454;&#25805;&#20316;&#25968;&#20998;&#24067;&#26469;&#26368;&#23567;&#21270;&#24179;&#22343;&#35823;&#24046;&#12290;&#25152;&#25552;&#20056;&#27861;&#22120;&#22312;DNN&#20013;&#36798;&#21040;&#20102;&#27604;&#26368;&#20339;&#22797;&#21046;&#30340;&#36817;&#20284;&#20056;&#27861;&#22120;&#39640;&#36798;50.24%&#30340;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#20855;&#26377;&#36739;&#23567;&#30340;&#38754;&#31215;&#12289;&#21151;&#32791;&#21644;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#35774;&#35745;&#36817;&#20284;&#20056;&#27861;&#22120;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#26681;&#25454;&#25805;&#20316;&#25968;&#20998;&#24067;&#26368;&#23567;&#21270;&#24179;&#22343;&#35823;&#24046;&#12290;&#25105;&#20204;&#30340;&#20056;&#27861;&#22120;&#22312;DNN&#20013;&#27604;&#26368;&#20339;&#22797;&#21046;&#30340;&#36817;&#20284;&#20056;&#27861;&#22120;&#39640;&#36798;50.24%&#30340;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#38754;&#31215;&#20943;&#23567;15.76%&#65292;&#21151;&#32791;&#20943;&#23569;25.05%&#65292;&#24310;&#36831;&#32553;&#30701;3.50%&#12290;&#19982;&#31934;&#30830;&#20056;&#27861;&#22120;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#20056;&#27861;&#22120;&#20998;&#21035;&#20943;&#23569;&#20102;44.94%&#30340;&#38754;&#31215;&#12289;47.63%&#30340;&#21151;&#32791;&#21644;16.78%&#30340;&#24310;&#36831;&#65292;&#20960;&#20046;&#27809;&#26377;&#20934;&#30830;&#24615;&#25439;&#22833;&#12290;&#20351;&#29992;&#25105;&#20204;&#30340;&#20056;&#27861;&#22120;&#36827;&#34892;&#27979;&#35797;&#30340;DNN&#21152;&#36895;&#22120;&#27169;&#22359;&#27604;&#21407;&#22987;&#27169;&#22359;&#38754;&#31215;&#20943;&#23567;&#20102;18.70%&#65292;&#21151;&#32791;&#20943;&#23569;&#20102;9.99%&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an optimization method for the automatic design of approximate multipliers, which minimizes the average error according to the operand distributions. Our multiplier achieves up to 50.24% higher accuracy than the best reproduced approximate multiplier in DNNs, with 15.76% smaller area, 25.05% less power consumption, and 3.50% shorter delay. Compared with an exact multiplier, our multiplier reduces the area, power consumption, and delay by 44.94%, 47.63%, and 16.78%, respectively, with negligible accuracy losses. The tested DNN accelerator modules with our multiplier obtain up to 18.70% smaller area and 9.99% less power consumption than the original modules.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34987;&#31216;&#20026;&#21069;&#21521;&#32452;&#21512;&#20256;&#25773;&#65288;FCP&#65289;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#22312;&#32467;&#26500;&#21270;&#20998;&#31867;&#38382;&#39064;&#19978;&#30340;&#39044;&#27979;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#32452;&#21512;&#21521;&#37327;&#25551;&#36848;&#27599;&#20010;&#31070;&#32463;&#20803;&#20013;&#38382;&#39064;&#29305;&#24449;&#30340;&#20316;&#29992;&#65292;&#24182;&#19988;&#27169;&#25311;&#32467;&#26524;&#34920;&#26126;&#32452;&#21512;&#20540;&#19982;&#21463;&#20445;&#25252;&#29305;&#24449;&#30340;&#39044;&#26399;&#34892;&#20026;&#32039;&#23494;&#23545;&#40784;&#12290;</title><link>http://arxiv.org/abs/2112.12717</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#31070;&#32463;&#25512;&#29702;&#30340;&#21069;&#21521;&#32452;&#21512;&#20256;&#25773;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Forward Composition Propagation for Explainable Neural Reasoning. (arXiv:2112.12717v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.12717
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34987;&#31216;&#20026;&#21069;&#21521;&#32452;&#21512;&#20256;&#25773;&#65288;FCP&#65289;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#22312;&#32467;&#26500;&#21270;&#20998;&#31867;&#38382;&#39064;&#19978;&#30340;&#39044;&#27979;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#32452;&#21512;&#21521;&#37327;&#25551;&#36848;&#27599;&#20010;&#31070;&#32463;&#20803;&#20013;&#38382;&#39064;&#29305;&#24449;&#30340;&#20316;&#29992;&#65292;&#24182;&#19988;&#27169;&#25311;&#32467;&#26524;&#34920;&#26126;&#32452;&#21512;&#20540;&#19982;&#21463;&#20445;&#25252;&#29305;&#24449;&#30340;&#39044;&#26399;&#34892;&#20026;&#32039;&#23494;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#31216;&#20026;&#21069;&#21521;&#32452;&#21512;&#20256;&#25773;&#65288;FCP&#65289;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;&#22312;&#32467;&#26500;&#21270;&#20998;&#31867;&#38382;&#39064;&#19978;&#25805;&#20316;&#30340;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#27979;&#12290;&#22312;&#25552;&#20986;&#30340;FCP&#31639;&#27861;&#20013;&#65292;&#27599;&#20010;&#31070;&#32463;&#20803;&#37117;&#30001;&#19968;&#20010;&#32452;&#21512;&#21521;&#37327;&#25551;&#36848;&#65292;&#35813;&#21521;&#37327;&#25351;&#31034;&#20102;&#27599;&#20010;&#38382;&#39064;&#29305;&#24449;&#22312;&#35813;&#31070;&#32463;&#20803;&#20013;&#30340;&#20316;&#29992;&#12290;&#32452;&#21512;&#21521;&#37327;&#20351;&#29992;&#32473;&#23450;&#30340;&#36755;&#20837;&#23454;&#20363;&#21021;&#22987;&#21270;&#65292;&#24182;&#38543;&#21518;&#36890;&#36807;&#25972;&#20010;&#32593;&#32476;&#20256;&#25773;&#65292;&#30452;&#21040;&#36798;&#21040;&#36755;&#20986;&#23618;&#12290;&#27599;&#20010;&#32452;&#21512;&#20540;&#30340;&#31526;&#21495;&#34920;&#31034;&#30456;&#24212;&#29305;&#24449;&#26159;&#21542;&#28608;&#27963;&#25110;&#25233;&#21046;&#31070;&#32463;&#20803;&#65292;&#32780;&#32477;&#23545;&#20540; quantifies &#20102;&#20854;&#24433;&#21709;&#12290;FCP&#31639;&#27861;&#26159;&#22312;&#21518;&#32493;&#22522;&#30784;&#19978;&#25191;&#34892;&#30340;&#65292;&#21363;&#22312;&#23398;&#20064;&#36807;&#31243;&#23436;&#25104;&#21518;&#12290;&#20026;&#20102;&#35828;&#26126;FCP&#31639;&#27861;&#65292;&#26412;&#25991;&#24320;&#23637;&#20102;&#19968;&#20010;&#20851;&#20110;&#20844;&#24179;&#38382;&#39064;&#20013;&#20559;&#35265;&#26816;&#27979;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#20854;&#20013;&#22320;&#38754;&#30495;&#30456;&#26159;&#24050;&#30693;&#30340;&#12290;&#27169;&#25311;&#32467;&#26524;&#34920;&#26126;&#65292;&#32452;&#21512;&#20540;&#19982;&#21463;&#20445;&#25252;&#29305;&#24449;&#30340;&#39044;&#26399;&#34892;&#20026;&#32039;&#23494;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an algorithm called Forward Composition Propagation (FCP) to explain the predictions of feed-forward neural networks operating on structured classification problems. In the proposed FCP algorithm, each neuron is described by a composition vector indicating the role of each problem feature in that neuron. Composition vectors are initialized using a given input instance and subsequently propagated through the whole network until reaching the output layer. The sign of each composition value indicates whether the corresponding feature excites or inhibits the neuron, while the absolute value quantifies its impact. The FCP algorithm is executed on a post-hoc basis, i.e., once the learning process is completed. Aiming to illustrate the FCP algorithm, this paper develops a case study concerning bias detection in a fairness problem in which the ground truth is known. The simulation results show that the composition values closely align with the expected behavior of protected
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#23545;&#20302;&#26679;&#26412;&#29289;&#20307;&#26816;&#27979;&#20013;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20102;&#32508;&#21512;&#22238;&#39038;&#65292;&#25552;&#20986;&#20102;LSOD&#26041;&#27861;&#30340;&#20998;&#31867;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#31995;&#32479;&#20998;&#26512;&#65292;&#20026;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#22330;&#26223;&#19979;&#30340;&#29289;&#20307;&#26816;&#27979;&#38382;&#39064;&#25552;&#20379;&#20102;&#21442;&#32771;&#12290;</title><link>http://arxiv.org/abs/2112.02814</link><description>&lt;p&gt;
&#23545;&#20302;&#26679;&#26412;&#29289;&#20307;&#26816;&#27979;&#20013;&#28145;&#24230;&#23398;&#20064;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey of Deep Learning for Low-Shot Object Detection. (arXiv:2112.02814v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.02814
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#23545;&#20302;&#26679;&#26412;&#29289;&#20307;&#26816;&#27979;&#20013;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20102;&#32508;&#21512;&#22238;&#39038;&#65292;&#25552;&#20986;&#20102;LSOD&#26041;&#27861;&#30340;&#20998;&#31867;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#31995;&#32479;&#20998;&#26512;&#65292;&#20026;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#22330;&#26223;&#19979;&#30340;&#29289;&#20307;&#26816;&#27979;&#38382;&#39064;&#25552;&#20379;&#20102;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#22823;&#35268;&#27169;&#26631;&#27880;&#25968;&#25454;&#30340;&#20986;&#29616;&#65292;&#29289;&#20307;&#26816;&#27979;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#31361;&#30772;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20005;&#37325;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#24403;&#21069;&#30340;&#26816;&#27979;&#26041;&#27861;&#19981;&#33021;&#30452;&#25509;&#24212;&#29992;&#20110;&#26631;&#27880;&#25968;&#25454;&#31232;&#32570;&#30340;&#22330;&#26223;&#12290;&#34429;&#28982;&#22312;&#22270;&#20687;&#20998;&#31867;&#39046;&#22495;&#20013;&#24191;&#27867;&#25506;&#32034;&#20102;&#23569;&#26679;&#26412;&#23398;&#20064;&#21644;&#38646;&#26679;&#26412;&#23398;&#20064;&#65292;&#20294;&#22312;&#25968;&#25454;&#31232;&#32570;&#24773;&#20917;&#19979;&#35774;&#35745;&#26032;&#30340;&#29289;&#20307;&#26816;&#27979;&#26041;&#27861;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#65292;&#22240;&#20026;&#29289;&#20307;&#26816;&#27979;&#36824;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20301;&#32622;&#23450;&#20301;&#20219;&#21153;&#12290;&#20302;&#26679;&#26412;&#29289;&#20307;&#26816;&#27979;&#65288;LSOD&#65289;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#30740;&#31350;&#35838;&#39064;&#65292;&#29992;&#20110;&#20174;&#23569;&#37327;&#29978;&#33267;&#27809;&#26377;&#26631;&#27880;&#26679;&#26412;&#20013;&#26816;&#27979;&#29289;&#20307;&#65292;&#21253;&#25324;&#19968;&#27425;&#24615;&#29289;&#20307;&#26816;&#27979;&#65288;OSOD&#65289;&#12289;&#23569;&#26679;&#26412;&#29289;&#20307;&#26816;&#27979;&#65288;FSOD&#65289;&#21644;&#38646;&#26679;&#26412;&#29289;&#20307;&#26816;&#27979;&#65288;ZSD&#65289;&#12290;&#26412;&#32508;&#36848;&#23545;LSOD&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#22238;&#39038;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LSOD&#26041;&#27861;&#30340;&#35814;&#32454;&#20998;&#31867;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#31995;&#32479;&#20998;&#26512;&#65292;&#21253;&#25324;LSOD&#30340;&#19968;&#20123;&#25193;&#23637;&#20027;&#39064;&#65288;&#21322;&#30417;&#30563;LSOD&#12289;
&lt;/p&gt;
&lt;p&gt;
Object detection has achieved a huge breakthrough with deep neural networks and massive annotated data. However, current detection methods cannot be directly transferred to the scenario where the annotated data is scarce due to the severe overfitting problem. Although few-shot learning and zero-shot learning have been extensively explored in the field of image classification, it is indispensable to design new methods for object detection in the data-scarce scenario since object detection has an additional challenging localization task. Low-Shot Object Detection (LSOD) is an emerging research topic of detecting objects from a few or even no annotated samples, consisting of One-Shot Object Detection (OSOD), Few-Shot Object Detection (FSOD) and Zero-Shot Object Detection (ZSD). This survey provides a comprehensive review of LSOD methods. First, we propose a thorough taxonomy of LSOD methods and analyze them systematically, comprising some extensional topics of LSOD (semi-supervised LSOD, 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35780;&#20272;&#20102;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#27969;&#31243;&#27169;&#22411;&#30340;&#29616;&#26377;&#25216;&#26415;&#65292;&#24182;&#25552;&#20379;&#20102;&#26410;&#26469;&#25361;&#25112;&#30340;&#25351;&#23548;&#12290;&#36890;&#36807;&#27604;&#36739;10&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#23450;&#24615;&#21644;&#23450;&#37327;&#35780;&#20272;&#65292;&#30740;&#31350;&#32773;&#20204;&#25214;&#21040;&#20102;&#35299;&#20915;&#27169;&#22411;&#25552;&#21462;&#38382;&#39064;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2110.03754</link><description>&lt;p&gt;
&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#27969;&#31243;&#65306;&#35780;&#20272;&#29616;&#26377;&#25216;&#26415;&#24182;&#20026;&#26410;&#26469;&#25361;&#25112;&#38138;&#24179;&#36947;&#36335;
&lt;/p&gt;
&lt;p&gt;
Process Extraction from Text: Benchmarking the State of the Art and Paving the Way for Future Challenges. (arXiv:2110.03754v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.03754
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35780;&#20272;&#20102;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#27969;&#31243;&#27169;&#22411;&#30340;&#29616;&#26377;&#25216;&#26415;&#65292;&#24182;&#25552;&#20379;&#20102;&#26410;&#26469;&#25361;&#25112;&#30340;&#25351;&#23548;&#12290;&#36890;&#36807;&#27604;&#36739;10&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#23450;&#24615;&#21644;&#23450;&#37327;&#35780;&#20272;&#65292;&#30740;&#31350;&#32773;&#20204;&#25214;&#21040;&#20102;&#35299;&#20915;&#27169;&#22411;&#25552;&#21462;&#38382;&#39064;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#27969;&#31243;&#27169;&#22411;&#26159;&#23558;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#27969;&#31243;&#25551;&#36848;&#20013;&#30340;&#20449;&#24687;&#36716;&#21270;&#20026;&#24418;&#24335;&#21270;&#34920;&#31034;&#65288;&#21363;&#27969;&#31243;&#27169;&#22411;&#65289;&#30340;&#38382;&#39064;&#12290;&#30446;&#21069;&#24050;&#32463;&#25552;&#20986;&#20102;&#22810;&#31181;&#33258;&#21160;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#22312;&#33539;&#22260;&#21644;&#22522;&#26412;&#20551;&#35774;&#19978;&#23384;&#22312;&#24456;&#22823;&#30340;&#24322;&#36136;&#24615;&#65292;&#21253;&#25324;&#36755;&#20837;&#12289;&#30446;&#26631;&#36755;&#20986;&#21644;&#35780;&#20272;&#25968;&#25454;&#30340;&#24046;&#24322;&#12290;&#22240;&#27492;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#33021;&#22815;&#22914;&#20309;&#26377;&#25928;&#22320;&#35299;&#20915;&#27169;&#22411;&#25552;&#21462;&#38382;&#39064;&#20197;&#21450;&#23427;&#20204;&#20043;&#38388;&#30340;&#27604;&#36739;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#27604;&#36739;&#20102;10&#31181;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#25552;&#21462;&#26041;&#27861;&#65292;&#28085;&#30422;&#20102;&#23450;&#24615;&#21644;&#23450;&#37327;&#26041;&#38754;&#30340;&#35780;&#20272;&#12290;&#23450;&#24615;&#35780;&#20272;&#27604;&#36739;&#20102;&#20027;&#35201;&#30740;&#31350;&#30340;&#20197;&#19979;&#26041;&#38754;&#65306;1.&#27599;&#31181;&#26041;&#27861;&#30340;&#20027;&#35201;&#29305;&#24449;&#65307;2.&#20174;&#36755;&#20837;&#25968;&#25454;&#20013;&#25552;&#21462;&#30340;&#27969;&#31243;&#27169;&#22411;&#20803;&#32032;&#30340;&#31867;&#22411;&#65307;3.&#29992;&#20110;&#35780;&#20272;&#25552;&#20986;&#30340;&#26694;&#26550;&#30340;&#23454;&#39564;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
The extraction of process models from text refers to the problem of turning the information contained in an unstructured textual process descriptions into a formal representation,i.e.,a process model. Several automated approaches have been proposed to tackle this problem, but they are highly heterogeneous in scope and underlying assumptions,i.e., differences in input, target output, and data used in their evaluation.As a result, it is currently unclear how well existing solutions are able to solve the model-extraction problem and how they compare to each other.We overcome this issue by comparing 10 state-of-the-art approaches for model extraction in a systematic manner, covering both qualitative and quantitative aspects.The qualitative evaluation compares the analysis of the primary studies on: 1 the main characteristics of each solution;2 the type of process model elements extracted from the input data;3 the experimental evaluation performed to evaluate the proposed framework.The resu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#39044;&#27979;&#36807;&#31243;&#30417;&#25511;&#27169;&#22411;&#30340;&#38887;&#24615;&#38382;&#39064;&#65292;&#35780;&#20272;&#20102;&#19977;&#31181;&#19981;&#21516;&#31574;&#30053;&#65292;&#24182;&#21457;&#29616;&#22686;&#37327;&#23398;&#20064;&#31639;&#27861;&#20855;&#26377;&#28508;&#21147;&#26469;&#35299;&#20915;&#39044;&#27979;&#27169;&#22411;&#30340;&#26356;&#26032;&#21644;&#21487;&#21464;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2109.03501</link><description>&lt;p&gt;
&#25105;&#35813;&#22914;&#20309;&#26356;&#26032;&#25105;&#30340;&#27169;&#22411;&#65311;&#20851;&#20110;&#39044;&#27979;&#36807;&#31243;&#30417;&#25511;&#27169;&#22411;&#23545;&#21464;&#21270;&#30340;&#38887;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
How do I update my model? On the resilience of Predictive Process Monitoring models to change. (arXiv:2109.03501v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.03501
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#39044;&#27979;&#36807;&#31243;&#30417;&#25511;&#27169;&#22411;&#30340;&#38887;&#24615;&#38382;&#39064;&#65292;&#35780;&#20272;&#20102;&#19977;&#31181;&#19981;&#21516;&#31574;&#30053;&#65292;&#24182;&#21457;&#29616;&#22686;&#37327;&#23398;&#20064;&#31639;&#27861;&#20855;&#26377;&#28508;&#21147;&#26469;&#35299;&#20915;&#39044;&#27979;&#27169;&#22411;&#30340;&#26356;&#26032;&#21644;&#21487;&#21464;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#32463;&#36807;&#20805;&#20998;&#30740;&#31350;&#30340;&#39044;&#27979;&#36807;&#31243;&#30417;&#25511;&#25216;&#26415;&#36890;&#24120;&#22522;&#20110;&#36807;&#21435;&#30340;&#27969;&#31243;&#25191;&#34892;&#26500;&#24314;&#39044;&#27979;&#27169;&#22411;&#65292;&#28982;&#21518;&#20351;&#29992;&#35813;&#27169;&#22411;&#39044;&#27979;&#26032;&#36827;&#34892;&#20013;&#26696;&#20363;&#30340;&#26410;&#26469;&#65292;&#26080;&#27861;&#36890;&#36807;&#26032;&#26696;&#20363;&#30340;&#25191;&#34892;&#26469;&#26356;&#26032;&#23427;&#12290;&#36825;&#20351;&#24471;&#39044;&#27979;&#36807;&#31243;&#30417;&#25511;&#23545;&#20110;&#22312;&#19981;&#26029;&#28436;&#21464;&#21644;/&#25110;&#38543;&#26102;&#38388;&#23637;&#29616;&#26032;&#21464;&#20307;&#34892;&#20026;&#30340;&#23454;&#38469;&#29615;&#22659;&#20013;&#30340;&#36807;&#31243;&#30340;&#21487;&#21464;&#24615;&#22826;&#36807;&#20725;&#21270;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#31574;&#30053;&#65292;&#20801;&#35768;&#21608;&#26399;&#24615;&#22320;&#37325;&#26032;&#21457;&#29616;&#25110;&#22686;&#37327;&#26500;&#24314;&#39044;&#27979;&#27169;&#22411;&#65292;&#20197;&#21033;&#29992;&#26032;&#30340;&#21487;&#29992;&#25968;&#25454;&#12290;&#35780;&#20272;&#37325;&#28857;&#25918;&#22312;&#26032;&#23398;&#20064;&#30340;&#39044;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#19978;&#65292;&#21253;&#25324;&#20934;&#30830;&#24615;&#21644;&#26102;&#38388;&#65292;&#19982;&#21407;&#22987;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#20351;&#29992;&#20102;&#19968;&#20123;&#30495;&#23454;&#21644;&#21512;&#25104;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#26126;&#30830;&#30340;&#27010;&#24565;&#28418;&#31227;&#21644;&#27809;&#26377;&#26126;&#30830;&#30340;&#27010;&#24565;&#28418;&#31227;&#12290;&#32467;&#26524;&#35777;&#26126;&#20102;&#22686;&#37327;&#23398;&#20064;&#31639;&#27861;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing well investigated Predictive Process Monitoring techniques typically construct a predictive model based on past process executions, and then use it to predict the future of new ongoing cases, without the possibility of updating it with new cases when they complete their execution. This can make Predictive Process Monitoring too rigid to deal with the variability of processes working in real environments that continuously evolve and/or exhibit new variant behaviours over time. As a solution to this problem, we evaluate the use of three different strategies that allow the periodic rediscovery or incremental construction of the predictive model so as to exploit new available data. The evaluation focuses on the performance of the new learned predictive models, in terms of accuracy and time, against the original one, and uses a number of real and synthetic datasets with and without explicit Concept Drift. The results provide an evidence of the potential of incremental learning algo
&lt;/p&gt;</description></item><item><title>&#31163;&#32447;&#29983;&#25104;&#23545;&#25239;&#27169;&#20223;&#23398;&#20064;&#20013;&#65292;&#23558;&#23398;&#20064;&#21040;&#30340;&#22870;&#21169;&#20989;&#25968;&#24378;&#21046;&#21464;&#25104;&#23616;&#37096;&#21033;&#26222;&#24076;&#33576;&#36830;&#32493;&#26159;&#21462;&#24471;&#33391;&#22909;&#34920;&#29616;&#30340;&#24517;&#35201;&#26465;&#20214;&#65292;&#24182;&#19988;&#28385;&#36275;&#22870;&#21169;&#30340;&#21033;&#26222;&#24076;&#33576;&#24615;&#32422;&#26463;&#23545;&#27169;&#20223;&#24615;&#33021;&#20855;&#26377;&#31215;&#26497;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2006.16785</link><description>&lt;p&gt;
&#21807;&#26377;&#21033;&#26222;&#24076;&#33576;&#24615;&#33021;&#22815;&#39535;&#26381;&#31163;&#32447;&#29983;&#25104;&#23545;&#25239;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Lipschitzness Is All You Need To Tame Off-policy Generative Adversarial Imitation Learning. (arXiv:2006.16785v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2006.16785
&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#29983;&#25104;&#23545;&#25239;&#27169;&#20223;&#23398;&#20064;&#20013;&#65292;&#23558;&#23398;&#20064;&#21040;&#30340;&#22870;&#21169;&#20989;&#25968;&#24378;&#21046;&#21464;&#25104;&#23616;&#37096;&#21033;&#26222;&#24076;&#33576;&#36830;&#32493;&#26159;&#21462;&#24471;&#33391;&#22909;&#34920;&#29616;&#30340;&#24517;&#35201;&#26465;&#20214;&#65292;&#24182;&#19988;&#28385;&#36275;&#22870;&#21169;&#30340;&#21033;&#26222;&#24076;&#33576;&#24615;&#32422;&#26463;&#23545;&#27169;&#20223;&#24615;&#33021;&#20855;&#26377;&#31215;&#26497;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24378;&#21270;&#23398;&#20064;&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#26368;&#36817;&#30340;&#25104;&#21151;&#65292;&#20294;&#26159;&#36825;&#20123;&#26041;&#27861;&#22823;&#22810;&#23545;&#36229;&#21442;&#25968;&#25935;&#24863;&#65292;&#24182;&#19988;&#36890;&#24120;&#38656;&#35201;&#36827;&#34892;&#19968;&#20123;&#20851;&#38190;&#30340;&#24037;&#31243;&#25805;&#20316;&#25165;&#33021;&#21462;&#24471;&#25104;&#21151;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#31163;&#32447;&#29983;&#25104;&#23545;&#25239;&#27169;&#20223;&#23398;&#20064;&#30340;&#24773;&#20917;&#65292;&#24182;&#23545;&#35813;&#26041;&#27861;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#23450;&#24615;&#21644;&#23450;&#37327;&#20998;&#26512;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#23558;&#23398;&#20064;&#21040;&#30340;&#22870;&#21169;&#20989;&#25968;&#24378;&#21046;&#21464;&#25104;&#23616;&#37096;&#21033;&#26222;&#24076;&#33576;&#36830;&#32493;&#26159;&#35813;&#26041;&#27861;&#34920;&#29616;&#33391;&#22909;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20010;&#24517;&#35201;&#26465;&#20214;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20379;&#20102;&#28041;&#21450;&#29366;&#24577;-&#20215;&#20540;&#20989;&#25968;&#30340;&#23616;&#37096;&#21033;&#26222;&#24076;&#33576;&#24615;&#36136;&#30340;&#20960;&#20010;&#29702;&#35770;&#32467;&#26524;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#35777;&#25454;&#35777;&#26126;&#65292;&#22870;&#21169;&#30340;&#21033;&#26222;&#24076;&#33576;&#24615;&#32422;&#26463;&#30340;&#19968;&#33268;&#28385;&#36275;&#23545;&#27169;&#20223;&#24615;&#33021;&#20855;&#26377;&#26497;&#24378;&#30340;&#31215;&#26497;&#24433;&#21709;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#24754;&#35266;&#22870;&#21169;&#39044;&#22788;&#29702;&#38468;&#21152;&#39033;&#65292;&#24418;&#25104;&#20102;&#19968;&#20010;&#22823;&#31867;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the recent success of reinforcement learning in various domains, these approaches remain, for the most part, deterringly sensitive to hyper-parameters and are often riddled with essential engineering feats allowing their success. We consider the case of off-policy generative adversarial imitation learning, and perform an in-depth review, qualitative and quantitative, of the method. We show that forcing the learned reward function to be local Lipschitz-continuous is a sine qua non condition for the method to perform well. We then study the effects of this necessary condition and provide several theoretical results involving the local Lipschitzness of the state-value function. We complement these guarantees with empirical evidence attesting to the strong positive effect that the consistent satisfaction of the Lipschitzness constraint on the reward has on imitation performance. Finally, we tackle a generic pessimistic reward preconditioning add-on spawning a large class of reward 
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31070;&#32463;&#35745;&#31639;&#26694;&#26550;&#19979;&#30340;&#24847;&#35782;&#29702;&#35770;&#65292;&#31216;&#20026;&#8220;&#30446;&#26631;&#23545;&#40784;&#30340;&#20869;&#37096;&#34920;&#31034;&#25805;&#20316;&#8221;&#65288;GARIM&#65289;&#12290;&#35813;&#29702;&#35770;&#35748;&#20026;&#24847;&#35782;&#25903;&#25345;&#23545;&#30446;&#26631;&#30456;&#20851;&#30340;&#20869;&#37096;&#34920;&#31034;&#36827;&#34892;&#20027;&#21160;&#25805;&#20316;&#65292;&#20351;&#20854;&#19982;&#36861;&#27714;&#30340;&#30446;&#26631;&#26356;&#21152;&#23545;&#40784;&#65292;&#20174;&#32780;&#22686;&#21152;&#30446;&#26631;&#23548;&#21521;&#34892;&#20026;&#30340;&#28789;&#27963;&#24615;&#12290;</title><link>http://arxiv.org/abs/1912.13490</link><description>&lt;p&gt;
&#24847;&#35782;&#30340;&#31070;&#32463;&#35745;&#31639;&#27169;&#22411;&#65306;&#30446;&#26631;&#23545;&#40784;&#30340;&#20869;&#37096;&#34920;&#31034;&#25805;&#20316;&#29702;&#35770;&#65288;GARIM&#65289;
&lt;/p&gt;
&lt;p&gt;
A Neurocomputational Account of Consciousness: The Goal-Aligning Representation Internal Manipulation Theory (GARIM). (arXiv:1912.13490v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1912.13490
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31070;&#32463;&#35745;&#31639;&#26694;&#26550;&#19979;&#30340;&#24847;&#35782;&#29702;&#35770;&#65292;&#31216;&#20026;&#8220;&#30446;&#26631;&#23545;&#40784;&#30340;&#20869;&#37096;&#34920;&#31034;&#25805;&#20316;&#8221;&#65288;GARIM&#65289;&#12290;&#35813;&#29702;&#35770;&#35748;&#20026;&#24847;&#35782;&#25903;&#25345;&#23545;&#30446;&#26631;&#30456;&#20851;&#30340;&#20869;&#37096;&#34920;&#31034;&#36827;&#34892;&#20027;&#21160;&#25805;&#20316;&#65292;&#20351;&#20854;&#19982;&#36861;&#27714;&#30340;&#30446;&#26631;&#26356;&#21152;&#23545;&#40784;&#65292;&#20174;&#32780;&#22686;&#21152;&#30446;&#26631;&#23548;&#21521;&#34892;&#20026;&#30340;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24847;&#35782;&#20316;&#20026;&#20154;&#31867;&#35748;&#30693;&#30340;&#26680;&#24515;&#35201;&#32032;&#65292;&#24050;&#32463;&#36890;&#36807;&#31070;&#32463;&#31185;&#23398;&#12289;&#24515;&#29702;&#23398;&#12289;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#20154;&#25216;&#26415;&#31561;&#22810;&#31181;&#31185;&#23398;&#26041;&#27861;&#36827;&#34892;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#39046;&#22495;&#20043;&#38388;&#30340;&#19981;&#33391;&#25972;&#21512;&#38480;&#21046;&#20102;&#23545;&#24847;&#35782;&#30340;&#23436;&#25972;&#21644;&#28165;&#26224;&#29702;&#35299;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#31070;&#32463;&#35745;&#31639;&#26694;&#26550;&#19979;&#30340;&#8220;&#30446;&#26631;&#23545;&#40784;&#30340;&#20869;&#37096;&#34920;&#31034;&#25805;&#20316;&#8221;&#65288;GARIM&#65289;&#24847;&#35782;&#29702;&#35770;&#65292;&#20026;&#25913;&#21892;&#36825;&#31181;&#25972;&#21512;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;GARIM&#29702;&#35770;&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#65292;&#24847;&#35782;&#25903;&#25345;&#23545;&#30446;&#26631;&#30456;&#20851;&#30340;&#20869;&#37096;&#34920;&#31034;&#65288;&#22914;&#19990;&#30028;&#29366;&#24577;&#12289;&#23545;&#35937;&#21644;&#34892;&#20026;&#24207;&#21015;&#65289;&#36827;&#34892;&#20027;&#21160;&#25805;&#20316;&#65292;&#20351;&#23427;&#20204;&#19982;&#36861;&#27714;&#30340;&#30446;&#26631;&#26356;&#21152;&#23545;&#40784;&#12290;&#36825;&#20123;&#25805;&#20316;&#20351;&#24471;&#24847;&#35782;&#20195;&#29702;&#33021;&#22815;&#22312;&#20869;&#37096;&#20135;&#29983;&#20854;&#25152;&#32570;&#20047;&#30340;&#30693;&#35782;&#65292;&#20197;&#24212;&#23545;&#26032;&#26465;&#20214;&#21644;&#30446;&#26631;&#65292;&#20174;&#32780;&#22686;&#21152;&#30446;&#26631;&#23548;&#21521;&#34892;&#20026;&#30340;&#28789;&#27963;&#24615;&#12290;&#34920;&#31034;&#30340;&#25805;&#20316;&#30001;&#22235;&#20010;&#31070;&#32463;&#21151;&#33021;&#23439;&#31995;&#32479;&#65288;Hierarc...
&lt;/p&gt;
&lt;p&gt;
Consciousness, a central element of human cognition, has been studied with multiple scientific approaches spanning neuroscience, psychology, artificial intelligence and robotics. Unfortunately, poor integration between these fields limits a full and clear understanding of consciousness. Here we contribute to improving this integration by proposing, within a neurocomputational framework, the `Goal-Aligning Representations Internal Manipulation' (GARIM) theory of consciousness. The central idea of the GARIM theory is that consciousness supports the active manipulation of goal-relevant internal representations (e.g., world states, objects, and action sequences), making them more aligned with the goals pursued. These manipulations allow the conscious agent to internally produce the knowledge it lacks to cope with novel conditions and goals, increasing the flexibility of goal-directed behaviour. The manipulation of representations is supported by four neuro-functional macro-systems (hierarc
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20351;&#29992;&#22686;&#37327;&#23398;&#20064;&#31639;&#27861;&#26469;&#35299;&#20915;&#29616;&#26377;&#39044;&#27979;&#24615;&#27969;&#31243;&#30417;&#25511;&#25216;&#26415;&#26080;&#27861;&#22788;&#29702;&#30495;&#23454;&#29615;&#22659;&#30340;&#21487;&#21464;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#26356;&#26032;&#27169;&#22411;&#36866;&#24212;&#24403;&#21069;&#24773;&#20917;&#65292;&#23454;&#39564;&#35777;&#26126;&#22686;&#37327;&#23398;&#20064;&#22312;&#39044;&#27979;&#24615;&#27969;&#31243;&#30417;&#25511;&#20013;&#20855;&#26377;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/1804.03967</link><description>&lt;p&gt;
&#22686;&#37327;&#39044;&#27979;&#24615;&#27969;&#31243;&#30417;&#25511;&#65306;&#22914;&#20309;&#22788;&#29702;&#30495;&#23454;&#29615;&#22659;&#30340;&#21464;&#21270;&#24615;
&lt;/p&gt;
&lt;p&gt;
Incremental Predictive Process Monitoring: How to Deal with the Variability of Real Environments. (arXiv:1804.03967v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1804.03967
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20351;&#29992;&#22686;&#37327;&#23398;&#20064;&#31639;&#27861;&#26469;&#35299;&#20915;&#29616;&#26377;&#39044;&#27979;&#24615;&#27969;&#31243;&#30417;&#25511;&#25216;&#26415;&#26080;&#27861;&#22788;&#29702;&#30495;&#23454;&#29615;&#22659;&#30340;&#21487;&#21464;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#26356;&#26032;&#27169;&#22411;&#36866;&#24212;&#24403;&#21069;&#24773;&#20917;&#65292;&#23454;&#39564;&#35777;&#26126;&#22686;&#37327;&#23398;&#20064;&#22312;&#39044;&#27979;&#24615;&#27969;&#31243;&#30417;&#25511;&#20013;&#20855;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#39044;&#27979;&#24615;&#27969;&#31243;&#30417;&#25511;&#25216;&#26415;&#30340;&#19968;&#20010;&#29305;&#28857;&#26159;&#39318;&#20808;&#22522;&#20110;&#36807;&#21435;&#30340;&#27969;&#31243;&#25191;&#34892;&#26500;&#24314;&#39044;&#27979;&#27169;&#22411;&#65292;&#28982;&#21518;&#20351;&#29992;&#35813;&#27169;&#22411;&#39044;&#27979;&#26032;&#30340;&#36827;&#34892;&#20013;&#26696;&#20363;&#30340;&#26410;&#26469;&#65292;&#20294;&#19981;&#33021;&#36890;&#36807;&#26356;&#26032;&#23427;&#26469;&#22788;&#29702;&#26032;&#23436;&#25104;&#25191;&#34892;&#30340;&#26696;&#20363;&#12290;&#36825;&#20351;&#24471;&#39044;&#27979;&#24615;&#27969;&#31243;&#30417;&#25511;&#36807;&#20110;&#21018;&#24615;&#65292;&#26080;&#27861;&#24212;&#23545;&#19981;&#26029;&#28436;&#21464;&#21644;/&#25110;&#38543;&#26102;&#38388;&#23637;&#31034;&#26032;&#21464;&#20307;&#34892;&#20026;&#30340;&#30495;&#23454;&#29615;&#22659;&#20013;&#30340;&#27969;&#31243;&#30340;&#21487;&#21464;&#24615;&#12290;&#20316;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#20801;&#35768;&#22686;&#37327;&#26500;&#24314;&#39044;&#27979;&#27169;&#22411;&#30340;&#31639;&#27861;&#12290;&#36825;&#20123;&#22686;&#37327;&#23398;&#20064;&#31639;&#27861;&#22312;&#26032;&#26696;&#20363;&#21487;&#29992;&#26102;&#26356;&#26032;&#27169;&#22411;&#65292;&#20351;&#24471;&#39044;&#27979;&#27169;&#22411;&#38543;&#26102;&#38388;&#28436;&#21464;&#20197;&#36866;&#24212;&#24403;&#21069;&#24773;&#20917;&#12290;&#31639;&#27861;&#24050;&#32463;&#20351;&#29992;&#19981;&#21516;&#30340;&#26696;&#20363;&#32534;&#30721;&#31574;&#30053;&#36827;&#34892;&#23454;&#29616;&#65292;&#24182;&#22312;&#19968;&#20123;&#30495;&#23454;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#32467;&#26524;&#39318;&#27425;&#35777;&#26126;&#20102;&#22686;&#37327;&#23398;&#20064;&#21487;&#20197;&#24212;&#29992;&#20110;&#39044;&#27979;&#24615;&#27969;&#31243;&#30417;&#25511;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
A characteristic of existing predictive process monitoring techniques is to first construct a predictive model based on past process executions, and then use it to predict the future of new ongoing cases, without the possibility of updating it with new cases when they complete their execution. This can make predictive process monitoring too rigid to deal with the variability of processes working in real environments that continuously evolve and/or exhibit new variant behaviors over time. As a solution to this problem, we propose the use of algorithms that allow the incremental construction of the predictive model. These incremental learning algorithms update the model whenever new cases become available so that the predictive model evolves over time to fit the current circumstances. The algorithms have been implemented using different case encoding strategies and evaluated on a number of real and synthetic datasets. The results provide a first evidence of the potential of incremental l
&lt;/p&gt;</description></item></channel></rss>