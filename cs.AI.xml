<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>DeepSeek LLM&#26159;&#19968;&#20010;&#33268;&#21147;&#20110;&#20197;&#38271;&#26399;&#35270;&#37326;&#25512;&#36827;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#21457;&#23637;&#30340;&#39033;&#30446;&#65292;&#36890;&#36807;&#30740;&#31350;&#21644;&#24212;&#29992;&#25193;&#23637;&#35268;&#24459;&#65292;&#25104;&#21151;&#21019;&#24314;&#20102;DeepSeek Chat&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#29305;&#21035;&#26159;&#22312;&#20195;&#30721;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2401.02954</link><description>&lt;p&gt;
DeepSeek LLM&#65306;&#20511;&#21161;&#38271;&#26399;&#20027;&#20041;&#25193;&#23637;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DeepSeek LLM: Scaling Open-Source Language Models with Longtermism. (arXiv:2401.02954v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02954
&lt;/p&gt;
&lt;p&gt;
DeepSeek LLM&#26159;&#19968;&#20010;&#33268;&#21147;&#20110;&#20197;&#38271;&#26399;&#35270;&#37326;&#25512;&#36827;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#21457;&#23637;&#30340;&#39033;&#30446;&#65292;&#36890;&#36807;&#30740;&#31350;&#21644;&#24212;&#29992;&#25193;&#23637;&#35268;&#24459;&#65292;&#25104;&#21151;&#21019;&#24314;&#20102;DeepSeek Chat&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#29305;&#21035;&#26159;&#22312;&#20195;&#30721;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#28304;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#20196;&#20154;&#30633;&#30446;&#12290;&#28982;&#32780;&#65292;&#20043;&#21069;&#25991;&#29486;&#20013;&#25551;&#36848;&#30340;&#25193;&#23637;&#35268;&#24459;&#24471;&#20986;&#20102;&#19981;&#21516;&#30340;&#32467;&#35770;&#65292;&#36825;&#23545;&#25193;&#23637;LLM&#20135;&#29983;&#20102;&#36127;&#38754;&#24433;&#21709;&#12290;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;&#25193;&#23637;&#35268;&#24459;&#65292;&#24182;&#25552;&#20986;&#20102;&#25105;&#20204;&#29420;&#29305;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#20197;&#20419;&#36827;&#20004;&#31181;&#24120;&#29992;&#30340;&#24320;&#28304;&#37197;&#32622;&#65288;7B&#21644;67B&#65289;&#20013;&#22823;&#35268;&#27169;&#27169;&#22411;&#30340;&#25193;&#23637;&#12290;&#22312;&#25193;&#23637;&#35268;&#24459;&#30340;&#25351;&#23548;&#19979;&#65292;&#25105;&#20204;&#25512;&#20986;&#20102;DeepSeek LLM&#39033;&#30446;&#65292;&#33268;&#21147;&#20110;&#20197;&#38271;&#26399;&#35270;&#37326;&#25512;&#36827;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#12290;&#20026;&#20102;&#25903;&#25345;&#39044;&#35757;&#32451;&#38454;&#27573;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#30446;&#21069;&#21253;&#21547;2&#19975;&#20159;&#20010;&#26631;&#35760;&#65292;&#24182;&#19981;&#26029;&#25193;&#22823;&#12290;&#25105;&#20204;&#36824;&#22312;DeepSeek LLM&#22522;&#26412;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#21644;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#65292;&#20174;&#32780;&#21019;&#24314;&#20102;DeepSeek Chat&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;DeepSeek LLM 67B&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36807;&#20102;LLaMA-2 70B&#65292;&#29305;&#21035;&#26159;&#22312;&#20195;&#30721;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid development of open-source large language models (LLMs) has been truly remarkable. However, the scaling law described in previous literature presents varying conclusions, which casts a dark cloud over scaling LLMs. We delve into the study of scaling laws and present our distinctive findings that facilitate scaling of large scale models in two commonly used open-source configurations, 7B and 67B. Guided by the scaling laws, we introduce DeepSeek LLM, a project dedicated to advancing open-source language models with a long-term perspective. To support the pre-training phase, we have developed a dataset that currently consists of 2 trillion tokens and is continuously expanding. We further conduct supervised fine-tuning (SFT) and Direct Preference Optimization (DPO) on DeepSeek LLM Base models, resulting in the creation of DeepSeek Chat models. Our evaluation results demonstrate that DeepSeek LLM 67B surpasses LLaMA-2 70B on various benchmarks, particularly in the domains of code
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Graph2Tac&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#23450;&#29702;&#35777;&#26126;&#20013;&#23398;&#20064;&#25968;&#23398;&#27010;&#24565;&#30340;&#20998;&#23618;&#34920;&#31034;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#21160;&#24577;&#22320;&#23558;&#26032;&#30340;&#25968;&#23398;&#27010;&#24565;&#32435;&#20837;&#21040;&#30693;&#35782;&#24211;&#20013;&#65292;&#24182;&#22312;Coq&#35777;&#26126;&#21161;&#25163;&#20013;&#36827;&#34892;&#35757;&#32451;&#21644;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.02949</link><description>&lt;p&gt;
Graph2Tac: &#22312;&#23450;&#29702;&#35777;&#26126;&#20013;&#23398;&#20064;&#25968;&#23398;&#27010;&#24565;&#30340;&#20998;&#23618;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Graph2Tac: Learning Hierarchical Representations of Math Concepts in Theorem proving. (arXiv:2401.02949v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02949
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Graph2Tac&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#23450;&#29702;&#35777;&#26126;&#20013;&#23398;&#20064;&#25968;&#23398;&#27010;&#24565;&#30340;&#20998;&#23618;&#34920;&#31034;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#21160;&#24577;&#22320;&#23558;&#26032;&#30340;&#25968;&#23398;&#27010;&#24565;&#32435;&#20837;&#21040;&#30693;&#35782;&#24211;&#20013;&#65292;&#24182;&#22312;Coq&#35777;&#26126;&#21161;&#25163;&#20013;&#36827;&#34892;&#35757;&#32451;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23398;&#21450;&#20854;&#24212;&#29992;&#20013;&#23384;&#22312;&#22823;&#37327;&#30340;&#27010;&#24565;&#12290;&#23427;&#20204;&#22312;&#19981;&#21516;&#30340;&#23398;&#31185;&#39046;&#22495;&#20013;&#26377;&#24456;&#22823;&#30340;&#21464;&#21270;&#65292;&#24182;&#19988;&#27599;&#31687;&#25968;&#23398;&#35770;&#25991;&#25110;&#24212;&#29992;&#20013;&#37117;&#20250;&#24341;&#20837;&#26032;&#30340;&#27010;&#24565;&#12290;&#24418;&#24335;&#21270;&#29702;&#35770;&#24314;&#31435;&#20102;&#19968;&#20010;&#23618;&#27425;&#32467;&#26500;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;&#23450;&#20041;&#12289;&#23450;&#29702;&#21644;&#30456;&#20114;&#24341;&#29992;&#30340;&#35777;&#26126;&#12290;&#24403;&#19968;&#20010;AI&#20195;&#29702;&#20154;&#35777;&#26126;&#19968;&#20010;&#26032;&#30340;&#23450;&#29702;&#26102;&#65292;&#22823;&#22810;&#25968;&#19982;&#35813;&#23450;&#29702;&#30456;&#20851;&#30340;&#25968;&#23398;&#27010;&#24565;&#21644;&#24341;&#29702;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21487;&#33021;&#20174;&#26410;&#34987;&#35265;&#36807;&#12290;&#36825;&#22312;Coq&#35777;&#26126;&#21161;&#25163;&#20013;&#23588;&#20026;&#26126;&#26174;&#65292;&#35813;&#21161;&#25163;&#25317;&#26377;&#21508;&#31181;&#21508;&#26679;&#30340;Coq&#39033;&#30446;&#65292;&#27599;&#20010;&#39033;&#30446;&#37117;&#26377;&#33258;&#24049;&#30340;&#23450;&#20041;&#12289;&#24341;&#29702;&#65292;&#29978;&#33267;&#29992;&#20110;&#35777;&#26126;&#36825;&#20123;&#24341;&#29702;&#30340;&#33258;&#23450;&#20041;&#31574;&#30053;&#36807;&#31243;&#12290;&#23558;&#36825;&#26679;&#30340;&#26032;&#20449;&#24687;&#21363;&#26102;&#22320;&#34701;&#20837;&#21040;&#20195;&#29702;&#20154;&#30340;&#30693;&#35782;&#24211;&#20013;&#23545;&#20110;&#20195;&#29702;&#20154;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#19968;&#20010;&#26032;&#30340;&#12289;&#22823;&#35268;&#27169;&#30340;&#12289;&#22522;&#20110;&#22270;&#30340;&#25968;&#25454;&#38598;&#65292;&#22312;Coq&#20013;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#12290;&#25105;&#20204;&#21033;&#29992;Coq&#26415;&#35821;&#30340;&#24544;&#23454;&#22270;&#34920;&#31034;&#65292;&#21019;&#24314;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;Graph2Tac&#65292;&#35813;&#32593;&#32476;&#36890;&#36807;&#23450;&#20041;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#21019;&#24314;&#20102;&#19968;&#20010;&#26377;&#21521;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Concepts abound in mathematics and its applications. They vary greatly between subject areas, and new ones are introduced in each mathematical paper or application. A formal theory builds a hierarchy of definitions, theorems and proofs that reference each other. When an AI agent is proving a new theorem, most of the mathematical concepts and lemmas relevant to that theorem may have never been seen during training. This is especially true in the Coq proof assistant, which has a diverse library of Coq projects, each with its own definitions, lemmas, and even custom tactic procedures used to prove those lemmas. It is essential for agents to incorporate such new information into their knowledge base on the fly. We work towards this goal by utilizing a new, large-scale, graph-based dataset for machine learning in Coq. We leverage a faithful graph-representation of Coq terms that induces a directed graph of dependencies between definitions to create a novel graph neural network, Graph2Tac (G
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#32852;&#37030;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#26410;&#27880;&#37322;&#30340;&#30446;&#26631;&#22495;&#20013;&#20351;&#29992;&#22810;&#20010;&#24102;&#26377;&#27880;&#37322;&#30340;&#28304;&#39046;&#22495;&#30340;&#30693;&#35782;&#26469;&#36827;&#34892;MRI&#22270;&#20687;&#20998;&#21106;&#12290;</title><link>http://arxiv.org/abs/2401.02941</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#30340;&#32852;&#37030;&#39046;&#22495;&#36866;&#24212;&#29992;&#20110;MRI&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Federated Domain Adaptation for Segmentation of MRI Images. (arXiv:2401.02941v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02941
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#32852;&#37030;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#26410;&#27880;&#37322;&#30340;&#30446;&#26631;&#22495;&#20013;&#20351;&#29992;&#22810;&#20010;&#24102;&#26377;&#27880;&#37322;&#30340;&#28304;&#39046;&#22495;&#30340;&#30693;&#35782;&#26469;&#36827;&#34892;MRI&#22270;&#20687;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#33258;&#21160;&#23545;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;MRI&#65289;&#22270;&#20687;&#36827;&#34892;&#35821;&#20041;&#20998;&#21106;&#26497;&#22823;&#22320;&#26377;&#21161;&#20110;&#35780;&#20272;&#21644;&#35268;&#21010;&#21508;&#31181;&#20020;&#24202;&#24212;&#29992;&#30340;&#27835;&#30103;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#26631;&#27880;&#25968;&#25454;&#26469;&#23454;&#26045;&#31471;&#21040;&#31471;&#30340;&#30417;&#30563;&#23398;&#20064;&#36807;&#31243;&#12290;&#21363;&#20351;&#25105;&#20204;&#26631;&#27880;&#20102;&#36275;&#22815;&#30340;&#25968;&#25454;&#65292;MRI&#22270;&#20687;&#30001;&#20110;&#24739;&#32773;&#12289;MRI&#25195;&#25551;&#20202;&#21644;&#25104;&#20687;&#21327;&#35758;&#30340;&#24046;&#24322;&#32780;&#26174;&#31034;&#20986;&#30456;&#24403;&#22823;&#30340;&#21464;&#24322;&#24615;&#12290;&#36825;&#31181;&#21464;&#24322;&#24615;&#38656;&#35201;&#23545;&#27599;&#20010;&#29305;&#23450;&#30340;&#24212;&#29992;&#39046;&#22495;&#37325;&#26032;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#65292;&#36825;&#21448;&#38656;&#35201;&#19987;&#23478;&#25918;&#23556;&#31185;&#21307;&#29983;&#23545;&#25152;&#26377;&#26032;&#39046;&#22495;&#36827;&#34892;&#25163;&#24037;&#27880;&#37322;&#12290;&#20026;&#20102;&#20943;&#36731;&#23545;&#25345;&#32493;&#25968;&#25454;&#27880;&#37322;&#30340;&#38656;&#27714;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#32852;&#37030;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#22810;&#20010;&#24102;&#26377;&#27880;&#37322;&#30340;&#28304;&#39046;&#22495;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#20174;&#22810;&#20010;&#24102;&#26377;&#27880;&#37322;&#30340;&#28304;&#22495;&#20013;&#36716;&#31227;&#30693;&#35782;&#65292;&#20197;&#36866;&#24212;&#22312;&#26410;&#27880;&#37322;&#30340;&#30446;&#26631;&#22495;&#20013;&#30340;&#26377;&#25928;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic semantic segmentation of magnetic resonance imaging (MRI) images using deep neural networks greatly assists in evaluating and planning treatments for various clinical applications. However, training these models is conditioned on the availability of abundant annotated data to implement the end-to-end supervised learning procedure. Even if we annotate enough data, MRI images display considerable variability due to factors such as differences in patients, MRI scanners, and imaging protocols. This variability necessitates retraining neural networks for each specific application domain, which, in turn, requires manual annotation by expert radiologists for all new domains. To relax the need for persistent data annotation, we develop a method for unsupervised federated domain adaptation using multiple annotated source domains. Our approach enables the transfer of knowledge from several annotated source domains to adapt a model for effective use in an unannotated target domain. Init
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#26512;&#27169;&#22411;&#30340;&#36731;&#37327;&#32423;&#20113;&#21407;&#29983;&#24494;&#26381;&#21153;&#36164;&#28304;&#31649;&#29702;&#31995;&#32479;Ursa&#65292;&#36890;&#36807;&#23558;&#31471;&#21040;&#31471;SLA&#20998;&#35299;&#20026;&#27599;&#20010;&#26381;&#21153;&#30340;SLA&#65292;&#24182;&#22312;&#27599;&#20010;&#24494;&#26381;&#21153;&#23618;&#36827;&#34892;&#36164;&#28304;&#20998;&#37197;&#65292;&#20197;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#26041;&#27861;&#20013;&#25968;&#25454;&#25910;&#38598;&#36807;&#31243;&#20887;&#38271;&#21644;&#21487;&#25193;&#23637;&#24615;&#26377;&#38480;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.02920</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#26512;&#30340;&#20113;&#21407;&#29983;&#24494;&#26381;&#21153;&#36164;&#28304;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
Analytically-Driven Resource Management for Cloud-Native Microservices. (arXiv:2401.02920v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02920
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#26512;&#27169;&#22411;&#30340;&#36731;&#37327;&#32423;&#20113;&#21407;&#29983;&#24494;&#26381;&#21153;&#36164;&#28304;&#31649;&#29702;&#31995;&#32479;Ursa&#65292;&#36890;&#36807;&#23558;&#31471;&#21040;&#31471;SLA&#20998;&#35299;&#20026;&#27599;&#20010;&#26381;&#21153;&#30340;SLA&#65292;&#24182;&#22312;&#27599;&#20010;&#24494;&#26381;&#21153;&#23618;&#36827;&#34892;&#36164;&#28304;&#20998;&#37197;&#65292;&#20197;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#26041;&#27861;&#20013;&#25968;&#25454;&#25910;&#38598;&#36807;&#31243;&#20887;&#38271;&#21644;&#21487;&#25193;&#23637;&#24615;&#26377;&#38480;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20113;&#21407;&#29983;&#24494;&#26381;&#21153;&#30340;&#36164;&#28304;&#31649;&#29702;&#36817;&#24180;&#26469;&#21463;&#21040;&#20102;&#24456;&#22810;&#20851;&#27880;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#30340;&#26041;&#27861;&#22312;SLA&#32500;&#25252;&#21644;&#36164;&#28304;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#30340;&#25216;&#26415;&#65292;&#22914;&#33258;&#21160;&#20280;&#32553;&#12290;&#28982;&#32780;&#65292;&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#30340;&#26041;&#27861;&#20063;&#38754;&#20020;&#30528;&#25968;&#25454;&#25910;&#38598;&#36807;&#31243;&#20887;&#38271;&#21644;&#21487;&#25193;&#23637;&#24615;&#26377;&#38480;&#31561;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Ursa&#65292;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#20113;&#21407;&#29983;&#24494;&#26381;&#21153;&#36164;&#28304;&#31649;&#29702;&#31995;&#32479;&#65292;&#26088;&#22312;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;Ursa&#20351;&#29992;&#19968;&#20010;&#20998;&#26512;&#27169;&#22411;&#23558;&#31471;&#21040;&#31471;SLA&#20998;&#35299;&#20026;&#27599;&#20010;&#26381;&#21153;&#30340;SLA&#65292;&#24182;&#23558;&#27599;&#20010;&#26381;&#21153;&#30340;SLA&#26144;&#23556;&#21040;&#27599;&#20010;&#24494;&#26381;&#21153;&#23618;&#30340;&#36164;&#28304;&#20998;&#37197;&#12290;&#20026;&#20102;&#21152;&#24555;&#25506;&#32034;&#36807;&#31243;&#24182;&#36991;&#20813;&#38271;&#26102;&#38388;&#30340;SLA&#36829;&#35268;&#65292;Ursa&#36880;&#20010;&#24494;&#26381;&#21153;&#36827;&#34892;&#25506;&#32034;&#65292;&#24182;&#22312;&#24310;&#36831;&#36229;&#36807;&#20854;SLA&#26102;&#24555;&#36895;&#20572;&#27490;&#25506;&#32034;&#12290;&#25105;&#20204;&#22312;&#19968;&#32452;&#20195;&#34920;&#24615;&#30340;&#31471;&#21040;&#31471;&#24494;&#26381;&#21153;&#25299;&#25169;&#19978;&#35780;&#20272;&#20102;Ursa&#65292;&#21253;&#25324;&#31038;&#20132;&#32593;&#32476;&#12289;&#23186;&#20307;&#26381;&#21153;&#21644;&#35270;&#39057;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Resource management for cloud-native microservices has attracted a lot of recent attention. Previous work has shown that machine learning (ML)-driven approaches outperform traditional techniques, such as autoscaling, in terms of both SLA maintenance and resource efficiency. However, ML-driven approaches also face challenges including lengthy data collection processes and limited scalability. We present Ursa, a lightweight resource management system for cloud-native microservices that addresses these challenges. Ursa uses an analytical model that decomposes the end-to-end SLA into per-service SLA, and maps per-service SLA to individual resource allocations per microservice tier. To speed up the exploration process and avoid prolonged SLA violations, Ursa explores each microservice individually, and swiftly stops exploration if latency exceeds its SLA.  We evaluate Ursa on a set of representative and end-to-end microservice topologies, including a social network, media service and video 
&lt;/p&gt;</description></item><item><title>H2G2-Net&#26159;&#19968;&#31181;&#29992;&#20110;&#21457;&#29616;&#22810;&#27169;&#24577;&#29983;&#29702;&#21453;&#24212;&#30340;&#20998;&#23618;&#24322;&#26500;&#22270;&#29983;&#25104;&#32593;&#32476;&#26694;&#26550;&#65292;&#33021;&#22815;&#33258;&#21160;&#23398;&#20064;&#22270;&#32467;&#26500;&#32780;&#19981;&#38656;&#35201;&#39044;&#23450;&#20041;&#30340;&#39046;&#22495;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2401.02905</link><description>&lt;p&gt;
H2G2-Net:&#19968;&#31181;&#29992;&#20110;&#22810;&#27169;&#24577;&#29983;&#29702;&#21453;&#24212;&#21457;&#29616;&#30340;&#20998;&#23618;&#24322;&#26500;&#22270;&#29983;&#25104;&#32593;&#32476;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
H2G2-Net: A Hierarchical Heterogeneous Graph Generative Network Framework for Discovery of Multi-Modal Physiological Responses. (arXiv:2401.02905v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02905
&lt;/p&gt;
&lt;p&gt;
H2G2-Net&#26159;&#19968;&#31181;&#29992;&#20110;&#21457;&#29616;&#22810;&#27169;&#24577;&#29983;&#29702;&#21453;&#24212;&#30340;&#20998;&#23618;&#24322;&#26500;&#22270;&#29983;&#25104;&#32593;&#32476;&#26694;&#26550;&#65292;&#33021;&#22815;&#33258;&#21160;&#23398;&#20064;&#22270;&#32467;&#26500;&#32780;&#19981;&#38656;&#35201;&#39044;&#23450;&#20041;&#30340;&#39046;&#22495;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#31181;&#30740;&#31350;&#24212;&#29992;&#20013;&#65292;&#21033;&#29992;&#22810;&#27169;&#24577;&#29983;&#29702;&#20449;&#21495;&#26469;&#21457;&#29616;&#20154;&#31867;&#35748;&#30693;&#21644;&#24773;&#24863;&#29366;&#24577;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#12290;&#20154;&#20307;&#30340;&#29983;&#29702;&#21453;&#24212;&#21463;&#21040;&#20154;&#31867;&#35748;&#30693;&#30340;&#24433;&#21709;&#65292;&#24120;&#29992;&#20110;&#20998;&#26512;&#35748;&#30693;&#29366;&#24577;&#12290;&#20174;&#32593;&#32476;&#31185;&#23398;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#20123;&#24322;&#26500;&#29983;&#29702;&#27169;&#24335;&#22312;&#22270;&#32467;&#26500;&#20013;&#30340;&#20114;&#21160;&#21487;&#33021;&#25552;&#20379;&#26377;&#30410;&#30340;&#20449;&#24687;&#26469;&#25903;&#25345;&#35748;&#30693;&#29366;&#24577;&#30340;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#27809;&#26377;&#21150;&#27861;&#24471;&#21040;&#24322;&#26500;&#27169;&#24577;&#20043;&#38388;&#30340;&#31934;&#30830;&#36830;&#25509;&#65292;&#24182;&#19988;&#23384;&#22312;&#19968;&#31181;&#20998;&#23618;&#32467;&#26500;&#30340;&#23376;&#27169;&#24577;&#12290;&#29616;&#26377;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#29992;&#20110;&#22312;&#39044;&#23450;&#20041;&#30340;&#22270;&#32467;&#26500;&#19978;&#23398;&#20064;&#38750;&#23618;&#27425;&#21270;&#30340;&#21516;&#36136;&#22270;&#65292;&#26080;&#27861;&#20174;&#23618;&#27425;&#21270;&#30340;&#22810;&#27169;&#24577;&#29983;&#29702;&#25968;&#25454;&#20013;&#23398;&#20064;&#65292;&#27809;&#26377;&#39044;&#23450;&#20041;&#30340;&#22270;&#32467;&#26500;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#24322;&#26500;&#22270;&#29983;&#25104;&#32593;&#32476;&#65288;H2G2-Net&#65289;&#65292;&#33021;&#22815;&#33258;&#21160;&#23398;&#20064;&#22270;&#32467;&#26500;&#32780;&#19981;&#38656;&#35201;&#20808;&#39564;&#39046;&#22495;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discovering human cognitive and emotional states using multi-modal physiological signals draws attention across various research applications. Physiological responses of the human body are influenced by human cognition and commonly used to analyze cognitive states. From a network science perspective, the interactions of these heterogeneous physiological modalities in a graph structure may provide insightful information to support prediction of cognitive states. However, there is no clue to derive exact connectivity between heterogeneous modalities and there exists a hierarchical structure of sub-modalities. Existing graph neural networks are designed to learn on non-hierarchical homogeneous graphs with pre-defined graph structures; they failed to learn from hierarchical, multi-modal physiological data without a pre-defined graph structure. To this end, we propose a hierarchical heterogeneous graph generative network (H2G2-Net) that automatically learns a graph structure without domain 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MsDC-DEQ-Net&#30340;&#28145;&#24230;&#24179;&#34913;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#21387;&#32553;&#24863;&#30693;&#37325;&#24314;&#31639;&#27861;&#30340;&#27493;&#39588;&#26144;&#23556;&#21040;&#28145;&#24230;&#32593;&#32476;&#22359;&#20013;&#65292;&#24182;&#32467;&#21512;&#20102;&#32858;&#21512;&#27531;&#24046;&#21464;&#25442;&#21644;&#25380;&#21387;&#19982;&#28608;&#21169;&#26426;&#21046;&#65292;&#22312;&#21387;&#32553;&#24863;&#30693;&#37325;&#24314;&#20013;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.02884</link><description>&lt;p&gt;
MsDC-DEQ-Net: &#29992;&#20110;&#22270;&#20687;&#21387;&#32553;&#24863;&#30693;&#30340;&#22810;&#23610;&#24230;&#25193;&#24352;&#21367;&#31215;DEQ&#28145;&#24230;&#24179;&#34913;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MsDC-DEQ-Net: Deep Equilibrium Model (DEQ) with Multi-scale Dilated Convolution for Image Compressive Sensing (CS). (arXiv:2401.02884v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02884
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MsDC-DEQ-Net&#30340;&#28145;&#24230;&#24179;&#34913;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#21387;&#32553;&#24863;&#30693;&#37325;&#24314;&#31639;&#27861;&#30340;&#27493;&#39588;&#26144;&#23556;&#21040;&#28145;&#24230;&#32593;&#32476;&#22359;&#20013;&#65292;&#24182;&#32467;&#21512;&#20102;&#32858;&#21512;&#27531;&#24046;&#21464;&#25442;&#21644;&#25380;&#21387;&#19982;&#28608;&#21169;&#26426;&#21046;&#65292;&#22312;&#21387;&#32553;&#24863;&#30693;&#37325;&#24314;&#20013;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21387;&#32553;&#24863;&#30693;&#26159;&#19968;&#31181;&#21033;&#29992;&#36739;&#23569;&#27979;&#37327;&#37327;&#24674;&#22797;&#31232;&#30095;&#20449;&#21495;&#30340;&#25216;&#26415;&#12290;&#20026;&#20102;&#35299;&#20915;&#21387;&#32553;&#24863;&#30693;&#37325;&#24314;&#30340;&#35745;&#31639;&#25361;&#25112;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#24320;&#21457;&#19968;&#20010;&#21487;&#35299;&#37322;&#19988;&#31616;&#26126;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#20351;&#29992;&#21387;&#32553;&#24863;&#30693;&#37325;&#24314;&#33258;&#28982;&#22270;&#20687;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#36845;&#20195;&#32553;&#20943;&#38408;&#20540;&#31639;&#27861;&#65288;ISTA&#65289;&#30340;&#19968;&#20010;&#27493;&#39588;&#26144;&#23556;&#21040;&#19968;&#20010;&#28145;&#24230;&#32593;&#32476;&#22359;&#20013;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#34920;&#31034;&#19968;&#20010;ISTA&#30340;&#19968;&#27425;&#36845;&#20195;&#12290;&#20026;&#20102;&#22686;&#24378;&#23398;&#20064;&#33021;&#21147;&#21644;&#34701;&#20837;&#32467;&#26500;&#22810;&#26679;&#24615;&#65292;&#25105;&#20204;&#23558;&#32858;&#21512;&#27531;&#24046;&#21464;&#25442;&#65288;ResNeXt&#65289;&#21644;&#25380;&#21387;&#19982;&#28608;&#21169;&#26426;&#21046;&#65288;SE&#65289;&#38598;&#25104;&#21040;ISTA&#22359;&#20013;&#12290;&#35813;&#22359;&#20316;&#20026;&#28145;&#24230;&#24179;&#34913;&#23618;&#36830;&#25509;&#21040;&#19968;&#20010;&#21322;&#24352;&#37327;&#31215;&#32593;&#32476;&#65288;STP-Net&#65289;&#65292;&#29992;&#20110;&#26041;&#20415;&#37319;&#26679;&#24182;&#25552;&#20379;&#21021;&#22987;&#37325;&#24314;&#12290;&#24471;&#21040;&#30340;&#27169;&#22411;&#31216;&#20026;MsDC-DEQ-Net&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#32593;&#32476;&#30340;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#31454;&#20105;&#24615;&#33021;&#12290;&#23427;&#26174;&#33879;&#20943;&#23569;&#20102;&#23384;&#20648;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Compressive sensing (CS) is a technique that enables the recovery of sparse signals using fewer measurements than traditional sampling methods. To address the computational challenges of CS reconstruction, our objective is to develop an interpretable and concise neural network model for reconstructing natural images using CS. We achieve this by mapping one step of the iterative shrinkage thresholding algorithm (ISTA) to a deep network block, representing one iteration of ISTA. To enhance learning ability and incorporate structural diversity, we integrate aggregated residual transformations (ResNeXt) and squeeze-and-excitation (SE) mechanisms into the ISTA block. This block serves as a deep equilibrium layer, connected to a semi-tensor product network (STP-Net) for convenient sampling and providing an initial reconstruction. The resulting model, called MsDC-DEQ-Net, exhibits competitive performance compared to state-of-the-art network-based methods. It significantly reduces storage requ
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#24102;&#26377;&#26102;&#38388;&#31383;&#21475;&#30340;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#30340;&#26368;&#20248;&#38142;&#36335;&#26041;&#27861;&#65292;&#32771;&#34385;&#20102;&#35745;&#21010;&#30340;&#26102;&#38388;&#28789;&#27963;&#24615;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#35299;&#20915;&#38745;&#24577;&#25320;&#25171;&#36710;&#38382;&#39064;&#26102;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.02873</link><description>&lt;p&gt;
&#36710;&#36742;&#35745;&#21010;&#19982;&#26102;&#38388;&#31383;&#21475;&#30340;&#26368;&#20248;&#38142;&#36335;
&lt;/p&gt;
&lt;p&gt;
Optimal Chaining of Vehicle Plans with Time Windows. (arXiv:2401.02873v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02873
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#24102;&#26377;&#26102;&#38388;&#31383;&#21475;&#30340;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#30340;&#26368;&#20248;&#38142;&#36335;&#26041;&#27861;&#65292;&#32771;&#34385;&#20102;&#35745;&#21010;&#30340;&#26102;&#38388;&#28789;&#27963;&#24615;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#35299;&#20915;&#38745;&#24577;&#25320;&#25171;&#36710;&#38382;&#39064;&#26102;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35299;&#20915;&#24102;&#26377;&#26102;&#38388;&#31383;&#21475;&#30340;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#26102;&#65292;&#25105;&#20204;&#32463;&#24120;&#38656;&#35201;&#23558;&#36710;&#36742;&#35745;&#21010;&#36830;&#25509;&#25104;&#36328;&#36234;&#26356;&#38271;&#26102;&#38388;&#21306;&#38388;&#30340;&#24207;&#21015;&#65292;&#25442;&#21477;&#35805;&#35828;&#65292;&#25105;&#20204;&#38656;&#35201;&#25191;&#34892;&#35745;&#21010;&#38142;&#36335;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32593;&#32476;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36710;&#38431;&#35268;&#27169;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#35813;&#26041;&#27861;&#19981;&#32771;&#34385;&#35745;&#21010;&#30340;&#26102;&#38388;&#28789;&#27963;&#24615;&#65292;&#36825;&#26159;&#25152;&#26377;&#24102;&#26377;&#26102;&#38388;&#31383;&#21475;&#30340;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#30340;&#37325;&#35201;&#23646;&#24615;&#12290;&#30456;&#21453;&#65292;&#35745;&#21010;&#20855;&#26377;&#22266;&#23450;&#26102;&#38388;&#65292;&#19981;&#33021;&#24310;&#36831;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38382;&#39064;&#24314;&#27169;&#65292;&#32771;&#34385;&#20102;&#24310;&#36831;&#21644;&#32473;&#23450;&#26102;&#38388;&#31383;&#21475;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#26159;&#26368;&#20248;&#30340;&#65292;&#24182;&#23545;&#20854;&#22797;&#26434;&#24615;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21015;&#20030;&#20102;&#19968;&#20123;&#23454;&#38469;&#24212;&#29992;&#65292;&#24182;&#23545;&#20854;&#20013;&#19968;&#20010;&#24212;&#29992;&#36827;&#34892;&#20102;&#28436;&#31034;&#65306;&#38745;&#24577;&#25320;&#25171;&#36710;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;&#28436;&#31034;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#22823;&#37327;&#23454;&#20363;&#20013;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
For solving problems from the domain of vehicle routing with time windows, we often need to connect vehicle plans into sequences spanning a longer time horizon or, in other words, we need to perform a plan chaining. Recently, a network-based solution has been proposed to solve the fleet-sizing problem. The method, however, does not consider the time flexibility of the plans, an essential property of all vehicle routing problems with time windows. Instead, plans have fixed times and cannot be delayed. This work presents a new problem formulation that considers delays in line with the given time windows and a method that can be used to solve it. Moreover, we prove that the method is optimal, and we analyze its complexity. Finally, we list some practical applications and perform a demonstration for one of them: the method for solving the static Dial-a-ride problem. The demonstration results show that for a significant number of instances, the proposed method provides a better solution tha
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#22609;&#36896;&#20559;&#22909;&#21644;&#20010;&#24615;&#30340;&#20195;&#29702;&#26694;&#26550;&#65288;AFSPP&#65289;&#65292;&#21487;&#20197;&#25506;&#32034;&#31038;&#20132;&#32593;&#32476;&#21644;&#20027;&#35266;&#24847;&#35782;&#23545;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#29702;&#30340;&#20559;&#22909;&#21644;&#20010;&#24615;&#24418;&#25104;&#30340;&#24433;&#21709;&#65292;&#24182;&#25104;&#21151;&#22797;&#21046;&#20102;&#20960;&#20010;&#20154;&#31867;&#20010;&#24615;&#23454;&#39564;&#30340;&#20851;&#38190;&#21457;&#29616;&#12290;</title><link>http://arxiv.org/abs/2401.02870</link><description>&lt;p&gt;
AFSPP: &#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#22609;&#36896;&#20559;&#22909;&#21644;&#20010;&#24615;&#30340;&#20195;&#29702;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
AFSPP: Agent Framework for Shaping Preference and Personality with Large Language Models. (arXiv:2401.02870v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02870
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#22609;&#36896;&#20559;&#22909;&#21644;&#20010;&#24615;&#30340;&#20195;&#29702;&#26694;&#26550;&#65288;AFSPP&#65289;&#65292;&#21487;&#20197;&#25506;&#32034;&#31038;&#20132;&#32593;&#32476;&#21644;&#20027;&#35266;&#24847;&#35782;&#23545;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#29702;&#30340;&#20559;&#22909;&#21644;&#20010;&#24615;&#24418;&#25104;&#30340;&#24433;&#21709;&#65292;&#24182;&#25104;&#21151;&#22797;&#21046;&#20102;&#20960;&#20010;&#20154;&#31867;&#20010;&#24615;&#23454;&#39564;&#30340;&#20851;&#38190;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#30740;&#31350;&#20154;&#31867;&#34892;&#20026;&#20223;&#30495;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#21033;&#29992;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#29702;&#21019;&#24314;&#20102;&#19968;&#20010;&#31038;&#20250;&#23398;&#30740;&#31350;&#29615;&#22659;&#65292;&#20195;&#29702;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#26410;&#36807;&#28388;&#29305;&#24449;&#23637;&#31034;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#24573;&#35270;&#20102;&#22312;&#31867;&#20284;&#20154;&#31867;&#29615;&#22659;&#20013;&#30340;&#36845;&#20195;&#21457;&#23637; - &#20154;&#31867;&#30340;&#20559;&#22909;&#21644;&#20010;&#24615;&#26159;&#22797;&#26434;&#30340;&#65292;&#21463;&#21040;&#21508;&#31181;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#24182;&#19988;&#22240;&#29615;&#22659;&#21644;&#20027;&#35266;&#24433;&#21709;&#32780;&#19981;&#26029;&#21464;&#21270;&#12290;&#37492;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#22609;&#36896;&#20559;&#22909;&#21644;&#20010;&#24615;&#30340;&#20195;&#29702;&#26694;&#26550;&#65288;AFSPP&#65289;&#65292;&#25506;&#32034;&#31038;&#20132;&#32593;&#32476;&#21644;&#20027;&#35266;&#24847;&#35782;&#23545;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#29702;&#30340;&#20559;&#22909;&#21644;&#20010;&#24615;&#24418;&#25104;&#30340;&#22810;&#26041;&#38754;&#24433;&#21709;&#12290;&#36890;&#36807;AFSPP&#65292;&#25105;&#20204;&#39318;&#27425;&#25104;&#21151;&#22797;&#21046;&#20102;&#20960;&#20010;&#20154;&#31867;&#20010;&#24615;&#23454;&#39564;&#30340;&#20851;&#38190;&#21457;&#29616;&#12290;&#20854;&#20182;&#22522;&#20110;AFSPP&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35745;&#21010;&#21046;&#23450;...
&lt;/p&gt;
&lt;p&gt;
The evolution of Large Language Models (LLMs) has introduced a new paradigm for investigating human behavior emulation. Recent research has employed LLM-based Agents to create a sociological research environment, in which agents exhibit behavior based on the unfiltered characteristics of large language models. However, these studies overlook the iterative development within a human-like setting - Human preferences and personalities are complex, shaped by various factors and subject to ongoing change as a result of environmental and subjective influences. In light of this observation, we propose Agent Framework for Shaping Preference and Personality (AFSPP), exploring the multifaceted impact of social networks and subjective consciousness on LLM-based Agents' preference and personality formation. With AFSPP, we have, for the first time, successfully replicated several key findings from human personality experiments. And other AFSPP-based experimental results indicate that plan making, s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29702;&#35770;&#21551;&#21457;&#30340;&#28459;&#30011;&#39118;&#26684;&#35270;&#35273;&#21465;&#36848;&#29983;&#25104;&#22120;&#65292;&#36890;&#36807;&#23558;&#28459;&#30011;&#21019;&#20316;&#20064;&#24815;&#34701;&#20837;&#31995;&#32479;&#23618;&#32423;&#20013;&#30340;&#19981;&#21516;&#20915;&#31574;&#23618;&#27425;&#65292;&#23454;&#29616;&#20102;&#26681;&#25454;&#21465;&#20107;&#30446;&#26631;&#29983;&#25104;&#22810;&#26679;&#21270;&#28459;&#30011;&#30340;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.02863</link><description>&lt;p&gt;
&#19968;&#20010;&#21487;&#23450;&#21046;&#30340;&#28459;&#30011;&#39118;&#26684;&#35270;&#35273;&#21465;&#36848;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
A Customizable Generator for Comic-Style Visual Narrative. (arXiv:2401.02863v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02863
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29702;&#35770;&#21551;&#21457;&#30340;&#28459;&#30011;&#39118;&#26684;&#35270;&#35273;&#21465;&#36848;&#29983;&#25104;&#22120;&#65292;&#36890;&#36807;&#23558;&#28459;&#30011;&#21019;&#20316;&#20064;&#24815;&#34701;&#20837;&#31995;&#32479;&#23618;&#32423;&#20013;&#30340;&#19981;&#21516;&#20915;&#31574;&#23618;&#27425;&#65292;&#23454;&#29616;&#20102;&#26681;&#25454;&#21465;&#20107;&#30446;&#26631;&#29983;&#25104;&#22810;&#26679;&#21270;&#28459;&#30011;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#21551;&#21457;&#30340;&#35270;&#35273;&#21465;&#36848;&#29983;&#25104;&#22120;&#65292;&#23558;&#28459;&#30011;&#21019;&#20316;&#20064;&#24815;&#34701;&#20837;&#21040;&#25972;&#20010;&#31995;&#32479;&#23618;&#32423;&#20013;&#65292;&#20197;&#21019;&#24314;&#28459;&#30011;&#20869;&#23481;&#12290;&#29983;&#25104;&#22120;&#36890;&#36807;&#20174;&#38754;&#26495;&#26500;&#22270;&#12289;&#29289;&#20307;&#20301;&#32622;&#12289;&#38754;&#26495;&#36716;&#25442;&#21644;&#21465;&#20107;&#20803;&#32032;&#30340;&#23618;&#32423;&#20013;&#36827;&#34892;&#39034;&#24207;&#20915;&#31574;&#26469;&#21019;&#24314;&#28459;&#30011;&#12290;&#27599;&#20010;&#23618;&#32423;&#30340;&#20915;&#31574;&#37117;&#22522;&#20110;&#21465;&#20107;&#30446;&#26631;&#65292;&#24182;&#36981;&#24490;&#23186;&#20171;&#30340;&#30456;&#24212;&#23618;&#32423;&#20064;&#24815;&#12290;Cohn&#30340;&#21465;&#20107;&#35821;&#27861;&#25552;&#20379;&#25972;&#20307;&#25925;&#20107;&#32447;&#12290;&#22522;&#20110;&#19977;&#20998;&#27861;&#21017;&#30340;&#25668;&#24433;&#26500;&#22270;&#29992;&#20110;&#25552;&#20379;&#38754;&#26495;&#26500;&#22270;&#12290;&#22522;&#20110;McCloud&#25552;&#20986;&#30340;&#20197;&#23616;&#37096;&#12289;&#22330;&#26223;&#12289;&#35282;&#33394;&#21644;&#26102;&#38388;&#21464;&#21270;&#20026;&#37325;&#28857;&#30340;&#38754;&#26495;&#36716;&#25442;&#34987;&#32534;&#30721;&#22312;&#36716;&#25442;&#23618;&#20013;&#12290;&#26368;&#21518;&#65292;&#20351;&#29992;&#34892;&#21160;&#21160;&#35789;&#26412;&#20307;&#35770;&#26469;&#20998;&#26512;&#34892;&#21160;&#21160;&#35789;&#24182;&#22522;&#20110;&#20854;&#28155;&#21152;&#24120;&#35265;&#30340;&#21472;&#21152;&#31526;&#21495;&#65288;&#22914;&#24863;&#21497;&#21495;&#65289;&#12290;&#36890;&#36807;&#21508;&#31181;&#35774;&#32622;&#21644;&#31034;&#20363;&#26469;&#23637;&#31034;&#29983;&#25104;&#30340;&#28459;&#30011;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a theory-inspired visual narrative generator that incorporates comic-authoring idioms, which transfers the conceptual principles of comics into system layers that integrate the theories to create comic content. The generator creates comics through sequential decision-making across layers from panel composition, object positions, panel transitions, and narrative elements. Each layer's decisions are based on narrative goals and follow the respective layer idioms of the medium. Cohn's narrative grammar provides the overall story arc. Photographic compositions inspired by the rule of thirds is used to provide panel compositions. McCloud's proposed panel transitions based on focus shifts between scene, character, and temporal changes are encoded in the transition layer. Finally, common overlay symbols (such as the exclamation) are added based on analyzing action verbs using an action-verb ontology. We demonstrate the variety of generated comics through various settings with examp
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#34987;&#24212;&#29992;&#20110;&#22522;&#20110;&#35777;&#25454;&#30340;&#21307;&#23398;&#23454;&#36341;&#65292;&#33021;&#22815;&#24110;&#21161;&#31649;&#29702;&#20020;&#24202;&#21307;&#29983;&#38754;&#20020;&#30340;&#20449;&#24687;&#36807;&#36733;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.02851</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#26159;&#22522;&#20110;&#35777;&#25454;&#30340;&#21307;&#23398;&#23454;&#36341;&#30340;&#33258;&#20027;&#20174;&#19994;&#32773;&#12290; (arXiv:2401.02851v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
Generative Large Language Models are autonomous practitioners of evidence-based medicine. (arXiv:2401.02851v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02851
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#34987;&#24212;&#29992;&#20110;&#22522;&#20110;&#35777;&#25454;&#30340;&#21307;&#23398;&#23454;&#36341;&#65292;&#33021;&#22815;&#24110;&#21161;&#31649;&#29702;&#20020;&#24202;&#21307;&#29983;&#38754;&#20020;&#30340;&#20449;&#24687;&#36807;&#36733;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#22522;&#20110;&#35777;&#25454;&#30340;&#21307;&#23398; (EBM) &#26159;&#29616;&#20195;&#20020;&#24202;&#23454;&#36341;&#30340;&#22522;&#30784;&#65292;&#35201;&#27714;&#20020;&#24202;&#21307;&#29983;&#19981;&#26029;&#26356;&#26032;&#30693;&#35782;&#24182;&#22312;&#24739;&#32773;&#25252;&#29702;&#20013;&#24212;&#29992;&#26368;&#20339;&#20020;&#24202;&#35777;&#25454;&#12290;EBM&#30340;&#23454;&#36341;&#38754;&#20020;&#30528;&#21307;&#23398;&#30740;&#31350;&#30340;&#24555;&#36895;&#36827;&#23637;&#24102;&#26469;&#30340;&#20449;&#24687;&#36807;&#36733;&#25361;&#25112;&#12290;&#20154;&#24037;&#26234;&#33021; (AI)&#65292;&#29305;&#21035;&#26159;&#29983;&#25104;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411; (LLMs)&#65292;&#25552;&#20379;&#20102;&#31649;&#29702;&#36825;&#31181;&#22797;&#26434;&#24615;&#30340;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26041;&#27861;&#65306;&#26412;&#30740;&#31350;&#28041;&#21450;&#22312;&#21508;&#20010;&#19987;&#19994;&#39046;&#22495;&#25910;&#38598;&#29616;&#23454;&#19990;&#30028;&#30340;&#20020;&#24202;&#26696;&#20363;&#65292;&#23558;&#20854;&#36716;&#21270;&#20026;.json&#25991;&#20214;&#36827;&#34892;&#20998;&#26512;&#12290;&#20351;&#29992;&#20102;LLMs&#65292;&#21253;&#25324;ChatGPT 3.5&#21644;4&#31561;&#19987;&#26377;&#27169;&#22411;&#65292;Gemini Pro&#31561;&#65292;&#20197;&#21450;&#24320;&#28304;&#27169;&#22411;&#65292;&#22914;LLaMA v2&#21644;Mixtral-8x7B&#12290;&#36825;&#20123;&#27169;&#22411;&#37197;&#22791;&#20102;&#20174;&#26696;&#20363;&#25991;&#20214;&#20013;&#26816;&#32034;&#20449;&#24687;&#24182;&#20570;&#20986;&#20020;&#24202;&#20915;&#31574;&#30340;&#24037;&#20855;&#65292;&#31867;&#20284;&#20110;&#20020;&#24202;&#21307;&#29983;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#25805;&#20316;&#26041;&#24335;&#12290;&#27169;&#22411;&#34920;&#29616;&#26681;&#25454;&#27491;&#30830;&#24615;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Background: Evidence-based medicine (EBM) is fundamental to modern clinical practice, requiring clinicians to continually update their knowledge and apply the best clinical evidence in patient care. The practice of EBM faces challenges due to rapid advancements in medical research, leading to information overload for clinicians. The integration of artificial intelligence (AI), specifically Generative Large Language Models (LLMs), offers a promising solution towards managing this complexity.  Methods: This study involved the curation of real-world clinical cases across various specialties, converting them into .json files for analysis. LLMs, including proprietary models like ChatGPT 3.5 and 4, Gemini Pro, and open-source models like LLaMA v2 and Mixtral-8x7B, were employed. These models were equipped with tools to retrieve information from case files and make clinical decisions similar to how clinicians must operate in the real world. Model performance was evaluated based on correctness
&lt;/p&gt;</description></item><item><title>&#25968;&#21315;&#20301;AI&#20316;&#32773;&#23545;&#26410;&#26469;AI&#30340;&#39044;&#27979;&#26174;&#31034;&#65292;&#21040;2028&#24180;&#65292;AI&#31995;&#32479;&#26377;50%&#30340;&#20960;&#29575;&#23454;&#29616;&#22810;&#20010;&#37324;&#31243;&#30865;&#65292;&#21253;&#25324;&#33258;&#20027;&#26500;&#24314;&#20840;&#26032;&#30340;&#20184;&#27454;&#22788;&#29702;&#32593;&#31449;&#12289;&#21019;&#20316;&#19968;&#39318;&#19982;&#30693;&#21517;&#38899;&#20048;&#23478;&#30340;&#26032;&#27468;&#38590;&#20197;&#21306;&#20998;&#30340;&#27468;&#26354;&#65292;&#24182;&#33258;&#20027;&#19979;&#36733;&#21644;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#21516;&#26102;&#65292;&#26080;&#38656;&#36741;&#21161;&#30340;&#26426;&#22120;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#32988;&#36807;&#20154;&#31867;&#30340;&#20960;&#29575;&#20272;&#35745;&#20026;10%&#21040;2047&#24180;&#20026;50%&#12290;</title><link>http://arxiv.org/abs/2401.02843</link><description>&lt;p&gt;
&#25968;&#21315;&#20301;AI&#20316;&#32773;&#23545;&#26410;&#26469;AI&#30340;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Thousands of AI Authors on the Future of AI. (arXiv:2401.02843v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02843
&lt;/p&gt;
&lt;p&gt;
&#25968;&#21315;&#20301;AI&#20316;&#32773;&#23545;&#26410;&#26469;AI&#30340;&#39044;&#27979;&#26174;&#31034;&#65292;&#21040;2028&#24180;&#65292;AI&#31995;&#32479;&#26377;50%&#30340;&#20960;&#29575;&#23454;&#29616;&#22810;&#20010;&#37324;&#31243;&#30865;&#65292;&#21253;&#25324;&#33258;&#20027;&#26500;&#24314;&#20840;&#26032;&#30340;&#20184;&#27454;&#22788;&#29702;&#32593;&#31449;&#12289;&#21019;&#20316;&#19968;&#39318;&#19982;&#30693;&#21517;&#38899;&#20048;&#23478;&#30340;&#26032;&#27468;&#38590;&#20197;&#21306;&#20998;&#30340;&#27468;&#26354;&#65292;&#24182;&#33258;&#20027;&#19979;&#36733;&#21644;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#21516;&#26102;&#65292;&#26080;&#38656;&#36741;&#21161;&#30340;&#26426;&#22120;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#32988;&#36807;&#20154;&#31867;&#30340;&#20960;&#29575;&#20272;&#35745;&#20026;10%&#21040;2047&#24180;&#20026;50%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#35268;&#27169;&#30340;&#35843;&#26597;&#20013;&#65292;2778&#21517;&#22312;&#39030;&#32423;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#20250;&#35758;&#19978;&#21457;&#34920;&#36807;&#35770;&#25991;&#30340;&#30740;&#31350;&#20154;&#21592;&#23545;AI&#36827;&#23637;&#30340;&#36895;&#24230;&#12289;&#39640;&#32423;AI&#31995;&#32479;&#30340;&#24615;&#36136;&#21644;&#24433;&#21709;&#36827;&#34892;&#20102;&#39044;&#27979;&#12290;&#24635;&#20307;&#39044;&#27979;&#26174;&#31034;&#65292;&#21040;2028&#24180;&#65292;AI&#31995;&#32479;&#33267;&#23569;&#26377;50%&#30340;&#20960;&#29575;&#23454;&#29616;&#22810;&#20010;&#37324;&#31243;&#30865;&#65292;&#21253;&#25324;&#33258;&#20027;&#26500;&#24314;&#19968;&#20010;&#20840;&#26032;&#30340;&#20184;&#27454;&#22788;&#29702;&#32593;&#31449;&#12289;&#21019;&#20316;&#19968;&#39318;&#21487;&#20197;&#19982;&#30693;&#21517;&#38899;&#20048;&#23478;&#30340;&#26032;&#27468;&#38590;&#20197;&#21306;&#20998;&#30340;&#27468;&#26354;&#65292;&#24182;&#33258;&#20027;&#19979;&#36733;&#21644;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#22914;&#26524;&#31185;&#23398;&#25345;&#32493;&#19981;&#21463;&#24178;&#25200;&#65292;2027&#24180;&#26080;&#38656;&#36741;&#21161;&#30340;&#26426;&#22120;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#32988;&#36807;&#20154;&#31867;&#30340;&#20960;&#29575;&#20272;&#35745;&#20026;10%&#65292;&#21040;2047&#24180;&#20026;50%&#12290;&#21518;&#32773;&#30340;&#20272;&#35745;&#27604;&#25105;&#20204;&#19968;&#24180;&#21069;&#36827;&#34892;&#30340;&#31867;&#20284;&#35843;&#26597;[Grace et al., 2022]&#25552;&#21069;&#20102;13&#24180;&#12290;&#28982;&#32780;&#65292;&#25152;&#26377;&#20154;&#31867;&#32844;&#19994;&#23436;&#20840;&#21487;&#33258;&#21160;&#21270;&#30340;&#20960;&#29575;&#39044;&#35745;&#35201;&#21040;2037&#24180;&#36798;&#21040;10%&#65292;&#21040;2116&#24180;&#25165;&#36798;&#21040;50%&#65288;&#19982;2022&#24180;&#35843;&#26597;&#20013;&#30340;2164&#24180;&#30456;&#27604;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the largest survey of its kind, 2,778 researchers who had published in top-tier artificial intelligence (AI) venues gave predictions on the pace of AI progress and the nature and impacts of advanced AI systems The aggregate forecasts give at least a 50% chance of AI systems achieving several milestones by 2028, including autonomously constructing a payment processing site from scratch, creating a song indistinguishable from a new song by a popular musician, and autonomously downloading and fine-tuning a large language model. If science continues undisrupted, the chance of unaided machines outperforming humans in every possible task was estimated at 10% by 2027, and 50% by 2047. The latter estimate is 13 years earlier than that reached in a similar survey we conducted only one year earlier [Grace et al., 2022]. However, the chance of all human occupations becoming fully automatable was forecast to reach 10% by 2037, and 50% as late as 2116 (compared to 2164 in the 2022 survey).  Most
&lt;/p&gt;</description></item><item><title>Pheme&#26159;&#19968;&#20010;&#39640;&#25928;&#21644;&#23545;&#35805;&#24335;&#35821;&#38899;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#23454;&#26102;&#24773;&#20917;&#19979;&#24037;&#20316;&#65292;&#19981;&#20165;&#20855;&#22791;&#33258;&#28982;&#30340;&#35821;&#38899;&#29983;&#25104;&#33021;&#21147;&#65292;&#36824;&#33021;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.02839</link><description>&lt;p&gt;
Pheme: &#39640;&#25928;&#21644;&#23545;&#35805;&#24335;&#35821;&#38899;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Pheme: Efficient and Conversational Speech Generation. (arXiv:2401.02839v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02839
&lt;/p&gt;
&lt;p&gt;
Pheme&#26159;&#19968;&#20010;&#39640;&#25928;&#21644;&#23545;&#35805;&#24335;&#35821;&#38899;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#23454;&#26102;&#24773;&#20917;&#19979;&#24037;&#20316;&#65292;&#19981;&#20165;&#20855;&#22791;&#33258;&#28982;&#30340;&#35821;&#38899;&#29983;&#25104;&#33021;&#21147;&#65292;&#36824;&#33021;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#35821;&#38899;&#29983;&#25104;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#29616;&#22312;&#24050;&#32463;&#23454;&#29616;&#20102;&#19968;&#27425;&#29983;&#25104;&#33021;&#21147;&#65292;&#24448;&#24448;&#20960;&#20046;&#26080;&#27861;&#21306;&#20998;&#26159;&#21542;&#20026;&#30495;&#23454;&#30340;&#20154;&#22768;&#12290;&#23558;&#36825;&#26679;&#30340;&#35821;&#38899;&#29983;&#25104;&#36827;&#23637;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#65292;&#21487;&#33021;&#20250;&#24443;&#24213;&#25913;&#21464;&#21508;&#31181;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#26576;&#20123;&#24212;&#29992;&#65292;&#22914;&#36741;&#21161;&#23545;&#35805;&#31995;&#32479;&#65292;&#38656;&#35201;&#33258;&#28982;&#21644;&#23545;&#35805;&#24335;&#35821;&#38899;&#29983;&#25104;&#24037;&#20855;&#65292;&#20063;&#38656;&#35201;&#22312;&#23454;&#26102;&#24773;&#20917;&#19979;&#36816;&#34892;&#25928;&#29575;&#39640;&#12290;&#24403;&#21069;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#22914;VALL-E&#21644;SoundStorm&#65292;&#30001;&#20998;&#23618;&#31070;&#32463;&#38899;&#39057;&#32534;&#35299;&#30721;&#22120;&#39537;&#21160;&#65292;&#38656;&#35201;&#22823;&#22411;&#31070;&#32463;&#32452;&#20214;&#21644;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#25165;&#33021;&#33391;&#22909;&#36816;&#20316;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;MQTTS&#26088;&#22312;&#26500;&#24314;&#26356;&#32039;&#20945;&#30340;&#23545;&#35805;&#24335;TTS&#27169;&#22411;&#65292;&#21516;&#26102;&#21033;&#29992;&#23567;&#35268;&#27169;&#30495;&#23454;&#23545;&#35805;&#24335;&#35821;&#38899;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#20854;&#33258;&#22238;&#24402;&#24615;&#36136;&#23548;&#33268;&#25512;&#29702;&#24310;&#36831;&#39640;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23454;&#26102;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, speech generation has seen remarkable progress, now achieving one-shot generation capability that is often virtually indistinguishable from real human voice. Integrating such advancements in speech generation with large language models might revolutionize a wide range of applications. However, certain applications, such as assistive conversational systems, require natural and conversational speech generation tools that also operate efficiently in real time. Current state-of-the-art models like VALL-E and SoundStorm, powered by hierarchical neural audio codecs, require large neural components and extensive training data to work well. In contrast, MQTTS aims to build more compact conversational TTS models while capitalizing on smaller-scale real-life conversational speech data. However, its autoregressive nature yields high inference latency and thus limits its real-time usage. In order to mitigate the current limitations of the state-of-the-art TTS models while capitali
&lt;/p&gt;</description></item><item><title>CrisisViT&#26159;&#19968;&#31181;&#29992;&#20110;&#21361;&#26426;&#22270;&#20687;&#20998;&#31867;&#30340;&#24378;&#22823;&#35270;&#35273;Transformer&#65292;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#27169;&#22411;&#33258;&#21160;&#20998;&#31867;&#21644;&#26631;&#35760;&#22270;&#20687;&#65292;&#24212;&#23545;&#20102;&#31038;&#20132;&#23186;&#20307;&#19978;&#22823;&#37327;&#21361;&#26426;&#22270;&#29255;&#20998;&#26512;&#38656;&#35201;&#26356;&#22810;&#26102;&#38388;&#21644;&#21162;&#21147;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.02838</link><description>&lt;p&gt;
CrisisViT:&#19968;&#31181;&#29992;&#20110;&#21361;&#26426;&#22270;&#20687;&#20998;&#31867;&#30340;&#24378;&#22823;&#35270;&#35273;Transformer
&lt;/p&gt;
&lt;p&gt;
CrisisViT: A Robust Vision Transformer for Crisis Image Classification. (arXiv:2401.02838v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02838
&lt;/p&gt;
&lt;p&gt;
CrisisViT&#26159;&#19968;&#31181;&#29992;&#20110;&#21361;&#26426;&#22270;&#20687;&#20998;&#31867;&#30340;&#24378;&#22823;&#35270;&#35273;Transformer&#65292;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#27169;&#22411;&#33258;&#21160;&#20998;&#31867;&#21644;&#26631;&#35760;&#22270;&#20687;&#65292;&#24212;&#23545;&#20102;&#31038;&#20132;&#23186;&#20307;&#19978;&#22823;&#37327;&#21361;&#26426;&#22270;&#29255;&#20998;&#26512;&#38656;&#35201;&#26356;&#22810;&#26102;&#38388;&#21644;&#21162;&#21147;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32039;&#24613;&#24773;&#20917;&#19979;&#65292;&#21361;&#26426;&#24212;&#23545;&#26426;&#26500;&#38656;&#35201;&#24555;&#36895;&#20934;&#30830;&#22320;&#35780;&#20272;&#22320;&#38754;&#24773;&#20917;&#65292;&#20197;&#20415;&#37096;&#32626;&#30456;&#20851;&#26381;&#21153;&#21644;&#36164;&#28304;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21463;&#24433;&#21709;&#22320;&#21306;&#30340;&#25968;&#25454;&#21487;&#33021;&#31232;&#32570;&#65292;&#30452;&#21040;&#24403;&#22320;&#30340;&#24212;&#24613;&#21709;&#24212;&#26381;&#21153;&#33021;&#22815;&#25552;&#20379;&#31532;&#19968;&#25163;&#25253;&#21578;&#65292;&#24403;&#23616;&#24120;&#24120;&#24517;&#39035;&#26681;&#25454;&#26377;&#38480;&#20449;&#24687;&#20570;&#20986;&#20915;&#31574;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#26234;&#33021;&#25163;&#26426;&#26222;&#21450;&#21644;&#39640;&#36136;&#37327;&#30456;&#26426;&#30340;&#26222;&#36941;&#21487;&#29992;&#24615;&#20351;&#24471;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20844;&#27665;&#35760;&#32773;&#25104;&#20026;&#21361;&#26426;&#21709;&#24212;&#32773;&#23453;&#36149;&#30340;&#20449;&#24687;&#26469;&#28304;&#12290;&#28982;&#32780;&#65292;&#20998;&#26512;&#20844;&#27665;&#21457;&#24067;&#30340;&#22823;&#37327;&#22270;&#29255;&#38656;&#35201;&#27604;&#36890;&#24120;&#21487;&#29992;&#30340;&#26102;&#38388;&#21644;&#21162;&#21147;&#26356;&#22810;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#31070;&#32463;&#27169;&#22411;&#36827;&#34892;&#33258;&#21160;&#22270;&#20687;&#20998;&#31867;/&#26631;&#35760;&#30340;&#26041;&#27861;&#65292;&#20855;&#20307;&#26469;&#35828;&#26159;&#36890;&#36807;&#23558;Transformer&#26550;&#26500;&#24212;&#29992;&#20110;&#21361;&#26426;&#22270;&#20687;&#20998;&#31867;&#20013;&#65288;CrisisViT&#65289;&#12290;&#25105;&#20204;&#21033;&#29992;&#26032;&#30340;Incidents1M&#21361;&#26426;&#22270;&#20687;&#25968;&#25454;&#38598;&#24320;&#21457;&#20102;&#19968;&#31995;&#21015;&#22522;&#20110;Transformer&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In times of emergency, crisis response agencies need to quickly and accurately assess the situation on the ground in order to deploy relevant services and resources. However, authorities often have to make decisions based on limited information, as data on affected regions can be scarce until local response services can provide first-hand reports. Fortunately, the widespread availability of smartphones with high-quality cameras has made citizen journalism through social media a valuable source of information for crisis responders. However, analyzing the large volume of images posted by citizens requires more time and effort than is typically available. To address this issue, this paper proposes the use of state-of-the-art deep neural models for automatic image classification/tagging, specifically by adapting transformer-based architectures for crisis image classification (CrisisViT). We leverage the new Incidents1M crisis image dataset to develop a range of new transformer-based image 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#26469;&#25552;&#39640;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#35757;&#32451;&#30340;&#40065;&#26834;&#24615;&#21644;&#25910;&#25947;&#24615;&#12290;&#32463;&#36807;&#20004;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#21457;&#29616;&#36801;&#31227;&#23398;&#20064;&#21487;&#20197;&#26377;&#25928;&#22320;&#35757;&#32451;PINN&#26469;&#36817;&#20284;&#35299;&#20915;&#26041;&#26696;&#65292;&#20174;&#20302;&#39057;&#29575;&#38382;&#39064;&#21040;&#39640;&#39057;&#29575;&#38382;&#39064;&#65292;&#32780;&#19981;&#22686;&#21152;&#32593;&#32476;&#21442;&#25968;&#65292;&#19988;&#38656;&#35201;&#26356;&#23569;&#30340;&#25968;&#25454;&#28857;&#21644;&#26356;&#30701;&#30340;&#35757;&#32451;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2401.02810</link><description>&lt;p&gt;
&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#39640;&#39057;&#29575;&#21644;&#22810;&#23610;&#24230;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Physics-Informed Neural Networks for High-Frequency and Multi-Scale Problems using Transfer Learning. (arXiv:2401.02810v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02810
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#26469;&#25552;&#39640;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#35757;&#32451;&#30340;&#40065;&#26834;&#24615;&#21644;&#25910;&#25947;&#24615;&#12290;&#32463;&#36807;&#20004;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#21457;&#29616;&#36801;&#31227;&#23398;&#20064;&#21487;&#20197;&#26377;&#25928;&#22320;&#35757;&#32451;PINN&#26469;&#36817;&#20284;&#35299;&#20915;&#26041;&#26696;&#65292;&#20174;&#20302;&#39057;&#29575;&#38382;&#39064;&#21040;&#39640;&#39057;&#29575;&#38382;&#39064;&#65292;&#32780;&#19981;&#22686;&#21152;&#32593;&#32476;&#21442;&#25968;&#65292;&#19988;&#38656;&#35201;&#26356;&#23569;&#30340;&#25968;&#25454;&#28857;&#21644;&#26356;&#30701;&#30340;&#35757;&#32451;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#26159;&#19968;&#31181;&#29992;&#20110;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;ODEs/PDEs&#65289;&#30340;&#25968;&#25454;&#39537;&#21160;&#27714;&#35299;&#22120;&#65292;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#26694;&#26550;&#26469;&#22788;&#29702;&#21069;&#21521;&#21644;&#21453;&#21521;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30446;&#26631;&#20989;&#25968;&#30340;&#22797;&#26434;&#24615;&#24120;&#24120;&#23548;&#33268;&#35757;&#32451;&#22833;&#36133;&#12290;&#24403;&#35299;&#20915;&#39640;&#39057;&#29575;&#21644;&#22810;&#23610;&#24230;&#38382;&#39064;&#26102;&#65292;&#36825;&#20010;&#38382;&#39064;&#23588;&#20026;&#31361;&#20986;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#26469;&#25552;&#39640;&#35757;&#32451;PINN&#30340;&#40065;&#26834;&#24615;&#21644;&#25910;&#25947;&#24615;&#65292;&#20174;&#20302;&#39057;&#29575;&#38382;&#39064;&#24320;&#22987;&#35757;&#32451;&#65292;&#24182;&#36880;&#28176;&#25509;&#36817;&#39640;&#39057;&#29575;&#38382;&#39064;&#12290;&#36890;&#36807;&#20004;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;&#36801;&#31227;&#23398;&#20064;&#21487;&#20197;&#26377;&#25928;&#22320;&#35757;&#32451;PINN&#26469;&#36817;&#20284;&#35299;&#20915;&#26041;&#26696;&#65292;&#20174;&#20302;&#39057;&#29575;&#38382;&#39064;&#21040;&#39640;&#39057;&#29575;&#38382;&#39064;&#65292;&#32780;&#19981;&#22686;&#21152;&#32593;&#32476;&#21442;&#25968;&#12290;&#27492;&#22806;&#65292;&#23427;&#38656;&#35201;&#26356;&#23569;&#30340;&#25968;&#25454;&#28857;&#21644;&#26356;&#30701;&#30340;&#35757;&#32451;&#26102;&#38388;&#12290;&#25105;&#20204;&#35814;&#32454;&#25551;&#36848;&#20102;&#25105;&#20204;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#21253;&#25324;&#20248;&#21270;&#22120;&#30340;&#36873;&#25321;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#26469;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physics-informed neural network (PINN) is a data-driven solver for partial and ordinary differential equations(ODEs/PDEs). It provides a unified framework to address both forward and inverse problems. However, the complexity of the objective function often leads to training failures. This issue is particularly prominent when solving high-frequency and multi-scale problems. We proposed using transfer learning to boost the robustness and convergence of training PINN, starting training from low-frequency problems and gradually approaching high-frequency problems. Through two case studies, we discovered that transfer learning can effectively train PINN to approximate solutions from low-frequency problems to high-frequency problems without increasing network parameters. Furthermore, it requires fewer data points and less training time. We elaborately described our training strategy, including optimizer selection, and suggested guidelines for using transfer learning to train neural networks 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26694;&#26550;&#65292;&#24182;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#23553;&#38381;&#24335;&#38382;&#39064;&#19978;&#27604;GPT-4v&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;26&#65285;&#12290;</title><link>http://arxiv.org/abs/2401.02797</link><description>&lt;p&gt;
PeFoMed&#65306;&#38024;&#23545;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
PeFoMed: Parameter Efficient Fine-tuning on Multimodal Large Language Models for Medical Visual Question Answering. (arXiv:2401.02797v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02797
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26694;&#26550;&#65292;&#24182;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#23553;&#38381;&#24335;&#38382;&#39064;&#19978;&#27604;GPT-4v&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;26&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#22312;&#20256;&#32479;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#19978;&#36827;&#34892;&#20102;&#36827;&#21270;&#25193;&#23637;&#65292;&#20351;&#23427;&#20204;&#33021;&#22815;&#24212;&#23545;&#36229;&#36234;&#32431;&#25991;&#26412;&#24212;&#29992;&#33539;&#22260;&#30340;&#25361;&#25112;&#12290;&#23427;&#21033;&#29992;&#20102;&#20808;&#21069;&#32534;&#30721;&#22312;&#36825;&#20123;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30693;&#35782;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#23427;&#20204;&#22312;&#22810;&#27169;&#24577;&#29615;&#22659;&#19979;&#30340;&#36866;&#29992;&#24615;&#21644;&#21151;&#33021;&#24615;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;MLLMs&#36866;&#24212;&#20026;&#29983;&#25104;&#20219;&#21153;&#20197;&#35299;&#20915;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#65288;Med-VQA&#65289;&#20219;&#21153;&#30340;&#33258;&#30001;&#24418;&#24335;&#31572;&#26696;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;Med-VQA&#24212;&#29992;&#29305;&#21035;&#23450;&#21046;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26694;&#26550;&#65292;&#24182;&#22312;&#20844;&#20849;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#39564;&#35777;&#12290;&#20026;&#20102;&#20934;&#30830;&#34913;&#37327;&#24615;&#33021;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20154;&#24037;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#23553;&#38381;&#24335;&#38382;&#39064;&#30340;&#25972;&#20307;&#20934;&#30830;&#29575;&#19978;&#36798;&#21040;&#20102;81.9&#65285;&#65292;&#19988;&#20854;&#30456;&#23545;&#20110;GPT-4v&#27169;&#22411;&#30340;&#32477;&#23545;&#20934;&#30830;&#29575;&#36229;&#36807;&#20102;26&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal large language models (MLLMs) represent an evolutionary expansion in the capabilities of traditional large language models, enabling them to tackle challenges that surpass the scope of purely text-based applications. It leverages the knowledge previously encoded within these language models, thereby enhancing their applicability and functionality in the reign of multimodal contexts. Recent works investigate the adaptation of MLLMs to predict free-form answers as a generative task to solve medical visual question answering (Med-VQA) tasks. In this paper, we propose a parameter efficient framework for fine-tuning MLLM specifically tailored to Med-VQA applications, and empirically validate it on a public benchmark dataset. To accurately measure the performance, we employ human evaluation and the results reveal that our model achieves an overall accuracy of 81.9%, and outperforms the GPT-4v model by a significant margin of 26% absolute accuracy on closed-ended questions. The cod
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;RAISE&#30340;&#26550;&#26500;&#65292;&#23427;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;GPT-4&#25972;&#21512;&#21040;&#23545;&#35805;&#20195;&#29702;&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#21452;&#32452;&#20214;&#35760;&#24518;&#31995;&#32479;&#26469;&#22686;&#24378;&#20195;&#29702;&#22312;&#22810;&#36718;&#23545;&#35805;&#20013;&#30340;&#21487;&#25511;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;&#39044;liminary evaluations&#34920;&#26126;&#65292;RAISE&#22312;&#25151;&#22320;&#20135;&#38144;&#21806;&#39046;&#22495;&#20855;&#26377;&#20248;&#21183;&#65292;&#24182;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.02777</link><description>&lt;p&gt;
&#20174;LLM&#21040;&#23545;&#35805;&#20195;&#29702;&#65306;&#20855;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#30340;&#35760;&#24518;&#22686;&#24378;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
From LLM to Conversational Agent: A Memory Enhanced Architecture with Fine-Tuning of Large Language Models. (arXiv:2401.02777v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02777
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;RAISE&#30340;&#26550;&#26500;&#65292;&#23427;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;GPT-4&#25972;&#21512;&#21040;&#23545;&#35805;&#20195;&#29702;&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#21452;&#32452;&#20214;&#35760;&#24518;&#31995;&#32479;&#26469;&#22686;&#24378;&#20195;&#29702;&#22312;&#22810;&#36718;&#23545;&#35805;&#20013;&#30340;&#21487;&#25511;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;&#39044;liminary evaluations&#34920;&#26126;&#65292;RAISE&#22312;&#25151;&#22320;&#20135;&#38144;&#21806;&#39046;&#22495;&#20855;&#26377;&#20248;&#21183;&#65292;&#24182;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;RAISE&#65288;Scratchpad&#21644;Examples&#36741;&#21161;&#25512;&#29702;&#21644;&#34892;&#20026;&#65289;,&#19968;&#31181;&#20808;&#36827;&#30340;&#26550;&#26500;&#65292;&#22686;&#24378;&#20102;&#23558;GPT-4&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25972;&#21512;&#21040;&#23545;&#35805;&#20195;&#29702;&#20013;&#30340;&#33021;&#21147;&#12290;RAISE&#26159;ReAct&#26694;&#26550;&#30340;&#25913;&#36827;&#29256;&#26412;&#65292;&#21253;&#25324;&#19968;&#20010;&#21452;&#32452;&#20214;&#35760;&#24518;&#31995;&#32479;&#65292;&#27169;&#20223;&#20154;&#31867;&#30340;&#30701;&#26399;&#35760;&#24518;&#21644;&#38271;&#26399;&#35760;&#24518;&#65292;&#20197;&#20445;&#25345;&#23545;&#35805;&#30340;&#19978;&#19979;&#25991;&#21644;&#36830;&#32493;&#24615;&#12290;&#23427;&#21253;&#25324;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#20195;&#29702;&#26500;&#24314;&#24773;&#26223;&#65292;&#21253;&#25324;&#23545;&#35805;&#36873;&#25321;&#65292;&#22330;&#26223;&#25552;&#21462;&#65292;CoT&#23436;&#25104;&#21644;&#22330;&#26223;&#22686;&#24378;&#31561;&#38454;&#27573;&#65292;&#26368;&#32456;&#23548;&#33268;LLMs&#30340;&#35757;&#32451;&#38454;&#27573;&#12290;&#36825;&#31181;&#26041;&#27861;&#20284;&#20046;&#25552;&#39640;&#20102;&#20195;&#29702;&#22312;&#22797;&#26434;&#30340;&#22810;&#36718;&#23545;&#35805;&#20013;&#30340;&#21487;&#25511;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;&#25105;&#20204;&#22312;&#25151;&#22320;&#20135;&#38144;&#21806;&#29615;&#22659;&#20013;&#30340;&#21021;&#27493;&#35780;&#20272;&#34920;&#26126;&#65292;RAISE&#30456;&#23545;&#20110;&#20256;&#32479;&#20195;&#29702;&#26377;&#19968;&#20123;&#20248;&#21183;&#65292;&#34920;&#26126;&#23427;&#22312;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#20013;&#20855;&#26377;&#28508;&#21147;&#12290;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#24378;&#22823;&#30340;&#26694;&#26550;&#26469;&#24320;&#21457;&#26356;&#20855;&#19978;&#19979;&#25991;&#24863;&#30693;&#21644;&#22810;&#21151;&#33021;&#30340;&#23545;&#35805;&#20195;&#29702;&#65292;&#36825;&#39033;&#24037;&#20316;&#20026;AI&#39046;&#22495;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces RAISE (Reasoning and Acting through Scratchpad and Examples), an advanced architecture enhancing the integration of Large Language Models (LLMs) like GPT-4 into conversational agents. RAISE, an enhancement of the ReAct framework, incorporates a dual-component memory system, mirroring human short-term and long-term memory, to maintain context and continuity in conversations. It entails a comprehensive agent construction scenario, including phases like Conversation Selection, Scene Extraction, CoT Completion, and Scene Augmentation, leading to the LLMs Training phase. This approach appears to enhance agent controllability and adaptability in complex, multi-turn dialogues. Our preliminary evaluations in a real estate sales context suggest that RAISE has some advantages over traditional agents, indicating its potential for broader applications. This work contributes to the AI field by providing a robust framework for developing more context-aware and versatile convers
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#23039;&#21183;&#35782;&#21035;&#20013;&#30005;&#26497;&#28418;&#31227;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;HD-EMG&#30005;&#26497;&#23376;&#38598;&#65292;&#24182;&#22686;&#21152;&#26469;&#33258;&#19981;&#21516;&#30005;&#26497;&#20301;&#32622;&#30340;&#25968;&#25454;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#31283;&#20581;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.02773</link><description>&lt;p&gt;
&#36890;&#36807;HD-EMG&#30005;&#26497;&#23376;&#38598;&#35299;&#20915;&#23039;&#21183;&#35782;&#21035;&#20013;&#30340;&#30005;&#26497;&#28418;&#31227;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Tackling Electrode Shift In Gesture Recognition with HD-EMG Electrode Subsets. (arXiv:2401.02773v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02773
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#23039;&#21183;&#35782;&#21035;&#20013;&#30005;&#26497;&#28418;&#31227;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;HD-EMG&#30005;&#26497;&#23376;&#38598;&#65292;&#24182;&#22686;&#21152;&#26469;&#33258;&#19981;&#21516;&#30005;&#26497;&#20301;&#32622;&#30340;&#25968;&#25454;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#31283;&#20581;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
sEMG&#27169;&#24335;&#35782;&#21035;&#31639;&#27861;&#22312;&#35299;&#30721;&#36816;&#21160;&#24847;&#22270;&#26041;&#38754;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#24050;&#30693;&#23545;&#20110;&#19981;&#26029;&#21464;&#21270;&#30340;&#35760;&#24405;&#26465;&#20214;&#38750;&#24120;&#33030;&#24369;&#65292;&#24615;&#33021;&#22312;&#19981;&#21516;&#21463;&#35797;&#32773;&#21644;&#19981;&#21516;&#20250;&#35805;&#20043;&#38388;&#20250;&#26174;&#33879;&#19979;&#38477;&#12290;&#22810;&#36890;&#36947;&#34920;&#38754;&#32908;&#30005;&#22270;&#65288;&#20063;&#31216;&#20026;&#39640;&#23494;&#24230;sEMG&#65289;&#31995;&#32479;&#36890;&#36807;&#20351;&#29992;&#39069;&#22806;&#30340;&#30005;&#26497;&#25910;&#38598;&#20449;&#24687;&#65292;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26377;&#38480;&#30340;&#25968;&#25454;&#38598;&#21644;&#35299;&#20915;&#30005;&#26497;&#25918;&#32622;&#31561;&#21464;&#37327;&#26469;&#28304;&#30340;&#22256;&#38590;&#65292;&#32570;&#20047;&#31283;&#20581;&#24615;&#19968;&#30452;&#23384;&#22312;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#22312;&#19968;&#32452;&#36755;&#20837;&#36890;&#36947;&#23376;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807;&#26469;&#33258;&#19981;&#21516;&#30005;&#26497;&#20301;&#32622;&#30340;&#25968;&#25454;&#22686;&#24378;&#25105;&#20204;&#30340;&#35757;&#32451;&#20998;&#24067;&#65292;&#20174;&#32780;&#21516;&#26102;&#35299;&#20915;&#30005;&#26497;&#28418;&#31227;&#38382;&#39064;&#21644;&#38477;&#20302;&#36755;&#20837;&#32500;&#24230;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;&#23545;&#30005;&#26497;&#28418;&#31227;&#30340;&#31283;&#20581;&#24615;&#65292;&#24182;&#22312;&#19981;&#21516;&#21463;&#35797;&#32773;&#21644;&#20998;&#31867;&#31639;&#27861;&#38388;&#26174;&#33879;&#25552;&#39640;&#20102;&#20250;&#35805;&#38388;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
sEMG pattern recognition algorithms have been explored extensively in decoding movement intent, yet are known to be vulnerable to changing recording conditions, exhibiting significant drops in performance across subjects, and even across sessions. Multi-channel surface EMG, also referred to as high-density sEMG (HD-sEMG) systems, have been used to improve performance with the information collected through the use of additional electrodes. However, a lack of robustness is ever present due to limited datasets and the difficulties in addressing sources of variability, such as electrode placement. In this study, we propose training on a collection of input channel subsets and augmenting our training distribution with data from different electrode locations, simultaneously targeting electrode shift and reducing input dimensionality. Our method increases robustness against electrode shift and results in significantly higher intersession performance across subjects and classification algorith
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#36229;&#21442;&#25968;&#30340;&#36817;&#20284;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#65288;AMBR&#65289;&#35299;&#30721;&#26041;&#27861;&#65292;&#29992;&#20110;&#26356;&#24555;&#22320;&#36827;&#34892;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#36845;&#20195;&#28040;&#38500;&#27861;&#31639;&#27861;&#26469;&#35299;&#20915;&#20013;&#20301;&#25968;&#35782;&#21035;&#38382;&#39064;&#65292;&#20197;&#36798;&#21040;&#21152;&#36895;&#35299;&#30721;&#30340;&#30446;&#30340;&#12290;</title><link>http://arxiv.org/abs/2401.02749</link><description>&lt;p&gt;
&#26080;&#38656;&#36229;&#21442;&#25968;&#30340;&#26356;&#24555;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#35299;&#30721;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Hyperparameter-Free Approach for Faster Minimum Bayes Risk Decoding. (arXiv:2401.02749v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02749
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#36229;&#21442;&#25968;&#30340;&#36817;&#20284;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#65288;AMBR&#65289;&#35299;&#30721;&#26041;&#27861;&#65292;&#29992;&#20110;&#26356;&#24555;&#22320;&#36827;&#34892;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#36845;&#20195;&#28040;&#38500;&#27861;&#31639;&#27861;&#26469;&#35299;&#20915;&#20013;&#20301;&#25968;&#35782;&#21035;&#38382;&#39064;&#65292;&#20197;&#36798;&#21040;&#21152;&#36895;&#35299;&#30721;&#30340;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#35299;&#30721;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#22312;&#24191;&#27867;&#30340;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#26367;&#20195;&#26463;&#25628;&#32034;&#35299;&#30721;&#30340;&#24378;&#22823;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;MBR&#38656;&#35201;&#22823;&#37327;&#30340;&#26102;&#38388;&#26469;&#35745;&#31639;MBR&#30446;&#26631;&#65292;&#36825;&#20351;&#24471;&#35813;&#26041;&#27861;&#22312;&#35768;&#22810;&#38656;&#35201;&#21709;&#24212;&#26102;&#38388;&#33267;&#20851;&#37325;&#35201;&#30340;&#24773;&#20917;&#19979;&#19981;&#21487;&#34892;&#12290;&#26368;&#36817;&#25552;&#20986;&#20102;&#22522;&#20110;&#32622;&#20449;&#24230;&#30340;&#21098;&#26525;(CBP)&#26041;&#27861;&#26469;&#38477;&#20302;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#20013;&#30340;&#25512;&#29702;&#26102;&#38388;&#12290;&#23613;&#31649;&#24050;&#32463;&#35777;&#26126;&#23427;&#33021;&#26174;&#33879;&#20943;&#23569;&#35745;&#31639;&#37327;&#65292;&#20294;&#26159;&#23427;&#38656;&#35201;&#20351;&#29992;&#24320;&#21457;&#38598;&#36827;&#34892;&#36229;&#21442;&#25968;&#35843;&#20248;&#25165;&#33021;&#21457;&#25381;&#20316;&#29992;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#36229;&#21442;&#25968;&#30340;&#36817;&#20284;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#65288;AMBR&#65289;&#35299;&#30721;&#26041;&#27861;&#12290;AMBR&#22522;&#20110;&#20197;&#19979;&#35266;&#23519;&#24471;&#20986;&#65306;&#35745;&#31639;&#22522;&#20110;&#26679;&#26412;&#30340;MBR&#30446;&#26631;&#30340;&#38382;&#39064;&#26159;&#20013;&#20301;&#25968;&#35782;&#21035;&#38382;&#39064;&#12290;AMBR&#20351;&#29992;&#20102;&#36845;&#20195;&#28040;&#38500;&#27861;&#65288;CSH&#65289;&#31639;&#27861;&#65292;&#36825;&#26159;&#36804;&#20170;&#20026;&#27490;&#26368;&#22909;&#30340;&#36817;&#20284;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Minimum Bayes-Risk (MBR) decoding is shown to be a powerful alternative to beam search decoding for a wide range of text generation tasks. However, MBR requires a huge amount of time for inference to compute the MBR objective, which makes the method infeasible in many situations where response time is critical. Confidence-based pruning (CBP) (Cheng and Vlachos, 2023) has recently been proposed to reduce the inference time in machine translation tasks. Although it is shown to significantly reduce the amount of computation, it requires hyperparameter tuning using a development set to be effective. To this end, we propose Approximate Minimum Bayes-Risk (AMBR) decoding, a hyperparameter-free method to run MBR decoding approximately. AMBR is derived from the observation that the problem of computing the sample-based MBR objective is the medoid identification problem. AMBR uses the Correlated Sequential Halving (CSH) algorithm (Baharav and Tse, 2019), the best approximation algorithm to date
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;MAMI&#26041;&#27861;&#65292;&#21033;&#29992;&#22810;&#27880;&#24847;&#21147;&#20114;&#20449;&#24687;&#29992;&#20110;&#38271;&#24207;&#21015;&#31070;&#32463;&#20803;&#23383;&#24149;&#29983;&#25104;&#65292;&#24182;&#36890;&#36807;&#19981;&#21516;&#31867;&#22411;&#30340;&#27880;&#24847;&#26426;&#21046;&#21644;&#22810;&#20010;&#27880;&#24847;&#21147;&#32467;&#26524;&#30340;&#27719;&#32858;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;MILAN&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.02744</link><description>&lt;p&gt;
MAMI: &#22810;&#27880;&#24847;&#21147;&#20114;&#20449;&#24687;&#29992;&#20110;&#38271;&#24207;&#21015;&#31070;&#32463;&#20803;&#23383;&#24149;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
MAMI: Multi-Attentional Mutual-Information for Long Sequence Neuron Captioning. (arXiv:2401.02744v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02744
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;MAMI&#26041;&#27861;&#65292;&#21033;&#29992;&#22810;&#27880;&#24847;&#21147;&#20114;&#20449;&#24687;&#29992;&#20110;&#38271;&#24207;&#21015;&#31070;&#32463;&#20803;&#23383;&#24149;&#29983;&#25104;&#65292;&#24182;&#36890;&#36807;&#19981;&#21516;&#31867;&#22411;&#30340;&#27880;&#24847;&#26426;&#21046;&#21644;&#22810;&#20010;&#27880;&#24847;&#21147;&#32467;&#26524;&#30340;&#27719;&#32858;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;MILAN&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#20803;&#26631;&#35760;&#26159;&#19968;&#31181;&#21487;&#35270;&#21270;&#29305;&#23450;&#31070;&#32463;&#20803;&#23545;&#28608;&#27963;&#35813;&#31070;&#32463;&#20803;&#30340;&#27169;&#24335;&#30340;&#34892;&#20026;&#21644;&#21453;&#24212;&#30340;&#26041;&#27861;&#12290;&#31070;&#32463;&#20803;&#26631;&#35760;&#25552;&#21462;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#26576;&#20123;&#31070;&#32463;&#20803;&#25429;&#33719;&#30340;&#29305;&#24449;&#20449;&#24687;&#65292;&#20854;&#20013;&#20043;&#19968;&#20351;&#29992;&#20102;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#26041;&#27861;&#12290;&#32534;&#30721;&#22120;&#21487;&#20197;&#26159;&#39044;&#35757;&#32451;&#30340;&#22522;&#20110;CNN&#30340;&#27169;&#22411;&#65292;&#35299;&#30721;&#22120;&#26159;&#22522;&#20110;RNN&#30340;&#27169;&#22411;&#29992;&#20110;&#25991;&#26412;&#29983;&#25104;&#12290;&#20043;&#21069;&#30340;&#24037;&#20316;&#65292;&#21363;MILAN&#65288;Mutual Information-guided Linguistic Annotation of Neuron&#65289;&#65292;&#23581;&#35797;&#20351;&#29992;&#20462;&#25913;&#21518;&#30340;Show, Attend, and Tell&#65288;SAT&#65289;&#27169;&#22411;&#36827;&#34892;&#31070;&#32463;&#20803;&#34892;&#20026;&#30340;&#21487;&#35270;&#21270;&#65292;&#22312;&#32534;&#30721;&#22120;&#20013;&#20351;&#29992;LSTM&#19982;Bahdanau&#27880;&#24847;&#21147;&#12290;MILAN&#22312;&#30701;&#24207;&#21015;&#31070;&#32463;&#20803;&#23383;&#24149;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#38271;&#24207;&#21015;&#31070;&#32463;&#20803;&#23383;&#24149;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#65292;&#22240;&#27492;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24076;&#26395;&#36890;&#36807;&#21033;&#29992;&#19981;&#21516;&#31867;&#22411;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#24182;&#39069;&#22806;&#28155;&#21152;&#33509;&#24178;&#27880;&#24847;&#21147;&#32467;&#26524;&#26469;&#36827;&#19968;&#27493;&#25552;&#21319;MILAN&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neuron labeling is an approach to visualize the behaviour and respond of a certain neuron to a certain pattern that activates the neuron. Neuron labeling extract information about the features captured by certain neurons in a deep neural network, one of which uses the encoder-decoder image captioning approach. The encoder used can be a pretrained CNN-based model and the decoder is an RNN-based model for text generation. Previous work, namely MILAN (Mutual Information-guided Linguistic Annotation of Neuron), has tried to visualize the neuron behaviour using modified Show, Attend, and Tell (SAT) model in the encoder, and LSTM added with Bahdanau attention in the decoder. MILAN can show great result on short sequence neuron captioning, but it does not show great result on long sequence neuron captioning, so in this work, we would like to improve the performance of MILAN even more by utilizing different kind of attention mechanism and additionally adding several attention result into one, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#24863;&#30693;&#32852;&#37030;&#20316;&#19994;&#35843;&#24230;&#65288;FairFedJS&#65289;&#26041;&#27861;&#65292;&#20197;&#30830;&#20445;&#23558;&#39640;&#38656;&#27714;&#30340;FL&#23458;&#25143;&#31471;&#25968;&#25454;&#38598;&#20844;&#24179;&#20998;&#37197;&#32473;&#38656;&#35201;&#23427;&#20204;&#30340;FL&#20316;&#19994;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2401.02740</link><description>&lt;p&gt;
&#20026;&#22810;&#20219;&#21153;&#32852;&#37030;&#23398;&#20064;&#25552;&#20379;&#20844;&#24179;&#24615;&#24863;&#30693;&#30340;&#20316;&#19994;&#35843;&#24230;
&lt;/p&gt;
&lt;p&gt;
Fairness-Aware Job Scheduling for Multi-Job Federated Learning. (arXiv:2401.02740v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02740
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#24863;&#30693;&#32852;&#37030;&#20316;&#19994;&#35843;&#24230;&#65288;FairFedJS&#65289;&#26041;&#27861;&#65292;&#20197;&#30830;&#20445;&#23558;&#39640;&#38656;&#27714;&#30340;FL&#23458;&#25143;&#31471;&#25968;&#25454;&#38598;&#20844;&#24179;&#20998;&#37197;&#32473;&#38656;&#35201;&#23427;&#20204;&#30340;FL&#20316;&#19994;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20351;&#22810;&#20010;&#25968;&#25454;&#25152;&#26377;&#32773;&#65288;&#21363;FL&#23458;&#25143;&#31471;&#65289;&#33021;&#22815;&#22312;&#19981;&#27844;&#38706;&#25935;&#24863;&#31169;&#20154;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20849;&#21516;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#29616;&#26377;&#30340;FL&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#22404;&#26029;&#22330;&#26223;&#65292;&#22312;&#35813;&#22330;&#26223;&#20013;&#65292;&#21333;&#20010;FL&#26381;&#21153;&#22120;&#22312;&#27599;&#36718;&#35757;&#32451;&#20013;&#36873;&#25321;&#19968;&#37096;&#20998;FL&#23458;&#25143;&#31471;&#26469;&#26356;&#26032;&#20854;&#26412;&#22320;&#27169;&#22411;&#12290;&#23454;&#38469;&#19978;&#65292;&#21487;&#33021;&#20250;&#26377;&#22810;&#20010;FL&#26381;&#21153;&#22120;&#21516;&#26102;&#23581;&#35797;&#20174;&#21516;&#19968;&#20010;&#27744;&#20013;&#36873;&#25321;&#23458;&#25143;&#31471;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39318;&#21019;&#30340;&#20844;&#24179;&#24863;&#30693;&#32852;&#37030;&#20316;&#19994;&#35843;&#24230;&#65288;FairFedJS&#65289;&#26041;&#27861;&#26469;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#12290;&#22522;&#20110;Lyapunov&#20248;&#21270;&#65292;&#23427;&#36890;&#36807;&#21516;&#26102;&#32771;&#34385;&#24403;&#21069;&#38656;&#27714;&#21644;&#20316;&#19994;&#20184;&#27454;&#20986;&#20215;&#65292;&#30830;&#20445;&#23558;&#39640;&#38656;&#27714;&#30340;FL&#23458;&#25143;&#31471;&#25968;&#25454;&#38598;&#20844;&#24179;&#20998;&#37197;&#32473;&#38656;&#35201;&#23427;&#20204;&#30340;FL&#20316;&#19994;&#65292;&#20197;&#38450;&#27490;&#31561;&#24453;&#26102;&#38388;&#36807;&#38271;&#12290;&#22522;&#20110;&#20004;&#20010;&#25968;&#25454;&#38598;&#23545;FairFedJS&#19982;&#22235;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26174;&#33879;&#20248;&#21183;&#12290;&#23427;&#22312;&#24179;&#22343;&#19978;&#20987;&#36133;&#20102;&#26368;&#20339;&#22522;&#20934;&#32447;31.9%&#21644;1.0%&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) enables multiple data owners (a.k.a. FL clients) to collaboratively train machine learning models without disclosing sensitive private data. Existing FL research mostly focuses on the monopoly scenario in which a single FL server selects a subset of FL clients to update their local models in each round of training. In practice, there can be multiple FL servers simultaneously trying to select clients from the same pool. In this paper, we propose a first-of-its-kind Fairness-aware Federated Job Scheduling (FairFedJS) approach to bridge this gap. Based on Lyapunov optimization, it ensures fair allocation of high-demand FL client datasets to FL jobs in need of them, by jointly considering the current demand and the job payment bids, in order to prevent prolonged waiting. Extensive experiments comparing FairFedJS against four state-of-the-art approaches on two datasets demonstrate its significant advantages. It outperforms the best baseline by 31.9% and 1.0% on avera
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#31232;&#30095;&#21046;&#20316;&#30340;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#19987;&#23478;&#28151;&#21512;&#24335;&#26550;&#26500;&#23558;&#23494;&#38598;&#27169;&#22411;&#36716;&#25442;&#20026;&#31232;&#30095;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#22312;&#27169;&#22411;&#23481;&#37327;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#21644;&#27867;&#21270;&#33021;&#21147;&#22686;&#24378;&#12290;</title><link>http://arxiv.org/abs/2401.02731</link><description>&lt;p&gt;
&#21442;&#25968;&#39640;&#25928;&#31232;&#30095;&#21046;&#20316;&#65306;&#20174;&#23494;&#38598;&#22411;&#21040;&#19987;&#23478;&#28151;&#21512;&#24335;&#29992;&#20110;&#36890;&#29992;&#20219;&#21153;&#30340;&#25351;&#20196;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Parameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for Instruction Tuning on General Tasks. (arXiv:2401.02731v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#31232;&#30095;&#21046;&#20316;&#30340;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#19987;&#23478;&#28151;&#21512;&#24335;&#26550;&#26500;&#23558;&#23494;&#38598;&#27169;&#22411;&#36716;&#25442;&#20026;&#31232;&#30095;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#22312;&#27169;&#22411;&#23481;&#37327;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#21644;&#27867;&#21270;&#33021;&#21147;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#36890;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#30456;&#24403;&#30340;&#29087;&#32451;&#31243;&#24230;&#12290;&#25351;&#20196;&#35843;&#25972;&#20316;&#20026;&#19968;&#31181;&#25104;&#21151;&#30340;&#33539;&#20363;&#65292;&#22686;&#24378;&#20102;LLMs&#36981;&#24490;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27169;&#22411;&#23481;&#37327;&#38480;&#21046;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#32463;&#24120;&#36935;&#21040;&#24615;&#33021;&#38480;&#21046;&#12290;&#22312;&#25351;&#20196;&#35843;&#25972;&#38454;&#27573;&#25193;&#23637;&#27169;&#22411;&#23481;&#37327;&#38754;&#20020;&#30528;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21442;&#25968;&#39640;&#25928;&#31232;&#30095;&#21046;&#20316;(PESC)&#65292;&#23427;&#20351;&#29992;&#19987;&#23478;&#28151;&#21512;&#24335;(MoE)&#26550;&#26500;&#23558;&#23494;&#38598;&#27169;&#22411;&#36716;&#25442;&#20026;&#31232;&#30095;&#27169;&#22411;&#12290;PESC&#23558;&#36866;&#37197;&#22120;&#38598;&#25104;&#21040;&#31232;&#30095;&#27169;&#22411;&#30340;MoE&#23618;&#20013;&#65292;&#21306;&#20998;&#19981;&#21516;&#30340;&#19987;&#23478;&#32780;&#19981;&#25913;&#21464;&#36825;&#20123;&#23618;&#20013;&#30340;&#20010;&#20307;&#26435;&#37325;&#12290;&#36825;&#31181;&#26041;&#27861;&#26174;&#33879;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#21644;GPU&#20869;&#23384;&#38656;&#27714;&#65292;&#36890;&#36807;&#26368;&#23567;&#30340;&#22686;&#21152;&#23454;&#29616;&#20102;&#27169;&#22411;&#23481;&#37327;&#30340;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated considerable proficiency in general natural language processing (NLP) tasks. Instruction tuning, a successful paradigm, enhances the ability of LLMs to follow natural language instructions and exhibit robust generalization across a wide range of tasks. However, these models often encounter performance limitations across multiple tasks due to constrained model capacity. Expanding this capacity during the instruction tuning phase poses significant challenges. To address this issue, we introduce a novel approach, Parameter-Efficient Sparsity Crafting (PESC), which transitions dense models to sparse models using a Mixture of Experts (MoE) architecture. PESC integrates adapters into the MoE layers of sparse models, differentiating experts without altering the individual weights within these layers. This method significantly reduces computational costs and GPU memory requirements, facilitating model capacity expansion through a minimal increase 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#29305;&#24449;&#31354;&#38388;&#24494;&#35843;AE&#65292;&#26174;&#33879;&#25552;&#39640;&#29616;&#26377;&#25915;&#20987;&#30340;&#30446;&#26631;&#20256;&#36882;&#24615;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21482;&#38656;&#23569;&#37327;&#30340;&#24494;&#35843;&#21363;&#21487;&#26222;&#36941;&#22320;&#22686;&#24378;&#25915;&#20987;&#30340;&#20256;&#36882;&#33021;&#21147;&#65292;&#24182;&#26174;&#31034;&#20986;&#31616;&#21333;&#30340;&#36845;&#20195;&#25915;&#20987;&#21487;&#20197;&#19982;&#36164;&#28304;&#23494;&#38598;&#22411;&#26041;&#27861;&#30456;&#23218;&#32654;&#29978;&#33267;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2401.02727</link><description>&lt;p&gt;
&#36890;&#36807;&#29305;&#24449;&#31354;&#38388;&#24494;&#35843;&#25552;&#39640;&#30446;&#26631;&#21487;&#20256;&#36882;&#24615;
&lt;/p&gt;
&lt;p&gt;
Enhancing targeted transferability via feature space fine-tuning. (arXiv:2401.02727v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02727
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#29305;&#24449;&#31354;&#38388;&#24494;&#35843;AE&#65292;&#26174;&#33879;&#25552;&#39640;&#29616;&#26377;&#25915;&#20987;&#30340;&#30446;&#26631;&#20256;&#36882;&#24615;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21482;&#38656;&#23569;&#37327;&#30340;&#24494;&#35843;&#21363;&#21487;&#26222;&#36941;&#22320;&#22686;&#24378;&#25915;&#20987;&#30340;&#20256;&#36882;&#33021;&#21147;&#65292;&#24182;&#26174;&#31034;&#20986;&#31616;&#21333;&#30340;&#36845;&#20195;&#25915;&#20987;&#21487;&#20197;&#19982;&#36164;&#28304;&#23494;&#38598;&#22411;&#26041;&#27861;&#30456;&#23218;&#32654;&#29978;&#33267;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#23545;&#38544;&#31169;&#20445;&#25252;&#30340;&#28508;&#21147;&#21644;&#28608;&#21457;&#40065;&#26834;&#24615;&#31070;&#32463;&#32593;&#32476;&#30340;&#33021;&#21147;&#65292;&#23545;&#25239;&#24615;&#31034;&#20363;&#65288;AEs&#65289;&#24050;&#32463;&#34987;&#24191;&#27867;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#20351;&#30446;&#26631;AE&#22312;&#26410;&#30693;&#27169;&#22411;&#20043;&#38388;&#21487;&#20256;&#36882;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#20026;&#20102;&#20943;&#36731;&#29616;&#26377;&#31616;&#21333;&#36845;&#20195;&#25915;&#20987;&#25152;&#29983;&#25104;&#30340;AE&#20013;&#24120;&#35265;&#30340;&#36807;&#25311;&#21512;&#22256;&#22659;&#65292;&#25105;&#20204;&#25552;&#20986;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#23545;&#20854;&#36827;&#34892;&#24494;&#35843;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20174;&#22522;&#32447;&#25915;&#20987;&#29983;&#25104;&#30340;AE&#24320;&#22987;&#65292;&#22312;&#28304;&#27169;&#22411;&#30340;&#20013;&#38388;&#23618;&#20013;&#40723;&#21169;&#26377;&#21161;&#20110;&#30446;&#26631;&#31867;&#21035;&#30340;&#29305;&#24449;&#65292;&#38459;&#30861;&#26377;&#21161;&#20110;&#21407;&#22987;&#31867;&#21035;&#30340;&#29305;&#24449;&#12290;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#20165;&#20351;&#29992;&#20960;&#27425;&#24494;&#35843;&#21363;&#21487;&#26174;&#33879;&#21644;&#26222;&#36941;&#22320;&#25552;&#39640;&#29616;&#26377;&#25915;&#20987;&#30340;&#30446;&#26631;&#20256;&#36882;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#36824;&#39564;&#35777;&#20102;&#31616;&#21333;&#30340;&#36845;&#20195;&#25915;&#20987;&#21487;&#20197;&#20135;&#29983;&#19982;&#36164;&#28304;&#23494;&#38598;&#22411;&#26041;&#27861;&#30456;&#24403;&#29978;&#33267;&#26356;&#22909;&#30340;&#20256;&#36882;&#24615;&#65292;&#21518;&#32773;&#20381;&#36182;&#20110;&#35757;&#32451;&#29305;&#23450;&#30446;&#26631;&#20998;&#31867;&#22120;&#25110;&#29983;&#25104;&#29305;&#23450;&#30340;AE&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial examples (AEs) have been extensively studied due to their potential for privacy protection and inspiring robust neural networks. However, making a targeted AE transferable across unknown models remains challenging. In this paper, to alleviate the overfitting dilemma common in an AE crafted by existing simple iterative attacks, we propose fine-tuning it in the feature space. Specifically, starting with an AE generated by a baseline attack, we encourage the features that contribute to the target class and discourage the features that contribute to the original class in a middle layer of the source model. Extensive experiments demonstrate that only a few iterations of fine-tuning can boost existing attacks in terms of targeted transferability nontrivially and universally. Our results also verify that the simple iterative attacks can yield comparable or even better transferability than the resource-intensive methods, which rely on training target-specific classifiers or generat
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26234;&#33021;&#22478;&#24066;&#20013;&#29615;&#22659;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#26412;&#20307;&#35770;&#65292;&#29992;&#20110;&#25551;&#36848;&#23545;&#35937;&#22522;&#30784;&#35774;&#26045;&#30340;&#35821;&#20041;&#32467;&#26500;&#65292;&#20197;&#20415;&#21033;&#29992;&#36830;&#25509;&#35774;&#22791;&#25552;&#20379;&#20010;&#24615;&#21270;&#26381;&#21153;&#12290;&#26412;&#20307;&#35770;&#22312;&#26234;&#33021;&#21487;&#31227;&#21160;&#24615;&#39046;&#22495;&#24471;&#21040;&#20102;&#24212;&#29992;&#65292;&#24182;&#19988;&#21487;&#36866;&#29992;&#20110;&#20854;&#20182;&#26234;&#33021;&#22478;&#24066;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2401.02726</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#26234;&#33021;&#22478;&#24066;&#20013;&#29615;&#22659;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#26412;&#20307;&#35770;
&lt;/p&gt;
&lt;p&gt;
Une ontologie pour les syst{\`e}mes multi-agents ambiants dans les villes intelligentes. (arXiv:2401.02726v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02726
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26234;&#33021;&#22478;&#24066;&#20013;&#29615;&#22659;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#26412;&#20307;&#35770;&#65292;&#29992;&#20110;&#25551;&#36848;&#23545;&#35937;&#22522;&#30784;&#35774;&#26045;&#30340;&#35821;&#20041;&#32467;&#26500;&#65292;&#20197;&#20415;&#21033;&#29992;&#36830;&#25509;&#35774;&#22791;&#25552;&#20379;&#20010;&#24615;&#21270;&#26381;&#21153;&#12290;&#26412;&#20307;&#35770;&#22312;&#26234;&#33021;&#21487;&#31227;&#21160;&#24615;&#39046;&#22495;&#24471;&#21040;&#20102;&#24212;&#29992;&#65292;&#24182;&#19988;&#21487;&#36866;&#29992;&#20110;&#20854;&#20182;&#26234;&#33021;&#22478;&#24066;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#22478;&#24066;&#27491;&#22312;&#37319;&#36141;&#21508;&#31181;&#36830;&#25509;&#35774;&#22791;&#65292;&#20197;&#20415;&#23558;&#20854;&#36716;&#21464;&#20026;&#8220;&#26234;&#33021;&#22478;&#24066;&#8221;&#12290;&#20026;&#20102;&#31649;&#29702;&#36825;&#20123;&#36830;&#25509;&#23545;&#35937;&#30340;&#22823;&#37327;&#25968;&#25454;&#65292;&#21487;&#20197;&#23558;&#31216;&#20026;&#26234;&#33021;&#20307;&#30340;&#33258;&#20027;&#36719;&#20214;&#23454;&#20307;&#38468;&#21152;&#21040;&#23427;&#20204;&#19978;&#65292;&#20197;&#21327;&#20316;&#24182;&#21033;&#29992;&#36825;&#20123;&#35774;&#22791;&#25552;&#20379;&#20010;&#24615;&#21270;&#26381;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#23545;&#35937;&#22522;&#30784;&#35774;&#26045;&#38656;&#35201;&#35821;&#20041;&#32467;&#26500;&#21270;&#25165;&#33021;&#34987;&#21033;&#29992;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#26159;&#19968;&#31181;&#26684;&#24335;&#21270;&#20026;OWL&#30340;&#26412;&#20307;&#35770;&#65292;&#25551;&#36848;&#20102;&#23545;&#35937;&#22522;&#30784;&#35774;&#26045;&#65292;&#23427;&#20204;&#19982;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#32452;&#32455;&#20043;&#38388;&#30340;&#20851;&#32852;&#20197;&#21450;&#26681;&#25454;&#31995;&#32479;&#29992;&#25143;&#30340;&#38656;&#27714;&#25552;&#20379;&#30340;&#26381;&#21153;&#12290;&#35813;&#26412;&#20307;&#35770;&#22312;&#26234;&#33021;&#21487;&#31227;&#21160;&#24615;&#39046;&#22495;&#24212;&#29992;&#20110;&#20943;&#23569;&#27969;&#21160;&#24615;&#30340;&#20154;&#32676;&#65292;&#24182;&#19988;&#21487;&#20197;&#24212;&#29992;&#21040;&#20854;&#20182;&#26234;&#33021;&#22478;&#24066;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Towns and cities are currently equipping themselves with a host of connected devices, with a view to transforming themselves into ''smart cities''. To manage this mass of connected objects, autonomous software entities, known as agents, can be attached to them to cooperate and use these devices to offer personalized services. However, this object infrastructure needs to be semantically structured in order to be exploited. This is why the proposal of this article is an ontology, formatted in OWL, describing the object infrastructures, their links with the organization of the multi-agent system and the services to be delivered according to the users of the system. The ontology is applied to smart mobility for people with reduced mobility, and could be adapted to other smart city axes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#19981;&#25104;&#23545;&#30340;&#30495;&#23454;&#25968;&#25454;&#20013;&#23398;&#20064;&#22270;&#20687;&#21435;&#33707;&#23572;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21512;&#25104;&#19982;&#28165;&#26224;&#22270;&#20687;&#23545;&#24212;&#30340;&#20266;&#33707;&#23572;&#22270;&#20687;&#65292;&#24182;&#23454;&#29616;&#20102;&#22810;&#26679;&#21270;&#33707;&#23572;&#29305;&#24449;&#21644;&#31867;&#20284;&#30495;&#23454;&#26080;&#33707;&#23572;&#22270;&#20687;&#30340;&#32454;&#33410;&#65292;&#26368;&#32456;&#21435;&#38500;&#22270;&#20687;&#20013;&#30340;&#33707;&#23572;&#25928;&#24212;&#12290;</title><link>http://arxiv.org/abs/2401.02719</link><description>&lt;p&gt;
&#20174;&#19981;&#25104;&#23545;&#30340;&#30495;&#23454;&#25968;&#25454;&#20013;&#23398;&#20064;&#22270;&#20687;&#21435;&#33707;&#23572;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Learning Image Demoireing from Unpaired Real Data. (arXiv:2401.02719v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02719
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#19981;&#25104;&#23545;&#30340;&#30495;&#23454;&#25968;&#25454;&#20013;&#23398;&#20064;&#22270;&#20687;&#21435;&#33707;&#23572;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21512;&#25104;&#19982;&#28165;&#26224;&#22270;&#20687;&#23545;&#24212;&#30340;&#20266;&#33707;&#23572;&#22270;&#20687;&#65292;&#24182;&#23454;&#29616;&#20102;&#22810;&#26679;&#21270;&#33707;&#23572;&#29305;&#24449;&#21644;&#31867;&#20284;&#30495;&#23454;&#26080;&#33707;&#23572;&#22270;&#20687;&#30340;&#32454;&#33410;&#65292;&#26368;&#32456;&#21435;&#38500;&#22270;&#20687;&#20013;&#30340;&#33707;&#23572;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#19987;&#27880;&#20110;&#35299;&#20915;&#22270;&#20687;&#21435;&#33707;&#23572;&#38382;&#39064;&#12290;&#19982;&#29616;&#26377;&#22823;&#37327;&#30740;&#31350;&#20381;&#36182;&#20110;&#25104;&#23545;&#30495;&#23454;&#25968;&#25454;&#23398;&#20064;&#30340;&#24773;&#20917;&#19981;&#21516;&#65292;&#25105;&#20204;&#23581;&#35797;&#20174;&#19981;&#25104;&#23545;&#30340;&#30495;&#23454;&#25968;&#25454;&#20013;&#23398;&#20064;&#33707;&#23572;&#38382;&#39064;&#27169;&#22411;&#65292;&#21363;&#20851;&#32852;&#21040;&#19981;&#30456;&#20851;&#28165;&#26224;&#22270;&#20687;&#30340;&#33707;&#23572;&#22270;&#20687;&#12290;&#25552;&#20986;&#30340;&#26041;&#27861;&#31216;&#20026;&#26080;&#23545;&#40784;&#21435;&#33707;&#23572;&#65288;Unpaired Demoireing&#65292;UnDeM&#65289;&#65292;&#20174;&#19981;&#25104;&#23545;&#25968;&#25454;&#38598;&#20013;&#21512;&#25104;&#20266;&#33707;&#23572;&#22270;&#20687;&#65292;&#20026;&#35757;&#32451;&#21435;&#33707;&#23572;&#27169;&#22411;&#29983;&#25104;&#19982;&#28165;&#26224;&#22270;&#20687;&#23545;&#24212;&#30340;&#22270;&#20687;&#23545;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#23558;&#30495;&#23454;&#33707;&#23572;&#22270;&#20687;&#20998;&#25104;&#23567;&#22359;&#24182;&#25353;&#29031;&#33707;&#23572;&#22797;&#26434;&#31243;&#24230;&#36827;&#34892;&#20998;&#32452;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#33707;&#23572;&#29983;&#25104;&#26694;&#26550;&#65292;&#20197;&#21512;&#25104;&#20855;&#26377;&#22810;&#26679;&#21270;&#33707;&#23572;&#29305;&#24449;&#30340;&#33707;&#23572;&#22270;&#20687;&#65292;&#31867;&#20284;&#20110;&#30495;&#23454;&#30340;&#33707;&#23572;&#22359;&#65292;&#24182;&#20855;&#26377;&#31867;&#20284;&#30495;&#23454;&#26080;&#33707;&#23572;&#22270;&#20687;&#30340;&#32454;&#33410;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#33258;&#36866;&#24212;&#21435;&#22122;&#26041;&#27861;&#65292;&#20197;&#28040;&#38500;&#20302;&#36136;&#37327;&#30340;&#20266;&#33707;&#23572;&#22270;&#20687;&#23545;&#21435;&#33707;&#23572;&#27169;&#22411;&#30340;&#23398;&#20064;&#20135;&#29983;&#30340;&#19981;&#21033;&#24433;&#21709;&#12290;&#25105;&#20204;&#22312;&#24120;&#29992;&#30340;FHDMi&#21644;UHDM&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper focuses on addressing the issue of image demoireing. Unlike the large volume of existing studies that rely on learning from paired real data, we attempt to learn a demoireing model from unpaired real data, i.e., moire images associated with irrelevant clean images. The proposed method, referred to as Unpaired Demoireing (UnDeM), synthesizes pseudo moire images from unpaired datasets, generating pairs with clean images for training demoireing models. To achieve this, we divide real moire images into patches and group them in compliance with their moire complexity. We introduce a novel moire generation framework to synthesize moire images with diverse moire features, resembling real moire patches, and details akin to real moire-free images. Additionally, we introduce an adaptive denoise method to eliminate the low-quality pseudo moire images that adversely impact the learning of demoireing models. We conduct extensive experiments on the commonly-used FHDMi and UHDM datasets. R
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#20114;&#34917;&#20449;&#24687;&#30456;&#20114;&#23398;&#20064;&#65288;CIML&#65289;&#30340;&#26694;&#26550;&#65292;&#22312;&#22810;&#27169;&#24577;&#21307;&#23398;&#24433;&#20687;&#20998;&#21106;&#20013;&#35299;&#20915;&#20102;&#27169;&#24577;&#38388;&#20887;&#20313;&#20449;&#24687;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#36890;&#36807;&#37319;&#29992;&#21152;&#27861;&#21644;&#20219;&#21153;&#20998;&#35299;&#30340;&#26041;&#27861;&#65292;CIML&#25104;&#21151;&#22320;&#28040;&#38500;&#20102;&#20887;&#20313;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#20998;&#21106;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.02717</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#21307;&#23398;&#24433;&#20687;&#20998;&#21106;&#30340;&#20114;&#34917;&#20449;&#24687;&#30456;&#20114;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Complementary Information Mutual Learning for Multimodality Medical Image Segmentation. (arXiv:2401.02717v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02717
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#20114;&#34917;&#20449;&#24687;&#30456;&#20114;&#23398;&#20064;&#65288;CIML&#65289;&#30340;&#26694;&#26550;&#65292;&#22312;&#22810;&#27169;&#24577;&#21307;&#23398;&#24433;&#20687;&#20998;&#21106;&#20013;&#35299;&#20915;&#20102;&#27169;&#24577;&#38388;&#20887;&#20313;&#20449;&#24687;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#36890;&#36807;&#37319;&#29992;&#21152;&#27861;&#21644;&#20219;&#21153;&#20998;&#35299;&#30340;&#26041;&#27861;&#65292;CIML&#25104;&#21151;&#22320;&#28040;&#38500;&#20102;&#20887;&#20313;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#20998;&#21106;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#21307;&#23398;&#24433;&#20687;&#30340;&#23616;&#38480;&#24615;&#21644;&#32959;&#30244;&#20449;&#21495;&#30340;&#22810;&#26679;&#24615;&#65292;&#25918;&#23556;&#31185;&#21307;&#29983;&#24517;&#39035;&#21033;&#29992;&#22810;&#27169;&#24577;&#22270;&#20687;&#36827;&#34892;&#32959;&#30244;&#20998;&#21106;&#21644;&#35786;&#26029;&#12290;&#36825;&#23548;&#33268;&#20102;&#22810;&#27169;&#24577;&#23398;&#20064;&#22312;&#20998;&#21106;&#20013;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#27169;&#24577;&#20043;&#38388;&#30340;&#20887;&#20313;&#24615;&#32473;&#29616;&#26377;&#30340;&#22522;&#20110;&#20943;&#27861;&#30340;&#32852;&#21512;&#23398;&#20064;&#26041;&#27861;&#24102;&#26469;&#20102;&#25361;&#25112;&#65292;&#20363;&#22914;&#38169;&#35823;&#21028;&#26029;&#27169;&#24577;&#30340;&#37325;&#35201;&#24615;&#65292;&#24573;&#35270;&#29305;&#23450;&#30340;&#27169;&#24577;&#20449;&#24687;&#65292;&#20197;&#21450;&#22686;&#21152;&#35748;&#30693;&#36127;&#33655;&#12290;&#36825;&#20123;&#26840;&#25163;&#30340;&#38382;&#39064;&#26368;&#32456;&#38477;&#20302;&#20102;&#20998;&#21106;&#30340;&#20934;&#30830;&#24615;&#24182;&#22686;&#21152;&#20102;&#36807;&#25311;&#21512;&#30340;&#39118;&#38505;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20114;&#34917;&#20449;&#24687;&#30456;&#20114;&#23398;&#20064;&#65288;CIML&#65289;&#26694;&#26550;&#65292;&#21487;&#20197;&#23545;&#27169;&#24577;&#38388;&#20887;&#20313;&#20449;&#24687;&#30340;&#36127;&#38754;&#24433;&#21709;&#36827;&#34892;&#25968;&#23398;&#24314;&#27169;&#21644;&#35299;&#20915;&#12290;CIML&#37319;&#29992;&#20102;&#21152;&#27861;&#30340;&#24605;&#24819;&#65292;&#24182;&#36890;&#36807;&#24402;&#32435;&#20559;&#32622;&#39537;&#21160;&#30340;&#20219;&#21153;&#20998;&#35299;&#21644;&#22522;&#20110;&#28040;&#24687;&#20256;&#36882;&#30340;&#20887;&#20313;&#24615;&#36807;&#28388;&#26469;&#28040;&#38500;&#27169;&#24577;&#38388;&#30340;&#20887;&#20313;&#20449;&#24687;&#12290;CIML&#23558;&#22810;&#27169;&#24577;&#20998;&#21106;&#20219;&#21153;&#39318;&#20808;&#20998;&#35299;&#20026;&#22810;&#20010;&#23376;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Radiologists must utilize multiple modal images for tumor segmentation and diagnosis due to the limitations of medical imaging and the diversity of tumor signals. This leads to the development of multimodal learning in segmentation. However, the redundancy among modalities creates challenges for existing subtraction-based joint learning methods, such as misjudging the importance of modalities, ignoring specific modal information, and increasing cognitive load. These thorny issues ultimately decrease segmentation accuracy and increase the risk of overfitting. This paper presents the complementary information mutual learning (CIML) framework, which can mathematically model and address the negative impact of inter-modal redundant information. CIML adopts the idea of addition and removes inter-modal redundant information through inductive bias-driven task decomposition and message passing-based redundancy filtering. CIML first decomposes the multimodal segmentation task into multiple subta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32467;&#26500;&#30693;&#35782;&#25552;&#28860;&#65288;SKR&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#35299;&#20915;&#20102;&#22270;&#32423;&#34920;&#31034;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2401.02713</link><description>&lt;p&gt;
&#36890;&#36807;&#32467;&#26500;&#30693;&#35782;&#25552;&#28860;&#36827;&#34892;&#22270;&#32423;&#34507;&#30333;&#36136;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Graph-level Protein Representation Learning by Structure Knowledge Refinement. (arXiv:2401.02713v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02713
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32467;&#26500;&#30693;&#35782;&#25552;&#28860;&#65288;SKR&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#35299;&#20915;&#20102;&#22270;&#32423;&#34920;&#31034;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#20197;&#38750;&#30417;&#30563;&#26041;&#24335;&#22312;&#25972;&#20010;&#22270;&#32423;&#21035;&#19978;&#23398;&#20064;&#34920;&#31034;&#12290;&#23398;&#20064;&#22270;&#32423;&#34920;&#31034;&#22312;&#35832;&#22914;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#12289;&#34507;&#30333;&#36136;&#32467;&#26500;&#29305;&#24449;&#25552;&#21462;&#21644;&#31038;&#20132;&#32593;&#32476;&#20998;&#26512;&#31561;&#21508;&#31181;&#23454;&#38469;&#38382;&#39064;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#20027;&#27969;&#26041;&#27861;&#26159;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#20419;&#36827;&#22270;&#29305;&#24449;&#25552;&#21462;&#65292;&#31216;&#20026;&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;GCL&#65289;&#12290;&#23613;&#31649;GCL&#26377;&#25928;&#65292;&#20294;&#22312;&#23545;&#27604;&#23398;&#20064;&#20013;&#23384;&#22312;&#19968;&#20123;&#22797;&#26434;&#38382;&#39064;&#65292;&#27604;&#22914;&#34394;&#20551;&#36127;&#26679;&#26412;&#23545;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;GCL&#20013;&#30340;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#23545;&#19981;&#21516;&#30340;&#22270;&#25968;&#25454;&#38598;&#36866;&#24212;&#24615;&#36739;&#24369;&#12290;&#21463;&#21040;&#36825;&#20123;&#38382;&#39064;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#32467;&#26500;&#30693;&#35782;&#25552;&#28860;&#65288;SKR&#65289;&#65292;&#23427;&#21033;&#29992;&#25968;&#25454;&#32467;&#26500;&#30830;&#23450;&#19968;&#23545;&#26679;&#26412;&#26159;&#27491;&#26679;&#26412;&#36824;&#26159;&#36127;&#26679;&#26412;&#30340;&#27010;&#29575;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#31574;&#30053;&#65292;&#33021;&#22815;&#33258;&#28982;&#22320;&#20445;&#30041;&#21407;&#22987;&#25968;&#25454;&#30340;&#35821;&#20041;&#21547;&#20041;&#65292;&#24182;&#19988;&#19982;&#25105;&#20204;&#30340;SKR&#26694;&#26550;&#20860;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper focuses on learning representation on the whole graph level in an unsupervised manner. Learning graph-level representation plays an important role in a variety of real-world issues such as molecule property prediction, protein structure feature extraction, and social network analysis. The mainstream method is utilizing contrastive learning to facilitate graph feature extraction, known as Graph Contrastive Learning (GCL). GCL, although effective, suffers from some complications in contrastive learning, such as the effect of false negative pairs. Moreover, augmentation strategies in GCL are weakly adaptive to diverse graph datasets. Motivated by these problems, we propose a novel framework called Structure Knowledge Refinement (SKR) which uses data structure to determine the probability of whether a pair is positive or negative. Meanwhile, we propose an augmentation strategy that naturally preserves the semantic meaning of the original data and is compatible with our SKR frame
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#23637;&#25628;&#32034;&#31354;&#38388;&#21644;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#20844;&#24335;&#21270;alpha&#38598;&#26469;&#29983;&#25104;&#21327;&#21516;&#20844;&#24335;&#21270;alpha&#22240;&#23376;&#65292;&#20174;&#32780;&#25552;&#39640;&#37327;&#21270;&#20132;&#26131;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.02710</link><description>&lt;p&gt;
&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#37327;&#21270;&#20132;&#26131;&#20013;&#21327;&#21516;&#20844;&#24335;Alpha&#29983;&#25104;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Synergistic Formulaic Alpha Generation for Quantitative Trading based on Reinforcement Learning. (arXiv:2401.02710v1 [cs.CE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02710
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#23637;&#25628;&#32034;&#31354;&#38388;&#21644;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#20844;&#24335;&#21270;alpha&#38598;&#26469;&#29983;&#25104;&#21327;&#21516;&#20844;&#24335;&#21270;alpha&#22240;&#23376;&#65292;&#20174;&#32780;&#25552;&#39640;&#37327;&#21270;&#20132;&#26131;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#24335;&#21270;alpha&#22240;&#23376;&#30340;&#25366;&#25496;&#26159;&#25351;&#22312;&#32929;&#31080;&#24066;&#22330;&#20013;&#21457;&#29616;&#21644;&#24320;&#21457;&#29305;&#23450;&#30340;&#22240;&#23376;&#25110;&#25351;&#26631;&#65288;&#31216;&#20026;alpha&#22240;&#23376;&#65289;&#20197;&#29992;&#20110;&#37327;&#21270;&#20132;&#26131;&#30340;&#36807;&#31243;&#12290;&#20026;&#20102;&#22312;&#24222;&#22823;&#30340;&#25628;&#32034;&#31354;&#38388;&#20013;&#39640;&#25928;&#22320;&#21457;&#29616;alpha&#22240;&#23376;&#65292;&#24120;&#24120;&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25193;&#23637;&#25628;&#32034;&#31354;&#38388;&#24182;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#20844;&#24335;&#21270;alpha&#38598;&#20316;&#20026;&#21021;&#22987;&#31181;&#23376;&#20540;&#26469;&#29983;&#25104;&#21327;&#21516;&#20844;&#24335;&#21270;alpha&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#20449;&#24687;&#31995;&#25968;&#65288;IC&#65289;&#21644;&#25490;&#21517;&#20449;&#24687;&#31995;&#25968;&#65288;Rank IC&#65289;&#20316;&#20026;&#27169;&#22411;&#30340;&#24615;&#33021;&#35780;&#20272;&#25351;&#26631;&#12290;&#36890;&#36807;&#20351;&#29992;CSI300&#24066;&#22330;&#25968;&#25454;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#38469;&#25237;&#36164;&#27169;&#25311;&#65292;&#24182;&#35266;&#23519;&#21040;&#19982;&#29616;&#26377;&#25216;&#26415;&#30456;&#27604;&#30340;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mining of formulaic alpha factors refers to the process of discovering and developing specific factors or indicators (referred to as alpha factors) for quantitative trading in stock market. To efficiently discover alpha factors in vast search space, reinforcement learning (RL) is commonly employed. This paper proposes a method to enhance existing alpha factor mining approaches by expanding a search space and utilizing pretrained formulaic alpha set as initial seed values to generate synergistic formulaic alpha. We employ information coefficient (IC) and rank information coefficient (Rank IC) as performance evaluation metrics for the model. Using CSI300 market data, we conducted real investment simulations and observed significant performance improvement compared to existing techniques.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#19981;&#21516;&#39046;&#22495;&#20013;&#32858;&#31867;&#24503;&#35821;&#25991;&#26412;&#23884;&#20837;&#24615;&#33021;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#32858;&#31867;&#31639;&#27861;&#36827;&#34892;&#20102;&#21021;&#27493;&#20998;&#26512;&#12290;&#32467;&#26524;&#26174;&#31034;&#21333;&#35821;&#21644;&#22810;&#35821;&#27169;&#22411;&#34920;&#29616;&#24378;&#21170;&#65292;&#32553;&#20943;&#23884;&#20837;&#32500;&#24230;&#21487;&#20197;&#25552;&#39640;&#32858;&#31867;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;&#25345;&#32493;&#39044;&#35757;&#32451;&#24503;&#35821;BERT&#27169;&#22411;&#21487;&#33021;&#23545;&#30701;&#25991;&#26412;&#24615;&#33021;&#26377;&#26174;&#33879;&#25913;&#21892;&#12290;&#25152;&#26377;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#22343;&#20844;&#24320;&#21487;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.02709</link><description>&lt;p&gt;
&#24503;&#35821;&#25991;&#26412;&#23884;&#20837;&#32858;&#31867;&#22522;&#20934;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
German Text Embedding Clustering Benchmark. (arXiv:2401.02709v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02709
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#19981;&#21516;&#39046;&#22495;&#20013;&#32858;&#31867;&#24503;&#35821;&#25991;&#26412;&#23884;&#20837;&#24615;&#33021;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#32858;&#31867;&#31639;&#27861;&#36827;&#34892;&#20102;&#21021;&#27493;&#20998;&#26512;&#12290;&#32467;&#26524;&#26174;&#31034;&#21333;&#35821;&#21644;&#22810;&#35821;&#27169;&#22411;&#34920;&#29616;&#24378;&#21170;&#65292;&#32553;&#20943;&#23884;&#20837;&#32500;&#24230;&#21487;&#20197;&#25552;&#39640;&#32858;&#31867;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;&#25345;&#32493;&#39044;&#35757;&#32451;&#24503;&#35821;BERT&#27169;&#22411;&#21487;&#33021;&#23545;&#30701;&#25991;&#26412;&#24615;&#33021;&#26377;&#26174;&#33879;&#25913;&#21892;&#12290;&#25152;&#26377;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#22343;&#20844;&#24320;&#21487;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#32858;&#31867;&#24503;&#35821;&#25991;&#26412;&#23884;&#20837;&#30340;&#24615;&#33021;&#12290;&#36825;&#20010;&#22522;&#20934;&#27979;&#35797;&#26159;&#30001;&#23545;&#32858;&#31867;&#31070;&#32463;&#25991;&#26412;&#23884;&#20837;&#22312;&#38656;&#35201;&#23545;&#25991;&#26412;&#36827;&#34892;&#20998;&#32452;&#65288;&#22914;&#20027;&#39064;&#24314;&#27169;&#65289;&#30340;&#20219;&#21153;&#20013;&#30340;&#22686;&#21152;&#20351;&#29992;&#25152;&#39537;&#21160;&#30340;&#65292;&#24182;&#19988;&#22312;&#29616;&#26377;&#22522;&#20934;&#27979;&#35797;&#20013;&#38656;&#35201;&#24503;&#35821;&#36164;&#28304;&#12290;&#25105;&#20204;&#23545;&#19968;&#31995;&#21015;&#39044;&#35757;&#32451;&#30340;&#21333;&#35821;&#21644;&#22810;&#35821;&#27169;&#22411;&#22312;&#19981;&#21516;&#32858;&#31867;&#31639;&#27861;&#30340;&#32467;&#26524;&#19978;&#36827;&#34892;&#20102;&#21021;&#27493;&#20998;&#26512;&#12290;&#32467;&#26524;&#21253;&#25324;&#34920;&#29616;&#24378;&#21170;&#30340;&#21333;&#35821;&#21644;&#22810;&#35821;&#27169;&#22411;&#12290;&#32553;&#20943;&#23884;&#20837;&#30340;&#32500;&#24230;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#32858;&#31867;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#19982;&#24503;&#35821;BERT&#27169;&#22411;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#30340;&#23454;&#39564;&#65292;&#20197;&#35780;&#20272;&#36825;&#31181;&#39069;&#22806;&#35757;&#32451;&#30340;&#22909;&#22788;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#23545;&#20110;&#30701;&#25991;&#26412;&#21487;&#33021;&#23384;&#22312;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;&#25152;&#26377;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#22343;&#21487;&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work introduces a benchmark assessing the performance of clustering German text embeddings in different domains. This benchmark is driven by the increasing use of clustering neural text embeddings in tasks that require the grouping of texts (such as topic modeling) and the need for German resources in existing benchmarks. We provide an initial analysis for a range of pre-trained mono- and multilingual models evaluated on the outcome of different clustering algorithms. Results include strong performing mono- and multilingual models. Reducing the dimensions of embeddings can further improve clustering. Additionally, we conduct experiments with continued pre-training for German BERT models to estimate the benefits of this additional training. Our experiments suggest that significant performance improvements are possible for short text. All code and datasets are publicly available.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#26102;&#38388;&#30340;&#19977;&#20803;&#32452;&#22352;&#26631;&#25439;&#22833;&#20989;&#25968;TripleSurv&#65292;&#36890;&#36807;&#24341;&#20837;&#26679;&#26412;&#23545;&#20043;&#38388;&#30340;&#29983;&#23384;&#26102;&#38388;&#24046;&#24322;&#26469;&#40723;&#21169;&#27169;&#22411;&#37327;&#21270;&#25490;&#21517;&#30456;&#23545;&#39118;&#38505;&#65292;&#20174;&#32780;&#25552;&#39640;&#29983;&#23384;&#20998;&#26512;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.02708</link><description>&lt;p&gt;
TripleSurv&#65306;&#36866;&#24212;&#26102;&#38388;&#19977;&#20803;&#32452;&#22352;&#26631;&#25439;&#22833;&#29992;&#20110;&#29983;&#23384;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
TripleSurv: Triplet Time-adaptive Coordinate Loss for Survival Analysis. (arXiv:2401.02708v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02708
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#26102;&#38388;&#30340;&#19977;&#20803;&#32452;&#22352;&#26631;&#25439;&#22833;&#20989;&#25968;TripleSurv&#65292;&#36890;&#36807;&#24341;&#20837;&#26679;&#26412;&#23545;&#20043;&#38388;&#30340;&#29983;&#23384;&#26102;&#38388;&#24046;&#24322;&#26469;&#40723;&#21169;&#27169;&#22411;&#37327;&#21270;&#25490;&#21517;&#30456;&#23545;&#39118;&#38505;&#65292;&#20174;&#32780;&#25552;&#39640;&#29983;&#23384;&#20998;&#26512;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#23384;&#20998;&#26512;&#20013;&#30340;&#19968;&#20010;&#26680;&#24515;&#25361;&#25112;&#26159;&#23545;&#34987;&#25130;&#23614;&#30340;&#20107;&#20214;&#26102;&#38388;&#25968;&#25454;&#36827;&#34892;&#24314;&#27169;&#65292;&#20854;&#20013;&#24863;&#20852;&#36259;&#30340;&#20107;&#20214;&#21487;&#33021;&#26159;&#27515;&#20129;&#12289;&#22833;&#36133;&#25110;&#29305;&#23450;&#20107;&#20214;&#30340;&#21457;&#29983;&#12290;&#36807;&#21435;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#25490;&#24207;&#25439;&#22833;&#21644;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65288;MLE&#65289;&#25439;&#22833;&#20989;&#25968;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#29983;&#23384;&#20998;&#26512;&#12290;&#28982;&#32780;&#65292;&#25490;&#24207;&#25439;&#22833;&#20165;&#20851;&#27880;&#29983;&#23384;&#26102;&#38388;&#25490;&#21517;&#65292;&#19981;&#32771;&#34385;&#26679;&#26412;&#23545;&#20110;&#30830;&#20999;&#29983;&#23384;&#26102;&#38388;&#20540;&#30340;&#28508;&#22312;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;MLE&#26159;&#26080;&#30028;&#30340;&#19988;&#23481;&#26131;&#21463;&#21040;&#24322;&#24120;&#20540;&#65288;&#20363;&#22914;&#65292;&#34987;&#25130;&#23614;&#25968;&#25454;&#65289;&#30340;&#24433;&#21709;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#24314;&#27169;&#24615;&#33021;&#36739;&#24046;&#12290;&#20026;&#20102;&#22788;&#29702;&#23398;&#20064;&#36807;&#31243;&#30340;&#22797;&#26434;&#24615;&#24182;&#21033;&#29992;&#26377;&#20215;&#20540;&#30340;&#29983;&#23384;&#26102;&#38388;&#20540;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#26102;&#38388;&#19977;&#20803;&#32452;&#22352;&#26631;&#25439;&#22833;&#20989;&#25968;TripleSurv&#65292;&#36890;&#36807;&#23558;&#26679;&#26412;&#23545;&#20043;&#38388;&#30340;&#29983;&#23384;&#26102;&#38388;&#24046;&#24322;&#24341;&#20837;&#25490;&#24207;&#20013;&#65292;&#20197;&#40723;&#21169;&#27169;&#22411;&#37327;&#21270;&#25490;&#21517;&#30456;&#23545;&#39118;&#38505;&#65292;&#26368;&#32456;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
A core challenge in survival analysis is to model the distribution of censored time-to-event data, where the event of interest may be a death, failure, or occurrence of a specific event. Previous studies have showed that ranking and maximum likelihood estimation (MLE)loss functions are widely-used for survival analysis. However, ranking loss only focus on the ranking of survival time and does not consider potential effect of samples for exact survival time values. Furthermore, the MLE is unbounded and easily subject to outliers (e.g., censored data), which may cause poor performance of modeling. To handle the complexities of learning process and exploit valuable survival time values, we propose a time-adaptive coordinate loss function, TripleSurv, to achieve adaptive adjustments by introducing the differences in the survival time between sample pairs into the ranking, which can encourage the model to quantitatively rank relative risk of pairs, ultimately enhancing the accuracy of predi
&lt;/p&gt;</description></item><item><title>XUAT-Copilot&#26159;&#19968;&#20010;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#31995;&#32479;&#65292;&#26088;&#22312;&#25552;&#39640;&#33258;&#21160;&#29992;&#25143;&#39564;&#25910;&#27979;&#35797;&#30340;&#33258;&#21160;&#21270;&#27700;&#24179;&#21644;&#27979;&#35797;&#33050;&#26412;&#29983;&#25104;&#38454;&#27573;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.02705</link><description>&lt;p&gt;
XUAT-Copilot: &#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#31995;&#32479;&#29992;&#20110;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33258;&#21160;&#29992;&#25143;&#39564;&#25910;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
XUAT-Copilot: Multi-Agent Collaborative System for Automated User Acceptance Testing with Large Language Model. (arXiv:2401.02705v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02705
&lt;/p&gt;
&lt;p&gt;
XUAT-Copilot&#26159;&#19968;&#20010;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#31995;&#32479;&#65292;&#26088;&#22312;&#25552;&#39640;&#33258;&#21160;&#29992;&#25143;&#39564;&#25910;&#27979;&#35797;&#30340;&#33258;&#21160;&#21270;&#27700;&#24179;&#21644;&#27979;&#35797;&#33050;&#26412;&#29983;&#25104;&#38454;&#27573;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#20960;&#24180;&#65292;&#25105;&#20204;&#33268;&#21147;&#20110;&#33258;&#21160;&#21270;&#24494;&#20449;&#25903;&#20184;&#30340;&#29992;&#25143;&#39564;&#25910;&#27979;&#35797;&#65288;UAT&#65289;&#36807;&#31243;&#65292;&#36825;&#26159;&#20013;&#22269;&#26368;&#20855;&#24433;&#21709;&#21147;&#30340;&#31227;&#21160;&#25903;&#20184;&#24212;&#29992;&#20043;&#19968;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;XUAT&#30340;&#31995;&#32479;&#29992;&#20110;&#36825;&#20010;&#30446;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#24403;&#21069;&#31995;&#32479;&#20013;&#20173;&#28982;&#23384;&#22312;&#19968;&#20010;&#20154;&#21147;&#23494;&#38598;&#38454;&#27573;&#65292;&#21363;&#27979;&#35797;&#33050;&#26412;&#30340;&#29983;&#25104;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#38598;&#20013;&#30740;&#31350;&#25552;&#39640;&#24403;&#21069;&#31995;&#32479;&#33258;&#21160;&#21270;&#27700;&#24179;&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#27979;&#35797;&#33050;&#26412;&#29983;&#25104;&#38454;&#27573;&#12290;&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#23637;&#31034;&#20102;&#33719;&#21462;&#20154;&#31867;&#26234;&#33021;&#30340;&#28508;&#21147;&#65292;&#24182;&#19988;&#24050;&#32463;&#24418;&#25104;&#20102;&#19968;&#20010;&#19981;&#26029;&#22686;&#38271;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#20351;&#29992;LLMs&#20316;&#20026;&#33258;&#20027;&#26234;&#33021;&#20307;&#26469;&#33719;&#24471;&#31867;&#20284;&#20154;&#31867;&#30340;&#20915;&#31574;&#33021;&#21147;&#12290;&#21463;&#21040;&#36825;&#20123;&#24037;&#20316;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;LLM&#30340;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#31995;&#32479;&#65292;&#21517;&#20026;XUAT-Copilot&#65292;&#29992;&#20110;&#33258;&#21160;UAT&#12290;&#35813;&#31995;&#32479;&#20027;&#35201;&#30001;&#19977;&#20010;&#20197;LLM&#20026;&#22522;&#30784;&#30340;&#26234;&#33021;&#20307;&#32452;&#25104;&#65292;&#36127;&#36131;&#21160;&#20316;&#35268;&#21010;&#12289;&#29366;&#24577;&#26816;&#27979;&#21644;...
&lt;/p&gt;
&lt;p&gt;
In past years, we have been dedicated to automating user acceptance testing (UAT) process of WeChat Pay, one of the most influential mobile payment applications in China. A system titled XUAT has been developed for this purpose. However, there is still a human-labor-intensive stage, i.e, test scripts generation, in the current system. Therefore, in this paper, we concentrate on methods of boosting the automation level of the current system, particularly the stage of test scripts generation. With recent notable successes, large language models (LLMs) demonstrate significant potential in attaining human-like intelligence and there has been a growing research area that employs LLMs as autonomous agents to obtain human-like decision-making capabilities. Inspired by these works, we propose an LLM-powered multi-agent collaborative system, named XUAT-Copilot, for automated UAT. The proposed system mainly consists of three LLM-based agents responsible for action planning, state checking and pa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#29575;&#26041;&#27861;&#65292;&#29992;&#20110;&#39564;&#35777;&#20851;&#32852;&#25968;&#25454;&#19978;&#30340;&#35299;&#37322;&#12290;&#36890;&#36807;&#22312;&#21407;&#22987;&#25968;&#25454;&#19978;&#29983;&#25104;&#23545;&#31216;&#36817;&#20284;&#30340;&#21453;&#20107;&#23454;&#31034;&#20363;&#65292;&#23398;&#20064;&#20102;&#19968;&#20010;&#22240;&#23376;&#22270;&#27169;&#22411;&#26469;&#37327;&#21270;&#35299;&#37322;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#24110;&#21161;&#39564;&#35777;&#35299;&#37322;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2401.02703</link><description>&lt;p&gt;
&#39564;&#35777;&#20851;&#32852;&#35299;&#37322;:&#19968;&#31181;&#27010;&#29575;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Verifying Relational Explanations: A Probabilistic Approach. (arXiv:2401.02703v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#29575;&#26041;&#27861;&#65292;&#29992;&#20110;&#39564;&#35777;&#20851;&#32852;&#25968;&#25454;&#19978;&#30340;&#35299;&#37322;&#12290;&#36890;&#36807;&#22312;&#21407;&#22987;&#25968;&#25454;&#19978;&#29983;&#25104;&#23545;&#31216;&#36817;&#20284;&#30340;&#21453;&#20107;&#23454;&#31034;&#20363;&#65292;&#23398;&#20064;&#20102;&#19968;&#20010;&#22240;&#23376;&#22270;&#27169;&#22411;&#26469;&#37327;&#21270;&#35299;&#37322;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#24110;&#21161;&#39564;&#35777;&#35299;&#37322;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20851;&#32852;&#25968;&#25454;&#19978;&#35299;&#37322;&#26159;&#24456;&#38590;&#39564;&#35777;&#30340;&#65292;&#22240;&#20026;&#35299;&#37322;&#32467;&#26500;&#26356;&#21152;&#22797;&#26434;&#65288;&#20363;&#22914;&#22270;&#24418;&#65289;&#12290;&#20026;&#20102;&#39564;&#35777;&#21487;&#35299;&#37322;&#30340;&#35299;&#37322;&#65288;&#20363;&#22914;&#22312;&#22270;&#20687;&#12289;&#25991;&#26412;&#31561;&#20013;&#20570;&#20986;&#30340;&#39044;&#27979;&#30340;&#35299;&#37322;&#65289;&#65292;&#36890;&#24120;&#20351;&#29992;&#20154;&#31867;&#20027;&#20307;&#65292;&#22240;&#20026;&#36825;&#19981;&#19968;&#23450;&#38656;&#35201;&#24456;&#22810;&#19987;&#19994;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#39564;&#35777;&#20851;&#32852;&#35299;&#37322;&#30340;&#36136;&#37327;&#38656;&#35201;&#19987;&#19994;&#30693;&#35782;&#65292;&#24182;&#19988;&#24456;&#38590;&#35268;&#27169;&#21270;&#12290;GNNExplainer&#21487;&#20197;&#35828;&#26159;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#26041;&#27861;&#20013;&#26368;&#21463;&#27426;&#36814;&#30340;&#26041;&#27861;&#20043;&#19968;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;GNNExplainer&#29983;&#25104;&#30340;&#35299;&#37322;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35201;&#27714;&#35299;&#37322;&#22120;&#20026;&#20960;&#20010;&#21453;&#20107;&#23454;&#31034;&#20363;&#29983;&#25104;&#35299;&#37322;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#31034;&#20363;&#29983;&#25104;&#20026;&#21407;&#22987;&#25968;&#25454;&#20013;&#20851;&#32852;&#32467;&#26500;&#30340;&#23545;&#31216;&#36817;&#20284;&#12290;&#20174;&#36825;&#20123;&#35299;&#37322;&#20013;&#65292;&#25105;&#20204;&#23398;&#20064;&#20102;&#19968;&#20010;&#22240;&#23376;&#22270;&#27169;&#22411;&#26469;&#37327;&#21270;&#35299;&#37322;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#24110;&#21161;&#39564;&#35777;&#35299;&#37322;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explanations on relational data are hard to verify since the explanation structures are more complex (e.g. graphs). To verify interpretable explanations (e.g. explanations of predictions made in images, text, etc.), typically human subjects are used since it does not necessarily require a lot of expertise. However, to verify the quality of a relational explanation requires expertise and is hard to scale-up. GNNExplainer is arguably one of the most popular explanation methods for Graph Neural Networks. In this paper, we develop an approach where we assess the uncertainty in explanations generated by GNNExplainer. Specifically, we ask the explainer to generate explanations for several counterfactual examples. We generate these examples as symmetric approximations of the relational structure in the original data. From these explanations, we learn a factor graph model to quantify uncertainty in an explanation. Our results on several datasets show that our approach can help verify explanati
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20960;&#20309;&#20415;&#21033;&#30340;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;3D&#20998;&#23376;&#29983;&#25104;&#20013;&#30340;&#22810;&#20307;&#21407;&#23376;&#20851;&#31995;&#24314;&#27169;&#21644;&#38190;&#30340;&#39044;&#27979;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.02683</link><description>&lt;p&gt;
&#29992;&#20110;3D&#20998;&#23376;&#29983;&#25104;&#30340;&#20960;&#20309;&#20415;&#21033;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Geometric-Facilitated Denoising Diffusion Model for 3D Molecule Generation. (arXiv:2401.02683v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02683
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20960;&#20309;&#20415;&#21033;&#30340;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;3D&#20998;&#23376;&#29983;&#25104;&#20013;&#30340;&#22810;&#20307;&#21407;&#23376;&#20851;&#31995;&#24314;&#27169;&#21644;&#38190;&#30340;&#39044;&#27979;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#22312;&#22810;&#20010;&#30740;&#31350;&#39046;&#22495;&#26174;&#31034;&#20986;&#24040;&#22823;&#28508;&#21147;&#12290;&#29616;&#26377;&#30340;&#38754;&#21521;&#20840;&#26032;3D&#20998;&#23376;&#29983;&#25104;&#30340;&#25193;&#25955;&#22522;&#20110;&#29983;&#25104;&#26041;&#27861;&#38754;&#20020;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#12290;&#30001;&#20110;&#20998;&#23376;&#20013;&#30340;&#22823;&#22810;&#25968;&#37325;&#21407;&#23376;&#36890;&#36807;&#21333;&#38190;&#19982;&#22810;&#20010;&#21407;&#23376;&#30456;&#36830;&#65292;&#20165;&#20351;&#29992;&#25104;&#23545;&#36317;&#31163;&#26469;&#27169;&#25311;&#20998;&#23376;&#20960;&#20309;&#26159;&#19981;&#36275;&#30340;&#12290;&#22240;&#27492;&#65292;&#31532;&#19968;&#20010;&#25361;&#25112;&#28041;&#21450;&#25552;&#20986;&#19968;&#20010;&#33021;&#22815;&#25429;&#25417;&#22797;&#26434;&#30340;&#22810;&#20307;&#21407;&#23376;&#38388;&#20851;&#31995;&#21644;&#23398;&#20064;&#39640;&#36136;&#37327;&#29305;&#24449;&#30340;&#26377;&#25928;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#21435;&#22122;&#20869;&#26680;&#12290;&#30001;&#20110;&#22270;&#30340;&#31163;&#25955;&#24615;&#36136;&#65292;&#38754;&#23545;&#20998;&#23376;&#30340;&#20027;&#27969;&#25193;&#25955;&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#39044;&#23450;&#20041;&#35268;&#21017;&#65292;&#24182;&#20197;&#38388;&#25509;&#26041;&#24335;&#29983;&#25104;&#36793;&#32536;&#12290;&#31532;&#20108;&#20010;&#25361;&#25112;&#28041;&#21450;&#23558;&#20998;&#23376;&#30340;&#29983;&#25104;&#19982;&#25193;&#25955;&#30456;&#32467;&#21512;&#65292;&#24182;&#20934;&#30830;&#39044;&#27979;&#38190;&#30340;&#23384;&#22312;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#22312;&#25193;&#25955;&#36807;&#31243;&#20013;&#26356;&#26032;&#20998;&#23376;&#26500;&#22411;&#30340;&#36845;&#20195;&#26041;&#24335;&#19982;&#20998;&#23376;&#21160;&#21147;&#23398;&#19968;&#33268;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#20004;&#20010;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Denoising diffusion models have shown great potential in multiple research areas. Existing diffusion-based generative methods on de novo 3D molecule generation face two major challenges. Since majority heavy atoms in molecules allow connections to multiple atoms through single bonds, solely using pair-wise distance to model molecule geometries is insufficient. Therefore, the first one involves proposing an effective neural network as the denoising kernel that is capable to capture complex multi-body interatomic relationships and learn high-quality features. Due to the discrete nature of graphs, mainstream diffusion-based methods for molecules heavily rely on predefined rules and generate edges in an indirect manner. The second challenge involves accommodating molecule generation to diffusion and accurately predicting the existence of bonds. In our research, we view the iterative way of updating molecule conformations in diffusion process is consistent with molecular dynamics and introd
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#36880;&#23618;&#25439;&#22833;&#30340;&#28176;&#36827;&#24335;&#21066;&#20943;&#65292;&#24341;&#20837;&#20102;&#20004;&#20010;&#31934;&#31616;&#21464;&#20307;Segmind&#31283;&#23450;&#25193;&#25955;&#65288;SSD-1B&#65289;&#21644;Segmind-Vega&#65292;&#20998;&#21035;&#20351;&#29992;1.3B&#21644;0.74B&#21442;&#25968;&#30340;UNet&#32467;&#26500;&#23454;&#29616;&#12290;&#36825;&#20123;&#31934;&#31616;&#27169;&#22411;&#36890;&#36807;&#36716;&#31227;&#23398;&#20064;&#26377;&#25928;&#22320;&#27169;&#25311;&#20102;&#21407;&#22987;&#30340;SDXL&#65292;&#24182;&#22312;&#19982;&#26356;&#22823;&#30340;SDXL&#27169;&#22411;&#30456;&#27604;&#30340;&#31454;&#20105;&#20013;&#21462;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.02677</link><description>&lt;p&gt;
&#36880;&#23618;&#25439;&#22833;&#31283;&#23450;&#25193;&#25955;XL&#30340;&#28176;&#36827;&#24335;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Progressive Knowledge Distillation Of Stable Diffusion XL Using Layer Level Loss. (arXiv:2401.02677v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02677
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#36880;&#23618;&#25439;&#22833;&#30340;&#28176;&#36827;&#24335;&#21066;&#20943;&#65292;&#24341;&#20837;&#20102;&#20004;&#20010;&#31934;&#31616;&#21464;&#20307;Segmind&#31283;&#23450;&#25193;&#25955;&#65288;SSD-1B&#65289;&#21644;Segmind-Vega&#65292;&#20998;&#21035;&#20351;&#29992;1.3B&#21644;0.74B&#21442;&#25968;&#30340;UNet&#32467;&#26500;&#23454;&#29616;&#12290;&#36825;&#20123;&#31934;&#31616;&#27169;&#22411;&#36890;&#36807;&#36716;&#31227;&#23398;&#20064;&#26377;&#25928;&#22320;&#27169;&#25311;&#20102;&#21407;&#22987;&#30340;SDXL&#65292;&#24182;&#22312;&#19982;&#26356;&#22823;&#30340;SDXL&#27169;&#22411;&#30456;&#27604;&#30340;&#31454;&#20105;&#20013;&#21462;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31283;&#23450;&#25193;&#25955;XL&#65288;SDXL&#65289;&#24050;&#25104;&#20026;&#38750;&#24120;&#22810;&#21151;&#33021;&#19988;&#36136;&#37327;&#21331;&#36234;&#30340;&#24320;&#28304;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#65288;T2I&#65289;&#12290;&#39640;&#25928;&#22320;&#35299;&#20915;SDXL&#27169;&#22411;&#30340;&#35745;&#31639;&#38656;&#27714;&#23545;&#20110;&#25193;&#22823;&#20854;&#24212;&#29992;&#33539;&#22260;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#36880;&#23618;&#25439;&#22833;&#30340;&#28176;&#36827;&#24335;&#21066;&#20943;&#24341;&#20837;&#20102;&#20004;&#20010;&#31934;&#31616;&#21464;&#20307;&#65292;&#21363;Segmind&#31283;&#23450;&#25193;&#25955;&#65288;SSD-1B&#65289;&#21644;Segmind-Vega&#65292;&#20998;&#21035;&#20351;&#29992;13&#20159;&#21644;0.74&#20159;&#21442;&#25968;&#30340;UNet&#32467;&#26500;&#23454;&#29616;&#65292;&#24182;&#27880;&#37325;&#20943;&#23567;&#27169;&#22411;&#22823;&#23567;&#21516;&#26102;&#20445;&#25345;&#29983;&#25104;&#36136;&#37327;&#12290;&#25105;&#20204;&#22312;https://hf.co/Segmind&#19978;&#21457;&#24067;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#26435;&#37325;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#20174;SDXL&#30340;U-Net&#32467;&#26500;&#20013;&#28040;&#38500;&#27531;&#24046;&#32593;&#32476;&#21644;Transformer&#22359;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#21442;&#25968;&#21644;&#24310;&#36831;&#12290;&#25105;&#20204;&#30340;&#31934;&#31616;&#27169;&#22411;&#36890;&#36807;&#21033;&#29992;&#36716;&#31227;&#23398;&#20064;&#33021;&#22815;&#26377;&#25928;&#22320;&#27169;&#25311;&#21407;&#22987;&#30340;SDXL&#65292;&#24182;&#22312;&#19982;&#26356;&#22823;&#30340;&#21315;&#20159;&#21442;&#25968;&#32423;SDXL&#30456;&#27604;&#30340;&#31454;&#20105;&#20013;&#21462;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24378;&#35843;&#20102;&#30693;&#35782;&#33976;&#39311;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stable Diffusion XL (SDXL) has become the best open source text-to-image model (T2I) for its versatility and top-notch image quality. Efficiently addressing the computational demands of SDXL models is crucial for wider reach and applicability. In this work, we introduce two scaled-down variants, Segmind Stable Diffusion (SSD-1B) and Segmind-Vega, with 1.3B and 0.74B parameter UNets, respectively, achieved through progressive removal using layer-level losses focusing on reducing the model size while preserving generative quality. We release these models weights at https://hf.co/Segmind. Our methodology involves the elimination of residual networks and transformer blocks from the U-Net structure of SDXL, resulting in significant reductions in parameters, and latency. Our compact models effectively emulate the original SDXL by capitalizing on transferred knowledge, achieving competitive results against larger multi-billion parameter SDXL. Our work underscores the efficacy of knowledge dis
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#22810;&#36890;&#36947;&#36828;&#22330;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#65292;&#23558;&#31070;&#32463;&#27874;&#26463;&#24418;&#25104;&#21644;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#31471;&#21040;&#31471;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#24182;&#36827;&#34892;&#20102;&#32852;&#21512;&#35757;&#32451;&#12290;&#20351;&#29992;&#20998;&#35299;&#22797;&#32447;&#24615;&#25237;&#24433;&#26469;&#24418;&#25104;&#31070;&#32463;&#27874;&#26463;&#24418;&#25104;&#65292;&#24182;&#25506;&#32034;&#28304;&#26041;&#21521;&#20316;&#20026;&#20808;&#39564;&#30693;&#35782;&#30340;&#26377;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.02673</link><description>&lt;p&gt;
&#19968;&#20010;&#32479;&#19968;&#30340;&#22810;&#36890;&#36947;&#36828;&#22330;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#65306;&#23558;&#31070;&#32463;&#27874;&#26463;&#24418;&#25104;&#19982;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#31471;&#21040;&#31471;&#27169;&#22411;&#30456;&#32467;&#21512;
&lt;/p&gt;
&lt;p&gt;
A unified multichannel far-field speech recognition system: combining neural beamforming with attention based end-to-end model. (arXiv:2401.02673v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02673
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#22810;&#36890;&#36947;&#36828;&#22330;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#65292;&#23558;&#31070;&#32463;&#27874;&#26463;&#24418;&#25104;&#21644;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#31471;&#21040;&#31471;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#24182;&#36827;&#34892;&#20102;&#32852;&#21512;&#35757;&#32451;&#12290;&#20351;&#29992;&#20998;&#35299;&#22797;&#32447;&#24615;&#25237;&#24433;&#26469;&#24418;&#25104;&#31070;&#32463;&#27874;&#26463;&#24418;&#25104;&#65292;&#24182;&#25506;&#32034;&#28304;&#26041;&#21521;&#20316;&#20026;&#20808;&#39564;&#30693;&#35782;&#30340;&#26377;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36828;&#22330;&#35821;&#38899;&#35782;&#21035;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#20256;&#32479;&#19978;&#20351;&#29992;&#20449;&#21495;&#22788;&#29702;&#27874;&#26463;&#24418;&#25104;&#26469;&#35299;&#20915;&#22122;&#22768;&#21644;&#24178;&#25200;&#38382;&#39064;&#12290;&#20294;&#26159;&#30001;&#20110;&#23545;&#29615;&#22659;&#20551;&#35774;&#30340;&#20005;&#37325;&#20381;&#36182;&#65292;&#24615;&#33021;&#36890;&#24120;&#21463;&#21040;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#22810;&#36890;&#36947;&#36828;&#22330;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#32467;&#21512;&#20102;&#31070;&#32463;&#27874;&#26463;&#24418;&#25104;&#21644;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;Listen, Spell, Attend (LAS)&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#65292;&#36827;&#19968;&#27493;&#25193;&#23637;&#20102;&#31471;&#21040;&#31471;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#65292;&#21253;&#25324;&#35821;&#38899;&#22686;&#24378;&#12290;&#28982;&#21518;&#65292;&#36825;&#20010;&#26694;&#26550;&#34987;&#20849;&#21516;&#35757;&#32451;&#20197;&#20248;&#21270;&#26368;&#32456;&#30340;&#30446;&#26631;&#12290;&#20855;&#20307;&#22320;&#65292;&#37319;&#29992;&#20998;&#35299;&#22797;&#32447;&#24615;&#25237;&#24433;(fCLP)&#26469;&#24418;&#25104;&#31070;&#32463;&#27874;&#26463;&#24418;&#25104;&#12290;&#28982;&#21518;&#27604;&#36739;&#20102;&#20960;&#31181;&#21512;&#24182;&#26041;&#21521;&#30340;&#27719;&#32858;&#31574;&#30053;&#65292;&#20197;&#25214;&#21040;&#26368;&#20248;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#28304;&#26041;&#21521;&#30340;&#20449;&#24687;&#20063;&#34987;&#38598;&#25104;&#21040;&#27874;&#26463;&#24418;&#25104;&#20013;&#65292;&#25506;&#32034;&#28304;&#26041;&#21521;&#20316;&#20026;&#20808;&#39564;&#30693;&#35782;&#30340;&#26377;&#29992;&#24615;&#65292;&#36825;&#36890;&#24120;&#26159;&#19981;&#24120;&#35265;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Far-field speech recognition is a challenging task that conventionally uses signal processing beamforming to attack noise and interference problem. But the performance has been found usually limited due to heavy reliance on environmental assumption. In this paper, we propose a unified multichannel far-field speech recognition system that combines the neural beamforming and transformer-based Listen, Spell, Attend (LAS) speech recognition system, which extends the end-to-end speech recognition system further to include speech enhancement. Such framework is then jointly trained to optimize the final objective of interest. Specifically, factored complex linear projection (fCLP) has been adopted to form the neural beamforming. Several pooling strategies to combine look directions are then compared in order to find the optimal approach. Moreover, information of the source direction is also integrated in the beamforming to explore the usefulness of source direction as a prior, which is usuall
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#20174;&#20854;&#20182;&#22320;&#29702;&#20301;&#32622;&#25552;&#21462;&#30340;&#30693;&#35782;&#65292;&#23454;&#29616;&#20102;&#23545;&#26032;&#30340;&#21644;&#26410;&#30417;&#27979;&#21040;&#30340;&#22320;&#28857;&#30340;&#21508;&#31181;&#27668;&#20505;&#27979;&#37327;&#30340;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2401.02665</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#28145;&#24230;&#23398;&#20064;&#23454;&#29616;&#24494;&#27668;&#20505;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Zero-shot Microclimate Prediction with Deep Learning. (arXiv:2401.02665v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02665
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#20174;&#20854;&#20182;&#22320;&#29702;&#20301;&#32622;&#25552;&#21462;&#30340;&#30693;&#35782;&#65292;&#23454;&#29616;&#20102;&#23545;&#26032;&#30340;&#21644;&#26410;&#30417;&#27979;&#21040;&#30340;&#22320;&#28857;&#30340;&#21508;&#31181;&#27668;&#20505;&#27979;&#37327;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22825;&#27668;&#31449;&#25968;&#25454;&#26159;&#27668;&#20505;&#39044;&#27979;&#30340;&#23453;&#36149;&#36164;&#28304;&#65292;&#20294;&#22312;&#20559;&#36828;&#22320;&#21306;&#20854;&#21487;&#38752;&#24615;&#21487;&#33021;&#21463;&#38480;&#12290;&#27492;&#22806;&#65292;&#36827;&#34892;&#26412;&#22320;&#39044;&#27979;&#36890;&#24120;&#20381;&#36182;&#20110;&#21487;&#33021;&#26080;&#27861;&#35775;&#38382;&#30340;&#20256;&#24863;&#22120;&#25968;&#25454;&#65292;&#36825;&#23545;&#20110;&#26032;&#30340;&#12289;&#20197;&#21069;&#26410;&#30417;&#27979;&#21040;&#30340;&#22320;&#28857;&#26469;&#35828;&#23588;&#20854;&#22256;&#38590;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38646;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#39044;&#27979;&#26032;&#30340;&#12289;&#26410;&#30417;&#27979;&#21040;&#30340;&#22320;&#28857;&#30340;&#21508;&#31181;&#27668;&#20505;&#27979;&#37327;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#20174;&#20854;&#20182;&#22320;&#29702;&#20301;&#32622;&#25552;&#21462;&#30340;&#30693;&#35782;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#22825;&#27668;&#39044;&#25253;&#25216;&#26415;&#65292;&#39044;&#27979;&#24494;&#27668;&#20505;&#21464;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Weather station data is a valuable resource for climate prediction, however, its reliability can be limited in remote locations. To compound the issue, making local predictions often relies on sensor data that may not be accessible for a new, previously unmonitored location. In response to these challenges, we propose a novel zero-shot learning approach designed to forecast various climate measurements at new and unmonitored locations. Our method surpasses conventional weather forecasting techniques in predicting microclimate variables by leveraging knowledge extracted from other geographic locations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#38024;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#21457;&#29616;GNN&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#65292;&#25552;&#20986;&#20102;&#38024;&#23545;&#35813;&#20219;&#21153;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2401.02663</link><description>&lt;p&gt;
&#29992;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#30340;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
A backdoor attack against link prediction tasks with graph neural networks. (arXiv:2401.02663v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02663
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#38024;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#21457;&#29616;GNN&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#65292;&#25552;&#20986;&#20102;&#38024;&#23545;&#35813;&#20219;&#21153;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26159;&#19968;&#31867;&#33021;&#22815;&#22788;&#29702;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#22312;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;GNN&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#12290;&#24403;&#20855;&#20307;&#30340;&#27169;&#24335;&#65288;&#31216;&#20026;&#21518;&#38376;&#35302;&#21457;&#22120;&#65292;&#20363;&#22914;&#23376;&#22270;&#12289;&#33410;&#28857;&#31561;&#65289;&#20986;&#29616;&#22312;&#36755;&#20837;&#25968;&#25454;&#20013;&#26102;&#65292;&#23884;&#20837;&#22312;GNN&#27169;&#22411;&#20013;&#30340;&#21518;&#38376;&#20250;&#34987;&#28608;&#27963;&#65292;&#23558;&#36755;&#20837;&#25968;&#25454;&#35823;&#20998;&#31867;&#20026;&#25915;&#20987;&#32773;&#25351;&#23450;&#30340;&#30446;&#26631;&#31867;&#26631;&#31614;&#65292;&#32780;&#24403;&#36755;&#20837;&#20013;&#27809;&#26377;&#21518;&#38376;&#35302;&#21457;&#22120;&#26102;&#65292;&#23884;&#20837;&#22312;GNN&#27169;&#22411;&#20013;&#30340;&#21518;&#38376;&#19981;&#20250;&#34987;&#28608;&#27963;&#65292;&#27169;&#22411;&#27491;&#24120;&#24037;&#20316;&#12290;&#21518;&#38376;&#25915;&#20987;&#20855;&#26377;&#26497;&#39640;&#30340;&#38544;&#34109;&#24615;&#65292;&#32473;GNN&#27169;&#22411;&#24102;&#26469;&#20005;&#37325;&#30340;&#23433;&#20840;&#39118;&#38505;&#12290;&#30446;&#21069;&#65292;&#23545;GNN&#30340;&#21518;&#38376;&#25915;&#20987;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#22270;&#20998;&#31867;&#21644;&#33410;&#28857;&#20998;&#31867;&#31561;&#20219;&#21153;&#19978;&#65292;&#23545;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#30340;&#21518;&#38376;&#25915;&#20987;&#30740;&#31350;&#36739;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) are a class of deep learning models capable of processing graph-structured data, and they have demonstrated significant performance in a variety of real-world applications. Recent studies have found that GNN models are vulnerable to backdoor attacks. When specific patterns (called backdoor triggers, e.g., subgraphs, nodes, etc.) appear in the input data, the backdoor embedded in the GNN models is activated, which misclassifies the input data into the target class label specified by the attacker, whereas when there are no backdoor triggers in the input, the backdoor embedded in the GNN models is not activated, and the models work normally. Backdoor attacks are highly stealthy and expose GNN models to serious security risks. Currently, research on backdoor attacks against GNNs mainly focus on tasks such as graph classification and node classification, and backdoor attacks against link prediction tasks are rarely studied. In this paper, we propose a backdoor a
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#25252;&#22763;&#21442;&#19982;&#22411;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#21033;&#29992;&#36716;&#31227;&#23398;&#20064;&#30340;&#39044;&#27979;&#25968;&#23383;&#23402;&#29983;&#20307;&#26469;&#23454;&#29616;&#38024;&#23545;2&#22411;&#31958;&#23615;&#30149;&#24739;&#32773;&#30340;&#31934;&#20934;&#31649;&#29702;&#12290;&#22312;&#20020;&#24202;&#35797;&#39564;&#20013;&#65292;&#35813;&#31995;&#32479;&#36890;&#36807;&#32467;&#21512;&#21508;&#31181;&#25968;&#25454;&#28304;&#30340;&#27169;&#24335;&#21644;&#25252;&#22763;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#27835;&#30103;&#21453;&#39304;&#65292;&#20197;&#25913;&#21892;&#24739;&#32773;&#30340;&#27835;&#30103;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.02661</link><description>&lt;p&gt;
&#25252;&#22763;&#21442;&#19982;&#22411;&#20154;&#24037;&#26234;&#33021;&#22312;&#20020;&#24202;&#35797;&#39564;&#20013;&#38024;&#23545;2&#22411;&#31958;&#23615;&#30149;&#30340;&#31934;&#20934;&#31649;&#29702;&#65306;&#21033;&#29992;&#36716;&#31227;&#23398;&#20064;&#30340;&#39044;&#27979;&#25968;&#23383;&#23402;&#29983;&#20307;
&lt;/p&gt;
&lt;p&gt;
Nurse-in-the-Loop Artificial Intelligence for Precision Management of Type 2 Diabetes in a Clinical Trial Utilizing Transfer-Learned Predictive Digital Twin. (arXiv:2401.02661v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02661
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#25252;&#22763;&#21442;&#19982;&#22411;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#21033;&#29992;&#36716;&#31227;&#23398;&#20064;&#30340;&#39044;&#27979;&#25968;&#23383;&#23402;&#29983;&#20307;&#26469;&#23454;&#29616;&#38024;&#23545;2&#22411;&#31958;&#23615;&#30149;&#24739;&#32773;&#30340;&#31934;&#20934;&#31649;&#29702;&#12290;&#22312;&#20020;&#24202;&#35797;&#39564;&#20013;&#65292;&#35813;&#31995;&#32479;&#36890;&#36807;&#32467;&#21512;&#21508;&#31181;&#25968;&#25454;&#28304;&#30340;&#27169;&#24335;&#21644;&#25252;&#22763;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#27835;&#30103;&#21453;&#39304;&#65292;&#20197;&#25913;&#21892;&#24739;&#32773;&#30340;&#27835;&#30103;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;2&#22411;&#31958;&#23615;&#30149;&#65288;T2D&#65289;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#24930;&#24615;&#30142;&#30149;&#65292;&#20854;&#20250;&#22686;&#21152;&#20005;&#37325;&#20581;&#24247;&#24182;&#23545;&#29983;&#27963;&#36136;&#37327;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#32771;&#34385;&#21040;&#20010;&#20307;&#29305;&#24449;&#21644;&#29983;&#27963;&#26041;&#24335;&#23545;&#27835;&#30103;&#35745;&#21010;&#21644;&#24739;&#32773;&#32467;&#26524;&#30340;&#24433;&#21709;&#65292;&#24320;&#21457;&#31934;&#30830;&#21644;&#20010;&#24615;&#21270;&#30340;&#31649;&#29702;&#31574;&#30053;&#33267;&#20851;&#37325;&#35201;&#12290;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#32467;&#21512;&#26469;&#33258;&#21508;&#31181;&#25968;&#25454;&#28304;&#30340;&#27169;&#24335;&#21644;&#25252;&#22763;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#21487;&#20197;&#23454;&#29616;&#26368;&#20339;&#25252;&#29702;&#25928;&#26524;&#12290;&#26041;&#27861;&#65306;&#36825;&#26159;&#19968;&#39033;&#38024;&#23545;T2D&#24739;&#32773;&#65288;n = 20&#65292;&#24180;&#40836;= 57+-10&#65289;&#30340;&#20026;&#26399;6&#20010;&#26376;&#30340;&#38468;&#23646;&#30740;&#31350;&#12290;&#21442;&#19982;&#32773;&#38543;&#26426;&#20998;&#37197;&#20026;&#24178;&#39044;&#32452;&#65288;AI&#65292;n = 10&#65289;&#21644;&#23545;&#29031;&#32452;&#65292;&#22312;&#26368;&#21518;&#19977;&#20010;&#26376;&#20013;&#24178;&#39044;&#32452;&#27599;&#22825;&#25509;&#25910;AI&#29983;&#25104;&#30340;&#20010;&#20307;&#21270;&#21453;&#39304;&#65292;&#32780;&#23545;&#29031;&#32452;&#19981;&#25509;&#25910;&#27599;&#26085;&#21453;&#39304;&#65288;&#38750;AI&#65292;n = 10&#65289;&#12290;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#22312;&#32447;&#25252;&#22763;&#21442;&#19982;&#22411;&#39044;&#27979;&#25511;&#21046;&#27169;&#22411;&#65288;ONLC&#65289;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#39044;&#27979;&#25968;&#23383;&#23402;&#29983;&#20307;&#65288;PDT&#65289;&#12290;PDT&#26159;&#36890;&#36807;&#22522;&#20110;&#36716;&#31227;&#23398;&#20064;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Background: Type 2 diabetes (T2D) is a prevalent chronic disease with a significant risk of serious health complications and negative impacts on the quality of life. Given the impact of individual characteristics and lifestyle on the treatment plan and patient outcomes, it is crucial to develop precise and personalized management strategies. Artificial intelligence (AI) provides great promise in combining patterns from various data sources with nurses' expertise to achieve optimal care. Methods: This is a 6-month ancillary study among T2D patients (n = 20, age = 57 +- 10). Participants were randomly assigned to an intervention (AI, n=10) group to receive daily AI-generated individualized feedback or a control group without receiving the daily feedback (non-AI, n=10) in the last three months. The study developed an online nurse-in-the-loop predictive control (ONLC) model that utilizes a predictive digital twin (PDT). The PDT was developed using a transfer-learning-based Artificial Neura
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26234;&#33021;&#35843;&#24230;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#30005;&#21160;&#36710;&#22312;&#26234;&#33021;&#30005;&#32593;&#20013;&#30340;&#38656;&#27714;&#21709;&#24212;&#38382;&#39064;&#12290;&#36890;&#36807;&#35843;&#24230;&#30005;&#21160;&#36710;&#30340;&#20805;&#25918;&#30005;&#27963;&#21160;&#65292;&#20197;&#19982;&#37197;&#30005;&#31995;&#32479;&#25805;&#20316;&#21592;&#25552;&#20379;&#30340;&#30446;&#26631;&#33021;&#37327;&#37197;&#32622;&#25991;&#20214;&#19968;&#33268;&#65292;&#21487;&#20197;&#23454;&#29616;&#23616;&#37096;&#32593;&#32476;&#30340;&#24179;&#34913;&#21644;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2401.02653</link><description>&lt;p&gt;
EV&#22312;&#26234;&#33021;&#30005;&#32593;&#20013;&#30340;&#38656;&#27714;&#21709;&#24212;&#26234;&#33021;&#35843;&#24230;&#30340;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Deep Q-Learning based Smart Scheduling of EVs for Demand Response in Smart Grids. (arXiv:2401.02653v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02653
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26234;&#33021;&#35843;&#24230;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#30005;&#21160;&#36710;&#22312;&#26234;&#33021;&#30005;&#32593;&#20013;&#30340;&#38656;&#27714;&#21709;&#24212;&#38382;&#39064;&#12290;&#36890;&#36807;&#35843;&#24230;&#30005;&#21160;&#36710;&#30340;&#20805;&#25918;&#30005;&#27963;&#21160;&#65292;&#20197;&#19982;&#37197;&#30005;&#31995;&#32479;&#25805;&#20316;&#21592;&#25552;&#20379;&#30340;&#30446;&#26631;&#33021;&#37327;&#37197;&#32622;&#25991;&#20214;&#19968;&#33268;&#65292;&#21487;&#20197;&#23454;&#29616;&#23616;&#37096;&#32593;&#32476;&#30340;&#24179;&#34913;&#21644;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32463;&#27982;&#21644;&#25919;&#31574;&#22240;&#32032;&#25512;&#21160;&#20102;&#30005;&#21160;&#36710;(EV)&#30340;&#19981;&#26029;&#22686;&#21152;&#21644;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;EV&#26159;&#19968;&#31181;&#27604;&#29123;&#27833;&#36710;&#26356;&#28165;&#27905;&#30340;&#26367;&#20195;&#21697;&#65292;&#20294;&#30001;&#20110;&#30005;&#21147;&#38656;&#27714;&#22686;&#21152;&#21644;&#20351;&#29992;&#26102;&#38388;&#23548;&#33268;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;EV&#23545;&#24494;&#30005;&#32593;&#35774;&#22791;&#23551;&#21629;&#21644;&#33021;&#28304;&#24179;&#34913;&#20135;&#29983;&#20102;&#36127;&#38754;&#24433;&#21709;&#12290;&#22312;&#25105;&#20204;&#30475;&#26469;&#65292;&#30005;&#32593;&#31649;&#29702;&#24212;&#35813;&#21033;&#29992;EV&#30340;&#35843;&#24230;&#28789;&#27963;&#24615;&#65292;&#36890;&#36807;&#31215;&#26497;&#21442;&#19982;&#38656;&#27714;&#21709;&#24212;&#35745;&#21010;&#26469;&#25903;&#25345;&#23616;&#37096;&#32593;&#32476;&#24179;&#34913;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26080;&#27169;&#22411;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#23558;EV&#30340;&#20805;&#30005;&#21644;&#25918;&#30005;&#27963;&#21160;&#35843;&#24230;&#21040;&#24494;&#30005;&#32593;&#20013;&#65292;&#20197;&#19982;&#37197;&#30005;&#31995;&#32479;&#25805;&#20316;&#21592;&#25552;&#20379;&#30340;&#30446;&#26631;&#33021;&#37327;&#37197;&#32622;&#25991;&#20214;&#19968;&#33268;&#12290;&#25105;&#20204;&#25913;&#36827;&#20102;Bellman&#26041;&#31243;&#65292;&#36890;&#36807;&#29305;&#23450;&#30340;&#22870;&#21169;&#35780;&#20272;&#29366;&#24577;&#30340;&#20215;&#20540;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#20272;&#35745;&#21487;&#29992;&#21160;&#20316;&#30340;Q&#20540;&#65292;&#24182;&#20351;&#29992;epsilon-greedy&#31639;&#27861;&#24179;&#34913;&#24320;&#21457;&#21644;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Economic and policy factors are driving the continuous increase in the adoption and usage of electrical vehicles (EVs). However, despite being a cleaner alternative to combustion engine vehicles, EVs have negative impacts on the lifespan of microgrid equipment and energy balance due to increased power demand and the timing of their usage. In our view grid management should leverage on EVs scheduling flexibility to support local network balancing through active participation in demand response programs. In this paper, we propose a model-free solution, leveraging Deep Q-Learning to schedule the charging and discharging activities of EVs within a microgrid to align with a target energy profile provided by the distribution system operator. We adapted the Bellman Equation to assess the value of a state based on specific rewards for EV scheduling actions and used a neural network to estimate Q-values for available actions and the epsilon-greedy algorithm to balance exploitation and explorati
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22312;&#23384;&#22312;&#29615;&#22659;&#21160;&#24577;&#21644;&#30456;&#23545;&#20110;&#21463;&#23475;&#32773;&#30446;&#26631;&#30340;&#38750;&#26368;&#20248;&#24615;&#26102;&#65292;&#21363;&#20351;&#30446;&#26631;&#34892;&#20026;&#26080;&#27861;&#34987;&#37319;&#32435;&#65292;&#20173;&#28982;&#21487;&#33021;&#36827;&#34892;C-TTA&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;gammaDDPG&#31639;&#27861;&#26469;&#23398;&#20064;&#36825;&#31181;&#26356;&#24378;&#29256;&#26412;&#30340;C-TTA&#65292;&#24182;&#26681;&#25454;&#21463;&#23475;&#32773;&#24403;&#21069;&#30340;&#34892;&#20026;&#21160;&#24577;&#25913;&#21464;&#25915;&#20987;&#31574;&#30053;&#35268;&#21010;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2401.02652</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#25240;&#25187;&#35757;&#32451;&#26102;&#38388;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Adaptive Discounting of Training Time Attacks. (arXiv:2401.02652v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02652
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22312;&#23384;&#22312;&#29615;&#22659;&#21160;&#24577;&#21644;&#30456;&#23545;&#20110;&#21463;&#23475;&#32773;&#30446;&#26631;&#30340;&#38750;&#26368;&#20248;&#24615;&#26102;&#65292;&#21363;&#20351;&#30446;&#26631;&#34892;&#20026;&#26080;&#27861;&#34987;&#37319;&#32435;&#65292;&#20173;&#28982;&#21487;&#33021;&#36827;&#34892;C-TTA&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;gammaDDPG&#31639;&#27861;&#26469;&#23398;&#20064;&#36825;&#31181;&#26356;&#24378;&#29256;&#26412;&#30340;C-TTA&#65292;&#24182;&#26681;&#25454;&#21463;&#23475;&#32773;&#24403;&#21069;&#30340;&#34892;&#20026;&#21160;&#24577;&#25913;&#21464;&#25915;&#20987;&#31574;&#30053;&#35268;&#21010;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#20013;&#65292;&#35757;&#32451;&#26102;&#38388;&#25915;&#20987;&#65288;TTAs&#65289;&#26159;&#26368;&#38452;&#38505;&#30340;&#25915;&#20987;&#20043;&#19968;&#65292;&#21487;&#20197;&#22312;&#23398;&#20064;&#21040;&#30340;&#34892;&#20026;&#20013;&#21046;&#36896;&#28431;&#27934;&#21644;&#21518;&#38376;&#12290;&#29616;&#22312;&#24050;&#32463;&#26377;&#20102;&#19981;&#20165;&#20165;&#26159;&#31616;&#21333;&#30772;&#22351;&#30340;&#24314;&#35774;&#24615;TTAs&#65288;C-TTAs&#65289;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#24378;&#21046;&#35757;&#32451;RL agent&#65288;&#21463;&#23475;&#32773;&#65289;&#34920;&#29616;&#20986;&#29305;&#23450;&#30340;&#30446;&#26631;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26159;&#26368;&#20808;&#36827;&#30340;C-TTAs&#20063;&#21482;&#38024;&#23545;&#37027;&#20123;&#22914;&#26524;&#19981;&#22240;&#29615;&#22659;&#21160;&#24577;&#30340;&#19968;&#20010;&#29305;&#23450;&#29305;&#24449;&#34987;&#21033;&#29992;&#65292;&#21463;&#23475;&#32773;&#26412;&#21487;&#20197;&#33258;&#28982;&#22320;&#37319;&#32435;&#30340;&#30446;&#26631;&#34892;&#20026;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21363;&#20351;&#30446;&#26631;&#34892;&#20026;&#30001;&#20110;&#29615;&#22659;&#21160;&#24577;&#21644;&#30456;&#23545;&#20110;&#21463;&#23475;&#32773;&#30446;&#26631;&#30340;&#38750;&#26368;&#20248;&#24615;&#32780;&#26080;&#27861;&#37319;&#32435;&#65292;C-TTA&#20063;&#26159;&#21487;&#33021;&#30340;&#12290;&#20026;&#20102;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#25214;&#21040;&#39640;&#25928;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#29305;&#23450;&#30340;DDPG&#31639;&#27861;&#65292;&#31216;&#20026;gammaDDPG&#65292;&#29992;&#20110;&#23398;&#20064;&#36825;&#31181;&#26356;&#24378;&#29256;&#26412;&#30340;C-TTA&#12290;gammaDDPG&#26681;&#25454;&#21463;&#23475;&#32773;&#24403;&#21069;&#30340;&#34892;&#20026;&#21160;&#24577;&#25913;&#21464;&#25915;&#20987;&#31574;&#30053;&#35268;&#21010;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Among the most insidious attacks on Reinforcement Learning (RL) solutions are training-time attacks (TTAs) that create loopholes and backdoors in the learned behaviour. Not limited to a simple disruption, constructive TTAs (C-TTAs) are now available, where the attacker forces a specific, target behaviour upon a training RL agent (victim). However, even state-of-the-art C-TTAs focus on target behaviours that could be naturally adopted by the victim if not for a particular feature of the environment dynamics, which C-TTAs exploit. In this work, we show that a C-TTA is possible even when the target behaviour is un-adoptable due to both environment dynamics as well as non-optimality with respect to the victim objective(s). To find efficient attacks in this context, we develop a specialised flavour of the DDPG algorithm, which we term gammaDDPG, that learns this stronger version of C-TTA. gammaDDPG dynamically alters the attack policy planning horizon based on the victim's current behaviour
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#20998;&#23618;&#25193;&#25955;&#35268;&#21010;(Hierarchical Diffuser)&#65292;&#32467;&#21512;&#20102;&#20998;&#23618;&#21644;&#22522;&#20110;&#25193;&#25955;&#30340;&#35268;&#21010;&#30340;&#20248;&#28857;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#12289;&#24555;&#36895;&#19988;&#26377;&#25928;&#30340;&#35268;&#21010;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#36739;&#39640;&#23618;&#38754;&#19978;&#37319;&#29992;&#8220;&#36339;&#36291;&#8221;&#35268;&#21010;&#31574;&#30053;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#20855;&#26377;&#36739;&#22823;&#30340;&#24863;&#21463;&#37326;&#19988;&#35745;&#31639;&#25104;&#26412;&#36739;&#20302;&#65292;&#21516;&#26102;&#36339;&#36291;&#30340;&#23376;&#30446;&#26631;&#36824;&#33021;&#25351;&#23548;&#20302;&#23618;&#35268;&#21010;&#22120;&#65292;&#22312;&#24494;&#35843;&#38454;&#27573;&#36827;&#19968;&#27493;&#25552;&#39640;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;&#22312;&#23454;&#39564;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24615;&#33021;&#21644;&#25928;&#29575;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.02644</link><description>&lt;p&gt;
&#31616;&#21333;&#30340;&#20998;&#23618;&#25193;&#25955;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Simple Hierarchical Planning with Diffusion. (arXiv:2401.02644v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02644
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#20998;&#23618;&#25193;&#25955;&#35268;&#21010;(Hierarchical Diffuser)&#65292;&#32467;&#21512;&#20102;&#20998;&#23618;&#21644;&#22522;&#20110;&#25193;&#25955;&#30340;&#35268;&#21010;&#30340;&#20248;&#28857;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#12289;&#24555;&#36895;&#19988;&#26377;&#25928;&#30340;&#35268;&#21010;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#36739;&#39640;&#23618;&#38754;&#19978;&#37319;&#29992;&#8220;&#36339;&#36291;&#8221;&#35268;&#21010;&#31574;&#30053;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#20855;&#26377;&#36739;&#22823;&#30340;&#24863;&#21463;&#37326;&#19988;&#35745;&#31639;&#25104;&#26412;&#36739;&#20302;&#65292;&#21516;&#26102;&#36339;&#36291;&#30340;&#23376;&#30446;&#26631;&#36824;&#33021;&#25351;&#23548;&#20302;&#23618;&#35268;&#21010;&#22120;&#65292;&#22312;&#24494;&#35843;&#38454;&#27573;&#36827;&#19968;&#27493;&#25552;&#39640;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;&#22312;&#23454;&#39564;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24615;&#33021;&#21644;&#25928;&#29575;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#26041;&#27861;&#22312;&#24314;&#27169;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#30340;&#36712;&#36857;&#26041;&#38754;&#24050;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24120;&#24120;&#38754;&#20020;&#35745;&#31639;&#25361;&#25112;&#65292;&#24182;&#19988;&#22312;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#21487;&#33021;&#19981;&#36275;&#65292;&#29305;&#21035;&#26159;&#22312;&#25429;&#25417;&#38271;&#26102;&#20219;&#21153;&#30340;&#26102;&#38388;&#25277;&#35937;&#26041;&#38754;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20998;&#23618;&#25193;&#25955;&#35268;&#21010;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#12289;&#24555;&#36895;&#12289;&#20294;&#20196;&#20154;&#24778;&#35766;&#22320;&#26377;&#25928;&#30340;&#35268;&#21010;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#20998;&#23618;&#21644;&#22522;&#20110;&#25193;&#25955;&#30340;&#35268;&#21010;&#30340;&#20248;&#28857;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#36739;&#39640;&#23618;&#38754;&#19978;&#37319;&#29992;&#20102;&#8220;&#36339;&#36291;&#8221;&#35268;&#21010;&#31574;&#30053;&#65292;&#20351;&#20854;&#20855;&#26377;&#36739;&#22823;&#30340;&#24863;&#21463;&#37326;&#20294;&#35745;&#31639;&#25104;&#26412;&#36739;&#20302; - &#36825;&#23545;&#20110;&#22522;&#20110;&#25193;&#25955;&#30340;&#35268;&#21010;&#26041;&#27861;&#26469;&#35828;&#26159;&#19968;&#20010;&#20851;&#38190;&#22240;&#32032;&#65292;&#25105;&#20204;&#24050;&#32463;&#32463;&#39564;&#24615;&#22320;&#39564;&#35777;&#20102;&#36825;&#19968;&#28857;&#12290;&#27492;&#22806;&#65292;&#36339;&#36291;&#30340;&#23376;&#30446;&#26631;&#25351;&#23548;&#25105;&#20204;&#30340;&#20302;&#23618;&#35268;&#21010;&#22120;&#65292;&#20419;&#36827;&#20102;&#19968;&#20010;&#24494;&#35843;&#38454;&#27573;&#65292;&#24182;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#23545;&#26631;&#20934;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#36827;&#34892;&#20102;&#32463;&#39564;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#22312;&#24615;&#33021;&#21644;&#25928;&#29575;&#26041;&#38754;&#30340;&#20248;&#36234;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion-based generative methods have proven effective in modeling trajectories with offline datasets. However, they often face computational challenges and can falter in generalization, especially in capturing temporal abstractions for long-horizon tasks. To overcome this, we introduce the Hierarchical Diffuser, a simple, fast, yet surprisingly effective planning method combining the advantages of hierarchical and diffusion-based planning. Our model adopts a "jumpy" planning strategy at the higher level, which allows it to have a larger receptive field but at a lower computational cost -- a crucial factor for diffusion-based planning methods, as we have empirically verified. Additionally, the jumpy sub-goals guide our low-level planner, facilitating a fine-tuning stage and further improving our approach's effectiveness. We conducted empirical evaluations on standard offline reinforcement learning benchmarks, demonstrating our method's superior performance and efficiency in terms of 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#26159;&#19968;&#39033;&#23545;&#22522;&#30784;&#27169;&#22411;&#30340;&#35757;&#32451;&#19982;&#37096;&#32626;&#31995;&#32479;&#36827;&#34892;&#32508;&#21512;&#35843;&#30740;&#30340;&#24037;&#20316;&#65292;&#35752;&#35770;&#20102;&#22312;&#35757;&#32451;&#21644;&#37096;&#32626;&#36807;&#31243;&#20013;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#37319;&#29992;&#30340;&#39640;&#25928;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2401.02643</link><description>&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#30340;&#35757;&#32451;&#19982;&#37096;&#32626;&#31995;&#32479;&#65306;&#19968;&#39033;&#32508;&#21512;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Training and Serving System of Foundation Models: A Comprehensive Survey. (arXiv:2401.02643v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02643
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#26159;&#19968;&#39033;&#23545;&#22522;&#30784;&#27169;&#22411;&#30340;&#35757;&#32451;&#19982;&#37096;&#32626;&#31995;&#32479;&#36827;&#34892;&#32508;&#21512;&#35843;&#30740;&#30340;&#24037;&#20316;&#65292;&#35752;&#35770;&#20102;&#22312;&#35757;&#32451;&#21644;&#37096;&#32626;&#36807;&#31243;&#20013;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#37319;&#29992;&#30340;&#39640;&#25928;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#65288;&#20363;&#22914;ChatGPT&#12289;DALL-E&#12289;&#24429;&#31243;Mind&#12289;PanGu-$\Sigma$&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35270;&#35273;&#35782;&#21035;&#31561;&#20851;&#38190;&#25216;&#26415;&#39046;&#22495;&#23637;&#29616;&#20102;&#38750;&#20961;&#30340;&#24615;&#33021;&#65292;&#24182;&#25104;&#20026;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#20027;&#27969;&#36235;&#21183;&#12290;&#36825;&#23548;&#33268;&#36234;&#26469;&#36234;&#22810;&#30340;&#31185;&#25216;&#24040;&#22836;&#25237;&#20837;&#22823;&#37327;&#20154;&#21147;&#21644;&#36130;&#21147;&#31215;&#26497;&#24320;&#21457;&#22522;&#30784;&#27169;&#22411;&#31995;&#32479;&#65292;&#25512;&#21160;&#20102;&#36825;&#20123;&#27169;&#22411;&#21442;&#25968;&#30340;&#25345;&#32493;&#22686;&#38271;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#35757;&#32451;&#19982;&#37096;&#32626;&#38754;&#20020;&#30528;&#26174;&#33879;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#22823;&#37327;&#30340;&#35745;&#31639;&#33021;&#21147;&#12289;&#20869;&#23384;&#28040;&#32791;&#12289;&#24102;&#23485;&#38656;&#27714;&#31561;&#12290;&#22240;&#27492;&#65292;&#37319;&#29992;&#39640;&#25928;&#30340;&#35757;&#32451;&#21644;&#37096;&#32626;&#31574;&#30053;&#21464;&#24471;&#23588;&#20026;&#20851;&#38190;&#12290;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#31215;&#26497;&#25506;&#32034;&#24182;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;&#22240;&#27492;&#65292;&#23545;&#23427;&#20204;&#36827;&#34892;&#32508;&#21512;&#35843;&#30740;&#23545;&#20110;&#31995;&#32479;&#24320;&#21457;&#32773;&#21644;&#30740;&#31350;&#20154;&#21592;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#24191;&#27867;&#25506;&#35752;&#20102;&#22522;&#30784;&#27169;&#22411;&#35757;&#32451;&#21644;&#37096;&#32626;&#20013;&#37319;&#29992;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation models (e.g., ChatGPT, DALL-E, PengCheng Mind, PanGu-$\Sigma$) have demonstrated extraordinary performance in key technological areas, such as natural language processing and visual recognition, and have become the mainstream trend of artificial general intelligence. This has led more and more major technology giants to dedicate significant human and financial resources to actively develop their foundation model systems, which drives continuous growth of these models' parameters. As a result, the training and serving of these models have posed significant challenges, including substantial computing power, memory consumption, bandwidth demands, etc. Therefore, employing efficient training and serving strategies becomes particularly crucial. Many researchers have actively explored and proposed effective methods. So, a comprehensive survey of them is essential for system developers and researchers. This paper extensively explores the methods employed in training and serving fou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#20351;&#29992;AI&#29983;&#25104;&#30340;&#20154;&#33080;&#20316;&#20026;&#22836;&#20687;&#30340;&#20266;&#36896;&#31038;&#20132;&#23186;&#20307;&#36134;&#25143;&#65292;&#21457;&#29616;&#23427;&#20204;&#29992;&#20110;&#20256;&#25773;&#35784;&#39575;&#12289;&#22403;&#22334;&#20449;&#24687;&#20197;&#21450;&#25918;&#22823;&#21327;&#21516;&#20449;&#24687;&#31561;&#19981;&#30495;&#23454;&#27963;&#21160;&#12290;&#36890;&#36807;&#24320;&#21457;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#20316;&#32773;&#20272;&#35745;&#20351;&#29992;GAN&#29983;&#25104;&#30340;&#38754;&#23380;&#30340;&#36134;&#25143;&#26222;&#36941;&#24615;&#19979;&#38480;&#22312;0.021%&#21040;0.044%&#20043;&#38388;&#65292;&#32422;&#20026;&#27599;&#26085;&#27963;&#36291;&#36134;&#25143;10K&#20010;&#12290;</title><link>http://arxiv.org/abs/2401.02627</link><description>&lt;p&gt;
&#20266;&#36896;&#31038;&#20132;&#23186;&#20307;&#36134;&#25143;&#30340;&#29305;&#28857;&#21644;&#26222;&#36941;&#24615;&#65292;&#20351;&#29992;&#30340;&#26159;AI&#29983;&#25104;&#30340;&#38754;&#23380;
&lt;/p&gt;
&lt;p&gt;
Characteristics and prevalence of fake social media profiles with AI-generated faces. (arXiv:2401.02627v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02627
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#20351;&#29992;AI&#29983;&#25104;&#30340;&#20154;&#33080;&#20316;&#20026;&#22836;&#20687;&#30340;&#20266;&#36896;&#31038;&#20132;&#23186;&#20307;&#36134;&#25143;&#65292;&#21457;&#29616;&#23427;&#20204;&#29992;&#20110;&#20256;&#25773;&#35784;&#39575;&#12289;&#22403;&#22334;&#20449;&#24687;&#20197;&#21450;&#25918;&#22823;&#21327;&#21516;&#20449;&#24687;&#31561;&#19981;&#30495;&#23454;&#27963;&#21160;&#12290;&#36890;&#36807;&#24320;&#21457;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#20316;&#32773;&#20272;&#35745;&#20351;&#29992;GAN&#29983;&#25104;&#30340;&#38754;&#23380;&#30340;&#36134;&#25143;&#26222;&#36941;&#24615;&#19979;&#38480;&#22312;0.021%&#21040;0.044%&#20043;&#38388;&#65292;&#32422;&#20026;&#27599;&#26085;&#27963;&#36291;&#36134;&#25143;10K&#20010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#27169;&#22411;&#30340;&#36827;&#27493;&#24341;&#21457;&#20102;&#23545;&#20854;&#21487;&#33021;&#21019;&#24314;&#20986;&#36924;&#30495;&#20266;&#36896;&#31038;&#20132;&#23186;&#20307;&#36134;&#25143;&#30340;&#25285;&#24551;&#65292;&#20294;&#32570;&#20047;&#23454;&#35777;&#35777;&#25454;&#12290;&#26412;&#25991;&#23545;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#29983;&#25104;&#30340;&#20154;&#33080;&#20316;&#20026;&#22836;&#20687;&#30340;Twitter(X)&#36134;&#25143;&#36827;&#34892;&#20102;&#31995;&#32479;&#20998;&#26512;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#21253;&#21547;1,353&#20010;&#27492;&#31867;&#36134;&#25143;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#29992;&#20110;&#20256;&#25773;&#35784;&#39575;&#12289;&#22403;&#22334;&#20449;&#24687;&#20197;&#21450;&#25918;&#22823;&#21327;&#21516;&#20449;&#24687;&#31561;&#19981;&#30495;&#23454;&#27963;&#21160;&#12290;&#36890;&#36807;&#21033;&#29992;GAN&#29983;&#25104;&#30340;&#38754;&#23380;&#30340;&#19968;&#20010;&#29305;&#24449;&#8212;&#8212;&#30524;&#30555;&#30340;&#19968;&#33268;&#20301;&#32622;&#65292;&#24182;&#19982;&#20154;&#24037;&#27880;&#37322;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#37326;&#22806;&#20013;&#20351;&#29992;GAN&#29983;&#25104;&#30340;&#36134;&#25143;&#12290;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#38543;&#26426;&#26679;&#26412;&#30340;&#27963;&#36291;Twitter&#29992;&#25143;&#20013;&#65292;&#25105;&#20204;&#20272;&#35745;&#20351;&#29992;GAN&#29983;&#25104;&#30340;&#38754;&#23380;&#30340;&#36134;&#25143;&#26222;&#36941;&#24615;&#19979;&#38480;&#22312;0.021%&#21040;0.044%&#20043;&#38388;&#65292;&#32422;&#20026;&#27599;&#26085;&#27963;&#36291;&#36134;&#25143;10K&#20010;&#12290;&#36825;&#20123;&#21457;&#29616;&#31361;&#26174;&#20102;&#22810;&#27169;&#24335;&#36134;&#25143;&#23545;&#20110;&#26032;&#20852;&#23041;&#32961;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in generative artificial intelligence (AI) have raised concerns about their potential to create convincing fake social media accounts, but empirical evidence is lacking. In this paper, we present a systematic analysis of Twitter(X) accounts using human faces generated by Generative Adversarial Networks (GANs) for their profile pictures. We present a dataset of 1,353 such accounts and show that they are used to spread scams, spam, and amplify coordinated messages, among other inauthentic activities. Leveraging a feature of GAN-generated faces -- consistent eye placement -- and supplementing it with human annotation, we devise an effective method for identifying GAN-generated profiles in the wild. Applying this method to a random sample of active Twitter users, we estimate a lower bound for the prevalence of profiles using GAN-generated faces between 0.021% and 0.044% -- around 10K daily active accounts. These findings underscore the emerging threats posed by multimod
&lt;/p&gt;</description></item><item><title>3D&#29983;&#25104;AI&#39046;&#22495;&#30340;&#36827;&#23637;&#21450;&#21069;&#26223;&#12290;&#31283;&#23450;&#30340;&#25193;&#25955;&#26041;&#27861;&#12289;&#22810;&#35270;&#35282;&#19968;&#33268;&#24615;&#25511;&#21046;&#21644;&#36924;&#30495;&#30340;&#20154;&#20307;&#27169;&#22411;&#20026;&#39640;&#19968;&#33268;&#24615;&#21644;&#36924;&#30495;&#22806;&#35266;&#30340;3D&#27169;&#22411;&#30340;&#29983;&#25104;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;&#31070;&#32463;&#32593;&#32476;&#22522;&#30784;&#30340;3D&#23384;&#20648;&#21644;&#28210;&#26579;&#27169;&#22411;&#21152;&#36895;&#20102;&#31070;&#32463;&#28210;&#26579;&#27169;&#22411;&#30340;&#25928;&#29575;&#21644;&#36924;&#30495;&#24230;&#12290;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#33021;&#21147;&#22686;&#21152;&#20102;&#23545;&#22810;&#27169;&#24577;&#22330;&#26223;&#29983;&#25104;&#30340;&#28789;&#27963;&#24615;&#21644;&#21019;&#36896;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.02620</link><description>&lt;p&gt;
3D&#29983;&#25104;AI&#20013;&#30340;&#36827;&#23637;&#19982;&#21069;&#26223;: &#21253;&#25324;3D&#20154;&#20307;&#30340;&#25216;&#26415;&#27010;&#36848;
&lt;/p&gt;
&lt;p&gt;
Progress and Prospects in 3D Generative AI: A Technical Overview including 3D human. (arXiv:2401.02620v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02620
&lt;/p&gt;
&lt;p&gt;
3D&#29983;&#25104;AI&#39046;&#22495;&#30340;&#36827;&#23637;&#21450;&#21069;&#26223;&#12290;&#31283;&#23450;&#30340;&#25193;&#25955;&#26041;&#27861;&#12289;&#22810;&#35270;&#35282;&#19968;&#33268;&#24615;&#25511;&#21046;&#21644;&#36924;&#30495;&#30340;&#20154;&#20307;&#27169;&#22411;&#20026;&#39640;&#19968;&#33268;&#24615;&#21644;&#36924;&#30495;&#22806;&#35266;&#30340;3D&#27169;&#22411;&#30340;&#29983;&#25104;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;&#31070;&#32463;&#32593;&#32476;&#22522;&#30784;&#30340;3D&#23384;&#20648;&#21644;&#28210;&#26579;&#27169;&#22411;&#21152;&#36895;&#20102;&#31070;&#32463;&#28210;&#26579;&#27169;&#22411;&#30340;&#25928;&#29575;&#21644;&#36924;&#30495;&#24230;&#12290;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#33021;&#21147;&#22686;&#21152;&#20102;&#23545;&#22810;&#27169;&#24577;&#22330;&#26223;&#29983;&#25104;&#30340;&#28789;&#27963;&#24615;&#21644;&#21019;&#36896;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#21644;2D&#22270;&#20687;&#32487;&#32493;&#25193;&#23637;&#20854;&#39046;&#22495;&#26102;&#65292;3D&#29983;&#25104;&#36880;&#28176;&#25104;&#20026;&#19968;&#20010;&#19981;&#21487;&#24573;&#35270;&#30340;&#36235;&#21183;&#12290;&#33258;2023&#24180;&#20197;&#26469;&#65292;&#22823;&#37327;&#30340;&#30740;&#31350;&#35770;&#25991;&#24050;&#22312;3D&#29983;&#25104;&#39046;&#22495;&#28044;&#29616;&#12290;&#36825;&#31181;&#22686;&#38271;&#19981;&#20165;&#21253;&#25324;3D&#29289;&#20307;&#30340;&#21019;&#36896;&#65292;&#36824;&#21253;&#25324;3D&#35282;&#33394;&#21644;&#21160;&#30011;&#29983;&#25104;&#30340;&#24555;&#36895;&#21457;&#23637;&#12290;&#20960;&#20010;&#20851;&#38190;&#22240;&#32032;&#20419;&#36827;&#20102;&#36825;&#19968;&#36827;&#23637;&#12290;&#31283;&#23450;&#25193;&#25955;&#20013;&#30340;&#22686;&#24378;&#20445;&#30495;&#24230;&#65292;&#32467;&#21512;&#20445;&#35777;&#22810;&#35270;&#35282;&#19968;&#33268;&#24615;&#21644;&#36924;&#30495;&#20154;&#20307;&#27169;&#22411;&#65288;&#22914;SMPL-X&#65289;&#30340;&#25511;&#21046;&#26041;&#27861;&#65292;&#21327;&#21516;&#20419;&#36827;&#20102;&#20855;&#26377;&#26174;&#33879;&#19968;&#33268;&#24615;&#21644;&#25509;&#36817;&#36924;&#30495;&#22806;&#35266;&#30340;3D&#27169;&#22411;&#30340;&#20135;&#29983;&#12290;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;3D&#20648;&#23384;&#21644;&#28210;&#26579;&#27169;&#22411;&#30340;&#36827;&#27493;(&#22914;Neural Radiance Fields (NeRF)&#21644;3D Gaussian Splatting (3DGS))&#21152;&#36895;&#20102;&#31070;&#32463;&#28210;&#26579;&#27169;&#22411;&#30340;&#25928;&#29575;&#21644;&#36924;&#30495;&#24230;&#12290;&#27492;&#22806;&#65292;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#33021;&#21147;&#20351;&#35821;&#35328;&#29983;&#25104;&#19982;3D&#27169;&#22411;&#29983;&#25104;&#32039;&#23494;&#32467;&#21512;&#65292;&#20026;&#22810;&#27169;&#24577;&#22330;&#26223;&#29983;&#25104;&#25552;&#20379;&#20102;&#26356;&#39640;&#30340;&#28789;&#27963;&#24615;&#21644;&#21019;&#36896;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
While AI-generated text and 2D images continue to expand its territory, 3D generation has gradually emerged as a trend that cannot be ignored. Since the year 2023 an abundant amount of research papers has emerged in the domain of 3D generation. This growth encompasses not just the creation of 3D objects, but also the rapid development of 3D character and motion generation. Several key factors contribute to this progress. The enhanced fidelity in stable diffusion, coupled with control methods that ensure multi-view consistency, and realistic human models like SMPL-X, contribute synergistically to the production of 3D models with remarkable consistency and near-realistic appearances. The advancements in neural network-based 3D storing and rendering models, such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS), have accelerated the efficiency and realism of neural rendered models. Furthermore, the multimodality capabilities of large language models have enabled language i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#22240;&#26524;&#25277;&#35937;&#26041;&#27861;&#65292;&#36890;&#36807;&#32858;&#31867;&#21464;&#37327;&#21644;&#20854;&#22495;&#65292;&#29992;&#20110;&#35299;&#20915;&#30495;&#23454;&#22240;&#26524;&#25512;&#26029;&#20219;&#21153;&#20013;&#30340;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#31070;&#32463;&#22240;&#26524;&#27169;&#22411;&#23454;&#29616;&#20102;&#23398;&#20064;&#21644;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.02602</link><description>&lt;p&gt;
&#31070;&#32463;&#22240;&#26524;&#25277;&#35937;
&lt;/p&gt;
&lt;p&gt;
Neural Causal Abstractions. (arXiv:2401.02602v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02602
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#22240;&#26524;&#25277;&#35937;&#26041;&#27861;&#65292;&#36890;&#36807;&#32858;&#31867;&#21464;&#37327;&#21644;&#20854;&#22495;&#65292;&#29992;&#20110;&#35299;&#20915;&#30495;&#23454;&#22240;&#26524;&#25512;&#26029;&#20219;&#21153;&#20013;&#30340;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#31070;&#32463;&#22240;&#26524;&#27169;&#22411;&#23454;&#29616;&#20102;&#23398;&#20064;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#29702;&#35299;&#19990;&#30028;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#20197;&#21450;&#23558;&#20449;&#24687;&#21387;&#32553;&#25104;&#25277;&#35937;&#27010;&#24565;&#30340;&#33021;&#21147;&#26159;&#20154;&#31867;&#26234;&#24935;&#30340;&#20004;&#20010;&#26631;&#24535;&#24615;&#29305;&#24449;&#12290;&#36825;&#20004;&#20010;&#20027;&#39064;&#22312;&#25991;&#29486;&#20013;&#34987;&#32479;&#31216;&#20026;&#22240;&#26524;&#25277;&#35937;&#29702;&#35770;&#21516;&#26102;&#36827;&#34892;&#30740;&#31350;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#22914;&#20309;&#22312;&#30495;&#23454;&#30340;&#22240;&#26524;&#25512;&#26029;&#20219;&#21153;&#20013;&#20805;&#20998;&#21033;&#29992;&#25277;&#35937;&#29702;&#35770;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#30495;&#23454;&#26426;&#21046;&#26159;&#26410;&#30693;&#30340;&#65292;&#21482;&#26377;&#26377;&#38480;&#30340;&#25968;&#25454;&#21487;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#21464;&#37327;&#21450;&#20854;&#22495;&#36827;&#34892;&#32858;&#31867;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#22240;&#26524;&#25277;&#35937;&#23478;&#26063;&#12290;&#36825;&#31181;&#26041;&#27861;&#25913;&#36827;&#21644;&#27010;&#25324;&#20102;&#20043;&#21069;&#30340;&#25277;&#35937;&#27010;&#24565;&#65292;&#20197;&#26356;&#22909;&#22320;&#36866;&#24212;Pearl&#30340;&#22240;&#26524;&#23618;&#27425;&#32467;&#26500;&#24341;&#21457;&#30340;&#20010;&#20307;&#22240;&#26524;&#20998;&#24067;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#36890;&#36807;&#31070;&#32463;&#22240;&#26524;&#27169;&#22411;&#65288;Xia&#31561;&#65292;2021&#65289;&#21487;&#20197;&#23398;&#24471;&#36825;&#26679;&#30340;&#25277;&#35937;&#27010;&#24565;&#65292;&#20174;&#32780;&#33021;&#22815;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#35299;&#20915;&#21508;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22240;&#26524;&#25512;&#26029;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
The abilities of humans to understand the world in terms of cause and effect relationships, as well as to compress information into abstract concepts, are two hallmark features of human intelligence. These two topics have been studied in tandem in the literature under the rubric of causal abstractions theory. In practice, it remains an open problem how to best leverage abstraction theory in real-world causal inference tasks, where the true mechanisms are unknown and only limited data is available. In this paper, we develop a new family of causal abstractions by clustering variables and their domains. This approach refines and generalizes previous notions of abstractions to better accommodate individual causal distributions that are spawned by Pearl's causal hierarchy. We show that such abstractions are learnable in practical settings through Neural Causal Models (Xia et al., 2021), enabling the use of the deep learning toolkit to solve various challenging causal inference tasks -- iden
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22270;&#20687;&#25551;&#36848;&#27169;&#22411;&#30340;&#38754;&#21521;&#23545;&#35937;&#21518;&#38376;&#25915;&#20987;&#65292;&#36890;&#36807;&#27745;&#26579;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#38754;&#21521;&#23545;&#35937;&#30340;&#26041;&#27861;&#26469;&#21046;&#20316;&#27745;&#26579;&#26679;&#26412;&#12290;&#25915;&#20987;&#21518;&#65292;&#27169;&#22411;&#22312;&#33391;&#24615;&#22270;&#20687;&#19978;&#34920;&#29616;&#27491;&#24120;&#65292;&#20294;&#23545;&#20110;&#27745;&#26579;&#22270;&#20687;&#65292;&#27169;&#22411;&#23558;&#29983;&#25104;&#19982;&#22270;&#20687;&#26080;&#20851;&#30340;&#21477;&#23376;&#12290;</title><link>http://arxiv.org/abs/2401.02600</link><description>&lt;p&gt;
&#38754;&#21521;&#22270;&#20687;&#25551;&#36848;&#30340;&#38754;&#21521;&#23545;&#35937;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Object-oriented backdoor attack against image captioning. (arXiv:2401.02600v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02600
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22270;&#20687;&#25551;&#36848;&#27169;&#22411;&#30340;&#38754;&#21521;&#23545;&#35937;&#21518;&#38376;&#25915;&#20987;&#65292;&#36890;&#36807;&#27745;&#26579;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#38754;&#21521;&#23545;&#35937;&#30340;&#26041;&#27861;&#26469;&#21046;&#20316;&#27745;&#26579;&#26679;&#26412;&#12290;&#25915;&#20987;&#21518;&#65292;&#27169;&#22411;&#22312;&#33391;&#24615;&#22270;&#20687;&#19978;&#34920;&#29616;&#27491;&#24120;&#65292;&#20294;&#23545;&#20110;&#27745;&#26579;&#22270;&#20687;&#65292;&#27169;&#22411;&#23558;&#29983;&#25104;&#19982;&#22270;&#20687;&#26080;&#20851;&#30340;&#21477;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23545;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#36827;&#34892;&#21518;&#38376;&#25915;&#20987;&#24050;&#32463;&#34987;&#24191;&#27867;&#30740;&#31350;&#24182;&#34987;&#35777;&#26126;&#25104;&#21151;&#30340;&#21516;&#26102;&#65292;&#23545;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21518;&#38376;&#25915;&#20987;&#30340;&#30740;&#31350;&#36824;&#24456;&#23569;&#12290;&#26412;&#25991;&#36890;&#36807;&#27745;&#26579;&#35757;&#32451;&#25968;&#25454;&#65292;&#25506;&#32034;&#20102;&#23545;&#22270;&#20687;&#25551;&#36848;&#27169;&#22411;&#30340;&#21518;&#38376;&#25915;&#20987;&#12290;&#20551;&#35774;&#25915;&#20987;&#32773;&#23436;&#20840;&#35775;&#38382;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#20294;&#19981;&#33021;&#24178;&#39044;&#27169;&#22411;&#30340;&#26500;&#24314;&#25110;&#35757;&#32451;&#36807;&#31243;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#38543;&#26426;&#36873;&#25321;&#37096;&#20998;&#33391;&#24615;&#35757;&#32451;&#26679;&#26412;&#36827;&#34892;&#27745;&#26579;&#12290;&#28982;&#21518;&#65292;&#32771;&#34385;&#21040;&#25551;&#36848;&#36890;&#24120;&#22260;&#32469;&#22270;&#20687;&#20013;&#30340;&#23545;&#35937;&#23637;&#24320;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#38754;&#21521;&#23545;&#35937;&#30340;&#26041;&#27861;&#26469;&#21046;&#20316;&#27745;&#26579;&#26679;&#26412;&#65292;&#35813;&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#24494;&#23567;&#30340;&#20687;&#32032;&#20540;&#20462;&#25913;&#26469;&#20462;&#25913;&#24403;&#21069;&#26816;&#27979;&#21040;&#30340;&#23545;&#35937;&#21306;&#22495;&#30340;&#35268;&#27169;&#30456;&#24212;&#30340;&#20462;&#25913;&#25968;&#37327;&#12290;&#22312;&#20351;&#29992;&#32463;&#36807;&#27745;&#26579;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#19978;&#65292;&#23545;&#20110;&#33391;&#24615;&#22270;&#20687;&#65292;&#25915;&#20987;&#27169;&#22411;&#30340;&#34892;&#20026;&#27491;&#24120;&#65292;&#20294;&#23545;&#20110;&#27745;&#26579;&#22270;&#20687;&#65292;&#27169;&#22411;&#23558;&#29983;&#25104;&#19982;&#32473;&#23450;&#22270;&#20687;&#26080;&#20851;&#30340;&#21477;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
Backdoor attack against image classification task has been widely studied and proven to be successful, while there exist little research on the backdoor attack against vision-language models. In this paper, we explore backdoor attack towards image captioning models by poisoning training data. Assuming the attacker has total access to the training dataset, and cannot intervene in model construction or training process. Specifically, a portion of benign training samples is randomly selected to be poisoned. Afterwards, considering that the captions are usually unfolded around objects in an image, we design an object-oriented method to craft poisons, which aims to modify pixel values by a slight range with the modification number proportional to the scale of the current detected object region. After training with the poisoned data, the attacked model behaves normally on benign images, but for poisoned images, the model will generate some sentences irrelevant to the given image. The attack 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#26469;&#24179;&#34913;&#31867;&#21035;&#19981;&#24179;&#34913;&#25968;&#25454;&#30340;&#25216;&#26415;&#65292;&#20248;&#20808;&#24179;&#34913;&#20449;&#24687;&#20016;&#23500;&#21306;&#22495;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#31867;&#21035;&#21518;&#39564;&#27604;&#29575;&#26469;&#26368;&#22823;&#21270;&#22312;&#27491;&#30830;&#30340;&#31867;&#21035;&#21306;&#22495;&#29983;&#25104;&#21512;&#25104;&#26679;&#26412;&#30340;&#27010;&#29575;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#25216;&#26415;&#22312;&#25552;&#21319;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26041;&#38754;&#20855;&#26377;&#21331;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.02591</link><description>&lt;p&gt;
&#36890;&#36807;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#26469;&#23454;&#29616;&#23545;&#19981;&#22343;&#34913;&#25968;&#25454;&#19978;&#30340;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#22823;&#21518;&#39564;&#27604;&#29575;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Synthetic Information towards Maximum Posterior Ratio for deep learning on Imbalanced Data. (arXiv:2401.02591v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02591
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#26469;&#24179;&#34913;&#31867;&#21035;&#19981;&#24179;&#34913;&#25968;&#25454;&#30340;&#25216;&#26415;&#65292;&#20248;&#20808;&#24179;&#34913;&#20449;&#24687;&#20016;&#23500;&#21306;&#22495;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#31867;&#21035;&#21518;&#39564;&#27604;&#29575;&#26469;&#26368;&#22823;&#21270;&#22312;&#27491;&#30830;&#30340;&#31867;&#21035;&#21306;&#22495;&#29983;&#25104;&#21512;&#25104;&#26679;&#26412;&#30340;&#27010;&#29575;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#25216;&#26415;&#22312;&#25552;&#21319;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26041;&#38754;&#20855;&#26377;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26816;&#39564;&#20102;&#31867;&#21035;&#19981;&#24179;&#34913;&#25968;&#25454;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#29983;&#25104;&#23569;&#25968;&#31867;&#21035;&#30340;&#21512;&#25104;&#25968;&#25454;&#26469;&#24179;&#34913;&#25968;&#25454;&#30340;&#25216;&#26415;&#12290;&#19982;&#22522;&#20110;&#38543;&#26426;&#36807;&#37319;&#26679;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20808;&#24179;&#34913;&#20449;&#24687;&#20016;&#23500;&#21306;&#22495;&#65292;&#36890;&#36807;&#35782;&#21035;&#39640;&#29109;&#26679;&#26412;&#12290;&#29983;&#25104;&#36866;&#24403;&#20301;&#32622;&#30340;&#21512;&#25104;&#25968;&#25454;&#21487;&#20197;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#65292;&#32780;&#29983;&#25104;&#20301;&#32622;&#19981;&#24403;&#30340;&#21512;&#25104;&#25968;&#25454;&#21487;&#33021;&#23548;&#33268;&#26356;&#39640;&#30340;&#35823;&#20998;&#31867;&#29575;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#20248;&#21270;&#31867;&#21035;&#21518;&#39564;&#27604;&#29575;&#26469;&#26368;&#22823;&#21270;&#22312;&#27491;&#30830;&#30340;&#31867;&#21035;&#21306;&#22495;&#29983;&#25104;&#21512;&#25104;&#26679;&#26412;&#30340;&#27010;&#29575;&#30340;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20445;&#25345;&#25968;&#25454;&#25299;&#25169;&#65292;&#21512;&#25104;&#25968;&#25454;&#22312;&#27599;&#20010;&#23569;&#25968;&#31867;&#21035;&#26679;&#26412;&#30340;&#37051;&#22495;&#20869;&#29983;&#25104;&#12290;&#25105;&#20204;&#22312;41&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#25105;&#20204;&#30340;&#25216;&#26415;&#22312;&#25552;&#21319;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26041;&#38754;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study examines the impact of class-imbalanced data on deep learning models and proposes a technique for data balancing by generating synthetic data for the minority class. Unlike random-based oversampling, our method prioritizes balancing the informative regions by identifying high entropy samples. Generating well-placed synthetic data can enhance machine learning algorithms accuracy and efficiency, whereas poorly-placed ones may lead to higher misclassification rates. We introduce an algorithm that maximizes the probability of generating a synthetic sample in the correct region of its class by optimizing the class posterior ratio. Additionally, to maintain data topology, synthetic data are generated within each minority sample's neighborhood. Our experimental results on forty-one datasets demonstrate the superior performance of our technique in enhancing deep-learning models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;4FGL DR3&#19981;&#30830;&#23450;&#28304;&#20013;&#25214;&#21040;AGN&#20505;&#36873;&#28304;&#21644;&#35782;&#21035;BL Lac/FSRQ&#20505;&#36873;&#28304;&#30340;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FDIDWT&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#36870;&#31163;&#25955;&#23567;&#27874;&#21464;&#25442;&#65288;IDWT&#65289;&#30340;&#22810;&#20998;&#36776;&#29575;&#20998;&#26512;&#21644;&#20998;&#24418;&#32500;&#25968;&#65288;FD&#65289;&#29702;&#35770;&#30340;&#30456;&#20851;&#29305;&#24449;&#20272;&#35745;&#65292;&#23558;&#21407;&#22987;&#25968;&#25454;&#36716;&#25442;&#25104;&#20302;&#32500;&#24230;&#21644;&#31361;&#20986;&#29305;&#24449;&#30340;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2401.02589</link><description>&lt;p&gt;
&#21033;&#29992;&#36870;&#31163;&#25955;&#23567;&#27874;&#21464;&#25442;&#22312;&#26356;&#39640;&#20998;&#36776;&#29575;&#19979;&#35782;&#21035;4FGL&#19981;&#30830;&#23450;&#28304;
&lt;/p&gt;
&lt;p&gt;
Identification of 4FGL uncertain sources at Higher Resolutions with Inverse Discrete Wavelet Transform. (arXiv:2401.02589v1 [astro-ph.HE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02589
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;4FGL DR3&#19981;&#30830;&#23450;&#28304;&#20013;&#25214;&#21040;AGN&#20505;&#36873;&#28304;&#21644;&#35782;&#21035;BL Lac/FSRQ&#20505;&#36873;&#28304;&#30340;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FDIDWT&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#36870;&#31163;&#25955;&#23567;&#27874;&#21464;&#25442;&#65288;IDWT&#65289;&#30340;&#22810;&#20998;&#36776;&#29575;&#20998;&#26512;&#21644;&#20998;&#24418;&#32500;&#25968;&#65288;FD&#65289;&#29702;&#35770;&#30340;&#30456;&#20851;&#29305;&#24449;&#20272;&#35745;&#65292;&#23558;&#21407;&#22987;&#25968;&#25454;&#36716;&#25442;&#25104;&#20302;&#32500;&#24230;&#21644;&#31361;&#20986;&#29305;&#24449;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#22825;&#25991;&#25968;&#25454;&#26102;&#20195;&#26469;&#20020;&#20043;&#38469;&#65292;&#20174;&#22320;&#22522;&#21644;&#31354;&#38388;&#26395;&#36828;&#38236;&#20013;&#25214;&#21040;&#30446;&#26631;&#28304;&#26159;&#19968;&#39033;&#36127;&#25285;&#12290;&#34429;&#28982;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#27861;&#24050;&#34987;&#24191;&#27867;&#29992;&#20110;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#28145;&#20837;&#25968;&#25454;&#20998;&#26512;&#30340;&#25972;&#21512;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22788;&#29702;&#22823;&#37327;&#22825;&#25991;&#25968;&#25454;&#26102;&#35782;&#21035;&#30446;&#26631;&#28304;&#30340;&#25928;&#29575;&#12290;&#26412;&#30740;&#31350;&#37325;&#28857;&#20851;&#27880;&#20174;4FGL DR3&#19981;&#30830;&#23450;&#28304;&#20013;&#25214;&#21040;AGN&#20505;&#36873;&#28304;&#21644;&#35782;&#21035;BL Lac/FSRQ&#20505;&#36873;&#28304;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;4FGL DR3&#30446;&#24405;&#23646;&#24615;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FDIDWT&#30340;&#26032;&#26041;&#27861;&#20197;&#36716;&#25442;&#21407;&#22987;&#25968;&#25454;&#12290;&#36716;&#25442;&#21518;&#30340;&#25968;&#25454;&#38598;&#20855;&#26377;&#20302;&#32500;&#24230;&#21644;&#31361;&#20986;&#29305;&#24449;&#65292;&#36890;&#36807;&#20998;&#24418;&#32500;&#25968;&#65288;FD&#65289;&#29702;&#35770;&#30340;&#30456;&#20851;&#29305;&#24449;&#20272;&#35745;&#21644;&#36870;&#31163;&#25955;&#23567;&#27874;&#21464;&#25442;&#65288;IDWT&#65289;&#30340;&#22810;&#20998;&#36776;&#29575;&#20998;&#26512;&#12290;&#23558;FDIDWT&#26041;&#27861;&#19982;&#25913;&#36827;&#30340;&#36731;&#37327;&#32423;MatchboxCon&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;
&lt;/p&gt;
&lt;p&gt;
In the forthcoming era of big astronomical data, it is a burden to find out target sources from ground-based and space-based telescopes. Although Machine Learning (ML) methods have been extensively utilized to address this issue, the incorporation of in-depth data analysis can significantly enhance the efficiency of identifying target sources when dealing with massive volumes of astronomical data. In this work, we focused on the task of finding AGN candidates and identifying BL Lac/FSRQ candidates from the 4FGL DR3 uncertain sources. We studied the correlations among the attributes of the 4FGL DR3 catalogue and proposed a novel method, named FDIDWT, to transform the original data. The transformed dataset is characterized as low-dimensional and feature-highlighted, with the estimation of correlation features by Fractal Dimension (FD) theory and the multi-resolution analysis by Inverse Discrete Wavelet Transform (IDWT). Combining the FDIDWT method with an improved lightweight MatchboxCon
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;3D&#39640;&#26031;&#31890;&#23376;&#25237;&#24433;&#30340;&#26041;&#27861;&#26469;&#26144;&#23556;&#21355;&#26143;&#22312;&#36712;&#36947;&#19978;&#30340;&#20960;&#20309;&#24418;&#29366;&#65292;&#33021;&#22815;&#22312;&#24403;&#21069;&#33322;&#22825;&#30828;&#20214;&#19978;&#36816;&#34892;&#65292;&#24182;&#19988;&#22312;&#35745;&#31639;&#36895;&#24230;&#19978;&#27604;&#20043;&#21069;&#24555;&#36817;&#20004;&#20010;&#25968;&#37327;&#32423;&#65292;&#21487;&#20197;&#35757;&#32451;&#21644;&#28210;&#26579;&#26410;&#30693;&#21355;&#26143;&#30340;&#39640;&#36136;&#37327;&#26032;&#35270;&#35282;&#12290;</title><link>http://arxiv.org/abs/2401.02588</link><description>&lt;p&gt;
&#36890;&#36807;&#21152;&#36895;3D&#39640;&#26031;&#31890;&#23376;&#25237;&#24433;&#26469;&#34920;&#24449;&#21355;&#26143;&#20960;&#20309;&#24418;&#29366;
&lt;/p&gt;
&lt;p&gt;
Characterizing Satellite Geometry via Accelerated 3D Gaussian Splatting. (arXiv:2401.02588v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02588
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;3D&#39640;&#26031;&#31890;&#23376;&#25237;&#24433;&#30340;&#26041;&#27861;&#26469;&#26144;&#23556;&#21355;&#26143;&#22312;&#36712;&#36947;&#19978;&#30340;&#20960;&#20309;&#24418;&#29366;&#65292;&#33021;&#22815;&#22312;&#24403;&#21069;&#33322;&#22825;&#30828;&#20214;&#19978;&#36816;&#34892;&#65292;&#24182;&#19988;&#22312;&#35745;&#31639;&#36895;&#24230;&#19978;&#27604;&#20043;&#21069;&#24555;&#36817;&#20004;&#20010;&#25968;&#37327;&#32423;&#65292;&#21487;&#20197;&#35757;&#32451;&#21644;&#28210;&#26579;&#26410;&#30693;&#21355;&#26143;&#30340;&#39640;&#36136;&#37327;&#26032;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21355;&#26143;&#22312;&#36712;&#36947;&#19978;&#30340;&#21152;&#36895;&#37096;&#32626;&#24341;&#36215;&#20102;&#23545;&#22312;&#36712;&#32500;&#20462;&#65288;OOS&#65289;&#12289;&#21355;&#26143;&#26816;&#26597;&#21644;&#20027;&#21160;&#28165;&#38500;&#30862;&#29255;&#65288;ADR&#65289;&#30340;&#20852;&#36259;&#12290;&#36825;&#31867;&#20219;&#21153;&#38656;&#35201;&#22312;&#38750;&#21512;&#20316;&#30340;&#12289;&#21487;&#33021;&#26159;&#26410;&#30693;&#30340;&#39547;&#30041;&#31354;&#38388;&#29289;&#20307;&#38468;&#36817;&#36827;&#34892;&#31934;&#30830;&#30340;&#20132;&#20250;&#21644;&#38752;&#36817;&#25805;&#20316;&#12290;&#30001;&#20110;&#36733;&#20154;&#20219;&#21153;&#30340;&#23433;&#20840;&#38382;&#39064;&#21644;&#22320;&#38754;&#25511;&#21046;&#30340;&#24310;&#36831;&#65292;&#24517;&#39035;&#23454;&#29616;&#23436;&#20840;&#33258;&#20027;&#21270;&#12290;&#36825;&#38656;&#35201;&#23545;&#30446;&#26631;&#30340;&#20960;&#20309;&#24418;&#29366;&#36827;&#34892;&#21487;&#38752;&#30340;&#34920;&#24449;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;3D&#39640;&#26031;&#31890;&#23376;&#25237;&#24433;&#30340;&#21355;&#26143;&#20960;&#20309;&#24418;&#29366;&#26144;&#23556;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#24403;&#21069;&#33322;&#22825;&#30828;&#20214;&#19978;&#30340;&#35745;&#31639;&#36164;&#28304;&#19978;&#36816;&#34892;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#30828;&#20214;&#29615;&#36335;&#21355;&#26143;&#27169;&#25311;&#35013;&#32622;&#19978;&#23637;&#31034;&#20102;&#27169;&#22411;&#35757;&#32451;&#21644;3D&#28210;&#26579;&#24615;&#33021;&#65292;&#22312;&#20960;&#31181;&#36924;&#30495;&#30340;&#20809;&#29031;&#21644;&#36816;&#21160;&#26465;&#20214;&#19979;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#26174;&#31034;&#20986;&#33021;&#22815;&#22312;&#26426;&#36733;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#20197;&#27604;&#20043;&#21069;&#24555;&#36817;&#20004;&#20010;&#25968;&#37327;&#32423;&#30340;&#36895;&#24230;&#29983;&#25104;&#26410;&#30693;&#21355;&#26143;&#30340;&#26356;&#39640;&#36136;&#37327;&#30340;&#26032;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
The accelerating deployment of spacecraft in orbit have generated interest in on-orbit servicing (OOS), inspection of spacecraft, and active debris removal (ADR). Such missions require precise rendezvous and proximity operations in the vicinity of non-cooperative, possible unknown, resident space objects. Safety concerns with manned missions and lag times with ground-based control necessitate complete autonomy. This requires robust characterization of the target's geometry. In this article, we present an approach for mapping geometries of satellites on orbit based on 3D Gaussian Splatting that can run on computing resources available on current spaceflight hardware. We demonstrate model training and 3D rendering performance on a hardware-in-the-loop satellite mock-up under several realistic lighting and motion conditions. Our model is shown to be capable of training on-board and rendering higher quality novel views of an unknown satellite nearly 2 orders of magnitude faster than previo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#20998;&#24067;&#20559;&#26012;&#25968;&#25454;&#24773;&#20917;&#19979;&#22914;&#20309;&#36890;&#36807;&#20351;&#29992;&#26679;&#26412;&#26435;&#37325;&#26469;&#25913;&#36827;&#32852;&#37030;&#23398;&#20064;&#24615;&#33021;&#12290;&#20027;&#35201;&#24605;&#36335;&#26159;&#36890;&#36807;&#35843;&#25972;&#23458;&#25143;&#31471;&#20998;&#24067;&#20351;&#20854;&#26356;&#25509;&#36817;&#20840;&#23616;&#20998;&#24067;&#65292;&#20174;&#32780;&#23454;&#29616;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26356;&#24555;&#22320;&#25910;&#25947;&#21644;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.02586</link><description>&lt;p&gt;
&#20351;&#29992;&#26679;&#26412;&#26435;&#37325;&#36827;&#34892;&#20998;&#24067;&#20559;&#26012;&#25968;&#25454;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Learning for distribution skewed data using sample weights. (arXiv:2401.02586v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02586
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#20998;&#24067;&#20559;&#26012;&#25968;&#25454;&#24773;&#20917;&#19979;&#22914;&#20309;&#36890;&#36807;&#20351;&#29992;&#26679;&#26412;&#26435;&#37325;&#26469;&#25913;&#36827;&#32852;&#37030;&#23398;&#20064;&#24615;&#33021;&#12290;&#20027;&#35201;&#24605;&#36335;&#26159;&#36890;&#36807;&#35843;&#25972;&#23458;&#25143;&#31471;&#20998;&#24067;&#20351;&#20854;&#26356;&#25509;&#36817;&#20840;&#23616;&#20998;&#24067;&#65292;&#20174;&#32780;&#23454;&#29616;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26356;&#24555;&#22320;&#25910;&#25947;&#21644;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#20013;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#20043;&#19968;&#26159;&#25968;&#25454;&#36890;&#24120;&#19981;&#26159;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#65288;nonIID&#65289;&#12290;&#23458;&#25143;&#31471;&#34987;&#26399;&#26395;&#36129;&#29486;&#30456;&#21516;&#31867;&#22411;&#30340;&#25968;&#25454;&#24182;&#20174;&#19968;&#20010;&#20840;&#23616;&#20998;&#24067;&#20013;&#25277;&#21462;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#24448;&#24448;&#20197;&#19981;&#21516;&#30340;&#26041;&#24335;&#20174;&#19981;&#21516;&#36164;&#28304;&#25910;&#38598;&#12290;&#22240;&#27492;&#65292;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#25968;&#25454;&#20998;&#24067;&#21487;&#33021;&#19982;&#24213;&#23618;&#20840;&#23616;&#20998;&#24067;&#19981;&#21516;&#12290;&#36825;&#23601;&#20135;&#29983;&#20102;&#26435;&#37325;&#21457;&#25955;&#38382;&#39064;&#65292;&#24182;&#38477;&#20302;&#20102;&#32852;&#37030;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;&#35813;&#24037;&#20316;&#20391;&#37325;&#20110;&#25913;&#21892;&#23458;&#25143;&#31471;&#20043;&#38388;&#20998;&#24067;&#20559;&#26012;&#30340;&#32852;&#37030;&#23398;&#20064;&#24615;&#33021;&#12290;&#20027;&#35201;&#24605;&#24819;&#26159;&#20351;&#29992;&#26679;&#26412;&#26435;&#37325;&#23558;&#23458;&#25143;&#31471;&#20998;&#24067;&#35843;&#25972;&#21040;&#20840;&#23616;&#20998;&#24067;&#26356;&#25509;&#36817;&#65292;&#20174;&#32780;&#20351;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25910;&#25947;&#26356;&#24555;&#19988;&#31934;&#24230;&#26356;&#39640;&#12290;&#25105;&#20204;&#20174;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#30340;&#22522;&#26412;&#27010;&#24565;&#24320;&#22987;&#65292;&#20174;&#29702;&#35770;&#19978;&#25512;&#23548;&#20986;&#20351;&#29992;&#26679;&#26412;&#26435;&#37325;&#35843;&#25972;&#20998;&#24067;&#20559;&#26012;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#20102;&#30830;&#23450;&#26679;&#26412;&#26435;&#37325;&#65292;&#25105;&#20204;&#38544;&#21547;&#22320;&#20132;&#25442;...
&lt;/p&gt;
&lt;p&gt;
One of the most challenging issues in federated learning is that the data is often not independent and identically distributed (nonIID). Clients are expected to contribute the same type of data and drawn from one global distribution. However, data are often collected in different ways from different resources. Thus, the data distributions among clients might be different from the underlying global distribution. This creates a weight divergence issue and reduces federated learning performance. This work focuses on improving federated learning performance for skewed data distribution across clients. The main idea is to adjust the client distribution closer to the global distribution using sample weights. Thus, the machine learning model converges faster with higher accuracy. We start from the fundamental concept of empirical risk minimization and theoretically derive a solution for adjusting the distribution skewness using sample weights. To determine sample weights, we implicitly exchan
&lt;/p&gt;</description></item><item><title>t-DGR&#26159;&#19968;&#31181;&#29992;&#20110;&#20915;&#31574;&#21046;&#23450;&#20013;&#25345;&#32493;&#23398;&#20064;&#30340;&#22522;&#20110;&#36712;&#36857;&#30340;&#28145;&#24230;&#29983;&#25104;&#22238;&#25918;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#20219;&#21153;&#26679;&#26412;&#26469;&#35299;&#20915;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#65292;&#24182;&#22312;&#36830;&#32493;&#19990;&#30028;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.02576</link><description>&lt;p&gt;
t-DGR: &#19968;&#31181;&#22522;&#20110;&#36712;&#36857;&#30340;&#28145;&#24230;&#29983;&#25104;&#22238;&#25918;&#26041;&#27861;&#29992;&#20110;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
t-DGR: A Trajectory-Based Deep Generative Replay Method for Continual Learning in Decision Making. (arXiv:2401.02576v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02576
&lt;/p&gt;
&lt;p&gt;
t-DGR&#26159;&#19968;&#31181;&#29992;&#20110;&#20915;&#31574;&#21046;&#23450;&#20013;&#25345;&#32493;&#23398;&#20064;&#30340;&#22522;&#20110;&#36712;&#36857;&#30340;&#28145;&#24230;&#29983;&#25104;&#22238;&#25918;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#20219;&#21153;&#26679;&#26412;&#26469;&#35299;&#20915;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#65292;&#24182;&#22312;&#36830;&#32493;&#19990;&#30028;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#22238;&#25918;&#24050;&#32463;&#25104;&#20026;&#20915;&#31574;&#21046;&#23450;&#20013;&#25345;&#32493;&#23398;&#20064;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#20174;&#20197;&#21069;&#36935;&#21040;&#30340;&#20219;&#21153;&#29983;&#25104;&#36712;&#36857;&#26469;&#22686;&#21152;&#24403;&#21069;&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#28145;&#24230;&#29983;&#25104;&#22238;&#25918;&#26041;&#27861;&#20381;&#36182;&#20110;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#22312;&#29983;&#25104;&#30340;&#36712;&#36857;&#20013;&#20250;&#20986;&#29616;&#32047;&#31215;&#35823;&#24046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#12289;&#21487;&#25193;&#23637;&#19988;&#38750;&#33258;&#22238;&#24402;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#25345;&#32493;&#23398;&#20064;&#65292;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#26681;&#25454;&#36712;&#36857;&#26102;&#38388;&#27493;&#29983;&#25104;&#20219;&#21153;&#26679;&#26412;&#12290;&#25105;&#20204;&#22312;&#36830;&#32493;&#19990;&#30028;&#22522;&#20934;&#27979;&#35797;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#22312;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24179;&#22343;&#25104;&#21151;&#29575;&#25351;&#26631;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/WilliamYue37/t-DGR&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep generative replay has emerged as a promising approach for continual learning in decision-making tasks. This approach addresses the problem of catastrophic forgetting by leveraging the generation of trajectories from previously encountered tasks to augment the current dataset. However, existing deep generative replay methods for continual learning rely on autoregressive models, which suffer from compounding errors in the generated trajectories. In this paper, we propose a simple, scalable, and non-autoregressive method for continual learning in decision-making tasks using a generative model that generates task samples conditioned on the trajectory timestep. We evaluate our method on Continual World benchmarks and find that our approach achieves state-of-the-art performance on the average success rate metric among continual learning methods. Code is available at https://github.com/WilliamYue37/t-DGR .
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22914;&#20309;&#22312;&#22312;&#32447;&#31038;&#20132;&#32593;&#32476;&#20013;&#24320;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24212;&#29992;&#12290;&#25105;&#20204;&#23558;LLM&#24212;&#29992;&#20998;&#20026;&#30693;&#35782;&#20219;&#21153;&#12289;&#23089;&#20048;&#20219;&#21153;&#21644;&#22522;&#30784;&#20219;&#21153;&#65292;&#24182;&#20998;&#20139;&#20102;&#25361;&#25112;&#12289;&#35299;&#20915;&#26041;&#26696;&#21644;&#32463;&#39564;&#25945;&#35757;&#12290;</title><link>http://arxiv.org/abs/2401.02575</link><description>&lt;p&gt;
&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#24212;&#29992;&#12289;&#25361;&#25112;&#21644;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Large Language Models for Social Networks: Applications, Challenges, and Solutions. (arXiv:2401.02575v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02575
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22914;&#20309;&#22312;&#22312;&#32447;&#31038;&#20132;&#32593;&#32476;&#20013;&#24320;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24212;&#29992;&#12290;&#25105;&#20204;&#23558;LLM&#24212;&#29992;&#20998;&#20026;&#30693;&#35782;&#20219;&#21153;&#12289;&#23089;&#20048;&#20219;&#21153;&#21644;&#22522;&#30784;&#20219;&#21153;&#65292;&#24182;&#20998;&#20139;&#20102;&#25361;&#25112;&#12289;&#35299;&#20915;&#26041;&#26696;&#21644;&#32463;&#39564;&#25945;&#35757;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#27491;&#22312;&#25913;&#21464;&#20154;&#20204;&#29983;&#25104;&#12289;&#25506;&#32034;&#21644;&#21442;&#19982;&#20869;&#23481;&#30340;&#26041;&#24335;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#22914;&#20309;&#22312;&#22312;&#32447;&#31038;&#20132;&#32593;&#32476;&#20013;&#24320;&#21457;LLM&#24212;&#29992;&#12290;&#23613;&#31649;LLMs&#22312;&#20854;&#20182;&#39046;&#22495;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#22312;&#31038;&#20132;&#32593;&#32476;&#20013;&#24320;&#21457;&#22522;&#20110;LLM&#30340;&#20135;&#21697;&#38754;&#20020;&#30528;&#35768;&#22810;&#25361;&#25112;&#65292;&#24182;&#19988;&#22312;&#30740;&#31350;&#30028;&#20013;&#30456;&#23545;&#36739;&#23569;&#25253;&#36947;&#12290;&#25105;&#20204;&#23558;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;LLM&#24212;&#29992;&#20998;&#20026;&#19977;&#31867;&#12290;&#31532;&#19968;&#31867;&#26159;&#30693;&#35782;&#20219;&#21153;&#65292;&#29992;&#25143;&#24819;&#35201;&#26597;&#25214;&#26032;&#30693;&#35782;&#21644;&#20449;&#24687;&#65292;&#20363;&#22914;&#25628;&#32034;&#21644;&#38382;&#31572;&#12290;&#31532;&#20108;&#31867;&#26159;&#23089;&#20048;&#20219;&#21153;&#65292;&#29992;&#25143;&#24819;&#35201;&#28040;&#36153;&#26377;&#36259;&#30340;&#20869;&#23481;&#65292;&#20363;&#22914;&#33719;&#21462;&#23089;&#20048;&#24615;&#36890;&#30693;&#20869;&#23481;&#12290;&#31532;&#19977;&#31867;&#26159;&#22522;&#30784;&#20219;&#21153;&#65292;&#38656;&#35201;&#36827;&#34892;&#31038;&#20132;&#32593;&#32476;&#30340;&#20869;&#23481;&#27880;&#37322;&#21644;LLM&#30417;&#25511;&#12290;&#38024;&#23545;&#27599;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#20998;&#20139;&#20102;&#25105;&#20204;&#21457;&#29616;&#30340;&#25361;&#25112;&#12289;&#24320;&#21457;&#30340;&#35299;&#20915;&#26041;&#26696;&#21644;&#21560;&#21462;&#30340;&#32463;&#39564;&#25945;&#35757;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20840;&#38754;&#30340;&#30740;&#31350;&#20851;&#20110;&#22312;&#31038;&#20132;&#32593;&#32476;&#20013;&#24212;&#29992;LLMs&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are transforming the way people generate, explore, and engage with content. We study how we can develop LLM applications for online social networks. Despite LLMs' successes in other domains, it is challenging to develop LLM-based products for social networks for numerous reasons, and it has been relatively under-reported in the research community. We categorize LLM applications for social networks into three categories. First is knowledge tasks where users want to find new knowledge and information, such as search and question-answering. Second is entertainment tasks where users want to consume interesting content, such as getting entertaining notification content. Third is foundational tasks that need to be done to moderate and operate the social networks, such as content annotation and LLM monitoring. For each task, we share the challenges we found, solutions we developed, and lessons we learned. To the best of our knowledge, this is the first comprehensi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#23450;&#37327;&#25216;&#26415;&#39044;&#27979;&#20013;&#30340;&#36235;&#21183;&#22806;&#25512;&#26041;&#27861;&#36827;&#34892;&#20102;&#31995;&#32479;&#22238;&#39038;&#65292;&#21457;&#29616;&#22686;&#38271;&#26354;&#32447;&#21644;&#26102;&#38388;&#24207;&#21015;&#26041;&#27861;&#26159;&#36807;&#21435;&#21313;&#24180;&#20013;&#26368;&#21463;&#27426;&#36814;&#30340;&#25216;&#26415;&#26041;&#27861;&#65292;&#32780;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#28151;&#21512;&#27169;&#22411;&#21017;&#26159;&#36817;&#24180;&#30340;&#26032;&#20852;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.02549</link><description>&lt;p&gt;
&#23450;&#37327;&#25216;&#26415;&#39044;&#27979;&#65306;&#36235;&#21183;&#22806;&#25512;&#26041;&#27861;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Quantitative Technology Forecasting: a Review of Trend Extrapolation Methods. (arXiv:2401.02549v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02549
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#23450;&#37327;&#25216;&#26415;&#39044;&#27979;&#20013;&#30340;&#36235;&#21183;&#22806;&#25512;&#26041;&#27861;&#36827;&#34892;&#20102;&#31995;&#32479;&#22238;&#39038;&#65292;&#21457;&#29616;&#22686;&#38271;&#26354;&#32447;&#21644;&#26102;&#38388;&#24207;&#21015;&#26041;&#27861;&#26159;&#36807;&#21435;&#21313;&#24180;&#20013;&#26368;&#21463;&#27426;&#36814;&#30340;&#25216;&#26415;&#26041;&#27861;&#65292;&#32780;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#28151;&#21512;&#27169;&#22411;&#21017;&#26159;&#36817;&#24180;&#30340;&#26032;&#20852;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23450;&#37327;&#25216;&#26415;&#39044;&#27979;&#21033;&#29992;&#23450;&#37327;&#26041;&#27861;&#26469;&#29702;&#35299;&#21644;&#39044;&#27979;&#25216;&#26415;&#21464;&#21270;&#12290;&#36825;&#26159;&#19968;&#20010;&#24191;&#27867;&#30340;&#39046;&#22495;&#65292;&#28085;&#30422;&#20102;&#35768;&#22810;&#19981;&#21516;&#30340;&#25216;&#26415;&#65292;&#24182;&#19988;&#24050;&#24212;&#29992;&#20110;&#24191;&#27867;&#30340;&#25216;&#26415;&#33539;&#22260;&#12290;&#36825;&#20010;&#39046;&#22495;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#26041;&#27861;&#20043;&#19968;&#26159;&#36235;&#21183;&#22806;&#25512;&#12290;&#26681;&#25454;&#25105;&#20204;&#25152;&#24471;&#21040;&#30340;&#20986;&#29256;&#29289;&#65292;&#24456;&#23569;&#25110;&#27809;&#26377;&#31995;&#32479;&#22320;&#22238;&#39038;&#23450;&#37327;&#36235;&#21183;&#22806;&#25512;&#25216;&#26415;&#30340;&#32463;&#39564;&#35777;&#25454;&#12290;&#26412;&#30740;&#31350;&#35797;&#22270;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#36890;&#36807;&#23545;&#28041;&#21450;&#23450;&#37327;&#36235;&#21183;&#22806;&#25512;&#25216;&#26415;&#24212;&#29992;&#30340;&#25216;&#26415;&#39044;&#27979;&#25991;&#29486;&#36827;&#34892;&#31995;&#32479;&#22238;&#39038;&#12290;&#25105;&#20204;&#37492;&#21035;&#20986;&#20102;&#19982;&#26412;&#30740;&#31350;&#30446;&#26631;&#30456;&#20851;&#30340;25&#39033;&#30740;&#31350;&#65292;&#24182;&#23558;&#36825;&#20123;&#30740;&#31350;&#20013;&#20351;&#29992;&#30340;&#25216;&#26415;&#20998;&#31867;&#20026;&#19981;&#21516;&#30340;&#31867;&#21035;&#65292;&#20854;&#20013;&#22686;&#38271;&#26354;&#32447;&#21644;&#26102;&#38388;&#24207;&#21015;&#26041;&#27861;&#22312;&#36807;&#21435;&#21313;&#24180;&#20013;&#20173;&#28982;&#21463;&#21040;&#27426;&#36814;&#65292;&#32780;&#26032;&#30340;&#26041;&#27861;&#65292;&#22914;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#28151;&#21512;&#27169;&#22411;&#65292;&#21017;&#22312;&#36817;&#24180;&#20852;&#36215;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantitative technology forecasting uses quantitative methods to understand and project technological changes. It is a broad field encompassing many different techniques and has been applied to a vast range of technologies. A widely used approach in this field is trend extrapolation. Based on the publications available to us, there has been little or no attempt made to systematically review the empirical evidence on quantitative trend extrapolation techniques. This study attempts to close this gap by conducting a systematic review of technology forecasting literature addressing the application of quantitative trend extrapolation techniques. We identified 25 studies relevant to the objective of this research and classified the techniques used in the studies into different categories, among which growth curves and time series methods were shown to remain popular over the past decade, while newer methods, such as machine learning-based hybrid models, have emerged in recent years. As more 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#31038;&#21306;&#26816;&#27979;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#31185;&#23398;&#25991;&#29486;&#32593;&#32476;&#20013;&#30340;&#38142;&#25509;&#39044;&#27979;&#33021;&#21147;&#12290;&#36890;&#36807;&#23558;&#31038;&#21306;&#26816;&#27979;&#30340;&#32467;&#26524;&#19982;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#32467;&#21512;&#65292;&#25105;&#20204;&#31361;&#30772;&#20102;&#38142;&#25509;&#39044;&#27979;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#20998;&#36776;&#29575;&#38480;&#21046;&#65292;&#25552;&#39640;&#20102;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.02542</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#31038;&#21306;&#26816;&#27979;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#31185;&#23398;&#25991;&#29486;&#38142;&#25509;&#39044;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Community Detection and Graph Neural Network Based Link Prediction Approach for Scientific Literature. (arXiv:2401.02542v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02542
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#31038;&#21306;&#26816;&#27979;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#31185;&#23398;&#25991;&#29486;&#32593;&#32476;&#20013;&#30340;&#38142;&#25509;&#39044;&#27979;&#33021;&#21147;&#12290;&#36890;&#36807;&#23558;&#31038;&#21306;&#26816;&#27979;&#30340;&#32467;&#26524;&#19982;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#32467;&#21512;&#65292;&#25105;&#20204;&#31361;&#30772;&#20102;&#38142;&#25509;&#39044;&#27979;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#20998;&#36776;&#29575;&#38480;&#21046;&#65292;&#25552;&#39640;&#20102;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#31038;&#21306;&#26816;&#27979;&#31639;&#27861;&#19982;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#20197;&#22686;&#24378;&#31185;&#23398;&#25991;&#29486;&#32593;&#32476;&#20013;&#30340;&#38142;&#25509;&#39044;&#27979;&#33021;&#21147;&#12290;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#21033;&#29992;Louvain&#31038;&#21306;&#26816;&#27979;&#31639;&#27861;&#25581;&#31034;&#36825;&#20123;&#32593;&#32476;&#20013;&#30340;&#28508;&#22312;&#31038;&#21306;&#32467;&#26500;&#65292;&#24182;&#23558;&#20854;&#32435;&#20837;GNN&#26550;&#26500;&#20013;&#20197;&#39044;&#27979;&#28508;&#22312;&#38142;&#25509;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#31034;&#20102;&#22312;&#22797;&#26434;&#32593;&#32476;&#20013;&#29702;&#35299;&#31038;&#21306;&#21160;&#24577;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#21033;&#29992;&#31038;&#21306;&#26816;&#27979;&#21644;GNN&#30340;&#20248;&#21183;&#26469;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#23545;&#20195;&#34920;&#31185;&#23398;&#21512;&#20316;&#21644;&#24341;&#25991;&#20851;&#31995;&#30340;&#20108;&#20998;&#22270;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#31361;&#26174;&#20102;&#31038;&#21306;&#26816;&#27979;&#21644;GNN&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#65292;&#36824;&#35299;&#20915;&#20102;&#38142;&#25509;&#39044;&#27979;&#20013;&#30340;&#19968;&#20123;&#26222;&#36941;&#25361;&#25112;&#65292;&#22914;&#21487;&#25193;&#23637;&#24615;&#21644;&#20998;&#36776;&#29575;&#38480;&#21046;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#32435;&#20837;&#31038;&#21306;&#32423;&#21035;&#20449;&#24687;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#38142;&#25509;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study introduces an innovative approach that integrates community detection algorithms with Graph Neural Network (GNN) models to enhance link prediction in scientific literature networks. We specifically focus on the utilization of the Louvain community detection algorithm to uncover latent community structures within these networks, which are then incorporated into GNN architectures to predict potential links. Our methodology demonstrates the importance of understanding community dynamics in complex networks and leverages the strengths of both community detection and GNNs to improve predictive accuracy. Through extensive experiments on bipartite graphs representing scientific collaborations and citations, our approach not only highlights the synergy between community detection and GNNs but also addresses some of the prevalent challenges in link prediction, such as scalability and resolution limits. The results suggest that incorporating community-level information can significant
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DISO&#30340;&#39046;&#22495;&#26412;&#20307;&#65292;&#29992;&#20110;&#24314;&#27169;&#26230;&#20307;&#26448;&#26009;&#20013;&#30340;&#20301;&#38169;&#32570;&#38519;&#12290;&#35813;&#26412;&#20307;&#36890;&#36807;&#23450;&#20041;&#19982;&#32447;&#29366;&#32570;&#38519;&#30456;&#20851;&#30340;&#27010;&#24565;&#21644;&#20851;&#31995;&#65292;&#26377;&#21161;&#20110;&#29702;&#35299;&#20301;&#38169;&#34892;&#20026;&#24182;&#22312;&#20301;&#38169;&#21160;&#21147;&#23398;&#39046;&#22495;&#20013;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2401.02540</link><description>&lt;p&gt;
DISO&#65306;&#29992;&#20110;&#24314;&#27169;&#26230;&#20307;&#26448;&#26009;&#20013;&#20301;&#38169;&#30340;&#39046;&#22495;&#26412;&#20307;
&lt;/p&gt;
&lt;p&gt;
DISO: A Domain Ontology for Modeling Dislocations in Crystalline Materials. (arXiv:2401.02540v1 [cond-mat.mtrl-sci])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02540
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DISO&#30340;&#39046;&#22495;&#26412;&#20307;&#65292;&#29992;&#20110;&#24314;&#27169;&#26230;&#20307;&#26448;&#26009;&#20013;&#30340;&#20301;&#38169;&#32570;&#38519;&#12290;&#35813;&#26412;&#20307;&#36890;&#36807;&#23450;&#20041;&#19982;&#32447;&#29366;&#32570;&#38519;&#30456;&#20851;&#30340;&#27010;&#24565;&#21644;&#20851;&#31995;&#65292;&#26377;&#21161;&#20110;&#29702;&#35299;&#20301;&#38169;&#34892;&#20026;&#24182;&#22312;&#20301;&#38169;&#21160;&#21147;&#23398;&#39046;&#22495;&#20013;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26230;&#20307;&#26448;&#26009;&#65288;&#22914;&#37329;&#23646;&#21644;&#21322;&#23548;&#20307;&#65289;&#20960;&#20046;&#22987;&#32456;&#21253;&#21547;&#19968;&#31181;&#29305;&#27530;&#30340;&#32570;&#38519;&#31867;&#22411;&#65292;&#31216;&#20026;&#20301;&#38169;&#12290;&#36825;&#31181;&#32570;&#38519;&#20915;&#23450;&#20102;&#35768;&#22810;&#37325;&#35201;&#30340;&#26448;&#26009;&#24615;&#36136;&#65292;&#20363;&#22914;&#24378;&#24230;&#12289;&#26029;&#35010;&#38887;&#24615;&#25110;&#24310;&#23637;&#24615;&#12290;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#20154;&#20204;&#36890;&#36807;&#23454;&#39564;&#34920;&#24449;&#25216;&#26415;&#21644;&#27169;&#25311;&#30740;&#31350;&#19981;&#21516;&#38271;&#24230;&#23610;&#24230;&#19979;&#30340;&#20301;&#38169;&#34892;&#20026;&#25237;&#20837;&#20102;&#22823;&#37327;&#30340;&#24037;&#20316;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#20301;&#38169;&#26412;&#20307;&#65288;DISO&#65289;&#65292;&#23427;&#23450;&#20041;&#20102;&#26230;&#20307;&#26448;&#26009;&#20013;&#19982;&#32447;&#29366;&#32570;&#38519;&#30456;&#20851;&#30340;&#27010;&#24565;&#21644;&#20851;&#31995;&#12290;&#25105;&#20204;&#20351;&#29992;&#33258;&#39030;&#21521;&#19979;&#30340;&#26041;&#27861;&#24320;&#21457;&#20102;DISO&#65292;&#21363;&#39318;&#20808;&#23450;&#20041;&#20301;&#38169;&#39046;&#22495;&#20013;&#26368;&#36890;&#29992;&#30340;&#27010;&#24565;&#65292;&#28982;&#21518;&#23545;&#20854;&#36827;&#34892;&#29305;&#21270;&#12290;DISO&#36890;&#36807;&#31526;&#21512;W3C&#26368;&#20339;&#23454;&#36341;&#30340;&#25345;&#20037;URL&#36827;&#34892;&#21457;&#24067;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;DISO&#30340;&#20004;&#20010;&#28508;&#22312;&#29992;&#20363;&#65292;&#20197;&#35828;&#26126;&#20854;&#22312;&#20301;&#38169;&#21160;&#21147;&#23398;&#39046;&#22495;&#30340;&#26377;&#29992;&#24615;&#12290;&#23545;&#26412;&#20307;&#30340;&#35780;&#20272;&#26159;&#22312;...
&lt;/p&gt;
&lt;p&gt;
Crystalline materials, such as metals and semiconductors, nearly always contain a special defect type called dislocation. This defect decisively determines many important material properties, e.g., strength, fracture toughness, or ductility. Over the past years, significant effort has been put into understanding dislocation behavior across different length scales via experimental characterization techniques and simulations. This paper introduces the dislocation ontology (DISO), which defines the concepts and relationships related to linear defects in crystalline materials. We developed DISO using a top-down approach in which we start defining the most general concepts in the dislocation domain and subsequent specialization of them. DISO is published through a persistent URL following W3C best practices for publishing Linked Data. Two potential use cases for DISO are presented to illustrate its usefulness in the dislocation dynamics domain. The evaluation of the ontology is performed in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#36807;&#21435;&#21313;&#24180;&#20013;417&#20010;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#23545;&#27169;&#22411;&#31867;&#22411;&#12289;&#21151;&#33021;&#21644;&#25913;&#36827;&#30340;&#20840;&#38754;&#27010;&#36848;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#27169;&#22411;&#24615;&#33021;&#21644;&#22797;&#26434;&#24615;&#22686;&#21152;&#65292;&#20197;&#31070;&#32463;&#32593;&#32476;&#20026;&#20027;&#35201;&#26041;&#27861;&#65292;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#21344;&#25454;&#20027;&#23548;&#22320;&#20301;&#12290;</title><link>http://arxiv.org/abs/2401.02524</link><description>&lt;p&gt;
&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#30340;&#32508;&#21512;&#25506;&#32034;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Comprehensive Exploration of Synthetic Data Generation: A Survey. (arXiv:2401.02524v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02524
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#36807;&#21435;&#21313;&#24180;&#20013;417&#20010;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#23545;&#27169;&#22411;&#31867;&#22411;&#12289;&#21151;&#33021;&#21644;&#25913;&#36827;&#30340;&#20840;&#38754;&#27010;&#36848;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#27169;&#22411;&#24615;&#33021;&#21644;&#22797;&#26434;&#24615;&#22686;&#21152;&#65292;&#20197;&#31070;&#32463;&#32593;&#32476;&#20026;&#20027;&#35201;&#26041;&#27861;&#65292;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#21344;&#25454;&#20027;&#23548;&#22320;&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#26426;&#22120;&#23398;&#20064;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#24212;&#29992;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#25454;&#37319;&#38598;&#25104;&#26412;&#39640;&#26114;&#21644;&#38544;&#31169;&#27861;&#35268;&#30340;&#38480;&#21046;&#65292;&#35757;&#32451;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#38459;&#30861;&#20102;&#36827;&#23637;&#12290;&#21512;&#25104;&#25968;&#25454;&#25104;&#20026;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#21457;&#24067;&#30340;&#27169;&#22411;&#36807;&#22810;&#21644;&#26377;&#38480;&#30340;&#32508;&#36848;&#25991;&#29486;&#32473;&#20915;&#31574;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#26412;&#25991;&#35843;&#26597;&#20102;&#36807;&#21435;&#21313;&#24180;&#20013;417&#20010;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#23545;&#27169;&#22411;&#31867;&#22411;&#12289;&#21151;&#33021;&#21644;&#25913;&#36827;&#30340;&#20840;&#38754;&#27010;&#36848;&#12290;&#30830;&#23450;&#20102;&#20849;&#21516;&#30340;&#29305;&#24449;&#65292;&#36827;&#34892;&#20102;&#20998;&#31867;&#21644;&#36235;&#21183;&#20998;&#26512;&#12290;&#32467;&#26524;&#26174;&#31034;&#27169;&#22411;&#24615;&#33021;&#21644;&#22797;&#26434;&#24615;&#22686;&#21152;&#65292;&#20197;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#20026;&#20027;&#35201;&#36235;&#21183;&#65292;&#38500;&#20102;&#38544;&#31169;&#20445;&#25252;&#25968;&#25454;&#29983;&#25104;&#12290;&#35745;&#31639;&#26426;&#35270;&#35273;&#21344;&#25454;&#20027;&#23548;&#22320;&#20301;&#65292;&#29983;&#25104;&#27169;&#22411;&#20027;&#35201;&#26159;GAN&#65292;&#32780;&#25193;&#25955;&#27169;&#22411;&#12289;&#36716;&#25442;&#22120;&#21644;RNN&#20063;&#22312;&#31454;&#20105;&#20013;&#12290;&#36890;&#36807;&#24615;&#33021;&#35780;&#20272;&#30340;&#32467;&#26524;&#26174;&#31034;&#20102;&#20849;&#21516;&#25351;&#26631;&#21644;&#25968;&#25454;&#38598;&#31232;&#32570;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed a surge in the popularity of Machine Learning (ML), applied across diverse domains. However, progress is impeded by the scarcity of training data due to expensive acquisition and privacy legislation. Synthetic data emerges as a solution, but the abundance of released models and limited overview literature pose challenges for decision-making. This work surveys 417 Synthetic Data Generation (SDG) models over the last decade, providing a comprehensive overview of model types, functionality, and improvements. Common attributes are identified, leading to a classification and trend analysis. The findings reveal increased model performance and complexity, with neural network-based approaches prevailing, except for privacy-preserving data generation. Computer vision dominates, with GANs as primary generative models, while diffusion models, transformers, and RNNs compete. Implications from our performance evaluation highlight the scarcity of common metrics and datase
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;&#22270;&#20687;&#30340;&#26234;&#33021;&#25968;&#23383;&#23402;&#29983;&#20307; (SDTs) &#30340;&#21457;&#23637;&#26041;&#27861;&#21644;&#25361;&#25112;&#65292;&#37325;&#28857;&#35752;&#35770;&#20102;&#36890;&#36807;&#25345;&#32493;&#21516;&#21270;&#22270;&#20687;&#25968;&#25454;&#26469;&#35266;&#23519;&#21644;&#23398;&#20064;&#31995;&#32479;&#34892;&#20026;&#30340;&#26041;&#27861;&#65292;&#20197;&#21450;&#35774;&#35745;&#21644;&#23454;&#29616;SDTs&#30340;&#28145;&#24230;&#23398;&#20064; (DL) &#27169;&#22411;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#25552;&#20379;&#20102;&#23545;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#21644;&#26426;&#36935;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2401.02523</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#20687;&#30340;&#28145;&#24230;&#23398;&#20064;&#29992;&#20110;&#26234;&#33021;&#25968;&#23383;&#23402;&#29983;&#20307;: &#19968;&#20221;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Image-based Deep Learning for Smart Digital Twins: a Review. (arXiv:2401.02523v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02523
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;&#22270;&#20687;&#30340;&#26234;&#33021;&#25968;&#23383;&#23402;&#29983;&#20307; (SDTs) &#30340;&#21457;&#23637;&#26041;&#27861;&#21644;&#25361;&#25112;&#65292;&#37325;&#28857;&#35752;&#35770;&#20102;&#36890;&#36807;&#25345;&#32493;&#21516;&#21270;&#22270;&#20687;&#25968;&#25454;&#26469;&#35266;&#23519;&#21644;&#23398;&#20064;&#31995;&#32479;&#34892;&#20026;&#30340;&#26041;&#27861;&#65292;&#20197;&#21450;&#35774;&#35745;&#21644;&#23454;&#29616;SDTs&#30340;&#28145;&#24230;&#23398;&#20064; (DL) &#27169;&#22411;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#25552;&#20379;&#20102;&#23545;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#21644;&#26426;&#36935;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#25968;&#23383;&#23402;&#29983;&#20307;(SDTs)&#36890;&#36807;&#25345;&#32493;&#25968;&#25454;&#21516;&#21270;&#26469;&#34394;&#25311;&#22797;&#21046;&#21644;&#39044;&#27979;&#22797;&#26434;&#29289;&#29702;&#31995;&#32479;&#30340;&#34892;&#20026;&#65292;&#20174;&#32780;&#36890;&#36807;&#25511;&#21046;&#31995;&#32479;&#30340;&#34892;&#20026;&#26469;&#20248;&#21270;&#36825;&#20123;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;(DL)&#27169;&#22411;&#26174;&#33879;&#22686;&#24378;&#20102;SDTs&#30340;&#33021;&#21147;&#65292;&#23588;&#20854;&#26159;&#22312;&#39044;&#27979;&#24615;&#32500;&#20462;&#12289;&#24322;&#24120;&#26816;&#27979;&#21644;&#20248;&#21270;&#31561;&#20219;&#21153;&#19978;&#12290;&#22312;&#35768;&#22810;&#39046;&#22495;&#65292;&#21253;&#25324;&#21307;&#23398;&#12289;&#24037;&#31243;&#21644;&#25945;&#32946;&#65292;&#22312;&#35266;&#23519;&#21644;&#23398;&#20064;&#31995;&#32479;&#34892;&#20026;&#21644;&#25511;&#21046;&#20854;&#34892;&#20026;&#26041;&#38754;&#65292;SDTs&#20351;&#29992;&#22270;&#20687;&#25968;&#25454;(&#22522;&#20110;&#22270;&#20687;&#30340;SDTs)&#12290;&#26412;&#25991;&#37325;&#28857;&#20171;&#32461;&#20102;&#24320;&#21457;&#22522;&#20110;&#22270;&#20687;&#30340;SDTs&#30340;&#21508;&#31181;&#26041;&#27861;&#21644;&#30456;&#20851;&#25361;&#25112;&#65292;&#21253;&#25324;&#25345;&#32493;&#21516;&#21270;&#26469;&#33258;&#29289;&#29702;&#31995;&#32479;&#30340;&#22270;&#20687;&#25968;&#25454;&#12290;&#26412;&#25991;&#36824;&#35752;&#35770;&#20102;&#35774;&#35745;&#21644;&#23454;&#29616;SDTs&#30340;DL&#27169;&#22411;&#25152;&#28041;&#21450;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#25968;&#25454;&#33719;&#21462;&#12289;&#22788;&#29702;&#21644;&#35299;&#37322;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20379;&#20102;&#23545;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#21644;&#26426;&#36935;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Smart Digital twins (SDTs) are being increasingly used to virtually replicate and predict the behaviors of complex physical systems through continual data assimilation enabling the optimization of the performance of these systems by controlling the actions of systems. Recently, deep learning (DL) models have significantly enhanced the capabilities of SDTs, particularly for tasks such as predictive maintenance, anomaly detection, and optimization. In many domains, including medicine, engineering, and education, SDTs use image data (image-based SDTs) to observe and learn system behaviors and control their behaviors. This paper focuses on various approaches and associated challenges in developing image-based SDTs by continually assimilating image data from physical systems. The paper also discusses the challenges involved in designing and implementing DL models for SDTs, including data acquisition, processing, and interpretation. In addition, insights into the future directions and opport
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#21452;&#26354;&#21644;&#25243;&#29289;&#22411;PDE&#30340;&#31227;&#21160;&#26102;&#22495;&#20272;&#35745;&#22120;&#65292;&#36890;&#36807;PDE&#21453;&#27493;&#27861;&#23558;&#38590;&#20197;&#35299;&#20915;&#30340;&#35266;&#27979;&#22120;PDE&#36716;&#21270;&#20026;&#21487;&#20197;&#26126;&#30830;&#35299;&#20915;&#30340;&#30446;&#26631;&#35266;&#27979;&#22120;PDE&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22312;&#23454;&#26102;&#29615;&#22659;&#19979;&#28040;&#38500;&#25968;&#20540;&#35299;&#35266;&#27979;&#22120;PDE&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2401.02516</link><description>&lt;p&gt;
&#22312;&#19968;&#32500;&#20013;&#20026;&#21452;&#26354;&#21644;&#25243;&#29289;&#22411;PDE&#24341;&#20837;&#31227;&#21160;&#26102;&#22495;&#20272;&#35745;&#22120;
&lt;/p&gt;
&lt;p&gt;
Moving-Horizon Estimators for Hyperbolic and Parabolic PDEs in 1-D. (arXiv:2401.02516v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#21452;&#26354;&#21644;&#25243;&#29289;&#22411;PDE&#30340;&#31227;&#21160;&#26102;&#22495;&#20272;&#35745;&#22120;&#65292;&#36890;&#36807;PDE&#21453;&#27493;&#27861;&#23558;&#38590;&#20197;&#35299;&#20915;&#30340;&#35266;&#27979;&#22120;PDE&#36716;&#21270;&#20026;&#21487;&#20197;&#26126;&#30830;&#35299;&#20915;&#30340;&#30446;&#26631;&#35266;&#27979;&#22120;PDE&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22312;&#23454;&#26102;&#29615;&#22659;&#19979;&#28040;&#38500;&#25968;&#20540;&#35299;&#35266;&#27979;&#22120;PDE&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;PDE&#30340;&#35266;&#27979;&#22120;&#26412;&#36523;&#20063;&#26159;PDE&#12290;&#22240;&#27492;&#65292;&#20351;&#29992;&#36825;&#26679;&#30340;&#35266;&#27979;&#22120;&#20135;&#29983;&#23454;&#26102;&#20272;&#35745;&#26159;&#35745;&#31639;&#36127;&#25285;&#24456;&#37325;&#30340;&#12290;&#23545;&#20110;&#26377;&#38480;&#32500;&#21644;ODE&#31995;&#32479;&#65292;&#31227;&#21160;&#26102;&#22495;&#20272;&#35745;&#22120;&#65288;MHE&#65289;&#26159;&#19968;&#31181;&#25805;&#20316;&#31526;&#65292;&#20854;&#36755;&#20986;&#26159;&#29366;&#24577;&#20272;&#35745;&#65292;&#32780;&#36755;&#20837;&#26159;&#26102;&#22495;&#36215;&#22987;&#22788;&#30340;&#21021;&#22987;&#29366;&#24577;&#20272;&#35745;&#20197;&#21450;&#31227;&#21160;&#26102;&#38388;&#22495;&#20869;&#30340;&#27979;&#37327;&#36755;&#20986;&#21644;&#36755;&#20837;&#20449;&#21495;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#29992;&#20110;&#35299;&#20915;PDE&#30340;MHE&#65292;&#20197;&#28040;&#38500;&#23454;&#26102;&#25968;&#20540;&#35299;&#35266;&#27979;&#22120;PDE&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#20351;&#29992;PDE&#21453;&#27493;&#27861;&#23454;&#29616;&#20102;&#36825;&#19968;&#28857;&#65292;&#23545;&#20110;&#26576;&#20123;&#29305;&#23450;&#31867;&#21035;&#30340;&#21452;&#26354;&#21644;&#25243;&#29289;&#22411;PDE&#65292;&#23427;&#33021;&#22815;&#26126;&#30830;&#22320;&#20135;&#29983;&#31227;&#21160;&#26102;&#22495;&#29366;&#24577;&#20272;&#35745;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#20026;&#20102;&#26126;&#30830;&#22320;&#20135;&#29983;&#29366;&#24577;&#20272;&#35745;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#38590;&#20197;&#35299;&#20915;&#30340;&#35266;&#27979;&#22120;PDE&#30340;&#21453;&#27493;&#21464;&#25442;&#65292;&#23558;&#20854;&#36716;&#21270;&#20026;&#19968;&#20010;&#21487;&#20197;&#26126;&#30830;&#35299;&#20915;&#30340;&#30446;&#26631;&#35266;&#27979;&#22120;PDE&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;MHE&#24182;&#19981;&#26159;&#26032;&#30340;&#35266;&#27979;&#22120;&#35774;&#35745;&#65292;&#32780;&#21482;&#26159;&#26126;&#30830;&#30340;MHE&#23454;&#29616;&#65292;&#23427;&#33021;&#22815;&#22312;&#31227;&#21160;&#26102;&#22495;&#20869;&#20135;&#29983;&#29366;&#24577;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Observers for PDEs are themselves PDEs. Therefore, producing real time estimates with such observers is computationally burdensome. For both finite-dimensional and ODE systems, moving-horizon estimators (MHE) are operators whose output is the state estimate, while their inputs are the initial state estimate at the beginning of the horizon as well as the measured output and input signals over the moving time horizon. In this paper we introduce MHEs for PDEs which remove the need for a numerical solution of an observer PDE in real time. We accomplish this using the PDE backstepping method which, for certain classes of both hyperbolic and parabolic PDEs, produces moving-horizon state estimates explicitly. Precisely, to explicitly produce the state estimates, we employ a backstepping transformation of a hard-to-solve observer PDE into a target observer PDE, which is explicitly solvable. The MHEs we propose are not new observer designs but simply the explicit MHE realizations, over a moving
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#31070;&#32463;&#31639;&#23376;&#36827;&#34892;&#22686;&#30410;&#35843;&#24230;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#20855;&#26377;&#38750;&#32447;&#24615;&#24490;&#29615;&#30340;&#36755;&#36816;PDE&#65292;&#24182;&#22312;&#23616;&#37096;&#23454;&#29616;&#20102;&#31283;&#23450;&#12290;</title><link>http://arxiv.org/abs/2401.02511</link><description>&lt;p&gt;
&#20351;&#29992;&#31070;&#32463;&#31639;&#23376;&#30340;&#22686;&#30410;&#35843;&#24230;&#26041;&#27861;&#22788;&#29702;&#20855;&#26377;&#38750;&#32447;&#24615;&#24490;&#29615;&#30340;&#36755;&#36816;PDE
&lt;/p&gt;
&lt;p&gt;
Gain Scheduling with a Neural Operator for a Transport PDE with Nonlinear Recirculation. (arXiv:2401.02511v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02511
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#31070;&#32463;&#31639;&#23376;&#36827;&#34892;&#22686;&#30410;&#35843;&#24230;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#20855;&#26377;&#38750;&#32447;&#24615;&#24490;&#29615;&#30340;&#36755;&#36816;PDE&#65292;&#24182;&#22312;&#23616;&#37096;&#23454;&#29616;&#20102;&#31283;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#31283;&#23450;PDE&#27169;&#22411;&#65292;&#25511;&#21046;&#24459;&#38656;&#35201;&#36890;&#36807;&#38750;&#32447;&#24615;&#31639;&#23376;&#23558;&#31354;&#38388;&#30456;&#20851;&#30340;&#22686;&#30410;&#26144;&#23556;&#21040;PDE&#20989;&#25968;&#31995;&#25968;&#19978;&#12290;&#24403;PDE&#26159;&#38750;&#32447;&#24615;&#30340;&#19988;"&#20266;&#31995;&#25968;"&#20989;&#25968;&#20381;&#36182;&#20110;&#29366;&#24577;&#26102;&#65292;&#22686;&#30410;&#35843;&#24230;&#38750;&#32447;&#24615;&#35774;&#35745;&#26159;&#22788;&#29702;&#38750;&#32447;&#24615;&#21453;&#39304;&#35774;&#35745;&#30340;&#26368;&#31616;&#21333;&#26041;&#27861;&#12290;PDE&#22238;&#28335;&#30340;&#22686;&#30410;&#35843;&#24230;&#29256;&#26412;&#21033;&#29992;&#22312;&#27599;&#20010;&#29366;&#24577;&#20540;&#22788;&#27714;&#35299;PDE&#24471;&#21040;&#30340;&#22686;&#30410;&#12290;&#20294;&#26159;&#22312;&#23454;&#26102;&#24773;&#20917;&#19979;&#36827;&#34892;&#36825;&#31181;PDE&#35745;&#31639;&#21487;&#33021;&#26159;&#22256;&#38590;&#30340;&#12290;&#26368;&#36817;&#24341;&#20837;&#30340;&#31070;&#32463;&#31639;&#23376;&#65288;NO&#65289;&#21487;&#20197;&#22312;&#27599;&#20010;&#29366;&#24577;&#20540;&#19978;&#24555;&#36895;&#19988;&#23454;&#26102;&#22320;&#35757;&#32451;&#65292;&#20135;&#29983;&#22686;&#30410;&#20989;&#25968;&#65292;&#32780;&#26080;&#38656;&#27714;&#35299;PDE&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#23558;NO&#29992;&#20110;GS-PDE&#22238;&#28335;&#30340;&#26041;&#27861;&#12290;GS&#25511;&#21046;&#22120;&#20551;&#35774;&#29366;&#24577;&#21464;&#21270;&#32531;&#24930;&#65292;&#24182;&#19988;&#20165;&#20445;&#35777;&#23616;&#37096;&#31283;&#23450;&#24615;&#65292;&#21363;&#20351;&#23545;&#20110;ODE&#20063;&#26159;&#22914;&#27492;&#12290;&#25105;&#20204;&#36890;&#36807;"&#20840;&#26680;"&#26041;&#27861;&#21644;"&#20165;&#22686;&#30410;"&#26041;&#27861;&#65292;&#30830;&#31435;&#20102;&#20855;&#26377;&#38750;&#32447;&#24615;&#24490;&#29615;&#30340;&#21452;&#26354;&#22411;PDE&#30340;&#23616;&#37096;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
To stabilize PDE models, control laws require space-dependent functional gains mapped by nonlinear operators from the PDE functional coefficients. When a PDE is nonlinear and its "pseudo-coefficient" functions are state-dependent, a gain-scheduling (GS) nonlinear design is the simplest approach to the design of nonlinear feedback. The GS version of PDE backstepping employs gains obtained by solving a PDE at each value of the state. Performing such PDE computations in real time may be prohibitive. The recently introduced neural operators (NO) can be trained to produce the gain functions, rapidly in real time, for each state value, without requiring a PDE solution. In this paper we introduce NOs for GS-PDE backstepping. GS controllers act on the premise that the state change is slow and, as a result, guarantee only local stability, even for ODEs. We establish local stabilization of hyperbolic PDEs with nonlinear recirculation using both a "full-kernel" approach and the "gain-only" approa
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#22270;&#23572;&#25991;&#30340;&#35760;&#24518;&#29702;&#35770;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#24847;&#35782;&#21487;&#33021;&#26159;&#19968;&#31181;&#22522;&#20110;&#36825;&#31181;&#23545;&#24212;&#20851;&#31995;&#30340;&#26032;&#20852;&#33021;&#21147;&#30340;&#29468;&#24819;&#12290;</title><link>http://arxiv.org/abs/2401.02509</link><description>&lt;p&gt;
&#35760;&#24518;&#12289;&#24847;&#35782;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Memory, Consciousness and Large Language Model. (arXiv:2401.02509v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02509
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#22270;&#23572;&#25991;&#30340;&#35760;&#24518;&#29702;&#35770;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#24847;&#35782;&#21487;&#33021;&#26159;&#19968;&#31181;&#22522;&#20110;&#36825;&#31181;&#23545;&#24212;&#20851;&#31995;&#30340;&#26032;&#20852;&#33021;&#21147;&#30340;&#29468;&#24819;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35748;&#30693;&#31185;&#23398;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21457;&#23637;&#65292;&#36825;&#20004;&#20010;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#32852;&#31995;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#25581;&#31034;&#20986;&#26469;&#12290;&#22312;&#36825;&#20123;&#32852;&#31995;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29468;&#24819;&#65292;&#21363;LLM&#21644;&#22270;&#23572;&#25991;&#30340;&#35760;&#24518;&#29702;&#35770;&#20043;&#38388;&#23384;&#22312;&#19968;&#31181;&#23545;&#20598;&#20851;&#31995;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#22270;&#23572;&#25991;&#30340;&#21327;&#21516;&#24341;&#21457;&#65288;SEM&#65289;&#26816;&#32034;&#27169;&#22411;&#21644;LLM&#20013;&#35266;&#23519;&#21040;&#30340;&#26032;&#20852;&#33021;&#21147;&#20043;&#38388;&#30340;&#28508;&#22312;&#23545;&#24212;&#20851;&#31995;&#65292;&#20026;&#25105;&#20204;&#30340;&#29468;&#24819;&#25552;&#20379;&#20102;&#25903;&#25345;&#35777;&#25454;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25512;&#27979;&#24847;&#35782;&#21487;&#33021;&#34987;&#35748;&#20026;&#26159;&#36825;&#31181;&#23545;&#20598;&#24615;&#30340;&#19968;&#31181;&#26032;&#20852;&#33021;&#21147;&#24418;&#24335;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#20854;&#20182;&#24847;&#35782;&#29702;&#35770;&#22914;&#20309;&#19982;&#25105;&#20204;&#30340;&#30740;&#31350;&#30456;&#20132;&#21449;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the development in cognitive science and Large Language Models (LLMs), increasing connections have come to light between these two distinct fields. Building upon these connections, we propose a conjecture suggesting the existence of a duality between LLMs and Tulving's theory of memory. We identify a potential correspondence between Tulving's synergistic ecphory model (SEM) of retrieval and the emergent abilities observed in LLMs, serving as supporting evidence for our conjecture. Furthermore, we speculate that consciousness may be considered a form of emergent ability based on this duality. We also discuss how other theories of consciousness intersect with our research.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20256;&#32479;&#31526;&#21495;&#35268;&#21010;&#22120;&#38598;&#25104;&#30340;&#21069;&#26223;&#65292;&#24182;&#25351;&#20986;&#36825;&#31181;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#26377;&#26395;&#22312;&#35299;&#20915;&#22797;&#26434;&#35268;&#21010;&#38382;&#39064;&#20013;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.02500</link><description>&lt;p&gt;
&#35770;&#33258;&#21160;&#35268;&#21010;&#19982;&#35843;&#24230;&#20013;&#24341;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21069;&#26223;
&lt;/p&gt;
&lt;p&gt;
On the Prospects of Incorporating Large Language Models (LLMs) in Automated Planning and Scheduling (APS). (arXiv:2401.02500v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02500
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20256;&#32479;&#31526;&#21495;&#35268;&#21010;&#22120;&#38598;&#25104;&#30340;&#21069;&#26223;&#65292;&#24182;&#25351;&#20986;&#36825;&#31181;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#26377;&#26395;&#22312;&#35299;&#20915;&#22797;&#26434;&#35268;&#21010;&#38382;&#39064;&#20013;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#35268;&#21010;&#19982;&#35843;&#24230;&#26159;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#22686;&#38271;&#36805;&#36895;&#30340;&#39046;&#22495;&#20043;&#19968;&#65292;&#20854;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#26412;&#25991;&#22522;&#20110;&#23545;126&#31687;&#35770;&#25991;&#30340;&#32508;&#21512;&#22238;&#39038;&#65292;&#30740;&#31350;&#20102;LLMs&#22312;&#35299;&#20915;&#35268;&#21010;&#38382;&#39064;&#30340;&#21508;&#20010;&#26041;&#38754;&#20013;&#29420;&#29305;&#24212;&#29992;&#30340;&#20843;&#20010;&#31867;&#21035;&#65306;&#35821;&#35328;&#32763;&#35793;&#12289;&#35745;&#21010;&#29983;&#25104;&#12289;&#27169;&#22411;&#26500;&#24314;&#12289;&#22810;&#26234;&#33021;&#20307;&#35268;&#21010;&#12289;&#20132;&#20114;&#24335;&#35268;&#21010;&#12289;&#21551;&#21457;&#24335;&#20248;&#21270;&#12289;&#24037;&#20855;&#38598;&#25104;&#21644;&#33041;&#21551;&#21457;&#24335;&#35268;&#21010;&#12290;&#38024;&#23545;&#27599;&#20010;&#31867;&#21035;&#65292;&#25105;&#20204;&#26126;&#30830;&#20102;&#32771;&#34385;&#30340;&#38382;&#39064;&#21644;&#29616;&#26377;&#30340;&#31354;&#30333;&#12290;&#25105;&#20204;&#22238;&#39038;&#30340;&#19968;&#20010;&#37325;&#35201;&#35266;&#28857;&#26159;&#65292;&#22312;&#19982;&#20256;&#32479;&#31526;&#21495;&#35268;&#21010;&#22120;&#38598;&#25104;&#26102;&#65292;LLMs&#30340;&#30495;&#27491;&#28508;&#21147;&#24471;&#20197;&#21457;&#25381;&#65292;&#36825;&#25351;&#21521;&#20102;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#25928;&#22320;&#23558;LLMs&#30340;&#29983;&#25104;&#33021;&#21147;&#19982;&#32463;&#20856;&#35268;&#21010;&#26041;&#27861;&#30340;&#31934;&#30830;&#24615;&#32467;&#21512;&#36215;&#26469;&#12290;&#36890;&#36807;&#32508;&#21512;&#29616;&#26377;&#25991;&#29486;&#30340;&#35265;&#35299;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#36825;&#31181;&#38598;&#25104;&#22312;&#35299;&#20915;&#22797;&#26434;&#35268;&#21010;&#38382;&#39064;&#19978;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated Planning and Scheduling is among the growing areas in Artificial Intelligence (AI) where mention of LLMs has gained popularity. Based on a comprehensive review of 126 papers, this paper investigates eight categories based on the unique applications of LLMs in addressing various aspects of planning problems: language translation, plan generation, model construction, multi-agent planning, interactive planning, heuristics optimization, tool integration, and brain-inspired planning. For each category, we articulate the issues considered and existing gaps. A critical insight resulting from our review is that the true potential of LLMs unfolds when they are integrated with traditional symbolic planners, pointing towards a promising neuro-symbolic approach. This approach effectively combines the generative aspects of LLMs with the precision of classical planning methods. By synthesizing insights from existing literature, we underline the potential of this integration to address comp
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#27668;&#20505;&#21464;&#21270;&#23545;&#27745;&#27700;&#31649;&#29702;&#24102;&#26469;&#30340;&#22797;&#26434;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#35299;&#37322;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#30340;&#24223;&#27700;&#24314;&#27169;&#26041;&#27861;&#65292;&#21487;&#20197;&#24110;&#21161;&#39044;&#27979;&#20851;&#38190;&#27700;&#20301;&#28857;&#24182;&#25913;&#21892;&#24223;&#27700;&#31649;&#29702;&#21644;&#29615;&#22659;&#27745;&#26579;&#39044;&#38450;&#12290;</title><link>http://arxiv.org/abs/2401.02465</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#29992;&#20110;&#32508;&#21512;&#19979;&#27700;&#36947;&#28322;&#27969;&#30340;&#24223;&#27700;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Interpretable Time Series Models for Wastewater Modeling in Combined Sewer Overflows. (arXiv:2401.02465v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02465
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#27668;&#20505;&#21464;&#21270;&#23545;&#27745;&#27700;&#31649;&#29702;&#24102;&#26469;&#30340;&#22797;&#26434;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#35299;&#37322;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#30340;&#24223;&#27700;&#24314;&#27169;&#26041;&#27861;&#65292;&#21487;&#20197;&#24110;&#21161;&#39044;&#27979;&#20851;&#38190;&#27700;&#20301;&#28857;&#24182;&#25913;&#21892;&#24223;&#27700;&#31649;&#29702;&#21644;&#29615;&#22659;&#27745;&#26579;&#39044;&#38450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27668;&#20505;&#21464;&#21270;&#23545;&#25105;&#20204;&#30340;&#31038;&#20250;&#25552;&#20986;&#20102;&#36234;&#26469;&#36234;&#22797;&#26434;&#30340;&#25361;&#25112;&#12290;&#27946;&#27700;&#12289;&#37326;&#28779;&#25110;&#24178;&#26097;&#31561;&#26497;&#31471;&#22825;&#27668;&#20107;&#20214;&#21464;&#24471;&#36234;&#26469;&#36234;&#39057;&#32321;&#12289;&#31361;&#21457;&#19988;&#38590;&#20197;&#39044;&#35265;&#25110;&#24212;&#23545;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#29305;&#21035;&#35299;&#20915;&#20102;&#37325;&#38632;&#20107;&#20214;&#23548;&#33268;&#19979;&#38632;&#32592;&#28322;&#20986;&#20197;&#21518;&#65292;&#27745;&#27700;&#27745;&#26579;&#22320;&#34920;&#27700;&#20307;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#26368;&#20808;&#36827;&#30340;&#21487;&#35299;&#37322;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#22312;&#22914;&#20309;&#39044;&#27979;&#36825;&#20123;&#20020;&#30028;&#27700;&#20301;&#28857;&#26041;&#38754;&#30340;&#20316;&#29992;&#65292;&#20197;&#20415;&#33021;&#22815;&#21450;&#26102;&#23558;&#22810;&#20313;&#30340;&#27700;&#37325;&#26032;&#20998;&#37197;&#21040;&#19979;&#27700;&#36947;&#32593;&#32476;&#20013;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#29616;&#20195;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#21487;&#20197;&#20026;&#25913;&#21892;&#24223;&#27700;&#31649;&#29702;&#21644;&#39044;&#38450;&#19979;&#27700;&#36947;&#31995;&#32479;&#23545;&#29615;&#22659;&#30340;&#27745;&#26579;&#20570;&#20986;&#36129;&#29486;&#12290;&#25152;&#26377;&#30340;&#20195;&#30721;&#21644;&#23454;&#39564;&#21487;&#20197;&#22312;&#25105;&#20204;&#30340;&#23384;&#20648;&#24211;&#20013;&#25214;&#21040;&#65306;https://github.com/TeodorChiaburu/RIWWER_TimeSeries&#12290;
&lt;/p&gt;
&lt;p&gt;
Climate change poses increasingly complex challenges to our society. Extreme weather events such as floods, wild fires or droughts are becoming more frequent, spontaneous and difficult to foresee or counteract. In this work we specifically address the problem of sewage water polluting surface water bodies after spilling over from rain tanks as a consequence of heavy rain events. We investigate to what extent state-of-the-art interpretable time series models can help predict such critical water level points, so that the excess can promptly be redistributed across the sewage network. Our results indicate that modern time series models can contribute to better waste water management and prevention of environmental pollution from sewer systems. All the code and experiments can be found in our repository: https://github.com/TeodorChiaburu/RIWWER_TimeSeries.
&lt;/p&gt;</description></item><item><title>&#35745;&#31639;&#21307;&#30103;&#20013;&#30340;&#25968;&#25454;&#20013;&#24515;&#22522;&#30784;&#27169;&#22411;&#26159;&#19968;&#39033;&#35843;&#26597;&#30740;&#31350;&#65292;&#20026;&#21307;&#30103;&#24037;&#20316;&#27969;&#31243;&#30340;&#25913;&#36827;&#25552;&#20379;&#20102;&#22522;&#20110;&#25968;&#25454;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#23433;&#20840;&#24615;&#12289;&#35780;&#20272;&#21644;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#19968;&#33268;&#24615;&#12290;&#22522;&#20110;FM&#30340;&#20998;&#26512;&#26377;&#26395;&#25552;&#39640;&#24739;&#32773;&#32467;&#26524;&#21644;&#20020;&#24202;&#24037;&#20316;&#27969;&#31243;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2401.02458</link><description>&lt;p&gt;
&#35745;&#31639;&#21307;&#30103;&#20013;&#30340;&#25968;&#25454;&#20013;&#24515;&#22522;&#30784;&#27169;&#22411;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Data-Centric Foundation Models in Computational Healthcare: A Survey. (arXiv:2401.02458v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02458
&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#21307;&#30103;&#20013;&#30340;&#25968;&#25454;&#20013;&#24515;&#22522;&#30784;&#27169;&#22411;&#26159;&#19968;&#39033;&#35843;&#26597;&#30740;&#31350;&#65292;&#20026;&#21307;&#30103;&#24037;&#20316;&#27969;&#31243;&#30340;&#25913;&#36827;&#25552;&#20379;&#20102;&#22522;&#20110;&#25968;&#25454;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#23433;&#20840;&#24615;&#12289;&#35780;&#20272;&#21644;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#19968;&#33268;&#24615;&#12290;&#22522;&#20110;FM&#30340;&#20998;&#26512;&#26377;&#26395;&#25552;&#39640;&#24739;&#32773;&#32467;&#26524;&#21644;&#20020;&#24202;&#24037;&#20316;&#27969;&#31243;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#19968;&#22871;&#26032;&#20852;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#22522;&#30784;&#27169;&#22411;&#65288;FMs&#65289;&#30340;&#20986;&#29616;&#20026;&#35745;&#31639;&#21307;&#30103;&#24102;&#26469;&#20102;&#19968;&#31995;&#21015;&#26426;&#36935;&#12290;&#36825;&#20123;&#27169;&#22411;&#30340;&#20132;&#20114;&#24615;&#30001;&#39044;&#35757;&#32451;&#25968;&#25454;&#21644;&#20154;&#31867;&#25351;&#20196;&#24341;&#23548;&#65292;&#24341;&#21457;&#20102;&#19968;&#20010;&#25968;&#25454;&#20013;&#24515;&#30340;&#20154;&#24037;&#26234;&#33021;&#33539;&#24335;&#65292;&#24378;&#35843;&#26356;&#22909;&#30340;&#25968;&#25454;&#34920;&#24449;&#12289;&#36136;&#37327;&#21644;&#35268;&#27169;&#12290;&#22312;&#21307;&#30103;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#33719;&#21462;&#21644;&#22788;&#29702;&#39640;&#36136;&#37327;&#30340;&#20020;&#24202;&#25968;&#25454;&#35760;&#24405;&#19968;&#30452;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#65292;&#28041;&#21450;&#25968;&#25454;&#25968;&#37327;&#12289;&#27880;&#37322;&#12289;&#24739;&#32773;&#38544;&#31169;&#21644;&#20262;&#29702;&#31561;&#26041;&#38754;&#12290;&#22312;&#36825;&#39033;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;FM&#26102;&#20195;&#30340;&#24191;&#27867;&#25968;&#25454;&#20013;&#24515;&#26041;&#27861;&#65288;&#20174;&#27169;&#22411;&#39044;&#35757;&#32451;&#21040;&#25512;&#29702;&#65289;&#65292;&#20197;&#25913;&#36827;&#21307;&#30103;&#24037;&#20316;&#27969;&#31243;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#20154;&#24037;&#26234;&#33021;&#23433;&#20840;&#24615;&#12289;&#35780;&#20272;&#20197;&#21450;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#19968;&#33268;&#24615;&#30340;&#20851;&#38190;&#35266;&#28857;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#26395;&#20102;&#22522;&#20110;FM&#30340;&#20998;&#26512;&#22312;&#21307;&#30103;&#21644;&#21307;&#33647;&#39046;&#22495;&#19981;&#26029;&#21457;&#23637;&#30340;&#26684;&#23616;&#20013;&#25552;&#39640;&#24739;&#32773;&#32467;&#26524;&#21644;&#20020;&#24202;&#24037;&#20316;&#27969;&#31243;&#34920;&#29616;&#30340;&#21069;&#26223;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26368;&#26032;&#30340;&#21307;&#30103;&#28165;&#21333;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of foundation models (FMs) as an emerging suite of AI techniques has struck a wave of opportunities in computational healthcare. The interactive nature of these models, guided by pre-training data and human instructions, has ignited a data-centric AI paradigm that emphasizes better data characterization, quality, and scale. In healthcare AI, obtaining and processing high-quality clinical data records has been a longstanding challenge, ranging from data quantity, annotation, patient privacy, and ethics. In this survey, we investigate a wide range of data-centric approaches in the FM era (from model pre-training to inference) towards improving the healthcare workflow. We discuss key perspectives in AI security, assessment, and alignment with human values. Finally, we offer a promising outlook of FM-based analytics to enhance the performance of patient outcome and clinical workflow in the evolving landscape of healthcare and medicine. We provide an up-to-date list of healthcare
&lt;/p&gt;</description></item><item><title>eCIL-MU&#26159;&#19968;&#31181;&#22522;&#20110;&#23884;&#20837;&#25216;&#26415;&#30340;&#36880;&#31867;&#22686;&#37327;&#23398;&#20064;&#21644;&#26426;&#22120;&#21462;&#28040;&#23398;&#20064;&#30340;&#38750;&#30772;&#22351;&#24615;&#26694;&#26550;&#65292;&#21487;&#29992;&#20110;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#24555;&#36895;&#33719;&#21462;&#20851;&#20110;&#26032;&#31867;&#21035;&#30340;&#30693;&#35782;&#65292;&#24182;&#28040;&#38500;&#20808;&#21069;&#23398;&#20064;&#31867;&#21035;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.02457</link><description>&lt;p&gt;
eCIL-MU: &#22522;&#20110;&#23884;&#20837;&#30340;&#36880;&#31867;&#22686;&#37327;&#23398;&#20064;&#21644;&#26426;&#22120;&#21462;&#28040;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
eCIL-MU: Embedding based Class Incremental Learning and Machine Unlearning. (arXiv:2401.02457v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02457
&lt;/p&gt;
&lt;p&gt;
eCIL-MU&#26159;&#19968;&#31181;&#22522;&#20110;&#23884;&#20837;&#25216;&#26415;&#30340;&#36880;&#31867;&#22686;&#37327;&#23398;&#20064;&#21644;&#26426;&#22120;&#21462;&#28040;&#23398;&#20064;&#30340;&#38750;&#30772;&#22351;&#24615;&#26694;&#26550;&#65292;&#21487;&#29992;&#20110;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#24555;&#36895;&#33719;&#21462;&#20851;&#20110;&#26032;&#31867;&#21035;&#30340;&#30693;&#35782;&#65292;&#24182;&#28040;&#38500;&#20808;&#21069;&#23398;&#20064;&#31867;&#21035;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#65292;&#21487;&#33021;&#20250;&#19981;&#26029;&#24341;&#20837;&#26032;&#30340;&#31867;&#21035;&#65292;&#25110;&#32773;&#38656;&#35201;&#23545;&#29616;&#26377;&#31867;&#21035;&#36827;&#34892;&#37325;&#26032;&#20998;&#31867;&#12290;&#36880;&#31867;&#22686;&#37327;&#23398;&#20064; (CIL) &#29992;&#20110;&#22312;&#33719;&#21462;&#20851;&#20110;&#26032;&#31867;&#21035;&#30340;&#30693;&#35782;&#30340;&#21516;&#26102;&#65292;&#20445;&#30041;&#23545;&#20808;&#21069;&#23398;&#20064;&#31867;&#21035;&#30340;&#20449;&#24687;&#12290;&#20026;&#20102;&#36866;&#24212;&#37325;&#26032;&#20998;&#31867;&#65292;&#36824;&#21487;&#33021;&#38656;&#35201;&#28040;&#38500;&#30456;&#20851;&#31867;&#21035;&#23545;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#22312;CIL&#20013;&#24341;&#20837;&#20102;&#22522;&#20110;&#31867;&#21035;&#30340;&#26426;&#22120;&#21462;&#28040;&#23398;&#20064; (MU)&#12290;&#36890;&#24120;&#65292;MU&#26041;&#27861;&#20542;&#21521;&#20110;&#32791;&#26102;&#65292;&#24182;&#19988;&#21487;&#33021;&#20250;&#25439;&#23475;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#36830;&#32493;&#30340;&#21462;&#28040;&#23398;&#20064;&#35831;&#27714;&#21487;&#33021;&#23548;&#33268;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#30772;&#22351;&#24615;&#30340;&#22522;&#20110;&#23884;&#20837;&#25216;&#26415;&#30340;eCIL-MU&#26694;&#26550;&#65292;&#23558;&#25968;&#25454;&#26144;&#23556;&#21040;&#21521;&#37327;&#24182;&#23384;&#20648;&#22312;&#21521;&#37327;&#25968;&#25454;&#24211;&#20013;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;CIL&#21644;MU&#20219;&#21153;&#20043;&#38388;&#30340;&#37325;&#21472;&#26469;&#21152;&#36895;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#23454;&#29616;&#21462;&#28040;&#23398;&#20064;&#25928;&#26524;&#21644;&#25968;&#37327;&#32423;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
New categories may be introduced over time, or existing categories may need to be reclassified. Class incremental learning (CIL) is employed for the gradual acquisition of knowledge about new categories while preserving information about previously learned ones in such dynamic environments. It might also be necessary to also eliminate the influence of related categories on the model to adapt to reclassification. We thus introduce class-level machine unlearning (MU) within CIL. Typically, MU methods tend to be time-consuming and can potentially harm the model's performance. A continuous stream of unlearning requests could lead to catastrophic forgetting. To address these issues, we propose a non-destructive eCIL-MU framework based on embedding techniques to map data into vectors and then be stored in vector databases. Our approach exploits the overlap between CIL and MU tasks for acceleration. Experiments demonstrate the capability of achieving unlearning effectiveness and orders of mag
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32508;&#36848;&#25506;&#35752;&#20102;AI&#25903;&#25345;&#30340;&#26080;&#20154;&#26426;&#31995;&#32479;&#22312;&#28779;&#28798;&#31649;&#29702;&#20013;&#30340;&#24212;&#29992;&#65292;&#20174;&#28779;&#28798;&#21069;&#21040;&#20027;&#21160;&#28779;&#28798;&#38454;&#27573;&#20877;&#21040;&#28779;&#28798;&#21518;&#65292;&#36890;&#36807;&#25972;&#21512;&#26080;&#20154;&#26426;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#26356;&#26377;&#25928;&#30340;&#37326;&#28779;&#31649;&#29702;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2401.02456</link><description>&lt;p&gt;
&#38754;&#21521;&#28779;&#28798;&#31649;&#29702;&#30340;AI&#25903;&#25345;&#26080;&#20154;&#26426;&#31995;&#32479;&#30740;&#31350;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A comprehensive survey of research towards AI-enabled unmanned aerial systems in pre-, active-, and post-wildfire management. (arXiv:2401.02456v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02456
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32508;&#36848;&#25506;&#35752;&#20102;AI&#25903;&#25345;&#30340;&#26080;&#20154;&#26426;&#31995;&#32479;&#22312;&#28779;&#28798;&#31649;&#29702;&#20013;&#30340;&#24212;&#29992;&#65292;&#20174;&#28779;&#28798;&#21069;&#21040;&#20027;&#21160;&#28779;&#28798;&#38454;&#27573;&#20877;&#21040;&#28779;&#28798;&#21518;&#65292;&#36890;&#36807;&#25972;&#21512;&#26080;&#20154;&#26426;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#26356;&#26377;&#25928;&#30340;&#37326;&#28779;&#31649;&#29702;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37326;&#28779;&#24050;&#25104;&#20026;&#20840;&#29699;&#26368;&#20855;&#30772;&#22351;&#24615;&#30340;&#33258;&#28982;&#28798;&#23475;&#20043;&#19968;&#65292;&#32473;&#20154;&#31867;&#29983;&#21629;&#21644;&#26862;&#26519;&#37326;&#29983;&#21160;&#29289;&#36896;&#25104;&#20102;&#28798;&#38590;&#24615;&#25439;&#22833;&#12290;&#26368;&#36817;&#65292;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#22312;&#37326;&#28779;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#26080;&#20154;&#26426;&#65288;UAV&#65289;&#19982;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25972;&#21512;&#65292;&#21019;&#36896;&#20102;&#23454;&#26045;&#21644;&#21457;&#23637;&#26356;&#26377;&#25928;&#30340;&#37326;&#28779;&#31649;&#29702;&#30340;&#21069;&#25152;&#26410;&#26377;&#30340;&#21160;&#21147;&#12290;&#23613;&#31649;&#19968;&#20123;&#29616;&#26377;&#30340;&#35843;&#26597;&#35770;&#25991;&#24050;&#25506;&#35752;&#20102;&#21508;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20294;&#26126;&#26174;&#32570;&#20047;&#19968;&#31687;&#20840;&#38754;&#32508;&#36848;&#65292;&#24378;&#35843;AI&#25903;&#25345;&#30340;UAV&#31995;&#32479;&#22312;&#22810;&#38454;&#27573;&#37326;&#28779;&#31649;&#29702;&#20013;&#30340;&#24212;&#29992;&#21450;&#20854;&#21518;&#32493;&#24433;&#21709;&#12290;&#26412;&#32508;&#36848;&#26088;&#22312;&#22635;&#34917;&#36825;&#20123;&#24046;&#36317;&#65292;&#36890;&#36807;&#31995;&#32479;&#22238;&#39038;&#26368;&#26032;&#30340;&#25216;&#26415;&#36827;&#23637;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#20174;&#28779;&#28798;&#21069;&#21040;&#20027;&#21160;&#28779;&#28798;&#38454;&#27573;&#20877;&#21040;&#28779;&#28798;&#21518;&#31649;&#29702;&#30340;UAV&#31995;&#32479;&#21644;AI&#27169;&#22411;&#30340;&#36827;&#23637;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23545;&#29616;&#26377;&#30340;&#36965;&#24863;&#31995;&#32479;&#36827;&#34892;&#20102;&#24191;&#27867;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Wildfires have emerged as one of the most destructive natural disasters worldwide, causing catastrophic losses in both human lives and forest wildlife. Recently, the use of Artificial Intelligence (AI) in wildfires, propelled by the integration of Unmanned Aerial Vehicles (UAVs) and deep learning models, has created an unprecedented momentum to implement and develop more effective wildfire management. Although some of the existing survey papers have explored various learning-based approaches, a comprehensive review emphasizing the application of AI-enabled UAV systems and their subsequent impact on multi-stage wildfire management is notably lacking. This survey aims to bridge these gaps by offering a systematic review of the recent state-of-the-art technologies, highlighting the advancements of UAV systems and AI models from pre-fire, through the active-fire stage, to post-fire management. To this aim, we provide an extensive analysis of the existing remote sensing systems with a parti
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35843;&#26597;&#20102;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20013;&#35745;&#31639;&#20998;&#27495;&#23545;&#23398;&#26415;&#36129;&#29486;&#21644;&#23457;&#26597;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#35745;&#31639;&#20998;&#27495;&#23548;&#33268;&#23398;&#26415;&#30028;&#22312;&#35745;&#31639;&#23494;&#38598;&#22411;&#30740;&#31350;&#20027;&#39064;&#20013;&#30340;&#24433;&#21709;&#21147;&#38477;&#20302;&#65292;&#24182;&#19988;&#23398;&#26415;&#30740;&#31350;&#36235;&#21521;&#20110;&#20351;&#29992;&#24037;&#19994;&#30028;&#24320;&#21457;&#30340;&#24320;&#28304;&#12289;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#24314;&#35758;&#36890;&#36807;&#22269;&#23478;&#25903;&#25345;&#30340;&#35745;&#31639;&#22522;&#30784;&#35774;&#26045;&#19982;&#24320;&#25918;&#31185;&#23398;&#20513;&#35758;&#30340;&#32467;&#21512;&#26469;&#25299;&#23637;&#23398;&#26415;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2401.02452</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#35745;&#31639;&#20998;&#27495;&#65306;&#23545;&#23398;&#26415;&#36129;&#29486;&#21644;&#23457;&#26597;&#30340;&#23041;&#32961;&#65311;
&lt;/p&gt;
&lt;p&gt;
The Compute Divide in Machine Learning: A Threat to Academic Contribution and Scrutiny?. (arXiv:2401.02452v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02452
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35843;&#26597;&#20102;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20013;&#35745;&#31639;&#20998;&#27495;&#23545;&#23398;&#26415;&#36129;&#29486;&#21644;&#23457;&#26597;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#35745;&#31639;&#20998;&#27495;&#23548;&#33268;&#23398;&#26415;&#30028;&#22312;&#35745;&#31639;&#23494;&#38598;&#22411;&#30740;&#31350;&#20027;&#39064;&#20013;&#30340;&#24433;&#21709;&#21147;&#38477;&#20302;&#65292;&#24182;&#19988;&#23398;&#26415;&#30740;&#31350;&#36235;&#21521;&#20110;&#20351;&#29992;&#24037;&#19994;&#30028;&#24320;&#21457;&#30340;&#24320;&#28304;&#12289;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#24314;&#35758;&#36890;&#36807;&#22269;&#23478;&#25903;&#25345;&#30340;&#35745;&#31639;&#22522;&#30784;&#35774;&#26045;&#19982;&#24320;&#25918;&#31185;&#23398;&#20513;&#35758;&#30340;&#32467;&#21512;&#26469;&#25299;&#23637;&#23398;&#26415;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#19994;&#21644;&#23398;&#26415;&#20154;&#24037;&#26234;&#33021;&#23454;&#39564;&#23460;&#22312;&#20351;&#29992;&#35745;&#31639;&#36164;&#28304;&#26041;&#38754;&#23384;&#22312;&#26126;&#26174;&#24046;&#24322;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#39033;&#25968;&#25454;&#39537;&#21160;&#30340;&#35843;&#26597;&#65292;&#20197;&#20102;&#35299;&#35745;&#31639;&#20998;&#27495;&#23545;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#35745;&#31639;&#20998;&#27495;&#19982;&#35745;&#31639;&#23494;&#38598;&#22411;&#30740;&#31350;&#20027;&#39064;&#20013;&#23398;&#26415;&#29420;&#31435;&#30740;&#31350;&#22242;&#38431;&#30340;&#34920;&#31034;&#20943;&#23569;&#26377;&#20851;&#65292;&#29305;&#21035;&#26159;&#22522;&#30784;&#27169;&#22411;&#26041;&#38754;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#23398;&#26415;&#30028;&#22312;&#25512;&#36827;&#30456;&#20851;&#25216;&#26415;&#12289;&#25552;&#20379;&#20851;&#38190;&#30340;&#35780;&#20272;&#21644;&#23457;&#26597;&#20197;&#21450;&#22312;&#36825;&#20123;&#27169;&#22411;&#30340;&#20256;&#25773;&#26041;&#38754;&#21487;&#33021;&#21457;&#25381;&#30340;&#20316;&#29992;&#23558;&#36739;&#23567;&#12290;&#19982;&#30740;&#31350;&#37325;&#24515;&#30340;&#36825;&#31181;&#21464;&#21270;&#21516;&#26102;&#65292;&#23398;&#26415;&#30740;&#31350;&#22312;&#20513;&#23548;&#24037;&#19994;&#30028;&#24320;&#21457;&#30340;&#24320;&#28304;&#12289;&#39044;&#35757;&#32451;&#27169;&#22411;&#26041;&#38754;&#20986;&#29616;&#20102;&#26126;&#26174;&#36716;&#21464;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#36235;&#21183;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#23588;&#20854;&#26159;&#23545;&#24433;&#21709;&#21147;&#27169;&#22411;&#30340;&#23457;&#26597;&#20943;&#23569;&#65292;&#25105;&#20204;&#24314;&#35758;&#37319;&#21462;&#19968;&#20123;&#26041;&#27861;&#26469;&#26377;&#38024;&#23545;&#24615;&#22320;&#25299;&#23637;&#23398;&#26415;&#35265;&#35299;&#12290;&#36825;&#20123;&#26041;&#27861;&#21253;&#25324;&#22269;&#23478;&#25903;&#25345;&#30340;&#35745;&#31639;&#22522;&#30784;&#35774;&#26045;&#19982;&#24320;&#25918;&#31185;&#23398;&#20513;&#35758;&#30340;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
There are pronounced differences in the extent to which industrial and academic AI labs use computing resources. We provide a data-driven survey of the role of the compute divide in shaping machine learning research. We show that a compute divide has coincided with a reduced representation of academic-only research teams in compute intensive research topics, especially foundation models. We argue that, academia will likely play a smaller role in advancing the associated techniques, providing critical evaluation and scrutiny, and in the diffusion of such models. Concurrent with this change in research focus, there is a noticeable shift in academic research towards embracing open source, pre-trained models developed within the industry. To address the challenges arising from this trend, especially reduced scrutiny of influential models, we recommend approaches aimed at thoughtfully expanding academic insights. Nationally-sponsored computing infrastructure coupled with open science initia
&lt;/p&gt;</description></item><item><title>FedDiff&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#21327;&#20316;&#25193;&#25955;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#23454;&#29616;&#26469;&#33258;&#19981;&#21516;&#23458;&#25143;&#30340;&#24322;&#26500;&#25968;&#25454;&#30340;&#23433;&#20840;&#34701;&#21512;&#65292;&#36890;&#36807;&#24314;&#31435;&#21452;&#20998;&#25903;&#25193;&#25955;&#27169;&#22411;&#29305;&#24449;&#25552;&#21462;&#35774;&#32622;&#26469;&#39537;&#21160;&#32852;&#37030;&#23398;&#20064;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2401.02433</link><description>&lt;p&gt;
FedDiff: &#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#21644;&#22810;&#23458;&#25143;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FedDiff: Diffusion Model Driven Federated Learning for Multi-Modal and Multi-Clients. (arXiv:2401.02433v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02433
&lt;/p&gt;
&lt;p&gt;
FedDiff&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#21327;&#20316;&#25193;&#25955;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#23454;&#29616;&#26469;&#33258;&#19981;&#21516;&#23458;&#25143;&#30340;&#24322;&#26500;&#25968;&#25454;&#30340;&#23433;&#20840;&#34701;&#21512;&#65292;&#36890;&#36807;&#24314;&#31435;&#21452;&#20998;&#25903;&#25193;&#25955;&#27169;&#22411;&#29305;&#24449;&#25552;&#21462;&#35774;&#32622;&#26469;&#39537;&#21160;&#32852;&#37030;&#23398;&#20064;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36965;&#24863;&#39046;&#22495;&#25104;&#20687;&#20256;&#24863;&#22120;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#22810;&#27169;&#24577;&#36965;&#24863;&#25968;&#25454;&#34701;&#21512;&#24050;&#25104;&#20026;&#22303;&#22320;&#35206;&#30422;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#30740;&#31350;&#26041;&#21521;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25193;&#25955;&#27169;&#22411;&#20027;&#35201;&#38598;&#20013;&#22312;&#21333;&#27169;&#24577;&#21644;&#21333;&#23458;&#25143;&#25511;&#21046;&#19978;&#65292;&#21363;&#25193;&#25955;&#36807;&#31243;&#30001;&#21333;&#20010;&#27169;&#24577;&#22312;&#21333;&#20010;&#35745;&#31639;&#33410;&#28857;&#39537;&#21160;&#12290;&#20026;&#20102;&#23454;&#29616;&#26469;&#33258;&#23458;&#25143;&#30340;&#24322;&#26500;&#25968;&#25454;&#30340;&#23433;&#20840;&#34701;&#21512;&#65292;&#38656;&#35201;&#23454;&#29616;&#20998;&#24067;&#24335;&#30340;&#22810;&#27169;&#24577;&#25511;&#21046;&#65292;&#20363;&#22914;&#22312;&#27599;&#20010;&#22522;&#31449;&#23458;&#25143;&#31471;&#19978;&#65292;&#23558;&#32452;&#32455;A&#30340;&#39640;&#20809;&#35889;&#25968;&#25454;&#21644;&#32452;&#32455;B&#30340;&#28608;&#20809;&#38647;&#36798;&#25968;&#25454;&#36827;&#34892;&#31169;&#23494;&#21512;&#24182;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedDiff&#30340;&#22810;&#27169;&#24577;&#21327;&#20316;&#25193;&#25955;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#24314;&#31435;&#20102;&#19968;&#20010;&#21452;&#20998;&#25903;&#25193;&#25955;&#27169;&#22411;&#29305;&#24449;&#25552;&#21462;&#35774;&#32622;&#65292;&#20854;&#20013;&#20004;&#31181;&#27169;&#24577;&#25968;&#25454;&#34987;&#36755;&#20837;&#21040;&#32534;&#30721;&#22120;&#30340;&#20998;&#25903;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rapid development of imaging sensor technology in the field of remote sensing, multi-modal remote sensing data fusion has emerged as a crucial research direction for land cover classification tasks. While diffusion models have made great progress in generative models and image classification tasks, existing models primarily focus on single-modality and single-client control, that is, the diffusion process is driven by a single modal in a single computing node. To facilitate the secure fusion of heterogeneous data from clients, it is necessary to enable distributed multi-modal control, such as merging the hyperspectral data of organization A and the LiDAR data of organization B privately on each base station client. In this study, we propose a multi-modal collaborative diffusion federated learning framework called FedDiff. Our framework establishes a dual-branch diffusion model feature extraction setup, where the two modal data are inputted into separate branches of the encoder
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#33258;&#21160;&#38169;&#35823;&#20998;&#31867;&#26694;&#26550;&#26469;&#35299;&#20915;ImageNet&#25968;&#25454;&#38598;&#20013;&#30340;&#26631;&#31614;&#22122;&#22768;&#21644;&#38169;&#35823;&#38382;&#39064;&#65292;&#20026;&#30740;&#31350;&#27169;&#22411;&#36873;&#25321;&#22914;&#20309;&#24433;&#21709;&#38169;&#35823;&#20998;&#24067;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2401.02430</link><description>&lt;p&gt;
&#33258;&#21160;&#20998;&#31867;ImageNet&#19978;&#30340;&#27169;&#22411;&#38169;&#35823;
&lt;/p&gt;
&lt;p&gt;
Automated Classification of Model Errors on ImageNet. (arXiv:2401.02430v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02430
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#33258;&#21160;&#38169;&#35823;&#20998;&#31867;&#26694;&#26550;&#26469;&#35299;&#20915;ImageNet&#25968;&#25454;&#38598;&#20013;&#30340;&#26631;&#31614;&#22122;&#22768;&#21644;&#38169;&#35823;&#38382;&#39064;&#65292;&#20026;&#30740;&#31350;&#27169;&#22411;&#36873;&#25321;&#22914;&#20309;&#24433;&#21709;&#38169;&#35823;&#20998;&#24067;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;ImageNet&#25968;&#25454;&#38598;&#22312;&#36807;&#21435;&#21313;&#24180;&#20013;&#25512;&#21160;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#30740;&#31350;&#65292;&#20294;&#26174;&#33879;&#30340;&#26631;&#31614;&#22122;&#22768;&#21644;&#27495;&#20041;&#20351;&#24471;Top-1&#20934;&#30830;&#29575;&#25104;&#20026;&#36827;&#19968;&#27493;&#36827;&#23637;&#30340;&#19981;&#22815;&#20805;&#20998;&#30340;&#25351;&#26631;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#26631;&#31614;&#38598;&#21644;&#35780;&#20272;&#21327;&#35758;&#65292;&#26174;&#31034;&#20986;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#24050;&#32463;&#23454;&#29616;&#20102;&#36229;&#36807;95%&#30340;&#20934;&#30830;&#29575;&#65292;&#24182;&#23558;&#37325;&#28857;&#36716;&#21521;&#20026;&#20160;&#20040;&#21097;&#20313;&#30340;&#38169;&#35823;&#20173;&#28982;&#23384;&#22312;&#30340;&#35843;&#26597;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#37319;&#29992;&#19987;&#23478;&#23567;&#32452;&#23545;&#20004;&#20010;&#36873;&#25321;&#30340;&#27169;&#22411;&#30340;&#25152;&#26377;&#21097;&#20313;&#20998;&#31867;&#38169;&#35823;&#36827;&#34892;&#25163;&#21160;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#36807;&#31243;&#32791;&#26102;&#65292;&#23481;&#26131;&#20135;&#29983;&#19981;&#19968;&#33268;&#24615;&#65292;&#24182;&#19988;&#38656;&#35201;&#21463;&#36807;&#35757;&#32451;&#30340;&#19987;&#23478;&#65292;&#20351;&#20854;&#19981;&#36866;&#29992;&#20110;&#24120;&#35268;&#27169;&#22411;&#35780;&#20272;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#20854;&#25928;&#29992;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#33258;&#21160;&#38169;&#35823;&#20998;&#31867;&#26694;&#26550;&#65292;&#36825;&#26159;&#19968;&#20010;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#30740;&#31350;&#24314;&#27169;&#36873;&#25321;&#22914;&#20309;&#24433;&#21709;&#38169;&#35823;&#20998;&#24067;&#12290;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#26694;&#26550;&#20840;&#38754;&#35780;&#20272;&#20102;&#36229;&#36807;90&#20010;&#27169;&#22411;&#30340;&#38169;&#35823;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
While the ImageNet dataset has been driving computer vision research over the past decade, significant label noise and ambiguity have made top-1 accuracy an insufficient measure of further progress. To address this, new label-sets and evaluation protocols have been proposed for ImageNet showing that state-of-the-art models already achieve over 95% accuracy and shifting the focus on investigating why the remaining errors persist.  Recent work in this direction employed a panel of experts to manually categorize all remaining classification errors for two selected models. However, this process is time-consuming, prone to inconsistencies, and requires trained experts, making it unsuitable for regular model evaluation thus limiting its utility. To overcome these limitations, we propose the first automated error classification framework, a valuable tool to study how modeling choices affect error distributions. We use our framework to comprehensively evaluate the error distribution of over 90
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#33041;&#21551;&#21457;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#26159;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#24212;&#29992;&#20110;&#24037;&#19994;&#25925;&#38556;&#35786;&#26029;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#21487;&#20197;&#20811;&#26381;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#38480;&#21046;&#65292;&#25552;&#20379;&#26356;&#31934;&#30830;&#21644;&#26377;&#25928;&#30340;&#25925;&#38556;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2401.02429</link><description>&lt;p&gt;
&#22522;&#20110;&#33041;&#21551;&#21457;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#22312;&#24037;&#19994;&#25925;&#38556;&#35786;&#26029;&#20013;&#30340;&#24212;&#29992;&#65306;&#35843;&#26597;&#12289;&#25361;&#25112;&#21644;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
Brain-Inspired Spiking Neural Networks for Industrial Fault Diagnosis: A Survey, Challenges, and Opportunities. (arXiv:2401.02429v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02429
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#33041;&#21551;&#21457;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#26159;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#24212;&#29992;&#20110;&#24037;&#19994;&#25925;&#38556;&#35786;&#26029;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#21487;&#20197;&#20811;&#26381;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#38480;&#21046;&#65292;&#25552;&#20379;&#26356;&#31934;&#30830;&#21644;&#26377;&#25928;&#30340;&#25925;&#38556;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#20960;&#21313;&#24180;&#26469;&#65292;&#24037;&#19994;&#25925;&#38556;&#35786;&#26029;&#65288;IFD&#65289;&#20316;&#20026;&#19968;&#38376;&#20851;&#27880;&#26816;&#27979;&#21644;&#25910;&#38598;&#24037;&#19994;&#35774;&#22791;&#20581;&#24247;&#29366;&#20917;&#37325;&#35201;&#20449;&#24687;&#30340;&#23398;&#31185;&#32780;&#20986;&#29616;&#65292;&#20174;&#32780;&#20419;&#36827;&#20102;&#23545;&#25925;&#38556;&#31867;&#22411;&#21644;&#20005;&#37325;&#31243;&#24230;&#30340;&#35782;&#21035;&#12290;&#31934;&#30830;&#21644;&#26377;&#25928;&#30340;&#25925;&#38556;&#35782;&#21035;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#23548;&#33268;&#23545;&#33258;&#21160;&#21270;&#35774;&#22791;&#30417;&#27979;&#30340;&#20851;&#27880;&#65292;&#20197;&#36991;&#20813;&#23433;&#20840;&#20107;&#25925;&#24182;&#20943;&#23569;&#23545;&#20154;&#21147;&#30340;&#20381;&#36182;&#12290;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANNs&#65289;&#30340;&#20986;&#29616;&#22312;&#22686;&#24378;&#26234;&#33021;IFD&#31639;&#27861;&#26041;&#38754;&#36215;&#21040;&#20102;&#37325;&#35201;&#20316;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#22823;&#25968;&#25454;&#32972;&#26223;&#19979;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#36827;&#23637;&#65292;&#20316;&#20026;&#19968;&#31181;&#31616;&#21270;&#30340;&#20223;&#29983;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;ANNs&#23384;&#22312;&#22266;&#26377;&#30340;&#38480;&#21046;&#65292;&#22914;&#36164;&#28304;&#21644;&#25968;&#25454;&#20381;&#36182;&#24615;&#20197;&#21450;&#21463;&#38480;&#30340;&#35748;&#30693;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#22522;&#20110;&#33041;&#21551;&#21457;&#35745;&#31639;&#21407;&#29702;&#30340;&#31532;&#19977;&#20195;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent decades, Industrial Fault Diagnosis (IFD) has emerged as a crucial discipline concerned with detecting and gathering vital information about industrial equipment's health condition, thereby facilitating the identification of failure types and severities. The pursuit of precise and effective fault recognition has garnered substantial attention, culminating in a focus on automating equipment monitoring to preclude safety accidents and reduce reliance on human labor. The advent of artificial neural networks (ANNs) has been instrumental in augmenting intelligent IFD algorithms, particularly in the context of big data. Despite these advancements, ANNs, being a simplified biomimetic neural network model, exhibit inherent limitations such as resource and data dependencies and restricted cognitive capabilities. To address these limitations, the third-generation Spiking Neural Network (SNN), founded on principles of Brain-inspired computing, has surfaced as a promising alternative. Th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#22522;&#20110;AI/ML&#30340;5G&#31995;&#32479;&#20869;&#30340;&#30452;&#25509;&#23450;&#20301;&#36827;&#34892;&#20102;&#20840;&#38754;&#22238;&#39038;&#65292;&#37325;&#28857;&#35752;&#35770;&#20102;&#22312;&#20256;&#32479;&#26041;&#27861;&#19981;&#36866;&#29992;&#30340;&#25361;&#25112;&#22330;&#26223;&#21644;&#26465;&#20214;&#19979;&#30340;&#28508;&#21147;&#12290;&#30740;&#31350;&#21457;&#29616;&#20102;&#37325;&#35201;&#30340;&#27169;&#25311;&#32467;&#26524;&#21644;&#35266;&#23519;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#27979;&#37327;&#25253;&#21578;&#12289;&#25968;&#25454;&#25910;&#38598;&#21644;&#27169;&#22411;&#31649;&#29702;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20419;&#36827;&#20102;&#30452;&#25509;&#23450;&#20301;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2401.02427</link><description>&lt;p&gt;
&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;/&#26426;&#22120;&#23398;&#20064;&#30340;5G&#23450;&#20301;&#25216;&#26415;&#30340;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;
5G Positioning Advancements with AI/ML. (arXiv:2401.02427v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02427
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22522;&#20110;AI/ML&#30340;5G&#31995;&#32479;&#20869;&#30340;&#30452;&#25509;&#23450;&#20301;&#36827;&#34892;&#20102;&#20840;&#38754;&#22238;&#39038;&#65292;&#37325;&#28857;&#35752;&#35770;&#20102;&#22312;&#20256;&#32479;&#26041;&#27861;&#19981;&#36866;&#29992;&#30340;&#25361;&#25112;&#22330;&#26223;&#21644;&#26465;&#20214;&#19979;&#30340;&#28508;&#21147;&#12290;&#30740;&#31350;&#21457;&#29616;&#20102;&#37325;&#35201;&#30340;&#27169;&#25311;&#32467;&#26524;&#21644;&#35266;&#23519;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#27979;&#37327;&#25253;&#21578;&#12289;&#25968;&#25454;&#25910;&#38598;&#21644;&#27169;&#22411;&#31649;&#29702;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20419;&#36827;&#20102;&#30452;&#25509;&#23450;&#20301;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;/&#26426;&#22120;&#23398;&#20064;&#30340;5G&#31995;&#32479;&#20869;&#30340;&#30452;&#25509;&#23450;&#20301;&#65292;&#37325;&#28857;&#35752;&#35770;&#22312;&#20256;&#32479;&#26041;&#27861;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#30340;&#25361;&#25112;&#22330;&#26223;&#21644;&#26465;&#20214;&#19979;&#30340;&#28508;&#21147;&#12290;&#22312;&#25216;&#26415;&#25253;&#21578;TR38.843&#30340;&#27934;&#35265;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#30528;&#37325;&#30740;&#31350;&#20102;&#29983;&#21629;&#21608;&#26399;&#31649;&#29702;&#65288;LCM&#65289;&#19982;&#30452;&#25509;&#23450;&#20301;&#36807;&#31243;&#30456;&#20851;&#30340;&#26041;&#38754;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;&#26469;&#33258;&#25253;&#21578;&#20013;&#23545;&#21508;&#31181;&#25361;&#25112;&#26465;&#20214;&#19979;&#30340;&#30452;&#25509;&#23450;&#20301;&#30340;&#37325;&#35201;&#27169;&#25311;&#32467;&#26524;&#21644;&#20851;&#38190;&#35266;&#23519;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#35299;&#20915;&#27979;&#37327;&#25253;&#21578;&#12289;&#25968;&#25454;&#25910;&#38598;&#21644;&#27169;&#22411;&#31649;&#29702;&#30340;&#36873;&#25321;&#24615;&#35299;&#20915;&#26041;&#26696;&#65292;&#24378;&#35843;&#23427;&#20204;&#23545;&#25512;&#36827;&#30452;&#25509;&#23450;&#20301;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper provides a comprehensive review of AI/ML-based direct positioning within 5G systems, focusing on its potential in challenging scenarios and conditions where conventional methods often fall short. Building upon the insights from the technical report TR38.843, we examine the Life Cycle Management (LCM) with a focus on to the aspects associated direct positioning process. We highlight significant simulation results and key observations from the report on the direct positioning under the various challenging conditions. Additionally, we discuss selected solutions that address measurement reporting, data collection, and model management, emphasizing their importance for advancing direct positioning.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#26080;&#20154;&#26426;&#30340;&#36712;&#36857;&#35268;&#21010;&#65292;&#26368;&#23567;&#21270;&#20102;&#22312;&#29289;&#32852;&#32593;&#32593;&#32476;&#20013;&#25910;&#38598;&#30340;&#25968;&#25454;&#30340;&#24635;&#24180;&#40836;&#65292;&#20174;&#32780;&#20445;&#25345;&#20102;&#25968;&#25454;&#30340;&#26032;&#40092;&#24230;&#12290;</title><link>http://arxiv.org/abs/2401.02425</link><description>&lt;p&gt;
&#26080;&#20154;&#26426;&#36741;&#21161;&#29289;&#32852;&#32593;&#32593;&#32476;&#20013;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#26368;&#23567;AoI&#25968;&#25454;&#25910;&#38598;&#30340;&#26080;&#20154;&#26426;&#36712;&#36857;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
UAV Trajectory Planning for AoI-Minimal Data Collection in UAV-Aided IoT Networks by Transformer. (arXiv:2401.02425v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02425
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#26080;&#20154;&#26426;&#30340;&#36712;&#36857;&#35268;&#21010;&#65292;&#26368;&#23567;&#21270;&#20102;&#22312;&#29289;&#32852;&#32593;&#32593;&#32476;&#20013;&#25910;&#38598;&#30340;&#25968;&#25454;&#30340;&#24635;&#24180;&#40836;&#65292;&#20174;&#32780;&#20445;&#25345;&#20102;&#25968;&#25454;&#30340;&#26032;&#40092;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29289;&#32852;&#32593;&#65288;IoT&#65289;&#32593;&#32476;&#20013;&#65292;&#20445;&#25345;&#25968;&#25454;&#25910;&#38598;&#30340;&#26032;&#40092;&#24230;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#32771;&#34385;&#21040;&#20449;&#24687;&#30340;&#24180;&#40836;&#65288;AoI&#65289;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26080;&#20154;&#26426;&#65288;UAV&#65289;&#30340;&#36712;&#36857;&#35268;&#21010;&#38382;&#39064;&#65292;&#35813;&#26080;&#20154;&#26426;&#29992;&#20110;&#36741;&#21161;&#22522;&#20110;&#38598;&#32676;&#30340;&#29289;&#32852;&#32593;&#32593;&#32476;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#20248;&#21270;&#36873;&#25321;&#24748;&#20572;&#28857;&#21644;&#35775;&#38382;&#36825;&#20123;&#28857;&#30340;&#39034;&#24207;&#65292;&#20197;&#26368;&#23567;&#21270;UAV&#20174;&#22320;&#38754;IoT&#32593;&#32476;&#25910;&#38598;&#30340;&#25968;&#25454;&#30340;&#24635;AoI&#12290;&#25105;&#20204;&#21033;&#29992;&#20102;&#26368;&#20808;&#36827;&#30340;&#36716;&#25442;&#22120;&#21644;&#21152;&#26435;A*&#31639;&#27861;&#26469;&#35774;&#35745;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25972;&#20010;UAV-IoT&#31995;&#32479;&#34987;&#36755;&#20837;&#21040;&#25152;&#25552;&#20986;&#31639;&#27861;&#30340;&#32534;&#30721;&#22120;&#32593;&#32476;&#20013;&#65292;&#31639;&#27861;&#30340;&#35299;&#30721;&#22120;&#32593;&#32476;&#36755;&#20986;&#20102;&#35775;&#38382;&#22320;&#38754;&#38598;&#32676;&#30340;&#39034;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
Maintaining freshness of data collection in Internet-of-Things (IoT) networks has attracted increasing attention. By taking into account age-of-information (AoI), we investigate the trajectory planning problem of an unmanned aerial vehicle (UAV) that is used to aid a cluster-based IoT network. An optimization problem is formulated to minimize the total AoI of the collected data by the UAV from the ground IoT network. Since the total AoI of the IoT network depends on the flight time of the UAV and the data collection time at hovering points, we jointly optimize the selection of hovering points and the visiting order to these points. We exploit the state-of-the-art transformer and the weighted A*, which is a path search algorithm, to design a machine learning algorithm to solve the formulated problem. The whole UAV-IoT system is fed into the encoder network of the proposed algorithm, and the algorithm's decoder network outputs the visiting order to ground clusters. Then, the weighted A* 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#21033;&#29992;EuroSAT&#21644;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#23545;&#22303;&#22320;&#21033;&#29992;&#21644;&#22303;&#22320;&#35206;&#30422;&#36827;&#34892;&#26144;&#23556;&#65292;&#36890;&#36807;&#20351;&#29992;&#39068;&#33394;&#27874;&#27573;&#36827;&#34892;&#24494;&#35843;&#65292;&#21462;&#24471;&#20102;99.19%&#30340;&#31934;&#30830;&#24230;&#65292;&#26377;&#21161;&#20110;&#21046;&#23450;&#20445;&#25252;&#21644;&#22478;&#24066;&#35268;&#21010;&#25919;&#31574;&#12290;</title><link>http://arxiv.org/abs/2401.02424</link><description>&lt;p&gt;
&#20351;&#29992;EuroSAT&#21644;&#36801;&#31227;&#23398;&#20064;&#36827;&#34892;&#22303;&#22320;&#21033;&#29992;&#21644;&#22303;&#22320;&#35206;&#30422;&#65288;LULC&#65289;&#30340;&#26144;&#23556;
&lt;/p&gt;
&lt;p&gt;
Mapping of Land Use and Land Cover (LULC) using EuroSAT and Transfer Learning. (arXiv:2401.02424v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02424
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#21033;&#29992;EuroSAT&#21644;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#23545;&#22303;&#22320;&#21033;&#29992;&#21644;&#22303;&#22320;&#35206;&#30422;&#36827;&#34892;&#26144;&#23556;&#65292;&#36890;&#36807;&#20351;&#29992;&#39068;&#33394;&#27874;&#27573;&#36827;&#34892;&#24494;&#35843;&#65292;&#21462;&#24471;&#20102;99.19%&#30340;&#31934;&#30830;&#24230;&#65292;&#26377;&#21161;&#20110;&#21046;&#23450;&#20445;&#25252;&#21644;&#22478;&#24066;&#35268;&#21010;&#25919;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20840;&#29699;&#20154;&#21475;&#30340;&#19981;&#26029;&#22686;&#38271;&#65292;&#23545;&#33258;&#28982;&#36164;&#28304;&#30340;&#38656;&#27714;&#20063;&#22312;&#22686;&#21152;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#20154;&#31867;&#27963;&#21160;&#21344;&#28201;&#23460;&#27668;&#20307;&#25490;&#25918;&#30340;23%&#12290;&#22909;&#28040;&#24687;&#26159;&#65292;&#36965;&#24863;&#25216;&#26415;&#24050;&#32463;&#25104;&#20026;&#31649;&#29702;&#25105;&#20204;&#29615;&#22659;&#30340;&#26377;&#20215;&#20540;&#24037;&#20855;&#12290;&#36825;&#20123;&#25216;&#26415;&#20351;&#25105;&#20204;&#33021;&#22815;&#30417;&#27979;&#22303;&#22320;&#21033;&#29992;&#65292;&#35268;&#21010;&#22478;&#24066;&#21306;&#22495;&#65292;&#24182;&#25512;&#21160;&#20892;&#19994;&#12289;&#27668;&#20505;&#21464;&#21270;&#32531;&#35299;&#12289;&#28798;&#23475;&#24674;&#22797;&#21644;&#29615;&#22659;&#30417;&#27979;&#31561;&#39046;&#22495;&#30340;&#36827;&#27493;&#12290;&#36817;&#24180;&#26469;&#65292;&#20154;&#24037;&#26234;&#33021;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#22320;&#29699;&#35266;&#27979;&#25968;&#25454;&#30340;&#36827;&#23637;&#20351;&#22303;&#22320;&#21033;&#29992;&#26144;&#23556;&#30340;&#20934;&#30830;&#24615;&#36798;&#21040;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#27700;&#24179;&#12290;&#36890;&#36807;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#21644;&#29992;RGB&#27874;&#27573;&#24494;&#35843;&#65292;&#25105;&#20204;&#22312;&#22303;&#22320;&#21033;&#29992;&#20998;&#26512;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;99.19%&#30340;&#20934;&#30830;&#24615;&#12290;&#36825;&#26679;&#30340;&#21457;&#29616;&#21487;&#20197;&#29992;&#20110;&#21046;&#23450;&#20445;&#25252;&#21644;&#22478;&#24066;&#35268;&#21010;&#25919;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the global population continues to expand, the demand for natural resources increases. Unfortunately, human activities account for 23% of greenhouse gas emissions. On a positive note, remote sensing technologies have emerged as a valuable tool in managing our environment. These technologies allow us to monitor land use, plan urban areas, and drive advancements in areas such as agriculture, climate change mitigation, disaster recovery, and environmental monitoring. Recent advances in AI, computer vision, and earth observation data have enabled unprecedented accuracy in land use mapping. By using transfer learning and fine-tuning with RGB bands, we achieved an impressive 99.19% accuracy in land use analysis. Such findings can be used to inform conservation and urban planning policies.
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;&#21548;&#35273;&#26426;&#22120;&#26234;&#33021;&#65288;NEURO-AMI&#65289;&#26159;&#19968;&#31181;&#20174;&#20154;&#31867;&#22823;&#33041;&#20013;&#33719;&#21462;&#28789;&#24863;&#30340;&#20154;&#24037;&#31070;&#32463;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#65292;&#22312;&#36719;&#35745;&#31639;&#39046;&#22495;&#26377;&#30528;&#24191;&#38420;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#28385;&#36275;&#29992;&#25143;&#38656;&#27714;&#65292;&#20173;&#28982;&#38656;&#35201;&#25913;&#36827;&#20854;&#21487;&#25215;&#21463;&#24615;&#12289;&#22797;&#26434;&#24615;&#21644;&#25968;&#25454;&#23398;&#20064;&#22823;&#23567;&#35201;&#27714;&#31561;&#26041;&#38754;&#30340;&#20248;&#21270;&#25514;&#26045;&#12290;</title><link>http://arxiv.org/abs/2401.02421</link><description>&lt;p&gt;
&#31070;&#32463;&#21548;&#35273;&#26426;&#22120;&#26234;&#33021;&#65288;NEURO-AMI&#65289;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Neuronal Auditory Machine Intelligence (NEURO-AMI) In Perspective. (arXiv:2401.02421v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02421
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#21548;&#35273;&#26426;&#22120;&#26234;&#33021;&#65288;NEURO-AMI&#65289;&#26159;&#19968;&#31181;&#20174;&#20154;&#31867;&#22823;&#33041;&#20013;&#33719;&#21462;&#28789;&#24863;&#30340;&#20154;&#24037;&#31070;&#32463;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#65292;&#22312;&#36719;&#35745;&#31639;&#39046;&#22495;&#26377;&#30528;&#24191;&#38420;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#28385;&#36275;&#29992;&#25143;&#38656;&#27714;&#65292;&#20173;&#28982;&#38656;&#35201;&#25913;&#36827;&#20854;&#21487;&#25215;&#21463;&#24615;&#12289;&#22797;&#26434;&#24615;&#21644;&#25968;&#25454;&#23398;&#20064;&#22823;&#23567;&#35201;&#27714;&#31561;&#26041;&#38754;&#30340;&#20248;&#21270;&#25514;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36719;&#35745;&#31639;&#30340;&#26368;&#26032;&#21457;&#23637;&#20013;&#65292;&#19981;&#33021;&#19981;&#25552;&#21040;&#20174;&#30495;&#23454;&#30340;&#22823;&#33041;&#30382;&#23618;&#32452;&#32455;&#25110;&#20154;&#31867;&#22823;&#33041;&#20013;&#21457;&#29983;&#30340;&#36807;&#31243;&#20013;&#27762;&#21462;&#28789;&#24863;&#30340;&#20154;&#24037;&#31070;&#32463;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#36129;&#29486;&#12290;&#36825;&#20123;&#31070;&#32463;&#31995;&#32479;&#30340;&#26222;&#36941;&#36924;&#36817;&#24615;&#24050;&#32463;&#23548;&#33268;&#20102;&#20854;&#24191;&#27867;&#20351;&#29992;&#65292;&#36825;&#31181;&#36827;&#21270;&#25216;&#26415;&#30340;&#26032;&#21457;&#23637;&#34920;&#26126;&#65292;&#22312;&#36719;&#35745;&#31639;&#39046;&#22495;&#20013;&#65292;&#36825;&#31181;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#26377;&#30528;&#20809;&#26126;&#30340;&#26410;&#26469;&#12290;&#23454;&#38469;&#19978;&#65292;&#22823;&#35268;&#27169;&#21644;&#38750;&#24120;&#28145;&#23618;&#30340;&#20154;&#24037;&#31070;&#32463;&#31995;&#32479;&#32593;&#32476;&#30340;&#27867;&#28389;&#20197;&#21450;&#30456;&#24212;&#30340;&#31070;&#32463;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#22686;&#24378;&#21644;&#21457;&#23637;&#65292;&#23545;&#20110;&#28145;&#24230;&#23398;&#20064;&#29616;&#20195;&#39046;&#22495;&#30340;&#21457;&#23637;&#20570;&#20986;&#20102;&#24040;&#22823;&#36129;&#29486;&#65292;&#21487;&#20197;&#22312;Lecun&#12289;Bengio&#21644;Hinton&#30340;&#30740;&#31350;&#25104;&#26524;&#20013;&#25214;&#21040;&#12290;&#28982;&#32780;&#65292;&#32456;&#31471;&#29992;&#25143;&#20215;&#26684;&#21487;&#25215;&#21463;&#24615;&#12289;&#38477;&#20302;&#22797;&#26434;&#24615;&#21644;&#38477;&#20302;&#25968;&#25454;&#23398;&#20064;&#22823;&#23567;&#35201;&#27714;&#31561;&#20851;&#38190;&#38656;&#27714;&#24847;&#21619;&#30528;&#65292;&#20173;&#28982;&#38656;&#35201;&#28385;&#36275;&#21477;&#27861;&#35201;&#27714;&#30340;&#20248;&#21270;&#25514;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent developments in soft computing cannot be complete without noting the contributions of artificial neural machine learning systems that draw inspiration from real cortical tissue or processes that occur in human brain. The universal approximability of such neural systems has led to its wide spread use, and novel developments in this evolving technology has shown that there is a bright future for such Artificial Intelligent (AI) techniques in the soft computing field. Indeed, the proliferation of large and very deep networks of artificial neural systems and the corresponding enhancement and development of neural machine learning algorithms have contributed immensely to the development of the modern field of Deep Learning as may be found in the well documented research works of Lecun, Bengio and Hinton. However, the key requirements of end user affordability in addition to reduced complexity and reduced data learning size requirement means there still remains a need for the synt
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#27010;&#24565;&#12304;&#30456;&#23545;&#21019;&#36896;&#21147;&#12305;&#65292;&#36890;&#36807;&#23558;&#28966;&#28857;&#36716;&#21521;AI&#26159;&#21542;&#33021;&#22815;&#19982;&#20154;&#31867;&#20855;&#22791;&#30456;&#21516;&#30340;&#21019;&#36896;&#33021;&#21147;&#65292;&#23454;&#29616;&#23545;&#21019;&#36896;&#21147;&#30340;&#32479;&#35745;&#37327;&#21270;&#35780;&#20272;&#21644;&#30452;&#25509;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2401.01623</link><description>&lt;p&gt;
AI&#26159;&#21542;&#33021;&#20687;&#20154;&#31867;&#19968;&#26679;&#20855;&#22791;&#21019;&#36896;&#21147;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can AI Be as Creative as Humans?. (arXiv:2401.01623v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01623
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#27010;&#24565;&#12304;&#30456;&#23545;&#21019;&#36896;&#21147;&#12305;&#65292;&#36890;&#36807;&#23558;&#28966;&#28857;&#36716;&#21521;AI&#26159;&#21542;&#33021;&#22815;&#19982;&#20154;&#31867;&#20855;&#22791;&#30456;&#21516;&#30340;&#21019;&#36896;&#33021;&#21147;&#65292;&#23454;&#29616;&#23545;&#21019;&#36896;&#21147;&#30340;&#32479;&#35745;&#37327;&#21270;&#35780;&#20272;&#21644;&#30452;&#25509;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21019;&#36896;&#21147;&#26159;&#31038;&#20250;&#36827;&#27493;&#21644;&#21019;&#26032;&#30340;&#22522;&#30707;&#65292;&#20294;&#20854;&#35780;&#20272;&#20173;&#28982;&#26159;&#19968;&#20010;&#22797;&#26434;&#19988;&#20027;&#35266;&#30340;&#20219;&#21153;&#12290;&#38543;&#30528;&#20808;&#36827;&#30340;&#29983;&#25104;&#22411;AI&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#33021;&#22815;&#23436;&#25104;&#26366;&#32463;&#21482;&#23646;&#20110;&#20154;&#31867;&#21019;&#36896;&#21147;&#30340;&#20219;&#21153;&#65292;&#25506;&#32034;AI&#30340;&#21019;&#36896;&#28508;&#21147;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#30830;&#20445;&#20854;&#36127;&#36131;&#20219;&#30340;&#21457;&#23637;&#21644;&#24212;&#29992;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#21517;&#20026;&#8220;&#30456;&#23545;&#21019;&#36896;&#21147;&#8221;&#30340;&#26032;&#27010;&#24565;&#26469;&#35299;&#20915;&#23450;&#20041;&#21644;&#35780;&#20272;&#21019;&#36896;&#21147;&#30340;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#19981;&#20877;&#35797;&#22270;&#23545;&#21019;&#36896;&#21147;&#36827;&#34892;&#26222;&#36941;&#23450;&#20041;&#65292;&#32780;&#26159;&#23558;&#28966;&#28857;&#36716;&#21521;AI&#26159;&#21542;&#33021;&#22815;&#19982;&#19968;&#20301;&#20551;&#35774;&#30340;&#20154;&#31867;&#20855;&#22791;&#30456;&#21516;&#30340;&#21019;&#36896;&#33021;&#21147;&#12290;&#36825;&#31181;&#35266;&#28857;&#20511;&#37492;&#20102;&#22270;&#28789;&#27979;&#35797;&#30340;&#24605;&#24819;&#65292;&#24182;&#25193;&#23637;&#20854;&#33539;&#22260;&#20197;&#35299;&#20915;&#35780;&#20272;&#21019;&#36896;&#21147;&#20013;&#25152;&#22266;&#26377;&#30340;&#25361;&#25112;&#21644;&#20027;&#35266;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#36716;&#21464;&#20351;&#24471;&#23545;AI&#21019;&#36896;&#21147;&#30340;&#32479;&#35745;&#37327;&#21270;&#35780;&#20272;&#25104;&#20026;&#21487;&#33021;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#32479;&#35745;&#21019;&#36896;&#21147;&#12290;&#36825;&#31181;&#26041;&#27861;&#20801;&#35768;&#30452;&#25509;&#27604;&#36739;AI&#19982;&#29305;&#23450;&#20154;&#31867;&#30340;&#21019;&#36896;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Creativity serves as a cornerstone for societal progress and innovation, but its assessment remains a complex and often subjective endeavor. With the rise of advanced generative AI models capable of tasks once reserved for human creativity, the study of AI's creative potential becomes imperative for its responsible development and application. This paper addresses the complexities in defining and evaluating creativity by introducing a new concept called Relative Creativity. Instead of trying to define creativity universally, we shift the focus to whether AI can match the creative abilities of a hypothetical human. This perspective draws inspiration from the Turing Test, expanding upon it to address the challenges and subjectivities inherent in evaluating creativity. This methodological shift facilitates a statistically quantifiable evaluation of AI's creativity, which we term Statistical Creativity. This approach allows for direct comparisons of AI's creative abilities with those of sp
&lt;/p&gt;</description></item><item><title>&#24352;&#37327;&#32593;&#32476;&#21487;&#20197;&#24110;&#21161;&#21457;&#23637;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#20016;&#23500;&#30340;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#32593;&#32476;&#23433;&#20840;&#20013;&#65292;&#25105;&#20204;&#30340;&#26080;&#30417;&#30563;&#32858;&#31867;&#31639;&#27861;&#22522;&#20110;&#30697;&#38453;&#20056;&#31215;&#29366;&#24577;&#65292;&#22312;&#24615;&#33021;&#19978;&#19982;&#20256;&#32479;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30456;&#23218;&#32654;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#33021;&#25552;&#21462;&#29305;&#24449;&#27010;&#29575;&#12289;&#29109;&#21644;&#20114;&#20449;&#24687;&#65292;&#25552;&#20379;&#20102;&#20998;&#31867;&#24322;&#24120;&#30340;&#24341;&#20154;&#20837;&#32988;&#30340;&#21465;&#36848;&#65292;&#24182;&#23454;&#29616;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2401.00867</link><description>&lt;p&gt;
&#24352;&#37327;&#32593;&#32476;&#22312;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#20013;&#22312;&#32593;&#32476;&#23433;&#20840;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Tensor Networks for Explainable Machine Learning in Cybersecurity. (arXiv:2401.00867v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00867
&lt;/p&gt;
&lt;p&gt;
&#24352;&#37327;&#32593;&#32476;&#21487;&#20197;&#24110;&#21161;&#21457;&#23637;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#20016;&#23500;&#30340;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#32593;&#32476;&#23433;&#20840;&#20013;&#65292;&#25105;&#20204;&#30340;&#26080;&#30417;&#30563;&#32858;&#31867;&#31639;&#27861;&#22522;&#20110;&#30697;&#38453;&#20056;&#31215;&#29366;&#24577;&#65292;&#22312;&#24615;&#33021;&#19978;&#19982;&#20256;&#32479;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30456;&#23218;&#32654;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#33021;&#25552;&#21462;&#29305;&#24449;&#27010;&#29575;&#12289;&#29109;&#21644;&#20114;&#20449;&#24687;&#65292;&#25552;&#20379;&#20102;&#20998;&#31867;&#24322;&#24120;&#30340;&#24341;&#20154;&#20837;&#32988;&#30340;&#21465;&#36848;&#65292;&#24182;&#23454;&#29616;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#24352;&#37327;&#32593;&#32476;&#22914;&#20309;&#24110;&#21161;&#21457;&#23637;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22522;&#20110;&#30697;&#38453;&#20056;&#31215;&#29366;&#24577;&#65288;MPS&#65289;&#24320;&#21457;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#32858;&#31867;&#31639;&#27861;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#23454;&#38469;&#20351;&#29992;&#26696;&#20363;&#20013;&#30340;&#23545;&#25163;&#29983;&#25104;&#30340;&#23041;&#32961;&#24773;&#25253;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#35777;&#26126;&#65292;MPS&#22312;&#24615;&#33021;&#26041;&#38754;&#21487;&#20197;&#19982;&#20256;&#32479;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22914;&#33258;&#32534;&#30721;&#22120;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30456;&#23218;&#32654;&#65292;&#21516;&#26102;&#25552;&#20379;&#26356;&#20016;&#23500;&#30340;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#33258;&#28982;&#22320;&#20419;&#36827;&#20102;&#29305;&#24449;&#27010;&#29575;&#12289;&#20911;&#183;&#35834;&#20234;&#26364;&#29109;&#21644;&#20114;&#20449;&#24687;&#30340;&#25552;&#21462;&#65292;&#20026;&#24322;&#24120;&#20998;&#31867;&#25552;&#20379;&#20102;&#24341;&#20154;&#20837;&#32988;&#30340;&#21465;&#36848;&#65292;&#24182;&#20419;&#36827;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#27700;&#24179;&#65292;&#36825;&#23545;&#20110;&#29702;&#35299;&#20154;&#24037;&#26234;&#33021;&#20915;&#31574;&#30340;&#22522;&#26412;&#21407;&#29702;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we show how tensor networks help in developing explainability of machine learning algorithms. Specifically, we develop an unsupervised clustering algorithm based on Matrix Product States (MPS) and apply it in the context of a real use-case of adversary-generated threat intelligence. Our investigation proves that MPS rival traditional deep learning models such as autoencoders and GANs in terms of performance, while providing much richer model interpretability. Our approach naturally facilitates the extraction of feature-wise probabilities, Von Neumann Entropy, and mutual information, offering a compelling narrative for classification of anomalies and fostering an unprecedented level of transparency and interpretability, something fundamental to understand the rationale behind artificial intelligence decisions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35748;&#20026;&#23558;&#22823;&#35268;&#27169;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#20013;&#30340;&#30693;&#35782;&#19982;&#20915;&#31574;&#38382;&#39064;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#35299;&#20915;&#20915;&#31574;&#20013;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#27867;&#21270;&#24615;&#38382;&#39064;&#12290;&#36890;&#36807;&#25552;&#20986;&#20808;&#39044;&#35757;&#32451;&#20877;&#33258;&#36866;&#24212;&#30340;&#27969;&#31243;&#65292;&#24182;&#35843;&#30740;&#20102;&#20915;&#31574;&#39044;&#35757;&#32451;&#21644;&#19979;&#28216;&#25512;&#29702;&#20013;&#30340;&#25968;&#25454;&#25910;&#38598;&#12289;&#39044;&#35757;&#32451;&#30446;&#26631;&#21644;&#33258;&#36866;&#24212;&#31574;&#30053;&#30340;&#26368;&#26032;&#30740;&#31350;&#12290;&#26368;&#21518;&#65292;&#30830;&#23450;&#20102;&#21457;&#23637;&#20915;&#31574;&#22522;&#30784;&#27169;&#22411;&#30340;&#20851;&#38190;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2401.00031</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#29992;&#20110;&#20915;&#31574;&#22522;&#30784;&#27169;&#22411;&#65306;&#24418;&#24335;&#21270;&#12289;&#27969;&#31243;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Self-supervised Pretraining for Decision Foundation Model: Formulation, Pipeline and Challenges. (arXiv:2401.00031v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00031
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35748;&#20026;&#23558;&#22823;&#35268;&#27169;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#20013;&#30340;&#30693;&#35782;&#19982;&#20915;&#31574;&#38382;&#39064;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#35299;&#20915;&#20915;&#31574;&#20013;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#27867;&#21270;&#24615;&#38382;&#39064;&#12290;&#36890;&#36807;&#25552;&#20986;&#20808;&#39044;&#35757;&#32451;&#20877;&#33258;&#36866;&#24212;&#30340;&#27969;&#31243;&#65292;&#24182;&#35843;&#30740;&#20102;&#20915;&#31574;&#39044;&#35757;&#32451;&#21644;&#19979;&#28216;&#25512;&#29702;&#20013;&#30340;&#25968;&#25454;&#25910;&#38598;&#12289;&#39044;&#35757;&#32451;&#30446;&#26631;&#21644;&#33258;&#36866;&#24212;&#31574;&#30053;&#30340;&#26368;&#26032;&#30740;&#31350;&#12290;&#26368;&#21518;&#65292;&#30830;&#23450;&#20102;&#21457;&#23637;&#20915;&#31574;&#22522;&#30784;&#27169;&#22411;&#30340;&#20851;&#38190;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#26159;&#19968;&#20010;&#21160;&#24577;&#36807;&#31243;&#65292;&#38656;&#35201;&#24863;&#30693;&#12289;&#35760;&#24518;&#21644;&#25512;&#29702;&#26469;&#36827;&#34892;&#36873;&#25321;&#24182;&#25214;&#21040;&#26368;&#20248;&#31574;&#30053;&#12290;&#20256;&#32479;&#30340;&#20915;&#31574;&#26041;&#27861;&#22312;&#26679;&#26412;&#25928;&#29575;&#21644;&#27867;&#21270;&#24615;&#19978;&#23384;&#22312;&#38382;&#39064;&#65292;&#32780;&#22823;&#35268;&#27169;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#24050;&#32463;&#20351;&#24471;&#35821;&#35328;&#21644;&#35270;&#35273;&#39046;&#22495;&#30340;&#24555;&#36895;&#36866;&#24212;&#25104;&#20026;&#21487;&#33021;&#65292;&#36890;&#36807;&#24494;&#35843;&#25110;&#23569;&#26679;&#26412;&#23398;&#20064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#20174;&#22823;&#35268;&#27169;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#20013;&#33719;&#21462;&#30340;&#30693;&#35782;&#19982;&#19979;&#28216;&#20915;&#31574;&#38382;&#39064;&#34701;&#21512;&#36215;&#26469;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20808;&#39044;&#35757;&#32451;&#20877;&#33258;&#36866;&#24212;&#30340;&#27969;&#31243;&#65292;&#24182;&#35843;&#30740;&#20102;&#20915;&#31574;&#39044;&#35757;&#32451;&#21644;&#19979;&#28216;&#25512;&#29702;&#20013;&#30340;&#25968;&#25454;&#25910;&#38598;&#12289;&#39044;&#35757;&#32451;&#30446;&#26631;&#21644;&#33258;&#36866;&#24212;&#31574;&#30053;&#30340;&#26368;&#26032;&#30740;&#31350;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#22312;&#36890;&#29992;&#28789;&#27963;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;&#24110;&#21161;&#19979;&#65292;&#21457;&#23637;&#20915;&#31574;&#22522;&#30784;&#27169;&#22411;&#38754;&#20020;&#30340;&#20851;&#38190;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decision-making is a dynamic process requiring perception, memory, and reasoning to make choices and find optimal policies. Traditional approaches to decision-making suffer from sample efficiency and generalization, while large-scale self-supervised pretraining has enabled fast adaptation with fine-tuning or few-shot learning in language and vision. We thus argue to integrate knowledge acquired from generic large-scale self-supervised pretraining into downstream decision-making problems. We propose Pretrain-Then-Adapt pipeline and survey recent work on data collection, pretraining objectives and adaptation strategies for decision-making pretraining and downstream inference. Finally, we identify critical challenges and future directions for developing decision foundation model with the help of generic and flexible self-supervised pretraining.
&lt;/p&gt;</description></item><item><title>FENet&#26159;&#19968;&#20010;&#22686;&#24378;&#32858;&#28966;&#32593;&#32476;&#29992;&#20110;&#31934;&#20934;&#36710;&#36947;&#26816;&#27979;&#65292;&#36890;&#36807;&#32858;&#28966;&#37319;&#26679;&#21644;&#37096;&#20998;&#35270;&#37326;&#35780;&#20272;&#31561;&#21019;&#26032;&#26041;&#27861;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#26816;&#27979;&#20934;&#30830;&#24615;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#26354;&#32447;&#21644;&#36828;&#36317;&#31163;&#36710;&#36947;&#65292;&#22312;&#23433;&#20840;&#24615;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2312.17163</link><description>&lt;p&gt;
FENet: &#22686;&#24378;&#32858;&#28966;&#32593;&#32476;&#29992;&#20110;&#36710;&#36947;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
FENet: Focusing Enhanced Network for Lane Detection. (arXiv:2312.17163v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.17163
&lt;/p&gt;
&lt;p&gt;
FENet&#26159;&#19968;&#20010;&#22686;&#24378;&#32858;&#28966;&#32593;&#32476;&#29992;&#20110;&#31934;&#20934;&#36710;&#36947;&#26816;&#27979;&#65292;&#36890;&#36807;&#32858;&#28966;&#37319;&#26679;&#21644;&#37096;&#20998;&#35270;&#37326;&#35780;&#20272;&#31561;&#21019;&#26032;&#26041;&#27861;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#26816;&#27979;&#20934;&#30830;&#24615;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#26354;&#32447;&#21644;&#36828;&#36317;&#31163;&#36710;&#36947;&#65292;&#22312;&#23433;&#20840;&#24615;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#20154;&#31867;&#39550;&#39542;&#27880;&#24847;&#21147;&#30340;&#21551;&#21457;&#65292;&#26412;&#30740;&#31350;&#39318;&#27425;&#24320;&#21457;&#20102;&#22686;&#24378;&#32858;&#28966;&#37319;&#26679;&#12289;&#37096;&#20998;&#35270;&#37326;&#35780;&#20272;&#12289;&#22686;&#24378;FPN&#26550;&#26500;&#21644;&#23450;&#21521;IoU&#25439;&#22833;&#30340;&#32593;&#32476;&#21019;&#26032;&#65292;&#35299;&#20915;&#20102;&#33258;&#21160;&#39550;&#39542;&#36710;&#36947;&#26816;&#27979;&#20013;&#30340;&#31934;&#20934;&#24615;&#38556;&#30861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#32858;&#28966;&#37319;&#26679;&#31574;&#30053;&#65292;&#24378;&#35843;&#36828;&#22788;&#37325;&#35201;&#32454;&#33410;&#65292;&#19982;&#22343;&#21248;&#26041;&#27861;&#30456;&#27604;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22522;&#20934;&#21644;&#23454;&#38469;&#26354;&#32447;/&#36828;&#36317;&#31163;&#36710;&#36947;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#65292;&#36825;&#23545;&#23433;&#20840;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;FENetV1&#36890;&#36807;&#27169;&#25311;&#39550;&#39542;&#21592;&#35270;&#35273;&#30340;&#36879;&#35270;&#24863;&#30693;&#19978;&#19979;&#25991;&#25913;&#36827;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20256;&#32479;&#24230;&#37327;&#24615;&#33021;&#65292;&#20294;FENetV2&#22312;&#25552;&#20986;&#30340;&#37096;&#20998;&#35270;&#37326;&#20998;&#26512;&#20013;&#35777;&#26126;&#26159;&#26368;&#21487;&#38752;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#29305;&#21035;&#25512;&#33616;V2&#29992;&#20110;&#23454;&#38469;&#36710;&#36947;&#23548;&#33322;&#65292;&#23613;&#31649;&#22312;&#26631;&#20934;&#30340;&#25972;&#24352;&#22270;&#20687;&#27979;&#37327;&#19978;&#26377;&#36731;&#24494;&#30340;&#38477;&#32423;&#12290;&#26410;&#26469;&#30340;&#26041;&#21521;&#21253;&#25324;&#25910;&#38598;&#23454;&#38469;&#36947;&#36335;&#25968;&#25454;&#21644;&#38598;&#25104;&#20114;&#34917;&#30340;&#21452;&#37325;&#26694;&#26550;&#65292;&#20197;&#36827;&#19968;&#27493;&#36890;&#36807;&#20154;&#31867;&#24863;&#30693;&#25351;&#23548;&#23454;&#29616;&#31361;&#30772;&#24615;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspired by human driving focus, this research pioneers networks augmented with Focusing Sampling, Partial Field of View Evaluation, Enhanced FPN architecture and Directional IoU Loss - targeted innovations addressing obstacles to precise lane detection for autonomous driving. Experiments demonstrate our Focusing Sampling strategy, emphasizing vital distant details unlike uniform approaches, significantly boosts both benchmark and practical curved/distant lane recognition accuracy essential for safety. While FENetV1 achieves state-of-the-art conventional metric performance via enhancements isolating perspective-aware contexts mimicking driver vision, FENetV2 proves most reliable on the proposed Partial Field analysis. Hence we specifically recommend V2 for practical lane navigation despite fractional degradation on standard entire-image measures. Future directions include collecting on-road data and integrating complementary dual frameworks to further breakthroughs guided by human perc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26377;&#38480;&#20869;&#23384;&#26465;&#20214;&#19979;&#39640;&#25928;&#36816;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#27169;&#22411;&#21442;&#25968;&#23384;&#20648;&#22312;&#38378;&#23384;&#20013;&#24182;&#25353;&#38656;&#20256;&#36755;&#21040;DRAM&#30340;&#26041;&#24335;&#26469;&#35299;&#20915;&#20869;&#23384;&#38480;&#21046;&#30340;&#25361;&#25112;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#26500;&#24314;&#25512;&#29702;&#25104;&#26412;&#27169;&#22411;&#24182;&#20248;&#21270;&#25968;&#25454;&#20256;&#36755;&#21644;&#35835;&#21462;&#26041;&#24335;&#65292;&#24341;&#20837;&#20102;&#31383;&#21475;&#21270;&#21644;&#34892;&#21015;&#32465;&#23450;&#20004;&#31181;&#20027;&#35201;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2312.11514</link><description>&lt;p&gt;
&#38378;&#23384;LLM&#65306;&#22312;&#26377;&#38480;&#20869;&#23384;&#19979;&#39640;&#25928;&#36816;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LLM in a flash: Efficient Large Language Model Inference with Limited Memory. (arXiv:2312.11514v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.11514
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26377;&#38480;&#20869;&#23384;&#26465;&#20214;&#19979;&#39640;&#25928;&#36816;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#27169;&#22411;&#21442;&#25968;&#23384;&#20648;&#22312;&#38378;&#23384;&#20013;&#24182;&#25353;&#38656;&#20256;&#36755;&#21040;DRAM&#30340;&#26041;&#24335;&#26469;&#35299;&#20915;&#20869;&#23384;&#38480;&#21046;&#30340;&#25361;&#25112;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#26500;&#24314;&#25512;&#29702;&#25104;&#26412;&#27169;&#22411;&#24182;&#20248;&#21270;&#25968;&#25454;&#20256;&#36755;&#21644;&#35835;&#21462;&#26041;&#24335;&#65292;&#24341;&#20837;&#20102;&#31383;&#21475;&#21270;&#21644;&#34892;&#21015;&#32465;&#23450;&#20004;&#31181;&#20027;&#35201;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#29616;&#20195;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24222;&#22823;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#38656;&#27714;&#24102;&#26469;&#20102;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20855;&#26377;&#26377;&#38480;DRAM&#23481;&#37327;&#30340;&#35774;&#22791;&#32780;&#35328;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#27169;&#22411;&#21442;&#25968;&#23384;&#20648;&#22312;&#38378;&#23384;&#20013;&#65292;&#24182;&#25353;&#38656;&#23558;&#20854;&#20256;&#36755;&#21040;DRAM&#30340;&#26041;&#24335;&#65292;&#35299;&#20915;&#20102;&#36229;&#36807;&#21487;&#29992;DRAM&#23481;&#37327;&#30340;LLM&#39640;&#25928;&#36816;&#34892;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#26500;&#24314;&#19968;&#20010;&#32771;&#34385;&#38378;&#23384;&#29305;&#24615;&#30340;&#25512;&#29702;&#25104;&#26412;&#27169;&#22411;&#65292;&#24341;&#23548;&#25105;&#20204;&#22312;&#20004;&#20010;&#20851;&#38190;&#39046;&#22495;&#36827;&#34892;&#20248;&#21270;&#65306;&#20943;&#23569;&#20174;&#38378;&#23384;&#20256;&#36755;&#30340;&#25968;&#25454;&#37327;&#65292;&#24182;&#20197;&#36739;&#22823;&#12289;&#26356;&#36830;&#32493;&#30340;&#22359;&#35835;&#21462;&#25968;&#25454;&#12290;&#22312;&#36825;&#20010;&#21463;&#30828;&#20214;&#21551;&#21457;&#30340;&#26694;&#26550;&#20869;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#20027;&#35201;&#25216;&#26415;&#12290;&#39318;&#20808;&#65292;&#8220;&#31383;&#21475;&#21270;&#8221;&#36890;&#36807;&#37325;&#22797;&#20351;&#29992;&#20043;&#21069;&#28608;&#27963;&#30340;&#31070;&#32463;&#20803;&#26469;&#31574;&#30053;&#24615;&#22320;&#20943;&#23569;&#25968;&#25454;&#20256;&#36755;&#65292;&#20854;&#27425;&#65292;&#8220;&#34892;&#21015;&#32465;&#23450;&#8221;&#36866;&#24212;&#20102;&#38378;&#23384;&#30340;&#39034;&#24207;&#25968;&#25454;&#35775;&#38382;&#29305;&#28857;&#65292;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are central to modern natural language processing, delivering exceptional performance in various tasks. However, their substantial computational and memory requirements present challenges, especially for devices with limited DRAM capacity. This paper tackles the challenge of efficiently running LLMs that exceed the available DRAM capacity by storing the model parameters in flash memory, but bringing them on demand to DRAM. Our method involves constructing an inference cost model that takes into account the characteristics of flash memory, guiding us to optimize in two critical areas: reducing the volume of data transferred from flash and reading data in larger, more contiguous chunks. Within this hardware-informed framework, we introduce two principal techniques. First, "windowing" strategically reduces data transfer by reusing previously activated neurons, and second, "row-column bundling", tailored to the sequential data access strengths of flash memory, 
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#35770;&#25991;&#35843;&#26597;&#20102;&#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#21253;&#25324;&#19977;&#20010;&#20027;&#35201;&#33539;&#24335;&#65306;Naive RAG&#12289;Advanced RAG&#21644;Modular RAG&#12290;RAG&#36890;&#36807;&#25972;&#21512;&#22806;&#37096;&#25968;&#25454;&#24211;&#30340;&#30693;&#35782;&#65292;&#22686;&#24378;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#20449;&#24230;&#65292;&#24182;&#23454;&#29616;&#20102;&#25345;&#32493;&#26356;&#26032;&#30693;&#35782;&#21644;&#25972;&#21512;&#39046;&#22495;&#29305;&#23450;&#20449;&#24687;&#30340;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2312.10997</link><description>&lt;p&gt;
&#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#19968;&#39033;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Retrieval-Augmented Generation for Large Language Models: A Survey. (arXiv:2312.10997v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.10997
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35770;&#25991;&#35843;&#26597;&#20102;&#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#21253;&#25324;&#19977;&#20010;&#20027;&#35201;&#33539;&#24335;&#65306;Naive RAG&#12289;Advanced RAG&#21644;Modular RAG&#12290;RAG&#36890;&#36807;&#25972;&#21512;&#22806;&#37096;&#25968;&#25454;&#24211;&#30340;&#30693;&#35782;&#65292;&#22686;&#24378;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#20449;&#24230;&#65292;&#24182;&#23454;&#29616;&#20102;&#25345;&#32493;&#26356;&#26032;&#30693;&#35782;&#21644;&#25972;&#21512;&#39046;&#22495;&#29305;&#23450;&#20449;&#24687;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#33021;&#21147;&#65292;&#20294;&#38754;&#20020;&#24187;&#35273;&#12289;&#36807;&#26102;&#30693;&#35782;&#21644;&#38750;&#36879;&#26126;&#12289;&#19981;&#21487;&#36861;&#28335;&#30340;&#25512;&#29702;&#36807;&#31243;&#31561;&#25361;&#25112;&#12290;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#25972;&#21512;&#26469;&#33258;&#22806;&#37096;&#25968;&#25454;&#24211;&#30340;&#30693;&#35782;&#65292;&#22686;&#24378;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#20449;&#24230;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#65292;&#24182;&#20801;&#35768;&#25345;&#32493;&#26356;&#26032;&#30693;&#35782;&#21644;&#25972;&#21512;&#39046;&#22495;&#29305;&#23450;&#20449;&#24687;&#12290;RAG&#23558;LLMs&#33258;&#36523;&#30340;&#30693;&#35782;&#19982;&#24222;&#22823;&#12289;&#21160;&#24577;&#30340;&#22806;&#37096;&#25968;&#25454;&#24211;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#21327;&#21516;&#25928;&#24212;&#12290;&#26412;&#32508;&#36848;&#35770;&#25991;&#35814;&#32454;&#32771;&#23519;&#20102;RAG&#33539;&#24335;&#30340;&#21457;&#23637;&#65292;&#21253;&#25324;Naive RAG&#12289;Advanced RAG&#21644;Modular RAG&#12290;&#23427;&#35814;&#32454;&#23457;&#35270;&#20102;RAG&#26694;&#26550;&#30340;&#19977;&#20010;&#22522;&#26412;&#35201;&#32032;&#65292;&#21253;&#25324;&#26816;&#32034;&#12289;&#29983;&#25104;&#21644;&#22686;&#24378;&#25216;&#26415;&#12290;&#26412;&#25991;&#24378;&#35843;&#20102;&#23884;&#20837;&#22312;RAG&#26694;&#26550;&#20013;&#30340;&#26368;&#26032;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) demonstrate significant capabilities but face challenges such as hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the models, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval , the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in e
&lt;/p&gt;</description></item><item><title>PromptBench&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32479;&#19968;&#24211;&#65292;&#21253;&#25324;&#20102;&#25552;&#31034;&#35821;&#26500;&#24314;&#12289;&#25552;&#31034;&#35821;&#24037;&#31243;&#12289;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#21152;&#36733;&#12289;&#23545;&#25239;&#24615;&#25552;&#31034;&#25915;&#20987;&#12289;&#21160;&#24577;&#35780;&#20272;&#21327;&#35758;&#21644;&#20998;&#26512;&#24037;&#20855;&#31561;&#32452;&#20214;&#65292;&#26088;&#22312;&#20419;&#36827;&#21407;&#21019;&#30740;&#31350;&#21644;&#21019;&#24314;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#12289;&#37096;&#32626;&#19979;&#28216;&#24212;&#29992;&#20197;&#21450;&#35774;&#35745;&#26032;&#30340;&#35780;&#20272;&#21327;&#35758;&#12290;</title><link>http://arxiv.org/abs/2312.07910</link><description>&lt;p&gt;
PromptBench&#65306;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32479;&#19968;&#24211;
&lt;/p&gt;
&lt;p&gt;
PromptBench: A Unified Library for Evaluation of Large Language Models. (arXiv:2312.07910v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.07910
&lt;/p&gt;
&lt;p&gt;
PromptBench&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32479;&#19968;&#24211;&#65292;&#21253;&#25324;&#20102;&#25552;&#31034;&#35821;&#26500;&#24314;&#12289;&#25552;&#31034;&#35821;&#24037;&#31243;&#12289;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#21152;&#36733;&#12289;&#23545;&#25239;&#24615;&#25552;&#31034;&#25915;&#20987;&#12289;&#21160;&#24577;&#35780;&#20272;&#21327;&#35758;&#21644;&#20998;&#26512;&#24037;&#20855;&#31561;&#32452;&#20214;&#65292;&#26088;&#22312;&#20419;&#36827;&#21407;&#21019;&#30740;&#31350;&#21644;&#21019;&#24314;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#12289;&#37096;&#32626;&#19979;&#28216;&#24212;&#29992;&#20197;&#21450;&#35774;&#35745;&#26032;&#30340;&#35780;&#20272;&#21327;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35780;&#20272;&#23545;&#20110;&#35780;&#20272;&#20854;&#24615;&#33021;&#21644;&#20943;&#36731;&#28508;&#22312;&#30340;&#23433;&#20840;&#39118;&#38505;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;PromptBench&#65292;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLMs&#30340;&#32479;&#19968;&#24211;&#12290;&#23427;&#30001;&#20960;&#20010;&#20851;&#38190;&#32452;&#20214;&#32452;&#25104;&#65292;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#36731;&#26494;&#20351;&#29992;&#21644;&#25193;&#23637;&#65306;&#25552;&#31034;&#35821;&#26500;&#24314;&#12289;&#25552;&#31034;&#35821;&#24037;&#31243;&#12289;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#21152;&#36733;&#12289;&#23545;&#25239;&#24615;&#25552;&#31034;&#25915;&#20987;&#12289;&#21160;&#24577;&#35780;&#20272;&#21327;&#35758;&#21644;&#20998;&#26512;&#24037;&#20855;&#12290;PromptBench&#26088;&#22312;&#25104;&#20026;&#19968;&#20010;&#24320;&#25918;&#12289;&#36890;&#29992;&#21644;&#28789;&#27963;&#30340;&#20195;&#30721;&#24211;&#65292;&#20197;&#20419;&#36827;&#21407;&#21019;&#30740;&#31350;&#65292;&#21019;&#24314;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#12289;&#37096;&#32626;&#19979;&#28216;&#24212;&#29992;&#21644;&#35774;&#35745;&#26032;&#30340;&#35780;&#20272;&#21327;&#35758;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/microsoft/promptbench&#19978;&#25214;&#21040;&#65292;&#24182;&#23558;&#25345;&#32493;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
The evaluation of large language models (LLMs) is crucial to assess their performance and mitigate potential security risks. In this paper, we introduce PromptBench, a unified library to evaluate LLMs. It consists of several key components that are easily used and extended by researchers: prompt construction, prompt engineering, dataset and model loading, adversarial prompt attack, dynamic evaluation protocols, and analysis tools. PromptBench is designed to be an open, general, and flexible codebase for research purposes that can facilitate original study in creating new benchmarks, deploying downstream applications, and designing new evaluation protocols. The code is available at: https://github.com/microsoft/promptbench and will be continuously supported.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102; KwaiAgents&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#29992;&#20449;&#24687;&#25628;&#32034;&#26234;&#33021;&#20307;&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#33021;&#22815;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35748;&#30693;&#26680;&#24515;&#65292;&#29702;&#35299;&#29992;&#25143;&#30340;&#26597;&#35810;&#65292;&#34892;&#20026;&#20934;&#21017;&#24182;&#21442;&#32771;&#22806;&#37096;&#25991;&#26723;&#65292;&#20197;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#30693;&#35782;&#21644;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2312.04889</link><description>&lt;p&gt;
KwaiAgents&#65306;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#29992;&#20449;&#24687;&#25628;&#32034;&#26234;&#33021;&#20307;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
KwaiAgents: Generalized Information-seeking Agent System with Large Language Models. (arXiv:2312.04889v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.04889
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102; KwaiAgents&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#29992;&#20449;&#24687;&#25628;&#32034;&#26234;&#33021;&#20307;&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#33021;&#22815;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35748;&#30693;&#26680;&#24515;&#65292;&#29702;&#35299;&#29992;&#25143;&#30340;&#26597;&#35810;&#65292;&#34892;&#20026;&#20934;&#21017;&#24182;&#21442;&#32771;&#22806;&#37096;&#25991;&#26723;&#65292;&#20197;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#30693;&#35782;&#21644;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#30001;&#20110;&#22909;&#22855;&#24515;&#30340;&#39537;&#20351;&#65292;&#19981;&#26029;&#25506;&#32034;&#21644;&#29702;&#35299;&#21608;&#22260;&#30340;&#19990;&#30028;&#65292;&#20174;&#32780;&#21457;&#26126;&#20102;&#21508;&#31181;&#24037;&#20855;&#26469;&#28385;&#36275;&#36825;&#31181;&#22909;&#22855;&#24515;&#12290;&#23613;&#31649;&#20154;&#31867;&#26080;&#27861;&#22312;&#22823;&#33041;&#20013;&#22788;&#29702;&#21644;&#35760;&#24518;&#22823;&#37327;&#20449;&#24687;&#65292;&#20294;&#22312;&#25209;&#21028;&#24605;&#32500;&#12289;&#35268;&#21010;&#12289;&#21453;&#24605;&#20197;&#21450;&#21033;&#29992;&#29616;&#26377;&#24037;&#20855;&#19982;&#19990;&#30028;&#36827;&#34892;&#20132;&#20114;&#21644;&#35299;&#37322;&#26041;&#38754;&#21331;&#36234;&#20986;&#33394;&#65292;&#20351;&#20854;&#33021;&#22815;&#39640;&#25928;&#22320;&#23547;&#25214;&#31572;&#26696;&#12290;&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36827;&#27493;&#34920;&#26126;&#65292;&#26426;&#22120;&#21487;&#33021;&#20063;&#20855;&#22791;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#33021;&#21147;&#65292;&#21363;&#20351;&#21442;&#25968;&#25968;&#37327;&#21463;&#38480;&#65292;&#20063;&#33021;&#23637;&#31034;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102; KwaiAgents&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;LLM&#30340;&#36890;&#29992;&#20449;&#24687;&#25628;&#32034;&#26234;&#33021;&#20307;&#31995;&#32479;&#12290;&#22312; KwaiAgents &#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;LLM&#20316;&#20026;&#35748;&#30693;&#26680;&#24515;&#30340;&#26234;&#33021;&#20307;&#31995;&#32479;&#65292;&#23427;&#33021;&#22815;&#29702;&#35299;&#29992;&#25143;&#30340;&#26597;&#35810;&#12289;&#34892;&#20026;&#20934;&#21017;&#21644;&#21442;&#32771;&#22806;&#37096;&#25991;&#26723;&#12290;&#26234;&#33021;&#20307;&#36824;&#21487;&#20197;&#26356;&#26032;&#26597;&#35810;&#32467;&#26524;&#65292;&#19982;&#29992;&#25143;&#36827;&#34892;&#20114;&#21160;&#65292;&#24182;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#30693;&#35782;&#21644;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Driven by curiosity, humans have continually sought to explore and understand the world around them, leading to the invention of various tools to satiate this inquisitiveness. Despite not having the capacity to process and memorize vast amounts of information in their brains, humans excel in critical thinking, planning, reflection, and harnessing available tools to interact with and interpret the world, enabling them to find answers efficiently. The recent advancements in large language models (LLMs) suggest that machines might also possess the aforementioned human-like capabilities, allowing them to exhibit powerful abilities even with a constrained parameter count. In this paper, we introduce KwaiAgents, a generalized information-seeking agent system based on LLMs. Within KwaiAgents, we propose an agent system that employs LLMs as its cognitive core, which is capable of understanding a user's query, behavior guidelines, and referencing external documents. The agent can also update an
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#36890;&#36807;&#32852;&#37030;&#36801;&#31227;&#23398;&#20064;&#23558;&#22522;&#30784;&#27169;&#22411;&#25509;&#22320;&#65292;&#20197;&#35299;&#20915;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#22914;&#21463;&#38480;&#30340;&#35745;&#31639;&#36164;&#28304;&#12289;&#25968;&#25454;&#38544;&#31169;&#12289;&#27169;&#22411;&#24322;&#26500;&#24615;&#21644;&#27169;&#22411;&#25152;&#26377;&#26435;&#12290;&#36825;&#20010;&#26694;&#26550;&#21487;&#20197;&#24110;&#21161;&#37322;&#25918;&#22522;&#30784;&#27169;&#22411;&#30340;&#28508;&#21147;&#65292;&#24182;&#22312;&#19981;&#21516;&#34892;&#19994;&#20013;&#20135;&#29983;&#37325;&#35201;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2311.17431</link><description>&lt;p&gt;
&#36890;&#36807;&#32852;&#37030;&#36801;&#31227;&#23398;&#20064;&#23545;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#25509;&#22320;&#65306;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Grounding Foundation Models through Federated Transfer Learning: A General Framework. (arXiv:2311.17431v7 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.17431
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#36890;&#36807;&#32852;&#37030;&#36801;&#31227;&#23398;&#20064;&#23558;&#22522;&#30784;&#27169;&#22411;&#25509;&#22320;&#65292;&#20197;&#35299;&#20915;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#22914;&#21463;&#38480;&#30340;&#35745;&#31639;&#36164;&#28304;&#12289;&#25968;&#25454;&#38544;&#31169;&#12289;&#27169;&#22411;&#24322;&#26500;&#24615;&#21644;&#27169;&#22411;&#25152;&#26377;&#26435;&#12290;&#36825;&#20010;&#26694;&#26550;&#21487;&#20197;&#24110;&#21161;&#37322;&#25918;&#22522;&#30784;&#27169;&#22411;&#30340;&#28508;&#21147;&#65292;&#24182;&#22312;&#19981;&#21516;&#34892;&#19994;&#20013;&#20135;&#29983;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#24191;&#27867;&#30693;&#35782;&#21644;&#24378;&#22823;&#30340;&#26032;&#20852;&#33021;&#21147;&#32534;&#30721;&#30340;Foundation Models&#65288;FMs&#65289;&#65292;&#22914;GPT-4&#65292;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#12290;&#36890;&#36807;&#23558;FMs&#36866;&#24212;&#20110;&#29305;&#23450;&#39046;&#22495;&#20219;&#21153;&#25110;&#22686;&#21152;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#26469;&#23545;&#20854;&#36827;&#34892;&#25509;&#22320;&#65292;&#25105;&#20204;&#21487;&#20197;&#20805;&#20998;&#21457;&#25381;FMs&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#25509;&#22320;FMs&#38754;&#20020;&#30528;&#22810;&#20010;&#25361;&#25112;&#65292;&#20027;&#35201;&#26159;&#21463;&#38480;&#30340;&#35745;&#31639;&#36164;&#28304;&#12289;&#25968;&#25454;&#38544;&#31169;&#12289;&#27169;&#22411;&#24322;&#26500;&#24615;&#21644;&#27169;&#22411;&#25152;&#26377;&#26435;&#12290;&#32852;&#37030;&#36801;&#31227;&#23398;&#20064;&#65288;FTL&#65289;&#65292;&#21363;&#32852;&#37030;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#30340;&#32467;&#21512;&#65292;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36817;&#24180;&#26469;&#65292;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#23545;&#36890;&#36807;FTL-FM&#21033;&#29992;FMs&#36827;&#34892;&#25509;&#22320;&#30340;&#38656;&#27714;&#24378;&#28872;&#22686;&#38271;&#12290;&#21463;&#21040;FTL-FM&#30740;&#31350;&#30340;&#24378;&#21170;&#22686;&#38271;&#21644;FTL-FM&#23545;&#24037;&#19994;&#24212;&#29992;&#30340;&#28508;&#22312;&#24433;&#21709;&#30340;&#25512;&#21160;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;FTL-FM&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#32852;&#37030;&#23398;&#20064;&#29615;&#22659;&#20013;&#24314;&#31435;FMs&#30340;&#25509;&#22320;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation Models (FMs) such as GPT-4 encoded with vast knowledge and powerful emergent abilities have achieved remarkable success in various natural language processing and computer vision tasks. Grounding FMs by adapting them to domain-specific tasks or augmenting them with domain-specific knowledge enables us to exploit the full potential of FMs. However, grounding FMs faces several challenges, stemming primarily from constrained computing resources, data privacy, model heterogeneity, and model ownership. Federated Transfer Learning (FTL), the combination of federated learning and transfer learning, provides promising solutions to address these challenges. In recent years, the need for grounding FMs leveraging FTL, coined FTL-FM, has arisen strongly in both academia and industry. Motivated by the strong growth in FTL-FM research and the potential impact of FTL-FM on industrial applications, we propose an FTL-FM framework that formulates problems of grounding FMs in the federated lea
&lt;/p&gt;</description></item><item><title>GeoLocator&#26159;&#19968;&#31181;&#29992;&#20110;&#25512;&#26029;&#22320;&#29702;&#38544;&#31169;&#30340;&#20301;&#32622;&#38598;&#25104;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#33021;&#22815;&#39640;&#31934;&#24230;&#22320;&#29983;&#25104;&#20855;&#20307;&#30340;&#22320;&#29702;&#32454;&#33410;&#65292;&#31361;&#26174;&#20102;&#22312;&#32447;&#25968;&#25454;&#20849;&#20139;&#21644;&#20449;&#24687;&#33719;&#21462;&#30340;&#23041;&#32961;&#12290;</title><link>http://arxiv.org/abs/2311.13018</link><description>&lt;p&gt;
GeoLocator:&#19968;&#31181;&#29992;&#20110;&#25512;&#26029;&#22320;&#29702;&#38544;&#31169;&#30340;&#20301;&#32622;&#38598;&#25104;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
GeoLocator: a location-integrated large multimodal model for inferring geo-privacy. (arXiv:2311.13018v3 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.13018
&lt;/p&gt;
&lt;p&gt;
GeoLocator&#26159;&#19968;&#31181;&#29992;&#20110;&#25512;&#26029;&#22320;&#29702;&#38544;&#31169;&#30340;&#20301;&#32622;&#38598;&#25104;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#33021;&#22815;&#39640;&#31934;&#24230;&#22320;&#29983;&#25104;&#20855;&#20307;&#30340;&#22320;&#29702;&#32454;&#33410;&#65292;&#31361;&#26174;&#20102;&#22312;&#32447;&#25968;&#25454;&#20849;&#20139;&#21644;&#20449;&#24687;&#33719;&#21462;&#30340;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22320;&#29702;&#38544;&#31169;&#26159;&#25351;&#20445;&#25252;&#20010;&#20154;&#22320;&#29702;&#20301;&#32622;&#30340;&#31169;&#23494;&#24615;&#65292;&#29305;&#21035;&#26159;&#38480;&#21046;&#20010;&#20154;&#30005;&#23376;&#35774;&#22791;&#20013;&#32500;&#25252;&#30340;&#22320;&#29702;&#25968;&#25454;&#12290;&#22320;&#29702;&#38544;&#31169;&#26159;&#20010;&#20154;&#23433;&#20840;&#30340;&#37325;&#35201;&#26041;&#38754;&#65292;&#28982;&#32780;&#22312;&#26085;&#24120;&#27963;&#21160;&#20013;&#24120;&#24120;&#34987;&#24573;&#35270;&#12290;&#38543;&#30528;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;&#22914;GPT-4&#65289;&#22312;&#24320;&#28304;&#24773;&#25253;&#65288;OSINT&#65289;&#20013;&#30340;&#20351;&#29992;&#28608;&#22686;&#65292;&#19982;&#22320;&#29702;&#38544;&#31169;&#27844;&#38706;&#30456;&#20851;&#30340;&#28508;&#22312;&#39118;&#38505;&#21152;&#21095;&#12290;&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;GeoLocator&#30340;&#22522;&#20110;GPT-4&#30340;&#20301;&#32622;&#38598;&#25104;&#27169;&#22411;&#65292;&#24182;&#35774;&#35745;&#20102;&#22235;&#32500;&#23454;&#39564;&#26469;&#23637;&#31034;&#20854;&#25512;&#26029;&#36755;&#20837;&#22270;&#20687;&#21644;/&#25110;&#31038;&#20132;&#23186;&#20307;&#20869;&#23481;&#30340;&#20301;&#32622;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;GeoLocator&#33021;&#22815;&#39640;&#31934;&#24230;&#22320;&#29983;&#25104;&#20855;&#20307;&#30340;&#22320;&#29702;&#32454;&#33410;&#65292;&#20174;&#32780;&#23558;&#27169;&#22411;&#29992;&#25143;&#26080;&#24847;&#20013;&#26292;&#38706;&#22320;&#29702;&#31354;&#38388;&#20449;&#24687;&#30340;&#39118;&#38505;&#23884;&#20837;&#20854;&#20013;&#65292;&#31361;&#26174;&#20102;&#22312;&#32447;&#25968;&#25454;&#20849;&#20139;&#21644;&#20449;&#24687;&#33719;&#21462;&#30340;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;
Geographic privacy or geo-privacy refers to the keeping private of one's geographic location, especially the restriction of geographical data maintained by personal electronic devices. Geo-privacy is a crucial aspect of personal security; however, it often goes unnoticed in daily activities. With the surge in the use of Large Multimodal Models (LMMs), such as GPT-4, for Open Source Intelligence (OSINT), the potential risks associated with geo-privacy breaches have intensified. This study develops a location-integrated GPT-4 based model named GeoLocator and designs four-dimensional experiments to demonstrate its capability in inferring the locational information of input imageries and/or social media contents. Our experiments reveal that GeoLocator generates specific geographic details with high accuracy and consequently embeds the risk of the model users exposing geospatial information to the public unintentionally, highlighting the thread of online data sharing, information gathering 
&lt;/p&gt;</description></item><item><title>&#22823;&#22810;&#25968;&#35745;&#31639;&#26426;&#31185;&#23398;&#26412;&#31185;&#29983;&#23545;ChatGPT&#25345;&#26377;&#31215;&#26497;&#24577;&#24230;&#65292;&#24182;&#35748;&#20026;&#23427;&#33021;&#22312;&#35838;&#31243;&#30456;&#20851;&#20219;&#21153;&#20013;&#21457;&#25381;&#20316;&#29992;&#65292;&#20294;&#20063;&#23384;&#22312;&#19968;&#20123;&#38656;&#35299;&#20915;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2311.09651</link><description>&lt;p&gt;
"&#23427;&#19981;&#20687;Jarvis&#65292;&#20294;&#24456;&#25509;&#36817;&#65281;" -- &#25506;&#31350;ChatGPT&#22312;&#35745;&#31639;&#26426;&#31185;&#23398;&#26412;&#31185;&#29983;&#20013;&#30340;&#20351;&#29992;&#24773;&#20917;
&lt;/p&gt;
&lt;p&gt;
"It's not like Jarvis, but it's pretty close!" -- Examining ChatGPT's Usage among Undergraduate Students in Computer Science. (arXiv:2311.09651v2 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.09651
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#35745;&#31639;&#26426;&#31185;&#23398;&#26412;&#31185;&#29983;&#23545;ChatGPT&#25345;&#26377;&#31215;&#26497;&#24577;&#24230;&#65292;&#24182;&#35748;&#20026;&#23427;&#33021;&#22312;&#35838;&#31243;&#30456;&#20851;&#20219;&#21153;&#20013;&#21457;&#25381;&#20316;&#29992;&#65292;&#20294;&#20063;&#23384;&#22312;&#19968;&#20123;&#38656;&#35299;&#20915;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;ChatGPT&#21644;Google Bard&#22312;&#23398;&#26415;&#30028;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#35780;&#20272;&#20102;&#36825;&#20123;LLM&#22312;&#29983;&#25104;&#32534;&#31243;&#32451;&#20064;&#21644;&#35299;&#20915;&#26041;&#26696;&#31561;&#21508;&#31181;&#24212;&#29992;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35780;&#20272;&#20027;&#35201;&#30001;&#25945;&#24072;&#21644;&#30740;&#31350;&#20154;&#21592;&#36827;&#34892;&#65292;&#24182;&#26410;&#32771;&#34385;&#21040;&#23398;&#29983;&#23545;LLM&#30340;&#23454;&#38469;&#20351;&#29992;&#24773;&#20917;&#12290;&#26412;&#30740;&#31350;&#37319;&#29992;&#20197;&#23398;&#29983;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#65292;&#20840;&#38754;&#20102;&#35299;&#26412;&#31185;&#35745;&#31639;&#26426;&#31185;&#23398;&#23398;&#29983;&#22914;&#20309;&#21033;&#29992;OpenAI&#21457;&#24067;&#30340;&#28909;&#38376;LLM ChatGPT&#12290;&#25105;&#20204;&#36890;&#36807;&#23398;&#29983;&#35843;&#26597;&#21644;&#35775;&#35848;&#30456;&#32467;&#21512;&#30340;&#26041;&#24335;&#33719;&#21462;&#26377;&#20215;&#20540;&#30340;&#27934;&#23519;&#65292;&#20102;&#35299;&#19982;ChatGPT&#30456;&#20851;&#30340;&#22909;&#22788;&#12289;&#25361;&#25112;&#21644;&#25913;&#36827;&#24314;&#35758;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22823;&#22810;&#25968;&#23398;&#29983;&#65288;&#36229;&#36807;57%&#65289;&#23545;&#23558;ChatGPT&#20316;&#20026;&#35838;&#31243;&#30456;&#20851;&#20219;&#21153;&#30340;&#36741;&#21161;&#24037;&#20855;&#25345;&#26377;&#31215;&#26497;&#30340;&#24577;&#24230;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#20063;&#24378;&#35843;&#20102;&#24517;&#39035;&#35299;&#20915;&#30340;&#21508;&#31181;&#25361;&#25112;&#65292;&#20197;&#23454;&#29616;&#23545;ChatGPT&#30340;&#38271;&#26399;&#25509;&#21463;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) such as ChatGPT and Google Bard have garnered significant attention in the academic community. Previous research has evaluated these LLMs for various applications such as generating programming exercises and solutions. However, these evaluations have predominantly been conducted by instructors and researchers, not considering the actual usage of LLMs by students. This study adopts a student-first approach to comprehensively understand how undergraduate computer science students utilize ChatGPT, a popular LLM, released by OpenAI. We employ a combination of student surveys and interviews to obtain valuable insights into the benefits, challenges, and suggested improvements related to ChatGPT. Our findings suggest that a majority of students (over 57%) have a convincingly positive outlook towards adopting ChatGPT as an aid in coursework-related tasks. However, our research also highlights various challenges that must be resolved for long-term acceptance of Chat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#21106;&#32852;&#37030;&#23398;&#20064;&#65288;SFL&#65289;&#20013;&#38544;&#31169;&#21644;&#33021;&#32791;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24378;&#35843;&#20102;&#24555;&#36895;&#25910;&#25947;&#30340;&#20248;&#21183;&#65292;&#24182;&#20998;&#26512;&#20102;&#20999;&#21106;&#23618;&#23545;&#23458;&#25143;&#31471;&#33021;&#32791;&#21644;&#38544;&#31169;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2311.09441</link><description>&lt;p&gt;
&#25506;&#32034;&#20998;&#21106;&#32852;&#37030;&#23398;&#20064;&#30340;&#38544;&#31169;-&#33021;&#32791;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Exploring the Privacy-Energy Consumption Tradeoff for Split Federated Learning. (arXiv:2311.09441v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.09441
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#21106;&#32852;&#37030;&#23398;&#20064;&#65288;SFL&#65289;&#20013;&#38544;&#31169;&#21644;&#33021;&#32791;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24378;&#35843;&#20102;&#24555;&#36895;&#25910;&#25947;&#30340;&#20248;&#21183;&#65292;&#24182;&#20998;&#26512;&#20102;&#20999;&#21106;&#23618;&#23545;&#23458;&#25143;&#31471;&#33021;&#32791;&#21644;&#38544;&#31169;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#21106;&#32852;&#37030;&#23398;&#20064;&#65288;SFL&#65289;&#26368;&#36817;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#25216;&#26415;&#65292;&#20805;&#20998;&#21033;&#29992;&#20102;&#32852;&#37030;&#23398;&#20064;&#21644;&#20998;&#21106;&#23398;&#20064;&#30340;&#20248;&#21183;&#12290;&#23427;&#24378;&#35843;&#20102;&#24555;&#36895;&#25910;&#25947;&#30340;&#20248;&#21183;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#38544;&#31169;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#36825;&#19968;&#21019;&#26032;&#21463;&#21040;&#20102;&#24037;&#19994;&#30028;&#21644;&#23398;&#26415;&#30028;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;SFL&#20013;&#27169;&#22411;&#22312;&#29305;&#23450;&#23618;&#65288;&#31216;&#20026;&#20999;&#21106;&#23618;&#65289;&#19978;&#34987;&#20998;&#21106;&#20026;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#31471;&#27169;&#22411;&#65292;&#36873;&#25321;&#20999;&#21106;&#23618;&#21487;&#33021;&#23545;&#23458;&#25143;&#31471;&#30340;&#33021;&#32791;&#21644;&#38544;&#31169;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#65292;&#22240;&#20026;&#23427;&#24433;&#21709;&#20102;&#35757;&#32451;&#36127;&#25285;&#21644;&#23458;&#25143;&#31471;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;&#27492;&#22806;&#65292;&#30830;&#23450;&#20999;&#21106;&#23618;&#30340;&#35774;&#35745;&#25361;&#25112;&#38750;&#24120;&#22797;&#26434;&#65292;&#20027;&#35201;&#30001;&#20110;&#23458;&#25143;&#31471;&#30340;&#35745;&#31639;&#21644;&#32593;&#32476;&#33021;&#21147;&#30340;&#22266;&#26377;&#24322;&#36136;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#27010;&#36848;&#20102;SFL&#30340;&#36807;&#31243;&#65292;&#24182;&#23545;&#33021;&#32791;&#21644;&#38544;&#31169;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Split Federated Learning (SFL) has recently emerged as a promising distributed learning technology, leveraging the strengths of both federated learning and split learning. It emphasizes the advantages of rapid convergence while addressing privacy concerns. As a result, this innovation has received significant attention from both industry and academia. However, since the model is split at a specific layer, known as a cut layer, into both client-side and server-side models for the SFL, the choice of the cut layer in SFL can have a substantial impact on the energy consumption of clients and their privacy, as it influences the training burden and the output of the client-side models. Moreover, the design challenge of determining the cut layer is highly intricate, primarily due to the inherent heterogeneity in the computing and networking capabilities of clients. In this article, we provide a comprehensive overview of the SFL process and conduct a thorough analysis of energy consumption and
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#32954;&#21160;&#33033;&#36896;&#24433;&#20013;&#30340;&#32954;&#26643;&#22622;&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#20998;&#31867;&#22120;&#30340;&#27010;&#29575;&#25512;&#29702;&#26469;&#25351;&#23548;&#26816;&#27979;&#39044;&#27979;&#65292;&#22312;&#33258;&#21160;PE&#35786;&#26029;&#39046;&#22495;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#36129;&#29486;&#12290;&#25105;&#20204;&#30340;&#20998;&#31867;&#22120;&#21033;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#21516;&#26102;&#20851;&#27880;&#25972;&#20307;&#22806;&#35980;&#21644;&#23616;&#37096;&#30149;&#21464;&#21306;&#22495;&#65292;&#23454;&#29616;&#20102;&#31283;&#20581;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.05197</link><description>&lt;p&gt;
&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#32954;&#21160;&#33033;&#36896;&#24433;&#20013;&#30340;&#28145;&#24230;&#23398;&#20064;&#65306;&#29992;&#20110;&#32954;&#26643;&#22622;&#26816;&#27979;&#30340;&#21452;&#37325;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Deep learning in computed tomography pulmonary angiography imaging: a dual-pronged approach for pulmonary embolism detection. (arXiv:2311.05197v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.05197
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#32954;&#21160;&#33033;&#36896;&#24433;&#20013;&#30340;&#32954;&#26643;&#22622;&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#20998;&#31867;&#22120;&#30340;&#27010;&#29575;&#25512;&#29702;&#26469;&#25351;&#23548;&#26816;&#27979;&#39044;&#27979;&#65292;&#22312;&#33258;&#21160;PE&#35786;&#26029;&#39046;&#22495;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#36129;&#29486;&#12290;&#25105;&#20204;&#30340;&#20998;&#31867;&#22120;&#21033;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#21516;&#26102;&#20851;&#27880;&#25972;&#20307;&#22806;&#35980;&#21644;&#23616;&#37096;&#30149;&#21464;&#21306;&#22495;&#65292;&#23454;&#29616;&#20102;&#31283;&#20581;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32954;&#26643;&#22622;&#65288;PE&#65289;&#35786;&#26029;&#23545;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#32954;&#21160;&#33033;&#36896;&#24433;&#65288;CTPA&#65289;&#30340;&#20381;&#36182;&#24230;&#19981;&#26029;&#22686;&#21152;&#65292;&#36825;&#23548;&#33268;&#20102;&#23545;&#25913;&#36827;&#35786;&#26029;&#26041;&#27861;&#30340;&#25361;&#25112;&#21644;&#36843;&#20999;&#38656;&#27714;&#12290;&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#25552;&#21319;PE&#30340;&#35745;&#31639;&#26426;&#36741;&#21161;&#35786;&#26029;&#65288;CAD&#65289;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#31867;&#22120;&#24341;&#23548;&#30340;&#26816;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#26377;&#25928;&#21033;&#29992;&#20998;&#31867;&#22120;&#30340;&#27010;&#29575;&#25512;&#29702;&#26469;&#25351;&#23548;&#26816;&#27979;&#39044;&#27979;&#65292;&#22312;&#33258;&#21160;PE&#35786;&#26029;&#39046;&#22495;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#36129;&#29486;&#12290;&#25105;&#20204;&#30340;&#20998;&#31867;&#31995;&#32479;&#21253;&#25324;&#19968;&#20010;&#37319;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;AG-CNN&#65289;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#23616;&#37096;&#19978;&#19979;&#25991;&#20449;&#24687;&#26469;&#27169;&#25311;&#20154;&#31867;&#19987;&#23478;&#30340;&#20851;&#27880;&#65292;&#21363;&#22312;&#20570;&#20986;&#21028;&#26029;&#20043;&#21069;&#21516;&#26102;&#20851;&#27880;&#25972;&#20307;&#22806;&#35980;&#21644;&#23616;&#37096;&#30149;&#21464;&#21306;&#22495;&#12290;&#35813;&#20998;&#31867;&#22120;&#22312;FUMPE&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#31283;&#20581;&#30340;&#24615;&#33021;&#65292;&#23454;&#29616;&#20102;0.927&#30340;AUROC&#12289;0.862&#30340;&#25935;&#24863;&#24230;&#12289;0.879&#30340;&#29305;&#24322;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing reliance on Computed Tomography Pulmonary Angiography (CTPA) for Pulmonary Embolism (PE) diagnosis presents challenges and a pressing need for improved diagnostic solutions. The primary objective of this study is to leverage deep learning techniques to enhance the Computer Assisted Diagnosis (CAD) of PE. With this aim, we propose a classifier-guided detection approach that effectively leverages the classifier's probabilistic inference to direct the detection predictions, marking a novel contribution in the domain of automated PE diagnosis. Our classification system includes an Attention-Guided Convolutional Neural Network (AG-CNN) that uses local context by employing an attention mechanism. This approach emulates a human expert's attention by looking at both global appearances and local lesion regions before making a decision. The classifier demonstrates robust performance on the FUMPE dataset, achieving an AUROC of 0.927, sensitivity of 0.862, specificity of 0.879, and 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#36339;&#36291;&#27491;&#20132;&#21015;&#34920;&#26469;&#35299;&#20915;&#21160;&#24577;&#26368;&#20248;&#20256;&#36755;&#38382;&#39064;&#65292;&#22312;&#32771;&#34385;&#25968;&#25454;&#28857;&#26435;&#37325;&#25110;&#20301;&#32622;&#21464;&#21270;&#26102;&#33021;&#22815;&#26377;&#25928;&#22320;&#26356;&#26032;&#26368;&#20248;&#20256;&#36755;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2310.18446</link><description>&lt;p&gt;
&#21160;&#24577;&#26368;&#20248;&#20256;&#36755;&#38382;&#39064;&#30340;&#19968;&#31181;&#26032;&#22411;&#36339;&#36291;&#27491;&#20132;&#21015;&#34920;
&lt;/p&gt;
&lt;p&gt;
A Novel Skip Orthogonal List for Dynamic Optimal Transport Problem. (arXiv:2310.18446v4 [cs.DS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18446
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#36339;&#36291;&#27491;&#20132;&#21015;&#34920;&#26469;&#35299;&#20915;&#21160;&#24577;&#26368;&#20248;&#20256;&#36755;&#38382;&#39064;&#65292;&#22312;&#32771;&#34385;&#25968;&#25454;&#28857;&#26435;&#37325;&#25110;&#20301;&#32622;&#21464;&#21270;&#26102;&#33021;&#22815;&#26377;&#25928;&#22320;&#26356;&#26032;&#26368;&#20248;&#20256;&#36755;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20248;&#20256;&#36755;&#26159;&#36807;&#21435;&#20960;&#21313;&#24180;&#26469;&#24341;&#36215;&#20102;&#20248;&#21270;&#31038;&#21306;&#26497;&#22823;&#20851;&#27880;&#30340;&#19968;&#20010;&#22522;&#26412;&#20027;&#39064;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#31163;&#25955;&#21160;&#24577;&#26368;&#20248;&#20256;&#36755;&#38382;&#39064;&#65306;&#24403;&#25968;&#25454;&#28857;&#30340;&#26435;&#37325;&#25110;&#20301;&#32622;&#21457;&#29983;&#21464;&#21270;&#26102;&#65292;&#25105;&#20204;&#26159;&#21542;&#21487;&#20197;&#39640;&#25928;&#22320;&#26356;&#26032;&#26368;&#20248;&#20256;&#36755;&#26041;&#26696;&#65311;&#36825;&#20010;&#38382;&#39064;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#26377;&#22810;&#20010;&#24212;&#29992;&#12290;&#25105;&#20204;&#32463;&#24120;&#38656;&#35201;&#35745;&#31639;&#20004;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#26368;&#20248;&#20256;&#36755;&#25104;&#26412;&#65307;&#22914;&#26524;&#19968;&#20123;&#25968;&#25454;&#28857;&#21457;&#29983;&#20102;&#21464;&#21270;&#65292;&#25105;&#20204;&#24212;&#35813;&#37325;&#26032;&#35745;&#31639;&#39640;&#22797;&#26434;&#24230;&#30340;&#25104;&#26412;&#20989;&#25968;&#65292;&#36824;&#26159;&#36890;&#36807;&#19968;&#20123;&#39640;&#25928;&#30340;&#21160;&#24577;&#25968;&#25454;&#32467;&#26500;&#26469;&#26356;&#26032;&#25104;&#26412;&#65311;&#25105;&#20204;&#27880;&#24847;&#21040;&#20808;&#21069;&#24050;&#25552;&#20986;&#20102;&#20960;&#31181;&#21160;&#24577;&#26368;&#22823;&#27969;&#31639;&#27861;&#65292;&#20294;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#23545;&#20110;&#21160;&#24577;&#26368;&#23567;&#36153;&#29992;&#27969;&#38382;&#39064;&#30340;&#30740;&#31350;&#20173;&#28982;&#38750;&#24120;&#26377;&#38480;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;2D&#36339;&#36291;&#27491;&#20132;&#21015;&#34920;&#65292;&#32467;&#21512;&#19968;&#20123;&#21160;&#24577;&#26641;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimal transport is a fundamental topic that has attracted a great amount of attention from the optimization community in the past decades. In this paper, we consider an interesting discrete dynamic optimal transport problem: can we efficiently update the optimal transport plan when the weights or the locations of the data points change? This problem is naturally motivated by several applications in machine learning. For example, we often need to compute the optimal transport cost between two different data sets; if some changes happen to a few data points, should we re-compute the high complexity cost function or update the cost by some efficient dynamic data structure? We are aware that several dynamic maximum flow algorithms have been proposed before, however, the research on dynamic minimum cost flow problem is still quite limited, to the best of our knowledge. We propose a novel 2D Skip Orthogonal List together with some dynamic tree techniques. Although our algorithm is based on
&lt;/p&gt;</description></item><item><title>&#20998;&#23618;&#38543;&#26426;&#24179;&#28369;&#26159;&#19968;&#31181;&#22312;&#22797;&#26434;&#25968;&#25454;&#19978;&#36827;&#34892;&#40065;&#26834;&#24615;&#35748;&#35777;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#21482;&#22312;&#19968;&#20010;&#23545;&#35937;&#30340;&#23376;&#38598;&#19978;&#28155;&#21152;&#38543;&#26426;&#22122;&#22768;&#65292;&#20197;&#26356;&#26377;&#38024;&#23545;&#24615;&#30340;&#26041;&#24335;&#25552;&#20379;&#20102;&#26356;&#24378;&#30340;&#40065;&#26834;&#24615;&#20445;&#35777;&#21644;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.16221</link><description>&lt;p&gt;
&#20998;&#23618;&#38543;&#26426;&#24179;&#28369;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Randomized Smoothing. (arXiv:2310.16221v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16221
&lt;/p&gt;
&lt;p&gt;
&#20998;&#23618;&#38543;&#26426;&#24179;&#28369;&#26159;&#19968;&#31181;&#22312;&#22797;&#26434;&#25968;&#25454;&#19978;&#36827;&#34892;&#40065;&#26834;&#24615;&#35748;&#35777;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#21482;&#22312;&#19968;&#20010;&#23545;&#35937;&#30340;&#23376;&#38598;&#19978;&#28155;&#21152;&#38543;&#26426;&#22122;&#22768;&#65292;&#20197;&#26356;&#26377;&#38024;&#23545;&#24615;&#30340;&#26041;&#24335;&#25552;&#20379;&#20102;&#26356;&#24378;&#30340;&#40065;&#26834;&#24615;&#20445;&#35777;&#21644;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#26159;&#22797;&#26434;&#30340;&#65292;&#36890;&#24120;&#30001;&#21487;&#20998;&#35299;&#20026;&#22810;&#20010;&#23454;&#20307;&#30340;&#23545;&#35937;&#32452;&#25104;&#65288;&#20363;&#22914;&#65292;&#23558;&#22270;&#20687;&#20998;&#35299;&#20026;&#20687;&#32032;&#65292;&#23558;&#22270;&#24418;&#20998;&#35299;&#20026;&#30456;&#20114;&#36830;&#25509;&#30340;&#33410;&#28857;&#65289;&#12290;&#38543;&#26426;&#24179;&#28369;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#20351;&#27169;&#22411;&#22312;&#20854;&#36755;&#20837;&#30340;&#24494;&#23567;&#21464;&#21270;&#19978;&#20855;&#26377;&#35777;&#26126;&#30340;&#40065;&#26834;&#24615;-&#36890;&#36807;&#22312;&#20998;&#31867;&#20043;&#21069;&#38543;&#26426;&#28155;&#21152;&#22122;&#22768;&#26469;&#20445;&#35777;&#22810;&#25968;&#25237;&#31080;&#30340;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;&#24403;&#23545;&#25163;&#19981;&#26159;&#20219;&#24847;&#24178;&#25200;&#25972;&#20010;&#23545;&#35937;&#65288;&#20363;&#22914;&#22270;&#20687;&#65289;&#65292;&#32780;&#26159;&#23545;&#35937;&#30340;&#26576;&#20010;&#23454;&#20307;&#30340;&#23376;&#38598;&#65288;&#20363;&#22914;&#20687;&#32032;&#65289;&#26102;&#65292;&#36890;&#36807;&#38543;&#26426;&#24179;&#28369;&#23545;&#36825;&#31181;&#22797;&#26434;&#25968;&#25454;&#36827;&#34892;&#40065;&#26834;&#24615;&#35748;&#35777;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20316;&#20026;&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20998;&#23618;&#38543;&#26426;&#24179;&#28369;&#65306;&#25105;&#20204;&#36890;&#36807;&#20165;&#22312;&#38543;&#26426;&#36873;&#25321;&#30340;&#23454;&#20307;&#23376;&#38598;&#19978;&#28155;&#21152;&#38543;&#26426;&#22122;&#22768;&#26469;&#37096;&#20998;&#24179;&#28369;&#23545;&#35937;&#12290;&#36890;&#36807;&#20197;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#26377;&#38024;&#23545;&#24615;&#30340;&#26041;&#24335;&#28155;&#21152;&#22122;&#22768;&#65292;&#25105;&#20204;&#33719;&#24471;&#26356;&#24378;&#30340;&#40065;&#26834;&#24615;&#20445;&#35777;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#19981;&#21516;&#30340;&#22122;&#22768;&#20998;&#24067;&#21021;&#22987;&#21270;&#20998;&#23618;&#24179;&#28369;&#65292;&#24471;&#21040;&#20102;&#26032;&#30340;&#40065;&#26834;&#24615;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world data is complex and often consists of objects that can be decomposed into multiple entities (e.g. images into pixels, graphs into interconnected nodes). Randomized smoothing is a powerful framework for making models provably robust against small changes to their inputs - by guaranteeing robustness of the majority vote when randomly adding noise before classification. Yet, certifying robustness on such complex data via randomized smoothing is challenging when adversaries do not arbitrarily perturb entire objects (e.g. images) but only a subset of their entities (e.g. pixels). As a solution, we introduce hierarchical randomized smoothing: We partially smooth objects by adding random noise only on a randomly selected subset of their entities. By adding noise in a more targeted manner than existing methods we obtain stronger robustness guarantees while maintaining high accuracy. We initialize hierarchical smoothing using different noising distributions, yielding novel robustness
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#24320;&#25918;&#25968;&#25454;&#21644;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31995;&#32479;&#26469;&#25512;&#33616;&#22242;&#38431;&#65292;&#20351;&#24471;&#27599;&#20010;&#22242;&#38431;&#33021;&#22815;&#28385;&#36275;&#39033;&#30446;&#35201;&#27714;&#30340;&#25216;&#33021;&#35206;&#30422;&#65292;&#24182;&#19988;&#24179;&#34913;&#20505;&#36873;&#25104;&#21592;&#20043;&#38388;&#30340;&#24037;&#20316;&#20998;&#37197;&#12290;</title><link>http://arxiv.org/abs/2309.09404</link><description>&lt;p&gt;
&#29992;&#24320;&#25918;&#25968;&#25454;&#39537;&#21160;&#30340;&#22242;&#38431;&#25512;&#33616;&#20419;&#36827;&#30740;&#31350;&#21512;&#20316;
&lt;/p&gt;
&lt;p&gt;
Promoting Research Collaboration with Open Data Driven Team Recommendation in Response to Call for Proposals. (arXiv:2309.09404v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09404
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#24320;&#25918;&#25968;&#25454;&#21644;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31995;&#32479;&#26469;&#25512;&#33616;&#22242;&#38431;&#65292;&#20351;&#24471;&#27599;&#20010;&#22242;&#38431;&#33021;&#22815;&#28385;&#36275;&#39033;&#30446;&#35201;&#27714;&#30340;&#25216;&#33021;&#35206;&#30422;&#65292;&#24182;&#19988;&#24179;&#34913;&#20505;&#36873;&#25104;&#21592;&#20043;&#38388;&#30340;&#24037;&#20316;&#20998;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22242;&#38431;&#24314;&#35774;&#21644;&#20419;&#36827;&#21512;&#20316;&#26159;&#38750;&#24120;&#24120;&#35265;&#30340;&#21830;&#19994;&#27963;&#21160;&#12290;&#19968;&#20010;&#20363;&#23376;&#23601;&#26159;TeamingForFunding&#38382;&#39064;&#65292;&#30740;&#31350;&#26426;&#26500;&#21644;&#30740;&#31350;&#20154;&#21592;&#22312;&#21521;&#36164;&#21161;&#26426;&#26500;&#30003;&#35831;&#26102;&#65292;&#24076;&#26395;&#33021;&#22815;&#25214;&#21040;&#21512;&#20316;&#26426;&#20250;&#20197;&#22238;&#24212;&#21518;&#32773;&#30340;&#39033;&#30446;&#30003;&#35831;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#31995;&#32479;&#65292;&#21033;&#29992;&#21508;&#31181;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#26469;&#25512;&#33616;&#22242;&#38431;&#65292;&#20351;&#24471;&#27599;&#20010;&#22242;&#38431;&#37117;&#33021;&#22815;&#36798;&#21040;&#26426;&#20250;&#35201;&#27714;&#30340;&#26368;&#39640;&#25216;&#33021;&#35206;&#30422;&#65292;&#24182;&#19988;&#20505;&#36873;&#25104;&#21592;&#20043;&#38388;&#30340;&#24037;&#20316;&#20998;&#37197;&#26159;&#24179;&#34913;&#30340;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#21462;&#24320;&#25918;&#25968;&#25454;&#20013;&#30340;&#39033;&#30446;&#30003;&#35831;&#65288;&#38656;&#27714;&#65289;&#21644;&#30740;&#31350;&#20154;&#21592;&#31616;&#20171;&#65288;&#20379;&#32473;&#65289;&#20013;&#30340;&#25216;&#33021;&#28508;&#21147;&#65292;&#20351;&#29992;&#20998;&#31867;&#27861;&#23545;&#20854;&#36827;&#34892;&#24402;&#19968;&#21270;&#65292;&#21019;&#24314;&#20102;&#26377;&#25928;&#30340;&#31639;&#27861;&#26469;&#21305;&#37197;&#38656;&#27714;&#21644;&#20379;&#32473;&#12290;&#25105;&#20204;&#21019;&#24314;&#22242;&#38431;&#20197;&#26368;&#22823;&#21270;&#19968;&#20010;&#26032;&#30340;&#34913;&#37327;&#30701;&#26399;&#21644;&#38271;&#26399;&#30446;&#26631;&#30340;&#24230;&#37327;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#23450;&#37327;&#39564;&#35777;&#20102;&#25105;&#20204;&#31639;&#27861;&#30340;&#25104;&#21151;&#65292;&#36890;&#36807;&#8230;
&lt;/p&gt;
&lt;p&gt;
Building teams and promoting collaboration are two very common business activities. An example of these are seen in the TeamingForFunding problem, where research institutions and researchers are interested to identify collaborative opportunities when applying to funding agencies in response to latter's calls for proposals. We describe a novel system to recommend teams using a variety of AI methods, such that (1) each team achieves the highest possible skill coverage that is demanded by the opportunity, and (2) the workload of distributing the opportunities is balanced amongst the candidate members. We address these questions by extracting skills latent in open data of proposal calls (demand) and researcher profiles (supply), normalizing them using taxonomies, and creating efficient algorithms that match demand to supply. We create teams to maximize goodness along a novel metric balancing short- and long-term objectives. We validate the success of our algorithms (1) quantitatively, by e
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#25991;&#26412;&#21040;&#38899;&#39057;&#29983;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#38271;&#23614;&#25991;&#26412;&#21040;&#38899;&#39057;&#29983;&#25104;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#21033;&#29992;&#26816;&#32034;&#21040;&#30340;&#30456;&#20851;&#25991;&#26412;-&#38899;&#39057;&#25968;&#25454;&#20316;&#20026;&#39069;&#22806;&#26465;&#20214;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#23398;&#20064;&#33021;&#21147;&#65292;&#22312;AudioCaps&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.08051</link><description>&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#22411;&#25991;&#26412;&#21040;&#38899;&#39057;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Retrieval-Augmented Text-to-Audio Generation. (arXiv:2309.08051v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08051
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#25991;&#26412;&#21040;&#38899;&#39057;&#29983;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#38271;&#23614;&#25991;&#26412;&#21040;&#38899;&#39057;&#29983;&#25104;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#21033;&#29992;&#26816;&#32034;&#21040;&#30340;&#30456;&#20851;&#25991;&#26412;-&#38899;&#39057;&#25968;&#25454;&#20316;&#20026;&#39069;&#22806;&#26465;&#20214;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#23398;&#20064;&#33021;&#21147;&#65292;&#22312;AudioCaps&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#25991;&#26412;&#21040;&#38899;&#39057;(TTA)&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#25105;&#20204;&#21457;&#29616;&#29366;&#24577;-&#33402;&#26415;&#27169;&#22411;&#65292;&#22914;AudioLDM&#65292;&#22312;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#31867;&#21035;&#20998;&#24067;&#19981;&#24179;&#34913;&#65288;&#22914;AudioCaps&#65289;&#30340;&#35757;&#32451;&#20013;&#65292;&#20854;&#29983;&#25104;&#24615;&#33021;&#23384;&#22312;&#20559;&#24046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23427;&#20204;&#22312;&#29983;&#25104;&#24120;&#35265;&#38899;&#39057;&#31867;&#21035;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#32780;&#22312;&#32597;&#35265;&#31867;&#21035;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#25972;&#20307;&#29983;&#25104;&#24615;&#33021;&#12290;&#25105;&#20204;&#23558;&#27492;&#38382;&#39064;&#31216;&#20026;&#38271;&#23614;&#25991;&#26412;&#21040;&#38899;&#39057;&#29983;&#25104;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26816;&#32034;&#22686;&#24378;&#26041;&#27861;&#26469;&#36827;&#34892;TTA&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#32473;&#23450;&#19968;&#20010;&#25991;&#26412;&#36755;&#20837;&#25552;&#31034;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#23545;&#27604;&#35821;&#38899;&#35821;&#35328;&#39044;&#35757;&#32451;&#65288;CLAP&#65289;&#27169;&#22411;&#26469;&#26816;&#32034;&#30456;&#20851;&#30340;&#25991;&#26412;-&#38899;&#39057;&#23545;&#12290;&#28982;&#21518;&#20351;&#29992;&#26816;&#32034;&#21040;&#30340;&#38899;&#39057;-&#25991;&#26412;&#25968;&#25454;&#30340;&#29305;&#24449;&#20316;&#20026;&#39069;&#22806;&#26465;&#20214;&#26469;&#25351;&#23548;TTA&#27169;&#22411;&#30340;&#23398;&#20064;&#12290;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22686;&#24378;&#20102;AudioLDM&#65292;&#24182;&#23558;&#25152;&#24471;&#21040;&#30340;&#22686;&#24378;&#31995;&#32479;&#31216;&#20026;Re-AudioLDM&#12290;&#22312;AudioCaps&#25968;&#25454;&#38598;&#19978;&#65292;Re-AudioLDM&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;Frechet&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite recent progress in text-to-audio (TTA) generation, we show that the state-of-the-art models, such as AudioLDM, trained on datasets with an imbalanced class distribution, such as AudioCaps, are biased in their generation performance. Specifically, they excel in generating common audio classes while underperforming in the rare ones, thus degrading the overall generation performance. We refer to this problem as long-tailed text-to-audio generation. To address this issue, we propose a simple retrieval-augmented approach for TTA models. Specifically, given an input text prompt, we first leverage a Contrastive Language Audio Pretraining (CLAP) model to retrieve relevant text-audio pairs. The features of the retrieved audio-text data are then used as additional conditions to guide the learning of TTA models. We enhance AudioLDM with our proposed approach and denote the resulting augmented system as Re-AudioLDM. On the AudioCaps dataset, Re-AudioLDM achieves a state-of-the-art Frechet 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#32534;&#31243;&#39118;&#26684;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22522;&#20110;&#30693;&#35782;&#30340;&#38382;&#31572;&#20013;&#29983;&#25104;&#36923;&#36753;&#34920;&#36798;&#24335;&#30340;&#26684;&#24335;&#38169;&#35823;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.04695</link><description>&lt;p&gt;
&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#32534;&#31243;&#39118;&#26684;&#20197;&#35299;&#20915;&#22522;&#20110;&#30693;&#35782;&#30340;&#38382;&#31572;&#20013;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Code-Style In-Context Learning for Knowledge-Based Question Answering. (arXiv:2309.04695v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04695
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#32534;&#31243;&#39118;&#26684;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22522;&#20110;&#30693;&#35782;&#30340;&#38382;&#31572;&#20013;&#29983;&#25104;&#36923;&#36753;&#34920;&#36798;&#24335;&#30340;&#26684;&#24335;&#38169;&#35823;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#38024;&#23545;&#22522;&#20110;&#30693;&#35782;&#30340;&#38382;&#31572;(KBQA)&#30340;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#22797;&#26434;&#30340;&#35757;&#32451;&#25216;&#26415;&#21644;&#27169;&#22411;&#26694;&#26550;&#65292;&#23548;&#33268;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#23384;&#22312;&#35768;&#22810;&#38480;&#21046;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20013;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;(ICL)&#33021;&#21147;&#30340;&#20986;&#29616;&#20026;KBQA&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#26080;&#38656;&#35757;&#32451;&#30340;&#35821;&#20041;&#35299;&#26512;&#33539;&#24335;&#65306;&#32473;&#23450;&#23569;&#37327;&#38382;&#39064;&#21450;&#20854;&#26631;&#35760;&#30340;&#36923;&#36753;&#34920;&#36798;&#24335;&#20316;&#20026;&#28436;&#31034;&#31034;&#20363;&#65292;LLMs&#33021;&#22815;&#29702;&#35299;&#20219;&#21153;&#24847;&#22270;&#24182;&#20026;&#26032;&#38382;&#39064;&#29983;&#25104;&#36923;&#36753;&#34920;&#36798;&#24335;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#24378;&#22823;&#30340;LLMs&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#36923;&#36753;&#34920;&#36798;&#24335;&#30340;&#20102;&#35299;&#24456;&#23569;&#65292;&#23548;&#33268;&#26684;&#24335;&#38169;&#35823;&#29575;&#36739;&#39640;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;KBQA&#30340;&#20195;&#30721;&#39118;&#26684;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#38476;&#29983;&#36923;&#36753;&#34920;&#36798;&#24335;&#30340;&#29983;&#25104;&#36807;&#31243;&#36716;&#25442;&#20026;&#26356;&#20026;&#29087;&#24713;&#30340;&#20195;&#30721;&#29983;&#25104;&#36807;&#31243;&#12290;&#23545;&#19977;&#20010;&#20027;&#27969;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#20943;&#36731;&#20102;&#29983;&#25104;&#36923;&#36753;&#34920;&#36798;&#24335;&#20013;&#30340;&#26684;&#24335;&#38169;&#35823;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current methods for Knowledge-Based Question Answering (KBQA) usually rely on complex training techniques and model frameworks, leading to many limitations in practical applications. Recently, the emergence of In-Context Learning (ICL) capabilities in Large Language Models (LLMs) provides a simple and training-free semantic parsing paradigm for KBQA: Given a small number of questions and their labeled logical forms as demo examples, LLMs can understand the task intent and generate the logic form for a new question. However, current powerful LLMs have little exposure to logic forms during pre-training, resulting in a high format error rate. To solve this problem, we propose a code-style in-context learning method for KBQA, which converts the generation process of unfamiliar logical form into the more familiar code generation process for LLMs. Experimental results on three mainstream datasets show that our method dramatically mitigated the formatting error problem in generating logic for
&lt;/p&gt;</description></item><item><title>&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36873;&#25321;&#20855;&#26377;&#20027;&#35266;&#24615;&#65292;&#27169;&#22411;&#36873;&#25321;&#32467;&#26524;&#21463;&#27169;&#22411;&#26500;&#24314;&#32773;&#20559;&#22909;&#30340;&#24433;&#21709;&#65292;&#24182;&#21487;&#33021;&#23548;&#33268;&#36873;&#25321;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#38656;&#35201;&#23545;&#27169;&#22411;&#36873;&#25321;&#36807;&#31243;&#36827;&#34892;&#26356;&#21152;&#28145;&#20837;&#30340;&#30740;&#31350;&#21644;&#26631;&#20934;&#21270;&#12290;</title><link>http://arxiv.org/abs/2309.00201</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36873;&#25321;&#20013;&#30340;&#20027;&#35266;&#24615;
&lt;/p&gt;
&lt;p&gt;
Subjectivity in Unsupervised Machine Learning Model Selection. (arXiv:2309.00201v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00201
&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36873;&#25321;&#20855;&#26377;&#20027;&#35266;&#24615;&#65292;&#27169;&#22411;&#36873;&#25321;&#32467;&#26524;&#21463;&#27169;&#22411;&#26500;&#24314;&#32773;&#20559;&#22909;&#30340;&#24433;&#21709;&#65292;&#24182;&#21487;&#33021;&#23548;&#33268;&#36873;&#25321;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#38656;&#35201;&#23545;&#27169;&#22411;&#36873;&#25321;&#36807;&#31243;&#36827;&#34892;&#26356;&#21152;&#28145;&#20837;&#30340;&#30740;&#31350;&#21644;&#26631;&#20934;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#36873;&#25321;&#26159;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#20013;&#24517;&#35201;&#30340;&#27493;&#39588;&#12290;&#23613;&#31649;&#26377;&#24456;&#22810;&#26631;&#20934;&#21644;&#25351;&#26631;&#65292;&#20294;&#27169;&#22411;&#36873;&#25321;&#20173;&#28982;&#23384;&#22312;&#20027;&#35266;&#24615;&#12290;&#39640;&#24230;&#20027;&#35266;&#24615;&#21487;&#33021;&#20250;&#23545;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#30340;&#37325;&#22797;&#24615;&#21644;&#21487;&#20877;&#29616;&#24615;&#20135;&#29983;&#30097;&#38382;&#65292;&#24182;&#23545;&#23454;&#38469;&#37096;&#32626;&#30340;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#20135;&#29983;&#24576;&#30097;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#36873;&#25321;&#32467;&#26524;&#20013;&#27169;&#22411;&#26500;&#24314;&#32773;&#30340;&#20559;&#22909;&#24433;&#21709;&#30340;&#24433;&#21709;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#26412;&#30740;&#31350;&#20197;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#20026;&#20363;&#65292;&#35843;&#26597;&#20102;&#27169;&#22411;&#36873;&#25321;&#20013;&#30340;&#20027;&#35266;&#24615;&#12290;&#25105;&#20204;&#36992;&#35831;&#20102;33&#20301;&#21442;&#19982;&#32773;&#21644;&#19977;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22312;&#19977;&#20010;&#22330;&#26223;&#20013;&#36827;&#34892;&#27169;&#22411;&#36873;&#25321;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#26080;&#35770;&#26159;&#21442;&#19982;&#32773;&#36824;&#26159;LLMs&#30340;&#36873;&#25321;&#37117;&#23384;&#22312;&#21464;&#24322;&#24615;&#21644;&#19981;&#19968;&#33268;&#24615;&#65292;&#23588;&#20854;&#26159;&#24403;&#19981;&#21516;&#30340;&#26631;&#20934;&#21644;&#25351;&#26631;&#23384;&#22312;&#20998;&#27495;&#26102;&#12290;&#20027;&#35266;&#24615;&#26469;&#28304;&#21253;&#25324;&#23545;&#19981;&#21516;&#26631;&#20934;&#21644;&#25351;&#26631;&#37325;&#35201;&#24615;&#30340;&#19981;&#21516;&#24847;&#35265;&#65292;&#23545;&#27169;&#22411;&#24212;&#35813;&#26377;&#22810;&#31616;&#27905;&#30340;&#19981;&#21516;&#30475;&#27861;&#65292;&#20197;&#21450;&#23545;&#25968;&#25454;&#35268;&#27169;&#30340;&#22823;&#23567;&#30340;&#30475;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model selection is a necessary step in unsupervised machine learning. Despite numerous criteria and metrics, model selection remains subjective. A high degree of subjectivity may lead to questions about repeatability and reproducibility of various machine learning studies and doubts about the robustness of models deployed in the real world. Yet, the impact of modelers' preferences on model selection outcomes remains largely unexplored. This study uses the Hidden Markov Model as an example to investigate the subjectivity involved in model selection. We asked 33 participants and three Large Language Models (LLMs) to make model selections in three scenarios. Results revealed variability and inconsistencies in both the participants' and the LLMs' choices, especially when different criteria and metrics disagree. Sources of subjectivity include varying opinions on the importance of different criteria and metrics, differing views on how parsimonious a model should be, and how the size of a da
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#39044;&#35757;&#32451;&#32593;&#32476;&#23454;&#29616;&#23616;&#37096;&#31283;&#23450;&#24615;&#65292;&#25299;&#23637;&#20102;&#24050;&#26377;&#30340;&#31283;&#23450;&#24615;&#29702;&#35770;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#22797;&#26434;&#32467;&#26500;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2308.12075</link><description>&lt;p&gt;
&#36890;&#36807;&#39044;&#35757;&#32451;&#31283;&#23450;RNN&#30340;&#26799;&#24230;
&lt;/p&gt;
&lt;p&gt;
Stabilizing RNN Gradients through Pre-training. (arXiv:2308.12075v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12075
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#39044;&#35757;&#32451;&#32593;&#32476;&#23454;&#29616;&#23616;&#37096;&#31283;&#23450;&#24615;&#65292;&#25299;&#23637;&#20102;&#24050;&#26377;&#30340;&#31283;&#23450;&#24615;&#29702;&#35770;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#22797;&#26434;&#32467;&#26500;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#30340;&#20247;&#22810;&#29702;&#35770;&#37117;&#24314;&#35758;&#36890;&#36807;&#38450;&#27490;&#26799;&#24230;&#30340;&#26041;&#24046;&#20197;&#25351;&#25968;&#24418;&#24335;&#38543;&#28145;&#24230;&#25110;&#26102;&#38388;&#22686;&#38271;&#26469;&#31283;&#23450;&#21644;&#25913;&#21892;&#35757;&#32451;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#20998;&#26512;&#26159;&#22312;&#21069;&#39304;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#25110;&#21333;&#23618;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#19978;&#36827;&#34892;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#25968;&#23398;&#30340;&#21487;&#35299;&#24615;&#12290;&#19982;&#27492;&#30456;&#21453;&#65292;&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#24403;&#20307;&#31995;&#32467;&#26500;&#36807;&#20110;&#22797;&#26434;&#20197;&#33267;&#20110;&#26080;&#27861;&#36827;&#34892;&#35299;&#26512;&#21021;&#22987;&#21270;&#26102;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#32593;&#32476;&#23454;&#29616;&#23616;&#37096;&#31283;&#23450;&#24615;&#26159;&#26377;&#25928;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#24050;&#30693;&#30340;&#31283;&#23450;&#24615;&#29702;&#35770;&#65292;&#28085;&#30422;&#20102;&#19968;&#20010;&#26356;&#24191;&#27867;&#30340;&#28145;&#23618;&#24490;&#29615;&#32593;&#32476;&#23478;&#26063;&#65292;&#23545;&#25968;&#25454;&#21644;&#21442;&#25968;&#20998;&#24067;&#30340;&#35201;&#27714;&#36739;&#23569;&#65292;&#36825;&#20010;&#29702;&#35770;&#34987;&#31216;&#20026;&#23616;&#37096;&#31283;&#23450;&#24615;&#26465;&#20214;&#65288;LSC&#65289;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#21457;&#29616;&#65292;&#32463;&#20856;&#30340;Glorot&#12289;He&#21644;&#27491;&#20132;&#21021;&#22987;&#21270;&#26041;&#26696;&#22312;&#24212;&#29992;&#20110;&#21069;&#39304;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#26102;&#21487;&#20197;&#28385;&#36275;LSC&#12290;&#28982;&#32780;&#65292;&#22312;&#23545;&#28145;&#23618;&#24490;&#29615;&#32593;&#32476;&#36827;&#34892;&#20998;&#26512;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#31181;&#26032;&#30340;&#25351;&#25968;&#22686;&#38271;&#26469;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Numerous theories of learning suggest to prevent the gradient variance from exponential growth with depth or time, to stabilize and improve training. Typically, these analyses are conducted on feed-forward fully-connected neural networks or single-layer recurrent neural networks, given their mathematical tractability. In contrast, this study demonstrates that pre-training the network to local stability can be effective whenever the architectures are too complex for an analytical initialization. Furthermore, we extend known stability theories to encompass a broader family of deep recurrent networks, requiring minimal assumptions on data and parameter distribution, a theory that we refer to as the Local Stability Condition (LSC). Our investigation reveals that the classical Glorot, He, and Orthogonal initialization schemes satisfy the LSC when applied to feed-forward fully-connected neural networks. However, analysing deep recurrent networks, we identify a new additive source of exponent
&lt;/p&gt;</description></item><item><title>&#20256;&#32479;&#30340;&#31526;&#21495;AI&#26041;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;AI&#26041;&#27861;&#26080;&#27861;&#28385;&#36275;&#21019;&#24314;&#24378;&#22823;&#21644;&#21487;&#20449;&#36182;&#30340;AI&#30340;&#25361;&#25112;&#65292;&#28982;&#32780;&#65292;&#21457;&#23637;&#33073;&#38772;&#27861;&#36890;&#36807;&#27169;&#20223;&#20154;&#31867;&#20799;&#31461;&#30340;&#33021;&#21147;&#21457;&#23637;&#36807;&#31243;&#65292;&#20026;&#21019;&#24314;&#31283;&#20581;&#21487;&#38752;&#30340;AI&#25552;&#20379;&#20102;&#24076;&#26395;&#12290;</title><link>http://arxiv.org/abs/2308.04586</link><description>&lt;p&gt;
AIs&#30340;&#21457;&#23637;&#33073;&#38772;&#27861;
&lt;/p&gt;
&lt;p&gt;
Developmental Bootstrapping of AIs. (arXiv:2308.04586v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04586
&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#31526;&#21495;AI&#26041;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;AI&#26041;&#27861;&#26080;&#27861;&#28385;&#36275;&#21019;&#24314;&#24378;&#22823;&#21644;&#21487;&#20449;&#36182;&#30340;AI&#30340;&#25361;&#25112;&#65292;&#28982;&#32780;&#65292;&#21457;&#23637;&#33073;&#38772;&#27861;&#36890;&#36807;&#27169;&#20223;&#20154;&#31867;&#20799;&#31461;&#30340;&#33021;&#21147;&#21457;&#23637;&#36807;&#31243;&#65292;&#20026;&#21019;&#24314;&#31283;&#20581;&#21487;&#38752;&#30340;AI&#25552;&#20379;&#20102;&#24076;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24403;&#21069;&#19968;&#20123;AI&#22312;&#23553;&#38381;&#30340;&#19990;&#30028;&#65292;&#22914;&#26827;&#30424;&#28216;&#25103;&#20013;&#36229;&#36234;&#20102;&#20154;&#31867;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#28151;&#20081;&#30340;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#34920;&#29616;&#26377;&#38480;&#12290;&#23427;&#20204;&#20250;&#29359;&#22855;&#24618;&#30340;&#38169;&#35823;&#32780;&#19988;&#27809;&#26377;&#24847;&#35782;&#21040;&#12290;&#23427;&#20204;&#24456;&#38590;&#21463;&#21040;&#25351;&#23548;&#65292;&#19981;&#33021;&#36816;&#29992;&#24120;&#35782;&#65292;&#32570;&#20047;&#22909;&#22855;&#24515;&#12290;&#23427;&#20204;&#19981;&#33021;&#25104;&#20026;&#33391;&#22909;&#30340;&#21512;&#20316;&#32773;&#12290;&#20256;&#32479;&#25163;&#21160;&#26500;&#24314;&#30340;&#31526;&#21495;AI&#26041;&#27861;&#26500;&#24314;&#30340;&#31995;&#32479;&#21644;&#20351;&#29992;&#29983;&#25104;&#21644;&#28145;&#24230;&#23398;&#20064;AI&#26041;&#27861;(&#21253;&#25324;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;)&#26500;&#24314;&#30340;&#31995;&#32479;&#37117;&#26080;&#27861;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;&#23427;&#20204;&#19981;&#36866;&#21512;&#21019;&#24314;&#24378;&#22823;&#21644;&#21487;&#20449;&#36182;&#30340;AI&#12290;&#23613;&#31649;&#27492;&#26041;&#27861;&#19981;&#23646;&#20110;&#20027;&#27969;&#30340;AI&#26041;&#27861;&#65292;&#20294;&#21457;&#23637;&#33073;&#38772;&#27861;&#26174;&#31034;&#20986;&#24076;&#26395;&#12290;&#22312;&#21457;&#23637;&#33073;&#38772;&#27861;&#20013;&#65292;AI&#20687;&#20154;&#31867;&#20799;&#31461;&#19968;&#26679;&#21457;&#23637;&#33021;&#21147;&#12290;&#23427;&#20204;&#20174;&#20808;&#22825;&#33021;&#21147;&#24320;&#22987;&#12290;&#20687;&#20154;&#31867;&#19968;&#26679;&#65292;&#23427;&#20204;&#19982;&#29615;&#22659;&#20114;&#21160;&#65292;&#24182;&#20174;&#20114;&#21160;&#20013;&#23398;&#20064;&#12290;&#23427;&#20204;&#36890;&#36807;&#33258;&#25105;&#21457;&#23637;&#30340;&#33021;&#21147;&#36880;&#27493;&#25193;&#23637;&#20808;&#22825;&#33021;&#21147;&#12290;&#23427;&#20204;&#20114;&#21160;&#24182;&#36880;&#28176;&#23558;&#25152;&#23398;&#24212;&#29992;&#20110;&#23454;&#38469;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although some current AIs surpass human abilities especially in closed worlds such as board games, their performance in the messy real world is limited. They make strange mistakes and do not notice them. They cannot be instructed easily, fail to use common sense, and lack curiosity. They do not make good collaborators. Neither systems built using the traditional manually-constructed symbolic AI approach nor systems built using generative and deep learning AI approaches including large language models (LLMs) can meet the challenges. They are not well suited for creating robust and trustworthy AIs. Although it is outside of mainstream AI approaches, developmental bootstrapping shows promise. In developmental bootstrapping, AIs develop competences like human children do. They start with innate competences. Like humans, they interact with the environment and learn from their interactions. They incrementally extend their innate competences with self-developed competences. They interact and 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#24102;&#26377;&#22270;&#35770;&#32422;&#26463;&#30340;&#32972;&#21253;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#24182;&#25552;&#20986;&#20102;&#36817;&#20284;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.12547</link><description>&lt;p&gt;
&#32972;&#21253;&#38382;&#39064;&#65306;&#36830;&#36890;&#24615;&#12289;&#36335;&#24452;&#21644;&#26368;&#30701;&#36335;&#24452;
&lt;/p&gt;
&lt;p&gt;
Knapsack: Connectedness, Path, and Shortest-Path. (arXiv:2307.12547v2 [cs.DS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12547
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#24102;&#26377;&#22270;&#35770;&#32422;&#26463;&#30340;&#32972;&#21253;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#24182;&#25552;&#20986;&#20102;&#36817;&#20284;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#24102;&#26377;&#22270;&#35770;&#32422;&#26463;&#30340;&#32972;&#21253;&#38382;&#39064;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#25105;&#20204;&#20551;&#35774;&#32972;&#21253;&#39033;&#30446;&#38598;&#19978;&#23384;&#22312;&#19968;&#20010;&#22270;&#32467;&#26500;&#65292;&#24182;&#19988;&#35299;&#20915;&#26041;&#26696;&#36824;&#38656;&#35201;&#28385;&#36275;&#32972;&#21253;&#32422;&#26463;&#20043;&#19978;&#30340;&#26576;&#20123;&#22270;&#35770;&#24615;&#36136;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;&#36830;&#36890;&#32972;&#21253;&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#38656;&#35201;&#35745;&#31639;&#19968;&#20010;&#36830;&#36890;&#30340;&#39033;&#30446;&#23376;&#38598;&#65292;&#20854;&#20215;&#20540;&#26368;&#22823;&#65292;&#19988;&#28385;&#36275;&#32972;&#21253;&#32422;&#26463;&#30340;&#22823;&#23567;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#21363;&#20351;&#23545;&#20110;&#26368;&#22823;&#24230;&#20026;&#22235;&#30340;&#22270;&#65292;&#36825;&#20010;&#38382;&#39064;&#20063;&#26159;&#24378;NP&#23436;&#20840;&#30340;&#65292;&#23545;&#20110;&#26143;&#24418;&#22270;&#65292;&#36825;&#20010;&#38382;&#39064;&#20063;&#26159;NP&#23436;&#20840;&#30340;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#36816;&#34892;&#26102;&#38388;&#20026;$O\left(2^{tw\log tw}\cdot\text{poly}(\min\{s^2,d^2\})\right)$&#30340;&#31639;&#27861;&#65292;&#22312;&#20854;&#20013;$tw,s,d$&#20998;&#21035;&#26159;&#22270;&#30340;&#26641;&#23485;&#24230;&#65292;&#32972;&#21253;&#30340;&#22823;&#23567;&#21644;&#30446;&#26631;&#20540;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#19968;&#20010;$(1-\epsilon)$&#36817;&#20284;&#31639;&#27861;&#65292;&#20854;&#36816;&#34892;&#26102;&#38388;&#20026;$O\left(2^{tw\log tw}\cdot\text{poly}(n,1/\epsilon)\right)$&#65292;&#23545;&#20110;&#27599;&#20010;$\epsilon&gt;0$&#37117;&#25104;&#31435;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#23545;&#20960;&#20010;&#20854;&#20182;&#22270;&#35770;&#24615;&#36136;&#30340;&#31867;&#20284;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the knapsack problem with graph theoretic constraints. That is, we assume that there exists a graph structure on the set of items of knapsack and the solution also needs to satisfy certain graph theoretic properties on top of knapsack constraints. In particular, we need to compute in the connected knapsack problem a connected subset of items which has maximum value subject to the size of knapsack constraint. We show that this problem is strongly NP-complete even for graphs of maximum degree four and NP-complete even for star graphs. On the other hand, we develop an algorithm running in time $O\left(2^{tw\log tw}\cdot\text{poly}(\min\{s^2,d^2\})\right)$ where $tw,s,d$ are respectively treewidth of the graph, size, and target value of the knapsack. We further exhibit a $(1-\epsilon)$ factor approximation algorithm running in time $O\left(2^{tw\log tw}\cdot\text{poly}(n,1/\epsilon)\right)$ for every $\epsilon&gt;0$. We show similar results for several other graph theoretic propertie
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26641;&#25628;&#32034;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#35821;&#27861;&#24341;&#23548;&#32508;&#21512;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#25628;&#32034;&#38382;&#39064;&#22797;&#26434;&#24615;&#21644;&#25968;&#25454;&#38598;&#36739;&#23567;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.09564</link><description>&lt;p&gt;
&#35821;&#27861;&#24341;&#23548;&#32508;&#21512;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning for Syntax-Guided Synthesis. (arXiv:2307.09564v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26641;&#25628;&#32034;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#35821;&#27861;&#24341;&#23548;&#32508;&#21512;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#25628;&#32034;&#38382;&#39064;&#22797;&#26434;&#24615;&#21644;&#25968;&#25454;&#38598;&#36739;&#23567;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31243;&#24207;&#32508;&#21512;&#26159;&#26681;&#25454;&#35268;&#33539;&#33258;&#21160;&#29983;&#25104;&#20195;&#30721;&#30340;&#20219;&#21153;&#12290;&#22312;&#35821;&#27861;&#24341;&#23548;&#32508;&#21512;&#65288;SyGuS&#65289;&#20013;&#65292;&#35268;&#33539;&#26159;&#19968;&#20010;&#35821;&#27861;&#27169;&#26495;&#21644;&#19968;&#20010;&#36923;&#36753;&#20844;&#24335;&#30340;&#32452;&#21512;&#65292;&#29983;&#25104;&#30340;&#20195;&#30721;&#34987;&#35777;&#26126;&#28385;&#36275;&#35268;&#33539;&#12290;&#20687;SyGuS&#36825;&#26679;&#30340;&#25216;&#26415;&#23545;&#20110;&#30830;&#20445;&#27491;&#30830;&#30340;&#32508;&#21512;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#26426;&#22120;&#23398;&#20064;&#22312;&#20854;&#20182;&#31867;&#22411;&#30340;&#31243;&#24207;&#32508;&#21512;&#20013;&#24050;&#32463;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#22312;SyGuS&#20013;&#30446;&#21069;&#30340;&#25216;&#26415;&#20173;&#28982;&#20027;&#35201;&#20381;&#36182;&#33258;&#21160;&#25512;&#29702;&#24037;&#20855;&#21644;&#31616;&#21333;&#30340;&#26522;&#20030;&#26041;&#27861;&#12290;&#25105;&#20204;&#20551;&#35774;&#36825;&#26159;&#30001;&#20110;&#25628;&#32034;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#21644;&#21487;&#29992;&#25968;&#25454;&#38598;&#30456;&#23545;&#36739;&#23567;&#30340;&#21407;&#22240;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#36890;&#29992;SyGuS&#38382;&#39064;&#26500;&#24314;&#20026;&#26641;&#25628;&#32034;&#65292;&#24182;&#22522;&#20110;Monte-Carlo Tree Search (MCTS)&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#24341;&#23548;&#30340;SyGuS&#32508;&#21512;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#32467;&#21512;&#20102;&#23398;&#20064;&#30340;&#31574;&#30053;&#21644;&#20540;&#20989;&#25968;&#19982;&#29992;&#20110;&#24179;&#34913;&#25506;&#32034;&#21644;&#21033;&#29992;&#30340;&#26641;&#30340;&#19978;&#38480;&#32622;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Program synthesis is the task of automatically generating code based on a specification. In Syntax-Guided Synthesis(SyGuS) this specification is a combination of a syntactic template and a logical formula, and any generated code is proven to satisfy both. Techniques like SyGuS are critical to guaranteeing correct synthesis results. Despite the proliferation of machine learning in other types of program synthesis, state-of-the-art techniques in SyGuS are still driven by automated reasoning tools and simple enumeration. We hypothesize this is for two reasons: first the complexity of the search problem, and second the relatively small data sets available. In this work, we tackle these challenges by framing general SyGuS problems as a tree-search, and present a reinforcement learning guided synthesis algorithm for SyGuS based on Monte-Carlo Tree Search (MCTS). Our algorithm incorporates learned policy and value functions combined with the upper confidence bound for trees to balance explora
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#23545;GPT&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#21487;&#20449;&#24230;&#35780;&#20272;&#65292;&#32771;&#34385;&#20102;&#22810;&#20010;&#26041;&#38754;&#30340;&#39118;&#38505;&#65292;&#21457;&#29616;&#20102;&#20197;&#21069;&#26410;&#20844;&#24320;&#30340;&#23041;&#32961;&#28431;&#27934;&#65292;&#20363;&#22914;&#23545;&#27602;&#24615;&#36755;&#20986;&#21644;&#20010;&#20154;&#20449;&#24687;&#27844;&#28431;&#30340;&#26131;&#34987;&#35823;&#23548;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.11698</link><description>&lt;p&gt;
DecodingTrust: GPT&#27169;&#22411;&#30340;&#20840;&#38754;&#21487;&#20449;&#24230;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models. (arXiv:2306.11698v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11698
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#23545;GPT&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#21487;&#20449;&#24230;&#35780;&#20272;&#65292;&#32771;&#34385;&#20102;&#22810;&#20010;&#26041;&#38754;&#30340;&#39118;&#38505;&#65292;&#21457;&#29616;&#20102;&#20197;&#21069;&#26410;&#20844;&#24320;&#30340;&#23041;&#32961;&#28431;&#27934;&#65292;&#20363;&#22914;&#23545;&#27602;&#24615;&#36755;&#20986;&#21644;&#20010;&#20154;&#20449;&#24687;&#27844;&#28431;&#30340;&#26131;&#34987;&#35823;&#23548;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#65288;GPT&#65289;&#27169;&#22411;&#22312;&#20854;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#20852;&#22859;&#30340;&#36827;&#23637;&#65292;&#24341;&#36215;&#20102;&#20174;&#20174;&#19994;&#32773;&#21040;&#20844;&#20247;&#30340;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#20851;&#20110;GPT&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#30340;&#25991;&#29486;&#20173;&#28982;&#26377;&#38480;&#65292;&#20174;&#19994;&#32773;&#20204;&#25552;&#35758;&#23558;&#24378;&#22823;&#30340;GPT&#27169;&#22411;&#29992;&#20110;&#25935;&#24863;&#24212;&#29992;&#65292;&#22914;&#21307;&#30103;&#20445;&#20581;&#21644;&#37329;&#34701;&#39046;&#22495;&#65292;&#20854;&#20013;&#38169;&#35823;&#21487;&#33021;&#20195;&#20215;&#39640;&#26114;&#12290;&#20026;&#27492;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#37325;&#28857;&#25918;&#22312;GPT-4&#21644;GPT-3.5&#19978;&#65289;&#36827;&#34892;&#20840;&#38754;&#30340;&#21487;&#20449;&#24230;&#35780;&#20272;&#65292;&#32771;&#34385;&#20102;&#22810;&#26679;&#30340;&#35266;&#28857; - &#21253;&#25324;&#26377;&#27602;&#24615;&#12289;&#38472;&#35268;&#20559;&#35265;&#12289;&#23545;&#25239;&#24378;&#24230;&#12289;&#36229;&#20986;&#20998;&#24067;&#30340;&#24378;&#24230;&#12289;&#23545;&#25239;&#31034;&#33539;&#30340;&#24378;&#24230;&#12289;&#38544;&#31169;&#12289;&#26426;&#22120;&#20262;&#29702;&#21644;&#20844;&#24179;&#24615;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#20197;&#21069;&#26410;&#20844;&#24320;&#30340;&#21487;&#20449;&#24230;&#23041;&#32961;&#28431;&#27934;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#21457;&#29616;GPT&#27169;&#22411;&#21487;&#20197;&#36731;&#26494;&#34987;&#35823;&#23548;&#29983;&#25104;&#26377;&#27602;&#21644;&#20559;&#35265;&#30340;&#36755;&#20986;&#65292;&#24182;&#22312;&#35757;&#32451;&#25968;&#25454;&#21644;&#19978;&#19979;&#25991;&#20013;&#27844;&#28431;&#31169;&#20154;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Pre-trained Transformer (GPT) models have exhibited exciting progress in their capabilities, capturing the interest of practitioners and the public alike. Yet, while the literature on the trustworthiness of GPT models remains limited, practitioners have proposed employing capable GPT models for sensitive applications such as healthcare and finance -- where mistakes can be costly. To this end, this work proposes a comprehensive trustworthiness evaluation for large language models with a focus on GPT-4 and GPT-3.5, considering diverse perspectives -- including toxicity, stereotype bias, adversarial robustness, out-of-distribution robustness, robustness on adversarial demonstrations, privacy, machine ethics, and fairness. Based on our evaluations, we discover previously unpublished vulnerabilities to trustworthiness threats. For instance, we find that GPT models can be easily misled to generate toxic and biased outputs and leak private information in both training data and conv
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#30446;&#21069;AI&#31995;&#32479;&#24320;&#21457;&#20013;&#32570;&#23569;&#35201;&#27714;&#24037;&#31243;&#65288;RE&#65289;&#36825;&#19968;&#29615;&#33410;&#19988;&#20262;&#29702;&#25351;&#21335;&#26415;&#35821;&#21644;&#21407;&#21017;&#35206;&#30422;&#30340;&#19981;&#19968;&#33268;&#24615;&#65292;&#20026;&#35299;&#20915;&#35813;&#38382;&#39064;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#20010;&#26415;&#35821;&#34920;&#24182;&#30740;&#31350;&#20102;&#20262;&#29702;AI&#24320;&#21457;&#26694;&#26550;&#22312;&#25191;&#34892;RE&#26041;&#38754;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.01774</link><description>&lt;p&gt;
&#38754;&#21521;&#21487;&#20449;&#33258;&#21160;&#31995;&#32479;&#24320;&#21457;&#30340;RE&#20013;&#24515;&#21270;&#24314;&#35758;
&lt;/p&gt;
&lt;p&gt;
RE-centric Recommendations for the Development of Trustworthy(er) Autonomous Systems. (arXiv:2306.01774v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01774
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#30446;&#21069;AI&#31995;&#32479;&#24320;&#21457;&#20013;&#32570;&#23569;&#35201;&#27714;&#24037;&#31243;&#65288;RE&#65289;&#36825;&#19968;&#29615;&#33410;&#19988;&#20262;&#29702;&#25351;&#21335;&#26415;&#35821;&#21644;&#21407;&#21017;&#35206;&#30422;&#30340;&#19981;&#19968;&#33268;&#24615;&#65292;&#20026;&#35299;&#20915;&#35813;&#38382;&#39064;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#20010;&#26415;&#35821;&#34920;&#24182;&#30740;&#31350;&#20102;&#20262;&#29702;AI&#24320;&#21457;&#26694;&#26550;&#22312;&#25191;&#34892;RE&#26041;&#38754;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27431;&#30431;&#20869;&#24320;&#21457;&#21644;&#23454;&#26045;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#26102;&#31526;&#21512;&#27431;&#30431;AI&#27861;&#26696;&#65288;AIA&#65289;&#25351;&#21335;&#23558;&#24456;&#24555;&#26159;&#24378;&#21046;&#24615;&#30340;&#12290;&#28982;&#32780;&#65292;&#20174;&#34892;&#21160;&#25351;&#21335;&#26041;&#38754;&#65292;&#23454;&#36341;&#32773;&#32570;&#20047;&#22312;AI&#31995;&#32479;&#24320;&#21457;&#26399;&#38388;&#23454;&#26045;&#20262;&#29702;&#30340;&#21487;&#25805;&#20316;&#35828;&#26126;&#12290;&#23545;&#19981;&#21516;&#20262;&#29702;&#25351;&#21335;&#30340;&#25991;&#29486;&#32508;&#36848;&#25581;&#31034;&#20102;&#25152;&#28041;&#21450;&#21407;&#21017;&#21644;&#26415;&#35821;&#25551;&#36848;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#27492;&#22806;&#65292;&#35201;&#22312;AI&#24320;&#21457;&#36807;&#31243;&#30340;&#26089;&#26399;&#38454;&#27573;&#22521;&#20859;&#20449;&#20219;&#30340;&#35201;&#27714;&#24037;&#31243;&#65288;RE&#65289;&#34987;&#35777;&#26126;&#22312;&#35768;&#22810;&#25903;&#25345;&#36947;&#24503;&#21644;&#21487;&#20449;AI&#24320;&#21457;&#30340;&#26694;&#26550;&#20013;&#32570;&#22833;&#12290;&#36825;&#31181;&#19981;&#21327;&#35843;&#30340;&#25514;&#36766;&#21152;&#19978;&#32570;&#20047;&#20855;&#20307;&#30340;&#24320;&#21457;&#23454;&#36341;&#20351;&#21487;&#20449;AI&#24320;&#21457;&#26356;&#21152;&#22256;&#38590;&#12290;&#20026;&#35299;&#20915;&#27492;&#38382;&#39064;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#20010;&#26415;&#35821;&#34920;&#65292;&#29992;&#20110;&#27604;&#36739;&#20027;&#35201;&#20262;&#29702;AI&#25351;&#21335;&#20013;&#20351;&#29992;&#30340;&#26415;&#35821;&#21644;&#20262;&#29702;AI&#21407;&#21017;&#30340;&#35206;&#30422;&#33539;&#22260;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20262;&#29702;AI&#24320;&#21457;&#26694;&#26550;&#22312;&#25191;&#34892;RE&#26041;&#38754;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Complying with the EU AI Act (AIA) guidelines while developing and implementing AI systems will soon be mandatory within the EU. However, practitioners lack actionable instructions to operationalise ethics during AI systems development. A literature review of different ethical guidelines revealed inconsistencies in the principles addressed and the terminology used to describe them. Furthermore, requirements engineering (RE), which is identified to foster trustworthiness in the AI development process from the early stages was observed to be absent in a lot of frameworks that support the development of ethical and trustworthy AI. This incongruous phrasing combined with a lack of concrete development practices makes trustworthy AI development harder. To address this concern, we formulated a comparison table for the terminology used and the coverage of the ethical AI principles in major ethical AI guidelines. We then examined the applicability of ethical AI development frameworks for perfo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;DeepMerge&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21306;&#22495;&#21512;&#24182;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#20998;&#21106;&#38382;&#39064;&#65292;&#24182;&#25104;&#21151;&#20351;&#29992;&#36828;&#31243;&#24863;&#30693;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.19787</link><description>&lt;p&gt;
DeepMerge: &#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21306;&#22495;&#21512;&#24182;&#26041;&#27861;&#29992;&#20110;&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
DeepMerge: Deep Learning-Based Region-Merging for Image Segmentation. (arXiv:2305.19787v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19787
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;DeepMerge&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21306;&#22495;&#21512;&#24182;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#20998;&#21106;&#38382;&#39064;&#65292;&#24182;&#25104;&#21151;&#20351;&#29992;&#36828;&#31243;&#24863;&#30693;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22270;&#20687;&#20998;&#26512;&#20013;&#65292;&#20174;&#39640;&#20998;&#36776;&#29575;&#36965;&#24863;&#22270;&#20687;&#20013;&#31934;&#30830;&#22320;&#20998;&#21106;&#22823;&#21306;&#22495;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#26377;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#26041;&#27861;&#37117;&#21463;&#21040;&#23545;&#35937;&#23610;&#23544;&#24040;&#22823;&#21464;&#21270;&#21644;&#23610;&#24230;&#36873;&#25321;&#22256;&#38590;&#30340;&#24433;&#21709;&#65292;&#32463;&#24120;&#23548;&#33268;&#20998;&#21106;&#31934;&#24230;&#19981;&#20339;&#12290;&#20026;&#20102;&#24212;&#23545;&#19978;&#36848;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21306;&#22495;&#21512;&#24182;&#26041;&#27861;&#65288;DeepMerge&#65289;&#65292;&#36890;&#36807;&#23558;Transformers&#12289;&#22810;&#32423;&#23884;&#20837;&#27169;&#22359;&#12289;&#22522;&#20110;&#27573;&#30340;&#29305;&#24449;&#23884;&#20837;&#27169;&#22359;&#21644;&#21306;&#22495;&#30456;&#37051;&#22270;&#27169;&#22411;&#30456;&#32467;&#21512;&#26469;&#22788;&#29702;&#22823;&#30340;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#20998;&#21106;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#20108;&#21449;&#26641;&#37319;&#26679;&#26041;&#27861;&#65292;&#20197;&#29983;&#25104;&#22810;&#32423;&#36755;&#20837;&#65292;&#29992;&#20110;DeepMerge&#27169;&#22411;&#30340;&#36755;&#20837;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#26368;&#20339;&#30693;&#35782;&#65292;&#25552;&#20986;&#30340;&#26041;&#27861;&#26159;&#31532;&#19968;&#20010;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#23398;&#20064;&#30456;&#37051;&#27573;&#20043;&#38388;&#30456;&#20284;&#24615;&#36827;&#34892;&#21306;&#22495;&#21512;&#24182;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#36828;&#31243;&#24863;&#30693;&#25968;&#25454;&#38598;&#23545;&#25152;&#25552;&#20986;&#30340;DeepMerge&#26041;&#27861;&#36827;&#34892;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate segmentation of large areas from very high spatial-resolution (VHR) remote sensing imagery remains a challenging issue in image analysis. Existing supervised and unsupervised methods both suffer from the large variance of object sizes and the difficulty in scale selection, which often result in poor segmentation accuracies. To address the above challenges, we propose a deep learning-based region-merging method (DeepMerge) to handle the segmentation in large VHR images by integrating a Transformer with a multi-level embedding module, a segment-based feature embedding module and a region-adjacency graph model. In addition, we propose a modified binary tree sampling method to generate multi-level inputs from initial segmentation results, serving as inputs for the DeepMerge model. To our best knowledge, the proposed method is the first to use deep learning to learn the similarity between adjacent segments for region-merging. The proposed DeepMerge method is validated using a remot
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30452;&#25509;&#20248;&#21270;&#25193;&#25955;&#27169;&#22411;&#20197;&#23454;&#29616;&#19979;&#28216;&#23545;&#35937;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#31216;&#20043;&#20026;&#21435;&#22122;&#25193;&#25955;&#31574;&#30053;&#20248;&#21270;&#65288;DDPO&#65289;&#30340;&#26377;&#25928;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#33021;&#22815;&#36866;&#24212;&#38590;&#20197;&#36890;&#36807;&#25552;&#31034;&#34920;&#36798;&#30340;&#22270;&#20687;&#21387;&#32553;&#31561;&#30446;&#26631;&#65292;&#20197;&#21450;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#24471;&#20986;&#30340;&#32654;&#23398;&#36136;&#37327;&#31561;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2305.13301</link><description>&lt;p&gt;
&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Training Diffusion Models with Reinforcement Learning. (arXiv:2305.13301v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13301
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30452;&#25509;&#20248;&#21270;&#25193;&#25955;&#27169;&#22411;&#20197;&#23454;&#29616;&#19979;&#28216;&#23545;&#35937;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#31216;&#20043;&#20026;&#21435;&#22122;&#25193;&#25955;&#31574;&#30053;&#20248;&#21270;&#65288;DDPO&#65289;&#30340;&#26377;&#25928;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#33021;&#22815;&#36866;&#24212;&#38590;&#20197;&#36890;&#36807;&#25552;&#31034;&#34920;&#36798;&#30340;&#22270;&#20687;&#21387;&#32553;&#31561;&#30446;&#26631;&#65292;&#20197;&#21450;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#24471;&#20986;&#30340;&#32654;&#23398;&#36136;&#37327;&#31561;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26159;&#19968;&#31867;&#28789;&#27963;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#37319;&#29992;&#23545;&#25968;&#20284;&#28982;&#30446;&#26631;&#30340;&#36817;&#20284;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#25193;&#25955;&#27169;&#22411;&#30340;&#20351;&#29992;&#26696;&#20363;&#24182;&#19981;&#20851;&#27880;&#20284;&#28982;&#65292;&#32780;&#26159;&#20851;&#27880;&#20154;&#31867;&#24863;&#30693;&#30340;&#22270;&#20687;&#36136;&#37327;&#25110;&#33647;&#29289;&#25928;&#21147;&#31561;&#19979;&#28216;&#30446;&#26631;&#12290;&#26412;&#25991;&#30740;&#31350;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30452;&#25509;&#20248;&#21270;&#25193;&#25955;&#27169;&#22411;&#20197;&#23454;&#29616;&#27492;&#31867;&#30446;&#26631;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#23558;&#21435;&#22122;&#35270;&#20026;&#22810;&#27493;&#20915;&#31574;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#31216;&#20043;&#20026;&#21435;&#22122;&#25193;&#25955;&#31574;&#30053;&#20248;&#21270;&#65288;DDPO&#65289;&#30340;&#19968;&#31867;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#30456;&#23545;&#20110;&#26367;&#20195;&#30340;&#22870;&#21169;&#21152;&#26435;&#20284;&#28982;&#26041;&#27861;&#26356;&#20026;&#26377;&#25928;&#12290;&#22312;&#23454;&#35777;&#30740;&#31350;&#20013;&#65292;DDPO&#33021;&#22815;&#36866;&#24212;&#38590;&#20197;&#36890;&#36807;&#25552;&#31034;&#34920;&#36798;&#30340;&#22270;&#20687;&#21387;&#32553;&#31561;&#30446;&#26631;&#65292;&#20197;&#21450;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#24471;&#20986;&#30340;&#32654;&#23398;&#36136;&#37327;&#31561;&#30446;&#26631;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;DDPO&#21487;&#20197;&#21033;&#29992;&#26469;&#33258;&#21453;&#39304;&#30340;&#25552;&#31034;-&#22270;&#20687;&#23545;&#40784;&#26041;&#24335;&#26469;&#36827;&#34892;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models are a class of flexible generative models trained with an approximation to the log-likelihood objective. However, most use cases of diffusion models are not concerned with likelihoods, but instead with downstream objectives such as human-perceived image quality or drug effectiveness. In this paper, we investigate reinforcement learning methods for directly optimizing diffusion models for such objectives. We describe how posing denoising as a multi-step decision-making problem enables a class of policy gradient algorithms, which we refer to as denoising diffusion policy optimization (DDPO), that are more effective than alternative reward-weighted likelihood approaches. Empirically, DDPO is able to adapt text-to-image diffusion models to objectives that are difficult to express via prompting, such as image compressibility, and those derived from human feedback, such as aesthetic quality. Finally, we show that DDPO can improve prompt-image alignment using feedback from a 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#37325;&#26368;&#20248;&#20256;&#36755;&#26631;&#31614;&#20998;&#37197;(DOTLA)&#26694;&#26550;&#65292;&#20197;&#21516;&#26102;&#23558;&#19968;&#20010;&#27169;&#24577;&#20013;&#29983;&#25104;&#30340;&#26631;&#31614;&#20998;&#37197;&#32473;&#20854;&#23545;&#24212;&#30340;&#27169;&#24577;&#65292;&#23454;&#29616;&#26080;&#30417;&#30563;&#21487;&#35265;-&#32418;&#22806;&#20154;&#21592;&#20877;&#35782;&#21035;&#12290;&#22312;&#30456;&#24212;&#27169;&#24577;&#20013;&#37051;&#23621;&#26679;&#26412;&#30340;&#25351;&#23548;&#19979;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#36328;&#27169;&#24577;&#37051;&#23621;&#19968;&#33268;&#24615;&#24341;&#23548;&#30340;&#26631;&#31614;&#31934;&#28860;&#21644;&#27491;&#21017;&#21270;&#27169;&#22359;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#31639;&#27861;&#30340;&#31934;&#24230;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.12711</link><description>&lt;p&gt;
&#36890;&#36807;&#37051;&#23621;&#24341;&#23548;&#30340;&#26631;&#31614;&#31934;&#28860;&#21327;&#21516;&#23398;&#20064;&#23454;&#29616;&#26080;&#30417;&#30563;&#21487;&#35265;-&#32418;&#22806;&#20154;&#21592;&#20877;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Visible-Infrared Person ReID by Collaborative Learning with Neighbor-Guided Label Refinement. (arXiv:2305.12711v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12711
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#37325;&#26368;&#20248;&#20256;&#36755;&#26631;&#31614;&#20998;&#37197;(DOTLA)&#26694;&#26550;&#65292;&#20197;&#21516;&#26102;&#23558;&#19968;&#20010;&#27169;&#24577;&#20013;&#29983;&#25104;&#30340;&#26631;&#31614;&#20998;&#37197;&#32473;&#20854;&#23545;&#24212;&#30340;&#27169;&#24577;&#65292;&#23454;&#29616;&#26080;&#30417;&#30563;&#21487;&#35265;-&#32418;&#22806;&#20154;&#21592;&#20877;&#35782;&#21035;&#12290;&#22312;&#30456;&#24212;&#27169;&#24577;&#20013;&#37051;&#23621;&#26679;&#26412;&#30340;&#25351;&#23548;&#19979;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#36328;&#27169;&#24577;&#37051;&#23621;&#19968;&#33268;&#24615;&#24341;&#23548;&#30340;&#26631;&#31614;&#31934;&#28860;&#21644;&#27491;&#21017;&#21270;&#27169;&#22359;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#31639;&#27861;&#30340;&#31934;&#24230;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#23398;&#20064;&#21487;&#35265;-&#32418;&#22806;&#20154;&#21592;&#20877;&#35782;&#21035;(USL-VI-ReID)&#26088;&#22312;&#20174;&#26410;&#26631;&#35760;&#30340;&#36328;&#27169;&#24577;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#27169;&#24577;&#19981;&#21464;&#29305;&#24449;&#65292;&#36825;&#22312;&#35270;&#39057;&#30417;&#25511;&#31995;&#32479;&#31561;&#23454;&#38469;&#24212;&#29992;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#35299;&#20915;&#36328;&#27169;&#24577;&#25968;&#25454;&#20851;&#32852;&#38382;&#39064;&#23545;&#20110;&#36827;&#19968;&#27493;&#36827;&#34892;&#24322;&#36136;&#32852;&#21512;&#23398;&#20064;&#38750;&#24120;&#20851;&#38190;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#37325;&#26368;&#20248;&#20256;&#36755;&#26631;&#31614;&#20998;&#37197;(DOTLA)&#26694;&#26550;&#65292;&#21516;&#26102;&#23558;&#19968;&#20010;&#27169;&#24577;&#20013;&#29983;&#25104;&#30340;&#26631;&#31614;&#20998;&#37197;&#32473;&#20854;&#23545;&#24212;&#30340;&#27169;&#24577;&#12290;&#25152;&#25552;&#20986;&#30340;DOTLA&#26426;&#21046;formulate&#20102;&#19968;&#31181;&#30456;&#20114;&#22686;&#24378;&#21644;&#39640;&#25928;&#30340;&#36328;&#27169;&#24577;&#25968;&#25454;&#20851;&#32852;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#20943;&#23569;&#19968;&#20123;&#19981;&#36275;&#21644;&#22122;&#22768;&#26631;&#31614;&#20851;&#32852;&#30340;&#21103;&#20316;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#36328;&#27169;&#24577;&#37051;&#23621;&#19968;&#33268;&#24615;&#24341;&#23548;&#30340;&#26631;&#31614;&#31934;&#28860;&#21644;&#27491;&#21017;&#21270;&#27169;&#22359;&#65292;&#22312;&#30456;&#24212;&#27169;&#24577;&#20013;&#37051;&#23621;&#26679;&#26412;&#30340;&#25351;&#23548;&#19979;&#28040;&#38500;&#30001;&#19981;&#20934;&#30830;&#30340;&#30417;&#30563;&#20449;&#21495;&#24102;&#26469;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;USL-VI-ReID&#27169;&#22411;&#19982;&#29616;&#26377;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#29978;&#33267;&#19968;&#20123;&#26377;&#30417;&#30563;&#26041;&#27861;&#30456;&#27604;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised learning visible-infrared person re-identification (USL-VI-ReID) aims at learning modality-invariant features from unlabeled cross-modality dataset, which is crucial for practical applications in video surveillance systems. The key to essentially address the USL-VI-ReID task is to solve the cross-modality data association problem for further heterogeneous joint learning. To address this issue, we propose a Dual Optimal Transport Label Assignment (DOTLA) framework to simultaneously assign the generated labels from one modality to its counterpart modality. The proposed DOTLA mechanism formulates a mutual reinforcement and efficient solution to cross-modality data association, which could effectively reduce the side-effects of some insufficient and noisy label associations. Besides, we further propose a cross-modality neighbor consistency guided label refinement and regularization module, to eliminate the negative effects brought by the inaccurate supervised signals, under th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;MC-ViViT&#65292;&#29992;&#20110;&#36890;&#36807;&#20998;&#26512;&#38754;&#37096;&#29305;&#24449;&#26816;&#27979;&#32769;&#24180;&#20154;&#36731;&#24230;&#35748;&#30693;&#38556;&#30861;&#12290;&#36890;&#36807;MC&#27169;&#22359;&#21644;&#32467;&#21512;&#25439;&#22833;&#20989;&#25968;&#26469;&#35299;&#20915;&#25968;&#25454;&#38598;&#26679;&#26412;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.05292</link><description>&lt;p&gt;
MC-ViViT: &#22810;&#20998;&#25903;&#20998;&#31867;&#22120;-ViViT&#29992;&#20110;&#20351;&#29992;&#38754;&#37096;&#35270;&#39057;&#26816;&#27979;&#32769;&#24180;&#20154;&#36731;&#24230;&#35748;&#30693;&#38556;&#30861;
&lt;/p&gt;
&lt;p&gt;
MC-ViViT: Multi-branch Classifier-ViViT to Detect Mild Cognitive Impairment in Older Adults using Facial Videos. (arXiv:2304.05292v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05292
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;MC-ViViT&#65292;&#29992;&#20110;&#36890;&#36807;&#20998;&#26512;&#38754;&#37096;&#29305;&#24449;&#26816;&#27979;&#32769;&#24180;&#20154;&#36731;&#24230;&#35748;&#30693;&#38556;&#30861;&#12290;&#36890;&#36807;MC&#27169;&#22359;&#21644;&#32467;&#21512;&#25439;&#22833;&#20989;&#25968;&#26469;&#35299;&#20915;&#25968;&#25454;&#38598;&#26679;&#26412;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21253;&#25324;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#24050;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;&#20351;&#29992;&#21307;&#23398;&#22270;&#20687;&#12289;&#38382;&#21367;&#21644;&#35270;&#39057;&#26816;&#27979;&#36731;&#24230;&#35748;&#30693;&#38556;&#30861;(MCI)&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#20998;&#25903;&#20998;&#31867;&#22120;-&#35270;&#39057;&#35270;&#35273;&#21464;&#25442;&#22120;(MC-ViViT)&#27169;&#22411;&#65292;&#36890;&#36807;&#20998;&#26512;&#38754;&#37096;&#29305;&#24449;&#21306;&#20998;MCI&#21644;&#27491;&#24120;&#35748;&#30693;&#12290;&#25968;&#25454;&#26469;&#33258;I-CONECT&#65292;&#19968;&#20010;&#26088;&#22312;&#36890;&#36807;&#25552;&#20379;&#39057;&#32321;&#35270;&#39057;&#32842;&#22825;&#26469;&#25913;&#21892;&#35748;&#30693;&#21151;&#33021;&#30340;&#34892;&#20026;&#24178;&#39044;&#35797;&#39564;&#12290;MC-ViViT&#22312;&#19968;&#20010;&#20998;&#25903;&#20013;&#25552;&#21462;&#35270;&#39057;&#30340;&#26102;&#31354;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;MC&#27169;&#22359;&#22686;&#24378;&#34920;&#31034;&#12290;&#30001;&#20110;I-CONECT&#25968;&#25454;&#38598;&#20013;&#30340;&#26679;&#26412;&#19981;&#24179;&#34913;&#38382;&#39064;&#65288;&#21253;&#21547;&#38590;&#26131;&#21644;&#27491;&#36127;&#26679;&#26412;&#65289;&#65292;&#36825;&#20351;MC-ViViT&#30340;&#24615;&#33021;&#21463;&#21040;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;Hard-Easy&#21644;Positive-Negative&#26679;&#26412;&#30340;&#25439;&#22833;&#20989;&#25968;&#65288;HP Loss&#65289;&#26469;&#32467;&#21512;&#23545;&#27604;&#24230;&#35843;&#33410;&#25439;&#22833;Focal loss&#21644;AD-CORRE loss&#26469;&#35299;&#20915;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;I-CONECT&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20986;&#35813;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep machine learning models including Convolutional Neural Networks (CNN) have been successful in the detection of Mild Cognitive Impairment (MCI) using medical images, questionnaires, and videos. This paper proposes a novel Multi-branch Classifier-Video Vision Transformer (MC-ViViT) model to distinguish MCI from those with normal cognition by analyzing facial features. The data comes from the I-CONECT, a behavioral intervention trial aimed at improving cognitive function by providing frequent video chats. MC-ViViT extracts spatiotemporal features of videos in one branch and augments representations by the MC module. The I-CONECT dataset is challenging as the dataset is imbalanced containing Hard-Easy and Positive-Negative samples, which impedes the performance of MC-ViViT. We propose a loss function for Hard-Easy and Positive-Negative Samples (HP Loss) by combining Focal loss and AD-CORRE loss to address the imbalanced problem. Our experimental results on the I-CONECT dataset show th
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25163;&#26415;&#32858;&#21512;&#30340;&#21327;&#21516;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21487;&#29992;&#20110;&#21327;&#35843;&#21644;&#32858;&#21512;&#20998;&#24067;&#24335;&#21307;&#23398;&#24433;&#20687;&#25968;&#25454;&#38598;&#30340;&#30693;&#35782;&#65292;&#24182;&#24102;&#26377;&#37096;&#20998;&#30142;&#30149;&#27880;&#37322;&#65292;&#20174;&#32780;&#21487;&#35757;&#32451;&#20855;&#26377;&#23436;&#25972;&#33016;&#37096;&#20869;&#21487;&#33021;&#20986;&#29616;&#30340;&#25152;&#26377;&#24322;&#24120;&#30340;&#20020;&#24202;&#23454;&#29992;&#12289;&#24378;&#22823;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2301.06683</link><description>&lt;p&gt;
&#25163;&#26415;&#32858;&#21512;&#65306;&#19968;&#31181;&#29992;&#20110;&#21327;&#21516;&#23398;&#20064;&#30340;&#20998;&#24067;&#24335;&#21307;&#23398;&#24433;&#20687;&#25968;&#25454;&#21644;&#22810;&#26679;&#20219;&#21153;&#21327;&#35843;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Surgical Aggregation: A Collaborative Learning Framework for Harmonizing Distributed Medical Imaging Datasets with Diverse Tasks. (arXiv:2301.06683v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.06683
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25163;&#26415;&#32858;&#21512;&#30340;&#21327;&#21516;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21487;&#29992;&#20110;&#21327;&#35843;&#21644;&#32858;&#21512;&#20998;&#24067;&#24335;&#21307;&#23398;&#24433;&#20687;&#25968;&#25454;&#38598;&#30340;&#30693;&#35782;&#65292;&#24182;&#24102;&#26377;&#37096;&#20998;&#30142;&#30149;&#27880;&#37322;&#65292;&#20174;&#32780;&#21487;&#35757;&#32451;&#20855;&#26377;&#23436;&#25972;&#33016;&#37096;&#20869;&#21487;&#33021;&#20986;&#29616;&#30340;&#25152;&#26377;&#24322;&#24120;&#30340;&#20020;&#24202;&#23454;&#29992;&#12289;&#24378;&#22823;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#30340;&#33016;&#37096;X&#20809;&#25968;&#25454;&#38598;&#24050;&#32463;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#65292;&#24182;&#26377;&#28508;&#21147;&#20026;&#35768;&#22810;&#20020;&#24202;&#24212;&#29992;&#25552;&#20379;&#24040;&#22823;&#30340;&#30410;&#22788;&#12290;&#28982;&#32780;&#65292;&#27599;&#20010;&#25968;&#25454;&#38598;&#20165;&#19987;&#27880;&#20110;&#26816;&#27979;&#24739;&#32773;&#21487;&#33021;&#21516;&#26102;&#20986;&#29616;&#30340;&#19968;&#37096;&#20998;&#21457;&#29616;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#20854;&#20020;&#24202;&#25928;&#29992;&#12290;&#22240;&#27492;&#65292;&#25968;&#25454;&#21327;&#35843;&#23545;&#20110;&#32858;&#21512;&#36825;&#20123;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#20855;&#26377;&#23436;&#25972;&#33016;&#37096;&#20869;&#21487;&#33021;&#20986;&#29616;&#30340;&#25152;&#26377;&#24322;&#24120;&#30340;&#20020;&#24202;&#23454;&#29992;&#12289;&#24378;&#22823;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25163;&#26415;&#32858;&#21512;&#65292;&#19968;&#31181;&#21327;&#21516;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#21327;&#35843;&#21644;&#32858;&#21512;&#20998;&#24067;&#24335;&#24322;&#26500;&#25968;&#25454;&#38598;&#30340;&#30693;&#35782;&#65292;&#24182;&#24102;&#26377;&#37096;&#20998;&#30142;&#30149;&#27880;&#37322;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#30340;iid&#25968;&#25454;&#38598;&#21644;&#20855;&#26377;&#37096;&#20998;&#27880;&#37322;&#30340;&#30495;&#23454;&#22823;&#35268;&#27169;&#38750;iid&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25163;&#26415;&#32858;&#21512;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25163;&#26415;&#32858;&#21512;&#26174;&#33879;&#20248;&#20110;&#24403;&#21069;&#30340;&#31574;&#30053;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#36890;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale chest x-ray datasets have been curated for the detection of abnormalities using deep learning, with the potential to provide substantial benefits across many clinical applications. However, each dataset focuses only on detecting a subset of findings that can be simultaneously present in a patient, thereby limiting its clinical utility. Therefore, data harmonization is crucial to leverage these datasets in aggregate to train clinically-useful, robust models with a complete representation of all abnormalities that may occur within the thorax. To that end, we propose surgical aggregation, a collaborative learning framework for harmonizing and aggregating knowledge from distributed heterogeneous datasets with partial disease annotations. We evaluate surgical aggregation across synthetic iid datasets and real-world large-scale non-iid datasets with partial annotations. Our results indicate that surgical aggregation significantly outperforms current strategies, has better general
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;Fast-iTPN&#65292;&#36825;&#26159;&#19968;&#20010;&#25972;&#20307;&#39044;&#35757;&#32451;Transformer&#37329;&#23383;&#22612;&#32593;&#32476;&#65292;&#36890;&#36807;&#20196;&#29260;&#36801;&#31227;&#21644;&#20196;&#29260;&#32858;&#38598;&#31561;&#28789;&#27963;&#35774;&#35745;&#26469;&#20943;&#23569;&#35745;&#31639;&#20869;&#23384;&#24320;&#38144;&#21644;&#21152;&#36895;&#25512;&#26029;&#12290;&#22522;&#20110;ImageNet-1K&#21644;COCO&#25968;&#25454;&#38598;&#65292;Fast-iTPN&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#30446;&#26631;&#26816;&#27979;&#20219;&#21153;&#20013;&#36798;&#21040;&#20102;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.12735</link><description>&lt;p&gt;
&#24555;&#36895;iTPN&#65306;&#20855;&#26377;&#20196;&#29260;&#36801;&#31227;&#30340;&#25972;&#20307;&#39044;&#35757;&#32451;Transformer&#37329;&#23383;&#22612;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Fast-iTPN: Integrally Pre-Trained Transformer Pyramid Network with Token Migration. (arXiv:2211.12735v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.12735
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Fast-iTPN&#65292;&#36825;&#26159;&#19968;&#20010;&#25972;&#20307;&#39044;&#35757;&#32451;Transformer&#37329;&#23383;&#22612;&#32593;&#32476;&#65292;&#36890;&#36807;&#20196;&#29260;&#36801;&#31227;&#21644;&#20196;&#29260;&#32858;&#38598;&#31561;&#28789;&#27963;&#35774;&#35745;&#26469;&#20943;&#23569;&#35745;&#31639;&#20869;&#23384;&#24320;&#38144;&#21644;&#21152;&#36895;&#25512;&#26029;&#12290;&#22522;&#20110;ImageNet-1K&#21644;COCO&#25968;&#25454;&#38598;&#65292;Fast-iTPN&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#30446;&#26631;&#26816;&#27979;&#20219;&#21153;&#20013;&#36798;&#21040;&#20102;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25972;&#20307;&#39044;&#35757;&#32451;&#30340;Transformer&#37329;&#23383;&#22612;&#32593;&#32476;&#65288;iTPN&#65289;&#65292;&#26088;&#22312;&#20849;&#21516;&#20248;&#21270;&#32593;&#32476;&#39592;&#24178;&#21644;&#29942;&#39048;&#65292;&#20197;&#20351;&#34920;&#31034;&#27169;&#22411;&#19982;&#19979;&#28216;&#20219;&#21153;&#20043;&#38388;&#30340;&#36716;&#31227;&#24046;&#36317;&#26368;&#23567;&#21270;&#12290;iTPN&#20855;&#26377;&#20004;&#20010;&#31934;&#24515;&#35774;&#35745;&#65306;1&#65289;&#22522;&#20110;&#35270;&#35273;Transformer&#65288;ViT&#65289;&#30340;&#31532;&#19968;&#20010;&#39044;&#35757;&#32451;&#29305;&#24449;&#37329;&#23383;&#22612;&#12290;2&#65289;&#20351;&#29992;&#36974;&#34109;&#29305;&#24449;&#24314;&#27169;&#65288;MFM&#65289;&#23545;&#29305;&#24449;&#37329;&#23383;&#22612;&#36827;&#34892;&#22810;&#38454;&#27573;&#30417;&#30563;&#12290;Fast-iTPN&#26159;iTPN&#30340;&#21319;&#32423;&#29256;&#65292;&#36890;&#36807;&#20004;&#20010;&#28789;&#27963;&#30340;&#35774;&#35745;&#20943;&#23569;&#20102;&#35745;&#31639;&#20869;&#23384;&#24320;&#38144;&#24182;&#21152;&#36895;&#25512;&#26029;&#12290;1&#65289;&#20196;&#29260;&#36801;&#31227;&#65306;&#33293;&#24323;&#39592;&#24178;&#30340;&#20887;&#20313;&#26631;&#35760;&#65292;&#21516;&#26102;&#22312;&#29305;&#24449;&#37329;&#23383;&#22612;&#20013;&#36827;&#34892;&#34917;&#20805;&#65292;&#26080;&#38656;&#27880;&#24847;&#21147;&#25805;&#20316;&#12290;2&#65289;&#20196;&#29260;&#32858;&#38598;&#65306;&#36890;&#36807;&#24341;&#20837;&#23569;&#37327;&#30340;&#32858;&#38598;&#26631;&#35760;&#20943;&#23569;&#20840;&#23616;&#27880;&#24847;&#21147;&#24341;&#36215;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#22522;&#20110;ImageNet-1K&#25968;&#25454;&#38598;&#65292;&#22522;&#26412;/&#39640;&#32423;&#30340;Fast-iTPN&#20998;&#21035;&#36798;&#21040;88.75%/89.5%&#30340;top-1&#20934;&#30830;&#29575;&#12290;&#22312;COCO&#30446;&#26631;&#26816;&#27979;&#20219;&#21153;&#20013;&#65292;&#20351;&#29992;DINO&#30340;1x&#35757;&#32451;&#35745;&#21010;&#65292;&#22522;&#26412;/&#39640;&#32423;&#30340;Fast-iTPN&#20998;&#21035;&#36798;&#21040;58.4%/58.8%&#30340;&#26694;AP&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose integrally pre-trained transformer pyramid network (iTPN), towards jointly optimizing the network backbone and the neck, so that transfer gap between representation models and downstream tasks is minimal. iTPN is born with two elaborated designs: 1) The first pre-trained feature pyramid upon vision transformer (ViT). 2) Multi-stage supervision to the feature pyramid using masked feature modeling (MFM). iTPN is updated to Fast-iTPN, reducing computational memory overhead and accelerating inference through two flexible designs. 1) Token migration: dropping redundant tokens of the backbone while replenishing them in the feature pyramid without attention operations. 2) Token gathering: reducing computation cost caused by global attention by introducing few gathering tokens. The base/large-level Fast-iTPN achieve 88.75%/89.5% top-1 accuracy on ImageNet-1K. With 1x training schedule using DINO, the base/large-level Fast-iTPN achieves 58.4%/58.8% box AP on COCO object detection, an
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#20998;&#37327;&#23376;&#30005;&#36335;&#30340;&#37327;&#23376;&#32858;&#31867;&#31639;&#27861;&#12290;&#36890;&#36807;&#23558;&#32858;&#31867;&#38382;&#39064;&#36716;&#21270;&#20026;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#21464;&#20998;&#37327;&#23376;&#26412;&#24449;&#27714;&#35299;&#22120;&#65288;VQE&#65289;&#32467;&#21512;&#38750;&#27491;&#20132;&#37327;&#23376;&#27604;&#29305;&#29366;&#24577;&#26469;&#35299;&#20915;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#22312;NISQ&#35774;&#22791;&#19978;&#23454;&#29616;&#12290;&#25968;&#20540;&#27169;&#25311;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#21482;&#26377;&#19968;&#20010;&#37327;&#23376;&#27604;&#29305;&#65292;&#31639;&#27861;&#20063;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#24352;&#37327;&#32593;&#32476;&#27169;&#25311;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#32463;&#20856;&#30828;&#20214;&#19978;&#36816;&#34892;&#37327;&#23376;&#21551;&#21457;&#24335;&#32858;&#31867;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2206.09893</link><description>&lt;p&gt;
&#21464;&#20998;&#37327;&#23376;&#21644;&#37327;&#23376;&#21551;&#21457;&#24335;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Variational Quantum and Quantum-Inspired Clustering. (arXiv:2206.09893v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.09893
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#20998;&#37327;&#23376;&#30005;&#36335;&#30340;&#37327;&#23376;&#32858;&#31867;&#31639;&#27861;&#12290;&#36890;&#36807;&#23558;&#32858;&#31867;&#38382;&#39064;&#36716;&#21270;&#20026;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#21464;&#20998;&#37327;&#23376;&#26412;&#24449;&#27714;&#35299;&#22120;&#65288;VQE&#65289;&#32467;&#21512;&#38750;&#27491;&#20132;&#37327;&#23376;&#27604;&#29305;&#29366;&#24577;&#26469;&#35299;&#20915;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#22312;NISQ&#35774;&#22791;&#19978;&#23454;&#29616;&#12290;&#25968;&#20540;&#27169;&#25311;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#21482;&#26377;&#19968;&#20010;&#37327;&#23376;&#27604;&#29305;&#65292;&#31639;&#27861;&#20063;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#24352;&#37327;&#32593;&#32476;&#27169;&#25311;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#32463;&#20856;&#30828;&#20214;&#19978;&#36816;&#34892;&#37327;&#23376;&#21551;&#21457;&#24335;&#32858;&#31867;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#20998;&#37327;&#23376;&#30005;&#36335;&#30340;&#37327;&#23376;&#32858;&#31867;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#21487;&#20197;&#23558;&#25968;&#25454;&#20998;&#31867;&#20026;&#22810;&#20010;&#32858;&#31867;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#23569;&#37327;&#37327;&#23376;&#27604;&#29305;&#30340;&#22122;&#22768;&#20013;&#38388;&#23610;&#24230;&#37327;&#23376;&#65288;NISQ&#65289;&#35774;&#22791;&#19978;&#36731;&#26494;&#23454;&#29616;&#12290;&#35813;&#31639;&#27861;&#30340;&#24605;&#24819;&#26159;&#23558;&#32858;&#31867;&#38382;&#39064;&#32553;&#20943;&#20026;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#65292;&#28982;&#21518;&#36890;&#36807;&#21464;&#20998;&#37327;&#23376;&#26412;&#24449;&#27714;&#35299;&#22120;&#65288;VQE&#65289;&#19982;&#38750;&#27491;&#20132;&#37327;&#23376;&#27604;&#29305;&#29366;&#24577;&#30456;&#32467;&#21512;&#26469;&#35299;&#20915;&#23427;&#12290;&#23454;&#38469;&#19978;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#30446;&#26631;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#30340;&#26368;&#22823;&#27491;&#20132;&#29366;&#24577;&#32780;&#19981;&#26159;&#36890;&#24120;&#30340;&#35745;&#31639;&#22522;&#30784;&#65292;&#21363;&#20351;&#26377;&#23569;&#37327;&#37327;&#23376;&#27604;&#29305;&#20063;&#21487;&#20197;&#32771;&#34385;&#22823;&#37327;&#32858;&#31867;&#12290;&#25105;&#20204;&#20351;&#29992;&#30495;&#23454;&#25968;&#25454;&#38598;&#36827;&#34892;&#25968;&#20540;&#27169;&#25311;&#26469;&#35780;&#20272;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#34920;&#26126;&#21363;&#20351;&#21482;&#26377;&#19968;&#20010;&#37327;&#23376;&#27604;&#29305;&#65292;&#31639;&#27861;&#30340;&#24615;&#33021;&#20063;&#24456;&#22909;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#24352;&#37327;&#32593;&#32476;&#27169;&#25311;&#31639;&#27861;&#65292;&#25105;&#20204;&#21487;&#20197;&#26500;&#24314;&#19968;&#20010;&#21487;&#20197;&#22312;&#24403;&#21069;&#32463;&#20856;&#30828;&#20214;&#19978;&#36816;&#34892;&#30340;&#37327;&#23376;&#21551;&#21457;&#24335;&#32858;&#31867;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Here we present a quantum algorithm for clustering data based on a variational quantum circuit. The algorithm allows to classify data into many clusters, and can easily be implemented in few-qubit Noisy Intermediate-Scale Quantum (NISQ) devices. The idea of the algorithm relies on reducing the clustering problem to an optimization, and then solving it via a Variational Quantum Eigensolver (VQE) combined with non-orthogonal qubit states. In practice, the method uses maximally-orthogonal states of the target Hilbert space instead of the usual computational basis, allowing for a large number of clusters to be considered even with few qubits. We benchmark the algorithm with numerical simulations using real datasets, showing excellent performance even with one single qubit. Moreover, a tensor network simulation of the algorithm implements, by construction, a quantum-inspired clustering algorithm that can run on current classical hardware.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#31283;&#23450;LIF&#31070;&#32463;&#20803;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23454;&#39564;&#21644;&#29702;&#35770;&#20998;&#26512;&#65292;&#30830;&#23450;&#20102;&#22312;&#19981;&#21516;&#20219;&#21153;&#21644;&#32593;&#32476;&#20013;&#36873;&#25321;&#26368;&#20339;&#26367;&#20195;&#26799;&#24230;&#30340;&#31283;&#23450;&#24615;&#19982;&#25928;&#26524;&#30340;&#20851;&#31995;&#65292;&#20943;&#23569;&#20102;&#23545;&#36229;&#21442;&#25968;&#25628;&#32034;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2202.00282</link><description>&lt;p&gt;
&#31283;&#23450;LIF&#31070;&#32463;&#20803;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Stabilizing the LIF Neuron Training. (arXiv:2202.00282v3 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.00282
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#31283;&#23450;LIF&#31070;&#32463;&#20803;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23454;&#39564;&#21644;&#29702;&#35770;&#20998;&#26512;&#65292;&#30830;&#23450;&#20102;&#22312;&#19981;&#21516;&#20219;&#21153;&#21644;&#32593;&#32476;&#20013;&#36873;&#25321;&#26368;&#20339;&#26367;&#20195;&#26799;&#24230;&#30340;&#31283;&#23450;&#24615;&#19982;&#25928;&#26524;&#30340;&#20851;&#31995;&#65292;&#20943;&#23569;&#20102;&#23545;&#36229;&#21442;&#25968;&#25628;&#32034;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#21033;&#29992;&#20108;&#36827;&#21046;&#27963;&#21160;&#26469;&#25552;&#39640;&#20154;&#24037;&#26234;&#33021;&#30340;&#33021;&#28304;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#20108;&#36827;&#21046;&#27963;&#21160;&#30340;&#38750;&#24179;&#28369;&#24615;&#35201;&#27714;&#20351;&#29992;&#36817;&#20284;&#26799;&#24230;&#65292;&#20063;&#31216;&#20026;&#26367;&#20195;&#26799;&#24230;&#65288;SG&#65289;&#65292;&#20197;&#24357;&#21512;&#19982;&#28145;&#24230;&#23398;&#20064;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#25991;&#29486;&#20013;&#24050;&#25552;&#20986;&#20102;&#20960;&#31181;SG&#65292;&#20294;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#22914;&#20309;&#30830;&#23450;&#36866;&#21512;&#29305;&#23450;&#20219;&#21153;&#21644;&#32593;&#32476;&#30340;&#26368;&#20339;SG&#12290;&#22312;&#26114;&#36149;&#30340;&#36229;&#21442;&#25968;&#25628;&#32034;&#21518;&#65292;&#22823;&#22810;&#25968;SG&#24418;&#29366;&#37117;&#21487;&#20197;&#23454;&#29616;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#26088;&#22312;&#22312;&#19981;&#21516;&#30340;&#21387;&#21147;&#27979;&#35797;&#20013;&#23454;&#39564;&#35777;&#26126;&#26368;&#20339;SG&#65292;&#24182;&#22312;&#23454;&#39564;&#21644;&#29702;&#35770;&#19978;&#20943;&#23569;&#26410;&#26469;&#23545;&#32593;&#26684;&#25628;&#32034;&#30340;&#38656;&#27714;&#12290;&#20026;&#20102;&#29702;&#35299;&#35813;&#39046;&#22495;&#30340;&#24046;&#36317;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#21644;&#32593;&#32476;&#38656;&#35201;&#26356;&#24910;&#37325;&#22320;&#36873;&#25321;SG&#65292;&#21363;&#20351;&#25972;&#20307;&#19978;&#65292;&#24555;&#36895;Sigmoid&#20989;&#25968;&#30340;&#23548;&#25968;&#22312;&#21508;&#31181;&#23398;&#20064;&#29575;&#19979;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;SG&#12290;&#22240;&#27492;&#65292;&#22312;&#35757;&#32451;&#20043;&#21069;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#31283;&#23450;&#24615;&#30340;&#29702;&#35770;&#26041;&#27861;&#26469;&#36873;&#25321;&#21021;&#22987;&#21270;&#21644;SG&#24418;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spiking Neuromorphic Computing uses binary activity to improve Artificial Intelligence energy efficiency. However, the non-smoothness of binary activity requires approximate gradients, known as Surrogate Gradients (SG), to close the performance gap with Deep Learning. Several SG have been proposed in the literature, but it remains unclear how to determine the best SG for a given task and network. Good performance can be achieved with most SG shapes, after a costly search of hyper-parameters. Thus, we aim at experimentally and theoretically define the best SG across different stress tests, to reduce future need of grid search. To understand the gap for this line of work, we show that more complex tasks and networks need more careful choice of SG, even if overall the derivative of the fast sigmoid outperforms other SG across tasks and networks, for a wide range of learning rates. We therefore design a stability based theoretical method to choose initialization and SG shape before trainin
&lt;/p&gt;</description></item><item><title>INVIGORATE&#26159;&#19968;&#20010;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#19982;&#20154;&#31867;&#36827;&#34892;&#20132;&#20114;&#24182;&#22312;&#26434;&#20081;&#29615;&#22659;&#20013;&#25235;&#21462;&#25351;&#23450;&#29289;&#20307;&#30340;&#26426;&#22120;&#20154;&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#33021;&#22815;&#25512;&#26029;&#30446;&#26631;&#29289;&#20307;&#12289;&#25512;&#26029;&#29289;&#20307;&#38459;&#25377;&#20851;&#31995;&#65292;&#24182;&#29983;&#25104;&#22810;&#27493;&#35745;&#21010;&#26469;&#28040;&#38500;&#27495;&#20041;&#24182;&#25104;&#21151;&#25235;&#21462;&#30446;&#26631;&#29289;&#20307;&#12290;</title><link>http://arxiv.org/abs/2108.11092</link><description>&lt;p&gt;
INVIGORATE: &#20114;&#21160;&#35270;&#35273;&#22330;&#26223;&#29702;&#35299;&#21644;&#25235;&#21462;
&lt;/p&gt;
&lt;p&gt;
INVIGORATE: Interactive Visual Grounding and Grasping in Clutter. (arXiv:2108.11092v1 [cs.RO] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.11092
&lt;/p&gt;
&lt;p&gt;
INVIGORATE&#26159;&#19968;&#20010;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#19982;&#20154;&#31867;&#36827;&#34892;&#20132;&#20114;&#24182;&#22312;&#26434;&#20081;&#29615;&#22659;&#20013;&#25235;&#21462;&#25351;&#23450;&#29289;&#20307;&#30340;&#26426;&#22120;&#20154;&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#33021;&#22815;&#25512;&#26029;&#30446;&#26631;&#29289;&#20307;&#12289;&#25512;&#26029;&#29289;&#20307;&#38459;&#25377;&#20851;&#31995;&#65292;&#24182;&#29983;&#25104;&#22810;&#27493;&#35745;&#21010;&#26469;&#28040;&#38500;&#27495;&#20041;&#24182;&#25104;&#21151;&#25235;&#21462;&#30446;&#26631;&#29289;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;INVIGORATE&#65292;&#19968;&#20010;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#19982;&#20154;&#31867;&#36827;&#34892;&#20132;&#20114;&#24182;&#22312;&#26434;&#20081;&#29615;&#22659;&#20013;&#25235;&#21462;&#25351;&#23450;&#29289;&#20307;&#30340;&#26426;&#22120;&#20154;&#31995;&#32479;&#12290;&#22312;&#36825;&#31181;&#26434;&#20081;&#29615;&#22659;&#20013;&#65292;&#29289;&#20307;&#21487;&#33021;&#20250;&#30456;&#20114;&#36974;&#25377;&#12289;&#38459;&#25377;&#29978;&#33267;&#21472;&#25918;&#22312;&#19968;&#36215;&#12290;INVIGORATE&#38754;&#20020;&#30528;&#20960;&#20010;&#25361;&#25112;&#65306;&#65288;i&#65289;&#20174;&#36755;&#20837;&#30340;&#35821;&#35328;&#34920;&#36798;&#21644;RGB&#22270;&#20687;&#20013;&#25512;&#26029;&#20986;&#30446;&#26631;&#29289;&#20307;&#65292;&#32780;&#24573;&#30053;&#20854;&#20182;&#36974;&#25377;&#29289;&#20307;&#65307;&#65288;ii&#65289;&#20174;&#22270;&#20687;&#20013;&#25512;&#26029;&#20986;&#29289;&#20307;&#30340;&#38459;&#25377;&#20851;&#31995;&#65288;OBR&#65289;&#65307;&#65288;iii&#65289;&#29983;&#25104;&#19968;&#20010;&#22810;&#27493;&#35745;&#21010;&#65292;&#36890;&#36807;&#25552;&#38382;&#28040;&#38500;&#30446;&#26631;&#29289;&#20307;&#30340;&#27495;&#20041;&#24182;&#25104;&#21151;&#25235;&#21462;&#23427;&#12290;&#25105;&#20204;&#20026;&#30446;&#26631;&#26816;&#27979;&#12289;&#35270;&#35273;&#22330;&#26223;&#29702;&#35299;&#12289;&#38382;&#39064;&#29983;&#25104;&#21644;OBR&#26816;&#27979;&#20197;&#21450;&#25235;&#21462;&#35757;&#32451;&#20102;&#29420;&#31435;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#23427;&#20204;&#21487;&#20197;&#22788;&#29702;&#20219;&#24847;&#30340;&#29289;&#20307;&#31867;&#21035;&#21644;&#35821;&#35328;&#34920;&#36798;&#65292;&#21482;&#35201;&#26377;&#30456;&#24212;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#35270;&#35273;&#24863;&#30693;&#20013;&#30340;&#35823;&#24046;&#20197;&#21450;&#20154;&#31867;&#35821;&#35328;&#20013;&#30340;&#27495;&#20041;&#19981;&#21487;&#36991;&#20813;&#22320;&#20250;&#23545;&#26426;&#22120;&#20154;&#30340;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#19981;&#30830;&#23450;&#24615;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents INVIGORATE, a robot system that interacts with human through natural language and grasps a specified object in clutter. The objects may occlude, obstruct, or even stack on top of one another. INVIGORATE embodies several challenges: (i) infer the target object among other occluding objects, from input language expressions and RGB images, (ii) infer object blocking relationships (OBRs) from the images, and (iii) synthesize a multi-step plan to ask questions that disambiguate the target object and to grasp it successfully. We train separate neural networks for object detection, for visual grounding, for question generation, and for OBR detection and grasping. They allow for unrestricted object categories and language expressions, subject to the training datasets. However, errors in visual perception and ambiguity in human languages are inevitable and negatively impact the robot's performance. To overcome these uncertainties, we build a partially observable Markov decis
&lt;/p&gt;</description></item></channel></rss>