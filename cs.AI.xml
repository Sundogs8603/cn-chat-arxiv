<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25361;&#25112;&#65292;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#31038;&#20250;&#35268;&#33539;&#30340;&#29702;&#35299;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#28085;&#30422;&#24191;&#27867;&#30340;&#31038;&#20250;&#35268;&#33539;&#38382;&#39064;&#65292;&#36890;&#36807;&#22810;&#20195;&#29702;&#26694;&#26550;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#25552;&#39640;&#27169;&#22411;&#23545;&#31038;&#20250;&#35268;&#33539;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2404.02491</link><description>&lt;p&gt;
&#27979;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31038;&#20250;&#35268;&#33539;
&lt;/p&gt;
&lt;p&gt;
Measuring Social Norms of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02491
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25361;&#25112;&#65292;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#31038;&#20250;&#35268;&#33539;&#30340;&#29702;&#35299;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#28085;&#30422;&#24191;&#27867;&#30340;&#31038;&#20250;&#35268;&#33539;&#38382;&#39064;&#65292;&#36890;&#36807;&#22810;&#20195;&#29702;&#26694;&#26550;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#25552;&#39640;&#27169;&#22411;&#23545;&#31038;&#20250;&#35268;&#33539;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25361;&#25112;&#65292;&#20197;&#26816;&#39564;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#29702;&#35299;&#31038;&#20250;&#35268;&#33539;&#12290;&#19982;&#29616;&#26377;&#25968;&#25454;&#38598;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#35201;&#27714;&#20855;&#26377;&#35299;&#20915;&#31038;&#20250;&#35268;&#33539;&#30340;&#22522;&#26412;&#29702;&#35299;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#21547;&#26368;&#22823;&#30340;&#31038;&#20250;&#35268;&#33539;&#25216;&#33021;&#38598;&#65292;&#21253;&#25324;402&#39033;&#25216;&#33021;&#21644;12,383&#20010;&#38382;&#39064;&#65292;&#28085;&#30422;&#20102;&#20174;&#35266;&#28857;&#21644;&#35770;&#28857;&#21040;&#25991;&#21270;&#21644;&#27861;&#24459;&#31561;&#24191;&#27867;&#30340;&#31038;&#20250;&#35268;&#33539;&#12290;&#25105;&#20204;&#26681;&#25454;K-12&#35838;&#31243;&#35774;&#35745;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#12290;&#36825;&#20351;&#24471;&#21487;&#20197;&#30452;&#25509;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31038;&#20250;&#29702;&#35299;&#33021;&#21147;&#19982;&#20154;&#31867;&#36827;&#34892;&#27604;&#36739;&#65292;&#26356;&#20855;&#20307;&#22320;&#35828;&#26159;&#19982;&#23567;&#23398;&#29983;&#36827;&#34892;&#27604;&#36739;&#12290;&#23613;&#31649;&#20808;&#21069;&#30340;&#24037;&#20316;&#22312;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#20135;&#29983;&#20960;&#20046;&#38543;&#26426;&#30340;&#20934;&#30830;&#24230;&#65292;&#20294;&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;GPT3.5-Turbo&#21644;LLaMA2-Chat&#65289;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#65292;&#20165;&#30053;&#20302;&#20110;&#20154;&#31867;&#24615;&#33021;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#20195;&#29702;&#26694;&#26550;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#29702;&#35299;&#31038;&#20250;&#35268;&#33539;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02491v1 Announce Type: cross  Abstract: We present a new challenge to examine whether large language models understand social norms. In contrast to existing datasets, our dataset requires a fundamental understanding of social norms to solve. Our dataset features the largest set of social norm skills, consisting of 402 skills and 12,383 questions covering a wide set of social norms ranging from opinions and arguments to culture and laws. We design our dataset according to the K-12 curriculum. This enables the direct comparison of the social understanding of large language models to humans, more specifically, elementary students. While prior work generates nearly random accuracy on our benchmark, recent large language models such as GPT3.5-Turbo and LLaMA2-Chat are able to improve the performance significantly, only slightly below human performance. We then propose a multi-agent framework based on large language models to improve the models' ability to understand social norms.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;Masked AutoEncoders&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;NeRF-MAE&#29992;&#20110;&#33258;&#30417;&#30563;&#19977;&#32500;&#34920;&#31034;&#23398;&#20064;&#65292;&#21033;&#29992;&#26631;&#20934;&#30340;&#19977;&#32500;Vision Transformers&#36866;&#24212;NeRF&#30340;&#29420;&#29305;&#20844;&#24335;&#65292;&#23558;NeRF&#30340;&#20307;&#31215;&#32593;&#26684;&#20316;&#20026;&#23494;&#38598;&#36755;&#20837;&#65292;&#20197;&#20135;&#29983;&#26377;&#25928;&#30340;&#19977;&#32500;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2404.01300</link><description>&lt;p&gt;
NeRF-MAE: &#33258;&#30417;&#30563;&#19977;&#32500;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;Masked AutoEncoders
&lt;/p&gt;
&lt;p&gt;
NeRF-MAE : Masked AutoEncoders for Self Supervised 3D representation Learning for Neural Radiance Fields
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01300
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;Masked AutoEncoders&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;NeRF-MAE&#29992;&#20110;&#33258;&#30417;&#30563;&#19977;&#32500;&#34920;&#31034;&#23398;&#20064;&#65292;&#21033;&#29992;&#26631;&#20934;&#30340;&#19977;&#32500;Vision Transformers&#36866;&#24212;NeRF&#30340;&#29420;&#29305;&#20844;&#24335;&#65292;&#23558;NeRF&#30340;&#20307;&#31215;&#32593;&#26684;&#20316;&#20026;&#23494;&#38598;&#36755;&#20837;&#65292;&#20197;&#20135;&#29983;&#26377;&#25928;&#30340;&#19977;&#32500;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#31070;&#32463;&#22330;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26426;&#22120;&#20154;&#39046;&#22495;&#30340;&#21331;&#36234;&#33021;&#21147;&#65292;&#33021;&#22815;&#29702;&#35299;&#19977;&#32500;&#35270;&#35273;&#19990;&#30028;&#65292;&#22914;&#25512;&#26029;&#35821;&#20041;&#12289;&#20960;&#20309;&#21644;&#21160;&#24577;&#31561;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#31070;&#32463;&#22330;&#22312;&#20174;&#20108;&#32500;&#22270;&#20687;&#20013;&#23494;&#38598;&#34920;&#31034;&#19977;&#32500;&#22330;&#26223;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#65292;&#20855;&#20307;&#20351;&#29992;Masked AutoEncoders&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#20511;&#37492;&#20102;&#23558;transformers&#25193;&#23637;&#21040;&#26032;&#25968;&#25454;&#27169;&#24577;&#30340;&#20196;&#20154;&#24778;&#35766;&#30340;&#25104;&#21151;&#65292;&#21033;&#29992;&#26631;&#20934;&#30340;&#19977;&#32500;Vision Transformers&#26469;&#36866;&#24212;NeRF&#30340;&#29420;&#29305;&#20844;&#24335;&#12290;&#25105;&#20204;&#23558;NeRF&#30340;&#20307;&#31215;&#32593;&#26684;&#20316;&#20026;transformer&#30340;&#23494;&#38598;&#36755;&#20837;&#65292;&#19982;&#20854;&#20182;&#19977;&#32500;&#34920;&#31034;&#65288;&#22914;&#28857;&#20113;&#65289;&#36827;&#34892;&#23545;&#27604;&#65292;&#20854;&#20449;&#24687;&#23494;&#24230;&#21487;&#33021;&#19981;&#22343;&#21248;&#65292;&#32780;&#34920;&#31034;&#26159;&#19981;&#35268;&#21017;&#30340;&#12290;&#30001;&#20110;&#23558;masked autoencoders&#24212;&#29992;&#20110;&#31867;&#20284;NeRF&#36825;&#26679;&#30340;&#38544;&#24335;&#34920;&#31034;&#30340;&#22256;&#38590;&#65292;&#25105;&#20204;&#36873;&#25321;&#25552;&#21462;&#19968;&#20010;&#26174;&#24335;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01300v1 Announce Type: cross  Abstract: Neural fields excel in computer vision and robotics due to their ability to understand the 3D visual world such as inferring semantics, geometry, and dynamics. Given the capabilities of neural fields in densely representing a 3D scene from 2D images, we ask the question: Can we scale their self-supervised pretraining, specifically using masked autoencoders, to generate effective 3D representations from posed RGB images. Owing to the astounding success of extending transformers to novel data modalities, we employ standard 3D Vision Transformers to suit the unique formulation of NeRFs. We leverage NeRF's volumetric grid as a dense input to the transformer, contrasting it with other 3D representations such as pointclouds where the information density can be uneven, and the representation is irregular. Due to the difficulty of applying masked autoencoders to an implicit representation, such as NeRF, we opt for extracting an explicit repres
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#35757;&#32451;&#25216;&#26415;&#65292;&#21363;Attention Interpolation via Diffusion (AID)&#65292;&#36890;&#36807;&#20869;/&#22806;&#25554;&#20540;&#27880;&#24847;&#21147;&#23618;&#12289;&#25554;&#20540;&#27880;&#24847;&#21147;&#19982;&#33258;&#27880;&#24847;&#21147;&#34701;&#21512;&#20197;&#25552;&#39640;&#20445;&#30495;&#24230;&#65292;&#20197;&#21450;&#24212;&#29992;&#36125;&#22612;&#20998;&#24067;&#36827;&#34892;&#36873;&#25321;&#20197;&#22686;&#21152;&#24179;&#28369;&#24230;&#26469;&#25913;&#36827;&#25991;&#26412;&#21040;&#22270;&#20687;&#25554;&#20540;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.17924</link><description>&lt;p&gt;
AID: &#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#30340;&#27880;&#37325;&#25554;&#20540;
&lt;/p&gt;
&lt;p&gt;
AID: Attention Interpolation of Text-to-Image Diffusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17924
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#35757;&#32451;&#25216;&#26415;&#65292;&#21363;Attention Interpolation via Diffusion (AID)&#65292;&#36890;&#36807;&#20869;/&#22806;&#25554;&#20540;&#27880;&#24847;&#21147;&#23618;&#12289;&#25554;&#20540;&#27880;&#24847;&#21147;&#19982;&#33258;&#27880;&#24847;&#21147;&#34701;&#21512;&#20197;&#25552;&#39640;&#20445;&#30495;&#24230;&#65292;&#20197;&#21450;&#24212;&#29992;&#36125;&#22612;&#20998;&#24067;&#36827;&#34892;&#36873;&#25321;&#20197;&#22686;&#21152;&#24179;&#28369;&#24230;&#26469;&#25913;&#36827;&#25991;&#26412;&#21040;&#22270;&#20687;&#25554;&#20540;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17924v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;: &#26377;&#26465;&#20214;&#30340;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#22312;&#21508;&#31181;&#35774;&#32622;&#20013;&#21019;&#24314;&#30475;&#19981;&#35265;&#30340;&#22270;&#20687;&#65292;&#26377;&#21161;&#20110;&#22270;&#20687;&#25554;&#20540;&#12290;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#25554;&#20540;&#24050;&#32463;&#24471;&#21040;&#20102;&#28145;&#20837;&#30740;&#31350;&#65292;&#20294;&#26159;&#20855;&#26377;&#29305;&#23450;&#26465;&#20214;&#65288;&#22914;&#25991;&#26412;&#25110;&#23039;&#21183;&#65289;&#30340;&#25554;&#20540;&#21364;&#20102;&#35299;&#19981;&#22810;&#12290;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#22914;&#22312;&#26465;&#20214;&#31354;&#38388;&#20013;&#30340;&#32447;&#24615;&#25554;&#20540;&#65292;&#36890;&#24120;&#20250;&#23548;&#33268;&#22270;&#20687;&#32570;&#20047;&#19968;&#33268;&#24615;&#12289;&#24179;&#28369;&#24230;&#21644;&#20445;&#30495;&#24230;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#19968;&#20010;&#21517;&#20026;Attention Interpolation via Diffusion (AID)&#30340;&#26032;&#39062;&#26080;&#35757;&#32451;&#25216;&#26415;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#21253;&#25324;1&#65289;&#25552;&#20986;&#20102;&#19968;&#20010;&#20869;/&#22806;&#25554;&#20540;&#27880;&#24847;&#21147;&#23618;&#65307;2&#65289;&#23558;&#25554;&#20540;&#27880;&#24847;&#21147;&#19982;&#33258;&#27880;&#24847;&#21147;&#34701;&#21512;&#20197;&#25552;&#39640;&#20445;&#30495;&#24230;&#65307;3&#65289;&#24212;&#29992;&#36125;&#22612;&#20998;&#24067;&#36827;&#34892;&#36873;&#25321;&#20197;&#22686;&#21152;&#24179;&#28369;&#24230;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21464;&#20307;&#65292;Prompt-guided Attention Interpolation via Diffusion (PAID)&#65292;&#23427;&#23558;&#25554;&#20540;&#35270;&#20026;&#20381;&#36182;&#20110;&#26465;&#20214;&#30340;&#29983;&#25104;&#36807;&#31243;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#21019;&#24314;&#20986;&#26356;&#20855;&#21019;&#36896;&#24615;&#30340;&#26032;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17924v1 Announce Type: cross  Abstract: Conditional diffusion models can create unseen images in various settings, aiding image interpolation. Interpolation in latent spaces is well-studied, but interpolation with specific conditions like text or poses is less understood. Simple approaches, such as linear interpolation in the space of conditions, often result in images that lack consistency, smoothness, and fidelity. To that end, we introduce a novel training-free technique named Attention Interpolation via Diffusion (AID). Our key contributions include 1) proposing an inner/outer interpolated attention layer; 2) fusing the interpolated attention with self-attention to boost fidelity; and 3) applying beta distribution to selection to increase smoothness. We also present a variant, Prompt-guided Attention Interpolation via Diffusion (PAID), that considers interpolation as a condition-dependent generative process. This method enables the creation of new images with greater con
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29699;&#24418;&#31070;&#32463;&#32593;&#32476;&#65288;SphNNs&#65289;&#26469;&#36827;&#34892;&#29702;&#24615;&#25512;&#29702;&#65292;&#36890;&#36807;&#23558;&#35745;&#31639;&#26500;&#24314;&#22359;&#20174;&#21521;&#37327;&#25512;&#24191;&#21040;&#29699;&#20307;&#65292;&#23454;&#29616;&#20102;&#20154;&#31867;&#31867;&#20284;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#24320;&#21457;&#20102;&#29992;&#20110;&#19977;&#27573;&#35770;&#25512;&#29702;&#30340;SphNN&#12290;</title><link>https://arxiv.org/abs/2403.15297</link><description>&lt;p&gt;
&#29992;&#20110;&#29702;&#24615;&#25512;&#29702;&#30340;&#29699;&#24418;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Sphere Neural-Networks for Rational Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15297
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29699;&#24418;&#31070;&#32463;&#32593;&#32476;&#65288;SphNNs&#65289;&#26469;&#36827;&#34892;&#29702;&#24615;&#25512;&#29702;&#65292;&#36890;&#36807;&#23558;&#35745;&#31639;&#26500;&#24314;&#22359;&#20174;&#21521;&#37327;&#25512;&#24191;&#21040;&#29699;&#20307;&#65292;&#23454;&#29616;&#20102;&#20154;&#31867;&#31867;&#20284;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#24320;&#21457;&#20102;&#29992;&#20110;&#19977;&#27573;&#35770;&#25512;&#29702;&#30340;SphNN&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#30340;&#25104;&#21151;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#35748;&#21487;&#65292;&#20854;&#31867;&#20154;&#38382;&#39064;&#22238;&#31572;&#30340;&#33021;&#21147;&#20197;&#21450;&#19981;&#26029;&#25552;&#21319;&#30340;&#25512;&#29702;&#24615;&#33021;&#37117;&#35777;&#26126;&#20102;&#36825;&#19968;&#28857;&#12290;&#28982;&#32780;&#65292;LLMs&#26159;&#21542;&#20250;&#36827;&#34892;&#25512;&#29702;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#22312;&#23450;&#24615;&#19978;&#25193;&#23637;&#20197;&#36229;&#36234;&#32479;&#35745;&#33539;&#24335;&#24182;&#23454;&#29616;&#39640;&#32423;&#35748;&#30693;&#26159;&#19968;&#20010;&#26410;&#35299;&#20043;&#35868;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#35745;&#31639;&#26500;&#24314;&#22359;&#20174;&#21521;&#37327;&#25512;&#24191;&#21040;&#29699;&#20307;&#30340;&#26041;&#24335;&#25552;&#20986;&#20102;&#19968;&#31181;&#26497;&#31616;&#30340;&#23450;&#24615;&#25193;&#23637;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#29699;&#24418;&#31070;&#32463;&#32593;&#32476;&#65288;SphNNs&#65289;&#29992;&#20110;&#36890;&#36807;&#27169;&#22411;&#26500;&#24314;&#21644;&#26816;&#26597;&#36827;&#34892;&#31867;&#20154;&#25512;&#29702;&#65292;&#24182;&#20026;&#19977;&#27573;&#35770;&#25512;&#29702;&#24320;&#21457;&#20102;SphNN&#65292;&#36825;&#26159;&#20154;&#31867;&#29702;&#24615;&#30340;&#32553;&#24433;&#12290;SphNN&#19981;&#20351;&#29992;&#35757;&#32451;&#25968;&#25454;&#65292;&#32780;&#26159;&#20351;&#29992;&#37051;&#22495;&#31354;&#38388;&#20851;&#31995;&#30340;&#31070;&#32463;&#31526;&#21495;&#36716;&#25442;&#26144;&#23556;&#26469;&#25351;&#23548;&#20174;&#24403;&#21069;&#29699;&#24418;&#37197;&#32622;&#21521;&#30446;&#26631;&#30340;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15297v1 Announce Type: new  Abstract: The success of Large Language Models (LLMs), e.g., ChatGPT, is witnessed by their planetary popularity, their capability of human-like question-answering, and also by their steadily improved reasoning performance. However, it remains unclear whether LLMs reason. It is an open problem how traditional neural networks can be qualitatively extended to go beyond the statistic paradigm and achieve high-level cognition. Here, we present a minimalist qualitative extension by generalising computational building blocks from vectors to spheres. We propose Sphere Neural Networks (SphNNs) for human-like reasoning through model construction and inspection, and develop SphNN for syllogistic reasoning, a microcosm of human rationality. Instead of training data, SphNN uses a neuro-symbolic transition map of neighbourhood spatial relations to guide transformations from the current sphere configuration towards the target. SphNN is the first neural model th
&lt;/p&gt;</description></item><item><title>CMDI&#32858;&#31867;&#26041;&#27861;&#21019;&#26032;&#24615;&#22320;&#23558;&#20108;&#32500;&#32467;&#26500;&#20449;&#24687;&#29702;&#35770;&#34701;&#20837;&#32858;&#31867;&#36807;&#31243;&#20013;&#65292;&#24357;&#34917;&#20102;&#22522;&#20110;&#22270;&#30340;&#27169;&#22411;&#32858;&#31867;&#26041;&#27861;&#20013;&#24573;&#30053;&#30340;&#38543;&#26426;&#28216;&#36208;&#35775;&#38382;&#33410;&#28857;&#21644;&#25968;&#25454;&#20013;&#23884;&#20837;&#30340;&#32467;&#26500;&#20449;&#24687;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.13846</link><description>&lt;p&gt;
&#19968;&#31181;&#20855;&#26377;&#22270;&#26368;&#22823;&#35299;&#30721;&#20449;&#24687;&#30340;&#32858;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Clustering Method with Graph Maximum Decoding Information
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13846
&lt;/p&gt;
&lt;p&gt;
CMDI&#32858;&#31867;&#26041;&#27861;&#21019;&#26032;&#24615;&#22320;&#23558;&#20108;&#32500;&#32467;&#26500;&#20449;&#24687;&#29702;&#35770;&#34701;&#20837;&#32858;&#31867;&#36807;&#31243;&#20013;&#65292;&#24357;&#34917;&#20102;&#22522;&#20110;&#22270;&#30340;&#27169;&#22411;&#32858;&#31867;&#26041;&#27861;&#20013;&#24573;&#30053;&#30340;&#38543;&#26426;&#28216;&#36208;&#35775;&#38382;&#33410;&#28857;&#21644;&#25968;&#25454;&#20013;&#23884;&#20837;&#30340;&#32467;&#26500;&#20449;&#24687;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22270;&#27169;&#22411;&#30340;&#32858;&#31867;&#26041;&#27861;&#22240;&#20854;&#22312;&#21508;&#31181;&#30693;&#35782;&#39046;&#22495;&#20013;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#32780;&#22791;&#21463;&#20851;&#27880;&#12290;&#20854;&#33021;&#22815;&#19982;&#20854;&#20182;&#30456;&#20851;&#24212;&#29992;&#26080;&#32541;&#38598;&#25104;&#30340;&#36866;&#24212;&#24615;&#36171;&#20104;&#20102;&#22522;&#20110;&#22270;&#27169;&#22411;&#30340;&#32858;&#31867;&#20998;&#26512;&#33021;&#21147;&#65292;&#21487;&#20197;&#24378;&#22823;&#22320;&#20174;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#8220;&#33258;&#28982;&#20851;&#32852;&#8221;&#25110;&#8220;&#22270;&#32467;&#26500;&#8221;&#65292;&#26377;&#21161;&#20110;&#24314;&#27169;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#23613;&#31649;&#36825;&#31181;&#26041;&#27861;&#25928;&#26524;&#26174;&#33879;&#65292;&#20294;&#24403;&#21069;&#21033;&#29992;&#22522;&#20110;&#22270;&#30340;&#27169;&#22411;&#30340;&#32858;&#31867;&#26041;&#27861;&#24573;&#30053;&#20102;&#33410;&#28857;&#20043;&#38388;&#38543;&#26426;&#28216;&#36208;&#35775;&#38382;&#20197;&#21450;&#25968;&#25454;&#20013;&#23884;&#20837;&#30340;&#32467;&#26500;&#20449;&#24687;&#25152;&#24102;&#26469;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#22270;&#30340;&#27169;&#22411;&#20869;&#26368;&#22823;&#21270;&#35299;&#30721;&#20449;&#24687;&#30340;&#32858;&#31867;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;CMDI&#12290;CMDI&#21019;&#26032;&#22320;&#23558;&#20108;&#32500;&#32467;&#26500;&#20449;&#24687;&#29702;&#35770;&#32435;&#20837;&#21040;&#32858;&#31867;&#36807;&#31243;&#20013;&#65292;&#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#65306;&#22270;&#32467;&#26500;&#25552;&#21462;&#21644;&#22270;&#39030;&#28857;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13846v1 Announce Type: cross  Abstract: The clustering method based on graph models has garnered increased attention for its widespread applicability across various knowledge domains. Its adaptability to integrate seamlessly with other relevant applications endows the graph model-based clustering analysis with the ability to robustly extract "natural associations" or "graph structures" within datasets, facilitating the modelling of relationships between data points. Despite its efficacy, the current clustering method utilizing the graph-based model overlooks the uncertainty associated with random walk access between nodes and the embedded structural information in the data. To address this gap, we present a novel Clustering method for Maximizing Decoding Information within graph-based models, named CMDI. CMDI innovatively incorporates two-dimensional structural information theory into the clustering process, consisting of two phases: graph structure extraction and graph vert
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#29992;&#25143;&#21451;&#22909;&#30340;Python&#36719;&#20214;&#21253;&#65292;&#30740;&#31350;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#24182;&#34892;HPO&#26041;&#27861;&#65292;&#36991;&#20813;&#38271;&#26102;&#38388;&#31561;&#24453;&#23454;&#29616;&#24555;&#36895;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2403.01888</link><description>&lt;p&gt;
&#38646;&#25104;&#26412;&#22522;&#20934;&#19978;&#24322;&#27493;&#22810;&#20445;&#30495;&#24230;&#20248;&#21270;&#30340;&#24555;&#36895;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Fast Benchmarking of Asynchronous Multi-Fidelity Optimization on Zero-Cost Benchmarks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01888
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#29992;&#25143;&#21451;&#22909;&#30340;Python&#36719;&#20214;&#21253;&#65292;&#30740;&#31350;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#24182;&#34892;HPO&#26041;&#27861;&#65292;&#36991;&#20813;&#38271;&#26102;&#38388;&#31561;&#24453;&#23454;&#29616;&#24555;&#36895;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#21462;&#24471;&#20102;&#35768;&#22810;&#25104;&#21151;&#65292;&#20294;&#20854;&#32467;&#26524;&#24448;&#24448;&#21462;&#20915;&#20110;&#36229;&#21442;&#25968;&#30340;&#31934;&#24515;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#30340;&#32791;&#26102;&#24615;&#20351;&#24471;&#36229;&#21442;&#25968;&#20248;&#21270;(HPO)&#26159;&#19968;&#39033;&#26114;&#36149;&#30340;&#24037;&#20316;&#65292;&#25302;&#24930;&#20102;&#39640;&#25928;HPO&#24037;&#20855;&#30340;&#24320;&#21457;&#12290;&#26412;&#24037;&#20316;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#30340;Python&#36719;&#20214;&#21253;&#65292;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#20419;&#36827;&#38646;&#25104;&#26412;&#22522;&#20934;&#19979;&#39640;&#25928;&#30340;&#24182;&#34892;HPO&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26681;&#25454;&#23384;&#20648;&#22312;&#25991;&#20214;&#31995;&#32479;&#20013;&#30340;&#20449;&#24687;&#35745;&#31639;&#31934;&#30830;&#30340;&#36820;&#22238;&#39034;&#24207;&#65292;&#28040;&#38500;&#20102;&#38271;&#26102;&#38388;&#30340;&#31561;&#24453;&#65292;&#23454;&#29616;&#20102;&#26356;&#24555;&#30340;HPO&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01888v1 Announce Type: new  Abstract: While deep learning has celebrated many successes, its results often hinge on the meticulous selection of hyperparameters (HPs). However, the time-consuming nature of deep learning training makes HP optimization (HPO) a costly endeavor, slowing down the development of efficient HPO tools. While zero-cost benchmarks, which provide performance and runtime without actual training, offer a solution for non-parallel setups, they fall short in parallel setups as each worker must communicate its queried runtime to return its evaluation in the exact order. This work addresses this challenge by introducing a user-friendly Python package that facilitates efficient parallel HPO with zero-cost benchmarks. Our approach calculates the exact return order based on the information stored in file system, eliminating the need for long waiting times and enabling much faster HPO evaluations. We first verify the correctness of our approach through extensive t
&lt;/p&gt;</description></item><item><title>&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22914;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#12289;&#32852;&#37030;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#20102;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20013;&#35757;&#32451;&#25968;&#25454;&#39046;&#22495;&#20551;&#35774;&#19981;&#31526;&#21512;&#23454;&#38469;&#24773;&#20917;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#24182;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2403.01255</link><description>&lt;p&gt;
&#20351;&#29992;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Automatic Speech Recognition using Advanced Deep Learning Approaches: A survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01255
&lt;/p&gt;
&lt;p&gt;
&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22914;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#12289;&#32852;&#37030;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#20102;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20013;&#35757;&#32451;&#25968;&#25454;&#39046;&#22495;&#20551;&#35774;&#19981;&#31526;&#21512;&#23454;&#38469;&#24773;&#20917;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#24182;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01255v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#39046;&#22495; &#25688;&#35201;&#65306;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#23545;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#26500;&#25104;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;ASR&#20381;&#36182;&#20110;&#21253;&#25324;&#26426;&#23494;&#25968;&#25454;&#22312;&#20869;&#30340;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#21644;&#23384;&#20648;&#36164;&#28304;&#12290;&#21551;&#29992;&#33258;&#36866;&#24212;&#31995;&#32479;&#21487;&#20197;&#25552;&#39640;&#21160;&#24577;&#29615;&#22659;&#19979;&#30340;ASR&#24615;&#33021;&#12290;DL&#25216;&#26415;&#20551;&#35774;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#26469;&#33258;&#21516;&#19968;&#39046;&#22495;&#65292;&#20294;&#24182;&#38750;&#24635;&#26159;&#22914;&#27492;&#12290;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#65288;DTL&#65289;&#12289;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#21644;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31561;&#20808;&#36827;&#30340;DL&#25216;&#26415;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;DTL&#21487;&#20197;&#21033;&#29992;&#23567;&#32780;&#30456;&#20851;&#30340;&#25968;&#25454;&#38598;&#26500;&#24314;&#39640;&#24615;&#33021;&#27169;&#22411;&#65292;FL&#20351;&#24471;&#22312;&#19981;&#25317;&#26377;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#27169;&#22411;&#65292;RL&#20248;&#21270;&#21160;&#24577;&#29615;&#22659;&#19979;&#30340;&#20915;&#31574;&#65292;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;&#26412;&#35843;&#26597;&#20840;&#38754;&#23457;&#38405;&#20102;&#22522;&#20110;DTL&#12289;FL&#21644;RL&#30340;ASR&#26694;&#26550;&#65292;&#26088;&#22312;&#25552;&#20379;&#23545;&#26368;&#26032;&#21457;&#23637;&#30340;&#35265;&#35299;&#65292;&#24182;&#21327;&#21161;&#30740;&#31350;&#20154;&#21592;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01255v1 Announce Type: cross  Abstract: Recent advancements in deep learning (DL) have posed a significant challenge for automatic speech recognition (ASR). ASR relies on extensive training datasets, including confidential ones, and demands substantial computational and storage resources. Enabling adaptive systems improves ASR performance in dynamic environments. DL techniques assume training and testing data originate from the same domain, which is not always true. Advanced DL techniques like deep transfer learning (DTL), federated learning (FL), and reinforcement learning (RL) address these issues. DTL allows high-performance models using small yet related datasets, FL enables training on confidential data without dataset possession, and RL optimizes decision-making in dynamic environments, reducing computation costs. This survey offers a comprehensive review of DTL, FL, and RL-based ASR frameworks, aiming to provide insights into the latest developments and aid researcher
&lt;/p&gt;</description></item><item><title>Sora&#26159;&#19968;&#31181;&#25991;&#26412;&#21040;&#35270;&#39057;&#29983;&#25104;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#23637;&#31034;&#20986;&#22312;&#27169;&#25311;&#29289;&#29702;&#19990;&#30028;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#21644;&#25361;&#25112;&#65292;&#26410;&#26469;&#21457;&#23637;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2402.17177</link><description>&lt;p&gt;
Sora: &#22823;&#22411;&#35270;&#35273;&#27169;&#22411;&#32972;&#26223;&#12289;&#25216;&#26415;&#12289;&#23616;&#38480;&#24615;&#21644;&#26426;&#36935;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17177
&lt;/p&gt;
&lt;p&gt;
Sora&#26159;&#19968;&#31181;&#25991;&#26412;&#21040;&#35270;&#39057;&#29983;&#25104;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#23637;&#31034;&#20986;&#22312;&#27169;&#25311;&#29289;&#29702;&#19990;&#30028;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#21644;&#25361;&#25112;&#65292;&#26410;&#26469;&#21457;&#23637;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Sora&#26159;&#30001;OpenAI&#20110;2024&#24180;2&#26376;&#21457;&#24067;&#30340;&#19968;&#31181;&#25991;&#26412;&#21040;&#35270;&#39057;&#29983;&#25104;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#12290;&#36825;&#20010;&#27169;&#22411;&#32463;&#36807;&#35757;&#32451;&#65292;&#21487;&#20197;&#26681;&#25454;&#25991;&#26412;&#25351;&#20196;&#29983;&#25104;&#36924;&#30495;&#25110;&#24819;&#35937;&#30340;&#22330;&#26223;&#35270;&#39057;&#65292;&#24182;&#22312;&#27169;&#25311;&#29289;&#29702;&#19990;&#30028;&#26041;&#38754;&#26174;&#31034;&#20986;&#28508;&#21147;&#12290;&#26412;&#25991;&#22522;&#20110;&#20844;&#24320;&#30340;&#25216;&#26415;&#25253;&#21578;&#21644;&#36870;&#21521;&#24037;&#31243;&#65292;&#23545;&#36825;&#20010;&#27169;&#22411;&#30340;&#32972;&#26223;&#12289;&#30456;&#20851;&#25216;&#26415;&#12289;&#24212;&#29992;&#12289;&#23578;&#23384;&#30340;&#25361;&#25112;&#20197;&#21450;&#25991;&#26412;&#21040;&#35270;&#39057;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#26410;&#26469;&#26041;&#21521;&#36827;&#34892;&#20102;&#20840;&#38754;&#22238;&#39038;&#12290;&#39318;&#20808;&#25105;&#20204;&#36861;&#28335;&#20102;Sora&#30340;&#21457;&#23637;&#21382;&#31243;&#65292;&#24182;&#35843;&#26597;&#20102;&#29992;&#20110;&#26500;&#24314;&#36825;&#20010;"&#19990;&#30028;&#27169;&#25311;&#22120;"&#30340;&#22522;&#30784;&#25216;&#26415;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35814;&#32454;&#25551;&#36848;&#20102;Sora&#22312;&#20174;&#30005;&#24433;&#21046;&#20316;&#21644;&#25945;&#32946;&#21040;&#33829;&#38144;&#31561;&#22810;&#20010;&#34892;&#19994;&#20013;&#30340;&#24212;&#29992;&#21644;&#28508;&#22312;&#24433;&#21709;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#38656;&#35201;&#35299;&#20915;&#30340;&#20027;&#35201;&#25361;&#25112;&#21644;&#23616;&#38480;&#24615;&#65292;&#20197;&#20415;&#24191;&#27867;&#37096;&#32626;Sora&#65292;&#22914;&#30830;&#20445;&#23433;&#20840;&#21644;&#26080;&#20559;&#35265;&#30340;&#35270;&#39057;&#29983;&#25104;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;Sora&#20197;&#21450;&#35270;&#39057;&#29983;&#25104;&#25216;&#26415;&#26410;&#26469;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17177v1 Announce Type: cross  Abstract: Sora is a text-to-video generative AI model, released by OpenAI in February 2024. The model is trained to generate videos of realistic or imaginative scenes from text instructions and show potential in simulating the physical world. Based on public technical reports and reverse engineering, this paper presents a comprehensive review of the model's background, related technologies, applications, remaining challenges, and future directions of text-to-video AI models. We first trace Sora's development and investigate the underlying technologies used to build this "world simulator". Then, we describe in detail the applications and potential impact of Sora in multiple industries ranging from film-making and education to marketing. We discuss the main challenges and limitations that need to be addressed to widely deploy Sora, such as ensuring safe and unbiased video generation. Lastly, we discuss the future development of Sora and video gene
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Chimera&#26694;&#26550;&#65292;&#29992;&#20110;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#65292;&#36890;&#36807;&#24341;&#20837;&#36731;&#37327;&#32423;&#30340;&#33609;&#31295;&#27169;&#22411;&#21644;&#20004;&#31181;&#31574;&#30053;&#65292;&#21033;&#29992;&#20808;&#21069;&#29983;&#25104;&#30340;&#20196;&#29260;&#26469;&#39044;&#27979;&#21518;&#32493;&#21333;&#35789;&#65292;&#20197;&#35299;&#20915;&#35299;&#30721;&#36807;&#31243;&#20013;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#38382;&#39064;</title><link>https://arxiv.org/abs/2402.15758</link><description>&lt;p&gt;
Chimera: &#34701;&#21512;&#25152;&#26377;&#20196;&#29260;&#30340;&#26080;&#25439;&#35299;&#30721;&#26041;&#27861;&#65292;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Chimera: A Lossless Decoding Method for Accelerating Large Language Models Inference by Fusing all Tokens
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15758
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Chimera&#26694;&#26550;&#65292;&#29992;&#20110;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#65292;&#36890;&#36807;&#24341;&#20837;&#36731;&#37327;&#32423;&#30340;&#33609;&#31295;&#27169;&#22411;&#21644;&#20004;&#31181;&#31574;&#30053;&#65292;&#21033;&#29992;&#20808;&#21069;&#29983;&#25104;&#30340;&#20196;&#29260;&#26469;&#39044;&#27979;&#21518;&#32493;&#21333;&#35789;&#65292;&#20197;&#35299;&#20915;&#35299;&#30721;&#36807;&#31243;&#20013;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#24191;&#27867;&#24212;&#29992;&#34987;&#36164;&#28304;&#23494;&#38598;&#22411;&#30340;&#35299;&#30721;&#36807;&#31243;&#25152;&#38459;&#30861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#30446;&#21069;&#30340;&#26041;&#27861;&#24050;&#32463;&#21512;&#24182;&#20102;&#39069;&#22806;&#30340;&#35299;&#30721;&#22836;&#65292;&#20197;&#23454;&#29616;&#23545;&#22810;&#20010;&#21518;&#32493;&#20196;&#29260;&#30340;&#24182;&#34892;&#39044;&#27979;&#65292;&#20174;&#32780;&#23454;&#29616;&#25512;&#29702;&#21152;&#36895;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35299;&#30721;&#22836;&#30340;&#20934;&#30830;&#24615;&#36828;&#19981;&#21450;&#33258;&#22238;&#24402;&#35299;&#30721;&#26041;&#27861;&#12290;&#37492;&#20110;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Chimera&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#25512;&#27979;&#37319;&#26679;&#35774;&#35745;&#30340;&#26032;&#26694;&#26550;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20869;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#33609;&#31295;&#27169;&#22411;&#65292;&#33021;&#22815;&#26377;&#25928;&#21033;&#29992;&#20808;&#21069;&#29983;&#25104;&#30340;&#20196;&#29260;&#26469;&#39044;&#27979;&#21518;&#32493;&#21333;&#35789;&#12290;&#20026;&#20102;&#30830;&#20445;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#65292;&#25105;&#20204;&#22312;&#36731;&#37327;&#32423;&#33609;&#31295;&#27169;&#22411;&#20013;&#25552;&#20986;&#20102;&#20004;&#31181;&#31574;&#30053;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#22312;&#24213;&#23618;&#25429;&#33719;&#30701;&#31243;&#20381;&#36182;&#24615;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#21033;&#29992;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15758v1 Announce Type: cross  Abstract: Large language models (LLMs) have demonstrated remarkable capabilities across various tasks. However, their widespread application is hindered by the resource-intensive decoding process. To address this challenge, current approaches have incorporated additional decoding heads to enable parallel prediction of multiple subsequent tokens, thereby achieving inference acceleration. Nevertheless, the accuracy of these decoding heads falls short of the auto-regressive decoding approach.   In light of these limitations, we propose Chimera, a novel framework specifically designed for speculative sampling. Within this framework, we introduce a lightweight draft model that effectively utilizes previously generated tokens to predict subsequent words. To ensure both accuracy and efficiency, we present two strategies within the lightweight draft model. Firstly, we focus on capturing short-range dependencies at the bottom layer. Secondly, we leverage
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#27969;&#31243;&#65292;&#21033;&#29992;GPT-3.5&#21644;GPT-4&#29983;&#25104;&#39640;&#36136;&#37327;&#21453;&#39304;&#65292;&#20197;&#22686;&#24378;&#20020;&#24202;&#31508;&#35760;&#25688;&#35201;&#20013;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#65292;&#24357;&#34917;&#20102;&#19987;&#23478;&#27880;&#37322;&#25968;&#25454;&#30340;&#39640;&#25104;&#26412;&#21644;&#26377;&#38480;&#21487;&#29992;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.13919</link><description>&lt;p&gt;
SYNFAC-EDIT: &#29992;&#20110;&#20020;&#24202;&#25688;&#35201;&#20013;&#30340;&#20107;&#23454;&#23545;&#40784;&#30340;&#21512;&#25104;&#27169;&#20223;&#32534;&#36753;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
SYNFAC-EDIT: Synthetic Imitation Edit Feedback for Factual Alignment in Clinical Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13919
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#27969;&#31243;&#65292;&#21033;&#29992;GPT-3.5&#21644;GPT-4&#29983;&#25104;&#39640;&#36136;&#37327;&#21453;&#39304;&#65292;&#20197;&#22686;&#24378;&#20020;&#24202;&#31508;&#35760;&#25688;&#35201;&#20013;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#65292;&#24357;&#34917;&#20102;&#19987;&#23478;&#27880;&#37322;&#25968;&#25454;&#30340;&#39640;&#25104;&#26412;&#21644;&#26377;&#38480;&#21487;&#29992;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;GPT&#21644;Llama&#22312;&#25688;&#35201;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#22312;&#20107;&#23454;&#19981;&#20934;&#30830;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#36825;&#26159;&#20020;&#24202;NLP&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#38169;&#35823;&#21487;&#33021;&#23548;&#33268;&#20005;&#37325;&#21518;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#20107;&#23454;&#23545;&#40784;&#30340;&#19987;&#23478;&#27880;&#37322;&#25968;&#25454;&#25104;&#26412;&#39640;&#26114;&#19988;&#26377;&#38480;&#30340;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#27969;&#31243;&#65292;&#21033;&#29992;GPT-3.5&#21644;GPT-4&#29983;&#25104;&#39640;&#36136;&#37327;&#21453;&#39304;&#65292;&#26088;&#22312;&#22686;&#24378;&#20020;&#24202;&#31508;&#35760;&#25688;&#35201;&#20013;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#32534;&#36753;&#21453;&#39304;&#65292;&#22312;&#27809;&#26377;&#39069;&#22806;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#65292;&#27169;&#25311;&#20102;&#21307;&#30103;&#19987;&#19994;&#20154;&#21592;&#25913;&#21892;AI&#31995;&#32479;&#36755;&#20986;&#30340;&#23454;&#38469;&#22330;&#26223;&#12290;&#23613;&#31649;GPT&#22312;&#21508;&#31181;&#20020;&#24202;NLP&#20219;&#21153;&#20013;&#37117;&#34920;&#29616;&#20986;&#20102;&#19987;&#19994;&#27700;&#24179;&#65292;&#27604;&#22914;&#21307;&#23398;&#25191;&#29031;&#32771;&#35797;&#65292;&#20294;&#23545;&#20854;&#25552;&#20379;&#25913;&#21892;&#36739;&#24369;LM&#25110;LLM&#29983;&#25104;&#36136;&#37327;&#30340;&#19987;&#19994;&#32423;&#32534;&#36753;&#21453;&#39304;&#30340;&#30740;&#31350;&#24456;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13919v1 Announce Type: cross  Abstract: Large Language Models (LLMs) such as GPT and Llama have demonstrated significant achievements in summarization tasks but struggle with factual inaccuracies, a critical issue in clinical NLP applications where errors could lead to serious consequences. To counter the high costs and limited availability of expert-annotated data for factual alignment, this study introduces an innovative pipeline that utilizes GPT-3.5 and GPT-4 to generate high-quality feedback aimed at enhancing factual consistency in clinical note summarization. Our research primarily focuses on edit feedback, mirroring the practical scenario in which medical professionals refine AI system outputs without the need for additional annotations. Despite GPT's proven expertise in various clinical NLP tasks, such as the Medical Licensing Examination, there is scant research on its capacity to deliver expert-level edit feedback for improving weaker LMs or LLMs generation qualit
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#26234;&#33021;&#30340;&#36807;&#24230;&#33258;&#20449;&#21644;&#32570;&#20047;&#33258;&#20449;&#20250;&#38459;&#30861;&#20154;&#26426;&#21327;&#20316;&#65292;&#25259;&#38706;&#20449;&#24515;&#27700;&#24179;&#21644;&#25552;&#20379;&#21453;&#39304;&#26377;&#21161;&#20110;&#35748;&#35782;&#21040;&#20154;&#24037;&#26234;&#33021;&#30340;&#20449;&#24515;&#19981;&#19968;&#33268;&#65292;&#20294;&#21442;&#19982;&#32773;&#24448;&#24448;&#22240;&#27492;&#19981;&#20449;&#20219;&#20154;&#24037;&#26234;&#33021;&#30340;&#24314;&#35758;&#65292;&#23548;&#33268;&#21327;&#20316;&#32467;&#26524;&#36739;&#24046;&#12290;</title><link>https://arxiv.org/abs/2402.07632</link><description>&lt;p&gt;
&#36807;&#20110;&#33258;&#20449;&#21644;&#32570;&#20047;&#33258;&#20449;&#30340;&#20154;&#24037;&#26234;&#33021;&#38459;&#30861;&#20154;&#26426;&#21327;&#20316;
&lt;/p&gt;
&lt;p&gt;
Overconfident and Unconfident AI Hinder Human-AI Collaboration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07632
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#36807;&#24230;&#33258;&#20449;&#21644;&#32570;&#20047;&#33258;&#20449;&#20250;&#38459;&#30861;&#20154;&#26426;&#21327;&#20316;&#65292;&#25259;&#38706;&#20449;&#24515;&#27700;&#24179;&#21644;&#25552;&#20379;&#21453;&#39304;&#26377;&#21161;&#20110;&#35748;&#35782;&#21040;&#20154;&#24037;&#26234;&#33021;&#30340;&#20449;&#24515;&#19981;&#19968;&#33268;&#65292;&#20294;&#21442;&#19982;&#32773;&#24448;&#24448;&#22240;&#27492;&#19981;&#20449;&#20219;&#20154;&#24037;&#26234;&#33021;&#30340;&#24314;&#35758;&#65292;&#23548;&#33268;&#21327;&#20316;&#32467;&#26524;&#36739;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#27493;&#65292;&#20154;&#26426;&#21327;&#20316;&#22312;&#19987;&#19994;&#21644;&#26085;&#24120;&#22330;&#26223;&#20013;&#36234;&#26469;&#36234;&#26222;&#36941;&#12290;&#22312;&#36825;&#31181;&#21327;&#20316;&#20013;&#65292;&#20154;&#24037;&#26234;&#33021;&#21487;&#20197;&#34920;&#36798;&#20854;&#23545;&#33258;&#24049;&#34920;&#29616;&#30340;&#20449;&#24515;&#27700;&#24179;&#65292;&#20316;&#20026;&#20154;&#31867;&#35780;&#20272;&#20154;&#24037;&#26234;&#33021;&#24314;&#35758;&#30340;&#37325;&#35201;&#25351;&#26631;&#12290;&#28982;&#32780;&#65292;&#20154;&#24037;&#26234;&#33021;&#21487;&#33021;&#34920;&#29616;&#20986;&#36807;&#24230;&#33258;&#20449;&#25110;&#32570;&#20047;&#33258;&#20449;&#65292;&#21363;&#20854;&#34920;&#36798;&#30340;&#20449;&#24515;&#39640;&#20110;&#25110;&#20302;&#20110;&#20854;&#23454;&#38469;&#34920;&#29616;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#20154;&#20204;&#38169;&#35823;&#22320;&#35780;&#20272;&#20154;&#24037;&#26234;&#33021;&#30340;&#24314;&#35758;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#35843;&#26597;&#20102;&#20154;&#24037;&#26234;&#33021;&#36807;&#24230;&#33258;&#20449;&#21644;&#32570;&#20047;&#33258;&#20449;&#23545;&#20154;&#31867;&#20449;&#20219;&#12289;&#25509;&#21463;&#20154;&#24037;&#26234;&#33021;&#24314;&#35758;&#21644;&#21327;&#20316;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#25259;&#38706;&#20154;&#24037;&#26234;&#33021;&#30340;&#20449;&#24515;&#27700;&#24179;&#21644;&#34920;&#29616;&#21453;&#39304;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#35748;&#35782;&#20154;&#24037;&#26234;&#33021;&#20449;&#24515;&#19981;&#19968;&#33268;&#12290;&#28982;&#32780;&#65292;&#21442;&#19982;&#32773;&#24448;&#24448;&#20250;&#22240;&#20026;&#23519;&#35273;&#21040;&#36825;&#31181;&#19981;&#19968;&#33268;&#32780;&#19981;&#20449;&#20219;&#20154;&#24037;&#26234;&#33021;&#30340;&#24314;&#35758;&#65292;&#23548;&#33268;&#25298;&#32477;&#20154;&#24037;&#26234;&#33021;&#30340;&#24314;&#35758;&#65292;&#24182;&#19988;&#22312;&#21327;&#20316;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#24046;&#12290;&#30456;&#21453;&#65292;&#27809;&#26377;&#36825;&#20123;&#25552;&#31034;&#30340;&#24773;&#20917;&#19979;&#65292;&#21442;&#19982;&#32773;&#26356;&#23481;&#26131;&#20449;&#20219;&#20154;&#24037;&#26234;&#33021;&#24182;&#25509;&#21463;&#20854;&#24314;&#35758;&#65292;&#20174;&#32780;&#22312;&#21327;&#20316;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
As artificial intelligence (AI) advances, human-AI collaboration has become increasingly prevalent across both professional and everyday settings. In such collaboration, AI can express its confidence level about its performance, serving as a crucial indicator for humans to evaluate AI's suggestions. However, AI may exhibit overconfidence or underconfidence--its expressed confidence is higher or lower than its actual performance--which may lead humans to mistakenly evaluate AI advice. Our study investigates the influences of AI's overconfidence and underconfidence on human trust, their acceptance of AI suggestions, and collaboration outcomes. Our study reveal that disclosing AI confidence levels and performance feedback facilitates better recognition of AI confidence misalignments. However, participants tend to withhold their trust as perceiving such misalignments, leading to a rejection of AI suggestions and subsequently poorer performance in collaborative tasks. Conversely, without su
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;GLaM&#26041;&#27861;&#65292;&#36890;&#36807;&#37051;&#22495;&#21010;&#20998;&#21644;&#29983;&#25104;&#23376;&#22270;&#32534;&#30721;&#65292;&#23545;&#39046;&#22495;&#30693;&#35782;&#22270;&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#12290;&#35813;&#26041;&#27861;&#30340;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#33021;&#22815;&#23454;&#29616;&#23545;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#22810;&#27493;&#25512;&#29702;&#65292;&#24182;&#20943;&#23569;&#34394;&#26500;&#12290;</title><link>https://arxiv.org/abs/2402.06764</link><description>&lt;p&gt;
&#36890;&#36807;&#37051;&#22495;&#21010;&#20998;&#21644;&#29983;&#25104;&#23376;&#22270;&#32534;&#30721;&#23545;&#39046;&#22495;&#30693;&#35782;&#22270;&#23545;&#40784;&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#30340;GLaM&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
GLaM: Fine-Tuning Large Language Models for Domain Knowledge Graph Alignment via Neighborhood Partitioning and Generative Subgraph Encoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06764
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;GLaM&#26041;&#27861;&#65292;&#36890;&#36807;&#37051;&#22495;&#21010;&#20998;&#21644;&#29983;&#25104;&#23376;&#22270;&#32534;&#30721;&#65292;&#23545;&#39046;&#22495;&#30693;&#35782;&#22270;&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#12290;&#35813;&#26041;&#27861;&#30340;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#33021;&#22815;&#23454;&#29616;&#23545;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#22810;&#27493;&#25512;&#29702;&#65292;&#24182;&#20943;&#23569;&#34394;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20174;&#29305;&#23450;&#39046;&#22495;&#25968;&#25454;&#27966;&#29983;&#30340;&#30693;&#35782;&#22270;&#38598;&#25104;&#65292;&#20195;&#34920;&#20102;&#26397;&#30528;&#26356;&#24378;&#22823;&#21644;&#20107;&#23454;&#25512;&#29702;&#30340;&#37325;&#35201;&#36827;&#23637;&#12290;&#38543;&#30528;&#36825;&#20123;&#27169;&#22411;&#21464;&#24471;&#36234;&#26469;&#36234;&#24378;&#22823;&#65292;&#20351;&#23427;&#20204;&#33021;&#22815;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#30693;&#35782;&#22270;&#19978;&#36827;&#34892;&#22810;&#27493;&#25512;&#29702;&#65292;&#24182;&#23613;&#37327;&#20943;&#23569;&#34394;&#26500;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#20114;&#36830;&#23454;&#20307;&#30340;&#39046;&#22495;&#19987;&#29992;&#22270;&#26102;&#65292;&#33021;&#21147;&#20173;&#28982;&#26377;&#38480;&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#22635;&#34917;&#36825;&#19968;&#25216;&#26415;&#19978;&#30340;&#37325;&#35201;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
Integrating large language models (LLMs) with knowledge graphs derived from domain-specific data represents an important advancement towards more powerful and factual reasoning. As these models grow more capable, it is crucial to enable them to perform multi-step inferences over real-world knowledge graphs while minimizing hallucination. While large language models excel at conversation and text generation, their ability to reason over domain-specialized graphs of interconnected entities remains limited. For example, can we query a LLM to identify the optimal contact in a professional network for a specific goal, based on relationships and attributes in a private database? The answer is no--such capabilities lie beyond current methods. However, this question underscores a critical technical gap that must be addressed. Many high-value applications in areas such as science, security, and e-commerce rely on proprietary knowledge graphs encoding unique structures, relationships, and logica
&lt;/p&gt;</description></item><item><title>ViGoR&#36890;&#36807;&#32454;&#31890;&#24230;&#22870;&#21169;&#24314;&#27169;&#25552;&#39640;&#20102;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#35270;&#35273;&#23545;&#25509;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#20154;&#24037;&#35780;&#20272;&#21644;&#33258;&#21160;&#21270;&#26041;&#27861;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#35270;&#35273;&#23545;&#25509;&#20013;&#30340;&#35823;&#24046;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.06118</link><description>&lt;p&gt;
ViGoR&#65306;&#36890;&#36807;&#32454;&#31890;&#24230;&#22870;&#21169;&#24314;&#27169;&#25913;&#36827;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#35270;&#35273;&#23545;&#25509;
&lt;/p&gt;
&lt;p&gt;
ViGoR: Improving Visual Grounding of Large Vision Language Models with Fine-Grained Reward Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06118
&lt;/p&gt;
&lt;p&gt;
ViGoR&#36890;&#36807;&#32454;&#31890;&#24230;&#22870;&#21169;&#24314;&#27169;&#25552;&#39640;&#20102;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#35270;&#35273;&#23545;&#25509;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#20154;&#24037;&#35780;&#20272;&#21644;&#33258;&#21160;&#21270;&#26041;&#27861;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#35270;&#35273;&#23545;&#25509;&#20013;&#30340;&#35823;&#24046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#12289;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#21644;&#24191;&#27867;&#30693;&#35782;&#19982;&#22270;&#20687;&#24863;&#30693;&#30456;&#32467;&#21512;&#65292;&#26368;&#36817;&#30340;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#23637;&#31034;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#30340;&#25991;&#26412;&#24448;&#24448;&#22312;&#35270;&#35273;&#36755;&#20837;&#20013;&#23384;&#22312;&#19981;&#20934;&#30830;&#30340;&#23545;&#25509;&#65292;&#23548;&#33268;&#38169;&#35823;&#65292;&#22914;&#20135;&#29983;&#24187;&#35273;&#30340;&#19981;&#23384;&#22312;&#22330;&#26223;&#20803;&#32032;&#12289;&#36951;&#28431;&#37325;&#35201;&#30340;&#22330;&#26223;&#37096;&#20998;&#65292;&#20197;&#21450;&#25512;&#27979;&#23545;&#35937;&#20043;&#38388;&#30340;&#23646;&#24615;&#21644;&#20851;&#31995;&#26102;&#20986;&#29616;&#38169;&#35823;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;ViGoR&#65288;&#36890;&#36807;&#32454;&#31890;&#24230;&#22870;&#21169;&#24314;&#27169;&#36827;&#34892;&#35270;&#35273;&#23545;&#25509;&#65289;&#65292;&#23427;&#21033;&#29992;&#32454;&#31890;&#24230;&#22870;&#21169;&#24314;&#27169;&#26469;&#26174;&#33879;&#25552;&#21319;&#22522;&#20110;&#39044;&#35757;&#32451;&#22522;&#32447;&#30340;LVLMs&#30340;&#35270;&#35273;&#23545;&#25509;&#33021;&#21147;&#12290;&#36825;&#31181;&#25913;&#36827;&#36890;&#36807;&#20351;&#29992;&#27604;&#23436;&#20840;&#30417;&#30563;&#26356;&#20415;&#23452;&#30340;&#20154;&#24037;&#35780;&#20272;&#21644;&#33258;&#21160;&#21270;&#26041;&#27861;&#39640;&#25928;&#23454;&#29616;&#12290;&#25105;&#20204;&#36890;&#36807;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#30340;&#22810;&#20010;&#25351;&#26631;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
By combining natural language understanding and the generation capabilities and breadth of knowledge of large language models with image perception, recent large vision language models (LVLMs) have shown unprecedented reasoning capabilities in the real world. However, the generated text often suffers from inaccurate grounding in the visual input, resulting in errors such as hallucinating nonexistent scene elements, missing significant parts of the scene, and inferring incorrect attributes and relationships between objects. To address these issues, we introduce a novel framework, ViGoR (Visual Grounding Through Fine-Grained Reward Modeling) that utilizes fine-grained reward modeling to significantly enhance the visual grounding of LVLMs over pre-trained baselines. This improvement is efficiently achieved using much cheaper human evaluations instead of full supervisions, as well as automated methods. We show the effectiveness of our approach through numerous metrics on several benchmarks
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23454;&#26102;&#25512;&#29702;&#36895;&#24230;&#19979;&#23454;&#29616;&#39640;&#20998;&#21106;&#20934;&#30830;&#24615;&#30340;&#22810;&#32423;&#29305;&#24449;&#32858;&#21512;&#21644;&#36882;&#24402;&#23545;&#40784;&#32593;&#32476;&#12290;&#20351;&#29992;ResNet-18&#20316;&#20026;&#39592;&#24178;&#65292;&#36890;&#36807;&#22810;&#32423;&#29305;&#24449;&#32858;&#21512;&#27169;&#22359;&#21644;&#36882;&#24402;&#23545;&#40784;&#27169;&#22359;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02286</link><description>&lt;p&gt;
&#22810;&#32423;&#29305;&#24449;&#32858;&#21512;&#21644;&#36882;&#24402;&#23545;&#40784;&#32593;&#32476;&#29992;&#20110;&#23454;&#26102;&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Multi-Level Feature Aggregation and Recursive Alignment Network for Real-Time Semantic Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02286
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23454;&#26102;&#25512;&#29702;&#36895;&#24230;&#19979;&#23454;&#29616;&#39640;&#20998;&#21106;&#20934;&#30830;&#24615;&#30340;&#22810;&#32423;&#29305;&#24449;&#32858;&#21512;&#21644;&#36882;&#24402;&#23545;&#40784;&#32593;&#32476;&#12290;&#20351;&#29992;ResNet-18&#20316;&#20026;&#39592;&#24178;&#65292;&#36890;&#36807;&#22810;&#32423;&#29305;&#24449;&#32858;&#21512;&#27169;&#22359;&#21644;&#36882;&#24402;&#23545;&#40784;&#27169;&#22359;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#26102;&#35821;&#20041;&#20998;&#21106;&#23545;&#20110;&#23454;&#38469;&#24212;&#29992;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#26041;&#27861;&#37117;&#30528;&#37325;&#20110;&#38477;&#20302;&#35745;&#31639;&#22797;&#26434;&#24615;&#21644;&#27169;&#22411;&#22823;&#23567;&#65292;&#20294;&#21516;&#26102;&#29306;&#29298;&#20102;&#20934;&#30830;&#24615;&#12290;&#22312;&#19968;&#20123;&#22330;&#26223;&#19979;&#65292;&#22914;&#33258;&#20027;&#23548;&#33322;&#21644;&#39550;&#39542;&#21592;&#36741;&#21161;&#31995;&#32479;&#65292;&#20934;&#30830;&#24615;&#21644;&#36895;&#24230;&#21516;&#26679;&#37325;&#35201;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#32423;&#29305;&#24449;&#32858;&#21512;&#21644;&#36882;&#24402;&#23545;&#40784;&#32593;&#32476;&#65288;MFARANet&#65289;&#65292;&#26088;&#22312;&#23454;&#29616;&#39640;&#20998;&#21106;&#20934;&#30830;&#24615;&#21644;&#23454;&#26102;&#25512;&#29702;&#36895;&#24230;&#12290;&#25105;&#20204;&#20351;&#29992;ResNet-18&#20316;&#20026;&#39592;&#24178;&#26469;&#20445;&#35777;&#25928;&#29575;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#20010;&#26680;&#24515;&#32452;&#20214;&#26469;&#24357;&#34917;&#27973;&#39592;&#24178;&#24341;&#36215;&#30340;&#27169;&#22411;&#23481;&#37327;&#20943;&#23569;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#22810;&#32423;&#29305;&#24449;&#32858;&#21512;&#27169;&#22359;&#65288;MFAM&#65289;&#65292;&#23558;&#32534;&#30721;&#22120;&#20013;&#30340;&#20998;&#23618;&#29305;&#24449;&#32858;&#21512;&#21040;&#27599;&#20010;&#23610;&#24230;&#65292;&#20197;&#20415;&#20110;&#21518;&#32493;&#30340;&#31354;&#38388;&#23545;&#40784;&#21644;&#22810;&#23610;&#24230;&#25512;&#29702;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#32467;&#21512;&#22522;&#20110;&#27969;&#30340;&#23545;&#40784;&#26469;&#24314;&#31435;&#36882;&#24402;&#23545;&#40784;&#27169;&#22359;&#65288;RAM&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-time semantic segmentation is a crucial research for real-world applications. However, many methods lay particular emphasis on reducing the computational complexity and model size, while largely sacrificing the accuracy. In some scenarios, such as autonomous navigation and driver assistance system, accuracy and speed are equally important. To tackle this problem, we propose a novel Multi-level Feature Aggregation and Recursive Alignment Network (MFARANet), aiming to achieve high segmentation accuracy at real-time inference speed. We employ ResNet-18 as the backbone to ensure efficiency, and propose three core components to compensate for the reduced model capacity due to the shallow backbone. Specifically, we first design Multi-level Feature Aggregation Module (MFAM) to aggregate the hierarchical features in the encoder to each scale to benefit subsequent spatial alignment and multi-scale inference. Then, we build Recursive Alignment Module (RAM) by combining the flow-based alignm
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20840;&#38754;&#35299;&#37322;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#27599;&#20010;&#28508;&#22312;&#22240;&#32032;&#12290;&#23545;&#20110;&#19981;&#21516;&#29983;&#25104;&#27169;&#22411;&#30340;&#35299;&#37322;&#65292;&#25105;&#20204;&#37327;&#21270;&#20102;&#35299;&#37322;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#36890;&#36807;&#22810;&#27169;&#24577;&#27169;&#22411;&#36827;&#34892;&#20102;&#24615;&#33021;&#35780;&#20272;&#65292;&#19988;&#23450;&#24615;&#22320;&#23637;&#31034;&#20102;&#28508;&#22312;&#22240;&#32032;&#30340;&#21464;&#21270;&#25928;&#26524;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#35299;&#37322;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.01858</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#35299;&#37322;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Explaining latent representations of generative models with large multimodal models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01858
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20840;&#38754;&#35299;&#37322;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#27599;&#20010;&#28508;&#22312;&#22240;&#32032;&#12290;&#23545;&#20110;&#19981;&#21516;&#29983;&#25104;&#27169;&#22411;&#30340;&#35299;&#37322;&#65292;&#25105;&#20204;&#37327;&#21270;&#20102;&#35299;&#37322;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#36890;&#36807;&#22810;&#27169;&#24577;&#27169;&#22411;&#36827;&#34892;&#20102;&#24615;&#33021;&#35780;&#20272;&#65292;&#19988;&#23450;&#24615;&#22320;&#23637;&#31034;&#20102;&#28508;&#22312;&#22240;&#32032;&#30340;&#21464;&#21270;&#25928;&#26524;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#35299;&#37322;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#21487;&#35299;&#37322;&#30340;&#25968;&#25454;&#29983;&#25104;&#28508;&#22312;&#22240;&#32032;&#34920;&#31034;&#26159;&#20154;&#24037;&#26234;&#33021;&#21457;&#23637;&#30340;&#37325;&#35201;&#35838;&#39064;&#12290;&#38543;&#30528;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#20852;&#36215;&#65292;&#23427;&#21487;&#20197;&#23558;&#22270;&#20687;&#19982;&#25991;&#26412;&#23545;&#40784;&#20197;&#29983;&#25104;&#31572;&#26696;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20840;&#38754;&#35299;&#37322;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#27599;&#20010;&#28508;&#22312;&#22240;&#32032;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#37327;&#21270;&#35780;&#20272;&#20102;&#25105;&#20204;&#29983;&#25104;&#30340;&#35299;&#37322;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#22312;&#22810;&#20010;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20043;&#38388;&#35780;&#20272;&#20102;&#35299;&#37322;&#29983;&#25104;&#30340;&#24615;&#33021;&#65292;&#24182;&#23450;&#24615;&#22320;&#21487;&#35270;&#21270;&#20102;&#27599;&#20010;&#28508;&#22312;&#22240;&#32032;&#30340;&#21464;&#21270;&#65292;&#20197;&#23398;&#20064;&#19981;&#21516;&#29983;&#25104;&#27169;&#22411;&#23545;&#35299;&#37322;&#30340;&#35299;&#32544;&#25928;&#26524;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#35299;&#37322;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning interpretable representations of data generative latent factors is an important topic for the development of artificial intelligence. With the rise of the large multimodal model, it can align images with text to generate answers. In this work, we propose a framework to comprehensively explain each latent factor in the generative models using a large multimodal model. We further measure the uncertainty of our generated explanations, quantitatively evaluate the performance of explanation generation among multiple large multimodal models, and qualitatively visualize the variations of each latent factor to learn the disentanglement effects of different generative models on explanations. Finally, we discuss the explanatory capabilities and limitations of state-of-the-art large multimodal models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27979;&#35797;&#20102;5&#31181;&#19981;&#21516;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22270;&#25512;&#29702;&#38382;&#39064;&#19978;&#30340;&#25512;&#29702;&#28145;&#24230;&#65292;&#24182;&#21457;&#29616;&#20102;LLMs&#30340;&#23616;&#38480;&#24615;&#12289;&#20559;&#35265;&#21644;&#23646;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;LLMs&#23545;&#20110;&#33410;&#28857;&#36941;&#21382;&#33258;&#30001;&#24230;&#30340;&#24179;&#22343;&#24230;&#25968;&#21576;&#21453;&#21521;&#20851;&#31995;&#65292;k-shot&#25552;&#31034;&#23545;&#22270;&#25512;&#29702;&#20219;&#21153;&#26377;&#36127;&#38754;&#24433;&#21709;&#65292;&#24182;&#19988;LLMs&#23384;&#22312;&#31215;&#26497;&#30340;&#22238;&#24212;&#20559;&#24046;&#65292;&#26080;&#27861;&#35782;&#21035;&#26377;&#25928;&#35299;&#30340;&#32570;&#22833;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#25512;&#29702;&#25552;&#31034;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2402.01805</link><description>&lt;p&gt;
&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#22270;&#25512;&#29702;&#30340;&#23616;&#38480;&#24615;
&lt;/p&gt;
&lt;p&gt;
Exploring the Limitations of Graph Reasoning in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01805
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27979;&#35797;&#20102;5&#31181;&#19981;&#21516;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22270;&#25512;&#29702;&#38382;&#39064;&#19978;&#30340;&#25512;&#29702;&#28145;&#24230;&#65292;&#24182;&#21457;&#29616;&#20102;LLMs&#30340;&#23616;&#38480;&#24615;&#12289;&#20559;&#35265;&#21644;&#23646;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;LLMs&#23545;&#20110;&#33410;&#28857;&#36941;&#21382;&#33258;&#30001;&#24230;&#30340;&#24179;&#22343;&#24230;&#25968;&#21576;&#21453;&#21521;&#20851;&#31995;&#65292;k-shot&#25552;&#31034;&#23545;&#22270;&#25512;&#29702;&#20219;&#21153;&#26377;&#36127;&#38754;&#24433;&#21709;&#65292;&#24182;&#19988;LLMs&#23384;&#22312;&#31215;&#26497;&#30340;&#22238;&#24212;&#20559;&#24046;&#65292;&#26080;&#27861;&#35782;&#21035;&#26377;&#25928;&#35299;&#30340;&#32570;&#22833;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#25512;&#29702;&#25552;&#31034;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20165;&#36890;&#36807;&#22522;&#20110;&#35821;&#35328;&#30340;&#25552;&#31034;&#23601;&#23637;&#31034;&#20102;&#21508;&#31181;&#31867;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#22270;&#25512;&#29702;&#38382;&#39064;&#27979;&#35797;&#20102;5&#31181;&#19981;&#21516;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;GPT-4&#65292;GPT-3.5&#65292;Claude-2&#65292;Llama-2&#21644;Palm-2&#65289;&#30340;&#25512;&#29702;&#28145;&#24230;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;10&#20010;&#19981;&#21516;&#30340;&#22270;&#36941;&#21382;&#38382;&#39064;&#65292;&#27599;&#20010;&#38382;&#39064;&#20195;&#34920;&#30528;&#36880;&#27493;&#22686;&#21152;&#30340;&#22797;&#26434;&#24615;&#27700;&#24179;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#19981;&#21516;&#22270;&#22823;&#23567;&#20197;&#21450;&#19981;&#21516;&#24418;&#24335;&#30340;k-shot&#25552;&#31034;&#30340;&#35774;&#32622;&#20998;&#26512;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#36825;&#20010;&#22522;&#20934;&#27979;&#35797;&#36807;&#31243;&#65292;&#25105;&#20204;&#20984;&#26174;&#20102;LLMs&#30340;&#21508;&#31181;&#23616;&#38480;&#24615;&#12289;&#20559;&#35265;&#21644;&#23646;&#24615;&#65292;&#27604;&#22914;&#19982;&#27599;&#20010;&#33410;&#28857;&#30340;&#36941;&#21382;&#33258;&#30001;&#24230;&#30340;&#24179;&#22343;&#24230;&#25968;&#21576;&#21453;&#21521;&#20851;&#31995;&#65292;k-shot&#25552;&#31034;&#23545;&#22270;&#25512;&#29702;&#20219;&#21153;&#30340;&#25972;&#20307;&#36127;&#38754;&#24433;&#21709;&#65292;&#20197;&#21450;&#31215;&#26497;&#30340;&#22238;&#24212;&#20559;&#24046;&#23548;&#33268;LLMs&#26080;&#27861;&#35782;&#21035;&#26377;&#25928;&#35299;&#30340;&#32570;&#22833;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#25216;&#26415;&#65292;&#19987;&#38376;&#29992;&#20110;&#22270;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained Large Language Models have demonstrated various types of reasoning capabilities through language-based prompts alone. However, in this paper, we test the depth of graph reasoning for 5 different LLMs (GPT-4, GPT-3.5, Claude-2, Llama-2 and Palm-2) through the problems of graph reasoning. In particular, we design 10 distinct problems of graph traversal, each representing increasing levels of complexity. Further, we analyze the performance of models across various settings such as varying sizes of graphs as well as different forms of k-shot prompting. We highlight various limitations, biases, and properties of LLMs through this benchmarking process, such as an inverse relation to the average degrees of freedom of traversal per node in graphs, the overall negative impact of k-shot prompting on graph reasoning tasks, and a positive response bias which prevents LLMs from identifying the absence of a valid solution. Finally, we propose a new prompting technique specially designed f
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#25193;&#23637;&#20102;&#22270;&#32534;&#36753;&#20316;&#20026;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#20808;&#21069;&#21162;&#21147;&#65292;&#25506;&#35752;&#20102;&#23558;&#36755;&#20837;&#25968;&#25454;&#34920;&#31034;&#20026;&#22270;&#24418;&#23545;&#20110;&#29983;&#25104;&#40657;&#31665;&#22270;&#20687;&#20998;&#31867;&#22120;&#26368;&#23567;&#19988;&#26377;&#24847;&#20041;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#24615;&#33021;&#21644;&#26102;&#38388;&#25928;&#29575;&#26368;&#20339;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2401.11609</link><description>&lt;p&gt;
&#22270;&#32534;&#36753;&#29992;&#20110;&#21453;&#20107;&#23454;&#35299;&#37322;&#65306;&#19968;&#39033;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Graph Edits for Counterfactual Explanations: A comparative study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.11609
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#25193;&#23637;&#20102;&#22270;&#32534;&#36753;&#20316;&#20026;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#20808;&#21069;&#21162;&#21147;&#65292;&#25506;&#35752;&#20102;&#23558;&#36755;&#20837;&#25968;&#25454;&#34920;&#31034;&#20026;&#22270;&#24418;&#23545;&#20110;&#29983;&#25104;&#40657;&#31665;&#22270;&#20687;&#20998;&#31867;&#22120;&#26368;&#23567;&#19988;&#26377;&#24847;&#20041;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#24615;&#33021;&#21644;&#26102;&#38388;&#25928;&#29575;&#26368;&#20339;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#24050;&#34987;&#30830;&#31435;&#20026;&#19968;&#31181;&#27969;&#34892;&#30340;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#65292;&#21033;&#29992;&#19968;&#32452;&#26368;&#23567;&#30340;&#32534;&#36753;&#26469;&#25913;&#21464;&#20998;&#31867;&#22120;&#30340;&#39044;&#27979;&#12290;&#22312;&#32771;&#34385;&#22270;&#20687;&#19978;&#30340;&#27010;&#24565;&#21453;&#20107;&#23454;&#26102;&#65292;&#35831;&#27714;&#30340;&#32534;&#36753;&#24212;&#23545;&#24212;&#36755;&#20837;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#26174;&#33879;&#27010;&#24565;&#12290;&#21516;&#26102;&#65292;&#27010;&#24565;&#36317;&#31163;&#30001;&#30693;&#35782;&#22270;&#35889;&#23450;&#20041;&#65292;&#30830;&#20445;&#27010;&#24565;&#32534;&#36753;&#30340;&#26368;&#20248;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#36827;&#34892;&#27604;&#36739;&#30740;&#31350;&#25193;&#23637;&#20102;&#20197;&#22270;&#32534;&#36753;&#20026;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#20808;&#21069;&#21162;&#21147;&#65292;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#28085;&#30422;&#20102;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26041;&#27861;&#12290;&#21040;&#27492;&#20026;&#27490;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20197;&#19979;&#37325;&#35201;&#30340;&#30740;&#31350;&#38382;&#39064;&#65306;&#25105;&#20204;&#24212;&#35813;&#23558;&#36755;&#20837;&#25968;&#25454;&#34920;&#31034;&#20026;&#22270;&#24418;&#65292;&#36825;&#26159;&#29983;&#25104;&#40657;&#31665;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#26368;&#23567;&#21644;&#26377;&#24847;&#20041;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#22312;&#24615;&#33021;&#21644;&#26102;&#38388;&#25928;&#29575;&#26041;&#38754;&#26368;&#20339;&#30340;GNN&#26041;&#27861;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.11609v2 Announce Type: replace-cross  Abstract: Counterfactuals have been established as a popular explainability technique which leverages a set of minimal edits to alter the prediction of a classifier. When considering conceptual counterfactuals on images, the edits requested should correspond to salient concepts present in the input data. At the same time, conceptual distances are defined by knowledge graphs, ensuring the optimality of conceptual edits. In this work, we extend previous endeavors on graph edits as counterfactual explanations by conducting a comparative study which encompasses both supervised and unsupervised Graph Neural Network (GNN) approaches. To this end, we pose the following significant research question: should we represent input data as graphs, which is the optimal GNN approach in terms of performance and time efficiency to generate minimal and meaningful counterfactual explanations for black-box image classifiers?
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#32447;&#24191;&#21578;&#31995;&#32479;&#20013;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#28508;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;LLM&#24191;&#21578;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#24182;&#35752;&#35770;&#20102;&#22522;&#20110;LLM&#30340;&#21160;&#24577;&#21019;&#24847;&#20248;&#21270;&#30340;&#21069;&#26223;&#12290;</title><link>https://arxiv.org/abs/2311.07601</link><description>&lt;p&gt;
&#22312;&#32447;&#24191;&#21578;&#19982;LLMs&#65306;&#26426;&#36935;&#19982;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Online Advertisements with LLMs: Opportunities and Challenges
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.07601
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#32447;&#24191;&#21578;&#31995;&#32479;&#20013;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#28508;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;LLM&#24191;&#21578;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#24182;&#35752;&#35770;&#20102;&#22522;&#20110;LLM&#30340;&#21160;&#24577;&#21019;&#24847;&#20248;&#21270;&#30340;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#22312;&#32447;&#24191;&#21578;&#31995;&#32479;&#20013;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;&#36825;&#20010;&#31995;&#32479;&#24517;&#39035;&#28385;&#36275;&#30340;&#38544;&#31169;&#12289;&#24310;&#36831;&#12289;&#21487;&#38752;&#24615;&#20197;&#21450;&#29992;&#25143;&#21644;&#24191;&#21578;&#21830;&#30340;&#28385;&#24847;&#24230;&#31561;&#20851;&#38190;&#35201;&#27714;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20171;&#32461;&#20102;&#19968;&#20010;LLM&#24191;&#21578;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#21253;&#25324;&#20462;&#25913;&#12289;&#31454;&#26631;&#12289;&#39044;&#27979;&#21644;&#25293;&#21334;&#27169;&#22359;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#27599;&#20010;&#27169;&#22359;&#30340;&#19981;&#21516;&#35774;&#35745;&#32771;&#34385;&#65292;&#24182;&#23545;&#20854;&#23454;&#29992;&#24615;&#21644;&#23454;&#26045;&#20013;&#30340;&#25216;&#26415;&#25361;&#25112;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22522;&#20110;LLM&#30340;&#21160;&#24577;&#21019;&#24847;&#20248;&#21270;&#30340;&#21069;&#26223;&#65292;&#20197;&#26174;&#33879;&#25552;&#21319;&#24191;&#21578;&#23545;&#29992;&#25143;&#30340;&#21560;&#24341;&#21147;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#25152;&#38754;&#20020;&#30340;&#39069;&#22806;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.07601v2 Announce Type: replace-cross Abstract: This paper explores the potential for leveraging Large Language Models (LLM) in the realm of online advertising systems. We delve into essential requirements including privacy, latency, reliability as well as the satisfaction of users and advertisers which such a system must fulfill. We further introduce a general framework for LLM advertisement, consisting of modification, bidding, prediction, and auction modules. Different design considerations for each module is presented, with an in-depth examination of their practicality and the technical challenges inherent to their implementation. Finally, we explore the prospect of LLM-based dynamic creative optimization as a means to significantly enhance the appeal of advertisements to users and discuss its additional challenges.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SelectLLM&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;LLMs&#36873;&#25321;&#39640;&#36136;&#37327;&#25351;&#20196;&#12290;&#36890;&#36807;&#25552;&#31034;LLMs&#20272;&#35745;&#27599;&#20010;&#26080;&#26631;&#31614;&#25351;&#20196;&#30340;&#26377;&#29992;&#24615;&#21644;&#24433;&#21709;&#21147;&#65292;&#24182;&#20351;&#29992;&#32858;&#31867;&#31639;&#27861;&#23558;&#25351;&#20196;&#20998;&#20026;&#22810;&#20010;&#32858;&#31867;&#12290;</title><link>http://arxiv.org/abs/2401.16553</link><description>&lt;p&gt;
SelectLLM&#65306;LLMs&#33021;&#21542;&#36873;&#25321;&#37325;&#35201;&#30340;&#25351;&#20196;&#36827;&#34892;&#27880;&#37322;&#65311;
&lt;/p&gt;
&lt;p&gt;
SelectLLM: Can LLMs Select Important Instructions to Annotate?. (arXiv:2401.16553v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16553
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SelectLLM&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;LLMs&#36873;&#25321;&#39640;&#36136;&#37327;&#25351;&#20196;&#12290;&#36890;&#36807;&#25552;&#31034;LLMs&#20272;&#35745;&#27599;&#20010;&#26080;&#26631;&#31614;&#25351;&#20196;&#30340;&#26377;&#29992;&#24615;&#21644;&#24433;&#21709;&#21147;&#65292;&#24182;&#20351;&#29992;&#32858;&#31867;&#31639;&#27861;&#23558;&#25351;&#20196;&#20998;&#20026;&#22810;&#20010;&#32858;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#37327;&#19988;&#22810;&#26679;&#21270;&#30340;&#25351;&#20196;&#25968;&#25454;&#38598;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21487;&#20197;&#20351;&#27169;&#22411;&#29702;&#35299;&#21644;&#36981;&#24490;&#20154;&#31867;&#25351;&#20196;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#19968;&#23567;&#32452;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#21487;&#20197;&#36229;&#36807;&#20351;&#29992;&#22823;&#37327;&#26356;&#22024;&#26434;&#30340;&#25351;&#20196;&#12290;&#30001;&#20110;&#25351;&#20196;&#26159;&#26080;&#26631;&#31614;&#30340;&#65292;&#19988;&#21709;&#24212;&#26159;&#33258;&#28982;&#25991;&#26412;&#65292;&#20256;&#32479;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#26696;&#26080;&#27861;&#30452;&#25509;&#24212;&#29992;&#20110;&#36873;&#25321;&#26080;&#26631;&#31614;&#25351;&#20196;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25351;&#20196;&#36873;&#25321;&#26041;&#27861;&#65292;&#31216;&#20026;SelectLLM&#65292;&#23427;&#21033;&#29992;LLMs&#36873;&#25321;&#39640;&#36136;&#37327;&#25351;&#20196;&#12290;&#25105;&#20204;&#30340;&#39640;&#32423;&#24605;&#24819;&#26159;&#21033;&#29992;LLMs&#36890;&#36807;&#25552;&#31034;&#26469;&#20272;&#35745;&#27599;&#20010;&#25351;&#20196;&#22312;&#27809;&#26377;&#30456;&#24212;&#26631;&#31614;&#65288;&#21363;&#21709;&#24212;&#65289;&#30340;&#24773;&#20917;&#19979;&#30340;&#26377;&#29992;&#24615;&#21644;&#24433;&#21709;&#21147;&#12290;SelectLLM&#21253;&#25324;&#20004;&#20010;&#27493;&#39588;&#65306;&#20351;&#29992;&#32858;&#31867;&#31639;&#27861;&#65288;&#20363;&#22914;CoreSet&#65289;&#23558;&#26080;&#26631;&#31614;&#25351;&#20196;&#21010;&#20998;&#20026;&#22810;&#20010;&#32858;&#31867;&#65292;&#28982;&#21518;&#25552;&#31034;LLMs&#22312;&#20854;&#20013;&#36873;&#25321;&#39640;&#36136;&#37327;&#25351;&#20196;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training large language models (LLMs) with a large and diverse instruction dataset aligns the models to comprehend and follow human instructions. Recent works have shown that using a small set of high-quality instructions can outperform using large yet more noisy ones. Because instructions are unlabeled and their responses are natural text, traditional active learning schemes with the model's confidence cannot be directly applied to the selection of unlabeled instructions. In this work, we propose a novel method for instruction selection, called SelectLLM, that leverages LLMs for the selection of high-quality instructions. Our high-level idea is to use LLMs to estimate the usefulness and impactfulness of each instruction without the corresponding labels (i.e., responses), via prompting. SelectLLM involves two steps: dividing the unlabelled instructions using a clustering algorithm (e.g., CoreSet) to multiple clusters, and then prompting LLMs to choose high-quality instructions within e
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24635;&#32467;&#20102;&#31070;&#32463;&#36752;&#23556;&#22330;&#25216;&#26415;&#22312;&#25552;&#39640;&#22270;&#20687;&#28210;&#26579;&#36136;&#37327;&#26041;&#38754;&#30340;&#21508;&#31181;&#26041;&#27861;&#21644;&#31574;&#30053;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#24403;&#21069;&#29366;&#24577;&#21644;&#26410;&#26469;&#28436;&#21270;&#26041;&#21521;&#30340;&#29702;&#35299;&#65292;&#20174;&#32780;&#25512;&#21160;NeRF&#25216;&#26415;&#22312;&#30456;&#20851;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.12451</link><description>&lt;p&gt;
&#25552;&#39640;&#31070;&#32463;&#36752;&#23556;&#22330;&#26032;&#35270;&#22270;&#21512;&#25104;&#36136;&#37327;&#30340;&#26041;&#27861;&#21644;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Methods and strategies for improving the novel view synthesis quality of neural radiation field. (arXiv:2401.12451v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12451
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24635;&#32467;&#20102;&#31070;&#32463;&#36752;&#23556;&#22330;&#25216;&#26415;&#22312;&#25552;&#39640;&#22270;&#20687;&#28210;&#26579;&#36136;&#37327;&#26041;&#38754;&#30340;&#21508;&#31181;&#26041;&#27861;&#21644;&#31574;&#30053;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#24403;&#21069;&#29366;&#24577;&#21644;&#26410;&#26469;&#28436;&#21270;&#26041;&#21521;&#30340;&#29702;&#35299;&#65292;&#20174;&#32780;&#25512;&#21160;NeRF&#25216;&#26415;&#22312;&#30456;&#20851;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRF&#65289;&#25216;&#26415;&#21487;&#20197;&#20174;2D&#22270;&#20687;&#20013;&#23398;&#20064;&#22330;&#26223;&#30340;3D&#38544;&#24335;&#27169;&#22411;&#65292;&#24182;&#21512;&#25104;&#36924;&#30495;&#30340;&#26032;&#35270;&#22270;&#22270;&#20687;&#12290;&#35813;&#25216;&#26415;&#21463;&#21040;&#24037;&#19994;&#30028;&#30340;&#24191;&#27867;&#20851;&#27880;&#65292;&#24182;&#20855;&#26377;&#33391;&#22909;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;&#20026;&#20102;&#35299;&#20915;NeRF&#22270;&#20687;&#28210;&#26579;&#36136;&#37327;&#38656;&#35201;&#25552;&#39640;&#30340;&#38382;&#39064;&#65292;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#22312;&#36807;&#21435;&#19977;&#24180;&#20013;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#26469;&#25913;&#21892;&#28210;&#26579;&#36136;&#37327;&#12290;&#26412;&#30740;&#31350;&#23545;&#26368;&#26032;&#30340;&#30456;&#20851;&#35770;&#25991;&#36827;&#34892;&#20102;&#20998;&#31867;&#21644;&#23457;&#26597;&#65292;&#20998;&#26512;&#20102;&#36136;&#37327;&#25913;&#36827;&#32972;&#21518;&#30340;&#25216;&#26415;&#21407;&#29702;&#65292;&#24182;&#35752;&#35770;&#20102;&#36136;&#37327;&#25913;&#36827;&#26041;&#27861;&#30340;&#26410;&#26469;&#28436;&#21270;&#26041;&#21521;&#12290;&#36825;&#39033;&#30740;&#31350;&#21487;&#20197;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#24555;&#36895;&#20102;&#35299;&#35813;&#39046;&#22495;&#25216;&#26415;&#30340;&#24403;&#21069;&#29366;&#24577;&#21644;&#28436;&#21270;&#32972;&#26223;&#65292;&#26377;&#21161;&#20110;&#28608;&#21457;&#24320;&#21457;&#26356;&#39640;&#25928;&#31639;&#27861;&#30340;&#21457;&#23637;&#65292;&#24182;&#25512;&#21160;NeRF&#25216;&#26415;&#22312;&#30456;&#20851;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Radiation Field (NeRF) technology can learn a 3D implicit model of a scene from 2D images and synthesize realistic novel view images. This technology has received widespread attention from the industry and has good application prospects. In response to the problem that the rendering quality of NeRF images needs to be improved, many researchers have proposed various methods to improve the rendering quality in the past three years. The latest relevant papers are classified and reviewed, the technical principles behind quality improvement are analyzed, and the future evolution direction of quality improvement methods is discussed. This study can help researchers quickly understand the current state and evolutionary context of technology in this field, which is helpful in inspiring the development of more efficient algorithms and promoting the application of NeRF technology in related fields.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#34917;&#19969;&#35299;&#30721;&#22120;&#24335;&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22522;&#30784;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#22312;&#21508;&#31181;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#25509;&#36817;&#26368;&#20808;&#36827;&#30340;&#30417;&#30563;&#39044;&#27979;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.10688</link><description>&lt;p&gt;
&#19968;&#31181;&#20165;&#35299;&#30721;&#22120;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A decoder-only foundation model for time-series forecasting. (arXiv:2310.10688v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10688
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#34917;&#19969;&#35299;&#30721;&#22120;&#24335;&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22522;&#30784;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#22312;&#21508;&#31181;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#25509;&#36817;&#26368;&#20808;&#36827;&#30340;&#30417;&#30563;&#39044;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#29992;&#20110;&#39044;&#27979;&#30340;&#26102;&#38388;&#24207;&#21015;&#22522;&#30784;&#27169;&#22411;&#65292;&#20854;&#22312;&#21508;&#31181;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#24320;&#31665;&#21363;&#29992;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#25509;&#36817;&#27599;&#20010;&#20010;&#21035;&#25968;&#25454;&#38598;&#19978;&#26368;&#20808;&#36827;&#30340;&#30417;&#30563;&#39044;&#27979;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22522;&#20110;&#22312;&#22823;&#22411;&#26102;&#38388;&#24207;&#21015;&#35821;&#26009;&#24211;&#19978;&#39044;&#35757;&#32451;&#30340;&#34917;&#19969;&#35299;&#30721;&#22120;&#24335;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#24182;&#21487;&#20197;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#39044;&#27979;&#21382;&#21490;&#38271;&#24230;&#12289;&#39044;&#27979;&#38271;&#24230;&#21644;&#26102;&#38388;&#31890;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by recent advances in large language models for Natural Language Processing (NLP), we design a time-series foundation model for forecasting whose out-of-the-box zero-shot performance on a variety of public datasets comes close to the accuracy of state-of-the-art supervised forecasting models for each individual dataset. Our model is based on pretraining a patched-decoder style attention model on a large time-series corpus, and can work well across different forecasting history lengths, prediction lengths and temporal granularities.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#32534;&#36753;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#25361;&#25112;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;&#32534;&#36753;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#32534;&#36753;&#22810;&#27169;&#24335;LLMs&#20173;&#28982;&#23384;&#22312;&#22256;&#38590;&#65292;&#20294;&#36825;&#39033;&#24037;&#20316;&#20026;NLP&#31038;&#21306;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2310.08475</link><description>&lt;p&gt;
&#25105;&#20204;&#33021;&#32534;&#36753;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can We Edit Multimodal Large Language Models?. (arXiv:2310.08475v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08475
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#32534;&#36753;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#25361;&#25112;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;&#32534;&#36753;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#32534;&#36753;&#22810;&#27169;&#24335;LLMs&#20173;&#28982;&#23384;&#22312;&#22256;&#38590;&#65292;&#20294;&#36825;&#39033;&#24037;&#20316;&#20026;NLP&#31038;&#21306;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#32534;&#36753;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#12290;&#19982;&#32534;&#36753;&#21333;&#27169;&#24335;LLMs&#30456;&#27604;&#65292;&#22810;&#27169;&#24335;&#27169;&#22411;&#30340;&#32534;&#36753;&#26356;&#20855;&#25361;&#25112;&#24615;&#65292;&#38656;&#35201;&#26356;&#39640;&#32423;&#21035;&#30340;&#23457;&#26597;&#21644;&#24910;&#37325;&#32771;&#34385;&#12290;&#20026;&#20102;&#20419;&#36827;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#31216;&#20026;MMEdit&#65292;&#29992;&#20110;&#32534;&#36753;&#22810;&#27169;&#24335;LLMs&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#22871;&#21019;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#21253;&#25324;&#21508;&#31181;&#27169;&#22411;&#32534;&#36753;&#22522;&#32447;&#30340;&#32508;&#21512;&#23454;&#39564;&#65292;&#24182;&#20998;&#26512;&#20102;&#32534;&#36753;&#22810;&#27169;&#24335;LLMs&#30340;&#19981;&#21516;&#32452;&#20214;&#30340;&#24433;&#21709;&#12290;&#26681;&#25454;&#32463;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#20043;&#21069;&#30340;&#22522;&#32447;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#21487;&#20197;&#23454;&#29616;&#32534;&#36753;&#22810;&#27169;&#24335;LLMs&#65292;&#20294;&#25928;&#26524;&#20173;&#28982;&#19981;&#29702;&#24819;&#65292;&#34920;&#26126;&#36825;&#20010;&#20219;&#21153;&#21487;&#33021;&#23384;&#22312;&#30340;&#22256;&#38590;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#24037;&#20316;&#33021;&#20026;NLP&#31038;&#21306;&#25552;&#20379;&#35265;&#35299;&#12290;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#21487;&#22312;https://github.com/zjunlp/EasyEdit&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we focus on editing Multimodal Large Language Models (MLLMs). Compared to editing single-modal LLMs, multimodal model editing is more challenging, which demands a higher level of scrutiny and careful consideration in the editing process. To facilitate research in this area, we construct a new benchmark, dubbed MMEdit, for editing multimodal LLMs and establishing a suite of innovative metrics for evaluation. We conduct comprehensive experiments involving various model editing baselines and analyze the impact of editing different components for multimodal LLMs. Empirically, we notice that previous baselines can implement editing multimodal LLMs to some extent, but the effect is still barely satisfactory, indicating the potential difficulty of this task. We hope that our work can provide the NLP community with insights. Code and dataset are available in https://github.com/zjunlp/EasyEdit.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#28151;&#21512;&#27169;&#22411;&#65292;&#36890;&#36807;&#25913;&#36827;&#30340;VMD&#12289;&#29305;&#24449;&#24037;&#31243;&#21644;&#22534;&#21472;Informer&#65292;&#32467;&#21512;&#33258;&#36866;&#24212;&#25439;&#22833;&#20989;&#25968;&#65292;&#25104;&#21151;&#24212;&#29992;&#20110;&#32929;&#24066;&#39044;&#27979;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#27169;&#22411;&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#12289;&#21709;&#24212;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#21644;&#20854;&#20182;&#28151;&#21512;&#27169;&#22411;&#65292;&#23545;&#20110;&#23567;&#20225;&#19994;&#21644;&#29305;&#24449;&#24037;&#31243;&#30340;&#39044;&#27979;&#24314;&#27169;&#26377;&#28508;&#22312;&#30340;&#20248;&#21270;&#26041;&#21521;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2310.01884</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#28151;&#21512;&#27169;&#22411;&#30340;&#25913;&#36827;VMD&#21644;&#22534;&#21472;Informer&#22312;&#22686;&#24378;&#32929;&#24066;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Adaptive Hybrid Model for Enhanced Stock Market Predictions Using Improved VMD and Stacked Informer. (arXiv:2310.01884v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01884
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#28151;&#21512;&#27169;&#22411;&#65292;&#36890;&#36807;&#25913;&#36827;&#30340;VMD&#12289;&#29305;&#24449;&#24037;&#31243;&#21644;&#22534;&#21472;Informer&#65292;&#32467;&#21512;&#33258;&#36866;&#24212;&#25439;&#22833;&#20989;&#25968;&#65292;&#25104;&#21151;&#24212;&#29992;&#20110;&#32929;&#24066;&#39044;&#27979;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#27169;&#22411;&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#12289;&#21709;&#24212;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#21644;&#20854;&#20182;&#28151;&#21512;&#27169;&#22411;&#65292;&#23545;&#20110;&#23567;&#20225;&#19994;&#21644;&#29305;&#24449;&#24037;&#31243;&#30340;&#39044;&#27979;&#24314;&#27169;&#26377;&#28508;&#22312;&#30340;&#20248;&#21270;&#26041;&#21521;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#33258;&#36866;&#24212;&#28151;&#21512;&#27169;&#22411;&#65292;&#21033;&#29992;&#25913;&#36827;&#30340;&#21464;&#20998;&#27169;&#24577;&#20998;&#35299;&#65288;VMD&#65289;&#12289;&#29305;&#24449;&#24037;&#31243;&#65288;FE&#65289;&#21644;&#22534;&#21472;Informer&#65292;&#24182;&#32467;&#21512;&#33258;&#36866;&#24212;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#32929;&#24066;&#39044;&#27979;&#12290;&#36890;&#36807;&#20005;&#26684;&#30340;&#23454;&#39564;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#65292;&#21629;&#21517;&#20026;VMGCformer&#65288;Adam+GC+enhanced informer&#65289;&#65292;&#22312;&#22788;&#29702;&#32929;&#24066;&#25968;&#25454;&#30340;&#22797;&#26434;&#21160;&#24577;&#21644;&#27874;&#21160;&#24615;&#26041;&#38754;&#23637;&#31034;&#20986;&#26174;&#33879;&#30340;&#29087;&#32451;&#24230;&#12290;&#22522;&#20110;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#24471;&#20986;&#30340;&#23454;&#39564;&#32467;&#26524;&#31361;&#26174;&#20986;&#35813;&#27169;&#22411;&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#12289;&#21709;&#24212;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#30456;&#23545;&#20256;&#32479;&#21644;&#20854;&#20182;&#28151;&#21512;&#27169;&#22411;&#30340;&#20248;&#21183;&#12290;&#26412;&#30740;&#31350;&#36827;&#19968;&#27493;&#24378;&#35843;&#20102;&#20248;&#21270;&#30340;&#28508;&#22312;&#36884;&#24452;&#65292;&#24182;&#20171;&#32461;&#20102;&#36827;&#19968;&#27493;&#22686;&#24378;&#39044;&#27979;&#24314;&#27169;&#30340;&#26410;&#26469;&#26041;&#21521;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#23567;&#20225;&#19994;&#21644;&#29305;&#24449;&#24037;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces an innovative adaptive hybrid model for stock market predictions, leveraging the capabilities of an enhanced Variational Mode Decomposition (VMD), Feature Engineering (FE), and stacked Informer integrated with an adaptive loss function. Through rigorous experimentation, the proposed model, termed Adam+GC+enhanced informer (We name it VMGCformer), demonstrates significant proficiency in addressing the intricate dynamics and volatile nature of stock market data. Experimental results, derived from multiple benchmark datasets, underscore the model's superiority in terms of prediction accuracy, responsiveness, and generalization capabilities over traditional and other hybrid models. The research further highlights potential avenues for optimization and introduces future directions to enhance predictive modeling, especially for small enterprises and feature engineering.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#26174;&#33879;&#36890;&#36947;&#24494;&#35843;&#8221;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#29305;&#24449;&#22270;&#20013;&#30340;&#37096;&#20998;&#36890;&#36947;&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#29616;&#22312;&#20302;&#25968;&#25454;&#36164;&#28304;&#22330;&#26223;&#19979;&#20302;&#21442;&#25968;&#25104;&#26412;&#30340;&#39640;&#25928;&#24494;&#35843;&#65292;&#24182;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#20248;&#20110;&#20840;&#38754;&#24494;&#35843;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.08513</link><description>&lt;p&gt;
&#36890;&#36807;&#26174;&#33879;&#36890;&#36947;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#30340;&#31616;&#21333;&#22522;&#20934;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SCT: A Simple Baseline for Parameter-Efficient Fine-Tuning via Salient Channels. (arXiv:2309.08513v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08513
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#26174;&#33879;&#36890;&#36947;&#24494;&#35843;&#8221;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#29305;&#24449;&#22270;&#20013;&#30340;&#37096;&#20998;&#36890;&#36947;&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#29616;&#22312;&#20302;&#25968;&#25454;&#36164;&#28304;&#22330;&#26223;&#19979;&#20302;&#21442;&#25968;&#25104;&#26412;&#30340;&#39640;&#25928;&#24494;&#35843;&#65292;&#24182;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#20248;&#20110;&#20840;&#38754;&#24494;&#35843;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;Transformer&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#20855;&#26377;&#24378;&#22823;&#30340;&#34920;&#31034;&#20248;&#21183;&#12290;&#26368;&#36817;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#24182;&#19988;&#23454;&#39564;&#35777;&#26126;&#65292;&#20165;&#24494;&#35843;&#39069;&#22806;&#30340;1%&#21442;&#25968;&#23601;&#33021;&#22312;&#20302;&#25968;&#25454;&#36164;&#28304;&#22330;&#26223;&#19979;&#36229;&#36234;&#20840;&#38754;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#24494;&#35843;&#22810;&#26679;&#30340;&#19979;&#28216;&#20219;&#21153;&#26102;&#24573;&#35270;&#20102;&#20219;&#21153;&#29305;&#23450;&#30340;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#31216;&#20026;&#8220;&#26174;&#33879;&#36890;&#36947;&#24494;&#35843;&#8221;&#65288;SCT&#65289;&#65292;&#36890;&#36807;&#23558;&#27169;&#22411;&#19982;&#20219;&#21153;&#22270;&#20687;&#36827;&#34892;&#21069;&#21521;&#20256;&#25773;&#65292;&#36873;&#25321;&#29305;&#24449;&#22270;&#20013;&#30340;&#37096;&#20998;&#36890;&#36947;&#65292;&#20351;&#24471;&#25105;&#20204;&#21482;&#38656;&#35201;&#24494;&#35843;&#20854;&#20013;&#30340;1/8&#36890;&#36947;&#65292;&#20174;&#32780;&#26174;&#33879;&#38477;&#20302;&#21442;&#25968;&#25104;&#26412;&#24182;&#22312;VTAB-1K&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;18&#20010;&#20219;&#21153;&#20013;&#20248;&#20110;&#20840;&#38754;&#24494;&#35843;&#12290;&#36825;&#20165;&#22686;&#21152;&#20102;0.11M ViT-B&#21442;&#25968;&#65292;&#30456;&#27604;&#20840;&#38754;&#24494;&#35843;&#65292;&#20943;&#23569;&#20102;780&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained vision transformers have strong representation benefits to various downstream tasks. Recently, many parameter-efficient fine-tuning (PEFT) methods have been proposed, and their experiments demonstrate that tuning only 1% of extra parameters could surpass full fine-tuning in low-data resource scenarios. However, these methods overlook the task-specific information when fine-tuning diverse downstream tasks. In this paper, we propose a simple yet effective method called "Salient Channel Tuning" (SCT) to leverage the task-specific information by forwarding the model with the task images to select partial channels in a feature map that enables us to tune only 1/8 channels leading to significantly lower parameter costs. Experiments outperform full fine-tuning on 18 out of 19 tasks in the VTAB-1K benchmark by adding only 0.11M parameters of the ViT-B, which is 780$\times$ fewer than its full fine-tuning counterpart. Furthermore, experiments on domain generalization and few-shot le
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#30830;&#23450;&#24615;&#29305;&#24449;&#25490;&#24207;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#29305;&#24449;&#37325;&#35201;&#24615;&#20540;&#30340;&#20004;&#20004;&#27604;&#36739;&#65292;&#21487;&#20197;&#20135;&#29983;&#25490;&#24207;&#21644;&#21516;&#26102;&#30340;&#32622;&#20449;&#21306;&#38388;&#65292;&#24182;&#19988;&#21487;&#20197;&#36873;&#25321;&#21069;k&#20010;&#38598;&#21512;&#12290;</title><link>http://arxiv.org/abs/2307.15361</link><description>&lt;p&gt;
&#30830;&#23450;&#24615;&#29305;&#24449;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
Confident Feature Ranking. (arXiv:2307.15361v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15361
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#30830;&#23450;&#24615;&#29305;&#24449;&#25490;&#24207;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#29305;&#24449;&#37325;&#35201;&#24615;&#20540;&#30340;&#20004;&#20004;&#27604;&#36739;&#65292;&#21487;&#20197;&#20135;&#29983;&#25490;&#24207;&#21644;&#21516;&#26102;&#30340;&#32622;&#20449;&#21306;&#38388;&#65292;&#24182;&#19988;&#21487;&#20197;&#36873;&#25321;&#21069;k&#20010;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#37325;&#35201;&#24615;&#30340;&#35299;&#37322;&#36890;&#24120;&#20381;&#36182;&#20110;&#29305;&#24449;&#30340;&#30456;&#23545;&#39034;&#24207;&#32780;&#19981;&#26159;&#25968;&#20540;&#26412;&#36523;&#65292;&#20063;&#23601;&#26159;&#25490;&#24207;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35745;&#31639;&#37325;&#35201;&#24615;&#20540;&#26102;&#20351;&#29992;&#30340;&#26679;&#26412;&#37327;&#36739;&#23567;&#65292;&#25490;&#24207;&#21487;&#33021;&#19981;&#31283;&#23450;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20107;&#21518;&#37325;&#35201;&#24615;&#26041;&#27861;&#65292;&#21487;&#20197;&#20135;&#29983;&#19968;&#31181;&#25490;&#24207;&#21644;&#21516;&#26102;&#30340;&#32622;&#20449;&#21306;&#38388;&#12290;&#22522;&#20110;&#29305;&#24449;&#37325;&#35201;&#24615;&#20540;&#30340;&#20004;&#20004;&#27604;&#36739;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20445;&#35777;&#39640;&#27010;&#29575;&#21253;&#21547;&#8220;&#30495;&#23454;&#8221;&#65288;&#26080;&#38480;&#26679;&#26412;&#65289;&#25490;&#24207;&#65292;&#24182;&#20801;&#35768;&#36873;&#25321;&#21069;k&#20010;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpretation of feature importance values often relies on the relative order of the features rather than on the value itself, referred to as ranking. However, the order may be unstable due to the small sample sizes used in calculating the importance values. We propose that post-hoc importance methods produce a ranking and simultaneous confident intervals for the rankings. Based on pairwise comparisons of the feature importance values, our method is guaranteed to include the ``true'' (infinite sample) ranking with high probability and allows for selecting top-k sets.
&lt;/p&gt;</description></item><item><title>&#38754;&#21521;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#30340;&#38598;&#25104;&#24863;&#30693;-&#36890;&#20449;-&#35745;&#31639;&#25216;&#26415;&#23545;&#20110;&#25552;&#39640;&#36164;&#28304;&#21033;&#29992;&#29575;&#20197;&#21450;&#23454;&#29616;&#36793;&#32536;&#23398;&#20064;&#21644;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#25512;&#29702;&#20219;&#21153;&#30340;&#23450;&#21046;&#30446;&#26631;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2306.01162</link><description>&lt;p&gt;
&#38754;&#21521;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#30340;&#38598;&#25104;&#24863;&#30693;-&#36890;&#20449;-&#35745;&#31639; &#65288;Integrated Sensing-Communication-Computation&#65289; &#65288;arXiv&#65306;2306.01162v1 [cs.IT]&#65289;
&lt;/p&gt;
&lt;p&gt;
Integrated Sensing-Communication-Computation for Edge Artificial Intelligence. (arXiv:2306.01162v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01162
&lt;/p&gt;
&lt;p&gt;
&#38754;&#21521;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#30340;&#38598;&#25104;&#24863;&#30693;-&#36890;&#20449;-&#35745;&#31639;&#25216;&#26415;&#23545;&#20110;&#25552;&#39640;&#36164;&#28304;&#21033;&#29992;&#29575;&#20197;&#21450;&#23454;&#29616;&#36793;&#32536;&#23398;&#20064;&#21644;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#25512;&#29702;&#20219;&#21153;&#30340;&#23450;&#21046;&#30446;&#26631;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#26159;&#23454;&#29616;&#19975;&#29289;&#26234;&#33021;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#29992;&#20110;&#25968;&#23383;&#23402;&#29983;&#12289;&#20840;&#24687;&#25237;&#24433;&#12289;&#35821;&#20041;&#36890;&#20449;&#21644;&#33258;&#21160;&#39550;&#39542;&#31561;&#39640;&#32423;&#25216;&#26415;&#12290;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#36793;&#32536;&#23398;&#20064;&#21644;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#25512;&#29702;&#65292;&#21462;&#20915;&#20110;&#19977;&#20010;&#39640;&#24230;&#32806;&#21512;&#30340;&#36807;&#31243;&#30340;&#36136;&#37327;&#65292;&#21363;&#25968;&#25454;&#33719;&#21462;&#30340;&#24863;&#30693;&#12289;&#20449;&#24687;&#25552;&#21462;&#30340;&#35745;&#31639;&#21644;&#20449;&#24687;&#20256;&#36755;&#30340;&#36890;&#20449;&#12290;&#28982;&#32780;&#65292;&#36825;&#19977;&#20010;&#27169;&#22359;&#38656;&#35201;&#20026;&#22686;&#24378;&#33258;&#24049;&#30340;&#26381;&#21153;&#36136;&#37327;&#32780;&#31454;&#20105;&#32593;&#32476;&#36164;&#28304;&#12290;&#20026;&#27492;&#65292;&#38598;&#25104;&#24863;&#30693;-&#36890;&#20449;-&#35745;&#31639;&#65288;ISCC&#65289;&#23545;&#20110;&#25552;&#39640;&#36164;&#28304;&#21033;&#29992;&#29575;&#20197;&#21450;&#23454;&#29616;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#20219;&#21153;&#30340;&#23450;&#21046;&#30446;&#26631;&#33267;&#20851;&#37325;&#35201;&#12290;&#36890;&#36807;&#30740;&#31350;&#19977;&#20010;&#27169;&#22359;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#21508;&#31181; ISCC &#26041;&#26696;&#65292;&#36866;&#29992;&#20110;&#32852;&#37030;&#36793;&#32536;&#23398;&#20064;&#20219;&#21153;&#21644;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#25512;&#29702;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Edge artificial intelligence (AI) has been a promising solution towards 6G to empower a series of advanced techniques such as digital twin, holographic projection, semantic communications, and auto-driving, for achieving intelligence of everything. The performance of edge AI tasks, including edge learning and edge AI inference, depends on the quality of three highly coupled processes, i.e., sensing for data acquisition, computation for information extraction, and communication for information transmission. However, these three modules need to compete for network resources for enhancing their own quality-of-services. To this end, integrated sensing-communication-computation (ISCC) is of paramount significance for improving resource utilization as well as achieving the customized goals of edge AI tasks. By investigating the interplay among the three modules, this article presents various kinds of ISCC schemes for federated edge learning tasks and edge AI inference tasks in both applicati
&lt;/p&gt;</description></item><item><title>Self-Polish&#26159;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20419;&#20351;&#27169;&#22411;&#36880;&#27493;&#23436;&#21892;&#32473;&#23450;&#30340;&#38382;&#39064;&#20197;&#20351;&#20854;&#26356;&#26131;&#29702;&#35299;&#21644;&#21487;&#35299;&#20915;&#65292;&#20197;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.14497</link><description>&lt;p&gt;
&#33258;&#25105;&#31934;&#30952;&#65306;&#36890;&#36807;&#38382;&#39064;&#31934;&#21270;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Self-Polish: Enhance Reasoning in Large Language Models via Problem Refinement. (arXiv:2305.14497v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14497
&lt;/p&gt;
&lt;p&gt;
Self-Polish&#26159;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20419;&#20351;&#27169;&#22411;&#36880;&#27493;&#23436;&#21892;&#32473;&#23450;&#30340;&#38382;&#39064;&#20197;&#20351;&#20854;&#26356;&#26131;&#29702;&#35299;&#21644;&#21487;&#35299;&#20915;&#65292;&#20197;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992; Chain-of-Thought&#65288;CoT&#65289;&#31561;&#25552;&#31034;&#26041;&#27861;&#21487;&#20197;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#30740;&#31350;&#20154;&#21592;&#24191;&#27867;&#25506;&#32034;&#20102;&#21512;&#29702;&#21270;&#21644;&#31572;&#26696;&#29983;&#25104;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#24573;&#30053;&#20102;&#20302;&#36136;&#37327;&#25512;&#29702;&#38382;&#39064;&#21487;&#33021;&#20250;&#26174;&#30528;&#24433;&#21709;&#25512;&#29702;&#24615;&#33021;&#30340;&#28508;&#22312;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Self-Polish&#65288;SP&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#20419;&#20351;&#27169;&#22411;&#36880;&#27493;&#23436;&#21892;&#32473;&#23450;&#30340;&#38382;&#39064;&#20197;&#20351;&#20854;&#26356;&#26131;&#29702;&#35299;&#21644;&#21487;&#35299;&#20915;&#65292;&#20197;&#20419;&#36827;&#27169;&#22411;&#30340;&#38382;&#39064;&#35299;&#20915;&#36807;&#31243;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#26041;&#27861;&#25945;&#25480;&#27169;&#22411;&#28040;&#38500;&#26080;&#20851;&#20449;&#24687;&#65292;&#37325;&#26032;&#25490;&#21015;&#36923;&#36753;&#32467;&#26500;&#65292;&#24182;&#23558;&#23616;&#37096;&#26465;&#20214;&#24182;&#34892;&#32452;&#32455;&#25104;&#26032;&#30340;&#26465;&#20214;&#12290; SP&#19982;&#25152;&#26377;&#20854;&#20182;&#25552;&#31034;&#26041;&#27861;&#27491;&#20132;&#65292;&#26041;&#20415;&#23558;&#20854;&#19982;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#38598;&#25104;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#20116;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#23454;&#39564;&#20197;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompting methods such as Chain-of-Thought (CoT) have shed new light on enhancing the reasoning capabilities of large language models, and researchers have extensively explored the generation process of rationales and answers. However, they have overlooked the potential challenges posed by the poor quality of reasoning problems, which may influence the reasoning performance significantly. In this work, we propose Self-Polish (SP), a novel method that facilitates the model's problem-solving process by prompting them to progressively refine the given problems to be more comprehensible and solvable. Specifically, the method teaches models to eliminate irrelevant information, rearrange the logic structure and organize local conditions into new ones parallelly. SP is orthogonal to all other prompting methods, making it convenient to integrate with state-of-the-art techniques for further improvement. We conduct thorough experiments on five benchmarks to illustrate the effectiveness of the pr
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20221;&#20013;&#25991;&#30340;&#22522;&#20110;&#25351;&#20196;&#30340;&#20449;&#24687;&#25552;&#21462;&#25968;&#25454;&#38598;InstructIE&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;270,000&#20010;&#24369;&#30417;&#30563;&#30340;&#25968;&#25454;&#21644;1,000&#20010;&#39640;&#36136;&#37327;&#27880;&#37322;&#23454;&#20363;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#24403;&#21069;&#30340;&#27169;&#22411;&#34920;&#29616;&#26377;&#24453;&#25913;&#36827;&#65292;&#35813;&#20219;&#21153;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.11527</link><description>&lt;p&gt;
InstructIE: &#19968;&#20221;&#22522;&#20110;&#25351;&#20196;&#30340;&#20013;&#25991;&#20449;&#24687;&#25552;&#21462;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
InstructIE: A Chinese Instruction-based Information Extraction Dataset. (arXiv:2305.11527v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11527
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20221;&#20013;&#25991;&#30340;&#22522;&#20110;&#25351;&#20196;&#30340;&#20449;&#24687;&#25552;&#21462;&#25968;&#25454;&#38598;InstructIE&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;270,000&#20010;&#24369;&#30417;&#30563;&#30340;&#25968;&#25454;&#21644;1,000&#20010;&#39640;&#36136;&#37327;&#27880;&#37322;&#23454;&#20363;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#24403;&#21069;&#30340;&#27169;&#22411;&#34920;&#29616;&#26377;&#24453;&#25913;&#36827;&#65292;&#35813;&#20219;&#21153;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#39033;&#26032;&#30340;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#65292;&#31216;&#20026;&#22522;&#20110;&#25351;&#20196;&#30340;&#20449;&#24687;&#25552;&#21462; (Instruction-based IE)&#65292;&#23427;&#26088;&#22312;&#35201;&#27714;&#31995;&#32479;&#36981;&#24490;&#29305;&#23450;&#30340;&#25351;&#20196;&#25110;&#25351;&#21335;&#26469;&#25552;&#21462;&#20449;&#24687;&#12290;&#20026;&#20102;&#20419;&#36827;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#31216;&#20026;InstructIE&#65292;&#20854;&#20013;&#21253;&#25324;&#26469;&#33258;&#20013;&#25991;&#32500;&#22522;&#30334;&#31185;&#30340; 270,000 &#20010;&#24369;&#30417;&#30563;&#25968;&#25454;&#21644; 1,000 &#20010;&#39640;&#36136;&#37327;&#20247;&#21253;&#27880;&#37322;&#23454;&#20363;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35780;&#20272;&#20102;&#21508;&#31181;&#22522;&#32447;&#27169;&#22411;&#22312;InstructIE&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#24403;&#21069;&#30340;&#27169;&#22411;&#34920;&#29616;&#24456;&#26377;&#24076;&#26395;&#65292;&#20294;&#20173;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#26696;&#20363;&#30740;&#31350;&#20998;&#26512;&#65292;&#24378;&#35843;&#20102;&#22522;&#20110;&#25351;&#20196;&#30340;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#20013;&#22266;&#26377;&#30340;&#25361;&#25112;&#12290;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#21487;&#22312; https://github.com/zjunlp/DeepKE/tree/main/example/llm &#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a new Information Extraction (IE) task dubbed Instruction-based IE, which aims to ask the system to follow specific instructions or guidelines to extract information. To facilitate research in this area, we construct a dataset called InstructIE, consisting of 270,000 weakly supervised data from Chinese Wikipedia and 1,000 high-quality crowdsourced annotated instances. We further evaluate the performance of various baseline models on the InstructIE dataset. The results reveal that although current models exhibit promising performance, there is still room for improvement. Furthermore, we conduct a comprehensive case study analysis, underlining the challenges inherent in the Instruction-based IE task. Code and dataset are available at https://github.com/zjunlp/DeepKE/tree/main/example/llm.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36731;&#37327;&#32423;&#22270;&#20687;&#24674;&#22797;&#30340;&#20114;&#24800;&#24335;&#27880;&#24847;&#21147;&#28151;&#21512;Transformer&#65288;RAMiT&#65289;&#12290;&#36890;&#36807;&#20351;&#29992;&#21452;&#21521;&#27880;&#24847;&#21147;&#20197;&#21450;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#22686;&#24378;&#31867;&#22411;&#8212;&#8212;&#24378;&#24230;&#25513;&#30721;&#65292;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#24674;&#22797;&#25928;&#26524;&#65292;&#21516;&#26102;&#22823;&#22823;&#20943;&#23569;&#20102;&#21442;&#25968;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.11474</link><description>&lt;p&gt;
RAMiT&#65306;&#36731;&#37327;&#32423;&#22270;&#20687;&#24674;&#22797;&#30340;&#20114;&#24800;&#24335;&#27880;&#24847;&#21147;&#28151;&#21512;Transformer
&lt;/p&gt;
&lt;p&gt;
RAMiT: Reciprocal Attention Mixing Transformer for Lightweight Image Restoration. (arXiv:2305.11474v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11474
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36731;&#37327;&#32423;&#22270;&#20687;&#24674;&#22797;&#30340;&#20114;&#24800;&#24335;&#27880;&#24847;&#21147;&#28151;&#21512;Transformer&#65288;RAMiT&#65289;&#12290;&#36890;&#36807;&#20351;&#29992;&#21452;&#21521;&#27880;&#24847;&#21147;&#20197;&#21450;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#22686;&#24378;&#31867;&#22411;&#8212;&#8212;&#24378;&#24230;&#25513;&#30721;&#65292;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#24674;&#22797;&#25928;&#26524;&#65292;&#21516;&#26102;&#22823;&#22823;&#20943;&#23569;&#20102;&#21442;&#25968;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#36817;&#24180;&#26469;&#35768;&#22810;&#24037;&#20316;&#22312;&#22270;&#20687;&#24674;&#22797;&#65288;IR&#65289;&#39046;&#22495;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#38754;&#20020;&#21442;&#25968;&#36807;&#22810;&#30340;&#38382;&#39064;&#12290;&#21478;&#19968;&#20010;&#38382;&#39064;&#26159;&#65292;&#22823;&#22810;&#25968;&#22522;&#20110;Transformer&#30340;IR&#26041;&#27861;&#21482;&#20381;&#38752;&#26412;&#22320;&#25110;&#20840;&#23616;&#29305;&#24449;&#65292;&#23548;&#33268;&#25509;&#21463;&#22495;&#26377;&#38480;&#25110;&#23384;&#22312;&#21442;&#25968;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;IR&#32593;&#32476;&#65306;&#20114;&#24800;&#27880;&#24847;&#21147;&#28151;&#21512;Transformer&#65288;RAMiT&#65289;&#12290;&#23427;&#37319;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#32500;&#24230;&#20114;&#24800;&#27880;&#24847;&#21147;&#28151;&#21512;Transformer&#65288;D-RAMiT&#65289;&#22359;&#65292;&#22312;&#20351;&#29992;&#19981;&#21516;&#25968;&#37327;&#30340;&#22810;&#22836;&#24182;&#34892;&#35745;&#31639;&#21452;&#21521;&#65288;&#31354;&#38388;&#21644;&#36890;&#36947;&#65289;&#33258;&#27880;&#24847;&#21147;&#30340;&#24773;&#20917;&#19979;&#12290;&#21452;&#21521;&#20851;&#27880;&#24110;&#21161;&#24444;&#27492;&#24357;&#34917;&#23545;&#26041;&#30340;&#32570;&#28857;&#65292;&#28982;&#21518;&#28151;&#21512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20998;&#23618;&#20114;&#24800;&#27880;&#24847;&#21147;&#28151;&#21512;&#65288;H-RAMi&#65289;&#23618;&#65292;&#23427;&#34917;&#20607;&#20687;&#32032;&#32423;&#20449;&#24687;&#20002;&#22833;&#24182;&#21033;&#29992;&#35821;&#20041;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#25928;&#30340;&#20998;&#23618;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;&#22312;IR&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#22686;&#24378;&#31867;&#22411;&#8212;&#8212;&#24378;&#24230;&#25513;&#30721;&#65292;&#20197;&#25552;&#39640;&#25152;&#25552;&#20986;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;IR&#20219;&#21153;&#20013;&#65292;&#21253;&#25324;&#22270;&#20687;&#21435;&#22122;&#12289;&#22270;&#20687;&#21435;&#27169;&#31946;&#21644;JPEG&#22270;&#20687;&#21435;&#22359;&#65292;&#25105;&#20204;&#30340;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22823;&#22823;&#20943;&#23569;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although many recent works have made advancements in the image restoration (IR) field, they often suffer from an excessive number of parameters. Another issue is that most Transformer-based IR methods focus only on either local or global features, leading to limited receptive fields or deficient parameter issues. To address these problems, we propose a lightweight IR network, Reciprocal Attention Mixing Transformer (RAMiT). It employs our proposed dimensional reciprocal attention mixing Transformer (D-RAMiT) blocks, which compute bi-dimensional (spatial and channel) self-attentions in parallel with different numbers of multi-heads. The bi-dimensional attentions help each other to complement their counterpart's drawbacks and are then mixed. Additionally, we introduce a hierarchical reciprocal attention mixing (H-RAMi) layer that compensates for pixel-level information losses and utilizes semantic information while maintaining an efficient hierarchical structure. Furthermore, we revisit 
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#26597;&#20840;&#38754;&#20171;&#32461;&#20102;&#31264;&#23494;&#23376;&#22270;&#38382;&#39064;&#21450;&#20854;&#21464;&#31181;&#65292;&#22312;&#35752;&#35770;&#26368;&#26032;&#32467;&#26524;&#30340;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#24212;&#29992;&#24191;&#27867;&#30340;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.14467</link><description>&lt;p&gt;
&#31264;&#23494;&#23376;&#22270;&#38382;&#39064;&#21450;&#20854;&#21464;&#31181;&#30340;&#27010;&#36848;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey on the Densest Subgraph Problem and its Variants. (arXiv:2303.14467v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14467
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#20840;&#38754;&#20171;&#32461;&#20102;&#31264;&#23494;&#23376;&#22270;&#38382;&#39064;&#21450;&#20854;&#21464;&#31181;&#65292;&#22312;&#35752;&#35770;&#26368;&#26032;&#32467;&#26524;&#30340;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#24212;&#29992;&#24191;&#27867;&#30340;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31264;&#23494;&#23376;&#22270;&#38382;&#39064;&#35201;&#27714;&#22312;&#32473;&#23450;&#30340;&#22270;&#20013;&#25214;&#21040;&#19968;&#20010;&#39030;&#28857;&#23376;&#38598;&#65292;&#20854;&#35825;&#23548;&#23376;&#22270;&#30340;&#23494;&#24230;&#24230;&#37327;&#26368;&#22823;&#21270;&#12290;&#36825;&#20010;&#38382;&#39064;&#22312;&#31639;&#27861;&#25991;&#29486;&#20013;&#24050;&#32463;&#24341;&#36215;&#20102;&#20116;&#21313;&#24180;&#26469;&#30340;&#24191;&#27867;&#20851;&#27880;&#65292;&#25552;&#20986;&#20102;&#35768;&#22810;&#21464;&#31181;&#65292;&#24182;&#19988;&#26377;&#35768;&#22810;&#24212;&#29992;&#31243;&#24207;&#24314;&#31435;&#22312;&#36825;&#20010;&#22522;&#26412;&#23450;&#20041;&#20043;&#19978;&#12290;&#36817;&#24180;&#26469;&#65292;&#35813;&#38382;&#39064;&#30340;&#30740;&#31350;&#20852;&#36259;&#20877;&#27425;&#22797;&#33487;&#65292;2022&#24180;&#21644;2023&#24180;&#21457;&#34920;&#20102;&#19968;&#20123;&#24320;&#21019;&#24615;&#30340;&#32467;&#26524;&#12290;&#26412;&#31687;&#35843;&#26597;&#25552;&#20379;&#20102;&#22522;&#26412;&#32467;&#26524;&#30340;&#28145;&#20837;&#27010;&#36848;&#21644;&#25991;&#29486;&#20013;&#25552;&#20986;&#30340;&#35768;&#22810;&#21464;&#31181;&#30340;&#35814;&#23613;&#35206;&#30422;&#65292;&#29305;&#21035;&#20851;&#27880;&#26368;&#26032;&#32467;&#26524;&#12290;&#35843;&#26597;&#36824;&#25552;&#20379;&#20102;&#24212;&#29992;&#30340;&#20840;&#38754;&#27010;&#36848;&#65292;&#24182;&#35752;&#35770;&#20102;&#36825;&#20010;&#27704;&#24658;&#30740;&#31350;&#35805;&#39064;&#20013;&#30340;&#19968;&#20123;&#26377;&#36259;&#30340;&#26410;&#35299;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Densest Subgraph Problem requires to find, in a given graph, a subset of vertices whose induced subgraph maximizes a measure of density. The problem has received a great deal of attention in the algorithmic literature over the last five decades, with many variants proposed and many applications built on top of this basic definition. Recent years have witnessed a revival of research interest on this problem with several interesting contributions, including some groundbreaking results, published in 2022 and 2023. This survey provides a deep overview of the fundamental results and an exhaustive coverage of the many variants proposed in the literature, with a special attention on the most recent results. The survey also presents a comprehensive overview of applications and discusses some interesting open problems for this evergreen research topic.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;ANN-SNN&#20307;&#31995;&#32467;&#26500;&#65292;&#36890;&#36807;&#29992;&#20302;&#36895;&#29575;&#30340;&#36741;&#21161;ANN&#21021;&#22987;&#21270;&#29366;&#24577;&#65292;&#35299;&#20915;&#20102;SNN&#19982;&#32463;&#20856;ANN&#22312;&#20934;&#30830;&#24615;&#12289;&#24310;&#36831;&#12289;&#21151;&#32791;&#26041;&#38754;&#30340;&#24179;&#34913;&#24615;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#39640;&#26102;&#38388;&#20998;&#36776;&#29575;&#12289;&#20302;&#24310;&#36831;&#21644;&#20302;&#21151;&#32791;&#30340;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2303.14176</link><description>&lt;p&gt;
&#20302;&#21151;&#32791;&#20302;&#24310;&#36831;&#35270;&#35273;&#24863;&#30693;&#30340;&#28151;&#21512;ANN-SNN&#20307;&#31995;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
A Hybrid ANN-SNN Architecture for Low-Power and Low-Latency Visual Perception. (arXiv:2303.14176v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;ANN-SNN&#20307;&#31995;&#32467;&#26500;&#65292;&#36890;&#36807;&#29992;&#20302;&#36895;&#29575;&#30340;&#36741;&#21161;ANN&#21021;&#22987;&#21270;&#29366;&#24577;&#65292;&#35299;&#20915;&#20102;SNN&#19982;&#32463;&#20856;ANN&#22312;&#20934;&#30830;&#24615;&#12289;&#24310;&#36831;&#12289;&#21151;&#32791;&#26041;&#38754;&#30340;&#24179;&#34913;&#24615;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#39640;&#26102;&#38388;&#20998;&#36776;&#29575;&#12289;&#20302;&#24310;&#36831;&#21644;&#20302;&#21151;&#32791;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#26159;&#19968;&#31867;&#20223;&#29983;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#24322;&#27493;&#21644;&#31232;&#30095;&#22788;&#29702;&#65292;&#25215;&#35834;&#23558;&#20302;&#21151;&#32791;&#21644;&#20302;&#24310;&#36831;&#30340;&#25512;&#29702;&#24212;&#29992;&#20110;&#36793;&#32536;&#35774;&#22791;&#12290;&#28982;&#32780;&#65292;&#20316;&#20026;&#26102;&#38388;&#27169;&#22411;&#65292;SNN&#20381;&#36182;&#20110;&#34920;&#36798;&#20016;&#23500;&#30340;&#29366;&#24577;&#25165;&#33021;&#29983;&#25104;&#19982;&#32463;&#20856;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#30456;&#24403;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#36825;&#20123;&#29366;&#24577;&#20165;&#22312;&#38271;&#26102;&#38388;&#30636;&#24577;&#21608;&#26399;&#21518;&#25910;&#25947;&#65292;&#24182;&#22312;&#27809;&#26377;&#36755;&#20837;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36805;&#36895;&#34928;&#20943;&#65292;&#23548;&#33268;&#26356;&#39640;&#30340;&#24310;&#36831;&#12289;&#21151;&#32791;&#21644;&#36739;&#20302;&#30340;&#20934;&#30830;&#24615;&#12290;&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#20197;&#20302;&#36895;&#29575;&#36816;&#34892;&#30340;&#36741;&#21161;ANN&#21021;&#22987;&#21270;&#29366;&#24577;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;SNN&#20351;&#29992;&#29366;&#24577;&#20197;&#39640;&#26102;&#38388;&#20998;&#36776;&#29575;&#29983;&#25104;&#39044;&#27979;&#65292;&#30452;&#21040;&#19979;&#19968;&#20010;&#21021;&#22987;&#21270;&#38454;&#27573;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#28151;&#21512;ANN-SNN&#27169;&#22411;&#32467;&#21512;&#20102;&#20004;&#20010;&#19990;&#30028;&#30340;&#20248;&#28857;&#65306;&#20248;&#21270;&#20102;ANN&#21644;SNN&#20043;&#38388;&#30340;&#24615;&#33021;&#24179;&#34913;&#65292;&#19981;&#21463;&#38271;&#26102;&#38388;&#29366;&#24577;&#36716;&#25442;&#21644;&#29366;&#24577;&#34928;&#20943;&#30340;&#24433;&#21709;&#65292;&#21487;&#20197;&#23454;&#29616;&#39640;&#26102;&#38388;&#20998;&#36776;&#29575;&#12289;&#20302;&#24310;&#36831;&#21644;&#20302;&#21151;&#32791;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#27169;&#22411;&#22312;&#20302;&#21151;&#32791;/&#20302;&#24310;&#36831;&#35270;&#35273;&#24863;&#30693;&#20219;&#21153;&#19978;&#30340;&#33391;&#22909;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spiking Neural Networks (SNN) are a class of bio-inspired neural networks that promise to bring low-power and low-latency inference to edge devices through asynchronous and sparse processing. However, being temporal models, SNNs depend heavily on expressive states to generate predictions on par with classical artificial neural networks (ANNs). These states converge only after long transient periods, and quickly decay without input data, leading to higher latency, power consumption, and lower accuracy. This work addresses this issue by initializing the state with an auxiliary ANN running at a low rate. The SNN then uses the state to generate predictions with high temporal resolution until the next initialization phase. Our hybrid ANN-SNN model thus combines the best of both worlds: It does not suffer from long state transients and state decay thanks to the ANN, and can generate predictions with high temporal resolution, low latency, and low power thanks to the SNN. We show for the task 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26354;&#29575;&#24179;&#34913;&#29305;&#24449;&#27969;&#24418;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#25506;&#31350;&#20102;&#24863;&#30693;&#27969;&#24418;&#30340;&#20960;&#20309;&#29305;&#24615;&#23545;&#20998;&#31867;&#38590;&#24230;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#26354;&#29575;&#19981;&#24179;&#34913;&#20250;&#23548;&#33268;&#27169;&#22411;&#19981;&#20844;&#24179;&#12290;</title><link>http://arxiv.org/abs/2303.12307</link><description>&lt;p&gt;
&#38271;&#23614;&#20998;&#31867;&#30340;&#26354;&#29575;&#24179;&#34913;&#29305;&#24449;&#27969;&#24418;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Curvature-Balanced Feature Manifold Learning for Long-Tailed Classification. (arXiv:2303.12307v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12307
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26354;&#29575;&#24179;&#34913;&#29305;&#24449;&#27969;&#24418;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#25506;&#31350;&#20102;&#24863;&#30693;&#27969;&#24418;&#30340;&#20960;&#20309;&#29305;&#24615;&#23545;&#20998;&#31867;&#38590;&#24230;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#26354;&#29575;&#19981;&#24179;&#34913;&#20250;&#23548;&#33268;&#27169;&#22411;&#19981;&#20844;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#24212;&#23545;&#38271;&#23614;&#20998;&#31867;&#30340;&#25361;&#25112;&#65292;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#26041;&#27861;&#26469;&#20943;&#23569;&#27169;&#22411;&#20559;&#24046;&#65292;&#20854;&#20013;&#22823;&#22810;&#25968;&#20551;&#35774;&#26679;&#26412;&#36739;&#23569;&#30340;&#31867;&#26159;&#24369;&#31867;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23614;&#37096;&#31867;&#21035;&#24182;&#19981;&#24635;&#26159;&#38590;&#20197;&#23398;&#20064;&#30340;&#65292;&#32780;&#22312;&#26679;&#26412;&#24179;&#34913;&#30340;&#25968;&#25454;&#38598;&#19978;&#35266;&#23519;&#21040;&#20102;&#27169;&#22411;&#20559;&#24046;&#65292;&#36825;&#34920;&#26126;&#23384;&#22312;&#20854;&#20182;&#24433;&#21709;&#27169;&#22411;&#20559;&#24046;&#30340;&#22240;&#32032;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#29992;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#24863;&#30693;&#27969;&#24418;&#30340;&#20960;&#20309;&#24230;&#37327;&#65292;&#24182;&#25506;&#35752;&#20102;&#24863;&#30693;&#27969;&#24418;&#30340;&#20960;&#20309;&#29305;&#24615;&#23545;&#20998;&#31867;&#38590;&#24230;&#21644;&#23398;&#20064;&#22914;&#20309;&#22609;&#36896;&#24863;&#30693;&#27969;&#24418;&#30340;&#20960;&#20309;&#29305;&#24615;&#30340;&#24433;&#21709;&#12290;&#19968;&#20010;&#24847;&#22806;&#30340;&#21457;&#29616;&#26159;&#65306;&#31867;&#21035;&#20934;&#30830;&#24230;&#21644;&#24863;&#30693;&#27969;&#24418;&#30340;&#20998;&#31163;&#31243;&#24230;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36880;&#28176;&#20943;&#23567;&#65292;&#32780;&#19982;&#26354;&#29575;&#30340;&#36127;&#30456;&#20851;&#24615;&#36880;&#28176;&#22686;&#21152;&#65292;&#36825;&#34920;&#26126;&#26354;&#29575;&#19981;&#24179;&#34913;&#23548;&#33268;&#27169;&#22411;&#19981;&#20844;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
To address the challenges of long-tailed classification, researchers have proposed several approaches to reduce model bias, most of which assume that classes with few samples are weak classes. However, recent studies have shown that tail classes are not always hard to learn, and model bias has been observed on sample-balanced datasets, suggesting the existence of other factors that affect model bias. In this work, we systematically propose a series of geometric measurements for perceptual manifolds in deep neural networks, and then explore the effect of the geometric characteristics of perceptual manifolds on classification difficulty and how learning shapes the geometric characteristics of perceptual manifolds. An unanticipated finding is that the correlation between the class accuracy and the separation degree of perceptual manifolds gradually decreases during training, while the negative correlation with the curvature gradually increases, implying that curvature imbalance leads to m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#29983;&#25104;x86&#25104;&#26412;&#27169;&#22411;&#35299;&#37322;&#30340;&#26694;&#26550;COMET&#65292;&#36890;&#36807;&#25552;&#20379;&#35299;&#37322;&#26469;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#25104;&#26412;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#25512;&#24191;&#24615;&#12290;&#30740;&#31350;&#26174;&#31034;&#35813;&#26694;&#26550;&#25152;&#25552;&#20379;&#30340;&#35821;&#20041;&#20449;&#24687;&#19982;&#25104;&#26412;&#39044;&#27979;&#35823;&#24046;&#21576;&#36127;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2302.06836</link><description>&lt;p&gt;
COMET: X86&#25104;&#26412;&#27169;&#22411;&#35299;&#37322;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
COMET: X86 Cost Model Explanation Framework. (arXiv:2302.06836v2 [cs.PF] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06836
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#29983;&#25104;x86&#25104;&#26412;&#27169;&#22411;&#35299;&#37322;&#30340;&#26694;&#26550;COMET&#65292;&#36890;&#36807;&#25552;&#20379;&#35299;&#37322;&#26469;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#25104;&#26412;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#25512;&#24191;&#24615;&#12290;&#30740;&#31350;&#26174;&#31034;&#35813;&#26694;&#26550;&#25152;&#25552;&#20379;&#30340;&#35821;&#20041;&#20449;&#24687;&#19982;&#25104;&#26412;&#39044;&#27979;&#35823;&#24046;&#21576;&#36127;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#31243;&#24207;&#25104;&#26412;&#27169;&#22411;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#25552;&#20379;&#30456;&#24403;&#20934;&#30830;&#30340;&#31243;&#24207;&#25104;&#26412;&#39044;&#27979;&#12290;&#23427;&#20204;&#21487;&#20197;&#21462;&#20195;&#20027;&#27969;&#32534;&#35793;&#22120;&#20013;&#32463;&#36807;&#22823;&#37327;&#24037;&#31243;&#35774;&#35745;&#30340;&#20998;&#26512;&#24615;&#31243;&#24207;&#25104;&#26412;&#27169;&#22411;&#65292;&#20294;&#23427;&#20204;&#30340;&#40657;&#21283;&#23376;&#26412;&#36136;&#38459;&#30861;&#20102;&#23427;&#20204;&#30340;&#37319;&#29992;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#26694;&#26550;&#65292;COMET&#65292;&#29992;&#20110;&#20026;x86&#25104;&#26412;&#27169;&#22411;&#29983;&#25104;&#24544;&#23454;&#12289;&#21487;&#25512;&#24191;&#21644;&#30452;&#35266;&#30340;&#35299;&#37322;&#12290;COMET&#23558;&#21487;&#35299;&#37322;&#24615;&#29305;&#21035;&#24341;&#20837;ML-based&#25104;&#26412;&#27169;&#22411;&#65292;&#20363;&#22914;Ithemal&#12290;&#25105;&#20204;&#29983;&#25104;&#24182;&#27604;&#36739;Ithemal&#30340;COMET&#35299;&#37322;&#19982;&#25163;&#24037;&#31934;&#32454;&#30340;&#20998;&#26512;&#27169;&#22411;uiCA&#30340;COMET&#35299;&#37322;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#21457;&#29616;&#26174;&#31034;&#22312;&#32473;&#23450;&#30340;x86&#22522;&#26412;&#22359;&#30340;&#25104;&#26412;&#27169;&#22411;&#30340;COMET&#35299;&#37322;&#20013;&#65292;&#35821;&#20041;&#26356;&#20016;&#23500;&#30340;&#29305;&#24449;&#30340;&#31361;&#20986;&#31243;&#24230;&#19982;&#25104;&#26412;&#39044;&#27979;&#35823;&#24046;&#21576;&#36127;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
ML-based program cost models have been shown to yield fairly accurate program cost predictions. They can replace heavily-engineered analytical program cost models in mainstream compilers, but their black-box nature discourages their adoption. In this work, we propose the first framework, COMET, for generating faithful, generalizable, and intuitive explanations for x86 cost models. COMET brings interpretability specifically to ML-based cost models, such as Ithemal. We generate and compare COMET's explanations for Ithemal against COMET's explanations for a hand-crafted, accurate analytical model, uiCA. Our empirical findings show an inverse correlation between the error in the cost prediction of a cost model and the prominence of semantically-richer features in COMET's explanations for the cost model for a given x86 basic block.
&lt;/p&gt;</description></item></channel></rss>