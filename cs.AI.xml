<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#32508;&#36848;&#20840;&#38754;&#23457;&#35270;&#20102;&#22522;&#20110;&#20248;&#21270;&#30340;&#20219;&#21153;&#19982;&#36816;&#21160;&#35268;&#21010;&#65292;&#37325;&#28857;&#35752;&#35770;&#20102;&#22914;&#20309;&#36890;&#36807;&#28151;&#21512;&#20248;&#21270;&#26041;&#27861;&#35299;&#20915;&#39640;&#24230;&#22797;&#26434;&#12289;&#25509;&#35302;&#20016;&#23500;&#30340;&#26426;&#22120;&#20154;&#36816;&#21160;&#21644;&#25805;&#20316;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2404.02817</link><description>&lt;p&gt;
&#20248;&#21270;&#22411;&#20219;&#21153;&#19982;&#36816;&#21160;&#35268;&#21010;&#32508;&#36848;&#65306;&#20174;&#32463;&#20856;&#21040;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Survey of Optimization-based Task and Motion Planning: From Classical To Learning Approaches
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02817
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#20840;&#38754;&#23457;&#35270;&#20102;&#22522;&#20110;&#20248;&#21270;&#30340;&#20219;&#21153;&#19982;&#36816;&#21160;&#35268;&#21010;&#65292;&#37325;&#28857;&#35752;&#35770;&#20102;&#22914;&#20309;&#36890;&#36807;&#28151;&#21512;&#20248;&#21270;&#26041;&#27861;&#35299;&#20915;&#39640;&#24230;&#22797;&#26434;&#12289;&#25509;&#35302;&#20016;&#23500;&#30340;&#26426;&#22120;&#20154;&#36816;&#21160;&#21644;&#25805;&#20316;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20219;&#21153;&#19982;&#36816;&#21160;&#35268;&#21010;&#65288;TAMP&#65289;&#23558;&#39640;&#23618;&#20219;&#21153;&#35268;&#21010;&#21644;&#20302;&#23618;&#36816;&#21160;&#35268;&#21010;&#32467;&#21512;&#36215;&#26469;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#26377;&#25928;&#22320;&#25512;&#29702;&#35299;&#20915;&#38271;&#26102;&#22495;&#12289;&#21160;&#24577;&#20219;&#21153;&#12290;&#22522;&#20110;&#20248;&#21270;&#30340;TAMP&#19987;&#27880;&#20110;&#36890;&#36807;&#30446;&#26631;&#20989;&#25968;&#23450;&#20041;&#30446;&#26631;&#26465;&#20214;&#30340;&#28151;&#21512;&#20248;&#21270;&#26041;&#27861;&#65292;&#24182;&#19988;&#33021;&#22815;&#22788;&#29702;&#24320;&#25918;&#24335;&#30446;&#26631;&#12289;&#26426;&#22120;&#20154;&#21160;&#24577;&#21644;&#26426;&#22120;&#20154;&#19982;&#29615;&#22659;&#20043;&#38388;&#30340;&#29289;&#29702;&#20132;&#20114;&#12290;&#22240;&#27492;&#65292;&#22522;&#20110;&#20248;&#21270;&#30340;TAMP&#29305;&#21035;&#36866;&#21512;&#35299;&#20915;&#39640;&#24230;&#22797;&#26434;&#12289;&#25509;&#35302;&#20016;&#23500;&#30340;&#36816;&#21160;&#21644;&#25805;&#20316;&#38382;&#39064;&#12290;&#26412;&#32508;&#36848;&#20840;&#38754;&#23457;&#35270;&#20102;&#22522;&#20110;&#20248;&#21270;&#30340;TAMP&#65292;&#28085;&#30422;&#20102;&#65288;i&#65289;&#35268;&#21010;&#39046;&#22495;&#34920;&#31034;&#65292;&#21253;&#25324;&#21160;&#20316;&#25551;&#36848;&#35821;&#35328;&#21644;&#26102;&#24577;&#36923;&#36753;&#65292;&#65288;ii&#65289;TAMP&#21508;&#32452;&#20214;&#30340;&#20010;&#21035;&#35299;&#20915;&#31574;&#30053;&#65292;&#21253;&#25324;&#20154;&#24037;&#26234;&#33021;&#35268;&#21010;&#21644;&#36712;&#36857;&#20248;&#21270;&#65288;TO&#65289;&#65292;&#20197;&#21450;&#65288;iii&#65289;&#22522;&#20110;&#36923;&#36753;&#30340;&#20219;&#21153;&#35268;&#21010;&#19982;&#22522;&#20110;&#27169;&#22411;&#30340;TO&#20043;&#38388;&#30340;&#21160;&#24577;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02817v1 Announce Type: cross  Abstract: Task and Motion Planning (TAMP) integrates high-level task planning and low-level motion planning to equip robots with the autonomy to effectively reason over long-horizon, dynamic tasks. Optimization-based TAMP focuses on hybrid optimization approaches that define goal conditions via objective functions and are capable of handling open-ended goals, robotic dynamics, and physical interaction between the robot and the environment. Therefore, optimization-based TAMP is particularly suited to solve highly complex, contact-rich locomotion and manipulation problems. This survey provides a comprehensive review on optimization-based TAMP, covering (i) planning domain representations, including action description languages and temporal logic, (ii) individual solution strategies for components of TAMP, including AI planning and trajectory optimization (TO), and (iii) the dynamic interplay between logic-based task planning and model-based TO. A 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;GPT-3.5-Turbo&#27169;&#22411;&#20316;&#20026;AI&#36741;&#23548;&#21592;&#38598;&#25104;&#21040;APAS Artemis&#20013;&#65292;&#25506;&#35752;&#20102;&#36719;&#20214;&#24037;&#31243;&#25945;&#32946;&#20013;&#23398;&#29983;&#19982;AI&#36741;&#23548;&#21592;&#30340;&#20114;&#21160;&#27169;&#24335;&#65292;&#21457;&#29616;&#20102;&#19981;&#21516;&#29992;&#25143;&#31867;&#22411;&#65292;&#24182;&#24378;&#35843;&#20102;&#21450;&#26102;&#21453;&#39304;&#21644;&#21487;&#25193;&#23637;&#24615;&#31561;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2404.02548</link><description>&lt;p&gt;
&#36719;&#20214;&#24037;&#31243;&#25945;&#32946;&#20013;&#30340;AI&#36741;&#23548;
&lt;/p&gt;
&lt;p&gt;
AI-Tutoring in Software Engineering Education
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02548
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;GPT-3.5-Turbo&#27169;&#22411;&#20316;&#20026;AI&#36741;&#23548;&#21592;&#38598;&#25104;&#21040;APAS Artemis&#20013;&#65292;&#25506;&#35752;&#20102;&#36719;&#20214;&#24037;&#31243;&#25945;&#32946;&#20013;&#23398;&#29983;&#19982;AI&#36741;&#23548;&#21592;&#30340;&#20114;&#21160;&#27169;&#24335;&#65292;&#21457;&#29616;&#20102;&#19981;&#21516;&#29992;&#25143;&#31867;&#22411;&#65292;&#24182;&#24378;&#35843;&#20102;&#21450;&#26102;&#21453;&#39304;&#21644;&#21487;&#25193;&#23637;&#24615;&#31561;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#25945;&#32946;&#34892;&#19994;&#27491;&#38754;&#20020;&#21464;&#38761;&#12290;AI&#39537;&#21160;&#24037;&#20855;&#22312;&#22686;&#24378;&#23398;&#20064;&#20307;&#39564;&#26041;&#38754;&#30340;&#28508;&#21147;&#26159;&#24040;&#22823;&#30340;&#65292;&#29305;&#21035;&#26159;&#22312;&#32534;&#31243;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;LLMs&#22312;&#33258;&#21160;&#21270;&#32534;&#31243;&#35780;&#20272;&#31995;&#32479;(APASs)&#20013;&#20316;&#20026;AI&#36741;&#23548;&#21592;&#30340;&#31185;&#23398;&#35780;&#20272;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#20102;&#35299;&#23398;&#29983;&#22914;&#20309;&#19982;&#36825;&#20123;AI&#36741;&#23548;&#21592;&#20114;&#21160;&#65292;&#24182;&#20998;&#26512;&#20182;&#20204;&#30340;&#20307;&#39564;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;GPT-3.5-Turbo&#27169;&#22411;&#20316;&#20026;AI&#36741;&#23548;&#21592;&#38598;&#25104;&#21040;APAS Artemis&#20013;&#36827;&#34892;&#20102;&#25506;&#32034;&#24615;&#26696;&#20363;&#30740;&#31350;&#12290;&#36890;&#36807;&#32467;&#21512;&#23454;&#35777;&#25968;&#25454;&#25910;&#38598;&#21644;&#25506;&#32034;&#24615;&#35843;&#26597;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#22522;&#20110;&#20182;&#20204;&#19982;AI&#36741;&#23548;&#21592;&#20114;&#21160;&#27169;&#24335;&#30340;&#19981;&#21516;&#29992;&#25143;&#31867;&#22411;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;&#35832;&#22914;&#21450;&#26102;&#21453;&#39304;&#21644;&#21487;&#25193;&#23637;&#24615;&#31561;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#25361;&#25112;&#22914;&#36890;&#29992;&#39046;&#22495;&#22238;&#22797;&#31561;&#20063;&#21516;&#26679;&#20540;&#24471;&#20851;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02548v1 Announce Type: cross  Abstract: With the rapid advancement of artificial intelligence (AI) in various domains, the education sector is set for transformation. The potential of AI-driven tools in enhancing the learning experience, especially in programming, is immense. However, the scientific evaluation of Large Language Models (LLMs) used in Automated Programming Assessment Systems (APASs) as an AI-Tutor remains largely unexplored. Therefore, there is a need to understand how students interact with such AI-Tutors and to analyze their experiences. In this paper, we conducted an exploratory case study by integrating the GPT-3.5-Turbo model as an AI-Tutor within the APAS Artemis. Through a combination of empirical data collection and an exploratory survey, we identified different user types based on their interaction patterns with the AI-Tutor. Additionally, the findings highlight advantages, such as timely feedback and scalability. However, challenges like generic resp
&lt;/p&gt;</description></item><item><title>PhonologyBench&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#26126;&#30830;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33521;&#35821;&#20013;&#30340;&#38899;&#38901;&#25216;&#33021;&#65292;&#23637;&#31034;&#20102;LLMs&#22312;&#27809;&#26377;&#35821;&#38899;&#25968;&#25454;&#24773;&#20917;&#19979;&#22312;PhonologyBench&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.02456</link><description>&lt;p&gt;
PhonologyBench&#65306;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38899;&#38901;&#25216;&#33021;
&lt;/p&gt;
&lt;p&gt;
PhonologyBench: Evaluating Phonological Skills of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02456
&lt;/p&gt;
&lt;p&gt;
PhonologyBench&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#26126;&#30830;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33521;&#35821;&#20013;&#30340;&#38899;&#38901;&#25216;&#33021;&#65292;&#23637;&#31034;&#20102;LLMs&#22312;&#27809;&#26377;&#35821;&#38899;&#25968;&#25454;&#24773;&#20917;&#19979;&#22312;PhonologyBench&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#38901;&#23398;&#26159;&#30740;&#31350;&#35821;&#38899;&#32467;&#26500;&#21644;&#21457;&#38899;&#35268;&#21017;&#30340;&#23398;&#31185;&#65292;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30740;&#31350;&#20013;&#19968;&#20010;&#20851;&#38190;&#20294;&#32463;&#24120;&#34987;&#24573;&#35270;&#30340;&#32452;&#25104;&#37096;&#20998;&#12290;LLMs&#22312;&#21508;&#31181;&#21033;&#29992;&#38899;&#38901;&#23398;&#30340;&#19979;&#28216;&#24212;&#29992;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#22914;&#25945;&#32946;&#24037;&#20855;&#21644;&#35799;&#27468;&#29983;&#25104;&#12290;&#27492;&#22806;&#65292;LLMs&#21487;&#33021;&#20250;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#23398;&#20064;&#19981;&#23436;&#32654;&#30340;&#27491;&#23383;&#21644;&#38899;&#26631;&#24418;&#24335;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#22240;&#27492;&#65292;&#23545;LLMs&#30340;&#38899;&#38901;&#25216;&#33021;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PhonologyBench&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#19977;&#20010;&#35786;&#26029;&#20219;&#21153;&#65292;&#26088;&#22312;&#26126;&#30830;&#27979;&#35797;LLMs&#22312;&#33521;&#35821;&#20013;&#30340;&#38899;&#38901;&#25216;&#33021;&#65306;&#24418;&#38899;&#36716;&#25442;&#12289;&#38899;&#33410;&#35745;&#25968;&#21644;&#25276;&#38901;&#35789;&#29983;&#25104;&#12290;&#23613;&#31649;&#27809;&#26377;&#35775;&#38382;&#35821;&#38899;&#25968;&#25454;&#65292;LLMs&#22312;PhonologyBench&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#25276;&#38901;&#35789;&#29983;&#25104;&#21644;&#38899;&#33410;&#35745;&#25968;&#26041;&#38754;&#23384;&#22312;&#26174;&#33879;&#30340;17%&#21644;45%&#30340;&#24046;&#36317;&#65292; respectively, when...
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02456v1 Announce Type: cross  Abstract: Phonology, the study of speech's structure and pronunciation rules, is a critical yet often overlooked component in Large Language Model (LLM) research. LLMs are widely used in various downstream applications that leverage phonology such as educational tools and poetry generation. Moreover, LLMs can potentially learn imperfect associations between orthographic and phonological forms from the training data. Thus, it is imperative to benchmark the phonological skills of LLMs. To this end, we present PhonologyBench, a novel benchmark consisting of three diagnostic tasks designed to explicitly test the phonological skills of LLMs in English: grapheme-to-phoneme conversion, syllable counting, and rhyme word generation. Despite having no access to speech data, LLMs showcased notable performance on the PhonologyBench tasks. However, we observe a significant gap of 17% and 45% on Rhyme Word Generation and Syllable counting, respectively, when 
&lt;/p&gt;</description></item><item><title>RAT&#27169;&#22411;&#26159;&#20026;&#20102;&#35299;&#20915;&#24403;&#21069;CTR&#39044;&#27979;&#27169;&#22411;&#20165;&#20851;&#27880;&#26679;&#26412;&#20869;&#29305;&#24449;&#20132;&#20114;&#32780;&#24573;&#30053;&#36328;&#26679;&#26412;&#20851;&#31995;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#26816;&#32034;&#30456;&#20284;&#26679;&#26412;&#26500;&#24314;&#22686;&#24378;&#36755;&#20837;&#65292;&#23454;&#29616;&#20102;&#23545;&#26679;&#26412;&#20869;&#21644;&#36328;&#26679;&#26412;&#30340;&#20840;&#38754;&#29305;&#24449;&#20132;&#20114;&#25512;&#29702;&#65292;&#25552;&#39640;&#20102;CTR&#39044;&#27979;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2404.02249</link><description>&lt;p&gt;
RAT: &#26816;&#32034;&#22686;&#24378;&#21464;&#25442;&#22120;&#29992;&#20110;&#28857;&#20987;&#29575;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
RAT: Retrieval-Augmented Transformer for Click-Through Rate Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02249
&lt;/p&gt;
&lt;p&gt;
RAT&#27169;&#22411;&#26159;&#20026;&#20102;&#35299;&#20915;&#24403;&#21069;CTR&#39044;&#27979;&#27169;&#22411;&#20165;&#20851;&#27880;&#26679;&#26412;&#20869;&#29305;&#24449;&#20132;&#20114;&#32780;&#24573;&#30053;&#36328;&#26679;&#26412;&#20851;&#31995;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#26816;&#32034;&#30456;&#20284;&#26679;&#26412;&#26500;&#24314;&#22686;&#24378;&#36755;&#20837;&#65292;&#23454;&#29616;&#20102;&#23545;&#26679;&#26412;&#20869;&#21644;&#36328;&#26679;&#26412;&#30340;&#20840;&#38754;&#29305;&#24449;&#20132;&#20114;&#25512;&#29702;&#65292;&#25552;&#39640;&#20102;CTR&#39044;&#27979;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#28857;&#20987;&#29575;&#65288;CTR&#65289;&#26159;Web&#24212;&#29992;&#31243;&#24207;&#30340;&#22522;&#26412;&#20219;&#21153;&#65292;&#20854;&#20013;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#26159;&#35774;&#35745;&#26377;&#25928;&#30340;&#29305;&#24449;&#20132;&#20114;&#27169;&#22411;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#20110;&#23545;&#21333;&#20010;&#26679;&#26412;&#20869;&#30340;&#29305;&#24449;&#20132;&#20114;&#36827;&#34892;&#24314;&#27169;&#65292;&#32780;&#24573;&#30053;&#20102;&#21487;&#20197;&#20316;&#20026;&#21442;&#32771;&#32972;&#26223;&#26469;&#22686;&#24378;&#39044;&#27979;&#30340;&#28508;&#22312;&#36328;&#26679;&#26412;&#20851;&#31995;&#12290;&#20026;&#24357;&#34917;&#36825;&#31181;&#19981;&#36275;&#65292;&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#21464;&#25442;&#22120;&#65288;RAT&#65289;&#65292;&#26088;&#22312;&#33719;&#21462;&#26679;&#26412;&#20869;&#21644;&#36328;&#26679;&#26412;&#20043;&#38388;&#30340;&#32454;&#31890;&#24230;&#29305;&#24449;&#20132;&#20114;&#12290;&#36890;&#36807;&#26816;&#32034;&#30456;&#20284;&#26679;&#26412;&#65292;&#25105;&#20204;&#20026;&#27599;&#20010;&#30446;&#26631;&#26679;&#26412;&#26500;&#24314;&#22686;&#24378;&#36755;&#20837;&#12290;&#28982;&#21518;&#21033;&#29992;&#32423;&#32852;&#27880;&#24847;&#21147;&#26500;&#24314;Transformer&#23618;&#65292;&#20197;&#25429;&#33719;&#26679;&#26412;&#20869;&#21644;&#36328;&#26679;&#26412;&#29305;&#24449;&#20132;&#20114;&#65292;&#20419;&#36827;&#20840;&#38754;&#25512;&#29702;&#20197;&#25913;&#21892;CTR&#39044;&#27979;&#30340;&#21516;&#26102;&#20445;&#25345;&#25928;&#29575;&#12290;&#23545;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#20102;RAT&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02249v1 Announce Type: cross  Abstract: Predicting click-through rates (CTR) is a fundamental task for Web applications, where a key issue is to devise effective models for feature interactions. Current methodologies predominantly concentrate on modeling feature interactions within an individual sample, while overlooking the potential cross-sample relationships that can serve as a reference context to enhance the prediction. To make up for such deficiency, this paper develops a Retrieval-Augmented Transformer (RAT), aiming to acquire fine-grained feature interactions within and across samples. By retrieving similar samples, we construct augmented input for each target sample. We then build Transformer layers with cascaded attention to capture both intra- and cross-sample feature interactions, facilitating comprehensive reasoning for improved CTR prediction while retaining efficiency. Extensive experiments on real-world datasets substantiate the effectiveness of RAT and sugge
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GRIFFIN&#30340;&#35757;&#32451;-free MoE&#65292;&#33021;&#22815;&#22312;&#21508;&#31181;LLM&#27169;&#22411;&#20013;&#36873;&#25321;&#21807;&#19968;&#30340;FF&#19987;&#23478;&#20197;&#23454;&#29616;&#39640;&#25928;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2404.01365</link><description>&lt;p&gt;
&#22522;&#20110;&#25552;&#31034;&#30340;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#29992;&#20110;&#39640;&#25928;&#29983;&#25104;LLM
&lt;/p&gt;
&lt;p&gt;
Prompt-prompted Mixture of Experts for Efficient LLM Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01365
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GRIFFIN&#30340;&#35757;&#32451;-free MoE&#65292;&#33021;&#22815;&#22312;&#21508;&#31181;LLM&#27169;&#22411;&#20013;&#36873;&#25321;&#21807;&#19968;&#30340;FF&#19987;&#23478;&#20197;&#23454;&#29616;&#39640;&#25928;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22522;&#20110;transformer&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#65292;&#30001;&#20110;&#20854;&#20986;&#33394;&#30340;&#23454;&#29992;&#24615;&#65292;&#23427;&#20204;&#24050;&#34987;&#24212;&#29992;&#20110;&#35768;&#22810;&#39046;&#22495;&#65292;&#20294;&#22312;&#37096;&#32626;&#26102;&#23384;&#22312;&#30456;&#24403;&#22823;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#19968;&#20123;&#26041;&#27861;&#65292;&#22914;&#20462;&#21098;&#25110;&#26500;&#24314;&#28151;&#21512;&#19987;&#23478;&#65288;MoE&#65289;&#65292;&#26088;&#22312;&#21033;&#29992;transformer&#21069;&#39304;&#65288;FF&#65289;&#22359;&#20013;&#30340;&#31232;&#30095;&#24615;&#65292;&#20197;&#25552;&#39640;&#36895;&#24230;&#24182;&#38477;&#20302;&#20869;&#23384;&#38656;&#27714;&#12290;&#20294;&#26159;&#65292;&#36825;&#20123;&#25216;&#26415;&#22312;&#23454;&#36341;&#20013;&#21487;&#33021;&#38750;&#24120;&#26114;&#36149;&#21644;&#19981;&#28789;&#27963;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#24120;&#38656;&#35201;&#35757;&#32451;&#25110;&#20165;&#38480;&#20110;&#29305;&#23450;&#31867;&#22411;&#30340;&#26550;&#26500;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;GRIFFIN&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;MoE&#65292;&#23427;&#22312;&#24207;&#21015;&#32423;&#21035;&#20026;&#19981;&#21516;&#38750;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;&#22823;&#37327;LLMs&#36873;&#25321;&#29420;&#29305;&#30340;FF&#19987;&#23478;&#20197;&#23454;&#29616;&#39640;&#25928;&#29983;&#25104;&#12290;&#36825;&#26159;&#21487;&#33021;&#30340;&#65292;&#22240;&#20026;&#25105;&#20204;&#20851;&#38190;&#35266;&#23519;&#21040;&#65292;&#35768;&#22810;&#32463;&#36807;&#35757;&#32451;&#30340;LLMs&#22312;&#24207;&#21015;&#20013;&#33258;&#28982;&#20135;&#29983;&#39640;&#24230;&#32467;&#26500;&#21270;&#30340;FF&#28608;&#27963;&#27169;&#24335;&#65292;&#36825;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01365v1 Announce Type: cross  Abstract: With the development of transformer-based large language models (LLMs), they have been applied to many fields due to their remarkable utility, but this comes at a considerable computational cost at deployment. Fortunately, some methods such as pruning or constructing a mixture of experts (MoE) aim at exploiting sparsity in transformer feedforward (FF) blocks to gain boosts in speed and reduction in memory requirements. However, these techniques can be very costly and inflexible in practice, as they often require training or are restricted to specific types of architectures. To address this, we introduce GRIFFIN, a novel training-free MoE that selects unique FF experts at the sequence level for efficient generation across a plethora of LLMs with different non-ReLU activation functions. This is possible due to a critical observation that many trained LLMs naturally produce highly structured FF activation patterns within a sequence, which
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Regularized Best-of-N (RBoN)&#65292;&#36890;&#36807;&#24341;&#20837;&#25509;&#36817;&#24615;&#39033;&#26469;&#20943;&#36731;&#22870;&#21169;&#27450;&#39575;&#65292;&#25552;&#39640;&#20102;&#31639;&#27861;&#22312;&#35299;&#30721;&#26102;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2404.01054</link><description>&lt;p&gt;
&#27491;&#21017;&#21270;&#30340;&#26368;&#20339;-N&#37319;&#26679;&#20197;&#20943;&#36731;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#20013;&#30340;&#22870;&#21169;&#27450;&#39575;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Regularized Best-of-N Sampling to Mitigate Reward Hacking for Language Model Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01054
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Regularized Best-of-N (RBoN)&#65292;&#36890;&#36807;&#24341;&#20837;&#25509;&#36817;&#24615;&#39033;&#26469;&#20943;&#36731;&#22870;&#21169;&#27450;&#39575;&#65292;&#25552;&#39640;&#20102;&#31639;&#27861;&#22312;&#35299;&#30721;&#26102;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Best-of-N (BoN)&#37319;&#26679;&#19982;&#22870;&#21169;&#27169;&#22411;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#31574;&#30053;&#65292;&#29992;&#20110;&#22312;&#35299;&#30721;&#26102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;BoN&#37319;&#26679;&#23481;&#26131;&#21463;&#21040;&#22870;&#21169;&#27450;&#39575;&#38382;&#39064;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#38450;&#27490;&#22870;&#21169;&#27450;&#39575;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Regularized Best-of-N (RBoN)&#30340;&#21464;&#20307;&#65292;&#36890;&#36807;&#22312;&#21709;&#24212;&#36873;&#25321;&#20013;&#32467;&#21512;&#25509;&#36817;&#24615;&#39033;&#26469;&#20943;&#36731;&#22870;&#21169;&#27450;&#39575;&#65292;&#31867;&#20284;&#20110;&#20559;&#22909;&#23398;&#20064;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01054v1 Announce Type: cross  Abstract: Best-of-N (BoN) sampling with a reward model has been shown to be an effective strategy for aligning Large Language Models (LLMs) to human preferences at the time of decoding. BoN sampling is susceptible to a problem known as reward hacking. Because the reward model is an imperfect proxy for the true objective, over-optimizing its value can compromise its performance on the true objective. A common solution to prevent reward hacking in preference learning techniques is to optimize a reward using proximity regularization (e.g., KL regularization), which ensures that the language model remains close to the reference model. In this research, we propose Regularized Best-of-N (RBoN), a variant of BoN that aims to mitigate reward hacking by incorporating a proximity term in response selection, similar to preference learning techniques. We evaluate two variants of RBoN on the AlpacaFarm dataset and find that they outperform BoN, especially wh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20197;&#26426;&#22120;&#23398;&#20064;&#35270;&#35282;&#32508;&#36848;&#20102;&#35745;&#31639;&#26426;&#33258;&#36866;&#24212;&#27979;&#35797;&#65288;CAT&#65289;&#65292;&#37325;&#28857;&#35299;&#26512;&#20854;&#27979;&#35797;&#38382;&#39064;&#36873;&#25321;&#31639;&#27861;&#21644;&#22914;&#20309;&#20248;&#21270;&#35748;&#30693;&#35786;&#26029;&#27169;&#22411;&#12289;&#39064;&#24211;&#26500;&#24314;&#21644;&#27979;&#35797;&#25511;&#21046;&#12290;</title><link>https://arxiv.org/abs/2404.00712</link><description>&lt;p&gt;
&#35745;&#31639;&#26426;&#33258;&#36866;&#24212;&#27979;&#35797;&#32508;&#36848;&#65306;&#26426;&#22120;&#23398;&#20064;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Survey of Computerized Adaptive Testing: A Machine Learning Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20197;&#26426;&#22120;&#23398;&#20064;&#35270;&#35282;&#32508;&#36848;&#20102;&#35745;&#31639;&#26426;&#33258;&#36866;&#24212;&#27979;&#35797;&#65288;CAT&#65289;&#65292;&#37325;&#28857;&#35299;&#26512;&#20854;&#27979;&#35797;&#38382;&#39064;&#36873;&#25321;&#31639;&#27861;&#21644;&#22914;&#20309;&#20248;&#21270;&#35748;&#30693;&#35786;&#26029;&#27169;&#22411;&#12289;&#39064;&#24211;&#26500;&#24314;&#21644;&#27979;&#35797;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#33258;&#36866;&#24212;&#27979;&#35797;&#65288;CAT&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#25928;&#12289;&#37327;&#36523;&#23450;&#21046;&#30340;&#35780;&#20272;&#32771;&#29983;&#29087;&#32451;&#31243;&#24230;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26681;&#25454;&#20182;&#20204;&#30340;&#34920;&#29616;&#21160;&#24577;&#35843;&#25972;&#27979;&#35797;&#38382;&#39064;&#12290;CAT&#24191;&#27867;&#24212;&#29992;&#20110;&#25945;&#32946;&#12289;&#21307;&#30103;&#12289;&#20307;&#32946;&#21644;&#31038;&#20250;&#23398;&#31561;&#22810;&#20010;&#39046;&#22495;&#65292;&#24443;&#24213;&#25913;&#21464;&#20102;&#27979;&#35797;&#23454;&#36341;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#22823;&#35268;&#27169;&#27979;&#35797;&#30340;&#22686;&#21152;&#22797;&#26434;&#24615;&#65292;CAT&#24050;&#32463;&#34701;&#21512;&#20102;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#19968;&#20010;&#20197;&#26426;&#22120;&#23398;&#20064;&#20026;&#37325;&#28857;&#30340;CAT&#32508;&#36848;&#65292;&#20174;&#26032;&#30340;&#35282;&#24230;&#35299;&#35835;&#36825;&#31181;&#33258;&#36866;&#24212;&#27979;&#35797;&#26041;&#27861;&#12290;&#36890;&#36807;&#30740;&#31350;CAT&#36866;&#24212;&#24615;&#26680;&#24515;&#30340;&#27979;&#35797;&#38382;&#39064;&#36873;&#25321;&#31639;&#27861;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#20854;&#21151;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#35748;&#30693;&#35786;&#26029;&#27169;&#22411;&#12289;&#39064;&#24211;&#26500;&#24314;&#21644;CAT&#20013;&#30340;&#27979;&#35797;&#25511;&#21046;&#65292;&#25506;&#32034;&#20102;&#26426;&#22120;&#23398;&#20064;&#22914;&#20309;&#20248;&#21270;&#36825;&#20123;&#32452;&#25104;&#37096;&#20998;&#12290;&#36890;&#36807;&#23545;&#24403;&#21069;&#24773;&#20917;&#30340;&#20998;&#26512;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00712v1 Announce Type: cross  Abstract: Computerized Adaptive Testing (CAT) provides an efficient and tailored method for assessing the proficiency of examinees, by dynamically adjusting test questions based on their performance. Widely adopted across diverse fields like education, healthcare, sports, and sociology, CAT has revolutionized testing practices. While traditional methods rely on psychometrics and statistics, the increasing complexity of large-scale testing has spurred the integration of machine learning techniques. This paper aims to provide a machine learning-focused survey on CAT, presenting a fresh perspective on this adaptive testing method. By examining the test question selection algorithm at the heart of CAT's adaptivity, we shed light on its functionality. Furthermore, we delve into cognitive diagnosis models, question bank construction, and test control within CAT, exploring how machine learning can optimize these components. Through an analysis of curre
&lt;/p&gt;</description></item><item><title>&#20027;&#21160;&#35270;&#35273;&#26426;&#21046;&#30340;&#38598;&#25104;&#21487;&#20197;&#25552;&#20379;&#24403;&#21069;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#20248;&#21183;&#65292;&#24182;&#19988;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;GFNet&#21644;FALcon&#36825;&#20004;&#31181;&#20027;&#21160;&#35270;&#35273;&#26041;&#27861;&#22312;&#40657;&#30418;&#23041;&#32961;&#27169;&#22411;&#19979;&#30340;&#22266;&#26377;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.00185</link><description>&lt;p&gt;
&#35770;&#20027;&#21160;&#35270;&#35273;&#31995;&#32479;&#22266;&#26377;&#23545;&#25239;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
On Inherent Adversarial Robustness of Active Vision Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00185
&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#35270;&#35273;&#26426;&#21046;&#30340;&#38598;&#25104;&#21487;&#20197;&#25552;&#20379;&#24403;&#21069;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#20248;&#21183;&#65292;&#24182;&#19988;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;GFNet&#21644;FALcon&#36825;&#20004;&#31181;&#20027;&#21160;&#35270;&#35273;&#26041;&#27861;&#22312;&#40657;&#30418;&#23041;&#32961;&#27169;&#22411;&#19979;&#30340;&#22266;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#26679;&#26412;&#30340;&#25915;&#20987;&#65292;&#36825;&#31181;&#23545;&#25239;&#26679;&#26412;&#36890;&#36807;&#28155;&#21152;&#31934;&#24515;&#35774;&#35745;&#30340;&#22122;&#22768;&#26469;&#25913;&#21464;&#23427;&#20204;&#30340;&#39044;&#27979;&#12290;&#30001;&#20110;&#20154;&#31867;&#30524;&#30555;&#23545;&#36825;&#31181;&#36755;&#20837;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#21487;&#33021;&#30340;&#24369;&#28857;&#28304;&#20110;&#29992;&#30456;&#21516;&#37325;&#35201;&#24615;&#22788;&#29702;&#27599;&#20010;&#20687;&#32032;&#30340;&#26631;&#20934;&#26041;&#24335;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#31070;&#32463;&#31185;&#23398;&#34920;&#26126;&#20154;&#31867;&#35270;&#35273;&#31995;&#32479;&#21487;&#20197;&#36890;&#36807;&#65288;1&#65289;&#20999;&#25442;&#22810;&#20010;&#27880;&#35270;&#28857;&#65288;&#25195;&#35270;&#65289;&#21644;&#65288;2&#65289;&#20197;&#38750;&#22343;&#21248;&#22806;&#37096;&#20998;&#36776;&#29575;&#65288;&#35270;&#31070;&#32463;&#65289;&#22788;&#29702;&#21608;&#22260;&#20449;&#24687;&#26469;&#21306;&#20998;&#26174;&#33879;&#29305;&#24449;&#12290;&#26412;&#30740;&#31350;&#25552;&#20513;&#23558;&#36825;&#31181;&#20027;&#21160;&#35270;&#35273;&#26426;&#21046;&#34701;&#20837;&#24403;&#21069;&#30340;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#20013;&#65292;&#20197;&#25552;&#20379;&#40065;&#26834;&#24615;&#20248;&#21183;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#22312;&#40657;&#30418;&#23041;&#32961;&#27169;&#22411;&#19979;&#65292;&#32463;&#39564;&#24615;&#22320;&#23637;&#31034;&#20102;&#20004;&#31181;&#20027;&#21160;&#35270;&#35273;&#26041;&#27861;GFNet&#21644;FALcon&#30340;&#22266;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00185v1 Announce Type: cross  Abstract: Current Deep Neural Networks are vulnerable to adversarial examples, which alter their predictions by adding carefully crafted noise. Since human eyes are robust to such inputs, it is possible that the vulnerability stems from the standard way of processing inputs in one shot by processing every pixel with the same importance. In contrast, neuroscience suggests that the human vision system can differentiate salient features by (1) switching between multiple fixation points (saccades) and (2) processing the surrounding with a non-uniform external resolution (foveation). In this work, we advocate that the integration of such active vision mechanisms into current deep learning systems can offer robustness benefits. Specifically, we empirically demonstrate the inherent robustness of two active vision methods - GFNet and FALcon - under a black box threat model. By learning and inferencing based on downsampled glimpses obtained from multiple
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; AI &#22312;&#23398;&#20064;&#21644;&#25945;&#32946;&#20013;&#30340;&#22810;&#32500;&#35270;&#35282;&#65292;&#24378;&#35843;&#20102; AI&#12289;&#20998;&#26512;&#21644;&#23398;&#20064;&#36807;&#31243;&#20043;&#38388;&#38169;&#32508;&#22797;&#26434;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#25361;&#25112;&#20102;&#23558; AI &#35270;&#20026;&#38543;&#26426;&#24037;&#20855;&#30340;&#35266;&#24565;&#65292;&#24378;&#35843;&#20102; AI &#20316;&#20026;&#29702;&#35299;&#20154;&#31867;&#23398;&#20064;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#31181;&#29420;&#29305;&#30340;&#25945;&#32946;&#20013;&#20154;&#24037;&#26234;&#33021;&#30340;&#27010;&#24565;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.16081</link><description>&lt;p&gt;
&#25945;&#32946;&#20013;&#23398;&#20064;&#12289;&#20998;&#26512;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#30456;&#20114;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
The Interplay of Learning, Analytics, and Artificial Intelligence in Education
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16081
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; AI &#22312;&#23398;&#20064;&#21644;&#25945;&#32946;&#20013;&#30340;&#22810;&#32500;&#35270;&#35282;&#65292;&#24378;&#35843;&#20102; AI&#12289;&#20998;&#26512;&#21644;&#23398;&#20064;&#36807;&#31243;&#20043;&#38388;&#38169;&#32508;&#22797;&#26434;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#25361;&#25112;&#20102;&#23558; AI &#35270;&#20026;&#38543;&#26426;&#24037;&#20855;&#30340;&#35266;&#24565;&#65292;&#24378;&#35843;&#20102; AI &#20316;&#20026;&#29702;&#35299;&#20154;&#31867;&#23398;&#20064;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#31181;&#29420;&#29305;&#30340;&#25945;&#32946;&#20013;&#20154;&#24037;&#26234;&#33021;&#30340;&#27010;&#24565;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#23398;&#20064;&#21644;&#25945;&#32946;&#20013;&#30340;&#22810;&#32500;&#35270;&#35282;&#65292;&#24378;&#35843;&#20154;&#24037;&#26234;&#33021;&#12289;&#20998;&#26512;&#21644;&#23398;&#20064;&#36807;&#31243;&#20043;&#38388;&#38169;&#32508;&#22797;&#26434;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#31508;&#32773;&#25361;&#25112;&#20102;&#23558;&#20154;&#24037;&#26234;&#33021;&#20165;&#35270;&#20026;&#38543;&#26426;&#24037;&#20855;&#30340;&#26222;&#36941;&#35266;&#24565;&#65292;&#20363;&#22914;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65292;&#20027;&#24352;&#37325;&#35270;&#23545;&#20154;&#24037;&#26234;&#33021;&#30340;&#26367;&#20195;&#27010;&#24565;&#12290;&#25991;&#31456;&#31361;&#20986;&#20102;&#20154;&#31867;&#26234;&#33021;&#19982;&#20154;&#24037;&#20449;&#24687;&#22788;&#29702;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;AI&#31639;&#27861;&#20013;&#22266;&#26377;&#30340;&#35748;&#30693;&#22810;&#26679;&#24615;&#65292;&#24182;&#25552;&#20986;AI&#20063;&#21487;&#20197;&#20316;&#20026;&#29702;&#35299;&#20154;&#31867;&#23398;&#20064;&#30340;&#24037;&#20855;&#12290;&#20174;&#23558;AI&#35270;&#20026;&#20154;&#31867;&#26234;&#33021;&#30340;&#31867;&#27604;&#30340;&#26089;&#26399;&#23398;&#20064;&#31185;&#23398;&#21644;&#25945;&#32946;&#20013;&#30340;AI&#30740;&#31350;&#24050;&#32463;&#20559;&#31163;&#36825;&#19968;&#35266;&#28857;&#65292;&#20419;&#20351;&#26377;&#24517;&#35201;&#37325;&#26032;&#28857;&#29123;&#36825;&#31181;&#32852;&#31995;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#29420;&#29305;&#30340;&#25945;&#32946;&#20013;&#20154;&#24037;&#26234;&#33021;&#30340;&#27010;&#24565;&#21270;&#65306;&#20154;&#31867;&#35748;&#30693;&#30340;&#22806;&#37096;&#21270;&#12289;&#20869;&#21270;AI&#27169;&#22411;&#20197;&#24433;&#21709;&#20154;&#31867;&#24605;&#32500;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16081v1 Announce Type: cross  Abstract: This paper presents a multi dimensional view of AI's role in learning and education, emphasizing the intricate interplay between AI, analytics, and the learning processes. Here, I challenge the prevalent narrow conceptualization of AI as stochastic tools, as exemplified in generative AI, and argue for the importance of alternative conceptualisations of AI. I highlight the differences between human intelligence and artificial information processing, the cognitive diversity inherent in AI algorithms, and posit that AI can also serve as an instrument for understanding human learning. Early learning sciences and AI in Education research, which saw AI as an analogy for human intelligence, have diverged from this perspective, prompting a need to rekindle this connection. The paper presents three unique conceptualizations of AI in education: the externalization of human cognition, the internalization of AI models to influence human thought pr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;ChatGPT&#25972;&#21512;&#21040;Python&#32534;&#31243;&#35838;&#31243;&#20013;&#23545;&#23398;&#20064;&#30340;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;&#23398;&#29983;&#23545;ChatGPT&#30340;&#31215;&#26497;&#24577;&#24230;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#20854;&#22312;&#22686;&#24378;&#32534;&#31243;&#25945;&#32946;&#20307;&#39564;&#20013;&#20316;&#29992;&#30340;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.15472</link><description>&lt;p&gt;
&#29992;ChatGPT&#22686;&#24378;&#32534;&#31243;&#25945;&#32946;&#65306;&#38024;&#23545;Python&#35838;&#31243;&#20013;&#23398;&#29983;&#24863;&#30693;&#21644;&#20114;&#21160;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Enhancing Programming Education with ChatGPT: A Case Study on Student Perceptions and Interactions in a Python Course
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15472
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;ChatGPT&#25972;&#21512;&#21040;Python&#32534;&#31243;&#35838;&#31243;&#20013;&#23545;&#23398;&#20064;&#30340;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;&#23398;&#29983;&#23545;ChatGPT&#30340;&#31215;&#26497;&#24577;&#24230;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#20854;&#22312;&#22686;&#24378;&#32534;&#31243;&#25945;&#32946;&#20307;&#39564;&#20013;&#20316;&#29992;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#20316;&#20026;&#19968;&#31181;&#25903;&#25345;&#24615;&#24037;&#20855;&#25972;&#21512;&#21040;&#25945;&#32946;&#20013;&#65292;&#29305;&#21035;&#26159;&#22312;&#32534;&#31243;&#35838;&#31243;&#20013;&#65292;&#36890;&#36807;&#25552;&#20379;&#35843;&#35797;&#12289;&#20195;&#30721;&#29983;&#25104;&#21644;&#35299;&#37322;&#26041;&#38754;&#30340;&#24110;&#21161;&#65292;&#35299;&#20915;&#20102;&#32534;&#31243;&#25945;&#32946;&#30340;&#29420;&#29305;&#25361;&#25112;&#12290;&#23613;&#31649;&#24050;&#26377;&#30740;&#31350;&#39564;&#35777;&#20102;ChatGPT&#30340;&#26377;&#25928;&#24615;&#65292;&#20294;&#20854;&#22312;&#22823;&#23398;&#32423;&#21035;&#30340;&#32534;&#31243;&#25945;&#32946;&#20013;&#30340;&#24212;&#29992;&#20197;&#21450;&#23398;&#29983;&#20114;&#21160;&#21644;&#35266;&#28857;&#30340;&#35814;&#32454;&#29702;&#35299;&#20173;&#26377;&#38480;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#20026;&#22823;&#19968;&#23398;&#29983;&#37327;&#36523;&#23450;&#21046;&#30340;Python&#32534;&#31243;&#35838;&#31243;&#20013;&#65292;&#22312;&#20843;&#21608;&#30340;&#26102;&#38388;&#20869;&#65292;ChatGPT&#23545;&#23398;&#20064;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#20998;&#26512;&#26469;&#33258;&#35843;&#26597;&#12289;&#24320;&#25918;&#24615;&#38382;&#39064;&#20197;&#21450;&#23398;&#29983;-ChatGPT&#23545;&#35805;&#25968;&#25454;&#30340;&#22238;&#24212;&#65292;&#25105;&#20204;&#26088;&#22312;&#20840;&#38754;&#20102;&#35299;ChatGPT&#30340;&#23454;&#29992;&#24615;&#65292;&#24182;&#30830;&#23450;&#23398;&#29983;&#25152;&#24863;&#30693;&#30340;&#20854;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#23398;&#29983;&#23545;ChatGPT&#26222;&#36941;&#25345;&#32943;&#23450;&#24577;&#24230;&#65292;&#24182;&#25552;&#20379;&#20102;&#26377;&#20851;&#20854;&#22312;&#22686;&#24378;&#32534;&#31243;&#25945;&#32946;&#20307;&#39564;&#20013;&#30340;&#20316;&#29992;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15472v1 Announce Type: cross  Abstract: The integration of ChatGPT as a supportive tool in education, notably in programming courses, addresses the unique challenges of programming education by providing assistance with debugging, code generation, and explanations. Despite existing research validating ChatGPT's effectiveness, its application in university-level programming education and a detailed understanding of student interactions and perspectives remain limited. This paper explores ChatGPT's impact on learning in a Python programming course tailored for first-year students over eight weeks. By analyzing responses from surveys, open-ended questions, and student-ChatGPT dialog data, we aim to provide a comprehensive view of ChatGPT's utility and identify both its advantages and limitations as perceived by students. Our study uncovers a generally positive reception toward ChatGPT and offers insights into its role in enhancing the programming education experience. These fin
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#27880;&#24847;&#21147;&#26426;&#21046;&#20248;&#21270;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#23588;&#20854;&#23545;&#20110;&#38750;STEM&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.14932</link><description>&lt;p&gt;
&#19987;&#27880;&#39537;&#21160;&#30340;&#25512;&#29702;:&#37322;&#25918;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Attention-Driven Reasoning: Unlocking the Potential of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14932
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27880;&#24847;&#21147;&#26426;&#21046;&#20248;&#21270;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#23588;&#20854;&#23545;&#20110;&#38750;STEM&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#30340;&#25512;&#29702;&#33021;&#21147;&#21644;&#22522;&#30784;&#26426;&#21046;&#20173;&#19981;&#20026;&#20154;&#25152;&#20102;&#35299;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#27880;&#24847;&#21147;&#26426;&#21046;&#20248;&#21270;&#26469;&#22686;&#24378;LLMs&#25512;&#29702;&#33021;&#21147;&#30340;&#26032;&#26041;&#27861;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#30001;&#38750;&#35821;&#20041;&#26631;&#35760;&#23548;&#33268;&#30340;&#27880;&#24847;&#21147;&#20998;&#24067;&#30340;&#20302;&#25928;&#29575;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#26469;&#37325;&#26032;&#24179;&#34913;&#20559;&#26012;&#20998;&#24067;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#25277;&#35937;&#26356;&#21152;&#24494;&#22937;&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25512;&#29702;&#33021;&#21147;&#24471;&#21040;&#20102;&#26174;&#30528;&#25913;&#36827;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#38750;STEM&#38382;&#39064;&#12290;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#27880;&#24847;&#21147;&#27169;&#24335;&#22312;LLMs&#25512;&#29702;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#36825;&#20123;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#20026;&#26356;&#24378;&#22823;&#21644;&#22810;&#21151;&#33021;&#30340;&#35821;&#35328;&#27169;&#22411;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14932v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have shown remarkable capabilities, but their reasoning abilities and underlying mechanisms remain poorly understood. We present a novel approach to enhance LLMs' reasoning through attention mechanism optimization, without additional training data. We identify inefficiencies in the attention distribution caused by non-semantic tokens and propose an algorithm to re-balance the skewed distribution, enabling the model to abstract more nuanced knowledge. Our experiments demonstrate significantly improved reasoning capabilities, particularly for non-STEM questions. We provide insights into the role of attention patterns in LLMs' reasoning and propose a method to enhance these abilities, paving the way for more powerful and versatile language models.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#33268;&#21147;&#20110;&#21457;&#23637;&#19968;&#20010;&#22312;&#35780;&#20272;&#23433;&#20840;&#20851;&#38190;&#33258;&#20027;&#23433;&#20840;&#24615;&#30340;&#37325;&#35201;&#24615;&#26041;&#38754;&#65292;&#22312;&#31934;&#24230;&#21644;&#21484;&#22238;&#29575;&#26041;&#38754;&#37117;&#34920;&#29616;&#20986;&#33394;&#30340;&#20851;&#38190;&#24615;&#39044;&#27979;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.13869</link><description>&lt;p&gt;
&#20934;&#30830;&#39044;&#27979;&#26234;&#33021;&#31995;&#32479;&#30340;&#23433;&#20840;&#20851;&#38190;&#31232;&#26377;&#20107;&#20214;&#27010;&#29575;
&lt;/p&gt;
&lt;p&gt;
Accurately Predicting Probabilities of Safety-Critical Rare Events for Intelligent Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13869
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#33268;&#21147;&#20110;&#21457;&#23637;&#19968;&#20010;&#22312;&#35780;&#20272;&#23433;&#20840;&#20851;&#38190;&#33258;&#20027;&#23433;&#20840;&#24615;&#30340;&#37325;&#35201;&#24615;&#26041;&#38754;&#65292;&#22312;&#31934;&#24230;&#21644;&#21484;&#22238;&#29575;&#26041;&#38754;&#37117;&#34920;&#29616;&#20986;&#33394;&#30340;&#20851;&#38190;&#24615;&#39044;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#31995;&#32479;&#36234;&#26469;&#36234;&#25104;&#20026;&#25105;&#20204;&#26085;&#24120;&#29983;&#27963;&#20013;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#28982;&#32780;&#32597;&#35265;&#30340;&#23433;&#20840;&#20851;&#38190;&#20107;&#20214;&#23545;&#23427;&#20204;&#30340;&#23454;&#38469;&#37096;&#32626;&#26500;&#25104;&#20102;&#37325;&#22823;&#28508;&#22312;&#23041;&#32961;&#12290;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#30340;&#20851;&#38190;&#22312;&#20110;&#20934;&#30830;&#39044;&#27979;&#22312;&#32473;&#23450;&#26102;&#38388;&#27493;&#38271;&#20869;&#20174;&#24403;&#21069;&#29366;&#24577;&#21457;&#29983;&#23433;&#20840;&#20851;&#38190;&#20107;&#20214;&#30340;&#27010;&#29575;&#65292;&#19968;&#20010;&#25105;&#20204;&#23450;&#20041;&#20026;&#8220;&#37325;&#35201;&#24615;&#8221;&#30340;&#25351;&#26631;&#12290;&#39044;&#27979;&#37325;&#35201;&#24615;&#30340;&#22797;&#26434;&#24615;&#28304;&#33258;&#20110;&#26497;&#31471;&#25968;&#25454;&#19981;&#24179;&#34913;&#65292;&#36825;&#26159;&#30001;&#39640;&#32500;&#21464;&#37327;&#20013;&#19982;&#32597;&#35265;&#20107;&#20214;&#30456;&#20851;&#32852;&#24341;&#36215;&#30340;&#19968;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#32597;&#35265;&#24615;&#35781;&#21650;&#12290;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#35201;&#20040;&#36807;&#20110;&#20445;&#23432;&#65292;&#35201;&#20040;&#23481;&#26131;&#24573;&#35270;&#23433;&#20840;&#20851;&#38190;&#20107;&#20214;&#65292;&#22240;&#27492;&#24456;&#38590;&#21516;&#26102;&#23454;&#29616;&#39640;&#31934;&#24230;&#21644;&#21484;&#22238;&#29575;&#65292;&#36825;&#20005;&#37325;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#36866;&#29992;&#24615;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#37325;&#35201;&#24615;&#39044;&#27979;&#27169;&#22411;&#65292;&#22312;&#35780;&#20272;&#23433;&#20840;&#20851;&#38190;&#33258;&#20027;&#23433;&#20840;&#24615;&#30340;&#37325;&#35201;&#24615;&#26041;&#38754;&#65292;&#22312;&#31934;&#24230;&#21644;&#21484;&#22238;&#29575;&#26041;&#38754;&#37117;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13869v1 Announce Type: cross  Abstract: Intelligent systems are increasingly integral to our daily lives, yet rare safety-critical events present significant latent threats to their practical deployment. Addressing this challenge hinges on accurately predicting the probability of safety-critical events occurring within a given time step from the current state, a metric we define as 'criticality'. The complexity of predicting criticality arises from the extreme data imbalance caused by rare events in high dimensional variables associated with the rare events, a challenge we refer to as the curse of rarity. Existing methods tend to be either overly conservative or prone to overlooking safety-critical events, thus struggling to achieve both high precision and recall rates, which severely limits their applicability. This study endeavors to develop a criticality prediction model that excels in both precision and recall rates for evaluating the criticality of safety-critical auton
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22240;&#23376;&#22270;&#20013;&#39640;&#25928;&#26816;&#27979;&#21487;&#20132;&#25442;&#22240;&#23376;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#22823;&#22823;&#20943;&#23569;&#35745;&#31639;&#24037;&#20316;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.10167</link><description>&lt;p&gt;
&#22240;&#23376;&#22270;&#20013;&#21487;&#20132;&#25442;&#22240;&#23376;&#30340;&#39640;&#25928;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Efficient Detection of Exchangeable Factors in Factor Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10167
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22240;&#23376;&#22270;&#20013;&#39640;&#25928;&#26816;&#27979;&#21487;&#20132;&#25442;&#22240;&#23376;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#22823;&#22823;&#20943;&#23569;&#35745;&#31639;&#24037;&#20316;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#23454;&#29616;&#23545;&#39046;&#22495;&#22823;&#23567;&#30340;&#21487;&#35745;&#31639;&#27010;&#29575;&#25512;&#26029;&#65292;&#25552;&#20986;&#20102;&#25552;&#21319;&#24335;&#27010;&#29575;&#25512;&#26029;&#65292;&#21033;&#29992;&#27010;&#29575;&#22270;&#27169;&#22411;&#20013;&#30340;&#23545;&#31216;&#24615;&#12290;&#28982;&#32780;&#65292;&#26816;&#26597;&#20004;&#20010;&#22240;&#23376;&#26159;&#21542;&#32534;&#30721;&#31561;&#25928;&#35821;&#20041;&#20174;&#32780;&#26159;&#21487;&#20132;&#25442;&#30340;&#65292;&#22312;&#35745;&#31639;&#19978;&#26159;&#26114;&#36149;&#30340;&#12290;&#26412;&#25991;&#39640;&#25928;&#35299;&#20915;&#20102;&#22312;&#22240;&#23376;&#22270;&#20013;&#26816;&#27979;&#21487;&#20132;&#25442;&#22240;&#23376;&#30340;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#24341;&#20837;&#20102;&#26816;&#27979;&#21487;&#20132;&#25442;&#22240;&#23376;&#65288;DEFT&#65289;&#31639;&#27861;&#65292;&#21487;&#20197;&#22823;&#22823;&#20943;&#23569;&#23454;&#36341;&#20013;&#26816;&#26597;&#20004;&#20010;&#22240;&#23376;&#26159;&#21542;&#21487;&#20132;&#25442;&#30340;&#35745;&#31639;&#24037;&#20316;&#37327;&#12290;&#25105;&#20204;&#35777;&#26126;DEFT&#26377;&#25928;&#22320;&#35782;&#21035;&#38480;&#21046;&#20197;&#22823;&#24133;&#20943;&#23569;&#25490;&#21015;&#25968;&#37327;&#65292;&#24182;&#22312;&#23454;&#35777;&#35780;&#20272;&#20013;&#39564;&#35777;&#20102;DEFT&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10167v1 Announce Type: new  Abstract: To allow for tractable probabilistic inference with respect to domain sizes, lifted probabilistic inference exploits symmetries in probabilistic graphical models. However, checking whether two factors encode equivalent semantics and hence are exchangeable is computationally expensive. In this paper, we efficiently solve the problem of detecting exchangeable factors in a factor graph. In particular, we introduce the detection of exchangeable factors (DEFT) algorithm, which allows us to drastically reduce the computational effort for checking whether two factors are exchangeable in practice. While previous approaches iterate all $O(n!)$ permutations of a factor's argument list in the worst case (where $n$ is the number of arguments of the factor), we prove that DEFT efficiently identifies restrictions to drastically reduce the number of permutations and validate the efficiency of DEFT in our empirical evaluation.
&lt;/p&gt;</description></item><item><title>Llama2&#27169;&#22411;&#22312;&#32763;&#35793;&#20013;&#34920;&#29616;&#20986;&#20934;&#30830;&#24230;&#39640;&#65292;&#37096;&#20998;&#26410;&#35265;&#35821;&#35328;&#38656;&#35201;&#26356;&#22823;&#35268;&#27169;&#30340;&#27169;&#22411;&#26469;&#25552;&#21319;&#32763;&#35793;&#36136;&#37327;&#65292;&#21478;&#22806;&#35821;&#35328;&#30340;&#21477;&#27861;&#30456;&#20284;&#24615;&#24182;&#38750;&#32763;&#35793;&#36136;&#37327;&#30340;&#20027;&#35201;&#22240;&#32032;&#65292;&#26576;&#20123;&#35821;&#35328;&#21363;&#20351;&#25968;&#25454;&#23569;&#20381;&#28982;&#34920;&#29616;&#20986;&#24378;&#30456;&#20851;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.13917</link><description>&lt;p&gt;
LLM&#32763;&#35793;&#20013;&#30340;&#35821;&#35328;&#29305;&#24449;&#21644;&#37325;&#35201;&#35821;&#35328;&#26159;&#20160;&#20040;&#65311;
&lt;/p&gt;
&lt;p&gt;
What Linguistic Features and Languages are Important in LLM Translation?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13917
&lt;/p&gt;
&lt;p&gt;
Llama2&#27169;&#22411;&#22312;&#32763;&#35793;&#20013;&#34920;&#29616;&#20986;&#20934;&#30830;&#24230;&#39640;&#65292;&#37096;&#20998;&#26410;&#35265;&#35821;&#35328;&#38656;&#35201;&#26356;&#22823;&#35268;&#27169;&#30340;&#27169;&#22411;&#26469;&#25552;&#21319;&#32763;&#35793;&#36136;&#37327;&#65292;&#21478;&#22806;&#35821;&#35328;&#30340;&#21477;&#27861;&#30456;&#20284;&#24615;&#24182;&#38750;&#32763;&#35793;&#36136;&#37327;&#30340;&#20027;&#35201;&#22240;&#32032;&#65292;&#26576;&#20123;&#35821;&#35328;&#21363;&#20351;&#25968;&#25454;&#23569;&#20381;&#28982;&#34920;&#29616;&#20986;&#24378;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv&#65306;2402.13917v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#39046;&#22495; &#25688;&#35201;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#20855;&#26377;&#24378;&#22823;&#33021;&#21147;&#65292;&#21253;&#25324;&#26426;&#22120;&#32763;&#35793;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#37325;&#28857;&#22312;&#20110;&#35780;&#20272;Llama2&#30340;&#26426;&#22120;&#32763;&#35793;&#33021;&#21147;&#65292;&#24182;&#25506;&#32034;&#32763;&#35793;&#22914;&#20309;&#21462;&#20915;&#20110;&#20854;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#35821;&#35328;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;7B Llama2&#27169;&#22411;&#23545;&#20854;&#25152;&#35265;&#30340;&#25152;&#26377;&#35821;&#35328;&#37117;&#21487;&#20197;&#33719;&#24471;&#36229;&#36807;10&#30340;BLEU&#20998;&#25968;&#65292;&#20294;&#24182;&#38750;&#24635;&#26159;&#23545;&#20854;&#26410;&#35265;&#30340;&#35821;&#35328;&#12290;&#23545;&#20110;&#36825;&#20123;&#26410;&#35265;&#35821;&#35328;&#65292;&#19982;&#20351;&#29992;&#32842;&#22825;&#29256;&#26412;&#25110;&#28155;&#21152;&#23569;&#37327;&#25968;&#25454;&#30456;&#27604;&#65292;&#22312;&#27169;&#22411;&#35268;&#27169;&#19978;&#35266;&#23519;&#21040;&#30340;&#26368;&#22823;&#25910;&#30410;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#35821;&#35328;&#36317;&#31163;&#20998;&#26512;&#26174;&#31034;&#65292;&#21477;&#27861;&#30456;&#20284;&#24615;&#24182;&#38750;&#22987;&#32456;&#26159;&#20915;&#23450;&#32763;&#35793;&#36136;&#37327;&#30340;&#20027;&#35201;&#35821;&#35328;&#22240;&#32032;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#29305;&#23450;&#24773;&#20917;&#19979;&#65292;&#19968;&#20123;&#35821;&#35328;&#65292;&#23613;&#31649;&#35757;&#32451;&#25968;&#25454;&#26126;&#26174;&#23569;&#20110;&#33521;&#35821;&#65292;&#21364;&#34920;&#29616;&#20986;&#19982;&#33521;&#35821;&#21487;&#27604;&#30340;&#24378;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#30340;&#21457;&#29616;&#20026;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13917v1 Announce Type: cross  Abstract: Large Language Models (LLMs) demonstrate strong capability across multiple tasks, including machine translation. Our study focuses on evaluating Llama2's machine translation capabilities and exploring how translation depends on languages in its training data. Our experiments show that the 7B Llama2 model yields above 10 BLEU score for all languages it has seen, but not always for languages it has not seen. Most gains for those unseen languages are observed the most with the model scale compared to using chat versions or adding shot count. Furthermore, our linguistic distance analysis reveals that syntactic similarity is not always the primary linguistic factor in determining translation quality. Interestingly, we discovered that under specific circumstances, some languages, despite having significantly less training data than English, exhibit strong correlations comparable to English. Our discoveries here give new perspectives for the 
&lt;/p&gt;</description></item><item><title>SpikeNAS&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#20869;&#23384;&#24863;&#30693;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26694;&#26550;&#65292;&#26088;&#22312;&#24110;&#21161;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#31995;&#32479;&#24555;&#36895;&#25214;&#21040;&#22312;&#32473;&#23450;&#20869;&#23384;&#39044;&#31639;&#19979;&#39640;&#20934;&#30830;&#24615;&#30340;&#36866;&#24403;&#26550;&#26500;&#12290;</title><link>https://arxiv.org/abs/2402.11322</link><description>&lt;p&gt;
SpikeNAS: &#19968;&#31181;&#38754;&#21521;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#31995;&#32479;&#30340;&#24555;&#36895;&#20869;&#23384;&#24863;&#30693;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
SpikeNAS: A Fast Memory-Aware Neural Architecture Search Framework for Spiking Neural Network Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11322
&lt;/p&gt;
&lt;p&gt;
SpikeNAS&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#20869;&#23384;&#24863;&#30693;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26694;&#26550;&#65292;&#26088;&#22312;&#24110;&#21161;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#31995;&#32479;&#24555;&#36895;&#25214;&#21040;&#22312;&#32473;&#23450;&#20869;&#23384;&#39044;&#31639;&#19979;&#39640;&#20934;&#30830;&#24615;&#30340;&#36866;&#24403;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#20026;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#25552;&#20379;&#20102;&#23454;&#29616;&#36229;&#20302;&#21151;&#32791;&#35745;&#31639;&#30340;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#30446;&#21069;&#65292;&#22823;&#22810;&#25968;SNN&#26550;&#26500;&#37117;&#28304;&#33258;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65292;&#20854;&#31070;&#32463;&#20803;&#30340;&#26550;&#26500;&#21644;&#25805;&#20316;&#19982;SNN&#19981;&#21516;&#65292;&#25110;&#32773;&#22312;&#19981;&#32771;&#34385;&#26469;&#33258;&#24213;&#23618;&#22788;&#29702;&#30828;&#20214;&#30340;&#20869;&#23384;&#39044;&#31639;&#30340;&#24773;&#20917;&#19979;&#24320;&#21457;&#12290;&#36825;&#20123;&#38480;&#21046;&#38459;&#30861;&#20102;SNN&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#20805;&#20998;&#21457;&#25381;&#28508;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SpikeNAS&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#20869;&#23384;&#24863;&#30693;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#26694;&#26550;&#65292;&#21487;&#22312;&#32473;&#23450;&#20869;&#23384;&#39044;&#31639;&#19979;&#24555;&#36895;&#25214;&#21040;&#19968;&#20010;&#20855;&#26377;&#39640;&#20934;&#30830;&#24615;&#30340;&#36866;&#24403;SNN&#26550;&#26500;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#30340;SpikeNAS&#37319;&#29992;&#20102;&#20960;&#20010;&#20851;&#38190;&#27493;&#39588;&#65306;&#20998;&#26512;&#32593;&#32476;&#25805;&#20316;&#23545;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#65292;&#22686;&#24378;&#32593;&#32476;&#26550;&#26500;&#20197;&#25552;&#39640;&#23398;&#20064;&#36136;&#37327;&#65292;&#24182;&#24320;&#21457;&#24555;&#36895;&#20869;&#23384;&#24863;&#30693;&#25628;&#32034;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11322v1 Announce Type: cross  Abstract: Spiking Neural Networks (SNNs) offer a promising solution to achieve ultra low-power/energy computation for solving machine learning tasks. Currently, most of the SNN architectures are derived from Artificial Neural Networks whose neurons' architectures and operations are different from SNNs, or developed without considering memory budgets from the underlying processing hardware. These limitations hinder the SNNs from reaching their full potential in accuracy and efficiency. Towards this, we propose SpikeNAS, a novel memory-aware neural architecture search (NAS) framework for SNNs that can quickly find an appropriate SNN architecture with high accuracy under the given memory budgets. To do this, our SpikeNAS employs several key steps: analyzing the impacts of network operations on the accuracy, enhancing the network architecture to improve the learning quality, and developing a fast memory-aware search algorithm. The experimental resul
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#29983;&#25104;&#27169;&#22411;&#35757;&#32451;&#26102;&#21487;&#33021;&#20986;&#29616;&#30340;&#33258;&#25105;&#28040;&#32791;&#24490;&#29615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24341;&#20837;&#29702;&#24819;&#30340;&#20462;&#27491;&#20989;&#25968;&#26469;&#31283;&#23450;&#35757;&#32451;&#30340;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#33258;&#25105;&#20462;&#27491;&#20989;&#25968;&#26469;&#36817;&#20284;&#29702;&#24819;&#30340;&#20462;&#27491;&#20989;&#25968;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.07087</link><description>&lt;p&gt;
&#33258;&#25105;&#32416;&#27491;&#33258;&#25105;&#28040;&#32791;&#24490;&#29615;&#29992;&#20110;&#29983;&#25104;&#27169;&#22411;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Self-Correcting Self-Consuming Loops for Generative Model Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07087
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#29983;&#25104;&#27169;&#22411;&#35757;&#32451;&#26102;&#21487;&#33021;&#20986;&#29616;&#30340;&#33258;&#25105;&#28040;&#32791;&#24490;&#29615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24341;&#20837;&#29702;&#24819;&#30340;&#20462;&#27491;&#20989;&#25968;&#26469;&#31283;&#23450;&#35757;&#32451;&#30340;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#33258;&#25105;&#20462;&#27491;&#20989;&#25968;&#26469;&#36817;&#20284;&#29702;&#24819;&#30340;&#20462;&#27491;&#20989;&#25968;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21512;&#25104;&#25968;&#25454;&#22312;&#20114;&#32852;&#32593;&#19978;&#30340;&#36136;&#37327;&#36234;&#26469;&#36234;&#39640;&#20197;&#21450;&#25968;&#37327;&#19981;&#26029;&#22686;&#21152;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36234;&#26469;&#36234;&#22810;&#22320;&#22312;&#20154;&#24037;&#21644;&#26426;&#22120;&#29983;&#25104;&#30340;&#25968;&#25454;&#30340;&#28151;&#21512;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#23613;&#31649;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#34920;&#24449;&#23398;&#20064;&#30340;&#25104;&#21151;&#26696;&#20363;&#26377;&#24456;&#22810;&#65292;&#20294;&#26159;&#22312;&#29983;&#25104;&#27169;&#22411;&#35757;&#32451;&#20013;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#20250;&#20135;&#29983;"&#33258;&#25105;&#28040;&#32791;&#24490;&#29615;"&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#35757;&#32451;&#19981;&#31283;&#23450;&#29978;&#33267;&#23849;&#28291;&#65292;&#38500;&#38750;&#28385;&#36275;&#26576;&#20123;&#26465;&#20214;&#12290;&#25105;&#20204;&#30340;&#35770;&#25991;&#26088;&#22312;&#31283;&#23450;&#33258;&#25105;&#28040;&#32791;&#30340;&#29983;&#25104;&#27169;&#22411;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#29702;&#24819;&#30340;&#20462;&#27491;&#20989;&#25968;&#65292;&#23558;&#25968;&#25454;&#28857;&#26144;&#23556;&#20026;&#26356;&#26377;&#21487;&#33021;&#26469;&#33258;&#30495;&#23454;&#25968;&#25454;&#20998;&#24067;&#30340;&#26679;&#26412;&#65292;&#21487;&#20197;&#20351;&#33258;&#25105;&#28040;&#32791;&#24490;&#29615;&#30340;&#31283;&#23450;&#24615;&#21576;&#25351;&#25968;&#22686;&#21152;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#25105;&#20462;&#27491;&#20989;&#25968;&#65292;&#23427;&#20381;&#36182;&#20110;&#19987;&#23478;&#30693;&#35782;&#65288;&#20363;&#22914;&#65292;&#32534;&#31243;&#22312;&#27169;&#25311;&#22120;&#20013;&#30340;&#29289;&#29702;&#23450;&#24459;&#65289;&#65292;&#24182;&#19988;&#26088;&#22312;&#33258;&#21160;&#19988;&#22823;&#35268;&#27169;&#22320;&#36817;&#20284;&#29702;&#24819;&#30340;&#20462;&#27491;&#20989;&#25968;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#33258;&#25105;&#32416;&#27491;&#33258;&#25105;&#28040;&#32791;&#24490;&#29615;&#22312;&#29983;&#25104;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As synthetic data becomes higher quality and proliferates on the internet, machine learning models are increasingly trained on a mix of human- and machine-generated data. Despite the successful stories of using synthetic data for representation learning, using synthetic data for generative model training creates "self-consuming loops" which may lead to training instability or even collapse, unless certain conditions are met. Our paper aims to stabilize self-consuming generative model training. Our theoretical results demonstrate that by introducing an idealized correction function, which maps a data point to be more likely under the true data distribution, self-consuming loops can be made exponentially more stable. We then propose self-correction functions, which rely on expert knowledge (e.g. the laws of physics programmed in a simulator), and aim to approximate the idealized corrector automatically and at scale. We empirically validate the effectiveness of self-correcting self-consum
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32454;&#31890;&#24230;&#22797;&#26434;&#24615;&#20998;&#26512;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#29616;&#20195;Hopfield&#27169;&#22411;&#30340;&#35760;&#24518;&#26816;&#32034;&#35745;&#31639;&#38480;&#21046;&#65292;&#21457;&#29616;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#24335;&#33539;&#25968;&#30340;&#30456;&#21464;&#34892;&#20026;&#65292;&#24182;&#19988;&#24314;&#31435;&#20102;&#26377;&#25928;&#21464;&#20307;&#30340;&#19978;&#30028;&#26465;&#20214;&#12290;&#20351;&#29992;&#20302;&#31209;&#36924;&#36817;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26377;&#25928;&#26500;&#36896;&#30340;&#31034;&#20363;&#65292;&#21516;&#26102;&#35777;&#26126;&#20102;&#35745;&#31639;&#26102;&#38388;&#19979;&#30028;&#12289;&#35760;&#24518;&#26816;&#32034;&#35823;&#24046;&#30028;&#21644;&#25351;&#25968;&#35760;&#24518;&#23481;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.04520</link><description>&lt;p&gt;
&#20851;&#20110;&#29616;&#20195;Hopfield&#27169;&#22411;&#35745;&#31639;&#38480;&#21046;&#30340;&#19968;&#20010;&#32454;&#31890;&#24230;&#22797;&#26434;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
On Computational Limits of Modern Hopfield Models: A Fine-Grained Complexity Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04520
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32454;&#31890;&#24230;&#22797;&#26434;&#24615;&#20998;&#26512;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#29616;&#20195;Hopfield&#27169;&#22411;&#30340;&#35760;&#24518;&#26816;&#32034;&#35745;&#31639;&#38480;&#21046;&#65292;&#21457;&#29616;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#24335;&#33539;&#25968;&#30340;&#30456;&#21464;&#34892;&#20026;&#65292;&#24182;&#19988;&#24314;&#31435;&#20102;&#26377;&#25928;&#21464;&#20307;&#30340;&#19978;&#30028;&#26465;&#20214;&#12290;&#20351;&#29992;&#20302;&#31209;&#36924;&#36817;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26377;&#25928;&#26500;&#36896;&#30340;&#31034;&#20363;&#65292;&#21516;&#26102;&#35777;&#26126;&#20102;&#35745;&#31639;&#26102;&#38388;&#19979;&#30028;&#12289;&#35760;&#24518;&#26816;&#32034;&#35823;&#24046;&#30028;&#21644;&#25351;&#25968;&#35760;&#24518;&#23481;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20174;&#32454;&#31890;&#24230;&#22797;&#26434;&#24615;&#20998;&#26512;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#29616;&#20195;Hopfield&#27169;&#22411;&#30340;&#35760;&#24518;&#26816;&#32034;&#21160;&#21147;&#23398;&#30340;&#35745;&#31639;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#22522;&#20110;&#27169;&#24335;&#30340;&#33539;&#25968;&#23545;&#25152;&#26377;&#21487;&#33021;&#30340;&#29616;&#20195;Hopfield&#27169;&#22411;&#30340;&#25928;&#29575;&#36827;&#34892;&#30456;&#21464;&#34892;&#20026;&#30340;&#21051;&#30011;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#23545;&#36755;&#20837;&#26597;&#35810;&#27169;&#24335;&#21644;&#35760;&#24518;&#27169;&#24335;&#30340;&#33539;&#25968;&#30340;&#19978;&#30028;&#26631;&#20934;&#12290;&#20165;&#22312;&#36825;&#20010;&#26631;&#20934;&#20043;&#19979;&#65292;&#20551;&#35774;&#28385;&#36275;Strong Exponential Time Hypothesis (SETH)&#65292;&#23384;&#22312;&#23376;&#20108;&#27425;&#65288;&#39640;&#25928;&#65289;&#21464;&#20307;&#30340;&#29616;&#20195;Hopfield&#27169;&#22411;&#12290;&#20026;&#20102;&#23637;&#31034;&#25105;&#20204;&#30340;&#29702;&#35770;&#65292;&#24403;&#26377;&#25928;&#26631;&#20934;&#25104;&#31435;&#26102;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#29616;&#20195;Hopfield&#27169;&#22411;&#20351;&#29992;&#20302;&#31209;&#36924;&#36817;&#30340;&#26377;&#25928;&#26500;&#36896;&#30340;&#27491;&#24335;&#31034;&#20363;&#12290;&#36825;&#21253;&#25324;&#19968;&#20010;&#35745;&#31639;&#26102;&#38388;&#30340;&#19979;&#30028;&#23548;&#20986;&#65292;&#19982;$\Max\{$&#23384;&#20648;&#30340;&#35760;&#24518;&#27169;&#24335;&#25968;&#37327;&#65292;&#36755;&#20837;&#26597;&#35810;&#24207;&#21015;&#30340;&#38271;&#24230;$\}$&#32447;&#24615;&#32553;&#25918;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35760;&#24518;&#26816;&#32034;&#35823;&#24046;&#30028;&#21644;&#25351;&#25968;&#35760;&#24518;&#23481;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the computational limits of the memory retrieval dynamics of modern Hopfield models from the fine-grained complexity analysis. Our key contribution is the characterization of a phase transition behavior in the efficiency of all possible modern Hopfield models based on the norm of patterns. Specifically, we establish an upper bound criterion for the norm of input query patterns and memory patterns. Only below this criterion, sub-quadratic (efficient) variants of the modern Hopfield model exist, assuming the Strong Exponential Time Hypothesis (SETH). To showcase our theory, we provide a formal example of efficient constructions of modern Hopfield models using low-rank approximation when the efficient criterion holds. This includes a derivation of a lower bound on the computational time, scaling linearly with $\Max\{$# of stored memory patterns, length of input query sequence$\}$. In addition, we prove its memory retrieval error bound and exponential memory capacity.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36873;&#25321;&#23545;&#35937;&#26102;&#30340;&#20559;&#35265;&#65292;&#21457;&#29616;&#20559;&#35265;&#32467;&#26500;&#20381;&#36182;&#20110;&#27169;&#22411;&#65292;&#23545;&#35937;&#31867;&#22411;&#35843;&#33410;&#20102;&#20559;&#35265;&#30340;&#24433;&#21709;&#31243;&#24230;&#65292;&#23548;&#33268;&#21015;&#34920;&#20013;&#30340;&#31532;&#19968;&#20010;&#23545;&#35937;&#22312;&#36755;&#20986;&#20013;&#34987;&#36807;&#24230;&#21576;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.01740</link><description>&lt;p&gt;
&#22312;&#35748;&#30693;&#36127;&#33655;&#19979;&#30340;&#34917;&#20607;&#24615;&#20559;&#35265;&#65306;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#36873;&#25321;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Compensatory Biases Under Cognitive Load: Reducing Selection Bias in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01740
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36873;&#25321;&#23545;&#35937;&#26102;&#30340;&#20559;&#35265;&#65292;&#21457;&#29616;&#20559;&#35265;&#32467;&#26500;&#20381;&#36182;&#20110;&#27169;&#22411;&#65292;&#23545;&#35937;&#31867;&#22411;&#35843;&#33410;&#20102;&#20559;&#35265;&#30340;&#24433;&#21709;&#31243;&#24230;&#65292;&#23548;&#33268;&#21015;&#34920;&#20013;&#30340;&#31532;&#19968;&#20010;&#23545;&#35937;&#22312;&#36755;&#20986;&#20013;&#34987;&#36807;&#24230;&#21576;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;gpt-3.5-turbo&#21644;claude-instant-1.2&#22312;&#35299;&#37322;&#21644;&#25191;&#34892;&#35821;&#20041;&#20219;&#21153;&#26041;&#38754;&#21464;&#24471;&#38750;&#24120;&#37325;&#35201;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#20123;&#27169;&#22411;&#22266;&#26377;&#30340;&#20559;&#35265;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#35748;&#30693;&#20559;&#35265;&#65292;&#20250;&#23545;&#23427;&#20204;&#30340;&#24615;&#33021;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#20854;&#20013;&#19968;&#20010;&#21463;&#21040;&#24433;&#21709;&#26368;&#22823;&#30340;&#26159;&#20174;&#21015;&#34920;&#20013;&#36827;&#34892;&#23545;&#35937;&#36873;&#25321;&#65292;&#36825;&#26159;&#25968;&#23383;&#23548;&#33322;&#21644;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#22522;&#26412;&#25805;&#20316;&#12290;&#26412;&#30740;&#31350;&#37325;&#28857;&#26816;&#26597;&#36825;&#20123;&#20559;&#35265;&#65292;&#24182;&#37327;&#21270;&#20854;&#23545;&#20195;&#34920;&#24615;&#21015;&#34920;&#36873;&#25321;&#20219;&#21153;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#36827;&#34892;&#19968;&#31995;&#21015;&#25511;&#21046;&#23454;&#39564;&#65292;&#25105;&#20204;&#25805;&#32437;&#20102;&#28201;&#24230;&#12289;&#21015;&#34920;&#38271;&#24230;&#12289;&#23545;&#35937;&#36523;&#20221;&#12289;&#23545;&#35937;&#31867;&#22411;&#12289;&#25552;&#31034;&#22797;&#26434;&#24230;&#21644;&#27169;&#22411;&#65292;&#20197;&#25506;&#32034;&#36825;&#20123;&#20559;&#35265;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#23396;&#31435;&#21644;&#27979;&#37327;&#36825;&#20123;&#20559;&#35265;&#23545;&#36873;&#25321;&#34892;&#20026;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20559;&#35265;&#32467;&#26500;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#27169;&#22411;&#65292;&#32780;&#23545;&#35937;&#31867;&#22411;&#35843;&#33410;&#20102;&#20559;&#35265;&#24433;&#21709;&#30340;&#31243;&#24230;&#12290;&#30001;&#20110;&#23384;&#22312;&#36739;&#24378;&#30340;&#21021;&#29616;&#25928;&#24212;&#65292;&#21015;&#34920;&#20013;&#30340;&#31532;&#19968;&#20010;&#23545;&#35937;&#20250;&#22312;&#36755;&#20986;&#20013;&#34987;&#36807;&#24230;&#21576;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) like gpt-3.5-turbo and claude-instant-1.2 have become instrumental in interpreting and executing semantic-based tasks. Unfortunately, these models' inherent biases, akin to human cognitive biases, adversely affect their performance. Particularly affected is object selection from lists; a fundamental operation in digital navigation and decision-making. This research critically examines these biases and quantifies the effects on a representative list selection task. To explore these biases, we conducted a series of controlled experiments, manipulating temperature, list length, object identity, object type, prompt complexity, and model. This enabled us to isolate and measure the influence of the biases on selection behavior. Our findings show that bias structure is strongly dependent on the model, with object type modulating the magnitude of the effect. With a strong primacy effect, causing the first objects in a list to be disproprotionately represented in ou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#24320;&#21457;&#29992;&#20110;&#21508;&#31181;&#22522;&#20110;&#34920;&#26684;&#20219;&#21153;&#30340;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#26500;&#24314;&#26032;&#25968;&#25454;&#38598;TableInstruct&#21644;&#24320;&#21457;&#31532;&#19968;&#20010;&#38754;&#21521;&#34920;&#26684;&#30340;&#24320;&#28304;&#36890;&#29992;&#27169;&#22411;TableLlama&#65292;&#22312;&#34920;&#29616;&#26041;&#38754;&#21462;&#24471;&#20102;&#21487;&#27604;&#25110;&#26356;&#22909;&#30340;&#25104;&#26524;&#12290;</title><link>https://arxiv.org/abs/2311.09206</link><description>&lt;p&gt;
TableLlama&#65306;&#38754;&#21521;&#34920;&#26684;&#30340;&#24320;&#25918;&#22823;&#22411;&#36890;&#29992;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
TableLlama: Towards Open Large Generalist Models for Tables
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09206
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#24320;&#21457;&#29992;&#20110;&#21508;&#31181;&#22522;&#20110;&#34920;&#26684;&#20219;&#21153;&#30340;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#26500;&#24314;&#26032;&#25968;&#25454;&#38598;TableInstruct&#21644;&#24320;&#21457;&#31532;&#19968;&#20010;&#38754;&#21521;&#34920;&#26684;&#30340;&#24320;&#28304;&#36890;&#29992;&#27169;&#22411;TableLlama&#65292;&#22312;&#34920;&#29616;&#26041;&#38754;&#21462;&#24471;&#20102;&#21487;&#27604;&#25110;&#26356;&#22909;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#32467;&#26500;&#21270;&#34920;&#26684;&#26159;&#26080;&#22788;&#19981;&#22312;&#30340;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#23545;&#34920;&#26684;&#36827;&#34892;&#39044;&#35757;&#32451;&#25110;&#29305;&#27530;&#30340;&#27169;&#22411;&#26550;&#26500;&#35774;&#35745;&#65292;&#21463;&#38480;&#20110;&#29305;&#23450;&#30340;&#34920;&#26684;&#31867;&#22411;&#65292;&#25110;&#23545;&#34920;&#26684;&#21644;&#20219;&#21153;&#26377;&#31616;&#21270;&#30340;&#20551;&#35774;&#12290;&#26412;&#25991;&#26088;&#22312;&#24320;&#21457;&#24320;&#28304;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#20316;&#20026;&#21508;&#31181;&#22522;&#20110;&#34920;&#26684;&#20219;&#21153;&#30340;&#36890;&#29992;&#24037;&#20855;&#30340;&#31532;&#19968;&#27493;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#21508;&#31181;&#30495;&#23454;&#34920;&#26684;&#21644;&#20219;&#21153;&#30340;&#26032;&#25968;&#25454;&#38598;TableInstruct&#65292;&#20197;&#29992;&#20110;&#25351;&#20196;&#35843;&#25972;&#21644;&#35780;&#20272;LLMs&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#21033;&#29992;LongLoRA&#23545;Llama 2 (7B)&#36827;&#34892;&#24494;&#35843;&#65292;&#24320;&#21457;&#20102;&#31532;&#19968;&#20010;&#38754;&#21521;&#34920;&#26684;&#30340;&#24320;&#28304;&#36890;&#29992;&#27169;&#22411;TableLlama&#65292;&#20197;&#24212;&#23545;&#38271;&#19978;&#19979;&#25991;&#25361;&#25112;&#12290;&#25105;&#20204;&#22312;&#21516;&#39046;&#22495;&#21644;&#36328;&#39046;&#22495;&#29615;&#22659;&#19979;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#22312;8&#20010;&#21516;&#39046;&#22495;&#20219;&#21153;&#20013;&#30340;7&#20010;&#20219;&#21153;&#20013;&#65292;TableLlama&#22312;&#24615;&#33021;&#19978;&#23454;&#29616;&#20102;&#19982;SOT&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09206v2 Announce Type: replace  Abstract: Semi-structured tables are ubiquitous. There has been a variety of tasks that aim to automatically interpret, augment, and query tables. Current methods often require pretraining on tables or special model architecture design, are restricted to specific table types, or have simplifying assumptions about tables and tasks. This paper makes the first step towards developing open-source large language models (LLMs) as generalists for a diversity of table-based tasks. Towards that end, we construct TableInstruct, a new dataset with a variety of realistic tables and tasks, for instruction tuning and evaluating LLMs. We further develop the first open-source generalist model for tables, TableLlama, by fine-tuning Llama 2 (7B) with LongLoRA to address the long context challenge. We experiment under both in-domain setting and out-of-domain setting. On 7 out of 8 in-domain tasks, TableLlama achieves comparable or better performance than the SOT
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#32467;&#21512;&#32467;&#26500;&#30340;&#25552;&#31034;&#24037;&#31243;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#24615;&#33021;&#26041;&#38754;&#30340;&#21069;&#26223;&#65292;&#36890;&#36807;&#24605;&#32500;&#38142;&#12289;&#24605;&#32500;&#26641;&#25110;&#24605;&#32500;&#22270;&#30340;&#35774;&#35745;&#26469;&#24341;&#23548;&#25972;&#20307;&#25512;&#29702;&#36807;&#31243;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#20363;&#65292;&#36825;&#31181;&#33539;&#24335;&#26174;&#33879;&#22686;&#24378;&#20102;&#27169;&#22411;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#36890;&#29992;&#34013;&#22270;&#65292;&#20026;&#26410;&#26469;&#30340;&#21457;&#23637;&#38138;&#24179;&#36947;&#36335;&#12290;</title><link>http://arxiv.org/abs/2401.14295</link><description>&lt;p&gt;
&#25512;&#29702;&#30340;&#25299;&#25169;&#23398;&#65306;&#25581;&#31192;&#24605;&#32500;&#38142;&#12289;&#26641;&#21644;&#22270;
&lt;/p&gt;
&lt;p&gt;
Topologies of Reasoning: Demystifying Chains, Trees, and Graphs of Thoughts. (arXiv:2401.14295v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14295
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#32467;&#21512;&#32467;&#26500;&#30340;&#25552;&#31034;&#24037;&#31243;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#24615;&#33021;&#26041;&#38754;&#30340;&#21069;&#26223;&#65292;&#36890;&#36807;&#24605;&#32500;&#38142;&#12289;&#24605;&#32500;&#26641;&#25110;&#24605;&#32500;&#22270;&#30340;&#35774;&#35745;&#26469;&#24341;&#23548;&#25972;&#20307;&#25512;&#29702;&#36807;&#31243;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#20363;&#65292;&#36825;&#31181;&#33539;&#24335;&#26174;&#33879;&#22686;&#24378;&#20102;&#27169;&#22411;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#36890;&#29992;&#34013;&#22270;&#65292;&#20026;&#26410;&#26469;&#30340;&#21457;&#23637;&#38138;&#24179;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#29305;&#21035;&#26159;&#22312;&#36890;&#36807;&#21019;&#26032;&#30340;&#25552;&#31034;&#25216;&#26415;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24615;&#33021;&#26041;&#38754;&#12290;&#20854;&#20013;&#65292;&#19982;&#32467;&#26500;&#30456;&#32467;&#21512;&#30340;&#25552;&#31034;&#24037;&#31243;&#34987;&#35270;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#33539;&#24335;&#65292;&#20854;&#35774;&#35745;&#22914;&#24605;&#32500;&#38142;&#12289;&#24605;&#32500;&#26641;&#25110;&#24605;&#32500;&#22270;&#31561;&#65292;&#36890;&#36807;&#32467;&#26500;&#25351;&#23548;&#25972;&#20307;LLM&#25512;&#29702;&#36807;&#31243;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#20363;&#30340;&#35828;&#26126;&#65292;&#36825;&#31181;&#33539;&#24335;&#26174;&#33879;&#22686;&#24378;&#20102;LLM&#22312;&#36923;&#36753;&#25110;&#25968;&#23398;&#25512;&#29702;&#12289;&#35268;&#21010;&#25110;&#21019;&#36896;&#24615;&#20889;&#20316;&#31561;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#26041;&#20415;&#29702;&#35299;&#36825;&#20010;&#19981;&#26029;&#21457;&#23637;&#30340;&#39046;&#22495;&#24182;&#20026;&#26410;&#26469;&#30340;&#21457;&#23637;&#38138;&#24179;&#36947;&#36335;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;LLM&#25512;&#29702;&#26041;&#26696;&#30340;&#36890;&#29992;&#34013;&#22270;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23545;&#25552;&#31034;&#25191;&#34892;&#27969;&#31243;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#28548;&#28165;&#24182;&#26126;&#30830;&#23450;&#20041;&#20102;&#19981;&#21516;&#30340;&#27010;&#24565;&#12290;&#28982;&#21518;&#25105;&#20204;&#24314;&#31435;&#31532;&#19968;&#20010;&#20998;&#31867;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
The field of natural language processing (NLP) has witnessed significant progress in recent years, with a notable focus on improving large language models' (LLM) performance through innovative prompting techniques. Among these, prompt engineering coupled with structures has emerged as a promising paradigm, with designs such as Chain-of-Thought, Tree of Thoughts, or Graph of Thoughts, in which the overall LLM reasoning is guided by a structure such as a graph. As illustrated with numerous examples, this paradigm significantly enhances the LLM's capability to solve numerous tasks, ranging from logical or mathematical reasoning to planning or creative writing. To facilitate the understanding of this growing field and pave the way for future developments, we devise a general blueprint for effective and efficient LLM reasoning schemes. For this, we conduct an in-depth analysis of the prompt execution pipeline, clarifying and clearly defining different concepts. We then build the first taxon
&lt;/p&gt;</description></item><item><title>&#24352;&#37327;&#32593;&#32476;&#21487;&#20197;&#24110;&#21161;&#21457;&#23637;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#20016;&#23500;&#30340;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#32593;&#32476;&#23433;&#20840;&#20013;&#65292;&#25105;&#20204;&#30340;&#26080;&#30417;&#30563;&#32858;&#31867;&#31639;&#27861;&#22522;&#20110;&#30697;&#38453;&#20056;&#31215;&#29366;&#24577;&#65292;&#22312;&#24615;&#33021;&#19978;&#19982;&#20256;&#32479;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30456;&#23218;&#32654;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#33021;&#25552;&#21462;&#29305;&#24449;&#27010;&#29575;&#12289;&#29109;&#21644;&#20114;&#20449;&#24687;&#65292;&#25552;&#20379;&#20102;&#20998;&#31867;&#24322;&#24120;&#30340;&#24341;&#20154;&#20837;&#32988;&#30340;&#21465;&#36848;&#65292;&#24182;&#23454;&#29616;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2401.00867</link><description>&lt;p&gt;
&#24352;&#37327;&#32593;&#32476;&#22312;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#20013;&#22312;&#32593;&#32476;&#23433;&#20840;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Tensor Networks for Explainable Machine Learning in Cybersecurity. (arXiv:2401.00867v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00867
&lt;/p&gt;
&lt;p&gt;
&#24352;&#37327;&#32593;&#32476;&#21487;&#20197;&#24110;&#21161;&#21457;&#23637;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#20016;&#23500;&#30340;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#32593;&#32476;&#23433;&#20840;&#20013;&#65292;&#25105;&#20204;&#30340;&#26080;&#30417;&#30563;&#32858;&#31867;&#31639;&#27861;&#22522;&#20110;&#30697;&#38453;&#20056;&#31215;&#29366;&#24577;&#65292;&#22312;&#24615;&#33021;&#19978;&#19982;&#20256;&#32479;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30456;&#23218;&#32654;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#33021;&#25552;&#21462;&#29305;&#24449;&#27010;&#29575;&#12289;&#29109;&#21644;&#20114;&#20449;&#24687;&#65292;&#25552;&#20379;&#20102;&#20998;&#31867;&#24322;&#24120;&#30340;&#24341;&#20154;&#20837;&#32988;&#30340;&#21465;&#36848;&#65292;&#24182;&#23454;&#29616;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#24352;&#37327;&#32593;&#32476;&#22914;&#20309;&#24110;&#21161;&#21457;&#23637;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22522;&#20110;&#30697;&#38453;&#20056;&#31215;&#29366;&#24577;&#65288;MPS&#65289;&#24320;&#21457;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#32858;&#31867;&#31639;&#27861;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#23454;&#38469;&#20351;&#29992;&#26696;&#20363;&#20013;&#30340;&#23545;&#25163;&#29983;&#25104;&#30340;&#23041;&#32961;&#24773;&#25253;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#35777;&#26126;&#65292;MPS&#22312;&#24615;&#33021;&#26041;&#38754;&#21487;&#20197;&#19982;&#20256;&#32479;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22914;&#33258;&#32534;&#30721;&#22120;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30456;&#23218;&#32654;&#65292;&#21516;&#26102;&#25552;&#20379;&#26356;&#20016;&#23500;&#30340;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#33258;&#28982;&#22320;&#20419;&#36827;&#20102;&#29305;&#24449;&#27010;&#29575;&#12289;&#20911;&#183;&#35834;&#20234;&#26364;&#29109;&#21644;&#20114;&#20449;&#24687;&#30340;&#25552;&#21462;&#65292;&#20026;&#24322;&#24120;&#20998;&#31867;&#25552;&#20379;&#20102;&#24341;&#20154;&#20837;&#32988;&#30340;&#21465;&#36848;&#65292;&#24182;&#20419;&#36827;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#27700;&#24179;&#65292;&#36825;&#23545;&#20110;&#29702;&#35299;&#20154;&#24037;&#26234;&#33021;&#20915;&#31574;&#30340;&#22522;&#26412;&#21407;&#29702;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we show how tensor networks help in developing explainability of machine learning algorithms. Specifically, we develop an unsupervised clustering algorithm based on Matrix Product States (MPS) and apply it in the context of a real use-case of adversary-generated threat intelligence. Our investigation proves that MPS rival traditional deep learning models such as autoencoders and GANs in terms of performance, while providing much richer model interpretability. Our approach naturally facilitates the extraction of feature-wise probabilities, Von Neumann Entropy, and mutual information, offering a compelling narrative for classification of anomalies and fostering an unprecedented level of transparency and interpretability, something fundamental to understand the rationale behind artificial intelligence decisions.
&lt;/p&gt;</description></item><item><title>CapsFusion&#26159;&#19968;&#20010;&#20808;&#36827;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25972;&#21512;&#21644;&#32454;&#21270;&#26469;&#33258;&#32593;&#32476;&#22270;&#20687;-&#25991;&#26412;&#23545;&#21644;&#21512;&#25104;&#23383;&#24149;&#30340;&#20449;&#24687;&#65292;&#25552;&#20379;&#20102;&#26356;&#39640;&#36136;&#37327;&#12289;&#26356;&#21487;&#25193;&#23637;&#30340;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2310.20550</link><description>&lt;p&gt;
CapsFusion: &#37325;&#26032;&#24605;&#32771;&#22823;&#35268;&#27169;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
CapsFusion: Rethinking Image-Text Data at Scale. (arXiv:2310.20550v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20550
&lt;/p&gt;
&lt;p&gt;
CapsFusion&#26159;&#19968;&#20010;&#20808;&#36827;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25972;&#21512;&#21644;&#32454;&#21270;&#26469;&#33258;&#32593;&#32476;&#22270;&#20687;-&#25991;&#26412;&#23545;&#21644;&#21512;&#25104;&#23383;&#24149;&#30340;&#20449;&#24687;&#65292;&#25552;&#20379;&#20102;&#26356;&#39640;&#36136;&#37327;&#12289;&#26356;&#21487;&#25193;&#23637;&#30340;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#27169;&#22411;&#23637;&#31034;&#20102;&#22312;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#25191;&#34892;&#22810;&#26679;&#21270;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;&#26174;&#33879;&#27867;&#21270;&#33021;&#21147;&#12290;&#22823;&#35268;&#27169;&#22522;&#20110;&#32593;&#32476;&#30340;&#22270;&#20687;-&#25991;&#26412;&#23545;&#22312;&#36825;&#19968;&#25104;&#21151;&#20013;&#36215;&#30528;&#26681;&#26412;&#24615;&#30340;&#36129;&#29486;&#65292;&#20294;&#23384;&#22312;&#30528;&#36807;&#22810;&#30340;&#22122;&#22768;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20351;&#29992;&#30001;&#29983;&#25104;&#24335;&#23383;&#24149;&#27169;&#22411;&#21512;&#25104;&#30340;&#26367;&#20195;&#23383;&#24149;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#22522;&#20934;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#20351;&#29992;&#21512;&#25104;&#23383;&#24149;&#35757;&#32451;&#30340;&#27169;&#22411;&#20013;&#23384;&#22312;&#30528;&#26174;&#33879;&#30340;&#21487;&#25193;&#23637;&#24615;&#19981;&#36275;&#21644;&#19990;&#30028;&#30693;&#35782;&#20007;&#22833;&#38382;&#39064;&#65292;&#36825;&#20123;&#38382;&#39064;&#22312;&#20854;&#21021;&#22987;&#22522;&#20934;&#25104;&#21151;&#20013;&#22823;&#37096;&#20998;&#34987;&#25513;&#30422;&#20102;&#12290;&#32463;&#36807;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#30830;&#23450;&#26681;&#26412;&#21407;&#22240;&#26159;&#29616;&#26377;&#21512;&#25104;&#23383;&#24149;&#20013;&#36807;&#20110;&#31616;&#21270;&#30340;&#35821;&#35328;&#32467;&#26500;&#21644;&#32570;&#20047;&#30693;&#35782;&#32454;&#33410;&#12290;&#20026;&#20102;&#25552;&#20379;&#26356;&#39640;&#36136;&#37327;&#12289;&#26356;&#21487;&#25193;&#23637;&#30340;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#25968;&#25454;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CapsFusion&#65292;&#36825;&#26159;&#19968;&#20010;&#20808;&#36827;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#25972;&#21512;&#21644;&#32454;&#21270;&#26469;&#33258;&#32593;&#32476;&#22270;&#20687;-&#25991;&#26412;&#23545;&#21644;&#21512;&#25104;&#23383;&#24149;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large multimodal models demonstrate remarkable generalist ability to perform diverse multimodal tasks in a zero-shot manner. Large-scale web-based image-text pairs contribute fundamentally to this success, but suffer from excessive noise. Recent studies use alternative captions synthesized by captioning models and have achieved notable benchmark performance. However, our experiments reveal significant Scalability Deficiency and World Knowledge Loss issues in models trained with synthetic captions, which have been largely obscured by their initial benchmark success. Upon closer examination, we identify the root cause as the overly-simplified language structure and lack of knowledge details in existing synthetic captions. To provide higher-quality and more scalable multimodal pretraining data, we propose CapsFusion, an advanced framework that leverages large language models to consolidate and refine information from both web-based image-text pairs and synthetic captions. Extensive experi
&lt;/p&gt;</description></item><item><title>LLMs&#27169;&#22411;&#22312;&#35757;&#32451;&#20013;&#21482;&#33021;&#23398;&#20064;&#21040;"A&#26159;B"&#30340;&#32467;&#26500;&#65292;&#26080;&#27861;&#33258;&#21160;&#25512;&#24191;&#21040;"B&#26159;A"&#12290;&#36825;&#34920;&#26126;&#27169;&#22411;&#22312;&#36923;&#36753;&#25512;&#26029;&#19978;&#23384;&#22312;&#22522;&#26412;&#22833;&#36133;&#21644;&#35757;&#32451;&#38598;&#20013;&#27169;&#24335;&#30340;&#25512;&#24191;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.12288</link><description>&lt;p&gt;
&#32763;&#36716;&#35781;&#21650;: &#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#35757;&#32451;&#30340;"A&#26159;B"&#26080;&#27861;&#23398;&#20064;"B&#26159;A"
&lt;/p&gt;
&lt;p&gt;
The Reversal Curse: LLMs trained on "A is B" fail to learn "B is A". (arXiv:2309.12288v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12288
&lt;/p&gt;
&lt;p&gt;
LLMs&#27169;&#22411;&#22312;&#35757;&#32451;&#20013;&#21482;&#33021;&#23398;&#20064;&#21040;"A&#26159;B"&#30340;&#32467;&#26500;&#65292;&#26080;&#27861;&#33258;&#21160;&#25512;&#24191;&#21040;"B&#26159;A"&#12290;&#36825;&#34920;&#26126;&#27169;&#22411;&#22312;&#36923;&#36753;&#25512;&#26029;&#19978;&#23384;&#22312;&#22522;&#26412;&#22833;&#36133;&#21644;&#35757;&#32451;&#38598;&#20013;&#27169;&#24335;&#30340;&#25512;&#24191;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25581;&#31034;&#20102;&#33258;&#22238;&#24402;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#27867;&#21270;&#19978;&#30340;&#20196;&#20154;&#24778;&#35766;&#30340;&#22833;&#36133;&#12290;&#22914;&#26524;&#19968;&#20010;&#27169;&#22411;&#26159;&#22522;&#20110;"A&#26159;B"&#24418;&#24335;&#30340;&#21477;&#23376;&#36827;&#34892;&#35757;&#32451;&#65292;&#23427;&#19981;&#20250;&#33258;&#21160;&#25512;&#24191;&#21040;&#30456;&#21453;&#30340;&#26041;&#21521;"B&#26159;A"&#12290;&#36825;&#23601;&#26159;&#32763;&#36716;&#35781;&#21650;&#12290;&#20363;&#22914;&#65292;&#22914;&#26524;&#19968;&#20010;&#27169;&#22411;&#26159;&#22522;&#20110;"Olaf Scholz&#26159;&#24503;&#22269;&#31532;&#20061;&#20219;&#24635;&#29702;"&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#23427;&#19981;&#20250;&#33258;&#21160;&#33021;&#22815;&#22238;&#31572;&#38382;&#39064;"&#35841;&#26159;&#24503;&#22269;&#31532;&#20061;&#20219;&#24635;&#29702;&#65311;"&#12290;&#27492;&#22806;&#65292;&#27491;&#30830;&#31572;&#26696;&#65288;"Olaf Scholz"&#65289;&#30340;&#21487;&#33021;&#24615;&#19981;&#20250;&#27604;&#38543;&#26426;&#21517;&#23383;&#26356;&#39640;&#12290;&#22240;&#27492;&#65292;&#27169;&#22411;&#22312;&#36923;&#36753;&#25512;&#26029;&#19978;&#23384;&#22312;&#22522;&#26412;&#22833;&#36133;&#65292;&#24182;&#19988;&#19981;&#20250;&#25512;&#24191;&#21040;&#23427;&#20204;&#35757;&#32451;&#38598;&#20013;&#30340;&#26222;&#36941;&#27169;&#24335;&#65288;&#21363;&#22914;&#26524;&#20986;&#29616;"A&#26159;B"&#65292;&#21017;"B&#26159;A"&#26356;&#21487;&#33021;&#20986;&#29616;&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#34394;&#26500;&#30340;&#38472;&#36848;&#65288;&#22914;"Uriah Hawthorne&#26159;'Abyssal Melodies'&#30340;&#20316;&#26354;&#23478;"&#65289;&#19978;&#23545;GPT-3&#21644;Llama-1&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#23637;&#31034;&#23427;&#20204;&#26080;&#27861;&#27491;&#30830;&#22238;&#31572;"&#35841;&#21019;&#20316;&#20102;'Abyssal Melodies'?"&#26469;&#25552;&#20379;&#32763;&#36716;&#35781;&#21650;&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
We expose a surprising failure of generalization in auto-regressive large language models (LLMs). If a model is trained on a sentence of the form "A is B", it will not automatically generalize to the reverse direction "B is A". This is the Reversal Curse. For instance, if a model is trained on "Olaf Scholz was the ninth Chancellor of Germany", it will not automatically be able to answer the question, "Who was the ninth Chancellor of Germany?". Moreover, the likelihood of the correct answer ("Olaf Scholz") will not be higher than for a random name. Thus, models exhibit a basic failure of logical deduction and do not generalize a prevalent pattern in their training set (i.e. if "A is B'' occurs, "B is A" is more likely to occur). We provide evidence for the Reversal Curse by finetuning GPT-3 and Llama-1 on fictitious statements such as "Uriah Hawthorne is the composer of 'Abyssal Melodies'" and showing that they fail to correctly answer "Who composed 'Abyssal Melodies?'". The Reversal Cu
&lt;/p&gt;</description></item><item><title>&#23398;&#20064;&#36890;&#36807;&#33258;&#25105;&#35299;&#37322;&#65288;LSX&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#36890;&#36807;&#32473;&#20104;&#35299;&#37322;&#21644;&#25209;&#35780;&#32773;&#30340;&#21453;&#39304;&#26469;&#25913;&#36827;&#23398;&#20064;&#32773;&#30340;&#24615;&#33021;&#12290;&#36825;&#31181;&#26041;&#27861;&#36866;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#31561;&#22522;&#26412;&#20219;&#21153;&#65292;&#24182;&#26377;&#28508;&#21147;&#22312;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20013;&#21457;&#25381;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.08395</link><description>&lt;p&gt;
&#23398;&#20064;&#36890;&#36807;&#33258;&#25105;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Learning by Self-Explaining. (arXiv:2309.08395v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08395
&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#36890;&#36807;&#33258;&#25105;&#35299;&#37322;&#65288;LSX&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#36890;&#36807;&#32473;&#20104;&#35299;&#37322;&#21644;&#25209;&#35780;&#32773;&#30340;&#21453;&#39304;&#26469;&#25913;&#36827;&#23398;&#20064;&#32773;&#30340;&#24615;&#33021;&#12290;&#36825;&#31181;&#26041;&#27861;&#36866;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#31561;&#22522;&#26412;&#20219;&#21153;&#65292;&#24182;&#26377;&#28508;&#21147;&#22312;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20013;&#21457;&#25381;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#20174;&#29983;&#29289;&#23398;&#20013;&#23547;&#25214;&#28789;&#24863;&#65292;&#29305;&#21035;&#26159;&#20154;&#31867;&#26234;&#33021;&#12290;&#19982;&#30446;&#21069;&#20027;&#35201;&#23558;&#35299;&#37322;&#35270;&#20026;&#27169;&#22411;&#26816;&#26597;&#25163;&#27573;&#30340;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#30456;&#27604;&#65292;&#20174;&#24515;&#29702;&#23398;&#20013;&#21457;&#29616;&#33258;&#25105;&#35299;&#37322;&#22312;&#20195;&#29702;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#22909;&#22788;&#26377;&#20123;&#34987;&#24573;&#35270;&#20102;&#12290;&#21463;&#21040;&#36825;&#20010;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#31216;&#20026;&#23398;&#20064;&#36890;&#36807;&#33258;&#25105;&#35299;&#37322; (LSX)&#12290;&#20854;&#20013;&#30340;&#22522;&#26412;&#24605;&#24819;&#26159;&#65292;&#19968;&#20010;&#23398;&#20064;&#27169;&#22359; (&#23398;&#20064;&#32773;) &#25191;&#34892;&#19968;&#20010;&#22522;&#26412;&#20219;&#21153;&#65292;&#27604;&#22914;&#22270;&#20687;&#20998;&#31867;&#65292;&#24182;&#23545;&#20854;&#20915;&#31574;&#36827;&#34892;&#35299;&#37322;&#12290;&#38543;&#21518;&#65292;&#19968;&#20010;&#20869;&#37096;&#25209;&#35780;&#32773;&#27169;&#22359;&#22522;&#20110;&#21407;&#22987;&#20219;&#21153;&#35780;&#20272;&#36825;&#20123;&#35299;&#37322;&#30340;&#36136;&#37327;&#12290;&#26368;&#21518;&#65292;&#23398;&#20064;&#32773;&#36890;&#36807;&#25209;&#35780;&#32773;&#30340;&#21453;&#39304;&#24471;&#21040;&#25913;&#36827;&#65292;&#24182;&#26681;&#25454;&#38656;&#35201;&#37325;&#22797;&#36825;&#20010;&#24490;&#29615;&#12290;&#32972;&#21518;&#30340;&#30452;&#35273;&#26159;&#65292;&#22914;&#26524;&#25209;&#35780;&#32773;&#33021;&#22815;&#26681;&#25454;&#30456;&#24212;&#30340;&#35299;&#37322;&#25191;&#34892;&#30456;&#21516;&#30340;&#20219;&#21153;&#65292;&#21017;&#35813;&#35299;&#37322;&#34987;&#35748;&#20026;&#26159;&#8220;&#22909;&#8221;&#30340;&#12290;&#23613;&#31649;&#26377;&#35768;&#22810;&#23454;&#29616;&#21487;&#33021;&#24615;&#65292;&#20294;&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#20851;&#20110;&#23454;&#26045;&#23398;&#20064;&#36890;&#36807;&#33258;&#25105;&#35299;&#37322;&#30340;&#19968;&#33324;&#25351;&#23548;&#21407;&#21017;&#12290;&#26377;&#24453;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#21644;&#23454;&#36341;&#26469;&#25506;&#32034;&#36825;&#19968;&#23398;&#20064;&#33539;&#24335;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI) research has a long track record of drawing inspirations from findings from biology, in particular human intelligence. In contrast to current AI research that mainly treats explanations as a means for model inspection, a somewhat neglected finding from human psychology is the benefit of self-explaining in an agents' learning process. Motivated by this, we introduce a novel learning paradigm, termed Learning by Self-Explaining (LSX). The underlying idea is that a learning module (learner) performs a base task, e.g. image classification, and provides explanations to its decisions. An internal critic module next evaluates the quality of these explanations given the original task. Finally, the learner is refined with the critic's feedback and the loop is repeated as required. The intuition behind this is that an explanation is considered "good" if the critic can perform the same task given the respective explanation. Despite many implementation possibilities th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#24402;&#32435;&#20559;&#22909;&#26041;&#27861;&#8212;&#8212;&#20851;&#31995;&#29942;&#39048;&#65292;&#29992;&#20110;&#26377;&#25928;&#22320;&#35825;&#23548;&#25277;&#35937;&#27010;&#24565;&#30340;&#27169;&#22411;&#65292;&#24378;&#35843;&#20102;&#20854;&#22312;&#20154;&#31867;&#24605;&#32500;&#21644;&#22823;&#33041;&#20013;&#25277;&#35937;&#27010;&#24565;&#20064;&#24471;&#20013;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.06629</link><description>&lt;p&gt;
&#20316;&#20026;&#26377;&#25928;&#25277;&#35937;&#30340;&#24402;&#32435;&#20559;&#22909;&#30340;&#20851;&#31995;&#29942;&#39048;
&lt;/p&gt;
&lt;p&gt;
The Relational Bottleneck as an Inductive Bias for Efficient Abstraction. (arXiv:2309.06629v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06629
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#24402;&#32435;&#20559;&#22909;&#26041;&#27861;&#8212;&#8212;&#20851;&#31995;&#29942;&#39048;&#65292;&#29992;&#20110;&#26377;&#25928;&#22320;&#35825;&#23548;&#25277;&#35937;&#27010;&#24565;&#30340;&#27169;&#22411;&#65292;&#24378;&#35843;&#20102;&#20854;&#22312;&#20154;&#31867;&#24605;&#32500;&#21644;&#22823;&#33041;&#20013;&#25277;&#35937;&#27010;&#24565;&#20064;&#24471;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35748;&#30693;&#31185;&#23398;&#30340;&#19968;&#20010;&#26680;&#24515;&#25361;&#25112;&#26159;&#35299;&#37322;&#22914;&#20309;&#20174;&#26377;&#38480;&#32463;&#39564;&#20013;&#33719;&#21462;&#25277;&#35937;&#27010;&#24565;&#12290;&#36825;&#19968;&#21162;&#21147;&#24120;&#24120;&#34987;&#25551;&#36848;&#20026;&#32463;&#39564;&#20027;&#20041;&#21644;&#22825;&#36171;&#20027;&#20041;&#26041;&#27861;&#20043;&#38388;&#30340;&#20108;&#20998;&#27861;&#65292;&#26368;&#36817;&#20027;&#35201;&#20307;&#29616;&#22312;&#26377;&#20851;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#31526;&#21495;&#35748;&#30693;&#27169;&#22411;&#30340;&#20105;&#35770;&#20013;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#19968;&#31181;&#26368;&#36817;&#20852;&#36215;&#30340;&#24037;&#20316;&#32447;&#36335;&#65292;&#35813;&#32447;&#36335;&#36890;&#36807;&#21033;&#29992;&#25105;&#20204;&#31216;&#20043;&#20026;&#20851;&#31995;&#29942;&#39048;&#30340;&#24402;&#32435;&#20559;&#22909;&#65292;&#25552;&#20986;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#19968;&#31181;&#26032;&#30340;&#35843;&#21644;&#26041;&#24335;&#12290;&#25105;&#20204;&#22238;&#39038;&#20102;&#19968;&#31995;&#21015;&#37319;&#29992;&#36825;&#31181;&#26041;&#27861;&#22312;&#25968;&#25454;&#26377;&#25928;&#30340;&#26041;&#24335;&#19979;&#35825;&#23548;&#20986;&#25277;&#35937;&#30340;&#27169;&#22411;&#65292;&#24378;&#35843;&#20102;&#23427;&#20204;&#20316;&#20026;&#20154;&#31867;&#24605;&#32500;&#21644;&#22823;&#33041;&#20013;&#25277;&#35937;&#27010;&#24565;&#20064;&#24471;&#30340;&#20505;&#36873;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
A central challenge for cognitive science is to explain how abstract concepts are acquired from limited experience. This effort has often been framed in terms of a dichotomy between empiricist and nativist approaches, most recently embodied by debates concerning deep neural networks and symbolic cognitive models. Here, we highlight a recently emerging line of work that suggests a novel reconciliation of these approaches, by exploiting an inductive bias that we term the relational bottleneck. We review a family of models that employ this approach to induce abstractions in a data-efficient manner, emphasizing their potential as candidate models for the acquisition of abstract concepts in the human mind and brain.
&lt;/p&gt;</description></item><item><title>CSM-H-R&#26159;&#19968;&#20010;&#33258;&#21160;&#19978;&#19979;&#25991;&#25512;&#29702;&#26694;&#26550;&#65292;&#29992;&#20110;&#21487;&#20114;&#25805;&#20316;&#30340;&#26234;&#33021;&#31995;&#32479;&#21644;&#38544;&#31169;&#20445;&#25252;&#12290;&#35813;&#26694;&#26550;&#32467;&#21512;&#20102;&#26412;&#20307;&#21644;&#29366;&#24577;&#65292;&#22312;&#36816;&#34892;&#26102;&#35782;&#21035;&#26377;&#24847;&#20041;&#30340;&#39640;&#32423;&#19978;&#19979;&#25991;&#65292;&#24182;&#21487;&#24212;&#29992;&#20110;&#19981;&#21516;&#30340;&#25512;&#29702;&#25216;&#26415;&#12290;&#22312;&#26234;&#33021;&#26657;&#22253;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#26234;&#33021;&#30005;&#26799;&#31995;&#32479;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#24182;&#23637;&#31034;&#20102;&#20351;&#29992;&#20808;&#36827;&#30340;&#25968;&#23398;&#21644;&#27010;&#29575;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.11066</link><description>&lt;p&gt;
CSM-H-R: &#19968;&#31181;&#29992;&#20110;&#21487;&#20114;&#25805;&#20316;&#26234;&#33021;&#31995;&#32479;&#21644;&#38544;&#31169;&#20445;&#25252;&#30340;&#33258;&#21160;&#19978;&#19979;&#25991;&#25512;&#29702;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
CSM-H-R: An Automatic Context Reasoning Framework for Interoperable Intelligent Systems and Privacy Protection. (arXiv:2308.11066v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11066
&lt;/p&gt;
&lt;p&gt;
CSM-H-R&#26159;&#19968;&#20010;&#33258;&#21160;&#19978;&#19979;&#25991;&#25512;&#29702;&#26694;&#26550;&#65292;&#29992;&#20110;&#21487;&#20114;&#25805;&#20316;&#30340;&#26234;&#33021;&#31995;&#32479;&#21644;&#38544;&#31169;&#20445;&#25252;&#12290;&#35813;&#26694;&#26550;&#32467;&#21512;&#20102;&#26412;&#20307;&#21644;&#29366;&#24577;&#65292;&#22312;&#36816;&#34892;&#26102;&#35782;&#21035;&#26377;&#24847;&#20041;&#30340;&#39640;&#32423;&#19978;&#19979;&#25991;&#65292;&#24182;&#21487;&#24212;&#29992;&#20110;&#19981;&#21516;&#30340;&#25512;&#29702;&#25216;&#26415;&#12290;&#22312;&#26234;&#33021;&#26657;&#22253;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#26234;&#33021;&#30005;&#26799;&#31995;&#32479;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#24182;&#23637;&#31034;&#20102;&#20351;&#29992;&#20808;&#36827;&#30340;&#25968;&#23398;&#21644;&#27010;&#29575;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29289;&#32852;&#32593;&#26102;&#20195;&#65292;&#26234;&#33021;&#31995;&#32479;&#23545;&#39640;&#32423;&#19978;&#19979;&#25991;(HLC)&#25512;&#29702;&#30340;&#33258;&#21160;&#21270;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#36825;&#26159;&#22240;&#20026;&#19978;&#19979;&#25991;&#25968;&#25454;&#30340;&#19981;&#26029;&#31215;&#32047;&#12289;&#22810;&#28304;&#25968;&#25454;&#34701;&#21512;&#30340;&#36235;&#21183;&#20197;&#21450;&#22522;&#20110;&#19978;&#19979;&#25991;&#20915;&#31574;&#36807;&#31243;&#30340;&#22266;&#26377;&#22797;&#26434;&#24615;&#21644;&#21160;&#24577;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#19978;&#19979;&#25991;&#25512;&#29702;&#26694;&#26550;CSM-H-R&#65292;&#35813;&#26694;&#26550;&#22312;&#36816;&#34892;&#26102;&#20197;&#32534;&#31243;&#26041;&#24335;&#32452;&#21512;&#26412;&#20307;&#21644;&#29366;&#24577;&#65292;&#24182;&#32467;&#21512;&#27169;&#22411;&#23384;&#20648;&#38454;&#27573;&#65292;&#20197;&#23454;&#29616;&#35782;&#21035;&#26377;&#24847;&#20041;&#30340;HLC&#30340;&#33021;&#21147;&#65292;&#25152;&#24471;&#21040;&#30340;&#25968;&#25454;&#34920;&#31034;&#21487;&#24212;&#29992;&#20110;&#19981;&#21516;&#30340;&#25512;&#29702;&#25216;&#26415;&#12290;&#22312;&#26234;&#33021;&#26657;&#22253;&#29615;&#22659;&#20013;&#22522;&#20110;&#26234;&#33021;&#30005;&#26799;&#31995;&#32479;&#24320;&#23637;&#20102;&#26696;&#20363;&#30740;&#31350;&#12290;&#26694;&#26550;&#30340;&#23454;&#29616;-CSM&#24341;&#25806;&#20197;&#21450;&#23558;HLC&#25512;&#29702;&#36716;&#21270;&#20026;&#30690;&#37327;&#21644;&#30697;&#38453;&#35745;&#31639;&#30340;&#23454;&#39564;&#65292;&#29305;&#21035;&#20851;&#27880;&#19978;&#19979;&#25991;&#30340;&#21160;&#24577;&#29305;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#20351;&#29992;&#20808;&#36827;&#30340;&#25968;&#23398;&#21644;&#27010;&#29575;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automation of High-Level Context (HLC) reasoning for intelligent systems at scale is imperative due to the unceasing accumulation of contextual data in the IoT era, the trend of the fusion of data from multi-sources, and the intrinsic complexity and dynamism of the context-based decision-making process. To mitigate this issue, we propose an automatic context reasoning framework CSM-H-R, which programmatically combines ontologies and states at runtime and the model-storage phase for attaining the ability to recognize meaningful HLC, and the resulting data representation can be applied to different reasoning techniques. Case studies are developed based on an intelligent elevator system in a smart campus setting. An implementation of the framework - a CSM Engine, and the experiments of translating the HLC reasoning into vector and matrix computing especially take care of the dynamic aspects of context and present the potentiality of using advanced mathematical and probabilistic models to 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38382;&#39064;&#35774;&#32622;&#65306;&#24341;&#29992;&#20154;&#31867;&#33310;&#36424;&#29983;&#25104;&#12290;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#33310;&#36424;&#22330;&#26223;&#20013;&#65292;&#36890;&#36807;&#35299;&#32806;&#25511;&#21046;&#26469;&#35299;&#20915;&#33310;&#36424;&#21512;&#25104;&#20013;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#24544;&#23454;&#24615;&#12289;&#27867;&#21270;&#33021;&#21147;&#21644;&#32452;&#21512;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.00040</link><description>&lt;p&gt;
DisCo: &#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#20013;&#22522;&#20110;&#20154;&#31867;&#33310;&#36424;&#30340;&#24341;&#29992;&#29983;&#25104;&#30340;&#35299;&#32806;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
DisCo: Disentangled Control for Referring Human Dance Generation in Real World. (arXiv:2307.00040v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00040
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38382;&#39064;&#35774;&#32622;&#65306;&#24341;&#29992;&#20154;&#31867;&#33310;&#36424;&#29983;&#25104;&#12290;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#33310;&#36424;&#22330;&#26223;&#20013;&#65292;&#36890;&#36807;&#35299;&#32806;&#25511;&#21046;&#26469;&#35299;&#20915;&#33310;&#36424;&#21512;&#25104;&#20013;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#24544;&#23454;&#24615;&#12289;&#27867;&#21270;&#33021;&#21147;&#21644;&#32452;&#21512;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;AI&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#29305;&#21035;&#26159;&#22312;&#22522;&#20110;&#25991;&#26412;&#25551;&#36848;&#30340;&#22270;&#20687;/&#35270;&#39057;&#21512;&#25104;&#26041;&#38754;&#12290;&#23613;&#31649;&#26377;&#20102;&#36825;&#20123;&#36827;&#27493;&#65292;&#20294;&#22312;&#29983;&#25104;&#20154;&#31867;&#20013;&#24515;&#20869;&#23481;&#65288;&#22914;&#33310;&#36424;&#21512;&#25104;&#65289;&#26041;&#38754;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#33310;&#36424;&#21512;&#25104;&#26041;&#27861;&#22312;&#21512;&#25104;&#20869;&#23481;&#19982;&#29616;&#23454;&#19990;&#30028;&#33310;&#36424;&#22330;&#26223;&#20043;&#38388;&#23384;&#22312;&#24046;&#36317;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#26032;&#30340;&#38382;&#39064;&#35774;&#32622;&#65306;&#24341;&#29992;&#20154;&#31867;&#33310;&#36424;&#29983;&#25104;&#65292;&#37325;&#28857;&#20851;&#27880;&#20855;&#26377;&#19977;&#20010;&#37325;&#35201;&#23646;&#24615;&#30340;&#29616;&#23454;&#19990;&#30028;&#33310;&#36424;&#22330;&#26223;&#65306;&#65288;i&#65289;&#24544;&#23454;&#24615;&#65306;&#21512;&#25104;&#24212;&#35813;&#20445;&#30041;&#24341;&#29992;&#22270;&#20687;&#20013;&#20154;&#31867;&#20027;&#20307;&#21069;&#26223;&#21644;&#32972;&#26223;&#30340;&#22806;&#35266;&#65292;&#24182;&#31934;&#30830;&#22320;&#36981;&#24490;&#30446;&#26631;&#23039;&#21183;&#65307;&#65288;ii&#65289;&#27867;&#21270;&#33021;&#21147;&#65306;&#27169;&#22411;&#24212;&#35813;&#36866;&#29992;&#20110;&#26410;&#35265;&#36807;&#30340;&#20154;&#31867;&#20027;&#20307;&#12289;&#32972;&#26223;&#21644;&#23039;&#21183;&#65307;&#65288;iii&#65289;&#32452;&#21512;&#24615;&#65306;&#23427;&#24212;&#35813;&#20801;&#35768;&#26469;&#33258;&#19981;&#21516;&#26469;&#28304;&#30340;&#24050;&#35265;/&#26410;&#35265;&#20027;&#20307;&#12289;&#32972;&#26223;&#21644;&#23039;&#21183;&#30340;&#32452;&#21512;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;D
&lt;/p&gt;
&lt;p&gt;
Generative AI has made significant strides in computer vision, particularly in image/video synthesis conditioned on text descriptions. Despite the advancements, it remains challenging especially in the generation of human-centric content such as dance synthesis. Existing dance synthesis methods struggle with the gap between synthesized content and real-world dance scenarios. In this paper, we define a new problem setting: Referring Human Dance Generation, which focuses on real-world dance scenarios with three important properties: (i) Faithfulness: the synthesis should retain the appearance of both human subject foreground and background from the reference image, and precisely follow the target pose; (ii) Generalizability: the model should generalize to unseen human subjects, backgrounds, and poses; (iii) Compositionality: it should allow for composition of seen/unseen subjects, backgrounds, and poses from different sources. To address these challenges, we introduce a novel approach, D
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30417;&#30563;&#24335;&#27880;&#24847;&#21147;&#22810;&#23454;&#20363;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#20998;&#26512;&#36229;&#22768;&#22270;&#20687;&#65292;&#23454;&#29616;AS&#30340;&#31934;&#30830;&#31579;&#26597;&#19988;&#31934;&#24230;&#20248;&#20110;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.00003</link><description>&lt;p&gt;
&#36890;&#36807;&#30417;&#30563;&#24335;&#27880;&#24847;&#21147;&#22810;&#23454;&#20363;&#23398;&#20064;&#20174;&#22810;&#35270;&#35282;&#36229;&#22768;&#22270;&#20687;&#26816;&#27979;&#24515;&#33039;&#30149;
&lt;/p&gt;
&lt;p&gt;
Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning. (arXiv:2306.00003v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00003
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30417;&#30563;&#24335;&#27880;&#24847;&#21147;&#22810;&#23454;&#20363;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#20998;&#26512;&#36229;&#22768;&#22270;&#20687;&#65292;&#23454;&#29616;AS&#30340;&#31934;&#30830;&#31579;&#26597;&#19988;&#31934;&#24230;&#20248;&#20110;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#33033;&#29923;&#29421;&#31364;(AS)&#26159;&#19968;&#31181;&#23548;&#33268;&#20005;&#37325;&#21457;&#30149;&#29575;&#21644;&#27515;&#20129;&#29575;&#30340;&#36864;&#34892;&#24615;&#29923;&#33180;&#30142;&#30149;&#12290;&#36825;&#31181;&#24773;&#20917;&#32463;&#24120;&#34987;&#20302;&#20272;&#21644;&#20302;&#27835;&#30103;&#12290;&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#65292;AS&#26159;&#36890;&#36807;&#36229;&#22768;&#24515;&#21160;&#22270;&#30340;&#19987;&#23478;&#23457;&#26597;&#26469;&#35786;&#26029;&#30340;&#65292;&#36825;&#20250;&#20135;&#29983;&#25968;&#21313;&#20010;&#19979;&#32954;&#37319;&#26679;&#30340;&#36229;&#22768;&#22270;&#20687;&#12290;&#21482;&#26377;&#19968;&#20123;&#35270;&#22270;&#26174;&#31034;&#20027;&#21160;&#33033;&#29923;&#12290;&#20026;&#20102;&#33258;&#21160;&#21270;&#31579;&#26597;AS&#65292;&#28145;&#24230;&#32593;&#32476;&#24517;&#39035;&#23398;&#20064;&#27169;&#20223;&#20154;&#31867;&#19987;&#23478;&#35782;&#21035;&#20027;&#21160;&#33033;&#29923;&#35270;&#22270;&#30340;&#33021;&#21147;&#65292;&#28982;&#21518;&#27719;&#24635;&#36825;&#20123;&#30456;&#20851;&#22270;&#20687;&#20197;&#20135;&#29983;&#30740;&#31350;&#32423;&#35786;&#26029;&#12290;&#25105;&#20204;&#21457;&#29616;&#20808;&#21069;&#30340;AS&#26816;&#27979;&#26041;&#27861;&#30001;&#20110;&#20381;&#36182;&#20110;&#36328;&#22270;&#20687;&#30340;&#19981;&#28789;&#27963;&#24179;&#22343;&#20540;&#32780;&#23548;&#33268;&#31934;&#24230;&#19981;&#36275;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#21457;&#29616;&#65292;&#29616;&#25104;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#22810;&#23454;&#20363;(MIL)&#23398;&#20064;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31471;&#21040;&#31471;MIL&#26041;&#27861;&#65292;&#21253;&#21547;&#20004;&#20010;&#20851;&#38190;&#26041;&#27861;&#21019;&#26032;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#30417;&#30563;&#24335;&#27880;&#24847;&#25216;&#26415;&#65292;&#24341;&#23548;&#23398;&#20064;&#30340;&#27880;&#24847;&#26426;&#21046;&#20559;&#29233;&#30456;&#20851;&#35270;&#22270;&#12290;&#20854;&#27425;&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#25552;&#39640;&#20102;&#27599;&#20010;&#21333;&#29420;&#22270;&#20687;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19968;&#20010;&#30495;&#23454;&#30340;&#20020;&#24202;&#25968;&#25454;&#38598;&#65288;4569&#21517;&#24739;&#32773;&#65289;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#22312;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#20043;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aortic stenosis (AS) is a degenerative valve condition that causes substantial morbidity and mortality. This condition is under-diagnosed and under-treated. In clinical practice, AS is diagnosed with expert review of transthoracic echocardiography, which produces dozens of ultrasound images of the heart. Only some of these views show the aortic valve. To automate screening for AS, deep networks must learn to mimic a human expert's ability to identify views of the aortic valve then aggregate across these relevant images to produce a study-level diagnosis. We find previous approaches to AS detection yield insufficient accuracy due to relying on inflexible averages across images. We further find that off-the-shelf attention-based multiple instance learning (MIL) performs poorly. We contribute a new end-to-end MIL approach with two key methodological innovations. First, a supervised attention technique guides the learned attention mechanism to favor relevant views. Second, a novel self-sup
&lt;/p&gt;</description></item></channel></rss>