<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#22522;&#20110;&#20840;&#26223;&#22270;&#20687;&#30340;&#21333;&#22270;&#12289;&#21452;&#22270;&#25110;&#22810;&#22270;&#30340;3D&#22330;&#26223;&#20960;&#20309;&#20272;&#35745;&#26041;&#27861;&#12290;&#20027;&#35201;&#21253;&#25324;&#29699;&#38754;&#30456;&#26426;&#27169;&#22411;&#12289;&#20840;&#26223;&#22270;&#20687;&#33719;&#21462;&#25216;&#26415;&#21644;&#34920;&#31034;&#26684;&#24335;&#12289;&#21333;&#30524;&#24067;&#23616;&#21644;&#28145;&#24230;&#25512;&#26029;&#26041;&#27861;&#12289;&#31435;&#20307;&#21305;&#37197;&#22312;&#29699;&#38754;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#20197;&#21450;&#22810;&#35270;&#35282;&#25668;&#20687;&#26426;&#35774;&#32622;&#19979;&#30340;&#31435;&#20307;&#21305;&#37197;&#26041;&#27861;&#12290;&#21516;&#26102;&#36824;&#25552;&#20379;&#20102;&#20851;&#20110;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#23610;&#24230;&#30340;&#32508;&#21512;&#21015;&#34920;&#12290;</title><link>http://arxiv.org/abs/2401.09252</link><description>&lt;p&gt;
&#20174;360&#24230;&#22270;&#20687;&#20013;&#20272;&#35745;3D&#22330;&#26223;&#20960;&#20309;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
3D Scene Geometry Estimation from 360$^\circ$ Imagery: A Survey. (arXiv:2401.09252v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09252
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#22522;&#20110;&#20840;&#26223;&#22270;&#20687;&#30340;&#21333;&#22270;&#12289;&#21452;&#22270;&#25110;&#22810;&#22270;&#30340;3D&#22330;&#26223;&#20960;&#20309;&#20272;&#35745;&#26041;&#27861;&#12290;&#20027;&#35201;&#21253;&#25324;&#29699;&#38754;&#30456;&#26426;&#27169;&#22411;&#12289;&#20840;&#26223;&#22270;&#20687;&#33719;&#21462;&#25216;&#26415;&#21644;&#34920;&#31034;&#26684;&#24335;&#12289;&#21333;&#30524;&#24067;&#23616;&#21644;&#28145;&#24230;&#25512;&#26029;&#26041;&#27861;&#12289;&#31435;&#20307;&#21305;&#37197;&#22312;&#29699;&#38754;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#20197;&#21450;&#22810;&#35270;&#35282;&#25668;&#20687;&#26426;&#35774;&#32622;&#19979;&#30340;&#31435;&#20307;&#21305;&#37197;&#26041;&#27861;&#12290;&#21516;&#26102;&#36824;&#25552;&#20379;&#20102;&#20851;&#20110;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#23610;&#24230;&#30340;&#32508;&#21512;&#21015;&#34920;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22522;&#20110;&#21333;&#20010;&#12289;&#20004;&#20010;&#25110;&#22810;&#20010;&#22312;&#20840;&#26223;&#20809;&#23398;&#19979;&#25429;&#33719;&#30340;&#22270;&#20687;&#30340;&#20808;&#39537;&#21644;&#26368;&#26032;&#30340;3D&#22330;&#26223;&#20960;&#20309;&#20272;&#35745;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35843;&#26597;&#12290;&#25105;&#20204;&#39318;&#20808;&#22238;&#39038;&#20102;&#29699;&#38754;&#30456;&#26426;&#27169;&#22411;&#30340;&#22522;&#26412;&#27010;&#24565;&#65292;&#24182;&#22238;&#39038;&#20102;&#36866;&#29992;&#20110;&#20840;&#26223;&#65288;&#20063;&#31216;&#20026;360&#24230;&#12289;&#29699;&#24418;&#25110;&#20840;&#26223;&#65289;&#22270;&#20687;&#21644;&#35270;&#39057;&#30340;&#26368;&#24120;&#35265;&#30340;&#37319;&#38598;&#25216;&#26415;&#21644;&#34920;&#31034;&#26684;&#24335;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#21333;&#30524;&#24067;&#23616;&#21644;&#28145;&#24230;&#25512;&#26029;&#26041;&#27861;&#65292;&#31361;&#20986;&#20102;&#36866;&#29992;&#20110;&#29699;&#24418;&#25968;&#25454;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#25509;&#30528;&#65292;&#22312;&#29699;&#38754;&#39046;&#22495;&#23545;&#32463;&#20856;&#30340;&#31435;&#20307;&#21305;&#37197;&#36827;&#34892;&#20102;&#20462;&#35746;&#65292;&#20854;&#20013;&#26816;&#27979;&#21644;&#25551;&#36848;&#31232;&#30095;&#21644;&#31264;&#23494;&#29305;&#24449;&#30340;&#26041;&#27861;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#21518;&#65292;&#31435;&#20307;&#21305;&#37197;&#27010;&#24565;&#34987;&#25512;&#24191;&#21040;&#22810;&#35270;&#35282;&#25668;&#20687;&#26426;&#35774;&#32622;&#20013;&#65292;&#23558;&#23427;&#20204;&#24402;&#31867;&#20026;&#20809;&#22330;&#12289;&#22810;&#35270;&#35282;&#31435;&#20307;&#21305;&#37197;&#21644;&#32467;&#26500;&#36816;&#21160;&#65288;&#25110;&#35270;&#35273;&#21516;&#26102;&#23450;&#20301;&#21644;&#24314;&#22270;&#65289;&#12290;&#25105;&#20204;&#36824;&#32534;&#21046;&#20102;&#19968;&#20010;&#20851;&#20110;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#23610;&#24230;&#30340;&#32508;&#21512;&#21015;&#34920;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper provides a comprehensive survey on pioneer and state-of-the-art 3D scene geometry estimation methodologies based on single, two, or multiple images captured under the omnidirectional optics. We first revisit the basic concepts of the spherical camera model, and review the most common acquisition technologies and representation formats suitable for omnidirectional (also called 360$^\circ$, spherical or panoramic) images and videos. We then survey monocular layout and depth inference approaches, highlighting the recent advances in learning-based solutions suited for spherical data. The classical stereo matching is then revised on the spherical domain, where methodologies for detecting and describing sparse and dense features become crucial. The stereo matching concepts are then extrapolated for multiple view camera setups, categorizing them among light fields, multi-view stereo, and structure from motion (or visual simultaneous localization and mapping). We also compile and di
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;DiffClone&#65292;&#19968;&#31181;&#36890;&#36807;&#25193;&#25955;&#39537;&#21160;&#30340;&#31574;&#30053;&#23398;&#20064;&#22686;&#24378;&#34892;&#20026;&#20811;&#38534;&#20195;&#29702;&#30340;&#31163;&#32447;&#31639;&#27861;&#12290;&#22312;&#30495;&#23454;&#30340;&#22312;&#32447;&#29289;&#29702;&#26426;&#22120;&#20154;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#37319;&#29992;MOCO&#24494;&#35843;&#30340;ResNet50&#30340;&#25928;&#26524;&#26368;&#22909;&#12290;</title><link>http://arxiv.org/abs/2401.09243</link><description>&lt;p&gt;
DiffClone: &#20351;&#29992;&#25193;&#25955;&#39537;&#21160;&#30340;&#31574;&#30053;&#23398;&#20064;&#22686;&#24378;&#26426;&#22120;&#20154;&#34892;&#20026;&#20811;&#38534;
&lt;/p&gt;
&lt;p&gt;
DiffClone: Enhanced Behaviour Cloning in Robotics with Diffusion-Driven Policy Learning. (arXiv:2401.09243v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09243
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;DiffClone&#65292;&#19968;&#31181;&#36890;&#36807;&#25193;&#25955;&#39537;&#21160;&#30340;&#31574;&#30053;&#23398;&#20064;&#22686;&#24378;&#34892;&#20026;&#20811;&#38534;&#20195;&#29702;&#30340;&#31163;&#32447;&#31639;&#27861;&#12290;&#22312;&#30495;&#23454;&#30340;&#22312;&#32447;&#29289;&#29702;&#26426;&#22120;&#20154;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#37319;&#29992;MOCO&#24494;&#35843;&#30340;ResNet50&#30340;&#25928;&#26524;&#26368;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#23398;&#20064;&#20219;&#21153;&#22312;&#35745;&#31639;&#19978;&#38750;&#24120;&#23494;&#38598;&#19988;&#30828;&#20214;&#29305;&#23450;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#26679;&#21270;&#30340;&#31163;&#32447;&#28436;&#31034;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#26426;&#22120;&#20154;&#25805;&#20316;&#20195;&#29702;&#65292;&#26469;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#30340;&#26041;&#24335;&#38750;&#24120;&#21560;&#24341;&#20154;&#12290;Train-Offline-Test-Online&#65288;TOTO&#65289;&#22522;&#20934;&#25552;&#20379;&#20102;&#19968;&#20010;&#32463;&#36807;&#31934;&#24515;&#31574;&#21010;&#30340;&#24320;&#28304;&#31163;&#32447;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#20027;&#35201;&#30001;&#19987;&#23478;&#25968;&#25454;&#32452;&#25104;&#65292;&#24182;&#25552;&#20379;&#20102;&#24120;&#35265;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#21644;&#34892;&#20026;&#20811;&#38534;&#20195;&#29702;&#30340;&#22522;&#20934;&#20998;&#25968;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;DiffClone&#65292;&#19968;&#31181;&#22686;&#24378;&#34892;&#20026;&#20811;&#38534;&#20195;&#29702;&#30340;&#31163;&#32447;&#31639;&#27861;&#65292;&#37319;&#29992;&#22522;&#20110;&#25193;&#25955;&#30340;&#31574;&#30053;&#23398;&#20064;&#65292;&#24182;&#22312;&#27979;&#35797;&#26102;&#22312;&#30495;&#23454;&#30340;&#22312;&#32447;&#29289;&#29702;&#26426;&#22120;&#20154;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#21516;&#26102;&#65292;&#36825;&#20063;&#26159;&#25105;&#20204;&#22312;NeurIPS 2023&#20030;&#21150;&#30340;Train-Offline-Test-Online&#65288;TOTO&#65289;&#22522;&#20934;&#25361;&#25112;&#36187;&#20013;&#30340;&#23448;&#26041;&#25552;&#20132;&#12290;&#25105;&#20204;&#23581;&#35797;&#20102;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#34920;&#31034;&#21644;&#20195;&#29702;&#31574;&#30053;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;MOCO&#24494;&#35843;&#30340;ResNet50&#30456;&#27604;&#20854;&#20182;&#24494;&#35843;&#26041;&#27861;&#34920;&#29616;&#26368;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robot learning tasks are extremely compute-intensive and hardware-specific. Thus the avenues of tackling these challenges, using a diverse dataset of offline demonstrations that can be used to train robot manipulation agents, is very appealing. The Train-Offline-Test-Online (TOTO) Benchmark provides a well-curated open-source dataset for offline training comprised mostly of expert data and also benchmark scores of the common offline-RL and behaviour cloning agents. In this paper, we introduce DiffClone, an offline algorithm of enhanced behaviour cloning agent with diffusion-based policy learning, and measured the efficacy of our method on real online physical robots at test time. This is also our official submission to the Train-Offline-Test-Online (TOTO) Benchmark Challenge organized at NeurIPS 2023. We experimented with both pre-trained visual representation and agent policies. In our experiments, we find that MOCO finetuned ResNet50 performs the best in comparison to other finetuned
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#24322;&#26500;&#20449;&#24687;&#31995;&#32479;&#25968;&#25454;&#31649;&#36947;&#23433;&#20840;&#27169;&#22411;&#65292;&#26088;&#22312;&#20351;&#29992;&#38598;&#25104;&#30340;&#22810;&#21361;&#38505;&#39044;&#35686;&#31995;&#32479;&#26469;&#35299;&#20915;&#25968;&#25454;&#31649;&#36947;&#20013;&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#21306;&#22359;&#38142;&#30340;&#22266;&#26377;&#23433;&#20840;&#29305;&#24615;&#65292;&#35813;&#27169;&#22411;&#30830;&#20445;&#20102;&#25968;&#25454;&#30340;&#23436;&#25972;&#24615;&#12289;&#26426;&#23494;&#24615;&#21644;&#30495;&#23454;&#24615;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20854;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2401.09240</link><description>&lt;p&gt;
&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#24322;&#26500;&#20449;&#24687;&#31995;&#32479;&#25968;&#25454;&#31649;&#36947;&#23433;&#20840;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Blockchain-based Model for Securing Data Pipeline in a Heterogeneous Information System. (arXiv:2401.09240v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09240
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#24322;&#26500;&#20449;&#24687;&#31995;&#32479;&#25968;&#25454;&#31649;&#36947;&#23433;&#20840;&#27169;&#22411;&#65292;&#26088;&#22312;&#20351;&#29992;&#38598;&#25104;&#30340;&#22810;&#21361;&#38505;&#39044;&#35686;&#31995;&#32479;&#26469;&#35299;&#20915;&#25968;&#25454;&#31649;&#36947;&#20013;&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#21306;&#22359;&#38142;&#30340;&#22266;&#26377;&#23433;&#20840;&#29305;&#24615;&#65292;&#35813;&#27169;&#22411;&#30830;&#20445;&#20102;&#25968;&#25454;&#30340;&#23436;&#25972;&#24615;&#12289;&#26426;&#23494;&#24615;&#21644;&#30495;&#23454;&#24615;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20854;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25105;&#20204;&#30340;&#25968;&#23383;&#19990;&#30028;&#20013;&#65292;&#35775;&#38382;&#20010;&#20154;&#21644;&#20844;&#20849;&#25968;&#25454;&#24050;&#25104;&#20026;&#19968;&#20010;&#20851;&#27880;&#30340;&#28966;&#28857;&#65292;&#38754;&#20020;&#30528;&#25361;&#25112;&#24615;&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;&#29616;&#20195;&#20449;&#24687;&#31995;&#32479;&#20855;&#26377;&#24322;&#26500;&#24615;&#65292;&#24182;&#19988;&#20855;&#26377;&#22266;&#26377;&#30340;&#23433;&#20840;&#28431;&#27934;&#65292;&#26131;&#21463;&#25968;&#25454;&#25318;&#25130;&#21644;&#25968;&#25454;&#20462;&#25913;&#30340;&#24433;&#21709;&#65292;&#36825;&#26159;&#30001;&#20110;&#36830;&#25509;&#31471;&#28857;&#20043;&#38388;&#30340;&#19981;&#23433;&#20840;&#36890;&#20449;&#25968;&#25454;&#31649;&#36947;&#12290;&#26412;&#30740;&#31350;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#20351;&#29992;&#38598;&#25104;&#22810;&#21361;&#38505;&#39044;&#35686;&#31995;&#32479;&#65288;MHEWS&#65289;&#20316;&#20026;&#26696;&#20363;&#30740;&#31350;&#26469;&#20445;&#25252;&#24322;&#26500;&#20449;&#24687;&#31995;&#32479;&#20013;&#30340;&#25968;&#25454;&#31649;&#36947;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#21033;&#29992;&#21306;&#22359;&#38142;&#25216;&#26415;&#30340;&#22266;&#26377;&#23433;&#20840;&#29305;&#24615;&#26469;&#35299;&#20915;&#25968;&#25454;&#31649;&#36947;&#20013;&#20986;&#29616;&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#26088;&#22312;&#20197;&#20998;&#25955;&#30340;&#26041;&#24335;&#30830;&#20445;&#25968;&#25454;&#23436;&#25972;&#24615;&#12289;&#26426;&#23494;&#24615;&#21644;&#30495;&#23454;&#24615;&#12290;&#35813;&#27169;&#22411;&#22312;&#28151;&#21512;&#29615;&#22659;&#20013;&#20351;&#29992;&#21407;&#22411;&#23454;&#26045;&#21644;&#20223;&#30495;&#23454;&#39564;&#36827;&#34892;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#23427;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#19968;&#20123;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
In our digital world, access to personal and public data has become an item of concern, with challenging security and privacy aspects. Modern information systems are heterogeneous in nature and have an inherent security vulnerability, which is susceptible to data interception and data modification due to unsecured communication data pipelines between connected endpoints. This re-search article presents a blockchain-based model for securing data pipelines in a heterogeneous information system using an integrated multi-hazard early warning system (MHEWS) as a case study. The proposed model utilizes the inherent security features of blockchain technology to address the security and privacy concerns that arise in data pipelines. The model is designed to ensure data integrity, confidentiality, and authenticity in a decentralized manner. The model is evaluated in a hybrid environment using a prototype implementation and simulation experiments with outcomes that demonstrate advantages over tr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#35273;-&#35302;&#35273;&#25968;&#25454;&#38598;&#65288;DaFoEs&#65289;&#65292;&#29992;&#20110;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20197;&#39044;&#27979;&#24494;&#21019;&#26426;&#22120;&#20154;&#25163;&#26415;&#20013;&#33145;&#33108;&#38236;&#24037;&#20855;&#26045;&#21152;&#30340;&#21147;&#37327;&#12290;&#36890;&#36807;&#28151;&#21512;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#35757;&#32451;&#65292;&#20197;&#20943;&#23569;&#20559;&#35265;&#65292;&#24182;&#25552;&#20986;&#20102;&#21487;&#21464;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#26469;&#22788;&#29702;&#21333;&#20010;&#36755;&#20837;&#25110;&#36755;&#20837;&#24207;&#21015;&#12290;</title><link>http://arxiv.org/abs/2401.09239</link><description>&lt;p&gt;
DaFoEs&#65306;&#28151;&#21512;&#25968;&#25454;&#38598;&#20197;&#23454;&#29616;&#22312;&#24494;&#21019;&#26426;&#22120;&#20154;&#25163;&#26415;&#20013;&#35270;&#35273;&#29366;&#24577;&#28145;&#24230;&#23398;&#20064;&#21147;&#37327;&#20272;&#35745;&#30340;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
DaFoEs: Mixing Datasets towards the generalization of vision-state deep-learning Force Estimation in Minimally Invasive Robotic Surgery. (arXiv:2401.09239v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09239
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#35273;-&#35302;&#35273;&#25968;&#25454;&#38598;&#65288;DaFoEs&#65289;&#65292;&#29992;&#20110;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20197;&#39044;&#27979;&#24494;&#21019;&#26426;&#22120;&#20154;&#25163;&#26415;&#20013;&#33145;&#33108;&#38236;&#24037;&#20855;&#26045;&#21152;&#30340;&#21147;&#37327;&#12290;&#36890;&#36807;&#28151;&#21512;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#35757;&#32451;&#65292;&#20197;&#20943;&#23569;&#20559;&#35265;&#65292;&#24182;&#25552;&#20986;&#20102;&#21487;&#21464;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#26469;&#22788;&#29702;&#21333;&#20010;&#36755;&#20837;&#25110;&#36755;&#20837;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24494;&#21019;&#26426;&#22120;&#20154;&#25163;&#26415;&#65288;MIRS&#65289;&#20013;&#20934;&#30830;&#30830;&#23450;&#23433;&#20840;&#20132;&#20114;&#36807;&#31243;&#20013;&#30340;&#25509;&#35302;&#21147;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#30740;&#31350;&#38590;&#39064;&#12290;&#21463;&#25163;&#26415;&#35270;&#39057;&#30340;&#26415;&#21518;&#23450;&#24615;&#20998;&#26512;&#30340;&#21551;&#21457;&#65292;&#20351;&#29992;&#36328;&#27169;&#24577;&#25968;&#25454;&#39537;&#21160;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#39044;&#27979;&#26080;&#24863;&#30693;&#21147;&#37327;&#36235;&#21183;&#30340;&#26368;&#26032;&#26041;&#27861;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#19988;&#21487;&#21464;&#30340;&#25968;&#25454;&#38598;&#65292;&#30446;&#21069;&#36824;&#19981;&#21487;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#35273;-&#35302;&#35273;&#25968;&#25454;&#38598;&#65288;DaFoEs&#65289;&#65292;&#20854;&#20013;&#21253;&#21547;&#21487;&#20197;&#29992;&#20110;&#28145;&#24230;&#31070;&#32463;&#27169;&#22411;&#35757;&#32451;&#30340;&#21487;&#21464;&#36719;&#29615;&#22659;&#12290;&#20026;&#20102;&#20943;&#23569;&#21333;&#19968;&#25968;&#25454;&#38598;&#30340;&#20559;&#35265;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#27969;&#31243;&#23558;&#19981;&#21516;&#30340;&#35270;&#35273;&#21644;&#29366;&#24577;&#25968;&#25454;&#36755;&#20837;&#27867;&#21270;&#21040;&#28151;&#21512;&#25968;&#25454;&#38598;&#35757;&#32451;&#20013;&#65292;&#20351;&#29992;&#20808;&#21069;&#39564;&#35777;&#36807;&#30340;&#20855;&#26377;&#19981;&#21516;&#35774;&#32622;&#30340;&#25968;&#25454;&#38598;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#21464;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#20197;&#39044;&#27979;&#33145;&#33108;&#38236;&#24037;&#20855;&#26045;&#21152;&#30340;&#21147;&#37327;&#65292;&#21487;&#20197;&#20351;&#29992;&#21333;&#20010;&#36755;&#20837;&#25110;&#36755;&#20837;&#24207;&#21015;&#12290;&#23545;&#20110;&#36755;&#20837;&#24207;&#21015;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#24490;&#29615;&#35299;&#30721;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Precisely determining the contact force during safe interaction in Minimally Invasive Robotic Surgery (MIRS) is still an open research challenge. Inspired by post-operative qualitative analysis from surgical videos, the use of cross-modality data driven deep neural network models has been one of the newest approaches to predict sensorless force trends. However, these methods required for large and variable datasets which are not currently available. In this paper, we present a new vision-haptic dataset (DaFoEs) with variable soft environments for the training of deep neural models. In order to reduce the bias from a single dataset, we present a pipeline to generalize different vision and state data inputs for mixed dataset training, using a previously validated dataset with different setup. Finally, we present a variable encoder-decoder architecture to predict the forces done by the laparoscopic tool using single input or sequence of inputs. For input sequence, we use a recurrent decod
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23450;&#29702;&#65292;&#25551;&#36848;&#20102;&#25152;&#26377;&#21487;&#33021;&#30340;&#26377;&#38480;&#32500;&#34920;&#31034;&#12289;&#22352;&#26631;&#36873;&#25321;&#21644;&#28857;&#29366;&#28608;&#27963;&#20989;&#25968;&#30340;&#32452;&#21512;&#65292;&#20197;&#33719;&#24471;&#19968;&#20010;&#23436;&#20840;&#31561;&#21464;&#30340;&#23618;&#65292;&#20174;&#32780;&#25512;&#24191;&#21644;&#21152;&#24378;&#20102;&#29616;&#26377;&#30340;&#29305;&#24449;&#23450;&#29702;&#12290;&#20855;&#26377;&#23454;&#38469;&#30456;&#20851;&#24615;&#30340;&#37325;&#35201;&#24773;&#20917;&#20316;&#20026;&#25512;&#35770;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;</title><link>http://arxiv.org/abs/2401.09235</link><description>&lt;p&gt;
&#23545;&#20855;&#26377;&#28857;&#29366;&#28608;&#27963;&#30340;&#31561;&#21464;&#32593;&#32476;&#30340;&#19968;&#20010;&#29305;&#24449;&#23450;&#29702;
&lt;/p&gt;
&lt;p&gt;
A Characterization Theorem for Equivariant Networks with Point-wise Activations. (arXiv:2401.09235v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09235
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23450;&#29702;&#65292;&#25551;&#36848;&#20102;&#25152;&#26377;&#21487;&#33021;&#30340;&#26377;&#38480;&#32500;&#34920;&#31034;&#12289;&#22352;&#26631;&#36873;&#25321;&#21644;&#28857;&#29366;&#28608;&#27963;&#20989;&#25968;&#30340;&#32452;&#21512;&#65292;&#20197;&#33719;&#24471;&#19968;&#20010;&#23436;&#20840;&#31561;&#21464;&#30340;&#23618;&#65292;&#20174;&#32780;&#25512;&#24191;&#21644;&#21152;&#24378;&#20102;&#29616;&#26377;&#30340;&#29305;&#24449;&#23450;&#29702;&#12290;&#20855;&#26377;&#23454;&#38469;&#30456;&#20851;&#24615;&#30340;&#37325;&#35201;&#24773;&#20917;&#20316;&#20026;&#25512;&#35770;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#22312;&#23545;&#31216;&#39046;&#22495;&#34920;&#29616;&#20986;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12289;&#34920;&#36798;&#33021;&#21147;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#20294;&#23545;&#20110;&#26576;&#20123;&#29305;&#23450;&#30340;&#23545;&#31216;&#24615;&#12289;&#34920;&#31034;&#21644;&#22352;&#26631;&#36873;&#25321;&#65292;&#26368;&#24120;&#35265;&#30340;&#28857;&#29366;&#28608;&#27963;&#20989;&#25968;&#65288;&#22914;ReLU&#65289;&#24182;&#19981;&#20855;&#22791;&#31561;&#21464;&#24615;&#65292;&#22240;&#27492;&#19981;&#33021;&#29992;&#20110;&#35774;&#35745;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#12290;&#26412;&#25991;&#20171;&#32461;&#30340;&#23450;&#29702;&#25551;&#36848;&#20102;&#25152;&#26377;&#21487;&#33021;&#30340;&#26377;&#38480;&#32500;&#34920;&#31034;&#12289;&#22352;&#26631;&#36873;&#25321;&#21644;&#28857;&#29366;&#28608;&#27963;&#20989;&#25968;&#30340;&#32452;&#21512;&#65292;&#20197;&#33719;&#24471;&#19968;&#20010;&#23436;&#20840;&#31561;&#21464;&#30340;&#23618;&#65292;&#20174;&#32780;&#25512;&#24191;&#21644;&#21152;&#24378;&#20102;&#29616;&#26377;&#30340;&#29305;&#24449;&#23450;&#29702;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#20855;&#26377;&#23454;&#38469;&#30456;&#20851;&#24615;&#30340;&#37325;&#35201;&#24773;&#20917;&#20316;&#20026;&#25512;&#35770;&#12290;&#20107;&#23454;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#26059;&#36716;&#31561;&#21464;&#32593;&#32476;&#21482;&#33021;&#26159;&#19981;&#21464;&#30340;&#65292;&#23601;&#20687;&#23545;&#20110;&#20219;&#20309;&#23545;&#36830;&#36890;&#32039;&#33268;&#32676;&#31561;&#21464;&#30340;&#32593;&#32476;&#19968;&#26679;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#23558;&#25105;&#20204;&#30340;&#32467;&#26524;&#24212;&#29992;&#20110;&#37325;&#35201;&#30340;&#23436;&#20840;&#31561;&#21464;&#32593;&#32476;&#23454;&#20363;&#26102;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Equivariant neural networks have shown improved performance, expressiveness and sample complexity on symmetrical domains. But for some specific symmetries, representations, and choice of coordinates, the most common point-wise activations, such as ReLU, are not equivariant, hence they cannot be employed in the design of equivariant neural networks. The theorem we present in this paper describes all possible combinations of finite-dimensional representations, choice of coordinates and point-wise activations to obtain an exactly equivariant layer, generalizing and strengthening existing characterizations. Notable cases of practical relevance are discussed as corollaries. Indeed, we prove that rotation-equivariant networks can only be invariant, as it happens for any network which is equivariant with respect to connected compact groups. Then, we discuss implications of our findings when applied to important instances of exactly equivariant networks. First, we completely characterize permu
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Apollo&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20302;&#23618;&#35757;&#32451;&#26399;&#38388;&#23398;&#20064;&#39640;&#23618;&#21151;&#33021;&#65292;&#20026;&#28176;&#36827;&#24335;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#20102;&#35838;&#31243;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#21152;&#36895;&#27604;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.09192</link><description>&lt;p&gt;
&#20026;&#28176;&#36827;&#24335;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20934;&#22791;&#35838;&#31243;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Preparing Lessons for Progressive Training on Language Models. (arXiv:2401.09192v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09192
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Apollo&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20302;&#23618;&#35757;&#32451;&#26399;&#38388;&#23398;&#20064;&#39640;&#23618;&#21151;&#33021;&#65292;&#20026;&#28176;&#36827;&#24335;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#20102;&#35838;&#31243;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#21152;&#36895;&#27604;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformers&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#36805;&#36895;&#21457;&#23637;&#24102;&#26469;&#20102;&#36164;&#28304;&#28040;&#32791;&#21644;&#28201;&#23460;&#27668;&#20307;&#25490;&#25918;&#30340;&#22686;&#21152;&#65292;&#36825;&#26159;&#30001;&#20110;&#27169;&#22411;&#35268;&#27169;&#30340;&#22686;&#38271;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#23567;&#27169;&#22411;&#21487;&#20197;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#23545;&#20110;&#26032;&#30340;&#27169;&#22411;&#32467;&#26500;&#21487;&#33021;&#19981;&#36866;&#29992;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#21487;&#33021;&#24456;&#24930;&#65292;&#24182;&#19988;&#28176;&#36827;&#22534;&#21472;&#23618;&#24448;&#24448;&#26080;&#27861;&#23454;&#29616;&#26174;&#33879;&#30340;&#21152;&#36895;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Apollo&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#22312;&#20302;&#23618;&#35757;&#32451;&#26399;&#38388;&#23398;&#20064;&#39640;&#23618;&#21151;&#33021;&#26469;&#20934;&#22791;&#33192;&#32960;&#25805;&#20316;&#30340;&#35838;&#31243;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#20302;&#20540;&#20248;&#20808;&#37319;&#26679;(LVPS)&#26469;&#35757;&#32451;&#19981;&#21516;&#28145;&#24230;&#65292;&#24182;&#24341;&#20837;&#26435;&#37325;&#20849;&#20139;&#20197;&#20419;&#36827;&#39640;&#25928;&#25193;&#23637;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#31181;&#25554;&#20540;&#26041;&#27861;&#26469;&#23454;&#29616;&#31283;&#23450;&#30340;&#27169;&#22411;&#28145;&#24230;&#25193;&#23637;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;Apollo&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#21152;&#36895;&#27604;&#29575;&#65292;&#29978;&#33267;&#8230;&#8230;
&lt;/p&gt;
&lt;p&gt;
The rapid progress of Transformers in artificial intelligence has come at the cost of increased resource consumption and greenhouse gas emissions due to growing model sizes. Prior work suggests using pretrained small models to improve training efficiency, but this approach may not be suitable for new model structures. On the other hand, training from scratch can be slow, and progressively stacking layers often fails to achieve significant acceleration. To address these challenges, we propose a novel method called Apollo, which prep\textbf{a}res lessons for ex\textbf{p}anding \textbf{o}perations by \textbf{l}earning high-\textbf{l}ayer functi\textbf{o}nality during training of low layers. Our approach involves low-value-prioritized sampling (LVPS) to train different depths and weight sharing to facilitate efficient expansion. We also introduce an interpolation method for stable model depth extension. Experiments demonstrate that Apollo achieves state-of-the-art acceleration ratios, even
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20197;&#23562;&#37325;&#20026;&#35270;&#35282;&#35780;&#20272;&#19982;&#35821;&#35328;&#20195;&#29702;&#30340;&#20132;&#20114;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#21152;&#20851;&#27880;&#20851;&#31995;&#21644;&#24773;&#22659;&#22240;&#32032;&#30340;&#20262;&#29702;&#26041;&#27861;&#65292;&#26088;&#22312;&#24110;&#21161;LLM&#25216;&#26415;&#34920;&#29616;&#24471;&#8220;&#22909;&#8221;</title><link>http://arxiv.org/abs/2401.09082</link><description>&lt;p&gt;
&#20160;&#20040;&#26159;&#8220;&#22909;&#8221;&#30340;&#31038;&#20132;&#34892;&#20026;&#32773;&#65311;&#20197;&#23562;&#37325;&#20026;&#35270;&#35282;&#35780;&#20272;&#19982;&#35821;&#35328;&#20195;&#29702;&#30340;&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;
What makes for a 'good' social actor? Using respect as a lens to evaluate interactions with language agents. (arXiv:2401.09082v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09082
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20197;&#23562;&#37325;&#20026;&#35270;&#35282;&#35780;&#20272;&#19982;&#35821;&#35328;&#20195;&#29702;&#30340;&#20132;&#20114;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#21152;&#20851;&#27880;&#20851;&#31995;&#21644;&#24773;&#22659;&#22240;&#32032;&#30340;&#20262;&#29702;&#26041;&#27861;&#65292;&#26088;&#22312;&#24110;&#21161;LLM&#25216;&#26415;&#34920;&#29616;&#24471;&#8220;&#22909;&#8221;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#23545;&#35805;&#20195;&#29702;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#22914;&#20309;&#30830;&#20445;&#23427;&#20204;&#30340;&#34892;&#20026;&#36947;&#24503;&#21644;&#36866;&#24403;&#24615;&#24050;&#32463;&#24341;&#36215;&#20102;&#32039;&#24613;&#20851;&#27880;&#12290;&#20174;&#8220;HHH&#8221;&#26631;&#20934;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#20027;&#35201;&#20307;&#29616;&#22312;&#35753;&#36755;&#20986;&#26356;&#26377;&#24110;&#21161;&#21644;&#35802;&#23454;&#65292;&#24182;&#36991;&#20813;&#26377;&#23475;&#65288;&#26377;&#20559;&#35265;&#12289;&#26377;&#27602;&#25110;&#19981;&#20934;&#30830;&#65289;&#30340;&#38472;&#36848;&#12290;&#34429;&#28982;&#36825;&#31181;&#35821;&#20041;&#28966;&#28857;&#23545;&#20110;&#23558;LLM&#20195;&#29702;&#35270;&#20026;&#32431;&#31929;&#30340;&#20449;&#24687;&#23186;&#20171;&#26159;&#26377;&#29992;&#30340;&#65292;&#20294;&#23427;&#26410;&#33021;&#32771;&#34385;&#21040;&#22312;&#19981;&#21516;&#31038;&#20132;&#24773;&#22659;&#20013;&#65292;&#21516;&#26679;&#30340;&#35805;&#35821;&#21487;&#33021;&#20250;&#26174;&#24471;&#26356;&#25110;&#32773;&#26356;&#23569;&#20882;&#29359;&#25110;&#19981;&#24471;&#20307;&#30340;&#23454;&#38469;&#22240;&#32032;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#21152;&#20851;&#27880;&#20851;&#31995;&#21644;&#24773;&#22659;&#22240;&#32032;&#30340;&#20262;&#29702;&#26041;&#27861;&#65292;&#25506;&#35752;&#20316;&#20026;&#31038;&#20132;&#34892;&#20026;&#32773;&#30340;&#31995;&#32479;&#22914;&#20309;&#22312;&#20132;&#20114;&#20013;&#20197;&#23562;&#37325;&#30340;&#26041;&#24335;&#23545;&#24453;&#20010;&#20307;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#39044;&#35265;&#20102;&#22312;&#24773;&#22659;&#20132;&#20114;&#23618;&#38754;&#19978;&#19968;&#31995;&#21015;&#23578;&#26410;&#34987;&#25506;&#32034;&#30340;&#39118;&#38505;&#65292;&#24182;&#25552;&#20379;&#20102;&#23454;&#29992;&#24314;&#35758;&#65292;&#20197;&#24110;&#21161;LLM&#25216;&#26415;&#34920;&#29616;&#24471;&#8220;&#22909;&#8221;
&lt;/p&gt;
&lt;p&gt;
With the growing popularity of dialogue agents based on large language models (LLMs), urgent attention has been drawn to finding ways to ensure their behaviour is ethical and appropriate. These are largely interpreted in terms of the 'HHH' criteria: making outputs more helpful and honest, and avoiding harmful (biased, toxic, or inaccurate) statements. Whilst this semantic focus is useful from the perspective of viewing LLM agents as mere mediums for information, it fails to account for pragmatic factors that can make the same utterance seem more or less offensive or tactless in different social situations. We propose an approach to ethics that is more centred on relational and situational factors, exploring what it means for a system, as a social actor, to treat an individual respectfully in a (series of) interaction(s). Our work anticipates a set of largely unexplored risks at the level of situated interaction, and offers practical suggestions to help LLM technologies behave as 'good'
&lt;/p&gt;</description></item><item><title>GPTs&#30340;&#23450;&#21046;&#21270;&#20351;&#29992;&#21487;&#33021;&#23384;&#22312;&#38544;&#31169;&#21644;&#23433;&#20840;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2401.09075</link><description>&lt;p&gt;
&#25259;&#30528;&#32501;&#32650;&#30382;&#30340;GPT&#65306;&#23450;&#21046;&#21270;GPTs&#30340;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
GPT in Sheep's Clothing: The Risk of Customized GPTs. (arXiv:2401.09075v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09075
&lt;/p&gt;
&lt;p&gt;
GPTs&#30340;&#23450;&#21046;&#21270;&#20351;&#29992;&#21487;&#33021;&#23384;&#22312;&#38544;&#31169;&#21644;&#23433;&#20840;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
2023&#24180;11&#26376;&#65292;OpenAI&#25512;&#20986;&#20102;&#19968;&#39033;&#26032;&#26381;&#21153;&#65292;&#20801;&#35768;&#29992;&#25143;&#36890;&#36807;&#20351;&#29992;&#29305;&#23450;&#30340;&#25351;&#20196;&#21644;&#30693;&#35782;&#26469;&#25351;&#23548;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#21019;&#24314;&#23450;&#21046;&#29256;&#26412;&#30340;ChatGPT&#65288;GPTs&#65289;&#12290;&#25105;&#20204;&#26088;&#22312;&#25552;&#37266;&#20154;&#20204;&#24847;&#35782;&#21040;GPTs&#21487;&#33021;&#34987;&#24694;&#24847;&#20351;&#29992;&#65292;&#32473;&#29992;&#25143;&#30340;&#38544;&#31169;&#21644;&#23433;&#20840;&#24102;&#26469;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
In November 2023, OpenAI introduced a new service allowing users to create custom versions of ChatGPT (GPTs) by using specific instructions and knowledge to guide the model's behavior. We aim to raise awareness of the fact that GPTs can be used maliciously, posing privacy and security risks to their users.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27169;&#25311;&#35745;&#31639;&#26426;&#20195;&#30721;&#21644;&#31639;&#27861;&#25191;&#34892;&#26041;&#38754;&#36935;&#21040;&#25361;&#25112;&#65292;&#24615;&#33021;&#38543;&#30528;&#20195;&#30721;&#38271;&#24230;&#30340;&#22686;&#21152;&#32780;&#36805;&#36895;&#19979;&#38477;&#12290;&#22312;&#22788;&#29702;&#30701;&#31243;&#24207;&#25110;&#26631;&#20934;&#36807;&#31243;&#26102;&#65292;&#23427;&#20204;&#33021;&#20197;&#20302;&#38169;&#35823;&#29575;&#25353;&#39034;&#24207;&#25191;&#34892;&#25351;&#20196;&#65292;&#20294;&#23545;&#20110;&#22797;&#26434;&#30340;&#31243;&#24207;&#65292;&#29305;&#21035;&#26159;&#21253;&#21547;&#20851;&#38190;&#36335;&#24452;&#21644;&#20887;&#20313;&#25351;&#20196;&#30340;&#31243;&#24207;&#65292;&#27169;&#25311;&#25928;&#26524;&#36739;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36880;&#34892;&#27169;&#25311;&#20195;&#30721;&#25191;&#34892;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.09074</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20195;&#30721;&#27169;&#25311;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Code Simulation Challenges for Large Language Models. (arXiv:2401.09074v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09074
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27169;&#25311;&#35745;&#31639;&#26426;&#20195;&#30721;&#21644;&#31639;&#27861;&#25191;&#34892;&#26041;&#38754;&#36935;&#21040;&#25361;&#25112;&#65292;&#24615;&#33021;&#38543;&#30528;&#20195;&#30721;&#38271;&#24230;&#30340;&#22686;&#21152;&#32780;&#36805;&#36895;&#19979;&#38477;&#12290;&#22312;&#22788;&#29702;&#30701;&#31243;&#24207;&#25110;&#26631;&#20934;&#36807;&#31243;&#26102;&#65292;&#23427;&#20204;&#33021;&#20197;&#20302;&#38169;&#35823;&#29575;&#25353;&#39034;&#24207;&#25191;&#34892;&#25351;&#20196;&#65292;&#20294;&#23545;&#20110;&#22797;&#26434;&#30340;&#31243;&#24207;&#65292;&#29305;&#21035;&#26159;&#21253;&#21547;&#20851;&#38190;&#36335;&#24452;&#21644;&#20887;&#20313;&#25351;&#20196;&#30340;&#31243;&#24207;&#65292;&#27169;&#25311;&#25928;&#26524;&#36739;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36880;&#34892;&#27169;&#25311;&#20195;&#30721;&#25191;&#34892;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#27169;&#25311;&#35745;&#31639;&#26426;&#20195;&#30721;&#21644;&#31639;&#27861;&#25191;&#34892;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#20102;&#30452;&#32447;&#31243;&#24207;&#65292;&#24182;&#23637;&#31034;&#20102;&#24403;&#21069;LLMs&#22312;&#22788;&#29702;&#36825;&#26679;&#31616;&#21333;&#30340;&#31243;&#24207;&#26102;&#34920;&#29616;&#20986;&#30340;&#24615;&#33021;&#36739;&#24046;&#8212;&#8212;&#24615;&#33021;&#38543;&#30528;&#20195;&#30721;&#38271;&#24230;&#30340;&#22686;&#21152;&#32780;&#36805;&#36895;&#19979;&#38477;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;LLMs&#22312;&#27169;&#25311;&#21253;&#21547;&#20851;&#38190;&#36335;&#24452;&#21644;&#20887;&#20313;&#25351;&#20196;&#30340;&#31243;&#24207;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#25490;&#24207;&#31639;&#27861;&#21644;&#23884;&#22871;&#24490;&#29615;&#36229;&#36234;&#20102;&#30452;&#32447;&#31243;&#24207;&#30340;&#27169;&#25311;&#65292;&#24182;&#23637;&#31034;&#20102;&#31243;&#24207;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#30452;&#25509;&#24433;&#21709;LLMs&#27169;&#25311;&#20854;&#25191;&#34892;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;LLMs&#21482;&#26377;&#22312;&#22788;&#29702;&#30701;&#31243;&#24207;&#25110;&#26631;&#20934;&#36807;&#31243;&#26102;&#25165;&#33021;&#20197;&#20302;&#38169;&#35823;&#29575;&#25353;&#39034;&#24207;&#25191;&#34892;&#25351;&#20196;&#12290;LLMs&#30340;&#20195;&#30721;&#27169;&#25311;&#19982;&#23427;&#20204;&#30340;&#27169;&#24335;&#35782;&#21035;&#21644;&#35760;&#24518;&#33021;&#21147;&#23384;&#22312;&#30683;&#30462;&#65306;&#22312;&#35760;&#24518;&#23545;&#20219;&#21153;&#26377;&#23475;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#36880;&#34892;&#27169;&#25311;&#20195;&#30721;&#30340;&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the extent to which Large Language Models (LLMs) can simulate the execution of computer code and algorithms. We begin by looking straight line programs, and show that current LLMs demonstrate poor performance even with such simple programs -- performance rapidly degrades with the length of code. We then investigate the ability of LLMs to simulate programs that contain critical paths and redundant instructions. We also go beyond straight line program simulation with sorting algorithms and nested loops, and we show the computational complexity of a routine directly affects the ability of an LLM to simulate its execution. We observe that LLMs execute instructions sequentially and with a low error margin only for short programs or standard procedures. LLMs' code simulation is in tension with their pattern recognition and memorisation capabilities: on tasks where memorisation is detrimental, we propose a novel prompting method to simulate code execution line by line. Empirica
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#24046;&#20998;&#38544;&#31169;&#32422;&#26463;&#19979;&#22266;&#23450;&#39044;&#31639;&#26465;&#20214;&#19979;&#30340;&#26368;&#20339;&#33218;&#35782;&#21035;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#28385;&#36275;&#24046;&#20998;&#38544;&#31169;&#32422;&#26463;&#30340;&#31574;&#30053;DP-BAI&#65292;&#24182;&#24471;&#21040;&#20102;&#38169;&#35823;&#27010;&#29575;&#30340;&#19978;&#30028;&#21644;&#26368;&#23567;-&#26368;&#22823;&#19979;&#30028;&#30340;&#25351;&#25968;&#34928;&#20943;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2401.09073</link><description>&lt;p&gt;
&#22266;&#23450;&#39044;&#31639;&#24046;&#20998;&#38544;&#31169;&#26368;&#20339;&#33218;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Fixed-Budget Differentially Private Best Arm Identification. (arXiv:2401.09073v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09073
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#24046;&#20998;&#38544;&#31169;&#32422;&#26463;&#19979;&#22266;&#23450;&#39044;&#31639;&#26465;&#20214;&#19979;&#30340;&#26368;&#20339;&#33218;&#35782;&#21035;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#28385;&#36275;&#24046;&#20998;&#38544;&#31169;&#32422;&#26463;&#30340;&#31574;&#30053;DP-BAI&#65292;&#24182;&#24471;&#21040;&#20102;&#38169;&#35823;&#27010;&#29575;&#30340;&#19978;&#30028;&#21644;&#26368;&#23567;-&#26368;&#22823;&#19979;&#30028;&#30340;&#25351;&#25968;&#34928;&#20943;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#24046;&#20998;&#38544;&#31169;&#32422;&#26463;&#19979;&#30740;&#31350;&#20102;&#32447;&#24615;&#36172;&#21338;&#26426;&#20013;&#22266;&#23450;&#39044;&#31639;&#26465;&#20214;&#19979;&#30340;&#26368;&#20339;&#33218;&#35782;&#21035;&#38382;&#39064;&#65292;&#20854;&#20013;&#33218;&#30340;&#22870;&#21169;&#22312;&#21333;&#20301;&#21306;&#38388;&#19978;&#12290;&#32473;&#23450;&#19968;&#20010;&#26377;&#38480;&#30340;&#39044;&#31639;$T$&#21644;&#38544;&#31169;&#21442;&#25968;$\varepsilon&gt;0$&#65292;&#30446;&#26631;&#26159;&#22312;$T$&#20010;&#37319;&#26679;&#36718;&#21518;&#26368;&#23567;&#21270;&#23547;&#25214;&#24179;&#22343;&#20540;&#26368;&#22823;&#30340;&#33218;&#30340;&#38169;&#35823;&#27010;&#29575;&#65292;&#21516;&#26102;&#28385;&#36275;&#20915;&#31574;&#32773;&#31574;&#30053;&#28385;&#36275;&#29305;&#23450;&#30340;$\varepsilon$-&#24046;&#20998;&#38544;&#31169;($\varepsilon$-DP)&#32422;&#26463;&#26465;&#20214;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#8220;&#26368;&#22823;&#32477;&#23545;&#34892;&#21015;&#24335;&#8221;&#21407;&#21017;&#26500;&#24314;&#28385;&#36275;$\varepsilon$-DP&#32422;&#26463;&#30340;&#31574;&#30053;(&#31216;&#20026;DP-BAI)&#65292;&#24182;&#32473;&#20986;&#20854;&#38169;&#35823;&#27010;&#29575;&#30340;&#19978;&#30028;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#38169;&#35823;&#27010;&#29575;&#30340;&#26368;&#23567;-&#26368;&#22823;&#19979;&#30028;&#65292;&#24182;&#35777;&#26126;&#36825;&#20004;&#20010;&#30028;&#22312;$T$&#19978;&#25353;&#25351;&#25968;&#34928;&#20943;&#65292;&#30028;&#20013;&#30340;&#25351;&#25968;&#19982;(a)&#33218;&#30340;&#27425;&#20248;&#38388;&#38553;&#65292;(b)$\varepsilon$&#21644;(c)&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study best arm identification (BAI) in linear bandits in the fixed-budget regime under differential privacy constraints, when the arm rewards are supported on the unit interval. Given a finite budget $T$ and a privacy parameter $\varepsilon&gt;0$, the goal is to minimise the error probability in finding the arm with the largest mean after $T$ sampling rounds, subject to the constraint that the policy of the decision maker satisfies a certain {\em $\varepsilon$-differential privacy} ($\varepsilon$-DP) constraint. We construct a policy satisfying the $\varepsilon$-DP constraint (called {\sc DP-BAI}) by proposing the principle of {\em maximum absolute determinants}, and derive an upper bound on its error probability. Furthermore, we derive a minimax lower bound on the error probability, and demonstrate that the lower and the upper bounds decay exponentially in $T$, with exponents in the two bounds matching order-wise in (a) the sub-optimality gaps of the arms, (b) $\varepsilon$, and (c) t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#24605;&#32771;&#20102;&#35889;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#25581;&#31034;&#20102;&#35889;&#28388;&#27874;&#21644;&#31354;&#38388;&#32858;&#21512;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#35813;&#30740;&#31350;&#21457;&#29616;&#65292;&#35889;&#28388;&#27874;&#22312;&#38544;&#21547;&#22320;&#23558;&#21407;&#22987;&#22270;&#36716;&#25442;&#25104;&#36866;&#24212;&#24615;&#26032;&#22270;&#65292;&#24182;&#26126;&#30830;&#35745;&#31639;&#29992;&#20110;&#31354;&#38388;&#32858;&#21512;&#30340;&#26032;&#22270;&#12290;&#36866;&#24212;&#24615;&#26032;&#22270;&#23637;&#29616;&#20986;&#38750;&#23616;&#37096;&#24615;&#65292;&#24182;&#33021;&#22815;&#21453;&#26144;&#33410;&#28857;&#20043;&#38388;&#30340;&#26631;&#31614;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.09071</link><description>&lt;p&gt;
&#29992;&#31354;&#38388;&#33258;&#36866;&#24212;&#28388;&#27874;&#37325;&#26032;&#24605;&#32771;&#35889;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Rethinking Spectral Graph Neural Networks with Spatially Adaptive Filtering. (arXiv:2401.09071v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09071
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#24605;&#32771;&#20102;&#35889;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#25581;&#31034;&#20102;&#35889;&#28388;&#27874;&#21644;&#31354;&#38388;&#32858;&#21512;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#35813;&#30740;&#31350;&#21457;&#29616;&#65292;&#35889;&#28388;&#27874;&#22312;&#38544;&#21547;&#22320;&#23558;&#21407;&#22987;&#22270;&#36716;&#25442;&#25104;&#36866;&#24212;&#24615;&#26032;&#22270;&#65292;&#24182;&#26126;&#30830;&#35745;&#31639;&#29992;&#20110;&#31354;&#38388;&#32858;&#21512;&#30340;&#26032;&#22270;&#12290;&#36866;&#24212;&#24615;&#26032;&#22270;&#23637;&#29616;&#20986;&#38750;&#23616;&#37096;&#24615;&#65292;&#24182;&#33021;&#22815;&#21453;&#26144;&#33410;&#28857;&#20043;&#38388;&#30340;&#26631;&#31614;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#35889;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#29702;&#35770;&#19978;&#22312;&#35889;&#22495;&#20013;&#26377;&#24456;&#22909;&#30340;&#22522;&#30784;&#65292;&#20294;&#23427;&#20204;&#23454;&#38469;&#19978;&#20381;&#36182;&#20110;&#22810;&#39033;&#24335;&#36924;&#36817;&#65292;&#24847;&#21619;&#30528;&#23427;&#20204;&#19982;&#31354;&#38388;&#22495;&#26377;&#30528;&#28145;&#21051;&#30340;&#32852;&#31995;&#12290;&#30001;&#20110;&#20197;&#21069;&#30340;&#30740;&#31350;&#24456;&#23569;&#20174;&#31354;&#38388;&#35282;&#24230;&#30740;&#31350;&#35889;&#22270;GNN&#65292;&#22240;&#27492;&#23427;&#20204;&#22312;&#31354;&#38388;&#22495;&#30340;&#21487;&#35299;&#37322;&#24615;&#20173;&#28982;&#38590;&#20197;&#25417;&#25720;&#65292;&#20363;&#22914;&#65292;&#35889;&#22270;GNN&#22312;&#31354;&#38388;&#22495;&#20013;&#23454;&#38469;&#19978;&#32534;&#30721;&#20102;&#21738;&#20123;&#20449;&#24687;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#22312;&#35889;&#28388;&#27874;&#21644;&#31354;&#38388;&#32858;&#21512;&#20043;&#38388;&#24314;&#31435;&#20102;&#19968;&#20010;&#29702;&#35770;&#19978;&#30340;&#32852;&#31995;&#65292;&#25581;&#31034;&#20102;&#35889;&#28388;&#27874;&#38544;&#21547;&#22320;&#23558;&#21407;&#22987;&#22270;&#36716;&#25442;&#25104;&#36866;&#24212;&#24615;&#26032;&#22270;&#30340;&#20869;&#22312;&#20132;&#20114;&#20316;&#29992;&#65292;&#24182;&#26126;&#30830;&#22320;&#35745;&#31639;&#29992;&#20110;&#31354;&#38388;&#32858;&#21512;&#30340;&#36866;&#24212;&#24615;&#26032;&#22270;&#12290;&#29702;&#35770;&#21644;&#32463;&#39564;&#30740;&#31350;&#34920;&#26126;&#65292;&#36866;&#24212;&#24615;&#26032;&#22270;&#19981;&#20165;&#34920;&#29616;&#20986;&#38750;&#23616;&#37096;&#24615;&#65292;&#36824;&#33021;&#22815;&#23481;&#32435;&#26377;&#31526;&#21495;&#30340;&#36793;&#26435;&#37325;&#20197;&#21453;&#26144;&#33410;&#28857;&#20043;&#38388;&#30340;&#26631;&#31614;&#19968;&#33268;&#24615;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#21457;&#29616;&#31361;&#26174;&#20102;&#35889;&#22270;GNN&#22312;&#31354;&#38388;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#35282;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Whilst spectral Graph Neural Networks (GNNs) are theoretically well-founded in the spectral domain, their practical reliance on polynomial approximation implies a profound linkage to the spatial domain. As previous studies rarely examine spectral GNNs from the spatial perspective, their spatial-domain interpretability remains elusive, e.g., what information is essentially encoded by spectral GNNs in the spatial domain? In this paper, to answer this question, we establish a theoretical connection between spectral filtering and spatial aggregation, unveiling an intrinsic interaction that spectral filtering implicitly leads the original graph to an adapted new graph, explicitly computed for spatial aggregation. Both theoretical and empirical investigations reveal that the adapted new graph not only exhibits non-locality but also accommodates signed edge weights to reflect label consistency between nodes. These findings thus highlight the interpretable role of spectral GNNs in the spatial 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26032;&#39062;&#30340;&#30693;&#35782;&#22686;&#24378;&#31574;&#30053;&#65292;&#20174;&#20302;&#23618;&#30693;&#35782;&#20013;&#25552;&#21462;&#39640;&#23618;&#37329;&#23383;&#22612;&#29366;&#30693;&#35782;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#22810;&#23618;&#20998;&#23618;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#25512;&#29702;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#25512;&#29702;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.09070</link><description>&lt;p&gt;
&#30693;&#35782;&#37329;&#23383;&#22612;&#65306;&#19968;&#31181;&#29992;&#20110;&#25512;&#24191;&#30693;&#35782;&#22686;&#24378;&#21644;&#25512;&#29702;&#30340;&#26032;&#22411;&#20998;&#23618;&#25512;&#29702;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Knowledge Pyramid: A Novel Hierarchical Reasoning Structure for Generalized Knowledge Augmentation and Inference. (arXiv:2401.09070v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09070
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26032;&#39062;&#30340;&#30693;&#35782;&#22686;&#24378;&#31574;&#30053;&#65292;&#20174;&#20302;&#23618;&#30693;&#35782;&#20013;&#25552;&#21462;&#39640;&#23618;&#37329;&#23383;&#22612;&#29366;&#30693;&#35782;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#22810;&#23618;&#20998;&#23618;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#25512;&#29702;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#25512;&#29702;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#30340;&#25512;&#29702;&#34987;&#35748;&#20026;&#26159;&#20998;&#26512;&#35821;&#20041;&#32593;&#32476;&#30340;&#26377;&#25928;&#25163;&#27573;&#65292;&#24182;&#22312;&#20449;&#24687;&#26816;&#32034;&#12289;&#25512;&#33616;&#12289;&#20915;&#31574;&#21644;&#20154;&#26426;&#20132;&#20114;&#31561;&#39046;&#22495;&#20855;&#26377;&#24040;&#22823;&#30340;&#29992;&#36884;&#12290;&#23427;&#24191;&#27867;&#24212;&#29992;&#20110;&#25512;&#33616;&#12289;&#20915;&#31574;&#12289;&#38382;&#31572;&#12289;&#25628;&#32034;&#31561;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#30740;&#31350;&#20027;&#35201;&#20351;&#29992;KG&#20013;&#30340;&#20302;&#23618;&#30693;&#35782;&#36827;&#34892;&#25512;&#29702;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#25512;&#29702;&#30340;&#27867;&#21270;&#19981;&#36275;&#21644;&#40065;&#26834;&#24615;&#24046;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25512;&#29702;&#26041;&#27861;&#65292;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30693;&#35782;&#22686;&#24378;&#31574;&#30053;&#65292;&#20197;&#25552;&#39640;KG&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#35813;&#26694;&#26550;&#20174;&#20302;&#23618;&#30693;&#35782;&#20013;&#25552;&#21462;&#39640;&#23618;&#37329;&#23383;&#22612;&#29366;&#30693;&#35782;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#22312;&#22810;&#23618;&#23618;&#27425;&#21270;KG&#20013;&#30340;&#25512;&#29702;&#65292;&#26412;&#25991;&#31216;&#20043;&#20026;&#30693;&#35782;&#37329;&#23383;&#22612;&#12290;&#25105;&#20204;&#20351;&#29992;&#25552;&#20986;&#30340;&#26041;&#27861;&#23545;&#19968;&#20123;&#21307;&#30103;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25552;&#20986;&#30340;&#30693;&#35782;&#37329;&#23383;&#22612;&#25913;&#36827;&#20102;&#30693;&#35782;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graph (KG) based reasoning has been regarded as an effective means for the analysis of semantic networks and is of great usefulness in areas of information retrieval, recommendation, decision-making, and man-machine interaction. It is widely used in recommendation, decision-making, question-answering, search, and other fields. However, previous studies mainly used low-level knowledge in the KG for reasoning, which may result in insufficient generalization and poor robustness of reasoning. To this end, this paper proposes a new inference approach using a novel knowledge augmentation strategy to improve the generalization capability of KG. This framework extracts high-level pyramidal knowledge from low-level knowledge and applies it to reasoning in a multi-level hierarchical KG, called knowledge pyramid in this paper. We tested some medical data sets using the proposed approach, and the experimental results show that the proposed knowledge pyramid has improved the knowledge inf
&lt;/p&gt;</description></item><item><title>DTMM&#26159;&#19968;&#20010;&#24211;&#65292;&#26088;&#22312;&#22312;&#24369;&#29289;&#32852;&#32593;&#35774;&#22791;&#19978;&#39640;&#25928;&#37096;&#32626;&#21644;&#25191;&#34892;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#20043;&#21069;&#30340;&#35299;&#20915;&#26041;&#26696;&#26080;&#27861;&#21516;&#26102;&#23454;&#29616;&#22312;&#19981;&#25439;&#23475;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#28145;&#24230;&#21387;&#32553;&#27169;&#22411;&#21644;&#39640;&#25928;&#25191;&#34892;&#30340;&#30446;&#26631;&#65292;&#32780;DTMM&#36890;&#36807;&#20462;&#21098;&#21333;&#20803;&#36873;&#25321;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.09068</link><description>&lt;p&gt;
DTMM&#65306;&#21033;&#29992;&#20462;&#21098;&#22312;&#26497;&#24369;&#30340;&#29289;&#32852;&#32593;&#35774;&#22791;&#19978;&#37096;&#32626;TinyML&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DTMM: Deploying TinyML Models on Extremely Weak IoT Devices with Pruning. (arXiv:2401.09068v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09068
&lt;/p&gt;
&lt;p&gt;
DTMM&#26159;&#19968;&#20010;&#24211;&#65292;&#26088;&#22312;&#22312;&#24369;&#29289;&#32852;&#32593;&#35774;&#22791;&#19978;&#39640;&#25928;&#37096;&#32626;&#21644;&#25191;&#34892;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#20043;&#21069;&#30340;&#35299;&#20915;&#26041;&#26696;&#26080;&#27861;&#21516;&#26102;&#23454;&#29616;&#22312;&#19981;&#25439;&#23475;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#28145;&#24230;&#21387;&#32553;&#27169;&#22411;&#21644;&#39640;&#25928;&#25191;&#34892;&#30340;&#30446;&#26631;&#65292;&#32780;DTMM&#36890;&#36807;&#20462;&#21098;&#21333;&#20803;&#36873;&#25321;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
DTMM&#26159;&#19968;&#20010;&#20026;&#24369;&#29289;&#32852;&#32593;&#35774;&#22791;&#65288;&#22914;&#24494;&#25511;&#21046;&#22120;&#21333;&#20803;&#65289;&#19978;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39640;&#25928;&#37096;&#32626;&#21644;&#25191;&#34892;&#32780;&#35774;&#35745;&#30340;&#24211;&#12290;&#35774;&#35745;DTMM&#30340;&#21160;&#26426;&#26469;&#33258;&#20110;&#26032;&#20852;&#39046;&#22495;&#30340;&#23567;&#22411;&#26426;&#22120;&#23398;&#20064;&#65288;TinyML&#65289;&#65292;&#23427;&#25506;&#32034;&#23558;&#26426;&#22120;&#23398;&#20064;&#25193;&#23637;&#21040;&#35768;&#22810;&#20302;&#31471;&#29289;&#32852;&#32593;&#35774;&#22791;&#20197;&#23454;&#29616;&#26222;&#36941;&#26234;&#33021;&#12290;&#30001;&#20110;&#23884;&#20837;&#24335;&#35774;&#22791;&#30340;&#33021;&#21147;&#36739;&#24369;&#65292;&#38656;&#35201;&#22312;&#37096;&#32626;&#20043;&#21069;&#36890;&#36807;&#20462;&#21098;&#36275;&#22815;&#30340;&#26435;&#37325;&#26469;&#21387;&#32553;&#27169;&#22411;&#12290;&#23613;&#31649;&#20462;&#21098;&#24050;&#22312;&#35768;&#22810;&#35745;&#31639;&#24179;&#21488;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#20462;&#21098;&#26041;&#27861;&#22312;MCUs&#19978;&#38754;&#20020;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#30340;&#21152;&#21095;&#65306;&#38656;&#35201;&#22312;&#19981;&#26174;&#33879;&#25439;&#23475;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#28145;&#24230;&#21387;&#32553;&#27169;&#22411;&#65292;&#24182;&#19988;&#20462;&#21098;&#21518;&#30340;&#27169;&#22411;&#22312;&#25191;&#34892;&#25928;&#29575;&#19978;&#24212;&#20855;&#22791;&#39640;&#25928;&#24615;&#12290;&#30446;&#21069;&#30340;&#35299;&#20915;&#26041;&#26696;&#21482;&#33021;&#23454;&#29616;&#20854;&#20013;&#19968;&#20010;&#30446;&#26631;&#65292;&#32780;&#19981;&#33021;&#21516;&#26102;&#23454;&#29616;&#20004;&#32773;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20462;&#21098;&#21518;&#30340;&#27169;&#22411;&#22312;MCUs&#19978;&#20855;&#26377;&#39640;&#25928;&#37096;&#32626;&#21644;&#25191;&#34892;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20855;&#26377;&#20462;&#21098;&#21333;&#20803;&#36873;&#25321;&#30340;DTMM&#12290;
&lt;/p&gt;
&lt;p&gt;
DTMM is a library designed for efficient deployment and execution of machine learning models on weak IoT devices such as microcontroller units (MCUs). The motivation for designing DTMM comes from the emerging field of tiny machine learning (TinyML), which explores extending the reach of machine learning to many low-end IoT devices to achieve ubiquitous intelligence. Due to the weak capability of embedded devices, it is necessary to compress models by pruning enough weights before deploying. Although pruning has been studied extensively on many computing platforms, two key issues with pruning methods are exacerbated on MCUs: models need to be deeply compressed without significantly compromising accuracy, and they should perform efficiently after pruning. Current solutions only achieve one of these objectives, but not both. In this paper, we find that pruned models have great potential for efficient deployment and execution on MCUs. Therefore, we propose DTMM with pruning unit selection,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;HSIC-Bottleneck&#27491;&#20132;&#21270;&#21644;&#24179;&#22343;&#35282;&#23884;&#20837;&#65292;&#23454;&#29616;&#20102;&#23545;&#25345;&#32493;&#23398;&#20064;&#20013;&#36951;&#24536;&#38382;&#39064;&#30340;&#35299;&#20915;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#20351;&#29992;&#20808;&#21069;&#20219;&#21153;&#30340;&#35757;&#32451;&#25968;&#25454;&#19988;&#27169;&#22411;&#22823;&#23567;&#30456;&#23545;&#24658;&#23450;&#30340;&#26465;&#20214;&#19979;&#65292;&#36890;&#36807;&#38480;&#21046;&#36880;&#23618;&#21442;&#25968;&#35206;&#30422;&#21644;&#20915;&#31574;&#36793;&#30028;&#30072;&#21464;&#26469;&#36991;&#20813;&#36951;&#24536;&#12290;</title><link>http://arxiv.org/abs/2401.09067</link><description>&lt;p&gt;
&#36890;&#36807;HSIC-Bottleneck&#27491;&#20132;&#21270;&#21644;&#24179;&#22343;&#35282;&#23884;&#20837;&#23454;&#29616;&#25345;&#32493;&#23398;&#20064;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;
Towards Continual Learning Desiderata via HSIC-Bottleneck Orthogonalization and Equiangular Embedding. (arXiv:2401.09067v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09067
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;HSIC-Bottleneck&#27491;&#20132;&#21270;&#21644;&#24179;&#22343;&#35282;&#23884;&#20837;&#65292;&#23454;&#29616;&#20102;&#23545;&#25345;&#32493;&#23398;&#20064;&#20013;&#36951;&#24536;&#38382;&#39064;&#30340;&#35299;&#20915;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#20351;&#29992;&#20808;&#21069;&#20219;&#21153;&#30340;&#35757;&#32451;&#25968;&#25454;&#19988;&#27169;&#22411;&#22823;&#23567;&#30456;&#23545;&#24658;&#23450;&#30340;&#26465;&#20214;&#19979;&#65292;&#36890;&#36807;&#38480;&#21046;&#36880;&#23618;&#21442;&#25968;&#35206;&#30422;&#21644;&#20915;&#31574;&#36793;&#30028;&#30072;&#21464;&#26469;&#36991;&#20813;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39034;&#24207;&#20219;&#21153;&#35757;&#32451;&#20013;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23481;&#26131;&#36973;&#21463;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#21508;&#31181;&#25345;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#26679;&#26412;&#32531;&#20914;&#21306;&#21644;/&#25110;&#32593;&#32476;&#25193;&#23637;&#65292;&#20197;&#24179;&#34913;&#27169;&#22411;&#30340;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#65292;&#20294;&#36825;&#20250;&#25439;&#23475;&#20854;&#23454;&#38469;&#20215;&#20540;&#65292;&#22240;&#20026;&#28041;&#21450;&#21040;&#38544;&#31169;&#21644;&#20869;&#23384;&#38382;&#39064;&#12290;&#30456;&#21453;&#65292;&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#20010;&#20005;&#26684;&#20294;&#29616;&#23454;&#30340;&#35774;&#32622;&#65292;&#21363;&#20197;&#21069;&#20219;&#21153;&#30340;&#35757;&#32451;&#25968;&#25454;&#19981;&#21487;&#29992;&#65292;&#19988;&#22312;&#39034;&#24207;&#35757;&#32451;&#26399;&#38388;&#27169;&#22411;&#30340;&#22823;&#23567;&#20445;&#25345;&#30456;&#23545;&#24658;&#23450;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#26679;&#30340;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#24565;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#23558;&#36951;&#24536;&#24402;&#22240;&#20110;&#36880;&#23618;&#21442;&#25968;&#35206;&#30422;&#21644;&#30001;&#27492;&#20135;&#29983;&#30340;&#20915;&#31574;&#36793;&#30028;&#30072;&#21464;&#12290;&#36825;&#36890;&#36807;&#20004;&#20010;&#20851;&#38190;&#32452;&#20214;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#23454;&#29616;&#65306;HSIC-Bottleneck&#27491;&#20132;&#21270;&#65288;HBO&#65289;&#22312;&#27491;&#20132;&#31354;&#38388;&#20013;&#23454;&#29616;&#38750;&#35206;&#30422;&#21442;&#25968;&#30340;&#26356;&#26032;&#65292;&#36890;&#36807;Hilbert-Schmidt&#29420;&#31435;&#24615;&#20934;&#21017;&#36827;&#34892;&#20013;&#20171;&#65307;&#32780;&#24179;&#22343;&#35282;&#23884;&#20837;&#65288;EAE&#65289;&#21017;&#22686;&#24378;&#20102;&#20915;&#31574;&#36793;&#30028;&#30340;&#36866;&#24212;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks are susceptible to catastrophic forgetting when trained on sequential tasks. Various continual learning (CL) methods often rely on exemplar buffers or/and network expansion for balancing model stability and plasticity, which, however, compromises their practical value due to privacy and memory concerns. Instead, this paper considers a strict yet realistic setting, where the training data from previous tasks is unavailable and the model size remains relatively constant during sequential training. To achieve such desiderata, we propose a conceptually simple yet effective method that attributes forgetting to layer-wise parameter overwriting and the resulting decision boundary distortion. This is achieved by the synergy between two key components: HSIC-Bottleneck Orthogonalization (HBO) implements non-overwritten parameter updates mediated by Hilbert-Schmidt independence criterion in an orthogonal space and EquiAngular Embedding (EAE) enhances decision boundary adaptat
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23545;&#22810;&#31181;&#26368;&#20808;&#36827;&#30340;LLM&#30340;&#25512;&#29702;&#33021;&#21147;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#34920;&#29616;&#27424;&#20339;&#65292;&#36825;&#25361;&#25112;&#20102;&#23427;&#20204;&#22312;&#22788;&#29702;&#24120;&#35782;&#35268;&#21010;&#30340;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.09042</link><description>&lt;p&gt;
LLM&#23545;&#20110;&#20851;&#31995;&#25512;&#29702;&#30340;&#23454;&#29616;&#31243;&#24230;&#26377;&#22810;&#36828;&#65311;
&lt;/p&gt;
&lt;p&gt;
LLMs for Relational Reasoning: How Far are We?. (arXiv:2401.09042v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09042
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23545;&#22810;&#31181;&#26368;&#20808;&#36827;&#30340;LLM&#30340;&#25512;&#29702;&#33021;&#21147;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#34920;&#29616;&#27424;&#20339;&#65292;&#36825;&#25361;&#25112;&#20102;&#23427;&#20204;&#22312;&#22788;&#29702;&#24120;&#35782;&#35268;&#21010;&#30340;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36890;&#36807;&#22312;&#24191;&#27867;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20174;&#32780;&#38761;&#26032;&#20102;&#35768;&#22810;&#39046;&#22495;&#65288;&#20363;&#22914;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#36719;&#20214;&#24037;&#31243;&#31561;&#65289;&#12290;&#20026;&#20102;&#23454;&#29616;&#24378;&#22823;&#19988;&#36890;&#29992;&#30340;&#20154;&#24037;&#26234;&#33021;&#65292;&#20154;&#20204;&#23545;LLM&#30340;&#25512;&#29702;&#33021;&#21147;&#20135;&#29983;&#20102;&#27987;&#21402;&#30340;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30740;&#31350;&#37319;&#29992;&#30340;&#25991;&#26412;&#21644;&#25968;&#20540;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;&#30456;&#23545;&#27973;&#26174;&#31616;&#21333;&#65292;&#20165;&#20165;&#36890;&#36807;&#22312;&#36825;&#20123;&#22522;&#20934;&#19978;&#21462;&#24471;&#31215;&#26497;&#32467;&#26524;&#38590;&#20197;&#24471;&#20986;LLM&#20855;&#26377;&#24378;&#22823;&#25512;&#29702;&#33021;&#21147;&#30340;&#32467;&#35770;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#21162;&#21147;&#36890;&#36807;&#22312;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#19978;&#35780;&#20272;LLM&#30340;&#24615;&#33021;&#65292;&#35777;&#26126;LLM&#22312;&#35299;&#20915;&#38656;&#35201;&#24120;&#35782;&#35268;&#21010;&#30340;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#22522;&#20110;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;&#65288;ILP&#65289;&#22522;&#20934;&#27979;&#35797;&#23545;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;LLM&#30340;&#25512;&#29702;&#33021;&#21147;&#36827;&#34892;&#20102;&#28145;&#20837;&#35780;&#20272;&#65292;&#36825;&#19968;&#22522;&#20934;&#27979;&#35797;&#34987;&#24191;&#27867;&#35748;&#21487;&#20026;&#23545;&#25512;&#29702;&#33021;&#21147;&#30340;&#20195;&#34920;&#24615;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have revolutionized many areas (e.g. natural language processing, software engineering, etc.) by achieving state-of-the-art performance on extensive downstream tasks. Aiming to achieve robust and general artificial intelligence, there has been a surge of interest in investigating the reasoning ability of the LLMs. Whereas the textual and numerical reasoning benchmarks adopted by previous works are rather shallow and simple, it is hard to conclude that the LLMs possess strong reasoning ability by merely achieving positive results on these benchmarks. Recent efforts have demonstrated that the LLMs are poor at solving sequential decision-making problems that require common-sense planning by evaluating their performance on the reinforcement learning benchmarks. In this work, we conduct an in-depth assessment of several state-of-the-art LLMs' reasoning ability based on the inductive logic programming (ILP) benchmark, which is broadly recognized as a representati
&lt;/p&gt;</description></item><item><title>UOEP&#26159;&#19968;&#31181;&#29992;&#25143;&#23548;&#21521;&#30340;&#25506;&#32034;&#31574;&#30053;&#65292;&#38024;&#23545;&#25512;&#33616;&#31995;&#32479;&#20013;&#19981;&#21516;&#27963;&#36291;&#27700;&#24179;&#30340;&#29992;&#25143;&#32676;&#20307;&#23454;&#29616;&#32454;&#31890;&#24230;&#25506;&#32034;&#65292;&#20197;&#22686;&#24378;&#29992;&#25143;&#30340;&#38271;&#26399;&#20307;&#39564;&#12290;</title><link>http://arxiv.org/abs/2401.09034</link><description>&lt;p&gt;
UOEP: &#29992;&#25143;&#23548;&#21521;&#30340;&#25506;&#32034;&#31574;&#30053;&#20197;&#22686;&#24378;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#38271;&#26399;&#29992;&#25143;&#20307;&#39564;
&lt;/p&gt;
&lt;p&gt;
UOEP: User-Oriented Exploration Policy for Enhancing Long-Term User Experiences in Recommender Systems. (arXiv:2401.09034v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09034
&lt;/p&gt;
&lt;p&gt;
UOEP&#26159;&#19968;&#31181;&#29992;&#25143;&#23548;&#21521;&#30340;&#25506;&#32034;&#31574;&#30053;&#65292;&#38024;&#23545;&#25512;&#33616;&#31995;&#32479;&#20013;&#19981;&#21516;&#27963;&#36291;&#27700;&#24179;&#30340;&#29992;&#25143;&#32676;&#20307;&#23454;&#29616;&#32454;&#31890;&#24230;&#25506;&#32034;&#65292;&#20197;&#22686;&#24378;&#29992;&#25143;&#30340;&#38271;&#26399;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#24050;&#32463;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#26377;&#25928;&#22320;&#25506;&#32034;&#29992;&#25143;&#30340;&#20852;&#36259;&#65292;&#20197;&#25552;&#21319;&#29992;&#25143;&#30340;&#38271;&#26399;&#20307;&#39564;&#12290;&#28982;&#32780;&#65292;&#29616;&#20195;&#25512;&#33616;&#31995;&#32479;&#20013;&#23384;&#22312;&#30528;&#25968;&#21315;&#19975;&#20010;&#39033;&#30446;&#20043;&#38388;&#30340;&#19981;&#21516;&#29992;&#25143;&#34892;&#20026;&#27169;&#24335;&#65292;&#36825;&#22686;&#21152;&#20102;&#25506;&#32034;&#30340;&#38590;&#24230;&#12290;&#20363;&#22914;&#65292;&#19981;&#21516;&#27963;&#36291;&#27700;&#24179;&#30340;&#29992;&#25143;&#34892;&#20026;&#38656;&#35201;&#19981;&#21516;&#24378;&#24230;&#30340;&#25506;&#32034;&#65292;&#32780;&#20043;&#21069;&#30340;&#30740;&#31350;&#24448;&#24448;&#24573;&#35270;&#20102;&#36825;&#19968;&#26041;&#38754;&#65292;&#23545;&#25152;&#26377;&#29992;&#25143;&#24212;&#29992;&#32479;&#19968;&#30340;&#25506;&#32034;&#31574;&#30053;&#65292;&#26368;&#32456;&#25439;&#23475;&#20102;&#29992;&#25143;&#30340;&#38271;&#26399;&#20307;&#39564;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#25143;&#23548;&#21521;&#30340;&#25506;&#32034;&#31574;&#30053;&#65288;UOEP&#65289;&#65292;&#19968;&#31181;&#22312;&#29992;&#25143;&#32676;&#20307;&#20013;&#23454;&#29616;&#32454;&#31890;&#24230;&#25506;&#32034;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#26500;&#24314;&#20102;&#19968;&#20010;&#20998;&#24067;&#24335;&#35780;&#35770;&#23478;&#65292;&#23427;&#20801;&#35768;&#22312;&#19981;&#21516;&#30340;&#32047;&#31215;&#22870;&#21169;&#21453;&#39304;&#30340;&#20998;&#20301;&#25968;&#27700;&#24179;&#19979;&#36827;&#34892;&#31574;&#30053;&#20248;&#21270;&#65292;&#34920;&#31034;&#20855;&#26377;&#19981;&#21516;&#27963;&#21160;&#27700;&#24179;&#30340;&#29992;&#25143;&#32676;&#20307;&#12290;&#22312;&#36825;&#20010;&#35780;&#35770;&#23478;&#30340;&#25351;&#23548;&#19979;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#32452;&#19981;&#21516;&#30340;&#28436;&#21592;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) has gained traction for enhancing user long-term experiences in recommender systems by effectively exploring users' interests. However, modern recommender systems exhibit distinct user behavioral patterns among tens of millions of items, which increases the difficulty of exploration. For example, user behaviors with different activity levels require varying intensity of exploration, while previous studies often overlook this aspect and apply a uniform exploration strategy to all users, which ultimately hurts user experiences in the long run. To address these challenges, we propose User-Oriented Exploration Policy (UOEP), a novel approach facilitating fine-grained exploration among user groups. We first construct a distributional critic which allows policy optimization under varying quantile levels of cumulative reward feedbacks from users, representing user groups with varying activity levels. Guided by this critic, we devise a population of distinct actors 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;MRI&#33041;&#32959;&#30244;&#20998;&#32423;&#30340;&#36328;&#27169;&#24577;&#24341;&#23548;&#36741;&#21161;&#22810;&#27169;&#24577;&#23398;&#20064;&#19982;&#21452;&#37325;&#27880;&#24847;&#26426;&#21046;&#65292;&#20197;&#25552;&#39640;&#33041;&#32959;&#30244;&#35786;&#26029;&#30340;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.09029</link><description>&lt;p&gt;
MRI&#33041;&#32959;&#30244;&#20998;&#32423;&#30340;&#36328;&#27169;&#24577;&#24341;&#23548;&#36741;&#21161;&#22810;&#27169;&#24577;&#23398;&#20064;&#19982;&#21452;&#37325;&#27880;&#24847;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Cross-modality Guidance-aided Multi-modal Learning with Dual Attention for MRI Brain Tumor Grading. (arXiv:2401.09029v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09029
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;MRI&#33041;&#32959;&#30244;&#20998;&#32423;&#30340;&#36328;&#27169;&#24577;&#24341;&#23548;&#36741;&#21161;&#22810;&#27169;&#24577;&#23398;&#20064;&#19982;&#21452;&#37325;&#27880;&#24847;&#26426;&#21046;&#65292;&#20197;&#25552;&#39640;&#33041;&#32959;&#30244;&#35786;&#26029;&#30340;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;&#32959;&#30244;&#26159;&#20840;&#29699;&#26368;&#33268;&#21629;&#30340;&#30284;&#30151;&#20043;&#19968;&#65292;&#22312;&#20799;&#31461;&#21644;&#32769;&#24180;&#20154;&#20013;&#38750;&#24120;&#24120;&#35265;&#12290;&#20934;&#30830;&#35782;&#21035;&#26089;&#26399;&#32959;&#30244;&#30340;&#31867;&#22411;&#21644;&#32423;&#21035;&#22312;&#36873;&#25321;&#31934;&#30830;&#27835;&#30103;&#26041;&#26696;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#19981;&#21516;&#24207;&#21015;&#30340;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;MRI&#65289;&#21327;&#35758;&#20026;&#20020;&#24202;&#21307;&#29983;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#30683;&#30462;&#20449;&#24687;&#26469;&#35782;&#21035;&#32959;&#30244;&#21306;&#22495;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#25454;&#37327;&#22823;&#19988;&#33041;&#32959;&#30244;&#31867;&#22411;&#22810;&#26679;&#65292;&#25163;&#21160;&#35780;&#20272;&#32791;&#26102;&#19988;&#23481;&#26131;&#20986;&#38169;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#24320;&#21457;MRI&#33258;&#21160;&#21270;&#33041;&#32959;&#30244;&#35786;&#26029;&#26041;&#27861;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#21333;&#27169;&#24577;&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#26377;&#38480;&#65292;&#20854;&#24615;&#33021;&#22312;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#24046;&#24322;&#24456;&#22823;&#65292;&#24120;&#29992;&#30340;&#27169;&#24577;&#34701;&#21512;&#26041;&#27861;&#20250;&#24341;&#20837;&#28508;&#22312;&#22122;&#22768;&#65292;&#23548;&#33268;&#24615;&#33021;&#20005;&#37325;&#19979;&#38477;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36328;&#27169;&#24577;&#24341;&#23548;&#36741;&#21161;&#22810;&#27169;&#24577;&#23398;&#20064;&#19982;&#21452;&#37325;&#27880;&#24847;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Brain tumor represents one of the most fatal cancers around the world, and is very common in children and the elderly. Accurate identification of the type and grade of tumor in the early stages plays an important role in choosing a precise treatment plan. The Magnetic Resonance Imaging (MRI) protocols of different sequences provide clinicians with important contradictory information to identify tumor regions. However, manual assessment is time-consuming and error-prone due to big amount of data and the diversity of brain tumor types. Hence, there is an unmet need for MRI automated brain tumor diagnosis. We observe that the predictive capability of uni-modality models is limited and their performance varies widely across modalities, and the commonly used modality fusion methods would introduce potential noise, which results in significant performance degradation. To overcome these challenges, we propose a novel cross-modality guidance-aided multi-modal learning with dual attention for a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#20998;&#27573;&#20219;&#24847;&#27169;&#22411;&#65288;SAM&#65289;&#23454;&#29616;&#20102;&#20809;&#23398;&#36965;&#24863;&#22270;&#20687;&#21644;&#22320;&#22270;&#25968;&#25454;&#20043;&#38388;&#30340;&#26080;&#30417;&#30563;&#22810;&#27169;&#24577;&#21464;&#21270;&#26816;&#27979;&#12290;&#36890;&#36807;SAM&#30340;&#38646;&#26679;&#26412;&#36801;&#31227;&#33021;&#21147;&#65292;&#21487;&#20197;&#33719;&#24471;&#39640;&#36136;&#37327;&#30340;&#20809;&#23398;&#22270;&#20687;&#20998;&#21106;&#22320;&#22270;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#20004;&#31181;&#31574;&#30053;&#26469;&#25351;&#23548;&#20998;&#21106;&#36807;&#31243;&#65292;&#20174;&#32780;&#22312;&#22303;&#22320;&#35206;&#30422;&#21464;&#21270;&#21644;&#26032;&#22303;&#22320;&#35206;&#30422;&#23545;&#35937;&#35782;&#21035;&#26041;&#38754;&#20855;&#26377;&#33391;&#22909;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.09019</link><description>&lt;p&gt;
&#36890;&#36807;&#20998;&#27573;&#20219;&#24847;&#27169;&#22411;&#65288;SAM&#65289;&#22312;&#20809;&#23398;&#36965;&#24863;&#22270;&#20687;&#21644;&#22320;&#22270;&#25968;&#25454;&#20043;&#38388;&#36827;&#34892;&#21464;&#21270;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Change Detection Between Optical Remote Sensing Imagery and Map Data via Segment Anything Model (SAM). (arXiv:2401.09019v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09019
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#20998;&#27573;&#20219;&#24847;&#27169;&#22411;&#65288;SAM&#65289;&#23454;&#29616;&#20102;&#20809;&#23398;&#36965;&#24863;&#22270;&#20687;&#21644;&#22320;&#22270;&#25968;&#25454;&#20043;&#38388;&#30340;&#26080;&#30417;&#30563;&#22810;&#27169;&#24577;&#21464;&#21270;&#26816;&#27979;&#12290;&#36890;&#36807;SAM&#30340;&#38646;&#26679;&#26412;&#36801;&#31227;&#33021;&#21147;&#65292;&#21487;&#20197;&#33719;&#24471;&#39640;&#36136;&#37327;&#30340;&#20809;&#23398;&#22270;&#20687;&#20998;&#21106;&#22320;&#22270;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#20004;&#31181;&#31574;&#30053;&#26469;&#25351;&#23548;&#20998;&#21106;&#36807;&#31243;&#65292;&#20174;&#32780;&#22312;&#22303;&#22320;&#35206;&#30422;&#21464;&#21270;&#21644;&#26032;&#22303;&#22320;&#35206;&#30422;&#23545;&#35937;&#35782;&#21035;&#26041;&#38754;&#20855;&#26377;&#33391;&#22909;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#22810;&#27169;&#24577;&#21464;&#21270;&#26816;&#27979;&#23545;&#20110;&#26102;&#38388;&#25935;&#24863;&#20219;&#21153;&#21644;&#20840;&#38754;&#30340;&#22810;&#26102;&#30456;&#22320;&#29699;&#30417;&#27979;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20809;&#23398;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#21644;OpenStreetMap&#65288;OSM&#65289;&#25968;&#25454;&#20043;&#38388;&#30340;&#26080;&#30417;&#30563;&#22810;&#27169;&#24577;&#21464;&#21270;&#26816;&#27979;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;-&#20998;&#27573;&#20219;&#24847;&#27169;&#22411;&#65288;SAM&#65289;&#26469;&#35299;&#20915;&#25105;&#20204;&#30340;&#20219;&#21153;&#12290;&#36890;&#36807;&#21033;&#29992;SAM&#30340;&#20986;&#33394;&#30340;&#38646;&#26679;&#26412;&#36801;&#31227;&#33021;&#21147;&#65292;&#21487;&#20197;&#33719;&#24471;&#20809;&#23398;&#22270;&#20687;&#30340;&#39640;&#36136;&#37327;&#20998;&#21106;&#22320;&#22270;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21487;&#20197;&#30452;&#25509;&#22312;&#25152;&#35859;&#30340;&#20998;&#21106;&#22495;&#27604;&#36739;&#36825;&#20004;&#31181;&#24322;&#26500;&#25968;&#25454;&#24418;&#24335;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#31574;&#30053;&#26469;&#25351;&#23548;SAM&#30340;&#20998;&#21106;&#36807;&#31243;&#65306;'&#26080;&#25552;&#31034;'&#21644;'&#26694;/&#33945;&#29256;&#25552;&#31034;'&#26041;&#27861;&#12290;&#36825;&#20004;&#31181;&#31574;&#30053;&#20998;&#21035;&#29992;&#20110;&#26816;&#27979;&#19968;&#33324;&#24773;&#20917;&#19979;&#30340;&#22303;&#22320;&#35206;&#30422;&#21464;&#21270;&#21644;&#35782;&#21035;&#29616;&#26377;&#32972;&#26223;&#20013;&#30340;&#26032;&#22303;&#22320;&#35206;&#30422;&#23545;&#35937;&#12290;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#21464;&#21270;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#26041;&#38754;&#20855;&#26377;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised multimodal change detection is pivotal for time-sensitive tasks and comprehensive multi-temporal Earth monitoring. In this study, we explore unsupervised multimodal change detection between two key remote sensing data sources: optical high-resolution imagery and OpenStreetMap (OSM) data. Specifically, we propose to utilize the vision foundation model Segmentation Anything Model (SAM), for addressing our task. Leveraging SAM's exceptional zero-shot transfer capability, high-quality segmentation maps of optical images can be obtained. Thus, we can directly compare these two heterogeneous data forms in the so-called segmentation domain. We then introduce two strategies for guiding SAM's segmentation process: the 'no-prompt' and 'box/mask prompt' methods. The two strategies are designed to detect land-cover changes in general scenarios and to identify new land-cover objects within existing backgrounds, respectively. Experimental results on three datasets indicate that the prop
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#65292;&#29305;&#21035;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#36879;&#26126;&#24230;&#21644;&#35299;&#37322;&#33021;&#21147;&#19981;&#36275;&#65292;&#25552;&#20986;AI&#31995;&#32479;&#19981;&#20165;&#38656;&#35201;&#39044;&#27979;&#33021;&#21147;&#65292;&#36824;&#38656;&#35201;&#25552;&#20379;&#33391;&#22909;&#30340;&#35299;&#37322;&#33021;&#21147;&#21644;&#27934;&#23519;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.09011</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#24402;&#32435;&#33021;&#21147;&#19981;&#36275;&#65292;&#32570;&#20047;&#33391;&#22909;&#30340;&#35299;&#37322;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Inductive Models for Artificial Intelligence Systems are Insufficient without Good Explanations. (arXiv:2401.09011v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09011
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#65292;&#29305;&#21035;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#36879;&#26126;&#24230;&#21644;&#35299;&#37322;&#33021;&#21147;&#19981;&#36275;&#65292;&#25552;&#20986;AI&#31995;&#32479;&#19981;&#20165;&#38656;&#35201;&#39044;&#27979;&#33021;&#21147;&#65292;&#36824;&#38656;&#35201;&#25552;&#20379;&#33391;&#22909;&#30340;&#35299;&#37322;&#33021;&#21147;&#21644;&#27934;&#23519;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#65292;&#29305;&#21035;&#26159;&#28145;&#24230;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANNs&#65289;&#30340;&#23616;&#38480;&#24615;&#65292;&#23427;&#20204;&#22312;&#36924;&#36817;&#22797;&#26434;&#20989;&#25968;&#26041;&#38754;&#26377;&#25928;&#65292;&#20294;&#24120;&#24120;&#32570;&#20047;&#36879;&#26126;&#24230;&#21644;&#35299;&#37322;&#33021;&#21147;&#12290;&#26412;&#25991;&#24378;&#35843;&#20102;&#24402;&#32435;&#38382;&#39064;&#65306;&#21363;&#36807;&#21435;&#30340;&#35266;&#23519;&#19981;&#19968;&#23450;&#33021;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#65292;&#36825;&#26159;ML&#27169;&#22411;&#22312;&#36973;&#36935;&#26032;&#30340;&#12289;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#20027;&#24352;&#37325;&#35201;&#30340;&#19981;&#20165;&#26159;&#20570;&#20986;&#39044;&#27979;&#65292;&#36824;&#35201;&#25552;&#20379;&#33391;&#22909;&#30340;&#35299;&#37322;&#65292;&#32780;&#24403;&#21069;&#27169;&#22411;&#24448;&#24448;&#26410;&#33021;&#20570;&#21040;&#36825;&#19968;&#28857;&#12290;&#23427;&#24314;&#35758;&#20026;&#20102;AI&#30340;&#36827;&#27493;&#65292;&#25105;&#20204;&#24517;&#39035;&#23547;&#27714;&#33021;&#22815;&#25552;&#20379;&#27934;&#23519;&#21147;&#21644;&#35299;&#37322;&#33021;&#21147;&#30340;&#27169;&#22411;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper discusses the limitations of machine learning (ML), particularly deep artificial neural networks (ANNs), which are effective at approximating complex functions but often lack transparency and explanatory power. It highlights the `problem of induction' : the philosophical issue that past observations may not necessarily predict future events, a challenge that ML models face when encountering new, unseen data. The paper argues for the importance of not just making predictions but also providing good explanations, a feature that current models often fail to deliver. It suggests that for AI to progress, we must seek models that offer insights and explanations, not just predictions.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;DiffStride&#21644;&#35889;&#27744;&#21270;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#21453;&#21521;&#20256;&#25773;&#25191;&#34892;&#30340;&#19979;&#37319;&#26679;&#21487;&#23398;&#20064;&#27493;&#24133;&#25216;&#26415;&#21644;&#35889;&#27744;&#21270;&#25216;&#26415;&#65292;&#20197;&#20445;&#30041;&#22270;&#20687;&#20013;&#30340;&#37325;&#35201;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2401.09008</link><description>&lt;p&gt;
&#28151;&#21512;DiffStride&#21644;&#35889;&#27744;&#21270;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Hybrid of DiffStride and Spectral Pooling in Convolutional Neural Networks. (arXiv:2401.09008v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09008
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;DiffStride&#21644;&#35889;&#27744;&#21270;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#21453;&#21521;&#20256;&#25773;&#25191;&#34892;&#30340;&#19979;&#37319;&#26679;&#21487;&#23398;&#20064;&#27493;&#24133;&#25216;&#26415;&#21644;&#35889;&#27744;&#21270;&#25216;&#26415;&#65292;&#20197;&#20445;&#30041;&#22270;&#20687;&#20013;&#30340;&#37325;&#35201;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27493;&#24133;&#20915;&#23450;&#20102;&#28388;&#27874;&#22120;&#22312;&#36755;&#20837;&#19978;&#31227;&#21160;&#26102;&#30456;&#37051;&#20301;&#32622;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#22266;&#23450;&#30340;&#27493;&#24133;&#23548;&#33268;&#26080;&#27861;&#25429;&#25417;&#21040;&#22270;&#20687;&#20013;&#37325;&#35201;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#26080;&#27861;&#23545;&#37325;&#35201;&#20449;&#24687;&#36827;&#34892;&#20998;&#31867;&#12290;&#22240;&#27492;&#65292;&#22312;&#20808;&#21069;&#30340;&#30740;&#31350;&#20013;&#65292;&#24212;&#29992;&#20102;DiffStride&#26041;&#27861;&#65292;&#21363;&#20855;&#26377;&#23398;&#20064;&#33258;&#24049;&#27493;&#24133;&#20540;&#30340;&#27493;&#24133;&#21367;&#31215;&#26041;&#27861;&#12290;&#20005;&#26684;&#37327;&#21270;&#21644;&#23545;&#20445;&#30041;&#20449;&#24687;&#30340;&#32422;&#26463;&#19979;&#30028;&#22312;Max Pooling&#19979;&#37319;&#26679;&#26041;&#27861;&#20013;&#24341;&#36215;&#38382;&#39064;&#12290;&#35889;&#27744;&#21270;&#36890;&#36807;&#22312;&#39057;&#22495;&#25130;&#26029;&#34920;&#31034;&#26469;&#38477;&#20302;&#23545;&#20445;&#30041;&#20449;&#24687;&#30340;&#32422;&#26463;&#19979;&#30028;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21453;&#21521;&#20256;&#25773;&#25191;&#34892;&#30340;&#19979;&#37319;&#26679;&#21487;&#23398;&#20064;&#27493;&#24133;&#25216;&#26415;&#19982;&#35889;&#27744;&#21270;&#25216;&#26415;&#30456;&#32467;&#21512;&#30340;CNN&#27169;&#22411;&#12290;DiffStride&#21644;&#35889;&#27744;&#21270;&#25216;&#26415;&#26377;&#26395;&#20445;&#25345;&#22270;&#20687;&#20013;&#21253;&#21547;&#30340;&#22823;&#37096;&#20998;&#20449;&#24687;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#28151;&#21512;&#26041;&#27861;&#65292;&#21363;&#23558;DiffStride&#21644;&#35889;&#27744;&#21270;&#25216;&#26415;&#32467;&#21512;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stride determines the distance between adjacent filter positions as the filter moves across the input. A fixed stride causes important information contained in the image can not be captured, so that important information is not classified. Therefore, in previous research, the DiffStride Method was applied, namely the Strided Convolution Method with which it can learn its own stride value. Severe Quantization and a constraining lower bound on preserved information are arises with Max Pooling Downsampling Method. Spectral Pooling reduce the constraint lower bound on preserved information by cutting off the representation in the frequency domain. In this research a CNN Model is proposed with the Downsampling Learnable Stride Technique performed by Backpropagation combined with the Spectral Pooling Technique. Diffstride and Spectral Pooling techniques are expected to maintain most of the information contained in the image. In this study, we compare the Hybrid Method, which is a combined im
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;MMIQC&#25968;&#25454;&#38598;&#21644;&#36845;&#20195;&#32452;&#21512;&#38382;&#39064;(IQC)&#30340;&#26032;&#39062;&#22686;&#24378;&#26041;&#27861;&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#22312;&#31454;&#36187;&#32423;&#25968;&#23398;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#20808;&#21069;&#26368;&#20339;&#32467;&#26524;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.09003</link><description>&lt;p&gt;
&#36890;&#36807;&#36845;&#20195;&#32452;&#21512;&#38382;&#39064;&#26469;&#22686;&#24378;&#25968;&#23398;&#38382;&#39064;&#27714;&#35299;
&lt;/p&gt;
&lt;p&gt;
Augmenting Math Word Problems via Iterative Question Composing. (arXiv:2401.09003v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09003
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;MMIQC&#25968;&#25454;&#38598;&#21644;&#36845;&#20195;&#32452;&#21512;&#38382;&#39064;(IQC)&#30340;&#26032;&#39062;&#22686;&#24378;&#26041;&#27861;&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#22312;&#31454;&#36187;&#32423;&#25968;&#23398;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#20808;&#21069;&#26368;&#20339;&#32467;&#26524;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#19968;&#23450;&#36827;&#23637;&#65292;&#20294;&#22312;&#19981;&#20351;&#29992;&#22806;&#37096;&#24037;&#20855;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#31454;&#36187;&#32423;&#25968;&#23398;&#38382;&#39064;&#20173;&#28982;&#23545;&#24320;&#28304;LLMs&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MMIQC&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#28151;&#21512;&#22788;&#29702;&#30340;&#32593;&#32476;&#25968;&#25454;&#21644;&#21512;&#25104;&#38382;&#39064;-&#21709;&#24212;&#23545;&#30340;&#28151;&#21512;&#25968;&#25454;&#38598;&#65292;&#20197;&#25552;&#20379;&#22522;&#30784;&#27169;&#22411;&#26356;&#22909;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#22312;MMIQC&#19978;&#23545;Mistral-7B(arXiv:2310.06825)&#36827;&#34892;&#24494;&#35843;&#33719;&#24471;&#30340;&#27169;&#22411;Mistral-7B-MMIQC&#65292;&#22312;MATH(arXiv:2103.03874)&#19978;&#36798;&#21040;&#20102;36.0%&#30340;&#20934;&#30830;&#29575;&#65292;&#27604;&#20043;&#21069;(model size $\sim$7B)&#30340;&#26368;&#20339;&#32467;&#26524;&#39640;&#20986;5.8%&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#36824;&#34920;&#26126;&#65292;&#25913;&#36827;&#30340;&#19968;&#20010;&#37325;&#35201;&#37096;&#20998;&#24402;&#21151;&#20110;&#25105;&#20204;&#30340;&#26032;&#39062;&#22686;&#24378;&#26041;&#27861;IQC(&#36845;&#20195;&#32452;&#21512;&#38382;&#39064;)&#65292;&#20854;&#20013;&#25105;&#20204;&#36845;&#20195;&#22320;&#35201;&#27714;LLM&#20174;&#32473;&#23450;&#30340;&#31181;&#23376;&#38382;&#39064;&#20013;&#32452;&#21512;&#26032;&#38382;&#39064;&#65292;&#24182;&#20174;&#21478;&#19968;&#20010;LLM&#20013;&#36827;&#34892;&#25298;&#32477;&#25277;&#26679;&#12290;MMIQC&#29616;&#24050;&#22312;https://huggingface.co/datasets/Vivacem/MMIQC&#19978;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite recent progress in improving the mathematical reasoning ability of large language models(LLMs), solving competition-level math problems without the use of external tools remains challenging for open-source LLMs. In this work, we introduce the MMIQC dataset, a mixture of processed web data and synthetic question-response pairs, to equip base models with better mathematical reasoning skills. Mistral-7B-MMIQC, the model obtained by fine-tuning Mistral-7B(arXiv:2310.06825) on MMIQC, achieves 36.0\% accuracy on MATH(arXiv:2103.03874), 5.8\% higher than the previous (model size $\sim$7B) SOTA. Our experiments also show that a large part of the improvement attributes to our novel augmentation method IQC(Iterative Question Composing), where we iteratively ask an LLM to compose new questions from the given seed problems and do rejection sampling from another LLM. MMIQC has now been released on https://huggingface.co/datasets/Vivacem/MMIQC.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36830;&#32493;&#26102;&#38388;&#36830;&#32493;&#31354;&#38388;&#30340;&#31283;&#24577;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;CTCS-HRRL&#65292;&#24182;&#36890;&#36807;&#27169;&#25311;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#21644;&#19982;&#20195;&#29702;&#33021;&#21147;&#30456;&#20851;&#30340;&#35777;&#25454;&#12290;</title><link>http://arxiv.org/abs/2401.08999</link><description>&lt;p&gt;
&#36830;&#32493;&#26102;&#38388;&#36830;&#32493;&#31354;&#38388;&#30340;&#31283;&#24577;&#24378;&#21270;&#23398;&#20064;&#65288;CTCS-HRRL&#65289;&#65306;&#26397;&#21521;&#29983;&#29289;&#33258;&#20027;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Continuous Time Continuous Space Homeostatic Reinforcement Learning (CTCS-HRRL) : Towards Biological Self-Autonomous Agent. (arXiv:2401.08999v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08999
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36830;&#32493;&#26102;&#38388;&#36830;&#32493;&#31354;&#38388;&#30340;&#31283;&#24577;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;CTCS-HRRL&#65292;&#24182;&#36890;&#36807;&#27169;&#25311;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#21644;&#19982;&#20195;&#29702;&#33021;&#21147;&#30456;&#20851;&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31283;&#24577;&#26159;&#29983;&#29289;&#32500;&#25345;&#20869;&#37096;&#24179;&#34913;&#30340;&#29983;&#29702;&#36807;&#31243;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#31283;&#24577;&#26159;&#19968;&#31181;&#23398;&#20064;&#34892;&#20026;&#12290;&#26368;&#36817;&#24341;&#20837;&#30340;&#31283;&#24577;&#35843;&#33410;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;HRRL&#65289;&#26694;&#26550;&#35797;&#22270;&#36890;&#36807;&#23558;&#39537;&#21160;&#20943;&#23569;&#29702;&#35770;&#21644;&#24378;&#21270;&#23398;&#20064;&#30456;&#32467;&#21512;&#26469;&#35299;&#37322;&#36825;&#31181;&#23398;&#20064;&#30340;&#31283;&#24577;&#34892;&#20026;&#12290;&#36825;&#31181;&#38142;&#25509;&#24050;&#32463;&#22312;&#31163;&#25955;&#26102;&#38388;&#31354;&#38388;&#19978;&#24471;&#21040;&#35777;&#26126;&#65292;&#20294;&#26159;&#22312;&#36830;&#32493;&#26102;&#38388;&#31354;&#38388;&#19978;&#23578;&#26410;&#24471;&#21040;&#35777;&#26126;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;HRRL&#26694;&#26550;&#25512;&#24191;&#21040;&#36830;&#32493;&#26102;&#38388;&#31354;&#38388;&#29615;&#22659;&#65292;&#24182;&#39564;&#35777;&#36830;&#32493;&#26102;&#38388;&#36830;&#32493;&#31354;&#38388;HRRL&#65288;CTCS-HRRL&#65289;&#26694;&#26550;&#12290;&#25105;&#20204;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#27169;&#22411;&#26469;&#27169;&#25311;&#30495;&#23454;&#29983;&#29289;&#20195;&#29702;&#20013;&#30340;&#31283;&#24577;&#26426;&#21046;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#35813;&#27169;&#22411;&#20351;&#29992;Hamilton-Jacobian Bellman&#26041;&#31243;&#20197;&#21450;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#20989;&#25968;&#36817;&#20284;&#12290;&#36890;&#36807;&#22522;&#20110;&#27169;&#25311;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25581;&#31034;&#20102;&#19982;&#20195;&#29702;&#33021;&#21147;&#26377;&#20851;&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Homeostasis is a biological process by which living beings maintain their internal balance. Previous research suggests that homeostasis is a learned behaviour. Recently introduced Homeostatic Regulated Reinforcement Learning (HRRL) framework attempts to explain this learned homeostatic behavior by linking Drive Reduction Theory and Reinforcement Learning. This linkage has been proven in the discrete time-space, but not in the continuous time-space. In this work, we advance the HRRL framework to a continuous time-space environment and validate the CTCS-HRRL (Continuous Time Continuous Space HRRL) framework. We achieve this by designing a model that mimics the homeostatic mechanisms in a real-world biological agent. This model uses the Hamilton-Jacobian Bellman Equation, and function approximation based on neural networks and Reinforcement Learning. Through a simulation-based experiment we demonstrate the efficacy of this model and uncover the evidence linked to the agent's ability to dy
&lt;/p&gt;</description></item><item><title>MicroNAS&#26159;&#19968;&#20010;&#38024;&#23545;&#36793;&#32536;&#35745;&#31639;&#20013;&#30340;&#24494;&#25511;&#21046;&#22120;&#21333;&#20803;&#65288;MCUs&#65289;&#35774;&#35745;&#30340;&#30828;&#20214;&#24863;&#30693;&#38646;&#26679;&#26412;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26694;&#26550;&#12290;&#19982;&#20043;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;MicroNAS&#22312;&#25628;&#32034;&#25928;&#29575;&#21644;MCU&#25512;&#29702;&#36895;&#24230;&#26041;&#38754;&#37117;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25552;&#21319;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#30456;&#20284;&#30340;&#31934;&#24230;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2401.08996</link><description>&lt;p&gt;
MicroNAS: &#38646;&#26679;&#26412;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#29992;&#20110;MCUs
&lt;/p&gt;
&lt;p&gt;
MicroNAS: Zero-Shot Neural Architecture Search for MCUs. (arXiv:2401.08996v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08996
&lt;/p&gt;
&lt;p&gt;
MicroNAS&#26159;&#19968;&#20010;&#38024;&#23545;&#36793;&#32536;&#35745;&#31639;&#20013;&#30340;&#24494;&#25511;&#21046;&#22120;&#21333;&#20803;&#65288;MCUs&#65289;&#35774;&#35745;&#30340;&#30828;&#20214;&#24863;&#30693;&#38646;&#26679;&#26412;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26694;&#26550;&#12290;&#19982;&#20043;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;MicroNAS&#22312;&#25628;&#32034;&#25928;&#29575;&#21644;MCU&#25512;&#29702;&#36895;&#24230;&#26041;&#38754;&#37117;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25552;&#21319;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#30456;&#20284;&#30340;&#31934;&#24230;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#26550;&#26500;&#25628;&#32034; (NAS) &#21487;&#20197;&#26377;&#25928;&#22320;&#21457;&#29616;&#26032;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476; (CNN) &#26550;&#26500;&#65292;&#29305;&#21035;&#26159;&#29992;&#20110;&#31934;&#24230;&#20248;&#21270;&#12290;&#28982;&#32780;&#65292;&#20043;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#22312;&#36229;&#32423;&#32593;&#32476;&#19978;&#36827;&#34892;&#36164;&#28304;&#23494;&#38598;&#22411;&#35757;&#32451;&#25110;&#24191;&#27867;&#30340;&#26550;&#26500;&#35780;&#20272;&#65292;&#38480;&#21046;&#20102;&#23454;&#38469;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MicroNAS&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#20026;&#36793;&#32536;&#35745;&#31639;&#20013;&#30340;&#24494;&#25511;&#21046;&#22120;&#21333;&#20803; (MCUs) &#35774;&#35745;&#30340;&#30828;&#20214;&#24863;&#30693;&#38646;&#26679;&#26412;NAS&#26694;&#26550;&#12290;MicroNAS&#22312;&#25628;&#32034;&#36807;&#31243;&#20013;&#32771;&#34385;&#20102;&#30446;&#26631;&#30828;&#20214;&#20248;&#21270;&#24615;&#33021;&#65292;&#21033;&#29992;&#19987;&#38376;&#30340;&#24615;&#33021;&#25351;&#26631;&#26469;&#35782;&#21035;&#26368;&#20339;&#30340;&#31070;&#32463;&#26550;&#26500;&#65292;&#32780;&#19981;&#38656;&#35201;&#39640;&#35745;&#31639;&#25104;&#26412;&#12290;&#19982;&#20043;&#21069;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;MicroNAS&#22312;&#25628;&#32034;&#25928;&#29575;&#26041;&#38754;&#25552;&#39640;&#20102;1104&#20493;&#65292;&#24182;&#19988;&#21457;&#29616;&#20102;&#22312;&#32500;&#25345;&#30456;&#20284;&#31934;&#24230;&#30340;&#24773;&#20917;&#19979;MCU&#25512;&#29702;&#36895;&#24230;&#25552;&#39640;&#20102;3.23&#20493;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Architecture Search (NAS) effectively discovers new Convolutional Neural Network (CNN) architectures, particularly for accuracy optimization. However, prior approaches often require resource-intensive training on super networks or extensive architecture evaluations, limiting practical applications. To address these challenges, we propose MicroNAS, a hardware-aware zero-shot NAS framework designed for microcontroller units (MCUs) in edge computing. MicroNAS considers target hardware optimality during the search, utilizing specialized performance indicators to identify optimal neural architectures without high computational costs. Compared to previous works, MicroNAS achieves up to 1104x improvement in search efficiency and discovers models with over 3.23x faster MCU inference while maintaining similar accuracy
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;GAN&#30340;&#25968;&#25454;&#27745;&#26579;&#26694;&#26550;&#65288;P-GAN&#65289;&#65292;&#29992;&#20110;&#23545;&#25239;&#32437;&#21521;&#32852;&#21512;&#23398;&#20064;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#12290;&#36890;&#36807;&#20351;&#29992;&#21322;&#30417;&#30563;&#23398;&#20064;&#35757;&#32451;&#19968;&#20010;&#26367;&#20195;&#30446;&#26631;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;GAN&#29983;&#25104;&#23545;&#25239;&#24615;&#25200;&#21160;&#26469;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;&#65292;&#26368;&#21518;&#36890;&#36807;&#28145;&#24230;&#33258;&#32534;&#30721;&#22120;&#24320;&#21457;&#30340;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#38450;&#24481;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2401.08984</link><description>&lt;p&gt;
&#22522;&#20110;GAN&#30340;&#25968;&#25454;&#27745;&#26579;&#26694;&#26550;&#23545;&#25239;&#32437;&#21521;&#32852;&#21512;&#23398;&#20064;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
A GAN-based data poisoning framework against anomaly detection in vertical federated learning. (arXiv:2401.08984v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08984
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;GAN&#30340;&#25968;&#25454;&#27745;&#26579;&#26694;&#26550;&#65288;P-GAN&#65289;&#65292;&#29992;&#20110;&#23545;&#25239;&#32437;&#21521;&#32852;&#21512;&#23398;&#20064;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#12290;&#36890;&#36807;&#20351;&#29992;&#21322;&#30417;&#30563;&#23398;&#20064;&#35757;&#32451;&#19968;&#20010;&#26367;&#20195;&#30446;&#26631;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;GAN&#29983;&#25104;&#23545;&#25239;&#24615;&#25200;&#21160;&#26469;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;&#65292;&#26368;&#21518;&#36890;&#36807;&#28145;&#24230;&#33258;&#32534;&#30721;&#22120;&#24320;&#21457;&#30340;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#38450;&#24481;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32437;&#21521;&#32852;&#21512;&#23398;&#20064; (VFL) &#20013;&#65292;&#21830;&#19994;&#23454;&#20307;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#21516;&#26102;&#21327;&#20316;&#35757;&#32451;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#24694;&#24847;&#21442;&#19982;&#32773;&#30340;&#27745;&#26579;&#25915;&#20987;&#21487;&#33021;&#20250;&#38477;&#20302;&#36825;&#20010;&#21327;&#20316;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#23454;&#29616;&#27745;&#26579;&#25915;&#20987;&#30340;&#20027;&#35201;&#25361;&#25112;&#26159;&#32570;&#20047;&#23545;&#26381;&#21153;&#22120;&#31471;&#39030;&#23618;&#27169;&#22411;&#30340;&#35775;&#38382;&#65292;&#20351;&#24471;&#24694;&#24847;&#21442;&#19982;&#32773;&#27809;&#26377;&#26126;&#30830;&#30340;&#30446;&#26631;&#27169;&#22411;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#31471;&#21040;&#31471;&#27745;&#26579;&#26694;&#26550; P-GAN&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#24694;&#24847;&#21442;&#19982;&#32773;&#26368;&#21021;&#37319;&#29992;&#21322;&#30417;&#30563;&#23398;&#20064;&#35757;&#32451;&#19968;&#20010;&#26367;&#20195;&#30446;&#26631;&#27169;&#22411;&#12290;&#38543;&#21518;&#65292;&#35813;&#21442;&#19982;&#32773;&#37319;&#29992;&#22522;&#20110;GAN&#30340;&#26041;&#27861;&#20135;&#29983;&#23545;&#25239;&#24615;&#25200;&#21160;&#65292;&#20197;&#38477;&#20302;&#26367;&#20195;&#30446;&#26631;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;&#29983;&#25104;&#22120;&#34987;&#33719;&#24471;&#24182;&#38024;&#23545;VFL&#27745;&#26579;&#36827;&#34892;&#20102;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#22522;&#20110;&#28145;&#24230;&#33258;&#32534;&#30721;&#22120; (DAE) &#24320;&#21457;&#20102;&#19968;&#31181;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#65292;&#20026;VFL&#22330;&#26223;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#38450;&#24481;&#26426;&#21046;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#8230;
&lt;/p&gt;
&lt;p&gt;
In vertical federated learning (VFL), commercial entities collaboratively train a model while preserving data privacy. However, a malicious participant's poisoning attack may degrade the performance of this collaborative model. The main challenge in achieving the poisoning attack is the absence of access to the server-side top model, leaving the malicious participant without a clear target model. To address this challenge, we introduce an innovative end-to-end poisoning framework P-GAN. Specifically, the malicious participant initially employs semi-supervised learning to train a surrogate target model. Subsequently, this participant employs a GAN-based method to produce adversarial perturbations to degrade the surrogate target model's performance. Finally, the generator is obtained and tailored for VFL poisoning. Besides, we develop an anomaly detection algorithm based on a deep auto-encoder (DAE), offering a robust defense mechanism to VFL scenarios. Through extensive experiments, we 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;FedLoGe&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#22312;&#31070;&#32463;&#32593;&#32476;&#23849;&#28291;&#26694;&#26550;&#20013;&#38598;&#25104;&#34920;&#31034;&#23398;&#20064;&#21644;&#20998;&#31867;&#22120;&#23545;&#40784;&#26469;&#25552;&#39640;&#21306;&#22495;&#21644;&#20840;&#23616;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#35299;&#20915;&#20102;&#22312;&#32852;&#37030;&#38271;&#23614;&#23398;&#20064;&#20013;&#24573;&#35270;&#26412;&#22320;&#32423;&#21035;&#24615;&#33021;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.08977</link><description>&lt;p&gt;
FedLoGe: &#38271;&#23614;&#25968;&#25454;&#19979;&#30340;&#26412;&#22320;&#21644;&#36890;&#29992;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FedLoGe: Joint Local and Generic Federated Learning under Long-tailed Data. (arXiv:2401.08977v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08977
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;FedLoGe&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#22312;&#31070;&#32463;&#32593;&#32476;&#23849;&#28291;&#26694;&#26550;&#20013;&#38598;&#25104;&#34920;&#31034;&#23398;&#20064;&#21644;&#20998;&#31867;&#22120;&#23545;&#40784;&#26469;&#25552;&#39640;&#21306;&#22495;&#21644;&#20840;&#23616;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#35299;&#20915;&#20102;&#22312;&#32852;&#37030;&#38271;&#23614;&#23398;&#20064;&#20013;&#24573;&#35270;&#26412;&#22320;&#32423;&#21035;&#24615;&#33021;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#38271;&#23614;&#23398;&#20064;&#65288;Fed-LT&#65289;&#26159;&#19968;&#31181;&#22312;&#21435;&#20013;&#24515;&#21270;&#30340;&#26412;&#22320;&#23458;&#25143;&#31471;&#25910;&#38598;&#30340;&#25968;&#25454;&#21576;&#29616;&#20840;&#29699;&#26222;&#36941;&#23384;&#22312;&#30340;&#38271;&#23614;&#20998;&#24067;&#30340;&#33539;&#20363;&#65292;&#36817;&#24180;&#26469;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#12290;&#22312;Fed-LT&#30340;&#32972;&#26223;&#19979;&#65292;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#20110;&#35299;&#20915;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#20197;&#25552;&#39640;&#36890;&#29992;&#20840;&#23616;&#27169;&#22411;&#30340;&#25928;&#33021;&#65292;&#32780;&#24573;&#35270;&#20102;&#26412;&#22320;&#32423;&#21035;&#30340;&#24615;&#33021;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#24120;&#35268;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;pFL&#65289;&#25216;&#26415;&#20027;&#35201;&#26159;&#22312;&#24179;&#34913;&#30340;&#20840;&#23616;&#25968;&#25454;&#20998;&#24067;&#30340;&#20551;&#35774;&#19979;&#65292;&#20248;&#21270;&#20010;&#24615;&#21270;&#30340;&#26412;&#22320;&#27169;&#22411;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedLoGe&#30340;&#26041;&#27861;&#65292;&#22312;Fed-LT&#20013;&#36890;&#36807;&#22312;&#31070;&#32463;&#32593;&#32476;&#23849;&#28291;&#26694;&#26550;&#20013;&#38598;&#25104;&#34920;&#31034;&#23398;&#20064;&#21644;&#20998;&#31867;&#22120;&#23545;&#40784;&#65292;&#25552;&#39640;&#26412;&#22320;&#21644;&#36890;&#29992;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#20351;&#29992;&#20849;&#20139;&#39592;&#24178;&#20316;&#20026;&#22522;&#30784;&#26694;&#26550;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Long-Tailed Learning (Fed-LT), a paradigm wherein data collected from decentralized local clients manifests a globally prevalent long-tailed distribution, has garnered considerable attention in recent times. In the context of Fed-LT, existing works have predominantly centered on addressing the data imbalance issue to enhance the efficacy of the generic global model while neglecting the performance at the local level. In contrast, conventional Personalized Federated Learning (pFL) techniques are primarily devised to optimize personalized local models under the presumption of a balanced global data distribution. This paper introduces an approach termed Federated Local and Generic Model Training in Fed-LT (FedLoGe), which enhances both local and generic model performance through the integration of representation learning and classifier alignment within a neural collapse framework. Our investigation reveals the feasibility of employing a shared backbone as a foundational framewor
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;OCTO + &#30340;&#22871;&#20214;&#65292;&#29992;&#20110;&#22312;&#28151;&#21512;&#29616;&#23454;&#20013;&#33258;&#21160;&#23558;&#34394;&#25311;&#29289;&#20307;&#25918;&#32622;&#22312;&#21512;&#36866;&#30340;&#20301;&#32622;&#12290;&#36890;&#36807;&#20351;&#29992;&#26368;&#26032;&#30340;&#24320;&#25918;&#35789;&#27719;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#35780;&#20272;&#26631;&#20934;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#25928;&#26524;&#65292;&#36229;&#36807;&#20854;&#20182;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#29992;&#20110;&#33258;&#21160;&#35780;&#20272;&#22686;&#24378;&#29616;&#23454;&#20013;&#34394;&#25311;&#29289;&#20307;&#25918;&#32622;&#30340;&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2401.08973</link><description>&lt;p&gt;
OCTO + &#65306;&#33258;&#21160;&#24320;&#25918;&#35789;&#27719;&#29289;&#20307;&#25918;&#32622;&#22312;&#28151;&#21512;&#29616;&#23454;&#20013;&#30340;&#22871;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
OCTO+: A Suite for Automatic Open-Vocabulary Object Placement in Mixed Reality. (arXiv:2401.08973v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08973
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;OCTO + &#30340;&#22871;&#20214;&#65292;&#29992;&#20110;&#22312;&#28151;&#21512;&#29616;&#23454;&#20013;&#33258;&#21160;&#23558;&#34394;&#25311;&#29289;&#20307;&#25918;&#32622;&#22312;&#21512;&#36866;&#30340;&#20301;&#32622;&#12290;&#36890;&#36807;&#20351;&#29992;&#26368;&#26032;&#30340;&#24320;&#25918;&#35789;&#27719;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#35780;&#20272;&#26631;&#20934;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#25928;&#26524;&#65292;&#36229;&#36807;&#20854;&#20182;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#29992;&#20110;&#33258;&#21160;&#35780;&#20272;&#22686;&#24378;&#29616;&#23454;&#20013;&#34394;&#25311;&#29289;&#20307;&#25918;&#32622;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22686;&#24378;&#29616;&#23454;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#23558;&#34394;&#25311;&#20869;&#23481;&#25918;&#32622;&#22312;&#33258;&#28982;&#20301;&#32622;&#19978;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#33258;&#21160;&#21270;&#25216;&#26415;&#21482;&#33021;&#20351;&#29992;&#23553;&#38381;&#35789;&#27719;&#12289;&#22266;&#23450;&#30340;&#29289;&#20307;&#38598;&#21512;&#26469;&#24037;&#20316;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#24182;&#35780;&#20272;&#20102;&#20960;&#31181;&#20351;&#29992;&#26368;&#26032;&#36827;&#23637;&#30340;&#24320;&#25918;&#35789;&#27719;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33258;&#21160;&#29289;&#20307;&#25918;&#32622;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#22810;&#26041;&#38754;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#31181;&#26032;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;OCTO +&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#29992;&#20110;&#33258;&#21160;&#35780;&#20272;&#22686;&#24378;&#29616;&#23454;&#20013;&#34394;&#25311;&#29289;&#20307;&#25918;&#32622;&#30340;&#22522;&#20934;&#65292;&#20943;&#36731;&#20102;&#38656;&#35201;&#26114;&#36149;&#29992;&#25143;&#30740;&#31350;&#30340;&#38656;&#27714;&#12290;&#36890;&#36807;&#36825;&#20010;&#22522;&#20934;&#65292;&#38500;&#20102;&#20154;&#31867;&#35780;&#20272;&#20043;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;OCTO + &#26377;&#25928;&#22320;&#23558;&#29289;&#20307;&#25918;&#32622;&#22312;&#19968;&#20010;&#21512;&#29702;&#30340;&#21306;&#22495;&#20869;&#36229;&#36807;70&#65285;&#30340;&#26102;&#38388;&#65292;&#22312;&#21508;&#31181;&#25351;&#26631;&#19978;&#36229;&#36234;&#20102;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
One key challenge in Augmented Reality is the placement of virtual content in natural locations. Most existing automated techniques can only work with a closed-vocabulary, fixed set of objects. In this paper, we introduce and evaluate several methods for automatic object placement using recent advances in open-vocabulary vision-language models. Through a multifaceted evaluation, we identify a new state-of-the-art method, OCTO+. We also introduce a benchmark for automatically evaluating the placement of virtual objects in augmented reality, alleviating the need for costly user studies. Through this, in addition to human evaluations, we find that OCTO+ places objects in a valid region over 70% of the time, outperforming other methods on a range of metrics.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#65292;&#20197;&#20102;&#35299;AI&#22522;&#20110;&#30340;&#29983;&#20135;&#21147;&#20195;&#29702;&#30340;&#20559;&#22909;&#65292;&#24182;&#24320;&#21457;&#20986;&#20010;&#24615;&#21270;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#35843;&#26597;&#21644;&#20351;&#29992;&#36965;&#27979;&#25968;&#25454;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;GPT-4&#39537;&#21160;&#30340;&#20010;&#24615;&#21270;&#29983;&#20135;&#21147;&#20195;&#29702;&#65292;&#24182;&#22312;&#30740;&#31350;&#20013;&#19982;&#20854;&#20182;&#36741;&#21161;&#24037;&#20855;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#31361;&#20986;&#20102;&#29992;&#25143;&#20013;&#24515;&#35774;&#35745;&#12289;&#36866;&#24212;&#24615;&#21644;&#20010;&#24615;&#21270;&#19982;&#38544;&#31169;&#20043;&#38388;&#30340;&#24179;&#34913;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.08960</link><description>&lt;p&gt;
&#20174;&#29992;&#25143;&#35843;&#26597;&#21040;&#36965;&#27979;&#39537;&#21160;&#20195;&#29702;&#65306;&#25506;&#32034;&#20010;&#24615;&#21270;&#30340;&#29983;&#20135;&#21147;&#35299;&#20915;&#26041;&#26696;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
From User Surveys to Telemetry-Driven Agents: Exploring the Potential of Personalized Productivity Solutions. (arXiv:2401.08960v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08960
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#65292;&#20197;&#20102;&#35299;AI&#22522;&#20110;&#30340;&#29983;&#20135;&#21147;&#20195;&#29702;&#30340;&#20559;&#22909;&#65292;&#24182;&#24320;&#21457;&#20986;&#20010;&#24615;&#21270;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#35843;&#26597;&#21644;&#20351;&#29992;&#36965;&#27979;&#25968;&#25454;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;GPT-4&#39537;&#21160;&#30340;&#20010;&#24615;&#21270;&#29983;&#20135;&#21147;&#20195;&#29702;&#65292;&#24182;&#22312;&#30740;&#31350;&#20013;&#19982;&#20854;&#20182;&#36741;&#21161;&#24037;&#20855;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#31361;&#20986;&#20102;&#29992;&#25143;&#20013;&#24515;&#35774;&#35745;&#12289;&#36866;&#24212;&#24615;&#21644;&#20010;&#24615;&#21270;&#19982;&#38544;&#31169;&#20043;&#38388;&#30340;&#24179;&#34913;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#30340;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20102;&#35299;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#29983;&#20135;&#21147;&#20195;&#29702;&#30340;&#20559;&#22909;&#65292;&#24182;&#24320;&#21457;&#20986;&#26681;&#25454;&#29992;&#25143;&#38656;&#27714;&#23450;&#21046;&#30340;&#20010;&#24615;&#21270;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#20004;&#20010;&#38454;&#27573;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#39318;&#20808;&#23545;363&#21517;&#21442;&#19982;&#32773;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#25506;&#32034;&#20102;&#29983;&#20135;&#21147;&#12289;&#27807;&#36890;&#39118;&#26684;&#12289;&#20195;&#29702;&#26041;&#27861;&#12289;&#20010;&#24615;&#29305;&#24449;&#12289;&#20010;&#24615;&#21270;&#21644;&#38544;&#31169;&#31561;&#21508;&#20010;&#26041;&#38754;&#12290;&#20511;&#21161;&#35843;&#26597;&#32467;&#26524;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#30001;Viva Insights&#25910;&#38598;&#30340;&#36965;&#27979;&#25968;&#25454;&#39537;&#21160;&#30340;&#20010;&#24615;&#21270;&#29983;&#20135;&#21147;&#20195;&#29702;&#65292;&#35813;&#20195;&#29702;&#21033;&#29992;GPT-4&#25552;&#20379;&#23450;&#21046;&#30340;&#24110;&#21161;&#12290;&#25105;&#20204;&#22312;&#28041;&#21450;40&#21517;&#21442;&#19982;&#32773;&#30340;&#30740;&#31350;&#20013;&#65292;&#23558;&#20854;&#24615;&#33021;&#19982;&#20202;&#34920;&#26495;&#21644;&#21465;&#36848;&#31561;&#26367;&#20195;&#30340;&#29983;&#20135;&#21147;&#36741;&#21161;&#24037;&#20855;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#20984;&#26174;&#20102;&#29992;&#25143;&#20013;&#24515;&#35774;&#35745;&#12289;&#36866;&#24212;&#24615;&#20197;&#21450;&#20010;&#24615;&#21270;&#21644;&#38544;&#31169;&#20043;&#38388;&#30340;&#24179;&#34913;&#22312;AI&#36741;&#21161;&#29983;&#20135;&#21147;&#24037;&#20855;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;&#36890;&#36807;&#20511;&#37492;&#25105;&#20204;&#30740;&#31350;&#20013;&#25552;&#28860;&#30340;&#35265;&#35299;&#65292;&#25105;&#20204;&#30456;&#20449;&#25105;&#20204;&#30340;&#24037;&#20316;&#21487;&#20197;&#21551;&#21457;&#21644;&#25351;&#23548;&#26410;&#26469;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a comprehensive, user-centric approach to understand preferences in AI-based productivity agents and develop personalized solutions tailored to users' needs. Utilizing a two-phase method, we first conducted a survey with 363 participants, exploring various aspects of productivity, communication style, agent approach, personality traits, personalization, and privacy. Drawing on the survey insights, we developed a GPT-4 powered personalized productivity agent that utilizes telemetry data gathered via Viva Insights from information workers to provide tailored assistance. We compared its performance with alternative productivity-assistive tools, such as dashboard and narrative, in a study involving 40 participants. Our findings highlight the importance of user-centric design, adaptability, and the balance between personalization and privacy in AI-assisted productivity tools. By building on the insights distilled from our study, we believe that our work can enable and guide futur
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31163;&#31574;&#30053;&#20215;&#20540;&#25490;&#21517;&#65288;VR&#65289;&#31639;&#27861;&#65292;&#36890;&#36807;&#32479;&#19968;&#30340;&#26399;&#26395;&#26368;&#22823;&#21270;&#65288;EM&#65289;&#26694;&#26550;&#65292;&#22312;&#19981;&#38656;&#35201;&#22312;&#32447;&#20132;&#20114;&#30340;&#24773;&#20917;&#19979;&#26368;&#22823;&#21270;&#29992;&#25143;&#30340;&#38271;&#26399;&#22238;&#25253;&#21644;&#20248;&#21270;&#25490;&#21517;&#25351;&#26631;&#65292;&#20197;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.08959</link><description>&lt;p&gt;
&#38754;&#21521;&#24102;&#26377;&#20154;&#31867;&#21453;&#39304;&#30340;&#25490;&#21517;&#31574;&#30053;&#30340;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Towards Off-Policy Reinforcement Learning for Ranking Policies with Human Feedback. (arXiv:2401.08959v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08959
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31163;&#31574;&#30053;&#20215;&#20540;&#25490;&#21517;&#65288;VR&#65289;&#31639;&#27861;&#65292;&#36890;&#36807;&#32479;&#19968;&#30340;&#26399;&#26395;&#26368;&#22823;&#21270;&#65288;EM&#65289;&#26694;&#26550;&#65292;&#22312;&#19981;&#38656;&#35201;&#22312;&#32447;&#20132;&#20114;&#30340;&#24773;&#20917;&#19979;&#26368;&#22823;&#21270;&#29992;&#25143;&#30340;&#38271;&#26399;&#22238;&#25253;&#21644;&#20248;&#21270;&#25490;&#21517;&#25351;&#26631;&#65292;&#20197;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#29575;&#23398;&#20064;&#25490;&#21517;&#65288;LTR&#65289;&#19968;&#30452;&#26159;&#20248;&#21270;&#25490;&#21517;&#25351;&#26631;&#30340;&#20027;&#35201;&#26041;&#27861;&#65292;&#20294;&#26080;&#27861;&#26368;&#22823;&#21270;&#38271;&#26399;&#22238;&#25253;&#12290;&#25552;&#20986;&#20102;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#26469;&#23558;&#25512;&#33616;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#24207;&#36143;&#20915;&#31574;&#38382;&#39064;&#65292;&#20197;&#26368;&#22823;&#21270;&#29992;&#25143;&#30340;&#38271;&#26399;&#22238;&#25253;&#65292;&#20294;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#19982;LTR&#26041;&#27861;&#30456;&#27604;&#20173;&#28982;&#23384;&#22312;&#19981;&#36275;&#65292;&#20027;&#35201;&#21407;&#22240;&#26159;&#32570;&#20047;&#22312;&#32447;&#20132;&#20114;&#21644;&#25490;&#21517;&#29305;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31163;&#31574;&#30053;&#20215;&#20540;&#25490;&#21517;&#65288;VR&#65289;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#32479;&#19968;&#30340;&#26399;&#26395;&#26368;&#22823;&#21270;&#65288;EM&#65289;&#26694;&#26550;&#19979;&#21516;&#26102;&#26368;&#22823;&#21270;&#29992;&#25143;&#30340;&#38271;&#26399;&#22238;&#25253;&#21644;&#20248;&#21270;&#25490;&#21517;&#25351;&#26631;&#65292;&#20174;&#32780;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#21644;&#23454;&#39564;&#35777;&#26126;&#20102;EM&#36807;&#31243;&#24341;&#23548;&#23398;&#20064;&#31574;&#30053;&#20139;&#21463;&#26410;&#26469;&#22238;&#25253;&#21644;&#25490;&#21517;&#25351;&#26631;&#34701;&#21512;&#30340;&#22909;&#22788;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#20219;&#20309;&#22312;&#32447;&#20132;&#20114;&#12290;&#22823;&#37327;&#30340;&#31163;&#32447;&#21644;&#22312;&#32447;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Probabilistic learning to rank (LTR) has been the dominating approach for optimizing the ranking metric, but cannot maximize long-term rewards. Reinforcement learning models have been proposed to maximize user long-term rewards by formulating the recommendation as a sequential decision-making problem, but could only achieve inferior accuracy compared to LTR counterparts, primarily due to the lack of online interactions and the characteristics of ranking. In this paper, we propose a new off-policy value ranking (VR) algorithm that can simultaneously maximize user long-term rewards and optimize the ranking metric offline for improved sample efficiency in a unified Expectation-Maximization (EM) framework. We theoretically and empirically show that the EM process guides the leaned policy to enjoy the benefit of integration of the future reward and ranking metric, and learn without any online interactions. Extensive offline and online experiments demonstrate the effectiveness of our methods
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#26694;&#26550;SWBT&#65292;&#33021;&#22815;&#22312;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#20013;&#26377;&#25928;&#22320;&#20174;&#19987;&#23478;&#28436;&#31034;&#21644;&#19981;&#23436;&#32654;&#28436;&#31034;&#20013;&#23398;&#20064;&#65292;&#32780;&#26080;&#38656;&#19982;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#23558;&#19981;&#23436;&#32654;&#28436;&#31034;&#25972;&#21512;&#21040;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#35774;&#32622;&#20013;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2401.08957</link><description>&lt;p&gt;
SWBT&#65306;&#20855;&#26377;&#19981;&#23436;&#32654;&#28436;&#31034;&#30340;&#30456;&#20284;&#24615;&#21152;&#26435;&#34892;&#20026;&#36716;&#25442;&#22120;&#29992;&#20110;&#26426;&#22120;&#20154;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
SWBT: Similarity Weighted Behavior Transformer with the Imperfect Demonstration for Robotic Manipulation. (arXiv:2401.08957v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08957
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#26694;&#26550;SWBT&#65292;&#33021;&#22815;&#22312;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#20013;&#26377;&#25928;&#22320;&#20174;&#19987;&#23478;&#28436;&#31034;&#21644;&#19981;&#23436;&#32654;&#28436;&#31034;&#20013;&#23398;&#20064;&#65292;&#32780;&#26080;&#38656;&#19982;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#23558;&#19981;&#23436;&#32654;&#28436;&#31034;&#25972;&#21512;&#21040;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#35774;&#32622;&#20013;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#20223;&#23398;&#20064;&#26088;&#22312;&#20174;&#19987;&#23478;&#28436;&#31034;&#20013;&#23398;&#20064;&#26368;&#20339;&#25511;&#21046;&#31574;&#30053;&#65292;&#24050;&#25104;&#20026;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#35201;&#20040;&#20165;&#20351;&#29992;&#26114;&#36149;&#30340;&#19987;&#23478;&#28436;&#31034;&#24182;&#24573;&#30053;&#19981;&#23436;&#32654;&#30340;&#28436;&#31034;&#65292;&#35201;&#20040;&#20381;&#36182;&#20110;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#21644;&#20174;&#22312;&#32447;&#32463;&#39564;&#20013;&#23398;&#20064;&#12290;&#22312;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#26088;&#22312;&#20811;&#26381;&#19978;&#36848;&#20004;&#20010;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Similarity Weighted Behavior Transformer&#65288;SWBT&#65289;&#30340;&#26032;&#22411;&#26694;&#26550;&#12290;SWBT&#33021;&#22815;&#26377;&#25928;&#22320;&#20174;&#19987;&#23478;&#28436;&#31034;&#21644;&#19981;&#23436;&#32654;&#28436;&#31034;&#20013;&#23398;&#20064;&#65292;&#32780;&#26080;&#38656;&#19982;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#12290;&#25105;&#20204;&#25581;&#31034;&#20102;&#26131;&#33719;&#21462;&#30340;&#19981;&#23436;&#32654;&#28436;&#31034;&#65292;&#22914;&#27491;&#21521;&#21644;&#21453;&#21521;&#21160;&#21147;&#23398;&#65292;&#36890;&#36807;&#23398;&#20064;&#26377;&#30410;&#20449;&#24687;&#26174;&#33879;&#22686;&#24378;&#20102;&#32593;&#32476;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#23581;&#35797;&#23558;&#19981;&#23436;&#32654;&#28436;&#31034;&#25972;&#21512;&#21040;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#35774;&#32622;&#20013;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#20013;&#30340;&#30740;&#31350;&#12290;&#22312;ManiSkill2 bench&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imitation learning (IL), aiming to learn optimal control policies from expert demonstrations, has been an effective method for robot manipulation tasks. However, previous IL methods either only use expensive expert demonstrations and omit imperfect demonstrations or rely on interacting with the environment and learning from online experiences. In the context of robotic manipulation, we aim to conquer the above two challenges and propose a novel framework named Similarity Weighted Behavior Transformer (SWBT). SWBT effectively learn from both expert and imperfect demonstrations without interaction with environments. We reveal that the easy-to-get imperfect demonstrations, such as forward and inverse dynamics, significantly enhance the network by learning fruitful information. To the best of our knowledge, we are the first to attempt to integrate imperfect demonstrations into the offline imitation learning setting for robot manipulation tasks. Extensive experiments on the ManiSkill2 bench
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CEL&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#39046;&#22495;&#33258;&#36866;&#24212;&#21644;&#24377;&#24615;&#26435;&#37325;&#25972;&#21512;&#65292;&#23454;&#29616;&#20102;&#23545;&#30142;&#30149;&#29190;&#21457;&#39044;&#27979;&#30340;&#25345;&#32493;&#23398;&#20064;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#24809;&#32602;&#37325;&#35201;&#21442;&#25968;&#30340;&#25913;&#21464;&#26469;&#32531;&#35299;&#28798;&#38590;&#24615;&#36951;&#24536;&#29616;&#35937;&#65292;&#24182;&#22312;&#22810;&#20010;&#30142;&#30149;&#19978;&#21462;&#24471;&#20102;&#27604;&#20854;&#20182;&#27169;&#22411;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.08940</link><description>&lt;p&gt;
CEL&#65306;&#36890;&#36807;&#24377;&#24615;&#26435;&#37325;&#25972;&#21512;&#21033;&#29992;&#39046;&#22495;&#33258;&#36866;&#24212;&#26469;&#36827;&#34892;&#30142;&#30149;&#29190;&#21457;&#39044;&#27979;&#30340;&#25345;&#32493;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CEL: A Continual Learning Model for Disease Outbreak Prediction by Leveraging Domain Adaptation via Elastic Weight Consolidation. (arXiv:2401.08940v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08940
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CEL&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#39046;&#22495;&#33258;&#36866;&#24212;&#21644;&#24377;&#24615;&#26435;&#37325;&#25972;&#21512;&#65292;&#23454;&#29616;&#20102;&#23545;&#30142;&#30149;&#29190;&#21457;&#39044;&#27979;&#30340;&#25345;&#32493;&#23398;&#20064;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#24809;&#32602;&#37325;&#35201;&#21442;&#25968;&#30340;&#25913;&#21464;&#26469;&#32531;&#35299;&#28798;&#38590;&#24615;&#36951;&#24536;&#29616;&#35937;&#65292;&#24182;&#22312;&#22810;&#20010;&#30142;&#30149;&#19978;&#21462;&#24471;&#20102;&#27604;&#20854;&#20182;&#27169;&#22411;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;&#26159;&#27169;&#22411;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#19981;&#24536;&#35760;&#20043;&#21069;&#30693;&#35782;&#24182;&#33021;&#36866;&#24212;&#26032;&#25968;&#25454;&#30340;&#33021;&#21147;&#65292;&#22312;&#30142;&#30149;&#29190;&#21457;&#39044;&#27979;&#31561;&#21160;&#24577;&#39046;&#22495;&#23588;&#20026;&#37325;&#35201;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#22914;LSTM&#65292;&#30001;&#20110;&#28798;&#38590;&#24615;&#36951;&#24536;&#32780;&#23481;&#26131;&#20986;&#38169;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#24377;&#24615;&#26435;&#37325;&#25972;&#21512;&#65288;EWC&#65289;&#21033;&#29992;&#39046;&#22495;&#33258;&#36866;&#24212;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;CEL&#27169;&#22411;&#36827;&#34892;&#25345;&#32493;&#23398;&#20064;&#12290;&#35813;&#27169;&#22411;&#26088;&#22312;&#32531;&#35299;&#39046;&#22495;&#22686;&#37327;&#35774;&#32622;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#29616;&#35937;&#12290;&#21033;&#29992;EWC&#26500;&#24314;Fisher&#20449;&#24687;&#30697;&#38453;&#65288;FIM&#65289;&#20197;&#24320;&#21457;&#20986;&#19968;&#20010;&#24809;&#32602;&#23545;&#37325;&#35201;&#21442;&#25968;&#21363;&#37325;&#35201;&#20808;&#21069;&#30693;&#35782;&#30340;&#25913;&#21464;&#30340;&#27491;&#21017;&#21270;&#39033;&#12290;&#22312;&#35780;&#20272;&#21644;&#37325;&#26032;&#35780;&#20272;&#20013;&#65292;CEL&#30340;&#24615;&#33021;&#20351;&#29992;&#19981;&#21516;&#25351;&#26631;&#22312;&#19977;&#31181;&#19981;&#21516;&#30340;&#30142;&#30149;&#65288;&#27969;&#24863;&#65292;&#30168;&#30137;&#21644;&#40635;&#30137;&#65289;&#19978;&#24471;&#21040;&#20102;&#24456;&#39640;&#30340;R-squared&#20540;&#65292;&#32988;&#36807;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#22312;&#22810;&#20010;&#19978;&#19979;&#25991;&#20013;&#34920;&#26126;CEL&#23545;&#22686;&#37327;&#25968;&#25454;&#20855;&#26377;&#36866;&#24212;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual learning, the ability of a model to learn over time without forgetting previous knowledge and, therefore, be adaptive to new data, is paramount in dynamic fields such as disease outbreak prediction. Deep neural networks, i.e., LSTM, are prone to error due to catastrophic forgetting. This study introduces a novel CEL model for continual learning by leveraging domain adaptation via Elastic Weight Consolidation (EWC). This model aims to mitigate the catastrophic forgetting phenomenon in a domain incremental setting. The Fisher Information Matrix (FIM) is constructed with EWC to develop a regularization term that penalizes changes to important parameters, namely, the important previous knowledge. CEL's performance is evaluated on three distinct diseases, Influenza, Mpox, and Measles, with different metrics. The high R-squared values during evaluation and reevaluation outperform the other state-of-the-art models in several contexts, indicating that CEL adapts to incremental data w
&lt;/p&gt;</description></item><item><title>DeLF&#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#23398;&#20064;&#29615;&#22659;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#35299;&#20915;&#22312;&#23454;&#36341;&#20013;&#24212;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#22256;&#38590;&#12290;&#36890;&#36807;&#27979;&#35797;&#65292;&#35777;&#26126;DeLF&#21487;&#20197;&#20026;&#19981;&#21516;&#30340;&#23398;&#20064;&#29615;&#22659;&#33719;&#24471;&#21487;&#25191;&#34892;&#30340;&#20195;&#30721;&#12290;</title><link>http://arxiv.org/abs/2401.08936</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#30784;&#27169;&#22411;&#35774;&#35745;&#23398;&#20064;&#29615;&#22659;&#30340; DeLF
&lt;/p&gt;
&lt;p&gt;
DeLF: Designing Learning Environments with Foundation Models. (arXiv:2401.08936v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08936
&lt;/p&gt;
&lt;p&gt;
DeLF&#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#23398;&#20064;&#29615;&#22659;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#35299;&#20915;&#22312;&#23454;&#36341;&#20013;&#24212;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#22256;&#38590;&#12290;&#36890;&#36807;&#27979;&#35797;&#65292;&#35777;&#26126;DeLF&#21487;&#20197;&#20026;&#19981;&#21516;&#30340;&#23398;&#20064;&#29615;&#22659;&#33719;&#24471;&#21487;&#25191;&#34892;&#30340;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20026;&#22522;&#26412;&#30340;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#31181;&#33021;&#21147;&#24378;&#22823;&#19988;&#30452;&#35266;&#30340;&#32467;&#26500;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#31361;&#30772;&#65292;&#20294;&#22312;&#35768;&#22810;&#31616;&#21333;&#24212;&#29992;&#20013;&#23454;&#38469;&#24212;&#29992;RL&#20173;&#28982;&#24456;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#29992;&#20110;&#20026;&#32473;&#23450;&#30340;&#12289;&#29992;&#25143;&#39044;&#26399;&#30340;&#24212;&#29992;&#35774;&#35745;RL&#29615;&#22659;&#32452;&#20214;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;RL&#32452;&#20214;&#35774;&#35745;&#38382;&#39064;&#30340;&#21021;&#22987;&#24418;&#24335;&#21270;&#65292;&#37325;&#28857;&#26159;&#35774;&#35745;&#35266;&#23519;&#21644;&#21160;&#20316;&#31354;&#38388;&#30340;&#33391;&#22909;&#34920;&#31034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DeLF&#65306;&#20351;&#29992;&#22522;&#30784;&#27169;&#22411;&#35774;&#35745;&#23398;&#20064;&#29615;&#22659;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#35774;&#35745;&#21644;&#32534;&#30721;&#29992;&#25143;&#39044;&#26399;&#30340;&#23398;&#20064;&#22330;&#26223;&#12290;&#36890;&#36807;&#22312;&#22235;&#20010;&#19981;&#21516;&#30340;&#23398;&#20064;&#29615;&#22659;&#19978;&#27979;&#35797;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;DeLF&#21487;&#20197;&#20026;&#30456;&#24212;&#30340;RL&#38382;&#39064;&#33719;&#24471;&#21487;&#25191;&#34892;&#30340;&#29615;&#22659;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) offers a capable and intuitive structure for the fundamental sequential decision-making problem. Despite impressive breakthroughs, it can still be difficult to employ RL in practice in many simple applications. In this paper, we try to address this issue by introducing a method for designing the components of the RL environment for a given, user-intended application. We provide an initial formalization for the problem of RL component design, that concentrates on designing a good representation for observation and action space. We propose a method named DeLF: Designing Learning Environments with Foundation Models, that employs large language models to design and codify the user's intended learning scenario. By testing our method on four different learning environments, we demonstrate that DeLF can obtain executable environment codes for the corresponding RL problems.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#26500;&#24314;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#24341;&#20837;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#36965;&#24863;&#22270;&#20687;&#20013;&#20113;&#21644;&#38634;&#26816;&#27979;&#30340;&#35757;&#32451;&#31574;&#30053;&#12290;&#21516;&#26102;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#26356;&#21512;&#36866;&#30340;&#27169;&#22411;&#24615;&#33021;&#35780;&#20272;&#26041;&#27861;&#65292;&#26377;&#25928;&#20943;&#23569;&#20102;&#30001;&#20110;&#22122;&#22768;&#26631;&#31614;&#24341;&#36215;&#30340;&#24615;&#33021;&#35780;&#20272;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2401.08932</link><description>&lt;p&gt;
&#23398;&#20064;&#20174;&#22122;&#22768;&#26631;&#31614;&#20013;&#26816;&#27979;&#36965;&#24863;&#22270;&#20687;&#20013;&#30340;&#20113;&#21644;&#38634;
&lt;/p&gt;
&lt;p&gt;
Learning to detect cloud and snow in remote sensing images from noisy labels. (arXiv:2401.08932v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08932
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#26500;&#24314;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#24341;&#20837;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#36965;&#24863;&#22270;&#20687;&#20013;&#20113;&#21644;&#38634;&#26816;&#27979;&#30340;&#35757;&#32451;&#31574;&#30053;&#12290;&#21516;&#26102;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#26356;&#21512;&#36866;&#30340;&#27169;&#22411;&#24615;&#33021;&#35780;&#20272;&#26041;&#27861;&#65292;&#26377;&#25928;&#20943;&#23569;&#20102;&#30001;&#20110;&#22122;&#22768;&#26631;&#31614;&#24341;&#36215;&#30340;&#24615;&#33021;&#35780;&#20272;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#27979;&#36965;&#24863;&#22270;&#20687;&#20013;&#30340;&#20113;&#21644;&#38634;&#26159;&#36965;&#24863;&#22270;&#20687;&#30340;&#37325;&#35201;&#39044;&#22788;&#29702;&#20219;&#21153;&#12290;&#20197;&#21069;&#30340;&#24037;&#20316;&#20511;&#37492;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#65292;&#22823;&#37096;&#20998;&#30740;&#31350;&#38598;&#20013;&#22312;&#25913;&#36827;&#27169;&#22411;&#32467;&#26500;&#20197;&#25552;&#39640;&#26816;&#27979;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#19982;&#33258;&#28982;&#22270;&#20687;&#19981;&#21516;&#65292;&#36965;&#24863;&#22270;&#20687;&#20013;&#22330;&#26223;&#30340;&#22797;&#26434;&#24615;&#21644;&#20113;&#31867;&#22411;&#30340;&#22810;&#26679;&#24615;&#23548;&#33268;&#20113;&#21644;&#38634;&#26816;&#27979;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#35768;&#22810;&#19981;&#20934;&#30830;&#30340;&#26631;&#31614;&#65292;&#23548;&#33268;&#35757;&#32451;&#21644;&#27979;&#35797;&#36807;&#31243;&#20013;&#24341;&#20837;&#20102;&#19981;&#24517;&#35201;&#30340;&#22122;&#22768;&#12290;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#24182;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#21363;&#35838;&#31243;&#23398;&#20064;&#27169;&#24335;&#65292;&#25105;&#20204;&#24341;&#23548;&#27169;&#22411;&#20943;&#23569;&#23545;&#22122;&#22768;&#26631;&#31614;&#30340;&#36807;&#25311;&#21512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26356;&#21512;&#36866;&#30340;&#27169;&#22411;&#24615;&#33021;&#35780;&#20272;&#26041;&#27861;&#65292;&#32531;&#35299;&#20102;&#30001;&#22122;&#22768;&#26631;&#31614;&#24341;&#36215;&#30340;&#24615;&#33021;&#35780;&#20272;&#20559;&#24046;&#12290;&#36890;&#36807;&#22312;UNet&#21644;Segformer&#27169;&#22411;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detecting clouds and snow in remote sensing images is an essential preprocessing task for remote sensing imagery. Previous works draw inspiration from semantic segmentation models in computer vision, with most research focusing on improving model architectures to enhance detection performance. However, unlike natural images, the complexity of scenes and the diversity of cloud types in remote sensing images result in many inaccurate labels in cloud and snow detection datasets, introducing unnecessary noises into the training and testing processes. By constructing a new dataset and proposing a novel training strategy with the curriculum learning paradigm, we guide the model in reducing overfitting to noisy labels. Additionally, we design a more appropriate model performance evaluation method, that alleviates the performance assessment bias caused by noisy labels. By conducting experiments on models with UNet and Segformer, we have validated the effectiveness of our proposed method. This 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PADS&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25193;&#25955;&#21512;&#25104;&#36807;&#31243;&#23398;&#20064;&#23039;&#21183;&#20808;&#39564;&#65292;&#35299;&#20915;3D&#20154;&#20307;&#23039;&#21183;&#20998;&#26512;&#20013;&#30340;&#21508;&#31181;&#25361;&#25112;&#65292;&#23558;&#22810;&#20010;&#23039;&#21183;&#20998;&#26512;&#20219;&#21153;&#32479;&#19968;&#20026;&#36870;&#38382;&#39064;&#30340;&#23454;&#20363;&#65292;&#39564;&#35777;&#20102;&#20854;&#24615;&#33021;&#21644;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.08930</link><description>&lt;p&gt;
&#36890;&#36807;&#25193;&#25955;&#21512;&#25104;&#36827;&#34892;3D&#20154;&#20307;&#23039;&#21183;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
3D Human Pose Analysis via Diffusion Synthesis. (arXiv:2401.08930v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08930
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PADS&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25193;&#25955;&#21512;&#25104;&#36807;&#31243;&#23398;&#20064;&#23039;&#21183;&#20808;&#39564;&#65292;&#35299;&#20915;3D&#20154;&#20307;&#23039;&#21183;&#20998;&#26512;&#20013;&#30340;&#21508;&#31181;&#25361;&#25112;&#65292;&#23558;&#22810;&#20010;&#23039;&#21183;&#20998;&#26512;&#20219;&#21153;&#32479;&#19968;&#20026;&#36870;&#38382;&#39064;&#30340;&#23454;&#20363;&#65292;&#39564;&#35777;&#20102;&#20854;&#24615;&#33021;&#21644;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#29983;&#25104;&#24314;&#27169;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PADS&#65288;&#36890;&#36807;&#25193;&#25955;&#21512;&#25104;&#36827;&#34892;&#23039;&#21183;&#20998;&#26512;&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#19968;&#20010;&#32479;&#19968;&#30340;&#27969;&#31243;&#35299;&#20915;3D&#20154;&#20307;&#23039;&#21183;&#20998;&#26512;&#20013;&#30340;&#21508;&#31181;&#25361;&#25112;&#12290;PADS&#30340;&#26680;&#24515;&#26159;&#20004;&#20010;&#29420;&#29305;&#30340;&#31574;&#30053;&#65306;i&#65289;&#20351;&#29992;&#25193;&#25955;&#21512;&#25104;&#36807;&#31243;&#23398;&#20064;&#19968;&#20010;&#20219;&#21153;&#26080;&#20851;&#30340;&#23039;&#21183;&#20808;&#39564;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#25429;&#25417;&#20154;&#20307;&#23039;&#21183;&#25968;&#25454;&#20013;&#30340;&#36816;&#21160;&#32422;&#26463;&#65307;ii&#65289;&#23558;&#20272;&#35745;&#12289;&#34917;&#20840;&#12289;&#21435;&#22122;&#31561;&#22810;&#20010;&#23039;&#21183;&#20998;&#26512;&#20219;&#21153;&#32479;&#19968;&#20026;&#36870;&#38382;&#39064;&#30340;&#23454;&#20363;&#12290;&#23398;&#20064;&#21040;&#30340;&#23039;&#21183;&#20808;&#39564;&#23558;&#34987;&#35270;&#20026;&#23545;&#20219;&#21153;&#29305;&#23450;&#32422;&#26463;&#30340;&#27491;&#21017;&#21270;&#65292;&#36890;&#36807;&#19968;&#31995;&#21015;&#26465;&#20214;&#21435;&#22122;&#27493;&#39588;&#24341;&#23548;&#20248;&#21270;&#36807;&#31243;&#12290;PADS&#20195;&#34920;&#20102;&#39318;&#20010;&#22522;&#20110;&#25193;&#25955;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#36870;&#38382;&#39064;&#26694;&#26550;&#20869;&#30340;&#36890;&#29992;3D&#20154;&#20307;&#23039;&#21183;&#20998;&#26512;&#12290;&#20854;&#24615;&#33021;&#24050;&#22312;&#19981;&#21516;&#22522;&#20934;&#27979;&#35797;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#65292;&#26174;&#31034;&#20986;&#20854;&#36866;&#24212;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have demonstrated remarkable success in generative modeling. In this paper, we propose PADS (Pose Analysis by Diffusion Synthesis), a novel framework designed to address various challenges in 3D human pose analysis through a unified pipeline. Central to PADS are two distinctive strategies: i) learning a task-agnostic pose prior using a diffusion synthesis process to effectively capture the kinematic constraints in human pose data, and ii) unifying multiple pose analysis tasks like estimation, completion, denoising, etc, as instances of inverse problems. The learned pose prior will be treated as a regularization imposing on task-specific constraints, guiding the optimization process through a series of conditional denoising steps. PADS represents the first diffusion-based framework for tackling general 3D human pose analysis within the inverse problem framework. Its performance has been validated on different benchmarks, signaling the adaptability and robustness of this
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#29366;&#24577;&#21644;&#21382;&#21490;&#34920;&#31034;&#38388;&#30340;&#20851;&#31995;&#65292;&#21457;&#29616;&#20102;&#36825;&#20123;&#26041;&#27861;&#21644;&#26694;&#26550;&#23454;&#38469;&#19978;&#37117;&#22522;&#20110;&#33258;&#39044;&#27979;&#25277;&#35937;&#30340;&#20849;&#21516;&#24605;&#24819;&#65292;&#24182;&#25552;&#20379;&#20102;&#29702;&#35770;&#27934;&#35265;&#21644;&#31616;&#21270;&#31639;&#27861;&#26469;&#23398;&#20064;&#33258;&#39044;&#27979;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2401.08898</link><description>&lt;p&gt;
&#26725;&#25509;&#29366;&#24577;&#21644;&#21382;&#21490;&#34920;&#31034;&#65306;&#29702;&#35299;&#33258;&#39044;&#27979;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Bridging State and History Representations: Understanding Self-Predictive RL. (arXiv:2401.08898v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08898
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#29366;&#24577;&#21644;&#21382;&#21490;&#34920;&#31034;&#38388;&#30340;&#20851;&#31995;&#65292;&#21457;&#29616;&#20102;&#36825;&#20123;&#26041;&#27861;&#21644;&#26694;&#26550;&#23454;&#38469;&#19978;&#37117;&#22522;&#20110;&#33258;&#39044;&#27979;&#25277;&#35937;&#30340;&#20849;&#21516;&#24605;&#24819;&#65292;&#24182;&#25552;&#20379;&#20102;&#29702;&#35770;&#27934;&#35265;&#21644;&#31616;&#21270;&#31639;&#27861;&#26469;&#23398;&#20064;&#33258;&#39044;&#27979;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#31034;&#26159;&#25152;&#26377;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#26680;&#24515;&#65292;&#36866;&#29992;&#20110;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#21644;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDP&#65289;&#12290;&#35768;&#22810;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#21644;&#29702;&#35770;&#26694;&#26550;&#34987;&#24320;&#21457;&#29992;&#20110;&#29702;&#35299;&#20160;&#20040;&#26500;&#25104;&#20102;&#26377;&#25928;&#30340;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20043;&#38388;&#30340;&#20851;&#31995;&#21644;&#23427;&#20204;&#20043;&#38388;&#30340;&#20849;&#21516;&#23646;&#24615;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35768;&#22810;&#30475;&#20284;&#19981;&#21516;&#30340;&#29366;&#24577;&#21644;&#21382;&#21490;&#25277;&#35937;&#26041;&#27861;&#21644;&#26694;&#26550;&#23454;&#38469;&#19978;&#22522;&#20110;&#33258;&#39044;&#27979;&#25277;&#35937;&#30340;&#20849;&#21516;&#24605;&#24819;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#24191;&#27867;&#37319;&#29992;&#30340;&#30446;&#26631;&#21644;&#20248;&#21270;&#65288;&#22914;&#20572;&#26799;&#24230;&#25216;&#26415;&#65289;&#22312;&#23398;&#20064;&#33258;&#39044;&#27979;&#34920;&#31034;&#20013;&#30340;&#29702;&#35770;&#27934;&#35265;&#12290;&#36825;&#20123;&#21457;&#29616;&#20849;&#21516;&#20135;&#29983;&#20102;&#19968;&#31181;&#31616;&#21270;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#29366;&#24577;&#21644;&#21382;&#21490;&#30340;&#33258;&#39044;&#27979;&#34920;&#31034;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#25105;&#20204;&#30340;&#31639;&#27861;&#24212;&#29992;&#20110;&#26631;&#20934;MDP&#12289;&#24102;&#26377;dist&#30340;MDP&#36827;&#34892;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Representations are at the core of all deep reinforcement learning (RL) methods for both Markov decision processes (MDPs) and partially observable Markov decision processes (POMDPs). Many representation learning methods and theoretical frameworks have been developed to understand what constitutes an effective representation. However, the relationships between these methods and the shared properties among them remain unclear. In this paper, we show that many of these seemingly distinct methods and frameworks for state and history abstractions are, in fact, based on a common idea of self-predictive abstraction. Furthermore, we provide theoretical insights into the widely adopted objectives and optimization, such as the stop-gradient technique, in learning self-predictive representations. These findings together yield a minimalist algorithm to learn self-predictive representations for states and histories. We validate our theories by applying our algorithm to standard MDPs, MDPs with dist
&lt;/p&gt;</description></item><item><title>CFASL&#26159;&#19968;&#31181;&#29992;&#20110;&#35299;&#32544;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#23558;&#23545;&#31216;&#24615;&#23398;&#20064;&#19982;VAE&#38598;&#25104;&#65292;&#26080;&#38656;&#20219;&#20309;&#25968;&#25454;&#38598;&#22240;&#23376;&#20449;&#24687;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#20855;&#26377;&#19977;&#20010;&#26032;&#29305;&#24449;&#65306;&#23545;&#40784;&#28508;&#22312;&#21521;&#37327;&#32500;&#24230;&#21040;&#21487;&#23398;&#20064;&#23545;&#31216;&#20195;&#30721;&#31807;&#20013;&#30340;&#23545;&#31216;&#24615;&#65292;&#23398;&#20064;&#22797;&#21512;&#23545;&#31216;&#24615;&#26469;&#34920;&#36798;&#26410;&#30693;&#22240;&#32032;&#30340;&#21464;&#21270;&#65292;&#20197;&#21450;&#24341;&#20837;&#32676;&#31561;&#21464;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#26469;&#35757;&#32451;VAE&#12290;</title><link>http://arxiv.org/abs/2401.08897</link><description>&lt;p&gt;
CFASL&#65306;&#29992;&#20110;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#20013;&#30340;&#35299;&#32544;&#23398;&#20064;&#30340;&#22797;&#21512;&#22240;&#23376;&#23545;&#40784;&#23545;&#31216;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
CFASL: Composite Factor-Aligned Symmetry Learning for Disentanglement in Variational AutoEncoder. (arXiv:2401.08897v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08897
&lt;/p&gt;
&lt;p&gt;
CFASL&#26159;&#19968;&#31181;&#29992;&#20110;&#35299;&#32544;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#23558;&#23545;&#31216;&#24615;&#23398;&#20064;&#19982;VAE&#38598;&#25104;&#65292;&#26080;&#38656;&#20219;&#20309;&#25968;&#25454;&#38598;&#22240;&#23376;&#20449;&#24687;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#20855;&#26377;&#19977;&#20010;&#26032;&#29305;&#24449;&#65306;&#23545;&#40784;&#28508;&#22312;&#21521;&#37327;&#32500;&#24230;&#21040;&#21487;&#23398;&#20064;&#23545;&#31216;&#20195;&#30721;&#31807;&#20013;&#30340;&#23545;&#31216;&#24615;&#65292;&#23398;&#20064;&#22797;&#21512;&#23545;&#31216;&#24615;&#26469;&#34920;&#36798;&#26410;&#30693;&#22240;&#32032;&#30340;&#21464;&#21270;&#65292;&#20197;&#21450;&#24341;&#20837;&#32676;&#31561;&#21464;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#26469;&#35757;&#32451;VAE&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36755;&#20837;&#21644;&#28508;&#22312;&#21521;&#37327;&#30340;&#23545;&#31216;&#24615;&#20026;VAE&#20013;&#30340;&#35299;&#32544;&#23398;&#20064;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;&#28982;&#32780;&#65292;&#21482;&#26377;&#23569;&#25968;&#20960;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#29978;&#33267;&#36825;&#20123;&#26041;&#27861;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#20063;&#38656;&#35201;&#24050;&#30693;&#30340;&#22240;&#23376;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;Composite Factor-Aligned Symmetry Learning (CFASL)&#65292;&#23558;&#20854;&#38598;&#25104;&#21040;VAE&#20013;&#65292;&#29992;&#20110;&#23398;&#20064;&#22522;&#20110;&#23545;&#31216;&#24615;&#30340;&#35299;&#32544;&#65292;&#26080;&#30417;&#30563;&#23398;&#20064;&#20013;&#19981;&#38656;&#35201;&#20219;&#20309;&#25968;&#25454;&#38598;&#22240;&#23376;&#20449;&#24687;&#30340;&#30693;&#35782;&#12290;CFASL&#21253;&#25324;&#19977;&#20010;&#29992;&#20110;&#23398;&#20064;&#22522;&#20110;&#23545;&#31216;&#24615;&#30340;&#35299;&#32544;&#30340;&#26032;&#29305;&#24449;&#65306;1)&#27880;&#20837;&#24402;&#32435;&#20559;&#32622;&#65292;&#23558;&#28508;&#22312;&#21521;&#37327;&#32500;&#24230;&#23545;&#40784;&#21040;&#26126;&#30830;&#21487;&#23398;&#20064;&#30340;&#23545;&#31216;&#20195;&#30721;&#31807;&#20013;&#30340;&#22240;&#23376;&#23545;&#40784;&#23545;&#31216;&#24615;&#65307;2)&#23398;&#20064;&#19968;&#20010;&#22797;&#21512;&#23545;&#31216;&#24615;&#65292;&#36890;&#36807;&#23398;&#20064;&#20195;&#30721;&#31807;&#20013;&#30340;&#22240;&#23376;&#23545;&#40784;&#23545;&#31216;&#24615;&#65292;&#26469;&#34920;&#36798;&#20004;&#20010;&#38543;&#26426;&#26679;&#26412;&#20043;&#38388;&#30340;&#26410;&#30693;&#22240;&#32032;&#30340;&#21464;&#21270;&#65307;3)&#22312;&#35757;&#32451;VAE&#26102;&#65292;&#24341;&#20837;&#20855;&#26377;&#32676;&#31561;&#21464;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#30340;&#20004;&#20010;&#26465;&#20214;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Symmetries of input and latent vectors have provided valuable insights for disentanglement learning in VAEs.However, only a few works were proposed as an unsupervised method, and even these works require known factor information in training data. We propose a novel method, Composite Factor-Aligned Symmetry Learning (CFASL), which is integrated into VAEs for learning symmetry-based disentanglement in unsupervised learning without any knowledge of the dataset factor information.CFASL incorporates three novel features for learning symmetry-based disentanglement: 1) Injecting inductive bias to align latent vector dimensions to factor-aligned symmetries within an explicit learnable symmetry codebook 2) Learning a composite symmetry to express unknown factors change between two random samples by learning factor-aligned symmetries within the codebook 3) Inducing group equivariant encoder and decoder in training VAEs with the two conditions. In addition, we propose an extended evaluation metri
&lt;/p&gt;</description></item><item><title>NOTSOFAR-1&#25361;&#25112;&#26088;&#22312;&#35299;&#20915;&#36828;&#36317;&#31163;&#20250;&#35758;&#36716;&#24405;&#20013;&#30340;&#36828;&#36317;&#31163;&#21457;&#35328;&#20154;&#21010;&#20998;&#21644;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#38382;&#39064;&#65292;&#22312;&#19968;&#20010;&#26032;&#21457;&#24067;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2401.08887</link><description>&lt;p&gt;
NOTSOFAR-1&#25361;&#25112;&#65306;&#36828;&#36317;&#31163;&#20250;&#35758;&#36716;&#24405;&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#22522;&#20934;&#32447;&#21644;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
NOTSOFAR-1 Challenge: New Datasets, Baseline, and Tasks for Distant Meeting Transcription. (arXiv:2401.08887v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08887
&lt;/p&gt;
&lt;p&gt;
NOTSOFAR-1&#25361;&#25112;&#26088;&#22312;&#35299;&#20915;&#36828;&#36317;&#31163;&#20250;&#35758;&#36716;&#24405;&#20013;&#30340;&#36828;&#36317;&#31163;&#21457;&#35328;&#20154;&#21010;&#20998;&#21644;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#38382;&#39064;&#65292;&#22312;&#19968;&#20010;&#26032;&#21457;&#24067;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#33258;&#28982;&#21150;&#20844;&#23460;&#35848;&#35805;&#32773;&#22312;&#36828;&#22330;&#38899;&#39057;&#35760;&#24405;&#29615;&#22659;&#20013;&#65288;&#8220;NOTSOFAR-1&#8221;&#65289;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#31995;&#32479;&#12290;&#35813;&#25361;&#25112;&#20027;&#35201;&#20851;&#27880;&#36828;&#22330;&#20250;&#35758;&#20013;&#30340;&#36828;&#36317;&#31163;&#21457;&#35328;&#20154;&#21010;&#20998;&#21644;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;DASR&#65289;&#65292;&#21253;&#25324;&#21333;&#22768;&#36947;&#21644;&#24050;&#30693;&#20960;&#20309;&#22810;&#36890;&#36947;&#30340;&#36712;&#36947;&#65292;&#24182;&#19988;&#20316;&#20026;&#20004;&#20010;&#26032;&#25968;&#25454;&#38598;&#30340;&#21457;&#24067;&#24179;&#21488;&#65306;&#31532;&#19968;&#20010;&#25968;&#25454;&#38598;&#26159;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;315&#20010;&#20250;&#35758;&#65292;&#27599;&#20010;&#20250;&#35758;&#24179;&#22343;6&#20998;&#38047;&#65292;&#25429;&#25417;&#21040;&#24191;&#27867;&#30340;&#30495;&#23454;&#19990;&#30028;&#22768;&#23398;&#29615;&#22659;&#21644;&#23545;&#35805;&#21160;&#24577;&#12290;&#23427;&#26159;&#22312;30&#20010;&#20250;&#35758;&#23460;&#20013;&#24405;&#21046;&#30340;&#65292;&#27599;&#20010;&#20250;&#35758;&#23460;&#26377;4-8&#20010;&#19982;&#20250;&#32773;&#21644;&#20849;&#35745;35&#20010;&#19981;&#21516;&#30340;&#21457;&#35328;&#32773;&#12290;&#31532;&#20108;&#20010;&#25968;&#25454;&#38598;&#26159;&#19968;&#20010;1000&#23567;&#26102;&#30340;&#27169;&#25311;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#20855;&#26377;&#22686;&#24378;&#30340;&#30495;&#23454;&#24615;&#20197;&#23454;&#29616;&#30495;&#23454;&#19990;&#30028;&#30340;&#27867;&#21270;&#65292;&#21253;&#25324;15000&#20010;&#30495;&#23454;&#22768;&#23398;&#36716;&#31227;&#20989;&#25968;&#12290;&#20219;&#21153;&#38598;&#20013;&#22312;&#21333;&#35774;&#22791;DASR&#19978;&#65292;&#20854;&#20013;&#22810;&#36890;&#36947;&#35774;&#22791;&#22987;&#32456;&#20849;&#20139;&#30456;&#21516;&#24050;&#30693;&#30340;&#20960;&#20309;&#24418;&#29366;&#12290;&#36825;&#19982;&#23454;&#38469;&#20250;&#35758;&#23460;&#30340;&#24120;&#35265;&#35774;&#32622;&#26159;&#19968;&#33268;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the first Natural Office Talkers in Settings of Far-field Audio Recordings (``NOTSOFAR-1'') Challenge alongside datasets and baseline system. The challenge focuses on distant speaker diarization and automatic speech recognition (DASR) in far-field meeting scenarios, with single-channel and known-geometry multi-channel tracks, and serves as a launch platform for two new datasets: First, a benchmarking dataset of 315 meetings, averaging 6 minutes each, capturing a broad spectrum of real-world acoustic conditions and conversational dynamics. It is recorded across 30 conference rooms, featuring 4-8 attendees and a total of 35 unique speakers. Second, a 1000-hour simulated training dataset, synthesized with enhanced authenticity for real-world generalization, incorporating 15,000 real acoustic transfer functions. The tasks focus on single-device DASR, where multi-channel devices always share the same known geometry. This is aligned with common setups in actual conference rooms,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#37327;&#21270;&#21452;&#26497;&#35770;&#35777;&#22270;&#30340;&#36129;&#29486;&#20989;&#25968;&#36827;&#34892;&#20102;&#22522;&#20110;&#21407;&#21017;&#30340;&#20998;&#26512;&#65292;&#20026;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#20989;&#25968;&#25552;&#20379;&#20102;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2401.08879</link><description>&lt;p&gt;
&#37327;&#21270;&#21452;&#26497;&#35770;&#35777;&#22270;&#30340;&#36129;&#29486;&#20989;&#25968;&#65306;&#22522;&#20110;&#21407;&#21017;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Contribution Functions for Quantitative Bipolar Argumentation Graphs: A Principle-based Analysis. (arXiv:2401.08879v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08879
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#37327;&#21270;&#21452;&#26497;&#35770;&#35777;&#22270;&#30340;&#36129;&#29486;&#20989;&#25968;&#36827;&#34892;&#20102;&#22522;&#20110;&#21407;&#21017;&#30340;&#20998;&#26512;&#65292;&#20026;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#20989;&#25968;&#25552;&#20379;&#20102;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#37327;&#21270;&#21452;&#26497;&#35770;&#35777;&#22270;&#30340;&#36129;&#29486;&#20989;&#25968;&#36827;&#34892;&#20102;&#22522;&#20110;&#21407;&#21017;&#30340;&#20998;&#26512;&#65292;&#35813;&#20989;&#25968;&#37327;&#21270;&#20102;&#19968;&#20010;&#35770;&#35777;&#23545;&#21478;&#19968;&#20010;&#35770;&#35777;&#30340;&#36129;&#29486;&#12290;&#24341;&#20837;&#30340;&#21407;&#21017;&#23558;&#19981;&#21516;&#30340;&#36129;&#29486;&#20989;&#25968;&#30340;&#30452;&#35273;&#24418;&#24335;&#21270;&#65292;&#24182;&#23545;&#36129;&#29486;&#20989;&#25968;&#30340;&#34892;&#20026;&#26377;&#20102;&#26399;&#26395;&#12290;&#30001;&#20110;&#27809;&#26377;&#19968;&#20010;&#35206;&#30422;&#30340;&#36129;&#29486;&#20989;&#25968;&#28385;&#36275;&#25152;&#26377;&#21407;&#21017;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#21487;&#20197;&#20316;&#20026;&#19968;&#20010;&#24037;&#20855;&#65292;&#26681;&#25454;&#32473;&#23450;&#29992;&#20363;&#30340;&#35201;&#27714;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a principle-based analysis of contribution functions for quantitative bipolar argumentation graphs that quantify the contribution of one argument to another. The introduced principles formalise the intuitions underlying different contribution functions as well as expectations one would have regarding the behaviour of contribution functions in general. As none of the covered contribution functions satisfies all principles, our analysis can serve as a tool that enables the selection of the most suitable function based on the requirements of a given use case.
&lt;/p&gt;</description></item><item><title>DCRMTA&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#20559;&#30340;&#22810;&#35302;&#28857;&#24402;&#22240;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;&#36716;&#21270;&#39044;&#27979;&#27169;&#22411;&#21644;&#26500;&#24314;&#23545;&#29031;&#35302;&#28857;&#24207;&#21015;&#26469;&#20943;&#36731;&#20559;&#24046;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.08875</link><description>&lt;p&gt;
DCRMTA: &#26080;&#20559;&#30340;&#22810;&#35302;&#28857;&#24402;&#22240;&#30340;&#22240;&#26524;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
DCRMTA: Unbiased Causal Representation for Multi-touch Attribution. (arXiv:2401.08875v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08875
&lt;/p&gt;
&lt;p&gt;
DCRMTA&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#20559;&#30340;&#22810;&#35302;&#28857;&#24402;&#22240;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;&#36716;&#21270;&#39044;&#27979;&#27169;&#22411;&#21644;&#26500;&#24314;&#23545;&#29031;&#35302;&#28857;&#24207;&#21015;&#26469;&#20943;&#36731;&#20559;&#24046;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35302;&#28857;&#24402;&#22240;&#65288;MTA&#65289;&#22312;&#23454;&#29616;&#23545;&#27599;&#20010;&#24191;&#21578;&#35302;&#28857;&#23545;&#20110;&#36716;&#21270;&#34892;&#20026;&#30340;&#36129;&#29486;&#30340;&#20844;&#27491;&#20272;&#35745;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#28145;&#21051;&#24433;&#21709;&#39044;&#31639;&#20998;&#37197;&#21644;&#24191;&#21578;&#25512;&#33616;&#12290;&#20256;&#32479;&#30340;&#22810;&#35302;&#28857;&#24402;&#22240;&#26041;&#27861;&#39318;&#20808;&#26500;&#24314;&#19968;&#20010;&#36716;&#21270;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#21382;&#21490;&#25968;&#25454;&#23398;&#20064;&#35302;&#28857;&#24207;&#21015;&#21644;&#29992;&#25143;&#36141;&#20080;&#34892;&#20026;&#20043;&#38388;&#30340;&#20869;&#22312;&#20851;&#31995;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#20174;&#21407;&#22987;&#24207;&#21015;&#23376;&#38598;&#20013;&#26500;&#24314;&#23545;&#29031;&#35302;&#28857;&#24207;&#21015;&#65292;&#24182;&#20351;&#29992;&#39044;&#27979;&#27169;&#22411;&#20272;&#35745;&#36716;&#21270;&#65292;&#20174;&#32780;&#35745;&#31639;&#24191;&#21578;&#36129;&#29486;&#12290;&#36825;&#20123;&#26041;&#27861;&#30340;&#19968;&#20010;&#38544;&#21547;&#20551;&#35774;&#26159;&#36716;&#21270;&#39044;&#27979;&#27169;&#22411;&#30340;&#26080;&#20559;&#24615;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#29992;&#25143;&#20559;&#22909;&#21644;&#20114;&#32852;&#32593;&#25512;&#33616;&#26426;&#21046;&#65288;&#22914;&#36807;&#21435;&#30340;&#36141;&#29289;&#35760;&#24405;&#23548;&#33268;&#30340;&#24191;&#21578;&#25512;&#33616;&#21516;&#36136;&#21270;&#65289;&#24341;&#36215;&#30340;&#28151;&#26434;&#21464;&#37327;&#22240;&#32032;&#65292;&#36716;&#21270;&#20013;&#24456;&#23481;&#26131;&#20135;&#29983;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-touch attribution (MTA) currently plays a pivotal role in achieving a fair estimation of the contributions of each advertising touchpoint to-wards conversion behavior, deeply influencing budget allocation and advertising recommenda-tion. Traditional multi-touch attribution methods initially build a conversion prediction model, an-ticipating learning the inherent relationship be-tween touchpoint sequences and user purchasing behavior through historical data. Based on this, counterfactual touchpoint sequences are con-structed from the original sequence subset, and conversions are estimated using the prediction model, thus calculating advertising contributions. A covert assumption of these methods is the un-biased nature of conversion prediction models. However, due to confounding variables factors arising from user preferences and internet recom-mendation mechanisms such as homogenization of ad recommendations resulting from past shop-ping records, bias can easily occur in conversi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#21644;&#36229;&#23485;&#24102;&#20256;&#24863;&#22120;&#23545;&#36710;&#36742;&#20013;&#30340;&#38053;&#21273;&#25187;&#36827;&#34892;&#23450;&#20301;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#40065;&#26834;&#23450;&#20301;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#27809;&#26377;&#23545;&#25239;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#27604;&#22522;&#20934;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.08863</link><description>&lt;p&gt;
&#20351;&#29992;&#36229;&#23485;&#24102;&#20256;&#24863;&#22120;&#30340;&#36890;&#36947;&#33033;&#20914;&#21709;&#24212;&#26469;&#23454;&#29616;&#38053;&#21273;&#26080;&#32447;&#36827;&#20837;&#31995;&#32479;&#20013;&#30340;&#23494;&#38053;&#25187;&#30340;&#40065;&#26834;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Robust Localization of Key Fob Using Channel Impulse Response of Ultra Wide Band Sensors for Keyless Entry Systems. (arXiv:2401.08863v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08863
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#21644;&#36229;&#23485;&#24102;&#20256;&#24863;&#22120;&#23545;&#36710;&#36742;&#20013;&#30340;&#38053;&#21273;&#25187;&#36827;&#34892;&#23450;&#20301;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#40065;&#26834;&#23450;&#20301;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#27809;&#26377;&#23545;&#25239;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#27604;&#22522;&#20934;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#23545;&#36710;&#36742;&#20869;&#37096;&#21644;&#21608;&#22260;&#30340;&#38053;&#21273;&#25187;&#36827;&#34892;&#23450;&#20301;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#36710;&#36742;&#23433;&#20840;&#21151;&#33021;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20197;&#19979;&#38382;&#39064;&#65306;1&#65289;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#36229;&#23485;&#24102;&#65288;UWB&#65289;&#23450;&#20301;&#20998;&#31867;&#30340;&#39044;&#35745;&#31639;&#29305;&#24449;&#30340;&#24615;&#33021;&#20316;&#20026;&#25105;&#20204;&#23454;&#39564;&#30340;&#22522;&#20934;&#12290;2&#65289;&#30740;&#31350;&#21508;&#31181;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#65292;&#21253;&#25324;&#27809;&#26377;&#20219;&#20309;&#23545;&#25239;&#35757;&#32451;&#30340;&#23545;&#25239;&#31034;&#20363;&#30340;&#40065;&#26834;&#24615;&#12290;3&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#22836;&#33258;&#30417;&#30563;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#22312;&#27809;&#26377;&#20219;&#20309;&#23545;&#25239;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#20248;&#20110;&#22522;&#20934;&#31070;&#32463;&#32593;&#32476;&#12290;&#27169;&#22411;&#30340;&#24615;&#33021;&#22312;&#24555;&#36895;&#26799;&#24230;&#31526;&#21495;&#26041;&#27861;&#30340;&#26576;&#20123;&#23545;&#25239;&#24378;&#24230;&#33539;&#22260;&#20869;&#25552;&#39640;&#20102;67&#65285;&#65292;&#23545;&#20110;&#22522;&#26412;&#36845;&#20195;&#26041;&#27861;&#21644;&#25237;&#24433;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#20998;&#21035;&#25552;&#39640;&#20102;37&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Using neural networks for localization of key fob within and surrounding a car as a security feature for keyless entry is fast emerging. In this paper we study: 1) the performance of pre-computed features of neural networks based UWB (ultra wide band) localization classification forming the baseline of our experiments. 2) Investigate the inherent robustness of various neural networks; therefore, we include the study of robustness of the adversarial examples without any adversarial training in this work. 3) Propose a multi-head self-supervised neural network architecture which outperforms the baseline neural networks without any adversarial training. The model's performance improved by 67% at certain ranges of adversarial magnitude for fast gradient sign method and 37% each for basic iterative method and projected gradient descent method.
&lt;/p&gt;</description></item><item><title>REValueD&#26159;&#19968;&#31181;&#36890;&#36807;&#27491;&#21017;&#21270;&#38598;&#21512;&#20540;&#20998;&#35299;&#30340;&#26032;&#31639;&#27861;&#65292;&#38024;&#23545;&#39640;&#32500;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#30340;&#20219;&#21153;&#25552;&#20379;&#20102;&#20248;&#36234;&#24615;&#33021;&#65292;&#23588;&#20854;&#22312;&#20154;&#24418;&#21644;&#29399;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#23427;&#36943;&#21046;&#20102;Q-learning&#31639;&#27861;&#30340;&#39640;&#20272;&#20559;&#24046;&#65292;&#24182;&#20943;&#36731;&#20102;&#30446;&#26631;&#26041;&#24046;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.08850</link><description>&lt;p&gt;
REValueD: &#23545;&#21487;&#20998;&#35299;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#36827;&#34892;&#27491;&#21017;&#21270;&#38598;&#21512;&#20540;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
REValueD: Regularised Ensemble Value-Decomposition for Factorisable Markov Decision Processes. (arXiv:2401.08850v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08850
&lt;/p&gt;
&lt;p&gt;
REValueD&#26159;&#19968;&#31181;&#36890;&#36807;&#27491;&#21017;&#21270;&#38598;&#21512;&#20540;&#20998;&#35299;&#30340;&#26032;&#31639;&#27861;&#65292;&#38024;&#23545;&#39640;&#32500;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#30340;&#20219;&#21153;&#25552;&#20379;&#20102;&#20248;&#36234;&#24615;&#33021;&#65292;&#23588;&#20854;&#22312;&#20154;&#24418;&#21644;&#29399;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#23427;&#36943;&#21046;&#20102;Q-learning&#31639;&#27861;&#30340;&#39640;&#20272;&#20559;&#24046;&#65292;&#24182;&#20943;&#36731;&#20102;&#30446;&#26631;&#26041;&#24046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#21487;&#33021;&#30340;&#21160;&#20316;&#25968;&#37327;&#24222;&#22823;&#65292;&#31163;&#25955;&#21160;&#20316;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#20855;&#26377;&#39640;&#32500;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#30340;&#20219;&#21153;&#20013;&#32463;&#24120;&#22833;&#36133;&#12290;&#26368;&#36817;&#30340;&#19968;&#39033;&#36827;&#23637;&#21033;&#29992;&#20102;&#26469;&#33258;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#27010;&#24565;&#8212;&#8212;&#20540;&#20998;&#35299;&#65292;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#36825;&#39033;&#30740;&#31350;&#28145;&#20837;&#25506;&#35752;&#20102;&#20540;&#20998;&#35299;&#30340;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;&#23427;&#34429;&#28982;&#21487;&#20197;&#36943;&#21046;Q&#23398;&#20064;&#31639;&#27861;&#22266;&#26377;&#30340;&#39640;&#20272;&#20559;&#24046;&#65292;&#20294;&#20063;&#20250;&#25918;&#22823;&#30446;&#26631;&#26041;&#24046;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#35770;&#23478;&#30340;&#38598;&#21512;&#20197;&#20943;&#36731;&#30446;&#26631;&#26041;&#24046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#27491;&#21017;&#21270;&#25439;&#22833;&#65292;&#26377;&#21161;&#20110;&#20943;&#36731;&#19968;&#20010;&#32500;&#24230;&#19978;&#30340;&#25506;&#32034;&#24615;&#21160;&#20316;&#23545;&#20854;&#20182;&#32500;&#24230;&#19978;&#26368;&#20248;&#21160;&#20316;&#20215;&#20540;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#26032;&#39062;&#31639;&#27861;REValueD&#65292;&#22312;&#32463;&#36807;&#31163;&#25955;&#21270;&#30340;DeepMind&#25511;&#21046;&#22871;&#20214;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#22256;&#38590;&#30340;&#20154;&#24418;&#21644;&#29399;&#31867;&#20219;&#21153;&#20013;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20998;&#26512;&#20102;&#24433;&#21709;REValueD&#34920;&#29616;&#30340;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discrete-action reinforcement learning algorithms often falter in tasks with high-dimensional discrete action spaces due to the vast number of possible actions. A recent advancement leverages value-decomposition, a concept from multi-agent reinforcement learning, to tackle this challenge. This study delves deep into the effects of this value-decomposition, revealing that whilst it curtails the over-estimation bias inherent to Q-learning algorithms, it amplifies target variance. To counteract this, we present an ensemble of critics to mitigate target variance. Moreover, we introduce a regularisation loss that helps to mitigate the effects that exploratory actions in one dimension can have on the value of optimal actions in other dimensions. Our novel algorithm, REValueD, tested on discretised versions of the DeepMind Control Suite tasks, showcases superior performance, especially in the challenging humanoid and dog tasks. We further dissect the factors influencing REValueD's performance
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20445;&#23432;&#23494;&#24230;&#20272;&#35745;&#65288;CDE&#65289;&#30340;&#35757;&#32451;&#31639;&#27861;&#65292;&#36890;&#36807;&#26126;&#30830;&#32422;&#26463;&#29366;&#24577;-&#34892;&#20026;&#21344;&#25454;&#31283;&#24577;&#20998;&#24067;&#26469;&#35299;&#20915;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#22806;&#25512;&#38169;&#35823;&#38382;&#39064;&#12290;&#22312;&#31232;&#30095;&#22870;&#21169;&#25110;&#19981;&#36275;&#25968;&#25454;&#30340;&#20219;&#21153;&#20013;&#65292;CDE&#26174;&#31034;&#20986;&#26126;&#26174;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.08819</link><description>&lt;p&gt;
&#36890;&#36807;&#20445;&#23432;&#23494;&#24230;&#20272;&#35745;&#20174;&#31232;&#30095;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning from Sparse Offline Datasets via Conservative Density Estimation. (arXiv:2401.08819v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08819
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20445;&#23432;&#23494;&#24230;&#20272;&#35745;&#65288;CDE&#65289;&#30340;&#35757;&#32451;&#31639;&#27861;&#65292;&#36890;&#36807;&#26126;&#30830;&#32422;&#26463;&#29366;&#24577;-&#34892;&#20026;&#21344;&#25454;&#31283;&#24577;&#20998;&#24067;&#26469;&#35299;&#20915;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#22806;&#25512;&#38169;&#35823;&#38382;&#39064;&#12290;&#22312;&#31232;&#30095;&#22870;&#21169;&#25110;&#19981;&#36275;&#25968;&#25454;&#30340;&#20219;&#21153;&#20013;&#65292;CDE&#26174;&#31034;&#20986;&#26126;&#26174;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20026;&#20174;&#39044;&#20808;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#31574;&#30053;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#21521;&#65292;&#32780;&#26080;&#38656;&#19982;&#29615;&#22659;&#36827;&#19968;&#27493;&#20132;&#20114;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#22312;&#22788;&#29702;&#20998;&#24067;&#22806;&#65288;OOD&#65289;&#22806;&#25512;&#38169;&#35823;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#29305;&#21035;&#26159;&#22312;&#31232;&#30095;&#22870;&#21169;&#25110;&#25968;&#25454;&#31232;&#32570;&#30340;&#24773;&#20917;&#19979;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20445;&#23432;&#23494;&#24230;&#20272;&#35745;&#65288;CDE&#65289;&#30340;&#26032;&#30340;&#35757;&#32451;&#31639;&#27861;&#65292;&#36890;&#36807;&#26126;&#30830;&#32422;&#26463;&#29366;&#24577;-&#34892;&#20026;&#21344;&#25454;&#31283;&#24577;&#20998;&#24067;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#12290;CDE&#36890;&#36807;&#35299;&#20915;&#36793;&#38469;&#37325;&#35201;&#24615;&#25277;&#26679;&#20013;&#30340;&#25903;&#25345;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#22914;&#31283;&#24577;&#20998;&#24067;&#26657;&#27491;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;D4RL&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;CDE&#22312;&#20855;&#26377;&#31232;&#30095;&#22870;&#21169;&#25110;&#19981;&#36275;&#25968;&#25454;&#30340;&#25361;&#25112;&#24615;&#20219;&#21153;&#20013;&#25345;&#32493;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35299;&#20915;&#22806;&#25512;&#38169;&#35823;&#38382;&#39064;&#19978;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (RL) offers a promising direction for learning policies from pre-collected datasets without requiring further interactions with the environment. However, existing methods struggle to handle out-of-distribution (OOD) extrapolation errors, especially in sparse reward or scarce data settings. In this paper, we propose a novel training algorithm called Conservative Density Estimation (CDE), which addresses this challenge by explicitly imposing constraints on the state-action occupancy stationary distribution. CDE overcomes the limitations of existing approaches, such as the stationary distribution correction method, by addressing the support mismatch issue in marginal importance sampling. Our method achieves state-of-the-art performance on the D4RL benchmark. Notably, CDE consistently outperforms baselines in challenging tasks with sparse rewards or insufficient data, demonstrating the advantages of our approach in addressing the extrapolation error problem i
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#30417;&#30563;&#30340;&#24067;&#23616;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65288;ALDM&#65289;&#65292;&#36890;&#36807;&#24341;&#20837;&#20998;&#21106;&#21028;&#21035;&#22120;&#21644;&#22810;&#27493;&#23637;&#24320;&#31574;&#30053;&#65292;&#21487;&#20197;&#25552;&#21319;&#24067;&#23616;&#21040;&#22270;&#20687;&#21512;&#25104;&#20219;&#21153;&#20013;&#30340;&#25991;&#26412;&#32534;&#36753;&#33021;&#21147;&#21644;&#29983;&#25104;&#22270;&#20687;&#19982;&#36755;&#20837;&#24067;&#23616;&#20043;&#38388;&#30340;&#23545;&#40784;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.08815</link><description>&lt;p&gt;
&#23545;&#25239;&#30417;&#30563;&#20351;&#24067;&#23616;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#34028;&#21187;&#21457;&#23637;
&lt;/p&gt;
&lt;p&gt;
Adversarial Supervision Makes Layout-to-Image Diffusion Models Thrive. (arXiv:2401.08815v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08815
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#30417;&#30563;&#30340;&#24067;&#23616;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65288;ALDM&#65289;&#65292;&#36890;&#36807;&#24341;&#20837;&#20998;&#21106;&#21028;&#21035;&#22120;&#21644;&#22810;&#27493;&#23637;&#24320;&#31574;&#30053;&#65292;&#21487;&#20197;&#25552;&#21319;&#24067;&#23616;&#21040;&#22270;&#20687;&#21512;&#25104;&#20219;&#21153;&#20013;&#30340;&#25991;&#26412;&#32534;&#36753;&#33021;&#21147;&#21644;&#29983;&#25104;&#22270;&#20687;&#19982;&#36755;&#20837;&#24067;&#23616;&#20043;&#38388;&#30340;&#23545;&#40784;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#35268;&#27169;&#25193;&#25955;&#27169;&#22411;&#21462;&#24471;&#20102;&#36817;&#26399;&#30340;&#36827;&#23637;&#65292;&#20294;&#24067;&#23616;&#21040;&#22270;&#20687;&#21512;&#25104;&#20219;&#21153;&#30340;&#36827;&#23637;&#36739;&#23567;&#12290;&#24403;&#21069;&#30340;&#24067;&#23616;&#21040;&#22270;&#20687;&#27169;&#22411;&#35201;&#20040;&#22312;&#25991;&#26412;&#32534;&#36753;&#33021;&#21147;&#19978;&#27424;&#20339;&#65292;&#35201;&#20040;&#22312;&#29983;&#25104;&#30340;&#22270;&#20687;&#19982;&#36755;&#20837;&#24067;&#23616;&#20043;&#38388;&#30340;&#23545;&#40784;&#19978;&#36739;&#24369;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#23454;&#38469;&#24212;&#29992;&#24615;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#23545;&#25239;&#30417;&#30563;&#38598;&#25104;&#21040;&#20256;&#32479;&#24067;&#23616;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#35757;&#32451;&#27969;&#31243;&#20013;&#65288;ALDM&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;&#20998;&#21106;&#30340;&#21028;&#21035;&#22120;&#65292;&#35813;&#21028;&#21035;&#22120;&#22312;&#20687;&#32032;&#32423;&#21035;&#19978;&#20026;&#25193;&#25955;&#29983;&#25104;&#22120;&#25552;&#20379;&#26126;&#30830;&#30340;&#21453;&#39304;&#65292;&#29992;&#20110;&#25351;&#23548;&#21435;&#22122;&#22270;&#20687;&#19982;&#36755;&#20837;&#24067;&#23616;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;&#20026;&#20102;&#40723;&#21169;&#22312;&#37319;&#26679;&#27493;&#39588;&#20013;&#23545;&#36755;&#20837;&#24067;&#23616;&#30340;&#19968;&#33268;&#20381;&#20174;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;&#22810;&#27493;&#23637;&#24320;&#31574;&#30053;&#12290;&#25105;&#20204;&#19981;&#26159;&#21482;&#20851;&#27880;&#21333;&#20010;&#26102;&#38388;&#27493;&#65292;&#32780;&#26159;&#36882;&#24402;&#22320;&#23637;&#24320;&#20960;&#20010;&#27493;&#39588;&#26469;&#27169;&#25311;&#25512;&#29702;&#36807;&#31243;&#65292;&#24182;&#35201;&#27714;&#21028;&#21035;&#22120;&#22312;&#19968;&#23450;&#26102;&#38388;&#31383;&#21475;&#20869;&#35780;&#20272;&#21435;&#22122;&#22270;&#20687;&#19982;&#24067;&#23616;&#30340;&#23545;&#40784;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the recent advances in large-scale diffusion models, little progress has been made on the layout-to-image (L2I) synthesis task. Current L2I models either suffer from poor editability via text or weak alignment between the generated image and the input layout. This limits their usability in practice. To mitigate this, we propose to integrate adversarial supervision into the conventional training pipeline of L2I diffusion models (ALDM). Specifically, we employ a segmentation-based discriminator which provides explicit feedback to the diffusion generator on the pixel-level alignment between the denoised image and the input layout. To encourage consistent adherence to the input layout over the sampling steps, we further introduce the multistep unrolling strategy. Instead of looking at a single timestep, we unroll a few steps recursively to imitate the inference process, and ask the discriminator to assess the alignment of denoised images with the layout over a certain time window. 
&lt;/p&gt;</description></item><item><title>MMToM-QA&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#24515;&#26234;&#29702;&#35770;&#38382;&#31572;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#26426;&#22120;&#23545;&#20110;&#20154;&#30340;&#24515;&#26234;&#29702;&#35770;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;BIP-ALM&#29992;&#20110;&#23454;&#29616;&#22810;&#27169;&#24577;&#24515;&#26234;&#29702;&#35770;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.08743</link><description>&lt;p&gt;
MMToM-QA: &#22810;&#27169;&#24577;&#24515;&#26234;&#29702;&#35770;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
MMToM-QA: Multimodal Theory of Mind Question Answering. (arXiv:2401.08743v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08743
&lt;/p&gt;
&lt;p&gt;
MMToM-QA&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#24515;&#26234;&#29702;&#35770;&#38382;&#31572;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#26426;&#22120;&#23545;&#20110;&#20154;&#30340;&#24515;&#26234;&#29702;&#35770;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;BIP-ALM&#29992;&#20110;&#23454;&#29616;&#22810;&#27169;&#24577;&#24515;&#26234;&#29702;&#35770;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#20154;&#20204;&#30340;&#24515;&#26234;&#26159;&#24320;&#21457;&#20855;&#26377;&#20154;&#31867;&#27700;&#24179;&#31038;&#20132;&#26234;&#33021;&#30340;&#26426;&#22120;&#30340;&#19968;&#20010;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#26368;&#36817;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20284;&#20046;&#23637;&#29616;&#20986;&#26576;&#20123;&#24515;&#26234;&#29702;&#35299;&#30340;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24515;&#26234;&#29702;&#35770;&#22522;&#20934;&#20351;&#29992;&#21333;&#27169;&#24577;&#25968;&#25454;&#38598;-&#25110;&#32773;&#35270;&#39057;&#25110;&#32773;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#30340;&#24515;&#26234;&#29702;&#35770;&#19981;&#20165;&#20165;&#26159;&#35270;&#39057;&#25110;&#25991;&#26412;&#29702;&#35299;&#12290;&#20154;&#20204;&#21487;&#20197;&#26681;&#25454;&#20174;&#20219;&#20309;&#21487;&#29992;&#25968;&#25454;&#20013;&#25552;&#21462;&#30340;&#27010;&#24565;&#34920;&#31034;&#65288;&#20363;&#22914;&#30446;&#26631;&#65292;&#20449;&#24565;&#65292;&#35745;&#21010;&#65289;&#28789;&#27963;&#22320;&#25512;&#29702;&#21478;&#19968;&#20010;&#20154;&#30340;&#24515;&#26234;&#65292;&#36825;&#20123;&#25968;&#25454;&#21487;&#20197;&#21253;&#25324;&#35270;&#35273;&#32447;&#32034;&#65292;&#35821;&#35328;&#21465;&#20107;&#25110;&#20004;&#32773;&#20860;&#26377;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#30340;&#24515;&#26234;&#29702;&#35770;&#38382;&#31572;&#65288;MMToM-QA&#65289;&#22522;&#20934;&#12290;MMToM-QA&#22312;&#22810;&#27169;&#24577;&#25968;&#25454;&#21644;&#20851;&#20110;&#19968;&#20010;&#20154;&#22312;&#23478;&#24237;&#29615;&#22659;&#20013;&#30340;&#27963;&#21160;&#30340;&#19981;&#21516;&#31181;&#31867;&#30340;&#21333;&#27169;&#24577;&#25968;&#25454;&#19978;&#20840;&#38754;&#35780;&#20272;&#26426;&#22120;&#30340;&#24515;&#26234;&#29702;&#35770;&#12290;&#20026;&#20102;&#23454;&#29616;&#22810;&#27169;&#24577;&#24515;&#26234;&#29702;&#35770;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;BIP-ALM&#65288;&#36125;&#21494;&#26031;&#36870;&#21521;&#35268;&#21010;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Theory of Mind (ToM), the ability to understand people's minds, is an essential ingredient for developing machines with human-level social intelligence. Recent machine learning models, particularly large language models, seem to show some aspects of ToM understanding. However, existing ToM benchmarks use unimodal datasets - either video or text. Human ToM, on the other hand, is more than video or text understanding. People can flexibly reason about another person's mind based on conceptual representations (e.g., goals, beliefs, plans) extracted from any available data, which can include visual cues, linguistic narratives, or both. To address this, we introduce a multimodal Theory of Mind question answering (MMToM-QA) benchmark. MMToM-QA comprehensively evaluates machine ToM both on multimodal data and on different kinds of unimodal data about a person's activity in a household environment. To engineer multimodal ToM capacity, we propose a novel method, BIP-ALM (Bayesian Inverse Plannin
&lt;/p&gt;</description></item><item><title>&#22266;&#23450;&#28857;&#25193;&#25955;&#27169;&#22411;&#65288;FPDM&#65289;&#26159;&#19968;&#31181;&#23558;&#22266;&#23450;&#28857;&#27714;&#35299;&#24341;&#20837;&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#26694;&#26550;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23884;&#20837;&#22266;&#23450;&#28857;&#27714;&#35299;&#23618;&#21644;&#37319;&#29992;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#26174;&#33879;&#20943;&#23567;&#20102;&#27169;&#22411;&#22823;&#23567;&#12289;&#20869;&#23384;&#20351;&#29992;&#37327;&#24182;&#21152;&#24555;&#20102;&#35757;&#32451;&#36895;&#24230;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#25216;&#26415;&#26469;&#25552;&#39640;&#37319;&#26679;&#25928;&#29575;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#29616;&#26377;&#27169;&#22411;&#30456;&#27604;&#65292;FPDM&#22312;&#24615;&#33021;&#21644;&#25928;&#29575;&#19978;&#26377;&#26126;&#26174;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2401.08741</link><description>&lt;p&gt;
&#22266;&#23450;&#28857;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Fixed Point Diffusion Models. (arXiv:2401.08741v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08741
&lt;/p&gt;
&lt;p&gt;
&#22266;&#23450;&#28857;&#25193;&#25955;&#27169;&#22411;&#65288;FPDM&#65289;&#26159;&#19968;&#31181;&#23558;&#22266;&#23450;&#28857;&#27714;&#35299;&#24341;&#20837;&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#26694;&#26550;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23884;&#20837;&#22266;&#23450;&#28857;&#27714;&#35299;&#23618;&#21644;&#37319;&#29992;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#26174;&#33879;&#20943;&#23567;&#20102;&#27169;&#22411;&#22823;&#23567;&#12289;&#20869;&#23384;&#20351;&#29992;&#37327;&#24182;&#21152;&#24555;&#20102;&#35757;&#32451;&#36895;&#24230;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#25216;&#26415;&#26469;&#25552;&#39640;&#37319;&#26679;&#25928;&#29575;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#29616;&#26377;&#27169;&#22411;&#30456;&#27604;&#65292;FPDM&#22312;&#24615;&#33021;&#21644;&#25928;&#29575;&#19978;&#26377;&#26126;&#26174;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#22266;&#23450;&#28857;&#25193;&#25955;&#27169;&#22411;&#65288;FPDM&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#23558;&#22266;&#23450;&#28857;&#27714;&#35299;&#30340;&#27010;&#24565;&#34701;&#20837;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#26694;&#26550;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#19968;&#20010;&#38544;&#24335;&#30340;&#22266;&#23450;&#28857;&#27714;&#35299;&#23618;&#23884;&#20837;&#21040;&#25193;&#25955;&#27169;&#22411;&#30340;&#21435;&#22122;&#32593;&#32476;&#20013;&#65292;&#23558;&#25193;&#25955;&#36807;&#31243;&#36716;&#21270;&#20026;&#19968;&#31995;&#21015;&#30456;&#20851;&#30340;&#22266;&#23450;&#28857;&#38382;&#39064;&#12290;&#32467;&#21512;&#19968;&#31181;&#26032;&#30340;&#38543;&#26426;&#35757;&#32451;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#26174;&#33879;&#20943;&#23569;&#20102;&#27169;&#22411;&#22823;&#23567;&#65292;&#20943;&#23567;&#20102;&#20869;&#23384;&#20351;&#29992;&#37327;&#65292;&#24182;&#21152;&#24555;&#20102;&#35757;&#32451;&#36895;&#24230;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#33021;&#22815;&#24320;&#21457;&#20986;&#20004;&#31181;&#26032;&#30340;&#25216;&#26415;&#26469;&#25552;&#39640;&#37319;&#26679;&#25928;&#29575;&#65306;&#22312;&#26102;&#38388;&#27493;&#20043;&#38388;&#37325;&#26032;&#20998;&#37197;&#35745;&#31639;&#21644;&#37325;&#29992;&#22266;&#23450;&#28857;&#35299;&#12290;&#25105;&#20204;&#23545;ImageNet&#12289;FFHQ&#12289;CelebA-HQ&#21644;LSUN-Church&#31561;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#24615;&#33021;&#21644;&#25928;&#29575;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;&#19982;&#26368;&#20808;&#36827;&#30340;DiT&#27169;&#22411;&#30456;&#27604;&#65292;FPDM&#21442;&#25968;&#20943;&#23569;&#20102;87%&#65292;&#20869;&#23384;&#28040;&#32791;&#20943;&#23569;&#20102;60%&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the Fixed Point Diffusion Model (FPDM), a novel approach to image generation that integrates the concept of fixed point solving into the framework of diffusion-based generative modeling. Our approach embeds an implicit fixed point solving layer into the denoising network of a diffusion model, transforming the diffusion process into a sequence of closely-related fixed point problems. Combined with a new stochastic training method, this approach significantly reduces model size, reduces memory usage, and accelerates training. Moreover, it enables the development of two new techniques to improve sampling efficiency: reallocating computation across timesteps and reusing fixed point solutions between timesteps. We conduct extensive experiments with state-of-the-art models on ImageNet, FFHQ, CelebA-HQ, and LSUN-Church, demonstrating substantial improvements in performance and efficiency. Compared to the state-of-the-art DiT model, FPDM contains 87% fewer parameters, consumes 60%
&lt;/p&gt;</description></item><item><title>EgoGen&#26159;&#19968;&#31181;&#26032;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#22120;&#65292;&#23427;&#21487;&#20197;&#20026;&#20855;&#20307;&#21270;&#33258;&#25105;&#30340;&#24863;&#30693;&#20219;&#21153;&#29983;&#25104;&#20934;&#30830;&#21644;&#20016;&#23500;&#30340;&#22320;&#38754;&#30495;&#23454;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#21033;&#29992;&#33258;&#25105;&#35270;&#35273;&#36755;&#20837;&#26469;&#24863;&#30693;3D&#29615;&#22659;&#12290;</title><link>http://arxiv.org/abs/2401.08739</link><description>&lt;p&gt;
EgoGen:&#19968;&#31181;&#33258;&#25105;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
EgoGen: An Egocentric Synthetic Data Generator. (arXiv:2401.08739v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08739
&lt;/p&gt;
&lt;p&gt;
EgoGen&#26159;&#19968;&#31181;&#26032;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#22120;&#65292;&#23427;&#21487;&#20197;&#20026;&#20855;&#20307;&#21270;&#33258;&#25105;&#30340;&#24863;&#30693;&#20219;&#21153;&#29983;&#25104;&#20934;&#30830;&#21644;&#20016;&#23500;&#30340;&#22320;&#38754;&#30495;&#23454;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#21033;&#29992;&#33258;&#25105;&#35270;&#35273;&#36755;&#20837;&#26469;&#24863;&#30693;3D&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22686;&#24378;&#29616;&#23454;&#65288;AR&#65289;&#20013;&#65292;&#20197;&#31532;&#19968;&#20154;&#31216;&#35270;&#35282;&#29702;&#35299;&#19990;&#30028;&#26159;&#22522;&#30784;&#24615;&#30340;&#12290;&#36825;&#31181;&#36523;&#20020;&#20854;&#22659;&#30340;&#35270;&#35282;&#30456;&#23545;&#20110;&#31532;&#19977;&#20154;&#31216;&#35270;&#35282;&#24102;&#26469;&#20102;&#25103;&#21095;&#24615;&#30340;&#35270;&#35273;&#21464;&#21270;&#21644;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#21512;&#25104;&#25968;&#25454;&#24050;&#32463;&#36171;&#20104;&#20102;&#31532;&#19977;&#20154;&#31216;&#35270;&#35282;&#35270;&#35273;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#20294;&#20854;&#24212;&#29992;&#20110;&#20855;&#20307;&#21270;&#33258;&#25105;&#30340;&#24863;&#30693;&#20219;&#21153;&#20173;&#28982;&#24456;&#23569;&#34987;&#25506;&#32034;&#12290;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#22312;&#20110;&#27169;&#25311;&#33258;&#28982;&#30340;&#20154;&#31867;&#36816;&#21160;&#21644;&#34892;&#20026;&#65292;&#20197;&#26377;&#25928;&#22320;&#25805;&#32437;&#23454;&#20307;&#30456;&#26426;&#25429;&#25417;&#21040;3D&#19990;&#30028;&#30340;&#30495;&#23454;&#33258;&#25105;&#30340;&#34920;&#36798;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#22120;EgoGen&#65292;&#23427;&#21487;&#20197;&#20026;&#20855;&#20307;&#21270;&#33258;&#25105;&#30340;&#24863;&#30693;&#20219;&#21153;&#29983;&#25104;&#20934;&#30830;&#21644;&#20016;&#23500;&#30340;&#22320;&#38754;&#30495;&#23454;&#35757;&#32451;&#25968;&#25454;&#12290;EgoGen&#30340;&#26680;&#24515;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#20154;&#20307;&#36816;&#21160;&#21512;&#25104;&#27169;&#22411;&#65292;&#23427;&#30452;&#25509;&#21033;&#29992;&#34394;&#25311;&#20154;&#31867;&#30340;&#33258;&#25105;&#35270;&#35273;&#36755;&#20837;&#24863;&#30693;3D&#29615;&#22659;&#12290;&#32467;&#21512;&#36991;&#20813;&#30896;&#25758;&#30340;&#36816;&#21160;&#22522;&#20803;&#21644;&#20004;&#38454;&#27573;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#36816;&#21160;&#21512;&#25104;&#27169;&#22411;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#33258;&#25105;&#30340;&#21512;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the world in first-person view is fundamental in Augmented Reality (AR). This immersive perspective brings dramatic visual changes and unique challenges compared to third-person views. Synthetic data has empowered third-person-view vision models, but its application to embodied egocentric perception tasks remains largely unexplored. A critical challenge lies in simulating natural human movements and behaviors that effectively steer the embodied cameras to capture a faithful egocentric representation of the 3D world. To address this challenge, we introduce EgoGen, a new synthetic data generator that can produce accurate and rich ground-truth training data for egocentric perception tasks. At the heart of EgoGen is a novel human motion synthesis model that directly leverages egocentric visual inputs of a virtual human to sense the 3D environment. Combined with collision-avoiding motion primitives and a two-stage reinforcement learning approach, our motion synthesis model off
&lt;/p&gt;</description></item><item><title>AgentMixer&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#20801;&#35768;&#26234;&#33021;&#20307;&#36890;&#36807;&#31574;&#30053;&#20462;&#25913;&#26469;&#23454;&#29616;&#21327;&#21516;&#20915;&#31574;&#12290;&#36890;&#36807;&#26500;&#36896;&#32852;&#21512;&#31574;&#30053;&#20026;&#21508;&#20010;&#37096;&#20998;&#31574;&#30053;&#30340;&#38750;&#32447;&#24615;&#32452;&#21512;&#65292;&#21487;&#23454;&#29616;&#37096;&#20998;&#21487;&#35266;&#27979;&#26234;&#33021;&#20307;&#30340;&#31283;&#23450;&#35757;&#32451;&#21644;&#20998;&#25955;&#25191;&#34892;&#12290;</title><link>http://arxiv.org/abs/2401.08728</link><description>&lt;p&gt;
AgentMixer: &#22810;&#26234;&#33021;&#20307;&#30456;&#20851;&#31574;&#30053;&#22240;&#23376;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
AgentMixer: Multi-Agent Correlated Policy Factorization. (arXiv:2401.08728v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08728
&lt;/p&gt;
&lt;p&gt;
AgentMixer&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#20801;&#35768;&#26234;&#33021;&#20307;&#36890;&#36807;&#31574;&#30053;&#20462;&#25913;&#26469;&#23454;&#29616;&#21327;&#21516;&#20915;&#31574;&#12290;&#36890;&#36807;&#26500;&#36896;&#32852;&#21512;&#31574;&#30053;&#20026;&#21508;&#20010;&#37096;&#20998;&#31574;&#30053;&#30340;&#38750;&#32447;&#24615;&#32452;&#21512;&#65292;&#21487;&#23454;&#29616;&#37096;&#20998;&#21487;&#35266;&#27979;&#26234;&#33021;&#20307;&#30340;&#31283;&#23450;&#35757;&#32451;&#21644;&#20998;&#25955;&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38598;&#20013;&#24335;&#35757;&#32451;&#19982;&#20998;&#25955;&#24335;&#25191;&#34892;&#65288;CTDE&#65289;&#24191;&#27867;&#24212;&#29992;&#20110;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21033;&#29992;&#38598;&#20013;&#24335;&#20540;&#20989;&#25968;&#26469;&#31283;&#23450;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#20551;&#35774;&#26234;&#33021;&#20307;&#22522;&#20110;&#26412;&#22320;&#35266;&#27979;&#29420;&#31435;&#22320;&#20570;&#20915;&#31574;&#65292;&#36825;&#21487;&#33021;&#19981;&#20250;&#23548;&#33268;&#20855;&#26377;&#36275;&#22815;&#21327;&#35843;&#24615;&#30340;&#30456;&#20851;&#32852;&#30340;&#32852;&#21512;&#31574;&#30053;&#12290;&#21463;&#30456;&#20851;&#22343;&#34913;&#27010;&#24565;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#24341;&#20837;"&#31574;&#30053;&#20462;&#25913;"&#26469;&#20026;&#26234;&#33021;&#20307;&#25552;&#20379;&#21327;&#35843;&#31574;&#30053;&#30340;&#26426;&#21046;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;AgentMixer&#65292;&#23558;&#32852;&#21512;&#23436;&#20840;&#21487;&#35266;&#27979;&#31574;&#30053;&#26500;&#36896;&#20026;&#21508;&#20010;&#37096;&#20998;&#21487;&#35266;&#27979;&#31574;&#30053;&#30340;&#38750;&#32447;&#24615;&#32452;&#21512;&#12290;&#20026;&#20102;&#23454;&#29616;&#20998;&#25955;&#24335;&#25191;&#34892;&#65292;&#21487;&#20197;&#36890;&#36807;&#27169;&#20223;&#32852;&#21512;&#31574;&#30053;&#26469;&#24471;&#21040;&#21508;&#20010;&#37096;&#20998;&#31574;&#30053;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#31181;&#27169;&#20223;&#23398;&#20064;&#21487;&#33021;&#20250;&#23548;&#33268;&#30001;&#20110;&#32852;&#21512;&#31574;&#30053;&#21644;&#20010;&#20307;&#31574;&#30053;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#32780;&#23548;&#33268;&#30340;&#38750;&#23545;&#31216;&#23398;&#20064;&#22833;&#36133;&#12290;
&lt;/p&gt;
&lt;p&gt;
Centralized training with decentralized execution (CTDE) is widely employed to stabilize partially observable multi-agent reinforcement learning (MARL) by utilizing a centralized value function during training. However, existing methods typically assume that agents make decisions based on their local observations independently, which may not lead to a correlated joint policy with sufficient coordination. Inspired by the concept of correlated equilibrium, we propose to introduce a \textit{strategy modification} to provide a mechanism for agents to correlate their policies. Specifically, we present a novel framework, AgentMixer, which constructs the joint fully observable policy as a non-linear combination of individual partially observable policies. To enable decentralized execution, one can derive individual policies by imitating the joint policy. Unfortunately, such imitation learning can lead to \textit{asymmetric learning failure} caused by the mismatch between joint policy and indi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20132;&#36890;&#25317;&#22581;&#39044;&#27979;&#27169;&#22411;&#65292;&#20351;&#29992;&#36710;&#36742;&#36712;&#36857;&#25968;&#25454;&#20197;&#21450;&#22810;&#37051;&#25509;&#20851;&#31995;&#27880;&#24847;&#21147;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;MA2GCN&#65289;&#26469;&#39044;&#27979;&#20132;&#36890;&#25317;&#22581;&#24773;&#20917;&#65292;&#19981;&#20381;&#36182;&#20110;&#20256;&#24863;&#22120;&#25968;&#25454;&#65292;&#25552;&#21462;&#28789;&#27963;&#19988;&#20934;&#30830;&#30340;&#20132;&#36890;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2401.08727</link><description>&lt;p&gt;
MA2GCN: &#20351;&#29992;&#36712;&#36857;&#25968;&#25454;&#36827;&#34892;&#20132;&#36890;&#39044;&#27979;&#30340;&#22810;&#37051;&#25509;&#20851;&#31995;&#27880;&#24847;&#21147;&#22270;&#21367;&#31215;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
MA2GCN: Multi Adjacency relationship Attention Graph Convolutional Networks for Traffic Prediction using Trajectory data. (arXiv:2401.08727v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08727
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20132;&#36890;&#25317;&#22581;&#39044;&#27979;&#27169;&#22411;&#65292;&#20351;&#29992;&#36710;&#36742;&#36712;&#36857;&#25968;&#25454;&#20197;&#21450;&#22810;&#37051;&#25509;&#20851;&#31995;&#27880;&#24847;&#21147;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;MA2GCN&#65289;&#26469;&#39044;&#27979;&#20132;&#36890;&#25317;&#22581;&#24773;&#20917;&#65292;&#19981;&#20381;&#36182;&#20110;&#20256;&#24863;&#22120;&#25968;&#25454;&#65292;&#25552;&#21462;&#28789;&#27963;&#19988;&#20934;&#30830;&#30340;&#20132;&#36890;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#25317;&#22581;&#38382;&#39064;&#19981;&#20165;&#23548;&#33268;&#24040;&#22823;&#30340;&#32463;&#27982;&#25439;&#22833;&#65292;&#32780;&#19988;&#20005;&#37325;&#21361;&#23475;&#22478;&#24066;&#29615;&#22659;&#12290;&#39044;&#27979;&#20132;&#36890;&#25317;&#22581;&#20855;&#26377;&#37325;&#35201;&#30340;&#23454;&#38469;&#24847;&#20041;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#37117;&#26159;&#22522;&#20110;&#19981;&#21516;&#36335;&#27573;&#19978;&#30340;&#20256;&#24863;&#22120;&#30340;&#21382;&#21490;&#25968;&#25454;&#26469;&#39044;&#27979;&#26410;&#26469;&#30340;&#20132;&#36890;&#27969;&#37327;&#21644;&#36895;&#24230;&#65292;&#20998;&#26512;&#26576;&#20010;&#36947;&#36335;&#27573;&#30340;&#20132;&#36890;&#25317;&#22581;&#24773;&#20917;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20256;&#24863;&#22120;&#30340;&#22266;&#23450;&#20301;&#32622;&#65292;&#24456;&#38590;&#25366;&#25496;&#26032;&#30340;&#20449;&#24687;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#36710;&#36742;&#36712;&#36857;&#25968;&#25454;&#26356;&#21152;&#28789;&#27963;&#65292;&#21487;&#20197;&#26681;&#25454;&#38656;&#35201;&#25552;&#21462;&#20132;&#36890;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20132;&#36890;&#25317;&#22581;&#39044;&#27979;&#27169;&#22411;&#8212;&#8212;&#22810;&#37051;&#25509;&#20851;&#31995;&#27880;&#24847;&#21147;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;MA2GCN&#65289;&#12290;&#35813;&#27169;&#22411;&#23558;&#36710;&#36742;&#36712;&#36857;&#25968;&#25454;&#36716;&#21270;&#20026;&#32593;&#26684;&#24418;&#24335;&#30340;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#65292;&#24182;&#22522;&#20110;&#19981;&#21516;&#32593;&#26684;&#20043;&#38388;&#30340;&#27969;&#21160;&#24615;&#25552;&#20986;&#20102;&#36710;&#36742;&#36827;&#20986;&#30697;&#38453;&#12290;&#21516;&#26102;&#65292;&#20026;&#20102;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;
&lt;/p&gt;
&lt;p&gt;
The problem of traffic congestion not only causes a large amount of economic losses, but also seriously endangers the urban environment. Predicting traffic congestion has important practical significance. So far, most studies have been based on historical data from sensors placed on different roads to predict future traffic flow and speed, to analyze the traffic congestion conditions of a certain road segment. However, due to the fixed position of sensors, it is difficult to mine new information. On the other hand, vehicle trajectory data is more flexible and can extract traffic information as needed. Therefore, we proposed a new traffic congestion prediction model - Multi Adjacency relationship Attention Graph Convolutional Networks(MA2GCN). This model transformed vehicle trajectory data into graph structured data in grid form, and proposed a vehicle entry and exit matrix based on the mobility between different grids. At the same time, in order to improve the performance of the model,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#22522;&#20110;Kinect&#30340;&#36828;&#31243;&#24247;&#22797;&#31995;&#32479;&#65292;&#23427;&#20855;&#26377;&#30693;&#35782;&#25552;&#21462;&#21151;&#33021;&#21644;&#36828;&#31243;&#27785;&#28024;&#21151;&#33021;&#65292;&#21487;&#23454;&#29616;&#23545;&#24739;&#32773;&#30340;&#24247;&#22797;&#35757;&#32451;&#30340;&#36873;&#25321;&#12289;&#35780;&#20272;&#21644;&#36828;&#31243;&#31649;&#29702;&#12290;</title><link>http://arxiv.org/abs/2401.08721</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#36873;&#25321;&#12289;&#35780;&#20272;&#21644;&#36828;&#31243;&#31649;&#29702;&#27835;&#30103;&#30340;&#36828;&#31243;&#24247;&#22797;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
A Telerehabilitation System for the Selection, Evaluation and Remote Management of Therapies. (arXiv:2401.08721v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08721
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#22522;&#20110;Kinect&#30340;&#36828;&#31243;&#24247;&#22797;&#31995;&#32479;&#65292;&#23427;&#20855;&#26377;&#30693;&#35782;&#25552;&#21462;&#21151;&#33021;&#21644;&#36828;&#31243;&#27785;&#28024;&#21151;&#33021;&#65292;&#21487;&#23454;&#29616;&#23545;&#24739;&#32773;&#30340;&#24247;&#22797;&#35757;&#32451;&#30340;&#36873;&#25321;&#12289;&#35780;&#20272;&#21644;&#36828;&#31243;&#31649;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25903;&#25345;&#29289;&#29702;&#30103;&#27861;&#22312;&#20219;&#20309;&#22320;&#26041;&#36827;&#34892;&#30340;&#36828;&#31243;&#24247;&#22797;&#31995;&#32479;&#21487;&#20197;&#24110;&#21161;&#33410;&#30465;&#21307;&#30103;&#36153;&#29992;&#65292;&#21516;&#26102;&#25913;&#21892;&#38656;&#35201;&#24247;&#22797;&#30340;&#29992;&#25143;&#30340;&#29983;&#27963;&#36136;&#37327;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#20316;&#20026;&#19968;&#20010;&#25972;&#20307;&#21576;&#29616;&#21019;&#26032;&#30340;&#22522;&#20110;Kinect&#30340;&#36828;&#31243;&#24247;&#22797;&#31995;&#32479;&#65288;KiReS&#65289;&#25152;&#25903;&#25345;&#30340;&#25152;&#26377;&#21151;&#33021;&#12290;&#38500;&#20102;&#24403;&#21069;&#31995;&#32479;&#25552;&#20379;&#30340;&#21151;&#33021;&#22806;&#65292;&#23427;&#36824;&#22788;&#29702;&#20102;&#20004;&#20010;&#26032;&#21151;&#33021;&#65292;&#21487;&#20197;&#23558;&#20854;&#32435;&#20837;&#20854;&#20013;&#65292;&#20197;&#20415;&#21521;&#26032;&#19968;&#20195;&#36828;&#31243;&#24247;&#22797;&#31995;&#32479;&#36808;&#36827;&#12290;&#30693;&#35782;&#25552;&#21462;&#21151;&#33021;&#22788;&#29702;&#26377;&#20851;&#24739;&#32773;&#29289;&#29702;&#30103;&#27861;&#35760;&#24405;&#21644;&#22312;&#21517;&#20026;TRHONT&#30340;&#26412;&#20307;&#20013;&#25551;&#36848;&#30340;&#27835;&#30103;&#26041;&#26696;&#30340;&#30693;&#35782;&#65292;&#20197;&#36873;&#25321;&#36866;&#24403;&#30340;&#24247;&#22797;&#35757;&#32451;&#12290;&#36828;&#31243;&#27785;&#28024;&#21151;&#33021;&#36890;&#36807;&#21452;&#21521;&#23454;&#26102;&#22810;&#23186;&#20307;&#36890;&#20449;&#65292;&#22312;&#36827;&#34892;&#36828;&#31243;&#24247;&#22797;&#26102;&#25552;&#20379;&#26041;&#20415;&#12289;&#26377;&#25928;&#21644;&#29992;&#25143;&#21451;&#22909;&#30340;&#20307;&#39564;&#12290;&#26412;&#26412;&#20307;&#21253;&#21547;&#20102;&#22240;&#24247;&#22797;&#31995;&#32479;&#32780;&#35201;&#27714;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Telerehabilitation systems that support physical therapy sessions anywhere can help save healthcare costs while also improving the quality of life of the users that need rehabilitation. The main contribution of this paper is to present, as a whole, all the features supported by the innovative Kinect-based Telerehabilitation System (KiReS). In addition to the functionalities provided by current systems, it handles two new ones that could be incorporated into them, in order to give a step forward towards a new generation of telerehabilitation systems. The knowledge extraction functionality handles knowledge about the physical therapy record of patients and treatment protocols described in an ontology, named TRHONT, to select the adequate exercises for the rehabilitation of patients. The teleimmersion functionality provides a convenient, effective and user-friendly experience when performing the telerehabilitation, through a two-way real-time multimedia communication. The ontology contain
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#26041;&#27861;&#26469;&#23547;&#25214;&#36866;&#24403;&#30340;&#28304;&#25968;&#25454;&#23376;&#38598;&#65292;&#29992;&#20110;&#37329;&#23646;&#22686;&#26448;&#21046;&#36896;&#20013;&#30340;&#36801;&#31227;&#23398;&#20064;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#28304;&#25968;&#25454;&#21644;&#30446;&#26631;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#26469;&#36873;&#25321;&#23376;&#38598;&#65292;&#24182;&#36890;&#36807;&#20004;&#20010;&#30456;&#20284;&#24230;&#36317;&#31163;&#24230;&#37327;&#23450;&#20041;&#30340;Pareto frontier&#36827;&#34892;&#36845;&#20195;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2401.08715</link><description>&lt;p&gt;
&#29992;&#20110;&#37329;&#23646;&#22686;&#26448;&#21046;&#36896;&#30340;&#36801;&#31227;&#23398;&#20064;&#20013;&#30340;&#28304;&#25968;&#25454;&#23376;&#38598;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Selecting Subsets of Source Data for Transfer Learning with Applications in Metal Additive Manufacturing. (arXiv:2401.08715v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08715
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#26041;&#27861;&#26469;&#23547;&#25214;&#36866;&#24403;&#30340;&#28304;&#25968;&#25454;&#23376;&#38598;&#65292;&#29992;&#20110;&#37329;&#23646;&#22686;&#26448;&#21046;&#36896;&#20013;&#30340;&#36801;&#31227;&#23398;&#20064;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#28304;&#25968;&#25454;&#21644;&#30446;&#26631;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#26469;&#36873;&#25321;&#23376;&#38598;&#65292;&#24182;&#36890;&#36807;&#20004;&#20010;&#30456;&#20284;&#24230;&#36317;&#31163;&#24230;&#37327;&#23450;&#20041;&#30340;Pareto frontier&#36827;&#34892;&#36845;&#20195;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#37329;&#23646;&#22686;&#26448;&#21046;&#36896;&#20013;&#25968;&#25454;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#24050;&#32463;&#37319;&#29992;&#36801;&#31227;&#23398;&#20064;&#65288;TL&#65289;&#20174;&#28304;&#39046;&#22495;&#65288;&#20363;&#22914;&#23436;&#25104;&#30340;&#25171;&#21360;&#65289;&#20013;&#25552;&#21462;&#30693;&#35782;&#65292;&#20197;&#25913;&#21892;&#30446;&#26631;&#39046;&#22495;&#65288;&#20363;&#22914;&#26032;&#30340;&#25171;&#21360;&#65289;&#20013;&#30340;&#24314;&#27169;&#24615;&#33021;&#12290;&#24403;&#21069;&#30340;&#24212;&#29992;&#31243;&#24207;&#30452;&#25509;&#22312;&#36801;&#31227;&#23398;&#20064;&#20013;&#20351;&#29992;&#25152;&#26377;&#21487;&#35775;&#38382;&#30340;&#28304;&#25968;&#25454;&#65292;&#32780;&#19981;&#32771;&#34385;&#28304;&#25968;&#25454;&#21644;&#30446;&#26631;&#25968;&#25454;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#28304;&#25968;&#25454;&#21644;&#30446;&#26631;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#26469;&#23547;&#25214;&#36866;&#24403;&#30340;&#28304;&#25968;&#25454;&#23376;&#38598;&#65292;&#38024;&#23545;&#19968;&#32452;&#26377;&#38480;&#30340;&#30446;&#26631;&#39046;&#22495;&#25968;&#25454;&#12290;&#36825;&#31181;&#30456;&#20284;&#24615;&#36890;&#36807;&#31354;&#38388;&#21644;&#27169;&#22411;&#36317;&#31163;&#24230;&#37327;&#26469;&#21051;&#30011;&#12290;&#22522;&#20110;Pareto frontier&#30340;&#28304;&#25968;&#25454;&#36873;&#25321;&#26041;&#27861;&#34987;&#24320;&#21457;&#20986;&#26469;&#65292;&#36890;&#36807;&#20004;&#20010;&#30456;&#20284;&#24230;&#36317;&#31163;&#24230;&#37327;&#23450;&#20041;&#30340;Pareto frontier&#19978;&#30340;&#28304;&#25968;&#25454;&#34987;&#36845;&#20195;&#36873;&#25321;&#12290;&#35813;&#26041;&#27861;&#34987;&#38598;&#25104;&#21040;&#22522;&#20110;&#23454;&#20363;&#30340;TL&#26041;&#27861;&#65288;&#20915;&#31574;&#26641;&#22238;&#24402;&#27169;&#22411;&#65289;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;TL&#26041;&#27861;&#65288;&#24494;&#35843;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65289;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Considering data insufficiency in metal additive manufacturing (AM), transfer learning (TL) has been adopted to extract knowledge from source domains (e.g., completed printings) to improve the modeling performance in target domains (e.g., new printings). Current applications use all accessible source data directly in TL with no regard to the similarity between source and target data. This paper proposes a systematic method to find appropriate subsets of source data based on similarities between the source and target datasets for a given set of limited target domain data. Such similarity is characterized by the spatial and model distance metrics. A Pareto frontier-based source data selection method is developed, where the source data located on the Pareto frontier defined by two similarity distance metrics are selected iteratively. The method is integrated into an instance-based TL method (decision tree regression model) and a model-based TL method (fine-tuned artificial neural network)
&lt;/p&gt;</description></item><item><title>ISENSE&#39033;&#30446;&#20013;&#30340;&#34394;&#25311;&#29616;&#23454;&#22521;&#35757;&#31243;&#24207;&#26088;&#22312;&#36890;&#36807;&#25552;&#20379;&#25216;&#26415;&#24037;&#20855;&#22521;&#35757;&#21548;&#35273;&#27491;&#24120;&#30340;&#20154;&#23398;&#20064;&#25163;&#35821;&#65292;&#20174;&#32780;&#20419;&#36827;&#31038;&#20250;&#34701;&#21512;&#12290;</title><link>http://arxiv.org/abs/2401.08714</link><description>&lt;p&gt;
&#25163;&#35821;&#22521;&#35757;&#35745;&#21010;&#65306;ISENSE&#39033;&#30446;&#20013;&#30340;&#34394;&#25311;&#29616;&#23454;&#20419;&#36827;&#31038;&#20250;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
Training program on sign language: social inclusion through Virtual Reality in ISENSE project. (arXiv:2401.08714v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08714
&lt;/p&gt;
&lt;p&gt;
ISENSE&#39033;&#30446;&#20013;&#30340;&#34394;&#25311;&#29616;&#23454;&#22521;&#35757;&#31243;&#24207;&#26088;&#22312;&#36890;&#36807;&#25552;&#20379;&#25216;&#26415;&#24037;&#20855;&#22521;&#35757;&#21548;&#35273;&#27491;&#24120;&#30340;&#20154;&#23398;&#20064;&#25163;&#35821;&#65292;&#20174;&#32780;&#20419;&#36827;&#31038;&#20250;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25163;&#35821;&#37319;&#29992;&#20102;&#32467;&#26500;&#21270;&#30340;&#25163;&#21183;&#21644;&#35270;&#35273;&#21160;&#20316;&#65292;&#26159;&#23545;&#20110;&#21548;&#38556;&#25110;&#35328;&#35821;&#21463;&#25439;&#20010;&#20307;&#26469;&#35828;&#19968;&#31181;&#23453;&#36149;&#30340;&#26085;&#24120;&#20132;&#27969;&#25163;&#27573;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#21548;&#35273;&#27491;&#24120;&#30340;&#20154;&#26469;&#35828;&#65292;&#25484;&#25569;&#25163;&#35821;&#20173;&#28982;&#36739;&#23569;&#35265;&#65292;&#33021;&#22815;&#29702;&#35299;&#25163;&#35821;&#30340;&#20154;&#26356;&#26159;&#31232;&#32570;&#12290;&#22312;&#23398;&#26415;&#32972;&#26223;&#19979;&#65292;&#29238;&#27597;&#21644;&#25945;&#24072;&#22312;&#25903;&#25345;&#32843;&#21713;&#23398;&#29983;&#20174;&#24188;&#24180;&#26102;&#26399;&#24320;&#22987;&#23398;&#20064;&#25163;&#35821;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#36817;&#24180;&#26469;&#65292;&#22312;&#23398;&#20064;&#25163;&#35821;&#30340;&#25945;&#32946;&#24037;&#20855;&#20013;&#65292;&#34394;&#25311;&#29616;&#23454;&#65288;VR&#65289;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#22810;&#65292;&#22240;&#20026;&#24050;&#32463;&#35777;&#26126;&#23427;&#33021;&#22815;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#25552;&#39640;&#35760;&#24518;&#12289;&#20445;&#25345;&#21644;&#27880;&#24847;&#21147;&#12290;ISENSE&#39033;&#30446;&#26088;&#22312;&#20026;&#32843;&#21713;&#23398;&#29983;&#22312;&#23398;&#26415;&#29983;&#27963;&#20013;&#25552;&#20379;&#25903;&#25345;&#65292;&#36890;&#36807;&#22312;&#23398;&#26415;&#29615;&#22659;&#20013;&#21521;&#21548;&#35273;&#27491;&#24120;&#30340;&#20154;&#32676;&#25945;&#25480;&#25163;&#35821;&#30340;&#19981;&#21516;&#25216;&#26415;&#24037;&#20855;&#12290;&#20316;&#20026;ISENSE&#39033;&#30446;&#30340;&#19968;&#37096;&#20998;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#24212;&#29992;&#31243;&#24207;&#65292;&#29992;&#20110;&#25945;&#25480;&#35199;&#29677;&#29273;&#25163;&#35821;&#21644;&#24847;&#22823;&#21033;&#25163;&#35821;&#12290;
&lt;/p&gt;
&lt;p&gt;
Structured hand gestures that incorporate visual motions and signs are used in sign language. Sign language is a valuable means of daily communication for individuals who are deaf or have speech impairments, but it is still rare among hearing people, and fewer are capable of understand it. Within the academic context, parents and teachers play a crucial role in supporting deaf students from childhood by facilitating their learning of sign language. In the last years, among all the teaching tools useful for learning sign language, the use of Virtual Reality (VR) has increased, as it has been demonstrated to improve retention, memory and attention during the learning process. The ISENSE project has been created to assist students with deafness during their academic life by proposing different technological tools for teaching sign language to the hearing community in the academic context. As part of the ISENSE project, this work aims to develop an application for Spanish and Italian sign 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#35752;&#35770;AI&#38544;&#21947;&#22914;&#20309;&#24110;&#21161;&#22521;&#20859;&#23545;AI&#31995;&#32479;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29702;&#35299;&#65292;&#20197;&#21450;&#22914;&#20309;&#20419;&#36827;&#25209;&#21028;&#24615;AI&#32032;&#20859;&#30340;&#21457;&#23637;&#12290;&#36890;&#36807;&#20998;&#26512;&#38544;&#21947;&#24182;&#19982;&#20854;&#20182;&#20154;&#35752;&#35770;&#65292;&#30740;&#31350;&#22242;&#38431;&#21457;&#29616;&#38544;&#21947;&#26377;&#21161;&#20110;&#23545;AI&#30340;&#20262;&#29702;&#12289;&#20844;&#24179;&#21644;&#21487;&#35775;&#38382;&#24615;&#31561;&#38382;&#39064;&#36827;&#34892;&#24605;&#32771;&#65292;&#24182;&#35780;&#20272;&#20102;&#38544;&#21947;&#22312;&#26159;&#21542;&#20419;&#36827;&#25311;&#20154;&#21270;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.08711</link><description>&lt;p&gt;
&#21161;&#25163;&#12289;&#40550;&#40521;&#36824;&#26159;&#27542;&#27665;&#24335;&#21895;&#21485;&#65311;ChatGPT&#38544;&#21947;&#23545;&#22521;&#20859;AI&#25209;&#21028;&#32032;&#20859;&#30340;&#20316;&#29992;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Assistant, Parrot, or Colonizing Loudspeaker? ChatGPT Metaphors for Developing Critical AI Literacies. (arXiv:2401.08711v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08711
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#35752;&#35770;AI&#38544;&#21947;&#22914;&#20309;&#24110;&#21161;&#22521;&#20859;&#23545;AI&#31995;&#32479;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29702;&#35299;&#65292;&#20197;&#21450;&#22914;&#20309;&#20419;&#36827;&#25209;&#21028;&#24615;AI&#32032;&#20859;&#30340;&#21457;&#23637;&#12290;&#36890;&#36807;&#20998;&#26512;&#38544;&#21947;&#24182;&#19982;&#20854;&#20182;&#20154;&#35752;&#35770;&#65292;&#30740;&#31350;&#22242;&#38431;&#21457;&#29616;&#38544;&#21947;&#26377;&#21161;&#20110;&#23545;AI&#30340;&#20262;&#29702;&#12289;&#20844;&#24179;&#21644;&#21487;&#35775;&#38382;&#24615;&#31561;&#38382;&#39064;&#36827;&#34892;&#24605;&#32771;&#65292;&#24182;&#35780;&#20272;&#20102;&#38544;&#21947;&#22312;&#26159;&#21542;&#20419;&#36827;&#25311;&#20154;&#21270;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#35752;&#35770;AI&#38544;&#21947;&#22914;&#20309;&#24110;&#21161;&#25105;&#20204;&#35748;&#35782;&#21040;&#24433;&#21709;&#25105;&#20204;&#23545;AI&#31995;&#32479;&#21450;&#20854;&#29305;&#21035;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;ChatGPT&#30340;&#29702;&#35299;&#30340;&#26694;&#26550;&#65292;&#29305;&#21035;&#26159;&#22312;&#36843;&#20999;&#38656;&#35201;&#25945;&#25480;&#8220;&#25209;&#21028;&#24615;AI&#32032;&#20859;&#8221;&#30340;&#24773;&#20917;&#19979;&#65292;&#38544;&#21947;&#30340;&#35752;&#35770;&#25552;&#20379;&#20102;&#19968;&#20010;&#25506;&#31350;&#21644;&#23545;&#35805;&#30340;&#26426;&#20250;&#65292;&#20026;&#32454;&#24494;&#20043;&#22788;&#12289;&#36259;&#21619;&#24615;&#21644;&#25209;&#35780;&#25552;&#20379;&#20102;&#31354;&#38388;&#12290;&#36890;&#36807;&#21327;&#20316;&#30340;&#33258;&#20307;&#27665;&#26063;&#24535;&#26041;&#27861;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#26469;&#33258;&#21508;&#31181;&#26469;&#28304;&#30340;&#38544;&#21947;&#65292;&#24182;&#26681;&#25454;&#19971;&#20010;&#38382;&#39064;&#23545;&#20854;&#36827;&#34892;&#20102;&#20010;&#21035;&#21453;&#24605;&#65292;&#28982;&#21518;&#36827;&#34892;&#20102;&#30456;&#20114;&#20250;&#35758;&#21644;&#35752;&#35770;&#25105;&#20204;&#30340;&#35299;&#37322;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#25105;&#20204;&#30340;&#21453;&#24605;&#22914;&#20309;&#26377;&#21161;&#20110;Selber&#30340;&#22810;&#20803;&#32032;&#32032;&#20859;&#26694;&#26550;&#20013;&#25152;&#25551;&#36848;&#30340;&#19977;&#31181;&#32032;&#20859;&#65306;&#21151;&#33021;&#24615;&#12289;&#25209;&#21028;&#24615;&#21644;&#20462;&#36766;&#24615;&#12290;&#36825;&#20123;&#20998;&#26512;&#24110;&#21161;&#25105;&#20204;&#25506;&#31350;&#20102;&#20262;&#29702;&#12289;&#20844;&#24179;&#21644;&#21487;&#35775;&#38382;&#24615;&#19982;AI&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#27599;&#20010;&#38544;&#21947;&#26159;&#21542;&#20419;&#36827;&#25311;&#20154;&#21270;&#65292;&#20197;&#21450;&#36825;&#31181;&#38544;&#21947;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#26263;&#31034;&#20102;AI&#30340;&#26377;&#24863;&#30693;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study explores how discussing metaphors for AI can help build awareness of the frames that shape our understanding of AI systems, particularly large language models (LLMs) like ChatGPT. Given the pressing need to teach "critical AI literacy", discussion of metaphor provides an opportunity for inquiry and dialogue with space for nuance, playfulness, and critique. Using a collaborative autoethnographic methodology, we analyzed metaphors from a range of sources, and reflected on them individually according to seven questions, then met and discussed our interpretations. We then analyzed how our reflections contributed to the three kinds of literacies delineated in Selber's multiliteracies framework: functional, critical, and rhetorical. These allowed us to analyze questions of ethics, equity, and accessibility in relation to AI. We explored each metaphor along the dimension of whether or not it was promoting anthropomorphizing, and to what extent such metaphors imply that AI is sentie
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#25628;&#32034;&#25216;&#26415;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#20026;&#35745;&#31639;&#30149;&#29702;&#23398;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#31616;&#26126;&#30340;&#27010;&#36848;&#65292;&#26088;&#22312;&#23547;&#27714;&#26377;&#25928;&#12289;&#24555;&#36895;&#21644;&#39640;&#25928;&#30340;&#22270;&#20687;&#25628;&#32034;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.08699</link><description>&lt;p&gt;
&#20851;&#20110;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#25628;&#32034;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Image Search in Histopathology. (arXiv:2401.08699v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08699
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#25628;&#32034;&#25216;&#26415;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#20026;&#35745;&#31639;&#30149;&#29702;&#23398;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#31616;&#26126;&#30340;&#27010;&#36848;&#65292;&#26088;&#22312;&#23547;&#27714;&#26377;&#25928;&#12289;&#24555;&#36895;&#21644;&#39640;&#25928;&#30340;&#22270;&#20687;&#25628;&#32034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32452;&#32455;&#30149;&#29702;&#23398;&#30340;&#30149;&#29702;&#22270;&#20687;&#21487;&#20197;&#36890;&#36807;&#35013;&#26377;&#25668;&#20687;&#22836;&#30340;&#26174;&#24494;&#38236;&#25110;&#20840;&#25195;&#25551;&#20202;&#33719;&#21462;&#12290;&#21033;&#29992;&#30456;&#20284;&#24615;&#35745;&#31639;&#22522;&#20110;&#36825;&#20123;&#22270;&#20687;&#21305;&#37197;&#24739;&#32773;&#65292;&#22312;&#30740;&#31350;&#21644;&#20020;&#24202;&#29615;&#22659;&#20013;&#20855;&#26377;&#37325;&#35201;&#28508;&#21147;&#12290;&#26368;&#36817;&#25628;&#32034;&#25216;&#26415;&#30340;&#36827;&#23637;&#20351;&#24471;&#21487;&#20197;&#23545;&#21508;&#31181;&#32452;&#32455;&#31867;&#22411;&#30340;&#32454;&#32990;&#32467;&#26500;&#36827;&#34892;&#24494;&#22937;&#30340;&#37327;&#21270;&#65292;&#20419;&#36827;&#27604;&#36739;&#65292;&#24182;&#22312;&#19982;&#35786;&#26029;&#21644;&#27835;&#30103;&#36807;&#30340;&#30149;&#20363;&#25968;&#25454;&#24211;&#36827;&#34892;&#27604;&#36739;&#26102;&#23454;&#29616;&#20851;&#20110;&#35786;&#26029;&#12289;&#39044;&#21518;&#21644;&#26032;&#24739;&#32773;&#39044;&#27979;&#30340;&#25512;&#26029;&#12290;&#26412;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#25628;&#32034;&#25216;&#26415;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#20026;&#35745;&#31639;&#30149;&#29702;&#23398;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#31616;&#26126;&#30340;&#27010;&#36848;&#65292;&#20197;&#23547;&#27714;&#26377;&#25928;&#12289;&#24555;&#36895;&#21644;&#39640;&#25928;&#30340;&#22270;&#20687;&#25628;&#32034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pathology images of histopathology can be acquired from camera-mounted microscopes or whole slide scanners. Utilizing similarity calculations to match patients based on these images holds significant potential in research and clinical contexts. Recent advancements in search technologies allow for nuanced quantification of cellular structures across diverse tissue types, facilitating comparisons and enabling inferences about diagnosis, prognosis, and predictions for new patients when compared against a curated database of diagnosed and treated cases. In this paper, we comprehensively review the latest developments in image search technologies for histopathology, offering a concise overview tailored for computational pathology researchers seeking effective, fast and efficient image search methods in their work.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#39640;&#23618;&#27425;&#32508;&#21512;&#20013;&#20351;&#29992;GNN&#36827;&#34892;&#28304;-&#21518;&#21521;&#36335;&#36136;&#37327;&#39044;&#27979;&#30340;&#20998;&#23618;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24314;&#27169;&#27969;&#31243;&#12289;&#22270;&#26500;&#24314;&#26041;&#27861;&#21644;&#20998;&#23618;GNN&#35757;&#32451;&#21644;&#39044;&#27979;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#39044;&#27979;QoR&#25351;&#26631;&#65292;&#24182;&#22312;&#35774;&#35745;&#31354;&#38388;&#25506;&#32034;&#20013;&#20943;&#23569;&#20102;&#36816;&#34892;&#26102;&#12290;</title><link>http://arxiv.org/abs/2401.08696</link><description>&lt;p&gt;
&#22312;&#39640;&#23618;&#27425;&#32508;&#21512;&#20013;&#20351;&#29992;GNN&#36827;&#34892;&#28304;-&#21518;&#21521;&#36335;&#36136;&#37327;&#39044;&#27979;&#30340;&#20998;&#23618;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Source-to-Post-Route QoR Prediction in High-Level Synthesis with GNNs. (arXiv:2401.08696v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08696
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#39640;&#23618;&#27425;&#32508;&#21512;&#20013;&#20351;&#29992;GNN&#36827;&#34892;&#28304;-&#21518;&#21521;&#36335;&#36136;&#37327;&#39044;&#27979;&#30340;&#20998;&#23618;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24314;&#27169;&#27969;&#31243;&#12289;&#22270;&#26500;&#24314;&#26041;&#27861;&#21644;&#20998;&#23618;GNN&#35757;&#32451;&#21644;&#39044;&#27979;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#39044;&#27979;QoR&#25351;&#26631;&#65292;&#24182;&#22312;&#35774;&#35745;&#31354;&#38388;&#25506;&#32034;&#20013;&#20943;&#23569;&#20102;&#36816;&#34892;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#23618;&#27425;&#32508;&#21512;&#65288;HLS&#65289;&#36890;&#36807;&#36991;&#20813;RTL&#32534;&#31243;&#26174;&#30528;&#21152;&#24555;&#20102;&#30828;&#20214;&#35774;&#35745;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#32771;&#34385;&#21518;&#21521;&#36335;&#32467;&#26524;&#36136;&#37327;&#65288;QoR&#65289;&#26102;&#65292;HLS&#30340;&#21608;&#36716;&#26102;&#38388;&#26174;&#33879;&#22686;&#21152;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;FPGA HLS&#30340;&#20998;&#23618;&#21518;&#21521;&#36335;QoR&#39044;&#27979;&#26041;&#27861;&#65292;&#20855;&#26377;&#20197;&#19979;&#29305;&#28857;&#65306;&#65288;1&#65289;&#30452;&#25509;&#20174;C / C ++&#31243;&#24207;&#20272;&#35745;&#24310;&#36831;&#21644;&#21518;&#21521;&#36335;&#36164;&#28304;&#20351;&#29992;&#30340;&#24314;&#27169;&#27969;&#31243;&#65307;&#65288;2&#65289;&#26377;&#25928;&#34920;&#31034;&#28304;&#20195;&#30721;&#30340;&#25511;&#21046;&#21644;&#25968;&#25454;&#27969;&#22270;&#20197;&#21450;HLS&#20266;&#25351;&#20196;&#30340;&#24433;&#21709;&#30340;&#22270;&#26500;&#24314;&#26041;&#27861;&#65307;&#65288;3&#65289;&#33021;&#22815;&#25429;&#33719;&#24490;&#29615;&#23618;&#27425;&#24433;&#21709;&#30340;&#20998;&#23618;GNN&#35757;&#32451;&#21644;&#39044;&#27979;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;GNN&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;QoR&#25351;&#26631;&#19979;&#30340;&#39044;&#27979;&#35823;&#24046;&#23567;&#20110;10%&#65292;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25913;&#36827;&#12290;&#36890;&#36807;&#37319;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#35770;&#65292;&#22312;HLS&#30340;&#35774;&#35745;&#31354;&#38388;&#25506;&#32034;&#20013;&#20943;&#23569;&#20102;&#36816;&#34892;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-level synthesis (HLS) notably speeds up the hardware design process by avoiding RTL programming. However, the turnaround time of HLS increases significantly when post-route quality of results (QoR) are considered during optimization. To tackle this issue, we propose a hierarchical post-route QoR prediction approach for FPGA HLS, which features: (1) a modeling flow that directly estimates latency and post-route resource usage from C/C++ programs; (2) a graph construction method that effectively represents the control and data flow graph of source code and effects of HLS pragmas; and (3) a hierarchical GNN training and prediction method capable of capturing the impact of loop hierarchies. Experimental results show that our method presents a prediction error of less than 10% for different types of QoR metrics, which gains tremendous improvement compared with the state-of-the-art GNN methods. By adopting our proposed methodology, the runtime for design space exploration in HLS is shor
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25972;&#21512;&#19987;&#23478;&#30693;&#35782;&#21644;&#21487;&#35299;&#37322;&#30340;&#25968;&#25454;&#39537;&#21160;&#26234;&#33021;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#24341;&#23548;&#35786;&#26029;&#27169;&#22411;&#65288;KGDM&#65289;&#65292;&#29992;&#20110;&#21327;&#21516;&#20020;&#24202;&#35786;&#26029;&#20256;&#26579;&#24615;&#35282;&#33180;&#28814;&#65288;IK&#65289;&#12290;KGDM&#36890;&#36807;&#19982;&#20020;&#24202;&#21307;&#29983;&#30340;&#20114;&#21160;&#65292;&#25552;&#20379;&#21487;&#35270;&#21270;&#25512;&#29702;&#36807;&#31243;&#65292;&#21253;&#25324;AI&#22522;&#20110;&#29983;&#29289;&#26631;&#35760;&#21644;&#20855;&#26377;&#30456;&#21516;&#35786;&#26029;&#27169;&#24335;&#30340;&#30149;&#20363;&#12290;&#36825;&#19968;&#26041;&#27861;&#26377;&#26395;&#25552;&#39640;&#23433;&#20840;&#24615;&#21644;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.08695</link><description>&lt;p&gt;
&#36890;&#36807;&#25972;&#21512;&#19987;&#23478;&#30693;&#35782;&#21644;&#21487;&#35299;&#37322;&#30340;&#25968;&#25454;&#39537;&#21160;&#26234;&#33021;&#23454;&#29616;&#20256;&#26579;&#24615;&#35282;&#33180;&#28814;&#30340;&#21327;&#21516;&#20020;&#24202;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Enabling Collaborative Clinical Diagnosis of Infectious Keratitis by Integrating Expert Knowledge and Interpretable Data-driven Intelligence. (arXiv:2401.08695v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08695
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25972;&#21512;&#19987;&#23478;&#30693;&#35782;&#21644;&#21487;&#35299;&#37322;&#30340;&#25968;&#25454;&#39537;&#21160;&#26234;&#33021;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#24341;&#23548;&#35786;&#26029;&#27169;&#22411;&#65288;KGDM&#65289;&#65292;&#29992;&#20110;&#21327;&#21516;&#20020;&#24202;&#35786;&#26029;&#20256;&#26579;&#24615;&#35282;&#33180;&#28814;&#65288;IK&#65289;&#12290;KGDM&#36890;&#36807;&#19982;&#20020;&#24202;&#21307;&#29983;&#30340;&#20114;&#21160;&#65292;&#25552;&#20379;&#21487;&#35270;&#21270;&#25512;&#29702;&#36807;&#31243;&#65292;&#21253;&#25324;AI&#22522;&#20110;&#29983;&#29289;&#26631;&#35760;&#21644;&#20855;&#26377;&#30456;&#21516;&#35786;&#26029;&#27169;&#24335;&#30340;&#30149;&#20363;&#12290;&#36825;&#19968;&#26041;&#27861;&#26377;&#26395;&#25552;&#39640;&#23433;&#20840;&#24615;&#21644;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#20154;&#24037;&#26234;&#33021;&#22312;&#21307;&#23398;&#22270;&#20687;&#35786;&#26029;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20854;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#20351;&#24471;&#23558;&#8220;&#40657;&#30418;&#23376;&#8221;&#34701;&#20837;&#20020;&#24202;&#24037;&#20316;&#27969;&#31243;&#21464;&#24471;&#22256;&#38590;&#12290;&#20026;&#20102;&#20351;&#20174;&#25968;&#25454;&#20013;&#23398;&#21040;&#30340;&#35786;&#26029;&#27169;&#24335;&#23545;&#20020;&#24202;&#21307;&#29983;&#21487;&#29702;&#35299;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#65292;&#21363;&#30693;&#35782;&#24341;&#23548;&#35786;&#26029;&#27169;&#22411;&#65288;KGDM&#65289;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#35270;&#21270;&#30340;&#25512;&#29702;&#36807;&#31243;&#65292;&#21253;&#21547;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#29983;&#29289;&#26631;&#35760;&#21644;&#20855;&#26377;&#30456;&#21516;&#35786;&#26029;&#27169;&#24335;&#30340;&#26816;&#32034;&#30149;&#20363;&#12290;&#36890;&#36807;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#30340;&#20114;&#21160;&#65292;KGDM&#23558;&#20020;&#24202;&#21307;&#29983;&#30340;&#25552;&#31034;&#34701;&#20837;&#21040;&#35299;&#37322;&#25512;&#29702;&#20013;&#65292;&#20174;&#32780;&#21487;&#33021;&#25552;&#39640;&#23433;&#20840;&#24615;&#21644;&#26356;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;KGDM&#22312;&#20256;&#26579;&#24615;&#35282;&#33180;&#28814;&#65288;IK&#65289;&#35786;&#26029;&#20013;&#30340;&#24615;&#33021;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#20020;&#24202;&#23454;&#29992;&#24615;&#65292;IK &#26159;&#35282;&#33180;&#22833;&#26126;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290; KGDM &#30340;&#20998;&#31867;&#24615;&#33021;&#22312;&#19968;&#32452;&#21069;&#30651;&#24615;&#39564;&#35777;&#25968;&#25454;&#38598;&#12289;&#22806;&#37096;&#27979;&#35797;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although data-driven artificial intelligence (AI) in medical image diagnosis has shown impressive performance in silico, the lack of interpretability makes it difficult to incorporate the "black box" into clinicians' workflows. To make the diagnostic patterns learned from data understandable by clinicians, we develop an interpretable model, knowledge-guided diagnosis model (KGDM), that provides a visualized reasoning process containing AI-based biomarkers and retrieved cases that with the same diagnostic patterns. It embraces clinicians' prompts into the interpreted reasoning through human-AI interaction, leading to potentially enhanced safety and more accurate predictions. This study investigates the performance, interpretability, and clinical utility of KGDM in the diagnosis of infectious keratitis (IK), which is the leading cause of corneal blindness. The classification performance of KGDM is evaluated on a prospective validation dataset, an external testing dataset, and an publicly
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#32622;&#20449;&#24230;&#24341;&#23548;&#21644;&#22522;&#20110;&#26679;&#26412;&#30340;&#26041;&#27861;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#35823;&#20449;&#24687;&#28040;&#38500;&#20013;&#30340;&#24187;&#35273;&#21644;&#36807;&#24230;&#33258;&#20449;&#30340;&#39044;&#27979;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#28151;&#21512;&#26694;&#26550;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2401.08694</link><description>&lt;p&gt;
&#32467;&#21512;&#32622;&#20449;&#24230;&#24341;&#23548;&#21644;&#22522;&#20110;&#26679;&#26412;&#30340;&#26041;&#27861;&#29992;&#20110;&#28040;&#38500;&#35823;&#20449;&#24687;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Combining Confidence Elicitation and Sample-based Methods for Uncertainty Quantification in Misinformation Mitigation. (arXiv:2401.08694v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08694
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#32622;&#20449;&#24230;&#24341;&#23548;&#21644;&#22522;&#20110;&#26679;&#26412;&#30340;&#26041;&#27861;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#35823;&#20449;&#24687;&#28040;&#38500;&#20013;&#30340;&#24187;&#35273;&#21644;&#36807;&#24230;&#33258;&#20449;&#30340;&#39044;&#27979;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#28151;&#21512;&#26694;&#26550;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#35299;&#20915;&#35823;&#20449;&#24687;&#28040;&#38500;&#30340;&#20027;&#35201;&#20505;&#36873;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#22312;&#24187;&#35273;&#21644;&#36807;&#24230;&#33258;&#20449;&#30340;&#39044;&#27979;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26694;&#26550;&#65292;&#21033;&#29992;&#30452;&#25509;&#32622;&#20449;&#24230;&#24341;&#23548;&#21644;&#22522;&#20110;&#26679;&#26412;&#30340;&#19968;&#33268;&#24615;&#26041;&#27861;&#65292;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#35823;&#20449;&#24687;&#28040;&#38500;&#35299;&#20915;&#26041;&#26696;&#25552;&#20379;&#26356;&#22909;&#30340;&#26657;&#20934;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30740;&#31350;&#22522;&#20110;&#26679;&#26412;&#19968;&#33268;&#24615;&#26041;&#27861;&#30340;&#26657;&#20934;&#24615;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#26679;&#26412;&#35268;&#27169;&#21644;&#38543;&#26426;&#27700;&#24179;&#30340;&#19968;&#33268;&#24615;&#30340;&#19981;&#21516;&#29305;&#24449;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#40065;&#26834;&#30340;&#25968;&#23383;&#21270;&#21475;&#22836;&#25552;&#31034;&#22312;&#21333;&#27493;&#21644;&#20004;&#27493;&#32622;&#20449;&#24230;&#24341;&#23548;&#36807;&#31243;&#20013;&#30340;&#24615;&#33021;&#21644;&#20998;&#24067;&#21464;&#21270;&#12290;&#25105;&#20204;&#36824;&#27604;&#36739;&#20102;&#30456;&#21516;&#25552;&#31034;&#22312;&#19981;&#21516;&#29256;&#26412;&#30340;GPT&#21644;&#19981;&#21516;&#25968;&#23383;&#23610;&#24230;&#19979;&#30340;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#32467;&#21512;&#22522;&#20110;&#26679;&#26412;&#19968;&#33268;&#24615;&#21644;&#25968;&#23383;&#21270;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#26694;&#26550;&#65292;&#20026;GPT&#27169;&#22411;&#25552;&#20379;&#26356;&#22909;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models have emerged as prime candidates to tackle misinformation mitigation. However, existing approaches struggle with hallucinations and overconfident predictions. We propose an uncertainty quantification framework that leverages both direct confidence elicitation and sampled-based consistency methods to provide better calibration for NLP misinformation mitigation solutions. We first investigate the calibration of sample-based consistency methods that exploit distinct features of consistency across sample sizes and stochastic levels. Next, we evaluate the performance and distributional shift of a robust numeric verbalization prompt across single vs. two-step confidence elicitation procedure. We also compare the performance of the same prompt with different versions of GPT and different numerical scales. Finally, we combine the sample-based consistency and verbalized methods to propose a hybrid framework that yields a better uncertainty estimation for GPT models. Overal
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#30828;&#20214;&#35774;&#35745;&#20013;&#24555;&#36895;&#29983;&#25104;RTL&#20195;&#30721;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#26032;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#22914;&#20309;&#25552;&#20379;&#21151;&#33021;&#12289;&#20248;&#21270;&#21644;&#31526;&#21512;&#34892;&#19994;&#26631;&#20934;&#30340;RTL&#20195;&#30721;&#12290;</title><link>http://arxiv.org/abs/2401.08683</link><description>&lt;p&gt;
&#20351;&#29992;&#22686;&#24378;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#38646;&#26679;&#26412;RTL&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot RTL Code Generation with Attention Sink Augmented Large Language Models. (arXiv:2401.08683v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08683
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#30828;&#20214;&#35774;&#35745;&#20013;&#24555;&#36895;&#29983;&#25104;RTL&#20195;&#30721;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#26032;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#22914;&#20309;&#25552;&#20379;&#21151;&#33021;&#12289;&#20248;&#21270;&#21644;&#31526;&#21512;&#34892;&#19994;&#26631;&#20934;&#30340;RTL&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#30828;&#20214;&#35774;&#35745;&#21644;&#20248;&#21270;&#38656;&#32791;&#36153;&#22823;&#37327;&#36164;&#28304;&#65292;&#38656;&#35201;&#30456;&#24403;&#19987;&#19994;&#30693;&#35782;&#65292;&#24182;&#20381;&#36182;&#20110;&#24050;&#24314;&#31435;&#30340;&#35774;&#35745;&#33258;&#21160;&#21270;&#24037;&#20855;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#31616;&#21270;&#30828;&#20214;&#35774;&#35745;&#20013;&#30340;&#20195;&#30721;&#29983;&#25104;&#36807;&#31243;&#30340;&#21487;&#33021;&#24615;&#12290;&#19982;&#20043;&#21069;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;&#26412;&#25991;&#26088;&#22312;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#19968;&#20010;&#21333;&#19968;&#25552;&#31034;&#25509;&#21463;&#39640;&#23618;&#35774;&#35745;&#35268;&#33539;&#65292;&#29983;&#25104;&#30456;&#24212;&#30340;&#23492;&#23384;&#22120;&#20256;&#36755;&#32423;&#65288;RTL&#65289;&#20195;&#30721;&#12290;&#33021;&#22815;&#22312;RTL&#20195;&#30721;&#29983;&#25104;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19981;&#20165;&#21152;&#24555;&#20102;&#35774;&#35745;&#36845;&#20195;&#21608;&#26399;&#65292;&#36824;&#20415;&#20110;&#25506;&#32034;&#20256;&#32479;&#25216;&#26415;&#38590;&#20197;&#22788;&#29702;&#30340;&#35774;&#35745;&#31354;&#38388;&#30340;&#35745;&#31639;&#25361;&#25112;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#29616;&#26377;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#19981;&#36275;&#65292;&#24182;&#20171;&#32461;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#26102;&#20135;&#29983;&#21151;&#33021;&#12289;&#20248;&#21270;&#19988;&#31526;&#21512;&#34892;&#19994;&#26631;&#20934;&#30340;RTL&#20195;&#30721;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The design and optimization of hardware have traditionally been resource-intensive, demanding considerable expertise and dependence on established design automation tools. This paper discusses the possibility of exploiting large language models to streamline the code generation process in hardware design. In contrast to earlier studies, this paper aims to use large language models that accepts high-level design specifications through a single prompt to generate corresponding Register-Transfer Level (RTL) code. The ability to use large language models on RTL code generation not only expedites design iteration cycles but also facilitates the exploration of design spaces that have computational challenges for conventional techniques. Through our evaluation, we demonstrate the shortcoming of existing attention mechanisms, and present the abilities of language models to produce functional, optimized, and industry-standard compliant RTL code when a novel attention mechanism is used. These fi
&lt;/p&gt;</description></item><item><title>&#22312;AI&#23545;&#40784;&#30340;&#35752;&#35770;&#20013;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#27010;&#24565;&#23545;&#40784;&#30340;&#37325;&#35201;&#24615;&#65292;&#35748;&#20026;&#22312;&#23545;&#40784;&#20215;&#20540;&#35266;&#20043;&#21069;&#65292;AI&#31995;&#32479;&#21644;&#20154;&#31867;&#24517;&#39035;&#23545;&#20854;&#29702;&#35299;&#19990;&#30028;&#25152;&#20351;&#29992;&#30340;&#27010;&#24565;&#36827;&#34892;&#23545;&#40784;&#12290;&#25105;&#20204;&#25972;&#21512;&#20102;&#21746;&#23398;&#12289;&#35748;&#30693;&#31185;&#23398;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#24605;&#24819;&#65292;&#24182;&#25552;&#20986;&#20102;&#23454;&#29616;&#20849;&#20139;&#27010;&#24565;&#30340;&#26426;&#20250;&#21644;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.08672</link><description>&lt;p&gt;
&#27010;&#24565;&#23545;&#40784;&#65288;arXiv&#65306;2401.08672v1 [cs.LG]&#65289;
&lt;/p&gt;
&lt;p&gt;
Concept Alignment. (arXiv:2401.08672v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08672
&lt;/p&gt;
&lt;p&gt;
&#22312;AI&#23545;&#40784;&#30340;&#35752;&#35770;&#20013;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#27010;&#24565;&#23545;&#40784;&#30340;&#37325;&#35201;&#24615;&#65292;&#35748;&#20026;&#22312;&#23545;&#40784;&#20215;&#20540;&#35266;&#20043;&#21069;&#65292;AI&#31995;&#32479;&#21644;&#20154;&#31867;&#24517;&#39035;&#23545;&#20854;&#29702;&#35299;&#19990;&#30028;&#25152;&#20351;&#29992;&#30340;&#27010;&#24565;&#36827;&#34892;&#23545;&#40784;&#12290;&#25105;&#20204;&#25972;&#21512;&#20102;&#21746;&#23398;&#12289;&#35748;&#30693;&#31185;&#23398;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#24605;&#24819;&#65292;&#24182;&#25552;&#20986;&#20102;&#23454;&#29616;&#20849;&#20139;&#27010;&#24565;&#30340;&#26426;&#20250;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#23545;&#40784;&#65288;&#20154;&#31867;&#21644;AI&#31995;&#32479;&#20043;&#38388;&#30340;&#23545;&#40784;&#65289;&#30340;&#35752;&#35770;&#20027;&#35201;&#38598;&#20013;&#22312;&#20215;&#20540;&#23545;&#40784;&#19978;&#65292;&#24191;&#20041;&#19978;&#25351;&#30340;&#26159;&#21019;&#24314;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30456;&#21516;&#30340;AI&#31995;&#32479;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#22312;&#25105;&#20204;&#23581;&#35797;&#23545;&#40784;&#20215;&#20540;&#35266;&#20043;&#21069;&#65292;AI&#31995;&#32479;&#21644;&#20154;&#31867;&#23545;&#20110;&#29702;&#35299;&#19990;&#30028;&#25152;&#20351;&#29992;&#30340;&#27010;&#24565;&#24517;&#39035;&#39318;&#20808;&#23545;&#40784;&#12290;&#25105;&#20204;&#25972;&#21512;&#20102;&#21746;&#23398;&#12289;&#35748;&#30693;&#31185;&#23398;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#24605;&#24819;&#65292;&#35299;&#37322;&#20102;&#20154;&#31867;&#21644;&#26426;&#22120;&#20043;&#38388;&#38656;&#35201;&#27010;&#24565;&#23545;&#40784;&#32780;&#19981;&#20165;&#20165;&#26159;&#20215;&#20540;&#23545;&#40784;&#30340;&#24517;&#35201;&#24615;&#12290;&#25105;&#20204;&#24635;&#32467;&#20102;&#30446;&#21069;&#20154;&#31867;&#21644;&#26426;&#22120;&#23398;&#20064;&#27010;&#24565;&#30340;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#27010;&#36848;&#20102;&#23454;&#29616;&#20849;&#20139;&#27010;&#24565;&#30340;&#26426;&#20250;&#21644;&#25361;&#25112;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35299;&#37322;&#20102;&#22914;&#20309;&#21033;&#29992;&#35748;&#30693;&#31185;&#23398;&#21644;AI&#30740;&#31350;&#20013;&#24050;&#32463;&#24320;&#21457;&#30340;&#24037;&#20855;&#21152;&#36895;&#27010;&#24565;&#23545;&#40784;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discussion of AI alignment (alignment between humans and AI systems) has focused on value alignment, broadly referring to creating AI systems that share human values. We argue that before we can even attempt to align values, it is imperative that AI systems and humans align the concepts they use to understand the world. We integrate ideas from philosophy, cognitive science, and deep learning to explain the need for concept alignment, not just value alignment, between humans and machines. We summarize existing accounts of how humans and machines currently learn concepts, and we outline opportunities and challenges in the path towards shared concepts. Finally, we explain how we can leverage the tools already being developed in cognitive science and AI research to accelerate progress towards concept alignment.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#22810;&#21345;&#36710;&#22810;&#20998;&#27573;&#38656;&#27714;&#36335;&#24452;&#30340;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#65292;&#36890;&#36807;&#23545;&#29616;&#26377;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27880;&#24847;&#21147;&#27169;&#22411;&#36827;&#34892;&#25193;&#23637;&#65292;&#23454;&#29616;&#20102;&#22312;&#24037;&#19994;&#20379;&#24212;&#38142;&#29289;&#27969;&#20013;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26377;&#25928;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2401.08669</link><description>&lt;p&gt;
&#22810;&#21345;&#36710;&#22810;&#20998;&#27573;&#38656;&#27714;&#36335;&#24452;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning for Multi-Truck Vehicle Routing Problems with Multi-Leg Demand Routes. (arXiv:2401.08669v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08669
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#22810;&#21345;&#36710;&#22810;&#20998;&#27573;&#38656;&#27714;&#36335;&#24452;&#30340;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#65292;&#36890;&#36807;&#23545;&#29616;&#26377;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27880;&#24847;&#21147;&#27169;&#22411;&#36827;&#34892;&#25193;&#23637;&#65292;&#23454;&#29616;&#20102;&#22312;&#24037;&#19994;&#20379;&#24212;&#38142;&#29289;&#27969;&#20013;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26377;&#25928;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#24050;&#34987;&#35777;&#26126;&#22312;&#35299;&#20915;&#19968;&#20123;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#26102;&#38750;&#24120;&#26377;&#25928;&#65292;&#29305;&#21035;&#26159;&#22312;&#20351;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27880;&#24847;&#21147;&#26426;&#21046;&#29983;&#25104;&#30340;&#31574;&#30053;&#26102;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#19968;&#20123;&#30456;&#23545;&#31616;&#21333;&#30340;&#38382;&#39064;&#23454;&#20363;&#65292;&#36825;&#20123;&#25216;&#26415;&#24050;&#32463;&#21462;&#24471;&#20102;&#30456;&#24403;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#23545;&#20110;&#19968;&#20123;&#20173;&#26410;&#24471;&#21040;&#30740;&#31350;&#21644;&#38750;&#24120;&#22797;&#26434;&#30340;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#21464;&#20307;&#65292;&#23578;&#26410;&#35777;&#26126;&#26377;&#26377;&#25928;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#21487;&#29992;&#12290;&#26412;&#25991;&#32858;&#28966;&#20110;&#19968;&#31181;&#36825;&#26679;&#30340;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#21464;&#20307;&#65292;&#20854;&#20013;&#21253;&#21547;&#22810;&#36742;&#21345;&#36710;&#21644;&#22810;&#20998;&#27573;&#36335;&#24452;&#35201;&#27714;&#12290;&#22312;&#36825;&#20123;&#38382;&#39064;&#20013;&#65292;&#38656;&#27714;&#38656;&#35201;&#27839;&#30528;&#33410;&#28857;&#24207;&#21015;&#31227;&#21160;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#20174;&#36215;&#28857;&#21040;&#32456;&#28857;&#12290;&#20026;&#20102;&#20351;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25104;&#20026;&#36866;&#29992;&#20110;&#23454;&#38469;&#24037;&#19994;&#35268;&#27169;&#30340;&#20379;&#24212;&#38142;&#29289;&#27969;&#30340;&#26377;&#25928;&#31574;&#30053;&#65292;&#25105;&#20204;&#23545;&#29616;&#26377;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27880;&#24847;&#21147;&#27169;&#22411;&#36827;&#34892;&#20102;&#26032;&#25193;&#23637;&#65292;&#20351;&#20854;&#33021;&#22788;&#29702;&#22810;&#21345;&#36710;&#21644;&#22810;&#20998;&#27573;&#36335;&#24452;&#35201;&#27714;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20855;&#26377;&#36825;&#26679;&#30340;&#20248;&#21183;&#65292;&#21487;&#20197;&#22312;&#23567;&#35268;&#27169;&#25968;&#25454;&#35757;&#32451;&#19979;&#36827;&#34892;&#65292;&#24182;&#33021;&#22312;&#24037;&#19994;&#20379;&#24212;&#38142;&#29289;&#27969;&#20013;&#36827;&#34892;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning (RL) has been shown to be effective in producing approximate solutions to some vehicle routing problems (VRPs), especially when using policies generated by encoder-decoder attention mechanisms. While these techniques have been quite successful for relatively simple problem instances, there are still under-researched and highly complex VRP variants for which no effective RL method has been demonstrated. In this work we focus on one such VRP variant, which contains multiple trucks and multi-leg routing requirements. In these problems, demand is required to move along sequences of nodes, instead of just from a start node to an end node. With the goal of making deep RL a viable strategy for real-world industrial-scale supply chain logistics, we develop new extensions to existing encoder-decoder attention models which allow them to handle multiple trucks and multi-leg routing requirements. Our models have the advantage that they can be trained for a small number 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22238;&#39038;&#20102;&#38024;&#23545;&#25945;&#32946;&#33021;&#21147;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#65292;&#21253;&#25324;&#25968;&#23398;&#12289;&#20889;&#20316;&#12289;&#32534;&#31243;&#12289;&#25512;&#29702;&#21644;&#22522;&#20110;&#30693;&#35782;&#30340;&#38382;&#31572;&#65292;&#26088;&#22312;&#25506;&#32034;&#20854;&#22312;&#26500;&#24314;&#19979;&#19968;&#20195;&#26234;&#33021;&#25945;&#32946;&#31995;&#32479;&#20013;&#30340;&#28508;&#21147;&#21644;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.08664</link><description>&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#25945;&#32946;&#65306;&#22522;&#26412;&#33021;&#21147;&#12289;&#28508;&#21147;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Adapting Large Language Models for Education: Foundational Capabilities, Potentials, and Challenges. (arXiv:2401.08664v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08664
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#20102;&#38024;&#23545;&#25945;&#32946;&#33021;&#21147;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#65292;&#21253;&#25324;&#25968;&#23398;&#12289;&#20889;&#20316;&#12289;&#32534;&#31243;&#12289;&#25512;&#29702;&#21644;&#22522;&#20110;&#30693;&#35782;&#30340;&#38382;&#31572;&#65292;&#26088;&#22312;&#25506;&#32034;&#20854;&#22312;&#26500;&#24314;&#19979;&#19968;&#20195;&#26234;&#33021;&#25945;&#32946;&#31995;&#32479;&#20013;&#30340;&#28508;&#21147;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#25945;&#32946;&#24179;&#21488;&#21033;&#29992;&#20114;&#32852;&#32593;&#20998;&#21457;&#25945;&#32946;&#36164;&#28304;&#65292;&#26088;&#22312;&#25552;&#20379;&#20415;&#25463;&#30340;&#25945;&#32946;&#65292;&#20294;&#24448;&#24448;&#22312;&#19982;&#23398;&#29983;&#30340;&#23454;&#26102;&#20132;&#27969;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#12290;&#30001;&#20110;&#38656;&#35201;&#35299;&#20915;&#23398;&#29983;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#36935;&#21040;&#30340;&#22810;&#26679;&#21270;&#38556;&#30861;&#30340;&#25361;&#25112;&#65292;&#23427;&#20204;&#32463;&#24120;&#38590;&#20197;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#25945;&#32946;&#36164;&#28304;&#12290;&#26368;&#36817;&#20986;&#29616;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;ChatGPT&#65292;&#25552;&#20379;&#20102;&#36890;&#36807;&#29702;&#35299;&#20010;&#20307;&#35831;&#27714;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#21487;&#33021;&#24615;&#12290;&#34429;&#28982;LLMs&#22312;&#21508;&#20010;&#39046;&#22495;&#37117;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#22522;&#20110;LLM&#30340;&#25945;&#32946;&#31995;&#32479;&#30340;&#26500;&#24314;&#20173;&#28982;&#38754;&#20020;&#30528;&#24191;&#27867;&#30340;&#25945;&#32946;&#25216;&#33021;&#35201;&#27714;&#12290;&#26412;&#25991;&#22238;&#39038;&#20102;&#19982;&#25945;&#32946;&#33021;&#21147;&#30456;&#20851;&#30340;&#36817;&#26399;&#20986;&#29616;&#30340;LLM&#30740;&#31350;&#65292;&#21253;&#25324;&#25968;&#23398;&#12289;&#20889;&#20316;&#12289;&#32534;&#31243;&#12289;&#25512;&#29702;&#21644;&#22522;&#20110;&#30693;&#35782;&#30340;&#38382;&#31572;&#65292;&#26088;&#22312;&#25506;&#32034;&#23427;&#20204;&#22312;&#26500;&#24314;&#19979;&#19968;&#20195;&#26234;&#33021;&#25945;&#32946;&#31995;&#32479;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online education platforms, leveraging the internet to distribute education resources, seek to provide convenient education but often fall short in real-time communication with students. They often struggle to offer personalized education resources due to the challenge of addressing the diverse obstacles students encounter throughout their learning journey. Recently, the emergence of large language models (LLMs), such as ChatGPT, offers the possibility for resolving this issue by comprehending individual requests. Although LLMs have been successful in various fields, creating an LLM-based education system is still challenging for the wide range of educational skills required. This paper reviews the recently emerged LLM researches related to educational capabilities, including mathematics, writing, programming, reasoning, and knowledge-based question answering, with the aim to explore their potential in constructing the next-generation intelligent education system. Based on the current 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26377;&#38480;&#30340;&#39134;&#34892;&#21592;&#31034;&#33539;&#25968;&#25454;&#30340;&#38598;&#25104;&#27169;&#20223;&#21644;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#24314;&#31435;&#36866;&#29992;&#20110;&#24191;&#27867;&#26465;&#20214;&#30340;&#25935;&#25463;&#39134;&#34892;&#22120;&#30340;&#25968;&#25454;&#39537;&#21160;&#26426;&#21160;&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#26500;&#24314;&#27169;&#22411;&#25152;&#38656;&#30340;&#22823;&#37327;&#30495;&#23454;&#25968;&#25454;&#30340;&#26102;&#38388;&#21644;&#25104;&#26412;&#38382;&#39064;&#65292;&#24182;&#33021;&#22815;&#27867;&#21270;&#21040;&#20854;&#20182;&#39134;&#34892;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2401.08663</link><description>&lt;p&gt;
&#29992;&#26377;&#38480;&#30340;&#39134;&#34892;&#21592;&#31034;&#33539;&#25968;&#25454;&#36827;&#34892;&#31283;&#20581;&#25935;&#25463;&#39134;&#34892;&#22120;&#25511;&#21046;&#30340;&#38598;&#25104;&#27169;&#20223;&#21644;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Integrated Imitation and Reinforcement Learning Methodology for Robust Agile Aircraft Control with Limited Pilot Demonstration Data. (arXiv:2401.08663v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08663
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26377;&#38480;&#30340;&#39134;&#34892;&#21592;&#31034;&#33539;&#25968;&#25454;&#30340;&#38598;&#25104;&#27169;&#20223;&#21644;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#24314;&#31435;&#36866;&#29992;&#20110;&#24191;&#27867;&#26465;&#20214;&#30340;&#25935;&#25463;&#39134;&#34892;&#22120;&#30340;&#25968;&#25454;&#39537;&#21160;&#26426;&#21160;&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#26500;&#24314;&#27169;&#22411;&#25152;&#38656;&#30340;&#22823;&#37327;&#30495;&#23454;&#25968;&#25454;&#30340;&#26102;&#38388;&#21644;&#25104;&#26412;&#38382;&#39064;&#65292;&#24182;&#33021;&#22815;&#27867;&#21270;&#21040;&#20854;&#20182;&#39134;&#34892;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#26500;&#24314;&#36866;&#29992;&#20110;&#25935;&#25463;&#39134;&#34892;&#22120;&#22312;&#24191;&#27867;&#30340;&#24179;&#34913;&#26465;&#20214;&#21644;&#39134;&#34892;&#22120;&#27169;&#22411;&#21442;&#25968;&#19979;&#21487;&#20197;&#27867;&#21270;&#30340;&#25968;&#25454;&#39537;&#21160;&#26426;&#21160;&#29983;&#25104;&#27169;&#22411;&#12290;&#26426;&#21160;&#29983;&#25104;&#27169;&#22411;&#22312;&#39134;&#34892;&#22120;&#21407;&#22411;&#30340;&#27979;&#35797;&#21644;&#35780;&#20272;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#33021;&#22815;&#25552;&#20379;&#20851;&#20110;&#39134;&#34892;&#22120;&#30340;&#26426;&#21160;&#24615;&#21644;&#25935;&#25463;&#24615;&#30340;&#35265;&#35299;&#12290;&#28982;&#32780;&#65292;&#26500;&#24314;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#30495;&#23454;&#39134;&#34892;&#21592;&#25968;&#25454;&#65292;&#36825;&#21487;&#33021;&#32791;&#26102;&#19988;&#26114;&#36149;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#26377;&#38480;&#25968;&#25454;&#26500;&#24314;&#30340;&#27169;&#22411;&#24448;&#24448;&#38590;&#20197;&#22312;&#21407;&#22987;&#25968;&#25454;&#38598;&#25152;&#21253;&#21547;&#30340;&#29305;&#23450;&#39134;&#34892;&#26465;&#20214;&#20043;&#22806;&#36827;&#34892;&#27867;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26550;&#26500;&#65292;&#21033;&#29992;&#19968;&#20010;&#31216;&#20026;&#28304;&#27169;&#22411;&#30340;&#20223;&#30495;&#27169;&#22411;&#12290;&#36825;&#20010;&#24320;&#28304;&#25935;&#25463;&#39134;&#34892;&#22120;&#27169;&#25311;&#22120;&#19982;&#30446;&#26631;&#39134;&#34892;&#22120;&#20855;&#26377;&#30456;&#20284;&#30340;&#21160;&#21147;&#23398;&#29305;&#24615;&#65292;&#24182;&#20801;&#35768;&#25105;&#20204;&#29983;&#25104;&#26080;&#38480;&#30340;&#25968;&#25454;&#26469;&#26500;&#24314;&#20195;&#29702;&#26426;&#21160;&#29983;&#25104;&#27169;&#22411;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;&#35813;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a methodology for constructing data-driven maneuver generation models for agile aircraft that can generalize across a wide range of trim conditions and aircraft model parameters. Maneuver generation models play a crucial role in the testing and evaluation of aircraft prototypes, providing insights into the maneuverability and agility of the aircraft. However, constructing the models typically requires extensive amounts of real pilot data, which can be time-consuming and costly to obtain. Moreover, models built with limited data often struggle to generalize beyond the specific flight conditions covered in the original dataset. To address these challenges, we propose a hybrid architecture that leverages a simulation model, referred to as the source model. This open-source agile aircraft simulator shares similar dynamics with the target aircraft and allows us to generate unlimited data for building a proxy maneuver generation model. We then fine-tune this model t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;Gemini Pro&#21644;GPT-4V&#22312;&#25945;&#32946;&#39046;&#22495;&#30340;&#20998;&#31867;&#24615;&#33021;&#65292;&#21457;&#29616;GPT-4V&#22312;&#35780;&#20998;&#20934;&#30830;&#24230;&#21644;Quadratic Weighted Kappa&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;Gemini Pro&#65292;&#21487;&#33021;&#26159;&#22240;&#20026;GPT-4V&#33021;&#22815;&#22788;&#29702;&#22270;&#20687;&#20013;&#30340;&#32454;&#33268;&#25991;&#26412;&#24182;&#20855;&#26377;&#26356;&#22909;&#30340;&#25972;&#20307;&#22270;&#20687;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.08660</link><description>&lt;p&gt;
Gemini Pro&#34987;GPT-4V&#20987;&#36133;&#65306;&#26469;&#33258;&#25945;&#32946;&#39046;&#22495;&#30340;&#35777;&#25454;
&lt;/p&gt;
&lt;p&gt;
Gemini Pro Defeated by GPT-4V: Evidence from Education. (arXiv:2401.08660v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08660
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;Gemini Pro&#21644;GPT-4V&#22312;&#25945;&#32946;&#39046;&#22495;&#30340;&#20998;&#31867;&#24615;&#33021;&#65292;&#21457;&#29616;GPT-4V&#22312;&#35780;&#20998;&#20934;&#30830;&#24230;&#21644;Quadratic Weighted Kappa&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;Gemini Pro&#65292;&#21487;&#33021;&#26159;&#22240;&#20026;GPT-4V&#33021;&#22815;&#22788;&#29702;&#22270;&#20687;&#20013;&#30340;&#32454;&#33268;&#25991;&#26412;&#24182;&#20855;&#26377;&#26356;&#22909;&#30340;&#25972;&#20307;&#22270;&#20687;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#25945;&#32946;&#29615;&#22659;&#20013;&#27604;&#36739;&#20102;Gemini Pro&#21644;GPT-4V&#22312;&#20998;&#31867;&#24615;&#33021;&#19978;&#30340;&#34920;&#29616;&#12290;&#36890;&#36807;&#20351;&#29992;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#25216;&#26415;&#65292;&#26412;&#30740;&#31350;&#32771;&#23519;&#20102;&#20004;&#20010;&#27169;&#22411;&#22312;&#38405;&#35835;&#22522;&#20110;&#25991;&#26412;&#30340;&#35780;&#20998;&#26631;&#20934;&#24182;&#33258;&#21160;&#35780;&#20998;&#31185;&#23398;&#25945;&#32946;&#20013;&#23398;&#29983;&#32472;&#21046;&#30340;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#23450;&#37327;&#21644;&#23450;&#24615;&#20998;&#26512;&#65292;&#24182;&#20351;&#29992;&#20102;&#20174;&#23398;&#29983;&#32472;&#21046;&#30340;&#31185;&#23398;&#27169;&#22411;&#20013;&#24471;&#20986;&#30340;&#25968;&#25454;&#38598;&#20197;&#21450;NERIF&#65288;Notation-Enhanced Rubrics for Image Feedback&#65289;&#25552;&#38382;&#26041;&#27861;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;GPT-4V&#22312;&#35780;&#20998;&#20934;&#30830;&#24230;&#21644;Quadratic Weighted Kappa&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;Gemini Pro&#12290;&#23450;&#24615;&#20998;&#26512;&#26174;&#31034;&#65292;&#36825;&#31181;&#24046;&#24322;&#21487;&#33021;&#28304;&#20110;&#27169;&#22411;&#22312;&#22270;&#20687;&#20013;&#22788;&#29702;&#32454;&#33268;&#25991;&#26412;&#21644;&#25972;&#20307;&#22270;&#20687;&#20998;&#31867;&#24615;&#33021;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#21363;&#20351;&#36890;&#36807;&#36827;&#19968;&#27493;&#32553;&#23567;&#36755;&#20837;&#22270;&#20687;&#30340;NERIF&#26041;&#27861;&#26469;&#36866;&#24212;Gemini Pro&#65292;&#20854;&#24615;&#33021;&#20173;&#19981;&#22914;GPT-4V&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;GPT-4V&#22312;&#22788;&#29702;&#22797;&#26434;&#22810;&#26679;&#30340;&#23398;&#29983;&#32472;&#21046;&#27169;&#22411;&#26041;&#38754;&#20855;&#26377;&#26356;&#24378;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study compared the classification performance of Gemini Pro and GPT-4V in educational settings. Employing visual question answering (VQA) techniques, the study examined both models' abilities to read text-based rubrics and then automatically score student-drawn models in science education. We employed both quantitative and qualitative analyses using a dataset derived from student-drawn scientific models and employing NERIF (Notation-Enhanced Rubrics for Image Feedback) prompting methods. The findings reveal that GPT-4V significantly outperforms Gemini Pro in terms of scoring accuracy and Quadratic Weighted Kappa. The qualitative analysis reveals that the differences may be due to the models' ability to process fine-grained texts in images and overall image classification performance. Even adapting the NERIF approach by further de-sizing the input images, Gemini Pro seems not able to perform as well as GPT-4V. The findings suggest GPT-4V's superior capability in handling complex mu
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#24635;&#32467;&#20102;&#24037;&#19994;&#21644;&#23398;&#26415;&#30028;&#20013;&#30340;&#33258;&#21160;&#39550;&#39542;&#31471;&#21040;&#31471;&#35268;&#21010;&#26041;&#27861;&#65292;&#21253;&#25324;&#29305;&#26031;&#25289;FSD V12&#12289;Momenta 2023&#12289;Horizon Robotics 2023&#12289;Motional RoboTaxi 2022&#12289;Woven Planet&#65288;&#20016;&#30000;&#65289;&#65306;&#22478;&#24066;&#39550;&#39542;&#21592;&#21644;Nvidia&#65292;&#24182;&#22238;&#39038;&#20102;&#26368;&#26032;&#30340;&#23398;&#26415;&#30740;&#31350;&#12290;&#36825;&#31687;&#25991;&#31456;&#25552;&#20379;&#20102;2022-2023&#24180;&#31471;&#21040;&#31471;&#35268;&#21010;&#30340;&#26368;&#26032;&#32467;&#26500;&#21644;&#24555;&#36895;&#23398;&#20064;&#65292;&#24182;&#36866;&#29992;&#20110;&#21021;&#23398;&#32773;&#21644;&#39640;&#32423;&#30740;&#31350;&#20154;&#21592;&#12290;</title><link>http://arxiv.org/abs/2401.08658</link><description>&lt;p&gt;
&#24037;&#19994;&#21644;&#23398;&#26415;&#30028;&#20013;&#30340;&#33258;&#21160;&#39550;&#39542;&#31471;&#21040;&#31471;&#35268;&#21010;&#65306;2022-2023&#24180;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
End-To-End Planning of Autonomous Driving in Industry and Academia: 2022-2023. (arXiv:2401.08658v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08658
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#24635;&#32467;&#20102;&#24037;&#19994;&#21644;&#23398;&#26415;&#30028;&#20013;&#30340;&#33258;&#21160;&#39550;&#39542;&#31471;&#21040;&#31471;&#35268;&#21010;&#26041;&#27861;&#65292;&#21253;&#25324;&#29305;&#26031;&#25289;FSD V12&#12289;Momenta 2023&#12289;Horizon Robotics 2023&#12289;Motional RoboTaxi 2022&#12289;Woven Planet&#65288;&#20016;&#30000;&#65289;&#65306;&#22478;&#24066;&#39550;&#39542;&#21592;&#21644;Nvidia&#65292;&#24182;&#22238;&#39038;&#20102;&#26368;&#26032;&#30340;&#23398;&#26415;&#30740;&#31350;&#12290;&#36825;&#31687;&#25991;&#31456;&#25552;&#20379;&#20102;2022-2023&#24180;&#31471;&#21040;&#31471;&#35268;&#21010;&#30340;&#26368;&#26032;&#32467;&#26500;&#21644;&#24555;&#36895;&#23398;&#20064;&#65292;&#24182;&#36866;&#29992;&#20110;&#21021;&#23398;&#32773;&#21644;&#39640;&#32423;&#30740;&#31350;&#20154;&#21592;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#23545;&#30446;&#21069;&#22312;&#24037;&#19994;&#21644;&#23398;&#26415;&#30028;&#25253;&#21578;&#30340;&#21253;&#25324;&#35814;&#32454;&#25216;&#26415;&#22312;&#20869;&#30340;&#26041;&#27861;&#36827;&#34892;&#24555;&#36895;&#22238;&#39038;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26412;&#25991;&#22238;&#39038;&#20102;&#21253;&#25324;&#29305;&#26031;&#25289;FSD V12&#12289;Momenta 2023&#12289;Horizon Robotics 2023&#12289;Motional RoboTaxi 2022&#12289;Woven Planet&#65288;&#20016;&#30000;&#65289;&#65306;&#22478;&#24066;&#39550;&#39542;&#21592;&#21644;Nvidia&#22312;&#20869;&#30340;&#31471;&#21040;&#31471;&#35268;&#21010;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#22238;&#39038;&#20102;&#35843;&#26597;&#33258;&#21160;&#39550;&#39542;&#31471;&#21040;&#31471;&#35268;&#21010;&#30340;&#26368;&#26032;&#23398;&#26415;&#30740;&#31350;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;2022-2023&#24180;&#31471;&#21040;&#31471;&#35268;&#21010;&#30340;&#26368;&#26032;&#32467;&#26500;&#21644;&#24555;&#36895;&#23398;&#20064;&#65292;&#20026;&#21021;&#23398;&#32773;&#25552;&#20379;&#20102;&#20837;&#38376;&#26448;&#26009;&#65292;&#20379;&#20854;&#20102;&#35299;&#24037;&#19994;&#21644;&#23398;&#26415;&#30028;&#20013;&#30340;&#31471;&#21040;&#31471;&#33258;&#21160;&#39550;&#39542;&#35268;&#21010;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#21516;&#26102;&#20063;&#20026;&#39640;&#32423;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#34917;&#20805;&#36164;&#26009;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper aims to provide a quick review of the methods including the technologies in detail that are currently reported in industry and academia. Specifically, this paper reviews the end-to-end planning, including Tesla FSD V12, Momenta 2023, Horizon Robotics 2023, Motional RoboTaxi 2022, Woven Planet (Toyota): Urban Driver, and Nvidia. In addition, we review the state-of-the-art academic studies that investigate end-to-end planning of autonomous driving. This paper provides readers with a concise structure and fast learning of state-of-the-art end-to-end planning for 2022-2023. This article provides a meaningful overview as introductory material for beginners to follow the state-of-the-art end-to-end planning of autonomous driving in industry and academia, as well as supplementary material for advanced researchers.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#65288;SAiD&#65289;&#39537;&#21160;&#30340;&#35821;&#38899;&#39537;&#21160;&#30340;&#19977;&#32500;&#38754;&#37096;&#21160;&#30011;&#26041;&#27861;&#65292;&#36890;&#36807;&#36731;&#37327;&#32423;&#30340;Transformer-based U-Net&#27169;&#22411;&#21644;&#38899;&#39057;&#19982;&#35270;&#35273;&#30340;&#20132;&#21449;&#27169;&#24577;&#23545;&#40784;&#20559;&#24046;&#65292;&#23454;&#29616;&#20102;&#36739;&#22909;&#30340;&#21767;&#37096;&#21516;&#27493;&#21644;&#26356;&#22810;&#26679;&#21270;&#30340;&#21767;&#37096;&#36816;&#21160;&#12290;</title><link>http://arxiv.org/abs/2401.08655</link><description>&lt;p&gt;
SAiD: &#20351;&#29992;&#25193;&#25955;&#26041;&#27861;&#39537;&#21160;&#30340;&#35821;&#38899;&#39537;&#21160;&#34920;&#24773;&#21160;&#30011;
&lt;/p&gt;
&lt;p&gt;
SAiD: Speech-driven Blendshape Facial Animation with Diffusion. (arXiv:2401.08655v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08655
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#65288;SAiD&#65289;&#39537;&#21160;&#30340;&#35821;&#38899;&#39537;&#21160;&#30340;&#19977;&#32500;&#38754;&#37096;&#21160;&#30011;&#26041;&#27861;&#65292;&#36890;&#36807;&#36731;&#37327;&#32423;&#30340;Transformer-based U-Net&#27169;&#22411;&#21644;&#38899;&#39057;&#19982;&#35270;&#35273;&#30340;&#20132;&#21449;&#27169;&#24577;&#23545;&#40784;&#20559;&#24046;&#65292;&#23454;&#29616;&#20102;&#36739;&#22909;&#30340;&#21767;&#37096;&#21516;&#27493;&#21644;&#26356;&#22810;&#26679;&#21270;&#30340;&#21767;&#37096;&#36816;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#36827;&#34892;&#20102;&#22823;&#37327;&#30740;&#31350;&#65292;&#20294;&#30001;&#20110;&#32570;&#20047;&#22823;&#35268;&#27169;&#30340;&#35270;&#21548;&#25968;&#25454;&#38598;&#65292;&#35821;&#38899;&#39537;&#21160;&#30340;&#19977;&#32500;&#38754;&#37096;&#21160;&#30011;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22823;&#22810;&#25968;&#36807;&#21435;&#30340;&#24037;&#20316;&#36890;&#24120;&#37319;&#29992;&#26368;&#23567;&#20108;&#20056;&#27861;&#22312;&#23567;&#25968;&#25454;&#38598;&#19978;&#23398;&#20064;&#22238;&#24402;&#27169;&#22411;&#65292;&#20294;&#22312;&#20174;&#35821;&#38899;&#29983;&#25104;&#21508;&#31181;&#21767;&#37096;&#21160;&#20316;&#26041;&#38754;&#36935;&#21040;&#22256;&#38590;&#65292;&#24182;&#19988;&#38656;&#35201;&#22823;&#37327;&#31934;&#32454;&#35843;&#25972;&#29983;&#25104;&#30340;&#36755;&#20986;&#32467;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#65288;SAiD&#65289;&#39537;&#21160;&#30340;&#35821;&#38899;&#39537;&#21160;&#30340;&#19977;&#32500;&#38754;&#37096;&#21160;&#30011;&#65292;&#36825;&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#22522;&#20110;Transformer&#30340;U-Net&#27169;&#22411;&#65292;&#20855;&#26377;&#38899;&#39057;&#21644;&#35270;&#35273;&#20043;&#38388;&#30340;&#20132;&#21449;&#27169;&#24577;&#23545;&#40784;&#20559;&#24046;&#65292;&#20197;&#22686;&#24378;&#21767;&#37096;&#21516;&#27493;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;BlendVOCA&#65292;&#36825;&#26159;&#19968;&#31181;&#35821;&#38899;&#38899;&#39057;&#21644;&#28151;&#21512;&#24418;&#29366;&#38754;&#37096;&#27169;&#22411;&#21442;&#25968;&#23545;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20197;&#35299;&#20915;&#20844;&#20849;&#36164;&#28304;&#30340;&#32570;&#20047;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#21767;&#37096;&#21516;&#27493;&#26041;&#38754;&#36798;&#21040;&#20102;&#19982;&#22522;&#32447;&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#30830;&#20445;&#20102;&#26356;&#22810;&#26679;&#21270;&#30340;&#21767;&#37096;&#36816;&#21160;&#65292;&#24182;&#31616;&#21270;&#20102;&#21160;&#30011;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech-driven 3D facial animation is challenging due to the scarcity of large-scale visual-audio datasets despite extensive research. Most prior works, typically focused on learning regression models on a small dataset using the method of least squares, encounter difficulties generating diverse lip movements from speech and require substantial effort in refining the generated outputs. To address these issues, we propose a speech-driven 3D facial animation with a diffusion model (SAiD), a lightweight Transformer-based U-Net with a cross-modality alignment bias between audio and visual to enhance lip synchronization. Moreover, we introduce BlendVOCA, a benchmark dataset of pairs of speech audio and parameters of a blendshape facial model, to address the scarcity of public resources. Our experimental results demonstrate that the proposed approach achieves comparable or superior performance in lip synchronization to baselines, ensures more diverse lip movements, and streamlines the animati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25253;&#21578;&#20102;MLCommons&#31185;&#23398;&#24037;&#20316;&#32452;&#22312;&#20113;&#36974;&#25377;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#24037;&#20316;&#65292;&#21253;&#25324;&#23545;&#20113;&#36974;&#25377;&#22522;&#20934;&#27979;&#35797;&#30340;&#21442;&#32771;&#23454;&#29616;&#30340;&#20462;&#25913;&#65292;&#20197;&#23454;&#29616;&#25552;&#21069;&#20572;&#27490;&#12290;</title><link>http://arxiv.org/abs/2401.08636</link><description>&lt;p&gt;
MLCommons&#20113;&#36974;&#25377;&#22522;&#20934;&#27979;&#35797;&#19982;&#25552;&#21069;&#20572;&#27490;
&lt;/p&gt;
&lt;p&gt;
MLCommons Cloud Masking Benchmark with Early Stopping. (arXiv:2401.08636v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08636
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25253;&#21578;&#20102;MLCommons&#31185;&#23398;&#24037;&#20316;&#32452;&#22312;&#20113;&#36974;&#25377;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#24037;&#20316;&#65292;&#21253;&#25324;&#23545;&#20113;&#36974;&#25377;&#22522;&#20934;&#27979;&#35797;&#30340;&#21442;&#32771;&#23454;&#29616;&#30340;&#20462;&#25913;&#65292;&#20197;&#23454;&#29616;&#25552;&#21069;&#20572;&#27490;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25253;&#21578;&#20102;MLCommons&#31185;&#23398;&#24037;&#20316;&#32452;&#22312;&#20113;&#36974;&#25377;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#24037;&#20316;&#12290; MLCommons&#26159;&#19968;&#20010;&#32852;&#30431;&#65292;&#24320;&#21457;&#21644;&#32500;&#25252;&#20960;&#20010;&#31185;&#23398;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#20419;&#36827;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#12290;&#36825;&#20123;&#22522;&#20934;&#27979;&#35797;&#22312;&#32445;&#32422;&#22823;&#23398;&#21644;&#24343;&#21513;&#23612;&#20122;&#22823;&#23398;&#30340;&#39640;&#24615;&#33021;&#35745;&#31639;&#65288;HPC&#65289;&#38598;&#32676;&#20197;&#21450;&#26222;&#36890;&#26700;&#38754;&#19978;&#36827;&#34892;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20113;&#36974;&#25377;&#22522;&#20934;&#27979;&#35797;&#30340;&#25551;&#36848;&#65292;&#24182;&#23545;&#25105;&#20204;&#22312;MLCommons&#22522;&#20934;&#27979;&#35797;&#23454;&#39564;&#20013;&#30340;&#25552;&#20132;&#36827;&#34892;&#20102;&#24635;&#32467;&#12290;&#23427;&#21253;&#25324;&#23545;&#20113;&#36974;&#25377;&#22522;&#20934;&#27979;&#35797;&#30340;&#21442;&#32771;&#23454;&#29616;&#30340;&#20462;&#25913;&#65292;&#20197;&#23454;&#29616;&#25552;&#21069;&#20572;&#27490;&#12290;&#35813;&#22522;&#20934;&#27979;&#35797;&#36890;&#36807;&#33258;&#23450;&#20041;&#25209;&#22788;&#29702;&#33050;&#26412;&#22312;&#32445;&#32422;&#22823;&#23398;&#30340;HPC&#19978;&#25191;&#34892;&#65292;&#35813;&#25209;&#22788;&#29702;&#33050;&#26412;&#36890;&#36807;&#25209;&#22788;&#29702;&#38431;&#21015;&#31995;&#32479;&#36816;&#34892;&#21508;&#31181;&#23454;&#39564;&#65292;&#24182;&#20801;&#35768;&#23545;&#35757;&#32451;&#36718;&#25968;&#36827;&#34892;&#21464;&#21270;&#12290;&#25105;&#20204;&#30340;&#25552;&#20132;&#21253;&#25324;&#20462;&#25913;&#21518;&#30340;&#20195;&#30721;&#65292;&#33258;&#23450;&#20041;&#30340;&#25209;&#22788;&#29702;&#33050;&#26412;&#26469;&#20462;&#25913;&#35757;&#32451;&#36718;&#25968;&#65292;&#25991;&#26723;&#21644;&#22522;&#20934;&#27979;&#35797;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we report on work performed for the MLCommons Science Working Group on the cloud masking benchmark. MLCommons is a consortium that develops and maintains several scientific benchmarks that aim to benefit developments in AI. The benchmarks are conducted on the High Performance Computing (HPC) Clusters of New York University and University of Virginia, as well as a commodity desktop. We provide a description of the cloud masking benchmark, as well as a summary of our submission to MLCommons on the benchmark experiment we conducted. It includes a modification to the reference implementation of the cloud masking benchmark enabling early stopping. This benchmark is executed on the NYU HPC through a custom batch script that runs the various experiments through the batch queuing system while allowing for variation on the number of epochs trained. Our submission includes the modified code, a custom batch script to modify epochs, documentation, and the benchmark results. We repor
&lt;/p&gt;</description></item><item><title>&#23558;&#36136;&#37327;&#22810;&#26679;&#24615;&#20248;&#21270;&#19982;&#25551;&#36848;&#31526;&#26465;&#20214;&#21152;&#24378;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#20197;&#20811;&#26381;&#36827;&#21270;&#31639;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#22312;&#29983;&#25104;&#26082;&#22810;&#26679;&#21448;&#39640;&#24615;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#38598;&#21512;&#26041;&#38754;&#21462;&#24471;&#25104;&#21151;&#12290;</title><link>http://arxiv.org/abs/2401.08632</link><description>&lt;p&gt;
&#23558;&#36136;&#37327;&#22810;&#26679;&#24615;&#19982;&#25551;&#36848;&#31526;&#26465;&#20214;&#21152;&#24378;&#23398;&#20064;&#30456;&#32467;&#21512;
&lt;/p&gt;
&lt;p&gt;
Synergizing Quality-Diversity with Descriptor-Conditioned Reinforcement Learning. (arXiv:2401.08632v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08632
&lt;/p&gt;
&lt;p&gt;
&#23558;&#36136;&#37327;&#22810;&#26679;&#24615;&#20248;&#21270;&#19982;&#25551;&#36848;&#31526;&#26465;&#20214;&#21152;&#24378;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#20197;&#20811;&#26381;&#36827;&#21270;&#31639;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#22312;&#29983;&#25104;&#26082;&#22810;&#26679;&#21448;&#39640;&#24615;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#38598;&#21512;&#26041;&#38754;&#21462;&#24471;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#30340;&#22522;&#26412;&#29305;&#24449;&#20043;&#19968;&#26159;&#25214;&#21040;&#26032;&#39062;&#21644;&#26377;&#21019;&#36896;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#35299;&#20915;&#32473;&#23450;&#30340;&#25361;&#25112;&#25110;&#36866;&#24212;&#26410;&#39044;&#26009;&#21040;&#30340;&#24773;&#20917;&#12290;&#36136;&#37327;&#22810;&#26679;&#24615;&#20248;&#21270;&#26159;&#19968;&#31867;&#36827;&#21270;&#31639;&#27861;&#65292;&#21487;&#20197;&#29983;&#25104;&#26082;&#22810;&#26679;&#21448;&#39640;&#24615;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#38598;&#21512;&#12290;&#20854;&#20013;&#65292;MAP-Elites&#26159;&#19968;&#20010;&#33879;&#21517;&#30340;&#20363;&#23376;&#65292;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#21508;&#31181;&#39046;&#22495;&#65292;&#21253;&#25324;&#36827;&#21270;&#26426;&#22120;&#20154;&#23398;&#12290;&#28982;&#32780;&#65292;MAP-Elites&#36890;&#36807;&#36951;&#20256;&#31639;&#27861;&#30340;&#38543;&#26426;&#31361;&#21464;&#36827;&#34892;&#21457;&#25955;&#25628;&#32034;&#65292;&#22240;&#27492;&#20165;&#38480;&#20110;&#36827;&#21270;&#20302;&#32500;&#35299;&#20915;&#26041;&#26696;&#30340;&#31181;&#32676;&#12290;PGA-MAP-Elites&#36890;&#36807;&#21463;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21551;&#21457;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#21464;&#24322;&#31639;&#23376;&#20811;&#26381;&#20102;&#36825;&#19968;&#38480;&#21046;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#36827;&#21270;&#12290;&#23613;&#31649;&#22312;&#35768;&#22810;&#29615;&#22659;&#20013;&#24615;&#33021;&#20248;&#31168;&#65292;&#20294;PGA-MAP-Elites&#22312;&#19968;&#20123;&#20219;&#21153;&#20013;&#22833;&#36133;&#65292;&#20854;&#20013;&#22522;&#20110;&#26799;&#24230;&#30340;&#21464;&#24322;&#31639;&#23376;&#30340;&#25910;&#25947;&#25628;&#32034;&#38459;&#30861;&#20102;&#22810;&#26679;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;...
&lt;/p&gt;
&lt;p&gt;
A fundamental trait of intelligence involves finding novel and creative solutions to address a given challenge or to adapt to unforeseen situations. Reflecting this, Quality-Diversity optimization is a family of Evolutionary Algorithms, that generates collections of both diverse and high-performing solutions. Among these, MAP-Elites is a prominent example, that has been successfully applied to a variety of domains, including evolutionary robotics. However, MAP-Elites performs a divergent search with random mutations originating from Genetic Algorithms, and thus, is limited to evolving populations of low-dimensional solutions. PGA-MAP-Elites overcomes this limitation using a gradient-based variation operator inspired by deep reinforcement learning which enables the evolution of large neural networks. Although high-performing in many environments, PGA-MAP-Elites fails on several tasks where the convergent search of the gradient-based variation operator hinders diversity. In this work, we
&lt;/p&gt;</description></item><item><title>Wake-Sleep Consolidated Learning&#65288;WSCL&#65289;&#26159;&#19968;&#31181;&#20511;&#37492;&#20154;&#33041;&#35273;&#37266;-&#30561;&#30496;&#38454;&#27573;&#30340;&#23398;&#20064;&#31574;&#30053;&#65292;&#29992;&#20110;&#25913;&#36827;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#35270;&#35273;&#20998;&#31867;&#20219;&#21153;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35273;&#37266;&#21644;&#30561;&#30496;&#38454;&#27573;&#20043;&#38388;&#30340;&#21516;&#27493;&#23398;&#20064;&#26469;&#23454;&#29616;&#25345;&#32493;&#23398;&#20064;&#65292;&#35273;&#37266;&#38454;&#27573;&#20013;&#27169;&#22411;&#36866;&#24212;&#24863;&#23448;&#36755;&#20837;&#24182;&#21033;&#29992;&#21160;&#24577;&#21442;&#25968;&#20923;&#32467;&#26426;&#21046;&#20445;&#25345;&#31283;&#23450;&#65292;&#30561;&#30496;&#38454;&#27573;&#26681;&#25454;NREM&#21644;REM&#38454;&#27573;&#23545;&#27169;&#22411;&#30340;&#31361;&#35302;&#26435;&#37325;&#36827;&#34892;&#24041;&#22266;&#21644;&#35843;&#25972;&#65292;&#24378;&#21270;&#37325;&#35201;&#36830;&#25509;&#24182;&#21066;&#24369;&#19981;&#37325;&#35201;&#30340;&#36830;&#25509;&#12290;</title><link>http://arxiv.org/abs/2401.08623</link><description>&lt;p&gt;
Wake-Sleep Consolidated Learning&#65288;WSCL&#65289;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Wake-Sleep Consolidated Learning. (arXiv:2401.08623v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08623
&lt;/p&gt;
&lt;p&gt;
Wake-Sleep Consolidated Learning&#65288;WSCL&#65289;&#26159;&#19968;&#31181;&#20511;&#37492;&#20154;&#33041;&#35273;&#37266;-&#30561;&#30496;&#38454;&#27573;&#30340;&#23398;&#20064;&#31574;&#30053;&#65292;&#29992;&#20110;&#25913;&#36827;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#35270;&#35273;&#20998;&#31867;&#20219;&#21153;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35273;&#37266;&#21644;&#30561;&#30496;&#38454;&#27573;&#20043;&#38388;&#30340;&#21516;&#27493;&#23398;&#20064;&#26469;&#23454;&#29616;&#25345;&#32493;&#23398;&#20064;&#65292;&#35273;&#37266;&#38454;&#27573;&#20013;&#27169;&#22411;&#36866;&#24212;&#24863;&#23448;&#36755;&#20837;&#24182;&#21033;&#29992;&#21160;&#24577;&#21442;&#25968;&#20923;&#32467;&#26426;&#21046;&#20445;&#25345;&#31283;&#23450;&#65292;&#30561;&#30496;&#38454;&#27573;&#26681;&#25454;NREM&#21644;REM&#38454;&#27573;&#23545;&#27169;&#22411;&#30340;&#31361;&#35302;&#26435;&#37325;&#36827;&#34892;&#24041;&#22266;&#21644;&#35843;&#25972;&#65292;&#24378;&#21270;&#37325;&#35201;&#36830;&#25509;&#24182;&#21066;&#24369;&#19981;&#37325;&#35201;&#30340;&#36830;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102; Wake-Sleep Consolidated Learning&#65288;WSCL&#65289;&#30340;&#23398;&#20064;&#31574;&#30053;&#65292;&#21033;&#29992;&#20114;&#34917;&#23398;&#20064;&#31995;&#32479;&#29702;&#35770;&#21644;&#20154;&#33041;&#30340;&#35273;&#37266;-&#30561;&#30496;&#38454;&#27573;&#26469;&#25913;&#36827;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#36830;&#32493;&#23398;&#20064;&#35774;&#32622;&#20013;&#30340;&#35270;&#35273;&#20998;&#31867;&#20219;&#21153;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#35273;&#37266;&#21644;&#30561;&#30496;&#38454;&#27573;&#20043;&#38388;&#30340;&#21516;&#27493;&#23398;&#20064;&#26469;&#23454;&#29616;&#25345;&#32493;&#23398;&#20064;&#12290;&#22312;&#35273;&#37266;&#38454;&#27573;&#65292;&#27169;&#22411;&#26292;&#38706;&#20110;&#24863;&#23448;&#36755;&#20837;&#24182;&#35843;&#25972;&#20854;&#34920;&#31034;&#65292;&#36890;&#36807;&#21160;&#24577;&#21442;&#25968;&#20923;&#32467;&#26426;&#21046;&#30830;&#20445;&#31283;&#23450;&#24615;&#65292;&#24182;&#23558;&#24773;&#33410;&#35760;&#24518;&#23384;&#20648;&#22312;&#30701;&#26399;&#20020;&#26102;&#35760;&#24518;&#20013;&#65288;&#31867;&#20284;&#20110;&#28023;&#39532;&#20307;&#20013;&#30340;&#24773;&#20917;&#65289;&#12290;&#22312;&#30561;&#30496;&#38454;&#27573;&#65292;&#35757;&#32451;&#36807;&#31243;&#20998;&#20026;NREM&#21644;REM&#38454;&#27573;&#12290;&#22312;NREM&#38454;&#27573;&#65292;&#27169;&#22411;&#30340;&#31361;&#35302;&#26435;&#37325;&#21033;&#29992;&#26469;&#33258;&#30701;&#26399;&#21644;&#38271;&#26399;&#35760;&#24518;&#30340;&#22238;&#25918;&#26679;&#26412;&#36827;&#34892;&#24041;&#22266;&#65292;&#24182;&#28608;&#27963;&#31361;&#35302;&#21487;&#22609;&#24615;&#26426;&#21046;&#65292;&#24378;&#21270;&#37325;&#35201;&#36830;&#25509;&#24182;&#21066;&#24369;&#19981;&#37325;&#35201;&#30340;&#36830;&#25509;&#12290;&#22312;REM&#38454;&#27573;&#65292;&#27169;&#22411;...
&lt;/p&gt;
&lt;p&gt;
We propose Wake-Sleep Consolidated Learning (WSCL), a learning strategy leveraging Complementary Learning System theory and the wake-sleep phases of the human brain to improve the performance of deep neural networks for visual classification tasks in continual learning settings. Our method learns continually via the synchronization between distinct wake and sleep phases. During the wake phase, the model is exposed to sensory input and adapts its representations, ensuring stability through a dynamic parameter freezing mechanism and storing episodic memories in a short-term temporary memory (similarly to what happens in the hippocampus). During the sleep phase, the training process is split into NREM and REM stages. In the NREM stage, the model's synaptic weights are consolidated using replayed samples from the short-term and long-term memory and the synaptic plasticity mechanism is activated, strengthening important connections and weakening unimportant ones. In the REM stage, the model
&lt;/p&gt;</description></item><item><title>MATE-Pred&#26159;&#19968;&#31181;&#22810;&#27169;&#24577;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;TCR-Epitope&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#22120;&#65292;&#36890;&#36807;&#25972;&#21512;&#36827;&#21270;&#29305;&#24449;&#21644;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20934;&#30830;&#39044;&#27979;T&#32454;&#32990;&#21463;&#20307;&#21644;&#34920;&#20301;&#30340;&#32467;&#21512;&#20146;&#21644;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.08619</link><description>&lt;p&gt;
MATE-Pred: &#22810;&#27169;&#24577;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;TCR-Epitope&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
MATE-Pred: Multimodal Attention-based TCR-Epitope interaction Predictor. (arXiv:2401.08619v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08619
&lt;/p&gt;
&lt;p&gt;
MATE-Pred&#26159;&#19968;&#31181;&#22810;&#27169;&#24577;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;TCR-Epitope&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#22120;&#65292;&#36890;&#36807;&#25972;&#21512;&#36827;&#21270;&#29305;&#24449;&#21644;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20934;&#30830;&#39044;&#27979;T&#32454;&#32990;&#21463;&#20307;&#21644;&#34920;&#20301;&#30340;&#32467;&#21512;&#20146;&#21644;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#22320;&#39044;&#27979;T&#32454;&#32990;&#21463;&#20307;&#21644;&#34920;&#20301;&#20043;&#38388;&#30340;&#32467;&#21512;&#20146;&#21644;&#21147;&#23545;&#20110;&#24320;&#21457;&#25104;&#21151;&#30340;&#20813;&#30123;&#30103;&#27861;&#31574;&#30053;&#33267;&#20851;&#37325;&#35201;&#12290;&#19968;&#20123;&#26368;&#20808;&#36827;&#30340;&#35745;&#31639;&#26041;&#27861;&#36890;&#36807;&#23558;&#32454;&#32990;&#21463;&#20307;&#21644;&#34920;&#20301;&#24207;&#21015;&#30340;&#27688;&#22522;&#37240;&#27531;&#22522;&#36716;&#21270;&#20026;&#25968;&#20540;&#26469;&#23454;&#29616;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#24182;&#38598;&#25104;&#20102;&#36827;&#21270;&#29305;&#24449;&#65292;&#32780;&#21478;&#19968;&#20123;&#26041;&#27861;&#21017;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#27688;&#22522;&#37240;&#27531;&#22522;&#23618;&#38754;&#19978;&#24635;&#32467;&#23884;&#20837;&#21521;&#37327;&#20197;&#33719;&#21462;&#24207;&#21015;&#32423;&#34920;&#31034;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#24230;&#21487;&#38752;&#30340;&#26032;&#26041;&#27861;MATE-Pred&#65292;&#23427;&#36890;&#36807;&#22810;&#27169;&#24577;&#27880;&#24847;&#21147;&#36827;&#34892;T&#32454;&#32990;&#21463;&#20307;&#21644;&#34920;&#20301;&#32467;&#21512;&#20146;&#21644;&#21147;&#30340;&#39044;&#27979;&#12290;MATE-Pred&#19982;&#20854;&#20182;&#21033;&#29992;T&#32454;&#32990;&#21463;&#20307;&#21644;&#34920;&#20301;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#21644;&#22522;&#20934;&#27979;&#35797;&#12290;&#22312;&#25552;&#20986;&#30340;&#26041;&#27861;&#20013;&#65292;&#34507;&#30333;&#36136;&#30340;&#25991;&#26412;&#34920;&#31034;&#20197;&#39044;&#35757;&#32451;&#30340;&#21452;&#21521;&#32534;&#30721;&#22120;&#27169;&#22411;&#36827;&#34892;&#23884;&#20837;&#65292;&#24182;&#19982;&#20004;&#31181;&#38468;&#21152;&#29305;&#24449;&#36827;&#34892;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
An accurate binding affinity prediction between T-cell receptors and epitopes contributes decisively to develop successful immunotherapy strategies. Some state-of-the-art computational methods implement deep learning techniques by integrating evolutionary features to convert the amino acid residues of cell receptors and epitope sequences into numerical values, while some other methods employ pre-trained language models to summarize the embedding vectors at the amino acid residue level to obtain sequence-wise representations.  Here, we propose a highly reliable novel method, MATE-Pred, that performs multi-modal attention-based prediction of T-cell receptors and epitopes binding affinity. The MATE-Pred is compared and benchmarked with other deep learning models that leverage multi-modal representations of T-cell receptors and epitopes. In the proposed method, the textual representation of proteins is embedded with a pre-trained bi-directional encoder model and combined with two additiona
&lt;/p&gt;</description></item><item><title>SAM4UDASS&#26159;&#19968;&#31181;&#22312;&#33258;&#35757;&#32451;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#35821;&#20041;&#20998;&#21106;&#26041;&#27861;&#20013;&#20351;&#29992;Segment Anything Model&#65288;SAM&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#29983;&#25104;&#31934;&#30830;&#20266;&#26631;&#31614;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.08604</link><description>&lt;p&gt;
&#24403;SAM&#36935;&#35265;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#35821;&#20041;&#20998;&#21106;&#22312;&#26234;&#33021;&#36710;&#36742;&#20013;
&lt;/p&gt;
&lt;p&gt;
SAM4UDASS: When SAM Meets Unsupervised Domain Adaptive Semantic Segmentation in Intelligent Vehicles. (arXiv:2401.08604v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08604
&lt;/p&gt;
&lt;p&gt;
SAM4UDASS&#26159;&#19968;&#31181;&#22312;&#33258;&#35757;&#32451;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#35821;&#20041;&#20998;&#21106;&#26041;&#27861;&#20013;&#20351;&#29992;Segment Anything Model&#65288;SAM&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#29983;&#25104;&#31934;&#30830;&#20266;&#26631;&#31614;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#20998;&#21106;&#22312;&#20351;&#26234;&#33021;&#36710;&#36742;&#29702;&#35299;&#20854;&#21608;&#22260;&#29615;&#22659;&#26041;&#38754;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#36890;&#24120;&#22312;&#39046;&#22495;&#36716;&#31227;&#22330;&#26223;&#20013;&#34920;&#29616;&#19981;&#20339;&#65292;&#21407;&#22240;&#26159;&#32570;&#20047;&#29992;&#20110;&#35757;&#32451;&#30340;&#26631;&#35760;&#25968;&#25454;&#12290;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;UDA&#65289;&#25216;&#26415;&#24050;&#32463;&#20986;&#29616;&#65292;&#20197;&#24357;&#21512;&#19981;&#21516;&#39550;&#39542;&#22330;&#26223;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#25552;&#39640;&#26080;&#26631;&#27880;&#30446;&#26631;&#29615;&#22659;&#19978;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;&#23613;&#31649;&#33258;&#35757;&#32451;UDA&#26041;&#27861;&#24050;&#32463;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#20294;&#31934;&#30830;&#20266;&#26631;&#31614;&#30340;&#29983;&#25104;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#36825;&#20123;&#20266;&#26631;&#31614;&#24448;&#24448;&#20559;&#21521;&#20110;&#22810;&#25968;&#31867;&#21035;&#65292;&#20174;&#32780;&#29306;&#29298;&#20102;&#32597;&#35265;&#31867;&#21035;&#25110;&#31867;&#20284;&#20132;&#36890;&#28783;&#21644;&#26631;&#24535;&#31561;&#23567;&#29289;&#20307;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;SAM4UDASS&#65292;&#19968;&#31181;&#23558;Segment Anything Model&#65288;SAM&#65289;&#32467;&#21512;&#21040;&#33258;&#35757;&#32451;UDA&#26041;&#27861;&#20013;&#20197;&#25552;&#21319;&#20266;&#26631;&#31614;&#30340;&#26032;&#26041;&#27861;&#12290;&#23427;&#28041;&#21450;&#35821;&#20041;&#24341;&#23548;&#30340;&#33945;&#29256;&#26631;&#31614;&#65292;&#36890;&#36807;&#32473;&#26080;&#26631;&#31614;&#30340;SAM&#33945;&#29256;&#20998;&#37197;&#35821;&#20041;&#26631;&#31614;&#26469;&#32454;&#21270;&#20266;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic segmentation plays a critical role in enabling intelligent vehicles to comprehend their surrounding environments. However, deep learning-based methods usually perform poorly in domain shift scenarios due to the lack of labeled data for training. Unsupervised domain adaptation (UDA) techniques have emerged to bridge the gap across different driving scenes and enhance model performance on unlabeled target environments. Although self-training UDA methods have achieved state-of-the-art results, the challenge of generating precise pseudo-labels persists. These pseudo-labels tend to favor majority classes, consequently sacrificing the performance of rare classes or small objects like traffic lights and signs. To address this challenge, we introduce SAM4UDASS, a novel approach that incorporates the Segment Anything Model (SAM) into self-training UDA methods for refining pseudo-labels. It involves Semantic-Guided Mask Labeling, which assigns semantic labels to unlabeled SAM masks usin
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#25913;&#36827;&#30340;DPC&#31639;&#27861;&#30340;&#33258;&#21160;&#28857;&#20113;&#25968;&#25454;&#20998;&#21106;&#21644;&#19977;&#32500;&#37325;&#24314;&#26041;&#27861;&#65292;&#36890;&#36807;&#35745;&#31639;&#20113;&#28857;&#20013;&#27599;&#20010;&#28857;&#30340;&#30456;&#23545;&#22352;&#26631;&#65292;&#20351;&#29992;&#38598;&#21512;&#31639;&#27861;&#20998;&#26512;&#20998;&#21106;&#32447;&#25968;&#37327;&#65292;&#28982;&#21518;&#20351;&#29992;&#25311;&#21512;&#26041;&#27861;&#24471;&#21040;&#28165;&#26224;&#30340;&#20998;&#30028;&#32447;&#12290;&#20197;&#19979;&#26159;&#35813;&#35770;&#25991;&#30340;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2401.08587</link><description>&lt;p&gt;
&#22522;&#20110;&#25913;&#36827;&#30340;DPC&#31639;&#27861;&#30340;&#28857;&#20113;&#25968;&#25454;&#20998;&#21106;&#21644;&#19977;&#32500;&#37325;&#24314;split wire&#30340;&#33258;&#21160;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Automatic extraction and 3D reconstruction of split wire from point cloud data based on improved DPC algorithm. (arXiv:2401.08587v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08587
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#25913;&#36827;&#30340;DPC&#31639;&#27861;&#30340;&#33258;&#21160;&#28857;&#20113;&#25968;&#25454;&#20998;&#21106;&#21644;&#19977;&#32500;&#37325;&#24314;&#26041;&#27861;&#65292;&#36890;&#36807;&#35745;&#31639;&#20113;&#28857;&#20013;&#27599;&#20010;&#28857;&#30340;&#30456;&#23545;&#22352;&#26631;&#65292;&#20351;&#29992;&#38598;&#21512;&#31639;&#27861;&#20998;&#26512;&#20998;&#21106;&#32447;&#25968;&#37327;&#65292;&#28982;&#21518;&#20351;&#29992;&#25311;&#21512;&#26041;&#27861;&#24471;&#21040;&#28165;&#26224;&#30340;&#20998;&#30028;&#32447;&#12290;&#20197;&#19979;&#26159;&#35813;&#35770;&#25991;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35299;&#20915;DPC&#31639;&#27861;&#25913;&#36827;&#30340;&#28857;&#20113;&#25968;&#25454;&#20998;&#21106;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20851;&#20110;&#28857;&#20113;&#25968;&#25454;&#20998;&#21106;&#32447;&#30340;&#33258;&#21160;&#20998;&#31163;&#21644;&#19977;&#32500;&#37325;&#24314;&#30340;&#30740;&#31350;&#12290;&#39318;&#20808;&#35745;&#31639;&#20113;&#28857;&#20013;&#27599;&#20010;&#28857;&#30340;&#30456;&#23545;&#22352;&#26631;&#12290;&#20854;&#27425;&#65292;&#35745;&#21010;&#37319;&#29992;&#22522;&#20110;&#30456;&#23545;&#38598;&#21512;&#30340;DPC&#32676;&#31639;&#27861;&#65292;&#20998;&#26512;&#20998;&#21106;&#32447;&#30340;&#25968;&#37327;&#20197;&#30830;&#23450;&#20113;&#20869;&#23481;&#20013;&#30340;&#25152;&#26377;&#37096;&#20998;&#12290;&#26368;&#21518;&#65292;&#20351;&#29992;&#26368;&#23567;&#20108;&#20056;&#27861;&#25311;&#21512;&#27599;&#20010;&#20998;&#21106;&#32447;&#12290;&#26368;&#32456;&#24471;&#21040;&#30340;&#20998;&#21106;&#23376;&#23548;&#32447;&#30340;&#20113;&#28857;&#20855;&#26377;&#28165;&#26224;&#30340;&#20998;&#30028;&#32447;&#65292;&#30456;&#37051;&#20998;&#21106;&#23376;&#23548;&#32447;&#20043;&#38388;&#30340;&#36317;&#31163;&#20026;0.45 m&#65292;&#30001;&#27491;&#26041;&#24418;&#30340;&#22235;&#20010;&#39030;&#28857;&#21010;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
In order to solve the problem of point cloud data splitting improved by DPC algorithm, a research on automatic separation and 3D reconstruction of point cloud data split lines is proposed. First, the relative coordinates of each point in the cloud point are calculated. Second, it is planned to develop a relative ensemble-based DPC swarm algorithm for analyzing the number of separation lines to determine all parts in the cloud content. Finally, fit each separator using the least squares method. iron. The cloud point of the resulting split subconductors has a clear demarcation line, and the distance between adjacent split subconductors is 0.45 m, divided by the four vertices of the square.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#24314;&#27169;&#26694;&#26550;&#65292;&#21487;&#20197;&#20351;&#29992;&#33539;&#30068;&#35770;&#30340;&#25512;&#24191;&#26469;&#24314;&#31435;&#32467;&#26500;&#21270;&#27010;&#24565;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#33258;&#21160;&#23398;&#20064;&#20174;&#25968;&#25454;&#20013;&#33719;&#21462;&#27010;&#24565;&#34920;&#31034;&#12290;&#20351;&#29992;&#33539;&#30068;&#35770;&#21644;&#24358;&#22270;&#25551;&#36848;&#37327;&#23376;&#36807;&#31243;&#26377;&#21161;&#20110;&#25581;&#31034;&#35813;&#26041;&#27861;&#30340;&#20851;&#38190;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2401.08585</link><description>&lt;p&gt;
&#20174;&#27010;&#24565;&#31354;&#38388;&#21040;&#37327;&#23376;&#27010;&#24565;&#65306;&#24418;&#24335;&#21270;&#21644;&#23398;&#20064;&#32467;&#26500;&#21270;&#27010;&#24565;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
From Conceptual Spaces to Quantum Concepts: Formalising and Learning Structured Conceptual Models. (arXiv:2401.08585v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08585
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#24314;&#27169;&#26694;&#26550;&#65292;&#21487;&#20197;&#20351;&#29992;&#33539;&#30068;&#35770;&#30340;&#25512;&#24191;&#26469;&#24314;&#31435;&#32467;&#26500;&#21270;&#27010;&#24565;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#33258;&#21160;&#23398;&#20064;&#20174;&#25968;&#25454;&#20013;&#33719;&#21462;&#27010;&#24565;&#34920;&#31034;&#12290;&#20351;&#29992;&#33539;&#30068;&#35770;&#21644;&#24358;&#22270;&#25551;&#36848;&#37327;&#23376;&#36807;&#31243;&#26377;&#21161;&#20110;&#25581;&#31034;&#35813;&#26041;&#27861;&#30340;&#20851;&#38190;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#24314;&#27169;&#26694;&#26550;&#65292;&#20351;&#29992;&#27010;&#24565;&#31354;&#38388;&#30340;&#33539;&#30068;&#35770;&#25512;&#24191;&#26469;&#24314;&#31435;&#32467;&#26500;&#21270;&#27010;&#24565;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#20004;&#31181;&#38750;&#24120;&#19981;&#21516;&#30340;&#23454;&#20363;&#65288;&#32463;&#20856;&#21644;&#37327;&#23376;&#65289;&#33258;&#21160;&#22320;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#27010;&#24565;&#34920;&#31034;&#12290;&#35813;&#24037;&#20316;&#30340;&#19968;&#20010;&#36129;&#29486;&#26159;&#23545;&#25105;&#20204;&#26694;&#26550;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#33539;&#30068;&#35770;&#24418;&#24335;&#21270;&#12290;&#25105;&#20204;&#22768;&#31216;&#33539;&#30068;&#35770;&#30340;&#20351;&#29992;&#65292;&#29305;&#21035;&#26159;&#20351;&#29992;&#24358;&#22270;&#26469;&#25551;&#36848;&#37327;&#23376;&#36807;&#31243;&#65292;&#26377;&#21161;&#20110;&#38416;&#26126;&#25105;&#20204;&#26041;&#27861;&#30340;&#19968;&#20123;&#26368;&#37325;&#35201;&#30340;&#29305;&#28857;&#12290;&#25105;&#20204;&#24314;&#31435;&#22312;Gardenfors&#30340;&#27010;&#24565;&#31354;&#38388;&#30340;&#32463;&#20856;&#26694;&#26550;&#20043;&#19978;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#20351;&#29992;&#20984;&#31354;&#38388;&#20960;&#20309;&#22320;&#24314;&#27169;&#35748;&#30693;&#65292;&#24182;&#36890;&#36807;&#31216;&#20026;&#22495;&#30340;&#31616;&#21333;&#31354;&#38388;&#20998;&#35299;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20174;&#31616;&#21333;&#24418;&#29366;&#30340;&#22270;&#20687;&#20013;&#23398;&#20064;&#24418;&#29366;&#12289;&#39068;&#33394;&#12289;&#22823;&#23567;&#21644;&#20301;&#32622;&#31561;&#22495;&#30340;&#27010;&#24565;&#65292;&#20854;&#20013;&#27010;&#24565;&#22312;&#32463;&#20856;&#23454;&#29616;&#20013;&#34987;&#34920;&#31034;&#20026;&#39640;&#26031;&#20998;&#24067;&#65292;&#32780;&#22312;&#37327;&#23376;&#23454;&#29616;&#20013;&#34987;&#34920;&#31034;&#20026;&#37327;&#23376;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this article we present a new modelling framework for structured concepts using a category-theoretic generalisation of conceptual spaces, and show how the conceptual representations can be learned automatically from data, using two very different instantiations: one classical and one quantum. A contribution of the work is a thorough category-theoretic formalisation of our framework. We claim that the use of category theory, and in particular the use of string diagrams to describe quantum processes, helps elucidate some of the most important features of our approach. We build upon Gardenfors' classical framework of conceptual spaces, in which cognition is modelled geometrically through the use of convex spaces, which in turn factorise in terms of simpler spaces called domains. We show how concepts from the domains of shape, colour, size and position can be learned from images of simple shapes, where concepts are represented as Gaussians in the classical implementation, and quantum ef
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36719;&#20214;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#25216;&#26415;&#30340;&#20840;&#33258;&#21160;&#25163;&#26415;&#26041;&#27861;&#65292;&#33021;&#22815;&#33258;&#21160;&#35786;&#26029;&#21644;&#27835;&#30103;&#23396;&#31435;&#24615;&#21365;&#24034;&#23376;&#23467;&#20869;&#33180;&#24322;&#20301;&#30151;&#12290;</title><link>http://arxiv.org/abs/2401.08584</link><description>&lt;p&gt;
Nahid: &#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#30340;&#23436;&#20840;&#33258;&#21160;&#25163;&#26415;
&lt;/p&gt;
&lt;p&gt;
Nahid: AI-based Algorithm for operating fully-automatic surgery. (arXiv:2401.08584v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08584
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36719;&#20214;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#25216;&#26415;&#30340;&#20840;&#33258;&#21160;&#25163;&#26415;&#26041;&#27861;&#65292;&#33021;&#22815;&#33258;&#21160;&#35786;&#26029;&#21644;&#27835;&#30103;&#23396;&#31435;&#24615;&#21365;&#24034;&#23376;&#23467;&#20869;&#33180;&#24322;&#20301;&#30151;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36719;&#20214;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#25216;&#26415;&#30340;&#20840;&#33258;&#21160;&#25163;&#26415;&#26041;&#27861;&#12290;&#28982;&#21518;&#65292;&#23545;&#21307;&#30103;&#25163;&#26415;&#30340;&#35745;&#31639;&#26426;&#21270;&#20248;&#21183;&#21644;&#25361;&#25112;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#26368;&#21518;&#65292;&#23545;&#23396;&#31435;&#24615;&#21365;&#24034;&#23376;&#23467;&#20869;&#33180;&#24322;&#20301;&#30151;&#30456;&#20851;&#30340;&#25163;&#26415;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#22522;&#20110;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#35814;&#32454;&#30340;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#25163;&#26415;&#36807;&#31243;&#20013;&#33258;&#21160;&#35786;&#26029;&#21644;&#27835;&#30103;&#35813;&#30142;&#30149;&#65292;&#20854;&#20013;&#20351;&#29992;&#20102;&#19968;&#20010;U-net&#27169;&#22411;&#26469;&#26816;&#27979;&#25163;&#26415;&#36807;&#31243;&#20013;&#30340;&#23376;&#23467;&#20869;&#33180;&#24322;&#20301;&#30151;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, for the first time, a method is presented that can provide a fully automated surgery based on software and computer vision techniques. Then, the advantages and challenges of computerization of medical surgery are examined. Finally, the surgery related to isolated ovarian endometriosis disease has been examined, and based on the presented method, a more detailed algorithm is presented that is capable of automatically diagnosing and treating this disease during surgery as proof of our proposed method where a U-net is trained to detect the endometriosis during surgery.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#30340;&#26102;&#38388;&#23884;&#20837;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#22320;&#29702;&#27963;&#21160;&#30340;&#26102;&#38388;&#27169;&#24335;&#19982;&#22303;&#22320;&#21033;&#29992;&#31867;&#22411;&#30456;&#23545;&#24212;&#12290;&#36890;&#36807;&#23558;&#26102;&#38388;&#24207;&#21015;&#20449;&#21495;&#36716;&#25442;&#21040;&#39057;&#22495;&#24182;&#21387;&#32553;&#20026;&#35821;&#20041;&#20998;&#21106;&#25152;&#38656;&#30340;&#22270;&#20687;&#36890;&#36947;&#65292;&#26102;&#38388;&#23884;&#20837;&#21487;&#20197;&#26377;&#25928;&#22320;&#34920;&#31034;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#24182;&#29992;&#20110;&#22810;&#20010;&#22320;&#29702;&#31354;&#38388;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2401.08581</link><description>&lt;p&gt;
&#26102;&#38388;&#23884;&#20837;&#65306;&#20174;&#26102;&#31354;&#25968;&#25454;&#20013;&#36827;&#34892;&#21487;&#25193;&#23637;&#30340;&#33258;&#30417;&#30563;&#26102;&#24207;&#34920;&#31034;&#23398;&#20064;&#65292;&#29992;&#20110;&#22810;&#27169;&#24577;&#35745;&#31639;&#26426;&#35270;&#35273;
&lt;/p&gt;
&lt;p&gt;
Temporal Embeddings: Scalable Self-Supervised Temporal Representation Learning from Spatiotemporal Data for Multimodal Computer Vision. (arXiv:2401.08581v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08581
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#30340;&#26102;&#38388;&#23884;&#20837;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#22320;&#29702;&#27963;&#21160;&#30340;&#26102;&#38388;&#27169;&#24335;&#19982;&#22303;&#22320;&#21033;&#29992;&#31867;&#22411;&#30456;&#23545;&#24212;&#12290;&#36890;&#36807;&#23558;&#26102;&#38388;&#24207;&#21015;&#20449;&#21495;&#36716;&#25442;&#21040;&#39057;&#22495;&#24182;&#21387;&#32553;&#20026;&#35821;&#20041;&#20998;&#21106;&#25152;&#38656;&#30340;&#22270;&#20687;&#36890;&#36947;&#65292;&#26102;&#38388;&#23884;&#20837;&#21487;&#20197;&#26377;&#25928;&#22320;&#34920;&#31034;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#24182;&#29992;&#20110;&#22810;&#20010;&#22320;&#29702;&#31354;&#38388;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22320;&#29702;&#27963;&#21160;&#30340;&#26102;&#38388;&#27169;&#24335;&#19982;&#22303;&#22320;&#21033;&#29992;&#31867;&#22411;&#20043;&#38388;&#23384;&#22312;&#30456;&#20851;&#24615;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;&#36890;&#36807;&#21387;&#32553;&#33258;&#32534;&#30721;&#22120;&#23558;&#26102;&#38388;&#24207;&#21015;&#20449;&#21495;&#36716;&#25442;&#21040;&#39057;&#22495;&#65292;&#24182;&#23558;&#20854;&#36716;&#25442;&#20026;&#22270;&#20687;&#36890;&#36947;&#65292;&#29992;&#20110;&#19979;&#28216;&#22320;&#29702;&#31354;&#38388;&#20219;&#21153;&#30340;&#28145;&#24230;&#35821;&#20041;&#20998;&#21106;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#26102;&#38388;&#23884;&#20837;&#26159;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#35821;&#20041;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#65292;&#23545;&#20110;&#20998;&#31867;&#20303;&#23429;&#21306;&#21644;&#21830;&#19994;&#21306;&#31561;&#19981;&#21516;&#20219;&#21153;&#20855;&#26377;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
There exists a correlation between geospatial activity temporal patterns and type of land use. A novel self-supervised approach is proposed to stratify landscape based on mobility activity time series. First, the time series signal is transformed to the frequency domain and then compressed into task-agnostic temporal embeddings by a contractive autoencoder, which preserves cyclic temporal patterns observed in time series. The pixel-wise embeddings are converted to image-like channels that can be used for task-based, multimodal modeling of downstream geospatial tasks using deep semantic segmentation. Experiments show that temporal embeddings are semantically meaningful representations of time series data and are effective across different tasks such as classifying residential area and commercial areas. Temporal embeddings transform sequential, spatiotemporal motion trajectory data into semantically meaningful image-like tensor representations that can be combined (multimodal fusion) wit
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26354;&#32447;&#30340;&#31070;&#32463;&#39118;&#26684;&#36801;&#31227;&#26041;&#27861;&#65292;&#36890;&#36807;&#21442;&#25968;&#21270;&#24418;&#29366;&#32534;&#36753;&#21644;&#31934;&#30830;&#30340;&#39118;&#26684;&#25552;&#21462;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#22312;&#22788;&#29702;&#20108;&#36827;&#21046;&#33609;&#22270;&#36716;&#25442;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#20174;&#32780;&#25552;&#21319;&#20102;&#35774;&#35745;&#34920;&#36798;&#33021;&#21147;&#21644;&#39118;&#26684;&#36801;&#31227;&#30340;&#23454;&#36341;&#12290;</title><link>http://arxiv.org/abs/2401.08579</link><description>&lt;p&gt;
&#22522;&#20110;&#26354;&#32447;&#30340;&#31070;&#32463;&#39118;&#26684;&#36801;&#31227;
&lt;/p&gt;
&lt;p&gt;
Curve-based Neural Style Transfer. (arXiv:2401.08579v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08579
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26354;&#32447;&#30340;&#31070;&#32463;&#39118;&#26684;&#36801;&#31227;&#26041;&#27861;&#65292;&#36890;&#36807;&#21442;&#25968;&#21270;&#24418;&#29366;&#32534;&#36753;&#21644;&#31934;&#30830;&#30340;&#39118;&#26684;&#25552;&#21462;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#22312;&#22788;&#29702;&#20108;&#36827;&#21046;&#33609;&#22270;&#36716;&#25442;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#20174;&#32780;&#25552;&#21319;&#20102;&#35774;&#35745;&#34920;&#36798;&#33021;&#21147;&#21644;&#39118;&#26684;&#36801;&#31227;&#30340;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#20026;&#22522;&#20110;&#26354;&#32447;&#35774;&#35745;&#33609;&#22270;&#35774;&#35745;&#30340;&#21442;&#25968;&#21270;&#39118;&#26684;&#36801;&#31227;&#26694;&#26550;&#12290;&#36890;&#36807;&#21033;&#29992;&#21442;&#25968;&#21270;&#24418;&#29366;&#32534;&#36753;&#35268;&#21017;&#12289;&#39640;&#25928;&#30340;&#26354;&#32447;&#21040;&#20687;&#32032;&#36716;&#25442;&#25216;&#26415;&#20197;&#21450;&#22312;ImageNet-Sketch&#19978;&#23545;VGG19&#36827;&#34892;&#24494;&#35843;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#31070;&#32463;&#39118;&#26684;&#36801;&#31227;&#26041;&#27861;&#22312;&#22788;&#29702;&#20108;&#36827;&#21046;&#33609;&#22270;&#36716;&#25442;&#26102;&#38754;&#20020;&#30340;&#20256;&#32479;&#25361;&#25112;&#65292;&#24182;&#25552;&#39640;&#20102;VGG19&#22312;&#29305;&#24449;&#37329;&#23383;&#22612;&#32593;&#32476;&#20013;&#23545;&#39118;&#26684;&#30340;&#31934;&#30830;&#25552;&#21462;&#20316;&#29992;&#12290;&#36890;&#36807;&#23558;&#30452;&#35266;&#30340;&#22522;&#20110;&#26354;&#32447;&#30340;&#22270;&#20687;&#19982;&#22522;&#20110;&#35268;&#21017;&#30340;&#32534;&#36753;&#30456;&#32467;&#21512;&#65292;&#35813;&#30740;&#31350;&#26377;&#28508;&#21147;&#26174;&#33879;&#25552;&#21319;&#35774;&#35745;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#22312;&#20135;&#21697;&#35774;&#35745;&#39046;&#22495;&#20013;&#25512;&#21160;&#39118;&#26684;&#36801;&#31227;&#30340;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research presents a new parametric style transfer framework specifically designed for curve-based design sketches. In this research, traditional challenges faced by neural style transfer methods in handling binary sketch transformations are effectively addressed through the utilization of parametric shape-editing rules, efficient curve-to-pixel conversion techniques, and the fine-tuning of VGG19 on ImageNet-Sketch, enhancing its role as a feature pyramid network for precise style extraction. By harmonizing intuitive curve-based imagery with rule-based editing, this study holds the potential to significantly enhance design articulation and elevate the practice of style transfer within the realm of product design.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;ExFlow&#30340;&#36731;&#37327;&#32423;&#20248;&#21270;&#25216;&#26415;&#65292;&#36890;&#36807;&#21033;&#29992;&#23618;&#38388;&#19987;&#23478;&#20146;&#21644;&#24615;&#65292;&#22823;&#22823;&#21152;&#36895;&#20102;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#25512;&#29702;&#30340;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2401.08383</link><description>&lt;p&gt;
&#21033;&#29992;&#23618;&#38388;&#19987;&#23478;&#20146;&#21644;&#24615;&#21152;&#36895;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Exploiting Inter-Layer Expert Affinity for Accelerating Mixture-of-Experts Model Inference. (arXiv:2401.08383v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08383
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;ExFlow&#30340;&#36731;&#37327;&#32423;&#20248;&#21270;&#25216;&#26415;&#65292;&#36890;&#36807;&#21033;&#29992;&#23618;&#38388;&#19987;&#23478;&#20146;&#21644;&#24615;&#65292;&#22823;&#22823;&#21152;&#36895;&#20102;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#25512;&#29702;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20687;&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#28151;&#21512;&#19987;&#23478;&#33539;&#24335;&#24050;&#32463;&#25104;&#20026;&#22686;&#24378;&#27169;&#22411;&#34920;&#36798;&#33021;&#21147;&#21644;&#20934;&#30830;&#24615;&#30340;&#24378;&#22823;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#23558;GPT MoE&#27169;&#22411;&#37096;&#32626;&#21040;&#20998;&#24067;&#24335;&#31995;&#32479;&#19978;&#36827;&#34892;&#24182;&#34892;&#25512;&#29702;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#19987;&#23478;&#36335;&#30001;&#21644;&#32858;&#21512;&#25152;&#38656;&#30340;&#24191;&#27867;Alltoall&#36890;&#20449;&#12290;&#36825;&#31181;&#36890;&#20449;&#29942;&#39048;&#21152;&#21095;&#20102;&#24050;&#32463;&#22797;&#26434;&#30340;&#35745;&#31639;&#29615;&#22659;&#65292;&#20174;&#32780;&#22952;&#30861;&#20102;&#39640;&#24615;&#33021;&#35745;&#31639;&#36164;&#28304;&#30340;&#39640;&#25928;&#21033;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#20248;&#21270;&#25216;&#26415;ExFlow&#65292;&#20197;&#22823;&#22823;&#21152;&#36895;&#36825;&#20123;MoE&#27169;&#22411;&#30340;&#25512;&#29702;&#12290;&#25105;&#20204;&#20174;&#21033;&#29992;&#23618;&#38388;&#19987;&#23478;&#20146;&#21644;&#24615;&#30340;&#26032;&#35270;&#35282;&#26469;&#20943;&#36731;&#36890;&#20449;&#24320;&#38144;&#12290;&#19982;&#20043;&#21069;&#30340;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#39044;&#35757;&#32451;&#30340;MoE&#27169;&#22411;&#65292;&#26080;&#38656;&#20219;&#20309;&#24494;&#35843;&#25110;&#31934;&#24230;&#19979;&#38477;&#12290;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#19978;&#19979;&#25991;&#36830;&#36143;&#30340;&#19987;&#23478;&#24182;&#34892;&#24615;
&lt;/p&gt;
&lt;p&gt;
In large language models like the Generative Pre-trained Transformer, the Mixture of Experts paradigm has emerged as a powerful technique for enhancing model expressiveness and accuracy. However, deploying GPT MoE models for parallel inference on distributed systems presents significant challenges, primarily due to the extensive Alltoall communication required for expert routing and aggregation. This communication bottleneck exacerbates the already complex computational landscape, hindering the efficient utilization of high-performance computing resources. In this paper, we propose a lightweight optimization technique called ExFlow, to largely accelerate the inference of these MoE models. We take a new perspective on alleviating the communication overhead by exploiting the inter-layer expert affinity. Unlike previous methods, our solution can be directly applied to pre-trained MoE models without any fine-tuning or accuracy degradation. By proposing a context-coherent expert parallelism
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#22810;&#26631;&#31614;&#38899;&#39057;&#20998;&#21106;&#20195;&#29702;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#23558;&#23884;&#20837;&#26144;&#23556;&#21040;&#39057;&#22495;&#65292;&#23454;&#29616;&#20102;&#23545;&#35821;&#38899;&#27963;&#21160;&#12289;&#38899;&#20048;&#12289;&#22122;&#38899;&#21644;&#37325;&#21472;&#35821;&#38899;&#30340;&#21516;&#26102;&#26816;&#27979;&#65292;&#24182;&#19988;&#20855;&#26377;&#24378;&#22823;&#30340;&#21487;&#35299;&#37322;&#24615;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.08268</link><description>&lt;p&gt;
&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#22810;&#26631;&#31614;&#38899;&#39057;&#20998;&#21106;&#20195;&#29702;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
An Explainable Proxy Model for Multiabel Audio Segmentation. (arXiv:2401.08268v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08268
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#22810;&#26631;&#31614;&#38899;&#39057;&#20998;&#21106;&#20195;&#29702;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#23558;&#23884;&#20837;&#26144;&#23556;&#21040;&#39057;&#22495;&#65292;&#23454;&#29616;&#20102;&#23545;&#35821;&#38899;&#27963;&#21160;&#12289;&#38899;&#20048;&#12289;&#22122;&#38899;&#21644;&#37325;&#21472;&#35821;&#38899;&#30340;&#21516;&#26102;&#26816;&#27979;&#65292;&#24182;&#19988;&#20855;&#26377;&#24378;&#22823;&#30340;&#21487;&#35299;&#37322;&#24615;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#39057;&#20449;&#21495;&#20998;&#21106;&#26159;&#33258;&#21160;&#38899;&#39057;&#32034;&#24341;&#30340;&#20851;&#38190;&#20219;&#21153;&#12290;&#23427;&#21253;&#25324;&#22312;&#20449;&#21495;&#20013;&#26816;&#27979;&#31867;&#21516;&#36136;&#29255;&#27573;&#30340;&#36793;&#30028;&#12290;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#26159;&#36879;&#26126;&#20915;&#31574;&#30340;&#37325;&#35201;&#36807;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#22810;&#26631;&#31614;&#20998;&#21106;&#27169;&#22411;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#35821;&#38899;&#27963;&#21160;&#65288;SAD&#65289;&#12289;&#38899;&#20048;&#65288;MD&#65289;&#12289;&#22122;&#38899;&#65288;ND&#65289;&#21644;&#37325;&#21472;&#35821;&#38899;&#26816;&#27979;&#65288;OSD&#65289;&#12290;&#35813;&#20195;&#29702;&#27169;&#22411;&#20351;&#29992;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#65288;NMF&#65289;&#23558;&#29992;&#20110;&#20998;&#21106;&#30340;&#23884;&#20837;&#26144;&#23556;&#21040;&#39057;&#22495;&#12290;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#39044;&#35757;&#32451;&#30340;&#40657;&#30418;&#27169;&#22411;&#30456;&#27604;&#34920;&#29616;&#30456;&#20284;&#65292;&#21516;&#26102;&#26174;&#31034;&#20986;&#24378;&#22823;&#30340;&#21487;&#35299;&#37322;&#24615;&#21151;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20915;&#31574;&#25152;&#20351;&#29992;&#30340;&#39057;&#29575;&#21306;&#38388;&#21487;&#20197;&#22312;&#29255;&#27573;&#32423;&#21035;&#65288;&#23616;&#37096;&#35299;&#37322;&#65289;&#21644;&#20840;&#23616;&#32423;&#21035;&#65288;&#31867;&#21407;&#22411;&#65289;&#19978;&#36731;&#26494;&#35782;&#21035;&#20986;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
Audio signal segmentation is a key task for automatic audio indexing. It consists of detecting the boundaries of class-homogeneous segments in the signal. In many applications, explainable AI is a vital process for transparency of decision-making with machine learning. In this paper, we propose an explainable multilabel segmentation model that solves speech activity (SAD), music (MD), noise (ND), and overlapped speech detection (OSD) simultaneously. This proxy uses the non-negative matrix factorization (NMF) to map the embedding used for the segmentation to the frequency domain. Experiments conducted on two datasets show similar performances as the pre-trained black box model while showing strong explainability features. Specifically, the frequency bins used for the decision can be easily identified at both the segment level (local explanations) and global level (class prototypes).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;Transformer&#27169;&#22411;&#20013;&#23454;&#29616;&#36827;&#20301;&#31639;&#27861;&#30340;&#26041;&#24335;&#65292;&#21457;&#29616;&#22312;&#20004;&#23618;&#30340;&#20165;&#32534;&#30721;&#22120;&#27169;&#22411;&#20013;&#65292;&#31532;&#19968;&#23618;&#36127;&#36131;&#30456;&#21516;&#20301;&#32622;&#30340;&#25968;&#23383;&#30456;&#21152;&#65292;&#31532;&#20108;&#23618;&#26681;&#25454;&#27880;&#24847;&#21147;&#26426;&#21046;&#20915;&#23450;&#26159;&#21542;&#38656;&#35201;&#36827;&#20301;&#65292;&#24182;&#36890;&#36807;&#22810;&#23618;&#24863;&#30693;&#26426;&#26469;&#25191;&#34892;&#36827;&#20301;&#25805;&#20316;&#12290;</title><link>http://arxiv.org/abs/2401.07993</link><description>&lt;p&gt;
&#22312;Transformer&#27169;&#22411;&#20013;&#23454;&#29616;&#36827;&#20301;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Carrying over algorithm in transformers. (arXiv:2401.07993v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.07993
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;Transformer&#27169;&#22411;&#20013;&#23454;&#29616;&#36827;&#20301;&#31639;&#27861;&#30340;&#26041;&#24335;&#65292;&#21457;&#29616;&#22312;&#20004;&#23618;&#30340;&#20165;&#32534;&#30721;&#22120;&#27169;&#22411;&#20013;&#65292;&#31532;&#19968;&#23618;&#36127;&#36131;&#30456;&#21516;&#20301;&#32622;&#30340;&#25968;&#23383;&#30456;&#21152;&#65292;&#31532;&#20108;&#23618;&#26681;&#25454;&#27880;&#24847;&#21147;&#26426;&#21046;&#20915;&#23450;&#26159;&#21542;&#38656;&#35201;&#36827;&#20301;&#65292;&#24182;&#36890;&#36807;&#22810;&#23618;&#24863;&#30693;&#26426;&#26469;&#25191;&#34892;&#36827;&#20301;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21152;&#27861;&#21487;&#33021;&#26159;&#26368;&#31616;&#21333;&#30340;&#31639;&#26415;&#20219;&#21153;&#20043;&#19968;&#65292;&#36890;&#24120;&#20351;&#29992;&#36827;&#20301;&#31639;&#27861;&#36827;&#34892;&#35745;&#31639;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;Transformer&#27169;&#22411;&#22914;&#20309;&#23454;&#29616;&#36825;&#20010;&#31639;&#27861;&#65292;&#20197;&#21450;&#22914;&#20309;&#23558;&#20004;&#20010;&#20219;&#21153;&#20998;&#37197;&#32473;&#32593;&#32476;&#30340;&#19981;&#21516;&#37096;&#20998;&#12290;&#25105;&#20204;&#39318;&#20808;&#20851;&#27880;&#20004;&#23618;&#30340;&#20165;&#32534;&#30721;&#22120;&#27169;&#22411;&#65292;&#21457;&#29616;&#36827;&#20301;&#31639;&#27861;&#20197;&#19968;&#31181;&#27169;&#22359;&#21270;&#30340;&#26041;&#24335;&#23454;&#29616;&#12290;&#31532;&#19968;&#23618;&#20027;&#35201;&#36127;&#36131;&#22312;&#30456;&#21516;&#20301;&#32622;&#19978;&#28155;&#21152;&#25968;&#23383;&#12290;&#31532;&#20108;&#23618;&#39318;&#20808;&#22312;&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#20915;&#23450;&#21738;&#20123;&#20301;&#32622;&#38656;&#35201;&#36827;&#20301;&#65292;&#28982;&#21518;&#22312;&#26368;&#32456;&#30340;&#22810;&#23618;&#24863;&#30693;&#26426;&#20013;&#25191;&#34892;&#36827;&#20301;&#25805;&#20316;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#31934;&#30830;&#23450;&#20301;&#36127;&#36131;&#36825;&#20010;&#20219;&#21153;&#30340;&#31070;&#32463;&#20803;&#12290;&#36827;&#20301;&#31639;&#27861;&#30340;&#36825;&#31181;&#23454;&#29616;&#22312;&#20004;&#23618;&#21644;&#19977;&#23618;&#27169;&#22411;&#30340;&#19968;&#31995;&#21015;&#36229;&#21442;&#25968;&#20013;&#37117;&#23384;&#22312;&#12290;&#23545;&#20110;&#23567;&#22411;&#30340;&#20165;&#35299;&#30721;&#22120;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Addition is perhaps one of the simplest arithmetic tasks one can think of and is usually performed using the carrying over algorithm. This algorithm consists of two tasks: adding digits in the same position and carrying over a one whenever necessary. We study how transformer models implement this algorithm and how the two aforementioned tasks are allocated to different parts of the network. We first focus on two-layer encoder-only models and show that the carrying over algorithm is implemented in a modular fashion. The first layer is mostly responsible for adding digits in the same position. The second layer first decides, in the attention, which positions need a carried one or not, and then performs the carrying of the one in the final MLP. We provide a simple way of precisely identifying which neurons are responsible for that task. This implementation of the carrying over algorithm occurs across a range of hyperparameters for two as well as three-layer models. For small decoder-only 
&lt;/p&gt;</description></item><item><title>E3x&#26159;&#19968;&#31181;&#31616;&#21270;&#20102;$\mathrm{E}(3)$&#31561;&#21464;&#28145;&#24230;&#23398;&#20064;&#30340;&#36719;&#20214;&#21253;&#65292;&#36890;&#36807;&#20869;&#32622;&#31561;&#21464;&#24615;&#23454;&#29616;&#26356;&#39640;&#30340;&#25968;&#25454;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.07595</link><description>&lt;p&gt;
E3x&#65306;&#31616;&#21270;&#30340;$\mathrm{E}(3)$&#31561;&#21464;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
E3x: $\mathrm{E}(3)$-Equivariant Deep Learning Made Easy. (arXiv:2401.07595v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.07595
&lt;/p&gt;
&lt;p&gt;
E3x&#26159;&#19968;&#31181;&#31616;&#21270;&#20102;$\mathrm{E}(3)$&#31561;&#21464;&#28145;&#24230;&#23398;&#20064;&#30340;&#36719;&#20214;&#21253;&#65292;&#36890;&#36807;&#20869;&#32622;&#31561;&#21464;&#24615;&#23454;&#29616;&#26356;&#39640;&#30340;&#25968;&#25454;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;E3x&#65292;&#19968;&#31181;&#29992;&#20110;&#26500;&#24314;&#31070;&#32463;&#32593;&#32476;&#30340;&#36719;&#20214;&#21253;&#65292;&#35813;&#32593;&#32476;&#22312;&#19977;&#32500;&#31354;&#38388;&#30340;&#24179;&#31227;&#12289;&#26059;&#36716;&#21644;&#21453;&#23556;&#26041;&#38754;&#31561;&#21464;&#12290;&#19982;&#26222;&#36890;&#31070;&#32463;&#32593;&#32476;&#30456;&#27604;&#65292;$\mathrm{E}(3)$-&#31561;&#21464;&#27169;&#22411;&#22312;&#36755;&#20837;&#21644;/&#25110;&#36755;&#20986;&#25968;&#25454;&#26159;&#19982;&#19977;&#32500;&#23545;&#35937;&#30456;&#20851;&#30340;&#25968;&#37327;&#26102;&#20855;&#26377;&#20248;&#21183;&#12290;&#36825;&#26159;&#22240;&#20026;&#27492;&#31867;&#25968;&#37327;&#65288;&#20363;&#22914;&#20301;&#32622;&#65289;&#30340;&#25968;&#20540;&#36890;&#24120;&#21462;&#20915;&#20110;&#25152;&#36873;&#25321;&#30340;&#22352;&#26631;&#31995;&#32479;&#12290;&#22312;&#21442;&#32771;&#31995;&#30340;&#21464;&#25442;&#19979;&#65292;&#36825;&#20123;&#20540;&#20250;&#21487;&#39044;&#27979;&#22320;&#21457;&#29983;&#21464;&#21270;&#65292;&#20294;&#23545;&#20110;&#26222;&#36890;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#35828;&#65292;&#23398;&#20064;&#20854;&#28508;&#22312;&#35268;&#21017;&#21487;&#33021;&#24456;&#22256;&#38590;&#12290;&#20351;&#29992;&#20869;&#32622;&#30340;$\mathrm{E}(3)$-&#31561;&#21464;&#24615;&#65292;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#20445;&#35777;&#23436;&#20840;&#28385;&#36275;&#30456;&#20851;&#30340;&#21464;&#25442;&#35268;&#21017;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#39640;&#30340;&#25968;&#25454;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;E3x&#30340;&#20195;&#30721;&#21487;&#20174;https://github.com/google-research/e3x&#33719;&#24471;&#65292;&#36824;&#25552;&#20379;&#20102;&#35814;&#32454;&#30340;&#25991;&#26723;&#21644;&#20351;&#29992;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work introduces E3x, a software package for building neural networks that are equivariant with respect to the Euclidean group $\mathrm{E}(3)$, consisting of translations, rotations, and reflections of three-dimensional space. Compared to ordinary neural networks, $\mathrm{E}(3)$-equivariant models promise benefits whenever input and/or output data are quantities associated with three-dimensional objects. This is because the numeric values of such quantities (e.g. positions) typically depend on the chosen coordinate system. Under transformations of the reference frame, the values change predictably, but the underlying rules can be difficult to learn for ordinary machine learning models. With built-in $\mathrm{E}(3)$-equivariance, neural networks are guaranteed to satisfy the relevant transformation rules exactly, resulting in superior data efficiency and accuracy. The code for E3x is available from https://github.com/google-research/e3x, detailed documentation and usage examples ca
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;AGI&#65288;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65289;&#30340;&#24187;&#35273;&#29616;&#35937;&#65292;&#24635;&#32467;&#20102;&#24403;&#21069;&#23545;AGI&#24187;&#35273;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2401.06792</link><description>&lt;p&gt;
LightHouse: AGI&#24187;&#35273;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
LightHouse: A Survey of AGI Hallucination. (arXiv:2401.06792v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06792
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;AGI&#65288;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65289;&#30340;&#24187;&#35273;&#29616;&#35937;&#65292;&#24635;&#32467;&#20102;&#24403;&#21069;&#23545;AGI&#24187;&#35273;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#65292;&#22823;&#35268;&#27169;&#27169;&#22411;&#21464;&#24471;&#36234;&#26469;&#36234;&#26234;&#33021;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#20123;&#22823;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#25104;&#20026;&#20102;&#38459;&#30861;AI&#30740;&#31350;&#21457;&#23637;&#30340;&#29942;&#39048;&#12290;&#20026;&#20102;&#23454;&#29616;&#24378;&#20154;&#24037;&#26234;&#33021;&#65292;&#22823;&#37327;&#30340;&#30740;&#31350;&#24037;&#20316;&#27491;&#22312;&#25237;&#20837;&#21040;AGI&#65288;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65289;&#24187;&#35273;&#30740;&#31350;&#20013;&#12290;&#20197;&#24448;&#30340;&#25506;&#32034;&#20027;&#35201;&#38598;&#20013;&#22312;&#30740;&#31350;LLM&#65288;&#22823;&#35821;&#35328;&#27169;&#22411;&#65289;&#20013;&#30340;&#24187;&#35273;&#65292;&#32780;&#23545;&#20110;&#22810;&#27169;&#24577;AGI&#65292;&#24187;&#35273;&#30740;&#31350;&#20173;&#22788;&#20110;&#26089;&#26399;&#38454;&#27573;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25512;&#21160;&#24187;&#35273;&#29616;&#35937;&#39046;&#22495;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;AGI&#24187;&#35273;&#30340;&#24635;&#35272;&#65292;&#24635;&#32467;&#20102;&#24403;&#21069;&#23545;AGI&#24187;&#35273;&#30340;&#30740;&#31350;&#24037;&#20316;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the development of artificial intelligence, large-scale models have become increasingly intelligent. However, numerous studies indicate that hallucinations within these large models are a bottleneck hindering the development of AI research. In the pursuit of achieving strong artificial intelligence, a significant volume of research effort is being invested in the AGI (Artificial General Intelligence) hallucination research. Previous explorations have been conducted in researching hallucinations within LLMs (Large Language Models). As for multimodal AGI, research on hallucinations is still in an early stage. To further the progress of research in the domain of hallucinatory phenomena, we present a bird's eye view of hallucinations in AGI, summarizing the current work on AGI hallucinations and proposing some directions for future research.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#36890;&#29992;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#38382;&#39064;&#65292;&#39318;&#27425;&#23581;&#35797;&#25506;&#32034;&#20102;&#23558;&#23631;&#34109;&#22270;&#20687;&#24314;&#27169;&#24212;&#29992;&#20110;&#39057;&#29575;&#23631;&#34109;&#30340;&#26041;&#27861;&#65292;&#30456;&#36739;&#20110;&#24050;&#26377;&#26041;&#27861;&#65292;&#22312;&#26816;&#27979;&#24615;&#33021;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2401.06506</link><description>&lt;p&gt;
&#39057;&#29575;&#23631;&#34109;&#29992;&#20110;&#36890;&#29992;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Frequency Masking for Universal Deepfake Detection. (arXiv:2401.06506v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06506
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#36890;&#29992;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#38382;&#39064;&#65292;&#39318;&#27425;&#23581;&#35797;&#25506;&#32034;&#20102;&#23558;&#23631;&#34109;&#22270;&#20687;&#24314;&#27169;&#24212;&#29992;&#20110;&#39057;&#29575;&#23631;&#34109;&#30340;&#26041;&#27861;&#65292;&#30456;&#36739;&#20110;&#24050;&#26377;&#26041;&#27861;&#65292;&#22312;&#26816;&#27979;&#24615;&#33021;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#36890;&#29992;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#26816;&#27979;&#19968;&#31995;&#21015;&#29983;&#25104;&#22411;AI&#26041;&#27861;&#20013;&#30340;&#21512;&#25104;&#22270;&#20687;&#65292;&#23588;&#20854;&#26159;&#22312;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#26410;&#26366;&#35265;&#36807;&#30340;&#26032;&#20852;&#26041;&#27861;&#12290;&#36890;&#29992;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#38656;&#35201;&#20986;&#33394;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#21463;&#26368;&#36817;&#25552;&#20986;&#30340;&#23631;&#34109;&#22270;&#20687;&#24314;&#27169;&#30340;&#21551;&#21457;&#65292;&#35813;&#26041;&#27861;&#22312;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#20013;&#23637;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#25105;&#20204;&#39318;&#27425;&#23581;&#35797;&#25506;&#32034;&#23558;&#23631;&#34109;&#22270;&#20687;&#24314;&#27169;&#24212;&#29992;&#20110;&#36890;&#29992;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#12290;&#25105;&#20204;&#30740;&#31350;&#22312;&#35757;&#32451;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#22120;&#26102;&#30340;&#31354;&#38388;&#21644;&#39057;&#29575;&#22495;&#23631;&#34109;&#12290;&#22522;&#20110;&#32463;&#39564;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#39057;&#29575;&#23631;&#34109;&#30340;&#26032;&#22411;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#22120;&#12290;&#25105;&#20204;&#30340;&#37325;&#28857;&#26159;&#39057;&#29575;&#22495;&#65292;&#19982;&#22823;&#22810;&#25968;&#26041;&#27861;&#20027;&#35201;&#38024;&#23545;&#31354;&#38388;&#22495;&#26816;&#27979;&#26377;&#25152;&#19981;&#21516;&#12290;&#25105;&#20204;&#30340;&#27604;&#36739;&#20998;&#26512;&#25581;&#31034;&#20102;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#30340;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;&#20195;&#30721;&#21644;&#27169;&#22411;&#24050;&#20844;&#24320;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study universal deepfake detection. Our goal is to detect synthetic images from a range of generative AI approaches, particularly from emerging ones which are unseen during training of the deepfake detector. Universal deepfake detection requires outstanding generalization capability. Motivated by recently proposed masked image modeling which has demonstrated excellent generalization in self-supervised pre-training, we make the first attempt to explore masked image modeling for universal deepfake detection. We study spatial and frequency domain masking in training deepfake detectors. Based on empirical analysis, we propose a novel deepfake detector via frequency masking. Our focus on frequency domain is different from the majority, which primarily target spatial domain detection. Our comparative analyses reveal substantial performance gains over existing methods. Code and models are publicly available.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35814;&#32454;&#30740;&#31350;&#20102;&#23558;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24212;&#29992;&#20110;&#21326;&#20026;&#20113;&#30340;OptVerse AI Solver&#65292;&#20027;&#35201;&#21253;&#25324;&#29983;&#25104;&#22797;&#26434;&#23454;&#20363;&#12289;&#35757;&#32451;&#26694;&#26550;&#32500;&#25252;&#23454;&#29992;&#24615;&#21644;&#20010;&#24615;&#21270;&#30340;&#35299;&#31639;&#22120;&#31574;&#30053;&#31561;&#26041;&#38754;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;&#30740;&#31350;&#26088;&#22312;&#32531;&#35299;&#25968;&#23398;&#35268;&#21010;&#23454;&#20363;&#31232;&#32570;&#38382;&#39064;&#65292;&#24182;&#36229;&#36234;&#20256;&#32479;&#20248;&#21270;&#25216;&#26415;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.05960</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;OptVerse AI Solver&#20013;&#30340;&#24212;&#29992;&#65306;&#35774;&#35745;&#21407;&#29702;&#19982;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Machine Learning Insides OptVerse AI Solver: Design Principles and Applications. (arXiv:2401.05960v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05960
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35814;&#32454;&#30740;&#31350;&#20102;&#23558;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24212;&#29992;&#20110;&#21326;&#20026;&#20113;&#30340;OptVerse AI Solver&#65292;&#20027;&#35201;&#21253;&#25324;&#29983;&#25104;&#22797;&#26434;&#23454;&#20363;&#12289;&#35757;&#32451;&#26694;&#26550;&#32500;&#25252;&#23454;&#29992;&#24615;&#21644;&#20010;&#24615;&#21270;&#30340;&#35299;&#31639;&#22120;&#31574;&#30053;&#31561;&#26041;&#38754;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;&#30740;&#31350;&#26088;&#22312;&#32531;&#35299;&#25968;&#23398;&#35268;&#21010;&#23454;&#20363;&#31232;&#32570;&#38382;&#39064;&#65292;&#24182;&#36229;&#36234;&#20256;&#32479;&#20248;&#21270;&#25216;&#26415;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#23383;&#26080;&#22788;&#19981;&#22312;&#30340;&#26102;&#20195;&#65292;&#39640;&#25928;&#30340;&#36164;&#28304;&#31649;&#29702;&#21644;&#20915;&#31574;&#26159;&#21508;&#20010;&#34892;&#19994;&#30340;&#37325;&#35201;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23545;&#23558;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24212;&#29992;&#20110;&#21326;&#20026;&#20113;&#30340;OptVerse AI Solver&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#30740;&#31350;&#65292;&#26088;&#22312;&#32531;&#35299;&#29616;&#23454;&#19990;&#30028;&#25968;&#23398;&#35268;&#21010;&#23454;&#20363;&#30340;&#31232;&#32570;&#65292;&#24182;&#36229;&#36234;&#20256;&#32479;&#20248;&#21270;&#25216;&#26415;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#22797;&#26434;&#30340;SAT&#21644;MILP&#23454;&#20363;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#23454;&#20363;&#21453;&#26144;&#20102;&#29616;&#23454;&#19990;&#30028;&#38382;&#39064;&#30340;&#22810;&#38754;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#35757;&#32451;&#26694;&#26550;&#65292;&#21033;&#29992;&#22686;&#24378;&#31574;&#30053;&#32500;&#25252;&#35299;&#31639;&#22120;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;&#38500;&#20102;&#25968;&#25454;&#29983;&#25104;&#21644;&#22686;&#24378;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#36824;&#21253;&#25324;&#20010;&#24615;&#21270;&#35299;&#31639;&#22120;&#31574;&#30053;&#30340;&#26032;&#22411;&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#31574;&#30053;&#65292;&#37325;&#28857;&#20851;&#27880;&#22270;&#21367;&#31215;&#32593;&#32476;&#29992;&#20110;&#21021;&#22987;&#22522;&#30784;&#36873;&#25321;&#21644;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#39640;&#32423;&#39044;&#27714;&#35299;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
In an era of digital ubiquity, efficient resource management and decision-making are paramount across numerous industries. To this end, we present a comprehensive study on the integration of machine learning (ML) techniques into Huawei Cloud's OptVerse AI Solver, which aims to mitigate the scarcity of real-world mathematical programming instances, and to surpass the capabilities of traditional optimization techniques. We showcase our methods for generating complex SAT and MILP instances utilizing generative models that mirror multifaceted structures of real-world problem. Furthermore, we introduce a training framework leveraging augmentation policies to maintain solvers' utility in dynamic environments. Besides the data generation and augmentation, our proposed approaches also include novel ML-driven policies for personalized solver strategies, with an emphasis on applications like graph convolutional networks for initial basis selection and reinforcement learning for advanced presolvi
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20154;&#26426;&#20132;&#20114;&#20013;&#30340;&#24212;&#29992;&#20855;&#26377;&#36739;&#39640;&#30340;&#39118;&#38505;&#21644;&#28508;&#22312;&#30340;&#19981;&#21487;&#36870;&#21518;&#26524;&#12290;&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24515;&#26234;&#29702;&#35770;&#33021;&#21147;&#65292;&#29305;&#21035;&#20851;&#27880;&#26426;&#22120;&#20154;&#24863;&#30693;&#34892;&#20026;&#35782;&#21035;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2401.05302</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20154;&#26426;&#20132;&#20114;&#20013;&#30340;&#24515;&#26234;&#29702;&#35770;&#33021;&#21147;:&#19968;&#20010;&#24187;&#35273;?
&lt;/p&gt;
&lt;p&gt;
Theory of Mind abilities of Large Language Models in Human-Robot Interaction : An Illusion?. (arXiv:2401.05302v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05302
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20154;&#26426;&#20132;&#20114;&#20013;&#30340;&#24212;&#29992;&#20855;&#26377;&#36739;&#39640;&#30340;&#39118;&#38505;&#21644;&#28508;&#22312;&#30340;&#19981;&#21487;&#36870;&#21518;&#26524;&#12290;&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24515;&#26234;&#29702;&#35770;&#33021;&#21147;&#65292;&#29305;&#21035;&#20851;&#27880;&#26426;&#22120;&#20154;&#24863;&#30693;&#34892;&#20026;&#35782;&#21035;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#21644;&#29983;&#25104;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#24322;&#24120;&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#21487;&#33021;&#20986;&#29616;&#20154;&#24418;&#21270;&#21644;&#23545;&#22833;&#36133;&#26696;&#20363;&#30340;&#23485;&#23481;&#24615;&#24341;&#21457;&#20102;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20986;&#29616;&#30340;&#24515;&#26234;&#29702;&#35770;&#65288;ToM&#65289;&#33021;&#21147;&#30340;&#35752;&#35770;&#12290;&#34429;&#28982;&#23384;&#22312;&#20960;&#31181;&#20551;&#20449;&#24565;&#27979;&#35797;&#26469;&#39564;&#35777;&#25512;&#26029;&#21644;&#32500;&#25252;&#21478;&#19968;&#20010;&#23454;&#20307;&#30340;&#24515;&#26234;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;ToM&#33021;&#21147;&#30340;&#19968;&#20010;&#29305;&#27530;&#24212;&#29992;&#65292;&#36825;&#20855;&#26377;&#26356;&#39640;&#30340;&#39118;&#38505;&#21644;&#21487;&#33021;&#26159;&#19981;&#21487;&#36870;&#30340;&#21518;&#26524;&#65306;&#20154;&#26426;&#20132;&#20114;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#24863;&#30693;&#34892;&#20026;&#35782;&#21035;&#30340;&#20219;&#21153;&#65292;&#20854;&#20013;&#26426;&#22120;&#20154;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#35780;&#20272;&#26426;&#22120;&#20154;&#29983;&#25104;&#30340;&#34892;&#20026;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#35266;&#23519;&#32773;&#30340;&#26041;&#24335;&#12290;&#25105;&#20204;&#20851;&#27880;&#22235;&#31181;&#34892;&#20026;&#31867;&#22411;&#65292;&#21363;&#21487;&#20197;&#35299;&#37322;&#30340;&#12289;&#21487;&#35835;&#30340;&#12289;&#21487;&#39044;&#27979;&#30340;&#21644;&#28151;&#28102;&#30340;&#34892;&#20026;&#65292;&#36825;&#20123;&#34892;&#20026;&#24050;&#34987;&#24191;&#27867;&#29992;&#20110;&#21512;&#25104;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#20154;&#34892;&#20026;&#12290;&#22240;&#27492;&#65292;LLM&#30340;&#30446;&#26631;&#26159;...
&lt;/p&gt;
&lt;p&gt;
Large Language Models have shown exceptional generative abilities in various natural language and generation tasks. However, possible anthropomorphization and leniency towards failure cases have propelled discussions on emergent abilities of Large Language Models especially on Theory of Mind (ToM) abilities in Large Language Models. While several false-belief tests exists to verify the ability to infer and maintain mental models of another entity, we study a special application of ToM abilities that has higher stakes and possibly irreversible consequences : Human Robot Interaction. In this work, we explore the task of Perceived Behavior Recognition, where a robot employs a Large Language Model (LLM) to assess the robot's generated behavior in a manner similar to human observer. We focus on four behavior types, namely explicable, legible, predictable, and obfuscatory behavior which have been extensively used to synthesize interpretable robot behaviors. The LLMs goal is, therefore to b
&lt;/p&gt;</description></item><item><title>AUTOACT&#26159;&#19968;&#20010;&#33258;&#21160;&#20195;&#29702;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#20027;&#35268;&#21010;&#21512;&#25104;&#36712;&#36857;&#65292;&#19981;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#21644;&#38381;&#28304;&#27169;&#22411;&#65292;&#33021;&#22815;&#23454;&#29616;&#26356;&#22909;&#25110;&#31867;&#20284;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.05268</link><description>&lt;p&gt;
AUTOACT&#65306;&#36890;&#36807;&#33258;&#20027;&#35268;&#21010;&#23454;&#29616;&#30340;&#33258;&#21160;&#20195;&#29702;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
AUTOACT: Automatic Agent Learning from Scratch via Self-Planning. (arXiv:2401.05268v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05268
&lt;/p&gt;
&lt;p&gt;
AUTOACT&#26159;&#19968;&#20010;&#33258;&#21160;&#20195;&#29702;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#20027;&#35268;&#21010;&#21512;&#25104;&#36712;&#36857;&#65292;&#19981;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#21644;&#38381;&#28304;&#27169;&#22411;&#65292;&#33021;&#22815;&#23454;&#29616;&#26356;&#22909;&#25110;&#31867;&#20284;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#20195;&#29702;&#22312;&#21508;&#31181;&#22797;&#26434;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#22312;&#36825;&#20010;&#39046;&#22495;&#36827;&#34892;&#20102;&#19981;&#26029;&#30340;&#25506;&#32034;&#65292;&#20294;&#29616;&#26377;&#30340;&#35821;&#35328;&#20195;&#29702;&#31995;&#32479;&#20173;&#28982;&#38754;&#20020;&#26114;&#36149;&#12289;&#19981;&#21487;&#37325;&#22797;&#30340;&#25968;&#25454;&#20381;&#36182;&#38382;&#39064;&#65292;&#24182;&#19988;&#38754;&#20020;&#23558;&#21333;&#19968;&#27169;&#22411;&#24212;&#29992;&#20110;&#22810;&#20010;&#21151;&#33021;&#30340;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;AutoAct&#65292;&#36825;&#26159;&#19968;&#20010;&#33258;&#21160;&#20195;&#29702;&#23398;&#20064;&#26694;&#26550;&#65292;&#19981;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#24102;&#27880;&#37322;&#30340;&#25968;&#25454;&#21644;&#26469;&#33258;&#38381;&#28304;&#27169;&#22411;&#65288;&#22914;GPT-4&#65289;&#30340;&#21512;&#25104;&#36712;&#36857;&#12290;&#32473;&#23450;&#26377;&#38480;&#30340;&#25968;&#25454;&#21644;&#24037;&#20855;&#24211;&#65292;AutoAct&#39318;&#20808;&#33258;&#21160;&#21512;&#25104;&#35268;&#21010;&#36712;&#36857;&#65292;&#19981;&#38656;&#35201;&#20154;&#31867;&#25110;&#24378;&#38381;&#28304;&#27169;&#22411;&#30340;&#20219;&#20309;&#36741;&#21161;&#12290;&#28982;&#21518;&#65292;AutoAct&#21033;&#29992;&#20998;&#24037;&#31574;&#30053;&#65292;&#26681;&#25454;&#30446;&#26631;&#20219;&#21153;&#20449;&#24687;&#21644;&#21512;&#25104;&#36712;&#36857;&#33258;&#21160;&#21306;&#20998;&#65292;&#20135;&#29983;&#19968;&#20010;&#23376;&#20195;&#29702;&#32452;&#26469;&#23436;&#25104;&#20219;&#21153;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22810;&#31181;LLMs&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;AutoAct&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#25110;&#19982;&#20854;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language agents have achieved considerable performance on various complex tasks. Despite the incessant exploration in this field, existing language agent systems still struggle with costly, non-reproducible data reliance and face the challenge of compelling a single model for multiple functions. To this end, we introduce AutoAct, an automatic agent learning framework that does not rely on large-scale annotated data and synthetic trajectories from closed-source models (e.g., GPT-4). Given limited data with a tool library, AutoAct first automatically synthesizes planning trajectories without any assistance from humans or strong closed-source models. Then, AutoAct leverages a division-of-labor strategy to automatically differentiate based on the target task information and synthesized trajectories, producing a sub-agent group to complete the task. We conduct comprehensive experiments with different LLMs, which demonstrates that AutoAct yields better or parallel performance compared to var
&lt;/p&gt;</description></item><item><title>MobileAgent&#36890;&#36807;&#20154;&#26426;&#20132;&#20114;&#21644;SOP&#38598;&#25104;&#65292;&#25552;&#39640;&#20102;&#31227;&#21160;&#25511;&#21046;&#30340;&#25928;&#29575;&#21644;&#20010;&#24615;&#21270;&#29992;&#25143;&#38656;&#27714;&#30340;&#28385;&#36275;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#38544;&#31169;&#38382;&#39064;&#21644;&#20195;&#29702;&#23398;&#20064;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.04124</link><description>&lt;p&gt;
MobileAgent&#65306;&#36890;&#36807;&#20154;&#26426;&#20132;&#20114;&#21644;SOP&#38598;&#25104;&#22686;&#24378;&#31227;&#21160;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
MobileAgent: enhancing mobile control via human-machine interaction and SOP integration. (arXiv:2401.04124v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04124
&lt;/p&gt;
&lt;p&gt;
MobileAgent&#36890;&#36807;&#20154;&#26426;&#20132;&#20114;&#21644;SOP&#38598;&#25104;&#65292;&#25552;&#39640;&#20102;&#31227;&#21160;&#25511;&#21046;&#30340;&#25928;&#29575;&#21644;&#20010;&#24615;&#21270;&#29992;&#25143;&#38656;&#27714;&#30340;&#28385;&#36275;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#38544;&#31169;&#38382;&#39064;&#21644;&#20195;&#29702;&#23398;&#20064;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#22312;&#20197;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20026;&#20013;&#24515;&#30340;&#20195;&#29702;&#33021;&#22815;&#20026;&#29992;&#25143;&#33258;&#21160;&#21270;&#31227;&#21160;&#35774;&#22791;&#25805;&#20316;&#12290;&#22312;&#38024;&#23545;&#23398;&#20064;&#29992;&#25143;&#30340;&#31227;&#21160;&#25805;&#20316;&#36827;&#34892;&#24494;&#35843;&#21518;&#65292;&#36825;&#20123;&#20195;&#29702;&#21487;&#20197;&#22312;&#32447;&#36981;&#24490;&#39640;&#32423;&#29992;&#25143;&#25351;&#20196;&#12290;&#23427;&#20204;&#25191;&#34892;&#30446;&#26631;&#20998;&#35299;&#12289;&#23376;&#30446;&#26631;&#24207;&#21015;&#21270;&#21644;&#20132;&#20114;&#24335;&#29615;&#22659;&#25506;&#32034;&#31561;&#20219;&#21153;&#65292;&#30452;&#21040;&#23454;&#29616;&#26368;&#32456;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#22312;&#31227;&#21160;&#25805;&#20316;&#20013;&#23384;&#22312;&#19982;&#20010;&#24615;&#21270;&#29992;&#25143;&#25968;&#25454;&#30456;&#20851;&#30340;&#38544;&#31169;&#38382;&#39064;&#65292;&#38656;&#35201;&#29992;&#25143;&#30830;&#35748;&#12290;&#27492;&#22806;&#65292;&#29992;&#25143;&#30340;&#30495;&#23454;&#25805;&#20316;&#26159;&#25506;&#32034;&#24615;&#30340;&#65292;&#34892;&#21160;&#25968;&#25454;&#22797;&#26434;&#19988;&#20887;&#20313;&#65292;&#32473;&#20195;&#29702;&#23398;&#20064;&#24102;&#26469;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#22312;&#25105;&#20204;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20195;&#29702;&#19982;&#20154;&#20043;&#38388;&#30340;&#20132;&#20114;&#20219;&#21153;&#65292;&#20197;&#35782;&#21035;&#25935;&#24863;&#20449;&#24687;&#24182;&#19982;&#20010;&#24615;&#21270;&#29992;&#25143;&#38656;&#27714;&#23545;&#40784;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#38598;&#25104;&#20102;&#26631;&#20934;&#25805;&#20316;&#35268;&#31243;&#65288;SOP&#65289;&#20449;&#24687;&#65292;&#20197;&#22686;&#24378;&#20195;&#29702;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Agents centered around Large Language Models (LLMs) are now capable of automating mobile device operations for users. After fine-tuning to learn a user's mobile operations, these agents can adhere to high-level user instructions online. They execute tasks such as goal decomposition, sequencing of sub-goals, and interactive environmental exploration, until the final objective is achieved. However, privacy concerns related to personalized user data arise during mobile operations, requiring user confirmation. Moreover, users' real-world operations are exploratory, with action data being complex and redundant, posing challenges for agent learning. To address these issues, in our practical application, we have designed interactive tasks between agents and humans to identify sensitive information and align with personalized user needs. Additionally, we integrated Standard Operating Procedure (SOP) information within the model's in-context learning to enhance the agent's comprehension of comp
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#24494;&#23567;&#26102;&#38388;&#28151;&#21512;&#22120; (TTMs) &#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#38024;&#23545;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#30340;&#38646;/&#23569;&#26679;&#26412;&#39044;&#27979;&#36827;&#34892;&#20102;&#20248;&#21270;&#12290;&#19982;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30456;&#27604;&#65292;TTMs&#27169;&#22411;&#26356;&#23567;&#12289;&#26356;&#24555;&#65292;&#24182;&#32771;&#34385;&#20102;&#36328;&#36890;&#36947;&#30456;&#20851;&#24615;&#65292;&#33021;&#22815;&#22312;&#30701;&#26102;&#38388;&#20869;&#36827;&#34892;&#26377;&#25928;&#30340;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2401.03955</link><description>&lt;p&gt;
&#24494;&#23567;&#26102;&#38388;&#28151;&#21512;&#22120; (TTMs): &#38024;&#23545;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#30340;&#22686;&#24378;&#38646;/&#23569;&#26679;&#26412;&#39044;&#27979;&#30340;&#24555;&#36895;&#39044;&#35757;&#32451;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Tiny Time Mixers (TTMs): Fast Pretrained Models for Enhanced Zero/Few-Shot Forecasting of Multivariate Time Series. (arXiv:2401.03955v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03955
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#24494;&#23567;&#26102;&#38388;&#28151;&#21512;&#22120; (TTMs) &#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#38024;&#23545;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#30340;&#38646;/&#23569;&#26679;&#26412;&#39044;&#27979;&#36827;&#34892;&#20102;&#20248;&#21270;&#12290;&#19982;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30456;&#27604;&#65292;TTMs&#27169;&#22411;&#26356;&#23567;&#12289;&#26356;&#24555;&#65292;&#24182;&#32771;&#34385;&#20102;&#36328;&#36890;&#36947;&#30456;&#20851;&#24615;&#65292;&#33021;&#22815;&#22312;&#30701;&#26102;&#38388;&#20869;&#36827;&#34892;&#26377;&#25928;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;/&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#35821;&#35328;&#21644;&#35270;&#35273;&#39046;&#22495;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015; (TS) &#20013;&#38754;&#20020;&#30528;&#22810;&#26679;&#24615;&#21644;&#20844;&#24320;&#39044;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#30340;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#26368;&#36817;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#36827;&#34892;&#21508;&#31181;&#36866;&#24212;&#30340;&#36235;&#21183;&#36880;&#28176;&#22686;&#21152;&#12290;&#36825;&#20123;&#26041;&#27861;&#21033;&#29992;&#36328;&#39046;&#22495;&#36801;&#31227;&#23398;&#20064;&#65292;&#20986;&#22855;&#22320;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#38750;&#24120;&#32531;&#24930;&#19988;&#24222;&#22823;&#65288;&#22823;&#32422;&#21313;&#20159;&#20010;&#21442;&#25968;&#65289;&#65292;&#24182;&#19988;&#19981;&#32771;&#34385;&#36328;&#36890;&#36947;&#30456;&#20851;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#23618;&#24494;&#23567;&#26102;&#38388;&#28151;&#21512;&#22120; (TTM)&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#36731;&#37327;&#32423; TSMixer &#32467;&#26500;&#30340;&#26174;&#33879;&#23567;&#22411;&#27169;&#22411;&#12290;TTM &#26159;&#39318;&#20010;&#25104;&#21151;&#24320;&#21457;&#30340;&#24494;&#22411;&#36890;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;&#8804;100&#19975;&#20010;&#21442;&#25968;&#65289;&#65292;&#19987;&#38376;&#22312;&#20844;&#24320;TS&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24555;&#36895;&#35757;&#32451;&#65288;&#20165;&#38656;4-8&#23567;&#26102;&#65289;&#65292;&#20855;&#26377;&#26377;&#25928;&#30340;&#36801;&#31227;&#23398;&#20064;&#33021;&#21147;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Pretrained models for zero/few-shot learning excel in language and vision domains but encounter challenges in multivariate time series (TS) due to the diverse nature and scarcity of publicly available pretraining data. Consequently, there has been a recent surge in utilizing pretrained large language models (LLMs) with various adaptations for time series forecasting. These approaches employ cross-domain transfer learning and surprisingly yield impressive results. However, these models are typically very slow and large ($\sim$billion parameters) and do not consider cross-channel correlations. To address this, we present Multi-level Tiny Time Mixers (TTM), a significantly small model based on the lightweight TSMixer architecture. TTM marks the first success in developing tiny general-pretrained models ($\le$1 million parameters), exclusively trained on public TS datasets in a flash of just 4-8 hrs with effective transfer learning capabilities for forecasting. To tackle the complexi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20379;&#20102;&#23545;&#36825;&#20123;&#27169;&#22411;&#30340;&#20840;&#38754;&#32972;&#26223;&#20449;&#24687;&#21644;&#35814;&#32454;&#35828;&#26126;&#65292;&#21516;&#26102;&#20063;&#23545;&#23427;&#20204;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#21644;&#24444;&#27492;&#20043;&#38388;&#30340;&#27604;&#36739;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#20854;&#36129;&#29486;&#21253;&#25324;&#23545;&#25193;&#25955;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#24212;&#29992;&#30340;&#24443;&#24213;&#25506;&#32034;&#21644;&#25353;&#26102;&#38388;&#39034;&#24207;&#25490;&#24207;&#30340;&#27169;&#22411;&#27010;&#36848;&#12290;&#36825;&#26159;&#19968;&#20221;&#23545;&#20154;&#24037;&#26234;&#33021;&#21644;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#26469;&#35828;&#20855;&#26377;&#20215;&#20540;&#30340;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2401.03006</link><description>&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#25193;&#25955;&#27169;&#22411;&#30340;&#20852;&#36215;
&lt;/p&gt;
&lt;p&gt;
The Rise of Diffusion Models in Time-Series Forecasting. (arXiv:2401.03006v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03006
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20379;&#20102;&#23545;&#36825;&#20123;&#27169;&#22411;&#30340;&#20840;&#38754;&#32972;&#26223;&#20449;&#24687;&#21644;&#35814;&#32454;&#35828;&#26126;&#65292;&#21516;&#26102;&#20063;&#23545;&#23427;&#20204;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#21644;&#24444;&#27492;&#20043;&#38388;&#30340;&#27604;&#36739;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#20854;&#36129;&#29486;&#21253;&#25324;&#23545;&#25193;&#25955;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#24212;&#29992;&#30340;&#24443;&#24213;&#25506;&#32034;&#21644;&#25353;&#26102;&#38388;&#39034;&#24207;&#25490;&#24207;&#30340;&#27169;&#22411;&#27010;&#36848;&#12290;&#36825;&#26159;&#19968;&#20221;&#23545;&#20154;&#24037;&#26234;&#33021;&#21644;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#26469;&#35828;&#20855;&#26377;&#20215;&#20540;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#25506;&#35752;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#12290;&#25193;&#25955;&#27169;&#22411;&#22312;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#30340;&#21508;&#20010;&#39046;&#22495;&#20013;&#23637;&#31034;&#20986;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#26412;&#25991;&#21253;&#25324;&#23545;&#25193;&#25955;&#27169;&#22411;&#30340;&#20840;&#38754;&#32972;&#26223;&#20449;&#24687;&#65292;&#35814;&#32454;&#20171;&#32461;&#20854;&#26465;&#20214;&#26041;&#27861;&#65292;&#24182;&#23457;&#26597;&#20102;&#20854;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#12290;&#20998;&#26512;&#28085;&#30422;&#20102;11&#20010;&#20855;&#20307;&#30340;&#26102;&#38388;&#24207;&#21015;&#23454;&#29616;&#65292;&#23427;&#20204;&#30340;&#30452;&#35273;&#21644;&#29702;&#35770;&#22522;&#30784;&#65292;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#20197;&#21450;&#24444;&#27492;&#20043;&#38388;&#30340;&#27604;&#36739;&#12290;&#35813;&#24037;&#20316;&#30340;&#20851;&#38190;&#36129;&#29486;&#26159;&#23545;&#25193;&#25955;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#24212;&#29992;&#30340;&#24443;&#24213;&#25506;&#32034;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#25353;&#26102;&#38388;&#39034;&#24207;&#25490;&#24207;&#30340;&#27169;&#22411;&#27010;&#36848;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#23545;&#35813;&#39046;&#22495;&#30340;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#36827;&#34892;&#20102;&#28145;&#20837;&#35752;&#35770;&#65292;&#24182;&#27010;&#36848;&#20102;&#28508;&#22312;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;&#36825;&#23545;&#20110;&#20154;&#24037;&#26234;&#33021;&#21644;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#26469;&#35828;&#26159;&#19968;&#20221;&#23453;&#36149;&#30340;&#36164;&#28304;&#65292;&#25552;&#20379;&#20102;&#23545;&#26368;&#26032;&#36827;&#23637;&#21644;&#26410;&#26469;&#21457;&#23637;&#30340;&#28165;&#26224;&#35270;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
This survey delves into the application of diffusion models in time-series forecasting. Diffusion models are demonstrating state-of-the-art results in various fields of generative AI. The paper includes comprehensive background information on diffusion models, detailing their conditioning methods and reviewing their use in time-series forecasting. The analysis covers 11 specific time-series implementations, the intuition and theory behind them, the effectiveness on different datasets, and a comparison among each other. Key contributions of this work are the thorough exploration of diffusion models' applications in time-series forecasting and a chronologically ordered overview of these models. Additionally, the paper offers an insightful discussion on the current state-of-the-art in this domain and outlines potential future research directions. This serves as a valuable resource for researchers in AI and time-series analysis, offering a clear view of the latest advancements and future p
&lt;/p&gt;</description></item><item><title>MIMIR&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#20013;&#21033;&#29992;&#36974;&#32617;&#22270;&#20687;&#24314;&#27169;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#19981;&#21516;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#26088;&#22312;&#22686;&#24378;Vision Transformers&#65288;ViTs&#65289;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2312.04960</link><description>&lt;p&gt;
MIMIR: &#22522;&#20110;&#20114;&#20449;&#24687;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#36974;&#32617;&#22270;&#20687;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
MIMIR: Masked Image Modeling for Mutual Information-based Adversarial Robustness. (arXiv:2312.04960v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.04960
&lt;/p&gt;
&lt;p&gt;
MIMIR&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#20013;&#21033;&#29992;&#36974;&#32617;&#22270;&#20687;&#24314;&#27169;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#19981;&#21516;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#26088;&#22312;&#22686;&#24378;Vision Transformers&#65288;ViTs&#65289;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#21464;&#21387;&#22120;&#65288;ViTs&#65289;&#30456;&#23545;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#20294;ViTs&#20063;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;&#23545;&#25239;&#24615;&#35757;&#32451;&#26159;&#24314;&#31435;&#24378;&#22823;&#30340;CNN&#27169;&#22411;&#30340;&#26368;&#25104;&#21151;&#26041;&#27861;&#20043;&#19968;&#12290;&#22240;&#27492;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#22522;&#20110;ViTs&#21644;CNNs&#20043;&#38388;&#30340;&#24046;&#24322;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#30340;&#26032;&#26041;&#27861;&#65292;&#22914;&#26356;&#22909;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#38450;&#27490;&#27880;&#24847;&#21147;&#38598;&#20013;&#22312;&#21333;&#20010;&#22359;&#19978;&#65292;&#25110;&#20002;&#24323;&#20302;&#27880;&#24847;&#21147;&#30340;&#23884;&#20837;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20173;&#28982;&#36981;&#24490;&#20256;&#32479;&#30417;&#30563;&#23545;&#25239;&#35757;&#32451;&#30340;&#35774;&#35745;&#65292;&#38480;&#21046;&#20102;&#23545;ViTs&#30340;&#23545;&#25239;&#35757;&#32451;&#30340;&#28508;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38450;&#24481;&#26041;&#27861;MIMIR&#65292;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#20013;&#30340;&#36974;&#32617;&#22270;&#20687;&#24314;&#27169;&#26500;&#24314;&#19981;&#21516;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#26041;&#27861;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#33258;&#32534;&#30721;&#22120;&#65292;&#23427;&#25509;&#21463;&#23545;&#25239;&#24615;&#20363;&#23376;&#20316;&#20026;&#36755;&#20837;&#65292;&#20294;&#23558;&#24178;&#20928;&#30340;&#20363;&#23376;&#20316;&#20026;&#24314;&#27169;&#30446;&#26631;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#20114;&#20449;&#24687;&#65288;MI&#65289;
&lt;/p&gt;
&lt;p&gt;
Vision Transformers (ViTs) achieve superior performance on various tasks compared to convolutional neural networks (CNNs), but ViTs are also vulnerable to adversarial attacks. Adversarial training is one of the most successful methods to build robust CNN models. Thus, recent works explored new methodologies for adversarial training of ViTs based on the differences between ViTs and CNNs, such as better training strategies, preventing attention from focusing on a single block, or discarding low-attention embeddings. However, these methods still follow the design of traditional supervised adversarial training, limiting the potential of adversarial training on ViTs. This paper proposes a novel defense method, MIMIR, which aims to build a different adversarial training methodology by utilizing Masked Image Modeling at pre-training. We create an autoencoder that accepts adversarial examples as input but takes the clean examples as the modeling target. Then, we create a mutual information (MI
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;PCA&#12289;t-SNE&#21644;UMAP&#21487;&#35270;&#21270;&#25216;&#26415;&#65292;&#20197;&#21450;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26469;&#38477;&#20302;5G&#32593;&#32476;&#27969;&#37327;&#20837;&#20405;&#30340;&#39118;&#38505;&#65292;&#24182;&#35299;&#20915;&#31867;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#33258;&#27835;&#31471;&#21040;&#31471;&#23433;&#20840;&#35774;&#35745;&#12290;</title><link>http://arxiv.org/abs/2312.04864</link><description>&lt;p&gt;
&#20351;&#29992;PCA&#12289;t-SNE&#21644;UMAP&#21487;&#35270;&#21270;&#21644;&#20998;&#31867;&#25915;&#20987;&#26469;&#23545;5G&#32593;&#32476;&#27969;&#37327;&#20837;&#20405;&#36827;&#34892;&#20851;&#38190;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Critical Analysis of 5G Networks Traffic Intrusion using PCA, t-SNE and UMAP Visualization and Classifying Attacks. (arXiv:2312.04864v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.04864
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;PCA&#12289;t-SNE&#21644;UMAP&#21487;&#35270;&#21270;&#25216;&#26415;&#65292;&#20197;&#21450;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26469;&#38477;&#20302;5G&#32593;&#32476;&#27969;&#37327;&#20837;&#20405;&#30340;&#39118;&#38505;&#65292;&#24182;&#35299;&#20915;&#31867;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#33258;&#27835;&#31471;&#21040;&#31471;&#23433;&#20840;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#12289;&#23041;&#32961;&#27169;&#22411;&#21644;&#24694;&#24847;&#34892;&#20026;&#32773;&#22312;&#19981;&#26029;&#21457;&#23637;&#12290;&#38543;&#30528;5G&#32593;&#32476;&#30340;&#22686;&#21152;&#37096;&#32626;&#65292;&#38468;&#21152;&#30340;5G&#29289;&#29702;&#35774;&#22791;&#30340;&#23433;&#20840;&#38382;&#39064;&#20063;&#22312;&#22686;&#21152;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#33258;&#27835;&#31471;&#21040;&#31471;&#23433;&#20840;&#35774;&#35745;&#65292;&#33021;&#22815;&#36890;&#36807;&#26816;&#27979;&#32593;&#32476;&#27969;&#37327;&#24322;&#24120;&#26469;&#22788;&#29702;&#21363;&#23558;&#21040;&#26469;&#30340;&#23041;&#32961;&#12290;&#20026;&#28385;&#36275;&#36825;&#19968;&#35201;&#27714;&#65292;&#26412;&#30740;&#31350;&#20351;&#29992;&#26368;&#36817;&#21457;&#24067;&#30340;5G&#27969;&#37327;&#25968;&#25454;&#38598;5G-NIDD&#65292;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26469;&#26816;&#27979;&#32593;&#32476;&#27969;&#37327;&#24322;&#24120;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#19977;&#31181;&#21487;&#35270;&#21270;&#25216;&#26415;&#65306;t-SNE&#12289;UMAP&#21644;PCA&#20998;&#26512;&#25968;&#25454;&#38598;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20351;&#29992;&#20114;&#20449;&#24687;&#21644;PCA&#25216;&#26415;&#38477;&#20302;&#25968;&#25454;&#32500;&#24230;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#25554;&#20837;&#23569;&#25968;&#31867;&#30340;&#21512;&#25104;&#35760;&#24405;&#26469;&#35299;&#20915;&#31867;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#20845;&#31181;&#19981;&#21516;&#30340;&#20998;&#31867;&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Networks, threat models, and malicious actors are advancing quickly. With the increased deployment of the 5G networks, the security issues of the attached 5G physical devices have also increased. Therefore, artificial intelligence based autonomous end-to-end security design is needed that can deal with incoming threats by detecting network traffic anomalies. To address this requirement, in this research, we used a recently published 5G traffic dataset, 5G-NIDD, to detect network traffic anomalies using machine and deep learning approaches. First, we analyzed the dataset using three visualization techniques: t-Distributed Stochastic Neighbor Embedding (t-SNE), Uniform Manifold Approximation and Projection (UMAP), and Principal Component Analysis (PCA). Second, we reduced the data dimensionality using mutual information and PCA techniques. Third, we solve the class imbalance issue by inserting synthetic records of minority classes. Last, we performed classification using six different cl
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;NLP&#20219;&#21153;&#65292;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#22240;&#26524;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#20316;&#32773;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;CLadder&#65292;&#24182;&#21033;&#29992;oracle&#22240;&#26524;&#25512;&#29702;&#24341;&#25806;&#23558;&#31526;&#21495;&#38382;&#39064;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#22810;&#20010;LLMs&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#24341;&#20837;&#24182;&#35780;&#20272;&#20102;&#19968;&#31181;&#23450;&#21046;&#30340;&#38142;&#24335;&#25512;&#29702;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2312.04350</link><description>&lt;p&gt;
CLadder: &#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22240;&#26524;&#25512;&#29702;&#33021;&#21147;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
CLadder: A Benchmark to Assess Causal Reasoning Capabilities of Language Models. (arXiv:2312.04350v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.04350
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;NLP&#20219;&#21153;&#65292;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#22240;&#26524;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#20316;&#32773;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;CLadder&#65292;&#24182;&#21033;&#29992;oracle&#22240;&#26524;&#25512;&#29702;&#24341;&#25806;&#23558;&#31526;&#21495;&#38382;&#39064;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#22810;&#20010;LLMs&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#24341;&#20837;&#24182;&#35780;&#20272;&#20102;&#19968;&#31181;&#23450;&#21046;&#30340;&#38142;&#24335;&#25512;&#29702;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36827;&#34892;&#22240;&#26524;&#25512;&#29702;&#30340;&#33021;&#21147;&#34987;&#24191;&#27867;&#35270;&#20026;&#26234;&#33021;&#30340;&#26680;&#24515;&#29305;&#24449;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#33021;&#21542;&#36830;&#36143;&#22320;&#25512;&#29702;&#22240;&#26524;&#20851;&#31995;&#12290;&#29616;&#26377;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#24037;&#20316;&#20027;&#35201;&#20851;&#27880;&#35780;&#20272;LLMs&#20013;&#30340;&#24120;&#35782;&#22240;&#26524;&#25512;&#29702;&#65292;&#26410;&#33021;&#35780;&#20272;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#25353;&#29031;&#19968;&#32452;&#26126;&#30830;&#23450;&#20041;&#30340;&#24418;&#24335;&#35268;&#21017;&#25191;&#34892;&#22240;&#26524;&#25512;&#26029;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;NLP&#20219;&#21153;&#65292;&#33258;&#28982;&#35821;&#35328;&#20013;&#30340;&#22240;&#26524;&#25512;&#26029;&#65292;&#21463;&#21040;Judea Pearl&#31561;&#20154;&#25552;&#20986;&#30340;&#8220;&#22240;&#26524;&#25512;&#26029;&#24341;&#25806;&#8221;&#30340;&#21551;&#21457;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;10K&#20010;&#26679;&#26412;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;CLadder&#65292;&#36890;&#36807;&#19968;&#31181;oracle&#22240;&#26524;&#25512;&#29702;&#24341;&#25806;&#65292;&#22522;&#20110;&#19968;&#32452;&#22240;&#26524;&#22270;&#21644;&#26597;&#35810;(&#32852;&#21512;&#12289;&#24178;&#39044;&#21644;&#21453;&#20107;&#23454;)&#65292;&#24471;&#21040;&#31526;&#21495;&#38382;&#39064;&#21644;&#30495;&#23454;&#31572;&#26696;&#65292;&#24182;&#23558;&#20854;&#32763;&#35793;&#20026;&#33258;&#28982;&#35821;&#35328;&#12290;&#25105;&#20204;&#23545;&#25968;&#25454;&#38598;&#19978;&#30340;&#22810;&#20010;LLMs&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#24341;&#20837;&#21644;&#35780;&#20272;&#20102;&#19968;&#31181;&#23450;&#21046;&#30340;&#38142;&#24335;&#25512;&#29702;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to perform causal reasoning is widely considered a core feature of intelligence. In this work, we investigate whether large language models (LLMs) can coherently reason about causality. Much of the existing work in natural language processing (NLP) focuses on evaluating commonsense causal reasoning in LLMs, thus failing to assess whether a model can perform causal inference in accordance with a set of well-defined formal rules. To address this, we propose a new NLP task, causal inference in natural language, inspired by the "causal inference engine" postulated by Judea Pearl et al. We compose a large dataset, CLadder, with 10K samples: based on a collection of causal graphs and queries (associational, interventional, and counterfactual), we obtain symbolic questions and ground-truth answers, through an oracle causal inference engine. These are then translated into natural language. We evaluate multiple LLMs on our dataset, and we introduce and evaluate a bespoke chain-of-th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35745;&#31639;&#27169;&#22411;&#25506;&#31350;&#20102;&#25252;&#29702;&#32773;&#30340;&#35848;&#35805;&#23545;&#24188;&#20799;&#35270;&#35273;&#34920;&#24449;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#21363;&#20351;&#22312;&#35821;&#35328;&#36755;&#20837;&#26377;&#38480;&#30340;&#24773;&#22659;&#19979;&#65292;&#25252;&#29702;&#32773;&#30340;&#35848;&#35805;&#20173;&#28982;&#33021;&#22815;&#25552;&#21319;&#24188;&#20799;&#30340;&#35270;&#35273;&#34920;&#24449;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2312.04118</link><description>&lt;p&gt;
&#25252;&#29702;&#32773;&#30340;&#35848;&#35805;&#22609;&#36896;&#24188;&#20799;&#35270;&#35273;&#65306;&#19968;&#39033;&#20851;&#20110;&#21452;&#21442;&#19982;&#28216;&#25103;&#30340;&#35745;&#31639;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Caregiver Talk Shapes Toddler Vision: A Computational Study of Dyadic Play. (arXiv:2312.04118v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.04118
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35745;&#31639;&#27169;&#22411;&#25506;&#31350;&#20102;&#25252;&#29702;&#32773;&#30340;&#35848;&#35805;&#23545;&#24188;&#20799;&#35270;&#35273;&#34920;&#24449;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#21363;&#20351;&#22312;&#35821;&#35328;&#36755;&#20837;&#26377;&#38480;&#30340;&#24773;&#22659;&#19979;&#65292;&#25252;&#29702;&#32773;&#30340;&#35848;&#35805;&#20173;&#28982;&#33021;&#22815;&#25552;&#21319;&#24188;&#20799;&#30340;&#35270;&#35273;&#34920;&#24449;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23156;&#20799;&#35782;&#21035;&#21644;&#20998;&#31867;&#29289;&#20307;&#30340;&#33021;&#21147;&#36880;&#28176;&#21457;&#23637;&#12290;&#29983;&#21629;&#30340;&#31532;&#20108;&#24180;&#26631;&#24535;&#30528;&#26356;&#22810;&#35821;&#20041;&#35270;&#35273;&#34920;&#24449;&#30340;&#20986;&#29616;&#21644;&#23545;&#35789;&#27719;&#21547;&#20041;&#30340;&#26356;&#22909;&#29702;&#35299;&#12290;&#36825;&#34920;&#26126;&#35821;&#35328;&#36755;&#20837;&#21487;&#33021;&#22312;&#22609;&#36896;&#35270;&#35273;&#34920;&#24449;&#20013;&#36215;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#22312;&#36866;&#21512;&#23398;&#20064;&#21333;&#35789;&#30340;&#24773;&#22659;&#19979;&#65292;&#22914;&#21452;&#21442;&#19982;&#28216;&#25103;&#20250;&#35805;&#20013;&#65292;&#25252;&#29702;&#32773;&#30340;&#35805;&#35821;&#20063;&#26159;&#31232;&#23569;&#21644;&#19981;&#26126;&#30830;&#30340;&#65292;&#24120;&#24120;&#25351;&#30340;&#26159;&#19982;&#20799;&#31461;&#27880;&#24847;&#30340;&#29289;&#20307;&#19981;&#21516;&#30340;&#29289;&#20307;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#25252;&#29702;&#32773;&#30340;&#35805;&#35821;&#21040;&#24213;&#33021;&#22815;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#22686;&#24378;&#35270;&#35273;&#34920;&#24449;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35745;&#31639;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#21452;&#21442;&#19982;&#28216;&#25103;&#36807;&#31243;&#20013;&#23398;&#20064;&#35270;&#35273;&#34920;&#24449;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#30001;&#24188;&#20799;&#20195;&#29702;&#20154;&#24863;&#30693;&#21040;&#30340;&#20197;&#33258;&#25105;&#20026;&#20013;&#24515;&#30340;&#22270;&#20687;&#65292;&#22312;&#19981;&#21516;&#30340;&#23478;&#24237;&#29615;&#22659;&#20013;&#31227;&#21160;&#21644;&#26059;&#36716;&#29609;&#20855;&#29289;&#20307;&#65292;&#24182;&#21516;&#26102;&#21548;&#21040;&#34987;&#24314;&#27169;&#20026;&#23383;&#24149;&#30340;&#25252;&#29702;&#32773;&#30340;&#35805;&#35821;&#12290;
&lt;/p&gt;
&lt;p&gt;
Infants' ability to recognize and categorize objects develops gradually. The second year of life is marked by both the emergence of more semantic visual representations and a better understanding of word meaning. This suggests that language input may play an important role in shaping visual representations. However, even in suitable contexts for word learning like dyadic play sessions, caregivers utterances are sparse and ambiguous, often referring to objects that are different from the one to which the child attends. Here, we systematically investigate to what extent caregivers' utterances can nevertheless enhance visual representations. For this we propose a computational model of visual representation learning during dyadic play. We introduce a synthetic dataset of ego-centric images perceived by a toddler-agent that moves and rotates toy objects in different parts of its home environment while hearing caregivers' utterances, modeled as captions. We propose to model toddlers' learni
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20998;&#26512;&#20102;&#22312;&#28151;&#21512;&#20132;&#36890;&#25511;&#21046;&#31574;&#30053;&#19979;&#65292;&#38750;&#20449;&#21495;&#21270;&#29616;&#23454;&#19990;&#30028;&#20132;&#21449;&#21475;&#30340;&#25490;&#25918;&#21644;&#33021;&#28304;&#25928;&#29575;&#65292;&#25552;&#20379;&#20102;&#23545;&#20855;&#26377;&#22797;&#26434;&#25299;&#25169;&#21644;&#20132;&#36890;&#38656;&#27714;&#30340;&#38750;&#20449;&#21495;&#21270;&#20132;&#21449;&#21475;&#30340;&#25490;&#25918;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2311.11866</link><description>&lt;p&gt;
&#20998;&#26512;&#28151;&#21512;&#20132;&#36890;&#25511;&#21046;&#19979;&#38750;&#20449;&#21495;&#21270;&#29616;&#23454;&#19990;&#30028;&#20132;&#21449;&#21475;&#30340;&#25490;&#25918;&#21644;&#33021;&#28304;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Analyzing Emissions and Energy Efficiency at Unsignalized Real-world Intersections Under Mixed Traffic Control. (arXiv:2311.11866v2 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.11866
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20998;&#26512;&#20102;&#22312;&#28151;&#21512;&#20132;&#36890;&#25511;&#21046;&#31574;&#30053;&#19979;&#65292;&#38750;&#20449;&#21495;&#21270;&#29616;&#23454;&#19990;&#30028;&#20132;&#21449;&#21475;&#30340;&#25490;&#25918;&#21644;&#33021;&#28304;&#25928;&#29575;&#65292;&#25552;&#20379;&#20102;&#23545;&#20855;&#26377;&#22797;&#26434;&#25299;&#25169;&#21644;&#20132;&#36890;&#38656;&#27714;&#30340;&#38750;&#20449;&#21495;&#21270;&#20132;&#21449;&#21475;&#30340;&#25490;&#25918;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;20&#19990;&#32426;&#21021;&#20197;&#26469;&#65292;&#28201;&#23460;&#27668;&#20307;&#25490;&#25918;&#37327;&#24613;&#21095;&#19978;&#21319;&#65292;&#32654;&#22269;&#20132;&#36890;&#36816;&#36755;&#21344;&#32654;&#22269;&#25490;&#25918;&#37327;&#30340;28%&#12290;&#22240;&#27492;&#65292;&#20943;&#23569;&#20132;&#36890;&#30456;&#20851;&#30340;&#25490;&#25918;&#37327;&#25104;&#20026;&#20851;&#27880;&#30340;&#28966;&#28857;&#12290;&#29305;&#21035;&#26159;&#65292;&#21487;&#25345;&#32493;&#24615;&#30740;&#31350;&#24050;&#32463;&#22312;&#20449;&#21495;&#21270;&#20132;&#21449;&#21475;&#21608;&#22260;&#23637;&#24320;&#65292;&#22240;&#20026;&#20132;&#21449;&#21475;&#20801;&#35768;&#19981;&#21516;&#27969;&#37327;&#30340;&#20132;&#36890;&#31359;&#36807;&#21644;&#25913;&#21464;&#26041;&#21521;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#22312;&#20449;&#21495;&#21270;&#20132;&#21449;&#21475;&#24320;&#21457;&#20102;&#28151;&#21512;&#20132;&#36890;&#25511;&#21046;&#30340;&#29983;&#24577;&#39550;&#39542;&#31574;&#30053;&#65292;&#20197;&#20943;&#23569;&#25490;&#25918;&#12290;&#28982;&#32780;&#65292;&#20449;&#21495;&#21270;&#20132;&#21449;&#21475;&#30340;&#22825;&#28982;&#32467;&#26500;&#36890;&#36807;&#21019;&#24314;&#39057;&#32321;&#30340;&#21152;&#36895;/&#20943;&#36895;&#20107;&#20214;&#12289;&#20132;&#36890;&#25317;&#22581;&#24341;&#36215;&#30340;&#36807;&#24230;&#24608;&#36895;&#21644;&#36215;&#27874;&#29616;&#35937;&#65292;&#22686;&#21152;&#20102;&#25490;&#25918;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30456;&#20449;&#38750;&#20449;&#21495;&#21270;&#20132;&#21449;&#21475;&#20855;&#26377;&#36827;&#19968;&#27493;&#25913;&#21892;&#21487;&#25345;&#32493;&#24615;&#30340;&#28508;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;&#20855;&#26377;&#22797;&#26434;&#30340;&#29616;&#23454;&#19990;&#30028;&#25299;&#25169;&#21644;&#20132;&#36890;&#38656;&#27714;&#30340;&#38750;&#20449;&#21495;&#21270;&#20132;&#21449;&#21475;&#36827;&#34892;&#20102;&#25490;&#25918;&#20998;&#26512;&#65292;&#37319;&#29992;&#20102;&#28151;&#21512;&#20132;&#36890;&#25511;&#21046;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Greenhouse gas emissions have dramatically risen since the early 1900s with U.S. transportation generating 28% of U.S. emissions. As such, there is interest in reducing transportation-related emissions. Specifically, sustainability research has sprouted around signalized intersections as intersections allow different streams of traffic to cross and change directions. Recent research has developed mixed traffic control eco-driving strategies at signalized intersections to decrease emissions. However, the inherent structure of a signalized intersection generates increased emissions by creating frequent acceleration/deceleration events, excessive idling from traffic congestion, and stop-and-go waves. Thus, we believe unsignalized intersections hold potential for further sustainability improvements. In this work, we provide an emissions analysis on unsignalized intersections with complex, real-world topologies and traffic demands where mixed traffic control strategies are employed by robot
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#36951;&#20256;&#31639;&#27861;&#30340;&#29238;&#20195;&#36873;&#25321;&#26426;&#21046;&#21644;&#21464;&#24322;&#26041;&#38754;&#36827;&#34892;&#22686;&#24378;&#65292;&#20174;&#32780;&#22312;&#25490;&#21015;&#27969;&#27700;&#36710;&#38388;&#35843;&#24230;&#38382;&#39064;&#20013;&#26368;&#23567;&#21270;&#23436;&#24037;&#26102;&#38388;&#12290;&#36825;&#31181;RL+GA&#26041;&#27861;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#36873;&#25321;&#21644;&#21464;&#24322;&#65292;&#26377;&#25928;&#22320;&#25913;&#36827;&#20102;&#21407;&#22987;GA&#30340;&#24615;&#33021;&#65292;&#24182;&#34920;&#29616;&#20986;&#36866;&#24212;&#32676;&#20307;&#22810;&#26679;&#24615;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2311.05937</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22686;&#24378;&#30340;&#36951;&#20256;&#31639;&#27861;&#22312;&#29238;&#20195;&#36873;&#25321;&#26426;&#21046;&#21644;&#21464;&#24322;&#26041;&#38754;&#65306;&#22312;&#25490;&#21015;&#27969;&#27700;&#36710;&#38388;&#35843;&#24230;&#38382;&#39064;&#20013;&#26368;&#23567;&#21270;&#23436;&#24037;&#26102;&#38388;
&lt;/p&gt;
&lt;p&gt;
Genetic Algorithm enhanced by Deep Reinforcement Learning in parent selection mechanism and mutation : Minimizing makespan in permutation flow shop scheduling problems. (arXiv:2311.05937v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.05937
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#36951;&#20256;&#31639;&#27861;&#30340;&#29238;&#20195;&#36873;&#25321;&#26426;&#21046;&#21644;&#21464;&#24322;&#26041;&#38754;&#36827;&#34892;&#22686;&#24378;&#65292;&#20174;&#32780;&#22312;&#25490;&#21015;&#27969;&#27700;&#36710;&#38388;&#35843;&#24230;&#38382;&#39064;&#20013;&#26368;&#23567;&#21270;&#23436;&#24037;&#26102;&#38388;&#12290;&#36825;&#31181;RL+GA&#26041;&#27861;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#36873;&#25321;&#21644;&#21464;&#24322;&#65292;&#26377;&#25928;&#22320;&#25913;&#36827;&#20102;&#21407;&#22987;GA&#30340;&#24615;&#33021;&#65292;&#24182;&#34920;&#29616;&#20986;&#36866;&#24212;&#32676;&#20307;&#22810;&#26679;&#24615;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#27861;&#26469;&#35299;&#20915;&#37197;&#32622;&#21644;&#20248;&#21270;&#36951;&#20256;&#31639;&#27861;&#65288;GA&#65289;&#20197;&#35299;&#20915;&#22256;&#38590;&#30340;&#32452;&#21512;&#25110;&#38750;&#32447;&#24615;&#38382;&#39064;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#25552;&#20986;&#30340;RL+GA&#26041;&#27861;&#19987;&#38376;&#22312;&#27969;&#27700;&#36710;&#38388;&#35843;&#24230;&#38382;&#39064;&#65288;FSP&#65289;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#35813;&#28151;&#21512;&#31639;&#27861;&#32467;&#21512;&#20102;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#65292;&#24182;&#20351;&#29992;&#31163;&#31574;&#30053;&#26041;&#27861;Q-learning&#25110;&#22312;&#32447;&#31574;&#30053;&#26041;&#27861;Sarsa&#65288;0&#65289;&#26469;&#25511;&#21046;&#20004;&#20010;&#20851;&#38190;&#30340;&#36951;&#20256;&#31639;&#27861;&#65288;GA&#65289;&#36816;&#31639;&#31526;&#65306;&#29238;&#20195;&#36873;&#25321;&#26426;&#21046;&#21644;&#21464;&#24322;&#12290;&#22312;&#27599;&#19968;&#20195;&#20013;&#65292;RL&#20195;&#29702;&#30340;&#21160;&#20316;&#20915;&#23450;&#20102;&#36873;&#25321;&#26041;&#27861;&#65292;&#29238;&#20195;&#36873;&#25321;&#30340;&#27010;&#29575;&#21644;&#21518;&#20195;&#21464;&#24322;&#30340;&#27010;&#29575;&#12290;&#36825;&#20351;&#24471;RL&#20195;&#29702;&#33021;&#22815;&#26681;&#25454;&#20854;&#23398;&#20064;&#30340;&#31574;&#30053;&#21160;&#24577;&#35843;&#25972;&#36873;&#25321;&#21644;&#21464;&#24322;&#12290;&#30740;&#31350;&#32467;&#26524;&#31361;&#20986;&#20102;RL+GA&#26041;&#27861;&#22312;&#25913;&#36827;&#21407;&#22987;GA&#24615;&#33021;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#23427;&#20204;&#36824;&#35777;&#26126;&#20102;&#23427;&#23398;&#20064;&#21644;&#36866;&#24212;&#32676;&#20307;&#22810;&#26679;&#24615;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a reinforcement learning (RL) approach to address the challenges associated with configuring and optimizing genetic algorithms (GAs) for solving difficult combinatorial or non-linear problems. The proposed RL+GA method was specifically tested on the flow shop scheduling problem (FSP). The hybrid algorithm incorporates neural networks (NN) and uses the off-policy method Q-learning or the on-policy method Sarsa(0) to control two key genetic algorithm (GA) operators: parent selection mechanism and mutation. At each generation, the RL agent's action is determining the selection method, the probability of the parent selection and the probability of the offspring mutation. This allows the RL agent to dynamically adjust the selection and mutation based on its learned policy. The results of the study highlight the effectiveness of the RL+GA approach in improving the performance of the primitive GA. They also demonstrate its ability to learn and adapt from population diver
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#32463;&#36807;&#35757;&#32451;&#36827;&#34892;&#25972;&#25968;&#21152;&#27861;&#30340;&#21333;&#23618;Transformer&#27169;&#22411;&#30340;&#28145;&#20837;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#35813;&#27169;&#22411;&#23558;&#20219;&#21153;&#20998;&#20026;&#24182;&#34892;&#30340;&#12289;&#29305;&#23450;&#20110;&#25968;&#23383;&#30340;&#27969;&#65292;&#24182;&#38024;&#23545;&#19981;&#21516;&#30340;&#25968;&#23383;&#20301;&#32622;&#37319;&#29992;&#19981;&#21516;&#30340;&#31639;&#27861;&#65292;&#21516;&#26102;&#36824;&#21457;&#29616;&#20102;&#19968;&#31181;&#32597;&#35265;&#30340;&#39640;&#25439;&#22833;&#30340;&#20351;&#29992;&#24773;&#20917;&#12290;&#36825;&#20123;&#21457;&#29616;&#23545;&#20110;&#26426;&#21046;&#21487;&#35299;&#37322;&#24615;&#12289;&#20154;&#24037;&#26234;&#33021;&#23433;&#20840;&#24615;&#21644;&#23545;&#40784;&#24615;&#31561;&#26041;&#38754;&#30340;&#30740;&#31350;&#20855;&#26377;&#37325;&#35201;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2310.13121</link><description>&lt;p&gt;
&#29702;&#35299;Transformer&#20013;&#30340;&#21152;&#27861;
&lt;/p&gt;
&lt;p&gt;
Understanding Addition in Transformers. (arXiv:2310.13121v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13121
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#32463;&#36807;&#35757;&#32451;&#36827;&#34892;&#25972;&#25968;&#21152;&#27861;&#30340;&#21333;&#23618;Transformer&#27169;&#22411;&#30340;&#28145;&#20837;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#35813;&#27169;&#22411;&#23558;&#20219;&#21153;&#20998;&#20026;&#24182;&#34892;&#30340;&#12289;&#29305;&#23450;&#20110;&#25968;&#23383;&#30340;&#27969;&#65292;&#24182;&#38024;&#23545;&#19981;&#21516;&#30340;&#25968;&#23383;&#20301;&#32622;&#37319;&#29992;&#19981;&#21516;&#30340;&#31639;&#27861;&#65292;&#21516;&#26102;&#36824;&#21457;&#29616;&#20102;&#19968;&#31181;&#32597;&#35265;&#30340;&#39640;&#25439;&#22833;&#30340;&#20351;&#29992;&#24773;&#20917;&#12290;&#36825;&#20123;&#21457;&#29616;&#23545;&#20110;&#26426;&#21046;&#21487;&#35299;&#37322;&#24615;&#12289;&#20154;&#24037;&#26234;&#33021;&#23433;&#20840;&#24615;&#21644;&#23545;&#40784;&#24615;&#31561;&#26041;&#38754;&#30340;&#30740;&#31350;&#20855;&#26377;&#37325;&#35201;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20102;&#35299;&#20687;Transformer&#36825;&#26679;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20869;&#37096;&#24037;&#20316;&#26041;&#24335;&#23545;&#20110;&#20854;&#23433;&#20840;&#21644;&#36947;&#24503;&#20351;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#23545;&#32463;&#36807;&#35757;&#32451;&#36827;&#34892;&#25972;&#25968;&#21152;&#27861;&#30340;&#21333;&#23618;Transformer&#27169;&#22411;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;&#25105;&#20204;&#25581;&#31034;&#20102;&#35813;&#27169;&#22411;&#23558;&#20219;&#21153;&#20998;&#20026;&#24182;&#34892;&#30340;&#12289;&#29305;&#23450;&#20110;&#25968;&#23383;&#30340;&#27969;&#65292;&#24182;&#38024;&#23545;&#19981;&#21516;&#30340;&#25968;&#23383;&#20301;&#32622;&#37319;&#29992;&#19981;&#21516;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36824;&#21457;&#29616;&#35813;&#27169;&#22411;&#24320;&#22987;&#35745;&#31639;&#36739;&#26202;&#65292;&#20294;&#25191;&#34892;&#36895;&#24230;&#38750;&#24120;&#24555;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#20102;&#19968;&#31181;&#32597;&#35265;&#30340;&#39640;&#25439;&#22833;&#30340;&#20351;&#29992;&#24773;&#20917;&#65292;&#24182;&#20104;&#20197;&#35299;&#37322;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35814;&#32454;&#35299;&#37322;&#20102;&#35813;&#27169;&#22411;&#30340;&#31639;&#27861;&#12290;&#36825;&#20123;&#21457;&#29616;&#36890;&#36807;&#20005;&#26684;&#27979;&#35797;&#21644;&#25968;&#23398;&#24314;&#27169;&#24471;&#21040;&#20102;&#39564;&#35777;&#65292;&#23545;&#20110;&#26426;&#21046;&#21487;&#35299;&#37322;&#24615;&#12289;&#20154;&#24037;&#26234;&#33021;&#23433;&#20840;&#24615;&#21644;&#23545;&#40784;&#24615;&#31561;&#24191;&#27867;&#30740;&#31350;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20026;&#20998;&#26512;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#21644;&#22810;&#23618;Transformer&#27169;&#22411;&#25171;&#24320;&#20102;&#22823;&#38376;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the inner workings of machine learning models like Transformers is vital for their safe and ethical use. This paper presents an in-depth analysis of a one-layer Transformer model trained for integer addition. We reveal that the model divides the task into parallel, digit-specific streams and employs distinct algorithms for different digit positions. Our study also finds that the model starts calculations late but executes them rapidly. A rare use case with high loss is identified and explained. Overall, the model's algorithm is explained in detail. These findings are validated through rigorous testing and mathematical modeling, contributing to the broader works in Mechanistic Interpretability, AI safety, and alignment. Our approach opens the door for analyzing more complex tasks and multi-layer Transformer models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#26631;&#31614;&#27604;&#20363;&#20013;&#23398;&#20064;&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#36890;&#36807;&#20266;&#26631;&#31614;&#21270;&#21644;&#23884;&#20837;&#32454;&#21270;&#20004;&#20010;&#27493;&#39588;&#36845;&#20195;&#22320;&#25552;&#39640;&#26377;&#30417;&#30563;&#23398;&#20064;&#22120;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.08056</link><description>&lt;p&gt;
&#20174;&#26631;&#31614;&#27604;&#20363;&#20013;&#23398;&#20064;&#65306;&#36890;&#36807;&#20449;&#24565;&#20256;&#25773;&#23545;&#26377;&#30417;&#30563;&#23398;&#20064;&#22120;&#36827;&#34892;&#24341;&#23548;
&lt;/p&gt;
&lt;p&gt;
Learning from Label Proportions: Bootstrapping Supervised Learners via Belief Propagation. (arXiv:2310.08056v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08056
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#26631;&#31614;&#27604;&#20363;&#20013;&#23398;&#20064;&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#36890;&#36807;&#20266;&#26631;&#31614;&#21270;&#21644;&#23884;&#20837;&#32454;&#21270;&#20004;&#20010;&#27493;&#39588;&#36845;&#20195;&#22320;&#25552;&#39640;&#26377;&#30417;&#30563;&#23398;&#20064;&#22120;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#31614;&#27604;&#20363;&#23398;&#20064;&#65288;LLP&#65289;&#26159;&#19968;&#20010;&#23398;&#20064;&#38382;&#39064;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#21482;&#26377;&#38024;&#23545;&#19968;&#32452;&#23454;&#20363;&#65288;&#31216;&#20026;&#21253;&#65289;&#30340;&#32858;&#21512;&#32423;&#21035;&#26631;&#31614;&#21487;&#29992;&#65292;&#24182;&#19988;&#30446;&#30340;&#26159;&#22312;&#27979;&#35797;&#25968;&#25454;&#30340;&#23454;&#20363;&#32423;&#21035;&#19978;&#33719;&#24471;&#26368;&#20339;&#24615;&#33021;&#12290;&#36825;&#31181;&#35774;&#32622;&#22312;&#24191;&#21578;&#21644;&#21307;&#23398;&#31561;&#39046;&#22495;&#30001;&#20110;&#38544;&#31169;&#32771;&#34385;&#32780;&#20986;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#31639;&#27861;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#23427;&#36890;&#36807;&#20004;&#20010;&#20027;&#35201;&#27493;&#39588;&#36827;&#34892;&#36845;&#20195;&#12290;&#22312;&#27599;&#27425;&#36845;&#20195;&#30340;&#31532;&#19968;&#27493;&#65288;&#20266;&#26631;&#31614;&#21270;&#65289;&#20013;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#22522;&#20110;&#20108;&#36827;&#21046;&#23454;&#20363;&#26631;&#31614;&#30340;&#21513;&#24067;&#26031;&#20998;&#24067;&#65292;&#35813;&#20998;&#24067;&#36890;&#36807;&#20197;&#19979;&#32422;&#26463;&#23558;covariate&#20449;&#24687;&#65288;&#21327;&#21464;&#37327;&#20449;&#24687;&#65289;&#21512;&#24182;&#36827;&#21435;&#65306;&#20855;&#26377;&#30456;&#20284;covariates&#30340;&#23454;&#20363;&#24212;&#35813;&#20855;&#26377;&#30456;&#20284;&#30340;&#26631;&#31614;&#65292;&#24182;&#19988;&#36890;&#36807;&#21253;&#32423;&#21035;&#30340;&#32858;&#21512;&#26631;&#31614;&#26469;&#32508;&#21512;covariate&#20449;&#24687;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#20449;&#24565;&#20256;&#25773;&#65288;BP&#65289;&#26469;&#36793;&#32536;&#21270;&#21513;&#24067;&#26031;&#20998;&#24067;&#20197;&#33719;&#24471;&#20266;&#26631;&#31614;&#12290;&#22312;&#31532;&#20108;&#27493;&#65288;&#23884;&#20837;&#32454;&#21270;&#65289;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20266;&#26631;&#31614;&#20026;&#23398;&#20064;&#22120;&#25552;&#20379;&#30417;&#30563;&#65292;&#20174;&#32780;&#33719;&#24471;&#26356;&#22909;&#30340;&#23884;&#20837;&#12290;&#27492;&#21518;&#65292;&#25105;&#20204;&#23545;&#36825;&#20004;&#20010;&#27493;&#39588;&#36827;&#34892;&#36845;&#20195;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning from Label Proportions (LLP) is a learning problem where only aggregate level labels are available for groups of instances, called bags, during training, and the aim is to get the best performance at the instance-level on the test data. This setting arises in domains like advertising and medicine due to privacy considerations. We propose a novel algorithmic framework for this problem that iteratively performs two main steps. For the first step (Pseudo Labeling) in every iteration, we define a Gibbs distribution over binary instance labels that incorporates a) covariate information through the constraint that instances with similar covariates should have similar labels and b) the bag level aggregated label. We then use Belief Propagation (BP) to marginalize the Gibbs distribution to obtain pseudo labels. In the second step (Embedding Refinement), we use the pseudo labels to provide supervision for a learner that yields a better embedding. Further, we iterate on the two steps ag
&lt;/p&gt;</description></item><item><title>Skipper&#26159;&#19968;&#20010;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#65292;&#21033;&#29992;&#26102;&#31354;&#25277;&#35937;&#26469;&#22312;&#26032;&#24773;&#22659;&#20013;&#25512;&#24191;&#23398;&#21040;&#30340;&#25216;&#33021;&#12290;&#23427;&#33258;&#21160;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#23376;&#20219;&#21153;&#65292;&#23454;&#29616;&#31232;&#30095;&#20915;&#31574;&#21644;&#23545;&#29615;&#22659;&#30456;&#20851;&#37096;&#20998;&#30340;&#19987;&#27880;&#35745;&#31639;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Skipper&#22312;&#38646;&#26679;&#26412;&#27867;&#21270;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2310.00229</link><description>&lt;p&gt;
&#22312;&#35268;&#21010;&#20013;&#32467;&#21512;&#31354;&#38388;&#21644;&#26102;&#38388;&#25277;&#35937;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Combining Spatial and Temporal Abstraction in Planning for Better Generalization. (arXiv:2310.00229v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00229
&lt;/p&gt;
&lt;p&gt;
Skipper&#26159;&#19968;&#20010;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#65292;&#21033;&#29992;&#26102;&#31354;&#25277;&#35937;&#26469;&#22312;&#26032;&#24773;&#22659;&#20013;&#25512;&#24191;&#23398;&#21040;&#30340;&#25216;&#33021;&#12290;&#23427;&#33258;&#21160;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#23376;&#20219;&#21153;&#65292;&#23454;&#29616;&#31232;&#30095;&#20915;&#31574;&#21644;&#23545;&#29615;&#22659;&#30456;&#20851;&#37096;&#20998;&#30340;&#19987;&#27880;&#35745;&#31639;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Skipper&#22312;&#38646;&#26679;&#26412;&#27867;&#21270;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#20154;&#31867;&#26377;&#24847;&#35782;&#35268;&#21010;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Skipper&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;&#26102;&#31354;&#25277;&#35937;&#26469;&#25512;&#24191;&#22312;&#26032;&#24773;&#22659;&#20013;&#23398;&#21040;&#30340;&#25216;&#33021;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#12290;&#23427;&#33258;&#21160;&#23558;&#32473;&#23450;&#20219;&#21153;&#20998;&#35299;&#20026;&#26356;&#23567;&#12289;&#26356;&#21487;&#31649;&#29702;&#30340;&#23376;&#20219;&#21153;&#65292;&#20174;&#32780;&#23454;&#29616;&#31232;&#30095;&#20915;&#31574;&#21644;&#23545;&#29615;&#22659;&#30456;&#20851;&#37096;&#20998;&#30340;&#19987;&#27880;&#35745;&#31639;&#12290;&#36825;&#20381;&#36182;&#20110;&#20174;&#22238;&#28335;&#20013;&#23398;&#20064;&#24471;&#21040;&#30340;&#34920;&#31034;&#20026;&#26377;&#21521;&#22270;&#30340;&#25277;&#35937;&#20195;&#29702;&#38382;&#39064;&#30340;&#25552;&#21462;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#22312;&#36866;&#24403;&#30340;&#20551;&#35774;&#19979;&#25552;&#20379;&#20102;&#24615;&#33021;&#20445;&#35777;&#65292;&#24182;&#30830;&#23450;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21738;&#20123;&#26041;&#38754;&#26377;&#26395;&#25552;&#20379;&#24110;&#21161;&#12290;&#38024;&#23545;&#27867;&#21270;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;Skipper&#22312;&#38646;&#26679;&#26412;&#27867;&#21270;&#26041;&#38754;&#19982;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#20998;&#23618;&#35268;&#21010;&#26041;&#27861;&#30456;&#27604;&#30340;&#26174;&#33879;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspired by human conscious planning, we propose Skipper, a model-based reinforcement learning agent utilizing spatio-temporal abstractions to generalize learned skills in novel situations. It automatically decomposes the given task into smaller, more manageable subtasks, and hence enables sparse decision-making and focused computation on the relevant parts of the environment. This relies on the extraction of an abstracted proxy problem represented as a directed graph, in which vertices and edges are learned end-to-end from hindsight. Our theoretical analyses provide performance guarantees under appropriate assumptions and establish where our approach is expected to be helpful. Generalization-focused experiments validate Skipper's significant advantage in zero-shot generalization, compared to existing state-of-the-art hierarchical planning methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#31995;&#32479;&#22320;&#32771;&#23519;&#20102;&#28608;&#27963;&#36335;&#24452;&#20462;&#22797;&#20013;&#30340;&#26041;&#27861;&#32454;&#33410;&#23545;&#35821;&#35328;&#27169;&#22411;&#35299;&#37322;&#24615;&#32467;&#26524;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#26368;&#20339;&#23454;&#36341;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2309.16042</link><description>&lt;p&gt;
&#12298;&#35821;&#35328;&#27169;&#22411;&#20013;&#28608;&#27963;&#36335;&#24452;&#20462;&#22797;&#30340;&#26368;&#20339;&#23454;&#36341;&#65306;&#24230;&#37327;&#21644;&#26041;&#27861;&#12299;&#30340;&#35770;&#25991;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Towards Best Practices of Activation Patching in Language Models: Metrics and Methods. (arXiv:2309.16042v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16042
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#31995;&#32479;&#22320;&#32771;&#23519;&#20102;&#28608;&#27963;&#36335;&#24452;&#20462;&#22797;&#20013;&#30340;&#26041;&#27861;&#32454;&#33410;&#23545;&#35821;&#35328;&#27169;&#22411;&#35299;&#37322;&#24615;&#32467;&#26524;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#26368;&#20339;&#23454;&#36341;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#26800;&#35299;&#37322;&#24615;&#26088;&#22312;&#29702;&#35299;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20869;&#37096;&#26426;&#21046;&#65292;&#20854;&#20013;&#23450;&#20301;-&#35782;&#21035;&#37325;&#35201;&#30340;&#27169;&#22411;&#32452;&#20214;&#26159;&#20851;&#38190;&#27493;&#39588;&#12290;&#28608;&#27963;&#36335;&#24452;&#20462;&#22797;&#65292;&#20063;&#31216;&#20026;&#22240;&#26524;&#36861;&#36394;&#25110;&#20132;&#25442;&#24178;&#39044;&#65292;&#26159;&#23436;&#25104;&#36825;&#19968;&#20219;&#21153;&#30340;&#26631;&#20934;&#25216;&#26415;&#65292;&#20294;&#25991;&#29486;&#20013;&#23384;&#22312;&#35768;&#22810;&#21464;&#20307;&#65292;&#23545;&#36229;&#21442;&#25968;&#25110;&#26041;&#27861;&#36873;&#25321;&#32570;&#20047;&#19968;&#33268;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#32771;&#23519;&#20102;&#28608;&#27963;&#36335;&#24452;&#20462;&#22797;&#20013;&#30340;&#26041;&#27861;&#32454;&#33410;&#23545;&#32467;&#26524;&#30340;&#24433;&#21709;&#65292;&#21253;&#25324;&#35780;&#20272;&#25351;&#26631;&#21644;&#25439;&#22351;&#26041;&#27861;&#12290;&#22312;&#35821;&#35328;&#27169;&#22411;&#30340;&#23450;&#20301;&#21644;&#30005;&#36335;&#21457;&#29616;&#30340;&#20960;&#31181;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#19981;&#21516;&#30340;&#36229;&#21442;&#25968;&#21487;&#33021;&#23548;&#33268;&#19981;&#21516;&#30340;&#35299;&#37322;&#32467;&#26524;&#12290;&#36890;&#36807;&#32463;&#39564;&#35266;&#23519;&#25903;&#25345;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20026;&#20160;&#20040;&#26576;&#20123;&#25351;&#26631;&#25110;&#26041;&#27861;&#21487;&#33021;&#26356;&#21463;&#27426;&#36814;&#30340;&#27010;&#24565;&#24615;&#35770;&#35777;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#28608;&#27963;&#36335;&#24452;&#20462;&#22797;&#30340;&#26368;&#20339;&#23454;&#36341;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mechanistic interpretability seeks to understand the internal mechanisms of machine learning models, where localization -- identifying the important model components -- is a key step. Activation patching, also known as causal tracing or interchange intervention, is a standard technique for this task (Vig et al., 2020), but the literature contains many variants with little consensus on the choice of hyperparameters or methodology. In this work, we systematically examine the impact of methodological details in activation patching, including evaluation metrics and corruption methods. In several settings of localization and circuit discovery in language models, we find that varying these hyperparameters could lead to disparate interpretability results. Backed by empirical observations, we give conceptual arguments for why certain metrics or methods may be preferred. Finally, we provide recommendations for the best practices of activation patching going forwards.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20869;&#23481;&#23457;&#26597;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#22522;&#20110;&#35268;&#21017;&#30340;&#31038;&#21306;&#23457;&#26597;&#21644;&#26377;&#23475;&#20869;&#23481;&#26816;&#27979;&#26041;&#38754;&#20855;&#26377;&#24456;&#22909;&#30340;&#25928;&#26524;&#65292;&#22312;&#26377;&#23475;&#20869;&#23481;&#26816;&#27979;&#26041;&#38754;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#20998;&#31867;&#22120;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#27169;&#22411;&#35268;&#27169;&#30340;&#22686;&#21152;&#23545;&#26377;&#23475;&#20869;&#23481;&#26816;&#27979;&#30340;&#25913;&#36827;&#25928;&#26524;&#24456;&#23567;&#12290;</title><link>http://arxiv.org/abs/2309.14517</link><description>&lt;p&gt;
&#27880;&#24847;&#35328;&#36766;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#20869;&#23481;&#23457;&#26597;
&lt;/p&gt;
&lt;p&gt;
Watch Your Language: Large Language Models and Content Moderation. (arXiv:2309.14517v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14517
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20869;&#23481;&#23457;&#26597;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#22522;&#20110;&#35268;&#21017;&#30340;&#31038;&#21306;&#23457;&#26597;&#21644;&#26377;&#23475;&#20869;&#23481;&#26816;&#27979;&#26041;&#38754;&#20855;&#26377;&#24456;&#22909;&#30340;&#25928;&#26524;&#65292;&#22312;&#26377;&#23475;&#20869;&#23481;&#26816;&#27979;&#26041;&#38754;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#20998;&#31867;&#22120;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#27169;&#22411;&#35268;&#27169;&#30340;&#22686;&#21152;&#23545;&#26377;&#23475;&#20869;&#23481;&#26816;&#27979;&#30340;&#25913;&#36827;&#25928;&#26524;&#24456;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21464;&#24471;&#38750;&#24120;&#21463;&#27426;&#36814;&#12290;&#22522;&#20110;&#25991;&#26412;&#30340;&#20869;&#23481;&#23457;&#26597;&#26159;&#20854;&#20013;&#19968;&#20010;&#21463;&#21040;&#36817;&#26399;&#28909;&#24773;&#20851;&#27880;&#30340;LLM&#24212;&#29992;&#26696;&#20363;&#65292;&#28982;&#32780;&#65292;&#40092;&#26377;&#30740;&#31350;&#35843;&#26597;LLMs&#22312;&#20869;&#23481;&#23457;&#26597;&#35774;&#32622;&#20013;&#30340;&#34920;&#29616;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#19968;&#22871;&#29616;&#20195;&#12289;&#21830;&#19994;&#21270;&#30340;LLMs&#65288;GPT-3&#12289;GPT-3.5&#12289;GPT-4&#65289;&#22312;&#20004;&#20010;&#24120;&#35265;&#30340;&#20869;&#23481;&#23457;&#26597;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65306;&#22522;&#20110;&#35268;&#21017;&#30340;&#31038;&#21306;&#23457;&#26597;&#21644;&#26377;&#23475;&#20869;&#23481;&#26816;&#27979;&#12290;&#23545;&#20110;&#22522;&#20110;&#35268;&#21017;&#30340;&#31038;&#21306;&#23457;&#26597;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;95&#20010;LLM&#23457;&#26597;&#24341;&#25806;&#65292;&#24182;&#20351;&#29992;95&#20010;Reddit&#23376;&#31038;&#21306;&#30340;&#35268;&#21017;&#36827;&#34892;&#25351;&#23548;&#65292;&#21457;&#29616;LLMs&#22312;&#35768;&#22810;&#31038;&#21306;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#23457;&#26597;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#23454;&#29616;&#20102;&#20013;&#20301;&#25968;&#20934;&#30830;&#29575;&#20026;64%&#21644;&#20013;&#20301;&#25968;&#31934;&#30830;&#24230;&#20026;83%&#12290;&#22312;&#26377;&#23475;&#20869;&#23481;&#26816;&#27979;&#26041;&#38754;&#65292;&#25105;&#20204;&#21457;&#29616;LLMs&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#21830;&#19994;&#21487;&#29992;&#30340;&#26377;&#23475;&#24615;&#20998;&#31867;&#22120;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#26368;&#36817;&#27169;&#22411;&#35268;&#27169;&#30340;&#22686;&#21152;&#23545;&#26377;&#23475;&#20869;&#23481;&#26816;&#27979;&#20960;&#20046;&#27809;&#26377;&#24102;&#26469;&#26126;&#26174;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have exploded in popularity due to their ability to perform a wide array of natural language tasks. Text-based content moderation is one LLM use case that has received recent enthusiasm, however, there is little research investigating how LLMs perform in content moderation settings. In this work, we evaluate a suite of modern, commercial LLMs (GPT-3, GPT-3.5, GPT-4) on two common content moderation tasks: rule-based community moderation and toxic content detection. For rule-based community moderation, we construct 95 LLM moderation-engines prompted with rules from 95 Reddit subcommunities and find that LLMs can be effective at rule-based moderation for many communities, achieving a median accuracy of 64% and a median precision of 83%. For toxicity detection, we find that LLMs significantly outperform existing commercially available toxicity classifiers. However, we also find that recent increases in model size add only marginal benefit to toxicity detection
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;GPT&#30340;&#25991;&#26412;&#35268;&#33539;&#21270;&#20219;&#21153;&#12290;&#36890;&#36807;&#32467;&#21512;&#33258;&#27965;&#25512;&#29702;&#21644;&#22522;&#20110;&#35821;&#35328;&#30693;&#35782;&#30340;&#25552;&#31034;&#24037;&#31243;&#65292;&#25105;&#20204;&#21457;&#29616;&#22522;&#20110;LLM&#30340;&#25991;&#26412;&#35268;&#33539;&#21270;&#30340;&#38169;&#35823;&#29575;&#27604;&#39030;&#32423;&#35268;&#33539;&#21270;&#31995;&#32479;&#20302;&#32422;40&#65285;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#23545;&#38169;&#35823;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#20256;&#32479;&#30340;&#25991;&#26412;&#35268;&#33539;&#21270;&#20219;&#21153;&#35774;&#35745;&#23384;&#22312;&#20851;&#38190;&#38480;&#21046;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#20197;&#35782;&#21035;&#22522;&#20110;GPT&#30340;&#25991;&#26412;&#35268;&#33539;&#21270;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#12290;&#36825;&#20026;&#26410;&#26469;&#30340;&#24037;&#20316;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;</title><link>http://arxiv.org/abs/2309.13426</link><description>&lt;p&gt;
&#38386;&#35848;&#20196;&#20154;&#26080;&#32842;&#30340;&#38382;&#39064;&#65306;&#30740;&#31350;&#22522;&#20110;GPT&#30340;&#25991;&#26412;&#35268;&#33539;&#21270;
&lt;/p&gt;
&lt;p&gt;
A Chat About Boring Problems: Studying GPT-based text normalization. (arXiv:2309.13426v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13426
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;GPT&#30340;&#25991;&#26412;&#35268;&#33539;&#21270;&#20219;&#21153;&#12290;&#36890;&#36807;&#32467;&#21512;&#33258;&#27965;&#25512;&#29702;&#21644;&#22522;&#20110;&#35821;&#35328;&#30693;&#35782;&#30340;&#25552;&#31034;&#24037;&#31243;&#65292;&#25105;&#20204;&#21457;&#29616;&#22522;&#20110;LLM&#30340;&#25991;&#26412;&#35268;&#33539;&#21270;&#30340;&#38169;&#35823;&#29575;&#27604;&#39030;&#32423;&#35268;&#33539;&#21270;&#31995;&#32479;&#20302;&#32422;40&#65285;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#23545;&#38169;&#35823;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#20256;&#32479;&#30340;&#25991;&#26412;&#35268;&#33539;&#21270;&#20219;&#21153;&#35774;&#35745;&#23384;&#22312;&#20851;&#38190;&#38480;&#21046;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#20197;&#35782;&#21035;&#22522;&#20110;GPT&#30340;&#25991;&#26412;&#35268;&#33539;&#21270;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#12290;&#36825;&#20026;&#26410;&#26469;&#30340;&#24037;&#20316;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#35268;&#33539;&#21270;-&#23558;&#25991;&#26412;&#20174;&#20070;&#38754;&#24418;&#24335;&#36716;&#21270;&#20026;&#21475;&#35821;&#24418;&#24335;-&#36890;&#24120;&#34987;&#35748;&#20026;&#26159;&#35821;&#35328;&#27169;&#22411;&#38754;&#20020;&#30340;&#19968;&#20010;&#19981;&#23436;&#21892;&#30340;&#20219;&#21153;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19982;&#20043;&#30456;&#21453;&#30340;&#35266;&#28857;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#23569;&#26679;&#26412;&#24773;&#22659;&#19979;&#36827;&#34892;&#25991;&#26412;&#35268;&#33539;&#21270;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#32467;&#21512;&#33258;&#27965;&#25512;&#29702;&#21644;&#22522;&#20110;&#35821;&#35328;&#30693;&#35782;&#30340;&#25552;&#31034;&#24037;&#31243;&#65292;&#25105;&#20204;&#21457;&#29616;&#22522;&#20110;LLM&#30340;&#25991;&#26412;&#35268;&#33539;&#21270;&#30340;&#38169;&#35823;&#29575;&#27604;&#39030;&#32423;&#35268;&#33539;&#21270;&#31995;&#32479;&#20302;&#32422;40&#65285;&#12290;&#36827;&#19968;&#27493;&#30340;&#38169;&#35823;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#27880;&#24847;&#21040;&#20256;&#32479;&#30340;&#25991;&#26412;&#35268;&#33539;&#21270;&#20219;&#21153;&#35774;&#35745;&#30340;&#20851;&#38190;&#38480;&#21046;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#25991;&#26412;&#35268;&#33539;&#21270;&#38169;&#35823;&#20998;&#31867;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;GPT-3.5-Turbo&#21644;GPT-4.0&#30340;&#32467;&#26524;&#20013;&#12290;&#36890;&#36807;&#36825;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#21487;&#20197;&#35782;&#21035;&#20986;&#22522;&#20110;GPT&#30340;&#25991;&#26412;&#35268;&#33539;&#21270;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#65292;&#20174;&#32780;&#20026;&#26410;&#26469;&#30340;&#24037;&#20316;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text normalization - the conversion of text from written to spoken form - is traditionally assumed to be an ill-formed task for language models. In this work, we argue otherwise. We empirically show the capacity of Large-Language Models (LLM) for text normalization in few-shot scenarios. Combining self-consistency reasoning with linguistic-informed prompt engineering, we find LLM based text normalization to achieve error rates around 40\% lower than top normalization systems. Further, upon error analysis, we note key limitations in the conventional design of text normalization tasks. We create a new taxonomy of text normalization errors and apply it to results from GPT-3.5-Turbo and GPT-4.0. Through this new framework, we can identify strengths and weaknesses of GPT-based TN, opening opportunities for future work.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#29983;&#25104;&#27169;&#22411;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#29575;&#27874;&#21160;&#35780;&#20272;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#26041;&#27861;(PFAMI)&#65292;&#36890;&#36807;&#26816;&#27979;&#27010;&#29575;&#20998;&#24067;&#30340;&#27874;&#21160;&#24615;&#26469;&#25512;&#26029;&#27169;&#22411;&#20013;&#26159;&#21542;&#23384;&#22312;&#26576;&#26465;&#35757;&#32451;&#35760;&#24405;&#30340;&#25104;&#21592;&#36523;&#20221;&#12290;</title><link>http://arxiv.org/abs/2308.12143</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#27874;&#21160;&#30340;&#29983;&#25104;&#27169;&#22411;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Probabilistic Fluctuation based Membership Inference Attack for Generative Models. (arXiv:2308.12143v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12143
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#29983;&#25104;&#27169;&#22411;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#29575;&#27874;&#21160;&#35780;&#20272;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#26041;&#27861;(PFAMI)&#65292;&#36890;&#36807;&#26816;&#27979;&#27010;&#29575;&#20998;&#24067;&#30340;&#27874;&#21160;&#24615;&#26469;&#25512;&#26029;&#27169;&#22411;&#20013;&#26159;&#21542;&#23384;&#22312;&#26576;&#26465;&#35757;&#32451;&#35760;&#24405;&#30340;&#25104;&#21592;&#36523;&#20221;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;(MIA)&#36890;&#36807;&#26597;&#35810;&#27169;&#22411;&#26469;&#35782;&#21035;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#38598;&#20013;&#26159;&#21542;&#23384;&#22312;&#26576;&#26465;&#35760;&#24405;&#12290;&#23545;&#32463;&#20856;&#20998;&#31867;&#27169;&#22411;&#30340;MIA&#24050;&#26377;&#24456;&#22810;&#30740;&#31350;&#65292;&#26368;&#36817;&#30340;&#24037;&#20316;&#24320;&#22987;&#25506;&#32034;&#22914;&#20309;&#23558;MIA&#24212;&#29992;&#21040;&#29983;&#25104;&#27169;&#22411;&#19978;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#29616;&#26377;&#30340;&#38754;&#21521;&#29983;&#25104;&#27169;&#22411;&#30340;MIA&#20027;&#35201;&#20381;&#36182;&#20110;&#30446;&#26631;&#27169;&#22411;&#30340;&#36807;&#25311;&#21512;&#29616;&#35937;&#12290;&#28982;&#32780;&#65292;&#36807;&#25311;&#21512;&#21487;&#20197;&#36890;&#36807;&#37319;&#29992;&#21508;&#31181;&#27491;&#21017;&#21270;&#25216;&#26415;&#26469;&#36991;&#20813;&#65292;&#32780;&#29616;&#26377;&#30340;MIA&#22312;&#23454;&#36341;&#20013;&#34920;&#29616;&#19981;&#20339;&#12290;&#19982;&#36807;&#25311;&#21512;&#19981;&#21516;&#65292;&#35760;&#24518;&#23545;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23454;&#29616;&#26368;&#20339;&#24615;&#33021;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#20351;&#20854;&#25104;&#20026;&#19968;&#31181;&#26356;&#20026;&#26222;&#36941;&#30340;&#29616;&#35937;&#12290;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#35760;&#24518;&#23548;&#33268;&#29983;&#25104;&#35760;&#24405;&#30340;&#27010;&#29575;&#20998;&#24067;&#21576;&#29616;&#20986;&#22686;&#38271;&#30340;&#36235;&#21183;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#27874;&#21160;&#30340;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#26041;&#27861;(PFAMI)&#65292;&#23427;&#26159;&#19968;&#31181;&#40657;&#30418;MIA&#65292;&#36890;&#36807;&#26816;&#27979;&#27010;&#29575;&#27874;&#21160;&#26469;&#25512;&#26029;&#25104;&#21592;&#36523;&#20221;&#12290;
&lt;/p&gt;
&lt;p&gt;
Membership Inference Attack (MIA) identifies whether a record exists in a machine learning model's training set by querying the model. MIAs on the classic classification models have been well-studied, and recent works have started to explore how to transplant MIA onto generative models. Our investigation indicates that existing MIAs designed for generative models mainly depend on the overfitting in target models. However, overfitting can be avoided by employing various regularization techniques, whereas existing MIAs demonstrate poor performance in practice. Unlike overfitting, memorization is essential for deep learning models to attain optimal performance, making it a more prevalent phenomenon. Memorization in generative models leads to an increasing trend in the probability distribution of generating records around the member record. Therefore, we propose a Probabilistic Fluctuation Assessing Membership Inference Attack (PFAMI), a black-box MIA that infers memberships by detecting t
&lt;/p&gt;</description></item><item><title>&#27602;&#31661;&#34521;&#26159;&#19968;&#31181;&#26080;&#26631;&#31614;&#25915;&#20987;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#35757;&#32451;&#25968;&#25454;&#65292;&#21482;&#38656;&#35201;&#30446;&#26631;&#31867;&#21035;&#30340;&#30693;&#35782;&#12290;&#23427;&#20855;&#26377;&#20302;&#20013;&#27602;&#29575;&#21644;&#39640;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.09487</link><description>&lt;p&gt;
&#27602;&#31661;&#34521;&#65306;&#19968;&#31181;&#22312;&#27809;&#26377;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#20855;&#26377;&#20302;&#20013;&#27602;&#29575;&#21644;&#39640;&#25915;&#20987;&#25104;&#21151;&#29575;&#30340;&#26080;&#26631;&#31614;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Poison Dart Frog: A Clean-Label Attack with Low Poisoning Rate and High Attack Success Rate in the Absence of Training Data. (arXiv:2308.09487v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09487
&lt;/p&gt;
&lt;p&gt;
&#27602;&#31661;&#34521;&#26159;&#19968;&#31181;&#26080;&#26631;&#31614;&#25915;&#20987;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#35757;&#32451;&#25968;&#25454;&#65292;&#21482;&#38656;&#35201;&#30446;&#26631;&#31867;&#21035;&#30340;&#30693;&#35782;&#12290;&#23427;&#20855;&#26377;&#20302;&#20013;&#27602;&#29575;&#21644;&#39640;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25104;&#21151;&#21457;&#21160;&#21518;&#38376;&#25915;&#20987;&#65292;&#27880;&#20837;&#30340;&#25968;&#25454;&#38656;&#35201;&#34987;&#27491;&#30830;&#26631;&#35760;&#65292;&#21542;&#21017;&#65292;&#21363;&#20351;&#26159;&#22522;&#26412;&#30340;&#25968;&#25454;&#36807;&#28388;&#22120;&#20063;&#33021;&#36731;&#26131;&#26816;&#27979;&#20986;&#26469;&#12290;&#22240;&#27492;&#65292;&#24341;&#20837;&#20102;&#26080;&#26631;&#31614;&#25915;&#20987;&#30340;&#27010;&#24565;&#65292;&#36825;&#31181;&#25915;&#20987;&#26356;&#21152;&#21361;&#38505;&#65292;&#22240;&#20026;&#23427;&#19981;&#38656;&#35201;&#25913;&#21464;&#27880;&#20837;&#25968;&#25454;&#30340;&#26631;&#31614;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#29616;&#26377;&#30340;&#26080;&#26631;&#31614;&#21518;&#38376;&#25915;&#20987;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#23545;&#25972;&#20010;&#35757;&#32451;&#38598;&#25110;&#20854;&#20013;&#19968;&#37096;&#20998;&#30340;&#29702;&#35299;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#25915;&#20987;&#32773;&#24456;&#38590;&#25317;&#26377;&#36825;&#20123;&#29702;&#35299;&#65292;&#22240;&#20026;&#35757;&#32451;&#25968;&#25454;&#38598;&#36890;&#24120;&#26469;&#33258;&#22810;&#20010;&#29420;&#31435;&#30340;&#26469;&#28304;&#12290;&#19982;&#25152;&#26377;&#24403;&#21069;&#30340;&#26080;&#26631;&#31614;&#25915;&#20987;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#26631;&#31614;&#26041;&#27861;&#65292;&#21517;&#20026;&#8220;&#27602;&#31661;&#34521;&#8221;&#12290;&#27602;&#31661;&#34521;&#19981;&#38656;&#35201;&#35775;&#38382;&#20219;&#20309;&#35757;&#32451;&#25968;&#25454;&#65292;&#21482;&#38656;&#35201;&#20102;&#35299;&#25915;&#20987;&#30446;&#26631;&#31867;&#21035;&#65292;&#27604;&#22914;&#8220;&#34521;&#8221;&#12290;&#22312;CIFAR10&#12289;Tiny-ImageNet&#21644;TSRD&#19978;&#65292;&#20165;&#38656;&#35201;&#20998;&#21035;&#21344;&#35757;&#32451;&#38598;&#22823;&#23567;&#30340;0.1%&#12289;0.025%&#21644;0.4%&#30340;&#20013;&#27602;&#29575;&#65292;&#27602;&#31661;&#34521;&#23601;&#33021;&#21462;&#24471;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;
To successfully launch backdoor attacks, injected data needs to be correctly labeled; otherwise, they can be easily detected by even basic data filters. Hence, the concept of clean-label attacks was introduced, which is more dangerous as it doesn't require changing the labels of injected data. To the best of our knowledge, the existing clean-label backdoor attacks largely relies on an understanding of the entire training set or a portion of it. However, in practice, it is very difficult for attackers to have it because of training datasets often collected from multiple independent sources. Unlike all current clean-label attacks, we propose a novel clean label method called 'Poison Dart Frog'. Poison Dart Frog does not require access to any training data; it only necessitates knowledge of the target class for the attack, such as 'frog'. On CIFAR10, Tiny-ImageNet, and TSRD, with a mere 0.1\%, 0.025\%, and 0.4\% poisoning rate of the training set size, respectively, Poison Dart Frog achie
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;IDE&#20869;&#29983;&#25104;&#24335;&#20449;&#24687;&#25903;&#25345;&#65292;&#36890;&#36807;&#22312;IDE&#20013;&#20869;&#23884;&#23545;&#35805;&#24335;&#29992;&#25143;&#30028;&#38754;&#65292;&#21521;&#24320;&#21457;&#32773;&#25552;&#20379;&#20851;&#20110;&#20195;&#30721;&#29702;&#35299;&#30340;&#24110;&#21161;&#12290;&#36890;&#36807;&#26597;&#35810;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#35299;&#37322;&#20195;&#30721;&#12289;&#25552;&#20379;API&#35843;&#29992;&#30340;&#35814;&#32454;&#20449;&#24687;&#12289;&#35299;&#37322;&#29305;&#23450;&#39046;&#22495;&#26415;&#35821;&#20197;&#21450;&#20026;API&#25552;&#20379;&#20351;&#29992;&#31034;&#20363;&#65292;&#35813;&#31995;&#32479;&#22312;&#29992;&#25143;&#30740;&#31350;&#20013;&#33719;&#24471;&#20102;&#31215;&#26497;&#30340;&#35780;&#20215;&#12290;</title><link>http://arxiv.org/abs/2307.08177</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;IDE&#20869;&#29983;&#25104;&#24335;&#20449;&#24687;&#25903;&#25345;
&lt;/p&gt;
&lt;p&gt;
In-IDE Generation-based Information Support with a Large Language Model. (arXiv:2307.08177v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08177
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;IDE&#20869;&#29983;&#25104;&#24335;&#20449;&#24687;&#25903;&#25345;&#65292;&#36890;&#36807;&#22312;IDE&#20013;&#20869;&#23884;&#23545;&#35805;&#24335;&#29992;&#25143;&#30028;&#38754;&#65292;&#21521;&#24320;&#21457;&#32773;&#25552;&#20379;&#20851;&#20110;&#20195;&#30721;&#29702;&#35299;&#30340;&#24110;&#21161;&#12290;&#36890;&#36807;&#26597;&#35810;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#35299;&#37322;&#20195;&#30721;&#12289;&#25552;&#20379;API&#35843;&#29992;&#30340;&#35814;&#32454;&#20449;&#24687;&#12289;&#35299;&#37322;&#29305;&#23450;&#39046;&#22495;&#26415;&#35821;&#20197;&#21450;&#20026;API&#25552;&#20379;&#20351;&#29992;&#31034;&#20363;&#65292;&#35813;&#31995;&#32479;&#22312;&#29992;&#25143;&#30740;&#31350;&#20013;&#33719;&#24471;&#20102;&#31215;&#26497;&#30340;&#35780;&#20215;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#20195;&#30721;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#23588;&#20854;&#26159;&#22312;&#26032;&#30340;&#21644;&#22797;&#26434;&#30340;&#24320;&#21457;&#29615;&#22659;&#20013;&#24037;&#20316;&#26102;&#12290;&#20195;&#30721;&#27880;&#37322;&#21644;&#25991;&#26723;&#21487;&#20197;&#24110;&#21161;&#65292;&#20294;&#24448;&#24448;&#31232;&#32570;&#25110;&#38590;&#20197;&#23548;&#33322;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#27491;&#22312;&#24443;&#24213;&#25913;&#21464;&#32534;&#20889;&#20195;&#30721;&#30340;&#36807;&#31243;&#12290;&#23427;&#20204;&#33021;&#21542;&#23545;&#29702;&#35299;&#20195;&#30721;&#25552;&#20379;&#21516;&#26679;&#30340;&#24110;&#21161;&#65311;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#25506;&#32034;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#30452;&#25509;&#20869;&#23884;&#24335;&#23545;&#35805;&#24335;&#29992;&#25143;&#30028;&#38754;&#65292;&#24182;&#38024;&#23545;&#20195;&#30721;&#29702;&#35299;&#36827;&#34892;&#20102;&#20248;&#21270;&#12290;&#25105;&#20204;&#30340;IDE&#25554;&#20214;&#36890;&#36807;&#22235;&#20010;&#39640;&#32423;&#35831;&#27714;&#65288;&#26080;&#38656;&#29992;&#25143;&#32534;&#20889;&#26174;&#24335;&#25552;&#31034;&#20449;&#24687;&#65289;&#26597;&#35810;OpenAI&#30340;GPT-3.5&#21644;GPT-4&#27169;&#22411;&#65292;&#35831;&#27714;&#20869;&#23481;&#21253;&#25324;&#35299;&#37322;&#20195;&#30721;&#20013;&#30340;&#31361;&#20986;&#37096;&#20998;&#12289;&#25552;&#20379;&#20195;&#30721;&#20013;&#20351;&#29992;&#30340;API&#35843;&#29992;&#35814;&#32454;&#20449;&#24687;&#65292;&#35299;&#37322;&#20851;&#38190;&#39046;&#22495;&#29305;&#23450;&#26415;&#35821;&#65292;&#20197;&#21450;&#20026;API&#25552;&#20379;&#20351;&#29992;&#31034;&#20363;&#12290;&#35813;&#25554;&#20214;&#36824;&#25903;&#25345;&#33258;&#30001;&#21709;&#24212;&#24335;&#25552;&#31034;&#65292;&#21487;&#20197;&#33258;&#21160;&#19978;&#19979;&#25991;&#21270;&#21040;&#27491;&#22312;&#32534;&#36753;&#30340;&#31243;&#24207;&#20013;&#12290;&#25105;&#20204;&#36890;&#36807;32&#21517;&#21442;&#19982;&#32773;&#36827;&#34892;&#29992;&#25143;&#30740;&#31350;&#26469;&#35780;&#20272;&#35813;&#31995;&#32479;&#65292;&#32467;&#26524;&#30830;&#35748;&#20351;&#29992;&#25105;&#20204;&#30340;&#25554;&#20214;&#33021;&#22815;
&lt;/p&gt;
&lt;p&gt;
Understanding code is challenging, especially when working in new and complex development environments. Code comments and documentation can help, but are typically scarce or hard to navigate. Large language models (LLMs) are revolutionizing the process of writing code. Can they do the same for helping understand it? In this study, we provide a first investigation of an LLM-based conversational UI built directly in the IDE that is geared towards code understanding. Our IDE plugin queries OpenAI's GPT-3.5 and GPT-4 models with four high-level requests without the user having to write explicit prompts: to explain a highlighted section of code, provide details of API calls used in the code, explain key domain-specific terms, and provide usage examples for an API. The plugin also allows for open-ended prompts, which are automatically contextualized to the LLM with the program being edited. We evaluate this system in a user study with 32 participants, which confirms that using our plugin can
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#20004;&#20010;&#24433;&#21709;VFL&#24615;&#33021;&#30340;&#20851;&#38190;&#22240;&#32032;&#65306;&#29305;&#24449;&#37325;&#35201;&#24615;&#21644;&#29305;&#24449;&#30456;&#20851;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#30456;&#20851;&#30340;&#35780;&#20272;&#25351;&#26631;&#21644;&#25968;&#25454;&#38598;&#21010;&#20998;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#24341;&#20837;&#30495;&#23454;&#30340;VFL&#25968;&#25454;&#38598;&#65292;&#22635;&#34917;&#20102;&#22270;&#20687;-&#22270;&#20687;VFL&#24773;&#26223;&#20013;&#30340;&#19981;&#36275;&#12290;&#30740;&#31350;&#23545;&#20110;&#26410;&#26469;&#30340;VFL&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2307.02040</link><description>&lt;p&gt;
VertiBench: &#22312;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#22522;&#20934;&#20013;&#25512;&#36827;&#29305;&#24449;&#20998;&#24067;&#22810;&#26679;&#24615;
&lt;/p&gt;
&lt;p&gt;
VertiBench: Advancing Feature Distribution Diversity in Vertical Federated Learning Benchmarks. (arXiv:2307.02040v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02040
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#20004;&#20010;&#24433;&#21709;VFL&#24615;&#33021;&#30340;&#20851;&#38190;&#22240;&#32032;&#65306;&#29305;&#24449;&#37325;&#35201;&#24615;&#21644;&#29305;&#24449;&#30456;&#20851;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#30456;&#20851;&#30340;&#35780;&#20272;&#25351;&#26631;&#21644;&#25968;&#25454;&#38598;&#21010;&#20998;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#24341;&#20837;&#30495;&#23454;&#30340;VFL&#25968;&#25454;&#38598;&#65292;&#22635;&#34917;&#20102;&#22270;&#20687;-&#22270;&#20687;VFL&#24773;&#26223;&#20013;&#30340;&#19981;&#36275;&#12290;&#30740;&#31350;&#23545;&#20110;&#26410;&#26469;&#30340;VFL&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#65288;VFL&#65289;&#26159;&#22312;&#29305;&#24449;&#21010;&#20998;&#30340;&#20998;&#24067;&#24335;&#25968;&#25454;&#19978;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#37325;&#35201;&#33539;&#20363;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38544;&#31169;&#38480;&#21046;&#65292;&#24456;&#23569;&#26377;&#20844;&#24320;&#30340;&#30495;&#23454;&#19990;&#30028;VFL&#25968;&#25454;&#38598;&#29992;&#20110;&#31639;&#27861;&#35780;&#20272;&#65292;&#32780;&#36825;&#20123;&#25968;&#25454;&#38598;&#21482;&#20195;&#34920;&#20102;&#26377;&#38480;&#30340;&#29305;&#24449;&#20998;&#24067;&#12290;&#29616;&#26377;&#30340;&#22522;&#20934;&#36890;&#24120;&#37319;&#29992;&#20174;&#20840;&#23616;&#38598;&#21512;&#20013;&#30340;&#20219;&#24847;&#29305;&#24449;&#21010;&#20998;&#23548;&#20986;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#36825;&#21482;&#25429;&#25417;&#21040;&#20102;&#19968;&#37096;&#20998;&#29305;&#24449;&#20998;&#24067;&#65292;&#23548;&#33268;&#31639;&#27861;&#24615;&#33021;&#35780;&#20272;&#19981;&#36275;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#24433;&#21709;VFL&#24615;&#33021;&#30340;&#20004;&#20010;&#20851;&#38190;&#22240;&#32032;&#8212;&#8212;&#29305;&#24449;&#37325;&#35201;&#24615;&#21644;&#29305;&#24449;&#30456;&#20851;&#24615;&#65292;&#24182;&#25552;&#20986;&#30456;&#20851;&#30340;&#35780;&#20272;&#25351;&#26631;&#21644;&#25968;&#25454;&#38598;&#21010;&#20998;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#30495;&#23454;&#30340;VFL&#25968;&#25454;&#38598;&#26469;&#24357;&#34917;&#22270;&#20687;-&#22270;&#20687;VFL&#24773;&#26223;&#20013;&#30340;&#19981;&#36275;&#12290;&#25105;&#20204;&#23545;&#23574;&#31471;VFL&#31639;&#27861;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#65292;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vertical Federated Learning (VFL) is a crucial paradigm for training machine learning models on feature-partitioned, distributed data. However, due to privacy restrictions, few public real-world VFL datasets exist for algorithm evaluation, and these represent a limited array of feature distributions. Existing benchmarks often resort to synthetic datasets, derived from arbitrary feature splits from a global set, which only capture a subset of feature distributions, leading to inadequate algorithm performance assessment. This paper addresses these shortcomings by introducing two key factors affecting VFL performance - feature importance and feature correlation - and proposing associated evaluation metrics and dataset splitting methods. Additionally, we introduce a real VFL dataset to address the deficit in image-image VFL scenarios. Our comprehensive evaluation of cutting-edge VFL algorithms provides valuable insights for future research in the field.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#24191;&#20041;&#30693;&#35782;&#33976;&#39311;&#65288;GKD&#65289;&#65292;&#36890;&#36807;&#20174;&#23398;&#29983;&#20013;&#37319;&#26679;&#36755;&#20986;&#24207;&#21015;&#26469;&#32531;&#35299;&#20998;&#24067;&#19981;&#21305;&#37197;&#65292;&#24182;&#22312;&#20248;&#21270;&#26367;&#20195;KL&#31561;&#31163;&#25955;&#24230;&#26041;&#38754;&#22788;&#29702;&#27169;&#22411;&#27424;&#35268;&#33539;&#65292;&#36798;&#21040;&#20102;&#22312;&#25688;&#35201;&#20219;&#21153;&#19978;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.13649</link><description>&lt;p&gt;
GKD&#65306;&#33258;&#22238;&#24402;&#24207;&#21015;&#27169;&#22411;&#30340;&#24191;&#20041;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
GKD: Generalized Knowledge Distillation for Auto-regressive Sequence Models. (arXiv:2306.13649v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13649
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#24191;&#20041;&#30693;&#35782;&#33976;&#39311;&#65288;GKD&#65289;&#65292;&#36890;&#36807;&#20174;&#23398;&#29983;&#20013;&#37319;&#26679;&#36755;&#20986;&#24207;&#21015;&#26469;&#32531;&#35299;&#20998;&#24067;&#19981;&#21305;&#37197;&#65292;&#24182;&#22312;&#20248;&#21270;&#26367;&#20195;KL&#31561;&#31163;&#25955;&#24230;&#26041;&#38754;&#22788;&#29702;&#27169;&#22411;&#27424;&#35268;&#33539;&#65292;&#36798;&#21040;&#20102;&#22312;&#25688;&#35201;&#20219;&#21153;&#19978;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#36890;&#24120;&#29992;&#20110;&#21387;&#32553;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#20943;&#23569;&#25512;&#29702;&#25104;&#26412;&#21644;&#20869;&#23384;&#21344;&#29992;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#38024;&#23545;&#33258;&#22238;&#24402;&#27169;&#22411;&#65288;&#22914;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#65289;&#30340;&#33976;&#39311;&#26041;&#27861;&#23384;&#22312;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#65288;1&#65289;&#35757;&#32451;&#26399;&#38388;&#36755;&#20986;&#24207;&#21015;&#21644;&#37096;&#32626;&#26102;&#30001;&#23398;&#29983;&#27169;&#22411;&#29983;&#25104;&#30340;&#24207;&#21015;&#20043;&#38388;&#20998;&#24067;&#19981;&#21305;&#37197;&#65292;&#65288;2&#65289;&#27169;&#22411;&#27424;&#35268;&#33539;&#65292;&#23398;&#29983;&#27169;&#22411;&#21487;&#33021;&#19981;&#22815;&#34920;&#36798;&#32769;&#24072;&#20998;&#24067;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24191;&#20041;&#30693;&#35782;&#33976;&#39311;&#65288;GKD&#65289;&#12290;GKD&#36890;&#36807;&#22312;&#35757;&#32451;&#26399;&#38388;&#20174;&#23398;&#29983;&#20013;&#37319;&#26679;&#36755;&#20986;&#24207;&#21015;&#26469;&#32531;&#35299;&#20998;&#24067;&#19981;&#21305;&#37197;&#12290;&#27492;&#22806;&#65292;GKD&#36890;&#36807;&#20248;&#21270;&#26367;&#20195;KL&#31561;&#31163;&#25955;&#24230;&#26469;&#22788;&#29702;&#27169;&#22411;&#27424;&#35268;&#33539;&#65292;&#36825;&#20123;&#31163;&#25955;&#24230;&#38598;&#20013;&#20110;&#29983;&#25104;&#21487;&#33021;&#31526;&#21512;&#32769;&#24072;&#20998;&#24067;&#30340;&#23398;&#29983;&#26679;&#26412;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#25688;&#35201;&#20219;&#21153;&#19978;&#65292;GKD&#20248;&#20110;&#24120;&#29992;&#30340;LLM&#33976;&#39311;&#26041;&#27861;&#65292;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge distillation is commonly used for compressing neural networks to reduce their inference cost and memory footprint. However, current distillation methods for auto-regressive models, such as generative language models (LMs), suffer from two key issues: (1) distribution mismatch between output sequences during training and the sequences generated by the student during its deployment, and (2) model under-specification, where the student model may not be expressive enough to fit the teacher's distribution. To address these issues, we propose Generalized Knowledge Distillation (GKD). GKD mitigates distribution mismatch by sampling output sequences from the student during training. Furthermore, GKD handles model under-specification by optimizing alternative divergences, such as reverse KL, that focus on generating samples from the student that are likely under the teacher's distribution. We demonstrate that GKD outperforms commonly-used approaches for distilling LLMs on summarizatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#29702;&#20154;&#19982;&#29615;&#22659;&#20132;&#20114;&#30340;&#22270;&#24418;&#32467;&#26500;&#30340;&#31572;&#26696;&#65292;&#20351;&#29992;&#20998;&#23618;&#22270;&#21010;&#20998;&#20135;&#29983;&#20855;&#26377;&#22810;&#20010;&#25277;&#35937;&#23618;&#27425;&#30340;&#25216;&#33021;&#23618;&#27425;&#32467;&#26500;&#12290;&#25216;&#33021;&#33021;&#23558;&#20195;&#29702;&#20154;&#31227;&#21160;&#21040;&#29366;&#24577;&#31354;&#38388;&#20013;&#20114;&#30456;&#36830;&#25509;&#32039;&#23494;&#20294;&#30456;&#20114;&#36830;&#25509;&#36739;&#24369;&#30340;&#21306;&#22495;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#24378;&#21270;&#23398;&#20064;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.09980</link><description>&lt;p&gt;
&#35770;&#25991;&#26631;&#39064;&#65306;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#21019;&#24314;&#22810;&#32423;&#25216;&#33021;&#23618;&#27425;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Creating Multi-Level Skill Hierarchies in Reinforcement Learning. (arXiv:2306.09980v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09980
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#29702;&#20154;&#19982;&#29615;&#22659;&#20132;&#20114;&#30340;&#22270;&#24418;&#32467;&#26500;&#30340;&#31572;&#26696;&#65292;&#20351;&#29992;&#20998;&#23618;&#22270;&#21010;&#20998;&#20135;&#29983;&#20855;&#26377;&#22810;&#20010;&#25277;&#35937;&#23618;&#27425;&#30340;&#25216;&#33021;&#23618;&#27425;&#32467;&#26500;&#12290;&#25216;&#33021;&#33021;&#23558;&#20195;&#29702;&#20154;&#31227;&#21160;&#21040;&#29366;&#24577;&#31354;&#38388;&#20013;&#20114;&#30456;&#36830;&#25509;&#32039;&#23494;&#20294;&#30456;&#20114;&#36830;&#25509;&#36739;&#24369;&#30340;&#21306;&#22495;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#24378;&#21270;&#23398;&#20064;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25688;&#35201;&#65306;&#20160;&#20040;&#26679;&#30340;&#25216;&#33021;&#23618;&#27425;&#32467;&#26500;&#23545;&#20110;&#33258;&#20027;&#20195;&#29702;&#20154;&#26159;&#26377;&#29992;&#30340;&#65311;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#29702;&#20154;&#19982;&#29615;&#22659;&#20132;&#20114;&#30340;&#22270;&#24418;&#32467;&#26500;&#30340;&#31572;&#26696;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#20998;&#23618;&#22270;&#21010;&#20998;&#26469;&#25581;&#31034;&#22270;&#22312;&#19981;&#21516;&#26102;&#38388;&#23610;&#24230;&#19978;&#30340;&#32467;&#26500;&#65292;&#20174;&#32780;&#20135;&#29983;&#20855;&#26377;&#22810;&#20010;&#25277;&#35937;&#23618;&#27425;&#30340;&#25216;&#33021;&#23618;&#27425;&#32467;&#26500;&#12290;&#22312;&#23618;&#27425;&#32467;&#26500;&#30340;&#27599;&#20010;&#23618;&#27425;&#19978;&#65292;&#25216;&#33021;&#23558;&#20195;&#29702;&#20154;&#31227;&#21160;&#21040;&#29366;&#24577;&#31354;&#38388;&#20013;&#20114;&#30456;&#36830;&#25509;&#32039;&#23494;&#20294;&#30456;&#20114;&#36830;&#25509;&#36739;&#24369;&#30340;&#21306;&#22495;&#12290;&#25105;&#20204;&#22312;&#24378;&#21270;&#23398;&#20064;&#30340;&#24191;&#27867;&#39046;&#22495;&#20013;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#25216;&#33021;&#23618;&#27425;&#32467;&#26500;&#30340;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
What is a useful skill hierarchy for an autonomous agent? We propose an answer based on the graphical structure of an agent's interaction with its environment. Our approach uses hierarchical graph partitioning to expose the structure of the graph at varying timescales, producing a skill hierarchy with multiple levels of abstraction. At each level of the hierarchy, skills move the agent between regions of the state space that are well connected within themselves but weakly connected to each other. We illustrate the utility of the proposed skill hierarchy in a wide variety of domains in the context of reinforcement learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#30721;&#30456;&#20114;&#36716;&#25442;&#26041;&#27861;&#65292;&#21033;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#12289;&#32534;&#35793;&#21644;&#31526;&#21495;&#25191;&#34892;&#27979;&#35797;&#29983;&#25104;&#36827;&#34892;&#31561;&#20215;&#27979;&#35797;&#12290;&#22312;&#24191;&#27867;&#30340;&#23454;&#39564;&#20013;&#65292;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#32534;&#35793;&#21644;&#36816;&#34892;&#26102;&#31561;&#20215;&#20934;&#30830;&#24615;&#31561;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#36716;&#25442;&#22120;&#21644;&#32763;&#35793;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2306.06755</link><description>&lt;p&gt;
&#27880;&#24847;&#21147;&#12289;&#32534;&#35793;&#21644;&#22522;&#20110;&#27714;&#35299;&#22120;&#30340;&#31526;&#21495;&#20998;&#26512;&#26159;&#24744;&#25152;&#38656;&#35201;&#30340;&#19968;&#20999;
&lt;/p&gt;
&lt;p&gt;
Attention, Compilation, and Solver-based Symbolic Analysis are All You Need. (arXiv:2306.06755v2 [cs.PL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06755
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#30721;&#30456;&#20114;&#36716;&#25442;&#26041;&#27861;&#65292;&#21033;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#12289;&#32534;&#35793;&#21644;&#31526;&#21495;&#25191;&#34892;&#27979;&#35797;&#29983;&#25104;&#36827;&#34892;&#31561;&#20215;&#27979;&#35797;&#12290;&#22312;&#24191;&#27867;&#30340;&#23454;&#39564;&#20013;&#65292;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#32534;&#35793;&#21644;&#36816;&#34892;&#26102;&#31561;&#20215;&#20934;&#30830;&#24615;&#31561;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#36716;&#25442;&#22120;&#21644;&#32763;&#35793;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;Java&#21040;Python&#65288;J2P&#65289;&#21644;Python&#21040;Java&#65288;P2J&#65289;&#20195;&#30721;&#30456;&#20114;&#36716;&#25442;&#26041;&#27861;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;CoTran&#30340;&#30456;&#20851;&#24037;&#20855;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#12289;&#32534;&#35793;&#21644;&#22522;&#20110;&#31526;&#21495;&#25191;&#34892;&#30340;&#27979;&#35797;&#29983;&#25104;&#65292;&#29992;&#20110;&#36755;&#20837;&#21644;&#36755;&#20986;&#31243;&#24207;&#20043;&#38388;&#30340;&#31561;&#20215;&#27979;&#35797;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20462;&#25913;&#20102;&#20856;&#22411;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#24490;&#29615;&#65292;&#21152;&#20837;&#20102;&#32534;&#35793;&#22120;&#21644;&#31526;&#21495;&#25191;&#34892;&#25439;&#22833;&#12290;&#36890;&#36807;&#22312;&#36229;&#36807;57,000&#20010;Java-Python&#31561;&#20215;&#23545;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#23558;CoTran&#19982;&#20854;&#20182;12&#20010;&#36716;&#25442;&#22120;&#21644;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32763;&#35793;&#24037;&#20855;&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#39564;&#27604;&#36739;&#65292;&#25105;&#20204;&#21457;&#29616;CoTran&#22312;&#35832;&#22914;&#32534;&#35793;&#21644;&#36816;&#34892;&#26102;&#31561;&#20215;&#20934;&#30830;&#24615;&#31561;&#30456;&#20851;&#25351;&#26631;&#19978;&#34920;&#29616;&#20248;&#20110;&#23427;&#20204;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#30340;&#24037;&#20855;&#22312;J2P&#36716;&#25442;&#20013;&#33719;&#24471;97.43%&#30340;&#32534;&#35793;&#20934;&#30830;&#24615;&#21644;49.66%&#30340;&#36816;&#34892;&#26102;&#31561;&#20215;&#20934;&#30830;&#24615;&#65292;&#32780;&#26368;&#25509;&#36817;&#30340;&#31454;&#20105;&#24037;&#20855;&#20998;&#21035;&#21482;&#26377;92.84%&#21644;40.95%&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a Java-to-Python (J2P) and Python-to-Java (P2J) back-to-back code translation method, and an associated tool called CoTran, based on large language models (LLMs). Our method leverages the attention mechanism of LLMs, compilation, and symbolic execution-based test generation for equivalence testing between the input and output programs. More precisely, we modify the typical LLM training loop to incorporate compiler and symbolic execution loss. Via extensive experiments comparing CoTran with 12 other transpilers and LLM-based translation tools over a benchmark of more than 57,000 Java-Python equivalent pairs, we show that CoTran outperforms them on relevant metrics such as compilation and runtime equivalence accuracy. For example, our tool gets 97.43% compilation accuracy and 49.66% runtime equivalence accuracy for J2P translation, whereas the nearest competing tool only gets 92.84% and 40.95% respectively.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20013;&#38388;&#38382;&#39064;&#65306;&#22240;&#26524;&#25104;&#20998;&#20998;&#26512;(CauCA)&#65292;&#23427;&#26159;&#29420;&#31435;&#25104;&#20998;&#20998;&#26512;(ICA)&#21644;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;(CRL)&#30340;&#27867;&#21270;&#21644;&#29305;&#20363;&#65292;&#20854;&#30446;&#26631;&#26159;&#23398;&#20064;&#35299;&#28151;&#20989;&#25968;&#21644;&#22240;&#26524;&#26426;&#21046;&#65292;&#39044;&#35774;&#20102;&#22240;&#26524;&#22270;&#30340;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2305.17225</link><description>&lt;p&gt;
&#22240;&#26524;&#25104;&#20998;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Causal Component Analysis. (arXiv:2305.17225v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17225
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20013;&#38388;&#38382;&#39064;&#65306;&#22240;&#26524;&#25104;&#20998;&#20998;&#26512;(CauCA)&#65292;&#23427;&#26159;&#29420;&#31435;&#25104;&#20998;&#20998;&#26512;(ICA)&#21644;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;(CRL)&#30340;&#27867;&#21270;&#21644;&#29305;&#20363;&#65292;&#20854;&#30446;&#26631;&#26159;&#23398;&#20064;&#35299;&#28151;&#20989;&#25968;&#21644;&#22240;&#26524;&#26426;&#21046;&#65292;&#39044;&#35774;&#20102;&#22240;&#26524;&#22270;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29420;&#31435;&#25104;&#20998;&#20998;&#26512;(ICA)&#30340;&#30446;&#26631;&#26159;&#20174;&#28151;&#21512;&#35266;&#27979;&#21040;&#30340;&#21464;&#37327;&#20013;&#24674;&#22797;&#29420;&#31435;&#30340;&#28508;&#22312;&#21464;&#37327;&#12290;&#32780;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;(CRL)&#30340;&#30446;&#26631;&#26159;&#25512;&#26029;&#22240;&#26524;&#20851;&#31995;&#24378;&#30456;&#20851;&#24615;&#30340;&#28508;&#22312;&#21464;&#37327;&#65292;&#20197;&#21450;&#32534;&#30721;&#23427;&#20204;&#30340;&#22240;&#26524;&#20851;&#31995;&#30340;&#26410;&#30693;&#22270;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20013;&#38388;&#38382;&#39064;&#65292;&#31216;&#20026;&#22240;&#26524;&#25104;&#20998;&#20998;&#26512;(CauCA)&#12290;CauCA&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;ICA&#30340;&#19968;&#31181;&#25512;&#24191;&#65292;&#23545;&#28508;&#22312;&#25104;&#20998;&#20043;&#38388;&#30340;&#22240;&#26524;&#20381;&#36182;&#24314;&#27169;&#65292;&#20063;&#26159;CRL&#30340;&#19968;&#20010;&#29305;&#20363;&#12290;&#19982;CRL&#19981;&#21516;&#30340;&#26159;&#65292;&#23427;&#39044;&#35774;&#20102;&#22240;&#26524;&#22270;&#30340;&#30693;&#35782;&#65292;&#20165;&#20851;&#27880;&#20110;&#23398;&#20064;&#35299;&#28151;&#20989;&#25968;&#21644;&#22240;&#26524;&#26426;&#21046;&#12290;&#25152;&#26377;&#20851;&#20110;CauCA&#22238;&#25910;&#22522;&#30784;&#30495;&#30456;&#30340;&#19981;&#21487;&#33021;&#32467;&#26524;&#20063;&#36866;&#29992;&#20110;CRL&#65292;&#32780;&#21487;&#33021;&#24615;&#32467;&#26524;&#21487;&#20197;&#20316;&#20026;&#25193;&#23637;CRL&#30340;&#22522;&#30784;&#12290;&#25105;&#20204;&#23558;&#20174;&#23545;&#28508;&#22312;&#22240;&#26524;&#21464;&#37327;&#23454;&#26045;&#19981;&#21516;&#31867;&#22411;&#24178;&#39044;&#30340;&#22810;&#20010;&#25968;&#25454;&#38598;&#20013;&#34920;&#24449;CauCA&#30340;&#21487;&#35782;&#21035;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Independent Component Analysis (ICA) aims to recover independent latent variables from observed mixtures thereof. Causal Representation Learning (CRL) aims instead to infer causally related (thus often statistically dependent) latent variables, together with the unknown graph encoding their causal relationships. We introduce an intermediate problem termed Causal Component Analysis (CauCA). CauCA can be viewed as a generalization of ICA, modelling the causal dependence among the latent components, and as a special case of CRL. In contrast to CRL, it presupposes knowledge of the causal graph, focusing solely on learning the unmixing function and the causal mechanisms. Any impossibility results regarding the recovery of the ground truth in CauCA also apply for CRL, while possibility results may serve as a stepping stone for extensions to CRL. We characterize CauCA identifiability from multiple datasets generated through different types of interventions on the latent causal variables. As a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Diff-PGD&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#25509;&#36817;&#21407;&#22987;&#25968;&#25454;&#20998;&#24067;&#12289;&#36924;&#30495;&#30340;&#23545;&#25239;&#26679;&#26412;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#38544;&#34109;&#24615;&#21644;&#23545;&#25239;&#24378;&#24230;&#21487;&#35843;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.16494</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30340;&#23545;&#25239;&#26679;&#26412;&#29983;&#25104;&#20197;&#25552;&#39640;&#38544;&#34109;&#24615;&#21644;&#21487;&#25511;&#24615;
&lt;/p&gt;
&lt;p&gt;
Diffusion-Based Adversarial Sample Generation for Improved Stealthiness and Controllability. (arXiv:2305.16494v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16494
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Diff-PGD&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#25509;&#36817;&#21407;&#22987;&#25968;&#25454;&#20998;&#24067;&#12289;&#36924;&#30495;&#30340;&#23545;&#25239;&#26679;&#26412;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#38544;&#34109;&#24615;&#21644;&#23545;&#25239;&#24378;&#24230;&#21487;&#35843;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#26679;&#26412;&#30340;&#24433;&#21709;&#65306;&#36825;&#26159;&#19968;&#31181;&#29305;&#24847;&#21046;&#20316;&#30340;&#33258;&#28982;&#22270;&#29255;&#30340;&#24494;&#23567;&#21464;&#21270;&#65292;&#26088;&#22312;&#35823;&#23548;&#27169;&#22411;&#12290;&#34429;&#28982;&#36825;&#20123;&#23545;&#25239;&#26679;&#26412;&#22312;&#25968;&#23383;&#21644;&#29289;&#29702;&#22330;&#26223;&#20013;&#21487;&#20197;&#36731;&#26494;&#29983;&#25104;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#19982;&#33258;&#28982;&#22270;&#20687;&#30340;&#23454;&#38469;&#25968;&#25454;&#20998;&#24067;&#24046;&#24322;&#24456;&#22823;&#65292;&#23548;&#33268;&#24378;&#24230;&#19982;&#38544;&#34109;&#24615;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#25193;&#25955;-&#25237;&#24433;&#26799;&#24230;&#19979;&#38477;&#65288;Diff-PGD&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#36924;&#30495;&#30340;&#23545;&#25239;&#26679;&#26412;&#12290;&#36890;&#36807;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#24341;&#23548;&#30340;&#26799;&#24230;&#65292;Diff-PGD&#30830;&#20445;&#23545;&#25239;&#26679;&#26412;&#20445;&#25345;&#25509;&#36817;&#21407;&#22987;&#25968;&#25454;&#20998;&#24067;&#65292;&#21516;&#26102;&#20445;&#25345;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#36731;&#26494;&#23450;&#21046;&#29305;&#23450;&#20219;&#21153;&#65292;&#22914;&#25968;&#23383;&#25915;&#20987;&#12289;&#29289;&#29702;&#25915;&#20987;&#21644;&#22522;&#20110;&#26679;&#24335;&#30340;&#25915;&#20987;&#12290;&#19982;&#29616;&#26377;&#30340;&#29983;&#25104;&#33258;&#28982;&#39118;&#26684;&#23545;&#25239;&#26679;&#26412;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#20998;&#31163;&#20248;&#21270;&#23545;&#25239;&#24378;&#24230;&#21644;&#38544;&#34109;&#24615;&#65292;&#25552;&#20379;&#26356;&#22823;&#30340;&#28789;&#27963;&#24615;&#21644;&#23545;&#29983;&#25104;&#26679;&#26412;&#30340;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks are known to be susceptible to adversarial samples: small variations of natural examples crafted to deliberately mislead the models. While they can be easily generated using gradient-based techniques in digital and physical scenarios, they often differ greatly from the actual data distribution of natural images, resulting in a trade-off between strength and stealthiness. In this paper, we propose a novel framework dubbed Diffusion-Based Projected Gradient Descent (Diff-PGD) for generating realistic adversarial samples. By exploiting a gradient guided by a diffusion model, Diff-PGD ensures that adversarial samples remain close to the original data distribution while maintaining their effectiveness. Moreover, our framework can be easily customized for specific tasks such as digital attacks, physical-world attacks, and style-based attacks. Compared with existing methods for generating natural-style adversarial samples, our framework enables the separation of optimizing adv
&lt;/p&gt;</description></item><item><title>LoReTTa&#26159;&#19968;&#20010;&#33258;&#30417;&#30563;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#20855;&#26377;&#19981;&#21516;&#27169;&#24577;&#30340;&#25968;&#25454;&#38598;&#20013;&#36716;&#25442;&#12290;&#36890;&#36807;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.14243</link><description>&lt;p&gt;
&#20869;&#23481;&#20016;&#23500;&#30340;&#22810;&#27169;&#24577;&#36716;&#25442;&#22120;&#35757;&#32451;&#19982; LoReTTa
&lt;/p&gt;
&lt;p&gt;
Training Transitive and Commutative Multimodal Transformers with LoReTTa. (arXiv:2305.14243v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14243
&lt;/p&gt;
&lt;p&gt;
LoReTTa&#26159;&#19968;&#20010;&#33258;&#30417;&#30563;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#20855;&#26377;&#19981;&#21516;&#27169;&#24577;&#30340;&#25968;&#25454;&#38598;&#20013;&#36716;&#25442;&#12290;&#36890;&#36807;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#36341;&#20013;&#65292;&#25910;&#38598;&#20004;&#20010;&#21305;&#37197;&#30340;&#24418;&#24577;A&#21644;B&#25110;B&#21644;C&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#24456;&#22256;&#38590;&#65292;&#33719;&#24471;&#21253;&#21547;&#19977;&#20010;&#23545;&#40784;&#24418;&#24577;A&#12289;B&#21644;C&#30340;&#25968;&#25454;&#38598;&#26356;&#21152;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;LoReTTa&#20197;&#24212;&#23545;&#36825;&#20010;&#26410;&#34987;&#20805;&#20998;&#30740;&#31350;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#33258;&#30417;&#30563;&#26694;&#26550;&#32467;&#21512;&#20102;&#22240;&#26524;&#25513;&#30721;&#24314;&#27169;&#21644;&#20132;&#25442;&#24459;&#21644;&#20256;&#36882;&#24615;&#30340;&#35268;&#21017;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#30340;&#27169;&#24577;&#20013;&#36716;&#25442;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#31181;&#21512;&#25104;&#26174;&#30528;&#25552;&#39640;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Collecting a multimodal dataset with two paired modalities A and B or B and C is difficult in practice. Obtaining a dataset with three aligned modalities A, B, and C is even more challenging. For example, some public medical datasets have only genetic sequences and microscopic images for one patient, and only genetic sequences and radiological images for another - but no dataset includes both microscopic and radiological images for the same patient. This makes it difficult to integrate and combine all modalities into a large pre-trained neural network. We introduce LoReTTa (Linking mOdalities with a tRansitive and commutativE pre-Training sTrAtegy) to address this understudied problem. Our self-supervised framework combines causal masked modeling with the rules of commutativity and transitivity to transition within and between different modalities. Thus, it can model the relation A -&gt; C with A -&gt; B -&gt; C. Given a dataset containing only the disjoint combinations (A, B) and (B, C), we sh
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#32593;&#32476;AMenuNet&#26469;&#26500;&#36896;AMAs&#21442;&#25968;&#21644;&#29983;&#25104;&#20505;&#36873;&#20998;&#37197;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#21344;&#20248;&#31574;&#30053;&#28608;&#21169;&#20860;&#23481;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#30340;&#38480;&#21046;&#65292;&#20854;&#22312;&#21327;&#21830;&#19968;&#33268;&#30340;&#20215;&#20540;&#21644;&#31038;&#20250;&#27531;&#20313;&#20215;&#20540;&#26041;&#38754;&#20248;&#20110;&#24378;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.12162</link><description>&lt;p&gt;
&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;DSIC&#20223;&#23556;&#26497;&#22823;&#20215;&#25293;&#21334;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
A Scalable Neural Network for DSIC Affine Maximizer Auction Design. (arXiv:2305.12162v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12162
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#32593;&#32476;AMenuNet&#26469;&#26500;&#36896;AMAs&#21442;&#25968;&#21644;&#29983;&#25104;&#20505;&#36873;&#20998;&#37197;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#21344;&#20248;&#31574;&#30053;&#28608;&#21169;&#20860;&#23481;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#30340;&#38480;&#21046;&#65292;&#20854;&#22312;&#21327;&#21830;&#19968;&#33268;&#30340;&#20215;&#20540;&#21644;&#31038;&#20250;&#27531;&#20313;&#20215;&#20540;&#26041;&#38754;&#20248;&#20110;&#24378;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#25293;&#21334;&#35774;&#35745;&#26088;&#22312;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#23547;&#25214;&#32463;&#39564;&#19978;&#39640;&#25910;&#20837;&#30340;&#26426;&#21046;&#12290;&#29616;&#26377;&#30340;&#22810;&#29289;&#21697;&#25293;&#21334;&#24773;&#26223;&#30340;&#24037;&#20316;&#21487;&#20197;&#31895;&#30053;&#22320;&#20998;&#20026;RegretNet&#31867;&#21644;&#20223;&#23556;&#26497;&#22823;&#20215;&#65288;AMAs&#65289;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#21069;&#32773;&#19981;&#33021;&#20005;&#26684;&#20445;&#35777;&#21344;&#20248;&#31574;&#30053;&#28608;&#21169;&#20860;&#23481;&#24615;&#65288;DSIC&#65289;&#65292;&#32780;&#21518;&#32773;&#22240;&#20026;&#20998;&#37197;&#20505;&#36873;&#20154;&#25968;&#36807;&#22810;&#32780;&#38754;&#20020;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AMenuNet&#65292;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#23427;&#20174;&#20986;&#20215;&#20154;&#21644;&#29289;&#21697;&#34920;&#31034;&#20013;&#26500;&#36896;AMA&#21442;&#25968;&#65288;&#29978;&#33267;&#21253;&#25324;&#20998;&#37197;&#33756;&#21333;&#65289;&#12290;&#30001;&#20110;AMA&#30340;&#23646;&#24615;&#65292;AMenuNet&#22987;&#32456;&#26159;DSIC&#21644;&#20010;&#20154;&#29702;&#24615;&#65288;IR&#65289;&#30340;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#20505;&#36873;&#20998;&#37197;&#26469;&#22686;&#24378;&#21487;&#20280;&#32553;&#24615;&#12290;&#27492;&#22806;&#65292;AMenuNet&#26159;&#32622;&#25442;&#31561;&#21464;&#30340;&#65292;&#20854;&#21442;&#25968;&#25968;&#37327;&#19981;&#21463;&#25293;&#21334;&#35268;&#27169;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#35777;&#26126;AMenuNet&#22312;&#21327;&#21830;&#19968;&#33268;&#30340;&#20215;&#20540;&#21644;&#31038;&#20250;&#27531;&#20313;&#20215;&#20540;&#26041;&#38754;&#20248;&#20110;&#24378;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated auction design aims to find empirically high-revenue mechanisms through machine learning. Existing works on multi item auction scenarios can be roughly divided into RegretNet-like and affine maximizer auctions (AMAs) approaches. However, the former cannot strictly ensure dominant strategy incentive compatibility (DSIC), while the latter faces scalability issue due to the large number of allocation candidates. To address these limitations, we propose AMenuNet, a scalable neural network that constructs the AMA parameters (even including the allocation menu) from bidder and item representations. AMenuNet is always DSIC and individually rational (IR) due to the properties of AMAs, and it enhances scalability by generating candidate allocations through a neural network. Additionally, AMenuNet is permutation equivariant, and its number of parameters is independent of auction scale. We conduct extensive experiments to demonstrate that AMenuNet outperforms strong baselines in both co
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;Color&#35299;&#20915;&#26041;&#26696;&#30340;Actor-Sharer-Learner&#65288;ASL&#65289;&#35757;&#32451;&#26694;&#26550;&#21644;&#38754;&#21521;&#31227;&#21160;&#26426;&#22120;&#20154;&#30340;&#27169;&#25311;&#22120;Sparrow&#65292;&#20351;&#24471;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#35757;&#32451;&#23616;&#37096;&#36335;&#24452;&#35268;&#21010;&#22120;&#21464;&#24471;&#21487;&#34892;&#12290;</title><link>http://arxiv.org/abs/2305.04180</link><description>&lt;p&gt;
&#36890;&#36807;&#37096;&#20998;&#35299;&#32806;&#24378;&#21270;&#23398;&#20064;&#21644;&#21521;&#37327;&#22810;&#26679;&#24615;&#65292;&#19968;&#23567;&#26102;&#20869;&#35757;&#32451;&#36866;&#29992;&#20110;&#23454;&#38469;&#19990;&#30028;&#30340;&#23616;&#37096;&#36335;&#24452;&#35268;&#21010;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Train a Real-world Local Path Planner in One Hour via Partially Decoupled Reinforcement Learning and Vectorized Diversity. (arXiv:2305.04180v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04180
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;Color&#35299;&#20915;&#26041;&#26696;&#30340;Actor-Sharer-Learner&#65288;ASL&#65289;&#35757;&#32451;&#26694;&#26550;&#21644;&#38754;&#21521;&#31227;&#21160;&#26426;&#22120;&#20154;&#30340;&#27169;&#25311;&#22120;Sparrow&#65292;&#20351;&#24471;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#35757;&#32451;&#23616;&#37096;&#36335;&#24452;&#35268;&#21010;&#22120;&#21464;&#24471;&#21487;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#24050;&#32463;&#22312;&#35299;&#20915;&#23616;&#37096;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#19978;&#26174;&#31034;&#20986;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;DRL&#30340;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#19981;&#36275;&#65292;&#20854;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21463;&#21040;&#20102;&#26497;&#22823;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Color&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#30001;Actor-Sharer-Learner&#65288;ASL&#65289;&#35757;&#32451;&#26694;&#26550;&#21644;&#38754;&#21521;&#31227;&#21160;&#26426;&#22120;&#20154;&#30340;&#27169;&#25311;&#22120;Sparrow&#32452;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning (DRL) has exhibited efficacy in resolving the Local Path Planning (LPP) problem. However, such application in the real world is immensely limited due to the deficient efficiency and generalization capability of DRL. To alleviate these two issues, a solution named Color is proposed, which consists of an Actor-Sharer-Learner (ASL) training framework and a mobile robot-oriented simulator Sparrow. Specifically, the ASL framework, intending to improve the efficiency of the DRL algorithm, employs a Vectorized Data Collection (VDC) mode to expedite data acquisition, decouples the data collection from model optimization by multithreading, and partially connects the two procedures by harnessing a Time Feedback Mechanism (TFM) to evade data underuse or overuse. Meanwhile, the Sparrow simulator utilizes a 2D grid-based world, simplified kinematics, and conversion-free data flow to achieve a lightweight design. The lightness facilitates vectorized diversity, allowing di
&lt;/p&gt;</description></item><item><title>CryCeleb&#26159;&#19968;&#20010;&#22522;&#20110;&#23156;&#20799;&#21741;&#22768;&#30340;&#35828;&#35805;&#20154;&#35748;&#35777;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#36229;&#36807;6&#23567;&#26102;&#30340;&#25163;&#21160;&#20998;&#21106;&#21741;&#22768;&#65292;&#21487;&#29992;&#20110;&#30740;&#31350;&#23156;&#20799;&#21741;&#22768;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2305.00969</link><description>&lt;p&gt;
CryCeleb: &#22522;&#20110;&#23156;&#20799;&#21741;&#22768;&#30340;&#35828;&#35805;&#20154;&#35748;&#35777;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
CryCeleb: A Speaker Verification Dataset Based on Infant Cry Sounds. (arXiv:2305.00969v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00969
&lt;/p&gt;
&lt;p&gt;
CryCeleb&#26159;&#19968;&#20010;&#22522;&#20110;&#23156;&#20799;&#21741;&#22768;&#30340;&#35828;&#35805;&#20154;&#35748;&#35777;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#36229;&#36807;6&#23567;&#26102;&#30340;&#25163;&#21160;&#20998;&#21106;&#21741;&#22768;&#65292;&#21487;&#29992;&#20110;&#30740;&#31350;&#23156;&#20799;&#21741;&#22768;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;Ubenwa CryCeleb&#25968;&#25454;&#38598;&#8212;&#8212;&#19968;&#20010;&#26631;&#35760;&#30340;&#23156;&#20799;&#21741;&#22768;&#25910;&#38598;&#65292;&#20197;&#21450;&#38468;&#24102;&#30340;CryCeleb 2023&#20219;&#21153;&#8212;&#8212;&#19968;&#20010;&#22522;&#20110;&#23156;&#20799;&#21741;&#22768;&#30340;&#20844;&#20849;&#35828;&#35805;&#20154;&#39564;&#35777;&#25361;&#25112;&#12290;&#25105;&#20204;&#37322;&#25918;&#20986;786&#21517;&#26032;&#29983;&#20799;&#36229;&#36807;6&#23567;&#26102;&#30340;&#25163;&#21160;&#20998;&#21106;&#21741;&#22768;&#65292;&#20197;&#40723;&#21169;&#23156;&#20799;&#21741;&#22768;&#20998;&#26512;&#26041;&#38754;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes the Ubenwa CryCeleb dataset - a labeled collection of infant cries, and the accompanying CryCeleb 2023 task - a public speaker verification challenge based on infant cry sounds. We release for academic usage more than 6 hours of manually segmented cry sounds from 786 newborns to encourage research in infant cry analysis.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36523;&#20221;&#28151;&#21512;&#30340;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#36890;&#36807;&#22270;&#24418;&#22686;&#24378;&#24471;&#21040;&#30340;&#19981;&#21516;&#20294;&#30456;&#20284;&#30340;&#22270;&#24418;&#32467;&#26500;&#21644;&#26631;&#31614;&#30340;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#34920;&#31034;&#25429;&#33719;&#12290;</title><link>http://arxiv.org/abs/2304.10045</link><description>&lt;p&gt;
ID-MixGCL: &#22522;&#20110;&#36523;&#20221;&#28151;&#21512;&#30340;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
ID-MixGCL: Identity Mixup for Graph Contrastive Learning. (arXiv:2304.10045v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10045
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36523;&#20221;&#28151;&#21512;&#30340;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#36890;&#36807;&#22270;&#24418;&#22686;&#24378;&#24471;&#21040;&#30340;&#19981;&#21516;&#20294;&#30456;&#20284;&#30340;&#22270;&#24418;&#32467;&#26500;&#21644;&#26631;&#31614;&#30340;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#34920;&#31034;&#25429;&#33719;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#21457;&#23637;&#30340;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#65288;GCL&#65289;&#26041;&#27861;&#26159;&#27604;&#36739;&#21516;&#19968;&#20010;&#22270;&#24418;&#30340;&#20004;&#20010;&#19981;&#21516;&#30340;&#8220;&#35270;&#22270;&#8221;&#20197;&#23398;&#20064;&#33410;&#28857;/&#22270;&#24418;&#34920;&#31034;&#12290;&#36825;&#20123;&#26041;&#27861;&#30340;&#26680;&#24515;&#20551;&#35774;&#26159;&#36890;&#36807;&#22270;&#24418;&#22686;&#24378;&#65292;&#21487;&#20197;&#29983;&#25104;&#20960;&#20010;&#32467;&#26500;&#19981;&#21516;&#20294;&#35821;&#20041;&#30456;&#20284;&#30340;&#22270;&#24418;&#32467;&#26500;&#65292;&#22240;&#27492;&#21407;&#22987;&#21644;&#22686;&#24378;&#30340;&#22270;&#24418;/&#33410;&#28857;&#30340;&#36523;&#20221;&#26631;&#31614;&#24212;&#35813;&#26159;&#30456;&#21516;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20010;&#20551;&#35774;&#24182;&#19981;&#24635;&#26159;&#25104;&#31435;&#65292;&#20363;&#22914;&#20998;&#23376;&#22270;&#20013;&#23545;&#33410;&#28857;&#25110;&#36793;&#30340;&#20219;&#20309;&#25200;&#21160;&#37117;&#20250;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#25913;&#21464;&#22270;&#24418;&#26631;&#31614;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35748;&#20026;&#22686;&#24378;&#22270;&#24418;&#32467;&#26500;&#24212;&#35813;&#20276;&#38543;&#30528;&#23545;&#23545;&#27604;&#25439;&#22833;&#20351;&#29992;&#30340;&#26631;&#31614;&#30340;&#36866;&#24212;&#12290;&#22522;&#20110;&#36825;&#20010;&#24819;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; ID-MixGCL&#65292;&#23427;&#20801;&#35768;&#21516;&#26102;&#35843;&#33410;&#36755;&#20837;&#22270;&#24418;&#21644;&#30456;&#24212;&#30340;&#36523;&#20221;&#26631;&#31614;&#65292;&#20855;&#26377;&#21487;&#25511;&#30340;&#25913;&#21464;&#31243;&#24230;&#65292;&#20174;&#32780;&#25429;&#33719;&#32454;&#31890;&#24230;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently developed graph contrastive learning (GCL) approaches compare two different "views" of the same graph in order to learn node/graph representations. The core assumption of these approaches is that by graph augmentation, it is possible to generate several structurally different but semantically similar graph structures, and therefore, the identity labels of the original and augmented graph/nodes should be identical. However, in this paper, we observe that this assumption does not always hold, for example, any perturbation to nodes or edges in a molecular graph will change the graph labels to some degree. Therefore, we believe that augmenting the graph structure should be accompanied by an adaptation of the labels used for the contrastive loss. Based on this idea, we propose ID-MixGCL, which allows for simultaneous modulation of both the input graph and the corresponding identity labels, with a controllable degree of change, leading to the capture of fine-grained representations 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#8212;&#8212;&#22522;&#20110;&#21516;&#20262;&#30340;&#29289;&#29702;&#30693;&#35782;&#31070;&#32463;&#32593;&#32476;&#65292;&#26469;&#35299;&#20915;&#20855;&#26377;&#22810;&#20010;&#35299;&#30340;&#38750;&#32447;&#24615;&#24494;&#20998;&#26041;&#31243;&#30340;&#21453;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#24050;&#30693;&#35266;&#27979;&#32467;&#26524;&#24182;&#31526;&#21512;DEs&#30340;&#32422;&#26463;&#26465;&#20214;&#65292;&#36890;&#36807;&#21516;&#20262;&#36830;&#32493;&#26041;&#27861;&#35299;&#20915;&#21453;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#21487;&#20280;&#32553;&#19988;&#36866;&#24212;&#24615;&#24378;&#65292;&#20026;&#35299;&#20915;&#20855;&#26377;&#22810;&#20010;&#35299;&#30340;DEs&#25552;&#20379;&#20102;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2304.02811</link><description>&lt;p&gt;
HomPINNs&#65306;&#22522;&#20110;&#21516;&#20262;&#30340;&#29289;&#29702;&#30693;&#35782;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#22810;&#20010;&#35299;&#30340;&#38750;&#32447;&#24615;&#24494;&#20998;&#26041;&#31243;&#30340;&#21453;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
HomPINNs: homotopy physics-informed neural networks for solving the inverse problems of nonlinear differential equations with multiple solutions. (arXiv:2304.02811v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02811
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#8212;&#8212;&#22522;&#20110;&#21516;&#20262;&#30340;&#29289;&#29702;&#30693;&#35782;&#31070;&#32463;&#32593;&#32476;&#65292;&#26469;&#35299;&#20915;&#20855;&#26377;&#22810;&#20010;&#35299;&#30340;&#38750;&#32447;&#24615;&#24494;&#20998;&#26041;&#31243;&#30340;&#21453;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#24050;&#30693;&#35266;&#27979;&#32467;&#26524;&#24182;&#31526;&#21512;DEs&#30340;&#32422;&#26463;&#26465;&#20214;&#65292;&#36890;&#36807;&#21516;&#20262;&#36830;&#32493;&#26041;&#27861;&#35299;&#20915;&#21453;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#21487;&#20280;&#32553;&#19988;&#36866;&#24212;&#24615;&#24378;&#65292;&#20026;&#35299;&#20915;&#20855;&#26377;&#22810;&#20010;&#35299;&#30340;DEs&#25552;&#20379;&#20102;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#35299;&#31354;&#38388;&#20013;&#30340;&#38750;&#21807;&#19968;&#24615;&#12289;&#23545;&#31216;&#24615;&#21644;&#20998;&#23700;&#31561;&#22797;&#26434;&#34892;&#20026;&#65292;&#35299;&#20915;&#20855;&#26377;&#22810;&#20010;&#35299;&#30340;&#38750;&#32447;&#24615;&#24494;&#20998;&#26041;&#31243;&#65288;DEs&#65289;&#30340;&#21453;&#38382;&#39064;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21516;&#20262;&#29289;&#29702;&#30693;&#35782;&#31070;&#32463;&#32593;&#32476;&#65288;HomPINNs&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;&#21516;&#20262;&#36830;&#32493;&#21644;&#31070;&#32463;&#32593;&#32476;&#65288;NNs&#65289;&#26469;&#35299;&#20915;&#21453;&#38382;&#39064;&#30340;&#26032;&#26694;&#26550;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#39318;&#20808;&#20351;&#29992;NNs&#21516;&#26102;&#36924;&#36817;&#24050;&#30693;&#35266;&#27979;&#32467;&#26524;&#21644;&#31526;&#21512;DEs&#30340;&#32422;&#26463;&#26465;&#20214;&#12290;&#36890;&#36807;&#21033;&#29992;&#21516;&#20262;&#36830;&#32493;&#26041;&#27861;&#65292;&#36924;&#36817;&#21487;&#36861;&#36394;&#35266;&#23519;&#32467;&#26524;&#20197;&#30830;&#23450;&#22810;&#20010;&#35299;&#24182;&#35299;&#20915;&#21453;&#38382;&#39064;&#12290;&#23454;&#39564;&#28085;&#30422;&#22312;&#19968;&#32500;DEs&#19978;&#27979;&#35797;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#24212;&#29992;&#23427;&#26469;&#35299;&#20915;&#20108;&#32500;Gray-Scott&#27169;&#25311;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26159;&#21487;&#20280;&#32553;&#19988;&#36866;&#24212;&#24615;&#24378;&#30340;&#65292;&#20026;&#35299;&#20915;&#20855;&#26377;&#22810;&#20010;&#35299;&#30340;DEs&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the complex behavior arising from non-uniqueness, symmetry, and bifurcations in the solution space, solving inverse problems of nonlinear differential equations (DEs) with multiple solutions is a challenging task. To address this issue, we propose homotopy physics-informed neural networks (HomPINNs), a novel framework that leverages homotopy continuation and neural networks (NNs) to solve inverse problems. The proposed framework begins with the use of a NN to simultaneously approximate known observations and conform to the constraints of DEs. By utilizing the homotopy continuation method, the approximation traces the observations to identify multiple solutions and solve the inverse problem. The experiments involve testing the performance of the proposed method on one-dimensional DEs and applying it to solve a two-dimensional Gray-Scott simulation. Our findings demonstrate that the proposed method is scalable and adaptable, providing an effective solution for solving DEs with mul
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#30340;&#20869;&#22312;&#21160;&#26426;&#31639;&#27861;&#31867;&#21035;SND&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#25506;&#32034;&#22256;&#38590;&#29615;&#22659;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#26159;&#26377;&#25928;&#30340;&#12290;</title><link>http://arxiv.org/abs/2302.11563</link><description>&lt;p&gt;
&#33258;&#25105;&#30417;&#30563;&#21033;&#29992;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Exploration by self-supervised exploitation. (arXiv:2302.11563v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11563
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#30340;&#20869;&#22312;&#21160;&#26426;&#31639;&#27861;&#31867;&#21035;SND&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#25506;&#32034;&#22256;&#38590;&#29615;&#22659;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#21487;&#20197;&#35299;&#20915;&#20915;&#31574;&#38382;&#39064;&#65292;&#24182;&#35757;&#32451;&#19968;&#20010;&#20195;&#29702;&#26681;&#25454;&#39044;&#20808;&#35774;&#35745;&#30340;&#22870;&#21169;&#20989;&#25968;&#22312;&#29615;&#22659;&#20013;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#22870;&#21169;&#36807;&#20110;&#31232;&#30095;&#65292;&#20195;&#29702;&#22312;&#29615;&#22659;&#25506;&#32034;&#20013;&#19981;&#20250;&#36935;&#21040;&#22870;&#21169;&#65292;&#36825;&#31181;&#26041;&#27861;&#23601;&#21464;&#24471;&#38750;&#24120;&#26840;&#25163;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#21487;&#33021;&#26159;&#20026;&#20195;&#29702;&#35013;&#22791;&#20869;&#22312;&#21160;&#26426;&#65292;&#36825;&#26679;&#20195;&#29702;&#22312;&#25506;&#32034;&#36807;&#31243;&#20013;&#20063;&#21487;&#33021;&#36935;&#21040;&#22806;&#37096;&#22870;&#21169;&#12290;&#26032;&#39062;&#24615;&#26816;&#27979;&#26159;&#20869;&#22312;&#21160;&#26426;&#30740;&#31350;&#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#20998;&#25903;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33976;&#39311;&#35823;&#24046;&#20316;&#20026;&#26032;&#39062;&#24615;&#25351;&#26631;&#30340;&#33258;&#25105;&#30417;&#30563;&#32593;&#32476;&#33976;&#39311;&#65288;SND&#65289;&#31639;&#27861;&#31867;&#21035;&#65292;&#20854;&#20013;&#30446;&#26631;&#27169;&#22411;&#20351;&#29992;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#20026;&#27492;&#25913;&#32534;&#20102;&#19977;&#31181;&#29616;&#26377;&#30340;&#33258;&#25105;&#30417;&#30563;&#26041;&#27861;&#65292;&#24182;&#22312;&#34987;&#35748;&#20026;&#38590;&#20197;&#25506;&#32034;&#30340;&#21313;&#20010;&#29615;&#22659;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning can solve decision-making problems and train an agent to behave in an environment according to a predesigned reward function. However, such an approach becomes very problematic if the reward is too sparse and the agent does not come across the reward during the environmental exploration. The solution to such a problem may be in equipping the agent with an intrinsic motivation, which will provide informed exploration, during which the agent is likely to also encounter external reward. Novelty detection is one of the promising branches of intrinsic motivation research. We present Self-supervised Network Distillation (SND), a class of internal motivation algorithms based on the distillation error as a novelty indicator, where the target model is trained using self-supervised learning. We adapted three existing self-supervised methods for this purpose and experimentally tested them on a set of ten environments that are considered difficult to explore. The results sho
&lt;/p&gt;</description></item><item><title>&#22312;&#32447;&#25439;&#22833;&#20989;&#25968;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#30340;&#20803;&#23398;&#20064;&#33539;&#20363;&#65292;&#26088;&#22312;&#33258;&#21160;&#21270;&#20026;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35774;&#35745;&#25439;&#22833;&#20989;&#25968;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#23398;&#20064;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#27599;&#27425;&#26356;&#26032;&#22522;&#26412;&#27169;&#22411;&#21442;&#25968;&#21518;&#33258;&#36866;&#24212;&#22320;&#22312;&#32447;&#26356;&#26032;&#25439;&#22833;&#20989;&#25968;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#31283;&#23450;&#22320;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2301.13247</link><description>&lt;p&gt;
&#22312;&#32447;&#25439;&#22833;&#20989;&#25968;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Online Loss Function Learning. (arXiv:2301.13247v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13247
&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#25439;&#22833;&#20989;&#25968;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#30340;&#20803;&#23398;&#20064;&#33539;&#20363;&#65292;&#26088;&#22312;&#33258;&#21160;&#21270;&#20026;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35774;&#35745;&#25439;&#22833;&#20989;&#25968;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#23398;&#20064;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#27599;&#27425;&#26356;&#26032;&#22522;&#26412;&#27169;&#22411;&#21442;&#25968;&#21518;&#33258;&#36866;&#24212;&#22320;&#22312;&#32447;&#26356;&#26032;&#25439;&#22833;&#20989;&#25968;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#31283;&#23450;&#22320;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25439;&#22833;&#20989;&#25968;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#30340;&#20803;&#23398;&#20064;&#33539;&#20363;&#65292;&#26088;&#22312;&#33258;&#21160;&#21270;&#20026;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35774;&#35745;&#25439;&#22833;&#20989;&#25968;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#29616;&#26377;&#30340;&#25439;&#22833;&#20989;&#25968;&#23398;&#20064;&#25216;&#26415;&#24050;&#32463;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#32463;&#24120;&#25913;&#21892;&#27169;&#22411;&#30340;&#35757;&#32451;&#21160;&#24577;&#21644;&#26368;&#32456;&#25512;&#29702;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25216;&#26415;&#30340;&#19968;&#20010;&#37325;&#35201;&#38480;&#21046;&#26159;&#25439;&#22833;&#20989;&#25968;&#20197;&#32447;&#19979;&#26041;&#24335;&#36827;&#34892;&#20803;&#23398;&#20064;&#65292;&#20803;&#30446;&#26631;&#20165;&#32771;&#34385;&#35757;&#32451;&#30340;&#21069;&#20960;&#20010;&#27493;&#39588;&#65292;&#36825;&#19982;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#20351;&#29992;&#30340;&#26102;&#38388;&#33539;&#22260;&#30456;&#27604;&#26174;&#33879;&#36739;&#30701;&#12290;&#36825;&#23548;&#33268;&#23545;&#20110;&#22312;&#35757;&#32451;&#24320;&#22987;&#26102;&#34920;&#29616;&#33391;&#22909;&#20294;&#22312;&#35757;&#32451;&#32467;&#26463;&#26102;&#34920;&#29616;&#19981;&#20339;&#30340;&#25439;&#22833;&#20989;&#25968;&#23384;&#22312;&#26126;&#26174;&#30340;&#20559;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#23398;&#20064;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#27599;&#27425;&#26356;&#26032;&#22522;&#26412;&#27169;&#22411;&#21442;&#25968;&#21518;&#33258;&#36866;&#24212;&#22320;&#22312;&#32447;&#26356;&#26032;&#25439;&#22833;&#20989;&#25968;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#31283;&#23450;&#22320;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Loss function learning is a new meta-learning paradigm that aims to automate the essential task of designing a loss function for a machine learning model. Existing techniques for loss function learning have shown promising results, often improving a model's training dynamics and final inference performance. However, a significant limitation of these techniques is that the loss functions are meta-learned in an offline fashion, where the meta-objective only considers the very first few steps of training, which is a significantly shorter time horizon than the one typically used for training deep neural networks. This causes significant bias towards loss functions that perform well at the very start of training but perform poorly at the end of training. To address this issue we propose a new loss function learning technique for adaptively updating the loss function online after each update to the base model parameters. The experimental results show that our proposed method consistently out
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;Lyapunov&#25511;&#21046;&#29702;&#35770;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#23558;&#22522;&#20110;&#27969;&#37327;&#21644;&#22522;&#20110;&#21387;&#21147;&#30340;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#26041;&#27861;&#30456;&#32467;&#21512;&#12290;&#36890;&#36807;&#24341;&#20837;&#32972;&#21387;&#26041;&#27861;&#21644;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#23454;&#29616;&#23545;&#22797;&#26434;&#20132;&#36890;&#32593;&#32476;&#30340;&#26377;&#25928;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2210.02612</link><description>&lt;p&gt;
&#20855;&#26377;&#32972;&#21387;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;Lyapunov&#20989;&#25968;&#19968;&#33268;&#33258;&#36866;&#24212;&#32593;&#32476;&#20449;&#21495;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Lyapunov Function Consistent Adaptive Network Signal Control with Back Pressure and Reinforcement Learning. (arXiv:2210.02612v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.02612
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;Lyapunov&#25511;&#21046;&#29702;&#35770;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#23558;&#22522;&#20110;&#27969;&#37327;&#21644;&#22522;&#20110;&#21387;&#21147;&#30340;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#26041;&#27861;&#30456;&#32467;&#21512;&#12290;&#36890;&#36807;&#24341;&#20837;&#32972;&#21387;&#26041;&#27861;&#21644;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#23454;&#29616;&#23545;&#22797;&#26434;&#20132;&#36890;&#32593;&#32476;&#30340;&#26377;&#25928;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#20013;&#65292;&#22522;&#20110;&#27969;&#37327;&#21644;&#22522;&#20110;&#21387;&#21147;&#30340;&#26041;&#27861;&#36890;&#24120;&#20998;&#21035;&#20351;&#29992;&#65292;&#20294;&#24448;&#24448;&#34987;&#21333;&#29420;&#32771;&#34385;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;Lyapunov&#25511;&#21046;&#29702;&#35770;&#65292;&#20998;&#21035;&#20026;&#36825;&#20123;&#26041;&#27861;&#23450;&#20041;&#20102;&#29305;&#23450;&#30340;Lyapunov&#20989;&#25968;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;&#26377;&#36259;&#30340;&#32467;&#26524;&#12290;&#20363;&#22914;&#65292;&#33879;&#21517;&#30340;&#32972;&#21387;&#26041;&#27861;&#31561;&#20110;&#20132;&#21449;&#21475;&#36710;&#36947;&#39281;&#21644;&#27969;&#37327;&#21152;&#26435;&#30340;&#24046;&#20998;&#38431;&#21015;&#38271;&#24230;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#28155;&#21152;&#22522;&#26412;&#30340;&#20132;&#36890;&#27969;&#29702;&#35770;&#26469;&#25913;&#36827;&#23427;&#12290;&#25511;&#21046;&#31995;&#32479;&#19981;&#20165;&#24212;&#35813;&#30830;&#20445;&#31283;&#23450;&#65292;&#36824;&#24212;&#35813;&#33021;&#22815;&#36866;&#24212;&#21508;&#31181;&#24615;&#33021;&#25351;&#26631;&#12290;&#22522;&#20110;Lyapunov&#29702;&#35770;&#30340;&#21551;&#31034;&#65292;&#26412;&#30740;&#31350;&#20026;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#32593;&#32476;&#20449;&#21495;&#25511;&#21046;&#35774;&#35745;&#20102;&#19968;&#20010;&#22870;&#21169;&#20989;&#25968;&#65292;&#35813;&#20989;&#25968;&#20351;&#29992;Double Deep Q-Network (DDQN)&#35757;&#32451;&#26234;&#33021;&#20307;&#65292;&#20197;&#26377;&#25928;&#25511;&#21046;&#22797;&#26434;&#30340;&#20132;&#36890;&#32593;&#32476;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#19982;&#20960;&#31181;&#24120;&#29992;&#30340;&#25511;&#21046;&#31639;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
In traffic signal control, flow-based (optimizing the overall flow) and pressure-based methods (equalizing and alleviating congestion) are commonly used but often considered separately. This study introduces a unified framework using Lyapunov control theory, defining specific Lyapunov functions respectively for these methods. We have found interesting results. For example, the well-recognized back-pressure method is equal to differential queue lengths weighted by intersection lane saturation flows. We further improve it by adding basic traffic flow theory. Rather than ensuring that the control system be stable, the system should be also capable of adaptive to various performance metrics. Building on insights from Lyapunov theory, this study designs a reward function for the Reinforcement Learning (RL)-based network signal control, whose agent is trained with Double Deep Q-Network (DDQN) for effective control over complex traffic networks. The proposed algorithm is compared with several
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#20154;&#31867;&#29615;&#22659;&#20013;&#20154;&#24037;&#26234;&#33021;&#30340;&#36879;&#26126;&#21644;&#21487;&#35299;&#37322;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#20110;&#25968;&#23383;&#31995;&#32479;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#21033;&#29992;&#27169;&#24335;&#21644;&#27169;&#24335;&#36716;&#25442;&#30340;&#27010;&#24565;&#23545;&#31995;&#32479;&#30340;&#32467;&#26500;&#36827;&#34892;&#25277;&#35937;&#12290;&#36890;&#36807;&#23558;&#20449;&#24565;&#20989;&#25968;&#30340;&#35780;&#20272;&#35270;&#35273;&#21270;&#20026;&#39640;&#32500;&#20960;&#20309;&#31354;&#38388;&#20013;&#30340;&#36712;&#36857;&#65292;&#35299;&#37322;&#20102;&#31995;&#32479;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2208.05764</link><description>&lt;p&gt;
&#20449;&#24565;&#30340;&#21160;&#24577;&#65306;&#36830;&#32493;&#30417;&#27979;&#21644;&#21487;&#35270;&#21270;&#22797;&#26434;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
The dynamics of belief: continuously monitoring and visualising complex systems. (arXiv:2208.05764v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.05764
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#20154;&#31867;&#29615;&#22659;&#20013;&#20154;&#24037;&#26234;&#33021;&#30340;&#36879;&#26126;&#21644;&#21487;&#35299;&#37322;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#20110;&#25968;&#23383;&#31995;&#32479;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#21033;&#29992;&#27169;&#24335;&#21644;&#27169;&#24335;&#36716;&#25442;&#30340;&#27010;&#24565;&#23545;&#31995;&#32479;&#30340;&#32467;&#26500;&#36827;&#34892;&#25277;&#35937;&#12290;&#36890;&#36807;&#23558;&#20449;&#24565;&#20989;&#25968;&#30340;&#35780;&#20272;&#35270;&#35273;&#21270;&#20026;&#39640;&#32500;&#20960;&#20309;&#31354;&#38388;&#20013;&#30340;&#36712;&#36857;&#65292;&#35299;&#37322;&#20102;&#31995;&#32479;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#29615;&#22659;&#20013;&#20154;&#24037;&#26234;&#33021;&#30340;&#23835;&#36215;&#23545;&#33258;&#21160;&#21270;&#31995;&#32479;&#25552;&#20986;&#20102;&#26032;&#30340;&#36879;&#26126;&#21644;&#21487;&#35299;&#37322;&#24615;&#35201;&#27714;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19982;&#36825;&#31181;&#21487;&#35299;&#37322;&#24615;&#30456;&#20851;&#30340;&#19968;&#20123;&#25311;&#20154;&#21270;&#24605;&#24819;&#21644;&#21407;&#21017;&#65292;&#20197;&#20415;&#22312;&#22797;&#26434;&#20154;&#31867;&#29615;&#22659;&#20013;&#24320;&#21457;&#20851;&#20110;&#25968;&#23383;&#31995;&#32479;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#24182;&#35299;&#20915;&#35299;&#37322;&#20854;&#34892;&#20026;&#30340;&#38382;&#39064;&#12290;&#23601;&#32467;&#26500;&#32780;&#35328;&#65292;&#31995;&#32479;&#30001;&#27169;&#22359;&#21270;&#21644;&#23618;&#27425;&#21270;&#32452;&#20214;&#26500;&#25104;&#65292;&#25105;&#20204;&#20351;&#29992;&#27169;&#24335;&#21644;&#27169;&#24335;&#36716;&#25442;&#30340;&#27010;&#24565;&#22312;&#19968;&#20010;&#26032;&#30340;&#31995;&#32479;&#27169;&#22411;&#20013;&#23545;&#20854;&#36827;&#34892;&#25277;&#35937;&#12290;&#27169;&#24335;&#26159;&#31995;&#32479;&#30340;&#29420;&#31435;&#32452;&#20214;&#65292;&#20855;&#26377;&#33258;&#24049;&#30340;&#30446;&#26631;&#12289;&#30417;&#27979;&#25968;&#25454;&#21644;&#31639;&#27861;&#12290;&#27169;&#24335;&#30340;&#34892;&#20026;&#65292;&#21253;&#25324;&#20854;&#21521;&#20854;&#20182;&#27169;&#24335;&#30340;&#36716;&#25442;&#65292;&#30001;&#35299;&#37322;&#27599;&#20010;&#27169;&#24335;&#30340;&#30417;&#27979;&#25968;&#25454;&#24182;&#26681;&#25454;&#20854;&#30446;&#26631;&#21644;&#31639;&#27861;&#30340;&#20989;&#25968;&#30830;&#23450;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#36825;&#20123;&#20449;&#24565;&#20989;&#25968;&#30340;&#35780;&#20272;&#35270;&#35273;&#21270;&#20026;&#39640;&#32500;&#20960;&#20309;&#31354;&#38388;&#20013;&#30340;&#36712;&#36857;&#26469;&#23637;&#31034;&#23427;&#20204;&#22914;&#20309;&#24110;&#21161;&#35299;&#37322;&#31995;&#32479;&#34892;&#20026;&#12290;&#36825;&#20123;&#24605;&#24819;&#22312;&#25968;&#23398;&#19978;&#34987;&#25277;&#35937;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rise of AI in human contexts places new demands on automated systems to be transparent and explainable. We examine some anthropomorphic ideas and principles relevant to such accountablity in order to develop a theoretical framework for thinking about digital systems in complex human contexts and the problem of explaining their behaviour. Structurally, systems are made of modular and hierachical components, which we abstract in a new system model using notions of modes and mode transitions. A mode is an independent component of the system with its own objectives, monitoring data, and algorithms. The behaviour of a mode, including its transitions to other modes, is determined by functions that interpret each mode's monitoring data in the light of its objectives and algorithms. We show how these belief functions can help explain system behaviour by visualising their evaluation as trajectories in higher-dimensional geometric spaces. These ideas are formalised mathematically by abstract
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Dempster-Shafer&#26041;&#27861;&#30340;&#20540;&#24471;&#20449;&#36182;&#30340;&#20154;&#24037;&#26234;&#33021;&#26694;&#26550;&#21644;&#23454;&#29992;&#31995;&#32479;&#65292;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#12290;&#36890;&#36807;&#26816;&#27979;&#21644;&#32416;&#27491;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#22833;&#36133;&#65292;&#22686;&#24378;&#20102;&#20854;&#21487;&#20449;&#24615;&#21644;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2204.02779</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;Dempster-Shafer&#26041;&#27861;&#30340;&#20540;&#24471;&#20449;&#36182;&#30340;&#20154;&#24037;&#26234;&#33021;&#65306;&#32974;&#20799;&#33041;MRI&#20998;&#21106;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
A Dempster-Shafer approach to trustworthy AI with application to fetal brain MRI segmentation. (arXiv:2204.02779v4 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.02779
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Dempster-Shafer&#26041;&#27861;&#30340;&#20540;&#24471;&#20449;&#36182;&#30340;&#20154;&#24037;&#26234;&#33021;&#26694;&#26550;&#21644;&#23454;&#29992;&#31995;&#32479;&#65292;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#12290;&#36890;&#36807;&#26816;&#27979;&#21644;&#32416;&#27491;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#22833;&#36133;&#65292;&#22686;&#24378;&#20102;&#20854;&#21487;&#20449;&#24615;&#21644;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#30149;&#29702;&#24773;&#20917;&#21644;&#19982;&#35757;&#32451;&#22270;&#20687;&#25293;&#25668;&#22312;&#19981;&#21516;&#20013;&#24515;&#30340;&#22270;&#20687;&#20013;&#21487;&#33021;&#20986;&#29616;&#24847;&#22806;&#21644;&#26174;&#33879;&#30340;&#22833;&#36133;&#65292;&#20854;&#26631;&#31614;&#38169;&#35823;&#36829;&#21453;&#20102;&#19987;&#23478;&#30693;&#35782;&#12290;&#36825;&#20123;&#38169;&#35823;&#21066;&#24369;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#21487;&#20449;&#24615;&#12290;&#26816;&#27979;&#21644;&#32416;&#27491;&#36825;&#20123;&#22833;&#36133;&#30340;&#26426;&#21046;&#23545;&#20110;&#23433;&#20840;&#22320;&#23558;&#36825;&#39033;&#25216;&#26415;&#24212;&#29992;&#20110;&#20020;&#24202;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#65292;&#24182;&#19988;&#21487;&#33021;&#25104;&#20026;&#26410;&#26469;&#20851;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#27861;&#35268;&#30340;&#35201;&#27714;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20540;&#24471;&#20449;&#36182;&#30340;&#20154;&#24037;&#26234;&#33021;&#29702;&#35770;&#26694;&#26550;&#21644;&#19968;&#20010;&#23454;&#29992;&#31995;&#32479;&#65292;&#21487;&#20197;&#20351;&#29992;Dempster-Shafer&#29702;&#35770;&#30340;&#22791;&#29992;&#26041;&#27861;&#21644;&#25925;&#38556;&#23433;&#20840;&#26426;&#21046;&#26469;&#22686;&#24378;&#20219;&#20309;&#39592;&#24178;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#23545;&#20540;&#24471;&#20449;&#36182;&#30340;&#20154;&#24037;&#26234;&#33021;&#30340;&#21487;&#25805;&#20316;&#23450;&#20041;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#33258;&#21160;&#20002;&#24323;&#39592;&#24178;&#20154;&#24037;&#26234;&#33021;&#39044;&#27979;&#30340;&#20307;&#32032;&#32423;&#26631;&#31614;&#65292;&#36825;&#20123;&#26631;&#31614;&#36829;&#21453;&#20102;&#19987;&#23478;&#30693;&#35782;&#65292;&#24182;&#23545;&#36825;&#20123;&#20307;&#32032;&#20351;&#29992;&#22791;&#29992;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning models for medical image segmentation can fail unexpectedly and spectacularly for pathological cases and images acquired at different centers than training images, with labeling errors that violate expert knowledge. Such errors undermine the trustworthiness of deep learning models for medical image segmentation. Mechanisms for detecting and correcting such failures are essential for safely translating this technology into clinics and are likely to be a requirement of future regulations on artificial intelligence (AI). In this work, we propose a trustworthy AI theoretical framework and a practical system that can augment any backbone AI system using a fallback method and a fail-safe mechanism based on Dempster-Shafer theory. Our approach relies on an actionable definition of trustworthy AI. Our method automatically discards the voxel-level labeling predicted by the backbone AI that violate expert knowledge and relies on a fallback for those voxels. We demonstrate the effec
&lt;/p&gt;</description></item></channel></rss>