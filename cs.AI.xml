<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#22810;&#27169;&#24577;&#22810;&#20219;&#21153;&#27169;&#22411;&#36873;&#25321;&#22120;&#65288;EMMS&#65289;&#65292;&#23427;&#20351;&#29992;&#22823;&#35268;&#27169;&#30340;&#22522;&#30784;&#27169;&#22411;&#23558;&#19981;&#21516;&#20219;&#21153;&#30340;&#26631;&#31614;&#26684;&#24335;&#36716;&#25442;&#20026;&#32479;&#19968;&#30340;&#22122;&#22768;&#26631;&#31614;&#23884;&#20837;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#21152;&#26435;&#32447;&#24615;&#22238;&#24402;&#26469;&#20272;&#35745;&#27169;&#22411;&#30340;&#21487;&#36801;&#31227;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.06262</link><description>&lt;p&gt;
Foundation Model&#26159;&#19968;&#20010;&#39640;&#25928;&#30340;&#22810;&#27169;&#24577;&#22810;&#20219;&#21153;&#27169;&#22411;&#36873;&#25321;&#22120;
&lt;/p&gt;
&lt;p&gt;
Foundation Model is Efficient Multimodal Multitask Model Selector. (arXiv:2308.06262v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06262
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#22810;&#27169;&#24577;&#22810;&#20219;&#21153;&#27169;&#22411;&#36873;&#25321;&#22120;&#65288;EMMS&#65289;&#65292;&#23427;&#20351;&#29992;&#22823;&#35268;&#27169;&#30340;&#22522;&#30784;&#27169;&#22411;&#23558;&#19981;&#21516;&#20219;&#21153;&#30340;&#26631;&#31614;&#26684;&#24335;&#36716;&#25442;&#20026;&#32479;&#19968;&#30340;&#22122;&#22768;&#26631;&#31614;&#23884;&#20837;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#21152;&#26435;&#32447;&#24615;&#22238;&#24402;&#26469;&#20272;&#35745;&#27169;&#22411;&#30340;&#21487;&#36801;&#31227;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#19981;&#24120;&#35265;&#20294;&#38750;&#24120;&#37325;&#35201;&#30340;&#38382;&#39064;&#65306;&#22312;&#32473;&#23450;&#19968;&#32452;&#39044;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#20309;&#22312;&#19981;&#23545;&#23427;&#20204;&#36827;&#34892;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#23427;&#20204;&#22312;&#27599;&#20010;&#22810;&#27169;&#24577;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#27604;&#22914;&#22270;&#20687;&#35782;&#21035;&#12289;&#25351;&#20195;&#12289;&#23383;&#24149;&#29983;&#25104;&#12289;&#35270;&#35273;&#38382;&#31572;&#21644;&#25991;&#23383;&#38382;&#31572;&#12290;&#19968;&#31181;&#34542;&#21147;&#30340;&#26041;&#27861;&#26159;&#23545;&#25152;&#26377;&#27169;&#22411;&#22312;&#25152;&#26377;&#30446;&#26631;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#36825;&#20250;&#24102;&#26469;&#39640;&#35745;&#31639;&#25104;&#26412;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#19968;&#20123;&#20808;&#36827;&#26041;&#27861;&#37319;&#29992;&#36731;&#37327;&#32423;&#25351;&#26631;&#26469;&#34913;&#37327;&#27169;&#22411;&#30340;&#21487;&#36801;&#31227;&#24615;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#20005;&#37325;&#20381;&#36182;&#20110;&#21333;&#20010;&#20219;&#21153;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#20351;&#24471;&#23427;&#20204;&#22312;&#22810;&#27169;&#24577;&#22810;&#20219;&#21153;&#30340;&#22330;&#26223;&#20013;&#19981;&#36866;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#22810;&#20219;&#21153;&#27169;&#22411;&#36873;&#25321;&#22120;&#65288;EMMS&#65289;&#65292;&#23427;&#20351;&#29992;&#22823;&#35268;&#27169;&#30340;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#65292;&#23558;&#19981;&#21516;&#19979;&#28216;&#20219;&#21153;&#30340;&#21508;&#31181;&#26631;&#31614;&#26684;&#24335;&#65292;&#22914;&#31867;&#21035;&#12289;&#25991;&#26412;&#21644;&#36793;&#30028;&#26694;&#36716;&#25442;&#20026;&#32479;&#19968;&#30340;&#22122;&#22768;&#26631;&#31614;&#23884;&#20837;&#12290;EMMS&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#30340;&#21152;&#26435;&#32447;&#24615;&#22238;&#24402;&#26469;&#20272;&#35745;&#27169;&#22411;&#30340;&#21487;&#36801;&#31227;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates an under-explored but important problem: given a collection of pre-trained neural networks, predicting their performance on each multi-modal task without fine-tuning them, such as image recognition, referring, captioning, visual question answering, and text question answering. A brute-force approach is to finetune all models on all target datasets, bringing high computational costs. Although recent-advanced approaches employed lightweight metrics to measure models' transferability,they often depend heavily on the prior knowledge of a single task, making them inapplicable in a multi-modal multi-task scenario. To tackle this issue, we propose an efficient multi-task model selector (EMMS), which employs large-scale foundation models to transform diverse label formats such as categories, texts, and bounding boxes of different downstream tasks into a unified noisy label embedding. EMMS can estimate a model's transferability through a simple weighted linear regression
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20195;&#30721;&#30340;&#26041;&#27861;&#26469;&#22686;&#24378;&#32593;&#32476;&#31649;&#29702;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#29983;&#25104;&#29305;&#23450;&#20219;&#21153;&#30340;&#20195;&#30721;&#65292;&#35299;&#20915;&#20102;&#21487;&#35299;&#37322;&#24615;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#38544;&#31169;&#24615;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#24182;&#23637;&#31034;&#20102;&#39640;&#20934;&#30830;&#24615;&#12289;&#25104;&#26412;&#25928;&#30410;&#20197;&#21450;&#21518;&#32493;&#20351;&#29992;&#31243;&#24207;&#32508;&#21512;&#25216;&#26415;&#36827;&#19968;&#27493;&#22686;&#24378;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.06261</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#20195;&#30721;&#22686;&#24378;&#32593;&#32476;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
Enhancing Network Management Using Code Generated by Large Language Models. (arXiv:2308.06261v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06261
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20195;&#30721;&#30340;&#26041;&#27861;&#26469;&#22686;&#24378;&#32593;&#32476;&#31649;&#29702;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#29983;&#25104;&#29305;&#23450;&#20219;&#21153;&#30340;&#20195;&#30721;&#65292;&#35299;&#20915;&#20102;&#21487;&#35299;&#37322;&#24615;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#38544;&#31169;&#24615;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#24182;&#23637;&#31034;&#20102;&#39640;&#20934;&#30830;&#24615;&#12289;&#25104;&#26412;&#25928;&#30410;&#20197;&#21450;&#21518;&#32493;&#20351;&#29992;&#31243;&#24207;&#32508;&#21512;&#25216;&#26415;&#36827;&#19968;&#27493;&#22686;&#24378;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#26512;&#32593;&#32476;&#25299;&#25169;&#21644;&#36890;&#20449;&#22270;&#22312;&#24403;&#20195;&#32593;&#32476;&#31649;&#29702;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#19968;&#31181;&#19968;&#33268;&#30340;&#26041;&#27861;&#23548;&#33268;&#20102;&#23398;&#20064;&#26354;&#32447;&#30340;&#25361;&#25112;&#24615;&#22686;&#21152;&#12289;&#38169;&#35823;&#29575;&#30340;&#22686;&#21152;&#21644;&#25928;&#29575;&#30340;&#38477;&#20302;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20174;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#20013;&#29983;&#25104;&#29305;&#23450;&#20219;&#21153;&#30340;&#20195;&#30721;&#65292;&#20197;&#26041;&#20415;&#33258;&#28982;&#35821;&#35328;&#32593;&#32476;&#31649;&#29702;&#20307;&#39564;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#20801;&#35768;&#32593;&#32476;&#36816;&#33829;&#21830;&#26816;&#26597;&#29983;&#25104;&#30340;&#20195;&#30721;&#26469;&#35299;&#20915;&#20102;&#21487;&#35299;&#37322;&#24615;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#38544;&#31169;&#24615;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#36991;&#20813;&#20102;&#19982;LLMs&#20849;&#20139;&#32593;&#32476;&#25968;&#25454;&#65292;&#24182;&#38598;&#20013;&#24037;&#20316;&#20110;&#24212;&#29992;&#31243;&#24207;&#29305;&#23450;&#35831;&#27714;&#19982;&#36890;&#29992;&#31243;&#24207;&#32508;&#21512;&#25216;&#26415;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#35774;&#35745;&#21644;&#35780;&#20272;&#20102;&#19968;&#20010;&#21407;&#22411;&#31995;&#32479;&#65292;&#20351;&#29992;&#22522;&#20934;&#24212;&#29992;&#31243;&#24207;&#23637;&#31034;&#20102;&#39640;&#20934;&#30830;&#24615;&#12289;&#25104;&#26412;&#25928;&#30410;&#20197;&#21450;&#20351;&#29992;&#20114;&#34917;&#31243;&#24207;&#32508;&#21512;&#25216;&#26415;&#36827;&#19968;&#27493;&#22686;&#24378;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Analyzing network topologies and communication graphs plays a crucial role in contemporary network management. However, the absence of a cohesive approach leads to a challenging learning curve, heightened errors, and inefficiencies. In this paper, we introduce a novel approach to facilitate a natural-language-based network management experience, utilizing large language models (LLMs) to generate task-specific code from natural language queries. This method tackles the challenges of explainability, scalability, and privacy by allowing network operators to inspect the generated code, eliminating the need to share network data with LLMs, and concentrating on application-specific requests combined with general program synthesis techniques. We design and evaluate a prototype system using benchmark applications, showcasing high accuracy, cost-effectiveness, and the potential for further enhancements using complementary program synthesis techniques.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;ChatGPT&#30340;&#25237;&#36164;&#32452;&#21512;&#36873;&#25321;&#26041;&#27861;&#65292;&#21457;&#29616;ChatGPT&#23545;&#20110;&#32929;&#31080;&#36873;&#25321;&#26377;&#25928;&#65292;&#20294;&#22312;&#26435;&#37325;&#20998;&#37197;&#26041;&#38754;&#21487;&#33021;&#19981;&#22815;&#29702;&#24819;&#12290;&#28982;&#32780;&#65292;&#24403;&#23558;ChatGPT&#30340;&#32929;&#31080;&#36873;&#25321;&#19982;&#32452;&#21512;&#20248;&#21270;&#27169;&#22411;&#30456;&#32467;&#21512;&#26102;&#65292;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.06260</link><description>&lt;p&gt;
&#22522;&#20110;ChatGPT&#30340;&#25237;&#36164;&#32452;&#21512;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
ChatGPT-based Investment Portfolio Selection. (arXiv:2308.06260v1 [q-fin.PM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06260
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;ChatGPT&#30340;&#25237;&#36164;&#32452;&#21512;&#36873;&#25321;&#26041;&#27861;&#65292;&#21457;&#29616;ChatGPT&#23545;&#20110;&#32929;&#31080;&#36873;&#25321;&#26377;&#25928;&#65292;&#20294;&#22312;&#26435;&#37325;&#20998;&#37197;&#26041;&#38754;&#21487;&#33021;&#19981;&#22815;&#29702;&#24819;&#12290;&#28982;&#32780;&#65292;&#24403;&#23558;ChatGPT&#30340;&#32929;&#31080;&#36873;&#25321;&#19982;&#32452;&#21512;&#20248;&#21270;&#27169;&#22411;&#30456;&#32467;&#21512;&#26102;&#65292;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#21033;&#29992;&#29983;&#25104;&#24335;AI&#27169;&#22411;&#65288;&#22914;ChatGPT&#65289;&#36827;&#34892;&#25237;&#36164;&#32452;&#21512;&#36873;&#25321;&#30340;&#28508;&#22312;&#29992;&#36884;&#12290;&#30001;&#20110;&#27169;&#22411;&#30340;&#8220;&#24187;&#35273;&#8221;&#65292;&#23545;&#26469;&#33258;&#29983;&#25104;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;&#65288;GPT&#65289;&#27169;&#22411;&#30340;&#25237;&#36164;&#24314;&#35758;&#30340;&#20449;&#20219;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#38656;&#35201;&#23545;&#36755;&#20986;&#36827;&#34892;&#20180;&#32454;&#39564;&#35777;&#21644;&#39564;&#35777;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;ChatGPT&#20174;S&amp;P500&#24066;&#22330;&#25351;&#25968;&#20013;&#33719;&#21462;&#19968;&#32452;&#26377;&#25237;&#36164;&#21560;&#24341;&#21147;&#30340;&#32929;&#31080;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23558;&#21033;&#29992;&#36825;&#20010;&#30001;AI&#29983;&#25104;&#30340;&#20132;&#26131;&#32929;&#31080;&#32452;&#21512;&#19982;&#23450;&#37327;&#32452;&#21512;&#20248;&#21270;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#19982;&#19968;&#20123;&#27969;&#34892;&#30340;&#25237;&#36164;&#22522;&#37329;&#36827;&#34892;&#23545;&#27604;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;ChatGPT&#22312;&#32929;&#31080;&#36873;&#25321;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#22312;&#20998;&#37197;&#32929;&#31080;&#32452;&#21512;&#20013;&#30340;&#26368;&#20248;&#26435;&#37325;&#26041;&#38754;&#21487;&#33021;&#34920;&#29616;&#19981;&#20339;&#12290;&#20294;&#26159;&#65292;&#24403;ChatGPT&#30340;&#32929;&#31080;&#36873;&#25321;&#19982;&#24050;&#24314;&#31435;&#30340;&#32452;&#21512;&#20248;&#21270;&#27169;&#22411;&#30456;&#32467;&#21512;&#26102;&#65292;&#25105;&#20204;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we explore potential uses of generative AI models, such as ChatGPT, for investment portfolio selection. Trusting investment advice from Generative Pre-Trained Transformer (GPT) models is a challenge due to model "hallucinations", necessitating careful verification and validation of the output. Therefore, we take an alternative approach. We use ChatGPT to obtain a universe of stocks from S&amp;P500 market index that are potentially attractive for investing. Subsequently, we compared various portfolio optimization strategies that utilized this AI-generated trading universe, evaluating those against quantitative portfolio optimization models as well as comparing to some of the popular investment funds. Our findings indicate that ChatGPT is effective in stock selection but may not perform as well in assigning optimal weights to stocks within the portfolio. But when stocks selection by ChatGPT is combined with established portfolio optimization models, we achieve even better resu
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27493;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#35774;&#35745;&#28145;&#24230;&#33258;&#32534;&#30721;&#22120;&#65292;&#24182;&#36890;&#36807;&#20462;&#21098;&#21644;&#22686;&#38271;&#26041;&#27861;&#20197;&#21450;&#20248;&#21270;&#38544;&#34255;&#23618;&#22823;&#23567;&#21644;&#35757;&#32451;&#26102;&#38271;&#26469;&#25913;&#21892;&#20854;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.06221</link><description>&lt;p&gt;
&#20351;&#29992;&#20108;&#38454;&#31639;&#27861;&#33258;&#21160;&#35843;&#25972;&#21644;&#35757;&#32451;&#39640;&#25928;&#30340;&#28145;&#24230;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Automated Sizing and Training of Efficient Deep Autoencoders using Second Order Algorithms. (arXiv:2308.06221v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06221
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27493;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#35774;&#35745;&#28145;&#24230;&#33258;&#32534;&#30721;&#22120;&#65292;&#24182;&#36890;&#36807;&#20462;&#21098;&#21644;&#22686;&#38271;&#26041;&#27861;&#20197;&#21450;&#20248;&#21270;&#38544;&#34255;&#23618;&#22823;&#23567;&#21644;&#35757;&#32451;&#26102;&#38271;&#26469;&#25913;&#21892;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27493;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#35774;&#35745;&#24191;&#20041;&#32447;&#24615;&#20998;&#31867;&#22120;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#22238;&#24402;&#25214;&#21040;&#19968;&#20010;&#21021;&#22987;&#30340;&#22810;&#31867;&#32447;&#24615;&#20998;&#31867;&#22120;&#12290;&#28982;&#21518;&#36890;&#36807;&#20462;&#21098;&#19981;&#24517;&#35201;&#30340;&#36755;&#20837;&#26469;&#26368;&#23567;&#21270;&#39564;&#35777;&#35823;&#24046;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#31867;&#20284;&#20110;Ho-Kashyap&#35268;&#21017;&#30340;&#26041;&#27861;&#25913;&#21892;&#26399;&#26395;&#36755;&#20986;&#12290;&#25509;&#19979;&#26469;&#65292;&#23558;&#36755;&#20986;&#21028;&#21035;&#24335;&#32553;&#25918;&#20026;&#24191;&#20041;&#32447;&#24615;&#20998;&#31867;&#22120;&#20013;S&#22411;&#36755;&#20986;&#21333;&#20803;&#30340;&#32593;&#32476;&#20989;&#25968;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#26063;&#25209;&#37327;&#35757;&#32451;&#31639;&#27861;&#65292;&#29992;&#20110;&#20248;&#21270;&#22810;&#23618;&#24863;&#30693;&#26426;&#30340;&#38544;&#34255;&#23618;&#22823;&#23567;&#21644;&#35757;&#32451;&#26102;&#38271;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#23558;&#20462;&#21098;&#19982;&#22686;&#38271;&#26041;&#27861;&#30456;&#32467;&#21512;&#12290;&#28982;&#21518;&#65292;&#23558;&#36755;&#20837;&#21333;&#20803;&#32553;&#25918;&#20026;S&#22411;&#36755;&#20986;&#21333;&#20803;&#30340;&#32593;&#32476;&#20989;&#25968;&#65292;&#28982;&#21518;&#23558;&#20854;&#20316;&#20026;&#36755;&#20837;&#39304;&#36865;&#21040;MLP&#20013;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22359;&#20013;&#30340;&#25913;&#36827;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#28145;&#24230;&#26550;&#26500;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#20851;&#20110;d&#30340;&#23398;&#20064;&#31639;&#27861;&#30340;&#21407;&#21017;&#21644;&#20844;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a multi-step training method for designing generalized linear classifiers. First, an initial multi-class linear classifier is found through regression. Then validation error is minimized by pruning of unnecessary inputs. Simultaneously, desired outputs are improved via a method similar to the Ho-Kashyap rule. Next, the output discriminants are scaled to be net functions of sigmoidal output units in a generalized linear classifier. We then develop a family of batch training algorithm for the multi layer perceptron that optimizes its hidden layer size and number of training epochs. Next, we combine pruning with a growing approach. Later, the input units are scaled to be the net function of the sigmoidal output units that are then feed into as input to the MLP. We then propose resulting improvements in each of the deep learning blocks thereby improving the overall performance of the deep architecture. We discuss the principles and formulation regarding learning algorithms for d
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32508;&#36848;&#20102;&#20132;&#36890;&#31649;&#29702;&#31995;&#32479;&#23433;&#20840;&#24615;&#30340;&#25991;&#29486;&#65292;&#24635;&#32467;&#20102;&#20132;&#36890;&#31649;&#29702;&#31995;&#32479;&#20013;&#30340;&#23433;&#20840;&#38382;&#39064;&#12289;&#24403;&#21069;&#30340;&#30740;&#31350;&#29616;&#29366;&#20197;&#21450;&#30830;&#20445;&#23433;&#20840;&#24615;&#30340;&#25216;&#26415;&#21644;&#26041;&#27861;&#12290;&#36825;&#39033;&#32508;&#36848;&#36824;&#25552;&#20986;&#20102;&#29616;&#26377;&#30740;&#31350;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2308.06204</link><description>&lt;p&gt;
&#20132;&#36890;&#31649;&#29702;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#65306;&#19968;&#39033;&#20840;&#38754;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Safety in Traffic Management Systems: A Comprehensive Survey. (arXiv:2308.06204v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06204
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32508;&#36848;&#20102;&#20132;&#36890;&#31649;&#29702;&#31995;&#32479;&#23433;&#20840;&#24615;&#30340;&#25991;&#29486;&#65292;&#24635;&#32467;&#20102;&#20132;&#36890;&#31649;&#29702;&#31995;&#32479;&#20013;&#30340;&#23433;&#20840;&#38382;&#39064;&#12289;&#24403;&#21069;&#30340;&#30740;&#31350;&#29616;&#29366;&#20197;&#21450;&#30830;&#20445;&#23433;&#20840;&#24615;&#30340;&#25216;&#26415;&#21644;&#26041;&#27861;&#12290;&#36825;&#39033;&#32508;&#36848;&#36824;&#25552;&#20986;&#20102;&#29616;&#26377;&#30740;&#31350;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#31649;&#29702;&#31995;&#32479;&#22312;&#20445;&#38556;&#36947;&#36335;&#19978;&#30340;&#23433;&#20840;&#21644;&#39640;&#25928;&#20132;&#36890;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#20132;&#36890;&#31649;&#29702;&#31995;&#32479;&#20013;&#30340;&#20808;&#36827;&#25216;&#26415;&#24341;&#20837;&#20102;&#26032;&#30340;&#23433;&#20840;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#30830;&#20445;&#36825;&#20123;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#26159;&#37325;&#35201;&#30340;&#65292;&#20197;&#38450;&#27490;&#20107;&#25925;&#24182;&#20943;&#23567;&#23545;&#36947;&#36335;&#29992;&#25143;&#30340;&#24433;&#21709;&#12290;&#26412;&#35843;&#26597;&#32508;&#36848;&#20102;&#20851;&#20110;&#20132;&#36890;&#31649;&#29702;&#31995;&#32479;&#23433;&#20840;&#24615;&#30340;&#25991;&#29486;&#65292;&#20855;&#20307;&#35752;&#35770;&#20102;&#20132;&#36890;&#31649;&#29702;&#31995;&#32479;&#20013;&#28041;&#21450;&#30340;&#19981;&#21516;&#23433;&#20840;&#38382;&#39064;&#12289;&#24403;&#21069;&#20851;&#20110;&#36825;&#20123;&#31995;&#32479;&#23433;&#20840;&#24615;&#30340;&#30740;&#31350;&#29616;&#29366;&#20197;&#21450;&#30830;&#20445;&#36825;&#20123;&#31995;&#32479;&#23433;&#20840;&#24615;&#30340;&#25216;&#26415;&#21644;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#25351;&#20986;&#20102;&#29616;&#26377;&#30740;&#31350;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic management systems play a vital role in ensuring safe and efficient transportation on roads. However, the use of advanced technologies in traffic management systems has introduced new safety challenges. Therefore, it is important to ensure the safety of these systems to prevent accidents and minimize their impact on road users. In this survey, we provide a comprehensive review of the literature on safety in traffic management systems. Specifically, we discuss the different safety issues that arise in traffic management systems, the current state of research on safety in these systems, and the techniques and methods proposed to ensure the safety of these systems. We also identify the limitations of the existing research and suggest future research directions.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22240;&#26524;&#24615;&#27010;&#29575;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#26426;&#22120;&#20154;&#22534;&#31215;&#26041;&#22359;&#20219;&#21153;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#32467;&#21512;&#22240;&#26524;&#25512;&#26029;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#29702;&#35299;&#12289;&#25512;&#29702;&#21644;&#35299;&#37322;&#20854;&#29615;&#22659;&#12290;</title><link>http://arxiv.org/abs/2308.06203</link><description>&lt;p&gt;
&#20026;&#26426;&#22120;&#20154;&#22534;&#31215;&#26041;&#22359;&#20219;&#21153;&#26500;&#24314;&#22240;&#26524;&#24615;&#27010;&#29575;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Towards a Causal Probabilistic Framework for Prediction, Action-Selection &amp; Explanations for Robot Block-Stacking Tasks. (arXiv:2308.06203v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06203
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22240;&#26524;&#24615;&#27010;&#29575;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#26426;&#22120;&#20154;&#22534;&#31215;&#26041;&#22359;&#20219;&#21153;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#32467;&#21512;&#22240;&#26524;&#25512;&#26029;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#29702;&#35299;&#12289;&#25512;&#29702;&#21644;&#35299;&#37322;&#20854;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#24847;&#21619;&#30528;&#31995;&#32479;&#35774;&#35745;&#32773;&#26080;&#27861;&#39044;&#27979;&#24182;&#26126;&#30830;&#35774;&#35745;&#20986;&#26426;&#22120;&#20154;&#21487;&#33021;&#36935;&#21040;&#30340;&#25152;&#26377;&#22330;&#26223;&#12290;&#22240;&#27492;&#65292;&#20197;&#36825;&#31181;&#26041;&#24335;&#35774;&#35745;&#30340;&#26426;&#22120;&#20154;&#22312;&#39640;&#24230;&#21463;&#25511;&#30340;&#29615;&#22659;&#20043;&#22806;&#23481;&#26131;&#20986;&#29616;&#25925;&#38556;&#12290;&#22240;&#26524;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#20010;&#21407;&#21017;&#24615;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#32534;&#30721;&#26426;&#22120;&#20154;&#19982;&#20854;&#29615;&#22659;&#30456;&#20114;&#20316;&#29992;&#30340;&#22240;&#26524;&#20851;&#31995;&#30340;&#24418;&#24335;&#21270;&#30693;&#35782;&#65292;&#24182;&#32467;&#21512;&#29616;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#36890;&#24120;&#36935;&#21040;&#30340;&#22122;&#22768;&#21644;&#19981;&#30830;&#23450;&#24615;&#30340;&#27010;&#29575;&#34920;&#31034;&#12290;&#32467;&#21512;&#22240;&#26524;&#25512;&#26029;&#65292;&#36825;&#20123;&#27169;&#22411;&#20351;&#33258;&#20027;&#20195;&#29702;&#33021;&#22815;&#29702;&#35299;&#12289;&#25512;&#29702;&#21644;&#35299;&#37322;&#20854;&#29615;&#22659;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#26426;&#22120;&#20154;&#22534;&#31215;&#26041;&#22359;&#20219;&#21153;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#23637;&#31034;&#20102;&#35768;&#22810;&#24212;&#29992;&#25152;&#38656;&#30340;&#22522;&#26412;&#24863;&#30693;&#21644;&#25805;&#20316;&#33021;&#21147;&#65292;&#21253;&#25324;&#20179;&#24211;&#29289;&#27969;&#21644;&#23478;&#24237;&#20154;&#24037;&#25903;&#25345;&#26426;&#22120;&#20154;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22240;&#26524;&#24615;&#27010;&#29575;&#26694;&#26550;&#65292;&#23558;&#29289;&#29702;&#27169;&#25311;&#21151;&#33021;&#23884;&#20837;&#21040;&#36825;&#20010;&#20219;&#21153;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Uncertainties in the real world mean that is impossible for system designers to anticipate and explicitly design for all scenarios that a robot might encounter. Thus, robots designed like this are fragile and fail outside of highly-controlled environments. Causal models provide a principled framework to encode formal knowledge of the causal relationships that govern the robot's interaction with its environment, in addition to probabilistic representations of noise and uncertainty typically encountered by real-world robots. Combined with causal inference, these models permit an autonomous agent to understand, reason about, and explain its environment. In this work, we focus on the problem of a robot block-stacking task due to the fundamental perception and manipulation capabilities it demonstrates, required by many applications including warehouse logistics and domestic human support robotics. We propose a novel causal probabilistic framework to embed a physics simulation capability int
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20154;&#29289;&#21644;&#29289;&#20307;&#20132;&#20114;&#26816;&#27979;&#20013;&#30340;&#35859;&#35789;&#35270;&#35273;&#32972;&#26223;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20132;&#21449;&#27880;&#24847;&#21147;&#21644;&#30418;&#23376;&#37197;&#23545;&#20301;&#32622;&#23884;&#20837;&#31561;&#26041;&#24335;&#26469;&#25913;&#36827;&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.06202</link><description>&lt;p&gt;
&#22312;&#26816;&#27979;&#20154;&#29289;&#21644;&#29289;&#20307;&#20132;&#20114;&#20013;&#25506;&#32034;&#35859;&#35789;&#35270;&#35273;&#32972;&#26223;
&lt;/p&gt;
&lt;p&gt;
Exploring Predicate Visual Context in Detecting of Human-Object Interactions. (arXiv:2308.06202v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06202
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20154;&#29289;&#21644;&#29289;&#20307;&#20132;&#20114;&#26816;&#27979;&#20013;&#30340;&#35859;&#35789;&#35270;&#35273;&#32972;&#26223;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20132;&#21449;&#27880;&#24847;&#21147;&#21644;&#30418;&#23376;&#37197;&#23545;&#20301;&#32622;&#23884;&#20837;&#31561;&#26041;&#24335;&#26469;&#25913;&#36827;&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;DETR&#26694;&#26550;&#24050;&#25104;&#20026;&#20154;&#29289;&#21644;&#29289;&#20307;&#20132;&#20114;&#65288;HOI&#65289;&#30740;&#31350;&#30340;&#20027;&#35201;&#26041;&#27861;&#12290;&#29305;&#21035;&#26159;&#65292;&#22522;&#20110;&#20004;&#38454;&#27573;&#21464;&#25442;&#22120;&#30340;HOI&#26816;&#27979;&#22120;&#26159;&#24615;&#33021;&#26368;&#22909;&#21644;&#35757;&#32451;&#26368;&#39640;&#25928;&#30340;&#26041;&#27861;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#20197;&#32570;&#20047;&#32454;&#31890;&#24230;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#29289;&#20307;&#29305;&#24449;&#20316;&#20026;HOI&#20998;&#31867;&#30340;&#26465;&#20214;&#65292;&#32780;&#24573;&#35270;&#20102;&#23039;&#21183;&#21644;&#26041;&#21521;&#20449;&#24687;&#65292;&#32780;&#26356;&#27880;&#37325;&#20851;&#20110;&#29289;&#20307;&#36523;&#20221;&#21644;&#36793;&#30028;&#30340;&#35270;&#35273;&#25552;&#31034;&#12290;&#36825;&#33258;&#28982;&#22320;&#38459;&#30861;&#20102;&#23545;&#22797;&#26434;&#25110;&#27169;&#31946;&#20132;&#20114;&#30340;&#35782;&#21035;&#12290;&#26412;&#25991;&#36890;&#36807;&#21487;&#35270;&#21270;&#21644;&#31934;&#24515;&#35774;&#35745;&#30340;&#23454;&#39564;&#30740;&#31350;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36890;&#36807;&#20132;&#21449;&#27880;&#24847;&#21147;&#37325;&#26032;&#24341;&#20837;&#22270;&#20687;&#29305;&#24449;&#65292;&#24182;&#25913;&#36827;&#20102;&#26597;&#35810;&#35774;&#35745;&#65292;&#24191;&#27867;&#25506;&#32034;&#20102;&#38190;&#21644;&#20540;&#65292;&#20197;&#21450;&#20351;&#29992;&#30418;&#23376;&#37197;&#23545;&#20301;&#32622;&#23884;&#20837;&#20316;&#20026;&#31354;&#38388;&#25351;&#23548;&#12290;&#25105;&#20204;&#30340;&#25913;&#36827;&#35859;&#35789;&#35270;&#35273;&#32972;&#26223;&#65288;PViC&#65289;&#27169;&#22411;&#22312;HICO-DET&#21644;V-COCO&#22522;&#20934;&#27979;&#35797;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, the DETR framework has emerged as the dominant approach for human--object interaction (HOI) research. In particular, two-stage transformer-based HOI detectors are amongst the most performant and training-efficient approaches. However, these often condition HOI classification on object features that lack fine-grained contextual information, eschewing pose and orientation information in favour of visual cues about object identity and box extremities. This naturally hinders the recognition of complex or ambiguous interactions. In this work, we study these issues through visualisations and carefully designed experiments. Accordingly, we investigate how best to re-introduce image features via cross-attention. With an improved query design, extensive exploration of keys and values, and box pair positional embeddings as spatial guidance, our model with enhanced predicate visual context (PViC) outperforms state-of-the-art methods on the HICO-DET and V-COCO benchmarks, while maintaini
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#30693;&#35782;&#33976;&#39311;&#30340;&#26032;&#39062;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23569;&#37327;&#35757;&#32451;&#26679;&#26412;&#65292;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#26032;&#30340;&#22797;&#21512;&#34920;&#24773;&#31867;&#21035;&#12290;</title><link>http://arxiv.org/abs/2308.06197</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#30693;&#35782;&#33976;&#39311;&#22522;&#26412;&#29305;&#24449;&#36827;&#34892;&#22797;&#26434;&#38754;&#37096;&#34920;&#24773;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Complex Facial Expression Recognition Using Deep Knowledge Distillation of Basic Features. (arXiv:2308.06197v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06197
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#30693;&#35782;&#33976;&#39311;&#30340;&#26032;&#39062;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23569;&#37327;&#35757;&#32451;&#26679;&#26412;&#65292;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#26032;&#30340;&#22797;&#21512;&#34920;&#24773;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#24773;&#32490;&#35782;&#21035;&#26159;&#19968;&#39033;&#35748;&#30693;&#20219;&#21153;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#20854;&#34920;&#29616;&#22312;&#19982;&#20154;&#31867;&#35748;&#30693;&#27700;&#24179;&#30456;&#31561;&#25110;&#20197;&#19978;&#30340;&#20854;&#20182;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#31168;&#30340;&#31243;&#24230;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#36739;&#20026;&#22256;&#38590;&#30340;&#12290;&#30001;&#20110;&#20154;&#33080;&#34920;&#36798;&#30340;&#24773;&#32490;&#22797;&#26434;&#24615;&#65292;&#36890;&#36807;&#38754;&#37096;&#34920;&#24773;&#36827;&#34892;&#24773;&#32490;&#35782;&#21035;&#29305;&#21035;&#22256;&#38590;&#12290;&#20026;&#20102;&#20351;&#26426;&#22120;&#22312;&#36825;&#19968;&#39046;&#22495;&#36798;&#21040;&#19982;&#20154;&#31867;&#30456;&#21516;&#30340;&#34920;&#29616;&#27700;&#24179;&#65292;&#23427;&#21487;&#33021;&#38656;&#35201;&#23454;&#26102;&#21512;&#25104;&#30693;&#35782;&#24182;&#29702;&#35299;&#26032;&#27010;&#24565;&#12290;&#20154;&#31867;&#33021;&#22815;&#20165;&#20165;&#20351;&#29992;&#20960;&#20010;&#20363;&#23376;&#23398;&#20064;&#26032;&#27010;&#24565;&#65292;&#36890;&#36807;&#20174;&#35760;&#24518;&#20013;&#25552;&#21462;&#37325;&#35201;&#20449;&#24687;&#24182;&#20002;&#24323;&#20854;&#20313;&#20449;&#24687;&#12290;&#21516;&#26679;&#65292;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#22312;&#20445;&#30041;&#24050;&#30693;&#31867;&#21035;&#30693;&#35782;&#30340;&#21516;&#26102;&#23398;&#20064;&#26032;&#31867;&#21035;&#65292;&#32780;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#33021;&#22815;&#20351;&#29992;&#38750;&#24120;&#23569;&#30340;&#35757;&#32451;&#26679;&#26412;&#23398;&#20064;&#26032;&#31867;&#21035;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#21463;&#21040;&#20154;&#31867;&#35748;&#30693;&#21644;&#23398;&#20064;&#30340;&#21551;&#21457;&#65292;&#21487;&#20197;&#20351;&#29992;&#23569;&#37327;&#30340;&#35757;&#32451;&#26679;&#26412;&#20934;&#30830;&#22320;&#35782;&#21035;&#26032;&#30340;&#22797;&#21512;&#34920;&#24773;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Complex emotion recognition is a cognitive task that has so far eluded the same excellent performance of other tasks that are at or above the level of human cognition. Emotion recognition through facial expressions is particularly difficult due to the complexity of emotions expressed by the human face. For a machine to approach the same level of performance in this domain as a human, it may need to synthesise knowledge and understand new concepts in real-time as humans do. Humans are able to learn new concepts using only few examples, by distilling the important information from memories and discarding the rest. Similarly, continual learning methods learn new classes whilst retaining the knowledge of known classes, whilst few-shot learning methods are able to learn new classes using very few training examples. We propose a novel continual learning method inspired by human cognition and learning that can accurately recognise new compound expression classes using few training samples, by
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26088;&#22312;&#20943;&#36731;&#36719;&#20214;&#21487;&#33021;&#24102;&#26469;&#30340;&#31038;&#20250;&#39118;&#38505;&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#32467;&#21512;&#20102;&#36719;&#20214;&#28389;&#29992;&#20998;&#26512;&#30340;&#24418;&#24335;&#22522;&#30784;&#19982;&#27010;&#29575;&#20266;&#36896;&#25216;&#26415;&#65292;&#29992;&#20110;&#35782;&#21035;&#36719;&#20214;&#19981;&#33391;&#25928;&#26524;&#12290;&#20316;&#32773;&#24212;&#29992;&#35813;&#25216;&#26415;&#26469;&#35780;&#20272;&#26612;&#27833;&#27773;&#36710;&#25490;&#25918;&#28165;&#27905;&#31995;&#32479;&#21644;&#39640;&#39118;&#38505;&#35780;&#20272;&#20154;&#31867;&#30340;&#20915;&#31574;&#31995;&#32479;&#65292;&#24182;&#25552;&#20986;&#20102;&#25913;&#36827;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2308.06186</link><description>&lt;p&gt;
&#36719;&#20214;&#28389;&#29992;&#20998;&#26512;&#19982;&#20154;&#31867;&#30417;&#30563;
&lt;/p&gt;
&lt;p&gt;
Software Doping Analysis for Human Oversight. (arXiv:2308.06186v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06186
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26088;&#22312;&#20943;&#36731;&#36719;&#20214;&#21487;&#33021;&#24102;&#26469;&#30340;&#31038;&#20250;&#39118;&#38505;&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#32467;&#21512;&#20102;&#36719;&#20214;&#28389;&#29992;&#20998;&#26512;&#30340;&#24418;&#24335;&#22522;&#30784;&#19982;&#27010;&#29575;&#20266;&#36896;&#25216;&#26415;&#65292;&#29992;&#20110;&#35782;&#21035;&#36719;&#20214;&#19981;&#33391;&#25928;&#26524;&#12290;&#20316;&#32773;&#24212;&#29992;&#35813;&#25216;&#26415;&#26469;&#35780;&#20272;&#26612;&#27833;&#27773;&#36710;&#25490;&#25918;&#28165;&#27905;&#31995;&#32479;&#21644;&#39640;&#39118;&#38505;&#35780;&#20272;&#20154;&#31867;&#30340;&#20915;&#31574;&#31995;&#32479;&#65292;&#24182;&#25552;&#20986;&#20102;&#25913;&#36827;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26088;&#22312;&#20943;&#36731;&#36719;&#20214;&#21487;&#33021;&#24102;&#26469;&#30340;&#31038;&#20250;&#39118;&#38505;&#30340;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36825;&#21253;&#25324;&#20102;&#36719;&#20214;&#28389;&#29992;&#12289;&#19981;&#20844;&#24179;&#21644;&#39640;&#39118;&#38505;&#20915;&#31574;&#31995;&#32479;&#20013;&#30340;&#27495;&#35270;&#12290;&#36719;&#20214;&#28389;&#29992;&#26159;&#25351;&#21253;&#21547;&#20102;&#38024;&#23545;&#29992;&#25143;&#21033;&#30410;&#30340;&#31192;&#23494;&#28155;&#21152;&#21151;&#33021;&#30340;&#36719;&#20214;&#12290;&#19968;&#20010;&#33879;&#21517;&#30340;&#36719;&#20214;&#28389;&#29992;&#31034;&#20363;&#23601;&#26159;&#26612;&#27833;&#25490;&#25918;&#19985;&#38395;&#26333;&#20809;&#26102;&#22312;&#20840;&#29699;&#25968;&#30334;&#19975;&#36742;&#27773;&#36710;&#20013;&#21457;&#29616;&#30340;&#25805;&#32437;&#25490;&#25918;&#28165;&#27905;&#31995;&#32479;&#12290;&#26412;&#25991;&#30340;&#31532;&#19968;&#37096;&#20998;&#23558;&#36719;&#20214;&#28389;&#29992;&#20998;&#26512;&#30340;&#24418;&#24335;&#22522;&#30784;&#19982;&#24050;&#24314;&#31435;&#30340;&#27010;&#29575;&#20266;&#36896;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#24471;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35782;&#21035;&#36719;&#20214;&#19981;&#33391;&#25928;&#26524;&#30340;&#40657;&#31665;&#20998;&#26512;&#25216;&#26415;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#25216;&#26415;&#24212;&#29992;&#20110;&#26612;&#27833;&#27773;&#36710;&#25490;&#25918;&#28165;&#27905;&#31995;&#32479;&#65292;&#20063;&#24212;&#29992;&#20110;&#20197;&#21487;&#33021;&#19981;&#20844;&#24179;&#25110;&#27495;&#35270;&#30340;&#26041;&#24335;&#35780;&#20272;&#20154;&#31867;&#30340;&#39640;&#39118;&#38505;&#31995;&#32479;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22914;&#20309;&#21487;&#20197;&#23545;&#20915;&#31574;&#32467;&#26524;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#20026;&#25913;&#36827;&#31995;&#32479;&#25552;&#20986;&#20102;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article introduces a framework that is meant to assist in mitigating societal risks that software can pose. Concretely, this encompasses facets of software doping as well as unfairness and discrimination in high-risk decision-making systems. The term software doping refers to software that contains surreptitiously added functionality that is against the interest of the user. A prominent example of software doping are the tampered emission cleaning systems that were found in millions of cars around the world when the diesel emissions scandal surfaced. The first part of this article combines the formal foundations of software doping analysis with established probabilistic falsification techniques to arrive at a black-box analysis technique for identifying undesired effects of software. We apply this technique to emission cleaning systems in diesel cars but also to high-risk systems that evaluate humans in a possibly unfair or discriminating way. We demonstrate how our approach can a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#24403;&#21069;&#30340;&#29289;&#29702;&#23545;&#25239;&#25915;&#20987;&#36235;&#21183;&#65292;&#20998;&#26512;&#20102;&#29289;&#29702;&#23545;&#25239;&#25915;&#20987;&#30340;&#29305;&#28857;&#21644;&#25361;&#25112;&#12290;&#30740;&#31350;&#20102;&#19981;&#21516;&#24212;&#29992;&#20013;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#24182;&#35780;&#20272;&#20102;&#23427;&#20204;&#30340;&#25928;&#26524;&#21644;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;&#35752;&#35770;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2308.06173</link><description>&lt;p&gt;
&#22522;&#20110;&#25668;&#20687;&#22836;&#30340;&#26234;&#33021;&#31995;&#32479;&#30340;&#29289;&#29702;&#23545;&#25239;&#25915;&#20987;&#65306;&#24403;&#21069;&#36235;&#21183;&#65292;&#20998;&#31867;&#65292;&#24212;&#29992;&#65292;&#30740;&#31350;&#25361;&#25112;&#21644;&#26410;&#26469;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Physical Adversarial Attacks For Camera-based Smart Systems: Current Trends, Categorization, Applications, Research Challenges, and Future Outlook. (arXiv:2308.06173v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06173
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#24403;&#21069;&#30340;&#29289;&#29702;&#23545;&#25239;&#25915;&#20987;&#36235;&#21183;&#65292;&#20998;&#26512;&#20102;&#29289;&#29702;&#23545;&#25239;&#25915;&#20987;&#30340;&#29305;&#28857;&#21644;&#25361;&#25112;&#12290;&#30740;&#31350;&#20102;&#19981;&#21516;&#24212;&#29992;&#20013;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#24182;&#35780;&#20272;&#20102;&#23427;&#20204;&#30340;&#25928;&#26524;&#21644;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;&#35752;&#35770;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#29289;&#29702;&#23545;&#25239;&#25915;&#20987;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#35843;&#26597;&#30740;&#31350;&#65292;&#37325;&#28857;&#20851;&#27880;&#24403;&#21069;&#30340;&#36235;&#21183;&#12290;&#25105;&#20204;&#26088;&#22312;&#25552;&#20379;&#23545;&#29289;&#29702;&#23545;&#25239;&#25915;&#20987;&#27010;&#24565;&#30340;&#20840;&#38754;&#29702;&#35299;&#65292;&#20998;&#26512;&#20854;&#20851;&#38190;&#29305;&#24449;&#21644;&#21306;&#21035;&#24615;&#29305;&#28857;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#29289;&#29702;&#19990;&#30028;&#20013;&#25191;&#34892;&#25915;&#20987;&#25152;&#28041;&#21450;&#30340;&#20855;&#20307;&#38656;&#27714;&#21644;&#25361;&#25112;&#12290;&#26412;&#25991;&#28145;&#20837;&#30740;&#31350;&#20102;&#21508;&#31181;&#29289;&#29702;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65292;&#26681;&#25454;&#23427;&#20204;&#22312;&#19981;&#21516;&#24212;&#29992;&#20013;&#30340;&#30446;&#26631;&#20219;&#21153;&#36827;&#34892;&#20998;&#31867;&#65292;&#21253;&#25324;&#20998;&#31867;&#65292;&#26816;&#27979;&#65292;&#20154;&#33080;&#35782;&#21035;&#65292;&#35821;&#20041;&#20998;&#21106;&#21644;&#28145;&#24230;&#20272;&#35745;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#36825;&#20123;&#25915;&#20987;&#26041;&#27861;&#22312;&#25928;&#26524;&#12289;&#38544;&#34109;&#24615;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#27599;&#31181;&#25216;&#26415;&#22914;&#20309;&#21162;&#21147;&#30830;&#20445;&#25104;&#21151;&#25805;&#20316;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#21516;&#26102;&#20943;&#23567;&#34987;&#26816;&#27979;&#30340;&#39118;&#38505;&#21644;&#25215;&#21463;&#30495;&#23454;&#19990;&#30028;&#30340;&#24178;&#25200;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#24403;&#21069;&#30340;&#25361;&#25112;&#65292;&#24182;&#21246;&#30011;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a comprehensive survey of the current trends focusing specifically on physical adversarial attacks. We aim to provide a thorough understanding of the concept of physical adversarial attacks, analyzing their key characteristics and distinguishing features. Furthermore, we explore the specific requirements and challenges associated with executing attacks in the physical world. Our article delves into various physical adversarial attack methods, categorized according to their target tasks in different applications, including classification, detection, face recognition, semantic segmentation and depth estimation. We assess the performance of these attack methods in terms of their effectiveness, stealthiness, and robustness. We examine how each technique strives to ensure the successful manipulation of DNNs while mitigating the risk of detection and withstanding real-world distortions. Lastly, we discuss the current challenges and outline potential future research 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38454;&#27573;&#24615;&#28145;&#24230;&#26102;&#31354;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#22478;&#38469;&#39640;&#36895;&#20844;&#36335;&#30340;&#27599;&#26085;&#20132;&#36890;&#37327;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#31934;&#24515;&#30340;&#25968;&#25454;&#35268;&#33539;&#21270;&#12289;&#28151;&#21512;&#27169;&#22411;&#32467;&#21512;&#20197;&#21450;&#32771;&#34385;&#26469;&#33258;&#24322;&#26500;&#25968;&#25454;&#30340;&#22810;&#20010;&#35201;&#32032;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#20132;&#36890;&#37327;&#39044;&#27979;&#20013;&#30340;&#26102;&#31354;&#29305;&#24449;&#21644;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.06155</link><description>&lt;p&gt;
&#22522;&#20110;&#38454;&#27573;&#24615;&#28145;&#24230;&#26102;&#31354;&#23398;&#20064;&#30340;&#39640;&#36895;&#20844;&#36335;&#20132;&#36890;&#37327;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Phased Deep Spatio-temporal Learning for Highway Traffic Volume Prediction. (arXiv:2308.06155v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06155
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38454;&#27573;&#24615;&#28145;&#24230;&#26102;&#31354;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#22478;&#38469;&#39640;&#36895;&#20844;&#36335;&#30340;&#27599;&#26085;&#20132;&#36890;&#37327;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#31934;&#24515;&#30340;&#25968;&#25454;&#35268;&#33539;&#21270;&#12289;&#28151;&#21512;&#27169;&#22411;&#32467;&#21512;&#20197;&#21450;&#32771;&#34385;&#26469;&#33258;&#24322;&#26500;&#25968;&#25454;&#30340;&#22810;&#20010;&#35201;&#32032;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#20132;&#36890;&#37327;&#39044;&#27979;&#20013;&#30340;&#26102;&#31354;&#29305;&#24449;&#21644;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22478;&#38469;&#39640;&#36895;&#20844;&#36335;&#20132;&#36890;&#23545;&#20110;&#29616;&#20195;&#37117;&#24066;&#29983;&#27963;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#29983;&#25104;&#24102;&#26377;&#26102;&#31354;&#29305;&#24449;&#30340;&#24322;&#26500;&#24863;&#30693;&#25968;&#25454;&#12290;&#20316;&#20026;&#20132;&#36890;&#39046;&#22495;&#30340;&#20363;&#34892;&#20998;&#26512;&#65292;&#27599;&#26085;&#20132;&#36890;&#37327;&#20272;&#35745;&#38754;&#20020;&#30528;&#25361;&#25112;&#65292;&#21253;&#25324;&#32570;&#20047;&#23545;&#30456;&#20851;&#26102;&#31354;&#29305;&#24449;&#30340;&#38271;&#26399;&#32771;&#23519;&#21644;&#22788;&#29702;&#25968;&#25454;&#19981;&#24179;&#34913;&#30340;&#26377;&#25928;&#25163;&#27573;&#65292;&#21518;&#32773;&#20250;&#24694;&#21270;&#39044;&#27979;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38454;&#27573;&#24615;&#28145;&#24230;&#26102;&#31354;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#27599;&#26085;&#20132;&#36890;&#37327;&#12290;&#22312;&#29305;&#24449;&#39044;&#22788;&#29702;&#38454;&#27573;&#65292;&#26681;&#25454;&#28508;&#22312;&#30340;&#38271;&#23614;&#20998;&#24067;&#31934;&#24515;&#36827;&#34892;&#25968;&#25454;&#35268;&#33539;&#21270;&#12290;&#22312;&#26102;&#31354;&#23398;&#20064;&#38454;&#27573;&#65292;&#37319;&#29992;&#20102;&#19968;&#31181;&#28151;&#21512;&#27169;&#22411;&#65292;&#23558;&#20840;&#21367;&#31215;&#32593;&#32476;&#65288;FCN&#65289;&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#65288;LSTM&#65289;&#32467;&#21512;&#36215;&#26469;&#65292;&#32771;&#34385;&#20102;&#26469;&#33258;&#24322;&#26500;&#25968;&#25454;&#30340;&#26102;&#38388;&#12289;&#31354;&#38388;&#12289;&#27668;&#35937;&#21644;&#26085;&#21382;&#20449;&#24687;&#12290;&#22312;&#20915;&#31574;&#38454;&#27573;&#65292;&#39044;&#27979;&#20102;&#32593;&#32476;&#33539;&#22260;&#25910;&#36153;&#21345;&#21475;&#26410;&#26469;&#19968;&#22825;&#30340;&#20132;&#36890;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inter-city highway transportation is significant for citizens' modern urban life and generates heterogeneous sensory data with spatio-temporal characteristics. As a routine analysis in transportation domain, daily traffic volume estimation faces challenges for highway toll stations including lacking of exploration of correlative spatio-temporal features from a long-term perspective and effective means to deal with data imbalance which always deteriorates the predictive performance. In this paper, a deep spatio-temporal learning method is proposed to predict daily traffic volume in three phases. In feature pre-processing phase, data is normalized elaborately according to latent long-tail distribution. In spatio-temporal learning phase, a hybrid model is employed combining fully convolution network (FCN) and long short-term memory (LSTM), which considers time, space, meteorology, and calendar from heterogeneous data. In decision phase, traffic volumes on a coming day at network-wide toll
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#21033;&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#25104;&#21151;&#39044;&#27979;&#20102;&#38156;&#29983;&#20135;&#21387;&#21147;&#36807;&#28388;&#36807;&#31243;&#20013;&#30340;&#28388;&#39292;&#21547;&#27700;&#29575;&#65292;&#20026;&#38156;&#29983;&#20135;&#24037;&#33402;&#25552;&#20379;&#20102;&#21487;&#38752;&#30340;&#39044;&#27979;&#25163;&#27573;&#12290;</title><link>http://arxiv.org/abs/2308.06138</link><description>&lt;p&gt;
&#24212;&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30740;&#31350;&#21387;&#21147;&#36807;&#28388;&#24615;&#33021;&#30340;&#25506;&#32034;&#21644;&#38156;&#28024;&#20986;&#28388;&#39292;&#21547;&#27700;&#29575;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Application of Artificial Neural Networks for Investigation of Pressure Filtration Performance, a Zinc Leaching Filter Cake Moisture Modeling. (arXiv:2308.06138v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06138
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#21033;&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#25104;&#21151;&#39044;&#27979;&#20102;&#38156;&#29983;&#20135;&#21387;&#21147;&#36807;&#28388;&#36807;&#31243;&#20013;&#30340;&#28388;&#39292;&#21547;&#27700;&#29575;&#65292;&#20026;&#38156;&#29983;&#20135;&#24037;&#33402;&#25552;&#20379;&#20102;&#21487;&#38752;&#30340;&#39044;&#27979;&#25163;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#26159;&#26448;&#26009;&#31185;&#23398;&#24212;&#29992;&#20013;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#26159;&#19968;&#31181;&#33021;&#22815;&#25552;&#20379;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#38156;&#29983;&#20135;&#30340;&#21387;&#21147;&#36807;&#28388;&#36807;&#31243;&#20013;&#30340;&#28388;&#39292;&#21547;&#27700;&#29575;&#12290;&#28388;&#39292;&#21547;&#27700;&#29575;&#21463;&#21040;&#19971;&#20010;&#21442;&#25968;&#30340;&#24433;&#21709;&#65306;&#28201;&#24230;&#65288;35&#25668;&#27663;&#24230;&#21644;65&#25668;&#27663;&#24230;&#65289;&#65292;&#22266;&#20307;&#27987;&#24230;&#65288;0.2&#20811;/&#21319;&#21644;0.38&#20811;/&#21319;&#65289;&#65292;pH&#20540;&#65288;2&#12289;3.5&#21644;5&#65289;&#65292;&#21561;&#27668;&#26102;&#38388;&#65288;2&#20998;&#38047;&#12289;10&#20998;&#38047;&#21644;15&#20998;&#38047;&#65289;&#65292;&#28388;&#39292;&#21402;&#24230;&#65288;14&#27627;&#31859;&#12289;20&#27627;&#31859;&#12289;26&#27627;&#31859;&#21644;34&#27627;&#31859;&#65289;&#65292;&#21387;&#21147;&#21644;&#36807;&#28388;&#26102;&#38388;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#20004;&#31181;&#31867;&#22411;&#30340;&#32455;&#29289;&#36827;&#34892;&#20102;288&#27425;&#27979;&#35797;&#65306;&#32858;&#19993;&#28911;&#65288;S1&#65289;&#21644;&#28068;&#32438;&#65288;S2&#65289;&#12290;&#36890;&#36807;&#20915;&#23450;&#31995;&#25968;&#65288;R2&#65289;&#12289;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#21644;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#65288;MAE&#65289;&#25351;&#26631;&#35780;&#20272;&#20102;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#23545;&#20110;S1&#21644;S2&#65292;R2&#20540;&#20998;&#21035;&#20026;0.88&#21644;0.83&#65292;MSE&#20540;&#20998;&#21035;&#20026;6.243x10-07&#21644;1.086x10-06&#65292;MAE&#20540;&#20998;&#21035;&#20026;0.00056&#21644;0.00088&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Learning (ML) is a powerful tool for material science applications. Artificial Neural Network (ANN) is a machine learning technique that can provide high prediction accuracy. This study aimed to develop an ANN model to predict the cake moisture of the pressure filtration process of zinc production. The cake moisture was influenced by seven parameters: temperature (35 and 65 Celsius), solid concentration (0.2 and 0.38 g/L), pH (2, 3.5, and 5), air-blow time (2, 10, and 15 min), cake thickness (14, 20, 26, and 34 mm), pressure, and filtration time. The study conducted 288 tests using two types of fabrics: polypropylene (S1) and polyester (S2). The ANN model was evaluated by the Coefficient of determination (R2), the Mean Square Error (MSE), and the Mean Absolute Error (MAE) metrics for both datasets. The results showed R2 values of 0.88 and 0.83, MSE values of 6.243x10-07 and 1.086x10-06, and MAE values of 0.00056 and 0.00088 for S1 and S2, respectively. These results indicated t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#21338;&#24328;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#32852;&#21512;&#35268;&#21010;&#21644;&#39044;&#27979;&#65292;&#20197;&#25913;&#36827;&#26426;&#22120;&#20154;&#21160;&#20316;&#30340;&#23433;&#20840;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;&#28216;&#25103;&#29702;&#35770;&#30340;&#26041;&#27861;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#23398;&#20064;&#39044;&#27979;&#20154;&#31867;&#38450;&#33539;&#23545;&#31574;&#65292;&#24182;&#36890;&#36807;&#23454;&#38469;&#31639;&#27861;&#23454;&#29616;&#20102;&#31471;&#21040;&#31471;&#30340;&#27169;&#22411;&#35757;&#32451;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#22312;&#20154;&#32676;&#23548;&#33322;&#21644;&#29616;&#23454;&#19990;&#30028;&#30340;&#34892;&#20154;&#36816;&#21160;&#25968;&#25454;&#38598;&#20013;&#21487;&#20197;&#20135;&#29983;&#26356;&#23433;&#20840;&#30340;&#35268;&#21010;&#12290;</title><link>http://arxiv.org/abs/2308.06137</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#32852;&#21512;&#39044;&#27979;&#21644;&#35268;&#21010;&#30340;&#21338;&#24328;&#35770;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Game-Theoretic Framework for Joint Forecasting and Planning. (arXiv:2308.06137v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06137
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#21338;&#24328;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#32852;&#21512;&#35268;&#21010;&#21644;&#39044;&#27979;&#65292;&#20197;&#25913;&#36827;&#26426;&#22120;&#20154;&#21160;&#20316;&#30340;&#23433;&#20840;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;&#28216;&#25103;&#29702;&#35770;&#30340;&#26041;&#27861;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#23398;&#20064;&#39044;&#27979;&#20154;&#31867;&#38450;&#33539;&#23545;&#31574;&#65292;&#24182;&#36890;&#36807;&#23454;&#38469;&#31639;&#27861;&#23454;&#29616;&#20102;&#31471;&#21040;&#31471;&#30340;&#27169;&#22411;&#35757;&#32451;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#22312;&#20154;&#32676;&#23548;&#33322;&#21644;&#29616;&#23454;&#19990;&#30028;&#30340;&#34892;&#20154;&#36816;&#21160;&#25968;&#25454;&#38598;&#20013;&#21487;&#20197;&#20135;&#29983;&#26356;&#23433;&#20840;&#30340;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23384;&#22312;&#20154;&#30340;&#24773;&#20917;&#19979;&#65292;&#23433;&#20840;&#35268;&#21010;&#26426;&#22120;&#20154;&#30340;&#36816;&#21160;&#38656;&#35201;&#21487;&#38752;&#30340;&#23545;&#26410;&#26469;&#20154;&#31867;&#36816;&#21160;&#30340;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#31616;&#21333;&#22320;&#39044;&#27979;&#21069;&#26399;&#20132;&#20114;&#20013;&#26368;&#21487;&#33021;&#30340;&#21160;&#20316;&#24182;&#19981;&#33021;&#20445;&#35777;&#23433;&#20840;&#12290;&#36825;&#26679;&#30340;&#39044;&#27979;&#26410;&#33021;&#27169;&#25311;&#20986;&#21487;&#33021;&#20107;&#20214;&#30340;&#38271;&#23614;&#37096;&#20998;&#65292;&#36825;&#20123;&#20107;&#20214;&#22312;&#26377;&#38480;&#30340;&#25968;&#25454;&#38598;&#20013;&#24456;&#23569;&#34987;&#35266;&#23519;&#21040;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20026;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#21160;&#20316;&#36827;&#34892;&#35268;&#21010;&#20250;&#23548;&#33268;&#36807;&#20998;&#20445;&#23432;&#30340;&#34892;&#20026;&#21644;"&#20725;&#21270;&#30340;&#26426;&#22120;&#20154;"&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#23398;&#20064;&#39044;&#27979;&#20154;&#31867;&#38450;&#33539;&#23545;&#31574;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#28216;&#25103;&#29702;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#32852;&#21512;&#35268;&#21010;&#21644;&#39044;&#27979;&#65292;&#20197;&#35268;&#21010;&#22120;&#19982;&#28436;&#31034;&#32773;&#20043;&#38388;&#30340;&#32489;&#25928;&#20316;&#20026;&#25910;&#30410;&#65292;&#24182;&#25552;&#20986;&#20102;&#23454;&#38469;&#31639;&#27861;&#65292;&#20197;&#31471;&#23545;&#31471;&#30340;&#26041;&#24335;&#35757;&#32451;&#27169;&#22411;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#20154;&#32676;&#23548;&#33322;&#27169;&#25311;&#22120;&#21644;&#29616;&#23454;&#19990;&#30028;&#30340;&#34892;&#20154;&#36816;&#21160;&#25968;&#25454;&#38598;&#20013;&#20135;&#29983;&#20102;&#26356;&#23433;&#20840;&#30340;&#35745;&#21010;&#12290;&#25105;&#20204;&#23558;&#20195;&#30721;&#21457;&#24067;&#22312;https://github.com/portal-cornell/Game-Theoretic-Forecasting-Planning&#12290;
&lt;/p&gt;
&lt;p&gt;
Planning safe robot motions in the presence of humans requires reliable forecasts of future human motion. However, simply predicting the most likely motion from prior interactions does not guarantee safety. Such forecasts fail to model the long tail of possible events, which are rarely observed in limited datasets. On the other hand, planning for worst-case motions leads to overtly conservative behavior and a ``frozen robot''. Instead, we aim to learn forecasts that predict counterfactuals that humans guard against. We propose a novel game-theoretic framework for joint planning and forecasting with the payoff being the performance of the planner against the demonstrator, and present practical algorithms to train models in an end-to-end fashion. We demonstrate that our proposed algorithm results in safer plans in a crowd navigation simulator and real-world datasets of pedestrian motion. We release our code at https://github.com/portal-cornell/Game-Theoretic-Forecasting-Planning.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#32852;&#21512;&#35821;&#38899;-&#25991;&#26412;&#34920;&#31034;&#20013;&#65292;&#24573;&#30053;&#24207;&#21015;&#38271;&#24230;&#38382;&#39064;&#33021;&#22815;&#33258;&#28982;&#22320;&#23454;&#29616;&#19968;&#33268;&#30340;&#34920;&#31034;&#65292;&#19968;&#33268;&#24615;&#25439;&#22833;&#21487;&#20197;&#25552;&#39640;&#19979;&#28216;&#30340;&#23383;&#38169;&#35823;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.06125</link><description>&lt;p&gt;
&#22312;&#19981;&#36827;&#34892;&#23545;&#40784;&#30340;&#24773;&#20917;&#19979;&#25913;&#36827;&#32852;&#21512;&#35821;&#38899;-&#25991;&#26412;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Improving Joint Speech-Text Representations Without Alignment. (arXiv:2308.06125v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06125
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#32852;&#21512;&#35821;&#38899;-&#25991;&#26412;&#34920;&#31034;&#20013;&#65292;&#24573;&#30053;&#24207;&#21015;&#38271;&#24230;&#38382;&#39064;&#33021;&#22815;&#33258;&#28982;&#22320;&#23454;&#29616;&#19968;&#33268;&#30340;&#34920;&#31034;&#65292;&#19968;&#33268;&#24615;&#25439;&#22833;&#21487;&#20197;&#25552;&#39640;&#19979;&#28216;&#30340;&#23383;&#38169;&#35823;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#19968;&#24180;&#65292;&#22522;&#20110;&#36328;&#27169;&#24577;&#34920;&#31034;&#31354;&#38388;&#30340;&#25991;&#26412;&#24341;&#23548;&#22270;&#20687;&#29983;&#25104;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#36827;&#23637;&#65292;&#20854;&#20013;&#25991;&#26412;&#21644;&#22270;&#20687;&#39046;&#22495;&#20197;&#32852;&#21512;&#30340;&#26041;&#24335;&#34920;&#31034;&#12290;&#22312;ASR&#20013;&#65292;&#36825;&#20010;&#24819;&#27861;&#34987;&#24212;&#29992;&#20026;&#32852;&#21512;&#35821;&#38899;-&#25991;&#26412;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#21516;&#26102;&#35757;&#32451;&#19981;&#21305;&#37197;&#30340;&#35821;&#38899;&#21644;&#25991;&#26412;&#65292;&#21487;&#20197;&#25193;&#23637;&#21040;&#38750;&#24120;&#22823;&#30340;&#21442;&#25968;&#27169;&#22411;&#30340;&#23481;&#37327;&#12290;&#34429;&#28982;&#36825;&#20123;&#26041;&#27861;&#34920;&#29616;&#20986;&#20102;&#24076;&#26395;&#65292;&#20294;&#23427;&#20204;&#38656;&#35201;&#29305;&#27530;&#22788;&#29702;&#35821;&#38899;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#24207;&#21015;&#38271;&#24230;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#35201;&#20040;&#36890;&#36807;&#19978;&#37319;&#26679;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#35201;&#20040;&#36890;&#36807;&#19968;&#20010;&#26174;&#24335;&#30340;&#23545;&#40784;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#35777;&#25454;&#34920;&#26126;&#65292;&#32852;&#21512;&#35821;&#38899;-&#25991;&#26412;&#32534;&#30721;&#22120;&#36890;&#36807;&#24573;&#30053;&#24207;&#21015;&#38271;&#24230;&#33258;&#28982;&#32780;&#28982;&#22320;&#23454;&#29616;&#20102;&#19968;&#33268;&#30340;&#34920;&#31034;&#65292;&#24182;&#19988;&#35748;&#20026;&#19968;&#33268;&#24615;&#25439;&#22833;&#21487;&#20197;&#24357;&#34917;&#38271;&#24230;&#24046;&#24322;&#65292;&#24182;&#31616;&#21333;&#22320;&#20551;&#35774;&#26368;&#20339;&#23545;&#40784;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#26679;&#30340;&#25439;&#22833;&#22312;&#22823;&#21442;&#25968;&#21333;&#35821;&#35328;&#21644;&#22810;&#35821;&#35328;&#31995;&#32479;&#20013;&#25552;&#39640;&#20102;&#19979;&#28216;&#30340;&#23383;&#38169;&#35823;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The last year has seen astonishing progress in text-prompted image generation premised on the idea of a cross-modal representation space in which the text and image domains are represented jointly. In ASR, this idea has found application as joint speech-text encoders that can scale to the capacities of very large parameter models by being trained on both unpaired speech and text. While these methods show promise, they have required special treatment of the sequence-length mismatch inherent in speech and text, either by up-sampling heuristics or an explicit alignment model. In this work, we offer evidence that joint speech-text encoders naturally achieve consistent representations across modalities by disregarding sequence length, and argue that consistency losses could forgive length differences and simply assume the best alignment. We show that such a loss improves downstream WER in both a large-parameter monolingual and multilingual system.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;ZeroShotALI&#65292;&#23427;&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25512;&#33616;&#31995;&#32479;&#26469;&#25913;&#36827;&#37329;&#34701;&#23457;&#35745;&#20013;&#30340;&#38646;&#26679;&#26412;&#25991;&#26412;&#21305;&#37197;&#12290;&#36890;&#36807;&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#32463;&#36807;&#39046;&#22495;&#20248;&#21270;&#30340;&#22522;&#20110;transformer&#30340;&#25991;&#26412;&#21305;&#37197;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#31995;&#32479;&#23454;&#29616;&#20102;&#20174;&#25253;&#21578;&#20013;&#25512;&#33616;&#19982;&#27861;&#24459;&#35201;&#27714;&#30456;&#31526;&#30340;&#30456;&#20851;&#25991;&#26412;&#27573;&#33853;&#65292;&#24182;&#22312;&#29616;&#26377;&#26041;&#27861;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2308.06111</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#21319;&#37329;&#34701;&#23457;&#35745;&#30340;&#38646;&#26679;&#26412;&#25991;&#26412;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Improving Zero-Shot Text Matching for Financial Auditing with Large Language Models. (arXiv:2308.06111v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06111
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;ZeroShotALI&#65292;&#23427;&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25512;&#33616;&#31995;&#32479;&#26469;&#25913;&#36827;&#37329;&#34701;&#23457;&#35745;&#20013;&#30340;&#38646;&#26679;&#26412;&#25991;&#26412;&#21305;&#37197;&#12290;&#36890;&#36807;&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#32463;&#36807;&#39046;&#22495;&#20248;&#21270;&#30340;&#22522;&#20110;transformer&#30340;&#25991;&#26412;&#21305;&#37197;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#31995;&#32479;&#23454;&#29616;&#20102;&#20174;&#25253;&#21578;&#20013;&#25512;&#33616;&#19982;&#27861;&#24459;&#35201;&#27714;&#30456;&#31526;&#30340;&#30456;&#20851;&#25991;&#26412;&#27573;&#33853;&#65292;&#24182;&#22312;&#29616;&#26377;&#26041;&#27861;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23457;&#35745;&#37329;&#34701;&#25991;&#20214;&#26159;&#19968;&#20010;&#38750;&#24120;&#32321;&#29712;&#21644;&#32791;&#26102;&#30340;&#36807;&#31243;&#12290;&#30446;&#21069;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#31616;&#21270;&#36825;&#19968;&#36807;&#31243;&#65292;&#20197;&#25512;&#33616;&#19982;&#20005;&#26684;&#20250;&#35745;&#26631;&#20934;&#30340;&#27861;&#24459;&#35201;&#27714;&#30456;&#31526;&#30340;&#25253;&#21578;&#20013;&#30340;&#30456;&#20851;&#25991;&#26412;&#27573;&#33853;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#23450;&#26399;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#19988;&#36890;&#24120;&#22312;&#24037;&#19994;&#29615;&#22659;&#20013;&#32570;&#20047;&#22823;&#37327;&#30340;&#27880;&#37322;&#25968;&#25454;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ZeroShotALI&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#21033;&#29992;&#20102;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#39046;&#22495;&#29305;&#23450;&#30340;&#20248;&#21270;&#30340;&#22522;&#20110;transformer&#30340;&#25991;&#26412;&#21305;&#37197;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#39318;&#20808;&#20351;&#29992;&#33258;&#23450;&#20041;BERT&#27169;&#22411;&#26816;&#32034;&#19982;&#27861;&#24459;&#35201;&#27714;&#30456;&#31526;&#30340;&#33509;&#24178;&#26368;&#20339;&#21305;&#37197;&#30340;&#25991;&#26723;&#37096;&#20998;&#65292;&#28982;&#21518;&#20351;&#29992;LLM&#23545;&#36825;&#20123;&#36873;&#25321;&#36827;&#34892;&#36807;&#28388;&#65292;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#29616;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Auditing financial documents is a very tedious and time-consuming process. As of today, it can already be simplified by employing AI-based solutions to recommend relevant text passages from a report for each legal requirement of rigorous accounting standards. However, these methods need to be fine-tuned regularly, and they require abundant annotated data, which is often lacking in industrial environments. Hence, we present ZeroShotALI, a novel recommender system that leverages a state-of-the-art large language model (LLM) in conjunction with a domain-specifically optimized transformer-based text-matching solution. We find that a two-step approach of first retrieving a number of best matching document sections per legal requirement with a custom BERT-based model and second filtering these selections using an LLM yields significant performance improvements over existing approaches.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#32508;&#36848;&#30740;&#31350;&#20102;&#20197;&#24378;&#22823;&#35821;&#35328;&#27169;&#22411;&#20026;&#22522;&#30784;&#30340;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#31995;&#32479;&#65292;&#24182;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#24178;&#39044;&#24213;&#23618;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#21516;&#26041;&#38754;&#65292;&#22914;&#25968;&#25454;&#12289;&#35757;&#32451;&#21046;&#24230;&#25110;&#35299;&#30721;&#65292;&#26469;&#20445;&#35777;&#27169;&#22411;&#30340;&#27969;&#30021;&#24615;&#12289;&#20449;&#24687;&#20016;&#23500;&#24615;&#12289;&#19968;&#33268;&#24615;&#12289;&#36830;&#36143;&#24615;&#20197;&#21450;&#36981;&#24490;&#31038;&#20250;&#20934;&#21017;&#30340;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.06095</link><description>&lt;p&gt;
&#31070;&#32463;&#23545;&#35805;&#27169;&#22411;&#21450;&#20854;&#25511;&#21046;&#26041;&#27861;&#65306;&#22833;&#36133;&#21644;&#20462;&#22797;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Neural Conversation Models and How to Rein Them in: A Survey of Failures and Fixes. (arXiv:2308.06095v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06095
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#32508;&#36848;&#30740;&#31350;&#20102;&#20197;&#24378;&#22823;&#35821;&#35328;&#27169;&#22411;&#20026;&#22522;&#30784;&#30340;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#31995;&#32479;&#65292;&#24182;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#24178;&#39044;&#24213;&#23618;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#21516;&#26041;&#38754;&#65292;&#22914;&#25968;&#25454;&#12289;&#35757;&#32451;&#21046;&#24230;&#25110;&#35299;&#30721;&#65292;&#26469;&#20445;&#35777;&#27169;&#22411;&#30340;&#27969;&#30021;&#24615;&#12289;&#20449;&#24687;&#20016;&#23500;&#24615;&#12289;&#19968;&#33268;&#24615;&#12289;&#36830;&#36143;&#24615;&#20197;&#21450;&#36981;&#24490;&#31038;&#20250;&#20934;&#21017;&#30340;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20197;&#24378;&#22823;&#35821;&#35328;&#27169;&#22411;&#20026;&#22522;&#30784;&#30340;&#26465;&#20214;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20197;&#30475;&#20284;&#27969;&#21033;&#30340;&#26041;&#24335;&#24310;&#32493;&#20219;&#20309;&#31867;&#22411;&#30340;&#25991;&#26412;&#26469;&#28304;&#12290;&#36825;&#20010;&#20107;&#23454;&#20419;&#36827;&#20102;&#23545;&#22522;&#20110;&#24378;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#31995;&#32479;&#30340;&#30740;&#31350;&#65292;&#26088;&#22312;&#36890;&#36807;&#29983;&#25104;&#36866;&#24403;&#30340;&#23545;&#35805;&#20869;&#23481;&#26469;&#27169;&#20223;&#23545;&#35805;&#26041;&#30340;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#20174;&#35821;&#35328;&#23398;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#21442;&#19982;&#23545;&#35805;&#30340;&#22797;&#26434;&#24615;&#24456;&#39640;&#12290;&#22312;&#36825;&#39033;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#20174;&#36825;&#19968;&#29305;&#23450;&#30740;&#31350;&#39046;&#22495;&#30340;&#35282;&#24230;&#35299;&#37322;&#20102;Grice&#30340;&#21512;&#20316;&#24615;&#23545;&#35805;&#26368;&#22823;&#35268;&#21017;&#65292;&#24182;&#23558;&#25991;&#29486;&#31995;&#32479;&#21270;&#22320;&#24402;&#32435;&#20026;&#19968;&#20010;&#36129;&#29486;&#20309;&#31181;&#20869;&#23481;&#26159;&#36866;&#24403;&#30340;&#26041;&#38754;&#65306;&#31070;&#32463;&#23545;&#35805;&#27169;&#22411;&#24517;&#39035;&#27969;&#30021;&#12289;&#20449;&#24687;&#20016;&#23500;&#12289;&#19968;&#33268;&#12289;&#36830;&#36143;&#65292;&#24182;&#36981;&#24490;&#31038;&#20250;&#20934;&#21017;&#12290;&#20026;&#20102;&#30830;&#20445;&#36825;&#20123;&#29305;&#24615;&#65292;&#26368;&#36817;&#30340;&#26041;&#27861;&#23581;&#35797;&#22312;&#25968;&#25454;&#12289;&#35757;&#32451;&#21046;&#24230;&#25110;&#35299;&#30721;&#31561;&#21508;&#20010;&#24178;&#39044;&#28857;&#19978;&#25511;&#21046;&#24213;&#23618;&#35821;&#35328;&#27169;&#22411;&#12290;&#25353;&#29031;&#36825;&#20123;&#31867;&#21035;&#21644;&#24178;&#39044;&#28857;&#36827;&#34892;&#25490;&#24207;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#19968;&#20123;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent conditional language models are able to continue any kind of text source in an often seemingly fluent way. This fact encouraged research in the area of open-domain conversational systems that are based on powerful language models and aim to imitate an interlocutor by generating appropriate contributions to a written dialogue. From a linguistic perspective, however, the complexity of contributing to a conversation is high. In this survey, we interpret Grice's maxims of cooperative conversation from the perspective of this specific research area and systematize the literature under the aspect of what makes a contribution appropriate: A neural conversation model has to be fluent, informative, consistent, coherent, and follow social norms. In order to ensure these qualities, recent approaches try to tame the underlying language models at various intervention points, such as data, training regime or decoding. Sorted by these categories and intervention points, we discuss promising at
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#26102;&#38388;&#28857;&#36807;&#31243;&#30340;&#24378;&#21270;&#36923;&#36753;&#35268;&#21017;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#36880;&#27493;&#20248;&#21270;&#30340;&#26041;&#27861;&#25193;&#23637;&#35299;&#37322;&#24615;&#30340;&#35268;&#21017;&#38598;&#26469;&#35299;&#37322;&#26102;&#38388;&#20107;&#20214;&#30340;&#21457;&#29983;&#12290;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#25628;&#32034;&#31574;&#30053;&#21644;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#29983;&#25104;&#26032;&#30340;&#35268;&#21017;&#20869;&#23481;&#21644;&#26435;&#37325;&#12290;</title><link>http://arxiv.org/abs/2308.06094</link><description>&lt;p&gt;
&#24378;&#21270;&#36923;&#36753;&#35268;&#21017;&#23398;&#20064;&#29992;&#20110;&#26102;&#38388;&#28857;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Logic Rule Learning for Temporal Point Processes. (arXiv:2308.06094v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06094
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#26102;&#38388;&#28857;&#36807;&#31243;&#30340;&#24378;&#21270;&#36923;&#36753;&#35268;&#21017;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#36880;&#27493;&#20248;&#21270;&#30340;&#26041;&#27861;&#25193;&#23637;&#35299;&#37322;&#24615;&#30340;&#35268;&#21017;&#38598;&#26469;&#35299;&#37322;&#26102;&#38388;&#20107;&#20214;&#30340;&#21457;&#29983;&#12290;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#25628;&#32034;&#31574;&#30053;&#21644;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#29983;&#25104;&#26032;&#30340;&#35268;&#21017;&#20869;&#23481;&#21644;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21487;&#20197;&#36880;&#27493;&#25193;&#23637;&#35299;&#37322;&#24615;&#30340;&#26102;&#38388;&#36923;&#36753;&#35268;&#21017;&#38598;&#65292;&#20197;&#35299;&#37322;&#26102;&#38388;&#20107;&#20214;&#30340;&#21457;&#29983;&#12290;&#21033;&#29992;&#26102;&#38388;&#28857;&#36807;&#31243;&#24314;&#27169;&#21644;&#23398;&#20064;&#26694;&#26550;&#65292;&#35268;&#21017;&#20869;&#23481;&#21644;&#26435;&#37325;&#23558;&#36880;&#28176;&#20248;&#21270;&#65292;&#30452;&#21040;&#35266;&#27979;&#20107;&#20214;&#24207;&#21015;&#30340;&#20284;&#28982;&#24615;&#26368;&#20248;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#24403;&#21069;&#35268;&#21017;&#38598;&#26435;&#37325;&#26356;&#26032;&#30340;&#20027;&#38382;&#39064;&#21644;&#25628;&#32034;&#24182;&#21253;&#21547;&#26368;&#20339;&#22686;&#21152;&#20284;&#28982;&#24615;&#30340;&#26032;&#35268;&#21017;&#30340;&#23376;&#38382;&#39064;&#20043;&#38388;&#20132;&#26367;&#36827;&#34892;&#12290;&#25152;&#21046;&#23450;&#30340;&#20027;&#38382;&#39064;&#26159;&#20984;&#30340;&#65292;&#20351;&#29992;&#36830;&#32493;&#20248;&#21270;&#30456;&#23545;&#23481;&#26131;&#27714;&#35299;&#65292;&#32780;&#23376;&#38382;&#39064;&#21017;&#38656;&#35201;&#25628;&#32034;&#24040;&#22823;&#30340;&#32452;&#21512;&#35268;&#21017;&#35859;&#35789;&#21644;&#20851;&#31995;&#31354;&#38388;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31070;&#32463;&#25628;&#32034;&#31574;&#30053;&#65292;&#23398;&#20064;&#29983;&#25104;&#26032;&#35268;&#21017;&#20869;&#23481;&#30340;&#19968;&#31995;&#21015;&#21160;&#20316;&#12290;&#31574;&#30053;&#21442;&#25968;&#23558;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#36827;&#34892;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;&#20854;&#20013;&#22870;&#21169;&#20449;&#21495;&#21487;&#20197;&#39640;&#25928;&#22320;&#24471;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a framework that can incrementally expand the explanatory temporal logic rule set to explain the occurrence of temporal events. Leveraging the temporal point process modeling and learning framework, the rule content and weights will be gradually optimized until the likelihood of the observational event sequences is optimal. The proposed algorithm alternates between a master problem, where the current rule set weights are updated, and a subproblem, where a new rule is searched and included to best increase the likelihood. The formulated master problem is convex and relatively easy to solve using continuous optimization, whereas the subproblem requires searching the huge combinatorial rule predicate and relationship space. To tackle this challenge, we propose a neural search policy to learn to generate the new rule content as a sequence of actions. The policy parameters will be trained end-to-end using the reinforcement learning framework, where the reward signals can be effic
&lt;/p&gt;</description></item><item><title>&#29616;&#26377;&#30740;&#31350;&#24050;&#32463;&#34920;&#26126;&#65292;&#36890;&#36807;&#25913;&#36827;&#23545;&#40784;&#21644;&#22343;&#21248;&#24615;&#35774;&#35745;&#30340;&#25439;&#22833;&#20989;&#25968;&#21487;&#20197;&#23454;&#29616;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#31216;&#20026;MAWU&#65292;&#23427;&#32771;&#34385;&#20102;&#25968;&#25454;&#38598;&#30340;&#29420;&#29305;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2308.06091</link><description>&lt;p&gt;
&#23545;&#21327;&#21516;&#36807;&#28388;&#20002;&#22833;&#20989;&#25968;&#30340;&#26356;&#22909;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Toward a Better Understanding of Loss Functions for Collaborative Filtering. (arXiv:2308.06091v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06091
&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30740;&#31350;&#24050;&#32463;&#34920;&#26126;&#65292;&#36890;&#36807;&#25913;&#36827;&#23545;&#40784;&#21644;&#22343;&#21248;&#24615;&#35774;&#35745;&#30340;&#25439;&#22833;&#20989;&#25968;&#21487;&#20197;&#23454;&#29616;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#31216;&#20026;MAWU&#65292;&#23427;&#32771;&#34385;&#20102;&#25968;&#25454;&#38598;&#30340;&#29420;&#29305;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21327;&#21516;&#36807;&#28388;&#65288;CF&#65289;&#26159;&#29616;&#20195;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;CF&#27169;&#22411;&#30340;&#23398;&#20064;&#36807;&#31243;&#36890;&#24120;&#30001;&#19977;&#20010;&#32452;&#20214;&#32452;&#25104;&#65306;&#20132;&#20114;&#32534;&#30721;&#22120;&#12289;&#25439;&#22833;&#20989;&#25968;&#21644;&#36127;&#37319;&#26679;&#12290;&#23613;&#31649;&#35768;&#22810;&#29616;&#26377;&#30740;&#31350;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;CF&#27169;&#22411;&#26469;&#35774;&#35745;&#22797;&#26434;&#30340;&#20132;&#20114;&#32534;&#30721;&#22120;&#65292;&#20294;&#26368;&#36817;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#31616;&#21333;&#22320;&#37325;&#26032;&#21046;&#23450;&#25439;&#22833;&#20989;&#25968;&#21487;&#20197;&#23454;&#29616;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#26412;&#25991;&#28145;&#20837;&#20998;&#26512;&#20102;&#29616;&#26377;&#25439;&#22833;&#20989;&#25968;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#25968;&#23398;&#20998;&#26512;&#25581;&#31034;&#20102;&#20808;&#21069;&#30340;&#25439;&#22833;&#20989;&#25968;&#21487;&#20197;&#35299;&#37322;&#20026;&#23545;&#40784;&#21644;&#22343;&#21248;&#24615;&#20989;&#25968;&#65306;&#65288;i&#65289;&#23545;&#40784;&#21305;&#37197;&#29992;&#25143;&#21644;&#29289;&#21697;&#34920;&#31034;&#65292;&#65288;ii&#65289;&#22343;&#21248;&#24615;&#20998;&#25955;&#29992;&#25143;&#21644;&#29289;&#21697;&#20998;&#24067;&#12290;&#21463;&#21040;&#36825;&#20010;&#20998;&#26512;&#30340;&#21551;&#31034;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#23545;&#40784;&#21644;&#22343;&#21248;&#24615;&#35774;&#35745;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#32771;&#34385;&#21040;&#25968;&#25454;&#38598;&#30340;&#29420;&#29305;&#27169;&#24335;&#65292;&#31216;&#20026;Margin-aware Alignment and Weighted Uniformity&#65288;MAWU&#65289;&#12290;MAWU&#30340;&#20851;&#38190;&#21019;&#26032;&#26159;
&lt;/p&gt;
&lt;p&gt;
Collaborative filtering (CF) is a pivotal technique in modern recommender systems. The learning process of CF models typically consists of three components: interaction encoder, loss function, and negative sampling. Although many existing studies have proposed various CF models to design sophisticated interaction encoders, recent work shows that simply reformulating the loss functions can achieve significant performance gains. This paper delves into analyzing the relationship among existing loss functions. Our mathematical analysis reveals that the previous loss functions can be interpreted as alignment and uniformity functions: (i) the alignment matches user and item representations, and (ii) the uniformity disperses user and item distributions. Inspired by this analysis, we propose a novel loss function that improves the design of alignment and uniformity considering the unique patterns of datasets called Margin-aware Alignment and Weighted Uniformity (MAWU). The key novelty of MAWU 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26159;&#19968;&#39033;&#20351;&#29992;MeasureVAE&#29983;&#25104;&#38899;&#20048;XAI&#27169;&#22411;&#36827;&#34892;&#33258;&#20256;&#24335;&#30740;&#31350;&#30340;&#25506;&#32034;&#65292;&#21457;&#29616;&#35813;&#27169;&#22411;&#22312;&#38899;&#20048;&#21019;&#20316;&#24037;&#20316;&#27969;&#31243;&#20013;&#31361;&#26174;&#20102;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#38899;&#20048;&#29305;&#24449;&#65292;&#24182;&#19988;&#26174;&#31034;&#20986;XAI&#27169;&#22411;&#21487;&#20197;&#22312;&#20016;&#23500;&#21644;&#22797;&#26434;&#30340;&#24037;&#20316;&#27969;&#31243;&#20013;&#21457;&#25381;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.06089</link><description>&lt;p&gt;
&#33258;&#20256;&#24335;&#25506;&#32034;&#31639;&#27861;&#21019;&#20316;&#20013;&#30340;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
An Autoethnographic Exploration of XAI in Algorithmic Composition. (arXiv:2308.06089v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06089
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26159;&#19968;&#39033;&#20351;&#29992;MeasureVAE&#29983;&#25104;&#38899;&#20048;XAI&#27169;&#22411;&#36827;&#34892;&#33258;&#20256;&#24335;&#30740;&#31350;&#30340;&#25506;&#32034;&#65292;&#21457;&#29616;&#35813;&#27169;&#22411;&#22312;&#38899;&#20048;&#21019;&#20316;&#24037;&#20316;&#27969;&#31243;&#20013;&#31361;&#26174;&#20102;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#38899;&#20048;&#29305;&#24449;&#65292;&#24182;&#19988;&#26174;&#31034;&#20986;XAI&#27169;&#22411;&#21487;&#20197;&#22312;&#20016;&#23500;&#21644;&#22797;&#26434;&#30340;&#24037;&#20316;&#27969;&#31243;&#20013;&#21457;&#25381;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#21508;&#31181;&#39118;&#26684;&#30340;&#22797;&#26434;&#38899;&#20048;&#65292;&#20174;&#27665;&#38388;&#38899;&#20048;&#21040;&#21476;&#20856;&#38899;&#20048;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#29983;&#25104;&#38899;&#20048;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#36890;&#24120;&#22312;&#24847;&#20041;&#19978;&#38590;&#20197;&#29702;&#35299;&#21644;&#25511;&#21046;&#12290;&#34429;&#28982;&#30740;&#31350;&#24050;&#24320;&#22987;&#25506;&#32034;&#22914;&#20309;&#20026;&#38899;&#20048;&#21019;&#24314;&#21487;&#35299;&#37322;&#30340;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#27169;&#22411;&#65292;&#20294;&#23578;&#26410;&#30740;&#31350;&#36807;&#22312;&#38899;&#20048;&#21019;&#20316;&#23454;&#36341;&#20013;&#30340;&#29983;&#25104;XAI&#27169;&#22411;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#20851;&#20110;&#20351;&#29992;&#22312;&#29233;&#23572;&#20848;&#27665;&#38388;&#38899;&#20048;&#19978;&#35757;&#32451;&#30340;&#21487;&#35299;&#37322;&#28508;&#22312;&#32500;&#24230;&#27979;&#37327;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;MeasureVAE&#65289;&#29983;&#25104;&#38899;&#20048;XAI&#27169;&#22411;&#30340;&#33258;&#20256;&#24335;&#30740;&#31350;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#38899;&#20048;&#21019;&#20316;&#24037;&#20316;&#27969;&#31243;&#30340;&#25506;&#32034;&#24615;&#36136;&#31361;&#26174;&#20102;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#38899;&#20048;&#29305;&#24449;&#65292;&#32780;&#38750;&#29983;&#25104;&#27169;&#22411;&#26412;&#36523;&#30340;&#29305;&#24449;&#12290;&#23558;XAI&#27169;&#22411;&#32435;&#20837;&#36845;&#20195;&#24335;&#24037;&#20316;&#27969;&#31243;&#20013;&#31361;&#26174;&#20102;XAI&#27169;&#22411;&#22312;&#26356;&#20016;&#23500;&#12289;&#26356;&#22797;&#26434;&#30340;&#24037;&#20316;&#27969;&#31243;&#20013;&#30340;&#28508;&#21147;&#65292;&#32780;&#36825;&#20123;&#27169;&#22411;&#26368;&#21021;&#24182;&#19981;&#26159;&#20026;&#27492;&#35774;&#35745;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Learning models are capable of generating complex music across a range of genres from folk to classical music. However, current generative music AI models are typically difficult to understand and control in meaningful ways. Whilst research has started to explore how explainable AI (XAI) generative models might be created for music, no generative XAI models have been studied in music making practice. This paper introduces an autoethnographic study of the use of the MeasureVAE generative music XAI model with interpretable latent dimensions trained on Irish folk music. Findings suggest that the exploratory nature of the music-making workflow foregrounds musical features of the training dataset rather than features of the generative model itself. The appropriation of an XAI model within an iterative workflow highlights the potential of XAI models to form part of a richer and more complex workflow than they were initially designed for.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#21160;&#35782;&#21035;&#23398;&#29983;&#38169;&#35823;&#21644;&#31616;&#21270;&#25945;&#24072;&#35780;&#20272;&#20013;&#30340;&#28508;&#21147;&#12290;&#20351;&#29992;65&#20221;&#23398;&#29983;&#23454;&#39564;&#26041;&#26696;&#30340;&#25968;&#25454;&#38598;&#65292;&#30740;&#21457;&#20102;&#22522;&#20110;GPT-3.5&#21644;GPT-4&#31995;&#21015;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#31995;&#32479;&#65292;&#24182;&#19982;&#20154;&#24037;&#35780;&#20998;&#21592;&#36827;&#34892;&#27979;&#35797;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;AI&#31995;&#32479;&#22312;&#38169;&#35823;&#26816;&#27979;&#26041;&#38754;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#35768;&#22810;&#22522;&#26412;&#23398;&#29983;&#38169;&#35823;&#12290;</title><link>http://arxiv.org/abs/2308.06088</link><description>&lt;p&gt;
&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#23398;&#29983;&#23454;&#39564;&#38169;&#35823;&#65306;&#19982;&#20154;&#24037;&#35780;&#20998;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Assessing Student Errors in Experimentation Using Artificial Intelligence and Large Language Models: A Comparative Study with Human Raters. (arXiv:2308.06088v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06088
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#21160;&#35782;&#21035;&#23398;&#29983;&#38169;&#35823;&#21644;&#31616;&#21270;&#25945;&#24072;&#35780;&#20272;&#20013;&#30340;&#28508;&#21147;&#12290;&#20351;&#29992;65&#20221;&#23398;&#29983;&#23454;&#39564;&#26041;&#26696;&#30340;&#25968;&#25454;&#38598;&#65292;&#30740;&#21457;&#20102;&#22522;&#20110;GPT-3.5&#21644;GPT-4&#31995;&#21015;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#31995;&#32479;&#65292;&#24182;&#19982;&#20154;&#24037;&#35780;&#20998;&#21592;&#36827;&#34892;&#27979;&#35797;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;AI&#31995;&#32479;&#22312;&#38169;&#35823;&#26816;&#27979;&#26041;&#38754;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#35768;&#22810;&#22522;&#26412;&#23398;&#29983;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36776;&#21035;&#22797;&#26434;&#12289;&#19981;&#23436;&#25972;&#29978;&#33267;&#30683;&#30462;&#20197;&#21450;&#25972;&#20307;&#24322;&#36136;&#21270;&#30340;&#25968;&#25454;&#65292;&#22914;&#23398;&#29983;&#30340;&#23454;&#39564;&#26041;&#26696;&#65292;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#37492;&#20110;&#24403;&#21069;&#35780;&#20272;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#33258;&#21160;&#35782;&#21035;&#23398;&#29983;&#38169;&#35823;&#21644;&#31616;&#21270;&#25945;&#24072;&#35780;&#20272;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20026;&#26377;&#25928;&#30340;&#20010;&#24615;&#21270;&#21453;&#39304;&#25171;&#19979;&#22522;&#30784;&#12290;&#36890;&#36807;&#20351;&#29992;65&#20221;&#23398;&#29983;&#23454;&#39564;&#26041;&#26696;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;GPT-3.5&#21644;GPT-4&#31995;&#21015;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#31995;&#32479;&#65292;&#24182;&#23558;&#20854;&#19982;&#20154;&#24037;&#35780;&#20998;&#21592;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;AI&#31995;&#32479;&#21644;&#20154;&#24037;&#35780;&#20998;&#21592;&#20043;&#38388;&#22312;&#38169;&#35823;&#26816;&#27979;&#26041;&#38754;&#30340;&#20934;&#30830;&#24615;&#23384;&#22312;&#19981;&#21516;&#27700;&#24179;&#12290;AI&#31995;&#32479;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#35768;&#22810;&#22522;&#26412;&#30340;&#23398;&#29983;&#38169;&#35823;&#65292;&#20363;&#22914;&#65292;&#24403;&#23398;&#29983;&#25226;&#20551;&#35774;&#37325;&#28857;&#25918;&#22312;&#29420;&#31435;&#21464;&#37327;&#32780;&#19981;&#26159;&#39044;&#26399;&#30340;&#35266;&#23519;&#19978;&#26102;&#65292;AI&#31995;&#32479;&#33021;&#22815;&#35782;&#21035;&#20986;&#26469;&#65288;&#20934;&#30830;&#24230;=0.90&#65289;&#65292;&#24403;&#23398;&#29983;&#22312;&#36827;&#34892;&#20013;&#30340;&#35843;&#26597;&#20013;&#25913;&#21464;&#35797;&#39564;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Identifying logical errors in complex, incomplete or even contradictory and overall heterogeneous data like students' experimentation protocols is challenging. Recognizing the limitations of current evaluation methods, we investigate the potential of Large Language Models (LLMs) for automatically identifying student errors and streamlining teacher assessments. Our aim is to provide a foundation for productive, personalized feedback. Using a dataset of 65 student protocols, an Artificial Intelligence (AI) system based on the GPT-3.5 and GPT-4 series was developed and tested against human raters. Our results indicate varying levels of accuracy in error detection between the AI system and human raters. The AI system can accurately identify many fundamental student errors, for instance, the AI system identifies when a student is focusing the hypothesis not on the dependent variable but solely on an expected observation (acc. = 0.90), when a student modifies the trials in an ongoing investi
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#26159;&#39318;&#27425;&#25506;&#32034;&#22522;&#20110;&#23618;&#27425;&#20869;&#23384;&#22238;&#25918;&#30340;&#25345;&#32493;&#23398;&#20064;&#30340;&#35774;&#35745;&#31354;&#38388;&#65292;&#26088;&#22312;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#23454;&#29616;&#25104;&#26412;&#25928;&#30410;&#12290;&#25552;&#20986;&#20102;Miro&#65292;&#19968;&#20010;&#36890;&#36807;&#21160;&#24577;&#37197;&#32622;&#25345;&#32493;&#23398;&#20064;&#31995;&#32479;&#30340;&#26032;&#39062;&#31995;&#32479;&#36816;&#34892;&#26102;&#65292;&#20197;&#23454;&#29616;&#26368;&#20339;&#30340;&#25104;&#26412;&#25928;&#30410;&#12290;&#24191;&#27867;&#30340;&#35780;&#20272;&#26174;&#31034;Miro&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2308.06053</link><description>&lt;p&gt;
&#22312;&#20869;&#23384;&#23618;&#27425;&#32467;&#26500;&#19978;&#20855;&#26377;MiRo&#30340;&#25104;&#26412;&#25928;&#30410;&#30340;&#35774;&#22791;&#19978;&#30340;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Cost-effective On-device Continual Learning over Memory Hierarchy with Miro. (arXiv:2308.06053v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06053
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#26159;&#39318;&#27425;&#25506;&#32034;&#22522;&#20110;&#23618;&#27425;&#20869;&#23384;&#22238;&#25918;&#30340;&#25345;&#32493;&#23398;&#20064;&#30340;&#35774;&#35745;&#31354;&#38388;&#65292;&#26088;&#22312;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#23454;&#29616;&#25104;&#26412;&#25928;&#30410;&#12290;&#25552;&#20986;&#20102;Miro&#65292;&#19968;&#20010;&#36890;&#36807;&#21160;&#24577;&#37197;&#32622;&#25345;&#32493;&#23398;&#20064;&#31995;&#32479;&#30340;&#26032;&#39062;&#31995;&#32479;&#36816;&#34892;&#26102;&#65292;&#20197;&#23454;&#29616;&#26368;&#20339;&#30340;&#25104;&#26412;&#25928;&#30410;&#12290;&#24191;&#27867;&#30340;&#35780;&#20272;&#26174;&#31034;Miro&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#26159;&#20174;&#25345;&#32493;&#30340;&#20219;&#21153;&#27969;&#20013;&#36880;&#27493;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#20026;&#20102;&#35760;&#20303;&#20808;&#21069;&#23398;&#21040;&#30340;&#30693;&#35782;&#65292;&#20043;&#21069;&#30340;&#30740;&#31350;&#23558;&#26087;&#26679;&#26412;&#23384;&#20648;&#22312;&#19968;&#20010;&#20869;&#23384;&#23618;&#27425;&#32467;&#26500;&#20013;&#65292;&#24182;&#22312;&#26032;&#20219;&#21153;&#21040;&#26469;&#26102;&#36827;&#34892;&#22238;&#25918;&#12290;&#37319;&#29992;&#25345;&#32493;&#23398;&#20064;&#20197;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#36793;&#32536;&#35774;&#22791;&#36890;&#24120;&#23545;&#33021;&#28304;&#25935;&#24863;&#65292;&#22240;&#27492;&#38656;&#35201;&#22312;&#19981;&#25439;&#23475;&#33021;&#28304;&#25928;&#29575;&#30340;&#24773;&#20917;&#19979;&#20445;&#25345;&#39640;&#27169;&#22411;&#20934;&#30830;&#24230;&#65292;&#21363;&#25104;&#26412;&#25928;&#30410;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26159;&#39318;&#27425;&#25506;&#32034;&#22522;&#20110;&#23618;&#27425;&#20869;&#23384;&#22238;&#25918;&#30340;&#25345;&#32493;&#23398;&#20064;&#30340;&#35774;&#35745;&#31354;&#38388;&#65292;&#20197;&#33719;&#24471;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#25104;&#26412;&#25928;&#30410;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Miro&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;&#31995;&#32479;&#36816;&#34892;&#26102;&#65292;&#36890;&#36807;&#20351;&#20854;&#33021;&#22815;&#26681;&#25454;&#36164;&#28304;&#29366;&#24577;&#21160;&#24577;&#37197;&#32622;&#25345;&#32493;&#23398;&#20064;&#31995;&#32479;&#65292;&#20174;&#32780;&#23558;&#25105;&#20204;&#30340;&#35265;&#35299;&#31934;&#30830;&#22320;&#25972;&#21512;&#21040;&#25345;&#32493;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;&#20197;&#23454;&#29616;&#26368;&#20339;&#25104;&#26412;&#25928;&#30410;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;Miro&#36824;&#23545;&#24102;&#26377;&#26126;&#30830;&#20934;&#30830;&#24230;-&#33021;&#37327;&#24179;&#34913;&#30340;&#21442;&#25968;&#36827;&#34892;&#22312;&#32447;&#20998;&#26512;&#65292;&#24182;&#20197;&#20302;&#24320;&#38144;&#22320;&#36866;&#24212;&#26368;&#20339;&#20540;&#12290;&#24191;&#27867;&#30340;&#35780;&#20272;&#26174;&#31034;Miro&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual learning (CL) trains NN models incrementally from a continuous stream of tasks. To remember previously learned knowledge, prior studies store old samples over a memory hierarchy and replay them when new tasks arrive. Edge devices that adopt CL to preserve data privacy are typically energy-sensitive and thus require high model accuracy while not compromising energy efficiency, i.e., cost-effectiveness. Our work is the first to explore the design space of hierarchical memory replay-based CL to gain insights into achieving cost-effectiveness on edge devices. We present Miro, a novel system runtime that carefully integrates our insights into the CL framework by enabling it to dynamically configure the CL system based on resource states for the best cost-effectiveness. To reach this goal, Miro also performs online profiling on parameters with clear accuracy-energy trade-offs and adapts to optimal values with low overhead. Extensive evaluations show that Miro significantly outperfo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#23398;&#20064;&#24341;&#23548;&#65288;LTG&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#20010;&#24615;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#20154;&#31867;&#19987;&#23478;&#25552;&#20379;&#26377;&#21161;&#20110;&#25351;&#23548;&#20915;&#31574;&#30340;&#25351;&#23548;&#65292;&#35299;&#20915;&#20102;&#26426;&#22120;&#20915;&#31574;&#21644;&#20154;&#31867;&#20915;&#31574;&#20043;&#38388;&#30340;&#20381;&#36182;&#38382;&#39064;&#12290;&#21033;&#29992;SLOG&#23454;&#29616;&#65292;&#35813;&#26694;&#26550;&#22312;&#21307;&#23398;&#35786;&#26029;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#21021;&#27493;&#20294;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.06039</link><description>&lt;p&gt;
&#23398;&#20064;&#36890;&#36807;&#20010;&#24615;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25351;&#23548;&#20154;&#31867;&#19987;&#23478;
&lt;/p&gt;
&lt;p&gt;
Learning to Guide Human Experts via Personalized Large Language Models. (arXiv:2308.06039v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06039
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#23398;&#20064;&#24341;&#23548;&#65288;LTG&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#20010;&#24615;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#20154;&#31867;&#19987;&#23478;&#25552;&#20379;&#26377;&#21161;&#20110;&#25351;&#23548;&#20915;&#31574;&#30340;&#25351;&#23548;&#65292;&#35299;&#20915;&#20102;&#26426;&#22120;&#20915;&#31574;&#21644;&#20154;&#31867;&#20915;&#31574;&#20043;&#38388;&#30340;&#20381;&#36182;&#38382;&#39064;&#12290;&#21033;&#29992;SLOG&#23454;&#29616;&#65292;&#35813;&#26694;&#26550;&#22312;&#21307;&#23398;&#35786;&#26029;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#21021;&#27493;&#20294;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23398;&#20064;&#25512;&#36831;&#30340;&#36807;&#31243;&#20013;&#65292;&#19968;&#20010;&#39044;&#27979;&#22120;&#35782;&#21035;&#20986;&#39118;&#38505;&#20915;&#31574;&#24182;&#23558;&#23427;&#20204;&#25512;&#36831;&#32473;&#20154;&#31867;&#19987;&#23478;&#12290;&#36825;&#31181;&#35774;&#32622;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#26159;&#65292;&#30001;&#20110;&#38170;&#23450;&#20559;&#35265;&#65292;&#19987;&#23478;&#21487;&#33021;&#20250;&#36807;&#20998;&#20381;&#36182;&#20110;&#26426;&#22120;&#30340;&#20915;&#31574;&#12290;&#21516;&#26102;&#65292;&#27599;&#24403;&#26426;&#22120;&#36873;&#25321;&#25512;&#36831;&#36873;&#39033;&#26102;&#65292;&#19987;&#23478;&#24517;&#39035;&#23436;&#20840;&#33258;&#20027;&#22320;&#20570;&#20986;&#20915;&#31574;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23398;&#20064;&#24341;&#23548;&#65288;LTG&#65289;&#30340;&#21478;&#19968;&#31181;&#26694;&#26550;&#65292;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#26426;&#22120;&#19981;&#26159;&#25552;&#20379;&#29616;&#25104;&#30340;&#20915;&#31574;&#65292;&#32780;&#26159;&#25552;&#20379;&#26377;&#21161;&#20110;&#25351;&#23548;&#20915;&#31574;&#30340;&#25351;&#23548;&#65292;&#24182;&#19988;&#20154;&#31867;&#23436;&#20840;&#36127;&#36131;&#20570;&#20986;&#20915;&#31574;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;SLOG&#65292;&#36825;&#26159;&#19968;&#20010;LTG&#23454;&#29616;&#65292;&#21033;&#29992;&#65288;&#23569;&#37327;&#65289;&#20154;&#31867;&#30417;&#30563;&#23558;&#36890;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36716;&#21270;&#20026;&#33021;&#22815;&#29983;&#25104;&#25991;&#26412;&#25351;&#23548;&#30340;&#27169;&#22359;&#65292;&#24182;&#22312;&#21307;&#23398;&#35786;&#26029;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#21021;&#27493;&#20294;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In learning to defer, a predictor identifies risky decisions and defers them to a human expert. One key issue with this setup is that the expert may end up over-relying on the machine's decisions, due to anchoring bias. At the same time, whenever the machine chooses the deferral option the expert has to take decisions entirely unassisted. As a remedy, we propose learning to guide (LTG), an alternative framework in which -- rather than suggesting ready-made decisions -- the machine provides guidance useful to guide decision-making, and the human is entirely responsible for coming up with a decision. We also introduce SLOG, an LTG implementation that leverages (a small amount of) human supervision to convert a generic large language model into a module capable of generating textual guidance, and present preliminary but promising results on a medical diagnosis task.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#28145;&#24230;&#19978;&#19979;&#25991;&#20852;&#36259;&#32593;&#32476;&#65288;DCIN&#65289;&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#23436;&#25972;&#22320;&#24314;&#27169;&#28857;&#20987;&#21450;&#20854;&#23637;&#31034;&#19978;&#19979;&#25991;&#26469;&#23398;&#20064;&#29992;&#25143;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#20852;&#36259;&#65292;&#20197;&#25552;&#39640;&#28857;&#20987;&#29575;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.06037</link><description>&lt;p&gt;
&#28145;&#24230;&#19978;&#19979;&#25991;&#20852;&#36259;&#32593;&#32476;&#29992;&#20110;&#28857;&#20987;&#29575;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Deep Context Interest Network for Click-Through Rate Prediction. (arXiv:2308.06037v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06037
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#28145;&#24230;&#19978;&#19979;&#25991;&#20852;&#36259;&#32593;&#32476;&#65288;DCIN&#65289;&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#23436;&#25972;&#22320;&#24314;&#27169;&#28857;&#20987;&#21450;&#20854;&#23637;&#31034;&#19978;&#19979;&#25991;&#26469;&#23398;&#20064;&#29992;&#25143;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#20852;&#36259;&#65292;&#20197;&#25552;&#39640;&#28857;&#20987;&#29575;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28857;&#20987;&#29575;&#65288;CTR&#65289;&#39044;&#27979;&#26159;&#22312;&#32447;&#24191;&#21578;&#31561;&#24037;&#19994;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#23427;&#20272;&#35745;&#29992;&#25143;&#28857;&#20987;&#26576;&#20010;&#39033;&#30446;&#30340;&#27010;&#29575;&#12290;&#35768;&#22810;&#30740;&#31350;&#33268;&#21147;&#20110;&#29992;&#25143;&#34892;&#20026;&#24314;&#27169;&#20197;&#25552;&#39640;CTR&#39044;&#27979;&#24615;&#33021;&#65292;&#20294;&#22823;&#22810;&#25968;&#26041;&#27861;&#21482;&#20174;&#29992;&#25143;&#28857;&#20987;&#39033;&#30446;&#20013;&#24314;&#27169;&#29992;&#25143;&#30340;&#27491;&#21521;&#20852;&#36259;&#65292;&#24573;&#30053;&#20102;&#23637;&#31034;&#39033;&#30446;&#21608;&#22260;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#23548;&#33268;&#24615;&#33021;&#36739;&#24046;&#12290;&#26412;&#25991;&#24378;&#35843;&#20102;&#19978;&#19979;&#25991;&#20449;&#24687;&#23545;&#29992;&#25143;&#34892;&#20026;&#24314;&#27169;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#28145;&#24230;&#19978;&#19979;&#25991;&#20852;&#36259;&#32593;&#32476;&#65288;DCIN&#65289;&#30340;&#26032;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#23436;&#25972;&#22320;&#24314;&#27169;&#28857;&#20987;&#21450;&#20854;&#23637;&#31034;&#19978;&#19979;&#25991;&#26469;&#23398;&#20064;&#29992;&#25143;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#20852;&#36259;&#12290;DCIN&#21253;&#25324;&#19977;&#20010;&#20851;&#38190;&#27169;&#22359;&#65306;1&#65289;&#20301;&#32622;&#24863;&#30693;&#19978;&#19979;&#25991;&#32858;&#21512;&#27169;&#22359;&#65288;PCAM&#65289;&#65292;&#36890;&#36807;&#27880;&#24847;&#26426;&#21046;&#23545;&#23637;&#31034;&#39033;&#30446;&#36827;&#34892;&#32858;&#21512;&#65307;2&#65289;&#21453;&#39304;-&#19978;&#19979;&#25991;&#34701;&#21512;&#27169;&#22359;&#65288;FCFM&#65289;&#65292;&#36890;&#36807;&#38750;&#32447;&#24615;&#20989;&#25968;&#23558;&#28857;&#20987;&#21644;&#23637;&#31034;&#19978;&#19979;&#25991;&#30340;&#34920;&#31034;&#36827;&#34892;&#34701;&#21512;&#65307;
&lt;/p&gt;
&lt;p&gt;
Click-Through Rate (CTR) prediction, estimating the probability of a user clicking on an item, is essential in industrial applications, such as online advertising. Many works focus on user behavior modeling to improve CTR prediction performance. However, most of those methods only model users' positive interests from users' click items while ignoring the context information, which is the display items around the clicks, resulting in inferior performance. In this paper, we highlight the importance of context information on user behavior modeling and propose a novel model named Deep Context Interest Network (DCIN), which integrally models the click and its display context to learn users' context-aware interests. DCIN consists of three key modules: 1) Position-aware Context Aggregation Module (PCAM), which performs aggregation of display items with an attention mechanism; 2) Feedback-Context Fusion Module (FCFM), which fuses the representation of clicks and display contexts through non-li
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;mLLMs&#65289;&#22312;&#39044;&#27979;&#35821;&#35328;&#22788;&#29702;&#36807;&#31243;&#20013;&#19982;&#20154;&#31867;&#30340;&#35270;&#35273;-&#35821;&#35328;&#38598;&#25104;&#33021;&#21147;&#26159;&#21542;&#19968;&#33268;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;mLLMs&#30340;&#22810;&#27169;&#24577;&#36755;&#20837;&#26041;&#27861;&#21487;&#20197;&#20943;&#23569;&#35748;&#30693;&#36127;&#33655;&#65292;&#25552;&#39640;&#24863;&#30693;&#21644;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.06035</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#27979;&#35821;&#35328;&#22788;&#29702;&#26399;&#38388;&#34920;&#29616;&#20986;&#20154;&#31867;&#35270;&#35273;-&#35821;&#35328;&#38598;&#25104;&#30340;&#35777;&#25454;
&lt;/p&gt;
&lt;p&gt;
Evidence of Human-Like Visual-Linguistic Integration in Multimodal Large Language Models During Predictive Language Processing. (arXiv:2308.06035v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06035
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;mLLMs&#65289;&#22312;&#39044;&#27979;&#35821;&#35328;&#22788;&#29702;&#36807;&#31243;&#20013;&#19982;&#20154;&#31867;&#30340;&#35270;&#35273;-&#35821;&#35328;&#38598;&#25104;&#33021;&#21147;&#26159;&#21542;&#19968;&#33268;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;mLLMs&#30340;&#22810;&#27169;&#24577;&#36755;&#20837;&#26041;&#27861;&#21487;&#20197;&#20943;&#23569;&#35748;&#30693;&#36127;&#33655;&#65292;&#25552;&#39640;&#24863;&#30693;&#21644;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20808;&#36827;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#24341;&#21457;&#20102;&#20851;&#20110;&#23427;&#20204;&#26159;&#21542;&#33021;&#22815;&#22797;&#21046;&#20154;&#31867;&#35748;&#30693;&#36807;&#31243;&#30340;&#20105;&#35758;&#12290;LLMs&#21644;&#20154;&#31867;&#22312;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#30340;&#19968;&#20010;&#21306;&#21035;&#22312;&#20110;&#65292;&#35821;&#35328;&#36755;&#20837;&#36890;&#24120;&#24314;&#31435;&#22312;&#22810;&#20010;&#30693;&#35273;&#27169;&#24577;&#19978;&#65292;&#32780;&#22823;&#22810;&#25968;LLMs&#20165;&#22788;&#29702;&#22522;&#20110;&#25991;&#26412;&#30340;&#20449;&#24687;&#12290;&#22810;&#27169;&#24577;&#22522;&#30784;&#20351;&#20154;&#31867;&#33021;&#22815;&#25972;&#21512;&#35270;&#35273;&#32972;&#26223;&#19982;&#35821;&#35328;&#20449;&#24687;&#65292;&#20174;&#32780;&#23545;&#21363;&#23558;&#20986;&#29616;&#30340;&#21333;&#35789;&#30340;&#31354;&#38388;&#26045;&#21152;&#38480;&#21046;&#65292;&#20943;&#23569;&#35748;&#30693;&#36127;&#33655;&#65292;&#25552;&#39640;&#24863;&#30693;&#21644;&#29702;&#35299;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#22810;&#27169;&#24577;LLMs&#65288;mLLMs&#65289;&#32467;&#21512;&#20102;&#35270;&#35273;&#21644;&#35821;&#35328;&#23884;&#20837;&#31354;&#38388;&#65292;&#24182;&#20351;&#29992;&#21464;&#21387;&#22120;&#31867;&#22411;&#30340;&#27880;&#24847;&#26426;&#21046;&#36827;&#34892;&#19979;&#19968;&#20010;&#21333;&#35789;&#30340;&#39044;&#27979;&#12290;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#65292;&#22522;&#20110;&#22810;&#27169;&#24577;&#36755;&#20837;&#30340;&#39044;&#27979;&#35821;&#35328;&#22788;&#29702;&#22312;mLLMs&#21644;&#20154;&#31867;&#20013;&#21563;&#21512;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;200&#21517;&#34987;&#35797;&#35266;&#30475;&#20102;&#30701;&#30340;&#35270;&#21548;&#21098;&#36753;&#65292;&#24182;&#20272;&#35745;&#20102;&#21363;&#23558;&#20986;&#29616;&#30340;&#21160;&#35789;&#25110;&#21517;&#35789;&#30340;&#21487;&#39044;&#27979;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advanced language processing abilities of large language models (LLMs) have stimulated debate over their capacity to replicate human-like cognitive processes. One differentiating factor between language processing in LLMs and humans is that language input is often grounded in more than one perceptual modality, whereas most LLMs process solely text-based information. Multimodal grounding allows humans to integrate - e.g. visual context with linguistic information and thereby place constraints on the space of upcoming words, reducing cognitive load and improving perception and comprehension. Recent multimodal LLMs (mLLMs) combine visual and linguistic embedding spaces with a transformer type attention mechanism for next-word prediction. To what extent does predictive language processing based on multimodal input align in mLLMs and humans? To answer this question, 200 human participants watched short audio-visual clips and estimated the predictability of an upcoming verb or noun. The 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#21152;&#23494;&#36135;&#24065;&#35777;&#21048;&#26696;&#20214;&#20013;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#21542;&#33021;&#22815;&#20934;&#30830;&#21028;&#26029;&#36829;&#27861;&#34892;&#20026;&#65292;&#24182;&#27604;&#36739;&#20102;&#30001;LLM&#21644;&#24459;&#24072;&#25776;&#20889;&#30340;&#25237;&#35785;&#20070;&#23545;&#38506;&#23457;&#22242;&#20915;&#31574;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#30446;&#21069;&#30340;LLMs&#22312;&#27861;&#24459;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#36739;&#24369;&#65292;&#20294;&#38543;&#30528;&#26410;&#26469;&#27169;&#22411;&#30340;&#25913;&#36827;&#65292;&#20854;&#28508;&#21147;&#26377;&#26395;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2308.06032</link><description>&lt;p&gt;
&#21152;&#23494;&#36135;&#24065;&#35777;&#21048;&#26696;&#20214;&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;ChatGPT&#33021;&#21542;&#21462;&#20195;&#24459;&#24072;&#65311;
&lt;/p&gt;
&lt;p&gt;
Large Language Models in Cryptocurrency Securities Cases: Can ChatGPT Replace Lawyers?. (arXiv:2308.06032v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06032
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#21152;&#23494;&#36135;&#24065;&#35777;&#21048;&#26696;&#20214;&#20013;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#21542;&#33021;&#22815;&#20934;&#30830;&#21028;&#26029;&#36829;&#27861;&#34892;&#20026;&#65292;&#24182;&#27604;&#36739;&#20102;&#30001;LLM&#21644;&#24459;&#24072;&#25776;&#20889;&#30340;&#25237;&#35785;&#20070;&#23545;&#38506;&#23457;&#22242;&#20915;&#31574;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#30446;&#21069;&#30340;LLMs&#22312;&#27861;&#24459;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#36739;&#24369;&#65292;&#20294;&#38543;&#30528;&#26410;&#26469;&#27169;&#22411;&#30340;&#25913;&#36827;&#65292;&#20854;&#28508;&#21147;&#26377;&#26395;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#20197;&#22686;&#24378;&#23545;&#27861;&#24459;&#31995;&#32479;&#30340;&#35775;&#38382;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#23427;&#20204;&#22312;&#36827;&#34892;&#27861;&#24459;&#20219;&#21153;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#30340;&#23454;&#35777;&#30740;&#31350;&#38750;&#24120;&#26377;&#38480;&#12290;&#25105;&#20204;&#30740;&#31350;&#28041;&#21450;&#21152;&#23494;&#36135;&#24065;&#30340;&#35777;&#21048;&#26696;&#20214;&#65292;&#20316;&#20026;AI&#21487;&#20197;&#25903;&#25345;&#27861;&#24459;&#36807;&#31243;&#30340;&#20247;&#22810;&#24773;&#22659;&#20043;&#19968;&#65292;&#30740;&#31350;LLMs&#30340;&#27861;&#24459;&#25512;&#29702;&#21644;&#36215;&#33609;&#33021;&#21147;&#12290;&#25105;&#20204;&#26816;&#26597;&#20197;&#19979;&#20004;&#20010;&#26041;&#38754;&#65306;a&#65289;LLM&#33021;&#21542;&#20934;&#30830;&#30830;&#23450;&#20107;&#23454;&#27169;&#24335;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#36829;&#27861;&#34892;&#20026;&#65292;b&#65289;&#22522;&#20110;LLM&#21644;&#24459;&#24072;&#25776;&#20889;&#30340;&#25237;&#35785;&#20070;&#65292;&#38506;&#23457;&#22242;&#30340;&#20915;&#31574;&#26159;&#21542;&#26377;&#25152;&#24046;&#24322;&#12290;&#25105;&#20204;&#23558;&#30495;&#23454;&#26696;&#20363;&#20013;&#30340;&#20107;&#23454;&#27169;&#24335;&#36755;&#20837;GPT-3.5&#65292;&#24182;&#35780;&#20272;&#20854;&#30830;&#23450;&#27491;&#30830;&#28508;&#22312;&#36829;&#27861;&#34892;&#20026;&#24182;&#25490;&#38500;&#34394;&#20551;&#36829;&#27861;&#34892;&#20026;&#30340;&#33021;&#21147;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35831;&#27169;&#25311;&#38506;&#23457;&#21592;&#35780;&#20272;LLM&#21644;&#24459;&#24072;&#25776;&#20889;&#30340;&#25237;&#35785;&#20070;&#12290;GPT-3.5&#30340;&#27861;&#24459;&#25512;&#29702;&#33021;&#21147;&#36739;&#24369;&#65292;&#20294;&#25105;&#20204;&#39044;&#26399;&#26410;&#26469;&#27169;&#22411;&#30340;&#25913;&#36827;&#65292;&#29305;&#21035;&#26159;&#32771;&#34385;&#21040;&#23427;&#24314;&#35758;&#30340;&#36829;&#27861;&#34892;&#20026;&#24448;&#24448;&#26159;&#27491;&#30830;&#30340;&#65288;&#23427;&#20165;&#20165;&#36807;&#20110;&#20445;&#23432;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) could enhance access to the legal system. However, empirical research on their effectiveness in conducting legal tasks is scant. We study securities cases involving cryptocurrencies as one of numerous contexts where AI could support the legal process, studying LLMs' legal reasoning and drafting capabilities. We examine whether a) an LLM can accurately determine which laws are potentially being violated from a fact pattern, and b) whether there is a difference in juror decision-making based on complaints written by a lawyer compared to an LLM. We feed fact patterns from real-life cases to GPT-3.5 and evaluate its ability to determine correct potential violations from the scenario and exclude spurious violations. Second, we had mock jurors assess complaints written by the LLM and lawyers. GPT-3.5's legal reasoning skills proved weak, though we expect improvement in future models, particularly given the violations it suggested tended to be correct (it merely m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;VQ-VAE&#21644;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#30340;&#19968;&#31181;&#26032;&#22411;&#22522;&#20110;&#20196;&#29260;&#32423;&#25511;&#21046;&#31574;&#30053;&#65292;&#36890;&#36807;&#22312;&#39030;&#37096;&#27880;&#20837;&#39640;&#32423;&#20808;&#39564;&#27169;&#22411;&#26469;&#29983;&#25104;&#26080;&#38480;&#38271;&#19988;&#22810;&#26679;&#21270;&#30340;&#24207;&#21015;&#12290;</title><link>http://arxiv.org/abs/2308.06025</link><description>&lt;p&gt;
&#19981;&#21487;&#35266;&#27979;&#30340;&#39537;&#21160;&#28304;&#19979;&#25511;&#21046;&#35282;&#33394;&#21160;&#20316;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Controlling Character Motions without Observable Driving Source. (arXiv:2308.06025v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06025
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;VQ-VAE&#21644;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#30340;&#19968;&#31181;&#26032;&#22411;&#22522;&#20110;&#20196;&#29260;&#32423;&#25511;&#21046;&#31574;&#30053;&#65292;&#36890;&#36807;&#22312;&#39030;&#37096;&#27880;&#20837;&#39640;&#32423;&#20808;&#39564;&#27169;&#22411;&#26469;&#29983;&#25104;&#26080;&#38480;&#38271;&#19988;&#22810;&#26679;&#21270;&#30340;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20309;&#22312;&#27809;&#26377;&#20219;&#20309;&#39537;&#21160;&#28304;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#22810;&#26679;&#21270;&#12289;&#36924;&#30495;&#19988;&#26080;&#38480;&#38271;&#30340;&#22836;&#37096;/&#36523;&#20307;&#24207;&#21015;&#65311;&#25105;&#20204;&#35748;&#20026;&#36825;&#20010;&#26410;&#32463;&#20805;&#20998;&#25506;&#31350;&#30340;&#30740;&#31350;&#38382;&#39064;&#24182;&#19981;&#26159;&#19968;&#20214;&#36731;&#26494;&#30340;&#20107;&#65292;&#24182;&#19988;&#23384;&#22312;&#30528;&#29420;&#29305;&#30340;&#25216;&#26415;&#25361;&#25112;&#12290;&#22312;&#27809;&#26377;&#26469;&#33258;&#39537;&#21160;&#28304;&#30340;&#35821;&#20041;&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#26631;&#20934;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#29983;&#25104;&#26080;&#38480;&#38271;&#24207;&#21015;&#24456;&#23481;&#26131;&#23548;&#33268;&#20197;&#19979;&#38382;&#39064;&#65306;1&#65289;&#30001;&#20110;&#32047;&#31215;&#35823;&#24046;&#20135;&#29983;&#20102;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#38382;&#39064;&#65307;2&#65289;&#29983;&#25104;&#30340;&#36816;&#21160;&#24207;&#21015;&#32570;&#20047;&#22810;&#26679;&#24615;&#65292;&#26080;&#27861;&#20135;&#29983;&#36924;&#30495;&#21644;&#29983;&#21160;&#30340;&#21160;&#20316;&#24207;&#21015;&#65307;3&#65289;&#26102;&#38388;&#19978;&#20986;&#29616;&#20102;&#19981;&#26399;&#26395;&#30340;&#21608;&#26399;&#24615;&#27169;&#24335;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#26694;&#26550;&#65292;&#23558;VQ-VAE&#30340;&#20248;&#28857;&#19982;&#20351;&#29992;&#32463;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#22870;&#21169;&#20989;&#25968;&#35757;&#32451;&#30340;&#19968;&#31181;&#26032;&#22411;&#22522;&#20110;&#20196;&#29260;&#32423;&#25511;&#21046;&#31574;&#30053;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#22312;&#39030;&#37096;&#27880;&#20837;&#39640;&#32423;&#20808;&#39564;&#27169;&#22411;&#26469;&#29983;&#25104;&#26080;&#38480;&#38271;&#19988;&#22810;&#26679;&#21270;&#30340;&#24207;&#21015;&#12290;&#23613;&#31649;&#25105;&#20204;&#29616;&#22312;&#20851;&#27880;&#20110;&#27809;&#26377;&#39537;&#21160;&#28304;&#65292;&#20294;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#25512;&#24191;&#21040;&#21463;&#25511;&#21512;&#25104;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
How to generate diverse, life-like, and unlimited long head/body sequences without any driving source? We argue that this under-investigated research problem is non-trivial at all, and has unique technical challenges behind it. Without semantic constraints from the driving sources, using the standard autoregressive model to generate infinitely long sequences would easily result in 1) out-of-distribution (OOD) issue due to the accumulated error, 2) insufficient diversity to produce natural and life-like motion sequences and 3) undesired periodic patterns along the time. To tackle the above challenges, we propose a systematic framework that marries the benefits of VQ-VAE and a novel token-level control policy trained with reinforcement learning using carefully designed reward functions. A high-level prior model can be easily injected on top to generate unlimited long and diverse sequences. Although we focus on no driving sources now, our framework can be generalized for controlled synthe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#28040;&#34701;&#23454;&#39564;&#21457;&#29616;&#65292;&#22312;&#21333;&#20010;GPU&#19978;&#65292;&#21442;&#25968;&#25968;&#37327;&#26368;&#22810;&#30340;&#32452;&#21512;&#24182;&#19981;&#19968;&#23450;&#26159;&#26368;&#26377;&#25928;&#30340;&#65292;&#36890;&#36807;&#20943;&#23569;&#21442;&#25968;&#22823;&#23567;&#21487;&#20197;&#22312;&#19981;&#38477;&#20302;&#32763;&#35793;&#36136;&#37327;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#22797;&#26434;&#27169;&#22411;&#65292;&#25581;&#31034;&#20102;&#36229;&#21442;&#25968;&#36873;&#25321;&#12289;&#27169;&#22411;&#22823;&#23567;&#21644;&#35745;&#31639;&#36164;&#28304;&#38656;&#27714;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2308.06017</link><description>&lt;p&gt;
&#20248;&#21270;&#21333;GPU&#35757;&#32451;&#30340;&#22522;&#20110;transformer&#30340;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#65306;&#36229;&#21442;&#25968;&#28040;&#34701;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Optimizing transformer-based machine translation model for single GPU training: a hyperparameter ablation study. (arXiv:2308.06017v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06017
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#28040;&#34701;&#23454;&#39564;&#21457;&#29616;&#65292;&#22312;&#21333;&#20010;GPU&#19978;&#65292;&#21442;&#25968;&#25968;&#37327;&#26368;&#22810;&#30340;&#32452;&#21512;&#24182;&#19981;&#19968;&#23450;&#26159;&#26368;&#26377;&#25928;&#30340;&#65292;&#36890;&#36807;&#20943;&#23569;&#21442;&#25968;&#22823;&#23567;&#21487;&#20197;&#22312;&#19981;&#38477;&#20302;&#32763;&#35793;&#36136;&#37327;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#22797;&#26434;&#27169;&#22411;&#65292;&#25581;&#31034;&#20102;&#36229;&#21442;&#25968;&#36873;&#25321;&#12289;&#27169;&#22411;&#22823;&#23567;&#21644;&#35745;&#31639;&#36164;&#28304;&#38656;&#27714;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#20013;&#65292;&#27169;&#22411;&#22797;&#26434;&#24615;&#21644;&#24615;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#36890;&#24120;&#34987;&#35748;&#20026;&#26159;&#32447;&#24615;&#30340;&#65292;&#39537;&#21160;&#21442;&#25968;&#25968;&#37327;&#22686;&#21152;&#24182;&#23545;&#22810;&#20010;GPU&#31561;&#35745;&#31639;&#36164;&#28304;&#25552;&#20986;&#35201;&#27714;&#12290;&#20026;&#20102;&#25506;&#32034;&#36825;&#19968;&#20551;&#35774;&#65292;&#26412;&#30740;&#31350;&#36890;&#36807;&#36229;&#21442;&#25968;&#28040;&#34701;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#19968;&#20010;&#22522;&#20110;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#26426;&#22120;&#32763;&#35793;&#31649;&#36947;&#22312;&#21333;&#20010;NVIDIA A100 GPU&#19978;&#30340;&#24433;&#21709;&#12290;&#19982;&#39044;&#26399;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#21457;&#29616;&#20855;&#26377;&#26368;&#22810;&#21442;&#25968;&#30340;&#32452;&#21512;&#26410;&#24517;&#26159;&#26368;&#26377;&#25928;&#30340;&#12290;&#36825;&#19968;&#24847;&#22806;&#30340;&#21457;&#29616;&#20419;&#20351;&#25105;&#20204;&#20180;&#32454;&#20943;&#23569;&#21442;&#25968;&#22823;&#23567;&#65292;&#25581;&#31034;&#20102;&#33021;&#22815;&#22312;&#21333;&#20010;GPU&#19978;&#35757;&#32451;&#22797;&#26434;&#27169;&#22411;&#32780;&#19981;&#25439;&#23475;&#32763;&#35793;&#36136;&#37327;&#30340;&#8220;&#29980;&#28857;&#8221;&#12290;&#36825;&#20123;&#21457;&#29616;&#23637;&#31034;&#20102;&#36229;&#21442;&#25968;&#36873;&#25321;&#12289;&#27169;&#22411;&#22823;&#23567;&#21644;&#35745;&#31639;&#36164;&#28304;&#38656;&#27714;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;&#36825;&#39033;&#30740;&#31350;&#30340;&#35265;&#35299;&#23545;&#20110;&#21162;&#21147;&#36827;&#34892;&#26426;&#22120;&#32763;&#35793;&#30340;&#25913;&#36827;&#24037;&#20316;&#26377;&#25152;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
In machine translation tasks, the relationship between model complexity and performance is often presumed to be linear, driving an increase in the number of parameters and consequent demands for computational resources like multiple GPUs. To explore this assumption, this study systematically investigates the effects of hyperparameters through ablation on a sequence-to-sequence machine translation pipeline, utilizing a single NVIDIA A100 GPU. Contrary to expectations, our experiments reveal that combinations with the most parameters were not necessarily the most effective. This unexpected insight prompted a careful reduction in parameter sizes, uncovering "sweet spots" that enable training sophisticated models on a single GPU without compromising translation quality. The findings demonstrate an intricate relationship between hyperparameter selection, model size, and computational resource needs. The insights from this study contribute to the ongoing efforts to make machine translation m
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#30005;&#20449;&#34892;&#19994;&#23558;&#20135;&#29983;&#37325;&#35201;&#30340;&#24433;&#21709;&#12290;&#23427;&#20204;&#21487;&#20197;&#25552;&#39640;&#36816;&#33829;&#25928;&#29575;&#65292;&#31616;&#21270;&#20219;&#21153;&#65292;&#24182;&#38656;&#35201;&#35299;&#20915;&#20351;&#29992;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.06013</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#30005;&#20449;&#34892;&#19994;&#30340;&#26410;&#26469;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Large Language Models for Telecom: Forthcoming Impact on the Industry. (arXiv:2308.06013v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06013
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#30005;&#20449;&#34892;&#19994;&#23558;&#20135;&#29983;&#37325;&#35201;&#30340;&#24433;&#21709;&#12290;&#23427;&#20204;&#21487;&#20197;&#25552;&#39640;&#36816;&#33829;&#25928;&#29575;&#65292;&#31616;&#21270;&#20219;&#21153;&#65292;&#24182;&#38656;&#35201;&#35299;&#20915;&#20351;&#29992;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#32929;&#21464;&#38761;&#30340;&#21147;&#37327;&#65292;&#19981;&#20165;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;&#20256;&#32479;&#39046;&#22495;&#20043;&#22806;&#65292;&#36824;&#22312;&#35768;&#22810;&#39046;&#22495;&#24341;&#36215;&#20102;&#38761;&#21629;&#24615;&#30340;&#20851;&#27880;&#12290;&#38543;&#30528;LLM&#25216;&#26415;&#30340;&#19981;&#26029;&#21457;&#23637;&#65292;&#30005;&#20449;&#34892;&#19994;&#38754;&#20020;&#30528;&#28508;&#22312;&#24433;&#21709;&#30340;&#21069;&#26223;&#12290;&#20026;&#20102;&#38416;&#26126;&#36825;&#20123;&#24433;&#21709;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;LLMs&#30340;&#20869;&#37096;&#26426;&#21046;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#23427;&#20204;&#30446;&#21069;&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#22312;&#30005;&#20449;&#34892;&#19994;&#21487;&#20197;&#26041;&#20415;&#23454;&#26045;&#30340;&#20351;&#29992;&#26696;&#20363;&#65292;&#31616;&#21270;&#20102;&#30446;&#21069;&#22952;&#30861;&#36816;&#33829;&#25928;&#29575;&#24182;&#38656;&#35201;&#22823;&#37327;&#20154;&#21147;&#21644;&#24037;&#31243;&#19987;&#19994;&#30693;&#35782;&#30340;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25581;&#31034;&#20102;&#22312;&#30005;&#20449;&#39046;&#22495;&#21033;&#29992;LLMs&#25152;&#38754;&#20020;&#30340;&#29420;&#29305;&#25361;&#25112;&#30340;&#37325;&#35201;&#30740;&#31350;&#26041;&#21521;&#12290;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#26159;&#20805;&#20998;&#21033;&#29992;LLMs&#28508;&#21147;&#21644;&#21457;&#25381;&#20854;&#33021;&#21147;&#30340;&#37325;&#35201;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have emerged as a transformative force, revolutionizing numerous fields well beyond the conventional domain of Natural Language Processing (NLP) and garnering unprecedented attention. As LLM technology continues to progress, the telecom industry is facing the prospect of its potential impact on its landscape. To elucidate these implications, we delve into the inner workings of LLMs, providing insights into their current capabilities and limitations. We also examine the use cases that can be readily implemented in the telecom industry, streamlining numerous tasks that currently hinder operational efficiency and demand significant manpower and engineering expertise. Furthermore, we uncover essential research directions that deal with the distinctive challenges of utilizing the LLMs within the telecom domain. Addressing these challenges represents a significant stride towards fully harnessing the potential of LLMs and unlocking their capabilities to the fulles
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#20219;&#21153;&#29305;&#23450;&#30340;&#24213;&#23618;&#34920;&#31034;&#32593;&#32476;&#65288;DTRN&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#20219;&#21153;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#36127;&#36801;&#31227;&#38382;&#39064;&#65292;&#36890;&#36807;&#26126;&#30830;&#33719;&#21462;&#27599;&#20010;&#20219;&#21153;&#30340;&#24213;&#23618;&#34920;&#31034;&#26469;&#25913;&#21892;&#20219;&#21153;&#29305;&#23450;&#29305;&#24449;&#30340;&#25429;&#25417;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.05996</link><description>&lt;p&gt;
&#28145;&#24230;&#20219;&#21153;&#29305;&#23450;&#30340;&#24213;&#23618;&#34920;&#31034;&#32593;&#32476;&#29992;&#20110;&#22810;&#20219;&#21153;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Deep Task-specific Bottom Representation Network for Multi-Task Recommendation. (arXiv:2308.05996v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05996
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#20219;&#21153;&#29305;&#23450;&#30340;&#24213;&#23618;&#34920;&#31034;&#32593;&#32476;&#65288;DTRN&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#20219;&#21153;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#36127;&#36801;&#31227;&#38382;&#39064;&#65292;&#36890;&#36807;&#26126;&#30830;&#33719;&#21462;&#27599;&#20010;&#20219;&#21153;&#30340;&#24213;&#23618;&#34920;&#31034;&#26469;&#25913;&#21892;&#20219;&#21153;&#29305;&#23450;&#29305;&#24449;&#30340;&#25429;&#25417;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#24182;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#12290;&#26368;&#36817;&#30340;&#28145;&#24230;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#65288;&#22914;MMoE&#12289;PLE&#65289;&#19987;&#27880;&#20110;&#35774;&#35745;&#22522;&#20110;&#36719;&#38376;&#25511;&#30340;&#21442;&#25968;&#20849;&#20139;&#32593;&#32476;&#65292;&#38544;&#24335;&#22320;&#23398;&#20064;&#27599;&#20010;&#20219;&#21153;&#30340;&#27867;&#21270;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#24403;&#22788;&#29702;&#20914;&#31361;&#20219;&#21153;&#26102;&#65292;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#21487;&#33021;&#20250;&#36973;&#21463;&#24615;&#33021;&#36864;&#21270;&#65292;&#22240;&#20026;&#36127;&#36801;&#31227;&#25928;&#24212;&#21487;&#33021;&#21457;&#29983;&#22312;&#20219;&#21153;&#20849;&#20139;&#30340;&#24213;&#23618;&#34920;&#31034;&#19978;&#12290;&#36825;&#21487;&#33021;&#23548;&#33268;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#25429;&#25417;&#20219;&#21153;&#29305;&#23450;&#29305;&#24449;&#30340;&#33021;&#21147;&#38477;&#20302;&#65292;&#26368;&#32456;&#24433;&#21709;&#20854;&#25928;&#26524;&#65292;&#24182;&#22952;&#30861;&#20854;&#22312;&#25152;&#26377;&#20219;&#21153;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#25512;&#33616;&#31995;&#32479;&#20013;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#24213;&#23618;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#25552;&#20986;&#20102;&#28145;&#24230;&#20219;&#21153;&#29305;&#23450;&#30340;&#24213;&#23618;&#34920;&#31034;&#32593;&#32476;&#65288;DTRN&#65289;&#20197;&#32531;&#35299;&#36127;&#36801;&#31227;&#38382;&#39064;&#12290;DTRN&#36890;&#36807;&#20351;&#27599;&#20010;&#20219;&#21153;&#20855;&#26377;&#33258;&#24049;&#30340;&#34920;&#31034;&#23398;&#20064;&#26126;&#30830;&#22320;&#33719;&#21462;&#20219;&#21153;&#29305;&#23450;&#30340;&#24213;&#23618;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural-based multi-task learning (MTL) has gained significant improvement, and it has been successfully applied to recommendation system (RS). Recent deep MTL methods for RS (e.g. MMoE, PLE) focus on designing soft gating-based parameter-sharing networks that implicitly learn a generalized representation for each task. However, MTL methods may suffer from performance degeneration when dealing with conflicting tasks, as negative transfer effects can occur on the task-shared bottom representation. This can result in a reduced capacity for MTL methods to capture task-specific characteristics, ultimately impeding their effectiveness and hindering the ability to generalize well on all tasks. In this paper, we focus on the bottom representation learning of MTL in RS and propose the Deep Task-specific Bottom Representation Network (DTRN) to alleviate the negative transfer problem. DTRN obtains task-specific bottom representation explicitly by making each task has its own representation learni
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;WavLM&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#22522;&#20110;&#35821;&#38899;&#39537;&#21160;&#30340;&#25163;&#21183;&#21512;&#25104;&#26041;&#27861;&#65292;&#23454;&#29616;&#21482;&#20351;&#29992;&#21407;&#22987;&#35821;&#38899;&#38899;&#39057;&#29983;&#25104;&#20010;&#24615;&#21270;&#20840;&#36523;&#25163;&#21183;&#65292;&#28040;&#38500;&#20102;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#22788;&#29702;&#21644;&#25163;&#21160;&#27880;&#37322;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2308.05995</link><description>&lt;p&gt;
&#19968;&#20307;&#21270;&#38899;&#39057;&#65306;&#21033;&#29992;WavLM&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#22522;&#20110;&#35821;&#38899;&#39537;&#21160;&#30340;&#25163;&#21183;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Audio is all in one: speech-driven gesture synthetics using WavLM pre-trained model. (arXiv:2308.05995v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05995
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;WavLM&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#22522;&#20110;&#35821;&#38899;&#39537;&#21160;&#30340;&#25163;&#21183;&#21512;&#25104;&#26041;&#27861;&#65292;&#23454;&#29616;&#21482;&#20351;&#29992;&#21407;&#22987;&#35821;&#38899;&#38899;&#39057;&#29983;&#25104;&#20010;&#24615;&#21270;&#20840;&#36523;&#25163;&#21183;&#65292;&#28040;&#38500;&#20102;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#22788;&#29702;&#21644;&#25163;&#21160;&#27880;&#37322;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#34394;&#25311;&#20154;&#21019;&#20316;&#39046;&#22495;&#65292;&#29983;&#25104;&#19982;&#35821;&#38899;&#37197;&#22871;&#30340;&#25163;&#21183;&#26159;&#19968;&#20010;&#27491;&#22312;&#20852;&#36215;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#36807;&#20197;&#22768;&#23398;&#21644;&#35821;&#20041;&#20449;&#24687;&#20316;&#20026;&#36755;&#20837;&#65292;&#37319;&#29992;&#20998;&#31867;&#26041;&#27861;&#26469;&#35782;&#21035;&#20154;&#29289;&#30340;ID&#21644;&#24773;&#24863;&#65292;&#20197;&#39537;&#21160;&#19982;&#35821;&#38899;&#37197;&#22871;&#30340;&#25163;&#21183;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#36825;&#39033;&#24037;&#20316;&#20173;&#28982;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#36825;&#20123;&#25361;&#25112;&#19981;&#20165;&#28041;&#21450;&#25163;&#21183;&#12289;&#35821;&#38899;&#22768;&#23398;&#21644;&#35821;&#20041;&#20043;&#38388;&#38169;&#32508;&#22797;&#26434;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#36824;&#21253;&#25324;&#19982;&#20010;&#24615;&#12289;&#24773;&#24863;&#21644;&#20854;&#20182;&#19981;&#26126;&#30830;&#20294;&#37325;&#35201;&#30340;&#22240;&#32032;&#30456;&#20851;&#30340;&#22797;&#26434;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#8220;diffmotion-v2&#8221;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#35821;&#38899;&#26465;&#20214;&#25193;&#25955;&#21644;&#22522;&#20110;&#38750;&#33258;&#22238;&#24402;Transformer&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#37319;&#29992;WavLM&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#23427;&#21487;&#20197;&#20165;&#20351;&#29992;&#21407;&#22987;&#35821;&#38899;&#38899;&#39057;&#29983;&#25104;&#20010;&#24615;&#21270;&#30340;&#20840;&#36523;&#25163;&#21183;&#65292;&#28040;&#38500;&#20102;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#22788;&#29702;&#21644;&#25163;&#21160;&#27880;&#37322;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
The generation of co-speech gestures for digital humans is an emerging area in the field of virtual human creation. Prior research has made progress by using acoustic and semantic information as input and adopting classify method to identify the person's ID and emotion for driving co-speech gesture generation. However, this endeavour still faces significant challenges. These challenges go beyond the intricate interplay between co-speech gestures, speech acoustic, and semantics; they also encompass the complexities associated with personality, emotion, and other obscure but important factors. This paper introduces "diffmotion-v2," a speech-conditional diffusion-based and non-autoregressive transformer-based generative model with WavLM pre-trained model. It can produce individual and stylized full-body co-speech gestures only using raw speech audio, eliminating the need for complex multimodal processing and manually annotated. Firstly, considering that speech audio not only contains acou
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;TrajPAC&#30340;&#21407;&#22411;&#24037;&#20855;&#65292;&#23427;&#37319;&#29992;&#21487;&#33021;&#36817;&#20284;&#27491;&#30830; (PAC) &#30340;&#26694;&#26550;&#26469;&#39564;&#35777;&#34892;&#20154;&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#23450;&#20041;&#26631;&#31614;&#40065;&#26834;&#24615;&#21644;&#32431;&#40065;&#26834;&#24615;&#65292;&#24182;&#32771;&#34385;&#24178;&#25200;&#21306;&#38388;&#20013;&#25152;&#26377;&#28857;&#30340;&#40065;&#26834;&#24615;&#65292;TrajPAC&#19981;&#20165;&#21487;&#20197;&#35782;&#21035;&#28508;&#22312;&#30340;&#21453;&#20363;&#65292;&#36824;&#21487;&#20197;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#20998;&#26512;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.05985</link><description>&lt;p&gt;
&#36816;&#21160;&#32773;&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#39564;&#35777;&#65306;TrajPAC
&lt;/p&gt;
&lt;p&gt;
TrajPAC: Towards Robustness Verification of Pedestrian Trajectory Prediction Models. (arXiv:2308.05985v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05985
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;TrajPAC&#30340;&#21407;&#22411;&#24037;&#20855;&#65292;&#23427;&#37319;&#29992;&#21487;&#33021;&#36817;&#20284;&#27491;&#30830; (PAC) &#30340;&#26694;&#26550;&#26469;&#39564;&#35777;&#34892;&#20154;&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#23450;&#20041;&#26631;&#31614;&#40065;&#26834;&#24615;&#21644;&#32431;&#40065;&#26834;&#24615;&#65292;&#24182;&#32771;&#34385;&#24178;&#25200;&#21306;&#38388;&#20013;&#25152;&#26377;&#28857;&#30340;&#40065;&#26834;&#24615;&#65292;TrajPAC&#19981;&#20165;&#21487;&#20197;&#35782;&#21035;&#28508;&#22312;&#30340;&#21453;&#20363;&#65292;&#36824;&#21487;&#20197;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#20998;&#26512;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40065;&#26834;&#30340;&#34892;&#20154;&#36712;&#36857;&#39044;&#27979;&#23545;&#20110;&#24320;&#21457;&#23433;&#20840;&#30340;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#30740;&#31350;&#20102;&#36712;&#36857;&#39044;&#27979;&#20013;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#20294;&#20173;&#23384;&#22312;&#19968;&#20123;&#37325;&#35201;&#30340;&#38382;&#39064;&#23578;&#26410;&#35299;&#20915;&#12290;&#26412;&#30740;&#31350;&#35797;&#22270;&#35299;&#20915;&#36825;&#20123;&#20851;&#38190;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#20808;&#21069;&#30340;&#36712;&#36857;&#39044;&#27979;&#40065;&#26834;&#24615;&#23450;&#20041;&#27169;&#31946;&#19981;&#28165;&#65292;&#22240;&#27492;&#25105;&#20204;&#25552;&#20379;&#20102;&#20004;&#31181;&#40065;&#26834;&#24615;&#30340;&#24418;&#24335;&#23450;&#20041;&#65292;&#21363;&#26631;&#31614;&#40065;&#26834;&#24615;&#21644;&#32431;&#40065;&#26834;&#24615;&#12290;&#20854;&#27425;&#65292;&#30001;&#20110;&#20808;&#21069;&#30340;&#30740;&#31350;&#26410;&#33021;&#32771;&#34385;&#24178;&#25200;&#21306;&#38388;&#20013;&#25152;&#26377;&#28857;&#30340;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#20010;&#21487;&#33021;&#36817;&#20284;&#27491;&#30830; (PAC) &#30340;&#26694;&#26550;&#36827;&#34892;&#40065;&#26834;&#24615;&#39564;&#35777;&#12290;&#27492;&#22806;&#65292;&#35813;&#26694;&#26550;&#19981;&#20165;&#21487;&#20197;&#35782;&#21035;&#28508;&#22312;&#30340;&#21453;&#20363;&#65292;&#36824;&#21487;&#20197;&#23545;&#21407;&#22987;&#26041;&#27861;&#36827;&#34892;&#21487;&#35299;&#37322;&#30340;&#20998;&#26512;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#21517;&#20026;TrajPAC&#30340;&#21407;&#22411;&#24037;&#20855;&#26469;&#24212;&#29992;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;TrajPAC&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22235;&#31181;&#26368;&#20808;&#36827;&#30340;&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;- Trajectron++&#65292;Mem...
&lt;/p&gt;
&lt;p&gt;
Robust pedestrian trajectory forecasting is crucial to developing safe autonomous vehicles. Although previous works have studied adversarial robustness in the context of trajectory forecasting, some significant issues remain unaddressed. In this work, we try to tackle these crucial problems. Firstly, the previous definitions of robustness in trajectory prediction are ambiguous. We thus provide formal definitions for two kinds of robustness, namely label robustness and pure robustness. Secondly, as previous works fail to consider robustness about all points in a disturbance interval, we utilise a probably approximately correct (PAC) framework for robustness verification. Additionally, this framework can not only identify potential counterexamples, but also provides interpretable analyses of the original methods. Our approach is applied using a prototype tool named TrajPAC. With TrajPAC, we evaluate the robustness of four state-of-the-art trajectory prediction models -Trajectron++, Mem
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;MAoE&#65292;&#19968;&#31181;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#20248;&#21270;&#38382;&#39064;&#30340;&#23545;&#27604;&#35299;&#37322;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#29983;&#25104;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#24182;&#31361;&#20986;&#26174;&#31034;&#19982;&#21021;&#22987;&#35299;&#20915;&#26041;&#26696;&#30340;&#24046;&#24322;&#65292;&#24110;&#21161;&#26234;&#33021;&#20307;&#29702;&#35299;&#20026;&#20160;&#20040;&#21021;&#22987;&#35299;&#20915;&#26041;&#26696;&#20248;&#20110;&#20182;&#20204;&#30340;&#26399;&#26395;&#12290;</title><link>http://arxiv.org/abs/2308.05984</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#20248;&#21270;&#35299;&#20915;&#26041;&#26696;&#30340;&#23545;&#27604;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Contrastive Explanations of Multi-agent Optimization Solutions. (arXiv:2308.05984v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05984
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;MAoE&#65292;&#19968;&#31181;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#20248;&#21270;&#38382;&#39064;&#30340;&#23545;&#27604;&#35299;&#37322;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#29983;&#25104;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#24182;&#31361;&#20986;&#26174;&#31034;&#19982;&#21021;&#22987;&#35299;&#20915;&#26041;&#26696;&#30340;&#24046;&#24322;&#65292;&#24110;&#21161;&#26234;&#33021;&#20307;&#29702;&#35299;&#20026;&#20160;&#20040;&#21021;&#22987;&#35299;&#20915;&#26041;&#26696;&#20248;&#20110;&#20182;&#20204;&#30340;&#26399;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#22330;&#26223;&#20013;&#65292;&#26234;&#33021;&#20307;&#21442;&#19982;&#20248;&#21270;&#38382;&#39064;&#12290;&#30001;&#20110;&#22823;&#22810;&#25968;&#24773;&#20917;&#37117;&#26159;&#36229;&#32422;&#26463;&#30340;&#65292;&#26368;&#20248;&#35299;&#24182;&#19981;&#24635;&#33021;&#28385;&#36275;&#25152;&#26377;&#26234;&#33021;&#20307;&#30340;&#35201;&#27714;&#12290;&#26377;&#20123;&#26234;&#33021;&#20307;&#21487;&#33021;&#19981;&#28385;&#24847;&#65292;&#24182;&#25552;&#20986;&#31867;&#20284;&#8220;&#20026;&#20160;&#20040;&#35299;&#20915;&#26041;&#26696;S&#19981;&#28385;&#36275;&#23646;&#24615;P&#65311;&#8221;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MAoE&#65292;&#36825;&#26159;&#19968;&#31181;&#39046;&#22495;&#26080;&#20851;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#65288;i&#65289;&#29983;&#25104;&#19968;&#20010;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;S'&#65292;&#35813;&#35299;&#20915;&#26041;&#26696;&#24378;&#21046;&#28385;&#36275;&#23646;&#24615;P&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;S&#21644;S'&#20043;&#38388;&#30340;&#24046;&#24322;&#65307;&#20197;&#21450;&#65288;ii&#65289;&#31361;&#20986;&#26174;&#31034;&#36825;&#20004;&#20010;&#35299;&#20915;&#26041;&#26696;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#36825;&#26679;&#30340;&#35299;&#37322;&#26088;&#22312;&#24110;&#21161;&#26234;&#33021;&#20307;&#29702;&#35299;&#20026;&#20160;&#20040;&#21021;&#22987;&#35299;&#20915;&#26041;&#26696;&#20248;&#20110;&#20182;&#20204;&#30340;&#26399;&#26395;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#35745;&#31639;&#35780;&#20272;&#65292;&#34920;&#26126;MAoE&#21487;&#20197;&#20026;&#22823;&#22411;&#22810;&#26234;&#33021;&#20307;&#20248;&#21270;&#38382;&#39064;&#29983;&#25104;&#23545;&#27604;&#35299;&#37322;&#12290;&#25105;&#20204;&#36824;&#22312;&#22235;&#20010;&#19981;&#21516;&#30340;&#39046;&#22495;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#29992;&#25143;&#30740;&#31350;&#65292;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#25552;&#20379;&#36825;&#20123;&#35299;&#37322;&#21518;&#65292;&#29992;&#25143;&#23545;&#35299;&#20915;&#26041;&#26696;&#30340;&#29702;&#35299;&#26377;&#25152;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many real-world scenarios, agents are involved in optimization problems. Since most of these scenarios are over-constrained, optimal solutions do not always satisfy all agents. Some agents might be unhappy and ask questions of the form ``Why does solution $S$ not satisfy property $P$?''. In this paper, we propose MAoE, a domain-independent approach to obtain contrastive explanations by (i) generating a new solution $S^\prime$ where the property $P$ is enforced, while also minimizing the differences between $S$ and $S^\prime$; and (ii) highlighting the differences between the two solutions. Such explanations aim to help agents understanding why the initial solution is better than what they expected. We have carried out a computational evaluation that shows that MAoE can generate contrastive explanations for large multi-agent optimization problems. We have also performed an extensive user study in four different domains that shows that, after being presented with these explanations, h
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21463;&#38480;&#39057;&#29575;&#30340;&#36523;&#20221;&#19981;&#21487;&#30693;&#25915;&#20987;&#36827;&#34892;&#20154;&#33080;&#21152;&#23494;&#65292;&#35299;&#20915;&#20102;&#20351;&#29992;&#22806;&#37096;&#25200;&#21160;&#21152;&#23494;&#20154;&#33080;&#22270;&#20687;&#30340;&#38544;&#31169;&#20445;&#25252;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24369;&#40657;&#30418;&#22330;&#26223;&#19979;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2308.05983</link><description>&lt;p&gt;
&#36890;&#36807;&#21463;&#38480;&#39057;&#29575;&#30340;&#36523;&#20221;&#19981;&#21487;&#30693;&#25915;&#20987;&#36827;&#34892;&#20154;&#33080;&#21152;&#23494;
&lt;/p&gt;
&lt;p&gt;
Face Encryption via Frequency-Restricted Identity-Agnostic Attacks. (arXiv:2308.05983v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05983
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21463;&#38480;&#39057;&#29575;&#30340;&#36523;&#20221;&#19981;&#21487;&#30693;&#25915;&#20987;&#36827;&#34892;&#20154;&#33080;&#21152;&#23494;&#65292;&#35299;&#20915;&#20102;&#20351;&#29992;&#22806;&#37096;&#25200;&#21160;&#21152;&#23494;&#20154;&#33080;&#22270;&#20687;&#30340;&#38544;&#31169;&#20445;&#25252;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24369;&#40657;&#30418;&#22330;&#26223;&#19979;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27599;&#22825;&#26377;&#25968;&#21313;&#20159;&#20154;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#20998;&#20139;&#20182;&#20204;&#30340;&#26085;&#24120;&#29031;&#29255;&#12290;&#28982;&#32780;&#65292;&#24694;&#24847;&#37319;&#38598;&#32773;&#21033;&#29992;&#28145;&#24230;&#20154;&#33080;&#35782;&#21035;&#31995;&#32479;&#36731;&#26494;&#22320;&#20174;&#36825;&#20123;&#22270;&#29255;&#20013;&#31363;&#21462;&#20182;&#20204;&#30340;&#29983;&#29289;&#29305;&#24449;&#20449;&#24687;&#65288;&#20363;&#22914;&#20154;&#33080;&#65289;&#12290;&#19968;&#20123;&#30740;&#31350;&#27491;&#22312;&#36827;&#34892;&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#38590;&#20197;&#23519;&#35273;&#30340;&#25200;&#21160;&#26469;&#29983;&#25104;&#21152;&#23494;&#20154;&#33080;&#29031;&#29255;&#65292;&#20197;&#20943;&#23569;&#20154;&#33080;&#20449;&#24687;&#27844;&#28431;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#38656;&#35201;&#26356;&#24378;&#30340;&#40657;&#30418;&#22330;&#26223;&#21487;&#34892;&#24615;&#21644;&#26356;&#33258;&#28982;&#30340;&#35270;&#35273;&#22806;&#35266;&#65292;&#36825;&#23545;&#38544;&#31169;&#20445;&#25252;&#30340;&#21487;&#34892;&#24615;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#38480;&#39057;&#29575;&#30340;&#36523;&#20221;&#19981;&#21487;&#30693;&#65288;FRIA&#65289;&#26694;&#26550;&#65292;&#20197;&#20174;&#26410;&#32463;&#25480;&#26435;&#30340;&#20154;&#33080;&#35782;&#21035;&#20013;&#21152;&#23494;&#20154;&#33080;&#22270;&#20687;&#65292;&#32780;&#26080;&#38656;&#35775;&#38382;&#20010;&#20154;&#20449;&#24687;&#12290;&#23545;&#20110;&#24369;&#40657;&#30418;&#22330;&#26223;&#30340;&#21487;&#34892;&#24615;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22810;&#20010;&#20154;&#33080;&#35782;&#21035;&#27169;&#22411;&#20013;&#30340;&#24179;&#22343;&#29305;&#24449;&#34920;&#31034;&#30456;&#20284;&#65292;&#22240;&#27492;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#36890;&#36807;&#20114;&#32852;&#32593;&#29228;&#21462;&#30340;&#25968;&#25454;&#38598;&#20013;&#30340;&#24179;&#22343;&#29305;&#24449;&#20316;&#20026;t
&lt;/p&gt;
&lt;p&gt;
Billions of people are sharing their daily live images on social media everyday. However, malicious collectors use deep face recognition systems to easily steal their biometric information (e.g., faces) from these images. Some studies are being conducted to generate encrypted face photos using adversarial attacks by introducing imperceptible perturbations to reduce face information leakage. However, existing studies need stronger black-box scenario feasibility and more natural visual appearances, which challenge the feasibility of privacy protection. To address these problems, we propose a frequency-restricted identity-agnostic (FRIA) framework to encrypt face images from unauthorized face recognition without access to personal information. As for the weak black-box scenario feasibility, we obverse that representations of the average feature in multiple face recognition models are similar, thus we propose to utilize the average feature via the crawled dataset from the Internet as the t
&lt;/p&gt;</description></item><item><title>CyberForce&#26159;&#19968;&#20010;&#32852;&#37030;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#29289;&#32852;&#32593;&#35774;&#22791;&#20013;&#21327;&#21516;&#31169;&#23494;&#22320;&#30830;&#23450;&#36866;&#21512;&#32531;&#35299;&#21508;&#31181;&#38646;&#26085;&#25915;&#20987;&#30340;MTD&#25216;&#26415;&#12290;&#23427;&#25972;&#21512;&#20102;&#35774;&#22791;&#25351;&#32441;&#35782;&#21035;&#21644;&#24322;&#24120;&#26816;&#27979;&#65292;&#24182;&#36890;&#36807;&#22870;&#21169;&#25110;&#24809;&#32602;FRL agent&#36873;&#25321;&#30340;MTD&#26426;&#21046;&#26469;&#25552;&#39640;&#32593;&#32476;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.05978</link><description>&lt;p&gt;
CyberForce: &#19968;&#20010;&#29992;&#20110;&#24694;&#24847;&#36719;&#20214;&#32531;&#35299;&#30340;&#32852;&#37030;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
CyberForce: A Federated Reinforcement Learning Framework for Malware Mitigation. (arXiv:2308.05978v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05978
&lt;/p&gt;
&lt;p&gt;
CyberForce&#26159;&#19968;&#20010;&#32852;&#37030;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#29289;&#32852;&#32593;&#35774;&#22791;&#20013;&#21327;&#21516;&#31169;&#23494;&#22320;&#30830;&#23450;&#36866;&#21512;&#32531;&#35299;&#21508;&#31181;&#38646;&#26085;&#25915;&#20987;&#30340;MTD&#25216;&#26415;&#12290;&#23427;&#25972;&#21512;&#20102;&#35774;&#22791;&#25351;&#32441;&#35782;&#21035;&#21644;&#24322;&#24120;&#26816;&#27979;&#65292;&#24182;&#36890;&#36807;&#22870;&#21169;&#25110;&#24809;&#32602;FRL agent&#36873;&#25321;&#30340;MTD&#26426;&#21046;&#26469;&#25552;&#39640;&#32593;&#32476;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20114;&#32852;&#32593;&#29289;&#32852;&#32593;(IoT)&#33539;&#20363;&#30340;&#25193;&#23637;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#65292;&#20294;&#26159;&#23545;&#20110;IoT&#35774;&#22791;&#23545;&#24694;&#24847;&#36719;&#20214;&#20107;&#20214;&#30340;&#33030;&#24369;&#24615;&#24050;&#25104;&#20026;&#19968;&#20010;&#36234;&#26469;&#36234;&#20851;&#27880;&#30340;&#38382;&#39064;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#26174;&#31034;&#65292;&#23558;&#24378;&#21270;&#23398;&#20064;&#19982;&#31227;&#21160;&#30446;&#26631;&#38450;&#24481;(MTD)&#26426;&#21046;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#22686;&#24378;IoT&#35774;&#22791;&#30340;&#32593;&#32476;&#23433;&#20840;&#24615;&#12290;&#28982;&#32780;&#65292;&#22823;&#37327;&#30340;&#26032;&#24694;&#24847;&#36719;&#20214;&#25915;&#20987;&#21644;&#20195;&#29702;&#20154;&#23398;&#20064;&#21644;&#36873;&#25321;&#26377;&#25928;&#30340;MTD&#25216;&#26415;&#25152;&#38656;&#30340;&#26102;&#38388;&#20351;&#24471;&#36825;&#31181;&#26041;&#27861;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;IoT&#22330;&#26223;&#20013;&#19981;&#20999;&#23454;&#38469;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;CyberForce&#65292;&#19968;&#20010;&#37319;&#29992;&#32852;&#37030;&#24378;&#21270;&#23398;&#20064;(FRL)&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#38598;&#20307;&#19988;&#20445;&#23494;&#22320;&#30830;&#23450;&#36866;&#21512;&#32531;&#35299;&#21508;&#31181;&#38646;&#26085;&#25915;&#20987;&#30340;MTD&#25216;&#26415;&#12290;CyberForce&#32467;&#21512;&#20102;&#35774;&#22791;&#25351;&#32441;&#35782;&#21035;&#21644;&#24322;&#24120;&#26816;&#27979;&#65292;&#36890;&#36807;&#22870;&#21169;&#25110;&#24809;&#32602;FRL agent&#36873;&#25321;&#30340;MTD&#26426;&#21046;&#12290;&#35813;&#26694;&#26550;&#22312;&#19968;&#20010;&#30001;&#21313;&#21488;&#30495;&#23454;IoT&#24179;&#21488;&#35774;&#22791;&#32452;&#25104;&#30340;&#32852;&#37030;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#36890;&#36807;&#20845;&#20010;&#24694;&#24847;&#36719;&#20214;&#26679;&#26412;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
The expansion of the Internet-of-Things (IoT) paradigm is inevitable, but vulnerabilities of IoT devices to malware incidents have become an increasing concern. Recent research has shown that the integration of Reinforcement Learning with Moving Target Defense (MTD) mechanisms can enhance cybersecurity in IoT devices. Nevertheless, the numerous new malware attacks and the time that agents take to learn and select effective MTD techniques make this approach impractical for real-world IoT scenarios. To tackle this issue, this work presents CyberForce, a framework that employs Federated Reinforcement Learning (FRL) to collectively and privately determine suitable MTD techniques for mitigating diverse zero-day attacks. CyberForce integrates device fingerprinting and anomaly detection to reward or penalize MTD mechanisms chosen by an FRL-based agent. The framework has been evaluated in a federation consisting of ten devices of a real IoT platform. A pool of experiments with six malware samp
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20351;&#29992;Viterbi&#31639;&#27861;&#21644;&#36801;&#31227;&#23398;&#20064;&#26469;&#25552;&#21462;&#25512;&#29305;&#24773;&#32490;&#65292;&#24341;&#20837;&#32622;&#20449;&#24230;&#20998;&#25968;&#21644;&#21521;&#37327;&#20316;&#20026;&#35780;&#20272;&#27169;&#22411;&#30340;&#25351;&#26631;&#65292;&#36827;&#34892;&#27169;&#22411;&#30340;&#20869;&#37096;&#35780;&#20272;&#21644;&#24494;&#35843;&#12290;</title><link>http://arxiv.org/abs/2308.05973</link><description>&lt;p&gt;
&#20351;&#29992;Viterbi&#31639;&#27861;&#21644;&#36801;&#31227;&#23398;&#20064;&#36827;&#34892;&#25512;&#29305;&#24773;&#32490;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Tweet Sentiment Extraction using Viterbi Algorithm with Transfer Learning. (arXiv:2308.05973v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05973
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20351;&#29992;Viterbi&#31639;&#27861;&#21644;&#36801;&#31227;&#23398;&#20064;&#26469;&#25552;&#21462;&#25512;&#29305;&#24773;&#32490;&#65292;&#24341;&#20837;&#32622;&#20449;&#24230;&#20998;&#25968;&#21644;&#21521;&#37327;&#20316;&#20026;&#35780;&#20272;&#27169;&#22411;&#30340;&#25351;&#26631;&#65292;&#36827;&#34892;&#27169;&#22411;&#30340;&#20869;&#37096;&#35780;&#20272;&#21644;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#29305;&#24773;&#32490;&#25552;&#21462;&#26159;&#25552;&#21462;&#21477;&#23376;&#20013;&#26368;&#37325;&#35201;&#37096;&#20998;&#30340;&#36807;&#31243;&#65292;&#21028;&#26029;&#24773;&#32490;&#26159;&#31215;&#26497;&#36824;&#26159;&#28040;&#26497;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#35782;&#21035;&#25512;&#29305;&#21477;&#23376;&#20013;&#24341;&#36215;&#24773;&#24863;&#30340;&#37096;&#20998;&#12290;&#20026;&#20102;&#36798;&#21040;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#32487;&#32493;&#25913;&#36827;&#20316;&#32773;&#20043;&#21069;&#20462;&#25913;&#30340;Viterbi&#31639;&#27861;&#65292;&#20351;&#20854;&#33021;&#22815;&#25509;&#25910;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#21442;&#25968;&#12290;&#25105;&#20204;&#24341;&#20837;&#32622;&#20449;&#24230;&#20998;&#25968;&#21644;&#21521;&#37327;&#20316;&#20026;&#20004;&#20010;&#25351;&#26631;&#65292;&#29992;&#20110;&#22312;&#35780;&#20272;&#26368;&#32456;&#32467;&#26524;&#20043;&#21069;&#23545;&#27169;&#22411;&#36827;&#34892;&#20869;&#37096;&#35780;&#20272;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#24494;&#35843;&#36825;&#20010;&#38750;&#21442;&#25968;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#38543;&#30528;&#32622;&#20449;&#24230;&#20998;&#25968;&#21521;&#37327;&#20934;&#30830;&#22320;&#26174;&#31034;&#20986;&#26368;&#19981;&#33258;&#20449;&#30340;&#39044;&#27979;&#29366;&#24577;&#30340;&#20301;&#32622;&#65292;&#20197;&#21450;&#20462;&#25913;&#26159;&#21542;&#25913;&#21892;&#20102;&#32622;&#20449;&#24230;&#20998;&#25968;&#25110;&#24494;&#35843;&#26159;&#21542;&#26397;&#30528;&#38169;&#35823;&#30340;&#26041;&#21521;&#36827;&#34892;&#65292;&#27169;&#22411;&#21464;&#24471;&#26356;&#26131;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tweet sentiment extraction extracts the most significant portion of the sentence, determining whether the sentiment is positive or negative. This research aims to identify the part of tweet sentences that strikes any emotion. To reach this objective, we continue improving the Viterbi algorithm previously modified by the author to make it able to receive pre-trained model parameters. We introduce the confidence score and vector as two indicators responsible for evaluating the model internally before assessing the final results. We then present a method to fine-tune this nonparametric model. We found that the model gets highly explainable as the confidence score vector reveals precisely where the least confidence predicted states are and if the modifications approved ameliorate the confidence score or if the tuning is going in the wrong direction.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22522;&#20110;&#22522;&#37329;&#20250;&#27169;&#22411;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#25972;&#20010;&#29983;&#21629;&#21608;&#26399;&#20013;&#25152;&#38754;&#20020;&#30340;&#27835;&#29702;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#21033;&#29992;&#21306;&#22359;&#38142;&#23454;&#29616;&#21435;&#20013;&#24515;&#21270;&#27835;&#29702;&#30340;&#26550;&#26500;&#12290;</title><link>http://arxiv.org/abs/2308.05962</link><description>&lt;p&gt;
&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#22522;&#37329;&#20250;&#27169;&#22411;&#31995;&#32479;&#30340;&#21435;&#20013;&#24515;&#21270;&#27835;&#29702;&#65306;&#25506;&#35752;&#21306;&#22359;&#38142;&#22312;&#36127;&#36131;&#20219;&#30340;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decentralised Governance for Foundation Model based Systems: Exploring the Role of Blockchain in Responsible AI. (arXiv:2308.05962v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05962
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22522;&#20110;&#22522;&#37329;&#20250;&#27169;&#22411;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#25972;&#20010;&#29983;&#21629;&#21608;&#26399;&#20013;&#25152;&#38754;&#20020;&#30340;&#27835;&#29702;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#21033;&#29992;&#21306;&#22359;&#38142;&#23454;&#29616;&#21435;&#20013;&#24515;&#21270;&#27835;&#29702;&#30340;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#37329;&#20250;&#27169;&#22411;&#22240;&#20854;&#21331;&#36234;&#30340;&#33021;&#21147;&#21644;&#28508;&#21147;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#65292;&#33021;&#22815;&#25191;&#34892;&#21508;&#31181;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#20154;&#20204;&#25285;&#24515;&#22522;&#20110;&#22522;&#37329;&#20250;&#27169;&#22411;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#26159;&#21542;&#24471;&#21040;&#20102;&#36866;&#24403;&#30340;&#27835;&#29702;&#65292;&#20197;&#30830;&#20445;&#20854;&#21487;&#20449;&#24230;&#65292;&#24182;&#38450;&#27490;&#21487;&#33021;&#23545;&#20154;&#31867;&#12289;&#31038;&#20250;&#21644;&#29615;&#22659;&#36896;&#25104;&#20260;&#23475;&#30340;&#28389;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#22522;&#37329;&#20250;&#27169;&#22411;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#25972;&#20010;&#29983;&#21629;&#21608;&#26399;&#20013;&#38754;&#20020;&#30340;&#20843;&#20010;&#27835;&#29702;&#25361;&#25112;&#65292;&#28041;&#21450;&#27835;&#29702;&#30340;&#19977;&#20010;&#22522;&#26412;&#32500;&#24230;&#65306;&#20915;&#31574;&#26435;&#12289;&#28608;&#21169;&#26426;&#21046;&#21644;&#38382;&#36131;&#21046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#21306;&#22359;&#38142;&#20316;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#25552;&#20379;&#20998;&#24067;&#24335;&#36134;&#26412;&#26469;&#20419;&#36827;&#21435;&#20013;&#24515;&#21270;&#30340;&#27835;&#29702;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26550;&#26500;&#65292;&#28436;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#21306;&#22359;&#38142;&#23454;&#29616;&#22522;&#37329;&#20250;&#27169;&#22411;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#27835;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation models are increasingly attracting interest worldwide for their distinguished capabilities and potential to perform a wide variety of tasks. Nevertheless, people are concerned about whether foundation model based AI systems are properly governed to ensure trustworthiness of foundation model based AI systems and to prevent misuse that could harm humans, society and the environment. In this paper, we identify eight governance challenges in the entire lifecycle of foundation model based AI systems regarding the three fundamental dimensions of governance: decision rights, incentives, and accountability. Furthermore, we explore the potential of blockchain as a solution to address the challenges by providing a distributed ledger to facilitate decentralised governance. We present an architecture that demonstrates how blockchain can be leveraged to realise governance in foundation model based AI systems.
&lt;/p&gt;</description></item><item><title>BOLAA&#36890;&#36807;&#25552;&#20379;&#23545;LAA&#30340;&#32508;&#21512;&#27604;&#36739;&#65292;&#24182;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#32534;&#25490;&#31574;&#30053;&#65292;&#20026;&#35774;&#35745;LAA&#26550;&#26500;&#21644;&#20248;&#21270;&#20195;&#29702;&#32534;&#25490;&#31574;&#30053;&#25552;&#20379;&#20102;&#37327;&#21270;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2308.05960</link><description>&lt;p&gt;
BOLAA:&#22522;&#20934;&#27979;&#35797;&#21644;&#32534;&#25490;LLM&#22686;&#24378;&#30340;&#33258;&#27835;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
BOLAA: Benchmarking and Orchestrating LLM-augmented Autonomous Agents. (arXiv:2308.05960v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05960
&lt;/p&gt;
&lt;p&gt;
BOLAA&#36890;&#36807;&#25552;&#20379;&#23545;LAA&#30340;&#32508;&#21512;&#27604;&#36739;&#65292;&#24182;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#32534;&#25490;&#31574;&#30053;&#65292;&#20026;&#35774;&#35745;LAA&#26550;&#26500;&#21644;&#20248;&#21270;&#20195;&#29702;&#32534;&#25490;&#31574;&#30053;&#25552;&#20379;&#20102;&#37327;&#21270;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24040;&#22823;&#25104;&#21151;&#40723;&#21169;&#20102;&#23545;LLM&#22686;&#24378;&#33258;&#27835;&#20195;&#29702;&#65288;LAAs&#65289;&#30340;&#26032;&#20852;&#25506;&#32034;&#12290;LAAs&#33021;&#22815;&#20511;&#21161;&#20854;&#26680;&#24515;LLM&#29983;&#25104;&#21160;&#20316;&#24182;&#19982;&#29615;&#22659;&#20132;&#20114;&#65292;&#36890;&#36807;&#23545;&#36807;&#21435;&#30340;&#20132;&#20114;&#65288;&#22914;&#35266;&#23519;&#21644;&#21160;&#20316;&#65289;&#36827;&#34892;&#26465;&#20214;&#35843;&#33410;&#65292;&#20197;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#12290;&#30001;&#20110;&#23545;LAA&#30340;&#30740;&#31350;&#20173;&#28982;&#38750;&#24120;&#26032;&#39062;&#65292;&#21487;&#29992;&#30340;&#25506;&#32034;&#26377;&#38480;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#32508;&#21512;&#27604;&#36739;&#20102;LAA&#22312;&#20195;&#29702;&#26550;&#26500;&#21644;LLM&#32972;&#26223;&#26041;&#38754;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#65292;&#20197;&#25353;&#31867;&#22411;&#23558;&#22810;&#20010;LAAs&#32534;&#25490;&#22312;&#19968;&#36215;&#65292;&#21363;BOLAA&#65292;&#20854;&#20013;&#25511;&#21046;&#22120;&#31649;&#29702;&#22810;&#20010;&#20195;&#29702;&#20043;&#38388;&#30340;&#36890;&#20449;&#12290;&#25105;&#20204;&#22312;&#20915;&#31574;&#21644;&#22810;&#27493;&#25512;&#29702;&#29615;&#22659;&#20013;&#36827;&#34892;&#27169;&#25311;&#65292;&#20840;&#38754;&#39564;&#35777;&#20102;LAAs&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#24615;&#33021;&#32467;&#26524;&#20026;&#35774;&#35745;LAA&#26550;&#26500;&#21644;&#20248;&#21270;&#20195;&#29702;&#32534;&#25490;&#31574;&#30053;&#25552;&#20379;&#20102;&#37327;&#21270;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
The massive successes of large language models (LLMs) encourage the emerging exploration of LLM-augmented Autonomous Agents (LAAs). An LAA is able to generate actions with its core LLM and interact with environments, which facilitates the ability to resolve complex tasks by conditioning on past interactions such as observations and actions. Since the investigation of LAA is still very recent, limited explorations are available. Therefore, we provide a comprehensive comparison of LAA in terms of both agent architectures and LLM backbones. Additionally, we propose a new strategy to orchestrate multiple LAAs such that each labor LAA focuses on one type of action, \textit{i.e.} BOLAA, where a controller manages the communication among multiple agents. We conduct simulations on both decision-making and multi-step reasoning environments, which comprehensively justify the capacity of LAAs. Our performance results provide quantitative suggestions for designing LAA architectures and the optimal
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;FoodSAM&#65292;&#23427;&#26159;&#19968;&#20010;&#38598;&#25104;&#20102;&#31895;&#31961;&#35821;&#20041;&#25513;&#33180;&#21644;SAM&#29983;&#25104;&#25513;&#33180;&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#21487;&#20197;&#25552;&#39640;&#39135;&#29289;&#22270;&#20687;&#20998;&#21106;&#30340;&#36136;&#37327;&#12290;&#21516;&#26102;&#65292;&#23427;&#36824;&#33021;&#36827;&#34892;&#23454;&#20363;&#20998;&#21106;&#12289;&#20840;&#26223;&#20998;&#21106;&#21644;&#21487;&#25552;&#31034;&#20998;&#21106;&#65292;&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#39135;&#29289;&#20998;&#21106;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2308.05938</link><description>&lt;p&gt;
FoodSAM: &#20219;&#20309;&#39135;&#29289;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
FoodSAM: Any Food Segmentation. (arXiv:2308.05938v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05938
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;FoodSAM&#65292;&#23427;&#26159;&#19968;&#20010;&#38598;&#25104;&#20102;&#31895;&#31961;&#35821;&#20041;&#25513;&#33180;&#21644;SAM&#29983;&#25104;&#25513;&#33180;&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#21487;&#20197;&#25552;&#39640;&#39135;&#29289;&#22270;&#20687;&#20998;&#21106;&#30340;&#36136;&#37327;&#12290;&#21516;&#26102;&#65292;&#23427;&#36824;&#33021;&#36827;&#34892;&#23454;&#20363;&#20998;&#21106;&#12289;&#20840;&#26223;&#20998;&#21106;&#21644;&#21487;&#25552;&#31034;&#20998;&#21106;&#65292;&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#39135;&#29289;&#20998;&#21106;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;Segment Anything Model (SAM) &#22312;&#39135;&#29289;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#38646;&#26679;&#26412;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;SAM&#29983;&#25104;&#30340;&#25513;&#33180;&#20013;&#32570;&#20047;&#29305;&#23450;&#31867;&#21035;&#20449;&#24687;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;FoodSAM&#12290;&#36825;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#23558;&#31895;&#31961;&#30340;&#35821;&#20041;&#25513;&#33180;&#19982;SAM&#29983;&#25104;&#30340;&#25513;&#33180;&#32467;&#21512;&#36215;&#26469;&#65292;&#25552;&#39640;&#35821;&#20041;&#20998;&#21106;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24847;&#35782;&#21040;&#39135;&#29289;&#20013;&#30340;&#25104;&#20998;&#21487;&#20197;&#34987;&#35270;&#20026;&#29420;&#31435;&#30340;&#20010;&#20307;&#65292;&#22240;&#27492;&#25105;&#20204;&#23545;&#39135;&#29289;&#22270;&#20687;&#36827;&#34892;&#20102;&#23454;&#20363;&#20998;&#21106;&#12290;&#27492;&#22806;&#65292;FoodSAM&#23558;&#20854;&#38646;&#26679;&#26412;&#33021;&#21147;&#25193;&#23637;&#21040;&#20840;&#26223;&#20998;&#21106;&#65292;&#24182;&#32467;&#21512;&#20102;&#29289;&#20307;&#26816;&#27979;&#22120;&#65292;&#20351;FoodSAM&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#38750;&#39135;&#29289;&#23545;&#35937;&#20449;&#24687;&#12290;&#21463;&#21040;&#25552;&#31034;&#20998;&#21106;&#30340;&#26368;&#36817;&#25104;&#21151;&#21551;&#21457;&#65292;&#25105;&#20204;&#36824;&#23558;FoodSAM&#25193;&#23637;&#21040;&#20102;&#21487;&#25552;&#31034;&#20998;&#21106;&#65292;&#25903;&#25345;&#21508;&#31181;&#25552;&#31034;&#21464;&#20307;&#12290;&#22240;&#27492;&#65292;FoodSAM&#25104;&#20026;&#19968;&#20010;&#20840;&#38754;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#33021;&#22815;&#22312;&#22810;&#20010;&#23618;&#38754;&#19978;&#20998;&#21106;&#39135;&#29289;&#29289;&#21697;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we explore the zero-shot capability of the Segment Anything Model (SAM) for food image segmentation. To address the lack of class-specific information in SAM-generated masks, we propose a novel framework, called FoodSAM. This innovative approach integrates the coarse semantic mask with SAM-generated masks to enhance semantic segmentation quality. Besides, we recognize that the ingredients in food can be supposed as independent individuals, which motivated us to perform instance segmentation on food images. Furthermore, FoodSAM extends its zero-shot capability to encompass panoptic segmentation by incorporating an object detector, which renders FoodSAM to effectively capture non-food object information. Drawing inspiration from the recent success of promptable segmentation, we also extend FoodSAM to promptable segmentation, supporting various prompt variants. Consequently, FoodSAM emerges as an all-encompassing solution capable of segmenting food items at multiple levels 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#26234;&#33021;&#33258;&#21160;&#32553;&#25918;&#26080;&#26381;&#21153;&#22120;&#20989;&#25968;&#30340;&#28145;&#24230;&#24490;&#29615;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#38024;&#23545;&#27874;&#21160;&#30340;&#24037;&#20316;&#36127;&#36733;&#21644;&#20005;&#26684;&#30340;&#24615;&#33021;&#32422;&#26463;&#65292;&#36890;&#36807;&#24314;&#31435;&#19968;&#20010;&#36866;&#24212;&#24615;&#31574;&#30053;&#26469;&#23454;&#29616;&#26368;&#22823;&#21270;&#26399;&#26395;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2308.05937</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#26234;&#33021;&#33258;&#21160;&#32553;&#25918;&#26080;&#26381;&#21153;&#22120;&#20989;&#25968;&#30340;&#28145;&#24230;&#24490;&#29615;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Deep Recurrent-Reinforcement Learning Method for Intelligent AutoScaling of Serverless Functions. (arXiv:2308.05937v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05937
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#26234;&#33021;&#33258;&#21160;&#32553;&#25918;&#26080;&#26381;&#21153;&#22120;&#20989;&#25968;&#30340;&#28145;&#24230;&#24490;&#29615;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#38024;&#23545;&#27874;&#21160;&#30340;&#24037;&#20316;&#36127;&#36733;&#21644;&#20005;&#26684;&#30340;&#24615;&#33021;&#32422;&#26463;&#65292;&#36890;&#36807;&#24314;&#31435;&#19968;&#20010;&#36866;&#24212;&#24615;&#31574;&#30053;&#26469;&#23454;&#29616;&#26368;&#22823;&#21270;&#26399;&#26395;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20989;&#25968;&#21363;&#26381;&#21153;&#65288;FaaS&#65289;&#24341;&#20837;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#22522;&#20110;&#20989;&#25968;&#30340;&#20113;&#25191;&#34892;&#27169;&#22411;&#65292;&#22312;&#29289;&#32852;&#32593;&#36793;&#32536;&#25968;&#25454;&#22788;&#29702;&#21644;&#24322;&#24120;&#26816;&#27979;&#31561;&#24212;&#29992;&#20013;&#20855;&#26377;&#30456;&#20851;&#24615;&#12290;&#34429;&#28982;&#20113;&#26381;&#21153;&#25552;&#20379;&#21830;&#25552;&#20379;&#20102;&#20960;&#20046;&#26080;&#38480;&#30340;&#20989;&#25968;&#24377;&#24615;&#65292;&#20294;&#36825;&#20123;&#24212;&#29992;&#32463;&#24120;&#36935;&#21040;&#27874;&#21160;&#30340;&#24037;&#20316;&#36127;&#36733;&#21644;&#26356;&#20005;&#26684;&#30340;&#24615;&#33021;&#32422;&#26463;&#12290;&#20856;&#22411;&#30340;&#20113;&#26381;&#21153;&#25552;&#20379;&#21830;&#31574;&#30053;&#26159;&#26681;&#25454;&#22522;&#20110;&#30417;&#25511;&#30340;&#38408;&#20540;&#65288;&#22914;CPU&#25110;&#20869;&#23384;&#65289;&#26469;&#32463;&#39564;&#24615;&#22320;&#30830;&#23450;&#21644;&#35843;&#25972;&#25152;&#38656;&#30340;&#20989;&#25968;&#23454;&#20363;&#20197;&#36866;&#24212;&#38656;&#27714;&#21644;&#24615;&#33021;&#65292;&#21363;"&#33258;&#21160;&#32553;&#25918;"&#12290;&#28982;&#32780;&#65292;&#38408;&#20540;&#37197;&#32622;&#35201;&#20040;&#38656;&#35201;&#19987;&#23478;&#30693;&#35782;&#65292;&#35201;&#20040;&#38656;&#35201;&#21382;&#21490;&#25968;&#25454;&#25110;&#23545;&#29615;&#22659;&#30340;&#23436;&#25972;&#35270;&#22270;&#65292;&#20351;&#24471;&#33258;&#21160;&#32553;&#25918;&#25104;&#20026;&#32570;&#20047;&#36866;&#24212;&#24615;&#35299;&#20915;&#26041;&#26696;&#30340;&#24615;&#33021;&#29942;&#39048;&#12290;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#24050;&#34987;&#35777;&#26126;&#22312;&#20998;&#26512;&#22797;&#26434;&#30340;&#20113;&#29615;&#22659;&#20013;&#26159;&#26377;&#30410;&#30340;&#65292;&#24182;&#20135;&#29983;&#36866;&#24212;&#24615;&#31574;&#30053;&#20197;&#26368;&#22823;&#21270;&#26399;&#26395;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Function-as-a-Service (FaaS) introduces a lightweight, function-based cloud execution model that finds its relevance in applications like IoT-edge data processing and anomaly detection. While CSP offer a near-infinite function elasticity, these applications often experience fluctuating workloads and stricter performance constraints. A typical CSP strategy is to empirically determine and adjust desired function instances, "autoscaling", based on monitoring-based thresholds such as CPU or memory, to cope with demand and performance. However, threshold configuration either requires expert knowledge, historical data or a complete view of environment, making autoscaling a performance bottleneck lacking an adaptable solution.RL algorithms are proven to be beneficial in analysing complex cloud environments and result in an adaptable policy that maximizes the expected objectives. Most realistic cloud environments usually involve operational interference and have limited visibility, making them
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#34394;&#25311;&#30340;MOOC&#21161;&#25945; LittleMu&#65292;&#36890;&#36807;&#25972;&#21512;&#24322;&#26500;&#25968;&#25454;&#28304;&#21644;&#25945;&#23398;&#25552;&#31034;&#38142;&#36335;&#26469;&#25903;&#25345;&#24191;&#27867;&#33539;&#22260;&#30340;&#20934;&#30830;&#22238;&#31572;&#21644;&#30693;&#35782;&#30456;&#20851;&#30340;&#38386;&#32842;&#26381;&#21153;&#12290;</title><link>http://arxiv.org/abs/2308.05935</link><description>&lt;p&gt;
LittleMu&#65306;&#36890;&#36807;&#24322;&#26500;&#25968;&#25454;&#28304;&#25972;&#21512;&#21644;&#25945;&#23398;&#25552;&#31034;&#38142;&#36335;&#37096;&#32626;&#22312;&#32447;&#34394;&#25311;&#21161;&#25945;
&lt;/p&gt;
&lt;p&gt;
LittleMu: Deploying an Online Virtual Teaching Assistant via Heterogeneous Sources Integration and Chain of Teach Prompts. (arXiv:2308.05935v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05935
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#34394;&#25311;&#30340;MOOC&#21161;&#25945; LittleMu&#65292;&#36890;&#36807;&#25972;&#21512;&#24322;&#26500;&#25968;&#25454;&#28304;&#21644;&#25945;&#23398;&#25552;&#31034;&#38142;&#36335;&#26469;&#25903;&#25345;&#24191;&#27867;&#33539;&#22260;&#30340;&#20934;&#30830;&#22238;&#31572;&#21644;&#30693;&#35782;&#30456;&#20851;&#30340;&#38386;&#32842;&#26381;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25945;&#32946;&#30340;&#28459;&#38271;&#21382;&#21490;&#20013;&#65292;&#21161;&#25945;&#22312;&#23398;&#20064;&#20013;&#21457;&#25381;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#30495;&#23454;&#22312;&#32447;&#25945;&#32946;&#22330;&#26223;&#30340;&#22797;&#26434;&#24615;&#21644;&#32570;&#20047;&#35757;&#32451;&#25968;&#25454;&#65292;&#24456;&#23569;&#26377;MOOC&#24179;&#21488;&#25552;&#20379;&#20154;&#24037;&#25110;&#34394;&#25311;&#21161;&#25945;&#26469;&#25903;&#25345;&#22823;&#37327;&#22312;&#32447;&#23398;&#29983;&#30340;&#23398;&#20064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#34394;&#25311;&#30340;MOOC&#21161;&#25945;LittleMu&#65292;&#20165;&#20351;&#29992;&#23569;&#37327;&#26631;&#27880;&#35757;&#32451;&#25968;&#25454;&#65292;&#25552;&#20379;&#38382;&#39064;&#22238;&#31572;&#21644;&#38386;&#32842;&#26381;&#21153;&#12290;LittleMu&#30001;&#20004;&#20010;&#20132;&#20114;&#27169;&#22359;&#32452;&#25104;&#65292;&#21253;&#25324;&#24322;&#26500;&#26816;&#32034;&#21644;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#65292;&#39318;&#20808;&#25972;&#21512;&#32467;&#26500;&#21270;&#12289;&#21322;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#30340;&#30693;&#35782;&#28304;&#65292;&#25903;&#25345;&#24191;&#27867;&#33539;&#22260;&#30340;&#38382;&#39064;&#30340;&#20934;&#30830;&#22238;&#31572;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#21517;&#20026;&#8220;Chain of Teach&#8221;&#25552;&#31034;&#30340;&#31934;&#24515;&#31034;&#33539;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#22788;&#29702;&#22797;&#26434;&#30340;&#26410;&#25910;&#38598;&#38382;&#39064;&#12290;&#38500;&#20102;&#38382;&#39064;&#22238;&#31572;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#20854;&#20182;&#25945;&#32946;&#26381;&#21153;&#65292;&#22914;&#30693;&#35782;&#30456;&#20851;&#30340;&#38386;&#32842;&#12290;&#25105;&#20204;&#36890;&#36807;&#26426;&#22120;&#20154;&#27979;&#35797;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Teaching assistants have played essential roles in the long history of education. However, few MOOC platforms are providing human or virtual teaching assistants to support learning for massive online students due to the complexity of real-world online education scenarios and the lack of training data. In this paper, we present a virtual MOOC teaching assistant, LittleMu with minimum labeled training data, to provide question answering and chit-chat services. Consisting of two interactive modules of heterogeneous retrieval and language model prompting, LittleMu first integrates structural, semi- and unstructured knowledge sources to support accurate answers for a wide range of questions. Then, we design delicate demonstrations named "Chain of Teach" prompts to exploit the large-scale pre-trained model to handle complex uncollected questions. Except for question answering, we develop other educational services such as knowledge-grounded chit-chat. We test the system's performance via bot
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22312;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#20013;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#30340;&#24212;&#29992;&#12290;&#19982;&#20854;&#20182;&#30740;&#31350;&#19981;&#21516;&#65292;&#25105;&#20204;&#37325;&#28857;&#20171;&#32461;&#20102;DRL&#26041;&#27861;&#22312;MAPF&#20013;&#30340;&#25972;&#21512;&#65292;&#24182;&#35299;&#20915;&#20102;MAPF&#35299;&#20915;&#26041;&#26696;&#35780;&#20272;&#25351;&#26631;&#32570;&#20047;&#32479;&#19968;&#24615;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#22522;&#20110;&#27169;&#22411;&#30340;DRL&#20316;&#20026;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#65292;&#24182;&#25552;&#20379;&#20102;&#35299;&#20915;MAPF&#24403;&#21069;&#25361;&#25112;&#25152;&#38656;&#30340;&#22522;&#30784;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2308.05893</link><description>&lt;p&gt;
&#23398;&#20064;&#22522;&#20110;&#22242;&#38431;&#23548;&#33322;&#65306;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#22312;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#20013;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Learning to Team-Based Navigation: A Review of Deep Reinforcement Learning Techniques for Multi-Agent Pathfinding. (arXiv:2308.05893v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05893
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22312;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#20013;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#30340;&#24212;&#29992;&#12290;&#19982;&#20854;&#20182;&#30740;&#31350;&#19981;&#21516;&#65292;&#25105;&#20204;&#37325;&#28857;&#20171;&#32461;&#20102;DRL&#26041;&#27861;&#22312;MAPF&#20013;&#30340;&#25972;&#21512;&#65292;&#24182;&#35299;&#20915;&#20102;MAPF&#35299;&#20915;&#26041;&#26696;&#35780;&#20272;&#25351;&#26631;&#32570;&#20047;&#32479;&#19968;&#24615;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#22522;&#20110;&#27169;&#22411;&#30340;DRL&#20316;&#20026;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#65292;&#24182;&#25552;&#20379;&#20102;&#35299;&#20915;MAPF&#24403;&#21069;&#25361;&#25112;&#25152;&#38656;&#30340;&#22522;&#30784;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;(MAPF)&#26159;&#35768;&#22810;&#22823;&#35268;&#27169;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#39046;&#22495;&#65292;&#36890;&#24120;&#26159;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#22522;&#26412;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#22312;&#22797;&#26434;&#21644;&#25317;&#25380;&#30340;&#29615;&#22659;&#20013;&#65292;MAPF&#30340;&#22797;&#26434;&#24615;&#19981;&#26029;&#22686;&#21152;&#65292;&#24050;&#26377;&#35299;&#20915;&#26041;&#26696;&#30340;&#26377;&#25928;&#24615;&#20005;&#37325;&#38477;&#20302;&#12290;&#19982;&#20854;&#20182;&#30740;&#31350;&#19981;&#21516;&#65292;&#25105;&#20204;&#22312;&#26412;&#32508;&#36848;&#35770;&#25991;&#20013;&#37325;&#28857;&#20171;&#32461;&#20102;DRL&#26041;&#27861;&#22312;MAPF&#20013;&#30340;&#24212;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#26088;&#22312;&#22635;&#34917;&#30446;&#21069;&#22312;&#35780;&#20272;MAPF&#35299;&#20915;&#26041;&#26696;&#26041;&#38754;&#30340;&#32570;&#21475;&#65292;&#36890;&#36807;&#35299;&#20915;&#32570;&#20047;&#32479;&#19968;&#35780;&#20272;&#25351;&#26631;&#30340;&#38382;&#39064;&#24182;&#23545;&#36825;&#20123;&#25351;&#26631;&#36827;&#34892;&#20840;&#38754;&#38416;&#37322;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30340;&#35770;&#25991;&#35752;&#35770;&#20102;&#20316;&#20026;&#26410;&#26469;&#26041;&#21521;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;DRL&#30340;&#28508;&#21147;&#65292;&#24182;&#25552;&#20379;&#20102;&#24517;&#35201;&#30340;&#22522;&#30784;&#29702;&#35299;&#20197;&#24212;&#23545;MAPF&#20013;&#30340;&#24403;&#21069;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-agent pathfinding (MAPF) is a critical field in many large-scale robotic applications, often being the fundamental step in multi-agent systems. The increasing complexity of MAPF in complex and crowded environments, however, critically diminishes the effectiveness of existing solutions. In contrast to other studies that have either presented a general overview of the recent advancements in MAPF or extensively reviewed Deep Reinforcement Learning (DRL) within multi-agent system settings independently, our work presented in this review paper focuses on highlighting the integration of DRL-based approaches in MAPF. Moreover, we aim to bridge the current gap in evaluating MAPF solutions by addressing the lack of unified evaluation metrics and providing comprehensive clarification on these metrics. Finally, our paper discusses the potential of model-based DRL as a promising future direction and provides its required foundational understanding to address current challenges in MAPF. Our o
&lt;/p&gt;</description></item><item><title>DF2&#26159;&#19968;&#31181;&#26080;&#20998;&#24067;&#30340;&#20915;&#31574;&#28966;&#28857;&#23398;&#20064;&#26041;&#27861;&#65292;&#29305;&#21035;&#35299;&#20915;&#20102;&#27169;&#22411;&#19981;&#21305;&#37197;&#38169;&#35823;&#12289;&#26679;&#26412;&#24179;&#22343;&#36924;&#36817;&#35823;&#24046;&#21644;&#26799;&#24230;&#36924;&#36817;&#35823;&#24046;&#19977;&#20010;&#29942;&#39048;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.05889</link><description>&lt;p&gt;
DF2: &#26080;&#20998;&#24067;&#30340;&#20915;&#31574;&#28966;&#28857;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DF2: Distribution-Free Decision-Focused Learning. (arXiv:2308.05889v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05889
&lt;/p&gt;
&lt;p&gt;
DF2&#26159;&#19968;&#31181;&#26080;&#20998;&#24067;&#30340;&#20915;&#31574;&#28966;&#28857;&#23398;&#20064;&#26041;&#27861;&#65292;&#29305;&#21035;&#35299;&#20915;&#20102;&#27169;&#22411;&#19981;&#21305;&#37197;&#38169;&#35823;&#12289;&#26679;&#26412;&#24179;&#22343;&#36924;&#36817;&#35823;&#24046;&#21644;&#26799;&#24230;&#36924;&#36817;&#35823;&#24046;&#19977;&#20010;&#29942;&#39048;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20915;&#31574;&#28966;&#28857;&#23398;&#20064;&#65288;DFL&#65289;&#20316;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#26041;&#27861;&#22312;&#35299;&#20915;&#39044;&#27979;-&#20248;&#21270;&#38382;&#39064;&#26102;&#65292;&#36890;&#36807;&#23558;&#39044;&#27979;&#27169;&#22411;&#23450;&#21046;&#21040;&#19968;&#20010;&#19979;&#28216;&#20248;&#21270;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#31471;&#21040;&#31471;DFL&#26041;&#27861;&#21463;&#21040;&#19977;&#20010;&#37325;&#35201;&#29942;&#39048;&#30340;&#21046;&#32422;&#65306;&#27169;&#22411;&#19981;&#21305;&#37197;&#38169;&#35823;&#12289;&#26679;&#26412;&#24179;&#22343;&#36924;&#36817;&#35823;&#24046;&#21644;&#26799;&#24230;&#36924;&#36817;&#35823;&#24046;&#12290;&#27169;&#22411;&#19981;&#21305;&#37197;&#38169;&#35823;&#28304;&#20110;&#27169;&#22411;&#21442;&#25968;&#21270;&#30340;&#39044;&#27979;&#20998;&#24067;&#19982;&#30495;&#23454;&#27010;&#29575;&#20998;&#24067;&#20043;&#38388;&#30340;&#19981;&#21327;&#35843;&#12290;&#26679;&#26412;&#24179;&#22343;&#36924;&#36817;&#35823;&#24046;&#26159;&#20351;&#29992;&#26377;&#38480;&#26679;&#26412;&#26469;&#36817;&#20284;&#26399;&#26395;&#20248;&#21270;&#30446;&#26631;&#26102;&#20135;&#29983;&#30340;&#12290;&#26799;&#24230;&#36924;&#36817;&#35823;&#24046;&#21457;&#29983;&#22312;DFL&#20381;&#38752;KKT&#26465;&#20214;&#36827;&#34892;&#31934;&#30830;&#26799;&#24230;&#35745;&#31639;&#26102;&#65292;&#32780;&#22823;&#22810;&#25968;&#26041;&#27861;&#22312;&#38750;&#20984;&#30446;&#26631;&#20013;&#36817;&#20284;&#26799;&#24230;&#36827;&#34892;&#21453;&#21521;&#20256;&#25773;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;DF2 - &#31532;&#19968;&#20010;&#26126;&#30830;&#35774;&#35745;&#26469;&#35299;&#20915;&#36825;&#19977;&#20010;&#29942;&#39048;&#30340;&#26080;&#20998;&#24067;&#20915;&#31574;&#28966;&#28857;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decision-focused learning (DFL) has recently emerged as a powerful approach for predict-then-optimize problems by customizing a predictive model to a downstream optimization task. However, existing end-to-end DFL methods are hindered by three significant bottlenecks: model mismatch error, sample average approximation error, and gradient approximation error. Model mismatch error stems from the misalignment between the model's parameterized predictive distribution and the true probability distribution. Sample average approximation error arises when using finite samples to approximate the expected optimization objective. Gradient approximation error occurs as DFL relies on the KKT condition for exact gradient computation, while most methods approximate the gradient for backpropagation in non-convex objectives. In this paper, we present DF2 -- the first \textit{distribution-free} decision-focused learning method explicitly designed to address these three bottlenecks. Rather than depending 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; HaX-CoNN &#30340;&#26041;&#26696;&#65292;&#21487;&#20197;&#23454;&#29616;&#22312;&#24322;&#26500; SoC &#19978;&#24182;&#21457;&#25191;&#34892; DNN &#25512;&#29702;&#24037;&#20316;&#36127;&#36733;&#65292;&#24182;&#32771;&#34385;&#20102;&#20849;&#20139;&#20869;&#23384;&#31454;&#20105;&#21644;&#21152;&#36895;&#22120;&#20043;&#38388;&#30340;&#36716;&#25442;&#20197;&#25214;&#21040;&#26368;&#20248;&#30340;&#35843;&#24230;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2308.05869</link><description>&lt;p&gt;
&#38754;&#21521;&#24322;&#26500; SoC &#30340;&#20849;&#20139;&#20869;&#23384;&#31454;&#20105;&#24863;&#30693;&#30340;&#24182;&#21457; DNN &#25191;&#34892;
&lt;/p&gt;
&lt;p&gt;
Shared Memory-contention-aware Concurrent DNN Execution for Diversely Heterogeneous System-on-Chips. (arXiv:2308.05869v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05869
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; HaX-CoNN &#30340;&#26041;&#26696;&#65292;&#21487;&#20197;&#23454;&#29616;&#22312;&#24322;&#26500; SoC &#19978;&#24182;&#21457;&#25191;&#34892; DNN &#25512;&#29702;&#24037;&#20316;&#36127;&#36733;&#65292;&#24182;&#32771;&#34385;&#20102;&#20849;&#20139;&#20869;&#23384;&#31454;&#20105;&#21644;&#21152;&#36895;&#22120;&#20043;&#38388;&#30340;&#36716;&#25442;&#20197;&#25214;&#21040;&#26368;&#20248;&#30340;&#35843;&#24230;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#31227;&#21160;&#21644;&#33258;&#20027;&#31995;&#32479;&#30340;&#20004;&#20010;&#26174;&#33879;&#29305;&#28857;&#26159;&#65306;1&#65289;&#36890;&#24120;&#21516;&#26102;&#36830;&#32493;&#36816;&#34892;&#22810;&#20010;&#24037;&#20316;&#36127;&#36733;&#65292;&#20027;&#35201;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476; (DNN) &#25512;&#29702;&#65307;2&#65289;&#23427;&#20204;&#22312;&#23884;&#20837;&#20102;&#38024;&#23545;&#29305;&#23450;&#25805;&#20316;&#30340;&#24322;&#26500;&#21152;&#36895;&#22120;&#30340;&#20849;&#20139;&#20869;&#23384;&#31995;&#32479;&#33455;&#29255;&#19978;&#36816;&#34892;&#12290;&#30446;&#21069;&#30340;&#25216;&#26415;&#32570;&#20047;&#39640;&#25928;&#30340;&#24615;&#33021;&#21644;&#36164;&#28304;&#31649;&#29702;&#25216;&#26415;&#65292;&#26080;&#27861;&#26368;&#22823;&#21270;&#24635;&#31995;&#32479;&#21534;&#21520;&#37327;&#25110;&#26368;&#23567;&#21270;&#31471;&#21040;&#31471;&#24037;&#20316;&#36127;&#36733;&#24310;&#36831;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; HaX-CoNN &#30340;&#26032;&#26041;&#26696;&#65292;&#23558;&#24182;&#21457;&#25191;&#34892;&#30340; DNN &#25512;&#29702;&#24037;&#20316;&#36127;&#36733;&#30340;&#23618;&#36827;&#34892;&#29305;&#24449;&#21270;&#21644;&#26144;&#23556;&#21040; SoC &#20013;&#30340;&#22810;&#26679;&#21152;&#36895;&#22120;&#12290;&#25105;&#20204;&#30340;&#26041;&#26696;&#29420;&#29305;&#22320;&#32771;&#34385;&#20102;&#27599;&#23618;&#30340;&#25191;&#34892;&#29305;&#24615;&#12289;&#20849;&#20139;&#20869;&#23384; (SM) &#30340;&#31454;&#20105;&#20197;&#21450;&#21152;&#36895;&#22120;&#20043;&#38388;&#30340;&#36716;&#25442;&#65292;&#20197;&#25214;&#21040;&#26368;&#20248;&#30340;&#35843;&#24230;&#26041;&#26696;&#12290;&#25105;&#20204;&#22312; NVIDIA Orin&#12289;NVIDIA Xavier &#21644; Qualcomm Snapdragon 865 SoC &#19978;&#35780;&#20272;&#20102; HaX-CoNN&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;HaX-CoNN &#21487;&#20197;&#26368;&#23567;&#21270;
&lt;/p&gt;
&lt;p&gt;
Two distinguishing features of state-of-the-art mobile and autonomous systems are 1) there are often multiple workloads, mainly deep neural network (DNN) inference, running concurrently and continuously; and 2) they operate on shared memory system-on-chips (SoC) that embed heterogeneous accelerators tailored for specific operations. State-of-the-art lacks efficient performance and resource management techniques necessary to either maximize total system throughput or minimize end-to-end workload latency. In this work, we propose HaX-CoNN, a novel scheme that characterizes and maps layers in concurrently executing DNN inference workloads to a diverse set of accelerators within a SoC. Our scheme uniquely takes per-layer execution characteristics, shared memory (SM) contention, and inter-accelerator transitions into account to find optimal schedules. We evaluate HaX-CoNN on NVIDIA Orin, NVIDIA Xavier, and Qualcomm Snapdragon 865 SoCs. Our experimental results indicate that HaX-CoNN minimiz
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;FLARE22&#25361;&#25112;&#65292;&#26088;&#22312;&#35780;&#20272;&#36895;&#24230;&#24555;&#12289;&#36164;&#28304;&#23569;&#12289;&#20934;&#30830;&#12289;&#33410;&#32422;&#27880;&#37322;&#25104;&#26412;&#19988;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#30340;AI&#31639;&#27861;&#22312;&#33145;&#37096;&#22120;&#23448;&#20998;&#26512;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#36890;&#36807;&#20351;&#29992;50&#20010;&#26631;&#35760;&#25195;&#25551;&#21644;2000&#20010;&#26410;&#26631;&#35760;&#25195;&#25551;&#65292;&#19968;&#32452;AI&#31639;&#27861;&#36798;&#21040;&#20102;90.0\%&#30340;&#20013;&#20301;&#25968;Dice&#30456;&#20284;&#31995;&#25968;&#65288;DSC&#65289;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#27880;&#37322;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2308.05862</link><description>&lt;p&gt;
&#22312;&#27867;&#30284;&#30151;&#33145;&#37096;&#22120;&#23448;&#25968;&#37327;&#21270;&#20013;&#37322;&#25918;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#21147;&#37327;&#65306;FLARE22&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Unleashing the Strengths of Unlabeled Data in Pan-cancer Abdominal Organ Quantification: the FLARE22 Challenge. (arXiv:2308.05862v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05862
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;FLARE22&#25361;&#25112;&#65292;&#26088;&#22312;&#35780;&#20272;&#36895;&#24230;&#24555;&#12289;&#36164;&#28304;&#23569;&#12289;&#20934;&#30830;&#12289;&#33410;&#32422;&#27880;&#37322;&#25104;&#26412;&#19988;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#30340;AI&#31639;&#27861;&#22312;&#33145;&#37096;&#22120;&#23448;&#20998;&#26512;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#36890;&#36807;&#20351;&#29992;50&#20010;&#26631;&#35760;&#25195;&#25551;&#21644;2000&#20010;&#26410;&#26631;&#35760;&#25195;&#25551;&#65292;&#19968;&#32452;AI&#31639;&#27861;&#36798;&#21040;&#20102;90.0\%&#30340;&#20013;&#20301;&#25968;Dice&#30456;&#20284;&#31995;&#25968;&#65288;DSC&#65289;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#27880;&#37322;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23450;&#37327;&#22120;&#23448;&#35780;&#20272;&#26159;&#33258;&#21160;&#21270;&#33145;&#37096;&#30142;&#30149;&#35786;&#26029;&#21644;&#27835;&#30103;&#35745;&#21010;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#24050;&#32463;&#23637;&#29616;&#20986;&#33258;&#21160;&#21270;&#36825;&#19968;&#36807;&#31243;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;AI&#31639;&#27861;&#20381;&#36182;&#20110;&#35768;&#22810;&#19987;&#23478;&#27880;&#37322;&#65292;&#24182;&#19988;&#22312;&#30495;&#23454;&#19990;&#30028;&#22810;&#22269;&#35774;&#32622;&#20013;&#32570;&#20047;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#30340;&#32508;&#21512;&#35780;&#20272;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#32452;&#32455;&#20102;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#35268;&#27169;&#30340;&#33145;&#37096;&#22120;&#23448;&#20998;&#26512;&#25361;&#25112;&#8212;&#8212;FLARE 2022&#25361;&#25112;&#65292;&#26088;&#22312;&#35780;&#20272;&#36895;&#24230;&#24555;&#12289;&#36164;&#28304;&#23569;&#12289;&#20934;&#30830;&#12289;&#33410;&#32422;&#27880;&#37322;&#25104;&#26412;&#19988;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#30340;AI&#31639;&#27861;&#12290;&#25105;&#20204;&#20174;50&#22810;&#20010;&#21307;&#30103;&#22242;&#38431;&#26500;&#24314;&#20102;&#19968;&#20010;&#36328;&#27954;&#38469;&#21644;&#36328;&#22269;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#20855;&#26377;&#19981;&#21516;&#31181;&#26063;&#12289;&#30142;&#30149;&#12289;&#38454;&#27573;&#21644;&#21046;&#36896;&#21830;&#30340;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#12290;&#25105;&#20204;&#29420;&#31435;&#39564;&#35777;&#20102;&#19968;&#32452;AI&#31639;&#27861;&#36890;&#36807;&#20351;&#29992;50&#20010;&#26631;&#35760;&#25195;&#25551;&#21644;2000&#20010;&#26410;&#26631;&#35760;&#25195;&#25551;&#36798;&#21040;&#20102;90.0\%&#30340;&#20013;&#20301;&#25968;Dice&#30456;&#20284;&#31995;&#25968;&#65288;DSC&#65289;&#65292;&#36825;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#27880;&#37322;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantitative organ assessment is an essential step in automated abdominal disease diagnosis and treatment planning. Artificial intelligence (AI) has shown great potential to automatize this process. However, most existing AI algorithms rely on many expert annotations and lack a comprehensive evaluation of accuracy and efficiency in real-world multinational settings. To overcome these limitations, we organized the FLARE 2022 Challenge, the largest abdominal organ analysis challenge to date, to benchmark fast, low-resource, accurate, annotation-efficient, and generalized AI algorithms. We constructed an intercontinental and multinational dataset from more than 50 medical groups, including Computed Tomography (CT) scans with different races, diseases, phases, and manufacturers. We independently validated that a set of AI algorithms achieved a median Dice Similarity Coefficient (DSC) of 90.0\% by using 50 labeled scans and 2000 unlabeled scans, which can significantly reduce annotation req
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#22312;&#26465;&#20214;&#29420;&#31435;&#22270;&#19978;&#36827;&#34892;&#30693;&#35782;&#20256;&#25773;&#30340;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#22312;Cora&#21644;PubMed&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.05857</link><description>&lt;p&gt;
&#22312;&#26465;&#20214;&#29420;&#31435;&#22270;&#19978;&#30340;&#30693;&#35782;&#20256;&#25773;
&lt;/p&gt;
&lt;p&gt;
Knowledge Propagation over Conditional Independence Graphs. (arXiv:2308.05857v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05857
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#22312;&#26465;&#20214;&#29420;&#31435;&#22270;&#19978;&#36827;&#34892;&#30693;&#35782;&#20256;&#25773;&#30340;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#22312;Cora&#21644;PubMed&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26465;&#20214;&#29420;&#31435;&#65288;CI&#65289;&#22270;&#26159;&#19968;&#31181;&#29305;&#27530;&#31867;&#22411;&#30340;&#27010;&#29575;&#22270;&#27169;&#22411;&#65288;PGM&#65289;&#65292;&#20854;&#20013;&#29305;&#24449;&#36830;&#25509;&#20351;&#29992;&#26080;&#21521;&#22270;&#24314;&#27169;&#65292;&#36793;&#26435;&#37325;&#34920;&#31034;&#29305;&#24449;&#20043;&#38388;&#30340;&#37096;&#20998;&#30456;&#20851;&#24615;&#24378;&#24230;&#12290;&#30001;&#20110;CI&#22270;&#25429;&#25417;&#20102;&#29305;&#24449;&#20043;&#38388;&#30340;&#30452;&#25509;&#20381;&#36182;&#20851;&#31995;&#65292;&#23427;&#20204;&#22312;&#30740;&#31350;&#31038;&#21306;&#20013;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#29305;&#21035;&#26159;&#22312;&#21457;&#29616;&#39046;&#22495;&#25299;&#25169;&#26041;&#38754;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;CI&#22270;&#19978;&#25191;&#34892;&#30693;&#35782;&#20256;&#25773;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#25216;&#26415;&#22312;&#20844;&#24320;&#30340;Cora&#21644;PubMed&#25968;&#25454;&#38598;&#19978;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conditional Independence (CI) graph is a special type of a Probabilistic Graphical Model (PGM) where the feature connections are modeled using an undirected graph and the edge weights show the partial correlation strength between the features. Since the CI graphs capture direct dependence between features, they have been garnering increasing interest within the research community for gaining insights into the systems from various domains, in particular discovering the domain topology. In this work, we propose algorithms for performing knowledge propagation over the CI graphs. Our experiments demonstrate that our techniques improve upon the state-of-the-art on the publicly available Cora and PubMed datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39046;&#22495;&#38543;&#26426;&#21270;&#21644;&#30446;&#26631;&#36319;&#36394;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#31181;&#23376;&#26680;&#35745;&#25968;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#21512;&#25104;&#22270;&#20687;&#20316;&#20026;&#35757;&#32451;&#25968;&#25454;&#30340;&#26367;&#20195;&#21697;&#65292;&#35299;&#20915;&#20102;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#26377;&#26631;&#31614;&#35757;&#32451;&#25968;&#25454;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#20302;&#25104;&#26412;&#20272;&#35745;&#35895;&#29289;&#20135;&#37327;&#12290;</title><link>http://arxiv.org/abs/2308.05846</link><description>&lt;p&gt;
&#20351;&#29992;&#39046;&#22495;&#38543;&#26426;&#21270;&#21644;&#30446;&#26631;&#36319;&#36394;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#31181;&#23376;&#26680;&#35745;&#25968;
&lt;/p&gt;
&lt;p&gt;
Seed Kernel Counting using Domain Randomization and Object Tracking Neural Networks. (arXiv:2308.05846v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05846
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39046;&#22495;&#38543;&#26426;&#21270;&#21644;&#30446;&#26631;&#36319;&#36394;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#31181;&#23376;&#26680;&#35745;&#25968;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#21512;&#25104;&#22270;&#20687;&#20316;&#20026;&#35757;&#32451;&#25968;&#25454;&#30340;&#26367;&#20195;&#21697;&#65292;&#35299;&#20915;&#20102;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#26377;&#26631;&#31614;&#35757;&#32451;&#25968;&#25454;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#20302;&#25104;&#26412;&#20272;&#35745;&#35895;&#29289;&#20135;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#36890;&#37327;&#34920;&#22411;&#65288;HTP&#65289;&#23545;&#31181;&#23376;&#30340;&#35780;&#20272;&#26159;&#23545;&#29983;&#38271;&#12289;&#21457;&#32946;&#12289;&#32784;&#21463;&#24615;&#12289;&#25239;&#24615;&#12289;&#29983;&#24577;&#12289;&#20135;&#37327;&#31561;&#22797;&#26434;&#31181;&#23376;&#29305;&#24615;&#30340;&#20840;&#38754;&#35780;&#20272;&#65292;&#20197;&#21450;&#34913;&#37327;&#24418;&#25104;&#26356;&#22797;&#26434;&#29305;&#24615;&#30340;&#21442;&#25968;&#12290;&#31181;&#23376;&#34920;&#22411;&#30340;&#20851;&#38190;&#20043;&#19968;&#26159;&#35895;&#29289;&#20135;&#37327;&#20272;&#35745;&#65292;&#31181;&#23376;&#29983;&#20135;&#34892;&#19994;&#20381;&#36182;&#20110;&#36825;&#19968;&#20272;&#35745;&#26469;&#36827;&#34892;&#19994;&#21153;&#36816;&#20316;&#12290;&#30446;&#21069;&#24066;&#22330;&#19978;&#24050;&#26377;&#26426;&#26800;&#21270;&#30340;&#31181;&#23376;&#26680;&#35745;&#25968;&#22120;&#65292;&#20294;&#20215;&#26684;&#24448;&#24448;&#24456;&#39640;&#65292;&#26377;&#26102;&#36229;&#20986;&#23567;&#35268;&#27169;&#31181;&#23376;&#29983;&#20135;&#20225;&#19994;&#30340;&#25215;&#21463;&#33539;&#22260;&#12290;&#30446;&#26631;&#36319;&#36394;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;(&#22914;YOLO)&#30340;&#21457;&#23637;&#20351;&#35745;&#31639;&#26426;&#31185;&#23398;&#23478;&#33021;&#22815;&#35774;&#35745;&#20986;&#21487;&#20197;&#20302;&#25104;&#26412;&#20272;&#35745;&#35895;&#29289;&#20135;&#37327;&#30340;&#31639;&#27861;&#12290;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#20851;&#38190;&#29942;&#39048;&#26159;&#38656;&#35201;&#22823;&#37327;&#26377;&#26631;&#31614;&#30340;&#35757;&#32451;&#25968;&#25454;&#25165;&#33021;&#25237;&#20837;&#20351;&#29992;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20351;&#29992;&#21512;&#25104;&#22270;&#20687;&#20316;&#20026;&#21487;&#34892;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-throughput phenotyping (HTP) of seeds, also known as seed phenotyping, is the comprehensive assessment of complex seed traits such as growth, development, tolerance, resistance, ecology, yield, and the measurement of parameters that form more complex traits. One of the key aspects of seed phenotyping is cereal yield estimation that the seed production industry relies upon to conduct their business. While mechanized seed kernel counters are available in the market currently, they are often priced high and sometimes outside the range of small scale seed production firms' affordability. The development of object tracking neural network models such as You Only Look Once (YOLO) enables computer scientists to design algorithms that can estimate cereal yield inexpensively. The key bottleneck with neural network models is that they require a plethora of labelled training data before they can be put to task. We demonstrate that the use of synthetic imagery serves as a feasible substitute t
&lt;/p&gt;</description></item><item><title>DiLogics&#26159;&#19968;&#20010;&#36890;&#36807;&#28436;&#31034;&#32534;&#31243;&#30340;&#31995;&#32479;&#65292;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24110;&#21161;&#29992;&#25143;&#21019;&#24314;&#22788;&#29702;&#22810;&#26679;&#21270;&#35268;&#33539;&#30340;Web&#33258;&#21160;&#21270;&#31243;&#24207;&#12290;</title><link>http://arxiv.org/abs/2308.05828</link><description>&lt;p&gt;
DiLogics&#65306;&#21033;&#29992;&#19981;&#21516;&#36923;&#36753;&#21019;&#24314;Web&#33258;&#21160;&#21270;&#31243;&#24207;
&lt;/p&gt;
&lt;p&gt;
DiLogics: Creating Web Automation Programs With Diverse Logics. (arXiv:2308.05828v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05828
&lt;/p&gt;
&lt;p&gt;
DiLogics&#26159;&#19968;&#20010;&#36890;&#36807;&#28436;&#31034;&#32534;&#31243;&#30340;&#31995;&#32479;&#65292;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24110;&#21161;&#29992;&#25143;&#21019;&#24314;&#22788;&#29702;&#22810;&#26679;&#21270;&#35268;&#33539;&#30340;Web&#33258;&#21160;&#21270;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#24037;&#20316;&#32773;&#32463;&#24120;&#36935;&#21040;&#37325;&#22797;&#30340;&#32593;&#32476;&#25968;&#25454;&#36755;&#20837;&#20219;&#21153;&#65292;&#20363;&#22914;&#26356;&#26032;&#35760;&#24405;&#25110;&#19979;&#35746;&#21333;&#12290;&#32593;&#32476;&#33258;&#21160;&#21270;&#21487;&#20197;&#25552;&#39640;&#29983;&#20135;&#21147;&#65292;&#20294;&#20934;&#30830;&#22320;&#23558;&#20219;&#21153;&#36716;&#21270;&#20026;&#32593;&#32476;&#25805;&#20316;&#24182;&#25193;&#23637;&#21040;&#26032;&#30340;&#35268;&#33539;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#29616;&#26377;&#30340;&#24037;&#20855;&#21487;&#20197;&#33258;&#21160;&#21270;&#25191;&#34892;&#30456;&#21516;UI&#25805;&#20316;&#30340;&#20219;&#21153;&#65288;&#20363;&#22914;&#65292;&#25353;&#39034;&#24207;&#22312;&#27599;&#20010;&#23383;&#27573;&#20013;&#36755;&#20837;&#25991;&#26412;&#65289;&#65292;&#20294;&#19981;&#25903;&#25345;&#26681;&#25454;&#19981;&#21516;&#30340;&#36755;&#20837;&#26465;&#20214;&#36827;&#34892;&#19981;&#21516;&#25191;&#34892;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;DiLogics&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#36807;&#28436;&#31034;&#32534;&#31243;&#30340;&#31995;&#32479;&#65292;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24110;&#21161;&#29992;&#25143;&#21019;&#24314;&#22788;&#29702;&#22810;&#26679;&#21270;&#35268;&#33539;&#30340;Web&#33258;&#21160;&#21270;&#31243;&#24207;&#12290;DiLogics&#39318;&#20808;&#23558;&#36755;&#20837;&#25968;&#25454;&#35821;&#20041;&#20998;&#21106;&#20026;&#32467;&#26500;&#21270;&#30340;&#20219;&#21153;&#27493;&#39588;&#12290;&#36890;&#36807;&#20026;&#27599;&#20010;&#27493;&#39588;&#35760;&#24405;&#29992;&#25143;&#28436;&#31034;&#65292;DiLogics&#23558;&#32593;&#32476;&#23439;&#27867;&#21270;&#20026;&#26032;&#39062;&#20294;&#22312;&#35821;&#20041;&#19978;&#30456;&#20284;&#30340;&#20219;&#21153;&#35201;&#27714;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;&#38750;&#19987;&#23478;&#21487;&#20197;&#26377;&#25928;&#20351;&#29992;DiLogics&#21019;&#24314;&#28385;&#36275;&#22810;&#26679;&#21270;&#36755;&#20837;&#25351;&#20196;&#30340;&#33258;&#21160;&#21270;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge workers frequently encounter repetitive web data entry tasks, like updating records or placing orders. Web automation increases productivity, but translating tasks to web actions accurately and extending to new specifications is challenging. Existing tools can automate tasks that perform the same logical trace of UI actions (e.g., input text in each field in order), but do not support tasks requiring different executions based on varied input conditions. We present DiLogics, a programming-by-demonstration system that utilizes NLP to assist users in creating web automation programs that handle diverse specifications. DiLogics first semantically segments input data to structured task steps. By recording user demonstrations for each step, DiLogics generalizes the web macros to novel but semantically similar task requirements. Our evaluation showed that non-experts can effectively use DiLogics to create automation programs that fulfill diverse input instructions. DiLogics provide
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35760;&#24518;&#22686;&#24378;&#31995;&#32479;&#65292;&#23427;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#32534;&#30721;&#35270;&#39057;&#25968;&#25454;&#24182;&#23558;&#20854;&#23384;&#20648;&#22312;&#21521;&#37327;&#25968;&#25454;&#24211;&#20013;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#24378;&#22823;&#21151;&#33021;&#26469;&#36827;&#34892;&#35821;&#35328;&#32534;&#30721;&#30340;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2308.05822</link><description>&lt;p&gt;
&#32534;&#30721;-&#23384;&#20648;-&#26816;&#32034;&#65306;&#36890;&#36807;&#35821;&#35328;&#32534;&#30721;&#30340;&#33258;&#25105;&#20013;&#24515;&#24863;&#30693;&#22686;&#24378;&#35760;&#24518;
&lt;/p&gt;
&lt;p&gt;
Encode-Store-Retrieve: Enhancing Memory Augmentation through Language-Encoded Egocentric Perception. (arXiv:2308.05822v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05822
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35760;&#24518;&#22686;&#24378;&#31995;&#32479;&#65292;&#23427;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#32534;&#30721;&#35270;&#39057;&#25968;&#25454;&#24182;&#23558;&#20854;&#23384;&#20648;&#22312;&#21521;&#37327;&#25968;&#25454;&#24211;&#20013;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#24378;&#22823;&#21151;&#33021;&#26469;&#36827;&#34892;&#35821;&#35328;&#32534;&#30721;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20381;&#36182;&#20110;&#33258;&#24049;&#30340;&#35760;&#24518;&#26469;&#32534;&#30721;&#12289;&#23384;&#20648;&#21644;&#26816;&#32034;&#25105;&#20204;&#30340;&#32463;&#21382;&#12290;&#28982;&#32780;&#65292;&#35760;&#24518;&#38388;&#38548;&#26377;&#26102;&#20250;&#21457;&#29983;&#12290;&#23454;&#29616;&#35760;&#24518;&#22686;&#24378;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#26159;&#36890;&#36807;&#20351;&#29992;&#22686;&#24378;&#29616;&#23454;&#22836;&#25140;&#24335;&#26174;&#31034;&#35774;&#22791;&#26469;&#25429;&#25417;&#21644;&#20445;&#30041;&#33258;&#25105;&#20013;&#24515;&#30340;&#35270;&#39057;&#65292;&#36825;&#31181;&#20570;&#27861;&#36890;&#24120;&#34987;&#31216;&#20026;&#29983;&#27963;&#35760;&#24405;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24403;&#21069;&#25216;&#26415;&#32570;&#20047;&#39640;&#25928;&#32534;&#30721;&#21644;&#23384;&#20648;&#22914;&#27492;&#22823;&#37327;&#30340;&#35270;&#39057;&#25968;&#25454;&#30340;&#33021;&#21147;&#65292;&#20174;&#24222;&#22823;&#30340;&#35270;&#39057;&#23384;&#26723;&#20013;&#26816;&#32034;&#29305;&#23450;&#20449;&#24687;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#33021;&#21147;&#65292;&#36827;&#19968;&#27493;&#22797;&#26434;&#20102;&#24555;&#36895;&#35775;&#38382;&#25152;&#38656;&#20869;&#23481;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
We depend on our own memory to encode, store, and retrieve our experiences. However, memory lapses can occur. One promising avenue for achieving memory augmentation is through the use of augmented reality head-mounted displays to capture and preserve egocentric videos, a practice commonly referred to as life logging. However, a significant challenge arises from the sheer volume of video data generated through life logging, as the current technology lacks the capability to encode and store such large amounts of data efficiently. Further, retrieving specific information from extensive video archives requires substantial computational power, further complicating the task of quickly accessing desired content. To address these challenges, we propose a memory augmentation system that involves leveraging natural language encoding for video data and storing them in a vector database. This approach harnesses the power of large vision language models to perform the language encoding process. Add
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35843;&#26597;&#20102;&#21360;&#24230;&#22810;&#35821;&#35328;&#23383;&#20307;&#30340;&#20809;&#23398;&#33050;&#26412;&#35782;&#21035;&#25216;&#26415;&#30340;&#36827;&#23637;&#65292;&#37325;&#28857;&#35752;&#35770;&#20102;&#33050;&#26412;&#39044;&#22788;&#29702;&#21644;&#25991;&#26412;&#35782;&#21035;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#23545;&#20110;&#21360;&#24230;&#23383;&#20307;&#29305;&#24449;&#30340;&#25361;&#25112;&#21644;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2308.05780</link><description>&lt;p&gt;
&#20855;&#26377;&#22810;&#35821;&#35328;&#21360;&#24230;&#23383;&#20307;&#30340;&#20809;&#23398;&#33050;&#26412;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Optical Script Identification for multi-lingual Indic-script. (arXiv:2308.05780v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35843;&#26597;&#20102;&#21360;&#24230;&#22810;&#35821;&#35328;&#23383;&#20307;&#30340;&#20809;&#23398;&#33050;&#26412;&#35782;&#21035;&#25216;&#26415;&#30340;&#36827;&#23637;&#65292;&#37325;&#28857;&#35752;&#35770;&#20102;&#33050;&#26412;&#39044;&#22788;&#29702;&#21644;&#25991;&#26412;&#35782;&#21035;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#23545;&#20110;&#21360;&#24230;&#23383;&#20307;&#29305;&#24449;&#30340;&#25361;&#25112;&#21644;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33050;&#26412;&#35782;&#21035;&#21644;&#25991;&#26412;&#35782;&#21035;&#26159;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20013;&#30340;&#37325;&#35201;&#39046;&#22495;&#12290;&#22312;&#25968;&#23383;&#21270;&#26102;&#20195;&#65292;&#20351;&#29992;&#25968;&#23383;&#35760;&#31508;&#35760;&#24050;&#32463;&#25104;&#20026;&#24120;&#35265;&#30340;&#20570;&#27861;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#32440;&#31508;&#26041;&#27861;&#20173;&#28982;&#26159;&#19968;&#31181;&#20027;&#35201;&#30340;&#20070;&#20889;&#26041;&#24335;&#12290;&#36825;&#23548;&#33268;&#33050;&#26412;&#34987;&#22522;&#20110;&#33719;&#21462;&#26041;&#24335;&#36827;&#34892;&#20998;&#31867;&#12290;&#23545;&#24403;&#21069;&#22788;&#29702;&#21644;&#35782;&#21035;&#26041;&#27861;&#30340;&#35843;&#26597;&#23558;&#23545;&#30740;&#31350;&#20154;&#21592;&#26377;&#30410;&#12290;&#26412;&#25991;&#26088;&#22312;&#35752;&#35770;&#33050;&#26412;&#39044;&#22788;&#29702;&#21644;&#25991;&#26412;&#35782;&#21035;&#25216;&#26415;&#30340;&#36827;&#23637;&#12290;&#22312;&#21360;&#24230;&#65292;&#26377;&#21313;&#20108;&#31181;&#26480;&#20986;&#30340;&#21360;&#24230;&#23383;&#20307;&#65292;&#19982;&#33521;&#35821;&#19981;&#21516;&#65292;&#36825;&#20123;&#23383;&#20307;&#20855;&#26377;&#22810;&#23618;&#27425;&#30340;&#29305;&#24449;&#12290;&#35832;&#22914;&#25991;&#26412;&#24418;&#29366;&#30456;&#20284;&#24615;&#31561;&#22797;&#26434;&#29305;&#24449;&#20351;&#24471;&#23427;&#20204;&#38590;&#20197;&#35782;&#21035;&#21644;&#20998;&#26512;&#65292;&#22240;&#27492;&#38656;&#35201;&#20808;&#36827;&#30340;&#39044;&#22788;&#29702;&#26041;&#27861;&#26469;&#36827;&#34892;&#20934;&#30830;&#35782;&#21035;&#12290;&#26412;&#35843;&#26597;&#35797;&#22270;&#25552;&#20379;&#19968;&#20221;&#20840;&#38754;&#30340;&#27010;&#36848;&#65292;&#20171;&#32461;&#20102;&#24403;&#21069;&#30340;&#30740;&#31350;&#21644;&#26041;&#27861;&#65292;&#24182;&#23637;&#26395;&#26410;&#26469;&#30340;&#25361;&#25112;&#21644;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Script identification and text recognition are some of the major domains in the application of Artificial Intelligence. In this era of digitalization, the use of digital note-taking has become a common practice. Still, conventional methods of using pen and paper is a prominent way of writing. This leads to the classification of scripts based on the method they are obtained. A survey on the current methodologies and state-of-art methods used for processing and identification would prove beneficial for researchers. The aim of this article is to discuss the advancement in the techniques for script pre-processing and text recognition. In India there are twelve prominent Indic scripts, unlike the English language, these scripts have layers of characteristics. Complex characteristics such as similarity in text shape make them difficult to recognize and analyze, thus this requires advance preprocessing methods for their accurate recognition. A sincere attempt is made in this survey to provide
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#25968;&#25454;&#39044;&#22788;&#29702;&#25216;&#26415;&#21644;Extra-Tree&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#19982;Random Forest&#20998;&#31867;&#22120;&#30456;&#32467;&#21512;&#65292;&#26412;&#30740;&#31350;&#25552;&#39640;&#20102;&#24515;&#21147;&#34928;&#31469;&#24739;&#32773;&#29983;&#23384;&#39044;&#27979;&#33021;&#21147;&#65292;&#24182;&#21462;&#24471;&#20102;98.33%&#30340;&#20934;&#30830;&#29575;</title><link>http://arxiv.org/abs/2308.05765</link><description>&lt;p&gt;
&#21457;&#25381;&#29305;&#24449;&#36873;&#25321;&#21644;&#38543;&#26426;&#26862;&#26519;&#20998;&#31867;&#22120;&#30340;&#20316;&#29992;&#65292;&#25552;&#39640;&#24515;&#21147;&#34928;&#31469;&#24739;&#32773;&#29983;&#23384;&#39044;&#27979;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Unleashing the Power of Extra-Tree Feature Selection and Random Forest Classifier for Improved Survival Prediction in Heart Failure Patients. (arXiv:2308.05765v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05765
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#25968;&#25454;&#39044;&#22788;&#29702;&#25216;&#26415;&#21644;Extra-Tree&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#19982;Random Forest&#20998;&#31867;&#22120;&#30456;&#32467;&#21512;&#65292;&#26412;&#30740;&#31350;&#25552;&#39640;&#20102;&#24515;&#21147;&#34928;&#31469;&#24739;&#32773;&#29983;&#23384;&#39044;&#27979;&#33021;&#21147;&#65292;&#24182;&#21462;&#24471;&#20102;98.33%&#30340;&#20934;&#30830;&#29575;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#21147;&#34928;&#31469;&#26159;&#19968;&#31181;&#21361;&#21450;&#29983;&#21629;&#30340;&#30142;&#30149;&#65292;&#20840;&#29699;&#25968;&#30334;&#19975;&#20154;&#21463;&#21040;&#24433;&#21709;&#12290;&#20934;&#30830;&#39044;&#27979;&#24739;&#32773;&#30340;&#29983;&#23384;&#33021;&#21147;&#21487;&#20197;&#24110;&#21161;&#21450;&#26089;&#24178;&#39044;&#65292;&#24182;&#25913;&#21892;&#24739;&#32773;&#39044;&#21518;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#21033;&#29992;&#25968;&#25454;&#39044;&#22788;&#29702;&#25216;&#26415;&#21644;Extra-Tree (ET) &#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#19982;Random Forest (RF) &#20998;&#31867;&#22120;&#30456;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;&#24515;&#21147;&#34928;&#31469;&#24739;&#32773;&#30340;&#29983;&#23384;&#39044;&#27979;&#33021;&#21147;&#30340;&#28508;&#21147;&#12290;&#36890;&#36807;&#21457;&#25381;ET&#29305;&#24449;&#36873;&#25321;&#30340;&#20248;&#21183;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#35782;&#21035;&#19982;&#24515;&#21147;&#34928;&#31469;&#29983;&#23384;&#30456;&#20851;&#30340;&#26368;&#37325;&#35201;&#30340;&#39044;&#27979;&#22240;&#23376;&#12290;&#21033;&#29992;&#20844;&#24320;&#30340;UCL&#24515;&#21147;&#34928;&#31469; (HF) &#29983;&#23384;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#37319;&#29992;ET&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;&#26469;&#35782;&#21035;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#23558;&#36825;&#20123;&#29305;&#24449;&#29992;&#20316;RF&#30340;&#32593;&#26684;&#25628;&#32034;&#30340;&#36755;&#20837;&#12290;&#26368;&#21518;&#65292;&#20351;&#29992;&#19981;&#21516;&#30340;&#25351;&#26631;&#23545;&#35843;&#20248;&#21518;&#30340;RF&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#21644;&#35780;&#20272;&#12290;&#25152;&#37319;&#29992;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;98.33%&#30340;&#20934;&#30830;&#29575;&#65292;&#26159;&#30446;&#21069;&#30340;&#26368;&#39640;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Heart failure is a life-threatening condition that affects millions of people worldwide. The ability to accurately predict patient survival can aid in early intervention and improve patient outcomes. In this study, we explore the potential of utilizing data pre-processing techniques and the Extra-Tree (ET) feature selection method in conjunction with the Random Forest (RF) classifier to improve survival prediction in heart failure patients. By leveraging the strengths of ET feature selection, we aim to identify the most significant predictors associated with heart failure survival. Using the public UCL Heart failure (HF) survival dataset, we employ the ET feature selection algorithm to identify the most informative features. These features are then used as input for grid search of RF. Finally, the tuned RF Model was trained and evaluated using different matrices. The approach was achieved 98.33% accuracy that is the highest over the exiting work.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20174;&#24515;&#33039;MRI&#20013;&#30340;&#30693;&#35782;&#36716;&#31227;&#35299;&#38145;&#24515;&#30005;&#22270;&#30340;&#35786;&#26029;&#28508;&#21147;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;CMR&#22270;&#20687;&#20013;&#30340;&#39046;&#22495;&#29305;&#23450;&#20449;&#24687;&#36716;&#31227;&#21040;ECG&#23884;&#20837;&#20013;&#65292;&#35813;&#26041;&#27861;&#23454;&#29616;&#20102;&#20165;&#26681;&#25454;ECG&#25968;&#25454;&#36827;&#34892;&#20840;&#38754;&#30340;&#24515;&#33039;&#31579;&#26597;&#65292;&#24182;&#33021;&#39044;&#27979;&#24515;&#34880;&#31649;&#30142;&#30149;&#30340;&#20010;&#20307;&#39118;&#38505;&#21644;&#30830;&#23450;&#24515;&#33039;&#34920;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.05764</link><description>&lt;p&gt;
&#36890;&#36807;&#20174;&#24515;&#33039;MRI&#20013;&#30340;&#30693;&#35782;&#36716;&#31227;&#35299;&#38145;&#24515;&#30005;&#22270;&#30340;&#35786;&#26029;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Unlocking the Diagnostic Potential of ECG through Knowledge Transfer from Cardiac MRI. (arXiv:2308.05764v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05764
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20174;&#24515;&#33039;MRI&#20013;&#30340;&#30693;&#35782;&#36716;&#31227;&#35299;&#38145;&#24515;&#30005;&#22270;&#30340;&#35786;&#26029;&#28508;&#21147;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;CMR&#22270;&#20687;&#20013;&#30340;&#39046;&#22495;&#29305;&#23450;&#20449;&#24687;&#36716;&#31227;&#21040;ECG&#23884;&#20837;&#20013;&#65292;&#35813;&#26041;&#27861;&#23454;&#29616;&#20102;&#20165;&#26681;&#25454;ECG&#25968;&#25454;&#36827;&#34892;&#20840;&#38754;&#30340;&#24515;&#33039;&#31579;&#26597;&#65292;&#24182;&#33021;&#39044;&#27979;&#24515;&#34880;&#31649;&#30142;&#30149;&#30340;&#20010;&#20307;&#39118;&#38505;&#21644;&#30830;&#23450;&#24515;&#33039;&#34920;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#30005;&#22270; (ECG) &#26159;&#19968;&#31181;&#24191;&#27867;&#21487;&#29992;&#30340;&#35786;&#26029;&#24037;&#20855;&#65292;&#21487;&#20197;&#24555;&#36895;&#21644;&#32463;&#27982;&#39640;&#25928;&#22320;&#35780;&#20272;&#24515;&#34880;&#31649;&#20581;&#24247;&#29366;&#20917;&#12290;&#28982;&#32780;&#65292;&#22312;&#24515;&#34880;&#31649;&#30142;&#30149;&#30340;&#35786;&#26029;&#20013;&#65292;&#36890;&#24120;&#26356;&#21916;&#27426;&#20351;&#29992;&#26114;&#36149;&#30340;&#24515;&#33039;&#30913;&#20849;&#25391; (CMR) &#25104;&#20687;&#36827;&#34892;&#26356;&#35814;&#32454;&#30340;&#26816;&#26597;&#12290;&#34429;&#28982; CMR &#25104;&#20687;&#21487;&#20197;&#25552;&#20379;&#35814;&#32454;&#30340;&#24515;&#33039;&#35299;&#21078;&#21487;&#35270;&#21270;&#65292;&#20294;&#30001;&#20110;&#38271;&#26102;&#38388;&#25195;&#25551;&#21644;&#39640;&#26114;&#30340;&#36153;&#29992;&#65292;&#23427;&#24182;&#19981;&#24191;&#27867;&#21487;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31532;&#19968;&#31181;&#33258;&#30417;&#30563;&#23545;&#27604;&#26041;&#27861;&#65292;&#23558;CMR&#22270;&#20687;&#20013;&#30340;&#39046;&#22495;&#29305;&#23450;&#20449;&#24687;&#36716;&#31227;&#21040;ECG&#23884;&#20837;&#20013;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#22810;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#19982;&#23631;&#34109;&#25968;&#25454;&#24314;&#27169;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#20165;&#26681;&#25454;ECG&#25968;&#25454;&#36827;&#34892;&#20840;&#38754;&#30340;&#24515;&#33039;&#31579;&#26597;&#12290;&#22312;&#20351;&#29992;&#26469;&#33258;40044&#21517;UK Biobank&#21463;&#35797;&#32773;&#30340;&#25968;&#25454;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#21644;&#21487;&#25512;&#24191;&#24615;&#12290;&#25105;&#20204;&#39044;&#27979;&#20102;&#21508;&#31181;&#24515;&#34880;&#31649;&#30142;&#30149;&#30340;&#20010;&#20307;&#39118;&#38505;&#65292;&#24182;&#20165;&#26681;&#25454;ECG&#25968;&#25454;&#30830;&#23450;&#20102;&#19981;&#21516;&#30340;&#24515;&#33039;&#34920;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The electrocardiogram (ECG) is a widely available diagnostic tool that allows for a cost-effective and fast assessment of the cardiovascular health. However, more detailed examination with expensive cardiac magnetic resonance (CMR) imaging is often preferred for the diagnosis of cardiovascular diseases. While providing detailed visualization of the cardiac anatomy, CMR imaging is not widely available due to long scan times and high costs. To address this issue, we propose the first self-supervised contrastive approach that transfers domain-specific information from CMR images to ECG embeddings. Our approach combines multimodal contrastive learning with masked data modeling to enable holistic cardiac screening solely from ECG data. In extensive experiments using data from 40,044 UK Biobank subjects, we demonstrate the utility and generalizability of our method. We predict the subject-specific risk of various cardiovascular diseases and determine distinct cardiac phenotypes solely from E
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#30561;&#30496;-&#28165;&#37266;&#20998;&#31867;&#27169;&#22411;&#65292;&#21033;&#29992;&#20809;&#30005;&#27969;&#34880;&#23481;&#31215;&#27874;&#24418;&#21644;&#27963;&#21160;&#20449;&#21495;&#25552;&#21462;&#30340;&#29305;&#24449;&#65292;&#21487;&#20197;&#35780;&#20272;&#30561;&#30496;&#36136;&#37327;&#65292;&#35782;&#21035;&#30561;&#30496;&#38382;&#39064;&#21644;&#25913;&#21892;&#25972;&#20307;&#20581;&#24247;&#12290;</title><link>http://arxiv.org/abs/2308.05759</link><description>&lt;p&gt;
&#19968;&#31181;&#21033;&#29992;&#20174;&#20809;&#30005;&#27969;&#34880;&#23481;&#31215;&#27874;&#24418;&#21644;&#27963;&#21160;&#20449;&#21495;&#20013;&#25552;&#21462;&#30340;&#23569;&#37327;&#29305;&#24449;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#30561;&#30496;-&#28165;&#37266;&#20998;&#31867;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A machine-learning sleep-wake classification model using a reduced number of features derived from photoplethysmography and activity signals. (arXiv:2308.05759v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05759
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#30561;&#30496;-&#28165;&#37266;&#20998;&#31867;&#27169;&#22411;&#65292;&#21033;&#29992;&#20809;&#30005;&#27969;&#34880;&#23481;&#31215;&#27874;&#24418;&#21644;&#27963;&#21160;&#20449;&#21495;&#25552;&#21462;&#30340;&#29305;&#24449;&#65292;&#21487;&#20197;&#35780;&#20272;&#30561;&#30496;&#36136;&#37327;&#65292;&#35782;&#21035;&#30561;&#30496;&#38382;&#39064;&#21644;&#25913;&#21892;&#25972;&#20307;&#20581;&#24247;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30561;&#30496;&#23545;&#25105;&#20204;&#30340;&#25972;&#20307;&#20581;&#24247;&#21644;&#24184;&#31119;&#24863;&#33267;&#20851;&#37325;&#35201;&#12290;&#23427;&#22312;&#35843;&#33410;&#25105;&#20204;&#30340;&#24515;&#29702;&#21644;&#36523;&#20307;&#20581;&#24247;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#23545;&#25105;&#20204;&#30340;&#24773;&#32490;&#12289;&#35760;&#24518;&#21644;&#35748;&#30693;&#21151;&#33021;&#20197;&#21450;&#36523;&#20307;&#30340;&#38887;&#24615;&#21644;&#20813;&#30123;&#31995;&#32479;&#37117;&#26377;&#24433;&#21709;&#12290;&#30561;&#30496;&#38454;&#27573;&#30340;&#20998;&#31867;&#26159;&#35780;&#20272;&#30561;&#30496;&#36136;&#37327;&#30340;&#24517;&#35201;&#27493;&#39588;&#65292;&#25552;&#20379;&#20102;&#20272;&#35745;&#30561;&#30496;&#36136;&#37327;&#21644;&#25105;&#20204;&#30340;&#36523;&#20307;&#22312;&#36825;&#20010;&#37325;&#35201;&#30340;&#20241;&#24687;&#26102;&#26399;&#20869;&#21151;&#33021;&#22914;&#20309;&#30340;&#25351;&#26631;&#12290;&#20809;&#30005;&#27969;&#34880;&#23481;&#31215;&#27874;&#24418;&#65288;PPG&#65289;&#24050;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#30561;&#30496;&#38454;&#27573;&#25512;&#26029;&#20449;&#21495;&#65292;&#24847;&#21619;&#30528;&#23427;&#21487;&#20197;&#21333;&#29420;&#20351;&#29992;&#25110;&#19982;&#20854;&#20182;&#20449;&#21495;&#32467;&#21512;&#20351;&#29992;&#26469;&#30830;&#23450;&#30561;&#30496;&#38454;&#27573;&#12290;&#36825;&#20123;&#20449;&#24687;&#23545;&#20110;&#35782;&#21035;&#28508;&#22312;&#30340;&#30561;&#30496;&#38382;&#39064;&#21644;&#21046;&#23450;&#25913;&#21892;&#30561;&#30496;&#36136;&#37327;&#21644;&#25972;&#20307;&#20581;&#24247;&#31574;&#30053;&#38750;&#24120;&#26377;&#20215;&#20540;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;eXtreme Gradient Boosting&#65288;XGBoost&#65289;&#31639;&#27861;&#21644;&#20174;PPG&#20449;&#21495;&#21644;&#27963;&#21160;&#35745;&#25968;&#20013;&#25552;&#21462;&#30340;&#29305;&#24449;&#30340;&#26426;&#22120;&#23398;&#20064;&#30561;&#30496;-&#28165;&#37266;&#20998;&#31867;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sleep is a crucial aspect of our overall health and well-being. It plays a vital role in regulating our mental and physical health, impacting our mood, memory, and cognitive function to our physical resilience and immune system. The classification of sleep stages is a mandatory step to assess sleep quality, providing the metrics to estimate the quality of sleep and how well our body is functioning during this essential period of rest. Photoplethysmography (PPG) has been demonstrated to be an effective signal for sleep stage inference, meaning it can be used on its own or in a combination with others signals to determine sleep stage. This information is valuable in identifying potential sleep issues and developing strategies to improve sleep quality and overall health. In this work, we present a machine learning sleep-wake classification model based on the eXtreme Gradient Boosting (XGBoost) algorithm and features extracted from PPG signal and activity counts. The performance of our met
&lt;/p&gt;</description></item><item><title>LLM&#21464;&#25104;DBA&#65292;&#25552;&#20379;&#25968;&#25454;&#24211;&#32500;&#25252;&#30340;&#35786;&#26029;&#21644;&#20248;&#21270;&#24314;&#35758;&#65292;&#36890;&#36807;&#20174;&#25991;&#26412;&#26469;&#28304;&#20013;&#33719;&#21462;&#32463;&#39564;&#21644;&#22810;&#20010;LLMs&#30340;&#21327;&#20316;&#35786;&#26029;&#12290;</title><link>http://arxiv.org/abs/2308.05481</link><description>&lt;p&gt;
LLM&#21464;&#25104;DBA
&lt;/p&gt;
&lt;p&gt;
LLM As DBA. (arXiv:2308.05481v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05481
&lt;/p&gt;
&lt;p&gt;
LLM&#21464;&#25104;DBA&#65292;&#25552;&#20379;&#25968;&#25454;&#24211;&#32500;&#25252;&#30340;&#35786;&#26029;&#21644;&#20248;&#21270;&#24314;&#35758;&#65292;&#36890;&#36807;&#20174;&#25991;&#26412;&#26469;&#28304;&#20013;&#33719;&#21462;&#32463;&#39564;&#21644;&#22810;&#20010;LLMs&#30340;&#21327;&#20316;&#35786;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#24211;&#31649;&#29702;&#21592;&#65288;DBA&#65289;&#22312;&#31649;&#29702;&#12289;&#32500;&#25252;&#21644;&#20248;&#21270;&#25968;&#25454;&#24211;&#31995;&#32479;&#20197;&#30830;&#20445;&#25968;&#25454;&#21487;&#29992;&#24615;&#12289;&#24615;&#33021;&#21644;&#21487;&#38752;&#24615;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;DBA&#26469;&#35828;&#65292;&#31649;&#29702;&#22823;&#37327;&#25968;&#25454;&#24211;&#23454;&#20363;&#65288;&#20363;&#22914;&#65292;&#20113;&#25968;&#25454;&#24211;&#19978;&#30340;&#25968;&#30334;&#19975;&#20010;&#23454;&#20363;&#65289;&#26159;&#22256;&#38590;&#21644;&#32321;&#29712;&#30340;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#26174;&#31034;&#20986;&#20102;&#29702;&#35299;&#26377;&#20215;&#20540;&#25991;&#20214;&#24182;&#29983;&#25104;&#21512;&#29702;&#31572;&#26696;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;D-Bot&#65292;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#25968;&#25454;&#24211;&#31649;&#29702;&#21592;&#65292;&#23427;&#21487;&#20197;&#25345;&#32493;&#20174;&#25991;&#26412;&#26469;&#28304;&#20013;&#33719;&#21462;&#25968;&#25454;&#24211;&#32500;&#25252;&#32463;&#39564;&#65292;&#24182;&#20026;&#30446;&#26631;&#25968;&#25454;&#24211;&#25552;&#20379;&#21512;&#29702;&#12289;&#26377;&#29702;&#12289;&#21450;&#26102;&#30340;&#35786;&#26029;&#21644;&#20248;&#21270;&#24314;&#35758;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#38761;&#21629;&#24615;&#30340;&#20197;LLM&#20026;&#20013;&#24515;&#30340;&#25968;&#25454;&#24211;&#32500;&#25252;&#26694;&#26550;&#65292;&#21253;&#25324;&#65288;i&#65289;&#20174;&#25991;&#26723;&#21644;&#24037;&#20855;&#20013;&#26816;&#27979;&#25968;&#25454;&#24211;&#32500;&#25252;&#30693;&#35782;&#65292;&#65288;ii&#65289;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#30340;&#24605;&#32500;&#26641;&#65292;&#21644;&#65288;iii&#65289;&#22810;&#20010;LLM&#20043;&#38388;&#30340;&#21327;&#20316;&#35786;&#26029;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#21021;&#27493;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Database administrators (DBAs) play a crucial role in managing, maintaining and optimizing a database system to ensure data availability, performance, and reliability. However, it is hard and tedious for DBAs to manage a large number of database instances (e.g., millions of instances on the cloud databases). Recently large language models (LLMs) have shown great potential to understand valuable documents and accordingly generate reasonable answers. Thus, we propose D-Bot, a LLM-based database administrator that can continuously acquire database maintenance experience from textual sources, and provide reasonable, well-founded, in-time diagnosis and optimization advice for target databases. This paper presents a revolutionary LLM-centric framework for database maintenance, including (i) database maintenance knowledge detection from documents and tools, (ii) tree of thought reasoning for root cause analysis, and (iii) collaborative diagnosis among multiple LLMs. Our preliminary experiment
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HoLe&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#22270;&#32467;&#26500;&#20013;&#22686;&#24378;&#21516;&#31867;&#24615;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#22270;&#32858;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.05309</link><description>&lt;p&gt;
&#22270;&#32858;&#31867;&#30340;&#21516;&#31867;&#24615;&#22686;&#24378;&#32467;&#26500;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Homophily-enhanced Structure Learning for Graph Clustering. (arXiv:2308.05309v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05309
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HoLe&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#22270;&#32467;&#26500;&#20013;&#22686;&#24378;&#21516;&#31867;&#24615;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#22270;&#32858;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#32858;&#31867;&#26159;&#22270;&#20998;&#26512;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#20219;&#21153;&#65292;&#22312;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26041;&#38754;&#30340;&#26368;&#26032;&#36827;&#23637;&#24050;&#32463;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25104;&#26524;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#22522;&#20110;GNN&#30340;&#22270;&#32858;&#31867;&#26041;&#27861;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#24573;&#35270;&#20102;&#22270;&#32467;&#26500;&#30340;&#36136;&#37327;&#65292;&#36825;&#26159;&#30001;&#20110;&#29616;&#23454;&#19990;&#30028;&#22270;&#30340;&#31232;&#30095;&#24615;&#21644;&#22810;&#26679;&#24615;&#25152;&#22266;&#26377;&#30340;&#65292;&#20174;&#32780;&#23548;&#33268;&#20102;&#27425;&#20248;&#30340;&#24615;&#33021;&#12290;&#22270;&#32467;&#26500;&#23398;&#20064;&#21487;&#20197;&#36890;&#36807;&#28155;&#21152;&#32570;&#22833;&#30340;&#36830;&#25509;&#21644;&#21024;&#38500;&#38169;&#35823;&#30340;&#36830;&#25509;&#26469;&#20248;&#21270;&#36755;&#20837;&#22270;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#22270;&#32467;&#26500;&#23398;&#20064;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#26377;&#30417;&#30563;&#30340;&#35774;&#32622;&#19978;&#65292;&#24182;&#19988;&#30001;&#20110;&#32570;&#20047;&#30495;&#23454;&#26631;&#31614;&#65292;&#19981;&#33021;&#30452;&#25509;&#24212;&#29992;&#20110;&#25105;&#20204;&#30340;&#29305;&#23450;&#32858;&#31867;&#20219;&#21153;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#20010;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#21516;&#31867;&#24615;&#22686;&#24378;&#32467;&#26500;&#23398;&#20064;&#22270;&#32858;&#31867;&#65288;HoLe&#65289;&#12290;&#25105;&#20204;&#30340;&#21160;&#26426;&#28304;&#20110;&#35266;&#23519;&#21040;&#65292;&#24494;&#22937;&#22320;&#22686;&#24378;&#22270;&#32467;&#26500;&#20013;&#30340;&#21516;&#31867;&#24615;&#31243;&#24230;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;GNNs&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph clustering is a fundamental task in graph analysis, and recent advances in utilizing graph neural networks (GNNs) have shown impressive results. Despite the success of existing GNN-based graph clustering methods, they often overlook the quality of graph structure, which is inherent in real-world graphs due to their sparse and multifarious nature, leading to subpar performance. Graph structure learning allows refining the input graph by adding missing links and removing spurious connections. However, previous endeavors in graph structure learning have predominantly centered around supervised settings, and cannot be directly applied to our specific clustering tasks due to the absence of ground-truth labels. To bridge the gap, we propose a novel method called \textbf{ho}mophily-enhanced structure \textbf{le}arning for graph clustering (HoLe). Our motivation stems from the observation that subtly enhancing the degree of homophily within the graph structure can significantly improve G
&lt;/p&gt;</description></item><item><title>&#20256;&#32479;&#30340;&#31526;&#21495;AI&#26041;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;AI&#26041;&#27861;&#26080;&#27861;&#28385;&#36275;&#21019;&#24314;&#24378;&#22823;&#21644;&#21487;&#20449;&#36182;&#30340;AI&#30340;&#25361;&#25112;&#65292;&#28982;&#32780;&#65292;&#21457;&#23637;&#33073;&#38772;&#27861;&#36890;&#36807;&#27169;&#20223;&#20154;&#31867;&#20799;&#31461;&#30340;&#33021;&#21147;&#21457;&#23637;&#36807;&#31243;&#65292;&#20026;&#21019;&#24314;&#31283;&#20581;&#21487;&#38752;&#30340;AI&#25552;&#20379;&#20102;&#24076;&#26395;&#12290;</title><link>http://arxiv.org/abs/2308.04586</link><description>&lt;p&gt;
AIs&#30340;&#21457;&#23637;&#33073;&#38772;&#27861;
&lt;/p&gt;
&lt;p&gt;
Developmental Bootstrapping of AIs. (arXiv:2308.04586v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04586
&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#31526;&#21495;AI&#26041;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;AI&#26041;&#27861;&#26080;&#27861;&#28385;&#36275;&#21019;&#24314;&#24378;&#22823;&#21644;&#21487;&#20449;&#36182;&#30340;AI&#30340;&#25361;&#25112;&#65292;&#28982;&#32780;&#65292;&#21457;&#23637;&#33073;&#38772;&#27861;&#36890;&#36807;&#27169;&#20223;&#20154;&#31867;&#20799;&#31461;&#30340;&#33021;&#21147;&#21457;&#23637;&#36807;&#31243;&#65292;&#20026;&#21019;&#24314;&#31283;&#20581;&#21487;&#38752;&#30340;AI&#25552;&#20379;&#20102;&#24076;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24403;&#21069;&#19968;&#20123;AI&#22312;&#23553;&#38381;&#30340;&#19990;&#30028;&#65292;&#22914;&#26827;&#30424;&#28216;&#25103;&#20013;&#36229;&#36234;&#20102;&#20154;&#31867;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#28151;&#20081;&#30340;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#34920;&#29616;&#26377;&#38480;&#12290;&#23427;&#20204;&#20250;&#29359;&#22855;&#24618;&#30340;&#38169;&#35823;&#32780;&#19988;&#27809;&#26377;&#24847;&#35782;&#21040;&#12290;&#23427;&#20204;&#24456;&#38590;&#21463;&#21040;&#25351;&#23548;&#65292;&#19981;&#33021;&#36816;&#29992;&#24120;&#35782;&#65292;&#32570;&#20047;&#22909;&#22855;&#24515;&#12290;&#23427;&#20204;&#19981;&#33021;&#25104;&#20026;&#33391;&#22909;&#30340;&#21512;&#20316;&#32773;&#12290;&#20256;&#32479;&#25163;&#21160;&#26500;&#24314;&#30340;&#31526;&#21495;AI&#26041;&#27861;&#26500;&#24314;&#30340;&#31995;&#32479;&#21644;&#20351;&#29992;&#29983;&#25104;&#21644;&#28145;&#24230;&#23398;&#20064;AI&#26041;&#27861;(&#21253;&#25324;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;)&#26500;&#24314;&#30340;&#31995;&#32479;&#37117;&#26080;&#27861;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;&#23427;&#20204;&#19981;&#36866;&#21512;&#21019;&#24314;&#24378;&#22823;&#21644;&#21487;&#20449;&#36182;&#30340;AI&#12290;&#23613;&#31649;&#27492;&#26041;&#27861;&#19981;&#23646;&#20110;&#20027;&#27969;&#30340;AI&#26041;&#27861;&#65292;&#20294;&#21457;&#23637;&#33073;&#38772;&#27861;&#26174;&#31034;&#20986;&#24076;&#26395;&#12290;&#22312;&#21457;&#23637;&#33073;&#38772;&#27861;&#20013;&#65292;AI&#20687;&#20154;&#31867;&#20799;&#31461;&#19968;&#26679;&#21457;&#23637;&#33021;&#21147;&#12290;&#23427;&#20204;&#20174;&#20808;&#22825;&#33021;&#21147;&#24320;&#22987;&#12290;&#20687;&#20154;&#31867;&#19968;&#26679;&#65292;&#23427;&#20204;&#19982;&#29615;&#22659;&#20114;&#21160;&#65292;&#24182;&#20174;&#20114;&#21160;&#20013;&#23398;&#20064;&#12290;&#23427;&#20204;&#36890;&#36807;&#33258;&#25105;&#21457;&#23637;&#30340;&#33021;&#21147;&#36880;&#27493;&#25193;&#23637;&#20808;&#22825;&#33021;&#21147;&#12290;&#23427;&#20204;&#20114;&#21160;&#24182;&#36880;&#28176;&#23558;&#25152;&#23398;&#24212;&#29992;&#20110;&#23454;&#38469;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although some current AIs surpass human abilities especially in closed worlds such as board games, their performance in the messy real world is limited. They make strange mistakes and do not notice them. They cannot be instructed easily, fail to use common sense, and lack curiosity. They do not make good collaborators. Neither systems built using the traditional manually-constructed symbolic AI approach nor systems built using generative and deep learning AI approaches including large language models (LLMs) can meet the challenges. They are not well suited for creating robust and trustworthy AIs. Although it is outside of mainstream AI approaches, developmental bootstrapping shows promise. In developmental bootstrapping, AIs develop competences like human children do. They start with innate competences. Like humans, they interact with the environment and learn from their interactions. They incrementally extend their innate competences with self-developed competences. They interact and 
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#35770;&#25991;&#35814;&#32454;&#32508;&#36848;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38544;&#20889;&#20998;&#26512;&#25216;&#26415;&#22312;&#25968;&#23383;&#23186;&#20307;&#20013;&#26816;&#27979;&#38544;&#34255;&#20449;&#24687;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2308.04522</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#19981;&#21516;&#25968;&#25454;&#31867;&#22411;&#38544;&#20889;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Deep Learning for Diverse Data Types Steganalysis: A Review. (arXiv:2308.04522v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04522
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35770;&#25991;&#35814;&#32454;&#32508;&#36848;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38544;&#20889;&#20998;&#26512;&#25216;&#26415;&#22312;&#25968;&#23383;&#23186;&#20307;&#20013;&#26816;&#27979;&#38544;&#34255;&#20449;&#24687;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#20889;&#26415;&#21644;&#38544;&#20889;&#20998;&#26512;&#26159;&#20449;&#24687;&#23433;&#20840;&#39046;&#22495;&#30340;&#20004;&#20010;&#30456;&#20851;&#26041;&#38754;&#12290;&#38544;&#20889;&#26415;&#26088;&#22312;&#38544;&#34255;&#36890;&#20449;&#65292;&#32780;&#38544;&#20889;&#20998;&#26512;&#21017;&#26088;&#22312;&#25214;&#21040;&#36825;&#20123;&#38544;&#34255;&#20449;&#24687;&#65292;&#29978;&#33267;&#23581;&#35797;&#24674;&#22797;&#20854;&#25152;&#21253;&#21547;&#30340;&#25968;&#25454;&#12290;&#38544;&#20889;&#26415;&#21644;&#38544;&#20889;&#20998;&#26512;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#65292;&#29305;&#21035;&#21463;&#21040;&#25191;&#27861;&#37096;&#38376;&#30340;&#20851;&#27880;&#12290;&#38544;&#20889;&#26415;&#24120;&#34987;&#32593;&#32476;&#29359;&#32618;&#20998;&#23376;&#29978;&#33267;&#24656;&#24598;&#20998;&#23376;&#29992;&#26469;&#36991;&#20813;&#22312;&#25317;&#26377;&#35777;&#25454;&#26102;&#34987;&#25429;&#65292;&#21363;&#20351;&#21152;&#23494;&#20063;&#19968;&#26679;&#65292;&#22240;&#20026;&#22312;&#35768;&#22810;&#22269;&#23478;&#31105;&#27490;&#25110;&#38480;&#21046;&#20351;&#29992;&#23494;&#30721;&#23398;&#12290;&#22240;&#27492;&#65292;&#20102;&#35299;&#25581;&#31034;&#38544;&#34255;&#20449;&#24687;&#30340;&#23574;&#31471;&#25216;&#26415;&#23545;&#25581;&#38706;&#38750;&#27861;&#34892;&#20026;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#25991;&#29486;&#20013;&#24341;&#20837;&#20102;&#35768;&#22810;&#24378;&#22823;&#21487;&#38752;&#30340;&#38544;&#20889;&#26415;&#21644;&#38544;&#20889;&#20998;&#26512;&#25216;&#26415;&#12290;&#26412;&#32508;&#36848;&#35770;&#25991;&#25552;&#20379;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38544;&#20889;&#20998;&#26512;&#25216;&#26415;&#22312;&#25968;&#23383;&#23186;&#20307;&#20013;&#26816;&#27979;&#38544;&#34255;&#20449;&#24687;&#30340;&#20840;&#38754;&#27010;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Steganography and steganalysis are two interrelated aspects of the field of information security. Steganography seeks to conceal communications, whereas steganalysis is aimed to either find them or even, if possible, recover the data they contain. Steganography and steganalysis have attracted a great deal of interest, particularly from law enforcement. Steganography is often used by cybercriminals and even terrorists to avoid being captured while in possession of incriminating evidence, even encrypted, since cryptography is prohibited or restricted in many countries. Therefore, knowledge of cutting-edge techniques to uncover concealed information is crucial in exposing illegal acts. Over the last few years, a number of strong and reliable steganography and steganalysis techniques have been introduced in the literature. This review paper provides a comprehensive overview of deep learning-based steganalysis techniques used to detect hidden information within digital media. The paper cove
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#33258;&#21160;&#21270;AI&#39537;&#21160;&#30340;PCF&#26680;&#31639;&#26694;&#26550;AutoPCF&#65292;&#35813;&#26694;&#26550;&#20855;&#26377;&#23454;&#29616;&#20135;&#21697;&#30899;&#36275;&#36857;&#30340;&#33258;&#21160;&#24314;&#27169;&#21644;&#20272;&#31639;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.04241</link><description>&lt;p&gt;
AutoPCF:&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#39640;&#25928;&#30340;&#20135;&#21697;&#30899;&#36275;&#36857;&#26680;&#31639;
&lt;/p&gt;
&lt;p&gt;
AutoPCF: Efficient Product Carbon Footprint Accounting with Large Language Models. (arXiv:2308.04241v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04241
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#33258;&#21160;&#21270;AI&#39537;&#21160;&#30340;PCF&#26680;&#31639;&#26694;&#26550;AutoPCF&#65292;&#35813;&#26694;&#26550;&#20855;&#26377;&#23454;&#29616;&#20135;&#21697;&#30899;&#36275;&#36857;&#30340;&#33258;&#21160;&#24314;&#27169;&#21644;&#20272;&#31639;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20135;&#21697;&#30899;&#36275;&#36857;&#65288;PCF&#65289;&#23545;&#20110;&#20943;&#32531;&#20379;&#24212;&#38142;&#30340;&#30899;&#25490;&#25918;&#33267;&#20851;&#37325;&#35201;&#65292;&#23427;&#34913;&#37327;&#20102;&#20135;&#21697;&#29983;&#21629;&#21608;&#26399;&#20013;&#25152;&#26377;&#27963;&#21160;&#25152;&#23548;&#33268;&#30340;&#30452;&#25509;&#21644;&#38388;&#25509;&#28201;&#23460;&#27668;&#20307;&#25490;&#25918;&#37327;&#12290;&#28982;&#32780;&#65292;PCF&#26680;&#31639;&#36890;&#24120;&#38656;&#35201;&#19987;&#19994;&#30693;&#35782;&#21644;&#22823;&#37327;&#26102;&#38388;&#26469;&#26500;&#24314;&#29983;&#21629;&#21608;&#26399;&#27169;&#22411;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#27979;&#35797;&#27604;&#36739;&#20102;&#20116;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#24314;&#27169;&#20135;&#21697;&#30340;&#8220;&#25671;&#31726;&#21040;&#22823;&#38376;&#8221;&#29983;&#21629;&#21608;&#26399;&#24182;&#29983;&#25104;&#36755;&#20837;&#21644;&#36755;&#20986;&#30340;&#24211;&#23384;&#25968;&#25454;&#26041;&#38754;&#30340;&#24212;&#29992;&#33021;&#21147;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#20316;&#20026;&#19968;&#20010;&#24191;&#20041;PCF&#30693;&#35782;&#25968;&#25454;&#24211;&#30340;&#23616;&#38480;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;LLMs&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;AI&#39537;&#21160;&#30340;PCF&#26680;&#31639;&#26694;&#26550;&#65292;&#31216;&#20026;AutoPCF&#65292;&#35813;&#26694;&#26550;&#36824;&#24212;&#29992;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#33258;&#21160;&#21305;&#37197;&#35745;&#31639;&#21442;&#25968;&#65292;&#24182;&#26368;&#32456;&#35745;&#31639;PCF&#12290;&#21033;&#29992;AutoPCF&#26694;&#26550;&#23545;&#19977;&#20010;&#26696;&#20363;&#20135;&#21697;&#30340;&#30899;&#36275;&#36857;&#36827;&#34892;&#20272;&#31639;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23427;&#22312;&#23454;&#29616;PCF&#30340;&#33258;&#21160;&#24314;&#27169;&#21644;&#20272;&#31639;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The product carbon footprint (PCF) is crucial for decarbonizing the supply chain, as it measures the direct and indirect greenhouse gas emissions caused by all activities during the product's life cycle. However, PCF accounting often requires expert knowledge and significant time to construct life cycle models. In this study, we test and compare the emergent ability of five large language models (LLMs) in modeling the 'cradle-to-gate' life cycles of products and generating the inventory data of inputs and outputs, revealing their limitations as a generalized PCF knowledge database. By utilizing LLMs, we propose an automatic AI-driven PCF accounting framework, called AutoPCF, which also applies deep learning algorithms to automatically match calculation parameters, and ultimately calculate the PCF. The results of estimating the carbon footprint for three case products using the AutoPCF framework demonstrate its potential in achieving automatic modeling and estimation of PCF with a large
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;medicX&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#39044;&#27979;&#33647;&#29289;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#26368;&#22909;&#30340;&#34920;&#29616;&#32452;&#21512;&#26159;ComplEx&#23884;&#20837;&#26041;&#27861;&#21644;LSTM&#32593;&#32476;&#65292;&#36798;&#21040;&#20102;95.19%&#30340;F1&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2308.04172</link><description>&lt;p&gt;
&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#39044;&#27979;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Predicting Drug-Drug Interactions Using Knowledge Graphs. (arXiv:2308.04172v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04172
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;medicX&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#39044;&#27979;&#33647;&#29289;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#26368;&#22909;&#30340;&#34920;&#29616;&#32452;&#21512;&#26159;ComplEx&#23884;&#20837;&#26041;&#27861;&#21644;LSTM&#32593;&#32476;&#65292;&#36798;&#21040;&#20102;95.19%&#30340;F1&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#20960;&#21313;&#24180;&#37324;&#65292;&#20154;&#20204;&#23545;&#33647;&#29289;&#30340;&#28040;&#36153;&#21644;&#32452;&#21512;&#27604;&#20197;&#21069;&#26356;&#22810;&#65292;&#23548;&#33268;&#20102;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#65288;DDIs&#65289;&#30340;&#22686;&#21152;&#12290;&#20026;&#20102;&#39044;&#27979;&#26410;&#30693;&#30340;DDIs&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#24320;&#22987;&#23558;&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#32435;&#20837;&#32771;&#34385;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#25429;&#25417;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25552;&#20379;&#27604;&#21333;&#20010;&#33647;&#29289;&#23646;&#24615;&#26356;&#22909;&#30340;&#33647;&#29289;&#34920;&#31034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;medicX&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#23427;&#23558;&#26469;&#33258;&#20844;&#20849;&#33647;&#29289;&#24211;&#30340;&#22810;&#20010;&#33647;&#29289;&#29305;&#24449;&#38598;&#25104;&#21040;KG&#20013;&#65292;&#24182;&#20351;&#29992;&#21508;&#31181;&#32763;&#35793;&#12289;&#20998;&#35299;&#21644;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#30340;KG Embedding&#65288;KGE&#65289;&#26041;&#27861;&#23545;&#22270;&#20013;&#30340;&#33410;&#28857;&#36827;&#34892;&#23884;&#20837;&#12290;&#26368;&#32456;&#65292;&#25105;&#20204;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#31639;&#27861;&#39044;&#27979;&#26410;&#30693;&#30340;DDIs&#12290;&#22312;&#19981;&#21516;&#30340;&#32763;&#35793;&#21644;&#20998;&#35299;&#22411;KGE&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#34920;&#29616;&#26368;&#20339;&#30340;&#32452;&#21512;&#26159;ComplEx&#23884;&#20837;&#26041;&#27861;&#21644;Long Short-Term Memory&#65288;LSTM&#65289;&#32593;&#32476;&#65292;&#23427;&#22312;&#22522;&#20110;DrugBank&#30340;DDIs&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;95.19%&#30340;F1&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the last decades, people have been consuming and combining more drugs than before, increasing the number of Drug-Drug Interactions (DDIs). To predict unknown DDIs, recently, studies started incorporating Knowledge Graphs (KGs) since they are able to capture the relationships among entities providing better drug representations than using a single drug property. In this paper, we propose the medicX end-to-end framework that integrates several drug features from public drug repositories into a KG and embeds the nodes in the graph using various translation, factorisation and Neural Network (NN) based KG Embedding (KGE) methods. Ultimately, we use a Machine Learning (ML) algorithm that predicts unknown DDIs. Among the different translation and factorisation-based KGE models, we found that the best performing combination was the ComplEx embedding method with a Long Short-Term Memory (LSTM) network, which obtained an F1-score of 95.19% on a dataset based on the DDIs found in DrugBank vers
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#23545;&#25239;&#25273;&#38500;&#30340;&#26041;&#27861;&#26469;&#22686;&#24378;&#22270;&#24425;&#31080;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#37325;&#26032;&#32771;&#34385;&#20462;&#21098;&#20449;&#24687;&#20013;&#30340;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ACE-GLT&#65292;&#36825;&#26159;&#19968;&#31181;&#26356;&#24378;&#22823;&#30340;&#22270;&#24425;&#31080;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.02916</link><description>&lt;p&gt;
&#21033;&#29992;&#20462;&#21098;&#20803;&#32032;&#30340;&#23545;&#25239;&#25273;&#38500;&#65306;&#36808;&#21521;&#26356;&#22909;&#30340;&#22270;&#24425;&#31080;
&lt;/p&gt;
&lt;p&gt;
Adversarial Erasing with Pruned Elements: Towards Better Graph Lottery Ticket. (arXiv:2308.02916v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#23545;&#25239;&#25273;&#38500;&#30340;&#26041;&#27861;&#26469;&#22686;&#24378;&#22270;&#24425;&#31080;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#37325;&#26032;&#32771;&#34385;&#20462;&#21098;&#20449;&#24687;&#20013;&#30340;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ACE-GLT&#65292;&#36825;&#26159;&#19968;&#31181;&#26356;&#24378;&#22823;&#30340;&#22270;&#24425;&#31080;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#24425;&#31080;&#65288;GLT&#65289;&#26159;&#26680;&#24515;&#23376;&#22270;&#21644;&#31232;&#30095;&#23376;&#32593;&#32476;&#30340;&#32452;&#21512;&#65292;&#26088;&#22312;&#20943;&#36731;&#22823;&#22411;&#36755;&#20837;&#22270;&#19978;&#28145;&#24230;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#21516;&#26102;&#20445;&#25345;&#21407;&#22987;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#20013;&#33719;&#32988;&#30340;GLT&#26159;&#36890;&#36807;&#24212;&#29992;&#36845;&#20195;&#24133;&#20540;&#20462;&#21098;&#65288;IMP&#65289;&#32780;&#24471;&#21040;&#30340;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35780;&#20272;&#21644;&#37325;&#26032;&#32771;&#34385;&#20462;&#21098;&#20449;&#24687;&#65292;&#36825;&#24573;&#35270;&#20102;&#22312;&#22270;/&#27169;&#22411;&#32467;&#26500;&#20462;&#21098;&#36807;&#31243;&#20013;&#36793;&#32536;/&#26435;&#37325;&#37325;&#35201;&#24615;&#30340;&#21160;&#24577;&#21464;&#21270;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#33719;&#32988;&#30340;&#24425;&#31080;&#30340;&#21560;&#24341;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29468;&#24819;&#65292;&#21363;&#20462;&#21098;&#22270;&#36830;&#25509;&#21644;&#27169;&#22411;&#21442;&#25968;&#20013;&#23384;&#22312;&#34987;&#24573;&#35270;&#30340;&#26377;&#20215;&#20540;&#20449;&#24687;&#65292;&#36825;&#20123;&#20449;&#24687;&#21487;&#20197;&#37325;&#26032;&#20998;&#32452;&#21040;GLT&#20013;&#20197;&#22686;&#24378;&#26368;&#32456;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23545;&#25239;&#24615;&#34917;&#20805;&#25273;&#38500;&#65288;ACE&#65289;&#26694;&#26550;&#65292;&#20197;&#20174;&#20462;&#21098;&#32452;&#20214;&#20013;&#25506;&#32034;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#24320;&#21457;&#20986;&#26356;&#24378;&#22823;&#30340;GLT&#65292;&#31216;&#20026;ACE-GLT&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Lottery Ticket (GLT), a combination of core subgraph and sparse subnetwork, has been proposed to mitigate the computational cost of deep Graph Neural Networks (GNNs) on large input graphs while preserving original performance. However, the winning GLTs in exisiting studies are obtained by applying iterative magnitude-based pruning (IMP) without re-evaluating and re-considering the pruned information, which disregards the dynamic changes in the significance of edges/weights during graph/model structure pruning, and thus limits the appeal of the winning tickets. In this paper, we formulate a conjecture, i.e., existing overlooked valuable information in the pruned graph connections and model parameters which can be re-grouped into GLT to enhance the final performance. Specifically, we propose an adversarial complementary erasing (ACE) framework to explore the valuable information from the pruned components, thereby developing a more powerful GLT, referred to as the ACE-GLT. The main
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#21327;&#21516;&#36807;&#28388;&#26041;&#27861;&#26500;&#24314;&#35268;&#33539;&#20197;&#25429;&#25417;AI&#29992;&#25143;&#20559;&#22909;&#30340;&#26032;&#35270;&#35282;&#12290;</title><link>http://arxiv.org/abs/2308.02542</link><description>&lt;p&gt;
&#20197;&#21327;&#21516;&#36807;&#28388;&#25429;&#25417;AI&#29992;&#25143;&#20559;&#22909;&#20316;&#20026;&#35268;&#33539;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Collaborative filtering to capture AI user's preferences as norms. (arXiv:2308.02542v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02542
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#21327;&#21516;&#36807;&#28388;&#26041;&#27861;&#26500;&#24314;&#35268;&#33539;&#20197;&#25429;&#25417;AI&#29992;&#25143;&#20559;&#22909;&#30340;&#26032;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;AI&#25216;&#26415;&#26681;&#25454;&#27599;&#20010;&#29992;&#25143;&#30340;&#20559;&#22909;&#36827;&#34892;&#23450;&#21046;&#26159;&#20854;&#33391;&#22909;&#36816;&#34892;&#30340;&#22522;&#30784;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#38656;&#35201;&#29992;&#25143;&#36807;&#22810;&#21442;&#19982;&#65292;&#24182;&#19988;&#26410;&#33021;&#30495;&#27491;&#25429;&#25417;&#21040;&#20182;&#20204;&#30340;&#30495;&#23454;&#20559;&#22909;&#12290;&#20107;&#23454;&#19978;&#65292;&#20026;&#20102;&#36991;&#20813;&#25163;&#21160;&#35774;&#32622;&#20559;&#22909;&#30340;&#40635;&#28902;&#65292;&#29992;&#25143;&#36890;&#24120;&#20250;&#25509;&#21463;&#40664;&#35748;&#35774;&#32622;&#65292;&#21363;&#20351;&#36825;&#20123;&#35774;&#32622;&#19982;&#20182;&#20204;&#30340;&#30495;&#23454;&#20559;&#22909;&#19981;&#31526;&#12290;&#35268;&#33539;&#21487;&#20197;&#29992;&#26469;&#35843;&#33410;&#34892;&#20026;&#65292;&#30830;&#20445;&#20854;&#31526;&#21512;&#29992;&#25143;&#30340;&#20559;&#22909;&#65292;&#20294;&#26159;&#23613;&#31649;&#25991;&#29486;&#24050;&#32463;&#35814;&#32454;&#30740;&#31350;&#20102;&#35268;&#33539;&#65292;&#22823;&#37096;&#20998;&#25552;&#35758;&#37117;&#26159;&#20174;&#24418;&#24335;&#21270;&#30340;&#35282;&#24230;&#20986;&#21457;&#12290;&#23454;&#38469;&#19978;&#65292;&#34429;&#28982;&#24050;&#32463;&#26377;&#19968;&#20123;&#20851;&#20110;&#26500;&#24314;&#35268;&#33539;&#20197;&#25429;&#25417;&#29992;&#25143;&#38544;&#31169;&#20559;&#22909;&#30340;&#30740;&#31350;&#65292;&#20294;&#26159;&#36825;&#20123;&#26041;&#27861;&#20381;&#36182;&#20110;&#39046;&#22495;&#30693;&#35782;&#65292;&#22312;AI&#25216;&#26415;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#24456;&#38590;&#33719;&#24471;&#21644;&#32500;&#25252;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#22312;&#26500;&#24314;&#35268;&#33539;&#26102;&#38656;&#35201;&#19968;&#31181;&#26032;&#30340;&#35270;&#35282;&#65292;&#21363;&#21033;&#29992;&#31995;&#32479;&#20013;&#22823;&#37327;&#29992;&#25143;&#30340;&#20559;&#22909;&#20449;&#24687;&#12290;&#21463;&#21040;&#25512;&#33616;&#31995;&#32479;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30456;&#20449;&#21327;&#21516;&#36807;&#28388;&#21487;&#20197;&#25104;&#20026;&#26500;&#24314;&#35268;&#33539;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Customising AI technologies to each user's preferences is fundamental to them functioning well. Unfortunately, current methods require too much user involvement and fail to capture their true preferences. In fact, to avoid the nuisance of manually setting preferences, users usually accept the default settings even if these do not conform to their true preferences. Norms can be useful to regulate behaviour and ensure it adheres to user preferences but, while the literature has thoroughly studied norms, most proposals take a formal perspective. Indeed, while there has been some research on constructing norms to capture a user's privacy preferences, these methods rely on domain knowledge which, in the case of AI technologies, is difficult to obtain and maintain. We argue that a new perspective is required when constructing norms, which is to exploit the large amount of preference information readily available from whole systems of users. Inspired by recommender systems, we believe that co
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#35770;&#25991;&#35752;&#35770;&#20102;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#27969;&#34892;&#20559;&#24046;&#38382;&#39064;&#65292;&#24182;&#22238;&#39038;&#20102;&#29616;&#26377;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#12289;&#37327;&#21270;&#21644;&#20943;&#23569;&#27969;&#34892;&#20559;&#24046;&#12290;&#23427;&#21516;&#26102;&#25552;&#20379;&#20102;&#35745;&#31639;&#24230;&#37327;&#30340;&#27010;&#36848;&#21644;&#20027;&#35201;&#25216;&#26415;&#26041;&#27861;&#30340;&#22238;&#39038;&#12290;</title><link>http://arxiv.org/abs/2308.01118</link><description>&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#27969;&#34892;&#20559;&#24046;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Popularity Bias in Recommender Systems. (arXiv:2308.01118v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01118
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#35770;&#25991;&#35752;&#35770;&#20102;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#27969;&#34892;&#20559;&#24046;&#38382;&#39064;&#65292;&#24182;&#22238;&#39038;&#20102;&#29616;&#26377;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#12289;&#37327;&#21270;&#21644;&#20943;&#23569;&#27969;&#34892;&#20559;&#24046;&#12290;&#23427;&#21516;&#26102;&#25552;&#20379;&#20102;&#35745;&#31639;&#24230;&#37327;&#30340;&#27010;&#36848;&#21644;&#20027;&#35201;&#25216;&#26415;&#26041;&#27861;&#30340;&#22238;&#39038;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#20197;&#20010;&#24615;&#21270;&#30340;&#26041;&#24335;&#24110;&#21161;&#20154;&#20204;&#25214;&#21040;&#30456;&#20851;&#20869;&#23481;&#12290;&#36825;&#20123;&#31995;&#32479;&#30340;&#19968;&#20010;&#20027;&#35201;&#25215;&#35834;&#26159;&#33021;&#22815;&#22686;&#21152;&#30446;&#24405;&#20013;&#36739;&#23569;&#30693;&#21517;&#30340;&#29289;&#21697;&#30340;&#21487;&#35265;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#29616;&#20170;&#30340;&#25512;&#33616;&#31639;&#27861;&#21453;&#32780;&#34920;&#29616;&#20986;&#27969;&#34892;&#20559;&#24046;&#65292;&#21363;&#23427;&#20204;&#22312;&#25512;&#33616;&#20013;&#32463;&#24120;&#20851;&#27880;&#30456;&#24403;&#27969;&#34892;&#30340;&#29289;&#21697;&#12290;&#36825;&#31181;&#20559;&#24046;&#19981;&#20165;&#21487;&#33021;&#23548;&#33268;&#30701;&#26399;&#20869;&#23545;&#28040;&#36153;&#32773;&#21644;&#25552;&#20379;&#32773;&#30340;&#25512;&#33616;&#20215;&#20540;&#26377;&#38480;&#65292;&#32780;&#19988;&#36824;&#21487;&#33021;&#24341;&#36215;&#19981;&#24076;&#26395;&#30340;&#24378;&#21270;&#25928;&#24212;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#27969;&#34892;&#20559;&#24046;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#24182;&#22238;&#39038;&#20102;&#29616;&#26377;&#30340;&#26816;&#27979;&#12289;&#37327;&#21270;&#21644;&#20943;&#23569;&#25512;&#33616;&#31995;&#32479;&#20013;&#27969;&#34892;&#20559;&#24046;&#30340;&#26041;&#27861;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#32508;&#36848;&#26082;&#21253;&#25324;&#20102;&#25991;&#29486;&#20013;&#20351;&#29992;&#30340;&#35745;&#31639;&#24230;&#37327;&#30340;&#27010;&#36848;&#65292;&#20063;&#21253;&#25324;&#20102;&#20943;&#23569;&#20559;&#24046;&#30340;&#20027;&#35201;&#25216;&#26415;&#26041;&#27861;&#30340;&#22238;&#39038;&#12290;&#25105;&#20204;&#36824;&#23545;&#36825;&#20123;&#26041;&#27861;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender systems help people find relevant content in a personalized way. One main promise of such systems is that they are able to increase the visibility of items in the long tail, i.e., the lesser-known items in a catalogue. Existing research, however, suggests that in many situations today's recommendation algorithms instead exhibit a popularity bias, meaning that they often focus on rather popular items in their recommendations. Such a bias may not only lead to limited value of the recommendations for consumers and providers in the short run, but it may also cause undesired reinforcement effects over time. In this paper, we discuss the potential reasons for popularity bias and we review existing approaches to detect, quantify and mitigate popularity bias in recommender systems. Our survey therefore includes both an overview of the computational metrics used in the literature as well as a review of the main technical approaches to reduce the bias. We furthermore critically discu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;Fine-Tuned&#30340;OpenAI LLM&#36827;&#34892;&#32763;&#35793;&#36136;&#37327;&#20272;&#35745;&#30340;&#33021;&#21147;&#65292;&#23454;&#39564;&#35777;&#26126;&#21487;&#20197;&#36890;&#36807;Fine-Tuned&#30340;ChatGPT&#26469;&#39044;&#27979;&#26426;&#22120;&#32763;&#35793;&#30340;&#36136;&#37327;&#65292;&#20294;&#20173;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2308.00158</link><description>&lt;p&gt;
&#20351;&#29992;Fine-Tuned&#30340;OpenAI LLM&#39044;&#27979;&#26426;&#22120;&#32763;&#35793;&#36755;&#20986;&#20013;&#30340;&#23436;&#32654;&#36136;&#37327;&#27573;&#33853;&#65306;&#26159;&#21542;&#21487;&#20197;&#20174;&#21382;&#21490;&#25968;&#25454;&#20013;&#25429;&#25417;&#32534;&#36753;&#36317;&#31163;&#27169;&#24335;&#65311;
&lt;/p&gt;
&lt;p&gt;
Predicting Perfect Quality Segments in MT Output with Fine-Tuned OpenAI LLM: Is it possible to capture editing distance patterns from historical data?. (arXiv:2308.00158v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00158
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;Fine-Tuned&#30340;OpenAI LLM&#36827;&#34892;&#32763;&#35793;&#36136;&#37327;&#20272;&#35745;&#30340;&#33021;&#21147;&#65292;&#23454;&#39564;&#35777;&#26126;&#21487;&#20197;&#36890;&#36807;Fine-Tuned&#30340;ChatGPT&#26469;&#39044;&#27979;&#26426;&#22120;&#32763;&#35793;&#30340;&#36136;&#37327;&#65292;&#20294;&#20173;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32763;&#35793;&#36136;&#37327;&#20272;&#35745;&#65288;TQE&#65289;&#26159;&#23558;&#36755;&#20986;&#32763;&#35793;&#37096;&#32626;&#21040;&#20351;&#29992;&#20013;&#20043;&#21069;&#30340;&#37325;&#35201;&#27493;&#39588;&#12290; TQE&#23545;&#20110;&#35780;&#20272;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#21644;&#20154;&#24037;&#32763;&#35793;&#65288;HT&#65289;&#30340;&#36136;&#37327;&#20063;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#32780;&#19981;&#38656;&#35201;&#26597;&#30475;&#21442;&#32771;&#32763;&#35793;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26816;&#26597;&#20102;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#21542;&#21487;&#20197;&#20026;TQE&#20219;&#21153;&#21644;&#23427;&#20204;&#30340;&#33021;&#21147;&#36827;&#34892;Fine-Tune&#12290;&#25105;&#20204;&#20197;ChatGPT&#20026;&#20363;&#65292;&#23558;TQE&#35270;&#20026;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#12290;&#20351;&#29992;&#33521;&#24847;&#21644;&#33521;&#24503;&#35757;&#32451;&#35821;&#26009;&#24211;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#36890;&#36807;ChatGPT&#30340;API Fine-Tuned&#21487;&#20197;&#22312;&#39044;&#27979;&#32763;&#35793;&#36136;&#37327;&#26041;&#38754;&#33719;&#24471;&#30456;&#23545;&#36739;&#39640;&#30340;&#24471;&#20998;&#65292;&#21363;&#26159;&#21542;&#38656;&#35201;&#32534;&#36753;&#32763;&#35793;&#65292;&#20294;&#32943;&#23450;&#26377;&#25913;&#36827;&#20934;&#30830;&#24615;&#30340;&#31354;&#38388;&#12290;&#33521;&#24847;&#21452;&#35821;&#25688;&#35201;&#21487;&#22312;&#35770;&#25991;&#20013;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Translation Quality Estimation (TQE) is an important step before deploying the output translation into usage. TQE is also critical in assessing machine translation (MT) and human translation (HT) quality without seeing the reference translations. In this work, we examine if the state-of-the-art large language models (LLMs) can be fine-tuned for the TQE task and their capability. We take ChatGPT as one example and approach TQE as a binary classification task. Using English-Italian and English-German training corpus, our experimental results show that fine-tuned ChatGPT via its API can achieve a relatively high score on predicting translation quality, i.e. if the translation needs to be edited, but there is definitely space to improve the accuracy. English-Italiano bilingual Abstract is available in the paper.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#26410;&#32463;&#27979;&#37327;&#27969;&#22495;&#30340;&#26497;&#31471;&#27700;&#25991;&#20107;&#20214;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20840;&#29699;&#27946;&#27700;&#39044;&#35686;&#30340;&#35206;&#30422;&#33539;&#22260;&#12290;</title><link>http://arxiv.org/abs/2307.16104</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#25552;&#39640;&#20102;&#20840;&#29699;&#21487;&#38752;&#27946;&#27700;&#39044;&#35686;&#30340;&#35206;&#30422;&#33539;&#22260;
&lt;/p&gt;
&lt;p&gt;
AI Increases Global Access to Reliable Flood Forecasts. (arXiv:2307.16104v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16104
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#26410;&#32463;&#27979;&#37327;&#27969;&#22495;&#30340;&#26497;&#31471;&#27700;&#25991;&#20107;&#20214;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20840;&#29699;&#27946;&#27700;&#39044;&#35686;&#30340;&#35206;&#30422;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27946;&#27700;&#26159;&#26368;&#24120;&#35265;&#21644;&#24433;&#21709;&#26368;&#22823;&#30340;&#33258;&#28982;&#28798;&#23475;&#20043;&#19968;&#65292;&#23545;&#21457;&#23637;&#20013;&#22269;&#23478;&#23588;&#20854;&#20855;&#26377;&#19981;&#23545;&#31216;&#30340;&#24433;&#21709;&#65292;&#36825;&#20123;&#22269;&#23478;&#24448;&#24448;&#32570;&#20047;&#23494;&#38598;&#30340;&#27700;&#27969;&#30417;&#27979;&#32593;&#32476;&#12290;&#20934;&#30830;&#21450;&#26102;&#30340;&#39044;&#35686;&#23545;&#20110;&#20943;&#36731;&#27946;&#27700;&#39118;&#38505;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#20934;&#30830;&#30340;&#27700;&#25991;&#27169;&#25311;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#26681;&#25454;&#27599;&#20010;&#24212;&#29992;&#30340;&#27969;&#22495;&#20013;&#30340;&#38271;&#26102;&#38388;&#25968;&#25454;&#35760;&#24405;&#36827;&#34892;&#26657;&#20934;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27169;&#22411;&#65292;&#21487;&#20197;&#39044;&#27979;7&#22825;&#20869;&#30340;&#26497;&#31471;&#27700;&#25991;&#20107;&#20214;&#12290;&#35813;&#27169;&#22411;&#22312;&#25152;&#26377;&#22823;&#27954;&#12289;&#21069;&#23548;&#26102;&#38388;&#21644;&#37325;&#29616;&#26399;&#20013;&#22343;&#26126;&#26174;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#20840;&#29699;&#27700;&#25991;&#27169;&#22411;&#65288;Copernicus&#24212;&#24613;&#31649;&#29702;&#26381;&#21153;&#20840;&#29699;&#27946;&#27700;&#24847;&#35782;&#31995;&#32479;&#65289;&#12290;AI&#22312;&#26410;&#32463;&#27979;&#37327;&#30340;&#27969;&#22495;&#20013;&#30340;&#39044;&#27979;&#23588;&#20854;&#26377;&#25928;&#65292;&#36825;&#24456;&#37325;&#35201;&#65292;&#22240;&#20026;&#20840;&#29699;&#21482;&#26377;&#30334;&#20998;&#20043;&#20960;&#30340;&#27969;&#22495;&#20855;&#26377;&#27969;&#37327;&#35266;&#27979;&#31449;&#65292;&#32780;&#21457;&#23637;&#20013;&#22269;&#23478;&#30340;&#26410;&#32463;&#27979;&#37327;&#30340;&#27969;&#22495;&#25968;&#37327;&#21344;&#27604;&#24456;&#39640;&#65292;&#23545;&#20154;&#31867;&#29305;&#21035;&#33030;&#24369;&#12290;
&lt;/p&gt;
&lt;p&gt;
Floods are one of the most common and impactful natural disasters, with a disproportionate impact in developing countries that often lack dense streamflow monitoring networks. Accurate and timely warnings are critical for mitigating flood risks, but accurate hydrological simulation models typically must be calibrated to long data records in each watershed where they are applied. We developed an Artificial Intelligence (AI) model to predict extreme hydrological events at timescales up to 7 days in advance. This model significantly outperforms current state of the art global hydrology models (the Copernicus Emergency Management Service Global Flood Awareness System) across all continents, lead times, and return periods. AI is especially effective at forecasting in ungauged basins, which is important because only a few percent of the world's watersheds have stream gauges, with a disproportionate number of ungauged basins in developing countries that are especially vulnerable to the human 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;ChatGPT&#22312;&#23433;&#20840;&#23548;&#21521;&#30340;&#31243;&#24207;&#20998;&#26512;&#20013;&#30340;&#34920;&#29616;&#36827;&#34892;&#30740;&#31350;&#65292;&#26088;&#22312;&#20102;&#35299;&#20854;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#26356;&#22909;&#22320;&#29702;&#35299;ChatGPT&#22312;&#23433;&#20840;&#39046;&#22495;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.12488</link><description>&lt;p&gt;
ChatGPT&#29992;&#20110;&#36719;&#20214;&#23433;&#20840;&#65306;&#25506;&#32034;ChatGPT&#22312;&#23433;&#20840;&#24212;&#29992;&#20013;&#30340;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;
&lt;/p&gt;
&lt;p&gt;
ChatGPT for Software Security: Exploring the Strengths and Limitations of ChatGPT in the Security Applications. (arXiv:2307.12488v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12488
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;ChatGPT&#22312;&#23433;&#20840;&#23548;&#21521;&#30340;&#31243;&#24207;&#20998;&#26512;&#20013;&#30340;&#34920;&#29616;&#36827;&#34892;&#30740;&#31350;&#65292;&#26088;&#22312;&#20102;&#35299;&#20854;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#26356;&#22909;&#22320;&#29702;&#35299;ChatGPT&#22312;&#23433;&#20840;&#39046;&#22495;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#19968;&#20010;&#22810;&#25165;&#22810;&#33402;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;ChatGPT&#22312;&#21508;&#20010;&#39046;&#22495;&#24212;&#23545;&#38382;&#39064;&#30340;&#28508;&#21147;&#24471;&#21040;&#20102;&#26174;&#33879;&#30340;&#23637;&#31034;&#12290;&#23427;&#33021;&#22815;&#20998;&#26512;&#12289;&#29702;&#35299;&#21644;&#32508;&#21512;&#26469;&#33258;&#22312;&#32447;&#36164;&#28304;&#21644;&#29992;&#25143;&#36755;&#20837;&#30340;&#20449;&#24687;&#65292;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#25506;&#32034;&#20102;ChatGPT&#22312;&#20195;&#30721;&#29983;&#25104;&#21644;&#20195;&#30721;&#23457;&#26597;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;ChatGPT&#22312;&#38754;&#21521;&#23433;&#20840;&#30340;&#31243;&#24207;&#20998;&#26512;&#20013;&#30340;&#33021;&#21147;&#65292;&#20174;&#25915;&#20987;&#32773;&#21644;&#23433;&#20840;&#20998;&#26512;&#24072;&#30340;&#35282;&#24230;&#36827;&#34892;&#20102;&#25506;&#35752;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#26469;&#35780;&#20272;ChatGPT&#22312;&#20960;&#20010;&#23433;&#20840;&#23548;&#21521;&#30340;&#31243;&#24207;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#22238;&#31572;&#36136;&#37327;&#65292;&#24182;&#26377;&#24847;&#22320;&#24341;&#20837;&#25361;&#25112;&#26469;&#35780;&#20272;&#20854;&#21709;&#24212;&#33021;&#21147;&#12290;&#36890;&#36807;&#23545;ChatGPT&#25552;&#20379;&#30340;&#31572;&#26696;&#36136;&#37327;&#30340;&#32771;&#23519;&#65292;&#25105;&#20204;&#23545;&#20854;&#22312;&#23433;&#20840;&#23548;&#21521;&#30340;&#31243;&#24207;&#20998;&#26512;&#39046;&#22495;&#30340;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#26377;&#20102;&#26356;&#28165;&#26224;&#30340;&#35748;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT, as a versatile large language model, has demonstrated remarkable potential in addressing inquiries across various domains. Its ability to analyze, comprehend, and synthesize information from both online sources and user inputs has garnered significant attention. Previous research has explored ChatGPT's competence in code generation and code reviews. In this paper, we delve into ChatGPT's capabilities in security-oriented program analysis, focusing on perspectives from both attackers and security analysts. We present a case study involving several security-oriented program analysis tasks while deliberately introducing challenges to assess ChatGPT's responses. Through an examination of the quality of answers provided by ChatGPT, we gain a clearer understanding of its strengths and limitations in the realm of security-oriented program analysis.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#25945;&#32946;&#39046;&#22495;&#20013;&#65292;&#30001;&#20154;&#31867;&#21644;&#29983;&#25104;&#24615;&#35821;&#35328;&#27169;&#22411;&#21327;&#20316;&#32534;&#20889;&#30340;&#28151;&#21512;&#25991;&#26412;&#30340;AI&#20869;&#23481;&#26816;&#27979;&#26041;&#27861;&#65292;&#23558;&#20854;&#24418;&#24335;&#21270;&#20026;&#35782;&#21035;&#36716;&#25442;&#28857;&#30340;&#20219;&#21153;&#65292;&#20197;&#21306;&#20998;&#20154;&#31867;&#32534;&#20889;&#21644;AI&#29983;&#25104;&#30340;&#37096;&#20998;&#12290;</title><link>http://arxiv.org/abs/2307.12267</link><description>&lt;p&gt;
&#38754;&#21521;&#25945;&#32946;&#20013;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#28151;&#21512;&#35770;&#25991;&#30340;&#33258;&#21160;&#36793;&#30028;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Towards Automatic Boundary Detection for Human-AI Collaborative Hybrid Essay in Education. (arXiv:2307.12267v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12267
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#25945;&#32946;&#39046;&#22495;&#20013;&#65292;&#30001;&#20154;&#31867;&#21644;&#29983;&#25104;&#24615;&#35821;&#35328;&#27169;&#22411;&#21327;&#20316;&#32534;&#20889;&#30340;&#28151;&#21512;&#25991;&#26412;&#30340;AI&#20869;&#23481;&#26816;&#27979;&#26041;&#27861;&#65292;&#23558;&#20854;&#24418;&#24335;&#21270;&#20026;&#35782;&#21035;&#36716;&#25442;&#28857;&#30340;&#20219;&#21153;&#65292;&#20197;&#21306;&#20998;&#20154;&#31867;&#32534;&#20889;&#21644;AI&#29983;&#25104;&#30340;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;ChatGPT&#65289;&#33021;&#22815;&#22312;&#25552;&#20379;&#20855;&#20307;&#25351;&#23548;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#27969;&#30021;&#22238;&#31572;&#12290;&#23613;&#31649;&#25215;&#35748;&#25216;&#26415;&#36827;&#27493;&#24102;&#26469;&#30340;&#20415;&#21033;&#65292;&#25945;&#32946;&#32773;&#20063;&#25285;&#24515;&#23398;&#29983;&#21487;&#33021;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#23436;&#25104;&#20889;&#20316;&#20219;&#21153;&#24182;&#23558;&#20854;&#20551;&#20882;&#20026;&#33258;&#24049;&#30340;&#21407;&#21019;&#20316;&#21697;&#12290;&#34429;&#28982;&#26377;&#24456;&#22810;AI&#20869;&#23481;&#26816;&#27979;&#30740;&#31350;&#26159;&#22522;&#20110;&#36825;&#20123;&#25285;&#24551;&#36827;&#34892;&#30340;&#65292;&#20294;&#22823;&#22810;&#25968;&#20043;&#21069;&#30340;&#30740;&#31350;&#23558;AI&#20869;&#23481;&#26816;&#27979;&#24314;&#27169;&#20026;&#19968;&#20010;&#20998;&#31867;&#38382;&#39064;&#65292;&#20551;&#35774;&#19968;&#20010;&#25991;&#26412;&#35201;&#20040;&#23436;&#20840;&#30001;&#20154;&#31867;&#32534;&#20889;&#65292;&#35201;&#20040;&#23436;&#20840;&#30001;AI&#29983;&#25104;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;AI&#20869;&#23481;&#26816;&#27979;&#22312;&#19968;&#20010;&#23569;&#26377;&#25506;&#32034;&#20294;&#21364;&#29616;&#23454;&#30340;&#24773;&#20917;&#19979;&#65292;&#21363;&#26816;&#27979;&#30340;&#25991;&#26412;&#30001;&#20154;&#31867;&#21644;&#29983;&#25104;&#24615;&#35821;&#35328;&#27169;&#22411;&#65288;&#21363;&#28151;&#21512;&#25991;&#26412;&#65289;&#21327;&#20316;&#32534;&#20889;&#12290;&#25105;&#20204;&#39318;&#20808;&#23558;&#26816;&#27979;&#20219;&#21153;&#24418;&#24335;&#21270;&#20026;&#20174;&#32473;&#23450;&#30340;&#28151;&#21512;&#25991;&#26412;&#20013;&#35782;&#21035;&#20154;&#31867;&#32534;&#20889;&#20869;&#23481;&#21644;AI&#29983;&#25104;&#20869;&#23481;&#20043;&#38388;&#30340;&#36716;&#25442;&#28857;&#65288;&#36793;&#30028;&#26816;&#27979;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent large language models (LLMs), e.g., ChatGPT, have been able to generate human-like and fluent responses when provided with specific instructions. While admitting the convenience brought by technological advancement, educators also have concerns that students might leverage LLMs to complete their writing assignments and pass them off as their original work. Although many AI content detection studies have been conducted as a result of such concerns, most of these prior studies modeled AI content detection as a classification problem, assuming that a text is either entirely human-written or entirely AI-generated. In this study, we investigated AI content detection in a rarely explored yet realistic setting where the text to be detected is collaboratively written by human and generative LLMs (i.e., hybrid text). We first formalized the detection task as identifying the transition points between human-written content and AI-generated content from a given hybrid text (boundary det
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;IDE&#20869;&#29983;&#25104;&#24335;&#20449;&#24687;&#25903;&#25345;&#65292;&#36890;&#36807;&#22312;IDE&#20013;&#20869;&#23884;&#23545;&#35805;&#24335;&#29992;&#25143;&#30028;&#38754;&#65292;&#21521;&#24320;&#21457;&#32773;&#25552;&#20379;&#20851;&#20110;&#20195;&#30721;&#29702;&#35299;&#30340;&#24110;&#21161;&#12290;&#36890;&#36807;&#26597;&#35810;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#35299;&#37322;&#20195;&#30721;&#12289;&#25552;&#20379;API&#35843;&#29992;&#30340;&#35814;&#32454;&#20449;&#24687;&#12289;&#35299;&#37322;&#29305;&#23450;&#39046;&#22495;&#26415;&#35821;&#20197;&#21450;&#20026;API&#25552;&#20379;&#20351;&#29992;&#31034;&#20363;&#65292;&#35813;&#31995;&#32479;&#22312;&#29992;&#25143;&#30740;&#31350;&#20013;&#33719;&#24471;&#20102;&#31215;&#26497;&#30340;&#35780;&#20215;&#12290;</title><link>http://arxiv.org/abs/2307.08177</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;IDE&#20869;&#29983;&#25104;&#24335;&#20449;&#24687;&#25903;&#25345;
&lt;/p&gt;
&lt;p&gt;
In-IDE Generation-based Information Support with a Large Language Model. (arXiv:2307.08177v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08177
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;IDE&#20869;&#29983;&#25104;&#24335;&#20449;&#24687;&#25903;&#25345;&#65292;&#36890;&#36807;&#22312;IDE&#20013;&#20869;&#23884;&#23545;&#35805;&#24335;&#29992;&#25143;&#30028;&#38754;&#65292;&#21521;&#24320;&#21457;&#32773;&#25552;&#20379;&#20851;&#20110;&#20195;&#30721;&#29702;&#35299;&#30340;&#24110;&#21161;&#12290;&#36890;&#36807;&#26597;&#35810;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#35299;&#37322;&#20195;&#30721;&#12289;&#25552;&#20379;API&#35843;&#29992;&#30340;&#35814;&#32454;&#20449;&#24687;&#12289;&#35299;&#37322;&#29305;&#23450;&#39046;&#22495;&#26415;&#35821;&#20197;&#21450;&#20026;API&#25552;&#20379;&#20351;&#29992;&#31034;&#20363;&#65292;&#35813;&#31995;&#32479;&#22312;&#29992;&#25143;&#30740;&#31350;&#20013;&#33719;&#24471;&#20102;&#31215;&#26497;&#30340;&#35780;&#20215;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#20195;&#30721;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#23588;&#20854;&#26159;&#22312;&#26032;&#30340;&#21644;&#22797;&#26434;&#30340;&#24320;&#21457;&#29615;&#22659;&#20013;&#24037;&#20316;&#26102;&#12290;&#20195;&#30721;&#27880;&#37322;&#21644;&#25991;&#26723;&#21487;&#20197;&#24110;&#21161;&#65292;&#20294;&#24448;&#24448;&#31232;&#32570;&#25110;&#38590;&#20197;&#23548;&#33322;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#27491;&#22312;&#24443;&#24213;&#25913;&#21464;&#32534;&#20889;&#20195;&#30721;&#30340;&#36807;&#31243;&#12290;&#23427;&#20204;&#33021;&#21542;&#23545;&#29702;&#35299;&#20195;&#30721;&#25552;&#20379;&#21516;&#26679;&#30340;&#24110;&#21161;&#65311;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#25506;&#32034;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#30452;&#25509;&#20869;&#23884;&#24335;&#23545;&#35805;&#24335;&#29992;&#25143;&#30028;&#38754;&#65292;&#24182;&#38024;&#23545;&#20195;&#30721;&#29702;&#35299;&#36827;&#34892;&#20102;&#20248;&#21270;&#12290;&#25105;&#20204;&#30340;IDE&#25554;&#20214;&#36890;&#36807;&#22235;&#20010;&#39640;&#32423;&#35831;&#27714;&#65288;&#26080;&#38656;&#29992;&#25143;&#32534;&#20889;&#26174;&#24335;&#25552;&#31034;&#20449;&#24687;&#65289;&#26597;&#35810;OpenAI&#30340;GPT-3.5&#21644;GPT-4&#27169;&#22411;&#65292;&#35831;&#27714;&#20869;&#23481;&#21253;&#25324;&#35299;&#37322;&#20195;&#30721;&#20013;&#30340;&#31361;&#20986;&#37096;&#20998;&#12289;&#25552;&#20379;&#20195;&#30721;&#20013;&#20351;&#29992;&#30340;API&#35843;&#29992;&#35814;&#32454;&#20449;&#24687;&#65292;&#35299;&#37322;&#20851;&#38190;&#39046;&#22495;&#29305;&#23450;&#26415;&#35821;&#65292;&#20197;&#21450;&#20026;API&#25552;&#20379;&#20351;&#29992;&#31034;&#20363;&#12290;&#35813;&#25554;&#20214;&#36824;&#25903;&#25345;&#33258;&#30001;&#21709;&#24212;&#24335;&#25552;&#31034;&#65292;&#21487;&#20197;&#33258;&#21160;&#19978;&#19979;&#25991;&#21270;&#21040;&#27491;&#22312;&#32534;&#36753;&#30340;&#31243;&#24207;&#20013;&#12290;&#25105;&#20204;&#36890;&#36807;32&#21517;&#21442;&#19982;&#32773;&#36827;&#34892;&#29992;&#25143;&#30740;&#31350;&#26469;&#35780;&#20272;&#35813;&#31995;&#32479;&#65292;&#32467;&#26524;&#30830;&#35748;&#20351;&#29992;&#25105;&#20204;&#30340;&#25554;&#20214;&#33021;&#22815;
&lt;/p&gt;
&lt;p&gt;
Understanding code is challenging, especially when working in new and complex development environments. Code comments and documentation can help, but are typically scarce or hard to navigate. Large language models (LLMs) are revolutionizing the process of writing code. Can they do the same for helping understand it? In this study, we provide a first investigation of an LLM-based conversational UI built directly in the IDE that is geared towards code understanding. Our IDE plugin queries OpenAI's GPT-3.5 and GPT-4 models with four high-level requests without the user having to write explicit prompts: to explain a highlighted section of code, provide details of API calls used in the code, explain key domain-specific terms, and provide usage examples for an API. The plugin also allows for open-ended prompts, which are automatically contextualized to the LLM with the program being edited. We evaluate this system in a user study with 32 participants, which confirms that using our plugin can
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35757;&#32451;&#36731;&#37327;&#32423;&#36866;&#37197;&#22120;&#26469;&#39640;&#25928;&#22495;&#33258;&#36866;&#24212;&#21477;&#23376;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#24494;&#35843;&#25972;&#20010;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#30340;&#36164;&#28304;&#28040;&#32791;&#12290;&#36890;&#36807;&#35757;&#32451;&#29305;&#23450;&#39046;&#22495;&#30340;&#36866;&#37197;&#22120;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#20351;&#29992;&#21516;&#19968;&#27169;&#22411;&#33719;&#24471;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.03104</link><description>&lt;p&gt;
&#20351;&#29992;&#36866;&#37197;&#22120;&#39640;&#25928;&#22495;&#33258;&#36866;&#24212;&#21477;&#23376;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Efficient Domain Adaptation of Sentence Embeddings using Adapters. (arXiv:2307.03104v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03104
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35757;&#32451;&#36731;&#37327;&#32423;&#36866;&#37197;&#22120;&#26469;&#39640;&#25928;&#22495;&#33258;&#36866;&#24212;&#21477;&#23376;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#24494;&#35843;&#25972;&#20010;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#30340;&#36164;&#28304;&#28040;&#32791;&#12290;&#36890;&#36807;&#35757;&#32451;&#29305;&#23450;&#39046;&#22495;&#30340;&#36866;&#37197;&#22120;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#20351;&#29992;&#21516;&#19968;&#27169;&#22411;&#33719;&#24471;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21477;&#23376;&#23884;&#20837;&#20351;&#25105;&#20204;&#33021;&#22815;&#25429;&#25417;&#30701;&#25991;&#26412;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#12290;&#22823;&#22810;&#25968;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#26159;&#38024;&#23545;&#19968;&#33324;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#65288;STS&#65289;&#20219;&#21153;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#22240;&#27492;&#65292;&#35201;&#22312;&#29305;&#23450;&#39046;&#22495;&#20013;&#20351;&#29992;&#21477;&#23376;&#23884;&#20837;&#65292;&#24517;&#39035;&#23558;&#27169;&#22411;&#36866;&#24212;&#20110;&#35813;&#39046;&#22495;&#20197;&#33719;&#24471;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;&#36890;&#24120;&#65292;&#36825;&#26159;&#36890;&#36807;&#23545;&#24863;&#20852;&#36259;&#30340;&#22495;&#23545;&#25972;&#20010;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#26469;&#23454;&#29616;&#30340;&#12290;&#34429;&#28982;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#20135;&#29983;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#20294;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#26356;&#26032;&#20102;&#25152;&#26377;&#27169;&#22411;&#30340;&#26435;&#37325;&#65292;&#20351;&#35813;&#26041;&#27861;&#22312;&#36164;&#28304;&#19978;&#35201;&#27714;&#36739;&#39640;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35757;&#32451;&#36731;&#37327;&#32423;&#36866;&#37197;&#22120;&#30340;&#26041;&#27861;&#65292;&#32780;&#19981;&#26159;&#21333;&#29420;&#20026;&#27599;&#20010;&#30446;&#26631;&#39046;&#22495;&#24494;&#35843;&#25972;&#20010;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#12290;&#36825;&#20123;&#29305;&#23450;&#39046;&#22495;&#30340;&#36866;&#37197;&#22120;&#19981;&#38656;&#35201;&#24494;&#35843;&#25152;&#26377;&#24213;&#23618;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#30340;&#21442;&#25968;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#21482;&#35757;&#32451;&#23569;&#37327;&#30340;&#39069;&#22806;&#21442;&#25968;&#65292;&#21516;&#26102;&#20445;&#25345;&#24213;&#23618;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#30340;&#26435;&#37325;&#19981;&#21464;&#12290;&#35757;&#32451;&#29305;&#23450;&#39046;&#22495;&#30340;&#36866;&#37197;&#22120;&#21487;&#20197;&#22987;&#32456;&#20351;&#29992;&#21516;&#19968;&#27169;&#22411;&#24182;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#33719;&#24471;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sentence embeddings enable us to capture the semantic similarity of short texts. Most sentence embedding models are trained for general semantic textual similarity (STS) tasks. Therefore, to use sentence embeddings in a particular domain, the model must be adapted to it in order to achieve good results. Usually, this is done by fine-tuning the entire sentence embedding model for the domain of interest. While this approach yields state-of-the-art results, all of the model's weights are updated during fine-tuning, making this method resource-intensive. Therefore, instead of fine-tuning entire sentence embedding models for each target domain individually, we propose to train lightweight adapters. These domain-specific adapters do not require fine-tuning all underlying sentence embedding model parameters. Instead, we only train a small number of additional parameters while keeping the weights of the underlying sentence embedding model fixed. Training domain-specific adapters allows always 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#20154;&#19982;&#20154;&#20043;&#38388;&#30340;&#20114;&#21160;&#26816;&#27979;&#20219;&#21153;&#65292;&#36890;&#36807;&#19968;&#31181;&#27169;&#22411;&#23454;&#29616;&#20102;&#21516;&#26102;&#26816;&#27979;&#20027;&#20307;&#12289;&#35782;&#21035;&#20010;&#20154;&#21160;&#20316;&#21644;&#26681;&#25454;&#20114;&#21160;&#20851;&#31995;&#20998;&#32452;&#30340;&#30446;&#26631;&#12290;&#36890;&#36807;&#22312;&#29616;&#26377;&#30340;AVA&#25968;&#25454;&#38598;&#19978;&#28155;&#21152;&#20114;&#21160;&#20851;&#31995;&#27880;&#37322;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;HID&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2307.00464</link><description>&lt;p&gt;
&#20154;&#19982;&#20154;&#20043;&#38388;&#30340;&#20114;&#21160;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Human-to-Human Interaction Detection. (arXiv:2307.00464v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00464
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#20154;&#19982;&#20154;&#20043;&#38388;&#30340;&#20114;&#21160;&#26816;&#27979;&#20219;&#21153;&#65292;&#36890;&#36807;&#19968;&#31181;&#27169;&#22411;&#23454;&#29616;&#20102;&#21516;&#26102;&#26816;&#27979;&#20027;&#20307;&#12289;&#35782;&#21035;&#20010;&#20154;&#21160;&#20316;&#21644;&#26681;&#25454;&#20114;&#21160;&#20851;&#31995;&#20998;&#32452;&#30340;&#30446;&#26631;&#12290;&#36890;&#36807;&#22312;&#29616;&#26377;&#30340;AVA&#25968;&#25454;&#38598;&#19978;&#28155;&#21152;&#20114;&#21160;&#20851;&#31995;&#27880;&#37322;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;HID&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#35270;&#39057;&#27969;&#20013;&#30340;&#20154;&#19982;&#20154;&#20043;&#38388;&#30340;&#20114;&#21160;&#34892;&#20026;&#65288;&#20363;&#22914;&#25490;&#38431;&#12289;&#25569;&#25163;&#12289;&#25171;&#26007;&#21644;&#36861;&#36880;&#65289;&#30340;&#20840;&#38754;&#29702;&#35299;&#65292;&#23545;&#20110;&#26657;&#22253;&#12289;&#24191;&#22330;&#21644;&#20844;&#22253;&#31561;&#22320;&#21306;&#30340;&#20844;&#20849;&#23433;&#20840;&#30417;&#25511;&#33267;&#20851;&#37325;&#35201;&#12290;&#19982;&#20351;&#29992;&#32534;&#25490;&#35270;&#39057;&#20316;&#20026;&#36755;&#20837;&#12289;&#24573;&#30053;&#24182;&#21457;&#20114;&#21160;&#32676;&#20307;&#65292;&#24182;&#23558;&#26816;&#27979;&#21644;&#35782;&#21035;&#20998;&#20026;&#19981;&#21516;&#38454;&#27573;&#30340;&#20256;&#32479;&#20154;&#38469;&#20114;&#21160;&#35782;&#21035;&#19981;&#21516;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31216;&#20026;&#20154;&#19982;&#20154;&#20043;&#38388;&#30340;&#20114;&#21160;&#26816;&#27979;&#65288;HID&#65289;&#30340;&#26032;&#20219;&#21153;&#12290;HID&#33268;&#21147;&#20110;&#22312;&#19968;&#20010;&#27169;&#22411;&#20013;&#26816;&#27979;&#20027;&#20307;&#12289;&#35782;&#21035;&#20010;&#20154;&#21160;&#20316;&#65292;&#24182;&#26681;&#25454;&#20182;&#20204;&#30340;&#20114;&#21160;&#20851;&#31995;&#23558;&#20154;&#21592;&#20998;&#32452;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#22522;&#20110;&#29992;&#20110;&#21160;&#20316;&#26816;&#27979;&#30340;&#27969;&#34892;AVA&#25968;&#25454;&#38598;&#65292;&#22312;&#36880;&#24103;&#30340;&#26041;&#24335;&#19978;&#28155;&#21152;&#20102;&#20114;&#21160;&#20851;&#31995;&#30340;&#27880;&#37322;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#21517;&#20026;AVA-Interaction&#65288;AVA-I&#65289;&#30340;&#26032;&#30340;HID&#22522;&#20934;&#12290;AVA-I&#21253;&#25324;85,254&#24103;&#21644;86,338&#20010;&#20114;&#21160;&#32676;&#20307;&#65292;&#27599;&#20010;&#22270;&#20687;&#26368;&#22810;&#21253;&#21547;4&#20010;&#24182;&#21457;&#20114;&#21160;&#32676;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
A comprehensive understanding of interested human-to-human interactions in video streams, such as queuing, handshaking, fighting and chasing, is of immense importance to the surveillance of public security in regions like campuses, squares and parks. Different from conventional human interaction recognition, which uses choreographed videos as inputs, neglects concurrent interactive groups, and performs detection and recognition in separate stages, we introduce a new task named human-to-human interaction detection (HID). HID devotes to detecting subjects, recognizing person-wise actions, and grouping people according to their interactive relations, in one model. First, based on the popular AVA dataset created for action detection, we establish a new HID benchmark, termed AVA-Interaction (AVA-I), by adding annotations on interactive relations in a frame-by-frame manner. AVA-I consists of 85,254 frames and 86,338 interactive groups, and each image includes up to 4 concurrent interactive g
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26102;&#38388;&#20248;&#21270;&#22810;&#26426;&#22120;&#20154;&#35206;&#30422;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#27169;&#22411;&#25552;&#20986;&#20102;&#19968;&#20010;&#26368;&#20248;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#30340;&#35206;&#30422;&#26102;&#38388;&#26368;&#22810;&#20026;&#26368;&#20248;&#35299;&#30340;&#22235;&#20493;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#20004;&#20010;&#26377;&#25928;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#26469;&#20943;&#23569;&#38382;&#39064;&#30340;&#35268;&#27169;&#65292;&#24182;&#36890;&#36807;&#27169;&#22411;&#20248;&#21270;&#26041;&#27861;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.17609</link><description>&lt;p&gt;
&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#29992;&#20110;&#39640;&#25928;&#21551;&#21457;&#24335;&#31639;&#27861;&#30340;&#26102;&#38388;&#20248;&#21270;&#22810;&#26426;&#22120;&#20154;&#35206;&#30422;&#36335;&#24452;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Mixed Integer Programming for Time-Optimal Multi-Robot Coverage Path Planning with Efficient Heuristics. (arXiv:2306.17609v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17609
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26102;&#38388;&#20248;&#21270;&#22810;&#26426;&#22120;&#20154;&#35206;&#30422;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#27169;&#22411;&#25552;&#20986;&#20102;&#19968;&#20010;&#26368;&#20248;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#30340;&#35206;&#30422;&#26102;&#38388;&#26368;&#22810;&#20026;&#26368;&#20248;&#35299;&#30340;&#22235;&#20493;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#20004;&#20010;&#26377;&#25928;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#26469;&#20943;&#23569;&#38382;&#39064;&#30340;&#35268;&#27169;&#65292;&#24182;&#36890;&#36807;&#27169;&#22411;&#20248;&#21270;&#26041;&#27861;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#26088;&#22312;&#26368;&#23567;&#21270;&#35206;&#30422;&#26102;&#38388;&#65288;&#23450;&#20041;&#20026;&#25152;&#26377;&#26426;&#22120;&#20154;&#30340;&#26368;&#22823;&#34892;&#31243;&#26102;&#38388;&#65289;&#30340;&#26410;&#21152;&#26435;&#21644;&#21152;&#26435;&#22320;&#24418;&#30340;&#26102;&#38388;&#20248;&#21270;&#22810;&#26426;&#22120;&#20154;&#35206;&#30422;&#36335;&#24452;&#35268;&#21010;&#65288;MCPP&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;MCPP&#20943;&#23569;&#21040;&#20102;&#26681;&#26368;&#23567;&#26368;&#22823;&#26641;&#35206;&#30422;&#65288;RMMTC&#65289;&#12290;&#39318;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#65288;MIP&#65289;&#27169;&#22411;&#26469;&#26368;&#20248;&#35299;&#20915;RMMTC&#38382;&#39064;&#65292;&#20174;&#32780;&#24471;&#21040;&#19968;&#20010;MCPP&#35299;&#20915;&#26041;&#26696;&#65292;&#20854;&#35206;&#30422;&#26102;&#38388;&#26368;&#22810;&#20026;&#26368;&#20248;&#35299;&#30340;&#22235;&#20493;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#27425;&#20248;&#20294;&#26377;&#25928;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#29992;&#20110;&#20943;&#23569;MIP&#27169;&#22411;&#20013;&#30340;&#21464;&#37327;&#25968;&#37327;&#65292;&#20174;&#32780;&#25552;&#39640;&#20854;&#22312;&#22823;&#35268;&#27169;MCPP&#23454;&#20363;&#19978;&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20004;&#31181;&#21551;&#21457;&#24335;&#31639;&#27861;&#23548;&#33268;&#20102;&#32553;&#23567;&#21518;&#30340;MIP&#27169;&#22411;&#65292;&#23545;&#20110;&#25152;&#26377;RMMTC&#23454;&#20363;&#26469;&#35828;&#20173;&#28982;&#26159;&#23436;&#25972;&#30340;&#65288;&#21363;&#20445;&#35777;&#25214;&#21040;&#35299;&#20915;&#26041;&#26696;&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#27169;&#22411;&#20248;&#21270;&#30340;&#28909;&#21551;&#21160;&#26041;&#27861;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#21407;&#22987;MIP&#27169;&#22411;&#21644;&#32553;&#23567;&#21518;&#30340;MIP&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate time-optimal Multi-Robot Coverage Path Planning (MCPP) for both unweighted and weighted terrains, which aims to minimize the coverage time, defined as the maximum travel time of all robots. Specifically, we focus on a reduction from MCPP to Rooted Min-Max Tree Cover (RMMTC). For the first time, we propose a Mixed Integer Programming (MIP) model to optimally solve RMMTC, resulting in an MCPP solution with a coverage time that is provably at most four times the optimal. Moreover, we propose two suboptimal yet effective heuristics that reduce the number of variables in the MIP model, thus improving its efficiency for large-scale MCPP instances. We show that both heuristics result in reduced-size MIP models that remain complete (i.e., guarantee to find a solution if one exists) for all RMMTC instances. Additionally, we explore the use of model optimization warm-startup to further improve the efficiency of both the original MIP model and the reduced-size MIP models. We valida
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22256;&#38590;&#26679;&#26412;&#25366;&#25496;&#30340;&#30417;&#30563;&#23545;&#27604;&#29305;&#24449;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#39118;&#21147;&#21457;&#30005;&#26426;&#26728;&#21494;&#31995;&#32479;&#25925;&#38556;&#35786;&#26029;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20313;&#24358;&#30456;&#20284;&#24230;&#35782;&#21035;&#22256;&#38590;&#26679;&#26412;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;&#22256;&#38590;&#26679;&#26412;&#23545;&#26469;&#23398;&#20064;&#26356;&#20855;&#21306;&#20998;&#24615;&#30340;&#34920;&#31034;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#22810;&#23618;&#24863;&#30693;&#26426;&#30340;&#35757;&#32451;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.14701</link><description>&lt;p&gt;
&#22522;&#20110;&#22256;&#38590;&#26679;&#26412;&#25366;&#25496;&#30340;&#30417;&#30563;&#23545;&#27604;&#29305;&#24449;&#23398;&#20064;&#29992;&#20110;&#39118;&#21147;&#21457;&#30005;&#26426;&#26728;&#21494;&#31995;&#32479;&#25925;&#38556;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Hard Sample Mining Enabled Supervised Contrastive Feature Learning for Wind Turbine Pitch System Fault Diagnosis. (arXiv:2306.14701v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14701
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22256;&#38590;&#26679;&#26412;&#25366;&#25496;&#30340;&#30417;&#30563;&#23545;&#27604;&#29305;&#24449;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#39118;&#21147;&#21457;&#30005;&#26426;&#26728;&#21494;&#31995;&#32479;&#25925;&#38556;&#35786;&#26029;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20313;&#24358;&#30456;&#20284;&#24230;&#35782;&#21035;&#22256;&#38590;&#26679;&#26412;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;&#22256;&#38590;&#26679;&#26412;&#23545;&#26469;&#23398;&#20064;&#26356;&#20855;&#21306;&#20998;&#24615;&#30340;&#34920;&#31034;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#22810;&#23618;&#24863;&#30693;&#26426;&#30340;&#35757;&#32451;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39118;&#21147;&#21457;&#30005;&#26426;&#26377;&#25928;&#21033;&#29992;&#39118;&#33021;&#20381;&#36182;&#20110;&#20854;&#26728;&#21494;&#31995;&#32479;&#33021;&#22815;&#26681;&#25454;&#39118;&#36895;&#21464;&#21270;&#35843;&#25972;&#26728;&#21494;&#35282;&#24230;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38271;&#26399;&#30952;&#25439;&#23548;&#33268;&#30340;&#26728;&#21494;&#31995;&#32479;&#20013;&#23384;&#22312;&#22810;&#31181;&#20581;&#24247;&#38382;&#39064;&#65292;&#32473;&#20934;&#30830;&#20998;&#31867;&#36896;&#25104;&#20102;&#25361;&#25112;&#65292;&#36827;&#32780;&#22686;&#21152;&#20102;&#39118;&#21147;&#21457;&#30005;&#26426;&#30340;&#32500;&#25252;&#25104;&#26412;&#29978;&#33267;&#21487;&#33021;&#25439;&#22351;&#23427;&#20204;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22256;&#38590;&#26679;&#26412;&#25366;&#25496;&#30340;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#65288;HSMSCL&#65289;&#30340;&#26032;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20313;&#24358;&#30456;&#20284;&#24230;&#35782;&#21035;&#22256;&#38590;&#26679;&#26412;&#65292;&#28982;&#21518;&#21033;&#29992;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#36890;&#36807;&#26500;&#24314;&#22256;&#38590;&#26679;&#26412;&#23545;&#26469;&#23398;&#20064;&#26356;&#20855;&#21306;&#20998;&#24615;&#30340;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#20013;&#30340;&#22256;&#38590;&#26679;&#26412;&#25366;&#25496;&#26694;&#26550;&#36824;&#29992;&#23398;&#21040;&#30340;&#34920;&#31034;&#26500;&#24314;&#22256;&#38590;&#26679;&#26412;&#65292;&#20351;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;MLP&#65289;&#30340;&#35757;&#32451;&#36807;&#31243;&#26356;&#20855;&#25361;&#25112;&#24615;&#24182;&#20351;&#20854;&#25104;&#20026;&#26356;&#26377;&#25928;&#30340;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
The efficient utilization of wind power by wind turbines relies on the ability of their pitch systems to adjust blade pitch angles in response to varying wind speeds. However, the presence of multiple health conditions in the pitch system due to the long-term wear and tear poses challenges in accurately classifying them, thus increasing the maintenance cost of wind turbines or even damaging them. This paper proposes a novel method based on hard sample mining-enabled supervised contrastive learning (HSMSCL) to address this problem. The proposed method employs cosine similarity to identify hard samples and subsequently, leverages supervised contrastive learning to learn more discriminative representations by constructing hard sample pairs. Furthermore, the hard sample mining framework in the proposed method also constructs hard samples with learned representations to make the training process of the multilayer perceptron (MLP) more challenging and make it a more effective classifier. The
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;ChatGPT&#25552;&#31034;&#24320;&#21457;&#25945;&#32946;&#32842;&#22825;&#26426;&#22120;&#20154;&#20855;&#26377;&#28508;&#21147;&#65292;&#33021;&#22815;&#23454;&#29616;&#20114;&#21160;&#21644;&#20010;&#24615;&#21270;&#23398;&#20064;&#20307;&#39564;&#12290;&#36890;&#36807;&#22312;&#31038;&#20132;&#23186;&#20307;&#32032;&#20859;&#26696;&#20363;&#20013;&#36827;&#34892;&#21021;&#27493;&#27979;&#35797;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#28151;&#21512;&#23545;&#35805;&#26426;&#22120;&#20154;&#20132;&#20114;&#30340;&#35265;&#35299;&#21644;&#21021;&#27493;&#25351;&#21335;&#12290;&#36825;&#20123;&#26426;&#22120;&#20154;&#20855;&#26377;&#36866;&#24212;&#24615;&#24378;&#12289;&#22810;&#26679;&#21270;&#30340;&#25945;&#32946;&#31574;&#30053;&#21644;&#23545;&#35805;&#39118;&#26684;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.10645</link><description>&lt;p&gt;
&#20351;&#29992;ChatGPT&#25552;&#31034;&#24320;&#21457;&#26377;&#25928;&#30340;&#25945;&#32946;&#32842;&#22825;&#26426;&#22120;&#20154;&#65306;&#22522;&#20110;&#31038;&#20132;&#23186;&#20307;&#32032;&#20859;&#26696;&#20363;&#21021;&#27493;&#27979;&#35797;&#30340;insights&#65288;&#21547;&#38468;&#24405;&#65289;
&lt;/p&gt;
&lt;p&gt;
Developing Effective Educational Chatbots with ChatGPT prompts: Insights from Preliminary Tests in a Case Study on Social Media Literacy (with appendix). (arXiv:2306.10645v2 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10645
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;ChatGPT&#25552;&#31034;&#24320;&#21457;&#25945;&#32946;&#32842;&#22825;&#26426;&#22120;&#20154;&#20855;&#26377;&#28508;&#21147;&#65292;&#33021;&#22815;&#23454;&#29616;&#20114;&#21160;&#21644;&#20010;&#24615;&#21270;&#23398;&#20064;&#20307;&#39564;&#12290;&#36890;&#36807;&#22312;&#31038;&#20132;&#23186;&#20307;&#32032;&#20859;&#26696;&#20363;&#20013;&#36827;&#34892;&#21021;&#27493;&#27979;&#35797;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#28151;&#21512;&#23545;&#35805;&#26426;&#22120;&#20154;&#20132;&#20114;&#30340;&#35265;&#35299;&#21644;&#21021;&#27493;&#25351;&#21335;&#12290;&#36825;&#20123;&#26426;&#22120;&#20154;&#20855;&#26377;&#36866;&#24212;&#24615;&#24378;&#12289;&#22810;&#26679;&#21270;&#30340;&#25945;&#32946;&#31574;&#30053;&#21644;&#23545;&#35805;&#39118;&#26684;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25945;&#32946;&#32842;&#22825;&#26426;&#22120;&#20154;&#25215;&#35834;&#25552;&#20379;&#20114;&#21160;&#21644;&#20010;&#24615;&#21270;&#23398;&#20064;&#20307;&#39564;&#65292;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#21457;&#23637;&#21463;&#21040;&#20102;&#29616;&#26377;&#24179;&#21488;&#21463;&#38480;&#30340;&#33258;&#30001;&#20114;&#21160;&#33021;&#21147;&#21644;&#30693;&#35782;&#20197;&#36866;&#23452;&#26684;&#24335;&#32534;&#30721;&#30340;&#22256;&#38590;&#30340;&#38480;&#21046;&#12290;&#26368;&#36817;&#30340;&#35821;&#35328;&#23398;&#20064;&#27169;&#22411;&#30340;&#36827;&#23637;&#34920;&#26126;&#65292;&#20351;&#29992;&#22522;&#20110;&#25552;&#31034;&#30340;&#26041;&#27861;&#24320;&#21457;&#25945;&#32946;&#32842;&#22825;&#26426;&#22120;&#20154;&#26159;&#19968;&#20010;&#26032;&#30340;&#21487;&#33021;&#24615;&#65292;&#22914;ChatGPT&#31561;&#20855;&#26377;&#38646;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#21033;&#29992;&#31616;&#21333;&#31995;&#32479;&#23454;&#29616;&#28151;&#21512;&#23545;&#35805;&#26426;&#22120;&#20154;&#20132;&#20114;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#24182;&#35752;&#35770;&#20102;&#20174;&#21021;&#22987;&#27979;&#35797;&#20013;&#33719;&#24471;&#30340;&#35265;&#35299;&#21644;&#21021;&#27493;&#25351;&#21335;&#12290;&#25105;&#20204;&#26816;&#26597;&#20102;ChatGPT&#36861;&#27714;&#22810;&#20010;&#30456;&#20114;&#20851;&#32852;&#30340;&#23398;&#20064;&#30446;&#26631;&#30340;&#33021;&#21147;&#65292;&#26681;&#25454;&#29992;&#25143;&#30340;&#29305;&#24449;&#65288;&#22914;&#25991;&#21270;&#12289;&#24180;&#40836;&#21644;&#25945;&#32946;&#27700;&#24179;&#65289;&#26469;&#35843;&#25972;&#25945;&#32946;&#27963;&#21160;&#30340;&#33021;&#21147;&#65292;&#20197;&#21450;&#20351;&#29992;&#22810;&#31181;&#25945;&#32946;&#31574;&#30053;&#21644;&#23545;&#35805;&#39118;&#26684;&#30340;&#33021;&#21147;&#12290;&#23613;&#31649;&#32467;&#26524;&#26159;&#20196;&#20154;&#40723;&#33310;&#30340;&#65292;&#20294;&#30001;&#20110;&#21463;&#21040;&#32500;&#25252;&#30340;&#21382;&#21490;&#25968;&#25454;&#30340;&#38480;&#21046;&#65292;&#20063;&#38754;&#20020;&#30528;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Educational chatbots come with a promise of interactive and personalized learning experiences, yet their development has been limited by the restricted free interaction capabilities of available platforms and the difficulty of encoding knowledge in a suitable format. Recent advances in language learning models with zero-shot learning capabilities, such as ChatGPT, suggest a new possibility for developing educational chatbots using a prompt-based approach. We present a case study with a simple system that enables mixed-turn chatbot interactions and discuss the insights and preliminary guidelines obtained from initial tests. We examine ChatGPT's ability to pursue multiple interconnected learning objectives, adapt the educational activity to users' characteristics, such as culture, age, and level of education, and its ability to use diverse educational strategies and conversational styles. Although the results are encouraging, challenges are posed by the limited history maintained for the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;Transformer&#22312;&#20855;&#26377;&#21333;&#23618;&#32447;&#24615;&#33258;&#27880;&#24847;&#23618;&#30340;&#32447;&#24615;&#22238;&#24402;&#20219;&#21153;&#19978;&#36890;&#36807;&#26799;&#24230;&#27969;&#36827;&#34892;&#35757;&#32451;&#30340;ICL&#26426;&#21046;&#65292;&#25581;&#31034;&#20102;&#26799;&#24230;&#27969;&#20855;&#26377;&#25214;&#21040;&#30446;&#26631;&#20989;&#25968;&#20840;&#23616;&#26368;&#23567;&#20540;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.09927</link><description>&lt;p&gt;
&#35757;&#32451;&#22909;&#30340;Transformer&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#32447;&#24615;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Trained Transformers Learn Linear Models In-Context. (arXiv:2306.09927v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09927
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Transformer&#22312;&#20855;&#26377;&#21333;&#23618;&#32447;&#24615;&#33258;&#27880;&#24847;&#23618;&#30340;&#32447;&#24615;&#22238;&#24402;&#20219;&#21153;&#19978;&#36890;&#36807;&#26799;&#24230;&#27969;&#36827;&#34892;&#35757;&#32451;&#30340;ICL&#26426;&#21046;&#65292;&#25581;&#31034;&#20102;&#26799;&#24230;&#27969;&#20855;&#26377;&#25214;&#21040;&#30446;&#26631;&#20989;&#25968;&#20840;&#23616;&#26368;&#23567;&#20540;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#20363;&#22914;Transformers&#65292;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#65306;&#32473;&#23450;&#19968;&#20010;&#26469;&#33258;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#30340;&#30701;&#35821;&#24207;&#21015;&#30340;&#25552;&#31034;&#65292;&#23427;&#20204;&#21487;&#20197;&#21046;&#23450;&#30456;&#20851;&#30340;&#27599;&#20010;&#20196;&#29260;&#21644;&#19979;&#19968;&#20010;&#20196;&#29260;&#30340;&#39044;&#27979;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#21442;&#25968;&#26356;&#26032;&#12290;&#36890;&#36807;&#23558;&#26631;&#35760;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#26410;&#26631;&#35760;&#30340;&#27979;&#35797;&#25968;&#25454;&#24207;&#21015;&#23884;&#20837;&#21040;&#25552;&#31034;&#20013;&#65292;&#36825;&#20351;&#24471;Transformer&#34920;&#29616;&#24471;&#20687;&#26377;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#12290;&#20107;&#23454;&#19978;&#65292;&#26368;&#36817;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#22312;&#38543;&#26426;&#23454;&#20363;&#19978;&#35757;&#32451;Transformer&#20307;&#31995;&#32467;&#26500;&#30340;&#32447;&#24615;&#22238;&#24402;&#38382;&#39064;&#26102;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#39044;&#27979;&#20250;&#27169;&#20223;&#26222;&#36890;&#26368;&#23567;&#20108;&#20056;&#27861;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attention-based neural networks such as transformers have demonstrated a remarkable ability to exhibit in-context learning (ICL): Given a short prompt sequence of tokens from an unseen task, they can formulate relevant per-token and next-token predictions without any parameter updates. By embedding a sequence of labeled training data and unlabeled test data as a prompt, this allows for transformers to behave like supervised learning algorithms. Indeed, recent work has shown that when training transformer architectures over random instances of linear regression problems, these models' predictions mimic those of ordinary least squares.  Towards understanding the mechanisms underlying this phenomenon, we investigate the dynamics of ICL in transformers with a single linear self-attention layer trained by gradient flow on linear regression tasks. We show that despite non-convexity, gradient flow with a suitable random initialization finds a global minimum of the objective function. At this 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#35774;&#35745;&#22522;&#30784;&#65292;&#21363;&#20854;&#19977;&#20010;&#20851;&#38190;&#32452;&#20214;&#65306;&#27491;&#21521;&#36807;&#31243;&#12289;&#36870;&#21521;&#36807;&#31243;&#21644;&#37319;&#26679;&#36807;&#31243;&#65292;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#30410;&#30340;&#32454;&#31890;&#24230;&#36879;&#35270;&#12290;</title><link>http://arxiv.org/abs/2306.04542</link><description>&lt;p&gt;
&#20851;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#35774;&#35745;&#22522;&#30784;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
On the Design Fundamentals of Diffusion Models: A Survey. (arXiv:2306.04542v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04542
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#35774;&#35745;&#22522;&#30784;&#65292;&#21363;&#20854;&#19977;&#20010;&#20851;&#38190;&#32452;&#20214;&#65306;&#27491;&#21521;&#36807;&#31243;&#12289;&#36870;&#21521;&#36807;&#31243;&#21644;&#37319;&#26679;&#36807;&#31243;&#65292;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#30410;&#30340;&#32454;&#31890;&#24230;&#36879;&#35270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26159;&#19968;&#31181;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#36880;&#28176;&#28155;&#21152;&#21644;&#21024;&#38500;&#22122;&#22768;&#26469;&#23398;&#20064;&#35757;&#32451;&#25968;&#25454;&#30340;&#28508;&#22312;&#20998;&#24067;&#20197;&#29983;&#25104;&#25968;&#25454;&#12290;&#25193;&#25955;&#27169;&#22411;&#30340;&#32452;&#25104;&#37096;&#20998;&#24050;&#32463;&#21463;&#21040;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#65292;&#35768;&#22810;&#35774;&#35745;&#36873;&#25321;&#34987;&#25552;&#20986;&#12290;&#29616;&#26377;&#30340;&#35780;&#35770;&#20027;&#35201;&#20851;&#27880;&#39640;&#23618;&#27425;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23545;&#32452;&#20214;&#30340;&#35774;&#35745;&#22522;&#30784;&#35206;&#30422;&#36739;&#23569;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#20840;&#38754;&#32780;&#36830;&#36143;&#30340;&#32508;&#36848;&#65292;&#38024;&#23545;&#25193;&#25955;&#27169;&#22411;&#30340;&#32452;&#20214;&#35774;&#35745;&#36873;&#25321;&#36827;&#34892;&#20998;&#26512;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#32508;&#36848;&#25353;&#29031;&#19977;&#20010;&#20851;&#38190;&#32452;&#20214;&#36827;&#34892;&#32452;&#32455;&#65292;&#21363;&#27491;&#21521;&#36807;&#31243;&#12289;&#36870;&#21521;&#36807;&#31243;&#21644;&#37319;&#26679;&#36807;&#31243;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#21487;&#20197;&#25552;&#20379;&#25193;&#25955;&#27169;&#22411;&#30340;&#32454;&#31890;&#24230;&#36879;&#35270;&#65292;&#26377;&#21161;&#20110;&#26410;&#26469;&#30740;&#31350;&#20998;&#26512;&#20010;&#20307;&#32452;&#20214;&#12289;&#35774;&#35745;&#36873;&#25321;&#30340;&#36866;&#29992;&#24615;&#20197;&#21450;&#25193;&#25955;&#27169;&#22411;&#30340;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models are generative models, which gradually add and remove noise to learn the underlying distribution of training data for data generation. The components of diffusion models have gained significant attention with many design choices proposed. Existing reviews have primarily focused on higher-level solutions, thereby covering less on the design fundamentals of components. This study seeks to address this gap by providing a comprehensive and coherent review on component-wise design choices in diffusion models. Specifically, we organize this review according to their three key components, namely the forward process, the reverse process, and the sampling procedure. This allows us to provide a fine-grained perspective of diffusion models, benefiting future studies in the analysis of individual components, the applicability of design choices, and the implementation of diffusion models.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#36710;&#36947;&#26816;&#27979;&#27969;&#27700;&#32447;&#65292;&#35813;&#27969;&#27700;&#32447;&#21253;&#25324;&#33258;&#39044;&#35757;&#32451;&#25513;&#27169;&#24207;&#21015;&#33258;&#32534;&#30721;&#22120;&#21644;&#20351;&#29992;&#23450;&#21046;PolyLoss&#24494;&#35843;&#30340;&#31471;&#21040;&#31471;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#25513;&#27169;&#24207;&#21015;&#33258;&#32534;&#30721;&#22120;&#34987;&#37319;&#29992;&#20197;&#36890;&#36807;&#37325;&#26500;&#38543;&#26426;&#25513;&#33180;&#22270;&#20687;&#20013;&#30340;&#20002;&#22833;&#20687;&#32032;&#20026;&#30446;&#26631;&#26469;&#39044;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#25552;&#21319;&#20102;&#36710;&#36947;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.17271</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#39044;&#35757;&#32451;&#25513;&#27169;&#24207;&#21015;&#33258;&#32534;&#30721;&#22120;&#21644;&#20351;&#29992;&#23450;&#21046;PolyLoss&#24494;&#35843;&#30340;&#26041;&#27861;&#23454;&#29616;&#40065;&#26834;&#36710;&#36947;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Robust Lane Detection through Self Pre-training with Masked Sequential Autoencoders and Fine-tuning with Customized PolyLoss. (arXiv:2305.17271v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17271
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#36710;&#36947;&#26816;&#27979;&#27969;&#27700;&#32447;&#65292;&#35813;&#27969;&#27700;&#32447;&#21253;&#25324;&#33258;&#39044;&#35757;&#32451;&#25513;&#27169;&#24207;&#21015;&#33258;&#32534;&#30721;&#22120;&#21644;&#20351;&#29992;&#23450;&#21046;PolyLoss&#24494;&#35843;&#30340;&#31471;&#21040;&#31471;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#25513;&#27169;&#24207;&#21015;&#33258;&#32534;&#30721;&#22120;&#34987;&#37319;&#29992;&#20197;&#36890;&#36807;&#37325;&#26500;&#38543;&#26426;&#25513;&#33180;&#22270;&#20687;&#20013;&#30340;&#20002;&#22833;&#20687;&#32032;&#20026;&#30446;&#26631;&#26469;&#39044;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#25552;&#21319;&#20102;&#36710;&#36947;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36710;&#36947;&#26816;&#27979;&#26159;&#36710;&#36742;&#23450;&#20301;&#30340;&#20851;&#38190;&#65292;&#26159;&#23454;&#29616;&#33258;&#21160;&#39550;&#39542;&#21644;&#35768;&#22810;&#26234;&#33021;&#39640;&#32423;&#39550;&#39542;&#36741;&#21161;&#31995;&#32479;&#30340;&#22522;&#30784;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#35270;&#35273;&#30340;&#36710;&#36947;&#26816;&#27979;&#26041;&#27861;&#26410;&#20805;&#20998;&#21033;&#29992;&#26377;&#20215;&#20540;&#30340;&#29305;&#24449;&#21644;&#32858;&#21512;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#23588;&#20854;&#26159;&#36710;&#36947;&#32447;&#21644;&#22270;&#20687;&#20013;&#20854;&#20182;&#21306;&#22495;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#24182;&#25552;&#21319;&#36710;&#36947;&#26816;&#27979;&#24615;&#33021;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27969;&#27700;&#32447;&#65292;&#20854;&#20013;&#21253;&#25324;&#20351;&#29992;&#33258;&#39044;&#35757;&#32451;&#25513;&#27169;&#24207;&#21015;&#33258;&#32534;&#30721;&#22120;&#21644;&#20351;&#29992;&#23450;&#21046;PolyLoss&#24494;&#35843;&#30340;&#31471;&#21040;&#31471;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#25513;&#27169;&#24207;&#21015;&#33258;&#32534;&#30721;&#22120;&#34987;&#37319;&#29992;&#20197;&#36890;&#36807;&#37325;&#26500;&#38543;&#26426;&#25513;&#27169;&#22270;&#20687;&#20013;&#30340;&#20002;&#22833;&#20687;&#32032;&#20026;&#30446;&#26631;&#26469;&#39044;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#28982;&#21518;&#65292;&#22312;&#32454;&#35843;&#20998;&#21106;&#38454;&#27573;&#20013;&#65292;&#36830;&#32493;&#30340;&#22270;&#20687;&#24103;&#34987;&#29992;&#20316;&#36755;&#20837;&#65292;
&lt;/p&gt;
&lt;p&gt;
Lane detection is crucial for vehicle localization which makes it the foundation for automated driving and many intelligent and advanced driving assistant systems. Available vision-based lane detection methods do not make full use of the valuable features and aggregate contextual information, especially the interrelationships between lane lines and other regions of the images in continuous frames. To fill this research gap and upgrade lane detection performance, this paper proposes a pipeline consisting of self pre-training with masked sequential autoencoders and fine-tuning with customized PolyLoss for the end-to-end neural network models using multi-continuous image frames. The masked sequential autoencoders are adopted to pre-train the neural network models with reconstructing the missing pixels from a random masked image as the objective. Then, in the fine-tuning segmentation phase where lane detection segmentation is performed, the continuous image frames are served as the inputs,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#65288;OT&#65289;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;DA&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#23884;&#20837;&#31354;&#38388;&#65292;&#20351;&#24471;OT&#38382;&#39064;&#30340;&#35299;&#26159;&#26368;&#20248;&#19988;&#35745;&#31639;&#37327;&#36739;&#23569;&#30340;&#65292;&#36866;&#29992;&#20110;&#21516;&#36136;&#21644;&#24322;&#36136;&#30340;DA&#35774;&#32622;&#12290;</title><link>http://arxiv.org/abs/2305.07500</link><description>&lt;p&gt;
&#36229;&#36234;&#19981;&#21464;&#34920;&#31034;&#23398;&#20064;&#65306;&#32447;&#24615;&#21487;&#23545;&#40784;&#30340;&#28508;&#22312;&#31354;&#38388;&#29992;&#20110;&#39640;&#25928;&#38381;&#21512;&#24418;&#24335;&#39046;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Beyond invariant representation learning: linearly alignable latent spaces for efficient closed-form domain adaptation. (arXiv:2305.07500v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07500
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#65288;OT&#65289;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;DA&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#23884;&#20837;&#31354;&#38388;&#65292;&#20351;&#24471;OT&#38382;&#39064;&#30340;&#35299;&#26159;&#26368;&#20248;&#19988;&#35745;&#31639;&#37327;&#36739;&#23569;&#30340;&#65292;&#36866;&#29992;&#20110;&#21516;&#36136;&#21644;&#24322;&#36136;&#30340;DA&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20248;&#36755;&#36816;&#65288;OT&#65289;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#20960;&#20309;&#24037;&#20855;&#65292;&#29992;&#20110;&#27604;&#36739;&#21644;&#23545;&#40784;&#27010;&#29575;&#27979;&#24230;&#65292;&#36981;&#24490;&#26368;&#23567;&#21162;&#21147;&#21407;&#21017;&#12290;&#22312;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#20013;&#65292;OT&#30340;&#35768;&#22810;&#25104;&#21151;&#24212;&#29992;&#20043;&#19968;&#26159;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;DA&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#30740;&#31350;&#39046;&#22495;&#65292;&#20854;&#30446;&#26631;&#26159;&#23558;&#20998;&#31867;&#22120;&#20174;&#19968;&#20010;&#24102;&#26631;&#31614;&#30340;&#39046;&#22495;&#36716;&#31227;&#21040;&#21478;&#19968;&#20010;&#31867;&#20284;&#20294;&#19981;&#21516;&#30340;&#26410;&#26631;&#35760;&#25110;&#31232;&#30095;&#26631;&#35760;&#30340;&#39046;&#22495;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#22522;&#20110;OT&#30340;DA&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#30001;&#20223;&#23556;&#26144;&#23556;&#32473;&#20986;&#30340;OT&#38382;&#39064;&#30340;&#38381;&#24335;&#35299;&#65292;&#24182;&#23398;&#20064;&#20102;&#19968;&#20010;&#23884;&#20837;&#31354;&#38388;&#65292;&#20351;&#24471;&#35813;&#35299;&#26159;&#26368;&#20248;&#19988;&#35745;&#31639;&#37327;&#36739;&#23569;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#21516;&#36136;&#21644;&#24322;&#36136;&#30340;DA&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimal transport (OT) is a powerful geometric tool used to compare and align probability measures following the least effort principle. Among many successful applications of OT in machine learning (ML), domain adaptation (DA) -- a field of study where the goal is to transfer a classifier from one labelled domain to another similar, yet different unlabelled or scarcely labelled domain -- has been historically among the most investigated ones. This success is due to the ability of OT to provide both a meaningful discrepancy measure to assess the similarity of two domains' distributions and a mapping that can project source domain data onto the target one. In this paper, we propose a principally new OT-based approach applied to DA that uses the closed-form solution of the OT problem given by an affine mapping and learns an embedding space for which this solution is optimal and computationally less complex. We show that our approach works in both homogeneous and heterogeneous DA settings 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22240;&#26524;&#24378;&#24230;&#21464;&#20998;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#20174;&#19981;&#30830;&#23450;&#25968;&#25454;&#20013;&#24674;&#22797;&#22240;&#26524;&#20851;&#31995;&#23384;&#22312;&#30340;&#20302;&#26679;&#26412;&#21033;&#29992;&#29575;&#21644;&#20998;&#24067;&#20551;&#35774;&#26080;&#33021;&#21147;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#28508;&#22312;&#28151;&#28102;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2305.02640</link><description>&lt;p&gt;
&#23398;&#20064;&#22312;&#23384;&#22312;&#38544;&#24615;&#28151;&#28102;&#22240;&#32032;&#30340;&#24773;&#20917;&#19979;&#20174;&#19981;&#30830;&#23450;&#25968;&#25454;&#20013;&#24674;&#22797;&#22240;&#26524;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Learning to Recover Causal Relationship from Indefinite Data in the Presence of Latent Confounders. (arXiv:2305.02640v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02640
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22240;&#26524;&#24378;&#24230;&#21464;&#20998;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#20174;&#19981;&#30830;&#23450;&#25968;&#25454;&#20013;&#24674;&#22797;&#22240;&#26524;&#20851;&#31995;&#23384;&#22312;&#30340;&#20302;&#26679;&#26412;&#21033;&#29992;&#29575;&#21644;&#20998;&#24067;&#20551;&#35774;&#26080;&#33021;&#21147;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#28508;&#22312;&#28151;&#28102;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20855;&#26377;&#28508;&#22312;&#21464;&#37327;&#30340;&#22240;&#26524;&#21457;&#29616;&#20013;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#20004;&#20010;&#25968;&#25454;&#33539;&#24335;&#65306;&#30830;&#23450;&#25968;&#25454;&#65306;&#20855;&#26377;&#35266;&#23519;&#33410;&#28857;&#21333;&#20540;&#30340;&#21333;&#20010;&#39592;&#26550;&#32467;&#26500;&#65292;&#21644;&#19981;&#30830;&#23450;&#25968;&#25454;&#65306;&#20855;&#26377;&#35266;&#23519;&#33410;&#28857;&#22810;&#20540;&#30340;&#19968;&#32452;&#22810;&#39592;&#26550;&#32467;&#26500;&#12290;&#22810;&#20010;&#39592;&#26550;&#24341;&#20837;&#20302;&#26679;&#26412;&#21033;&#29992;&#29575;&#65292;&#22810;&#20010;&#20540;&#24341;&#20837;&#20102;&#20998;&#24067;&#20551;&#35774;&#30340;&#26080;&#33021;&#21147;&#65292;&#36825;&#20004;&#32773;&#23548;&#33268;&#20174;&#19981;&#30830;&#23450;&#25968;&#25454;&#20013;&#24674;&#22797;&#22240;&#26524;&#20851;&#31995;&#33267;&#20170;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#22240;&#26524;&#24378;&#24230;&#21464;&#20998;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#21033;&#29992;&#22240;&#26524;&#24378;&#24230;&#32780;&#19981;&#26159;&#29420;&#31435;&#22122;&#22768;&#20316;&#20026;&#28508;&#21464;&#37327;&#26469;&#35843;&#33410;&#35777;&#25454;&#19979;&#30028;&#12290;&#36890;&#36807;&#36825;&#31181;&#35774;&#35745;&#24605;&#24819;&#65292;&#19981;&#21516;&#39592;&#26550;&#30340;&#22240;&#26524;&#24378;&#24230;&#34987;&#30475;&#20316;&#26159;&#19968;&#20010;&#20998;&#24067;&#65292;&#24182;&#21487;&#20197;&#34920;&#31034;&#20026;&#21333;&#20540;&#22240;&#26524;&#22270;&#30697;&#38453;&#12290;&#27492;&#22806;&#65292;&#32771;&#34385;&#21040;&#28508;&#22312;&#28151;&#28102;&#22240;&#32032;&#65292;&#25105;&#20204;&#23558;&#22240;&#26524;&#22270;G&#20998;&#35299;&#20026;&#20004;&#20010;&#30456;&#20851;&#23376;&#22270;O&#21644;C&#12290;O&#21253;&#21547;&#35266;&#23519;&#33410;&#28857;&#20043;&#38388;&#30340;&#32431;&#20851;&#31995;&#65292;&#32780;C&#34920;&#31034;&#28151;&#28102;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Causal Discovery with latent variables, We define two data paradigms: definite data: a single-skeleton structure with observed nodes single-value, and indefinite data: a set of multi-skeleton structures with observed nodes multi-value. Multi,skeletons induce low sample utilization and multi values induce incapability of the distribution assumption, both leading that recovering causal relations from indefinite data is, as of yet, largely unexplored. We design the causal strength variational model to settle down these two problems. Specifically, we leverage the causal strength instead of independent noise as latent variable to mediate evidence lower bound. By this design ethos, The causal strength of different skeletons is regarded as a distribution and can be expressed as a single-valued causal graph matrix. Moreover, considering the latent confounders, we disentangle the causal graph G into two relatisubgraphs O and C. O contains pure relations between observed nodes, while C repres
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AceCoder&#30340;&#20195;&#30721;&#29983;&#25104;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#65292;&#19982;&#26631;&#20934;&#19978;&#19979;&#25991;&#23398;&#20064;&#30456;&#27604;&#65292;&#23427;&#36890;&#36807;&#31034;&#20363;&#26816;&#32034;&#21644;&#24341;&#23548;&#20195;&#30721;&#29983;&#25104;&#26469;&#25552;&#39640;&#29983;&#25104;&#20195;&#30721;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.17780</link><description>&lt;p&gt;
&#12298;&#22686;&#24378;&#19978;&#19979;&#25991;&#23398;&#20064;&#25552;&#39640;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#12299;
&lt;/p&gt;
&lt;p&gt;
Towards Enhancing In-Context Learning for Code Generation. (arXiv:2303.17780v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AceCoder&#30340;&#20195;&#30721;&#29983;&#25104;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#65292;&#19982;&#26631;&#20934;&#19978;&#19979;&#25991;&#23398;&#20064;&#30456;&#27604;&#65292;&#23427;&#36890;&#36807;&#31034;&#20363;&#26816;&#32034;&#21644;&#24341;&#23548;&#20195;&#30721;&#29983;&#25104;&#26469;&#25552;&#39640;&#29983;&#25104;&#20195;&#30721;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#39044;&#20808;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#24050;&#32463;&#22312;&#20195;&#30721;&#29983;&#25104;&#39046;&#22495;&#34920;&#29616;&#20986;&#20102;&#24378;&#22823;&#30340;&#25104;&#21151;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#26080;&#38656;&#35757;&#32451;&#65292;&#27169;&#22411;&#21482;&#38656;&#35201;&#36755;&#20837;&#19968;&#20010;&#30001;&#23569;&#37327;&#38656;&#27714;-&#20195;&#30721;&#31034;&#20363;&#21644;&#19968;&#20010;&#26032;&#38656;&#27714;&#32452;&#25104;&#30340;&#25552;&#31034;&#65292;&#23601;&#33021;&#29983;&#25104;&#20986;&#26032;&#30340;&#31243;&#24207;&#12290;&#20294;&#26159;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#20165;&#20165;&#23558;&#19978;&#19979;&#25991;&#23398;&#20064;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65292;&#24573;&#30053;&#20102;&#20195;&#30721;&#29983;&#25104;&#30340;&#29420;&#29305;&#29305;&#24615;&#12290;&#25105;&#20204;&#31216;&#36825;&#20123;&#30740;&#31350;&#20026;&#26631;&#20934;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#20154;&#31867;&#32534;&#30721;&#36807;&#31243;&#30340;&#35266;&#23519;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21517;&#20026;AceCoder&#30340;&#20195;&#30721;&#29983;&#25104;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#12290;&#19982;&#26631;&#20934;&#19978;&#19979;&#25991;&#23398;&#20064;&#30456;&#27604;&#65292;AceCoder&#26377;&#20004;&#20010;&#26032;&#39062;&#20043;&#22788;&#12290;(1)&#31034;&#20363;&#26816;&#32034;&#12290;&#23427;&#26816;&#32034;&#31867;&#20284;&#31243;&#24207;&#20316;&#20026;&#31034;&#20363;&#65292;&#24182;&#20174;&#20013;&#23398;&#20064;&#32534;&#31243;&#25216;&#33021;(&#22914;&#31639;&#27861;&#12289;API)&#12290;(2)&#24341;&#23548;&#20195;&#30721;&#29983;&#25104;&#12290;&#23427;&#40723;&#21169;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20013;&#38388;&#39044;&#22791;&#20195;&#30721;(&#22914;&#27979;&#35797;&#29992;&#20363;&#12289;API)&#24182;&#24110;&#21161;&#27169;&#22411;&#29702;&#35299;&#38656;&#27714;&#21644;&#25351;&#23548;&#19979;&#19968;&#27493;&#20195;&#30721;&#29983;&#25104;&#12290;&#25105;&#20204;&#23558;AceCoder&#24212;&#29992;&#21040;&#22823;&#37327;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#30340;&#20195;&#30721;&#29983;&#25104;&#31995;&#32479;&#30456;&#27604;&#65292;AceCoder&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-context learning (ICL) with pre-trained language models (PTLMs) has shown great success in code generation. ICL does not require training. PTLMs take as the input a prompt consisting of a few requirement-code examples and a new requirement, and output a new program. However, existing studies simply reuse ICL techniques for natural language generation and ignore unique features of code generation. We refer to these studies as standard ICL.  Inspired by observations of the human coding process, we propose a novel ICL approach for code generation named AceCoder. Compared to standard ICL, AceCoder has two novelties. (1) Example retrieval. It retrieves similar programs as examples and learns programming skills (e.g., algorithms, APIs) from them. (2) Guided Code Generation. It encourages PTLMs to output an intermediate preliminary (e.g., test cases, APIs) before generating programs. The preliminary can help PTLMs understand requirements and guide the next code generation. We apply AceCode
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;&#20016;&#23500;&#30340;&#20803;&#25968;&#25454;&#27880;&#37322;&#30340;&#20449;&#24687;&#36827;&#34892;&#23631;&#24149;&#35282;&#33394;&#30340;&#20010;&#24615;&#21270;&#35821;&#35328;&#24314;&#27169;&#65292;&#27979;&#35797;&#34920;&#26126;&#36825;&#26679;&#21487;&#20197;&#26377;&#25928;&#22320;&#36827;&#34892;&#20010;&#24615;&#21270;&#35821;&#35328;&#27169;&#22411;&#30340;&#26500;&#24314;&#65292;&#21363;&#20351;&#23545;&#20110;&#38646;&#26679;&#26412;&#30340;&#28436;&#35828;&#23478;&#20063;&#21487;&#20197;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2303.16618</link><description>&lt;p&gt;
&#20351;&#29992;&#20016;&#23500;&#30340;&#20803;&#25968;&#25454;&#27880;&#37322;&#30340;&#23631;&#24149;&#35282;&#33394;&#30340;&#20010;&#24615;&#21270;&#35821;&#35328;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Personalised Language Modelling of Screen Characters Using Rich Metadata Annotations. (arXiv:2303.16618v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16618
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;&#20016;&#23500;&#30340;&#20803;&#25968;&#25454;&#27880;&#37322;&#30340;&#20449;&#24687;&#36827;&#34892;&#23631;&#24149;&#35282;&#33394;&#30340;&#20010;&#24615;&#21270;&#35821;&#35328;&#24314;&#27169;&#65292;&#27979;&#35797;&#34920;&#26126;&#36825;&#26679;&#21487;&#20197;&#26377;&#25928;&#22320;&#36827;&#34892;&#20010;&#24615;&#21270;&#35821;&#35328;&#27169;&#22411;&#30340;&#26500;&#24314;&#65292;&#21363;&#20351;&#23545;&#20110;&#38646;&#26679;&#26412;&#30340;&#28436;&#35828;&#23478;&#20063;&#21487;&#20197;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#20010;&#24615;&#21270;&#20026;&#23545;&#35805;&#25935;&#24863;&#65292;&#33021;&#26356;&#22909;&#22320;&#25429;&#25417;&#29305;&#23450;&#29305;&#24449;&#30340;&#20154;&#21592;&#21644;/&#25110;&#29305;&#23450;&#29615;&#22659;&#20013;&#30340;&#35828;&#35805;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#20016;&#23500;&#30340;&#35282;&#33394;&#27880;&#37322;&#38590;&#20197;&#24471;&#21040;&#21644;&#25104;&#21151;&#21033;&#29992;&#12290;&#22312;&#27492;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21457;&#24067;&#24182;&#25551;&#36848;&#20102;&#19968;&#32452;&#26032;&#39062;&#30340;&#25163;&#21160;&#27880;&#37322;&#65292;&#28085;&#30422;&#20102;&#26469;&#33258;&#27969;&#34892;&#30340; Cornell &#30005;&#24433;&#23545;&#35805;&#35821;&#26009;&#24211;&#30340; 863 &#21517;&#28436;&#35762;&#32773;&#65292;&#21253;&#25324;&#29305;&#24449;&#24341;&#29992;&#21644;&#35282;&#33394;&#25551;&#36848;&#65292;&#20197;&#21450;&#36229;&#36807; 95&#65285; &#30340;&#29305;&#33394;&#30005;&#24433;&#30340;&#19968;&#32452;&#33258;&#21160;&#25552;&#21462;&#30340;&#20845;&#20010;&#20803;&#25968;&#25454;&#12290;&#25105;&#20204;&#23545;&#20004;&#20010;&#35821;&#26009;&#24211;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#24182;&#34920;&#26126;&#21487;&#20197;&#26377;&#25928;&#22320;&#20351;&#29992;&#27492;&#31867;&#27880;&#37322;&#26469;&#20010;&#24615;&#21270;&#35821;&#35328;&#27169;&#22411;&#65292;&#23558;&#22256;&#24785;&#20943;&#23569;&#39640;&#36798; 8.5&#65285;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#29978;&#33267;&#21487;&#20197;&#24212;&#29992;&#20110;&#38646;&#26679;&#26412;&#30340;&#28436;&#35762;&#32773;&#65292;&#21363;&#23545;&#20110;&#27809;&#26377;&#20808;&#21069;&#22521;&#35757;&#25968;&#25454;&#30340;&#28436;&#35762;&#32773;&#65292;&#20381;&#36182;&#20110;&#35282;&#33394;&#30340;&#20154;&#21475;&#29305;&#24449;&#30340;&#32452;&#21512;&#12290;&#30001;&#20110;&#25910;&#38598;&#27492;&#31867;&#20803;&#25968;&#25454;&#25104;&#26412;&#39640;&#26114;&#65292;&#22240;&#27492;&#25105;&#20204;&#36824;&#36129;&#29486;&#20102;&#19968;&#39033;&#25104;&#26412;&#25928;&#30410;&#20998;&#26512;&#65292;&#20197;&#31361;&#20986;&#26174;&#31034;
&lt;/p&gt;
&lt;p&gt;
Personalisation of language models for dialogue sensitises them to better capture the speaking patterns of people of specific characteristics, and/or in specific environments. However, rich character annotations are difficult to come by and to successfully leverage. In this work, we release and describe a novel set of manual annotations for 863 speakers from the popular Cornell Movie Dialog Corpus, including features like characteristic quotes and character descriptions, and a set of six automatically extracted metadata for over 95% of the featured films. We perform extensive experiments on two corpora and show that such annotations can be effectively used to personalise language models, reducing perplexity by up to 8.5%. Our method can be applied even zero-shot for speakers for whom no prior training data is available, by relying on combinations of characters' demographic characteristics. Since collecting such metadata is costly, we also contribute a cost-benefit analysis to highlight
&lt;/p&gt;</description></item><item><title>ExBEHRT&#26159;&#19968;&#31181;&#25193;&#23637;Transformer&#27169;&#22411;&#65292;&#24212;&#29992;&#20110;&#30005;&#23376;&#30149;&#21382;&#25968;&#25454;&#65292;&#23558;&#22810;&#31181;&#31867;&#22411;&#30340;&#35760;&#24405;&#21253;&#25324;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#65292;&#21487;&#20197;&#39044;&#27979;&#19981;&#21516;&#30142;&#30149;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#26356;&#22909;&#65292;&#24182;&#20351;&#29992;&#39044;&#26399;&#26799;&#24230;&#23545;&#32467;&#26524;&#36827;&#34892;&#26356;&#32454;&#31890;&#24230;&#30340;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2303.12364</link><description>&lt;p&gt;
ExBEHRT&#65306;&#22522;&#20110;&#30005;&#23376;&#30149;&#21382;&#30340;&#25193;&#23637;Transformer&#39044;&#27979;&#30142;&#30149;&#20122;&#22411;&#21644;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;
ExBEHRT: Extended Transformer for Electronic Health Records to Predict Disease Subtypes &amp; Progressions. (arXiv:2303.12364v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12364
&lt;/p&gt;
&lt;p&gt;
ExBEHRT&#26159;&#19968;&#31181;&#25193;&#23637;Transformer&#27169;&#22411;&#65292;&#24212;&#29992;&#20110;&#30005;&#23376;&#30149;&#21382;&#25968;&#25454;&#65292;&#23558;&#22810;&#31181;&#31867;&#22411;&#30340;&#35760;&#24405;&#21253;&#25324;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#65292;&#21487;&#20197;&#39044;&#27979;&#19981;&#21516;&#30142;&#30149;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#26356;&#22909;&#65292;&#24182;&#20351;&#29992;&#39044;&#26399;&#26799;&#24230;&#23545;&#32467;&#26524;&#36827;&#34892;&#26356;&#32454;&#31890;&#24230;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;ExBEHRT&#65292;&#23427;&#26159;BEHRT&#65288;&#24212;&#29992;&#20110;&#30005;&#23376;&#30149;&#21382;&#30340;BERT&#65289;&#30340;&#25193;&#23637;&#29256;&#26412;&#65292;&#24182;&#24212;&#29992;&#19981;&#21516;&#30340;&#31639;&#27861;&#26469;&#35299;&#37322;&#20854;&#32467;&#26524;&#12290;&#25105;&#20204;&#23558;&#29305;&#24449;&#31354;&#38388;&#20174;&#20165;&#32771;&#34385;&#35786;&#26029;&#21644;&#24739;&#32773;&#24180;&#40836;&#25193;&#23637;&#21040;&#21253;&#25324;&#22810;&#31181;&#31867;&#22411;&#30340;&#35760;&#24405;&#65292;&#21253;&#25324;&#20154;&#21475;&#32479;&#35745;&#23398;&#12289;&#20020;&#24202;&#29305;&#24449;&#12289;&#29983;&#21629;&#20307;&#24449;&#12289;&#21560;&#28895;&#29366;&#24577;&#12289;&#35786;&#26029;&#12289;&#25163;&#26415;&#12289;&#33647;&#29289;&#21644;&#23454;&#39564;&#23460;&#26816;&#26597;&#65292;&#24182;&#37319;&#29992;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#32479;&#19968;&#19981;&#21516;&#29305;&#24449;&#30340;&#39057;&#29575;&#21644;&#26102;&#38388;&#32500;&#24230;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#38468;&#21152;&#29305;&#24449;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#19981;&#21516;&#30142;&#30149;&#19979;&#28216;&#20219;&#21153;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;&#20026;&#20102;&#20445;&#35777;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#39044;&#26399;&#26799;&#24230;&#30340;&#25913;&#36827;&#26041;&#27861;&#23545;&#27169;&#22411;&#39044;&#27979;&#32467;&#26524;&#36827;&#34892;&#35299;&#37322;&#65292;&#35813;&#26041;&#27861;&#20197;&#21069;&#26410;&#24212;&#29992;&#20110;&#23558;EHR&#25968;&#25454;&#19982;Transformer&#30456;&#32467;&#21512;&#65292;&#25552;&#20379;&#20102;&#27604;&#20197;&#21069;&#26041;&#27861;&#26356;&#32454;&#31890;&#24230;&#30340;&#35299;&#37322;&#65292;&#22914;&#29305;&#24449;&#21644;&#20196;&#29260;&#37325;&#35201;&#24615;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#23545;&#32959;&#30244;&#23398;&#24739;&#32773;&#30340;&#27169;&#22411;&#34920;&#31034;&#36827;&#34892;&#32858;&#31867;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;ExBEHRT&#21487;&#20197;&#29992;&#20110;&#39044;&#27979;&#30142;&#30149;&#20122;&#22411;&#21644;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we introduce ExBEHRT, an extended version of BEHRT (BERT applied to electronic health records), and apply different algorithms to interpret its results. While BEHRT considers only diagnoses and patient age, we extend the feature space to several multimodal records, namely demographics, clinical characteristics, vital signs, smoking status, diagnoses, procedures, medications, and laboratory tests, by applying a novel method to unify the frequencies and temporal dimensions of the different features. We show that additional features significantly improve model performance for various downstream tasks in different diseases. To ensure robustness, we interpret model predictions using an adaptation of expected gradients, which has not been previously applied to transformers with EHR data and provides more granular interpretations than previous approaches such as feature and token importances. Furthermore, by clustering the model representations of oncology patients, we show tha
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;NeTO&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20307;&#28210;&#26579;&#20174;2D&#22270;&#20687;&#20013;&#25429;&#25417;&#22266;&#20307;&#36879;&#26126;&#29289;&#20307;&#30340;3D&#20960;&#20309;&#20307;&#12290;&#36890;&#36807;&#37319;&#29992;&#38544;&#24335;&#26377;&#31526;&#21495;&#36317;&#31163;&#20989;&#25968;&#65288;SDF&#65289;&#20316;&#20026;&#34920;&#38754;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#33258;&#36974;&#25377;&#24863;&#30693;&#30340;&#25240;&#23556;&#36861;&#36394;&#36890;&#36807;&#20307;&#28210;&#26579;&#26469;&#20248;&#21270;SDF&#22330;&#65292;&#21487;&#20197;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#37325;&#24314;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.11219</link><description>&lt;p&gt;
NeTO: &#36879;&#26126;&#29289;&#20307;&#30340;&#31070;&#32463;&#37325;&#24314;&#19982;&#33258;&#36974;&#25377;&#24863;&#30693;&#25240;&#23556;&#36861;&#36394;
&lt;/p&gt;
&lt;p&gt;
NeTO:Neural Reconstruction of Transparent Objects with Self-Occlusion Aware Refraction-Tracing. (arXiv:2303.11219v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11219
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;NeTO&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20307;&#28210;&#26579;&#20174;2D&#22270;&#20687;&#20013;&#25429;&#25417;&#22266;&#20307;&#36879;&#26126;&#29289;&#20307;&#30340;3D&#20960;&#20309;&#20307;&#12290;&#36890;&#36807;&#37319;&#29992;&#38544;&#24335;&#26377;&#31526;&#21495;&#36317;&#31163;&#20989;&#25968;&#65288;SDF&#65289;&#20316;&#20026;&#34920;&#38754;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#33258;&#36974;&#25377;&#24863;&#30693;&#30340;&#25240;&#23556;&#36861;&#36394;&#36890;&#36807;&#20307;&#28210;&#26579;&#26469;&#20248;&#21270;SDF&#22330;&#65292;&#21487;&#20197;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#37325;&#24314;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;NeTO&#65292;&#36890;&#36807;&#20307;&#28210;&#26579;&#20174;2D&#22270;&#20687;&#20013;&#25429;&#25417;&#22266;&#20307;&#36879;&#26126;&#29289;&#20307;&#30340;3D&#20960;&#20309;&#20307;&#12290;&#36879;&#26126;&#29289;&#20307;&#30340;&#37325;&#24314;&#26159;&#19968;&#39033;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#19981;&#36866;&#21512;&#36890;&#29992;&#30340;&#37325;&#24314;&#25216;&#26415;&#65292;&#22240;&#20026;&#20854;&#22256;&#25200;&#20110;&#38236;&#38754;&#20809;&#20256;&#36755;&#29616;&#35937;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#22522;&#20110;&#25240;&#23556;&#36861;&#36394;&#30340;&#26041;&#27861;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#23384;&#22312;&#20248;&#21270;&#19981;&#31283;&#23450;&#21644;&#32454;&#33410;&#32570;&#22833;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#37319;&#29992;&#30340;&#26174;&#24335;&#34920;&#38754;&#34920;&#31034;&#38590;&#20197;&#20248;&#21270;&#65292;&#19988;&#24573;&#30053;&#20102;&#33258;&#36974;&#25377;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#38544;&#24335;&#26377;&#31526;&#21495;&#36317;&#31163;&#20989;&#25968;&#65288;SDF&#65289;&#20316;&#20026;&#34920;&#38754;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#33258;&#36974;&#25377;&#24863;&#30693;&#30340;&#25240;&#23556;&#36861;&#36394;&#36890;&#36807;&#20307;&#28210;&#26579;&#26469;&#20248;&#21270;SDF&#22330;&#12290;&#38544;&#24335;&#34920;&#31034;&#20351;&#24471;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#26377;&#38480;&#30340;&#22270;&#20687;&#38598;&#21512;&#19979;&#37325;&#24314;&#20986;&#39640;&#36136;&#37327;&#30340;&#37325;&#24314;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel method, called NeTO, for capturing 3D geometry of solid transparent objects from 2D images via volume rendering. Reconstructing transparent objects is a very challenging task, which is ill-suited for general-purpose reconstruction techniques due to the specular light transport phenomena. Although existing refraction-tracing based methods, designed specially for this task, achieve impressive results, they still suffer from unstable optimization and loss of fine details, since the explicit surface representation they adopted is difficult to be optimized, and the self-occlusion problem is ignored for refraction-tracing. In this paper, we propose to leverage implicit Signed Distance Function (SDF) as surface representation, and optimize the SDF field via volume rendering with a self-occlusion aware refractive ray tracing. The implicit representation enables our method to be capable of reconstructing high-quality reconstruction even with a limited set of images, and the s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26080;&#20154;&#26426;&#36741;&#21161;&#30340;&#21327;&#21516;&#23398;&#20064;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26234;&#33021;&#35774;&#22791;&#32676;&#19982;&#26080;&#20154;&#26426;&#21327;&#21516;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#22312;&#32771;&#34385;&#25968;&#25454;&#24322;&#36136;&#24615;&#21644;&#36890;&#20449;&#38169;&#35823;&#30340;&#24773;&#20917;&#19979;&#65292;&#23548;&#20986;&#20102;&#21327;&#21516;&#23398;&#20064;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#26080;&#20154;&#26426;&#36712;&#36857;&#26469;&#25552;&#39640;&#35757;&#32451;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.02266</link><description>&lt;p&gt;
&#37319;&#29992;&#26426;&#36733;&#21327;&#35843;&#22120;&#36827;&#34892;&#21327;&#21516;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Collaborative Learning with a Drone Orchestrator. (arXiv:2303.02266v2 [cs.IT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02266
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26080;&#20154;&#26426;&#36741;&#21161;&#30340;&#21327;&#21516;&#23398;&#20064;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26234;&#33021;&#35774;&#22791;&#32676;&#19982;&#26080;&#20154;&#26426;&#21327;&#21516;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#22312;&#32771;&#34385;&#25968;&#25454;&#24322;&#36136;&#24615;&#21644;&#36890;&#20449;&#38169;&#35823;&#30340;&#24773;&#20917;&#19979;&#65292;&#23548;&#20986;&#20102;&#21327;&#21516;&#23398;&#20064;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#26080;&#20154;&#26426;&#36712;&#36857;&#26469;&#25552;&#39640;&#35757;&#32451;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#26080;&#20154;&#26426;&#36741;&#21161;&#21327;&#21516;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#31181;&#22330;&#26223;&#19979;&#65292;&#26234;&#33021;&#26080;&#32447;&#35774;&#22791;&#32676;&#36890;&#36807;&#26080;&#20154;&#26426;&#20849;&#21516;&#35757;&#32451;&#19968;&#20010;&#20849;&#20139;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#27599;&#20010;&#35774;&#22791;&#20351;&#29992;&#20854;&#20256;&#24863;&#22120;&#35760;&#24405;&#26469;&#33258;&#29615;&#22659;&#30340;&#26679;&#26412;&#65292;&#20197;&#33719;&#21462;&#29992;&#20110;&#35757;&#32451;&#30340;&#26412;&#22320;&#25968;&#25454;&#38598;&#12290;&#30001;&#20110;&#21508;&#35774;&#22791;&#30340;&#25968;&#25454;&#37327;&#21644;&#20256;&#24863;&#22120;&#22122;&#22768;&#27700;&#24179;&#19981;&#21516;&#65292;&#35757;&#32451;&#25968;&#25454;&#20855;&#26377;&#20005;&#37325;&#30340;&#24322;&#36136;&#24615;&#12290;&#26234;&#33021;&#35774;&#22791;&#23545;&#20854;&#26412;&#22320;&#25968;&#25454;&#38598;&#36827;&#34892;&#36845;&#20195;&#35757;&#32451;&#65292;&#24182;&#23558;&#27169;&#22411;&#21442;&#25968;&#19982;&#26080;&#20154;&#26426;&#36827;&#34892;&#20132;&#25442;&#20197;&#36827;&#34892;&#32858;&#21512;&#12290;&#22312;&#32771;&#34385;&#25968;&#25454;&#24322;&#36136;&#24615;&#12289;&#20256;&#24863;&#22120;&#22122;&#22768;&#27700;&#24179;&#21644;&#36890;&#20449;&#38169;&#35823;&#30340;&#24773;&#20917;&#19979;&#65292;&#23548;&#20986;&#20102;&#21327;&#21516;&#23398;&#20064;&#30340;&#25910;&#25947;&#36895;&#29575;&#65292;&#24182;&#33719;&#24471;&#20102;&#26368;&#22823;&#21270;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#32456;&#20934;&#30830;&#29575;&#30340;&#26080;&#20154;&#26426;&#36712;&#36857;&#12290;&#25152;&#25552;&#20986;&#30340;&#36712;&#36857;&#20248;&#21270;&#26041;&#27861;&#32771;&#34385;&#20102;&#35774;&#22791;&#30340;&#25968;&#25454;&#29305;&#24615;&#65288;&#21363;&#26412;&#22320;&#25968;&#25454;&#38598;&#22823;&#23567;&#21644;&#22122;&#22768;&#27700;&#24179;&#65289;&#20197;&#21450;&#20854;&#26080;&#32447;&#36890;&#20449;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, the problem of drone-assisted collaborative learning is considered. In this scenario, swarm of intelligent wireless devices train a shared neural network (NN) model with the help of a drone. Using its sensors, each device records samples from its environment to gather a local dataset for training. The training data is severely heterogeneous as various devices have different amount of data and sensor noise level. The intelligent devices iteratively train the NN on their local datasets and exchange the model parameters with the drone for aggregation. For this system, the convergence rate of collaborative learning is derived while considering data heterogeneity, sensor noise levels, and communication errors, then, the drone trajectory that maximizes the final accuracy of the trained NN is obtained. The proposed trajectory optimization approach is aware of both the devices data characteristics (i.e., local dataset size and noise level) and their wireless channel conditions, 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;COOLANT&#65292;&#19968;&#20010;&#29992;&#20110;&#36328;&#27169;&#24577;&#20551;&#26032;&#38395;&#26816;&#27979;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#25552;&#21319;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#23545;&#40784;&#31934;&#24230;&#65292;&#24182;&#36890;&#36807;&#36328;&#27169;&#24577;&#34701;&#21512;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#23454;&#29616;&#26356;&#20934;&#30830;&#21644;&#21487;&#35299;&#37322;&#30340;&#29305;&#24449;&#32858;&#21512;&#12290;</title><link>http://arxiv.org/abs/2302.14057</link><description>&lt;p&gt;
&#36328;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;&#22810;&#27169;&#24577;&#20551;&#26032;&#38395;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Cross-modal Contrastive Learning for Multimodal Fake News Detection. (arXiv:2302.14057v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14057
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;COOLANT&#65292;&#19968;&#20010;&#29992;&#20110;&#36328;&#27169;&#24577;&#20551;&#26032;&#38395;&#26816;&#27979;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#25552;&#21319;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#23545;&#40784;&#31934;&#24230;&#65292;&#24182;&#36890;&#36807;&#36328;&#27169;&#24577;&#34701;&#21512;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#23454;&#29616;&#26356;&#20934;&#30830;&#21644;&#21487;&#35299;&#37322;&#30340;&#29305;&#24449;&#32858;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#26816;&#27979;&#22810;&#27169;&#24577;&#20551;&#26032;&#38395;&#36817;&#26469;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#35768;&#22810;&#29616;&#26377;&#26041;&#27861;&#33268;&#21147;&#20110;&#34701;&#21512;&#21333;&#27169;&#29305;&#24449;&#20197;&#20135;&#29983;&#22810;&#27169;&#24577;&#26032;&#38395;&#34920;&#36798;&#12290;&#28982;&#32780;&#65292;&#24378;&#22823;&#30340;&#36328;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#22312;&#20551;&#26032;&#38395;&#26816;&#27979;&#26041;&#38754;&#30340;&#28508;&#21147;&#23578;&#26410;&#20805;&#20998;&#21033;&#29992;&#12290;&#27492;&#22806;&#65292;&#22914;&#20309;&#32858;&#21512;&#19981;&#21516;&#27169;&#24577;&#30340;&#29305;&#24449;&#20197;&#25552;&#21319;&#20915;&#31574;&#36807;&#31243;&#30340;&#24615;&#33021;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;COOLANT&#65292;&#19968;&#20010;&#29992;&#20110;&#22810;&#27169;&#24577;&#20551;&#26032;&#38395;&#26816;&#27979;&#30340;&#36328;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#22270;&#20687;-&#25991;&#26412;&#23545;&#40784;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#23545;&#40784;&#31934;&#24230;&#65292;&#25105;&#20204;&#21033;&#29992;&#36741;&#21161;&#20219;&#21153;&#22312;&#23545;&#27604;&#36807;&#31243;&#20013;&#36719;&#21270;&#36127;&#26679;&#26412;&#30340;&#25439;&#22833;&#39033;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#36328;&#27169;&#24577;&#34701;&#21512;&#27169;&#22359;&#26469;&#23398;&#20064;&#36328;&#27169;&#24577;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;&#19968;&#20010;&#24102;&#26377;&#27880;&#24847;&#21147;&#24341;&#23548;&#27169;&#22359;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#20197;&#24110;&#21161;&#26377;&#25928;&#19988;&#21487;&#35299;&#37322;&#22320;&#32858;&#21512;&#23545;&#40784;&#30340;&#21333;&#27169;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic detection of multimodal fake news has gained a widespread attention recently. Many existing approaches seek to fuse unimodal features to produce multimodal news representations. However, the potential of powerful cross-modal contrastive learning methods for fake news detection has not been well exploited. Besides, how to aggregate features from different modalities to boost the performance of the decision-making process is still an open question. To address that, we propose COOLANT, a cross-modal contrastive learning framework for multimodal fake news detection, aiming to achieve more accurate image-text alignment. To further improve the alignment precision, we leverage an auxiliary task to soften the loss term of negative samples during the contrast process. A cross-modal fusion module is developed to learn the cross-modality correlations. An attention mechanism with an attention guidance module is implemented to help effectively and interpretably aggregate the aligned unimo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#31070;&#32463;&#22270;&#20687;&#21387;&#32553;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#38750;&#20108;&#20803;&#37492;&#21035;&#22120;&#20197;&#21450;&#20197;&#37327;&#21270;&#23616;&#37096;&#22270;&#20687;&#34920;&#31034;&#20026;&#26465;&#20214;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#22320;&#20248;&#21270;&#22833;&#30495;&#24230;&#21644;&#32479;&#35745;&#20445;&#30495;&#24230;&#12290;</title><link>http://arxiv.org/abs/2301.11189</link><description>&lt;p&gt;
&#20351;&#29992;&#38544;&#24335;&#23616;&#37096;&#27010;&#29575;&#27169;&#22411;&#25913;&#36827;&#31070;&#32463;&#22270;&#20687;&#21387;&#32553;&#30340;&#32479;&#35745;&#20445;&#30495;&#24230;
&lt;/p&gt;
&lt;p&gt;
Improving Statistical Fidelity for Neural Image Compression with Implicit Local Likelihood Models. (arXiv:2301.11189v3 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11189
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#31070;&#32463;&#22270;&#20687;&#21387;&#32553;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#38750;&#20108;&#20803;&#37492;&#21035;&#22120;&#20197;&#21450;&#20197;&#37327;&#21270;&#23616;&#37096;&#22270;&#20687;&#34920;&#31034;&#20026;&#26465;&#20214;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#22320;&#20248;&#21270;&#22833;&#30495;&#24230;&#21644;&#32479;&#35745;&#20445;&#30495;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25439;&#22270;&#20687;&#21387;&#32553;&#26088;&#22312;&#22312;&#23613;&#21487;&#33021;&#23569;&#30340;&#27604;&#29305;&#25968;&#19979;&#20445;&#25345;&#23545;&#21407;&#22987;&#22270;&#20687;&#30340;&#20445;&#30495;&#24230;&#12290;&#29702;&#35770;&#32467;&#26524;&#34920;&#26126;&#65292;&#20248;&#21270;&#22833;&#30495;&#24230;&#37327;&#65288;&#22914;PSNR&#25110;MS-SSIM&#65289;&#24517;&#28982;&#23548;&#33268;&#21407;&#22987;&#22270;&#20687;&#19982;&#37325;&#24314;&#22270;&#20687;&#30340;&#32479;&#35745;&#20449;&#24687;&#23384;&#22312;&#24046;&#24322;&#65292;&#29305;&#21035;&#26159;&#22312;&#20302;&#27604;&#29305;&#29575;&#19979;&#65292;&#36825;&#24120;&#24120;&#34920;&#29616;&#20026;&#21387;&#32553;&#22270;&#20687;&#30340;&#27169;&#31946;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#21033;&#29992;&#23545;&#25239;&#24615;&#37492;&#21035;&#22120;&#25552;&#39640;&#20102;&#32479;&#35745;&#20445;&#30495;&#24230;&#12290;&#28982;&#32780;&#65292;&#20174;&#29983;&#25104;&#24314;&#27169;&#20219;&#21153;&#20013;&#37319;&#29992;&#30340;&#36825;&#20123;&#20108;&#20803;&#37492;&#21035;&#22120;&#21487;&#33021;&#19981;&#36866;&#29992;&#20110;&#22270;&#20687;&#21387;&#32553;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#38750;&#20108;&#20803;&#30340;&#37492;&#21035;&#22120;&#65292;&#23427;&#20197;&#36890;&#36807;VQ-VAE&#33258;&#21160;&#32534;&#30721;&#22120;&#33719;&#24471;&#30340;&#37327;&#21270;&#23616;&#37096;&#22270;&#20687;&#34920;&#31034;&#20026;&#26465;&#20214;&#12290;&#25105;&#20204;&#22312;CLIC2020&#12289;DIV2K&#21644;Kodak&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#19982;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;HiFiC&#27169;&#22411;&#30340;PatchGAN&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#37492;&#21035;&#22120;&#26356;&#26377;&#25928;&#22320;&#32852;&#21512;&#20248;&#21270;&#22833;&#30495;&#24230;&#65288;&#20363;&#22914;PSNR&#65289;&#21644;&#32479;&#35745;&#20445;&#30495;&#24230;&#65288;&#20363;&#22914;FID&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lossy image compression aims to represent images in as few bits as possible while maintaining fidelity to the original. Theoretical results indicate that optimizing distortion metrics such as PSNR or MS-SSIM necessarily leads to a discrepancy in the statistics of original images from those of reconstructions, in particular at low bitrates, often manifested by the blurring of the compressed images. Previous work has leveraged adversarial discriminators to improve statistical fidelity. Yet these binary discriminators adopted from generative modeling tasks may not be ideal for image compression. In this paper, we introduce a non-binary discriminator that is conditioned on quantized local image representations obtained via VQ-VAE autoencoders. Our evaluations on the CLIC2020, DIV2K and Kodak datasets show that our discriminator is more effective for jointly optimizing distortion (e.g., PSNR) and statistical fidelity (e.g., FID) than the PatchGAN of the state-of-the-art HiFiC model. On CLIC
&lt;/p&gt;</description></item><item><title>&#32467;&#21512;&#20351;&#29992;&#31616;&#21333;&#22870;&#21169;&#30340;&#24378;&#21270;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#39550;&#39542;&#31574;&#30053;&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#36229;&#36807;&#20165;&#29992;&#27169;&#20223;&#23398;&#20064;&#23398;&#21040;&#30340;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2212.11419</link><description>&lt;p&gt;
&#19981;&#20165;&#20165;&#26159;&#27169;&#20223;&#65306;&#32467;&#21512;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#39550;&#39542;&#24773;&#26223;&#30340;&#40065;&#26834;&#21270;&#27169;&#20223;
&lt;/p&gt;
&lt;p&gt;
Imitation Is Not Enough: Robustifying Imitation with Reinforcement Learning for Challenging Driving Scenarios. (arXiv:2212.11419v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.11419
&lt;/p&gt;
&lt;p&gt;
&#32467;&#21512;&#20351;&#29992;&#31616;&#21333;&#22870;&#21169;&#30340;&#24378;&#21270;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#39550;&#39542;&#31574;&#30053;&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#36229;&#36807;&#20165;&#29992;&#27169;&#20223;&#23398;&#20064;&#23398;&#21040;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#20223;&#23398;&#20064;(imitation learning, IL)&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#30340;&#39640;&#36136;&#37327;&#20154;&#31867;&#39550;&#39542;&#25968;&#25454;&#20135;&#29983;&#31867;&#20154;&#39550;&#39542;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#20165;&#22522;&#20110;&#27169;&#20223;&#23398;&#20064;&#30340;&#31574;&#30053;&#24448;&#24448;&#26080;&#27861;&#20805;&#20998;&#32771;&#34385;&#21040;&#23433;&#20840;&#24615;&#21644;&#21487;&#38752;&#24615;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#32467;&#21512;&#20351;&#29992;&#31616;&#21333;&#22870;&#21169;&#30340;&#24378;&#21270;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#39550;&#39542;&#31574;&#30053;&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#36229;&#36807;&#20165;&#29992;&#27169;&#20223;&#23398;&#20064;&#23398;&#21040;&#30340;&#31574;&#30053;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#22312;&#36229;&#36807;10&#19975;&#33521;&#37324;&#30340;&#22478;&#24066;&#39550;&#39542;&#25968;&#25454;&#19978;&#35757;&#32451;&#20102;&#19968;&#20010;&#31574;&#30053;&#65292;&#24182;&#22312;&#19981;&#21516;&#30896;&#25758;&#21487;&#33021;&#24615;&#27700;&#24179;&#19979;&#30340;&#27979;&#35797;&#24773;&#26223;&#20013;&#27979;&#37327;&#20854;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#34429;&#28982;&#27169;&#20223;&#23398;&#20064;&#22312;&#30001;&#31034;&#33539;&#25968;&#25454;&#24456;&#22909;&#35206;&#30422;&#30340;&#20302;&#38590;&#24230;&#24773;&#26223;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#24773;&#26223;&#20013;&#26174;&#33879;&#25552;&#39640;&#20102;&#40065;&#26834;&#24615;(&#25925;&#38556;&#20943;&#23569;&#20102;&#36229;&#36807;38%)&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#23558;&#32467;&#21512;imitation learning&#21644;reinforcement learning&#24212;&#29992;&#20110;&#39550;&#39542;&#24773;&#26223;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imitation learning (IL) is a simple and powerful way to use high-quality human driving data, which can be collected at scale, to produce human-like behavior. However, policies based on imitation learning alone often fail to sufficiently account for safety and reliability concerns. In this paper, we show how imitation learning combined with reinforcement learning using simple rewards can substantially improve the safety and reliability of driving policies over those learned from imitation alone. In particular, we train a policy on over 100k miles of urban driving data, and measure its effectiveness in test scenarios grouped by different levels of collision likelihood. Our analysis shows that while imitation can perform well in low-difficulty scenarios that are well-covered by the demonstration data, our proposed approach significantly improves robustness on the most challenging scenarios (over 38% reduction in failures). To our knowledge, this is the first application of a combined imit
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26102;&#24577;&#35268;&#21010;&#30340;&#39640;&#25928;&#22686;&#37327;&#31616;&#21333;&#26102;&#24577;&#32593;&#32476;&#25968;&#25454;&#32467;&#26500;&#65292;&#36890;&#36807;&#37319;&#29992;&#22686;&#37327;&#37325;&#29992;&#35745;&#31639;&#21644;&#36991;&#20813;&#20869;&#23384;&#22797;&#21046;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#26102;&#24577;&#35268;&#21010;&#38382;&#39064;&#27714;&#35299;&#30340;&#25928;&#29575;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#25968;&#25454;&#32467;&#26500;&#22312;&#26102;&#24577;&#35268;&#21010;&#38382;&#39064;&#24207;&#21015;&#19978;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2212.07226</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#26102;&#24577;&#35268;&#21010;&#30340;&#39640;&#25928;&#22686;&#37327;&#31616;&#21333;&#26102;&#24577;&#32593;&#32476;&#25968;&#25454;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
An Efficient Incremental Simple Temporal Network Data Structure for Temporal Planning. (arXiv:2212.07226v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.07226
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26102;&#24577;&#35268;&#21010;&#30340;&#39640;&#25928;&#22686;&#37327;&#31616;&#21333;&#26102;&#24577;&#32593;&#32476;&#25968;&#25454;&#32467;&#26500;&#65292;&#36890;&#36807;&#37319;&#29992;&#22686;&#37327;&#37325;&#29992;&#35745;&#31639;&#21644;&#36991;&#20813;&#20869;&#23384;&#22797;&#21046;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#26102;&#24577;&#35268;&#21010;&#38382;&#39064;&#27714;&#35299;&#30340;&#25928;&#29575;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#25968;&#25454;&#32467;&#26500;&#22312;&#26102;&#24577;&#35268;&#21010;&#38382;&#39064;&#24207;&#21015;&#19978;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#26102;&#24577;&#35268;&#21010;&#38382;&#39064;&#30340;&#19968;&#31181;&#24120;&#29992;&#25216;&#26415;&#26159;&#23558;&#22240;&#26524;&#20915;&#31574;&#19982;&#21551;&#21457;&#24335;&#25628;&#32034;&#20998;&#31163;&#65292;&#23558;&#26102;&#24577;&#20915;&#31574;&#20132;&#30001;&#31616;&#21333;&#26102;&#24577;&#32593;&#32476;(STN)&#27714;&#35299;&#22120;&#22788;&#29702;&#12290;&#22312;&#36825;&#31181;&#26550;&#26500;&#20013;&#65292;&#38656;&#35201;&#26816;&#26597;&#19968;&#31995;&#21015;&#20114;&#30456;&#20851;&#32852;&#30340;STN&#30340;&#19968;&#33268;&#24615;&#65292;&#22240;&#27492;&#20855;&#26377;&#22686;&#37327;&#37325;&#29992;&#20043;&#21069;&#30340;&#35745;&#31639;&#21644;&#36991;&#20813;&#26114;&#36149;&#30340;&#20869;&#23384;&#22797;&#21046;&#30340;&#26041;&#27861;&#38750;&#24120;&#37325;&#35201;&#12290;&#26412;&#25991;&#35814;&#32454;&#25551;&#36848;&#20102;STN&#22312;&#26102;&#24577;&#35268;&#21010;&#20013;&#30340;&#20351;&#29992;&#26041;&#27861;&#65292;&#30830;&#23450;&#20102;&#25903;&#25345;&#27492;&#29992;&#20363;&#30340;&#26126;&#30830;&#25509;&#21475;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#26082;&#26102;&#38388;&#21448;&#20869;&#23384;&#39640;&#25928;&#30340;&#23454;&#29616;&#20102;&#27492;&#25509;&#21475;&#30340;&#39640;&#25928;&#25968;&#25454;&#32467;&#26500;---\deltastn&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#32467;&#26500;&#22312;&#26102;&#24577;&#35268;&#21010;&#38382;&#39064;&#24207;&#21015;&#19978;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
One popular technique to solve temporal planning problems consists in decoupling the causal decisions, demanding them to heuristic search, from temporal decisions, demanding them to a simple temporal network (STN) solver. In this architecture, one needs to check the consistency of a series of STNs that are related one another, therefore having methods to incrementally re-use previous computations and that avoid expensive memory duplication is of paramount importance. In this paper, we describe in detail how STNs are used in temporal planning, we identify a clear interface to support this use-case and we present an efficient data-structure implementing this interface that is both time- and memory-efficient. We show that our data structure, called \deltastn, is superior to other state-of-the-art approaches on temporal planning sequences of problems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#26426;&#22120;&#20154;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#36890;&#36807;&#20174;&#22823;&#35268;&#27169;&#12289;&#22810;&#26679;&#21270;&#12289;&#20219;&#21153;&#26080;&#20851;&#30340;&#25968;&#25454;&#38598;&#20013;&#33719;&#21462;&#30693;&#35782;&#65292;&#24182;&#32467;&#21512;&#39640;&#23481;&#37327;&#26550;&#26500;&#65292;&#23454;&#29616;&#20102;&#22312;&#23454;&#38469;&#25511;&#21046;&#39046;&#22495;&#30340;&#39640;&#24615;&#33021;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2212.06817</link><description>&lt;p&gt;
RT-1: &#29992;&#20110;&#23454;&#38469;&#25511;&#21046;&#30340;&#26426;&#22120;&#20154;&#21464;&#21387;&#22120;.
&lt;/p&gt;
&lt;p&gt;
RT-1: Robotics Transformer for Real-World Control at Scale. (arXiv:2212.06817v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.06817
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26426;&#22120;&#20154;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#36890;&#36807;&#20174;&#22823;&#35268;&#27169;&#12289;&#22810;&#26679;&#21270;&#12289;&#20219;&#21153;&#26080;&#20851;&#30340;&#25968;&#25454;&#38598;&#20013;&#33719;&#21462;&#30693;&#35782;&#65292;&#24182;&#32467;&#21512;&#39640;&#23481;&#37327;&#26550;&#26500;&#65292;&#23454;&#29616;&#20102;&#22312;&#23454;&#38469;&#25511;&#21046;&#39046;&#22495;&#30340;&#39640;&#24615;&#33021;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20174;&#22823;&#35268;&#27169;&#12289;&#22810;&#26679;&#21270;&#12289;&#20219;&#21153;&#26080;&#20851;&#30340;&#25968;&#25454;&#38598;&#20013;&#33719;&#21462;&#30693;&#35782;&#65292;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#22312;&#38646;&#26679;&#26412;&#23398;&#20064;&#25110;&#20351;&#29992;&#23569;&#37327;&#29305;&#23450;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#26469;&#39640;&#27700;&#24179;&#22320;&#35299;&#20915;&#20855;&#20307;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#34429;&#28982;&#36825;&#31181;&#33021;&#21147;&#24050;&#32463;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25110;&#35821;&#38899;&#35782;&#21035;&#31561;&#20854;&#20182;&#39046;&#22495;&#24471;&#21040;&#35777;&#26126;&#65292;&#20294;&#22312;&#26426;&#22120;&#20154;&#39046;&#22495;&#23578;&#26410;&#23637;&#31034;&#20986;&#26469;&#12290;&#36825;&#26159;&#22240;&#20026;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#22312;&#26426;&#22120;&#20154;&#39046;&#22495;&#23588;&#20026;&#20851;&#38190;&#65292;&#30001;&#20110;&#25910;&#38598;&#29616;&#23454;&#19990;&#30028;&#30340;&#26426;&#22120;&#20154;&#25968;&#25454;&#30340;&#38590;&#24230;&#36739;&#22823;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36825;&#31181;&#36890;&#29992;&#26426;&#22120;&#20154;&#27169;&#22411;&#25104;&#21151;&#30340;&#19968;&#20010;&#20851;&#38190;&#22240;&#32032;&#22312;&#20110;&#20219;&#21153;&#19981;&#21487;&#30693;&#30340;&#24320;&#25918;&#24335;&#35757;&#32451;&#65292;&#32467;&#21512;&#21487;&#20197;&#21560;&#25910;&#25152;&#26377;&#22810;&#26679;&#21270;&#26426;&#22120;&#20154;&#25968;&#25454;&#30340;&#39640;&#23481;&#37327;&#26550;&#26500;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#26426;&#22120;&#20154;&#21464;&#21387;&#22120;&#30340;&#27169;&#22411;&#31867;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#21487;&#25193;&#23637;&#27169;&#22411;&#29305;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#19981;&#21516;&#27169;&#22411;&#31867;&#21035;&#21450;&#20854;&#38543;&#25968;&#25454;&#22823;&#23567;&#32780;&#25512;&#24191;&#30340;&#33021;&#21147;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
By transferring knowledge from large, diverse, task-agnostic datasets, modern machine learning models can solve specific downstream tasks either zero-shot or with small task-specific datasets to a high level of performance. While this capability has been demonstrated in other fields such as computer vision, natural language processing or speech recognition, it remains to be shown in robotics, where the generalization capabilities of the models are particularly critical due to the difficulty of collecting real-world robotic data. We argue that one of the keys to the success of such general robotic models lies with open-ended task-agnostic training, combined with high-capacity architectures that can absorb all of the diverse, robotic data. In this paper, we present a model class, dubbed Robotics Transformer, that exhibits promising scalable model properties. We verify our conclusions in a study of different model classes and their ability to generalize as a function of the data size, mod
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#35757;&#32451;&#26041;&#27861;&#65292;&#21517;&#20026;R-Mix&#65292;&#36890;&#36807;&#32467;&#21512;&#38543;&#26426;&#24615;&#21644;&#26174;&#33879;&#24615;&#21033;&#29992;&#30340;&#26368;&#20339;&#20803;&#32032;&#65292;&#23454;&#29616;&#20102;&#36895;&#24230;&#12289;&#31616;&#21333;&#24615;&#21644;&#20934;&#30830;&#24615;&#30340;&#24179;&#34913;&#12290;&#35813;&#26041;&#27861;&#22312;&#27867;&#21270;&#33021;&#21147;&#12289;&#24369;&#30417;&#30563;&#30446;&#26631;&#23450;&#20301;&#12289;&#26657;&#20934;&#21644;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#26041;&#38754;&#20855;&#26377;&#26377;&#25928;&#24615;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#26469;&#20915;&#23450;&#28151;&#21512;&#35757;&#32451;&#36807;&#31243;&#65292;&#20197;&#25506;&#32034;&#26159;&#21542;&#23384;&#22312;&#26356;&#22909;&#30340;&#20915;&#31574;&#21327;&#35758;&#12290;</title><link>http://arxiv.org/abs/2212.04875</link><description>&lt;p&gt;
&#24555;&#36895;&#22522;&#20110;&#26174;&#33879;&#24615;&#24341;&#23548;&#30340;&#38543;&#26426;&#26799;&#24230;&#38408;&#20540;&#21270;&#30340;&#28151;&#21512;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Expeditious Saliency-guided Mix-up through Random Gradient Thresholding. (arXiv:2212.04875v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.04875
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#35757;&#32451;&#26041;&#27861;&#65292;&#21517;&#20026;R-Mix&#65292;&#36890;&#36807;&#32467;&#21512;&#38543;&#26426;&#24615;&#21644;&#26174;&#33879;&#24615;&#21033;&#29992;&#30340;&#26368;&#20339;&#20803;&#32032;&#65292;&#23454;&#29616;&#20102;&#36895;&#24230;&#12289;&#31616;&#21333;&#24615;&#21644;&#20934;&#30830;&#24615;&#30340;&#24179;&#34913;&#12290;&#35813;&#26041;&#27861;&#22312;&#27867;&#21270;&#33021;&#21147;&#12289;&#24369;&#30417;&#30563;&#30446;&#26631;&#23450;&#20301;&#12289;&#26657;&#20934;&#21644;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#26041;&#38754;&#20855;&#26377;&#26377;&#25928;&#24615;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#26469;&#20915;&#23450;&#28151;&#21512;&#35757;&#32451;&#36807;&#31243;&#65292;&#20197;&#25506;&#32034;&#26159;&#21542;&#23384;&#22312;&#26356;&#22909;&#30340;&#20915;&#31574;&#21327;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#35757;&#32451;&#26041;&#27861;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22810;&#24180;&#26469;&#65292;&#30740;&#31350;&#30028;&#23558;&#28151;&#21512;&#26041;&#27861;&#25193;&#23637;&#20026;&#20004;&#20010;&#26041;&#21521;&#65292;&#26088;&#22312;&#25913;&#36827;&#26174;&#33879;&#24615;&#24341;&#23548;&#36807;&#31243;&#24182;&#26368;&#23567;&#21270;&#23545;&#20219;&#24847;&#36335;&#24452;&#30340;&#20851;&#27880;&#65292;&#23558;&#38543;&#26426;&#24615;&#39046;&#22495;&#30041;&#32473;&#36827;&#19968;&#27493;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#21463;&#21040;&#27599;&#20010;&#26041;&#21521;&#30456;&#20114;&#20248;&#36234;&#30340;&#29305;&#24615;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20301;&#20110;&#20004;&#20010;&#36335;&#32447;&#30340;&#20132;&#27719;&#22788;&#12290;&#36890;&#36807;&#32467;&#21512;&#38543;&#26426;&#24615;&#21644;&#26174;&#33879;&#24615;&#21033;&#29992;&#30340;&#26368;&#20339;&#20803;&#32032;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#36895;&#24230;&#12289;&#31616;&#21333;&#24615;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;&#25105;&#20204;&#23558;&#26041;&#27861;&#21629;&#21517;&#20026;R-Mix&#65292;&#24847;&#20026;&#8220;&#38543;&#26426;&#28151;&#21512;&#8221;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#27867;&#21270;&#24615;&#33021;&#12289;&#24369;&#30417;&#30563;&#30446;&#26631;&#23450;&#20301;&#12289;&#26657;&#20934;&#21644;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#26368;&#21518;&#65292;&#20026;&#20102;&#22238;&#31572;&#26159;&#21542;&#23384;&#22312;&#26356;&#22909;&#30340;&#20915;&#31574;&#21327;&#35758;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#26469;&#20915;&#23450;&#28151;&#21512;&#35757;&#32451;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mix-up training approaches have proven to be effective in improving the generalization ability of Deep Neural Networks. Over the years, the research community expands mix-up methods into two directions, with extensive efforts to improve saliency-guided procedures but minimal focus on the arbitrary path, leaving the randomization domain unexplored. In this paper, inspired by the superior qualities of each direction over one another, we introduce a novel method that lies at the junction of the two routes. By combining the best elements of randomness and saliency utilization, our method balances speed, simplicity, and accuracy. We name our method R-Mix following the concept of "Random Mix-up". We demonstrate its effectiveness in generalization, weakly supervised object localization, calibration, and robustness to adversarial attacks. Finally, in order to address the question of whether there exists a better decision protocol, we train a Reinforcement Learning agent that decides the mix-up
&lt;/p&gt;</description></item><item><title>&#36807;&#24230;&#24179;&#28369;&#21644;&#36807;&#24230;&#21387;&#32553;&#26159;&#28145;&#24230;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#28155;&#21152;&#21644;&#21024;&#38500;&#36793;&#32536;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2212.02374</link><description>&lt;p&gt;
&#28145;&#24230;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#36807;&#24230;&#24179;&#28369;&#21644;&#36807;&#24230;&#21387;&#32553;&#30340;&#26435;&#34913;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Trade-off between Over-smoothing and Over-squashing in Deep Graph Neural Networks. (arXiv:2212.02374v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02374
&lt;/p&gt;
&lt;p&gt;
&#36807;&#24230;&#24179;&#28369;&#21644;&#36807;&#24230;&#21387;&#32553;&#26159;&#28145;&#24230;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#28155;&#21152;&#21644;&#21024;&#38500;&#36793;&#32536;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#21508;&#31181;&#35745;&#31639;&#26426;&#31185;&#23398;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#26159;&#28145;&#24230;GNN&#22312;&#24212;&#29992;&#20013;&#34920;&#29616;&#19981;&#20339;&#65292;&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#22312;&#20854;&#20182;&#39046;&#22495;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#24403;&#22534;&#21472;&#22270;&#21367;&#31215;&#23618;&#26102;&#65292;&#36807;&#24230;&#24179;&#28369;&#21644;&#36807;&#24230;&#21387;&#32553;&#26159;&#28145;&#24230;&#34920;&#31034;&#23398;&#20064;&#21644;&#20174;&#36828;&#22788;&#33410;&#28857;&#20256;&#25773;&#20449;&#24687;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25581;&#31034;&#20102;&#36807;&#24230;&#24179;&#28369;&#21644;&#36807;&#24230;&#21387;&#32553;&#19982;&#22270;&#25289;&#26222;&#25289;&#26031;&#31639;&#31526;&#30340;&#35889;&#38388;&#38548;&#26377;&#20869;&#22312;&#32852;&#31995;&#65292;&#23548;&#33268;&#36825;&#20004;&#20010;&#38382;&#39064;&#20043;&#38388;&#23384;&#22312;&#24517;&#28982;&#30340;&#26435;&#34913;&#65292;&#26080;&#27861;&#21516;&#26102;&#32531;&#35299;&#12290;&#20026;&#20102;&#36798;&#21040;&#21512;&#36866;&#30340;&#25240;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#28155;&#21152;&#21644;&#21024;&#38500;&#36793;&#32536;&#20316;&#20026;&#21487;&#34892;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#38543;&#26426;Jost&#21644;Liu&#26354;&#29575;&#37325;&#36830;&#65288;SJLR&#65289;&#31639;&#27861;&#65292;&#23427;&#22312;&#36895;&#24230;&#19978;&#20855;&#26377;&#35745;&#31639;&#25928;&#29575;&#65292;&#24182;&#19982;&#20197;&#21069;&#22522;&#20110;&#26354;&#29575;&#30340;&#26041;&#27861;&#30456;&#27604;&#20445;&#25345;&#20102;&#22522;&#26412;&#29305;&#24615;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;SJLR&#22312;GNN&#35757;&#32451;&#36807;&#31243;&#20013;&#25191;&#34892;&#36793;&#32536;&#28155;&#21152;&#21644;&#21024;&#38500;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#22522;&#26412;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have succeeded in various computer science applications, yet deep GNNs underperform their shallow counterparts despite deep learning's success in other domains. Over-smoothing and over-squashing are key challenges when stacking graph convolutional layers, hindering deep representation learning and information propagation from distant nodes. Our work reveals that over-smoothing and over-squashing are intrinsically related to the spectral gap of the graph Laplacian, resulting in an inevitable trade-off between these two issues, as they cannot be alleviated simultaneously. To achieve a suitable compromise, we propose adding and removing edges as a viable approach. We introduce the Stochastic Jost and Liu Curvature Rewiring (SJLR) algorithm, which is computationally efficient and preserves fundamental properties compared to previous curvature-based methods. Unlike existing approaches, SJLR performs edge addition and removal during GNN training while maintaining
&lt;/p&gt;</description></item><item><title>Kuaipedia&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#22810;&#27169;&#24335;&#30701;&#35270;&#39057;&#30334;&#31185;&#20840;&#20070;&#65292;&#36890;&#36807;&#30693;&#35782;&#35270;&#39057;&#30340;&#24418;&#24335;&#65292;&#33021;&#22815;&#36731;&#26494;&#34920;&#36798;&#32593;&#27665;&#23545;&#26576;&#20010;&#39033;&#30446;&#30340;&#21508;&#20010;&#26041;&#38754;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2211.00732</link><description>&lt;p&gt;
Kuaipedia:&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#22810;&#27169;&#24335;&#30701;&#35270;&#39057;&#30334;&#31185;&#20840;&#20070;
&lt;/p&gt;
&lt;p&gt;
Kuaipedia: a Large-scale Multi-modal Short-video Encyclopedia. (arXiv:2211.00732v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.00732
&lt;/p&gt;
&lt;p&gt;
Kuaipedia&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#22810;&#27169;&#24335;&#30701;&#35270;&#39057;&#30334;&#31185;&#20840;&#20070;&#65292;&#36890;&#36807;&#30693;&#35782;&#35270;&#39057;&#30340;&#24418;&#24335;&#65292;&#33021;&#22815;&#36731;&#26494;&#34920;&#36798;&#32593;&#27665;&#23545;&#26576;&#20010;&#39033;&#30446;&#30340;&#21508;&#20010;&#26041;&#38754;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;20&#24180;&#20013;&#65292;&#22312;&#32447;&#30334;&#31185;&#20840;&#20070;&#65288;&#22914;&#32500;&#22522;&#30334;&#31185;&#65289;&#24471;&#21040;&#20102;&#24456;&#22909;&#30340;&#21457;&#23637;&#21644;&#30740;&#31350;&#12290;&#20154;&#20204;&#21487;&#20197;&#22312;&#30001;&#24535;&#24895;&#32773;&#31038;&#21306;&#32534;&#36753;&#30340;&#32500;&#22522;&#39029;&#38754;&#19978;&#25214;&#21040;&#32500;&#22522;&#39033;&#30340;&#20219;&#20309;&#23646;&#24615;&#25110;&#20854;&#20182;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#25991;&#26412;&#12289;&#22270;&#29255;&#21644;&#34920;&#26684;&#24456;&#38590;&#34920;&#36798;&#32500;&#22522;&#39033;&#30340;&#26576;&#20123;&#26041;&#38754;&#12290;&#20363;&#22914;&#65292;&#24403;&#25105;&#20204;&#35848;&#35770;&#8220;&#26612;&#29356;&#8221;&#26102;&#65292;&#20154;&#20204;&#21487;&#33021;&#26356;&#20851;&#24515;&#8220;&#22914;&#20309;&#21890;&#20859;&#23427;&#8221;&#25110;&#8220;&#22914;&#20309;&#35757;&#32451;&#23427;&#19981;&#20445;&#25252;&#39135;&#29289;&#8221;&#12290;&#30446;&#21069;&#65292;&#30701;&#35270;&#39057;&#24179;&#21488;&#24050;&#25104;&#20026;&#22312;&#32447;&#19990;&#30028;&#30340;&#26631;&#24535;&#12290;&#26080;&#35770;&#20320;&#20351;&#29992;&#30340;&#26159;TikTok&#12289;Instagram&#12289;&#24555;&#25163;&#36824;&#26159;YouTube Shorts&#65292;&#30701;&#35270;&#39057;&#24212;&#29992;&#31243;&#24207;&#24050;&#25913;&#21464;&#20102;&#25105;&#20204;&#20170;&#22825;&#30340;&#20869;&#23481;&#28040;&#36153;&#21644;&#21019;&#20316;&#26041;&#24335;&#12290;&#38500;&#20102;&#20026;&#23089;&#20048;&#21046;&#20316;&#30701;&#35270;&#39057;&#22806;&#65292;&#25105;&#20204;&#36234;&#26469;&#36234;&#22810;&#22320;&#30475;&#21040;&#20316;&#32773;&#20204;&#22312;&#21508;&#34892;&#21508;&#19994;&#24191;&#27867;&#20998;&#20139;&#26377;&#35265;&#35299;&#30340;&#30693;&#35782;&#12290;&#36825;&#20123;&#30701;&#35270;&#39057;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#30693;&#35782;&#35270;&#39057;&#65292;&#21487;&#20197;&#36731;&#26494;&#34920;&#36798;&#28040;&#36153;&#32773;&#24819;&#20102;&#35299;&#26377;&#20851;&#26576;&#20010;&#39033;&#30446;&#65288;&#20363;&#22914;&#26612;&#29356;&#65289;&#30340;&#20219;&#20309;&#26041;&#38754;&#65288;&#20363;&#22914;&#27611;&#21457;&#25110;&#22914;&#20309;&#21890;&#20859;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online encyclopedias, such as Wikipedia, have been well-developed and researched in the last two decades. One can find any attributes or other information of a wiki item on a wiki page edited by a community of volunteers. However, the traditional text, images and tables can hardly express some aspects of an wiki item. For example, when we talk about ``Shiba Inu'', one may care more about ``How to feed it'' or ``How to train it not to protect its food''. Currently, short-video platforms have become a hallmark in the online world. Whether you're on TikTok, Instagram, Kuaishou, or YouTube Shorts, short-video apps have changed how we consume and create content today. Except for producing short videos for entertainment, we can find more and more authors sharing insightful knowledge widely across all walks of life. These short videos, which we call knowledge videos, can easily express any aspects (e.g. hair or how-to-feed) consumers want to know about an item (e.g. Shiba Inu), and they can b
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;Whisper&#27169;&#22411;&#34429;&#28982;&#22312;&#20998;&#24067;&#22806;&#36755;&#20837;&#21644;&#38543;&#26426;&#22122;&#22768;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#20986;&#33394;&#30340;&#31283;&#20581;&#24615;&#65292;&#20294;&#21364;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24178;&#25200;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36890;&#36807;&#29983;&#25104;&#26497;&#23567;&#30340;&#36755;&#20837;&#25200;&#21160;&#26469;&#22823;&#24133;&#38477;&#20302;Whisper&#30340;&#24615;&#33021;&#65292;&#24182;&#23545;&#22810;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#20135;&#29983;&#20102;&#24433;&#21709;&#12290;&#36825;&#20123;&#21457;&#29616;&#23545;&#20110;&#23454;&#38469;&#30340;&#23433;&#20840;&#38382;&#39064;&#21644;&#23545;&#25239;&#24615;&#31283;&#20581;ASR&#30340;&#38656;&#27714;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2210.17316</link><description>&lt;p&gt;
&#26377;&#19981;&#27490;&#19968;&#31181;&#31283;&#20581;&#24615;&#65306;&#29992;&#23545;&#25239;&#26679;&#26412;&#27450;&#39575;Whisper&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
There is more than one kind of robustness: Fooling Whisper with adversarial examples. (arXiv:2210.17316v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.17316
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;Whisper&#27169;&#22411;&#34429;&#28982;&#22312;&#20998;&#24067;&#22806;&#36755;&#20837;&#21644;&#38543;&#26426;&#22122;&#22768;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#20986;&#33394;&#30340;&#31283;&#20581;&#24615;&#65292;&#20294;&#21364;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24178;&#25200;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36890;&#36807;&#29983;&#25104;&#26497;&#23567;&#30340;&#36755;&#20837;&#25200;&#21160;&#26469;&#22823;&#24133;&#38477;&#20302;Whisper&#30340;&#24615;&#33021;&#65292;&#24182;&#23545;&#22810;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#20135;&#29983;&#20102;&#24433;&#21709;&#12290;&#36825;&#20123;&#21457;&#29616;&#23545;&#20110;&#23454;&#38469;&#30340;&#23433;&#20840;&#38382;&#39064;&#21644;&#23545;&#25239;&#24615;&#31283;&#20581;ASR&#30340;&#38656;&#27714;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Whisper&#26159;&#19968;&#31181;&#26368;&#36817;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#27169;&#22411;&#65292;&#23545;&#20110;&#20998;&#24067;&#22806;&#36755;&#20837;&#21644;&#38543;&#26426;&#22122;&#22768;&#37117;&#23637;&#31034;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#31283;&#20581;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#31283;&#20581;&#24615;&#22312;&#23545;&#25239;&#24178;&#25200;&#19979;&#24182;&#19981;&#36866;&#29992;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#29983;&#25104;&#26497;&#23567;&#30340;&#36755;&#20837;&#25200;&#21160;&#65288;&#20449;&#22122;&#27604;&#20026;35-45dB&#65289;&#65292;&#25105;&#20204;&#21487;&#20197;&#22823;&#24133;&#38477;&#20302;Whisper&#30340;&#24615;&#33021;&#65292;&#29978;&#33267;&#36716;&#24405;&#25105;&#20204;&#36873;&#25321;&#30340;&#30446;&#26631;&#21477;&#23376;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#36890;&#36807;&#27450;&#39575;Whisper&#35821;&#35328;&#26816;&#27979;&#22120;&#65292;&#25105;&#20204;&#21487;&#20197;&#36731;&#26494;&#38477;&#20302;&#22810;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#23545;&#19968;&#20010;&#24191;&#21463;&#27426;&#36814;&#30340;&#24320;&#28304;&#27169;&#22411;&#30340;&#33030;&#24369;&#24615;&#20855;&#26377;&#23454;&#38469;&#30340;&#23433;&#20840;&#24433;&#21709;&#65292;&#24182;&#24378;&#35843;&#20102;&#23545;&#25239;&#24615;&#31283;&#20581;ASR&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Whisper is a recent Automatic Speech Recognition (ASR) model displaying impressive robustness to both out-of-distribution inputs and random noise. In this work, we show that this robustness does not carry over to adversarial noise. We show that we can degrade Whisper performance dramatically, or even transcribe a target sentence of our choice, by generating very small input perturbations with Signal Noise Ratio of 35-45dB. We also show that by fooling the Whisper language detector we can very easily degrade the performance of multilingual models. These vulnerabilities of a widely popular open-source model have practical security implications and emphasize the need for adversarially robust ASR.
&lt;/p&gt;</description></item><item><title>CodeEditor&#26159;&#19968;&#20010;&#39044;&#35757;&#32451;&#20195;&#30721;&#32534;&#36753;&#27169;&#22411;&#65292;&#36890;&#36807;&#19987;&#38376;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#21644;&#20195;&#30721;&#32534;&#36753;&#20219;&#21153;&#30340;&#32467;&#21512;&#65292;&#25552;&#39640;&#20102;&#20195;&#30721;&#32534;&#36753;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2210.17040</link><description>&lt;p&gt;
CodeEditor: &#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#23398;&#20064;&#32534;&#36753;&#28304;&#20195;&#30721;
&lt;/p&gt;
&lt;p&gt;
CodeEditor: Learning to Edit Source Code with Pre-trained Models. (arXiv:2210.17040v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.17040
&lt;/p&gt;
&lt;p&gt;
CodeEditor&#26159;&#19968;&#20010;&#39044;&#35757;&#32451;&#20195;&#30721;&#32534;&#36753;&#27169;&#22411;&#65292;&#36890;&#36807;&#19987;&#38376;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#21644;&#20195;&#30721;&#32534;&#36753;&#20219;&#21153;&#30340;&#32467;&#21512;&#65292;&#25552;&#39640;&#20102;&#20195;&#30721;&#32534;&#36753;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36719;&#20214;&#24320;&#21457;&#36807;&#31243;&#20013;&#65292;&#24320;&#21457;&#20154;&#21592;&#32463;&#24120;&#38656;&#35201;&#36827;&#34892;&#37325;&#22797;&#30340;&#20195;&#30721;&#32534;&#36753;&#27963;&#21160;&#65292;&#20363;&#22914;&#20195;&#30721;&#37325;&#26500;&#31561;&#12290;&#39044;&#35757;&#32451;&#30340;&#20195;&#30721;&#32534;&#36753;&#27169;&#22411;&#24050;&#32463;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#39044;&#35757;&#32451;&#27169;&#22411;&#39318;&#20808;&#20351;&#29992;&#39044;&#35757;&#32451;&#20219;&#21153;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#20351;&#29992;&#20195;&#30721;&#32534;&#36753;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#12290;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#20027;&#35201;&#26159;&#20195;&#30721;&#22635;&#20805;&#20219;&#21153;&#65288;&#20363;&#22914;&#65292;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#65289;&#65292;&#36825;&#20123;&#20219;&#21153;&#26469;&#33258;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#19981;&#36866;&#29992;&#20110;&#33258;&#21160;&#20195;&#30721;&#32534;&#36753;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#20195;&#30721;&#32534;&#36753;&#30340;&#26032;&#22411;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;CodeEditor&#30340;&#26377;&#25928;&#39044;&#35757;&#32451;&#20195;&#30721;&#32534;&#36753;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#20195;&#30721;&#32534;&#36753;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#22823;&#37327;&#30340;&#30495;&#23454;&#20195;&#30721;&#29255;&#27573;&#20316;&#20026;&#21442;&#32771;&#65292;&#24182;&#20351;&#29992;&#19968;&#20010;&#24378;&#22823;&#30340;&#29983;&#25104;&#22120;&#23558;&#23427;&#20204;&#37325;&#20889;&#25104;&#21464;&#24322;&#29256;&#26412;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;CodeEditor&#23545;&#21464;&#24322;&#29256;&#26412;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#23558;&#20854;&#32534;&#36753;&#20026;&#27491;&#30830;&#30340;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developers often perform repetitive code editing activities for various reasons (e.g., code refactoring) during software development. Pre-trained code editing models have achieved the state-of-the-art (SOTA) results. Pre-trained models are first pre-trained with pre-training tasks and fine-tuned with the code editing task. Existing pre-training tasks mainly are code infilling tasks (e.g., masked language modeling), which are derived from the natural language processing field and are not designed for automatic code editing.  This paper proposes a novel pre-training task specialized in code editing and presents an effective pre-trained code editing model named CodeEditor. Our pre-training task further improves the performance and generalization ability of code editing models. Specifically, we collect lots of real-world code snippets as the ground truth and use a powerful generator to rewrite them into mutated versions. Then, we pre-train our CodeEditor to edit mutated versions into the c
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#23398;&#20064;&#20013;&#23384;&#22312;&#19968;&#20010;&#31616;&#21333;&#32780;&#23450;&#37327;&#30340;&#25968;&#25454;&#20998;&#31163;&#23450;&#24459;&#65292;&#27599;&#19968;&#23618;&#37117;&#20197;&#24658;&#23450;&#30340;&#20960;&#20309;&#36895;&#29575;&#25913;&#21892;&#25968;&#25454;&#30340;&#20998;&#31163;&#31243;&#24230;&#12290;&#36825;&#20010;&#23450;&#24459;&#20026;&#26550;&#26500;&#35774;&#35745;&#12289;&#25552;&#39640;&#27169;&#22411;&#30340;&#20581;&#22766;&#24615;&#21644;&#26679;&#26412;&#22806;&#24615;&#33021;&#20197;&#21450;&#39044;&#27979;&#30340;&#35299;&#37322;&#25552;&#20379;&#20102;&#23454;&#38469;&#30340;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2210.17020</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#20998;&#31163;&#23450;&#24459;
&lt;/p&gt;
&lt;p&gt;
A Law of Data Separation in Deep Learning. (arXiv:2210.17020v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.17020
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20013;&#23384;&#22312;&#19968;&#20010;&#31616;&#21333;&#32780;&#23450;&#37327;&#30340;&#25968;&#25454;&#20998;&#31163;&#23450;&#24459;&#65292;&#27599;&#19968;&#23618;&#37117;&#20197;&#24658;&#23450;&#30340;&#20960;&#20309;&#36895;&#29575;&#25913;&#21892;&#25968;&#25454;&#30340;&#20998;&#31163;&#31243;&#24230;&#12290;&#36825;&#20010;&#23450;&#24459;&#20026;&#26550;&#26500;&#35774;&#35745;&#12289;&#25552;&#39640;&#27169;&#22411;&#30340;&#20581;&#22766;&#24615;&#21644;&#26679;&#26412;&#22806;&#24615;&#33021;&#20197;&#21450;&#39044;&#27979;&#30340;&#35299;&#37322;&#25552;&#20379;&#20102;&#23454;&#38469;&#30340;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#28145;&#24230;&#23398;&#20064;&#22312;&#31185;&#23398;&#30340;&#35768;&#22810;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#20854;&#40657;&#30418;&#29305;&#24615;&#38459;&#30861;&#20102;&#26410;&#26469;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#30340;&#26550;&#26500;&#35774;&#35745;&#21644;&#39640;&#39118;&#38505;&#20915;&#31574;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#20013;&#38388;&#23618;&#20013;&#22914;&#20309;&#22788;&#29702;&#25968;&#25454;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#26159;&#19968;&#20010;&#31616;&#21333;&#32780;&#23450;&#37327;&#30340;&#23450;&#24459;&#65292;&#23427;&#35268;&#23450;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#26681;&#25454;&#31867;&#21035;&#25104;&#21592;&#23558;&#25968;&#25454;&#22312;&#25152;&#26377;&#23618;&#20013;&#20998;&#31163;&#20986;&#26469;&#36827;&#34892;&#20998;&#31867;&#12290;&#36825;&#20010;&#23450;&#24459;&#34920;&#26126;&#65292;&#27599;&#19968;&#23618;&#37117;&#20197;&#24658;&#23450;&#30340;&#20960;&#20309;&#36895;&#29575;&#25913;&#21892;&#25968;&#25454;&#30340;&#20998;&#31163;&#31243;&#24230;&#65292;&#24182;&#19988;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#35266;&#23519;&#21040;&#20102;&#23427;&#30340;&#20986;&#29616;&#65292;&#26080;&#35770;&#26159;&#22312;&#19968;&#31995;&#21015;&#32593;&#32476;&#26550;&#26500;&#36824;&#26159;&#25968;&#25454;&#38598;&#19978;&#12290;&#36825;&#20010;&#23450;&#24459;&#20026;&#26550;&#26500;&#35774;&#35745;&#12289;&#25552;&#39640;&#27169;&#22411;&#30340;&#20581;&#22766;&#24615;&#21644;&#26679;&#26412;&#22806;&#24615;&#33021;&#20197;&#21450;&#39044;&#27979;&#30340;&#35299;&#37322;&#25552;&#20379;&#20102;&#23454;&#38469;&#30340;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
While deep learning has enabled significant advances in many areas of science, its black-box nature hinders architecture design for future artificial intelligence applications and interpretation for high-stakes decision makings. We addressed this issue by studying the fundamental question of how deep neural networks process data in the intermediate layers. Our finding is a simple and quantitative law that governs how deep neural networks separate data according to class membership throughout all layers for classification. This law shows that each layer improves data separation at a constant geometric rate, and its emergence is observed in a collection of network architectures and datasets during training. This law offers practical guidelines for designing architectures, improving model robustness and out-of-sample performance, as well as interpreting the predictions.
&lt;/p&gt;</description></item><item><title>Lib-SibGMU&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#22823;&#23398;&#22270;&#20070;&#39302;&#20511;&#38405;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#24320;&#21457;&#12290;&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;fastText&#27169;&#22411;&#20316;&#20026;&#21521;&#37327;&#21270;&#22120;&#21487;&#20197;&#33719;&#24471;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2208.12356</link><description>&lt;p&gt;
Lib-SibGMU -- &#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#24320;&#21457;&#30340;&#22823;&#23398;&#22270;&#20070;&#39302;&#20511;&#38405;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Lib-SibGMU -- A University Library Circulation Dataset for Recommender Systems Developmen. (arXiv:2208.12356v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.12356
&lt;/p&gt;
&lt;p&gt;
Lib-SibGMU&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#22823;&#23398;&#22270;&#20070;&#39302;&#20511;&#38405;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#24320;&#21457;&#12290;&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;fastText&#27169;&#22411;&#20316;&#20026;&#21521;&#37327;&#21270;&#22120;&#21487;&#20197;&#33719;&#24471;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20197;CC BY 4.0&#35768;&#21487;&#35777;&#24320;&#28304;&#20102;Lib-SibGMU&#30340;&#22823;&#23398;&#22270;&#20070;&#39302;&#20511;&#38405;&#25968;&#25454;&#38598;&#65292;&#20379;&#24191;&#22823;&#30740;&#31350;&#31038;&#21306;&#20351;&#29992;&#65292;&#24182;&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#20027;&#35201;&#30340;&#25512;&#33616;&#31995;&#32479;&#31639;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#30001;&#23558;&#20511;&#38405;&#20070;&#31821;&#21382;&#21490;&#36716;&#21270;&#20026;&#21521;&#37327;&#30340;&#21521;&#37327;&#21270;&#22120;&#21644;&#19968;&#20010;&#22522;&#20110;&#37051;&#22495;&#30340;&#25512;&#33616;&#22120;&#32452;&#25104;&#30340;&#25512;&#33616;&#20307;&#31995;&#32467;&#26500;&#65292;&#20998;&#21035;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#35777;&#26126;&#20351;&#29992;fastText&#27169;&#22411;&#20316;&#20026;&#21521;&#37327;&#21270;&#22120;&#21487;&#20197;&#33719;&#24471;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We opensource under CC BY 4.0 license Lib-SibGMU - a university library circulation dataset - for a wide research community, and benchmark major algorithms for recommender systems on this dataset. For a recommender architecture that consists of a vectorizer that turns the history of the books borrowed into a vector, and a neighborhood-based recommender, trained separately, we show that using the fastText model as a vectorizer delivers competitive results.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20851;&#31995;&#34892;&#21160;&#22522;&#30784;&#65288;RABs&#65289;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#36890;&#36807;&#21462;&#28040;&#20851;&#31995;&#29366;&#24577;&#21644;&#34892;&#21160;&#30340;&#38480;&#21046;&#65292;&#23454;&#29616;&#20102;&#23545;&#21160;&#24577;&#31995;&#32479;&#30340;&#24314;&#27169;&#21644;&#39564;&#35777;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#21442;&#25968;&#21270;&#23433;&#20840;&#24615;&#30340;&#30740;&#31350;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2208.06377</link><description>&lt;p&gt;
&#20851;&#31995;&#34892;&#21160;&#22522;&#30784;&#65306;&#24418;&#24335;&#21270;&#12289;&#26377;&#25928;&#23433;&#20840;&#39564;&#35777;&#21644;&#19981;&#21464;&#37327;&#65288;&#25193;&#23637;&#29256;&#65289;
&lt;/p&gt;
&lt;p&gt;
Relational Action Bases: Formalization, Effective Safety Verification, and Invariants (Extended Version). (arXiv:2208.06377v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.06377
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20851;&#31995;&#34892;&#21160;&#22522;&#30784;&#65288;RABs&#65289;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#36890;&#36807;&#21462;&#28040;&#20851;&#31995;&#29366;&#24577;&#21644;&#34892;&#21160;&#30340;&#38480;&#21046;&#65292;&#23454;&#29616;&#20102;&#23545;&#21160;&#24577;&#31995;&#32479;&#30340;&#24314;&#27169;&#21644;&#39564;&#35777;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#21442;&#25968;&#21270;&#23433;&#20840;&#24615;&#30340;&#30740;&#31350;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#12289;&#19994;&#21153;&#27969;&#31243;&#31649;&#29702;&#21644;&#25968;&#25454;&#24211;&#29702;&#35770;&#20013;&#65292;&#23545;&#22522;&#20110;&#20851;&#31995;&#34920;&#31034;&#30340;&#21160;&#24577;&#31995;&#32479;&#30340;&#24314;&#27169;&#21644;&#39564;&#35777;&#26159;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#20351;&#36825;&#20123;&#31995;&#32479;&#36866;&#20110;&#39564;&#35777;&#65292;&#38656;&#35201;&#38480;&#21046;&#27599;&#20010;&#20851;&#31995;&#29366;&#24577;&#20013;&#23384;&#20648;&#30340;&#20449;&#24687;&#37327;&#65292;&#25110;&#32773;&#23545;&#34892;&#21160;&#30340;&#20808;&#20915;&#26465;&#20214;&#21644;&#24433;&#21709;&#26045;&#21152;&#38480;&#21046;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20851;&#31995;&#34892;&#21160;&#22522;&#30784;&#65288;RABs&#65289;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#36890;&#36807;&#21462;&#28040;&#36825;&#20123;&#38480;&#21046;&#20174;&#32780;&#25512;&#24191;&#29616;&#26377;&#27169;&#22411;&#65306;&#26080;&#38480;&#21046;&#30340;&#20851;&#31995;&#29366;&#24577;&#21487;&#20197;&#36890;&#36807;&#21487;&#20197;&#23545;&#25968;&#25454;&#36827;&#34892;&#23384;&#22312;&#37327;&#35789;&#21644;&#20840;&#31216;&#37327;&#35789;&#37327;&#21270;&#30340;&#34892;&#21160;&#28436;&#21464;&#65292;&#24182;&#19988;&#21487;&#20197;&#21033;&#29992;&#24102;&#26377;&#31639;&#26415;&#35859;&#35789;&#30340;&#25968;&#23383;&#25968;&#25454;&#31867;&#22411;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#65288;&#36817;&#20284;&#65289;&#22522;&#20110;SMT&#30340;&#21453;&#21521;&#25628;&#32034;&#30740;&#31350;&#20102;RABs&#30340;&#21442;&#25968;&#21270;&#23433;&#20840;&#24615;&#65292;&#25361;&#20986;&#20102;&#32467;&#26524;&#36807;&#31243;&#30340;&#22522;&#26412;&#20803;&#23646;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22914;&#20309;&#36890;&#36807;&#29616;&#26377;&#39564;&#35777;&#27169;&#22359;&#30340;&#32452;&#21512;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modeling and verification of dynamic systems operating over a relational representation of states are increasingly investigated problems in AI, Business Process Management, and Database Theory. To make these systems amenable to verification, the amount of information stored in each relational state needs to be bounded, or restrictions are imposed on the preconditions and effects of actions. We introduce the general framework of relational action bases (RABs), which generalizes existing models by lifting both these restrictions: unbounded relational states can be evolved through actions that can quantify both existentially and universally over the data, and that can exploit numerical datatypes with arithmetic predicates. We then study parameterized safety of RABs via (approximated) SMT-based backward search, singling out essential meta-properties of the resulting procedure, and showing how it can be realized by an off-the-shelf combination of existing verification modules of the state-o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26412;&#22320;&#32858;&#21512;&#25551;&#36848;&#31526;&#25552;&#21462;&#27010;&#24565;&#30340;&#26041;&#27861;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#39564;&#35777;&#36807;&#31243;&#65292;&#29992;&#20110;&#20943;&#23569;&#27010;&#24565;&#25552;&#21462;&#26041;&#27861;&#30340;&#20154;&#24037;&#24178;&#39044;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2206.04531</link><description>&lt;p&gt;
ECLAD: &#20351;&#29992;&#26412;&#22320;&#32858;&#21512;&#25551;&#36848;&#31526;&#25552;&#21462;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
ECLAD: Extracting Concepts with Local Aggregated Descriptors. (arXiv:2206.04531v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.04531
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26412;&#22320;&#32858;&#21512;&#25551;&#36848;&#31526;&#25552;&#21462;&#27010;&#24565;&#30340;&#26041;&#27861;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#39564;&#35777;&#36807;&#31243;&#65292;&#29992;&#20110;&#20943;&#23569;&#27010;&#24565;&#25552;&#21462;&#26041;&#27861;&#30340;&#20154;&#24037;&#24178;&#39044;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#22312;&#20851;&#38190;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#22810;&#65292;&#20854;&#20013;&#40065;&#26834;&#24615;&#21644;&#23545;&#40784;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#25552;&#20986;&#20102;&#36890;&#36807;&#27010;&#24565;&#25552;&#21462;&#26469;&#29983;&#25104;CNN&#39044;&#27979;&#36807;&#31243;&#30340;&#39640;&#32423;&#35299;&#37322;&#12290;&#34429;&#28982;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#26816;&#27979;&#22270;&#20687;&#20013;&#26159;&#21542;&#23384;&#22312;&#27010;&#24565;&#65292;&#20294;&#26080;&#27861;&#30830;&#23450;&#20854;&#20301;&#32622;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#32570;&#20047;&#36866;&#24403;&#30340;&#39564;&#35777;&#31243;&#24207;&#65292;&#24456;&#38590;&#23545;&#36825;&#20123;&#26041;&#27861;&#36827;&#34892;&#20844;&#24179;&#27604;&#36739;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CNN&#28608;&#27963;&#22270;&#20687;&#30340;&#20687;&#32032;&#32858;&#21512;&#34920;&#31034;&#30340;&#33258;&#21160;&#27010;&#24565;&#25552;&#21462;&#21644;&#23450;&#20301;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#27010;&#24565;&#25552;&#21462;&#25216;&#26415;&#39564;&#35777;&#36807;&#31243;&#65292;&#35813;&#25968;&#25454;&#38598;&#20855;&#26377;&#20854;&#20027;&#35201;&#32452;&#20214;&#30340;&#20687;&#32032;&#27880;&#37322;&#65292;&#20943;&#23569;&#20102;&#20154;&#24037;&#24178;&#39044;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Convolutional neural networks (CNNs) are increasingly being used in critical systems, where robustness and alignment are crucial. In this context, the field of explainable artificial intelligence has proposed the generation of high-level explanations of the prediction process of CNNs through concept extraction. While these methods can detect whether or not a concept is present in an image, they are unable to determine its location. What is more, a fair comparison of such approaches is difficult due to a lack of proper validation procedures. To address these issues, we propose a novel method for automatic concept extraction and localization based on representations obtained through pixel-wise aggregations of CNN activation maps. Further, we introduce a process for the validation of concept-extraction techniques based on synthetic datasets with pixel-wise annotations of their main components, reducing the need for human intervention. Extensive experimentation on both synthetic and real-w
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;DenseUAV&#65292;&#20026;&#26080;&#20154;&#26426;&#22312;&#20302;&#31354;&#22478;&#24066;&#29615;&#22659;&#20013;&#30340;&#33258;&#23450;&#20301;&#20219;&#21153;&#35774;&#35745;&#12290;&#35813;&#25968;&#25454;&#38598;&#37319;&#29992;&#20102;&#23494;&#38598;&#37319;&#26679;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#25968;&#25454;&#38598;&#20013;&#24573;&#35270;&#30340;&#23494;&#38598;&#37319;&#26679;&#21644;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2201.09201</link><description>&lt;p&gt;
&#22522;&#20110;&#35270;&#35273;&#30340;&#20302;&#31354;&#22478;&#24066;&#29615;&#22659;&#20013;&#26080;&#20154;&#26426;&#33258;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Vision-Based UAV Self-Positioning in Low-Altitude Urban Environments. (arXiv:2201.09201v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.09201
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;DenseUAV&#65292;&#20026;&#26080;&#20154;&#26426;&#22312;&#20302;&#31354;&#22478;&#24066;&#29615;&#22659;&#20013;&#30340;&#33258;&#23450;&#20301;&#20219;&#21153;&#35774;&#35745;&#12290;&#35813;&#25968;&#25454;&#38598;&#37319;&#29992;&#20102;&#23494;&#38598;&#37319;&#26679;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#25968;&#25454;&#38598;&#20013;&#24573;&#35270;&#30340;&#23494;&#38598;&#37319;&#26679;&#21644;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#20154;&#26426;&#20381;&#36182;&#20110;&#21355;&#26143;&#31995;&#32479;&#36827;&#34892;&#31283;&#23450;&#23450;&#20301;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21355;&#26143;&#35206;&#30422;&#26377;&#38480;&#25110;&#36890;&#20449;&#20013;&#26029;&#65292;&#26080;&#20154;&#26426;&#21487;&#33021;&#20250;&#22833;&#21435;&#26469;&#33258;&#21355;&#26143;&#23450;&#20301;&#31995;&#32479;&#30340;&#20449;&#21495;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#22522;&#20110;&#35270;&#35273;&#30340;&#25216;&#26415;&#21487;&#20197;&#20316;&#20026;&#26367;&#20195;&#26041;&#26696;&#65292;&#30830;&#20445;&#26080;&#20154;&#26426;&#30340;&#33258;&#23450;&#20301;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#25968;&#25454;&#38598;&#37117;&#26159;&#20026;&#26080;&#20154;&#26426;&#35782;&#21035;&#30340;&#22320;&#29702;&#23450;&#20301;&#20219;&#21153;&#32780;&#24320;&#21457;&#30340;&#65292;&#32780;&#19981;&#26159;&#20026;&#26080;&#20154;&#26426;&#30340;&#33258;&#23450;&#20301;&#20219;&#21153;&#32780;&#35774;&#35745;&#30340;&#12290;&#27492;&#22806;&#65292;&#24403;&#21069;&#30340;&#26080;&#20154;&#26426;&#25968;&#25454;&#38598;&#20351;&#29992;&#20102;&#23545;&#21512;&#25104;&#25968;&#25454;&#65288;&#22914;Google Maps&#65289;&#36827;&#34892;&#31163;&#25955;&#37319;&#26679;&#65292;&#22240;&#27492;&#24573;&#30053;&#20102;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#32463;&#24120;&#36935;&#21040;&#30340;&#23494;&#38598;&#37319;&#26679;&#21644;&#19981;&#30830;&#23450;&#24615;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;DenseUAV&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#20026;&#26080;&#20154;&#26426;&#33258;&#23450;&#20301;&#20219;&#21153;&#35774;&#35745;&#30340;&#25968;&#25454;&#38598;&#12290;DenseUAV&#22312;&#20302;&#31354;&#22478;&#24066;&#29615;&#22659;&#20013;&#37319;&#29992;&#20102;&#23545;&#26080;&#20154;&#26426;&#22270;&#20687;&#30340;&#23494;&#38598;&#37319;&#26679;&#12290;&#24635;&#20849;&#65292;&#36229;&#36807;27K&#20221;&#26080;&#20154;&#26426;&#22270;&#20687;&#34987;&#37319;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unmanned Aerial Vehicles (UAVs) rely on satellite systems for stable positioning. However, due to limited satellite coverage or communication disruptions, UAVs may lose signals from satellite-based positioning systems. In such situations, vision-based techniques can serve as an alternative, ensuring the self-positioning capability of UAVs. However, most of the existing datasets are developed for the geo-localization tasks of the objects identified by UAVs, rather than the self-positioning task of UAVs. Furthermore, the current UAV datasets use discrete sampling on synthetic data, such as Google Maps, thereby neglecting the crucial aspects of dense sampling and the uncertainties commonly experienced in real-world scenarios. To address these issues, this paper presents a new dataset, DenseUAV, which is the first publicly available dataset designed for the UAV self-positioning task. DenseUAV adopts dense sampling on UAV images obtained in low-altitude urban settings. In total, over 27K UA
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#22495;&#23383;&#31526;&#36317;&#31163;&#24863;&#30693;&#65288;MDCDP&#65289;&#30340;&#26032;&#39062;&#27169;&#22359;&#65292;&#29992;&#20110;&#35299;&#20915;&#22330;&#26223;&#25991;&#26412;&#35782;&#21035;&#20013;&#29305;&#24449;&#21644;&#23383;&#31526;&#23545;&#40784;&#19981;&#20934;&#30830;&#30340;&#38382;&#39064;&#12290;&#35813;&#27169;&#22359;&#36890;&#36807;&#20132;&#21449;&#27880;&#24847;&#26426;&#21046;&#34701;&#21512;&#35270;&#35273;&#21644;&#35821;&#20041;&#29305;&#24449;&#65292;&#24182;&#29983;&#25104;&#19968;&#20010;&#20869;&#23481;&#24863;&#30693;&#23884;&#20837;&#26469;&#24863;&#30693;&#23383;&#31526;&#20301;&#32622;&#12290;</title><link>http://arxiv.org/abs/2111.11011</link><description>&lt;p&gt;
CDistNet&#65306;&#24863;&#30693;&#22810;&#22495;&#23383;&#31526;&#36317;&#31163;&#30340;&#40065;&#26834;&#25991;&#26412;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
CDistNet: Perceiving Multi-Domain Character Distance for Robust Text Recognition. (arXiv:2111.11011v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.11011
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#22495;&#23383;&#31526;&#36317;&#31163;&#24863;&#30693;&#65288;MDCDP&#65289;&#30340;&#26032;&#39062;&#27169;&#22359;&#65292;&#29992;&#20110;&#35299;&#20915;&#22330;&#26223;&#25991;&#26412;&#35782;&#21035;&#20013;&#29305;&#24449;&#21644;&#23383;&#31526;&#23545;&#40784;&#19981;&#20934;&#30830;&#30340;&#38382;&#39064;&#12290;&#35813;&#27169;&#22359;&#36890;&#36807;&#20132;&#21449;&#27880;&#24847;&#26426;&#21046;&#34701;&#21512;&#35270;&#35273;&#21644;&#35821;&#20041;&#29305;&#24449;&#65292;&#24182;&#29983;&#25104;&#19968;&#20010;&#20869;&#23481;&#24863;&#30693;&#23884;&#20837;&#26469;&#24863;&#30693;&#23383;&#31526;&#20301;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#32534;&#30721;-&#35299;&#30721;&#26694;&#26550;&#22312;&#22330;&#26223;&#25991;&#26412;&#35782;&#21035;&#20013;&#36234;&#26469;&#36234;&#27969;&#34892;&#65292;&#20027;&#35201;&#26159;&#22240;&#20026;&#23427;&#33021;&#22815;&#33258;&#28982;&#22320;&#25972;&#21512;&#26469;&#33258;&#35270;&#35273;&#21644;&#35821;&#20041;&#39046;&#22495;&#30340;&#35782;&#21035;&#32447;&#32034;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#36825;&#20004;&#31181;&#32447;&#32034;&#24182;&#19981;&#24635;&#26159;&#24456;&#22909;&#22320;&#27880;&#20876;&#65292;&#22240;&#27492;&#22312;&#22256;&#38590;&#25991;&#26412;&#65288;&#20363;&#22914;&#65292;&#20855;&#26377;&#32597;&#35265;&#24418;&#29366;&#30340;&#25991;&#26412;&#65289;&#20013;&#65292;&#29305;&#24449;&#21644;&#23383;&#31526;&#21487;&#33021;&#19981;&#23545;&#40784;&#12290;&#22240;&#27492;&#65292;&#24341;&#20837;&#20102;&#23383;&#31526;&#20301;&#32622;&#31561;&#32422;&#26463;&#26469;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#23613;&#31649;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#35270;&#35273;&#21644;&#35821;&#20041;&#20173;&#28982;&#26159;&#20998;&#21035;&#24314;&#27169;&#30340;&#65292;&#23427;&#20204;&#20043;&#38388;&#21482;&#26159;&#26494;&#25955;&#20851;&#32852;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#27169;&#22359;&#65292;&#31216;&#20026;&#22810;&#22495;&#23383;&#31526;&#36317;&#31163;&#24863;&#30693;&#65288;MDCDP&#65289;&#65292;&#29992;&#20110;&#24314;&#31435;&#19968;&#20010;&#35270;&#35273;&#19978;&#21644;&#35821;&#20041;&#19978;&#30456;&#20851;&#30340;&#20301;&#32622;&#23884;&#20837;&#12290;MDCDP&#20351;&#29992;&#20301;&#32622;&#23884;&#20837;&#36890;&#36807;&#20132;&#21449;&#27880;&#24847;&#26426;&#21046;&#26597;&#35810;&#35270;&#35273;&#21644;&#35821;&#20041;&#29305;&#24449;&#12290;&#36825;&#20004;&#31181;&#32447;&#32034;&#34987;&#34701;&#21512;&#21040;&#20301;&#32622;&#20998;&#25903;&#20013;&#65292;&#29983;&#25104;&#19968;&#20010;&#33021;&#22815;&#24456;&#22909;&#22320;&#24863;&#30693;&#23383;&#31526;&#30340;&#20869;&#23481;&#24863;&#30693;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Transformer-based encoder-decoder framework is becoming popular in scene text recognition, largely because it naturally integrates recognition clues from both visual and semantic domains. However, recent studies show that the two kinds of clues are not always well registered and therefore, feature and character might be misaligned in difficult text (e.g., with a rare shape). As a result, constraints such as character position are introduced to alleviate this problem. Despite certain success, visual and semantic are still separately modeled and they are merely loosely associated. In this paper, we propose a novel module called Multi-Domain Character Distance Perception (MDCDP) to establish a visually and semantically related position embedding. MDCDP uses the position embedding to query both visual and semantic features following the cross-attention mechanism. The two kinds of clues are fused into the position branch, generating a content-aware embedding that well perceives characte
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30456;&#20284;&#24615;&#26144;&#23556;&#30340;&#31070;&#32463;&#27169;&#22411;&#37325;&#32534;&#31243;&#26041;&#27861;&#65292;&#29992;&#20110;&#20302;&#36164;&#28304;&#21475;&#35821;&#21629;&#20196;&#20998;&#31867;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#26377;&#38480;&#30340;&#25968;&#25454;&#26465;&#20214;&#19979;&#65292;&#35813;&#26041;&#27861;&#22312;&#38463;&#25289;&#20271;&#35821;&#21644;&#31435;&#38518;&#23451;&#35821;&#21629;&#20196;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2110.03894</link><description>&lt;p&gt;
&#22522;&#20110;&#30456;&#20284;&#24615;&#26144;&#23556;&#30340;&#31070;&#32463;&#27169;&#22411;&#37325;&#32534;&#31243;&#29992;&#20110;&#20302;&#36164;&#28304;&#21475;&#35821;&#21629;&#20196;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Neural Model Reprogramming with Similarity Based Mapping for Low-Resource Spoken Command Classification. (arXiv:2110.03894v4 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.03894
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30456;&#20284;&#24615;&#26144;&#23556;&#30340;&#31070;&#32463;&#27169;&#22411;&#37325;&#32534;&#31243;&#26041;&#27861;&#65292;&#29992;&#20110;&#20302;&#36164;&#28304;&#21475;&#35821;&#21629;&#20196;&#20998;&#31867;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#26377;&#38480;&#30340;&#25968;&#25454;&#26465;&#20214;&#19979;&#65292;&#35813;&#26041;&#27861;&#22312;&#38463;&#25289;&#20271;&#35821;&#21644;&#31435;&#38518;&#23451;&#35821;&#21629;&#20196;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#25239;&#37325;&#32534;&#31243;&#65288;AR&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#20302;&#36164;&#28304;&#21475;&#35821;&#21629;&#20196;&#35782;&#21035;&#65288;SCR&#65289;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;AR-SCR&#31995;&#32479;&#12290;AR&#36807;&#31243;&#26088;&#22312;&#20462;&#25913;&#26469;&#33258;&#30446;&#26631;&#39046;&#22495;&#30340;&#22768;&#23398;&#20449;&#21495;&#65292;&#20197;&#37325;&#26032;&#35843;&#25972;&#39044;&#35757;&#32451;&#30340;SCR&#27169;&#22411;&#65292;&#20174;&#32780;&#23454;&#29616;&#37325;&#32534;&#31243;&#12290;&#20026;&#20102;&#35299;&#20915;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#20043;&#38388;&#30340;&#26631;&#31614;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#24182;&#36827;&#19968;&#27493;&#25552;&#39640;AR&#30340;&#31283;&#23450;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#26631;&#31614;&#26144;&#23556;&#25216;&#26415;&#26469;&#23545;&#40784;&#31867;&#21035;&#12290;&#27492;&#22806;&#65292;&#23558;&#36801;&#31227;&#23398;&#20064;&#65288;TL&#65289;&#25216;&#26415;&#19982;&#21407;&#22987;AR&#36807;&#31243;&#30456;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#36866;&#24212;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#20302;&#36164;&#28304;SCR&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25552;&#20986;&#30340;AR-SCR&#31995;&#32479;&#65292;&#21253;&#25324;&#38463;&#25289;&#20271;&#35821;&#12289;&#31435;&#38518;&#23451;&#35821;&#21644;&#35328;&#35821;&#38556;&#30861;&#24615;&#26222;&#36890;&#35805;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22823;&#35268;&#27169;&#33521;&#35821;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#39044;&#35757;&#32451;AM&#30340;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#30340;AR-SCR&#31995;&#32479;&#22312;&#38463;&#25289;&#20271;&#35821;&#21644;&#31435;&#38518;&#23451;&#35821;&#21629;&#20196;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#19988;&#20165;&#20351;&#29992;&#26377;&#38480;&#25968;&#37327;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we propose a novel adversarial reprogramming (AR) approach for low-resource spoken command recognition (SCR), and build an AR-SCR system. The AR procedure aims to modify the acoustic signals (from the target domain) to repurpose a pretrained SCR model (from the source domain). To solve the label mismatches between source and target domains, and further improve the stability of AR, we propose a novel similarity-based label mapping technique to align classes. In addition, the transfer learning (TL) technique is combined with the original AR process to improve the model adaptation capability. We evaluate the proposed AR-SCR system on three low-resource SCR datasets, including Arabic, Lithuanian, and dysarthric Mandarin speech. Experimental results show that with a pretrained AM trained on a large-scale English dataset, the proposed AR-SCR system outperforms the current state-of-the-art results on Arabic and Lithuanian speech commands datasets, with only a limited amount of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#34394;&#25311;&#30693;&#35782;&#22270;&#35889;&#26144;&#23556;&#27169;&#24335;&#30446;&#24405;&#65292;&#29992;&#20110;&#25903;&#25345;&#38142;&#25509;&#25968;&#25454;&#24211;&#21644;&#26412;&#20307;&#30340;&#26144;&#23556;&#31649;&#29702;&#65292;&#35813;&#30446;&#24405;&#36890;&#36807;&#20998;&#26512;&#23454;&#38469;&#29992;&#20363;&#21644;VKG&#22522;&#20934;&#65292;&#25193;&#23637;&#21644;&#23436;&#21892;&#20102;&#24050;&#26377;&#26041;&#27861;&#21644;&#27169;&#24335;&#65292;&#24182;&#39564;&#35777;&#20102;&#20854;&#22312;&#22810;&#31181;&#22330;&#26223;&#20013;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2012.01917</link><description>&lt;p&gt;
&#34394;&#25311;&#30693;&#35782;&#22270;&#35889;&#30340;&#26144;&#23556;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Mapping Patterns for Virtual Knowledge Graphs. (arXiv:2012.01917v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2012.01917
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#34394;&#25311;&#30693;&#35782;&#22270;&#35889;&#26144;&#23556;&#27169;&#24335;&#30446;&#24405;&#65292;&#29992;&#20110;&#25903;&#25345;&#38142;&#25509;&#25968;&#25454;&#24211;&#21644;&#26412;&#20307;&#30340;&#26144;&#23556;&#31649;&#29702;&#65292;&#35813;&#30446;&#24405;&#36890;&#36807;&#20998;&#26512;&#23454;&#38469;&#29992;&#20363;&#21644;VKG&#22522;&#20934;&#65292;&#25193;&#23637;&#21644;&#23436;&#21892;&#20102;&#24050;&#26377;&#26041;&#27861;&#21644;&#27169;&#24335;&#65292;&#24182;&#39564;&#35777;&#20102;&#20854;&#22312;&#22810;&#31181;&#22330;&#26223;&#20013;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34394;&#25311;&#30693;&#35782;&#22270;&#35889;&#65288;VKG&#65289;&#26159;&#38598;&#25104;&#21644;&#35775;&#38382;&#36951;&#30041;&#25968;&#25454;&#28304;&#30340;&#26368;&#26377;&#21069;&#26223;&#30340;&#33539;&#24335;&#20043;&#19968;&#12290;&#38598;&#25104;&#36807;&#31243;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#29942;&#39048;&#26159;&#23450;&#20041;&#12289;&#39564;&#35777;&#21644;&#32500;&#25252;&#23558;&#25968;&#25454;&#28304;&#19982;&#39046;&#22495;&#26412;&#20307;&#38142;&#25509;&#36215;&#26469;&#30340;&#26144;&#23556;&#12290;&#20026;&#20102;&#25903;&#25345;&#25972;&#20010;&#26144;&#23556;&#29983;&#21629;&#21608;&#26399;&#30340;&#31649;&#29702;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#26144;&#23556;&#27169;&#24335;&#30446;&#24405;&#65292;&#29992;&#20110;&#36830;&#25509;&#25968;&#25454;&#24211;&#21644;&#26412;&#20307;&#26102;&#20986;&#29616;&#30340;&#22797;&#26434;&#26144;&#23556;&#27169;&#24335;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#20511;&#37492;&#20102;&#25968;&#25454;&#31649;&#29702;&#12289;&#25968;&#25454;&#20998;&#26512;&#21644;&#27010;&#24565;&#24314;&#27169;&#39046;&#22495;&#20013;&#30340;&#25104;&#29087;&#26041;&#27861;&#21644;&#27169;&#24335;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#20855;&#20307;&#30340;VKG&#22522;&#20934;&#21644;&#23454;&#38469;&#29992;&#20363;&#65292;&#32771;&#34385;&#20102;&#25968;&#25454;&#28304;&#21644;&#26412;&#20307;&#20043;&#38388;&#22266;&#26377;&#30340;&#38459;&#25239;&#19981;&#21305;&#37197;&#38382;&#39064;&#36827;&#34892;&#20102;&#25193;&#23637;&#21644;&#23436;&#21892;&#12290;&#25105;&#20204;&#22312;&#32771;&#34385;&#30340;VKG&#22330;&#26223;&#20013;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#30446;&#24405;&#65292;&#35777;&#26126;&#23427;&#28085;&#30422;&#20102;&#20854;&#20013;&#32477;&#22823;&#37096;&#20998;&#30340;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Virtual Knowledge Graphs (VKG) constitute one of the most promising paradigms for integrating and accessing legacy data sources. A critical bottleneck in the integration process involves the definition, validation, and maintenance of mappings that link data sources to a domain ontology. To support the management of mappings throughout their entire lifecycle, we propose a comprehensive catalog of sophisticated mapping patterns that emerge when linking databases to ontologies. To do so, we build on well-established methodologies and patterns studied in data management, data analysis, and conceptual modeling. These are extended and refined through the analysis of concrete VKG benchmarks and real-world use cases, and considering the inherent impedance mismatch between data sources and ontologies. We validate our catalog on the considered VKG scenarios, showing that it covers the vast majority of patterns present therein.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20851;&#27880;&#20154;&#24037;&#26234;&#33021;&#26159;&#21542;&#23548;&#33268;&#29305;&#23450;&#20107;&#20214;&#20197;&#21450;&#35302;&#21457;AI&#34892;&#21160;&#30340;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#21462;&#35777;&#35843;&#26597;&#31574;&#30053;&#65292;&#24182;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#35780;&#20272;&#20102;&#35782;&#21035;&#24694;&#24847;AI&#30340;&#25361;&#25112;&#21644;&#24605;&#36335;&#12290;</title><link>http://arxiv.org/abs/2005.13635</link><description>&lt;p&gt;
&#26397;&#21521;&#20154;&#24037;&#26234;&#33021;&#21462;&#35777;&#65306;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#26159;&#21542;&#20570;&#21040;&#20102;&#65311;(arXiv:2005.13635v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
Towards AI Forensics: Did the Artificial Intelligence System Do It?. (arXiv:2005.13635v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2005.13635
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#20154;&#24037;&#26234;&#33021;&#26159;&#21542;&#23548;&#33268;&#29305;&#23450;&#20107;&#20214;&#20197;&#21450;&#35302;&#21457;AI&#34892;&#21160;&#30340;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#21462;&#35777;&#35843;&#26597;&#31574;&#30053;&#65292;&#24182;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#35780;&#20272;&#20102;&#35782;&#21035;&#24694;&#24847;AI&#30340;&#25361;&#25112;&#21644;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#20197;&#26085;&#30410;&#33258;&#20027;&#30340;&#26041;&#24335;&#20570;&#20986;&#24433;&#21709;&#25105;&#20204;&#26085;&#24120;&#29983;&#27963;&#30340;&#20915;&#31574;&#12290;&#23427;&#20204;&#30340;&#34892;&#20026;&#21487;&#33021;&#20250;&#23548;&#33268;&#20107;&#25925;&#12289;&#20260;&#23475;&#65292;&#25110;&#32773;&#26356;&#26222;&#36941;&#22320;&#36829;&#21453;&#35268;&#23450;&#12290;&#30830;&#23450;AI&#26159;&#21542;&#23548;&#33268;&#20102;&#29305;&#23450;&#20107;&#20214;&#65292;&#20197;&#21450;&#22914;&#26524;&#26159;&#36825;&#26679;&#65292;&#26159;&#20160;&#20040;&#35302;&#21457;&#20102;AI&#30340;&#34892;&#21160;&#65292;&#26159;&#20851;&#38190;&#30340;&#21462;&#35777;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#36825;&#20123;&#38382;&#39064;&#30340;&#27010;&#24565;&#21270;&#21644;&#21462;&#35777;&#35843;&#26597;&#31574;&#30053;&#12290;&#25105;&#20204;&#37325;&#28857;&#30740;&#31350;&#21487;&#33021;&#26159;&#8220;&#24694;&#24847;&#35774;&#35745;&#8221;&#30340;AI&#21644;&#28784;&#30418;&#20998;&#26512;&#12290;&#25105;&#20204;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#35780;&#20272;&#65292;&#25581;&#31034;&#20102;&#35782;&#21035;&#24694;&#24847;AI&#30340;&#25361;&#25112;&#21644;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI) makes decisions impacting our daily lives in an increasingly autonomous manner. Their actions might cause accidents, harm, or, more generally, violate regulations. Determining whether an AI caused a specific event and, if so, what triggered the AI's action, are key forensic questions. We provide a conceptualization of the problems and strategies for forensic investigation. We focus on AI that is potentially ``malicious by design'' and grey box analysis. Our evaluation using convolutional neural networks illustrates challenges and ideas for identifying malicious AI.
&lt;/p&gt;</description></item></channel></rss>