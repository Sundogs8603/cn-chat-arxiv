<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#20581;&#22766;&#30340;&#21327;&#35758;2SFGL&#65292;&#29992;&#20110;&#22522;&#20110;&#22270;&#30340;&#27450;&#35784;&#26816;&#27979;&#12290;&#35813;&#21327;&#35758;&#37319;&#29992;&#32852;&#37030;&#23398;&#20064;&#30340;&#26041;&#24335;&#65292;&#36890;&#36807;&#34394;&#25311;&#34701;&#21512;&#22810;&#26041;&#22270;&#24182;&#22312;&#34394;&#25311;&#22270;&#19978;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#21644;&#25512;&#29702;&#65292;&#26469;&#35299;&#20915;&#22240;&#37329;&#34701;&#29359;&#32618;&#36328;&#26426;&#26500;&#36896;&#25104;&#30340;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.08335</link><description>&lt;p&gt;
2SFGL&#65306;&#19968;&#31181;&#29992;&#20110;&#22522;&#20110;&#22270;&#30340;&#27450;&#35784;&#26816;&#27979;&#30340;&#31616;&#21333;&#32780;&#20581;&#22766;&#30340;&#21327;&#35758;
&lt;/p&gt;
&lt;p&gt;
2SFGL: A Simple And Robust Protocol For Graph-Based Fraud Detection. (arXiv:2310.08335v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08335
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#20581;&#22766;&#30340;&#21327;&#35758;2SFGL&#65292;&#29992;&#20110;&#22522;&#20110;&#22270;&#30340;&#27450;&#35784;&#26816;&#27979;&#12290;&#35813;&#21327;&#35758;&#37319;&#29992;&#32852;&#37030;&#23398;&#20064;&#30340;&#26041;&#24335;&#65292;&#36890;&#36807;&#34394;&#25311;&#34701;&#21512;&#22810;&#26041;&#22270;&#24182;&#22312;&#34394;&#25311;&#22270;&#19978;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#21644;&#25512;&#29702;&#65292;&#26469;&#35299;&#20915;&#22240;&#37329;&#34701;&#29359;&#32618;&#36328;&#26426;&#26500;&#36896;&#25104;&#30340;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22270;&#23398;&#20064;&#36827;&#34892;&#37329;&#34701;&#29359;&#32618;&#26816;&#27979;&#21487;&#20197;&#25552;&#39640;&#37329;&#34701;&#23433;&#20840;&#21644;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#29359;&#32618;&#20998;&#23376;&#21487;&#33021;&#22312;&#19981;&#21516;&#26426;&#26500;&#20043;&#38388;&#36827;&#34892;&#37329;&#34701;&#29359;&#32618;&#65292;&#20197;&#36991;&#20813;&#34987;&#21457;&#29616;&#65292;&#36825;&#22686;&#21152;&#20102;&#37329;&#34701;&#26426;&#26500;&#20351;&#29992;&#26412;&#22320;&#25968;&#25454;&#36827;&#34892;&#22270;&#23398;&#20064;&#30340;&#38590;&#24230;&#12290;&#30001;&#20110;&#22823;&#22810;&#25968;&#37329;&#34701;&#26426;&#26500;&#22312;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#26041;&#38754;&#21463;&#21040;&#20005;&#26684;&#30340;&#30417;&#31649;&#65292;&#35757;&#32451;&#25968;&#25454;&#36890;&#24120;&#26159;&#23396;&#31435;&#30340;&#65292;&#20256;&#32479;&#30340;&#23398;&#20064;&#25216;&#26415;&#26080;&#27861;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20801;&#35768;&#22810;&#20010;&#26426;&#26500;&#22312;&#19981;&#23558;&#25968;&#25454;&#38598;&#23637;&#31034;&#32473;&#20854;&#20182;&#26426;&#26500;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#27169;&#22411;&#65292;&#20174;&#32780;&#30830;&#20445;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32852;&#37030;&#22270;&#23398;&#20064;&#65288;2SFGL&#65289;&#26041;&#27861;&#65306;2SFGL&#30340;&#31532;&#19968;&#38454;&#27573;&#28041;&#21450;&#22810;&#26041;&#22270;&#30340;&#34394;&#25311;&#34701;&#21512;&#65292;&#31532;&#20108;&#38454;&#27573;&#28041;&#21450;&#22312;&#34394;&#25311;&#22270;&#19978;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#21644;&#25512;&#29702;&#12290;&#25105;&#20204;&#22312;&#20256;&#32479;&#27450;&#35784;&#26816;&#27979;&#20219;&#21153;FraudAmazonDataset&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Financial crime detection using graph learning improves financial safety and efficiency. However, criminals may commit financial crimes across different institutions to avoid detection, which increases the difficulty of detection for financial institutions which use local data for graph learning. As most financial institutions are subject to strict regulations in regards to data privacy protection, the training data is often isolated and conventional learning technology cannot handle the problem. Federated learning (FL) allows multiple institutions to train a model without revealing their datasets to each other, hence ensuring data privacy protection. In this paper, we proposes a novel two-stage approach to federated graph learning (2SFGL): The first stage of 2SFGL involves the virtual fusion of multiparty graphs, and the second involves model training and inference on the virtual graph. We evaluate our framework on a conventional fraud detection task based on the FraudAmazonDataset an
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#36890;&#26530;&#32445;&#24863;&#30693;&#30340;&#26102;&#31354;&#33258;&#36866;&#24212;&#22270;&#36716;&#25442;&#22120; (H-STFormer) &#29992;&#20110;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#19981;&#20165;&#26377;&#25928;&#24314;&#27169;&#20102;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#65292;&#36824;&#20805;&#20998;&#21033;&#29992;&#20102;&#20132;&#36890;&#27969;&#37327;&#25968;&#25454;&#30340;&#22266;&#26377;&#23646;&#24615;&#65292;&#35299;&#20915;&#20102;&#22686;&#37327;&#23398;&#20064;&#21644;&#36716;&#21270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.08328</link><description>&lt;p&gt;
&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#30340;&#20132;&#36890;&#26530;&#32445;&#24863;&#30693;&#26102;&#31354;&#33258;&#36866;&#24212;&#22270;&#36716;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Transport-Hub-Aware Spatial-Temporal Adaptive Graph Transformer for Traffic Flow Prediction. (arXiv:2310.08328v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08328
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#36890;&#26530;&#32445;&#24863;&#30693;&#30340;&#26102;&#31354;&#33258;&#36866;&#24212;&#22270;&#36716;&#25442;&#22120; (H-STFormer) &#29992;&#20110;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#19981;&#20165;&#26377;&#25928;&#24314;&#27169;&#20102;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#65292;&#36824;&#20805;&#20998;&#21033;&#29992;&#20102;&#20132;&#36890;&#27969;&#37327;&#25968;&#25454;&#30340;&#22266;&#26377;&#23646;&#24615;&#65292;&#35299;&#20915;&#20102;&#22686;&#37327;&#23398;&#20064;&#21644;&#36716;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#65288;ITS&#65289;&#30340;&#26680;&#24515;&#25216;&#26415;&#65292;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#20132;&#36890;&#27969;&#37327;&#25968;&#25454;&#26159;&#26102;&#31354;&#25968;&#25454;&#65292;&#19981;&#20165;&#19982;&#36947;&#36335;&#32593;&#32476;&#20013;&#30340;&#31354;&#38388;&#20301;&#32622;&#30456;&#20851;&#65292;&#32780;&#19988;&#38543;&#26102;&#38388;&#21464;&#21270;&#12290;&#29616;&#26377;&#26041;&#27861;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#35299;&#20915;&#20102;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#20013;&#30340;&#25361;&#25112;&#65292;&#37325;&#28857;&#26159;&#26377;&#25928;&#24314;&#27169;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#65292;&#20294;&#24182;&#26410;&#20805;&#20998;&#21033;&#29992;&#20132;&#36890;&#27969;&#37327;&#25968;&#25454;&#30340;&#25152;&#26377;&#22266;&#26377;&#23646;&#24615;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#26102;&#31354;&#25968;&#25454;&#25366;&#25496;&#30340;&#22686;&#37327;&#23398;&#20064;&#20960;&#20046;&#27809;&#26377;&#23581;&#35797;&#65292;&#20197;&#21069;&#30340;&#24037;&#20316;&#20063;&#24456;&#38590;&#36716;&#21270;&#21040;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#20219;&#21153;&#20013;&#12290;&#21463;&#21040;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#30340;&#22686;&#37327;&#23398;&#20064;&#26041;&#27861;&#25361;&#25112;&#21644;&#36947;&#36335;&#32593;&#32476;&#22266;&#26377;&#23646;&#24615;&#30340;&#28508;&#21147;&#26410;&#34987;&#20805;&#20998;&#21033;&#29992;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#30340;&#20132;&#36890;&#26530;&#32445;&#24863;&#30693;&#26102;&#31354;&#33258;&#36866;&#24212;&#22270;&#36716;&#25442;&#22120;&#65288;H-STFormer&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31354;&#38388;&#33258;&#27880;&#24847;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
As a core technology of Intelligent Transportation System (ITS), traffic flow prediction has a wide range of applications. Traffic flow data are spatial-temporal, which are not only correlated to spatial locations in road networks, but also vary with temporal time indices. Existing methods have solved the challenges in traffic flow prediction partly, focusing on modeling spatial-temporal dependencies effectively, while not all intrinsic properties of traffic flow data are utilized fully. Besides, there are very few attempts at incremental learning of spatial-temporal data mining, and few previous works can be easily transferred to the traffic flow prediction task. Motivated by the challenge of incremental learning methods for traffic flow prediction and the underutilization of intrinsic properties of road networks, we propose a Transport-Hub-aware Spatial-Temporal adaptive graph transFormer (H-STFormer) for traffic flow prediction. Specifically, we first design a novel spatial self-att
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#25439;&#22833;&#30340;&#23618;&#27425;&#21270;&#20998;&#31867;&#27169;&#22411;&#65292;&#29992;&#20110;&#23569;&#26679;&#26412;&#30446;&#26631;&#20998;&#31867;&#20219;&#21153;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#23558;&#26410;&#35265;&#36807;&#30340;&#29289;&#20307;&#20998;&#31867;&#21040;&#30456;&#23545;&#36890;&#29992;&#30340;&#31867;&#21035;&#20013;&#65292;&#36890;&#36807;&#20174;&#22270;&#20687;&#23884;&#20837;&#20013;&#25552;&#21462;&#30340;&#29305;&#24449;&#36827;&#34892;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2310.08304</link><description>&lt;p&gt;
CHIP&#65306;&#23545;&#27604;&#23618;&#27425;&#21270;&#22270;&#20687;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
CHIP: Contrastive Hierarchical Image Pretraining. (arXiv:2310.08304v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08304
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#25439;&#22833;&#30340;&#23618;&#27425;&#21270;&#20998;&#31867;&#27169;&#22411;&#65292;&#29992;&#20110;&#23569;&#26679;&#26412;&#30446;&#26631;&#20998;&#31867;&#20219;&#21153;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#23558;&#26410;&#35265;&#36807;&#30340;&#29289;&#20307;&#20998;&#31867;&#21040;&#30456;&#23545;&#36890;&#29992;&#30340;&#31867;&#21035;&#20013;&#65292;&#36890;&#36807;&#20174;&#22270;&#20687;&#23884;&#20837;&#20013;&#25552;&#21462;&#30340;&#29305;&#24449;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23569;&#26679;&#26412;&#30446;&#26631;&#20998;&#31867;&#26159;&#22312;&#26377;&#38480;&#25968;&#37327;&#30340;&#26679;&#26412;&#20316;&#20026;&#30417;&#30563;&#26102;&#23545;&#22270;&#20687;&#20013;&#30340;&#29289;&#20307;&#36827;&#34892;&#20998;&#31867;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#27425;/&#23569;&#27425;&#20998;&#31867;&#27169;&#22411;&#65292;&#21487;&#20197;&#23558;&#20219;&#20309;&#26410;&#35265;&#36807;&#30340;&#31867;&#21035;&#30340;&#29289;&#20307;&#20998;&#31867;&#21040;&#30456;&#23545;&#36890;&#29992;&#30340;&#31867;&#21035;&#20013;&#65292;&#22522;&#20110;&#23618;&#27425;&#21270;&#30340;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20351;&#29992;&#20102;&#22522;&#20110;&#19977;&#32423;&#23618;&#27425;&#23545;&#27604;&#25439;&#22833;&#30340;ResNet152&#20998;&#31867;&#22120;&#65292;&#26681;&#25454;&#20174;&#22270;&#20687;&#23884;&#20837;&#20013;&#25552;&#21462;&#30340;&#29305;&#24449;&#23545;&#29289;&#20307;&#36827;&#34892;&#20998;&#31867;&#65292;&#32780;&#36825;&#20123;&#29305;&#24449;&#22312;&#35757;&#32451;&#38454;&#27573;&#26410;&#34987;&#20351;&#29992;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#20351;&#29992;&#20102;ImageNet&#65288;ILSVRC-12&#65289;&#25968;&#25454;&#38598;&#30340;&#19968;&#20010;&#23376;&#38598;&#65292;&#35813;&#23376;&#38598;&#20165;&#21253;&#21547;&#21160;&#29289;&#31867;&#21035;&#65292;&#29992;&#20110;&#35757;&#32451;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#24182;&#21019;&#24314;&#20102;&#25105;&#20204;&#33258;&#24049;&#30340;&#26410;&#35265;&#31867;&#21035;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#25105;&#20204;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#23558;&#26410;&#30693;&#29289;&#20307;&#20998;&#31867;&#20026;&#19968;&#20010;&#36890;&#29992;&#31867;&#21035;&#26041;&#38754;&#25552;&#20379;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#65292;&#36825;&#22312;&#21518;&#25991;&#20013;&#26377;&#26356;&#35814;&#32454;&#30340;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few-shot object classification is the task of classifying objects in an image with limited number of examples as supervision. We propose a one-shot/few-shot classification model that can classify an object of any unseen class into a relatively general category in an hierarchically based classification. Our model uses a three-level hierarchical contrastive loss based ResNet152 classifier for classifying an object based on its features extracted from Image embedding, not used during the training phase. For our experimentation, we have used a subset of the ImageNet (ILSVRC-12) dataset that contains only the animal classes for training our model and created our own dataset of unseen classes for evaluating our trained model. Our model provides satisfactory results in classifying the unknown objects into a generic category which has been later discussed in greater detail.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#23558;&#36947;&#24503;&#24314;&#31435;&#22312;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#20013;&#30340;&#26041;&#27861;&#65292;&#24182;&#23601;&#36947;&#24503;&#30340;&#21547;&#20041;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#30475;&#27861;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#33258;&#39030;&#21521;&#19979;&#21644;&#33258;&#24213;&#21521;&#19978;&#30340;&#35774;&#35745;&#26041;&#27861;&#65292;&#24182;&#24378;&#35843;&#24773;&#24863;&#21644;&#24863;&#30693;&#22312;&#36947;&#24503;&#20013;&#30340;&#20316;&#29992;&#12290;&#27492;&#22806;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#35774;&#35745;&#30340;&#28151;&#21512;&#26041;&#27861;&#21644;&#32467;&#21512;&#36947;&#24503;&#33539;&#24335;&#30340;&#23618;&#27425;&#21270;&#26041;&#27861;&#12290;&#23545;&#20110;&#20154;&#24037;&#26234;&#33021;&#20262;&#29702;&#26469;&#35828;&#65292;&#27835;&#29702;&#21644;&#25919;&#31574;&#30340;&#37325;&#35201;&#24615;&#20063;&#24471;&#21040;&#20102;&#24378;&#35843;&#12290;</title><link>http://arxiv.org/abs/2310.08295</link><description>&lt;p&gt;
&#22914;&#26524;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#21521;&#20154;&#24037;&#26234;&#33021;&#20013;&#26500;&#24314;&#36947;&#24503;&#65292;&#25105;&#20204;&#35813;&#22914;&#20309;&#30528;&#25163;&#65311;
&lt;/p&gt;
&lt;p&gt;
If our aim is to build morality into an artificial agent, how might we begin to go about doing so?. (arXiv:2310.08295v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08295
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#23558;&#36947;&#24503;&#24314;&#31435;&#22312;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#20013;&#30340;&#26041;&#27861;&#65292;&#24182;&#23601;&#36947;&#24503;&#30340;&#21547;&#20041;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#30475;&#27861;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#33258;&#39030;&#21521;&#19979;&#21644;&#33258;&#24213;&#21521;&#19978;&#30340;&#35774;&#35745;&#26041;&#27861;&#65292;&#24182;&#24378;&#35843;&#24773;&#24863;&#21644;&#24863;&#30693;&#22312;&#36947;&#24503;&#20013;&#30340;&#20316;&#29992;&#12290;&#27492;&#22806;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#35774;&#35745;&#30340;&#28151;&#21512;&#26041;&#27861;&#21644;&#32467;&#21512;&#36947;&#24503;&#33539;&#24335;&#30340;&#23618;&#27425;&#21270;&#26041;&#27861;&#12290;&#23545;&#20110;&#20154;&#24037;&#26234;&#33021;&#20262;&#29702;&#26469;&#35828;&#65292;&#27835;&#29702;&#21644;&#25919;&#31574;&#30340;&#37325;&#35201;&#24615;&#20063;&#24471;&#21040;&#20102;&#24378;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#26222;&#21450;&#65292;&#20174;&#21307;&#30103;&#20445;&#20581;&#21040;&#33258;&#21160;&#39550;&#39542;&#65292;&#26500;&#24314;&#36947;&#24503;&#30340;&#26426;&#22120;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#36947;&#24503;&#30340;&#21547;&#20041;&#22312;&#20154;&#24037;&#26234;&#33021;&#30340;&#32972;&#26223;&#19979;&#20173;&#28982;&#23384;&#22312;&#20105;&#35758;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#26500;&#24314;&#36947;&#24503;&#26426;&#22120;&#26102;&#24212;&#32771;&#34385;&#30340;&#19981;&#21516;&#26041;&#38754;&#65292;&#21253;&#25324;&#26368;&#30456;&#20851;&#30340;&#36947;&#24503;&#33539;&#24335;&#21644;&#25361;&#25112;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#33258;&#39030;&#21521;&#19979;&#21644;&#33258;&#24213;&#21521;&#19978;&#30340;&#35774;&#35745;&#26041;&#27861;&#20197;&#21450;&#24773;&#24863;&#21644;&#24863;&#30693;&#22312;&#36947;&#24503;&#20013;&#30340;&#20316;&#29992;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#35774;&#35745;&#30340;&#28151;&#21512;&#26041;&#27861;&#21644;&#32467;&#21512;&#36947;&#24503;&#33539;&#24335;&#30340;&#23618;&#27425;&#21270;&#26041;&#27861;&#12290;&#25105;&#20204;&#24378;&#35843;&#27835;&#29702;&#21644;&#25919;&#31574;&#22312;&#20154;&#24037;&#26234;&#33021;&#20262;&#29702;&#21644;&#30830;&#20445;&#25105;&#20204;&#32473;&#36947;&#24503;&#26426;&#22120;&#35774;&#32622;&#30340;&#20219;&#21153;&#21487;&#34892;&#24615;&#12289;&#23454;&#29616;&#36947;&#24503;&#34892;&#20026;&#21644;&#33719;&#24471;&#33391;&#22909;&#20154;&#24037;&#26234;&#33021;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As Artificial Intelligence (AI) becomes pervasive in most fields, from healthcare to autonomous driving, it is essential that we find successful ways of building morality into our machines, especially for decision-making. However, the question of what it means to be moral is still debated, particularly in the context of AI. In this paper, we highlight the different aspects that should be considered when building moral agents, including the most relevant moral paradigms and challenges. We also discuss the top-down and bottom-up approaches to design and the role of emotion and sentience in morality. We then propose solutions including a hybrid approach to design and a hierarchical approach to combining moral paradigms. We emphasize how governance and policy are becoming ever more critical in AI Ethics and in ensuring that the tasks we set for moral agents are attainable, that ethical behavior is achieved, and that we obtain good AI.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#23545;&#25239;&#26679;&#26412;&#36827;&#34892;&#38647;&#36798;&#20449;&#21495;&#30340;&#38544;&#34109;&#30005;&#23376;&#23545;&#25239;&#25514;&#26045;&#65292;&#36890;&#36807;&#22522;&#20110;&#26102;&#39057;&#22270;&#20687;&#30340;&#25915;&#20987;&#27969;&#31243;&#21644;&#20855;&#26377;&#39640;&#21487;&#36716;&#31227;&#24615;&#30340;&#25915;&#20987;&#31639;&#27861;&#65292;&#20197;&#21450;&#22522;&#20110;STFT&#30340;&#26102;&#22495;&#20449;&#21495;&#25915;&#20987;&#31639;&#27861;&#65292;&#25104;&#21151;&#23454;&#26045;&#20102;&#24178;&#25200;&#20449;&#21495;&#30340;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2310.08292</link><description>&lt;p&gt;
&#20351;&#29992;&#23545;&#25239;&#26679;&#26412;&#36827;&#34892;&#38647;&#36798;&#20449;&#21495;&#30340;&#38544;&#34109;&#30005;&#23376;&#23545;&#25239;&#25514;&#26045;
&lt;/p&gt;
&lt;p&gt;
Concealed Electronic Countermeasures of Radar Signal with Adversarial Examples. (arXiv:2310.08292v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08292
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#23545;&#25239;&#26679;&#26412;&#36827;&#34892;&#38647;&#36798;&#20449;&#21495;&#30340;&#38544;&#34109;&#30005;&#23376;&#23545;&#25239;&#25514;&#26045;&#65292;&#36890;&#36807;&#22522;&#20110;&#26102;&#39057;&#22270;&#20687;&#30340;&#25915;&#20987;&#27969;&#31243;&#21644;&#20855;&#26377;&#39640;&#21487;&#36716;&#31227;&#24615;&#30340;&#25915;&#20987;&#31639;&#27861;&#65292;&#20197;&#21450;&#22522;&#20110;STFT&#30340;&#26102;&#22495;&#20449;&#21495;&#25915;&#20987;&#31639;&#27861;&#65292;&#25104;&#21151;&#23454;&#26045;&#20102;&#24178;&#25200;&#20449;&#21495;&#30340;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#23545;&#25239;&#25216;&#26415;&#26159;&#29616;&#20195;&#25112;&#20105;&#20013;&#30340;&#37325;&#35201;&#26041;&#38754;&#65292;&#20256;&#32479;&#30340;&#30005;&#23376;&#23545;&#25239;&#25216;&#26415;&#24448;&#24448;&#36890;&#36807;&#21152;&#20837;&#22823;&#35268;&#27169;&#30340;&#24178;&#25200;&#20449;&#21495;&#26469;&#36798;&#21040;&#24178;&#25200;&#25928;&#26524;&#65292;&#20294;&#36825;&#26679;&#24448;&#24448;&#22826;&#36807;&#26174;&#30524;&#12290;&#36817;&#24180;&#26469;&#65292;&#20986;&#29616;&#20102;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#30446;&#21069;&#25915;&#20987;&#22330;&#26223;&#20165;&#38480;&#20110;&#26102;&#38388;&#22495;&#38647;&#36798;&#20449;&#21495;&#20998;&#31867;&#12290;&#26412;&#25991;&#38024;&#23545;&#38647;&#36798;&#20449;&#21495;&#30340;&#26102;&#39057;&#22270;&#20687;&#20998;&#31867;&#22330;&#26223;&#65292;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26102;&#39057;&#22270;&#20687;&#30340;&#25915;&#20987;&#27969;&#31243;&#21644;&#20855;&#26377;&#39640;&#21487;&#36716;&#31227;&#24615;&#30340;DITIMI-FGSM&#25915;&#20987;&#31639;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;STFT&#30340;&#26102;&#22495;&#20449;&#21495;&#25915;&#20987;(STDS)&#31639;&#27861;&#26469;&#35299;&#20915;&#26102;&#39057;&#20998;&#26512;&#20013;&#30340;&#19981;&#21487;&#36870;&#24615;&#38382;&#39064;&#65292;&#20174;&#32780;&#24471;&#21040;&#24178;&#25200;&#20449;&#21495;&#30340;&#26102;&#22495;&#34920;&#31034;&#12290;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#25915;&#20987;&#27969;&#31243;&#26159;&#21487;&#34892;&#30340;&#65292;&#25152;&#25552;&#20986;&#30340;&#25915;&#20987;&#26041;&#27861;&#21487;&#20197;&#25104;&#21151;&#23454;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electronic countermeasures involving radar signals are an important aspect of modern warfare. Traditional electronic countermeasures techniques typically add large-scale interference signals to ensure interference effects, which can lead to attacks being too obvious. In recent years, AI-based attack methods have emerged that can effectively solve this problem, but the attack scenarios are currently limited to time domain radar signal classification. In this paper, we focus on the time-frequency images classification scenario of radar signals. We first propose an attack pipeline under the time-frequency images scenario and DITIMI-FGSM attack algorithm with high transferability. Then, we propose STFT-based time domain signal attack(STDS) algorithm to solve the problem of non-invertibility in time-frequency analysis, thus obtaining the time-domain representation of the interference signal. A large number of experiments show that our attack pipeline is feasible and the proposed attack meth
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25193;&#23637;BERT&#35789;&#27719;&#34920;&#30340;&#26041;&#27861;&#65292;&#20197;&#29992;&#20110;&#30693;&#35782;&#24211;&#26500;&#24314;&#20219;&#21153;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#29305;&#23450;&#20219;&#21153;&#30340;&#20877;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#28155;&#21152;&#26032;&#21333;&#35789;&#26469;&#25193;&#23637;&#35821;&#35328;&#27169;&#22411;&#30340;&#35789;&#27719;&#34920;&#65292;&#24182;&#20445;&#30041;&#20102;&#26032;&#28155;&#21152;&#21333;&#35789;&#30340;&#35821;&#20041;&#23884;&#20837;&#12290;</title><link>http://arxiv.org/abs/2310.08291</link><description>&lt;p&gt;
&#25193;&#23637;BERT&#35789;&#27719;&#34920;&#20197;&#29992;&#20110;&#30693;&#35782;&#24211;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
Expanding the Vocabulary of BERT for Knowledge Base Construction. (arXiv:2310.08291v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08291
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25193;&#23637;BERT&#35789;&#27719;&#34920;&#30340;&#26041;&#27861;&#65292;&#20197;&#29992;&#20110;&#30693;&#35782;&#24211;&#26500;&#24314;&#20219;&#21153;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#29305;&#23450;&#20219;&#21153;&#30340;&#20877;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#28155;&#21152;&#26032;&#21333;&#35789;&#26469;&#25193;&#23637;&#35821;&#35328;&#27169;&#22411;&#30340;&#35789;&#27719;&#34920;&#65292;&#24182;&#20445;&#30041;&#20102;&#26032;&#28155;&#21152;&#21333;&#35789;&#30340;&#35821;&#20041;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#24211;&#26500;&#24314;&#28041;&#21450;&#33719;&#21462;&#32467;&#26500;&#21270;&#20449;&#24687;&#20197;&#21019;&#24314;&#19968;&#20010;&#21253;&#21547;&#20107;&#23454;&#21644;&#20851;&#31995;&#25968;&#25454;&#30340;&#30693;&#35782;&#24211;&#65292;&#20197;&#20415;&#20110;&#38382;&#31572;&#12289;&#20449;&#24687;&#26816;&#32034;&#21644;&#35821;&#20041;&#29702;&#35299;&#12290;&#22269;&#38469;&#35821;&#20041;&#32593;&#20250;&#35758;2023&#24180;&#30340;&#21517;&#20026;&#8220;&#26469;&#33258;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#24211;&#26500;&#24314;&#8221;&#25361;&#25112;&#23450;&#20041;&#20102;&#19987;&#27880;&#20110;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#30693;&#35782;&#24211;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#20851;&#27880;&#25361;&#25112;&#30340;&#31532;&#19968;&#36712;&#36947;&#65292;&#20854;&#20013;&#21442;&#25968;&#21463;&#38480;&#20110;10&#20159;&#65292;&#24182;&#19988;&#31105;&#27490;&#22312;&#25552;&#31034;&#20013;&#21253;&#21547;&#23454;&#20307;&#25551;&#36848;&#12290;&#23613;&#31649;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#20102;&#36275;&#22815;&#30340;&#28789;&#27963;&#24615;&#26469;&#25193;&#23637;&#20854;&#35789;&#27719;&#34920;&#65292;&#20294;&#23427;&#24182;&#38750;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#22810;&#20196;&#29260;&#39044;&#27979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#30693;&#35782;&#24211;&#26500;&#24314;&#30340;&#21487;&#25193;&#23637;&#35789;&#27719;BERT&#65292;&#23427;&#22312;&#25193;&#23637;&#35821;&#35328;&#27169;&#22411;&#30340;&#35789;&#27719;&#34920;&#30340;&#21516;&#26102;&#65292;&#20445;&#30041;&#20102;&#26032;&#28155;&#21152;&#21333;&#35789;&#30340;&#35821;&#20041;&#23884;&#20837;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#29305;&#23450;&#20219;&#21153;&#30340;&#20877;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#38024;&#23545;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge base construction entails acquiring structured information to create a knowledge base of factual and relational data, facilitating question answering, information retrieval, and semantic understanding. The challenge called "Knowledge Base Construction from Pretrained Language Models" at International Semantic Web Conference 2023 defines tasks focused on constructing knowledge base using language model. Our focus was on Track 1 of the challenge, where the parameters are constrained to a maximum of 1 billion, and the inclusion of entity descriptions within the prompt is prohibited.  Although the masked language model offers sufficient flexibility to extend its vocabulary, it is not inherently designed for multi-token prediction. To address this, we present Vocabulary Expandable BERT for knowledge base construction, which expand the language model's vocabulary while preserving semantic embeddings for newly added words. We adopt task-specific re-pre-training on masked language mo
&lt;/p&gt;</description></item><item><title>CP-KGC&#26041;&#27861;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#32422;&#26463;&#24335;&#25552;&#31034;&#26469;&#34917;&#20840;&#30693;&#35782;&#22270;&#35889;&#65292;&#25552;&#39640;&#25512;&#26029;&#25928;&#26524;&#65292;&#23637;&#31034;&#20102;&#22312;&#20302;&#36164;&#28304;&#35745;&#31639;&#26465;&#20214;&#19979;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#20043;&#21069;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.08279</link><description>&lt;p&gt;
CP-KGC: &#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32422;&#26463;&#24335;&#25552;&#31034;&#23545;&#30693;&#35782;&#22270;&#35889;&#36827;&#34892;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
CP-KGC: Constrained-Prompt Knowledge Graph Completion with Large Language Models. (arXiv:2310.08279v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08279
&lt;/p&gt;
&lt;p&gt;
CP-KGC&#26041;&#27861;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#32422;&#26463;&#24335;&#25552;&#31034;&#26469;&#34917;&#20840;&#30693;&#35782;&#22270;&#35889;&#65292;&#25552;&#39640;&#25512;&#26029;&#25928;&#26524;&#65292;&#23637;&#31034;&#20102;&#22312;&#20302;&#36164;&#28304;&#35745;&#31639;&#26465;&#20214;&#19979;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#20043;&#21069;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#26088;&#22312;&#21033;&#29992;&#29616;&#26377;&#30693;&#35782;&#25512;&#26029;&#21644;&#25512;&#27979;&#30693;&#35782;&#22270;&#35889;&#20013;&#32570;&#22833;&#30340;&#36830;&#25509;&#12290;SimKGC&#31561;&#22522;&#20110;&#25991;&#26412;&#30340;&#26041;&#27861;&#24050;&#32463;&#36229;&#36807;&#20102;&#22270;&#23884;&#20837;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#24402;&#32435;&#24335;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#25991;&#26412;&#30340;&#26041;&#27861;&#30340;&#25928;&#26524;&#21462;&#20915;&#20110;&#23454;&#20307;&#25991;&#26412;&#25551;&#36848;&#30340;&#36136;&#37327;&#12290;&#20026;&#20102;&#20943;&#36731;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#30340;&#24187;&#35273;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#32422;&#26463;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#21033;&#29992;&#23454;&#20307;&#21450;&#20854;&#25991;&#26412;&#25551;&#36848;&#20316;&#20026;&#19978;&#19979;&#25991;&#32422;&#26463;&#26469;&#25552;&#39640;&#25968;&#25454;&#36136;&#37327;&#12290;&#25105;&#20204;&#30340;&#32422;&#26463;&#24335;&#25552;&#31034;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#26041;&#27861;&#65288;CP-KGC&#65289;&#22312;&#20302;&#36164;&#28304;&#35745;&#31639;&#26465;&#20214;&#19979;&#34920;&#29616;&#20986;&#26377;&#25928;&#30340;&#25512;&#26029;&#33021;&#21147;&#65292;&#24182;&#36229;&#36807;&#20102;WN18RR&#21644;FB15K237&#25968;&#25454;&#38598;&#19978;&#30340;&#20043;&#21069;&#32467;&#26524;&#12290;&#36825;&#23637;&#31034;&#20102;LLMs&#22312;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20219;&#21153;&#20013;&#30340;&#25972;&#21512;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graph completion (KGC) aims to utilize existing knowledge to deduce and infer missing connections within knowledge graphs. Text-based approaches, like SimKGC, have outperformed graph embedding methods, showcasing the promise of inductive KGC. However, the efficacy of text-based methods hinges on the quality of entity textual descriptions. In this paper, we identify the key issue of whether large language models (LLMs) can generate effective text. To mitigate hallucination in LLM-generated text in this paper, we introduce a constraint-based prompt that utilizes the entity and its textual description as contextual constraints to enhance data quality. Our Constrained-Prompt Knowledge Graph Completion (CP-KGC) method demonstrates effective inference under low resource computing conditions and surpasses prior results on the WN18RR and FB15K237 datasets. This showcases the integration of LLMs in KGC tasks and provides new directions for future research.
&lt;/p&gt;</description></item><item><title>Lag-Llama&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#35757;&#32451;&#30340;&#36890;&#29992;&#39044;&#27979;&#27169;&#22411;&#65292;&#22312;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#38646;&#26679;&#26412;&#39044;&#27979;&#33021;&#21147;&#65292;&#24182;&#20351;&#29992;&#20809;&#28369;&#26029;&#35010;&#24130;&#24459;&#27169;&#22411;&#26469;&#25311;&#21512;&#21644;&#39044;&#27979;&#25193;&#23637;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2310.08278</link><description>&lt;p&gt;
Lag-Llama: &#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Lag-Llama: Towards Foundation Models for Time Series Forecasting. (arXiv:2310.08278v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08278
&lt;/p&gt;
&lt;p&gt;
Lag-Llama&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#35757;&#32451;&#30340;&#36890;&#29992;&#39044;&#27979;&#27169;&#22411;&#65292;&#22312;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#38646;&#26679;&#26412;&#39044;&#27979;&#33021;&#21147;&#65292;&#24182;&#20351;&#29992;&#20809;&#28369;&#26029;&#35010;&#24130;&#24459;&#27169;&#22411;&#26469;&#25311;&#21512;&#21644;&#39044;&#27979;&#25193;&#23637;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#26500;&#24314;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#22522;&#30784;&#27169;&#22411;&#24182;&#30740;&#31350;&#20854;&#25193;&#23637;&#34892;&#20026;&#65292;&#25105;&#20204;&#22312;&#36825;&#37324;&#20171;&#32461;&#20102;&#25105;&#20204;&#27491;&#22312;&#36827;&#34892;&#20013;&#30340; Lag-Llama &#24037;&#20316;&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#22823;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#36890;&#29992;&#21333;&#21464;&#37327;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#22312;&#26410;&#35265;&#36807;&#30340;&#8220;&#20998;&#24067;&#22806;&#8221;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#33391;&#22909;&#30340;&#38646;&#26679;&#26412;&#39044;&#27979;&#33021;&#21147;&#65292;&#20248;&#20110;&#26377;&#30417;&#30563;&#22522;&#32447;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#20809;&#28369;&#26029;&#35010;&#24130;&#24459;&#26469;&#25311;&#21512;&#21644;&#39044;&#27979;&#27169;&#22411;&#30340;&#25193;&#23637;&#34892;&#20026;&#12290;&#24320;&#28304;&#20195;&#30721;&#21487;&#22312; https://github.com/kashif/pytorch-transformer-ts &#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aiming to build foundation models for time-series forecasting and study their scaling behavior, we present here our work-in-progress on Lag-Llama, a general-purpose univariate probabilistic time-series forecasting model trained on a large collection of time-series data. The model shows good zero-shot prediction capabilities on unseen "out-of-distribution" time-series datasets, outperforming supervised baselines. We use smoothly broken power-laws to fit and predict model scaling behavior. The open source code is made available at https://github.com/kashif/pytorch-transformer-ts.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#26041;&#21521;&#30340;&#35270;&#35273;-&#35821;&#20041;&#23884;&#20837;&#27169;&#22411;&#65288;DOVE&#65289;&#65292;&#36890;&#36807;&#21306;&#22495;&#23548;&#21521;&#30340;&#27880;&#24847;&#21147;&#27169;&#22359;&#21644;&#36731;&#37327;&#32423;&#30340;&#25991;&#23383;&#22522;&#22240;&#36741;&#21161;&#27169;&#22359;&#65292;&#35299;&#20915;&#20102;&#36965;&#24863;&#22270;&#20687;-&#25991;&#26412;&#26816;&#32034;&#20013;&#30340;&#35270;&#35273;-&#35821;&#20041;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#26816;&#32034;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.08276</link><description>&lt;p&gt;
&#38754;&#21521;&#26041;&#21521;&#30340;&#35270;&#35273;-&#35821;&#20041;&#23884;&#20837;&#27169;&#22411;&#22312;&#36965;&#24863;&#22270;&#20687;-&#25991;&#26412;&#26816;&#32034;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Direction-Oriented Visual-semantic Embedding Model for Remote Sensing Image-text Retrieval. (arXiv:2310.08276v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08276
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#26041;&#21521;&#30340;&#35270;&#35273;-&#35821;&#20041;&#23884;&#20837;&#27169;&#22411;&#65288;DOVE&#65289;&#65292;&#36890;&#36807;&#21306;&#22495;&#23548;&#21521;&#30340;&#27880;&#24847;&#21147;&#27169;&#22359;&#21644;&#36731;&#37327;&#32423;&#30340;&#25991;&#23383;&#22522;&#22240;&#36741;&#21161;&#27169;&#22359;&#65292;&#35299;&#20915;&#20102;&#36965;&#24863;&#22270;&#20687;-&#25991;&#26412;&#26816;&#32034;&#20013;&#30340;&#35270;&#35273;-&#35821;&#20041;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#26816;&#32034;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;-&#25991;&#26412;&#26816;&#32034;&#22312;&#36817;&#24180;&#26469;&#24471;&#21040;&#20102;&#24555;&#36895;&#21457;&#23637;&#65292;&#28982;&#32780;&#22312;&#36965;&#24863;&#39046;&#22495;&#20173;&#28982;&#23384;&#22312;&#30528;&#35270;&#35273;-&#35821;&#20041;&#19981;&#24179;&#34913;&#30340;&#25361;&#25112;&#65292;&#36825;&#23548;&#33268;&#20102;&#38750;&#35821;&#20041;&#35270;&#35273;&#21644;&#25991;&#26412;&#29305;&#24449;&#30340;&#38169;&#35823;&#21305;&#37197;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38754;&#21521;&#26041;&#21521;&#30340;&#35270;&#35273;-&#35821;&#20041;&#23884;&#20837;&#27169;&#22411;&#65288;DOVE&#65289;&#65292;&#26469;&#25366;&#25496;&#35270;&#35273;&#21644;&#35821;&#35328;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#21306;&#22495;&#23548;&#21521;&#30340;&#27880;&#24847;&#21147;&#27169;&#22359;&#65288;ROAM&#65289;&#65292;&#22312;&#28508;&#22312;&#30340;&#35821;&#20041;&#31354;&#38388;&#20013;&#65292;&#26681;&#25454;&#21306;&#22495;&#35270;&#35273;&#29305;&#24449;&#33258;&#36866;&#24212;&#22320;&#35843;&#25972;&#26368;&#32456;&#30340;&#35270;&#35273;&#21644;&#25991;&#26412;&#23884;&#20837;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#21516;&#26102;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#25991;&#23383;&#22522;&#22240;&#36741;&#21161;&#27169;&#22359;&#65288;DTGA&#65289;&#65292;&#29992;&#36739;&#23569;&#30340;&#27880;&#24847;&#21147;&#25805;&#20316;&#26469;&#25193;&#23637;&#21487;&#22788;&#29702;&#30340;&#25991;&#26412;&#34920;&#31034;&#33539;&#22260;&#65292;&#22686;&#24378;&#20840;&#23616;&#35789;&#32423;&#35821;&#20041;&#36830;&#25509;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#20840;&#23616;&#35270;&#35273;-&#35821;&#20041;&#32422;&#26463;&#26469;&#20943;&#23569;&#21333;&#19968;&#35270;&#35273;&#20381;&#36182;&#65292;&#24182;&#20026;&#26368;&#32456;&#30340;&#35270;&#35273;&#21644;&#25991;&#26412;&#34920;&#31034;&#25552;&#20379;&#22806;&#37096;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image-text retrieval has developed rapidly in recent years. However, it is still a challenge in remote sensing due to visual-semantic imbalance, which leads to incorrect matching of non-semantic visual and textual features. To solve this problem, we propose a novel Direction-Oriented Visual-semantic Embedding Model (DOVE) to mine the relationship between vision and language. Concretely, a Regional-Oriented Attention Module (ROAM) adaptively adjusts the distance between the final visual and textual embeddings in the latent semantic space, oriented by regional visual features. Meanwhile, a lightweight Digging Text Genome Assistant (DTGA) is designed to expand the range of tractable textual representation and enhance global word-level semantic connections using less attention operations. Ultimately, we exploit a global visual-semantic constraint to reduce single visual dependency and serve as an external constraint for the final visual and textual representations. The effectiveness and su
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22238;&#31572;&#38382;&#39064;&#26102;&#24120;&#24120;&#20986;&#29616;&#20107;&#23454;&#38169;&#35823;&#65292;&#36825;&#20027;&#35201;&#26159;&#22240;&#20026;&#36807;&#24230;&#20381;&#36182;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#30340;&#20849;&#29616;&#32479;&#35745;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#20849;&#29616;&#20559;&#35265;&#30340;&#24433;&#21709;&#65292;&#23548;&#33268;&#38590;&#20197;&#22238;&#24518;&#36215;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#24456;&#23569;&#20849;&#29616;&#30340;&#20107;&#23454;&#12290;&#24314;&#35758;&#20351;&#29992;&#21435;&#20559;&#25968;&#25454;&#38598;&#36827;&#34892;&#24494;&#35843;&#26469;&#20943;&#36731;&#20559;&#35265;&#65292;&#20294;&#36825;&#23545;&#20110;&#24494;&#35843;&#26399;&#38388;&#26410;&#35265;&#36807;&#30340;&#31232;&#26377;&#20107;&#23454;&#30340;&#22238;&#24518;&#25928;&#26524;&#24182;&#19981;&#26126;&#26174;&#12290;</title><link>http://arxiv.org/abs/2310.08256</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20849;&#29616;&#23545;&#20107;&#23454;&#30693;&#35782;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Impact of Co-occurrence on Factual Knowledge of Large Language Models. (arXiv:2310.08256v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08256
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22238;&#31572;&#38382;&#39064;&#26102;&#24120;&#24120;&#20986;&#29616;&#20107;&#23454;&#38169;&#35823;&#65292;&#36825;&#20027;&#35201;&#26159;&#22240;&#20026;&#36807;&#24230;&#20381;&#36182;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#30340;&#20849;&#29616;&#32479;&#35745;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#20849;&#29616;&#20559;&#35265;&#30340;&#24433;&#21709;&#65292;&#23548;&#33268;&#38590;&#20197;&#22238;&#24518;&#36215;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#24456;&#23569;&#20849;&#29616;&#30340;&#20107;&#23454;&#12290;&#24314;&#35758;&#20351;&#29992;&#21435;&#20559;&#25968;&#25454;&#38598;&#36827;&#34892;&#24494;&#35843;&#26469;&#20943;&#36731;&#20559;&#35265;&#65292;&#20294;&#36825;&#23545;&#20110;&#24494;&#35843;&#26399;&#38388;&#26410;&#35265;&#36807;&#30340;&#31232;&#26377;&#20107;&#23454;&#30340;&#22238;&#24518;&#25928;&#26524;&#24182;&#19981;&#26126;&#26174;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#22312;&#20107;&#23454;&#19978;&#20570;&#20986;&#38169;&#35823;&#30340;&#22238;&#31572;&#12290;&#26412;&#25991;&#20551;&#35774;&#36807;&#24230;&#20381;&#36182;&#20110;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#30340;&#31616;&#21333;&#20849;&#29616;&#32479;&#35745;&#26159;&#23548;&#33268;&#20107;&#23454;&#38169;&#35823;&#30340;&#20027;&#35201;&#22240;&#32032;&#20043;&#19968;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#20849;&#29616;&#20559;&#35265;&#30340;&#24433;&#21709;&#65292;&#21363;&#26356;&#20542;&#21521;&#20110;&#36873;&#25321;&#39057;&#32321;&#20849;&#29616;&#30340;&#35789;&#32780;&#19981;&#26159;&#27491;&#30830;&#31572;&#26696;&#12290;&#22240;&#27492;&#65292;&#23613;&#31649;&#22312;&#24494;&#35843;&#26399;&#38388;&#24050;&#32463;&#35265;&#36807;&#36825;&#20123;&#20107;&#23454;&#30340;&#20027;&#39064;&#21644;&#23545;&#35937;&#22312;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#24456;&#23569;&#20849;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20173;&#28982;&#38590;&#20197;&#22238;&#24518;&#36215;&#36825;&#20123;&#20107;&#23454;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21363;&#20351;&#25193;&#22823;&#27169;&#22411;&#35268;&#27169;&#25110;&#36827;&#34892;&#24494;&#35843;&#65292;&#20849;&#29616;&#20559;&#35265;&#20173;&#28982;&#23384;&#22312;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24314;&#35758;&#22312;&#21435;&#20559;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#36890;&#36807;&#36807;&#28388;&#25481;&#20027;&#39064;-&#23545;&#35937;&#20849;&#29616;&#35745;&#25968;&#39640;&#30340;&#20559;&#35265;&#26679;&#26412;&#26469;&#20943;&#36731;&#20559;&#35265;&#12290;&#23613;&#31649;&#21435;&#20559;&#24494;&#35843;&#20801;&#35768;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35760;&#24518;&#35757;&#32451;&#38598;&#20013;&#30340;&#31232;&#26377;&#20107;&#23454;&#65292;&#20294;&#22312;&#24494;&#35843;&#26399;&#38388;&#26410;&#35265;&#36807;&#30340;&#31232;&#26377;&#20107;&#23454;&#30340;&#22238;&#24518;&#25928;&#26524;&#24182;&#19981;&#26126;&#26174;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) often make factually incorrect responses despite their success in various applications. In this paper, we hypothesize that relying heavily on simple co-occurrence statistics of the pre-training corpora is one of the main factors that cause factual errors. Our results reveal that LLMs are vulnerable to the co-occurrence bias, defined as preferring frequently co-occurred words over the correct answer. Consequently, LLMs struggle to recall facts whose subject and object rarely co-occur in the pre-training dataset although they are seen during finetuning. We show that co-occurrence bias remains despite scaling up model sizes or finetuning. Therefore, we suggest finetuning on a debiased dataset to mitigate the bias by filtering out biased samples whose subject-object co-occurrence count is high. Although debiased finetuning allows LLMs to memorize rare facts in the training set, it is not effective in recalling rare facts unseen during finetuning. Further resear
&lt;/p&gt;</description></item><item><title>MetaBox&#26159;&#19968;&#31181;&#29992;&#20110;&#24320;&#21457;&#21644;&#35780;&#20272;&#20803;&#40657;&#31665;&#20248;&#21270;&#19982;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#22522;&#20934;&#24179;&#21488;&#65292;&#25552;&#20379;&#28789;&#27963;&#30340;&#31639;&#27861;&#27169;&#26495;&#12289;&#24191;&#27867;&#30340;&#38382;&#39064;&#23454;&#20363;&#21644;&#22522;&#32447;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19977;&#20010;&#26631;&#20934;&#21270;&#30340;&#24615;&#33021;&#25351;&#26631;&#65292;&#20197;&#20419;&#36827;&#26041;&#27861;&#30340;&#20005;&#26684;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2310.08252</link><description>&lt;p&gt;
MetaBox&#65306;&#19968;&#31181;&#29992;&#20110;&#20803;&#40657;&#31665;&#20248;&#21270;&#19982;&#24378;&#21270;&#23398;&#20064;&#30340;&#22522;&#20934;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
MetaBox: A Benchmark Platform for Meta-Black-Box Optimization with Reinforcement Learning. (arXiv:2310.08252v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08252
&lt;/p&gt;
&lt;p&gt;
MetaBox&#26159;&#19968;&#31181;&#29992;&#20110;&#24320;&#21457;&#21644;&#35780;&#20272;&#20803;&#40657;&#31665;&#20248;&#21270;&#19982;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#22522;&#20934;&#24179;&#21488;&#65292;&#25552;&#20379;&#28789;&#27963;&#30340;&#31639;&#27861;&#27169;&#26495;&#12289;&#24191;&#27867;&#30340;&#38382;&#39064;&#23454;&#20363;&#21644;&#22522;&#32447;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19977;&#20010;&#26631;&#20934;&#21270;&#30340;&#24615;&#33021;&#25351;&#26631;&#65292;&#20197;&#20419;&#36827;&#26041;&#27861;&#30340;&#20005;&#26684;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20803;&#40657;&#31665;&#20248;&#21270;&#19982;&#24378;&#21270;&#23398;&#20064;&#65288;MetaBBO-RL&#65289;&#23637;&#31034;&#20102;&#22312;&#20803;&#32423;&#21035;&#19978;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#26469;&#20943;&#23569;&#23545;&#20302;&#32423;&#40657;&#31665;&#20248;&#21270;&#22120;&#30340;&#25163;&#21160;&#24494;&#35843;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#39046;&#22495;&#30001;&#20110;&#32570;&#20047;&#32479;&#19968;&#30340;&#22522;&#20934;&#32780;&#21463;&#21040;&#38459;&#30861;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MetaBox&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#24320;&#21457;&#21644;&#35780;&#20272;MetaBBO-RL&#26041;&#27861;&#32780;&#35774;&#35745;&#30340;&#31532;&#19968;&#20010;&#22522;&#20934;&#24179;&#21488;&#12290;MetaBox&#25552;&#20379;&#20102;&#19968;&#20010;&#28789;&#27963;&#30340;&#31639;&#27861;&#27169;&#26495;&#65292;&#35753;&#29992;&#25143;&#21487;&#20197;&#36731;&#26494;&#22320;&#22312;&#24179;&#21488;&#20869;&#23454;&#29616;&#33258;&#24049;&#30340;&#29420;&#29305;&#35774;&#35745;&#12290;&#27492;&#22806;&#65292;&#23427;&#25552;&#20379;&#20102;&#36229;&#36807;300&#20010;&#38382;&#39064;&#23454;&#20363;&#65292;&#20174;&#21512;&#25104;&#21040;&#30495;&#23454;&#22330;&#26223;&#30340;&#24191;&#27867;&#33539;&#22260;&#65292;&#24182;&#19988;&#21253;&#21547;&#20102;19&#31181;&#22522;&#32447;&#26041;&#27861;&#30340;&#35814;&#23613;&#24211;&#65292;&#21253;&#25324;&#20256;&#32479;&#40657;&#31665;&#20248;&#21270;&#22120;&#21644;&#26368;&#36817;&#30340;MetaBBO-RL&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;MetaBox&#24341;&#20837;&#20102;&#19977;&#20010;&#26631;&#20934;&#21270;&#30340;&#24615;&#33021;&#25351;&#26631;&#65292;&#20351;&#26041;&#27861;&#30340;&#35780;&#20272;&#26356;&#21152;&#20840;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Meta-Black-Box Optimization with Reinforcement Learning (MetaBBO-RL) has showcased the power of leveraging RL at the meta-level to mitigate manual fine-tuning of low-level black-box optimizers. However, this field is hindered by the lack of a unified benchmark. To fill this gap, we introduce MetaBox, the first benchmark platform expressly tailored for developing and evaluating MetaBBO-RL methods. MetaBox offers a flexible algorithmic template that allows users to effortlessly implement their unique designs within the platform. Moreover, it provides a broad spectrum of over 300 problem instances, collected from synthetic to realistic scenarios, and an extensive library of 19 baseline methods, including both traditional black-box optimizers and recent MetaBBO-RL methods. Besides, MetaBox introduces three standardized performance metrics, enabling a more thorough assessment of the methods. In a bid to illustrate the utility of MetaBox for facilitating rigorous evaluation and in-
&lt;/p&gt;</description></item><item><title>GROOT&#26159;&#19968;&#20010;&#36890;&#36807;&#35266;&#30475;&#28216;&#25103;&#35270;&#39057;&#23398;&#20064;&#36981;&#24490;&#25351;&#20196;&#30340;&#25511;&#21046;&#22120;&#65292;&#23427;&#36890;&#36807;&#20135;&#29983;&#19968;&#20010;&#32467;&#26500;&#21270;&#30340;&#30446;&#26631;&#31354;&#38388;&#26469;&#28040;&#38500;&#26114;&#36149;&#30340;&#25991;&#26412;-&#28216;&#25103;&#27880;&#37322;&#30340;&#38656;&#27714;&#65292;&#24182;&#22312;Minecraft SkillForge&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#29616;&#20986;&#19982;&#20154;&#31867;&#29609;&#23478;&#30456;&#24403;&#30340;&#34920;&#29616;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2310.08235</link><description>&lt;p&gt;
GROOT: &#36890;&#36807;&#35266;&#30475;&#28216;&#25103;&#35270;&#39057;&#23398;&#20064;&#36981;&#24490;&#25351;&#20196;
&lt;/p&gt;
&lt;p&gt;
GROOT: Learning to Follow Instructions by Watching Gameplay Videos. (arXiv:2310.08235v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08235
&lt;/p&gt;
&lt;p&gt;
GROOT&#26159;&#19968;&#20010;&#36890;&#36807;&#35266;&#30475;&#28216;&#25103;&#35270;&#39057;&#23398;&#20064;&#36981;&#24490;&#25351;&#20196;&#30340;&#25511;&#21046;&#22120;&#65292;&#23427;&#36890;&#36807;&#20135;&#29983;&#19968;&#20010;&#32467;&#26500;&#21270;&#30340;&#30446;&#26631;&#31354;&#38388;&#26469;&#28040;&#38500;&#26114;&#36149;&#30340;&#25991;&#26412;-&#28216;&#25103;&#27880;&#37322;&#30340;&#38656;&#27714;&#65292;&#24182;&#22312;Minecraft SkillForge&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#29616;&#20986;&#19982;&#20154;&#31867;&#29609;&#23478;&#30456;&#24403;&#30340;&#34920;&#29616;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#22312;&#24320;&#25918;&#19990;&#30028;&#29615;&#22659;&#20013;&#26500;&#24314;&#19968;&#20010;&#21487;&#20197;&#36981;&#24490;&#24320;&#25918;&#24335;&#25351;&#20196;&#30340;&#25511;&#21046;&#22120;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#35266;&#30475;&#35270;&#39057;&#20316;&#20026;&#25351;&#20196;&#30340;&#26041;&#24335;&#65292;&#36825;&#31181;&#26041;&#24335;&#25552;&#20379;&#20102;&#34920;&#36798;&#33021;&#21147;&#24378;&#30340;&#30446;&#26631;&#35268;&#33539;&#65292;&#21516;&#26102;&#28040;&#38500;&#20102;&#26114;&#36149;&#30340;&#25991;&#26412;-&#28216;&#25103;&#27880;&#37322;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#28216;&#25103;&#35270;&#39057;&#20013;&#23398;&#20064;&#36825;&#31181;&#25351;&#20196;&#36981;&#24490;&#25511;&#21046;&#22120;&#65292;&#24182;&#20135;&#29983;&#19968;&#20010;&#33021;&#20135;&#29983;&#32467;&#26500;&#21270;&#30446;&#26631;&#31354;&#38388;&#30340;&#35270;&#39057;&#25351;&#20196;&#32534;&#30721;&#22120;&#12290;&#25105;&#20204;&#22312;&#22522;&#20110;&#22240;&#26524;&#21464;&#21387;&#22120;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#20013;&#23454;&#29616;&#20102;&#25105;&#20204;&#30340;&#20195;&#29702;&#31243;&#24207;GROOT&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#25552;&#20986;&#30340;Minecraft SkillForge&#22522;&#20934;&#27979;&#35797;&#20013;&#23545;GROOT&#36827;&#34892;&#20102;&#19982;&#24320;&#25918;&#19990;&#30028;&#23545;&#25163;&#21644;&#20154;&#31867;&#29609;&#23478;&#30340;&#35780;&#20272;&#12290;Elo&#35780;&#32423;&#28165;&#26970;&#22320;&#26174;&#31034;GROOT&#27491;&#22312;&#32553;&#23567;&#20154;&#26426;&#24046;&#36317;&#65292;&#24182;&#19988;&#23545;&#26368;&#22909;&#30340;&#36890;&#29992;&#20195;&#29702;&#31243;&#24207;&#22522;&#32447;&#20855;&#26377;70%&#30340;&#32988;&#29575;&#12290;&#23545;&#25152;&#20135;&#29983;&#30340;&#30446;&#26631;&#31354;&#38388;&#30340;&#23450;&#24615;&#20998;&#26512;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#19968;&#20123;&#26377;&#36259;&#30340;&#26032;&#39062;&#24615;&#36136;&#65292;&#21253;&#25324;&#30446;&#26631;&#30340;&#32452;&#25104;&#21644;...
&lt;/p&gt;
&lt;p&gt;
We study the problem of building a controller that can follow open-ended instructions in open-world environments. We propose to follow reference videos as instructions, which offer expressive goal specifications while eliminating the need for expensive text-gameplay annotations. A new learning framework is derived to allow learning such instruction-following controllers from gameplay videos while producing a video instruction encoder that induces a structured goal space. We implement our agent GROOT in a simple yet effective encoder-decoder architecture based on causal transformers. We evaluate GROOT against open-world counterparts and human players on a proposed Minecraft SkillForge benchmark. The Elo ratings clearly show that GROOT is closing the human-machine gap as well as exhibiting a 70% winning rate over the best generalist agent baseline. Qualitative analysis of the induced goal space further demonstrates some interesting emergent properties, including the goal composition and 
&lt;/p&gt;</description></item><item><title>&#22686;&#21152;&#23567;&#23610;&#24230;&#29289;&#20307;&#30340;&#26102;&#38388;&#27493;&#39057;&#21487;&#20197;&#25552;&#39640;&#26426;&#22120;&#20154;&#25805;&#20316;&#20223;&#30495;&#20934;&#30830;&#24615;&#65292;&#20026;&#25913;&#36827;&#26426;&#22120;&#20154;&#35013;&#37197;&#36807;&#31243;&#20013;&#30340;&#20223;&#30495;&#21040;&#30495;&#23454;&#29615;&#22659;&#36716;&#25442;&#25552;&#20379;&#20102;&#36215;&#28857;&#12290;</title><link>http://arxiv.org/abs/2310.08233</link><description>&lt;p&gt;
&#26102;&#38388;&#27493;&#39057;&#23545;&#19981;&#21516;&#23610;&#24230;&#29289;&#20307;&#26426;&#22120;&#20154;&#25805;&#20316;&#20223;&#30495;&#36924;&#30495;&#24230;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The Impact of Time Step Frequency on the Realism of Robotic Manipulation Simulation for Objects of Different Scales. (arXiv:2310.08233v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08233
&lt;/p&gt;
&lt;p&gt;
&#22686;&#21152;&#23567;&#23610;&#24230;&#29289;&#20307;&#30340;&#26102;&#38388;&#27493;&#39057;&#21487;&#20197;&#25552;&#39640;&#26426;&#22120;&#20154;&#25805;&#20316;&#20223;&#30495;&#20934;&#30830;&#24615;&#65292;&#20026;&#25913;&#36827;&#26426;&#22120;&#20154;&#35013;&#37197;&#36807;&#31243;&#20013;&#30340;&#20223;&#30495;&#21040;&#30495;&#23454;&#29615;&#22659;&#36716;&#25442;&#25552;&#20379;&#20102;&#36215;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#26102;&#38388;&#27493;&#39057;&#21644;&#32452;&#20214;&#23610;&#24230;&#23545;&#26426;&#22120;&#20154;&#25805;&#20316;&#20223;&#30495;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22686;&#21152;&#23567;&#23610;&#24230;&#29289;&#20307;&#30340;&#26102;&#38388;&#27493;&#39057;&#21487;&#20197;&#25552;&#39640;&#20223;&#30495;&#20934;&#30830;&#24615;&#12290;&#27492;&#20223;&#30495;&#23637;&#31034;&#20102;&#20004;&#31181;&#29289;&#20307;&#20960;&#20309;&#24418;&#29366;&#30340;&#39044;&#35013;&#37197;&#37096;&#20214;&#25342;&#21462;&#65292;&#20026;&#35752;&#35770;&#22914;&#20309;&#25913;&#36827;&#26426;&#22120;&#20154;&#35013;&#37197;&#36807;&#31243;&#20013;&#30340;&#20223;&#30495;&#21040;&#30495;&#23454;&#29615;&#22659;&#36716;&#25442;&#25552;&#20379;&#20102;&#36215;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work evaluates the impact of time step frequency and component scale on robotic manipulation simulation accuracy. Increasing the time step frequency for small-scale objects is shown to improve simulation accuracy. This simulation, demonstrating pre-assembly part picking for two object geometries, serves as a starting point for discussing how to improve Sim2Real transfer in robotic assembly processes.
&lt;/p&gt;</description></item><item><title>SimCKP&#26159;&#19968;&#20010;&#31616;&#21333;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#30701;&#35821;&#32423;&#34920;&#31034;&#26469;&#25552;&#21462;&#20851;&#38190;&#35789;&#30701;&#35821;&#65292;&#24182;&#36890;&#36807;&#37325;&#26032;&#25490;&#24207;&#26469;&#35843;&#25972;&#29983;&#25104;&#30340;&#30701;&#35821;&#30340;&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2310.08221</link><description>&lt;p&gt;
SimCKP: &#31616;&#21333;&#23545;&#27604;&#23398;&#20064;&#20851;&#38190;&#35789;&#30701;&#35821;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
SimCKP: Simple Contrastive Learning of Keyphrase Representations. (arXiv:2310.08221v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08221
&lt;/p&gt;
&lt;p&gt;
SimCKP&#26159;&#19968;&#20010;&#31616;&#21333;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#30701;&#35821;&#32423;&#34920;&#31034;&#26469;&#25552;&#21462;&#20851;&#38190;&#35789;&#30701;&#35821;&#65292;&#24182;&#36890;&#36807;&#37325;&#26032;&#25490;&#24207;&#26469;&#35843;&#25972;&#29983;&#25104;&#30340;&#30701;&#35821;&#30340;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#38190;&#35789;&#29983;&#25104;&#65288;KG&#65289;&#26088;&#22312;&#29983;&#25104;&#19968;&#32452;&#24635;&#32467;&#24615;&#35789;&#35821;&#25110;&#30701;&#35821;&#65292;&#32473;&#23450;&#19968;&#20010;&#28304;&#25991;&#26723;&#65292;&#32780;&#20851;&#38190;&#35789;&#25552;&#21462;&#65288;KE&#65289;&#26088;&#22312;&#20174;&#25991;&#26412;&#20013;&#35782;&#21035;&#23427;&#20204;&#12290;&#30001;&#20110;&#22312;KE&#20013;&#25628;&#32034;&#31354;&#38388;&#36739;&#23567;&#65292;&#36890;&#24120;&#23558;&#20854;&#19982;KG&#30456;&#32467;&#21512;&#65292;&#39044;&#27979;&#21487;&#33021;&#23384;&#22312;&#25110;&#19981;&#23384;&#22312;&#20110;&#30456;&#24212;&#25991;&#26723;&#20013;&#30340;&#20851;&#38190;&#35789;&#30701;&#35821;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#32479;&#19968;&#26041;&#27861;&#37319;&#29992;&#24207;&#21015;&#26631;&#27880;&#21644;&#22522;&#20110;&#26368;&#22823;&#21270;&#30340;&#29983;&#25104;&#65292;&#20027;&#35201;&#22312;&#20196;&#29260;&#32423;&#21035;&#19978;&#25805;&#20316;&#65292;&#19981;&#33021;&#24456;&#22909;&#22320;&#35266;&#23519;&#21644;&#35780;&#20998;&#20851;&#38190;&#35789;&#30701;&#35821;&#20316;&#20026;&#19968;&#20010;&#25972;&#20307;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SimCKP&#65292;&#19968;&#20010;&#31616;&#21333;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#65306;1&#65289;&#25552;&#21462;&#22120;-&#29983;&#25104;&#22120;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#30701;&#35821;&#32423;&#34920;&#31034;&#26469;&#25552;&#21462;&#20851;&#38190;&#35789;&#30701;&#35821;&#65292;&#21516;&#26102;&#29983;&#25104;&#19981;&#20986;&#29616;&#22312;&#25991;&#26723;&#20013;&#30340;&#20851;&#38190;&#35789;&#30701;&#35821;&#65307;2&#65289;&#37325;&#26032;&#25490;&#24207;&#22120;&#65292;&#36890;&#36807;&#23558;&#23427;&#20204;&#30340;&#34920;&#31034;&#19982;&#30456;&#24212;&#25991;&#26723;&#23545;&#40784;&#65292;&#21516;&#26679;&#35843;&#25972;&#27599;&#20010;&#29983;&#25104;&#30340;&#30701;&#35821;&#30340;&#20998;&#25968;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#19978;&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;
&lt;/p&gt;
&lt;p&gt;
Keyphrase generation (KG) aims to generate a set of summarizing words or phrases given a source document, while keyphrase extraction (KE) aims to identify them from the text. Because the search space is much smaller in KE, it is often combined with KG to predict keyphrases that may or may not exist in the corresponding document. However, current unified approaches adopt sequence labeling and maximization-based generation that primarily operate at a token level, falling short in observing and scoring keyphrases as a whole. In this work, we propose SimCKP, a simple contrastive learning framework that consists of two stages: 1) An extractor-generator that extracts keyphrases by learning context-aware phrase-level representations in a contrastive manner while also generating keyphrases that do not appear in the document; 2) A reranker that adapts scores for each generated phrase by likewise aligning their representations with the corresponding document. Experimental results on multiple ben
&lt;/p&gt;</description></item><item><title>TriRE&#26159;&#19968;&#20010;&#26032;&#30340;&#25345;&#32493;&#23398;&#20064;&#33539;&#24335;&#65292;&#21463;&#21040;&#22823;&#33041;&#22914;&#20309;&#21033;&#29992;&#22810;&#31181;&#26426;&#21046;&#36827;&#34892;&#23398;&#20064;&#30340;&#21551;&#21457;&#12290;&#23427;&#36890;&#36807;&#20445;&#25345;&#27599;&#20010;&#20219;&#21153;&#20013;&#26368;&#26174;&#33879;&#30340;&#31070;&#32463;&#20803;&#65292;&#20462;&#35746;&#21644;&#24041;&#22266;&#36825;&#20123;&#31070;&#32463;&#20803;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#35299;&#20915;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.08217</link><description>&lt;p&gt;
TriRE: &#19968;&#31181;&#29992;&#20110;&#25345;&#32493;&#30693;&#35782;&#20445;&#25345;&#21644;&#25512;&#36827;&#30340;&#22810;&#26426;&#21046;&#23398;&#20064;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
TriRE: A Multi-Mechanism Learning Paradigm for Continual Knowledge Retention and Promotion. (arXiv:2310.08217v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08217
&lt;/p&gt;
&lt;p&gt;
TriRE&#26159;&#19968;&#20010;&#26032;&#30340;&#25345;&#32493;&#23398;&#20064;&#33539;&#24335;&#65292;&#21463;&#21040;&#22823;&#33041;&#22914;&#20309;&#21033;&#29992;&#22810;&#31181;&#26426;&#21046;&#36827;&#34892;&#23398;&#20064;&#30340;&#21551;&#21457;&#12290;&#23427;&#36890;&#36807;&#20445;&#25345;&#27599;&#20010;&#20219;&#21153;&#20013;&#26368;&#26174;&#33879;&#30340;&#31070;&#32463;&#20803;&#65292;&#20462;&#35746;&#21644;&#24041;&#22266;&#36825;&#20123;&#31070;&#32463;&#20803;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#35299;&#20915;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#23545;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26469;&#35828;&#19968;&#30452;&#26159;&#19968;&#20010;&#25345;&#20037;&#30340;&#25361;&#25112;&#65292;&#21407;&#22240;&#26159;&#20808;&#21069;&#23398;&#20064;&#30340;&#20219;&#21153;&#20250;&#21457;&#29983;&#28798;&#38590;&#24615;&#36951;&#24536;&#65288;CF&#65289;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#25216;&#26415;&#26469;&#20943;&#36731;CF&#65292;&#20363;&#22914;&#26435;&#37325;&#27491;&#21017;&#21270;&#65292;&#32463;&#39564;&#37325;&#28436;&#21644;&#21442;&#25968;&#38548;&#31163;&#12290;&#23613;&#31649;&#30456;&#23545;&#25104;&#21151;&#65292;&#20294;&#36825;&#20123;&#30740;&#31350;&#26041;&#21521;&#20027;&#35201;&#26159;&#30456;&#20114;&#29420;&#31435;&#30340;&#65292;&#23384;&#22312;&#19968;&#20123;&#32570;&#28857;&#65292;&#24182;&#19988;&#38169;&#36807;&#20102;&#31454;&#20105;&#31574;&#30053;&#30340;&#20248;&#21183;&#12290;&#30456;&#21453;&#65292;&#22823;&#33041;&#36890;&#36807;&#21516;&#26102;&#21033;&#29992;&#22810;&#31181;&#31070;&#32463;&#29983;&#29702;&#36807;&#31243;&#65288;&#21253;&#25324;&#31070;&#32463;&#21457;&#29983;&#65292;&#20027;&#21160;&#36951;&#24536;&#65292;&#31070;&#32463;&#35843;&#33410;&#65292;&#20803;&#21487;&#22609;&#24615;&#65292;&#32463;&#39564;&#37325;&#28436;&#21644;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#38376;&#25511;&#65289;&#65292;&#19981;&#24120;&#23548;&#33268;CF&#65292;&#32780;&#26159;&#25345;&#32493;&#23398;&#20064;&#65292;&#36866;&#24212;&#21644;&#36328;&#20219;&#21153;&#20256;&#36882;&#30693;&#35782;&#12290;&#21463;&#22823;&#33041;&#22914;&#20309;&#21516;&#26102;&#21033;&#29992;&#22810;&#20010;&#26426;&#21046;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TriRE&#65292;&#19968;&#31181;&#26032;&#30340;CL&#33539;&#24335;&#65292;&#26088;&#22312;&#20445;&#30041;&#27599;&#20010;&#20219;&#21153;&#20013;&#26368;&#26174;&#33879;&#30340;&#31070;&#32463;&#20803;&#65292;&#20462;&#35746;&#21644;&#24041;&#22266;&#36825;&#20123;&#31070;&#32463;&#20803;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual learning (CL) has remained a persistent challenge for deep neural networks due to catastrophic forgetting (CF) of previously learned tasks. Several techniques such as weight regularization, experience rehearsal, and parameter isolation have been proposed to alleviate CF. Despite their relative success, these research directions have predominantly remained orthogonal and suffer from several shortcomings, while missing out on the advantages of competing strategies. On the contrary, the brain continually learns, accommodates, and transfers knowledge across tasks by simultaneously leveraging several neurophysiological processes, including neurogenesis, active forgetting, neuromodulation, metaplasticity, experience rehearsal, and context-dependent gating, rarely resulting in CF. Inspired by how the brain exploits multiple mechanisms concurrently, we propose TriRE, a novel CL paradigm that encompasses retaining the most prominent neurons for each task, revising and solidifying the 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35752;&#35770;&#20102;&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#38754;&#20020;&#30340;&#21487;&#20449;&#24230;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20998;&#24067;&#20043;&#22806;&#30340;&#27867;&#21270;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#21487;&#20449;&#24230;&#35780;&#20272;&#22235;&#20010;&#20851;&#38190;&#20027;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2310.08215</link><description>&lt;p&gt;
&#21487;&#20449;&#20219;&#30340;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Trustworthy Machine Learning. (arXiv:2310.08215v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08215
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35752;&#35770;&#20102;&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#38754;&#20020;&#30340;&#21487;&#20449;&#24230;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20998;&#24067;&#20043;&#22806;&#30340;&#27867;&#21270;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#21487;&#20449;&#24230;&#35780;&#20272;&#22235;&#20010;&#20851;&#38190;&#20027;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24212;&#29992;&#20110;&#23454;&#38469;&#20135;&#21697;&#21644;&#35299;&#20915;&#26041;&#26696;&#65292;&#20986;&#29616;&#20102;&#26032;&#30340;&#25361;&#25112;&#12290;&#27169;&#22411;&#22312;&#20998;&#24067;&#30340;&#24494;&#23567;&#21464;&#21270;&#19979;&#24847;&#22806;&#22320;&#26080;&#27861;&#27867;&#21270;&#65292;&#23545;&#20110;&#20182;&#20204;&#20174;&#26410;&#35265;&#36807;&#30340;&#26032;&#25968;&#25454;&#34920;&#29616;&#20986;&#33258;&#20449;&#65292;&#25110;&#32773;&#26080;&#27861;&#26377;&#25928;&#22320;&#21521;&#26368;&#32456;&#29992;&#25143;&#35299;&#37322;&#20854;&#20915;&#31574;&#30340;&#21407;&#29702;&#12290;&#24635;&#32780;&#35328;&#20043;&#65292;&#25105;&#20204;&#38754;&#20020;&#30528;&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#21487;&#20449;&#24230;&#38382;&#39064;&#12290;&#36825;&#26412;&#12298;&#21487;&#20449;&#20219;&#30340;&#26426;&#22120;&#23398;&#20064;&#12299;&#25945;&#26448;&#28085;&#30422;&#20102;&#21487;&#20449;&#20219;&#30340;&#26426;&#22120;&#23398;&#20064;&#30340;&#22235;&#20010;&#20851;&#38190;&#20027;&#39064;&#30340;&#29702;&#35770;&#21644;&#25216;&#26415;&#32972;&#26223;&#65306;&#20998;&#24067;&#20043;&#22806;&#30340;&#27867;&#21270;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#21487;&#20449;&#24230;&#35780;&#20272;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#37325;&#35201;&#30340;&#32463;&#20856;&#21644;&#29616;&#20195;&#30740;&#31350;&#35770;&#25991;&#65292;&#24182;&#25581;&#31034;&#24182;&#36830;&#25509;&#20102;&#23427;&#20204;&#30340;&#22522;&#26412;&#30452;&#35273;&#12290;&#36825;&#26412;&#20070;&#26159;&#20174;2022/23&#20908;&#23395;&#23398;&#26399;&#24320;&#22987;&#22312;&#22270;&#23486;&#26681;&#22823;&#23398;&#24320;&#35774;&#30340;&#21516;&#21517;&#35838;&#31243;&#21457;&#23637;&#32780;&#26469;&#12290;&#23427;&#26088;&#22312;&#25104;&#20026;&#19968;&#26412;&#29420;&#31435;&#30340;&#20135;&#21697;&#65292;&#38468;&#26377;&#20195;&#30721;&#29255;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;
As machine learning technology gets applied to actual products and solutions, new challenges have emerged. Models unexpectedly fail to generalize to small changes in the distribution, tend to be confident on novel data they have never seen, or cannot communicate the rationale behind their decisions effectively with the end users. Collectively, we face a trustworthiness issue with the current machine learning technology. This textbook on Trustworthy Machine Learning (TML) covers a theoretical and technical background of four key topics in TML: Out-of-Distribution Generalization, Explainability, Uncertainty Quantification, and Evaluation of Trustworthiness. We discuss important classical and contemporary research papers of the aforementioned fields and uncover and connect their underlying intuitions. The book evolved from the homonymous course at the University of T\"ubingen, first offered in the Winter Semester of 2022/23. It is meant to be a stand-alone product accompanied by code snip
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31895;&#31890;&#24230;&#24341;&#23548;&#26862;&#26519;&#21644;&#22810;&#20013;&#24515;&#25439;&#22833;&#30340;&#38271;&#23614;&#20998;&#31867;&#26694;&#26550;&#65292;&#21517;&#20026;Cognisance&#12290;&#35813;&#26694;&#26550;&#33268;&#21147;&#20110;&#35299;&#20915;&#38271;&#23614;&#20998;&#31867;&#38382;&#39064;&#20013;&#30340;&#31867;&#38388;&#21644;&#31867;&#20869;&#19981;&#24179;&#34913;&#65292;&#24182;&#36890;&#36807;&#19981;&#21464;&#29305;&#24449;&#23398;&#20064;&#26500;&#24314;&#22810;&#31890;&#24230;&#32852;&#21512;&#35299;&#20915;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.08206</link><description>&lt;p&gt;
&#22522;&#20110;&#31895;&#31890;&#24230;&#24341;&#23548;&#26862;&#26519;&#21644;&#22810;&#20013;&#24515;&#25439;&#22833;&#30340;&#38271;&#23614;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Long-Tailed Classification Based on Coarse-Grained Leading Forest and Multi-Center Loss. (arXiv:2310.08206v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08206
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31895;&#31890;&#24230;&#24341;&#23548;&#26862;&#26519;&#21644;&#22810;&#20013;&#24515;&#25439;&#22833;&#30340;&#38271;&#23614;&#20998;&#31867;&#26694;&#26550;&#65292;&#21517;&#20026;Cognisance&#12290;&#35813;&#26694;&#26550;&#33268;&#21147;&#20110;&#35299;&#20915;&#38271;&#23614;&#20998;&#31867;&#38382;&#39064;&#20013;&#30340;&#31867;&#38388;&#21644;&#31867;&#20869;&#19981;&#24179;&#34913;&#65292;&#24182;&#36890;&#36807;&#19981;&#21464;&#29305;&#24449;&#23398;&#20064;&#26500;&#24314;&#22810;&#31890;&#24230;&#32852;&#21512;&#35299;&#20915;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#23614;&#20998;&#31867;&#26159;&#29616;&#23454;&#19990;&#30028;&#20013;&#19981;&#21487;&#36991;&#20813;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#22823;&#37096;&#20998;&#29616;&#26377;&#30340;&#38271;&#23614;&#20998;&#31867;&#26041;&#27861;&#20165;&#20851;&#27880;&#35299;&#20915;&#31867;&#38388;&#19981;&#24179;&#34913;&#65292;&#21363;&#22836;&#37096;&#31867;&#21035;&#30340;&#26679;&#26412;&#27604;&#23614;&#37096;&#31867;&#21035;&#30340;&#26679;&#26412;&#22810;&#65292;&#32780;&#24573;&#30053;&#20102;&#31867;&#20869;&#19981;&#24179;&#34913;&#65292;&#21363;&#21516;&#19968;&#31867;&#21035;&#20013;&#22836;&#37096;&#23646;&#24615;&#26679;&#26412;&#25968;&#37327;&#36828;&#22823;&#20110;&#23614;&#37096;&#23646;&#24615;&#26679;&#26412;&#25968;&#37327;&#12290;&#27169;&#22411;&#30340;&#20559;&#24046;&#26159;&#30001;&#36825;&#20004;&#20010;&#22240;&#32032;&#24341;&#36215;&#30340;&#65292;&#30001;&#20110;&#22823;&#22810;&#25968;&#25968;&#25454;&#38598;&#20013;&#30340;&#23646;&#24615;&#26159;&#38544;&#21547;&#30340;&#19988;&#23646;&#24615;&#32452;&#21512;&#38750;&#24120;&#22797;&#26434;&#65292;&#22788;&#29702;&#31867;&#20869;&#19981;&#24179;&#34913;&#26356;&#21152;&#22256;&#38590;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31895;&#31890;&#24230;&#24341;&#23548;&#26862;&#26519;&#65288;CLF&#65289;&#21644;&#22810;&#20013;&#24515;&#25439;&#22833;&#65288;MCL&#65289;&#30340;&#38271;&#23614;&#20998;&#31867;&#26694;&#26550;&#65292;&#21517;&#20026;Cognisance&#65292;&#26088;&#22312;&#36890;&#36807;&#19981;&#21464;&#29305;&#24449;&#23398;&#20064;&#26500;&#24314;&#22810;&#31890;&#24230;&#32852;&#21512;&#35299;&#20915;&#27169;&#22411;&#12290;&#22312;&#36825;&#20010;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26679;&#26412;&#36873;&#25321;&#31574;&#30053;&#21644;&#25439;&#22833;&#20989;&#25968;&#65292;&#20197;&#24179;&#34913;&#19981;&#21516;&#31867;&#21035;&#21644;&#23646;&#24615;&#20043;&#38388;&#30340;&#26679;&#26412;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Long-tailed(LT) classification is an unavoidable and challenging problem in the real world. Most of the existing long-tailed classification methods focus only on solving the inter-class imbalance in which there are more samples in the head class than in the tail class, while ignoring the intra-lass imbalance in which the number of samples of the head attribute within the same class is much larger than the number of samples of the tail attribute. The deviation in the model is caused by both of these factors, and due to the fact that attributes are implicit in most datasets and the combination of attributes is very complex, the intra-class imbalance is more difficult to handle. For this purpose, we proposed a long-tailed classification framework, known as \textbf{\textsc{Cognisance}}, which is founded on Coarse-Grained Leading Forest (CLF) and Multi-Center Loss (MCL), aiming to build a multi-granularity joint solution model by means of invariant feature learning. In this method, we desig
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26032;&#39062;DoE&#26041;&#27861;&#65292;&#29992;&#20110;&#20248;&#21270;&#30005;&#27744;&#21160;&#21147;&#23398;&#27169;&#22411;&#35782;&#21035;&#20013;&#30340;&#23454;&#39564;&#35774;&#35745;&#12290;&#35813;&#26041;&#27861;&#26681;&#25454;&#36807;&#21435;&#23454;&#39564;&#30340;&#32479;&#35745;&#20449;&#24687;&#21160;&#24577;&#22320;&#25913;&#21464;&#23454;&#39564;&#37197;&#32622;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#23454;&#39564;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.08198</link><description>&lt;p&gt;
&#36229;&#36234;&#20256;&#32479;DoE&#65306;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#20248;&#21270;&#30005;&#27744;&#21160;&#21147;&#23398;&#27169;&#22411;&#35782;&#21035;&#20013;&#30340;&#23454;&#39564;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Beyond Traditional DoE: Deep Reinforcement Learning for Optimizing Experiments in Model Identification of Battery Dynamics. (arXiv:2310.08198v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08198
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26032;&#39062;DoE&#26041;&#27861;&#65292;&#29992;&#20110;&#20248;&#21270;&#30005;&#27744;&#21160;&#21147;&#23398;&#27169;&#22411;&#35782;&#21035;&#20013;&#30340;&#23454;&#39564;&#35774;&#35745;&#12290;&#35813;&#26041;&#27861;&#26681;&#25454;&#36807;&#21435;&#23454;&#39564;&#30340;&#32479;&#35745;&#20449;&#24687;&#21160;&#24577;&#22320;&#25913;&#21464;&#23454;&#39564;&#37197;&#32622;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#23454;&#39564;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#27744;&#21160;&#21147;&#23398;&#27169;&#22411;&#35782;&#21035;&#26159;&#33021;&#28304;&#30740;&#31350;&#20013;&#30340;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#65307;&#35768;&#22810;&#33021;&#28304;&#31649;&#29702;&#31995;&#32479;&#21644;&#35774;&#35745;&#36807;&#31243;&#20381;&#36182;&#20110;&#20934;&#30830;&#30340;&#30005;&#27744;&#27169;&#22411;&#36827;&#34892;&#25928;&#29575;&#20248;&#21270;&#12290;&#20256;&#32479;&#30340;&#30005;&#27744;&#24314;&#27169;&#26041;&#27861;&#26159;&#21033;&#29992;&#20256;&#32479;&#30340;&#23454;&#39564;&#35774;&#35745;&#65288;DoE&#65289;&#65292;&#20854;&#20013;&#36890;&#36807;&#35768;&#22810;&#19981;&#21516;&#30340;&#30005;&#27969;&#37197;&#32622;&#26469;&#28608;&#21457;&#30005;&#27744;&#21160;&#21147;&#23398;&#65292;&#24182;&#21033;&#29992;&#27979;&#37327;&#36755;&#20986;&#26469;&#20272;&#35745;&#31995;&#32479;&#21160;&#21147;&#23398;&#12290;&#28982;&#32780;&#65292;&#34429;&#28982;&#20256;&#32479;&#26041;&#27861;&#21487;&#20197;&#33719;&#24471;&#26377;&#29992;&#30340;&#27169;&#22411;&#65292;&#20294;&#30001;&#20110;&#38656;&#35201;&#25195;&#25551;&#35768;&#22810;&#19981;&#21516;&#30340;&#30005;&#27969;&#37197;&#32622;&#65292;&#36825;&#19968;&#36807;&#31243;&#26102;&#38388;&#38271;&#19988;&#26114;&#36149;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;DoE&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26681;&#25454;&#36807;&#21435;&#23454;&#39564;&#30340;&#32479;&#35745;&#20449;&#24687;&#21160;&#24577;&#22320;&#25913;&#21464;&#23454;&#39564;&#37197;&#32622;&#12290;&#19982;&#22362;&#25345;&#39044;&#23450;&#20041;&#30005;&#27969;&#37197;&#32622;&#30340;&#24211;&#19981;&#21516;&#65292;&#25552;&#20986;&#30340;&#26041;&#27861;&#36890;&#36807;&#26356;&#26032;&#36755;&#20986;&#31354;&#38388;&#26469;&#21160;&#24577;&#20462;&#25913;&#30005;&#27969;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model identification of battery dynamics is a central problem in energy research; many energy management systems and design processes rely on accurate battery models for efficiency optimization. The standard methodology for battery modelling is traditional design of experiments (DoE), where the battery dynamics are excited with many different current profiles and the measured outputs are used to estimate the system dynamics. However, although it is possible to obtain useful models with the traditional approach, the process is time consuming and expensive because of the need to sweep many different current-profile configurations. In the present work, a novel DoE approach is developed based on deep reinforcement learning, which alters the configuration of the experiments on the fly based on the statistics of past experiments. Instead of sticking to a library of predefined current profiles, the proposed approach modifies the current profiles dynamically by updating the output space covere
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;EIPE-text&#26041;&#27861;&#29992;&#20110;&#38271;&#31687;&#21465;&#20107;&#25991;&#26412;&#29983;&#25104;&#65292;&#36890;&#36807;&#20174;&#35821;&#26009;&#24211;&#20013;&#25552;&#21462;&#35745;&#21010;&#24182;&#21033;&#29992;&#35780;&#20272;&#26426;&#21046;&#36827;&#34892;&#36845;&#20195;&#25913;&#36827;&#65292;&#26500;&#24314;&#20102;&#26356;&#22909;&#30340;&#35268;&#21010;&#22120;&#12290;</title><link>http://arxiv.org/abs/2310.08185</link><description>&lt;p&gt;
EIPE-text: &#38024;&#23545;&#38271;&#31687;&#21465;&#20107;&#25991;&#26412;&#29983;&#25104;&#30340;&#35780;&#20272;&#24341;&#23548;&#36845;&#20195;&#35745;&#21010;&#25552;&#21462;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
EIPE-text: Evaluation-Guided Iterative Plan Extraction for Long-Form Narrative Text Generation. (arXiv:2310.08185v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08185
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;EIPE-text&#26041;&#27861;&#29992;&#20110;&#38271;&#31687;&#21465;&#20107;&#25991;&#26412;&#29983;&#25104;&#65292;&#36890;&#36807;&#20174;&#35821;&#26009;&#24211;&#20013;&#25552;&#21462;&#35745;&#21010;&#24182;&#21033;&#29992;&#35780;&#20272;&#26426;&#21046;&#36827;&#34892;&#36845;&#20195;&#25913;&#36827;&#65292;&#26500;&#24314;&#20102;&#26356;&#22909;&#30340;&#35268;&#21010;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#21010;&#19982;&#20889;&#20316;&#26159;&#38271;&#31687;&#21465;&#20107;&#25991;&#26412;&#29983;&#25104;&#20013;&#24120;&#29992;&#30340;&#23618;&#27425;&#21270;&#26041;&#27861;&#65292;&#39318;&#20808;&#21019;&#24314;&#19968;&#20010;&#35745;&#21010;&#26469;&#25351;&#23548;&#21465;&#20107;&#20889;&#20316;&#12290;&#36981;&#24490;&#36825;&#31181;&#26041;&#27861;&#65292;&#19968;&#20123;&#30740;&#31350;&#20165;&#20165;&#20381;&#38752;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25552;&#31034;&#36827;&#34892;&#35745;&#21010;&#65292;&#36825;&#32463;&#24120;&#20135;&#29983;&#27425;&#20248;&#30340;&#32467;&#26524;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#35780;&#20272;&#24341;&#23548;&#36845;&#20195;&#35745;&#21010;&#25552;&#21462;&#26041;&#27861;&#65288;EIPE-text&#65289;&#29992;&#20110;&#38271;&#31687;&#21465;&#20107;&#25991;&#26412;&#29983;&#25104;&#65292;&#35813;&#26041;&#27861;&#20174;&#21465;&#20107;&#35821;&#26009;&#24211;&#20013;&#25552;&#21462;&#35745;&#21010;&#24182;&#21033;&#29992;&#25552;&#21462;&#30340;&#35745;&#21010;&#26500;&#24314;&#26356;&#22909;&#30340;&#35268;&#21010;&#22120;&#12290;EIPE-text&#26377;&#19977;&#20010;&#38454;&#27573;&#65306;&#35745;&#21010;&#25552;&#21462;&#12289;&#23398;&#20064;&#21644;&#25512;&#29702;&#12290;&#22312;&#35745;&#21010;&#25552;&#21462;&#38454;&#27573;&#65292;&#23427;&#36845;&#20195;&#22320;&#20174;&#21465;&#20107;&#35821;&#26009;&#24211;&#20013;&#25552;&#21462;&#21644;&#25913;&#36827;&#35745;&#21010;&#65292;&#24182;&#26500;&#24314;&#19968;&#20010;&#35745;&#21010;&#35821;&#26009;&#24211;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38382;&#31572;&#65288;QA&#65289;&#30340;&#35780;&#20272;&#26426;&#21046;&#65292;&#33258;&#21160;&#35780;&#20272;&#35745;&#21010;&#24182;&#29983;&#25104;&#35814;&#32454;&#30340;&#35745;&#21010;&#25913;&#36827;&#25351;&#31034;&#65292;&#20197;&#24341;&#23548;&#36845;&#20195;&#25913;&#36827;&#12290;&#22312;&#23398;&#20064;&#38454;&#27573;&#65292;&#25105;&#20204;&#36890;&#36807;&#24494;&#35843;&#26500;&#24314;&#19968;&#20010;&#26356;&#22909;&#30340;&#35268;&#21010;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Plan-and-Write is a common hierarchical approach in long-form narrative text generation, which first creates a plan to guide the narrative writing. Following this approach, several studies rely on simply prompting large language models for planning, which often yields suboptimal results. In this paper, we propose a new framework called Evaluation-guided Iterative Plan Extraction for long-form narrative text generation (EIPE-text), which extracts plans from the corpus of narratives and utilizes the extracted plans to construct a better planner. EIPE-text has three stages: plan extraction, learning, and inference. In the plan extraction stage, it iteratively extracts and improves plans from the narrative corpus and constructs a plan corpus. We propose a question answer (QA) based evaluation mechanism to automatically evaluate the plans and generate detailed plan refinement instructions to guide the iterative improvement. In the learning stage, we build a better planner by fine-tuning wit
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20197;Learn From Model (LFM)&#20026;&#21517;&#65292;&#25506;&#32034;&#20102;&#36229;&#36234;&#24494;&#35843;&#30340;&#27169;&#22411;&#23398;&#20064;&#25216;&#26415;&#65292;&#26088;&#22312;&#36890;&#36807;&#23545;&#27169;&#22411;&#25509;&#21475;&#36827;&#34892;&#30740;&#31350;&#21644;&#35774;&#35745;&#65292;&#23558;&#22522;&#20110;&#27169;&#22411;&#30340;&#23398;&#20064;&#25512;&#24191;&#21040;&#19979;&#28216;&#20219;&#21153;&#20013;&#12290;</title><link>http://arxiv.org/abs/2310.08184</link><description>&lt;p&gt;
&#36229;&#36234;&#24494;&#35843;&#30340;&#27169;&#22411;&#23398;&#20064;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Learn From Model Beyond Fine-Tuning: A Survey. (arXiv:2310.08184v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08184
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20197;Learn From Model (LFM)&#20026;&#21517;&#65292;&#25506;&#32034;&#20102;&#36229;&#36234;&#24494;&#35843;&#30340;&#27169;&#22411;&#23398;&#20064;&#25216;&#26415;&#65292;&#26088;&#22312;&#36890;&#36807;&#23545;&#27169;&#22411;&#25509;&#21475;&#36827;&#34892;&#30740;&#31350;&#21644;&#35774;&#35745;&#65292;&#23558;&#22522;&#20110;&#27169;&#22411;&#30340;&#23398;&#20064;&#25512;&#24191;&#21040;&#19979;&#28216;&#20219;&#21153;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27169;&#22411;&#30340;&#23398;&#20064;&#65288;LFM&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#30740;&#31350;&#36235;&#21183;&#65292;&#23427;&#19987;&#27880;&#20110;&#36890;&#36807;&#23545;&#27169;&#22411;&#25509;&#21475;&#36827;&#34892;&#30740;&#31350;&#12289;&#20462;&#25913;&#21644;&#35774;&#35745;&#26469;&#26356;&#22909;&#22320;&#29702;&#35299;&#27169;&#22411;&#30340;&#32467;&#26500;&#21644;&#26435;&#37325;&#65288;&#22312;&#40657;&#21283;&#23376;&#29615;&#22659;&#20013;&#65289;&#65292;&#24182;&#23558;&#27169;&#22411;&#27867;&#21270;&#21040;&#19979;&#28216;&#20219;&#21153;&#20013;&#12290;&#26412;&#25991;&#23558;LFM&#25216;&#26415;&#30340;&#30740;&#31350;&#20998;&#20026;&#20116;&#20010;&#20027;&#35201;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation models (FM) have demonstrated remarkable performance across a wide range of tasks (especially in the fields of natural language processing and computer vision), primarily attributed to their ability to comprehend instructions and access extensive, high-quality data. This not only showcases their current effectiveness but also sets a promising trajectory towards the development of artificial general intelligence. Unfortunately, due to multiple constraints, the raw data of the model used for large model training are often inaccessible, so the use of end-to-end models for downstream tasks has become a new research trend, which we call Learn From Model (LFM) in this article. LFM focuses on the research, modification, and design of FM based on the model interface, so as to better understand the model structure and weights (in a black box environment), and to generalize the model to downstream tasks. The study of LFM techniques can be broadly categorized into five major areas: mod
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MSSTRN&#30340;&#22810;&#23610;&#24230;&#26102;&#31354;&#24490;&#29615;&#32593;&#32476;&#65292;&#29992;&#20110;&#20132;&#36890;&#27969;&#39044;&#27979;&#12290;&#35813;&#32593;&#32476;&#36890;&#36807;&#20004;&#31181;&#19981;&#21516;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#23436;&#20840;&#25429;&#33719;&#20102;&#20132;&#36890;&#25968;&#25454;&#20013;&#19981;&#21516;&#26102;&#38388;&#31383;&#21475;&#19979;&#30340;&#22797;&#26434;&#26102;&#31354;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#24573;&#35270;&#22270;&#32467;&#26500;&#12289;&#25429;&#25417;&#19981;&#20805;&#20998;&#20197;&#21450;&#32570;&#20047;&#20851;&#27880;&#19981;&#21516;&#26102;&#38388;&#23610;&#24230;&#19978;&#26102;&#31354;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.08138</link><description>&lt;p&gt;
&#22810;&#23610;&#24230;&#26102;&#31354;&#24490;&#29615;&#32593;&#32476;&#29992;&#20110;&#20132;&#36890;&#27969;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Multi-Scale Spatial-Temporal Recurrent Networks for Traffic Flow Prediction. (arXiv:2310.08138v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08138
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MSSTRN&#30340;&#22810;&#23610;&#24230;&#26102;&#31354;&#24490;&#29615;&#32593;&#32476;&#65292;&#29992;&#20110;&#20132;&#36890;&#27969;&#39044;&#27979;&#12290;&#35813;&#32593;&#32476;&#36890;&#36807;&#20004;&#31181;&#19981;&#21516;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#23436;&#20840;&#25429;&#33719;&#20102;&#20132;&#36890;&#25968;&#25454;&#20013;&#19981;&#21516;&#26102;&#38388;&#31383;&#21475;&#19979;&#30340;&#22797;&#26434;&#26102;&#31354;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#24573;&#35270;&#22270;&#32467;&#26500;&#12289;&#25429;&#25417;&#19981;&#20805;&#20998;&#20197;&#21450;&#32570;&#20047;&#20851;&#27880;&#19981;&#21516;&#26102;&#38388;&#23610;&#24230;&#19978;&#26102;&#31354;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#27969;&#39044;&#27979;&#26159;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#20013;&#26368;&#22522;&#26412;&#30340;&#20219;&#21153;&#20043;&#19968;&#12290;&#22797;&#26434;&#32780;&#21160;&#24577;&#30340;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#20351;&#24471;&#20132;&#36890;&#27969;&#39044;&#27979;&#21464;&#24471;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#20132;&#36890;&#27969;&#39044;&#27979;&#20013;&#26377;&#26174;&#33879;&#20248;&#21183;&#65292;&#20294;&#23427;&#20204;&#24120;&#24120;&#38754;&#20020;&#20197;&#19979;&#25361;&#25112;&#65306;&#65288;1&#65289;&#24573;&#35270;&#20102;&#22266;&#23450;&#30340;&#22270;&#32467;&#26500;&#65292;&#38480;&#21046;&#20102;&#27169;&#22411;&#30340;&#39044;&#27979;&#24615;&#33021;&#65307;&#65288;2&#65289;&#19981;&#33021;&#21516;&#26102;&#20805;&#20998;&#25429;&#25417;&#22797;&#26434;&#30340;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#65307;&#65288;3&#65289;&#32570;&#20047;&#23545;&#19981;&#21516;&#26102;&#38388;&#23610;&#24230;&#19978;&#30340;&#26102;&#31354;&#20449;&#24687;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#26102;&#31354;&#24490;&#29615;&#32593;&#32476;&#65288;MSSTRN&#65289;&#29992;&#20110;&#20132;&#36890;&#27969;&#39044;&#27979;&#65292;&#23427;&#30001;&#20004;&#31181;&#19981;&#21516;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#32452;&#25104;&#65306;&#21333;&#27493;&#38376;&#24490;&#29615;&#21333;&#20803;&#21644;&#22810;&#27493;&#38376;&#24490;&#29615;&#21333;&#20803;&#65292;&#20197;&#23436;&#20840;&#25429;&#33719;&#20132;&#36890;&#25968;&#25454;&#20013;&#19981;&#21516;&#26102;&#38388;&#31383;&#21475;&#19979;&#30340;&#22797;&#26434;&#26102;&#31354;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26102;&#31354;&#21516;&#27493;&#27880;&#24847;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic flow prediction is one of the most fundamental tasks of intelligent transportation systems. The complex and dynamic spatial-temporal dependencies make the traffic flow prediction quite challenging. Although existing spatial-temporal graph neural networks hold prominent, they often encounter challenges such as (1) ignoring the fixed graph that limits the predictive performance of the model, (2) insufficiently capturing complex spatial-temporal dependencies simultaneously, and (3) lacking attention to spatial-temporal information at different time lengths. In this paper, we propose a Multi-Scale Spatial-Temporal Recurrent Network for traffic flow prediction, namely MSSTRN, which consists of two different recurrent neural networks: the single-step gate recurrent unit and the multi-step gate recurrent unit to fully capture the complex spatial-temporal information in the traffic data under different time windows. Moreover, we propose a spatial-temporal synchronous attention mechanis
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35745;&#21010;&#39046;&#22495;&#20013;&#30340;&#39564;&#35777;&#21644;&#33258;&#25105;&#25209;&#35780;&#33021;&#21147;&#12290;&#30740;&#31350;&#32467;&#26524;&#21457;&#29616;&#65292;&#33258;&#25105;&#25209;&#35780;&#20284;&#20046;&#20250;&#38477;&#20302;&#35745;&#21010;&#29983;&#25104;&#30340;&#24615;&#33021;&#65292;&#24182;&#23548;&#33268;&#22823;&#37327;&#35823;&#25253;&#65292;&#38477;&#20302;&#20102;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.08118</link><description>&lt;p&gt;
&#33021;&#21542;&#36890;&#36807;&#33258;&#25105;&#25209;&#35780;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#33258;&#36523;&#35745;&#21010;&#30340;&#33021;&#21147;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models Really Improve by Self-critiquing Their Own Plans?. (arXiv:2310.08118v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08118
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35745;&#21010;&#39046;&#22495;&#20013;&#30340;&#39564;&#35777;&#21644;&#33258;&#25105;&#25209;&#35780;&#33021;&#21147;&#12290;&#30740;&#31350;&#32467;&#26524;&#21457;&#29616;&#65292;&#33258;&#25105;&#25209;&#35780;&#20284;&#20046;&#20250;&#38477;&#20302;&#35745;&#21010;&#29983;&#25104;&#30340;&#24615;&#33021;&#65292;&#24182;&#23548;&#33268;&#22823;&#37327;&#35823;&#25253;&#65292;&#38477;&#20302;&#20102;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#21542;&#25104;&#21151;&#39564;&#35777;&#25110;&#33258;&#25105;&#25209;&#35780;&#20854;&#22312;&#25512;&#29702;&#38382;&#39064;&#20013;&#30340;&#20505;&#36873;&#35299;&#20915;&#26041;&#26696;&#30340;&#24191;&#27867;&#35828;&#27861;&#24050;&#32463;&#23384;&#22312;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#38024;&#23545;&#35745;&#21010;&#30340;&#24773;&#20917;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39564;&#35777;/&#33258;&#25105;&#25209;&#35780;&#33021;&#21147;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#19968;&#20010;&#20351;&#29992;LLMs&#36827;&#34892;&#35745;&#21010;&#29983;&#25104;&#21644;&#39564;&#35777;&#30340;&#35745;&#21010;&#31995;&#32479;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#39564;&#35777;&#22120;LLM&#22312;&#19982;&#22522;&#20934;&#39564;&#35777;&#30340;&#24615;&#33021;&#12289;&#33258;&#25105;&#25209;&#35780;&#23545;&#35745;&#21010;&#29983;&#25104;&#30340;&#24433;&#21709;&#20197;&#21450;&#31995;&#32479;&#24615;&#33021;&#20013;&#21453;&#39304;&#27700;&#24179;&#21464;&#21270;&#30340;&#24433;&#21709;&#12290;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;LLM GPT-4&#36827;&#34892;&#29983;&#25104;&#21644;&#39564;&#35777;&#65292;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#33258;&#25105;&#25209;&#35780;&#20284;&#20046;&#20250;&#38477;&#20302;&#35745;&#21010;&#29983;&#25104;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#19982;&#20855;&#26377;&#22806;&#37096;&#12289;&#21487;&#38752;&#39564;&#35777;&#22120;&#30340;&#31995;&#32479;&#30456;&#27604;&#65292;LLM&#39564;&#35777;&#22120;&#22312;&#35813;&#31995;&#32479;&#20013;&#20135;&#29983;&#20102;&#22823;&#37327;&#30340;&#35823;&#25253;&#65292;&#20174;&#32780;&#24433;&#21709;&#20102;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
There have been widespread claims about Large Language Models (LLMs) being able to successfully verify or self-critique their candidate solutions in reasoning problems in an iterative mode. Intrigued by those claims, in this paper we set out to investigate the verification/self-critiquing abilities of large language models in the context of planning. We evaluate a planning system that employs LLMs for both plan generation and verification. We assess the verifier LLM's performance against ground-truth verification, the impact of self-critiquing on plan generation, and the influence of varying feedback levels on system performance. Using GPT-4, a state-of-the-art LLM, for both generation and verification, our findings reveal that self-critiquing appears to diminish plan generation performance, especially when compared to systems with external, sound verifiers and the LLM verifiers in that system produce a notable number of false positives, compromising the system's reliability. Additiona
&lt;/p&gt;</description></item><item><title>DUSA&#26159;&#19968;&#31181;&#35299;&#32806;&#26080;&#30417;&#30563;&#30340;&#36710;&#32852;&#32593;&#21327;&#20316;&#24863;&#30693;&#30340;&#27169;&#25311;&#21040;&#23454;&#38469;&#36866;&#24212;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#27169;&#25311;&#25968;&#25454;&#19982;&#30495;&#23454;&#25968;&#25454;&#20043;&#38388;&#30340;&#39046;&#22495;&#24046;&#36317;&#20197;&#21450;&#21327;&#20316;&#20195;&#29702;&#20043;&#38388;&#30340;&#39046;&#22495;&#24046;&#36317;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#36710;&#32852;&#32593;&#21327;&#20316;&#24863;&#30693;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.08117</link><description>&lt;p&gt;
DUSA: &#35299;&#32806;&#26080;&#30417;&#30563;&#30340;&#36710;&#32852;&#32593;&#21327;&#20316;&#24863;&#30693;&#30340;&#27169;&#25311;&#21040;&#23454;&#38469;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
DUSA: Decoupled Unsupervised Sim2Real Adaptation for Vehicle-to-Everything Collaborative Perception. (arXiv:2310.08117v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08117
&lt;/p&gt;
&lt;p&gt;
DUSA&#26159;&#19968;&#31181;&#35299;&#32806;&#26080;&#30417;&#30563;&#30340;&#36710;&#32852;&#32593;&#21327;&#20316;&#24863;&#30693;&#30340;&#27169;&#25311;&#21040;&#23454;&#38469;&#36866;&#24212;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#27169;&#25311;&#25968;&#25454;&#19982;&#30495;&#23454;&#25968;&#25454;&#20043;&#38388;&#30340;&#39046;&#22495;&#24046;&#36317;&#20197;&#21450;&#21327;&#20316;&#20195;&#29702;&#20043;&#38388;&#30340;&#39046;&#22495;&#24046;&#36317;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#36710;&#32852;&#32593;&#21327;&#20316;&#24863;&#30693;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36710;&#32852;&#32593;&#21327;&#20316;&#24863;&#30693;&#23545;&#20110;&#33258;&#21160;&#39550;&#39542;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#23454;&#29616;&#39640;&#31934;&#24230;&#30340;&#36710;&#32852;&#32593;&#21327;&#20316;&#24863;&#30693;&#38656;&#35201;&#22823;&#37327;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#36827;&#34892;&#27880;&#37322;&#65292;&#36825;&#24448;&#24448;&#26159;&#26114;&#36149;&#19988;&#38590;&#20197;&#33719;&#21462;&#30340;&#12290;&#27169;&#25311;&#25968;&#25454;&#22240;&#20854;&#21487;&#20197;&#20197;&#26497;&#20302;&#30340;&#25104;&#26412;&#22823;&#37327;&#29983;&#20135;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#27169;&#25311;&#25968;&#25454;&#19982;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#30340;&#39046;&#22495;&#24046;&#36317;&#65292;&#21253;&#25324;&#20256;&#24863;&#22120;&#31867;&#22411;&#12289;&#21453;&#23556;&#27169;&#24335;&#21644;&#36947;&#36335;&#29615;&#22659;&#30340;&#24046;&#24322;&#65292;&#23548;&#33268;&#22312;&#27169;&#25311;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#19978;&#35780;&#20272;&#26102;&#24615;&#33021;&#36739;&#24046;&#12290;&#27492;&#22806;&#65292;&#30495;&#23454;&#19990;&#30028;&#21327;&#20316;&#20195;&#29702;&#20043;&#38388;&#20173;&#23384;&#22312;&#39046;&#22495;&#24046;&#36317;&#65292;&#20363;&#22914;&#19981;&#21516;&#31867;&#22411;&#30340;&#20256;&#24863;&#22120;&#21487;&#33021;&#23433;&#35013;&#22312;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#21644;&#36335;&#36793;&#22522;&#30784;&#35774;&#26045;&#19978;&#65292;&#19988;&#20855;&#26377;&#19981;&#21516;&#30340;&#22806;&#37096;&#21442;&#25968;&#65292;&#36827;&#19968;&#27493;&#22686;&#21152;&#20102;&#27169;&#25311;&#21040;&#23454;&#38469;&#30340;&#27867;&#21270;&#38590;&#24230;&#12290;&#20026;&#20805;&#20998;&#21033;&#29992;&#27169;&#25311;&#25968;&#25454;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#27169;&#25311;&#21040;&#23454;&#38469;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vehicle-to-Everything (V2X) collaborative perception is crucial for autonomous driving. However, achieving high-precision V2X perception requires a significant amount of annotated real-world data, which can always be expensive and hard to acquire. Simulated data have raised much attention since they can be massively produced at an extremely low cost. Nevertheless, the significant domain gap between simulated and real-world data, including differences in sensor type, reflectance patterns, and road surroundings, often leads to poor performance of models trained on simulated data when evaluated on real-world data. In addition, there remains a domain gap between real-world collaborative agents, e.g. different types of sensors may be installed on autonomous vehicles and roadside infrastructures with different extrinsics, further increasing the difficulty of sim2real generalization. To take full advantage of simulated data, we present a new unsupervised sim2real domain adaptation method for 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;Promptor&#65292;&#19968;&#20010;&#29992;&#20110;&#26234;&#33021;&#25991;&#26412;&#36755;&#20837;&#25216;&#26415;&#30340;&#23545;&#35805;&#24335;&#33258;&#20027;&#25552;&#31034;&#29983;&#25104;&#20195;&#29702;&#12290;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#21487;&#20197;&#20811;&#26381;&#25968;&#25454;&#25910;&#38598;&#21644;&#27169;&#22411;&#24494;&#35843;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#36890;&#36807;&#20197;GPT-3.5&#20026;&#20363;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#20165;&#36890;&#36807;&#25552;&#31034;&#21363;&#21487;&#36229;&#36807;GPT-2&#25903;&#25345;&#30340;&#31995;&#32479;&#65292;&#24182;&#19988;&#21487;&#19982;&#32463;&#36807;&#31934;&#35843;&#30340;GPT-3.5&#27169;&#22411;&#30456;&#23218;&#32654;&#12290;</title><link>http://arxiv.org/abs/2310.08101</link><description>&lt;p&gt;
Promptor:&#19968;&#31181;&#29992;&#20110;&#26234;&#33021;&#25991;&#26412;&#36755;&#20837;&#25216;&#26415;&#30340;&#23545;&#35805;&#24335;&#33258;&#20027;&#25552;&#31034;&#29983;&#25104;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Promptor: A Conversational and Autonomous Prompt Generation Agent for Intelligent Text Entry Techniques. (arXiv:2310.08101v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08101
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;Promptor&#65292;&#19968;&#20010;&#29992;&#20110;&#26234;&#33021;&#25991;&#26412;&#36755;&#20837;&#25216;&#26415;&#30340;&#23545;&#35805;&#24335;&#33258;&#20027;&#25552;&#31034;&#29983;&#25104;&#20195;&#29702;&#12290;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#21487;&#20197;&#20811;&#26381;&#25968;&#25454;&#25910;&#38598;&#21644;&#27169;&#22411;&#24494;&#35843;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#36890;&#36807;&#20197;GPT-3.5&#20026;&#20363;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#20165;&#36890;&#36807;&#25552;&#31034;&#21363;&#21487;&#36229;&#36807;GPT-2&#25903;&#25345;&#30340;&#31995;&#32479;&#65292;&#24182;&#19988;&#21487;&#19982;&#32463;&#36807;&#31934;&#35843;&#30340;GPT-3.5&#27169;&#22411;&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25105;&#20204;&#26085;&#24120;&#25968;&#23383;&#20132;&#20114;&#20013;&#65292;&#25991;&#26412;&#36755;&#20837;&#26159;&#19968;&#39033;&#37325;&#35201;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#20351;&#25991;&#26412;&#36755;&#20837;&#26356;&#26377;&#25928;&#12289;&#39640;&#25928;&#21644;&#27969;&#30021;&#65292;&#24050;&#32463;&#24320;&#21457;&#20986;&#20102;&#35768;&#22810;&#26234;&#33021;&#21151;&#33021;&#65292;&#21253;&#25324;&#21477;&#23376;&#39044;&#27979;&#21644;&#29992;&#25143;&#20010;&#24615;&#21270;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35821;&#35328;&#27169;&#22411;&#25104;&#20026;&#36825;&#20123;&#39640;&#32423;&#21151;&#33021;&#30340;&#24120;&#35268;&#65292;&#25968;&#25454;&#25910;&#38598;&#21644;&#27169;&#22411;&#24494;&#35843;&#30340;&#24517;&#35201;&#24615;&#20063;&#22686;&#21152;&#20102;&#12290;&#21033;&#29992;GPT-3.5&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#21487;&#20197;&#20943;&#36731;&#36825;&#20123;&#25361;&#25112;&#12290;&#36825;&#19968;&#29420;&#29305;&#30340;&#29305;&#24615;&#20801;&#35768;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#25552;&#31034;&#33719;&#24471;&#26032;&#30340;&#25216;&#33021;&#65292;&#28040;&#38500;&#20102;&#25968;&#25454;&#25910;&#38598;&#21644;&#24494;&#35843;&#30340;&#38656;&#35201;&#12290;&#22240;&#27492;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#21508;&#31181;&#25991;&#26412;&#39044;&#27979;&#25216;&#26415;&#12290;&#25105;&#20204;&#26368;&#21021;&#23637;&#31034;&#20102;&#20165;&#36890;&#36807;&#25552;&#31034;GPT-3.5&#21363;&#21487;&#36229;&#36807;GPT-2&#25903;&#25345;&#30340;&#31995;&#32479;&#65292;&#24182;&#19988;&#19982;&#32463;&#36807;&#31934;&#35843;&#30340;GPT-3.5&#27169;&#22411;&#30456;&#24403;&#65292;&#22312;&#21518;&#20004;&#31181;&#26041;&#27861;&#38656;&#35201;&#26114;&#36149;&#30340;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text entry is an essential task in our day-to-day digital interactions. Numerous intelligent features have been developed to streamline this process, making text entry more effective, efficient, and fluid. These improvements include sentence prediction and user personalization. However, as deep learning-based language models become the norm for these advanced features, the necessity for data collection and model fine-tuning increases. These challenges can be mitigated by harnessing the in-context learning capability of large language models such as GPT-3.5. This unique feature allows the language model to acquire new skills through prompts, eliminating the need for data collection and fine-tuning. Consequently, large language models can learn various text prediction techniques. We initially showed that, for a sentence prediction task, merely prompting GPT-3.5 surpassed a GPT-2 backed system and is comparable with a fine-tuned GPT-3.5 model, with the latter two methods requiring costly 
&lt;/p&gt;</description></item><item><title>Sentinel&#26159;&#19968;&#31181;&#29992;&#20110;&#20445;&#25252;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#30340;&#38450;&#24481;&#31574;&#30053;&#65292;&#36890;&#36807;&#21033;&#29992;&#26412;&#22320;&#25968;&#25454;&#24182;&#23450;&#20041;&#19968;&#20010;&#19977;&#27493;&#32858;&#21512;&#21327;&#35758;&#26469;&#23545;&#25239;&#27745;&#26579;&#25915;&#20987;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;Sentinel&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#19979;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2310.08097</link><description>&lt;p&gt;
Sentinel: &#19968;&#31181;&#29992;&#20110;&#20445;&#25252;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#30340;&#32858;&#21512;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Sentinel: An Aggregation Function to Secure Decentralized Federated Learning. (arXiv:2310.08097v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08097
&lt;/p&gt;
&lt;p&gt;
Sentinel&#26159;&#19968;&#31181;&#29992;&#20110;&#20445;&#25252;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#30340;&#38450;&#24481;&#31574;&#30053;&#65292;&#36890;&#36807;&#21033;&#29992;&#26412;&#22320;&#25968;&#25454;&#24182;&#23450;&#20041;&#19968;&#20010;&#19977;&#27493;&#32858;&#21512;&#21327;&#35758;&#26469;&#23545;&#25239;&#27745;&#26579;&#25915;&#20987;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;Sentinel&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#19979;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#24555;&#36895;&#25972;&#21512;&#21040;&#32593;&#32476;&#20013;&#28085;&#30422;&#20102;&#32593;&#32476;&#31649;&#29702;&#12289;&#26381;&#21153;&#36136;&#37327;&#21644;&#32593;&#32476;&#23433;&#20840;&#31561;&#21508;&#20010;&#26041;&#38754;&#65292;&#21516;&#26102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#65288;DFL&#65289;&#20316;&#20026;&#19968;&#31181;&#21019;&#26032;&#33539;&#24335;&#65292;&#29992;&#20110;&#35757;&#32451;&#21327;&#20316;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#21333;&#28857;&#22833;&#25928;&#30340;&#38480;&#21046;&#12290;&#28982;&#32780;&#65292;FL&#21644;DFL&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#20449;&#24615;&#21463;&#21040;&#27745;&#26579;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#23545;&#20854;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#29616;&#26377;&#30340;&#38450;&#24481;&#26426;&#21046;&#38024;&#23545;&#38598;&#20013;&#24335;FL&#36827;&#34892;&#35774;&#35745;&#65292;&#24182;&#26410;&#20805;&#20998;&#21033;&#29992;DFL&#30340;&#29305;&#28857;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;Sentinel&#65292;&#19968;&#31181;&#22312;DFL&#20013;&#23545;&#25239;&#27745;&#26579;&#25915;&#20987;&#30340;&#38450;&#24481;&#31574;&#30053;&#12290;Sentinel&#21033;&#29992;&#26412;&#22320;&#25968;&#25454;&#30340;&#21487;&#35775;&#38382;&#24615;&#65292;&#23450;&#20041;&#20102;&#19968;&#20010;&#19977;&#27493;&#32858;&#21512;&#21327;&#35758;&#65292;&#21253;&#25324;&#30456;&#20284;&#24615;&#36807;&#28388;&#12289;&#24341;&#23548;&#39564;&#35777;&#21644;&#26631;&#20934;&#21270;&#65292;&#20197;&#38450;&#27490;&#24694;&#24847;&#27169;&#22411;&#26356;&#26032;&#12290;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#25968;&#25454;&#38598;&#21644;&#19981;&#21516;&#30340;&#35780;&#20272;&#25351;&#26631;&#23545;Sentinel&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid integration of Federated Learning (FL) into networking encompasses various aspects such as network management, quality of service, and cybersecurity while preserving data privacy. In this context, Decentralized Federated Learning (DFL) emerges as an innovative paradigm to train collaborative models, addressing the single point of failure limitation. However, the security and trustworthiness of FL and DFL are compromised by poisoning attacks, negatively impacting its performance. Existing defense mechanisms have been designed for centralized FL and they do not adequately exploit the particularities of DFL. Thus, this work introduces Sentinel, a defense strategy to counteract poisoning attacks in DFL. Sentinel leverages the accessibility of local data and defines a three-step aggregation protocol consisting of similarity filtering, bootstrap validation, and normalization to safeguard against malicious model updates. Sentinel has been evaluated with diverse datasets and various 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20998;&#36776;TD&#23398;&#20064;(DTD)&#30340;&#26032;&#22411;TD&#31639;&#27861;&#65292;&#36890;&#36807;&#28789;&#27963;&#30340;&#24378;&#35843;&#20989;&#25968;&#26469;&#20998;&#37197;&#36164;&#28304;&#65292;&#20197;&#25913;&#21892;&#29366;&#24577;&#20043;&#38388;&#30340;&#36731;&#37325;&#20998;&#37197;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;TD&#23398;&#20064;&#20013;&#24573;&#35270;&#21382;&#21490;&#29366;&#24577;&#37325;&#35201;&#24615;&#21644;TD&#35823;&#24046;&#20256;&#25773;&#30340;&#38382;&#39064;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#21512;&#29702;&#30340;&#24378;&#35843;&#20989;&#25968;&#19981;&#20165;&#25913;&#36827;&#20102;&#20540;&#20272;&#35745;&#65292;&#36824;&#21152;&#36895;&#20102;&#23398;&#20064;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2310.08091</link><description>&lt;p&gt;
&#20998;&#36776;&#26102;&#38388;&#24046;&#20998;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Discerning Temporal Difference Learning. (arXiv:2310.08091v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08091
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20998;&#36776;TD&#23398;&#20064;(DTD)&#30340;&#26032;&#22411;TD&#31639;&#27861;&#65292;&#36890;&#36807;&#28789;&#27963;&#30340;&#24378;&#35843;&#20989;&#25968;&#26469;&#20998;&#37197;&#36164;&#28304;&#65292;&#20197;&#25913;&#21892;&#29366;&#24577;&#20043;&#38388;&#30340;&#36731;&#37325;&#20998;&#37197;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;TD&#23398;&#20064;&#20013;&#24573;&#35270;&#21382;&#21490;&#29366;&#24577;&#37325;&#35201;&#24615;&#21644;TD&#35823;&#24046;&#20256;&#25773;&#30340;&#38382;&#39064;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#21512;&#29702;&#30340;&#24378;&#35843;&#20989;&#25968;&#19981;&#20165;&#25913;&#36827;&#20102;&#20540;&#20272;&#35745;&#65292;&#36824;&#21152;&#36895;&#20102;&#23398;&#20064;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24046;&#20998;&#23398;&#20064;(TD)&#26159;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#22522;&#26412;&#27010;&#24565;&#65292;&#26088;&#22312;&#39640;&#25928;&#35780;&#20272;&#31574;&#30053;&#30340;&#20215;&#20540;&#20989;&#25968;&#12290;TD($\lambda$)&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#21464;&#20307;&#65292;&#36890;&#36807;&#24341;&#20837;&#35760;&#24518;&#36712;&#36857;&#23558;&#39044;&#27979;&#35823;&#24046;&#20998;&#25955;&#21040;&#21382;&#21490;&#19978;&#19979;&#25991;&#20013;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#32463;&#24120;&#24573;&#35270;&#21382;&#21490;&#29366;&#24577;&#30340;&#37325;&#35201;&#24615;&#20197;&#21450;&#20256;&#25773;TD&#35823;&#24046;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#65292;&#36825;&#21463;&#21040;&#35775;&#38382;&#22833;&#34913;&#25110;&#32467;&#26524;&#22122;&#22768;&#31561;&#25361;&#25112;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20998;&#36776;TD&#23398;&#20064;(DTD)&#30340;&#26032;&#22411;TD&#31639;&#27861;&#65292;&#23427;&#20801;&#35768;&#28789;&#27963;&#30340;&#24378;&#35843;&#20989;&#25968;-&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#39044;&#20808;&#30830;&#23450;&#25110;&#33258;&#36866;&#24212;&#22320;&#20998;&#37197;&#36164;&#28304;&#20197;&#25552;&#39640;&#29366;&#24577;&#20043;&#38388;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#22312;&#29305;&#23450;&#31867;&#21035;&#30340;&#24378;&#35843;&#20989;&#25968;&#20869;&#24314;&#31435;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#36136;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#28145;&#24230;RL&#29615;&#22659;&#20013;&#30340;&#28508;&#22312;&#24212;&#29992;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#21512;&#29702;&#30340;&#24378;&#35843;&#20989;&#25968;&#19981;&#20165;&#21487;&#20197;&#25913;&#36827;&#20540;&#20272;&#35745;&#65292;&#36824;&#21487;&#20197;&#21152;&#36895;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal difference learning (TD) is a foundational concept in reinforcement learning (RL), aimed at efficiently assessing a policy's value function. TD($\lambda$), a potent variant, incorporates a memory trace to distribute the prediction error into the historical context. However, this approach often neglects the significance of historical states and the relative importance of propagating the TD error, influenced by challenges such as visitation imbalance or outcome noise. To address this, we propose a novel TD algorithm named discerning TD learning (DTD), which allows flexible emphasis functions$-$predetermined or adapted during training$-$to allocate efforts effectively across states. We establish the convergence properties of our method within a specific class of emphasis functions and showcase its promising potential for adaptation to deep RL contexts. Empirical results underscore that employing a judicious emphasis function not only improves value estimation but also expedites l
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#20197;&#38382;&#31572;&#26041;&#24335;&#32763;&#35793;&#21360;&#23612;&#35821;&#20302;&#36164;&#28304;&#26631;&#39064;&#20826;&#22788;&#29702;&#30340;&#20219;&#21153;&#65292;&#36129;&#29486;&#21253;&#25324;&#26500;&#24314;&#25163;&#21160;&#26631;&#27880;&#30340;&#21360;&#23612;&#35821;&#26631;&#39064;&#20826;&#22788;&#29702;&#35821;&#26009;&#24211;&#65292;&#24182;&#35780;&#20272;&#20102;&#36328;&#35821;&#35328;&#38646;&#23556;&#20987;&#38382;&#31572;&#27169;&#22411;&#30340;&#24212;&#29992;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;XLM-RoBERTa&#65288;&#22823;&#65289;&#27169;&#22411;&#22312;&#30701;&#35821;&#21644;&#27573;&#33853;&#26631;&#39064;&#20826;&#22788;&#29702;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#65292;&#32780;mDeBERTa&#65288;&#22522;&#30784;&#65289;&#27169;&#22411;&#22312;&#22810;&#37096;&#20998;&#26631;&#39064;&#20826;&#22788;&#29702;&#20013;&#34920;&#29616;&#26368;&#20339;&#12290;</title><link>http://arxiv.org/abs/2310.08085</link><description>&lt;p&gt;
&#20197;&#38382;&#31572;&#26041;&#24335;&#32763;&#35793;&#21360;&#23612;&#35821;&#30340;&#20302;&#36164;&#28304;&#26631;&#39064;&#20826;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Low-Resource Clickbait Spoiling for Indonesian via Question Answering. (arXiv:2310.08085v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08085
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#20197;&#38382;&#31572;&#26041;&#24335;&#32763;&#35793;&#21360;&#23612;&#35821;&#20302;&#36164;&#28304;&#26631;&#39064;&#20826;&#22788;&#29702;&#30340;&#20219;&#21153;&#65292;&#36129;&#29486;&#21253;&#25324;&#26500;&#24314;&#25163;&#21160;&#26631;&#27880;&#30340;&#21360;&#23612;&#35821;&#26631;&#39064;&#20826;&#22788;&#29702;&#35821;&#26009;&#24211;&#65292;&#24182;&#35780;&#20272;&#20102;&#36328;&#35821;&#35328;&#38646;&#23556;&#20987;&#38382;&#31572;&#27169;&#22411;&#30340;&#24212;&#29992;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;XLM-RoBERTa&#65288;&#22823;&#65289;&#27169;&#22411;&#22312;&#30701;&#35821;&#21644;&#27573;&#33853;&#26631;&#39064;&#20826;&#22788;&#29702;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#65292;&#32780;mDeBERTa&#65288;&#22522;&#30784;&#65289;&#27169;&#22411;&#22312;&#22810;&#37096;&#20998;&#26631;&#39064;&#20826;&#22788;&#29702;&#20013;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#39064;&#20826;&#22788;&#29702;&#26088;&#22312;&#29983;&#25104;&#19968;&#20010;&#30701;&#25991;&#26412;&#65292;&#20197;&#28385;&#36275;&#26631;&#39064;&#20826;&#24086;&#23376;&#24341;&#36215;&#30340;&#22909;&#22855;&#24515;&#12290;&#30001;&#20110;&#36825;&#26159;&#19968;&#20010;&#26032;&#24341;&#20837;&#30340;&#20219;&#21153;&#65292;&#30446;&#21069;&#21482;&#26377;&#33521;&#25991;&#25968;&#25454;&#38598;&#21487;&#29992;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#21253;&#25324;&#22312;&#21360;&#23612;&#35821;&#20013;&#26500;&#24314;&#25163;&#21160;&#26631;&#27880;&#30340;&#26631;&#39064;&#20826;&#22788;&#29702;&#35821;&#26009;&#24211;&#65292;&#24182;&#35780;&#20272;&#20351;&#29992;&#36328;&#35821;&#35328;&#38646;&#23556;&#20987;&#38382;&#31572;&#27169;&#22411;&#26469;&#22788;&#29702;&#21360;&#23612;&#35821;&#31561;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#26631;&#39064;&#20826;&#22788;&#29702;&#12290;&#25105;&#20204;&#21033;&#29992;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#30340;&#36873;&#25321;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;XLM-RoBERTa&#65288;&#22823;&#65289;&#27169;&#22411;&#22312;&#30701;&#35821;&#21644;&#27573;&#33853;&#26631;&#39064;&#20826;&#22788;&#29702;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#65292;&#32780;mDeBERTa&#65288;&#22522;&#30784;&#65289;&#27169;&#22411;&#22312;&#22810;&#37096;&#20998;&#26631;&#39064;&#20826;&#22788;&#29702;&#20013;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clickbait spoiling aims to generate a short text to satisfy the curiosity induced by a clickbait post. As it is a newly introduced task, the dataset is only available in English so far. Our contributions include the construction of manually labeled clickbait spoiling corpus in Indonesian and an evaluation on using cross-lingual zero-shot question answering-based models to tackle clikcbait spoiling for low-resource language like Indonesian. We utilize selection of multilingual language models. The experimental results suggest that XLM-RoBERTa (large) model outperforms other models for phrase and passage spoilers, meanwhile, mDeBERTa (base) model outperforms other models for multipart spoilers.
&lt;/p&gt;</description></item><item><title>GameGPT&#26159;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#26694;&#26550;&#65292;&#29992;&#20110;&#33258;&#21160;&#21270;&#28216;&#25103;&#24320;&#21457;&#12290;&#23427;&#36890;&#36807;&#20351;&#29992;&#21452;&#37325;&#21327;&#20316;&#21644;&#20998;&#23618;&#26041;&#27861;&#65292;&#32467;&#21512;&#22810;&#20010;&#20869;&#37096;&#35789;&#24211;&#21644;&#35299;&#32806;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#28216;&#25103;&#24320;&#21457;&#20013;&#30340;&#24187;&#35273;&#21644;&#20887;&#20313;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.08067</link><description>&lt;p&gt;
GameGPT: &#28216;&#25103;&#24320;&#21457;&#30340;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
GameGPT: Multi-agent Collaborative Framework for Game Development. (arXiv:2310.08067v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08067
&lt;/p&gt;
&lt;p&gt;
GameGPT&#26159;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#26694;&#26550;&#65292;&#29992;&#20110;&#33258;&#21160;&#21270;&#28216;&#25103;&#24320;&#21457;&#12290;&#23427;&#36890;&#36807;&#20351;&#29992;&#21452;&#37325;&#21327;&#20316;&#21644;&#20998;&#23618;&#26041;&#27861;&#65292;&#32467;&#21512;&#22810;&#20010;&#20869;&#37096;&#35789;&#24211;&#21644;&#35299;&#32806;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#28216;&#25103;&#24320;&#21457;&#20013;&#30340;&#24187;&#35273;&#21644;&#20887;&#20313;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#26234;&#33021;&#20307;&#23637;&#31034;&#20102;&#23427;&#20204;&#33258;&#21160;&#21270;&#21644;&#21152;&#36895;&#36719;&#20214;&#24320;&#21457;&#36807;&#31243;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#38024;&#23545;&#28216;&#25103;&#24320;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GameGPT&#30340;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#26694;&#26550;&#65292;&#29992;&#20110;&#33258;&#21160;&#21270;&#28216;&#25103;&#24320;&#21457;&#12290;&#23613;&#31649;&#35768;&#22810;&#30740;&#31350;&#24050;&#32463;&#25351;&#20986;&#24187;&#35273;&#26159;&#37096;&#32626;LLM&#22312;&#29983;&#20135;&#20013;&#30340;&#20027;&#35201;&#38556;&#30861;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;&#36824;&#23384;&#22312;&#21478;&#19968;&#20010;&#38382;&#39064;&#65306;&#20887;&#20313;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#26041;&#27861;&#26469;&#32531;&#35299;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;&#36825;&#20123;&#26041;&#27861;&#21253;&#25324;&#21452;&#37325;&#21327;&#20316;&#21644;&#20998;&#23618;&#26041;&#27861;&#65292;&#32467;&#21512;&#22810;&#20010;&#20869;&#37096;&#35789;&#24211;&#65292;&#20197;&#38477;&#20302;&#35268;&#21010;&#12289;&#20219;&#21153;&#35782;&#21035;&#21644;&#23454;&#26045;&#38454;&#27573;&#30340;&#24187;&#35273;&#21644;&#20887;&#20313;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#35299;&#32806;&#26041;&#27861;&#65292;&#20197;&#26356;&#22909;&#22320;&#23454;&#29616;&#20195;&#30721;&#29983;&#25104;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The large language model (LLM) based agents have demonstrated their capacity to automate and expedite software development processes. In this paper, we focus on game development and propose a multi-agent collaborative framework, dubbed GameGPT, to automate game development. While many studies have pinpointed hallucination as a primary roadblock for deploying LLMs in production, we identify another concern: redundancy. Our framework presents a series of methods to mitigate both concerns. These methods include dual collaboration and layered approaches with several in-house lexicons, to mitigate the hallucination and redundancy in the planning, task identification, and implementation phases. Furthermore, a decoupling approach is also introduced to achieve code generation with better precision.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25628;&#32034;&#21644;&#28151;&#21512;&#33539;&#24335;&#30340;&#33258;&#21160;&#26041;&#27861;&#65292;&#29992;&#20110;&#36817;&#20284;&#35745;&#31639;&#20108;&#20154;&#21338;&#24328;&#20013;&#30340;&#32435;&#20160;&#22343;&#34913;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#23436;&#20840;&#33258;&#21160;&#21270;&#35774;&#35745;&#21644;&#20998;&#26512;&#28151;&#21512;&#38454;&#27573;&#65292;&#24182;&#19988;&#33021;&#22815;&#23545;&#24050;&#32463;&#23384;&#22312;&#30340;&#31639;&#27861;&#36827;&#34892;&#36817;&#20284;&#30028;&#38480;&#30340;&#20998;&#26512;&#65292;&#26080;&#38656;&#25163;&#20889;&#35777;&#26126;&#12290;</title><link>http://arxiv.org/abs/2310.08066</link><description>&lt;p&gt;
&#22312;&#36817;&#20284;&#32435;&#20160;&#22343;&#34913;&#31639;&#27861;&#20013;&#30340;&#25628;&#32034;&#19982;&#28151;&#21512;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
The Search-and-Mix Paradigm in Approximate Nash Equilibrium Algorithms. (arXiv:2310.08066v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08066
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25628;&#32034;&#21644;&#28151;&#21512;&#33539;&#24335;&#30340;&#33258;&#21160;&#26041;&#27861;&#65292;&#29992;&#20110;&#36817;&#20284;&#35745;&#31639;&#20108;&#20154;&#21338;&#24328;&#20013;&#30340;&#32435;&#20160;&#22343;&#34913;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#23436;&#20840;&#33258;&#21160;&#21270;&#35774;&#35745;&#21644;&#20998;&#26512;&#28151;&#21512;&#38454;&#27573;&#65292;&#24182;&#19988;&#33021;&#22815;&#23545;&#24050;&#32463;&#23384;&#22312;&#30340;&#31639;&#27861;&#36827;&#34892;&#36817;&#20284;&#30028;&#38480;&#30340;&#20998;&#26512;&#65292;&#26080;&#38656;&#25163;&#20889;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23398;&#20013;&#30340;&#20154;&#24037;&#26234;&#33021;&#20197;&#19968;&#31181;&#26500;&#36896;&#24615;&#30340;&#26041;&#24335;&#22788;&#29702;&#25968;&#23398;&#38382;&#39064;&#65292;&#20351;&#24471;&#25512;&#29702;&#33258;&#21160;&#21270;&#12289;&#20943;&#23569;&#21171;&#21160;&#37327;&#21644;&#20943;&#23569;&#38169;&#35823;&#12290;&#23545;&#20110;&#31639;&#27861;&#26469;&#35828;&#65292;&#38382;&#39064;&#21464;&#25104;&#20102;&#22914;&#20309;&#33258;&#21160;&#21270;&#20998;&#26512;&#29305;&#23450;&#38382;&#39064;&#12290;&#26412;&#25991;&#39318;&#27425;&#25552;&#20379;&#20102;&#19968;&#31181;&#33258;&#21160;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#29702;&#35770;&#35745;&#31639;&#26426;&#31185;&#23398;&#20013;&#30740;&#31350;&#30340;&#19968;&#20010;&#32463;&#20856;&#38382;&#39064;&#19978;&#36827;&#34892;&#36817;&#20284;&#20998;&#26512;&#65306;&#35745;&#31639;&#20108;&#20154;&#21338;&#24328;&#20013;&#30340;&#36817;&#20284;&#32435;&#20160;&#22343;&#34913;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#36825;&#26679;&#30340;&#31639;&#27861;&#21487;&#20197;&#37325;&#20889;&#20026;&#19968;&#20010;&#25628;&#32034;&#21644;&#28151;&#21512;&#30340;&#33539;&#24335;&#65292;&#20854;&#20013;&#21253;&#25324;&#25628;&#32034;&#38454;&#27573;&#21644;&#28151;&#21512;&#38454;&#27573;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#33021;&#22815;&#23436;&#20840;&#33258;&#21160;&#21270;&#35774;&#35745;&#21644;&#20998;&#26512;&#28151;&#21512;&#38454;&#27573;&#30340;&#36807;&#31243;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#25105;&#20204;&#30340;&#26041;&#27861;&#26469;&#20998;&#26512;&#25991;&#29486;&#20013;&#25152;&#26377;&#31639;&#27861;&#30340;&#36817;&#20284;&#30028;&#38480;&#12290;&#21516;&#26679;&#30340;&#36817;&#20284;&#30028;&#38480;&#21487;&#20197;&#22312;&#27809;&#26377;&#20219;&#20309;&#25163;&#20889;&#35777;&#26126;&#30340;&#24773;&#20917;&#19979;&#35745;&#31639;&#20986;&#26469;&#12290;&#25105;&#20204;&#30340;&#33258;&#21160;&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#20110;&#36817;&#20284;&#32435;&#20160;&#22343;&#34913;&#20013;&#30340;&#32447;&#24615;&#26494;&#24347;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
AI in Math deals with mathematics in a constructive manner so that reasoning becomes automated, less laborious, and less error-prone. For algorithms, the question becomes how to automate analyses for specific problems. For the first time, this work provides an automatic method for approximation analysis on a well-studied problem in theoretical computer science: computing approximate Nash equilibria in two-player games. We observe that such algorithms can be reformulated into a search-and-mix paradigm, which involves a search phase followed by a mixing phase. By doing so, we are able to fully automate the procedure of designing and analyzing the mixing phase. For example, we illustrate how to perform our method with a program to analyze the approximation bounds of all the algorithms in the literature. Same approximation bounds are computed without any hand-written proof. Our automatic method heavily relies on the LP-relaxation structure in approximate Nash equilibria. Since many approxi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#26631;&#31614;&#27604;&#20363;&#20013;&#23398;&#20064;&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#36890;&#36807;&#20266;&#26631;&#31614;&#21270;&#21644;&#23884;&#20837;&#32454;&#21270;&#20004;&#20010;&#27493;&#39588;&#36845;&#20195;&#22320;&#25552;&#39640;&#26377;&#30417;&#30563;&#23398;&#20064;&#22120;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.08056</link><description>&lt;p&gt;
&#20174;&#26631;&#31614;&#27604;&#20363;&#20013;&#23398;&#20064;&#65306;&#36890;&#36807;&#20449;&#24565;&#20256;&#25773;&#23545;&#26377;&#30417;&#30563;&#23398;&#20064;&#22120;&#36827;&#34892;&#24341;&#23548;
&lt;/p&gt;
&lt;p&gt;
Learning from Label Proportions: Bootstrapping Supervised Learners via Belief Propagation. (arXiv:2310.08056v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08056
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#26631;&#31614;&#27604;&#20363;&#20013;&#23398;&#20064;&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#36890;&#36807;&#20266;&#26631;&#31614;&#21270;&#21644;&#23884;&#20837;&#32454;&#21270;&#20004;&#20010;&#27493;&#39588;&#36845;&#20195;&#22320;&#25552;&#39640;&#26377;&#30417;&#30563;&#23398;&#20064;&#22120;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#31614;&#27604;&#20363;&#23398;&#20064;&#65288;LLP&#65289;&#26159;&#19968;&#20010;&#23398;&#20064;&#38382;&#39064;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#21482;&#26377;&#38024;&#23545;&#19968;&#32452;&#23454;&#20363;&#65288;&#31216;&#20026;&#21253;&#65289;&#30340;&#32858;&#21512;&#32423;&#21035;&#26631;&#31614;&#21487;&#29992;&#65292;&#24182;&#19988;&#30446;&#30340;&#26159;&#22312;&#27979;&#35797;&#25968;&#25454;&#30340;&#23454;&#20363;&#32423;&#21035;&#19978;&#33719;&#24471;&#26368;&#20339;&#24615;&#33021;&#12290;&#36825;&#31181;&#35774;&#32622;&#22312;&#24191;&#21578;&#21644;&#21307;&#23398;&#31561;&#39046;&#22495;&#30001;&#20110;&#38544;&#31169;&#32771;&#34385;&#32780;&#20986;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#31639;&#27861;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#23427;&#36890;&#36807;&#20004;&#20010;&#20027;&#35201;&#27493;&#39588;&#36827;&#34892;&#36845;&#20195;&#12290;&#22312;&#27599;&#27425;&#36845;&#20195;&#30340;&#31532;&#19968;&#27493;&#65288;&#20266;&#26631;&#31614;&#21270;&#65289;&#20013;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#22522;&#20110;&#20108;&#36827;&#21046;&#23454;&#20363;&#26631;&#31614;&#30340;&#21513;&#24067;&#26031;&#20998;&#24067;&#65292;&#35813;&#20998;&#24067;&#36890;&#36807;&#20197;&#19979;&#32422;&#26463;&#23558;covariate&#20449;&#24687;&#65288;&#21327;&#21464;&#37327;&#20449;&#24687;&#65289;&#21512;&#24182;&#36827;&#21435;&#65306;&#20855;&#26377;&#30456;&#20284;covariates&#30340;&#23454;&#20363;&#24212;&#35813;&#20855;&#26377;&#30456;&#20284;&#30340;&#26631;&#31614;&#65292;&#24182;&#19988;&#36890;&#36807;&#21253;&#32423;&#21035;&#30340;&#32858;&#21512;&#26631;&#31614;&#26469;&#32508;&#21512;covariate&#20449;&#24687;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#20449;&#24565;&#20256;&#25773;&#65288;BP&#65289;&#26469;&#36793;&#32536;&#21270;&#21513;&#24067;&#26031;&#20998;&#24067;&#20197;&#33719;&#24471;&#20266;&#26631;&#31614;&#12290;&#22312;&#31532;&#20108;&#27493;&#65288;&#23884;&#20837;&#32454;&#21270;&#65289;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20266;&#26631;&#31614;&#20026;&#23398;&#20064;&#22120;&#25552;&#20379;&#30417;&#30563;&#65292;&#20174;&#32780;&#33719;&#24471;&#26356;&#22909;&#30340;&#23884;&#20837;&#12290;&#27492;&#21518;&#65292;&#25105;&#20204;&#23545;&#36825;&#20004;&#20010;&#27493;&#39588;&#36827;&#34892;&#36845;&#20195;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning from Label Proportions (LLP) is a learning problem where only aggregate level labels are available for groups of instances, called bags, during training, and the aim is to get the best performance at the instance-level on the test data. This setting arises in domains like advertising and medicine due to privacy considerations. We propose a novel algorithmic framework for this problem that iteratively performs two main steps. For the first step (Pseudo Labeling) in every iteration, we define a Gibbs distribution over binary instance labels that incorporates a) covariate information through the constraint that instances with similar covariates should have similar labels and b) the bag level aggregated label. We then use Belief Propagation (BP) to marginalize the Gibbs distribution to obtain pseudo labels. In the second step (Embedding Refinement), we use the pseudo labels to provide supervision for a learner that yields a better embedding. Further, we iterate on the two steps ag
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#32593;&#32476;&#65292;&#24182;&#21457;&#29616;&#35813;&#32593;&#32476;&#36861;&#27714;&#22810;&#20010;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#30446;&#26631;&#12290;&#36890;&#36807;&#20462;&#25913;&#29305;&#23450;&#30340;&#36890;&#36947;&#65292;&#25105;&#20204;&#21487;&#20197;&#37096;&#20998;&#22320;&#25511;&#21046;&#31574;&#30053;&#12290;&#36825;&#34920;&#26126;&#35757;&#32451;&#31574;&#30053;&#32593;&#32476;&#20013;&#23384;&#22312;&#20887;&#20313;&#12289;&#20998;&#24067;&#24335;&#21644;&#21487;&#37325;&#26032;&#23450;&#21521;&#30340;&#30446;&#26631;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2310.08043</link><description>&lt;p&gt;
&#29702;&#35299;&#21644;&#25511;&#21046;&#36855;&#23467;&#35299;&#20915;&#31574;&#30053;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Understanding and Controlling a Maze-Solving Policy Network. (arXiv:2310.08043v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08043
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#32593;&#32476;&#65292;&#24182;&#21457;&#29616;&#35813;&#32593;&#32476;&#36861;&#27714;&#22810;&#20010;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#30446;&#26631;&#12290;&#36890;&#36807;&#20462;&#25913;&#29305;&#23450;&#30340;&#36890;&#36947;&#65292;&#25105;&#20204;&#21487;&#20197;&#37096;&#20998;&#22320;&#25511;&#21046;&#31574;&#30053;&#12290;&#36825;&#34920;&#26126;&#35757;&#32451;&#31574;&#30053;&#32593;&#32476;&#20013;&#23384;&#22312;&#20887;&#20313;&#12289;&#20998;&#24067;&#24335;&#21644;&#21487;&#37325;&#26032;&#23450;&#21521;&#30340;&#30446;&#26631;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20102;&#35299;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#30446;&#26631;&#21644;&#30446;&#26631;&#34920;&#31034;&#65292;&#25105;&#20204;&#20180;&#32454;&#30740;&#31350;&#20102;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#36890;&#36807;&#23548;&#33322;&#21040;&#19968;&#31995;&#21015;&#30446;&#26631;&#26041;&#22359;&#26469;&#35299;&#20915;&#36855;&#23467;&#38382;&#39064;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#20010;&#32593;&#32476;&#36861;&#27714;&#22810;&#20010;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#30446;&#26631;&#65292;&#24182;&#19988;&#25105;&#20204;&#36827;&#19968;&#27493;&#30830;&#23450;&#20102;&#32593;&#32476;&#20013;&#19982;&#20854;&#20013;&#19968;&#20010;&#30446;&#26631;&#23545;&#24212;&#30340;&#30005;&#36335;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;11&#20010;&#36319;&#36394;&#30446;&#26631;&#20301;&#32622;&#30340;&#36890;&#36947;&#12290;&#36890;&#36807;&#20462;&#25913;&#36825;&#20123;&#36890;&#36947;&#65292;&#19981;&#35770;&#26159;&#36890;&#36807;&#25163;&#24037;&#35774;&#35745;&#30340;&#24178;&#39044;&#36824;&#26159;&#36890;&#36807;&#32452;&#21512;&#21069;&#21521;&#20256;&#36882;&#65292;&#25105;&#20204;&#37117;&#21487;&#20197;&#37096;&#20998;&#22320;&#25511;&#21046;&#31574;&#30053;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20010;&#32593;&#32476;&#21253;&#21547;&#22810;&#20313;&#30340;&#12289;&#20998;&#24067;&#24335;&#30340;&#12289;&#21487;&#37325;&#26032;&#23450;&#21521;&#30340;&#30446;&#26631;&#34920;&#31034;&#65292;&#20026;&#35757;&#32451;&#31574;&#30053;&#32593;&#32476;&#20013;&#30446;&#26631;&#23548;&#21521;&#30340;&#26412;&#36136;&#25552;&#20379;&#20102;&#25581;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
To understand the goals and goal representations of AI systems, we carefully study a pretrained reinforcement learning policy that solves mazes by navigating to a range of target squares. We find this network pursues multiple context-dependent goals, and we further identify circuits within the network that correspond to one of these goals. In particular, we identified eleven channels that track the location of the goal. By modifying these channels, either with hand-designed interventions or by combining forward passes, we can partially control the policy. We show that this network contains redundant, distributed, and retargetable goal representations, shedding light on the nature of goal-direction in trained policy networks.
&lt;/p&gt;</description></item><item><title>QLLM&#26159;&#19968;&#31181;&#20026;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#30340;&#20934;&#30830;&#39640;&#25928;&#30340;&#20302;&#20301;&#23485;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#33258;&#36866;&#24212;&#36890;&#36947;&#37325;&#32452;&#25216;&#26415;&#65292;&#23558;&#31163;&#32676;&#20540;&#30340;&#22823;&#23567;&#37325;&#26032;&#20998;&#37197;&#32473;&#20854;&#20182;&#36890;&#36947;&#65292;&#20174;&#32780;&#20943;&#36731;&#23427;&#20204;&#23545;&#37327;&#21270;&#33539;&#22260;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.08041</link><description>&lt;p&gt;
QLLM: &#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#20934;&#30830;&#39640;&#25928;&#20302;&#20301;&#23485;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
QLLM: Accurate and Efficient Low-Bitwidth Quantization for Large Language Models. (arXiv:2310.08041v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08041
&lt;/p&gt;
&lt;p&gt;
QLLM&#26159;&#19968;&#31181;&#20026;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#30340;&#20934;&#30830;&#39640;&#25928;&#30340;&#20302;&#20301;&#23485;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#33258;&#36866;&#24212;&#36890;&#36947;&#37325;&#32452;&#25216;&#26415;&#65292;&#23558;&#31163;&#32676;&#20540;&#30340;&#22823;&#23567;&#37325;&#26032;&#20998;&#37197;&#32473;&#20854;&#20182;&#36890;&#36947;&#65292;&#20174;&#32780;&#20943;&#36731;&#23427;&#20204;&#23545;&#37327;&#21270;&#33539;&#22260;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#30001;&#20110;&#20854;&#25152;&#38656;&#36164;&#28304;&#36807;&#22823;&#65292;&#38480;&#21046;&#20102;&#20854;&#24191;&#27867;&#24212;&#29992;&#12290;&#34429;&#28982;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#65288;Quantization-Aware Training&#65292;QAT&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#23427;&#30340;&#35757;&#32451;&#25104;&#26412;&#36807;&#39640;&#65292;&#22240;&#27492;&#21518;&#35757;&#32451;&#37327;&#21270;&#65288;Post-Training Quantization&#65292;PTQ&#65289;&#25104;&#20026;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#26356;&#23454;&#38469;&#30340;&#26041;&#27861;&#12290;&#22312;&#29616;&#26377;&#30740;&#31350;&#20013;&#65292;&#29305;&#23450;&#36890;&#36947;&#20013;&#30340;&#28608;&#27963;&#31163;&#32676;&#20540;&#34987;&#35748;&#20026;&#26159;&#23548;&#33268;&#21518;&#35757;&#32451;&#37327;&#21270;&#20934;&#30830;&#24615;&#19979;&#38477;&#30340;&#29942;&#39048;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;QLLM&#65292;&#19968;&#31181;&#20026;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#30340;&#20934;&#30830;&#39640;&#25928;&#30340;&#20302;&#20301;&#23485;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#12290;QLLM&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#36890;&#36947;&#37325;&#32452;&#25216;&#26415;&#65292;&#23558;&#31163;&#32676;&#20540;&#30340;&#22823;&#23567;&#37325;&#26032;&#20998;&#37197;&#32473;&#20854;&#20182;&#36890;&#36947;&#65292;&#20174;&#32780;&#20943;&#36731;&#23427;&#20204;&#23545;&#37327;&#21270;&#33539;&#22260;&#30340;&#24433;&#21709;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#36890;&#36807;&#36890;&#36947;&#25286;&#20998;&#21644;&#36890;&#36947;&#32452;&#35013;&#65292;&#22312;&#20445;&#35777;&#20302;&#20301;&#23485;&#30340;&#24773;&#20917;&#19979;&#23558;&#31163;&#32676;&#36890;&#36947;&#20998;&#35299;&#25104;&#22810;&#20010;&#23376;&#36890;&#36947;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) excel in NLP, but their demands hinder their widespread deployment. While Quantization-Aware Training (QAT) offers a solution, its extensive training costs make Post-Training Quantization (PTQ) a more practical approach for LLMs. In existing studies, activation outliers in particular channels are identified as the bottleneck to PTQ accuracy. They propose to transform the magnitudes from activations to weights, which however offers limited alleviation or suffers from unstable gradients, resulting in a severe performance drop at low-bitwidth. In this paper, we propose QLLM, an accurate and efficient low-bitwidth PTQ method designed for LLMs. QLLM introduces an adaptive channel reassembly technique that reallocates the magnitude of outliers to other channels, thereby mitigating their impact on the quantization range. This is achieved by channel disassembly and channel assembly, which first breaks down the outlier channels into several sub-channels to ensure a 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#26032;&#24605;&#32771;&#22823;&#35268;&#27169;&#39044;&#25490;&#24207;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#20307;&#38142;&#36335;&#36328;&#22495;&#27169;&#22411;&#21644;&#32454;&#31890;&#24230;&#31070;&#32463;&#32467;&#26500;&#26469;&#35299;&#20915;&#26679;&#26412;&#36873;&#25321;&#20559;&#24046;&#38382;&#39064;&#65292;&#24182;&#25913;&#36827;&#39044;&#25490;&#24207;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.08039</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#22823;&#35268;&#27169;&#39044;&#25490;&#24207;&#31995;&#32479;&#65306;&#25972;&#20307;&#38142;&#36335;&#36328;&#22495;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Rethinking Large-scale Pre-ranking System: Entire-chain Cross-domain Models. (arXiv:2310.08039v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08039
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#26032;&#24605;&#32771;&#22823;&#35268;&#27169;&#39044;&#25490;&#24207;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#20307;&#38142;&#36335;&#36328;&#22495;&#27169;&#22411;&#21644;&#32454;&#31890;&#24230;&#31070;&#32463;&#32467;&#26500;&#26469;&#35299;&#20915;&#26679;&#26412;&#36873;&#25321;&#20559;&#24046;&#38382;&#39064;&#65292;&#24182;&#25913;&#36827;&#39044;&#25490;&#24207;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#19994;&#31995;&#32479;&#65292;&#22914;&#25512;&#33616;&#31995;&#32479;&#21644;&#22312;&#32447;&#24191;&#21578;&#65292;&#24050;&#24191;&#27867;&#37197;&#22791;&#20102;&#22810;&#38454;&#27573;&#26550;&#26500;&#65292;&#21253;&#25324;&#21305;&#37197;&#12289;&#39044;&#25490;&#24207;&#12289;&#25490;&#24207;&#21644;&#20877;&#25490;&#24207;&#12290;&#20316;&#20026;&#21305;&#37197;&#21644;&#25490;&#24207;&#20043;&#38388;&#30340;&#20851;&#38190;&#26725;&#26753;&#65292;&#29616;&#26377;&#30340;&#39044;&#25490;&#24207;&#26041;&#27861;&#20027;&#35201;&#24573;&#35270;&#20102;&#25972;&#20010;&#38142;&#36335;&#25968;&#25454;&#30340;&#20381;&#36182;&#24615;&#65292;&#23548;&#33268;&#23376;&#20248;&#21270;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#25972;&#20307;&#26679;&#26412;&#31354;&#38388;&#30340;&#35282;&#24230;&#37325;&#26032;&#24605;&#32771;&#39044;&#25490;&#24207;&#31995;&#32479;&#65292;&#24182;&#25552;&#20986;&#20102;&#25972;&#20307;&#38142;&#36335;&#36328;&#22495;&#27169;&#22411;&#65288;ECM&#65289;&#65292;&#21033;&#29992;&#25972;&#20010;&#32423;&#32852;&#38454;&#27573;&#30340;&#26679;&#26412;&#26469;&#26377;&#25928;&#20943;&#36731;&#26679;&#26412;&#36873;&#25321;&#20559;&#24046;&#65288;SSB&#65289;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#32454;&#31890;&#24230;&#31070;&#32463;&#32467;&#26500;&#65292;&#21517;&#20026;ECMM&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#39044;&#25490;&#24207;&#30340;&#20934;&#30830;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36328;&#22495;&#22810;&#22612;&#31070;&#32463;&#32593;&#32476;&#26469;&#32508;&#21512;&#39044;&#27979;&#27599;&#20010;&#38454;&#27573;&#30340;&#32467;&#26524;&#65292;&#24182;&#24341;&#20837;$L0$&#27491;&#21017;&#21270;&#30340;&#23376;&#32593;&#32476;&#36335;&#30001;&#31574;&#30053;&#26469;&#35843;&#25972;&#27599;&#20010;&#38454;&#27573;&#30340;&#36129;&#29486;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;
Industrial systems such as recommender systems and online advertising, have been widely equipped with multi-stage architectures, which are divided into several cascaded modules, including matching, pre-ranking, ranking and re-ranking. As a critical bridge between matching and ranking, existing pre-ranking approaches mainly endure sample selection bias (SSB) problem owing to ignoring the entire-chain data dependence, resulting in sub-optimal performances. In this paper, we rethink pre-ranking system from the perspective of the entire sample space, and propose Entire-chain Cross-domain Models (ECM), which leverage samples from the whole cascaded stages to effectively alleviate SSB problem. Besides, we design a fine-grained neural structure named ECMM to further improve the pre-ranking accuracy. Specifically, we propose a cross-domain multi-tower neural network to comprehensively predict for each stage result, and introduce the sub-networking routing strategy with $L0$ regularization to r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25913;&#36827;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20915;&#31574;&#36807;&#31243;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;LLM&#30340;&#35821;&#35328;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#23454;&#29616;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#19982;&#20056;&#23458;&#30340;&#20114;&#21160;&#19982;&#20010;&#24615;&#21270;&#12290;</title><link>http://arxiv.org/abs/2310.08034</link><description>&lt;p&gt;
&#25509;&#25910;&#12289;&#25512;&#29702;&#21644;&#21453;&#24212;&#65306;&#22312;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39550;&#39542;&#12290;
&lt;/p&gt;
&lt;p&gt;
Receive, Reason, and React: Drive as You Say with Large Language Models in Autonomous Vehicles. (arXiv:2310.08034v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08034
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25913;&#36827;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20915;&#31574;&#36807;&#31243;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;LLM&#30340;&#35821;&#35328;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#23454;&#29616;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#19982;&#20056;&#23458;&#30340;&#20114;&#21160;&#19982;&#20010;&#24615;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#26412;&#35774;&#35745;&#19982;&#20154;&#24037;&#26234;&#33021;&#33021;&#21147;&#30340;&#34701;&#21512;&#20026;&#36229;&#36234;&#20132;&#36890;&#30340;&#19979;&#19968;&#20195;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;&#36825;&#20123;&#36710;&#36742;&#21487;&#20197;&#19982;&#20056;&#23458;&#21160;&#24577;&#20114;&#21160;&#24182;&#26681;&#25454;&#20854;&#21916;&#22909;&#36827;&#34892;&#33258;&#36866;&#24212;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#22686;&#24378;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#36890;&#36807;&#21033;&#29992;LLM&#30340;&#35821;&#35328;&#21644;&#19978;&#19979;&#25991;&#29702;&#35299;&#33021;&#21147;&#20197;&#21450;&#19987;&#38376;&#30340;&#24037;&#20855;&#65292;&#25105;&#20204;&#26088;&#22312;&#23558;LLM&#30340;&#35821;&#35328;&#21644;&#25512;&#29702;&#33021;&#21147;&#25972;&#21512;&#21040;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21253;&#25324;&#22312;HighwayEnv&#20013;&#36827;&#34892;&#23454;&#39564;&#65292;&#36825;&#26159;&#19968;&#22871;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#21644;&#25112;&#26415;&#20915;&#31574;&#20219;&#21153;&#30340;&#29615;&#22659;&#38598;&#65292;&#20197;&#25506;&#32034;LLM&#22312;&#21508;&#31181;&#22330;&#26223;&#20013;&#30340;&#35299;&#37322;&#12289;&#20114;&#21160;&#21644;&#25512;&#29702;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#23454;&#26102;&#20010;&#24615;&#21270;&#65292;&#23637;&#31034;&#20102;LLM&#22914;&#20309;&#26681;&#25454;&#21475;&#22836;&#21629;&#20196;&#24433;&#21709;&#39550;&#39542;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#24378;&#35843;&#20102;LLM&#22312;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;&#30340;&#26174;&#33879;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
The fusion of human-centric design and artificial intelligence (AI) capabilities has opened up new possibilities for next-generation autonomous vehicles that go beyond transportation. These vehicles can dynamically interact with passengers and adapt to their preferences. This paper proposes a novel framework that leverages Large Language Models (LLMs) to enhance the decision-making process in autonomous vehicles. By utilizing LLMs' linguistic and contextual understanding abilities with specialized tools, we aim to integrate the language and reasoning capabilities of LLMs into autonomous vehicles. Our research includes experiments in HighwayEnv, a collection of environments for autonomous driving and tactical decision-making tasks, to explore LLMs' interpretation, interaction, and reasoning in various scenarios. We also examine real-time personalization, demonstrating how LLMs can influence driving behaviors based on verbal commands. Our empirical results highlight the substantial advan
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23558;&#39046;&#22495;&#30693;&#35782;&#22270;&#32435;&#20837;&#22810;&#27169;&#24577;&#24433;&#29255;&#31867;&#22411;&#20998;&#31867;&#20013;&#65292;&#36890;&#36807;&#21033;&#29992;&#32676;&#32452;&#20851;&#31995;&#21644;&#25913;&#36827;&#27880;&#24847;&#21147;&#20998;&#37197;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#21644;&#21487;&#38752;&#30340;&#20998;&#31867;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.08032</link><description>&lt;p&gt;
&#23558;&#39046;&#22495;&#30693;&#35782;&#22270;&#32435;&#20837;&#22810;&#27169;&#24577;&#24433;&#29255;&#31867;&#22411;&#20998;&#31867;&#20013;&#65292;&#37319;&#29992;&#33258;&#30417;&#30563;&#27880;&#24847;&#21147;&#21644;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Incorporating Domain Knowledge Graph into Multimodal Movie Genre Classification with Self-Supervised Attention and Contrastive Learning. (arXiv:2310.08032v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08032
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23558;&#39046;&#22495;&#30693;&#35782;&#22270;&#32435;&#20837;&#22810;&#27169;&#24577;&#24433;&#29255;&#31867;&#22411;&#20998;&#31867;&#20013;&#65292;&#36890;&#36807;&#21033;&#29992;&#32676;&#32452;&#20851;&#31995;&#21644;&#25913;&#36827;&#27880;&#24847;&#21147;&#20998;&#37197;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#21644;&#21487;&#38752;&#30340;&#20998;&#31867;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#24433;&#29255;&#31867;&#22411;&#20998;&#31867;&#19968;&#30452;&#34987;&#35270;&#20026;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#20219;&#21153;&#65292;&#30001;&#20110;&#22810;&#27169;&#24577;&#25968;&#25454;&#65288;&#22914;&#28023;&#25253;&#12289;&#21095;&#24773;&#25688;&#35201;&#12289;&#39044;&#21578;&#29255;&#21644;&#20803;&#25968;&#25454;&#65289;&#30340;&#22810;&#26679;&#24615;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#30740;&#31350;&#22312;&#24314;&#27169;&#21644;&#32452;&#21512;&#27599;&#20010;&#27169;&#24577;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#65292;&#20294;&#20173;&#38754;&#20020;&#30528;&#19977;&#20010;&#38382;&#39064;&#65306;1&#65289;&#26410;&#21033;&#29992;&#20803;&#25968;&#25454;&#20013;&#30340;&#32676;&#32452;&#20851;&#31995;&#65292;2&#65289;&#19981;&#21487;&#38752;&#30340;&#27880;&#24847;&#21147;&#20998;&#37197;&#65292;3&#65289;&#19981;&#21487;&#21306;&#20998;&#30340;&#34701;&#21512;&#29305;&#24449;&#12290;&#37492;&#20110;&#24050;&#35777;&#26126;&#30693;&#35782;&#22270;&#21253;&#21547;&#20016;&#23500;&#30340;&#20449;&#24687;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#20174;&#19981;&#21516;&#30340;&#35282;&#24230;&#21033;&#29992;&#30693;&#35782;&#22270;&#26469;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#20803;&#25968;&#25454;&#22788;&#29702;&#25104;&#19968;&#20010;&#39046;&#22495;&#30693;&#35782;&#22270;&#12290;&#25105;&#20204;&#37319;&#29992;&#19968;&#31181;&#30693;&#35782;&#22270;&#23884;&#20837;&#30340;&#32763;&#35793;&#27169;&#22411;&#26469;&#25429;&#25417;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#21033;&#29992;&#20803;&#25968;&#25454;&#20013;&#30340;&#32676;&#32452;&#20851;&#31995;&#65292;&#25105;&#20204;&#20174;&#30693;&#35782;&#22270;&#20013;&#26816;&#32034;&#30456;&#20851;&#30340;&#23884;&#20837;&#65292;&#28982;&#21518;&#19982;&#20854;&#20182;&#27169;&#24577;&#36827;&#34892;&#25972;&#21512;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#36825;&#31181;&#26426;&#21046;&#26159;&#33258;&#30417;&#30563;&#30340;&#65292;&#24182;&#37319;&#29992;&#20102;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#26469;&#25913;&#36827;&#27880;&#24847;&#21147;&#20998;&#37197;&#30340;&#21487;&#38752;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#37319;&#29992;&#19968;&#31181;&#20998;&#31867;&#22120;&#26469;&#36827;&#34892;&#24433;&#29255;&#31867;&#22411;&#20998;&#31867;&#65292;&#35813;&#20998;&#31867;&#22120;&#21033;&#29992;&#20102;&#19981;&#21516;&#27169;&#24577;&#30340;&#34701;&#21512;&#29305;&#24449;&#21644;&#27880;&#24847;&#21147;&#26435;&#37325;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#22343;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal movie genre classification has always been regarded as a demanding multi-label classification task due to the diversity of multimodal data such as posters, plot summaries, trailers and metadata. Although existing works have made great progress in modeling and combining each modality, they still face three issues: 1) unutilized group relations in metadata, 2) unreliable attention allocation, and 3) indiscriminative fused features. Given that the knowledge graph has been proven to contain rich information, we present a novel framework that exploits the knowledge graph from various perspectives to address the above problems. As a preparation, the metadata is processed into a domain knowledge graph. A translate model for knowledge graph embedding is adopted to capture the relations between entities. Firstly we retrieve the relevant embedding from the knowledge graph by utilizing group relations in metadata and then integrate it with other modalities. Next, we introduce an Attent
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26080;&#20154;&#26426;&#30340;&#36328;&#27169;&#24577;&#36710;&#36742;&#20877;&#35782;&#21035;&#65288;Re-ID&#65289;&#30340;&#21019;&#26032;&#30740;&#31350;&#65292;&#26088;&#22312;&#35299;&#20915;&#25968;&#25454;&#19981;&#36275;&#12289;&#36328;&#27169;&#24577;&#24046;&#24322;&#21644;&#26059;&#21521;&#24046;&#24322;&#30340;&#38382;&#39064;&#12290;&#25552;&#20986;&#20102;&#19968;&#20010;&#36328;&#27169;&#24577;&#36710;&#36742;Re-ID&#22522;&#20934;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#28151;&#21512;&#26435;&#37325;&#20998;&#31163;&#32593;&#32476;&#65288;HWDNet&#65289;&#26469;&#23398;&#20064;&#20849;&#20139;&#30340;&#21028;&#21035;&#24615;&#26059;&#21521;&#19981;&#21464;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2310.08026</link><description>&lt;p&gt;
&#36229;&#36234;&#26435;&#37325;&#20849;&#20139;&#30340;&#26080;&#20154;&#26426;RGB-&#32418;&#22806;&#36710;&#36742;&#20877;&#35782;&#21035;&#29305;&#24449;&#23398;&#20064;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Beyond Sharing Weights in Decoupling Feature Learning Network for UAV RGB-Infrared Vehicle Re-Identification. (arXiv:2310.08026v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08026
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26080;&#20154;&#26426;&#30340;&#36328;&#27169;&#24577;&#36710;&#36742;&#20877;&#35782;&#21035;&#65288;Re-ID&#65289;&#30340;&#21019;&#26032;&#30740;&#31350;&#65292;&#26088;&#22312;&#35299;&#20915;&#25968;&#25454;&#19981;&#36275;&#12289;&#36328;&#27169;&#24577;&#24046;&#24322;&#21644;&#26059;&#21521;&#24046;&#24322;&#30340;&#38382;&#39064;&#12290;&#25552;&#20986;&#20102;&#19968;&#20010;&#36328;&#27169;&#24577;&#36710;&#36742;Re-ID&#22522;&#20934;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#28151;&#21512;&#26435;&#37325;&#20998;&#31163;&#32593;&#32476;&#65288;HWDNet&#65289;&#26469;&#23398;&#20064;&#20849;&#20139;&#30340;&#21028;&#21035;&#24615;&#26059;&#21521;&#19981;&#21464;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#33021;&#22815;&#20840;&#22825;&#20505;&#36827;&#34892;&#30446;&#26631;&#25628;&#32034;&#65292;&#22522;&#20110;&#26080;&#20154;&#26426;&#30340;&#36328;&#27169;&#24577;&#36710;&#36742;&#20877;&#35782;&#21035;&#65288;Re-ID&#65289;&#22312;&#35270;&#39057;&#30417;&#25511;&#21644;&#20844;&#20849;&#23433;&#20840;&#39046;&#22495;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#25454;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#36825;&#20010;&#26377;&#21069;&#26223;&#21644;&#21019;&#26032;&#30340;&#30740;&#31350;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#30340;&#30740;&#31350;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#36328;&#27169;&#24577;&#24046;&#24322;&#21644;&#26059;&#21521;&#24046;&#24322;&#36827;&#19968;&#27493;&#22686;&#21152;&#20102;&#36825;&#19968;&#20219;&#21153;&#30340;&#38590;&#24230;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#26080;&#20154;&#26426;&#36328;&#27169;&#24577;&#36710;&#36742;&#20877;&#35782;&#21035;&#65288;UCM-VeID&#65289;&#30340;&#36328;&#27169;&#24577;&#36710;&#36742;Re-ID&#22522;&#20934;&#65292;&#21253;&#21547;753&#20010;&#36523;&#20221;&#65292;16015&#20010;RGB&#22270;&#20687;&#21644;13913&#20010;&#32418;&#22806;&#22270;&#20687;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#24212;&#23545;&#36328;&#27169;&#24577;&#24046;&#24322;&#21644;&#26059;&#21521;&#24046;&#24322;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#26435;&#37325;&#20998;&#31163;&#32593;&#32476;&#65288;HWDNet&#65289;&#65292;&#29992;&#20110;&#23398;&#20064;&#20849;&#20139;&#30340;&#21028;&#21035;&#24615;&#26059;&#21521;&#19981;&#21464;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Owing to the capacity of performing full-time target search, cross-modality vehicle re-identification (Re-ID) based on unmanned aerial vehicle (UAV) is gaining more attention in both video surveillance and public security. However, this promising and innovative research has not been studied sufficiently due to the data inadequacy issue. Meanwhile, the cross-modality discrepancy and orientation discrepancy challenges further aggravate the difficulty of this task. To this end, we pioneer a cross-modality vehicle Re-ID benchmark named UAV Cross-Modality Vehicle Re-ID (UCM-VeID), containing 753 identities with 16015 RGB and 13913 infrared images. Moreover, to meet cross-modality discrepancy and orientation discrepancy challenges, we present a hybrid weights decoupling network (HWDNet) to learn the shared discriminative orientation-invariant features. For the first challenge, we proposed a hybrid weights siamese network with a well-designed weight restrainer and its corresponding objective 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#23545;BERT&#27169;&#22411;&#30340;&#24191;&#20041;&#24615;&#24433;&#21709;&#65292;&#21457;&#29616;&#22312;&#22266;&#23450;&#22823;&#23567;&#30340;&#35757;&#32451;&#26679;&#26412;&#19978;&#65292;&#26377;10-30\%&#30340;&#20154;&#20026;&#23545;&#25239;&#23454;&#20363;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25991;&#26412;&#20998;&#31867;&#21644;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#30340;&#31934;&#24230;&#21644;F1&#20540;&#12290;</title><link>http://arxiv.org/abs/2310.08008</link><description>&lt;p&gt;
BERT&#24191;&#20041;&#24615;&#30340;&#24433;&#21709;&#65306;&#20154;&#20026;&#23545;&#25239;&#26679;&#26412;&#21644;&#21451;&#22909;&#26679;&#26412;&#30340;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Effects of Human Adversarial and Affable Samples on BERT Generalizability. (arXiv:2310.08008v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08008
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#23545;BERT&#27169;&#22411;&#30340;&#24191;&#20041;&#24615;&#24433;&#21709;&#65292;&#21457;&#29616;&#22312;&#22266;&#23450;&#22823;&#23567;&#30340;&#35757;&#32451;&#26679;&#26412;&#19978;&#65292;&#26377;10-30\%&#30340;&#20154;&#20026;&#23545;&#25239;&#23454;&#20363;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25991;&#26412;&#20998;&#31867;&#21644;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#30340;&#31934;&#24230;&#21644;F1&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;BERT&#30340;&#27169;&#22411;&#22312;&#39046;&#20808;&#27036;&#19978;&#34920;&#29616;&#24378;&#21170;&#65292;&#20294;&#22312;&#38656;&#35201;&#27867;&#21270;&#30340;&#23454;&#38469;&#22330;&#26223;&#20013;&#34920;&#29616;&#36739;&#24046;&#12290;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#34987;&#35748;&#20026;&#26159;&#26426;&#22120;&#23398;&#20064;&#27867;&#21270;&#33021;&#21147;&#30340;&#20027;&#35201;&#38556;&#30861;&#12290;&#26412;&#25991;&#30740;&#31350;&#35757;&#32451;&#25968;&#25454;&#36136;&#37327;&#23545;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#32780;&#19981;&#26159;&#25968;&#37327;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#35757;&#32451;&#25968;&#25454;&#30340;&#20004;&#20010;&#29305;&#24449;&#65306;&#20154;&#20026;&#23545;&#25239;&#26679;&#26412;&#65288;&#20855;&#26377;&#30475;&#20284;&#24494;&#23567;&#24046;&#24322;&#20294;&#20855;&#26377;&#19981;&#21516;&#26631;&#31614;&#30340;&#26679;&#26412;&#23545;&#65289;&#21644;&#20154;&#20026;&#21451;&#22909;&#26679;&#26412;&#65288;&#20855;&#26377;&#24494;&#23567;&#24046;&#24322;&#20294;&#20855;&#26377;&#30456;&#21516;&#26631;&#31614;&#30340;&#26679;&#26412;&#23545;&#65289;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#22266;&#23450;&#22823;&#23567;&#30340;&#35757;&#32451;&#26679;&#26412;&#19978;&#65292;&#20197;10-30\%&#30340;&#20154;&#20026;&#23545;&#25239;&#23454;&#20363;&#20026;&#32463;&#39564;&#65292;&#21487;&#20197;&#25552;&#39640;&#25991;&#26412;&#20998;&#31867;&#21644;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#30340;&#31934;&#24230;&#21644;F1&#20540;&#26368;&#22810;20&#20010;&#30334;&#20998;&#28857;&#12290;&#36229;&#36807;&#27492;&#33539;&#22260;&#30340;&#22686;&#21152;&#23545;&#27169;&#22411;&#24615;&#33021;&#26080;&#26174;&#33879;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
BERT-based models have had strong performance on leaderboards, yet have been demonstrably worse in real-world settings requiring generalization. Limited quantities of training data is considered a key impediment to achieving generalizability in machine learning. In this paper, we examine the impact of training \textit{data quality}, not quantity, on a model's generalizability. We consider two characteristics of training data: the portion of human-adversarial (h-adversarial), i.e., sample pairs with seemingly minor differences but different ground-truth labels, and human-affable (h-affable) training samples, i.e., sample pairs with minor differences but the same ground-truth label. We find that for a fixed size of training samples, as a rule of thumb, having 10-30\% h-adversarial instances improves the precision, and therefore F1, by up to 20 points in the tasks of text classification and relation extraction. Increasing h-adversarials beyond this range can result in performance plateaus
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#21644;&#26032;&#30340;&#32479;&#35745;&#24230;&#37327;&#26041;&#27861;&#65292;&#26088;&#22312;&#30740;&#31350;&#25968;&#25454;&#36136;&#37327;&#20445;&#35777;&#20013;&#30340;&#31163;&#32676;&#25968;&#25454;&#26816;&#27979;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#26377;&#25928;&#21306;&#20998;&#27491;&#24120;&#25968;&#25454;&#21644;&#31163;&#32676;&#25968;&#25454;&#30340;&#29305;&#24449;&#34920;&#31034;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.07998</link><description>&lt;p&gt;
&#25968;&#25454;&#36136;&#37327;&#20445;&#35777;&#20013;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#31163;&#32676;&#25968;&#25454;&#26816;&#27979;&#30340;&#32479;&#35745;&#24230;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Novel Statistical Measure for Out-of-Distribution Detection in Data Quality Assurance. (arXiv:2310.07998v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07998
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#21644;&#26032;&#30340;&#32479;&#35745;&#24230;&#37327;&#26041;&#27861;&#65292;&#26088;&#22312;&#30740;&#31350;&#25968;&#25454;&#36136;&#37327;&#20445;&#35777;&#20013;&#30340;&#31163;&#32676;&#25968;&#25454;&#26816;&#27979;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#26377;&#25928;&#21306;&#20998;&#27491;&#24120;&#25968;&#25454;&#21644;&#31163;&#32676;&#25968;&#25454;&#30340;&#29305;&#24449;&#34920;&#31034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#39046;&#22495;&#20043;&#22806;&#30340;&#25968;&#25454;&#23545;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#26234;&#33021;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#26500;&#25104;&#20102;&#37325;&#22823;&#23041;&#32961;&#12290;&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#20154;&#24037;&#26234;&#33021;&#36136;&#37327;&#31649;&#29702;&#20013;&#30340;&#25968;&#25454;&#39046;&#22495;&#21644;&#31163;&#32676;&#25968;&#25454;&#65292;&#24182;&#25552;&#20986;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#29305;&#24449;&#34920;&#31034;&#65292;&#24182;&#24320;&#21457;&#19968;&#31181;&#29992;&#20110;&#31163;&#32676;&#25968;&#25454;&#26816;&#27979;&#30340;&#26032;&#30340;&#32479;&#35745;&#24230;&#37327;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#20026;&#20102;&#25552;&#21462;&#33021;&#22815;&#21306;&#20998;&#27491;&#24120;&#25968;&#25454;&#21644;&#31163;&#32676;&#25968;&#25454;&#30340;&#20302;&#32500;&#20195;&#34920;&#24615;&#29305;&#24449;&#65292;&#26412;&#25991;&#30340;&#30740;&#31350;&#23558;&#28145;&#24230;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;AE&#65289;&#26550;&#26500;&#19982;&#31070;&#32463;&#20803;&#28608;&#27963;&#29366;&#24577;&#30456;&#32467;&#21512;&#36827;&#34892;&#29305;&#24449;&#24037;&#31243;&#12290;&#28982;&#21518;&#65292;&#21033;&#29992;&#25968;&#25454;&#37325;&#26500;&#20013;&#30340;&#23616;&#37096;&#26465;&#20214;&#27010;&#29575;&#65288;LCP&#65289;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#20248;&#36234;&#30340;&#32479;&#35745;&#24230;&#37327;&#26041;&#27861;&#26469;&#35745;&#31639;&#31163;&#32676;&#25968;&#25454;&#26816;&#27979;&#30340;&#24471;&#20998;&#12290;&#23454;&#39564;&#21644;&#35780;&#20272;&#22312;&#22270;&#20687;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#24037;&#19994;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#12290;&#36890;&#36807;&#19982;&#20854;&#20182;&#24120;&#35265;&#30340;&#31163;&#32676;&#25968;&#25454;&#26816;&#27979;&#30340;&#32479;&#35745;&#24230;&#37327;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#26412;&#30740;&#31350;&#22312;&#31163;&#32676;&#25968;&#25454;&#21644;&#20154;&#24037;&#26234;&#33021;&#26041;&#38754;&#26159;&#21487;&#34892;&#21644;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data outside the problem domain poses significant threats to the security of AI-based intelligent systems. Aiming to investigate the data domain and out-of-distribution (OOD) data in AI quality management (AIQM) study, this paper proposes to use deep learning techniques for feature representation and develop a novel statistical measure for OOD detection. First, to extract low-dimensional representative features distinguishing normal and OOD data, the proposed research combines the deep auto-encoder (AE) architecture and neuron activation status for feature engineering. Then, using local conditional probability (LCP) in data reconstruction, a novel and superior statistical measure is developed to calculate the score of OOD detection. Experiments and evaluations are conducted on image benchmark datasets and an industrial dataset. Through comparative analysis with other common statistical measures in OOD detection, the proposed research is validated as feasible and effective in OOD and AI
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Point-NeuS&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#28857;&#23548;&#21521;&#26426;&#21046;&#23454;&#29616;&#20934;&#30830;&#21644;&#39640;&#25928;&#30340;&#31070;&#32463;&#38544;&#24335;&#26354;&#38754;&#37325;&#24314;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#26041;&#27861;&#31934;&#24230;&#26377;&#38480;&#21644;&#26102;&#38388;&#22797;&#26434;&#24230;&#36807;&#39640;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.07997</link><description>&lt;p&gt;
Point-NeuS&#65306;&#36890;&#36807;&#20307;&#28210;&#26579;&#36827;&#34892;&#28857;&#23548;&#21521;&#30340;&#31070;&#32463;&#38544;&#24335;&#26354;&#38754;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
Point-NeuS: Point-Guided Neural Implicit Surface Reconstruction by Volume Rendering. (arXiv:2310.07997v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07997
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Point-NeuS&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#28857;&#23548;&#21521;&#26426;&#21046;&#23454;&#29616;&#20934;&#30830;&#21644;&#39640;&#25928;&#30340;&#31070;&#32463;&#38544;&#24335;&#26354;&#38754;&#37325;&#24314;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#26041;&#27861;&#31934;&#24230;&#26377;&#38480;&#21644;&#26102;&#38388;&#22797;&#26434;&#24230;&#36807;&#39640;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#36890;&#36807;&#20307;&#28210;&#26579;&#23398;&#20064;&#31070;&#32463;&#38544;&#24335;&#26354;&#38754;&#24050;&#25104;&#20026;&#22810;&#35270;&#35282;&#37325;&#24314;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#26041;&#27861;&#23384;&#22312;&#31934;&#24230;&#26377;&#38480;&#21644;&#26102;&#38388;&#22797;&#26434;&#24230;&#36807;&#39640;&#30340;&#29942;&#39048;&#38382;&#39064;&#65292;&#20127;&#38656;&#35299;&#20915;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Point-NeuS&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#28857;&#23548;&#21521;&#26426;&#21046;&#23454;&#29616;&#20934;&#30830;&#21644;&#39640;&#25928;&#30340;&#37325;&#24314;&#12290;&#28857;&#24314;&#27169;&#26377;&#26426;&#22320;&#23884;&#20837;&#21040;&#20307;&#28210;&#26579;&#20013;&#65292;&#20197;&#22686;&#24378;&#21644;&#35268;&#33539;&#38544;&#24335;&#26354;&#38754;&#30340;&#34920;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20026;&#20102;&#23454;&#29616;&#31934;&#30830;&#30340;&#28857;&#23548;&#21521;&#21644;&#25239;&#22122;&#33021;&#21147;&#65292;&#24314;&#27169;&#20102;&#28857;&#20113;&#30340;&#38543;&#26426;&#19981;&#30830;&#23450;&#24615;&#65292;&#20197;&#25429;&#25417;&#22122;&#22768;&#20998;&#24067;&#24182;&#20272;&#35745;&#28857;&#30340;&#21487;&#38752;&#24615;&#12290;&#27492;&#22806;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#36830;&#25509;&#28857;&#21644;&#22270;&#20687;&#30340;&#31070;&#32463;&#25237;&#24433;&#27169;&#22359;&#65292;&#20197;&#21521;&#31526;&#21495;&#36317;&#31163;&#20989;&#25968;&#65288;SDF&#65289;&#28155;&#21152;&#20960;&#20309;&#32422;&#26463;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#34917;&#20607;&#20307;&#28210;&#26579;&#21644;&#28857;&#24314;&#27169;&#20043;&#38388;&#30340;&#20960;&#20309;&#20559;&#24046;&#65292;&#23545;&#39640;&#20445;&#30495;&#24230;&#28857;&#36827;&#34892;&#20102;&#28388;&#27874;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, learning neural implicit surface by volume rendering has been a promising way for multi-view reconstruction. However, limited accuracy and excessive time complexity remain bottlenecks that current methods urgently need to overcome. To address these challenges, we propose a new method called Point-NeuS, utilizing point-guided mechanisms to achieve accurate and efficient reconstruction. Point modeling is organically embedded into the volume rendering to enhance and regularize the representation of implicit surface. Specifically, to achieve precise point guidance and noise robustness, aleatoric uncertainty of the point cloud is modeled to capture the distribution of noise and estimate the reliability of points. Additionally, a Neural Projection module connecting points and images is introduced to add geometric constraints to the Signed Distance Function (SDF). To better compensate for geometric bias between volume rendering and point modeling, high-fidelity points are filtered i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#33322;&#31354;&#24433;&#20687;&#30340;&#20840;&#38754;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#22810;&#23618;&#20132;&#20114;&#21644;&#22270;&#20687;&#33258;&#36866;&#24212;&#20998;&#31867;&#22238;&#24402;&#32593;&#32476;&#23454;&#29616;&#21333;&#30446;&#39640;&#24230;&#20272;&#35745;&#65292;&#35299;&#20915;&#20102;&#22266;&#23450;&#25509;&#21463;&#22495;&#21644;&#32570;&#20047;&#20840;&#23616;&#20449;&#24687;&#20132;&#20114;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.07995</link><description>&lt;p&gt;
HeightFormer:&#19968;&#31181;&#22810;&#23618;&#20132;&#20114;&#21644;&#22270;&#20687;&#33258;&#36866;&#24212;&#20998;&#31867;&#22238;&#24402;&#32593;&#32476;&#65292;&#29992;&#20110;&#33322;&#31354;&#24433;&#20687;&#30340;&#21333;&#30446;&#39640;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
HeightFormer: A Multilevel Interaction and Image-adaptive Classification-regression Network for Monocular Height Estimation with Aerial Images. (arXiv:2310.07995v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07995
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#33322;&#31354;&#24433;&#20687;&#30340;&#20840;&#38754;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#22810;&#23618;&#20132;&#20114;&#21644;&#22270;&#20687;&#33258;&#36866;&#24212;&#20998;&#31867;&#22238;&#24402;&#32593;&#32476;&#23454;&#29616;&#21333;&#30446;&#39640;&#24230;&#20272;&#35745;&#65292;&#35299;&#20915;&#20102;&#22266;&#23450;&#25509;&#21463;&#22495;&#21644;&#32570;&#20047;&#20840;&#23616;&#20449;&#24687;&#20132;&#20114;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#24230;&#20272;&#35745;&#19968;&#30452;&#26159;&#27979;&#37327;&#21644;&#36965;&#24863;&#23398;&#31185;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#20027;&#39064;&#65292;&#23545;&#20110;3D&#22478;&#24066;&#24314;&#27169;&#12289;MR&#21644;&#33258;&#21160;&#39550;&#39542;&#31561;&#21162;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#26041;&#27861;&#20351;&#29992;&#31435;&#20307;&#21305;&#37197;&#25110;&#22810;&#20256;&#24863;&#22120;&#34701;&#21512;&#65292;&#36825;&#20123;&#37117;&#26159;&#25104;&#29087;&#30340;&#25216;&#26415;&#65292;&#36890;&#24120;&#38656;&#35201;&#22810;&#20010;&#26469;&#33258;&#19981;&#21516;&#35282;&#24230;&#21644;&#38468;&#21152;&#20256;&#24863;&#22120;&#65288;&#22914;SAR&#65289;&#30340;&#22270;&#20687;&#65292;&#23548;&#33268;&#37096;&#32626;&#25104;&#26412;&#39640;&#12290;&#21333;&#22270;&#20687;&#39640;&#24230;&#20272;&#35745;&#24050;&#25104;&#20026;&#19968;&#31181;&#26377;&#21560;&#24341;&#21147;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#20855;&#26377;&#26356;&#22823;&#30340;&#25968;&#25454;&#26469;&#28304;&#22810;&#26679;&#24615;&#21644;&#26356;&#31616;&#21333;&#30340;&#37096;&#32626;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#26041;&#27861;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#20363;&#22914;&#22266;&#23450;&#30340;&#25509;&#21463;&#22495;&#12289;&#32570;&#20047;&#20840;&#23616;&#20449;&#24687;&#20132;&#20114;&#65292;&#23548;&#33268; noticeable instance-level &#39640;&#24230;&#20559;&#24046;&#12290;&#39640;&#24230;&#39044;&#27979;&#30340;&#22266;&#26377;&#22797;&#26434;&#24615;&#21487;&#33021;&#23548;&#33268;&#20351;&#29992;&#22522;&#20110;&#22266;&#23450;&#39640;&#24230;&#20998;&#21106;&#30340;&#20027;&#27969;&#22238;&#24402;&#26041;&#27861;&#26102;&#27169;&#31946;&#20272;&#35745;&#29289;&#20307;&#36793;&#32536;&#28145;&#24230;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#33322;&#31354;&#24433;&#20687;&#30340;&#21333;&#30446;&#39640;&#24230;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Height estimation has long been a pivotal topic within measurement and remote sensing disciplines, proving critical for endeavours such as 3D urban modelling, MR and autonomous driving. Traditional methods utilise stereo matching or multisensor fusion, both well-established techniques that typically necessitate multiple images from varying perspectives and adjunct sensors like SAR, leading to substantial deployment costs. Single image height estimation has emerged as an attractive alternative, boasting a larger data source variety and simpler deployment. However, current methods suffer from limitations such as fixed receptive fields, a lack of global information interaction, leading to noticeable instance-level height deviations. The inherent complexity of height prediction can result in a blurry estimation of object edge depth when using mainstream regression methods based on fixed height division. This paper presents a comprehensive solution for monocular height estimation in remote 
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#32508;&#21512;&#31185;&#23398;&#25991;&#29486;&#20135;&#29983;&#30693;&#35782;&#65292;&#33021;&#22815;&#22312;&#31185;&#23398;&#32508;&#21512;&#12289;&#25512;&#29702;&#21644;&#35299;&#37322;&#20013;&#21457;&#25381;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.07984</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#31185;&#23398;&#32508;&#21512;&#12289;&#25512;&#29702;&#21644;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Large Language Models for Scientific Synthesis, Inference and Explanation. (arXiv:2310.07984v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07984
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#32508;&#21512;&#31185;&#23398;&#25991;&#29486;&#20135;&#29983;&#30693;&#35782;&#65292;&#33021;&#22815;&#22312;&#31185;&#23398;&#32508;&#21512;&#12289;&#25512;&#29702;&#21644;&#35299;&#37322;&#20013;&#21457;&#25381;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#19968;&#31181;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#20854;&#20027;&#35201;&#30693;&#35782;&#21253;&#25324;&#35821;&#35328;&#30340;&#32479;&#35745;&#27169;&#24335;&#12289;&#35821;&#20041;&#20851;&#31995;&#21644;&#21477;&#27861;&#32467;&#26500;&#12290;&#23613;&#31649;&#23427;&#20204;&#30340;"&#30693;&#35782;"&#24418;&#24335;&#26377;&#38480;&#65292;&#20294;&#36825;&#20123;&#31995;&#32479;&#25797;&#38271;&#21508;&#31181;&#22797;&#26434;&#20219;&#21153;&#65292;&#21253;&#25324;&#21019;&#24847;&#20889;&#20316;&#12289;&#25925;&#20107;&#35762;&#36848;&#12289;&#32763;&#35793;&#12289;&#38382;&#31572;&#12289;&#24635;&#32467;&#21644;&#29983;&#25104;&#35745;&#31639;&#26426;&#20195;&#30721;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23578;&#26410;&#22312;&#33258;&#28982;&#31185;&#23398;&#39046;&#22495;&#23637;&#31034;&#20986;&#39640;&#32423;&#24212;&#29992;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31185;&#23398;&#32508;&#21512;&#12289;&#25512;&#29702;&#21644;&#35299;&#37322;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36890;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#36890;&#24120;&#19982;&#29305;&#27530;&#29992;&#36884;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30456;&#20851;&#32852;&#30340;&#31185;&#23398;&#25968;&#25454;&#38598;&#20013;&#36827;&#34892;&#25512;&#29702;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#20174;&#31185;&#23398;&#25991;&#29486;&#20013;&#32508;&#21512;&#20135;&#29983;"&#30693;&#35782;"&#12290;&#24403;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#19982;&#36825;&#31181;&#21512;&#25104;&#21644;&#25512;&#29702;&#30340;&#30693;&#35782;&#30456;&#32467;&#21512;&#26102;&#65292;&#21487;&#20197;&#22686;&#24378;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models are a form of artificial intelligence systems whose primary knowledge consists of the statistical patterns, semantic relationships, and syntactical structures of language1. Despite their limited forms of "knowledge", these systems are adept at numerous complex tasks including creative writing, storytelling, translation, question-answering, summarization, and computer code generation. However, they have yet to demonstrate advanced applications in natural science. Here we show how large language models can perform scientific synthesis, inference, and explanation. We present a method for using general-purpose large language models to make inferences from scientific datasets of the form usually associated with special-purpose machine learning algorithms. We show that the large language model can augment this "knowledge" by synthesizing from the scientific literature. When a conventional machine learning system is augmented with this synthesized and inferred knowledge 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#35270;&#35273;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#26512;&#32593;&#32476;&#19978;&#30340;&#26538;&#25903;&#36208;&#31169;&#27963;&#21160;&#12290;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#20174;&#22823;&#35268;&#27169;&#36890;&#29992;&#25968;&#25454;&#38598;&#39044;&#35757;&#32451;&#65292;&#20877;&#22312;&#29305;&#23450;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#29616;&#23545;&#26538;&#25903;&#30340;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2310.07975</link><description>&lt;p&gt;
&#29992;&#20110;&#20998;&#26512;&#32593;&#32476;&#19978;&#30340;&#26538;&#25903;&#36208;&#31169;&#27963;&#21160;&#30340;&#33258;&#30417;&#30563;&#35270;&#35273;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Self-supervised visual learning for analyzing firearms trafficking activities on the Web. (arXiv:2310.07975v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07975
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#35270;&#35273;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#26512;&#32593;&#32476;&#19978;&#30340;&#26538;&#25903;&#36208;&#31169;&#27963;&#21160;&#12290;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#20174;&#22823;&#35268;&#27169;&#36890;&#29992;&#25968;&#25454;&#38598;&#39044;&#35757;&#32451;&#65292;&#20877;&#22312;&#29305;&#23450;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#29616;&#23545;&#26538;&#25903;&#30340;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;RGB&#22270;&#20687;&#20013;&#33258;&#21160;&#21270;&#22320;&#20998;&#31867;&#26538;&#25903;&#26159;&#19968;&#39033;&#37325;&#35201;&#30340;&#29616;&#23454;&#20219;&#21153;&#65292;&#20854;&#24212;&#29992;&#33539;&#22260;&#28041;&#21450;&#20844;&#20849;&#31354;&#38388;&#23433;&#20840;&#12289;&#24773;&#25253;&#25910;&#38598;&#21644;&#25191;&#27861;&#35843;&#26597;&#12290;&#24403;&#24212;&#29992;&#20110;&#20174;&#20840;&#29699;&#33539;&#22260;&#20869;&#22823;&#35268;&#27169;&#25235;&#21462;&#30340;&#22270;&#20687;&#65288;&#21253;&#25324;&#31038;&#20132;&#23186;&#20307;&#21644;&#26263;&#32593;&#31449;&#65289;&#26102;&#65292;&#23427;&#21487;&#20197;&#20316;&#20026;&#20998;&#26512;&#24320;&#25918;&#28304;&#24773;&#25253;&#20013;&#30340;&#22823;&#25968;&#25454;&#30340;&#31995;&#32479;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#20197;&#35797;&#22270;&#35782;&#21035;&#29359;&#32618;&#26538;&#25903;&#36208;&#31169;&#32593;&#32476;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#26159;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#20854;&#20013;&#36890;&#24120;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#12290;&#24120;&#35265;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#21253;&#25324;&#22312;&#22823;&#35268;&#27169;&#30340;&#36890;&#29992;&#27880;&#37322;&#25968;&#25454;&#38598;&#65288;&#22914;ImageNet-1k&#65289;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#36827;&#34892;&#25972;&#20307;&#22270;&#20687;&#20998;&#31867;&#65292;&#28982;&#21518;&#22312;&#36739;&#23567;&#30340;&#12289;&#27880;&#37322;&#30340;&#12289;&#20219;&#21153;&#29305;&#23450;&#30340;&#19979;&#28216;&#25968;&#25454;&#38598;&#19978;&#23545;DNN&#36827;&#34892;&#24494;&#35843;&#20197;&#36827;&#34892;&#26538;&#25903;&#35270;&#35273;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated visual firearms classification from RGB images is an important real-world task with applications in public space security, intelligence gathering and law enforcement investigations. When applied to images massively crawled from the World Wide Web (including social media and dark Web sites), it can serve as an important component of systems that attempt to identify criminal firearms trafficking networks, by analyzing Big Data from open-source intelligence. Deep Neural Networks (DNN) are the state-of-the-art methodology for achieving this, with Convolutional Neural Networks (CNN) being typically employed. The common transfer learning approach consists of pretraining on a large-scale, generic annotated dataset for whole-image classification, such as ImageNet-1k, and then finetuning the DNN on a smaller, annotated, task-specific, downstream dataset for visual firearms classification. Neither Visual Transformer (ViT) neural architectures nor Self-Supervised Learning (SSL) approach
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35266;&#23519;&#25193;&#25955;&#21644;&#20449;&#24687;&#20998;&#35299;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25581;&#31034;&#20102;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#21040;&#30340;&#32454;&#31890;&#24230;&#20851;&#31995;&#65292;&#36827;&#19968;&#27493;&#35299;&#20915;&#20102;&#39640;&#32500;&#31354;&#38388;&#20013;&#20449;&#24687;&#25658;&#24102;&#21464;&#37327;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.07972</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#25193;&#25955;&#27169;&#22411;&#36890;&#36807;&#20449;&#24687;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
Interpretable Diffusion via Information Decomposition. (arXiv:2310.07972v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07972
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35266;&#23519;&#25193;&#25955;&#21644;&#20449;&#24687;&#20998;&#35299;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25581;&#31034;&#20102;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#21040;&#30340;&#32454;&#31890;&#24230;&#20851;&#31995;&#65292;&#36827;&#19968;&#27493;&#35299;&#20915;&#20102;&#39640;&#32500;&#31354;&#38388;&#20013;&#20449;&#24687;&#25658;&#24102;&#21464;&#37327;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#33021;&#22815;&#29992;&#20110;&#22797;&#26434;&#20851;&#31995;&#30340;&#26465;&#20214;&#29983;&#25104;&#21644;&#23494;&#24230;&#24314;&#27169;&#65292;&#22914;&#22270;&#20687;&#21644;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#23398;&#20064;&#21040;&#30340;&#20851;&#31995;&#30340;&#26412;&#36136;&#26159;&#19981;&#36879;&#26126;&#30340;&#65292;&#22240;&#27492;&#24456;&#38590;&#20934;&#30830;&#29702;&#35299;&#21333;&#35789;&#21644;&#22270;&#20687;&#37096;&#20998;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25110;&#32773;&#39044;&#27979;&#24178;&#39044;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#36890;&#36807;&#35266;&#23519;&#25193;&#25955;&#21644;&#20449;&#24687;&#20998;&#35299;&#20043;&#38388;&#30340;&#31934;&#30830;&#20851;&#31995;&#65292;&#25581;&#31034;&#20102;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#21040;&#30340;&#32454;&#31890;&#24230;&#20851;&#31995;&#12290;&#20114;&#20449;&#24687;&#21644;&#26465;&#20214;&#20114;&#20449;&#24687;&#30340;&#31934;&#30830;&#34920;&#36798;&#21487;&#20197;&#36890;&#36807;&#21435;&#22122;&#27169;&#22411;&#26469;&#35745;&#31639;&#12290;&#27492;&#22806;&#65292;&#20063;&#21487;&#20197;&#36731;&#26494;&#20272;&#35745;&#22312;&#29305;&#23450;&#22270;&#20687;&#21644;&#26631;&#39064;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#36827;&#19968;&#27493;&#23545;&#20449;&#24687;&#36827;&#34892;&#20998;&#35299;&#65292;&#20197;&#29702;&#35299;&#39640;&#32500;&#31354;&#38388;&#20013;&#21738;&#20123;&#21464;&#37327;&#25658;&#24102;&#20449;&#24687;&#65292;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#23545;&#20110;&#25193;&#25955;&#27169;&#22411;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#33258;&#28982;&#30340;&#38750;&#36127;&#20449;&#24687;&#20998;&#35299;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Denoising diffusion models enable conditional generation and density modeling of complex relationships like images and text. However, the nature of the learned relationships is opaque making it difficult to understand precisely what relationships between words and parts of an image are captured, or to predict the effect of an intervention. We illuminate the fine-grained relationships learned by diffusion models by noticing a precise relationship between diffusion and information decomposition. Exact expressions for mutual information and conditional mutual information can be written in terms of the denoising model. Furthermore, pointwise estimates can be easily estimated as well, allowing us to ask questions about the relationships between specific images and captions. Decomposing information even further to understand which variables in a high-dimensional space carry information is a long-standing problem. For diffusion models, we show that a natural non-negative decomposition of mutu
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#24212;&#23545;&#30740;&#31350;&#27700;&#24179;&#30340;&#25968;&#23398;&#33258;&#21160;&#24418;&#24335;&#21270;&#20219;&#21153;&#65292;&#36890;&#36807;&#23558;&#20219;&#21153;&#20998;&#35299;&#25104;&#26356;&#23481;&#26131;&#22788;&#29702;&#30340;&#23376;&#20219;&#21153;&#65292;&#21253;&#25324;&#26410;&#38142;&#25509;&#24418;&#24335;&#21270;&#12289;&#23454;&#20307;&#38142;&#25509;&#21644;&#31867;&#22411;&#35843;&#25972;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#26410;&#38142;&#25509;&#24418;&#24335;&#21270;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598; arXiv2Formal&#12290;</title><link>http://arxiv.org/abs/2310.07957</link><description>&lt;p&gt;
&#19968;&#31181;&#26032;&#30340;&#33258;&#21160;&#24418;&#24335;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A New Approach Towards Autoformalization. (arXiv:2310.07957v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07957
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#24212;&#23545;&#30740;&#31350;&#27700;&#24179;&#30340;&#25968;&#23398;&#33258;&#21160;&#24418;&#24335;&#21270;&#20219;&#21153;&#65292;&#36890;&#36807;&#23558;&#20219;&#21153;&#20998;&#35299;&#25104;&#26356;&#23481;&#26131;&#22788;&#29702;&#30340;&#23376;&#20219;&#21153;&#65292;&#21253;&#25324;&#26410;&#38142;&#25509;&#24418;&#24335;&#21270;&#12289;&#23454;&#20307;&#38142;&#25509;&#21644;&#31867;&#22411;&#35843;&#25972;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#26410;&#38142;&#25509;&#24418;&#24335;&#21270;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598; arXiv2Formal&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39564;&#35777;&#25968;&#23398;&#35777;&#26126;&#26159;&#22256;&#38590;&#30340;&#65292;&#20294;&#21487;&#20197;&#36890;&#36807;&#35745;&#31639;&#26426;&#30340;&#36741;&#21161;&#23454;&#29616;&#33258;&#21160;&#21270;&#12290;&#33258;&#21160;&#24418;&#24335;&#21270;&#26159;&#23558;&#33258;&#28982;&#35821;&#35328;&#25968;&#23398;&#33258;&#21160;&#36716;&#21270;&#20026;&#21487;&#20197;&#30001;&#31243;&#24207;&#39564;&#35777;&#30340;&#24418;&#24335;&#35821;&#35328;&#30340;&#20219;&#21153;&#12290;&#36825;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#23588;&#20854;&#23545;&#20110;&#30740;&#31350;&#35770;&#25991;&#20013;&#30340;&#39640;&#32423;&#25968;&#23398;&#26469;&#35828;&#12290;&#30740;&#31350;&#35770;&#25991;&#20013;&#30340;&#25968;&#23398;&#38656;&#35201;&#22823;&#37327;&#30340;&#32972;&#26223;&#21644;&#19978;&#19979;&#25991;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24212;&#23545;&#30740;&#31350;&#27700;&#24179;&#25968;&#23398;&#33258;&#21160;&#24418;&#24335;&#21270;&#30340;&#26041;&#27861;&#65292;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#26356;&#26131;&#20110;&#22788;&#29702;&#30340;&#23376;&#20219;&#21153;&#65306;&#26410;&#38142;&#25509;&#24418;&#24335;&#21270;&#65288;&#21253;&#21547;&#26410;&#38142;&#25509;&#30340;&#23450;&#20041;&#21644;&#23450;&#29702;&#30340;&#24418;&#24335;&#21270;&#65289;&#12289;&#23454;&#20307;&#38142;&#25509;&#65288;&#38142;&#25509;&#21040;&#27491;&#30830;&#30340;&#23450;&#29702;&#21644;&#23450;&#20041;&#65289;&#20197;&#21450;&#35843;&#25972;&#31867;&#22411;&#20197;&#36890;&#36807;&#31867;&#22411;&#26816;&#26597;&#22120;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;arXiv2Formal&#65292;&#19968;&#20010;&#29992;&#20110;&#26410;&#38142;&#25509;&#24418;&#24335;&#21270;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;&#20174;arXiv.org&#30340;&#35770;&#25991;&#20013;&#25277;&#21462;&#30340;50&#20010;&#23450;&#29702;&#22312;Lean&#23450;&#29702;&#35777;&#26126;&#22120;&#20013;&#36827;&#34892;&#24418;&#24335;&#21270;&#12290;&#25105;&#20204;&#27426;&#36814;&#20219;&#20309;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Verifying mathematical proofs is difficult, but can be automated with the assistance of a computer. Autoformalization is the task of automatically translating natural language mathematics into a formal language that can be verified by a program. This is a challenging task, and especially for higher-level mathematics found in research papers. Research paper mathematics requires large amounts of background and context. In this paper, we propose an avenue towards tackling autoformalization for research-level mathematics, by breaking the task into easier and more approachable subtasks: unlinked formalization (formalization with unlinked definitions and theorems), entity linking (linking to the proper theorems and definitions), and finally adjusting types so it passes the type checker. In addition, we present arXiv2Formal, a benchmark dataset for unlinked formalization consisting of 50 theorems formalized for the Lean theorem prover sampled from papers on arXiv.org. We welcome any contribut
&lt;/p&gt;</description></item><item><title>AutoRepo&#26159;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#21033;&#29992;&#26080;&#20154;&#39550;&#39542;&#39134;&#34892;&#22120;&#21644;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#33258;&#21160;&#29983;&#25104;&#24314;&#31569;&#26816;&#26597;&#25253;&#21578;&#65292;&#25552;&#39640;&#20102;&#26816;&#26597;&#25928;&#29575;&#21644;&#36164;&#28304;&#21033;&#29992;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.07944</link><description>&lt;p&gt;
AutoRepo&#65306;&#19968;&#31181;&#29992;&#20110;&#22810;&#27169;&#24577;LLM&#33258;&#21160;&#24314;&#35774;&#25253;&#21578;&#30340;&#36890;&#29992;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
AutoRepo: A general framework for multi-modal LLM-based automated construction reporting. (arXiv:2310.07944v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07944
&lt;/p&gt;
&lt;p&gt;
AutoRepo&#26159;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#21033;&#29992;&#26080;&#20154;&#39550;&#39542;&#39134;&#34892;&#22120;&#21644;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#33258;&#21160;&#29983;&#25104;&#24314;&#31569;&#26816;&#26597;&#25253;&#21578;&#65292;&#25552;&#39640;&#20102;&#26816;&#26597;&#25928;&#29575;&#21644;&#36164;&#28304;&#21033;&#29992;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30830;&#20445;&#24314;&#31569;&#39033;&#30446;&#30340;&#23433;&#20840;&#12289;&#36136;&#37327;&#21644;&#21450;&#26102;&#23436;&#25104;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#24314;&#31569;&#26816;&#26597;&#26159;&#23454;&#29616;&#36825;&#20123;&#30446;&#26631;&#30340;&#37325;&#35201;&#25163;&#27573;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#26816;&#26597;&#26041;&#27861;&#20027;&#35201;&#26159;&#25163;&#21160;&#30340;&#65292;&#32463;&#24120;&#23548;&#33268;&#25928;&#29575;&#20302;&#19979;&#21644;&#20449;&#24687;&#31649;&#29702;&#19981;&#24403;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#24120;&#19981;&#33021;&#25552;&#20379;&#20840;&#38754;&#12289;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#22240;&#27492;&#23481;&#26131;&#24341;&#21457;&#30417;&#31649;&#30095;&#24573;&#21644;&#28508;&#22312;&#30340;&#23433;&#20840;&#38544;&#24739;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AutoRepo&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#24314;&#31569;&#26816;&#26597;&#25253;&#21578;&#12290;&#26080;&#20154;&#39550;&#39542;&#39134;&#34892;&#22120;&#39640;&#25928;&#22320;&#36827;&#34892;&#24314;&#31569;&#26816;&#26597;&#24182;&#25910;&#38598;&#22330;&#26223;&#20449;&#24687;&#65292;&#32780;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21017;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#26816;&#26597;&#25253;&#21578;&#12290;&#35813;&#26694;&#26550;&#22312;&#19968;&#20010;&#30495;&#23454;&#30340;&#24314;&#31569;&#24037;&#22320;&#19978;&#24212;&#29992;&#21644;&#27979;&#35797;&#65292;&#23637;&#31034;&#20102;&#23427;&#21152;&#36895;&#26816;&#26597;&#27969;&#31243;&#12289;&#26174;&#33879;&#20943;&#23569;&#36164;&#28304;&#20998;&#37197;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ensuring the safety, quality, and timely completion of construction projects is paramount, with construction inspections serving as a vital instrument towards these goals. Nevertheless, the predominantly manual approach of present-day inspections frequently results in inefficiencies and inadequate information management. Such methods often fall short of providing holistic, exhaustive assessments, consequently engendering regulatory oversights and potential safety hazards. To address this issue, this paper presents a novel framework named AutoRepo for automated generation of construction inspection reports. The unmanned vehicles efficiently perform construction inspections and collect scene information, while the multimodal large language models (LLMs) are leveraged to automatically generate the inspection reports. The framework was applied and tested on a real-world construction site, demonstrating its potential to expedite the inspection process, significantly reduce resource allocati
&lt;/p&gt;</description></item><item><title>Co-NavGPT&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20840;&#23616;&#35268;&#21010;&#22120;&#65292;&#23454;&#29616;&#22810;&#26426;&#22120;&#20154;&#21512;&#20316;&#30340;&#35270;&#35273;&#30446;&#26631;&#23548;&#33322;&#12290;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20102;&#36229;&#36234;&#29616;&#26377;&#27169;&#22411;&#30340;&#25104;&#21151;&#29575;&#21644;&#25928;&#29575;&#65292;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.07937</link><description>&lt;p&gt;
Co-NavGPT: &#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#26426;&#22120;&#20154;&#21512;&#20316;&#35270;&#35273;&#35821;&#20041;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
Co-NavGPT: Multi-Robot Cooperative Visual Semantic Navigation using Large Language Models. (arXiv:2310.07937v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07937
&lt;/p&gt;
&lt;p&gt;
Co-NavGPT&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20840;&#23616;&#35268;&#21010;&#22120;&#65292;&#23454;&#29616;&#22810;&#26426;&#22120;&#20154;&#21512;&#20316;&#30340;&#35270;&#35273;&#30446;&#26631;&#23548;&#33322;&#12290;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20102;&#36229;&#36234;&#29616;&#26377;&#27169;&#22411;&#30340;&#25104;&#21151;&#29575;&#21644;&#25928;&#29575;&#65292;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#32423;&#20154;&#26426;&#20132;&#20114;&#20219;&#21153;&#20013;&#65292;&#23545;&#20110;&#33258;&#20027;&#26426;&#22120;&#20154;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#36827;&#34892;&#35270;&#35273;&#30446;&#26631;&#23548;&#33322;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#36807;&#21435;&#24050;&#32463;&#24320;&#21457;&#20102;&#35768;&#22810;&#26041;&#27861;&#65292;&#20294;&#22823;&#22810;&#25968;&#37117;&#26159;&#35774;&#35745;&#29992;&#20110;&#21333;&#19968;&#26426;&#22120;&#20154;&#25805;&#20316;&#65292;&#36825;&#24448;&#24448;&#30001;&#20110;&#29615;&#22659;&#22797;&#26434;&#24615;&#32780;&#23548;&#33268;&#25928;&#29575;&#21644;&#40065;&#26834;&#24615;&#38477;&#20302;&#12290;&#27492;&#22806;&#65292;&#23398;&#20064;&#22810;&#26426;&#22120;&#20154;&#21327;&#20316;&#30340;&#31574;&#30053;&#38656;&#35201;&#36164;&#28304;&#23494;&#38598;&#22411;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Co-NavGPT&#65292;&#36825;&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20316;&#20026;&#22810;&#26426;&#22120;&#20154;&#21512;&#20316;&#35270;&#35273;&#30446;&#26631;&#23548;&#33322;&#30340;&#20840;&#23616;&#35268;&#21010;&#22120;&#12290;Co-NavGPT&#23558;&#25506;&#32034;&#30340;&#29615;&#22659;&#25968;&#25454;&#32534;&#30721;&#20026;&#25552;&#31034;&#65292;&#22686;&#24378;LLMs&#23545;&#22330;&#26223;&#30340;&#29702;&#35299;&#12290;&#28982;&#21518;&#65292;&#23427;&#20026;&#27599;&#20010;&#26426;&#22120;&#20154;&#20998;&#37197;&#25506;&#32034;&#21069;&#27839;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#30446;&#26631;&#25628;&#32034;&#12290;&#22312;Habitat-Matterport 3D (HM3D)&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Co-NavGPT&#22312;&#25104;&#21151;&#29575;&#21644;&#25928;&#29575;&#26041;&#38754;&#36229;&#36807;&#20102;&#29616;&#26377;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#23398;&#20064;&#36807;&#31243;&#65292;&#23637;&#31034;&#20102;LLMs&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In advanced human-robot interaction tasks, visual target navigation is crucial for autonomous robots navigating unknown environments. While numerous approaches have been developed in the past, most are designed for single-robot operations, which often suffer from reduced efficiency and robustness due to environmental complexities. Furthermore, learning policies for multi-robot collaboration are resource-intensive. To address these challenges, we propose Co-NavGPT, an innovative framework that integrates Large Language Models (LLMs) as a global planner for multi-robot cooperative visual target navigation. Co-NavGPT encodes the explored environment data into prompts, enhancing LLMs' scene comprehension. It then assigns exploration frontiers to each robot for efficient target search. Experimental results on Habitat-Matterport 3D (HM3D) demonstrate that Co-NavGPT surpasses existing models in success rates and efficiency without any learning process, demonstrating the vast potential of LLMs
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RAPL&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#26426;&#22120;&#20154;&#23398;&#20064;&#20013;&#30340;&#35270;&#35273;&#34920;&#31034;&#23545;&#40784;&#38382;&#39064;&#65292;&#36890;&#36807;&#21033;&#29992;&#20154;&#31867;&#21453;&#39304;&#23558;&#26426;&#22120;&#20154;&#30340;&#35270;&#35273;&#34920;&#31034;&#19982;&#29992;&#25143;&#20559;&#22909;&#23545;&#40784;&#65292;&#24182;&#21306;&#20998;&#20219;&#21153;&#30340;&#20851;&#38190;&#35201;&#32032;&#12290;</title><link>http://arxiv.org/abs/2310.07932</link><description>&lt;p&gt;
&#20320;&#20851;&#24515;&#20160;&#20040;&#65311;&#20026;&#26426;&#22120;&#20154;&#23398;&#20064;&#23454;&#29616;&#35270;&#35273;&#34920;&#31034;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
What Matters to You? Towards Visual Representation Alignment for Robot Learning. (arXiv:2310.07932v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07932
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RAPL&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#26426;&#22120;&#20154;&#23398;&#20064;&#20013;&#30340;&#35270;&#35273;&#34920;&#31034;&#23545;&#40784;&#38382;&#39064;&#65292;&#36890;&#36807;&#21033;&#29992;&#20154;&#31867;&#21453;&#39304;&#23558;&#26426;&#22120;&#20154;&#30340;&#35270;&#35273;&#34920;&#31034;&#19982;&#29992;&#25143;&#20559;&#22909;&#23545;&#40784;&#65292;&#24182;&#21306;&#20998;&#20219;&#21153;&#30340;&#20851;&#38190;&#35201;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20026;&#20154;&#31867;&#26381;&#21153;&#26102;&#65292;&#26426;&#22120;&#20154;&#38656;&#35201;&#20248;&#21270;&#19982;&#26368;&#32456;&#29992;&#25143;&#20559;&#22909;&#19968;&#33268;&#30340;&#22870;&#21169;&#12290;&#30001;&#20110;&#26426;&#22120;&#20154;&#23558;&#20381;&#36182;&#21407;&#22987;&#24863;&#30693;&#36755;&#20837;&#22914;RGB&#22270;&#20687;&#65292;&#23427;&#20204;&#30340;&#22870;&#21169;&#23558;&#19981;&#21487;&#36991;&#20813;&#22320;&#20351;&#29992;&#35270;&#35273;&#34920;&#31034;&#12290;&#26368;&#36817;&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#35270;&#35273;&#27169;&#22411;&#30340;&#34920;&#31034;&#24341;&#21457;&#20102;&#20154;&#20204;&#30340;&#20852;&#36259;&#65292;&#20294;&#22312;&#26426;&#22120;&#20154;&#39046;&#22495;&#20351;&#20854;&#36215;&#20316;&#29992;&#30340;&#20851;&#38190;&#26159;&#24494;&#35843;&#65292;&#36890;&#24120;&#36890;&#36807;&#20195;&#29702;&#20219;&#21153;&#22914;&#21160;&#21147;&#23398;&#39044;&#27979;&#25110;&#24378;&#21046;&#26102;&#38388;&#24490;&#29615;&#19968;&#33268;&#24615;&#26469;&#23436;&#25104;&#12290;&#28982;&#32780;&#65292;&#25152;&#26377;&#36825;&#20123;&#20195;&#29702;&#20219;&#21153;&#37117;&#32469;&#36807;&#20102;&#20154;&#31867;&#23545;&#33258;&#24049;&#20851;&#24515;&#30340;&#20107;&#29289;&#30340;&#36755;&#20837;&#65292;&#21152;&#21095;&#20102;&#34394;&#20551;&#20851;&#32852;&#65292;&#24182;&#26368;&#32456;&#23548;&#33268;&#26426;&#22120;&#20154;&#30340;&#34892;&#20026;&#19982;&#29992;&#25143;&#20559;&#22909;&#19981;&#19968;&#33268;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#35758;&#26426;&#22120;&#20154;&#24212;&#35813;&#21033;&#29992;&#20154;&#31867;&#30340;&#21453;&#39304;&#26469;&#19982;&#26368;&#32456;&#29992;&#25143;&#30340;&#35270;&#35273;&#34920;&#31034;&#23545;&#40784;&#65292;&#24182;&#21306;&#20998;&#20219;&#21153;&#30340;&#20851;&#38190;&#35201;&#32032;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#35270;&#35273;&#34920;&#31034;&#23545;&#40784;&#38382;&#39064;&#21644;&#35270;&#35273;&#22870;&#21169;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#21363;&#22522;&#20110;&#20559;&#22909;&#30340;&#34920;&#31034;&#23545;&#40784;&#23398;&#20064;&#65288;RAPL&#65289;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
When operating in service of people, robots need to optimize rewards aligned with end-user preferences. Since robots will rely on raw perceptual inputs like RGB images, their rewards will inevitably use visual representations. Recently there has been excitement in using representations from pre-trained visual models, but key to making these work in robotics is fine-tuning, which is typically done via proxy tasks like dynamics prediction or enforcing temporal cycle-consistency. However, all these proxy tasks bypass the human's input on what matters to them, exacerbating spurious correlations and ultimately leading to robot behaviors that are misaligned with user preferences. In this work, we propose that robots should leverage human feedback to align their visual representations with the end-user and disentangle what matters for the task. We propose Representation-Aligned Preference-based Learning (RAPL), a method for solving the visual representation alignment problem and visual reward
&lt;/p&gt;</description></item><item><title>D2&#20462;&#21098;&#26159;&#19968;&#31181;&#24179;&#34913;&#25968;&#25454;&#22810;&#26679;&#24615;&#21644;&#22256;&#38590;&#24230;&#30340;&#26041;&#27861;&#65292;&#22312;coreset&#36873;&#25321;&#20013;&#21516;&#26102;&#32771;&#34385;&#25968;&#25454;&#22810;&#26679;&#24615;&#21644;&#37325;&#35201;&#24615;&#35780;&#20998;&#12290;</title><link>http://arxiv.org/abs/2310.07931</link><description>&lt;p&gt;
D2&#20462;&#21098;&#65306;&#20449;&#24687;&#20256;&#36882;&#24179;&#34913;&#25968;&#25454;&#20462;&#21098;&#20013;&#30340;&#22810;&#26679;&#24615;&#21644;&#22256;&#38590;
&lt;/p&gt;
&lt;p&gt;
D2 Pruning: Message Passing for Balancing Diversity and Difficulty in Data Pruning. (arXiv:2310.07931v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07931
&lt;/p&gt;
&lt;p&gt;
D2&#20462;&#21098;&#26159;&#19968;&#31181;&#24179;&#34913;&#25968;&#25454;&#22810;&#26679;&#24615;&#21644;&#22256;&#38590;&#24230;&#30340;&#26041;&#27861;&#65292;&#22312;coreset&#36873;&#25321;&#20013;&#21516;&#26102;&#32771;&#34385;&#25968;&#25454;&#22810;&#26679;&#24615;&#21644;&#37325;&#35201;&#24615;&#35780;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#26512;&#29702;&#35770;&#34920;&#26126;&#65292;&#22312;&#22266;&#23450;&#25968;&#25454;&#39044;&#31639;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#20013;&#65292;&#26356;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#21487;&#20197;&#23548;&#33268;&#26356;&#20302;&#30340;&#27979;&#35797;&#38169;&#35823;&#12290;&#27492;&#22806;&#65292;&#22914;&#26524;&#25968;&#25454;&#38598;&#21487;&#20197;&#21093;&#31163;&#20887;&#20313;&#39033;&#65292;&#21017;&#21487;&#20197;&#22312;&#36739;&#20302;&#30340;&#35745;&#31639;&#39044;&#31639;&#19978;&#35757;&#32451;&#27169;&#22411;&#32780;&#19981;&#38477;&#20302;&#24615;&#33021;&#12290;Coreset&#36873;&#25321;&#65288;&#25110;&#25968;&#25454;&#20462;&#21098;&#65289;&#23547;&#27714;&#36873;&#25321;&#35757;&#32451;&#25968;&#25454;&#30340;&#23376;&#38598;&#65292;&#20197;&#26368;&#22823;&#31243;&#24230;&#22320;&#25552;&#39640;&#22312;&#35813;&#23376;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20063;&#31216;&#20026;coreset&#12290;&#26377;&#20004;&#31181;&#20027;&#35201;&#26041;&#27861;&#65306;&#65288;1&#65289;&#22522;&#20110;&#20960;&#20309;&#30340;&#25968;&#25454;&#36873;&#21462;&#65292;&#20197;&#26368;&#22823;&#31243;&#24230;&#22320;&#25552;&#39640;coreset&#20013;&#30340;&#25968;&#25454;&#22810;&#26679;&#24615;&#65292;&#21644;&#65288;2&#65289;&#26681;&#25454;&#35757;&#32451;&#21160;&#24577;&#20026;&#26679;&#26412;&#20998;&#37197;&#22256;&#38590;&#24230;&#20998;&#25968;&#30340;&#20989;&#25968;&#12290;&#20026;&#25968;&#25454;&#22810;&#26679;&#24615;&#36827;&#34892;&#20248;&#21270;&#20250;&#23548;&#33268;&#20559;&#21521;&#36739;&#23481;&#26131;&#26679;&#26412;&#30340;coreset&#65292;&#32780;&#38590;&#24230;&#25490;&#21517;&#36873;&#25321;&#20250;&#24573;&#30053;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#25152;&#24517;&#38656;&#30340;&#23481;&#26131;&#26679;&#26412;&#12290;&#36825;&#34920;&#26126;&#25968;&#25454;&#22810;&#26679;&#24615;&#21644;&#37325;&#35201;&#24615;&#35780;&#20998;&#26159;&#20004;&#20010;&#20114;&#34917;&#22240;&#32032;&#65292;&#22312;coreset&#36873;&#25321;&#20013;&#38656;&#35201;&#21516;&#26102;&#32771;&#34385;&#12290;
&lt;/p&gt;
&lt;p&gt;
Analytical theories suggest that higher-quality data can lead to lower test errors in models trained on a fixed data budget. Moreover, a model can be trained on a lower compute budget without compromising performance if a dataset can be stripped of its redundancies. Coreset selection (or data pruning) seeks to select a subset of the training data so as to maximize the performance of models trained on this subset, also referred to as coreset. There are two dominant approaches: (1) geometry-based data selection for maximizing data diversity in the coreset, and (2) functions that assign difficulty scores to samples based on training dynamics. Optimizing for data diversity leads to a coreset that is biased towards easier samples, whereas, selection by difficulty ranking omits easy samples that are necessary for the training of deep learning models. This demonstrates that data diversity and importance scores are two complementary factors that need to be jointly considered during coreset sel
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#21270;&#25919;&#31574;&#24674;&#22797;&#26041;&#27861;&#29992;&#20110;&#24314;&#27169;&#22797;&#26434;&#30340;&#21307;&#30103;&#20915;&#31574;&#36807;&#31243;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#23558;&#20915;&#31574;&#31574;&#30053;&#25286;&#20998;&#20026;&#19978;&#19979;&#25991;&#29305;&#23450;&#31574;&#30053;&#65292;&#36890;&#36807;&#22810;&#20219;&#21153;&#23398;&#20064;&#26469;&#23454;&#29616;&#24314;&#27169;&#65292;&#24182;&#25552;&#20379;&#22797;&#26434;&#34892;&#20026;&#30340;&#31616;&#27905;&#25551;&#36848;&#12290;</title><link>http://arxiv.org/abs/2310.07918</link><description>&lt;p&gt;
&#19978;&#19979;&#25991;&#21270;&#25919;&#31574;&#24674;&#22797;&#65306;&#36890;&#36807;&#33258;&#36866;&#24212;&#27169;&#20223;&#23398;&#20064;&#23545;&#21307;&#30103;&#20915;&#31574;&#36827;&#34892;&#24314;&#27169;&#21644;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Contextualized Policy Recovery: Modeling and Interpreting Medical Decisions with Adaptive Imitation Learning. (arXiv:2310.07918v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07918
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#21270;&#25919;&#31574;&#24674;&#22797;&#26041;&#27861;&#29992;&#20110;&#24314;&#27169;&#22797;&#26434;&#30340;&#21307;&#30103;&#20915;&#31574;&#36807;&#31243;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#23558;&#20915;&#31574;&#31574;&#30053;&#25286;&#20998;&#20026;&#19978;&#19979;&#25991;&#29305;&#23450;&#31574;&#30053;&#65292;&#36890;&#36807;&#22810;&#20219;&#21153;&#23398;&#20064;&#26469;&#23454;&#29616;&#24314;&#27169;&#65292;&#24182;&#25552;&#20379;&#22797;&#26434;&#34892;&#20026;&#30340;&#31616;&#27905;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#31574;&#30053;&#23398;&#20064;&#26088;&#22312;&#20174;&#35266;&#23519;&#21040;&#30340;&#34892;&#20026;&#20013;&#20272;&#35745;&#21487;&#29702;&#35299;&#30340;&#20915;&#31574;&#31574;&#30053;&#65307;&#28982;&#32780;&#65292;&#29616;&#26377;&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#12290;&#36825;&#31181;&#26435;&#34913;&#38480;&#21046;&#20102;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#23545;&#20154;&#31867;&#20915;&#31574;&#36807;&#31243;&#30340;&#35299;&#37322;&#65292;&#20363;&#22914;&#65292;&#23457;&#35745;&#21307;&#30103;&#20915;&#31574;&#30340;&#20559;&#35265;&#21644;&#27425;&#20248;&#23454;&#36341;&#65292;&#25105;&#20204;&#38656;&#35201;&#20915;&#31574;&#36807;&#31243;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#25552;&#20379;&#22797;&#26434;&#34892;&#20026;&#30340;&#31616;&#27905;&#25551;&#36848;&#12290;&#29616;&#26377;&#26041;&#27861;&#22522;&#26412;&#19978;&#30001;&#20110;&#23558;&#28508;&#22312;&#20915;&#31574;&#36807;&#31243;&#34920;&#31034;&#20026;&#36890;&#29992;&#31574;&#30053;&#32780;&#36127;&#25285;&#20102;&#36825;&#31181;&#26435;&#34913;&#65292;&#32780;&#23454;&#38469;&#19978;&#20154;&#31867;&#20915;&#31574;&#26159;&#21160;&#24577;&#30340;&#65292;&#21487;&#20197;&#38543;&#19978;&#19979;&#25991;&#20449;&#24687;&#32780;&#22823;&#24133;&#25913;&#21464;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19978;&#19979;&#25991;&#21270;&#25919;&#31574;&#24674;&#22797;&#65288;CPR&#65289;&#65292;&#23558;&#24314;&#27169;&#22797;&#26434;&#20915;&#31574;&#36807;&#31243;&#30340;&#38382;&#39064;&#37325;&#26032;&#23450;&#20041;&#20026;&#22810;&#20219;&#21153;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#20013;&#22797;&#26434;&#20915;&#31574;&#31574;&#30053;&#30001;&#29305;&#23450;&#19978;&#19979;&#25991;&#30340;&#31574;&#30053;&#32452;&#25104;&#12290;CPR&#23558;&#27599;&#20010;&#19978;&#19979;&#25991;&#29305;&#23450;&#31574;&#30053;&#24314;&#27169;&#20026;&#32447;&#24615;&#30340;&#35266;&#23519;-&#21160;&#20316;&#26144;&#23556;
&lt;/p&gt;
&lt;p&gt;
Interpretable policy learning seeks to estimate intelligible decision policies from observed actions; however, existing models fall short by forcing a tradeoff between accuracy and interpretability. This tradeoff limits data-driven interpretations of human decision-making process. e.g. to audit medical decisions for biases and suboptimal practices, we require models of decision processes which provide concise descriptions of complex behaviors. Fundamentally, existing approaches are burdened by this tradeoff because they represent the underlying decision process as a universal policy, when in fact human decisions are dynamic and can change drastically with contextual information. Thus, we propose Contextualized Policy Recovery (CPR), which re-frames the problem of modeling complex decision processes as a multi-task learning problem in which complex decision policies are comprised of context-specific policies. CPR models each context-specific policy as a linear observation-to-action mapp
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#22312;&#38750;&#24179;&#34913;&#25968;&#25454;&#20013;&#20351;&#29992;&#30340;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#36890;&#29992;&#25351;&#21335;&#65292;&#26088;&#22312;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#22312;&#22823;&#35268;&#27169;&#38750;&#24179;&#34913;&#25968;&#25454;&#20013;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2310.07917</link><description>&lt;p&gt;
&#22312;&#38750;&#24179;&#34913;&#25968;&#25454;&#21644;&#26410;&#26469;&#36235;&#21183;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Review of Machine Learning Techniques in Imbalanced Data and Future Trends. (arXiv:2310.07917v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07917
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#22312;&#38750;&#24179;&#34913;&#25968;&#25454;&#20013;&#20351;&#29992;&#30340;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#36890;&#29992;&#25351;&#21335;&#65292;&#26088;&#22312;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#22312;&#22823;&#35268;&#27169;&#38750;&#24179;&#34913;&#25968;&#25454;&#20013;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20108;&#21313;&#24180;&#37324;&#65292;&#26816;&#27979;&#32597;&#35265;&#20107;&#20214;&#19968;&#30452;&#26159;&#25968;&#25454;&#25366;&#25496;&#21644;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#19968;&#20010;&#25361;&#25112;&#24615;&#20219;&#21153;&#12290;&#29616;&#23454;&#29983;&#27963;&#20013;&#30340;&#38382;&#39064;&#28608;&#21457;&#20102;&#30740;&#31350;&#20154;&#21592;&#36827;&#19968;&#27493;&#25913;&#36827;&#25968;&#25454;&#22788;&#29702;&#21644;&#31639;&#27861;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#21644;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#38750;&#24179;&#34913;&#23398;&#20064;&#26041;&#27861;&#12290;&#26412;&#35770;&#25991;&#25910;&#38598;&#21644;&#23457;&#26597;&#20102;258&#31687;&#26469;&#33258;&#26399;&#21002;&#21644;&#20250;&#35758;&#35770;&#25991;&#30340;&#21516;&#34892;&#35780;&#23457;&#35770;&#25991;&#65292;&#26088;&#22312;&#20174;&#25216;&#26415;&#21644;&#24212;&#29992;&#35282;&#24230;&#28145;&#20837;&#23457;&#26597;&#38750;&#24179;&#34913;&#23398;&#20064;&#20013;&#30340;&#21508;&#31181;&#26041;&#27861;&#12290;&#35813;&#24037;&#20316;&#26088;&#22312;&#20026;&#22312;&#23398;&#26415;&#30028;&#25110;&#24037;&#19994;&#30028;&#24076;&#26395;&#28145;&#20837;&#23398;&#20064;&#22823;&#35268;&#27169;&#38750;&#24179;&#34913;&#25968;&#25454;&#19979;&#30340;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#19968;&#20010;&#32467;&#26500;&#21270;&#30340;&#26041;&#27861;&#32508;&#36848;&#65292;&#24182;&#20026;&#20182;&#20204;&#25552;&#20379;&#19968;&#20010;&#36890;&#29992;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;
For over two decades, detecting rare events has been a challenging task among researchers in the data mining and machine learning domain. Real-life problems inspire researchers to navigate and further improve data processing and algorithmic approaches to achieve effective and computationally efficient methods for imbalanced learning. In this paper, we have collected and reviewed 258 peer-reviewed papers from archival journals and conference papers in an attempt to provide an in-depth review of various approaches in imbalanced learning from technical and application perspectives. This work aims to provide a structured review of methods used to address the problem of imbalanced data in various domains and create a general guideline for researchers in academia or industry who want to dive into the broad field of machine learning using large-scale imbalanced data.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#30456;&#20301;&#21464;&#21270;&#35782;&#21035;&#27169;&#24335;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#26426;&#21046;&#65292;&#24182;&#36890;&#36807;&#39564;&#35777;&#25163;&#24037;&#21046;&#20316;&#30340;&#25391;&#33633;&#27169;&#22411;&#35777;&#23454;&#20102;&#36825;&#19968;&#35299;&#37322;&#12290;&#35813;&#30740;&#31350;&#19981;&#20165;&#25552;&#20379;&#20102;&#19968;&#31181;&#28508;&#22312;&#30340;&#21160;&#21147;&#23398;&#26426;&#21046;&#29992;&#20110;&#27169;&#24335;&#35782;&#21035;&#65292;&#36824;&#26263;&#31034;&#20102;&#26377;&#38480;&#29366;&#24577;&#33258;&#21160;&#26426;&#30340;&#31070;&#32463;&#23454;&#29616;&#26041;&#24335;&#65292;&#24182;&#19988;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#36827;&#34892;&#20102;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2310.07908</link><description>&lt;p&gt;
&#24490;&#29615;&#32593;&#32476;&#36890;&#36807;&#20302;&#32500;&#25391;&#33633;&#35782;&#21035;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Recurrent networks recognize patterns with low-dimensional oscillations. (arXiv:2310.07908v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07908
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#30456;&#20301;&#21464;&#21270;&#35782;&#21035;&#27169;&#24335;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#26426;&#21046;&#65292;&#24182;&#36890;&#36807;&#39564;&#35777;&#25163;&#24037;&#21046;&#20316;&#30340;&#25391;&#33633;&#27169;&#22411;&#35777;&#23454;&#20102;&#36825;&#19968;&#35299;&#37322;&#12290;&#35813;&#30740;&#31350;&#19981;&#20165;&#25552;&#20379;&#20102;&#19968;&#31181;&#28508;&#22312;&#30340;&#21160;&#21147;&#23398;&#26426;&#21046;&#29992;&#20110;&#27169;&#24335;&#35782;&#21035;&#65292;&#36824;&#26263;&#31034;&#20102;&#26377;&#38480;&#29366;&#24577;&#33258;&#21160;&#26426;&#30340;&#31070;&#32463;&#23454;&#29616;&#26041;&#24335;&#65292;&#24182;&#19988;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#36827;&#34892;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35299;&#37322;&#22312;SET&#21345;&#29260;&#28216;&#25103;&#21551;&#21457;&#19979;&#36827;&#34892;&#35757;&#32451;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;(RNN)&#22312;&#31616;&#21333;&#20219;&#21153;&#19978;&#30340;&#21160;&#21147;&#23398;&#26426;&#21046;&#26469;&#35782;&#21035;&#27169;&#24335;&#12290;&#25105;&#20204;&#23558;&#35757;&#32451;&#21518;&#30340;RNN&#35299;&#37322;&#20026;&#36890;&#36807;&#20302;&#32500;&#26497;&#38480;&#29615;&#20013;&#30340;&#30456;&#20301;&#21464;&#21270;&#36827;&#34892;&#27169;&#24335;&#35782;&#21035;&#65292;&#31867;&#20284;&#20110;&#26377;&#38480;&#29366;&#24577;&#33258;&#21160;&#26426;(FSA)&#20013;&#30340;&#36716;&#25442;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#25163;&#24037;&#21046;&#20316;&#19968;&#20010;&#31616;&#21333;&#30340;&#25391;&#33633;&#27169;&#22411;&#26469;&#39564;&#35777;&#20102;&#36825;&#19968;&#35299;&#37322;&#65292;&#35813;&#27169;&#22411;&#22797;&#21046;&#20102;&#35757;&#32451;&#21518;&#30340;RNN&#30340;&#21160;&#21147;&#23398;&#29305;&#24615;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#19981;&#20165;&#26263;&#31034;&#20102;&#19968;&#31181;&#28508;&#22312;&#30340;&#21160;&#21147;&#23398;&#26426;&#21046;&#33021;&#22815;&#23454;&#29616;&#27169;&#24335;&#35782;&#21035;&#65292;&#36824;&#26263;&#31034;&#20102;&#19968;&#31181;&#26377;&#38480;&#29366;&#24577;&#33258;&#21160;&#26426;&#30340;&#28508;&#22312;&#31070;&#32463;&#23454;&#29616;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#36825;&#39033;&#24037;&#20316;&#26377;&#21161;&#20110;&#20851;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#30340;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study proposes a novel dynamical mechanism for pattern recognition discovered by interpreting a recurrent neural network (RNN) trained on a simple task inspired by the SET card game. We interpreted the trained RNN as recognizing patterns via phase shifts in a low-dimensional limit cycle in a manner analogous to transitions in a finite state automaton (FSA). We further validated this interpretation by handcrafting a simple oscillatory model that reproduces the dynamics of the trained RNN. Our findings not only suggest of a potential dynamical mechanism capable of pattern recognition, but also suggest of a potential neural implementation of FSA. Above all, this work contributes to the growing discourse on deep learning model interpretability.
&lt;/p&gt;</description></item><item><title>RoboCLIP&#26159;&#19968;&#31181;&#22312;&#32447;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21333;&#20010;&#35270;&#39057;&#28436;&#31034;&#25110;&#25991;&#26412;&#25551;&#36848;&#29983;&#25104;&#22870;&#21169;&#20989;&#25968;&#65292;&#19981;&#38656;&#35201;&#19987;&#23478;&#30417;&#30563;&#25110;&#35774;&#35745;&#22870;&#21169;&#20989;&#25968;&#12290;&#21516;&#26102;&#65292;&#23427;&#36824;&#21487;&#20197;&#21033;&#29992;&#39046;&#22495;&#22806;&#30340;&#28436;&#31034;&#36827;&#34892;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2310.07899</link><description>&lt;p&gt;
RoboCLIP&#65306;&#21482;&#38656;&#35201;&#19968;&#27425;&#28436;&#31034;&#21363;&#21487;&#23398;&#20064;&#26426;&#22120;&#20154;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
RoboCLIP: One Demonstration is Enough to Learn Robot Policies. (arXiv:2310.07899v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07899
&lt;/p&gt;
&lt;p&gt;
RoboCLIP&#26159;&#19968;&#31181;&#22312;&#32447;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21333;&#20010;&#35270;&#39057;&#28436;&#31034;&#25110;&#25991;&#26412;&#25551;&#36848;&#29983;&#25104;&#22870;&#21169;&#20989;&#25968;&#65292;&#19981;&#38656;&#35201;&#19987;&#23478;&#30417;&#30563;&#25110;&#35774;&#35745;&#22870;&#21169;&#20989;&#25968;&#12290;&#21516;&#26102;&#65292;&#23427;&#36824;&#21487;&#20197;&#21033;&#29992;&#39046;&#22495;&#22806;&#30340;&#28436;&#31034;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22870;&#21169;&#35268;&#33539;&#26159;&#24378;&#21270;&#23398;&#20064;&#20013;&#19968;&#20010;&#38750;&#24120;&#22256;&#38590;&#30340;&#38382;&#39064;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#19987;&#23478;&#30417;&#30563;&#26469;&#35774;&#35745;&#31283;&#20581;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;&#27169;&#20223;&#23398;&#20064;&#65288;IL&#65289;&#26041;&#27861;&#35797;&#22270;&#36890;&#36807;&#21033;&#29992;&#19987;&#23478;&#28436;&#31034;&#26469;&#35268;&#36991;&#36825;&#20123;&#38382;&#39064;&#65292;&#20294;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#39046;&#22495;&#20869;&#30340;&#19987;&#23478;&#28436;&#31034;&#12290;&#21463;&#21040;&#35270;&#39057;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#39046;&#22495;&#30340;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RoboCLIP&#65292;&#19968;&#31181;&#22312;&#32447;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#21333;&#20010;&#28436;&#31034;&#65288;&#20811;&#26381;&#20102;&#22823;&#25968;&#25454;&#35201;&#27714;&#65289;&#26469;&#29983;&#25104;&#22870;&#21169;&#65292;&#28436;&#31034;&#21487;&#20197;&#26159;&#35270;&#39057;&#28436;&#31034;&#25110;&#20219;&#21153;&#30340;&#25991;&#26412;&#25551;&#36848;&#65292;&#32780;&#26080;&#38656;&#25163;&#21160;&#35774;&#35745;&#22870;&#21169;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;RoboCLIP&#36824;&#21487;&#20197;&#21033;&#29992;&#39046;&#22495;&#22806;&#30340;&#28436;&#31034;&#65292;&#20363;&#22914;&#20154;&#31867;&#35299;&#20915;&#20219;&#21153;&#30340;&#35270;&#39057;&#65292;&#20197;&#29983;&#25104;&#22870;&#21169;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#38656;&#35201;&#30456;&#21516;&#28436;&#31034;&#21644;&#37096;&#32626;&#39046;&#22495;&#30340;&#35201;&#27714;&#12290;RoboCLIP&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;VLMs&#36827;&#34892;&#22870;&#21169;&#29983;&#25104;&#65292;&#26080;&#38656;&#20219;&#20309;&#24494;&#35843;&#12290;&#36890;&#36807;RoboCLIP&#35757;&#32451;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Reward specification is a notoriously difficult problem in reinforcement learning, requiring extensive expert supervision to design robust reward functions. Imitation learning (IL) methods attempt to circumvent these problems by utilizing expert demonstrations but typically require a large number of in-domain expert demonstrations. Inspired by advances in the field of Video-and-Language Models (VLMs), we present RoboCLIP, an online imitation learning method that uses a single demonstration (overcoming the large data requirement) in the form of a video demonstration or a textual description of the task to generate rewards without manual reward function design. Additionally, RoboCLIP can also utilize out-of-domain demonstrations, like videos of humans solving the task for reward generation, circumventing the need to have the same demonstration and deployment domains. RoboCLIP utilizes pretrained VLMs without any finetuning for reward generation. Reinforcement learning agents trained with
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20849;&#36717;&#31215;&#20998;&#22120;&#21644;&#20998;&#35010;&#31215;&#20998;&#22120;&#20004;&#31181;&#26694;&#26550;&#65292;&#29992;&#20110;&#21152;&#36895;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#26679;&#26412;&#29983;&#25104;&#12290;&#32463;&#36807;&#23454;&#35777;&#21644;&#29702;&#35770;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#20339;&#24615;&#33021;&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#33021;&#22815;&#24212;&#29992;&#20110;&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.07894</link><description>&lt;p&gt;
&#39640;&#25928;&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#30340;&#31215;&#20998;&#22120;
&lt;/p&gt;
&lt;p&gt;
Efficient Integrators for Diffusion Generative Models. (arXiv:2310.07894v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07894
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20849;&#36717;&#31215;&#20998;&#22120;&#21644;&#20998;&#35010;&#31215;&#20998;&#22120;&#20004;&#31181;&#26694;&#26550;&#65292;&#29992;&#20110;&#21152;&#36895;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#26679;&#26412;&#29983;&#25104;&#12290;&#32463;&#36807;&#23454;&#35777;&#21644;&#29702;&#35770;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#20339;&#24615;&#33021;&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#33021;&#22815;&#24212;&#29992;&#20110;&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#25512;&#29702;&#26102;&#38388;&#29983;&#25104;&#26679;&#26412;&#36895;&#24230;&#36739;&#24930;&#12290;&#22240;&#27492;&#65292;&#20026;&#26356;&#24191;&#27867;&#30340;&#25193;&#25955;&#27169;&#22411;&#24320;&#21457;&#24555;&#36895;&#30830;&#23450;&#24615;/&#38543;&#26426;&#37319;&#26679;&#30340;&#21407;&#21017;&#24615;&#26694;&#26550;&#26159;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#20114;&#34917;&#30340;&#21152;&#36895;&#39044;&#35757;&#32451;&#27169;&#22411;&#26679;&#26412;&#29983;&#25104;&#30340;&#26694;&#26550;&#65306;&#20849;&#36717;&#31215;&#20998;&#22120;&#21644;&#20998;&#35010;&#31215;&#20998;&#22120;&#12290;&#20849;&#36717;&#31215;&#20998;&#22120;&#23558;DDIM&#27867;&#21270;&#65292;&#23558;&#21453;&#21521;&#25193;&#25955;&#21160;&#21147;&#23398;&#26144;&#23556;&#21040;&#26356;&#26131;&#20110;&#37319;&#26679;&#30340;&#31354;&#38388;&#12290;&#30456;&#21453;&#65292;&#22522;&#20110;&#20998;&#35010;&#30340;&#31215;&#20998;&#22120;&#65292;&#24120;&#29992;&#20110;&#20998;&#23376;&#21160;&#21147;&#23398;&#65292;&#36890;&#36807;&#24039;&#22937;&#22320;&#22312;&#28041;&#21450;&#25968;&#25454;&#21644;&#36741;&#21161;&#21464;&#37327;&#30340;&#25968;&#20540;&#26356;&#26032;&#20043;&#38388;&#20132;&#26367;&#65292;&#20943;&#23569;&#20102;&#25968;&#20540;&#27169;&#25311;&#35823;&#24046;&#12290;&#32463;&#36807;&#24191;&#27867;&#30340;&#23454;&#35777;&#21644;&#29702;&#35770;&#30740;&#31350;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#22686;&#24378;&#31354;&#38388;&#20013;&#33719;&#24471;&#20102;&#25253;&#21578;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#26368;&#20339;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;CIFAR-10&#19978;&#24212;&#29992;&#30456;&#31354;&#38388;&#26391;&#20043;&#19975;&#25193;&#25955;[Pandey&#65286;Mandt&#65292;2023]&#65292;&#25105;&#20204;&#30340;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#26679;&#26412;&#29983;&#25104;&#33719;&#24471;&#20102;&#26368;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models suffer from slow sample generation at inference time. Therefore, developing a principled framework for fast deterministic/stochastic sampling for a broader class of diffusion models is a promising direction. We propose two complementary frameworks for accelerating sample generation in pre-trained models: Conjugate Integrators and Splitting Integrators. Conjugate integrators generalize DDIM, mapping the reverse diffusion dynamics to a more amenable space for sampling. In contrast, splitting-based integrators, commonly used in molecular dynamics, reduce the numerical simulation error by cleverly alternating between numerical updates involving the data and auxiliary variables. After extensively studying these methods empirically and theoretically, we present a hybrid method that leads to the best-reported performance for diffusion models in augmented spaces. Applied to Phase Space Langevin Diffusion [Pandey &amp; Mandt, 2023] on CIFAR-10, our deterministic and stochastic samp
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#32034;&#20102;&#23558;&#35821;&#35328;&#20316;&#20026;&#23548;&#33322;&#30340;&#24863;&#30693;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#29616;&#25104;&#30340;&#35270;&#35273;&#31995;&#32479;&#23558;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#30340;&#35270;&#22270;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#65292;&#36890;&#36807;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#36873;&#25321;&#26368;&#20339;&#30340;&#34892;&#21160;&#26469;&#28385;&#36275;&#23548;&#33322;&#25351;&#20196;&#12290;&#26377;&#20004;&#31181;&#29992;&#20363;&#30340;&#23454;&#39564;&#23545;&#36825;&#31181;&#22522;&#20110;&#35821;&#35328;&#30340;&#23548;&#33322;&#26041;&#27861;&#36827;&#34892;&#20102;&#25506;&#32034;&#65306;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#36712;&#36857;&#20197;&#24494;&#35843;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#21450;&#27169;&#25311;&#21040;&#23454;&#38469;&#30340;&#36716;&#25442;&#12290;</title><link>http://arxiv.org/abs/2310.07889</link><description>&lt;p&gt;
LangNav: &#35821;&#35328;&#20316;&#20026;&#23548;&#33322;&#30340;&#24863;&#30693;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
LangNav: Language as a Perceptual Representation for Navigation. (arXiv:2310.07889v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07889
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#32034;&#20102;&#23558;&#35821;&#35328;&#20316;&#20026;&#23548;&#33322;&#30340;&#24863;&#30693;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#29616;&#25104;&#30340;&#35270;&#35273;&#31995;&#32479;&#23558;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#30340;&#35270;&#22270;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#65292;&#36890;&#36807;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#36873;&#25321;&#26368;&#20339;&#30340;&#34892;&#21160;&#26469;&#28385;&#36275;&#23548;&#33322;&#25351;&#20196;&#12290;&#26377;&#20004;&#31181;&#29992;&#20363;&#30340;&#23454;&#39564;&#23545;&#36825;&#31181;&#22522;&#20110;&#35821;&#35328;&#30340;&#23548;&#33322;&#26041;&#27861;&#36827;&#34892;&#20102;&#25506;&#32034;&#65306;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#36712;&#36857;&#20197;&#24494;&#35843;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#21450;&#27169;&#25311;&#21040;&#23454;&#38469;&#30340;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#32034;&#23558;&#35821;&#35328;&#20316;&#20026;&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;&#30340;&#24863;&#30693;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#29616;&#25104;&#30340;&#35270;&#35273;&#31995;&#32479;&#65288;&#29992;&#20110;&#22270;&#20687;&#23383;&#24149;&#21644;&#29289;&#20307;&#26816;&#27979;&#65289;&#23558;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#20013;&#20195;&#29702;&#20154;&#30340;&#33258;&#25105;&#20013;&#24515;&#20840;&#26223;&#35270;&#22270;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#26681;&#25454;&#24403;&#21069;&#35270;&#22270;&#21644;&#36712;&#36857;&#21382;&#21490;&#36873;&#25321;&#26368;&#20339;&#30340;&#34892;&#21160;&#26469;&#28385;&#36275;&#23548;&#33322;&#25351;&#20196;&#12290;&#19982;&#26631;&#20934;&#35774;&#32622;&#30456;&#27604;&#65292;&#26631;&#20934;&#35774;&#32622;&#23558;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;&#20110;&#19982;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#27169;&#22411;&#36830;&#32493;&#35270;&#35273;&#29305;&#24449;&#30452;&#25509;&#37197;&#21512;&#20351;&#29992;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#65288;&#31163;&#25955;&#30340;&#65289;&#35821;&#35328;&#20316;&#20026;&#24863;&#30693;&#34920;&#31034;&#12290;&#25105;&#20204;&#22312;R2R&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;&#22522;&#20934;&#27979;&#35797;&#20013;&#25506;&#32034;&#20102;&#20004;&#20010;&#29992;&#20363;&#65306;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;GPT-4&#65289;&#29983;&#25104;&#21512;&#25104;&#36712;&#36857;&#65292;&#20197;&#20415;&#24494;&#35843;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#65307;&#20197;&#21450;&#27169;&#25311;&#21040;&#23454;&#38469;&#30340;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore the use of language as a perceptual representation for vision-and-language navigation. Our approach uses off-the-shelf vision systems (for image captioning and object detection) to convert an agent's egocentric panoramic view at each time step into natural language descriptions. We then finetune a pretrained language model to select an action, based on the current view and the trajectory history, that would best fulfill the navigation instructions. In contrast to the standard setup which adapts a pretrained language model to work directly with continuous visual features from pretrained vision models, our approach instead uses (discrete) language as the perceptual representation. We explore two use cases of our language-based navigation (LangNav) approach on the R2R vision-and-language navigation benchmark: generating synthetic trajectories from a prompted large language model (GPT-4) with which to finetune a smaller language model; and sim-to-real transfer where we transfer 
&lt;/p&gt;</description></item><item><title>&#39046;&#23548;&#32773;-&#36319;&#38543;&#32773;&#31070;&#32463;&#32593;&#32476;&#26159;&#21463;&#21040;&#33258;&#28982;&#32676;&#20307;&#38598;&#21512;&#20013;&#35266;&#23519;&#21040;&#30340;&#35268;&#21017;&#21551;&#21457;&#30340;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#21033;&#29992;&#23616;&#37096;&#35823;&#24046;&#20449;&#21495;&#35757;&#32451;&#24182;&#21487;&#36873;&#25321;&#24615;&#22320;&#32467;&#21512;&#21453;&#21521;&#20256;&#25773;&#21644;&#20840;&#23616;&#25439;&#22833;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#30740;&#31350;&#24037;&#20316;&#20154;&#21592;&#34892;&#20026;&#21644;&#35780;&#20272;&#65292;&#36825;&#31181;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#39640;&#24230;&#30340;&#33258;&#32452;&#32455;&#21644;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.07885</link><description>&lt;p&gt;
&#22522;&#20110;&#22797;&#26434;&#32676;&#20307;&#30340;&#23616;&#37096;&#35823;&#24046;&#20449;&#21495;&#21551;&#21457;&#30340;&#39046;&#23548;&#32773;-&#36319;&#38543;&#32773;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Leader-Follower Neural Networks with Local Error Signals Inspired by Complex Collectives. (arXiv:2310.07885v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07885
&lt;/p&gt;
&lt;p&gt;
&#39046;&#23548;&#32773;-&#36319;&#38543;&#32773;&#31070;&#32463;&#32593;&#32476;&#26159;&#21463;&#21040;&#33258;&#28982;&#32676;&#20307;&#38598;&#21512;&#20013;&#35266;&#23519;&#21040;&#30340;&#35268;&#21017;&#21551;&#21457;&#30340;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#21033;&#29992;&#23616;&#37096;&#35823;&#24046;&#20449;&#21495;&#35757;&#32451;&#24182;&#21487;&#36873;&#25321;&#24615;&#22320;&#32467;&#21512;&#21453;&#21521;&#20256;&#25773;&#21644;&#20840;&#23616;&#25439;&#22833;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#30740;&#31350;&#24037;&#20316;&#20154;&#21592;&#34892;&#20026;&#21644;&#35780;&#20272;&#65292;&#36825;&#31181;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#39640;&#24230;&#30340;&#33258;&#32452;&#32455;&#21644;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#20013;&#20855;&#26377;&#24322;&#36136;&#12289;&#36164;&#28304;&#26377;&#38480;&#30340;&#20449;&#24687;&#22788;&#29702;&#21333;&#20803;&#65288;&#20363;&#22914;&#65292;&#19968;&#32676;&#40060;&#12289;&#19968;&#32676;&#40479;&#25110;&#19968;&#32452;&#31070;&#32463;&#20803;&#32593;&#32476;&#65289;&#30340;&#38598;&#20307;&#34892;&#20026;&#34920;&#29616;&#20986;&#39640;&#24230;&#30340;&#33258;&#32452;&#32455;&#21644;&#22797;&#26434;&#24615;&#12290;&#36825;&#20123; emergent properties&#26159;&#36890;&#36807;&#31616;&#21333;&#30340;&#30456;&#20114;&#20316;&#29992;&#35268;&#21017;&#20135;&#29983;&#30340;&#65292;&#20854;&#20013;&#26576;&#20123;&#20010;&#20307;&#33021;&#22815;&#34920;&#29616;&#20986;&#39046;&#23548;&#34892;&#20026;&#24182;&#24433;&#21709;&#32676;&#20307;&#30340;&#27963;&#21160;&#12290;&#21463;&#21040;&#36825;&#20123;&#32676;&#20307;&#30340;&#22797;&#26434;&#24615;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#21463;&#21040;&#33258;&#28982;&#32676;&#20307;&#38598;&#21512;&#20013;&#35266;&#23519;&#21040;&#30340;&#35268;&#21017;&#30340;&#21551;&#21457;&#12290;&#36825;&#20010;NN&#32467;&#26500;&#21253;&#21547;&#20102;&#21253;&#21547;&#19968;&#20010;&#25110;&#22810;&#20010;&#20449;&#24687;&#22788;&#29702;&#21333;&#20803;&#65288;&#22914;&#31070;&#32463;&#20803;&#12289;&#28388;&#27874;&#22120;&#12289;&#23618;&#25110;&#23618;&#22359;&#65289;&#30340;&#24037;&#20316;&#20154;&#21592;&#12290;&#24037;&#20316;&#20154;&#21592;&#21487;&#20197;&#26159;&#39046;&#23548;&#32773;&#25110;&#36319;&#38543;&#32773;&#65292;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#23616;&#37096;&#35823;&#24046;&#20449;&#21495;&#35757;&#32451;&#20102;&#19968;&#20010;&#39046;&#23548;&#32773;-&#36319;&#38543;&#32773;&#31070;&#32463;&#32593;&#32476;&#65288;LFNN&#65289;&#65292;&#24182;&#36873;&#25321;&#24615;&#22320;&#32467;&#21512;&#21453;&#21521;&#20256;&#25773;&#65288;BP&#65289;&#21644;&#20840;&#23616;&#25439;&#22833;&#12290;&#25105;&#20204;&#36890;&#36807;&#22823;&#37327;&#30340;&#23454;&#39564;&#30740;&#31350;&#24037;&#20316;&#20154;&#21592;&#30340;&#34892;&#20026;&#24182;&#35780;&#20272;&#20102;LFNN&#12290;&#25105;&#20204;&#35757;&#32451;&#20102;&#20351;&#29992;&#23616;&#37096;&#35823;&#24046;&#20449;&#21495;&#30340;LFNNs&#65292;&#21487;&#36873;&#25321;&#24615;&#22320;&#23558;&#21453;&#21521;&#20256;&#25773;&#21644;&#20840;&#23616;&#25439;&#22833;&#32467;&#21512;&#22312;&#19968;&#36215;&#12290;
&lt;/p&gt;
&lt;p&gt;
The collective behavior of a network with heterogeneous, resource-limited information processing units (e.g., group of fish, flock of birds, or network of neurons) demonstrates high self-organization and complexity. These emergent properties arise from simple interaction rules where certain individuals can exhibit leadership-like behavior and influence the collective activity of the group. Motivated by the intricacy of these collectives, we propose a neural network (NN) architecture inspired by the rules observed in nature's collective ensembles. This NN structure contains workers that encompass one or more information processing units (e.g., neurons, filters, layers, or blocks of layers). Workers are either leaders or followers, and we train a leader-follower neural network (LFNN) by leveraging local error signals and optionally incorporating backpropagation (BP) and global loss. We investigate worker behavior and evaluate LFNNs through extensive experimentation. Our LFNNs trained wit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#22312;&#29983;&#20135;&#34892;&#19994;&#20013;&#30340;&#23454;&#38469;&#30456;&#20851;&#24615;&#65292;&#24182;&#23558;&#20854;&#19982;&#24403;&#21069;&#23398;&#26415;&#30028;XAI&#30740;&#31350;&#30340;&#29616;&#29366;&#30456;&#32852;&#31995;&#12290;&#36890;&#36807;&#23545;&#24191;&#27867;&#30340;&#35775;&#35848;&#21644;&#25991;&#29486;&#22238;&#39038;&#30340;&#20998;&#26512;&#65292;&#21457;&#29616;&#20102;&#35775;&#35848;&#32467;&#26524;&#19982;&#24403;&#21069;&#30340;&#30740;&#31350;&#26041;&#27861;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2310.07882</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#22312;&#26426;&#22120;&#23398;&#20064;&#29983;&#21629;&#21608;&#26399;&#20013;&#30340;&#20247;&#22810;&#38754;&#35980;&#65306;&#24037;&#19994;&#29616;&#23454;&#19982;&#24403;&#21069;&#30740;&#31350;&#29366;&#20917;
&lt;/p&gt;
&lt;p&gt;
The Thousand Faces of Explainable AI Along the Machine Learning Life Cycle: Industrial Reality and Current State of Research. (arXiv:2310.07882v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07882
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#22312;&#29983;&#20135;&#34892;&#19994;&#20013;&#30340;&#23454;&#38469;&#30456;&#20851;&#24615;&#65292;&#24182;&#23558;&#20854;&#19982;&#24403;&#21069;&#23398;&#26415;&#30028;XAI&#30740;&#31350;&#30340;&#29616;&#29366;&#30456;&#32852;&#31995;&#12290;&#36890;&#36807;&#23545;&#24191;&#27867;&#30340;&#35775;&#35848;&#21644;&#25991;&#29486;&#22238;&#39038;&#30340;&#20998;&#26512;&#65292;&#21457;&#29616;&#20102;&#35775;&#35848;&#32467;&#26524;&#19982;&#24403;&#21069;&#30340;&#30740;&#31350;&#26041;&#27861;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#22312;&#29983;&#20135;&#34892;&#19994;&#20013;&#30340;&#23454;&#38469;&#30456;&#20851;&#24615;&#65292;&#24182;&#23558;&#20854;&#19982;&#24403;&#21069;&#23398;&#26415;&#30028;XAI&#30740;&#31350;&#30340;&#29616;&#29366;&#30456;&#32852;&#31995;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#22522;&#20110;&#23545;&#24403;&#21069;&#24037;&#19994;&#23454;&#36341;&#20013;XAI&#22312;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#29983;&#21629;&#21608;&#26399;&#20013;&#30340;&#35282;&#33394;&#21644;&#36866;&#29992;&#24615;&#20197;&#21450;&#26410;&#26469;&#30340;&#39044;&#26399;&#37325;&#35201;&#24615;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35775;&#35848;&#12290;&#35775;&#35848;&#23545;&#35937;&#21253;&#25324;&#26469;&#33258;&#19981;&#21516;&#34892;&#19994;&#37096;&#38376;&#30340;&#21508;&#31181;&#35282;&#33394;&#21644;&#20851;&#38190;&#21033;&#30410;&#30456;&#20851;&#32773;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20379;&#23545;&#30456;&#20851;&#25991;&#29486;&#30340;&#31616;&#26126;&#22238;&#39038;&#65292;&#27010;&#36848;&#20102;XAI&#30740;&#31350;&#30340;&#29616;&#29366;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#25552;&#20379;&#19968;&#20010;&#20840;&#38754;&#30340;&#27010;&#36848;&#65292;&#28085;&#30422;&#20102;&#34987;&#35843;&#26597;&#32773;&#30340;&#24847;&#35265;&#20197;&#21450;&#24403;&#21069;&#23398;&#26415;&#30740;&#31350;&#30340;&#29616;&#29366;&#12290;&#36890;&#36807;&#23558;&#25105;&#20204;&#30340;&#35775;&#35848;&#32467;&#26524;&#19982;&#24403;&#21069;&#30340;&#30740;&#31350;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#20960;&#20010;&#24046;&#24322;&#12290;&#23613;&#31649;&#23384;&#22312;&#35768;&#22810;&#19981;&#21516;&#30340;XAI&#26041;&#27861;&#65292;&#20294;&#22823;&#22810;&#25968;&#26041;&#27861;&#37117;&#38598;&#20013;&#22312;...
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate the practical relevance of explainable artificial intelligence (XAI) with a special focus on the producing industries and relate them to the current state of academic XAI research. Our findings are based on an extensive series of interviews regarding the role and applicability of XAI along the Machine Learning (ML) lifecycle in current industrial practice and its expected relevance in the future. The interviews were conducted among a great variety of roles and key stakeholders from different industry sectors. On top of that, we outline the state of XAI research by providing a concise review of the relevant literature. This enables us to provide an encompassing overview covering the opinions of the surveyed persons as well as the current state of academic research. By comparing our interview results with the current research approaches we reveal several discrepancies. While a multitude of different XAI approaches exists, most of them are centered around the
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;DeePref&#65292;&#19968;&#31181;&#29992;&#20110;&#22312;&#20869;&#23481;&#20132;&#20184;&#32593;&#32476;&#20013;&#36827;&#34892;&#22312;&#32447;&#35270;&#39057;&#20869;&#23481;&#39044;&#21462;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#12290;&#20256;&#32479;&#30340;&#39044;&#21462;&#25216;&#26415;&#38590;&#20197;&#36866;&#24212;&#24037;&#20316;&#36127;&#36733;&#20013;&#30340;&#21464;&#21270;&#65292;&#32780;DeePref&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#36866;&#24212;&#19981;&#26029;&#21464;&#21270;&#30340;&#29992;&#25143;&#35775;&#38382;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2310.07881</link><description>&lt;p&gt;
DeePref: &#22312;&#20869;&#23481;&#20132;&#20184;&#32593;&#32476;&#20013;&#30340;&#35270;&#39057;&#39044;&#21462;&#20013;&#24212;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DeePref: Deep Reinforcement Learning For Video Prefetching In Content Delivery Networks. (arXiv:2310.07881v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07881
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;DeePref&#65292;&#19968;&#31181;&#29992;&#20110;&#22312;&#20869;&#23481;&#20132;&#20184;&#32593;&#32476;&#20013;&#36827;&#34892;&#22312;&#32447;&#35270;&#39057;&#20869;&#23481;&#39044;&#21462;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#12290;&#20256;&#32479;&#30340;&#39044;&#21462;&#25216;&#26415;&#38590;&#20197;&#36866;&#24212;&#24037;&#20316;&#36127;&#36733;&#20013;&#30340;&#21464;&#21270;&#65292;&#32780;DeePref&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#36866;&#24212;&#19981;&#26029;&#21464;&#21270;&#30340;&#29992;&#25143;&#35775;&#38382;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20869;&#23481;&#20132;&#20184;&#32593;&#32476;&#25215;&#36733;&#20102;&#22823;&#37096;&#20998;&#30340;&#20114;&#32852;&#32593;&#27969;&#37327;&#65292;&#23545;&#35270;&#39057;&#20869;&#23481;&#30340;&#38656;&#27714;&#19981;&#26029;&#22686;&#21152;&#20351;&#24471;&#32531;&#23384;&#21644;&#39044;&#21462;&#20248;&#21270;&#31639;&#27861;&#30340;&#37325;&#35201;&#24615;&#26085;&#30410;&#31361;&#20986;&#12290;&#39044;&#21462;&#30340;&#30446;&#26631;&#26159;&#22312;&#35831;&#27714;&#32773;&#21457;&#20986;&#35831;&#27714;&#20043;&#21069;&#23558;&#25968;&#25454;&#25552;&#21069;&#25918;&#20837;&#32531;&#23384;&#20013;&#65292;&#20197;&#20943;&#23569;&#35775;&#38382;&#26102;&#38388;&#24182;&#25913;&#21892;&#29992;&#25143;&#20307;&#39564;&#12290;&#20256;&#32479;&#30340;&#39044;&#21462;&#25216;&#26415;&#36866;&#24212;&#29305;&#23450;&#30340;&#35775;&#38382;&#27169;&#24335;&#65292;&#20294;&#24456;&#38590;&#36866;&#24212;&#24037;&#20316;&#36127;&#36733;&#20013;&#30340;&#31361;&#21464;&#25110;&#38543;&#26426;&#21270;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#26469;&#24212;&#23545;&#29992;&#25143;&#35775;&#38382;&#27169;&#24335;&#30340;&#21464;&#21270;&#65292;&#24182;&#38543;&#26102;&#38388;&#33258;&#21160;&#36866;&#24212;&#30340;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DeePref&#65292;&#19968;&#31181;&#29992;&#20110;&#22312;&#20869;&#23481;&#20132;&#20184;&#32593;&#32476;&#20013;&#22312;&#32447;&#35270;&#39057;&#20869;&#23481;&#39044;&#21462;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Content Delivery Networks carry the majority of Internet traffic, and the increasing demand for video content as a major IP traffic across the Internet highlights the importance of caching and prefetching optimization algorithms. Prefetching aims to make data available in the cache before the requester places its request to reduce access time and improve the Quality of Experience on the user side. Prefetching is well investigated in operating systems, compiler instructions, in-memory cache, local storage systems, high-speed networks, and cloud systems. Traditional prefetching techniques are well adapted to a particular access pattern, but fail to adapt to sudden variations or randomization in workloads. This paper explores the use of reinforcement learning to tackle the changes in user access patterns and automatically adapt over time. To this end, we propose, DeePref, a Deep Reinforcement Learning agent for online video content prefetching in Content Delivery Networks. DeePref is a pr
&lt;/p&gt;</description></item><item><title>TabLib&#26159;&#19968;&#20010;&#21253;&#21547;&#19978;&#20159;&#34920;&#26684;&#21644;&#19978;&#30334;&#20159;&#19978;&#19979;&#25991;&#30340;&#25968;&#25454;&#38598;&#65292;&#35268;&#27169;&#21644;&#22810;&#26679;&#24615;&#20351;&#20854;&#22312;&#34920;&#26684;&#27169;&#24577;&#19979;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.07875</link><description>&lt;p&gt;
TabLib&#65306;&#19968;&#20010;&#21253;&#21547;&#19978;&#20159;&#34920;&#26684;&#21644;&#19978;&#30334;&#20159;&#19978;&#19979;&#25991;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
TabLib: A Dataset of 627M Tables with Context. (arXiv:2310.07875v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07875
&lt;/p&gt;
&lt;p&gt;
TabLib&#26159;&#19968;&#20010;&#21253;&#21547;&#19978;&#20159;&#34920;&#26684;&#21644;&#19978;&#30334;&#20159;&#19978;&#19979;&#25991;&#30340;&#25968;&#25454;&#38598;&#65292;&#35268;&#27169;&#21644;&#22810;&#26679;&#24615;&#20351;&#20854;&#22312;&#34920;&#26684;&#27169;&#24577;&#19979;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24040;&#22823;&#22810;&#26679;&#30340;&#25968;&#25454;&#38598;&#22312;&#25552;&#21319;&#29616;&#20195;AI&#31995;&#32479;&#22312;&#25991;&#26412;&#21644;&#22270;&#20687;&#27169;&#24577;&#19979;&#30340;&#24615;&#33021;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#34920;&#26684;&#25968;&#25454;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#19982;&#25991;&#26412;&#21644;&#22270;&#20687;&#21487;&#27604;&#25311;&#30340;&#35268;&#27169;&#21644;&#22810;&#26679;&#24615;&#30340;&#25968;&#25454;&#38598;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;"TabLib"&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;6.27&#20159;&#20010;&#34920;&#26684;&#21644;86.7&#20159;&#20010;&#19978;&#19979;&#25991;&#20196;&#29260;&#24635;&#20849;&#36798;&#21040;69 TiB&#30340;&#32534;&#35793;&#25968;&#25454;&#38598;&#12290;TabLib&#30340;&#25968;&#25454;&#26469;&#33258;&#22810;&#20010;&#25991;&#20214;&#26684;&#24335;&#65292;&#21253;&#25324;CSV&#12289;HTML&#12289;SQLite&#12289;PDF&#12289;Excel&#31561;&#65292;&#36825;&#20123;&#25968;&#25454;&#28304;&#33258;GitHub&#21644;Common Crawl&#12290;TabLib&#30340;&#35268;&#27169;&#21644;&#22810;&#26679;&#24615;&#22312;&#34920;&#26684;&#27169;&#24577;&#19979;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#22914;&#21516;&#21407;&#22987;&#30340;&#29992;&#20110;&#25991;&#26412;&#21644;&#22270;&#20687;&#30340;&#22522;&#30784;&#25968;&#25454;&#38598;&#65292;&#20363;&#22914;The Pile&#21644;LAION&#12290;
&lt;/p&gt;
&lt;p&gt;
It is well-established that large, diverse datasets play a pivotal role in the performance of modern AI systems for text and image modalities. However, there are no datasets for tabular data of comparable size and diversity to those available for text and images. Thus we present "TabLib'', a compilation of 627 million tables totaling 69 TiB, along with 867B tokens of context. TabLib was extracted from numerous file formats, including CSV, HTML, SQLite, PDF, Excel, and others, sourced from GitHub and Common Crawl. The size and diversity of TabLib offer considerable promise in the table modality, reminiscent of the original promise of foundational datasets for text and images, such as The Pile and LAION.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#23618;&#27425;&#22810;&#27169;&#24577;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#30340;&#26032;&#39062;&#12289;&#36890;&#29992;&#19988;&#32479;&#19968;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;MEDHMP&#65292;&#22312;&#19977;&#20010;&#32423;&#21035;&#19978;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#24182;&#19988;&#22312;&#19982;&#21313;&#20843;&#20010;&#22522;&#20934;&#26041;&#27861;&#30340;&#27604;&#36739;&#20013;&#39564;&#35777;&#20102;&#20854;&#39640;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.07871</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#27169;&#24577;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#23618;&#27425;&#39044;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Pretraining on Multimodal Electronic Health Records. (arXiv:2310.07871v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07871
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#23618;&#27425;&#22810;&#27169;&#24577;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#30340;&#26032;&#39062;&#12289;&#36890;&#29992;&#19988;&#32479;&#19968;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;MEDHMP&#65292;&#22312;&#19977;&#20010;&#32423;&#21035;&#19978;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#24182;&#19988;&#22312;&#19982;&#21313;&#20843;&#20010;&#22522;&#20934;&#26041;&#27861;&#30340;&#27604;&#36739;&#20013;&#39564;&#35777;&#20102;&#20854;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#25216;&#26415;&#65292;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22312;&#21307;&#23398;&#39046;&#22495;&#65292;&#29616;&#26377;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#39044;&#35757;&#32451;&#27169;&#22411;&#26080;&#27861;&#25429;&#25417;&#21040;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#30340;&#23618;&#27425;&#24615;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#20351;&#29992;&#21333;&#20010;&#39044;&#35757;&#32451;&#27169;&#22411;&#36328;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#12289;&#36890;&#29992;&#19988;&#32479;&#19968;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;MEDHMP&#65292;&#19987;&#38376;&#38024;&#23545;&#23618;&#27425;&#22810;&#27169;&#24577;&#30340;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#36827;&#34892;&#35774;&#35745;&#12290;&#36890;&#36807;&#23454;&#39564;&#32467;&#26524;&#22312;&#19977;&#20010;&#32423;&#21035;&#19978;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;MEDHMP&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#36890;&#36807;&#19982;&#21313;&#20843;&#20010;&#22522;&#20934;&#27169;&#22411;&#30340;&#27604;&#36739;&#36827;&#19968;&#27493;&#31361;&#20986;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretraining has proven to be a powerful technique in natural language processing (NLP), exhibiting remarkable success in various NLP downstream tasks. However, in the medical domain, existing pretrained models on electronic health records (EHR) fail to capture the hierarchical nature of EHR data, limiting their generalization capability across diverse downstream tasks using a single pretrained model. To tackle this challenge, this paper introduces a novel, general, and unified pretraining framework called MEDHMP, specifically designed for hierarchically multimodal EHR data. The effectiveness of the proposed MEDHMP is demonstrated through experimental results on eight downstream tasks spanning three levels. Comparisons against eighteen baselines further highlight the efficacy of our approach.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#25112;&#30053;&#20449;&#24687;&#20256;&#36882;&#28216;&#25103;&#20013;&#65292;&#21033;&#29992;&#29420;&#31435;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#35757;&#32451;&#30340;&#21457;&#36865;&#32773;&#21644;&#25509;&#25910;&#32773;&#21487;&#20197;&#25910;&#25947;&#21040;&#25509;&#36817;&#26368;&#20248;&#22343;&#34913;&#31574;&#30053;&#65292;&#24182;&#19988;&#22312;&#20195;&#29702;&#20043;&#38388;&#30340;&#21033;&#30410;&#20914;&#31361;&#19979;&#23454;&#29616;&#20102;&#26368;&#22823;&#21270;&#30340;&#36890;&#20449;&#12290;&#36825;&#19968;&#32467;&#35770;&#31283;&#20581;&#65292;&#24182;&#23545;&#20449;&#24687;&#20256;&#36882;&#28216;&#25103;&#20013;&#30340;&#22343;&#34913;&#36873;&#25321;&#29702;&#35770;&#12289;&#35745;&#31639;&#26426;&#31185;&#23398;&#20013;&#30340;&#31639;&#27861;&#38388;&#36890;&#20449;&#21644;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#24066;&#22330;&#20013;&#30340;&#32463;&#27982;&#23398;&#20135;&#29983;&#20102;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.07867</link><description>&lt;p&gt;
&#24265;&#20215;&#23545;&#35805;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Cheap Talking Algorithms. (arXiv:2310.07867v1 [econ.TH])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07867
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#25112;&#30053;&#20449;&#24687;&#20256;&#36882;&#28216;&#25103;&#20013;&#65292;&#21033;&#29992;&#29420;&#31435;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#35757;&#32451;&#30340;&#21457;&#36865;&#32773;&#21644;&#25509;&#25910;&#32773;&#21487;&#20197;&#25910;&#25947;&#21040;&#25509;&#36817;&#26368;&#20248;&#22343;&#34913;&#31574;&#30053;&#65292;&#24182;&#19988;&#22312;&#20195;&#29702;&#20043;&#38388;&#30340;&#21033;&#30410;&#20914;&#31361;&#19979;&#23454;&#29616;&#20102;&#26368;&#22823;&#21270;&#30340;&#36890;&#20449;&#12290;&#36825;&#19968;&#32467;&#35770;&#31283;&#20581;&#65292;&#24182;&#23545;&#20449;&#24687;&#20256;&#36882;&#28216;&#25103;&#20013;&#30340;&#22343;&#34913;&#36873;&#25321;&#29702;&#35770;&#12289;&#35745;&#31639;&#26426;&#31185;&#23398;&#20013;&#30340;&#31639;&#27861;&#38388;&#36890;&#20449;&#21644;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#24066;&#22330;&#20013;&#30340;&#32463;&#27982;&#23398;&#20135;&#29983;&#20102;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#27169;&#25311;&#29420;&#31435;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#20811;&#21171;&#31119;&#24503;&#21644;&#32034;&#36125;&#23572;&#65288;1982&#65289;&#30340;&#25112;&#30053;&#20449;&#24687;&#20256;&#36882;&#28216;&#25103;&#20013;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#19968;&#20010;&#21457;&#36865;&#32773;&#21644;&#19968;&#20010;&#25509;&#25910;&#32773;&#19968;&#36215;&#36827;&#34892;&#35757;&#32451;&#65292;&#25910;&#25947;&#21040;&#25509;&#36817;&#28216;&#25103;&#20808;&#39564;&#26368;&#20248;&#22343;&#34913;&#30340;&#31574;&#30053;&#12290;&#22240;&#27492;&#65292;&#36890;&#20449;&#22312;&#19982;&#20195;&#29702;&#20043;&#38388;&#30340;&#21033;&#30410;&#20914;&#31361;&#31243;&#24230;&#32473;&#20986;&#30340;&#32435;&#20160;&#22343;&#34913;&#19979;&#65292;&#25353;&#29031;&#26368;&#22823;&#31243;&#24230;&#36827;&#34892;&#12290;&#36825;&#19968;&#32467;&#35770;&#23545;&#36229;&#21442;&#25968;&#21644;&#28216;&#25103;&#30340;&#22791;&#36873;&#35268;&#33539;&#31283;&#20581;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#20449;&#24687;&#20256;&#36882;&#28216;&#25103;&#20013;&#22343;&#34913;&#36873;&#25321;&#29702;&#35770;&#12289;&#35745;&#31639;&#26426;&#31185;&#23398;&#20013;&#31639;&#27861;&#38388;&#26032;&#20852;&#36890;&#20449;&#24037;&#20316;&#20197;&#21450;&#30001;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#32452;&#25104;&#30340;&#24066;&#22330;&#20013;&#30340;&#23467;&#26007;&#32463;&#27982;&#23398;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
We simulate behaviour of independent reinforcement learning algorithms playing the Crawford and Sobel (1982) game of strategic information transmission. We show that a sender and a receiver training together converge to strategies close to the exante optimal equilibrium of the game. Hence, communication takes place to the largest extent predicted by Nash equilibrium given the degree of conflict of interest between agents. The conclusion is shown to be robust to alternative specifications of the hyperparameters and of the game. We discuss implications for theories of equilibrium selection in information transmission games, for work on emerging communication among algorithms in computer science and for the economics of collusions in markets populated by artificially intelligent agents.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#35752;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#22312;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#28508;&#21147;&#21644;&#38480;&#21046;&#12290;&#30740;&#31350;&#32467;&#26524;&#21457;&#29616;&#65292;&#20027;&#35266;&#24615;&#20250;&#36127;&#38754;&#24433;&#21709;&#27169;&#22411;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#12290;&#36825;&#23545;&#20110;&#29702;&#35299;&#21644;&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#30340;&#26377;&#25928;&#24615;&#20855;&#26377;&#37325;&#35201;&#30340;&#21551;&#31034;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.07849</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#29992;&#20110;&#25991;&#26412;&#20998;&#31867;&#65306;&#28508;&#21147;&#21644;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations. (arXiv:2310.07849v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07849
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#35752;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#22312;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#28508;&#21147;&#21644;&#38480;&#21046;&#12290;&#30740;&#31350;&#32467;&#26524;&#21457;&#29616;&#65292;&#20027;&#35266;&#24615;&#20250;&#36127;&#38754;&#24433;&#21709;&#27169;&#22411;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#12290;&#36825;&#23545;&#20110;&#29702;&#35299;&#21644;&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#30340;&#26377;&#25928;&#24615;&#20855;&#26377;&#37325;&#35201;&#30340;&#21551;&#31034;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25910;&#38598;&#21644;&#25972;&#29702;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#23545;&#20110;&#24320;&#21457;&#20855;&#26377;&#21331;&#36234;&#24615;&#33021;&#30340;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#24448;&#24448;&#20276;&#38543;&#30528;&#24040;&#22823;&#30340;&#25104;&#26412;&#21644;&#26102;&#38388;&#25237;&#20837;&#12290;&#30740;&#31350;&#20154;&#21592;&#26368;&#36817;&#24320;&#22987;&#25506;&#32034;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#38598;&#20316;&#20026;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;LLM&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#22312;&#25903;&#25345;&#27169;&#22411;&#35757;&#32451;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#22312;&#19981;&#21516;&#30340;&#20998;&#31867;&#20219;&#21153;&#20013;&#26159;&#19981;&#19968;&#33268;&#30340;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#20102;&#35299;&#35843;&#33410;LLM&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#26377;&#25928;&#24615;&#30340;&#22240;&#32032;&#65292;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#20998;&#31867;&#30340;&#20027;&#35266;&#24615;&#22914;&#20309;&#24433;&#21709;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20027;&#35266;&#24615;&#22312;&#20219;&#21153;&#23618;&#38754;&#21644;&#23454;&#20363;&#23618;&#38754;&#19978;&#37117;&#19982;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#24615;&#33021;&#21576;&#36127;&#30456;&#20851;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#25105;&#20204;&#30340;&#24037;&#20316;&#23545;&#20110;&#21033;&#29992;LLM&#26469;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#22312;&#28508;&#21147;&#21644;&#38480;&#21046;&#26041;&#38754;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
The collection and curation of high-quality training data is crucial for developing text classification models with superior performance, but it is often associated with significant costs and time investment. Researchers have recently explored using large language models (LLMs) to generate synthetic datasets as an alternative approach. However, the effectiveness of the LLM-generated synthetic data in supporting model training is inconsistent across different classification tasks. To better understand factors that moderate the effectiveness of the LLM-generated synthetic data, in this study, we look into how the performance of models trained on these synthetic data may vary with the subjectivity of classification. Our results indicate that subjectivity, at both the task level and instance level, is negatively associated with the performance of the model trained on synthetic data. We conclude by discussing the implications of our work on the potential and limitations of leveraging LLM fo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#26377;&#38480;&#39046;&#22495;&#20013;&#20174;&#25945;&#24072;&#21040;&#23398;&#29983;&#20998;&#31867;&#22120;&#36827;&#34892;&#30693;&#35782;&#20256;&#36882;&#30340;&#32479;&#35745;&#25928;&#29575;&#65292;&#21457;&#29616;&#29305;&#26435;&#20449;&#24687;&#20250;&#21152;&#36895;&#20256;&#36882;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#25439;&#22833;&#20989;&#25968;&#36798;&#21040;&#20102;&#30693;&#35782;&#20256;&#36882;&#30340;&#22522;&#26412;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2310.07838</link><description>&lt;p&gt;
&#25506;&#32034;&#26377;&#38480;&#39046;&#22495;&#30693;&#35782;&#20256;&#36882;&#30340;&#22522;&#26412;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Towards the Fundamental Limits of Knowledge Transfer over Finite Domains. (arXiv:2310.07838v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07838
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#26377;&#38480;&#39046;&#22495;&#20013;&#20174;&#25945;&#24072;&#21040;&#23398;&#29983;&#20998;&#31867;&#22120;&#36827;&#34892;&#30693;&#35782;&#20256;&#36882;&#30340;&#32479;&#35745;&#25928;&#29575;&#65292;&#21457;&#29616;&#29305;&#26435;&#20449;&#24687;&#20250;&#21152;&#36895;&#20256;&#36882;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#25439;&#22833;&#20989;&#25968;&#36798;&#21040;&#20102;&#30693;&#35782;&#20256;&#36882;&#30340;&#22522;&#26412;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#36890;&#36807;&#20174;&#25945;&#24072;&#21040;&#27010;&#29575;&#21270;&#23398;&#29983;&#20998;&#31867;&#22120;&#30340;n&#20010;&#26679;&#26412;&#36827;&#34892;&#30693;&#35782;&#20256;&#36882;&#30340;&#32479;&#35745;&#25928;&#29575;&#36827;&#34892;&#20102;&#34920;&#24449;&#65292;&#20854;&#20013;&#36755;&#20837;&#31354;&#38388;S&#21644;&#26631;&#31614;A&#20026;&#26377;&#38480;&#22495;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#19977;&#20010;&#28176;&#36827;&#32423;&#21035;&#19978;&#30340;&#29305;&#26435;&#20449;&#24687;&#21487;&#20197;&#21152;&#24555;&#20256;&#36882;&#30340;&#36895;&#24230;&#12290;&#22312;&#31532;&#19968;&#32423;&#21035;&#19978;&#65292;&#21482;&#26377;&#20855;&#26377;&#22256;&#38590;&#26631;&#31614;&#30340;&#26679;&#26412;&#26159;&#24050;&#30693;&#30340;&#65292;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#22120;&#33021;&#22815;&#36798;&#21040;&#26368;&#23567;&#21270;&#36895;&#29575;sqrt(|S||A|/n)&#12290;&#31532;&#20108;&#32423;&#21035;&#19978;&#65292;&#38500;&#20102;&#24050;&#30693;&#30340;&#22256;&#38590;&#26631;&#31614;&#26679;&#26412;&#22806;&#65292;&#36824;&#26377;&#37319;&#26679;&#26631;&#31614;&#30340;&#25945;&#24072;&#27010;&#29575;&#21487;&#29992;&#65292;&#36825;&#23558;&#25910;&#25947;&#36895;&#24230;&#30340;&#19979;&#30028;&#25552;&#39640;&#21040;|S||A|/n&#12290;&#28982;&#32780;&#65292;&#22312;&#31532;&#20108;&#20010;&#25968;&#25454;&#37319;&#38598;&#21327;&#35758;&#19979;&#65292;&#26368;&#23567;&#21270;&#20132;&#21449;&#29109;&#25439;&#22833;&#30340;&#26420;&#32032;&#36866;&#24212;&#20250;&#23548;&#33268;&#28176;&#36817;&#20559;&#24046;&#30340;&#23398;&#29983;&#12290;&#25105;&#20204;&#20811;&#26381;&#20102;&#36825;&#20010;&#38480;&#21046;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#32463;&#39564;&#21464;&#20307;&#30340;&#24179;&#26041;&#35823;&#24046;&#36923;&#36753;&#25439;&#22833;&#26469;&#23454;&#29616;&#20102;&#22522;&#26412;&#38480;&#21046;&#12290;&#31532;&#19977;&#32423;&#21035;&#36827;&#19968;&#27493;&#36171;&#20104;&#23398;&#29983;&#36719;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;
We characterize the statistical efficiency of knowledge transfer through $n$ samples from a teacher to a probabilistic student classifier with input space $\mathcal S$ over labels $\mathcal A$. We show that privileged information at three progressive levels accelerates the transfer. At the first level, only samples with hard labels are known, via which the maximum likelihood estimator attains the minimax rate $\sqrt{{|{\mathcal S}||{\mathcal A}|}/{n}}$. The second level has the teacher probabilities of sampled labels available in addition, which turns out to boost the convergence rate lower bound to ${{|{\mathcal S}||{\mathcal A}|}/{n}}$. However, under this second data acquisition protocol, minimizing a naive adaptation of the cross-entropy loss results in an asymptotically biased student. We overcome this limitation and achieve the fundamental limit by using a novel empirical variant of the squared error logit loss. The third level further equips the student with the soft labels (com
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#32454;&#21270;&#20998;&#26512;&#23398;&#20064;&#29575;&#35843;&#24230;&#26469;&#35299;&#20915;&#23454;&#36341;&#20013;&#23398;&#20064;&#29575;&#35843;&#25972;&#19982;&#29702;&#35770;&#30340;&#19981;&#19968;&#33268;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#35266;&#23519;&#21040;&#30340;&#26799;&#24230;&#33539;&#25968;&#36827;&#34892;&#20998;&#26512;&#65292;&#24471;&#21040;&#20102;&#36866;&#24212;&#20110;&#29305;&#23450;&#20219;&#21153;&#30340;&#32454;&#21270;&#35843;&#24230;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#25913;&#21892;&#20248;&#21270;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.07831</link><description>&lt;p&gt;
&#20309;&#26102;&#65292;&#20026;&#20160;&#20040;&#20197;&#21450;&#22810;&#23569;&#65311;&#36890;&#36807;&#32454;&#21270;&#36827;&#34892;&#30340;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#35843;&#24230;
&lt;/p&gt;
&lt;p&gt;
When, Why and How Much? Adaptive Learning Rate Scheduling by Refinement. (arXiv:2310.07831v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07831
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#32454;&#21270;&#20998;&#26512;&#23398;&#20064;&#29575;&#35843;&#24230;&#26469;&#35299;&#20915;&#23454;&#36341;&#20013;&#23398;&#20064;&#29575;&#35843;&#25972;&#19982;&#29702;&#35770;&#30340;&#19981;&#19968;&#33268;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#35266;&#23519;&#21040;&#30340;&#26799;&#24230;&#33539;&#25968;&#36827;&#34892;&#20998;&#26512;&#65292;&#24471;&#21040;&#20102;&#36866;&#24212;&#20110;&#29305;&#23450;&#20219;&#21153;&#30340;&#32454;&#21270;&#35843;&#24230;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#25913;&#21892;&#20248;&#21270;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#36341;&#20013;&#20351;&#29992;&#30340;&#23398;&#20064;&#29575;&#35843;&#24230;&#19982;&#29702;&#35770;&#25512;&#33616;&#30340;&#20960;&#20046;&#23436;&#20840;&#19981;&#21516;&#12290;&#25105;&#20204;&#32553;&#23567;&#20102;&#22823;&#37096;&#20998;&#29702;&#35770;&#19982;&#23454;&#36341;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#24182;&#22240;&#27492;&#33021;&#22815;&#25512;&#23548;&#20986;&#26032;&#30340;&#38382;&#39064;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#35843;&#24230;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#25216;&#26415;&#36129;&#29486;&#26159;&#23545;&#24191;&#27867;&#31867;&#21035;&#30340;&#20248;&#21270;&#31639;&#27861;&#65288;&#21253;&#25324;SGD&#65289;&#30340;&#23398;&#20064;&#29575;&#35843;&#24230;&#36827;&#34892;&#32454;&#21270;&#20998;&#26512;&#12290;&#19982;&#22823;&#22810;&#25968;&#21069;&#26399;&#30740;&#31350;&#21482;&#30740;&#31350;&#24179;&#22343;&#36845;&#20195;&#30340;&#25910;&#25947;&#24615;&#19981;&#21516;&#65292;&#25105;&#20204;&#30740;&#31350;&#26368;&#21518;&#19968;&#27425;&#36845;&#20195;&#65292;&#36825;&#26159;&#22823;&#22810;&#25968;&#20154;&#22312;&#23454;&#36341;&#20013;&#20351;&#29992;&#30340;&#12290;&#24403;&#20165;&#32771;&#34385;&#26368;&#22351;&#24773;&#20917;&#20998;&#26512;&#26102;&#65292;&#25105;&#20204;&#30340;&#29702;&#35770;&#39044;&#27979;&#26368;&#20339;&#36873;&#25321;&#26159;&#32447;&#24615;&#34928;&#20943;&#35843;&#24230;&#65306;&#36825;&#26159;&#19968;&#31181;&#23454;&#36341;&#20013;&#24120;&#29992;&#30340;&#36873;&#25321;&#65292;&#20854;&#23558;&#27493;&#38271;&#19982;&#24403;&#21069;&#36845;&#20195;&#27425;&#25968;t&#21644;&#24635;&#27493;&#25968;T&#25104;&#27604;&#20363;&#22320;&#35774;&#32622;&#20026;1 - t/T&#12290;&#20026;&#20102;&#36229;&#36234;&#36825;&#31181;&#26368;&#22351;&#24773;&#20917;&#20998;&#26512;&#65292;&#25105;&#20204;&#20351;&#29992;&#35266;&#23519;&#21040;&#30340;&#26799;&#24230;&#33539;&#25968;&#26469;&#25512;&#23548;&#36866;&#24212;&#20110;&#29305;&#23450;&#20219;&#21153;&#30340;&#32454;&#21270;&#35843;&#24230;&#12290;&#36825;&#20123;&#32454;&#21270;&#35843;&#24230;&#34920;&#29616;&#20986;&#23398;&#20064;&#29575;&#36880;&#28176;&#22686;&#21152;&#21644;&#23398;&#20064;&#29575;&#36805;&#36895;&#36864;&#28779;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning rate schedules used in practice bear little resemblance to those recommended by theory. We close much of this theory/practice gap, and as a consequence are able to derive new problem-adaptive learning rate schedules. Our key technical contribution is a refined analysis of learning rate schedules for a wide class of optimization algorithms (including SGD). In contrast to most prior works that study the convergence of the average iterate, we study the last iterate, which is what most people use in practice. When considering only worst-case analysis, our theory predicts that the best choice is the linear decay schedule: a popular choice in practice that sets the stepsize proportionally to $1 - t/T$, where $t$ is the current iteration and $T$ is the total number of steps. To go beyond this worst-case analysis, we use the observed gradient norms to derive schedules refined for any particular task. These refined schedules exhibit learning rate warm-up and rapid learning rate anneali
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;NLP&#20013;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#30340;&#32454;&#24494;&#24046;&#21035;&#65292;&#37325;&#28857;&#20851;&#27880;&#22522;&#20110;&#27169;&#26495;&#30340;&#38382;&#39064;&#29983;&#25104;&#12290;&#36890;&#36807;&#35780;&#20272;&#20854;&#20248;&#21183;&#21644;&#22266;&#26377;&#38480;&#21046;&#65292;&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#21512;&#25104;&#25968;&#25454;&#23545;&#29616;&#20195;Transformer&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#24378;&#35843;&#20102;&#21512;&#25104;&#25968;&#25454;&#19982;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#20043;&#38388;&#25152;&#38656;&#30340;&#24494;&#22937;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2310.07830</link><description>&lt;p&gt;
&#21512;&#25104;&#25968;&#25454;&#26159;&#21542;&#33021;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26356;&#39640;&#25928;&#65311;
&lt;/p&gt;
&lt;p&gt;
Does Synthetic Data Make Large Language Models More Efficient?. (arXiv:2310.07830v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07830
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;NLP&#20013;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#30340;&#32454;&#24494;&#24046;&#21035;&#65292;&#37325;&#28857;&#20851;&#27880;&#22522;&#20110;&#27169;&#26495;&#30340;&#38382;&#39064;&#29983;&#25104;&#12290;&#36890;&#36807;&#35780;&#20272;&#20854;&#20248;&#21183;&#21644;&#22266;&#26377;&#38480;&#21046;&#65292;&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#21512;&#25104;&#25968;&#25454;&#23545;&#29616;&#20195;Transformer&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#24378;&#35843;&#20102;&#21512;&#25104;&#25968;&#25454;&#19982;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#20043;&#38388;&#25152;&#38656;&#30340;&#24494;&#22937;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#20986;&#29616;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#32463;&#21382;&#20102;&#28145;&#21051;&#30340;&#21464;&#38761;&#12290;&#30740;&#31350;&#20154;&#21592;&#25345;&#32493;&#38754;&#20020;&#30340;&#19968;&#20010;&#25361;&#25112;&#26159;&#39537;&#21160;&#36825;&#20123;&#27169;&#22411;&#30340;&#39640;&#36136;&#37327;&#26631;&#27880;&#25968;&#25454;&#38598;&#30340;&#31232;&#32570;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;NLP&#20013;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#30340;&#32454;&#24494;&#24046;&#21035;&#65292;&#37325;&#28857;&#20851;&#27880;&#22522;&#20110;&#27169;&#26495;&#30340;&#38382;&#39064;&#29983;&#25104;&#12290;&#36890;&#36807;&#35780;&#20272;&#20854;&#20248;&#21183;&#65292;&#21253;&#25324;&#25968;&#25454;&#25193;&#20805;&#28508;&#21147;&#21644;&#32467;&#26500;&#22810;&#26679;&#24615;&#30340;&#24341;&#20837;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#20248;&#28857;&#19982;&#22266;&#26377;&#38480;&#21046;&#36827;&#34892;&#20102;&#23545;&#27604;&#65292;&#22914;&#36807;&#25311;&#21512;&#39118;&#38505;&#21644;&#39044;&#23450;&#20041;&#27169;&#26495;&#25152;&#24102;&#26469;&#30340;&#38480;&#21046;&#12290;&#36890;&#36807;&#32463;&#39564;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22522;&#20110;&#27169;&#26495;&#30340;&#21512;&#25104;&#25968;&#25454;&#23545;&#29616;&#20195;Transformer&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36890;&#36807;&#24378;&#35843;&#21512;&#25104;&#25968;&#25454;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#20043;&#38388;&#25152;&#38656;&#30340;&#24494;&#22937;&#24179;&#34913;&#21450;&#23558;&#21512;&#25104;&#25968;&#25454;&#25972;&#21512;&#21040;&#27169;&#22411;&#35757;&#32451;&#27969;&#31243;&#20013;&#30340;&#26410;&#26469;&#36712;&#36857;&#26469;&#24635;&#32467;&#12290;&#36825;&#20123;&#21457;&#29616;&#26088;&#22312;&#25351;&#23548;NLP&#20174;&#19994;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural Language Processing (NLP) has undergone transformative changes with the advent of deep learning methodologies. One challenge persistently confronting researchers is the scarcity of high-quality, annotated datasets that drive these models. This paper explores the nuances of synthetic data generation in NLP, with a focal point on template-based question generation. By assessing its advantages, including data augmentation potential and the introduction of structured variety, we juxtapose these benefits against inherent limitations, such as the risk of overfitting and the constraints posed by pre-defined templates. Drawing from empirical evaluations, we demonstrate the impact of template-based synthetic data on the performance of modern transformer models. We conclude by emphasizing the delicate balance required between synthetic and real-world data, and the future trajectories of integrating synthetic data in model training pipelines. The findings aim to guide NLP practitioners in
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#35782;&#21035;&#21477;&#23376;&#31867;&#27604;&#30340;&#33021;&#21147;&#19982;&#20854;&#32534;&#30721;&#21477;&#27861;&#21644;&#35821;&#20041;&#32467;&#26500;&#33021;&#21147;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2310.07818</link><description>&lt;p&gt;
&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#31867;&#27604;&#35782;&#21035;&#19982;&#21477;&#23376;&#32467;&#26500;&#32534;&#30721;&#20043;&#38388;&#30340;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Exploring the Relationship between Analogy Identification and Sentence Structure Encoding in Large Language Models. (arXiv:2310.07818v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07818
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#35782;&#21035;&#21477;&#23376;&#31867;&#27604;&#30340;&#33021;&#21147;&#19982;&#20854;&#32534;&#30721;&#21477;&#27861;&#21644;&#35821;&#20041;&#32467;&#26500;&#33021;&#21147;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35782;&#21035;&#31867;&#27604;&#22312;&#20154;&#31867;&#35748;&#30693;&#21644;&#35821;&#35328;&#33021;&#21147;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#37324;&#65292;&#23545;&#20110;&#8220;A&#23545;B&#23601;&#20687;C&#23545;D&#8221;&#36825;&#31181;&#24418;&#24335;&#30340;&#35789;&#35821;&#31867;&#27604;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#28041;&#21450;&#26356;&#38271;&#25991;&#26412;&#30340;&#31867;&#27604;&#65292;&#22914;&#21477;&#23376;&#21644;&#21477;&#23376;&#38598;&#21512;&#65292;&#20256;&#36798;&#31867;&#27604;&#24847;&#20041;&#30340;&#38382;&#39064;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20852;&#36259;&#12290;&#24403;&#21069;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#31038;&#21306;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#35782;&#21035;&#27492;&#31867;&#31867;&#27604;&#30340;&#33021;&#21147;&#65292;&#20294;&#36825;&#20123;&#33021;&#21147;&#32972;&#21518;&#30340;&#21407;&#22240;&#38656;&#35201;&#36827;&#19968;&#27493;&#25506;&#31350;&#12290;&#27492;&#22806;&#65292;LLMs&#22312;&#20854;&#23884;&#20837;&#20013;&#32534;&#30721;&#35821;&#35328;&#30340;&#21477;&#27861;&#21644;&#35821;&#20041;&#32467;&#26500;&#30340;&#33021;&#21147;&#65292;&#36817;&#24180;&#26469;&#24471;&#21040;&#20102;&#26174;&#33879;&#20851;&#27880;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#20010;LLMs&#35782;&#21035;&#21477;&#23376;&#31867;&#27604;&#30340;&#33021;&#21147;&#19982;&#20854;&#32534;&#30721;&#21477;&#27861;&#21644;&#35821;&#20041;&#32467;&#26500;&#30340;&#33021;&#21147;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#36890;&#36807;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;LLMs&#30340;&#31867;&#27604;&#35782;&#21035;&#33021;&#21147;&#19982;&#20854;&#32534;&#30721;&#33021;&#21147;&#26377;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
Identifying analogies plays a pivotal role in human cognition and language proficiency. In the last decade, there has been extensive research on word analogies in the form of ``A is to B as C is to D.'' However, there is a growing interest in analogies that involve longer text, such as sentences and collections of sentences, which convey analogous meanings. While the current NLP research community evaluates the ability of Large Language Models (LLMs) to identify such analogies, the underlying reasons behind these abilities warrant deeper investigation. Furthermore, the capability of LLMs to encode both syntactic and semantic structures of language within their embeddings has garnered significant attention with the surge in their utilization. In this work, we examine the relationship between the abilities of multiple LLMs to identify sentence analogies, and their capacity to encode syntactic and semantic structures. Through our analysis, we find that analogy identification ability of LL
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;LabGym&#20154;&#24037;&#26234;&#33021;&#24320;&#21457;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#33021;&#22815;&#20934;&#30830;&#22320;&#35782;&#21035;&#26085;&#26412;&#29461;&#29492;&#30340;&#30707;&#22836;&#22788;&#29702;&#34892;&#20026;&#65292;&#20026;&#28789;&#38271;&#31867;&#23398;&#39046;&#22495;&#30340;&#34892;&#20026;&#20998;&#26512;&#25552;&#20379;&#20102;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2310.07812</link><description>&lt;p&gt;
&#20351;&#29992;LabGym&#20154;&#24037;&#26234;&#33021;&#33258;&#21160;&#35782;&#21035;&#26085;&#26412;&#29461;&#29492;&#30340;&#30707;&#22836;&#22788;&#29702;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Automatic Identification of Stone-Handling Behaviour in Japanese Macaques Using LabGym Artificial Intelligence. (arXiv:2310.07812v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07812
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;LabGym&#20154;&#24037;&#26234;&#33021;&#24320;&#21457;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#33021;&#22815;&#20934;&#30830;&#22320;&#35782;&#21035;&#26085;&#26412;&#29461;&#29492;&#30340;&#30707;&#22836;&#22788;&#29702;&#34892;&#20026;&#65292;&#20026;&#28789;&#38271;&#31867;&#23398;&#39046;&#22495;&#30340;&#34892;&#20026;&#20998;&#26512;&#25552;&#20379;&#20102;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#26368;&#26032;&#36827;&#23637;&#20026;&#22797;&#26434;&#34892;&#20026;&#30340;&#20998;&#26512;&#25171;&#24320;&#20102;&#26032;&#30340;&#21487;&#33021;&#12290;&#22312;&#27492;&#32972;&#26223;&#19979;&#65292;&#34892;&#20026;&#23398;&#23478;&#27491;&#22312;&#31215;&#26497;&#25506;&#32034;&#36825;&#20123;&#21019;&#26032;&#25216;&#26415;&#22312;&#20351;&#29992;&#35270;&#39057;&#25968;&#25454;&#36827;&#34892;&#34892;&#20026;&#20998;&#26512;&#30340;&#32791;&#26102;&#36807;&#31243;&#20013;&#30340;&#28508;&#21147;&#12290;&#22312;&#28789;&#38271;&#31867;&#23398;&#39046;&#22495;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#20960;&#31181;&#29992;&#20110;&#27492;&#30446;&#30340;&#30340;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#27599;&#20010;&#24037;&#20855;&#37117;&#38754;&#20020;&#25216;&#26415;&#38480;&#21046;&#65292;&#32780;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#22871;&#20840;&#38754;&#30340;&#21327;&#35758;&#65292;&#26088;&#22312;&#21457;&#25381;&#20808;&#36827;&#24037;&#20855;LabGym&#30340;&#21151;&#33021;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#35780;&#20272;LabGym&#22312;&#30740;&#31350;&#29461;&#29492;&#34892;&#20026;&#26041;&#38754;&#30340;&#36866;&#29992;&#24615;&#65292;&#37325;&#28857;&#20851;&#27880;&#26085;&#26412;&#29461;&#29492;&#20316;&#20026;&#25105;&#20204;&#30340;&#27169;&#22411;&#23545;&#35937;&#12290;&#25105;&#20204;&#25104;&#21151;&#22320;&#24320;&#21457;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#33021;&#22815;&#39640;&#20934;&#30830;&#24230;&#22320;&#26816;&#27979;&#26085;&#26412;&#29461;&#29492;&#30340;&#30707;&#22836;&#22788;&#29702;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#34892;&#20026;&#20998;&#26512;&#27169;&#22411;&#22914;&#39044;&#26399;&#23436;&#25104;&#65292;&#24182;&#19988;LabGym&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;
The latest advancements in artificial intelligence technology have opened doors to the analysis of intricate behaviours. In light of this, ethologists are actively exploring the potential of these innovations to streamline the time-intensive process of behavioural analysis using video data. In the realm of primatology, several tools have been developed for this purpose. Nonetheless, each of these tools grapples with technical constraints that we aim to surmount. To address these limitations, we have established a comprehensive protocol designed to harness the capabilities of a cutting-edge tool, LabGym. Our primary objective was to evaluate LabGym's suitability for the analysis of primate behaviour, with a focus on Japanese macaques as our model subjects. We have successfully developed a model that demonstrates a high degree of accuracy in detecting Japanese macaques stone-handling behaviour. Our behavioural analysis model was completed as per our initial expectations and LabGym succee
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#30456;&#20301;&#31354;&#38388;&#20013;&#26500;&#24314;&#36335;&#24452;&#27979;&#24230;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#24314;&#27169;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#21160;&#21147;&#20256;&#25773;&#30340;&#26089;&#26399;&#38454;&#27573;&#29983;&#25104;&#36924;&#30495;&#30340;&#25968;&#25454;&#28857;&#65292;&#24182;&#21033;&#29992;&#39069;&#22806;&#30340;&#36895;&#24230;&#20449;&#24687;&#23454;&#29616;&#39640;&#25928;&#30340;&#25968;&#25454;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2310.07805</link><description>&lt;p&gt;
&#20855;&#26377;&#30456;&#20301;&#38543;&#26426;&#26725;&#30340;&#29983;&#25104;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Generative Modeling with Phase Stochastic Bridges. (arXiv:2310.07805v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07805
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#30456;&#20301;&#31354;&#38388;&#20013;&#26500;&#24314;&#36335;&#24452;&#27979;&#24230;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#24314;&#27169;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#21160;&#21147;&#20256;&#25773;&#30340;&#26089;&#26399;&#38454;&#27573;&#29983;&#25104;&#36924;&#30495;&#30340;&#25968;&#25454;&#28857;&#65292;&#24182;&#21033;&#29992;&#39069;&#22806;&#30340;&#36895;&#24230;&#20449;&#24687;&#23454;&#29616;&#39640;&#25928;&#30340;&#25968;&#25454;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#65288;DMs&#65289;&#26159;&#29992;&#20110;&#36830;&#32493;&#36755;&#20837;&#30340;&#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;DMs&#36890;&#36807;&#22312;&#36755;&#20837;&#31354;&#38388;&#65288;&#21363;&#20301;&#32622;&#31354;&#38388;&#65289;&#20013;&#26500;&#24314;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;SDE&#65289;&#65292;&#24182;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21453;&#28436;&#26469;&#24037;&#20316;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#30456;&#20301;&#31354;&#38388;&#21160;&#21147;&#23398;&#30340;&#26032;&#22411;&#29983;&#25104;&#24314;&#27169;&#26694;&#26550;&#65292;&#20854;&#20013;&#30456;&#20301;&#31354;&#38388;&#34987;&#23450;&#20041;&#20026;&#19968;&#20010;&#21253;&#25324;&#20301;&#32622;&#21644;&#36895;&#24230;&#30340;&#22686;&#24378;&#31354;&#38388;&#12290;&#21033;&#29992;&#38543;&#26426;&#26368;&#20248;&#25511;&#21046;&#30340;&#27934;&#23519;&#21147;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#30456;&#20301;&#31354;&#38388;&#20013;&#30340;&#36335;&#24452;&#27979;&#24230;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#37319;&#26679;&#12290;&#19982;DMs&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#21160;&#21147;&#20256;&#25773;&#30340;&#26089;&#26399;&#38454;&#27573;&#23601;&#33021;&#22815;&#29983;&#25104;&#36924;&#30495;&#30340;&#25968;&#25454;&#28857;&#12290;&#36825;&#31181;&#26089;&#26399;&#39044;&#27979;&#20026;&#36890;&#36807;&#27839;&#36712;&#36857;&#21033;&#29992;&#39069;&#22806;&#30340;&#36895;&#24230;&#20449;&#24687;&#23454;&#29616;&#39640;&#25928;&#30340;&#25968;&#25454;&#29983;&#25104;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;&#22312;&#26631;&#20934;&#22270;&#20687;&#29983;&#25104;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#23567;&#20989;&#25968;&#35780;&#20272;&#25968;&#37327;&#30340;&#33539;&#22260;&#20869;&#34920;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models (DMs) represent state-of-the-art generative models for continuous inputs. DMs work by constructing a Stochastic Differential Equation (SDE) in the input space (ie, position space), and using a neural network to reverse it. In this work, we introduce a novel generative modeling framework grounded in \textbf{phase space dynamics}, where a phase space is defined as {an augmented space encompassing both position and velocity.} Leveraging insights from Stochastic Optimal Control, we construct a path measure in the phase space that enables efficient sampling. {In contrast to DMs, our framework demonstrates the capability to generate realistic data points at an early stage of dynamics propagation.} This early prediction sets the stage for efficient data generation by leveraging additional velocity information along the trajectory. On standard image generation benchmarks, our model yields favorable performance over baselines in the regime of small Number of Function Evaluation
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26222;&#36866;&#24615;&#30340;&#35748;&#30693;&#24189;&#40664;&#26426;&#21046;&#65292;&#24314;&#31435;&#22312;&#32422;&#26463;&#30340;&#27010;&#24565;&#19978;&#65292;&#36890;&#36807;&#37325;&#21472;&#32422;&#26463;&#30340;&#35266;&#23519;&#26469;&#35299;&#37322;&#24189;&#40664;&#30340;&#20135;&#29983;&#12290;</title><link>http://arxiv.org/abs/2310.07803</link><description>&lt;p&gt;
&#24189;&#40664;&#30340;&#19968;&#33324;&#26426;&#21046;&#65306;&#37325;&#26032;&#23450;&#20041;&#35821;&#20041;&#37325;&#21472;
&lt;/p&gt;
&lt;p&gt;
A general mechanism of humor: reformulating the semantic overlap. (arXiv:2310.07803v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07803
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26222;&#36866;&#24615;&#30340;&#35748;&#30693;&#24189;&#40664;&#26426;&#21046;&#65292;&#24314;&#31435;&#22312;&#32422;&#26463;&#30340;&#27010;&#24565;&#19978;&#65292;&#36890;&#36807;&#37325;&#21472;&#32422;&#26463;&#30340;&#35266;&#23519;&#26469;&#35299;&#37322;&#24189;&#40664;&#30340;&#20135;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26222;&#36866;&#24615;&#30340;&#35748;&#30693;&#24189;&#40664;&#26426;&#21046;&#65292;&#19981;&#20165;&#38480;&#20110;&#35821;&#35328;&#20132;&#27969;&#12290;&#23427;&#20511;&#37492;&#20102;Raskin&#20851;&#20110;&#24773;&#33410;&#37325;&#21472;&#30340;&#27010;&#24565;&#65292;&#24182;&#31526;&#21512;&#19981;&#19968;&#33268;&#24615;-&#35299;&#20915;&#29702;&#35770;&#26694;&#26550;&#65292;&#20294;&#26159;&#24314;&#31435;&#22312;&#32422;&#26463;&#30340;&#27010;&#24565;&#19978;&#65292;&#21363;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#25277;&#35937;&#23545;&#24212;&#20851;&#31995;&#12290;&#26681;&#25454;&#36825;&#31181;&#35266;&#28857;&#65292;&#24773;&#33410;&#37325;&#21472;&#26159;&#19968;&#31181;&#26356;&#25277;&#35937;&#25551;&#36848;&#30340;&#29616;&#35937;&#65292;&#21363;&#32422;&#26463;&#37325;&#21472;&#12290;&#25991;&#20013;&#24341;&#20837;&#20102;&#34987;&#24573;&#35270;&#30340;&#35770;&#35777;&#27010;&#24565;&#65292;&#29992;&#20110;&#25551;&#36848;&#20004;&#20010;&#37325;&#21472;&#30340;&#32422;&#26463;&#8212;&#8212;&#26126;&#26174;&#30340;&#21644;&#38544;&#34109;&#30340;&#12290;&#23427;&#20204;&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#24182;&#19981;&#30452;&#25509;&#32534;&#30721;&#22312;&#35805;&#35821;&#20013;&#65292;&#32780;&#26159;&#30001;&#35805;&#35821;&#26263;&#31034;&#65292;&#23427;&#20204;&#30340;&#37325;&#21472;&#23548;&#33268;&#22312;&#20132;&#27969;&#30340;&#35805;&#35821;&#23618;&#38754;&#19978;&#20135;&#29983;&#21478;&#19968;&#31181;&#37325;&#21472;&#65292;&#36825;&#31181;&#19981;&#19968;&#33268;&#24615;&#26174;&#38706;&#20102;&#20986;&#26469;&#12290;&#25105;&#20204;&#30340;&#20551;&#35774;&#26159;&#65292;&#21796;&#36215;&#36825;&#31181;&#32422;&#26463;&#26159;&#21548;&#20247;&#35299;&#37322;&#35805;&#35821;&#30340;&#25512;&#29702;&#36807;&#31243;&#30340;&#35748;&#30693;&#25928;&#24212;&#12290;&#25105;&#20204;&#22522;&#20110;Hofstadter&#20851;&#20110;&#26263;&#31034;&#30340;&#29702;&#35770;&#20551;&#35774;&#36825;&#19968;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article proposes a cognitive mechanism of humour of general applicability, not restricted to verbal communication. It is indebted to Raskin's concept of script overlap, and conforms to the incongruity-resolution theoretical framework, but it is built on the notion of constraint, an abstract correspondence between sets of data. Under this view, script overlap is an outcome of a more abstractly described phenomenon, constraint overlap. The important concept of the overlooked argument is introduced to characterise the two overlapping constraints -- overt and covert. Their inputs and outputs are not directly encoded in utterances, but implicated by them, and their overlap results in another overlap at the level of the communicated utterances, that the incongruity reveals. Our hypothesis assumes as a given that the evocation of such constraints is a cognitive effect of the inferential process by which a hearer interprets utterances. We base this assumption on Hofstadter's theory of ana
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#21033;&#29992;&#20449;&#24687;&#29942;&#39048;&#26041;&#27861;&#23545;&#24037;&#20316;&#36127;&#33655;&#21644;&#29702;&#35299;&#20043;&#38388;&#30340;&#24179;&#34913;&#36827;&#34892;&#20102;&#29305;&#24449;&#21270;&#65292;&#36890;&#36807;&#25277;&#35937;&#21270;&#26469;&#35299;&#37322;&#22797;&#26434;&#27010;&#24565;&#65292;&#20197;&#23454;&#29616;&#24037;&#20316;&#36127;&#33655;&#21644;&#29702;&#35299;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2310.07802</link><description>&lt;p&gt;
&#23545;&#29702;&#35299;-&#24037;&#20316;&#36127;&#33655;&#26435;&#34913;&#30340;&#20449;&#24687;&#29942;&#39048;&#29305;&#24449;&#21270;
&lt;/p&gt;
&lt;p&gt;
An Information Bottleneck Characterization of the Understanding-Workload Tradeoff. (arXiv:2310.07802v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07802
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#21033;&#29992;&#20449;&#24687;&#29942;&#39048;&#26041;&#27861;&#23545;&#24037;&#20316;&#36127;&#33655;&#21644;&#29702;&#35299;&#20043;&#38388;&#30340;&#24179;&#34913;&#36827;&#34892;&#20102;&#29305;&#24449;&#21270;&#65292;&#36890;&#36807;&#25277;&#35937;&#21270;&#26469;&#35299;&#37322;&#22797;&#26434;&#27010;&#24565;&#65292;&#20197;&#23454;&#29616;&#24037;&#20316;&#36127;&#33655;&#21644;&#29702;&#35299;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#23637;&#31361;&#26174;&#20102;&#23545;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#30340;&#38656;&#27714;&#65292;&#20197;&#25903;&#25345;&#20154;&#31867;&#23545;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#29702;&#35299;&#12290;&#32771;&#34385;&#21040;&#24433;&#21709;&#35299;&#37322;&#25928;&#21147;&#30340;&#20154;&#31867;&#22240;&#32032;&#65292;&#22914;&#24515;&#29702;&#36127;&#33655;&#21644;&#20154;&#31867;&#29702;&#35299;&#65292;&#26159;&#26377;&#25928;&#30340;XAI&#35774;&#35745;&#30340;&#26680;&#24515;&#12290;&#29616;&#26377;&#30340;XAI&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#20102;&#19981;&#21516;&#31867;&#22411;&#30340;&#35299;&#37322;&#24341;&#36215;&#30340;&#29702;&#35299;&#21644;&#24037;&#20316;&#36127;&#33655;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#36890;&#36807;&#25277;&#35937;&#65288;&#25163;&#24037;&#21046;&#20316;&#30340;&#30456;&#20851;&#38382;&#39064;&#29305;&#24449;&#30340;&#32452;&#21512;&#65289;&#35299;&#37322;&#22797;&#26434;&#27010;&#24565;&#24050;&#34987;&#35777;&#26126;&#33021;&#22815;&#26377;&#25928;&#22320;&#35299;&#20915;&#21644;&#24179;&#34913;&#36825;&#31181;&#24037;&#20316;&#36127;&#33655;-&#29702;&#35299;&#30340;&#26435;&#34913;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20449;&#24687;&#29942;&#39048;&#26041;&#27861;&#23545;&#24037;&#20316;&#36127;&#33655;-&#29702;&#35299;&#24179;&#34913;&#36827;&#34892;&#29305;&#24449;&#21270;&#65306;&#36825;&#26159;&#19968;&#31181;&#33258;&#21160;&#29983;&#25104;&#26368;&#22823;&#21270;&#20449;&#24687;&#37327;&#21644;&#26368;&#23567;&#21270;&#22797;&#26434;&#24615;&#30340;&#25277;&#35937;&#30340;&#20449;&#24687;&#35770;&#26041;&#27861;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#36890;&#36807;&#20154;&#20307;&#23454;&#39564;&#24314;&#31435;&#20102;&#24037;&#20316;&#36127;&#33655;&#19982;&#22797;&#26434;&#24615;&#20043;&#38388;&#20197;&#21450;&#29702;&#35299;&#19982;&#20449;&#24687;&#37327;&#20043;&#38388;&#30340;&#23454;&#35777;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in artificial intelligence (AI) have underscored the need for explainable AI (XAI) to support human understanding of AI systems. Consideration of human factors that impact explanation efficacy, such as mental workload and human understanding, is central to effective XAI design. Existing work in XAI has demonstrated a tradeoff between understanding and workload induced by different types of explanations. Explaining complex concepts through abstractions (hand-crafted groupings of related problem features) has been shown to effectively address and balance this workload-understanding tradeoff. In this work, we characterize the workload-understanding balance via the Information Bottleneck method: an information-theoretic approach which automatically generates abstractions that maximize informativeness and minimize complexity. In particular, we establish empirical connections between workload and complexity and between understanding and informativeness through human-subject e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36712;&#36857;&#24863;&#30693;&#30340;&#20027;&#35201;&#27969;&#24418;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#25968;&#25454;&#22686;&#24378;&#21644;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#25552;&#21462;&#26356;&#32039;&#20945;&#30340;&#27969;&#24418;&#34920;&#31034;&#65292;&#24182;&#23454;&#29616;&#23569;&#26679;&#26412;&#22270;&#20687;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2310.07801</link><description>&lt;p&gt;
&#36712;&#36857;&#24863;&#30693;&#30340;&#20027;&#35201;&#27969;&#24418;&#26694;&#26550;&#29992;&#20110;&#25968;&#25454;&#22686;&#24378;&#21644;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Trajectory-aware Principal Manifold Framework for Data Augmentation and Image Generation. (arXiv:2310.07801v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07801
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36712;&#36857;&#24863;&#30693;&#30340;&#20027;&#35201;&#27969;&#24418;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#25968;&#25454;&#22686;&#24378;&#21644;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#25552;&#21462;&#26356;&#32039;&#20945;&#30340;&#27969;&#24418;&#34920;&#31034;&#65292;&#24182;&#23454;&#29616;&#23569;&#26679;&#26412;&#22270;&#20687;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#25968;&#25454;&#22686;&#24378;&#22312;&#27169;&#22411;&#35757;&#32451;&#12289;&#22270;&#20687;&#36716;&#25442;&#12289;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#31561;&#39046;&#22495;&#26377;&#30410;&#22788;&#12290;&#35768;&#22810;&#29616;&#26377;&#26041;&#27861;&#20174;&#21442;&#25968;&#20998;&#24067;&#65288;&#22914;&#39640;&#26031;&#20998;&#24067;&#65289;&#20013;&#29983;&#25104;&#26032;&#26679;&#26412;&#65292;&#21364;&#24456;&#23569;&#20851;&#27880;&#22312;&#36755;&#20837;&#25110;&#29305;&#24449;&#31354;&#38388;&#27839;&#25968;&#25454;&#27969;&#24418;&#29983;&#25104;&#26679;&#26412;&#12290;&#26412;&#25991;&#39564;&#35777;&#20102;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#20351;&#29992;&#38544;&#34255;&#30340;&#20027;&#35201;&#27969;&#24418;&#27604;&#39640;&#26031;&#20998;&#24067;&#24102;&#26469;&#20102;&#29702;&#35770;&#21644;&#23454;&#38469;&#19978;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36712;&#36857;&#24863;&#30693;&#30340;&#20027;&#35201;&#27969;&#24418;&#26694;&#26550;&#65292;&#29992;&#20110;&#24674;&#22797;&#27969;&#24418;&#39592;&#24178;&#24182;&#27839;&#30528;&#29305;&#23450;&#36712;&#36857;&#29983;&#25104;&#26679;&#26412;&#12290;&#22312;&#33258;&#21160;&#32534;&#30721;&#22120;&#32467;&#26500;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;&#19968;&#20010;&#20869;&#22312;&#32500;&#24230;&#27491;&#21017;&#21270;&#39033;&#65292;&#20351;&#27969;&#24418;&#26356;&#32039;&#20945;&#65292;&#24182;&#23454;&#29616;&#23569;&#26679;&#26412;&#22270;&#20687;&#29983;&#25104;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#33021;&#22815;&#25552;&#21462;&#26356;&#32039;&#20945;&#30340;&#27969;&#24418;&#34920;&#31034;&#65292;&#25552;&#39640;&#20998;&#31867;&#20934;&#30830;&#24615;&#24182;&#29983;&#25104;&#24179;&#28369;&#30340;&#21464;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data augmentation for deep learning benefits model training, image transformation, medical imaging analysis and many other fields. Many existing methods generate new samples from a parametric distribution, like the Gaussian, with little attention to generate samples along the data manifold in either the input or feature space. In this paper, we verify that there are theoretical and practical advantages of using the principal manifold hidden in the feature space than the Gaussian distribution. We then propose a novel trajectory-aware principal manifold framework to restore the manifold backbone and generate samples along a specific trajectory. On top of the autoencoder architecture, we further introduce an intrinsic dimension regularization term to make the manifold more compact and enable few-shot image generation. Experimental results show that the novel framework is able to extract more compact manifold representation, improve classification accuracy and generate smooth transformatio
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#35299;&#37322;&#24615;&#27880;&#24847;&#21147;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#35782;&#21035;&#36755;&#20837;&#25968;&#25454;&#30340;&#20851;&#38190;&#37096;&#20998;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.07800</link><description>&lt;p&gt;
&#35299;&#37322;&#24615;&#27880;&#24847;&#21147;&#29992;&#20110;&#23569;&#26679;&#26412;&#23398;&#20064;&#21450;&#20854;&#23427;&#39046;&#22495;
&lt;/p&gt;
&lt;p&gt;
Explainable Attention for Few-shot Learning and Beyond. (arXiv:2310.07800v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07800
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#35299;&#37322;&#24615;&#27880;&#24847;&#21147;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#35782;&#21035;&#36755;&#20837;&#25968;&#25454;&#30340;&#20851;&#38190;&#37096;&#20998;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27880;&#24847;&#21147;&#26426;&#21046;&#22312;&#22686;&#24378;&#23398;&#20064;&#27169;&#22411;&#20013;&#26174;&#31034;&#20986;&#20102;&#26377;&#24076;&#26395;&#30340;&#28508;&#21147;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#35782;&#21035;&#36755;&#20837;&#25968;&#25454;&#20013;&#26174;&#33879;&#30340;&#37096;&#20998;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#36825;&#22312;&#25968;&#25454;&#25910;&#38598;&#21644;&#26631;&#35760;&#26041;&#38754;&#23384;&#22312;&#25361;&#25112;&#65292;&#23548;&#33268;&#35757;&#32451;&#26679;&#26412;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#23588;&#20854;&#26377;&#20215;&#20540;&#12290;&#21463;&#20154;&#31867;&#35748;&#30693;&#36807;&#31243;&#21551;&#21457;&#65292;&#25105;&#20204;&#35748;&#20026;&#65292;&#22914;&#26524;&#23558;AI&#22522;&#32447;&#26292;&#38706;&#20110;&#21407;&#22987;&#25968;&#25454;&#30340;&#20851;&#38190;&#37096;&#20998;&#32780;&#19981;&#26159;&#25972;&#20010;&#36755;&#20837;&#25968;&#25454;&#38598;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#24863;&#30693;&#65292;&#37027;&#20040;&#23427;&#30340;&#24615;&#33021;&#21487;&#33021;&#20250;&#26356;&#20934;&#30830;&#12289;&#26356;&#21487;&#38752;&#12290;&#28982;&#32780;&#65292;&#36873;&#25321;&#36825;&#20123;&#20449;&#24687;&#24615;&#25968;&#25454;&#37096;&#20998;&#30340;&#20219;&#21153;&#65292;&#21363;&#30828;&#27880;&#24847;&#21147;&#23547;&#25214;&#65292;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#22312;&#23569;&#37327;&#35757;&#32451;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#24456;&#38590;&#25214;&#21040;&#36825;&#20123;&#20449;&#24687;&#24615;&#21306;&#22495;&#65292;&#21407;&#22240;&#26159;&#22823;&#37327;&#30340;&#35757;&#32451;&#21442;&#25968;&#26080;&#27861;&#20174;&#26377;&#38480;&#30340;&#26679;&#26412;&#20013;&#26377;&#25928;&#23398;&#20064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#30828;&#27880;&#24847;&#21147;&#23547;&#25214;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attention mechanisms have exhibited promising potential in enhancing learning models by identifying salient portions of input data. This is particularly valuable in scenarios where limited training samples are accessible due to challenges in data collection and labeling. Drawing inspiration from human recognition processes, we posit that an AI baseline's performance could be more accurate and dependable if it is exposed to essential segments of raw data rather than the entire input dataset, akin to human perception. However, the task of selecting these informative data segments, referred to as hard attention finding, presents a formidable challenge. In situations with few training samples, existing studies struggle to locate such informative regions due to the large number of training parameters that cannot be effectively learned from the available limited samples. In this study, we introduce a novel and practical framework for achieving explainable hard attention finding, specifically
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36801;&#31227;&#23398;&#20064;&#30340;EMR&#25968;&#25454;&#38598;&#20043;&#38388;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#30340;&#39044;&#21518;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#26500;&#24314;&#36807;&#28193;&#27169;&#22411;&#35299;&#20915;&#20102;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#29305;&#24449;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.07799</link><description>&lt;p&gt;
&#22522;&#20110;&#36801;&#31227;&#23398;&#20064;&#30340;EMR&#25968;&#25454;&#38598;&#20043;&#38388;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#30340;&#39044;&#21518;&#39044;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Transfer-Learning-Based Prognosis Prediction Paradigm that Bridges Data Distribution Shift across EMR Datasets. (arXiv:2310.07799v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07799
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36801;&#31227;&#23398;&#20064;&#30340;EMR&#25968;&#25454;&#38598;&#20043;&#38388;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#30340;&#39044;&#21518;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#26500;&#24314;&#36807;&#28193;&#27169;&#22411;&#35299;&#20915;&#20102;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#29305;&#24449;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#23545;&#26032;&#20852;&#30142;&#30149;&#30340;&#20449;&#24687;&#26377;&#38480;&#65292;&#30151;&#29366;&#24456;&#38590;&#34987;&#23519;&#35273;&#21644;&#35748;&#35782;&#21040;&#65292;&#22240;&#27492;&#21487;&#33021;&#24573;&#35270;&#20020;&#24202;&#24178;&#39044;&#30340;&#31383;&#21475;&#12290;&#26399;&#26395;&#33021;&#22815;&#24314;&#31435;&#19968;&#20010;&#26377;&#25928;&#30340;&#39044;&#21518;&#27169;&#22411;&#65292;&#36741;&#21161;&#21307;&#29983;&#36827;&#34892;&#27491;&#30830;&#35786;&#26029;&#21644;&#21046;&#23450;&#20010;&#24615;&#21270;&#27835;&#30103;&#26041;&#26696;&#65292;&#20174;&#32780;&#21450;&#26102;&#39044;&#38450;&#19981;&#21033;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22312;&#30142;&#30149;&#26089;&#26399;&#38454;&#27573;&#65292;&#30001;&#20110;&#25968;&#25454;&#25910;&#38598;&#21644;&#20020;&#24202;&#32463;&#39564;&#26377;&#38480;&#65292;&#20877;&#21152;&#19978;&#23545;&#38544;&#31169;&#21644;&#20262;&#29702;&#30340;&#32771;&#34385;&#65292;&#23548;&#33268;&#21487;&#20379;&#21442;&#32771;&#30340;&#25968;&#25454;&#21463;&#38480;&#65292;&#29978;&#33267;&#38590;&#20197;&#27491;&#30830;&#26631;&#35760;&#25968;&#25454;&#26631;&#31614;&#12290;&#27492;&#22806;&#65292;&#19981;&#21516;&#30142;&#30149;&#25110;&#21516;&#19968;&#30142;&#30149;&#19981;&#21516;&#26469;&#28304;&#30340;&#30005;&#23376;&#21307;&#30103;&#35760;&#24405;&#65288;EMR&#65289;&#25968;&#25454;&#21487;&#33021;&#23384;&#22312;&#20005;&#37325;&#30340;&#36328;&#25968;&#25454;&#38598;&#29305;&#24449;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#20005;&#37325;&#24433;&#21709;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#24314;&#31435;&#19968;&#20010;&#20174;&#28304;&#25968;&#25454;&#38598;&#21040;&#30446;&#26631;&#25968;&#25454;&#38598;&#30340;&#36807;&#28193;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#29305;&#24449;&#36827;&#34892;&#32422;&#26463;&#65292;&#26469;&#35299;&#20915;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the limited information about emerging diseases, symptoms are hard to be noticed and recognized, so that the window for clinical intervention could be ignored. An effective prognostic model is expected to assist doctors in making right diagnosis and designing personalized treatment plan, so to promptly prevent unfavorable outcomes. However, in the early stage of a disease, limited data collection and clinical experiences, plus the concern out of privacy and ethics, may result in restricted data availability for reference, to the extent that even data labels are difficult to mark correctly. In addition, Electronic Medical Record (EMR) data of different diseases or of different sources of the same disease can prove to be having serious cross-dataset feature misalignment problems, greatly mutilating the efficiency of deep learning models. This article introduces a transfer learning method to build a transition model from source dataset to target dataset. By way of constraining the 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GenTKG&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#19978;&#36827;&#34892;&#39044;&#27979;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#32467;&#21512;&#22522;&#20110;&#26102;&#38388;&#36923;&#36753;&#35268;&#21017;&#30340;&#26816;&#32034;&#31574;&#30053;&#21644;&#36731;&#37327;&#32423;&#30340;&#21442;&#25968;&#25928;&#29575;&#25351;&#23548;&#65292;&#20811;&#26381;&#20102;&#22797;&#26434;&#30340;&#26102;&#38388;&#22270;&#25968;&#25454;&#32467;&#26500;&#21644;&#24222;&#22823;&#30340;&#25968;&#25454;&#37327;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.07793</link><description>&lt;p&gt;
GenTKG: &#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
GenTKG: Generative Forecasting on Temporal Knowledge Graph. (arXiv:2310.07793v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07793
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GenTKG&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#19978;&#36827;&#34892;&#39044;&#27979;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#32467;&#21512;&#22522;&#20110;&#26102;&#38388;&#36923;&#36753;&#35268;&#21017;&#30340;&#26816;&#32034;&#31574;&#30053;&#21644;&#36731;&#37327;&#32423;&#30340;&#21442;&#25968;&#25928;&#29575;&#25351;&#23548;&#65292;&#20811;&#26381;&#20102;&#22797;&#26434;&#30340;&#26102;&#38388;&#22270;&#25968;&#25454;&#32467;&#26500;&#21644;&#24222;&#22823;&#30340;&#25968;&#25454;&#37327;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#24555;&#36895;&#21457;&#23637;&#24341;&#21457;&#20102;&#23545;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;(tKG)&#39046;&#22495;&#30340;&#20852;&#36259;&#65292;&#20854;&#20013;&#20256;&#32479;&#30340;&#22522;&#20110;&#23884;&#20837;&#21644;&#35268;&#21017;&#30340;&#27169;&#22411;&#21344;&#20027;&#23548;&#22320;&#20301;&#12290;&#30446;&#21069;&#20173;&#28982;&#23384;&#22312;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#39044;&#35757;&#32451;&#30340;LLM&#26159;&#21542;&#33021;&#22815;&#29702;&#35299;&#32467;&#26500;&#21270;&#30340;&#26102;&#38388;&#20851;&#31995;&#25968;&#25454;&#65292;&#24182;&#21462;&#20195;&#23427;&#20204;&#25104;&#20026;&#26102;&#38388;&#20851;&#31995;&#39044;&#27979;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23558;&#26102;&#38388;&#30693;&#35782;&#39044;&#27979;&#24341;&#20837;&#29983;&#25104;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#22312;&#22797;&#26434;&#30340;&#26102;&#38388;&#22270;&#25968;&#25454;&#32467;&#26500;&#21644;LLM&#21487;&#20197;&#22788;&#29702;&#30340;&#24207;&#21015;&#33258;&#28982;&#34920;&#36798;&#20043;&#38388;&#23384;&#22312;&#24040;&#22823;&#30340;&#40511;&#27807;&#65292;&#22312;tKG&#30340;&#24222;&#22823;&#25968;&#25454;&#37327;&#21644;&#24494;&#35843;LLM&#30340;&#24040;&#22823;&#35745;&#31639;&#25104;&#26412;&#20043;&#38388;&#20063;&#23384;&#22312;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#26694;&#26550;&#65292;&#31216;&#20026;GenTKG&#65292;&#23427;&#22312;tKG&#19978;&#25191;&#34892;&#29983;&#25104;&#24335;&#39044;&#27979;&#65292;&#32467;&#21512;&#20102;&#22522;&#20110;&#26102;&#38388;&#36923;&#36753;&#35268;&#21017;&#30340;&#26816;&#32034;&#31574;&#30053;&#21644;&#36731;&#37327;&#32423;&#30340;&#21442;&#25968;&#25928;&#29575;&#25351;&#23548;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;GenTKG&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid advancements in large language models (LLMs) have ignited interest in the temporal knowledge graph (tKG) domain, where conventional carefully designed embedding-based and rule-based models dominate. The question remains open of whether pre-trained LLMs can understand structured temporal relational data and replace them as the foundation model for temporal relational forecasting. Therefore, we bring temporal knowledge forecasting into the generative setting. However, challenges occur in the huge chasms between complex temporal graph data structure and sequential natural expressions LLMs can handle, and between the enormous data sizes of tKGs and heavy computation costs of finetuning LLMs. To address these challenges, we propose a novel retrieval augmented generation framework that performs generative forecasting on tKGs named GenTKG, which combines a temporal logical rule-based retrieval strategy and lightweight parameter-efficient instruction tuning. Extensive experiments hav
&lt;/p&gt;</description></item><item><title>DrivingDiffusion&#26159;&#19968;&#20010;&#22522;&#20110;3D&#24067;&#23616;&#30340;&#22810;&#35270;&#35282;&#39550;&#39542;&#22330;&#26223;&#35270;&#39057;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#35299;&#20915;&#36328;&#35270;&#35282;&#21644;&#36328;&#24103;&#30340;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#20197;&#21450;&#20445;&#35777;&#29983;&#25104;&#23454;&#20363;&#36136;&#37327;&#65292;&#23454;&#29616;&#20102;&#36924;&#30495;&#30340;&#22810;&#35270;&#35282;&#35270;&#39057;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2310.07771</link><description>&lt;p&gt;
DrivingDiffusion&#65306;&#22522;&#20110;&#24067;&#23616;&#24341;&#23548;&#30340;&#22810;&#35270;&#35282;&#39550;&#39542;&#22330;&#26223;&#35270;&#39057;&#29983;&#25104;&#19982;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DrivingDiffusion: Layout-Guided multi-view driving scene video generation with latent diffusion model. (arXiv:2310.07771v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07771
&lt;/p&gt;
&lt;p&gt;
DrivingDiffusion&#26159;&#19968;&#20010;&#22522;&#20110;3D&#24067;&#23616;&#30340;&#22810;&#35270;&#35282;&#39550;&#39542;&#22330;&#26223;&#35270;&#39057;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#35299;&#20915;&#36328;&#35270;&#35282;&#21644;&#36328;&#24103;&#30340;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#20197;&#21450;&#20445;&#35777;&#29983;&#25104;&#23454;&#20363;&#36136;&#37327;&#65292;&#23454;&#29616;&#20102;&#36924;&#30495;&#30340;&#22810;&#35270;&#35282;&#35270;&#39057;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22522;&#20110;&#24378;&#22823;&#21644;&#32479;&#19968;&#30340;&#40479;&#30640;&#22270;&#65288;BEV&#65289;&#34920;&#31034;&#30340;&#33258;&#21160;&#39550;&#39542;&#30340;&#26222;&#21450;&#65292;&#23545;&#20855;&#26377;&#20934;&#30830;&#27880;&#37322;&#30340;&#39640;&#36136;&#37327;&#21644;&#22823;&#35268;&#27169;&#22810;&#35270;&#35282;&#35270;&#39057;&#25968;&#25454;&#30340;&#38656;&#27714;&#36843;&#20999;&#23384;&#22312;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26114;&#36149;&#30340;&#37319;&#38598;&#21644;&#27880;&#37322;&#25104;&#26412;&#65292;&#33719;&#24471;&#27492;&#31867;&#22823;&#35268;&#27169;&#22810;&#35270;&#35282;&#25968;&#25454;&#24182;&#38750;&#26131;&#20107;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31354;&#38388;-&#26102;&#38388;&#19968;&#33268;&#24615;&#30340;&#25193;&#25955;&#26694;&#26550;DrivingDiffusion&#65292;&#20197;&#29983;&#25104;&#30001;3D&#24067;&#23616;&#25511;&#21046;&#30340;&#36924;&#30495;&#22810;&#35270;&#35282;&#35270;&#39057;&#12290;&#22312;&#32473;&#23450;3D&#24067;&#23616;&#30340;&#24773;&#20917;&#19979;&#21512;&#25104;&#22810;&#35270;&#35282;&#35270;&#39057;&#26102;&#65292;&#23384;&#22312;&#19977;&#20010;&#25361;&#25112;&#65306;&#22914;&#20309;&#20445;&#25345;1&#65289;&#36328;&#35270;&#35282;&#30340;&#19968;&#33268;&#24615;&#21644;2&#65289;&#36328;&#24103;&#30340;&#19968;&#33268;&#24615;&#65311;3&#65289;&#22914;&#20309;&#20445;&#35777;&#29983;&#25104;&#23454;&#20363;&#30340;&#36136;&#37327;&#65311;&#25105;&#20204;&#30340;DrivingDiffusion&#36890;&#36807;&#32423;&#32852;&#22810;&#35270;&#35282;&#21333;&#24103;&#22270;&#20687;&#29983;&#25104;&#27493;&#39588;&#12289;&#34987;&#22810;&#20010;&#25668;&#20687;&#26426;&#20849;&#20139;&#30340;&#21333;&#35270;&#35282;&#35270;&#39057;&#29983;&#25104;&#27493;&#39588;&#20197;&#21450;&#33021;&#22815;&#22788;&#29702;&#38271;&#35270;&#39057;&#29983;&#25104;&#30340;&#21518;&#22788;&#29702;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#22312;&#22810;&#35270;&#35282;&#27169;&#22411;&#20013;&#65292;&#29983;&#25104;&#23454;&#20363;&#30340;&#19968;&#33268;&#24615;&#33021;&#22815;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increasing popularity of autonomous driving based on the powerful and unified bird's-eye-view (BEV) representation, a demand for high-quality and large-scale multi-view video data with accurate annotation is urgently required. However, such large-scale multi-view data is hard to obtain due to expensive collection and annotation costs. To alleviate the problem, we propose a spatial-temporal consistent diffusion framework DrivingDiffusion, to generate realistic multi-view videos controlled by 3D layout. There are three challenges when synthesizing multi-view videos given a 3D layout: How to keep 1) cross-view consistency and 2) cross-frame consistency? 3) How to guarantee the quality of the generated instances? Our DrivingDiffusion solves the problem by cascading the multi-view single-frame image generation step, the single-view video generation step shared by multiple cameras, and post-processing that can handle long video generation. In the multi-view model, the consistency of
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#31163;&#32447;&#25511;&#21046;&#22120;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#31163;&#32447;&#25968;&#25454;&#38598;&#20316;&#20026;&#20915;&#31574;&#35821;&#26009;&#24211;&#65292;&#22312;&#20302;&#25968;&#25454;&#22330;&#26223;&#20013;&#23454;&#29616;&#20102;&#38382;&#36131;&#21046;&#30340;&#25511;&#21046;&#65292;&#24182;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#23637;&#31034;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.07747</link><description>&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38382;&#36131;&#21046;&#65306;&#29992;&#35821;&#26009;&#24211;&#30340;&#20363;&#23376;&#35299;&#37322;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Accountability in Offline Reinforcement Learning: Explaining Decisions with a Corpus of Examples. (arXiv:2310.07747v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07747
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#31163;&#32447;&#25511;&#21046;&#22120;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#31163;&#32447;&#25968;&#25454;&#38598;&#20316;&#20026;&#20915;&#31574;&#35821;&#26009;&#24211;&#65292;&#22312;&#20302;&#25968;&#25454;&#22330;&#26223;&#20013;&#23454;&#29616;&#20102;&#38382;&#36131;&#21046;&#30340;&#25511;&#21046;&#65292;&#24182;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#23637;&#31034;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20915;&#31574;&#31995;&#32479;&#20013;&#20351;&#29992;&#31163;&#32447;&#25968;&#25454;&#23398;&#20064;&#36879;&#26126;&#12289;&#21487;&#35299;&#37322;&#30340;&#25511;&#21046;&#22120;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#22240;&#20026;&#23427;&#26377;&#28508;&#21147;&#38477;&#20302;&#22312;&#29616;&#23454;&#19990;&#30028;&#31995;&#32479;&#20013;&#24212;&#29992;&#30340;&#39118;&#38505;&#12290;&#28982;&#32780;&#65292;&#22312;&#36131;&#20219;&#25935;&#24863;&#30340;&#35774;&#32622;&#65288;&#22914;&#21307;&#30103;&#20445;&#20581;&#65289;&#20013;&#65292;&#20915;&#31574;&#38382;&#36131;&#21046;&#38750;&#24120;&#37325;&#35201;&#65292;&#20294;&#30446;&#21069;&#30340;&#25991;&#29486;&#23578;&#26410;&#20805;&#20998;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Accountable Offline Controller&#65288;AOC&#65289;&#30340;&#26041;&#27861;&#65292;&#23427;&#23558;&#31163;&#32447;&#25968;&#25454;&#38598;&#20316;&#20026;&#20915;&#31574;&#35821;&#26009;&#24211;&#65292;&#24182;&#26681;&#25454;&#19968;&#32452;&#23450;&#21046;&#30340;&#20363;&#23376;&#65288;&#31216;&#20026;&#35821;&#26009;&#24211;&#23376;&#38598;&#65289;&#36827;&#34892;&#38382;&#36131;&#21046;&#30340;&#25511;&#21046;&#12290;AOC&#22312;&#20302;&#25968;&#25454;&#22330;&#26223;&#20013;&#26377;&#25928;&#22320;&#36816;&#34892;&#65292;&#21487;&#20197;&#25193;&#23637;&#21040;&#20005;&#26684;&#30340;&#31163;&#32447;&#27169;&#20223;&#35774;&#32622;&#65292;&#24182;&#34920;&#29616;&#20986;&#20445;&#25252;&#21644;&#36866;&#24212;&#24615;&#30340;&#29305;&#28857;&#12290;&#25105;&#20204;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#30340;&#21307;&#30103;&#20445;&#20581;&#22330;&#26223;&#20013;&#35780;&#20272;&#20102;AOC&#30340;&#24615;&#33021;&#65292;&#24378;&#35843;&#20102;&#23427;&#22312;&#20445;&#25345;&#38382;&#36131;&#21046;&#30340;&#21516;&#26102;&#33021;&#22815;&#31649;&#29702;&#39640;&#27700;&#24179;&#30340;&#31163;&#32447;&#25511;&#21046;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning transparent, interpretable controllers with offline data in decision-making systems is an essential area of research due to its potential to reduce the risk of applications in real-world systems. However, in responsibility-sensitive settings such as healthcare, decision accountability is of paramount importance, yet has not been adequately addressed by the literature. This paper introduces the Accountable Offline Controller (AOC) that employs the offline dataset as the Decision Corpus and performs accountable control based on a tailored selection of examples, referred to as the Corpus Subset. ABC operates effectively in low-data scenarios, can be extended to the strictly offline imitation setting, and displays qualities of both conservation and adaptability. We assess ABC's performance in both simulated and real-world healthcare scenarios, emphasizing its capability to manage offline control tasks with high levels of performance while maintaining accountability.  Keywords: Int
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;AI&#31639;&#27861;&#22312;Grasshopper / Rhinoceros 7&#20013;&#29983;&#25104;&#19977;&#32500;&#26080;&#38556;&#30861;&#22369;&#36947;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#29615;&#22659;&#30340;3D&#27169;&#22411;&#65292;&#33258;&#21160;&#30830;&#23450;&#26368;&#20339;&#36335;&#24452;&#26469;&#25552;&#39640;&#26080;&#38556;&#30861;&#22369;&#36947;&#35774;&#35745;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.07728</link><description>&lt;p&gt;
AI&#31639;&#27861;&#29992;&#20110;&#22312;Grasshopper / Rhinoceros 7&#20013;&#29983;&#25104;&#19977;&#32500;&#26080;&#38556;&#30861;&#22369;&#36947;
&lt;/p&gt;
&lt;p&gt;
AI Algorithm for the Generation of Three-Dimensional Accessibility Ramps in Grasshopper / Rhinoceros 7. (arXiv:2310.07728v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07728
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;AI&#31639;&#27861;&#22312;Grasshopper / Rhinoceros 7&#20013;&#29983;&#25104;&#19977;&#32500;&#26080;&#38556;&#30861;&#22369;&#36947;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#29615;&#22659;&#30340;3D&#27169;&#22411;&#65292;&#33258;&#21160;&#30830;&#23450;&#26368;&#20339;&#36335;&#24452;&#26469;&#25552;&#39640;&#26080;&#38556;&#30861;&#22369;&#36947;&#35774;&#35745;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#38556;&#30861;&#22522;&#30784;&#35774;&#26045;&#20316;&#20026;&#22478;&#24066;&#21457;&#23637;&#30340;&#19968;&#37096;&#20998;&#32463;&#24120;&#34987;&#24573;&#35270;, &#20294;&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#19981;&#21487;&#21542;&#35748;&#20854;&#37325;&#35201;&#24615;&#12290;&#26080;&#38556;&#30861;&#22369;&#36947;&#26159;&#26368;&#24120;&#35265;&#30340;&#26080;&#38556;&#30861;&#22522;&#30784;&#35774;&#26045;&#20043;&#19968;&#65292;&#19981;&#20165;&#23545;&#34892;&#21160;&#21463;&#38480;&#30340;&#20154;&#32676;&#26377;&#30410;&#65292;&#36824;&#33021;&#24110;&#21161;&#36523;&#20307;&#20581;&#20840;&#30340;&#31532;&#19977;&#26041;&#12290;&#23613;&#31649;&#20154;&#20204;&#35748;&#35782;&#21040;&#26080;&#38556;&#30861;&#22369;&#36947;&#30340;&#24517;&#35201;&#24615;&#65292;&#20294;&#30001;&#20110;&#35774;&#35745;&#38454;&#27573;&#25152;&#38656;&#30340;&#20154;&#21147;&#30340;&#38480;&#21046;&#65292;&#23454;&#38469;&#23454;&#26045;&#20173;&#23384;&#22312;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#33021;&#22815;&#22522;&#20110;&#30456;&#20851;&#29615;&#22659;&#30340;3D&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#21487;&#34892;&#30340;&#26080;&#38556;&#30861;&#22369;&#36947;&#12290;&#36890;&#36807;&#22312;3D&#27169;&#22411;&#20013;&#25163;&#21160;&#25351;&#23450;&#36215;&#28857;&#21644;&#32456;&#28857;&#65292;&#31639;&#27861;&#20351;&#29992;AI&#25628;&#32034;&#31639;&#27861;&#30830;&#23450;&#36830;&#25509;&#36825;&#20123;&#28857;&#30340;&#26368;&#20339;&#36335;&#24452;&#12290;&#31639;&#27861;&#32771;&#34385;&#20102;&#21046;&#23450;&#36718;&#26885;&#26080;&#38556;&#30861;&#22369;&#36947;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#21253;&#25324;&#20294;&#19981;&#38480;&#20110;&#39640;&#24230;&#24046;&#12289;&#31354;&#38388;&#32422;&#26463;&#21644;&#22320;&#38754;&#20542;&#26012;&#24230;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Often overlooked as a component of urban development, accessibility infrastructure is undeniably crucial in daily life. Accessibility ramps are one of the most common types of accessibility infrastructure, and serve to benefit not only people with mobile impairments but also able-bodied third parties. While the necessity of accessibility ramps is acknowledged, actual implementation fails in light of the limits of manpower required for the design stage. In response, we present an algorithm capable of the automatic generation of a feasible accessibility ramp based on a 3D model of the relevant environment. Through the manual specification of initial and terminal points within a 3D model, the algorithm uses AI search algorithms to determine the optimal pathway connecting these points. Essential components in devising a wheelchair-accessible ramp are encoded within the process, as evaluated by the algorithm, including but not limited to elevation differentials, spatial constraints, and gra
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;&#27700;&#21360;&#25216;&#26415;&#24212;&#29992;&#20110;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20869;&#23481;&#30340;&#28431;&#27934;&#65292;&#24182;&#35777;&#26126;&#20102;&#29616;&#26377;&#30340;&#27700;&#21360;&#26426;&#21046;&#23481;&#26131;&#34987;&#23545;&#25163;&#30772;&#35299;&#12290;</title><link>http://arxiv.org/abs/2310.07726</link><description>&lt;p&gt;
&#23545;&#27700;&#21360;&#25216;&#26415;&#24212;&#29992;&#20110;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20869;&#23481;&#30340;&#28431;&#27934;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards the Vulnerability of Watermarking Artificial Intelligence Generated Content. (arXiv:2310.07726v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07726
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;&#27700;&#21360;&#25216;&#26415;&#24212;&#29992;&#20110;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20869;&#23481;&#30340;&#28431;&#27934;&#65292;&#24182;&#35777;&#26126;&#20102;&#29616;&#26377;&#30340;&#27700;&#21360;&#26426;&#21046;&#23481;&#26131;&#34987;&#23545;&#25163;&#30772;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20869;&#23481;&#65288;AIGC&#65289;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#35768;&#22810;&#21830;&#19994;&#26381;&#21153;&#24050;&#32463;&#25512;&#20986;&#12290;&#36825;&#20123;&#26381;&#21153;&#21033;&#29992;&#20808;&#36827;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#22914;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20026;&#29992;&#25143;&#29983;&#25104;&#21019;&#24847;&#20869;&#23481;&#65288;&#20363;&#22914;&#36924;&#30495;&#30340;&#22270;&#20687;&#12289;&#27969;&#30021;&#30340;&#21477;&#23376;&#65289;&#12290;&#23545;&#20110;&#27492;&#31867;&#29983;&#25104;&#20869;&#23481;&#30340;&#20351;&#29992;&#38656;&#35201;&#39640;&#24230;&#30417;&#31649;&#65292;&#22240;&#20026;&#26381;&#21153;&#25552;&#20379;&#21830;&#38656;&#35201;&#30830;&#20445;&#29992;&#25143;&#19981;&#36829;&#21453;&#20351;&#29992;&#25919;&#31574;&#65288;&#20363;&#22914;&#28389;&#29992;&#21830;&#19994;&#21270;&#12289;&#29983;&#25104;&#21644;&#20998;&#21457;&#19981;&#23433;&#20840;&#30340;&#20869;&#23481;&#65289;&#12290;&#26368;&#36817;&#25552;&#20986;&#20102;&#35768;&#22810;&#27700;&#21360;&#25216;&#26415;&#65292;&#20294;&#26159;&#26412;&#25991;&#34920;&#26126;&#23545;&#25163;&#21487;&#20197;&#36731;&#26131;&#30772;&#35299;&#36825;&#20123;&#27700;&#21360;&#26426;&#21046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20004;&#31181;&#21487;&#33021;&#30340;&#25915;&#20987;&#26041;&#24335;&#65306;&#65288;1&#65289;&#27700;&#21360;&#21435;&#38500;&#65306;&#23545;&#25163;&#21487;&#20197;&#36731;&#26494;&#22320;&#20174;&#29983;&#25104;&#20869;&#23481;&#20013;&#21024;&#38500;&#23884;&#20837;&#30340;&#27700;&#21360;&#65292;&#28982;&#21518;&#33258;&#30001;&#20351;&#29992;&#32780;&#19981;&#21463;&#26381;&#21153;&#25552;&#20379;&#21830;&#30340;&#38480;&#21046;&#65307;&#65288;2&#65289;&#27700;&#21360;&#20266;&#36896;&#65306;&#23545;&#25163;&#21487;&#20197;&#21019;&#24314;&#38750;&#27861;&#30340;&#27700;&#21360;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence Generated Content (AIGC) is gaining great popularity in social media, with many commercial services available. These services leverage advanced generative models, such as latent diffusion models and large language models, to generate creative content (e.g., realistic images, fluent sentences) for users. The usage of such generated content needs to be highly regulated, as the service providers need to ensure the users do not violate the usage policies (e.g., abuse for commercialization, generating and distributing unsafe content).  Numerous watermarking approaches have been proposed recently. However, in this paper, we show that an adversary can easily break these watermarking mechanisms. Specifically, we consider two possible attacks. (1) Watermark removal: the adversary can easily erase the embedded watermark from the generated content and then use it freely without the regulation of the service provider. (2) Watermark forge: the adversary can create illegal co
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26367;&#20195;&#26041;&#27861;&#8212;&#8212;&#35270;&#35273;&#39044;&#27979;&#65292;&#36890;&#36807;&#24341;&#20837;&#30452;&#35266;&#30340;&#35270;&#35273;&#25552;&#31034;&#65292;&#25237;&#23556;&#21160;&#24577;&#29289;&#20307;&#30340;&#26410;&#26469;&#36712;&#36857;&#65292;&#25552;&#39640;&#26234;&#33021;&#20307;&#30340;&#24863;&#30693;&#33021;&#21147;&#24182;&#23454;&#29616;&#39044;&#26399;&#30340;&#21160;&#20316;&#12290;&#30740;&#31350;&#32467;&#26524;&#39564;&#35777;&#20102;&#35270;&#35273;&#39044;&#27979;&#20316;&#20026;&#23548;&#33322;&#38382;&#39064;&#35299;&#20915;&#26041;&#26696;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.07724</link><description>&lt;p&gt;
&#35270;&#35273;&#39044;&#27979;&#20316;&#20026;&#36991;&#38556;&#30340;&#20013;&#23618;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Visual Forecasting as a Mid-level Representation for Avoidance. (arXiv:2310.07724v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07724
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26367;&#20195;&#26041;&#27861;&#8212;&#8212;&#35270;&#35273;&#39044;&#27979;&#65292;&#36890;&#36807;&#24341;&#20837;&#30452;&#35266;&#30340;&#35270;&#35273;&#25552;&#31034;&#65292;&#25237;&#23556;&#21160;&#24577;&#29289;&#20307;&#30340;&#26410;&#26469;&#36712;&#36857;&#65292;&#25552;&#39640;&#26234;&#33021;&#20307;&#30340;&#24863;&#30693;&#33021;&#21147;&#24182;&#23454;&#29616;&#39044;&#26399;&#30340;&#21160;&#20316;&#12290;&#30740;&#31350;&#32467;&#26524;&#39564;&#35777;&#20102;&#35270;&#35273;&#39044;&#27979;&#20316;&#20026;&#23548;&#33322;&#38382;&#39064;&#35299;&#20915;&#26041;&#26696;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#20027;&#26234;&#33021;&#20307;&#30740;&#31350;&#20013;&#65292;&#22312;&#20855;&#26377;&#21160;&#24577;&#29289;&#20307;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#23548;&#33322;&#30340;&#25361;&#25112;&#20173;&#28982;&#26159;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#12290;&#34429;&#28982;&#39044;&#27979;&#26041;&#27861;&#24456;&#26377;&#21069;&#26223;&#65292;&#20294;&#23427;&#20204;&#23545;&#31934;&#30830;&#29366;&#24577;&#20449;&#24687;&#30340;&#20381;&#36182;&#20351;&#20854;&#22312;&#23454;&#38469;&#19990;&#30028;&#20013;&#30340;&#24212;&#29992;&#19981;&#22826;&#23454;&#29992;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#35270;&#35273;&#39044;&#27979;&#20316;&#20026;&#19968;&#31181;&#21019;&#26032;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#36890;&#36807;&#24341;&#20837;&#30452;&#35266;&#30340;&#35270;&#35273;&#25552;&#31034;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25237;&#23556;&#21160;&#24577;&#29289;&#20307;&#30340;&#26410;&#26469;&#36712;&#36857;&#65292;&#20197;&#25552;&#39640;&#26234;&#33021;&#20307;&#30340;&#24863;&#30693;&#33021;&#21147;&#24182;&#23454;&#29616;&#39044;&#26399;&#30340;&#21160;&#20316;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#31574;&#30053;&#26469;&#36890;&#36807;&#35270;&#35273;&#39044;&#27979;&#20256;&#36798;&#39044;&#27979;&#20449;&#24687;&#65306;&#65288;1&#65289;&#36793;&#30028;&#26694;&#24207;&#21015;&#21644;&#65288;2&#65289;&#22686;&#24378;&#36335;&#24452;&#12290;&#20026;&#20102;&#39564;&#35777;&#25152;&#25552;&#20986;&#30340;&#35270;&#35273;&#39044;&#27979;&#31574;&#30053;&#65292;&#25105;&#20204;&#22312;Unity&#24341;&#25806;&#30340;&#27169;&#25311;&#29615;&#22659;&#20013;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#23558;&#36825;&#20123;&#35780;&#20272;&#25193;&#23637;&#21040;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#20197;&#35780;&#20272;&#20854;&#23454;&#29992;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;&#32467;&#26524;&#35777;&#23454;&#20102;&#35270;&#35273;&#39044;&#27979;&#20316;&#20026;&#23548;&#33322;&#38382;&#39064;&#30340;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The challenge of navigation in environments with dynamic objects continues to be a central issue in the study of autonomous agents. While predictive methods hold promise, their reliance on precise state information makes them less practical for real-world implementation. This study presents visual forecasting as an innovative alternative. By introducing intuitive visual cues, this approach projects the future trajectories of dynamic objects to improve agent perception and enable anticipatory actions. Our research explores two distinct strategies for conveying predictive information through visual forecasting: (1) sequences of bounding boxes, and (2) augmented paths. To validate the proposed visual forecasting strategies, we initiate evaluations in simulated environments using the Unity engine and then extend these evaluations to real-world scenarios to assess both practicality and effectiveness. The results confirm the viability of visual forecasting as a promising solution for navigat
&lt;/p&gt;</description></item><item><title>&#37325;&#26032;&#32771;&#34385;&#20102;&#22522;&#20110;DNA&#24207;&#21015;&#30340;BERT-like&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;K-mer&#37325;&#21472;&#26631;&#35760;&#21270;&#65292;&#22312;&#19979;&#28216;&#20219;&#21153;&#30340;&#24494;&#35843;&#38454;&#27573;&#21644;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#37117;&#21462;&#24471;&#20102;&#19968;&#33268;&#30340;&#24615;&#33021;&#25913;&#21892;&#12290;</title><link>http://arxiv.org/abs/2310.07644</link><description>&lt;p&gt;
&#37325;&#26032;&#32771;&#34385;&#22522;&#20110;DNA&#24207;&#21015;&#30340;BERT-like&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Rethinking the BERT-like Pretraining for DNA Sequences. (arXiv:2310.07644v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07644
&lt;/p&gt;
&lt;p&gt;
&#37325;&#26032;&#32771;&#34385;&#20102;&#22522;&#20110;DNA&#24207;&#21015;&#30340;BERT-like&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;K-mer&#37325;&#21472;&#26631;&#35760;&#21270;&#65292;&#22312;&#19979;&#28216;&#20219;&#21153;&#30340;&#24494;&#35843;&#38454;&#27573;&#21644;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#37117;&#21462;&#24471;&#20102;&#19968;&#33268;&#30340;&#24615;&#33021;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#25104;&#21151;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#29983;&#21629;&#31185;&#23398;&#39046;&#22495;&#30340;&#36235;&#21183;&#26085;&#30410;&#22686;&#38271;&#12290;&#29305;&#21035;&#26159;&#22522;&#20110;DNA&#24207;&#21015;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#22240;&#20854;&#25429;&#25417;&#22522;&#22240;&#30340;&#36890;&#29992;&#20449;&#24687;&#30340;&#28508;&#21147;&#32780;&#21463;&#21040;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;DNA&#24207;&#21015;&#39044;&#35757;&#32451;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#20174;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30452;&#25509;&#24341;&#20837;&#30340;BERT&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#32570;&#20047;&#20840;&#38754;&#30340;&#29702;&#35299;&#21644;&#19987;&#38376;&#23450;&#21046;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#65292;&#25105;&#20204;&#39318;&#20808;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#30340;&#25506;&#32034;&#24615;&#23454;&#39564;&#65292;&#24182;&#33719;&#24471;&#20102;&#20960;&#20010;&#26377;&#21551;&#21457;&#24615;&#30340;&#35266;&#23519;&#32467;&#26524;&#65306;1&#65289;&#22312;&#19979;&#28216;&#20219;&#21153;&#30340;&#24494;&#35843;&#38454;&#27573;&#65292;&#20351;&#29992;K-mer&#37325;&#21472;&#26631;&#35760;&#21270;&#32780;&#19981;&#26159;K-mer&#38750;&#37325;&#21472;&#26631;&#35760;&#21270;&#26102;&#65292;&#37325;&#21472;&#21644;&#38750;&#37325;&#21472;&#30340;&#39044;&#35757;&#32451;&#26435;&#37325;&#22343;&#34920;&#29616;&#20986;&#19968;&#33268;&#30340;&#24615;&#33021;&#25913;&#21892;&#12290;2&#65289;&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#20351;&#29992;K-mer&#37325;&#21472;&#26631;&#35760;&#21270;&#20250;&#36805;&#36895;&#20135;&#29983;&#28165;&#26224;&#30340;K-mer&#23884;&#20837;&#65292;&#24182;&#23558;&#25439;&#22833;&#38477;&#20302;&#21040;&#38750;&#24120;&#20302;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the success of large-scale pretraining in NLP, there is an increasing trend of applying it to the domain of life sciences. In particular, pretraining methods based on DNA sequences have garnered growing attention due to their potential to capture generic information about genes. However, existing pretraining methods for DNA sequences largely rely on direct adoptions of BERT pretraining from NLP, lacking a comprehensive understanding and a specifically tailored approach. To address this research gap, we first conducted a series of exploratory experiments and gained several insightful observations: 1) In the fine-tuning phase of downstream tasks, when using K-mer overlapping tokenization instead of K-mer non-overlapping tokenization, both overlapping and non-overlapping pretraining weights show consistent performance improvement.2) During the pre-training process, using K-mer overlapping tokenization quickly produces clear K-mer embeddings and reduces the loss to a very low level, w
&lt;/p&gt;</description></item><item><title>OpsEval&#26159;&#19968;&#20010;&#20840;&#38754;&#20219;&#21153;&#23548;&#21521;&#30340;AIOps&#22522;&#20934;&#27979;&#35797;&#65292;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26377;&#32447;&#32593;&#32476;&#25805;&#20316;&#12289;5G&#36890;&#20449;&#25805;&#20316;&#21644;&#25968;&#25454;&#24211;&#25805;&#20316;&#31561;&#20851;&#38190;&#22330;&#26223;&#19979;&#30340;&#33021;&#21147;&#27700;&#24179;&#65292;&#20026;&#25552;&#20379;&#38024;&#23545;AIOps&#23450;&#21046;&#30340;LLMs&#30340;&#20248;&#21270;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2310.07637</link><description>&lt;p&gt;
OpsEval: &#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#38754;&#20219;&#21153;&#23548;&#21521;&#30340;AIOps&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
OpsEval: A Comprehensive Task-Oriented AIOps Benchmark for Large Language Models. (arXiv:2310.07637v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07637
&lt;/p&gt;
&lt;p&gt;
OpsEval&#26159;&#19968;&#20010;&#20840;&#38754;&#20219;&#21153;&#23548;&#21521;&#30340;AIOps&#22522;&#20934;&#27979;&#35797;&#65292;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26377;&#32447;&#32593;&#32476;&#25805;&#20316;&#12289;5G&#36890;&#20449;&#25805;&#20316;&#21644;&#25968;&#25454;&#24211;&#25805;&#20316;&#31561;&#20851;&#38190;&#22330;&#26223;&#19979;&#30340;&#33021;&#21147;&#27700;&#24179;&#65292;&#20026;&#25552;&#20379;&#38024;&#23545;AIOps&#23450;&#21046;&#30340;LLMs&#30340;&#20248;&#21270;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(Large Language Models, LLMs)&#22312;&#32763;&#35793;&#12289;&#24635;&#32467;&#21644;&#29983;&#25104;&#31561;NLP&#30456;&#20851;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#33021;&#21147;&#12290;LLMs&#22312;&#29305;&#23450;&#39046;&#22495;&#20013;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;AIOps&#65288;&#38754;&#21521;IT&#36816;&#32500;&#30340;&#20154;&#24037;&#26234;&#33021;&#65289;&#20013;&#65292;&#30001;&#20110;&#20854;&#20808;&#36827;&#30340;&#20449;&#24687;&#27719;&#24635;&#12289;&#25253;&#21578;&#20998;&#26512;&#21644;API&#35843;&#29992;&#33021;&#21147;&#32780;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;LLMs&#22312;AIOps&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#23578;&#26410;&#30830;&#23450;&#12290;&#27492;&#22806;&#65292;&#38656;&#35201;&#19968;&#20010;&#20840;&#38754;&#30340;&#22522;&#20934;&#27979;&#35797;&#26469;&#24341;&#23548;&#38024;&#23545;AIOps&#23450;&#21046;&#30340;LLMs&#30340;&#20248;&#21270;&#12290;&#19982;&#29616;&#26377;&#30340;&#19987;&#27880;&#20110;&#35780;&#20272;&#32593;&#32476;&#37197;&#32622;&#31561;&#29305;&#23450;&#39046;&#22495;&#30340;&#22522;&#20934;&#27979;&#35797;&#19981;&#21516;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;OpsEval&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#20026;LLMs&#35774;&#35745;&#30340;&#20840;&#38754;&#20219;&#21153;&#23548;&#21521;&#30340;AIOps&#22522;&#20934;&#27979;&#35797;&#12290;OpsEval&#39318;&#27425;&#23545;LLMs&#22312;&#19977;&#20010;&#20851;&#38190;&#22330;&#26223;&#65288;&#26377;&#32447;&#32593;&#32476;&#25805;&#20316;&#12289;5G&#36890;&#20449;&#25805;&#20316;&#21644;&#25968;&#25454;&#24211;&#25805;&#20316;&#65289;&#20197;&#21450;&#19981;&#21516;&#30340;&#33021;&#21147;&#27700;&#24179;&#65288;&#30693;&#35782;&#22238;&#24518;&#12289;&#20998;&#26512;&#24605;&#32771;&#65289;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have exhibited remarkable capabilities in NLP-related tasks such as translation, summarizing, and generation. The application of LLMs in specific areas, notably AIOps (Artificial Intelligence for IT Operations), holds great potential due to their advanced abilities in information summarizing, report analyzing, and ability of API calling. Nevertheless, the performance of current LLMs in AIOps tasks is yet to be determined. Furthermore, a comprehensive benchmark is required to steer the optimization of LLMs tailored for AIOps. Compared with existing benchmarks that focus on evaluating specific fields like network configuration, in this paper, we present \textbf{OpsEval}, a comprehensive task-oriented AIOps benchmark designed for LLMs. For the first time, OpsEval assesses LLMs' proficiency in three crucial scenarios (Wired Network Operation, 5G Communication Operation, and Database Operation) at various ability levels (knowledge recall, analytical thinking, an
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;LLM&#28040;&#38500;&#26041;&#27861;&#65292;&#31216;&#20026;&#8220;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#28040;&#38500;&#8221;&#65292;&#23427;&#25552;&#20379;&#20102;&#19978;&#19979;&#25991;&#30340;&#36755;&#20837;&#19988;&#26080;&#38656;&#26356;&#26032;&#27169;&#22411;&#21442;&#25968;&#12290;&#36825;&#31181;&#26041;&#27861;&#35299;&#20915;&#20102;&#28040;&#38500;&#23545;&#20110;&#24456;&#22823;&#27169;&#22411;&#26469;&#35828;&#22312;&#35745;&#31639;&#19978;&#30340;&#22256;&#38590;&#65292;&#24182;&#22312;&#23454;&#36341;&#20013;&#20855;&#26377;&#26356;&#39640;&#30340;&#21487;&#34892;&#24615;&#21644;&#20415;&#25463;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.07579</link><description>&lt;p&gt;
In-Context Unlearning: &#22522;&#20110;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#28040;&#38500;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
In-Context Unlearning: Language Models as Few Shot Unlearners. (arXiv:2310.07579v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07579
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;LLM&#28040;&#38500;&#26041;&#27861;&#65292;&#31216;&#20026;&#8220;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#28040;&#38500;&#8221;&#65292;&#23427;&#25552;&#20379;&#20102;&#19978;&#19979;&#25991;&#30340;&#36755;&#20837;&#19988;&#26080;&#38656;&#26356;&#26032;&#27169;&#22411;&#21442;&#25968;&#12290;&#36825;&#31181;&#26041;&#27861;&#35299;&#20915;&#20102;&#28040;&#38500;&#23545;&#20110;&#24456;&#22823;&#27169;&#22411;&#26469;&#35828;&#22312;&#35745;&#31639;&#19978;&#30340;&#22256;&#38590;&#65292;&#24182;&#22312;&#23454;&#36341;&#20013;&#20855;&#26377;&#26356;&#39640;&#30340;&#21487;&#34892;&#24615;&#21644;&#20415;&#25463;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#28040;&#38500;&#23398;&#20064;&#26159;&#30740;&#31350;&#22914;&#20309;&#39640;&#25928;&#22320;&#21435;&#38500;&#29305;&#23450;&#35757;&#32451;&#25968;&#25454;&#23545;&#35757;&#32451;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#36817;&#26469;&#24341;&#36215;&#20102;&#26356;&#22810;&#30340;&#20851;&#27880;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#38656;&#35201;&#36981;&#23432;&#35832;&#22914;&#34987;&#36951;&#24536;&#26435;&#31561;&#38544;&#31169;&#27861;&#35268;&#30340;&#38656;&#27714;&#12290;&#23613;&#31649;&#22312;&#29256;&#26435;&#38382;&#39064;&#19978;LLM&#65288;&#35821;&#35328;&#27169;&#22411;&#65289;&#23588;&#20854;&#30456;&#20851;&#65292;&#20294;&#22312;&#38750;&#24120;&#22823;&#30340;&#27169;&#22411;&#19978;&#23454;&#29616;&#31934;&#30830;&#28040;&#38500;&#26159;&#35745;&#31639;&#19978;&#19981;&#21487;&#34892;&#30340;&#12290;&#20026;&#27492;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#20960;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#36817;&#20284;&#28040;&#38500;&#35757;&#32451;&#25968;&#25454;&#12290;&#36825;&#20123;&#31639;&#27861;&#20851;&#38190;&#20381;&#36182;&#20110;&#23545;&#27169;&#22411;&#21442;&#25968;&#30340;&#35775;&#38382;&#26469;&#26356;&#26032;&#23427;&#20204;&#65292;&#20294;&#22312;&#23454;&#36341;&#20013;&#21487;&#33021;&#30001;&#20110;&#35745;&#31639;&#32422;&#26463;&#25110;&#36890;&#36807;API&#35775;&#38382;LLM&#32780;&#26080;&#27861;&#28385;&#36275;&#36825;&#31181;&#20551;&#35774;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;LLM&#28040;&#38500;&#26041;&#27861;&#65292;&#31216;&#20026;&#8220;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#28040;&#38500;&#8221;&#65292;&#23427;&#25552;&#20379;&#20102;&#19978;&#19979;&#25991;&#30340;&#36755;&#20837;&#19988;&#26080;&#38656;&#26356;&#26032;&#27169;&#22411;&#21442;&#25968;&#12290;&#20026;&#20102;&#28040;&#38500;&#29305;&#23450;&#30340;&#35757;&#32451;&#23454;&#20363;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;i
&lt;/p&gt;
&lt;p&gt;
Machine unlearning, the study of efficiently removing the impact of specific training points on the trained model, has garnered increased attention of late, driven by the need to comply with privacy regulations like the Right to be Forgotten. Although unlearning is particularly relevant for LLMs in light of the copyright issues they raise, achieving precise unlearning is computationally infeasible for very large models. To this end, recent work has proposed several algorithms which approximate the removal of training data without retraining the model. These algorithms crucially rely on access to the model parameters in order to update them, an assumption that may not hold in practice due to computational constraints or when the LLM is accessed via API. In this work, we propose a new class of unlearning methods for LLMs we call ''In-Context Unlearning'', providing inputs in context and without having to update model parameters. To unlearn a particular training instance, we provide the i
&lt;/p&gt;</description></item><item><title>&#22810;&#27169;&#24577;&#22270;&#23398;&#20064;&#26159;&#19968;&#31181;&#36890;&#29992;&#19988;&#31995;&#32479;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#25429;&#25417;&#22810;&#27169;&#24577;&#25968;&#25454;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#65292;&#24182;&#22312;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.07478</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#22270;&#23398;&#20064;&#29992;&#20110;&#29983;&#25104;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Multimodal Graph Learning for Generative Tasks. (arXiv:2310.07478v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07478
&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22270;&#23398;&#20064;&#26159;&#19968;&#31181;&#36890;&#29992;&#19988;&#31995;&#32479;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#25429;&#25417;&#22810;&#27169;&#24577;&#25968;&#25454;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#65292;&#24182;&#22312;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#23398;&#20064;&#32467;&#21512;&#22810;&#31181;&#25968;&#25454;&#27169;&#24577;&#65292;&#25193;&#22823;&#20102;&#27169;&#22411;&#21487;&#20197;&#21033;&#29992;&#30340;&#25968;&#25454;&#31867;&#22411;&#21644;&#22797;&#26434;&#24230;&#65292;&#20363;&#22914;&#20174;&#32431;&#25991;&#26412;&#21040;&#22270;&#20687;-&#23383;&#24149;&#23545;&#12290;&#22823;&#22810;&#25968;&#22810;&#27169;&#24577;&#23398;&#20064;&#31639;&#27861;&#19987;&#27880;&#20110;&#23545;&#20004;&#31181;&#27169;&#24577;&#30340;&#31616;&#21333;&#19968;&#23545;&#19968;&#25968;&#25454;&#36827;&#34892;&#24314;&#27169;&#65292;&#22914;&#22270;&#20687;-&#23383;&#24149;&#23545;&#25110;&#38899;&#39057;-&#25991;&#26412;&#23545;&#12290;&#28982;&#32780;&#65292;&#22312;&#22823;&#22810;&#25968;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#19981;&#21516;&#27169;&#24577;&#30340;&#23454;&#20307;&#20197;&#26356;&#22797;&#26434;&#21644;&#22810;&#26679;&#21270;&#30340;&#26041;&#24335;&#30456;&#20114;&#20316;&#29992;&#65292;&#36229;&#36234;&#20102;&#19968;&#23545;&#19968;&#26144;&#23556;&#12290;&#25105;&#20204;&#25552;&#20986;&#23558;&#36825;&#20123;&#22797;&#26434;&#20851;&#31995;&#34920;&#31034;&#20026;&#22270;&#24418;&#65292;&#20801;&#35768;&#25105;&#20204;&#25429;&#25417;&#20219;&#24847;&#25968;&#37327;&#30340;&#27169;&#24577;&#25968;&#25454;&#65292;&#24182;&#25429;&#25417;&#27169;&#24577;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#65292;&#36825;&#20123;&#20851;&#31995;&#21487;&#20197;&#20174;&#19968;&#20010;&#26679;&#26412;&#21040;&#21478;&#19968;&#20010;&#26679;&#26412;&#28789;&#27963;&#21464;&#21270;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#27169;&#24577;&#22270;&#23398;&#20064;&#65288;MMGL&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#29992;&#19988;&#31995;&#32479;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#25429;&#33719;&#20855;&#26377;&#20851;&#31995;&#32467;&#26500;&#30340;&#22810;&#20010;&#22810;&#27169;&#24577;&#37051;&#23621;&#30340;&#20449;&#24687;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#20851;&#27880;&#22522;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;MMGL&#29992;&#20110;&#29983;&#25104;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal learning combines multiple data modalities, broadening the types and complexity of data our models can utilize: for example, from plain text to image-caption pairs. Most multimodal learning algorithms focus on modeling simple one-to-one pairs of data from two modalities, such as image-caption pairs, or audio-text pairs. However, in most real-world settings, entities of different modalities interact with each other in more complex and multifaceted ways, going beyond one-to-one mappings. We propose to represent these complex relationships as graphs, allowing us to capture data with any number of modalities, and with complex relationships between modalities that can flexibly vary from one sample to another. Toward this goal, we propose Multimodal Graph Learning (MMGL), a general and systematic framework for capturing information from multiple multimodal neighbors with relational structures among them. In particular, we focus on MMGL for generative tasks, building upon pretraine
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35266;&#23519;&#23398;&#20064;&#27169;&#20223;(ILfO)&#26694;&#26550;&#65292;&#33021;&#22815;&#35299;&#20915;&#30001;&#20110;&#22870;&#21169;&#20449;&#21495;&#20998;&#37197;&#38169;&#35823;&#23548;&#33268;&#20195;&#29702;&#26080;&#27861;&#23398;&#20064;&#21021;&#22987;&#34892;&#20026;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.07433</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#21160;&#25240;&#25187;&#35843;&#24230;&#20174;&#35266;&#23519;&#20013;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Imitation Learning from Observation with Automatic Discount Scheduling. (arXiv:2310.07433v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07433
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35266;&#23519;&#23398;&#20064;&#27169;&#20223;(ILfO)&#26694;&#26550;&#65292;&#33021;&#22815;&#35299;&#20915;&#30001;&#20110;&#22870;&#21169;&#20449;&#21495;&#20998;&#37197;&#38169;&#35823;&#23548;&#33268;&#20195;&#29702;&#26080;&#27861;&#23398;&#20064;&#21021;&#22987;&#34892;&#20026;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#36890;&#24120;&#36890;&#36807;&#35266;&#23519;&#21644;&#27169;&#20223;&#26469;&#33719;&#24471;&#26032;&#30340;&#25216;&#33021;&#12290;&#23545;&#20110;&#26426;&#22120;&#20154;&#20195;&#29702;&#65292;&#20174;&#20114;&#32852;&#32593;&#19978;&#21487;&#29992;&#30340;&#22823;&#37327;&#26080;&#26631;&#31614;&#35270;&#39057;&#28436;&#31034;&#25968;&#25454;&#20013;&#36827;&#34892;&#23398;&#20064;&#65292;&#38656;&#35201;&#22312;&#27809;&#26377;&#35775;&#38382;&#20854;&#21160;&#20316;&#30340;&#24773;&#20917;&#19979;&#27169;&#20223;&#19987;&#23478;&#65292;&#36825;&#26159;&#19968;&#31181;&#31216;&#20026;&#35266;&#23519;&#23398;&#20064;&#27169;&#20223;&#65288;ILfO&#65289;&#30340;&#25361;&#25112;&#12290;&#35299;&#20915;ILfO&#38382;&#39064;&#30340;&#24120;&#35265;&#26041;&#27861;&#26159;&#23558;&#20854;&#36716;&#21270;&#20026;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#21033;&#29992;&#20174;&#20195;&#29702;&#21644;&#19987;&#23478;&#35266;&#23519;&#20013;&#35745;&#31639;&#20986;&#30340;&#20195;&#29702;&#22870;&#21169;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#20855;&#26377;&#36827;&#23637;&#20381;&#36182;&#24615;&#23646;&#24615;&#30340;&#20219;&#21153;&#20013;&#65292;&#36825;&#26679;&#30340;&#26041;&#27861;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#65307;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#65292;&#20195;&#29702;&#38656;&#35201;&#22312;&#25484;&#25569;&#21518;&#32493;&#34892;&#20026;&#20043;&#21069;&#20808;&#23398;&#20064;&#19987;&#23478;&#30340;&#21069;&#24207;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20027;&#35201;&#21407;&#22240;&#26159;&#20998;&#37197;&#32473;&#21518;&#32493;&#27493;&#39588;&#30340;&#22870;&#21169;&#20449;&#21495;&#22952;&#30861;&#20102;&#23545;&#21021;&#22987;&#34892;&#20026;&#30340;&#23398;&#20064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;ILfO&#26694;&#26550;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#25484;&#25569;&#26089;&#26399;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans often acquire new skills through observation and imitation. For robotic agents, learning from the plethora of unlabeled video demonstration data available on the Internet necessitates imitating the expert without access to its action, presenting a challenge known as Imitation Learning from Observations (ILfO). A common approach to tackle ILfO problems is to convert them into inverse reinforcement learning problems, utilizing a proxy reward computed from the agent's and the expert's observations. Nonetheless, we identify that tasks characterized by a progress dependency property pose significant challenges for such approaches; in these tasks, the agent needs to initially learn the expert's preceding behaviors before mastering the subsequent ones. Our investigation reveals that the main cause is that the reward signals assigned to later steps hinder the learning of initial behaviors. To address this challenge, we present a novel ILfO framework that enables the agent to master earl
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#37319;&#29992;Transformer&#26550;&#26500;&#21644;&#25968;&#20540;&#22810;&#23610;&#24230;&#23884;&#20837;&#27169;&#22359;&#65292;&#20351;&#26102;&#38388;&#24207;&#21015;&#33258;&#30417;&#30563;&#27169;&#22411;&#33021;&#22815;&#25193;&#23637;&#21040;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2310.07402</link><description>&lt;p&gt;
NuTime: &#22823;&#35268;&#27169;&#26102;&#38388;&#24207;&#21015;&#39044;&#35757;&#32451;&#30340;&#25968;&#20540;&#22810;&#23610;&#24230;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
NuTime: Numerically Multi-Scaled Embedding for Large-Scale Time Series Pretraining. (arXiv:2310.07402v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07402
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#37319;&#29992;Transformer&#26550;&#26500;&#21644;&#25968;&#20540;&#22810;&#23610;&#24230;&#23884;&#20837;&#27169;&#22359;&#65292;&#20351;&#26102;&#38388;&#24207;&#21015;&#33258;&#30417;&#30563;&#27169;&#22411;&#33021;&#22815;&#25193;&#23637;&#21040;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20851;&#20110;&#26102;&#38388;&#24207;&#21015;&#33258;&#30417;&#30563;&#27169;&#22411;&#30340;&#30740;&#31350;&#26174;&#31034;&#20986;&#23398;&#20064;&#35821;&#20041;&#34920;&#31034;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#20165;&#38480;&#20110;&#23567;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#20363;&#22914;&#25968;&#21315;&#20010;&#26102;&#38388;&#24207;&#21015;&#12290;&#26412;&#25991;&#30340;&#20851;&#38190;&#25216;&#26415;&#36129;&#29486;&#38024;&#23545;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#25968;&#20540;&#29305;&#24615;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#25193;&#23637;&#21040;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#20363;&#22914;&#30334;&#19975;&#20010;&#26102;&#38388;&#24207;&#21015;&#12290;&#25105;&#20204;&#37319;&#29992;Transformer&#26550;&#26500;&#65292;&#39318;&#20808;&#23558;&#36755;&#20837;&#21010;&#20998;&#20026;&#38750;&#37325;&#21472;&#31383;&#21475;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#31383;&#21475;&#30340;&#26631;&#20934;&#21270;&#24418;&#29366;&#21644;&#20004;&#20010;&#26631;&#37327;&#20540;&#34920;&#31034;&#27599;&#20010;&#31383;&#21475;&#20869;&#30340;&#22343;&#20540;&#21644;&#26631;&#20934;&#24046;&#12290;&#20026;&#20102;&#23558;&#21487;&#33021;&#20855;&#26377;&#20219;&#24847;&#25968;&#20540;&#23610;&#24230;&#30340;&#26631;&#37327;&#20540;&#23884;&#20837;&#21040;&#39640;&#32500;&#21521;&#37327;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#20540;&#22810;&#23610;&#24230;&#23884;&#20837;&#27169;&#22359;&#65292;&#26522;&#20030;&#25152;&#26377;&#21487;&#33021;&#30340;&#26631;&#37327;&#20540;&#23610;&#24230;&#12290;&#35813;&#27169;&#22411;&#20351;&#29992;&#25552;&#20986;&#30340;&#25968;&#20540;&#22810;&#23610;&#24230;&#23884;&#20837;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#37319;&#29992;&#31616;&#21333;&#30340;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research on time-series self-supervised models shows great promise in learning semantic representations. However, it has been limited to small-scale datasets, e.g., thousands of temporal sequences. In this work, we make key technical contributions that are tailored to the numerical properties of time-series data and allow the model to scale to large datasets, e.g., millions of temporal sequences. We adopt the Transformer architecture by first partitioning the input into non-overlapping windows. Each window is then characterized by its normalized shape and two scalar values denoting the mean and standard deviation within each window. To embed scalar values that may possess arbitrary numerical scales to high-dimensional vectors, we propose a numerically multi-scaled embedding module enumerating all possible scales for the scalar values. The model undergoes pretraining using the proposed numerically multi-scaled embedding with a simple contrastive objective on a large-scale dataset
&lt;/p&gt;</description></item><item><title>WiGenAI&#36890;&#36807;&#24341;&#20837;&#25193;&#25955;&#27169;&#22411;&#65292;&#23558;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20110;&#26080;&#32447;&#36890;&#20449;&#31995;&#32479;&#20013;&#65292;&#20026;&#30740;&#31350;&#22880;&#23450;&#22522;&#30784;&#12290;&#36825;&#31687;&#25991;&#31456;&#20171;&#32461;&#20102;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#29983;&#25104;&#27169;&#22411;&#30340;&#26368;&#26032;&#33539;&#24335;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#22312;&#26080;&#32447;&#36890;&#20449;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#20004;&#20010;&#26696;&#20363;&#30740;&#31350;&#23637;&#31034;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#24320;&#21457;&#38887;&#24615;&#30340;AI&#26412;&#22320;&#36890;&#20449;&#31995;&#32479;&#20013;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.07312</link><description>&lt;p&gt;
WiGenAI: &#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#26080;&#32447;&#21644;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#20132;&#32455;
&lt;/p&gt;
&lt;p&gt;
WiGenAI: The Symphony of Wireless and Generative AI via Diffusion Models. (arXiv:2310.07312v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07312
&lt;/p&gt;
&lt;p&gt;
WiGenAI&#36890;&#36807;&#24341;&#20837;&#25193;&#25955;&#27169;&#22411;&#65292;&#23558;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20110;&#26080;&#32447;&#36890;&#20449;&#31995;&#32479;&#20013;&#65292;&#20026;&#30740;&#31350;&#22880;&#23450;&#22522;&#30784;&#12290;&#36825;&#31687;&#25991;&#31456;&#20171;&#32461;&#20102;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#29983;&#25104;&#27169;&#22411;&#30340;&#26368;&#26032;&#33539;&#24335;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#22312;&#26080;&#32447;&#36890;&#20449;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#20004;&#20010;&#26696;&#20363;&#30740;&#31350;&#23637;&#31034;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#24320;&#21457;&#38887;&#24615;&#30340;AI&#26412;&#22320;&#36890;&#20449;&#31995;&#32479;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21019;&#26032;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#22914;GPT-3&#21644;&#31283;&#23450;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#24050;&#32463;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#23454;&#29616;&#20102;&#33539;&#24335;&#36716;&#21464;&#65292;&#21521;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#21457;&#23637;&#12290;&#20174;&#25968;&#25454;&#36890;&#20449;&#21644;&#32593;&#32476;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#39044;&#35745;&#23558;&#24191;&#27867;&#24212;&#29992;&#20110;&#26410;&#26469;&#26080;&#32447;&#36890;&#20449;&#31995;&#32479;&#30340;&#26032;&#19968;&#20195;&#20013;&#65292;&#24378;&#35843;&#20102;&#22312;&#26032;&#20852;&#36890;&#20449;&#22330;&#26223;&#20013;&#38656;&#35201;&#26032;&#39062;&#30340;AI&#26412;&#22320;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#20171;&#32461;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#26080;&#32447;&#36890;&#20449;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#65292;&#20026;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#22880;&#23450;&#22522;&#30784;&#12290;&#20171;&#32461;&#20102;&#25193;&#25955;&#22411;&#29983;&#25104;&#27169;&#22411;&#20316;&#20026;&#29983;&#25104;&#27169;&#22411;&#30340;&#26368;&#26032;&#33539;&#24335;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#22312;&#26080;&#32447;&#36890;&#20449;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#12290;&#36824;&#25552;&#20379;&#20102;&#20004;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#24320;&#21457;&#20855;&#26377;&#38887;&#24615;&#30340;AI&#26412;&#22320;&#36890;&#20449;&#31995;&#32479;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#23637;&#31034;&#20854;&#22312;&#29983;&#25104;&#27169;&#22411;&#30340;&#24212;&#29992;&#20013;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Innovative foundation models, such as GPT-3 and stable diffusion models, have made a paradigm shift in the realm of artificial intelligence (AI) towards generative AI-based systems. In unison, from data communication and networking perspective, AI and machine learning (AI/ML) algorithms are envisioned to be pervasively incorporated into the future generations of wireless communications systems, highlighting the need for novel AI-native solutions for the emergent communication scenarios. In this article, we outline the applications of generative AI in wireless communication systems to lay the foundations for research in this field. Diffusion-based generative models, as the new state-of-the-art paradigm of generative models, are introduced, and their applications in wireless communication systems are discussed. Two case studies are also presented to showcase how diffusion models can be exploited for the development of resilient AI-native communication systems. Specifically, we propose de
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#23588;&#20854;&#26159;BioBERT&#65289;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#38024;&#23545;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#24494;&#35843;&#26041;&#27861;&#12290;&#30740;&#31350;&#31361;&#20986;&#20102;BioBERT&#23545;&#20110;&#35299;&#20915;&#19982;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#25366;&#25496;&#30456;&#20851;&#20219;&#21153;&#30340;&#29305;&#23450;&#35201;&#27714;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.07282</link><description>&lt;p&gt;
&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20013;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20998;&#26512;&#65306;BioBERT &#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Analysis on Large Language Models in Healthcare: A Case Study of BioBERT. (arXiv:2310.07282v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07282
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#23588;&#20854;&#26159;BioBERT&#65289;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#38024;&#23545;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#24494;&#35843;&#26041;&#27861;&#12290;&#30740;&#31350;&#31361;&#20986;&#20102;BioBERT&#23545;&#20110;&#35299;&#20915;&#19982;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#25366;&#25496;&#30456;&#20851;&#20219;&#21153;&#30340;&#29305;&#23450;&#35201;&#27714;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#23588;&#20854;&#26159;BioBERT&#65289;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#30740;&#31350;&#12290;&#23427;&#39318;&#20808;&#24443;&#24213;&#26816;&#26597;&#20102;&#20808;&#21069;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20013;&#24212;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#26041;&#27861;&#30340;&#24773;&#20917;&#65292;&#25581;&#31034;&#20102;&#36825;&#20123;&#26041;&#27861;&#38754;&#20020;&#30340;&#23616;&#38480;&#21644;&#25361;&#25112;&#12290;&#38543;&#21518;&#65292;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;BioBERT&#25972;&#21512;&#21040;&#21307;&#30103;&#20445;&#20581;&#24212;&#29992;&#20013;&#30340;&#36335;&#24452;&#65292;&#31361;&#20986;&#20854;&#36866;&#29992;&#20110;&#35299;&#20915;&#19982;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#25366;&#25496;&#30456;&#20851;&#20219;&#21153;&#30340;&#29305;&#23450;&#35201;&#27714;&#12290;&#35813;&#20998;&#26512;&#27010;&#36848;&#20102;&#29992;&#20110;&#38024;&#23545;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#29420;&#29305;&#38656;&#27714;&#24494;&#35843;BioBERT&#30340;&#31995;&#32479;&#26041;&#27861;&#35770;&#12290;&#36825;&#20010;&#26041;&#27861;&#21253;&#25324;&#20174;&#21508;&#31181;&#21307;&#30103;&#20445;&#20581;&#26469;&#28304;&#25910;&#38598;&#25968;&#25454;&#65292;&#20026;&#35782;&#21035;&#21307;&#30103;&#23454;&#20307;&#21644;&#23545;&#20854;&#36827;&#34892;&#20998;&#31867;&#31561;&#20219;&#21153;&#36827;&#34892;&#25968;&#25454;&#27880;&#37322;&#65292;&#24182;&#24212;&#29992;&#19987;&#38376;&#38024;&#23545;&#22788;&#29702;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#20013;&#22797;&#26434;&#24615;&#30340;&#39044;&#22788;&#29702;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper conducts a comprehensive investigation into applying large language models, particularly on BioBERT, in healthcare. It begins with thoroughly examining previous natural language processing (NLP) approaches in healthcare, shedding light on the limitations and challenges these methods face. Following that, this research explores the path that led to the incorporation of BioBERT into healthcare applications, highlighting its suitability for addressing the specific requirements of tasks related to biomedical text mining. The analysis outlines a systematic methodology for fine-tuning BioBERT to meet the unique needs of the healthcare domain. This approach includes various components, including the gathering of data from a wide range of healthcare sources, data annotation for tasks like identifying medical entities and categorizing them, and the application of specialized preprocessing techniques tailored to handle the complexities found in biomedical texts. Additionally, the pape
&lt;/p&gt;</description></item><item><title>NECO&#26159;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#22349;&#22604;&#30340;&#26032;&#39062;&#30340;&#36229;&#20986;&#20998;&#24067;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#20960;&#20309;&#23646;&#24615;&#21644;&#20027;&#25104;&#20998;&#31354;&#38388;&#35782;&#21035;OOD&#25968;&#25454;&#65292;&#22312;&#23567;&#35268;&#27169;&#21644;&#22823;&#35268;&#27169;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.06823</link><description>&lt;p&gt;
NECO: &#22522;&#20110;&#31070;&#32463;&#22349;&#22604;&#30340;&#36229;&#20986;&#20998;&#24067;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
NECO: NEural Collapse Based Out-of-distribution detection. (arXiv:2310.06823v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06823
&lt;/p&gt;
&lt;p&gt;
NECO&#26159;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#22349;&#22604;&#30340;&#26032;&#39062;&#30340;&#36229;&#20986;&#20998;&#24067;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#20960;&#20309;&#23646;&#24615;&#21644;&#20027;&#25104;&#20998;&#31354;&#38388;&#35782;&#21035;OOD&#25968;&#25454;&#65292;&#22312;&#23567;&#35268;&#27169;&#21644;&#22823;&#35268;&#27169;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#27169;&#22411;&#36807;&#20110;&#33258;&#20449;&#24182;&#19988;&#27809;&#26377;&#24847;&#35782;&#21040;&#20854;&#35748;&#35782;&#35770;&#38480;&#21046;&#65292;&#26816;&#27979;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#25968;&#25454;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#25105;&#20204;&#20551;&#35774;&#8220;&#31070;&#32463;&#22349;&#22604;&#8221;&#65292;&#19968;&#31181;&#24433;&#21709;&#36229;&#20986;&#20998;&#24067;&#25968;&#25454;&#30340;&#29616;&#35937;&#65292;&#20063;&#20250;&#24433;&#21709;&#36229;&#20986;&#20998;&#24067;&#25968;&#25454;&#12290;&#20026;&#20102;&#20174;&#36825;&#31181;&#30456;&#20114;&#20316;&#29992;&#20013;&#21463;&#30410;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;NECO&#65292;&#19968;&#31181;&#29992;&#20110;OOD&#26816;&#27979;&#30340;&#26032;&#39062;&#30340;&#20107;&#21518;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#8220;&#31070;&#32463;&#22349;&#22604;&#8221;&#21644;&#20027;&#25104;&#20998;&#31354;&#38388;&#30340;&#20960;&#20309;&#23646;&#24615;&#26469;&#35782;&#21035;OOD&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;NECO&#22312;&#23567;&#35268;&#27169;&#21644;&#22823;&#35268;&#27169;OOD&#26816;&#27979;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#22312;&#19981;&#21516;&#30340;&#32593;&#32476;&#26550;&#26500;&#19978;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;OOD&#26816;&#27979;&#20013;&#30340;&#26377;&#25928;&#24615;&#25552;&#20379;&#20102;&#29702;&#35770;&#35299;&#37322;&#12290;&#25105;&#20204;&#35745;&#21010;&#22312;&#21311;&#21517;&#26399;&#32467;&#26463;&#21518;&#21457;&#24067;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detecting out-of-distribution (OOD) data is a critical challenge in machine learning due to model overconfidence, often without awareness of their epistemological limits. We hypothesize that ``neural collapse'', a phenomenon affecting in-distribution data for models trained beyond loss convergence, also influences OOD data. To benefit from this interplay, we introduce NECO, a novel post-hoc method for OOD detection, which leverages the geometric properties of ``neural collapse'' and of principal component spaces to identify OOD data. Our extensive experiments demonstrate that NECO achieves state-of-the-art results on both small and large-scale OOD detection tasks while exhibiting strong generalization capabilities across different network architectures. Furthermore, we provide a theoretical explanation for the effectiveness of our method in OOD detection. We plan to release the code after the anonymity period.
&lt;/p&gt;</description></item><item><title>FABind&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;&#21475;&#34955;&#39044;&#27979;&#21644;&#23545;&#25509;&#30340;&#31471;&#21040;&#31471;&#27169;&#22411;&#65292;&#26088;&#22312;&#23454;&#29616;&#24555;&#36895;&#20934;&#30830;&#30340;&#34507;&#30333;-&#37197;&#20307;&#32467;&#21512;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2310.06763</link><description>&lt;p&gt;
FABind: &#24555;&#36895;&#20934;&#30830;&#30340;&#34507;&#30333;-&#37197;&#20307;&#32467;&#21512;
&lt;/p&gt;
&lt;p&gt;
FABind: Fast and Accurate Protein-Ligand Binding. (arXiv:2310.06763v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06763
&lt;/p&gt;
&lt;p&gt;
FABind&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;&#21475;&#34955;&#39044;&#27979;&#21644;&#23545;&#25509;&#30340;&#31471;&#21040;&#31471;&#27169;&#22411;&#65292;&#26088;&#22312;&#23454;&#29616;&#24555;&#36895;&#20934;&#30830;&#30340;&#34507;&#30333;-&#37197;&#20307;&#32467;&#21512;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33647;&#29289;&#21457;&#29616;&#20013;&#65292;&#23545;&#34507;&#30333;&#36136;&#21644;&#37197;&#20307;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#36827;&#34892;&#24314;&#27169;&#24182;&#20934;&#30830;&#39044;&#27979;&#20854;&#32467;&#21512;&#32467;&#26500;&#26159;&#19968;&#39033;&#20851;&#38190;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#22312;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#24076;&#26395;&#65292;&#37319;&#26679;&#27861;&#21644;&#22238;&#24402;&#27861;&#25104;&#20026;&#20004;&#31181;&#31361;&#20986;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#37117;&#23384;&#22312;&#26126;&#26174;&#30340;&#23616;&#38480;&#24615;&#12290;&#37319;&#26679;&#27861;&#36890;&#24120;&#30001;&#20110;&#38656;&#35201;&#29983;&#25104;&#22810;&#20010;&#20505;&#36873;&#32467;&#26500;&#26469;&#36827;&#34892;&#36873;&#25321;&#32780;&#25928;&#29575;&#36739;&#20302;&#12290;&#32780;&#22238;&#24402;&#27861;&#25552;&#20379;&#20102;&#24555;&#36895;&#30340;&#39044;&#27979;&#65292;&#20294;&#21487;&#33021;&#20250;&#23548;&#33268;&#20934;&#30830;&#24615;&#38477;&#20302;&#12290;&#21478;&#22806;&#65292;&#34507;&#30333;&#36136;&#22823;&#23567;&#30340;&#21464;&#21270;&#36890;&#24120;&#38656;&#35201;&#22806;&#37096;&#27169;&#22359;&#26469;&#36873;&#25321;&#21512;&#36866;&#30340;&#32467;&#21512;&#21475;&#34955;&#65292;&#36827;&#19968;&#27493;&#24433;&#21709;&#25928;&#29575;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FABind&#65292;&#19968;&#20010;&#23558;&#21475;&#34955;&#39044;&#27979;&#21644;&#23545;&#25509;&#30456;&#32467;&#21512;&#30340;&#31471;&#21040;&#31471;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#20934;&#30830;&#21644;&#24555;&#36895;&#30340;&#34507;&#30333;-&#37197;&#20307;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modeling the interaction between proteins and ligands and accurately predicting their binding structures is a critical yet challenging task in drug discovery. Recent advancements in deep learning have shown promise in addressing this challenge, with sampling-based and regression-based methods emerging as two prominent approaches. However, these methods have notable limitations. Sampling-based methods often suffer from low efficiency due to the need for generating multiple candidate structures for selection. On the other hand, regression-based methods offer fast predictions but may experience decreased accuracy. Additionally, the variation in protein sizes often requires external modules for selecting suitable binding pockets, further impacting efficiency. In this work, we propose $\mathbf{FABind}$, an end-to-end model that combines pocket prediction and docking to achieve accurate and fast protein-ligand binding. $\mathbf{FABind}$ incorporates a unique ligand-informed pocket prediction
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20840;&#38754;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22238;&#31572;&#20892;&#19994;&#30456;&#20851;&#38382;&#39064;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#20351;&#29992;RAG&#21644;ER&#25216;&#26415;&#25552;&#39640;&#20102;LLMs&#30340;&#24615;&#33021;&#12290;&#20854;&#20013;&#65292;GPT-4&#22312;&#20892;&#19994;&#32771;&#35797;&#20013;&#21462;&#24471;&#20102;&#21450;&#26684;&#20998;&#25968;&#20197;&#33719;&#24471;&#35748;&#35777;&#12290;</title><link>http://arxiv.org/abs/2310.06225</link><description>&lt;p&gt;
GPT-4&#20316;&#20026;&#20892;&#23398;&#21161;&#25163;&#65311;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22238;&#31572;&#20892;&#19994;&#32771;&#35797;
&lt;/p&gt;
&lt;p&gt;
GPT-4 as an Agronomist Assistant? Answering Agriculture Exams Using Large Language Models. (arXiv:2310.06225v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06225
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20840;&#38754;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22238;&#31572;&#20892;&#19994;&#30456;&#20851;&#38382;&#39064;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#20351;&#29992;RAG&#21644;ER&#25216;&#26415;&#25552;&#39640;&#20102;LLMs&#30340;&#24615;&#33021;&#12290;&#20854;&#20013;&#65292;GPT-4&#22312;&#20892;&#19994;&#32771;&#35797;&#20013;&#21462;&#24471;&#20102;&#21450;&#26684;&#20998;&#25968;&#20197;&#33719;&#24471;&#35748;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#65292;&#21253;&#25324;&#21307;&#30103;&#20445;&#20581;&#21644;&#37329;&#34701;&#39046;&#22495;&#65292;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#12290;&#22312;&#26576;&#20123;&#20219;&#21153;&#19978;&#65292;LLM&#30340;&#24615;&#33021;&#19982;&#35757;&#32451;&#26377;&#32032;&#30340;&#20154;&#31867;&#30456;&#20284;&#29978;&#33267;&#26356;&#22909;&#65292;&#22240;&#27492;&#21512;&#29702;&#22320;&#20351;&#29992;&#20154;&#31867;&#32771;&#35797;&#65288;&#20363;&#22914;&#35748;&#35777;&#32771;&#35797;&#65289;&#26469;&#35780;&#20272;LLM&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23545;&#27969;&#34892;&#30340;LLM&#65288;&#22914;Llama 2&#21644;GPT&#65289;&#22312;&#22238;&#31572;&#20892;&#19994;&#30456;&#20851;&#38382;&#39064;&#30340;&#33021;&#21147;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;&#22312;&#35780;&#20272;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#36824;&#36816;&#29992;&#20102;RAG&#65288;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65289;&#21644;ER&#65288;&#38598;&#21512;&#32454;&#21270;&#65289;&#25216;&#26415;&#65292;&#32467;&#21512;&#20449;&#24687;&#26816;&#32034;&#12289;&#29983;&#25104;&#33021;&#21147;&#21644;&#25552;&#31034;&#31574;&#30053;&#65292;&#25552;&#39640;LLM&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#23637;&#31034;LLM&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102;&#26469;&#33258;&#24052;&#35199;&#12289;&#21360;&#24230;&#21644;&#32654;&#22269;&#19977;&#20010;&#26368;&#22823;&#30340;&#20892;&#19994;&#29983;&#20135;&#22269;&#30340;&#20892;&#19994;&#32771;&#35797;&#21644;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#31361;&#20986;&#20102;GPT-4&#22312;&#32771;&#35797;&#20013;&#21462;&#24471;&#21450;&#26684;&#20998;&#25968;&#20197;&#33719;&#24471;&#35748;&#35777;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated remarkable capabilities in natural language understanding across various domains, including healthcare and finance. For some tasks, LLMs achieve similar or better performance than trained human beings, therefore it is reasonable to employ human exams (e.g., certification tests) to assess the performance of LLMs. We present a comprehensive evaluation of popular LLMs, such as Llama 2 and GPT, on their ability to answer agriculture-related questions. In our evaluation, we also employ RAG (Retrieval-Augmented Generation) and ER (Ensemble Refinement) techniques, which combine information retrieval, generation capabilities, and prompting strategies to improve the LLMs' performance. To demonstrate the capabilities of LLMs, we selected agriculture exams and benchmark datasets from three of the largest agriculture producer countries: Brazil, India, and the USA. Our analysis highlights GPT-4's ability to achieve a passing score on exams to earn cred
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#22411;&#28145;&#24230;&#23398;&#20064;&#30340;&#33258;&#21160;&#21270;&#33016;&#37096;X&#20809;&#25253;&#21578;&#29983;&#25104;&#22120;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#20010;&#20108;&#20803;&#20998;&#31867;&#27169;&#22411;&#26816;&#27979;&#22810;&#31181;&#24322;&#24120;&#65292;&#22312;&#21333;&#20010;&#22270;&#20687;&#20013;&#36741;&#21161;&#25918;&#23556;&#31185;&#21307;&#29983;&#30340;&#24037;&#20316;&#12290;&#35813;&#31995;&#32479;&#23558;&#25918;&#23556;&#23398;&#24322;&#24120;&#26816;&#27979;&#38480;&#21046;&#20026;&#24515;&#33039;&#32933;&#22823;&#12289;&#32954;&#31215;&#28082;&#21644;&#23454;&#21464;&#65292;&#24182;&#36890;&#36807;&#19977;&#20010;&#27493;&#39588;&#29983;&#25104;&#25918;&#23556;&#23398;&#25253;&#21578;&#65306;&#22270;&#20687;&#39044;&#22788;&#29702;&#12289;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26816;&#27979;&#24322;&#24120;&#21644;&#29983;&#25104;&#25253;&#21578;&#12290;</title><link>http://arxiv.org/abs/2310.05969</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#27169;&#22411;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#33258;&#21160;&#21270;&#33016;&#37096;X&#20809;&#25253;&#21578;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
Automated Chest X-Ray Report Generator Using Multi-Model Deep Learning Approach. (arXiv:2310.05969v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05969
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#22411;&#28145;&#24230;&#23398;&#20064;&#30340;&#33258;&#21160;&#21270;&#33016;&#37096;X&#20809;&#25253;&#21578;&#29983;&#25104;&#22120;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#20010;&#20108;&#20803;&#20998;&#31867;&#27169;&#22411;&#26816;&#27979;&#22810;&#31181;&#24322;&#24120;&#65292;&#22312;&#21333;&#20010;&#22270;&#20687;&#20013;&#36741;&#21161;&#25918;&#23556;&#31185;&#21307;&#29983;&#30340;&#24037;&#20316;&#12290;&#35813;&#31995;&#32479;&#23558;&#25918;&#23556;&#23398;&#24322;&#24120;&#26816;&#27979;&#38480;&#21046;&#20026;&#24515;&#33039;&#32933;&#22823;&#12289;&#32954;&#31215;&#28082;&#21644;&#23454;&#21464;&#65292;&#24182;&#36890;&#36807;&#19977;&#20010;&#27493;&#39588;&#29983;&#25104;&#25918;&#23556;&#23398;&#25253;&#21578;&#65306;&#22270;&#20687;&#39044;&#22788;&#29702;&#12289;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26816;&#27979;&#24322;&#24120;&#21644;&#29983;&#25104;&#25253;&#21578;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38405;&#35835;&#21644;&#35299;&#35835;&#33016;&#37096;X&#20809;&#22270;&#20687;&#26159;&#22823;&#22810;&#25968;&#25918;&#23556;&#31185;&#21307;&#29983;&#30340;&#20363;&#34892;&#24037;&#20316;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#23545;&#20110;&#32463;&#39564;&#26368;&#20016;&#23500;&#30340;&#21307;&#29983;&#26469;&#35828;&#65292;&#36825;&#20173;&#28982;&#21487;&#33021;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#22411;&#28145;&#24230;&#23398;&#20064;&#30340;&#33258;&#21160;&#21270;&#33016;&#37096;X&#20809;&#25253;&#21578;&#29983;&#25104;&#31995;&#32479;&#65292;&#26088;&#22312;&#36741;&#21161;&#25918;&#23556;&#31185;&#21307;&#29983;&#30340;&#24037;&#20316;&#12290;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#30340;&#22522;&#26412;&#24605;&#24819;&#26159;&#21033;&#29992;&#22810;&#20010;&#20108;&#20803;&#20998;&#31867;&#27169;&#22411;&#26469;&#26816;&#27979;&#22810;&#31181;&#24322;&#24120;&#65292;&#27599;&#20010;&#27169;&#22411;&#36127;&#36131;&#26816;&#27979;&#19968;&#31181;&#24322;&#24120;&#65292;&#22312;&#21333;&#20010;&#22270;&#20687;&#20013;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23558;&#25918;&#23556;&#23398;&#24322;&#24120;&#26816;&#27979;&#38480;&#21046;&#20026;&#24515;&#33039;&#32933;&#22823;&#12289;&#32954;&#31215;&#28082;&#21644;&#23454;&#21464;&#12290;&#31995;&#32479;&#36890;&#36807;&#25191;&#34892;&#20197;&#19979;&#19977;&#20010;&#27493;&#39588;&#29983;&#25104;&#25918;&#23556;&#23398;&#25253;&#21578;&#65306;&#22270;&#20687;&#39044;&#22788;&#29702;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26816;&#27979;&#24322;&#24120;&#65292;&#24182;&#29983;&#25104;&#25253;&#21578;&#12290;&#22270;&#20687;&#39044;&#22788;&#29702;&#27493;&#39588;&#30340;&#30446;&#30340;&#26159;&#36890;&#36807;&#23558;&#20854;&#32553;&#25918;&#20026;128x128&#20687;&#32032;&#24182;&#20998;&#21106;&#25104;&#19977;&#20010;&#37096;&#20998;&#26469;&#20351;&#36755;&#20837;&#26631;&#20934;&#21270;&#65292;&#20998;&#21035;&#28085;&#30422;&#19978;&#37096;&#12289;&#19979;&#37096;&#21644;&#20013;&#37096;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reading and interpreting chest X-ray images is one of the most radiologist's routines. However, it still can be challenging, even for the most experienced ones. Therefore, we proposed a multi-model deep learning-based automated chest X-ray report generator system designed to assist radiologists in their work. The basic idea of the proposed system is by utilizing multi binary-classification models for detecting multi abnormalities, with each model responsible for detecting one abnormality, in a single image. In this study, we limited the radiology abnormalities detection to only cardiomegaly, lung effusion, and consolidation. The system generates a radiology report by performing the following three steps: image pre-processing, utilizing deep learning models to detect abnormalities, and producing a report. The aim of the image pre-processing step is to standardize the input by scaling it to 128x128 pixels and slicing it into three segments, which covers the upper, lower, and middle parts
&lt;/p&gt;</description></item><item><title>Lion&#26159;&#36890;&#36807;&#31243;&#24207;&#25628;&#32034;&#21457;&#29616;&#30340;&#26032;&#20248;&#21270;&#22120;&#65292;&#22312;&#35757;&#32451;&#22823;&#22411;AI&#27169;&#22411;&#26041;&#38754;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#20869;&#23384;&#25928;&#29575;&#12290;&#23613;&#31649;&#20854;&#29702;&#35770;&#22522;&#30784;&#19981;&#26126;&#30830;&#65292;&#20294;&#22522;&#20110;&#36830;&#32493;&#26102;&#38388;&#21644;&#31163;&#25955;&#26102;&#38388;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;Lion&#26159;&#19968;&#31181;&#29702;&#35770;&#19978;&#26032;&#39062;&#19988;&#26377;&#21407;&#21017;&#30340;&#26041;&#27861;&#65292;&#21487;&#22312;&#26368;&#23567;&#21270;&#19968;&#33324;&#25439;&#22833;&#20989;&#25968;&#30340;&#21516;&#26102;&#24378;&#21046;&#25191;&#34892;&#36793;&#30028;&#32422;&#26463;&#12290;</title><link>http://arxiv.org/abs/2310.05898</link><description>&lt;p&gt;
&#29422;&#23376;&#31192;&#23494;&#22320;&#35299;&#20915;&#21463;&#38480;&#21046;&#20248;&#21270;&#38382;&#39064;&#65306;&#27491;&#22914;&#26446;&#38597;&#26222;&#35834;&#22827;&#25152;&#39044;&#27979;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lion Secretly Solves Constrained Optimization: As Lyapunov Predicts. (arXiv:2310.05898v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05898
&lt;/p&gt;
&lt;p&gt;
Lion&#26159;&#36890;&#36807;&#31243;&#24207;&#25628;&#32034;&#21457;&#29616;&#30340;&#26032;&#20248;&#21270;&#22120;&#65292;&#22312;&#35757;&#32451;&#22823;&#22411;AI&#27169;&#22411;&#26041;&#38754;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#20869;&#23384;&#25928;&#29575;&#12290;&#23613;&#31649;&#20854;&#29702;&#35770;&#22522;&#30784;&#19981;&#26126;&#30830;&#65292;&#20294;&#22522;&#20110;&#36830;&#32493;&#26102;&#38388;&#21644;&#31163;&#25955;&#26102;&#38388;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;Lion&#26159;&#19968;&#31181;&#29702;&#35770;&#19978;&#26032;&#39062;&#19988;&#26377;&#21407;&#21017;&#30340;&#26041;&#27861;&#65292;&#21487;&#22312;&#26368;&#23567;&#21270;&#19968;&#33324;&#25439;&#22833;&#20989;&#25968;&#30340;&#21516;&#26102;&#24378;&#21046;&#25191;&#34892;&#36793;&#30028;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31243;&#24207;&#25628;&#32034;&#21457;&#29616;&#30340;&#26032;&#20248;&#21270;&#22120;Lion&#65288;&#36827;&#21270;&#30340;&#31526;&#21495;&#21160;&#37327;&#65289;&#22312;&#35757;&#32451;&#22823;&#22411;AI&#27169;&#22411;&#26041;&#38754;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#23427;&#22312;&#35757;&#32451;&#25928;&#26524;&#19978;&#19982;AdamW&#30456;&#24403;&#25110;&#26356;&#22909;&#65292;&#24182;&#20855;&#26377;&#26356;&#39640;&#30340;&#20869;&#23384;&#25928;&#29575;&#12290;&#27491;&#22914;&#25105;&#20204;&#21487;&#20197;&#20174;&#38543;&#26426;&#25628;&#32034;&#31243;&#24207;&#30340;&#32467;&#26524;&#20013;&#26399;&#24453;&#30340;&#65292;Lion&#38598;&#25104;&#20102;&#20960;&#20010;&#29616;&#26377;&#31639;&#27861;&#30340;&#20803;&#32032;&#65292;&#21253;&#25324;&#31526;&#21495;&#21160;&#37327;&#12289;&#29420;&#31435;&#30340;&#26435;&#37325;&#34928;&#20943;&#12289;Polak&#21644;Nesterov&#21160;&#37327;&#65292;&#20294;&#21448;&#19981;&#23646;&#20110;&#20219;&#20309;&#29616;&#26377;&#30340;&#29702;&#35770;&#22522;&#30784;&#20248;&#21270;&#22120;&#31867;&#21035;&#12290;&#22240;&#27492;&#65292;&#23613;&#31649;Lion&#20316;&#20026;&#24191;&#27867;&#20219;&#21153;&#30340;&#36890;&#29992;&#20248;&#21270;&#22120;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#20854;&#29702;&#35770;&#22522;&#30784;&#20173;&#28982;&#19981;&#26126;&#30830;&#12290;&#36825;&#31181;&#32570;&#20047;&#29702;&#35770;&#30340;&#26126;&#30830;&#24615;&#38480;&#21046;&#20102;&#36827;&#19968;&#27493;&#22686;&#24378;&#21644;&#25193;&#23637;Lion&#30340;&#21487;&#33021;&#24615;&#12290;&#26412;&#25991;&#26088;&#22312;&#25581;&#24320;Lion&#30340;&#31070;&#31192;&#38754;&#32433;&#12290;&#22522;&#20110;&#36830;&#32493;&#26102;&#38388;&#21644;&#31163;&#25955;&#26102;&#38388;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;Lion&#26159;&#19968;&#31181;&#29702;&#35770;&#19978;&#26032;&#39062;&#19988;&#26377;&#21407;&#21017;&#30340;&#26041;&#27861;&#65292;&#21487;&#22312;&#26368;&#23567;&#21270;&#19968;&#33324;&#25439;&#22833;&#20989;&#25968;$f(x)$&#30340;&#21516;&#26102;&#24378;&#21046;&#25191;&#34892;&#36793;&#30028;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lion (Evolved Sign Momentum), a new optimizer discovered through program search, has shown promising results in training large AI models. It performs comparably or favorably to AdamW but with greater memory efficiency. As we can expect from the results of a random search program, Lion incorporates elements from several existing algorithms, including signed momentum, decoupled weight decay, Polak, and Nesterov momentum, but does not fit into any existing category of theoretically grounded optimizers. Thus, even though Lion appears to perform well as a general-purpose optimizer for a wide range of tasks, its theoretical basis remains uncertain. This lack of theoretical clarity limits opportunities to further enhance and expand Lion's efficacy.  This work aims to demystify Lion. Based on both continuous-time and discrete-time analysis, we demonstrate that Lion is a theoretically novel and principled approach for minimizing a general loss function $f(x)$ while enforcing a bound constraint 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;&#20174;&#27861;&#24459;&#26696;&#20214;&#20013;&#33258;&#21160;&#29983;&#25104;&#35770;&#35777;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#26368;&#20339;&#26041;&#27861;&#29983;&#25104;&#30340;&#35770;&#35777;&#19982;&#22522;&#20934;&#38598;&#30340;&#40644;&#37329;&#26631;&#20934;&#27880;&#37322;&#24179;&#22343;&#37325;&#21472;&#29575;&#20026;63%&#12290;</title><link>http://arxiv.org/abs/2310.05680</link><description>&lt;p&gt;
&#20174;&#27861;&#24459;&#20107;&#23454;&#20013;&#33258;&#21160;&#29983;&#25104;&#35770;&#35777;&#30340;&#33258;&#21160;&#21270;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated Argument Generation from Legal Facts. (arXiv:2310.05680v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05680
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;&#20174;&#27861;&#24459;&#26696;&#20214;&#20013;&#33258;&#21160;&#29983;&#25104;&#35770;&#35777;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#26368;&#20339;&#26041;&#27861;&#29983;&#25104;&#30340;&#35770;&#35777;&#19982;&#22522;&#20934;&#38598;&#30340;&#40644;&#37329;&#26631;&#20934;&#27880;&#37322;&#24179;&#22343;&#37325;&#21472;&#29575;&#20026;63%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#65292;&#24453;&#22788;&#29702;&#26696;&#20214;&#25968;&#37327;&#21576;&#25351;&#25968;&#22686;&#38271;&#65288;&#20363;&#22914;&#65292;&#20165;&#22312;&#21360;&#24230;&#23601;&#26377;&#36229;&#36807;1000&#19975;&#20010;&#24453;&#22788;&#29702;&#26696;&#20214;&#65289;&#12290;&#20027;&#35201;&#38382;&#39064;&#22312;&#20110;&#25552;&#20132;&#32473;&#27861;&#24459;&#31995;&#32479;&#30340;&#26696;&#20214;&#25968;&#37327;&#36828;&#36828;&#36229;&#36807;&#19968;&#20010;&#22269;&#23478;&#20013;&#29616;&#26377;&#30340;&#27861;&#24459;&#19987;&#19994;&#20154;&#21592;&#30340;&#25968;&#37327;&#12290;&#22312;&#36825;&#20010;&#20840;&#29699;&#32972;&#26223;&#19979;&#65292;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#26469;&#25552;&#39640;&#27861;&#24459;&#31243;&#24207;&#30340;&#25928;&#29575;&#21644;&#36895;&#24230;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#24110;&#21161;&#27861;&#24459;&#19987;&#19994;&#20154;&#21592;&#20998;&#26512;&#27861;&#24459;&#26696;&#20214;&#30340;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#20855;&#20307;&#35843;&#26597;&#25506;&#32034;&#20102;&#21033;&#29992;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;&#20174;&#27861;&#24459;&#26696;&#20214;&#20013;&#25552;&#21462;&#20986;&#35770;&#35777;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#26368;&#20339;&#26041;&#27861;&#29983;&#25104;&#30340;&#35770;&#35777;&#19982;&#22522;&#20934;&#38598;&#30340;&#40644;&#37329;&#26631;&#20934;&#27880;&#37322;&#24179;&#22343;&#37325;&#21472;&#29575;&#20026;63%&#12290;
&lt;/p&gt;
&lt;p&gt;
The count of pending cases has shown an exponential rise across nations (e.g., with more than 10 million pending cases in India alone). The main issue lies in the fact that the number of cases submitted to the law system is far greater than the available number of legal professionals present in a country. Given this worldwide context, the utilization of AI technology has gained paramount importance to enhance the efficiency and speed of legal procedures. In this study we partcularly focus on helping legal professionals in the process of analyzing a legal case. Our specific investigation delves into harnessing the generative capabilities of open-sourced large language models to create arguments derived from the facts present in legal cases. Experimental results show that the generated arguments from the best performing method have on average 63% overlap with the benchmark set gold standard annotations.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#26681;&#26412;&#24615;&#30340;&#25913;&#38761;&#65292;&#23558;&#30005;&#23376;&#36135;&#24065;&#32435;&#20837;&#32463;&#27982;&#19982;&#37329;&#34701;&#29702;&#35770;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#21253;&#25324;&#30005;&#23376;&#36135;&#24065;&#30340;&#20272;&#20540;&#22522;&#20110;&#23439;&#35266;&#32463;&#27982;&#29702;&#35770;&#21644;&#36135;&#24065;&#25919;&#31574;&#30340;&#22522;&#26412;&#26041;&#31243;&#65292;&#20197;&#21450;&#30005;&#23376;&#36135;&#24065;&#31649;&#29702;&#20844;&#21496;&#20316;&#20026;&#21327;&#35843;&#27425;&#32463;&#27982;&#20307;&#36135;&#24065;&#21644;&#36130;&#25919;&#25919;&#31574;&#30340;&#23454;&#20307;&#12290;&#35813;&#30740;&#31350;&#36991;&#20813;&#20351;&#29992;&#26222;&#36941;&#20294;&#19981;&#36866;&#24403;&#30340;&#25351;&#25968;&#39118;&#38505;&#27169;&#22411;&#65292;&#32780;&#26159;&#37319;&#29992;&#22810;&#26102;&#38388;&#23610;&#24230;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.04986</link><description>&lt;p&gt;
&#19968;&#31181;&#20851;&#20110;&#36135;&#24065;&#30340;&#26032;&#30340;&#32463;&#27982;&#19982;&#37329;&#34701;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
A new economic and financial theory of money. (arXiv:2310.04986v1 [econ.TH])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04986
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#26681;&#26412;&#24615;&#30340;&#25913;&#38761;&#65292;&#23558;&#30005;&#23376;&#36135;&#24065;&#32435;&#20837;&#32463;&#27982;&#19982;&#37329;&#34701;&#29702;&#35770;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#21253;&#25324;&#30005;&#23376;&#36135;&#24065;&#30340;&#20272;&#20540;&#22522;&#20110;&#23439;&#35266;&#32463;&#27982;&#29702;&#35770;&#21644;&#36135;&#24065;&#25919;&#31574;&#30340;&#22522;&#26412;&#26041;&#31243;&#65292;&#20197;&#21450;&#30005;&#23376;&#36135;&#24065;&#31649;&#29702;&#20844;&#21496;&#20316;&#20026;&#21327;&#35843;&#27425;&#32463;&#27982;&#20307;&#36135;&#24065;&#21644;&#36130;&#25919;&#25919;&#31574;&#30340;&#23454;&#20307;&#12290;&#35813;&#30740;&#31350;&#36991;&#20813;&#20351;&#29992;&#26222;&#36941;&#20294;&#19981;&#36866;&#24403;&#30340;&#25351;&#25968;&#39118;&#38505;&#27169;&#22411;&#65292;&#32780;&#26159;&#37319;&#29992;&#22810;&#26102;&#38388;&#23610;&#24230;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#32463;&#27982;&#19982;&#37329;&#34701;&#29702;&#35770;&#36827;&#34892;&#20102;&#26681;&#26412;&#24615;&#25913;&#38761;&#65292;&#21253;&#25324;&#30005;&#23376;&#36135;&#24065;&#22312;&#20869;&#12290;&#30005;&#23376;&#36135;&#24065;&#30340;&#20272;&#20540;&#23558;&#22522;&#20110;&#23439;&#35266;&#32463;&#27982;&#29702;&#35770;&#21644;&#36135;&#24065;&#25919;&#31574;&#30340;&#22522;&#26412;&#26041;&#31243;&#65292;&#32780;&#19981;&#26159;&#24494;&#35266;&#32463;&#27982;&#23398;&#20013;&#30340;&#36148;&#29616;&#29616;&#37329;&#27969;&#29702;&#35770;&#12290;&#19982;&#23558;&#32929;&#31080;&#35270;&#20026;&#19982;&#27425;&#32463;&#27982;&#20307;&#30340;&#26080;&#24418;&#36164;&#20135;&#20851;&#32852;&#30340;&#25152;&#26377;&#26435;&#19981;&#21516;&#65292;&#25105;&#20204;&#23558;&#21457;&#23637;&#30005;&#23376;&#36135;&#24065;&#20316;&#20026;&#19982;&#27425;&#32463;&#27982;&#20307;&#26377;&#24418;&#36164;&#20135;&#20851;&#32852;&#30340;&#20132;&#26131;&#26435;&#30410;&#30340;&#35266;&#28857;&#12290;&#25105;&#20204;&#36824;&#23558;&#21457;&#23637;&#30005;&#23376;&#36135;&#24065;&#31649;&#29702;&#20844;&#21496;&#20316;&#20026;&#19968;&#20010;&#36127;&#36131;&#21327;&#35843;&#27425;&#32463;&#27982;&#20307;&#30340;&#36135;&#24065;&#65288;&#30005;&#23376;&#36135;&#24065;&#20379;&#24212;&#21644;&#20215;&#20540;&#31283;&#23450;&#65289;&#21644;&#36130;&#25919;&#65288;&#25237;&#36164;&#21644;&#36816;&#33829;&#65289;&#25919;&#31574;&#30340;&#23454;&#20307;&#30340;&#35270;&#35282;&#65292;&#20197;&#23454;&#29616;&#30005;&#23376;&#36135;&#24065;&#30340;&#27969;&#21160;&#24615;&#12290;&#22312;&#20272;&#20540;&#21644;&#20915;&#31574;&#20013;&#20351;&#29992;&#30340;&#39118;&#38505;&#27169;&#22411;&#19981;&#20250;&#26159;&#26080;&#22788;&#19981;&#22312;&#20294;&#19981;&#21512;&#36866;&#30340;&#25351;&#25968;&#39118;&#38505;&#27169;&#22411;&#65292;&#23427;&#23558;&#23548;&#33268;&#36148;&#29616;&#29575;&#65292;&#32780;&#26159;&#22810;&#26102;&#38388;&#23610;&#24230;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper fundamentally reformulates economic and financial theory to include electronic currencies. The valuation of the electronic currencies will be based on macroeconomic theory and the fundamental equation of monetary policy, not the microeconomic theory of discounted cash flows. The view of electronic currency as a transactional equity associated with tangible assets of a sub-economy will be developed, in contrast to the view of stock as an equity associated mostly with intangible assets of a sub-economy. The view will be developed of the electronic currency management firm as an entity responsible for coordinated monetary (electronic currency supply and value stabilization) and fiscal (investment and operational) policies of a substantial (for liquidity of the electronic currency) sub-economy. The risk model used in the valuations and the decision-making will not be the ubiquitous, yet inappropriate, exponential risk model that leads to discount rates, but will be multi time sc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#24320;&#21019;&#24615;&#30340;&#25104;&#23545;GUI&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#35299;&#20915;Android&#25163;&#26426;&#21644;&#24179;&#26495;&#20043;&#38388;&#33258;&#21160;&#21270;GUI&#24320;&#21457;&#30340;&#38556;&#30861;&#12290;&#25968;&#25454;&#38598;&#21253;&#25324;10035&#20010;&#25163;&#26426;-&#24179;&#26495;GUI&#39029;&#38754;&#23545;&#65292;&#20026;&#25552;&#39640;&#24320;&#21457;&#20154;&#21592;&#30340;&#29983;&#20135;&#21147;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2310.04755</link><description>&lt;p&gt;
Android&#25163;&#26426;&#21644;&#24179;&#26495;&#20043;&#38388;&#30340;&#25104;&#23545;GUI&#25968;&#25454;&#38598;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
Pairwise GUI Dataset Construction Between Android Phones and Tablets. (arXiv:2310.04755v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04755
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#24320;&#21019;&#24615;&#30340;&#25104;&#23545;GUI&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#35299;&#20915;Android&#25163;&#26426;&#21644;&#24179;&#26495;&#20043;&#38388;&#33258;&#21160;&#21270;GUI&#24320;&#21457;&#30340;&#38556;&#30861;&#12290;&#25968;&#25454;&#38598;&#21253;&#25324;10035&#20010;&#25163;&#26426;-&#24179;&#26495;GUI&#39029;&#38754;&#23545;&#65292;&#20026;&#25552;&#39640;&#24320;&#21457;&#20154;&#21592;&#30340;&#29983;&#20135;&#21147;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#21069;&#26222;&#21450;&#24615;&#25163;&#26426;&#21644;&#24179;&#26495;&#30340;&#29615;&#22659;&#20013;&#65292;&#24212;&#29992;&#31243;&#24207;&#32463;&#24120;&#23384;&#22312;&#20110;&#20004;&#20010;&#24179;&#21488;&#19978;&#12290;&#34429;&#28982;&#24212;&#29992;&#31243;&#24207;&#22312;&#25163;&#26426;&#21644;&#24179;&#26495;&#19978;&#20849;&#20139;&#22823;&#37096;&#20998;&#22270;&#24418;&#29992;&#25143;&#30028;&#38754;&#65288;GUI&#65289;&#21644;&#21151;&#33021;&#65292;&#20294;&#24320;&#21457;&#20154;&#21592;&#32463;&#24120;&#38656;&#35201;&#20174;&#22836;&#24320;&#22987;&#20026;&#24179;&#26495;&#29256;&#26412;&#37325;&#26032;&#26500;&#24314;&#65292;&#22686;&#21152;&#20102;&#24320;&#21457;&#25104;&#26412;&#24182;&#28010;&#36153;&#20102;&#29616;&#26377;&#30340;&#35774;&#35745;&#36164;&#28304;&#12290;&#30740;&#31350;&#20154;&#21592;&#27491;&#23581;&#35797;&#25910;&#38598;&#25968;&#25454;&#65292;&#24182;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#22312;&#33258;&#21160;&#21270;GUI&#24320;&#21457;&#20013;&#25552;&#39640;&#24320;&#21457;&#20154;&#21592;&#30340;&#29983;&#20135;&#21147;&#12290;&#30446;&#21069;&#23384;&#22312;&#19968;&#20123;&#20844;&#24320;&#21487;&#35775;&#38382;&#30340;&#25163;&#26426;GUI&#39029;&#38754;&#25968;&#25454;&#38598;&#65292;&#20294;&#27809;&#26377;&#20851;&#20110;&#25163;&#26426;&#21644;&#24179;&#26495;&#20043;&#38388;&#25104;&#23545;GUI&#30340;&#25968;&#25454;&#38598;&#12290;&#36825;&#23545;&#20110;&#22312;&#33258;&#21160;&#21270;GUI&#24320;&#21457;&#20013;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26500;&#25104;&#20102;&#19968;&#20010;&#37325;&#35201;&#38556;&#30861;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Papt&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#20026;Android&#25163;&#26426;&#21644;&#24179;&#26495;&#37327;&#36523;&#23450;&#21046;&#30340;&#24320;&#21019;&#24615;&#30340;&#25104;&#23545;GUI&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#26469;&#33258;5593&#20010;&#19981;&#21516;&#24212;&#29992;&#31243;&#24207;&#23545;&#30340;10035&#20010;&#25163;&#26426;-&#24179;&#26495;GUI&#39029;&#38754;&#23545;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#25104;&#23545;GUI&#25910;&#38598;&#26041;&#27861;&#26469;&#26500;&#24314;&#36825;&#20010;&#25968;&#25454;&#38598;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#30028;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the current landscape of pervasive smartphones and tablets, apps frequently exist across both platforms. Although apps share most graphic user interfaces (GUIs) and functionalities across phones and tablets, developers often rebuild from scratch for tablet versions, escalating costs and squandering existing design resources. Researchers are attempting to collect data and employ deep learning in automated GUIs development to enhance developers' productivity. There are currently several publicly accessible GUI page datasets for phones, but none for pairwise GUIs between phones and tablets. This poses a significant barrier to the employment of deep learning in automated GUI development. In this paper, we introduce the Papt dataset, a pioneering pairwise GUI dataset tailored for Android phones and tablets, encompassing 10,035 phone-tablet GUI page pairs sourced from 5,593 unique app pairs. We propose novel pairwise GUI collection approaches for constructing this dataset and delineate it
&lt;/p&gt;</description></item><item><title>DeepSpeed4Science&#35745;&#21010;&#26088;&#22312;&#36890;&#36807;&#20808;&#36827;&#30340;AI&#31995;&#32479;&#25216;&#26415;&#21152;&#36895;&#31185;&#23398;&#21457;&#29616;&#65292;&#25552;&#20379;&#38024;&#23545;&#29420;&#29305;&#22797;&#26434;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20026;&#33258;&#28982;&#31185;&#23398;&#24102;&#26469;&#37325;&#22823;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2310.04610</link><description>&lt;p&gt;
DeepSpeed4Science&#35745;&#21010;&#65306;&#36890;&#36807;&#20808;&#36827;&#30340;AI&#31995;&#32479;&#25216;&#26415;&#23454;&#29616;&#22823;&#35268;&#27169;&#31185;&#23398;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
DeepSpeed4Science Initiative: Enabling Large-Scale Scientific Discovery through Sophisticated AI System Technologies. (arXiv:2310.04610v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04610
&lt;/p&gt;
&lt;p&gt;
DeepSpeed4Science&#35745;&#21010;&#26088;&#22312;&#36890;&#36807;&#20808;&#36827;&#30340;AI&#31995;&#32479;&#25216;&#26415;&#21152;&#36895;&#31185;&#23398;&#21457;&#29616;&#65292;&#25552;&#20379;&#38024;&#23545;&#29420;&#29305;&#22797;&#26434;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20026;&#33258;&#28982;&#31185;&#23398;&#24102;&#26469;&#37325;&#22823;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21363;&#23558;&#21040;&#26469;&#30340;&#21313;&#24180;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#21487;&#33021;&#20250;&#24443;&#24213;&#25913;&#21464;&#33258;&#28982;&#31185;&#23398;&#65292;&#22686;&#24378;&#25105;&#20204;&#23545;&#33258;&#28982;&#29616;&#35937;&#24314;&#27169;&#21644;&#39044;&#27979;&#30340;&#33021;&#21147;&#12290;&#36825;&#21487;&#33021;&#39044;&#31034;&#30528;&#31185;&#23398;&#25506;&#32034;&#30340;&#26032;&#26102;&#20195;&#65292;&#20174;&#33647;&#29289;&#24320;&#21457;&#21040;&#21487;&#20877;&#29983;&#33021;&#28304;&#31561;&#39046;&#22495;&#37117;&#23558;&#21462;&#24471;&#37325;&#22823;&#36827;&#23637;&#12290;&#20026;&#20102;&#22238;&#24212;&#36825;&#19968;&#21628;&#21505;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DeepSpeed4Science&#35745;&#21010;&#65288;deepspeed4science.ai&#65289;&#65292;&#26088;&#22312;&#36890;&#36807;AI&#31995;&#32479;&#25216;&#26415;&#21019;&#26032;&#26500;&#24314;&#29420;&#29305;&#30340;&#33021;&#21147;&#65292;&#24110;&#21161;&#39046;&#22495;&#19987;&#23478;&#35299;&#24320;&#20170;&#22825;&#26368;&#22823;&#30340;&#31185;&#23398;&#20043;&#35868;&#12290;&#36890;&#36807;&#21033;&#29992;DeepSpeed&#24403;&#21069;&#30340;&#25216;&#26415;&#25903;&#26609;&#65288;&#35757;&#32451;&#12289;&#25512;&#29702;&#21644;&#21387;&#32553;&#65289;&#20316;&#20026;&#22522;&#30784;&#25216;&#26415;&#25903;&#25345;&#65292;DeepSpeed4Science&#23558;&#21019;&#24314;&#19968;&#22871;&#38024;&#23545;&#21152;&#36895;&#31185;&#23398;&#21457;&#29616;&#30340;AI&#31995;&#32479;&#25216;&#26415;&#65292;&#20197;&#24212;&#23545;&#20854;&#29420;&#29305;&#30340;&#22797;&#26434;&#24615;&#65292;&#36229;&#36234;&#24120;&#35265;&#30340;&#29992;&#20110;&#21152;&#36895;&#36890;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25216;&#26415;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;DeepSpeed4Science&#22312;&#35299;&#20915;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#19978;&#30340;&#26089;&#26399;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the upcoming decade, deep learning may revolutionize the natural sciences, enhancing our capacity to model and predict natural occurrences. This could herald a new era of scientific exploration, bringing significant advancements across sectors from drug development to renewable energy. To answer this call, we present DeepSpeed4Science initiative (deepspeed4science.ai) which aims to build unique capabilities through AI system technology innovations to help domain experts to unlock today's biggest science mysteries. By leveraging DeepSpeed's current technology pillars (training, inference and compression) as base technology enablers, DeepSpeed4Science will create a new set of AI system technologies tailored for accelerating scientific discoveries by addressing their unique complexity beyond the common technical approaches used for accelerating generic large language models (LLMs). In this paper, we showcase the early progress we made with DeepSpeed4Science in addressing two of the cri
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#22522;&#20110;&#22122;&#22768;&#25200;&#21160;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;transformer T5&#27169;&#22411;&#29983;&#25104;&#29420;&#29305;&#19988;&#36830;&#36143;&#30340;&#21475;&#21495;&#65292;&#21516;&#26102;&#23558;&#20844;&#21496;&#21644;&#21697;&#29260;&#30340;&#25551;&#36848;&#32435;&#20837;&#21040;&#29983;&#25104;&#36807;&#31243;&#20013;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21475;&#21495;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.04472</link><description>&lt;p&gt;
&#22122;&#22768;&#25200;&#21160;&#19979;&#30340;&#26377;&#25928;&#21475;&#21495;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Effective Slogan Generation with Noise Perturbation. (arXiv:2310.04472v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04472
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#22522;&#20110;&#22122;&#22768;&#25200;&#21160;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;transformer T5&#27169;&#22411;&#29983;&#25104;&#29420;&#29305;&#19988;&#36830;&#36143;&#30340;&#21475;&#21495;&#65292;&#21516;&#26102;&#23558;&#20844;&#21496;&#21644;&#21697;&#29260;&#30340;&#25551;&#36848;&#32435;&#20837;&#21040;&#29983;&#25104;&#36807;&#31243;&#20013;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21475;&#21495;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21475;&#21495;&#22312;&#24314;&#31435;&#20844;&#21496;&#21697;&#29260;&#24418;&#35937;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#21475;&#21495;&#34987;&#26399;&#26395;&#20197;&#20196;&#20154;&#38590;&#24536;&#21644;&#35752;&#20154;&#21916;&#27426;&#30340;&#26041;&#24335;&#21453;&#26144;&#20844;&#21496;&#30340;&#24895;&#26223;&#21644;&#21697;&#29260;&#30340;&#20215;&#20540;&#20027;&#24352;&#12290;&#33258;&#21160;&#21270;&#29983;&#25104;&#20855;&#26377;&#36825;&#20123;&#29305;&#28857;&#30340;&#21475;&#21495;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#21457;&#23637;&#21644;&#27979;&#35797;&#20102;&#20855;&#26377;&#21477;&#27861;&#25511;&#21046;&#21644;&#25688;&#35201;&#27169;&#22411;&#30340;&#21475;&#21495;&#29983;&#25104;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#26080;&#27861;&#29983;&#25104;&#29420;&#29305;&#30340;&#21475;&#21495;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;transformer T5&#27169;&#22411;&#21644;&#26032;&#25552;&#20986;&#30340;1:N&#21305;&#37197;&#23545;&#25968;&#25454;&#38598;&#30340;&#22122;&#22768;&#25200;&#21160;&#12290;&#35813;&#26041;&#27861;&#22312;&#29983;&#25104;&#29420;&#29305;&#19988;&#36830;&#36143;&#30340;&#21475;&#21495;&#26041;&#38754;&#36215;&#21040;&#20102;&#20419;&#36827;&#20316;&#29992;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#23558;&#20844;&#21496;&#21644;&#21697;&#29260;&#30340;&#25551;&#36848;&#32435;&#20837;&#21040;&#21475;&#21495;&#29983;&#25104;&#20013;&#12290;&#25105;&#20204;&#26681;&#25454;ROUGE1&#12289;ROUGEL&#21644;&#20313;&#24358;&#30456;&#20284;&#24615;&#25351;&#26631;&#35780;&#20272;&#29983;&#25104;&#30340;&#21475;&#21495;&#65292;&#24182;&#36890;&#36807;&#20154;&#20026;&#20027;&#20307;&#35780;&#20272;&#23427;&#20204;&#30340;&#29420;&#29305;&#24615;&#12289;&#36830;&#36143;&#24615;&#21644;&#27969;&#30021;&#24615;&#12290;&#32467;&#26524;&#26174;&#31034;o
&lt;/p&gt;
&lt;p&gt;
Slogans play a crucial role in building the brand's identity of the firm. A slogan is expected to reflect firm's vision and brand's value propositions in memorable and likeable ways. Automating the generation of slogans with such characteristics is challenging. Previous studies developted and tested slogan generation with syntactic control and summarization models which are not capable of generating distinctive slogans. We introduce a a novel apporach that leverages pre-trained transformer T5 model with noise perturbation on newly proposed 1:N matching pair dataset. This approach serves as a contributing fator in generting distinctive and coherent slogans. Turthermore, the proposed approach incorporates descriptions about the firm and brand into the generation of slogans. We evaluate generated slogans based on ROUGE1, ROUGEL and Cosine Similarity metrics and also assess them with human subjects in terms of slogan's distinctiveness, coherence, and fluency. The results demonstrate that o
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#26679;&#31574;&#30053;&#20351;&#31574;&#30053;&#21482;&#21463;``&#22909;&#25968;&#25454;"&#38480;&#21046;&#65292;&#32780;&#19981;&#26159;&#25152;&#26377;&#30340;&#21160;&#20316;&#65292;&#25552;&#39640;&#20102;&#31163;&#32447;RL&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.04413</link><description>&lt;p&gt;
&#36229;&#36234;&#22343;&#21248;&#37319;&#26679;&#65306;&#20351;&#29992;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Beyond Uniform Sampling: Offline Reinforcement Learning with Imbalanced Datasets. (arXiv:2310.04413v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04413
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#26679;&#31574;&#30053;&#20351;&#31574;&#30053;&#21482;&#21463;``&#22909;&#25968;&#25454;"&#38480;&#21046;&#65292;&#32780;&#19981;&#26159;&#25152;&#26377;&#30340;&#21160;&#20316;&#65292;&#25552;&#39640;&#20102;&#31163;&#32447;RL&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#26088;&#22312;&#21033;&#29992;&#29616;&#26377;&#30340;&#36712;&#36857;&#25968;&#25454;&#38598;&#26469;&#23398;&#20064;&#20915;&#31574;&#31574;&#30053;&#65292;&#32780;&#26080;&#38656;&#25910;&#38598;&#39069;&#22806;&#30340;&#25968;&#25454;&#12290;&#19982;&#34892;&#20026;&#20811;&#38534;&#31561;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#30456;&#27604;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#20027;&#35201;&#21160;&#26426;&#26159;&#25214;&#21040;&#19968;&#20010;&#27604;&#25968;&#25454;&#38598;&#20013;&#30340;&#36712;&#36857;&#36798;&#21040;&#26356;&#39640;&#24179;&#22343;&#25910;&#30410;&#30340;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#22312;&#32463;&#39564;&#19978;&#21457;&#29616;&#65292;&#24403;&#19968;&#20010;&#25968;&#25454;&#38598;&#34987;&#27425;&#20248;&#36712;&#36857;&#25152;&#20027;&#23548;&#26102;&#65292;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#31163;&#32447;RL&#31639;&#27861;&#22312;&#24179;&#22343;&#25910;&#30410;&#19978;&#27809;&#26377;&#26174;&#33879;&#25552;&#39640;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#26159;&#30001;&#20110;&#24403;&#21069;&#31163;&#32447;RL&#31639;&#27861;&#20551;&#35774;&#19982;&#25968;&#25454;&#38598;&#20013;&#30340;&#36712;&#36857;&#20445;&#25345;&#25509;&#36817;&#12290;&#22914;&#26524;&#25968;&#25454;&#38598;&#20027;&#35201;&#30001;&#27425;&#20248;&#36712;&#36857;&#32452;&#25104;&#65292;&#36825;&#20010;&#20551;&#35774;&#23558;&#24378;&#21046;&#31574;&#30053;&#27169;&#20223;&#27425;&#20248;&#21160;&#20316;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#37319;&#26679;&#31574;&#30053;&#26469;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#20351;&#31574;&#30053;&#21482;&#21463;``&#22909;&#25968;&#25454;"&#38480;&#21046;&#65292;&#32780;&#19981;&#26159;&#25152;&#26377;&#30340;&#21160;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline policy learning is aimed at learning decision-making policies using existing datasets of trajectories without collecting additional data. The primary motivation for using reinforcement learning (RL) instead of supervised learning techniques such as behavior cloning is to find a policy that achieves a higher average return than the trajectories constituting the dataset. However, we empirically find that when a dataset is dominated by suboptimal trajectories, state-of-the-art offline RL algorithms do not substantially improve over the average return of trajectories in the dataset. We argue this is due to an assumption made by current offline RL algorithms of staying close to the trajectories in the dataset. If the dataset primarily consists of sub-optimal trajectories, this assumption forces the policy to mimic the suboptimal actions. We overcome this issue by proposing a sampling strategy that enables the policy to only be constrained to ``good data" rather than all actions in t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#23558;&#22810;&#31181;&#21487;&#33021;&#30340;&#24402;&#32435;&#35299;&#37322;&#27719;&#24635;&#20026;&#29305;&#24449;&#37325;&#35201;&#24615;&#20998;&#25968;&#30340;&#19977;&#31181;&#32858;&#21512;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#23384;&#22312;&#22810;&#20010;&#26377;&#25928;&#30340;&#24402;&#32435;&#35299;&#37322;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.03131</link><description>&lt;p&gt;
Axiomatic Aggregations of Abductive Explanations. (arXiv:2310.03131v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
Axiomatic Aggregations of Abductive Explanations. (arXiv:2310.03131v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03131
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#23558;&#22810;&#31181;&#21487;&#33021;&#30340;&#24402;&#32435;&#35299;&#37322;&#27719;&#24635;&#20026;&#29305;&#24449;&#37325;&#35201;&#24615;&#20998;&#25968;&#30340;&#19977;&#31181;&#32858;&#21512;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#23384;&#22312;&#22810;&#20010;&#26377;&#25928;&#30340;&#24402;&#32435;&#35299;&#37322;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;&#21518;&#39564;&#27169;&#22411;&#36817;&#20284;&#35299;&#37322;&#26041;&#27861;&#65288;&#22914;LIME&#21644;SHAP&#65289;&#20581;&#22766;&#24615;&#30340;&#25209;&#35780;&#23548;&#33268;&#20102;&#27169;&#22411;&#31934;&#30830;&#38416;&#37322;&#30340;&#20852;&#36215;&#12290;&#38024;&#23545;&#27599;&#20010;&#25968;&#25454;&#28857;&#65292;&#24402;&#32435;&#35299;&#37322;&#25552;&#20379;&#20102;&#33021;&#22815;&#29983;&#25104;&#32467;&#26524;&#30340;&#26368;&#23567;&#29305;&#24449;&#23376;&#38598;&#12290;&#23613;&#31649;&#22312;&#29702;&#35770;&#19978;&#26159;&#21487;&#38752;&#19988;&#20005;&#35880;&#30340;&#65292;&#20294;&#26159;&#24402;&#32435;&#35299;&#37322;&#23384;&#22312;&#19968;&#20010;&#20027;&#35201;&#38382;&#39064; - &#23545;&#20110;&#21516;&#19968;&#25968;&#25454;&#28857;&#21487;&#33021;&#23384;&#22312;&#22810;&#20010;&#26377;&#25928;&#30340;&#24402;&#32435;&#35299;&#37322;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25552;&#20379;&#21333;&#19968;&#30340;&#24402;&#32435;&#35299;&#37322;&#21487;&#33021;&#26159;&#19981;&#36275;&#22815;&#30340;&#65307;&#21478;&#19968;&#26041;&#38754;&#65292;&#25552;&#20379;&#25152;&#26377;&#26377;&#25928;&#30340;&#24402;&#32435;&#35299;&#37322;&#21487;&#33021;&#30001;&#20110;&#20854;&#25968;&#37327;&#24222;&#22823;&#32780;&#38590;&#20197;&#29702;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#22810;&#31181;&#21487;&#33021;&#30340;&#24402;&#32435;&#35299;&#37322;&#27719;&#24635;&#20026;&#29305;&#24449;&#37325;&#35201;&#24615;&#20998;&#25968;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#32858;&#21512;&#26041;&#27861;&#65306;&#20004;&#31181;&#22522;&#20110;&#21512;&#20316;&#21338;&#24328;&#35770;&#30340;&#26435;&#21147;&#25351;&#25968;&#21644;&#19968;&#31181;&#22522;&#20110;&#33879;&#21517;&#30340;&#22240;&#26524;&#24378;&#24230;&#24230;&#37327;&#12290;&#25105;&#20204;&#36890;&#36807;&#20844;&#29702;&#21270;&#34920;&#24449;&#36825;&#19977;&#31181;&#26041;&#27861;&#65292;&#35777;&#26126;&#23427;&#20204;&#27599;&#19968;&#20010;&#37117;&#20855;&#26377;
&lt;/p&gt;
&lt;p&gt;
The recent criticisms of the robustness of post hoc model approximation explanation methods (like LIME and SHAP) have led to the rise of model-precise abductive explanations. For each data point, abductive explanations provide a minimal subset of features that are sufficient to generate the outcome. While theoretically sound and rigorous, abductive explanations suffer from a major issue -- there can be several valid abductive explanations for the same data point. In such cases, providing a single abductive explanation can be insufficient; on the other hand, providing all valid abductive explanations can be incomprehensible due to their size. In this work, we solve this issue by aggregating the many possible abductive explanations into feature importance scores. We propose three aggregation methods: two based on power indices from cooperative game theory and a third based on a well-known measure of causal strength. We characterize these three methods axiomatically, showing that each of 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33033;&#20914;&#32047;&#31215;&#36716;&#21457;&#65288;SAF&#65289;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#35757;&#32451;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#12290;SAF&#19981;&#20165;&#21487;&#20197;&#20943;&#23569;&#21069;&#21521;&#36807;&#31243;&#20013;&#30340;&#25805;&#20316;&#27425;&#25968;&#65292;&#19982;Spike Representation&#21644;OTTT&#20445;&#25345;&#19968;&#33268;&#65292;&#32780;&#19988;&#21487;&#20197;&#35299;&#20915;SNNs&#35757;&#32451;&#20013;&#30340;&#38590;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.02772</link><description>&lt;p&gt;
&#29992;&#20110;&#26377;&#25928;&#35757;&#32451;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#33033;&#20914;&#32047;&#31215;&#36716;&#21457;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Spike Accumulation Forwarding for Effective Training of Spiking Neural Networks. (arXiv:2310.02772v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02772
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33033;&#20914;&#32047;&#31215;&#36716;&#21457;&#65288;SAF&#65289;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#35757;&#32451;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#12290;SAF&#19981;&#20165;&#21487;&#20197;&#20943;&#23569;&#21069;&#21521;&#36807;&#31243;&#20013;&#30340;&#25805;&#20316;&#27425;&#25968;&#65292;&#19982;Spike Representation&#21644;OTTT&#20445;&#25345;&#19968;&#33268;&#65292;&#32780;&#19988;&#21487;&#20197;&#35299;&#20915;SNNs&#35757;&#32451;&#20013;&#30340;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#35757;&#32451;&#33539;&#24335;&#65292;&#21363;&#33033;&#20914;&#32047;&#31215;&#36716;&#21457;&#65288;SAF&#65289;&#12290;&#24050;&#30693;SNNs&#20855;&#26377;&#39640;&#33021;&#25928;&#20294;&#38590;&#20197;&#35757;&#32451;&#30340;&#29305;&#28857;&#12290;&#35768;&#22810;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20854;&#20013;&#26102;&#38388;&#19978;&#30340;&#22312;&#32447;&#35757;&#32451;&#65288;OTTT&#65289;&#26159;&#19968;&#31181;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#25233;&#21046;&#20869;&#23384;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#22312;GPU&#19978;&#39640;&#25928;&#35745;&#31639;&#65292;OTTT&#38656;&#35201;&#36827;&#34892;&#33033;&#20914;&#24207;&#21015;&#25805;&#20316;&#21644;&#33033;&#20914;&#24207;&#21015;&#21152;&#26435;&#27714;&#21644;&#25805;&#20316;&#12290;&#27492;&#22806;&#65292;OTTT&#19982;Spike Representation&#65288;&#21478;&#19968;&#31181;&#35757;&#32451;&#26041;&#27861;&#65289;&#20043;&#38388;&#23384;&#22312;&#20851;&#32852;&#65292;&#20294;&#19982;Spike Representation&#30340;&#29702;&#35770;&#19968;&#33268;&#24615;&#23578;&#26410;&#24471;&#21040;&#35777;&#26126;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#21363;SAF&#21487;&#20197;&#22312;&#21069;&#21521;&#36807;&#31243;&#20013;&#20943;&#23569;&#19968;&#21322;&#30340;&#25805;&#20316;&#27425;&#25968;&#65292;&#24182;&#19988;&#21487;&#20197;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;SAF&#20998;&#21035;&#19982;Spike Representation&#21644;OTTT&#19968;&#33268;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30830;&#35748;&#20102;......
&lt;/p&gt;
&lt;p&gt;
In this article, we propose a new paradigm for training spiking neural networks (SNNs), spike accumulation forwarding (SAF). It is known that SNNs are energy-efficient but difficult to train. Consequently, many researchers have proposed various methods to solve this problem, among which online training through time (OTTT) is a method that allows inferring at each time step while suppressing the memory cost. However, to compute efficiently on GPUs, OTTT requires operations with spike trains and weighted summation of spike trains during forwarding. In addition, OTTT has shown a relationship with the Spike Representation, an alternative training method, though theoretical agreement with Spike Representation has yet to be proven. Our proposed method can solve these problems; namely, SAF can halve the number of operations during the forward process, and it can be theoretically proven that SAF is consistent with the Spike Representation and OTTT, respectively. Furthermore, we confirmed the a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23548;&#25968;&#32422;&#26463;&#31070;&#32463;&#32593;&#32476;&#65288;DC NN&#65289;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25972;&#21512;RELU&#28608;&#27963;&#20989;&#25968;&#65288;IReLU&#65289;&#26469;&#25913;&#36827;DC NN&#30340;&#35757;&#32451;&#65292;&#24182;&#25506;&#31350;&#20102;&#21435;&#24402;&#19968;&#21270;&#21644;&#26631;&#31614;&#32553;&#25918;&#23545;&#20110;&#31283;&#23450;DC&#35757;&#32451;&#30340;&#20316;&#29992;&#12290;&#30740;&#31350;&#36824;&#35780;&#20272;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#29289;&#29702;&#30456;&#20851;&#35774;&#32622;&#20013;&#30340;&#25928;&#26524;&#65292;&#35777;&#26126;&#20102;&#20351;&#29992;IReLU&#28608;&#27963;&#20989;&#25968;&#12289;&#21435;&#24402;&#19968;&#21270;&#21644;&#26631;&#31614;&#32553;&#25918;&#30340;&#29616;&#26377;&#26550;&#26500;&#26356;&#22909;&#22320;&#34701;&#20837;&#20102;&#23548;&#25968;&#32422;&#26463;&#25552;&#20379;&#30340;&#35757;&#32451;&#20449;&#21495;&#12290;</title><link>http://arxiv.org/abs/2310.01649</link><description>&lt;p&gt;
&#20851;&#20110;&#35757;&#32451;&#23548;&#25968;&#32422;&#26463;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
On Training Derivative-Constrained Neural Networks. (arXiv:2310.01649v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01649
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23548;&#25968;&#32422;&#26463;&#31070;&#32463;&#32593;&#32476;&#65288;DC NN&#65289;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25972;&#21512;RELU&#28608;&#27963;&#20989;&#25968;&#65288;IReLU&#65289;&#26469;&#25913;&#36827;DC NN&#30340;&#35757;&#32451;&#65292;&#24182;&#25506;&#31350;&#20102;&#21435;&#24402;&#19968;&#21270;&#21644;&#26631;&#31614;&#32553;&#25918;&#23545;&#20110;&#31283;&#23450;DC&#35757;&#32451;&#30340;&#20316;&#29992;&#12290;&#30740;&#31350;&#36824;&#35780;&#20272;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#29289;&#29702;&#30456;&#20851;&#35774;&#32622;&#20013;&#30340;&#25928;&#26524;&#65292;&#35777;&#26126;&#20102;&#20351;&#29992;IReLU&#28608;&#27963;&#20989;&#25968;&#12289;&#21435;&#24402;&#19968;&#21270;&#21644;&#26631;&#31614;&#32553;&#25918;&#30340;&#29616;&#26377;&#26550;&#26500;&#26356;&#22909;&#22320;&#34701;&#20837;&#20102;&#23548;&#25968;&#32422;&#26463;&#25552;&#20379;&#30340;&#35757;&#32451;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#31070;&#32463;&#32593;&#32476;&#23545;&#36755;&#20837;&#30340;&#39044;&#27979;&#30456;&#23545;&#20110;&#36755;&#20837;&#30340;&#65288;&#37096;&#20998;&#65289;&#23548;&#25968;&#20316;&#20026;&#39069;&#22806;&#30340;&#35757;&#32451;&#20449;&#21495;&#30340;&#24773;&#20917;&#31216;&#20043;&#20026;&#23548;&#25968;&#32422;&#26463;&#31070;&#32463;&#32593;&#32476;&#65288;DC NN&#65289;&#12290;&#36825;&#31181;&#24773;&#20917;&#22312;&#33258;&#28982;&#31185;&#23398;&#20013;&#30340;&#29289;&#29702;&#30456;&#20851;&#35774;&#32622;&#20013;&#24456;&#24120;&#35265;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25972;&#21512;RELU (IReLU)&#28608;&#27963;&#20989;&#25968;&#65292;&#20197;&#25913;&#36827;DC NN&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#21435;&#24402;&#19968;&#21270;&#21644;&#26631;&#31614;&#32553;&#25918;&#20197;&#24110;&#21161;&#31283;&#23450;DC&#35757;&#32451;&#12290;&#25105;&#20204;&#22312;&#21253;&#25324;&#37327;&#23376;&#21270;&#23398;&#21644;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#65288;SciML&#65289;&#20219;&#21153;&#22312;&#20869;&#30340;&#29289;&#29702;&#30456;&#20851;&#35774;&#32622;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20351;&#29992;IReLU&#28608;&#27963;&#20989;&#25968;&#32467;&#21512;&#21435;&#24402;&#19968;&#21270;&#21644;&#26631;&#31614;&#32553;&#25918;&#30340;&#29616;&#26377;&#26550;&#26500;&#26356;&#22909;&#22320;&#34701;&#20837;&#20102;&#23548;&#25968;&#32422;&#26463;&#25552;&#20379;&#30340;&#35757;&#32451;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;
We refer to the setting where the (partial) derivatives of a neural network's (NN's) predictions with respect to its inputs are used as additional training signal as a derivative-constrained (DC) NN. This situation is common in physics-informed settings in the natural sciences. We propose an integrated RELU (IReLU) activation function to improve training of DC NNs. We also investigate denormalization and label rescaling to help stabilize DC training. We evaluate our methods on physics-informed settings including quantum chemistry and Scientific Machine Learning (SciML) tasks. We demonstrate that existing architectures with IReLU activations combined with denormalization and label rescaling better incorporate training signal provided by derivative constraints.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#22312;&#28389;&#29992;&#65292;&#21628;&#21505;&#35748;&#35782;&#21040;&#36825;&#20123;&#25361;&#25112;&#30340;&#32039;&#36843;&#24615;&#12290;&#30740;&#31350;&#25581;&#31034;&#20102;&#36825;&#20123;&#25216;&#26415;&#22312;&#28145;&#24230;&#20266;&#36896;&#12289;&#21512;&#25104;&#36523;&#20221;&#24694;&#24847;&#27963;&#21160;&#20197;&#21450;&#34394;&#20551;&#20449;&#24687;&#21644;&#27450;&#35784;&#26041;&#38754;&#21487;&#33021;&#24102;&#26469;&#30340;&#31038;&#20250;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.00737</link><description>&lt;p&gt;
&#12298;GenAI&#23545;&#25239;&#20154;&#24615;&#65306;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#37034;&#24694;&#24212;&#29992;&#12299;
&lt;/p&gt;
&lt;p&gt;
GenAI Against Humanity: Nefarious Applications of Generative Artificial Intelligence and Large Language Models. (arXiv:2310.00737v2 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00737
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#22312;&#28389;&#29992;&#65292;&#21628;&#21505;&#35748;&#35782;&#21040;&#36825;&#20123;&#25361;&#25112;&#30340;&#32039;&#36843;&#24615;&#12290;&#30740;&#31350;&#25581;&#31034;&#20102;&#36825;&#20123;&#25216;&#26415;&#22312;&#28145;&#24230;&#20266;&#36896;&#12289;&#21512;&#25104;&#36523;&#20221;&#24694;&#24847;&#27963;&#21160;&#20197;&#21450;&#34394;&#20551;&#20449;&#24687;&#21644;&#27450;&#35784;&#26041;&#38754;&#21487;&#33021;&#24102;&#26469;&#30340;&#31038;&#20250;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26159;&#25216;&#26415;&#30340;&#22855;&#36857;&#65292;&#20197;&#20854;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#22810;&#27169;&#24335;&#20869;&#23481;&#29983;&#25104;&#26041;&#38754;&#30340;&#21331;&#36234;&#33021;&#21147;&#32780;&#21463;&#21040;&#36190;&#25196;&#65292;&#23427;&#20204;&#25215;&#35834;&#24102;&#26469;&#19968;&#20010;&#21464;&#38761;&#30340;&#26410;&#26469;&#12290;&#20294;&#23601;&#20687;&#25152;&#26377;&#24378;&#22823;&#30340;&#24037;&#20855;&#19968;&#26679;&#65292;&#23427;&#20204;&#20063;&#26377;&#20854;&#38452;&#24433;&#23384;&#22312;&#12290;&#24819;&#35937;&#19968;&#19979;&#29983;&#27963;&#22312;&#19968;&#20010;&#28145;&#24230;&#20266;&#36896;&#19982;&#29616;&#23454;&#26080;&#27861;&#21306;&#20998;&#12289;&#21512;&#25104;&#36523;&#20221;&#32452;&#32455;&#24694;&#24847;&#27963;&#21160;&#12289;&#20197;&#21450;&#26377;&#30528;&#26080;&#19982;&#20262;&#27604;&#31934;&#30830;&#24230;&#30340;&#26377;&#38024;&#23545;&#24615;&#30340;&#34394;&#20551;&#20449;&#24687;&#25110;&#27450;&#35784;&#25163;&#27861;&#30340;&#19990;&#30028;&#12290;&#27426;&#36814;&#26469;&#21040;GenAI&#24212;&#29992;&#30340;&#40657;&#26263;&#38754;&#12290;&#26412;&#25991;&#19981;&#20165;&#26159;&#25506;&#32034;GenAI&#21644;LLMs&#28508;&#22312;&#28389;&#29992;&#30340;&#26053;&#31243;&#65292;&#20063;&#26159;&#21628;&#21505;&#35748;&#35782;&#21040;&#38754;&#20020;&#30340;&#25361;&#25112;&#30340;&#32039;&#36843;&#24615;&#12290;&#22312;&#25105;&#20204;&#33322;&#34892;&#20110;&#34394;&#20551;&#20449;&#24687;&#27963;&#21160;&#12289;&#24694;&#24847;&#20869;&#23481;&#29983;&#25104;&#19982;&#31934;&#23494;&#24694;&#24847;&#36719;&#20214;&#26500;&#24314;&#30340;&#28023;&#27915;&#20013;&#65292;&#25105;&#20204;&#23558;&#25581;&#31034;&#36825;&#22330;&#25105;&#20204;&#27491;&#22312;&#35265;&#35777;&#30340;GenAI&#38761;&#21629;&#20013;&#30340;&#31038;&#20250;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Artificial Intelligence (GenAI) and Large Language Models (LLMs) are marvels of technology; celebrated for their prowess in natural language processing and multimodal content generation, they promise a transformative future. But as with all powerful tools, they come with their shadows. Picture living in a world where deepfakes are indistinguishable from reality, where synthetic identities orchestrate malicious campaigns, and where targeted misinformation or scams are crafted with unparalleled precision. Welcome to the darker side of GenAI applications. This article is not just a journey through the meanders of potential misuse of GenAI and LLMs, but also a call to recognize the urgency of the challenges ahead. As we navigate the seas of misinformation campaigns, malicious content generation, and the eerie creation of sophisticated malware, we'll uncover the societal implications that ripple through the GenAI revolution we are witnessing. From AI-powered botnets on social med
&lt;/p&gt;</description></item><item><title>LEGO-Prover&#26159;&#19968;&#20010;&#20351;&#29992;&#19981;&#26029;&#22686;&#38271;&#30340;&#25216;&#33021;&#24211;&#30340;&#31070;&#32463;&#23450;&#29702;&#35777;&#26126;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21033;&#29992;&#29616;&#26377;&#25216;&#33021;&#21644;&#21019;&#36896;&#26032;&#25216;&#33021;&#26469;&#35777;&#26126;&#23450;&#29702;&#12290;</title><link>http://arxiv.org/abs/2310.00656</link><description>&lt;p&gt;
LEGO-Prover: &#20351;&#29992;&#19981;&#26029;&#22686;&#38271;&#30340;&#24211;&#36827;&#34892;&#31070;&#32463;&#23450;&#29702;&#35777;&#26126;
&lt;/p&gt;
&lt;p&gt;
LEGO-Prover: Neural Theorem Proving with Growing Libraries. (arXiv:2310.00656v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00656
&lt;/p&gt;
&lt;p&gt;
LEGO-Prover&#26159;&#19968;&#20010;&#20351;&#29992;&#19981;&#26029;&#22686;&#38271;&#30340;&#25216;&#33021;&#24211;&#30340;&#31070;&#32463;&#23450;&#29702;&#35777;&#26126;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21033;&#29992;&#29616;&#26377;&#25216;&#33021;&#21644;&#21019;&#36896;&#26032;&#25216;&#33021;&#26469;&#35777;&#26126;&#23450;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23450;&#29702;&#35777;&#26126;&#20173;&#28982;&#26159;&#26368;&#38590;&#30340;&#25512;&#29702;&#20219;&#21153;&#20043;&#19968;&#65292;&#23578;&#26410;&#23436;&#20840;&#35299;&#20915;&#12290;&#20197;&#24448;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#20173;&#38590;&#20197;&#35777;&#26126;&#20013;&#23398;&#27700;&#24179;&#30340;&#23450;&#29702;&#12290;&#36825;&#20123;&#26041;&#27861;&#30340;&#19968;&#20010;&#24120;&#35265;&#38480;&#21046;&#26159;&#22312;&#25972;&#20010;&#23450;&#29702;&#35777;&#26126;&#36807;&#31243;&#20013;&#20551;&#35774;&#20102;&#19968;&#20010;&#22266;&#23450;&#30340;&#23450;&#29702;&#24211;&#12290;&#28982;&#32780;&#65292;&#20247;&#25152;&#21608;&#30693;&#65292;&#21019;&#36896;&#26032;&#30340;&#26377;&#29992;&#23450;&#29702;&#29978;&#33267;&#26032;&#30340;&#29702;&#35770;&#19981;&#20165;&#26377;&#24110;&#21161;&#32780;&#19988;&#23545;&#20110;&#25512;&#21160;&#25968;&#23398;&#21457;&#23637;&#21644;&#35777;&#26126;&#26356;&#22256;&#38590;&#21644;&#28145;&#20837;&#30340;&#32467;&#26524;&#26159;&#33267;&#20851;&#37325;&#35201;&#21644;&#24517;&#35201;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;LEGO-Prover&#65292;&#23427;&#37319;&#29992;&#19968;&#20010;&#21253;&#21547;&#32463;&#36807;&#39564;&#35777;&#30340;&#24341;&#29702;&#30340;&#19981;&#26029;&#22686;&#38271;&#30340;&#25216;&#33021;&#24211;&#65292;&#20197;&#22686;&#24378;&#29992;&#20110;&#23450;&#29702;&#35777;&#26126;&#30340;LLMs&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#27169;&#22359;&#21270;&#26500;&#24314;&#35777;&#26126;&#65292;LEGO-Prover&#20351;LLMs&#33021;&#22815;&#21033;&#29992;&#20174;&#24211;&#26816;&#32034;&#21040;&#30340;&#29616;&#26377;&#25216;&#33021;&#65292;&#20197;&#21450;&#22312;&#35777;&#26126;&#36807;&#31243;&#20013;&#21019;&#24314;&#26032;&#25216;&#33021;&#12290;&#36825;&#20123;&#25216;&#33021;&#36890;&#36807;&#31616;&#21270;&#35777;&#26126;&#36807;&#31243;&#24182;&#25552;&#20379;&#26356;&#24378;&#22823;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the success of large language models (LLMs), the task of theorem proving still remains one of the hardest reasoning tasks that is far from being fully solved. Prior methods using language models have demonstrated promising results, but they still struggle to prove even middle school level theorems. One common limitation of these methods is that they assume a fixed theorem library during the whole theorem proving process. However, as we all know, creating new useful theorems or even new theories is not only helpful but crucial and necessary for advancing mathematics and proving harder and deeper results. In this work, we present LEGO-Prover, which employs a growing skill library containing verified lemmas as skills to augment the capability of LLMs used in theorem proving. By constructing the proof modularly, LEGO-Prover enables LLMs to utilize existing skills retrieved from the library and to create new skills during the proving process. These skills are further evolved (by pro
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#22320;&#29702;&#31354;&#38388;&#20154;&#24037;&#26234;&#33021;&#22522;&#30784;&#27169;&#22411;&#30340;&#38544;&#31169;&#21644;&#23433;&#20840;&#39118;&#38505;&#65292;&#24182;&#25552;&#20986;&#20102;&#39044;&#38450;&#21644;&#25511;&#21046;&#31574;&#30053;&#34013;&#22270;&#12290;</title><link>http://arxiv.org/abs/2309.17319</link><description>&lt;p&gt;
&#26500;&#24314;&#20445;&#25252;&#38544;&#31169;&#21644;&#23433;&#20840;&#30340;&#22320;&#29702;&#31354;&#38388;&#20154;&#24037;&#26234;&#33021;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Building Privacy-Preserving and Secure Geospatial Artificial Intelligence Foundation Models. (arXiv:2309.17319v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17319
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#22320;&#29702;&#31354;&#38388;&#20154;&#24037;&#26234;&#33021;&#22522;&#30784;&#27169;&#22411;&#30340;&#38544;&#31169;&#21644;&#23433;&#20840;&#39118;&#38505;&#65292;&#24182;&#25552;&#20986;&#20102;&#39044;&#38450;&#21644;&#25511;&#21046;&#31574;&#30053;&#34013;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#25105;&#20204;&#22312;&#20154;&#24037;&#26234;&#33021;&#30340;&#22522;&#30784;&#27169;&#22411;&#20013;&#21462;&#24471;&#20102;&#23454;&#36136;&#24615;&#30340;&#36827;&#23637;&#65292;&#21253;&#25324;&#35821;&#35328;&#12289;&#35270;&#35273;&#21644;&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#31361;&#20986;&#20102;&#22312;&#22320;&#29702;&#31354;&#38388;&#20154;&#24037;&#26234;&#33021;&#20013;&#20351;&#29992;&#22522;&#30784;&#27169;&#22411;&#30340;&#28508;&#21147;&#65292;&#31216;&#20026;GeoAI&#22522;&#30784;&#27169;&#22411;&#25110;Geo-Foundation&#27169;&#22411;&#65292;&#29992;&#20110;&#22320;&#29702;&#38382;&#39064;&#22238;&#31572;&#12289;&#36965;&#24863;&#22270;&#20687;&#29702;&#35299;&#12289;&#22320;&#22270;&#29983;&#25104;&#21644;&#22522;&#20110;&#20301;&#32622;&#30340;&#26381;&#21153;&#31561;&#12290;&#28982;&#32780;&#65292;GeoAI&#22522;&#30784;&#27169;&#22411;&#30340;&#24320;&#21457;&#21644;&#24212;&#29992;&#21487;&#33021;&#24102;&#26469;&#20005;&#37325;&#30340;&#38544;&#31169;&#21644;&#23433;&#20840;&#39118;&#38505;&#65292;&#36825;&#20123;&#39118;&#38505;&#21040;&#30446;&#21069;&#20026;&#27490;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#30340;&#35752;&#35770;&#25110;&#35299;&#20915;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;GeoAI&#22522;&#30784;&#27169;&#22411;&#22312;&#25972;&#20010;&#29983;&#21629;&#21608;&#26399;&#20013;&#30340;&#28508;&#22312;&#38544;&#31169;&#21644;&#23433;&#20840;&#39118;&#38505;&#65292;&#24182;&#25552;&#20986;&#20102;&#20840;&#38754;&#30340;&#39044;&#38450;&#21644;&#25511;&#21046;&#31574;&#30053;&#34013;&#22270;&#12290;&#36890;&#36807;&#36825;&#31687;&#23637;&#26395;&#24615;&#35770;&#25991;&#65292;&#25105;&#20204;&#24076;&#26395;&#24341;&#36215;&#22320;&#29702;&#31354;&#38388;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#21644;&#20915;&#31574;&#32773;&#23545;GeoAI&#22522;&#30784;&#27169;&#22411;&#20013;&#22266;&#26377;&#30340;&#38544;&#31169;&#21644;&#23433;&#20840;&#39118;&#38505;&#30340;&#20851;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years we have seen substantial advances in foundation models for artificial intelligence, including language, vision, and multimodal models. Recent studies have highlighted the potential of using foundation models in geospatial artificial intelligence, known as GeoAI Foundation Models or Geo-Foundation Models, for geographic question answering, remote sensing image understanding, map generation, and location-based services, among others. However, the development and application of GeoAI foundation models can pose serious privacy and security risks, which have not been fully discussed or addressed to date. This paper introduces the potential privacy and security risks throughout the lifecycle of GeoAI foundation models and proposes a comprehensive blueprint for preventative and control strategies. Through this vision paper, we hope to draw the attention of researchers and policymakers in geospatial domains to these privacy and security risks inherent in GeoAI foundation models
&lt;/p&gt;</description></item><item><title>PRiSM&#26159;&#19968;&#31181;&#22686;&#24378;&#20302;&#36164;&#28304;&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20851;&#31995;&#24863;&#30693;&#20998;&#25968;&#26657;&#20934;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#25104;&#21151;&#22320;&#38477;&#20302;&#20102;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#35757;&#32451;&#27169;&#22411;&#26102;&#30340;&#26657;&#20934;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2309.13869</link><description>&lt;p&gt;
PRiSM: &#20351;&#29992;&#20851;&#31995;&#24863;&#30693;&#20998;&#25968;&#26657;&#20934;&#22686;&#24378;&#20302;&#36164;&#28304;&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
PRiSM: Enhancing Low-Resource Document-Level Relation Extraction with Relation-Aware Score Calibration. (arXiv:2309.13869v1 [cs.CL] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13869
&lt;/p&gt;
&lt;p&gt;
PRiSM&#26159;&#19968;&#31181;&#22686;&#24378;&#20302;&#36164;&#28304;&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20851;&#31995;&#24863;&#30693;&#20998;&#25968;&#26657;&#20934;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#25104;&#21151;&#22320;&#38477;&#20302;&#20102;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#35757;&#32451;&#27169;&#22411;&#26102;&#30340;&#26657;&#20934;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;&#65288;DocRE&#65289;&#26088;&#22312;&#25552;&#21462;&#25991;&#26723;&#20013;&#25152;&#26377;&#23454;&#20307;&#23545;&#30340;&#20851;&#31995;&#12290;&#22312;DocRE&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#27880;&#37322;&#36825;&#31867;&#25968;&#25454;&#30340;&#25104;&#26412;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#21147;&#25237;&#20837;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#30340;DocRE&#24773;&#20917;&#65292;&#24182;&#21457;&#29616;&#29616;&#26377;&#30340;&#22312;&#23569;&#37327;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#36807;&#39640;&#20272;&#35745;&#20102;NA&#65288;"no relation"&#65289;&#26631;&#31614;&#65292;&#23548;&#33268;&#24615;&#33021;&#21463;&#38480;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20174;&#26657;&#20934;&#30340;&#35282;&#24230;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;PRiSM&#65292;&#23427;&#21487;&#20197;&#26681;&#25454;&#20851;&#31995;&#35821;&#20041;&#20449;&#24687;&#26469;&#36866;&#24212;logits&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;DocRE&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#23558;&#29616;&#26377;&#27169;&#22411;&#19982;PRiSM&#38598;&#25104;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;F1&#20998;&#25968;&#25552;&#39640;&#20102;26.38%&#65292;&#32780;&#24403;&#29992;&#32422;3%&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#26657;&#20934;&#35823;&#24046;&#19979;&#38477;&#20102;36&#20493;&#12290;&#20195;&#30721;&#21487;&#20197;&#22312;https://github.com/brightjade/PRiSM&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Document-level relation extraction (DocRE) aims to extract relations of all entity pairs in a document. A key challenge in DocRE is the cost of annotating such data which requires intensive human effort. Thus, we investigate the case of DocRE in a low-resource setting, and we find that existing models trained on low data overestimate the NA ("no relation") label, causing limited performance. In this work, we approach the problem from a calibration perspective and propose PRiSM, which learns to adapt logits based on relation semantic information. We evaluate our method on three DocRE datasets and demonstrate that integrating existing models with PRiSM improves performance by as much as 26.38 F1 score, while the calibration error drops as much as 36 times when trained with about 3% of data. The code is publicly available at https://github.com/brightjade/PRiSM.
&lt;/p&gt;</description></item><item><title>MINT&#26159;&#19968;&#20010;&#35780;&#20272;LLMs&#22312;&#22810;&#36718;&#20132;&#20114;&#20013;&#35299;&#20915;&#20219;&#21153;&#33021;&#21147;&#30340;&#22522;&#20934;&#65292;&#36890;&#36807;&#20351;&#29992;&#24037;&#20855;&#21644;&#21033;&#29992;&#29992;&#25143;&#30340;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#12290;&#23427;&#35299;&#20915;&#20102;&#24403;&#21069;&#35780;&#20272;&#21327;&#35758;&#24573;&#30053;&#32454;&#33268;&#20114;&#21160;&#21644;&#20302;&#20272;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#30340;&#38382;&#39064;&#65292;&#20419;&#36827;&#20102;&#30740;&#31350;&#22522;&#20934;&#35780;&#20272;&#21644;&#23454;&#38469;&#24212;&#29992;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.10691</link><description>&lt;p&gt;
MINT: &#35780;&#20272;&#22312;&#19982;&#24037;&#20855;&#21644;&#35821;&#35328;&#21453;&#39304;&#36827;&#34892;&#22810;&#36718;&#20132;&#20114;&#20013;&#30340;LLMs&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
MINT: Evaluating LLMs in Multi-turn Interaction with Tools and Language Feedback. (arXiv:2309.10691v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10691
&lt;/p&gt;
&lt;p&gt;
MINT&#26159;&#19968;&#20010;&#35780;&#20272;LLMs&#22312;&#22810;&#36718;&#20132;&#20114;&#20013;&#35299;&#20915;&#20219;&#21153;&#33021;&#21147;&#30340;&#22522;&#20934;&#65292;&#36890;&#36807;&#20351;&#29992;&#24037;&#20855;&#21644;&#21033;&#29992;&#29992;&#25143;&#30340;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#12290;&#23427;&#35299;&#20915;&#20102;&#24403;&#21069;&#35780;&#20272;&#21327;&#35758;&#24573;&#30053;&#32454;&#33268;&#20114;&#21160;&#21644;&#20302;&#20272;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#30340;&#38382;&#39064;&#65292;&#20419;&#36827;&#20102;&#30740;&#31350;&#22522;&#20934;&#35780;&#20272;&#21644;&#23454;&#38469;&#24212;&#29992;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#65292;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#24120;&#38656;&#35201;&#19982;&#29992;&#25143;&#36827;&#34892;&#22810;&#36718;&#20132;&#20114;&#65292;&#26377;&#26102;&#20505;&#36741;&#20197;&#22806;&#37096;&#24037;&#20855;&#30340;&#24110;&#21161;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#35780;&#20272;&#21327;&#35758;&#24120;&#24120;&#24378;&#35843;&#29992;&#21333;&#36718;&#20132;&#27969;&#30340;&#22522;&#20934;&#24615;&#33021;&#65292;&#24573;&#30053;&#20102;&#29992;&#25143;&#12289;LLMs&#21644;&#22806;&#37096;&#24037;&#20855;&#20043;&#38388;&#30340;&#32454;&#33268;&#20114;&#21160;&#65292;&#24182;&#20302;&#20272;&#20102;&#29992;&#25143;&#30340;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#30340;&#37325;&#35201;&#24615;&#12290;&#36825;&#20123;&#30095;&#24573;&#23548;&#33268;&#20102;&#30740;&#31350;&#22522;&#20934;&#35780;&#20272;&#32467;&#26524;&#19982;&#23454;&#38469;&#24212;&#29992;&#24773;&#20917;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;MINT&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#36807;&#20351;&#29992;&#24037;&#20855;&#21644;&#21033;&#29992;&#29992;&#25143;&#30340;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#26469;&#35780;&#20272;LLMs&#35299;&#20915;&#22810;&#36718;&#20132;&#20114;&#20219;&#21153;&#33021;&#21147;&#30340;&#22522;&#20934;&#12290;&#20026;&#20102;&#20445;&#35777;&#21487;&#37325;&#22797;&#24615;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#35780;&#20272;&#26694;&#26550;&#65292;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;LLMs&#21487;&#20197;&#36890;&#36807;&#25191;&#34892;Python&#20195;&#30721;&#26469;&#35775;&#38382;&#24037;&#20855;&#65292;&#24182;&#25509;&#25910;&#30001;GPT-4&#27169;&#25311;&#30340;&#29992;&#25143;&#30340;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#12290;&#25105;&#20204;&#37325;&#26032;&#21033;&#29992;&#20102;&#19968;&#31995;&#21015;&#22810;&#26679;&#30340;&#24050;&#24314;&#31435;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#37325;&#28857;&#20851;&#27880;&#25512;&#29702;&#12289;&#32534;&#30721;&#21644;&#20915;&#31574;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
To solve complex tasks, large language models (LLMs) often require multiple rounds of interactions with the user, sometimes assisted by external tools. However, current evaluation protocols often emphasize benchmark performance with single-turn exchanges, neglecting the nuanced interactions among the user, LLMs, and external tools, while also underestimating the importance of natural language feedback from users. These oversights contribute to discrepancies between research benchmark evaluations and real-world use cases. We introduce MINT, a benchmark that evaluates LLMs' ability to solve tasks with multi-turn interactions by (1) using tools and (2) leveraging natural language feedback. To ensure reproducibility, we provide an evaluation framework where LLMs can access tools by executing Python code and receive users' natural language feedback simulated by GPT-4. We repurpose a diverse set of established evaluation datasets focusing on reasoning, coding, and decision-making and careful
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#35299;&#20915;&#27491;&#21017;&#31232;&#30095;&#36923;&#36753;&#22238;&#24402;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;$\ell_1$&#27491;&#21017;&#21270;&#31232;&#30095;&#36923;&#36753;&#22238;&#24402;&#21644;&#19968;&#20123;&#28385;&#36275;&#20808;&#20915;&#26465;&#20214;&#30340;&#38750;&#20984;&#24809;&#32602;&#27491;&#21017;&#21270;&#31232;&#30095;&#36923;&#36753;&#22238;&#24402;&#12290;&#32463;&#39564;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#20123;&#31639;&#27861;&#33021;&#22815;&#20197;&#36739;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#26377;&#25928;&#22320;&#36827;&#34892;&#20998;&#31867;&#21644;&#29305;&#24449;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2309.05925</link><description>&lt;p&gt;
&#20851;&#20110;&#27491;&#21017;&#31232;&#30095;&#36923;&#36753;&#22238;&#24402;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Regularized Sparse Logistic Regression. (arXiv:2309.05925v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05925
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#35299;&#20915;&#27491;&#21017;&#31232;&#30095;&#36923;&#36753;&#22238;&#24402;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;$\ell_1$&#27491;&#21017;&#21270;&#31232;&#30095;&#36923;&#36753;&#22238;&#24402;&#21644;&#19968;&#20123;&#28385;&#36275;&#20808;&#20915;&#26465;&#20214;&#30340;&#38750;&#20984;&#24809;&#32602;&#27491;&#21017;&#21270;&#31232;&#30095;&#36923;&#36753;&#22238;&#24402;&#12290;&#32463;&#39564;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#20123;&#31639;&#27861;&#33021;&#22815;&#20197;&#36739;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#26377;&#25928;&#22320;&#36827;&#34892;&#20998;&#31867;&#21644;&#29305;&#24449;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#36923;&#36753;&#22238;&#24402;&#26088;&#22312;&#21516;&#26102;&#36827;&#34892;&#39640;&#32500;&#25968;&#25454;&#30340;&#20998;&#31867;&#21644;&#29305;&#24449;&#36873;&#25321;&#12290;&#34429;&#28982;&#26377;&#35768;&#22810;&#30740;&#31350;&#35299;&#20915;&#20102;$\ell_1$&#27491;&#21017;&#21270;&#36923;&#36753;&#22238;&#24402;&#38382;&#39064;&#65292;&#20294;&#23545;&#20110;&#19982;&#38750;&#20984;&#24809;&#32602;&#30456;&#20851;&#30340;&#31232;&#30095;&#36923;&#36753;&#22238;&#24402;&#35299;&#20915;&#26041;&#26696;&#24182;&#27809;&#26377;&#31561;&#37327;&#30340;&#25991;&#29486;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#35299;&#20915;$\ell_1$&#27491;&#21017;&#21270;&#31232;&#30095;&#36923;&#36753;&#22238;&#24402;&#21644;&#19968;&#20123;&#28385;&#36275;&#19968;&#23450;&#20808;&#20915;&#26465;&#20214;&#30340;&#38750;&#20984;&#24809;&#32602;&#27491;&#21017;&#21270;&#31232;&#30095;&#36923;&#36753;&#22238;&#24402;&#30340;&#26041;&#27861;&#65292;&#24182;&#37319;&#29992;&#31867;&#20284;&#30340;&#20248;&#21270;&#26694;&#26550;&#12290;&#22312;&#25552;&#20986;&#30340;&#20248;&#21270;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#19981;&#21516;&#30340;&#32447;&#25628;&#32034;&#20934;&#21017;&#26469;&#20445;&#35777;&#19981;&#21516;&#27491;&#21017;&#21270;&#39033;&#30340;&#33391;&#22909;&#25910;&#25947;&#24615;&#33021;&#12290;&#36890;&#36807;&#23545;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#36827;&#34892;&#32463;&#39564;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#33021;&#22815;&#20197;&#36739;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#26377;&#25928;&#22320;&#36827;&#34892;&#20998;&#31867;&#21644;&#29305;&#24449;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sparse logistic regression aims to perform classification and feature selection simultaneously for high-dimensional data. Although many studies have been done to solve $\ell_1$-regularized logistic regression, there is no equivalently abundant literature about solving sparse logistic regression associated with nonconvex penalties. In this paper, we propose to solve $\ell_1$-regularized sparse logistic regression and some nonconvex penalties-regularized sparse logistic regression, when the nonconvex penalties satisfy some prerequisites, with similar optimization frameworks. In the proposed optimization frameworks, we utilize different line search criteria to guarantee good convergence performance for different regularization terms. Empirical experiments on binary classification tasks with real-world datasets demonstrate our proposed algorithms are capable of performing classification and feature selection effectively with a lower computational cost.
&lt;/p&gt;</description></item><item><title>DePT&#36890;&#36807;&#23558;&#36719;&#25552;&#31034;&#20998;&#35299;&#20026;&#36739;&#30701;&#30340;&#36719;&#25552;&#31034;&#21644;&#19968;&#23545;&#20302;&#31209;&#30697;&#38453;&#65292;&#24182;&#29992;&#20004;&#20010;&#19981;&#21516;&#30340;&#23398;&#20064;&#29575;&#26469;&#20248;&#21270;&#65292;&#20197;&#35299;&#20915;&#25552;&#31034;&#35843;&#25972;&#23545;&#35757;&#32451;&#21644;&#25512;&#29702;&#26102;&#38388;&#20197;&#21450;&#20869;&#23384;&#20351;&#29992;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.05173</link><description>&lt;p&gt;
DePT:&#20998;&#35299;&#25552;&#31034;&#35843;&#25972;&#20197;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning. (arXiv:2309.05173v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05173
&lt;/p&gt;
&lt;p&gt;
DePT&#36890;&#36807;&#23558;&#36719;&#25552;&#31034;&#20998;&#35299;&#20026;&#36739;&#30701;&#30340;&#36719;&#25552;&#31034;&#21644;&#19968;&#23545;&#20302;&#31209;&#30697;&#38453;&#65292;&#24182;&#29992;&#20004;&#20010;&#19981;&#21516;&#30340;&#23398;&#20064;&#29575;&#26469;&#20248;&#21270;&#65292;&#20197;&#35299;&#20915;&#25552;&#31034;&#35843;&#25972;&#23545;&#35757;&#32451;&#21644;&#25512;&#29702;&#26102;&#38388;&#20197;&#21450;&#20869;&#23384;&#20351;&#29992;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#35843;&#25972;&#65288;PT&#65289;&#26159;&#19968;&#31181;&#23558;&#21487;&#35757;&#32451;&#30340;&#23569;&#37327;&#36719;&#25552;&#31034;&#21521;&#37327;&#38468;&#21152;&#21040;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#36755;&#20837;&#20013;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#65292;&#24050;&#22312;&#21508;&#31181;&#20219;&#21153;&#21644;&#27169;&#22411;&#20013;&#26174;&#31034;&#20986;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290; &#19982;&#20854;&#20182;PEFT&#26041;&#27861;&#30456;&#27604;&#65292;PT&#30340;&#31454;&#20105;&#24615;&#33021;&#21487;&#20197;&#22312;&#21487;&#35757;&#32451;&#21442;&#25968;&#26356;&#23569;&#30340;&#24773;&#20917;&#19979;&#20445;&#25345;&#65292;&#24182;&#19988;&#38543;&#30528;&#27169;&#22411;&#35268;&#27169;&#30340;&#25193;&#22823;&#65292;&#20854;&#21442;&#25968;&#24182;&#19981;&#20250;&#26174;&#33879;&#22686;&#21152;&#12290; &#20294;&#26159;&#65292;PT&#24341;&#20837;&#20102;&#39069;&#22806;&#30340;&#36719;&#25552;&#31034;&#26631;&#35760;&#65292;&#23548;&#33268;&#36755;&#20837;&#24207;&#21015;&#21464;&#38271;&#65292;&#36825;&#23545;&#20110;Transformer&#30340;&#20108;&#27425;&#22797;&#26434;&#24230;&#32780;&#35328;&#65292;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#26102;&#38388;&#20197;&#21450;&#20869;&#23384;&#20351;&#29992;&#26041;&#38754;&#20250;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#12290; &#36825;&#23545;&#20110;&#38754;&#20020;&#22823;&#37327;&#27599;&#26085;&#26597;&#35810;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23588;&#20854;&#20196;&#20154;&#25285;&#24551;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt tuning (PT), where a small amount of trainable soft (continuous) prompt vectors is affixed to the input of language models (LM), has shown promising results across various tasks and models for parameter-efficient fine-tuning (PEFT). PT stands out from other PEFT approaches because it maintains competitive performance with fewer trainable parameters and does not drastically scale up its parameters as the model size expands. However, PT introduces additional soft prompt tokens, leading to longer input sequences, which significantly impacts training and inference time and memory usage due to the Transformer's quadratic complexity. Particularly concerning for Large Language Models (LLMs) that face heavy daily querying. To address this issue, we propose Decomposed Prompt Tuning (DePT), which decomposes the soft prompt into a shorter soft prompt and a pair of low-rank matrices that are then optimised with two different learning rates. This allows DePT to achieve better performance whi
&lt;/p&gt;</description></item><item><title>&#32431;&#33945;&#29305;&#21345;&#27931;&#21453;&#20107;&#23454;&#36951;&#25022;&#26368;&#23567;&#21270;&#31639;&#27861;&#65288;PCFR&#65289;&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#21453;&#20107;&#23454;&#36951;&#25022;&#26368;&#23567;&#21270;&#65288;CFR&#65289;&#21644;&#34394;&#25311;&#28216;&#25103;&#65288;FP&#65289;&#27010;&#24565;&#30340;&#26032;&#31639;&#27861;&#65292;&#33021;&#22815;&#19982;&#21508;&#31181;CFR&#21464;&#20307;&#30456;&#32467;&#21512;&#65292;&#21253;&#25324;&#33945;&#29305;&#21345;&#27931;CFR&#65288;MCCFR&#65289;&#12290;PCFR&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#36739;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#21516;&#26102;&#38477;&#20302;&#20102;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2309.03084</link><description>&lt;p&gt;
&#32431;&#33945;&#29305;&#21345;&#27931;&#21453;&#20107;&#23454;&#36951;&#25022;&#26368;&#23567;&#21270;
&lt;/p&gt;
&lt;p&gt;
Pure Monte Carlo Counterfactual Regret Minimization. (arXiv:2309.03084v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03084
&lt;/p&gt;
&lt;p&gt;
&#32431;&#33945;&#29305;&#21345;&#27931;&#21453;&#20107;&#23454;&#36951;&#25022;&#26368;&#23567;&#21270;&#31639;&#27861;&#65288;PCFR&#65289;&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#21453;&#20107;&#23454;&#36951;&#25022;&#26368;&#23567;&#21270;&#65288;CFR&#65289;&#21644;&#34394;&#25311;&#28216;&#25103;&#65288;FP&#65289;&#27010;&#24565;&#30340;&#26032;&#31639;&#27861;&#65292;&#33021;&#22815;&#19982;&#21508;&#31181;CFR&#21464;&#20307;&#30456;&#32467;&#21512;&#65292;&#21253;&#25324;&#33945;&#29305;&#21345;&#27931;CFR&#65288;MCCFR&#65289;&#12290;PCFR&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#36739;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#21516;&#26102;&#38477;&#20302;&#20102;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#36951;&#25022;&#26368;&#23567;&#21270;&#65288;CFR&#65289;&#21450;&#20854;&#21464;&#20307;&#26159;&#30446;&#21069;&#35299;&#20915;&#22823;&#35268;&#27169;&#19981;&#23436;&#20840;&#20449;&#24687;&#21338;&#24328;&#30340;&#26368;&#20339;&#31639;&#27861;&#12290;&#26412;&#25991;&#22312;CFR&#30340;&#22522;&#30784;&#19978;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32431;CFR&#65288;PCFR&#65289;&#30340;&#26032;&#31639;&#27861;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;PCFR&#21487;&#20197;&#30475;&#20316;&#26159;CFR&#21644;&#34394;&#25311;&#28216;&#25103;&#65288;FP&#65289;&#30340;&#32467;&#21512;&#65292;&#32487;&#25215;&#20102;CFR&#30340;&#21453;&#20107;&#23454;&#36951;&#25022;&#65288;&#20540;&#65289;&#30340;&#27010;&#24565;&#65292;&#24182;&#22312;&#19979;&#19968;&#27425;&#36845;&#20195;&#20013;&#20351;&#29992;&#26368;&#20339;&#21709;&#24212;&#31574;&#30053;&#32780;&#19981;&#26159;&#36951;&#25022;&#21305;&#37197;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#35777;&#26126;&#20102;PCFR&#21487;&#20197;&#23454;&#29616;Blackwell&#21487;&#36798;&#24615;&#65292;&#20351;PCFR&#33021;&#22815;&#19982;&#21253;&#25324;&#33945;&#29305;&#21345;&#27931;CFR&#65288;MCCFR&#65289;&#22312;&#20869;&#30340;&#20219;&#20309;CFR&#21464;&#20307;&#30456;&#32467;&#21512;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#32431;MCCFR&#65288;PMCCFR&#65289;&#21487;&#20197;&#22823;&#22823;&#38477;&#20302;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#12290;&#29305;&#21035;&#22320;&#65292;PMCCFR&#30340;&#25910;&#25947;&#36895;&#24230;&#33267;&#23569;&#27604;MCCFR&#24555;&#19977;&#20493;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;PMCCFR&#19981;&#36890;&#36807;&#20005;&#26684;&#34987;&#25903;&#37197;&#31574;&#30053;&#30340;&#36335;&#24452;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#21551;&#21160;&#31639;&#27861;&#65292;&#21463;&#21040;&#20102;&#20005;&#26684;&#34987;&#25903;&#37197;&#31574;&#30053;&#30340;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Regret Minimization (CFR) and its variants are the best algorithms so far for solving large-scale incomplete information games. Building upon CFR, this paper proposes a new algorithm named Pure CFR (PCFR) for achieving better performance. PCFR can be seen as a combination of CFR and Fictitious Play (FP), inheriting the concept of counterfactual regret (value) from CFR, and using the best response strategy instead of the regret matching strategy for the next iteration. Our theoretical proof that PCFR can achieve Blackwell approachability enables PCFR's ability to combine with any CFR variant including Monte Carlo CFR (MCCFR). The resultant Pure MCCFR (PMCCFR) can significantly reduce time and space complexity. Particularly, the convergence speed of PMCCFR is at least three times more than that of MCCFR. In addition, since PMCCFR does not pass through the path of strictly dominated strategies, we developed a new warm-start algorithm inspired by the strictly dominated strat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;Transformer-like&#27880;&#24847;&#26426;&#21046;&#22686;&#24378;LSTM&#32593;&#32476;&#65292;&#26816;&#27979;GNSS&#35266;&#27979;&#20013;&#30340;NLOS&#25509;&#25910;&#24182;&#39044;&#27979;GNSS&#20266;&#36317;&#35823;&#24046;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#36710;&#36742;&#23450;&#20301;&#30340;&#31934;&#24230;&#21644;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.00480</link><description>&lt;p&gt;
&#22522;&#20110;&#23398;&#20064;&#30340;GNSS&#35266;&#27979;&#30340;NLOS&#26816;&#27979;&#19982;&#19981;&#30830;&#23450;&#24615;&#39044;&#27979;&#30340;Transformer&#22686;&#24378;LSTM&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Learning-based NLOS Detection and Uncertainty Prediction of GNSS Observations with Transformer-Enhanced LSTM Network. (arXiv:2309.00480v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00480
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;Transformer-like&#27880;&#24847;&#26426;&#21046;&#22686;&#24378;LSTM&#32593;&#32476;&#65292;&#26816;&#27979;GNSS&#35266;&#27979;&#20013;&#30340;NLOS&#25509;&#25910;&#24182;&#39044;&#27979;GNSS&#20266;&#36317;&#35823;&#24046;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#36710;&#36742;&#23450;&#20301;&#30340;&#31934;&#24230;&#21644;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#29699;&#23548;&#33322;&#21355;&#26143;&#31995;&#32479;&#65288;GNSS&#65289;&#22312;&#20132;&#36890;&#31995;&#32479;&#20013;&#23545;&#20110;&#31934;&#30830;&#21644;&#19968;&#33268;&#30340;&#36710;&#36742;&#23450;&#20301;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22312;&#22478;&#24066;&#23777;&#35895;&#31561;&#22797;&#26434;&#29615;&#22659;&#20013;&#65292;&#30001;&#20110;&#22810;&#36335;&#24452;&#25928;&#24212;&#21644;&#38750;&#30452;&#36798;&#65288;NLOS&#65289;&#25509;&#25910;&#30340;&#24433;&#21709;&#65292;GNSS&#35266;&#27979;&#21487;&#20197;&#20135;&#29983;&#25197;&#26354;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20256;&#32479;&#30340;&#20998;&#31867;&#19982;&#25490;&#38500;&#38169;&#35823;GNSS&#35266;&#27979;&#30340;&#26041;&#27861;&#21487;&#33021;&#20250;&#22833;&#36133;&#65292;&#23548;&#33268;&#19981;&#21487;&#38752;&#30340;&#29366;&#24577;&#20272;&#35745;&#21644;&#19981;&#23433;&#20840;&#30340;&#31995;&#32479;&#25805;&#20316;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;GNSS&#35266;&#27979;&#35270;&#20026;&#31354;&#26102;&#24314;&#27169;&#38382;&#39064;&#65292;&#26816;&#27979;NLOS&#25509;&#25910;&#24182;&#39044;&#27979;GNSS&#20266;&#36317;&#35823;&#24046;&#12290;&#19982;&#20808;&#21069;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#31867;&#20284;Transformer&#30340;&#27880;&#24847;&#26426;&#21046;&#65292;&#22686;&#24378;&#20102;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#32593;&#32476;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#26222;&#36866;&#24615;&#12290;&#23545;&#20110;&#25152;&#25552;&#20986;&#30340;&#32593;&#32476;&#30340;&#35757;&#32451;&#21644;&#35780;&#20272;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#39321;&#28207;&#21644;&#20122;&#29723;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#29983;&#25104;&#36807;&#31243;&#26469;&#26631;&#35760;...
&lt;/p&gt;
&lt;p&gt;
The global navigation satellite systems (GNSS) play a vital role in transport systems for accurate and consistent vehicle localization. However, GNSS observations can be distorted due to multipath effects and non-line-of-sight (NLOS) receptions in challenging environments such as urban canyons. In such cases, traditional methods to classify and exclude faulty GNSS observations may fail, leading to unreliable state estimation and unsafe system operations. This work proposes a Deep-Learning-based method to detect NLOS receptions and predict GNSS pseudorange errors by analyzing GNSS observations as a spatio-temporal modeling problem. Compared to previous works, we construct a transformer-like attention mechanism to enhance the long short-term memory (LSTM) networks, improving model performance and generalization. For the training and evaluation of the proposed network, we used labeled datasets from the cities of Hong Kong and Aachen. We also introduce a dataset generation process to label
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#21327;&#20316;&#20449;&#24687;&#20256;&#25773;&#12290;&#36890;&#36807;&#25552;&#20986;&#20998;&#24067;&#24335;POMDP&#24418;&#24335;&#65292;&#22312;&#28040;&#24687;&#36716;&#21457;&#19978;&#23454;&#29616;&#20102;&#27599;&#20010;&#26234;&#33021;&#20307;&#30340;&#29420;&#31435;&#20915;&#31574;&#65292;&#30456;&#27604;&#20256;&#32479;&#30340;&#22522;&#20110;&#22810;&#28857;&#20013;&#32487;&#36873;&#25321;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#20855;&#26377;&#37325;&#22823;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;&#21516;&#26102;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#22270;&#21367;&#31215;&#24378;&#21270;&#23398;&#20064;&#21644;&#21160;&#24577;&#27880;&#24847;&#21147;&#26426;&#21046;&#25429;&#25417;&#20851;&#38190;&#32593;&#32476;&#29305;&#24449;&#65292;&#24182;&#25552;&#20986;&#20102;&#19981;&#21516;&#20449;&#24687;&#20132;&#25442;&#26041;&#24335;&#30340;&#20004;&#31181;&#26041;&#27861;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2308.16198</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#22270;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#23398;&#20064;&#21327;&#20316;&#20449;&#24687;&#20256;&#25773;
&lt;/p&gt;
&lt;p&gt;
Learning Collaborative Information Dissemination with Graph-based Multi-Agent Reinforcement Learning. (arXiv:2308.16198v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16198
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#21327;&#20316;&#20449;&#24687;&#20256;&#25773;&#12290;&#36890;&#36807;&#25552;&#20986;&#20998;&#24067;&#24335;POMDP&#24418;&#24335;&#65292;&#22312;&#28040;&#24687;&#36716;&#21457;&#19978;&#23454;&#29616;&#20102;&#27599;&#20010;&#26234;&#33021;&#20307;&#30340;&#29420;&#31435;&#20915;&#31574;&#65292;&#30456;&#27604;&#20256;&#32479;&#30340;&#22522;&#20110;&#22810;&#28857;&#20013;&#32487;&#36873;&#25321;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#20855;&#26377;&#37325;&#22823;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;&#21516;&#26102;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#22270;&#21367;&#31215;&#24378;&#21270;&#23398;&#20064;&#21644;&#21160;&#24577;&#27880;&#24847;&#21147;&#26426;&#21046;&#25429;&#25417;&#20851;&#38190;&#32593;&#32476;&#29305;&#24449;&#65292;&#24182;&#25552;&#20986;&#20102;&#19981;&#21516;&#20449;&#24687;&#20132;&#25442;&#26041;&#24335;&#30340;&#20004;&#31181;&#26041;&#27861;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#36890;&#20449;&#31995;&#32479;&#20013;&#65292;&#39640;&#25928;&#21487;&#38752;&#30340;&#20449;&#24687;&#20256;&#25773;&#23545;&#25903;&#25345;&#20851;&#38190;&#25805;&#20316;&#33267;&#20851;&#37325;&#35201;&#65292;&#22914;&#28798;&#38590;&#21709;&#24212;&#12289;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#21644;&#20256;&#24863;&#22120;&#32593;&#32476;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#26041;&#27861;&#65292;&#20316;&#20026;&#23454;&#29616;&#26356;&#20026;&#20998;&#25955;&#12289;&#39640;&#25928;&#21644;&#21327;&#20316;&#35299;&#20915;&#26041;&#26696;&#30340;&#37325;&#35201;&#36827;&#23637;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20449;&#24687;&#20256;&#25773;&#30340;&#20998;&#24067;&#24335;POMDP&#65288;Decentralized-POMDP&#65289;&#24418;&#24335;&#65292;&#20351;&#24471;&#27599;&#20010;&#26234;&#33021;&#20307;&#21487;&#20197;&#29420;&#31435;&#20915;&#23450;&#28040;&#24687;&#30340;&#36716;&#21457;&#12290;&#36825;&#26500;&#25104;&#20102;&#19968;&#31181;&#20174;&#20256;&#32479;&#22522;&#20110;&#22810;&#28857;&#20013;&#32487;&#65288;MPR&#65289;&#36873;&#25321;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#30340;&#37325;&#22823;&#33539;&#24335;&#36716;&#31227;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#22270;&#21367;&#31215;&#24378;&#21270;&#23398;&#20064;&#65292;&#37319;&#29992;&#20855;&#26377;&#21160;&#24577;&#27880;&#24847;&#21147;&#30340;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;GAT&#65289;&#26469;&#25429;&#25417;&#20851;&#38190;&#32593;&#32476;&#29305;&#24449;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#65292;L-DGN&#21644;HL-DGN&#65292;&#23427;&#20204;&#22312;&#26234;&#33021;&#20307;&#20043;&#38388;&#20132;&#25442;&#30340;&#20449;&#24687;&#19978;&#26377;&#25152;&#19981;&#21516;&#12290;&#36890;&#36807;&#23558;&#25105;&#20204;&#30340;&#20998;&#25955;&#26041;&#27861;&#19982;&#22522;&#20110;MPR&#30340;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In modern communication systems, efficient and reliable information dissemination is crucial for supporting critical operations across domains like disaster response, autonomous vehicles, and sensor networks. This paper introduces a Multi-Agent Reinforcement Learning (MARL) approach as a significant step forward in achieving more decentralized, efficient, and collaborative solutions. We propose a Decentralized-POMDP formulation for information dissemination, empowering each agent to independently decide on message forwarding. This constitutes a significant paradigm shift from traditional heuristics based on Multi-Point Relay (MPR) selection. Our approach harnesses Graph Convolutional Reinforcement Learning, employing Graph Attention Networks (GAT) with dynamic attention to capture essential network features. We propose two approaches, L-DGN and HL-DGN, which differ in the information that is exchanged among agents. We evaluate the performance of our decentralized approaches, by compari
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#26410;&#30693;&#32593;&#32476;&#19978;&#36827;&#34892;&#27969;&#34892;&#30149;&#25511;&#21046;&#12290;&#27169;&#25311;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.14311</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#30340;&#26410;&#30693;&#32593;&#32476;&#20256;&#25773;&#25511;&#21046;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Spread Control Method on Unknown Networks Based on Hierarchical Reinforcement Learning. (arXiv:2308.14311v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14311
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#26410;&#30693;&#32593;&#32476;&#19978;&#36827;&#34892;&#27969;&#34892;&#30149;&#25511;&#21046;&#12290;&#27169;&#25311;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
COVID-19&#31561;&#27969;&#34892;&#30149;&#23545;&#20844;&#20849;&#21355;&#29983;&#21644;&#31038;&#20250;&#26500;&#25104;&#20005;&#37325;&#23041;&#32961;&#65292;&#22240;&#27492;&#26377;&#24517;&#35201;&#30740;&#31350;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#25511;&#21046;&#27969;&#34892;&#30149;&#22312;&#32593;&#32476;&#20013;&#30340;&#20256;&#25773;&#12290;&#20197;&#24448;&#20851;&#20110;&#27969;&#34892;&#30149;&#25511;&#21046;&#30340;&#30740;&#31350;&#24448;&#24448;&#20551;&#35774;&#23436;&#20840;&#20102;&#35299;&#32593;&#32476;&#32467;&#26500;&#65292;&#36825;&#22312;&#23454;&#38469;&#24773;&#20917;&#20013;&#24456;&#23569;&#25104;&#31435;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26410;&#30693;&#32467;&#26500;&#30340;&#32593;&#32476;&#19978;&#30340;&#27969;&#34892;&#30149;&#25511;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#20197;&#21516;&#26102;&#25506;&#32034;&#32593;&#32476;&#32467;&#26500;&#21644;&#25511;&#21046;&#27969;&#34892;&#30149;&#12290;&#20026;&#20102;&#20943;&#23569;&#34892;&#21160;&#31354;&#38388;&#21644;&#23454;&#29616;&#21487;&#35745;&#31639;&#24615;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#21547;&#19977;&#20010;&#27169;&#22359;&#65306;&#31574;&#30053;&#36873;&#25321;&#27169;&#22359;&#65292;&#30830;&#23450;&#26159;&#25506;&#32034;&#32467;&#26500;&#36824;&#26159;&#31227;&#38500;&#33410;&#28857;&#20197;&#25511;&#21046;&#27969;&#34892;&#30149;&#65307;&#25506;&#32034;&#27169;&#22359;&#65292;&#36127;&#36131;&#36873;&#25321;&#35201;&#25506;&#32034;&#30340;&#33410;&#28857;&#65307;&#31227;&#38500;&#27169;&#22359;&#65292;&#20915;&#23450;&#31227;&#38500;&#21738;&#20123;&#33410;&#28857;&#20197;&#20572;&#27490;&#27969;&#34892;&#30149;&#20256;&#25773;&#12290;&#27169;&#25311;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Epidemics such as COVID-19 pose serious threats to public health and our society, and it is critical to investigate effective methods to control the spread of epidemics over networks. Prior works on epidemic control often assume complete knowledge of network structures, a presumption seldom valid in real-world situations. In this paper, we study epidemic control on networks with unknown structures, and propose a hierarchical reinforcement learning framework for joint network structure exploration and epidemic control. To reduce the action space and achieve computation tractability, our proposed framework contains three modules: the Policy Selection Module, which determines whether to explore the structure or remove nodes to control the epidemic; the Explore Module, responsible for selecting nodes to explore; and the Remove Module, which decides which nodes to remove to stop the epidemic spread. Simulation results show that our proposed method outperforms baseline methods.
&lt;/p&gt;</description></item><item><title>QKSAN&#26159;&#19968;&#20010;&#37327;&#23376;&#26680;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#36890;&#36807;&#32467;&#21512;&#37327;&#23376;&#26680;&#26041;&#27861;&#21644;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#25552;&#39640;&#20102;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#22823;&#35268;&#27169;&#39640;&#32500;&#37327;&#23376;&#25968;&#25454;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.13422</link><description>&lt;p&gt;
QKSAN: &#37327;&#23376;&#26680;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
QKSAN: A Quantum Kernel Self-Attention Network. (arXiv:2308.13422v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13422
&lt;/p&gt;
&lt;p&gt;
QKSAN&#26159;&#19968;&#20010;&#37327;&#23376;&#26680;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#36890;&#36807;&#32467;&#21512;&#37327;&#23376;&#26680;&#26041;&#27861;&#21644;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#25552;&#39640;&#20102;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#22823;&#35268;&#27169;&#39640;&#32500;&#37327;&#23376;&#25968;&#25454;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65288;SAM&#65289;&#25797;&#38271;&#20174;&#25968;&#25454;&#20869;&#37096;&#25552;&#21462;&#37325;&#35201;&#20449;&#24687;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#65288;QML&#65289;&#27169;&#22411;&#32570;&#20047;&#20687;SAM&#36825;&#26679;&#21306;&#20998;&#20449;&#24687;&#30340;&#22266;&#26377;&#36830;&#25509;&#30340;&#33021;&#21147;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#22823;&#35268;&#27169;&#39640;&#32500;&#37327;&#23376;&#25968;&#25454;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#37327;&#23376;&#26680;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65288;QKSAM&#65289;&#65292;&#23427;&#23558;&#37327;&#23376;&#26680;&#26041;&#27861;&#65288;QKM&#65289;&#30340;&#25968;&#25454;&#34920;&#31034;&#20248;&#21183;&#19982;SAM&#30340;&#26377;&#25928;&#20449;&#24687;&#25552;&#21462;&#33021;&#21147;&#30456;&#32467;&#21512;&#12290;&#22522;&#20110;QKSAM&#26500;&#24314;&#20102;&#19968;&#20010;&#37327;&#23376;&#26680;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;QKSAN&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;&#24310;&#36831;&#27979;&#37327;&#21407;&#29702;&#65288;DMP&#65289;&#21644;&#26465;&#20214;&#27979;&#37327;&#25216;&#26415;&#65292;&#22312;&#35745;&#31639;&#36807;&#31243;&#20013;&#20197;&#27010;&#29575;&#27979;&#37327;&#30340;&#26041;&#24335;&#37322;&#25918;&#20102;&#19968;&#21322;&#30340;&#37327;&#23376;&#36164;&#28304;&#12290;&#37327;&#23376;&#26680;&#33258;&#27880;&#24847;&#21147;&#20998;&#25968;&#65288;QKSAS&#65289;&#30830;&#23450;&#27979;&#37327;&#26465;&#20214;&#24182;&#21453;&#26144;&#20102;&#37327;&#23376;&#31995;&#32479;&#30340;&#27010;&#29575;&#26412;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-Attention Mechanism (SAM) is skilled at extracting important information from the interior of data to improve the computational efficiency of models. Nevertheless, many Quantum Machine Learning (QML) models lack the ability to distinguish the intrinsic connections of information like SAM, which limits their effectiveness on massive high-dimensional quantum data. To address this issue, a Quantum Kernel Self-Attention Mechanism (QKSAM) is introduced, which combines the data representation benefit of Quantum Kernel Methods (QKM) with the efficient information extraction capability of SAM. A Quantum Kernel Self-Attention Network (QKSAN) framework is built based on QKSAM, with Deferred Measurement Principle (DMP) and conditional measurement techniques, which releases half of the quantum resources with probabilistic measurements during computation. The Quantum Kernel Self-Attention Score (QKSAS) determines the measurement conditions and reflects the probabilistic nature of quantum syste
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31232;&#30095;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#21152;&#26435;Chebyshev&#26631;&#37327;&#21270;&#21644;&#22686;&#24191;Lagrangian&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#21516;&#26102;&#20248;&#21270;&#22810;&#20010;&#20219;&#21153;&#26102;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.12243</link><description>&lt;p&gt;
&#29992;&#20110;&#31232;&#30095;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Multi-Objective Optimization for Sparse Deep Neural Network Training. (arXiv:2308.12243v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12243
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31232;&#30095;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#21152;&#26435;Chebyshev&#26631;&#37327;&#21270;&#21644;&#22686;&#24191;Lagrangian&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#21516;&#26102;&#20248;&#21270;&#22810;&#20010;&#20219;&#21153;&#26102;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#30340;&#21508;&#31181;&#22330;&#26223;&#20013;&#65292;&#20250;&#33258;&#28982;&#22320;&#20986;&#29616;&#19981;&#21516;&#30340;&#20914;&#31361;&#20248;&#21270;&#20934;&#21017;&#12290;&#36825;&#20123;&#20934;&#21017;&#21487;&#20197;&#35299;&#20915;&#19981;&#21516;&#30340;&#20027;&#20219;&#21153;&#65288;&#22914;&#22810;&#20219;&#21153;&#23398;&#20064;&#35774;&#32622;&#65289;&#65292;&#20063;&#21487;&#20197;&#35299;&#20915;&#20027;&#35201;&#20219;&#21153;&#21644;&#27425;&#35201;&#20219;&#21153;&#65292;&#20363;&#22914;&#25439;&#22833;&#26368;&#23567;&#21270;&#19982;&#31232;&#30095;&#24615;&#12290;&#36890;&#24120;&#30340;&#26041;&#27861;&#26159;&#31616;&#21333;&#22320;&#21152;&#26435;&#20934;&#21017;&#65292;&#20294;&#22312;&#20984;&#35774;&#32622;&#20013;&#25165;&#26377;&#25928;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#30446;&#26631;&#20248;&#21270;&#31639;&#27861;&#65292;&#23545;&#22810;&#20219;&#21153;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#36827;&#34892;&#35757;&#32451;&#65292;&#20351;&#29992;&#25913;&#36827;&#30340;&#21152;&#26435;Chebyshev&#26631;&#37327;&#21270;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#36825;&#31181;&#26631;&#37327;&#21270;&#25216;&#26415;&#65292;&#31639;&#27861;&#21487;&#20197;&#35782;&#21035;&#21407;&#22987;&#38382;&#39064;&#30340;&#25152;&#26377;&#26368;&#20248;&#35299;&#65292;&#21516;&#26102;&#23558;&#20854;&#22797;&#26434;&#24615;&#38477;&#20302;&#20026;&#19968;&#31995;&#21015;&#21333;&#30446;&#26631;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#22686;&#24191;Lagrangian&#26041;&#27861;&#26469;&#35299;&#20915;&#31616;&#21270;&#21518;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#21487;&#20197;&#20351;&#29992;&#24120;&#35265;&#30340;&#20248;&#21270;&#25216;&#26415;&#65292;&#22914;Adam&#21644;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65292;&#21516;&#26102;&#26377;&#25928;&#22320;&#22788;&#29702;&#32422;&#26463;&#26465;&#20214;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26088;&#22312;&#35299;&#20915;&#32463;&#27982;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Different conflicting optimization criteria arise naturally in various Deep Learning scenarios. These can address different main tasks (i.e., in the setting of Multi-Task Learning), but also main and secondary tasks such as loss minimization versus sparsity. The usual approach is a simple weighting of the criteria, which formally only works in the convex setting. In this paper, we present a Multi-Objective Optimization algorithm using a modified Weighted Chebyshev scalarization for training Deep Neural Networks (DNNs) with respect to several tasks. By employing this scalarization technique, the algorithm can identify all optimal solutions of the original problem while reducing its complexity to a sequence of single-objective problems. The simplified problems are then solved using an Augmented Lagrangian method, enabling the use of popular optimization techniques such as Adam and Stochastic Gradient Descent, while efficaciously handling constraints. Our work aims to address the (economi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21019;&#26032;&#24615;&#22320;&#23558;&#20174;&#30495;&#23454;&#20154;&#26426;&#23545;&#35805;&#20013;&#25552;&#21462;&#30340;&#20154;&#31867;&#38382;&#39064;&#20316;&#20026;&#23398;&#20064;&#30446;&#26631;&#65292;&#24182;&#19988;&#35757;&#32451;&#20102;&#19968;&#20010;&#29992;&#25143;&#27169;&#25311;&#22120;UserGPT&#65292;&#24182;&#20351;&#29992;&#29983;&#25104;&#30340;&#39640;&#36136;&#37327;&#21512;&#25104;&#23545;&#35805;&#25968;&#25454;&#38598;RealChat&#26469;&#35757;&#32451;&#21161;&#25163;&#27169;&#22411;ReaLM&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;ReaLM&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36807;&#20102;&#22522;&#20934;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.11534</link><description>&lt;p&gt;
&#20316;&#20026;&#29992;&#25143;&#27169;&#25311;&#22120;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Large Language Model as a User Simulator. (arXiv:2308.11534v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11534
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21019;&#26032;&#24615;&#22320;&#23558;&#20174;&#30495;&#23454;&#20154;&#26426;&#23545;&#35805;&#20013;&#25552;&#21462;&#30340;&#20154;&#31867;&#38382;&#39064;&#20316;&#20026;&#23398;&#20064;&#30446;&#26631;&#65292;&#24182;&#19988;&#35757;&#32451;&#20102;&#19968;&#20010;&#29992;&#25143;&#27169;&#25311;&#22120;UserGPT&#65292;&#24182;&#20351;&#29992;&#29983;&#25104;&#30340;&#39640;&#36136;&#37327;&#21512;&#25104;&#23545;&#35805;&#25968;&#25454;&#38598;RealChat&#26469;&#35757;&#32451;&#21161;&#25163;&#27169;&#22411;ReaLM&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;ReaLM&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36807;&#20102;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38381;&#28304;ChatGPT&#30340;&#21331;&#36234;&#24615;&#33021;&#24341;&#21457;&#20102;&#23545;&#20854;&#27665;&#20027;&#21270;&#30340;&#21162;&#21147;&#65292;&#20511;&#21161;&#30495;&#23454;&#29992;&#25143;&#21644;ChatGPT&#23545;&#35805;&#30340;&#21162;&#21147;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;Vicuna&#26159;&#19968;&#20010;&#24456;&#22909;&#30340;&#20363;&#23376;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;Baize&#21644;UltraChat&#31561;&#21162;&#21147;&#20027;&#35201;&#20381;&#38752;ChatGPT&#26681;&#25454;&#25351;&#20196;&#27169;&#25311;&#20154;&#31867;&#34892;&#20026;&#65292;&#32780;&#19981;&#26159;&#30495;&#23454;&#30340;&#20154;&#31867;&#23398;&#20064;&#65292;&#23548;&#33268;&#33539;&#22260;&#26377;&#38480;&#65292;&#22810;&#26679;&#24615;&#20943;&#24369;&#65292;&#32570;&#20047;&#30495;&#27491;&#30340;&#22810;&#36718;&#23545;&#35805;&#21160;&#24577;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#21019;&#26032;&#24615;&#22320;&#25226;&#20174;&#30495;&#23454;&#20154;&#26426;&#23545;&#35805;&#20013;&#25552;&#21462;&#30340;&#20154;&#31867;&#38382;&#39064;&#20316;&#20026;&#23398;&#20064;&#30446;&#26631;&#65292;&#24182;&#35757;&#32451;&#19968;&#20010;&#29992;&#25143;&#27169;&#25311;&#22120;UserGPT&#26469;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#21512;&#25104;&#23545;&#35805;&#25968;&#25454;&#38598;RealChat&#12290;&#38543;&#21518;&#65292;&#35813;&#25968;&#25454;&#38598;&#35757;&#32451;&#25105;&#20204;&#30340;&#21161;&#25163;&#27169;&#22411;ReaLM&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;ReaLM&#22312;Vicuna-Bench&#21644;MT-Bench&#20013;&#22343;&#36229;&#36807;&#20102;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The unparalleled performance of closed-sourced ChatGPT has sparked efforts towards its democratization, with notable strides made by leveraging real user and ChatGPT conversations, as evidenced by Vicuna. However, while current endeavors like Baize and UltraChat aim to auto-generate conversational data due to challenges in gathering human participation, they primarily rely on ChatGPT to simulate human behaviors based on directives rather than genuine human learning. This results in a limited scope, diminished diversity, and an absence of genuine multi-round conversational dynamics. To address the above issues, we innovatively target human questions extracted from genuine human-machine conversations as a learning goal and train a user simulator, UserGPT, to produce a high-quality human-centric synthetic conversation dataset, RealChat. Subsequently, this dataset trains our assistant model, ReaLM. Experimentally, ReaLM outpaces baseline models in both Vicuna-Bench and MT-Bench by pairwise
&lt;/p&gt;</description></item><item><title>&#22312;&#20851;&#38190;&#24212;&#29992;&#39046;&#22495;&#30340;&#20154;&#26426;&#21327;&#21516;&#31995;&#32479;&#20013;&#65292;&#20026;&#20102;&#30830;&#20445;&#26368;&#23567;&#38169;&#35823;&#65292;&#38656;&#35201;&#26681;&#25454;&#27169;&#22411;&#32622;&#20449;&#24230;&#35774;&#23450;&#25805;&#20316;&#28857;&#36827;&#34892;&#20915;&#31574;&#22996;&#25176;&#32473;&#19987;&#23478;&#12290;&#20026;&#20102;&#25552;&#39640;&#31995;&#32479;&#25928;&#29992;&#65292;&#38656;&#35201;&#32771;&#34385;&#27169;&#22411;&#20165;&#23545;&#20934;&#30830;&#26679;&#26412;&#20855;&#26377;&#33258;&#20449;&#30340;&#29305;&#28857;&#20197;&#21450;&#23613;&#37327;&#20943;&#23569;&#22996;&#25176;&#32473;&#19987;&#23478;&#30340;&#26679;&#26412;&#25968;&#37327;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#32622;&#20449;&#24230;&#25805;&#20316;&#29305;&#24615;&#65288;COC&#65289;&#26354;&#32447;&#26469;&#34920;&#31034;&#27169;&#22411;&#20934;&#30830;&#24615;&#19982;&#19987;&#23478;&#22788;&#29702;&#26679;&#26412;&#25968;&#37327;&#30340;&#26435;&#34913;&#20851;&#31995;&#12290;&#36825;&#23545;&#20110;&#21307;&#30103;&#20445;&#20581;&#31561;&#21487;&#29992;&#19987;&#23478;&#26102;&#38388;&#26377;&#38480;&#19988;&#26114;&#36149;&#30340;&#39046;&#22495;&#23588;&#20026;&#20851;&#38190;&#12290;</title><link>http://arxiv.org/abs/2308.05035</link><description>&lt;p&gt;
&#19987;&#23478;&#36127;&#36733;&#24456;&#37325;&#35201;&#65306;&#20197;&#39640;&#20934;&#30830;&#24615;&#21644;&#20302;&#20154;&#24037;&#24037;&#20316;&#37327;&#36816;&#34892;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Expert load matters: operating networks at high accuracy and low manual effort. (arXiv:2308.05035v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05035
&lt;/p&gt;
&lt;p&gt;
&#22312;&#20851;&#38190;&#24212;&#29992;&#39046;&#22495;&#30340;&#20154;&#26426;&#21327;&#21516;&#31995;&#32479;&#20013;&#65292;&#20026;&#20102;&#30830;&#20445;&#26368;&#23567;&#38169;&#35823;&#65292;&#38656;&#35201;&#26681;&#25454;&#27169;&#22411;&#32622;&#20449;&#24230;&#35774;&#23450;&#25805;&#20316;&#28857;&#36827;&#34892;&#20915;&#31574;&#22996;&#25176;&#32473;&#19987;&#23478;&#12290;&#20026;&#20102;&#25552;&#39640;&#31995;&#32479;&#25928;&#29992;&#65292;&#38656;&#35201;&#32771;&#34385;&#27169;&#22411;&#20165;&#23545;&#20934;&#30830;&#26679;&#26412;&#20855;&#26377;&#33258;&#20449;&#30340;&#29305;&#28857;&#20197;&#21450;&#23613;&#37327;&#20943;&#23569;&#22996;&#25176;&#32473;&#19987;&#23478;&#30340;&#26679;&#26412;&#25968;&#37327;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#32622;&#20449;&#24230;&#25805;&#20316;&#29305;&#24615;&#65288;COC&#65289;&#26354;&#32447;&#26469;&#34920;&#31034;&#27169;&#22411;&#20934;&#30830;&#24615;&#19982;&#19987;&#23478;&#22788;&#29702;&#26679;&#26412;&#25968;&#37327;&#30340;&#26435;&#34913;&#20851;&#31995;&#12290;&#36825;&#23545;&#20110;&#21307;&#30103;&#20445;&#20581;&#31561;&#21487;&#29992;&#19987;&#23478;&#26102;&#38388;&#26377;&#38480;&#19988;&#26114;&#36149;&#30340;&#39046;&#22495;&#23588;&#20026;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21327;&#21516;&#31995;&#32479;&#30340;&#20851;&#38190;&#24212;&#29992;&#20013;&#65292;&#20026;&#20102;&#30830;&#20445;&#26368;&#23567;&#38169;&#35823;&#65292;&#29992;&#25143;&#24212;&#26681;&#25454;&#27169;&#22411;&#30340;&#32622;&#20449;&#24230;&#35774;&#23450;&#25805;&#20316;&#28857;&#65292;&#30830;&#23450;&#20309;&#26102;&#23558;&#20915;&#31574;&#22996;&#25176;&#32473;&#20154;&#31867;&#19987;&#23478;&#12290;&#27169;&#22411;&#32622;&#20449;&#24230;&#20302;&#20110;&#25805;&#20316;&#28857;&#30340;&#26679;&#26412;&#23558;&#30001;&#19987;&#23478;&#25163;&#21160;&#20998;&#26512;&#65292;&#20197;&#36991;&#20813;&#38169;&#35823;&#12290;&#21482;&#26377;&#22312;&#32771;&#34385;&#21040;&#20004;&#20010;&#26041;&#38754;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#26679;&#30340;&#31995;&#32479;&#25165;&#33021;&#30495;&#27491;&#26377;&#29992;&#65306;&#27169;&#22411;&#21482;&#23545;&#20934;&#30830;&#30340;&#26679;&#26412;&#20855;&#26377;&#33258;&#20449;&#65292;&#24182;&#19988;&#23613;&#37327;&#20943;&#23569;&#22996;&#25176;&#32473;&#19987;&#23478;&#30340;&#26679;&#26412;&#25968;&#37327;&#12290;&#21518;&#32773;&#23545;&#20110;&#21487;&#29992;&#19987;&#23478;&#26102;&#38388;&#26377;&#38480;&#19988;&#26114;&#36149;&#30340;&#24212;&#29992;&#65292;&#22914;&#21307;&#30103;&#20445;&#20581;&#65292;&#23588;&#20026;&#20851;&#38190;&#12290;&#27169;&#22411;&#20934;&#30830;&#24615;&#19982;&#22996;&#25176;&#32473;&#19987;&#23478;&#26679;&#26412;&#25968;&#37327;&#20043;&#38388;&#30340;&#26435;&#34913;&#21487;&#20197;&#29992;&#31867;&#20284;ROC&#26354;&#32447;&#30340;&#26354;&#32447;&#34920;&#31034;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#32622;&#20449;&#24230;&#25805;&#20316;&#29305;&#24615;&#65288;COC&#65289;&#26354;&#32447;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#24212;&#35813;&#36890;&#36807;&#32771;&#34385;&#21040;&#27169;&#22411;&#30340;&#32622;&#20449;&#24230;&#21644;&#19987;&#23478;&#22788;&#29702;&#30340;&#26679;&#26412;&#25968;&#37327;&#26469;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
In human-AI collaboration systems for critical applications, in order to ensure minimal error, users should set an operating point based on model confidence to determine when the decision should be delegated to human experts. Samples for which model confidence is lower than the operating point would be manually analysed by experts to avoid mistakes. Such systems can become truly useful only if they consider two aspects: models should be confident only for samples for which they are accurate, and the number of samples delegated to experts should be minimized. The latter aspect is especially crucial for applications where available expert time is limited and expensive, such as healthcare. The trade-off between the model accuracy and the number of samples delegated to experts can be represented by a curve that is similar to an ROC curve, which we refer to as confidence operating characteristic (COC) curve. In this paper, we argue that deep neural networks should be trained by taking into 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#25105;&#20204;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#25552;&#20986;&#20102;&#26032;&#30340;&#25104;&#23601;&#33976;&#39311;&#26041;&#27861;&#65292;&#21487;&#20197;&#21152;&#24378;&#20195;&#29702;&#23545;&#19979;&#19968;&#20010;&#35299;&#38145;&#25104;&#23601;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#24182;&#20248;&#20110;&#20808;&#21069;&#30340;&#27169;&#22411;&#39537;&#21160;&#21644;&#23618;&#27425;&#21270;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.03486</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#21457;&#29616;&#23618;&#27425;&#21270;&#25104;&#23601;
&lt;/p&gt;
&lt;p&gt;
Discovering Hierarchical Achievements in Reinforcement Learning via Contrastive Learning. (arXiv:2307.03486v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03486
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#25105;&#20204;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#25552;&#20986;&#20102;&#26032;&#30340;&#25104;&#23601;&#33976;&#39311;&#26041;&#27861;&#65292;&#21487;&#20197;&#21152;&#24378;&#20195;&#29702;&#23545;&#19979;&#19968;&#20010;&#35299;&#38145;&#25104;&#23601;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#24182;&#20248;&#20110;&#20808;&#21069;&#30340;&#27169;&#22411;&#39537;&#21160;&#21644;&#23618;&#27425;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#25104;&#29615;&#22659;&#20013;&#21457;&#29616;&#20855;&#26377;&#23618;&#27425;&#32467;&#26500;&#30340;&#25104;&#23601;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#36825;&#38656;&#35201;&#26234;&#33021;&#20307;&#20855;&#22791;&#24191;&#27867;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#27867;&#21270;&#21644;&#38271;&#26399;&#25512;&#29702;&#12290;&#35768;&#22810;&#20808;&#21069;&#30340;&#26041;&#27861;&#22522;&#20110;&#27169;&#22411;&#39537;&#21160;&#25110;&#23618;&#27425;&#21270;&#26041;&#27861;&#65292;&#35748;&#20026;&#26174;&#24335;&#30340;&#38271;&#26399;&#35268;&#21010;&#27169;&#22359;&#23545;&#20110;&#23398;&#20064;&#23618;&#27425;&#21270;&#25104;&#23601;&#26159;&#26377;&#30410;&#30340;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#30340;&#29615;&#22659;&#20132;&#20114;&#25110;&#22823;&#22411;&#27169;&#22411;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#23454;&#29992;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#36817;&#26399;&#23454;&#26045;&#23454;&#36341;&#20013;&#30340;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#65288;PPO&#65289;&#31639;&#27861;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;PPO&#26234;&#33021;&#20307;&#21487;&#20197;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#39044;&#27979;&#19979;&#19968;&#20010;&#35201;&#35299;&#38145;&#30340;&#25104;&#23601;&#65292;&#23613;&#31649;&#39044;&#27979;&#30340;&#32622;&#20449;&#24230;&#36739;&#20302;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;&#25104;&#23601;&#33976;&#39311;&#65292;&#21487;&#20197;&#21152;&#24378;PPO&#26234;&#33021;&#20307;&#23545;&#19979;&#19968;&#20010;&#35299;&#38145;&#25104;&#23601;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discovering achievements with a hierarchical structure on procedurally generated environments poses a significant challenge. This requires agents to possess a broad range of abilities, including generalization and long-term reasoning. Many prior methods are built upon model-based or hierarchical approaches, with the belief that an explicit module for long-term planning would be beneficial for learning hierarchical achievements. However, these methods require an excessive amount of environment interactions or large model sizes, limiting their practicality. In this work, we identify that proximal policy optimization (PPO), a simple and versatile model-free algorithm, outperforms the prior methods with recent implementation practices. Moreover, we find that the PPO agent can predict the next achievement to be unlocked to some extent, though with low confidence. Based on this observation, we propose a novel contrastive learning method, called achievement distillation, that strengthens the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#65292;&#23558;&#25945;&#24072;&#27169;&#22411;&#30340;&#35270;&#35273;&#34920;&#31034;&#21387;&#32553;&#21040;&#23398;&#29983;&#27169;&#22411;&#20013;&#12290;&#30740;&#31350;&#37325;&#28857;&#22312;&#20110;&#36229;&#20986;&#20998;&#24067;&#21487;&#27867;&#21270;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#21407;&#21017;&#26469;&#22686;&#24378;&#23398;&#29983;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.03135</link><description>&lt;p&gt;
&#29992;&#20110;&#36229;&#20986;&#20998;&#24067;&#21487;&#27867;&#21270;&#24615;&#30340;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Distilling Large Vision-Language Model with Out-of-Distribution Generalizability. (arXiv:2307.03135v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03135
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#65292;&#23558;&#25945;&#24072;&#27169;&#22411;&#30340;&#35270;&#35273;&#34920;&#31034;&#21387;&#32553;&#21040;&#23398;&#29983;&#27169;&#22411;&#20013;&#12290;&#30740;&#31350;&#37325;&#28857;&#22312;&#20110;&#36229;&#20986;&#20998;&#24067;&#21487;&#27867;&#21270;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#21407;&#21017;&#26469;&#22686;&#24378;&#23398;&#29983;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#20294;&#20854;&#35268;&#27169;&#21644;&#35745;&#31639;&#35201;&#27714;&#20351;&#23427;&#20204;&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#21644;&#26102;&#38388;&#25935;&#24863;&#20219;&#21153;&#19978;&#30340;&#37096;&#32626;&#21464;&#24471;&#19981;&#20999;&#23454;&#38469;&#12290;&#27169;&#22411;&#21387;&#32553;&#26159;&#21019;&#24314;&#26356;&#23567;&#12289;&#26356;&#24555;&#30340;&#27169;&#22411;&#20197;&#20445;&#25345;&#36739;&#22823;&#27169;&#22411;&#24615;&#33021;&#30340;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35270;&#35273;&#34920;&#31034;&#21387;&#32553;&#21040;&#36731;&#37327;&#32423;&#23398;&#29983;&#27169;&#22411;&#20013;&#30340;&#36807;&#31243;&#65292;&#20351;&#29992;&#23567;&#22411;&#25110;&#20013;&#22411;&#25968;&#25454;&#38598;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#26412;&#30740;&#31350;&#20851;&#27880;&#30340;&#26159;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#21487;&#27867;&#21270;&#30340;&#24320;&#25918;&#35789;&#27719;&#38382;&#39064;&#65292;&#36825;&#22312;&#20197;&#24448;&#30340;&#27169;&#22411;&#21387;&#32553;&#30740;&#31350;&#20013;&#34987;&#24573;&#35270;&#20102;&#12290;&#25105;&#20204;&#20174;&#35270;&#35273;&#21644;&#35821;&#35328;&#30340;&#35282;&#24230;&#25552;&#20986;&#20102;&#20004;&#20010;&#21407;&#21017;&#26469;&#22686;&#24378;&#23398;&#29983;&#27169;&#22411;&#30340;OOD&#21487;&#27867;&#21270;&#24615;&#65306;&#65288;1&#65289;&#26356;&#22909;&#22320;&#27169;&#20223;&#25945;&#24072;&#30340;&#35270;&#35273;&#34920;&#31034;&#31354;&#38388;&#65292;&#24182;&#22312;&#35270;&#35273;&#35821;&#35328;&#23545;&#40784;&#26041;&#38754;&#35880;&#24910;&#22320;&#20419;&#36827;&#26356;&#22909;&#30340;&#19968;&#33268;&#24615;&#65307;&#65288;2&#65289;&#36890;&#36807;&#20016;&#23500;&#23398;&#29983;&#27169;&#22411;&#30340;&#33258;&#20030;&#23398;&#20064;&#21644;&#25968;&#25454;&#25193;&#20805;&#26469;&#25552;&#39640;OOD&#21487;&#27867;&#21270;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large vision-language models have achieved outstanding performance, but their size and computational requirements make their deployment on resource-constrained devices and time-sensitive tasks impractical. Model distillation, the process of creating smaller, faster models that maintain the performance of larger models, is a promising direction towards the solution. This paper investigates the distillation of visual representations in large teacher vision-language models into lightweight student models using a smallor mid-scale dataset. Notably, this study focuses on open-vocabulary out-of-distribution (OOD) generalization, a challenging problem that has been overlooked in previous model distillation literature. We propose two principles from vision and language modality perspectives to enhance student's OOD generalization: (1) by better imitating teacher's visual representation space, and carefully promoting better coherence in vision-language alignment with the teacher; (2) by enric
&lt;/p&gt;</description></item><item><title>&#24377;&#24615;&#20915;&#31574;&#21464;&#21387;&#22120;&#65288;EDT&#65289;&#36890;&#36807;&#22312;&#27979;&#35797;&#26102;&#38388;&#36827;&#34892;&#21160;&#20316;&#25512;&#26029;&#26102;&#35843;&#25972;&#21382;&#21490;&#38271;&#24230;&#26469;&#23454;&#29616;&#36712;&#36857;&#25340;&#25509;&#65292;&#22635;&#34917;&#20102;&#20915;&#31574;&#21464;&#21387;&#22120;&#65288;DT&#65289;&#22312;&#36825;&#19968;&#26041;&#38754;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#24182;&#19988;&#22312;&#22810;&#20219;&#21153;&#24773;&#20917;&#19979;&#32988;&#36807;&#22522;&#20110;Q-Learning&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.02484</link><description>&lt;p&gt;
&#24377;&#24615;&#20915;&#31574;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
Elastic Decision Transformer. (arXiv:2307.02484v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02484
&lt;/p&gt;
&lt;p&gt;
&#24377;&#24615;&#20915;&#31574;&#21464;&#21387;&#22120;&#65288;EDT&#65289;&#36890;&#36807;&#22312;&#27979;&#35797;&#26102;&#38388;&#36827;&#34892;&#21160;&#20316;&#25512;&#26029;&#26102;&#35843;&#25972;&#21382;&#21490;&#38271;&#24230;&#26469;&#23454;&#29616;&#36712;&#36857;&#25340;&#25509;&#65292;&#22635;&#34917;&#20102;&#20915;&#31574;&#21464;&#21387;&#22120;&#65288;DT&#65289;&#22312;&#36825;&#19968;&#26041;&#38754;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#24182;&#19988;&#22312;&#22810;&#20219;&#21153;&#24773;&#20917;&#19979;&#32988;&#36807;&#22522;&#20110;Q-Learning&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#24377;&#24615;&#20915;&#31574;&#21464;&#21387;&#22120;&#65288;EDT&#65289;&#65292;&#23427;&#26159;&#29616;&#26377;&#20915;&#31574;&#21464;&#21387;&#22120;&#65288;DT&#65289;&#21450;&#20854;&#21464;&#20307;&#30340;&#37325;&#22823;&#36827;&#23637;&#12290;&#23613;&#31649;DT&#22768;&#31216;&#33021;&#22815;&#29983;&#25104;&#26368;&#20339;&#36712;&#36857;&#65292;&#20294;&#23454;&#35777;&#35777;&#25454;&#34920;&#26126;&#23427;&#22312;&#36712;&#36857;&#25340;&#25509;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#36712;&#36857;&#25340;&#25509;&#26159;&#25351;&#20174;&#19968;&#32452;&#27425;&#20248;&#36712;&#36857;&#20013;&#29983;&#25104;&#26368;&#20248;&#25110;&#25509;&#36817;&#26368;&#20248;&#36712;&#36857;&#30340;&#36807;&#31243;&#12290;&#25552;&#20986;&#30340;EDT&#36890;&#36807;&#22312;&#27979;&#35797;&#26102;&#38388;&#36827;&#34892;&#21160;&#20316;&#25512;&#26029;&#26102;&#35843;&#25972;DT&#20013;&#32500;&#25252;&#30340;&#21382;&#21490;&#38271;&#24230;&#26469;&#23454;&#29616;&#36712;&#36857;&#25340;&#25509;&#65292;&#20174;&#32780;&#20351;&#33258;&#24049;&#19982;&#20247;&#19981;&#21516;&#12290;&#27492;&#22806;&#65292;&#24403;&#21069;&#36712;&#36857;&#26159;&#26368;&#20248;&#30340;&#26102;&#20505;&#65292;EDT&#36890;&#36807;&#20445;&#25345;&#36739;&#38271;&#30340;&#21382;&#21490;&#65292;&#24403;&#24403;&#21069;&#36712;&#36857;&#26159;&#27425;&#20248;&#30340;&#26102;&#20505;&#65292;EDT&#36890;&#36807;&#20445;&#25345;&#36739;&#30701;&#30340;&#21382;&#21490;&#26469;&#20248;&#21270;&#36712;&#36857;&#65292;&#20351;&#20854;&#33021;&#22815;&#19982;&#26356;&#20248;&#30340;&#36712;&#36857;&#36827;&#34892;&#8220;&#25340;&#25509;&#8221;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;EDT&#33021;&#22815;&#22635;&#34917;&#22522;&#20110;DT&#21644;&#22522;&#20110;Q-Learning&#26041;&#27861;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#29305;&#21035;&#26159;&#65292;EDT&#22312;&#22810;&#20219;&#21153;&#24773;&#20917;&#19979;&#32988;&#36807;&#22522;&#20110;Q-Learning&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces Elastic Decision Transformer (EDT), a significant advancement over the existing Decision Transformer (DT) and its variants. Although DT purports to generate an optimal trajectory, empirical evidence suggests it struggles with trajectory stitching, a process involving the generation of an optimal or near-optimal trajectory from the best parts of a set of sub-optimal trajectories. The proposed EDT differentiates itself by facilitating trajectory stitching during action inference at test time, achieved by adjusting the history length maintained in DT. Further, the EDT optimizes the trajectory by retaining a longer history when the previous trajectory is optimal and a shorter one when it is sub-optimal, enabling it to "stitch" with a more optimal trajectory. Extensive experimentation demonstrates EDT's ability to bridge the performance gap between DT-based and Q Learning-based approaches. In particular, the EDT outperforms Q Learning-based methods in a multi-task regi
&lt;/p&gt;</description></item><item><title>GIO&#26159;&#19968;&#31181;&#21487;&#20280;&#32553;&#19988;&#20219;&#21153;&#19981;&#21487;&#30693;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#30446;&#26631;&#30340;&#31616;&#21333;&#25918;&#26494;&#21644;&#39640;&#25928;&#23454;&#29616;&#65292;&#21487;&#20197;&#22312;&#38750;&#24120;&#23567;&#30340;&#35757;&#32451;&#38598;&#19978;&#20135;&#29983;&#20986;&#33394;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.11670</link><description>&lt;p&gt;
GIO&#65306;&#29992;&#20110;&#35757;&#32451;&#25968;&#25454;&#38598;&#36873;&#25321;&#30340;&#26799;&#24230;&#20449;&#24687;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
GIO: Gradient Information Optimization for Training Dataset Selection. (arXiv:2306.11670v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11670
&lt;/p&gt;
&lt;p&gt;
GIO&#26159;&#19968;&#31181;&#21487;&#20280;&#32553;&#19988;&#20219;&#21153;&#19981;&#21487;&#30693;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#30446;&#26631;&#30340;&#31616;&#21333;&#25918;&#26494;&#21644;&#39640;&#25928;&#23454;&#29616;&#65292;&#21487;&#20197;&#22312;&#38750;&#24120;&#23567;&#30340;&#35757;&#32451;&#38598;&#19978;&#20135;&#29983;&#20986;&#33394;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35757;&#32451;&#27169;&#22411;&#26102;&#65292;&#36890;&#24120;&#26377;&#21033;&#20110;&#22312;&#21487;&#29992;&#35757;&#32451;&#26679;&#26412;&#30340;&#23376;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#22240;&#20026;&#36825;&#20123;&#26679;&#26412;&#20855;&#26377;&#19981;&#21516;&#30340;&#36136;&#37327;&#65292;&#25110;&#32773;&#24076;&#26395;&#22312;&#19981;&#24433;&#21709;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#26356;&#23569;&#30340;&#26679;&#26412;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#26799;&#24230;&#20449;&#24687;&#20248;&#21270;&#65288;GIO&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21487;&#20280;&#32553;&#19988;&#20219;&#21153;&#19981;&#21487;&#30693;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#36825;&#20010;&#25968;&#25454;&#36873;&#25321;&#38382;&#39064;&#65292;&#23427;&#21482;&#38656;&#35201;&#19968;&#23567;&#32452;&#65288;&#26410;&#26631;&#35760;&#30340;&#65289;&#26679;&#26412;&#26469;&#20195;&#34920;&#30446;&#26631;&#20998;&#24067;&#12290;GIO&#20174;&#19968;&#20010;&#23454;&#36341;&#20013;&#38590;&#20197;&#22788;&#29702;&#30340;&#33258;&#28982;&#30340;&#20449;&#24687;&#29702;&#35770;&#30446;&#26631;&#24320;&#22987;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#22312;&#20110;&#23637;&#31034;&#20986;&#36890;&#36807;&#23545;&#30446;&#26631;&#36827;&#34892;&#31616;&#21333;&#30340;&#25918;&#26494;&#21644;&#39640;&#25928;&#30340;&#23454;&#29616;&#65292;&#23427;&#21487;&#20197;&#34987;&#39640;&#24230;&#25193;&#23637;&#12290;&#22312;&#26426;&#22120;&#32763;&#35793;&#12289;&#25340;&#20889;&#32416;&#27491;&#21644;&#22270;&#20687;&#35782;&#21035;&#31561;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;GIO&#22312;&#38750;&#24120;&#23567;&#30340;&#35757;&#32451;&#38598;&#19978;&#20135;&#29983;&#20102;&#20986;&#33394;&#30340;&#32467;&#26524;&#12290;&#36825;&#20123;&#21457;&#29616;&#23545;&#20110;GIO&#26412;&#36523;&#30340;&#19981;&#21516;&#34920;&#31034;&#27169;&#22411;&#21644;&#36229;&#21442;&#25968;&#26159;&#31283;&#20581;&#30340;&#12290;GIO&#26159;&#20219;&#21153;&#21644;&#39046;&#22495;&#26080;&#20851;&#30340;&#65292;&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#26032;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is often advantageous to train models on a subset of the available train examples, because the examples are of variable quality or because one would like to train with fewer examples, without sacrificing performance. We present Gradient Information Optimization (GIO), a scalable, task-agnostic approach to this data selection problem that requires only a small set of (unlabeled) examples representing a target distribution. GIO begins from a natural, information-theoretic objective that is intractable in practice. Our contribution is in showing that it can be made highly scalable through a simple relaxation of the objective and a highly efficient implementation. In experiments with machine translation, spelling correction, and image recognition, we show that GIO delivers outstanding results with very small train sets. These findings are robust to different representation models and hyperparameters for GIO itself. GIO is task- and domain-agnostic and can be applied out-of-the-box to ne
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#37096;&#20998;&#22312;&#32447;&#29366;&#24577;&#20449;&#24687;&#30340;POMDP&#38382;&#39064;&#30340;&#29702;&#35770;&#22256;&#38590;&#24615;&#21644;&#21487;&#35745;&#31639;&#24615;&#12290;&#20316;&#32773;&#36890;&#36807;&#24314;&#31435;&#19979;&#30028;&#24471;&#20986;&#19968;&#20010;&#24778;&#20154;&#30340;&#38590;&#24230;&#32467;&#26524;&#65306;&#38500;&#38750;&#20855;&#26377;&#23436;&#25972;&#30340;&#22312;&#32447;&#29366;&#24577;&#20449;&#24687;&#65292;&#21542;&#21017;&#38656;&#35201;&#25351;&#25968;&#32423;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#25165;&#33021;&#24471;&#21040;POMDP&#30340;&#26368;&#20248;&#31574;&#30053;&#35299;&#12290;&#28982;&#32780;&#65292;&#20316;&#32773;&#36824;&#21457;&#29616;&#20102;&#20855;&#26377;&#37096;&#20998;&#22312;&#32447;&#29366;&#24577;&#20449;&#24687;&#19979;&#30340;&#21487;&#35745;&#31639;POMDP&#31867;&#21035;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#31639;&#27861;&#26469;&#35777;&#26126;&#20854;&#25509;&#36817;&#26368;&#20248;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.08762</link><description>&lt;p&gt;
&#22312;&#20855;&#26377;&#37096;&#20998;&#22312;&#32447;&#29366;&#24577;&#20449;&#24687;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;POMDP&#30340;&#29702;&#35770;&#38590;&#24230;&#21644;&#21487;&#35745;&#31639;&#24615;
&lt;/p&gt;
&lt;p&gt;
Theoretical Hardness and Tractability of POMDPs in RL with Partial Online State Information. (arXiv:2306.08762v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08762
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#37096;&#20998;&#22312;&#32447;&#29366;&#24577;&#20449;&#24687;&#30340;POMDP&#38382;&#39064;&#30340;&#29702;&#35770;&#22256;&#38590;&#24615;&#21644;&#21487;&#35745;&#31639;&#24615;&#12290;&#20316;&#32773;&#36890;&#36807;&#24314;&#31435;&#19979;&#30028;&#24471;&#20986;&#19968;&#20010;&#24778;&#20154;&#30340;&#38590;&#24230;&#32467;&#26524;&#65306;&#38500;&#38750;&#20855;&#26377;&#23436;&#25972;&#30340;&#22312;&#32447;&#29366;&#24577;&#20449;&#24687;&#65292;&#21542;&#21017;&#38656;&#35201;&#25351;&#25968;&#32423;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#25165;&#33021;&#24471;&#21040;POMDP&#30340;&#26368;&#20248;&#31574;&#30053;&#35299;&#12290;&#28982;&#32780;&#65292;&#20316;&#32773;&#36824;&#21457;&#29616;&#20102;&#20855;&#26377;&#37096;&#20998;&#22312;&#32447;&#29366;&#24577;&#20449;&#24687;&#19979;&#30340;&#21487;&#35745;&#31639;POMDP&#31867;&#21035;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#31639;&#27861;&#26469;&#35777;&#26126;&#20854;&#25509;&#36817;&#26368;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDP&#65289;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#25429;&#25417;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#29702;&#35770;&#32467;&#26524;&#24050;&#32463;&#34920;&#26126;&#65292;&#22312;&#19968;&#33324;&#30340;POMDP&#20013;&#23398;&#20064;&#21487;&#33021;&#26159;&#19981;&#21487;&#35745;&#31639;&#30340;&#65292;&#20027;&#35201;&#25361;&#25112;&#22312;&#20110;&#32570;&#20047;&#28508;&#22312;&#30340;&#29366;&#24577;&#20449;&#24687;&#12290;&#19968;&#20010;&#20851;&#38190;&#30340;&#22522;&#26412;&#38382;&#39064;&#26159;&#26377;&#22810;&#23569;&#22312;&#32447;&#29366;&#24577;&#20449;&#24687;&#65288;OSI&#65289;&#36275;&#20197;&#23454;&#29616;&#21487;&#35745;&#31639;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#19979;&#30028;&#65292;&#25581;&#31034;&#20102;&#19968;&#20010;&#24778;&#20154;&#30340;&#38590;&#24230;&#32467;&#26524;&#65306;&#38500;&#38750;&#25105;&#20204;&#20855;&#26377;&#23436;&#25972;&#30340;OSI&#65292;&#21542;&#21017;&#25105;&#20204;&#38656;&#35201;&#25351;&#25968;&#32423;&#30340;&#37319;&#26679;&#22797;&#26434;&#24230;&#25165;&#33021;&#33719;&#24471;POMDP&#30340;$\epsilon$-&#26368;&#20248;&#31574;&#30053;&#35299;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#21463;&#21040;&#25105;&#20204;&#19979;&#30028;&#35774;&#35745;&#30340;&#20851;&#38190;&#35265;&#35299;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#21457;&#29616;&#21363;&#20351;&#21482;&#26377;&#37096;&#20998;OSI&#65292;&#20063;&#23384;&#22312;&#37325;&#35201;&#30340;&#21487;&#35745;&#31639;&#30340;POMDP&#31867;&#21035;&#12290;&#29305;&#21035;&#22320;&#65292;&#23545;&#20110;&#20855;&#26377;&#37096;&#20998;OSI&#30340;&#20004;&#20010;&#26032;&#39062;&#30340;POMDP&#31867;&#21035;&#65292;&#25105;&#20204;&#36890;&#36807;&#24314;&#31435;&#26032;&#30340;&#36951;&#25022;&#19978;&#19979;&#30028;&#35777;&#26126;&#20102;&#26032;&#30340;&#31639;&#27861;&#26159;&#25509;&#36817;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Partially observable Markov decision processes (POMDPs) have been widely applied to capture many real-world applications. However, existing theoretical results have shown that learning in general POMDPs could be intractable, where the main challenge lies in the lack of latent state information. A key fundamental question here is how much online state information (OSI) is sufficient to achieve tractability. In this paper, we establish a lower bound that reveals a surprising hardness result: unless we have full OSI, we need an exponentially scaling sample complexity to obtain an $\epsilon$-optimal policy solution for POMDPs. Nonetheless, inspired by the key insights in our lower bound design, we find that there exist important tractable classes of POMDPs even with only partial OSI. In particular, for two novel classes of POMDPs with partial OSI, we provide new algorithms that are proved to be near-optimal by establishing new regret upper and lower bounds.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#26080;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;UnDiff&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#23398;&#20064;&#35299;&#20915;&#20102;&#35821;&#38899;&#36870;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#35821;&#38899;&#22788;&#29702;&#20219;&#21153;&#65292;&#24182;&#22312;&#24102;&#23485;&#25193;&#23637;&#12289;&#21435;&#21098;&#36753;&#12289;&#32534;&#30721;&#21644;&#35821;&#38899;&#28304;&#20998;&#31163;&#31561;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2306.00721</link><description>&lt;p&gt;
UnDiff: &#26080;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#19979;&#30340;&#26080;&#30417;&#30563;&#35821;&#38899;&#20462;&#22797;
&lt;/p&gt;
&lt;p&gt;
UnDiff: Unsupervised Voice Restoration with Unconditional Diffusion Model. (arXiv:2306.00721v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00721
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#26080;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;UnDiff&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#23398;&#20064;&#35299;&#20915;&#20102;&#35821;&#38899;&#36870;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#35821;&#38899;&#22788;&#29702;&#20219;&#21153;&#65292;&#24182;&#22312;&#24102;&#23485;&#25193;&#23637;&#12289;&#21435;&#21098;&#36753;&#12289;&#32534;&#30721;&#21644;&#35821;&#38899;&#28304;&#20998;&#31163;&#31561;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;UnDiff&#65292;&#19968;&#31181;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65292;&#33021;&#22815;&#35299;&#20915;&#21508;&#31181;&#35821;&#38899;&#36870;&#38382;&#39064;&#12290;&#23427;&#36890;&#36807;&#26080;&#26465;&#20214;&#26041;&#24335;&#35757;&#32451;&#35821;&#38899;&#27874;&#24418;&#29983;&#25104;&#65292;&#21487;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#21253;&#25324;&#38477;&#32423;&#36870;&#25512;&#12289;&#31070;&#32463;&#32534;&#30721;&#21644;&#28304;&#20998;&#31163;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#23545;&#27604;&#19981;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#21644;&#39044;&#26465;&#20214;&#22495;&#65292;&#35299;&#20915;&#20102;&#26080;&#26465;&#20214;&#27874;&#24418;&#29983;&#25104;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#12290;&#20043;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#26368;&#26032;&#30340;&#21518;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#30340;&#26465;&#20214;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#35757;&#32451;&#22909;&#30340;&#26080;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#22914;&#20309;&#36866;&#24212;&#35821;&#38899;&#22788;&#29702;&#30340;&#19981;&#21516;&#20219;&#21153;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#25216;&#26415;&#22312;&#24102;&#23485;&#25193;&#23637;&#12289;&#21435;&#21098;&#36753;&#12289;&#32534;&#30721;&#21644;&#35821;&#38899;&#28304;&#20998;&#31163;&#31561;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#19982;&#22522;&#20934;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#28304;&#20195;&#30721;&#20844;&#24320;&#21487;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces UnDiff, a diffusion probabilistic model capable of solving various speech inverse tasks. Being once trained for speech waveform generation in an unconditional manner, it can be adapted to different tasks including degradation inversion, neural vocoding, and source separation. In this paper, we, first, tackle the challenging problem of unconditional waveform generation by comparing different neural architectures and preconditioning domains. After that, we demonstrate how the trained unconditional diffusion could be adapted to different tasks of speech processing by the means of recent developments in post-training conditioning of diffusion models. Finally, we demonstrate the performance of the proposed technique on the tasks of bandwidth extension, declipping, vocoding, and speech source separation and compare it to the baselines. The codes are publicly available.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#25991;&#29486;&#30340;&#21457;&#29616;&#26041;&#27861;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#21270;&#30340;&#23398;&#20064;&#29983;&#25104;&#26032;&#30340;&#31185;&#23398;&#26041;&#21521;&#65292;&#20811;&#26381;&#20102;&#26631;&#20934;&#26041;&#27861;&#22312;&#39044;&#27979;&#20851;&#32852;&#12289;&#24573;&#30053;&#19978;&#19979;&#25991;&#31561;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;&#27169;&#22411;&#20351;&#29992;&#20102;&#24341;&#25991;&#21644;&#30693;&#35782;&#22270;&#20851;&#31995;&#30340;&#32593;&#32476;&#65292;&#24182;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#65292;&#21457;&#29616;GPT4&#22312;&#29983;&#25104;&#21019;&#26032;&#24605;&#24819;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2305.14259</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#25991;&#29486;&#30340;&#35821;&#22659;&#21270;&#23398;&#20064;&#29983;&#25104;&#26032;&#30340;&#31185;&#23398;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Learning to Generate Novel Scientific Directions with Contextualized Literature-based Discovery. (arXiv:2305.14259v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14259
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#25991;&#29486;&#30340;&#21457;&#29616;&#26041;&#27861;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#21270;&#30340;&#23398;&#20064;&#29983;&#25104;&#26032;&#30340;&#31185;&#23398;&#26041;&#21521;&#65292;&#20811;&#26381;&#20102;&#26631;&#20934;&#26041;&#27861;&#22312;&#39044;&#27979;&#20851;&#32852;&#12289;&#24573;&#30053;&#19978;&#19979;&#25991;&#31561;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;&#27169;&#22411;&#20351;&#29992;&#20102;&#24341;&#25991;&#21644;&#30693;&#35782;&#22270;&#20851;&#31995;&#30340;&#32593;&#32476;&#65292;&#24182;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#65292;&#21457;&#29616;GPT4&#22312;&#29983;&#25104;&#21019;&#26032;&#24605;&#24819;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25991;&#29486;&#30340;&#21457;&#29616;&#65288;LBD&#65289;&#26088;&#22312;&#36890;&#36807;&#25366;&#25496;&#35770;&#25991;&#24182;&#29983;&#25104;&#20551;&#35774;&#26469;&#21457;&#29616;&#26032;&#30340;&#31185;&#23398;&#30693;&#35782;&#12290;&#26631;&#20934;&#30340;LBD&#20165;&#38480;&#20110;&#39044;&#27979;&#31163;&#25955;&#27010;&#24565;&#20043;&#38388;&#30340;&#20004;&#20004;&#20851;&#31995;&#65288;&#20363;&#22914;&#65292;&#33647;&#29289;&#21644;&#30142;&#30149;&#30340;&#20851;&#32852;&#65289;&#12290;LBD&#36824;&#24573;&#30053;&#20102;&#20851;&#38190;&#30340;&#19978;&#19979;&#25991;&#65292;&#20363;&#22914;&#23454;&#39564;&#35774;&#32622;&#65288;&#20363;&#22914;&#65292;&#33647;&#29289;&#35780;&#20272;&#30340;&#29305;&#23450;&#24739;&#32773;&#32676;&#20307;&#65289;&#21644;&#20154;&#31867;&#31185;&#23398;&#23478;&#32771;&#34385;&#30340;&#32972;&#26223;&#30693;&#35782;&#21644;&#21160;&#26426;&#65288;&#20363;&#22914;&#65292;&#25214;&#21040;&#27809;&#26377;&#29305;&#23450;&#21103;&#20316;&#29992;&#30340;&#33647;&#29289;&#20505;&#36873;&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#19978;&#19979;&#25991;&#21270;LBD&#65288;C-LBD&#65289;&#34920;&#36848;&#26469;&#35299;&#20915;&#36825;&#20123;&#23616;&#38480;&#24615;&#65306;&#20197;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#31185;&#23398;&#20551;&#35774;&#65292;&#21516;&#26102;&#23558;&#23427;&#20204;&#32852;&#31995;&#21040;&#25511;&#21046;&#20551;&#35774;&#25628;&#32034;&#31354;&#38388;&#30340;&#19978;&#19979;&#25991;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24314;&#27169;&#26694;&#26550;&#65292;&#20351;&#29992;&#33719;&#24471;&#30340;&#24341;&#25991;&#21644;&#30693;&#35782;&#22270;&#20851;&#31995;&#30340;&#24322;&#26500;&#32593;&#32476;&#20013;&#30340;&#8220;&#28789;&#24863;&#8221;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#20174;&#35770;&#25991;&#20013;&#27966;&#29983;&#30340;&#26032;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20351;&#29992;&#24378;&#22823;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#35780;&#20272;&#65292;&#21457;&#29616;GPT4&#20542;&#21521;&#20110;&#29983;&#25104;&#20855;&#26377;&#21019;&#26032;&#24615;&#30340;&#24605;&#24819;&#12290;
&lt;/p&gt;
&lt;p&gt;
Literature-Based Discovery (LBD) aims to discover new scientific knowledge by mining papers and generating hypotheses. Standard LBD is limited to predicting pairwise relations between discrete concepts (e.g., drug-disease links). LBD also ignores critical contexts like experimental settings (e.g., a specific patient population where a drug is evaluated) and background knowledge and motivations that human scientists consider (e.g., to find a drug candidate without specific side effects). We address these limitations with a novel formulation of contextualized-LBD (C-LBD): generating scientific hypotheses in natural language, while grounding them in a context that controls the hypothesis search space. We present a modeling framework using retrieval of ``inspirations'' from a heterogeneous network of citations and knowledge graph relations, and create a new dataset derived from papers. Our evaluations with powerful large language models (LLMs) reveal that GPT4 tends to generate ideas with 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#21160;&#23450;&#29702;&#35777;&#26126;&#22120;&#21644;&#22768;&#26126;&#24615;&#20219;&#21153;&#35268;&#33539;&#30340;&#21487;&#28385;&#36275;&#24615;&#36741;&#21161;&#35821;&#35328;&#24314;&#27169;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.09656</link><description>&lt;p&gt;
&#22768;&#26126;&#25552;&#31034;&#19979;&#30340;&#21487;&#28385;&#36275;&#24615;&#36741;&#21161;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Satisfiability-Aided Language Models Using Declarative Prompting. (arXiv:2305.09656v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09656
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#21160;&#23450;&#29702;&#35777;&#26126;&#22120;&#21644;&#22768;&#26126;&#24615;&#20219;&#21153;&#35268;&#33539;&#30340;&#21487;&#28385;&#36275;&#24615;&#36741;&#21161;&#35821;&#35328;&#24314;&#27169;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#28385;&#36275;&#24615;&#36741;&#21161;&#35821;&#35328;&#24314;&#27169;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#19968;&#20010;&#22768;&#26126;&#24615;&#20219;&#21153;&#35268;&#33539;&#65292;&#24182;&#21033;&#29992;&#19968;&#20010;&#29616;&#25104;&#30340;&#33258;&#21160;&#23450;&#29702;&#35777;&#26126;&#22120;&#24471;&#20986;&#26368;&#32456;&#31572;&#26696;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#20004;&#20010;&#20851;&#38190;&#20248;&#28857;&#65306;&#31532;&#19968;&#65292;&#22768;&#26126;&#24615;&#35268;&#33539;&#27604;&#25512;&#29702;&#27493;&#39588;&#26356;&#25509;&#36817;&#38382;&#39064;&#25551;&#36848;&#65292;&#22240;&#27492;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#35299;&#26512;&#23427;&#65307;&#31532;&#20108;&#65292;&#36890;&#36807;&#23558;&#23454;&#38469;&#25512;&#29702;&#20219;&#21153;&#22996;&#25176;&#32473;&#33258;&#21160;&#23450;&#29702;&#35777;&#26126;&#22120;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20445;&#35777;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prior work has combined chain-of-thought prompting in large language models (LLMs) with programmatic representations to perform effective and transparent reasoning. While such an approach works very well for tasks that only require forward reasoning (e.g., straightforward arithmetic), it is less effective for constraint solving tasks that require more sophisticated planning and search. In this paper, we propose a new satisfiability-aided language modeling approach for improving the reasoning capabilities of LLMs. We use an LLM to generate a declarative task specification rather than an imperative program and leverage an off-the-shelf automated theorem prover to derive the final answer. This approach has two key advantages. The declarative specification is closer to the problem description than the reasoning steps are, so the LLM can parse it more accurately. Furthermore, by offloading the actual reasoning task to an automated theorem prover, our approach can guarantee the correctness o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;ChatGPT&#30340;&#22240;&#26524;&#25512;&#29702;&#33021;&#21147;&#36827;&#34892;&#20102;&#39318;&#27425;&#20840;&#38754;&#35780;&#20272;&#65292;&#23454;&#39564;&#35777;&#26126;ChatGPT&#26159;&#19968;&#20010;&#22909;&#30340;&#22240;&#26524;&#35299;&#37322;&#32773;&#65292;&#20294;&#19981;&#26159;&#19968;&#20010;&#22909;&#30340;&#22240;&#26524;&#25512;&#29702;&#32773;&#65292;&#23384;&#22312;&#20005;&#37325;&#30340;&#22240;&#26524;&#24187;&#35273;&#38382;&#39064;&#65292;&#23545;&#20110;&#26126;&#30830;&#30340;&#22240;&#26524;&#20851;&#31995;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.07375</link><description>&lt;p&gt;
ChatGPT&#26159;&#19968;&#20010;&#22909;&#30340;&#22240;&#26524;&#25512;&#26029;&#22120;&#21527;&#65311;&#20840;&#38754;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Is ChatGPT a Good Causal Reasoner? A Comprehensive Evaluation. (arXiv:2305.07375v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07375
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;ChatGPT&#30340;&#22240;&#26524;&#25512;&#29702;&#33021;&#21147;&#36827;&#34892;&#20102;&#39318;&#27425;&#20840;&#38754;&#35780;&#20272;&#65292;&#23454;&#39564;&#35777;&#26126;ChatGPT&#26159;&#19968;&#20010;&#22909;&#30340;&#22240;&#26524;&#35299;&#37322;&#32773;&#65292;&#20294;&#19981;&#26159;&#19968;&#20010;&#22909;&#30340;&#22240;&#26524;&#25512;&#29702;&#32773;&#65292;&#23384;&#22312;&#20005;&#37325;&#30340;&#22240;&#26524;&#24187;&#35273;&#38382;&#39064;&#65292;&#23545;&#20110;&#26126;&#30830;&#30340;&#22240;&#26524;&#20851;&#31995;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#25512;&#29702;&#33021;&#21147;&#23545;&#20110;&#20247;&#22810;NLP&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;ChatGPT&#22312;&#21508;&#31181;NLP&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#26032;&#20852;&#33021;&#21147;&#65292;&#20294;ChatGPT&#22312;&#22240;&#26524;&#25512;&#29702;&#26041;&#38754;&#30340;&#34920;&#29616;&#22914;&#20309;&#20173;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#23545;ChatGPT&#30340;&#22240;&#26524;&#25512;&#29702;&#33021;&#21147;&#36827;&#34892;&#20102;&#39318;&#27425;&#20840;&#38754;&#35780;&#20272;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;ChatGPT&#19981;&#26159;&#19968;&#20010;&#22909;&#30340;&#22240;&#26524;&#25512;&#29702;&#32773;&#65292;&#20294;&#26159;&#26159;&#19968;&#20010;&#22909;&#30340;&#22240;&#26524;&#35299;&#37322;&#32773;&#12290;&#27492;&#22806;&#65292;ChatGPT&#22312;&#22240;&#26524;&#25512;&#29702;&#26041;&#38754;&#23384;&#22312;&#20005;&#37325;&#30340;&#24187;&#35273;&#65292;&#21487;&#33021;&#26159;&#30001;&#20110;&#33258;&#28982;&#35821;&#35328;&#20013;&#22240;&#26524;&#20851;&#31995;&#21644;&#38750;&#22240;&#26524;&#20851;&#31995;&#30340;&#25253;&#21578;&#20559;&#35265;&#65292;&#20197;&#21450;ChatGPT&#30340;&#21319;&#32423;&#36807;&#31243;&#65292;&#22914;RLHF&#12290;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#21644;&#24605;&#32500;&#38142;&#65288;COT&#65289;&#25216;&#26415;&#26041;&#38754;&#65292;&#21487;&#33021;&#20250;&#36827;&#19968;&#27493;&#21152;&#21095;&#36825;&#31181;&#22240;&#26524;&#24187;&#35273;&#12290;&#27492;&#22806;&#65292;ChatGPT&#30340;&#22240;&#26524;&#25512;&#29702;&#33021;&#21147;&#23545;&#20110;&#22312;&#25552;&#31034;&#20013;&#34920;&#36798;&#22240;&#26524;&#27010;&#24565;&#30340;&#35789;&#35821;&#38750;&#24120;&#25935;&#24863;&#65292;&#24182;&#19988;&#23553;&#38381;&#25552;&#31034;&#27604;&#24320;&#25918;&#25552;&#31034;&#34920;&#29616;&#26356;&#22909;&#12290;&#23545;&#20110;&#21477;&#23376;&#20013;&#30340;&#20107;&#20214;&#65292;ChatGPT&#25797;&#38271;&#25429;&#25417;&#26126;&#30830;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal reasoning ability is crucial for numerous NLP applications. Despite the impressive emerging ability of ChatGPT in various NLP tasks, it is unclear how well ChatGPT performs in causal reasoning. In this paper, we conduct the first comprehensive evaluation of the ChatGPT's causal reasoning capabilities. Experiments show that ChatGPT is not a good causal reasoner, but a good causal interpreter. Besides, ChatGPT has a serious hallucination on causal reasoning, possibly due to the reporting biases between causal and non-causal relationships in natural language, as well as ChatGPT's upgrading processes, such as RLHF. The In-Context Learning (ICL) and Chain-of-Though (COT) techniques can further exacerbate such causal hallucination. Additionally, the causal reasoning ability of ChatGPT is sensitive to the words used to express the causal concept in prompts, and close-ended prompts perform better than open-ended prompts. For events in sentences, ChatGPT excels at capturing explicit caus
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#24335;&#26041;&#27861;(TSDiff)&#65292;&#29992;&#20110;&#20174;&#20108;&#32500;&#20998;&#23376;&#22270;&#20013;&#39044;&#27979;&#36807;&#28193;&#24577;&#20960;&#20309;&#32467;&#26500;&#12290;&#19982;&#29616;&#26377;&#20855;&#26377; 3D &#20960;&#20309;&#32467;&#26500;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30456;&#27604;&#65292;TSdiff &#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#37117;&#26356;&#39640;&#12290;TSDiff &#33021;&#22815;&#25214;&#21040;&#27604;&#21442;&#32771;&#25968;&#25454;&#24211;&#26356;&#20248;&#21270;&#30340;&#21453;&#24212;&#36884;&#24452;&#65292;&#22312;&#21453;&#24212;&#21183;&#22418;&#26356;&#20302;&#30340;&#24773;&#20917;&#19979;&#25214;&#21040;&#26356;&#20026;&#20248;&#21270;&#30340;&#21453;&#24212;&#36335;&#24452;&#12290;</title><link>http://arxiv.org/abs/2304.12233</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#25506;&#32034;&#20108;&#32500;&#20998;&#23376;&#22270;&#30340;&#36807;&#28193;&#24577;&#19978;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Diffusion-based Generative AI for Exploring Transition States from 2D Molecular Graphs. (arXiv:2304.12233v2 [physics.chem-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12233
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#24335;&#26041;&#27861;(TSDiff)&#65292;&#29992;&#20110;&#20174;&#20108;&#32500;&#20998;&#23376;&#22270;&#20013;&#39044;&#27979;&#36807;&#28193;&#24577;&#20960;&#20309;&#32467;&#26500;&#12290;&#19982;&#29616;&#26377;&#20855;&#26377; 3D &#20960;&#20309;&#32467;&#26500;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30456;&#27604;&#65292;TSdiff &#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#37117;&#26356;&#39640;&#12290;TSDiff &#33021;&#22815;&#25214;&#21040;&#27604;&#21442;&#32771;&#25968;&#25454;&#24211;&#26356;&#20248;&#21270;&#30340;&#21453;&#24212;&#36884;&#24452;&#65292;&#22312;&#21453;&#24212;&#21183;&#22418;&#26356;&#20302;&#30340;&#24773;&#20917;&#19979;&#25214;&#21040;&#26356;&#20026;&#20248;&#21270;&#30340;&#21453;&#24212;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25506;&#31350;&#36807;&#28193;&#24577;&#20960;&#20309;&#32467;&#26500;&#23545;&#20110;&#38416;&#26126;&#21270;&#23398;&#21453;&#24212;&#26426;&#29702;&#21644;&#27169;&#25311;&#21453;&#24212;&#21160;&#21147;&#23398;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#39044;&#27979;&#36807;&#28193;&#24577;&#20960;&#20309;&#32467;&#26500;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#38656;&#35201;&#21453;&#24212;&#29289;&#21644;&#20135;&#29289;&#30340; 3D &#24418;&#24577;&#21644;&#26041;&#21521;&#20316;&#20026;&#36755;&#20837;&#65292;&#36825;&#38656;&#35201;&#22823;&#37327;&#30340;&#21162;&#21147;&#21644;&#35745;&#31639;&#25104;&#26412;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#25193;&#25955;&#26041;&#27861;&#30340;&#29983;&#25104;&#24335;&#26041;&#27861;(TSDiff)&#65292;&#29992;&#20110;&#20174;&#20108;&#32500;&#20998;&#23376;&#22270;&#20013;&#39044;&#27979;&#36807;&#28193;&#24577;&#20960;&#20309;&#32467;&#26500;&#12290;TSDiff&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#20855;&#26377; 3D &#20960;&#20309;&#32467;&#26500;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#23427;&#21487;&#20197;&#20174;&#35757;&#32451;&#20013;&#20102;&#35299;&#21040;&#21508;&#31181;&#21453;&#24212;&#30340;&#36807;&#28193;&#24577;&#20960;&#20309;&#20998;&#24067;&#65292;&#20174;&#32780;&#33021;&#22815;&#37319;&#26679;&#21508;&#31181;&#36807;&#28193;&#24577;&#26500;&#35937;&#12290;&#22240;&#27492;&#65292;TSDiff &#33021;&#22815;&#25214;&#21040;&#27604;&#21442;&#32771;&#25968;&#25454;&#24211;&#20013;&#26356;&#20026;&#26377;&#21033;&#30340;&#21453;&#24212;&#36884;&#24452;&#65292;&#22312;&#21453;&#24212;&#21183;&#22418;&#26356;&#20302;&#30340;&#24773;&#20917;&#19979;&#25214;&#21040;&#26356;&#20026;&#20248;&#21270;&#30340;&#21453;&#24212;&#36335;&#24452;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;TSDiff &#22312;&#21152;&#36895;&#21270;&#23398;&#21453;&#24212;&#21644;&#21453;&#24212;&#36884;&#24452;&#30340;&#21457;&#29616;&#26041;&#38754;&#20855;&#26377;&#28508;&#22312;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
The exploration of transition state (TS) geometries is crucial for elucidating chemical reaction mechanisms and modeling their kinetics. Recently, machine learning (ML) models have shown remarkable performance for prediction of TS geometries. However, they require 3D conformations of reactants and products often with their appropriate orientations as input, which demands substantial efforts and computational cost. Here, we propose a generative approach based on the stochastic diffusion method, namely TSDiff, for prediction of TS geometries just from 2D molecular graphs. TSDiff outperformed the existing ML models with 3D geometries in terms of both accuracy and efficiency. Moreover, it enables to sample various TS conformations, because it learned the distribution of TS geometries for diverse reactions in training. Thus, TSDiff was able to find more favorable reaction pathways with lower barrier heights than those in the reference database. These results demonstrate that TSDiff shows pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986; Iter-CoT &#26041;&#27861;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36827;&#34892;&#36845;&#20195;&#22686;&#24378;&#30340;&#24605;&#32500;&#38142;&#25552;&#31034;&#65292;&#36890;&#36807;&#36873;&#25321;&#20855;&#26377;&#36866;&#24230;&#38590;&#24230;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#20294;&#21487;&#22238;&#31572;&#30340;&#38382;&#39064;&#65292;&#24182;&#20276;&#38543;&#25512;&#29702;&#38142;&#20316;&#20026;&#31034;&#20363;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21516;&#26102;&#20351;&#27169;&#22411;&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#29983;&#25104;&#25512;&#29702;&#38142;&#12290;</title><link>http://arxiv.org/abs/2304.11657</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21152;&#24378;&#36845;&#20195;&#22686;&#24378;&#30340;&#24605;&#32500;&#38142;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Enhancing Chain-of-Thoughts Prompting with Iterative Bootstrapping in Large Language Models. (arXiv:2304.11657v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11657
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986; Iter-CoT &#26041;&#27861;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36827;&#34892;&#36845;&#20195;&#22686;&#24378;&#30340;&#24605;&#32500;&#38142;&#25552;&#31034;&#65292;&#36890;&#36807;&#36873;&#25321;&#20855;&#26377;&#36866;&#24230;&#38590;&#24230;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#20294;&#21487;&#22238;&#31572;&#30340;&#38382;&#39064;&#65292;&#24182;&#20276;&#38543;&#25512;&#29702;&#38142;&#20316;&#20026;&#31034;&#20363;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21516;&#26102;&#20351;&#27169;&#22411;&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#29983;&#25104;&#25512;&#29702;&#38142;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36880;&#27493;&#24341;&#23548;&#24605;&#32500;&#38142; (CoT) &#20316;&#20026;&#31034;&#33539;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#21487;&#20197;&#22312;&#21508;&#31181;&#25512;&#29702;&#20219;&#21153;&#19978;&#23454;&#29616;&#39640;&#24230;&#26377;&#25928;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;LLMs &#29983;&#25104;&#30340;&#28436;&#31034;&#25512;&#29702;&#38142;&#23481;&#26131;&#20986;&#29616;&#38169;&#35823;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#38169;&#35823;&#12290;&#27492;&#22806;&#65292;&#19981;&#24688;&#24403;&#30340;&#31034;&#20363; (&#36807;&#20110;&#31616;&#21333;&#25110;&#22797;&#26434;) &#21487;&#20197;&#24433;&#21709;&#22312;&#19981;&#21516;&#38590;&#24230;&#32423;&#21035;&#19979;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;Iter-CoT (&#36845;&#20195;&#24341;&#23548;&#24605;&#32500;&#38142;&#25552;&#31034;) &#30340;&#36845;&#20195;&#24341;&#23548;&#26041;&#27861;&#65292;&#29992;&#20110;&#36873;&#25321;&#23454;&#20363;&#24182;&#29983;&#25104;&#25512;&#29702;&#38142;&#12290;&#36890;&#36807;&#21033;&#29992;&#36845;&#20195;&#22686;&#24378;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;LLMs &#33258;&#20027;&#26356;&#27491;&#38169;&#35823;&#65292;&#20174;&#32780;&#20135;&#29983;&#26356;&#31934;&#30830;&#12289;&#20840;&#38754;&#30340;&#25512;&#29702;&#38142;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36873;&#25321;&#20855;&#26377;&#36866;&#24230;&#38590;&#24230;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#20294;&#21487;&#22238;&#31572;&#30340;&#38382;&#39064;&#65292;&#24182;&#20276;&#38543;&#25512;&#29702;&#38142;&#20316;&#20026;&#31034;&#20363;&#65292;&#20174;&#32780;&#22686;&#24378;LLMs &#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) can achieve highly effective performance on various reasoning tasks by incorporating step-by-step chain-of-thought (CoT) prompting as demonstrations. However, the reasoning chains of demonstrations generated by LLMs are prone to errors, which can subsequently lead to incorrect reasoning during inference. Furthermore, inappropriate exemplars (overly simplistic or complex), can affect overall performance among varying levels of difficulty. We introduce Iter-CoT (Iterative bootstrapping in Chain-of-Thoughts Prompting), an iterative bootstrapping approach for selecting exemplars and generating reasoning chains. By utilizing iterative bootstrapping, our approach enables LLMs to autonomously rectify errors, resulting in more precise and comprehensive reasoning chains. Simultaneously, our approach selects challenging yet answerable questions accompanied by reasoning chains as exemplars with a moderate level of difficulty, which enhances the LLMs' generalizability 
&lt;/p&gt;</description></item><item><title>LLMMaps&#26159;&#19968;&#31181;&#20998;&#23618;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#21487;&#35270;&#21270;&#25216;&#26415;&#65292;&#33021;&#22815;&#25581;&#31034;&#21462;&#24471;&#39640;&#20934;&#30830;&#24230;&#21644;&#20135;&#29983;&#24187;&#35273;&#30340;&#23376;&#39046;&#22495;&#65292;&#24182;&#25351;&#23548;&#27169;&#22411;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2304.00457</link><description>&lt;p&gt;
LLMMaps&#8212;&#8212;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20998;&#23618;&#35780;&#20215;&#30340;&#21487;&#35270;&#21270;&#38544;&#21947;
&lt;/p&gt;
&lt;p&gt;
LLMMaps -- A Visual Metaphor for Stratified Evaluation of Large Language Models. (arXiv:2304.00457v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00457
&lt;/p&gt;
&lt;p&gt;
LLMMaps&#26159;&#19968;&#31181;&#20998;&#23618;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#21487;&#35270;&#21270;&#25216;&#26415;&#65292;&#33021;&#22815;&#25581;&#31034;&#21462;&#24471;&#39640;&#20934;&#30830;&#24230;&#21644;&#20135;&#29983;&#24187;&#35273;&#30340;&#23376;&#39046;&#22495;&#65292;&#24182;&#25351;&#23548;&#27169;&#22411;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#21462;&#24471;&#20102;&#38761;&#21629;&#24615;&#30340;&#36827;&#23637;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#24778;&#20154;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23481;&#26131;&#20135;&#29983;&#24187;&#35273;&#65292;&#21363;&#27169;&#22411;&#22312;&#21709;&#24212;&#20013;&#26292;&#38706;&#20986;&#19981;&#27491;&#30830;&#25110;&#38169;&#35823;&#30340;&#20449;&#24687;&#65292;&#36825;&#20351;&#24471;&#24517;&#39035;&#37319;&#29992;&#21220;&#22859;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;&#34429;&#28982;LLM&#22312;&#29305;&#23450;&#30693;&#35782;&#39046;&#22495;&#20013;&#30340;&#34920;&#29616;&#36890;&#24120;&#26159;&#22522;&#20110;&#38382;&#31572;(Q&amp;A)&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#65292;&#20294;&#36825;&#20123;&#35780;&#20272;&#36890;&#24120;&#20165;&#25253;&#21578;&#25972;&#20010;&#39046;&#22495;&#30340;&#21333;&#20010;&#20934;&#30830;&#24230;&#25968;&#23383;&#65292;&#36825;&#19968;&#31243;&#24207;&#22312;&#36879;&#26126;&#24230;&#21644;&#27169;&#22411;&#25913;&#36827;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#12290;&#20998;&#23618;&#35780;&#20272;&#21487;&#20197;&#25581;&#31034;&#21487;&#33021;&#26356;&#23481;&#26131;&#21457;&#29983;&#24187;&#35273;&#30340;&#23376;&#39046;&#22495;&#65292;&#20174;&#32780;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#35780;&#20272;LLMs&#30340;&#39118;&#38505;&#24182;&#25351;&#23548;&#23427;&#20204;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#12290;&#20026;&#25903;&#25345;&#36825;&#26679;&#30340;&#20998;&#23618;&#35780;&#20272;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LLMMaps&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#21487;&#35270;&#21270;&#25216;&#26415;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#26681;&#25454;Q&amp;A&#25968;&#25454;&#38598;&#35780;&#20272;LLMs&#30340;&#24615;&#33021;&#12290;LLMMaps&#25552;&#20379;&#20102;&#23545;LLMs&#22312;&#19981;&#21516;&#23376;&#39046;&#22495;&#20013;&#30340;&#30693;&#35782;&#20998;&#24067;&#30340;&#35814;&#32454;&#27934;&#23519;&#65292;&#20801;&#35768;&#29992;&#25143;&#25918;&#22823;&#39046;&#22495;&#30340;&#29305;&#23450;&#37096;&#20998;&#24182;&#25506;&#32034;&#27169;&#22411;&#24615;&#33021;&#19978;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;LLMMaps&#26377;&#21161;&#20110;&#35782;&#21035;&#20986;&#26356;&#23481;&#26131;&#20986;&#29616;LLM&#24187;&#35273;&#30340;&#23376;&#39046;&#22495;&#65292;&#24182;&#21487;&#20197;&#25351;&#23548;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#20197;&#25913;&#21892;&#36825;&#20123;&#39046;&#22495;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have revolutionized natural language processing and demonstrated impressive capabilities in various tasks. Unfortunately, they are prone to hallucinations, where the model exposes incorrect or false information in its responses, which renders diligent evaluation approaches mandatory. While LLM performance in specific knowledge fields is often evaluated based on question and answer (Q&amp;A) datasets, such evaluations usually report only a single accuracy number for the entire field, a procedure which is problematic with respect to transparency and model improvement. A stratified evaluation could instead reveal subfields, where hallucinations are more likely to occur and thus help to better assess LLMs' risks and guide their further development. To support such stratified evaluations, we propose LLMMaps as a novel visualization technique that enables users to evaluate LLMs' performance with respect to Q&amp;A datasets. LLMMaps provide detailed insights into LLMs' kn
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#24182;&#32534;&#36753;&#26263;&#34255;&#21518;&#38376;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#26426;&#21046;&#65292;&#21457;&#29616;&#26089;&#26399;&#23618;&#30340;MLP&#27169;&#22359;&#21644;&#21021;&#22987;&#23884;&#20837;&#25237;&#24433;&#26159;&#21518;&#38376;&#26426;&#21046;&#20013;&#26368;&#37325;&#35201;&#30340;&#37096;&#20998;&#12290;&#36890;&#36807;&#20351;&#29992;PCP&#28040;&#34701;&#25216;&#26415;&#26367;&#25442;&#21464;&#21387;&#22120;&#27169;&#22359;&#65292;&#25105;&#20204;&#25104;&#21151;&#21024;&#38500;&#12289;&#25554;&#20837;&#21644;&#20462;&#25913;&#21518;&#38376;&#26426;&#21046;&#65292;&#24182;&#26174;&#33879;&#25913;&#21892;&#20102;&#21518;&#38376;&#30340;&#36755;&#20986;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.12461</link><description>&lt;p&gt;
&#20998;&#26512;&#21644;&#32534;&#36753;&#26263;&#34255;&#21518;&#38376;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Analyzing And Editing Inner Mechanisms Of Backdoored Language Models. (arXiv:2302.12461v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12461
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#24182;&#32534;&#36753;&#26263;&#34255;&#21518;&#38376;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#26426;&#21046;&#65292;&#21457;&#29616;&#26089;&#26399;&#23618;&#30340;MLP&#27169;&#22359;&#21644;&#21021;&#22987;&#23884;&#20837;&#25237;&#24433;&#26159;&#21518;&#38376;&#26426;&#21046;&#20013;&#26368;&#37325;&#35201;&#30340;&#37096;&#20998;&#12290;&#36890;&#36807;&#20351;&#29992;PCP&#28040;&#34701;&#25216;&#26415;&#26367;&#25442;&#21464;&#21387;&#22120;&#27169;&#22359;&#65292;&#25105;&#20204;&#25104;&#21151;&#21024;&#38500;&#12289;&#25554;&#20837;&#21644;&#20462;&#25913;&#21518;&#38376;&#26426;&#21046;&#65292;&#24182;&#26174;&#33879;&#25913;&#21892;&#20102;&#21518;&#38376;&#30340;&#36755;&#20986;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#38598;&#20013;&#30340;&#27602;&#21270;&#26159;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#22312;&#23433;&#20840;&#23041;&#32961;&#65292;&#21487;&#33021;&#23548;&#33268;&#26263;&#34255;&#21518;&#38376;&#30340;&#27169;&#22411;&#12290;&#20851;&#20110;&#26263;&#34255;&#21518;&#38376;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#26426;&#21046;&#20197;&#21450;&#23427;&#20204;&#22914;&#20309;&#22788;&#29702;&#35302;&#21457;&#36755;&#20837;&#65288;&#20363;&#22914;&#65292;&#20999;&#25442;&#33267;&#26377;&#27602;&#35821;&#35328;&#65289;&#30340;&#25551;&#36848;&#23578;&#26410;&#25214;&#21040;&#12290;&#26412;&#25991;&#30740;&#31350;&#22522;&#20110;Transformer&#30340;&#26263;&#34255;&#21518;&#38376;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#34920;&#31034;&#65292;&#24182;&#30830;&#23450;&#26089;&#26399;&#23618;&#30340;MLP&#27169;&#22359;&#19982;&#21021;&#22987;&#23884;&#20837;&#25237;&#24433;&#32467;&#21512;&#26159;&#21518;&#38376;&#26426;&#21046;&#20013;&#26368;&#37325;&#35201;&#30340;&#37096;&#20998;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#30693;&#35782;&#26469;&#21024;&#38500;&#12289;&#25554;&#20837;&#21644;&#20462;&#25913;&#21518;&#38376;&#26426;&#21046;&#65292;&#24182;&#29992;&#24037;&#31243;&#21270;&#26367;&#20195;&#29289;&#38477;&#20302;MLP&#27169;&#22359;&#36755;&#20986;&#30340;&#37325;&#35201;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#20027;&#35201;&#25104;&#20998;&#30340;&#20302;&#31209;&#30697;&#38453;&#30340;PCP&#28040;&#34701;&#25216;&#26415;&#65292;&#29992;&#20854;&#26367;&#25442;&#21464;&#21387;&#22120;&#27169;&#22359;&#12290;&#25105;&#20204;&#22312;&#26263;&#34255;&#21518;&#38376;&#30340;&#29609;&#20855;&#27169;&#22411;&#12289;&#26263;&#34255;&#21518;&#38376;&#30340;&#22823;&#22411;&#27169;&#22411;&#21644;&#38750;&#26263;&#34255;&#21518;&#38376;&#30340;&#24320;&#28304;&#27169;&#22411;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#34920;&#26126;&#25105;&#20204;&#21487;&#20197;&#25913;&#21892;&#21518;&#38376;&#30340;&#36755;&#20986;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Poisoning of data sets is a potential security threat to large language models that can lead to backdoored models. A description of the internal mechanisms of backdoored language models and how they process trigger inputs, e.g., when switching to toxic language, has yet to be found. In this work, we study the internal representations of transformer-based backdoored language models and determine early-layer MLP modules as most important for the backdoor mechanism in combination with the initial embedding projection. We use this knowledge to remove, insert, and modify backdoor mechanisms with engineered replacements that reduce the MLP module outputs to essentials for the backdoor mechanism. To this end, we introduce PCP ablation, where we replace transformer modules with low-rank matrices based on the principal components of their activations. We demonstrate our results on backdoored toy, backdoored large, and non-backdoored open-source models. We show that we can improve the backdoor r
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#26080;&#27861;&#20165;&#20973;&#24120;&#35782;&#30693;&#35782;&#22238;&#31572;&#30340;&#20449;&#24687;&#23547;&#27714;&#38382;&#39064;&#32780;&#35774;&#35745;&#30340;&#35270;&#35273;&#38382;&#31572;&#25968;&#25454;&#38598;InfoSeek&#12290;&#20351;&#29992;InfoSeek&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#21457;&#29616;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#22238;&#31572;&#27714;&#30693;&#35270;&#35273;&#38382;&#39064;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#65292;&#20294;&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#28608;&#21457;&#27169;&#22411;&#20351;&#29992;&#32454;&#31890;&#24230;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2302.11713</link><description>&lt;p&gt;
Pre-trained Vision and Language Models&#33021;&#21542;&#22238;&#31572;&#27714;&#30693;&#35270;&#35273;&#38382;&#39064;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Pre-trained Vision and Language Models Answer Visual Information-Seeking Questions?. (arXiv:2302.11713v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11713
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#26080;&#27861;&#20165;&#20973;&#24120;&#35782;&#30693;&#35782;&#22238;&#31572;&#30340;&#20449;&#24687;&#23547;&#27714;&#38382;&#39064;&#32780;&#35774;&#35745;&#30340;&#35270;&#35273;&#38382;&#31572;&#25968;&#25454;&#38598;InfoSeek&#12290;&#20351;&#29992;InfoSeek&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#21457;&#29616;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#22238;&#31572;&#27714;&#30693;&#35270;&#35273;&#38382;&#39064;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#65292;&#20294;&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#28608;&#21457;&#27169;&#22411;&#20351;&#29992;&#32454;&#31890;&#24230;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Pre-trained vision and language models&#22312;&#28041;&#21450;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#39046;&#20808;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#35270;&#35273;&#38382;&#31572;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#26159;&#21542;&#20855;&#22791;&#22238;&#31572;&#19981;&#20165;&#20165;&#26597;&#35810;&#35270;&#35273;&#20869;&#23481;&#65292;&#32780;&#19988;&#36824;&#20855;&#26377;&#30693;&#35782;&#23494;&#38598;&#21644;&#20449;&#24687;&#23547;&#27714;&#24615;&#36136;&#30340;&#38382;&#39064;&#30340;&#33021;&#21147;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;InfoSeek&#65292;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#26080;&#27861;&#20165;&#20973;&#24120;&#35782;&#30693;&#35782;&#22238;&#31572;&#30340;&#20449;&#24687;&#23547;&#27714;&#38382;&#39064;&#32780;&#35774;&#35745;&#30340;&#35270;&#35273;&#38382;&#31572;&#25968;&#25454;&#38598;&#12290;&#20351;&#29992;InfoSeek&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#21508;&#31181;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#38382;&#31572;&#27169;&#22411;&#65292;&#24182;&#28145;&#20837;&#20102;&#35299;&#23427;&#20204;&#30340;&#29305;&#28857;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25581;&#31034;&#20102;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;&#22914;PaLI-X&#65292;BLIP2&#31561;&#65289;&#22312;&#22238;&#31572;&#27714;&#30693;&#35270;&#35273;&#38382;&#39064;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#65292;&#20294;&#22312;InfoSeek&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#33021;&#22815;&#28608;&#21457;&#27169;&#22411;&#20351;&#29992;&#20182;&#20204;&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#23398;&#21040;&#30340;&#32454;&#31890;&#24230;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#20934;&#30830;&#30340;&#35270;&#35273;&#23454;&#20307;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained vision and language models have demonstrated state-of-the-art capabilities over existing tasks involving images and texts, including visual question answering. However, it remains unclear whether these models possess the capability to answer questions that are not only querying visual content but knowledge-intensive and information-seeking. In this study, we introduce InfoSeek, a visual question answering dataset tailored for information-seeking questions that cannot be answered with only common sense knowledge. Using InfoSeek, we analyze various pre-trained visual question answering models and gain insights into their characteristics. Our findings reveal that state-of-the-art pre-trained multi-modal models (e.g., PaLI-X, BLIP2, etc.) face challenges in answering visual information-seeking questions, but fine-tuning on the InfoSeek dataset elicits models to use fine-grained knowledge that was learned during their pre-training. Furthermore, we show that accurate visual entit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#31163;&#32447;&#30446;&#26631;&#26465;&#20214;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#32463;&#36807;&#36731;&#24494;&#20462;&#25913;&#21518;&#65292;&#35813;&#31639;&#27861;&#22312;&#28385;&#36275;&#36890;&#29992;&#20989;&#25968;&#36924;&#36817;&#26102;&#20855;&#26377; O(poly(1/&#949;)) &#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2302.03770</link><description>&lt;p&gt;
&#21487;&#35777;&#26126;&#39640;&#25928;&#30340;&#31163;&#32447;&#30446;&#26631;&#26465;&#20214;&#24378;&#21270;&#23398;&#20064;&#19982;&#36890;&#29992;&#20989;&#25968;&#36924;&#36817;&#21644;&#21333;&#19968;&#31574;&#30053;&#38598;&#20013;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Provably Efficient Offline Goal-Conditioned Reinforcement Learning with General Function Approximation and Single-Policy Concentrability. (arXiv:2302.03770v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03770
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#31163;&#32447;&#30446;&#26631;&#26465;&#20214;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#32463;&#36807;&#36731;&#24494;&#20462;&#25913;&#21518;&#65292;&#35813;&#31639;&#27861;&#22312;&#28385;&#36275;&#36890;&#29992;&#20989;&#25968;&#36924;&#36817;&#26102;&#20855;&#26377; O(poly(1/&#949;)) &#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#26465;&#20214;&#24378;&#21270;&#23398;&#20064;&#65288;GCRL&#65289;&#26159;&#25351;&#23398;&#20064;&#36890;&#29992;&#25216;&#33021;&#65292;&#20197;&#36798;&#21040;&#19981;&#21516;&#30340;&#30446;&#26631;&#12290;&#29305;&#21035;&#26159;&#65292;&#31163;&#32447;GCRL&#21482;&#38656;&#35201;&#32431;&#39044;&#20808;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#26469;&#25191;&#34892;&#35757;&#32451;&#20219;&#21153;&#65292;&#32780;&#26080;&#38656;&#19982;&#29615;&#22659;&#36827;&#34892;&#39069;&#22806;&#30340;&#20132;&#20114;&#12290;&#23613;&#31649;&#31163;&#32447;GCRL&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#24182;&#19988;&#35768;&#22810;&#20043;&#21069;&#30340;&#24037;&#20316;&#24050;&#32463;&#35777;&#26126;&#20102;&#20854;&#23454;&#35777;&#25104;&#21151;&#65292;&#20294;&#23545;&#20110;&#22823;&#29366;&#24577;&#31354;&#38388;&#19988;&#31163;&#32447;&#25968;&#25454;&#38598;&#20165;&#35206;&#30422;&#25105;&#20204;&#24076;&#26395;&#23398;&#20064;&#30340;&#31574;&#30053;&#30340;&#39640;&#25928;&#31163;&#32447;GCRL&#31639;&#27861;&#30340;&#29702;&#35770;&#29702;&#35299;&#23578;&#26410;&#24314;&#31435;&#36215;&#26469;&#12290;&#26412;&#25991;&#23545;&#19968;&#31181;&#29616;&#26377;&#32463;&#39564;&#25104;&#21151;&#30340;&#31163;&#32447;GCRL&#31639;&#27861;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#36890;&#36807;&#36731;&#24494;&#20462;&#25913;&#65292;&#35813;&#31639;&#27861;&#22312;&#20855;&#26377;&#36890;&#29992;&#20989;&#25968;&#36924;&#36817;&#30340;&#24773;&#20917;&#19979;&#65292;&#20855;&#26377;$\widetilde{O}(\text{poly}(1/\epsilon))$&#26679;&#26412;&#22797;&#26434;&#24230;&#65288;&#20854;&#20013;$\epsilon$&#26159;&#23398;&#20064;&#31574;&#30053;&#30340;&#26399;&#26395;&#38750;&#26368;&#20248;&#24615;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Goal-conditioned reinforcement learning (GCRL) refers to learning general-purpose skills that aim to reach diverse goals. In particular, offline GCRL only requires purely pre-collected datasets to perform training tasks without additional interactions with the environment. Although offline GCRL has become increasingly prevalent and many previous works have demonstrated its empirical success, the theoretical understanding of efficient offline GCRL algorithms is not well established, especially when the state space is huge and the offline dataset only covers the policy we aim to learn. In this paper, we provide a rigorous theoretical analysis of an existing empirically successful offline GCRL algorithm. We prove that under slight modification, this algorithm enjoys an $\widetilde{O}(\text{poly}(1/\epsilon))$ sample complexity (where $\epsilon$ is the desired suboptimality of the learned policy) with general function approximation thanks to the property of (semi-)strong convexity of the o
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;IM-IAD&#24037;&#19994;&#21046;&#36896;&#20013;&#30340;&#22270;&#20687;&#24322;&#24120;&#26816;&#27979;&#22522;&#20934;&#65292;&#36890;&#36807;&#32479;&#19968;&#30340;&#35774;&#32622;&#35780;&#20272;&#20102;16&#20010;&#31639;&#27861;&#22312;7&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#65292;&#26088;&#22312;&#35299;&#20915;&#30446;&#21069;&#36825;&#19968;&#39046;&#22495;&#30740;&#31350;&#30340;&#19981;&#35268;&#33539;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2301.13359</link><description>&lt;p&gt;
IM-IAD&#65306;&#24037;&#19994;&#21046;&#36896;&#20013;&#30340;&#22270;&#20687;&#24322;&#24120;&#26816;&#27979;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
IM-IAD: Industrial Image Anomaly Detection Benchmark in Manufacturing. (arXiv:2301.13359v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13359
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;IM-IAD&#24037;&#19994;&#21046;&#36896;&#20013;&#30340;&#22270;&#20687;&#24322;&#24120;&#26816;&#27979;&#22522;&#20934;&#65292;&#36890;&#36807;&#32479;&#19968;&#30340;&#35774;&#32622;&#35780;&#20272;&#20102;16&#20010;&#31639;&#27861;&#22312;7&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#65292;&#26088;&#22312;&#35299;&#20915;&#30446;&#21069;&#36825;&#19968;&#39046;&#22495;&#30740;&#31350;&#30340;&#19981;&#35268;&#33539;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#24322;&#24120;&#26816;&#27979;&#65288;IAD&#65289;&#26159;&#24037;&#19994;&#21046;&#36896;&#20013;&#19968;&#39033;&#26032;&#20852;&#19988;&#37325;&#35201;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#12290;&#36817;&#24180;&#26469;&#65292;&#35768;&#22810;&#20808;&#36827;&#30340;&#31639;&#27861;&#24050;&#32463;&#34987;&#21457;&#24067;&#65292;&#20294;&#23427;&#20204;&#30340;&#24615;&#33021;&#23384;&#22312;&#24456;&#22823;&#24046;&#24322;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#32570;&#20047;&#23454;&#38469;&#30340;&#24037;&#19994;&#21046;&#36896;&#35774;&#32622;&#24456;&#21487;&#33021;&#38459;&#30861;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#21457;&#23637;&#21644;&#20351;&#29992;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;IAD&#26041;&#27861;&#23578;&#26410;&#32463;&#36807;&#31995;&#32479;&#35780;&#20272;&#12290;&#22240;&#27492;&#65292;&#36825;&#20351;&#24471;&#30740;&#31350;&#20154;&#21592;&#24456;&#38590;&#20998;&#26512;&#23427;&#20204;&#65292;&#22240;&#20026;&#23427;&#20204;&#26159;&#20026;&#19981;&#21516;&#25110;&#29305;&#27530;&#24773;&#20917;&#32780;&#35774;&#35745;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#24037;&#19994;&#21046;&#36896;&#35774;&#32622;&#26469;&#35780;&#20272;&#36825;&#20123;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#20960;&#20010;&#26041;&#38754;&#65292;&#22914;&#21508;&#31181;&#30417;&#30563;&#32423;&#21035;&#65288;&#26080;&#30417;&#30563;vs&#21322;&#30417;&#30563;&#65289;&#12289;&#23569;&#26679;&#26412;&#23398;&#20064;&#12289;&#25345;&#32493;&#23398;&#20064;&#12289;&#22122;&#22768;&#26631;&#31614;&#12289;&#20869;&#23384;&#20351;&#29992;&#21644;&#25512;&#26029;&#36895;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24039;&#22937;&#22320;&#26500;&#24314;&#20102;&#19968;&#20010;&#23436;&#25972;&#30340;&#22270;&#20687;&#24322;&#24120;&#26816;&#27979;&#22522;&#20934;&#65288;IM-IAD&#65289;&#65292;&#35813;&#22522;&#20934;&#22312;&#32479;&#19968;&#30340;&#35774;&#32622;&#19979;&#21253;&#25324;&#20102;16&#20010;&#31639;&#27861;&#21644;7&#20010;&#20027;&#27969;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image anomaly detection (IAD) is an emerging and vital computer vision task in industrial manufacturing (IM). Recently many advanced algorithms have been published, but their performance deviates greatly. We realize that the lack of actual IM settings most probably hinders the development and usage of these methods in real-world applications. As far as we know, IAD methods are not evaluated systematically. As a result, this makes it difficult for researchers to analyze them because they are designed for different or special cases. To solve this problem, we first propose a uniform IM setting to assess how well these algorithms perform, which includes several aspects, i.e., various levels of supervision (unsupervised vs. semi-supervised), few-shot learning, continual learning, noisy labels, memory usage, and inference speed. Moreover, we skillfully build a comprehensive image anomaly detection benchmark (IM-IAD) that includes 16 algorithms on 7 mainstream datasets with uniform settings. 
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#31163;&#32447;&#31639;&#27861;&#35843;&#25972;&#20026;&#21482;&#38656;&#35201;&#36172;&#24466;&#21453;&#39304;&#30340;&#20122;&#32447;&#24615;&#945;-&#21518;&#24724;&#26041;&#27861;&#26469;&#35299;&#20915;&#32452;&#21512;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#23454;&#29616;&#23545;&#20110;&#26102;&#38388;&#30028;T&#30340;&#39044;&#26399;&#32047;&#31215;&#945;-&#21518;&#24724;&#20381;&#36182;&#24230;&#20026;O(T^&#65288;2/3&#65289;log&#65288;T^&#65288;1/3&#65289;&#65289;)&#12290;</title><link>http://arxiv.org/abs/2301.13326</link><description>&lt;p&gt;
&#36866;&#24212;&#31163;&#32447;&#31639;&#27861;&#35299;&#20915;&#24102;&#26377;&#36172;&#24466;&#21453;&#39304;&#30340;&#32452;&#21512;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Framework for Adapting Offline Algorithms to Solve Combinatorial Multi-Armed Bandit Problems with Bandit Feedback. (arXiv:2301.13326v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13326
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#31163;&#32447;&#31639;&#27861;&#35843;&#25972;&#20026;&#21482;&#38656;&#35201;&#36172;&#24466;&#21453;&#39304;&#30340;&#20122;&#32447;&#24615;&#945;-&#21518;&#24724;&#26041;&#27861;&#26469;&#35299;&#20915;&#32452;&#21512;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#23454;&#29616;&#23545;&#20110;&#26102;&#38388;&#30028;T&#30340;&#39044;&#26399;&#32047;&#31215;&#945;-&#21518;&#24724;&#20381;&#36182;&#24230;&#20026;O(T^&#65288;2/3&#65289;log&#65288;T^&#65288;1/3&#65289;&#65289;)&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#38543;&#26426;&#30340;&#32452;&#21512;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#65292;&#20854;&#20013;&#23398;&#20064;&#32773;&#21482;&#33021;&#35775;&#38382;&#36172;&#24466;&#21453;&#39304;&#65292;&#24182;&#19988;&#22870;&#21169;&#20989;&#25968;&#21487;&#20197;&#26159;&#38750;&#32447;&#24615;&#30340;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#26694;&#26550;&#65292;&#23558;&#31163;&#32447;&#31163;&#25955;&#36924;&#36817;&#31639;&#27861;&#35843;&#25972;&#20026;&#21482;&#38656;&#35201;&#36172;&#24466;&#21453;&#39304;&#30340;&#20122;&#32447;&#24615;&#945;-&#21518;&#24724;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#20110;&#26102;&#38388;&#30028;T&#30340;&#39044;&#26399;&#32047;&#31215;&#945;-&#21518;&#24724;&#20381;&#36182;&#24230;&#20026;O(T^&#65288;2/3&#65289;log&#65288;T^&#65288;1/3&#65289;&#65289;)&#12290;&#35813;&#26694;&#26550;&#21482;&#38656;&#35201;&#31163;&#32447;&#31639;&#27861;&#23545;&#20989;&#25968;&#35780;&#20272;&#20013;&#30340;&#23567;&#35823;&#24046;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#36866;&#24212;&#36807;&#31243;&#29978;&#33267;&#19981;&#38656;&#35201;&#26174;&#24335;&#22320;&#30693;&#36947;&#31163;&#32447;&#36924;&#36817;&#31639;&#27861;--&#31163;&#32447;&#31639;&#27861;&#21487;&#20197;&#34987;&#29992;&#20316;&#40657;&#30418;&#23376;&#23376;&#31243;&#24207;&#12290;&#20026;&#20102;&#35777;&#26126;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#30340;&#23454;&#29992;&#24615;&#65292;&#23558;&#35813;&#26694;&#26550;&#24212;&#29992;&#20110;&#23376;&#27169;&#26368;&#22823;&#21270;&#30340;&#19981;&#21516;&#24212;&#29992;&#20013;&#12290;&#22312;&#20855;&#26377;&#32972;&#21253;&#32422;&#26463;&#30340;&#23376;&#27169;&#26368;&#22823;&#21270;&#20013;&#65292;&#26032;&#30340;CMAB&#31639;&#27861;&#20248;&#20110;&#24320;&#21457;&#30340;&#23436;&#20840;&#36172;&#24466;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the problem of stochastic, combinatorial multi-armed bandits where the learner only has access to bandit feedback and the reward function can be non-linear. We provide a general framework for adapting discrete offline approximation algorithms into sublinear $\alpha$-regret methods that only require bandit feedback, achieving $\mathcal{O}\left(T^\frac{2}{3}\log(T)^\frac{1}{3}\right)$ expected cumulative $\alpha$-regret dependence on the horizon $T$. The framework only requires the offline algorithms to be robust to small errors in function evaluation. The adaptation procedure does not even require explicit knowledge of the offline approximation algorithm -- the offline algorithm can be used as a black box subroutine. To demonstrate the utility of the proposed framework, the proposed framework is applied to diverse applications in submodular maximization. The new CMAB algorithms for submodular maximization with knapsack constraints outperform a full-bandit method developed
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AIRS&#30340;&#33258;&#21160;&#20869;&#22312;&#22870;&#21169;&#22609;&#36896;&#25506;&#32034;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#20869;&#22312;&#28608;&#21169;&#20197;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#24615;&#33021;&#65307;&#24182;&#24320;&#21457;&#20102;&#39640;&#25928;&#21487;&#38752;&#30340;&#20869;&#22312;&#22870;&#21169;&#24037;&#20855;&#21253;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;AIRS&#24615;&#33021;&#21331;&#36234;&#65292;&#33021;&#22815;&#32988;&#36807;&#22522;&#20934;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2301.10886</link><description>&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#33258;&#21160;&#20869;&#22312;&#22870;&#21169;&#22609;&#36896;&#25506;&#32034;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Automatic Intrinsic Reward Shaping for Exploration in Deep Reinforcement Learning. (arXiv:2301.10886v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10886
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AIRS&#30340;&#33258;&#21160;&#20869;&#22312;&#22870;&#21169;&#22609;&#36896;&#25506;&#32034;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#20869;&#22312;&#28608;&#21169;&#20197;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#24615;&#33021;&#65307;&#24182;&#24320;&#21457;&#20102;&#39640;&#25928;&#21487;&#38752;&#30340;&#20869;&#22312;&#22870;&#21169;&#24037;&#20855;&#21253;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;AIRS&#24615;&#33021;&#21331;&#36234;&#65292;&#33021;&#22815;&#32988;&#36807;&#22522;&#20934;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AIRS&#30340;&#33258;&#21160;&#20869;&#22312;&#22870;&#21169;&#22609;&#36896;&#26041;&#27861;&#65292;&#36890;&#36807;&#26234;&#33021;&#21644;&#36866;&#24212;&#24615;&#30340;&#22609;&#36896;&#20989;&#25968;&#65292;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#20869;&#22312;&#28608;&#21169;&#20197;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#24615;&#33021;&#12290;AIRS&#21487;&#20197;&#26681;&#25454;&#23454;&#26102;&#20272;&#35745;&#30340;&#20219;&#21153;&#22238;&#25253;&#20174;&#39044;&#23450;&#20041;&#30340;&#20989;&#25968;&#38598;&#20013;&#36873;&#25321;&#22609;&#36896;&#20989;&#25968;&#65292;&#25552;&#20379;&#21487;&#38752;&#30340;&#25506;&#32034;&#28608;&#21169;&#24182;&#35299;&#20915;&#20559;&#32622;&#30446;&#26631;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20869;&#22312;&#22870;&#21169;&#24037;&#20855;&#21253;&#65292;&#25552;&#20379;&#22810;&#31181;&#20869;&#22312;&#22870;&#21169;&#26041;&#27861;&#30340;&#39640;&#25928;&#21487;&#38752;&#23454;&#29616;&#26041;&#24335;&#12290;&#25105;&#20204;&#23558;AIRS&#24212;&#29992;&#22312;MiniGrid&#12289;Procgen&#21644;DeepMind&#25511;&#21046;&#22871;&#20214;&#30340;&#22810;&#39033;&#20219;&#21153;&#20013;&#36827;&#34892;&#27979;&#35797;&#12290;&#22823;&#37327;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;AIRS&#21487;&#20197;&#32988;&#36807;&#22522;&#20934;&#26041;&#26696;&#65292;&#24182;&#20855;&#26377;&#31616;&#21333;&#30340;&#26550;&#26500;&#21644;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present AIRS: Automatic Intrinsic Reward Shaping that intelligently and adaptively provides high-quality intrinsic rewards to enhance exploration in reinforcement learning (RL). More specifically, AIRS selects shaping function from a predefined set based on the estimated task return in real-time, providing reliable exploration incentives and alleviating the biased objective problem. Moreover, we develop an intrinsic reward toolkit to provide efficient and reliable implementations of diverse intrinsic reward approaches. We test AIRS on various tasks of MiniGrid, Procgen, and DeepMind Control Suite. Extensive simulation demonstrates that AIRS can outperform the benchmarking schemes and achieve superior performance with simple architecture.
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;&#31526;&#21495;&#35745;&#31639;&#26159;&#23558;&#31526;&#21495;&#21644;&#32479;&#35745;&#35748;&#30693;&#33539;&#24335;&#25972;&#21512;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#22312;&#25512;&#29702;&#21644;&#35299;&#37322;&#24615;&#20197;&#21450;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#31561;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#35813;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#37325;&#35201;&#36129;&#29486;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#25104;&#21151;&#24212;&#29992;&#26696;&#20363;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2210.15889</link><description>&lt;p&gt;
&#36208;&#21521;&#25968;&#25454;&#21644;&#30693;&#35782;&#39537;&#21160;&#30340;&#20154;&#24037;&#26234;&#33021;&#65306;&#31070;&#32463;&#31526;&#21495;&#35745;&#31639;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Towards Data-and Knowledge-Driven Artificial Intelligence: A Survey on Neuro-Symbolic Computing. (arXiv:2210.15889v4 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.15889
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31526;&#21495;&#35745;&#31639;&#26159;&#23558;&#31526;&#21495;&#21644;&#32479;&#35745;&#35748;&#30693;&#33539;&#24335;&#25972;&#21512;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#22312;&#25512;&#29702;&#21644;&#35299;&#37322;&#24615;&#20197;&#21450;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#31561;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#35813;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#37325;&#35201;&#36129;&#29486;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#25104;&#21151;&#24212;&#29992;&#26696;&#20363;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#24180;&#26469;&#65292;&#31070;&#32463;&#31526;&#21495;&#35745;&#31639;&#65288;NeSy&#65289;&#19968;&#30452;&#26159;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#39046;&#22495;&#30340;&#30740;&#31350;&#28909;&#28857;&#65292;&#36861;&#27714;&#31526;&#21495;&#21644;&#32479;&#35745;&#35748;&#30693;&#33539;&#24335;&#30340;&#25972;&#21512;&#12290;&#30001;&#20110;NeSy&#22312;&#31526;&#21495;&#34920;&#31034;&#30340;&#25512;&#29702;&#21644;&#21487;&#35299;&#37322;&#24615;&#20197;&#21450;&#31070;&#32463;&#32593;&#32476;&#30340;&#24378;&#22823;&#23398;&#20064;&#20013;&#20855;&#26377;&#28508;&#21147;&#65292;&#23427;&#21487;&#33021;&#25104;&#20026;&#19979;&#19968;&#20195;AI&#30340;&#20652;&#21270;&#21058;&#12290;&#26412;&#25991;&#31995;&#32479;&#32508;&#36848;&#20102;NeSy&#30740;&#31350;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#37325;&#35201;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#21382;&#21490;&#65292;&#28085;&#30422;&#20102;&#26089;&#26399;&#24037;&#20316;&#21644;&#22522;&#30784;&#30693;&#35782;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35752;&#35770;&#20102;&#32972;&#26223;&#27010;&#24565;&#65292;&#24182;&#30830;&#23450;&#20102;&#25512;&#21160;NeSy&#21457;&#23637;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25353;&#29031;&#20960;&#20010;&#20027;&#35201;&#29305;&#24449;&#23545;&#36817;&#26399;&#30340;&#37324;&#31243;&#30865;&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#65292;&#21253;&#25324;&#31070;&#32463;&#31526;&#21495;&#25972;&#21512;&#12289;&#30693;&#35782;&#34920;&#31034;&#12289;&#30693;&#35782;&#23884;&#20837;&#21644;&#21151;&#33021;&#24615;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#31616;&#35201;&#35752;&#35770;&#20102;&#25104;&#21151;&#30340;&#24212;&#29992;&#26696;&#20363;&#65292;&#20197;&#21450;NeSy&#39046;&#22495;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural-symbolic computing (NeSy), which pursues the integration of the symbolic and statistical paradigms of cognition, has been an active research area of Artificial Intelligence (AI) for many years. As NeSy shows promise of reconciling the advantages of reasoning and interpretability of symbolic representation and robust learning in neural networks, it may serve as a catalyst for the next generation of AI. In the present paper, we provide a systematic overview of the recent developments and important contributions of NeSy research. Firstly, we introduce study history of this area, covering early work and foundations. We further discuss background concepts and identify key driving factors behind the development of NeSy. Afterward, we categorize recent landmark approaches along several main characteristics that underline this research paradigm, including neural-symbolic integration, knowledge representation, knowledge embedding, and functionality. Next, we briefly discuss the successfu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27979;&#37327;&#24418;&#24577;&#21464;&#24322;&#24433;&#21709;&#30340;&#26041;&#27861;&#65292;&#24182;&#20998;&#26512;&#20102;&#21464;&#24322;&#24133;&#24230;&#21644;&#24341;&#20837;&#26041;&#24335;&#19982;&#28436;&#21270;&#20307;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2208.02809</link><description>&lt;p&gt;
&#28436;&#21270;&#26426;&#22120;&#20154;&#23398;&#20013;&#24418;&#24577;&#21464;&#24322;&#30340;&#20316;&#29992;&#65306;&#26368;&#22823;&#21270;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Role of Morphological Variation in Evolutionary Robotics: Maximizing Performance and Robustness. (arXiv:2208.02809v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.02809
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27979;&#37327;&#24418;&#24577;&#21464;&#24322;&#24433;&#21709;&#30340;&#26041;&#27861;&#65292;&#24182;&#20998;&#26512;&#20102;&#21464;&#24322;&#24133;&#24230;&#21644;&#24341;&#20837;&#26041;&#24335;&#19982;&#28436;&#21270;&#20307;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#33719;&#24471;&#40065;&#26834;&#19988;&#33021;&#22815;&#20811;&#26381;&#29616;&#23454;&#24046;&#36317;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#26377;&#24517;&#35201;&#26292;&#38706;&#29992;&#20110;&#28436;&#21270;&#26426;&#22120;&#20154;&#25511;&#21046;&#22120;&#21040;&#19981;&#21516;&#26465;&#20214;&#30340;&#28436;&#21270;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#36824;&#27809;&#26377;&#26041;&#27861;&#26469;&#20998;&#26512;&#21644;&#29702;&#35299;&#24433;&#21709;&#28436;&#21270;&#36807;&#31243;&#30340;&#21508;&#31181;&#24418;&#24577;&#26465;&#20214;&#30340;&#24433;&#21709;&#65292;&#20063;&#27809;&#26377;&#26041;&#27861;&#26469;&#36873;&#25321;&#21512;&#36866;&#30340;&#21464;&#24322;&#33539;&#22260;&#12290;&#36890;&#36807;&#24418;&#24577;&#26465;&#20214;&#65292;&#25105;&#20204;&#25351;&#30340;&#26159;&#26426;&#22120;&#20154;&#30340;&#36215;&#22987;&#29366;&#24577;&#20197;&#21450;&#30001;&#20110;&#22122;&#22768;&#23548;&#33268;&#25805;&#20316;&#36807;&#31243;&#20013;&#20256;&#24863;&#22120;&#35835;&#25968;&#30340;&#21464;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#20197;&#27979;&#37327;&#36825;&#20123;&#24418;&#24577;&#21464;&#24322;&#24433;&#21709;&#30340;&#26041;&#27861;&#65292;&#24182;&#20998;&#26512;&#20102;&#21464;&#24322;&#24133;&#24230;&#12289;&#24341;&#20837;&#26041;&#24335;&#19982;&#28436;&#21270;&#20307;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65306;&#65288;i&#65289;&#28436;&#21270;&#31639;&#27861;&#33021;&#22815;&#23481;&#24525;&#20855;&#26377;&#24456;&#39640;&#24433;&#21709;&#30340;&#24418;&#24577;&#21464;&#24322;&#65307;&#65288;ii&#65289;&#24433;&#21709;&#26426;&#22120;&#20154;&#34892;&#20026;&#30340;&#21464;&#24322;&#23545;&#28436;&#21270;&#20307;&#30340;&#24615;&#33021;&#26377;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exposing an Evolutionary Algorithm that is used to evolve robot controllers to variable conditions is necessary to obtain solutions which are robust and can cross the reality gap. However, we do not yet have methods for analyzing and understanding the impact of the varying morphological conditions which impact the evolutionary process, and therefore for choosing suitable variation ranges. By morphological conditions, we refer to the starting state of the robot, and to variations in its sensor readings during operation due to noise. In this article, we introduce a method that permits us to measure the impact of these morphological variations and we analyze the relation between the amplitude of variations, the modality with which they are introduced, and the performance and robustness of evolving agents. Our results demonstrate that (i) the evolutionary algorithm can tolerate morphological variations which have a very high impact, (ii) variations affecting the actions of the agent are to
&lt;/p&gt;</description></item><item><title>MemSAC&#25552;&#20986;&#20102;&#19968;&#31181;&#35760;&#24518;&#22686;&#24378;&#26679;&#26412;&#19968;&#33268;&#24615;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#20043;&#38388;&#30340;&#26679;&#26412;&#32423;&#30456;&#20284;&#24615;&#23454;&#29616;&#21028;&#21035;&#36716;&#31227;&#65292;&#24182;&#19988;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26126;&#26174;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2207.12389</link><description>&lt;p&gt;
MemSAC: &#22823;&#35268;&#27169;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#35760;&#24518;&#22686;&#24378;&#26679;&#26412;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
MemSAC: Memory Augmented Sample Consistency for Large Scale Unsupervised Domain Adaptation. (arXiv:2207.12389v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.12389
&lt;/p&gt;
&lt;p&gt;
MemSAC&#25552;&#20986;&#20102;&#19968;&#31181;&#35760;&#24518;&#22686;&#24378;&#26679;&#26412;&#19968;&#33268;&#24615;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#20043;&#38388;&#30340;&#26679;&#26412;&#32423;&#30456;&#20284;&#24615;&#23454;&#29616;&#21028;&#21035;&#36716;&#31227;&#65292;&#24182;&#19988;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26126;&#26174;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#30340;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#24341;&#20837;&#20102;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#26032;&#25361;&#25112;&#65292;&#22914;&#31867;&#38388;&#21306;&#20998;&#24230;&#23567;&#65292;&#29616;&#26377;&#30340;&#20165;&#20381;&#36182;&#20110;&#22495;&#19981;&#21464;&#24615;&#30340;&#26041;&#27861;&#26080;&#27861;&#24456;&#22909;&#22320;&#22788;&#29702;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MemSAC&#65292;&#23427;&#21033;&#29992;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#20043;&#38388;&#30340;&#26679;&#26412;&#32423;&#30456;&#20284;&#24615;&#23454;&#29616;&#21028;&#21035;&#36716;&#31227;&#65292;&#24182;&#21487;&#25193;&#23637;&#21040;&#22823;&#37327;&#31867;&#21035;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#31181;&#35760;&#24518;&#22686;&#24378;&#26041;&#27861;&#65292;&#20197;&#26377;&#25928;&#22320;&#25552;&#21462;&#26631;&#35760;&#28304;&#22495;&#21644;&#26410;&#26631;&#35760;&#30446;&#26631;&#22495;&#23454;&#20363;&#20043;&#38388;&#30340;&#25104;&#23545;&#30456;&#20284;&#24615;&#20851;&#31995;&#65292;&#36866;&#21512;&#22788;&#29702;&#20219;&#24847;&#25968;&#37327;&#30340;&#31867;&#21035;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#25439;&#22833;&#30340;&#21464;&#20307;&#65292;&#20197;&#20419;&#36827;&#31867;&#20869;&#36328;&#22495;&#26679;&#26412;&#20043;&#38388;&#30340;&#23616;&#37096;&#19968;&#33268;&#24615;&#65292;&#21516;&#26102;&#30830;&#20445;&#31867;&#21035;&#20043;&#38388;&#30340;&#20998;&#31163;&#65292;&#20174;&#32780;&#20445;&#25345;&#20174;&#28304;&#22495;&#21040;&#30446;&#26631;&#22495;&#30340;&#21028;&#21035;&#36716;&#31227;&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;MemSAC&#30340;&#20248;&#21183;&#21644;&#26174;&#33879;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Practical real world datasets with plentiful categories introduce new challenges for unsupervised domain adaptation like small inter-class discriminability, that existing approaches relying on domain invariance alone cannot handle sufficiently well. In this work we propose MemSAC, which exploits sample level similarity across source and target domains to achieve discriminative transfer, along with architectures that scale to a large number of categories. For this purpose, we first introduce a memory augmented approach to efficiently extract pairwise similarity relations between labeled source and unlabeled target domain instances, suited to handle an arbitrary number of classes. Next, we propose and theoretically justify a novel variant of the contrastive loss to promote local consistency among within-class cross domain samples while enforcing separation between classes, thus preserving discriminative transfer from source to target. We validate the advantages of MemSAC with significant
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#33258;&#22238;&#24402;GPT&#26679;&#24335;&#27169;&#22411;&#65292;&#20998;&#21035;&#20351;&#29992;13&#20159;&#21644;130&#20159;&#20010;&#21442;&#25968;&#65292;&#22312;60&#31181;&#35821;&#35328;&#20013;&#35757;&#32451;&#65292;&#24182;&#23637;&#31034;&#20102;&#19982;Facebook&#26368;&#36817;&#21457;&#24067;&#30340;XGLM&#27169;&#22411;&#24615;&#33021;&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;&#36825;&#20026;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25552;&#20379;&#20102;&#26356;&#22810;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2204.07580</link><description>&lt;p&gt;
mGPT: &#23569;&#26679;&#26412;&#23398;&#20064;&#32773;&#36208;&#21521;&#22810;&#35821;&#35328;&#65288;arXiv:2204.07580v2 [cs.CL] &#26356;&#26032;&#65289;
&lt;/p&gt;
&lt;p&gt;
mGPT: Few-Shot Learners Go Multilingual. (arXiv:2204.07580v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.07580
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#33258;&#22238;&#24402;GPT&#26679;&#24335;&#27169;&#22411;&#65292;&#20998;&#21035;&#20351;&#29992;13&#20159;&#21644;130&#20159;&#20010;&#21442;&#25968;&#65292;&#22312;60&#31181;&#35821;&#35328;&#20013;&#35757;&#32451;&#65292;&#24182;&#23637;&#31034;&#20102;&#19982;Facebook&#26368;&#36817;&#21457;&#24067;&#30340;XGLM&#27169;&#22411;&#24615;&#33021;&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;&#36825;&#20026;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25552;&#20379;&#20102;&#26356;&#22810;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#25253;&#21578;&#31216;&#65292;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#33539;&#24335;&#25104;&#21151;&#35299;&#20915;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#36825;&#20026;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#33258;&#22238;&#24402;GPT&#26679;&#24335;&#27169;&#22411;&#65292;&#20854;&#21442;&#25968;&#20998;&#21035;&#20026;13&#20159;&#21644;130&#20159;&#65292;&#20351;&#29992;&#32500;&#22522;&#30334;&#31185;&#21644;&#24040;&#22823;&#24178;&#20928;&#29228;&#21462;&#30340;&#35821;&#26009;&#24211;&#35757;&#32451;&#20102;25&#20010;&#35821;&#31995;&#20013;&#30340;60&#31181;&#35821;&#35328;&#12290;&#25105;&#20204;&#20351;&#29992;GPT-2&#28304;&#20195;&#30721;&#21644;&#31232;&#30095;&#27880;&#24847;&#26426;&#21046;&#22797;&#29616;&#20102;GPT-3&#26550;&#26500;&#65307;Deepspeed&#21644;Megatron&#26694;&#26550;&#20351;&#25105;&#20204;&#33021;&#22815;&#26377;&#25928;&#22320;&#24182;&#34892;&#21270;&#35757;&#32451;&#21644;&#25512;&#26029;&#27493;&#39588;&#12290;&#25152;&#24471;&#21040;&#30340;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#19982;Facebook&#26368;&#36817;&#21457;&#24067;&#30340;XGLM&#27169;&#22411;&#30456;&#24403;&#65292;&#22312;&#35206;&#30422;&#26356;&#22810;&#35821;&#35328;&#30340;&#21516;&#26102;&#22686;&#24378;&#20102;&#29420;&#32852;&#20307;&#22269;&#23478;&#21644;&#20420;&#32599;&#26031;&#23567;&#22269;&#23478;&#31561;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#35814;&#32454;&#35828;&#26126;&#20102;&#26550;&#26500;&#35774;&#35745;&#30340;&#21160;&#26426;&#65292;&#35814;&#32454;&#25551;&#36848;&#20102;&#25968;&#25454;&#20934;&#22791;&#27969;&#31243;&#65292;&#24182;&#35757;&#32451;&#20102;&#20116;&#20010;&#23567;&#29256;&#26412;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies report that autoregressive language models can successfully solve many NLP tasks via zero- and few-shot learning paradigms, which opens up new possibilities for using the pre-trained language models. This paper introduces two autoregressive GPT-like models with 1.3 billion and 13 billion parameters trained on 60 languages from 25 language families using Wikipedia and Colossal Clean Crawled Corpus. We reproduce the GPT-3 architecture using GPT-2 sources and the sparse attention mechanism; Deepspeed and Megatron frameworks allow us to parallelize the training and inference steps effectively. The resulting models show performance on par with the recently released XGLM models by Facebook, covering more languages and enhancing NLP possibilities for low resource languages of CIS countries and Russian small nations. We detail the motivation for the choices of the architecture design, thoroughly describe the data preparation pipeline, and train five small versions of the model t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#30740;&#31350;&#22270;&#30011;&#29616;&#23454;&#20027;&#20041;&#30340;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#36890;&#36807;&#27979;&#37327;&#33402;&#26415;&#23478;&#32472;&#21046;&#30340;&#20113;&#21644;&#20113;&#30340;&#29031;&#29255;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#26469;&#35780;&#20272;&#20889;&#23454;&#20027;&#20041;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#24247;&#26031;&#29305;&#24067;&#23572;&#27604;&#20182;&#30340;&#21516;&#26102;&#20195;&#26356;&#21152;&#19968;&#33268;&#22320;&#21576;&#29616;&#24418;&#24335;&#19978;&#30340;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2202.09348</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#30740;&#31350;&#22270;&#30011;&#29616;&#23454;&#20027;&#20041;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65306;&#24247;&#26031;&#29305;&#24067;&#23572;&#30340;&#20113;&#26159;&#21542;&#27604;&#20182;&#30340;&#21516;&#26102;&#20195;&#26356;&#30495;&#23454;&#65311;
&lt;/p&gt;
&lt;p&gt;
A Machine Learning Paradigm for Studying Pictorial Realism: Are Constable's Clouds More Real than His Contemporaries?. (arXiv:2202.09348v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.09348
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#30740;&#31350;&#22270;&#30011;&#29616;&#23454;&#20027;&#20041;&#30340;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#36890;&#36807;&#27979;&#37327;&#33402;&#26415;&#23478;&#32472;&#21046;&#30340;&#20113;&#21644;&#20113;&#30340;&#29031;&#29255;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#26469;&#35780;&#20272;&#20889;&#23454;&#20027;&#20041;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#24247;&#26031;&#29305;&#24067;&#23572;&#27604;&#20182;&#30340;&#21516;&#26102;&#20195;&#26356;&#21152;&#19968;&#33268;&#22320;&#21576;&#29616;&#24418;&#24335;&#19978;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33521;&#22269;&#23665;&#27700;&#30011;&#23478;&#32422;&#32752;&#183;&#24247;&#26031;&#29305;&#24067;&#23572;&#34987;&#35748;&#20026;&#26159;19&#19990;&#32426;&#27431;&#27954;&#32472;&#30011;&#29616;&#23454;&#20027;&#20041;&#36816;&#21160;&#30340;&#22880;&#22522;&#20154;&#12290;&#24247;&#26031;&#29305;&#24067;&#23572;&#30340;&#32472;&#21046;&#30340;&#22825;&#31354;&#65292;&#23588;&#20854;&#26159;&#20182;&#30340;&#21516;&#26102;&#20195;&#35748;&#20026;&#38750;&#24120;&#20934;&#30830;&#65292;&#29616;&#22312;&#35768;&#22810;&#35266;&#20247;&#20063;&#26377;&#30456;&#21516;&#30340;&#24863;&#21463;&#12290;&#28982;&#32780;&#65292;&#35780;&#20272;&#31867;&#20284;&#24247;&#26031;&#29305;&#24067;&#23572;&#36825;&#26679;&#30340;&#20889;&#23454;&#30011;&#20316;&#30340;&#20934;&#30830;&#24615;&#26159;&#20027;&#35266;&#25110;&#30452;&#35273;&#30340;&#65292;&#21363;&#20351;&#23545;&#20110;&#19987;&#19994;&#33402;&#26415;&#21490;&#23398;&#23478;&#26469;&#35828;&#20063;&#24456;&#22256;&#38590;&#65292;&#38590;&#20197;&#30830;&#23450;&#24247;&#26031;&#29305;&#24067;&#23572;&#30340;&#22825;&#31354;&#19982;&#20182;&#30340;&#21516;&#26102;&#20195;&#26377;&#20160;&#20040;&#19981;&#21516;&#20043;&#22788;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20026;&#24247;&#26031;&#29305;&#24067;&#23572;&#30340;&#20889;&#23454;&#20027;&#20041;&#25552;&#20379;&#26356;&#23458;&#35266;&#30340;&#29702;&#35299;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#22270;&#30011;&#29616;&#23454;&#20027;&#20041;&#30740;&#31350;&#33539;&#24335;&#65292;&#20197;&#21487;&#35299;&#37322;&#30340;&#26041;&#24335;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#27979;&#37327;&#20687;&#24247;&#26031;&#29305;&#24067;&#23572;&#36825;&#26679;&#20197;&#22825;&#31354;&#38395;&#21517;&#30340;&#33402;&#26415;&#23478;&#32472;&#21046;&#30340;&#20113;&#21644;&#20113;&#30340;&#29031;&#29255;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#26469;&#35780;&#20272;&#20889;&#23454;&#20027;&#20041;&#12290;&#20113;&#20998;&#31867;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#24247;&#26031;&#29305;&#24067;&#23572;&#27604;&#20182;&#30340;&#21516;&#26102;&#20195;&#26356;&#21152;&#19968;&#33268;&#22320;&#21576;&#29616;&#24418;&#24335;&#19978;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
The British landscape painter John Constable is considered foundational for the Realist movement in 19th-century European painting. Constable's painted skies, in particular, were seen as remarkably accurate by his contemporaries, an impression shared by many viewers today. Yet, assessing the accuracy of realist paintings like Constable's is subjective or intuitive, even for professional art historians, making it difficult to say with certainty what set Constable's skies apart from those of his contemporaries. Our goal is to contribute to a more objective understanding of Constable's realism. We propose a new machine-learning-based paradigm for studying pictorial realism in an explainable way. Our framework assesses realism by measuring the similarity between clouds painted by artists noted for their skies, like Constable, and photographs of clouds. The experimental results of cloud classification show that Constable approximates more consistently than his contemporaries the formal feat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#23567;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#32852;&#21512;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#27169;&#22411;&#32858;&#21512;&#19982;&#26412;&#22320;&#27169;&#22411;&#30340;&#32622;&#25442;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#26356;&#39640;&#25928;&#22320;&#22312;&#25968;&#25454;&#31232;&#30095;&#30340;&#39046;&#22495;&#36827;&#34892;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2110.03469</link><description>&lt;p&gt;
&#20174;&#23567;&#25968;&#25454;&#38598;&#20013;&#23454;&#29616;&#32852;&#21512;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Learning from Small Datasets. (arXiv:2110.03469v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.03469
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#23567;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#32852;&#21512;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#27169;&#22411;&#32858;&#21512;&#19982;&#26412;&#22320;&#27169;&#22411;&#30340;&#32622;&#25442;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#26356;&#39640;&#25928;&#22320;&#22312;&#25968;&#25454;&#31232;&#30095;&#30340;&#39046;&#22495;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#21512;&#23398;&#20064;&#20801;&#35768;&#22810;&#20010;&#21442;&#19982;&#26041;&#22312;&#19981;&#20849;&#20139;&#26412;&#22320;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21327;&#21516;&#35757;&#32451;&#19968;&#20010;&#32852;&#21512;&#27169;&#22411;&#12290;&#36825;&#22312;&#21307;&#30103;&#39046;&#22495;&#31561;&#25968;&#25454;&#26412;&#36523;&#20998;&#24067;&#20998;&#25955;&#12289;&#26080;&#27861;&#20844;&#24320;&#30340;&#24773;&#20917;&#19979;&#65292;&#20026;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#25552;&#20379;&#20102;&#21487;&#33021;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#36890;&#24120;&#36890;&#36807;&#32858;&#21512;&#26412;&#22320;&#27169;&#22411;&#23454;&#29616;&#32852;&#21512;&#35757;&#32451;&#65292;&#32780;&#26412;&#22320;&#35757;&#32451;&#30446;&#26631;&#30340;&#26399;&#26395;&#19982;&#20840;&#23616;&#30446;&#26631;&#30456;&#20284;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26412;&#22320;&#25968;&#25454;&#38598;&#36890;&#24120;&#24456;&#23567;&#65292;&#23548;&#33268;&#26412;&#22320;&#30446;&#26631;&#19982;&#20840;&#23616;&#30446;&#26631;&#24046;&#24322;&#24456;&#22823;&#65292;&#20174;&#32780;&#20351;&#32852;&#21512;&#23398;&#20064;&#22833;&#36133;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#27169;&#22411;&#32858;&#21512;&#19982;&#26412;&#22320;&#27169;&#22411;&#30340;&#32622;&#25442;&#30456;&#32467;&#21512;&#12290;&#36825;&#31181;&#32622;&#25442;&#23558;&#27599;&#20010;&#26412;&#22320;&#27169;&#22411;&#26292;&#38706;&#32473;&#19968;&#31995;&#21015;&#26412;&#22320;&#25968;&#25454;&#38598;&#65292;&#20174;&#32780;&#22312;&#25968;&#25454;&#31232;&#30095;&#30340;&#39046;&#22495;&#20013;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#35757;&#32451;&#12290;&#36825;&#20351;&#24471;&#21487;&#20197;&#22312;&#26497;&#23567;&#30340;&#26412;&#22320;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20363;&#22914;&#36328;&#21307;&#38498;&#30340;&#24739;&#32773;&#25968;&#25454;&#65292;&#21516;&#26102;&#20445;&#25345;&#32852;&#21512;&#23398;&#20064;&#30340;&#35757;&#32451;&#25928;&#29575;&#21644;&#38544;&#31169;&#20445;&#25252;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning allows multiple parties to collaboratively train a joint model without sharing local data. This enables applications of machine learning in settings of inherently distributed, undisclosable data such as in the medical domain. In practice, joint training is usually achieved by aggregating local models, for which local training objectives have to be in expectation similar to the joint (global) objective. Often, however, local datasets are so small that local objectives differ greatly from the global objective, resulting in federated learning to fail. We propose a novel approach that intertwines model aggregations with permutations of local models. The permutations expose each local model to a daisy chain of local datasets resulting in more efficient training in data-sparse domains. This enables training on extremely small local datasets, such as patient data across hospitals, while retaining the training efficiency and privacy benefits of federated learning.
&lt;/p&gt;</description></item></channel></rss>