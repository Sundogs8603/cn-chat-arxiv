<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>T-Dex&#26159;&#19968;&#31181;&#22522;&#20110;&#35302;&#35273;&#30340;&#28789;&#24039;&#24615;&#26041;&#27861;&#65292;&#21487;&#22312;&#33258;&#25105;&#30417;&#30563;&#24335;&#30340;&#35302;&#35273;&#32534;&#30721;&#22120;&#21644;&#19968;&#20123;&#28789;&#24039;&#24615;&#20219;&#21153;&#28436;&#31034;&#30340;&#25351;&#23548;&#19979;&#65292;&#23558;&#35302;&#35273;&#21644;&#35270;&#35273;&#32467;&#21512;&#36215;&#26469;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#32431;&#35270;&#35273;&#26041;&#27861;&#26356;&#26377;&#25928;&#65292;&#24182;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#20855;&#26377;&#26356;&#22909;&#30340;&#24212;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.12076</link><description>&lt;p&gt;
&#35302;&#35273;&#35757;&#32451;&#19979;&#30340;&#26426;&#22120;&#20154;&#28789;&#24039;&#24615;
&lt;/p&gt;
&lt;p&gt;
Dexterity from Touch: Self-Supervised Pre-Training of Tactile Representations with Robotic Play. (arXiv:2303.12076v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12076
&lt;/p&gt;
&lt;p&gt;
T-Dex&#26159;&#19968;&#31181;&#22522;&#20110;&#35302;&#35273;&#30340;&#28789;&#24039;&#24615;&#26041;&#27861;&#65292;&#21487;&#22312;&#33258;&#25105;&#30417;&#30563;&#24335;&#30340;&#35302;&#35273;&#32534;&#30721;&#22120;&#21644;&#19968;&#20123;&#28789;&#24039;&#24615;&#20219;&#21153;&#28436;&#31034;&#30340;&#25351;&#23548;&#19979;&#65292;&#23558;&#35302;&#35273;&#21644;&#35270;&#35273;&#32467;&#21512;&#36215;&#26469;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#32431;&#35270;&#35273;&#26041;&#27861;&#26356;&#26377;&#25928;&#65292;&#24182;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#20855;&#26377;&#26356;&#22909;&#30340;&#24212;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#20154;&#39046;&#22495;&#65292;&#35753;&#20855;&#26377;&#22810;&#25351;&#30340;&#26426;&#22120;&#20154;&#20855;&#26377;&#28789;&#24039;&#24615;&#19968;&#30452;&#26159;&#19968;&#20010;&#38271;&#26399;&#30340;&#25361;&#25112;&#12290;&#27492;&#21069;&#65292;&#26368;&#37325;&#35201;&#30340;&#24037;&#20316;&#37117;&#38598;&#20013;&#22312;&#23398;&#20064;&#25511;&#21046;&#22120;&#25110;&#31574;&#30053;&#19978;&#65292;&#36825;&#20123;&#25511;&#21046;&#22120;&#25110;&#31574;&#30053;&#35201;&#20040;&#22522;&#20110;&#35270;&#35273;&#35266;&#27979;&#65292;&#35201;&#20040;&#22522;&#20110;&#20174;&#35270;&#35273;&#25512;&#26029;&#24471;&#21040;&#30340;&#29366;&#24577;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#38656;&#35201;&#25512;&#29702;&#25509;&#35302;&#21147;&#25110;&#36890;&#36807;&#25163;&#26412;&#36523;&#36974;&#25377;&#30340;&#29289;&#20307;&#30340;&#32454;&#31890;&#24230;&#25805;&#20316;&#20219;&#21153;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#35302;&#35273;&#30340;&#28789;&#24039;&#24615;&#26041;&#27861;T-Dex&#65292;&#35813;&#26041;&#27861;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#65306;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#25910;&#38598;2.5&#23567;&#26102;&#30340;&#28216;&#25103;&#25968;&#25454;&#24182;&#20351;&#29992;&#36825;&#20123;&#25968;&#25454;&#35757;&#32451;&#33258;&#25105;&#30417;&#30563;&#22411;&#35302;&#35273;&#32534;&#30721;&#22120;&#65307;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;&#21033;&#29992;&#23569;&#37327;&#30340;&#28789;&#24039;&#24615;&#20219;&#21153;&#28436;&#31034;&#65292;&#23398;&#20064;&#23558;&#35302;&#35273;&#35266;&#27979;&#21644;&#35270;&#35273;&#35266;&#27979;&#30456;&#32467;&#21512;&#30340;&#38750;&#21442;&#25968;&#21270;&#31574;&#30053;&#12290;&#36890;&#36807;&#20116;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#28789;&#24039;&#24615;&#20219;&#21153;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#22522;&#20110;&#35302;&#35273;&#30340;&#28789;&#24039;&#24615;&#27169;&#22411;&#27604;&#32431;&#35270;&#35273;&#26041;&#27861;&#26356;&#26377;&#25928;&#65292;&#24182;&#19988;&#33021;&#22815;&#25512;&#24191;&#21040;&#29616;&#23454;&#19990;&#30028;&#20013;&#35270;&#35273;&#26041;&#27861;&#22833;&#36133;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Teaching dexterity to multi-fingered robots has been a longstanding challenge in robotics. Most prominent work in this area focuses on learning controllers or policies that either operate on visual observations or state estimates derived from vision. However, such methods perform poorly on fine-grained manipulation tasks that require reasoning about contact forces or about objects occluded by the hand itself. In this work, we present T-Dex, a new approach for tactile-based dexterity, that operates in two phases. In the first phase, we collect 2.5 hours of play data, which is used to train self-supervised tactile encoders. This is necessary to bring high-dimensional tactile readings to a lower-dimensional embedding. In the second phase, given a handful of demonstrations for a dexterous task, we learn non-parametric policies that combine the tactile observations with visual ones. Across five challenging dexterous tasks, we show that our tactile-based dexterity models outperform purely vi
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25506;&#35752;&#20102;AI&#21327;&#21516;&#21512;&#20316;&#30340;&#21382;&#21490;&#21644;&#35201;&#27714;&#65292;&#26159;&#21327;&#21516;AI&#30740;&#31350;&#30340;&#21160;&#26426;&#21644;&#32972;&#26223;&#12290;</title><link>http://arxiv.org/abs/2303.12040</link><description>&lt;p&gt;
&#21327;&#21516;&#20154;&#24037;&#26234;&#33021;&#30340;&#26681;&#28304;&#21644;&#35201;&#27714;
&lt;/p&gt;
&lt;p&gt;
Roots and Requirements for Collaborative AI. (arXiv:2303.12040v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12040
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25506;&#35752;&#20102;AI&#21327;&#21516;&#21512;&#20316;&#30340;&#21382;&#21490;&#21644;&#35201;&#27714;&#65292;&#26159;&#21327;&#21516;AI&#30740;&#31350;&#30340;&#21160;&#26426;&#21644;&#32972;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#21327;&#20316;&#32773;&#30340;&#24895;&#26223;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#26159;&#31185;&#24187;&#23567;&#35828;&#30340;&#32463;&#20856;&#32032;&#26448;&#65292;&#20854;&#20013;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#29702;&#35299;&#21327;&#20316;&#21644;&#20154;&#31867;&#27807;&#36890;&#30340;&#24494;&#22937;&#24046;&#21035;&#12290;&#23427;&#20204;&#36890;&#36807;&#36129;&#29486;&#29305;&#27530;&#30340;&#25165;&#33021;&#32473;&#20182;&#20204;&#30340;&#20154;&#31867;&#21512;&#20316;&#32773;&#21644;&#22242;&#38431;&#24102;&#26469;&#20248;&#21183;&#12290;&#22810;&#24180;&#26469;&#65292;&#25919;&#24220;&#21672;&#35810;&#22242;&#20307;&#21644;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#39046;&#34966;&#19968;&#30452;&#20513;&#23548;AIs&#24212;&#35813;&#20855;&#26377;&#20154;&#31867;&#20860;&#23481;&#24615;&#21644;&#26377;&#25928;&#21327;&#20316;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20855;&#22791;&#20687;&#25165;&#21326;&#27178;&#28322;&#30340;&#20154;&#37027;&#26679;&#21327;&#20316;&#33021;&#21147;&#30340;&#24378;&#22823;&#30340;AI&#20173;&#28982;&#36965;&#19981;&#21487;&#21450;&#12290;&#36825;&#31687;&#35770;&#25991;&#20381;&#25454;&#23545;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#20195;&#29702;&#26377;&#25928;&#21644;&#24378;&#22823;&#21327;&#20316;&#25152;&#38656;&#35748;&#30693;&#30340;&#20998;&#26512;&#65292;&#27010;&#36848;&#20102;&#20844;&#20247;&#21644;AI&#24895;&#26223;&#20013;&#20851;&#20110;&#20154;&#24037;&#21327;&#20316;&#32773;&#30340;&#21382;&#21490;&#65292;&#24320;&#22987;&#20110;&#26089;&#26399;&#26234;&#33021;&#22686;&#24378;(IA)&#21644;&#20154;&#24037;&#26234;&#33021;(AI)&#30340;&#24895;&#26223;&#12290;&#36825;&#31687;&#35770;&#25991;&#26088;&#22312;&#25104;&#20026;&#21327;&#21516;AI&#30340;&#31532;&#20108;&#20010;&#31435;&#22330;&#25991;&#20214;(Stefik &amp; Price, 2023)&#30340;&#21160;&#26426;&#21644;&#32972;&#26223;&#12290;&#31532;&#20108;&#31687;&#35770;&#25991;&#22238;&#39038;&#20102;&#22810;&#23398;&#31185;&#30340;&#29616;&#29366;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;AI&#21327;&#20316;&#30740;&#31350;&#30340;&#36335;&#32447;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
The vision of AI collaborators has long been a staple of science fiction, where artificial agents understand nuances of collaboration and human communication. They bring advantages to their human collaborators and teams by contributing their special talents. Government advisory groups and leaders in AI have advocated for years that AIs should be human compatible and be capable of effective collaboration. Nonetheless, robust AIs that can collaborate like talented people remain out of reach. This position paper draws on a cognitive analysis of what effective and robust collaboration requires of human and artificial agents. It sketches a history of public and AI visions for artificial collaborators, starting with early visions of intelligence augmentation (IA) and artificial intelligence (AI). It is intended as motivation and context for a second position paper on collaborative AI (Stefik &amp; Price, 2023). The second paper reviews the multi-disciplinary state-of-the-art and proposes a roadm
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#28548;&#28165;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#34920;&#24449;&#29366;&#24577;&#12290;&#23613;&#31649;&#36890;&#24120;&#31216;&#20026;&#8220;&#34920;&#24449;&#8221;&#65292;&#20294;&#23454;&#38469;&#19978;&#23427;&#20204;&#26356;&#36866;&#21512;&#29702;&#35299;&#20026;&#39640;&#24230;&#29702;&#24819;&#21270;&#30340;&#27169;&#22411;&#65292;&#36825;&#19968;&#32467;&#26524;&#23545;&#21487;&#35299;&#37322;&#30340;AI&#26377;&#30528;&#30452;&#25509;&#24433;&#21709;&#65292;&#20063;&#24341;&#36215;&#20102;&#21746;&#23398;&#23478;&#23545;&#20854;&#22312;&#26410;&#26469;&#31185;&#23398;&#30740;&#31350;&#20013;&#30340;&#20316;&#29992;&#30340;&#20851;&#27880;&#12290;</title><link>http://arxiv.org/abs/2303.12032</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#34920;&#24449;&#29366;&#24577;
&lt;/p&gt;
&lt;p&gt;
The Representational Status of Deep Learning Models. (arXiv:2303.12032v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12032
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#28548;&#28165;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#34920;&#24449;&#29366;&#24577;&#12290;&#23613;&#31649;&#36890;&#24120;&#31216;&#20026;&#8220;&#34920;&#24449;&#8221;&#65292;&#20294;&#23454;&#38469;&#19978;&#23427;&#20204;&#26356;&#36866;&#21512;&#29702;&#35299;&#20026;&#39640;&#24230;&#29702;&#24819;&#21270;&#30340;&#27169;&#22411;&#65292;&#36825;&#19968;&#32467;&#26524;&#23545;&#21487;&#35299;&#37322;&#30340;AI&#26377;&#30528;&#30452;&#25509;&#24433;&#21709;&#65292;&#20063;&#24341;&#36215;&#20102;&#21746;&#23398;&#23478;&#23545;&#20854;&#22312;&#26410;&#26469;&#31185;&#23398;&#30740;&#31350;&#20013;&#30340;&#20316;&#29992;&#30340;&#20851;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#28548;&#28165;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65288;DLMs&#65289;&#30340;&#34920;&#24449;&#29366;&#24577;&#12290;&#30001;&#20110;&#21151;&#33021;&#21644;&#20851;&#31995;&#27010;&#24565;&#30340;&#28151;&#28102;&#65292;&#23613;&#31649;&#36890;&#24120;&#31216;&#20026;&#8220;&#34920;&#24449;&#8221;&#65292;&#20294;&#36825;&#24847;&#21619;&#30528;&#21547;&#31946;&#19981;&#28165;&#12290;&#26412;&#25991;&#35748;&#20026;&#65292;&#34429;&#28982;DLM&#20197;&#20851;&#31995;&#24847;&#20041;&#19978;&#30340;&#34920;&#24449;&#20854;&#30446;&#26631;&#65292;&#20294;&#26368;&#22909;&#29702;&#35299;&#20026;&#39640;&#24230;&#29702;&#24819;&#21270;&#30340;&#27169;&#22411;&#12290;&#36825;&#20010;&#32467;&#26524;&#23545;&#21487;&#35299;&#37322;&#30340;AI&#65288;XAI&#65289;&#26377;&#30452;&#25509;&#24433;&#21709;&#65292;&#24182;&#24341;&#23548;&#21746;&#23398;&#20851;&#27880;DLM&#34920;&#24449;&#30340;&#29702;&#24819;&#21270;&#24615;&#36136;&#21450;&#20854;&#22312;&#26410;&#26469;&#31185;&#23398;&#30740;&#31350;&#20013;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper aims to clarify the representational status of Deep Learning Models (DLMs). While commonly referred to as 'representations', what this entails is ambiguous due to a conflation of functional and relational conceptions of representation. This paper argues that while DLMs represent their targets in a relational sense, they are best understood as highly idealized models. This result has immediate implications for explainable AI (XAI) and directs philosophical attention toward examining the idealized nature of DLM representations and their role in future scientific investigation.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;cTBL&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#34920;&#26684;&#20013;&#26816;&#32034;&#20449;&#24687;&#65292;&#24182;&#29983;&#25104;&#20855;&#26377;&#26816;&#32034;&#20449;&#24687;&#25903;&#25745;&#30340;&#23545;&#35805;&#21709;&#24212;&#65292;&#20854;&#20013;&#20351;&#29992;&#20102;&#36716;&#25442;&#22120;&#32534;&#30721;&#22120;&#23884;&#20837;&#36827;&#34892;&#27987;&#23494;&#34920;&#26816;&#32034;&#65292;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.12024</link><description>&lt;p&gt;
cTBL&#65306;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#23545;&#35805;&#34920;&#26684;
&lt;/p&gt;
&lt;p&gt;
cTBL: Augmenting Large Language Models for Conversational Tables. (arXiv:2303.12024v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12024
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;cTBL&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#34920;&#26684;&#20013;&#26816;&#32034;&#20449;&#24687;&#65292;&#24182;&#29983;&#25104;&#20855;&#26377;&#26816;&#32034;&#20449;&#24687;&#25903;&#25745;&#30340;&#23545;&#35805;&#21709;&#24212;&#65292;&#20854;&#20013;&#20351;&#29992;&#20102;&#36716;&#25442;&#22120;&#32534;&#30721;&#22120;&#23884;&#20837;&#36827;&#34892;&#27987;&#23494;&#34920;&#26816;&#32034;&#65292;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#23545;&#35805;&#20154;&#24037;&#26234;&#33021;&#20013;&#19968;&#20010;&#24320;&#25918;&#30340;&#25361;&#25112;&#26159;&#22914;&#20309;&#20174;&#25991;&#26412;&#21644;&#38750;&#25991;&#26412;&#26469;&#28304;&#20013;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#36827;&#34892;&#22810;&#36718;&#23545;&#35805;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;Conversation Table (cTBL)&#65292;&#36825;&#26159;&#19968;&#31181;&#19977;&#27493;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#32034;&#34920;&#26684;&#20449;&#24687;&#24182;&#29983;&#25104;&#22522;&#20110;&#26816;&#32034;&#20449;&#24687;&#30340;&#23545;&#35805;&#21709;&#24212;&#12290;cTBL&#20351;&#29992;&#36716;&#25442;&#22120;&#32534;&#30721;&#22120;&#23884;&#20837;&#36827;&#34892;&#27987;&#23494;&#34920;&#26816;&#32034;&#65292;&#24182;&#22312;HyrbiDialogue&#25968;&#25454;&#38598;Top-1&#21644;Top-3&#20934;&#30830;&#24615;&#19978;&#30456;&#23545;&#20110;&#31232;&#30095;&#26816;&#32034;&#25552;&#39640;&#20102;&#26368;&#22810;5%&#12290;&#27492;&#22806;&#65292;cTBL&#20351;&#29992;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#27169;&#22411;&#36827;&#34892;&#34920;&#26684;&#30693;&#35782;&#26816;&#32034;&#65292;&#22312;HyrbiDialogue&#19978;&#20135;&#29983;&#20102;&#26368;&#39640;46%&#30340;ROUGE&#20998;&#25968;&#30456;&#23545;&#25913;&#36827;&#65292;&#24182;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#20154;&#24037;&#35780;&#20272;&#21709;&#24212;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
An open challenge in multimodal conversational AI requires augmenting large language models with information from textual and non-textual sources for multi-turn dialogue. To address this problem, this paper introduces Conversational Tables (cTBL), a three-step encoder-decoder approach to retrieve tabular information and generate dialogue responses grounded on the retrieved information. cTBL uses Transformer encoder embeddings for Dense Table Retrieval and obtains up to 5% relative improvement in Top-1 and Top-3 accuracy over sparse retrieval on the HyrbiDialogue dataset. Additionally, cTBL performs tabular knowledge retrieval using both encoder and decoder models, resulting in up to 46% relative improvement in ROUGE scores and better human evaluation for response generation on HyrbiDialogue.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24635;&#32467;&#20102;&#19968;&#31181;&#26032;&#30340;&#36923;&#36753;&#25512;&#29702;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#20316;&#20026;&#30693;&#35782;&#34920;&#31034;&#65292;&#20855;&#26377;&#19981;&#21516;&#20110;&#31471;&#21040;&#31471;&#31070;&#32463;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;&#36825;&#31181;&#26032;&#27169;&#24335;&#22312;&#26410;&#26469;&#26377;&#30528;&#24456;&#39640;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.12023</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#20316;&#20026;&#30693;&#35782;&#34920;&#31034;&#30340;&#36923;&#36753;&#25512;&#29702;&#30740;&#31350;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Logical Reasoning over Natural Language as Knowledge Representation: A Survey. (arXiv:2303.12023v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12023
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24635;&#32467;&#20102;&#19968;&#31181;&#26032;&#30340;&#36923;&#36753;&#25512;&#29702;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#20316;&#20026;&#30693;&#35782;&#34920;&#31034;&#65292;&#20855;&#26377;&#19981;&#21516;&#20110;&#31471;&#21040;&#31471;&#31070;&#32463;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;&#36825;&#31181;&#26032;&#27169;&#24335;&#22312;&#26410;&#26469;&#26377;&#30528;&#24456;&#39640;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36923;&#36753;&#25512;&#29702;&#26159;&#20154;&#31867;&#35748;&#30693;&#21644;&#26234;&#33021;&#30340;&#26680;&#24515;&#12290;&#20197;&#24448;&#30340;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#36923;&#36753;&#25512;&#29702;&#30740;&#31350;&#20351;&#29992;&#24418;&#24335;&#21270;&#35821;&#35328;&#20316;&#20026;&#30693;&#35782;&#34920;&#31034;&#65288;&#21644;&#31526;&#21495;&#25512;&#29702;&#22120;&#65289;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#24418;&#24335;&#21270;&#35821;&#35328;&#36827;&#34892;&#25512;&#29702;&#35777;&#26126;&#20855;&#26377;&#22256;&#38590;&#65288;&#20363;&#22914;&#33030;&#24369;&#24615;&#21644;&#30693;&#35782;&#33719;&#21462;&#29942;&#39048;&#65289;&#12290;&#26412;&#25991;&#24635;&#32467;&#20102;&#19968;&#31181;&#26032;&#30340;&#36923;&#36753;&#25512;&#29702;&#26041;&#27861;&#30340;&#32508;&#21512;&#27010;&#36848;&#65292;&#23427;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#20316;&#20026;&#30693;&#35782;&#34920;&#31034;&#65288;&#20197;&#21450;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25512;&#29702;&#22120;&#65289;&#65292;&#21253;&#25324;&#36923;&#36753;&#25512;&#29702;&#30340;&#21746;&#23398;&#23450;&#20041;&#21644;&#20998;&#31867;&#65292;&#26032;&#27169;&#24335;&#30340;&#20248;&#21183;&#12289;&#22522;&#20934;&#21644;&#26041;&#27861;&#65292;&#26410;&#26469;&#38656;&#35201;&#30340;&#20219;&#21153;&#21644;&#26041;&#27861;&#20197;&#21450;&#19982;&#30456;&#20851; NLP &#39046;&#22495;&#30340;&#20851;&#31995;&#12290;&#36825;&#31181;&#26032;&#27169;&#24335;&#26159;&#24456;&#26377;&#21069;&#36884;&#30340;&#65292;&#22240;&#20026;&#23427;&#19981;&#20165;&#21487;&#20197;&#32531;&#35299;&#24418;&#24335;&#21270;&#34920;&#31034;&#30340;&#35768;&#22810;&#25361;&#25112;&#65292;&#32780;&#19988;&#20063;&#20855;&#26377;&#20248;&#20110;&#31471;&#21040;&#31471;&#31070;&#32463;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Logical reasoning is central to human cognition and intelligence. Past research of logical reasoning within AI uses formal language as knowledge representation~(and symbolic reasoners). However, reasoning with formal language has proved challenging~(e.g., brittleness and knowledge-acquisition bottleneck). This paper provides a comprehensive overview on a new paradigm of logical reasoning, which uses natural language as knowledge representation~(and pretrained language models as reasoners), including philosophical definition and categorization of logical reasoning, advantages of the new paradigm, benchmarks and methods, challenges of the new paradigm, desirable tasks &amp; methods in the future, and relation to related NLP fields. This new paradigm is promising since it not only alleviates many challenges of formal representation but also has advantages over end-to-end neural methods.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#23545;&#27604;&#20102;&#20845;&#20010;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#32842;&#22825;&#26426;&#22120;&#20154;&#21644;&#20154;&#31867;&#29983;&#25104;&#30340;&#21019;&#24847;&#65292;&#21457;&#29616;&#26377;9.4&#65285;&#30340;&#20154;&#31867;&#27604;&#26368;&#26377;&#21019;&#36896;&#21147;&#30340;GPT-4&#26356;&#26377;&#21019;&#36896;&#21147;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#21019;&#36896;&#24615;&#36807;&#31243;&#20013;&#26159;&#26377;&#20215;&#20540;&#30340;&#21161;&#25163;&#12290;</title><link>http://arxiv.org/abs/2303.12003</link><description>&lt;p&gt;
&#20154;&#24037;&#32554;&#26031;&#65306;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#32842;&#22825;&#26426;&#22120;&#20154;&#24050;&#36798;&#21040;&#20154;&#31867;&#21019;&#36896;&#21147;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial muses: Generative Artificial Intelligence Chatbots Have Risen to Human-Level Creativity. (arXiv:2303.12003v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12003
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#23545;&#27604;&#20102;&#20845;&#20010;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#32842;&#22825;&#26426;&#22120;&#20154;&#21644;&#20154;&#31867;&#29983;&#25104;&#30340;&#21019;&#24847;&#65292;&#21457;&#29616;&#26377;9.4&#65285;&#30340;&#20154;&#31867;&#27604;&#26368;&#26377;&#21019;&#36896;&#21147;&#30340;GPT-4&#26356;&#26377;&#21019;&#36896;&#21147;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#21019;&#36896;&#24615;&#36807;&#31243;&#20013;&#26159;&#26377;&#20215;&#20540;&#30340;&#21161;&#25163;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20204;&#26222;&#36941;&#35748;&#20026;&#20154;&#24037;&#26234;&#33021;&#19981;&#20855;&#22791;&#21019;&#36896;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#27604;&#36739;&#20154;&#31867;&#29983;&#25104;&#30340;&#21019;&#24847;&#19982;&#20845;&#20010;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#32842;&#22825;&#26426;&#22120;&#20154;&#65288;Alpa.ai&#65292;Copy.ai&#65292;ChatGPT&#65288;&#29256;&#26412;3&#21644;4&#65289;&#65292;Studio.ai&#21644;YouChat&#65289;&#29983;&#25104;&#30340;&#21019;&#24847;&#26469;&#27979;&#35797;&#36825;&#19968;&#20551;&#35774;&#12290;&#20154;&#31867;&#21644;&#19968;&#31181;&#32463;&#36807;&#29305;&#21035;&#35757;&#32451;&#30340;&#20154;&#24037;&#26234;&#33021;&#29420;&#31435;&#35780;&#20272;&#20102;&#21019;&#24847;&#30340;&#36136;&#37327;&#21644;&#25968;&#37327;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#21019;&#24847;&#22312;&#36136;&#37327;&#19978;&#19982;&#20154;&#31867;&#29983;&#25104;&#30340;&#21019;&#24847;&#27809;&#26377;&#23450;&#24615;&#24046;&#24322;&#65292;&#23613;&#31649;&#23427;&#20204;&#22312;&#21019;&#24847;&#29983;&#25104;&#26041;&#24335;&#19978;&#23384;&#22312;&#24046;&#24322;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;9.4&#65285;&#30340;&#20154;&#31867;&#27604;&#26368;&#26377;&#21019;&#36896;&#21147;&#30340;GAI&#65288;GPT-4&#65289;&#26356;&#26377;&#21019;&#36896;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#21019;&#36896;&#24615;&#36807;&#31243;&#20013;&#26159;&#26377;&#20215;&#20540;&#30340;&#21161;&#25163;&#12290;&#32487;&#32493;&#30740;&#31350;&#21644;&#24320;&#21457;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#21019;&#24847;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#23545;&#20110;&#20805;&#20998;&#20102;&#35299;&#35813;&#25216;&#26415;&#22312;&#22609;&#36896;&#21019;&#24847;&#26410;&#26469;&#30340;&#28508;&#22312;&#30410;&#22788;&#21644;&#32570;&#38519;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#26159;&#21542;&#33021;&#22815;&#30495;&#27491;&#20855;&#26377;&#21019;&#36896;&#21147;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
A widespread view is that Artificial Intelligence cannot be creative. We tested this assumption by comparing human-generated ideas with those generated by six Generative Artificial Intelligence (GAI) chatbots: alpa.ai, Copy.ai, ChatGPT (versions 3 and 4), Studio.ai, and YouChat. Humans and a specifically trained AI independently assessed the quality and quantity of ideas. We found no qualitative difference between AI and human-generated creativity, although there are differences in how ideas are generated. Interestingly, 9.4 percent of humans were more creative than the most creative GAI, GPT-4. Our findings suggest that GAIs are valuable assistants in the creative process. Continued research and development of GAI in creative tasks is crucial to fully understand this technology's potential benefits and drawbacks in shaping the future of creativity. Finally, we discuss the question of whether GAIs are capable of being truly creative.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20849;&#20139;&#21333;&#36710;&#31995;&#32479;&#25193;&#23637;&#30340;&#28145;&#24230;&#20986;&#34892;&#29983;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#22522;&#20110;&#22810;&#28304;&#22478;&#24066;&#24314;&#31569;&#21644;&#22320;&#29702;&#25968;&#25454;&#30340;&#31449;&#28857;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2303.11977</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20849;&#20139;&#21333;&#36710;&#31995;&#32479;&#25193;&#23637;&#30340;&#28145;&#24230;&#20986;&#34892;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Deep trip generation with graph neural networks for bike sharing system expansion. (arXiv:2303.11977v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11977
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20849;&#20139;&#21333;&#36710;&#31995;&#32479;&#25193;&#23637;&#30340;&#28145;&#24230;&#20986;&#34892;&#29983;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#22522;&#20110;&#22810;&#28304;&#22478;&#24066;&#24314;&#31569;&#21644;&#22320;&#29702;&#25968;&#25454;&#30340;&#31449;&#28857;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20849;&#20139;&#21333;&#36710;&#27491;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#20316;&#20026;&#19968;&#31181;&#27963;&#36291;&#65292;&#26041;&#20415;&#21644;&#21487;&#25345;&#32493;&#30340;&#20132;&#36890;&#26041;&#24335;&#32780;&#20852;&#36215;&#12290;&#20026;&#20102;&#35745;&#21010;&#25104;&#21151;&#30340;&#20849;&#20139;&#21333;&#36710;&#31995;&#32479;&#65288;BSS&#65289;&#65292;&#35768;&#22810;&#22478;&#24066;&#20174;&#23567;&#35268;&#27169;&#35797;&#28857;&#24320;&#22987;&#65292;&#24182;&#36880;&#27493;&#25193;&#22823;&#31995;&#32479;&#35206;&#30422;&#26356;&#22810;&#21306;&#22495;&#12290;&#23545;&#20110;&#22522;&#20110;&#31449;&#28857;&#30340;BSS&#65292;&#36825;&#24847;&#21619;&#30528;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#22522;&#20110;&#29616;&#26377;&#31449;&#28857;&#35268;&#21010;&#26032;&#31449;&#28857;&#65292;&#36825;&#38656;&#35201;&#39044;&#27979;&#25972;&#20010;&#31995;&#32479;&#20013;&#36825;&#20123;&#26032;&#31449;&#28857;&#20135;&#29983;&#30340;&#26053;&#34892;&#27425;&#25968;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#24120;&#20381;&#36182;&#20110;&#30456;&#23545;&#31616;&#21333;&#30340;&#22238;&#24402;&#25110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#22312;&#25429;&#25417;&#22797;&#26434;&#31354;&#38388;&#20851;&#31995;&#26041;&#38754;&#21463;&#21040;&#38480;&#21046;&#12290;&#23613;&#31649;&#22312;&#26053;&#34892;&#38656;&#27714;&#39044;&#27979;&#26041;&#38754;&#65292;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#25991;&#29486;&#36234;&#26469;&#36234;&#22810;&#65292;&#20294;&#23427;&#20204;&#22823;&#22810;&#26159;&#22522;&#20110;&#26102;&#24207;&#25968;&#25454;&#30340;&#30701;&#26399;&#39044;&#27979;&#65292;&#20551;&#35774;&#31995;&#32479;&#27809;&#26377;&#32467;&#26500;&#24615;&#21464;&#21270;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#37325;&#28857;&#30740;&#31350;&#20102;BSS&#25193;&#23637;&#30340;&#20986;&#34892;&#29983;&#25104;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#26041;&#27861;&#26469;&#39044;&#27979;&#22522;&#20110;&#22810;&#28304;&#22478;&#24066;&#24314;&#31569;&#21644;&#22320;&#29702;&#25968;&#25454;&#30340;&#31449;&#28857;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bike sharing is emerging globally as an active, convenient, and sustainable mode of transportation. To plan successful bike-sharing systems (BSSs), many cities start from a small-scale pilot and gradually expand the system to cover more areas. For station-based BSSs, this means planning new stations based on existing ones over time, which requires prediction of the number of trips generated by these new stations across the whole system. Previous studies typically rely on relatively simple regression or machine learning models, which are limited in capturing complex spatial relationships. Despite the growing literature in deep learning methods for travel demand prediction, they are mostly developed for short-term prediction based on time series data, assuming no structural changes to the system. In this study, we focus on the trip generation problem for BSS expansion, and propose a graph neural network (GNN) approach to predicting the station-level demand based on multi-source urban bui
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#28041;&#21450;&#25512;&#21160;&#12289;&#31034;&#20363;&#21644;&#25552;&#20986;&#19977;&#31181;&#24178;&#39044;&#26041;&#27861;&#25945;&#25480;&#40664;&#35748;&#23398;&#29983;&#20309;&#26102;&#22914;&#20309;&#20351;&#29992;&#21738;&#31181;&#31574;&#30053;&#65292;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;Nudge&#34920;&#29616;&#26368;&#20339;&#12290;</title><link>http://arxiv.org/abs/2303.11965</link><description>&lt;p&gt;
&#8220;&#25512;&#21160;&#21147;&#8221;&#65306;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#20013;&#20803;&#35748;&#30693;&#25216;&#33021;&#25945;&#23398;&#30340;&#19977;&#31181;&#24178;&#39044;&#26041;&#27861;&#30340;&#25506;&#31350;
&lt;/p&gt;
&lt;p&gt;
The Power of Nudging: Exploring Three Interventions for Metacognitive Skills Instruction across Intelligent Tutoring Systems. (arXiv:2303.11965v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11965
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#28041;&#21450;&#25512;&#21160;&#12289;&#31034;&#20363;&#21644;&#25552;&#20986;&#19977;&#31181;&#24178;&#39044;&#26041;&#27861;&#25945;&#25480;&#40664;&#35748;&#23398;&#29983;&#20309;&#26102;&#22914;&#20309;&#20351;&#29992;&#21738;&#31181;&#31574;&#30053;&#65292;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;Nudge&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35748;&#35777;&#22495;&#36890;&#24120;&#20855;&#26377;&#24456;&#22810;&#35748;&#30693;&#25216;&#33021;&#65292;&#20854;&#20013;&#27809;&#26377;&#19968;&#31181;&#21333;&#19968;&#30340;&#35299;&#20915;&#31574;&#30053;&#21487;&#36866;&#29992;&#20110;&#35299;&#20915;&#25152;&#26377;&#38382;&#39064;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#30693;&#36947;&#20309;&#26102;&#22914;&#20309;&#20351;&#29992;&#27599;&#31181;&#31574;&#30053;&#30340;&#23398;&#29983;&#65288;StrTime&#65289;&#34920;&#29616;&#20248;&#20110;&#37027;&#20123;&#35841;&#19981;&#30693;&#36947;&#24182;&#22362;&#25345;&#40664;&#35748;&#31574;&#30053;&#65288;Default&#65289;&#30340;&#23398;&#29983;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#23398;&#29983;&#22312;&#36923;&#36753;&#36741;&#23548;&#21592;&#30340;&#35757;&#32451;&#20013;&#20351;&#29992;&#40664;&#35748;&#30340;&#21069;&#21521;&#38142;&#25509;&#21644;&#21518;&#21521;&#38142;&#25509;&#65288;BC&#65289;&#31574;&#30053;&#65292;&#28982;&#21518;&#22312;&#27010;&#29575;&#36741;&#23548;&#21592;&#20013;&#21482;&#20351;&#29992;BC&#25903;&#25345;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19977;&#31181;&#22312;&#36923;&#36753;&#36741;&#23548;&#21592;&#19978;&#25945;&#25480;&#40664;&#35748;&#23398;&#29983;&#20309;&#26102;&#22914;&#20309;&#20351;&#29992;&#21738;&#31181;&#31574;&#30053;&#30340;&#24178;&#39044;&#25514;&#26045;&#65306;&#31034;&#20363;&#12289;&#25512;&#21160;&#21644;&#25552;&#20986;&#12290;&#21516;&#26102;&#65292;StrTime&#23398;&#29983;&#27809;&#26377;&#25509;&#21463;&#20219;&#20309;&#24178;&#39044;&#25514;&#26045;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;Nudge&#30340;&#34920;&#29616;&#20248;&#20110;&#20854;Default&#21516;&#20276;&#65292;&#24182;&#22312;&#20004;&#20010;&#36741;&#23548;&#21592;&#19978;&#36861;&#36214;StrTime&#12290;
&lt;/p&gt;
&lt;p&gt;
Deductive domains are typical of many cognitive skills in that no single problem-solving strategy is always optimal for solving all problems. It was shown that students who know how and when to use each strategy (StrTime) outperformed those who know neither and stick to the default strategy (Default). In this work, students were trained on a logic tutor that supports a default forward-chaining and a backward-chaining (BC) strategy, then a probability tutor that only supports BC. We investigated three types of interventions on teaching the Default students how and when to use which strategy on the logic tutor: Example, Nudge and Presented. Meanwhile, StrTime students received no interventions. Overall, our results show that Nudge outperformed their Default peers and caught up with StrTime on both tutors.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#12289;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#21644;&#20132;&#21449;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#36328;&#39046;&#22495;&#35875;&#35328;&#26816;&#27979;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#19981;&#20165;&#21487;&#20197;&#36827;&#34892;&#36328;&#39046;&#22495;&#29305;&#24449;&#23545;&#40784;&#65292;&#36824;&#21487;&#20197;&#24378;&#21046;&#30446;&#26631;&#26679;&#26412;&#19982;&#28304;&#22495;&#20013;&#30340;&#30456;&#24212;&#21407;&#22411;&#23545;&#40784;&#65292;&#24182;&#20351;&#29992;&#32858;&#31867;&#26041;&#27861;&#20135;&#29983;&#20266;&#26631;&#31614;&#26469;&#23398;&#20064;&#22495;&#19981;&#21464;&#34920;&#31034;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#36328;&#39046;&#22495;&#35875;&#35328;&#26816;&#27979;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.11945</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#36328;&#39046;&#22495;&#35875;&#35328;&#26816;&#27979;&#65306;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#21644;&#20132;&#21449;&#27880;&#24847;&#21147;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Cross-Domain Rumor Detection with Contrastive Learning and Cross-Attention. (arXiv:2303.11945v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11945
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#12289;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#21644;&#20132;&#21449;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#36328;&#39046;&#22495;&#35875;&#35328;&#26816;&#27979;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#19981;&#20165;&#21487;&#20197;&#36827;&#34892;&#36328;&#39046;&#22495;&#29305;&#24449;&#23545;&#40784;&#65292;&#36824;&#21487;&#20197;&#24378;&#21046;&#30446;&#26631;&#26679;&#26412;&#19982;&#28304;&#22495;&#20013;&#30340;&#30456;&#24212;&#21407;&#22411;&#23545;&#40784;&#65292;&#24182;&#20351;&#29992;&#32858;&#31867;&#26041;&#27861;&#20135;&#29983;&#20266;&#26631;&#31614;&#26469;&#23398;&#20064;&#22495;&#19981;&#21464;&#34920;&#31034;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#36328;&#39046;&#22495;&#35875;&#35328;&#26816;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35875;&#35328;&#36890;&#24120;&#20276;&#38543;&#30528;&#31361;&#21457;&#26032;&#38395;&#25110;&#28909;&#38376;&#35805;&#39064;&#32780;&#20986;&#29616;&#65292;&#20005;&#37325;&#38459;&#30861;&#30495;&#30456;&#30340;&#26597;&#35777;&#12290;&#29616;&#26377;&#30340;&#35875;&#35328;&#26816;&#27979;&#26041;&#27861;&#22823;&#22810;&#19987;&#27880;&#20110;&#30456;&#21516;&#39046;&#22495;&#65292;&#22240;&#27492;&#22312;&#36328;&#39046;&#22495;&#24773;&#20917;&#19979;&#34920;&#29616;&#19981;&#20339;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#22522;&#20110;&#23454;&#20363;&#21644;&#21407;&#22411;&#30340;&#65292;&#24102;&#26377;&#20132;&#21449;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#23545;&#27604;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#36328;&#39046;&#22495;&#35875;&#35328;&#26816;&#27979;&#12290;&#35813;&#27169;&#22411;&#19981;&#20165;&#21487;&#20197;&#36827;&#34892;&#36328;&#39046;&#22495;&#29305;&#24449;&#23545;&#40784;&#65292;&#36824;&#21487;&#20197;&#24378;&#21046;&#30446;&#26631;&#26679;&#26412;&#19982;&#32473;&#23450;&#28304;&#22495;&#30340;&#30456;&#24212;&#21407;&#22411;&#23545;&#40784;&#12290;&#30001;&#20110;&#30446;&#26631;&#22495;&#20013;&#30340;&#30446;&#26631;&#26631;&#31614;&#19981;&#21487;&#29992;&#65292;&#22240;&#27492;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#32858;&#31867;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#19968;&#25209;&#28304;&#22495;&#26679;&#26412;&#30340;&#20180;&#32454;&#21021;&#22987;&#21270;&#20013;&#24515;&#26469;&#20135;&#29983;&#20266;&#26631;&#31614;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#20132;&#21449;&#27880;&#24847;&#21147;&#26426;&#21046;&#22788;&#29702;&#20855;&#26377;&#30456;&#21516;&#26631;&#31614;&#30340;&#19968;&#23545;&#28304;&#25968;&#25454;&#21644;&#30446;&#26631;&#25968;&#25454;&#65292;&#20197;&#23398;&#20064;&#22495;&#19981;&#21464;&#34920;&#31034;&#12290;&#30001;&#20110;&#39046;&#22495;&#23545;&#20013;&#30340;&#26679;&#26412;&#20542;&#21521;&#20110;&#34920;&#36798;&#30456;&#20284;&#30340;&#35821;&#20041;&#27169;&#24335;&#65292;&#22240;&#27492;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#27169;&#22411;&#30340;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Massive rumors usually appear along with breaking news or trending topics, seriously hindering the truth. Existing rumor detection methods are mostly focused on the same domain, and thus have poor performance in cross-domain scenarios due to domain shift. In this work, we propose an end-to-end instance-wise and prototype-wise contrastive learning model with a cross-attention mechanism for cross-domain rumor detection. The model not only performs cross-domain feature alignment but also enforces target samples to align with the corresponding prototypes of a given source domain. Since target labels in a target domain are unavailable, we use a clustering-based approach with carefully initialized centers by a batch of source domain samples to produce pseudo labels. Moreover, we use a cross-attention mechanism on a pair of source data and target data with the same labels to learn domain-invariant representations. Because the samples in a domain pair tend to express similar semantic patterns,
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24635;&#32467;&#20102;&#19990;&#30028;&#32423;&#35760;&#24518;&#31454;&#25216;&#27604;&#36187;&#30340;&#29616;&#29366;&#21644;&#26041;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#24847;&#35782;&#23398;&#29702;&#35770;&#35270;&#35282;&#12290;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#32452;&#23454;&#39564;&#65292;&#20197;&#24110;&#21161;&#26356;&#22909;&#22320;&#20102;&#35299;&#19987;&#23478;&#35760;&#24518;&#34920;&#29616;&#30340;&#36793;&#30028;&#12290;</title><link>http://arxiv.org/abs/2303.11944</link><description>&lt;p&gt;
&#35760;&#24518;&#31454;&#25216;&#30340;&#34920;&#24449;&#21407;&#21017;
&lt;/p&gt;
&lt;p&gt;
Representational Tenets for Memory Athletics. (arXiv:2303.11944v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11944
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24635;&#32467;&#20102;&#19990;&#30028;&#32423;&#35760;&#24518;&#31454;&#25216;&#27604;&#36187;&#30340;&#29616;&#29366;&#21644;&#26041;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#24847;&#35782;&#23398;&#29702;&#35770;&#35270;&#35282;&#12290;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#32452;&#23454;&#39564;&#65292;&#20197;&#24110;&#21161;&#26356;&#22909;&#22320;&#20102;&#35299;&#19987;&#23478;&#35760;&#24518;&#34920;&#29616;&#30340;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25551;&#36848;&#20102;&#19990;&#30028;&#32423;&#35760;&#24518;&#31454;&#25216;&#27604;&#36187;&#30340;&#24403;&#21069;&#29366;&#24577;&#65292;&#21253;&#25324;&#20026;&#35760;&#24518;&#27604;&#36187;&#20570;&#20934;&#22791;&#21644;&#21442;&#21152;&#27604;&#36187;&#25152;&#20351;&#29992;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#19990;&#30028;&#35760;&#24518;&#22823;&#24072;&#21644;&#32852;&#21512;&#20316;&#32773;&#32435;&#23572;&#36874;&#183;&#24503;&#21033;&#26031;&#30340;&#20027;&#35266;&#25253;&#21578;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#24847;&#35782;&#30340;&#27169;&#25311;&#12289;&#24773;&#22659;&#21644;&#32467;&#26500;&#19968;&#33268;&#24615;&#37327;&#23376;&#21270;&#29702;&#35770;(S3Q)&#30340;&#35270;&#35282;&#25506;&#31350;&#20102;&#36825;&#20123;&#25253;&#36947;&#32463;&#39564;&#65292;&#20197;&#25552;&#20986;&#19968;&#32452;&#23454;&#39564;&#65292;&#20197;&#24110;&#21161;&#36827;&#19968;&#27493;&#20102;&#35299;&#19987;&#23478;&#35760;&#24518;&#34920;&#29616;&#30340;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
We describe the current state of world-class memory competitions, including the methods used to prepare for and compete in memory competitions, based on the subjective report of World Memory Championship Grandmaster and co-author Nelson Dellis. We then explore the reported experiences through the lens of the Simulated, Situated, and Structurally coherent Qualia (S3Q) theory of consciousness, in order to propose a set of experiments to help further understand the boundaries of expert memory performance.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#31232;&#30095;&#20998;&#24067;&#24335;&#20869;&#23384;&#30340;&#20462;&#25913;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#27169;&#22411;&#65292;&#23427;&#26159;&#19968;&#20010;&#24378;&#22823;&#30340;&#25345;&#32493;&#23398;&#20064;&#32773;&#65292;&#24182;&#19988;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#35760;&#24518;&#37325;&#25918;&#25110;&#20219;&#21153;&#20449;&#24687;&#12290;&#36825;&#26159;&#19968;&#31181;&#35757;&#32451;&#31232;&#30095;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.11934</link><description>&lt;p&gt;
&#31232;&#30095;&#20998;&#24067;&#24335;&#20869;&#23384;&#26159;&#19968;&#20010;&#25345;&#32493;&#23398;&#20064;&#32773;
&lt;/p&gt;
&lt;p&gt;
Sparse Distributed Memory is a Continual Learner. (arXiv:2303.11934v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11934
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#31232;&#30095;&#20998;&#24067;&#24335;&#20869;&#23384;&#30340;&#20462;&#25913;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#27169;&#22411;&#65292;&#23427;&#26159;&#19968;&#20010;&#24378;&#22823;&#30340;&#25345;&#32493;&#23398;&#20064;&#32773;&#65292;&#24182;&#19988;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#35760;&#24518;&#37325;&#25918;&#25110;&#20219;&#21153;&#20449;&#24687;&#12290;&#36825;&#26159;&#19968;&#31181;&#35757;&#32451;&#31232;&#30095;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#26159;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#38754;&#20020;&#30340;&#38382;&#39064;&#65292;&#32780;&#23427;&#20204;&#30340;&#29983;&#29289;&#23398;&#23545;&#24212;&#29289;&#25797;&#38271;&#35299;&#20915;&#12290;&#22312;&#21033;&#29992;&#31232;&#30095;&#20998;&#24067;&#24335;&#20869;&#23384;&#65288;SDM&#65289;&#23558;&#26680;&#24515;&#31070;&#32463;&#30005;&#36335;&#19982;&#24378;&#22823;&#30340;Transformer&#27169;&#22411;&#30456;&#36830;&#25509;&#30340;&#30740;&#31350;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#25913;&#36827;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#65292;&#23427;&#26159;&#19968;&#20010;&#24378;&#22823;&#30340;&#25345;&#32493;&#23398;&#20064;&#32773;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20174;&#29983;&#29289;&#23398;&#19978;&#32763;&#35793;&#36807;&#26469;&#30340;&#25105;&#20204;&#30340;MLP&#21464;&#20307;&#30340;&#27599;&#20010;&#32452;&#25104;&#37096;&#20998;&#37117;&#26159;&#25345;&#32493;&#23398;&#20064;&#25152;&#24517;&#38656;&#30340;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#20063;&#19981;&#38656;&#35201;&#20219;&#20309;&#35760;&#24518;&#37325;&#25918;&#25110;&#20219;&#21153;&#20449;&#24687;&#65292;&#24182;&#24341;&#20837;&#20102;&#35757;&#32451;&#31232;&#30095;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#65292;&#36825;&#21487;&#33021;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual learning is a problem for artificial neural networks that their biological counterparts are adept at solving. Building on work using Sparse Distributed Memory (SDM) to connect a core neural circuit with the powerful Transformer model, we create a modified Multi-Layered Perceptron (MLP) that is a strong continual learner. We find that every component of our MLP variant translated from biology is necessary for continual learning. Our solution is also free from any memory replay or task information, and introduces novel methods to train sparse networks that may be broadly applicable.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#20013;&#38388;&#27010;&#24565;&#30340;&#32423;&#21035;&#32467;&#26500;&#65292;&#21033;&#29992;&#39046;&#22495;&#19987;&#23478;&#24314;&#31435;&#37096;&#20998;-&#25972;&#20307;&#20851;&#31995;&#65292;&#20174;&#32780;&#22312;&#19981;&#21516;&#25277;&#35937;&#32423;&#21035;&#19978;&#24110;&#21161;&#35299;&#37322;&#40657;&#30418;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.11920</link><description>&lt;p&gt;
&#20013;&#38388;&#29305;&#24449;&#32852;&#30431;&#33021;&#24110;&#21161;&#35299;&#37322;&#40657;&#30418;&#27169;&#22411;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do intermediate feature coalitions aid explainability of black-box models?. (arXiv:2303.11920v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11920
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#20013;&#38388;&#27010;&#24565;&#30340;&#32423;&#21035;&#32467;&#26500;&#65292;&#21033;&#29992;&#39046;&#22495;&#19987;&#23478;&#24314;&#31435;&#37096;&#20998;-&#25972;&#20307;&#20851;&#31995;&#65292;&#20174;&#32780;&#22312;&#19981;&#21516;&#25277;&#35937;&#32423;&#21035;&#19978;&#24110;&#21161;&#35299;&#37322;&#40657;&#30418;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#22522;&#20110;&#32423;&#21035;&#32467;&#26500;&#30340;&#20013;&#38388;&#27010;&#24565;&#65292;&#20197;&#24110;&#21161;&#40657;&#30418;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#32423;&#21035;&#32467;&#26500;&#26159;&#19968;&#31181;&#20998;&#23618;&#32467;&#26500;&#65292;&#27599;&#20010;&#32423;&#21035;&#23545;&#24212;&#25968;&#25454;&#38598;&#30340;&#29305;&#24449;&#65288;&#21363;&#29609;&#23478;&#38598;&#20998;&#21306;&#65289;&#12290;&#20174;&#21482;&#21253;&#21547;&#21333;&#20803;&#32032;&#30340;&#24179;&#20961;&#38598;&#21512;&#21040;&#21482;&#21253;&#21547;&#22823;&#32852;&#30431;&#30340;&#38598;&#21512;&#65292;&#31895;&#31961;&#24230;&#30340;&#32423;&#21035;&#36880;&#28176;&#22686;&#21152;&#12290;&#27492;&#22806;&#65292;&#21487;&#20197;&#36890;&#36807;&#39046;&#22495;&#19987;&#23478;&#24314;&#31435;&#37096;&#20998;-&#25972;&#20307;&#20851;&#31995;&#26469;&#29983;&#25104;&#25277;&#35937;&#32423;&#21035;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#23454;&#38469;&#30340;&#27773;&#36710;&#27169;&#22411;&#31034;&#20363;&#21644;&#27888;&#22374;&#23612;&#20811;&#21495;&#30340;&#25968;&#25454;&#38598;&#20013;&#35828;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#21487;&#29992;&#24615;&#65292;&#20854;&#20013;&#20013;&#38388;&#27010;&#24565;&#22312;&#19981;&#21516;&#25277;&#35937;&#32423;&#21035;&#19978;&#24110;&#21161;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work introduces the notion of intermediate concepts based on levels structure to aid explainability for black-box models. The levels structure is a hierarchical structure in which each level corresponds to features of a dataset (i.e., a player-set partition). The level of coarseness increases from the trivial set, which only comprises singletons, to the set, which only contains the grand coalition. In addition, it is possible to establish meronomies, i.e., part-whole relationships, via a domain expert that can be utilised to generate explanations at an abstract level. We illustrate the usability of this approach in a real-world car model example and the Titanic dataset, where intermediate concepts aid in explainability at different levels of abstraction.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Deephys&#30340;&#21487;&#35270;&#21270;&#21644;&#29702;&#35299;DNN&#22312;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#22330;&#26223;&#20013;&#22833;&#36133;&#30340;&#24037;&#20855;&#65292;&#20351;&#29992;&#31070;&#32463;&#30005;&#29983;&#29702;&#23398;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#27604;&#36739;&#20869;&#20998;&#24067;&#21644;&#22806;&#20998;&#24067;&#25968;&#25454;&#38598;&#20013;&#30340;&#31070;&#32463;&#27963;&#21160;&#65292;&#26080;&#32541;&#20998;&#26512;&#21333;&#20010;&#31070;&#32463;&#20803;&#12289;&#21333;&#20010;&#22270;&#20687;&#21644;&#31867;&#21035;&#22270;&#20687;&#38598;&#65292;&#24182;&#33021;&#25581;&#31034;&#20551;&#29305;&#24449;&#21644;&#26032;&#29305;&#24449;&#23384;&#22312;&#23548;&#33268;&#30340;&#22833;&#36133;&#12290;</title><link>http://arxiv.org/abs/2303.11912</link><description>&lt;p&gt;
Deephys&#65306;&#20998;&#24067;&#28418;&#31227;&#19979;&#31070;&#32463;&#32593;&#32476;&#30340;&#35843;&#35797;&#19982;&#21487;&#35270;&#21270;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Deephys: Deep Electrophysiology, Debugging Neural Networks under Distribution Shifts. (arXiv:2303.11912v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11912
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Deephys&#30340;&#21487;&#35270;&#21270;&#21644;&#29702;&#35299;DNN&#22312;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#22330;&#26223;&#20013;&#22833;&#36133;&#30340;&#24037;&#20855;&#65292;&#20351;&#29992;&#31070;&#32463;&#30005;&#29983;&#29702;&#23398;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#27604;&#36739;&#20869;&#20998;&#24067;&#21644;&#22806;&#20998;&#24067;&#25968;&#25454;&#38598;&#20013;&#30340;&#31070;&#32463;&#27963;&#21160;&#65292;&#26080;&#32541;&#20998;&#26512;&#21333;&#20010;&#31070;&#32463;&#20803;&#12289;&#21333;&#20010;&#22270;&#20687;&#21644;&#31867;&#21035;&#22270;&#20687;&#38598;&#65292;&#24182;&#33021;&#25581;&#31034;&#20551;&#29305;&#24449;&#21644;&#26032;&#29305;&#24449;&#23384;&#22312;&#23548;&#33268;&#30340;&#22833;&#36133;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#22312;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#22330;&#26223;&#19979;&#32463;&#24120;&#20250;&#20986;&#29616;&#22833;&#36133;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24037;&#20855;&#26469;&#21487;&#35270;&#21270;&#21644;&#29702;&#35299;&#36825;&#31181;&#22833;&#36133;&#12290;&#25105;&#20204;&#20174;&#31070;&#32463;&#30005;&#29983;&#29702;&#23398;&#30340;&#27010;&#24565;&#20013;&#27762;&#21462;&#28789;&#24863;&#65292;&#36890;&#36807;&#20998;&#26512;&#21333;&#20010;&#31070;&#32463;&#20803;&#30340;&#29305;&#24449;&#35843;&#35856;&#21644;&#19981;&#21464;&#24615;&#65292;&#26469;&#26816;&#26597;&#31070;&#32463;&#32593;&#32476;&#30340;&#20869;&#37096;&#21151;&#33021;&#12290;Deep Electrophysiology&#65292;&#31616;&#31216;Deephys&#65292;&#36890;&#36807;&#27604;&#36739;&#21487;&#35270;&#21270;&#20869;&#20998;&#24067;&#21644;&#22806;&#20998;&#24067;&#25968;&#25454;&#38598;&#20013;&#30340;&#31070;&#32463;&#27963;&#21160;&#65292;&#25552;&#20379;&#20102;&#26377;&#20851;DNN&#22312;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#22330;&#26223;&#20013;&#22833;&#36133;&#30340;&#35265;&#35299;&#12290;Deephys&#25552;&#20379;&#20102;&#23545;&#21333;&#20010;&#31070;&#32463;&#20803;&#65292;&#21333;&#20010;&#22270;&#20687;&#20197;&#21450;&#31867;&#21035;&#22270;&#20687;&#38598;&#30340;&#26080;&#32541;&#20998;&#26512;&#65292;&#24182;&#19988;&#33021;&#22815;&#25581;&#31034;&#30001;&#20110;&#20551;&#29305;&#24449;&#21644;&#26032;&#29305;&#24449;&#30340;&#23384;&#22312;&#32780;&#23548;&#33268;&#30340;&#22833;&#36133;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#20960;&#20010;&#25968;&#25454;&#38598;&#21644;&#20998;&#24067;&#28418;&#31227;&#20013;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#21464;&#25442;&#22120;&#26550;&#26500;&#36827;&#34892;&#25968;&#37327;&#20998;&#26512;&#65292;&#35777;&#23454;&#20102;Deephys&#30340;&#23450;&#24615;&#21487;&#35270;&#21270;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks (DNNs) often fail in out-of-distribution scenarios. In this paper, we introduce a tool to visualize and understand such failures. We draw inspiration from concepts from neural electrophysiology, which are based on inspecting the internal functioning of a neural networks by analyzing the feature tuning and invariances of individual units. Deep Electrophysiology, in short Deephys, provides insights of the DNN's failures in out-of-distribution scenarios by comparative visualization of the neural activity in in-distribution and out-of-distribution datasets. Deephys provides seamless analyses of individual neurons, individual images, and a set of set of images from a category, and it is capable of revealing failures due to the presence of spurious features and novel features. We substantiate the validity of the qualitative visualizations of Deephys thorough quantitative analyses using convolutional and transformers architectures, in several datasets and distribution shi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26694;&#26550; RegionLight&#65292;&#22522;&#20110;&#20132;&#21449;&#21475;&#20043;&#38388;&#30340;&#37051;&#25509;&#20851;&#31995;&#23558;&#26234;&#33021;&#20307;&#20998;&#37197;&#21040;&#27599;&#20010;&#21306;&#22495;&#20013;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#20154;&#21592;&#25193;&#23637;&#20102;BDQ&#26041;&#27861;&#20026;DBDQ&#65292;&#20197;&#38480;&#21046;&#32852;&#21512;&#21160;&#20316;&#31354;&#38388;&#22823;&#23567;&#30340;&#22686;&#38271;&#24182;&#32531;&#35299;&#26234;&#33021;&#20307;&#35757;&#32451;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.11899</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#22823;&#35268;&#27169;&#26684;&#32593;&#20132;&#36890;&#32593;&#32476;&#21306;&#22495;&#20449;&#21495;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Multi-agent Reinforcement Learning for Regional Signal control in Large-scale Grid Traffic network. (arXiv:2303.11899v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11899
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26694;&#26550; RegionLight&#65292;&#22522;&#20110;&#20132;&#21449;&#21475;&#20043;&#38388;&#30340;&#37051;&#25509;&#20851;&#31995;&#23558;&#26234;&#33021;&#20307;&#20998;&#37197;&#21040;&#27599;&#20010;&#21306;&#22495;&#20013;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#20154;&#21592;&#25193;&#23637;&#20102;BDQ&#26041;&#27861;&#20026;DBDQ&#65292;&#20197;&#38480;&#21046;&#32852;&#21512;&#21160;&#20316;&#31354;&#38388;&#22823;&#23567;&#30340;&#22686;&#38271;&#24182;&#32531;&#35299;&#26234;&#33021;&#20307;&#35757;&#32451;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#30340;&#33258;&#36866;&#24212;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#26159;&#24403;&#21069;&#38750;&#24120;&#27969;&#34892;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20013;&#65292;&#19968;&#20010;&#26234;&#33021;&#20307;&#25511;&#21046;&#21333;&#20010;&#36335;&#21475;&#65292;&#36825;&#20123;&#26041;&#27861;&#20391;&#37325;&#20110;&#36335;&#21475;&#20043;&#38388;&#30340;&#21327;&#20316;&#12290;&#28982;&#32780;&#65292;MARL&#30340;&#38750;&#31283;&#24577;&#24615;&#36136;&#38543;&#30528;&#20132;&#36890;&#32593;&#32476;&#35268;&#27169;&#30340;&#22686;&#38271;&#65292;&#20173;&#28982;&#38480;&#21046;&#30528;&#19978;&#36848;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#19968;&#31181;&#22949;&#21327;&#30340;&#31574;&#30053;&#26159;&#23558;&#19968;&#21517;&#26234;&#33021;&#20307;&#20998;&#37197;&#21040;&#19968;&#32452;&#36335;&#21475;&#20013;&#65292;&#20197;&#20943;&#23569;&#26234;&#33021;&#20307;&#25968;&#37327;&#12290;&#36825;&#31181;&#31574;&#30053;&#23384;&#22312;&#20004;&#20010;&#25361;&#25112;&#65292;&#19968;&#20010;&#26159;&#22914;&#20309;&#23558;&#20132;&#36890;&#32593;&#32476;&#21010;&#20998;&#25104;&#23567;&#21306;&#22495;&#65292;&#21478;&#19968;&#20010;&#26159;&#22914;&#20309;&#25628;&#32034;&#21306;&#22495;&#20869;&#30340;&#26368;&#20248;&#32852;&#21512;&#21160;&#20316;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26694;&#26550;RegionLight&#65292;&#20854;&#20013;&#25105;&#20204;&#30340;&#21306;&#22495;&#21010;&#20998;&#35268;&#21017;&#22522;&#20110;&#20132;&#21449;&#21475;&#20043;&#38388;&#30340;&#37051;&#25509;&#20851;&#31995;&#65292;&#24182;&#25193;&#23637;&#20102;Branching Dueling Q-Network(BDQ)&#12290;&#35813;&#26041;&#27861;&#23558;BDQ&#36827;&#19968;&#27493;&#20248;&#21270;&#20026;Dynamic Branching Dueling Q-Network(DBDQ)&#65292;&#20197;&#38480;&#21046;&#32852;&#21512;&#21160;&#20316;&#31354;&#38388;&#22823;&#23567;&#30340;&#22686;&#38271;&#24182;&#32531;&#35299;&#26234;&#33021;&#20307;&#35757;&#32451;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adaptive traffic signal control with Multi-agent Reinforcement Learning(MARL) is a very popular topic nowadays. In most existing novel methods, one agent controls single intersections and these methods focus on the cooperation between intersections. However, the non-stationary property of MARL still limits the performance of the above methods as the size of traffic networks grows. One compromised strategy is to assign one agent with a region of intersections to reduce the number of agents. There are two challenges in this strategy, one is how to partition a traffic network into small regions and the other is how to search for the optimal joint actions for a region of intersections. In this paper, we propose a novel training framework RegionLight where our region partition rule is based on the adjacency between the intersection and extended Branching Dueling Q-Network(BDQ) to Dynamic Branching Dueling Q-Network(DBDQ) to bound the growth of the size of joint action space and alleviate th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#24809;&#32602;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#21644;&#29305;&#24449;&#32423;&#22810;&#20256;&#24863;&#22120;&#34701;&#21512;&#25216;&#26415;&#65292;&#24212;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#23548;&#33322;&#20013;&#12290;&#25991;&#20013;&#37325;&#28857;&#20171;&#32461;&#20102;&#38024;&#23545;&#28608;&#20809;&#38647;&#36798;&#21644;RGB&#20449;&#24687;&#30340;&#34701;&#21512;&#25216;&#26415;&#65292;&#26088;&#22312;&#25552;&#39640;&#27169;&#22411;&#23545;&#20132;&#36890;&#35268;&#21017;&#30340;&#36981;&#23432;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.11888</link><description>&lt;p&gt;
&#22522;&#20110;&#24809;&#32602;&#30340;&#27169;&#20223;&#23398;&#20064;&#21644;&#36328;&#35821;&#20041;&#29983;&#25104;&#20256;&#24863;&#22120;&#34701;&#21512;&#25216;&#26415;&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Penalty-Based Imitation Learning With Cross Semantics Generation Sensor Fusion for Autonomous Driving. (arXiv:2303.11888v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11888
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#24809;&#32602;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#21644;&#29305;&#24449;&#32423;&#22810;&#20256;&#24863;&#22120;&#34701;&#21512;&#25216;&#26415;&#65292;&#24212;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#23548;&#33322;&#20013;&#12290;&#25991;&#20013;&#37325;&#28857;&#20171;&#32461;&#20102;&#38024;&#23545;&#28608;&#20809;&#38647;&#36798;&#21644;RGB&#20449;&#24687;&#30340;&#34701;&#21512;&#25216;&#26415;&#65292;&#26088;&#22312;&#25552;&#39640;&#27169;&#22411;&#23545;&#20132;&#36890;&#35268;&#21017;&#30340;&#36981;&#23432;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#27169;&#24335;&#35782;&#21035;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#30446;&#26631;&#26816;&#27979;&#12289;&#35821;&#20041;&#20998;&#21106;&#31561;&#20219;&#21153;&#30340;&#20934;&#30830;&#24230;&#24050;&#32463;&#36229;&#36807;&#20154;&#31867;&#12290;&#33258;&#21160;&#39550;&#39542;&#20316;&#20026;&#19968;&#39033;&#37325;&#35201;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#26088;&#22312;&#24443;&#24213;&#25913;&#21464;&#26410;&#26469;&#30340;&#20132;&#36890;&#21644;&#20986;&#34892;&#26041;&#24335;&#12290;&#20256;&#24863;&#22120;&#23545;&#20110;&#33258;&#21160;&#39550;&#39542;&#30340;&#23433;&#20840;&#24615;&#21644;&#29615;&#22659;&#24863;&#30693;&#30340;&#21487;&#34892;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#22810;&#20256;&#24863;&#22120;&#34701;&#21512;&#30001;&#20110;&#20854;&#22810;&#32500;&#24863;&#30693;&#21644;&#38598;&#25104;&#33021;&#21147;&#30340;&#28508;&#21147;&#32780;&#25104;&#20026;&#24403;&#21069;&#30740;&#31350;&#30340;&#28909;&#28857;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29305;&#24449;&#32423;&#22810;&#20256;&#24863;&#22120;&#34701;&#21512;&#25216;&#26415;&#65292;&#29992;&#20110;&#31471;&#21040;&#31471;&#30340;&#33258;&#21160;&#39550;&#39542;&#23548;&#33322;&#21644;&#27169;&#20223;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#35770;&#25991;&#20027;&#35201;&#20851;&#27880;&#20110;&#28608;&#20809;&#38647;&#36798;&#21644;RGB&#20449;&#24687;&#30340;&#34701;&#21512;&#25216;&#26415;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#22522;&#20110;&#24809;&#32602;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#21152;&#24378;&#27169;&#22411;&#36981;&#23432;&#20132;&#36890;&#35268;&#21017;&#30340;&#33021;&#21147;&#24182;&#32479;&#19968;&#27169;&#20223;&#23398;&#20064;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rapid development of Pattern Recognition and Computer Vision technologies, tasks like object detection or semantic segmentation have achieved even better accuracy than human beings. Based on these solid foundations, autonomous driving is becoming an important research direction, aiming to revolute the future of transportation and mobility. Sensors are critical to autonomous driving's security and feasibility to perceive the surrounding environment. Multi-Sensor fusion has become a current research hot spot because of its potential for multidimensional perception and integration ability. In this paper, we propose a novel feature-level multi-sensor fusion technology for end-to-end autonomous driving navigation with imitation learning. Our paper mainly focuses on fusion technologies for Lidar and RGB information. We also provide a brand-new penalty-based imitation learning method to reinforce the model's compliance with traffic rules and unify the objective of imitation learning 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#26032;&#30340;&#35780;&#20272;&#26041;&#26696;&#65292;&#36890;&#36807;&#36825;&#20123;&#26041;&#26696;&#65292;&#21487;&#20197;&#26356;&#21487;&#38752;&#22320;&#27979;&#37327;&#24402;&#22240;&#26041;&#27861;&#30340;&#21487;&#20449;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.11884</link><description>&lt;p&gt;
&#36890;&#36807;&#31995;&#32479;&#35780;&#20272;&#26356;&#22909;&#22320;&#29702;&#35299;&#24402;&#22240;&#26041;&#27861;&#30340;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
Better Understanding Differences in Attribution Methods via Systematic Evaluations. (arXiv:2303.11884v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11884
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#26032;&#30340;&#35780;&#20272;&#26041;&#26696;&#65292;&#36890;&#36807;&#36825;&#20123;&#26041;&#26696;&#65292;&#21487;&#20197;&#26356;&#21487;&#38752;&#22320;&#27979;&#37327;&#24402;&#22240;&#26041;&#27861;&#30340;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#35768;&#22810;&#35270;&#35273;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#20854;&#40657;&#30418;&#24615;&#36136;&#20351;&#20854;&#38590;&#20197;&#35299;&#37322;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#21508;&#31181;&#21518;&#32493;&#24402;&#22240;&#26041;&#27861;&#26469;&#30830;&#23450;&#23545;&#27169;&#22411;&#20915;&#31574;&#26368;&#26377;&#24433;&#21709;&#21147;&#30340;&#22270;&#20687;&#21306;&#22495;&#12290;&#30001;&#20110;&#19981;&#23384;&#22312;&#22522;&#20934;&#24402;&#22240;&#65292;&#22240;&#27492;&#35780;&#20272;&#36825;&#20123;&#26041;&#27861;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#26032;&#30340;&#35780;&#20272;&#26041;&#26696;&#65292;&#20197;&#26356;&#21487;&#38752;&#22320;&#27979;&#37327;&#36825;&#20123;&#26041;&#27861;&#30340;&#21487;&#20449;&#24230;&#65292;&#20351;&#23427;&#20204;&#20043;&#38388;&#30340;&#27604;&#36739;&#26356;&#20844;&#24179;&#65292;&#24182;&#20351;&#35270;&#35273;&#26816;&#26597;&#26356;&#31995;&#32479;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks are very successful on many vision tasks, but hard to interpret due to their black box nature. To overcome this, various post-hoc attribution methods have been proposed to identify image regions most influential to the models' decisions. Evaluating such methods is challenging since no ground truth attributions exist. We thus propose three novel evaluation schemes to more reliably measure the faithfulness of those methods, to make comparisons between them more fair, and to make visual inspection more systematic. To address faithfulness, we propose a novel evaluation setting (DiFull) in which we carefully control which parts of the input can influence the output in order to distinguish possible from impossible attributions. To address fairness, we note that different methods are applied at different layers, which skews any comparison, and so evaluate all methods on the same layers (ML-Att) and discuss how this impacts their performance on quantitative metrics. For mo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#37319;&#29992;&#20102;&#28369;&#21160;&#31383;&#21475;&#27880;&#24847;&#26426;&#21046;&#26469;&#26367;&#20195;Transformer&#20013;&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#20174;&#32780;&#20351;&#24471;&#20351;&#29992;&#36880;&#20010;&#20803;&#32032;&#22320;&#22788;&#29702;&#24207;&#21015;&#65292;&#26356;&#21152;&#36866;&#29992;&#20110;&#22312;&#32447;&#20449;&#21495;&#22788;&#29702;&#65292;&#24182;&#19988;&#22312;&#25163;&#25351;&#20301;&#32622;&#22238;&#24402;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#22909;&#30340;&#20934;&#30830;&#24615;&#32426;&#24405;&#65292;&#27599;&#20010;&#25512;&#29702;&#27493;&#39588;&#20165;&#38656;&#35201;&#38750;&#24120;&#30701;&#30340;&#26102;&#38388;&#31383;&#21475;(3.5&#27627;&#31186;)&#12290;</title><link>http://arxiv.org/abs/2303.11860</link><description>&lt;p&gt;
&#20351;&#29992;&#33033;&#20914;&#31070;&#32463;&#20803;&#30340;&#22312;&#32447;Transformer&#29992;&#20110;&#24555;&#36895;&#20551;&#32930;&#25163;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Online Transformers with Spiking Neurons for Fast Prosthetic Hand Control. (arXiv:2303.11860v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11860
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#37319;&#29992;&#20102;&#28369;&#21160;&#31383;&#21475;&#27880;&#24847;&#26426;&#21046;&#26469;&#26367;&#20195;Transformer&#20013;&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#20174;&#32780;&#20351;&#24471;&#20351;&#29992;&#36880;&#20010;&#20803;&#32032;&#22320;&#22788;&#29702;&#24207;&#21015;&#65292;&#26356;&#21152;&#36866;&#29992;&#20110;&#22312;&#32447;&#20449;&#21495;&#22788;&#29702;&#65292;&#24182;&#19988;&#22312;&#25163;&#25351;&#20301;&#32622;&#22238;&#24402;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#22909;&#30340;&#20934;&#30830;&#24615;&#32426;&#24405;&#65292;&#27599;&#20010;&#25512;&#29702;&#27493;&#39588;&#20165;&#38656;&#35201;&#38750;&#24120;&#30701;&#30340;&#26102;&#38388;&#31383;&#21475;(3.5&#27627;&#31186;)&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#32593;&#32476;&#26159;&#22823;&#22810;&#25968;&#24207;&#21015;&#22788;&#29702;&#20219;&#21153;&#30340;&#26368;&#20808;&#36827;&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;Transformer&#20013;&#32463;&#24120;&#20351;&#29992;&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#38656;&#35201;&#22823;&#30340;&#26102;&#38388;&#31383;&#21475;&#26469;&#36827;&#34892;&#27599;&#20010;&#35745;&#31639;&#27493;&#39588;&#65292;&#22240;&#27492;&#19982;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;(RNN)&#30456;&#27604;&#65292;&#20351;&#24471;&#23427;&#20204;&#19981;&#22826;&#36866;&#29992;&#20110;&#22312;&#32447;&#20449;&#21495;&#22788;&#29702;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#28369;&#21160;&#31383;&#21475;&#27880;&#24847;&#26426;&#21046;&#26469;&#20195;&#26367;&#33258;&#27880;&#24847;&#26426;&#21046;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26426;&#21046;&#23545;&#20110;&#22312;&#36755;&#20837;&#21644;&#30446;&#26631;&#20043;&#38388;&#23384;&#22312;&#26377;&#38480;&#33539;&#22260;&#20381;&#36182;&#30340;&#36830;&#32493;&#20449;&#21495;&#26356;&#20026;&#39640;&#25928;&#65292;&#24182;&#19988;&#21487;&#20197;&#29992;&#23427;&#26469;&#36880;&#20010;&#20803;&#32032;&#22320;&#22788;&#29702;&#24207;&#21015;&#65292;&#22240;&#27492;&#20351;&#20854;&#36866;&#29992;&#20110;&#22312;&#32447;&#22788;&#29702;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#25351;&#23574;&#20301;&#32622;&#22238;&#24402;&#25968;&#25454;&#38598;(NinaproDB8)&#19978;&#27979;&#35797;&#20102;&#27169;&#22411;&#65292;&#35813;&#25968;&#25454;&#38598;&#20351;&#29992;&#22312;&#21069;&#33218;&#30382;&#32932;&#19978;&#27979;&#37327;&#30340;Surface Electromyographic (sEMG)&#20449;&#21495;&#26469;&#20272;&#35745;&#32908;&#32905;&#27963;&#21160;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27599;&#20010;&#25512;&#29702;&#27493;&#39588;&#20013;&#20165;&#38656;&#35201;&#38750;&#24120;&#30701;&#30340;&#26102;&#38388;&#31383;&#21475;(3.5&#27627;&#31186;)&#23601;&#33021;&#22312;&#36825;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#26368;&#26032;&#30340;&#20934;&#30830;&#24615;&#32426;&#24405;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers are state-of-the-art networks for most sequence processing tasks. However, the self-attention mechanism often used in Transformers requires large time windows for each computation step and thus makes them less suitable for online signal processing compared to Recurrent Neural Networks (RNNs). In this paper, instead of the self-attention mechanism, we use a sliding window attention mechanism. We show that this mechanism is more efficient for continuous signals with finite-range dependencies between input and target, and that we can use it to process sequences element-by-element, this making it compatible with online processing. We test our model on a finger position regression dataset (NinaproDB8) with Surface Electromyographic (sEMG) signals measured on the forearm skin to estimate muscle activities. Our approach sets the new state-of-the-art in terms of accuracy on this dataset while requiring only very short time windows of 3.5 ms at each inference step. Moreover, we inc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26597;&#35810;&#23884;&#20837;&#26041;&#27861;RoConE&#65292;&#23427;&#20801;&#35768;&#23398;&#20064;&#20851;&#31995;&#27169;&#24335;&#24182;&#25552;&#39640;&#20102;&#36923;&#36753;&#26597;&#35810;&#25512;&#29702;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.11858</link><description>&lt;p&gt;
&#23545;&#30693;&#35782;&#22270;&#35889;&#36827;&#34892;&#36923;&#36753;&#26597;&#35810;&#24212;&#31572;&#30340;&#20851;&#31995;&#27169;&#24335;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Modeling Relational Patterns for Logical Query Answering over Knowledge Graphs. (arXiv:2303.11858v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11858
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26597;&#35810;&#23884;&#20837;&#26041;&#27861;RoConE&#65292;&#23427;&#20801;&#35768;&#23398;&#20064;&#20851;&#31995;&#27169;&#24335;&#24182;&#25552;&#39640;&#20102;&#36923;&#36753;&#26597;&#35810;&#25512;&#29702;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#36827;&#34892;&#19968;&#38454;&#36923;&#36753;&#65288;FOL&#65289;&#26597;&#35810;&#30340;&#22238;&#31572;&#20173;&#28982;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;KG&#19981;&#23436;&#25972;&#24615;&#32780;&#23548;&#33268;&#30340;&#12290;&#26597;&#35810;&#23884;&#20837;&#26041;&#27861;&#36890;&#36807;&#35745;&#31639;&#23454;&#20307;&#12289;&#20851;&#31995;&#21644;&#36923;&#36753;&#26597;&#35810;&#30340;&#20302;&#32500;&#24230;&#21521;&#37327;&#34920;&#31034;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;KG&#34920;&#29616;&#20986;&#23545;&#31216;&#24615;&#21644;&#32452;&#21512;&#24615;&#31561;&#20851;&#31995;&#27169;&#24335;&#65292;&#24314;&#27169;&#36825;&#20123;&#27169;&#24335;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#26597;&#35810;&#23884;&#20837;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#24335;&#22312;&#26597;&#35810;&#23884;&#20837;&#27169;&#22411;&#20013;&#30340;&#20316;&#29992;&#23578;&#26410;&#22312;&#25991;&#29486;&#20013;&#36827;&#34892;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22635;&#34917;&#20102;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#20801;&#35768;&#23398;&#20064;&#20851;&#31995;&#27169;&#24335;&#30340;&#24402;&#32435;&#20559;&#24046;&#65292;&#21152;&#24378;FOL&#26597;&#35810;&#25512;&#29702;&#30340;&#27169;&#24335;&#25512;&#29702;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#26597;&#35810;&#23884;&#20837;&#26041;&#27861;RoConE&#65292;&#23427;&#23558;&#26597;&#35810;&#21306;&#22495;&#23450;&#20041;&#20026;&#20960;&#20309;&#38181;&#20307;&#65292;&#24182;&#36890;&#36807;&#22312;&#22797;&#26434;&#31354;&#38388;&#20013;&#26059;&#36716;&#20195;&#25968;&#26597;&#35810;&#31639;&#23376;&#12290;RoConE&#32467;&#21512;&#20102;&#20960;&#20309;&#38181;&#20307;&#20316;&#20026;&#21487;&#20197;&#26126;&#30830;&#23450;&#20041;&#26597;&#35810;&#34920;&#31034;&#30340;&#20960;&#20309;&#34920;&#31034;&#21644;&#26059;&#36716;&#20195;&#25968;&#26597;&#35810;&#31639;&#23376;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Answering first-order logical (FOL) queries over knowledge graphs (KG) remains a challenging task mainly due to KG incompleteness. Query embedding approaches this problem by computing the low-dimensional vector representations of entities, relations, and logical queries. KGs exhibit relational patterns such as symmetry and composition and modeling the patterns can further enhance the performance of query embedding models. However, the role of such patterns in answering FOL queries by query embedding models has not been yet studied in the literature. In this paper, we fill in this research gap and empower FOL queries reasoning with pattern inference by introducing an inductive bias that allows for learning relation patterns. To this end, we develop a novel query embedding method, RoConE, that defines query regions as geometric cones and algebraic query operators by rotations in complex space. RoConE combines the advantages of Cone as a well-specified geometric representation for query e
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38271;&#30701;&#26399;&#24490;&#29615;&#21367;&#31215;&#32593;&#32476;&#30340;&#28608;&#20809;&#38647;&#36798;&#37324;&#31243;&#35745;&#20272;&#35745;&#26041;&#27861;LoRCoN-LO&#12290;&#36890;&#36807;&#20351;&#29992;CNN&#21644;LSTM&#23618;&#22788;&#29702;&#31354;&#38388;&#21644;&#26102;&#38388;&#20449;&#24687;&#65292;&#21033;&#29992;&#28857;&#20113;&#23454;&#29616;&#23545;&#26426;&#22120;&#20154;&#36830;&#32493;&#36816;&#21160;&#30340;&#39044;&#27979;&#12290;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;(KITTI)&#19978;&#34920;&#29616;&#20986;&#20102;&#31934;&#30830;&#30340;&#37324;&#31243;&#35745;&#39044;&#27979;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.11853</link><description>&lt;p&gt;
LoRCoN-LO:&#22522;&#20110;&#38271;&#30701;&#26399;&#24490;&#29615;&#21367;&#31215;&#32593;&#32476;&#30340;&#28608;&#20809;&#38647;&#36798;&#27979;&#36317;&#20202;&#37324;&#31243;&#35745;&#20272;&#35745;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
LoRCoN-LO: Long-term Recurrent Convolutional Network-based LiDAR Odometry. (arXiv:2303.11853v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11853
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38271;&#30701;&#26399;&#24490;&#29615;&#21367;&#31215;&#32593;&#32476;&#30340;&#28608;&#20809;&#38647;&#36798;&#37324;&#31243;&#35745;&#20272;&#35745;&#26041;&#27861;LoRCoN-LO&#12290;&#36890;&#36807;&#20351;&#29992;CNN&#21644;LSTM&#23618;&#22788;&#29702;&#31354;&#38388;&#21644;&#26102;&#38388;&#20449;&#24687;&#65292;&#21033;&#29992;&#28857;&#20113;&#23454;&#29616;&#23545;&#26426;&#22120;&#20154;&#36830;&#32493;&#36816;&#21160;&#30340;&#39044;&#27979;&#12290;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;(KITTI)&#19978;&#34920;&#29616;&#20986;&#20102;&#31934;&#30830;&#30340;&#37324;&#31243;&#35745;&#39044;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#28608;&#20809;&#38647;&#36798;&#37324;&#31243;&#35745;&#20272;&#35745;&#26041;&#27861;&#65292;&#31216;&#20026;LoRCoN-LO&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#38271;&#26399;&#24490;&#29615;&#21367;&#31215;&#32593;&#32476;&#65288;LRCN&#65289;&#32467;&#26500;&#12290; LRCN&#23618;&#26159;&#19968;&#31181;&#32467;&#26500;&#65292;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;CNN&#21644;LSTM&#23618;&#21516;&#26102;&#22788;&#29702;&#31354;&#38388;&#21644;&#26102;&#38388;&#20449;&#24687;&#12290;&#30001;&#20110;&#23427;&#20351;&#29992;&#21253;&#21547;&#31354;&#38388;&#20449;&#24687;&#30340;&#28857;&#20113;&#36827;&#34892;&#36830;&#32493;&#26426;&#22120;&#20154;&#36816;&#21160;&#30340;&#39044;&#27979;&#65292;&#22240;&#27492;&#36825;&#20010;&#29305;&#24449;&#38750;&#24120;&#36866;&#21512;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;LRCN&#23618;&#26500;&#24314;&#20102;&#19968;&#20010;LoRCoN-LO&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#35813;&#27169;&#22411;&#39044;&#27979;&#20102;&#26426;&#22120;&#20154;&#30340;&#23039;&#24577;&#12290;&#20026;&#20102;&#39564;&#35777;&#24615;&#33021;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#20844;&#20849;&#25968;&#25454;&#38598;&#65288;KITTI&#65289;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LoRCoN-LO&#22312;&#25968;&#25454;&#38598;&#20013;&#26174;&#31034;&#20102;&#31934;&#30830;&#30340;&#37324;&#31243;&#35745;&#39044;&#27979;&#12290; &#35813;&#20195;&#30721;&#21487;&#22312; https://github.com/donghwijung/LoRCoN-LO &#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a deep learning-based LiDAR odometry estimation method called LoRCoN-LO that utilizes the long-term recurrent convolutional network (LRCN) structure. The LRCN layer is a structure that can process spatial and temporal information at once by using both CNN and LSTM layers. This feature is suitable for predicting continuous robot movements as it uses point clouds that contain spatial information. Therefore, we built a LoRCoN-LO model using the LRCN layer, and predicted the pose of the robot through this model. For performance verification, we conducted experiments exploiting a public dataset (KITTI). The results of the experiment show that LoRCoN-LO displays accurate odometry prediction in the dataset. The code is available at https://github.com/donghwijung/LoRCoN-LO.
&lt;/p&gt;</description></item><item><title>Dens-PU&#26159;&#19968;&#31181;&#22522;&#20110;&#23494;&#24230;&#30340;&#27491;&#26679;&#26412;&#22686;&#24378;&#30340;PU Learning&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#35299;&#20915;&#20108;&#20803;&#20998;&#31867;&#38382;&#39064;&#65292;&#19988;&#22312;&#22522;&#20934;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.11848</link><description>&lt;p&gt;
&#22522;&#20110;&#23494;&#24230;&#30340;&#27491;&#26679;&#26412;&#22686;&#24378;&#30340; PU Learning &#26041;&#27861;&#65306;Dens-PU
&lt;/p&gt;
&lt;p&gt;
Dens-PU: PU Learning with Density-Based Positive Labeled Augmentation. (arXiv:2303.11848v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11848
&lt;/p&gt;
&lt;p&gt;
Dens-PU&#26159;&#19968;&#31181;&#22522;&#20110;&#23494;&#24230;&#30340;&#27491;&#26679;&#26412;&#22686;&#24378;&#30340;PU Learning&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#35299;&#20915;&#20108;&#20803;&#20998;&#31867;&#38382;&#39064;&#65292;&#19988;&#22312;&#22522;&#20934;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340; PU Learning &#26041;&#27861;&#65292;&#22522;&#20110;&#24322;&#24120;&#26816;&#27979;&#31574;&#30053;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;&#20174;&#27491;&#26679;&#26412;&#25968;&#25454;&#20013;&#25552;&#21462;&#30340;&#28508;&#22312;&#32534;&#30721;&#32447;&#24615;&#32452;&#21512;&#20197;&#33719;&#24471;&#26032;&#26679;&#26412;&#12290;&#36825;&#20123;&#26032;&#26679;&#26412;&#34987;&#29992;&#20316;&#23884;&#20837;&#20197;&#22686;&#21152;&#27491;&#26679;&#26412;&#25968;&#25454;&#30340;&#23494;&#24230;&#65292;&#20174;&#32780;&#23450;&#20041;&#36817;&#20284;&#27491;&#31867;&#30340;&#36793;&#30028;&#12290;&#26679;&#26412;&#36317;&#31163;&#36793;&#30028;&#36234;&#36828;&#65292;&#21017;&#35748;&#20026;&#23427;&#26159;&#36127;&#26679;&#26412;&#30340;&#21487;&#33021;&#24615;&#36234;&#22823;&#12290;&#19968;&#26086;&#33719;&#24471;&#19968;&#32452;&#36127;&#26679;&#26412;&#65292;PU Learning &#38382;&#39064;&#23601;&#36716;&#21270;&#20026;&#20108;&#20803;&#20998;&#31867;&#12290;&#21517;&#20026; Dens-PU &#30340;&#26041;&#27861;&#65292;&#30001;&#20110;&#20854;&#20381;&#36182;&#20110;&#27491;&#26679;&#26412;&#25968;&#25454;&#30340;&#23494;&#24230;&#65292;&#32463;&#36807;&#22522;&#20934;&#22270;&#20687;&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;&#65292;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study proposes a novel approach for solving the PU learning problem based on an anomaly-detection strategy. Latent encodings extracted from positive-labeled data are linearly combined to acquire new samples. These new samples are used as embeddings to increase the density of positive-labeled data and, thus, define a boundary that approximates the positive class. The further a sample is from the boundary the more it is considered as a negative sample. Once a set of negative samples is obtained, the PU learning problem reduces to binary classification. The approach, named Dens-PU due to its reliance on the density of positive-labeled data, was evaluated using benchmark image datasets, and state-of-the-art results were attained.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#32852;&#37030;&#21322;&#30417;&#30563;&#23398;&#20064;&#19982;&#31867;&#21464;&#37327;&#19981;&#24179;&#34913;&#65288;FSSL-CVI&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#21160;&#24577;&#31867;&#21035;&#21152;&#26435;&#26041;&#26696;&#26469;&#22788;&#29702;FSSL&#20013;&#30340;&#31867;&#21035;&#21464;&#37327;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#12290;&#36890;&#36807;&#23454;&#39564;&#32467;&#26524;&#65292;&#26412;&#25991;&#34920;&#26126; FSSL-CVI &#26041;&#27861;&#22312;&#21508;&#26041;&#38754;&#24615;&#33021;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#32852;&#37030;&#23398;&#20064;&#21644;FSSL &#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.11809</link><description>&lt;p&gt;
&#35299;&#20915;&#32852;&#37030;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#31867;&#21464;&#37327;&#19981;&#24179;&#34913;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Addressing Class Variable Imbalance in Federated Semi-supervised Learning. (arXiv:2303.11809v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11809
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#32852;&#37030;&#21322;&#30417;&#30563;&#23398;&#20064;&#19982;&#31867;&#21464;&#37327;&#19981;&#24179;&#34913;&#65288;FSSL-CVI&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#21160;&#24577;&#31867;&#21035;&#21152;&#26435;&#26041;&#26696;&#26469;&#22788;&#29702;FSSL&#20013;&#30340;&#31867;&#21035;&#21464;&#37327;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#12290;&#36890;&#36807;&#23454;&#39564;&#32467;&#26524;&#65292;&#26412;&#25991;&#34920;&#26126; FSSL-CVI &#26041;&#27861;&#22312;&#21508;&#26041;&#38754;&#24615;&#33021;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#32852;&#37030;&#23398;&#20064;&#21644;FSSL &#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#21322;&#30417;&#30563;&#23398;&#20064;&#65288;FSSL&#65289;&#32467;&#21512;&#32852;&#37030;&#23398;&#20064;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#20351;&#29992;&#23569;&#37327;&#26631;&#27880;&#25968;&#25454;&#21644;&#22823;&#37327;&#26410;&#26631;&#27880;&#25968;&#25454;&#22312;&#20998;&#24067;&#24335;&#29615;&#22659;&#20013;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#12290;&#22312;&#19981;&#38656;&#35201;&#23558;&#25152;&#26377;&#25968;&#25454;&#38598;&#20013;&#20110;&#19968;&#22788;&#36827;&#34892;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#23427;&#20250;&#22312;&#35774;&#22791;&#26412;&#22320;&#35757;&#32451;&#27169;&#22411;&#21518;&#25910;&#38598;&#27169;&#22411;&#35757;&#32451;&#26356;&#26032;&#65292;&#20174;&#32780;&#21487;&#20197;&#20445;&#25252;&#29992;&#25143;&#25968;&#25454;&#30340;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#22312;&#32852;&#37030;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#19968;&#20123;&#35774;&#22791;&#26080;&#27861;&#25910;&#38598;&#36275;&#22815;&#30340;&#26412;&#22320;&#35757;&#32451;&#25968;&#25454;&#65292;&#21516;&#26102;&#26032;&#35774;&#22791;&#23558;&#34987;&#28155;&#21152;&#21040;&#32452;&#35757;&#32451;&#20013;&#12290;&#36825;&#23548;&#33268;&#19981;&#24179;&#34913;&#30340;&#20840;&#23616;&#25968;&#25454;&#20998;&#24067;&#65292;&#20174;&#32780;&#24433;&#21709;&#20840;&#23616;&#27169;&#22411;&#35757;&#32451;&#30340;&#24615;&#33021;&#12290;&#22823;&#22810;&#25968;&#24403;&#21069;&#30340;&#30740;&#31350;&#30528;&#37325;&#20110;&#22266;&#23450;&#31867;&#21035;&#25968;&#37327;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#32780;&#24456;&#23569;&#26377;&#27880;&#24847;&#21147;&#25918;&#22312;&#20855;&#26377;&#21487;&#21464;&#31867;&#21035;&#25968;&#37327;&#30340;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#19978;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32852;&#37030;&#21322;&#30417;&#30563;&#23398;&#20064;&#19982;&#31867;&#21464;&#37327;&#19981;&#24179;&#34913;&#65288;FSSL-CVI&#65289;&#65292;&#23427;&#20351;&#29992;&#21160;&#24577;&#31867;&#26435;&#37325;&#26041;&#26696;&#26469;&#35299;&#20915;FSSL&#20013;&#30340;&#31867;&#21464;&#37327;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20855;&#26377;&#31867;&#21464;&#37327;&#19981;&#24179;&#34913;&#30340;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#32852;&#37030;&#23398;&#20064;&#21644;FSSL&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Semi-supervised Learning (FSSL) combines techniques from both fields of federated and semi-supervised learning to improve the accuracy and performance of models in a distributed environment by using a small fraction of labeled data and a large amount of unlabeled data. Without the need to centralize all data in one place for training, it collect updates of model training after devices train models at local, and thus can protect the privacy of user data. However, during the federal training process, some of the devices fail to collect enough data for local training, while new devices will be included to the group training. This leads to an unbalanced global data distribution and thus affect the performance of the global model training. Most of the current research is focusing on class imbalance with a fixed number of classes, while little attention is paid to data imbalance with a variable number of classes. Therefore, in this paper, we propose Federated Semi-supervised Learni
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#34507;&#30333;&#36136;&#32467;&#26500;&#34920;&#31034;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#29992;&#24378;&#22823;&#30340;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#21644;&#33258;&#30417;&#30563;&#32467;&#26500;&#32422;&#26463;&#65292;&#36991;&#20813;&#20102;&#30772;&#22351;&#30495;&#23454;&#30340;&#31354;&#38388;&#32467;&#26500;&#34920;&#31034;&#21644;&#26631;&#35760;&#25968;&#25454;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2303.11783</link><description>&lt;p&gt;
&#36731;&#37327;&#32423;&#23545;&#27604;&#34507;&#30333;&#36136;&#32467;&#26500;-&#24207;&#21015;&#21464;&#25442;
&lt;/p&gt;
&lt;p&gt;
Lightweight Contrastive Protein Structure-Sequence Transformation. (arXiv:2303.11783v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11783
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#34507;&#30333;&#36136;&#32467;&#26500;&#34920;&#31034;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#29992;&#24378;&#22823;&#30340;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#21644;&#33258;&#30417;&#30563;&#32467;&#26500;&#32422;&#26463;&#65292;&#36991;&#20813;&#20102;&#30772;&#22351;&#30495;&#23454;&#30340;&#31354;&#38388;&#32467;&#26500;&#34920;&#31034;&#21644;&#26631;&#35760;&#25968;&#25454;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22810;&#25968;&#34507;&#30333;&#36136;&#19979;&#28216;&#24212;&#29992;&#20013;&#65292;&#26080;&#26631;&#31614;&#30340;&#39044;&#35757;&#32451;&#34507;&#30333;&#36136;&#32467;&#26500;&#27169;&#22411;&#26159;&#20851;&#38190;&#22522;&#30784;&#12290;&#20256;&#32479;&#30340;&#32467;&#26500;&#39044;&#35757;&#32451;&#26041;&#27861;&#36981;&#24490;&#25104;&#29087;&#30340;&#33258;&#28982;&#35821;&#35328;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#20363;&#22914;&#21435;&#22122;&#37325;&#26500;&#21644;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#65292;&#20294;&#36890;&#24120;&#20250;&#30772;&#22351;&#30495;&#23454;&#30340;&#31354;&#38388;&#32467;&#26500;&#34920;&#31034;&#12290;&#20854;&#20182;&#24120;&#35265;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#21487;&#33021;&#20250;&#39044;&#27979;&#19968;&#32452;&#22266;&#23450;&#30340;&#39044;&#23450;&#23545;&#35937;&#31867;&#21035;&#65292;&#20854;&#20013;&#21463;&#38480;&#30340;&#30417;&#30563;&#26041;&#24335;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#36890;&#29992;&#24615;&#21644;&#21487;&#29992;&#24615;&#65292;&#22240;&#20026;&#38656;&#35201;&#39069;&#22806;&#30340;&#26631;&#35760;&#25968;&#25454;&#26469;&#25351;&#23450;&#20219;&#20309;&#20854;&#20182;&#30340;&#34507;&#30333;&#36136;&#27010;&#24565;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#34507;&#30333;&#36136;&#32467;&#26500;&#34920;&#31034;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#20854;&#20013;&#20351;&#29992;&#24378;&#22823;&#30340;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#39318;&#20808;&#24314;&#35758;&#21033;&#29992;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#26080;&#30417;&#30563;&#30340;&#23545;&#27604;&#23545;&#40784;&#26469;&#25351;&#23548;&#32467;&#26500;&#27169;&#22411;&#30340;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#32467;&#26500;&#32422;&#26463;&#65292;&#20197;&#36827;&#19968;&#27493;&#23398;&#20064;&#20869;&#22312;&#30340;&#34507;&#30333;&#36136;&#32467;&#26500;&#34920;&#31034;&#24418;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained protein structure models without labels are crucial foundations for the majority of protein downstream applications. The conventional structure pretraining methods follow the mature natural language pretraining methods such as denoised reconstruction and masked language modeling but usually destroy the real representation of spatial structures. The other common pretraining methods might predict a fixed set of predetermined object categories, where a restricted supervised manner limits their generality and usability as additional labeled data is required to specify any other protein concepts. In this work, we introduce a novel unsupervised protein structure representation pretraining with a robust protein language model. In particular, we first propose to leverage an existing pretrained language model to guide structure model learning through an unsupervised contrastive alignment. In addition, a self-supervised structure constraint is proposed to further learn the intrinsic i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38754;&#21521;&#25968;&#23383;&#23402;&#29983;6G&#29289;&#32852;&#32593;&#30340;&#32852;&#37030;&#36793;&#32536;&#23398;&#20064;&#20013;&#30340;&#27602;&#21270;&#25915;&#20987;&#30340;&#39044;&#27979;&#24615;&#30740;&#31350;&#65292;&#21457;&#29616;&#25915;&#20987;&#32773;&#21487;&#20197;&#22312;&#38598;&#20013;&#24335;&#23398;&#20064;&#21644;&#32852;&#37030;&#23398;&#20064;&#20013;&#36827;&#34892;&#25915;&#20987;&#65292;&#24182;&#19988;&#25104;&#21151;&#30340;&#25915;&#20987;&#20250;&#38477;&#20302;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.11745</link><description>&lt;p&gt;
&#38754;&#21521;&#25968;&#23383;&#23402;&#29983;6G&#29289;&#32852;&#32593;&#30340;&#32852;&#37030;&#36793;&#32536;&#23398;&#20064;&#20013;&#30340;&#27602;&#21270;&#25915;&#20987;&#65306;&#19968;&#39033;&#39044;&#27979;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Poisoning Attacks in Federated Edge Learning for Digital Twin 6G-enabled IoTs: An Anticipatory Study. (arXiv:2303.11745v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11745
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38754;&#21521;&#25968;&#23383;&#23402;&#29983;6G&#29289;&#32852;&#32593;&#30340;&#32852;&#37030;&#36793;&#32536;&#23398;&#20064;&#20013;&#30340;&#27602;&#21270;&#25915;&#20987;&#30340;&#39044;&#27979;&#24615;&#30740;&#31350;&#65292;&#21457;&#29616;&#25915;&#20987;&#32773;&#21487;&#20197;&#22312;&#38598;&#20013;&#24335;&#23398;&#20064;&#21644;&#32852;&#37030;&#23398;&#20064;&#20013;&#36827;&#34892;&#25915;&#20987;&#65292;&#24182;&#19988;&#25104;&#21151;&#30340;&#25915;&#20987;&#20250;&#38477;&#20302;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#36793;&#32536;&#23398;&#20064;&#22312;&#25903;&#25345;&#25968;&#23383;&#23402;&#29983;6G&#29289;&#32852;&#32593;&#29615;&#22659;&#19979;&#30340;&#38544;&#31169;&#20445;&#25252;&#12289;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#24212;&#29992;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#38656;&#35201;&#32771;&#34385;&#21040;&#25915;&#20987;&#30446;&#26631;&#26159;&#22522;&#20110;AI&#31995;&#32479;&#30340;&#28508;&#22312;&#23041;&#32961;&#65292;&#20363;&#22914;&#65292;&#25915;&#20987;&#32773;&#35797;&#22270;&#22312;&#26412;&#22320;&#26356;&#26032;&#26399;&#38388;&#30772;&#22351;&#29289;&#32852;&#32593;&#35774;&#22791;&#19978;&#30340;&#25968;&#25454;&#25110;&#30772;&#22351;&#27169;&#22411;&#26356;&#26032;&#36807;&#31243;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#39033;&#38024;&#23545;&#25968;&#23383;&#23402;&#29983;6G&#29289;&#32852;&#32593;&#29615;&#22659;&#19979;&#32852;&#37030;&#36793;&#32536;&#23398;&#20064;&#20013;&#30340;&#27602;&#21270;&#25915;&#20987;&#30340;&#39044;&#27979;&#24615;&#30740;&#31350;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#25915;&#20987;&#23545;&#25968;&#23383;&#23402;&#29983;6G&#29289;&#32852;&#32593;&#29615;&#22659;&#20013;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#21457;&#23637;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25915;&#20987;&#32773;&#21487;&#20197;&#22312;&#20004;&#31181;&#19981;&#21516;&#30340;&#23398;&#20064;&#35774;&#32622;&#20013;&#65288;&#21363;&#38598;&#20013;&#24335;&#23398;&#20064;&#21644;&#32852;&#37030;&#23398;&#20064;&#65289;&#36827;&#34892;&#27602;&#21270;&#25915;&#20987;&#65292;&#24182;&#19988;&#25104;&#21151;&#30340;&#25915;&#20987;&#20250;&#20005;&#37325;&#38477;&#20302;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#26032;&#30340;&#32593;&#32476;&#23433;&#20840;&#25968;&#25454;&#38598;&#19978;&#20840;&#38754;&#35780;&#20272;&#20102;&#36825;&#20123;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated edge learning can be essential in supporting privacy-preserving, artificial intelligence (AI)-enabled activities in digital twin 6G-enabled Internet of Things (IoT) environments. However, we need to also consider the potential of attacks targeting the underlying AI systems (e.g., adversaries seek to corrupt data on the IoT devices during local updates or corrupt the model updates); hence, in this article, we propose an anticipatory study for poisoning attacks in federated edge learning for digital twin 6G-enabled IoT environments. Specifically, we study the influence of adversaries on the training and development of federated learning models in digital twin 6G-enabled IoT environments. We demonstrate that attackers can carry out poisoning attacks in two different learning settings, namely: centralized learning and federated learning, and successful attacks can severely reduce the model's accuracy. We comprehensively evaluate the attacks on a new cyber security dataset designe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;&#33258;&#32534;&#30721;&#22120;&#30340;&#24555;&#36895;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#23558;&#28145;&#24230;&#27888;&#21202;&#20998;&#35299;&#26694;&#26550;&#19982;&#36880;&#23618;&#30456;&#20851;&#20256;&#25773;&#26041;&#27861;&#30456;&#32467;&#21512;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#35745;&#31639;&#21644;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2303.11734</link><description>&lt;p&gt;
&#35299;&#38145;&#33258;&#32534;&#30721;&#22120;&#30340;&#36880;&#23618;&#30456;&#20851;&#20256;&#25773;
&lt;/p&gt;
&lt;p&gt;
Unlocking Layer-wise Relevance Propagation for Autoencoders. (arXiv:2303.11734v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11734
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;&#33258;&#32534;&#30721;&#22120;&#30340;&#24555;&#36895;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#23558;&#28145;&#24230;&#27888;&#21202;&#20998;&#35299;&#26694;&#26550;&#19982;&#36880;&#23618;&#30456;&#20851;&#20256;&#25773;&#26041;&#27861;&#30456;&#32467;&#21512;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#35745;&#31639;&#21644;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#32534;&#30721;&#22120;&#26159;&#19968;&#31181;&#24378;&#22823;&#32780;&#22810;&#21151;&#33021;&#30340;&#24037;&#20855;&#65292;&#24120;&#29992;&#20110;&#21508;&#31181;&#38382;&#39064;&#65292;&#22914;&#24322;&#24120;&#26816;&#27979;&#12289;&#22270;&#20687;&#22788;&#29702;&#21644;&#26426;&#22120;&#32763;&#35793;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#37325;&#24314;&#24182;&#19981;&#24635;&#26159;&#26131;&#20110;&#35299;&#37322;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23558;&#28145;&#24230;&#27888;&#21202;&#20998;&#35299;&#26694;&#26550;&#19982;&#36880;&#23618;&#30456;&#20851;&#20256;&#25773;&#26041;&#27861;&#30456;&#32467;&#21512;&#26469;&#24555;&#36895;&#35299;&#37322;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#39564;&#35777;&#25216;&#26415;&#65292;&#29992;&#20110;&#27604;&#36739;&#25105;&#20204;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#21644;&#32570;&#22833;&#22320;&#38754;&#30495;&#23454;&#25968;&#25454;&#30340;&#22522;&#20934;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20984;&#26174;&#20102;&#25152;&#25552;&#20986;&#30340;&#21487;&#35299;&#37322;&#24615;&#35299;&#20915;&#26041;&#26696;&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#35745;&#31639;&#21644;&#36136;&#37327;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autoencoders are a powerful and versatile tool often used for various problems such as anomaly detection, image processing and machine translation. However, their reconstructions are not always trivial to explain. Therefore, we propose a fast explainability solution by extending the Layer-wise Relevance Propagation method with the help of Deep Taylor Decomposition framework. Furthermore, we introduce a novel validation technique for comparing our explainability approach with baseline methods in the case of missing ground-truth data. Our results highlight computational as well as qualitative advantages of the proposed explainability solution with respect to existing methods.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20195;&#25968;&#26426;&#22120;&#25512;&#29702;&#26694;&#26550;&#65292;&#33021;&#22815;&#39640;&#25928;&#35299;&#20915;Raven&#28176;&#36827;&#30697;&#38453;&#38382;&#39064;&#65292;&#26368;&#39640;&#20934;&#30830;&#29575;&#21487;&#36798;93.8&#65285;&#12290;</title><link>http://arxiv.org/abs/2303.11730</link><description>&lt;p&gt;
&#25277;&#35937;&#35270;&#35273;&#25512;&#29702;: &#19968;&#31181;&#29992;&#20110;&#35299;&#20915;Raven&#28176;&#36827;&#30697;&#38453;&#30340;&#20195;&#25968;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Abstract Visual Reasoning: An Algebraic Approach for Solving Raven's Progressive Matrices. (arXiv:2303.11730v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11730
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20195;&#25968;&#26426;&#22120;&#25512;&#29702;&#26694;&#26550;&#65292;&#33021;&#22815;&#39640;&#25928;&#35299;&#20915;Raven&#28176;&#36827;&#30697;&#38453;&#38382;&#39064;&#65292;&#26368;&#39640;&#20934;&#30830;&#29575;&#21487;&#36798;93.8&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#25512;&#29702;&#26694;&#26550;---&#20195;&#25968;&#26426;&#22120;&#25512;&#29702;&#65292;&#35813;&#26694;&#26550;&#38750;&#24120;&#36866;&#21512;&#25277;&#35937;&#25512;&#29702;&#12290;&#20195;&#25968;&#26426;&#22120;&#25512;&#29702;&#26377;&#25928;&#22320;&#23558;&#35299;&#20915;&#26032;&#38382;&#39064;&#30340;&#22256;&#38590;&#36807;&#31243;&#31616;&#21270;&#20026;&#20363;&#34892;&#20195;&#25968;&#35745;&#31639;&#12290;&#24863;&#20852;&#36259;&#30340;&#22522;&#26412;&#20195;&#25968;&#23545;&#35937;&#26159;&#26576;&#20123;&#36866;&#24403;&#21021;&#22987;&#21270;&#30340;&#22810;&#39033;&#24335;&#29615;&#30340;&#29702;&#24819;&#12290;&#25105;&#20204;&#23558;&#35299;&#37322;&#22914;&#20309;&#23558;Raven&#28176;&#36827;&#30697;&#38453;&#65288;RPM&#65289;&#30340;&#35299;&#20915;&#21487;&#20197;&#20316;&#20026;&#20195;&#25968;&#35745;&#31639;&#38382;&#39064;&#23454;&#29616;&#65292;&#36825;&#20123;&#38382;&#39064;&#32467;&#21512;&#20102;&#21508;&#31181;&#20247;&#25152;&#21608;&#30693;&#30340;&#20195;&#25968;&#23376;&#20363;&#31243;&#65292;&#21253;&#25324;&#65306;&#35745;&#31639;&#29702;&#24819;&#30340;Gr&#246;bner&#22522;&#65292;&#26816;&#26597;&#29702;&#24819;&#21253;&#21547;&#31561;&#12290;&#20851;&#38190;&#26159;&#65292;&#29702;&#24819;&#28385;&#36275;&#30340;&#38468;&#21152;&#20195;&#25968;&#32467;&#26500;&#20801;&#35768;&#36827;&#34892;&#26356;&#22810;&#30340;&#29702;&#24819;&#25805;&#20316;&#65292;&#36229;&#36234;&#20102;&#38598;&#21512;&#29702;&#35770;&#25805;&#20316;&#12290;&#25105;&#20204;&#30340;&#20195;&#25968;&#26426;&#22120;&#25512;&#29702;&#26694;&#26550;&#19981;&#20165;&#33021;&#22815;&#20174;&#32473;&#23450;&#30340;&#31572;&#26696;&#38598;&#20013;&#36873;&#25321;&#27491;&#30830;&#30340;&#31572;&#26696;&#65292;&#32780;&#19988;&#36824;&#33021;&#20165;&#20973;&#38382;&#39064;&#30697;&#38453;&#32473;&#20986;&#27491;&#30830;&#30340;&#31572;&#26696;&#12290;&#22312;I-RAVEN&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#23545;&#20110;RPM&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#20195;&#25968;&#26426;&#22120;&#25512;&#29702;&#26694;&#26550;&#30340;&#24635;&#20307;&#20934;&#30830;&#29575;&#36798;&#21040;93.8&#65285;&#65292;&#20248;&#20110;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21644;&#20154;&#31867;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce algebraic machine reasoning, a new reasoning framework that is well-suited for abstract reasoning. Effectively, algebraic machine reasoning reduces the difficult process of novel problem-solving to routine algebraic computation. The fundamental algebraic objects of interest are the ideals of some suitably initialized polynomial ring. We shall explain how solving Raven's Progressive Matrices (RPMs) can be realized as computational problems in algebra, which combine various well-known algebraic subroutines that include: Computing the Gr\"obner basis of an ideal, checking for ideal containment, etc. Crucially, the additional algebraic structure satisfied by ideals allows for more operations on ideals beyond set-theoretic operations.  Our algebraic machine reasoning framework is not only able to select the correct answer from a given answer set, but also able to generate the correct answer with only the question matrix given. Experiments on the I-RAVEN dataset yield an overall
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#31070;&#32463;&#32593;&#32476;&#30340;&#22312;&#32447;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#31227;&#21160;&#26426;&#22120;&#20154;&#36718;&#24335;&#37324;&#31243;&#26657;&#27491;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#32467;&#26524;&#65292;&#36991;&#20813;&#20102;&#32791;&#26102;&#30340;&#25968;&#25454;&#25910;&#38598;&#36807;&#31243;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#23454;&#26102;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.11725</link><description>&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#31070;&#32463;&#32593;&#32476;&#30340;&#31227;&#21160;&#26426;&#22120;&#20154;&#36718;&#24335;&#37324;&#31243;&#26657;&#27491;&#30340;&#22312;&#32447;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Online Learning of Wheel Odometry Correction for Mobile Robots with Attention-based Neural Network. (arXiv:2303.11725v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11725
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#31070;&#32463;&#32593;&#32476;&#30340;&#22312;&#32447;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#31227;&#21160;&#26426;&#22120;&#20154;&#36718;&#24335;&#37324;&#31243;&#26657;&#27491;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#32467;&#26524;&#65292;&#36991;&#20813;&#20102;&#32791;&#26102;&#30340;&#25968;&#25454;&#25910;&#38598;&#36807;&#31243;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#23454;&#26102;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#26426;&#22120;&#20154;&#24179;&#21488;&#38656;&#35201;&#21487;&#38752;&#30340;&#23450;&#20301;&#31995;&#32479;&#26469;&#22312;&#20154;&#31867;&#26049;&#36793;&#26085;&#24120;&#36816;&#34892;&#12290;&#22522;&#20110;&#28388;&#27874;&#36718;&#24335;&#21644;&#24815;&#24615;&#37324;&#31243;&#35745;&#30340;&#31616;&#21333;&#23039;&#24577;&#20272;&#35745;&#31639;&#27861;&#36890;&#24120;&#20250;&#22312;&#20986;&#29616;&#31361;&#28982;&#30340;&#21160;&#21147;&#23398;&#21464;&#21270;&#21644;&#36718;&#28369;&#26102;&#22833;&#25928;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;&#35270;&#35273;&#37324;&#31243;&#35745;&#26368;&#36817;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#25552;&#20379;&#26381;&#21153;&#21644;&#36741;&#21161;&#26426;&#22120;&#20154;&#20219;&#21153;&#30340;&#29615;&#22659;&#26465;&#20214;&#24448;&#24448;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22312;&#36825;&#20123;&#26465;&#20214;&#19979;&#65292;&#22522;&#20110;&#35270;&#35273;&#30340;&#35299;&#20915;&#26041;&#26696;&#30001;&#20110;&#29031;&#26126;&#19981;&#33391;&#25110;&#37325;&#22797;&#29305;&#24449;&#27169;&#24335;&#32780;&#22833;&#36133;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#36718;&#24335;&#37324;&#31243;&#26657;&#27491;&#22312;&#32447;&#23398;&#20064;&#26041;&#27861;&#65292;&#20026;&#40065;&#26834;&#30340;&#22810;&#28304;&#23450;&#20301;&#31995;&#32479;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#23558;&#31934;&#30830;&#30340;&#24615;&#33021;&#19982;&#23454;&#26102;&#25512;&#26029;&#30456;&#32467;&#21512;&#12290;&#19982;&#26631;&#20934;&#31070;&#32463;&#32593;&#32476;&#21644;&#22522;&#20110;&#28388;&#27874;&#30340;&#37324;&#31243;&#26657;&#27491;&#31639;&#27861;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#26174;&#31034;&#20986;&#20196;&#20154;&#30633;&#30446;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22312;&#32447;&#23398;&#20064;&#33539;&#24335;&#36991;&#20813;&#20102;&#32791;&#26102;&#30340;&#25968;&#25454;&#25910;&#38598;&#36807;&#31243;&#65292;&#24182;&#19988;&#21487;&#20197;&#38543;&#30528;&#26102;&#38388;&#25913;&#21892;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern robotic platforms need a reliable localization system to operate daily beside humans. Simple pose estimation algorithms based on filtered wheel and inertial odometry often fail in the presence of abrupt kinematic changes and wheel slips. Moreover, despite the recent success of visual odometry, service and assistive robotic tasks often present challenging environmental conditions where visual-based solutions fail due to poor lighting or repetitive feature patterns. In this work, we propose an innovative online learning approach for wheel odometry correction, paving the way for a robust multi-source localization system. An efficient attention-based neural network architecture has been studied to combine precise performances with real-time inference. The proposed solution shows remarkable results compared to a standard neural network and filter-based odometry correction algorithms. Nonetheless, the online learning paradigm avoids the time-consuming data collection procedure and can
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23436;&#25972;&#30340;&#35843;&#30740;&#20171;&#32461;&#20102;&#29983;&#25104;&#22411;AI&#65292;&#21253;&#21547;&#20102;&#20174;&#25216;&#26415;&#21040;&#24212;&#29992;&#30340;&#21508;&#20010;&#26041;&#38754;&#12290;ChatGPT&#34429;&#28982;&#26159;&#19968;&#20010;&#26377;&#29992;&#30340;&#24037;&#20855;&#65292;&#20294;&#24182;&#19981;&#36275;&#20197;&#35206;&#30422;&#25152;&#26377;&#30340;AIGC&#20219;&#21153;&#65292;&#23545;&#20110;&#23454;&#29616;&#22810;&#26679;&#21270;&#30340;&#20869;&#23481;&#21019;&#36896;&#36824;&#38656;&#35201;GPT-5&#31561;&#26410;&#26469;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2303.11717</link><description>&lt;p&gt;
&#29983;&#25104;&#22411;AI&#65288;AIGC&#65289;&#23436;&#25972;&#35843;&#30740;&#65306;ChatGPT&#20174;GPT-4&#21040;GPT-5&#26159;&#20320;&#38656;&#35201;&#30340;&#20840;&#37096;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
A Complete Survey on Generative AI (AIGC): Is ChatGPT from GPT-4 to GPT-5 All You Need?. (arXiv:2303.11717v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11717
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23436;&#25972;&#30340;&#35843;&#30740;&#20171;&#32461;&#20102;&#29983;&#25104;&#22411;AI&#65292;&#21253;&#21547;&#20102;&#20174;&#25216;&#26415;&#21040;&#24212;&#29992;&#30340;&#21508;&#20010;&#26041;&#38754;&#12290;ChatGPT&#34429;&#28982;&#26159;&#19968;&#20010;&#26377;&#29992;&#30340;&#24037;&#20855;&#65292;&#20294;&#24182;&#19981;&#36275;&#20197;&#35206;&#30422;&#25152;&#26377;&#30340;AIGC&#20219;&#21153;&#65292;&#23545;&#20110;&#23454;&#29616;&#22810;&#26679;&#21270;&#30340;&#20869;&#23481;&#21019;&#36896;&#36824;&#38656;&#35201;GPT-5&#31561;&#26410;&#26469;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;ChatGPT&#30340;&#27969;&#34892;&#65292;&#29983;&#25104;&#22411;AI&#65288;AIGC&#65292;&#21363;AI&#29983;&#25104;&#30340;&#20869;&#23481;&#65289;&#22240;&#20854;&#20998;&#26512;&#21644;&#21019;&#36896;&#25991;&#26412;&#12289;&#22270;&#20687;&#31561;&#33021;&#21147;&#32780;&#22312;&#21508;&#20010;&#39046;&#22495;&#24341;&#36215;&#36720;&#21160;&#12290;&#22312;&#22914;&#27492;&#24191;&#27867;&#30340;&#23186;&#20307;&#25253;&#23548;&#19979;&#65292;&#20960;&#20046;&#19981;&#21487;&#33021;&#38169;&#36807;&#20174;&#29305;&#23450;&#35282;&#24230;&#31397;&#25506;AIGC&#30340;&#26426;&#20250;&#12290;&#22312;AI&#20174;&#32431;&#31929;&#30340;&#20998;&#26512;&#36716;&#21521;&#21019;&#36896;&#30340;&#26102;&#20195;&#65292;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;ChatGPT&#20165;&#20165;&#26159;&#20247;&#22810;AIGC&#20219;&#21153;&#20013;&#30340;&#19968;&#20010;&#24037;&#20855;&#65292;&#20854;&#26368;&#26032;&#30340;&#35821;&#35328;&#27169;&#22411;GPT-4&#12290;&#35768;&#22810;&#20154;&#23545;ChatGPT&#30340;&#33021;&#21147;&#21360;&#35937;&#28145;&#21051;&#65292;&#21516;&#26102;&#20063;&#22312;&#24605;&#32771;&#23427;&#30340;&#23616;&#38480;&#24615;&#65306;GPT-5&#65288;&#25110;&#20854;&#20182;&#26410;&#26469;&#30340;GPT&#21464;&#31181;&#65289;&#26159;&#21542;&#33021;&#24110;&#21161;ChatGPT&#32479;&#19968;&#21508;&#31181;AIGC&#20219;&#21153;&#65292;&#23454;&#29616;&#22810;&#26679;&#21270;&#30340;&#20869;&#23481;&#21019;&#36896;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#38656;&#35201;&#36827;&#34892;&#29616;&#26377;AIGC&#20219;&#21153;&#30340;&#20840;&#38754;&#23457;&#26597;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#36805;&#36895;&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#65292;&#39318;&#27425;&#20171;&#32461;&#20102;AIGC&#65292;&#28085;&#30422;&#20102;&#20174;&#25216;&#26415;&#21040;&#24212;&#29992;&#30340;&#25152;&#26377;&#26041;&#38754;&#12290;&#29616;&#20195;&#30340;&#29983;&#25104;&#22411;AI&#20381;&#36182;&#20110;&#21508;&#31181;&#25216;&#26415;&#22522;&#30784;&#65292;&#21253;&#25324;&#27169;&#22411;&#26550;&#26500;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
As ChatGPT goes viral, generative AI (AIGC, a.k.a AI-generated content) has made headlines everywhere because of its ability to analyze and create text, images, and beyond. With such overwhelming media coverage, it is almost impossible for us to miss the opportunity to glimpse AIGC from a certain angle. In the era of AI transitioning from pure analysis to creation, it is worth noting that ChatGPT, with its most recent language model GPT-4, is just a tool out of numerous AIGC tasks. Impressed by the capability of the ChatGPT, many people are wondering about its limits: can GPT-5 (or other future GPT variants) help ChatGPT unify all AIGC tasks for diversified content creation? Toward answering this question, a comprehensive review of existing AIGC tasks is needed. As such, our work comes to fill this gap promptly by offering a first look at AIGC, ranging from its techniques to applications. Modern generative AI relies on various technical foundations, ranging from model architecture and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#23545;&#20110;&#32422;&#26463;&#28385;&#36275;&#38382;&#39064;&#65292;&#37319;&#29992;&#19968;&#31181;&#39640;&#25928;&#30340;&#31639;&#27861;&#20197;&#26131;&#20110;&#29702;&#35299;&#30340;&#26041;&#24335;&#35299;&#37322;&#35299;&#20915;&#26041;&#26696;&#65292;&#36825;&#19968;&#26041;&#27861;&#21487;&#20197;&#35753;&#20154;&#31867;&#26356;&#22909;&#22320;&#29702;&#35299;&#26426;&#22120;&#25152;&#20570;&#30340;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2303.11712</link><description>&lt;p&gt;
&#39640;&#25928;&#35299;&#37322; CSPs &#30340;&#19981;&#21487;&#28385;&#36275;&#23376;&#38598;&#20248;&#21270;&#65288;&#25193;&#23637;&#31639;&#27861;&#21644;&#31034;&#20363;&#65289;
&lt;/p&gt;
&lt;p&gt;
Efficiently Explaining CSPs with Unsatisfiable Subset Optimization (extended algorithms and examples). (arXiv:2303.11712v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#23545;&#20110;&#32422;&#26463;&#28385;&#36275;&#38382;&#39064;&#65292;&#37319;&#29992;&#19968;&#31181;&#39640;&#25928;&#30340;&#31639;&#27861;&#20197;&#26131;&#20110;&#29702;&#35299;&#30340;&#26041;&#24335;&#35299;&#37322;&#35299;&#20915;&#26041;&#26696;&#65292;&#36825;&#19968;&#26041;&#27861;&#21487;&#20197;&#35753;&#20154;&#31867;&#26356;&#22909;&#22320;&#29702;&#35299;&#26426;&#22120;&#25152;&#20570;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#26368;&#36817;&#25552;&#20986;&#30340;&#26041;&#27861;&#22522;&#30784;&#19978;&#65292;&#20026;&#36880;&#27493;&#20197;&#26131;&#20110;&#29702;&#35299;&#26041;&#24335;&#35299;&#37322;&#32422;&#26463;&#28385;&#36275;&#38382;&#39064;&#65288;CSP&#65289;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36825;&#37324;&#30340;&#35299;&#37322;&#26159;&#19968;&#31995;&#21015;&#31616;&#21333;&#30340;&#25512;&#26029;&#27493;&#39588;&#65292;&#20854;&#20013;&#31616;&#21333;&#24615;&#20351;&#29992;&#25104;&#26412;&#20989;&#25968;&#37327;&#21270;&#12290;&#35299;&#37322;&#29983;&#25104;&#31639;&#27861;&#20381;&#36182;&#20110;&#20174;&#23548;&#20986;&#30340;&#19981;&#21487;&#28385;&#36275;&#20844;&#24335;&#20013;&#25552;&#21462;&#26368;&#23567;&#19981;&#28385;&#23376;&#38598;&#65288;MUS&#65289;&#65292;&#21033;&#29992;&#25152;&#35859;&#30340;&#38750;&#20887;&#20313;&#35299;&#37322;&#21644; MUS &#20043;&#38388;&#30340;&#19968;&#19968;&#23545;&#24212;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;MUS &#25552;&#21462;&#31639;&#27861;&#19981;&#25552;&#20379;&#20219;&#20309;&#38024;&#23545;&#32473;&#23450;&#25104;&#26412;&#20989;&#25968;&#30340;&#23376;&#38598;&#26368;&#23567;&#24615;&#25110;&#26368;&#20248;&#24615;&#20445;&#35777;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#22312;&#36825;&#20123;&#24418;&#24335;&#22522;&#30784;&#19978;&#24314;&#31435;&#65292;&#24182;&#30528;&#25163;&#25913;&#36827;&#30340;&#20027;&#35201;&#35201;&#28857;&#65292;&#21363;&#22914;&#20309;&#39640;&#25928;&#22320;&#29983;&#25104;&#21487;&#35777;&#26126;&#26159;&#26368;&#20248;&#30340;&#35299;&#37322;&#65288;&#19982;&#32473;&#23450;&#25104;&#26412;&#24230;&#37327;&#30456;&#20851;&#65289;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#65288;1&#65289;&#22522;&#20110;&#21629;&#20013;&#38598;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#26597;&#25214;&#26368;&#20339;&#21463;&#38480;&#19981;&#21487;&#28385;&#36275;&#23376;&#38598;&#65307;&#65288;2&#65289;&#19968;&#31181;&#37325;&#29992;&#30456;&#20851;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#21487;&#22312;&#19981;&#21516;&#30340;&#35299;&#37322;&#29983;&#25104;&#38454;&#27573;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We build on a recently proposed method for stepwise explaining solutions of Constraint Satisfaction Problems (CSP) in a human-understandable way. An explanation here is a sequence of simple inference steps where simplicity is quantified using a cost function. The algorithms for explanation generation rely on extracting Minimal Unsatisfiable Subsets (MUS) of a derived unsatisfiable formula, exploiting a one-to-one correspondence between so-called non-redundant explanations and MUSs. However, MUS extraction algorithms do not provide any guarantee of subset minimality or optimality with respect to a given cost function. Therefore, we build on these formal foundations and tackle the main points of improvement, namely how to generate explanations efficiently that are provably optimal (with respect to the given cost metric). For that, we developed (1) a hitting set-based algorithm for finding the optimal constrained unsatisfiable subsets; (2) a method for re-using relevant information over m
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#31350;&#29983;&#25104;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#29983;&#25104;&#24320;&#25918;&#38598;&#35782;&#21035;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;SSL-GANs&#21644;OSR-GANs&#26041;&#27861;&#30340;&#30456;&#20284;&#24615;&#22312;&#20110;&#37117;&#35201;&#27714;&#29983;&#25104;&#22120;&#22312;&#20114;&#34917;&#31354;&#38388;&#20013;&#20135;&#29983;&#26679;&#26412;&#65292;&#24182;&#36890;&#36807;&#27491;&#21017;&#21270;&#26469;&#25512;&#24191;&#24320;&#25918;&#31354;&#38388;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;SSL&#20248;&#21270;&#36793;&#32536;-GAN&#22312;&#32467;&#21512;SSL-OSR&#20219;&#21153;&#26041;&#38754;&#26641;&#31435;&#26032;&#30340;&#26631;&#20934;&#65292;&#20294;&#22312;&#26576;&#20123;OSR&#20219;&#21153;&#20013;OSR&#20248;&#21270;&#30340;ARP-GAN&#20173;&#28982;&#30053;&#20248;&#20110;SSL-GAN&#12290;</title><link>http://arxiv.org/abs/2303.11702</link><description>&lt;p&gt;
&#36830;&#25509;&#29983;&#25104;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#29983;&#25104;&#24320;&#25918;&#38598;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Linking generative semi-supervised learning and generative open-set recognition. (arXiv:2303.11702v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11702
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#31350;&#29983;&#25104;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#29983;&#25104;&#24320;&#25918;&#38598;&#35782;&#21035;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;SSL-GANs&#21644;OSR-GANs&#26041;&#27861;&#30340;&#30456;&#20284;&#24615;&#22312;&#20110;&#37117;&#35201;&#27714;&#29983;&#25104;&#22120;&#22312;&#20114;&#34917;&#31354;&#38388;&#20013;&#20135;&#29983;&#26679;&#26412;&#65292;&#24182;&#36890;&#36807;&#27491;&#21017;&#21270;&#26469;&#25512;&#24191;&#24320;&#25918;&#31354;&#38388;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;SSL&#20248;&#21270;&#36793;&#32536;-GAN&#22312;&#32467;&#21512;SSL-OSR&#20219;&#21153;&#26041;&#38754;&#26641;&#31435;&#26032;&#30340;&#26631;&#20934;&#65292;&#20294;&#22312;&#26576;&#20123;OSR&#20219;&#21153;&#20013;OSR&#20248;&#21270;&#30340;ARP-GAN&#20173;&#28982;&#30053;&#20248;&#20110;SSL-GAN&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#30340;&#32972;&#26223;&#19979;&#65292;&#25506;&#31350;&#20102;&#21322;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#21644;&#24320;&#25918;&#38598;&#35782;&#21035;&#65288;OSR&#65289;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#23613;&#31649;&#20197;&#21069;&#27809;&#26377;&#27491;&#24335;&#23558;SSL&#21644;OSR&#32852;&#31995;&#36215;&#26469;&#30340;&#30740;&#31350;&#65292;&#20294;&#23427;&#20204;&#21508;&#33258;&#30340;&#26041;&#27861;&#26377;&#24778;&#20154;&#30340;&#30456;&#20284;&#20043;&#22788;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;SSL-GAN&#21644;OSR-GAN&#35201;&#27714;&#29983;&#25104;&#22120;&#22312;&#20114;&#34917;&#31354;&#38388;&#20013;&#20135;&#29983;&#26679;&#26412;&#12290;&#38543;&#21518;&#65292;&#36890;&#36807;&#23545;&#29983;&#25104;&#26679;&#26412;&#36827;&#34892;&#27491;&#21017;&#21270;&#65292;SSL&#21644;OSR&#20998;&#31867;&#22120;&#37117;&#21487;&#20197;&#23436;&#20840;&#35782;&#21035;&#24320;&#25918;&#31354;&#38388;&#12290;&#20026;&#20102;&#35777;&#26126;SSL&#21644;OSR&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#21644;&#23454;&#39564;&#19978;&#27604;&#36739;&#20102;&#26368;&#20808;&#36827;&#30340;SSL-GAN&#26041;&#27861;&#21644;&#26368;&#20808;&#36827;&#30340;OSR-GAN&#26041;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25991;&#29486;&#22522;&#30784;&#26356;&#21152;&#29282;&#22266;&#30340;SSL&#20248;&#21270;&#36793;&#32536;-GAN&#22312;&#32467;&#21512;SSL-OSR&#20219;&#21153;&#26041;&#38754;&#26641;&#31435;&#26032;&#30340;&#26631;&#20934;&#65292;&#24182;&#22312;&#26576;&#20123;&#19968;&#33324;&#30340;OSR&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;OSR&#20248;&#21270;&#30340;&#23545;&#25239;&#24615;&#20114;&#24800;&#28857;&#65288;ARP&#65289;-GAN&#22312;&#19968;&#20123;OSR&#20219;&#21153;&#20013;&#20173;&#28982;&#30053;&#20248;&#20110;SSL-GAN&#12290;
&lt;/p&gt;
&lt;p&gt;
This study investigates the relationship between semi-supervised learning (SSL) and open-set recognition (OSR) in the context of generative adversarial networks (GANs). Although no previous study has formally linked SSL and OSR, their respective methods share striking similarities. Specifically, SSL-GANs and OSR-GANs require generator to produce samples in the complementary space. Subsequently, by regularising networks with generated samples, both SSL and OSR classifiers generalize the open space. To demonstrate the connection between SSL and OSR, we theoretically and experimentally compare state-of-the-art SSL-GAN methods with state-of-the-art OSR-GAN methods. Our results indicate that the SSL optimised margin-GANs, which have a stronger foundation in literature, set the new standard for the combined SSL-OSR task and achieves new state-of-other art results in certain general OSR experiments. However, the OSR optimised adversarial reciprocal point (ARP)-GANs still slightly out-performe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#21644;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#20102;&#35843;&#26597;&#21644;&#24635;&#32467;&#65292;&#21253;&#25324;&#24179;&#34913;&#24863;&#30693;&#20248;&#21270;&#12289;&#25968;&#25454;&#22686;&#24378;&#21644;&#23458;&#25143;&#31471;&#21152;&#26435;&#31561;&#12290;&#30446;&#21069;&#20173;&#23384;&#22312;&#30528;&#19968;&#20123;&#38382;&#39064;&#38656;&#35201;&#35299;&#20915;&#12290;</title><link>http://arxiv.org/abs/2303.11673</link><description>&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#20013;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey on Class Imbalance in Federated Learning. (arXiv:2303.11673v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11673
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#21644;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#20102;&#35843;&#26597;&#21644;&#24635;&#32467;&#65292;&#21253;&#25324;&#24179;&#34913;&#24863;&#30693;&#20248;&#21270;&#12289;&#25968;&#25454;&#22686;&#24378;&#21644;&#23458;&#25143;&#31471;&#21152;&#26435;&#31561;&#12290;&#30446;&#21069;&#20173;&#23384;&#22312;&#30528;&#19968;&#20123;&#38382;&#39064;&#38656;&#35201;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#25216;&#26415;&#65292;&#20801;&#35768;&#32593;&#32476;&#20013;&#30340;&#22810;&#20010;&#23458;&#25143;&#31471;&#35774;&#22791;&#22312;&#19981;&#30452;&#25509;&#26292;&#38706;&#23458;&#25143;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20849;&#21516;&#35757;&#32451;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#30001;&#20110;&#20854;&#38544;&#31169;&#20445;&#25252;&#30340;&#24615;&#36136;&#32780;&#22791;&#21463;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#35757;&#32451;&#30340;&#27169;&#22411;&#36890;&#24120;&#34920;&#29616;&#19981;&#22914;&#22312;&#26631;&#20934;&#38598;&#20013;&#24335;&#23398;&#20064;&#27169;&#24335;&#19979;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#22312;&#35757;&#32451;&#25968;&#25454;&#19981;&#24179;&#34913;&#30340;&#24773;&#20917;&#19979;&#12290;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#65292;&#25968;&#25454;&#22833;&#34913;&#21487;&#33021;&#22312;&#21333;&#20010;&#23458;&#25143;&#31471;&#35774;&#22791;&#19978;&#21457;&#29983;&#65292;&#20063;&#21487;&#33021;&#22312;&#35768;&#22810;&#35774;&#22791;&#19978;&#20840;&#23616;&#21457;&#29983;&#12290;&#19981;&#21516;&#31867;&#22411;&#25968;&#25454;&#19981;&#24179;&#34913;&#30340;&#22797;&#26434;&#24615;&#32473;&#32852;&#37030;&#23398;&#20064;&#25216;&#26415;&#30340;&#21457;&#23637;&#24102;&#26469;&#20102;&#25361;&#25112;&#65292;&#23588;&#20854;&#26159;&#32771;&#34385;&#21040;&#38656;&#35201;&#21516;&#26102;&#32531;&#35299;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#21644;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;&#22240;&#27492;&#65292;&#35768;&#22810;&#23581;&#35797;&#24050;&#32463;&#22312;&#25991;&#29486;&#20013;&#23545;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#36827;&#34892;&#20102;&#22788;&#29702;&#12290;&#26412;&#25991;&#23545;&#36825;&#19968;&#38382;&#39064;&#30340;&#26368;&#26032;&#36827;&#23637;&#36827;&#34892;&#20102;&#35814;&#32454;&#22238;&#39038;&#65292;&#21253;&#25324;&#24179;&#34913;&#24863;&#30693;&#20248;&#21270;&#12289;&#25968;&#25454;&#22686;&#24378;&#21644;&#23458;&#25143;&#31471;&#21152;&#26435;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#30340;&#24320;&#25918;&#38382;&#39064;&#21644;&#28508;&#22312;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning, which allows multiple client devices in a network to jointly train a machine learning model without direct exposure of clients' data, is an emerging distributed learning technique due to its nature of privacy preservation. However, it has been found that models trained with federated learning usually have worse performance than their counterparts trained in the standard centralized learning mode, especially when the training data is imbalanced. In the context of federated learning, data imbalance may occur either locally one one client device, or globally across many devices. The complexity of different types of data imbalance has posed challenges to the development of federated learning technique, especially considering the need of relieving data imbalance issue and preserving data privacy at the same time. Therefore, in the literature, many attempts have been made to handle class imbalance in federated learning. In this paper, we present a detailed review of recen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36716;&#31227;&#23398;&#20064;&#20013;&#30340;&#23646;&#24615;&#25512;&#26029;&#25915;&#20987;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#25805;&#32437;&#19978;&#28216;&#27169;&#22411;&#65292;&#23545;&#21463;&#23475;&#32773;&#35843;&#25972;&#30340;&#19979;&#28216;&#27169;&#22411;&#36827;&#34892;&#39640;&#25928;&#19988;&#29305;&#23450;&#30340;&#25512;&#26029;&#25915;&#20987;&#65292;&#38656;&#35201;&#27880;&#24847;&#21644;&#38450;&#33539;&#27492;&#31867;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2303.11643</link><description>&lt;p&gt;
&#21033;&#29992;&#36716;&#31227;&#23398;&#20064;&#26469;&#36827;&#34892;&#23646;&#24615;&#25512;&#26029;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Manipulating Transfer Learning for Property Inference. (arXiv:2303.11643v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11643
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36716;&#31227;&#23398;&#20064;&#20013;&#30340;&#23646;&#24615;&#25512;&#26029;&#25915;&#20987;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#25805;&#32437;&#19978;&#28216;&#27169;&#22411;&#65292;&#23545;&#21463;&#23475;&#32773;&#35843;&#25972;&#30340;&#19979;&#28216;&#27169;&#22411;&#36827;&#34892;&#39640;&#25928;&#19988;&#29305;&#23450;&#30340;&#25512;&#26029;&#25915;&#20987;&#65292;&#38656;&#35201;&#27880;&#24847;&#21644;&#38450;&#33539;&#27492;&#31867;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36716;&#31227;&#23398;&#20064;&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21033;&#29992;&#26377;&#38480;&#30340;&#25968;&#25454;&#21644;&#35745;&#31639;&#36164;&#28304;&#26469;&#35843;&#25972;&#39044;&#35757;&#32451;&#30340;&#65288;&#19978;&#28216;&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#25317;&#26377;&#23545;&#29992;&#20110;&#36716;&#31227;&#23398;&#20064;&#20013;&#30340;&#19978;&#28216;&#27169;&#22411;&#36827;&#34892;&#25511;&#21046;&#30340;&#23545;&#25163;&#22914;&#20309;&#23545;&#21463;&#23475;&#32773;&#35843;&#25972;&#30340;&#19979;&#28216;&#27169;&#22411;&#36827;&#34892;&#23646;&#24615;&#25512;&#26029;&#25915;&#20987;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25915;&#20987;&#30340;&#24773;&#20917;&#65292;&#21363;&#23545;&#25163;&#21487;&#20197;&#25805;&#32437;&#19978;&#28216;&#27169;&#22411;&#36827;&#34892;&#39640;&#25928;&#19988;&#29305;&#23450;&#30340;&#23646;&#24615;&#25512;&#26029;&#25915;&#20987;&#65288;AUC&#24471;&#20998;&gt;0.9&#65289;&#65292;&#32780;&#19981;&#20250;&#22312;&#20027;&#35201;&#20219;&#21153;&#20013;&#20135;&#29983;&#26174;&#33879;&#30340;&#24615;&#33021;&#25439;&#22833;&#12290;&#25805;&#32437;&#30340;&#20027;&#35201;&#24605;&#24819;&#26159;&#20351;&#19978;&#28216;&#27169;&#22411;&#20026;&#20855;&#26377;&#30446;&#26631;&#23646;&#24615;&#30340;&#26679;&#26412;&#29983;&#25104;&#20855;&#26377;&#19981;&#21516;&#20998;&#24067;&#30340;&#28608;&#27963;&#65288;&#20013;&#38388;&#29305;&#24449;&#65289;&#65292;&#20174;&#32780;&#20351;&#23545;&#25163;&#33021;&#22815;&#36731;&#26494;&#21306;&#20998;&#35757;&#32451;&#26377;&#20855;&#26377;&#30446;&#26631;&#23646;&#24615;&#30340;&#26679;&#26412;&#21644;&#27809;&#26377;&#30340;&#26679;&#26412;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#23454;&#39564;&#22522;&#20110;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#19978;&#28216;&#27169;&#22411;&#30340;&#38754;&#37096;&#35782;&#21035;&#20219;&#21153;&#21644;ColorFeret&#25968;&#25454;&#38598;&#20316;&#20026;&#19979;&#28216;&#27169;&#22411;&#30340;&#35757;&#32451;&#38598;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20174;&#19994;&#32773;&#22312;&#20351;&#29992;&#36716;&#31227;&#23398;&#20064;&#26102;&#38656;&#35201;&#27880;&#24847;&#23646;&#24615;&#25512;&#26029;&#25915;&#20987;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#37319;&#21462;&#25514;&#26045;&#26469;&#38450;&#27490;&#27492;&#31867;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transfer learning is a popular method for tuning pretrained (upstream) models for different downstream tasks using limited data and computational resources. We study how an adversary with control over an upstream model used in transfer learning can conduct property inference attacks on a victim's tuned downstream model. For example, to infer the presence of images of a specific individual in the downstream training set. We demonstrate attacks in which an adversary can manipulate the upstream model to conduct highly effective and specific property inference attacks (AUC score $&gt; 0.9$), without incurring significant performance loss on the main task. The main idea of the manipulation is to make the upstream model generate activations (intermediate features) with different distributions for samples with and without a target property, thus enabling the adversary to distinguish easily between downstream models trained with and without training examples that have the target property. Our cod
&lt;/p&gt;</description></item><item><title>BoxSnake&#26159;&#19968;&#31181;&#26032;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#25216;&#26415;&#65292;&#21487;&#20197;&#20165;&#20351;&#29992;&#26694;&#27880;&#37322;&#23454;&#29616;&#26377;&#25928;&#30340;&#22810;&#36793;&#24418;&#23454;&#20363;&#20998;&#21106;&#65292;&#30456;&#36739;&#20110;&#22522;&#20110;&#25513;&#33180;&#30340;&#24369;&#30417;&#30563;&#26041;&#27861;&#65292;BoxSnake&#26174;&#31034;&#20986;&#26174;&#30528;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.11630</link><description>&lt;p&gt;
BoxSnake&#65306;&#20351;&#29992;&#26694;&#27880;&#37322;&#30340;&#22810;&#36793;&#24418;&#23454;&#20363;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
BoxSnake: Polygonal Instance Segmentation with Box Supervision. (arXiv:2303.11630v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11630
&lt;/p&gt;
&lt;p&gt;
BoxSnake&#26159;&#19968;&#31181;&#26032;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#25216;&#26415;&#65292;&#21487;&#20197;&#20165;&#20351;&#29992;&#26694;&#27880;&#37322;&#23454;&#29616;&#26377;&#25928;&#30340;&#22810;&#36793;&#24418;&#23454;&#20363;&#20998;&#21106;&#65292;&#30456;&#36739;&#20110;&#22522;&#20110;&#25513;&#33180;&#30340;&#24369;&#30417;&#30563;&#26041;&#27861;&#65292;BoxSnake&#26174;&#31034;&#20986;&#26174;&#30528;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24102;&#26694;&#27880;&#37322;&#30340;&#23454;&#20363;&#20998;&#21106;&#22240;&#21482;&#38656;&#35201;&#31616;&#21333;&#30340;&#26694;&#26631;&#27880;&#32780;&#38750;&#26114;&#36149;&#30340;&#25513;&#33180;&#25110;&#22810;&#36793;&#24418;&#26631;&#27880;&#32780;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24102;&#26694;&#23454;&#20363;&#20998;&#21106;&#27169;&#22411;&#20027;&#35201;&#38598;&#20013;&#22312;&#22522;&#20110;&#25513;&#33180;&#30340;&#26694;&#26550;&#19978;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#25216;&#26415;BoxSnake&#65292;&#39318;&#27425;&#20165;&#20351;&#29992;&#26694;&#27880;&#37322;&#23454;&#29616;&#26377;&#25928;&#30340;&#22810;&#36793;&#24418;&#23454;&#20363;&#20998;&#21106;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#25439;&#22833;&#20989;&#25968;&#65306;&#65288;1&#65289;&#22522;&#20110;&#28857;&#30340;&#21333;&#20803;&#25439;&#22833;&#65292;&#32422;&#26463;&#39044;&#27979;&#22810;&#36793;&#24418;&#30340;&#36793;&#30028;&#26694;&#20197;&#23454;&#29616;&#31895;&#30053;&#20998;&#21106;&#65307;&#65288;2&#65289;&#36317;&#31163;&#24863;&#30693;&#30340;&#25104;&#23545;&#25439;&#22833;&#65292;&#20419;&#20351;&#39044;&#27979;&#30340;&#22810;&#36793;&#24418;&#36148;&#21512;&#29289;&#20307;&#36793;&#30028;&#12290;&#19982;&#22522;&#20110;&#25513;&#33180;&#30340;&#24369;&#30417;&#30563;&#26041;&#27861;&#30456;&#27604;&#65292;BoxSnake&#36827;&#19968;&#27493;&#38477;&#20302;&#20102;&#39044;&#27979;&#20998;&#21106;&#19982;&#36793;&#30028;&#26694;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#24182;&#22312;Cityscapes&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26174;&#30528;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Box-supervised instance segmentation has gained much attention as it requires only simple box annotations instead of costly mask or polygon annotations. However, existing box-supervised instance segmentation models mainly focus on mask-based frameworks. We propose a new end-to-end training technique, termed BoxSnake, to achieve effective polygonal instance segmentation using only box annotations for the first time. Our method consists of two loss functions: (1) a point-based unary loss that constrains the bounding box of predicted polygons to achieve coarse-grained segmentation; and (2) a distance-aware pairwise loss that encourages the predicted polygons to fit the object boundaries. Compared with the mask-based weakly-supervised methods, BoxSnake further reduces the performance gap between the predicted segmentation and the bounding box, and shows significant superiority on the Cityscapes dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#32773;&#25351;&#23548;&#23398;&#20064;&#31574;&#30053;&#65292;&#29992;&#20110;&#25345;&#32493;&#23398;&#20064;&#65292;&#20854;&#20013;&#35780;&#20272;&#32773;&#36890;&#36807;&#25511;&#21046;&#23398;&#20064;&#26041;&#21521;&#21644;&#36895;&#24230;&#26469;&#25351;&#23548;&#23398;&#20064;&#36807;&#31243;&#65292;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#26032;&#29615;&#22659;&#24182;&#36991;&#20813;&#28798;&#38590;&#24615;&#24178;&#25200;&#38382;&#39064;&#30340;&#21457;&#29983;&#12290;</title><link>http://arxiv.org/abs/2303.11624</link><description>&lt;p&gt;
&#38754;&#21521;&#25345;&#32493;&#29615;&#22659;&#30340;&#35780;&#20272;&#32773;&#25351;&#23548;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Assessor-Guided Learning for Continual Environments. (arXiv:2303.11624v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11624
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#32773;&#25351;&#23548;&#23398;&#20064;&#31574;&#30053;&#65292;&#29992;&#20110;&#25345;&#32493;&#23398;&#20064;&#65292;&#20854;&#20013;&#35780;&#20272;&#32773;&#36890;&#36807;&#25511;&#21046;&#23398;&#20064;&#26041;&#21521;&#21644;&#36895;&#24230;&#26469;&#25351;&#23548;&#23398;&#20064;&#36807;&#31243;&#65292;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#26032;&#29615;&#22659;&#24182;&#36991;&#20813;&#28798;&#38590;&#24615;&#24178;&#25200;&#38382;&#39064;&#30340;&#21457;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#32773;&#25351;&#23548;&#23398;&#20064;&#31574;&#30053;&#65292;&#29992;&#20110;&#25345;&#32493;&#23398;&#20064;&#65292;&#20854;&#20013;&#19968;&#20010;&#35780;&#20272;&#32773;&#36890;&#36807;&#25511;&#21046;&#23398;&#20064;&#36807;&#31243;&#30340;&#26041;&#21521;&#21644;&#36895;&#24230;&#26469;&#25351;&#23548;&#22522;&#30784;&#23398;&#20064;&#32773;&#30340;&#23398;&#20064;&#36807;&#31243;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#23398;&#20064;&#26032;&#29615;&#22659;&#24182;&#38450;&#27490;&#28798;&#38590;&#24615;&#24178;&#25200;&#38382;&#39064;&#30340;&#21457;&#29983;&#12290;&#35780;&#20272;&#32773;&#20197;&#20803;&#23398;&#20064;&#30340;&#26041;&#24335;&#36827;&#34892;&#35757;&#32451;&#65292;&#20854;&#20803;&#30446;&#26631;&#26159;&#22686;&#24378;&#22522;&#30784;&#23398;&#20064;&#32773;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;&#23427;&#25191;&#34892;&#27599;&#20010;&#26679;&#26412;&#30340;&#36719;&#21152;&#26435;&#26426;&#21046;&#65292;&#25509;&#21463;&#27491;&#26679;&#26412;&#24182;&#25298;&#32477;&#36127;&#26679;&#26412;&#12290;&#22522;&#30784;&#23398;&#20064;&#32773;&#30340;&#35757;&#32451;&#30446;&#26631;&#26159;&#26368;&#23567;&#21270;&#20132;&#21449;&#29109;&#25439;&#22833;&#20989;&#25968;&#12289;&#26263;&#20307;&#39564;&#37325;&#25918;&#65288;DER&#65289;&#25439;&#22833;&#20989;&#25968;&#21644;&#30693;&#35782;&#33976;&#39311;&#25439;&#22833;&#20989;&#25968;&#30340;&#20803;&#21152;&#26435;&#32452;&#21512;&#65292;&#36825;&#20123;&#20132;&#20114;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an assessor-guided learning strategy for continual learning where an assessor guides the learning process of a base learner by controlling the direction and pace of the learning process thus allowing an efficient learning of new environments while protecting against the catastrophic interference problem. The assessor is trained in a meta-learning manner with a meta-objective to boost the learning process of the base learner. It performs a soft-weighting mechanism of every sample accepting positive samples while rejecting negative samples. The training objective of a base learner is to minimize a meta-weighted combination of the cross entropy loss function, the dark experience replay (DER) loss function and the knowledge distillation loss function whose interactions are controlled in such a way to attain an improved performance. A compensated over-sampling (COS) strategy is developed to overcome the class imbalanced problem of the episodic memory due to limited memor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24322;&#26500;&#20998;&#25903;&#21327;&#20316;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#23545;&#35805;&#29983;&#25104;&#12290;&#35813;&#27169;&#22411;&#20351;&#29992;&#21327;&#20316;&#23398;&#20064;&#26041;&#27861;&#32780;&#38750;&#20256;&#32479;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#22312;&#32593;&#32476;&#20998;&#25903;&#30340;&#35757;&#32451;&#20013;&#32771;&#34385;&#21040;&#23545;&#35805;&#23646;&#24615;&#65292;&#20351;&#19981;&#21516;&#20998;&#25903;&#30340;&#29305;&#24449;&#22810;&#26679;&#21270;&#12290;</title><link>http://arxiv.org/abs/2303.11621</link><description>&lt;p&gt;
&#24322;&#26500;&#20998;&#25903;&#21327;&#20316;&#23398;&#20064;&#29992;&#20110;&#23545;&#35805;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous-Branch Collaborative Learning for Dialogue Generation. (arXiv:2303.11621v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11621
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24322;&#26500;&#20998;&#25903;&#21327;&#20316;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#23545;&#35805;&#29983;&#25104;&#12290;&#35813;&#27169;&#22411;&#20351;&#29992;&#21327;&#20316;&#23398;&#20064;&#26041;&#27861;&#32780;&#38750;&#20256;&#32479;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#22312;&#32593;&#32476;&#20998;&#25903;&#30340;&#35757;&#32451;&#20013;&#32771;&#34385;&#21040;&#23545;&#35805;&#23646;&#24615;&#65292;&#20351;&#19981;&#21516;&#20998;&#25903;&#30340;&#29305;&#24449;&#22810;&#26679;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#21457;&#23637;&#65292;&#39640;&#32423;&#23545;&#35805;&#29983;&#25104;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#26356;&#22810;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#19968;&#31181;&#26377;&#25928;&#30340;&#33719;&#24471;&#39640;&#24615;&#33021;&#21644;&#36731;&#37327;&#32423;&#27169;&#22411;&#30340;&#26041;&#27861;&#26159;&#30693;&#35782;&#33976;&#39311;&#65292;&#20854;&#20381;&#36182;&#20110;&#39044;&#35757;&#32451;&#30340;&#24378;&#22823;&#25945;&#24072;&#27169;&#22411;&#12290;&#21327;&#20316;&#23398;&#20064;&#65292;&#20063;&#31216;&#20026;&#22312;&#32447;&#30693;&#35782;&#33976;&#39311;&#65292;&#22312;&#32570;&#20047;&#35757;&#32451;&#33391;&#22909;&#30340;&#22823;&#22411;&#25945;&#24072;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#26159;&#36827;&#34892;&#21333;&#38454;&#27573;&#32676;&#20307;&#33976;&#39311;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#30001;&#20110;&#30456;&#21516;&#30340;&#35757;&#32451;&#30446;&#26631;&#21644;&#29420;&#31435;&#30456;&#21516;&#30340;&#35757;&#32451;&#38598;&#23384;&#22312;&#20005;&#37325;&#30340;&#20998;&#25903;&#21516;&#36136;&#24615;&#38382;&#39064;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#32771;&#34385;&#22312;&#32593;&#32476;&#20998;&#25903;&#30340;&#35757;&#32451;&#20013;&#32771;&#34385;&#23545;&#35805;&#23646;&#24615;&#12290;&#27599;&#20010;&#20998;&#25903;&#22522;&#20110;&#25152;&#36873;&#23376;&#38598;&#23398;&#20064;&#19982;&#23646;&#24615;&#30456;&#20851;&#30340;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#37325;&#22522;&#20110;&#32676;&#20307;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#21253;&#25324;&#31215;&#26497;&#33976;&#39311;&#21644;&#28040;&#26497;&#33976;&#39311;&#65292;&#36827;&#19968;&#27493;&#20351;&#19981;&#21516;&#20998;&#25903;&#30340;&#29305;&#24449;&#22810;&#26679;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the development of deep learning, advanced dialogue generation methods usually require a greater amount of computational resources. One promising approach to obtaining a high-performance and lightweight model is knowledge distillation, which relies heavily on the pre-trained powerful teacher. Collaborative learning, also known as online knowledge distillation, is an effective way to conduct one-stage group distillation in the absence of a well-trained large teacher model. However, previous work has a severe branch homogeneity problem due to the same training objective and the independent identical training sets. To alleviate this problem, we consider the dialogue attributes in the training of network branches. Each branch learns the attribute-related features based on the selected subset. Furthermore, we propose a dual group-based knowledge distillation method, consisting of positive distillation and negative distillation, to further diversify the features of different branches in
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#22797;&#26434;&#24230;&#28145;&#24230;&#35270;&#39057;&#21387;&#32553;&#30340;&#20998;&#24067;&#24335;&#32534;&#30721;&#32467;&#26500;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#26377;&#25928;&#30340;&#35793;&#30721;&#22120;&#36741;&#21161;&#20449;&#24687;&#29983;&#25104;&#27169;&#22359;&#65292;&#23454;&#29616;&#22312;&#35774;&#22791;&#36164;&#28304;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#20173;&#33021;&#22815;&#26377;&#25928;&#22320;&#21033;&#29992;&#35270;&#39057;&#24103;&#38388;&#30340;&#30456;&#20851;&#24615;&#36827;&#34892;&#39640;&#25928;&#21387;&#32553;&#12290;</title><link>http://arxiv.org/abs/2303.11599</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#32534;&#30721;&#32467;&#26500;&#30340;&#20302;&#22797;&#26434;&#24230;&#28145;&#24230;&#35270;&#39057;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Low-complexity Deep Video Compression with A Distributed Coding Architecture. (arXiv:2303.11599v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11599
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#22797;&#26434;&#24230;&#28145;&#24230;&#35270;&#39057;&#21387;&#32553;&#30340;&#20998;&#24067;&#24335;&#32534;&#30721;&#32467;&#26500;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#26377;&#25928;&#30340;&#35793;&#30721;&#22120;&#36741;&#21161;&#20449;&#24687;&#29983;&#25104;&#27169;&#22359;&#65292;&#23454;&#29616;&#22312;&#35774;&#22791;&#36164;&#28304;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#20173;&#33021;&#22815;&#26377;&#25928;&#22320;&#21033;&#29992;&#35270;&#39057;&#24103;&#38388;&#30340;&#30456;&#20851;&#24615;&#36827;&#34892;&#39640;&#25928;&#21387;&#32553;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#39044;&#27979;&#32534;&#30721;&#35270;&#39057;&#21387;&#32553;&#26041;&#27861;&#20381;&#38752;&#22797;&#26434;&#30340;&#32534;&#30721;&#22120;&#38477;&#20302;&#26102;&#22495;&#20887;&#20313;&#65292;&#23548;&#33268;&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#19978;&#37096;&#32626;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20256;&#32479;&#30340;&#20998;&#24067;&#24335;&#32534;&#30721;&#26041;&#27861;&#23384;&#22312;&#24615;&#33021;&#24046;&#36317;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#31471;&#23545;&#31471;&#20998;&#24067;&#24335;&#28145;&#24230;&#35270;&#39057;&#21387;&#32553;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#21547;&#19968;&#31181;&#26377;&#25928;&#30340;SI&#29983;&#25104;&#27169;&#22359;&#65292;&#26377;&#21161;&#20110;&#22312;&#27809;&#26377;&#35745;&#31639;&#23494;&#38598;&#22411;&#32534;&#30721;&#22120;&#20391;&#36816;&#21160;&#20272;&#35745;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#22320;&#21033;&#29992;&#24103;&#38388;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#36895;&#29575;&#22833;&#30495;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prevalent predictive coding-based video compression methods rely on a heavy encoder to reduce the temporal redundancy, which makes it challenging to deploy them on resource-constrained devices. Meanwhile, as early as the 1970s, distributed source coding theory has indicated that independent encoding and joint decoding with side information (SI) can achieve high-efficient compression of correlated sources. This has inspired a distributed coding architecture aiming at reducing the encoding complexity. However, traditional distributed coding methods suffer from a substantial performance gap to predictive coding ones. Inspired by the great success of learning-based compression, we propose the first end-to-end distributed deep video compression framework to improve the rate-distortion performance. A key ingredient is an effective SI generation module at the decoder, which helps to effectively exploit inter-frame correlations without computation-intensive encoder-side motion estimation and c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22238;&#39038;&#20102;&#26426;&#22120;&#24515;&#26234;&#29702;&#35770;&#22312;&#20449;&#24565;&#12289;&#27442;&#26395;&#21644;&#24847;&#22270;&#26041;&#38754;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#27604;&#36739;&#20102;&#20248;&#32570;&#28857;&#21644;&#36866;&#29992;&#26465;&#20214;&#65292;&#24076;&#26395;&#25351;&#23548;&#30740;&#31350;&#20154;&#21592;&#36319;&#19978;&#35813;&#39046;&#22495;&#30340;&#26368;&#26032;&#36235;&#21183;&#12290;</title><link>http://arxiv.org/abs/2303.11594</link><description>&lt;p&gt;
&#26426;&#22120;&#24515;&#26234;&#29702;&#35770;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Review on Machine Theory of Mind. (arXiv:2303.11594v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11594
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#20102;&#26426;&#22120;&#24515;&#26234;&#29702;&#35770;&#22312;&#20449;&#24565;&#12289;&#27442;&#26395;&#21644;&#24847;&#22270;&#26041;&#38754;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#27604;&#36739;&#20102;&#20248;&#32570;&#28857;&#21644;&#36866;&#29992;&#26465;&#20214;&#65292;&#24076;&#26395;&#25351;&#23548;&#30740;&#31350;&#20154;&#21592;&#36319;&#19978;&#35813;&#39046;&#22495;&#30340;&#26368;&#26032;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#26234;&#29702;&#35770; (ToM) &#26159;&#25351;&#23558;&#24515;&#29702;&#29366;&#24577;&#24402;&#22240;&#20110;&#20182;&#20154;&#30340;&#33021;&#21147;&#65292;&#26159;&#20154;&#31867;&#35748;&#30693;&#30340;&#22522;&#30784;&#12290;&#30446;&#21069;&#65292;&#23545;&#20855;&#26377;&#35748;&#30693;&#33021;&#21147;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#22312;&#21307;&#30103;&#20445;&#20581;&#21644;&#27773;&#36710;&#24037;&#19994;&#31561;&#26041;&#38754;&#30340;&#24212;&#29992;&#26085;&#30410;&#24191;&#27867;&#12290;&#20449;&#24565;&#12289;&#27442;&#26395;&#21644;&#24847;&#22270;&#26159;&#23156;&#20799;&#26089;&#26399;&#30340;&#33021;&#21147;&#65292;&#20063;&#26159;&#20154;&#31867;&#21644;&#20855;&#26377;&#24515;&#26234;&#29702;&#35770;&#30340;&#26426;&#22120;&#30340;&#35748;&#30693;&#33021;&#21147;&#30340;&#22522;&#30784;&#12290;&#26412;&#25991;&#22238;&#39038;&#20102;&#26426;&#22120;&#24515;&#26234;&#29702;&#35770;&#22312;&#20449;&#24565;&#12289;&#27442;&#26395;&#21644;&#24847;&#22270;&#26041;&#38754;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#20171;&#32461;&#20102;&#22312;&#36825;&#19977;&#20010;&#26041;&#38754;&#30340;&#23454;&#39564;&#12289;&#25968;&#25454;&#38598;&#21644;&#26041;&#27861;&#65292;&#24635;&#32467;&#20102;&#36817;&#24180;&#26469;&#19981;&#21516;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#30340;&#21457;&#23637;&#65292;&#24182;&#27604;&#36739;&#20102;&#34920;&#29616;&#33391;&#22909;&#30340;&#27169;&#22411;&#30340;&#20248;&#32570;&#28857;&#21644;&#36866;&#29992;&#26465;&#20214;&#65292;&#24076;&#26395;&#36825;&#39033;&#30740;&#31350;&#33021;&#22815;&#25351;&#23548;&#30740;&#31350;&#20154;&#21592;&#24555;&#36895;&#36319;&#19978;&#35813;&#39046;&#22495;&#30340;&#26368;&#26032;&#36235;&#21183;&#12290;&#19982;&#20855;&#26377;&#29305;&#23450;&#20219;&#21153;&#21644;&#35299;&#20915;&#26041;&#26696;&#26694;&#26550;&#30340;&#20854;&#20182;&#39046;&#22495;&#19981;&#21516;&#65292;&#26426;&#22120;&#24515;&#26234;&#29702;&#35770;&#32570;&#20047;&#32479;&#19968;&#30340;&#25351;&#23548;&#21644;&#19968;&#31995;&#21015;&#26631;&#20934;&#35780;&#20272;&#20219;&#21153;&#65292;&#36825;&#32473;&#35813;&#39046;&#22495;&#24102;&#26469;&#20102;&#35768;&#22810;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Theory of Mind (ToM) is the ability to attribute mental states to others, the basis of human cognition. At present, there has been growing interest in the AI with cognitive abilities, for example in healthcare and the motoring industry. Beliefs, desires, and intentions are the early abilities of infants and the foundation of human cognitive ability, as well as for machine with ToM. In this paper, we review recent progress in machine ToM on beliefs, desires, and intentions. And we shall introduce the experiments, datasets and methods of machine ToM on these three aspects, summarize the development of different tasks and datasets in recent years, and compare well-behaved models in aspects of advantages, limitations and applicable conditions, hoping that this study can guide researchers to quickly keep up with latest trend in this field. Unlike other domains with a specific task and resolution framework, machine ToM lacks a unified instruction and a series of standard evaluation tasks, wh
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;AI&#27169;&#22411;&#22312;&#20581;&#24247;&#20449;&#24687;&#23398;&#39046;&#22495;&#20855;&#26377;&#31361;&#30772;&#24615;&#24212;&#29992;&#65292;&#20294;&#20854;&#35268;&#27169;&#21644;&#25968;&#25454;&#37327;&#30340;&#25361;&#25112;&#38656;&#35201;&#20811;&#26381;&#65292;&#26410;&#26469;&#20173;&#38656;&#28145;&#20837;&#25506;&#32034;&#12290;</title><link>http://arxiv.org/abs/2303.11568</link><description>&lt;p&gt;
&#20581;&#24247;&#20449;&#24687;&#23398;&#20013;&#30340;&#22823;&#22411;AI&#27169;&#22411;&#65306;&#24212;&#29992;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Large AI Models in Health Informatics: Applications, Challenges, and the Future. (arXiv:2303.11568v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11568
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;AI&#27169;&#22411;&#22312;&#20581;&#24247;&#20449;&#24687;&#23398;&#39046;&#22495;&#20855;&#26377;&#31361;&#30772;&#24615;&#24212;&#29992;&#65292;&#20294;&#20854;&#35268;&#27169;&#21644;&#25968;&#25454;&#37327;&#30340;&#25361;&#25112;&#38656;&#35201;&#20811;&#26381;&#65292;&#26410;&#26469;&#20173;&#38656;&#28145;&#20837;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;AI&#27169;&#22411;&#26159;&#26368;&#36817;&#20986;&#29616;&#30340;&#27169;&#22411;&#65292;&#20855;&#26377;&#24222;&#22823;&#30340;&#21442;&#25968;&#21644;&#25968;&#25454;&#35268;&#27169;&#65292;&#20854;&#35268;&#27169;&#24448;&#24448;&#36229;&#36807;&#25968;&#21313;&#20159;&#12290;&#19968;&#26086;&#39044;&#35757;&#32451;&#65292;&#22823;&#22411;AI&#27169;&#22411;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#22312;&#20581;&#24247;&#20449;&#24687;&#23398;&#39046;&#22495;&#65292;&#22823;&#22411;AI&#27169;&#22411;&#30340;&#20986;&#29616;&#20026;&#26041;&#27861;&#23398;&#35774;&#35745;&#24102;&#26469;&#20102;&#26032;&#30340;&#33539;&#20363;&#65292;&#24182;&#20026;&#20581;&#24247;&#30456;&#20851;&#39046;&#22495;&#30340;&#31361;&#30772;&#25552;&#20379;&#20102;&#25512;&#21160;&#21147;&#37327;&#12290;&#26412;&#25991;&#23545;&#22823;&#22411;AI&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#32508;&#36848;&#65292;&#21253;&#25324;&#32972;&#26223;&#12289;&#24212;&#29992;&#21644;&#25361;&#25112;&#31561;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large AI models, or foundation models, are models recently emerging with massive scales both parameter-wise and data-wise, the magnitudes of which often reach beyond billions. Once pretrained, large AI models demonstrate impressive performance in various downstream tasks. A concrete example is the recent debut of ChatGPT, whose capability has compelled people's imagination about the far-reaching influence that large AI models can have and their potential to transform different domains of our life. In health informatics, the advent of large AI models has brought new paradigms for the design of methodologies. The scale of multimodality data in the biomedical and health domain has been ever-expanding especially since the community embraced the era of deep learning, which provides the ground to develop, validate, and advance large AI models for breakthroughs in health-related areas. This article presents an up-to-date comprehensive review of large AI models, from background to their applic
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#39640;&#20998;&#36776;&#29575;&#21355;&#26143;&#22270;&#20687;&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#25968;&#25454;&#31574;&#30053;&#35299;&#20915;&#20102;&#40857;&#33292;&#20848;&#20316;&#29289;&#20998;&#21106;&#20013;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#40857;&#33292;&#20848;&#20316;&#29289;&#25104;&#29087;&#24230;&#20998;&#31867;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.11564</link><description>&lt;p&gt;
&#21033;&#29992;&#39640;&#20998;&#36776;&#29575;&#21355;&#26143;&#22270;&#20687;&#30340;&#28145;&#24230;&#23398;&#20064;&#25968;&#25454;&#31574;&#30053;&#36827;&#34892;&#40857;&#33292;&#20848;&#20316;&#29289;&#20998;&#21106;&#21644;&#25104;&#29087;&#24230;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Agave crop segmentation and maturity classification with deep learning data-centric strategies using very high-resolution satellite imagery. (arXiv:2303.11564v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#39640;&#20998;&#36776;&#29575;&#21355;&#26143;&#22270;&#20687;&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#25968;&#25454;&#31574;&#30053;&#35299;&#20915;&#20102;&#40857;&#33292;&#20848;&#20316;&#29289;&#20998;&#21106;&#20013;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#40857;&#33292;&#20848;&#20316;&#29289;&#25104;&#29087;&#24230;&#20998;&#31867;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36127;&#36131;&#20219;&#21644;&#21487;&#25345;&#32493;&#30340;&#40857;&#33292;&#20848;-&#40857;&#33292;&#20848;&#37202;&#29983;&#20135;&#38142;&#23545;&#22696;&#35199;&#21733;&#40857;&#33292;&#20848;&#22320;&#21306;&#30340;&#31038;&#20250;&#12289;&#29615;&#22659;&#21644;&#32463;&#27982;&#21457;&#23637;&#33267;&#20851;&#37325;&#35201;&#12290;&#22240;&#27492;&#65292;&#24320;&#21457;&#26032;&#24037;&#20855;&#20197;&#23454;&#29616;&#22823;&#35268;&#27169;&#33258;&#21160;&#40857;&#33292;&#20848;&#22320;&#21306;&#30417;&#27979;&#21464;&#24471;&#23588;&#20026;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#39640;&#20998;&#36776;&#29575;&#21355;&#26143;&#22270;&#20687;&#36827;&#34892;&#40857;&#33292;&#20848;&#20316;&#29289;&#20998;&#21106;&#21644;&#25104;&#29087;&#24230;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23545;&#20110;&#27492;&#20219;&#21153;&#21487;&#33021;&#20250;&#26377;&#25152;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;
The responsible and sustainable agave-tequila production chain is fundamental for the social, environment and economic development of Mexico's agave regions. It is therefore relevant to develop new tools for large scale automatic agave region monitoring. In this work, we present an Agave tequilana Weber azul crop segmentation and maturity classification using very high resolution satellite imagery, which could be useful for this task. To achieve this, we solve real-world deep learning problems in the very specific context of agave crop segmentation such as lack of data, low quality labels, highly imbalanced data, and low model performance. The proposed strategies go beyond data augmentation and data transfer combining active learning and the creation of synthetic images with human supervision. As a result, the segmentation performance evaluated with Intersection over Union (IoU) value increased from 0.72 to 0.90 in the test set. We also propose a method for classifying agave crop matur
&lt;/p&gt;</description></item><item><title>DECENT&#26159;&#19968;&#31181;&#33258;&#21160;&#32534;&#30721;&#24322;&#26500;&#20849;&#21516;&#28436;&#21270;&#30340;&#21160;&#24577;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#20174;&#21508;&#31181;&#25968;&#25454;&#27969;&#20013;&#23398;&#20064;&#24739;&#32773;&#12289;&#21307;&#29983;&#12289;&#25151;&#38388;&#21644;&#33647;&#29289;&#30340;&#24322;&#26500;&#21160;&#24577;&#23884;&#20837;&#65292;&#29992;&#20110;&#25913;&#21892;&#24739;&#32773;&#25252;&#29702;&#65292;&#22914;&#33647;&#29289;&#25512;&#33616;&#12289;&#24739;&#32773;&#39118;&#38505;&#20998;&#23618;&#21644;&#21307;&#38498;&#23481;&#37327;&#31649;&#29702;&#12290;</title><link>http://arxiv.org/abs/2303.11563</link><description>&lt;p&gt;
&#21160;&#24577;&#20581;&#24247;&#23884;&#20837;&#25913;&#21892;&#24739;&#32773;&#25252;&#29702;
&lt;/p&gt;
&lt;p&gt;
Dynamic Healthcare Embeddings for Improving Patient Care. (arXiv:2303.11563v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11563
&lt;/p&gt;
&lt;p&gt;
DECENT&#26159;&#19968;&#31181;&#33258;&#21160;&#32534;&#30721;&#24322;&#26500;&#20849;&#21516;&#28436;&#21270;&#30340;&#21160;&#24577;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#20174;&#21508;&#31181;&#25968;&#25454;&#27969;&#20013;&#23398;&#20064;&#24739;&#32773;&#12289;&#21307;&#29983;&#12289;&#25151;&#38388;&#21644;&#33647;&#29289;&#30340;&#24322;&#26500;&#21160;&#24577;&#23884;&#20837;&#65292;&#29992;&#20110;&#25913;&#21892;&#24739;&#32773;&#25252;&#29702;&#65292;&#22914;&#33647;&#29289;&#25512;&#33616;&#12289;&#24739;&#32773;&#39118;&#38505;&#20998;&#23618;&#21644;&#21307;&#38498;&#23481;&#37327;&#31649;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21307;&#38498;&#21521;&#33258;&#21160;&#21270;&#21644;&#38598;&#25104;&#20182;&#20204;&#30340;&#35745;&#31639;&#31995;&#32479;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#31934;&#32454;&#21270;&#21307;&#38498;&#36816;&#33829;&#25968;&#25454;&#21464;&#24471;&#21487;&#29992;&#12290;&#36825;&#20123;&#25968;&#25454;&#21253;&#25324;&#21307;&#38498;&#24314;&#31569;&#22270;&#32440;&#65292;&#24739;&#32773;&#21644;&#21307;&#25252;&#20154;&#21592;&#20043;&#38388;&#30340;&#20132;&#20114;&#26085;&#24535;&#65292;&#22788;&#26041;&#25968;&#25454;&#65292;&#31243;&#24207;&#25968;&#25454;&#20197;&#21450;&#20851;&#20110;&#24739;&#32773;&#20837;&#38498;&#12289;&#20986;&#38498;&#21644;&#36716;&#31227;&#30340;&#25968;&#25454;&#12290;&#36825;&#20026;&#25913;&#21892;&#24739;&#32773;&#25252;&#29702;&#24320;&#36767;&#20102;&#35768;&#22810;&#36855;&#20154;&#30340;&#21487;&#33021;&#24615;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#21033;&#29992;&#29616;&#25104;&#30340;&#26426;&#22120;&#23398;&#20064;&#36719;&#20214;&#36827;&#34892;&#36825;&#20123;&#20219;&#21153;&#65292;&#25105;&#20204;&#38656;&#35201;&#20174;&#24322;&#26500;&#30340;&#12289;&#21160;&#24577;&#30340;&#25968;&#25454;&#27969;&#20013;&#23398;&#20064;&#28041;&#21450;&#23454;&#20307;&#30340;&#32467;&#26500;&#34920;&#31034;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DECTEN&#65292;&#19968;&#31181;&#33258;&#21160;&#32534;&#30721;&#24322;&#26500;&#20849;&#21516;&#28436;&#21270;&#30340;&#21160;&#24577;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#20174;&#21508;&#31181;&#25968;&#25454;&#27969;&#20013;&#23398;&#20064;&#24739;&#32773;&#12289;&#21307;&#29983;&#12289;&#25151;&#38388;&#21644;&#33647;&#29289;&#30340;&#24322;&#26500;&#21160;&#24577;&#23884;&#20837;&#12290;&#36825;&#20123;&#23884;&#20837;&#22522;&#20110;&#38745;&#24577;&#23646;&#24615;&#21644;&#21160;&#24577;&#34892;&#20026;&#25429;&#25417;&#21307;&#29983;&#12289;&#25151;&#38388;&#12289;&#24739;&#32773;&#21644;&#33647;&#29289;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#24182;&#21487;&#29992;&#20110;&#21508;&#31181;&#39044;&#27979;&#20219;&#21153;&#65292;&#22914;&#33647;&#29289;&#25512;&#33616;&#12289;&#24739;&#32773;&#39118;&#38505;&#20998;&#23618;&#21644;&#21307;&#38498;&#23481;&#37327;&#31649;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
As hospitals move towards automating and integrating their computing systems, more fine-grained hospital operations data are becoming available. These data include hospital architectural drawings, logs of interactions between patients and healthcare professionals, prescription data, procedures data, and data on patient admission, discharge, and transfers. This has opened up many fascinating avenues for healthcare-related prediction tasks for improving patient care. However, in order to leverage off-the-shelf machine learning software for these tasks, one needs to learn structured representations of entities involved from heterogeneous, dynamic data streams. Here, we propose DECENT, an auto-encoding heterogeneous co-evolving dynamic neural network, for learning heterogeneous dynamic embeddings of patients, doctors, rooms, and medications from diverse data streams. These embeddings capture similarities among doctors, rooms, patients, and medications based on static attributes and dynamic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#25511;&#39046;&#22495;&#32763;&#35793;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#30446;&#26631;&#29305;&#24449;&#31354;&#38388;&#30340;&#24050;&#20998;&#35299;&#23376;&#31354;&#38388;&#20013;&#20445;&#30041;&#28304;&#29305;&#24449;&#65292;&#20351;&#24471;&#21482;&#20351;&#29992;&#21333;&#20010;&#27169;&#22411;&#23601;&#33021;&#24179;&#28369;&#22320;&#25511;&#21046;&#20445;&#30041;&#28304;&#29305;&#24449;&#30340;&#31243;&#24230;&#65292;&#20135;&#29983;&#26356;&#19968;&#33268;&#12289;&#26356;&#36924;&#30495;&#30340;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2303.11545</link><description>&lt;p&gt;
&#20462;&#27491;&#22122;&#22768;&#65306;&#20026;&#21487;&#25511;&#39046;&#22495;&#32763;&#35793;&#20998;&#35299;&#28304;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Fix the Noise: Disentangling Source Feature for Controllable Domain Translation. (arXiv:2303.11545v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11545
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#25511;&#39046;&#22495;&#32763;&#35793;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#30446;&#26631;&#29305;&#24449;&#31354;&#38388;&#30340;&#24050;&#20998;&#35299;&#23376;&#31354;&#38388;&#20013;&#20445;&#30041;&#28304;&#29305;&#24449;&#65292;&#20351;&#24471;&#21482;&#20351;&#29992;&#21333;&#20010;&#27169;&#22411;&#23601;&#33021;&#24179;&#28369;&#22320;&#25511;&#21046;&#20445;&#30041;&#28304;&#29305;&#24449;&#30340;&#31243;&#24230;&#65292;&#20135;&#29983;&#26356;&#19968;&#33268;&#12289;&#26356;&#36924;&#30495;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#26080;&#26465;&#20214;&#29983;&#25104;&#22120;&#19978;&#20351;&#29992;&#36716;&#31227;&#23398;&#20064;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#22312;&#39046;&#22495;&#32763;&#35793;&#26041;&#38754;&#65292;&#34920;&#29616;&#20986;&#20102;&#24378;&#22823;&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;&#20294;&#26159;&#65292;&#20351;&#29992;&#21333;&#20010;&#27169;&#22411;&#25511;&#21046;&#19981;&#21516;&#39046;&#22495;&#29305;&#24449;&#20043;&#38388;&#30340;&#25511;&#21046;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#39069;&#22806;&#30340;&#27169;&#22411;&#65292;&#36825;&#22312;&#35745;&#31639;&#19978;&#26159;&#35201;&#27714;&#24456;&#39640;&#30340;&#65292;&#32780;&#19988;&#20250;&#23548;&#33268;&#19981;&#20196;&#20154;&#28385;&#24847;&#30340;&#35270;&#35273;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#20855;&#26377;&#21463;&#38480;&#25511;&#21046;&#27493;&#39588;&#65292;&#20174;&#32780;&#38450;&#27490;&#24179;&#28369;&#36807;&#28193;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39640;&#36136;&#37327;&#39046;&#22495;&#32763;&#35793;&#26041;&#27861;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#21487;&#25511;&#24615;&#12290;&#20854;&#20851;&#38190;&#24605;&#24819;&#26159;&#22312;&#30446;&#26631;&#29305;&#24449;&#31354;&#38388;&#30340;&#24050;&#20998;&#35299;&#23376;&#31354;&#38388;&#20013;&#20445;&#30041;&#28304;&#29305;&#24449;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#21482;&#20351;&#29992;&#21333;&#20010;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#24179;&#28369;&#22320;&#25511;&#21046;&#20445;&#30041;&#28304;&#29305;&#24449;&#30340;&#31243;&#24230;&#65292;&#21516;&#26102;&#20174;&#23436;&#20840;&#26032;&#30340;&#39046;&#22495;&#29983;&#25104;&#22270;&#20687;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#20135;&#29983;&#27604;&#20808;&#21069;&#30340;&#24037;&#20316;&#26356;&#19968;&#33268;&#12289;&#26356;&#36924;&#30495;&#30340;&#22270;&#20687;&#65292;&#24182;&#20445;&#25345;&#31934;&#30830;&#30340;&#21487;&#25511;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies show strong generative performance in domain translation especially by using transfer learning techniques on the unconditional generator. However, the control between different domain features using a single model is still challenging. Existing methods often require additional models, which is computationally demanding and leads to unsatisfactory visual quality. In addition, they have restricted control steps, which prevents a smooth transition. In this paper, we propose a new approach for high-quality domain translation with better controllability. The key idea is to preserve source features within a disentangled subspace of a target feature space. This allows our method to smoothly control the degree to which it preserves source features while generating images from an entirely new domain using only a single model. Our extensive experiments show that the proposed method can produce more consistent and realistic images than previous works and maintain precise controllab
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#36890;&#29992;&#27169;&#22411;&#8212;&#8212;&#19981;&#23450;&#27010;&#29575;&#31070;&#32463;&#32593;&#32476;&#65307;&#23427;&#21487;&#20197;&#36827;&#34892;&#26080;&#30417;&#30563;&#32858;&#31867;&#21644;&#20351;&#29992;&#24456;&#23567;&#30340;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#22823;&#35268;&#27169;&#20998;&#31867;&#65292;&#20854;&#29702;&#35770;&#20248;&#21183;&#20307;&#29616;&#22312;&#26032;&#30340;&#27010;&#29575;&#29702;&#35770;&#21644;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#20013;&#12290;</title><link>http://arxiv.org/abs/2303.11536</link><description>&lt;p&gt;
&#19981;&#23450;&#27010;&#29575;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Indeterminate Probability Neural Network. (arXiv:2303.11536v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11536
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#36890;&#29992;&#27169;&#22411;&#8212;&#8212;&#19981;&#23450;&#27010;&#29575;&#31070;&#32463;&#32593;&#32476;&#65307;&#23427;&#21487;&#20197;&#36827;&#34892;&#26080;&#30417;&#30563;&#32858;&#31867;&#21644;&#20351;&#29992;&#24456;&#23567;&#30340;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#22823;&#35268;&#27169;&#20998;&#31867;&#65292;&#20854;&#29702;&#35770;&#20248;&#21183;&#20307;&#29616;&#22312;&#26032;&#30340;&#27010;&#29575;&#29702;&#35770;&#21644;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;IPNN&#30340;&#26032;&#22411;&#36890;&#29992;&#27169;&#22411;&#65292;&#23427;&#23558;&#31070;&#32463;&#32593;&#32476;&#21644;&#27010;&#29575;&#35770;&#32467;&#21512;&#22312;&#19968;&#36215;&#12290;&#22312;&#20256;&#32479;&#27010;&#29575;&#35770;&#20013;&#65292;&#27010;&#29575;&#30340;&#35745;&#31639;&#26159;&#22522;&#20110;&#20107;&#20214;&#30340;&#21457;&#29983;&#65292;&#32780;&#36825;&#22312;&#24403;&#21069;&#30340;&#31070;&#32463;&#32593;&#32476;&#20013;&#20960;&#20046;&#19981;&#20351;&#29992;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27010;&#29575;&#29702;&#35770;&#65292;&#23427;&#26159;&#32463;&#20856;&#27010;&#29575;&#35770;&#30340;&#25193;&#23637;&#65292;&#24182;&#20351;&#32463;&#20856;&#27010;&#29575;&#35770;&#25104;&#20026;&#25105;&#20204;&#29702;&#35770;&#30340;&#19968;&#31181;&#29305;&#27530;&#24773;&#20917;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#25105;&#20204;&#25552;&#20986;&#30340;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20986;&#34987;&#23450;&#20041;&#20026;&#27010;&#29575;&#20107;&#20214;&#65292;&#24182;&#22522;&#20110;&#36825;&#20123;&#20107;&#20214;&#30340;&#32479;&#35745;&#20998;&#26512;&#65292;&#25512;&#23548;&#20986;&#20998;&#31867;&#20219;&#21153;&#30340;&#25512;&#29702;&#27169;&#22411;&#12290;IPNN&#23637;&#29616;&#20102;&#26032;&#30340;&#29305;&#24615;&#65306;&#23427;&#22312;&#36827;&#34892;&#20998;&#31867;&#30340;&#21516;&#26102;&#21487;&#20197;&#25191;&#34892;&#26080;&#30417;&#30563;&#32858;&#31867;&#12290;&#27492;&#22806;&#65292;IPNN&#33021;&#22815;&#20351;&#29992;&#38750;&#24120;&#23567;&#30340;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#38750;&#24120;&#22823;&#30340;&#20998;&#31867;&#65292;&#20363;&#22914;100&#20010;&#36755;&#20986;&#33410;&#28857;&#30340;&#27169;&#22411;&#21487;&#20197;&#20998;&#31867;10&#20159;&#31867;&#21035;&#12290;&#29702;&#35770;&#20248;&#21183;&#20307;&#29616;&#22312;&#26032;&#30340;&#27010;&#29575;&#29702;&#35770;&#21644;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#20013;&#65292;&#24182;&#19988;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;IPNN&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new general model called IPNN - Indeterminate Probability Neural Network, which combines neural network and probability theory together. In the classical probability theory, the calculation of probability is based on the occurrence of events, which is hardly used in current neural networks. In this paper, we propose a new general probability theory, which is an extension of classical probability theory, and makes classical probability theory a special case to our theory. Besides, for our proposed neural network framework, the output of neural network is defined as probability events, and based on the statistical analysis of these events, the inference model for classification task is deduced. IPNN shows new property: It can perform unsupervised clustering while doing classification. Besides, IPNN is capable of making very large classification with very small neural network, e.g. model with 100 output nodes can classify 10 billion categories. Theoretical advantages are refl
&lt;/p&gt;</description></item><item><title>&#20154;&#26426;&#20132;&#20114;(HMI)&#34987;&#21152;&#20837;&#21040;AI&#26550;&#26500;&#35774;&#35745;&#36807;&#31243;&#20013;&#21487;&#20197;&#26377;&#25928;&#20943;&#23569;AI&#24320;&#21457;&#25152;&#38656;&#30340;&#36164;&#28304;&#65292;&#36991;&#20813;&#35757;&#32451;&#21644;&#35780;&#20272;&#20855;&#26377;&#38750;&#29983;&#20135;&#23618;&#30340;AI&#26550;&#26500;&#65292;&#20174;&#32780;&#23548;&#33268;&#36731;&#37327;&#32423;AI&#26550;&#26500;&#30340;&#20135;&#29983;&#12290;</title><link>http://arxiv.org/abs/2303.11508</link><description>&lt;p&gt;
AI&#22312;&#20154;&#26426;&#20132;&#20114;&#20013;&#30340;&#24433;&#21709;-&#22522;&#20110;&#20154;&#26426;&#20132;&#20114;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
AI-in-the-Loop -- The impact of HMI in AI-based Application. (arXiv:2303.11508v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11508
&lt;/p&gt;
&lt;p&gt;
&#20154;&#26426;&#20132;&#20114;(HMI)&#34987;&#21152;&#20837;&#21040;AI&#26550;&#26500;&#35774;&#35745;&#36807;&#31243;&#20013;&#21487;&#20197;&#26377;&#25928;&#20943;&#23569;AI&#24320;&#21457;&#25152;&#38656;&#30340;&#36164;&#28304;&#65292;&#36991;&#20813;&#35757;&#32451;&#21644;&#35780;&#20272;&#20855;&#26377;&#38750;&#29983;&#20135;&#23618;&#30340;AI&#26550;&#26500;&#65292;&#20174;&#32780;&#23548;&#33268;&#36731;&#37327;&#32423;AI&#26550;&#26500;&#30340;&#20135;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;(AI)&#21644;&#20154;&#26426;&#20132;&#20114;(HMI)&#26159;&#36890;&#24120;&#19981;&#36866;&#29992;&#20110;&#23884;&#20837;&#24335;&#24212;&#29992;&#30340;&#20004;&#20010;&#20851;&#38190;&#35789;&#12290;&#22312;&#23558;AI&#24212;&#29992;&#20110;&#35299;&#20915;&#29305;&#23450;&#20219;&#21153;&#20043;&#21069;&#38656;&#35201;&#30340;&#27493;&#39588;&#20013;&#65292;HMI&#36890;&#24120;&#22312;AI&#26550;&#26500;&#35774;&#35745;&#21644;AI&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#32570;&#22833;&#12290;&#20154;&#22312;&#29615;&#36335;&#27010;&#24565;&#22312;AI&#24320;&#21457;&#30340;&#25152;&#26377;&#20854;&#20182;&#27493;&#39588;&#20013;&#37117;&#26222;&#36941;&#23384;&#22312;&#65292;&#20174;&#25968;&#25454;&#20998;&#26512;&#21040;&#25968;&#25454;&#36873;&#25321;&#21644;&#28165;&#27927;&#12289;&#24615;&#33021;&#35780;&#20272;&#12290;&#22312;AI&#26550;&#26500;&#35774;&#35745;&#36807;&#31243;&#20013;&#65292;HMI&#21487;&#20197;&#31435;&#21363;&#31361;&#20986;&#26174;&#31034;&#26550;&#26500;&#30340;&#38750;&#29983;&#20135;&#23618;&#65292;&#20174;&#32780;&#21487;&#20197;&#36731;&#26494;&#21019;&#24314;&#29992;&#20110;&#23884;&#20837;&#24335;&#24212;&#29992;&#30340;&#36731;&#37327;&#32423;&#32593;&#32476;&#26550;&#26500;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#20351;&#29992;&#36825;&#31181;HMI&#65292;&#29992;&#25143;&#21487;&#20197;&#31435;&#21363;&#21306;&#20998;&#21738;&#31181;AI&#26550;&#26500;&#24212;&#35813;&#39318;&#20808;&#36827;&#34892;&#35757;&#32451;&#21644;&#35780;&#20272;&#65292;&#22240;&#20026;&#21487;&#20197;&#39044;&#26399;&#22312;&#20219;&#21153;&#20013;&#39640;&#31934;&#24230;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#36991;&#20813;&#35757;&#32451;&#21644;&#35780;&#20272;&#20855;&#26377;&#38750;&#29983;&#20135;&#23618;&#30340;AI&#26550;&#26500;&#26469;&#20943;&#23569;AI&#24320;&#21457;&#25152;&#38656;&#30340;&#36164;&#28304;&#65292;&#20174;&#32780;&#23548;&#33268;&#36731;&#37327;&#32423;AI&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI) and human-machine interaction (HMI) are two keywords that usually do not fit embedded applications. Within the steps needed before applying AI to solve a specific task, HMI is usually missing during the AI architecture design and the training of an AI model. The human-in-the-loop concept is prevalent in all other steps of developing AI, from data analysis via data selection and cleaning to performance evaluation. During AI architecture design, HMI can immediately highlight unproductive layers of the architecture so that lightweight network architecture for embedded applications can be created easily. We show that by using this HMI, users can instantly distinguish which AI architecture should be trained and evaluated first since a high accuracy on the task could be expected. This approach reduces the resources needed for AI development by avoiding training and evaluating AI architectures with unproductive layers and leads to lightweight AI architectures. The
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32972;&#38376;&#25968;&#23383;&#27700;&#21360;&#30340;&#26041;&#27861;&#65292;&#20197;&#30830;&#20445;&#20844;&#20849;&#25968;&#25454;&#30340;&#23433;&#20840;&#12290;&#36890;&#36807;&#22312;&#25968;&#25454;&#38598;&#20013;&#25554;&#20837;&#26497;&#23569;&#37327;&#30340;&#25968;&#23383;&#27700;&#21360;&#26679;&#26412;&#65292;&#38544;&#24335;&#23398;&#20064;&#19968;&#20010;&#38544;&#34255;&#30340;&#20989;&#25968;&#20316;&#20026;&#25968;&#23383;&#27700;&#21360;&#65292;&#20197;&#36319;&#36394;&#38750;&#27861;&#20351;&#29992;&#27492;&#25968;&#25454;&#38598;&#30340;&#27169;&#22411;&#12290;&#20351;&#29992;&#8220;&#28165;&#27905;&#26631;&#31614;&#32972;&#38376;&#8221;&#26041;&#27861;&#23454;&#29616;&#20102;&#25968;&#23383;&#27700;&#21360;&#65292;&#19981;&#20250;&#30772;&#22351;&#21407;&#22987;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#26377;&#25928;&#22320;&#26816;&#27979;&#21040;&#38750;&#27861;&#21033;&#29992;&#25968;&#25454;&#38598;&#30340;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2303.11470</link><description>&lt;p&gt;
&#20320;&#26377;&#22312;&#20351;&#29992;&#25105;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#21527;&#65311;&#20351;&#29992;&#28165;&#27905;&#26631;&#31614;&#32972;&#38376;&#25968;&#23383;&#27700;&#21360;&#23454;&#29616;&#20844;&#20849;&#25968;&#25454;&#38598;&#20445;&#25252;
&lt;/p&gt;
&lt;p&gt;
Did You Train on My Dataset? Towards Public Dataset Protection with Clean-Label Backdoor Watermarking. (arXiv:2303.11470v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11470
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32972;&#38376;&#25968;&#23383;&#27700;&#21360;&#30340;&#26041;&#27861;&#65292;&#20197;&#30830;&#20445;&#20844;&#20849;&#25968;&#25454;&#30340;&#23433;&#20840;&#12290;&#36890;&#36807;&#22312;&#25968;&#25454;&#38598;&#20013;&#25554;&#20837;&#26497;&#23569;&#37327;&#30340;&#25968;&#23383;&#27700;&#21360;&#26679;&#26412;&#65292;&#38544;&#24335;&#23398;&#20064;&#19968;&#20010;&#38544;&#34255;&#30340;&#20989;&#25968;&#20316;&#20026;&#25968;&#23383;&#27700;&#21360;&#65292;&#20197;&#36319;&#36394;&#38750;&#27861;&#20351;&#29992;&#27492;&#25968;&#25454;&#38598;&#30340;&#27169;&#22411;&#12290;&#20351;&#29992;&#8220;&#28165;&#27905;&#26631;&#31614;&#32972;&#38376;&#8221;&#26041;&#27861;&#23454;&#29616;&#20102;&#25968;&#23383;&#27700;&#21360;&#65292;&#19981;&#20250;&#30772;&#22351;&#21407;&#22987;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#26377;&#25928;&#22320;&#26816;&#27979;&#21040;&#38750;&#27861;&#21033;&#29992;&#25968;&#25454;&#38598;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20114;&#32852;&#32593;&#19978;&#28304;&#28304;&#19981;&#26029;&#30340;&#25903;&#25345;&#35757;&#32451;&#25968;&#25454;&#26159;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25104;&#21151;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#22823;&#37327;&#30340;&#20844;&#20849;&#25968;&#25454;&#20063;&#24341;&#36215;&#20102;&#23545;&#25968;&#25454;&#38598;&#34987;&#26410;&#32463;&#25480;&#26435;&#30340;&#29992;&#20110;&#21830;&#19994;&#30446;&#30340;&#30340;&#25285;&#24551;&#65292;&#36825;&#26159;&#25968;&#25454;&#38598;&#35768;&#21487;&#35777;&#25152;&#31105;&#27490;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32972;&#38376;&#25968;&#23383;&#27700;&#21360;&#30340;&#26041;&#27861;&#65292;&#20316;&#20026;&#20445;&#25252;&#20844;&#20849;&#25968;&#25454;&#30340;&#36890;&#29992;&#26694;&#26550;&#12290;&#36890;&#36807;&#21521;&#25968;&#25454;&#38598;&#20013;&#25554;&#20837;&#23569;&#37327;&#30340;&#25968;&#23383;&#27700;&#21360;&#26679;&#26412;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#23398;&#20064;&#27169;&#22411;&#33021;&#22815;&#38544;&#24335;&#23398;&#20064;&#30001;&#38450;&#24481;&#32773;&#35774;&#32622;&#30340;&#31192;&#23494;&#20989;&#25968;&#12290;&#36825;&#20010;&#38544;&#34255;&#30340;&#20989;&#25968;&#21487;&#20197;&#20316;&#20026;&#25968;&#23383;&#27700;&#21360;&#65292;&#29992;&#20110;&#36319;&#36394;&#38750;&#27861;&#20351;&#29992;&#25968;&#25454;&#38598;&#30340;&#31532;&#19977;&#26041;&#27169;&#22411;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#29616;&#26377;&#30340;&#32972;&#38376;&#25554;&#20837;&#26041;&#27861;&#24448;&#24448;&#28041;&#21450;&#21521;&#35757;&#32451;&#38598;&#20013;&#28155;&#21152;&#20219;&#24847;&#30340;&#12289;&#38169;&#35823;&#26631;&#35760;&#30340;&#25968;&#25454;&#65292;&#23548;&#33268;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#65292;&#24182;&#23481;&#26131;&#34987;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#26816;&#27979;&#21040;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#28165;&#27905;&#26631;&#35760;&#32972;&#38376;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#25968;&#23383;&#27700;&#21360;&#32780;&#19981;&#30772;&#22351;&#21407;&#22987;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20960;&#20010;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#23427;&#22312;&#26816;&#27979;&#38750;&#27861;&#25968;&#25454;&#38598;&#20351;&#29992;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The huge supporting training data on the Internet has been a key factor in the success of deep learning models. However, this abundance of public-available data also raises concerns about the unauthorized exploitation of datasets for commercial purposes, which is forbidden by dataset licenses. In this paper, we propose a backdoor-based watermarking approach that serves as a general framework for safeguarding public-available data. By inserting a small number of watermarking samples into the dataset, our approach enables the learning model to implicitly learn a secret function set by defenders. This hidden function can then be used as a watermark to track down third-party models that use the dataset illegally. Unfortunately, existing backdoor insertion methods often entail adding arbitrary and mislabeled data to the training set, leading to a significant drop in performance and easy detection by anomaly detection algorithms. To overcome this challenge, we introduce a clean-label backdoo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;CommonsenseQA&#25968;&#25454;&#38598;&#20013;&#30340;&#19968;&#22871;&#24120;&#35782;&#25512;&#29702;&#38382;&#39064;&#19978;&#65292;GPT-4&#30340;&#34920;&#29616;&#21450;&#20854;&#23545;&#24120;&#35782;&#30693;&#35782;&#30340;&#22788;&#29702;&#21644;&#25972;&#21512;&#36807;&#31243;&#65292;&#22312;&#27492;&#36807;&#31243;&#20013;&#25105;&#20204;&#20063;&#21457;&#29616;&#20102;&#20854;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.11436</link><description>&lt;p&gt;
&#24515;&#28789;&#19982;&#26426;&#22120;: &#35299;&#24320;GPT-4&#30340;&#35748;&#30693;&#24515;&#29702;&#23398;&#20043;&#35868;
&lt;/p&gt;
&lt;p&gt;
Mind meets machine: Unravelling GPT-4's cognitive psychology. (arXiv:2303.11436v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11436
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;CommonsenseQA&#25968;&#25454;&#38598;&#20013;&#30340;&#19968;&#22871;&#24120;&#35782;&#25512;&#29702;&#38382;&#39064;&#19978;&#65292;GPT-4&#30340;&#34920;&#29616;&#21450;&#20854;&#23545;&#24120;&#35782;&#30693;&#35782;&#30340;&#22788;&#29702;&#21644;&#25972;&#21512;&#36807;&#31243;&#65292;&#22312;&#27492;&#36807;&#31243;&#20013;&#25105;&#20204;&#20063;&#21457;&#29616;&#20102;&#20854;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24120;&#35782;&#25512;&#29702;&#26159;&#20154;&#31867;&#26234;&#33021;&#30340;&#22522;&#26412;&#25104;&#20998;&#65292;&#20351;&#20854;&#33021;&#22815;&#26681;&#25454;&#29615;&#22659;&#35266;&#23519;&#25512;&#26029;&#32467;&#35770;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#27491;&#25104;&#20026;&#36234;&#26469;&#36234;&#33021;&#22815;&#25191;&#34892;&#20154;&#31867;&#32423;&#20219;&#21153;&#30340;&#24378;&#26377;&#21147;&#24037;&#20855;&#12290;&#26368;&#36817;&#24320;&#21457;&#30340;GPT-4&#21450;&#20854;&#22312;&#21307;&#23398;&#32771;&#35797;&#12289;&#24459;&#24072;&#32771;&#35797;&#31561;&#20154;&#31867;&#38590;&#20197;&#23436;&#25104;&#30340;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#30340;&#25104;&#21151;&#65292;&#22686;&#21152;&#20102;LLMs&#25104;&#20026;&#23436;&#32654;&#26234;&#33021;&#24037;&#20855;&#30340;&#20449;&#24515;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;GPT-4&#35770;&#25991;&#21521;&#25105;&#20204;&#23637;&#31034;&#20102;&#20854;&#22312;&#26576;&#20123;&#24120;&#35782;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#20294;&#23545;GPT-4&#22312;&#24120;&#35782;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#20840;&#38754;&#35780;&#20272;&#65292;&#29305;&#21035;&#26159;&#29616;&#26377;&#30340;&#24050;&#32463;&#30830;&#31435;&#22909;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#36824;&#26159;&#32570;&#22833;&#30340;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20851;&#27880;GPT-4&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;CommonsenseQA&#25968;&#25454;&#38598;&#20013;&#30340;&#19968;&#22871;&#24120;&#35782;&#25512;&#29702;&#38382;&#39064;&#19978;&#30340;&#34920;&#29616;&#35780;&#20272;&#21450;&#20854;&#35748;&#30693;&#24515;&#29702;&#23398;&#24037;&#20855;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#33021;&#22815;&#29702;&#35299;GPT-4&#22914;&#20309;&#22312;&#20854;&#35821;&#35328;&#29983;&#25104;&#36807;&#31243;&#20013;&#22788;&#29702;&#21644;&#25972;&#21512;&#24120;&#35782;&#30693;&#35782;&#65292;&#20197;&#21450;&#20854;&#22312;&#36825;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Commonsense reasoning is a basic ingredient of intelligence in humans, empowering the ability to deduce conclusions based on the observations of surroundings. Large language models (LLMs) are emerging as potent tools increasingly capable of performing human-level tasks. The recent development in the form of GPT-4 and its demonstrated success in tasks complex to humans such as medical exam, bar exam and others has led to an increased confidence in the LLMs to become perfect instruments of intelligence. Though, the GPT-4 paper has shown performance on some common sense reasoning tasks, a comprehensive assessment of GPT-4 on common sense reasoning tasks, particularly on the existing well-established datasets is missing. In this study, we focus on the evaluation of GPT-4's performance on a set of common sense reasoning questions from the widely used CommonsenseQA dataset along with tools from cognitive psychology. In doing so, we understand how GPT-4 processes and integrates common sense k
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#21416;&#25151;&#35774;&#35745;&#20026;&#20363;&#65292;&#30740;&#31350;&#21512;&#29702;&#30340;&#20849;&#20139;&#31354;&#38388;&#35774;&#35745;&#22914;&#20309;&#25552;&#39640;&#20154;&#26426;&#21327;&#20316;&#33021;&#21147;&#65292;&#20351;&#29992;&#20998;&#25955;&#24335;&#36816;&#21160;&#35268;&#21010;&#22120;&#26377;&#25928;&#35299;&#20915;&#22810;&#26234;&#33021;&#20307;&#36816;&#21160;&#35268;&#21010;&#38382;&#39064;&#65292;&#32467;&#26524;&#34920;&#26126;&#20248;&#21270;&#30340;&#21416;&#25151;&#35774;&#35745;&#21487;&#20197;&#26126;&#26174;&#25552;&#39640;&#20154;&#26426;&#21327;&#20316;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.11425</link><description>&lt;p&gt;
&#36890;&#36807;&#35745;&#31639;&#35774;&#35745;&#25552;&#39640;&#20154;&#26426;&#21327;&#20316;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improving Human-Robot Collaboration via Computational Design. (arXiv:2303.11425v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11425
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#21416;&#25151;&#35774;&#35745;&#20026;&#20363;&#65292;&#30740;&#31350;&#21512;&#29702;&#30340;&#20849;&#20139;&#31354;&#38388;&#35774;&#35745;&#22914;&#20309;&#25552;&#39640;&#20154;&#26426;&#21327;&#20316;&#33021;&#21147;&#65292;&#20351;&#29992;&#20998;&#25955;&#24335;&#36816;&#21160;&#35268;&#21010;&#22120;&#26377;&#25928;&#35299;&#20915;&#22810;&#26234;&#33021;&#20307;&#36816;&#21160;&#35268;&#21010;&#38382;&#39064;&#65292;&#32467;&#26524;&#34920;&#26126;&#20248;&#21270;&#30340;&#21416;&#25151;&#35774;&#35745;&#21487;&#20197;&#26126;&#26174;&#25552;&#39640;&#20154;&#26426;&#21327;&#20316;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#26426;&#22120;&#20154;&#36827;&#20837;&#25105;&#20204;&#30340;&#26085;&#24120;&#29983;&#27963;&#26102;&#65292;&#20154;&#19982;&#26426;&#22120;&#20154;&#20849;&#20139;&#30340;&#31354;&#38388;&#23545;&#20110;&#26377;&#25928;&#30340;&#20154;&#26426;&#21327;&#20316;&#33267;&#20851;&#37325;&#35201;&#12290;&#20849;&#20139;&#31354;&#38388;&#30340;&#35774;&#35745;&#24212;&#28385;&#36275;&#20154;&#31867;&#30340;&#20559;&#22909;&#21644;&#26426;&#22120;&#20154;&#30340;&#25928;&#29575;&#12290;&#26412;&#25991;&#20197;&#21416;&#25151;&#35774;&#35745;&#20026;&#20363;&#65292;&#35828;&#26126;&#33391;&#22909;&#30340;&#31354;&#38388;&#35774;&#35745;&#22312;&#20419;&#36827;&#20154;&#26426;&#21327;&#20316;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#12290;&#22312;&#32473;&#23450;&#21416;&#25151;&#36793;&#30028;&#12289;&#21488;&#38754;&#21644;&#33756;&#35889;&#30340;&#24773;&#20917;&#19979;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#35745;&#31639;&#28385;&#36275;&#21416;&#25151;&#35774;&#35745;&#35268;&#21017;&#24182;&#25913;&#21892;&#20154;&#26426;&#21327;&#20316;&#30340;&#21488;&#38754;&#30340;&#26368;&#20339;&#25670;&#25918;&#20301;&#32622;&#12290;&#20027;&#35201;&#25216;&#26415;&#25361;&#25112;&#22312;&#20110;&#20248;&#21270;&#26041;&#27861;&#36890;&#24120;&#35201;&#35780;&#20272;&#25968;&#21315;&#31181;&#35774;&#35745;&#65292;&#32780;&#35780;&#20272;&#20989;&#25968;&#30340;&#19968;&#37096;&#20998;&#8212;&#8212;&#36816;&#21160;&#35268;&#21010;&#30340;&#35745;&#31639;&#25104;&#26412;&#24456;&#39640;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#31181;&#21487;&#39640;&#25928;&#35299;&#20915;&#22810;&#26234;&#33021;&#20307;&#36816;&#21160;&#35268;&#21010;&#30340;&#20998;&#25955;&#24335;&#36816;&#21160;&#35268;&#21010;&#22120;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20248;&#21270;&#30340;&#21416;&#25151;&#35774;&#35745;&#21487;&#20197;&#26126;&#26174;&#25552;&#39640;&#20154;&#26426;&#21327;&#20316;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
When robots entered our day-to-day life, the shared space surrounding humans and robots is critical for effective Human-Robot collaboration. The design of shared space should satisfy humans' preferences and robots' efficiency. This work uses kitchen design as an example to illustrate the importance of good space design in facilitating such collaboration. Given the kitchen boundary, counters, and recipes, the proposed method computes the optimal placement of counters that meet the requirement of kitchen design rules and improve Human-Robot collaboration. The key technical challenge is that the optimization method usually evaluates thousands of designs and the computational cost of motion planning, which is part of the evaluation function, is expensive. We use a decentralized motion planner that can solve multi-agent motion planning efficiently. Our results indicate that optimized kitchen designs can provide noticeable performance improvement to Human-Robot collaboration.
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36328;&#22495;&#29305;&#24449;&#34701;&#21512;&#26041;&#27861;&#30340;EEG&#20449;&#21495;&#20998;&#31867;&#32593;&#32476;&#65292;&#20854;&#20013;&#34701;&#21512;&#20102;&#26102;&#39057;&#22495;&#21644;&#31354;&#38388;&#22495;&#30340;&#22810;&#37325;&#34920;&#31034;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;EEG&#24773;&#24863;&#35782;&#21035;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.11421</link><description>&lt;p&gt;
&#34701;&#21512;&#26102;&#39057;&#21644;&#31354;&#38388;&#34920;&#31034;&#26469;&#25913;&#21892;&#22522;&#20110;EEG&#30340;&#24773;&#24863;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Improving EEG-based Emotion Recognition by Fusing Time-frequency And Spatial Representations. (arXiv:2303.11421v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11421
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36328;&#22495;&#29305;&#24449;&#34701;&#21512;&#26041;&#27861;&#30340;EEG&#20449;&#21495;&#20998;&#31867;&#32593;&#32476;&#65292;&#20854;&#20013;&#34701;&#21512;&#20102;&#26102;&#39057;&#22495;&#21644;&#31354;&#38388;&#22495;&#30340;&#22810;&#37325;&#34920;&#31034;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;EEG&#24773;&#24863;&#35782;&#21035;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#23545;EEG&#20449;&#21495;&#36827;&#34892;&#20998;&#31867;&#21487;&#20197;&#20934;&#30830;&#35782;&#21035;&#20154;&#20204;&#30340;&#24773;&#32490;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#24456;&#23569;&#32771;&#34385;&#23558;&#21478;&#19968;&#20010;&#39046;&#22495;&#30340;&#34920;&#31034;&#20013;&#30340;&#20449;&#24687;&#24212;&#29992;&#20110;&#26102;&#39057;&#22495;&#30340;&#29305;&#24449;&#36873;&#25321;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36328;&#22495;&#29305;&#24449;&#34701;&#21512;&#26041;&#27861;&#30340;EEG&#20449;&#21495;&#20998;&#31867;&#32593;&#32476;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#22495;&#27880;&#24847;&#26426;&#21046;&#65292;&#20351;&#32593;&#32476;&#26356;&#19987;&#27880;&#20110;&#19982;&#33041;&#27963;&#21160;&#21644;&#24605;&#32500;&#21464;&#21270;&#26368;&#30456;&#20851;&#30340;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#27493;&#34701;&#21512;&#26041;&#27861;&#65292;&#24182;&#23558;&#36825;&#20123;&#26041;&#27861;&#24212;&#29992;&#20110;EEG&#24773;&#24863;&#35782;&#21035;&#32593;&#32476;&#20013;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#34701;&#21512;&#26102;&#39057;&#22495;&#21644;&#31354;&#38388;&#22495;&#30340;&#22810;&#37325;&#34920;&#31034;&#26041;&#27861;&#65292;&#20248;&#20110;&#20808;&#21069;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#30340;&#26041;&#27861;&#65292;&#22312;&#24403;&#21069;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Using deep learning methods to classify EEG signals can accurately identify people's emotions. However, existing studies have rarely considered the application of the information in another domain's representations to feature selection in the time-frequency domain. We propose a classification network of EEG signals based on the cross-domain feature fusion method, which makes the network more focused on the features most related to brain activities and thinking changes by using the multi-domain attention mechanism. In addition, we propose a two-step fusion method and apply these methods to the EEG emotion recognition network. Experimental results show that our proposed network, which combines multiple representations in the time-frequency domain and spatial domain, outperforms previous methods on public datasets and achieves state-of-the-art at present.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#21407;&#22987;&#38647;&#36798;&#27169;&#25311;&#25968;&#23383;&#65288;ADC&#65289;&#25968;&#25454;&#19978;&#25191;&#34892;&#31471;&#21040;&#31471;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;&#20449;&#21495;&#22788;&#29702;&#27169;&#22359;&#34987;&#23884;&#20837;&#32593;&#32476;&#20013;&#65292;&#23454;&#39564;&#32467;&#26524;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.11420</link><description>&lt;p&gt;
ADCNet&#65306;&#24102;&#21407;&#22987;&#38647;&#36798;ADC&#25968;&#25454;&#30340;&#31471;&#21040;&#31471;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
ADCNet: End-to-end perception with raw radar ADC data. (arXiv:2303.11420v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11420
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#21407;&#22987;&#38647;&#36798;&#27169;&#25311;&#25968;&#23383;&#65288;ADC&#65289;&#25968;&#25454;&#19978;&#25191;&#34892;&#31471;&#21040;&#31471;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;&#20449;&#21495;&#22788;&#29702;&#27169;&#22359;&#34987;&#23884;&#20837;&#32593;&#32476;&#20013;&#65292;&#23454;&#39564;&#32467;&#26524;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#34892;&#19994;&#23545;&#38647;&#36798;&#20256;&#24863;&#22120;&#30340;&#20852;&#36259;&#37325;&#26032;&#28608;&#21457;&#12290;&#38647;&#36798;&#20316;&#20026;&#19968;&#31181;&#30456;&#23545;&#25104;&#29087;&#30340;&#25216;&#26415;&#65292;&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#24471;&#21040;&#20102;&#31283;&#23450;&#30340;&#25913;&#36827;&#65292;&#20351;&#20854;&#25104;&#20026;&#24120;&#29992;&#30340;LiDAR&#30340;&#26377;&#21560;&#24341;&#21147;&#30340;&#26367;&#20195;&#21697;&#25110;&#34917;&#20805;&#12290;&#19968;&#31181;&#26032;&#20852;&#30340;&#36235;&#21183;&#26159;&#21033;&#29992;&#20016;&#23500;&#30340;&#20302;&#32423;&#21035;&#38647;&#36798;&#25968;&#25454;&#36827;&#34892;&#24863;&#30693;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#36235;&#21183;&#25512;&#21521;&#20102;&#26497;&#31471;--&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#21407;&#22987;&#38647;&#36798;&#27169;&#25311;&#25968;&#23383;&#65288;ADC&#65289;&#25968;&#25454;&#19978;&#25191;&#34892;&#31471;&#21040;&#31471;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#35774;&#35745;&#20102;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;&#20449;&#21495;&#22788;&#29702;&#27169;&#22359;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#30001;&#20256;&#32479;&#20449;&#21495;&#22788;&#29702;&#31639;&#27861;&#24341;&#23548;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#23454;&#20102;&#31471;&#21040;&#31471;&#23398;&#20064;&#26041;&#27861;&#30340;&#25972;&#20307;&#26377;&#25928;&#24615;&#65292;&#32780;&#28040;&#34701;&#30740;&#31350;&#39564;&#35777;&#20102;&#25105;&#20204;&#20010;&#20307;&#21019;&#26032;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is a renewed interest in radar sensors in the autonomous driving industry. As a relatively mature technology, radars have seen steady improvement over the last few years, making them an appealing alternative or complement to the commonly used LiDARs. An emerging trend is to leverage rich, low-level radar data for perception. In this work we push this trend to the extreme -- we propose a method to perform end-to-end learning on the raw radar analog-to-digital (ADC) data. Specifically, we design a learnable signal processing module inside the neural network, and a pre-training method guided by traditional signal processing algorithms. Experiment results corroborate the overall efficacy of the end-to-end learning method, while an ablation study validates the effectiveness of our individual innovations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21435;&#38500;&#33050;&#27493;&#24341;&#36215;&#30340;&#25391;&#21160;&#20449;&#21495;&#30340;&#22122;&#22768;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#39640;&#26031;&#22122;&#22768;&#21644;&#38750;&#24179;&#31283;&#22122;&#22768;&#12290;</title><link>http://arxiv.org/abs/2303.11413</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25391;&#21160;&#20449;&#21495;&#21435;&#22122;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Vibration Signal Denoising Using Deep Learning. (arXiv:2303.11413v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11413
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21435;&#38500;&#33050;&#27493;&#24341;&#36215;&#30340;&#25391;&#21160;&#20449;&#21495;&#30340;&#22122;&#22768;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#39640;&#26031;&#22122;&#22768;&#21644;&#38750;&#24179;&#31283;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#33050;&#27493;&#24341;&#36215;&#30340;&#32467;&#26500;&#25391;&#21160;&#20449;&#21495;&#34987;&#24191;&#27867;&#29992;&#20110;&#20154;&#21592;&#35782;&#21035;&#12289;&#23450;&#20301;&#12289;&#20154;&#31867;&#27963;&#21160;&#25512;&#26029;&#12289;&#32467;&#26500;&#20581;&#24247;&#30417;&#27979;&#31561;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#29615;&#22659;&#22122;&#22768;&#12289;&#30005;&#30913;&#24178;&#25200;&#31561;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#23454;&#38469;&#37319;&#38598;&#30340;&#20449;&#21495;&#36890;&#24120;&#20250;&#24102;&#26377;&#22122;&#22768;&#12290;&#22122;&#22768;&#30340;&#23384;&#22312;&#24433;&#21709;&#20102;&#20449;&#21495;&#22788;&#29702;&#36807;&#31243;&#65292;&#20174;&#32780;&#24433;&#21709;&#20102;&#26368;&#32456;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#21644;&#35823;&#24046;&#12290;&#26412;&#25991;&#20027;&#35201;&#25506;&#35752;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21435;&#38500;&#33050;&#27493;&#24341;&#36215;&#30340;&#25391;&#21160;&#20449;&#21495;&#30340;&#22122;&#22768;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19981;&#21516;&#31867;&#22411;&#30340;&#22122;&#22768;&#65292;&#21253;&#25324;&#39640;&#26031;&#22122;&#22768;&#21644;&#38750;&#24179;&#31283;&#22122;&#22768;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Structure vibration signals induced by footsteps are widely used for tasks like occupant identification, localization, human activity inference, structure health monitoring and so on. The vibration signals are collected as time series with amplitude values. However, the collected signals are always noisy in practice due to the influence of environmental noise, electromagnetic interference and other factors. The presence of noise affects the process of signal analysis, thus affecting the accuracy and error of the final tasks. In this paper, we mainly explore the denoising methods for footstep-induced vibration signals. We have considered different kinds of noise including stationary noises such as gaussian noises and non-stationary noises such as item-dropping vibration noise and music noises.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GNN-Ensemble&#30340;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#26500;&#24314;&#38543;&#26426;&#20915;&#31574;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#38598;&#21512;&#65292;&#20197;&#25552;&#39640;GNNs&#30340;&#24615;&#33021;&#65292;&#27867;&#21270;&#33021;&#21147;&#21644;&#25239;&#25915;&#20987;&#24615;&#65292;&#24182;&#36981;&#24490;&#38543;&#26426;&#24314;&#27169;&#30340;&#21407;&#21017;&#12290;</title><link>http://arxiv.org/abs/2303.11376</link><description>&lt;p&gt;
GNN-Ensemble&#65306;&#38754;&#21521;&#38543;&#26426;&#20915;&#31574;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
GNN-Ensemble: Towards Random Decision Graph Neural Networks. (arXiv:2303.11376v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11376
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GNN-Ensemble&#30340;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#26500;&#24314;&#38543;&#26426;&#20915;&#31574;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#38598;&#21512;&#65292;&#20197;&#25552;&#39640;GNNs&#30340;&#24615;&#33021;&#65292;&#27867;&#21270;&#33021;&#21147;&#21644;&#25239;&#25915;&#20987;&#24615;&#65292;&#24182;&#36981;&#24490;&#38543;&#26426;&#24314;&#27169;&#30340;&#21407;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#22270;&#32467;&#26500;&#25968;&#25454;&#26041;&#38754;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#26159;&#29616;&#26377;&#30340;&#22522;&#20110;&#22270;&#30340;&#24212;&#29992;&#36890;&#24120;&#32570;&#20047;&#27880;&#37322;&#25968;&#25454;&#12290;GNNs&#38656;&#35201;&#20174;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#20013;&#23398;&#20064;&#28508;&#22312;&#30340;&#27169;&#24335;&#65292;&#20197;&#23545;&#22823;&#37327;&#30340;&#27979;&#35797;&#25968;&#25454;&#36827;&#34892;&#25512;&#26029;&#12290;GNNs&#30340;&#22686;&#21152;&#22797;&#26434;&#24615;&#65292;&#20197;&#21450;&#21333;&#28857;&#27169;&#22411;&#21442;&#25968;&#21021;&#22987;&#21270;&#65292;&#36890;&#24120;&#20250;&#23548;&#33268;&#36807;&#24230;&#36866;&#24212;&#21644;&#27425;&#20248;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#20247;&#25152;&#21608;&#30693;GNNs&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GNN-Ensemble&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36981;&#24490;&#38543;&#26426;&#24314;&#27169;&#30340;&#21407;&#21017;&#65292;&#22312;&#25299;&#25169;&#19978;&#38543;&#26426;&#36873;&#25321;&#23376;&#32467;&#26500;&#20013;&#26500;&#24314;&#22810;&#20010;GNNs&#26469;&#26500;&#24314;&#38543;&#26426;&#20915;&#31574;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#20854;&#23481;&#37327;&#21487;&#20197;&#20219;&#24847;&#25193;&#23637;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have enjoyed wide spread applications in graph-structured data. However, existing graph based applications commonly lack annotated data. GNNs are required to learn latent patterns from a limited amount of training data to perform inferences on a vast amount of test data. The increased complexity of GNNs, as well as a single point of model parameter initialization, usually lead to overfitting and sub-optimal performance. In addition, it is known that GNNs are vulnerable to adversarial attacks. In this paper, we push one step forward on the ensemble learning of GNNs with improved accuracy, generalization, and adversarial robustness. Following the principles of stochastic modeling, we propose a new method called GNN-Ensemble to construct an ensemble of random decision graph neural networks whose capacity can be arbitrarily expanded for improvement in performance. The essence of the method is to build multiple GNNs in randomly selected substructures in the topo
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#32422;&#26463;&#28385;&#36275;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23618;&#32423;&#32467;&#26500;&#30340;&#25277;&#35937;&#23454;&#29616;&#20102;&#22312;&#32452;&#21512;&#29366;&#24577;&#19979;&#30340;&#29289;&#20307;&#37325;&#26032;&#25490;&#21015;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.11373</link><description>&lt;p&gt;
&#31070;&#32463;&#32422;&#26463;&#28385;&#36275;&#65306;&#23618;&#32423;&#25277;&#35937;&#23454;&#29616;&#32452;&#21512;&#24335;&#29289;&#20307;&#37325;&#26032;&#25490;&#21015;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Neural Constraint Satisfaction: Hierarchical Abstraction for Combinatorial Generalization in Object Rearrangement. (arXiv:2303.11373v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11373
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#32422;&#26463;&#28385;&#36275;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23618;&#32423;&#32467;&#26500;&#30340;&#25277;&#35937;&#23454;&#29616;&#20102;&#22312;&#32452;&#21512;&#29366;&#24577;&#19979;&#30340;&#29289;&#20307;&#37325;&#26032;&#25490;&#21015;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#20855;&#26377;&#23454;&#20307;&#26426;&#22120;&#20154;&#26234;&#33021;&#30340;&#38382;&#39064;&#32780;&#35328;&#65292;&#29289;&#20307;&#37325;&#26032;&#25490;&#21015;&#20219;&#21153;&#26159;&#19968;&#20010;&#24040;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#35299;&#20915;&#36825;&#20123;&#20219;&#21153;&#38656;&#35201;&#27867;&#21270;&#21040;&#26497;&#22823;&#32452;&#21512;&#30340;&#23454;&#20307;&#29366;&#24577;&#12290;&#26356;&#31967;&#31957;&#30340;&#26159;&#65292;&#36825;&#20123;&#23454;&#20307;&#30340;&#34920;&#31034;&#26159;&#26410;&#30693;&#30340;&#65292;&#24517;&#39035;&#20174;&#24863;&#23448;&#36755;&#20837;&#36827;&#34892;&#25512;&#26029;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23618;&#27425;&#25277;&#35937;&#30340;&#26041;&#27861;&#65292;&#20197;&#25581;&#31034;&#36825;&#20123;&#24213;&#23618;&#23454;&#20307;&#65292;&#24182;&#20174;&#38750;&#32467;&#26500;&#21270;&#35270;&#35273;&#36755;&#20837;&#20013;&#23454;&#29616;&#32452;&#21512;&#27867;&#21270;&#12290;&#36890;&#36807;&#22312;&#20687;&#32032;&#32858;&#31867;&#19978;&#26500;&#24314;&#19968;&#20010;&#22240;&#23376;&#21270;&#30340;&#36716;&#25442;&#22270;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23398;&#20064;&#20195;&#29702;&#27169;&#22411;&#20013;&#23454;&#20307;&#29366;&#24577;&#21644;&#29615;&#22659;&#29289;&#20307;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#31181;&#20851;&#31995;&#24320;&#21457;&#20102;&#19968;&#31181;&#25511;&#21046;&#26041;&#27861;&#65292;&#21487;&#27867;&#21270;&#21040;&#19981;&#21516;&#25968;&#37327;&#21644;&#37197;&#32622;&#30340;&#29289;&#20307;&#65292;&#24403;&#22312;&#27169;&#25311;&#37325;&#26032;&#25490;&#21015;&#20219;&#21153;&#19978;&#36827;&#34892;&#35780;&#20272;&#26102;&#65292;&#35813;&#26041;&#27861;&#32988;&#36807;&#24403;&#21069;&#30340;&#31163;&#32447;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Object rearrangement is a challenge for embodied agents because solving these tasks requires generalizing across a combinatorially large set of configurations of entities and their locations. Worse, the representations of these entities are unknown and must be inferred from sensory percepts. We present a hierarchical abstraction approach to uncover these underlying entities and achieve combinatorial generalization from unstructured visual inputs. By constructing a factorized transition graph over clusters of entity representations inferred from pixels, we show how to learn a correspondence between intervening on states of entities in the agent's model and acting on objects in the environment. We use this correspondence to develop a method for control that generalizes to different numbers and configurations of objects, which outperforms current offline deep RL methods when evaluated on simulated rearrangement tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;&#65292;iPSRL&#21644;iRLSVI&#65292;&#26088;&#22312;&#35299;&#20915;&#32473;&#23450;&#31163;&#32447;&#28436;&#31034;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36951;&#25022;&#65292;&#26725;&#25509;&#20102;&#22312;&#32447; RL &#21644;&#27169;&#20223;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2303.11369</link><description>&lt;p&gt;
&#12298;&#26725;&#25509;&#27169;&#20223;&#23398;&#20064;&#21644;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#65306;&#19968;&#20010;&#20048;&#35266;&#30340;&#25925;&#20107;&#12299;
&lt;/p&gt;
&lt;p&gt;
Bridging Imitation and Online Reinforcement Learning: An Optimistic Tale. (arXiv:2303.11369v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11369
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;&#65292;iPSRL&#21644;iRLSVI&#65292;&#26088;&#22312;&#35299;&#20915;&#32473;&#23450;&#31163;&#32447;&#28436;&#31034;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36951;&#25022;&#65292;&#26725;&#25509;&#20102;&#22312;&#32447; RL &#21644;&#27169;&#20223;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20197;&#19979;&#38382;&#39064;&#65306;&#32473;&#23450;&#19968;&#20010;&#26469;&#33258;&#19981;&#23436;&#32654;&#19987;&#23478;&#30340;&#31163;&#32447;&#28436;&#31034;&#25968;&#25454;&#38598;&#65292;&#26368;&#22909;&#30340;&#26041;&#24335;&#26159;&#20160;&#20040;&#26469;&#21033;&#29992;&#23427;&#26469;&#24341;&#23548; MDP &#20013;&#30340;&#22312;&#32447;&#23398;&#20064;&#34920;&#29616;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#24773;&#21518;&#39564;&#37319;&#26679;&#30340; RL&#65288;iPSRL&#65289;&#31639;&#27861;&#65292;&#23427;&#20351;&#29992;&#31163;&#32447;&#25968;&#25454;&#38598;&#21644;&#19987;&#23478;&#30340;&#34892;&#20026;&#31574;&#30053;&#20449;&#24687;&#26469;&#29983;&#25104;&#31163;&#32447;&#25968;&#25454;&#38598;&#12290;&#22914;&#26524;&#19987;&#23478;&#36275;&#22815;&#33021;&#24178;&#65292;&#21017;&#20854;&#32047;&#31215;&#36125;&#21494;&#26031;&#36951;&#25022;&#22312;&#31163;&#32447;&#25968;&#25454;&#38598;&#22823;&#23567; N &#19979;&#20250;&#25351;&#25968;&#24555;&#36895;&#19979;&#38477;&#21040;&#38646;&#12290;&#30001;&#20110;&#35813;&#31639;&#27861;&#35745;&#31639;&#26102;&#38388;&#22797;&#26434;&#24230;&#36807;&#39640;&#65292;&#25105;&#20204;&#38543;&#21518;&#25552;&#20986;&#20102; iRLSVI &#31639;&#27861;&#65292;&#21487;&#30475;&#20316;&#26159;&#22312;&#32447; RL &#21644;&#27169;&#20223;&#23398;&#20064;&#30340; RLSVI &#31639;&#27861;&#30340;&#32452;&#21512;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20004;&#20010;&#22522;&#20934;&#65288;&#27809;&#26377;&#31163;&#32447;&#25968;&#25454;&#65292;&#25110;&#20351;&#29992;&#31163;&#32447;&#25968;&#25454;&#38598;&#20294;&#19981;&#21033;&#29992;&#29983;&#25104;&#31574;&#30053;&#20449;&#24687;&#65289;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340; iRLSVI &#31639;&#27861;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#36951;&#25022;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#26725;&#25509;&#20102;&#22312;&#32447; RL &#21644;&#27169;&#20223;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we address the following problem: Given an offline demonstration dataset from an imperfect expert, what is the best way to leverage it to bootstrap online learning performance in MDPs. We first propose an Informed Posterior Sampling-based RL (iPSRL) algorithm that uses the offline dataset, and information about the expert's behavioral policy used to generate the offline dataset. Its cumulative Bayesian regret goes down to zero exponentially fast in N, the offline dataset size if the expert is competent enough. Since this algorithm is computationally impractical, we then propose the iRLSVI algorithm that can be seen as a combination of the RLSVI algorithm for online RL, and imitation learning. Our empirical results show that the proposed iRLSVI algorithm is able to achieve significant reduction in regret as compared to two baselines: no offline data, and offline dataset but used without information about the generative policy. Our algorithm bridges online RL and imitation
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986; Reflexion &#26041;&#27861;&#65292;&#32473;&#26234;&#33021;&#20307;&#36171;&#20104;&#20102;&#21160;&#24577;&#35760;&#24518;&#21644;&#33258;&#25105;&#21453;&#24605;&#33021;&#21147;&#65292;&#20197;&#22686;&#24378;&#20854;&#20219;&#21153;&#29305;&#23450;&#30340;&#34892;&#21160;&#36873;&#25321;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.11366</link><description>&lt;p&gt;
Reflexion&#65306;&#20855;&#26377;&#21160;&#24577;&#35760;&#24518;&#21644;&#33258;&#25105;&#21453;&#24605;&#30340;&#33258;&#20027;&#26234;&#33021;&#20307;
&lt;/p&gt;
&lt;p&gt;
Reflexion: an autonomous agent with dynamic memory and self-reflection. (arXiv:2303.11366v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11366
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986; Reflexion &#26041;&#27861;&#65292;&#32473;&#26234;&#33021;&#20307;&#36171;&#20104;&#20102;&#21160;&#24577;&#35760;&#24518;&#21644;&#33258;&#25105;&#21453;&#24605;&#33021;&#21147;&#65292;&#20197;&#22686;&#24378;&#20854;&#20219;&#21153;&#29305;&#23450;&#30340;&#34892;&#21160;&#36873;&#25321;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20915;&#31574;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20195;&#29702;&#30340;&#21457;&#23637;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#20869;&#37096;&#27169;&#22411;&#24494;&#35843;&#12289;&#22806;&#37096;&#27169;&#22411;&#24494;&#35843;&#25110;&#22312;&#23450;&#20041;&#30340;&#29366;&#24577;&#31354;&#38388;&#19978;&#36827;&#34892;&#31574;&#30053;&#20248;&#21270;&#12290;&#30001;&#20110;&#39640;&#36136;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#25110;&#32570;&#20047;&#33391;&#22909;&#23450;&#20041;&#30340;&#29366;&#24577;&#31354;&#38388;&#65292;&#23454;&#29616;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#20250;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#20195;&#29702;&#27809;&#26377;&#20154;&#31867;&#20915;&#31574;&#36807;&#31243;&#22266;&#26377;&#30340;&#26576;&#20123;&#21697;&#36136;&#65292;&#29305;&#21035;&#26159;&#20174;&#38169;&#35823;&#20013;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#21453;&#24605;&#65292;&#20154;&#31867;&#21487;&#20197;&#36890;&#36807;&#35797;&#38169;&#36807;&#31243;&#39640;&#25928;&#22320;&#35299;&#20915;&#26032;&#30340;&#38382;&#39064;&#12290;&#22312;&#26368;&#36817;&#30340;&#30740;&#31350;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986; Reflexion&#65292;&#19968;&#31181;&#23558;&#21160;&#24577;&#35760;&#24518;&#21644;&#33258;&#25105;&#21453;&#24605;&#33021;&#21147;&#36171;&#20104;&#26234;&#33021;&#20307;&#30340;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#20854;&#29616;&#26377;&#30340;&#25512;&#29702;&#36712;&#36857;&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;&#34892;&#21160;&#36873;&#25321;&#33021;&#21147;&#12290;&#20026;&#20102;&#23454;&#29616;&#23436;&#20840;&#33258;&#21160;&#21270;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in decision-making large language model (LLM) agents have demonstrated impressive performance across various benchmarks. However, these state-of-the-art approaches typically necessitate internal model fine-tuning, external model fine-tuning, or policy optimization over a defined state space. Implementing these methods can prove challenging due to the scarcity of high-quality training data or the lack of well-defined state space. Moreover, these agents do not possess certain qualities inherent to human decision-making processes, specifically the ability to learn from mistakes. Self-reflection allows humans to efficiently solve novel problems through a process of trial and error. Building on recent research, we propose Reflexion, an approach that endows an agent with dynamic memory and self-reflection capabilities to enhance its existing reasoning trace and task-specific action choice abilities. To achieve full automation, we introduce a straightforward yet effective 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#39640;&#32500;Transformer&#30340;&#26550;&#26500;HDformer&#65292;&#24182;&#21033;&#29992;&#38271;&#36317;&#31163;PPG&#20449;&#21495;&#36827;&#34892;&#31958;&#23615;&#30149;&#26816;&#27979;&#65292;&#20854;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27880;&#24847;&#21147;&#27169;&#22359;TSA&#65292;&#25104;&#21151;&#23558;&#26631;&#35760;&#20307;&#31215;&#20943;&#23569;10&#20493;&#20197;&#19978;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#33021;&#21147;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.11340</link><description>&lt;p&gt;
HDformer: &#19968;&#31181;&#21033;&#29992;&#38271;&#36317;&#31163;&#34880;&#31649;&#20449;&#21495;&#36827;&#34892;&#31958;&#23615;&#30149;&#26816;&#27979;&#30340;&#39640;&#32500;Transformer
&lt;/p&gt;
&lt;p&gt;
HDformer: A Higher Dimensional Transformer for Diabetes Detection Utilizing Long Range Vascular Signals. (arXiv:2303.11340v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11340
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#39640;&#32500;Transformer&#30340;&#26550;&#26500;HDformer&#65292;&#24182;&#21033;&#29992;&#38271;&#36317;&#31163;PPG&#20449;&#21495;&#36827;&#34892;&#31958;&#23615;&#30149;&#26816;&#27979;&#65292;&#20854;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27880;&#24847;&#21147;&#27169;&#22359;TSA&#65292;&#25104;&#21151;&#23558;&#26631;&#35760;&#20307;&#31215;&#20943;&#23569;10&#20493;&#20197;&#19978;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#33021;&#21147;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31958;&#23615;&#30149;&#26159;&#20840;&#29699;&#24615;&#38382;&#39064;&#65292;&#26089;&#26399;&#26816;&#27979;&#26377;&#21161;&#20110;&#39044;&#38450;&#20005;&#37325;&#24182;&#21457;&#30151;&#12290;&#24050;&#20986;&#29616;&#23558;&#24515;&#34880;&#31649;&#20449;&#21495;&#32435;&#20837;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#20302;&#25104;&#26412;&#12289;&#38750;&#20405;&#20837;&#24335;&#26816;&#27979;&#26041;&#27861;&#65292;&#20294;&#38480;&#21046;&#20854;&#20020;&#24202;&#24212;&#29992;&#30340;&#26159;&#26377;&#38480;&#30340;&#20934;&#30830;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#65292;&#21363;Higher Dimensional Transformer&#65288;HDformer&#65289;&#65292;&#23427;&#21033;&#29992;&#38271;&#36317;&#31163;&#20809;&#30005;&#23481;&#31215;&#22270;&#65288;PPG&#65289;&#20449;&#21495;&#26469;&#26816;&#27979;&#31958;&#23615;&#30149;&#12290;&#30456;&#36739;&#20110;&#29616;&#26377;&#30740;&#31350;&#24120;&#29992;&#30340;&#19981;&#36275;&#19968;&#20998;&#38047;&#30340;PPG&#20449;&#21495;&#65292;&#38271;&#36317;&#31163;PPG&#21253;&#21547;&#26356;&#24191;&#27867;&#12289;&#26356;&#28145;&#20837;&#30340;&#20449;&#21495;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#20026;&#20102;&#22686;&#21152;&#22788;&#29702;&#38271;&#36317;&#31163;&#25968;&#25454;&#30340;&#33021;&#21147;&#21644;&#25928;&#29575;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27880;&#24847;&#21147;&#27169;&#22359;Time Square Attention&#65288;TSA&#65289;&#65292;&#23558;&#26631;&#35760;&#20307;&#31215;&#20943;&#23569;10&#20493;&#20197;&#19978;&#65292;&#21516;&#26102;&#20445;&#30041;&#26412;&#22320;/&#20840;&#23616;&#20381;&#36182;&#20851;&#31995;&#12290;&#23427;&#23558;&#19968;&#32500;&#36755;&#20837; &#36716;&#25442;&#20026;&#20108;&#32500;&#34920;&#31034;&#65292;&#24182;&#23558;&#30456;&#37051;&#28857;&#32452;&#25104;&#19968;&#20010;&#21333;&#29420;&#30340;2D&#26631;&#35760;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diabetes mellitus is a worldwide concern, and early detection can help to prevent serious complications. Low-cost, non-invasive detection methods, which take cardiovascular signals into deep learning models, have emerged. However, limited accuracy constrains their clinical usage. In this paper, we present a new Transformer-based architecture, Higher Dimensional Transformer (HDformer), which takes long-range photoplethysmography (PPG) signals to detect diabetes. The long-range PPG contains broader and deeper signal contextual information compared to the less-than-one-minute PPG signals commonly utilized in existing research. To increase the capability and efficiency of processing the long range data, we propose a new attention module Time Square Attention (TSA), reducing the volume of the tokens by more than 10x, while retaining the local/global dependencies. It converts the 1-dimensional inputs into 2-dimensional representations and groups adjacent points into a single 2D token, using 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#32852;&#37030;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550; FedMAE&#65292;&#21487;&#20197;&#21033;&#29992;&#36731;&#37327;&#32423;&#35774;&#22791;&#19978;&#30340;&#22823;&#35268;&#27169;&#26410;&#26631;&#35760;&#22270;&#20687;&#36827;&#34892;&#32852;&#37030;&#23398;&#20064;&#12290;FedMAE&#21487;&#20197;&#39044;&#35757;&#32451;&#19968;&#20010;&#21333;&#22359;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;&#65292;&#24182;&#23558;&#22810;&#20010;&#39044;&#35757;&#32451;&#30340;&#21333;&#22359;MAE&#32423;&#32852;&#22312;&#26381;&#21153;&#22120;&#19978;&#26500;&#24314;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#22810;&#22359;ViT&#39592;&#24178;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;FedMAE&#30456;&#36739;&#20110;&#26368;&#20808;&#36827;&#30340;FSSL&#26041;&#27861;&#20855;&#26377;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.11339</link><description>&lt;p&gt;
FedMAE&#65306;&#24102;&#26377;&#21333;&#22359;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;&#30340;&#32852;&#37030;&#33258;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FedMAE: Federated Self-Supervised Learning with One-Block Masked Auto-Encoder. (arXiv:2303.11339v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11339
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#32852;&#37030;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550; FedMAE&#65292;&#21487;&#20197;&#21033;&#29992;&#36731;&#37327;&#32423;&#35774;&#22791;&#19978;&#30340;&#22823;&#35268;&#27169;&#26410;&#26631;&#35760;&#22270;&#20687;&#36827;&#34892;&#32852;&#37030;&#23398;&#20064;&#12290;FedMAE&#21487;&#20197;&#39044;&#35757;&#32451;&#19968;&#20010;&#21333;&#22359;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;&#65292;&#24182;&#23558;&#22810;&#20010;&#39044;&#35757;&#32451;&#30340;&#21333;&#22359;MAE&#32423;&#32852;&#22312;&#26381;&#21153;&#22120;&#19978;&#26500;&#24314;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#22810;&#22359;ViT&#39592;&#24178;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;FedMAE&#30456;&#36739;&#20110;&#26368;&#20808;&#36827;&#30340;FSSL&#26041;&#27861;&#20855;&#26377;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#24320;&#22987;&#20851;&#27880;&#22914;&#20309;&#21033;&#29992;&#29992;&#25143;&#35774;&#22791;&#20013;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#21407;&#22240;&#26159;&#29992;&#25143;&#20851;&#27880;&#38544;&#31169;&#65292;&#25104;&#26412;&#39640;&#65292;&#25110;&#32773;&#32570;&#20047;&#19987;&#19994;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;Federated Semi-Supervised/Self-Supervised Learning&#65288;FSSL&#65289;&#26041;&#27861;&#30001;&#20110;&#26412;&#22320;&#23458;&#25143;&#31471;&#30340;&#26377;&#38480;&#35745;&#31639;&#36164;&#28304;&#32780;&#26080;&#27861;&#23398;&#20064;&#22823;&#35268;&#27169;&#22270;&#20687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;FedMAE&#65292;&#21363;Federated Masked AutoEncoder&#65292;&#20197;&#35299;&#20915;&#22914;&#20309;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#22823;&#23610;&#24230;&#22270;&#20687;&#36827;&#34892;&#32852;&#37030;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;FedMAE&#21487;&#20197;&#20351;&#29992;&#36731;&#37327;&#32423;&#23458;&#25143;&#31471;&#35774;&#22791;&#20013;&#30340;&#22823;&#22411;&#22270;&#20687;&#39044;&#35757;&#32451;&#21333;&#22359;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;&#65288;MAE&#65289;&#65292;&#28982;&#21518;&#22312;&#26381;&#21153;&#22120;&#20013;&#32423;&#32852;&#22810;&#20010;&#39044;&#35757;&#32451;&#30340;&#21333;&#22359;MAE&#20197;&#26500;&#24314;&#19979;&#28216;&#20219;&#21153;&#30340;&#22810;&#22359;ViT&#39592;&#24178;&#12290; &#22270;&#20687;&#37325;&#24314;&#21644;&#20998;&#31867;&#30340;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;FSSL&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;FedMAE&#33719;&#24471;&#20102;&#26356;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Latest federated learning (FL) methods started to focus on how to use unlabeled data in clients for training due to users' privacy concerns, high labeling costs, or lack of expertise. However, current Federated Semi-Supervised/Self-Supervised Learning (FSSL) approaches fail to learn large-scale images because of the limited computing resources of local clients. In this paper, we introduce a new framework FedMAE, which stands for Federated Masked AutoEncoder, to address the problem of how to utilize unlabeled large-scale images for FL. Specifically, FedMAE can pre-train one-block Masked AutoEncoder (MAE) using large images in lightweight client devices, and then cascades multiple pre-trained one-block MAEs in the server to build a multi-block ViT backbone for downstream tasks. Theoretical analysis and experimental results on image reconstruction and classification show that our FedMAE achieves superior performance compared to the state-of-the-art FSSL methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36882;&#24402;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;&#35745;&#31639;&#30340;&#40065;&#26834;&#32858;&#21512;&#26041;&#27861;&#26469;&#38450;&#24481;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#24694;&#24847;&#25915;&#20987;&#65292;&#35813;&#26041;&#27861;&#20998;&#37197;&#26435;&#37325;&#20197;&#26368;&#23567;&#21270;&#25968;&#25454;&#27745;&#26579;&#25928;&#24212;&#65292;&#23454;&#39564;&#34920;&#26126;&#20854;&#31934;&#24230;&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#24182;&#19988;&#26102;&#38388;&#22797;&#26434;&#24230;&#38477;&#20302;&#12290;</title><link>http://arxiv.org/abs/2303.11337</link><description>&lt;p&gt;
&#36882;&#24402;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;&#22522;&#20110;&#40065;&#26834;&#32858;&#21512;&#25216;&#26415;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Recursive Euclidean Distance Based Robust Aggregation Technique For Federated Learning. (arXiv:2303.11337v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11337
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36882;&#24402;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;&#35745;&#31639;&#30340;&#40065;&#26834;&#32858;&#21512;&#26041;&#27861;&#26469;&#38450;&#24481;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#24694;&#24847;&#25915;&#20987;&#65292;&#35813;&#26041;&#27861;&#20998;&#37197;&#26435;&#37325;&#20197;&#26368;&#23567;&#21270;&#25968;&#25454;&#27745;&#26579;&#25928;&#24212;&#65292;&#23454;&#39564;&#34920;&#26126;&#20854;&#31934;&#24230;&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#24182;&#19988;&#26102;&#38388;&#22797;&#26434;&#24230;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#20316;&#20026;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#20013;&#25968;&#25454;&#21487;&#29992;&#24615;&#21644;&#38544;&#31169;&#25361;&#25112;&#30340;&#35299;&#20915;&#26041;&#26696;&#24050;&#32463;&#21464;&#24471;&#38750;&#24120;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#65292;&#26412;&#22320;&#27169;&#22411;&#26356;&#26032;&#30340;&#32858;&#21512;&#36807;&#31243;&#23481;&#26131;&#21463;&#21040;&#24694;&#24847;&#25915;&#20987;&#65292;&#22914;&#32972;&#21253;&#25915;&#20987;&#12289;&#26631;&#31614;&#32763;&#36716;&#21644;&#25104;&#21592;&#25512;&#26029;&#12290;&#24694;&#24847;&#29992;&#25143;&#26088;&#22312;&#36890;&#36807;&#24694;&#24847;&#25968;&#25454;&#35757;&#32451;&#26412;&#22320;&#27169;&#22411;&#26469;&#30772;&#22351;&#21512;&#20316;&#23398;&#20064;&#36807;&#31243;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36882;&#24402;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;&#35745;&#31639;&#30340;&#26032;&#22411;&#40065;&#26834;&#32858;&#21512;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#27979;&#37327;&#26412;&#22320;&#27169;&#22411;&#19982;&#20043;&#21069;&#20840;&#23616;&#27169;&#22411;&#30340;&#36317;&#31163;&#65292;&#24182;&#30456;&#24212;&#22320;&#20998;&#37197;&#26435;&#37325;&#12290;&#36828;&#31163;&#20840;&#23616;&#27169;&#22411;&#30340;&#26412;&#22320;&#27169;&#22411;&#34987;&#20998;&#37197;&#36739;&#23567;&#30340;&#26435;&#37325;&#65292;&#20197;&#26368;&#23567;&#21270;&#32858;&#21512;&#36807;&#31243;&#20013;&#30340;&#25968;&#25454;&#27745;&#26579;&#25928;&#24212;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#31934;&#24230;&#26041;&#38754;&#30340;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#65292;&#19988;&#26102;&#38388;&#22797;&#26434;&#24230;&#20943;&#23569;&#19981;&#36229;&#36807;$55\%$&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26377;&#20004;&#20010;&#65306;1&#65289;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36882;&#24402;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;&#32858;&#21512;&#25216;&#26415;&#65292;&#20197;&#38450;&#33539;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#24694;&#24847;&#25915;&#20987;&#12290;2&#65289;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#31639;&#27861;&#30340;&#31934;&#24230;&#21644;&#26102;&#38388;&#22797;&#26434;&#24230;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning has gained popularity as a solution to data availability and privacy challenges in machine learning. However, the aggregation process of local model updates to obtain a global model in federated learning is susceptible to malicious attacks, such as backdoor poisoning, label-flipping, and membership inference. Malicious users aim to sabotage the collaborative learning process by training the local model with malicious data. In this paper, we propose a novel robust aggregation approach based on recursive Euclidean distance calculation. Our approach measures the distance of the local models from the previous global model and assigns weights accordingly. Local models far away from the global model are assigned smaller weights to minimize the data poisoning effect during aggregation. Our experiments demonstrate that the proposed algorithm outperforms state-of-the-art algorithms by at least $5\%$ in accuracy while reducing time complexity by less than $55\%$. Our contribut
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31995;&#32479;Touch Dexterity&#65292;&#36890;&#36807;&#23494;&#38598;&#20108;&#36827;&#21046;&#21147;&#20256;&#24863;&#22120;&#23454;&#29616;&#20102;&#22810;&#25351;&#26426;&#22120;&#20154;&#25163;&#19981;&#30475;&#23601;&#33021;&#26059;&#36716;&#29289;&#20307;&#65292;&#21516;&#26102;&#22823;&#22823;&#38477;&#20302;&#20102;&#25104;&#26412;&#21644;&#19982;&#23454;&#38469;&#24212;&#29992;&#30340;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2303.10880</link><description>&lt;p&gt;
&#19981;&#30475;&#23601;&#33021;&#26059;&#36716;: &#36890;&#36807;&#35302;&#35273;&#23454;&#29616;&#25163;&#37096;&#28789;&#27963;&#24615;
&lt;/p&gt;
&lt;p&gt;
Rotating without Seeing: Towards In-hand Dexterity through Touch. (arXiv:2303.10880v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10880
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31995;&#32479;Touch Dexterity&#65292;&#36890;&#36807;&#23494;&#38598;&#20108;&#36827;&#21046;&#21147;&#20256;&#24863;&#22120;&#23454;&#29616;&#20102;&#22810;&#25351;&#26426;&#22120;&#20154;&#25163;&#19981;&#30475;&#23601;&#33021;&#26059;&#36716;&#29289;&#20307;&#65292;&#21516;&#26102;&#22823;&#22823;&#38477;&#20302;&#20102;&#25104;&#26412;&#21644;&#19982;&#23454;&#38469;&#24212;&#29992;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35302;&#24863;&#20449;&#24687;&#22312;&#20154;&#31867;&#28789;&#24039;&#24615;&#20013;&#25198;&#28436;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#35282;&#33394;&#65292;&#23427;&#21487;&#20197;&#25552;&#20379;&#26377;&#29992;&#30340;&#25509;&#35302;&#20449;&#24687;&#65292;&#30452;&#25509;&#20174;&#35270;&#35273;&#20013;&#26080;&#27861;&#25512;&#26029;&#12290;&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#26159;&#21542;&#33021;&#22815;&#20351;&#22810;&#25351;&#26426;&#22120;&#20154;&#25163;&#20855;&#22791;&#19982;&#20154;&#31867;&#31867;&#20284;&#30340;&#19981;&#30475;&#23601;&#33021;&#26059;&#36716;&#29289;&#20307;&#30340;&#33021;&#21147;&#12290;&#20316;&#32773;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31995;&#32479;Touch Dexterity&#65292;&#36890;&#36807;&#20351;&#29992;&#35206;&#30422;&#25972;&#20010;&#26426;&#22120;&#20154;&#25163;&#30340;&#23494;&#38598;&#20108;&#36827;&#21046;&#21147;&#20256;&#24863;&#22120;&#65288;&#35302;&#25720;&#25110;&#26410;&#35302;&#25720;&#65289;&#20195;&#26367;&#20165;&#20165;&#22312;&#23567;&#21306;&#22495;&#20869;&#36827;&#34892;&#31934;&#20934;&#30340;&#35302;&#35273;&#20256;&#24863;&#65292;&#20351;&#31995;&#32479;&#20855;&#26377;&#20302;&#25104;&#26412;&#12289;&#35206;&#30422;&#33539;&#22260;&#24191;&#31561;&#20248;&#28857;&#65292;&#24182;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#22312;&#22810;&#26679;&#30340;&#29289;&#20307;&#27169;&#25311;&#20013;&#35757;&#32451;&#20986;&#20102;&#19968;&#31181;&#35302;&#24863;&#26059;&#36716;&#31574;&#30053;&#65292;&#33021;&#22815;&#22312;&#30495;&#23454;&#30340;&#26426;&#22120;&#20154;&#25163;&#19978;&#30452;&#25509;&#23454;&#26045;&#19981;&#30475;&#23601;&#33021;&#26059;&#36716;&#26032;&#22411;&#29289;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tactile information plays a critical role in human dexterity. It reveals useful contact information that may not be inferred directly from vision. In fact, humans can even perform in-hand dexterous manipulation without using vision. Can we enable the same ability for the multi-finger robot hand? In this paper, we present Touch Dexterity, a new system that can perform in-hand object rotation using only touching without seeing the object. Instead of relying on precise tactile sensing in a small region, we introduce a new system design using dense binary force sensors (touch or no touch) overlaying one side of the whole robot hand (palm, finger links, fingertips). Such a design is low-cost, giving a larger coverage of the object, and minimizing the Sim2Real gap at the same time. We train an in-hand rotation policy using Reinforcement Learning on diverse objects in simulation. Relying on touch-only sensing, we can directly deploy the policy in a real robot hand and rotate novel objects tha
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20648;&#22791;&#33410;&#28857;&#30340;&#31070;&#32463;&#20803;&#35270;&#35273;&#24863;&#30693;&#32593;&#32476;&#65288;RN-Net&#65289;&#12290;RN-Net&#21487;&#20197;&#20197;&#20302;&#25104;&#26412;&#26377;&#25928;&#22320;&#22788;&#29702;&#24322;&#27493;&#30340;&#26102;&#38388;&#29305;&#24449;&#65292;&#24182;&#22312;DVS128&#25163;&#21183;&#19978;&#23454;&#29616;&#20102;&#36804;&#20170;&#26368;&#39640;&#30340;&#20934;&#30830;&#29575;99.2&#65285;&#65292;&#22312;&#26356;&#23567;&#30340;&#32593;&#32476;&#23610;&#23544;&#19979;&#23454;&#29616;&#20102;DVS Lip&#25968;&#25454;&#38598;&#30340;67.5&#65285;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.10770</link><description>&lt;p&gt;
RN-Net: &#22522;&#20110;&#20648;&#22791;&#33410;&#28857;&#30340;&#31070;&#32463;&#20803;&#35270;&#35273;&#24863;&#30693;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
RN-Net: Reservoir Nodes-Enabled Neuromorphic Vision Sensing Network. (arXiv:2303.10770v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10770
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20648;&#22791;&#33410;&#28857;&#30340;&#31070;&#32463;&#20803;&#35270;&#35273;&#24863;&#30693;&#32593;&#32476;&#65288;RN-Net&#65289;&#12290;RN-Net&#21487;&#20197;&#20197;&#20302;&#25104;&#26412;&#26377;&#25928;&#22320;&#22788;&#29702;&#24322;&#27493;&#30340;&#26102;&#38388;&#29305;&#24449;&#65292;&#24182;&#22312;DVS128&#25163;&#21183;&#19978;&#23454;&#29616;&#20102;&#36804;&#20170;&#26368;&#39640;&#30340;&#20934;&#30830;&#29575;99.2&#65285;&#65292;&#22312;&#26356;&#23567;&#30340;&#32593;&#32476;&#23610;&#23544;&#19979;&#23454;&#29616;&#20102;DVS Lip&#25968;&#25454;&#38598;&#30340;67.5&#65285;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#30456;&#26426;&#21463;&#29983;&#29289;&#35270;&#35273;&#31995;&#32479;&#31232;&#30095;&#21644;&#24322;&#27493;&#33033;&#20914;&#34920;&#31034;&#30340;&#21551;&#21457;&#12290;&#28982;&#32780;&#65292;&#22788;&#29702;&#20107;&#20214;&#25968;&#25454;&#35201;&#20040;&#38656;&#35201;&#20351;&#29992;&#26114;&#36149;&#30340;&#29305;&#24449;&#25551;&#36848;&#31526;&#23558;&#33033;&#20914;&#36716;&#25442;&#25104;&#24103;&#65292;&#35201;&#20040;&#20351;&#29992;&#38590;&#20197;&#35757;&#32451;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31616;&#21333;&#21367;&#31215;&#23618;&#21644;&#21160;&#24577;&#26102;&#38388;&#32534;&#30721;&#20648;&#22791;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#20855;&#26377;&#20302;&#30828;&#20214;&#21644;&#35757;&#32451;&#25104;&#26412;&#12290;&#20351;&#29992;&#20648;&#22791;&#33410;&#28857;&#30340;&#31070;&#32463;&#20803;&#35270;&#35273;&#24863;&#30693;&#32593;&#32476;&#65288;RN-Net&#65289;&#20351;&#32593;&#32476;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#24322;&#27493;&#30340;&#26102;&#38388;&#29305;&#24449;&#65292;&#24182;&#23454;&#29616;&#20102;DVS128&#25163;&#21183;&#30340;&#36804;&#20170;&#26368;&#39640;&#20934;&#30830;&#24230;99.2&#65285;&#65292;&#21516;&#26102;&#22312;&#26356;&#23567;&#30340;&#32593;&#32476;&#23610;&#23544;&#19979;&#23454;&#29616;&#20102;DVS Lip&#25968;&#25454;&#38598;&#30340;67.5&#65285;&#20934;&#30830;&#24230;&#12290;&#36890;&#36807;&#21033;&#29992;&#35760;&#24518;&#30005;&#38459;&#22120;&#30340;&#20869;&#37096;&#21160;&#24577;&#65292;&#21487;&#20197;&#20197;&#38750;&#24120;&#20302;&#30340;&#30828;&#20214;&#25104;&#26412;&#23454;&#29616;&#24322;&#27493;&#26102;&#38388;&#29305;&#24449;&#32534;&#30721;&#65292;&#32780;&#19981;&#38656;&#35201;&#39044;&#22788;&#29702;&#25110;&#19987;&#29992;&#30340;&#23384;&#20648;&#22120;&#21644;&#31639;&#26415;&#21333;&#20803;&#12290;
&lt;/p&gt;
&lt;p&gt;
Event-based cameras are inspired by the sparse and asynchronous spike representation of the biological visual system. However, processing the even data requires either using expensive feature descriptors to transform spikes into frames, or using spiking neural networks that are difficult to train. In this work, we propose a neural network architecture based on simple convolution layers integrated with dynamic temporal encoding reservoirs with low hardware and training costs. The Reservoir Nodes-enabled neuromorphic vision sensing Network (RN-Net) allows the network to efficiently process asynchronous temporal features, and achieves the highest accuracy of 99.2% for DVS128 Gesture reported to date, and one of the highest accuracy of 67.5% for DVS Lip dataset at a much smaller network size. By leveraging the internal dynamics of memristors, asynchronous temporal feature encoding can be implemented at very low hardware cost without preprocessing or dedicated memory and arithmetic units. T
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34701;&#21512;&#22270;&#20687;&#22330;&#26223;&#22270;&#30340;&#35270;&#35273;&#20851;&#31995;&#20449;&#24687;&#19982;&#22270;&#20687;&#30340;&#31354;&#38388;&#29305;&#24449;&#26144;&#23556;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#24341;&#20837;&#22810;&#27169;&#24577;&#22870;&#21169;&#20989;&#25968;&#36827;&#34892;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65292;&#20197;&#20248;&#21270;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#23458;&#35266;&#21644;&#20027;&#35266;&#35780;&#20272;&#25351;&#26631;&#19978;&#22343;&#20248;&#20110;&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2303.10766</link><description>&lt;p&gt;
&#22522;&#20110;&#35270;&#35273;&#20851;&#31995;&#30340;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#20013;&#30340;&#22810;&#27169;&#24577;&#22870;&#21169;
&lt;/p&gt;
&lt;p&gt;
Multi-modal reward for visual relationships-based image captioning. (arXiv:2303.10766v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10766
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34701;&#21512;&#22270;&#20687;&#22330;&#26223;&#22270;&#30340;&#35270;&#35273;&#20851;&#31995;&#20449;&#24687;&#19982;&#22270;&#20687;&#30340;&#31354;&#38388;&#29305;&#24449;&#26144;&#23556;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#24341;&#20837;&#22810;&#27169;&#24577;&#22870;&#21169;&#20989;&#25968;&#36827;&#34892;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65292;&#20197;&#20248;&#21270;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#23458;&#35266;&#21644;&#20027;&#35266;&#35780;&#20272;&#25351;&#26631;&#19978;&#22343;&#20248;&#20110;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#33258;&#21160;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#20013;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#65292;&#20854;&#26377;&#25928;&#30340;&#34920;&#31034;&#23398;&#20064;&#21644;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#20869;&#23481;&#29983;&#25104;&#33021;&#21147;&#26159;&#20851;&#38190;&#21407;&#22240;&#12290;&#28982;&#32780;&#65292;&#34429;&#28982; bottom-up &#29305;&#24449;&#26159;&#35768;&#22810;&#26368;&#36817;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#30340;&#28145;&#24230;&#21151;&#33021;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#36825;&#20123;&#29305;&#24449;&#19982;&#20174;&#21407;&#22987;&#22270;&#20687;&#30452;&#25509;&#25552;&#21462;&#30340;&#29305;&#24449;&#22270;&#30456;&#27604;&#25552;&#20379;&#20102;&#19981;&#21516;&#23545;&#35937;&#30340;&#35814;&#32454;&#34920;&#31034;&#65292;&#20294;&#26159;&#23427;&#20204;&#20851;&#20110;&#36825;&#20123;&#23545;&#35937;&#20043;&#38388;&#39640;&#23618;&#27425;&#35821;&#20041;&#20449;&#24687;&#30340;&#32570;&#20047;&#26159;&#23427;&#20204;&#30340;&#37325;&#35201;&#32570;&#28857;&#65292;&#23613;&#31649;&#20854;&#25552;&#21462;&#36807;&#31243;&#32791;&#26102;&#19988;&#38656;&#32791;&#36153;&#22823;&#37327;&#36164;&#28304;&#12290;&#20026;&#20102;&#21033;&#29992;&#35270;&#35273;&#20851;&#31995;&#22312;&#23383;&#24149;&#29983;&#25104;&#20013;&#30340;&#20248;&#21183;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#23427;&#22522;&#20110;&#25552;&#21462;&#30340;&#22270;&#20687;&#22330;&#26223;&#22270;&#30340;&#35270;&#35273;&#20851;&#31995;&#20449;&#24687;&#19982;&#22270;&#20687;&#30340;&#31354;&#38388;&#29305;&#24449;&#26144;&#23556;&#36827;&#34892;&#34701;&#21512;&#65292;&#28982;&#21518;&#24341;&#20837;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#22870;&#21169;&#20989;&#25968;&#26469;&#36827;&#34892;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65292;&#20197;&#20248;&#21270;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#23458;&#35266;&#21644;&#20027;&#35266;&#35780;&#20272;&#25351;&#26631;&#19978;&#22343;&#20248;&#20110;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks have achieved promising results in automatic image captioning due to their effective representation learning and context-based content generation capabilities. As a prominent type of deep features used in many of the recent image captioning methods, the well-known bottomup features provide a detailed representation of different objects of the image in comparison with the feature maps directly extracted from the raw image. However, the lack of high-level semantic information about the relationships between these objects is an important drawback of bottom-up features, despite their expensive and resource-demanding extraction procedure. To take advantage of visual relationships in caption generation, this paper proposes a deep neural network architecture for image captioning based on fusing the visual relationships information extracted from an image's scene graph with the spatial feature maps of the image. A multi-modal reward function is then introduced for deep rei
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#29616;&#26377;&#30340;&#33258;&#21160;&#39550;&#39542;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#65292;&#21253;&#25324;&#31649;&#36947;&#35268;&#21010;&#21644;&#31471;&#21040;&#31471;&#35268;&#21010;&#26041;&#27861;&#12290;&#22312;&#25361;&#25112;&#21644;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#26041;&#38754;&#25552;&#20379;&#20102;&#35752;&#35770;&#65292;&#26377;&#21161;&#20110;&#20026;&#26234;&#33021;&#27773;&#36710;&#30340;&#21457;&#23637;&#25552;&#20379;&#26356;&#22909;&#30340;&#35268;&#21010;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.09824</link><description>&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#36335;&#24452;&#35268;&#21010;&#65306;&#29616;&#29366;&#19982;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Path Planning for Autonomous Driving: The State of the Art and Perspectives. (arXiv:2303.09824v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09824
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#29616;&#26377;&#30340;&#33258;&#21160;&#39550;&#39542;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#65292;&#21253;&#25324;&#31649;&#36947;&#35268;&#21010;&#21644;&#31471;&#21040;&#31471;&#35268;&#21010;&#26041;&#27861;&#12290;&#22312;&#25361;&#25112;&#21644;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#26041;&#38754;&#25552;&#20379;&#20102;&#35752;&#35770;&#65292;&#26377;&#21161;&#20110;&#20026;&#26234;&#33021;&#27773;&#36710;&#30340;&#21457;&#23637;&#25552;&#20379;&#26356;&#22909;&#30340;&#35268;&#21010;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#27773;&#36710;&#30001;&#20110;&#25552;&#39640;&#30340;&#20415;&#21033;&#24615;&#12289;&#23433;&#20840;&#24615;&#21644;&#28508;&#22312;&#30340;&#21830;&#19994;&#20215;&#20540;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#20294;&#30001;&#20110;&#21508;&#31181;&#38382;&#39064;&#65292;&#22914;&#23433;&#20840;&#24615;&#12289;&#21487;&#38752;&#24615;&#21644;&#35268;&#21010;&#26041;&#27861;&#30340;&#27867;&#21270;&#31561;&#38480;&#21046;&#65292;&#23427;&#20204;&#30340;&#37096;&#32626;&#20173;&#23616;&#38480;&#20110;&#23567;&#35268;&#27169;&#39564;&#35777;&#38454;&#27573;&#12290;&#26412;&#25991;&#26088;&#22312;&#32508;&#36848;&#26368;&#20808;&#36827;&#30340;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#65292;&#21253;&#25324;&#31649;&#36947;&#35268;&#21010;&#21644;&#31471;&#21040;&#31471;&#35268;&#21010;&#26041;&#27861;&#12290;&#38024;&#23545;&#31649;&#36947;&#26041;&#27861;&#65292;&#26412;&#25991;&#25552;&#20379;&#20102;&#36873;&#21462;&#31639;&#27861;&#30340;&#27010;&#36848;&#65292;&#24182;&#35752;&#35770;&#20102;&#25193;&#23637;&#21644;&#20248;&#21270;&#26426;&#21046;&#65307;&#38024;&#23545;&#31471;&#21040;&#31471;&#26041;&#27861;&#65292;&#26412;&#25991;&#24378;&#35843;&#22521;&#35757;&#21644;&#39564;&#35777;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#35752;&#35770;&#20102;&#25361;&#25112;&#21644;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#65292;&#36825;&#26377;&#21161;&#20110;&#20026;&#26234;&#33021;&#27773;&#36710;&#30340;&#21457;&#23637;&#25552;&#20379;&#26356;&#22909;&#30340;&#35268;&#21010;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intelligent vehicles (IVs) have attracted wide attention thanks to the augmented convenience, safety advantages, and potential commercial value. Although a few of autonomous driving unicorns assert that IVs will be commercially deployable by 2025, their deployment is still restricted to small-scale validation due to various issues, among which safety, reliability, and generalization of planning methods are prominent concerns. Precise computation of control commands or trajectories by planning methods remains a prerequisite for IVs, owing to perceptual imperfections under complex environments, which pose an obstacle to the successful commercialization of IVs. This paper aims to review state-of-the-art planning methods, including pipeline planning and end-to-end planning methods. In terms of pipeline methods, a survey of selecting algorithms is provided along with a discussion of the expansion and optimization mechanisms, whereas in end-to-end methods, the training approaches and verific
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#30693;&#35782;&#31070;&#32463;&#32593;&#32476;&#30340;&#25299;&#25169;&#20248;&#21270;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#26080;&#20808;&#39564;&#30693;&#35782;&#30340;&#20960;&#20309;&#32467;&#26500;&#26816;&#27979;&#65292;&#36890;&#36807;&#26448;&#26009;&#23494;&#24230;&#22330;&#34920;&#31034;&#20219;&#24847;&#35299;&#20915;&#26041;&#26696;&#25299;&#25169;&#65292;&#24182;&#36890;&#36807;Eikonal&#27491;&#21017;&#21270;&#23454;&#29616;&#12290;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#21307;&#30103;&#21644;&#24037;&#19994;&#24212;&#29992;&#20013;&#30340;&#38750;&#20405;&#20837;&#24335;&#25104;&#20687;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2303.09280</link><description>&lt;p&gt;
&#29289;&#29702;&#30693;&#35782;&#31070;&#32463;&#32593;&#32476;&#25299;&#25169;&#20248;&#21270;&#65306;&#24212;&#29992;&#20110;&#38544;&#34255;&#20960;&#20309;&#32467;&#26500;&#30340;&#38750;&#20405;&#20837;&#24335;&#25506;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Topology optimization with physics-informed neural networks: application to noninvasive detection of hidden geometries. (arXiv:2303.09280v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09280
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#30693;&#35782;&#31070;&#32463;&#32593;&#32476;&#30340;&#25299;&#25169;&#20248;&#21270;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#26080;&#20808;&#39564;&#30693;&#35782;&#30340;&#20960;&#20309;&#32467;&#26500;&#26816;&#27979;&#65292;&#36890;&#36807;&#26448;&#26009;&#23494;&#24230;&#22330;&#34920;&#31034;&#20219;&#24847;&#35299;&#20915;&#26041;&#26696;&#25299;&#25169;&#65292;&#24182;&#36890;&#36807;Eikonal&#27491;&#21017;&#21270;&#23454;&#29616;&#12290;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#21307;&#30103;&#21644;&#24037;&#19994;&#24212;&#29992;&#20013;&#30340;&#38750;&#20405;&#20837;&#24335;&#25104;&#20687;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#30103;&#21644;&#24037;&#19994;&#24212;&#29992;&#20013;&#65292;&#36890;&#36807;&#30005;&#30913;&#12289;&#22768;&#23398;&#25110;&#26426;&#26800;&#36127;&#36733;&#20174;&#34920;&#38754;&#27979;&#37327;&#20013;&#26816;&#27979;&#38544;&#34255;&#30340;&#20960;&#20309;&#32467;&#26500;&#26159;&#38750;&#20405;&#20837;&#25104;&#20687;&#25216;&#26415;&#30340;&#30446;&#26631;&#12290;&#30001;&#20110;&#26410;&#30693;&#30340;&#25299;&#25169;&#21644;&#20960;&#20309;&#24418;&#29366;&#12289;&#25968;&#25454;&#30340;&#31232;&#30095;&#24615;&#20197;&#21450;&#29289;&#29702;&#35268;&#24459;&#30340;&#22797;&#26434;&#24615;&#65292;&#35299;&#20915;&#36870;&#38382;&#39064;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#29289;&#29702;&#30693;&#35782;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#34920;&#29616;&#20986;&#35768;&#22810;&#20248;&#28857;&#65292;&#26159;&#19968;&#20010;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#38382;&#39064;&#21453;&#28436;&#24037;&#20855;&#65292;&#20294;&#23427;&#20204;&#23578;&#26410;&#24212;&#29992;&#20110;&#20855;&#26377;&#20808;&#39564;&#26410;&#30693;&#25299;&#25169;&#30340;&#19968;&#33324;&#38382;&#39064;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;PINNs&#30340;&#25299;&#25169;&#20248;&#21270;&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#35299;&#20915;&#27809;&#26377;&#24418;&#29366;&#25968;&#37327;&#25110;&#31867;&#22411;&#20808;&#39564;&#30693;&#35782;&#30340;&#20960;&#20309;&#26816;&#27979;&#38382;&#39064;&#12290;&#25105;&#20204;&#20801;&#35768;&#20219;&#24847;&#30340;&#35299;&#20915;&#26041;&#26696;&#25299;&#25169;&#65292;&#36890;&#36807;&#20351;&#29992;&#26448;&#26009;&#23494;&#24230;&#22330;&#26469;&#34920;&#31034;&#20960;&#20309;&#24418;&#29366;&#65292;&#24182;&#36890;&#36807;&#26032;&#30340;Eikonal&#27491;&#21017;&#21270;&#25509;&#36817;&#20108;&#36827;&#21046;&#20540;&#12290;&#25105;&#20204;&#36890;&#36807;&#26816;&#27979;&#38544;&#21547;&#34394;&#31354;&#21644;&#21253;&#21547;&#29289;&#30340;&#25968;&#37327;&#12289;&#20301;&#32622;&#21644;&#24418;&#29366;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detecting hidden geometrical structures from surface measurements under electromagnetic, acoustic, or mechanical loading is the goal of noninvasive imaging techniques in medical and industrial applications. Solving the inverse problem can be challenging due to the unknown topology and geometry, the sparsity of the data, and the complexity of the physical laws. Physics-informed neural networks (PINNs) have shown promise as a simple-yet-powerful tool for problem inversion, but they have yet to be applied to general problems with a priori unknown topology. Here, we introduce a topology optimization framework based on PINNs that solves geometry detection problems without prior knowledge of the number or types of shapes. We allow for arbitrary solution topology by representing the geometry using a material density field that approaches binary values thanks to a novel eikonal regularization. We validate our framework by detecting the number, locations, and shapes of hidden voids and inclusio
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;t-SPN&#31639;&#27861;&#21644;&#28388;&#27874;&#25216;&#26415;&#30340;&#32454;&#32990;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#36793;&#32536;&#21644;L2&#27491;&#21017;&#21270;&#65292;&#35813;&#26041;&#27861;&#22312;HEp-2&#21644;Feulgen&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.09065</link><description>&lt;p&gt;
&#22522;&#20110;t-SPN&#21644;&#28388;&#27874;&#30340;&#32454;&#32990;&#20998;&#31867;&#30340;&#26368;&#22823;&#38388;&#38548;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Maximum Margin Learning of t-SPNs for Cell Classification with Filtering. (arXiv:2303.09065v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09065
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;t-SPN&#31639;&#27861;&#21644;&#28388;&#27874;&#25216;&#26415;&#30340;&#32454;&#32990;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#36793;&#32536;&#21644;L2&#27491;&#21017;&#21270;&#65292;&#35813;&#26041;&#27861;&#22312;HEp-2&#21644;Feulgen&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#27010;&#29575;&#20307;&#31995;&#32467;&#26500;&#30340;&#31639;&#27861;&#65292;&#31216;&#20026;&#26641;&#24418;&#27714;&#21644;&#20135;&#21697;&#32593;&#32476;(t-SPN)&#65292;&#29992;&#20110;&#32454;&#32990;&#20998;&#31867;&#12290;&#26500;&#24314;t-SPN&#30340;&#30446;&#30340;&#26159;&#34920;&#31034;&#26410;&#24402;&#19968;&#21270;&#27010;&#29575;&#20316;&#20026;&#26368;&#30456;&#20284;&#30340;&#32454;&#32990;&#31867;&#21035;&#30340;&#26465;&#20214;&#27010;&#29575;&#12290;&#36890;&#36807;&#26368;&#22823;&#21270;&#36793;&#32536;&#26469;&#23398;&#20064;&#26500;&#24314;&#30340;t-SPN&#20307;&#31995;&#32467;&#26500;&#65292;&#35813;&#36793;&#32536;&#26159;&#30495;&#23454;&#26631;&#31614;&#21644;&#26368;&#26377;&#31454;&#20105;&#21147;&#30340;&#38169;&#35823;&#26631;&#31614;&#20043;&#38388;&#30340;&#26465;&#20214;&#27010;&#29575;&#24046;&#12290;&#20026;&#20102;&#22686;&#24378;&#20307;&#31995;&#32467;&#26500;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#32771;&#34385;&#20102;L2&#27491;&#21017;&#21270;&#65288;REG&#65289;&#21644;&#26368;&#22823;&#38388;&#38548;&#65288;MM&#65289;&#26631;&#20934;&#12290;&#20026;&#20102;&#31361;&#20986;&#32454;&#32990;&#29305;&#24449;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#20004;&#31181;&#36890;&#29992;&#30340;&#39640;&#36890;&#28388;&#27874;&#22120;&#30340;&#26377;&#25928;&#24615;&#65306;&#29702;&#24819;&#39640;&#36890;&#28388;&#27874;&#21644;&#25289;&#26222;&#25289;&#26031;&#28388;&#27874;(Log)&#12290;&#22312;HEp-2&#21644;Feulgen&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;&#22522;&#20110;&#26368;&#22823;&#38388;&#38548;&#20934;&#21017;&#19982;&#27491;&#21017;&#21270;&#23398;&#20064;&#30340;t-SPN&#20307;&#31995;&#32467;&#26500;&#20135;&#29983;&#20102;&#26368;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
An algorithm based on a deep probabilistic architecture referred to as a tree-structured sum-product network (t-SPN) is considered for cell classification. The t-SPN is constructed such that the unnormalized probability is represented as conditional probabilities of a subset of most similar cell classes. The constructed t-SPN architecture is learned by maximizing the margin, which is the difference in the conditional probability between the true and the most competitive false label. To enhance the generalization ability of the architecture, L2-regularization (REG) is considered along with the maximum margin (MM) criterion in the learning process. To highlight cell features, this paper investigates the effectiveness of two generic high-pass filters: ideal high-pass filtering and the Laplacian of Gaussian (LOG) filtering. On both HEp-2 and Feulgen benchmark datasets, the t-SPN architecture learned based on the max-margin criterion with regularization produced the highest accuracy rate co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#35782;&#30693;&#35782;&#36741;&#21161;&#30340;&#32454;&#31890;&#24230;&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#36890;&#35782;&#30693;&#35782;&#25512;&#29702;&#27169;&#22359;&#22788;&#29702;&#30001;&#22522;&#20934;&#28145;&#24230;&#23398;&#20064;&#26816;&#27979;&#22120;&#32473;&#20986;&#30340;&#31895;&#31890;&#24230;&#26631;&#31614;&#65292;&#20174;&#32780;&#25552;&#39640;&#30446;&#26631;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#32463;&#36807;&#23454;&#39564;&#39564;&#35777;&#65292;&#35813;&#26041;&#27861;&#30456;&#27604;&#20110;&#29616;&#26377;&#26041;&#27861;&#38656;&#35201;&#26356;&#23569;&#30340;&#35745;&#31639;&#37327;&#21644;&#26631;&#27880;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2303.09026</link><description>&lt;p&gt;
&#36890;&#35782;&#30693;&#35782;&#36741;&#21161;&#30340;&#36164;&#28304;&#21463;&#38480;&#21644;&#32454;&#31890;&#24230;&#30446;&#26631;&#26816;&#27979;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Commonsense Knowledge Assisted Deep Learning for Resource-constrained and Fine-grained Object Detection. (arXiv:2303.09026v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09026
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#35782;&#30693;&#35782;&#36741;&#21161;&#30340;&#32454;&#31890;&#24230;&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#36890;&#35782;&#30693;&#35782;&#25512;&#29702;&#27169;&#22359;&#22788;&#29702;&#30001;&#22522;&#20934;&#28145;&#24230;&#23398;&#20064;&#26816;&#27979;&#22120;&#32473;&#20986;&#30340;&#31895;&#31890;&#24230;&#26631;&#31614;&#65292;&#20174;&#32780;&#25552;&#39640;&#30446;&#26631;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#32463;&#36807;&#23454;&#39564;&#39564;&#35777;&#65292;&#35813;&#26041;&#27861;&#30456;&#27604;&#20110;&#29616;&#26377;&#26041;&#27861;&#38656;&#35201;&#26356;&#23569;&#30340;&#35745;&#31639;&#37327;&#21644;&#26631;&#27880;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#36793;&#32536;&#35745;&#31639;&#31561;&#36164;&#28304;&#21463;&#38480;&#22330;&#26223;&#19979;&#30340;&#32454;&#31890;&#24230;&#22270;&#20687;&#30446;&#26631;&#26816;&#27979;&#38382;&#39064;&#12290;&#38024;&#23545;&#20351;&#29992;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#30446;&#26631;&#26816;&#27979;&#22120;&#26102;&#38656;&#35201;&#20351;&#29992;&#22823;&#22411;&#27169;&#22411;&#21644;&#22823;&#37327;&#25968;&#25454;&#26631;&#27880;&#30340;&#31934;&#20934;&#32454;&#31890;&#24230;&#26816;&#27979;&#38656;&#27714;&#65292;&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#65292;&#21363;&#21033;&#29992;&#36890;&#35782;&#30693;&#35782;&#36741;&#21161;&#31895;&#31890;&#24230;&#30446;&#26631;&#26816;&#27979;&#22120;&#33719;&#21462;&#31934;&#20934;&#30340;&#32454;&#31890;&#24230;&#26816;&#27979;&#32467;&#26524;&#12290;&#24341;&#20837;&#36890;&#35782;&#30693;&#35782;&#25512;&#29702;&#27169;&#22359;(CKIM)&#22788;&#29702;&#30001;&#22522;&#20934;&#28145;&#24230;&#23398;&#20064;&#26816;&#27979;&#22120;&#32473;&#20986;&#30340;&#31895;&#31890;&#24230;&#26631;&#31614;&#65292;&#20174;&#32780;&#29983;&#25104;&#32454;&#31890;&#24230;&#26631;&#31614;&#12290;&#35770;&#25991;&#20013;&#32771;&#34385;&#20102;&#27169;&#31946;&#35268;&#21017;&#21644;&#28165;&#26224;&#35268;&#21017;&#30340;&#25512;&#29702;&#65292;&#21069;&#32773;&#29992;&#20110;&#22788;&#29702;&#30446;&#26631;&#35821;&#20041;&#26631;&#31614;&#30340;&#27169;&#31946;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25152;&#25552;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#30446;&#26631;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#30456;&#27604;&#20110;&#29616;&#26377;&#26041;&#27861;&#38656;&#35201;&#26356;&#23569;&#30340;&#35745;&#31639;&#37327;&#21644;&#26631;&#27880;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we consider fine-grained image object detection in resource-constrained cases such as edge computing. Deep learning (DL), namely learning with deep neural networks (DNNs), has become the dominating approach to object detection. To achieve accurate fine-grained detection, one needs to employ a large enough DNN model and a vast amount of data annotations, which brings a challenge for using modern DL object detectors in resource-constrained cases. To this end, we propose an approach, which leverages commonsense knowledge to assist a coarse-grained object detector to get accurate fine-grained detection results. Specifically, we introduce a commonsense knowledge inference module (CKIM) to process coarse-grained lables given by a benchmark DL detector to produce fine-grained lables. We consider both crisp-rule and fuzzy-rule based inference in our CKIM; the latter is used to handle ambiguity in the target semantic labels. We implement our method based on several modern DL dete
&lt;/p&gt;</description></item><item><title>&#25506;&#31350;&#25968;&#25454;&#38544;&#31169;&#22686;&#24378;&#25216;&#26415;&#23545;AI&#27835;&#29702;&#30340;&#37325;&#35201;&#24615;&#65292;&#23558;&#19981;&#21516;&#30340;AI&#27835;&#29702;&#30446;&#26631;&#35270;&#20026;&#31995;&#32479;&#20449;&#24687;&#27969;&#65292;&#24378;&#35843;&#35299;&#20915;&#26041;&#26696;&#20043;&#38388;&#30340;&#20114;&#25805;&#20316;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.08956</link><description>&lt;p&gt;
&#25506;&#31350;&#25968;&#25454;&#38544;&#31169;&#22686;&#24378;&#25216;&#26415;&#22312;AI&#27835;&#29702;&#24212;&#29992;&#20013;&#30340;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
Exploring the Relevance of Data Privacy-Enhancing Technologies for AI Governance Use Cases. (arXiv:2303.08956v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08956
&lt;/p&gt;
&lt;p&gt;
&#25506;&#31350;&#25968;&#25454;&#38544;&#31169;&#22686;&#24378;&#25216;&#26415;&#23545;AI&#27835;&#29702;&#30340;&#37325;&#35201;&#24615;&#65292;&#23558;&#19981;&#21516;&#30340;AI&#27835;&#29702;&#30446;&#26631;&#35270;&#20026;&#31995;&#32479;&#20449;&#24687;&#27969;&#65292;&#24378;&#35843;&#35299;&#20915;&#26041;&#26696;&#20043;&#38388;&#30340;&#20114;&#25805;&#20316;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#38544;&#31169;&#22686;&#24378;&#25216;&#26415;&#30340;&#21457;&#23637;&#65292;&#25968;&#25454;&#20132;&#25442;&#21644;&#20998;&#26512;&#20013;&#38544;&#31169;&#19982;&#24615;&#33021;&#20043;&#38388;&#30340;&#26435;&#34913;&#24050;&#32463;&#24471;&#21040;&#20102;&#26497;&#22823;&#30340;&#25913;&#21892;&#12290;&#31867;&#20284;&#30340;&#36879;&#26126;&#24230;&#32467;&#26500;&#24037;&#20855;&#23545;&#20110;AI&#27835;&#29702;&#20063;&#24456;&#26377;&#29992;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#25552;&#20379;&#22806;&#37096;&#23457;&#26597;&#12289;&#23457;&#35745;&#21644;&#28304;&#39564;&#35777;&#31561;&#33021;&#21147;&#12290;&#20026;&#20102;&#36991;&#20813;&#27835;&#29702;&#19978;&#30340;&#37325;&#22823;&#28431;&#27934;&#21644;&#23616;&#38480;&#24615;&#65292;&#38656;&#35201;&#23558;&#36825;&#20123;&#19981;&#21516;&#30340;AI&#27835;&#29702;&#30446;&#26631;&#35270;&#20026;&#20449;&#24687;&#27969;&#31995;&#32479;&#12290;&#30001;&#20110;&#22312;&#26412;&#25991;&#25552;&#21040;&#30340;AI&#27835;&#29702;&#29992;&#20363;&#25152;&#38656;&#30340;&#36719;&#20214;&#26632;&#20013;&#21487;&#33021;&#23384;&#22312;&#37325;&#21472;&#65292;&#22240;&#27492;&#22312;&#25972;&#20307;&#19978;&#30475;&#24453;&#31995;&#32479;&#65292;&#20102;&#35299;&#36825;&#20123;&#19981;&#21516;&#30340;AI&#27835;&#29702;&#35299;&#20915;&#26041;&#26696;&#20043;&#38388;&#30340;&#20114;&#25805;&#20316;&#24615;&#21464;&#24471;&#24456;&#37325;&#35201;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#20123;&#26631;&#20934;&#12289;&#23457;&#35745;&#31243;&#24207;&#12289;&#36719;&#20214;&#21644;&#35268;&#33539;&#23450;&#22411;&#20043;&#21069;&#65292;&#39318;&#20808;&#32039;&#24613;&#38656;&#35201;&#23558;AI&#27835;&#29702;&#20013;&#30340;&#36825;&#20123;&#38382;&#39064;&#20316;&#20026;&#31995;&#32479;&#26469;&#30740;&#31350;&#21644;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of privacy-enhancing technologies has made immense progress in reducing trade-offs between privacy and performance in data exchange and analysis. Similar tools for structured transparency could be useful for AI governance by offering capabilities such as external scrutiny, auditing, and source verification. It is useful to view these different AI governance objectives as a system of information flows in order to avoid partial solutions and significant gaps in governance, as there may be significant overlap in the software stacks needed for the AI governance use cases mentioned in this text. When viewing the system as a whole, the importance of interoperability between these different AI governance solutions becomes clear. Therefore, it is imminently important to look at these problems in AI governance as a system, before these standards, auditing procedures, software, and norms settle into place.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;WDiscOOD&#30340;&#26032;&#22411;OOD&#26816;&#27979;&#26041;&#27861;&#65292;&#20854;&#20013;&#20351;&#29992;&#30333;&#21270;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#23558;&#29305;&#24449;&#25237;&#24433;&#21040;&#21028;&#21035;&#23376;&#31354;&#38388;&#21644;&#27531;&#30041;&#23376;&#31354;&#38388;&#20013;&#65292;&#30830;&#23450;OOD&#20998;&#25968;&#12290;&#22312;&#22823;&#35268;&#27169;ImageNet-1k&#22522;&#20934;&#27979;&#35797;&#21644;&#20845;&#20010;OOD&#25968;&#25454;&#38598;&#20013;&#65292;WDiscOOD&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.07543</link><description>&lt;p&gt;
WDiscOOD&#65306;&#36890;&#36807;&#30333;&#21270;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#36827;&#34892;&#21306;&#20998;&#24230;&#20248;&#21270;&#30340;OOD&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
WDiscOOD: Out-of-Distribution Detection via Whitened Linear Discriminative Analysis. (arXiv:2303.07543v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07543
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;WDiscOOD&#30340;&#26032;&#22411;OOD&#26816;&#27979;&#26041;&#27861;&#65292;&#20854;&#20013;&#20351;&#29992;&#30333;&#21270;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#23558;&#29305;&#24449;&#25237;&#24433;&#21040;&#21028;&#21035;&#23376;&#31354;&#38388;&#21644;&#27531;&#30041;&#23376;&#31354;&#38388;&#20013;&#65292;&#30830;&#23450;OOD&#20998;&#25968;&#12290;&#22312;&#22823;&#35268;&#27169;ImageNet-1k&#22522;&#20934;&#27979;&#35797;&#21644;&#20845;&#20010;OOD&#25968;&#25454;&#38598;&#20013;&#65292;WDiscOOD&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23481;&#26131;&#22312;&#36935;&#21040;&#26410;&#30693;&#27010;&#24565;&#30340;&#24773;&#24418;&#19979;&#20135;&#29983;&#36807;&#24230;&#33258;&#20449;&#20294;&#38169;&#35823;&#30340;&#39044;&#27979;&#12290;&#36825;&#20010;&#25361;&#25112;&#31361;&#26174;&#20102;&#22312;&#24320;&#25918;&#19990;&#30028;&#20013;&#26816;&#27979;OOD&#26679;&#26412;&#30340;&#37325;&#35201;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29305;&#24449;&#31354;&#38388;OOD&#26816;&#27979;&#20998;&#25968;&#65292;&#21516;&#26102;&#32467;&#21512;&#20102;&#31867;&#21035;&#29305;&#23450;&#21644;&#31867;&#21035;&#19981;&#21487;&#30693;&#30340;&#20449;&#24687;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#30333;&#21270;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#23558;&#29305;&#24449;&#25237;&#24433;&#21040;&#20004;&#20010;&#23376;&#31354;&#38388;&#20013;&#8212;&#8212;&#21028;&#21035;&#23376;&#31354;&#38388;&#21644;&#27531;&#30041;&#23376;&#31354;&#38388;&#65292;&#20854;&#20013;ID&#31867;&#22312;&#21028;&#21035;&#23376;&#31354;&#38388;&#20013;&#34987;&#26368;&#22823;&#21270;&#22320;&#20998;&#31163;&#65292;&#24182;&#22312;&#27531;&#24046;&#23376;&#31354;&#38388;&#20013;&#34987;&#32039;&#23494;&#22320;&#32858;&#31867;&#12290;&#28982;&#21518;&#65292;&#22312;&#20004;&#20010;&#23376;&#31354;&#38388;&#20013;&#23558;&#26469;&#33258;&#36755;&#20837;&#25968;&#25454;&#19982;ID&#20998;&#24067;&#30340;&#20559;&#24046;&#32452;&#21512;&#36215;&#26469;&#30830;&#23450;OOD&#20998;&#25968;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21517;&#20026;WDiscOOD&#65292;&#22312;&#35206;&#30422;&#22810;&#31181;&#20998;&#24067;&#20559;&#31227;&#30340;&#20845;&#20010;OOD&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#20854;&#39640;&#25928;&#24615;&#65292;&#21253;&#25324;&#22823;&#35268;&#27169;ImageNet-1k&#22522;&#20934;&#27979;&#35797;&#12290;WDiscOOD&#22312;&#28145;&#24230;&#20998;&#31867;&#22120;&#19978;&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks are susceptible to generating overconfident yet erroneous predictions when presented with data beyond known concepts. This challenge underscores the importance of detecting out-of-distribution (OOD) samples in the open world. In this work, we propose a novel feature-space OOD detection score that jointly reasons with both class-specific and class-agnostic information. Specifically, our approach utilizes Whitened Linear Discriminative Analysis to project features into two subspaces - the discriminative and residual subspaces - in which the ID classes are maximally separated and closely clustered, respectively. The OOD score is then determined by combining the deviation from the input data to the ID distribution in both subspaces. The efficacy of our method, named WDiscOOD, is verified on the large-scale ImageNet-1k benchmark, with six OOD datasets that covers a variety of distribution shifts. WDiscOOD demonstrates superior performance on deep classifiers with divers
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#20855;&#26377;&#21035;&#21517;&#35266;&#27979;&#30340;&#28508;&#22312;&#22270;&#19978;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#26368;&#22823;&#21270;&#25506;&#32034;&#25928;&#29575;&#30340;&#25919;&#31574;&#31639;&#27861; eFeX&#65292;&#30456;&#27604;&#20110;&#38543;&#26426;&#31574;&#30053;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#26356;&#24555;&#22320;&#24674;&#22797;&#21508;&#31181;&#25299;&#25169;&#32467;&#26500;&#19979;&#30340;&#22270;&#34920;&#12290;</title><link>http://arxiv.org/abs/2303.07397</link><description>&lt;p&gt;
&#20855;&#26377;&#21035;&#21517;&#35266;&#27979;&#30340;&#28508;&#22312;&#22270;&#30340;&#24555;&#36895;&#25506;&#32034;&#19982;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Fast exploration and learning of latent graphs with aliased observations. (arXiv:2303.07397v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07397
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#20855;&#26377;&#21035;&#21517;&#35266;&#27979;&#30340;&#28508;&#22312;&#22270;&#19978;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#26368;&#22823;&#21270;&#25506;&#32034;&#25928;&#29575;&#30340;&#25919;&#31574;&#31639;&#27861; eFeX&#65292;&#30456;&#27604;&#20110;&#38543;&#26426;&#31574;&#30053;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#26356;&#24555;&#22320;&#24674;&#22797;&#21508;&#31181;&#25299;&#25169;&#32467;&#26500;&#19979;&#30340;&#22270;&#34920;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32771;&#34385;&#36825;&#31181;&#22330;&#26223;&#65306;&#19968;&#20010;&#26234;&#33021;&#20307;&#36890;&#36807;&#25191;&#34892;&#25805;&#20316;&#20174;&#19968;&#20010;&#33410;&#28857;&#21040;&#21478;&#19968;&#20010;&#33410;&#28857;&#26469;&#23548;&#33322;&#28508;&#22312;&#22270;&#12290;&#25152;&#36873;&#25805;&#20316;&#30830;&#23450;&#20102;&#19979;&#19968;&#20010;&#35775;&#38382;&#33410;&#28857;&#19978;&#30340;&#27010;&#29575;&#20998;&#24067;&#12290;&#22312;&#27599;&#20010;&#33410;&#28857;&#22788;&#65292;&#26234;&#33021;&#20307;&#25910;&#21040;&#19968;&#20010;&#35266;&#27979;&#65292;&#20294;&#35813;&#35266;&#27979;&#19981;&#26159;&#21807;&#19968;&#30340;&#65292;&#22240;&#27492;&#23427;&#19981;&#33021;&#21807;&#19968;&#22320;&#26631;&#35782;&#33410;&#28857;&#65292;&#36825;&#20351;&#24471;&#38382;&#39064;&#21035;&#21517;&#21270;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#19968;&#20010;&#25919;&#31574;&#65292;&#35813;&#25919;&#31574;&#32422;&#31561;&#20110;&#26368;&#22823;&#21270;&#25506;&#32034;&#25928;&#29575;&#65288;&#21363;&#22312;&#32473;&#23450;&#30340;&#25506;&#32034;&#39044;&#31639;&#19979;&#22914;&#20309;&#24674;&#22797;&#22270;&#34920;&#65289;&#12290;&#22312;&#38750;&#21035;&#21517;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#30456;&#23545;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#24378;&#21270;&#23398;&#20064;&#22522;&#32447;&#30340;&#25913;&#36827;&#24615;&#33021;&#12290;&#23545;&#20110;&#21035;&#21517;&#21270;&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#19981;&#30693;&#36947;&#36866;&#29992;&#30340;&#22522;&#32447;&#65292;&#32780;&#26159;&#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#25299;&#25169;&#32467;&#26500;&#19979;&#30456;&#23545;&#20110;&#38543;&#26426;&#31574;&#30053;&#26356;&#24555;&#30340;&#24674;&#22797;&#36895;&#24230;&#65292;&#24182;&#19988;&#23545;&#20110;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25299;&#25169;&#32467;&#26500;&#65292;&#24674;&#22797;&#36895;&#24230;&#27604;&#38543;&#26426;&#31574;&#30053;&#24555;&#25351;&#25968;&#20493;&#12290;&#25105;&#20204;&#23558;&#35813;&#31639;&#27861;&#31216;&#20026; eFeX&#65288;&#26469;&#33258;&#20110; efficient exploration &#30340;&#32553;&#20889;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Consider this scenario: an agent navigates a latent graph by performing actions that take it from one node to another. The chosen action determines the probability distribution over the next visited node. At each node, the agent receives an observation, but this observation is not unique, so it does not identify the node, making the problem aliased. The purpose of this work is to provide a policy that approximately maximizes exploration efficiency (i.e., how well the graph is recovered for a given exploration budget). In the unaliased case, we show improved performance w.r.t. state-of-the-art reinforcement learning baselines. For the aliased case we are not aware of suitable baselines and instead show faster recovery w.r.t. a random policy for a wide variety of topologies, and exponentially faster recovery than a random policy for challenging topologies. We dub the algorithm eFeX (from eFficient eXploration).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#28436;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#36890;&#29992;&#20989;&#25968;&#34920;&#31034;&#35821;&#35328;&#21644;&#26694;&#26550;&#26469;&#34920;&#31034;&#29305;&#23450;&#23545;&#35937;&#21450;&#20854;&#21442;&#19982;&#25903;&#25345;&#20854;&#35774;&#35745;&#30340;&#36807;&#31243;&#65292;&#20174;&#32780;&#23454;&#29616;&#28145;&#20837;&#30340;&#27010;&#24565;&#29702;&#35299;&#65292;&#21487;&#35299;&#37322;&#24615;&#30340;&#21151;&#33021;&#65292;&#20351;&#31995;&#32479;&#33021;&#22815;&#22238;&#31572;&#8220;&#20026;&#20160;&#20040;&#8221;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.06152</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;&#36825;&#26159;&#19968;&#20010;&#22909;&#30340;&#25110;&#19981;&#26159;&#19968;&#20010;&#22909;&#30340;&#29006;&#38149;&#65311;&#8212;&#8212;&#23545;&#35937;&#21644;&#24037;&#20855;&#21151;&#33021;&#30340;&#30693;&#35782;&#34920;&#31034;&#65292;&#29992;&#20110;&#35774;&#35745;&#29702;&#35299;&#12289;&#25913;&#36827;&#21644;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Why is That a Good or Not a Good Frying Pan? -- Knowledge Representation for Functions of Objects and Tools for Design Understanding, Improvement, and Generation for Design Understanding, Improvement, and Generation. (arXiv:2303.06152v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06152
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28436;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#36890;&#29992;&#20989;&#25968;&#34920;&#31034;&#35821;&#35328;&#21644;&#26694;&#26550;&#26469;&#34920;&#31034;&#29305;&#23450;&#23545;&#35937;&#21450;&#20854;&#21442;&#19982;&#25903;&#25345;&#20854;&#35774;&#35745;&#30340;&#36807;&#31243;&#65292;&#20174;&#32780;&#23454;&#29616;&#28145;&#20837;&#30340;&#27010;&#24565;&#29702;&#35299;&#65292;&#21487;&#35299;&#37322;&#24615;&#30340;&#21151;&#33021;&#65292;&#20351;&#31995;&#32479;&#33021;&#22815;&#22238;&#31572;&#8220;&#20026;&#20160;&#20040;&#8221;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper demonstrates how a particular object and its participation in the processes it is designed to support can be represented in a general function representational language and framework, leading to a deep conceptual understanding with explainability of functionalities that allows the system to answer "why" questions.
&lt;/p&gt;
&lt;p&gt;
&#23545;&#35937;&#21644;&#24037;&#20855;&#30340;&#21151;&#33021;&#26041;&#38754;&#30340;&#29702;&#35299;&#23545;&#20110;&#25903;&#25345;&#26234;&#33021;&#31995;&#32479;&#22312;&#29615;&#22659;&#20013;&#23548;&#33322;&#21644;&#19982;&#21508;&#31181;&#23545;&#35937;&#12289;&#32467;&#26500;&#21644;&#31995;&#32479;&#36827;&#34892;&#20132;&#20114;&#20197;&#24110;&#21161;&#23454;&#29616;&#20854;&#30446;&#26631;&#33267;&#20851;&#37325;&#35201;&#12290;&#23545;&#21151;&#33021;&#30340;&#35814;&#32454;&#29702;&#35299;&#20063;&#21487;&#20197;&#23548;&#33268;&#35774;&#35745;&#25913;&#36827;&#21644;&#26032;&#39062;&#30340;&#35774;&#35745;&#65292;&#20174;&#32780;&#22686;&#24378;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#25805;&#20316;&#65292;&#19968;&#26041;&#38754;&#65292;&#22686;&#24378;&#20154;&#31867;&#29983;&#27963;&#12290;&#26412;&#25991;&#28436;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#36890;&#29992;&#20989;&#25968;&#34920;&#31034;&#35821;&#35328;&#21644;&#26694;&#26550;&#26469;&#34920;&#31034;&#29305;&#23450;&#23545;&#35937;&#65288;&#22312;&#26412;&#20363;&#20013;&#20026;&#29006;&#38149;&#65289;&#21450;&#20854;&#21442;&#19982;&#25903;&#25345;&#20854;&#35774;&#35745;&#30340;&#36807;&#31243;&#65288;&#22312;&#26412;&#20363;&#20013;&#20026;&#29006;&#28856;&#36807;&#31243;&#65289;&#65292;&#20174;&#32780;&#21487;&#20197;&#29992;&#20110;&#35814;&#32454;&#35828;&#26126;&#28041;&#21450;&#30340;&#36807;&#31243;&#21644;&#21151;&#33021;&#65292;&#20174;&#32780;&#23454;&#29616;&#28145;&#20837;&#30340;&#27010;&#24565;&#29702;&#35299;&#65292;&#21487;&#35299;&#37322;&#24615;&#30340;&#21151;&#33021;&#65292;&#20351;&#31995;&#32479;&#33021;&#22815;&#22238;&#31572;&#8220;&#20026;&#20160;&#20040;&#8221;&#38382;&#39064;&#8212;&#8212;&#20026;&#20160;&#20040;&#36825;&#26159;&#19968;&#20010;&#22909;&#30340;&#29006;&#38149;&#65292;&#25110;&#32773;&#20026;&#20160;&#20040;&#26576;&#20010;&#37096;&#20214;&#22312;&#29006;&#38149;&#20013;&#30340;&#20301;&#32622;&#24456;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
The understanding of the functional aspects of objects and tools is of paramount importance in supporting an intelligent system in navigating around in the environment and interacting with various objects, structures, and systems, to help fulfil its goals. A detailed understanding of functionalities can also lead to design improvements and novel designs that would enhance the operations of AI and robotic systems on the one hand, and human lives on the other. This paper demonstrates how a particular object - in this case, a frying pan - and its participation in the processes it is designed to support - in this case, the frying process - can be represented in a general function representational language and framework, that can be used to flesh out the processes and functionalities involved, leading to a deep conceptual understanding with explainability of functionalities that allows the system to answer "why" questions - why is something a good frying pan, say, or why a certain part on t
&lt;/p&gt;</description></item><item><title>CVT-SLR&#26159;&#19968;&#31181;&#26032;&#30340;&#25163;&#35821;&#35782;&#21035;&#27169;&#22411;&#65292;&#23427;&#37319;&#29992;&#22522;&#20110;&#23545;&#27604;&#35270;&#35273;-&#25991;&#26412;&#21464;&#25442;&#21644;&#21464;&#20998;&#23545;&#40784;&#30340;&#26041;&#27861;&#26469;&#20805;&#20998;&#21033;&#29992;&#36328;&#27169;&#24577;&#30693;&#35782;&#65292;&#20026;&#35299;&#20915;&#25163;&#35821;&#35782;&#21035;&#20013;&#32570;&#20047;&#22823;&#35268;&#27169;&#21487;&#29992;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2303.05725</link><description>&lt;p&gt;
CVT-SLR&#65306;&#22522;&#20110;&#23545;&#27604;&#35270;&#35273;-&#25991;&#26412;&#21464;&#25442;&#19982;&#21464;&#20998;&#23545;&#40784;&#30340;&#25163;&#35821;&#35782;&#21035;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CVT-SLR: Contrastive Visual-Textual Transformation for Sign Language Recognition with Variational Alignment. (arXiv:2303.05725v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05725
&lt;/p&gt;
&lt;p&gt;
CVT-SLR&#26159;&#19968;&#31181;&#26032;&#30340;&#25163;&#35821;&#35782;&#21035;&#27169;&#22411;&#65292;&#23427;&#37319;&#29992;&#22522;&#20110;&#23545;&#27604;&#35270;&#35273;-&#25991;&#26412;&#21464;&#25442;&#21644;&#21464;&#20998;&#23545;&#40784;&#30340;&#26041;&#27861;&#26469;&#20805;&#20998;&#21033;&#29992;&#36328;&#27169;&#24577;&#30693;&#35782;&#65292;&#20026;&#35299;&#20915;&#25163;&#35821;&#35782;&#21035;&#20013;&#32570;&#20047;&#22823;&#35268;&#27169;&#21487;&#29992;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25163;&#35821;&#35782;&#21035; (SLR) &#26159;&#19968;&#39033;&#24369;&#30417;&#30563;&#20219;&#21153;&#65292;&#21487;&#20197;&#23558;&#25163;&#35821;&#35270;&#39057;&#27880;&#37322;&#20026;&#25991;&#26412;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#30001;&#20110;&#32570;&#20047;&#22823;&#35268;&#27169;&#21487;&#29992;&#30340;&#25163;&#35821;&#25968;&#25454;&#38598;&#32780;&#23548;&#33268;&#30340;&#19981;&#20805;&#20998;&#35757;&#32451;&#25104;&#20026; SLR &#30340;&#20027;&#35201;&#29942;&#39048;&#12290;&#22240;&#27492;&#65292;&#22823;&#22810;&#25968; SLR &#30340;&#24037;&#20316;&#37319;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#27169;&#22359;&#65292;&#24182;&#24320;&#21457;&#20102;&#20004;&#31181;&#20027;&#27969;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#23545;&#27604;&#35270;&#35273;-&#25991;&#26412;&#21464;&#25442;&#27169;&#22411; CVT-SLR&#65292;&#20805;&#20998;&#21457;&#25381;&#20102;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#24577;&#30340;&#39044;&#35757;&#32451;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sign language recognition (SLR) is a weakly supervised task that annotates sign videos as textual glosses. Recent studies show that insufficient training caused by the lack of large-scale available sign language datasets becomes the main bottleneck for SLR. The majority of SLR works thereby adopt pretrained visual modules and develop two mainstream solutions. The multi-stream architectures extend multi-cue visual features, yielding the current SOTA performances but requiring complex designs and might introduce potential noise. Alternatively, the advanced single-cue SLR frameworks using explicit cross-modal alignment between visual and textual modalities are simple and effective, potentially competitive with the multi-cue framework. In this work, we propose a novel contrastive visual-textual transformation for SLR, CVT-SLR, to fully explore the pretrained knowledge of both the visual and language modalities. Based on the single-cue cross-modal alignment framework, we propose a variation
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;ChatGPT&#22312;&#35299;&#20915;&#20132;&#36890;&#38382;&#39064;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#21033;&#29992;&#20855;&#26377;&#36328;&#27169;&#24577;&#32534;&#30721;&#22120;&#30340;LLM&#65292;&#21487;&#20197;&#22788;&#29702;&#26469;&#33258;&#19981;&#21516;&#27169;&#24577;&#30340;&#20132;&#36890;&#25968;&#25454;&#24182;&#25191;&#34892;&#20132;&#36890;&#36816;&#33829;&#12290;&#20316;&#32773;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110;&#26234;&#33021;&#25163;&#26426;&#30340;&#30896;&#25758;&#25253;&#21578;&#33258;&#21160;&#29983;&#25104;&#21644;&#20998;&#26512;&#26694;&#26550;&#20316;&#20026;&#29992;&#20363;&#23637;&#31034;&#20102;&#36825;&#31181;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.05382</link><description>&lt;p&gt;
ChatGPT&#24050;&#22312;&#22320;&#24179;&#32447;&#19978;&#65306;&#22823;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#23601;&#26159;&#25105;&#20204;&#38656;&#35201;&#30340;&#26234;&#33021;&#20132;&#36890;&#35299;&#20915;&#26041;&#26696;&#65311;
&lt;/p&gt;
&lt;p&gt;
ChatGPT Is on the Horizon: Could a Large Language Model Be All We Need for Intelligent Transportation?. (arXiv:2303.05382v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05382
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;ChatGPT&#22312;&#35299;&#20915;&#20132;&#36890;&#38382;&#39064;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#21033;&#29992;&#20855;&#26377;&#36328;&#27169;&#24577;&#32534;&#30721;&#22120;&#30340;LLM&#65292;&#21487;&#20197;&#22788;&#29702;&#26469;&#33258;&#19981;&#21516;&#27169;&#24577;&#30340;&#20132;&#36890;&#25968;&#25454;&#24182;&#25191;&#34892;&#20132;&#36890;&#36816;&#33829;&#12290;&#20316;&#32773;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110;&#26234;&#33021;&#25163;&#26426;&#30340;&#30896;&#25758;&#25253;&#21578;&#33258;&#21160;&#29983;&#25104;&#21644;&#20998;&#26512;&#26694;&#26550;&#20316;&#20026;&#29992;&#20363;&#23637;&#31034;&#20102;&#36825;&#31181;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;&#30001;OpenAI&#24320;&#21457;&#30340;&#20855;&#26377;60&#20159;&#21442;&#25968;&#30340;&#37325;&#35201;&#22823;&#35821;&#35328;&#27169;&#22411;&#20043;&#19968;&#12290;ChatGPT&#23637;&#31034;&#20102;LLM&#30340;&#21331;&#36234;&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#29983;&#25104;&#23545;&#35805;&#21709;&#24212;&#26041;&#38754;&#12290;&#38543;&#30528;LLM&#22312;&#21508;&#31181;&#30740;&#31350;&#25110;&#24037;&#31243;&#39046;&#22495;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#65292;&#29616;&#22312;&#26159;&#26102;&#20505;&#35774;&#24819;LLM&#22914;&#20309;&#38761;&#26032;&#25105;&#20204;&#22788;&#29702;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#30340;&#26041;&#24335;&#20102;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;LLM&#22312;&#35299;&#20915;&#20851;&#38190;&#20132;&#36890;&#38382;&#39064;&#26041;&#38754;&#30340;&#26410;&#26469;&#24212;&#29992;&#12290;&#36890;&#36807;&#21033;&#29992;&#20855;&#26377;&#36328;&#27169;&#24577;&#32534;&#30721;&#22120;&#30340;LLM&#65292;&#26234;&#33021;&#31995;&#32479;&#36824;&#21487;&#20197;&#22788;&#29702;&#26469;&#33258;&#19981;&#21516;&#27169;&#24577;&#30340;&#20132;&#36890;&#25968;&#25454;&#24182;&#36890;&#36807;LLM&#25191;&#34892;&#20132;&#36890;&#36816;&#33829;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#39564;&#35777;&#20102;LLM&#35013;&#22791;&#30340;&#36825;&#20123;&#28508;&#22312;&#30340;&#20132;&#36890;&#24212;&#29992;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#35777;&#26126;&#36825;&#31181;&#28508;&#21147;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#20855;&#20307;&#30340;&#22522;&#20110;&#26234;&#33021;&#25163;&#26426;&#30340;&#30896;&#25758;&#25253;&#21578;&#33258;&#21160;&#29983;&#25104;&#21644;&#20998;&#26512;&#26694;&#26550;&#20316;&#20026;&#29992;&#20363;&#12290;&#23613;&#31649;&#23384;&#22312;&#28508;&#22312;&#30340;&#30410;&#22788;&#65292;&#20294;&#19982;&#25968;&#25454;&#38544;&#31169;&#30456;&#20851;&#30340;&#25361;&#25112;&#20173;&#28982;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT, developed by OpenAI, is one of the milestone large language models (LLMs) with 6 billion parameters. ChatGPT has demonstrated the impressive language understanding capability of LLM, particularly in generating conversational response. As LLMs start to gain more attention in various research or engineering domains, it is time to envision how LLM may revolutionize the way we approach intelligent transportation systems. This paper explores the future applications of LLM in addressing key transportation problems. By leveraging LLM with cross-modal encoder, an intelligent system can also process traffic data from different modalities and execute transportation operations through an LLM. We present and validate these potential transportation applications equipped by LLM. To further demonstrate this potential, we also provide a concrete smartphone-based crash report auto-generation and analysis framework as a use case. Despite the potential benefits, challenges related to data privac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#22914;&#20309;&#22266;&#26377;&#22320;&#25506;&#32034;&#34892;&#21160;&#31354;&#38388;&#65292;&#24182;&#39318;&#27425;&#30830;&#23450;&#20102;&#19981;&#38656;&#35201;&#25506;&#32034;&#31574;&#30053;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#31181;&#26041;&#27861;&#30340;&#26368;&#20248;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#26126;&#30830;&#30340;&#34892;&#21160;&#31354;&#38388;&#25506;&#32034;&#24182;&#19981;&#19968;&#23450;&#26159;&#24517;&#35201;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2303.04386</link><description>&lt;p&gt;
&#31574;&#30053;&#38236;&#20687;&#19979;&#38477;&#26041;&#27861;&#22266;&#26377;&#22320;&#25506;&#32034;&#34892;&#21160;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Policy Mirror Descent Inherently Explores Action Space. (arXiv:2303.04386v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04386
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#22914;&#20309;&#22266;&#26377;&#22320;&#25506;&#32034;&#34892;&#21160;&#31354;&#38388;&#65292;&#24182;&#39318;&#27425;&#30830;&#23450;&#20102;&#19981;&#38656;&#35201;&#25506;&#32034;&#31574;&#30053;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#31181;&#26041;&#27861;&#30340;&#26368;&#20248;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#26126;&#30830;&#30340;&#34892;&#21160;&#31354;&#38388;&#25506;&#32034;&#24182;&#19981;&#19968;&#23450;&#26159;&#24517;&#35201;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#27809;&#26377;&#21152;&#20837;&#25506;&#32034;&#31574;&#30053;&#30340;&#24773;&#20917;&#19979;&#65292;&#37319;&#29992;&#22312;&#32447;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#26469;&#35299;&#20915;&#26377;&#38480;&#29366;&#24577;&#21644;&#34892;&#21160;&#31354;&#38388;&#19979;&#30340;&#36890;&#29992;&#22686;&#24378;&#23398;&#20064;&#38382;&#39064;&#65292;&#21407;&#20197;&#20026;&#22312;&#34892;&#21160;&#31354;&#38388;&#20869;&#36827;&#34892;&#26174;&#24335;&#25506;&#32034;&#26159;&#19981;&#21487;&#25110;&#32570;&#30340;&#65292;&#20197;&#36991;&#20813;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#21095;&#28872;&#38477;&#20302;&#12290;&#26412;&#25991;&#39318;&#27425;&#30830;&#23450;&#20102;&#22312;&#32447;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#30340;$\tilde{\mathcal{O}}(1/\epsilon^2)$&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;&#31574;&#30053;&#35780;&#20272;&#31639;&#23376;&#21644;&#19968;&#31181;&#26032;&#30340;&#20998;&#26512;&#38543;&#26426;&#31574;&#30053;&#38236;&#20687;&#19979;&#38477;&#26041;&#27861;&#65288; SPMD&#65289;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;SPMD&#36890;&#36807;&#20854;&#22312;&#32447;&#35780;&#20272;&#31639;&#23376;&#22266;&#26377;&#22320;&#25506;&#32034;&#34892;&#21160;&#31354;&#38388;&#65292;&#26126;&#30830;&#30340;&#34892;&#21160;&#31354;&#38388;&#25506;&#32034;&#24182;&#19981;&#19968;&#23450;&#26159;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#23454;&#29616;&#26368;&#20339;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;&#20854;&#20013;&#19968;&#20010;&#35780;&#20272;&#31639;&#23376;&#31216;&#20026;&#22522;&#20110;&#20215;&#20540;&#20989;&#25968;&#30340;&#20272;&#35745;&#65292;&#21478;&#19968;&#20010;&#35780;&#20272;&#31639;&#23376;&#31216;&#20026;&#22522;&#20110;&#31574;&#30053;&#30340;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explicit exploration in the action space was assumed to be indispensable for online policy gradient methods to avoid a drastic degradation in sample complexity, for solving general reinforcement learning problems over finite state and action spaces. In this paper, we establish for the first time an $\tilde{\mathcal{O}}(1/\epsilon^2)$ sample complexity for online policy gradient methods without incorporating any exploration strategies. The essential development consists of two new on-policy evaluation operators and a novel analysis of the stochastic policy mirror descent method (SPMD). SPMD with the first evaluation operator, called value-based estimation, tailors to the Kullback-Leibler divergence. Provided the Markov chains on the state space of generated policies are uniformly mixing with non-diminishing minimal visitation measure, an $\tilde{\mathcal{O}}(1/\epsilon^2)$ sample complexity is obtained with a linear dependence on the size of the action space. SPMD with the second evalua
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21551;&#21457;&#24335;&#30340;Few-shot&#24847;&#22270;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#24847;&#22270;&#26816;&#27979;&#37325;&#26032;&#23450;&#20041;&#20026;&#19968;&#20010;&#38382;&#39064;-&#22238;&#31572;&#26816;&#32034;&#20219;&#21153;&#26469;&#35299;&#20915;&#35821;&#20041;&#30456;&#20284;&#30340;&#32454;&#31890;&#24230;&#24847;&#22270;&#38382;&#39064;&#65292;&#32467;&#26524;&#22312;&#19977;&#20010;few-shot&#24847;&#22270;&#26816;&#27979;&#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#20102;&#26368;&#20248;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.01593</link><description>&lt;p&gt;
QAID&#65306;&#21551;&#21457;&#24335;&#30340;Few-shot&#24847;&#22270;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
QAID: Question Answering Inspired Few-shot Intent Detection. (arXiv:2303.01593v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01593
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21551;&#21457;&#24335;&#30340;Few-shot&#24847;&#22270;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#24847;&#22270;&#26816;&#27979;&#37325;&#26032;&#23450;&#20041;&#20026;&#19968;&#20010;&#38382;&#39064;-&#22238;&#31572;&#26816;&#32034;&#20219;&#21153;&#26469;&#35299;&#20915;&#35821;&#20041;&#30456;&#20284;&#30340;&#32454;&#31890;&#24230;&#24847;&#22270;&#38382;&#39064;&#65292;&#32467;&#26524;&#22312;&#19977;&#20010;few-shot&#24847;&#22270;&#26816;&#27979;&#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#20102;&#26368;&#20248;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24847;&#22270;&#26816;&#27979;&#28041;&#21450;&#21040;&#19968;&#20123;&#35821;&#20041;&#30456;&#20284;&#30340;&#32454;&#31890;&#24230;&#24847;&#22270;&#65292;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#24847;&#22270;&#26816;&#27979;&#37325;&#26032;&#23450;&#20041;&#20026;&#19968;&#20010;&#38382;&#39064;-&#22238;&#31572;&#26816;&#32034;&#20219;&#21153;&#65292;&#23558;&#35805;&#35821;&#21644;&#24847;&#22270;&#21517;&#20316;&#20026;&#38382;&#39064;&#21644;&#31572;&#26696;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#19968;&#20010;&#38382;&#39064;-&#22238;&#31572;&#26816;&#32034;&#20307;&#31995;&#32467;&#26500;&#65292;&#24182;&#37319;&#29992;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#22521;&#35757;&#27169;&#24335;&#65292;&#20854;&#20013;&#21253;&#25324;&#25209;&#37327;&#23545;&#27604;&#25439;&#22833;&#12290;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#65292;&#25105;&#20204;&#36890;&#36807;&#33258;&#25105;&#30417;&#30563;&#22521;&#35757;&#26469;&#25913;&#21892;&#26597;&#35810;&#34920;&#31034;&#12290;&#28982;&#21518;&#65292;&#22312;&#24494;&#35843;&#38454;&#27573;&#20013;&#65292;&#25105;&#20204;&#22686;&#21152;&#20102;&#26597;&#35810;&#21644;&#21516;&#19968;&#24847;&#22270;&#31572;&#26696;&#20043;&#38388;&#30340;&#19978;&#19979;&#25991;&#21270;&#20196;&#29260;&#32423;&#30456;&#20284;&#24230;&#20998;&#25968;&#12290;&#25105;&#20497;&#22312;&#19977;&#20010;few-shot&#24847;&#22270;&#26816;&#27979;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#32467;&#26524;&#36798;&#21040;&#20102;&#26368;&#20248;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intent detection with semantically similar fine-grained intents is a challenging task. To address it, we reformulate intent detection as a question-answering retrieval task by treating utterances and intent names as questions and answers. To that end, we utilize a question-answering retrieval architecture and adopt a two stages training schema with batch contrastive loss. In the pre-training stage, we improve query representations through self-supervised training. Then, in the fine-tuning stage, we increase contextualized token-level similarity scores between queries and answers from the same intent. Our results on three few-shot intent detection benchmarks achieve state-of-the-art performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20840;&#21516;&#24577;&#21152;&#23494;&#30340;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#65292;&#33021;&#22815;&#38024;&#23545;&#21152;&#23494;&#34920;&#26684;&#25968;&#25454;&#36827;&#34892;&#20219;&#24847;&#35745;&#31639;&#65292;&#24182;&#24471;&#21040;&#20102;&#26368;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36866;&#29992;&#20110;&#19968;&#31995;&#21015;&#26641;&#22411;&#27169;&#22411;&#65292;&#21253;&#25324;&#20915;&#31574;&#26641;&#65292;&#38543;&#26426;&#26862;&#26519;&#21644;&#26799;&#24230;&#22686;&#24378;&#26641;&#12290;&#27492;&#26041;&#27861;&#24050;&#24212;&#29992;&#22312;Concrete-ML&#24320;&#28304;&#24211;&#20013;&#65292;&#33021;&#22815;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#25509;&#36817;&#26410;&#21463;&#20445;&#25252;&#30340;&#29256;&#26412;&#12290;</title><link>http://arxiv.org/abs/2303.01254</link><description>&lt;p&gt;
&#22522;&#20110;&#20840;&#21516;&#24577;&#21152;&#23494;&#30340;&#38544;&#31169;&#20445;&#25252;&#26641;&#22411;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Privacy-Preserving Tree-Based Inference with Fully Homomorphic Encryption. (arXiv:2303.01254v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01254
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20840;&#21516;&#24577;&#21152;&#23494;&#30340;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#65292;&#33021;&#22815;&#38024;&#23545;&#21152;&#23494;&#34920;&#26684;&#25968;&#25454;&#36827;&#34892;&#20219;&#24847;&#35745;&#31639;&#65292;&#24182;&#24471;&#21040;&#20102;&#26368;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36866;&#29992;&#20110;&#19968;&#31995;&#21015;&#26641;&#22411;&#27169;&#22411;&#65292;&#21253;&#25324;&#20915;&#31574;&#26641;&#65292;&#38543;&#26426;&#26862;&#26519;&#21644;&#26799;&#24230;&#22686;&#24378;&#26641;&#12290;&#27492;&#26041;&#27861;&#24050;&#24212;&#29992;&#22312;Concrete-ML&#24320;&#28304;&#24211;&#20013;&#65292;&#33021;&#22815;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#25509;&#36817;&#26410;&#21463;&#20445;&#25252;&#30340;&#29256;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#31169;&#22686;&#24378;&#25216;&#26415;(PETs)&#34987;&#25552;&#20986;&#20316;&#20026;&#19968;&#31181;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#21516;&#26102;&#20801;&#35768;&#25968;&#25454;&#20998;&#26512;&#30340;&#26041;&#24335;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#19968;&#31181;&#24378;&#22823;&#30340;&#24037;&#20855;&#8212;&#8212;&#20840;&#21516;&#24577;&#21152;&#23494;(FHE)&#65292;&#23427;&#20801;&#35768;&#23545;&#21152;&#23494;&#25968;&#25454;&#36827;&#34892;&#20219;&#24847;&#35745;&#31639;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;FHE&#24212;&#29992;&#20110;&#22522;&#20110;&#26641;&#22411;&#27169;&#22411;&#30340;&#25968;&#25454;&#20998;&#26512;&#20013;&#65292;&#24471;&#21040;&#20102;&#38024;&#23545;&#21152;&#23494;&#34920;&#26684;&#25968;&#25454;&#30340;&#26368;&#26032;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#19968;&#31995;&#21015;&#26641;&#22411;&#27169;&#22411;&#65292;&#21253;&#25324;&#20915;&#31574;&#26641;&#65292;&#38543;&#26426;&#26862;&#26519;&#21644;&#26799;&#24230;&#22686;&#24378;&#26641;&#65292;&#24182;&#24050;&#23454;&#29616;&#22312;Concrete-ML&#24211;&#20013;&#65292;&#35813;&#24211;&#22312;https://github.com/zama-ai/concrete-ml. &#24320;&#28304;&#12290;&#36890;&#36807;&#36873;&#25321;&#19968;&#32452;&#24212;&#29992;&#26696;&#20363;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;FHE&#29256;&#26412;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#38750;&#24120;&#25509;&#36817;&#26410;&#21463;&#20445;&#25252;&#30340;&#29256;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Privacy enhancing technologies (PETs) have been proposed as a way to protect the privacy of data while still allowing for data analysis. In this work, we focus on Fully Homomorphic Encryption (FHE), a powerful tool that allows for arbitrary computations to be performed on encrypted data. FHE has received lots of attention in the past few years and has reached realistic execution times and correctness.  More precisely, we explain in this paper how we apply FHE to tree-based models and get state-of-the-art solutions over encrypted tabular data. We show that our method is applicable to a wide range of tree-based models, including decision trees, random forests, and gradient boosted trees, and has been implemented within the Concrete-ML library, which is open-source at https://github.com/zama-ai/concrete-ml. With a selected set of use-cases, we demonstrate that our FHE version is very close to the unprotected version in terms of accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102; Vid2Seq&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#22823;&#35268;&#27169; narrated &#35270;&#39057;&#25968;&#25454;&#38598;&#19978;&#39044;&#20808;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#23494;&#38598;&#20107;&#20214;&#23383;&#24149;&#27169;&#22411;&#12290;&#36890;&#36807;&#23558;&#36716;&#24405;&#35821;&#38899;&#30340;&#21477;&#23376;&#36793;&#30028;&#36716;&#21270;&#20026;&#20266;&#20107;&#20214;&#36793;&#30028;&#65292;&#24182;&#20351;&#29992;&#36716;&#24405;&#35821;&#38899;&#21477;&#23376;&#20316;&#20026;&#20266;&#20107;&#20214;&#23383;&#24149;&#65292;&#25105;&#20204;&#26377;&#25928;&#21033;&#29992;&#26410;&#26631;&#27880; narrated &#35270;&#39057;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#23494;&#38598;&#35270;&#39057;&#23383;&#24149;&#30340;&#35757;&#32451;&#12290;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#26159;&#30446;&#21069;&#26368;&#20248;&#31168;&#30340;&#27169;&#22411;&#20043;&#19968;&#12290;</title><link>http://arxiv.org/abs/2302.14115</link><description>&lt;p&gt;
Vid2Seq: &#29992;&#20110;&#23494;&#38598;&#35270;&#39057;&#23383;&#24149;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Vid2Seq: Large-Scale Pretraining of a Visual Language Model for Dense Video Captioning. (arXiv:2302.14115v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14115
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102; Vid2Seq&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#22823;&#35268;&#27169; narrated &#35270;&#39057;&#25968;&#25454;&#38598;&#19978;&#39044;&#20808;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#23494;&#38598;&#20107;&#20214;&#23383;&#24149;&#27169;&#22411;&#12290;&#36890;&#36807;&#23558;&#36716;&#24405;&#35821;&#38899;&#30340;&#21477;&#23376;&#36793;&#30028;&#36716;&#21270;&#20026;&#20266;&#20107;&#20214;&#36793;&#30028;&#65292;&#24182;&#20351;&#29992;&#36716;&#24405;&#35821;&#38899;&#21477;&#23376;&#20316;&#20026;&#20266;&#20107;&#20214;&#23383;&#24149;&#65292;&#25105;&#20204;&#26377;&#25928;&#21033;&#29992;&#26410;&#26631;&#27880; narrated &#35270;&#39057;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#23494;&#38598;&#35270;&#39057;&#23383;&#24149;&#30340;&#35757;&#32451;&#12290;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#26159;&#30446;&#21069;&#26368;&#20248;&#31168;&#30340;&#27169;&#22411;&#20043;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102; Vid2Seq&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#30340;&#21333;&#32423;&#23494;&#38598;&#20107;&#20214;&#23383;&#24149;&#27169;&#22411;&#65292;&#23427;&#26159;&#22312;&#22823;&#35268;&#27169; narrated &#35270;&#39057;&#25968;&#25454;&#38598;&#19978;&#39044;&#20808;&#35757;&#32451;&#30340;&#12290; Vid2Seq &#26550;&#26500;&#36890;&#36807;&#29305;&#27530;&#30340;&#26102;&#38388;&#26631;&#35760;&#26469;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#20854;&#33021;&#22815;&#26080;&#32541;&#22320;&#39044;&#27979;&#20107;&#20214;&#36793;&#30028;&#21644;&#25991;&#26412;&#25551;&#36848;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#26410;&#26631;&#27880; narrated &#35270;&#39057;&#25968;&#25454;&#38598;&#36827;&#34892;&#23494;&#38598;&#35270;&#39057;&#23383;&#24149;&#30340;&#35757;&#32451;&#65292;&#36890;&#36807;&#23558;&#36716;&#24405;&#35821;&#38899;&#30340;&#21477;&#23376;&#36793;&#30028;&#36716;&#21270;&#20026;&#20266;&#20107;&#20214;&#36793;&#30028;&#65292;&#24182;&#20351;&#29992;&#36716;&#24405;&#35821;&#38899;&#21477;&#23376;&#20316;&#20026;&#20266;&#20107;&#20214;&#23383;&#24149;&#12290;&#20351;&#29992; YT-Temporal-1B &#25968;&#25454;&#38598;&#39044;&#35757;&#32451;&#30340; Vid2Seq &#27169;&#22411;&#22312;&#21508;&#31181;&#23494;&#38598;&#35270;&#39057;&#23383;&#24149;&#22522;&#20934;&#27979;&#35797;&#20013;&#22343;&#34920;&#29616;&#20986;&#33394;&#65292;&#21253;&#25324; YouCook2&#12289;ViTT &#21644; ActivityNet Captions&#12290; Vid2Seq &#36824;&#21487;&#20197;&#24456;&#22909;&#22320;&#25512;&#24191;&#21040;&#35270;&#39057;&#27573;&#33853;&#23383;&#24149;&#21644;&#35270;&#39057;&#29255;&#27573;&#23383;&#24149;&#30340;&#20219;&#21153;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we introduce Vid2Seq, a multi-modal single-stage dense event captioning model pretrained on narrated videos which are readily-available at scale. The Vid2Seq architecture augments a language model with special time tokens, allowing it to seamlessly predict event boundaries and textual descriptions in the same output sequence. Such a unified model requires large-scale training data, which is not available in current annotated datasets. We show that it is possible to leverage unlabeled narrated videos for dense video captioning, by reformulating sentence boundaries of transcribed speech as pseudo event boundaries, and using the transcribed speech sentences as pseudo event captions. The resulting Vid2Seq model pretrained on the YT-Temporal-1B dataset improves the state of the art on a variety of dense video captioning benchmarks including YouCook2, ViTT and ActivityNet Captions. Vid2Seq also generalizes well to the tasks of video paragraph captioning and video clip captionin
&lt;/p&gt;</description></item><item><title>&#21487;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#23545;&#25552;&#39640;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20449;&#20219;&#24230;&#30340;&#25928;&#26524;&#26377;&#23616;&#38480;&#24615;&#65292;&#36879;&#26126;&#24230;&#21644;&#20005;&#26684;&#30340;&#39564;&#35777;&#26356;&#36866;&#21512;&#25171;&#36896;&#21487;&#38752;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2302.11577</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#26080;&#27861;&#25552;&#20379;&#26368;&#32456;&#29992;&#25143;&#25152;&#35201;&#27714;&#30340;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Explainable AI does not provide the explanations end-users are asking for. (arXiv:2302.11577v2 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11577
&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#23545;&#25552;&#39640;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20449;&#20219;&#24230;&#30340;&#25928;&#26524;&#26377;&#23616;&#38480;&#24615;&#65292;&#36879;&#26126;&#24230;&#21644;&#20005;&#26684;&#30340;&#39564;&#35777;&#26356;&#36866;&#21512;&#25171;&#36896;&#21487;&#38752;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#25216;&#26415;&#32463;&#24120;&#34987;&#35768;&#22810;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#29992;&#25143;&#35201;&#27714;&#20351;&#29992;&#65292;&#26088;&#22312;&#20102;&#35299;&#22797;&#26434;&#27169;&#22411;&#21450;&#20854;&#30456;&#20851;&#39044;&#27979;&#65292;&#24182;&#24314;&#31435;&#20449;&#20219;&#12290;&#34429;&#28982;&#22312;&#24320;&#21457;&#30340;&#26576;&#20123;&#29305;&#23450;&#20219;&#21153;&#20013;&#26159;&#36866;&#29992;&#30340;&#65292;&#20294;&#32452;&#32455;&#37319;&#29992;&#36825;&#20123;&#25216;&#26415;&#26469;&#22686;&#24378;&#23545;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#20449;&#20219;&#26102;&#65292;&#20250;&#20135;&#29983;&#24847;&#24819;&#19981;&#21040;&#30340;&#21518;&#26524;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;XAI&#22312;&#37096;&#32626;&#20013;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#24471;&#20986;&#32467;&#35770;&#35748;&#20026;&#36879;&#26126;&#24230;&#21644;&#20005;&#26684;&#30340;&#39564;&#35777;&#26356;&#36866;&#21512;&#33719;&#24471;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#20449;&#20219;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainable Artificial Intelligence (XAI) techniques are frequently required by users in many AI systems with the goal of understanding complex models, their associated predictions, and gaining trust. While suitable for some specific tasks during development, their adoption by organisations to enhance trust in machine learning systems has unintended consequences. In this paper we discuss XAI's limitations in deployment and conclude that transparency alongside with rigorous validation are better suited to gaining trust in AI systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38750;&#39640;&#26031;&#31354;&#38388;&#20869;&#22312;&#26377;&#21521;&#32467;&#26500;&#30340;&#20998;&#31867;&#23398;&#25193;&#23637;&#31639;&#27861;DNG&#65292;&#36890;&#36807;&#26126;&#30830;&#34920;&#31034;&#27599;&#20010;&#33410;&#28857;&#30340;&#32487;&#25215;&#29305;&#24449;&#21644;&#22686;&#37327;&#29305;&#24449;&#30340;&#32452;&#21512;&#26469;&#25366;&#25496;&#32467;&#26500;&#20449;&#24687;&#65292;&#24182;&#25104;&#21151;&#25429;&#25417;&#33410;&#28857;&#20043;&#38388;is-a&#20851;&#31995;&#30340;&#26041;&#21521;&#65292;&#25928;&#26524;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.11165</link><description>&lt;p&gt;
&#22522;&#20110;&#38750;&#39640;&#26031;&#31354;&#38388;&#20869;&#22312;&#26377;&#21521;&#32467;&#26500;&#30340;&#20998;&#31867;&#23398;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
DNG: Taxonomy Expansion by Exploring the Intrinsic Directed Structure on Non-gaussian Space. (arXiv:2302.11165v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11165
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38750;&#39640;&#26031;&#31354;&#38388;&#20869;&#22312;&#26377;&#21521;&#32467;&#26500;&#30340;&#20998;&#31867;&#23398;&#25193;&#23637;&#31639;&#27861;DNG&#65292;&#36890;&#36807;&#26126;&#30830;&#34920;&#31034;&#27599;&#20010;&#33410;&#28857;&#30340;&#32487;&#25215;&#29305;&#24449;&#21644;&#22686;&#37327;&#29305;&#24449;&#30340;&#32452;&#21512;&#26469;&#25366;&#25496;&#32467;&#26500;&#20449;&#24687;&#65292;&#24182;&#25104;&#21151;&#25429;&#25417;&#33410;&#28857;&#20043;&#38388;is-a&#20851;&#31995;&#30340;&#26041;&#21521;&#65292;&#25928;&#26524;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#31867;&#23398;&#25193;&#23637;&#26159;&#23558;&#22823;&#37327;&#20854;&#20182;&#33410;&#28857;&#65288;&#21363;&#8220;&#26597;&#35810;&#8221;&#65289;&#32435;&#20837;&#29616;&#26377;&#20998;&#31867;&#23398;&#65288;&#21363;&#8220;&#31181;&#23376;&#8221;&#65289;&#30340;&#36807;&#31243;&#65292;&#20854;&#20013;&#26368;&#37325;&#35201;&#30340;&#27493;&#39588;&#26159;&#36873;&#25321;&#27599;&#20010;&#26597;&#35810;&#30340;&#36866;&#24403;&#20301;&#32622;&#12290;&#34429;&#28982;&#36890;&#36807;&#25506;&#32034;&#31181;&#23376;&#32467;&#26500;&#36827;&#34892;&#20102;&#22823;&#37327;&#21162;&#21147;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#22312;&#20004;&#26041;&#38754;&#25366;&#25496;&#32467;&#26500;&#20449;&#24687;&#23384;&#22312;&#19981;&#36275;&#65306;&#23545;&#23618;&#27425;&#35821;&#20041;&#30340;&#24314;&#27169;&#36739;&#24046;&#65292;&#26080;&#27861;&#25429;&#25417;is-a&#20851;&#31995;&#30340;&#26041;&#21521;&#24615;&#12290;&#26412;&#25991;&#35797;&#22270;&#36890;&#36807;&#23558;&#27599;&#20010;&#33410;&#28857;&#26126;&#30830;&#34920;&#31034;&#20026;&#32487;&#25215;&#29305;&#24449;&#65288;&#21363;&#32467;&#26500;&#37096;&#20998;&#65289;&#21644;&#22686;&#37327;&#29305;&#24449;&#65288;&#21363;&#34917;&#20805;&#37096;&#20998;&#65289;&#30340;&#32452;&#21512;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#32487;&#25215;&#29305;&#24449;&#28304;&#33258;&#8220;&#29238;&#8221;&#33410;&#28857;&#65292;&#24182;&#21152;&#19978;&#19968;&#20010;&#32487;&#25215;&#22240;&#23376;&#30340;&#26435;&#37325;&#12290;&#26377;&#20102;&#36825;&#31181;&#33410;&#28857;&#34920;&#31034;&#27861;&#65292;&#20998;&#31867;&#23398;&#20013;&#30340;&#35821;&#20041;&#23618;&#27425;&#32467;&#26500;&#65288;&#21363;&#20174;&#8220;&#29238;&#32423;&#8221;&#21040;&#8220;&#23376;&#32423;&#8221;&#30340;&#32487;&#25215;&#21644;&#32047;&#31215;&#29305;&#24449;&#65289;&#21487;&#20197;&#34987;&#20307;&#29616;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#36825;&#20123;&#34920;&#31034;&#65292;&#21487;&#20197;&#25429;&#25417;&#33410;&#28857;&#20043;&#38388;is-a&#20851;&#31995;&#30340;&#26041;&#21521;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#21306;&#20998;&#19981;&#21516;&#26041;&#21521;&#30340;&#20851;&#31995;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DNG&#65288;Directed Non-Gaussian&#65289;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#25506;&#32034;&#38750;&#39640;&#26031;&#31354;&#38388;&#19978;&#30340;&#20869;&#22312;&#26377;&#21521;&#32467;&#26500;&#26469;&#25193;&#23637;&#20998;&#31867;&#23398;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DNG&#22312;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#26041;&#38754;&#22343;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Taxonomy expansion is the process of incorporating a large number of additional nodes (i.e., "queries") into an existing taxonomy (i.e., "seed"), with the most important step being the selection of appropriate positions for each query. Enormous efforts have been made by exploring the seed's structure. However, existing approaches are deficient in their mining of structural information in two ways: poor modeling of the hierarchical semantics and failure to capture directionality of is-a relation. This paper seeks to address these issues by explicitly denoting each node as the combination of inherited feature (i.e., structural part) and incremental feature (i.e., supplementary part). Specifically, the inherited feature originates from "parent" nodes and is weighted by an inheritance factor. With this node representation, the hierarchy of semantics in taxonomies (i.e., the inheritance and accumulation of features from "parent" to "child") could be embodied. Additionally, based on this rep
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#27963;&#20307;&#24471;&#20998;&#30340;&#26041;&#27861;&#26469;&#20811;&#26381;&#23545;&#31532;&#19977;&#26041;&#32593;&#32476;&#21644;&#29992;&#25143;&#30340;&#20381;&#36182;&#65292;&#24182;&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#26631;&#31614;&#25216;&#26415;&#26469;&#20135;&#29983;&#34920;&#31034;&#19982;&#30495;&#23454;&#22270;&#20687;&#30456;&#20851;&#30340;&#20449;&#24687;&#37327;&#30340;&#31163;&#25955;&#21270;&#26631;&#31614;&#12290;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#20197;&#21069;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.09461</link><description>&lt;p&gt;
&#22522;&#20110;&#27963;&#20307;&#24471;&#20998;&#30340;&#22238;&#24402;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#38754;&#37096;&#21453;&#27450;&#35784;
&lt;/p&gt;
&lt;p&gt;
Liveness score-based regression neural networks for face anti-spoofing. (arXiv:2302.09461v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09461
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#27963;&#20307;&#24471;&#20998;&#30340;&#26041;&#27861;&#26469;&#20811;&#26381;&#23545;&#31532;&#19977;&#26041;&#32593;&#32476;&#21644;&#29992;&#25143;&#30340;&#20381;&#36182;&#65292;&#24182;&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#26631;&#31614;&#25216;&#26415;&#26469;&#20135;&#29983;&#34920;&#31034;&#19982;&#30495;&#23454;&#22270;&#20687;&#30456;&#20851;&#30340;&#20449;&#24687;&#37327;&#30340;&#31163;&#25955;&#21270;&#26631;&#31614;&#12290;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#20197;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#21453;&#27450;&#35784;&#26041;&#27861;&#26159;&#20351;&#29992;&#20266;&#22320;&#22270;&#25110;&#29992;&#25143;&#23450;&#20041;&#26631;&#31614;&#65292;&#27599;&#31181;&#26041;&#27861;&#30340;&#34920;&#29616;&#37117;&#20381;&#36182;&#20110;&#31532;&#19977;&#26041;&#32593;&#32476;&#29983;&#25104;&#20266;&#22320;&#22270;&#30340;&#20934;&#30830;&#24615;&#20197;&#21450;&#29992;&#25143;&#23450;&#20041;&#26631;&#31614;&#30340;&#26041;&#24335;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27963;&#20307;&#24471;&#20998;&#30340;&#22238;&#24402;&#32593;&#32476;&#26469;&#20811;&#26381;&#23545;&#31532;&#19977;&#26041;&#32593;&#32476;&#21644;&#29992;&#25143;&#30340;&#20381;&#36182;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26631;&#31614;&#25216;&#26415;&#65292;&#31216;&#20026;&#20266;&#20998;&#31163;&#26631;&#31614;&#32534;&#30721;&#65292;&#29992;&#20110;&#20135;&#29983;&#34920;&#31034;&#19982;&#30495;&#23454;&#22270;&#20687;&#30456;&#20851;&#30340;&#20449;&#24687;&#37327;&#30340;&#31163;&#25955;&#21270;&#26631;&#31614;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26399;&#26395;&#30340;&#27963;&#20307;&#24471;&#20998;&#65292;&#22522;&#20110;&#22238;&#24402;&#32593;&#32476;&#35757;&#32451;&#25552;&#20986;&#30340;&#30417;&#30563;&#21644;&#26399;&#26395;&#30340;&#27963;&#20307;&#24471;&#20998;&#30340;&#24046;&#24322;&#12290;&#26368;&#21518;&#65292;&#22312;&#22235;&#20010;&#38754;&#37096;&#21453;&#27450;&#35784;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#39564;&#35777;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20869;&#21644;&#36328;&#25968;&#25454;&#38598;&#27979;&#35797;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#20197;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Previous anti-spoofing methods have used either pseudo maps or user-defined labels, and the performance of each approach depends on the accuracy of the third party networks generating pseudo maps and the way in which the users define the labels. In this paper, we propose a liveness score-based regression network for overcoming the dependency on third party networks and users. First, we introduce a new labeling technique, called pseudo-discretized label encoding for generating discretized labels indicating the amount of information related to real images. Secondly, we suggest the expected liveness score based on a regression network for training the difference between the proposed supervision and the expected liveness score. Finally, extensive experiments were conducted on four face anti-spoofing benchmarks to verify our proposed method on both intra-and cross-dataset tests. The experimental results show our approach outperforms previous methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986; GPT4MIA &#26041;&#27861;&#65292;&#21033;&#29992; GPT-3 &#20316;&#20026;&#25554;&#20837;&#24335;&#26816;&#39564;&#24037;&#20855;&#36827;&#34892;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#65307;&#35813;&#26041;&#27861;&#22312;&#25552;&#31034;&#32467;&#26500;&#35774;&#35745;&#12289;&#26679;&#26412;&#36873;&#25321;&#21450;&#25552;&#31034;&#25490;&#24207;&#31561;&#26041;&#38754;&#20248;&#21270;&#65292;&#33021;&#26377;&#25928;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.08722</link><description>&lt;p&gt;
GPT4MIA: &#21033;&#29992;&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120; (GPT-3) &#20316;&#20026;&#25554;&#20837;&#24335;&#26816;&#39564;&#27169;&#22411;&#36827;&#34892;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
GPT4MIA: Utilizing Generative Pre-trained Transformer (GPT-3) as A Plug-and-Play Transductive Model for Medical Image Analysis. (arXiv:2302.08722v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08722
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986; GPT4MIA &#26041;&#27861;&#65292;&#21033;&#29992; GPT-3 &#20316;&#20026;&#25554;&#20837;&#24335;&#26816;&#39564;&#24037;&#20855;&#36827;&#34892;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#65307;&#35813;&#26041;&#27861;&#22312;&#25552;&#31034;&#32467;&#26500;&#35774;&#35745;&#12289;&#26679;&#26412;&#36873;&#25321;&#21450;&#25552;&#31034;&#25490;&#24207;&#31561;&#26041;&#38754;&#20248;&#21270;&#65292;&#33021;&#26377;&#25928;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026; GPT4MIA &#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120; (GPT) &#20316;&#20026;&#25554;&#20837;&#24335;&#26816;&#39564;&#24037;&#20855;&#65292;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512; (MIA)&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#20687; GPT-3 &#36825;&#26679;&#30340;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#20316;&#20026;&#25554;&#20837;&#24335;&#26816;&#39564;&#27169;&#22411;&#29992;&#20110; MIA&#12290;&#22312;&#26041;&#27861;&#23398;&#23618;&#38754;&#19978;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20960;&#31181;&#25216;&#26415;&#22788;&#29702;&#26041;&#27861;&#65292;&#21253;&#25324;&#26356;&#22909;&#30340;&#25552;&#31034;&#32467;&#26500;&#35774;&#35745;&#12289;&#26679;&#26412;&#36873;&#25321;&#20197;&#21450;&#20195;&#34920;&#24615;&#26679;&#26412;/&#29305;&#24449;&#30340;&#25552;&#31034;&#25490;&#24207;&#65292;&#20197;&#25552;&#39640; GPT4MIA &#30340;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#21576;&#29616;&#20102;&#20004;&#31181;&#20855;&#20307;&#30340; GPT4MIA &#20351;&#29992;&#26696;&#20363; (&#24102;&#26377;&#24037;&#20316;&#27969;&#31243;)&#65306;(1) &#26816;&#27979;&#39044;&#27979;&#38169;&#35823;&#21644; (2) &#25913;&#36827;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#19982;&#24050;&#32463;&#24314;&#31435;&#30340;&#22522;&#20110;&#35270;&#35273;&#30340;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411; (&#20363;&#22914; ResNet) &#21327;&#21516;&#24037;&#20316;&#12290;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#23545;&#20110;&#36825;&#20004;&#20010;&#20219;&#21153;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35752;&#35770;&#20102;&#21033;&#29992;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892; MIA &#20219;&#21153;&#30340;&#26426;&#20250;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a novel approach (called GPT4MIA) that utilizes Generative Pre-trained Transformer (GPT) as a plug-and-play transductive inference tool for medical image analysis (MIA). We provide theoretical analysis on why a large pre-trained language model such as GPT-3 can be used as a plug-and-play transductive inference model for MIA. At the methodological level, we develop several technical treatments to improve the efficiency and effectiveness of GPT4MIA, including better prompt structure design, sample selection, and prompt ordering of representative samples/features. We present two concrete use cases (with workflow) of GPT4MIA: (1) detecting prediction errors and (2) improving prediction accuracy, working in conjecture with well-established vision-based models for image classification (e.g., ResNet). Experiments validate that our proposed method is effective for these two tasks. We further discuss the opportunities and challenges in utilizing Transformer-based large
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#24402;&#19968;&#21270;&#26041;&#26696;Adap-$\tau$&#65292;&#36890;&#36807;&#21160;&#24577;&#35843;&#33410;&#27599;&#20010;&#29992;&#25143;-&#27599;&#20010;&#29289;&#21697;&#23545;&#30340;&#23884;&#20837;&#24133;&#24230;&#65292;&#23454;&#29616;&#20102;&#29702;&#24819;&#30340;&#25512;&#33616;&#24615;&#33021;&#65292;&#26041;&#27861;&#22312;&#22235;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#37117;&#36229;&#36807;&#20102;&#22522;&#20934;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.04775</link><description>&lt;p&gt;
Adap-$\tau$:&#33258;&#36866;&#24212;&#35843;&#25972;&#23884;&#20837;&#30340;&#24133;&#24230;&#29992;&#20110;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Adap-$\tau$: Adaptively Modulating Embedding Magnitude for Recommendation. (arXiv:2302.04775v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04775
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#24402;&#19968;&#21270;&#26041;&#26696;Adap-$\tau$&#65292;&#36890;&#36807;&#21160;&#24577;&#35843;&#33410;&#27599;&#20010;&#29992;&#25143;-&#27599;&#20010;&#29289;&#21697;&#23545;&#30340;&#23884;&#20837;&#24133;&#24230;&#65292;&#23454;&#29616;&#20102;&#29702;&#24819;&#30340;&#25512;&#33616;&#24615;&#33021;&#65292;&#26041;&#27861;&#22312;&#22235;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#37117;&#36229;&#36807;&#20102;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#26469;&#65292;&#22522;&#20110;&#23884;&#20837;&#30340;&#26041;&#27861;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#12290;&#23613;&#31649;&#23427;&#20204;&#30340;&#24615;&#33021;&#36824;&#19981;&#38169;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#23384;&#22312;&#19968;&#20010;&#28508;&#22312;&#30340;&#38480;&#21046;&#8212;&#8212;&#23884;&#20837;&#24133;&#24230;&#27809;&#26377;&#26126;&#30830;&#35843;&#33410;&#65292;&#36825;&#21487;&#33021;&#21152;&#21095;&#27969;&#34892;&#24230;&#20559;&#35265;&#21644;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#65292;&#20174;&#32780;&#38459;&#30861;&#27169;&#22411;&#20570;&#20986;&#22909;&#30340;&#25512;&#33616;&#12290;&#36825;&#20419;&#20351;&#25105;&#20204;&#21033;&#29992;&#23884;&#20837;&#24402;&#19968;&#21270;&#26469;&#25512;&#33616;&#12290;&#36890;&#36807;&#23558;&#29992;&#25143;/&#29289;&#21697;&#23884;&#20837;&#24402;&#19968;&#21270;&#20026;&#29305;&#23450;&#20540;&#65292;&#25105;&#20204;&#22312;&#22235;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#23454;&#35777;&#35266;&#23519;&#21040;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#25552;&#21319;&#65288;&#24179;&#22343;9&#65285;&#65289;&#12290;&#34429;&#28982;&#36825;&#26159;&#20196;&#20154;&#40723;&#33310;&#30340;&#65292;&#20294;&#25105;&#20204;&#20063;&#25581;&#31034;&#20102;&#22312;&#25512;&#33616;&#20013;&#24212;&#29992;&#24402;&#19968;&#21270;&#30340;&#20005;&#37325;&#23616;&#38480;&#24615;&#8212;&#8212;&#24615;&#33021;&#39640;&#24230;&#25935;&#24863;&#20110;&#25511;&#21046;&#26631;&#20934;&#21270;&#23884;&#20837;&#27604;&#20363;&#30340;&#28201;&#24230;&#964;&#30340;&#36873;&#25321;&#12290;&#20026;&#20102;&#20805;&#20998;&#21457;&#25381;&#24402;&#19968;&#21270;&#30340;&#20248;&#28857;&#24182;&#36991;&#20813;&#20854;&#23616;&#38480;&#24615;&#65292;&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22914;&#20309;&#33258;&#36866;&#24212;&#35774;&#32622;&#36866;&#24403;&#30340;&#964;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#25551;&#36848;&#20102;&#25512;&#33616;&#20013;&#24402;&#19968;&#21270;&#25805;&#20316;&#19982;&#20559;&#24046;-&#26041;&#24046;&#25240;&#34935;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#24402;&#19968;&#21270;&#26041;&#26696;&#65292;&#21517;&#20026;Adap-$\tau$&#65292;&#23427;&#21160;&#24577;&#35843;&#33410;&#27599;&#20010;&#29992;&#25143;-&#27599;&#20010;&#29289;&#21697;&#23545;&#30340;&#23884;&#20837;&#24133;&#24230;&#65292;&#26088;&#22312;&#23454;&#29616;&#29702;&#24819;&#30340;&#25512;&#33616;&#24615;&#33021;&#12290;&#22312;&#22235;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;Adap-$\tau$&#22987;&#32456;&#20248;&#20110;&#24378;&#22522;&#20934;&#26041;&#27861;&#65292;&#24182;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed the great successes of embedding-based methods in recommender systems. Despite their decent performance, we argue one potential limitation of these methods -- the embedding magnitude has not been explicitly modulated, which may aggravate popularity bias and training instability, hindering the model from making a good recommendation. It motivates us to leverage the embedding normalization in recommendation. By normalizing user/item embeddings to a specific value, we empirically observe impressive performance gains (9\% on average) on four real-world datasets. Although encouraging, we also reveal a serious limitation when applying normalization in recommendation -- the performance is highly sensitive to the choice of the temperature $\tau$ which controls the scale of the normalized embeddings.  To fully foster the merits of the normalization while circumvent its limitation, this work studied on how to adaptively set the proper $\tau$. Towards this end, we firs
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#23618;&#30693;&#35782;&#22270;&#35889;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#23884;&#20837;&#65292;&#23558;&#19977;&#20803;&#32452;&#20043;&#38388;&#30340;&#20851;&#31995;&#32771;&#34385;&#36827;&#21435;&#65292;&#24182;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#26469;&#22686;&#21152;&#21512;&#29702;&#30340;&#19977;&#20803;&#32452;&#12290;</title><link>http://arxiv.org/abs/2302.02601</link><description>&lt;p&gt;
&#23398;&#20064;&#21452;&#23618;&#30693;&#35782;&#22270;&#35889;&#30340;&#34920;&#31034;&#20197;&#36827;&#34892;&#36229;&#36234;&#38142;&#25509;&#39044;&#27979;&#30340;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Learning Representations of Bi-level Knowledge Graphs for Reasoning beyond Link Prediction. (arXiv:2302.02601v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02601
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#23618;&#30693;&#35782;&#22270;&#35889;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#23884;&#20837;&#65292;&#23558;&#19977;&#20803;&#32452;&#20043;&#38388;&#30340;&#20851;&#31995;&#32771;&#34385;&#36827;&#21435;&#65292;&#24182;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#26469;&#22686;&#21152;&#21512;&#29702;&#30340;&#19977;&#20803;&#32452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#20351;&#29992;&#19977;&#20803;&#32452;&#26469;&#34920;&#31034;&#24050;&#30693;&#20107;&#23454;&#12290;&#29616;&#26377;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#26041;&#27861;&#20165;&#32771;&#34385;&#23454;&#20307;&#20043;&#38388;&#30340;&#36830;&#25509;&#65292;&#32780;&#26412;&#25991;&#25552;&#20986;&#32771;&#34385;&#19977;&#20803;&#32452;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#26412;&#25991;&#23450;&#20041;&#20102;&#19968;&#20010;&#26356;&#39640;&#32423;&#30340;&#19977;&#20803;&#32452;&#26469;&#34920;&#31034;&#19977;&#20803;&#32452;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20363;&#22914;&#65292;$\langle T_1$, PrerequisiteFor, $T_2\rangle$&#65292;&#20854;&#20013;PrerequisiteFor&#26159;&#26356;&#39640;&#32423;&#21035;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#23450;&#20041;&#19968;&#20010;&#30001;&#22522;&#26412;&#32423;&#21035;&#21644;&#26356;&#39640;&#32423;&#21035;&#30340;&#19977;&#20803;&#32452;&#32452;&#25104;&#30340;&#21452;&#23618;&#30693;&#35782;&#22270;&#35889;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#23618;&#30693;&#35782;&#22270;&#35889;&#19978;&#30340;&#38543;&#26426;&#28216;&#36208;&#30340;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#26469;&#22686;&#21152;&#21512;&#29702;&#30340;&#19977;&#20803;&#32452;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;BiVE&#36890;&#36807;&#32771;&#34385;&#22522;&#26412;&#32423;&#21035;&#21644;&#26356;&#39640;&#32423;&#21035;&#19977;&#20803;&#32452;&#30340;&#32467;&#26500;&#26469;&#23398;&#20064;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graphs represent known facts using triplets. While existing knowledge graph embedding methods only consider the connections between entities, we propose considering the relationships between triplets. For example, let us consider two triplets $T_1$ and $T_2$ where $T_1$ is (Academy_Awards, Nominates, Avatar) and $T_2$ is (Avatar, Wins, Academy_Awards). Given these two base-level triplets, we see that $T_1$ is a prerequisite for $T_2$. In this paper, we define a higher-level triplet to represent a relationship between triplets, e.g., $\langle T_1$, PrerequisiteFor, $T_2\rangle$ where PrerequisiteFor is a higher-level relation. We define a bi-level knowledge graph that consists of the base-level and the higher-level triplets. We also propose a data augmentation strategy based on the random walks on the bi-level knowledge graph to augment plausible triplets. Our model called BiVE learns embeddings by taking into account the structures of the base-level and the higher-level tripl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#25105;&#23545;&#25112;&#25216;&#26415;&#30340;&#20219;&#21153;&#19981;&#21487;&#30693;&#26041;&#27861;&#65292;&#26469;&#35782;&#21035;&#29615;&#22659;&#20013;&#30340;&#35266;&#23519;/&#38544;&#34255;&#29366;&#24577;&#65292;&#24182;&#23558;&#22810;&#26679;&#24615;&#24341;&#20837;&#38750;&#30417;&#30563;&#29615;&#22659;&#35774;&#35745;&#26694;&#26550;&#20013;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#29615;&#22659;&#35774;&#35745;&#30340;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.02119</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#25105;&#23545;&#25112;&#23454;&#29616;&#22810;&#26679;&#21270;&#35825;&#23548;&#30340;&#29615;&#22659;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Diversity Induced Environment Design via Self-Play. (arXiv:2302.02119v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02119
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#25105;&#23545;&#25112;&#25216;&#26415;&#30340;&#20219;&#21153;&#19981;&#21487;&#30693;&#26041;&#27861;&#65292;&#26469;&#35782;&#21035;&#29615;&#22659;&#20013;&#30340;&#35266;&#23519;/&#38544;&#34255;&#29366;&#24577;&#65292;&#24182;&#23558;&#22810;&#26679;&#24615;&#24341;&#20837;&#38750;&#30417;&#30563;&#29615;&#22659;&#35774;&#35745;&#26694;&#26550;&#20013;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#29615;&#22659;&#35774;&#35745;&#30340;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20851;&#20110;&#29615;&#22659;&#20998;&#24067;&#35774;&#35745;&#30340;&#30740;&#31350;&#24050;&#32463;&#23637;&#31034;&#20986;&#35757;&#32451;&#26377;&#25928;&#30340;&#36890;&#29992;&#33021;&#21147;&#20195;&#29702;&#30340;&#21069;&#26223;&#12290;&#23427;&#30340;&#25104;&#21151;&#37096;&#20998;&#22312;&#20110;&#19968;&#31181;&#33258;&#36866;&#24212;&#35838;&#31243;&#23398;&#20064;&#30340;&#24418;&#24335;&#65292;&#35813;&#24418;&#24335;&#36890;&#36807;&#29983;&#25104;&#20195;&#29702;&#33021;&#21147;&#30340;&#21069;&#27839;&#29615;&#22659;&#23454;&#20363;&#65288;&#25110;&#32423;&#21035;&#65289;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#29615;&#22659;&#35774;&#35745;&#26694;&#26550;&#32463;&#24120;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35774;&#35745;&#31354;&#38388;&#20013;&#21457;&#29616;&#26377;&#25928;&#32423;&#21035;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#24182;&#38656;&#35201;&#19982;&#29615;&#22659;&#36827;&#34892;&#39640;&#25104;&#26412;&#20132;&#20114;&#12290;&#26412;&#25991;&#30340;&#30446;&#30340;&#26159;&#22312;&#38750;&#30417;&#30563;&#29615;&#22659;&#35774;&#35745;&#65288;UED&#65289;&#26694;&#26550;&#20013;&#24341;&#20837;&#22810;&#26679;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20219;&#21153;&#19981;&#21487;&#30693;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#23545;&#32473;&#23450;&#32423;&#21035;&#20855;&#26377;&#20195;&#34920;&#24615;&#30340;&#35266;&#23519;/&#38544;&#34255;&#29366;&#24577;&#12290;&#28982;&#21518;&#21033;&#29992;&#36825;&#31181;&#26041;&#27861;&#30340;&#32467;&#26524;&#26469;&#34920;&#24449;&#20004;&#20010;&#32423;&#21035;&#20043;&#38388;&#30340;&#22810;&#26679;&#24615;&#65292;&#27491;&#22914;&#25105;&#20204;&#25152;&#23637;&#31034;&#30340;&#65292;&#36825;&#23545;&#20110;&#26377;&#25928;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#25552;&#39640;&#37319;&#26679;&#25928;&#29575;&#65292;&#25105;&#20204;&#21152;&#20837;&#20102;&#33258;&#25105;&#23545;&#25112;&#25216;&#26415;&#65292;&#20351;&#24471;&#29615;&#22659;&#29983;&#25104;&#22120;&#33021;&#22815;&#33258;&#21160;&#29983;&#25104;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work on designing an appropriate distribution of environments has shown promise for training effective generally capable agents. Its success is partly because of a form of adaptive curriculum learning that generates environment instances (or levels) at the frontier of the agent's capabilities. However, such an environment design framework often struggles to find effective levels in challenging design spaces and requires costly interactions with the environment. In this paper, we aim to introduce diversity in the Unsupervised Environment Design (UED) framework. Specifically, we propose a task-agnostic method to identify observed/hidden states that are representative of a given level. The outcome of this method is then utilized to characterize the diversity between two levels, which as we show can be crucial to effective performance. In addition, to improve sampling efficiency, we incorporate the self-play technique that allows the environment generator to automatically generate e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GAN&#30340;&#31471;&#21040;&#31471;&#20449;&#21495;&#36716;&#21270;&#32593;&#32476;TEGAN&#65292;&#21487;&#20197;&#23558;&#30701;SSVEP&#20449;&#21495;&#36716;&#25442;&#25104;&#38271;&#30340;&#20154;&#24037;SSVEP&#20449;&#21495;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;BCI&#31995;&#32479;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.05599</link><description>&lt;p&gt;
&#22522;&#20110;&#23545;&#25239;&#29983;&#25104;&#32593;&#32476;&#30340;&#30701;SSVEP&#25968;&#25454;&#25193;&#23637;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Short-length SSVEP data extension by a novel generative adversarial networks based framework. (arXiv:2301.05599v3 [q-bio.NC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.05599
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GAN&#30340;&#31471;&#21040;&#31471;&#20449;&#21495;&#36716;&#21270;&#32593;&#32476;TEGAN&#65292;&#21487;&#20197;&#23558;&#30701;SSVEP&#20449;&#21495;&#36716;&#25442;&#25104;&#38271;&#30340;&#20154;&#24037;SSVEP&#20449;&#21495;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;BCI&#31995;&#32479;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;SSVEP&#30340;&#33041;&#26426;&#25509;&#21475;&#22240;&#20854;&#39640;&#20449;&#24687;&#20256;&#36755;&#36895;&#29575;&#21644;&#30446;&#26631;&#25968;&#37327;&#21487;&#29992;&#24615;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#39057;&#29575;&#35782;&#21035;&#26041;&#27861;&#30340;&#24615;&#33021;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#29992;&#25143;&#26657;&#20934;&#25968;&#25454;&#30340;&#25968;&#37327;&#21644;&#25968;&#25454;&#38271;&#24230;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#37096;&#32626;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#24050;&#34987;&#24191;&#27867;&#37319;&#29992;&#26469;&#21019;&#24314;&#21512;&#25104;&#30340;&#33041;&#30005;&#25968;&#25454;&#65292;&#26377;&#26395;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GANs&#30340;&#31471;&#21040;&#31471;&#20449;&#21495;&#36716;&#21270;&#32593;&#32476;TEGAN&#65292;&#29992;&#20110;&#25968;&#25454;&#38271;&#24230;&#25193;&#23637;&#12290;TEGAN&#21487;&#20197;&#23558;&#30701;SSVEP&#20449;&#21495;&#36716;&#25442;&#25104;&#38271;&#30340;&#20154;&#24037;SSVEP&#20449;&#21495;&#12290;&#36890;&#36807;&#23558;&#19968;&#20010;&#26032;&#39062;&#30340;U&#22411;&#29983;&#25104;&#22120;&#26550;&#26500;&#21644;&#19968;&#20010;&#36741;&#21161;&#20998;&#31867;&#22120;&#21152;&#20837;&#21040;&#32593;&#32476;&#32467;&#26500;&#20013;&#65292;TEGAN&#21487;&#20197;&#22312;&#21512;&#25104;&#25968;&#25454;&#20013;&#20135;&#29983;&#26377;&#26465;&#20214;&#30340;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23454;&#29616;&#24182;&#27604;&#36739;&#20102;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;&#39057;&#29575;&#35782;&#21035;&#26041;&#27861;&#65292;&#20197;&#35780;&#20272;TEGAN&#29983;&#25104;&#25968;&#25454;&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;TEGAN&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#30340;&#32447;&#24615;&#25554;&#20540;&#26041;&#27861;&#21644;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;TEGAN&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;BCI&#31995;&#32479;&#30340;&#25928;&#29575;&#65292;&#20943;&#23569;&#25152;&#38656;&#30340;&#26657;&#20934;&#26102;&#38388;&#24182;&#25913;&#21892;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Steady-state visual evoked potentials (SSVEPs) based brain-computer interface (BCI) has received considerable attention due to its high information transfer rate (ITR) and available quantity of targets. However, the performance of frequency identification methods heavily hinges on the amount of user calibration data and data length, which hinders the deployment in real-world applications. Recently, generative adversarial networks (GANs)-based data generation methods have been widely adopted to create synthetic electroencephalography (EEG) data, holds promise to address these issues. In this paper, we proposed a GAN-based end-to-end signal transformation network for data length extension, termed as TEGAN. TEGAN transforms short-length SSVEP signals into long-length artificial SSVEP signals. By incorporating a novel U-Net generator architecture and an auxiliary classifier into the network architecture, the TEGAN could produce conditioned features in the synthetic data. Additionally, we i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#39118;&#38505;&#25935;&#24863;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20351;&#29992;&#25351;&#25968;&#21028;&#25454;&#26469;&#25552;&#39640;&#20854;&#31995;&#32479;&#25239;&#24178;&#25200;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;&#20316;&#32773;&#36827;&#34892;&#20102;&#22312;&#27169;&#25311;&#21644;&#23454;&#38469;&#26426;&#22120;&#20154;&#19978;&#30340;&#23454;&#39564;&#39564;&#35777;&#65292;&#34920;&#26126;&#35813;&#31639;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#21644;&#25191;&#34892;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2212.09010</link><description>&lt;p&gt;
&#39118;&#38505;&#25935;&#24863;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65306;&#25351;&#25968;&#26631;&#20934;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Risk-Sensitive Reinforcement Learning with Exponential Criteria. (arXiv:2212.09010v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09010
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#39118;&#38505;&#25935;&#24863;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20351;&#29992;&#25351;&#25968;&#21028;&#25454;&#26469;&#25552;&#39640;&#20854;&#31995;&#32479;&#25239;&#24178;&#25200;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;&#20316;&#32773;&#36827;&#34892;&#20102;&#22312;&#27169;&#25311;&#21644;&#23454;&#38469;&#26426;&#22120;&#20154;&#19978;&#30340;&#23454;&#39564;&#39564;&#35777;&#65292;&#34920;&#26126;&#35813;&#31639;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#21644;&#25191;&#34892;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#39118;&#38505;&#20013;&#24615;&#30340;&#24378;&#21270;&#23398;&#20064;&#24050;&#32463;&#22312;&#24456;&#22810;&#24212;&#29992;&#20013;&#24471;&#21040;&#20102;&#23454;&#39564;&#25104;&#21151;&#65292;&#20294;&#26159;&#36825;&#31181;&#26041;&#27861;&#23481;&#26131;&#21463;&#21040;&#22122;&#22768;&#21644;&#31995;&#32479;&#21442;&#25968;&#25200;&#21160;&#30340;&#24433;&#21709;&#32780;&#19981;&#22815;&#31283;&#20581;&#12290;&#22240;&#27492;,&#23545;&#39118;&#38505;&#25935;&#24863;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#20197;&#25552;&#39640;&#20854;&#31995;&#32479;&#25239;&#24178;&#25200;&#24615;&#65292;&#26679;&#26412;&#25928;&#29575;&#21644;&#23454;&#29992;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#26080;&#27169;&#22411;&#39118;&#38505;&#25935;&#24863;&#23398;&#20064;&#31639;&#27861;&#65292;&#23558;&#24191;&#27867;&#20351;&#29992;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#36827;&#34892;&#21464;&#20307;&#65292;&#20854;&#23454;&#29616;&#36807;&#31243;&#31867;&#20284;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#25351;&#25968;&#26631;&#20934;&#23545;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#31574;&#30053;&#39118;&#38505;&#25935;&#24863;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#24320;&#21457;&#20102;&#33945;&#29305;&#21345;&#32599;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#21644;&#22312;&#32447;(&#26102;&#38388;&#24046;&#20998;)&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#30340;&#21464;&#20307;&#12290;&#20998;&#26512;&#32467;&#26524;&#34920;&#26126;&#65292;&#25351;&#25968;&#26631;&#20934;&#30340;&#20351;&#29992;&#33021;&#22815;&#25512;&#24191;&#24120;&#29992;&#30340;&#29305;&#23450;&#27491;&#21017;&#21270;&#26041;&#27861;&#12290;&#20316;&#32773;&#22312;&#25670;&#21160;&#26438;&#21644;&#25670;&#25670;&#26438;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#30340;&#23454;&#29616;&#24615;&#33021;&#21644;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
While risk-neutral reinforcement learning has shown experimental success in a number of applications, it is well-known to be non-robust with respect to noise and perturbations in the parameters of the system. For this reason, risk-sensitive reinforcement learning algorithms have been studied to introduce robustness and sample efficiency, and lead to better real-life performance. In this work, we introduce new model-free risk-sensitive reinforcement learning algorithms as variations of widely-used Policy Gradient algorithms with similar implementation properties. In particular, we study the effect of exponential criteria on the risk-sensitivity of the policy of a reinforcement learning agent, and develop variants of the Monte Carlo Policy Gradient algorithm and the online (temporal-difference) Actor-Critic algorithm. Analytical results showcase that the use of exponential criteria generalize commonly used ad-hoc regularization approaches. The implementation, performance, and robustness 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36880;&#28176;&#22686;&#21152;&#23376;&#38598;&#25968;&#37327;&#30340;&#20998;&#21306;&#24207;&#21015;&#30340;&#36890;&#29992;&#30340;&#20998;&#23618;&#23398;&#20064;&#32467;&#26500;&#65292;&#24182;&#20351;&#29992;&#26080;&#26799;&#24230;&#38543;&#26426;&#36924;&#36817;&#26356;&#26032;&#36827;&#34892;&#22312;&#32447;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23450;&#20041;&#20989;&#25968;&#36924;&#36817;&#38382;&#39064;&#24182;&#20351;&#29992;&#21452;&#26102;&#38388;&#23610;&#24230;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#30340;&#29702;&#35770;&#35299;&#20915;&#65292;&#27169;&#25311;&#20102;&#19968;&#31181;&#36864;&#28779;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2212.08189</link><description>&lt;p&gt;
&#22810;&#20998;&#36776;&#29575;&#22312;&#32447;&#30830;&#23450;&#24615;&#36864;&#28779;&#65306;&#19968;&#31181;&#20998;&#23618;&#21644;&#28176;&#36827;&#23398;&#20064;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Multi-Resolution Online Deterministic Annealing: A Hierarchical and Progressive Learning Architecture. (arXiv:2212.08189v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08189
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36880;&#28176;&#22686;&#21152;&#23376;&#38598;&#25968;&#37327;&#30340;&#20998;&#21306;&#24207;&#21015;&#30340;&#36890;&#29992;&#30340;&#20998;&#23618;&#23398;&#20064;&#32467;&#26500;&#65292;&#24182;&#20351;&#29992;&#26080;&#26799;&#24230;&#38543;&#26426;&#36924;&#36817;&#26356;&#26032;&#36827;&#34892;&#22312;&#32447;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23450;&#20041;&#20989;&#25968;&#36924;&#36817;&#38382;&#39064;&#24182;&#20351;&#29992;&#21452;&#26102;&#38388;&#23610;&#24230;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#30340;&#29702;&#35770;&#35299;&#20915;&#65292;&#27169;&#25311;&#20102;&#19968;&#31181;&#36864;&#28779;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26102;&#38388;&#21644;&#35745;&#31639;&#36164;&#28304;&#30340;&#38480;&#21046;&#65292;&#36880;&#27493;&#36924;&#36817;&#22522;&#20110;&#25968;&#25454;&#30340;&#20248;&#21270;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#20998;&#23618;&#23398;&#20064;&#31639;&#27861;&#23545;&#20110;&#20915;&#31574;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#20998;&#23618;&#23398;&#20064;&#32467;&#26500;&#65292;&#22522;&#20110;&#21487;&#33021;&#30340;&#22810;&#20998;&#36776;&#29575;&#25968;&#25454;&#31354;&#38388;&#30340;&#28176;&#36827;&#20998;&#21306;&#12290;&#26368;&#20248;&#20998;&#21306;&#36890;&#36807;&#35299;&#20915;&#19968;&#31995;&#21015;&#20248;&#21270;&#23376;&#38382;&#39064;&#36880;&#27493;&#36924;&#36817;&#65292;&#29983;&#25104;&#20855;&#26377;&#36880;&#28176;&#22686;&#21152;&#30340;&#23376;&#38598;&#25968;&#37327;&#30340;&#20998;&#21306;&#24207;&#21015;&#12290;&#25105;&#20204;&#23637;&#31034;&#23545;&#27599;&#20010;&#20248;&#21270;&#38382;&#39064;&#30340;&#35299;&#21487;&#20197;&#20351;&#29992;&#26080;&#26799;&#24230;&#38543;&#26426;&#36924;&#36817;&#26356;&#26032;&#36827;&#34892;&#22312;&#32447;&#20272;&#35745;&#12290;&#22240;&#27492;&#65292;&#21487;&#20197;&#22312;&#20998;&#21306;&#30340;&#27599;&#20010;&#23376;&#38598;&#20013;&#23450;&#20041;&#20989;&#25968;&#36924;&#36817;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#21452;&#26102;&#38388;&#23610;&#24230;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#30340;&#29702;&#35770;&#35299;&#20915;&#12290;&#36825;&#27169;&#25311;&#20102;&#19968;&#31181;&#36864;&#28779;&#36807;&#31243;&#65292;&#24182;&#23450;&#20041;&#20102;&#19968;&#31181;&#24378;&#22823;&#19988;&#21487;&#35299;&#37322;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#36880;&#27493;&#22686;&#21152;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hierarchical learning algorithms that gradually approximate a solution to a data-driven optimization problem are essential to decision-making systems, especially under limitations on time and computational resources. In this study, we introduce a general-purpose hierarchical learning architecture that is based on the progressive partitioning of a possibly multi-resolution data space. The optimal partition is gradually approximated by solving a sequence of optimization sub-problems that yield a sequence of partitions with increasing number of subsets. We show that the solution of each optimization problem can be estimated online using gradient-free stochastic approximation updates. As a consequence, a function approximation problem can be defined within each subset of the partition and solved using the theory of two-timescale stochastic approximation algorithms. This simulates an annealing process and defines a robust and interpretable heuristic method to gradually increase the complexi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#21453;&#39304;&#30340;&#31574;&#30053;&#36866;&#24212;&#65288;PAFF&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#35753;&#31574;&#30053;&#20351;&#29992;&#38543;&#26426;&#29983;&#25104;&#30340;&#25351;&#20196;&#36827;&#34892;&#28436;&#31034;&#65292;&#24182;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22522;&#30784;&#27169;&#22411;&#25552;&#20379;&#21453;&#39304;&#26469;&#37325;&#26032;&#26631;&#35760;&#28436;&#31034;&#65292;&#33258;&#21160;&#25552;&#20379;&#26032;&#30340;&#28436;&#31034;-&#25351;&#20196;&#25968;&#25454;&#23545;&#36827;&#34892;&#31574;&#30053;&#24494;&#35843;&#65292;&#20197;&#23454;&#29616;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#27867;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;PAFF&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2212.07398</link><description>&lt;p&gt;
&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#21453;&#39304;&#30340;&#31574;&#30053;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Policy Adaptation from Foundation Model Feedback. (arXiv:2212.07398v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.07398
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#21453;&#39304;&#30340;&#31574;&#30053;&#36866;&#24212;&#65288;PAFF&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#35753;&#31574;&#30053;&#20351;&#29992;&#38543;&#26426;&#29983;&#25104;&#30340;&#25351;&#20196;&#36827;&#34892;&#28436;&#31034;&#65292;&#24182;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22522;&#30784;&#27169;&#22411;&#25552;&#20379;&#21453;&#39304;&#26469;&#37325;&#26032;&#26631;&#35760;&#28436;&#31034;&#65292;&#33258;&#21160;&#25552;&#20379;&#26032;&#30340;&#28436;&#31034;-&#25351;&#20196;&#25968;&#25454;&#23545;&#36827;&#34892;&#31574;&#30053;&#24494;&#35843;&#65292;&#20197;&#23454;&#29616;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#27867;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;PAFF&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#26041;&#38754;&#30340;&#36827;&#23637;&#20026;&#26500;&#24314;&#36890;&#29992;&#26426;&#22120;&#20154;&#24102;&#26469;&#20102;&#26174;&#33879;&#36827;&#27493;&#12290;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#23558;&#22330;&#26223;&#21644;&#25351;&#20196;&#32534;&#30721;&#20026;&#20915;&#31574;&#36755;&#20837;&#65292;&#25351;&#20196;&#26465;&#20214;&#21270;&#31574;&#30053;&#21487;&#20197;&#22312;&#19981;&#21516;&#30340;&#23545;&#35937;&#21644;&#20219;&#21153;&#20043;&#38388;&#36827;&#34892;&#27867;&#21270;&#12290;&#23613;&#31649;&#36825;&#26159;&#20196;&#20154;&#40723;&#33310;&#30340;&#65292;&#20294;&#31574;&#30053;&#22312;&#36935;&#21040;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#25110;&#29615;&#22659;&#26102;&#20173;&#28982;&#22833;&#36133;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#21453;&#39304;&#30340;&#31574;&#30053;&#36866;&#24212;&#65288;PAFF&#65289;&#12290;&#24403;&#23558;&#35757;&#32451;&#22909;&#30340;&#31574;&#30053;&#37096;&#32626;&#21040;&#26032;&#20219;&#21153;&#25110;&#26032;&#29615;&#22659;&#26102;&#65292;&#25105;&#20204;&#39318;&#20808;&#35753;&#31574;&#30053;&#20351;&#29992;&#38543;&#26426;&#29983;&#25104;&#30340;&#25351;&#20196;&#36827;&#34892;&#28436;&#31034;&#12290;&#34429;&#28982;&#25191;&#34892;&#21487;&#33021;&#20986;&#29616;&#38169;&#35823;&#65292;&#20294;&#25105;&#20204;&#21487;&#20197;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22522;&#30784;&#27169;&#22411;&#25552;&#20379;&#21453;&#39304;&#26469;&#37325;&#26032;&#26631;&#35760;&#28436;&#31034;&#12290;&#36825;&#33258;&#21160;&#20026;&#31574;&#30053;&#24494;&#35843;&#25552;&#20379;&#20102;&#26032;&#30340;&#28436;&#31034;-&#25351;&#20196;&#25968;&#25454;&#23545;&#12290;&#25105;&#20204;&#22312;&#26426;&#22120;&#20154;&#25805;&#20316;&#35774;&#32622;&#20013;&#36827;&#34892;&#20102;&#21508;&#31181;&#23454;&#39564;&#30340;&#35780;&#20272;&#65292;&#37325;&#28857;&#26159;&#22312;&#26410;&#35265;&#36807;&#30340;&#23545;&#35937;&#12289;&#20219;&#21153;&#21644;&#26410;&#35266;&#23519;&#21040;&#30340;&#29615;&#22659;&#20013;&#30340;&#27867;&#21270;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;PAFF&#22312;&#26368;&#32456;&#20219;&#21153;&#25104;&#21151;&#29575;&#21644;&#35757;&#32451;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent progress on vision-language foundation models have brought significant advancement to building general-purpose robots. By using the pre-trained models to encode the scene and instructions as inputs for decision making, the instruction-conditioned policy can generalize across different objects and tasks. While this is encouraging, the policy still fails in most cases given an unseen task or environment. In this work, we propose Policy Adaptation from Foundation model Feedback (PAFF). When deploying the trained policy to a new task or a new environment, we first let the policy play with randomly generated instructions to record the demonstrations. While the execution could be wrong, we can use the pre-trained foundation models to provide feedback to relabel the demonstrations. This automatically provides new pairs of demonstration-instruction data for policy fine-tuning. We evaluate our method on a broad range of experiments with the focus on generalization on unseen objects, unse
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32454;&#31890;&#24230;&#36873;&#25321;&#24615;&#30456;&#20284;&#24615;&#38598;&#25104;&#26041;&#27861;&#65288;FGS&#65289;&#26469;&#25552;&#39640;&#33647;&#29289;&#38774;&#28857;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#30340;&#25928;&#26524;&#65292;&#37319;&#21462;&#23616;&#37096;&#30340;&#26041;&#27861;&#26469;&#31934;&#32454;&#36873;&#25321;&#24615;&#38598;&#25104;&#33647;&#29289;&#21644;&#38774;&#28857;&#30456;&#20284;&#24615;&#65292;&#24182;&#33258;&#36866;&#24212;&#22320;&#23398;&#20064;&#19981;&#21516;&#30456;&#20284;&#24615;&#35270;&#22270;&#30340;&#26435;&#37325;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;FGS&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#30456;&#20284;&#24615;&#38598;&#25104;&#26041;&#27861;&#36827;&#34892;DTI&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2212.00543</link><description>&lt;p&gt;
&#33647;&#29289;&#38774;&#28857;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#30340;&#32454;&#31890;&#24230;&#36873;&#25321;&#24615;&#30456;&#20284;&#24615;&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
Fine-Grained Selective Similarity Integration for Drug-Target Interaction Prediction. (arXiv:2212.00543v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.00543
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32454;&#31890;&#24230;&#36873;&#25321;&#24615;&#30456;&#20284;&#24615;&#38598;&#25104;&#26041;&#27861;&#65288;FGS&#65289;&#26469;&#25552;&#39640;&#33647;&#29289;&#38774;&#28857;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#30340;&#25928;&#26524;&#65292;&#37319;&#21462;&#23616;&#37096;&#30340;&#26041;&#27861;&#26469;&#31934;&#32454;&#36873;&#25321;&#24615;&#38598;&#25104;&#33647;&#29289;&#21644;&#38774;&#28857;&#30456;&#20284;&#24615;&#65292;&#24182;&#33258;&#36866;&#24212;&#22320;&#23398;&#20064;&#19981;&#21516;&#30456;&#20284;&#24615;&#35270;&#22270;&#30340;&#26435;&#37325;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;FGS&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#30456;&#20284;&#24615;&#38598;&#25104;&#26041;&#27861;&#36827;&#34892;DTI&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33647;&#29289;&#38774;&#28857;&#30456;&#20114;&#20316;&#29992;&#65288;DTIs&#65289;&#30340;&#21457;&#29616;&#26159;&#33647;&#29289;&#24320;&#21457;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#36807;&#31243;&#12290;&#35745;&#31639;&#26041;&#27861;&#26159;&#20174;&#20247;&#22810;&#20505;&#36873;&#39033;&#20013;&#39044;&#27979;&#26032;&#22411;DTIs&#30340;&#26377;&#24076;&#26395;&#21644;&#39640;&#25928;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#32321;&#29712;&#21644;&#26114;&#36149;&#30340;&#28287;&#23454;&#39564;&#12290;&#26368;&#36817;&#65292;&#30001;&#20110;&#26377;&#20102;&#26469;&#33258;&#19981;&#21516;&#25968;&#25454;&#28304;&#30340;&#20016;&#23500;&#24322;&#26500;&#29983;&#29289;&#20449;&#24687;&#65292;&#35745;&#31639;&#26041;&#27861;&#24050;&#32463;&#33021;&#22815;&#21033;&#29992;&#22810;&#20010;&#33647;&#29289;&#21644;&#38774;&#26631;&#30340;&#30456;&#20284;&#24615;&#26469;&#25552;&#39640;DTI&#39044;&#27979;&#24615;&#33021;&#12290;&#30456;&#20284;&#24615;&#38598;&#25104;&#26159;&#19968;&#31181;&#26377;&#25928;&#21644;&#28789;&#27963;&#30340;&#31574;&#30053;&#65292;&#33021;&#22815;&#25552;&#21462;&#34917;&#20805;&#30456;&#20284;&#24615;&#35270;&#22270;&#20013;&#30340;&#20851;&#38190;&#20449;&#24687;&#65292;&#20026;&#20219;&#20309;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;DTI&#39044;&#27979;&#27169;&#22411;&#25552;&#20379;&#21387;&#32553;&#36755;&#20837;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30456;&#20284;&#24615;&#38598;&#25104;&#26041;&#27861;&#20174;&#20840;&#23616;&#35282;&#24230;&#36807;&#28388;&#21644;&#34701;&#21512;&#30456;&#20284;&#24615;&#65292;&#24573;&#30053;&#20102;&#27599;&#20010;&#33647;&#29289;&#21644;&#38774;&#28857;&#30456;&#20284;&#24615;&#35270;&#22270;&#30340;&#23454;&#29992;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;FGS&#30340;&#32454;&#31890;&#24230;&#36873;&#25321;&#24615;&#30456;&#20284;&#24615;&#38598;&#25104;&#26041;&#27861;&#65292;&#37319;&#29992;&#19968;&#31181;&#23616;&#37096;&#30340;&#26041;&#27861;&#26469;&#31934;&#32454;&#36873;&#25321;&#24615;&#38598;&#25104;&#30456;&#20284;&#24615;&#12290;FGS&#22522;&#20110;&#21508;&#33258;&#30340;&#37051;&#22495;&#32467;&#26500;&#38598;&#25104;&#33647;&#29289;&#21644;&#38774;&#28857;&#30456;&#20284;&#24615;&#65292;&#24182;&#20801;&#35768;&#27169;&#22411;&#33258;&#36866;&#24212;&#22320;&#23398;&#20064;&#19981;&#21516;&#30456;&#20284;&#24615;&#35270;&#22270;&#30340;&#26435;&#37325;&#12290;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;FGS&#22987;&#32456;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#30456;&#20284;&#24615;&#38598;&#25104;&#26041;&#27861;&#26469;&#36827;&#34892;DTI&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
The discovery of drug-target interactions (DTIs) is a pivotal process in pharmaceutical development. Computational approaches are a promising and efficient alternative to tedious and costly wet-lab experiments for predicting novel DTIs from numerous candidates. Recently, with the availability of abundant heterogeneous biological information from diverse data sources, computational methods have been able to leverage multiple drug and target similarities to boost the performance of DTI prediction. Similarity integration is an effective and flexible strategy to extract crucial information across complementary similarity views, providing a compressed input for any similarity-based DTI prediction model. However, existing similarity integration methods filter and fuse similarities from a global perspective, neglecting the utility of similarity views for each drug and target. In this study, we propose a Fine-Grained Selective similarity integration approach, called FGS, which employs a local 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#22478;&#24066;&#22320;&#22270;&#19978;&#26381;&#21153;&#20110;&#38543;&#26426;&#20986;&#29616;&#30340;&#35831;&#27714;&#65292;&#21487;&#20197;&#20135;&#29983;&#21327;&#35843;&#20316;&#29992;&#24182;&#32771;&#34385;&#20808;&#21069;&#21487;&#33021;&#20986;&#29616;&#30340;&#26410;&#26469;&#35831;&#27714;&#65292;&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#38656;&#27714;&#20998;&#24067;&#30340;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2211.14983</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#22312;&#33258;&#20027;&#36335;&#30001;&#21644;&#25509;&#36865;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#65292;&#21487;&#36866;&#24212;&#38656;&#27714;&#30340;&#21464;&#21270;
&lt;/p&gt;
&lt;p&gt;
Multiagent Reinforcement Learning for Autonomous Routing and Pickup Problem with Adaptation to Variable Demand. (arXiv:2211.14983v2 [cs.MA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.14983
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#22478;&#24066;&#22320;&#22270;&#19978;&#26381;&#21153;&#20110;&#38543;&#26426;&#20986;&#29616;&#30340;&#35831;&#27714;&#65292;&#21487;&#20197;&#20135;&#29983;&#21327;&#35843;&#20316;&#29992;&#24182;&#32771;&#34385;&#20808;&#21069;&#21487;&#33021;&#20986;&#29616;&#30340;&#26410;&#26469;&#35831;&#27714;&#65292;&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#38656;&#27714;&#20998;&#24067;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25512;&#23548;&#20986;&#19968;&#20010;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#20026;&#19968;&#32452;&#33258;&#20027;&#36710;&#36742;&#22312;&#22478;&#24066;&#22320;&#22270;&#19978;&#26381;&#21153;&#20110;&#38543;&#26426;&#20986;&#29616;&#30340;&#35831;&#27714;&#26102;&#29983;&#25104;&#36335;&#30001;/&#25509;&#36865;&#31574;&#30053;&#12290;&#25105;&#20204;&#30528;&#37325;&#30740;&#31350;&#30340;&#31574;&#30053;&#26159;&#65306;1&#65289;&#20135;&#29983;&#21327;&#35843;&#20316;&#29992;&#65292;&#20174;&#32780;&#20943;&#23569;&#20026;&#26381;&#21153;&#35831;&#27714;&#31561;&#24453;&#30340;&#26102;&#38388;&#65307;2&#65289;&#26159;&#38750;&#36817;&#35270;&#31574;&#30053;&#65292;&#24182;&#32771;&#34385;&#20808;&#21069;&#21487;&#33021;&#20986;&#29616;&#30340;&#26410;&#26469;&#35831;&#27714;&#65307;3&#65289;&#21487;&#20197;&#36866;&#24212;&#22522;&#30784;&#38656;&#27714;&#20998;&#24067;&#30340;&#21464;&#21270;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24863;&#20852;&#36259;&#30340;&#31574;&#30053;&#26159;&#36866;&#24212;&#22478;&#24066;&#29615;&#22659;&#20013;&#23454;&#38469;&#38656;&#27714;&#26465;&#20214;&#30340;&#27874;&#21160;&#65292;&#20363;&#22914;&#39640;&#23792;&#26102;&#38388;&#21644;&#38750;&#39640;&#23792;&#26102;&#38388;&#31561;&#12290;&#25105;&#20204;&#36890;&#36807;&#20197;&#19979;&#26041;&#24335;&#23454;&#29616;&#65306;(i)&#33021;&#22815;&#25913;&#36827;&#31163;&#32447;&#35757;&#32451;&#31574;&#30053;&#24615;&#33021;&#30340;&#22312;&#32447;&#29609;&#31639;&#27861;&#65292;&#21644;(ii)&#19968;&#31181;&#31163;&#32447;&#36924;&#36817;&#26041;&#26696;&#65292;&#20801;&#35768;&#36866;&#24212;&#22522;&#20110;&#38656;&#27714;&#27169;&#22411;&#30340;&#21464;&#21270;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#36890;&#36807;&#35745;&#31639;Wasserstein&#36317;&#31163;&#30340;q-valid&#21322;&#24452;&#26469;&#37327;&#21270;&#26377;&#25928;&#21306;&#22495;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#25105;&#20204;&#24050;&#23398;&#20064;&#31574;&#30053;&#23545;&#19981;&#21516;&#38656;&#27714;&#20998;&#24067;&#30340;&#36866;&#24212;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#21487;&#20197;&#27604;&#22522;&#32447;&#21551;&#21457;&#24335;&#31574;&#30053;&#25913;&#21892;&#24179;&#22343;&#31561;&#24453;&#26102;&#38388;&#65292;&#24182;&#33021;&#22815;&#36866;&#24212;&#19981;&#26029;&#21464;&#21270;&#30340;&#38656;&#27714;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We derive a learning framework to generate routing/pickup policies for a fleet of autonomous vehicles tasked with servicing stochastically appearing requests on a city map. We focus on policies that 1) give rise to coordination amongst the vehicles, thereby reducing wait times for servicing requests, 2) are non-myopic, and consider a-priori potential future requests, 3) can adapt to changes in the underlying demand distribution. Specifically, we are interested in policies that are adaptive to fluctuations of actual demand conditions in urban environments, such as on-peak vs. off-peak hours. We achieve this through a combination of (i) an online play algorithm that improves the performance of an offline-trained policy, and (ii) an offline approximation scheme that allows for adapting to changes in the underlying demand model. In particular, we achieve adaptivity of our learned policy to different demand distributions by quantifying a region of validity using the q-valid radius of a Wass
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24212;&#29992;&#38160;&#24230;&#24863;&#30693;&#35757;&#32451;&#24182;&#23558;&#20854;&#20248;&#21270;&#20026;&#25439;&#22833;&#20540;&#21644;&#25439;&#22833;&#38160;&#24230;&#65292;SAMSON&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#23545;&#22122;&#22768;&#30828;&#20214;&#30340;&#25512;&#26029;&#40065;&#26834;&#24615;&#65292;&#32780;&#26080;&#38656;&#20851;&#20110;&#30446;&#26631;&#30828;&#20214;&#30340;&#20219;&#20309;&#20551;&#35774;&#12290;</title><link>http://arxiv.org/abs/2211.11561</link><description>&lt;p&gt;
SAMSON&#65306;&#38024;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#21644;&#40065;&#26834;&#24615;&#38382;&#39064;&#30340;&#24322;&#24120;&#24402;&#19968;&#21270;&#23610;&#24230;&#19979;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SAMSON: Sharpness-Aware Minimization Scaled by Outlier Normalization for Improving DNN Generalization and Robustness. (arXiv:2211.11561v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11561
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24212;&#29992;&#38160;&#24230;&#24863;&#30693;&#35757;&#32451;&#24182;&#23558;&#20854;&#20248;&#21270;&#20026;&#25439;&#22833;&#20540;&#21644;&#25439;&#22833;&#38160;&#24230;&#65292;SAMSON&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#23545;&#22122;&#22768;&#30828;&#20214;&#30340;&#25512;&#26029;&#40065;&#26834;&#24615;&#65292;&#32780;&#26080;&#38656;&#20851;&#20110;&#30446;&#26631;&#30828;&#20214;&#30340;&#20219;&#20309;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#25928;&#36739;&#39640;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#21152;&#36895;&#22120;&#23481;&#26131;&#20986;&#29616;&#38750;&#29702;&#24819;&#24773;&#20917;&#65292;&#20174;&#32780;&#38477;&#20302;DNN&#30340;&#25512;&#26029;&#24615;&#33021;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#31181;&#24773;&#20917;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#24120;&#22312;&#35757;&#32451;&#26399;&#38388;&#21521;DNN&#26435;&#37325;&#28155;&#21152;&#25200;&#21160;&#65292;&#20197;&#27169;&#25311;&#22122;&#22768;&#30828;&#20214;&#19978;&#30340;&#25512;&#26029;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#36825;&#36890;&#24120;&#38656;&#35201;&#20851;&#20110;&#30446;&#26631;&#30828;&#20214;&#30340;&#30693;&#35782;&#65292;&#24182;&#19988;&#20250;&#23548;&#33268;&#22312;DNN&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#65292;&#38477;&#20302;&#21069;&#32773;&#20197;&#25552;&#39640;&#21518;&#32773;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24212;&#29992;&#38160;&#24230;&#24863;&#30693;&#35757;&#32451;&#65292;&#22312;&#20248;&#21270;&#25439;&#22833;&#20540;&#21644;&#25439;&#22833;&#38160;&#24230;&#30340;&#21516;&#26102;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#23545;&#22122;&#22768;&#30828;&#20214;&#30340;&#25512;&#26029;&#40065;&#26834;&#24615;&#65292;&#32780;&#26080;&#38656;&#20381;&#36182;&#20110;&#26377;&#20851;&#30446;&#26631;&#30828;&#20214;&#30340;&#20219;&#20309;&#20551;&#35774;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;&#38160;&#24230;&#24863;&#30693;&#26041;&#27861;&#65292;&#23427;&#19981;&#20165;&#23558;&#32473;&#23450;&#26435;&#37325;&#30340;&#26368;&#22351;&#24773;&#20917;&#25200;&#21160;&#21462;&#20915;&#20110;&#20854;&#22823;&#23567;&#65292;&#32780;&#19988;&#36824;&#21462;&#20915;&#20110;&#26435;&#37325;&#20998;&#24067;&#30340;&#33539;&#22260;&#12290;&#36825;&#26159;&#36890;&#36807;&#25191;&#34892;&#22312;&#24322;&#24120;&#20540;&#24402;&#19968;&#21270;&#23610;&#24230;&#19979;&#30340;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;&#26469;&#23454;&#29616;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Energy-efficient deep neural network (DNN) accelerators are prone to non-idealities that degrade DNN performance at inference time. To mitigate such degradation, existing methods typically add perturbations to the DNN weights during training to simulate inference on noisy hardware. However, this often requires knowledge about the target hardware and leads to a trade-off between DNN performance and robustness, decreasing the former to increase the latter. In this work, we show that applying sharpness-aware training, by optimizing for both the loss value and loss sharpness, significantly improves robustness to noisy hardware at inference time without relying on any assumptions about the target hardware. In particular, we propose a new adaptive sharpness-aware method that conditions the worst-case perturbation of a given weight not only on its magnitude but also on the range of the weight distribution. This is achieved by performing sharpness-aware minimization scaled by outlier minimizat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#19982;&#21327;&#20316;&#20307;&#39564;&#26234;&#33021;&#20307;&#20132;&#20114;&#25910;&#38598;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#65292;&#24182;&#25910;&#38598;&#20102;&#39318;&#20010;&#20132;&#20114;&#22522;&#30784;&#35821;&#35328;&#29702;&#35299;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2211.06552</link><description>&lt;p&gt;
&#25910;&#38598;&#20132;&#20114;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#20197;&#36827;&#34892;&#22522;&#30784;&#35821;&#35328;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Collecting Interactive Multi-modal Datasets for Grounded Language Understanding. (arXiv:2211.06552v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.06552
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#19982;&#21327;&#20316;&#20307;&#39564;&#26234;&#33021;&#20307;&#20132;&#20114;&#25910;&#38598;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#65292;&#24182;&#25910;&#38598;&#20102;&#39318;&#20010;&#20132;&#20114;&#22522;&#30784;&#35821;&#35328;&#29702;&#35299;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#26234;&#24935;&#33021;&#22815;&#36805;&#36895;&#36866;&#24212;&#26032;&#30340;&#20219;&#21153;&#21644;&#29615;&#22659;&#12290;&#20174;&#24456;&#23567;&#30340;&#26102;&#20505;&#24320;&#22987;&#65292;&#20154;&#31867;&#36890;&#36807;&#27169;&#20223;&#20182;&#20154;&#30340;&#34892;&#20026;&#25110;&#25353;&#29031;&#25552;&#20379;&#30340;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#23398;&#20064;&#26032;&#25216;&#33021;&#24182;&#23398;&#20250;&#35299;&#20915;&#26032;&#20219;&#21153;&#12290;&#20026;&#20102;&#20419;&#36827;&#30740;&#31350;&#33021;&#22815;&#22312;&#26426;&#22120;&#20013;&#23454;&#29616;&#31867;&#20284;&#21151;&#33021;&#30340;&#26041;&#27861;&#65292;&#26412;&#25991;&#20316;&#20986;&#20102;&#20197;&#19979;&#36129;&#29486;&#65306;&#65288;1&#65289;&#24418;&#24335;&#21270;&#21327;&#20316;&#20307;&#39564;&#26234;&#33021;&#20307;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#65307;&#65288;2&#65289;&#24320;&#21457;&#20102;&#19968;&#20010;&#21487;&#36827;&#34892;&#24191;&#27867;&#21644;&#21487;&#25193;&#23637;&#30340;&#25968;&#25454;&#25910;&#38598;&#24037;&#20855;&#65307;&#65288;3&#65289;&#25910;&#38598;&#20102;&#39318;&#20010;&#20132;&#20114;&#22522;&#30784;&#35821;&#35328;&#29702;&#35299;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human intelligence can remarkably adapt quickly to new tasks and environments. Starting from a very young age, humans acquire new skills and learn how to solve new tasks either by imitating the behavior of others or by following provided natural language instructions. To facilitate research which can enable similar capabilities in machines, we made the following contributions (1) formalized the collaborative embodied agent using natural language task; (2) developed a tool for extensive and scalable data collection; and (3) collected the first dataset for interactive grounded language understanding.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#21327;&#22788;&#29702;&#22120;&#65292;&#21033;&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#21644;&#28145;&#24230;&#23398;&#20064;&#23398;&#20064;&#26368;&#20248;&#38381;&#29615;&#21050;&#28608;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#38024;&#23545;&#24615;&#20462;&#22797;&#21644;&#24247;&#22797;&#12290;&#24182;&#21033;&#29992;&#30382;&#23618;&#27169;&#22411;&#30340;&#27169;&#25311;&#65292;&#20026;&#31070;&#32463;&#21327;&#22788;&#29702;&#22120;&#30340;&#26410;&#26469;&#20307;&#20869;&#27979;&#35797;&#22880;&#23450;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2210.11478</link><description>&lt;p&gt;
&#24674;&#22797;&#33041;&#21151;&#33021;&#30340;&#31070;&#32463;&#21327;&#22788;&#29702;&#22120;&#65306;&#22522;&#20110;&#25235;&#21462;&#30340;&#30382;&#23618;&#27169;&#22411;&#30340;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
Neural Co-Processors for Restoring Brain Function: Results from a Cortical Model of Grasping. (arXiv:2210.11478v2 [q-bio.NC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.11478
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#21327;&#22788;&#29702;&#22120;&#65292;&#21033;&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#21644;&#28145;&#24230;&#23398;&#20064;&#23398;&#20064;&#26368;&#20248;&#38381;&#29615;&#21050;&#28608;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#38024;&#23545;&#24615;&#20462;&#22797;&#21644;&#24247;&#22797;&#12290;&#24182;&#21033;&#29992;&#30382;&#23618;&#27169;&#22411;&#30340;&#27169;&#25311;&#65292;&#20026;&#31070;&#32463;&#21327;&#22788;&#29702;&#22120;&#30340;&#26410;&#26469;&#20307;&#20869;&#27979;&#35797;&#22880;&#23450;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;&#35774;&#35745;&#38381;&#29615;&#33041;&#26426;&#25509;&#21475;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#25214;&#21040;&#19981;&#21516;&#21463;&#35797;&#32773;&#21644;&#30446;&#26631;&#30340;&#25345;&#32493;&#31070;&#32463;&#27963;&#21160;&#30340;&#26368;&#20339;&#21050;&#28608;&#27169;&#24335;&#12290;&#26041;&#27861;&#65306;&#20026;&#20102;&#23454;&#29616;&#30446;&#26631;&#23548;&#21521;&#30340;&#38381;&#29615;&#31070;&#32463;&#21050;&#28608;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#31070;&#32463;&#21327;&#22788;&#29702;&#22120;&#8221;&#65292;&#23427;&#20351;&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#21644;&#28145;&#24230;&#23398;&#20064;&#26469;&#23398;&#20064;&#26368;&#20248;&#38381;&#29615;&#21050;&#28608;&#31574;&#30053;&#65292;&#22609;&#36896;&#31070;&#32463;&#27963;&#21160;&#21644;&#36830;&#25509;&#21463;&#25439;&#30340;&#31070;&#32463;&#22238;&#36335;&#36827;&#34892;&#26377;&#38024;&#23545;&#24615;&#30340;&#20462;&#22797;&#21644;&#24247;&#22797;&#12290;&#21327;&#22788;&#29702;&#22120;&#38543;&#30528;&#29983;&#29289;&#30005;&#36335;&#33258;&#36523;&#36866;&#24212;&#21050;&#28608;&#32780;&#35843;&#25972;&#21050;&#28608;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#19968;&#31181;&#33041;-&#35774;&#22791;&#21327;&#21516;&#36866;&#24212;&#24418;&#24335;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20351;&#29992;&#27169;&#25311;&#20026;&#31070;&#32463;&#21327;&#22788;&#29702;&#22120;&#30340;&#26410;&#26469;&#20307;&#20869;&#27979;&#35797;&#22880;&#23450;&#22522;&#30784;&#12290;&#25105;&#20204;&#21033;&#29992;&#19968;&#20010;&#29992;&#20110;&#25235;&#21462;&#30340;&#30382;&#23618;&#27169;&#22411;&#65292;&#23545;&#20854;&#24212;&#29992;&#20102;&#21508;&#31181;&#24418;&#24335;&#30340;&#27169;&#25311;&#25439;&#20260;&#65292;&#20174;&#32780;&#20351;&#25105;&#20204;&#33021;&#22815;&#24320;&#21457;&#20851;&#38190;&#30340;&#23398;&#20064;&#31639;&#27861;&#24182;&#30740;&#31350;&#23545;&#38750;&#31283;&#24577;&#30340;&#36866;&#24212;&#24615;&#12290;&#20027;&#35201;&#32467;&#26524;&#65306;&#25105;&#20204;&#30340;&#27169;&#25311;&#34920;&#26126;&#65292;&#31070;&#32463;&#21327;&#22788;&#29702;&#22120;&#33021;&#22815;&#20174;&#19981;&#21516;&#30340;&#27169;&#25311;&#25613;&#20663;&#20013;&#65292;&#23398;&#20250;&#33258;&#36866;&#24212;&#30340;&#38381;&#29615;&#21050;&#28608;&#31574;&#30053;&#65292;&#24182;&#20419;&#36827;&#31070;&#32463;&#30005;&#36335;&#30340;&#20462;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Objective: A major challenge in designing closed-loop brain-computer interfaces is finding optimal stimulation patterns as a function of ongoing neural activity for different subjects and objectives. Approach: To achieve goal-directed closed-loop neurostimulation, we propose "neural co-processors" which use artificial neural networks and deep learning to learn optimal closed-loop stimulation policies, shaping neural activity and bridging injured neural circuits for targeted repair and rehabilitation. The co-processor adapts the stimulation policy as the biological circuit itself adapts to the stimulation, achieving a form of brain-device co-adaptation. Here we use simulations to lay the groundwork for future in vivo tests of neural co-processors. We leverage a cortical model of grasping, to which we applied various forms of simulated lesions, allowing us to develop the critical learning algorithms and study adaptations to non-stationarity. Main results: Our simulations show the ability
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22635;&#20805;&#24335;&#36974;&#30422;&#31574;&#30053;MixMask&#65292;&#22312;Siamese ConvNets&#20013;&#23454;&#29616;&#36974;&#30422;&#21644;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#30340;&#21305;&#37197;&#65292;&#25552;&#39640;&#20102;Siamese ConvNets&#30340;&#24615;&#33021;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2210.11456</link><description>&lt;p&gt;
MixMask: &#37325;&#26032;&#23457;&#35270;Siamese ConvNets&#30340;&#36974;&#30422;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
MixMask: Revisiting Masking Strategy for Siamese ConvNets. (arXiv:2210.11456v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.11456
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22635;&#20805;&#24335;&#36974;&#30422;&#31574;&#30053;MixMask&#65292;&#22312;Siamese ConvNets&#20013;&#23454;&#29616;&#36974;&#30422;&#21644;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#30340;&#21305;&#37197;&#65292;&#25552;&#39640;&#20102;Siamese ConvNets&#30340;&#24615;&#33021;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#36827;&#23637;&#23558;Masked Image Modeling&#65288;MIM&#65289;&#21644;Siamese&#32593;&#32476;&#25972;&#21512;&#25104;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#20102;&#20004;&#31181;&#25216;&#26415;&#30340;&#20248;&#28857;&#12290;&#28982;&#32780;&#65292;&#22312;Siamese ConvNets&#20013;&#24212;&#29992;&#20256;&#32479;&#30340;&#22522;&#20110;&#25830;&#38500;&#30340;&#36974;&#30422;&#31574;&#30053;&#26102;&#65292;&#23384;&#22312;&#19968;&#20123;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#65292;&#21253;&#25324;&#65288;I&#65289;&#22312;&#36830;&#32493;&#22788;&#29702;&#25968;&#25454;&#26102;&#19981;&#33021;&#25918;&#24323;&#19981;&#30456;&#20851;&#30340;&#36974;&#30422;&#21306;&#22495;&#65292;&#23548;&#33268;&#35757;&#32451;&#25928;&#29575;&#20302;&#20110;ViT&#27169;&#22411;;&#65288;II&#65289;&#22522;&#20110;&#25830;&#38500;&#30340;&#36974;&#30422;&#19982;Siamese ConvNets&#20013;&#30340;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#19981;&#21305;&#37197;&#65292;&#19982;MIM&#26041;&#27861;&#19981;&#21516;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;MixMask&#30340;&#22635;&#20805;&#24335;&#36974;&#30422;&#31574;&#30053;&#65292;&#20197;&#38450;&#27490;&#39321;&#33609;&#36974;&#30422;&#26041;&#27861;&#20013;&#22270;&#20687;&#20013;&#30340;&#38543;&#26426;&#36974;&#30422;&#21306;&#22495;&#23548;&#33268;&#20449;&#24687;&#19981;&#23436;&#25972;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#28789;&#27963;&#30340;&#25439;&#22833;&#20989;&#25968;&#35774;&#35745;&#65292;&#32771;&#34385;&#20004;&#20010;&#19981;&#21516;&#28151;&#21512;&#35270;&#22270;&#20043;&#38388;&#30340;&#35821;&#20041;&#36317;&#31163;&#21464;&#21270;&#65292;&#20197;&#36866;&#24212;&#38598;&#25104;&#26550;&#26500;&#24182;&#38450;&#27490;&#36974;&#30422;&#21644;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;MixMask&#26174;&#30528;&#25552;&#39640;&#20102;Siamese ConvNets&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#20960;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in self-supervised learning have integrated Masked Image Modeling (MIM) and Siamese Networks into a unified framework that leverages the benefits of both techniques. However, several issues remain unaddressed when applying conventional erase-based masking with Siamese ConvNets. These include (I) the inability to drop uninformative masked regions in ConvNets as they process data continuously, resulting in low training efficiency compared to ViT models; and (II) the mismatch between erase-based masking and the contrastive-based objective in Siamese ConvNets, which differs from the MIM approach. In this paper, we propose a filling-based masking strategy called MixMask to prevent information incompleteness caused by the randomly erased regions in an image in the vanilla masking method. Furthermore, we introduce a flexible loss function design that considers the semantic distance change between two different mixed views to adapt the integrated architecture and prevent mismat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#26234;&#33021;&#25216;&#26415;&#20013;&#26085;&#30410;&#26222;&#36941;&#30340;&#25991;&#26412;&#23433;&#20840;&#38382;&#39064;&#65292;&#24182;&#24378;&#35843;&#20102;&#19968;&#20010;&#34987;&#24573;&#35270;&#30340;&#31867;&#21035;&#65306;&#38544;&#34109;&#19981;&#23433;&#20840;&#25991;&#26412;&#12290;&#35813;&#25991;&#25552;&#20986;&#20102;&#32531;&#35299;&#31574;&#30053;&#20197;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#20197;&#25552;&#39640;&#26234;&#33021;&#31995;&#32479;&#20869;&#37096;&#30340;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.09306</link><description>&lt;p&gt;
&#32531;&#35299;&#33258;&#28982;&#35821;&#35328;&#31995;&#32479;&#20013;&#38544;&#34109;&#19981;&#23433;&#20840;&#30340;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
Mitigating Covertly Unsafe Text within Natural Language Systems. (arXiv:2210.09306v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.09306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#26234;&#33021;&#25216;&#26415;&#20013;&#26085;&#30410;&#26222;&#36941;&#30340;&#25991;&#26412;&#23433;&#20840;&#38382;&#39064;&#65292;&#24182;&#24378;&#35843;&#20102;&#19968;&#20010;&#34987;&#24573;&#35270;&#30340;&#31867;&#21035;&#65306;&#38544;&#34109;&#19981;&#23433;&#20840;&#25991;&#26412;&#12290;&#35813;&#25991;&#25552;&#20986;&#20102;&#32531;&#35299;&#31574;&#30053;&#20197;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#20197;&#25552;&#39640;&#26234;&#33021;&#31995;&#32479;&#20869;&#37096;&#30340;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#25216;&#26415;&#20013;&#19968;&#20010;&#26085;&#30410;&#26222;&#36941;&#30340;&#38382;&#39064;&#26159;&#25991;&#26412;&#23433;&#20840;&#24615;&#65292;&#22240;&#20026;&#19981;&#21463;&#25511;&#21046;&#30340;&#31995;&#32479;&#21487;&#33021;&#20250;&#21521;&#29992;&#25143;&#29983;&#25104;&#23548;&#33268;&#20260;&#23475;&#25110;&#23041;&#32961;&#29983;&#21629;&#30340;&#24314;&#35758;&#12290;&#28982;&#32780;&#65292;&#21487;&#33021;&#23548;&#33268;&#36523;&#20307;&#20260;&#23475;&#30340;&#29983;&#25104;&#35821;&#21477;&#30340;&#26126;&#30830;&#31243;&#24230;&#19981;&#21516;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21306;&#20998;&#20102;&#21487;&#33021;&#23548;&#33268;&#36523;&#20307;&#20260;&#23475;&#30340;&#25991;&#26412;&#31867;&#22411;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#23588;&#20854;&#26410;&#34987;&#25506;&#32034;&#30340;&#31867;&#21035;&#65306;&#38544;&#34109;&#19981;&#23433;&#20840;&#25991;&#26412;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#20998;&#35299;&#20102;&#36825;&#20010;&#31867;&#21035;&#24182;&#20998;&#26512;&#20102;&#27599;&#20010;&#23567;&#31867;&#21035;&#20013;&#25991;&#26412;&#30340;&#29983;&#25104;&#26041;&#24335;&#12290;&#26368;&#32456;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#23450;&#20041;&#20102;&#23548;&#33268;&#29289;&#29702;&#20260;&#23475;&#30340;&#38544;&#34109;&#19981;&#23433;&#20840;&#35821;&#35328;&#38382;&#39064;&#65292;&#24182;&#25351;&#20986;&#36825;&#20010;&#24494;&#22937;&#20294;&#21361;&#38505;&#30340;&#38382;&#39064;&#38656;&#35201;&#25104;&#20026;&#30456;&#20851;&#21033;&#30410;&#30456;&#20851;&#32773;&#21644;&#30417;&#31649;&#26426;&#26500;&#30340;&#20248;&#20808;&#32771;&#34385;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#32531;&#35299;&#31574;&#30053;&#65292;&#20197;&#21551;&#21457;&#26410;&#26469;&#30740;&#31350;&#20154;&#21592;&#35299;&#20915;&#36825;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#24110;&#21161;&#25552;&#39640;&#26234;&#33021;&#31995;&#32479;&#20869;&#37096;&#30340;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
An increasingly prevalent problem for intelligent technologies is text safety, as uncontrolled systems may generate recommendations to their users that lead to injury or life-threatening consequences. However, the degree of explicitness of a generated statement that can cause physical harm varies. In this paper, we distinguish types of text that can lead to physical harm and establish one particularly underexplored category: covertly unsafe text. Then, we further break down this category with respect to the system's information and discuss solutions to mitigate the generation of text in each of these subcategories. Ultimately, our work defines the problem of covertly unsafe language that causes physical harm and argues that this subtle yet dangerous issue needs to be prioritized by stakeholders and regulators. We highlight mitigation strategies to inspire future researchers to tackle this challenging problem and help improve safety within smart systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20998;&#26512;&#33310;&#36424;&#21160;&#20316;&#21644;&#29983;&#25104;&#26032;&#21160;&#20316;&#24207;&#21015;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#20063;&#32467;&#21512;&#20102;&#21069;&#20154;&#30340;&#21162;&#21147;&#26469;&#24320;&#21457;&#20986;&#19968;&#22871;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2210.04366</link><description>&lt;p&gt;
&#21033;&#29992;&#20154;&#20307;&#21160;&#20316;&#21512;&#25104;&#36827;&#34892;&#35745;&#31639;&#32534;&#33310;
&lt;/p&gt;
&lt;p&gt;
Computational Choreography using Human Motion Synthesis. (arXiv:2210.04366v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04366
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20998;&#26512;&#33310;&#36424;&#21160;&#20316;&#21644;&#29983;&#25104;&#26032;&#21160;&#20316;&#24207;&#21015;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#20063;&#32467;&#21512;&#20102;&#21069;&#20154;&#30340;&#21162;&#21147;&#26469;&#24320;&#21457;&#20986;&#19968;&#22871;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26159;&#21542;&#24212;&#35813;&#34987;&#35757;&#32451;&#26469;&#20998;&#26512;&#20154;&#20307;&#34920;&#28436;&#33402;&#26415;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#21512;&#25104;&#33402;&#26415;&#20154;&#20307;&#21160;&#20316;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#20154;&#20307;&#36816;&#21160;&#21512;&#25104;&#20013;&#30340;&#38382;&#39064;&#20219;&#21153;&#21253;&#25324;&#39044;&#27979;&#37326;&#22806;&#29615;&#22659;&#20013;&#20154;&#20307;&#36816;&#21160;&#65292;&#20197;&#21450;&#29983;&#25104;&#22522;&#20110;&#36825;&#20123;&#39044;&#27979;&#30340;&#26032;&#21160;&#20316;&#24207;&#21015;&#12290;&#25105;&#20204;&#23558;&#35752;&#35770;&#19968;&#20010;&#38750;&#20256;&#32479;&#30340;&#24212;&#29992;&#28508;&#21147;&#65292;&#21363;&#23558;&#23398;&#20064;&#27169;&#22411;&#24212;&#29992;&#20110;&#39044;&#27979;&#33310;&#36424;&#21160;&#20316;&#12290;&#26368;&#36817;&#26377;&#19968;&#20123;&#26174;&#33879;&#30340;&#21162;&#21147;&#65292;&#20197;&#35745;&#31639;&#30340;&#26041;&#24335;&#20998;&#26512;&#33310;&#36424;&#21160;&#20316;&#65292;&#20363;&#22914;Everybody Dance Now&#65288;EDN&#65289;&#23398;&#20064;&#27169;&#22411;&#21644;Cal Poly&#30805;&#22763;&#35770;&#25991;Take The Lead&#65288;TTL&#65289;&#12290;&#25105;&#20204;&#26377;&#25928;&#22320;&#23558;&#36825;&#20004;&#20010;&#20316;&#21697;&#19982;&#25105;&#20204;&#33258;&#24049;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#32467;&#21512;&#36215;&#26469;&#65292;&#29983;&#25104;&#20102;&#19968;&#31181;&#26032;&#30340;&#33310;&#36424;&#21160;&#20316;&#39044;&#27979;&#31995;&#32479;&#12289;&#22270;&#20687;&#21040;&#22270;&#20687;&#30340;&#36716;&#25442;&#21644;&#35270;&#39057;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Should deep learning models be trained to analyze human performance art? To help answer this question, we explore an application of deep neural networks to synthesize artistic human motion. Problem tasks in human motion synthesis can include predicting the motions of humans in-the-wild, as well as generating new sequences of motions based on said predictions. We will discuss the potential of a less traditional application, where learning models are applied to predicting dance movements. There have been notable, recent efforts to analyze dance movements in a computational light, such as the Everybody Dance Now (EDN) learning model and a Cal Poly master's thesis, Take The Lead (TTL). We have effectively combined these two works along with our own deep neural network to produce a new system for dance motion prediction, image-to-image translation, and video generation.
&lt;/p&gt;</description></item><item><title>TaDaa&#26159;&#19968;&#20010;&#21033;&#29992;&#26368;&#26032;&#30340;Transformer&#27169;&#22411;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#29992;&#20110;&#23458;&#25143;&#25903;&#25345;&#12289;&#24110;&#21161;&#21488;&#21644;&#38382;&#39064;&#30331;&#35760;&#31995;&#32479;&#30340;&#23454;&#26102;&#31080;&#21153;&#20998;&#37197;&#28145;&#24230;&#23398;&#20064;&#33258;&#21160;&#39038;&#38382;&#65292;&#23427;&#21487;&#20197;&#20998;&#37197;&#38382;&#39064;&#32473;&#27491;&#30830;&#30340;&#32452;&#12289;&#20998;&#37197;&#38382;&#39064;&#32473;&#26368;&#20339;&#30340;&#35299;&#20915;&#32773;&#65292;&#24182;&#21521;&#35299;&#20915;&#32773;&#25552;&#20379;&#26368;&#30456;&#20851;&#30340;&#20808;&#21069;&#35299;&#20915;&#30340;&#38382;&#39064;&#65292;&#20854;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#33021;&#22815;&#26497;&#22823;&#22320;&#25552;&#39640;&#24179;&#22343;&#35299;&#20915;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2207.11187</link><description>&lt;p&gt;
TaDaa: &#29992;&#20110;&#23458;&#25143;&#25903;&#25345;&#12289;&#24110;&#21161;&#21488;&#21644;&#38382;&#39064;&#30331;&#35760;&#31995;&#32479;&#30340;&#23454;&#26102;&#31080;&#21153;&#20998;&#37197;&#28145;&#24230;&#23398;&#20064;&#33258;&#21160;&#39038;&#38382;
&lt;/p&gt;
&lt;p&gt;
TaDaa: real time Ticket Assignment Deep learning Auto Advisor for customer support, help desk, and issue ticketing systems. (arXiv:2207.11187v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.11187
&lt;/p&gt;
&lt;p&gt;
TaDaa&#26159;&#19968;&#20010;&#21033;&#29992;&#26368;&#26032;&#30340;Transformer&#27169;&#22411;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#29992;&#20110;&#23458;&#25143;&#25903;&#25345;&#12289;&#24110;&#21161;&#21488;&#21644;&#38382;&#39064;&#30331;&#35760;&#31995;&#32479;&#30340;&#23454;&#26102;&#31080;&#21153;&#20998;&#37197;&#28145;&#24230;&#23398;&#20064;&#33258;&#21160;&#39038;&#38382;&#65292;&#23427;&#21487;&#20197;&#20998;&#37197;&#38382;&#39064;&#32473;&#27491;&#30830;&#30340;&#32452;&#12289;&#20998;&#37197;&#38382;&#39064;&#32473;&#26368;&#20339;&#30340;&#35299;&#20915;&#32773;&#65292;&#24182;&#21521;&#35299;&#20915;&#32773;&#25552;&#20379;&#26368;&#30456;&#20851;&#30340;&#20808;&#21069;&#35299;&#20915;&#30340;&#38382;&#39064;&#65292;&#20854;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#33021;&#22815;&#26497;&#22823;&#22320;&#25552;&#39640;&#24179;&#22343;&#35299;&#20915;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;TaDaa&#65306;&#31080;&#21153;&#20998;&#37197;&#28145;&#24230;&#23398;&#20064;&#33258;&#21160;&#39038;&#38382;&#65292;&#21033;&#29992;&#26368;&#26032;&#30340;Transformer&#27169;&#22411;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#24555;&#36895;&#20998;&#37197;&#32452;&#32455;&#20869;&#30340;&#38382;&#39064;&#65292;&#22914;&#23458;&#25143;&#25903;&#25345;&#12289;&#24110;&#21161;&#21488;&#21644;&#20854;&#20182;&#38382;&#39064;&#30331;&#35760;&#31995;&#32479;&#12290;&#35813;&#39033;&#30446;&#25552;&#20379;&#20197;&#19979;&#21151;&#33021;&#65306;1&#65289;&#23558;&#38382;&#39064;&#20998;&#37197;&#32473;&#27491;&#30830;&#30340;&#32452;&#65307;2&#65289;&#23558;&#38382;&#39064;&#20998;&#37197;&#32473;&#26368;&#20339;&#30340;&#35299;&#20915;&#32773;&#65307;3&#65289;&#21521;&#35299;&#20915;&#32773;&#25552;&#20379;&#26368;&#30456;&#20851;&#30340;&#20808;&#21069;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#21033;&#29992;&#19968;&#20010;&#26679;&#26412;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;3k+&#20010;&#32452;&#21644;10k+&#20010;&#35299;&#20915;&#32773;&#65292;&#23454;&#29616;&#20102;95.2%&#30340;&#21069;&#19977;&#24314;&#35758;&#20934;&#30830;&#29575;&#21644;79.0%&#30340;&#21069;&#20116;&#35299;&#20915;&#32773;&#24314;&#35758;&#20934;&#30830;&#29575;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#39033;&#30740;&#31350;&#23558;&#22823;&#22823;&#25552;&#39640;&#23458;&#25143;&#25903;&#25345;&#12289;&#24110;&#21161;&#21488;&#21644;&#38382;&#39064;&#30331;&#35760;&#31995;&#32479;&#30340;&#24179;&#22343;&#35299;&#20915;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes TaDaa: Ticket Assignment Deep learning Auto Advisor, which leverages the latest Transformers models and machine learning techniques quickly assign issues within an organization, like customer support, help desk and alike issue ticketing systems. The project provides functionality to 1) assign an issue to the correct group, 2) assign an issue to the best resolver, and 3) provide the most relevant previously solved tickets to resolvers. We leverage one ticketing system sample dataset, with over 3k+ groups and over 10k+ resolvers to obtain a 95.2% top 3 accuracy on group suggestions and a 79.0% top 5 accuracy on resolver suggestions. We hope this research will greatly improve average issue resolution time on customer support, help desk, and issue ticketing systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;$L_2$BN&#26041;&#27861;&#65292;&#36890;&#36807;&#31561;&#21270;&#26679;&#26412;&#29305;&#24449;&#30340;$L_2$&#33539;&#25968;&#26469;&#22686;&#24378;&#25209;&#37327;&#24402;&#19968;&#21270;&#65292;&#21487;&#20197;&#22686;&#24378;&#20869;&#31867;&#21035;&#29305;&#24449;&#30340;&#32039;&#20945;&#24615;&#24182;&#25193;&#22823;&#36328;&#31867;&#21035;&#29305;&#24449;&#30340;&#24046;&#24322;&#65292;&#26131;&#20110;&#23454;&#29616;&#65292;&#21487;&#20197;&#29992;&#20316;&#31070;&#32463;&#32593;&#32476;&#30340;&#22522;&#26412;&#24402;&#19968;&#21270;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2207.02625</link><description>&lt;p&gt;
$L_2$BN: &#36890;&#36807;&#31561;&#21270;&#29305;&#24449;&#30340;$L_2$&#33539;&#25968;&#26469;&#22686;&#24378;&#25209;&#37327;&#24402;&#19968;&#21270;
&lt;/p&gt;
&lt;p&gt;
$L_2$BN: Enhancing Batch Normalization by Equalizing the $L_2$ Norms of Features. (arXiv:2207.02625v5 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.02625
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;$L_2$BN&#26041;&#27861;&#65292;&#36890;&#36807;&#31561;&#21270;&#26679;&#26412;&#29305;&#24449;&#30340;$L_2$&#33539;&#25968;&#26469;&#22686;&#24378;&#25209;&#37327;&#24402;&#19968;&#21270;&#65292;&#21487;&#20197;&#22686;&#24378;&#20869;&#31867;&#21035;&#29305;&#24449;&#30340;&#32039;&#20945;&#24615;&#24182;&#25193;&#22823;&#36328;&#31867;&#21035;&#29305;&#24449;&#30340;&#24046;&#24322;&#65292;&#26131;&#20110;&#23454;&#29616;&#65292;&#21487;&#20197;&#29992;&#20316;&#31070;&#32463;&#32593;&#32476;&#30340;&#22522;&#26412;&#24402;&#19968;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an $L_2$BN method to enhance batch normalization by equalizing the $l_2$ norms of sample features, which can strengthen the compactness of intra-class features and enlarge the discrepancy of inter-class features, easy to implement, and can be used as a basic normalization method for neural networks.
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#34920;&#26126;&#65292;&#26679;&#26412;&#29305;&#24449;&#30340;$L_2$&#33539;&#25968;&#24046;&#24322;&#20250;&#22952;&#30861;&#25209;&#37327;&#24402;&#19968;&#21270;&#33719;&#24471;&#26356;&#21152;&#26174;&#33879;&#30340;&#36328;&#31867;&#21035;&#29305;&#24449;&#21644;&#26356;&#21152;&#32039;&#20945;&#30340;&#20869;&#31867;&#21035;&#29305;&#24449;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#35266;&#20294;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#31561;&#21270;&#26679;&#26412;&#29305;&#24449;&#30340;$L_2$&#33539;&#25968;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#22312;&#23558;&#26679;&#26412;&#29305;&#24449;&#36755;&#20837;&#25209;&#37327;&#24402;&#19968;&#21270;&#20043;&#21069;&#23545;&#27599;&#20010;&#26679;&#26412;&#29305;&#24449;&#36827;&#34892;$L_2$&#24402;&#19968;&#21270;&#65292;&#22240;&#27492;&#29305;&#24449;&#20855;&#26377;&#30456;&#21516;&#30340;&#25968;&#37327;&#32423;&#12290;&#30001;&#20110;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;$L_2$&#24402;&#19968;&#21270;&#21644;&#25209;&#37327;&#24402;&#19968;&#21270;&#65292;&#22240;&#27492;&#25105;&#20204;&#23558;&#20854;&#21629;&#21517;&#20026;$L_2$BN&#12290;$L_2$BN&#21487;&#20197;&#22686;&#24378;&#20869;&#31867;&#21035;&#29305;&#24449;&#30340;&#32039;&#20945;&#24615;&#24182;&#25193;&#22823;&#36328;&#31867;&#21035;&#29305;&#24449;&#30340;&#24046;&#24322;&#12290;$L_2$BN&#26131;&#20110;&#23454;&#29616;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#20219;&#20309;&#39069;&#22806;&#21442;&#25968;&#25110;&#36229;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#21457;&#25381;&#20854;&#20316;&#29992;&#12290;&#22240;&#27492;&#65292;&#23427;&#21487;&#20197;&#29992;&#20316;&#31070;&#32463;&#32593;&#32476;&#30340;&#22522;&#26412;&#24402;&#19968;&#21270;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#21508;&#31181;&#27169;&#22411;&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#39564;&#35780;&#20272;&#20102;$L_2$BN&#30340;&#26377;&#25928;&#24615;&#65292;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we show that the difference in $l_2$ norms of sample features can hinder batch normalization from obtaining more distinguished inter-class features and more compact intra-class features. To address this issue, we propose an intuitive but effective method to equalize the $l_2$ norms of sample features. Concretely, we $l_2$-normalize each sample feature before feeding them into batch normalization, and therefore the features are of the same magnitude. Since the proposed method combines the $l_2$ normalization and batch normalization, we name our method $L_2$BN. The $L_2$BN can strengthen the compactness of intra-class features and enlarge the discrepancy of inter-class features. The $L_2$BN is easy to implement and can exert its effect without any additional parameters or hyper-parameters. Therefore, it can be used as a basic normalization method for neural networks. We evaluate the effectiveness of $L_2$BN through extensive experiments with various models on image classif
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20027;&#21160;&#39038;&#38382;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26694;&#26550;&#65292;Ask-AC&#65292;&#23427;&#26367;&#25442;&#20102;&#20256;&#32479;&#30340;&#34987;&#21160;&#30417;&#30563;&#20449;&#21495;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#23450;&#21046;&#21270;&#21644;&#39640;&#25928;&#30340;&#20449;&#24687;&#20132;&#25442;&#65292;&#20854;&#20013;&#30340;&#20004;&#20010;&#20114;&#34917;&#32452;&#20214;&#20801;&#35768;&#20195;&#29702;&#20027;&#21160;&#23547;&#27714;&#39038;&#38382;&#24178;&#39044;&#21644;&#35782;&#21035;&#28431;&#25481;&#30340;&#19981;&#31283;&#23450;&#29366;&#24577;&#12290;</title><link>http://arxiv.org/abs/2207.01955</link><description>&lt;p&gt;
Ask-AC: &#19968;&#31181;&#24490;&#29615;&#20013;&#30340;&#20027;&#21160;&#39038;&#38382;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Ask-AC: An Initiative Advisor-in-the-Loop Actor-Critic Framework. (arXiv:2207.01955v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.01955
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20027;&#21160;&#39038;&#38382;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26694;&#26550;&#65292;Ask-AC&#65292;&#23427;&#26367;&#25442;&#20102;&#20256;&#32479;&#30340;&#34987;&#21160;&#30417;&#30563;&#20449;&#21495;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#23450;&#21046;&#21270;&#21644;&#39640;&#25928;&#30340;&#20449;&#24687;&#20132;&#25442;&#65292;&#20854;&#20013;&#30340;&#20004;&#20010;&#20114;&#34917;&#32452;&#20214;&#20801;&#35768;&#20195;&#29702;&#20027;&#21160;&#23547;&#27714;&#39038;&#38382;&#24178;&#39044;&#21644;&#35782;&#21035;&#28431;&#25481;&#30340;&#19981;&#31283;&#23450;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20132;&#20114;&#24335;&#24378;&#21270;&#23398;&#20064;&#26041;&#26696;&#21462;&#24471;&#20102;&#24456;&#22810;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#30446;&#21069;&#30340;&#26041;&#26696;&#20173;&#28982;&#20381;&#36182;&#20110;&#26469;&#33258;&#39038;&#38382;&#19987;&#23478;&#30340;&#34987;&#21160;&#30417;&#30563;&#20449;&#21495;&#65292;&#24418;&#24335;&#21253;&#25324;&#25345;&#32493;&#30417;&#25511;&#25110;&#39044;&#23450;&#20041;&#35268;&#21017;&#65292;&#36825;&#19981;&#21487;&#36991;&#20813;&#22320;&#23548;&#33268;&#20102;&#19968;&#31181;&#40635;&#28902;&#32780;&#26114;&#36149;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#20027;&#21160;&#39038;&#38382;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26694;&#26550;&#65292;&#31216;&#20026;Ask-AC&#65292;&#23427;&#29992;&#19968;&#20010;&#21452;&#21521;&#30340;&#23398;&#20064;&#32773;&#20027;&#21160;&#26426;&#21046;&#26367;&#25442;&#20102;&#21333;&#21521;&#30340;&#39038;&#38382;&#25351;&#23548;&#26426;&#21046;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23398;&#20064;&#32773;&#21644;&#39038;&#38382;&#20043;&#38388;&#30340;&#23450;&#21046;&#21270;&#21644;&#26377;&#25928;&#30340;&#20449;&#24687;&#20132;&#25442;&#12290;Ask-AC &#30340;&#26680;&#24515;&#26159;&#20004;&#20010;&#20114;&#34917;&#30340;&#32452;&#20214;&#65292;&#20998;&#21035;&#26159;&#21160;&#20316;&#35831;&#27714;&#32773;&#21644;&#33258;&#36866;&#24212;&#29366;&#24577;&#36873;&#25321;&#22120;&#65292;&#21487;&#20197;&#26041;&#20415;&#22320;&#32435;&#20837;&#21508;&#31181;&#31163;&#25955;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26550;&#26500;&#20013;&#12290;&#21069;&#32773;&#20801;&#35768;&#20195;&#29702;&#20027;&#21160;&#23547;&#27714;&#19981;&#30830;&#23450;&#29366;&#24577;&#19979;&#30340;&#39038;&#38382;&#24178;&#39044;&#65292;&#21518;&#32773;&#21017;&#21487;&#20197;&#35782;&#21035;&#28431;&#25481;&#30340;&#19981;&#31283;&#23450;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the promising results achieved, state-of-the-art interactive reinforcement learning schemes rely on passively receiving supervision signals from advisor experts, in the form of either continuous monitoring or pre-defined rules, which inevitably result in a cumbersome and expensive learning process. In this paper, we introduce a novel initiative advisor-in-the-loop actor-critic framework, termed as Ask-AC, that replaces the unilateral advisor-guidance mechanism with a bidirectional learner-initiative one, and thereby enables a customized and efficacious message exchange between learner and advisor. At the heart of Ask-AC are two complementary components, namely action requester and adaptive state selector, that can be readily incorporated into various discrete actor-critic architectures. The former component allows the agent to initiatively seek advisor intervention in the presence of uncertain states, while the latter identifies the unstable states potentially missed by the for
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26234;&#33021;&#37325;&#21551;&#26426;&#21046;&#30340;&#20998;&#24067;&#20272;&#35745;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#22312;&#22522;&#22240;&#28418;&#21464;&#39118;&#38505;&#39640;&#30340;&#24773;&#20917;&#19979;&#20572;&#27490;&#36816;&#34892;&#65292;&#24182;&#23547;&#25214;&#33391;&#22909;&#30340;&#21442;&#25968;&#33539;&#22260;&#20197;&#36816;&#34892;EDA&#65292;&#20174;&#32780;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2206.09090</link><description>&lt;p&gt;
&#20174;&#29702;&#35299;&#22522;&#22240;&#28418;&#21464;&#21040;&#22522;&#20110;&#26234;&#33021;&#37325;&#21551;&#26426;&#21046;&#30340;&#20998;&#24067;&#20272;&#35745;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
From Understanding Genetic Drift to a Smart-Restart Mechanism for Estimation-of-Distribution Algorithms. (arXiv:2206.09090v3 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.09090
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26234;&#33021;&#37325;&#21551;&#26426;&#21046;&#30340;&#20998;&#24067;&#20272;&#35745;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#22312;&#22522;&#22240;&#28418;&#21464;&#39118;&#38505;&#39640;&#30340;&#24773;&#20917;&#19979;&#20572;&#27490;&#36816;&#34892;&#65292;&#24182;&#23547;&#25214;&#33391;&#22909;&#30340;&#21442;&#25968;&#33539;&#22260;&#20197;&#36816;&#34892;EDA&#65292;&#20174;&#32780;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20272;&#35745;&#20998;&#24067;&#31639;&#27861;&#65288;EDAs&#65289;&#26159;&#19968;&#31181;&#20248;&#21270;&#31639;&#27861;&#65292;&#23427;&#20174;&#25628;&#32034;&#31354;&#38388;&#20013;&#23398;&#20064;&#19968;&#20010;&#20998;&#24067;&#65292;&#20174;&#20013;&#21487;&#20197;&#36731;&#26494;&#22320;&#37319;&#26679;&#20986;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22823;&#22810;&#25968;EDAs&#30340;&#20851;&#38190;&#21442;&#25968;&#26159;&#26679;&#26412;&#22823;&#23567;&#65288;&#31181;&#32676;&#22823;&#23567;&#65289;&#12290;&#22914;&#26524;&#31181;&#32676;&#22823;&#23567;&#22826;&#23567;&#65292;&#27010;&#29575;&#27169;&#22411;&#26356;&#26032;&#20165;&#22522;&#20110;&#23569;&#37327;&#26679;&#26412;&#65292;&#23548;&#33268;&#19981;&#24076;&#26395;&#20986;&#29616;&#30340;&#22522;&#22240;&#28418;&#21464;&#25928;&#24212;&#12290;&#31181;&#32676;&#22823;&#23567;&#36807;&#22823;&#20250;&#36991;&#20813;&#36951;&#20256;&#28418;&#21464;&#65292;&#20294;&#20250;&#20943;&#32531;&#36827;&#31243;&#12290;&#22522;&#20110;&#26368;&#36817;&#37327;&#21270;&#20998;&#26512;&#30340;&#31181;&#32676;&#22823;&#23567;&#22914;&#20309;&#23548;&#33268;&#22522;&#22240;&#28418;&#21464;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;EDAs&#30340;&#26234;&#33021;&#37325;&#21551;&#26426;&#21046;&#12290;&#24403;&#22522;&#22240;&#28418;&#21464;&#39118;&#38505;&#24456;&#39640;&#26102;&#20572;&#27490;&#36816;&#34892;&#65292;&#23427;&#20250;&#33258;&#21160;&#22312;&#33391;&#22909;&#30340;&#21442;&#25968;&#33539;&#22260;&#20869;&#36816;&#34892;EDA&#12290;&#36890;&#36807;&#25968;&#23398;&#36816;&#34892;&#26102;&#38388;&#20998;&#26512;&#65292;&#25105;&#20204;&#20026;&#36825;&#31181;&#26234;&#33021;&#37325;&#21551;&#26041;&#26696;&#35777;&#26126;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#24615;&#33021;&#20445;&#35777;&#12290;&#29305;&#21035;&#22320;&#65292;&#36825;&#34920;&#26126;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#22914;&#26524;&#24050;&#30693;&#26368;&#20339;&#30340;&#65288;&#38382;&#39064;&#29305;&#23450;&#30340;&#65289;&#21442;&#25968;&#20540;&#65292;&#37325;&#21551;&#26041;&#26696;&#20250;&#33258;&#21160;&#21457;&#29616;&#36825;&#20123;&#20540;&#65292;&#20174;&#32780;&#23548;&#33268;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimation-of-distribution algorithms (EDAs) are optimization algorithms that learn a distribution on the search space from which good solutions can be sampled easily. A key parameter of most EDAs is the sample size (population size). If the population size is too small, the update of the probabilistic model builds on few samples, leading to the undesired effect of genetic drift. Too large population sizes avoid genetic drift, but slow down the process.  Building on a recent quantitative analysis of how the population size leads to genetic drift, we design a smart-restart mechanism for EDAs. By stopping runs when the risk for genetic drift is high, it automatically runs the EDA in good parameter regimes.  Via a mathematical runtime analysis, we prove a general performance guarantee for this smart-restart scheme. This in particular shows that in many situations where the optimal (problem-specific) parameter values are known, the restart scheme automatically finds these, leading to the a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26435;&#37325;&#31995;&#32479;&#26694;&#26550;&#65292;&#28040;&#38500;&#20102;&#21629;&#39064;&#26694;&#26550;&#20013;&#19981;&#21516;&#20248;&#21270;&#35821;&#21477;&#20043;&#38388;&#30340;&#35821;&#27861;&#24046;&#24322;&#65292;&#20855;&#26377;&#37325;&#35201;&#30340;&#31616;&#21270;&#21644;&#35299;&#37322;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2206.06440</link><description>&lt;p&gt;
&#21629;&#39064;&#26694;&#26550;&#20013;&#30340;&#20248;&#21270;&#38382;&#39064;&#30340;&#25277;&#35937;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
An Abstract View on Optimizations in Propositional Frameworks. (arXiv:2206.06440v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.06440
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26435;&#37325;&#31995;&#32479;&#26694;&#26550;&#65292;&#28040;&#38500;&#20102;&#21629;&#39064;&#26694;&#26550;&#20013;&#19981;&#21516;&#20248;&#21270;&#35821;&#21477;&#20043;&#38388;&#30340;&#35821;&#27861;&#24046;&#24322;&#65292;&#20855;&#26377;&#37325;&#35201;&#30340;&#31616;&#21270;&#21644;&#35299;&#37322;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#21644;&#24037;&#31243;&#39046;&#22495;&#20013;&#23384;&#22312;&#22823;&#37327;&#30340;&#25628;&#32034;&#20248;&#21270;&#38382;&#39064;&#12290;&#20154;&#24037;&#26234;&#33021;&#38271;&#26399;&#20197;&#26469;&#20026;&#35299;&#20915;&#21644;&#24314;&#27169;&#25628;&#32034;&#20248;&#21270;&#38382;&#39064;&#30340;&#25628;&#32034;&#31639;&#27861;&#21644;&#22768;&#26126;&#24335;&#32534;&#31243;&#35821;&#35328;&#30340;&#21457;&#23637;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;&#33258;&#21160;&#25512;&#29702;&#21644;&#30693;&#35782;&#34920;&#31034;&#26159;&#20154;&#24037;&#26234;&#33021;&#30340;&#23376;&#39046;&#22495;&#65292;&#29305;&#21035;&#20851;&#27880;&#36825;&#20123;&#21457;&#23637;&#12290;&#35768;&#22810;&#27969;&#34892;&#30340;&#33258;&#21160;&#25512;&#29702;&#33539;&#24335;&#20026;&#29992;&#25143;&#25552;&#20379;&#25903;&#25345;&#20248;&#21270;&#35821;&#21477;&#30340;&#35821;&#35328;&#65306;&#20363;&#22914;MaxSAT on minone&#25110;&#31572;&#26696;&#38598;&#32534;&#31243;&#12290;&#36825;&#20123;&#33539;&#24335;&#22312;&#20854;&#35821;&#35328;&#21644;&#34920;&#36798;&#35745;&#31639;&#35299;&#30340;&#36136;&#37327;&#26465;&#20214;&#30340;&#26041;&#24335;&#19978;&#23384;&#22312;&#37325;&#22823;&#24046;&#24322;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;&#26435;&#37325;&#31995;&#32479;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#28040;&#38500;&#20102;&#33539;&#24335;&#20043;&#38388;&#30340;&#35821;&#27861;&#24046;&#24322;&#65292;&#24182;&#20801;&#35768;&#25105;&#20204;&#30475;&#21040;&#33539;&#24335;&#25552;&#20379;&#30340;&#20248;&#21270;&#35821;&#21477;&#20043;&#38388;&#30340;&#26412;&#36136;&#30456;&#20284;&#24615;&#21644;&#24046;&#24322;&#24615;&#12290;&#36825;&#31181;&#32479;&#19968;&#30340;&#35266;&#28857;&#20855;&#26377;&#37325;&#35201;&#30340;&#31616;&#21270;&#21644;&#35299;&#37322;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Search-optimization problems are plentiful in scientific and engineering domains. Artificial intelligence has long contributed to the development of search algorithms and declarative programming languages geared toward solving and modeling search-optimization problems. Automated reasoning and knowledge representation are the subfields of AI that are particularly vested in these developments. Many popular automated reasoning paradigms provide users with languages supporting optimization statements: answer set programming or MaxSAT on minone, to name a few. These paradigms vary significantly in their languages and in the ways they express quality conditions on computed solutions. Here we propose a unifying framework of so-called weight systems that eliminates syntactic distinctions between paradigms and allows us to see essential similarities and differences between optimization statements provided by paradigms. This unifying outlook has significant simplifying and explanatory potential 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#27169;&#22359;&#8212;&#8212;GAMR&#65292;&#23427;&#26159;&#19968;&#31181;&#29992;&#20110;(&#35270;&#35273;)&#25512;&#29702;&#30340;&#24341;&#23548;&#24335;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#20197;&#21160;&#24577;&#36873;&#25321;&#20219;&#21153;&#30456;&#20851;&#30340;&#35270;&#35273;&#20449;&#24687;&#24182;&#23558;&#20854;&#36335;&#30001;&#21040;&#35760;&#24518;&#20013;&#26469;&#35299;&#20915;&#22797;&#26434;&#30340;&#35270;&#35273;&#25512;&#29702;&#20219;&#21153;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#25104;&#21151;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2206.04928</link><description>&lt;p&gt;
GAMR: &#19968;&#31181;&#29992;&#20110; (&#35270;&#35273;) &#25512;&#29702;&#30340;&#24341;&#23548;&#24335;&#27880;&#24847;&#21147;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
GAMR: A Guided Attention Model for (visual) Reasoning. (arXiv:2206.04928v5 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.04928
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#27169;&#22359;&#8212;&#8212;GAMR&#65292;&#23427;&#26159;&#19968;&#31181;&#29992;&#20110;(&#35270;&#35273;)&#25512;&#29702;&#30340;&#24341;&#23548;&#24335;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#20197;&#21160;&#24577;&#36873;&#25321;&#20219;&#21153;&#30456;&#20851;&#30340;&#35270;&#35273;&#20449;&#24687;&#24182;&#23558;&#20854;&#36335;&#30001;&#21040;&#35760;&#24518;&#20013;&#26469;&#35299;&#20915;&#22797;&#26434;&#30340;&#35270;&#35273;&#25512;&#29702;&#20219;&#21153;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#25104;&#21151;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#22312;&#28789;&#27963;&#20998;&#26512;&#21644;&#29702;&#35299;&#22797;&#26434;&#30340;&#35270;&#35273;&#22330;&#26223;&#26041;&#38754;&#20173;&#28982;&#20248;&#20110;&#29616;&#20195;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35273;&#25512;&#29702;&#27169;&#22359;&#8212;&#8212;&#24341;&#23548;&#24335;&#27880;&#24847;&#21147;&#27169;&#22411;(GAMR)&#65292;&#23427;&#36890;&#36807;&#36873;&#25321;&#21644;&#36335;&#30001;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#35270;&#35273;&#20449;&#24687;&#21040;&#35760;&#24518;&#20013;&#30340;&#27880;&#24847;&#21147;&#36716;&#31227;&#24207;&#21015;&#26469;&#20307;&#29616;&#20027;&#21160;&#35270;&#35273;&#29702;&#35770;&#12290;&#22312;&#21508;&#31181;&#35270;&#35273;&#25512;&#29702;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#26174;&#31034;&#65292;GAMR&#33021;&#22815;&#20197;&#31283;&#20581;&#19988;&#26679;&#26412;&#39640;&#25928;&#30340;&#26041;&#24335;&#23398;&#20064;&#35270;&#35273;&#20363;&#31243;&#12290;&#27492;&#22806;&#65292;GAMR&#22312;&#20840;&#26032;&#30340;&#25512;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20102;&#38646;&#26679;&#26412;&#27867;&#21270;&#30340;&#33021;&#21147;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#35745;&#31639;&#25903;&#25345;&#65292;&#25903;&#25345;&#35748;&#30693;&#29702;&#35770;&#20551;&#35774;&#38656;&#35201;&#27880;&#24847;&#21147;&#21644;&#35760;&#24518;&#20043;&#38388;&#30340;&#20851;&#38190;&#30456;&#20114;&#20316;&#29992;&#65292;&#20197;&#21160;&#24577;&#22320;&#32500;&#25252;&#21644;&#25805;&#20316;&#20219;&#21153;&#30456;&#20851;&#30340;&#35270;&#35273;&#20449;&#24687;&#26469;&#35299;&#20915;&#22797;&#26434;&#30340;&#35270;&#35273;&#25512;&#29702;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans continue to outperform modern AI systems in their ability to flexibly parse and understand complex visual scenes. Here, we present a novel module for visual reasoning, the Guided Attention Model for (visual) Reasoning (GAMR), which instantiates an active vision theory -- positing that the brain solves complex visual reasoning problems dynamically -- via sequences of attention shifts to select and route task-relevant visual information into memory. Experiments on an array of visual reasoning tasks and datasets demonstrate GAMR's ability to learn visual routines in a robust and sample-efficient manner. In addition, GAMR is shown to be capable of zero-shot generalization on completely novel reasoning tasks. Overall, our work provides computational support for cognitive theories that postulate the need for a critical interplay between attention and memory to dynamically maintain and manipulate task-relevant visual information to solve complex visual reasoning tasks.
&lt;/p&gt;</description></item><item><title>MolScribe&#26159;&#19968;&#31181;&#24378;&#20581;&#30340;&#20998;&#23376;&#32467;&#26500;&#35782;&#21035;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#23558;&#20998;&#23376;&#22270;&#20687;&#36716;&#25442;&#25104;&#22270;&#24418;&#32467;&#26500;&#65292;&#21253;&#25324;&#21407;&#23376;&#21644;&#38190;&#21450;&#20854;&#20960;&#20309;&#24067;&#23616;&#65292;&#24182;&#19988;&#33021;&#22815;&#35782;&#21035;&#25163;&#24615;&#21644;&#25193;&#23637;&#32553;&#20889;&#32467;&#26500;&#12290;&#35813;&#27169;&#22411;&#22312;&#20844;&#20849;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#20934;&#30830;&#24615;&#20026;76-93&#65285;&#65292;&#24182;&#21487;&#36890;&#36807;&#32622;&#20449;&#24230;&#20272;&#35745;&#21644;&#21407;&#23376;&#32423;&#23545;&#40784;&#36827;&#34892;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2205.14311</link><description>&lt;p&gt;
MolScribe&#65306;&#20855;&#26377;&#22270;&#20687;&#21040;&#22270;&#24418;&#29983;&#25104;&#30340;&#24378;&#20581;&#20998;&#23376;&#32467;&#26500;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
MolScribe: Robust Molecular Structure Recognition with Image-To-Graph Generation. (arXiv:2205.14311v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.14311
&lt;/p&gt;
&lt;p&gt;
MolScribe&#26159;&#19968;&#31181;&#24378;&#20581;&#30340;&#20998;&#23376;&#32467;&#26500;&#35782;&#21035;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#23558;&#20998;&#23376;&#22270;&#20687;&#36716;&#25442;&#25104;&#22270;&#24418;&#32467;&#26500;&#65292;&#21253;&#25324;&#21407;&#23376;&#21644;&#38190;&#21450;&#20854;&#20960;&#20309;&#24067;&#23616;&#65292;&#24182;&#19988;&#33021;&#22815;&#35782;&#21035;&#25163;&#24615;&#21644;&#25193;&#23637;&#32553;&#20889;&#32467;&#26500;&#12290;&#35813;&#27169;&#22411;&#22312;&#20844;&#20849;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#20934;&#30830;&#24615;&#20026;76-93&#65285;&#65292;&#24182;&#21487;&#36890;&#36807;&#32622;&#20449;&#24230;&#20272;&#35745;&#21644;&#21407;&#23376;&#32423;&#23545;&#40784;&#36827;&#34892;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#32467;&#26500;&#35782;&#21035;&#26159;&#23558;&#20998;&#23376;&#22270;&#20687;&#32763;&#35793;&#25104;&#20854;&#22270;&#24418;&#32467;&#26500;&#30340;&#20219;&#21153;&#12290;&#21270;&#23398;&#25991;&#29486;&#20013;&#23637;&#31034;&#30340;&#32472;&#22270;&#39118;&#26684;&#21644;&#32422;&#23450;&#30340;&#26174;&#30528;&#21464;&#21270;&#20026;&#33258;&#21160;&#21270;&#27492;&#20219;&#21153;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MolScribe&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#20687;&#21040;&#22270;&#24418;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#26126;&#30830;&#22320;&#39044;&#27979;&#21407;&#23376;&#21644;&#38190;&#20197;&#21450;&#23427;&#20204;&#30340;&#20960;&#20309;&#24067;&#23616;&#26469;&#26500;&#24314;&#20998;&#23376;&#32467;&#26500;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#28789;&#27963;&#22320;&#34701;&#21512;&#20102;&#31526;&#21495;&#21270;&#23398;&#32422;&#26463;&#65292;&#20197;&#35782;&#21035;&#25163;&#24615;&#24182;&#25193;&#23637;&#32553;&#20889;&#32467;&#26500;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#20197;&#22686;&#24378;&#27169;&#22411;&#23545;&#39046;&#22495;&#21464;&#21270;&#30340;&#31283;&#20581;&#24615;&#12290;&#22312;&#21512;&#25104;&#21644;&#29616;&#23454;&#20998;&#23376;&#22270;&#20687;&#30340;&#23454;&#39564;&#20013;&#65292;MolScribe&#26174;&#30528;&#20248;&#20110;&#20197;&#21069;&#30340;&#27169;&#22411;&#65292;&#22312;&#20844;&#20849;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;76-93&#65285;&#30340;&#20934;&#30830;&#24615;&#12290;&#21270;&#23398;&#23478;&#20204;&#20063;&#21487;&#20197;&#36731;&#26494;&#39564;&#35777; MolScribe &#30340;&#39044;&#27979;&#65292;&#35813;&#39044;&#27979;&#21463;&#20854;&#32622;&#20449;&#24230;&#20272;&#35745;&#21644;&#19982;&#36755;&#20837;&#22270;&#20687;&#30340;&#21407;&#23376;&#32423;&#23545;&#40784;&#30340;&#25511;&#21046;&#12290;MolScribe&#26159;&#20844;&#24320;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Molecular structure recognition is the task of translating a molecular image into its graph structure. Significant variation in drawing styles and conventions exhibited in chemical literature poses a significant challenge for automating this task. In this paper, we propose MolScribe, a novel image-to-graph generation model that explicitly predicts atoms and bonds, along with their geometric layouts, to construct the molecular structure. Our model flexibly incorporates symbolic chemistry constraints to recognize chirality and expand abbreviated structures. We further develop data augmentation strategies to enhance the model robustness against domain shifts. In experiments on both synthetic and realistic molecular images, MolScribe significantly outperforms previous models, achieving 76-93% accuracy on public benchmarks. Chemists can also easily verify MolScribe's prediction, informed by its confidence estimation and atom-level alignment with the input image. MolScribe is publicly availa
&lt;/p&gt;</description></item><item><title>DeepGraviLens&#26159;&#19968;&#31181;&#22810;&#27169;&#24577;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#20998;&#31867;&#23646;&#20110;&#19981;&#21516;&#31867;&#22411;&#30340;&#24341;&#21147;&#36879;&#38236;&#25968;&#25454;&#65292;&#20855;&#26377;&#39640;&#31934;&#24230;&#21644;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2205.00701</link><description>&lt;p&gt;
DeepGraviLens&#65306;&#19968;&#31181;&#29992;&#20110;&#20998;&#31867;&#24341;&#21147;&#36879;&#38236;&#25968;&#25454;&#30340;&#22810;&#27169;&#24577;&#32593;&#32476;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
DeepGraviLens: a Multi-Modal Architecture for Classifying Gravitational Lensing Data. (arXiv:2205.00701v3 [astro-ph.IM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.00701
&lt;/p&gt;
&lt;p&gt;
DeepGraviLens&#26159;&#19968;&#31181;&#22810;&#27169;&#24577;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#20998;&#31867;&#23646;&#20110;&#19981;&#21516;&#31867;&#22411;&#30340;&#24341;&#21147;&#36879;&#38236;&#25968;&#25454;&#65292;&#20855;&#26377;&#39640;&#31934;&#24230;&#21644;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24341;&#21147;&#36879;&#38236;&#26159;&#30001;&#22823;&#36136;&#37327;&#29289;&#20307;&#20135;&#29983;&#30340;&#30456;&#23545;&#35770;&#25928;&#24212;&#65292;&#20250;&#24367;&#26354;&#20854;&#21608;&#22260;&#30340;&#26102;&#31354;&#12290;&#36825;&#26159;&#22825;&#20307;&#29289;&#29702;&#23398;&#20013;&#19968;&#20010;&#28145;&#20837;&#30740;&#31350;&#30340;&#35838;&#39064;&#65292;&#20801;&#35768;&#39564;&#35777;&#29702;&#35770;&#30456;&#23545;&#35770;&#32467;&#26524;&#24182;&#30740;&#31350;&#19968;&#20123;&#21542;&#21017;&#19981;&#21487;&#35265;&#30340;&#24494;&#24369;&#22825;&#20307;&#29289;&#20307;&#12290;&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#24050;&#34987;&#24212;&#29992;&#20110;&#25903;&#25345;&#24341;&#21147;&#36879;&#38236;&#29616;&#35937;&#30340;&#20998;&#26512;&#65292;&#36890;&#36807;&#26816;&#27979;&#19982;&#20142;&#24230;&#21464;&#21270;&#26102;&#38388;&#24207;&#21015;&#30456;&#20851;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#20013;&#30340;&#36879;&#38236;&#25928;&#24212;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#35201;&#20040;&#20165;&#32771;&#34385;&#22270;&#20687;&#32780;&#24573;&#30053;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#35201;&#20040;&#22312;&#26368;&#22256;&#38590;&#30340;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#30456;&#23545;&#36739;&#20302;&#30340;&#20934;&#30830;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102; DeepGraviLens&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#32593;&#32476;&#65292;&#29992;&#20110;&#20998;&#31867;&#23646;&#20110;&#19968;&#20010;&#38750;&#36879;&#38236;&#31995;&#32479;&#31867;&#22411;&#21644;&#19977;&#20010;&#36879;&#38236;&#31995;&#32479;&#31867;&#22411;&#30340;&#26102;&#31354;&#25968;&#25454;&#12290;&#23427;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#36229;&#36807;&#24403;&#21069;&#30340; state-of-art &#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#32422; 19% &#21040; 43%&#65292;&#20855;&#20307;&#21462;&#20915;&#20110;&#25152;&#32771;&#34385;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gravitational lensing is the relativistic effect generated by massive bodies, which bend the space-time surrounding them. It is a deeply investigated topic in astrophysics and allows validating theoretical relativistic results and studying faint astrophysical objects that would not be visible otherwise. In recent years Machine Learning methods have been applied to support the analysis of the gravitational lensing phenomena by detecting lensing effects in data sets consisting of images associated with brightness variation time series. However, the state-of-art approaches either consider only images and neglect time-series data or achieve relatively low accuracy on the most difficult data sets. This paper introduces DeepGraviLens, a novel multi-modal network that classifies spatio-temporal data belonging to one non-lensed system type and three lensed system types. It surpasses the current state of the art accuracy results by $\approx$ 19% to $\approx$ 43%, depending on the considered dat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;token-replaced&#26816;&#27979;&#27169;&#22411;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#22120;&#26041;&#27861;&#65292;&#23558;&#20219;&#21153;&#37325;&#26032;&#23450;&#20041;&#20026;token-replaced&#26816;&#27979;&#38382;&#39064;&#65292;&#33021;&#22815;&#20248;&#20110;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#22120;&#12290;</title><link>http://arxiv.org/abs/2203.03235</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;token-replaced&#26816;&#27979;&#27169;&#22411;&#20316;&#20026;&#23569;&#26679;&#26412;&#23398;&#20064;&#22120;
&lt;/p&gt;
&lt;p&gt;
Pre-trained Token-replaced Detection Model as Few-shot Learner. (arXiv:2203.03235v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.03235
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;token-replaced&#26816;&#27979;&#27169;&#22411;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#22120;&#26041;&#27861;&#65292;&#23558;&#20219;&#21153;&#37325;&#26032;&#23450;&#20041;&#20026;token-replaced&#26816;&#27979;&#38382;&#39064;&#65292;&#33021;&#22815;&#20248;&#20110;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#23637;&#31034;&#20986;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#38754;&#20855;&#26377;&#38750;&#20961;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;token-replaced&#26816;&#27979;&#27169;&#22411;&#65288;&#27604;&#22914;ELECTRA&#65289;&#20316;&#20026;&#23569;&#26679;&#26412;&#23398;&#20064;&#22120;&#12290;&#35813;&#26041;&#27861;&#23558;&#20998;&#31867;&#25110;&#22238;&#24402;&#20219;&#21153;&#37325;&#26032;&#23450;&#20041;&#20026;token-replaced&#26816;&#27979;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#20026;&#27599;&#20010;&#20219;&#21153;&#23450;&#20041;&#19968;&#20010;&#27169;&#26495;&#21644;&#26631;&#31614;&#25551;&#36848;&#35789;&#65292;&#24182;&#23558;&#23427;&#20204;&#25918;&#20837;&#36755;&#20837;&#20013;&#24418;&#25104;&#19968;&#20010;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;token-replaced&#26816;&#27979;&#27169;&#22411;&#26469;&#39044;&#27979;&#21738;&#20010;&#26631;&#31614;&#25551;&#36848;&#35789;&#22312;&#25552;&#31034;&#20013;&#26159;&#26368;&#21407;&#22987;&#30340;&#65288;&#21363;&#26368;&#23569;&#26356;&#25913;&#30340;&#65289;&#12290;&#23545;16&#20010;&#25968;&#25454;&#38598;&#30340;&#31995;&#32479;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19968;&#21477;&#35805;&#21644;&#20004;&#21477;&#35805;&#30340;&#23398;&#20064;&#20219;&#21153;&#20013;&#65292;&#37117;&#20248;&#20110;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained masked language models have demonstrated remarkable ability as few-shot learners. In this paper, as an alternative, we propose a novel approach to few-shot learning with pre-trained token-replaced detection models like ELECTRA. In this approach, we reformulate a classification or a regression task as a token-replaced detection problem. Specifically, we first define a template and label description words for each task and put them into the input to form a natural language prompt. Then, we employ the pre-trained token-replaced detection model to predict which label description word is the most original (i.e., least replaced) among all label description words in the prompt. A systematic evaluation on 16 datasets demonstrates that our approach outperforms few-shot learners with pre-trained masked language models in both one-sentence and two-sentence learning tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#35299;&#20915;&#36755;&#20837;&#25200;&#21160;&#30340;&#33030;&#24369;&#24615;&#12289;&#36807;&#24230;&#21442;&#25968;&#21270;&#21644;&#24615;&#33021;&#19981;&#31283;&#23450;&#24615;&#31561;&#25361;&#25112;&#65292;&#20840;&#38754;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#12289;&#31232;&#30095;&#24615;&#21644;&#31283;&#23450;&#24615;&#65292;&#36866;&#29992;&#20110;&#34920;&#26684;&#21644;&#22270;&#20687;&#25968;&#25454;&#38598;&#12290;&#25552;&#20379;&#20102;&#36873;&#25321;&#36866;&#24403;&#30340;&#35757;&#32451;&#25439;&#22833;&#20989;&#25968;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2110.15829</link><description>&lt;p&gt;
&#20840;&#38754;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Holistic Deep Learning. (arXiv:2110.15829v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.15829
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#35299;&#20915;&#36755;&#20837;&#25200;&#21160;&#30340;&#33030;&#24369;&#24615;&#12289;&#36807;&#24230;&#21442;&#25968;&#21270;&#21644;&#24615;&#33021;&#19981;&#31283;&#23450;&#24615;&#31561;&#25361;&#25112;&#65292;&#20840;&#38754;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#12289;&#31232;&#30095;&#24615;&#21644;&#31283;&#23450;&#24615;&#65292;&#36866;&#29992;&#20110;&#34920;&#26684;&#21644;&#22270;&#20687;&#25968;&#25454;&#38598;&#12290;&#25552;&#20379;&#20102;&#36873;&#25321;&#36866;&#24403;&#30340;&#35757;&#32451;&#25439;&#22833;&#20989;&#25968;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a holistic deep learning framework that addresses the challenges of vulnerability to input perturbations, overparametrization, and performance instability from different train-validation splits. The proposed framework improves accuracy, robustness, sparsity, and stability over standard deep learning models, as demonstrated by extensive experiments on both tabular and image data sets. A prescriptive approach is provided to support practitioners in selecting an appropriate training loss function based on their specific objectives.
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20840;&#38754;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#23545;&#36755;&#20837;&#25200;&#21160;&#30340;&#33030;&#24369;&#24615;&#12289;&#36807;&#24230;&#21442;&#25968;&#21270;&#21644;&#26469;&#33258;&#19981;&#21516;&#35757;&#32451;&#39564;&#35777;&#25286;&#20998;&#30340;&#24615;&#33021;&#19981;&#31283;&#23450;&#24615;&#31561;&#25361;&#25112;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#26631;&#20934;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19978;&#20840;&#38754;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#12289;&#31232;&#30095;&#24615;&#21644;&#31283;&#23450;&#24615;&#65292;&#36825;&#22312;&#23545;&#34920;&#26684;&#21644;&#22270;&#20687;&#25968;&#25454;&#38598;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#20013;&#24471;&#21040;&#20102;&#35777;&#26126;&#12290;&#32467;&#26524;&#36827;&#19968;&#27493;&#36890;&#36807;&#28040;&#34701;&#23454;&#39564;&#21644;SHAP&#20540;&#20998;&#26512;&#36827;&#34892;&#39564;&#35777;&#65292;&#25581;&#31034;&#20102;&#19981;&#21516;&#35780;&#20272;&#25351;&#26631;&#20043;&#38388;&#30340;&#20132;&#20114;&#20316;&#29992;&#21644;&#26435;&#34913;&#12290;&#20026;&#20102;&#25903;&#25345;&#23454;&#36341;&#32773;&#24212;&#29992;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#25351;&#23548;&#24615;&#26041;&#27861;&#65292;&#26681;&#25454;&#20182;&#20204;&#30340;&#20855;&#20307;&#30446;&#26631;&#65292;&#25552;&#20379;&#36873;&#25321;&#36866;&#24403;&#30340;&#35757;&#32451;&#25439;&#22833;&#20989;&#25968;&#30340;&#24314;&#35758;&#12290;&#25152;&#26377;&#29992;&#20110;&#37325;&#29616;&#32467;&#26524;&#30340;&#20195;&#30721;&#37117;&#21487;&#20197;&#22312;https://github.com/kimvc7/HDL&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel holistic deep learning framework that simultaneously addresses the challenges of vulnerability to input perturbations, overparametrization, and performance instability from different train-validation splits. The proposed framework holistically improves accuracy, robustness, sparsity, and stability over standard deep learning models, as demonstrated by extensive experiments on both tabular and image data sets. The results are further validated by ablation experiments and SHAP value analysis, which reveal the interactions and trade-offs between the different evaluation metrics. To support practitioners applying our framework, we provide a prescriptive approach that offers recommendations for selecting an appropriate training loss function based on their specific objectives. All the code to reproduce the results can be found at https://github.com/kimvc7/HDL.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#22122;&#22768;&#29305;&#24449;&#30340;&#19978;&#19979;&#25991;&#32447;&#24615;Bandit&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#65292;&#36890;&#36807;&#35266;&#23519;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#36125;&#21494;&#26031;&#31070;&#35861;&#24182;&#24471;&#21040;&#20102;$\tilde{O}(d\sqrt{T})$&#30340;&#36951;&#25022;&#30028;&#12290;</title><link>http://arxiv.org/abs/1703.01347</link><description>&lt;p&gt;
&#24102;&#22122;&#22768;&#29305;&#24449;&#30340;&#19978;&#19979;&#25991;&#32447;&#24615;Bandit&#65306;&#26397;&#21521;&#36125;&#21494;&#26031;&#31070;&#35861;&#21069;&#36827;
&lt;/p&gt;
&lt;p&gt;
Contextual Linear Bandits under Noisy Features: Towards Bayesian Oracles. (arXiv:1703.01347v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1703.01347
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#22122;&#22768;&#29305;&#24449;&#30340;&#19978;&#19979;&#25991;&#32447;&#24615;Bandit&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#65292;&#36890;&#36807;&#35266;&#23519;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#36125;&#21494;&#26031;&#31070;&#35861;&#24182;&#24471;&#21040;&#20102;$\tilde{O}(d\sqrt{T})$&#30340;&#36951;&#25022;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#24102;&#26377;&#22122;&#22768;&#21644;&#32570;&#22833;&#39033;&#30340;&#19978;&#19979;&#25991;&#32447;&#24615;Bandit&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#22122;&#22768;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#22312;&#35266;&#27979;&#22122;&#22768;&#29305;&#24449;&#30340;&#24773;&#20917;&#19979;&#32473;&#20986;&#30340;&#36125;&#21494;&#26031;&#31070;&#35861;&#12290;&#25105;&#20204;&#30340;&#36125;&#21494;&#26031;&#20998;&#26512;&#21457;&#29616;&#65292;&#26368;&#20248;&#20551;&#35774;&#21487;&#33021;&#20250;&#36828;&#31163;&#28508;&#22312;&#30340;&#21487;&#23454;&#29616;&#20989;&#25968;&#65292;&#36825;&#21462;&#20915;&#20110;&#22122;&#22768;&#29305;&#24449;&#65292;&#36825;&#26159;&#39640;&#24230;&#38750;&#30452;&#35266;&#30340;&#65292;&#24182;&#19988;&#22312;&#32463;&#20856;&#30340;&#26080;&#22122;&#22768;&#35774;&#32622;&#19979;&#19981;&#20250;&#21457;&#29983;&#12290;&#36825;&#24847;&#21619;&#30528;&#32463;&#20856;&#26041;&#27861;&#19981;&#33021;&#20445;&#35777;&#38750;&#24179;&#20961;&#30340;&#36951;&#25022;&#30028;&#65288;regret bound&#65289;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#65292;&#26088;&#22312;&#20174;&#36825;&#20010;&#27169;&#22411;&#19979;&#30340;&#35266;&#23519;&#20449;&#24687;&#20013;&#23454;&#29616;&#36125;&#21494;&#26031;&#31070;&#35861;&#65292;&#24403;&#26377;&#22823;&#37327;&#25163;&#33218;&#26102;&#65292;&#21487;&#20197;&#23454;&#29616;$\tilde{O}(d\sqrt{T})$&#36951;&#25022;&#30028;&#12290;&#25105;&#20204;&#20351;&#29992;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#28436;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study contextual linear bandit problems under feature uncertainty; they are noisy with missing entries. To address the challenges of the noise, we analyze Bayesian oracles given observed noisy features. Our Bayesian analysis finds that the optimal hypothesis can be far from the underlying realizability function, depending on the noise characteristics, which are highly non-intuitive and do not occur for classical noiseless setups. This implies that classical approaches cannot guarantee a non-trivial regret bound. Therefore, we propose an algorithm that aims at the Bayesian oracle from observed information under this model, achieving $\tilde{O}(d\sqrt{T})$ regret bound when there is a large number of arms. We demonstrate the proposed algorithm using synthetic and real-world datasets.
&lt;/p&gt;</description></item></channel></rss>