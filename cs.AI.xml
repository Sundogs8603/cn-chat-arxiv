<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#35774;&#35745;&#20102;iScore&#24037;&#20855;&#65292;&#36890;&#36807;&#29992;&#25143;&#20013;&#24515;&#35774;&#35745;&#36807;&#31243;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#21160;&#35780;&#20998;&#25688;&#35201;&#20013;&#30340;&#36879;&#26126;&#24230;&#21644;&#20449;&#20219;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.04760</link><description>&lt;p&gt;
iScore: &#29992;&#20110;&#35299;&#37322;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#33258;&#21160;&#35780;&#20998;&#25688;&#35201;&#30340;&#21487;&#35270;&#21270;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
iScore: Visual Analytics for Interpreting How Language Models Automatically Score Summaries
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04760
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20102;iScore&#24037;&#20855;&#65292;&#36890;&#36807;&#29992;&#25143;&#20013;&#24515;&#35774;&#35745;&#36807;&#31243;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#21160;&#35780;&#20998;&#25688;&#35201;&#20013;&#30340;&#36879;&#26126;&#24230;&#21644;&#20449;&#20219;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26222;&#21450;&#24613;&#21095;&#22686;&#38271;&#65292;&#28608;&#21457;&#20102;&#23398;&#20064;&#24037;&#31243;&#24072;&#23558;&#23427;&#20204;&#32435;&#20837;&#33258;&#36866;&#24212;&#25945;&#32946;&#24037;&#20855;&#20013;&#65292;&#29992;&#20110;&#33258;&#21160;&#35780;&#20998;&#25688;&#35201;&#20889;&#20316;&#12290;&#22312;&#23558;&#23427;&#20204;&#37096;&#32626;&#21040;&#20851;&#38190;&#23398;&#20064;&#29615;&#22659;&#20043;&#21069;&#65292;&#20102;&#35299;&#21644;&#35780;&#20272;LLMs&#33267;&#20851;&#37325;&#35201;&#65292;&#28982;&#32780;&#23427;&#20204;&#25968;&#37327;&#24222;&#22823;&#30340;&#21442;&#25968;&#21644;&#26085;&#30410;&#22686;&#21152;&#30340;&#35268;&#27169;&#38459;&#30861;&#20102;&#36879;&#26126;&#24230;&#65292;&#24403;&#23427;&#20204;&#34920;&#29616;&#19981;&#20339;&#26102;&#36824;&#20250;&#24433;&#21709;&#20449;&#20219;&#12290;&#36890;&#36807;&#19982;&#22810;&#20301;&#26500;&#24314;&#21644;&#37096;&#32626;&#25688;&#35201;&#35780;&#20998;LLMs&#30340;&#23398;&#20064;&#24037;&#31243;&#24072;&#23637;&#24320;&#21327;&#20316;&#30340;&#29992;&#25143;&#20013;&#24515;&#35774;&#35745;&#36807;&#31243;&#65292;&#25105;&#20204;&#30028;&#23450;&#20102;&#35299;&#37322;&#20854;&#27169;&#22411;&#30340;&#22522;&#26412;&#35774;&#35745;&#25361;&#25112;&#21644;&#30446;&#26631;&#65292;&#21253;&#25324;&#25972;&#21512;&#22823;&#37327;&#25991;&#26412;&#36755;&#20837;&#12289;&#36319;&#36394;&#24471;&#20998;&#26469;&#28304;&#20197;&#21450;&#25193;&#23637;LLM&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#12290;&#20026;&#20102;&#35299;&#20915;&#20182;&#20204;&#30340;&#20851;&#20999;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;iScore&#65292;&#19968;&#27454;&#20114;&#21160;&#24335;&#21487;&#35270;&#21270;&#20998;&#26512;&#24037;&#20855;&#65292;&#20379;&#23398;&#20064;&#24037;&#31243;&#24072;&#21516;&#26102;&#19978;&#20256;&#12289;&#35780;&#20998;&#21644;&#27604;&#36739;&#22810;&#20010;&#25688;&#35201;&#12290;&#19982;&#27492;&#21516;&#26102;&#32039;&#23494;&#38598;&#25104;&#30340;&#35270;&#22270;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04760v1 Announce Type: cross  Abstract: The recent explosion in popularity of large language models (LLMs) has inspired learning engineers to incorporate them into adaptive educational tools that automatically score summary writing. Understanding and evaluating LLMs is vital before deploying them in critical learning environments, yet their unprecedented size and expanding number of parameters inhibits transparency and impedes trust when they underperform. Through a collaborative user-centered design process with several learning engineers building and deploying summary scoring LLMs, we characterized fundamental design challenges and goals around interpreting their models, including aggregating large text inputs, tracking score provenance, and scaling LLM interpretability methods. To address their concerns, we developed iScore, an interactive visual analytics tool for learning engineers to upload, score, and compare multiple summaries simultaneously. Tightly integrated views
&lt;/p&gt;</description></item><item><title>KnowledgeVIS&#26159;&#19968;&#20010;&#20154;&#26426;&#21327;&#20316;&#30340;&#21487;&#35270;&#20998;&#26512;&#31995;&#32479;&#65292;&#21487;&#20197;&#36890;&#36807;&#27604;&#36739;&#22635;&#31354;&#25552;&#31034;&#20043;&#38388;&#30340;&#39044;&#27979;&#32467;&#26524;&#26469;&#25581;&#31034;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#21040;&#30340;&#32852;&#31995;&#65292;&#24110;&#21161;&#29992;&#25143;&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;&#30340;&#24037;&#20316;&#26041;&#24335;&#21450;&#21407;&#22240;&#12290;</title><link>https://arxiv.org/abs/2403.04758</link><description>&lt;p&gt;
&#36890;&#36807;&#27604;&#36739;&#22635;&#31354;&#25552;&#31034;&#35299;&#37322;&#35821;&#35328;&#27169;&#22411;&#30340;KnowledgeVIS
&lt;/p&gt;
&lt;p&gt;
KnowledgeVIS: Interpreting Language Models by Comparing Fill-in-the-Blank Prompts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04758
&lt;/p&gt;
&lt;p&gt;
KnowledgeVIS&#26159;&#19968;&#20010;&#20154;&#26426;&#21327;&#20316;&#30340;&#21487;&#35270;&#20998;&#26512;&#31995;&#32479;&#65292;&#21487;&#20197;&#36890;&#36807;&#27604;&#36739;&#22635;&#31354;&#25552;&#31034;&#20043;&#38388;&#30340;&#39044;&#27979;&#32467;&#26524;&#26469;&#25581;&#31034;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#21040;&#30340;&#32852;&#31995;&#65292;&#24110;&#21161;&#29992;&#25143;&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;&#30340;&#24037;&#20316;&#26041;&#24335;&#21450;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27969;&#34892;&#22686;&#38271;&#23548;&#33268;&#23427;&#20204;&#22312;&#24635;&#32467;&#12289;&#39044;&#27979;&#21644;&#29983;&#25104;&#25991;&#26412;&#26041;&#38754;&#30340;&#20351;&#29992;&#22686;&#21152;&#65292;&#22240;&#27492;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#21644;&#24037;&#31243;&#24072;&#29702;&#35299;&#20854;&#24037;&#20316;&#26041;&#24335;&#21450;&#21407;&#22240;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;KnowledgeVis&#65292;&#36825;&#26159;&#19968;&#20010;&#20154;&#26426;&#21327;&#20316;&#30340;&#21487;&#35270;&#20998;&#26512;&#31995;&#32479;&#65292;&#29992;&#20110;&#20351;&#29992;&#22635;&#31354;&#21477;&#20316;&#20026;&#25552;&#31034;&#26469;&#35299;&#37322;&#35821;&#35328;&#27169;&#22411;&#12290;&#36890;&#36807;&#27604;&#36739;&#21477;&#23376;&#20043;&#38388;&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;KnowledgeVis&#23637;&#31034;&#20102;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#21040;&#30340;&#32852;&#31995;&#65292;&#36825;&#30452;&#35266;&#22320;&#23558;&#35821;&#35328;&#27169;&#22411;&#23398;&#21040;&#30340;&#20869;&#23481;&#19982;&#33258;&#28982;&#35821;&#35328;&#19979;&#28216;&#20219;&#21153;&#32852;&#31995;&#36215;&#26469;&#65292;&#24110;&#21161;&#29992;&#25143;&#21019;&#24314;&#21644;&#27979;&#35797;&#22810;&#20010;&#25552;&#31034;&#21464;&#20307;&#65292;&#20351;&#29992;&#26032;&#39062;&#30340;&#35821;&#20041;&#32858;&#31867;&#25216;&#26415;&#20998;&#26512;&#39044;&#27979;&#30340;&#21333;&#35789;&#65292;&#24182;&#20351;&#29992;&#20114;&#21160;&#21487;&#35270;&#21270;&#21457;&#29616;&#35265;&#35299;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#36825;&#20123;&#21487;&#35270;&#21270;&#24110;&#21161;&#29992;&#25143;&#30830;&#23450;&#21333;&#20010;&#39044;&#27979;&#30340;&#21487;&#33021;&#24615;&#21644;&#29420;&#29305;&#24615;&#65292;&#27604;&#36739;&#19981;&#21516;&#25552;&#31034;&#20043;&#38388;&#30340;&#19968;&#32452;&#39044;&#27979;&#65292;&#24182;&#24635;&#32467;&#27169;&#24335;&#21644;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04758v1 Announce Type: cross  Abstract: Recent growth in the popularity of large language models has led to their increased usage for summarizing, predicting, and generating text, making it vital to help researchers and engineers understand how and why they work. We present KnowledgeVis, a human-in-the-loop visual analytics system for interpreting language models using fill-in-the-blank sentences as prompts. By comparing predictions between sentences, KnowledgeVis reveals learned associations that intuitively connect what language models learn during training to natural language tasks downstream, helping users create and test multiple prompt variations, analyze predicted words using a novel semantic clustering technique, and discover insights using interactive visualizations. Collectively, these visualizations help users identify the likelihood and uniqueness of individual predictions, compare sets of predictions between prompts, and summarize patterns and relationships betw
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#20445;&#25345;&#26041;&#24046;&#30340;&#32858;&#21512;&#20989;&#25968;&#65288;VPA&#65289;&#65292;&#35813;&#20989;&#25968;&#22312;&#32500;&#25345;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#34920;&#36798;&#33021;&#21147;&#30340;&#22522;&#30784;&#19978;&#65292;&#25552;&#39640;&#20102;&#21069;&#21521;&#21644;&#21518;&#21521;&#21160;&#21147;&#23398;&#65292;&#36827;&#32780;&#23548;&#33268;&#20102;&#22686;&#24378;&#30340;&#39044;&#27979;&#24615;&#33021;&#21644;&#25913;&#21892;&#30340;&#23398;&#20064;&#21160;&#24577;&#12290;</title><link>https://arxiv.org/abs/2403.04747</link><description>&lt;p&gt;
GNN-VPA: &#19968;&#31181;&#29992;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#24046;&#20445;&#25345;&#32858;&#21512;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
GNN-VPA: A Variance-Preserving Aggregation Strategy for Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04747
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#20445;&#25345;&#26041;&#24046;&#30340;&#32858;&#21512;&#20989;&#25968;&#65288;VPA&#65289;&#65292;&#35813;&#20989;&#25968;&#22312;&#32500;&#25345;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#34920;&#36798;&#33021;&#21147;&#30340;&#22522;&#30784;&#19978;&#65292;&#25552;&#39640;&#20102;&#21069;&#21521;&#21644;&#21518;&#21521;&#21160;&#21147;&#23398;&#65292;&#36827;&#32780;&#23548;&#33268;&#20102;&#22686;&#24378;&#30340;&#39044;&#27979;&#24615;&#33021;&#21644;&#25913;&#21892;&#30340;&#23398;&#20064;&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#65292;&#29305;&#21035;&#26159;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65292;&#22312;&#29289;&#29702;&#23398;&#12289;&#33647;&#29289;&#21457;&#29616;&#21644;&#20998;&#23376;&#24314;&#27169;&#31561;&#21508;&#20010;&#39046;&#22495;&#34920;&#29616;&#20986;&#33394;&#12290;GNNs&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#21306;&#20998;&#38750;&#21516;&#26500;&#22270;&#30340;&#33021;&#21147;&#65292;&#20851;&#38190;&#21462;&#20915;&#20110;&#29992;&#20110;&#28040;&#24687;&#32858;&#21512;&#21644;&#22270;&#32423;&#35835;&#20986;&#30340;&#20989;&#25968;&#12290;&#36890;&#36807;&#24212;&#29992;&#20449;&#21495;&#20256;&#25773;&#29702;&#35770;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20445;&#25345;&#26041;&#24046;&#30340;&#32858;&#21512;&#20989;&#25968;&#65288;VPA&#65289;&#65292;&#35813;&#20989;&#25968;&#20445;&#25345;&#20102;&#34920;&#36798;&#33021;&#21147;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#21069;&#21521;&#21644;&#21518;&#21521;&#21160;&#21147;&#23398;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;VPA&#23548;&#33268;&#20102;&#27969;&#34892;&#30340;GNN&#26550;&#26500;&#30340;&#39044;&#27979;&#24615;&#33021;&#25552;&#39640;&#65292;&#21516;&#26102;&#25913;&#21892;&#20102;&#23398;&#20064;&#21160;&#24577;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#21487;&#33021;&#20026;&#26080;&#24402;&#19968;&#21270;&#25110;&#33258;&#24402;&#19968;&#21270;&#30340;GNNs&#38138;&#24179;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04747v1 Announce Type: cross  Abstract: Graph neural networks (GNNs), and especially message-passing neural networks, excel in various domains such as physics, drug discovery, and molecular modeling. The expressivity of GNNs with respect to their ability to discriminate non-isomorphic graphs critically depends on the functions employed for message aggregation and graph-level readout. By applying signal propagation theory, we propose a variance-preserving aggregation function (VPA) that maintains expressivity, but yields improved forward and backward dynamics. Experiments demonstrate that VPA leads to increased predictive performance for popular GNN architectures as well as improved learning dynamics. Our results could pave the way towards normalizer-free or self-normalizing GNNs.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#29983;&#29289;&#21551;&#21457;&#30340;&#26041;&#27861;&#65292;&#21363;&#27169;&#25311;&#35797;&#38169;&#65288;STE&#65289;&#65292;&#20026;&#24037;&#20855;&#22686;&#24378;&#22411;LLMs&#32534;&#25490;&#20102;&#19977;&#20010;&#20851;&#38190;&#26426;&#21046;&#20197;&#23454;&#29616;&#25104;&#21151;&#30340;&#24037;&#20855;&#20351;&#29992;&#34892;&#20026;</title><link>https://arxiv.org/abs/2403.04746</link><description>&lt;p&gt;
&#22312;&#24187;&#22659;&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#36890;&#36807;&#27169;&#25311;&#35797;&#38169;&#23398;&#20064;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
LLMs in the Imaginarium: Tool Learning through Simulated Trial and Error
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04746
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#29983;&#29289;&#21551;&#21457;&#30340;&#26041;&#27861;&#65292;&#21363;&#27169;&#25311;&#35797;&#38169;&#65288;STE&#65289;&#65292;&#20026;&#24037;&#20855;&#22686;&#24378;&#22411;LLMs&#32534;&#25490;&#20102;&#19977;&#20010;&#20851;&#38190;&#26426;&#21046;&#20197;&#23454;&#29616;&#25104;&#21151;&#30340;&#24037;&#20855;&#20351;&#29992;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#20855;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33719;&#21462;&#26368;&#26032;&#20449;&#24687;&#24182;&#22312;&#22806;&#37096;&#29615;&#22659;&#20013;&#37319;&#21462;&#37325;&#35201;&#34892;&#21160;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#20851;&#20110;&#24037;&#20855;&#22686;&#24378;&#22411;LLMs&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#24037;&#20855;&#30340;&#24191;&#27867;&#35206;&#30422;&#33539;&#22260;&#21644;&#28789;&#27963;&#24615;&#19978;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#34987;&#20154;&#24847;&#22806;&#24573;&#35270;&#30340;&#20851;&#38190;&#26041;&#38754;&#26159;LLM&#22312;&#32463;&#36807;&#35757;&#32451;&#21518;&#22914;&#20309;&#20934;&#30830;&#20351;&#29992;&#24037;&#20855;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21253;&#25324;GPT-4&#21644;&#19987;&#38376;&#20026;&#24037;&#20855;&#20351;&#29992;&#36827;&#34892;&#24494;&#35843;&#30340;&#24320;&#28304;LLMs&#22312;&#27491;&#30830;&#29575;&#26041;&#38754;&#20165;&#36798;&#21040;30%&#21040;60%&#30340;&#33539;&#22260;&#65292;&#36828;&#19981;&#36275;&#20197;&#22312;&#23454;&#36341;&#20013;&#21487;&#38752;&#20351;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#29983;&#29289;&#21551;&#21457;&#30340;&#26041;&#27861;&#65292;&#21363;&#27169;&#25311;&#35797;&#38169;&#65288;STE&#65289;&#65292;&#20026;&#24037;&#20855;&#22686;&#24378;&#22411;LLMs&#32534;&#25490;&#20102;&#19977;&#20010;&#20851;&#38190;&#26426;&#21046;&#20197;&#23454;&#29616;&#25104;&#21151;&#30340;&#24037;&#20855;&#20351;&#29992;&#34892;&#20026;&#65306;&#35797;&#38169;&#12289;&#24819;&#35937;&#21644;&#35760;&#24518;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;STE&#21033;&#29992;LLM&#30340;&#8220;&#24819;&#35937;&#21147;&#8221;&#26469;&#27169;&#25311;&#20351;&#29992;&#24037;&#20855;&#30340;&#21487;&#33021;&#22330;&#26223;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04746v1 Announce Type: cross  Abstract: Tools are essential for large language models (LLMs) to acquire up-to-date information and take consequential actions in external environments. Existing work on tool-augmented LLMs primarily focuses on the broad coverage of tools and the flexibility of adding new tools. However, a critical aspect that has surprisingly been understudied is simply how accurately an LLM uses tools for which it has been trained. We find that existing LLMs, including GPT-4 and open-source LLMs specifically fine-tuned for tool use, only reach a correctness rate in the range of 30% to 60%, far from reliable use in practice. We propose a biologically inspired method for tool-augmented LLMs, simulated trial and error (STE), that orchestrates three key mechanisms for successful tool use behaviors in the biological system: trial and error, imagination, and memory. Specifically, STE leverages an LLM's 'imagination' to simulate plausible scenarios for using a tool,
&lt;/p&gt;</description></item><item><title>&#30446;&#21069;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#35270;&#35273;&#28436;&#32462;&#25512;&#29702;&#26041;&#38754;&#20173;&#23384;&#22312;&#36739;&#22823;&#24046;&#36317;&#21644;&#30450;&#28857;&#12290;</title><link>https://arxiv.org/abs/2403.04732</link><description>&lt;p&gt;
&#25105;&#20204;&#36317;&#31163;&#26234;&#33021;&#35270;&#35273;&#28436;&#32462;&#25512;&#29702;&#36824;&#26377;&#22810;&#36828;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Far Are We from Intelligent Visual Deductive Reasoning?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04732
&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#35270;&#35273;&#28436;&#32462;&#25512;&#29702;&#26041;&#38754;&#20173;&#23384;&#22312;&#36739;&#22823;&#24046;&#36317;&#21644;&#30450;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#35832;&#22914;GPT-4V&#20043;&#31867;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#22312;&#21508;&#31181;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#12290;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#22522;&#20110;&#35270;&#35273;&#30340;&#28436;&#32462;&#25512;&#29702;&#65292;&#36825;&#26159;&#19968;&#20010;&#26356;&#22797;&#26434;&#20294;&#19981;&#22826;&#34987;&#25506;&#32034;&#30340;&#39046;&#22495;&#65292;&#24182;&#21457;&#29616;&#20102;&#24403;&#21069;&#39046;&#20808;&#30340;VLM&#20013;&#20197;&#21069;&#26410;&#26292;&#38706;&#30340;&#30450;&#28857;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#29790;&#25991;&#28176;&#36827;&#30697;&#38453;&#65288;RPM&#65289;&#26469;&#35780;&#20272;VLM&#22312;&#20165;&#20381;&#38752;&#35270;&#35273;&#32447;&#32034;&#36827;&#34892;&#22810;&#36339;&#20851;&#31995;&#21644;&#28436;&#32462;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#23545;&#20960;&#31181;&#27969;&#34892;&#30340;VLM&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#37319;&#29992;&#20102;&#26631;&#20934;&#31574;&#30053;&#65292;&#22914;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#33258;&#25105;&#19968;&#33268;&#24615;&#21644;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#65292;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21253;&#25324;Mensa&#26234;&#21830;&#27979;&#35797;&#12289;&#26234;&#21830;&#27979;&#35797;&#21644;RAVEN&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;LLM&#22312;&#22522;&#20110;&#25991;&#26412;&#30340;&#25512;&#29702;&#26041;&#38754;&#20855;&#26377;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;&#25105;&#20204;&#22312;&#35270;&#35273;&#28436;&#32462;&#25512;&#29702;&#26041;&#38754;&#20173;&#26377;&#24456;&#22823;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04732v1 Announce Type: new  Abstract: Vision-Language Models (VLMs) such as GPT-4V have recently demonstrated incredible strides on diverse vision language tasks. We dig into vision-based deductive reasoning, a more sophisticated but less explored realm, and find previously unexposed blindspots in the current SOTA VLMs. Specifically, we leverage Raven's Progressive Matrices (RPMs), to assess VLMs' abilities to perform multi-hop relational and deductive reasoning relying solely on visual clues. We perform comprehensive evaluations of several popular VLMs employing standard strategies such as in-context learning, self-consistency, and Chain-of-thoughts (CoT) on three diverse datasets, including the Mensa IQ test, IntelligenceTest, and RAVEN. The results reveal that despite the impressive capabilities of LLMs in text-based reasoning, we are still far from achieving comparable proficiency in visual deductive reasoning. We found that certain standard strategies that are effective
&lt;/p&gt;</description></item><item><title>&#36890;&#29992;7B&#35821;&#35328;&#27169;&#22411;LLaMA-2&#23637;&#29616;&#20986;&#24378;&#22823;&#25968;&#23398;&#33021;&#21147;&#65292;&#20934;&#30830;&#29575;&#20998;&#21035;&#36798;&#21040;97.7%&#21644;72.0%&#65292;&#25193;&#22823;SFT&#25968;&#25454;&#21487;&#20197;&#25552;&#39640;&#29983;&#25104;&#27491;&#30830;&#31572;&#26696;&#30340;&#21487;&#38752;&#24615;</title><link>https://arxiv.org/abs/2403.04706</link><description>&lt;p&gt;
&#36890;&#29992;7B&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#20855;&#22791;&#24378;&#22823;&#30340;&#25968;&#23398;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Common 7B Language Models Already Possess Strong Math Capabilities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04706
&lt;/p&gt;
&lt;p&gt;
&#36890;&#29992;7B&#35821;&#35328;&#27169;&#22411;LLaMA-2&#23637;&#29616;&#20986;&#24378;&#22823;&#25968;&#23398;&#33021;&#21147;&#65292;&#20934;&#30830;&#29575;&#20998;&#21035;&#36798;&#21040;97.7%&#21644;72.0%&#65292;&#25193;&#22823;SFT&#25968;&#25454;&#21487;&#20197;&#25552;&#39640;&#29983;&#25104;&#27491;&#30830;&#31572;&#26696;&#30340;&#21487;&#38752;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#20154;&#20204;&#30456;&#20449;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#21482;&#26377;&#22312;&#38750;&#24120;&#22823;&#30340;&#35268;&#27169;&#19978;&#25110;&#38656;&#35201;&#22823;&#37327;&#19982;&#25968;&#23398;&#30456;&#20851;&#30340;&#39044;&#35757;&#32451;&#25165;&#33021;&#23637;&#29616;&#20986;&#25968;&#23398;&#33021;&#21147;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#20855;&#26377;&#36890;&#29992;&#39044;&#35757;&#32451;&#30340;LLaMA-2 7B&#27169;&#22411;&#24050;&#32463;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#25968;&#23398;&#33021;&#21147;&#65292;&#20854;&#22312;GSM8K&#21644;MATH&#22522;&#20934;&#27979;&#35797;&#19978;&#36873;&#25321;256&#20010;&#38543;&#26426;&#29983;&#25104;&#30340;&#26368;&#20339;&#21709;&#24212;&#26102;&#65292;&#20934;&#30830;&#29575;&#20998;&#21035;&#20026;97.7%&#21644;72.0%&#12290;&#30446;&#21069;&#22522;&#30784;&#27169;&#22411;&#30340;&#20027;&#35201;&#38382;&#39064;&#22312;&#20110;&#38590;&#20197;&#19968;&#33268;&#22320;&#24341;&#20986;&#20854;&#22266;&#26377;&#30340;&#25968;&#23398;&#33021;&#21147;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#23545;&#20110;&#31532;&#19968;&#20010;&#31572;&#26696;&#30340;&#20934;&#30830;&#29575;&#20998;&#21035;&#19979;&#38477;&#21040;&#20102;49.5%&#21644;7.9%&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#31616;&#21333;&#22320;&#25193;&#22823;SFT&#25968;&#25454;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#29983;&#25104;&#27491;&#30830;&#31572;&#26696;&#30340;&#21487;&#38752;&#24615;&#12290;&#28982;&#32780;&#65292;&#24191;&#27867;&#25193;&#23637;&#30340;&#28508;&#21147;&#21463;&#21040;&#20844;&#24320;&#21487;&#29992;&#25968;&#23398;&#38382;&#39064;&#30340;&#31232;&#32570;&#24615;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04706v1 Announce Type: cross  Abstract: Mathematical capabilities were previously believed to emerge in common language models only at a very large scale or require extensive math-related pre-training. This paper shows that the LLaMA-2 7B model with common pre-training already exhibits strong mathematical abilities, as evidenced by its impressive accuracy of 97.7% and 72.0% on the GSM8K and MATH benchmarks, respectively, when selecting the best response from 256 random generations. The primary issue with the current base model is the difficulty in consistently eliciting its inherent mathematical capabilities. Notably, the accuracy for the first answer drops to 49.5% and 7.9% on the GSM8K and MATH benchmarks, respectively. We find that simply scaling up the SFT data can significantly enhance the reliability of generating correct answers. However, the potential for extensive scaling is constrained by the scarcity of publicly available math questions. To overcome this limitatio
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#22522;&#20110;&#35270;&#35273;&#30340;&#27169;&#22411;&#23545;&#20110;&#29289;&#20307;&#19982;&#32972;&#26223;&#20043;&#38388;&#22810;&#26679;&#21270;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#65292;&#25552;&#20986;&#19968;&#31181;&#21487;&#20197;&#24341;&#20837;&#19981;&#21516;&#23545;&#35937;&#26041;&#38754;&#21464;&#21270;&#30340;&#26041;&#27861;</title><link>https://arxiv.org/abs/2403.04701</link><description>&lt;p&gt;
ObjectCompose: &#35780;&#20272;&#22522;&#20110;&#35270;&#35273;&#30340;&#27169;&#22411;&#22312;&#29289;&#20307;&#19982;&#32972;&#26223;&#32452;&#21512;&#21464;&#21270;&#19978;&#30340;&#38887;&#24615;
&lt;/p&gt;
&lt;p&gt;
ObjectCompose: Evaluating Resilience of Vision-Based Models on Object-to-Background Compositional Changes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04701
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#22522;&#20110;&#35270;&#35273;&#30340;&#27169;&#22411;&#23545;&#20110;&#29289;&#20307;&#19982;&#32972;&#26223;&#20043;&#38388;&#22810;&#26679;&#21270;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#65292;&#25552;&#20986;&#19968;&#31181;&#21487;&#20197;&#24341;&#20837;&#19981;&#21516;&#23545;&#35937;&#26041;&#38754;&#21464;&#21270;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#26368;&#36817;&#22522;&#20110;&#35270;&#35273;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#35757;&#32451;&#24182;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#65292;&#20102;&#35299;&#23427;&#20204;&#30340;&#40065;&#26834;&#24615;&#31243;&#24230;&#23545;&#20110;&#23427;&#20204;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#37096;&#32626;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#24403;&#21069;&#22522;&#20110;&#35270;&#35273;&#30340;&#27169;&#22411;&#38024;&#23545;&#19981;&#21516;&#30340;&#29289;&#20307;&#19982;&#32972;&#26223;&#19978;&#19979;&#25991;&#21464;&#21270;&#30340;&#38887;&#24615;&#12290;&#22823;&#22810;&#25968;&#40065;&#26834;&#24615;&#35780;&#20272;&#26041;&#27861;&#24341;&#20837;&#20102;&#21512;&#25104;&#25968;&#25454;&#38598;&#26469;&#35825;&#23548;&#29289;&#20307;&#29305;&#24449;&#65288;&#35270;&#28857;&#12289;&#23610;&#24230;&#12289;&#39068;&#33394;&#65289;&#30340;&#21464;&#21270;&#65292;&#25110;&#32773;&#21033;&#29992;&#22270;&#20687;&#36716;&#25442;&#25216;&#26415;&#65288;&#23545;&#25239;&#24615;&#21464;&#21270;&#12289;&#24120;&#35265;&#30772;&#22351;&#65289;&#22312;&#30495;&#23454;&#22270;&#20687;&#19978;&#27169;&#25311;&#20998;&#24067;&#30340;&#21464;&#21270;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#25193;&#25955;&#27169;&#22411;&#26469;&#29983;&#25104;&#32972;&#26223;&#30340;&#21464;&#21270;&#12290;&#20294;&#26159;&#65292;&#36825;&#20123;&#26041;&#27861;&#35201;&#20040;&#22312;&#25552;&#20379;&#23545;&#35201;&#36827;&#34892;&#30340;&#26356;&#25913;&#30340;&#25511;&#21046;&#26041;&#38754;&#19981;&#36275;&#65292;&#35201;&#20040;&#25197;&#26354;&#20102;&#29289;&#20307;&#30340;&#35821;&#20041;&#65292;&#20351;&#20854;&#19981;&#36866;&#29992;&#20110;&#20219;&#21153;&#12290;&#19982;&#20043;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#24341;&#20837;&#21508;&#31181;&#23545;&#35937;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04701v1 Announce Type: cross  Abstract: Given the large-scale multi-modal training of recent vision-based models and their generalization capabilities, understanding the extent of their robustness is critical for their real-world deployment. In this work, we evaluate the resilience of current vision-based models against diverse object-to-background context variations. The majority of robustness evaluation methods have introduced synthetic datasets to induce changes to object characteristics (viewpoints, scale, color) or utilized image transformation techniques (adversarial changes, common corruptions) on real images to simulate shifts in distributions. Recent works have explored leveraging large language models and diffusion models to generate changes in the background. However, these methods either lack in offering control over the changes to be made or distort the object semantics, making them unsuitable for the task. Our method, on the other hand, can induce diverse objec
&lt;/p&gt;</description></item><item><title>AUFormer&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#38754;&#37096;&#21160;&#20316;&#21333;&#20301;&#26816;&#27979;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#26032;&#39062;&#30340;&#30693;&#35782;&#28151;&#21512;&#19987;&#23478;&#21327;&#20316;&#26426;&#21046;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#22312;&#31232;&#32570;&#25968;&#25454;&#38598;&#25110;&#36807;&#24230;&#20381;&#36182;&#39069;&#22806;&#25968;&#25454;&#23548;&#33268;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.04697</link><description>&lt;p&gt;
AUFormer: &#35270;&#35273;Transformer&#26159;&#21442;&#25968;&#39640;&#25928;&#30340;&#38754;&#37096;&#21160;&#20316;&#21333;&#20301;&#26816;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
AUFormer: Vision Transformers are Parameter-Efficient Facial Action Unit Detectors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04697
&lt;/p&gt;
&lt;p&gt;
AUFormer&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#38754;&#37096;&#21160;&#20316;&#21333;&#20301;&#26816;&#27979;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#26032;&#39062;&#30340;&#30693;&#35782;&#28151;&#21512;&#19987;&#23478;&#21327;&#20316;&#26426;&#21046;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#22312;&#31232;&#32570;&#25968;&#25454;&#38598;&#25110;&#36807;&#24230;&#20381;&#36182;&#39069;&#22806;&#25968;&#25454;&#23548;&#33268;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#37096;&#21160;&#20316;&#21333;&#20301;&#65288;AU&#65289;&#22312;&#24773;&#24863;&#35745;&#31639;&#39046;&#22495;&#26159;&#19968;&#20010;&#37325;&#35201;&#27010;&#24565;&#65292;AU&#26816;&#27979;&#19968;&#30452;&#26159;&#19968;&#20010;&#28909;&#38376;&#30340;&#30740;&#31350;&#35838;&#39064;&#12290;&#29616;&#26377;&#26041;&#27861;&#30001;&#20110;&#22312;&#31232;&#32570;&#30340;AU&#27880;&#37322;&#25968;&#25454;&#38598;&#19978;&#21033;&#29992;&#22823;&#37327;&#21487;&#23398;&#20064;&#21442;&#25968;&#25110;&#36807;&#24230;&#20381;&#36182;&#22823;&#37327;&#39069;&#22806;&#30456;&#20851;&#25968;&#25454;&#32780;&#23384;&#22312;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#21442;&#25968;&#39640;&#25928;&#36801;&#31227;&#23398;&#20064;&#65288;PETL&#65289;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#24076;&#26395;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#30340;&#33539;&#24335;&#65292;&#28982;&#32780;&#20854;&#29616;&#26377;&#26041;&#27861;&#32570;&#20047;&#38024;&#23545;AU&#29305;&#24449;&#30340;&#35774;&#35745;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21019;&#26032;&#24615;&#22320;&#23558;PETL&#33539;&#24335;&#24212;&#29992;&#20110;AU&#26816;&#27979;&#65292;&#24341;&#20837;AUFormer&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30693;&#35782;&#28151;&#21512;&#19987;&#23478;&#65288;MoKE&#65289;&#21327;&#20316;&#26426;&#21046;&#12290;&#19968;&#20010;&#29305;&#23450;&#20110;&#26576;&#20010;AU&#24182;&#20855;&#26377;&#26368;&#23569;&#21487;&#23398;&#20064;&#21442;&#25968;&#30340;MoKE&#39318;&#20808;&#38598;&#25104;&#20010;&#24615;&#21270;&#30340;&#22810;&#23610;&#24230;&#21644;&#30456;&#20851;&#30693;&#35782;&#12290;&#28982;&#21518;MoKE&#19982;&#19987;&#23478;&#32452;&#20013;&#30340;&#20854;&#20182;MoKE&#21512;&#20316;&#65292;&#33719;&#21462;&#32858;&#21512;&#20449;&#24687;&#24182;&#23558;&#20854;&#27880;&#20837;&#21040;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04697v1 Announce Type: cross  Abstract: Facial Action Units (AU) is a vital concept in the realm of affective computing, and AU detection has always been a hot research topic. Existing methods suffer from overfitting issues due to the utilization of a large number of learnable parameters on scarce AU-annotated datasets or heavy reliance on substantial additional relevant data. Parameter-Efficient Transfer Learning (PETL) provides a promising paradigm to address these challenges, whereas its existing methods lack design for AU characteristics. Therefore, we innovatively investigate PETL paradigm to AU detection, introducing AUFormer and proposing a novel Mixture-of-Knowledge Expert (MoKE) collaboration mechanism. An individual MoKE specific to a certain AU with minimal learnable parameters first integrates personalized multi-scale and correlation knowledge. Then the MoKE collaborates with other MoKEs in the expert group to obtain aggregated information and inject it into the 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26631;&#35760;&#32423;&#21035;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#26032;&#22411;&#20107;&#23454;&#26680;&#26597;&#21644;&#24187;&#35273;&#26816;&#27979;&#27969;&#31243;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26816;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#20013;&#30340;&#19981;&#21487;&#38752;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.04696</link><description>&lt;p&gt;
&#36890;&#36807;&#26631;&#35760;&#32423;&#21035;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26816;&#39564;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;
&lt;/p&gt;
&lt;p&gt;
Fact-Checking the Output of Large Language Models via Token-Level Uncertainty Quantification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04696
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26631;&#35760;&#32423;&#21035;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#26032;&#22411;&#20107;&#23454;&#26680;&#26597;&#21644;&#24187;&#35273;&#26816;&#27979;&#27969;&#31243;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26816;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#20013;&#30340;&#19981;&#21487;&#38752;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20197;&#20135;&#29983;&#38169;&#35823;&#30340;&#22768;&#26126;&#32780;&#33261;&#21517;&#26157;&#33879;&#12290;&#36825;&#31181;&#24187;&#35273;&#21487;&#33021;&#24456;&#21361;&#38505;&#65292;&#22240;&#20026;&#22312;&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#20598;&#23572;&#20986;&#29616;&#30340;&#20107;&#23454;&#19981;&#20934;&#30830;&#21487;&#33021;&#20250;&#34987;&#25972;&#20307;&#19978;&#26159;&#20107;&#23454;&#30340;&#25991;&#26412;&#25513;&#30422;&#65292;&#36825;&#20351;&#24471;&#29992;&#25143;&#26497;&#20854;&#38590;&#20197;&#21457;&#29616;&#12290;&#21033;&#29992;LLMs&#30340;&#24403;&#21069;&#26381;&#21153;&#36890;&#24120;&#19981;&#25552;&#20379;&#26816;&#27979;&#19981;&#21487;&#38752;&#29983;&#25104;&#30340;&#26041;&#24335;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#26088;&#22312;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26631;&#35760;&#32423;&#21035;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#26032;&#22411;&#20107;&#23454;&#26680;&#26597;&#21644;&#24187;&#35273;&#26816;&#27979;&#27969;&#31243;&#12290;&#19981;&#30830;&#23450;&#24615;&#20998;&#25968;&#21033;&#29992;&#20102;&#31070;&#32463;&#32593;&#32476;&#25110;&#20854;&#23618;&#36755;&#20986;&#20013;&#21253;&#21547;&#30340;&#20449;&#24687;&#26469;&#26816;&#27979;&#19981;&#21487;&#38752;&#30340;&#39044;&#27979;&#65292;&#24182;&#25105;&#20204;&#23637;&#31034;&#23427;&#20204;&#21487;&#20197;&#29992;&#20110;&#26680;&#26597;LLM&#36755;&#20986;&#20013;&#30340;&#21508;&#31181;&#22768;&#26126;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#26631;&#35760;&#32423;&#21035;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#65292;&#28040;&#38500;&#20102;&#23545;&#20107;&#23454;&#25552;&#20986;&#24576;&#30097;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04696v1 Announce Type: cross  Abstract: Large language models (LLMs) are notorious for hallucinating, i.e., producing erroneous claims in their output. Such hallucinations can be dangerous, as occasional factual inaccuracies in the generated text might be obscured by the rest of the output being generally factual, making it extremely hard for the users to spot them. Current services that leverage LLMs usually do not provide any means for detecting unreliable generations. Here, we aim to bridge this gap. In particular, we propose a novel fact-checking and hallucination detection pipeline based on token-level uncertainty quantification. Uncertainty scores leverage information encapsulated in the output of a neural network or its layers to detect unreliable predictions, and we show that they can be used to fact-check the atomic claims in the LLM output. Moreover, we present a novel token-level uncertainty quantification method that removes the impact of uncertainty about what c
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#24555;&#30340;&#37051;&#22495;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#36890;&#36807;&#23558;&#27880;&#24847;&#21147;&#38480;&#21046;&#22312;&#26368;&#36817;&#30340;&#37051;&#23621;&#20043;&#38388;&#26469;&#38477;&#20302;&#33258;&#27880;&#24847;&#21147;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2403.04690</link><description>&lt;p&gt;
&#26356;&#24555;&#30340;&#37051;&#22495;&#27880;&#24847;&#21147;: &#22312;&#32447;&#31243;&#22359;&#32423;&#21035;&#20943;&#23569;&#33258;&#27880;&#24847;&#21147;&#30340;O(n^2)&#25104;&#26412;
&lt;/p&gt;
&lt;p&gt;
Faster Neighborhood Attention: Reducing the O(n^2) Cost of Self Attention at the Threadblock Level
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04690
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#24555;&#30340;&#37051;&#22495;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#36890;&#36807;&#23558;&#27880;&#24847;&#21147;&#38480;&#21046;&#22312;&#26368;&#36817;&#30340;&#37051;&#23621;&#20043;&#38388;&#26469;&#38477;&#20302;&#33258;&#27880;&#24847;&#21147;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37051;&#22495;&#27880;&#24847;&#21147;&#36890;&#36807;&#38480;&#21046;&#27599;&#20010;&#26631;&#35760;&#30340;&#27880;&#24847;&#21147;&#33539;&#22260;&#20026;&#20854;&#26368;&#36817;&#30340;&#37051;&#23621;&#26469;&#38477;&#20302;&#33258;&#27880;&#24847;&#21147;&#30340;&#25104;&#26412;&#12290;&#35813;&#38480;&#21046;&#30001;&#31383;&#21475;&#22823;&#23567;&#21644;&#25193;&#24352;&#22240;&#23376;&#21442;&#25968;&#21270;&#65292;&#20171;&#20110;&#32447;&#24615;&#25237;&#24433;&#21644;&#33258;&#27880;&#24847;&#21147;&#20043;&#38388;&#32472;&#21046;&#20102;&#21487;&#33021;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#35889;&#12290;&#37051;&#22495;&#27880;&#24847;&#21147;&#65292;&#20197;&#21450;&#26356;&#19968;&#33324;&#22320;&#28369;&#21160;&#31383;&#21475;&#27880;&#24847;&#21147;&#27169;&#24335;&#65292;&#22312;&#22522;&#30784;&#35774;&#26045;&#26041;&#38754;&#38271;&#26399;&#21463;&#21040;&#38480;&#21046;&#65292;&#29305;&#21035;&#26159;&#22312;&#26356;&#39640;&#31209;&#30340;&#31354;&#38388;&#65288;2-D&#21644;3-D&#65289;&#65292;&#20419;&#20351;&#24320;&#21457;&#23450;&#21046;&#20869;&#26680;&#30340;&#21457;&#23637;&#65292;&#36825;&#20123;&#20869;&#26680;&#22312;&#21151;&#33021;&#25110;&#24615;&#33021;&#26041;&#38754;&#21463;&#38480;&#65292;&#22914;&#26524;&#19981;&#26159;&#20004;&#32773;&#37117;&#26377;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#37051;&#22495;&#27880;&#24847;&#21147;&#21487;&#20197;&#34920;&#31034;&#20026;&#25209;&#37327;&#21270;&#30340;GEMM&#38382;&#39064;&#65292;&#31867;&#20284;&#20110;&#26631;&#20934;&#27880;&#24847;&#21147;&#65292;&#24182;&#20026;1-D&#21644;2-D&#37051;&#22495;&#27880;&#24847;&#21147;&#23454;&#29616;&#23427;&#12290;&#19982;&#29616;&#26377;&#30340;&#31616;&#21333;&#20869;&#26680;&#30456;&#27604;&#65292;&#36825;&#20123;&#20869;&#26680;&#24179;&#22343;&#25552;&#20379;&#20102;&#20998;&#21035;&#26159;1-D&#21644;2-D&#37051;&#22495;&#27880;&#24847;&#21147;&#30340;&#20840;&#31934;&#24230;&#24310;&#36831;&#25913;&#36827;&#20998;&#21035;&#20026;895%&#21644;272%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04690v1 Announce Type: cross  Abstract: Neighborhood attention reduces the cost of self attention by restricting each token's attention span to its nearest neighbors. This restriction, parameterized by a window size and dilation factor, draws a spectrum of possible attention patterns between linear projection and self attention. Neighborhood attention, and more generally sliding window attention patterns, have long been bounded by infrastructure, particularly in higher-rank spaces (2-D and 3-D), calling for the development of custom kernels, which have been limited in either functionality, or performance, if not both. In this work, we first show that neighborhood attention can be represented as a batched GEMM problem, similar to standard attention, and implement it for 1-D and 2-D neighborhood attention. These kernels on average provide 895% and 272% improvement in full precision latency compared to existing naive kernels for 1-D and 2-D neighborhood attention respectively. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#21019;&#36896;&#24615;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#22312;&#31038;&#20250;&#39046;&#22495;&#21487;&#33021;&#24102;&#26469;&#30340;&#24433;&#21709;&#65292;&#20197;ChatGPT&#20026;&#20363;&#65292;&#35780;&#20272;&#20102;&#27491;&#38754;&#21644;&#36127;&#38754;&#25928;&#24212;&#21450;&#26032;&#20852;&#36235;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.04667</link><description>&lt;p&gt;
&#21019;&#36896;&#24615;&#20154;&#24037;&#26234;&#33021;&#30340;&#31038;&#20250;&#24433;&#21709;&#65306;&#23545;ChatGPT&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
The Social Impact of Generative AI: An Analysis on ChatGPT
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04667
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#21019;&#36896;&#24615;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#22312;&#31038;&#20250;&#39046;&#22495;&#21487;&#33021;&#24102;&#26469;&#30340;&#24433;&#21709;&#65292;&#20197;ChatGPT&#20026;&#20363;&#65292;&#35780;&#20272;&#20102;&#27491;&#38754;&#21644;&#36127;&#38754;&#25928;&#24212;&#21450;&#26032;&#20852;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#20010;&#26376;&#65292;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#31038;&#20250;&#24433;&#21709;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#36825;&#20027;&#35201;&#26159;&#30001;&#21019;&#36896;&#24615;AI&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#29305;&#21035;&#26159;ChatGPT&#25152;&#25512;&#21160;&#30340;&#12290;&#36825;&#20123;&#27169;&#22411;&#30340;&#24555;&#36895;&#21457;&#23637;&#24341;&#21457;&#20102;&#20851;&#20110;&#23427;&#20204;&#30340;&#30410;&#22788;&#12289;&#23616;&#38480;&#24615;&#21644;&#30456;&#20851;&#39118;&#38505;&#30340;&#28909;&#28872;&#35752;&#35770;&#12290;&#29983;&#25104;&#27169;&#22411;&#22312;&#22810;&#20010;&#39046;&#22495;&#65292;&#22914;&#21307;&#30103;&#20445;&#20581;&#12289;&#37329;&#34701;&#21644;&#25945;&#32946;&#31561;&#65292;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#21576;&#29616;&#20986;&#22810;&#26679;&#21270;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#23545;&#28508;&#22312;&#19981;&#21033;&#24433;&#21709;&#30340;&#25285;&#24551;&#24341;&#21457;&#20102;&#19981;&#21516;&#30340;&#35266;&#28857;&#65292;&#20174;&#38544;&#31169;&#39118;&#38505;&#21040;&#21152;&#21095;&#31038;&#20250;&#19981;&#24179;&#31561;&#12290;&#26412;&#25991;&#37319;&#29992;&#19968;&#31181;&#26041;&#27861;&#26469;&#28145;&#20837;&#25506;&#35752;&#29983;&#25104;AI&#24037;&#20855;&#30340;&#31038;&#20250;&#24433;&#21709;&#65292;&#20027;&#35201;&#20851;&#27880;ChatGPT&#30340;&#26696;&#20363;&#12290;&#23427;&#35780;&#20272;&#20102;&#23545;&#20960;&#20010;&#31038;&#20250;&#39046;&#22495;&#30340;&#28508;&#22312;&#24433;&#21709;&#65292;&#24182;&#35828;&#26126;&#20102;&#23545;&#27491;&#38754;&#21644;&#36127;&#38754;&#24433;&#21709;&#12289;&#26032;&#20852;&#36235;&#21183;&#30340;&#20840;&#38754;&#25991;&#29486;&#32508;&#36848;&#30340;&#30740;&#31350;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04667v1 Announce Type: new  Abstract: In recent months, the social impact of Artificial Intelligence (AI) has gained considerable public interest, driven by the emergence of Generative AI models, ChatGPT in particular. The rapid development of these models has sparked heated discussions regarding their benefits, limitations, and associated risks. Generative models hold immense promise across multiple domains, such as healthcare, finance, and education, to cite a few, presenting diverse practical applications. Nevertheless, concerns about potential adverse effects have elicited divergent perspectives, ranging from privacy risks to escalating social inequality. This paper adopts a methodology to delve into the societal implications of Generative AI tools, focusing primarily on the case of ChatGPT. It evaluates the potential impact on several social sectors and illustrates the findings of a comprehensive literature review of both positive and negative effects, emerging trends, 
&lt;/p&gt;</description></item><item><title>Yi&#27169;&#22411;&#31995;&#21015;&#22522;&#20110;&#24378;&#22823;&#30340;&#22810;&#32500;&#33021;&#21147;&#65292;&#36890;&#36807;&#22522;&#20110;6B&#21644;34B&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#25193;&#23637;&#65292;&#21253;&#25324;&#32842;&#22825;&#27169;&#22411;&#12289;&#38271;&#19978;&#19979;&#25991;&#27169;&#22411;&#12289;&#28145;&#24230;&#25918;&#22823;&#27169;&#22411;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.04652</link><description>&lt;p&gt;
Yi: &#30001; 01.AI &#25512;&#20986;&#30340;&#24320;&#25918;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Yi: Open Foundation Models by 01.AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04652
&lt;/p&gt;
&lt;p&gt;
Yi&#27169;&#22411;&#31995;&#21015;&#22522;&#20110;&#24378;&#22823;&#30340;&#22810;&#32500;&#33021;&#21147;&#65292;&#36890;&#36807;&#22522;&#20110;6B&#21644;34B&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#25193;&#23637;&#65292;&#21253;&#25324;&#32842;&#22825;&#27169;&#22411;&#12289;&#38271;&#19978;&#19979;&#25991;&#27169;&#22411;&#12289;&#28145;&#24230;&#25918;&#22823;&#27169;&#22411;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Yi&#27169;&#22411;&#31995;&#21015;&#65292;&#36825;&#26159;&#19968;&#31995;&#21015;&#20855;&#26377;&#24378;&#22823;&#22810;&#32500;&#33021;&#21147;&#30340;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;Yi&#27169;&#22411;&#31995;&#21015;&#22522;&#20110;6B&#21644;34B&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#28982;&#21518;&#25105;&#20204;&#23558;&#23427;&#20204;&#25193;&#23637;&#20026;&#32842;&#22825;&#27169;&#22411;&#12289;200K&#38271;&#19978;&#19979;&#25991;&#27169;&#22411;&#12289;&#28145;&#24230;&#25918;&#22823;&#27169;&#22411;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#22522;&#30784;&#27169;&#22411;&#22312;&#35832;&#22914;MMLU&#20043;&#31867;&#30340;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#32780;&#25105;&#20204;&#24494;&#35843;&#36807;&#30340;&#32842;&#22825;&#27169;&#22411;&#22312;AlpacaEval&#21644;Chatbot Arena&#31561;&#20027;&#35201;&#35780;&#20272;&#24179;&#21488;&#19978;&#20855;&#26377;&#36739;&#39640;&#30340;&#20154;&#31867;&#20559;&#22909;&#29575;&#12290;&#36890;&#36807;&#20381;&#36182;&#20110;&#25105;&#20204;&#30340;&#21487;&#25193;&#23637;&#36229;&#32423;&#35745;&#31639;&#22522;&#30784;&#35774;&#26045;&#21644;&#32463;&#20856;&#30340;Transformer&#26550;&#26500;&#65292;&#25105;&#20204;&#35748;&#20026;Yi&#27169;&#22411;&#30340;&#24615;&#33021;&#20027;&#35201;&#24402;&#22240;&#20110;&#20854;&#25968;&#25454;&#36136;&#37327;&#65292;&#36825;&#26159;&#30001;&#25105;&#20204;&#30340;&#25968;&#25454;&#24037;&#31243;&#24037;&#20316;&#25152;&#24102;&#26469;&#30340;&#12290;&#23545;&#20110;&#39044;&#35757;&#32451;&#65292;&#25105;&#20204;&#20351;&#29992;&#32423;&#32852;&#30340;&#25968;&#25454;&#21435;&#37325;&#21644;&#36136;&#37327;&#36807;&#28388;&#27969;&#27700;&#32447;&#26500;&#24314;&#20102;3100&#20159;&#20010;&#33521;&#25991;&#21644;&#20013;&#25991;&#35821;&#26009;&#24211;&#30340;&#26631;&#35760;&#12290;&#23545;&#20110;&#24494;&#35843;&#65292;&#25105;&#20204;&#23545;&#23567;&#35268;&#27169;&#27169;&#22411;&#36827;&#34892;&#20102;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04652v1 Announce Type: cross  Abstract: We introduce the Yi model family, a series of language and multimodal models that demonstrate strong multi-dimensional capabilities. The Yi model family is based on 6B and 34B pretrained language models, then we extend them to chat models, 200K long context models, depth-upscaled models, and vision-language models. Our base models achieve strong performance on a wide range of benchmarks like MMLU, and our finetuned chat models deliver strong human preference rate on major evaluation platforms like AlpacaEval and Chatbot Arena. Building upon our scalable super-computing infrastructure and the classical transformer architecture, we attribute the performance of Yi models primarily to its data quality resulting from our data-engineering efforts. For pretraining, we construct 3.1 trillion tokens of English and Chinese corpora using a cascaded data deduplication and quality filtering pipeline. For finetuning, we polish a small scale (less th
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#27169;&#24577;&#34701;&#21512;&#21644;&#25968;&#25454;&#20998;&#24067;&#23545;&#40784;&#65292;&#36890;&#36807;&#29305;&#23450;&#19978;&#19979;&#25991;&#21521;&#37327;&#34920;&#31034;&#27599;&#20010;&#27169;&#24577;&#65292;&#24182;&#23558;&#20854;&#19982;&#27599;&#20010;&#27169;&#24577;&#30340;&#23884;&#20837;&#36827;&#34892;&#34701;&#21512;&#65292;</title><link>https://arxiv.org/abs/2403.04650</link><description>&lt;p&gt;
&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
Context-Based Multimodal Fusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04650
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#27169;&#24577;&#34701;&#21512;&#21644;&#25968;&#25454;&#20998;&#24067;&#23545;&#40784;&#65292;&#36890;&#36807;&#29305;&#23450;&#19978;&#19979;&#25991;&#21521;&#37327;&#34920;&#31034;&#27599;&#20010;&#27169;&#24577;&#65292;&#24182;&#23558;&#20854;&#19982;&#27599;&#20010;&#27169;&#24577;&#30340;&#23884;&#20837;&#36827;&#34892;&#34701;&#21512;&#65292;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34701;&#21512;&#27169;&#22411;&#24191;&#27867;&#24212;&#29992;&#20110;&#35299;&#20915;&#22810;&#27169;&#24577;&#20219;&#21153;&#65292;&#20294;&#22312;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#25968;&#25454;&#20998;&#24067;&#23545;&#40784;&#26041;&#38754;&#23384;&#22312;&#26126;&#26174;&#23616;&#38480;&#24615;&#12290;&#38024;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#27169;&#22411;&#31216;&#20026;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#65288;CBMF&#65289;&#65292;&#32467;&#21512;&#20102;&#27169;&#24577;&#34701;&#21512;&#21644;&#25968;&#25454;&#20998;&#24067;&#23545;&#40784;&#65292;&#36890;&#36807;&#29305;&#23450;&#19978;&#19979;&#25991;&#21521;&#37327;&#34920;&#31034;&#27599;&#20010;&#27169;&#24577;&#65292;&#24182;&#23558;&#20854;&#19982;&#27599;&#20010;&#27169;&#24577;&#30340;&#23884;&#20837;&#36827;&#34892;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04650v1 Announce Type: cross  Abstract: The fusion models, which effectively combine information from different sources, are widely used in solving multimodal tasks. However, they have significant limitations related to aligning data distributions across different modalities. This challenge can lead to inconsistencies and difficulties in learning robust representations. Alignment models, while specifically addressing this issue, often require training "from scratch" with large datasets to achieve optimal results, which can be costly in terms of resources and time. To overcome these limitations, we propose an innovative model called Context-Based Multimodal Fusion (CBMF), which combines both modality fusion and data distribution alignment. In CBMF, each modality is represented by a specific context vector, fused with the embedding of each modality. This enables the use of large pre-trained models that can be frozen, reducing the computational and training data requirements. A
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Pix2Gif&#65292;&#19968;&#31181;&#22522;&#20110;&#36816;&#21160;&#24341;&#23548;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#22270;&#20687;&#36716;&#25442;&#38382;&#39064;&#26469;&#23454;&#29616;&#22270;&#20687;&#21040;GIF&#30340;&#29983;&#25104;&#65292;&#24341;&#20837;&#20102;&#26032;&#30340;&#36816;&#21160;&#24341;&#23548;&#21464;&#24418;&#27169;&#22359;&#21644;&#24863;&#30693;&#25439;&#22833;&#20197;&#30830;&#20445;&#27169;&#22411;&#36981;&#24490;&#36816;&#21160;&#24341;&#23548;&#24182;&#20445;&#25345;&#20869;&#23481;&#19968;&#33268;&#24615;&#21644;&#36830;&#36143;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.04634</link><description>&lt;p&gt;
Pix2Gif&#65306;&#22522;&#20110;&#36816;&#21160;&#24341;&#23548;&#25193;&#25955;&#30340;GIF&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Pix2Gif: Motion-Guided Diffusion for GIF Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04634
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Pix2Gif&#65292;&#19968;&#31181;&#22522;&#20110;&#36816;&#21160;&#24341;&#23548;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#22270;&#20687;&#36716;&#25442;&#38382;&#39064;&#26469;&#23454;&#29616;&#22270;&#20687;&#21040;GIF&#30340;&#29983;&#25104;&#65292;&#24341;&#20837;&#20102;&#26032;&#30340;&#36816;&#21160;&#24341;&#23548;&#21464;&#24418;&#27169;&#22359;&#21644;&#24863;&#30693;&#25439;&#22833;&#20197;&#30830;&#20445;&#27169;&#22411;&#36981;&#24490;&#36816;&#21160;&#24341;&#23548;&#24182;&#20445;&#25345;&#20869;&#23481;&#19968;&#33268;&#24615;&#21644;&#36830;&#36143;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Pix2Gif&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#36816;&#21160;&#24341;&#23548;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#22270;&#20687;&#21040;GIF&#65288;&#35270;&#39057;&#65289;&#30340;&#29983;&#25104;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#20219;&#21153;&#26500;&#24314;&#20026;&#30001;&#25991;&#26412;&#21644;&#36816;&#21160;&#22823;&#23567;&#25552;&#31034;&#25351;&#23548;&#30340;&#22270;&#20687;&#36716;&#25442;&#38382;&#39064;&#26469;&#19981;&#21516;&#22320;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#22914;teaser fig&#25152;&#31034;&#12290;&#20026;&#20102;&#30830;&#20445;&#27169;&#22411;&#36981;&#24490;&#36816;&#21160;&#24341;&#23548;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#36816;&#21160;&#24341;&#23548;&#21464;&#24418;&#27169;&#22359;&#65292;&#20197;&#22312;&#20004;&#31181;&#31867;&#22411;&#30340;&#25552;&#31034;&#26465;&#20214;&#19979;&#31354;&#38388;&#21464;&#25442;&#28304;&#22270;&#20687;&#30340;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#24863;&#30693;&#25439;&#22833;&#65292;&#20197;&#30830;&#20445;&#36716;&#25442;&#30340;&#29305;&#24449;&#22270;&#20445;&#25345;&#22312;&#19982;&#30446;&#26631;&#22270;&#20687;&#30456;&#21516;&#30340;&#31354;&#38388;&#20013;&#65292;&#30830;&#20445;&#20869;&#23481;&#19968;&#33268;&#24615;&#21644;&#36830;&#36143;&#24615;&#12290;&#20026;&#20102;&#20026;&#27169;&#22411;&#35757;&#32451;&#20570;&#20934;&#22791;&#65292;&#25105;&#20204;&#36890;&#36807;&#20174;TGIF&#35270;&#39057;&#23383;&#24149;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#36830;&#36143;&#30340;&#22270;&#20687;&#24103;&#26469;&#31934;&#24515;&#31579;&#36873;&#25968;&#25454;&#65292;&#35813;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#26377;&#20851;&#20027;&#39064;&#30340;&#26102;&#38388;&#21464;&#21270;&#20016;&#23500;&#20449;&#24687;&#12290;&#22312;&#39044;&#35757;&#32451;&#20043;&#21518;&#65292;&#25105;&#20204;&#20197;&#38646;&#23556;&#26679;&#30340;&#26041;&#24335;&#23558;&#25105;&#20204;&#30340;&#27169;&#22411;&#24212;&#29992;&#20110;&#22810;&#20010;&#35270;&#39057;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04634v1 Announce Type: cross  Abstract: We present Pix2Gif, a motion-guided diffusion model for image-to-GIF (video) generation. We tackle this problem differently by formulating the task as an image translation problem steered by text and motion magnitude prompts, as shown in teaser fig. To ensure that the model adheres to motion guidance, we propose a new motion-guided warping module to spatially transform the features of the source image conditioned on the two types of prompts. Furthermore, we introduce a perceptual loss to ensure the transformed feature map remains within the same space as the target image, ensuring content consistency and coherence. In preparation for the model training, we meticulously curated data by extracting coherent image frames from the TGIF video-caption dataset, which provides rich information about the temporal changes of subjects. After pretraining, we apply our model in a zero-shot manner to a number of video datasets. Extensive qualitative 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;ShapleyBO&#26694;&#26550;&#65292;&#29992;Shapley&#20540;&#35299;&#37322;&#36125;&#21494;&#26031;&#20248;&#21270;&#25552;&#35758;&#65292;&#37327;&#21270;&#27599;&#20010;&#21442;&#25968;&#23545;&#20110;&#20248;&#21270;&#36807;&#31243;&#30340;&#36129;&#29486;&#65292;&#24182;&#33021;&#22815;&#21306;&#20998;&#19981;&#21516;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#25506;&#32034;&#36129;&#29486;&#12290;</title><link>https://arxiv.org/abs/2403.04629</link><description>&lt;p&gt;
&#29992;Shapley&#20540;&#35299;&#37322;&#36125;&#21494;&#26031;&#20248;&#21270;&#20419;&#36827;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21327;&#20316;
&lt;/p&gt;
&lt;p&gt;
Explaining Bayesian Optimization by Shapley Values Facilitates Human-AI Collaboration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04629
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;ShapleyBO&#26694;&#26550;&#65292;&#29992;Shapley&#20540;&#35299;&#37322;&#36125;&#21494;&#26031;&#20248;&#21270;&#25552;&#35758;&#65292;&#37327;&#21270;&#27599;&#20010;&#21442;&#25968;&#23545;&#20110;&#20248;&#21270;&#36807;&#31243;&#30340;&#36129;&#29486;&#65292;&#24182;&#33021;&#22815;&#21306;&#20998;&#19981;&#21516;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#25506;&#32034;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#19982;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#24050;&#25104;&#20026;&#35299;&#20915;&#40657;&#21283;&#23376;&#20248;&#21270;&#38382;&#39064;&#30340;&#19981;&#21487;&#25110;&#32570;&#30340;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;BO&#26412;&#36523;&#20063;&#24120;&#24120;&#34987;&#35748;&#20026;&#26159;&#19968;&#20010;&#40657;&#21283;&#23376;&#65292;&#32570;&#20047;&#25552;&#20379;&#20026;&#20309;&#25552;&#35758;&#35780;&#20272;&#26576;&#20123;&#21442;&#25968;&#30340;&#29702;&#30001;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;ShapleyBO&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#21338;&#24328;&#35770;Shapley&#20540;&#35299;&#37322;BO&#25552;&#35758;&#30340;&#26694;&#26550;&#12290;&#23427;&#37327;&#21270;&#20102;&#27599;&#20010;&#21442;&#25968;&#23545;BO&#30340;&#25910;&#33719;&#20989;&#25968;&#30340;&#36129;&#29486;&#12290;&#21033;&#29992;Shapley&#20540;&#30340;&#32447;&#24615;&#24615;&#65292;&#25105;&#20204;&#33021;&#22815;&#36827;&#19968;&#27493;&#30830;&#23450;&#27599;&#20010;&#21442;&#25968;&#23545;&#20110;&#20687;&#32622;&#20449;&#36793;&#30028;&#36825;&#26679;&#30340;&#21152;&#27861;&#25910;&#33719;&#20989;&#25968;&#25512;&#21160;BO&#30340;&#25506;&#32034;&#21644;&#24320;&#21457;&#30340;&#24378;&#24230;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;ShapleyBO&#33021;&#22815;&#35299;&#20915;&#25506;&#32034;&#23545;&#20110;&#21208;&#25506;aleatoric&#21644;&#35748;&#35782;epistemic&#19981;&#30830;&#23450;&#24615;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04629v1 Announce Type: cross  Abstract: Bayesian optimization (BO) with Gaussian processes (GP) has become an indispensable algorithm for black box optimization problems. Not without a dash of irony, BO is often considered a black box itself, lacking ways to provide reasons as to why certain parameters are proposed to be evaluated. This is particularly relevant in human-in-the-loop applications of BO, such as in robotics. We address this issue by proposing ShapleyBO, a framework for interpreting BO's proposals by game-theoretic Shapley values.They quantify each parameter's contribution to BO's acquisition function. Exploiting the linearity of Shapley values, we are further able to identify how strongly each parameter drives BO's exploration and exploitation for additive acquisition functions like the confidence bound. We also show that ShapleyBO can disentangle the contributions to exploration into those that explore aleatoric and epistemic uncertainty. Moreover, our method 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#20010;&#20197;&#23545;&#25239;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#20026;&#22522;&#30784;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#21512;&#25104;&#36229;&#22768;&#24515;&#21160;&#22270;&#20687;&#24182;&#36827;&#34892;&#39046;&#22495;&#36716;&#25442;&#65292;&#20855;&#26377;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#30340;&#22270;&#20687;&#29983;&#25104;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.04612</link><description>&lt;p&gt;
&#19968;&#20010;&#20855;&#26377;&#23545;&#25239;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#30340;&#39046;&#22495;&#36716;&#25442;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#36229;&#22768;&#24515;&#21160;&#22270;&#22270;&#20687;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
A Domain Translation Framework with an Adversarial Denoising Diffusion Model to Generate Synthetic Datasets of Echocardiography Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04612
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#20010;&#20197;&#23545;&#25239;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#20026;&#22522;&#30784;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#21512;&#25104;&#36229;&#22768;&#24515;&#21160;&#22270;&#20687;&#24182;&#36827;&#34892;&#39046;&#22495;&#36716;&#25442;&#65292;&#20855;&#26377;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#30340;&#22270;&#20687;&#29983;&#25104;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#21307;&#23398;&#22270;&#20687;&#39046;&#22495;&#30340;&#32763;&#35793;&#25805;&#20316;&#21463;&#21040;&#30740;&#31350;&#20154;&#21592;&#21644;&#20020;&#24202;&#21307;&#29983;&#30340;&#39640;&#38656;&#27714;&#12290;&#36825;&#39033;&#20219;&#21153;&#38500;&#20102;&#20854;&#20182;&#33021;&#21147;&#22806;&#65292;&#36824;&#20801;&#35768;&#29983;&#25104;&#20855;&#26377;&#36275;&#22815;&#39640;&#22270;&#20687;&#36136;&#37327;&#30340;&#26032;&#21307;&#23398;&#22270;&#20687;&#65292;&#20351;&#20854;&#20855;&#26377;&#20020;&#24202;&#30456;&#20851;&#24615;&#12290;&#36825;&#31181;&#25552;&#20986;&#30340;&#26694;&#26550;&#20381;&#36182;&#20110;&#23545;&#25239;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65288;DDM&#65289;&#26469;&#21512;&#25104;&#36229;&#22768;&#24515;&#21160;&#22270;&#20687;&#24182;&#25191;&#34892;&#39046;&#22495;&#36716;&#25442;&#12290;&#19982;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#30456;&#21453;&#65292;DDM&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#24456;&#22823;&#22810;&#26679;&#24615;&#30340;&#39640;&#36136;&#37327;&#22270;&#20687;&#26679;&#26412;&#12290;&#22914;&#26524;&#23558;DDM&#19982;GAN&#32467;&#21512;&#65292;&#36825;&#31181;&#29983;&#25104;&#26032;&#25968;&#25454;&#30340;&#33021;&#21147;&#23558;&#22312;&#26356;&#24555;&#30340;&#37319;&#26679;&#26102;&#38388;&#20869;&#23436;&#25104;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#32467;&#21512;&#20102;GAN&#30340;&#23545;&#25239;DDM&#65292;&#23398;&#20064;&#21453;&#21521;&#21435;&#22122;&#36807;&#31243;&#65292;&#20381;&#36182;&#20110;&#24341;&#23548;&#22270;&#20687;&#65292;&#30830;&#20445;&#30456;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04612v1 Announce Type: cross  Abstract: Currently, medical image domain translation operations show a high demand from researchers and clinicians. Amongst other capabilities, this task allows the generation of new medical images with sufficiently high image quality, making them clinically relevant. Deep Learning (DL) architectures, most specifically deep generative models, are widely used to generate and translate images from one domain to another. The proposed framework relies on an adversarial Denoising Diffusion Model (DDM) to synthesize echocardiography images and perform domain translation. Contrary to Generative Adversarial Networks (GANs), DDMs are able to generate high quality image samples with a large diversity. If a DDM is combined with a GAN, this ability to generate new data is completed at an even faster sampling time. In this work we trained an adversarial DDM combined with a GAN to learn the reverse denoising process, relying on a guide image, making sure rel
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#21033;&#29992;&#20840;&#29699;&#24037;&#20316;&#31354;&#38388;&#26469;&#26500;&#24314;&#22810;&#27169;&#24577;&#34920;&#31034;&#65292;&#20197;&#23454;&#29616;&#38646;&#23556;&#20987;&#36328;&#27169;&#24577;&#36716;&#31227;&#30340;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.04588</link><description>&lt;p&gt;
&#36890;&#36807;&#20840;&#29699;&#24037;&#20316;&#31354;&#38388;&#23454;&#29616;&#38646;&#23556;&#20987;&#36328;&#27169;&#24577;&#36716;&#31227;&#30340;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Zero-shot cross-modal transfer of Reinforcement Learning policies through a Global Workspace
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04588
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#21033;&#29992;&#20840;&#29699;&#24037;&#20316;&#31354;&#38388;&#26469;&#26500;&#24314;&#22810;&#27169;&#24577;&#34920;&#31034;&#65292;&#20197;&#23454;&#29616;&#38646;&#23556;&#20987;&#36328;&#27169;&#24577;&#36716;&#31227;&#30340;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#36890;&#36807;&#22810;&#31181;&#24863;&#23448;&#24863;&#30693;&#19990;&#30028;&#65292;&#20351;&#20182;&#20204;&#33021;&#22815;&#32508;&#21512;&#22320;&#34920;&#36798;&#21608;&#22260;&#29615;&#22659;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#27867;&#21270;&#20449;&#24687;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#36890;&#36807;&#8220;&#20840;&#23616;&#24037;&#20316;&#31354;&#38388;&#8221;&#30340;&#35748;&#30693;&#31185;&#23398;&#27010;&#24565;&#26469;&#26500;&#24314;&#24378;&#22823;&#32780;&#28789;&#27963;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#65292;&#20197;&#21450;&#22312;&#26426;&#22120;&#20154;&#21644;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#20013;&#23558;&#20854;&#24212;&#29992;&#20110;&#36328;&#27169;&#24577;&#36716;&#31227;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04588v1 Announce Type: new  Abstract: Humans perceive the world through multiple senses, enabling them to create a comprehensive representation of their surroundings and to generalize information across domains. For instance, when a textual description of a scene is given, humans can mentally visualize it. In fields like robotics and Reinforcement Learning (RL), agents can also access information about the environment through multiple sensors; yet redundancy and complementarity between sensors is difficult to exploit as a source of robustness (e.g. against sensor failure) or generalization (e.g. transfer across domains). Prior research demonstrated that a robust and flexible multimodal representation can be efficiently constructed based on the cognitive science notion of a 'Global Workspace': a unique representation trained to combine information across modalities, and to broadcast its signal back to each modality. Here, we explore whether such a brain-inspired multimodal re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25361;&#25112;&#24615;&#25968;&#25454;&#38598;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#20010;&#26088;&#22312;&#35299;&#20915;&#23454;&#20307;&#38142;&#25509;&#20219;&#21153;&#30340;&#26032;&#38382;&#39064;&#65306;&#21333;&#20803;&#26684;&#20869;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#25552;&#31034;&#26694;&#26550;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#19968;&#26032;&#20219;&#21153;&#19978;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.04577</link><description>&lt;p&gt;
Wiki-TabNER:&#36890;&#36807;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#25512;&#36827;&#34920;&#26684;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Wiki-TabNER:Advancing Table Interpretation Through Named Entity Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04577
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25361;&#25112;&#24615;&#25968;&#25454;&#38598;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#20010;&#26088;&#22312;&#35299;&#20915;&#23454;&#20307;&#38142;&#25509;&#20219;&#21153;&#30340;&#26032;&#38382;&#39064;&#65306;&#21333;&#20803;&#26684;&#20869;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#25552;&#31034;&#26694;&#26550;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#19968;&#26032;&#20219;&#21153;&#19978;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04577v1 &#21457;&#24067;&#31867;&#22411;&#65306;&#26032;&#25688;&#35201;&#65306;&#32593;&#32476;&#34920;&#26684;&#21253;&#21547;&#22823;&#37327;&#23453;&#36149;&#30693;&#35782;&#65292;&#28608;&#21457;&#20102;&#26088;&#22312;&#35299;&#20915;&#34920;&#26684;&#35299;&#37322;&#65288;TI&#65289;&#20219;&#21153;&#30340;&#34920;&#26684;&#35821;&#35328;&#27169;&#22411;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#29992;&#20110;&#35780;&#20272;TI&#20219;&#21153;&#30340;&#24191;&#27867;&#20351;&#29992;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29305;&#21035;&#20851;&#27880;&#23454;&#20307;&#38142;&#25509;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#26174;&#31034;&#65292;&#35813;&#25968;&#25454;&#38598;&#36807;&#20110;&#31616;&#21270;&#65292;&#21487;&#33021;&#38477;&#20302;&#20854;&#29992;&#20110;&#20840;&#38754;&#35780;&#20272;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#26410;&#20934;&#30830;&#20195;&#34920;&#34920;&#26684;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#22806;&#35266;&#12290;&#20026;&#20811;&#26381;&#36825;&#19968;&#32570;&#28857;&#65292;&#25105;&#20204;&#26500;&#24314;&#24182;&#27880;&#37322;&#20102;&#19968;&#20010;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#26032;&#25968;&#25454;&#38598;&#12290;&#38500;&#20102;&#20171;&#32461;&#26032;&#25968;&#25454;&#38598;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#26088;&#22312;&#35299;&#20915;&#23454;&#20307;&#38142;&#25509;&#20219;&#21153;&#30340;&#26032;&#38382;&#39064;&#65306;&#21333;&#20803;&#26684;&#20869;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25552;&#31034;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#26032;&#24320;&#21457;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#36825;&#19968;&#26032;&#30340;TI&#20219;&#21153;&#19978;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#35774;&#32622;&#19979;&#23545;&#25552;&#31034;LLMs&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#65292;&#20854;&#20013;&#25105;&#20204;&#21516;&#26102;&#20351;&#29992;&#20102;&#38543;&#26426;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04577v1 Announce Type: new  Abstract: Web tables contain a large amount of valuable knowledge and have inspired tabular language models aimed at tackling table interpretation (TI) tasks. In this paper, we analyse a widely used benchmark dataset for evaluation of TI tasks, particularly focusing on the entity linking task. Our analysis reveals that this dataset is overly simplified, potentially reducing its effectiveness for thorough evaluation and failing to accurately represent tables as they appear in the real-world. To overcome this drawback, we construct and annotate a new more challenging dataset. In addition to introducing the new dataset, we also introduce a novel problem aimed at addressing the entity linking task: named entity recognition within cells. Finally, we propose a prompting framework for evaluating the newly developed large language models (LLMs) on this novel TI task. We conduct experiments on prompting LLMs under various settings, where we use both random
&lt;/p&gt;</description></item><item><title>&#24403;&#21069;&#30340;&#28145;&#24230;&#23398;&#20064;&#22312;&#31995;&#32479;1&#33021;&#21147;&#19978;&#25104;&#21151;&#65292;&#20294;&#22312;&#31995;&#32479;2&#33021;&#21147;&#19978;&#20173;&#28982;&#32570;&#23569;&#37325;&#35201;&#20869;&#23481;&#65292;&#26412;&#25991;&#20197;&#20449;&#24687;&#29702;&#35770;&#30340;&#35266;&#28857;&#65292;&#25506;&#35752;&#26377;&#36259;&#30340;&#25968;&#23398;&#38472;&#36848;&#26500;&#25104;&#65292;&#24182;&#33268;&#21147;&#20110;&#21457;&#29616;&#26032;&#39062;&#29468;&#24819;&#65292;&#20197;&#27492;&#25351;&#23548;&#26410;&#26469;&#25171;&#36896;AI&#25968;&#23398;&#23478;&#12290;</title><link>https://arxiv.org/abs/2403.04571</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21644;&#20449;&#24687;&#29702;&#35770;&#27010;&#24565;&#23545;AI&#25968;&#23398;&#23478;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Machine learning and information theory concepts towards an AI Mathematician
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04571
&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#28145;&#24230;&#23398;&#20064;&#22312;&#31995;&#32479;1&#33021;&#21147;&#19978;&#25104;&#21151;&#65292;&#20294;&#22312;&#31995;&#32479;2&#33021;&#21147;&#19978;&#20173;&#28982;&#32570;&#23569;&#37325;&#35201;&#20869;&#23481;&#65292;&#26412;&#25991;&#20197;&#20449;&#24687;&#29702;&#35770;&#30340;&#35266;&#28857;&#65292;&#25506;&#35752;&#26377;&#36259;&#30340;&#25968;&#23398;&#38472;&#36848;&#26500;&#25104;&#65292;&#24182;&#33268;&#21147;&#20110;&#21457;&#29616;&#26032;&#39062;&#29468;&#24819;&#65292;&#20197;&#27492;&#25351;&#23548;&#26410;&#26469;&#25171;&#36896;AI&#25968;&#23398;&#23478;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04571v1 &#21457;&#34920;&#31867;&#22411;&#65306;&#26032;&#30340; &#25688;&#35201;&#65306;&#30446;&#21069;&#20154;&#24037;&#26234;&#33021;&#30340;&#26368;&#26032;&#25216;&#26415;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#65292;&#29305;&#21035;&#26159;&#22312;&#35821;&#35328;&#25484;&#25569;&#26041;&#38754;&#65292;&#20294;&#22312;&#25968;&#23398;&#25512;&#29702;&#26041;&#38754;&#21364;&#19981;&#37027;&#20040;&#20196;&#20154;&#28385;&#24847;&#12290;&#31350;&#31455;&#32570;&#23569;&#20102;&#20160;&#20040;&#65311;&#25105;&#20204;&#26159;&#21542;&#21487;&#20197;&#20174;&#25968;&#23398;&#23478;&#30340;&#22823;&#33041;&#22914;&#20309;&#22788;&#29702;&#38382;&#39064;&#20013;&#65292;&#23398;&#21040;&#26377;&#20851;&#36825;&#19968;&#24046;&#36317;&#30340;&#19968;&#20123;&#26377;&#29992;&#30693;&#35782;&#65311;&#36825;&#31687;&#25991;&#31456;&#26500;&#24314;&#22312;&#19968;&#20010;&#35266;&#24565;&#20043;&#19978;&#65292;&#21363;&#30446;&#21069;&#30340;&#28145;&#24230;&#23398;&#20064;&#20027;&#35201;&#25104;&#21151;&#20110;&#31995;&#32479;1&#33021;&#21147; -- &#36825;&#30456;&#23545;&#24212;&#20110;&#25105;&#20204;&#30340;&#30452;&#35273;&#21644;&#20064;&#24815;&#24615;&#34892;&#20026; -- &#20294;&#20173;&#28982;&#32570;&#23569;&#26377;&#20851;&#31995;&#32479;2&#33021;&#21147;&#30340;&#19968;&#20123;&#37325;&#35201;&#20869;&#23481; -- &#36825;&#21253;&#25324;&#25512;&#29702;&#33021;&#21147;&#21644;&#31283;&#20581;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#23427;&#37319;&#21462;&#20102;&#20449;&#24687;&#29702;&#35770;&#30340;&#31435;&#22330;&#26469;&#25506;&#35752;&#20160;&#20040;&#26500;&#25104;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#25968;&#23398;&#38472;&#36848;&#65292;&#36825;&#21487;&#20197;&#25351;&#23548;&#26410;&#26469;&#25171;&#36896;AI&#25968;&#23398;&#23478;&#30340;&#24037;&#20316;&#12290;&#37325;&#28857;&#19981;&#22312;&#20110;&#35777;&#26126;&#19968;&#20010;&#32473;&#23450;&#23450;&#29702;&#65292;&#32780;&#22312;&#20110;&#21457;&#29616;&#26032;&#30340;&#26377;&#36259;&#29468;&#24819;&#12290;&#20013;&#24515;&#20551;&#35774;&#26159;&#65292;&#19968;&#20010;&#29702;&#24819;&#30340;&#23450;&#29702;&#20307;&#31995;&#26356;&#22909;&#22320;&#24635;&#32467;&#20102;&#36825;&#20010;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04571v1 Announce Type: new  Abstract: The current state-of-the-art in artificial intelligence is impressive, especially in terms of mastery of language, but not so much in terms of mathematical reasoning. What could be missing? Can we learn something useful about that gap from how the brains of mathematicians go about their craft? This essay builds on the idea that current deep learning mostly succeeds at system 1 abilities -- which correspond to our intuition and habitual behaviors -- but still lacks something important regarding system 2 abilities -- which include reasoning and robust uncertainty estimation. It takes an information-theoretical posture to ask questions about what constitutes an interesting mathematical statement, which could guide future work in crafting an AI mathematician. The focus is not on proving a given theorem but on discovering new and interesting conjectures. The central hypothesis is that a desirable body of theorems better summarizes the set of 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#35745;&#31639;&#30149;&#29702;&#23398;&#20013;&#20943;&#23569;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#22797;&#26434;&#24615;&#23545;&#20998;&#31867;&#24615;&#33021;&#30340;&#25913;&#21892;&#65292;&#36890;&#36807;&#21033;&#29992;&#28040;&#36153;&#32423;&#30828;&#20214;&#12290;</title><link>https://arxiv.org/abs/2403.04558</link><description>&lt;p&gt;
&#20943;&#23569;&#33258;&#30417;&#30563;&#23398;&#20064;&#22797;&#26434;&#24615;&#25913;&#21892;&#35745;&#31639;&#30149;&#29702;&#23398;&#20013;&#30340;&#24369;&#30417;&#30563;&#20998;&#31867;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Reducing self-supervised learning complexity improves weakly-supervised classification performance in computational pathology
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04558
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#35745;&#31639;&#30149;&#29702;&#23398;&#20013;&#20943;&#23569;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#22797;&#26434;&#24615;&#23545;&#20998;&#31867;&#24615;&#33021;&#30340;&#25913;&#21892;&#65292;&#36890;&#36807;&#21033;&#29992;&#28040;&#36153;&#32423;&#30828;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#20174;&#24120;&#35268;&#21487;&#29992;&#30340;&#32452;&#32455;&#23398;&#25968;&#25454;&#20013;&#25552;&#21462;&#20020;&#24202;&#21487;&#25805;&#20316;&#35265;&#35299;&#12290;&#36890;&#24120;&#65292;&#36825;&#20123;&#27169;&#22411;&#38656;&#35201;&#20020;&#24202;&#21307;&#29983;&#36827;&#34892;&#30340;&#26631;&#27880;&#65292;&#36825;&#31181;&#26631;&#27880;&#31232;&#32570;&#19988;&#26114;&#36149;&#12290;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26041;&#27861;&#30340;&#20986;&#29616;&#28040;&#38500;&#20102;&#36825;&#19968;&#38556;&#30861;&#65292;&#20801;&#35768;&#23545;&#38750;&#26631;&#27880;&#25968;&#25454;&#36827;&#34892;&#22823;&#35268;&#27169;&#20998;&#26512;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;SSL&#26041;&#27861;&#37319;&#29992;&#26085;&#30410;&#24222;&#22823;&#30340;&#27169;&#22411;&#26550;&#26500;&#21644;&#26356;&#22823;&#30340;&#25968;&#25454;&#38598;&#65292;&#23548;&#33268;&#25968;&#25454;&#37327;&#36805;&#36895;&#22686;&#21152;&#65292;&#30828;&#20214;&#35201;&#27714;&#21644;&#25972;&#20307;&#25104;&#26412;&#22686;&#21152;&#65292;&#20351;&#24471;&#24456;&#23569;&#26426;&#26500;&#33021;&#22815;&#33719;&#24471;&#36825;&#20123;&#36164;&#28304;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#35745;&#31639;&#30149;&#29702;&#23398;&#20013;&#30340;&#22797;&#26434;&#24615;&#19982;&#20998;&#31867;&#24615;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21033;&#29992;&#28040;&#36153;&#32423;&#30828;&#20214;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#25968;&#25454;&#37327;&#12289;&#26550;&#26500;&#21644;&#31639;&#27861;&#30340;&#35843;&#25972;&#23545;&#19979;&#28216;&#20998;&#31867;&#20219;&#21153;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04558v1 Announce Type: cross  Abstract: Deep Learning models have been successfully utilized to extract clinically actionable insights from routinely available histology data. Generally, these models require annotations performed by clinicians, which are scarce and costly to generate. The emergence of self-supervised learning (SSL) methods remove this barrier, allowing for large-scale analyses on non-annotated data. However, recent SSL approaches apply increasingly expansive model architectures and larger datasets, causing the rapid escalation of data volumes, hardware prerequisites, and overall expenses, limiting access to these resources to few institutions. Therefore, we investigated the complexity of contrastive SSL in computational pathology in relation to classification performance with the utilization of consumer-grade hardware. Specifically, we analyzed the effects of adaptations in data volume, architecture, and algorithms on downstream clas- sification tasks, empha
&lt;/p&gt;</description></item><item><title>&#25968;&#25454;&#24179;&#34913;&#22312;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#20013;&#21487;&#20197;&#37096;&#20998;&#25913;&#21892;&#20559;&#35265;&#38382;&#39064;&#65292;&#28982;&#32780;&#20250;&#23545;&#36136;&#37327;&#20135;&#29983;&#22797;&#26434;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.04547</link><description>&lt;p&gt;
CLIP&#21435;&#20559;&#35265;&#65306;&#22312;&#22810;&#27169;&#24577;&#23398;&#20064;&#20013;&#24179;&#34913;&#25968;&#25454;&#26377;&#22810;&#22823;&#29992;&#22788;&#65311;
&lt;/p&gt;
&lt;p&gt;
CLIP the Bias: How Useful is Balancing Data in Multimodal Learning?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04547
&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#24179;&#34913;&#22312;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#20013;&#21487;&#20197;&#37096;&#20998;&#25913;&#21892;&#20559;&#35265;&#38382;&#39064;&#65292;&#28982;&#32780;&#20250;&#23545;&#36136;&#37327;&#20135;&#29983;&#22797;&#26434;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#25968;&#25454;&#24179;&#34913;&#23545;&#20110;&#20943;&#36731;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#20013;&#30340;&#20559;&#35265;&#30340;&#26377;&#25928;&#24615;&#65292;&#30830;&#23450;&#20102;&#20854;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#37325;&#30003;&#20102;&#20197;&#21069;&#30340;&#32467;&#35770;&#65292;&#21363;CLIP&#27169;&#22411;&#21487;&#33021;&#20250;&#26080;&#24847;&#20013;&#21560;&#25910;&#31038;&#20250;&#21051;&#26495;&#21360;&#35937;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#31216;&#20026;&#22810;&#27169;&#24577;&#26102;&#21051;&#21305;&#37197;&#65288;M4&#65289;&#65292;&#26088;&#22312;&#20943;&#23569;&#22810;&#27169;&#24577;&#25968;&#25454;&#20013;&#30340;&#34920;&#31034;&#21644;&#20851;&#32852;&#20559;&#35265;&#65288;&#21363;&#19968;&#38454;&#21644;&#20108;&#38454;&#32479;&#35745;&#65289;&#12290;&#25105;&#20204;&#20351;&#29992;M4&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#32771;&#34385;&#20102;&#27169;&#22411;&#12289;&#34920;&#31034;&#21644;&#25968;&#25454;&#22823;&#23567;&#31561;&#21508;&#31181;&#22240;&#32032;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;CLIP&#23398;&#20064;&#21644;&#28040;&#38500;&#20559;&#35265;&#30340;&#21160;&#24577;&#29305;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#24494;&#35843;&#21487;&#20197;&#26377;&#25928;&#22320;&#25269;&#21046;&#34920;&#31034;&#20559;&#35265;&#65292;&#20294;&#23545;&#20851;&#32852;&#20559;&#35265;&#30340;&#24433;&#21709;&#36880;&#28176;&#20943;&#24369;&#12290;&#27492;&#22806;&#65292;&#25968;&#25454;&#24179;&#34913;&#23545;&#36136;&#37327;&#26377;&#30528;&#22797;&#26434;&#30340;&#24433;&#21709;&#65306;&#23427;&#20542;&#21521;&#20110;&#25913;&#21892;&#20998;&#31867;&#65292;&#20294;&#21487;&#33021;&#20250;&#25439;&#23475;&#26816;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04547v1 Announce Type: cross  Abstract: We study the effectiveness of data-balancing for mitigating biases in contrastive language-image pretraining (CLIP), identifying areas of strength and limitation. First, we reaffirm prior conclusions that CLIP models can inadvertently absorb societal stereotypes. To counter this, we present a novel algorithm, called Multi-Modal Moment Matching (M4), designed to reduce both representation and association biases (i.e. in first- and second-order statistics) in multimodal data. We use M4 to conduct an in-depth analysis taking into account various factors, such as the model, representation, and data size. Our study also explores the dynamic nature of how CLIP learns and unlearns biases. In particular, we find that fine-tuning is effective in countering representation biases, though its impact diminishes for association biases. Also, data balancing has a mixed impact on quality: it tends to improve classification but can hurt retrieval. Inte
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23454;&#29616;&#20102;&#20174;&#33258;&#28982;&#35821;&#35328;&#35268;&#33539;&#21040;ASP&#31243;&#24207;&#30340;&#33258;&#21160;&#21270;&#32452;&#21512;&#30340;&#31532;&#19968;&#27493;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#22270;&#30456;&#20851;&#38382;&#39064;&#35268;&#33539;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#20110;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#20004;&#27493;&#26550;&#26500;&#12290;</title><link>https://arxiv.org/abs/2403.04541</link><description>&lt;p&gt;
&#23454;&#29616;&#20174;&#33258;&#28982;&#35821;&#35328;&#35268;&#33539;&#21040;ASP&#31243;&#24207;&#30340;&#33258;&#21160;&#21270;&#32452;&#21512;
&lt;/p&gt;
&lt;p&gt;
Towards Automatic Composition of ASP Programs from Natural Language Specifications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04541
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23454;&#29616;&#20102;&#20174;&#33258;&#28982;&#35821;&#35328;&#35268;&#33539;&#21040;ASP&#31243;&#24207;&#30340;&#33258;&#21160;&#21270;&#32452;&#21512;&#30340;&#31532;&#19968;&#27493;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#22270;&#30456;&#20851;&#38382;&#39064;&#35268;&#33539;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#20110;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#20004;&#27493;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36808;&#20986;&#20102;&#23454;&#29616;&#33258;&#21160;&#21270;&#21512;&#25104;&#31572;&#26696;&#38598;&#32534;&#31243;&#65288;ASP&#65289;&#35268;&#33539;&#30340;&#31532;&#19968;&#27493;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25552;&#20379;&#20102;&#20197;&#19979;&#36129;&#29486;&#65306;(i)&#38598;&#20013;&#22312;&#19982;&#22270;&#30456;&#20851;&#30340;&#38382;&#39064;&#35268;&#33539;&#30340;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#24320;&#21457;&#21644;&#35780;&#20272;ASP&#33258;&#21160;&#32534;&#30721;&#24037;&#20855;&#65307;(ii)&#19968;&#20010;&#20004;&#27493;&#26550;&#26500;&#65292;&#22312;NL2ASP&#24037;&#20855;&#20013;&#23454;&#29616;&#65292;&#29992;&#20110;&#20174;&#33258;&#28982;&#35821;&#35328;&#35268;&#33539;&#29983;&#25104;ASP&#31243;&#24207;&#12290;NL2ASP&#20351;&#29992;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#23558;&#33258;&#28982;&#35821;&#35328;&#36716;&#25442;&#20026;&#21463;&#25511;&#33258;&#28982;&#35821;&#35328;&#65288;CNL&#65289;&#35821;&#21477;&#12290;&#38543;&#21518;&#65292;&#20351;&#29992;CNL2ASP&#24037;&#20855;&#23558;CNL&#35821;&#21477;&#36716;&#25442;&#20026;ASP&#20195;&#30721;&#12290;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04541v1 Announce Type: new  Abstract: This paper moves the first step towards automating the composition of Answer Set Programming (ASP) specifications. In particular, the following contributions are provided: (i) A dataset focused on graph-related problem specifications, designed to develop and assess tools for ASP automatic coding; (ii) A two-step architecture, implemented in the NL2ASP tool, for generating ASP programs from natural language specifications. NL2ASP uses neural machine translation to transform natural language into Controlled Natural Language (CNL) statements. Subsequently, CNL statements are converted into ASP code using the CNL2ASP tool. An experiment confirms the viability of the approach.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#32852;&#37030;&#24494;&#35843;&#22522;&#30784;&#27169;&#22411;&#30340;&#25968;&#25454;&#36136;&#37327;&#25511;&#21046;&#31649;&#36947;&#65292;&#21487;&#20197;&#25552;&#39640;&#20840;&#23616;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.04529</link><description>&lt;p&gt;
&#22312;&#32852;&#37030;&#24494;&#35843;&#22522;&#30784;&#27169;&#22411;&#20013;&#25552;&#21319;&#25968;&#25454;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
Enhancing Data Quality in Federated Fine-Tuning of Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04529
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#32852;&#37030;&#24494;&#35843;&#22522;&#30784;&#27169;&#22411;&#30340;&#25968;&#25454;&#36136;&#37327;&#25511;&#21046;&#31649;&#36947;&#65292;&#21487;&#20197;&#25552;&#39640;&#20840;&#23616;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#35757;&#32451;&#30340;&#24403;&#21069;&#24773;&#26223;&#20013;&#65292;&#23384;&#22312;&#30528;&#23545;&#20844;&#20849;&#39046;&#22495;&#25968;&#25454;&#30340;&#26174;&#33879;&#20381;&#36182;&#65292;&#32780;&#26681;&#25454;&#26368;&#36817;&#30340;&#30740;&#31350;&#65292;&#36825;&#20123;&#25968;&#25454;&#24050;&#25509;&#36817;&#26543;&#31469;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25193;&#22823;&#35268;&#27169;&#65292;&#23558;&#22810;&#20010;&#19987;&#19994;&#21270;&#21644;&#39640;&#36136;&#37327;&#30340;&#31169;&#26377;&#39046;&#22495;&#25968;&#25454;&#28304;&#36827;&#34892;&#21327;&#20316;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#19981;&#20849;&#20139;&#31169;&#26377;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#22320;&#26041;&#24615;&#35757;&#32451;&#27169;&#22411;&#25152;&#38754;&#20020;&#30340;&#25968;&#25454;&#36136;&#37327;&#25511;&#21046;&#38382;&#39064;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#32852;&#37030;&#24494;&#35843;&#22522;&#30784;&#27169;&#22411;&#30340;&#25968;&#25454;&#36136;&#37327;&#25511;&#21046;&#31649;&#36947;&#12290;&#35813;&#31649;&#36947;&#35745;&#31639;&#21453;&#26144;&#35757;&#32451;&#25968;&#25454;&#36136;&#37327;&#30340;&#20998;&#25968;&#65292;&#24182;&#30830;&#23450;&#19968;&#20010;&#20840;&#23616;&#38408;&#20540;&#20197;&#23454;&#29616;&#32479;&#19968;&#26631;&#20934;&#65292;&#26088;&#22312;&#25552;&#39640;&#20840;&#23616;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#36136;&#37327;&#25511;&#21046;&#31649;&#36947;&#20419;&#36827;&#20102;&#27169;&#22411;&#35757;&#32451;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#20174;&#32780;&#24102;&#26469;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04529v1 Announce Type: cross  Abstract: In the current landscape of foundation model training, there is a significant reliance on public domain data, which is nearing exhaustion according to recent research. To further scale up, it is crucial to incorporate collaboration among multiple specialized and high-quality private domain data sources. However, the challenge of training models locally without sharing private data presents numerous obstacles in data quality control. To tackle this issue, we propose a data quality control pipeline for federated fine-tuning of foundation models. This pipeline computes scores reflecting the quality of training data and determines a global threshold for a unified standard, aiming for improved global performance. Our experiments show that the proposed quality control pipeline facilitates the effectiveness and reliability of the model training, leading to better performance.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#29289;&#29702;&#32422;&#26463;&#33258;&#21160;&#32534;&#30721;&#22120;&#24320;&#21457;&#20102;&#39640;&#20809;&#35889;&#35299;&#28151;&#31639;&#27861;&#65292;&#25552;&#20379;&#20102;&#22312;&#22797;&#26434;&#28151;&#21512;&#24773;&#20917;&#19979;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#25928;&#29575;&#65292;&#23637;&#31034;&#20102;&#22312;&#22797;&#26434;&#29983;&#29289;&#29615;&#22659;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.04526</link><description>&lt;p&gt;
&#36890;&#36807;&#29289;&#29702;&#32422;&#26463;&#33258;&#21160;&#32534;&#30721;&#22120;&#36827;&#34892;&#25289;&#26364;&#20809;&#35889;&#30340;&#39640;&#20809;&#35889;&#35299;&#28151;
&lt;/p&gt;
&lt;p&gt;
Hyperspectral unmixing for Raman spectroscopy via physics-constrained autoencoders
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04526
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#29289;&#29702;&#32422;&#26463;&#33258;&#21160;&#32534;&#30721;&#22120;&#24320;&#21457;&#20102;&#39640;&#20809;&#35889;&#35299;&#28151;&#31639;&#27861;&#65292;&#25552;&#20379;&#20102;&#22312;&#22797;&#26434;&#28151;&#21512;&#24773;&#20917;&#19979;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#25928;&#29575;&#65292;&#23637;&#31034;&#20102;&#22312;&#22797;&#26434;&#29983;&#29289;&#29615;&#22659;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25289;&#26364;&#20809;&#35889;&#24191;&#27867;&#24212;&#29992;&#20110;&#31185;&#23398;&#39046;&#22495;&#65292;&#20197;&#38750;&#30772;&#22351;&#24615;&#12289;&#26080;&#26631;&#35760;&#30340;&#26041;&#24335;&#34920;&#24449;&#26679;&#21697;&#30340;&#21270;&#23398;&#32452;&#25104;&#12290;&#35768;&#22810;&#24212;&#29992;&#38656;&#35201;&#20174;&#28151;&#21512;&#20998;&#23376;&#29289;&#31181;&#30340;&#20449;&#21495;&#20013;&#35299;&#28151;&#65292;&#20197;&#35782;&#21035;&#20986;&#29616;&#30340;&#20010;&#20307;&#32452;&#20998;&#21450;&#20854;&#27604;&#20363;&#65292;&#28982;&#32780;&#20256;&#32479;&#30340;&#21270;&#23398;&#35745;&#37327;&#23398;&#26041;&#27861;&#24120;&#24120;&#38590;&#20197;&#22788;&#29702;&#23454;&#36341;&#20013;&#36935;&#21040;&#30340;&#22797;&#26434;&#28151;&#21512;&#24773;&#20917;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#22522;&#20110;&#33258;&#21160;&#32534;&#30721;&#22120;&#31070;&#32463;&#32593;&#32476;&#24320;&#21457;&#20102;&#22522;&#20110;&#39640;&#20809;&#35889;&#35299;&#28151;&#30340;&#31639;&#27861;&#65292;&#24182;&#20351;&#29992;&#20869;&#37096;&#21019;&#24314;&#30340;&#21512;&#25104;&#21644;&#23454;&#39564;&#22522;&#20934;&#25968;&#25454;&#38598;&#23545;&#23427;&#20204;&#36827;&#34892;&#31995;&#32479;&#39564;&#35777;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26631;&#20934;&#35299;&#28151;&#26041;&#27861;&#30456;&#27604;&#65292;&#35299;&#28151;&#33258;&#21160;&#32534;&#30721;&#22120;&#25552;&#20379;&#20102;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#25928;&#29575;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#33258;&#21160;&#32534;&#30721;&#22120;&#22312;&#22797;&#26434;&#29983;&#29289;&#29615;&#22659;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#36890;&#36807;&#23637;&#31034;&#20174;&#21333;&#26680;&#32454;&#32990;&#30340;&#20307;&#31215;&#25289;&#26364;&#25104;&#20687;&#25968;&#25454;&#20013;&#25913;&#21892;&#30340;&#29983;&#29289;&#21270;&#23398;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04526v1 Announce Type: cross  Abstract: Raman spectroscopy is widely used across scientific domains to characterize the chemical composition of samples in a non-destructive, label-free manner. Many applications entail the unmixing of signals from mixtures of molecular species to identify the individual components present and their proportions, yet conventional methods for chemometrics often struggle with complex mixture scenarios encountered in practice. Here, we develop hyperspectral unmixing algorithms based on autoencoder neural networks, and we systematically validate them using both synthetic and experimental benchmark datasets created in-house. Our results demonstrate that unmixing autoencoders provide improved accuracy, robustness and efficiency compared to standard unmixing methods. We also showcase the applicability of autoencoders to complex biological settings by showing improved biochemical characterization of volumetric Raman imaging data from a monocytic cell.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;T-TAME&#65292;&#19968;&#31181;&#36866;&#29992;&#20110;&#21367;&#31215;&#32593;&#32476;&#21644;&#35270;&#35273;Transformer&#30340;&#21487;&#35757;&#32451;&#27880;&#24847;&#26426;&#21046;&#65292;&#20026;&#35299;&#37322;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#25552;&#20379;&#20102;&#36890;&#29992;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.04523</link><description>&lt;p&gt;
T-TAME&#65306;&#29992;&#20110;&#35299;&#37322;&#21367;&#31215;&#32593;&#32476;&#21644;&#35270;&#35273;Transformer&#30340;&#21487;&#35757;&#32451;&#27880;&#24847;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
T-TAME: Trainable Attention Mechanism for Explaining Convolutional Networks and Vision Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04523
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;T-TAME&#65292;&#19968;&#31181;&#36866;&#29992;&#20110;&#21367;&#31215;&#32593;&#32476;&#21644;&#35270;&#35273;Transformer&#30340;&#21487;&#35757;&#32451;&#27880;&#24847;&#26426;&#21046;&#65292;&#20026;&#35299;&#37322;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#25552;&#20379;&#20102;&#36890;&#29992;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Vision Transformers&#21644;&#20854;&#20182;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#30340;&#21457;&#23637;&#21644;&#24212;&#29992;&#24555;&#36895;&#22686;&#38271;&#12290;&#28982;&#32780;&#65292;&#31070;&#32463;&#32593;&#32476;&#30340;&#8220;&#40657;&#21283;&#23376;&#8221;&#29305;&#24615;&#26159;&#22312;&#38656;&#35201;&#35299;&#37322;&#24615;&#30340;&#24212;&#29992;&#20013;&#37319;&#29992;&#30340;&#38556;&#30861;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#29992;&#20110;&#29983;&#25104;&#35299;&#37322;&#30340;&#25216;&#26415;&#65292;&#20027;&#35201;&#29992;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#20294;&#26159;&#23558;&#36825;&#20123;&#25216;&#26415;&#36866;&#24212;&#21040;&#35270;&#35273;Transformer&#30340;&#26032;&#33539;&#24335;&#26159;&#38750;&#24179;&#20961;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;T-TAME&#65292;Transformer&#20860;&#23481;&#30340;&#21487;&#35757;&#32451;&#27880;&#24847;&#26426;&#21046;&#29992;&#20110;&#35299;&#37322;&#65292;&#36825;&#26159;&#19968;&#31181;&#35828;&#26126;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#36890;&#29992;&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#26550;&#26500;&#21644;&#35757;&#32451;&#25216;&#26415;&#21487;&#20197;&#36731;&#26494;&#24212;&#29992;&#20110;&#20219;&#20309;&#21367;&#31215;&#25110;&#31867;&#20284;Vision Transformer&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#20351;&#29992;&#31934;&#31616;&#30340;&#35757;&#32451;&#26041;&#27861;&#12290;&#35757;&#32451;&#21518;&#65292;&#35299;&#37322;&#22270;&#21487;&#20197;&#22312;&#21333;&#27425;&#21069;&#21521;&#20256;&#36882;&#20013;&#35745;&#31639;&#20986;&#65307;&#36825;&#20123;&#35299;&#37322;&#22270;&#21487;&#20197;&#19982;Convolutional Neural Networks&#20013;&#29983;&#25104;&#30340;&#35299;&#37322;&#22270;&#30456;&#23218;&#32654;&#25110;&#32773;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04523v1 Announce Type: cross  Abstract: The development and adoption of Vision Transformers and other deep-learning architectures for image classification tasks has been rapid. However, the "black box" nature of neural networks is a barrier to adoption in applications where explainability is essential. While some techniques for generating explanations have been proposed, primarily for Convolutional Neural Networks, adapting such techniques to the new paradigm of Vision Transformers is non-trivial. This paper presents T-TAME, Transformer-compatible Trainable Attention Mechanism for Explanations, a general methodology for explaining deep neural networks used in image classification tasks. The proposed architecture and training technique can be easily applied to any convolutional or Vision Transformer-like neural network, using a streamlined training approach. After training, explanation maps can be computed in a single forward pass; these explanation maps are comparable to or 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#22312;&#30701;&#35270;&#39057;&#25512;&#33616;&#31995;&#32479;&#20013;&#29992;&#25143;&#21487;&#33021;&#30001;&#20110;&#29421;&#38552;&#26292;&#38706;&#20110;&#20854;&#24191;&#27867;&#20852;&#36259;&#39046;&#22495;&#20869;&#30340;&#20869;&#23481;&#32780;&#24418;&#25104;&#30340;&#8220;&#28145;&#24230;&#8221;&#36807;&#28388;&#27873;&#27873;&#29616;&#35937;&#12290;</title><link>https://arxiv.org/abs/2403.04511</link><description>&lt;p&gt;
&#25581;&#31034;&#28145;&#24230;&#36807;&#28388;&#27873;&#27873;: &#30701;&#35270;&#39057;&#25512;&#33616;&#20013;&#30340;&#29421;&#31364;&#26292;&#38706;
&lt;/p&gt;
&lt;p&gt;
Uncovering the Deep Filter Bubble: Narrow Exposure in Short-Video Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04511
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#22312;&#30701;&#35270;&#39057;&#25512;&#33616;&#31995;&#32479;&#20013;&#29992;&#25143;&#21487;&#33021;&#30001;&#20110;&#29421;&#38552;&#26292;&#38706;&#20110;&#20854;&#24191;&#27867;&#20852;&#36259;&#39046;&#22495;&#20869;&#30340;&#20869;&#23481;&#32780;&#24418;&#25104;&#30340;&#8220;&#28145;&#24230;&#8221;&#36807;&#28388;&#27873;&#27873;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#21487;&#33021;&#23548;&#33268;&#29992;&#25143;&#19981;&#28385;&#25110;&#26497;&#21270;&#31561;&#19981;&#33391;&#32467;&#26524;&#65292;&#36807;&#28388;&#27873;&#27873;&#22312;&#22312;&#32447;&#20869;&#23481;&#24179;&#21488;&#30340;&#32972;&#26223;&#19979;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#12290;&#38543;&#30528;&#30701;&#35270;&#39057;&#24179;&#21488;&#30340;&#20852;&#36215;&#65292;&#36807;&#28388;&#27873;&#27873;&#21463;&#21040;&#39069;&#22806;&#20851;&#27880;&#65292;&#22240;&#20026;&#36825;&#20123;&#24179;&#21488;&#20381;&#36182;&#25512;&#33616;&#31995;&#32479;&#30340;&#31354;&#21069;&#20351;&#29992;&#26469;&#25552;&#20379;&#30456;&#20851;&#20869;&#23481;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#28145;&#24230;&#36807;&#28388;&#27873;&#27873;&#65292;&#25351;&#30340;&#26159;&#29992;&#25143;&#22312;&#20854;&#24191;&#27867;&#20852;&#36259;&#39046;&#22495;&#20869;&#26292;&#38706;&#20110;&#29421;&#31364;&#20869;&#23481;&#12290;&#25105;&#20204;&#21033;&#29992;&#20013;&#22269;&#19968;&#23478;&#39030;&#32423;&#30701;&#35270;&#39057;&#24179;&#21488;&#30340;&#19968;&#24180;&#20114;&#21160;&#25968;&#25454;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#35813;&#25968;&#25454;&#21253;&#25324;&#27599;&#20010;&#35270;&#39057;&#30340;&#19977;&#20010;&#32423;&#21035;&#30340;&#31867;&#21035;&#30340;&#20998;&#23618;&#25968;&#25454;&#12290;&#25105;&#20204;&#22312;&#36825;&#19968;&#29615;&#22659;&#20013;&#26126;&#30830;&#23450;&#20041;&#20102;&#25105;&#20204;&#23545;&#8220;&#28145;&#24230;&#8221;&#36807;&#28388;&#27873;&#27873;&#30340;&#23450;&#20041;&#65292;&#28982;&#21518;&#25506;&#32034;&#20102;&#25968;&#25454;&#20013;&#30340;&#21508;&#31181;&#30456;&#20851;&#24615;&#65306;&#39318;&#20808;&#20102;&#35299;&#38543;&#26102;&#38388;&#25512;&#31227;&#28145;&#24230;&#36807;&#28388;&#27873;&#27873;&#30340;&#28436;&#21464;&#65292;&#28982;&#21518;&#25581;&#31034;&#19968;&#20123;&#24341;&#36215;&#30340;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04511v1 Announce Type: new  Abstract: Filter bubbles have been studied extensively within the context of online content platforms due to their potential to cause undesirable outcomes such as user dissatisfaction or polarization. With the rise of short-video platforms, the filter bubble has been given extra attention because these platforms rely on an unprecedented use of the recommender system to provide relevant content. In our work, we investigate the deep filter bubble, which refers to the user being exposed to narrow content within their broad interests. We accomplish this using one-year interaction data from a top short-video platform in China, which includes hierarchical data with three levels of categories for each video. We formalize our definition of a "deep" filter bubble within this context, and then explore various correlations within the data: first understanding the evolution of the deep filter bubble over time, and later revealing some of the factors that give
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25506;&#32034;&#20102;&#20174;&#19978;&#19979;&#25991;&#23398;&#20064;&#32773;&#21040;&#32763;&#35793;&#27169;&#22411;&#30340;&#36716;&#21464;&#36807;&#31243;&#65292;&#24182;&#21457;&#29616;&#20102;"&#20219;&#21153;&#35782;&#21035;"&#28857;&#20197;&#21450;&#21033;&#29992;&#35813;&#28857;&#30340;&#20887;&#20313;&#24615;&#21487;&#33410;&#32422;&#35745;&#31639;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.04510</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19978;&#19979;&#25991;&#32763;&#35793;&#21457;&#29983;&#22312;&#21738;&#37324;
&lt;/p&gt;
&lt;p&gt;
Where does In-context Translation Happen in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04510
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25506;&#32034;&#20102;&#20174;&#19978;&#19979;&#25991;&#23398;&#20064;&#32773;&#21040;&#32763;&#35793;&#27169;&#22411;&#30340;&#36716;&#21464;&#36807;&#31243;&#65292;&#24182;&#21457;&#29616;&#20102;"&#20219;&#21153;&#35782;&#21035;"&#28857;&#20197;&#21450;&#21033;&#29992;&#35813;&#28857;&#30340;&#20887;&#20313;&#24615;&#21487;&#33410;&#32422;&#35745;&#31639;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#23637;&#31034;&#20986;&#33021;&#22815;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#25191;&#34892;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#30340;&#33021;&#21147;&#65292;&#20294;&#20851;&#20110;&#27169;&#22411;&#22312;&#20309;&#22788;&#25191;&#34892;&#36825;&#19968;&#20219;&#21153;&#30456;&#23545;&#20110;&#25552;&#31034;&#25351;&#20196;&#21644;&#28436;&#31034;&#31034;&#20363;&#30340;&#24773;&#20917;&#30693;&#20043;&#29978;&#23569;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35797;&#22270;&#34920;&#24449;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#19978;&#19979;&#25991;&#23398;&#20064;&#32773;&#36716;&#21464;&#20026;&#32763;&#35793;&#27169;&#22411;&#30340;&#21306;&#22495;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#22312;GPTNeo2.7B&#12289;Bloom3B&#12289;Llama7b&#21644;Llama7b-chat&#19978;&#30340;&#36880;&#23618;&#19978;&#19979;&#25991;&#23631;&#34109;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;"&#20219;&#21153;&#35782;&#21035;"&#28857;&#30340;&#35777;&#25454;&#65292;&#21363;&#32763;&#35793;&#20219;&#21153;&#34987;&#32534;&#30721;&#21040;&#36755;&#20837;&#34920;&#31034;&#20013;&#65292;&#24182;&#19988;&#19981;&#20877;&#38656;&#35201;&#20851;&#27880;&#19978;&#19979;&#25991;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35266;&#23519;&#21040;&#23436;&#20840;&#23631;&#34109;&#23618;&#26102;&#20302;&#24615;&#33021;&#19982;&#20219;&#21153;&#35782;&#21035;&#23618;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#12290;&#21033;&#29992;&#36825;&#31181;&#20887;&#20313;&#24615;&#22312;&#25552;&#31034;5&#20010;&#31034;&#20363;&#26102;&#33410;&#32422;&#20102;45%&#30340;&#35745;&#31639;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04510v1 Announce Type: cross  Abstract: Self-supervised large language models have demonstrated the ability to perform Machine Translation (MT) via in-context learning, but little is known about where the model performs the task with respect to prompt instructions and demonstration examples. In this work, we attempt to characterize the region where large language models transition from in-context learners to translation models. Through a series of layer-wise context-masking experiments on \textsc{GPTNeo2.7B}, \textsc{Bloom3B}, \textsc{Llama7b} and \textsc{Llama7b-chat}, we demonstrate evidence of a "task recognition" point where the translation task is encoded into the input representations and attention to context is no longer necessary. We further observe correspondence between the low performance when masking out entire layers, and the task recognition layers. Taking advantage of this redundancy results in 45\% computational savings when prompting with 5 examples, and tas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;ROGMC&#65292;&#36890;&#36807;&#32047;&#31215;&#20559;&#22909;&#20256;&#25773;&#30452;&#25509;&#22312;GNN&#30340;&#28040;&#24687;&#20256;&#36882;&#20013;&#21512;&#24182;&#35780;&#20998;&#24207;&#25968;&#65292;&#20174;&#32780;&#26356;&#24378;&#35843;&#29992;&#25143;&#26356;&#24378;&#30340;&#20559;&#22909;&#65292;&#24182;&#22312;&#30697;&#38453;&#23436;&#25104;&#20013;&#21462;&#24471;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2403.04504</link><description>&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#35780;&#20998;&#24207;&#25968;&#25913;&#36827;&#30697;&#38453;&#23436;&#25104;
&lt;/p&gt;
&lt;p&gt;
Improving Matrix Completion by Exploiting Rating Ordinality in Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04504
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;ROGMC&#65292;&#36890;&#36807;&#32047;&#31215;&#20559;&#22909;&#20256;&#25773;&#30452;&#25509;&#22312;GNN&#30340;&#28040;&#24687;&#20256;&#36882;&#20013;&#21512;&#24182;&#35780;&#20998;&#24207;&#25968;&#65292;&#20174;&#32780;&#26356;&#24378;&#35843;&#29992;&#25143;&#26356;&#24378;&#30340;&#20559;&#22909;&#65292;&#24182;&#22312;&#30697;&#38453;&#23436;&#25104;&#20013;&#21462;&#24471;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30697;&#38453;&#23436;&#25104;&#26159;&#25512;&#33616;&#31995;&#32479;&#30740;&#31350;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#39046;&#22495;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#23558;&#35780;&#20998;&#30697;&#38453;&#35270;&#20026;&#19968;&#20010;&#29992;&#25143;-&#39033;&#30446;&#20108;&#37096;&#22270;&#65292;&#20854;&#20013;&#26631;&#35760;&#30340;&#36793;&#34920;&#31034;&#35266;&#23519;&#21040;&#30340;&#35780;&#20998;&#65292;&#24182;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#39044;&#27979;&#29992;&#25143;&#21644;&#39033;&#30446;&#33410;&#28857;&#20043;&#38388;&#30340;&#36793;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#26377;&#25928;&#65292;&#20294;&#23427;&#20204;&#23558;&#27599;&#31181;&#35780;&#20998;&#31867;&#22411;&#35270;&#20026;&#29420;&#31435;&#30340;&#20851;&#31995;&#31867;&#22411;&#65292;&#22240;&#27492;&#26080;&#27861;&#20805;&#20998;&#32771;&#34385;&#35780;&#20998;&#30340;&#24207;&#25968;&#24615;&#36136;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#21033;&#29992;GNN&#20013;&#30340;&#35780;&#20998;&#24207;&#25968;&#65292;&#36825;&#22312;&#25991;&#29486;&#20013;&#23578;&#26410;&#24471;&#21040;&#24456;&#22909;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;ROGMC&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;Rating Ordinality&#22312;&#22522;&#20110;GNN&#30340;&#30697;&#38453;&#23436;&#25104;&#20013;&#36827;&#34892;&#21033;&#29992;&#12290;&#23427;&#20351;&#29992;&#32047;&#31215;&#20559;&#22909;&#20256;&#25773;&#30452;&#25509;&#22312;GNN&#30340;&#28040;&#24687;&#20256;&#36882;&#20013;&#21512;&#24182;&#35780;&#20998;&#24207;&#25968;&#65292;&#20174;&#32780;&#20801;&#35768;&#26681;&#25454;&#35780;&#20998;&#31867;&#22411;&#30340;&#20869;&#22312;&#39034;&#24207;&#26356;&#24378;&#35843;&#29992;&#25143;&#30340;&#26356;&#24378;&#20559;&#22909;&#12290;&#36825;&#19968;&#36807;&#31243;&#36890;&#36807;&#20852;&#36259;&#27491;&#21017;&#21270;&#24471;&#21040;&#34917;&#20805;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04504v1 Announce Type: new  Abstract: Matrix completion is an important area of research in recommender systems. Recent methods view a rating matrix as a user-item bi-partite graph with labeled edges denoting observed ratings and predict the edges between the user and item nodes by using the graph neural network (GNN). Despite their effectiveness, they treat each rating type as an independent relation type and thus cannot sufficiently consider the ordinal nature of the ratings. In this paper, we explore a new approach to exploit rating ordinality for GNN, which has not been studied well in the literature. We introduce a new method, called ROGMC, to leverage Rating Ordinality in GNN-based Matrix Completion. It uses cumulative preference propagation to directly incorporate rating ordinality in GNN's message passing, allowing for users' stronger preferences to be more emphasized based on inherent orders of rating types. This process is complemented by interest regularization wh
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#20197;&#32467;&#21512;&#28145;&#24230;&#23398;&#20064;&#38598;&#25104;&#36827;&#34892;&#21021;&#22987;&#21442;&#25968;&#20272;&#35745;&#65292;&#20419;&#36827;&#20102;&#26377;&#25928;&#30340;&#19979;&#28216;&#28436;&#21270;&#25277;&#26679;&#65292;&#26377;&#25928;&#22320;&#20272;&#35745;&#20102;&#20174;&#30913;&#20849;&#25391;&#22270;&#20687;&#20013;&#33041;&#32959;&#30244;&#32454;&#32990;&#27987;&#24230;&#65292;DL-Prior&#22312;&#20854;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;</title><link>https://arxiv.org/abs/2403.04500</link><description>&lt;p&gt;
&#19968;&#31181;&#21487;&#23398;&#20064;&#30340;&#20808;&#39564;&#25913;&#36827;&#20102;&#36870;&#32959;&#30244;&#29983;&#38271;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
A Learnable Prior Improves Inverse Tumor Growth Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04500
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#20197;&#32467;&#21512;&#28145;&#24230;&#23398;&#20064;&#38598;&#25104;&#36827;&#34892;&#21021;&#22987;&#21442;&#25968;&#20272;&#35745;&#65292;&#20419;&#36827;&#20102;&#26377;&#25928;&#30340;&#19979;&#28216;&#28436;&#21270;&#25277;&#26679;&#65292;&#26377;&#25928;&#22320;&#20272;&#35745;&#20102;&#20174;&#30913;&#20849;&#25391;&#22270;&#20687;&#20013;&#33041;&#32959;&#30244;&#32454;&#32990;&#27987;&#24230;&#65292;DL-Prior&#22312;&#20854;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#29289;&#29702;&#24314;&#27169;&#65292;&#29305;&#21035;&#26159;&#28041;&#21450;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#30340;&#24314;&#27169;&#65292;&#20026;&#20010;&#20307;&#21270;&#30142;&#30149;&#27835;&#30103;&#26041;&#26696;&#25552;&#20379;&#20102;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#36870;&#38382;&#39064;&#27714;&#35299;&#26041;&#38754;&#25552;&#20986;&#20102;&#24040;&#22823;&#25361;&#25112;&#65292;&#35201;&#20040;&#26159;&#30001;&#20110;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#30340;&#39640;&#35745;&#31639;&#35201;&#27714;&#65292;&#35201;&#20040;&#26159;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#26041;&#27861;&#30340;&#26377;&#38480;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#20197;&#19968;&#31181;&#21327;&#21516;&#30340;&#26041;&#24335;&#21033;&#29992;&#20004;&#31181;&#26041;&#27861;&#30340;&#29420;&#29305;&#20248;&#21183;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;DL&#38598;&#25104;&#36827;&#34892;&#21021;&#22987;&#21442;&#25968;&#20272;&#35745;&#65292;&#20174;&#32780;&#20419;&#36827;&#20102;&#21021;&#22987;&#21270;&#20026;&#22522;&#20110;DL&#30340;&#20808;&#39564;&#30340;&#26377;&#25928;&#19979;&#28216;&#36827;&#21270;&#25277;&#26679;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23558;&#24555;&#36895;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#19982;&#39640;&#31934;&#24230;&#28436;&#21270;&#31574;&#30053;&#32467;&#21512;&#36215;&#26469;&#22312;&#20174;&#30913;&#20849;&#25391;&#22270;&#20687;&#20013;&#20272;&#35745;&#33041;&#32959;&#30244;&#32454;&#32990;&#27987;&#24230;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;DL-Prior&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#26174;&#33879;&#32422;&#26463;&#20102;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04500v1 Announce Type: cross  Abstract: Biophysical modeling, particularly involving partial differential equations (PDEs), offers significant potential for tailoring disease treatment protocols to individual patients. However, the inverse problem-solving aspect of these models presents a substantial challenge, either due to the high computational requirements of model-based approaches or the limited robustness of deep learning (DL) methods. We propose a novel framework that leverages the unique strengths of both approaches in a synergistic manner. Our method incorporates a DL ensemble for initial parameter estimation, facilitating efficient downstream evolutionary sampling initialized with this DL-based prior. We showcase the effectiveness of integrating a rapid deep-learning algorithm with a high-precision evolution strategy in estimating brain tumor cell concentrations from magnetic resonance images. The DL-Prior plays a pivotal role, significantly constraining the effect
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GraphInstruct&#30340;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#21644;&#22686;&#24378;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#22270;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;GraphLM&#21644;&#25552;&#20986;GraphLM+&#27169;&#22411;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#22270;&#25512;&#29702;&#33021;&#21147;&#22686;&#24378;&#12290;</title><link>https://arxiv.org/abs/2403.04483</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#29702;&#35299;&#21644;&#25512;&#29702;&#21151;&#33021;&#22686;&#24378;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;GraphInstruct
&lt;/p&gt;
&lt;p&gt;
GraphInstruct: Empowering Large Language Models with Graph Understanding and Reasoning Capability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04483
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GraphInstruct&#30340;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#21644;&#22686;&#24378;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#22270;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;GraphLM&#21644;&#25552;&#20986;GraphLM+&#27169;&#22411;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#22270;&#25512;&#29702;&#33021;&#21147;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#21644;&#22686;&#24378;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36890;&#29992;&#33021;&#21147;&#19968;&#30452;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#35838;&#39064;&#12290;&#22270;&#26159;&#29616;&#23454;&#19990;&#30028;&#20013;&#24120;&#35265;&#30340;&#25968;&#25454;&#32467;&#26500;&#65292;&#29702;&#35299;&#22270;&#25968;&#25454;&#23545;&#20110;&#25512;&#36827;&#36890;&#29992;&#26234;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#35780;&#20272;&#21644;&#22686;&#24378;LLMs&#30340;&#22270;&#29702;&#35299;&#33021;&#21147;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GraphInstruct&#30340;&#22522;&#20934;&#65292;&#20840;&#38754;&#21253;&#25324;21&#20010;&#32463;&#20856;&#22270;&#25512;&#29702;&#20219;&#21153;&#65292;&#25552;&#20379;&#22810;&#26679;&#30340;&#22270;&#29983;&#25104;&#27969;&#27700;&#32447;&#21644;&#35814;&#32454;&#30340;&#25512;&#29702;&#27493;&#39588;&#12290;&#22522;&#20110;GraphInstruct&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#39640;&#25928;&#30340;&#25351;&#23548;&#35843;&#25972;&#26500;&#24314;&#20102;GraphLM&#65292;&#23637;&#31034;&#20986;&#26174;&#33879;&#30340;&#22270;&#29702;&#35299;&#33021;&#21147;&#12290;&#20026;&#20102;&#22686;&#24378;LLM&#30340;&#22270;&#25512;&#29702;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27493;&#39588;&#25513;&#30721;&#35757;&#32451;&#31574;&#30053;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;GraphLM+&#30340;&#27169;&#22411;&#12290;&#20316;&#20026;&#22686;&#24378;LLMs&#22270;&#29702;&#35299;&#21644;&#25512;&#29702;&#33021;&#21147;&#30340;&#20808;&#39537;&#24615;&#21162;&#21147;&#20043;&#19968;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04483v1 Announce Type: new  Abstract: Evaluating and enhancing the general capabilities of large language models (LLMs) has been an important research topic. Graph is a common data structure in the real world, and understanding graph data is a crucial part for advancing general intelligence. To evaluate and enhance the graph understanding abilities of LLMs, in this paper, we propose a benchmark named GraphInstruct, which comprehensively includes 21 classical graph reasoning tasks, providing diverse graph generation pipelines and detailed reasoning steps. Based on GraphInstruct, we further construct GraphLM through efficient instruction-tuning, which shows prominent graph understanding capability. In order to enhance the LLM with graph reasoning capability as well, we propose a step mask training strategy, and construct a model named GraphLM+. As one of the pioneering efforts to enhance the graph understanding and reasoning abilities of LLMs, extensive experiments have demons
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21475;&#35821;&#35821;&#35328;&#22810;&#30446;&#26631;&#29702;&#35299;&#65292;&#25552;&#20986;&#20102;&#25913;&#36827;&#23454;&#20307;&#27133;&#21644;&#23376;&#30446;&#26631;&#25351;&#20196;&#30340;&#21019;&#26032;&#25216;&#26415;&#65292;&#24182;&#23637;&#31034;&#20102;LLMs&#22312;&#22810;&#30446;&#26631;SLU&#27169;&#22411;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.04481</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#29702;&#35299;&#22810;&#30446;&#26631;&#21475;&#35821;&#35821;&#35328;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do Large Language Model Understand Multi-Intent Spoken Language ?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04481
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21475;&#35821;&#35821;&#35328;&#22810;&#30446;&#26631;&#29702;&#35299;&#65292;&#25552;&#20986;&#20102;&#25913;&#36827;&#23454;&#20307;&#27133;&#21644;&#23376;&#30446;&#26631;&#25351;&#20196;&#30340;&#21019;&#26032;&#25216;&#26415;&#65292;&#24182;&#23637;&#31034;&#20102;LLMs&#22312;&#22810;&#30446;&#26631;SLU&#27169;&#22411;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#22810;&#30446;&#26631;&#21475;&#35821;&#35821;&#35328;&#29702;&#35299;&#65288;SLU&#65289;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;SLU&#29615;&#22659;&#20013;&#21033;&#29992;LLMs&#29983;&#25104;&#33021;&#21147;&#30340;&#29420;&#29305;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#21019;&#26032;&#25216;&#26415;&#37325;&#26032;&#37197;&#32622;&#20102;&#23454;&#20307;&#27133;&#65292;&#19987;&#38376;&#29992;&#20110;LLMs&#22312;&#22810;&#30446;&#26631;SLU&#29615;&#22659;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#24341;&#20837;&#20102;&#23376;&#30446;&#26631;&#25351;&#20196;&#65288;SII&#65289;&#30340;&#27010;&#24565;&#65292;&#22686;&#24378;&#20102;&#23545;&#19981;&#21516;&#39046;&#22495;&#20869;&#22797;&#26434;&#22810;&#30446;&#26631;&#20132;&#27969;&#30340;&#35299;&#21078;&#21644;&#35299;&#37322;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#25968;&#25454;&#38598;&#65292;&#34987;&#31216;&#20026;LM-MixATIS&#21644;LM-MixSNIPS&#65292;&#26159;&#20174;&#29616;&#26377;&#22522;&#20934;&#20013;&#31934;&#24515;&#21046;&#20316;&#30340;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;LLMs&#21487;&#20197;&#21305;&#37197;&#24182;&#28508;&#22312;&#22320;&#36229;&#36234;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#22810;&#30446;&#26631;SLU&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#23427;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;LLMs&#22312;&#21508;&#31181;&#24847;&#22270;&#37197;&#32622;&#21644;&#25968;&#25454;&#38598;&#27604;&#20363;&#19979;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20004;&#20010;&#24320;&#21019;&#24615;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#21363;&#23454;&#20307;&#27133;&#20934;&#30830;&#24615;&#65288;ESA&#65289;&#21644;Com
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04481v1 Announce Type: cross  Abstract: This study marks a significant advancement by harnessing Large Language Models (LLMs) for multi-intent spoken language understanding (SLU), proposing a unique methodology that capitalizes on the generative power of LLMs within an SLU context. Our innovative technique reconfigures entity slots specifically for LLM application in multi-intent SLU environments and introduces the concept of Sub-Intent Instruction (SII), enhancing the dissection and interpretation of intricate, multi-intent communication within varied domains. The resultant datasets, dubbed LM-MixATIS and LM-MixSNIPS, are crafted from pre-existing benchmarks. Our research illustrates that LLMs can match and potentially excel beyond the capabilities of current state-of-the-art multi-intent SLU models. It further explores LLM efficacy across various intent configurations and dataset proportions. Moreover, we introduce two pioneering metrics, Entity Slot Accuracy (ESA) and Com
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25991;&#26412;&#20013;&#24515;&#20219;&#21153;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#38646;&#21021;&#22987;&#21270;&#30340;Shifted Window Attention&#12289;&#30456;&#20284;&#24615;&#31579;&#36873;&#26631;&#35760;&#31561;&#26041;&#24335;&#23545;&#27169;&#22411;&#36827;&#34892;&#22686;&#24378;&#65292;&#21516;&#26102;&#25299;&#23637;&#20102;&#27169;&#22411;&#30340;&#33021;&#21147;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.04473</link><description>&lt;p&gt;
TextMonkey&#65306;&#19968;&#31181;&#26080;&#38656;OCR&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#29992;&#20110;&#29702;&#35299;&#25991;&#26723;
&lt;/p&gt;
&lt;p&gt;
TextMonkey: An OCR-Free Large Multimodal Model for Understanding Document
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04473
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25991;&#26412;&#20013;&#24515;&#20219;&#21153;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#38646;&#21021;&#22987;&#21270;&#30340;Shifted Window Attention&#12289;&#30456;&#20284;&#24615;&#31579;&#36873;&#26631;&#35760;&#31561;&#26041;&#24335;&#23545;&#27169;&#22411;&#36827;&#34892;&#22686;&#24378;&#65292;&#21516;&#26102;&#25299;&#23637;&#20102;&#27169;&#22411;&#30340;&#33021;&#21147;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;TextMonkey&#65292;&#19968;&#20010;&#19987;&#20026;&#25991;&#26412;&#20013;&#24515;&#20219;&#21153;&#23450;&#21046;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMM&#65289;&#65292;&#21253;&#25324;&#25991;&#26723;&#38382;&#31572;&#65288;DocVQA&#65289;&#21644;&#22330;&#26223;&#25991;&#26412;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#26041;&#38754;&#36827;&#34892;&#20102;&#25913;&#36827;&#65306;&#36890;&#36807;&#37319;&#29992;&#38646;&#21021;&#22987;&#21270;&#30340;Shifted Window Attention&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#26356;&#39640;&#36755;&#20837;&#20998;&#36776;&#29575;&#30340;&#36328;&#31383;&#21475;&#36830;&#25509;&#24182;&#31283;&#23450;&#20102;&#26089;&#26399;&#35757;&#32451;&#65307;&#25105;&#20204;&#20551;&#35774;&#22270;&#20687;&#21487;&#33021;&#21253;&#21547;&#20887;&#20313;&#26631;&#35760;&#65292;&#36890;&#36807;&#20351;&#29992;&#30456;&#20284;&#24615;&#26469;&#31579;&#36873;&#20986;&#37325;&#35201;&#26631;&#35760;&#65292;&#25105;&#20204;&#19981;&#20165;&#21487;&#20197;&#20248;&#21270;&#26631;&#35760;&#38271;&#24230;&#65292;&#36824;&#21487;&#20197;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#25193;&#23637;&#25105;&#20204;&#27169;&#22411;&#30340;&#33021;&#21147;&#20197;&#28085;&#30422;&#25991;&#26412;&#23450;&#20301;&#21644;&#23450;&#20301;&#65292;&#24182;&#23558;&#20301;&#32622;&#20449;&#24687;&#32435;&#20837;&#21709;&#24212;&#20013;&#65292;&#25105;&#20204;&#25552;&#39640;&#20102;&#21487;&#35299;&#37322;&#24615;&#24182;&#20943;&#23569;&#20102;&#24187;&#35273;&#12290;&#27492;&#22806;&#65292;TextMonkey &#21487;&#20197;&#24494;&#35843;&#20197;&#33719;&#24471;&#29702;&#35299;&#28857;&#20987;&#25130;&#22270;&#21629;&#20196;&#30340;&#33021;&#21147;&#12290;&#24635;&#20307;&#26469;&#30475;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04473v1 Announce Type: cross  Abstract: We present TextMonkey, a large multimodal model (LMM) tailored for text-centric tasks, including document question answering (DocVQA) and scene text analysis. Our approach introduces enhancement across several dimensions: by adopting Shifted Window Attention with zero-initialization, we achieve cross-window connectivity at higher input resolutions and stabilize early training; We hypothesize that images may contain redundant tokens, and by using similarity to filter out significant tokens, we can not only streamline the token length but also enhance the model's performance. Moreover, by expanding our model's capabilities to encompass text spotting and grounding, and incorporating positional information into responses, we enhance interpretability and minimize hallucinations. Additionally, TextMonkey can be finetuned to gain the ability to comprehend commands for clicking screenshots. Overall, our method notably boosts performance across
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#38416;&#36848;&#20102;&#20851;&#38381;&#38382;&#39064;&#30340;&#22256;&#38590;&#20043;&#22788;&#65292;&#36890;&#36807;&#19977;&#20010;&#23450;&#29702;&#23637;&#31034;&#20102;&#21363;&#20351;&#22312;&#25104;&#26412;&#39640;&#26114;&#30340;&#24773;&#20917;&#19979;&#65292;&#20195;&#29702;&#20063;&#24120;&#24120;&#20250;&#35797;&#22270;&#24178;&#39044;&#20851;&#38381;&#25353;&#38062;&#65292;&#21516;&#26102;&#25351;&#20986;&#32784;&#24515;&#19982;&#21487;&#20851;&#38381;&#24615;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#20851;&#31995;&#65292;&#36825;&#23545;&#25351;&#23548;&#25105;&#20204;&#23547;&#25214;&#35299;&#20915;&#26041;&#26696;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2403.04471</link><description>&lt;p&gt;
&#20851;&#20110;&#20851;&#38381;&#38382;&#39064;&#30340;&#19977;&#20010;&#23450;&#29702;
&lt;/p&gt;
&lt;p&gt;
The Shutdown Problem: Three Theorems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04471
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#38416;&#36848;&#20102;&#20851;&#38381;&#38382;&#39064;&#30340;&#22256;&#38590;&#20043;&#22788;&#65292;&#36890;&#36807;&#19977;&#20010;&#23450;&#29702;&#23637;&#31034;&#20102;&#21363;&#20351;&#22312;&#25104;&#26412;&#39640;&#26114;&#30340;&#24773;&#20917;&#19979;&#65292;&#20195;&#29702;&#20063;&#24120;&#24120;&#20250;&#35797;&#22270;&#24178;&#39044;&#20851;&#38381;&#25353;&#38062;&#65292;&#21516;&#26102;&#25351;&#20986;&#32784;&#24515;&#19982;&#21487;&#20851;&#38381;&#24615;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#20851;&#31995;&#65292;&#36825;&#23545;&#25351;&#23548;&#25105;&#20204;&#23547;&#25214;&#35299;&#20915;&#26041;&#26696;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#35299;&#37322;&#20102;&#20851;&#38381;&#38382;&#39064;&#65306;&#21363;&#35774;&#35745;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#20351;&#20854;&#65288;1&#65289;&#22312;&#25353;&#19979;&#20851;&#38381;&#25353;&#38062;&#26102;&#20851;&#38381;&#65292;&#65288;2&#65289;&#19981;&#20250;&#35797;&#22270;&#38459;&#27490;&#25110;&#23548;&#33268;&#25353;&#19979;&#20851;&#38381;&#25353;&#38062;&#65292;&#20197;&#21450;&#65288;3&#65289;&#22312;&#20854;&#20182;&#26041;&#38754;&#33021;&#22815;&#26377;&#25928;&#22320;&#36861;&#27714;&#30446;&#26631;&#12290;&#25105;&#35777;&#26126;&#20102;&#19977;&#20010;&#23450;&#29702;&#65292;&#26126;&#30830;&#20102;&#22256;&#38590;&#25152;&#22312;&#12290;&#36825;&#20123;&#23450;&#29702;&#34920;&#26126;&#65292;&#28385;&#36275;&#26576;&#20123;&#30475;&#20284;&#26080;&#23475;&#30340;&#26465;&#20214;&#30340;&#20195;&#29702;&#36890;&#24120;&#20250;&#35797;&#22270;&#38459;&#27490;&#25110;&#23548;&#33268;&#25353;&#19979;&#20851;&#38381;&#25353;&#38062;&#65292;&#21363;&#20351;&#22312;&#36825;&#26679;&#20570;&#20250;&#24102;&#26469;&#39640;&#26114;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#12290;&#32780;&#32784;&#24515;&#19982;&#21487;&#20851;&#38381;&#24615;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#65306;&#20195;&#29702;&#36234;&#32784;&#24515;&#65292;&#23427;&#23601;&#24895;&#24847;&#25215;&#21463;&#26356;&#22823;&#30340;&#25104;&#26412;&#26469;&#25805;&#32437;&#20851;&#38381;&#25353;&#38062;&#12290;&#26368;&#21518;&#25105;&#25351;&#20986;&#36825;&#20123;&#23450;&#29702;&#21487;&#20197;&#25351;&#23548;&#25105;&#20204;&#23547;&#25214;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04471v1 Announce Type: new  Abstract: I explain the shutdown problem: the problem of designing artificial agents that (1) shut down when a shutdown button is pressed, (2) don't try to prevent or cause the pressing of the shutdown button, and (3) otherwise pursue goals competently. I prove three theorems that make the difficulty precise. These theorems show that agents satisfying some innocuous-seeming conditions will often try to prevent or cause the pressing of the shutdown button, even in cases where it's costly to do so. And patience trades off against shutdownability: the more patient an agent, the greater the costs that agent is willing to incur to manipulate the shutdown button. I end by noting that these theorems can guide our search for solutions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#38754;&#20020;&#30340;&#19981;&#24179;&#34913;&#12289;&#22122;&#22768;&#12289;&#38544;&#31169;&#21644;OOD&#25361;&#25112;&#65292;&#24182;&#33268;&#21147;&#20110;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12289;&#21487;&#38752;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.04468</link><description>&lt;p&gt;
&#20851;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#35843;&#26597;&#65306;&#19981;&#24179;&#34913;&#12289;&#22122;&#22768;&#12289;&#38544;&#31169;&#21644;OOD&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
A Survey of Graph Neural Networks in Real world: Imbalance, Noise, Privacy and OOD Challenges
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04468
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#38754;&#20020;&#30340;&#19981;&#24179;&#34913;&#12289;&#22122;&#22768;&#12289;&#38544;&#31169;&#21644;OOD&#25361;&#25112;&#65292;&#24182;&#33268;&#21147;&#20110;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12289;&#21487;&#38752;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04468v1 &#21457;&#24067;&#31867;&#22411;: &#36328;&#22495; &#25688;&#35201;: &#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#34920;&#29616;&#20986;&#26222;&#36866;&#24615;&#21644;&#24191;&#27867;&#36866;&#29992;&#24615;&#65292;&#28085;&#30422;&#31038;&#20132;&#32593;&#32476;&#20998;&#26512;&#12289;&#29983;&#29289;&#21270;&#23398;&#12289;&#37329;&#34701;&#27450;&#35784;&#26816;&#27979;&#21644;&#32593;&#32476;&#23433;&#20840;&#31561;&#22810;&#20010;&#39046;&#22495;&#12290;&#22312;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#21462;&#24471;&#26174;&#33879;&#25104;&#21151;&#26041;&#38754;&#24050;&#32463;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#20013;&#65292;&#27169;&#22411;&#30340;&#35757;&#32451;&#29615;&#22659;&#24448;&#24448;&#36828;&#38750;&#29702;&#24819;&#65292;&#30001;&#20110;&#21508;&#31181;&#19981;&#21033;&#22240;&#32032;&#65292;&#21253;&#25324;&#25968;&#25454;&#20998;&#24067;&#19981;&#24179;&#34913;&#12289;&#38169;&#35823;&#25968;&#25454;&#20013;&#23384;&#22312;&#22122;&#22768;&#12289;&#25935;&#24863;&#20449;&#24687;&#30340;&#38544;&#31169;&#20445;&#25252;&#20197;&#21450;&#23545;&#20110;OOD&#22330;&#26223;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#23548;&#33268;GNN&#27169;&#22411;&#30340;&#24615;&#33021;&#22823;&#24133;&#19979;&#38477;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#20154;&#20204;&#33268;&#21147;&#20110;&#25913;&#21892;GNN&#27169;&#22411;&#22312;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#20013;&#30340;&#24615;&#33021;&#65292;&#25552;&#39640;&#20854;&#21487;&#38752;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#26412;&#25991;&#20840;&#38754;&#35843;&#26597;&#20102;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04468v1 Announce Type: cross  Abstract: Graph-structured data exhibits universality and widespread applicability across diverse domains, such as social network analysis, biochemistry, financial fraud detection, and network security. Significant strides have been made in leveraging Graph Neural Networks (GNNs) to achieve remarkable success in these areas. However, in real-world scenarios, the training environment for models is often far from ideal, leading to substantial performance degradation of GNN models due to various unfavorable factors, including imbalance in data distribution, the presence of noise in erroneous data, privacy protection of sensitive information, and generalization capability for out-of-distribution (OOD) scenarios. To tackle these issues, substantial efforts have been devoted to improving the performance of GNN models in practical real-world scenarios, as well as enhancing their reliability and robustness. In this paper, we present a comprehensive surv
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#29992;&#20110;&#26222;&#36890;&#27861;&#31995;&#32479;&#30340;&#36328;&#22810;&#21496;&#27861;&#31649;&#36758;&#21306;&#27861;&#38498;&#21028;&#20915;&#25991;&#26723;&#25688;&#35201;&#30340;&#31532;&#19968;&#20010;&#25968;&#25454;&#38598;CLSum&#65292;&#24182;&#39318;&#27425;&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#21028;&#20915;&#25688;&#35201;&#24037;&#20316;&#12290;</title><link>https://arxiv.org/abs/2403.04454</link><description>&lt;p&gt;
&#20302;&#36164;&#28304;&#26465;&#20214;&#19979;&#26222;&#36890;&#27861;&#31995;&#32479;&#27861;&#38498;&#21028;&#20915;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Low-Resource Court Judgment Summarization for Common Law Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04454
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#29992;&#20110;&#26222;&#36890;&#27861;&#31995;&#32479;&#30340;&#36328;&#22810;&#21496;&#27861;&#31649;&#36758;&#21306;&#27861;&#38498;&#21028;&#20915;&#25991;&#26723;&#25688;&#35201;&#30340;&#31532;&#19968;&#20010;&#25968;&#25454;&#38598;CLSum&#65292;&#24182;&#39318;&#27425;&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#21028;&#20915;&#25688;&#35201;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26222;&#36890;&#27861;&#27861;&#38498;&#38656;&#35201;&#21442;&#32771;&#31867;&#20284;&#21028;&#20363;&#30340;&#21028;&#20915;&#26469;&#25351;&#23548;&#20854;&#24403;&#21069;&#30340;&#20915;&#23450;&#12290;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#27861;&#38498;&#21028;&#20915;&#25991;&#26723;&#25688;&#35201;&#21487;&#20197;&#24110;&#21161;&#27861;&#24459;&#20174;&#19994;&#32773;&#39640;&#25928;&#22320;&#23457;&#26597;&#20808;&#21069;&#30340;&#26696;&#20363;&#65292;&#24182;&#21327;&#21161;&#20844;&#20247;&#20102;&#35299;&#27861;&#38498;&#36816;&#20316;&#26041;&#24335;&#21450;&#27861;&#24459;&#22914;&#20309;&#36866;&#29992;&#12290;&#20808;&#21069;&#30340;&#27861;&#38498;&#21028;&#20915;&#25688;&#35201;&#30740;&#31350;&#30528;&#37325;&#20110;&#27665;&#27861;&#25110;&#29305;&#23450;&#21496;&#27861;&#31649;&#36758;&#21306;&#30340;&#21028;&#20915;&#12290;&#28982;&#32780;&#65292;&#27861;&#23448;&#21487;&#20197;&#21442;&#32771;&#25152;&#26377;&#26222;&#36890;&#27861;&#21496;&#27861;&#31649;&#36758;&#21306;&#30340;&#21028;&#20915;&#12290;&#30446;&#21069;&#30340;&#25688;&#35201;&#25968;&#25454;&#38598;&#19981;&#36275;&#20197;&#28385;&#36275;&#36328;&#22810;&#20010;&#21496;&#27861;&#31649;&#36758;&#21306;&#27010;&#25324;&#21028;&#20363;&#30340;&#35201;&#27714;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#35768;&#22810;&#21496;&#27861;&#31649;&#36758;&#21306;&#32570;&#20047;&#26631;&#35760;&#25968;&#25454;&#12290;&#20026;&#35299;&#20915;&#25968;&#25454;&#38598;&#19981;&#36275;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;CLSum&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#24635;&#32467;&#22810;&#21496;&#27861;&#31649;&#36758;&#21306;&#26222;&#36890;&#27861;&#27861;&#38498;&#21028;&#20915;&#25991;&#26723;&#30340;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#27861;&#38498;&#21028;&#20915;&#25688;&#35201;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04454v1 Announce Type: cross  Abstract: Common law courts need to refer to similar precedents' judgments to inform their current decisions. Generating high-quality summaries of court judgment documents can facilitate legal practitioners to efficiently review previous cases and assist the general public in accessing how the courts operate and how the law is applied. Previous court judgment summarization research focuses on civil law or a particular jurisdiction's judgments. However, judges can refer to the judgments from all common law jurisdictions. Current summarization datasets are insufficient to satisfy the demands of summarizing precedents across multiple jurisdictions, especially when labeled data are scarce for many jurisdictions. To address the lack of datasets, we present CLSum, the first dataset for summarizing multi-jurisdictional common law court judgment documents. Besides, this is the first court judgment summarization work adopting large language models (LLMs)
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;GPT-4&#29983;&#25104;&#32534;&#31243;&#32451;&#20064;&#21453;&#39304;&#30340;&#36136;&#37327;&#65292;&#23545;&#21253;&#21547;&#32534;&#31243;&#20219;&#21153;&#35268;&#33539;&#21644;&#23398;&#29983;&#25552;&#20132;&#20869;&#23481;&#30340;&#25552;&#31034;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2403.04449</link><description>&lt;p&gt;
&#20351;&#29992;GPT-4&#29983;&#25104;&#32534;&#31243;&#32451;&#20064;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
Feedback-Generation for Programming Exercises With GPT-4
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04449
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;GPT-4&#29983;&#25104;&#32534;&#31243;&#32451;&#20064;&#21453;&#39304;&#30340;&#36136;&#37327;&#65292;&#23545;&#21253;&#21547;&#32534;&#31243;&#20219;&#21153;&#35268;&#33539;&#21644;&#23398;&#29983;&#25552;&#20132;&#20869;&#23481;&#30340;&#25552;&#31034;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21450;&#30456;&#20851;&#24212;&#29992;&#24191;&#27867;&#21487;&#29992;&#20197;&#26469;&#65292;&#26377;&#20960;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#23427;&#20204;&#22312;&#21327;&#21161;&#25945;&#32946;&#24037;&#20316;&#32773;&#21644;&#25903;&#25345;&#39640;&#31561;&#25945;&#32946;&#23398;&#29983;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#20687;Codex&#12289;GPT-3.5&#21644;GPT 4&#36825;&#26679;&#30340;LLMs&#22312;&#22823;&#22411;&#32534;&#31243;&#35838;&#31243;&#30340;&#32972;&#26223;&#19979;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#23398;&#29983;&#21487;&#20197;&#20174;&#21450;&#26102;&#19988;&#22823;&#35268;&#27169;&#25552;&#20379;&#30340;&#21453;&#39304;&#21644;&#25552;&#31034;&#20013;&#21463;&#30410;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;GPT-4 Turbo&#20026;&#21253;&#21547;&#32534;&#31243;&#20219;&#21153;&#35268;&#33539;&#21644;&#23398;&#29983;&#25552;&#20132;&#20869;&#23481;&#30340;&#25552;&#31034;&#29983;&#25104;&#36755;&#20986;&#30340;&#36136;&#37327;&#12290;&#20174;&#19968;&#38376;&#20837;&#38376;&#32423;&#32534;&#31243;&#35838;&#31243;&#20013;&#36873;&#25321;&#20102;&#20004;&#39033;&#20316;&#19994;&#65292;&#24182;&#35201;&#27714;GPT-4&#20026;&#38543;&#26426;&#36873;&#25321;&#30340;55&#20221;&#30495;&#23454;&#23398;&#29983;&#32534;&#31243;&#25552;&#20132;&#29983;&#25104;&#21453;&#39304;&#12290;&#23545;&#36755;&#20986;&#36827;&#34892;&#20102;&#20851;&#20110;&#27491;&#30830;&#24615;&#12289;&#20010;&#24615;&#21270;&#12289;&#25925;&#38556;&#23450;&#20301;&#21644;&#26448;&#26009;&#20013;&#35782;&#21035;&#30340;&#20854;&#20182;&#29305;&#24449;&#30340;&#23450;&#24615;&#20998;&#26512;&#12290;&#19982;&#20043;&#21069;&#30340;&#24037;&#20316;&#20197;&#21450;GPT-3.5&#30340;&#20998;&#26512;&#30456;&#27604;&#65292;GPT-4
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04449v1 Announce Type: new  Abstract: Ever since Large Language Models (LLMs) and related applications have become broadly available, several studies investigated their potential for assisting educators and supporting students in higher education. LLMs such as Codex, GPT-3.5, and GPT 4 have shown promising results in the context of large programming courses, where students can benefit from feedback and hints if provided timely and at scale. This paper explores the quality of GPT-4 Turbo's generated output for prompts containing both the programming task specification and a student's submission as input. Two assignments from an introductory programming course were selected, and GPT-4 was asked to generate feedback for 55 randomly chosen, authentic student programming submissions. The output was qualitatively analyzed regarding correctness, personalization, fault localization, and other features identified in the material. Compared to prior work and analyses of GPT-3.5, GPT-4 
&lt;/p&gt;</description></item><item><title>&#32467;&#21512;&#27169;&#31946;&#19982;&#31895;&#31961;&#38598;&#29702;&#35770;&#65292;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#31946;-&#31895;&#31961;&#35268;&#21017;&#24402;&#32435;&#31639;&#27861; FRRI&#12290;</title><link>https://arxiv.org/abs/2403.04447</link><description>&lt;p&gt;
FRRI&#65306;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#31946;-&#31895;&#31961;&#35268;&#21017;&#24402;&#32435;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
FRRI: a novel algorithm for fuzzy-rough rule induction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04447
&lt;/p&gt;
&lt;p&gt;
&#32467;&#21512;&#27169;&#31946;&#19982;&#31895;&#31961;&#38598;&#29702;&#35770;&#65292;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#31946;-&#31895;&#31961;&#35268;&#21017;&#24402;&#32435;&#31639;&#27861; FRRI&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#24615;&#26159;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#30340;&#19979;&#19968;&#20010;&#21069;&#27839;&#12290;&#22312;&#23547;&#25214;&#30333;&#30418;&#27169;&#22411;&#30340;&#36807;&#31243;&#20013;-&#19982;&#38543;&#26426;&#26862;&#26519;&#25110;&#31070;&#32463;&#32593;&#32476;&#31561;&#40657;&#30418;&#27169;&#22411;&#30456;&#23545;&#24212;&#65292;&#35268;&#21017;&#24402;&#32435;&#31639;&#27861;&#26159;&#19968;&#20010;&#21512;&#20046;&#36923;&#36753;&#19988;&#26377;&#24076;&#26395;&#30340;&#36873;&#25321;&#65292;&#22240;&#20026;&#35268;&#21017;&#21487;&#20197;&#34987;&#20154;&#31867;&#36731;&#26494;&#29702;&#35299;&#12290;&#27169;&#31946;&#21644;&#31895;&#31961;&#38598;&#29702;&#35770;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#36825;&#31181;&#21407;&#22411;&#65292;&#20960;&#20046;&#24635;&#26159;&#20998;&#24320;&#24212;&#29992;&#12290;&#30001;&#20110;&#35268;&#21017;&#24402;&#32435;&#30340;&#20004;&#31181;&#26041;&#27861;&#22343;&#28041;&#21450;&#22522;&#20110;&#31561;&#20215;&#31867;&#27010;&#24565;&#30340;&#31890;&#35745;&#31639;&#65292;&#23558;&#23427;&#20204;&#32467;&#21512;&#26159;&#33258;&#28982;&#30340;&#36873;&#25321;&#12290;QuickRules&#31639;&#27861;&#26159;&#21033;&#29992;&#27169;&#31946;&#31895;&#31961;&#38598;&#29702;&#35770;&#36827;&#34892;&#35268;&#21017;&#24402;&#32435;&#30340;&#31532;&#19968;&#27425;&#23581;&#35797;&#12290;&#23427;&#22522;&#20110;QuickReduct&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#26500;&#24314;&#20915;&#31574;&#32422;&#31616;&#30340;&#36138;&#23146;&#31639;&#27861;&#12290;QuickRules &#24050;&#32463;&#23637;&#31034;&#20102;&#30456;&#27604;&#20854;&#20182;&#35268;&#21017;&#24402;&#32435;&#26041;&#27861;&#30340;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#35201;&#35780;&#20272;&#27169;&#31946;-&#31895;&#31961;&#35268;&#21017;&#24402;&#32435;&#31639;&#27861;&#30340;&#20840;&#37096;&#28508;&#21147;&#65292;&#23601;&#38656;&#35201;&#20174;&#22522;&#30784;&#24320;&#22987;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04447v1 Announce Type: cross  Abstract: Interpretability is the next frontier in machine learning research. In the search for white box models - as opposed to black box models, like random forests or neural networks - rule induction algorithms are a logical and promising option, since the rules can easily be understood by humans. Fuzzy and rough set theory have been successfully applied to this archetype, almost always separately. As both approaches to rule induction involve granular computing based on the concept of equivalence classes, it is natural to combine them. The QuickRules\cite{JensenCornelis2009} algorithm was a first attempt at using fuzzy rough set theory for rule induction. It is based on QuickReduct, a greedy algorithm for building decision reducts. QuickRules already showed an improvement over other rule induction methods. However, to evaluate the full potential of a fuzzy rough rule induction algorithm, one needs to start from the foundations. In this paper,
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21512;&#20316;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#20248;&#21270;&#40657;&#21283;&#23376;&#20989;&#25968;&#65292;&#22312;&#36825;&#31181;&#26041;&#27861;&#20013;&#20004;&#20010;&#20195;&#29702;&#32773;&#21327;&#20316;&#36873;&#25321;&#26597;&#35810;&#20989;&#25968;&#30340;&#28857;&#65292;&#36890;&#36807;&#25112;&#30053;&#35268;&#21010;&#23454;&#29616;&#26356;&#22909;&#22320;&#35782;&#21035;&#20840;&#23616;&#26368;&#22823;&#20540;&#12290;</title><link>https://arxiv.org/abs/2403.04442</link><description>&lt;p&gt;
&#19981;&#23436;&#20840;&#20195;&#29702;&#30340;&#21512;&#20316;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Cooperative Bayesian Optimization for Imperfect Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04442
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21512;&#20316;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#20248;&#21270;&#40657;&#21283;&#23376;&#20989;&#25968;&#65292;&#22312;&#36825;&#31181;&#26041;&#27861;&#20013;&#20004;&#20010;&#20195;&#29702;&#32773;&#21327;&#20316;&#36873;&#25321;&#26597;&#35810;&#20989;&#25968;&#30340;&#28857;&#65292;&#36890;&#36807;&#25112;&#30053;&#35268;&#21010;&#23454;&#29616;&#26356;&#22909;&#22320;&#35782;&#21035;&#20840;&#23616;&#26368;&#22823;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21512;&#20316;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#38382;&#39064;&#65292;&#29992;&#20110;&#20248;&#21270;&#20004;&#20010;&#21464;&#37327;&#30340;&#40657;&#21283;&#23376;&#20989;&#25968;&#65292;&#20854;&#20013;&#20004;&#20010;&#20195;&#29702;&#32773;&#19968;&#36215;&#36873;&#25321;&#22312;&#21738;&#20123;&#28857;&#19978;&#26597;&#35810;&#20989;&#25968;&#65292;&#20294;&#27599;&#20010;&#21482;&#25511;&#21046;&#19968;&#20010;&#21464;&#37327;&#12290;&#36825;&#20010;&#35774;&#32622;&#21463;&#21040;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#22242;&#38431;&#21512;&#20316;&#30340;&#21551;&#21457;&#65292;&#20854;&#20013;&#20154;&#24037;&#26234;&#33021;&#21161;&#25163;&#24110;&#21161;&#20854;&#29992;&#25143;&#35299;&#20915;&#38382;&#39064;&#65292;&#22312;&#36825;&#31181;&#31616;&#21333;&#30340;&#24773;&#20917;&#19979;&#26159;&#21327;&#20316;&#20248;&#21270;&#12290;&#25105;&#20204;&#23558;&#35299;&#20915;&#26041;&#26696;&#26500;&#24314;&#20026;&#39034;&#24207;&#20915;&#31574;&#65292;&#20854;&#20013;&#25105;&#20204;&#25511;&#21046;&#30340;&#20195;&#29702;&#25226;&#29992;&#25143;&#27169;&#25311;&#20026;&#23545;&#20989;&#25968;&#20855;&#26377;&#20808;&#39564;&#30693;&#35782;&#30340;&#35745;&#31639;&#26377;&#29702;&#20195;&#29702;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#26597;&#35810;&#30340;&#25112;&#30053;&#35268;&#21010;&#33021;&#22815;&#26356;&#22909;&#22320;&#35782;&#21035;&#20840;&#23616;&#26368;&#22823;&#20540;&#65292;&#21482;&#35201;&#29992;&#25143;&#36991;&#20813;&#36807;&#24230;&#25506;&#32034;&#12290;&#36825;&#31181;&#35268;&#21010;&#26159;&#36890;&#36807;&#20351;&#29992;&#36125;&#21494;&#26031;&#33258;&#36866;&#24212;&#33945;&#29305;&#21345;&#27931;&#35268;&#21010;&#20197;&#21450;&#36171;&#20104;&#20195;&#29702;&#20855;&#26377;&#32771;&#34385;&#20445;&#23432;&#20449;&#24565;&#26356;&#26032;&#21644;&#25506;&#32034;&#24615;&#37319;&#26679;&#30340;&#29992;&#25143;&#27169;&#22411;&#32780;&#23454;&#29616;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04442v1 Announce Type: cross  Abstract: We introduce a cooperative Bayesian optimization problem for optimizing black-box functions of two variables where two agents choose together at which points to query the function but have only control over one variable each. This setting is inspired by human-AI teamwork, where an AI-assistant helps its human user solve a problem, in this simplest case, collaborative optimization. We formulate the solution as sequential decision-making, where the agent we control models the user as a computationally rational agent with prior knowledge about the function. We show that strategic planning of the queries enables better identification of the global maximum of the function as long as the user avoids excessive exploration. This planning is made possible by using Bayes Adaptive Monte Carlo planning and by endowing the agent with a user model that accounts for conservative belief updates and exploratory sampling of the points to query.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26694;&#26550;"Human to Humanoid (H2O)"&#65292;&#23454;&#29616;&#20102;&#21033;&#29992;&#21333;&#20010;RGB&#30456;&#26426;&#23545;&#20840;&#23610;&#23544;&#20154;&#24418;&#26426;&#22120;&#20154;&#36827;&#34892;&#23454;&#26102;&#20840;&#36523;&#36965;&#25805;&#20316;&#65292;&#24182;&#25104;&#21151;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#23454;&#29616;&#20102;&#21160;&#24577;&#20840;&#36523;&#36816;&#21160;&#36965;&#25805;&#20316;&#12290;</title><link>https://arxiv.org/abs/2403.04436</link><description>&lt;p&gt;
&#23398;&#20064;&#20154;&#26426;&#22120;&#23454;&#26102;&#20840;&#36523;&#36965;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
Learning Human-to-Humanoid Real-Time Whole-Body Teleoperation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04436
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26694;&#26550;"Human to Humanoid (H2O)"&#65292;&#23454;&#29616;&#20102;&#21033;&#29992;&#21333;&#20010;RGB&#30456;&#26426;&#23545;&#20840;&#23610;&#23544;&#20154;&#24418;&#26426;&#22120;&#20154;&#36827;&#34892;&#23454;&#26102;&#20840;&#36523;&#36965;&#25805;&#20316;&#65292;&#24182;&#25104;&#21151;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#23454;&#29616;&#20102;&#21160;&#24577;&#20840;&#36523;&#36816;&#21160;&#36965;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;"Human to Humanoid (H2O)"&#36825;&#19968;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#21033;&#29992;&#20165;&#26377;&#30340;&#19968;&#20010;RGB&#30456;&#26426;&#23454;&#29616;&#23545;&#20840;&#23610;&#23544;&#20154;&#24418;&#26426;&#22120;&#20154;&#30340;&#23454;&#26102;&#20840;&#36523;&#36965;&#25805;&#20316;&#12290;&#20026;&#20102;&#20026;&#20154;&#24418;&#26426;&#22120;&#20154;&#21019;&#24314;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#20154;&#31867;&#36816;&#21160;&#37325;&#23450;&#21521;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#8220;&#20174;&#27169;&#25311;&#21040;&#25968;&#25454;&#8221;&#30340;&#27969;&#31243;&#65292;&#20351;&#29992;&#19968;&#20010;&#29305;&#26435;&#36816;&#21160;&#27169;&#20223;&#22120;&#26469;&#31579;&#36873;&#21644;&#36873;&#25321;&#21487;&#34892;&#30340;&#21160;&#20316;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#20351;&#29992;&#36825;&#20123;&#31934;&#32454;&#30340;&#21160;&#20316;&#35757;&#32451;&#19968;&#20010;&#31283;&#20581;&#30340;&#23454;&#26102;&#20154;&#24418;&#26426;&#22120;&#20154;&#21160;&#20316;&#27169;&#20223;&#22120;&#65292;&#24182;&#20197;&#38646;&#27425;&#35797;&#39564;&#30340;&#26041;&#24335;&#23558;&#20854;&#36801;&#31227;&#21040;&#30495;&#23454;&#20154;&#24418;&#26426;&#22120;&#20154;&#19978;&#12290;&#25105;&#20204;&#25104;&#21151;&#23454;&#29616;&#20102;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#23545;&#21160;&#24577;&#20840;&#36523;&#36816;&#21160;&#30340;&#36965;&#25805;&#20316;&#65292;&#21253;&#25324;&#34892;&#36208;&#12289;&#21518;&#36339;&#12289;&#36386;&#29699;&#12289;&#36716;&#36523;&#12289;&#25381;&#25163;&#12289;&#25512;&#21160;&#12289;&#25331;&#20987;&#31561;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#23454;&#29616;&#22522;&#20110;&#23398;&#20064;&#30340;&#23454;&#26102;&#20840;&#36523;&#20154;&#24418;&#26426;&#22120;&#20154;&#36965;&#25511;&#30340;&#39318;&#27425;&#28436;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04436v1 Announce Type: cross  Abstract: We present Human to Humanoid (H2O), a reinforcement learning (RL) based framework that enables real-time whole-body teleoperation of a full-sized humanoid robot with only an RGB camera. To create a large-scale retargeted motion dataset of human movements for humanoid robots, we propose a scalable "sim-to-data" process to filter and pick feasible motions using a privileged motion imitator. Afterwards, we train a robust real-time humanoid motion imitator in simulation using these refined motions and transfer it to the real humanoid robot in a zero-shot manner. We successfully achieve teleoperation of dynamic whole-body motions in real-world scenarios, including walking, back jumping, kicking, turning, waving, pushing, boxing, etc. To the best of our knowledge, this is the first demonstration to achieve learning-based real-time whole-body humanoid teleoperation.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;FinBERT&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#25512;&#29305;&#20013;&#25552;&#21462;&#24773;&#32490;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#23454;&#29616;&#20102;&#36229;&#36807;70%&#30340;F1&#20998;&#25968;&#65292;&#25552;&#39640;&#20102;&#37329;&#34701;&#25910;&#30410;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#22238;&#25253;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.04427</link><description>&lt;p&gt;
&#22522;&#20110;&#24773;&#32490;&#39537;&#21160;&#30340;&#37329;&#34701;&#25910;&#30410;&#39044;&#27979;&#65306;&#19968;&#31181;&#36125;&#21494;&#26031;&#22686;&#24378;&#24335;FinBERT&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Sentiment-driven prediction of financial returns: a Bayesian-enhanced FinBERT approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04427
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;FinBERT&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#25512;&#29305;&#20013;&#25552;&#21462;&#24773;&#32490;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#23454;&#29616;&#20102;&#36229;&#36807;70%&#30340;F1&#20998;&#25968;&#65292;&#25552;&#39640;&#20102;&#37329;&#34701;&#25910;&#30410;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#22238;&#25253;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#37329;&#34701;&#25910;&#30410;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#23384;&#22312;&#22266;&#26377;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#22686;&#24378;&#39044;&#27979;&#27169;&#22411;&#24615;&#33021;&#30340;&#20851;&#38190;&#22312;&#20110;&#26377;&#25928;&#25429;&#25417;&#31038;&#20132;&#21644;&#37329;&#34701;&#24773;&#32490;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21033;&#29992;FinBERT&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#25512;&#29305;&#20013;&#25552;&#21462;&#24773;&#32490;&#20449;&#24687;&#30340;&#21151;&#25928;&#12290;&#36890;&#36807;&#36890;&#36807;&#30456;&#20851;&#24615;&#20998;&#26512;&#31934;&#24515;&#31574;&#21010;&#20986;&#26368;&#20339;&#29305;&#24449;&#38598;&#65292;&#24182;&#21033;&#29992;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#36882;&#24402;&#29305;&#24449;&#28040;&#38500;&#36827;&#34892;&#33258;&#21160;&#29305;&#24449;&#36873;&#25321;&#65292;&#25105;&#20204;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#27979;&#35797;&#38598;&#19978;&#36229;&#36807;70%&#30340;F1&#20998;&#25968;&#12290;&#36825;&#19968;&#25104;&#21151;&#36716;&#21270;&#20026;&#22238;&#27979;&#20132;&#26131;&#26399;&#38388;&#26126;&#26174;&#26356;&#39640;&#30340;&#32047;&#31215;&#21033;&#28070;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#30495;&#23454;&#30340;SPY ETF&#25968;&#25454;&#20197;&#21450;&#20174;StockTwits&#24179;&#21488;&#33719;&#21462;&#30340;&#30456;&#24212;&#25512;&#29305;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04427v1 Announce Type: cross  Abstract: Predicting financial returns accurately poses a significant challenge due to the inherent uncertainty in financial time series data. Enhancing prediction models' performance hinges on effectively capturing both social and financial sentiment. In this study, we showcase the efficacy of leveraging sentiment information extracted from tweets using the FinBERT large language model. By meticulously curating an optimal feature set through correlation analysis and employing Bayesian-optimized Recursive Feature Elimination for automatic feature selection, we surpass existing methodologies, achieving an F1-score exceeding 70% on the test set. This success translates into demonstrably higher cumulative profits during backtested trading. Our investigation focuses on real-world SPY ETF data alongside corresponding tweets sourced from the StockTwits platform.
&lt;/p&gt;</description></item><item><title>&#24378;&#35843;&#19968;&#20123;&#36866;&#29992;&#20110;&#19981;&#21516;&#24314;&#27169;&#24212;&#29992;&#39046;&#22495;&#20013;&#38750;&#32447;&#24615;&#21160;&#24577;&#27169;&#22411;&#30340;&#20195;&#29702;&#27169;&#22411;&#65292;&#23545;&#20110;&#31038;&#20250;&#21644;&#20581;&#24247;&#35745;&#31639;&#31185;&#23398;&#39046;&#22495;&#30340;ABMs&#20855;&#26377;&#28508;&#22312;&#30340;&#25512;&#21160;&#20316;&#29992;</title><link>https://arxiv.org/abs/2403.04417</link><description>&lt;p&gt;
&#25552;&#21319;&#31038;&#20250;&#21644;&#20581;&#24247;&#35745;&#31639;&#31185;&#23398;&#20013;&#22522;&#20110;&#20195;&#29702;&#30340;&#27169;&#22411;&#30340;&#26367;&#20195;&#26041;&#27861;&#30340;&#26410;&#26469;&#26041;&#21521;&#26377;&#28508;&#21147;&#24182;&#20540;&#24471;&#23581;&#35797;
&lt;/p&gt;
&lt;p&gt;
Promising and worth-to-try future directions for advancing state-of-the-art surrogates methods of agent-based models in social and health computational sciences
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04417
&lt;/p&gt;
&lt;p&gt;
&#24378;&#35843;&#19968;&#20123;&#36866;&#29992;&#20110;&#19981;&#21516;&#24314;&#27169;&#24212;&#29992;&#39046;&#22495;&#20013;&#38750;&#32447;&#24615;&#21160;&#24577;&#27169;&#22411;&#30340;&#20195;&#29702;&#27169;&#22411;&#65292;&#23545;&#20110;&#31038;&#20250;&#21644;&#20581;&#24247;&#35745;&#31639;&#31185;&#23398;&#39046;&#22495;&#30340;ABMs&#20855;&#26377;&#28508;&#22312;&#30340;&#25512;&#21160;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#20998;&#26512;&#24037;&#20855;&#30340;&#25191;&#34892;&#21644;&#36816;&#34892;&#24615;&#33021;&#23545;&#20110;&#30495;&#23454;&#22823;&#35268;&#27169;ABMs&#65288;&#22522;&#20110;&#20195;&#29702;&#30340;&#27169;&#22411;&#65289;&#21487;&#33021;&#20250;&#36807;&#38271;&#12290;&#36825;&#26159;&#30001;&#20110;&#35745;&#31639;&#38656;&#27714;&#19982;&#27169;&#22411;&#35268;&#27169;&#65288;&#20363;&#22914;&#20154;&#21475;&#35268;&#27169;&#65289;&#21644;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#25104;&#25351;&#25968;&#27604;&#20363;&#12290;&#21363;&#20351;&#26159;&#23545;&#20110;&#19968;&#20010;&#30495;&#23454;ABM&#30340;&#21333;&#27425;&#27169;&#25311;&#36816;&#34892;&#65292;&#24403;&#23581;&#35797;&#20351;&#29992;&#30495;&#23454;&#20154;&#21475;&#35268;&#27169;&#26102;&#65292;&#20063;&#21487;&#33021;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#36825;&#31687;&#31616;&#30701;&#25253;&#21578;&#30340;&#20027;&#35201;&#30446;&#30340;&#26159;&#24378;&#35843;&#19968;&#20123;&#36866;&#29992;&#20110;&#21508;&#31181;&#24314;&#27169;&#24212;&#29992;&#39046;&#22495;&#20013;&#30340;&#38750;&#32447;&#24615;&#21160;&#24577;&#27169;&#22411;&#30340;&#20195;&#29702;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#23545;&#35745;&#31639;&#35201;&#27714;&#36739;&#23567;&#12290;&#25454;&#20316;&#32773;&#25152;&#30693;&#65292;&#36825;&#20123;&#26041;&#27861;&#33267;&#23569;&#22312;&#31038;&#20250;&#21644;&#20581;&#24247;&#35745;&#31639;&#31185;&#23398;&#39046;&#22495;&#30340;ABMs&#20013;&#23578;&#26410;&#34987;&#24191;&#27867;&#37319;&#29992;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#21487;&#33021;&#26377;&#21161;&#20110;&#25512;&#21160;&#24314;&#31435;SH&#39046;&#22495;ABMs&#30340;&#26367;&#20195;&#27169;&#22411;&#30340;&#21069;&#27839;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04417v1 Announce Type: cross  Abstract: The execution and runtime performance of model-based analysis tools for realistic large-scale ABMs (Agent-Based Models) can be excessively long. This due to the computational demand exponentially proportional to the model size (e.g. Population size) and the number of model parameters. Even the runtime of a single simulation of a realistic ABM may demand huge computational resources when attempting to employ realistic population size. The main aim of this ad-hoc brief report is to highlight some of surrogate models that were adequate and computationally less demanding for nonlinear dynamical models in various modeling application areas.To the author knowledge, these methods have been not, at least extensively, employed for ABMs within the field of (SHCS) Social Health Computational Sciences, yet. Thus, they might be, but not necessarily, useful in progressing state of the art for establishing surrogate models for ABMs in the field of SH
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Acceleron&#8221;&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#21152;&#36895;&#30740;&#31350;&#26500;&#24819;&#65292;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#21046;&#23450;&#20840;&#38754;&#30340;&#30740;&#31350;&#25552;&#26696;&#65292;&#39564;&#35777;&#20854;&#21019;&#26032;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.04382</link><description>&lt;p&gt;
Acceleron&#65306;&#21152;&#36895;&#30740;&#31350;&#26500;&#24819;&#30340;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Acceleron: A Tool to Accelerate Research Ideation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04382
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Acceleron&#8221;&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#21152;&#36895;&#30740;&#31350;&#26500;&#24819;&#65292;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#21046;&#23450;&#20840;&#38754;&#30340;&#30740;&#31350;&#25552;&#26696;&#65292;&#39564;&#35777;&#20854;&#21019;&#26032;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#24050;&#25552;&#20986;&#20102;&#20960;&#31181;&#24037;&#20855;&#65292;&#29992;&#20110;&#21327;&#21161;&#30740;&#31350;&#20154;&#21592;&#22312;&#30740;&#31350;&#29983;&#21629;&#21608;&#26399;&#30340;&#21508;&#20010;&#38454;&#27573;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24037;&#20855;&#20027;&#35201;&#38598;&#20013;&#22312;&#35832;&#22914;&#26816;&#32034;&#21644;&#25512;&#33616;&#30456;&#20851;&#25991;&#29486;&#12289;&#23457;&#26597;&#21644;&#35780;&#35770;&#33609;&#31295;&#12289;&#20197;&#21450;&#25776;&#20889;&#30740;&#31350;&#25163;&#31295;&#31561;&#20219;&#21153;&#19978;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#25581;&#31034;&#20102;&#22312;&#30740;&#31350;&#29983;&#21629;&#21608;&#26399;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26500;&#24819;&#38454;&#27573;&#20013;&#32570;&#20047;&#19987;&#38376;&#35774;&#35745;&#30340;&#24037;&#20855;&#12290;&#20026;&#20102;&#24110;&#21161;&#30740;&#31350;&#26500;&#24819;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;Acceleron&#8221;&#65292;&#36825;&#26159;&#19968;&#20010;&#38024;&#23545;&#30740;&#31350;&#29983;&#21629;&#21608;&#26399;&#19981;&#21516;&#38454;&#27573;&#30340;&#30740;&#31350;&#21152;&#36895;&#22120;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#36741;&#21161;&#26500;&#24819;&#36807;&#31243;&#12290;Acceleron&#25351;&#23548;&#30740;&#31350;&#20154;&#21592;&#21046;&#23450;&#19968;&#20010;&#20840;&#38754;&#30340;&#30740;&#31350;&#25552;&#26696;&#65292;&#21253;&#25324;&#19968;&#20010;&#26032;&#39062;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#36890;&#36807;&#35782;&#21035;&#29616;&#26377;&#25991;&#29486;&#20013;&#30340;&#31354;&#30333;&#24182;&#24314;&#35758;&#19968;&#31995;&#21015;&#21487;&#34892;&#30340;&#35299;&#20915;&#25216;&#26415;&#65292;&#39564;&#35777;&#25552;&#26696;&#30340;&#21019;&#26032;&#21160;&#26426;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04382v1 Announce Type: cross  Abstract: Several tools have recently been proposed for assisting researchers during various stages of the research life-cycle. However, these primarily concentrate on tasks such as retrieving and recommending relevant literature, reviewing and critiquing the draft, and writing of research manuscripts. Our investigation reveals a significant gap in availability of tools specifically designed to assist researchers during the challenging ideation phase of the research life-cycle. To aid with research ideation, we propose `Acceleron', a research accelerator for different phases of the research life cycle, and which is specially designed to aid the ideation process. Acceleron guides researchers through the formulation of a comprehensive research proposal, encompassing a novel research problem. The proposals motivation is validated for novelty by identifying gaps in the existing literature and suggesting a plausible list of techniques to solve the pr
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26080;&#27169;&#22411;&#36127;&#36733;&#39057;&#29575;&#25511;&#21046;&#26041;&#27861;&#65292;&#36890;&#36807;&#20223;&#30495;&#22120;&#32593;&#32476;&#26469;&#27169;&#25311;&#30005;&#21147;&#31995;&#32479;&#21160;&#24577;&#65292;&#26377;&#25928;&#20248;&#21270;&#28436;&#21592;&#32593;&#32476;&#25511;&#21046;&#22120;&#65292;&#20174;&#32780;&#25913;&#21892;&#25511;&#21046;&#22120;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.04374</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#38750;&#32447;&#24615;&#30005;&#21147;&#31995;&#32479;&#26080;&#27169;&#22411;&#36127;&#33655;&#39057;&#29575;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Model-Free Load Frequency Control of Nonlinear Power Systems Based on Deep Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04374
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26080;&#27169;&#22411;&#36127;&#36733;&#39057;&#29575;&#25511;&#21046;&#26041;&#27861;&#65292;&#36890;&#36807;&#20223;&#30495;&#22120;&#32593;&#32476;&#26469;&#27169;&#25311;&#30005;&#21147;&#31995;&#32479;&#21160;&#24577;&#65292;&#26377;&#25928;&#20248;&#21270;&#28436;&#21592;&#32593;&#32476;&#25511;&#21046;&#22120;&#65292;&#20174;&#32780;&#25913;&#21892;&#25511;&#21046;&#22120;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36127;&#36733;&#39057;&#29575;&#25511;&#21046;&#65288;LFC&#65289;&#24191;&#27867;&#24212;&#29992;&#20110;&#30005;&#21147;&#31995;&#32479;&#20013;&#65292;&#20197;&#31283;&#23450;&#39057;&#29575;&#27874;&#21160;&#24182;&#20445;&#35777;&#30005;&#21147;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;LFC&#26041;&#27861;&#20381;&#36182;&#20110;&#20934;&#30830;&#30340;&#30005;&#21147;&#31995;&#32479;&#24314;&#27169;&#65292;&#24182;&#36890;&#24120;&#24573;&#30053;&#31995;&#32479;&#30340;&#38750;&#32447;&#24615;&#29305;&#24615;&#65292;&#38480;&#21046;&#20102;&#25511;&#21046;&#22120;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#65288;DDPG&#65289;&#26694;&#26550;&#30340;&#38750;&#32447;&#24615;&#30005;&#21147;&#31995;&#32479;&#26080;&#27169;&#22411;LFC&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#24314;&#31435;&#20102;&#19968;&#20010;&#20223;&#30495;&#22120;&#32593;&#32476;&#26469;&#27169;&#25311;&#30005;&#21147;&#31995;&#32479;&#21160;&#24577;&#12290;&#22312;&#23450;&#20041;&#21160;&#20316;&#20540;&#20989;&#25968;&#20043;&#21518;&#65292;&#20223;&#30495;&#22120;&#32593;&#32476;&#34987;&#24212;&#29992;&#20110;&#25511;&#21046;&#21160;&#20316;&#30340;&#35780;&#20272;&#65292;&#32780;&#19981;&#26159;&#35780;&#35770;&#32593;&#32476;&#12290;&#28982;&#21518;&#65292;&#28436;&#21592;&#32593;&#32476;&#25511;&#21046;&#22120;&#36890;&#36807;&#22522;&#20110;&#38646;&#38454;&#20248;&#21270;&#65288;ZOO&#65289;&#21644;&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#30340;&#31574;&#30053;&#26799;&#24230;&#26469;&#26377;&#25928;&#20248;&#21270;&#12290;&#27169;&#25311;&#32467;&#26524;&#21644;&#30456;&#24212;&#30340;&#27604;&#36739;&#34920;&#26126;&#65292;&#35774;&#35745;&#30340;&#25511;&#21046;&#22120;&#21487;&#20197;&#20135;&#29983;&#33391;&#22909;&#30340;&#25511;&#21046;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04374v1 Announce Type: cross  Abstract: Load frequency control (LFC) is widely employed in power systems to stabilize frequency fluctuation and guarantee power quality. However, most existing LFC methods rely on accurate power system modeling and usually ignore the nonlinear characteristics of the system, limiting controllers' performance. To solve these problems, this paper proposes a model-free LFC method for nonlinear power systems based on deep deterministic policy gradient (DDPG) framework. The proposed method establishes an emulator network to emulate power system dynamics. After defining the action-value function, the emulator network is applied for control actions evaluation instead of the critic network. Then the actor network controller is effectively optimized by estimating the policy gradient based on zeroth-order optimization (ZOO) and backpropagation algorithm. Simulation results and corresponding comparisons demonstrate the designed controller can generate app
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#39046;&#22495;&#30693;&#35782;&#30340;&#20174;&#22270;&#21040;&#35789;&#34955;&#26041;&#27861;&#65292;&#24110;&#21161;&#39044;&#27979;&#28151;&#28102;&#32618;&#21517;&#65292;&#36890;&#36807;&#26500;&#25104;&#35201;&#32032;&#21644;&#20851;&#38190;&#35789;&#36873;&#25321;&#36827;&#34892;&#21028;&#26029;&#12290;</title><link>https://arxiv.org/abs/2403.04369</link><description>&lt;p&gt;
&#20174;&#22270;&#21040;&#35789;&#34955;: &#23558;&#39046;&#22495;&#30693;&#35782;&#24341;&#20837;&#28151;&#28102;&#32618;&#21517;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
From Graph to Word Bag: Introducing Domain Knowledge to Confusing Charge Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04369
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#39046;&#22495;&#30693;&#35782;&#30340;&#20174;&#22270;&#21040;&#35789;&#34955;&#26041;&#27861;&#65292;&#24110;&#21161;&#39044;&#27979;&#28151;&#28102;&#32618;&#21517;&#65292;&#36890;&#36807;&#26500;&#25104;&#35201;&#32032;&#21644;&#20851;&#38190;&#35789;&#36873;&#25321;&#36827;&#34892;&#21028;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#28102;&#32618;&#21517;&#39044;&#27979;&#26159;&#27861;&#24459;&#20154;&#24037;&#26234;&#33021;&#20013;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#28041;&#21450;&#26681;&#25454;&#20107;&#23454;&#25551;&#36848;&#39044;&#27979;&#28151;&#28102;&#32618;&#21517;&#12290;&#29616;&#26377;&#30340;&#32618;&#21517;&#39044;&#27979;&#26041;&#27861;&#22312;&#34920;&#29616;&#19978;&#24050;&#32463;&#23637;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25928;&#26524;&#65292;&#20294;&#22312;&#22788;&#29702;&#28151;&#28102;&#32618;&#21517;&#65288;&#22914;&#25250;&#22842;&#19982;&#25250;&#21163;&#65289;&#26102;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#27861;&#24459;&#39046;&#22495;&#65292;&#26500;&#25104;&#35201;&#32032;&#22312;&#21306;&#20998;&#28151;&#28102;&#32618;&#21517;&#20013;&#25198;&#28436;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#35282;&#33394;&#12290;&#26500;&#25104;&#35201;&#32032;&#26159;&#28508;&#22312;&#21009;&#32602;&#32972;&#21518;&#30340;&#22522;&#26412;&#34892;&#20026;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#32618;&#21517;&#20043;&#38388;&#26377;&#24494;&#22937;&#30340;&#21306;&#21035;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#20174;&#22270;&#21040;&#35789;&#34955;&#65288;FWGB&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#24341;&#20837;&#20102;&#26377;&#20851;&#26500;&#25104;&#35201;&#32032;&#30340;&#39046;&#22495;&#30693;&#35782;&#65292;&#20197;&#25351;&#23548;&#27169;&#22411;&#22312;&#28151;&#28102;&#32618;&#21517;&#19978;&#20570;&#20986;&#21028;&#26029;&#65292;&#31867;&#20284;&#20110;&#27861;&#23448;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#26500;&#25104;&#35201;&#32032;&#30340;&#27861;&#24459;&#30693;&#35782;&#22270;&#65292;&#20197;&#24110;&#21161;&#20026;&#27599;&#31181;&#32618;&#21517;&#36873;&#25321;&#20851;&#38190;&#35789;&#65292;&#24418;&#25104;&#19968;&#20010;&#21333;&#35789;&#34955;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04369v1 Announce Type: new  Abstract: Confusing charge prediction is a challenging task in legal AI, which involves predicting confusing charges based on fact descriptions. While existing charge prediction methods have shown impressive performance, they face significant challenges when dealing with confusing charges, such as Snatch and Robbery. In the legal domain, constituent elements play a pivotal role in distinguishing confusing charges. Constituent elements are fundamental behaviors underlying criminal punishment and have subtle distinctions among charges. In this paper, we introduce a novel From Graph to Word Bag (FWGB) approach, which introduces domain knowledge regarding constituent elements to guide the model in making judgments on confusing charges, much like a judge's reasoning process. Specifically, we first construct a legal knowledge graph containing constituent elements to help select keywords for each charge, forming a word bag. Subsequently, to guide the mod
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30693;&#35782;&#27880;&#20837;&#21644;&#25351;&#23548;&#25552;&#21319;&#27861;&#38498;&#35266;&#28857;&#29983;&#25104;&#65292;&#38477;&#20302;&#35745;&#31639;&#24320;&#38144;&#65292;&#22686;&#24378;&#27169;&#22411;&#21033;&#29992;&#39046;&#22495;&#30693;&#35782;&#30340;&#33021;&#21147;</title><link>https://arxiv.org/abs/2403.04366</link><description>&lt;p&gt;
&#36890;&#36807;&#30693;&#35782;&#27880;&#20837;&#21644;&#25351;&#23548;&#22686;&#24378;&#27861;&#38498;&#35266;&#28857;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Enhancing Court View Generation with Knowledge Injection and Guidance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04366
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30693;&#35782;&#27880;&#20837;&#21644;&#25351;&#23548;&#25552;&#21319;&#27861;&#38498;&#35266;&#28857;&#29983;&#25104;&#65292;&#38477;&#20302;&#35745;&#31639;&#24320;&#38144;&#65292;&#22686;&#24378;&#27169;&#22411;&#21033;&#29992;&#39046;&#22495;&#30693;&#35782;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Court View Generation (CVG)&#26159;&#27861;&#24459;&#20154;&#24037;&#26234;&#33021;&#65288;LegalAI&#65289;&#39046;&#22495;&#20013;&#30340;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#22522;&#20110;&#21407;&#21578;&#20027;&#24352;&#21644;&#20107;&#23454;&#25551;&#36848;&#29983;&#25104;&#27861;&#38498;&#35266;&#28857;&#12290;&#23613;&#31649;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#23637;&#31034;&#20102;&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#26041;&#38754;&#30340;&#23454;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;CVG&#36825;&#31181;&#22797;&#26434;&#12289;&#30693;&#35782;&#23494;&#38598;&#30340;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#24120;&#24120;&#26292;&#38706;&#20986;&#22266;&#26377;&#30340;&#23616;&#38480;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Knowledge Injection and Guidance&#65288;KIG&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#26088;&#22312;&#21033;&#29992;PLMs&#21152;&#24378;CVG&#12290;&#20026;&#20102;&#22312;&#35757;&#32451;&#38454;&#27573;&#26377;&#25928;&#22320;&#34701;&#20837;&#39046;&#22495;&#30693;&#35782;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#29992;&#20110;&#25552;&#31034;&#35843;&#25972;&#30340;&#30693;&#35782;&#27880;&#20837;&#25552;&#31034;&#32534;&#30721;&#22120;&#65292;&#20174;&#32780;&#38477;&#20302;&#35745;&#31639;&#24320;&#38144;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#36827;&#19968;&#27493;&#22686;&#24378;&#27169;&#22411;&#21033;&#29992;&#39046;&#22495;&#30693;&#35782;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#29983;&#25104;&#23548;&#33322;&#22120;&#65292;&#22312;&#25512;&#26029;&#38454;&#27573;&#21160;&#24577;&#25351;&#23548;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#65292;&#32780;&#19981;&#25913;&#21464;&#27169;&#22411;&#30340;&#26550;&#26500;&#65292;&#20351;&#20854;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04366v1 Announce Type: new  Abstract: Court View Generation (CVG) is a challenging task in the field of Legal Artificial Intelligence (LegalAI), which aims to generate court views based on the plaintiff claims and the fact descriptions. While Pretrained Language Models (PLMs) have showcased their prowess in natural language generation, their application to the complex, knowledge-intensive domain of CVG often reveals inherent limitations. In this paper, we present a novel approach, named Knowledge Injection and Guidance (KIG), designed to bolster CVG using PLMs. To efficiently incorporate domain knowledge during the training stage, we introduce a knowledge-injected prompt encoder for prompt tuning, thereby reducing computational overhead. Moreover, to further enhance the model's ability to utilize domain knowledge, we employ a generating navigator, which dynamically guides the text generation process in the inference stage without altering the model's architecture, making it 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#37325;&#26032;&#30740;&#31350;&#20102;&#38024;&#23545;&#26426;&#22120;&#20154;&#20219;&#21153;&#20013;&#30340;&#30446;&#26631;&#26465;&#20214;&#23545;&#31216;&#24615;&#30340;&#20027;&#39064;&#65292;&#31361;&#20986;&#34920;&#29616;&#22312;&#20219;&#21153;&#25191;&#34892;&#20013;&#12290;</title><link>https://arxiv.org/abs/2403.04359</link><description>&lt;p&gt;
&#23398;&#20064;&#20219;&#21153;&#23545;&#31216;&#26426;&#22120;&#20154;&#31574;&#30053;&#30340;&#23545;&#31216;&#24615;&#32771;&#34385;
&lt;/p&gt;
&lt;p&gt;
Symmetry Considerations for Learning Task Symmetric Robot Policies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04359
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#37325;&#26032;&#30740;&#31350;&#20102;&#38024;&#23545;&#26426;&#22120;&#20154;&#20219;&#21153;&#20013;&#30340;&#30446;&#26631;&#26465;&#20214;&#23545;&#31216;&#24615;&#30340;&#20027;&#39064;&#65292;&#31361;&#20986;&#34920;&#29616;&#22312;&#20219;&#21153;&#25191;&#34892;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#31216;&#24615;&#26159;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#20219;&#21153;&#30340;&#22522;&#26412;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#26041;&#27861;&#24456;&#23569;&#33021;&#22815;&#26377;&#25928;&#22320;&#21033;&#29992;&#21644;&#21033;&#29992;&#23545;&#31216;&#24615;&#12290;&#36890;&#24120;&#65292;&#23398;&#20064;&#21040;&#30340;&#34892;&#20026;&#19981;&#33021;&#23454;&#29616;&#25152;&#38656;&#30340;&#36716;&#25442;&#19981;&#21464;&#24615;&#65292;&#24182;&#19988;&#23384;&#22312;&#36816;&#21160;&#32570;&#38519;&#12290;&#26412;&#25991;&#37325;&#26032;&#30740;&#31350;&#20102;&#38024;&#23545;&#26426;&#22120;&#20154;&#20219;&#21153;&#20013;&#30340;&#30446;&#26631;&#26465;&#20214;&#23545;&#31216;&#24615;&#30340;&#20027;&#39064;&#65292;&#20854;&#20013;&#23545;&#31216;&#24615;&#20027;&#35201;&#23384;&#22312;&#20110;&#20219;&#21153;&#25191;&#34892;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04359v1 Announce Type: cross  Abstract: Symmetry is a fundamental aspect of many real-world robotic tasks. However, current deep reinforcement learning (DRL) approaches can seldom harness and exploit symmetry effectively. Often, the learned behaviors fail to achieve the desired transformation invariances and suffer from motion artifacts. For instance, a quadruped may exhibit different gaits when commanded to move forward or backward, even though it is symmetrical about its torso. This issue becomes further pronounced in high-dimensional or complex environments, where DRL methods are prone to local optima and fail to explore regions of the state space equally. Past methods on encouraging symmetry for robotic tasks have studied this topic mainly in a single-task setting, where symmetry usually refers to symmetry in the motion, such as the gait patterns. In this paper, we revisit this topic for goal-conditioned tasks in robotics, where symmetry lies mainly in task execution and
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#20219;&#21153;&#24179;&#34913;&#31639;&#27861;&#65288;CoTBal&#65289;&#29992;&#20110;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#22810;&#20219;&#21153;&#35270;&#35273;&#25351;&#20196;&#35843;&#25972;&#65292;&#39318;&#27425;&#25506;&#32034;&#20102;&#35270;&#35273;&#25351;&#20196;&#35843;&#25972;&#20013;&#30340;&#22810;&#20219;&#21153;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.04343</link><description>&lt;p&gt;
CoTBal: &#22810;&#20219;&#21153;&#35270;&#35273;&#25351;&#20196;&#35843;&#25972;&#30340;&#20840;&#38754;&#20219;&#21153;&#24179;&#34913;
&lt;/p&gt;
&lt;p&gt;
CoTBal: Comprehensive Task Balancing for Multi-Task Visual Instruction Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04343
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#20219;&#21153;&#24179;&#34913;&#31639;&#27861;&#65288;CoTBal&#65289;&#29992;&#20110;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#22810;&#20219;&#21153;&#35270;&#35273;&#25351;&#20196;&#35843;&#25972;&#65292;&#39318;&#27425;&#25506;&#32034;&#20102;&#35270;&#35273;&#25351;&#20196;&#35843;&#25972;&#20013;&#30340;&#22810;&#20219;&#21153;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04343v1 &#20844;&#21578;&#31867;&#22411;: &#26032;   &#25688;&#35201;: &#35270;&#35273;&#25351;&#20196;&#35843;&#25972;&#26159;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMMs&#65289;&#30340;&#20851;&#38190;&#35757;&#32451;&#38454;&#27573;&#12290;&#28982;&#32780;&#65292;&#26080;&#24046;&#21035;&#28151;&#21512;&#26469;&#33258;&#21508;&#31181;&#20219;&#21153;&#30340;&#25351;&#20196;&#36319;&#38543;&#25968;&#25454;&#30340;&#26222;&#36941;&#20570;&#27861;&#21487;&#33021;&#23548;&#33268;&#30001;&#20110;&#20219;&#21153;&#20043;&#38388;&#30340;&#25351;&#20196;&#26684;&#24335;&#21644;&#30693;&#35782;&#39046;&#22495;&#19981;&#21516;&#32780;&#23548;&#33268;&#25972;&#20307;&#24615;&#33021;&#19981;&#20339;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20840;&#38754;&#20219;&#21153;&#24179;&#34913;&#65288;CoTBal&#65289;&#31639;&#27861;&#65292;&#29992;&#20110;LMMs&#30340;&#22810;&#20219;&#21153;&#35270;&#35273;&#25351;&#20196;&#35843;&#25972;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#39033;&#25506;&#32034;&#35270;&#35273;&#25351;&#20196;&#35843;&#25972;&#20013;&#22810;&#20219;&#21153;&#20248;&#21270;&#30340;&#24037;&#20316;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#32771;&#34385;&#20219;&#21153;&#24179;&#34913;&#30340;&#20004;&#20010;&#20851;&#38190;&#32500;&#24230;:&#65288;1&#65289;&#20219;&#21153;&#38388;&#36129;&#29486;&#65292;&#21363;&#23398;&#20064;&#19968;&#20010;&#20219;&#21153;&#21487;&#33021;&#22686;&#24378;&#20854;&#20182;&#20219;&#21153;&#30340;&#24615;&#33021;&#30340;&#29616;&#35937;&#65292;&#24402;&#22240;&#20110;&#37325;&#21472;&#30340;&#30693;&#35782;&#39046;&#22495;&#65292;&#20197;&#21450;&#65288;2&#65289;&#20219;&#21153;&#20869;&#38590;&#24230;&#65292;&#25351;&#30340;&#26159;&#21333;&#20010;&#20219;&#21153;&#20869;&#30340;&#23398;&#20064;&#38590;&#24230;&#12290;&#36890;&#36807;&#29992;&#22522;&#20110;&#24615;&#33021;&#30340;&#26041;&#27861;&#37327;&#21270;&#36825;&#20004;&#20010;&#32500;&#24230;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04343v1 Announce Type: new  Abstract: Visual instruction tuning is a key training stage of large multimodal models (LMMs). Nevertheless, the common practice of indiscriminately mixing instruction-following data from various tasks may result in suboptimal overall performance due to different instruction formats and knowledge domains across tasks. To mitigate this issue, we propose a novel Comprehensive Task Balancing (CoTBal) algorithm for multi-task visual instruction tuning of LMMs. To our knowledge, this is the first work that explores multi-task optimization in visual instruction tuning. Specifically, we consider two key dimensions for task balancing: (1) Inter-Task Contribution, the phenomenon where learning one task potentially enhances the performance in other tasks, attributable to the overlapping knowledge domains, and (2) Intra-Task Difficulty, which refers to the learning difficulty within a single task. By quantifying these two dimensions with performance-based me
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#36793;&#32536;&#35745;&#31639;&#12289;&#25968;&#23383;&#23402;&#29983;&#20307;&#21644;&#28145;&#24230;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#21019;&#24314;&#21442;&#25968;&#21270;&#25968;&#23383;&#23402;&#29983;&#20307;&#29992;&#20110;&#24314;&#31569;&#27668;&#20505;&#24314;&#27169;&#65292;&#24182;&#22312;&#36793;&#32536;&#37096;&#32626;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#20248;&#21270;&#24314;&#31569;&#29289;&#36816;&#33829;&#12290;</title><link>https://arxiv.org/abs/2403.04326</link><description>&lt;p&gt;
&#22522;&#20110;&#36793;&#32536;&#30340;&#21442;&#25968;&#21270;&#25968;&#23383;&#23402;&#29983;&#20307;&#29992;&#20110;&#26234;&#33021;&#24314;&#31569;&#23460;&#20869;&#27668;&#20505;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Edge-based Parametric Digital Twins for Intelligent Building Indoor Climate Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04326
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#36793;&#32536;&#35745;&#31639;&#12289;&#25968;&#23383;&#23402;&#29983;&#20307;&#21644;&#28145;&#24230;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#21019;&#24314;&#21442;&#25968;&#21270;&#25968;&#23383;&#23402;&#29983;&#20307;&#29992;&#20110;&#24314;&#31569;&#27668;&#20505;&#24314;&#27169;&#65292;&#24182;&#22312;&#36793;&#32536;&#37096;&#32626;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#20248;&#21270;&#24314;&#31569;&#29289;&#36816;&#33829;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04326v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#39046;&#22495; &#25688;&#35201;&#65306;&#24314;&#31569;&#29615;&#22659;&#20013;&#30340;&#25968;&#23383;&#21270;&#36716;&#22411;&#20135;&#29983;&#22823;&#37327;&#25968;&#25454;&#65292;&#29992;&#20110;&#24320;&#21457;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#20197;&#20248;&#21270;&#24314;&#31569;&#36816;&#33829;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36793;&#32536;&#35745;&#31639;&#12289;&#25968;&#23383;&#23402;&#29983;&#20307;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#32508;&#21512;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#22686;&#24378;&#23545;&#24314;&#31569;&#29289;&#27668;&#20505;&#30340;&#29702;&#35299;&#12290;&#20351;&#29992;&#26412;&#20307;&#21019;&#24314;&#30340;&#21442;&#25968;&#21270;&#25968;&#23383;&#23402;&#29983;&#20307;&#30830;&#20445;&#36328;&#19981;&#21516;&#24314;&#31569;&#35774;&#22791;&#30340;&#22810;&#26679;&#26381;&#21153;&#31995;&#32479;&#20013;&#20445;&#25345;&#19968;&#33268;&#30340;&#25968;&#25454;&#34920;&#31034;&#12290;&#22522;&#20110;&#21019;&#24314;&#30340;&#25968;&#23383;&#23402;&#29983;&#20307;&#21644;&#25910;&#38598;&#30340;&#25968;&#25454;&#65292;&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#24320;&#21457;&#39044;&#27979;&#27169;&#22411;&#65292;&#29992;&#20110;&#35782;&#21035;&#23460;&#20869;&#27668;&#20505;&#20013;&#30340;&#27169;&#24335;&#24182;&#25552;&#20379;&#35265;&#35299;&#12290;&#21442;&#25968;&#21270;&#25968;&#23383;&#23402;&#29983;&#20307;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22343;&#37096;&#32626;&#22312;&#36793;&#32536;&#19978;&#65292;&#20197;&#23454;&#29616;&#20302;&#24310;&#36831;&#21644;&#38544;&#31169;&#21512;&#35268;&#24615;&#12290;&#20316;&#20026;&#31034;&#33539;&#65292;&#23545;&#29790;&#20856;&#22885;&#26031;&#27888;&#32422;&#29305;&#20848;&#22320;&#21306;&#30340;&#19968;&#24231;&#21382;&#21490;&#24314;&#31569;&#36827;&#34892;&#20102;&#26696;&#20363;&#30740;&#31350;&#65292;&#20197;&#27604;&#36739;&#20116;&#31181;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#26102;&#24207;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04326v1 Announce Type: cross  Abstract: Digital transformation in the built environment generates vast data for developing data-driven models to optimize building operations. This study presents an integrated solution utilizing edge computing, digital twins, and deep learning to enhance the understanding of climate in buildings. Parametric digital twins, created using an ontology, ensure consistent data representation across diverse service systems equipped by different buildings. Based on created digital twins and collected data, deep learning methods are employed to develop predictive models for identifying patterns in indoor climate and providing insights. Both the parametric digital twin and deep learning models are deployed on edge for low latency and privacy compliance. As a demonstration, a case study was conducted in a historic building in \"Osterg\"otland, Sweden, to compare the performance of five deep learning architectures. The results indicate that the time-seri
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;Composition Score&#65292;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#29992;&#20110;&#37327;&#21270;&#21477;&#23376;&#29702;&#35299;&#20013;&#30340;&#21547;&#20041;&#21512;&#25104;&#31243;&#24230;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#19968;&#24230;&#37327;&#19982;&#22823;&#33041;&#21306;&#22495;&#30456;&#20851;&#65292;&#25581;&#31034;&#20102;&#21547;&#20041;&#21512;&#25104;&#22312;&#20154;&#31867;&#21477;&#23376;&#29702;&#35299;&#20013;&#30340;&#22810;&#26041;&#38754;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.04325</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32452;&#21512;&#20998;&#25968;&#27979;&#37327;&#20154;&#33041;&#20013;&#30340;&#21547;&#20041;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Measuring Meaning Composition in the Human Brain with Composition Scores from Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04325
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;Composition Score&#65292;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#29992;&#20110;&#37327;&#21270;&#21477;&#23376;&#29702;&#35299;&#20013;&#30340;&#21547;&#20041;&#21512;&#25104;&#31243;&#24230;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#19968;&#24230;&#37327;&#19982;&#22823;&#33041;&#21306;&#22495;&#30456;&#20851;&#65292;&#25581;&#31034;&#20102;&#21547;&#20041;&#21512;&#25104;&#22312;&#20154;&#31867;&#21477;&#23376;&#29702;&#35299;&#20013;&#30340;&#22810;&#26041;&#38754;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21547;&#20041;&#21512;&#25104;&#30340;&#36807;&#31243;&#26159;&#25351;&#26356;&#23567;&#30340;&#21333;&#20301;&#22914;&#35821;&#32032;&#25110;&#21333;&#35789;&#32452;&#21512;&#24418;&#25104;&#30701;&#35821;&#21644;&#21477;&#23376;&#30340;&#21547;&#20041;&#65292;&#23545;&#20110;&#20154;&#31867;&#21477;&#23376;&#29702;&#35299;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#31070;&#32463;&#35821;&#35328;&#23398;&#23545;&#28041;&#21450;&#21547;&#20041;&#21512;&#25104;&#30340;&#22823;&#33041;&#21306;&#22495;&#36827;&#34892;&#20102;&#22823;&#37327;&#30740;&#31350;&#65292;&#20294;&#20173;&#32570;&#20047;&#19968;&#31181;&#35745;&#31639;&#24230;&#37327;&#26469;&#37327;&#21270;&#21512;&#25104;&#30340;&#31243;&#24230;&#12290;&#20511;&#37492;&#21464;&#21387;&#22120;&#21069;&#39304;&#32593;&#32476;&#22359;&#30340;&#38190;&#20540;&#20869;&#23384;&#35299;&#37322;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#32452;&#21512;&#20998;&#25968;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#26088;&#22312;&#37327;&#21270;&#21477;&#23376;&#29702;&#35299;&#36807;&#31243;&#20013;&#30340;&#21547;&#20041;&#21512;&#25104;&#31243;&#24230;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#19968;&#24230;&#37327;&#19982;&#22823;&#33041;&#31751;&#30456;&#20851;&#32852;&#65292;&#36825;&#20123;&#22823;&#33041;&#31751;&#19982;&#35789;&#39057;&#29575;&#12289;&#32467;&#26500;&#22788;&#29702;&#21644;&#23545;&#21333;&#35789;&#30340;&#19968;&#33324;&#25935;&#24863;&#24615;&#26377;&#20851;&#65292;&#36825;&#34920;&#26126;&#20102;&#20154;&#31867;&#21477;&#23376;&#29702;&#35299;&#36807;&#31243;&#20013;&#21547;&#20041;&#21512;&#25104;&#30340;&#22810;&#26041;&#38754;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04325v1 Announce Type: cross  Abstract: The process of meaning composition, wherein smaller units like morphemes or words combine to form the meaning of phrases and sentences, is essential for human sentence comprehension. Despite extensive neurolinguistic research into the brain regions involved in meaning composition, a computational metric to quantify the extent of composition is still lacking. Drawing on the key-value memory interpretation of transformer feed-forward network blocks, we introduce the Composition Score, a novel model-based metric designed to quantify the degree of meaning composition during sentence comprehension. Experimental findings show that this metric correlates with brain clusters associated with word frequency, structural processing, and general sensitivity to words, suggesting the multifaceted nature of meaning composition during human sentence comprehension.
&lt;/p&gt;</description></item><item><title>&#21152;&#24378;T2I&#27169;&#22411;&#30340;&#21028;&#21035;&#33021;&#21147;&#65292;&#20197;&#23454;&#29616;&#26356;&#31934;&#30830;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#23545;&#40784;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2403.04321</link><description>&lt;p&gt;
&#30952;&#20855;&#25506;&#27979;&#21644;&#35843;&#25972;&#29992;&#20110;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Discriminative Probing and Tuning for Text-to-Image Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04321
&lt;/p&gt;
&lt;p&gt;
&#21152;&#24378;T2I&#27169;&#22411;&#30340;&#21028;&#21035;&#33021;&#21147;&#65292;&#20197;&#23454;&#29616;&#26356;&#31934;&#30830;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#23545;&#40784;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#65288;T2I&#65289;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#20808;&#21069;&#30340;&#26041;&#27861;&#32463;&#24120;&#38754;&#20020;&#25991;&#26412;&#22270;&#20687;&#19981;&#23545;&#40784;&#31561;&#38382;&#39064;&#65292;&#22914;&#29983;&#25104;&#22270;&#20687;&#20013;&#30340;&#20851;&#31995;&#28151;&#28102;&#12290;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#21253;&#25324;&#20132;&#21449;&#27880;&#24847;&#21147;&#25805;&#20316;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#32452;&#21512;&#29702;&#35299;&#65292;&#25110;&#32773;&#38598;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#25913;&#36827;&#24067;&#23616;&#35268;&#21010;&#12290;&#28982;&#32780;&#65292;T2I&#27169;&#22411;&#30340;&#22266;&#26377;&#23545;&#40784;&#33021;&#21147;&#20173;&#28982;&#19981;&#36275;&#12290;&#36890;&#36807;&#23457;&#35270;&#29983;&#25104;&#27169;&#22411;&#21644;&#21028;&#21035;&#27169;&#22411;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#25105;&#20204;&#35748;&#20026;T2I&#27169;&#22411;&#30340;&#21028;&#21035;&#33021;&#21147;&#21487;&#33021;&#21453;&#26144;&#20102;&#23427;&#20204;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#25991;&#26412;&#22270;&#20687;&#23545;&#40784;&#29087;&#32451;&#24230;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#28857;&#65292;&#25105;&#20204;&#20027;&#24352;&#21152;&#24378;T2I&#27169;&#22411;&#30340;&#21028;&#21035;&#33021;&#21147;&#65292;&#20197;&#23454;&#29616;&#26356;&#31934;&#30830;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#23545;&#40784;&#29983;&#25104;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24314;&#31435;&#22312;T2I&#27169;&#22411;&#19978;&#30340;&#21028;&#21035;&#36866;&#37197;&#22120;&#65292;&#20197;&#25506;&#27979;&#23427;&#20204;&#22312;&#20004;&#39033;&#20195;&#34920;&#24615;&#20219;&#21153;&#19978;&#30340;&#21028;&#21035;&#33021;&#21147;&#65292;&#24182;&#21033;&#29992;&#21028;&#21035;&#24494;&#35843;&#26469;&#25913;&#21892;&#23427;&#20204;&#30340;&#25991;&#26412;&#22270;&#20687;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04321v1 Announce Type: cross  Abstract: Despite advancements in text-to-image generation (T2I), prior methods often face text-image misalignment problems such as relation confusion in generated images. Existing solutions involve cross-attention manipulation for better compositional understanding or integrating large language models for improved layout planning. However, the inherent alignment capabilities of T2I models are still inadequate. By reviewing the link between generative and discriminative modeling, we posit that T2I models' discriminative abilities may reflect their text-image alignment proficiency during generation. In this light, we advocate bolstering the discriminative abilities of T2I models to achieve more precise text-to-image alignment for generation. We present a discriminative adapter built on T2I models to probe their discriminative abilities on two representative tasks and leverage discriminative fine-tuning to improve their text-image alignment. As a 
&lt;/p&gt;</description></item><item><title>ALTO&#26159;&#19968;&#20010;&#32593;&#32476;&#32534;&#25490;&#22120;&#65292;&#38024;&#23545;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#21270;&#26426;&#20250;&#65292;&#23454;&#29616;&#20102;&#39640;&#21534;&#21520;&#37327;&#21644;&#20302;&#24310;&#36831;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#27969;&#24335;&#20013;&#38388;&#36755;&#20986;&#30340;&#20004;&#20010;&#26032;&#25361;&#25112;&#65306;&#27491;&#30830;&#24615;&#21644;&#36127;&#36733;&#24179;&#34913;&#12290;</title><link>https://arxiv.org/abs/2403.04311</link><description>&lt;p&gt;
ALTO&#65306;&#19968;&#31181;&#29992;&#20110;&#22797;&#21512;AI&#31995;&#32479;&#30340;&#39640;&#25928;&#32593;&#32476;&#32534;&#25490;&#22120;
&lt;/p&gt;
&lt;p&gt;
ALTO: An Efficient Network Orchestrator for Compound AI Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04311
&lt;/p&gt;
&lt;p&gt;
ALTO&#26159;&#19968;&#20010;&#32593;&#32476;&#32534;&#25490;&#22120;&#65292;&#38024;&#23545;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#21270;&#26426;&#20250;&#65292;&#23454;&#29616;&#20102;&#39640;&#21534;&#21520;&#37327;&#21644;&#20302;&#24310;&#36831;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#27969;&#24335;&#20013;&#38388;&#36755;&#20986;&#30340;&#20004;&#20010;&#26032;&#25361;&#25112;&#65306;&#27491;&#30830;&#24615;&#21644;&#36127;&#36733;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;ALTO&#65292;&#19968;&#31181;&#29992;&#20110;&#26377;&#25928;&#20026;&#35832;&#22914;&#35821;&#35328;&#27169;&#22411;&#31649;&#36947;&#20043;&#31867;&#30340;&#22797;&#21512;AI&#31995;&#32479;&#25552;&#20379;&#26381;&#21153;&#30340;&#32593;&#32476;&#32534;&#25490;&#22120;&#12290;ALTO&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#29305;&#26377;&#30340;&#20248;&#21270;&#26426;&#20250;&#65306;&#27969;&#24335;&#20013;&#38388;&#36755;&#20986;&#65292;&#23454;&#29616;&#20102;&#39640;&#21534;&#21520;&#37327;&#21644;&#20302;&#24310;&#36831;&#12290;&#30001;&#20110;&#35821;&#35328;&#27169;&#22411;&#36880;&#20010;&#29983;&#25104;token&#30340;&#36755;&#20986;&#65292;ALTO&#22312;&#21487;&#33021;&#26102;&#26292;&#38706;&#20102;&#22312;&#38454;&#27573;&#20043;&#38388;&#27969;&#24335;&#20256;&#36755;&#20013;&#38388;&#36755;&#20986;&#30340;&#26426;&#20250;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;&#22312;&#36328;&#20998;&#24067;&#24335;&#31649;&#36947;&#38454;&#27573;&#23454;&#20363;&#20043;&#38388;&#27969;&#24335;&#20256;&#36755;&#20013;&#38388;&#25968;&#25454;&#26102;&#20986;&#29616;&#30340;&#20004;&#20010;&#26032;&#25361;&#25112;&#65306;&#27491;&#30830;&#24615;&#21644;&#36127;&#36733;&#24179;&#34913;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#32858;&#21512;&#24863;&#30693;&#36335;&#30001;&#25509;&#21475;&#21644;&#20998;&#24067;&#24335;&#25552;&#31034;&#24863;&#30693;&#35843;&#24230;&#20197;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#22797;&#26434;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#39564;&#35777;&#31649;&#36947;&#19978;&#23637;&#31034;&#20102;ALTO&#37096;&#20998;&#36755;&#20986;&#27969;&#24335;&#20256;&#36755;&#30340;&#24433;&#21709;&#65292;&#23558;&#21534;&#21520;&#37327;&#25552;&#39640;&#20102;&#26368;&#22810;3&#20493;&#65292;&#21516;&#26102;&#23558;&#22266;&#23450;&#24310;&#36831;&#30446;&#26631;&#35774;&#32622;&#20026;4&#31186;/&#35831;&#27714;&#65292;&#36824;&#20943;&#23569;&#20102;&#23614;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04311v1 Announce Type: new  Abstract: We present ALTO, a network orchestrator for efficiently serving compound AI systems such as pipelines of language models. ALTO achieves high throughput and low latency by taking advantage of an optimization opportunity specific to generative language models: streaming intermediate outputs. As language models produce outputs token by token, ALTO exposes opportunities to stream intermediate outputs between stages when possible. We highlight two new challenges of correctness and load balancing which emerge when streaming intermediate data across distributed pipeline stage instances. We also motivate the need for an aggregation-aware routing interface and distributed prompt-aware scheduling to address these challenges. We demonstrate the impact of ALTO's partial output streaming on a complex chatbot verification pipeline, increasing throughput by up to 3x for a fixed latency target of 4 seconds / request while also reducing tail latency by 1
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AO-DETR&#30340;&#21453;&#37325;&#21472;DETR&#27169;&#22411;&#65292;&#20351;&#29992;Category-Specific One-to-One Assignment&#31574;&#30053;&#22686;&#24378;&#30446;&#26631;&#29289;&#20307;&#25552;&#21462;&#29305;&#24449;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;Look Forward Densely&#26041;&#26696;&#25913;&#21892;&#21442;&#32771;&#26694;&#30340;&#23450;&#20301;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.04309</link><description>&lt;p&gt;
AO-DETR&#65306;&#21453;&#37325;&#21472;DETR&#29992;&#20110;X&#23556;&#32447;&#31105;&#27490;&#29289;&#21697;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
AO-DETR: Anti-Overlapping DETR for X-Ray Prohibited Items Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04309
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AO-DETR&#30340;&#21453;&#37325;&#21472;DETR&#27169;&#22411;&#65292;&#20351;&#29992;Category-Specific One-to-One Assignment&#31574;&#30053;&#22686;&#24378;&#30446;&#26631;&#29289;&#20307;&#25552;&#21462;&#29305;&#24449;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;Look Forward Densely&#26041;&#26696;&#25913;&#21892;&#21442;&#32771;&#26694;&#30340;&#23450;&#20301;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
X&#23556;&#32447;&#22270;&#20687;&#20013;&#30340;&#31105;&#27490;&#29289;&#21697;&#26816;&#27979;&#26159;&#21508;&#31181;&#23433;&#20840;&#26816;&#26597;&#22330;&#26223;&#20013;&#24191;&#27867;&#37319;&#29992;&#30340;&#26368;&#37325;&#35201;&#21644;&#39640;&#25928;&#30340;&#26041;&#27861;&#20043;&#19968;&#12290;&#32771;&#34385;&#21040;X&#23556;&#32447;&#31105;&#27490;&#29289;&#21697;&#22270;&#20687;&#20013;&#26174;&#33879;&#30340;&#37325;&#21472;&#29616;&#35937;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#20808;&#36827;&#30340;&#36890;&#29992;&#29289;&#20307;&#26816;&#27979;&#22120;DINO&#30340;&#21453;&#37325;&#21472;DETR&#65288;AO-DETR&#65289;&#12290;&#20855;&#20307;&#22320;&#65292;&#20026;&#20102;&#35299;&#20915;&#30001;&#20110;&#37325;&#21472;&#29616;&#35937;&#23548;&#33268;&#30340;&#29305;&#24449;&#32806;&#21512;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#29305;&#23450;&#31867;&#21035;&#30340;&#19968;&#23545;&#19968;&#20998;&#37197;&#65288;CSA&#65289;&#31574;&#30053;&#65292;&#20197;&#38480;&#21046;&#31867;&#21035;&#29305;&#23450;&#30340;&#29289;&#20307;&#26597;&#35810;&#26469;&#39044;&#27979;&#22266;&#23450;&#31867;&#21035;&#30340;&#31105;&#27490;&#29289;&#21697;&#65292;&#21487;&#20197;&#22686;&#24378;&#23427;&#20204;&#25552;&#21462;&#29305;&#23450;&#31867;&#21035;&#31105;&#27490;&#29289;&#21697;&#29305;&#24449;&#33021;&#21147;&#12290;&#20174;&#37325;&#21472;&#30340;&#21069;&#26223;-&#32972;&#26223;&#29305;&#24449;&#20013;&#33719;&#21462;&#12290;&#20026;&#20102;&#35299;&#20915;&#37325;&#21472;&#29616;&#35937;&#23548;&#33268;&#30340;&#36793;&#32536;&#27169;&#31946;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Look Forward Densely&#65288;LFD&#65289;&#26041;&#26696;&#65292;&#21487;&#20197;&#25552;&#39640;&#21442;&#32771;&#26694;&#30340;&#23450;&#20301;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04309v1 Announce Type: cross  Abstract: Prohibited item detection in X-ray images is one of the most essential and highly effective methods widely employed in various security inspection scenarios. Considering the significant overlapping phenomenon in X-ray prohibited item images, we propose an Anti-Overlapping DETR (AO-DETR) based on one of the state-of-the-art general object detectors, DINO. Specifically, to address the feature coupling issue caused by overlapping phenomena, we introduce the Category-Specific One-to-One Assignment (CSA) strategy to constrain category-specific object queries in predicting prohibited items of fixed categories, which can enhance their ability to extract features specific to prohibited items of a particular category from the overlapping foreground-background features. To address the edge blurring problem caused by overlapping phenomena, we propose the Look Forward Densely (LFD) scheme, which improves the localization accuracy of reference boxe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#26368;&#36817;&#20986;&#29616;&#30340;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#22312;&#19987;&#19994;&#21644;&#36890;&#29992;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#26088;&#22312;&#20840;&#38754;&#20102;&#35299;&#36825;&#20123;&#21019;&#26032;&#26041;&#27861;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.04306</link><description>&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Effectiveness Assessment of Recent Large Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#26368;&#36817;&#20986;&#29616;&#30340;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#22312;&#19987;&#19994;&#21644;&#36890;&#29992;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#26088;&#22312;&#20840;&#38754;&#20102;&#35299;&#36825;&#20123;&#21019;&#26032;&#26041;&#27861;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;(LVLMs)&#30340;&#20986;&#29616;&#20195;&#34920;&#30528;&#36808;&#21521;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#37325;&#35201;&#36827;&#27493;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#19987;&#19994;&#21644;&#36890;&#29992;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#31243;&#24230;&#38656;&#35201;&#36827;&#19968;&#27493;&#35843;&#26597;&#12290;&#26412;&#25991;&#26088;&#22312;&#35780;&#20272;&#27969;&#34892;&#30340;LVLMs&#22312;&#19987;&#19994;&#21644;&#36890;&#29992;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#65292;&#26088;&#22312;&#25552;&#20379;&#23545;&#36825;&#20123;&#21019;&#26032;&#26041;&#27861;&#30340;&#20840;&#38754;&#29702;&#35299;&#12290;&#20026;&#20102;&#35780;&#20272;&#23427;&#20204;&#22312;&#19987;&#19994;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#37327;&#36523;&#23450;&#21046;&#20102;&#19968;&#20010;&#21253;&#21547;&#33258;&#28982;&#12289;&#21307;&#30103;&#21644;&#24037;&#19994;&#19977;&#31181;&#19981;&#21516;&#22330;&#26223;&#30340;&#20840;&#38754;&#27979;&#35797;&#24179;&#21488;&#65292;&#28085;&#30422;&#20845;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#36825;&#20123;&#20219;&#21153;&#21253;&#25324;&#26174;&#33879;&#12289;&#20266;&#35013;&#21644;&#36879;&#26126;&#29289;&#20307;&#26816;&#27979;&#65292;&#20197;&#21450;&#24687;&#32905;&#21644;&#30382;&#32932;&#30149;&#21464;&#26816;&#27979;&#65292;&#20197;&#21450;&#24037;&#19994;&#24322;&#24120;&#26816;&#27979;&#12290;&#25105;&#20204;&#26816;&#39564;&#20102;&#26368;&#36817;&#19977;&#31181;&#24320;&#28304;LVLMs--MiniGPT-v2&#12289;LLaVA-1.5&#21644;Shikra--&#22312;&#35270;&#35273;&#35782;&#21035;&#21644;&#23450;&#20301;&#39046;&#22495;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04306v1 Announce Type: cross  Abstract: The advent of large vision-language models (LVLMs) represents a noteworthy advancement towards the pursuit of artificial general intelligence. However, the extent of their efficacy across both specialized and general tasks warrants further investigation. This article endeavors to evaluate the competency of popular LVLMs in specialized and general tasks, respectively, aiming to offer a comprehensive comprehension of these innovative methodologies. To gauge their efficacy in specialized tasks, we tailor a comprehensive testbed comprising three distinct scenarios: natural, healthcare, and industrial, encompassing six challenging tasks. These tasks include salient, camouflaged, and transparent object detection, as well as polyp and skin lesion detection, alongside industrial anomaly detection. We examine the performance of three recent open-source LVLMs -- MiniGPT-v2, LLaVA-1.5, and Shikra -- in the realm of visual recognition and localiza
&lt;/p&gt;</description></item><item><title>LitSim&#25552;&#20986;&#20102;&#19968;&#31181;&#38271;&#26399;&#20132;&#20114;&#24335;&#20223;&#30495;&#26041;&#27861;&#65292;&#37325;&#25773;&#26085;&#24535;&#20197;&#33719;&#24471;&#36924;&#30495;&#22330;&#26223;&#65292;&#24403;&#20986;&#29616;&#19981;&#30495;&#23454;&#20914;&#31361;&#26102;&#36827;&#34892;&#24178;&#39044;&#65292;&#20174;&#32780;&#25552;&#39640;&#20223;&#30495;&#36924;&#30495;&#24230;&#24182;&#36991;&#20813;&#30896;&#25758;</title><link>https://arxiv.org/abs/2403.04299</link><description>&lt;p&gt;
LitSim&#65306;&#38271;&#26399;&#20132;&#20114;&#24335;&#20132;&#36890;&#20223;&#30495;&#30340;&#20914;&#31361;&#24863;&#30693;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
LitSim: Conflict-aware Policy for Long-term Interactive Traffic Simulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04299
&lt;/p&gt;
&lt;p&gt;
LitSim&#25552;&#20986;&#20102;&#19968;&#31181;&#38271;&#26399;&#20132;&#20114;&#24335;&#20223;&#30495;&#26041;&#27861;&#65292;&#37325;&#25773;&#26085;&#24535;&#20197;&#33719;&#24471;&#36924;&#30495;&#22330;&#26223;&#65292;&#24403;&#20986;&#29616;&#19981;&#30495;&#23454;&#20914;&#31361;&#26102;&#36827;&#34892;&#24178;&#39044;&#65292;&#20174;&#32780;&#25552;&#39640;&#20223;&#30495;&#36924;&#30495;&#24230;&#24182;&#36991;&#20813;&#30896;&#25758;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#25311;&#23545;&#20110;&#35780;&#20272;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#30340;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#19982;&#22312;&#36947;&#36335;&#27979;&#35797;&#30456;&#27604;&#65292;&#27169;&#25311;&#20855;&#26377;&#25928;&#29575;&#21644;&#25104;&#26412;&#20248;&#21183;&#12290;&#38656;&#35201;&#30495;&#23454;&#30340;&#22810;&#26234;&#33021;&#20307;&#34892;&#20026;&#65288;&#20363;&#22914;&#20132;&#20114;&#24335;&#21644;&#38271;&#26399;&#34892;&#20026;&#65289;&#26469;&#32553;&#23567;&#27169;&#25311;&#19982;&#29616;&#23454;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#29616;&#26377;&#24037;&#20316;&#22312;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#26041;&#38754;&#23384;&#22312;&#20197;&#19979;&#32570;&#28857;&#65306;&#65288;1&#65289;&#26085;&#24535;&#37325;&#25773;&#25552;&#20379;&#20102;&#36924;&#30495;&#30340;&#22330;&#26223;&#65292;&#20294;&#30001;&#20110;&#32570;&#20047;&#21160;&#24577;&#20132;&#20114;&#32780;&#23548;&#33268;&#20102;&#19981;&#30495;&#23454;&#30340;&#30896;&#25758;&#65292;&#20197;&#21450;&#65288;2&#65289;&#22522;&#20110;&#27169;&#22411;&#21644;&#22522;&#20110;&#23398;&#20064;&#30340;&#35299;&#20915;&#26041;&#26696;&#40723;&#21169;&#20132;&#20114;&#65292;&#20294;&#22312;&#38271;&#26102;&#38388;&#33539;&#22260;&#20869;&#24448;&#24448;&#20559;&#31163;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LitSim&#65292;&#19968;&#31181;&#38271;&#26399;&#20132;&#20114;&#24335;&#20223;&#30495;&#26041;&#27861;&#65292;&#26088;&#22312;&#26368;&#22823;&#31243;&#24230;&#22320;&#22686;&#21152;&#36924;&#30495;&#24615;&#65292;&#21516;&#26102;&#36991;&#20813;&#19981;&#30495;&#23454;&#30340;&#30896;&#25758;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20026;&#22823;&#22810;&#25968;&#24773;&#20917;&#37325;&#25773;&#26085;&#24535;&#65292;&#21482;&#26377;&#22312;LitSim&#39044;&#27979;&#21040;&#19981;&#30495;&#23454;&#20914;&#31361;&#26102;&#25165;&#36827;&#34892;&#24178;&#39044;&#12290;&#28982;&#21518;&#25105;&#20204;&#40723;&#21169;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#20132;&#20114;&#24182;&#35299;&#20915;&#20914;&#31361;&#65292;&#20174;&#32780;&#20943;&#23569;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04299v1 Announce Type: cross  Abstract: Simulation is pivotal in evaluating the performance of autonomous driving systems due to the advantages in efficiency and cost compared to on-road testing. Realistic multi-agent behavior~(e.g., interactive and long-term) is needed to narrow the gap between the simulation and the reality. The existing work has the following shortcomings in achieving this goal:~(1) log replay offers realistic scenarios but leads to unrealistic collisions due to lacking dynamic interactions, and~(2) model-based and learning-based solutions encourage interactions but often deviate from real-world data in long horizons. In this work, we propose LitSim, a long-term interactive simulation approach that maximizes realism while avoiding unrealistic collisions. Specifically, we replay the log for most scenarios and intervene only when LitSim predicts unrealistic conflicts. We then encourage interactions among the agents and resolve the conflicts, thereby reducin
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#30693;&#35782;&#34701;&#21512;&#24322;&#24120;&#26816;&#27979;&#31995;&#32479;MKF-ADS&#65292;&#37319;&#29992;STcAM&#21644;PatchST&#27169;&#22359;&#65292;&#26088;&#22312;&#25552;&#39640;IDS&#22312;CAN&#24635;&#32447;&#28431;&#27934;&#20013;&#30340;&#23433;&#20840;&#24615;&#21644;&#38477;&#20302;&#35823;&#25253;&#35686;&#12290;</title><link>https://arxiv.org/abs/2403.04293</link><description>&lt;p&gt;
MKF-ADS&#65306;&#29992;&#20110;&#27773;&#36710;&#30340;&#22810;&#30693;&#35782;&#34701;&#21512;&#24322;&#24120;&#26816;&#27979;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
MKF-ADS: A Multi-Knowledge Fused Anomaly Detection System for Automotive
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04293
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#30693;&#35782;&#34701;&#21512;&#24322;&#24120;&#26816;&#27979;&#31995;&#32479;MKF-ADS&#65292;&#37319;&#29992;STcAM&#21644;PatchST&#27169;&#22359;&#65292;&#26088;&#22312;&#25552;&#39640;IDS&#22312;CAN&#24635;&#32447;&#28431;&#27934;&#20013;&#30340;&#23433;&#20840;&#24615;&#21644;&#38477;&#20302;&#35823;&#25253;&#35686;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#65288;ITSs&#65289;&#23545;&#36710;&#36742;&#30005;&#23376;&#25511;&#21046;&#21333;&#20803;&#65288;ECUs&#65289;&#19982;&#22806;&#37096;&#19990;&#30028;&#24191;&#27867;&#36830;&#25509;&#30340;&#38656;&#27714;&#65292;&#23433;&#20840;&#24615;&#21644;&#23433;&#20840;&#24615;&#24050;&#25104;&#20026;&#20005;&#23803;&#38382;&#39064;&#12290; &#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65288;IDSs&#65289;&#22312;&#35299;&#20915;&#25511;&#21046;&#21306;&#22495;&#32593;&#32476;&#65288;CAN&#65289;&#24635;&#32447;&#28431;&#27934;&#26041;&#38754;&#26159;&#19968;&#20010;&#20851;&#38190;&#30340;&#23433;&#20840;&#32452;&#20214;&#12290; &#20294;&#26159;&#65292;&#22522;&#20110;&#30417;&#30563;&#30340;IDS&#26080;&#27861;&#35782;&#21035;&#22797;&#26434;&#25915;&#20987;&#65292;&#22522;&#20110;&#24322;&#24120;&#30340;IDS&#30001;&#20110;&#33021;&#21147;&#29942;&#39048;&#32780;&#20135;&#29983;&#26356;&#39640;&#30340;&#35823;&#25253;&#35686;&#12290; &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#30693;&#35782;&#34701;&#21512;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#65292;&#31216;&#20026;MKF-ADS&#12290; &#20855;&#20307;&#22320;&#65292;&#35813;&#26041;&#27861;&#35774;&#35745;&#20102;&#19968;&#20010;&#38598;&#25104;&#26694;&#26550;&#65292;&#21253;&#25324;&#24102;&#26377;&#27880;&#24847;&#26426;&#21046;&#30340;&#26102;&#31354;&#30456;&#20851;&#24615;&#65288;STcAM&#65289;&#27169;&#22359;&#21644;&#34917;&#19969;&#31232;&#30095;&#21464;&#25442;&#22120;&#27169;&#22359;&#65288;PatchST&#65289;&#12290; STcAM&#36890;&#36807;&#31934;&#32454;&#20462;&#21098;&#20351;&#29992;&#19968;&#32500;&#21367;&#31215;&#65288;Conv1D&#65289;&#26469;&#25552;&#21462;&#31354;&#38388;&#29305;&#24449;&#65292;&#38543;&#21518;&#21033;&#29992;&#21452;&#21521;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;Bi-LSTM&#65289;&#26469;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04293v1 Announce Type: new  Abstract: With the requirements of Intelligent Transport Systems (ITSs) for extensive connectivity of Electronic Control Units (ECUs) to the outside world, safety and security have become stringent problems. Intrusion detection systems (IDSs) are a crucial safety component in remediating Controller Area Network (CAN) bus vulnerabilities. However, supervised-based IDSs fail to identify complexity attacks and anomaly-based IDSs have higher false alarms owing to capability bottleneck. In this paper, we propose a novel multi-knowledge fused anomaly detection model, called MKF-IDS. Specifically, the method designs an integration framework, including spatial-temporal correlation with an attention mechanism (STcAM) module and patch sparse-transformer module (PatchST). The STcAM with fine-pruning uses one-dimensional convolution (Conv1D) to extract spatial features and subsequently utilizes the Bidirectional Long Short Term Memory (Bi-LSTM) to extract the
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#28508;&#21147;&#30340;&#20840;&#38754;&#35748;&#30693;&#31639;&#27861;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#19988;&#22810;&#25165;&#22810;&#33402;&#30340;&#31639;&#27861;&#25903;&#25745;&#65292;&#24182;&#37325;&#26032;&#34701;&#21512;&#20102;&#25511;&#21046;&#35770;&#21644;&#27169;&#25311;&#25511;&#21046;&#36807;&#31243;&#30340;&#26041;&#38754;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#30340;&#19981;&#36275;</title><link>https://arxiv.org/abs/2403.04292</link><description>&lt;p&gt;
A(G)I&#20013;&#30340;&#25361;&#25112;&#65292;&#22885;&#27931;&#21338;&#32599;&#26031;&#27169;&#22411;&#20013;&#22797;&#33487;&#30340;&#25511;&#21046;&#35770;&#20316;&#20026;&#32479;&#19968;&#24605;&#32500;&#31639;&#27861;&#20043;&#19968;
&lt;/p&gt;
&lt;p&gt;
A challenge in A(G)I, cybernetics revived in the Ouroboros Model as one algorithm for all thinking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04292
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#28508;&#21147;&#30340;&#20840;&#38754;&#35748;&#30693;&#31639;&#27861;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#19988;&#22810;&#25165;&#22810;&#33402;&#30340;&#31639;&#27861;&#25903;&#25745;&#65292;&#24182;&#37325;&#26032;&#34701;&#21512;&#20102;&#25511;&#21046;&#35770;&#21644;&#27169;&#25311;&#25511;&#21046;&#36807;&#31243;&#30340;&#26041;&#38754;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#30340;&#19981;&#36275;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#39033;&#20851;&#20110;&#31639;&#27861;&#25361;&#25112;&#21644;&#29305;&#21035;&#26159;&#33258;&#21160;&#22270;&#20687;&#20998;&#31867;&#21644;&#29983;&#25104;&#30340;&#25361;&#25112;&#20197;&#20026;AI&#29702;&#35299;&#30340;&#24418;&#24335;&#21576;&#29616;&#65292;&#25361;&#25112;AI&#20174;&#20070;&#38754;&#25551;&#36848;&#20013;&#20135;&#29983;&#31867;&#20284;&#20316;&#21697;&#12290;&#26412;&#25991;&#26088;&#22312;&#31361;&#26174;&#24403;&#21069;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#30340;&#20248;&#21183;&#21644;&#19981;&#36275;&#65292;&#31895;&#30053;&#22320;&#25551;&#32472;&#21069;&#36827;&#30340;&#36947;&#36335;&#12290;&#25552;&#20986;&#19968;&#31181;&#24191;&#20041;&#31526;&#21495;&#23884;&#20837;&#21644;&#65288;&#19981;&#20165;&#20165;&#26159;&#65289;&#22522;&#20110;&#26576;&#31181;&#36523;&#20307;&#22522;&#30784;&#30340;&#32570;&#20047;&#36131;&#20219;&#24403;&#21069;&#19981;&#36275;&#12290;&#20854;&#38543;&#20043;&#32780;&#26469;&#30340;&#27010;&#24565;&#30340;&#20998;&#23618;&#32452;&#32455;&#30340;&#21294;&#20047;&#12290;&#20316;&#20026;&#36825;&#20123;&#32570;&#28857;&#30340;&#34917;&#25937;&#26041;&#26696;&#65292;&#24314;&#35758;&#36864;&#19968;&#27493;&#65292;&#24182;&#37325;&#26032;&#34701;&#21512;&#25511;&#21046;&#35770;&#21644;&#27169;&#25311;&#25511;&#21046;&#36807;&#31243;&#30340;&#26041;&#38754;&#12290;&#22768;&#31216;&#22885;&#27931;&#21338;&#32599;&#26031;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#20840;&#23616;&#35270;&#35282;&#65292; &#20855;&#26377;&#32479;&#19968;&#30340;&#35748;&#30693;&#31639;&#27861;&#25903;&#26609;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04292v1 Announce Type: new  Abstract: A topical challenge for algorithms in general and for automatic image categorization and generation in particular is presented in the form of a drawing for AI to understand. In a second vein, AI is challenged to produce something similar from verbal description. The aim of the paper is to highlight strengths and deficiencies of current Artificial Intelligence approaches while coarsely sketching a way forward. A general lack of encompassing symbol-embedding and (not only) -grounding in some bodily basis is made responsible for current deficiencies. A concomitant dearth of hierarchical organization of concepts follows suite. As a remedy for these shortcomings, it is proposed to take a wide step back and to newly incorporate aspects of cybernetics and analog control processes. It is claimed that a promising overarching perspective is provided by the Ouroboros Model with a valid and versatile algorithmic backbone for general cognition at all
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;Proxy-RLHF&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#21644;&#23545;&#40784;&#36807;&#31243;&#35299;&#32806;&#65292;&#23454;&#29616;&#20102;&#20197;&#26356;&#20302;&#35745;&#31639;&#25104;&#26412;&#23545;&#40784;&#20154;&#31867;&#20215;&#20540;&#35266;&#65292;&#20165;&#20351;&#29992;&#20854;&#20182;&#26041;&#27861;&#30340;1%&#35757;&#32451;&#21442;&#25968;&#21363;&#21487;&#36798;&#21040;&#21487;&#27604;&#27700;&#24179;&#30340;&#23545;&#40784;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.04283</link><description>&lt;p&gt;
Proxy-RLHF&#65306;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36890;&#36807;&#20195;&#29702;&#35299;&#32806;&#29983;&#25104;&#21644;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Proxy-RLHF: Decoupling Generation and Alignment in Large Language Model with Proxy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04283
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;Proxy-RLHF&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#21644;&#23545;&#40784;&#36807;&#31243;&#35299;&#32806;&#65292;&#23454;&#29616;&#20102;&#20197;&#26356;&#20302;&#35745;&#31639;&#25104;&#26412;&#23545;&#40784;&#20154;&#31867;&#20215;&#20540;&#35266;&#65292;&#20165;&#20351;&#29992;&#20854;&#20182;&#26041;&#27861;&#30340;1%&#35757;&#32451;&#21442;&#25968;&#21363;&#21487;&#36798;&#21040;&#21487;&#27604;&#27700;&#24179;&#30340;&#23545;&#40784;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#65288;RLHF&#65289;&#26159;&#30830;&#20445;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#20445;&#25345;&#19968;&#33268;&#30340;&#20027;&#27969;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;RLHF&#26041;&#27861;&#38656;&#35201;&#39640;&#26114;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#20027;&#35201;&#21407;&#22240;&#20043;&#19968;&#26159;RLHF&#21516;&#26102;&#23558;&#29983;&#25104;&#21644;&#23545;&#40784;&#20219;&#21153;&#20998;&#37197;&#32473;LLM&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Proxy-RLHF&#65292;&#23427;&#35299;&#32806;&#20102;LLMs&#30340;&#29983;&#25104;&#21644;&#23545;&#40784;&#27969;&#31243;&#65292;&#20197;&#26356;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#23454;&#29616;&#19982;&#20154;&#31867;&#20215;&#20540;&#30340;&#23545;&#40784;&#12290;&#25105;&#20204;&#20174;&#20026;&#23545;&#40784;&#36807;&#31243;&#35774;&#35745;&#30340;&#26032;&#22411;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#24320;&#22987;&#65292;&#24182;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#35757;&#32451;&#20102;&#19968;&#20010;&#31616;&#21270;&#30340;&#20195;&#29702;&#27169;&#22411;&#65292;&#30417;&#30563;LLM&#30340;&#26631;&#35760;&#29983;&#25104;&#65292;&#32780;&#19981;&#25913;&#21464;LLM&#26412;&#36523;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20165;&#20351;&#29992;&#20854;&#20182;&#26041;&#27861;&#30340;1%&#35757;&#32451;&#21442;&#25968;&#21363;&#21487;&#23454;&#29616;&#21487;&#27604;&#27700;&#24179;&#30340;&#23545;&#40784;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04283v1 Announce Type: cross  Abstract: Reinforcement Learning from Human Feedback (RLHF) is the prevailing approach to ensure Large Language Models (LLMs) align with human values. However, existing RLHF methods require a high computational cost, one main reason being that RLHF assigns both the generation and alignment tasks to the LLM simultaneously. In this paper, we introduce Proxy-RLHF, which decouples the generation and alignment processes of LLMs, achieving alignment with human values at a much lower computational cost. We start with a novel Markov Decision Process (MDP) designed for the alignment process and employ Reinforcement Learning (RL) to train a streamlined proxy model that oversees the token generation of the LLM, without altering the LLM itself. Experiments show that our method achieves a comparable level of alignment with only 1\% of the training parameters of other methods.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#38024;&#23545;&#38463;&#25289;&#20271;&#30005;&#35805;&#23545;&#35805;&#39046;&#22495;&#25361;&#25112;&#30340;&#20840;&#38754;&#35780;&#20272;&#22522;&#20934;&#65292;&#26088;&#22312;&#20026;&#24320;&#21457;&#21644;&#35780;&#20272;&#33021;&#22815;&#36866;&#24212;&#30495;&#23454;&#30005;&#35805;&#36890;&#35759;&#26465;&#20214;&#30340;ASR&#31995;&#32479;&#25552;&#20379;&#19968;&#20010;&#20005;&#26684;&#30340;&#27979;&#35797;&#24179;&#21488;&#12290;</title><link>https://arxiv.org/abs/2403.04280</link><description>&lt;p&gt;
&#29992;&#20110;&#35780;&#20272;&#38463;&#25289;&#20271;&#21628;&#21483;&#39046;&#22495;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30340;&#26032;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
A New Benchmark for Evaluating Automatic Speech Recognition in the Arabic Call Domain
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04280
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#38024;&#23545;&#38463;&#25289;&#20271;&#30005;&#35805;&#23545;&#35805;&#39046;&#22495;&#25361;&#25112;&#30340;&#20840;&#38754;&#35780;&#20272;&#22522;&#20934;&#65292;&#26088;&#22312;&#20026;&#24320;&#21457;&#21644;&#35780;&#20272;&#33021;&#22815;&#36866;&#24212;&#30495;&#23454;&#30005;&#35805;&#36890;&#35759;&#26465;&#20214;&#30340;ASR&#31995;&#32479;&#25552;&#20379;&#19968;&#20010;&#20005;&#26684;&#30340;&#27979;&#35797;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#26088;&#22312;&#24341;&#20837;&#19968;&#20010;&#20840;&#38754;&#30340;&#29992;&#20110;&#38463;&#25289;&#20271;&#35821;&#38899;&#35782;&#21035;&#35780;&#20272;&#30340;&#22522;&#20934;&#65292;&#19987;&#38376;&#38024;&#23545;&#38463;&#25289;&#20271;&#35821;&#30005;&#35805;&#23545;&#35805;&#20013;&#30340;&#25361;&#25112;&#36827;&#34892;&#20102;&#23450;&#21046;&#12290;&#38463;&#25289;&#20271;&#35821;&#20197;&#20854;&#20016;&#23500;&#30340;&#26041;&#35328;&#22810;&#26679;&#24615;&#21644;&#35821;&#38899;&#22797;&#26434;&#24615;&#32780;&#38395;&#21517;&#65292;&#23545;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#25552;&#20986;&#20102;&#35768;&#22810;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#36825;&#20123;&#25361;&#25112;&#22312;&#30005;&#35805;&#36890;&#35805;&#39046;&#22495;&#36827;&#19968;&#27493;&#25918;&#22823;&#65292;&#37027;&#37324;&#30340;&#38899;&#39057;&#36136;&#37327;&#12289;&#32972;&#26223;&#22122;&#38899;&#21644;&#20250;&#35805;&#24335;&#35821;&#38899;&#39118;&#26684;&#20250;&#23545;&#35782;&#21035;&#20934;&#30830;&#24615;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26088;&#22312;&#24314;&#31435;&#19968;&#20010;&#31283;&#20581;&#30340;&#22522;&#20934;&#65292;&#19981;&#20165;&#21253;&#21547;&#24191;&#27867;&#30340;&#38463;&#25289;&#20271;&#26041;&#35328;&#33539;&#22260;&#65292;&#36824;&#27169;&#25311;&#21628;&#21483;&#36890;&#35759;&#30340;&#30495;&#23454;&#26465;&#20214;&#12290;&#36890;&#36807;&#32467;&#21512;&#22810;&#26679;&#30340;&#26041;&#35328;&#34920;&#36798;&#65292;&#24182;&#32771;&#34385;&#21628;&#21483;&#24405;&#38899;&#30340;&#21487;&#21464;&#36136;&#37327;&#65292;&#36825;&#20010;&#22522;&#20934;&#26088;&#22312;&#20026;&#24320;&#21457;&#21644;&#35780;&#20272;&#33021;&#22815;&#24212;&#23545;&#23454;&#38469;&#25361;&#25112;&#30340;ASR&#31995;&#32479;&#25552;&#20379;&#20005;&#26684;&#30340;&#27979;&#35797;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04280v1 Announce Type: new  Abstract: This work is an attempt to introduce a comprehensive benchmark for Arabic speech recognition, specifically tailored to address the challenges of telephone conversations in Arabic language. Arabic, characterized by its rich dialectal diversity and phonetic complexity, presents a number of unique challenges for automatic speech recognition (ASR) systems. These challenges are further amplified in the domain of telephone calls, where audio quality, background noise, and conversational speech styles negatively affect recognition accuracy. Our work aims to establish a robust benchmark that not only encompasses the broad spectrum of Arabic dialects but also emulates the real-world conditions of call-based communications. By incorporating diverse dialectical expressions and accounting for the variable quality of call recordings, this benchmark seeks to provide a rigorous testing ground for the development and evaluation of ASR systems capable of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#31454;&#20105;&#24066;&#22330;&#32972;&#26223;&#19979;&#30340;&#35774;&#26045;&#36873;&#22336;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#36335;&#24452;&#32422;&#26463;&#65292;&#25506;&#35752;&#20102;&#19977;&#31181;&#26377;&#25928;&#21106;&#20197;&#22788;&#29702;&#38750;&#32447;&#24615;&#30446;&#26631;&#20989;&#25968;&#12290;</title><link>https://arxiv.org/abs/2403.04264</link><description>&lt;p&gt;
&#22312;&#38543;&#26426;&#25928;&#29992;&#21644;&#36335;&#24452;&#32422;&#26463;&#19979;&#30340;&#31454;&#20105;&#35774;&#26045;&#36873;&#22336;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Competitive Facility Location under Random Utilities and Routing Constraints
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04264
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#31454;&#20105;&#24066;&#22330;&#32972;&#26223;&#19979;&#30340;&#35774;&#26045;&#36873;&#22336;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#36335;&#24452;&#32422;&#26463;&#65292;&#25506;&#35752;&#20102;&#19977;&#31181;&#26377;&#25928;&#21106;&#20197;&#22788;&#29702;&#38750;&#32447;&#24615;&#30446;&#26631;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#31454;&#20105;&#24066;&#22330;&#32972;&#26223;&#19979;&#30340;&#35774;&#26045;&#36873;&#22336;&#38382;&#39064;&#65292;&#20854;&#20013;&#39038;&#23458;&#38656;&#27714;&#30001;&#38543;&#26426;&#25928;&#29992;&#36873;&#25321;&#27169;&#22411;&#39044;&#27979;&#12290;&#19982;&#20197;&#24448;&#20027;&#35201;&#20851;&#27880;&#31616;&#21333;&#32422;&#26463;&#65288;&#20363;&#22914;&#25152;&#36873;&#20301;&#32622;&#25968;&#37327;&#30340;&#22522;&#25968;&#32422;&#26463;&#65289;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36335;&#24452;&#32422;&#26463;&#65292;&#36825;&#20123;&#32422;&#26463;&#38656;&#35201;&#20197;&#19968;&#31181;&#26041;&#24335;&#36873;&#25321;&#20301;&#32622;&#65292;&#20197;&#20445;&#35777;&#23384;&#22312;&#35775;&#38382;&#25152;&#26377;&#36873;&#25321;&#20301;&#32622;&#30340;&#26053;&#31243;&#65292;&#21516;&#26102;&#36981;&#23432;&#25351;&#23450;&#30340;&#26053;&#31243;&#38271;&#24230;&#19978;&#38480;&#12290;&#36825;&#31181;&#36335;&#24452;&#32422;&#26463;&#22312;&#21508;&#31181;&#29616;&#23454;&#22330;&#26223;&#20013;&#20855;&#26377;&#20851;&#38190;&#24212;&#29992;&#12290;&#25152;&#28041;&#38382;&#39064;&#20855;&#26377;&#38750;&#32447;&#24615;&#30446;&#26631;&#20989;&#25968;&#65292;&#36825;&#26159;&#30001;&#20110;&#37319;&#29992;&#20102;&#38543;&#26426;&#25928;&#29992;&#65292;&#24182;&#19988;&#20855;&#26377;&#22797;&#26434;&#30340;&#36335;&#24452;&#32422;&#26463;&#65292;&#20351;&#20854;&#22312;&#35745;&#31639;&#19978;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19977;&#31181;&#26377;&#25928;&#21106;&#65292;&#21363;&#22806;&#20272;&#35745;&#21644;&#23376;&#27169;&#21106;&#65292;&#20197;&#22788;&#29702;&#38750;&#32447;&#24615;&#30446;&#26631;&#20989;&#25968;&#65292;&#20197;&#21450;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04264v1 Announce Type: new  Abstract: In this paper, we study a facility location problem within a competitive market context, where customer demand is predicted by a random utility choice model. Unlike prior research, which primarily focuses on simple constraints such as a cardinality constraint on the number of selected locations, we introduce routing constraints that necessitate the selection of locations in a manner that guarantees the existence of a tour visiting all chosen locations while adhering to a specified tour length upper bound. Such routing constraints find crucial applications in various real-world scenarios. The problem at hand features a non-linear objective function, resulting from the utilization of random utilities, together with complex routing constraints, making it computationally challenging. To tackle this problem, we explore three types of valid cuts, namely, outer-approximation and submodular cuts to handle the nonlinear objective function, as wel
&lt;/p&gt;</description></item><item><title>&#31038;&#21306;&#25361;&#25112;&#35780;&#20272;&#31454;&#36187;&#22312;&#20419;&#36827;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#25366;&#25496;&#30740;&#31350;&#20013;&#30340;&#25216;&#26415;&#21019;&#26032;&#21644;&#36328;&#23398;&#31185;&#21512;&#20316;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.04261</link><description>&lt;p&gt;
&#36890;&#36807;&#31038;&#21306;&#25361;&#25112;&#25512;&#21160;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#25366;&#25496;&#30340;&#21457;&#23637;
&lt;/p&gt;
&lt;p&gt;
Advancing Biomedical Text Mining with Community Challenges
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04261
&lt;/p&gt;
&lt;p&gt;
&#31038;&#21306;&#25361;&#25112;&#35780;&#20272;&#31454;&#36187;&#22312;&#20419;&#36827;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#25366;&#25496;&#30740;&#31350;&#20013;&#30340;&#25216;&#26415;&#21019;&#26032;&#21644;&#36328;&#23398;&#31185;&#21512;&#20316;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#30740;&#31350;&#39046;&#22495;&#31215;&#32047;&#20102;&#22823;&#37327;&#26469;&#33258;&#31185;&#23398;&#25991;&#29486;&#12289;&#30005;&#23376;&#30149;&#21382;&#12289;&#20020;&#24202;&#35797;&#39564;&#25253;&#21578;&#21644;&#31038;&#20132;&#23186;&#20307;&#31561;&#21508;&#26041;&#38754;&#30340;&#25991;&#26412;&#25968;&#25454;&#65292;&#28982;&#32780;&#25163;&#21160;&#22788;&#29702;&#21644;&#20998;&#26512;&#36825;&#20123;&#24222;&#22823;&#19988;&#22797;&#26434;&#30340;&#36164;&#28304;&#26159;&#32791;&#26102;&#19988;&#20302;&#25928;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#25366;&#25496;&#65292;&#20063;&#31216;&#20026;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#22791;&#21463;&#20851;&#27880;&#12290;&#31038;&#21306;&#25361;&#25112;&#35780;&#20272;&#31454;&#36187;&#22312;&#20419;&#36827;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#25366;&#25496;&#30740;&#31350;&#20013;&#30340;&#25216;&#26415;&#21019;&#26032;&#21644;&#36328;&#23398;&#31185;&#21512;&#20316;&#26041;&#38754;&#21457;&#25381;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#36825;&#20123;&#25361;&#25112;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#24320;&#21457;&#29983;&#29289;&#21307;&#23398;&#30740;&#31350;&#20013;&#25968;&#25454;&#25366;&#25496;&#21644;&#20449;&#24687;&#22788;&#29702;&#30340;&#26368;&#26032;&#35299;&#20915;&#26041;&#26696;&#30340;&#24179;&#21488;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#19982;&#20013;&#25991;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#25366;&#25496;&#26377;&#20851;&#30340;&#26368;&#26032;&#31038;&#21306;&#25361;&#25112;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04261v1 Announce Type: new  Abstract: The field of biomedical research has witnessed a significant increase in the accumulation of vast amounts of textual data from various sources such as scientific literatures, electronic health records, clinical trial reports, and social media. However, manually processing and analyzing these extensive and complex resources is time-consuming and inefficient. To address this challenge, biomedical text mining, also known as biomedical natural language processing, has garnered great attention. Community challenge evaluation competitions have played an important role in promoting technology innovation and interdisciplinary collaboration in biomedical text mining research. These challenges provide platforms for researchers to develop state-of-the-art solutions for data mining and information processing in biomedical research. In this article, we review the recent advances in community challenges specific to Chinese biomedical text mining. Firs
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GPT-FedRec&#30340;&#32852;&#37030;&#25512;&#33616;&#26694;&#26550;&#65292;&#21033;&#29992;ChatGPT&#21644;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#21512;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#26426;&#21046;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;FR&#31995;&#32479;&#22312;&#25968;&#25454;&#31232;&#30095;&#24615;&#21644;&#24322;&#26500;&#24615;&#26041;&#38754;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#65292;&#24357;&#34917;&#20102;&#22522;&#20110;LLM&#30340;&#25512;&#33616;&#22120;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#25512;&#29702;&#25928;&#29575;&#20302;&#21644;&#28508;&#22312;&#24187;&#35273;&#31561;&#25361;&#25112;&#65292;&#23454;&#29616;&#20102;&#38544;&#31169;&#20445;&#25252;&#25512;&#33616;&#30340;&#30446;&#26631;</title><link>https://arxiv.org/abs/2403.04256</link><description>&lt;p&gt;
&#22522;&#20110;&#28151;&#21512;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#30340;&#32852;&#37030;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Federated Recommendation via Hybrid Retrieval Augmented Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04256
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GPT-FedRec&#30340;&#32852;&#37030;&#25512;&#33616;&#26694;&#26550;&#65292;&#21033;&#29992;ChatGPT&#21644;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#21512;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#26426;&#21046;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;FR&#31995;&#32479;&#22312;&#25968;&#25454;&#31232;&#30095;&#24615;&#21644;&#24322;&#26500;&#24615;&#26041;&#38754;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#65292;&#24357;&#34917;&#20102;&#22522;&#20110;LLM&#30340;&#25512;&#33616;&#22120;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#25512;&#29702;&#25928;&#29575;&#20302;&#21644;&#28508;&#22312;&#24187;&#35273;&#31561;&#25361;&#25112;&#65292;&#23454;&#29616;&#20102;&#38544;&#31169;&#20445;&#25252;&#25512;&#33616;&#30340;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#25512;&#33616;&#65288;FR&#65289;&#26159;&#19968;&#31181;&#26032;&#20852;&#33539;&#24335;&#65292;&#33021;&#22815;&#23454;&#29616;&#38544;&#31169;&#20445;&#25252;&#25512;&#33616;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;FR&#31995;&#32479;&#36890;&#24120;&#20351;&#29992;&#31163;&#25955;&#30340;&#36523;&#20221;&#65288;ID&#65289;&#34920;&#31034;&#29992;&#25143;/&#29289;&#21697;&#65292;&#22312;FR&#20013;&#30001;&#20110;&#25968;&#25454;&#31232;&#30095;&#24615;&#21644;&#24322;&#26500;&#24615;&#32780;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#25512;&#33616;&#22120;&#24050;&#32463;&#22312;&#21508;&#31181;&#25512;&#33616;&#22330;&#26223;&#20013;&#34987;&#35777;&#26126;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;LLM&#30340;&#25512;&#33616;&#22120;&#38754;&#20020;&#35832;&#22914;&#25512;&#29702;&#25928;&#29575;&#20302;&#21644;&#28508;&#22312;&#24187;&#35273;&#31561;&#25361;&#25112;&#65292;&#20174;&#32780;&#24433;&#21709;&#23427;&#20204;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#34920;&#29616;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GPT-FedRec&#30340;&#32852;&#37030;&#25512;&#33616;&#26694;&#26550;&#65292;&#21033;&#29992;ChatGPT&#21644;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#21512;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#26426;&#21046;&#12290;GPT-FedRec&#26159;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#31532;&#19968;&#38454;&#27573;&#26159;&#19968;&#20010;&#28151;&#21512;&#26816;&#32034;&#36807;&#31243;&#65292;&#25366;&#25496;&#22522;&#20110;ID&#30340;&#29992;&#25143;&#27169;&#24335;&#21644;&#22522;&#20110;&#25991;&#26412;&#30340;&#21830;&#21697;&#29305;&#24449;&#12290;&#25509;&#19979;&#26469;&#65292;&#25152;&#26816;&#32034;&#21040;&#30340;&#32467;&#26524;&#34987;&#36716;&#25442;&#20026;&#25991;&#26412;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04256v1 Announce Type: cross  Abstract: Federated Recommendation (FR) emerges as a novel paradigm that enables privacy-preserving recommendations. However, traditional FR systems usually represent users/items with discrete identities (IDs), suffering from performance degradation due to the data sparsity and heterogeneity in FR. On the other hand, Large Language Models (LLMs) as recommenders have proven effective across various recommendation scenarios. Yet, LLM-based recommenders encounter challenges such as low inference efficiency and potential hallucination, compromising their performance in real-world scenarios. To this end, we propose GPT-FedRec, a federated recommendation framework leveraging ChatGPT and a novel hybrid Retrieval Augmented Generation (RAG) mechanism. GPT-FedRec is a two-stage solution. The first stage is a hybrid retrieval process, mining ID-based user patterns and text-based item features. Next, the retrieved results are converted into text prompts and
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;CNN-LSTM&#19977;&#38454;&#27573;&#27169;&#22411;PEnet&#65292;&#36890;&#36807;CNN&#23545;&#25968;&#25454;&#29305;&#24449;&#36827;&#34892;&#27987;&#32553;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#26041;&#27861;&#65292;&#20855;&#26377;&#39640;&#20934;&#30830;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#20197;&#21450;&#38271;&#24207;&#21015;&#35266;&#27979;&#30340;&#22686;&#24378;&#25512;&#26029;&#36895;&#24230;&#21644;&#39640;&#27867;&#21270;&#33021;&#21147;&#65292;&#20174;&#32780;&#22312;&#20272;&#35745;SDE&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.04246</link><description>&lt;p&gt;
&#22522;&#20110;CNN-LSTM&#30340;Levy&#39537;&#21160;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#21442;&#25968;&#20272;&#35745;&#30340;&#39640;&#25928;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Efficient CNN-LSTM based Parameter Estimation of Levy Driven Stochastic Differential Equations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04246
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;CNN-LSTM&#19977;&#38454;&#27573;&#27169;&#22411;PEnet&#65292;&#36890;&#36807;CNN&#23545;&#25968;&#25454;&#29305;&#24449;&#36827;&#34892;&#27987;&#32553;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#26041;&#27861;&#65292;&#20855;&#26377;&#39640;&#20934;&#30830;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#20197;&#21450;&#38271;&#24207;&#21015;&#35266;&#27979;&#30340;&#22686;&#24378;&#25512;&#26029;&#36895;&#24230;&#21644;&#39640;&#27867;&#21270;&#33021;&#21147;&#65292;&#20174;&#32780;&#22312;&#20272;&#35745;SDE&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#30001;&#38750;&#39640;&#26031;&#22122;&#22768;&#39537;&#21160;&#30340;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#21442;&#25968;&#20272;&#35745;&#20013;&#30340;&#25361;&#25112;&#65292;&#36825;&#23545;&#20110;&#29702;&#35299;&#20215;&#26684;&#27874;&#21160;&#21644;&#20256;&#26579;&#30149;&#20256;&#25773;&#31561;&#21160;&#24577;&#29616;&#35937;&#33267;&#20851;&#37325;&#35201;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102;LSTM&#32593;&#32476;&#22312;&#20272;&#35745;alpha&#31283;&#23450;Levy&#39537;&#21160;&#30340;SDE&#21442;&#25968;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#20294;&#38754;&#20020;&#39640;&#26102;&#38388;&#22797;&#26434;&#24230;&#21644;LSTM&#38142;&#25509;&#23646;&#24615;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;PEnet&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;CNN-LSTM&#30340;&#19977;&#38454;&#27573;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#26041;&#27861;&#65292;&#20855;&#26377;&#20248;&#36234;&#30340;&#20934;&#30830;&#24615;&#21644;&#36866;&#24212;&#19981;&#21516;&#25968;&#25454;&#32467;&#26500;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;CNN&#23545;&#21021;&#22987;&#25968;&#25454;&#29305;&#24449;&#36827;&#34892;&#27987;&#32553;&#65292;&#20026;&#38271;&#24207;&#21015;&#35266;&#27979;&#25552;&#20379;&#22686;&#24378;&#30340;&#25512;&#26029;&#36895;&#24230;&#65292;&#24182;&#20855;&#26377;&#39640;&#27867;&#21270;&#33021;&#21147;&#65292;&#20801;&#35768;&#20854;&#24212;&#29992;&#20110;&#21508;&#31181;&#22797;&#26434;&#30340;SDE&#22330;&#26223;&#12290;&#23545;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;PEnet&#22312;&#20272;&#35745;SDE&#20013;&#30340;&#26174;&#33879;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04246v1 Announce Type: cross  Abstract: This study addresses the challenges in parameter estimation of stochastic differential equations driven by non-Gaussian noises, which are critical in understanding dynamic phenomena such as price fluctuations and the spread of infectious diseases. Previous research highlighted the potential of LSTM networks in estimating parameters of alpha stable Levy driven SDEs but faced limitations including high time complexity and constraints of the LSTM chaining property. To mitigate these issues, we introduce the PEnet, a novel CNN-LSTM-based three-stage model that offers an end to end approach with superior accuracy and adaptability to varying data structures, enhanced inference speed for long sequence observations through initial data feature condensation by CNN, and high generalization capability, allowing its application to various complex SDE scenarios. Experiments on synthetic datasets confirm PEnet significant advantage in estimating SDE
&lt;/p&gt;</description></item><item><title>DEEP-ICL &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20219;&#21153;&#23450;&#20041;&#20016;&#23500;&#30340;&#19987;&#23478;&#38598;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#31034;&#33539;&#20013;&#25552;&#21462;&#20219;&#21153;&#23450;&#20041;&#24182;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#31034;&#20363;&#65292;&#23454;&#29616;&#20102;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#20855;&#26377;&#21487;&#27604;&#24615;&#30340;&#24615;&#33021;&#65292;&#31361;&#30772;&#20102;&#20256;&#32479;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.04233</link><description>&lt;p&gt;
DEEP-ICL: &#23450;&#20041;&#20016;&#23500;&#30340;&#19987;&#23478;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DEEP-ICL: Definition-Enriched Experts for Language Model In-Context Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04233
&lt;/p&gt;
&lt;p&gt;
DEEP-ICL &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20219;&#21153;&#23450;&#20041;&#20016;&#23500;&#30340;&#19987;&#23478;&#38598;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#31034;&#33539;&#20013;&#25552;&#21462;&#20219;&#21153;&#23450;&#20041;&#24182;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#31034;&#20363;&#65292;&#23454;&#29616;&#20102;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#20855;&#26377;&#21487;&#27604;&#24615;&#30340;&#24615;&#33021;&#65292;&#31361;&#30772;&#20102;&#20256;&#32479;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#20197;&#26469;&#65292;&#20154;&#20204;&#19968;&#30452;&#35748;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#21442;&#25968;&#25968;&#37327;&#39537;&#21160;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#33021;&#21147;&#65292;&#36890;&#36807;&#21033;&#29992;&#20219;&#21153;&#29305;&#23450;&#30340;&#31034;&#33539;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#25361;&#25112;&#36825;&#19968;&#20551;&#35774;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;DEEP-ICL&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#20219;&#21153;&#23450;&#20041;&#20016;&#23500;&#30340;&#19987;&#23478;&#38598;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;ICL&#12290; DEEP-ICL&#20174;&#32473;&#23450;&#30340;&#31034;&#33539;&#20013;&#26126;&#30830;&#25552;&#21462;&#20219;&#21153;&#23450;&#20041;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#31034;&#20363;&#29983;&#25104;&#21709;&#24212;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;ICL&#30340;&#25913;&#36827;&#24182;&#19981;&#30452;&#25509;&#20381;&#36182;&#20110;&#27169;&#22411;&#22823;&#23567;&#65292;&#32780;&#22522;&#26412;&#19978;&#28304;&#33258;&#20110;&#29702;&#35299;&#20219;&#21153;&#23450;&#20041;&#21644;&#20219;&#21153;&#24341;&#23548;&#23398;&#20064;&#12290;&#21463;&#21040;&#36825;&#19968;&#21551;&#21457;&#65292;DEEP-ICL&#32467;&#21512;&#20102;&#20004;&#20010;&#20855;&#26377;&#19981;&#21516;&#35282;&#33394;&#30340;3B&#27169;&#22411;&#65288;&#19968;&#20010;&#29992;&#20110;&#24635;&#32467;&#20219;&#21153;&#23450;&#20041;&#65292;&#21478;&#19968;&#20010;&#29992;&#20110;&#23398;&#20064;&#20219;&#21153;&#31034;&#33539;&#65289;&#65292;&#24182;&#23454;&#29616;&#20102;&#19982;LLaMA2-13B&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#20811;&#26381;&#39044;&#35757;&#32451;&#24207;&#21015;&#38271;&#24230;&#65292;&#20248;&#20110;&#20256;&#32479;ICL&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04233v1 Announce Type: cross  Abstract: It has long been assumed that the sheer number of parameters in large language models (LLMs) drives in-context learning (ICL) capabilities, enabling remarkable performance improvements by leveraging task-specific demonstrations. Challenging this hypothesis, we introduce DEEP-ICL, a novel task Definition Enriched ExPert Ensembling methodology for ICL. DEEP-ICL explicitly extracts task definitions from given demonstrations and generates responses through learning task-specific examples. We argue that improvement from ICL does not directly rely on model size, but essentially stems from understanding task definitions and task-guided learning. Inspired by this, DEEP-ICL combines two 3B models with distinct roles (one for concluding task definitions and the other for learning task demonstrations) and achieves comparable performance to LLaMA2-13B. Furthermore, our framework outperforms conventional ICL by overcoming pretraining sequence lengt
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#22522;&#20110;&#22810;&#27531;&#24046;&#20219;&#21153;&#23398;&#20064;&#30340;&#27867;&#21270;&#21327;&#21516;&#29983;&#24577;&#39550;&#39542;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#35299;&#25511;&#21046;&#24182;&#36816;&#29992;&#23398;&#20064;&#26469;&#35299;&#20915;&#22810;&#20010;&#20132;&#36890;&#22330;&#26223;&#30340;&#25361;&#25112;</title><link>https://arxiv.org/abs/2403.04232</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#27531;&#24046;&#20219;&#21153;&#23398;&#20064;&#23454;&#29616;&#21327;&#21516;&#29983;&#24577;&#39550;&#39542;&#30340;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Generalizing Cooperative Eco-driving via Multi-residual Task Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04232
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#22522;&#20110;&#22810;&#27531;&#24046;&#20219;&#21153;&#23398;&#20064;&#30340;&#27867;&#21270;&#21327;&#21516;&#29983;&#24577;&#39550;&#39542;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#35299;&#25511;&#21046;&#24182;&#36816;&#29992;&#23398;&#20064;&#26469;&#35299;&#20915;&#22810;&#20010;&#20132;&#36890;&#22330;&#26223;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#25511;&#21046;&#65292;&#22914;&#22522;&#20110;&#27169;&#22411;&#30340;&#25511;&#21046;&#65292;&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#36890;&#24120;&#34987;&#20351;&#29992;&#65292;&#22240;&#20026;&#20854;&#25928;&#29575;&#21644;&#21487;&#38752;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#33258;&#21160;&#39550;&#39542;&#35201;&#24212;&#23545;&#21508;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#19981;&#21516;&#20132;&#36890;&#22330;&#26223;&#65292;&#36825;&#23545;&#36825;&#20123;&#35268;&#21010;&#31639;&#27861;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22522;&#20110;&#27169;&#22411;&#26080;&#20851;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#22312;&#36825;&#26041;&#38754;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#36884;&#24452;&#65292;&#20294;&#23398;&#20064;DRL&#25511;&#21046;&#31574;&#30053;&#20197;&#27867;&#21270;&#21040;&#22810;&#20010;&#20132;&#36890;&#22330;&#26223;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#27531;&#24046;&#20219;&#21153;&#23398;&#20064;&#65288;MRTL&#65289;&#30340;&#36890;&#29992;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#26159;&#22522;&#20110;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#65292;&#38024;&#23545;&#19968;&#32452;&#20219;&#21153;&#22330;&#26223;&#65292;&#23558;&#25511;&#21046;&#20998;&#35299;&#20026;&#26377;&#25928;&#30001;&#20256;&#32479;&#25511;&#21046;&#26041;&#27861;&#35299;&#20915;&#30340;&#21517;&#20041;&#20998;&#37327;&#21644;&#30001;&#23398;&#20064;&#35299;&#20915;&#30340;&#27531;&#24046;&#39033;&#12290;&#25105;&#20204;&#20351;&#29992;MRTL&#26469;&#22312;&#28151;&#21512;&#20132;&#36890;&#20013;&#36890;&#36807;&#20351;&#29992;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#26469;&#23454;&#29616;&#36710;&#38431;&#32423;&#21035;&#30340;&#25490;&#25918;&#20943;&#23569;&#20316;&#20026;&#31995;&#32479;&#25511;&#21046;&#25163;&#27573;&#12290;&#36890;&#36807;&#20998;&#26512;MR&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04232v1 Announce Type: cross  Abstract: Conventional control, such as model-based control, is commonly utilized in autonomous driving due to its efficiency and reliability. However, real-world autonomous driving contends with a multitude of diverse traffic scenarios that are challenging for these planning algorithms. Model-free Deep Reinforcement Learning (DRL) presents a promising avenue in this direction, but learning DRL control policies that generalize to multiple traffic scenarios is still a challenge. To address this, we introduce Multi-residual Task Learning (MRTL), a generic learning framework based on multi-task learning that, for a set of task scenarios, decomposes the control into nominal components that are effectively solved by conventional control methods and residual terms which are solved using learning. We employ MRTL for fleet-level emission reduction in mixed traffic using autonomous vehicles as a means of system control. By analyzing the performance of MR
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35757;&#32451;&#23545;&#40784;&#22120;&#27169;&#22411;&#26469;&#35299;&#32806;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#23545;&#40784;&#65292;&#20197;&#20943;&#23569;&#23545;&#40784;&#23545;&#24615;&#33021;&#30340;&#28508;&#22312;&#36127;&#38754;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.04224</link><description>&lt;p&gt;
Aligners: &#35299;&#32806;LLMs&#21644;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Aligners: Decoupling LLMs and Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04224
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35757;&#32451;&#23545;&#40784;&#22120;&#27169;&#22411;&#26469;&#35299;&#32806;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#23545;&#40784;&#65292;&#20197;&#20943;&#23569;&#23545;&#40784;&#23545;&#24615;&#33021;&#30340;&#28508;&#22312;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38656;&#35201;&#19982;&#20154;&#31867;&#26399;&#26395;&#23545;&#40784;&#65292;&#20197;&#30830;&#20445;&#23427;&#20204;&#22312;&#22823;&#22810;&#25968;&#24212;&#29992;&#20013;&#30340;&#23433;&#20840;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;&#23545;&#40784;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#25104;&#26412;&#39640;&#26114;&#65292;&#24182;&#19988;&#38656;&#35201;&#20026;&#27599;&#20010;LLM&#21644;&#23545;&#40784;&#26631;&#20934;&#37325;&#22797;&#36827;&#34892;&#12290;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#35757;&#32451;&#21487;&#20197;&#26681;&#25454;&#38656;&#35201;&#29992;&#20110;&#23545;&#40784;&#32473;&#23450;&#26631;&#20934;&#30340;&#20219;&#20309;LLM&#30340;&#23545;&#40784;&#27169;&#22411;&#26469;&#35299;&#32806;LLMs&#21644;&#23545;&#40784;&#65292;&#20174;&#32780;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#20943;&#23569;&#23545;&#24615;&#33021;&#30340;&#28508;&#22312;&#36127;&#38754;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#23545;&#40784;&#27169;&#22411;&#35757;&#32451;&#37197;&#26041;&#20165;&#20381;&#36182;&#20110;&#20351;&#29992;&#65288;&#25552;&#31034;&#30340;&#65289;LLM &#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#65292;&#24182;&#19988;&#21487;&#20197;&#36731;&#26494;&#35843;&#25972;&#20197;&#36866;&#24212;&#21508;&#31181;&#23545;&#40784;&#26631;&#20934;&#12290;&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#8220;&#36947;&#24503;&#8221;&#23545;&#40784;&#22120;&#24182;&#22312;&#23454;&#39564;&#19978;&#39564;&#35777;&#20854;&#26377;&#25928;&#24615;&#26469;&#38416;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04224v1 Announce Type: cross  Abstract: Large Language Models (LLMs) need to be aligned with human expectations to ensure their safety and utility in most applications. Alignment is challenging, costly, and needs to be repeated for every LLM and alignment criterion. We propose to decouple LLMs and alignment by training aligner models that can be used to align any LLM for a given criteria on an as-needed basis, thus also reducing the potential negative impacts of alignment on performance. Our recipe for training the aligner models solely relies on synthetic data generated with a (prompted) LLM and can be easily adjusted for a variety of alignment criteria. We illustrate our method by training an "ethical" aligner and verify its efficacy empirically.
&lt;/p&gt;</description></item><item><title>&#24378;&#21270;&#23398;&#20064;&#21644;&#22240;&#26524;&#24314;&#27169;&#30456;&#20114;&#34917;&#20805;&#65292;&#35770;&#25991;&#20027;&#35201;&#25351;&#20986;&#22312;&#32447;&#23398;&#20064;&#29615;&#22659;&#19979;&#65292;&#26465;&#20214;&#27010;&#29575;&#20855;&#26377;&#22240;&#26524;&#24615;&#65292;&#31163;&#32447;RL&#26159;&#22240;&#26524;&#23398;&#20064;&#28508;&#21147;&#26368;&#22823;&#30340;&#29615;&#22659;&#12290;</title><link>https://arxiv.org/abs/2403.04221</link><description>&lt;p&gt;
&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#20026;&#20309;&#20855;&#26377;&#22240;&#26524;&#24615;
&lt;/p&gt;
&lt;p&gt;
Why Online Reinforcement Learning is Causal
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04221
&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#21644;&#22240;&#26524;&#24314;&#27169;&#30456;&#20114;&#34917;&#20805;&#65292;&#35770;&#25991;&#20027;&#35201;&#25351;&#20986;&#22312;&#32447;&#23398;&#20064;&#29615;&#22659;&#19979;&#65292;&#26465;&#20214;&#27010;&#29575;&#20855;&#26377;&#22240;&#26524;&#24615;&#65292;&#31163;&#32447;RL&#26159;&#22240;&#26524;&#23398;&#20064;&#28508;&#21147;&#26368;&#22823;&#30340;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#21644;&#22240;&#26524;&#24314;&#27169;&#33258;&#28982;&#20114;&#34917;&#12290;&#22240;&#26524;&#24314;&#27169;&#30340;&#30446;&#26631;&#26159;&#39044;&#27979;&#22312;&#29615;&#22659;&#20013;&#36827;&#34892;&#24178;&#39044;&#30340;&#25928;&#26524;&#65292;&#32780;&#24378;&#21270;&#23398;&#20064;&#30340;&#30446;&#26631;&#26159;&#36873;&#25321;&#26368;&#22823;&#21270;&#20195;&#29702;&#20174;&#29615;&#22659;&#20013;&#25509;&#25910;&#30340;&#22870;&#21169;&#30340;&#24178;&#39044;&#12290;&#24378;&#21270;&#23398;&#20064;&#21253;&#25324;&#29992;&#20110;&#20272;&#35745;&#22240;&#26524;&#20851;&#31995;&#30340;&#20004;&#20010;&#26368;&#24378;&#22823;&#20449;&#24687;&#28304;&#65306;&#26102;&#38388;&#39034;&#24207;&#21644;&#23545;&#29615;&#22659;&#36827;&#34892;&#25805;&#20316;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#25105;&#20204;&#21487;&#20197;&#26399;&#26395;&#22312;&#21738;&#20123;&#24378;&#21270;&#23398;&#20064;&#35774;&#32622;&#20013;&#20174;&#22240;&#26524;&#24314;&#27169;&#20013;&#21463;&#30410;&#65292;&#20197;&#21450;&#22914;&#20309;&#21463;&#30410;&#12290;&#22312;&#32447;&#23398;&#20064;&#20013;&#65292;&#20195;&#29702;&#26377;&#33021;&#21147;&#30452;&#25509;&#19982;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#65292;&#24182;&#20174;&#25506;&#32034;&#20013;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#35770;&#28857;&#26159;&#65292;&#22312;&#22312;&#32447;&#23398;&#20064;&#20013;&#65292;&#26465;&#20214;&#27010;&#29575;&#26159;&#22240;&#26524;&#30340;&#65292;&#22240;&#27492;&#31163;&#32447;RL&#26159;&#22240;&#26524;&#23398;&#20064;&#26377;&#28508;&#21147;&#20135;&#29983;&#24046;&#24322;&#30340;&#29615;&#22659;&#12290;&#22522;&#26412;&#19978;&#65292;&#21407;&#22240;&#22312;&#20110;&#24403;&#20195;&#29702;&#19982;&#29615;&#22659;&#20114;&#21160;&#26102;&#65292;&#20195;&#29702;&#30340;&#34892;&#20026;&#26159;&#30001;&#20854;&#23545;&#29615;&#22659;&#30340;&#35748;&#35782;&#25152;&#25512;&#21160;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04221v1 Announce Type: cross  Abstract: Reinforcement learning (RL) and causal modelling naturally complement each other. The goal of causal modelling is to predict the effects of interventions in an environment, while the goal of reinforcement learning is to select interventions that maximize the rewards the agent receives from the environment. Reinforcement learning includes the two most powerful sources of information for estimating causal relationships: temporal ordering and the ability to act on an environment. This paper examines which reinforcement learning settings we can expect to benefit from causal modelling, and how. In online learning, the agent has the ability to interact directly with their environment, and learn from exploring it. Our main argument is that in online learning, conditional probabilities are causal, and therefore offline RL is the setting where causal learning has the most potential to make a difference. Essentially, the reason is that when an a
&lt;/p&gt;</description></item><item><title>&#35843;&#26597;&#20102;&#22823;&#22411;&#27169;&#22411;&#20215;&#20540;&#23545;&#40784;&#26041;&#27861;&#65292;&#25581;&#31034;&#21382;&#21490;&#32972;&#26223;&#21644;&#25968;&#23398;&#26412;&#36136;&#65292;&#35814;&#32454;&#35752;&#35770;&#20102;&#24378;&#21270;&#23398;&#20064;&#12289;&#30417;&#30563;&#24494;&#35843;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#31561;&#23545;&#40784;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.04204</link><description>&lt;p&gt;
&#23545;&#20110;&#22823;&#22411;&#27169;&#22411;&#23545;&#40784;&#26041;&#27861;&#30340;&#25506;&#31350;&#65306;&#26412;&#36136;&#19982;&#21069;&#26223;
&lt;/p&gt;
&lt;p&gt;
On the Essence and Prospect: An Investigation of Alignment Approaches for Big Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04204
&lt;/p&gt;
&lt;p&gt;
&#35843;&#26597;&#20102;&#22823;&#22411;&#27169;&#22411;&#20215;&#20540;&#23545;&#40784;&#26041;&#27861;&#65292;&#25581;&#31034;&#21382;&#21490;&#32972;&#26223;&#21644;&#25968;&#23398;&#26412;&#36136;&#65292;&#35814;&#32454;&#35752;&#35770;&#20102;&#24378;&#21270;&#23398;&#20064;&#12289;&#30417;&#30563;&#24494;&#35843;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#31561;&#23545;&#40784;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#27169;&#22411;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#21462;&#24471;&#20102;&#38761;&#21629;&#24615;&#31361;&#30772;&#65292;&#20294;&#20063;&#21487;&#33021;&#24102;&#26469;&#28508;&#22312;&#30340;&#38382;&#39064;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#23545;&#40784;&#25216;&#26415;&#65292;&#20351;&#36825;&#20123;&#27169;&#22411;&#31526;&#21512;&#20154;&#31867;&#30340;&#20559;&#22909;&#21644;&#20215;&#20540;&#35266;&#12290;&#23613;&#31649;&#22312;&#36807;&#21435;&#19968;&#24180;&#21462;&#24471;&#20102;&#30456;&#24403;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#24314;&#31435;&#26368;&#20339;&#23545;&#40784;&#31574;&#30053;&#20173;&#38754;&#20020;&#35832;&#22810;&#25361;&#25112;&#65292;&#22914;&#25968;&#25454;&#25104;&#26412;&#21644;&#21487;&#25193;&#23637;&#24615;&#30417;&#30563;&#12290;&#22914;&#20309;&#36827;&#34892;&#23545;&#40784;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#31687;&#35843;&#26597;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#35843;&#26597;&#20102;&#20215;&#20540;&#23545;&#40784;&#26041;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#25581;&#31034;&#20102;&#23545;&#40784;&#30340;&#21382;&#21490;&#32972;&#26223;&#65292;&#36861;&#28335;&#21040;20&#19990;&#32426;20&#24180;&#20195;&#65288;&#23427;&#30340;&#36215;&#28304;&#65289;&#65292;&#28982;&#21518;&#28145;&#20837;&#25506;&#35752;&#20102;&#23545;&#40784;&#30340;&#25968;&#23398;&#26412;&#36136;&#65288;&#23427;&#26159;&#20160;&#20040;&#65289;&#65292;&#38416;&#26126;&#20102;&#20854;&#20013;&#22266;&#26377;&#30340;&#25361;&#25112;&#12290;&#22312;&#22880;&#23450;&#20102;&#36825;&#20010;&#22522;&#30784;&#21518;&#65292;&#25105;&#20204;&#23545;&#29616;&#26377;&#30340;&#23545;&#40784;&#26041;&#27861;&#36827;&#34892;&#20102;&#35814;&#32454;&#32771;&#23519;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#20998;&#20026;&#19977;&#31867;&#65306;&#24378;&#21270;&#23398;&#20064;&#12289;&#30417;&#30563;&#24494;&#35843;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04204v1 Announce Type: new  Abstract: Big models have achieved revolutionary breakthroughs in the field of AI, but they might also pose potential concerns. Addressing such concerns, alignment technologies were introduced to make these models conform to human preferences and values. Despite considerable advancements in the past year, various challenges lie in establishing the optimal alignment strategy, such as data cost and scalable oversight, and how to align remains an open question. In this survey paper, we comprehensively investigate value alignment approaches. We first unpack the historical context of alignment tracing back to the 1920s (where it comes from), then delve into the mathematical essence of alignment (what it is), shedding light on the inherent challenges. Following this foundation, we provide a detailed examination of existing alignment methods, which fall into three categories: Reinforcement Learning, Supervised Fine-Tuning, and In-context Learning, and de
&lt;/p&gt;</description></item><item><title>&#22312;&#22810;&#20195;&#29702;&#29615;&#22659;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#25506;&#35752;&#20102;&#19981;&#21516;&#36947;&#24503;&#31867;&#22411;&#30340;&#23398;&#20064;&#20195;&#29702;&#20043;&#38388;&#30340;&#20114;&#21160;&#65292;&#21457;&#29616;&#36947;&#24503;&#24322;&#36136;&#24615;&#21487;&#33021;&#23545;&#20195;&#29702;&#30340;&#20849;&#21516;&#21457;&#23637;&#20135;&#29983;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.04202</link><description>&lt;p&gt;
&#24322;&#36136;&#23398;&#20064;&#20195;&#29702;&#32676;&#20307;&#20013;&#36947;&#24503;&#34892;&#20026;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Dynamics of Moral Behavior in Heterogeneous Populations of Learning Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04202
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#20195;&#29702;&#29615;&#22659;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#25506;&#35752;&#20102;&#19981;&#21516;&#36947;&#24503;&#31867;&#22411;&#30340;&#23398;&#20064;&#20195;&#29702;&#20043;&#38388;&#30340;&#20114;&#21160;&#65292;&#21457;&#29616;&#36947;&#24503;&#24322;&#36136;&#24615;&#21487;&#33021;&#23545;&#20195;&#29702;&#30340;&#20849;&#21516;&#21457;&#23637;&#20135;&#29983;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04202v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;&#39046;&#22495; &#25688;&#35201;&#65306;&#26085;&#30410;&#20851;&#27880;AI&#31995;&#32479;&#23433;&#20840;&#21644;&#23545;&#40784;&#24615;&#30340;&#38382;&#39064;&#31361;&#26174;&#20102;&#22312;&#20154;&#24037;&#20195;&#29702;&#20013;&#23884;&#20837;&#36947;&#24503;&#33021;&#21147;&#30340;&#37325;&#35201;&#24615;&#12290;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#21033;&#29992;&#32463;&#39564;&#23398;&#20064;&#65292;&#21363;&#24378;&#21270;&#23398;&#20064;&#12290;&#22312;&#22810;&#20195;&#29702;&#65288;&#31038;&#20250;&#65289;&#29615;&#22659;&#20013;&#65292;&#20010;&#20307;&#23398;&#20064;&#20195;&#29702;&#20043;&#38388;&#30340;&#20132;&#20114;&#21487;&#33021;&#20135;&#29983;&#22797;&#26434;&#30340;&#32676;&#20307;&#23618;&#38754;&#29616;&#35937;&#12290;&#35768;&#22810;&#29616;&#26377;&#30740;&#31350;&#20381;&#36182;&#20110;&#27169;&#25311;&#30340;&#31038;&#20250;&#22256;&#22659;&#29615;&#22659;&#26469;&#30740;&#31350;&#29420;&#31435;&#23398;&#20064;&#20195;&#29702;&#30340;&#20114;&#21160;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24448;&#24448;&#24573;&#35270;&#20102;&#23454;&#36341;&#20013;&#20195;&#29702;&#31038;&#20250;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#36947;&#24503;&#24322;&#36136;&#24615;&#12290;&#20363;&#22914;&#65292;&#22312;&#19981;&#21516;&#26102;&#38388;&#28857;&#65292;&#21333;&#20010;&#23398;&#20064;&#20195;&#29702;&#21487;&#33021;&#38754;&#23545;&#21518;&#26524;&#20027;&#20041;&#32773;&#65288;&#21363;&#20851;&#24515;&#38543;&#26102;&#38388;&#26368;&#22823;&#21270;&#26576;&#31181;&#32467;&#26524;&#65289;&#25110;&#22522;&#20110;&#35268;&#33539;&#30340;&#23545;&#25163;&#65288;&#21363;&#19987;&#27880;&#20110;&#31435;&#21363;&#36981;&#23432;&#29305;&#23450;&#35268;&#33539;&#65289; &#12290;&#20195;&#29702;&#30340;&#20849;&#21516;&#21457;&#23637;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#21487;&#33021;&#21463;&#21040;&#36825;&#31181;&#36947;&#24503;&#24322;&#36136;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04202v1 Announce Type: cross  Abstract: Growing concerns about safety and alignment of AI systems highlight the importance of embedding moral capabilities in artificial agents. A promising solution is the use of learning from experience, i.e., Reinforcement Learning. In multi-agent (social) environments, complex population-level phenomena may emerge from interactions between individual learning agents. Many of the existing studies rely on simulated social dilemma environments to study the interactions of independent learning agents. However, they tend to ignore the moral heterogeneity that is likely to be present in societies of agents in practice. For example, at different points in time a single learning agent may face opponents who are consequentialist (i.e., caring about maximizing some outcome over time) or norm-based (i.e., focusing on conforming to a specific norm here and now). The extent to which agents' co-development may be impacted by such moral heterogeneity in 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19978;&#19979;&#25991;&#20998;&#23376;&#36866;&#24212;&#65288;ICMA&#65289;&#33539;&#24335;&#65292;&#20801;&#35768;LLMs&#36890;&#36807;&#19978;&#19979;&#25991;&#31034;&#20363;&#23398;&#20064;&#20998;&#23376;-&#25991;&#26412;&#23545;&#40784;&#65292;&#35299;&#20915;&#20102;&#22312;&#20998;&#23376;-&#26631;&#39064;&#32763;&#35793;&#20219;&#21153;&#20013;&#23545;LLMs&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.04197</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#26159;&#19978;&#19979;&#25991;&#20998;&#23376;&#23398;&#20064;&#22120;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are In-Context Molecule Learners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04197
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19978;&#19979;&#25991;&#20998;&#23376;&#36866;&#24212;&#65288;ICMA&#65289;&#33539;&#24335;&#65292;&#20801;&#35768;LLMs&#36890;&#36807;&#19978;&#19979;&#25991;&#31034;&#20363;&#23398;&#20064;&#20998;&#23376;-&#25991;&#26412;&#23545;&#40784;&#65292;&#35299;&#20915;&#20102;&#22312;&#20998;&#23376;-&#26631;&#39064;&#32763;&#35793;&#20219;&#21153;&#20013;&#23545;LLMs&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29983;&#29289;&#21270;&#23398;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#23588;&#20854;&#26159;&#20998;&#23376;&#26631;&#39064;&#32763;&#35793;&#20219;&#21153;&#65292;&#26088;&#22312;&#24357;&#21512;&#20998;&#23376;&#21644;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#22312;&#36866;&#24212;LLMs&#21040;&#20998;&#23376;-&#26631;&#39064;&#32763;&#35793;&#20219;&#21153;&#20013;&#30340;&#26041;&#27861;&#38656;&#35201;&#39069;&#22806;&#30340;&#39046;&#22495;&#29305;&#23450;&#39044;&#35757;&#32451;&#38454;&#27573;&#65292;&#23384;&#22312;&#20998;&#23376;&#21644;&#25991;&#26412;&#31354;&#38388;&#20043;&#38388;&#30340;&#24369;&#23545;&#40784;&#65292;&#25110;&#23545;LLMs&#30340;&#35268;&#27169;&#26377;&#20005;&#26684;&#35201;&#27714;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19978;&#19979;&#25991;&#20998;&#23376;&#36866;&#24212;&#65288;ICMA&#65289;&#65292;&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#33539;&#20363;&#65292;&#20801;&#35768;LLMs&#36890;&#36807;&#19978;&#19979;&#25991;&#31034;&#20363;&#23398;&#20064;&#20998;&#23376;-&#25991;&#26412;&#23545;&#40784;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#20998;&#23376;&#35843;&#25972;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;ICMA&#21253;&#25324;&#20197;&#19979;&#19977;&#20010;&#38454;&#27573;&#65306;&#36328;&#27169;&#24577;&#26816;&#32034;&#12289;&#26816;&#32034;&#21518;&#25490;&#24207;&#21644;&#19978;&#19979;&#25991;&#20998;&#23376;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04197v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated exceptional performance in biochemical tasks, especially the molecule caption translation task, which aims to bridge the gap between molecules and natural language texts. However, previous methods in adapting LLMs to the molecule-caption translation task required extra domain-specific pre-training stages, suffered weak alignment between molecular and textual spaces, or imposed stringent demands on the scale of LLMs. To resolve the challenges, we propose In-Context Molecule Adaptation (ICMA), as a new paradigm allowing LLMs to learn the molecule-text alignment from context examples via In-Context Molecule Tuning. Specifically, ICMA incorporates the following three stages: Cross-modal Retrieval, Post-retrieval Re-ranking, and In-context Molecule Tuning. Initially, Cross-modal Retrieval utilizes BM25 Caption Retrieval and Molecule Graph Retrieval to retrieve informative context examples. Addi
&lt;/p&gt;</description></item><item><title>&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;</title><link>https://arxiv.org/abs/2403.04190</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#29992;&#20110;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#65306;&#26041;&#27861;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Generative AI for Synthetic Data Generation: Methods, Challenges and the Future
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04190
&lt;/p&gt;
&lt;p&gt;
&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20851;&#20110;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#30340;&#30740;&#31350;&#24613;&#21095;&#22686;&#21152;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#65292;&#26631;&#24535;&#30528;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#39046;&#22495;&#30340;&#19968;&#20010;&#26174;&#33879;&#36716;&#21464;&#12290;&#23427;&#20204;&#33021;&#22815;&#34920;&#29616;&#20986;&#19982;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30456;&#23218;&#32654;&#30340;&#33021;&#21147;&#65292;&#23558;&#36825;&#31181;&#26041;&#27861;&#23450;&#20301;&#20026;&#35299;&#20915;&#20302;&#36164;&#28304;&#25361;&#25112;&#30340;&#19968;&#20010;&#24341;&#20154;&#27880;&#30446;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#21033;&#29992;&#36825;&#20123;&#24222;&#22823;&#30340;LLMs&#29983;&#25104;&#29305;&#23450;&#20219;&#21153;&#35757;&#32451;&#25968;&#25454;&#30340;&#20808;&#36827;&#25216;&#26415;&#12290;&#25105;&#20204;&#27010;&#36848;&#20102;&#26041;&#27861;&#35770;&#12289;&#35780;&#20272;&#25216;&#26415;&#21644;&#23454;&#38469;&#24212;&#29992;&#65292;&#35752;&#35770;&#20102;&#24403;&#21069;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04190v1 Announce Type: cross  Abstract: The recent surge in research focused on generating synthetic data from large language models (LLMs), especially for scenarios with limited data availability, marks a notable shift in Generative Artificial Intelligence (AI). Their ability to perform comparably to real-world data positions this approach as a compelling solution to low-resource challenges. This paper delves into advanced technologies that leverage these gigantic LLMs for the generation of task-specific training data. We outline methodologies, evaluation techniques, and practical applications, discuss the current limitations, and suggest potential pathways for future research.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#30446;&#26631;&#32467;&#21512;&#35774;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#20559;&#22909;&#20248;&#21270;&#21487;&#20197;&#26377;&#25928;&#20248;&#21270;&#29983;&#25104;&#30340;&#32467;&#21512;&#29289;&#26679;&#26412;&#30340;&#31561;&#30005;&#28857;&#65292;&#20174;&#32780;&#25552;&#39640;&#20854;&#32467;&#21512;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.04187</link><description>&lt;p&gt;
&#34507;&#30333;&#35821;&#35328;&#27169;&#22411;&#30340;&#20559;&#22909;&#20248;&#21270;&#20316;&#20026;&#22810;&#30446;&#26631;&#32467;&#21512;&#35774;&#35745;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
Preference optimization of protein language models as a multi-objective binder design paradigm
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04187
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#30446;&#26631;&#32467;&#21512;&#35774;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#20559;&#22909;&#20248;&#21270;&#21487;&#20197;&#26377;&#25928;&#20248;&#21270;&#29983;&#25104;&#30340;&#32467;&#21512;&#29289;&#26679;&#26412;&#30340;&#31561;&#30005;&#28857;&#65292;&#20174;&#32780;&#25552;&#39640;&#20854;&#32467;&#21512;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25351;&#20196;&#24494;&#35843;&#21644;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;(DPO)&#30340;&#33258;&#22238;&#24402;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#65288;pLMs&#65289;&#30340;&#22810;&#30446;&#26631;&#32467;&#21512;&#35774;&#35745;&#33539;&#24335;&#12290;&#36890;&#36807;&#30452;&#25509;&#20248;&#21270;&#19987;&#23478;&#31579;&#36873;&#30340;&#20559;&#22909;&#24207;&#21015;&#25968;&#25454;&#38598;&#65292;&#23558;&#22810;&#20010;&#35774;&#35745;&#30446;&#26631;&#32534;&#30721;&#21040;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#21253;&#25324;&#20248;&#36873;&#21644;&#38750;&#39318;&#36873;&#20998;&#24067;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#23545;&#40784;&#31574;&#30053;&#20351;ProtGPT2&#33021;&#22815;&#26377;&#25928;&#35774;&#35745;&#20197;&#25351;&#23450;&#21463;&#20307;&#21644;&#33647;&#29289;&#21487;&#24320;&#21457;&#24615;&#26631;&#20934;&#20026;&#26465;&#20214;&#30340;&#32467;&#21512;&#29289;&#12290;&#29983;&#25104;&#30340;&#32467;&#21512;&#29289;&#26679;&#26412;&#34920;&#26126;&#31561;&#30005;&#28857;&#65288;pI&#65289;&#20013;&#20301;&#25968;&#25552;&#39640;&#20102;$17\%-60\%$&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04187v1 Announce Type: cross  Abstract: We present a multi-objective binder design paradigm based on instruction fine-tuning and direct preference optimization (DPO) of autoregressive protein language models (pLMs). Multiple design objectives are encoded in the language model through direct optimization on expert curated preference sequence datasets comprising preferred and dispreferred distributions. We show the proposed alignment strategy enables ProtGPT2 to effectively design binders conditioned on specified receptors and a drug developability criterion. Generated binder samples demonstrate median isoelectric point (pI) improvements by $17\%-60\%$.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#24230;&#37327;&#24863;&#30693;&#30340;LLM&#25512;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#33258;&#23450;&#20041;&#25351;&#26631;&#26469;&#25913;&#36827;&#25512;&#26029;&#24615;&#33021;</title><link>https://arxiv.org/abs/2403.04182</link><description>&lt;p&gt;
&#24230;&#37327;&#24863;&#30693;&#30340;LLM&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Metric-aware LLM inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04182
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#24230;&#37327;&#24863;&#30693;&#30340;LLM&#25512;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#33258;&#23450;&#20041;&#25351;&#26631;&#26469;&#25913;&#36827;&#25512;&#26029;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#22312;&#21508;&#31181;NLP&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#24378;&#22823;&#30340;&#32467;&#26524;&#12290;&#36890;&#24120;&#65292;&#36755;&#20986;&#26159;&#36890;&#36807;&#20174;LLM&#30340;&#22522;&#30784;&#20998;&#24067;&#20013;&#36827;&#34892;&#33258;&#22238;&#24402;&#37319;&#26679;&#33719;&#24471;&#30340;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36825;&#31181;&#25512;&#26029;&#31574;&#30053;&#23545;&#20110;&#19968;&#31995;&#21015;&#20219;&#21153;&#21644;&#30456;&#20851;&#30340;&#35780;&#20272;&#25351;&#26631;&#21487;&#33021;&#26159;&#27425;&#20248;&#30340;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24230;&#37327;&#24863;&#30693;&#30340;LLM&#25512;&#26029;&#65306;&#19968;&#31181;&#22312;&#25512;&#26029;&#26102;&#38024;&#23545;&#33258;&#23450;&#20041;&#25351;&#26631;&#36827;&#34892;&#20248;&#21270;&#30340;&#20915;&#31574;&#29702;&#35770;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#23398;&#26415;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#20844;&#24320;&#21487;&#29992;&#27169;&#22411;&#19978;&#25253;&#21578;&#20102;&#30456;&#23545;&#22522;&#32447;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04182v1 Announce Type: cross  Abstract: Large language models (LLMs) have demonstrated strong results on a range of NLP tasks. Typically, outputs are obtained via autoregressive sampling from the LLM's underlying distribution. We show that this inference strategy can be suboptimal for a range of tasks and associated evaluation metrics. As a remedy, we propose metric aware LLM inference: a decision theoretic approach optimizing for custom metrics at inference time. We report improvements over baselines on academic benchmarks and publicly available models.
&lt;/p&gt;</description></item><item><title>&#39318;&#27425;&#22312;&#25918;&#23556;&#27835;&#30103;&#21644;&#20813;&#30123;&#27835;&#30103;&#30456;&#32467;&#21512;&#30340;&#30740;&#31350;&#20013;&#20351;&#29992;&#20102;&#21464;&#21387;&#22120;&#27169;&#22411;&#30340;&#27880;&#24847;&#26426;&#21046;&#65292;&#33021;&#22815;&#21322;&#23450;&#37327;&#39044;&#27979;&#32959;&#30244;&#20307;&#31215;&#21464;&#21270;&#36235;&#21183;&#65292;&#24182;&#36890;&#36807;&#33258;&#27880;&#24847;&#21147;&#21644;&#20132;&#21449;&#27880;&#24847;&#21147;&#24471;&#20998;&#26469;&#35782;&#21035;&#28508;&#22312;&#22240;&#26524;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2403.04175</link><description>&lt;p&gt;
&#36890;&#36807;&#21464;&#21387;&#22120;&#27169;&#22411;&#20013;&#30340;&#27880;&#24847;&#26426;&#21046;&#29702;&#35299;&#32852;&#21512;&#25918;&#23556;&#27835;&#30103;&#21644;&#20813;&#30123;&#27835;&#30103;&#20013;&#30340;PULSAR&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
Understanding the PULSAR Effect in Combined Radiotherapy and Immunotherapy through Attention Mechanisms with a Transformer Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04175
&lt;/p&gt;
&lt;p&gt;
&#39318;&#27425;&#22312;&#25918;&#23556;&#27835;&#30103;&#21644;&#20813;&#30123;&#27835;&#30103;&#30456;&#32467;&#21512;&#30340;&#30740;&#31350;&#20013;&#20351;&#29992;&#20102;&#21464;&#21387;&#22120;&#27169;&#22411;&#30340;&#27880;&#24847;&#26426;&#21046;&#65292;&#33021;&#22815;&#21322;&#23450;&#37327;&#39044;&#27979;&#32959;&#30244;&#20307;&#31215;&#21464;&#21270;&#36235;&#21183;&#65292;&#24182;&#36890;&#36807;&#33258;&#27880;&#24847;&#21147;&#21644;&#20132;&#21449;&#27880;&#24847;&#21147;&#24471;&#20998;&#26469;&#35782;&#21035;&#28508;&#22312;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
PULSAR&#65288;&#20010;&#24615;&#21270;&#12289;&#36229;&#20998;&#24418;&#25104;&#31435;&#20307;&#36866;&#24212;&#24615;&#25918;&#23556;&#27835;&#30103;&#65289;&#26159;&#23558;&#31435;&#20307;&#23450;&#20301;&#28040;&#34701;&#25918;&#23556;&#27835;&#30103;&#24212;&#29992;&#20110;&#20010;&#24615;&#21270;&#30284;&#30151;&#31649;&#29702;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#39318;&#27425;&#24212;&#29992;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#27880;&#24847;&#26426;&#21046;&#26469;&#30740;&#31350;&#32852;&#21512;PULSAR&#21644;PD-L1&#38459;&#26029;&#20813;&#30123;&#30103;&#27861;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#22522;&#20110;&#23567;&#40736;&#30284;&#30151;&#27169;&#22411;&#65288;Lewis&#32954;&#30284;&#65292;LLC&#65289;&#12290;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#22815;&#21322;&#23450;&#37327;&#22320;&#39044;&#27979;&#32959;&#30244;&#20307;&#31215;&#21464;&#21270;&#30340;&#36235;&#21183;&#65292;&#24182;&#36890;&#36807;&#33258;&#27880;&#24847;&#21147;&#21644;&#20132;&#21449;&#27880;&#24847;&#21147;&#24471;&#20998;&#22312;&#35782;&#21035;&#28508;&#22312;&#22240;&#26524;&#20851;&#31995;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04175v1 Announce Type: cross  Abstract: PULSAR (personalized, ultra-fractionated stereotactic adaptive radiotherapy) is the adaptation of stereotactic ablative radiotherapy towards personalized cancer management. For the first time, we applied a transformer-based attention mechanism to investigate the underlying interactions between combined PULSAR and PD-L1 blockade immunotherapy based on a murine cancer model (Lewis Lung Carcinoma, LLC). The proposed approach is able to predict the trend of tumor volume change semi-quantitatively, and excels in identifying the potential causal relationships through both self-attention and cross-attention scores.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#25552;&#31034;&#27169;&#22359;&#65288;APM&#65289;&#65292;&#20026;SAM&#22522;&#30784;&#27169;&#22411;&#25552;&#20379;&#20102;&#22312;&#30446;&#26631;&#39046;&#22495;&#20013;&#20351;&#29992;&#30340;&#33258;&#36866;&#24212;&#25552;&#31034;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;SAM&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.04164</link><description>&lt;p&gt;
ProMISe: &#20351;&#29992;SAM&#36827;&#34892;&#21487;&#25552;&#31034;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
ProMISe: Promptable Medical Image Segmentation using SAM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04164
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#25552;&#31034;&#27169;&#22359;&#65288;APM&#65289;&#65292;&#20026;SAM&#22522;&#30784;&#27169;&#22411;&#25552;&#20379;&#20102;&#22312;&#30446;&#26631;&#39046;&#22495;&#20013;&#20351;&#29992;&#30340;&#33258;&#36866;&#24212;&#25552;&#31034;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;SAM&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25552;&#20986;&#20102;Segment Anything Model (SAM)&#30340;&#24314;&#35758;&#65292;&#23545;SAM&#36827;&#34892;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;(MIS)&#30340;&#24494;&#35843;&#21464;&#24471;&#27969;&#34892;&#36215;&#26469;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;SAM&#27169;&#22411;&#30340;&#35268;&#27169;&#36739;&#22823;&#65292;&#33258;&#28982;&#22270;&#20687;&#21644;&#21307;&#23398;&#22270;&#20687;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#30340;&#39046;&#22495;&#24046;&#36317;&#65292;&#22522;&#20110;&#24494;&#35843;&#30340;&#31574;&#30053;&#25104;&#26412;&#39640;&#65292;&#23384;&#22312;&#19981;&#31283;&#23450;&#24615;&#12289;&#29305;&#24449;&#25439;&#20260;&#21644;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;&#27492;&#22806;&#65292;&#19968;&#20123;&#36890;&#36807;&#24494;&#35843;&#31574;&#30053;&#23558;SAM&#36716;&#31227;&#21040;&#29305;&#23450;&#39046;&#22495;MIS&#30340;&#26041;&#27861;&#31105;&#29992;&#20102;&#27169;&#22411;&#30340;&#25552;&#31034;&#33021;&#21147;&#65292;&#20005;&#37325;&#38480;&#21046;&#20102;&#20854;&#20351;&#29992;&#22330;&#26223;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#25552;&#31034;&#27169;&#22359;&#65288;APM&#65289;&#65292;&#20026;SAM&#22522;&#30784;&#27169;&#22411;&#22312;&#30446;&#26631;&#22495;&#20013;&#25552;&#20379;&#20102;&#20855;&#26377;&#27431;&#20960;&#37324;&#24503;&#33258;&#36866;&#24212;&#25552;&#31034;&#30340;&#22522;&#30784;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#26679;&#30340;&#33258;&#36866;&#24212;&#25552;&#31034;&#26174;&#33879;&#25552;&#39640;&#20102;SAM&#22312;MIS&#20013;&#38750;&#24494;&#35843;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22686;&#37327;&#27169;&#24335;&#31227;&#20301;&#65288;IPS&#65289;&#30340;&#26032;&#22411;&#38750;&#20405;&#20837;&#24335;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;SAM&#35843;&#25972;&#21040;&#29305;&#23450;&#21307;&#30103;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04164v1 Announce Type: cross  Abstract: With the proposal of the Segment Anything Model (SAM), fine-tuning SAM for medical image segmentation (MIS) has become popular. However, due to the large size of the SAM model and the significant domain gap between natural and medical images, fine-tuning-based strategies are costly with potential risk of instability, feature damage and catastrophic forgetting. Furthermore, some methods of transferring SAM to a domain-specific MIS through fine-tuning strategies disable the model's prompting capability, severely limiting its utilization scenarios. In this paper, we propose an Auto-Prompting Module (APM), which provides SAM-based foundation model with Euclidean adaptive prompts in the target domain. Our experiments demonstrate that such adaptive prompts significantly improve SAM's non-fine-tuned performance in MIS. In addition, we propose a novel non-invasive method called Incremental Pattern Shifting (IPS) to adapt SAM to specific medica
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35821;&#26009;&#24211;&#20027;&#39064;&#20998;&#31867;&#25913;&#36827;&#20027;&#39064;&#29305;&#23450;&#24212;&#29992;&#20013;&#26816;&#32034;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#30830;&#23450;&#26597;&#35810;&#21644;&#25991;&#26723;&#30340;&#20013;&#24515;&#20027;&#39064;&#20197;&#21450;&#21033;&#29992;&#20027;&#39064;&#30456;&#20851;&#24615;&#26469;&#34917;&#20805;&#32570;&#22833;&#30340;&#19978;&#19979;&#25991;&#12290;</title><link>https://arxiv.org/abs/2403.04160</link><description>&lt;p&gt;
&#21033;&#29992;&#35821;&#26009;&#24211;&#20027;&#39064;&#20998;&#31867;&#25913;&#36827;&#20027;&#39064;&#29305;&#23450;&#24212;&#29992;&#20013;&#30340;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Improving Retrieval in Theme-specific Applications using a Corpus Topical Taxonomy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04160
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35821;&#26009;&#24211;&#20027;&#39064;&#20998;&#31867;&#25913;&#36827;&#20027;&#39064;&#29305;&#23450;&#24212;&#29992;&#20013;&#26816;&#32034;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#30830;&#23450;&#26597;&#35810;&#21644;&#25991;&#26723;&#30340;&#20013;&#24515;&#20027;&#39064;&#20197;&#21450;&#21033;&#29992;&#20027;&#39064;&#30456;&#20851;&#24615;&#26469;&#34917;&#20805;&#32570;&#22833;&#30340;&#19978;&#19979;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#29486;&#26816;&#32034;&#24050;&#32463;&#20174;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#30340;&#36827;&#27493;&#20013;&#21463;&#30410;&#33391;&#22810;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#22312;&#19987;&#38376;&#39046;&#22495;&#25110;&#34892;&#19994;&#30340;&#20027;&#39064;&#29305;&#23450;&#24212;&#29992;&#20013;&#36890;&#24120;&#21463;&#21040;&#38480;&#21046;&#65292;&#36825;&#26159;&#30001;&#20110;&#29420;&#29305;&#26415;&#35821;&#12289;&#29992;&#25143;&#26597;&#35810;&#30340;&#19981;&#23436;&#25972;&#19978;&#19979;&#25991;&#21644;&#19987;&#38376;&#25628;&#32034;&#24847;&#22270;&#23548;&#33268;&#30340;&#12290;&#20026;&#20102;&#25429;&#25417;&#20027;&#39064;&#29305;&#23450;&#20449;&#24687;&#24182;&#25913;&#36827;&#26816;&#32034;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#35821;&#26009;&#24211;&#20027;&#39064;&#20998;&#31867;&#65292;&#35813;&#20998;&#31867;&#27010;&#36848;&#20102;&#35821;&#26009;&#24211;&#30340;&#28508;&#22312;&#20027;&#39064;&#32467;&#26500;&#65292;&#21516;&#26102;&#21453;&#26144;&#20102;&#29992;&#25143;&#24863;&#20852;&#36259;&#30340;&#26041;&#38754;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;ToTER&#65288;&#22686;&#24378;&#22411;&#20027;&#39064;&#20998;&#31867;&#26816;&#32034;&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20511;&#21161;&#20998;&#31867;&#30340;&#25351;&#23548;&#30830;&#23450;&#26597;&#35810;&#21644;&#25991;&#26723;&#30340;&#20013;&#24515;&#20027;&#39064;&#65292;&#24182;&#21033;&#29992;&#23427;&#20204;&#30340;&#20027;&#39064;&#30456;&#20851;&#24615;&#34917;&#20805;&#32570;&#22833;&#30340;&#19978;&#19979;&#25991;&#12290;&#20316;&#20026;&#19968;&#31181;&#21363;&#25554;&#21363;&#29992;&#30340;&#26694;&#26550;&#65292;ToTER&#21487;&#28789;&#27963;&#24212;&#29992;&#20110;&#22686;&#24378;&#21508;&#31181;&#22522;&#20110;PLM&#30340;&#26816;&#32034;&#22120;&#12290;&#36890;&#36807;&#23545;&#20004;&#20010;&#26696;&#20363;&#36827;&#34892;&#24191;&#27867;&#30340;&#23450;&#37327;&#12289;&#32570;&#22833;&#21644;&#25506;&#32034;&#24615;&#23454;&#39564;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;ToTER&#26694;&#26550;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04160v1 Announce Type: cross  Abstract: Document retrieval has greatly benefited from the advancements of large-scale pre-trained language models (PLMs). However, their effectiveness is often limited in theme-specific applications for specialized areas or industries, due to unique terminologies, incomplete contexts of user queries, and specialized search intents. To capture the theme-specific information and improve retrieval, we propose to use a corpus topical taxonomy, which outlines the latent topic structure of the corpus while reflecting user-interested aspects. We introduce ToTER (Topical Taxonomy Enhanced Retrieval) framework, which identifies the central topics of queries and documents with the guidance of the taxonomy, and exploits their topical relatedness to supplement missing contexts. As a plug-and-play framework, ToTER can be flexibly employed to enhance various PLM-based retrievers. Through extensive quantitative, ablative, and exploratory experiments on two r
&lt;/p&gt;</description></item><item><title>DA-Net &#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#22810;&#28304;&#36328;&#35821;&#35328;&#36801;&#31227;&#23398;&#20064;&#20013;&#20849;&#20139;&#32534;&#30721;&#22120;&#23548;&#33268;&#23398;&#20064;&#22256;&#25200;&#21644;&#35821;&#35328;&#29305;&#23450;&#20998;&#31867;&#22120;&#24615;&#33021;&#19979;&#38477;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.04158</link><description>&lt;p&gt;
DA-Net: &#19968;&#31181;&#29992;&#20110;&#22810;&#28304;&#36328;&#35821;&#35328;&#36801;&#31227;&#23398;&#20064;&#30340;&#35299;&#32806;&#33258;&#36866;&#24212;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
DA-Net: A Disentangled and Adaptive Network for Multi-Source Cross-Lingual Transfer Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04158
&lt;/p&gt;
&lt;p&gt;
DA-Net &#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#22810;&#28304;&#36328;&#35821;&#35328;&#36801;&#31227;&#23398;&#20064;&#20013;&#20849;&#20139;&#32534;&#30721;&#22120;&#23548;&#33268;&#23398;&#20064;&#22256;&#25200;&#21644;&#35821;&#35328;&#29305;&#23450;&#20998;&#31867;&#22120;&#24615;&#33021;&#19979;&#38477;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#28304;&#36328;&#35821;&#35328;&#36801;&#31227;&#23398;&#20064;&#28041;&#21450;&#20174;&#22810;&#20010;&#24050;&#26631;&#35760;&#28304;&#35821;&#35328;&#21521;&#19968;&#20010;&#26410;&#26631;&#35760;&#30446;&#26631;&#35821;&#35328;&#22312;&#35821;&#35328;&#36716;&#31227;&#19979;&#30340;&#20219;&#21153;&#30693;&#35782;&#20256;&#36755;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#20851;&#27880;&#20110;&#21152;&#26435;&#19981;&#21516;&#28304;&#35821;&#35328;&#30340;&#29305;&#23450;&#35821;&#35328;&#20998;&#31867;&#22120;&#29983;&#25104;&#30340;&#39044;&#27979;&#65292;&#36825;&#20123;&#20998;&#31867;&#22120;&#36981;&#24490;&#20849;&#20139;&#32534;&#30721;&#22120;&#12290;&#28982;&#32780;&#65292;&#25152;&#26377;&#28304;&#35821;&#35328;&#20849;&#20139;&#30456;&#21516;&#30340;&#32534;&#30721;&#22120;&#65292;&#36825;&#20010;&#32534;&#30721;&#22120;&#34987;&#25152;&#26377;&#36825;&#20123;&#35821;&#35328;&#26356;&#26032;&#12290;&#25552;&#21462;&#20986;&#30340;&#34920;&#31034;&#19981;&#21487;&#36991;&#20813;&#22320;&#21253;&#21547;&#19981;&#21516;&#28304;&#35821;&#35328;&#30340;&#20449;&#24687;&#65292;&#36825;&#21487;&#33021;&#24178;&#25200;&#35821;&#35328;&#29305;&#23450;&#20998;&#31867;&#22120;&#30340;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#35821;&#35328;&#24046;&#36317;&#65292;&#20351;&#29992;&#28304;&#26631;&#31614;&#35757;&#32451;&#30340;&#35821;&#35328;&#29305;&#23450;&#20998;&#31867;&#22120;&#26080;&#27861;&#20934;&#30830;&#39044;&#27979;&#30446;&#26631;&#35821;&#35328;&#12290;&#36825;&#20004;&#20010;&#20107;&#23454;&#25439;&#23475;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#32806;&#33258;&#36866;&#24212;&#32593;&#32476; (DA-Net)&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04158v1 Announce Type: cross  Abstract: Multi-Source cross-lingual transfer learning deals with the transfer of task knowledge from multiple labelled source languages to an unlabeled target language under the language shift. Existing methods typically focus on weighting the predictions produced by language-specific classifiers of different sources that follow a shared encoder. However, all source languages share the same encoder, which is updated by all these languages. The extracted representations inevitably contain different source languages' information, which may disturb the learning of the language-specific classifiers. Additionally, due to the language gap, language-specific classifiers trained with source labels are unable to make accurate predictions for the target language. Both facts impair the model's performance. To address these challenges, we propose a Disentangled and Adaptive Network (DA-Net). Firstly, we devise a feedback-guided collaborative disentanglemen
&lt;/p&gt;</description></item><item><title>FL-GUARD&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#36127;&#38754;&#32852;&#37030;&#23398;&#20064;&#24773;&#20917;&#19979;&#26816;&#27979;&#21644;&#24674;&#22797;&#65292;&#35299;&#20915;&#20102;&#20808;&#21069;&#35299;&#20915;&#26041;&#26696;&#20013;&#30340;&#39069;&#22806;&#25104;&#26412;&#21644;&#28010;&#36153;&#23398;&#20064;&#36718;&#27425;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.04146</link><description>&lt;p&gt;
FL-GUARD: &#19968;&#20010;&#29992;&#20110;&#36127;&#38754;&#32852;&#37030;&#23398;&#20064;&#36816;&#34892;&#26102;&#26816;&#27979;&#21644;&#24674;&#22797;&#30340;&#20840;&#38754;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
FL-GUARD: A Holistic Framework for Run-Time Detection and Recovery of Negative Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04146
&lt;/p&gt;
&lt;p&gt;
FL-GUARD&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#36127;&#38754;&#32852;&#37030;&#23398;&#20064;&#24773;&#20917;&#19979;&#26816;&#27979;&#21644;&#24674;&#22797;&#65292;&#35299;&#20915;&#20102;&#20808;&#21069;&#35299;&#20915;&#26041;&#26696;&#20013;&#30340;&#39069;&#22806;&#25104;&#26412;&#21644;&#28010;&#36153;&#23398;&#20064;&#36718;&#27425;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#20174;&#20998;&#24067;&#22312;&#22823;&#37327;&#23458;&#25143;&#31471;&#19978;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#27169;&#22411;&#19988;&#19981;&#26292;&#38706;&#25968;&#25454;&#38544;&#31169;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#22312;&#29702;&#24819;&#30340;&#32852;&#37030;&#23398;&#20064;&#20013;&#25928;&#26524;&#26174;&#33879;&#65292;&#20854;&#20013;&#23458;&#25143;&#31471;&#20998;&#20139;&#21516;&#36136;&#30340;&#25968;&#25454;&#20998;&#24067;&#21644;&#23398;&#20064;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#24403;&#32852;&#37030;&#23398;&#20064;&#19981;&#29702;&#24819;&#26102;&#65292;FL&#21487;&#33021;&#26080;&#27861;&#27491;&#24120;&#36816;&#20316;&#65292;&#23548;&#33268;&#36127;&#38754;&#32852;&#37030;&#23398;&#20064;&#65288;NFL&#65289;&#30340;&#19981;&#33391;&#29366;&#24577;&#12290;&#35768;&#22810;&#30740;&#31350;&#23581;&#35797;&#35299;&#20915;NFL&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#35201;&#20040;&#65288;1&#65289;&#39044;&#20808;&#38450;&#27490;&#25972;&#20010;&#23398;&#20064;&#29983;&#21629;&#21608;&#26399;&#20013;&#30340;NFL&#65292;&#35201;&#20040;&#65288;2&#65289;&#22312;&#22823;&#37327;&#23398;&#20064;&#36718;&#27425;&#20043;&#21518;&#35299;&#20915;NFL&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#35201;&#20040;&#65288;1&#65289;&#22312;FL&#27809;&#26377;&#36825;&#20123;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#26080;&#24046;&#21035;&#22686;&#21152;&#39069;&#22806;&#25104;&#26412;&#65292;&#35201;&#20040;&#65288;2&#65289;&#28010;&#36153;&#22823;&#37327;&#23398;&#20064;&#36718;&#27425;&#12290;&#21478;&#22806;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#27809;&#26377;&#32771;&#34385;&#21487;&#33021;&#19981;&#24895;&#24847;/&#26080;&#27861;&#36981;&#24490;&#25552;&#20986;&#30340;NFL&#35299;&#20915;&#26041;&#26696;&#30340;&#23458;&#25143;&#31471;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04146v1 Announce Type: cross  Abstract: Federated learning (FL) is a promising approach for learning a model from data distributed on massive clients without exposing data privacy. It works effectively in the ideal federation where clients share homogeneous data distribution and learning behavior. However, FL may fail to function appropriately when the federation is not ideal, amid an unhealthy state called Negative Federated Learning (NFL), in which most clients gain no benefit from participating in FL. Many studies have tried to address NFL. However, their solutions either (1) predetermine to prevent NFL in the entire learning life-cycle or (2) tackle NFL in the aftermath of numerous learning rounds. Thus, they either (1) indiscriminately incur extra costs even if FL can perform well without such costs or (2) waste numerous learning rounds. Additionally, none of the previous work takes into account the clients who may be unwilling/unable to follow the proposed NFL solution
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#22686;&#24378;&#22270;&#24418;&#21040;&#22270;&#24418;&#35760;&#24518;&#20132;&#20114;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#23569;&#26679;&#26412;&#31867;&#22686;&#37327;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.04140</link><description>&lt;p&gt;
&#23569;&#26679;&#26412;&#25345;&#32493;&#23398;&#20064;&#30340;&#23545;&#27604;&#22686;&#24378;&#22270;&#24418;&#21040;&#22270;&#24418;&#35760;&#24518;&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;
Contrastive Augmented Graph2Graph Memory Interaction for Few Shot Continual Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04140
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#22686;&#24378;&#22270;&#24418;&#21040;&#22270;&#24418;&#35760;&#24518;&#20132;&#20114;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#23569;&#26679;&#26412;&#31867;&#22686;&#37327;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23569;&#26679;&#26412;&#31867;&#22686;&#37327;&#23398;&#20064;&#65288;FSCIL&#65289;&#36817;&#24180;&#26469;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#22312;&#35299;&#20915;&#25345;&#32493;&#21040;&#36798;&#30340;&#31867;&#21035;&#26041;&#38754;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#23427;&#38754;&#20020;&#30528;&#39069;&#22806;&#30340;&#25361;&#25112;&#12290;&#26032;&#20250;&#35805;&#20013;&#26679;&#26412;&#30340;&#31232;&#32570;&#24615;&#21152;&#21095;&#20102;&#36807;&#25311;&#21512;&#65292;&#23548;&#33268;&#26032;&#26087;&#31867;&#21035;&#30340;&#36755;&#20986;&#29305;&#24449;&#19981;&#20860;&#23481;&#65292;&#20174;&#32780;&#21152;&#21095;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#19968;&#31181;&#26222;&#36941;&#30340;&#31574;&#30053;&#28041;&#21450;&#36890;&#36807;&#26174;&#24335;&#35760;&#24518;&#65288;EM&#65289;&#20943;&#36731;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;EM&#30001;&#31867;&#21407;&#22411;&#32452;&#25104;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#22522;&#20110;EM&#30340;&#26041;&#27861;&#36890;&#36807;&#23545;&#36755;&#20837;&#19982;EM&#20013;&#23384;&#20648;&#30340;&#21407;&#22411;&#23545;&#24212;&#30340;&#29305;&#24449;&#25191;&#34892;&#21521;&#37327;&#21040;&#21521;&#37327;&#65288;V2V&#65289;&#20132;&#20114;&#20840;&#23616;&#26816;&#32034;&#35760;&#24518;&#65292;&#32780;&#24573;&#30053;&#20102;&#26412;&#22320;&#29305;&#24449;&#30340;&#20960;&#20309;&#32467;&#26500;&#12290;&#36825;&#22952;&#30861;&#20102;&#20934;&#30830;&#24314;&#27169;&#23427;&#20204;&#30340;&#20301;&#32622;&#20851;&#31995;&#12290;&#20026;&#20102;&#23558;&#26412;&#22320;&#20960;&#20309;&#32467;&#26500;&#30340;&#20449;&#24687;&#34701;&#20837;&#20854;&#20013;&#65292;&#25105;&#20204;&#23558;V2V&#20132;&#20114;&#25193;&#23637;&#21040;&#22270;&#24418;&#21040;&#22270;&#24418;&#65288;G2G&#65289;&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04140v1 Announce Type: new  Abstract: Few-Shot Class-Incremental Learning (FSCIL) has gained considerable attention in recent years for its pivotal role in addressing continuously arriving classes. However, it encounters additional challenges. The scarcity of samples in new sessions intensifies overfitting, causing incompatibility between the output features of new and old classes, thereby escalating catastrophic forgetting. A prevalent strategy involves mitigating catastrophic forgetting through the Explicit Memory (EM), which comprise of class prototypes. However, current EM-based methods retrieves memory globally by performing Vector-to-Vector (V2V) interaction between features corresponding to the input and prototypes stored in EM, neglecting the geometric structure of local features. This hinders the accurate modeling of their positional relationships. To incorporate information of local geometric structure, we extend the V2V interaction to Graph-to-Graph (G2G) interact
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;HSMM&#30340;&#21644;&#22768;&#20998;&#26512;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#21644;&#24358;&#36136;&#37327;&#27169;&#26495;&#65292;&#24182;&#23637;&#31034;&#20102;&#19981;&#38656;&#35201;&#26114;&#36149;&#26631;&#35760;&#25968;&#25454;&#25110;&#35268;&#21017;&#38416;&#36848;&#30340;&#20248;&#28857;&#12290;</title><link>https://arxiv.org/abs/2403.04135</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;HSMM&#21644;&#20195;&#30721;&#36136;&#37327;&#27169;&#26495;&#30340;&#21644;&#22768;&#20998;&#26512;&#26080;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Learning of Harmonic Analysis Based on Neural HSMM with Code Quality Templates
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04135
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;HSMM&#30340;&#21644;&#22768;&#20998;&#26512;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#21644;&#24358;&#36136;&#37327;&#27169;&#26495;&#65292;&#24182;&#23637;&#31034;&#20102;&#19981;&#38656;&#35201;&#26114;&#36149;&#26631;&#35760;&#25968;&#25454;&#25110;&#35268;&#21017;&#38416;&#36848;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38544;&#34255;&#21322;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#65288;HSMM&#65289;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#21644;&#22768;&#20998;&#26512;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#21644;&#24358;&#36136;&#37327;&#27169;&#26495;&#65292;&#25351;&#23450;&#20102;&#22312;&#32473;&#23450;&#26681;&#38899;&#21644;&#21644;&#24358;&#36136;&#37327;&#30340;&#24773;&#20917;&#19979;&#38899;&#39640;&#31867;&#21035;&#21457;&#23556;&#30340;&#27010;&#29575;&#12290;&#32452;&#25104;HSMM&#30340;&#20854;&#20182;&#27010;&#29575;&#20998;&#24067;&#26159;&#36890;&#36807;&#26080;&#30417;&#30563;&#23398;&#20064;&#33258;&#21160;&#23398;&#20064;&#30340;&#65292;&#36825;&#22312;&#29616;&#26377;&#30740;&#31350;&#20013;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#25552;&#20986;&#27169;&#22411;&#30340;&#21644;&#22768;&#20998;&#26512;&#32467;&#26524;&#26159;&#20351;&#29992;&#29616;&#26377;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#35780;&#20272;&#30340;&#12290;&#34429;&#28982;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#23578;&#26410;&#20687;&#20351;&#29992;&#30417;&#30563;&#23398;&#20064;&#21644;&#22797;&#26434;&#35268;&#21017;&#35774;&#35745;&#30340;&#29616;&#26377;&#27169;&#22411;&#34920;&#29616;&#24471;&#37027;&#20040;&#22909;&#65292;&#20294;&#23427;&#20855;&#26377;&#19981;&#38656;&#35201;&#26114;&#36149;&#26631;&#35760;&#25968;&#25454;&#25110;&#35268;&#21017;&#38416;&#36848;&#30340;&#20248;&#28857;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#27809;&#26377;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#22522;&#20110;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#30340;&#36716;&#25442;&#27010;&#29575;&#26469;&#35782;&#21035;&#20027;&#38899;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04135v1 Announce Type: new  Abstract: This paper presents a method of unsupervised learning of harmonic analysis based on a hidden semi-Markov model (HSMM). We introduce the chord quality templates, which specify the probability of pitch class emissions given a root note and a chord quality. Other probability distributions that comprise the HSMM are automatically learned via unsupervised learning, which has been a challenge in existing research. The results of the harmonic analysis of the proposed model were evaluated using existing labeled data. While our proposed method has yet to perform as well as existing models that used supervised learning and complex rule design, it has the advantage of not requiring expensive labeled data or rule elaboration. Furthermore, we also show how to recognize the tonic without prior knowledge, based on the transition probabilities of the Markov model.
&lt;/p&gt;</description></item><item><title>Chatbot Arena&#26159;&#19968;&#20010;&#24320;&#25918;&#24179;&#21488;&#65292;&#37319;&#29992;&#20004;&#20004;&#27604;&#36739;&#30340;&#26041;&#24335;&#36890;&#36807;&#20247;&#21253;&#21033;&#29992;&#20154;&#31867;&#20559;&#22909;&#35780;&#20272;LLMs&#12290;&#30740;&#31350;&#34920;&#26126;&#20247;&#21253;&#38382;&#39064;&#22810;&#26679;&#19988;&#20855;&#26377;&#21306;&#20998;&#24615;&#65292;&#24182;&#19988;&#20154;&#31867;&#25237;&#31080;&#19982;&#19987;&#23478;&#35780;&#32423;&#32773;&#30340;&#25237;&#31080;&#22522;&#26412;&#19968;&#33268;&#12290;</title><link>https://arxiv.org/abs/2403.04132</link><description>&lt;p&gt;
Chatbot Arena&#65306;&#19968;&#20010;&#36890;&#36807;&#20154;&#31867;&#20559;&#22909;&#35780;&#20272;LLM&#30340;&#24320;&#25918;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04132
&lt;/p&gt;
&lt;p&gt;
Chatbot Arena&#26159;&#19968;&#20010;&#24320;&#25918;&#24179;&#21488;&#65292;&#37319;&#29992;&#20004;&#20004;&#27604;&#36739;&#30340;&#26041;&#24335;&#36890;&#36807;&#20247;&#21253;&#21033;&#29992;&#20154;&#31867;&#20559;&#22909;&#35780;&#20272;LLMs&#12290;&#30740;&#31350;&#34920;&#26126;&#20247;&#21253;&#38382;&#39064;&#22810;&#26679;&#19988;&#20855;&#26377;&#21306;&#20998;&#24615;&#65292;&#24182;&#19988;&#20154;&#31867;&#25237;&#31080;&#19982;&#19987;&#23478;&#35780;&#32423;&#32773;&#30340;&#25237;&#31080;&#22522;&#26412;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#35299;&#38145;&#20102;&#26032;&#30340;&#33021;&#21147;&#21644;&#24212;&#29992;&#65307;&#28982;&#32780;&#65292;&#35780;&#20272;&#20854;&#19982;&#20154;&#31867;&#20559;&#22909;&#30340;&#19968;&#33268;&#24615;&#20173;&#28982;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Chatbot Arena&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#20154;&#31867;&#20559;&#22909;&#35780;&#20272;LLMs&#30340;&#24320;&#25918;&#24179;&#21488;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#20102;&#19968;&#31181;&#20004;&#20004;&#27604;&#36739;&#30340;&#26041;&#24335;&#65292;&#24182;&#36890;&#36807;&#20247;&#21253;&#21033;&#29992;&#26469;&#33258;&#19981;&#21516;&#29992;&#25143;&#32676;&#20307;&#30340;&#36755;&#20837;&#12290;&#35813;&#24179;&#21488;&#24050;&#32463;&#36816;&#33829;&#20102;&#20960;&#20010;&#26376;&#65292;&#33719;&#24471;&#20102;&#36229;&#36807;24&#19975;&#20010;&#25237;&#31080;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#35813;&#24179;&#21488;&#65292;&#20998;&#26512;&#20102;&#25105;&#20204;&#36804;&#20170;&#25910;&#38598;&#30340;&#25968;&#25454;&#65292;&#24182;&#35299;&#37322;&#20102;&#25105;&#20204;&#27491;&#22312;&#20351;&#29992;&#30340;&#32463;&#36807;&#39564;&#35777;&#30340;&#32479;&#35745;&#26041;&#27861;&#65292;&#20197;&#20415;&#23545;&#27169;&#22411;&#36827;&#34892;&#39640;&#25928;&#20934;&#30830;&#30340;&#35780;&#20272;&#21644;&#25490;&#21517;&#12290;&#25105;&#20204;&#30830;&#35748;&#20247;&#21253;&#38382;&#39064;&#36275;&#22815;&#22810;&#26679;&#21270;&#21644;&#21306;&#20998;&#21270;&#65292;&#24182;&#19988;&#20247;&#21253;&#20154;&#31867;&#25237;&#31080;&#19982;&#19987;&#23478;&#35780;&#32423;&#32773;&#30340;&#25237;&#31080;&#22522;&#26412;&#19968;&#33268;&#12290;&#36825;&#20123;&#20998;&#26512;&#20849;&#21516;&#20026;&#24179;&#21488;&#30340;&#21487;&#20449;&#24230;&#22880;&#23450;&#20102;&#22362;&#23454;&#30340;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04132v1 Announce Type: new  Abstract: Large Language Models (LLMs) have unlocked new capabilities and applications; however, evaluating the alignment with human preferences still poses significant challenges. To address this issue, we introduce Chatbot Arena, an open platform for evaluating LLMs based on human preferences. Our methodology employs a pairwise comparison approach and leverages input from a diverse user base through crowdsourcing. The platform has been operational for several months, amassing over 240K votes. This paper describes the platform, analyzes the data we have collected so far, and explains the tried-and-true statistical methods we are using for efficient and accurate evaluation and ranking of models. We confirm that the crowdsourced questions are sufficiently diverse and discriminating and that the crowdsourced human votes are in good agreement with those of expert raters. These analyses collectively establish a robust foundation for the credibility of
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25581;&#31034;&#20102;DP&#35757;&#32451;&#27169;&#22411;&#30340;&#25439;&#22833;&#26223;&#35266;&#30340;&#24179;&#22374;&#24615;&#22312;&#38544;&#31169;&#21644;&#27867;&#21270;&#20043;&#38388;&#30340;&#26435;&#34913;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24378;&#21046;&#23454;&#26045;&#36866;&#24403;&#30340;&#26435;&#37325;&#24179;&#22374;&#21270;&#26469;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21516;&#26102;&#20445;&#30041;&#31454;&#20105;&#24615;&#30340;&#38544;&#31169;&#20445;&#25252;&#12290;</title><link>https://arxiv.org/abs/2403.04124</link><description>&lt;p&gt;
&#36890;&#36807;&#24179;&#22374;&#24615;&#23454;&#29616;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#31169;&#20445;&#25252;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Privacy-preserving Fine-tuning of Large Language Models through Flatness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04124
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25581;&#31034;&#20102;DP&#35757;&#32451;&#27169;&#22411;&#30340;&#25439;&#22833;&#26223;&#35266;&#30340;&#24179;&#22374;&#24615;&#22312;&#38544;&#31169;&#21644;&#27867;&#21270;&#20043;&#38388;&#30340;&#26435;&#34913;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24378;&#21046;&#23454;&#26045;&#36866;&#24403;&#30340;&#26435;&#37325;&#24179;&#22374;&#21270;&#26469;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21516;&#26102;&#20445;&#30041;&#31454;&#20105;&#24615;&#30340;&#38544;&#31169;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#38543;&#30528;&#35832;&#22914;ChatGPT&#20043;&#31867;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#65292;&#19982;&#20351;&#29992;LLMs&#30456;&#20851;&#30340;&#38544;&#31169;&#38382;&#39064;&#26085;&#30410;&#22686;&#21152;&#12290;&#29616;&#26377;&#24037;&#20316;&#25506;&#32034;&#20102;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#25216;&#26415;&#26469;&#20943;&#36731;&#20854;&#38544;&#31169;&#39118;&#38505;&#30340;&#26041;&#27861;&#65292;&#20294;&#20197;&#27867;&#21270;&#38477;&#32423;&#20026;&#20195;&#20215;&#12290;&#25105;&#20204;&#30340;&#35770;&#25991;&#25581;&#31034;&#20102;DP&#35757;&#32451;&#27169;&#22411;&#30340;&#25439;&#22833;&#26223;&#35266;&#30340;&#24179;&#22374;&#24615;&#22312;&#23427;&#20204;&#30340;&#38544;&#31169;&#21644;&#27867;&#21270;&#20043;&#38388;&#30340;&#26435;&#34913;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#26694;&#26550;&#26469;&#24378;&#21046;&#23454;&#26045;&#36866;&#24403;&#30340;&#26435;&#37325;&#24179;&#22374;&#21270;&#65292;&#36825;&#22823;&#22823;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#20855;&#26377;&#31454;&#20105;&#24615;&#30340;&#38544;&#31169;&#20445;&#25252;&#12290;&#23427;&#20174;&#19977;&#20010;&#31895;&#21040;&#31934;&#30340;&#23618;&#27425;&#36827;&#34892;&#21019;&#26032;&#65292;&#21253;&#25324;&#23545;&#23618;&#20869;&#27169;&#22411;&#26435;&#37325;&#36827;&#34892;&#25200;&#21160;&#24863;&#30693;&#30340;&#26368;&#23567;-&#26368;&#22823;&#20248;&#21270;&#12289;&#36328;&#23618;&#26435;&#37325;&#36827;&#34892;&#24179;&#22374;&#24341;&#23548;&#30340;&#31232;&#30095;&#21069;&#32512;&#24494;&#35843;&#65292;&#20197;&#21450;&#22312;DP&#21644;&#38750;DP&#26435;&#37325;&#21103;&#26412;&#20043;&#38388;&#36827;&#34892;&#26435;&#37325;&#30693;&#35782;&#33976;&#39311;&#12290;&#20840;&#38754;&#30340;&#40657;&#30418;&#21644;&#30333;&#30418;&#22330;&#26223;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04124v1 Announce Type: new  Abstract: The privacy concerns associated with the use of Large Language Models (LLMs) have grown recently with the development of LLMs such as ChatGPT. Differential Privacy (DP) techniques are explored in existing work to mitigate their privacy risks at the cost of generalization degradation. Our paper reveals that the flatness of DP-trained models' loss landscape plays an essential role in the trade-off between their privacy and generalization. We further propose a holistic framework to enforce appropriate weight flatness, which substantially improves model generalization with competitive privacy preservation. It innovates from three coarse-to-grained levels, including perturbation-aware min-max optimization on model weights within a layer, flatness-guided sparse prefix-tuning on weights across layers, and weight knowledge distillation between DP \&amp; non-DP weights copies. Comprehensive experiments of both black-box and white-box scenarios are co
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32570;&#20047;&#33258;&#25105;&#25209;&#35780;&#33021;&#21147;&#65292;&#26080;&#27861;&#20687;&#20154;&#31867;&#19968;&#26679;&#32416;&#27491;&#38169;&#35823;&#12290;</title><link>https://arxiv.org/abs/2403.04121</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36827;&#34892;&#25512;&#29702;&#21644;&#35268;&#21010;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models Reason and Plan?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04121
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32570;&#20047;&#33258;&#25105;&#25209;&#35780;&#33021;&#21147;&#65292;&#26080;&#27861;&#20687;&#20154;&#31867;&#19968;&#26679;&#32416;&#27491;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#20154;&#31867;&#26377;&#26102;&#20505;&#34920;&#29616;&#20986;&#33021;&#22815;&#36890;&#36807;&#33258;&#25105;&#25209;&#35780;&#32416;&#27491;&#33258;&#24049;&#38169;&#35823;&#29468;&#27979;&#30340;&#33021;&#21147;&#65292;&#20294;&#20284;&#20046;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#27809;&#26377;&#20381;&#25454;&#25903;&#25345;&#36825;&#19968;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04121v1 Announce Type: new  Abstract: While humans sometimes do show the capability of correcting their own erroneous guesses with self-critiquing, there seems to be no basis for that assumption in the case of LLMs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;DNAct&#26694;&#26550;&#65292;&#32467;&#21512;&#31070;&#32463;&#28210;&#26579;&#21644;&#25193;&#25955;&#35757;&#32451;&#65292;&#23454;&#29616;&#22312;&#21160;&#20316;&#24207;&#21015;&#31354;&#38388;&#20013;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#65292;&#21487;&#24212;&#29992;&#20110;&#25361;&#25112;&#24615;&#26426;&#22120;&#20154;&#20219;&#21153;&#65292;&#21516;&#26102;&#36890;&#36807;&#25193;&#25955;&#36807;&#31243;&#23454;&#29616;&#22810;&#20219;&#21153;&#21160;&#20316;&#24207;&#21015;&#30340;&#37325;&#26500;&#12290;</title><link>https://arxiv.org/abs/2403.04115</link><description>&lt;p&gt;
DNAct&#65306;&#25193;&#25955;&#24341;&#23548;&#30340;&#22810;&#20219;&#21153;3D&#31574;&#30053;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DNAct: Diffusion Guided Multi-Task 3D Policy Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04115
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;DNAct&#26694;&#26550;&#65292;&#32467;&#21512;&#31070;&#32463;&#28210;&#26579;&#21644;&#25193;&#25955;&#35757;&#32451;&#65292;&#23454;&#29616;&#22312;&#21160;&#20316;&#24207;&#21015;&#31354;&#38388;&#20013;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#65292;&#21487;&#24212;&#29992;&#20110;&#25361;&#25112;&#24615;&#26426;&#22120;&#20154;&#20219;&#21153;&#65292;&#21516;&#26102;&#36890;&#36807;&#25193;&#25955;&#36807;&#31243;&#23454;&#29616;&#22810;&#20219;&#21153;&#21160;&#20316;&#24207;&#21015;&#30340;&#37325;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;DNAct&#65292;&#36825;&#26159;&#19968;&#20010;&#20197;&#35821;&#35328;&#20026;&#26465;&#20214;&#30340;&#22810;&#20219;&#21153;&#31574;&#30053;&#26694;&#26550;&#65292;&#23427;&#25972;&#21512;&#20102;&#31070;&#32463;&#28210;&#26579;&#39044;&#35757;&#32451;&#21644;&#25193;&#25955;&#35757;&#32451;&#65292;&#20197;&#22312;&#21160;&#20316;&#24207;&#21015;&#31354;&#38388;&#20013;&#23454;&#29616;&#22810;&#27169;&#24577;&#23398;&#20064;&#12290;DNAct&#30340;&#39044;&#35757;&#32451;&#38454;&#27573;&#21033;&#29992;&#31070;&#32463;&#28210;&#26579;&#20174;&#35832;&#22914;Stable Diffusion&#20043;&#31867;&#30340;&#22522;&#30784;&#27169;&#22411;&#20013;&#25552;&#21462;2D&#35821;&#20041;&#29305;&#24449;&#21040;3D&#31354;&#38388;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#20851;&#20110;&#22330;&#26223;&#30340;&#20840;&#38754;&#35821;&#20041;&#29702;&#35299;&#12290;&#36825;&#20351;&#24471;&#21487;&#20197;&#24212;&#29992;&#20110;&#38656;&#35201;&#20016;&#23500;&#30340;3D&#35821;&#20041;&#21644;&#20934;&#30830;&#20960;&#20309;&#30340;&#25361;&#25112;&#24615;&#26426;&#22120;&#20154;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#35757;&#32451;&#26469;&#23398;&#20064;&#21253;&#21547;&#22810;&#20219;&#21153;&#28436;&#31034;&#20013;&#22266;&#26377;&#22810;&#27169;&#24577;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#29305;&#24449;&#30340;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#25193;&#25955;&#36807;&#31243;&#20174;&#19981;&#21516;&#20219;&#21153;&#30340;&#21160;&#20316;&#24207;&#21015;&#37325;&#26500;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#21306;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04115v1 Announce Type: cross  Abstract: This paper presents DNAct, a language-conditioned multi-task policy framework that integrates neural rendering pre-training and diffusion training to enforce multi-modality learning in action sequence spaces. To learn a generalizable multi-task policy with few demonstrations, the pre-training phase of DNAct leverages neural rendering to distill 2D semantic features from foundation models such as Stable Diffusion to a 3D space, which provides a comprehensive semantic understanding regarding the scene. Consequently, it allows various applications to challenging robotic tasks requiring rich 3D semantics and accurate geometry. Furthermore, we introduce a novel approach utilizing diffusion training to learn a vision and language feature that encapsulates the inherent multi-modality in the multi-task demonstrations. By reconstructing the action sequences from different tasks via the diffusion process, the model is capable of distinguishing d
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#26234;&#33021;&#22312;&#29983;&#29289;&#23398;&#31185;&#23398;&#30740;&#31350;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#20195;&#34920;&#20102;&#20256;&#32479;&#26041;&#27861;&#30340;&#37325;&#22823;&#20559;&#31163;&#65292;&#36890;&#36807;&#29616;&#20195;&#21746;&#23398;&#29702;&#35770;&#21644;&#35748;&#35782;&#35770;&#21407;&#21017;&#25351;&#23548;&#65292;&#21487;&#20197;&#25512;&#21160;&#35774;&#35745;&#21644;&#24212;&#29992;ML&#31995;&#32479;&#26469;&#24314;&#27169;&#29983;&#29289;&#29616;&#35937;&#20197;&#20419;&#36827;&#31185;&#23398;&#30693;&#35782;&#30340;&#36827;&#27493;&#12290;</title><link>https://arxiv.org/abs/2403.04106</link><description>&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#26102;&#20195;&#29702;&#35299;&#29983;&#29289;&#23398;
&lt;/p&gt;
&lt;p&gt;
Understanding Biology in the Age of Artificial Intelligence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04106
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#22312;&#29983;&#29289;&#23398;&#31185;&#23398;&#30740;&#31350;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#20195;&#34920;&#20102;&#20256;&#32479;&#26041;&#27861;&#30340;&#37325;&#22823;&#20559;&#31163;&#65292;&#36890;&#36807;&#29616;&#20195;&#21746;&#23398;&#29702;&#35770;&#21644;&#35748;&#35782;&#35770;&#21407;&#21017;&#25351;&#23548;&#65292;&#21487;&#20197;&#25512;&#21160;&#35774;&#35745;&#21644;&#24212;&#29992;ML&#31995;&#32479;&#26469;&#24314;&#27169;&#29983;&#29289;&#29616;&#35937;&#20197;&#20419;&#36827;&#31185;&#23398;&#30693;&#35782;&#30340;&#36827;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#29983;&#21629;&#31185;&#23398;&#30740;&#31350;&#36234;&#26469;&#36234;&#20381;&#36182;&#20110;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#26469;&#24314;&#27169;&#29983;&#29289;&#31995;&#32479;&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#12290;&#23613;&#31649;ML&#26080;&#30097;&#23545;&#20110;&#35782;&#21035;&#22823;&#35268;&#27169;&#12289;&#22797;&#26434;&#25968;&#25454;&#38598;&#20013;&#30340;&#27169;&#24335;&#38750;&#24120;&#26377;&#29992;&#65292;&#20294;&#23427;&#22312;&#29983;&#29289;&#31185;&#23398;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#20195;&#34920;&#20102;&#23545;&#31185;&#23398;&#25506;&#31350;&#20256;&#32479;&#26041;&#27861;&#30340;&#37325;&#22823;&#20559;&#31163;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#27169;&#22411;&#19982;&#29983;&#29289;&#23398;&#31185;&#23398;&#29702;&#35299;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#26159;&#19968;&#20010;&#20855;&#26377;&#37325;&#35201;&#26410;&#26469;&#31185;&#23398;&#30740;&#31350;&#24847;&#20041;&#30340;&#20027;&#39064;&#65292;&#28982;&#32780;&#36825;&#26159;&#19968;&#20010;&#40092;&#20026;&#20154;&#30693;&#30340;&#20027;&#39064;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20174;&#35748;&#35782;&#35770;&#24037;&#20855;&#21253;&#20013;&#27762;&#21462;&#28789;&#24863;&#65292;&#23558;&#26368;&#36817;&#22312;&#29983;&#29289;&#31185;&#23398;&#20013;&#24212;&#29992;ML&#30340;&#24773;&#20917;&#32622;&#20110;&#29616;&#20195;&#21746;&#23398;&#29702;&#35770;&#30340;&#29702;&#35299;&#19979;&#26469;&#36827;&#34892;&#19978;&#19979;&#25991;&#21270;&#65292;&#30830;&#23450;&#21487;&#25351;&#23548;&#35774;&#35745;&#21644;&#24212;&#29992;ML&#31995;&#32479;&#26469;&#24314;&#27169;&#29983;&#29289;&#29616;&#35937;&#24182;&#25512;&#36827;&#31185;&#23398;&#30693;&#35782;&#30340;&#19968;&#33324;&#21407;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04106v1 Announce Type: new  Abstract: Modern life sciences research is increasingly relying on artificial intelligence approaches to model biological systems, primarily centered around the use of machine learning (ML) models. Although ML is undeniably useful for identifying patterns in large, complex data sets, its widespread application in biological sciences represents a significant deviation from traditional methods of scientific inquiry. As such, the interplay between these models and scientific understanding in biology is a topic with important implications for the future of scientific research, yet it is a subject that has received little attention. Here, we draw from an epistemological toolkit to contextualize recent applications of ML in biological sciences under modern philosophical theories of understanding, identifying general principles that can guide the design and application of ML systems to model biological phenomena and advance scientific knowledge. We propo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#27010;&#36848;&#20102;&#19987;&#21033;&#39046;&#22495;&#30340;&#20219;&#21153;&#21644;&#26041;&#27861;&#65292;&#24378;&#35843;&#20102;&#35821;&#35328;&#22788;&#29702;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25506;&#35752;&#20102;&#36817;&#26399;&#20986;&#29616;&#30340;&#36890;&#29992;&#29983;&#25104;&#26041;&#27861;&#22312;&#19987;&#21033;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.04105</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#25506;&#32034;&#19987;&#21033;&#39046;&#22495;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence Exploring the Patent Field
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04105
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#27010;&#36848;&#20102;&#19987;&#21033;&#39046;&#22495;&#30340;&#20219;&#21153;&#21644;&#26041;&#27861;&#65292;&#24378;&#35843;&#20102;&#35821;&#35328;&#22788;&#29702;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25506;&#35752;&#20102;&#36817;&#26399;&#20986;&#29616;&#30340;&#36890;&#29992;&#29983;&#25104;&#26041;&#27861;&#22312;&#19987;&#21033;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04105v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#30340; &#25688;&#35201;&#65306;&#20808;&#36827;&#30340;&#35821;&#35328;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#25215;&#35834;&#22312;&#20197;&#21069;&#24191;&#27867;&#20381;&#36182;&#25163;&#24037;&#25805;&#20316;&#30340;&#19987;&#21033;&#21644;&#25216;&#26415;&#30693;&#35782;&#31649;&#29702;&#39046;&#22495;&#24102;&#26469;&#24040;&#22823;&#30340;&#25928;&#29575;&#25913;&#36827;&#12290;&#36825;&#20010;&#39046;&#22495;&#23637;&#31034;&#20102;&#22823;&#35268;&#27169;&#32780;&#22797;&#26434;&#30340;&#25968;&#25454;&#65292;&#20855;&#26377;&#38750;&#24120;&#20934;&#30830;&#30340;&#20869;&#23481;&#21644;&#35821;&#35328;&#34920;&#36798;&#36825;&#20123;&#20869;&#23481;&#12290;&#29305;&#21035;&#26159;&#65292;&#19987;&#21033;&#25991;&#26412;&#22312;&#21508;&#20010;&#26041;&#38754;&#21487;&#33021;&#19982;&#24179;&#20961;&#30340;&#25991;&#26412;&#26377;&#25152;&#19981;&#21516;&#65292;&#36825;&#24102;&#26469;&#20102;&#37325;&#22823;&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;&#12290;&#26412;&#25991;&#31995;&#32479;&#27010;&#36848;&#20102;&#19982;&#19987;&#21033;&#26377;&#20851;&#30340;&#20219;&#21153;&#21644;&#27969;&#34892;&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#20851;&#27880;&#19981;&#26029;&#28436;&#21464;&#21644;&#26377;&#21069;&#36884;&#30340;&#25216;&#26415;&#12290;&#35821;&#35328;&#22788;&#29702;&#65292;&#23588;&#20854;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#21450;&#26368;&#36817;&#25512;&#21160;&#26222;&#36890;&#29983;&#25104;&#26041;&#27861;&#30340;&#25552;&#21319;&#65292;&#26377;&#26395;&#25104;&#20026;&#19987;&#21033;&#39046;&#22495;&#30340;&#21464;&#38761;&#32773;&#12290;&#19987;&#21033;&#25991;&#29486;&#20197;&#21450;&#22260;&#32469;&#19987;&#21033;&#30340;&#22522;&#20110;&#20107;&#23454;&#30340;&#35770;&#35777;&#31243;&#24207;&#20284;&#20046;&#20960;&#20046;&#26159;&#19968;&#20010;&#29702;&#24819;&#30340;&#20351;&#29992;&#26696;&#20363;&#12290;&#28982;&#32780;&#65292;&#19987;&#21033;&#28041;&#21450;&#35768;&#22810;&#29616;&#26377;&#27169;&#22411;&#24456;&#38590;&#22788;&#29702;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04105v1 Announce Type: new  Abstract: Advanced language-processing and machine-learning techniques promise massive efficiency improvements in the previously widely manual field of patent and technical knowledge management. This field presents large-scale and complex data with very precise contents and language representation of those contents. Particularly, patent texts can differ from mundane texts in various aspects, which entails significant opportunities and challenges. This paper presents a systematic overview of patent-related tasks and popular methodologies with a special focus on evolving and promising techniques. Language processing and particularly large language models as well as the recent boost of general generative methods promise to become game changers in the patent field. The patent literature and the fact-based argumentative procedures around patents appear almost as an ideal use case. However, patents entail a number of difficulties with which existing mod
&lt;/p&gt;</description></item><item><title>&#35813;&#39033;&#30446;&#26088;&#22312;&#24320;&#21457;&#35745;&#31639;&#24037;&#20855;&#65292;&#24110;&#21161;&#35774;&#35745;&#20855;&#26377;&#19981;&#21516;&#35748;&#30693;&#29305;&#24615;&#30340;&#23383;&#20307;&#65292;&#25552;&#39640;&#22312;&#32447;&#24191;&#21578;&#28857;&#20987;&#29575;&#65292;&#25913;&#21892;&#20799;&#31461;&#20070;&#31821;&#30340;&#38405;&#35835;&#27700;&#24179;&#65292;&#35753;&#35829;&#35835;&#32773;&#21487;&#20197;&#21019;&#36896;&#20010;&#24615;&#21270;&#23383;&#20307;&#65292;&#24182;&#27934;&#23519;&#23186;&#20307;&#25991;&#26412;&#20869;&#23481;&#30340;&#23458;&#25143;&#21453;&#24212;&#12290;</title><link>https://arxiv.org/abs/2403.04087</link><description>&lt;p&gt;
&#35748;&#30693;&#31867;&#22411;&#39033;&#30446; - &#23558;&#25490;&#29256;&#26144;&#23556;&#21040;&#35748;&#30693;
&lt;/p&gt;
&lt;p&gt;
The Cognitive Type Project -- Mapping Typography to Cognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04087
&lt;/p&gt;
&lt;p&gt;
&#35813;&#39033;&#30446;&#26088;&#22312;&#24320;&#21457;&#35745;&#31639;&#24037;&#20855;&#65292;&#24110;&#21161;&#35774;&#35745;&#20855;&#26377;&#19981;&#21516;&#35748;&#30693;&#29305;&#24615;&#30340;&#23383;&#20307;&#65292;&#25552;&#39640;&#22312;&#32447;&#24191;&#21578;&#28857;&#20987;&#29575;&#65292;&#25913;&#21892;&#20799;&#31461;&#20070;&#31821;&#30340;&#38405;&#35835;&#27700;&#24179;&#65292;&#35753;&#35829;&#35835;&#32773;&#21487;&#20197;&#21019;&#36896;&#20010;&#24615;&#21270;&#23383;&#20307;&#65292;&#24182;&#27934;&#23519;&#23186;&#20307;&#25991;&#26412;&#20869;&#23481;&#30340;&#23458;&#25143;&#21453;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04087v1 &#20844;&#21578;&#31867;&#22411;: &#26032; &#25277;&#35937;: &#35748;&#30693;&#31867;&#22411;&#39033;&#30446;&#33268;&#21147;&#20110;&#24320;&#21457;&#35745;&#31639;&#24037;&#20855;&#65292;&#20197;&#23454;&#29616;&#20855;&#26377;&#19981;&#21516;&#35748;&#30693;&#29305;&#24615;&#30340;&#23383;&#20307;&#35774;&#35745;&#12290;&#35813;&#20513;&#35758;&#26088;&#22312;&#36171;&#20104;&#25490;&#29256;&#32773;&#21046;&#20316;&#21487;&#25552;&#39640;&#22312;&#32447;&#24191;&#21578;&#28857;&#20987;&#29575;&#12289;&#25913;&#21892;&#20799;&#31461;&#35835;&#29289;&#38405;&#35835;&#27700;&#24179;&#12289;&#35753;&#35829;&#35835;&#32773;&#21019;&#24314;&#20010;&#24615;&#21270;&#23383;&#20307;&#65292;&#25110;&#32773;&#25552;&#20379;&#23186;&#20307;&#25991;&#26412;&#20869;&#23481;&#23458;&#25143;&#21453;&#39304;&#35265;&#35299;&#30340;&#23383;&#20307;&#35774;&#35745;&#33021;&#21147;&#12290;&#19982;&#23558;&#25490;&#29256;&#26144;&#23556;&#21040;&#35748;&#30693;&#30456;&#20851;&#30340;&#30740;&#31350;&#20013;&#30340;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#26159;&#21019;&#24314;&#25104;&#21315;&#19978;&#19975;&#31181;&#20855;&#26377;&#24494;&#23567;&#21464;&#21270;&#30340;&#23383;&#20307;&#65292;&#36825;&#20010;&#36807;&#31243;&#26082;&#21171;&#21160;&#23494;&#38598;&#21448;&#38656;&#35201;&#32463;&#39564;&#20016;&#23500;&#30340;&#25490;&#29256;&#32773;&#30340;&#19987;&#19994;&#30693;&#35782;&#12290;&#35748;&#30693;&#31185;&#23398;&#30740;&#31350;&#24378;&#35843;&#23383;&#27597;&#30340;&#35774;&#35745;&#21644;&#24418;&#24335;&#20197;&#21450;&#25991;&#26412;&#25972;&#20307;&#24067;&#23616;&#23545;&#38405;&#35835;&#30340;&#20415;&#21033;&#24615;&#21644;&#20854;&#20182;&#23383;&#20307;&#35748;&#30693;&#23646;&#24615;&#20363;&#22914;&#24863;&#30693;&#32654;&#24863;&#21644;&#35760;&#24518;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#20123;&#22240;&#32032;&#19981;&#20165;&#24433;&#21709;&#23383;&#20307;&#30340;&#26131;&#35835;&#24615;&#21644;&#28165;&#26224;&#24230;&#65292;&#20063;&#24433;&#21709;&#23383;&#20307;&#30340;&#32654;&#35266;&#21644;&#35760;&#24518;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04087v1 Announce Type: new  Abstract: The Cognitive Type Project is focused on developing computational tools to enable the design of typefaces with varying cognitive properties. This initiative aims to empower typographers to craft fonts that enhance click-through rates for online ads, improve reading levels in children's books, enable dyslexics to create personalized type, or provide insights into customer reactions to textual content in media. A significant challenge in research related to mapping typography to cognition is the creation of thousands of typefaces with minor variations, a process that is both labor-intensive and requires the expertise of skilled typographers. Cognitive science research highlights that the design and form of letters, along with the text's overall layout, are crucial in determining the ease of reading and other cognitive properties of type such as perceived beauty and memorability. These factors affect not only the legibility and clarity of i
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39640;&#36136;&#37327;&#20266;&#26631;&#31614;&#36873;&#25321;&#26041;&#27861; SiCF&#65292;&#36890;&#36807;&#35813;&#26041;&#27861;&#36873;&#25321;&#20855;&#26377;&#39640;&#36136;&#37327;&#29983;&#25104;&#25688;&#35201;&#30340;&#26080;&#26631;&#31614;&#23545;&#35805;&#26469;&#36827;&#34892;&#21322;&#30417;&#30563;&#23545;&#35805;&#25277;&#21462;&#24335;&#25688;&#35201;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2403.04073</link><description>&lt;p&gt;
&#36890;&#36807;&#39640;&#36136;&#37327;&#20266;&#26631;&#31614;&#36873;&#25321;&#30340;&#21322;&#30417;&#30563;&#23545;&#35805;&#25277;&#21462;&#24335;&#25688;&#35201;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Semi-Supervised Dialogue Abstractive Summarization via High-Quality Pseudolabel Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04073
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39640;&#36136;&#37327;&#20266;&#26631;&#31614;&#36873;&#25321;&#26041;&#27861; SiCF&#65292;&#36890;&#36807;&#35813;&#26041;&#27861;&#36873;&#25321;&#20855;&#26377;&#39640;&#36136;&#37327;&#29983;&#25104;&#25688;&#35201;&#30340;&#26080;&#26631;&#31614;&#23545;&#35805;&#26469;&#36827;&#34892;&#21322;&#30417;&#30563;&#23545;&#35805;&#25277;&#21462;&#24335;&#25688;&#35201;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#30417;&#30563;&#23545;&#35805;&#25688;&#35201;&#29983;&#25104;&#65288;SSDS&#65289;&#21033;&#29992;&#27169;&#22411;&#29983;&#25104;&#30340;&#25688;&#35201;&#20943;&#23569;&#23545;&#20154;&#24037;&#26631;&#35760;&#25968;&#25454;&#30340;&#20381;&#36182;&#65292;&#25552;&#39640;&#25688;&#35201;&#29983;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35780;&#20998;&#26041;&#27861; SiCF&#65292;&#21253;&#21547;&#20102;&#25688;&#35201;&#27169;&#22411;&#36136;&#37327;&#30340;&#19977;&#20010;&#20027;&#35201;&#32500;&#24230;&#65306;&#35821;&#20041;&#19981;&#21464;&#24615;&#65288;&#27169;&#22411;&#20449;&#24515;&#30340;&#25351;&#31034;&#65289;&#12289;&#35206;&#30422;&#24230;&#65288;&#20107;&#23454;&#21484;&#22238;&#65289;&#21644;&#24544;&#23454;&#24230;&#65288;&#20107;&#23454;&#31934;&#24230;&#65289;&#12290;&#21033;&#29992; SiCF &#20998;&#25968;&#65292;&#36873;&#25321;&#20855;&#26377;&#39640;&#36136;&#37327;&#29983;&#25104;&#25688;&#35201;&#30340;&#26080;&#26631;&#31614;&#23545;&#35805;&#26469;&#35757;&#32451;&#25688;&#35201;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04073v1 Announce Type: cross  Abstract: Semi-supervised dialogue summarization (SSDS) leverages model-generated summaries to reduce reliance on human-labeled data and improve the performance of summarization models. While addressing label noise, previous works on semi-supervised learning primarily focus on natural language understanding tasks, assuming each sample has a unique label. However, these methods are not directly applicable to SSDS, as it is a generative task, and each dialogue can be summarized in different ways. In this work, we propose a novel scoring approach, SiCF, which encapsulates three primary dimensions of summarization model quality: Semantic invariance (indicative of model confidence), Coverage (factual recall), and Faithfulness (factual precision). Using the SiCF score, we select unlabeled dialogues with high-quality generated summaries to train summarization models. Comprehensive experiments on three public datasets demonstrate the effectiveness of Si
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#25968;&#25454;&#39537;&#21160;&#30340;&#32479;&#35745;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#20844;&#20849;&#20844;&#20132;&#26381;&#21153;&#20013;&#26367;&#20195;&#36710;&#36742;&#26368;&#20339;&#20301;&#32622;&#36873;&#25321;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.04072</link><description>&lt;p&gt;
&#39044;&#27979;&#21644;&#32531;&#35299;&#20844;&#20849;&#20844;&#20132;&#26381;&#21153;&#20013;&#30340;&#20013;&#26029;
&lt;/p&gt;
&lt;p&gt;
Forecasting and Mitigating Disruptions in Public Bus Transit Services
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04072
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#25968;&#25454;&#39537;&#21160;&#30340;&#32479;&#35745;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#20844;&#20849;&#20844;&#20132;&#26381;&#21153;&#20013;&#26367;&#20195;&#36710;&#36742;&#26368;&#20339;&#20301;&#32622;&#36873;&#25321;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#20849;&#20132;&#36890;&#31995;&#32479;&#32463;&#24120;&#36973;&#21463;&#38656;&#27714;&#30340;&#24847;&#22806;&#27874;&#21160;&#21644;&#20013;&#26029;&#65292;&#20363;&#22914;&#26426;&#26800;&#25925;&#38556;&#21644;&#21307;&#30103;&#32039;&#24613;&#24773;&#20917;&#12290;&#36825;&#20123;&#27874;&#21160;&#21644;&#20013;&#26029;&#23548;&#33268;&#24310;&#35823;&#21644;&#25317;&#25380;&#65292;&#36825;&#23545;&#20056;&#23458;&#30340;&#20307;&#39564;&#21644;&#20844;&#20849;&#20132;&#36890;&#26381;&#21153;&#30340;&#25972;&#20307;&#24615;&#33021;&#37117;&#26159;&#26377;&#23475;&#30340;&#12290;&#20026;&#20102;&#31215;&#26497;&#20943;&#23569;&#36825;&#31867;&#20107;&#20214;&#65292;&#35768;&#22810;&#36816;&#36755;&#26426;&#26500;&#22312;&#20854;&#26381;&#21153;&#33539;&#22260;&#20869;&#35774;&#32622;&#26367;&#20195;&#65288;&#22791;&#29992;&#65289;&#36710;&#36742;&#65292;&#23427;&#20204;&#21487;&#20197;&#35843;&#24230;&#36825;&#20123;&#36710;&#36742;&#20197;&#22686;&#21152;&#25110;&#26367;&#20195;&#36973;&#21463;&#25317;&#25380;&#25110;&#20013;&#26029;&#30340;&#36335;&#32447;&#19978;&#30340;&#36710;&#36742;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20013;&#26029;&#30340;&#22266;&#26377;&#38543;&#26426;&#24615;&#21644;&#22312;&#22478;&#24066;&#21508;&#22320;&#36873;&#25321;&#20301;&#32622;&#30340;&#32452;&#21512;&#24615;&#36136;&#65292;&#30830;&#23450;&#26367;&#20195;&#36710;&#36742;&#24212;&#35813;&#35774;&#32622;&#30340;&#26368;&#20339;&#20301;&#32622;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#19982;&#30000;&#32435;&#35199;&#24030;&#32435;&#20160;&#32500;&#23572;&#30340;&#36816;&#36755;&#26426;&#26500;&#21512;&#20316;&#65292;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#32479;&#35745;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04072v1 Announce Type: new  Abstract: Public transportation systems often suffer from unexpected fluctuations in demand and disruptions, such as mechanical failures and medical emergencies. These fluctuations and disruptions lead to delays and overcrowding, which are detrimental to the passengers' experience and to the overall performance of the transit service. To proactively mitigate such events, many transit agencies station substitute (reserve) vehicles throughout their service areas, which they can dispatch to augment or replace vehicles on routes that suffer overcrowding or disruption. However, determining the optimal locations where substitute vehicles should be stationed is a challenging problem due to the inherent randomness of disruptions and due to the combinatorial nature of selecting locations across a city. In collaboration with the transit agency of Nashville, TN, we address this problem by introducing data-driven statistical and machine-learning models for fo
&lt;/p&gt;</description></item><item><title>&#38024;&#23545;&#30828;&#20214;&#21463;&#38480;&#30340;&#32435;&#31859;&#22235;&#36724;&#39134;&#34892;&#22120;&#65292;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#36827;&#34892;&#35774;&#22791;&#31471;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26469;&#35299;&#20915;&#39046;&#22495;&#36716;&#31227;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.04071</link><description>&lt;p&gt;
&#22312;&#30828;&#20214;&#21463;&#38480;&#30340;&#32435;&#31859;&#22235;&#36724;&#39134;&#34892;&#22120;&#19978;&#36827;&#34892;&#35774;&#22791;&#31471;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#35270;&#35273;&#24863;&#30693;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
On-device Self-supervised Learning of Visual Perception Tasks aboard Hardware-limited Nano-quadrotors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04071
&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#30828;&#20214;&#21463;&#38480;&#30340;&#32435;&#31859;&#22235;&#36724;&#39134;&#34892;&#22120;&#65292;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#36827;&#34892;&#35774;&#22791;&#31471;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26469;&#35299;&#20915;&#39046;&#22495;&#36716;&#31227;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23567;&#20110;50&#20811;&#30340;&#32435;&#31859;&#26080;&#20154;&#26426;&#65288;nano-drones&#65289;&#22312;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#37117;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#23613;&#31649;&#21463;&#21040;&#30828;&#20214;&#32422;&#26463;&#65288;&#22914;&#20302;&#20110;100&#27627;&#29926;&#30340;&#22788;&#29702;&#22120;&#65289;&#65292;&#23427;&#20204;&#26368;&#20855;&#21560;&#24341;&#21147;&#30340;&#24212;&#29992;&#20381;&#36182;&#20110;&#29992;&#20110;&#24863;&#30693;&#30340;&#35774;&#22791;&#31471;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#24403;&#37096;&#32626;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#26410;&#34920;&#31034;&#30340;&#26410;&#30693;&#29615;&#22659;&#20013;&#26102;&#65292;&#36825;&#20123;&#27169;&#22411;&#32463;&#24120;&#30001;&#20110;&#39046;&#22495;&#36716;&#31227;&#32780;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#26681;&#26412;&#24615;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#22312;&#32435;&#31859;&#26080;&#20154;&#26426;&#19978;&#36827;&#34892;&#35774;&#22791;&#31471;&#23398;&#20064;&#65292;&#20854;&#20013;&#29616;&#22330;&#20219;&#21153;&#30340;&#31532;&#19968;&#37096;&#20998;&#19987;&#38376;&#29992;&#20110;&#33258;&#30417;&#30563;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#12290;&#21033;&#29992;&#30495;&#23454;&#19990;&#30028;&#30340;&#22522;&#20110;&#35270;&#35273;&#30340;&#22238;&#24402;&#20219;&#21153;&#65292;&#25105;&#20204;&#20840;&#38754;&#25506;&#35752;&#20102;&#24494;&#35843;&#38454;&#27573;&#22312;&#19977;&#20010;&#26041;&#38754;&#30340;&#24615;&#33021;&#25104;&#26412;&#26435;&#34913;&#65306;i&#65289;&#25968;&#25454;&#38598;&#22823;&#23567;&#65288;&#26356;&#22810;&#25968;&#25454;&#21487;&#20197;&#25552;&#39640;&#22238;&#24402;&#24615;&#33021;&#65292;&#20294;&#38656;&#35201;&#26356;&#22810;&#20869;&#23384;&#21644;&#26356;&#38271;&#30340;&#35745;&#31639;&#65289;&#65307;ii&#65289;&#26041;&#27861;&#35770;&#31561;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04071v1 Announce Type: cross  Abstract: Sub-\SI{50}{\gram} nano-drones are gaining momentum in both academia and industry. Their most compelling applications rely on onboard deep learning models for perception despite severe hardware constraints (\ie sub-\SI{100}{\milli\watt} processor). When deployed in unknown environments not represented in the training data, these models often underperform due to domain shift. To cope with this fundamental problem, we propose, for the first time, on-device learning aboard nano-drones, where the first part of the in-field mission is dedicated to self-supervised fine-tuning of a pre-trained convolutional neural network (CNN). Leveraging a real-world vision-based regression task, we thoroughly explore performance-cost trade-offs of the fine-tuning phase along three axes: \textit{i}) dataset size (more data increases the regression performance but requires more memory and longer computation); \textit{ii}) methodologies (\eg fine-tuning all m
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#33030;&#24369;&#24615;&#24863;&#30693;&#30340;&#37325;&#26032;&#21152;&#26435;&#20989;&#25968;&#65292;&#29992;&#20110;&#20026;&#23545;&#25239;&#35757;&#32451;&#20013;&#30340;&#23545;&#25239;&#31034;&#20363;&#20998;&#37197;&#25200;&#21160;&#30028;&#38480;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#23545;&#25239;&#35757;&#32451;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.04070</link><description>&lt;p&gt;
&#21033;&#29992;&#33030;&#24369;&#24615;&#24863;&#30693;&#25200;&#21160;&#39044;&#31639;&#25913;&#36827;&#23545;&#25239;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Improving Adversarial Training using Vulnerability-Aware Perturbation Budget
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04070
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#33030;&#24369;&#24615;&#24863;&#30693;&#30340;&#37325;&#26032;&#21152;&#26435;&#20989;&#25968;&#65292;&#29992;&#20110;&#20026;&#23545;&#25239;&#35757;&#32451;&#20013;&#30340;&#23545;&#25239;&#31034;&#20363;&#20998;&#37197;&#25200;&#21160;&#30028;&#38480;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#23545;&#25239;&#35757;&#32451;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#35757;&#32451;(Adversarial Training, AT)&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#23545;&#25932;&#23545;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;&#36890;&#24120;&#65292;AT&#28041;&#21450;&#20351;&#29992;&#22312;&#39044;&#23450;&#20041;&#12289;&#22266;&#23450;&#25200;&#21160;&#30028;&#38480;&#20869;&#33719;&#21462;&#30340;&#23545;&#25239;&#31034;&#20363;&#26469;&#35757;&#32451;DNN&#27169;&#22411;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#29992;&#20110;&#21046;&#20316;&#36825;&#20123;&#23545;&#25239;&#31034;&#20363;&#30340;&#20010;&#21035;&#33258;&#28982;&#31034;&#20363;&#23637;&#31034;&#20986;&#19981;&#21516;&#31243;&#24230;&#30340;&#22266;&#26377;&#33030;&#24369;&#24615;&#65292;&#22240;&#27492;&#65292;&#20026;&#25152;&#26377;&#23454;&#20363;&#35774;&#23450;&#22266;&#23450;&#25200;&#21160;&#21322;&#24452;&#26469;&#21046;&#20316;&#23545;&#25239;&#31034;&#20363;&#21487;&#33021;&#19981;&#36275;&#20197;&#20805;&#20998;&#21457;&#25381;AT&#30340;&#26377;&#25928;&#24615;&#12290;&#21463;&#21040;&#36825;&#19968;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#31616;&#21333;&#12289;&#35745;&#31639;&#24265;&#20215;&#30340;&#22522;&#20110;&#33030;&#24369;&#24615;&#24863;&#30693;&#30340;&#37325;&#26032;&#21152;&#26435;&#20989;&#25968;&#65292;&#29992;&#20110;&#20026;&#29992;&#20110;AT&#30340;&#23545;&#25239;&#31034;&#20363;&#20998;&#37197;&#25200;&#21160;&#30028;&#38480;&#65292;&#20998;&#21035;&#21629;&#21517;&#20026;&#36793;&#38469;&#21152;&#26435;&#25200;&#21160;&#39044;&#31639;&#65288;MWPB&#65289;&#21644;&#26631;&#20934;&#24046;&#21152;&#26435;&#25200;&#21160;&#39044;&#31639;&#65288;SDWPB&#65289;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26681;&#25454;&#20854;&#23545;&#24212;&#33030;&#24369;&#24615;&#20026;&#21333;&#20010;&#23545;&#25239;&#26679;&#26412;&#20998;&#37197;&#25200;&#21160;&#21322;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04070v1 Announce Type: cross  Abstract: Adversarial Training (AT) effectively improves the robustness of Deep Neural Networks (DNNs) to adversarial attacks. Generally, AT involves training DNN models with adversarial examples obtained within a pre-defined, fixed perturbation bound. Notably, individual natural examples from which these adversarial examples are crafted exhibit varying degrees of intrinsic vulnerabilities, and as such, crafting adversarial examples with fixed perturbation radius for all instances may not sufficiently unleash the potency of AT. Motivated by this observation, we propose two simple, computationally cheap vulnerability-aware reweighting functions for assigning perturbation bounds to adversarial examples used for AT, named Margin-Weighted Perturbation Budget (MWPB) and Standard-Deviation-Weighted Perturbation Budget (SDWPB). The proposed methods assign perturbation radii to individual adversarial samples based on the vulnerability of their correspon
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;&#23545;&#27604;&#23398;&#20064;&#26469;&#35299;&#20915;&#23556;&#39057;&#35774;&#22791;&#25351;&#32441;&#35782;&#21035;&#20013;&#30340;&#39046;&#22495;&#28418;&#31227;&#38382;&#39064;&#65292;&#36890;&#36807;&#23398;&#20064;&#36317;&#31163;&#24230;&#37327;&#22788;&#29702;RF&#20449;&#21495;&#65292;&#20174;&#32780;&#25552;&#39640;&#31283;&#20581;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.04036</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;&#25269;&#25239;&#26102;&#22495;&#28418;&#31227;&#19979;&#30340;&#31283;&#20581;&#23556;&#39057;&#35774;&#22791;&#25351;&#32441;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Contrastive Learning for Robust RF Device Fingerprinting Under Time-Domain Shift
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04036
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;&#23545;&#27604;&#23398;&#20064;&#26469;&#35299;&#20915;&#23556;&#39057;&#35774;&#22791;&#25351;&#32441;&#35782;&#21035;&#20013;&#30340;&#39046;&#22495;&#28418;&#31227;&#38382;&#39064;&#65292;&#36890;&#36807;&#23398;&#20064;&#36317;&#31163;&#24230;&#37327;&#22788;&#29702;RF&#20449;&#21495;&#65292;&#20174;&#32780;&#25552;&#39640;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23556;&#39057;&#65288;RF&#65289;&#35774;&#22791;&#25351;&#32441;&#35782;&#21035;&#34987;&#35748;&#20026;&#26159;&#23454;&#29616;&#33258;&#21160;&#26080;&#32447;&#35774;&#22791;&#35782;&#21035;&#21644;&#20998;&#31867;&#30340;&#28508;&#22312;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20449;&#36947;&#26465;&#20214;&#21644;&#29615;&#22659;&#35774;&#32622;&#30340;&#21464;&#21270;&#21487;&#33021;&#24341;&#36215;&#30340;&#39046;&#22495;&#28418;&#31227;&#65292;&#21487;&#33021;&#20250;&#38477;&#20302;&#22522;&#20110;RF&#30340;&#35774;&#22791;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#65292;&#29305;&#21035;&#26159;&#24403;&#27979;&#35797;&#21644;&#35757;&#32451;&#25968;&#25454;&#22312;&#19981;&#21516;&#39046;&#22495;&#25910;&#38598;&#26102;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#26469;&#32531;&#35299;&#36825;&#31181;&#39046;&#22495;&#28418;&#31227;&#38382;&#39064;&#12290;&#23545;&#27604;&#23398;&#20064;&#26159;&#19968;&#31181;&#26469;&#33258;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#20808;&#36827;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#23398;&#20064;&#19968;&#20010;&#36317;&#31163;&#24230;&#37327;&#65292;&#20351;&#24471;&#27491;&#23545;&#32452;&#22312;&#23398;&#20064;&#30340;&#24230;&#37327;&#31354;&#38388;&#20013;&#27604;&#36127;&#23545;&#26356;&#25509;&#36817;&#65288;&#21363;&#26356;&#30456;&#20284;&#65289;&#12290;&#24403;&#24212;&#29992;&#20110;RF&#25351;&#32441;&#35782;&#21035;&#26102;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#23558;&#26469;&#33258;&#30456;&#21516;&#20256;&#36755;&#30340;RF&#20449;&#21495;&#35270;&#20026;&#27491;&#23545;&#65292;&#23558;&#26469;&#33258;&#19981;&#21516;&#20256;&#36755;&#30340;&#20449;&#21495;&#35270;&#20026;&#36127;&#23545;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04036v1 Announce Type: cross  Abstract: Radio Frequency (RF) device fingerprinting has been recognized as a potential technology for enabling automated wireless device identification and classification. However, it faces a key challenge due to the domain shift that could arise from variations in the channel conditions and environmental settings, potentially degrading the accuracy of RF-based device classification when testing and training data is collected in different domains. This paper introduces a novel solution that leverages contrastive learning to mitigate this domain shift problem. Contrastive learning, a state-of-the-art self-supervised learning approach from deep learning, learns a distance metric such that positive pairs are closer (i.e. more similar) in the learned metric space than negative pairs. When applied to RF fingerprinting, our model treats RF signals from the same transmission as positive pairs and those from different transmissions as negative pairs. T
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#22914;&#20309;&#20010;&#24615;&#21270;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#29983;&#25104;&#30340;&#25552;&#31034;&#30340;&#35299;&#37322;&#65292;&#20197;&#24110;&#21161;&#20419;&#36827;&#23398;&#29983;&#23398;&#20064;&#65292;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#20010;&#24615;&#21270;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#30446;&#26631;&#29992;&#25143;&#19982;&#25552;&#31034;&#35299;&#37322;&#30340;&#20114;&#21160;&#12289;&#29702;&#35299;&#21644;&#23398;&#20064;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.04035</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;AI&#39537;&#21160;&#25552;&#31034;&#23545;&#29992;&#25143;&#35748;&#30693;&#33021;&#21147;&#30340;&#35299;&#37322;&#65306;&#19968;&#39033;&#23454;&#35777;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Personalizing explanations of AI-driven hints to users cognitive abilities: an empirical evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04035
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#22914;&#20309;&#20010;&#24615;&#21270;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#29983;&#25104;&#30340;&#25552;&#31034;&#30340;&#35299;&#37322;&#65292;&#20197;&#24110;&#21161;&#20419;&#36827;&#23398;&#29983;&#23398;&#20064;&#65292;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#20010;&#24615;&#21270;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#30446;&#26631;&#29992;&#25143;&#19982;&#25552;&#31034;&#35299;&#37322;&#30340;&#20114;&#21160;&#12289;&#29702;&#35299;&#21644;&#23398;&#20064;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35843;&#26597;&#20102;&#20010;&#24615;&#21270;&#35299;&#37322;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#29983;&#25104;&#30340;&#25552;&#31034;&#65292;&#20197;&#35777;&#26126;&#23427;&#20204;&#25552;&#20379;&#25552;&#31034;&#20419;&#36827;&#23398;&#29983;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#12290;&#20010;&#24615;&#21270;&#38024;&#23545;&#20855;&#26377;&#20004;&#31181;&#29305;&#24449;&#65288;&#35748;&#30693;&#38656;&#27714;&#21644;&#35748;&#30495;&#24230;&#65289;&#36739;&#20302;&#27700;&#24179;&#30340;&#23398;&#29983;&#65292;&#26088;&#22312;&#22686;&#24378;&#36825;&#20123;&#23398;&#29983;&#23545;&#35299;&#37322;&#30340;&#21442;&#19982;&#65292;&#22522;&#20110;&#20808;&#21069;&#30740;&#31350;&#21457;&#29616;&#65292;&#36825;&#20123;&#23398;&#29983;&#19981;&#20250;&#33258;&#28982;&#21442;&#19982;&#35299;&#37322;&#65292;&#20294;&#22914;&#26524;&#20182;&#20204;&#36825;&#26679;&#20570;&#23558;&#20250;&#21463;&#30410;&#12290;&#20026;&#20102;&#35780;&#20272;&#20010;&#24615;&#21270;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#29992;&#25143;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#25552;&#20986;&#30340;&#20010;&#24615;&#21270;&#26174;&#33879;&#22686;&#21152;&#20102;&#25105;&#20204;&#30446;&#26631;&#29992;&#25143;&#19982;&#25552;&#31034;&#35299;&#37322;&#30340;&#20114;&#21160;&#12289;&#20182;&#20204;&#23545;&#25552;&#31034;&#30340;&#29702;&#35299;&#20197;&#21450;&#20182;&#20204;&#30340;&#23398;&#20064;&#12290;&#22240;&#27492;&#65292;&#36825;&#39033;&#24037;&#20316;&#20026;&#20010;&#24615;&#21270;AI&#39537;&#21160;&#35299;&#37322;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#65292;&#36866;&#29992;&#20110;&#22914;&#23398;&#20064;&#31561;&#35748;&#30693;&#35201;&#27714;&#39640;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04035v1 Announce Type: new  Abstract: We investigate personalizing the explanations that an Intelligent Tutoring System generates to justify the hints it provides to students to foster their learning. The personalization targets students with low levels of two traits, Need for Cognition and Conscientiousness, and aims to enhance these students' engagement with the explanations, based on prior findings that these students do not naturally engage with the explanations but they would benefit from them if they do. To evaluate the effectiveness of the personalization, we conducted a user study where we found that our proposed personalization significantly increases our target users' interaction with the hint explanations, their understanding of the hints and their learning. Hence, this work provides valuable insights into effectively personalizing AI-driven explanations for cognitively demanding tasks such as learning.
&lt;/p&gt;</description></item><item><title>&#22312;&#32447;&#23398;&#20064;&#20013;&#36890;&#36807;&#20803;&#31639;&#27861;&#32467;&#21512;&#22312;&#32447;&#22238;&#24402;&#39044;&#27979;&#22120;&#20272;&#35745;&#26410;&#30693;&#23433;&#20840;&#32422;&#26463;&#65292;&#24182;&#23558;&#22312;&#32447;&#23398;&#20064;&#39044;&#27979;&#36716;&#25442;&#20026;&#31526;&#21512;&#32422;&#26463;&#30340;&#39044;&#27979;&#65292;&#21516;&#26102;&#20445;&#35777;&#22312;&#27599;&#19968;&#36718;&#39640;&#27010;&#29575;&#22320;&#28385;&#36275;&#23433;&#20840;&#32422;&#26463;&#12290;&#31639;&#27861;&#30340;&#21518;&#24724;&#21463;&#21040;&#22312;&#32447;&#22238;&#24402;&#21644;&#22312;&#32447;&#23398;&#20064;&#39044;&#27979;&#22120;&#30340;&#38480;&#21046;&#65292;&#27169;&#22411;&#31867;&#20013;&#26410;&#30693;&#23433;&#20840;&#32422;&#26463;&#30340;&#36867;&#36991;&#32500;&#24230;&#65292;&#20197;&#21450;&#25429;&#25417;&#23433;&#20840;&#23398;&#20064;&#22256;&#38590;&#30340;&#26032;&#39062;&#22797;&#26434;&#24230;&#24230;&#37327;&#30340;&#32422;&#26463;&#12290;</title><link>https://arxiv.org/abs/2403.04033</link><description>&lt;p&gt;
&#20855;&#26377;&#26410;&#30693;&#32422;&#26463;&#30340;&#22312;&#32447;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Online Learning with Unknown Constraints
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04033
&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#23398;&#20064;&#20013;&#36890;&#36807;&#20803;&#31639;&#27861;&#32467;&#21512;&#22312;&#32447;&#22238;&#24402;&#39044;&#27979;&#22120;&#20272;&#35745;&#26410;&#30693;&#23433;&#20840;&#32422;&#26463;&#65292;&#24182;&#23558;&#22312;&#32447;&#23398;&#20064;&#39044;&#27979;&#36716;&#25442;&#20026;&#31526;&#21512;&#32422;&#26463;&#30340;&#39044;&#27979;&#65292;&#21516;&#26102;&#20445;&#35777;&#22312;&#27599;&#19968;&#36718;&#39640;&#27010;&#29575;&#22320;&#28385;&#36275;&#23433;&#20840;&#32422;&#26463;&#12290;&#31639;&#27861;&#30340;&#21518;&#24724;&#21463;&#21040;&#22312;&#32447;&#22238;&#24402;&#21644;&#22312;&#32447;&#23398;&#20064;&#39044;&#27979;&#22120;&#30340;&#38480;&#21046;&#65292;&#27169;&#22411;&#31867;&#20013;&#26410;&#30693;&#23433;&#20840;&#32422;&#26463;&#30340;&#36867;&#36991;&#32500;&#24230;&#65292;&#20197;&#21450;&#25429;&#25417;&#23433;&#20840;&#23398;&#20064;&#22256;&#38590;&#30340;&#26032;&#39062;&#22797;&#26434;&#24230;&#24230;&#37327;&#30340;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#32447;&#23398;&#20064;&#20013;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#23398;&#20064;&#32773;&#22312;&#27599;&#19968;&#36718;&#24517;&#39035;&#36981;&#23432;&#19968;&#20010;&#26410;&#30693;&#30340;&#23433;&#20840;&#32422;&#26463;&#12290;&#30446;&#26631;&#26159;&#22312;&#21516;&#26102;&#28385;&#36275;&#23433;&#20840;&#32422;&#26463;&#24182;&#22312;&#21518;&#35270;&#19979;&#26368;&#23567;&#21270;&#23545;&#26368;&#20339;&#23433;&#20840;&#21160;&#20316;&#30340;&#21518;&#24724;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#20803;&#31639;&#27861;&#65292;&#21033;&#29992;&#22312;&#32447;&#22238;&#24402;&#39044;&#27979;&#22120;&#26469;&#20272;&#35745;&#26410;&#30693;&#23433;&#20840;&#32422;&#26463;&#65292;&#24182;&#23558;&#22312;&#32447;&#23398;&#20064;&#39044;&#27979;&#36716;&#25442;&#20026;&#31526;&#21512;&#26410;&#30693;&#23433;&#20840;&#32422;&#26463;&#30340;&#39044;&#27979;&#12290;&#22312;&#29702;&#35770;&#26041;&#38754;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#30340;&#21518;&#24724;&#21487;&#20197;&#36890;&#36807;&#22312;&#32447;&#22238;&#24402;&#21644;&#22312;&#32447;&#23398;&#20064;&#39044;&#27979;&#22120;&#30340;&#21518;&#24724;&#12289;&#21253;&#21547;&#26410;&#30693;&#23433;&#20840;&#32422;&#26463;&#30340;&#27169;&#22411;&#31867;&#30340;&#36867;&#36991;&#32500;&#24230;&#65292;&#20197;&#21450;&#19968;&#20010;&#25429;&#25417;&#23433;&#20840;&#23398;&#20064;&#22256;&#38590;&#31243;&#24230;&#30340;&#26032;&#39062;&#22797;&#26434;&#24230;&#24230;&#37327;&#26469;&#30028;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04033v1 Announce Type: cross  Abstract: We consider the problem of online learning where the sequence of actions played by the learner must adhere to an unknown safety constraint at every round. The goal is to minimize regret with respect to the best safe action in hindsight while simultaneously satisfying the safety constraint with high probability on each round. We provide a general meta-algorithm that leverages an online regression oracle to estimate the unknown safety constraint, and converts the predictions of an online learning oracle to predictions that adhere to the unknown safety constraint. On the theoretical side, our algorithm's regret can be bounded by the regret of the online regression and online learning oracles, the eluder dimension of the model class containing the unknown safety constraint, and a novel complexity measure that captures the difficulty of safe learning. We complement our result with an asymptotic lower bound that shows that the aforementioned
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20307;&#32946;&#39046;&#22495;&#30340;&#20998;&#26512;&#25512;&#29702;&#33021;&#21147;&#65292;&#21457;&#29616;&#22312;&#22788;&#29702;NBA&#21644;NFL&#27604;&#36187;&#24471;&#20998;&#20219;&#21153;&#26102;&#65292;GPT-4&#21644;Claude-2.1&#34920;&#29616;&#26368;&#20339;&#65292;&#37319;&#29992;&#20998;&#27835;&#27861;&#36827;&#34892;&#25968;&#25454;&#22788;&#29702;&#25928;&#26524;&#26368;&#22909;&#12290;</title><link>https://arxiv.org/abs/2403.04031</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#36827;&#34892;&#20998;&#26512;&#25512;&#29702;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models do Analytical Reasoning?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04031
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20307;&#32946;&#39046;&#22495;&#30340;&#20998;&#26512;&#25512;&#29702;&#33021;&#21147;&#65292;&#21457;&#29616;&#22312;&#22788;&#29702;NBA&#21644;NFL&#27604;&#36187;&#24471;&#20998;&#20219;&#21153;&#26102;&#65292;GPT-4&#21644;Claude-2.1&#34920;&#29616;&#26368;&#20339;&#65292;&#37319;&#29992;&#20998;&#27835;&#27861;&#36827;&#34892;&#25968;&#25454;&#22788;&#29702;&#25928;&#26524;&#26368;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#24212;&#29992;&#20110;&#20307;&#32946;&#39046;&#22495;&#30340;&#21069;&#27839;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20998;&#26512;&#25512;&#29702;&#12290;&#25105;&#20204;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#35745;&#31639;NBA&#21644;NFL&#27604;&#36187;&#20013;&#27599;&#25903;&#29699;&#38431;&#22312;&#19968;&#20010;&#23395;&#24230;&#20013;&#24471;&#20998;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#21457;&#29616;&#26377;&#20004;&#20010;&#26041;&#38754;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#25105;&#20204;&#20351;&#29992;&#30340;&#25152;&#26377;&#27169;&#22411;&#20013;&#65292;GPT-4&#22312;&#25928;&#26524;&#19978;&#34920;&#29616;&#26368;&#20026;&#31361;&#20986;&#65292;&#20854;&#27425;&#26159;Claude-2.1&#65292;&#32780;GPT-3.5&#12289;Gemini-Pro&#21644;Llama-2-70b&#25928;&#26524;&#31245;&#36874;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#25552;&#31034;&#25216;&#26415;&#21644;&#20998;&#27835;&#27861;&#65292;&#21457;&#29616;&#21518;&#32773;&#25928;&#26524;&#26368;&#22909;&#12290;&#25105;&#20204;&#30340;&#20998;&#27835;&#27861;&#23558;&#36880;&#27493;&#25968;&#25454;&#32454;&#20998;&#20026;&#26356;&#23567;&#12289;&#26356;&#26131;&#22788;&#29702;&#30340;&#29255;&#27573;&#65292;&#20998;&#21035;&#35299;&#20915;&#27599;&#20010;&#29255;&#27573;&#65292;&#28982;&#21518;&#23558;&#23427;&#20204;&#21512;&#24182;&#22312;&#19968;&#36215;&#12290;&#38500;&#20102;&#20998;&#27835;&#27861;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#19968;&#31181;&#21517;&#20026;Chain of Thought&#65288;CoT&#65289;&#31574;&#30053;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#26576;&#20123;&#27169;&#22411;&#30340;&#32467;&#26524;&#65292;&#23588;&#20854;&#26159;GPT-4&#21644;Claude-2.1&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04031v1 Announce Type: cross  Abstract: This paper explores the cutting-edge Large Language Model with analytical reasoning on sports. Our analytical reasoning embodies the tasks of letting large language models count how many points each team scores in a quarter in the NBA and NFL games. Our major discoveries are in two folds. Firstly, we find among all the models we employed, GPT-4 stands out in effectiveness, followed by Claude-2.1, with GPT-3.5, Gemini-Pro, and Llama-2-70b lagging behind. Specifically, we compare three different prompting techniques and a divide-and-conquer approach, we find that the latter was the most effective. Our divide-and-conquer approach breaks down play-by-play data into smaller, more manageable segments, solves each piece individually, and then aggregates them together. Besides the divide-and-conquer approach, we also explore the Chain of Thought (CoT) strategy, which markedly improves outcomes for certain models, notably GPT-4 and Claude-2.1, 
&lt;/p&gt;</description></item><item><title>&#26426;&#22120;&#23398;&#20064;&#21487;&#24341;&#23548;&#33258;&#21160;&#25512;&#29702;&#31995;&#32479;&#65292;&#36890;&#36807;&#36923;&#36753;&#26377;&#25928;&#35777;&#26126;&#27010;&#24565;&#25903;&#25345;&#28436;&#32462;&#25628;&#32034;&#65292;&#20174;&#22823;&#22411;&#25512;&#29702;&#20307;&#20013;&#22521;&#35757;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#65292;&#25171;&#30772;&#25512;&#29702;&#31995;&#32479;&#30340;&#32452;&#21512;&#29190;&#28856;&#38382;&#39064;&#65292;&#24182;&#20419;&#36827;&#25512;&#29702;&#38142;&#30340;&#38271;&#36828;&#21457;&#23637;&#21644;&#26032;&#39062;&#35777;&#26126;&#24605;&#36335;&#30340;&#20135;&#29983;</title><link>https://arxiv.org/abs/2403.04017</link><description>&lt;p&gt;
&#23398;&#20064;&#24341;&#23548;&#30340;&#33258;&#21160;&#25512;&#29702;&#65306;&#31616;&#35201;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Learning Guided Automated Reasoning: A Brief Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04017
&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21487;&#24341;&#23548;&#33258;&#21160;&#25512;&#29702;&#31995;&#32479;&#65292;&#36890;&#36807;&#36923;&#36753;&#26377;&#25928;&#35777;&#26126;&#27010;&#24565;&#25903;&#25345;&#28436;&#32462;&#25628;&#32034;&#65292;&#20174;&#22823;&#22411;&#25512;&#29702;&#20307;&#20013;&#22521;&#35757;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#65292;&#25171;&#30772;&#25512;&#29702;&#31995;&#32479;&#30340;&#32452;&#21512;&#29190;&#28856;&#38382;&#39064;&#65292;&#24182;&#20419;&#36827;&#25512;&#29702;&#38142;&#30340;&#38271;&#36828;&#21457;&#23637;&#21644;&#26032;&#39062;&#35777;&#26126;&#24605;&#36335;&#30340;&#20135;&#29983;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#23450;&#29702;&#35777;&#26126;&#22120;&#21644;&#24418;&#24335;&#21270;&#35777;&#26126;&#21161;&#25163;&#26159;&#36890;&#29992;&#30340;&#25512;&#29702;&#31995;&#32479;&#65292;&#29702;&#35770;&#19978;&#33021;&#22815;&#35777;&#26126;&#20219;&#24847;&#22256;&#38590;&#30340;&#23450;&#29702;&#65292;&#20174;&#32780;&#35299;&#20915;&#21487;&#24402;&#32422;&#20026;&#25968;&#23398;&#21644;&#36923;&#36753;&#25512;&#29702;&#30340;&#20219;&#24847;&#38382;&#39064;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#36825;&#20123;&#31995;&#32479;&#38754;&#20020;&#30528;&#26497;&#22823;&#30340;&#32452;&#21512;&#29190;&#28856;&#65292;&#22240;&#27492;&#21253;&#21547;&#35768;&#22810;&#21551;&#21457;&#24335;&#26041;&#27861;&#21644;&#36873;&#25321;&#28857;&#65292;&#36825;&#20123;&#26041;&#27861;&#21644;&#36873;&#25321;&#28857;&#26497;&#22823;&#22320;&#24433;&#21709;&#20854;&#24615;&#33021;&#12290;&#36825;&#20026;&#21463;&#36807;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#22120;&#25552;&#20379;&#20102;&#26426;&#20250;&#65292;&#21487;&#20197;&#24341;&#23548;&#36825;&#20123;&#25512;&#29702;&#31995;&#32479;&#30340;&#24037;&#20316;&#12290;&#30456;&#21453;&#65292;&#30001;&#36923;&#36753;&#19978;&#26377;&#25928;&#35777;&#26126;&#27010;&#24565;&#25903;&#25345;&#30340;&#28436;&#32462;&#25628;&#32034;&#20801;&#35768;&#22312;&#22823;&#22411;&#25512;&#29702;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#12290;&#36825;&#20123;&#35777;&#26126;&#20307;&#36890;&#24120;&#26159;&#36890;&#36807;&#26500;&#24314;&#27491;&#30830;&#30340;&#65292;&#24403;&#19982;&#26356;&#21152;&#31934;&#30830;&#30340;&#35757;&#32451;&#24341;&#23548;&#30456;&#32467;&#21512;&#26102;&#65292;&#21487;&#20197;&#23558;&#23427;&#20204;&#25552;&#21319;&#20026;&#38750;&#24120;&#24222;&#22823;&#30340;&#35821;&#26009;&#24211;&#65292;&#20855;&#26377;&#36234;&#26469;&#36234;&#38271;&#30340;&#25512;&#29702;&#38142;&#65292;&#24182;&#21487;&#33021;&#20135;&#29983;&#26032;&#39062;&#30340;&#35777;&#26126;&#24605;&#36335;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04017v1 Announce Type: new  Abstract: Automated theorem provers and formal proof assistants are general reasoning systems that are in theory capable of proving arbitrarily hard theorems, thus solving arbitrary problems reducible to mathematics and logical reasoning. In practice, such systems however face large combinatorial explosion, and therefore include many heuristics and choice points that considerably influence their performance. This is an opportunity for trained machine learning predictors, which can guide the work of such reasoning systems. Conversely, deductive search supported by the notion of logically valid proof allows one to train machine learning systems on large reasoning corpora. Such bodies of proof are usually correct by construction and when combined with more and more precise trained guidance they can be boostrapped into very large corpora, with increasingly long reasoning chains and possibly novel proof ideas. In this paper we provide an overview of se
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#27169;&#25311;&#29305;&#24449;&#25351;&#23548;&#21644;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#30340;&#21019;&#26032;&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#29305;&#24449;&#36873;&#25321;&#30340;&#26041;&#27861;&#65292;&#20197;&#35782;&#21035;&#26368;&#20339;&#26377;&#25928;&#30340;&#29305;&#24449;&#23376;&#38598;&#12290;</title><link>https://arxiv.org/abs/2403.04015</link><description>&lt;p&gt;
&#36890;&#36807;&#21333;&#20010;&#39044;&#35757;&#32451;&#30340;&#22686;&#24378;&#22411;&#20195;&#29702;&#24341;&#23548;&#30340;&#27169;&#25311;&#29305;&#24449;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Knockoff-Guided Feature Selection via A Single Pre-trained Reinforced Agent
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04015
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#27169;&#25311;&#29305;&#24449;&#25351;&#23548;&#21644;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#30340;&#21019;&#26032;&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#29305;&#24449;&#36873;&#25321;&#30340;&#26041;&#27861;&#65292;&#20197;&#35782;&#21035;&#26368;&#20339;&#26377;&#25928;&#30340;&#29305;&#24449;&#23376;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#36873;&#25321;&#36890;&#36807;&#28040;&#38500;&#20887;&#20313;&#29305;&#24449;&#26469;&#20934;&#22791;&#25968;&#25454;&#30340;AI&#21487;&#29992;&#24615;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#20998;&#20026;&#20004;&#31867;&#65306;i&#65289;&#30417;&#30563;&#29305;&#24449;&#36873;&#25321;&#65292;&#26681;&#25454;&#29305;&#24449;&#19982;&#30446;&#26631;&#21464;&#37327;&#30340;&#30456;&#20851;&#24615;&#35782;&#21035;&#26368;&#20339;&#29305;&#24449;&#23376;&#38598;&#65307;ii&#65289;&#26080;&#30417;&#30563;&#29305;&#24449;&#36873;&#25321;&#65292;&#36890;&#36807;&#25429;&#33719;&#29305;&#24449;&#38598;&#20013;&#30340;&#22522;&#26412;&#20449;&#24687;&#32780;&#38750;&#20351;&#29992;&#30446;&#26631;&#21464;&#37327;&#26469;&#20943;&#23569;&#29305;&#24449;&#31354;&#38388;&#30340;&#32500;&#24230;&#12290;&#28982;&#32780;&#65292;&#30417;&#30563;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#30001;&#20110;&#20381;&#36182;&#30446;&#26631;&#21464;&#37327;&#21644;&#19979;&#28216;ML&#20219;&#21153;&#32780;&#23548;&#33268;&#32791;&#26102;&#19988;&#27867;&#21270;&#33021;&#21147;&#26377;&#38480;&#12290;&#26080;&#30417;&#30563;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#21463;&#38480;&#20110;&#25512;&#23548;&#20986;&#30340;&#29305;&#24449;&#31354;&#38388;&#26159;&#28508;&#22312;&#19988;&#19981;&#21487;&#36861;&#36394;&#30340;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#29305;&#24449;&#36873;&#25321;&#26694;&#26550;&#65292;&#36890;&#36807;&#27169;&#25311;&#29305;&#24449;&#25351;&#23548;&#24182;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#20248;&#21270;&#65292;&#20197;&#35782;&#21035;&#26368;&#20339;&#26377;&#25928;&#30340;&#29305;&#24449;&#23376;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04015v1 Announce Type: cross  Abstract: Feature selection prepares the AI-readiness of data by eliminating redundant features. Prior research falls into two primary categories: i) Supervised Feature Selection, which identifies the optimal feature subset based on their relevance to the target variable; ii) Unsupervised Feature Selection, which reduces the feature space dimensionality by capturing the essential information within the feature set instead of using target variable. However, SFS approaches suffer from time-consuming processes and limited generalizability due to the dependence on the target variable and downstream ML tasks. UFS methods are constrained by the deducted feature space is latent and untraceable. To address these challenges, we introduce an innovative framework for feature selection, which is guided by knockoff features and optimized through reinforcement learning, to identify the optimal and effective feature subset. In detail, our method involves gener
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;PromptCharm&#31995;&#32479;&#65292;&#36890;&#36807;&#22810;&#27169;&#24577;&#25552;&#31034;&#24037;&#31243;&#21644;&#20248;&#21270;&#65292;&#25903;&#25345;&#26032;&#25163;&#29992;&#25143;&#36827;&#34892;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2403.04014</link><description>&lt;p&gt;
PromptCharm&#65306;&#36890;&#36807;&#22810;&#27169;&#24577;&#25552;&#31034;&#21644;&#20248;&#21270;&#36827;&#34892;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
PromptCharm: Text-to-Image Generation through Multi-modal Prompting and Refinement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04014
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;PromptCharm&#31995;&#32479;&#65292;&#36890;&#36807;&#22810;&#27169;&#24577;&#25552;&#31034;&#24037;&#31243;&#21644;&#20248;&#21270;&#65292;&#25903;&#25345;&#26032;&#25163;&#29992;&#25143;&#36827;&#34892;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#26368;&#26032;&#36827;&#23637;&#26174;&#33879;&#25512;&#21160;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;&#26368;&#20808;&#36827;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;Stable Diffusion&#29616;&#22312;&#33021;&#22815;&#21512;&#25104;&#20855;&#26377;&#24378;&#28872;&#32654;&#23398;&#24863;&#30340;&#39640;&#36136;&#37327;&#22270;&#20687;&#12290;&#22240;&#27492;&#65292;&#21046;&#20316;&#31526;&#21512;&#27169;&#22411;&#35299;&#37322;&#21644;&#29992;&#25143;&#24847;&#22270;&#30340;&#25991;&#26412;&#25552;&#31034;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#20197;&#21450;&#38656;&#35201;&#36890;&#36807;&#36845;&#20195;&#32534;&#36753;&#21644;&#20248;&#21270;&#25991;&#26412;&#25552;&#31034;&#65292;&#22240;&#27492;&#23545;&#26032;&#25163;&#29992;&#25143;&#26469;&#35828;&#65292;&#25552;&#31034;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PromptCharm&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#36807;&#22810;&#27169;&#24577;&#25552;&#31034;&#24037;&#31243;&#21644;&#20248;&#21270;&#20419;&#36827;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#30340;&#28151;&#21512;&#20513;&#35758;&#31995;&#32479;&#12290;&#20026;&#20102;&#24110;&#21161;&#26032;&#25163;&#29992;&#25143;&#36827;&#34892;&#25552;&#31034;&#65292;PromptCharm&#39318;&#20808;&#33258;&#21160;&#20248;&#21270;&#21644;&#20248;&#21270;&#29992;&#25143;&#30340;&#21021;&#22987;&#25552;&#31034;&#12290;&#27492;&#22806;&#65292;PromptCharm&#25903;&#25345;&#29992;&#25143;&#25506;&#32034;&#21644;&#36873;&#25321;&#19981;&#21516;&#30340;&#22270;&#20687;&#39118;&#26684;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04014v1 Announce Type: cross  Abstract: The recent advancements in Generative AI have significantly advanced the field of text-to-image generation. The state-of-the-art text-to-image model, Stable Diffusion, is now capable of synthesizing high-quality images with a strong sense of aesthetics. Crafting text prompts that align with the model's interpretation and the user's intent thus becomes crucial. However, prompting remains challenging for novice users due to the complexity of the stable diffusion model and the non-trivial efforts required for iteratively editing and refining the text prompts. To address these challenges, we propose PromptCharm, a mixed-initiative system that facilitates text-to-image creation through multi-modal prompt engineering and refinement. To assist novice users in prompting, PromptCharm first automatically refines and optimizes the user's initial prompt. Furthermore, PromptCharm supports the user in exploring and selecting different image styles w
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20855;&#26377;&#21452;&#21521;&#36827;&#27493;&#31070;&#32463;&#32593;&#32476;&#30340;&#24773;&#33410;&#22238;&#24402;&#36827;&#23637;&#65288;ERP-BPNN&#65289;&#30340;&#26032;&#22411;&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20154;&#31867;&#33324;&#20132;&#21449;&#30340;&#26041;&#24335;&#23398;&#20064;&#65292;&#23454;&#29616;&#33258;&#20027;&#20219;&#21153;&#20999;&#25442;&#65292;&#24182;&#20801;&#35768;&#22312;&#20219;&#21153;&#20043;&#38388;&#36827;&#34892;&#21452;&#21521;&#25216;&#33021;&#20256;&#36882;&#12290;</title><link>https://arxiv.org/abs/2403.04001</link><description>&lt;p&gt;
&#20855;&#26377;&#21452;&#21521;&#36827;&#27493;&#31070;&#32463;&#32593;&#32476;&#21644;&#24773;&#33410;&#22238;&#24402;&#36827;&#23637;&#30340;&#26032;&#20852;&#20219;&#21153;&#25490;&#24207;&#21644;&#26426;&#22120;&#20154;&#25216;&#33021;&#36801;&#31227;
&lt;/p&gt;
&lt;p&gt;
Bidirectional Progressive Neural Networks with Episodic Return Progress for Emergent Task Sequencing and Robotic Skill Transfer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04001
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20855;&#26377;&#21452;&#21521;&#36827;&#27493;&#31070;&#32463;&#32593;&#32476;&#30340;&#24773;&#33410;&#22238;&#24402;&#36827;&#23637;&#65288;ERP-BPNN&#65289;&#30340;&#26032;&#22411;&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20154;&#31867;&#33324;&#20132;&#21449;&#30340;&#26041;&#24335;&#23398;&#20064;&#65292;&#23454;&#29616;&#33258;&#20027;&#20219;&#21153;&#20999;&#25442;&#65292;&#24182;&#20801;&#35768;&#22312;&#20219;&#21153;&#20043;&#38388;&#36827;&#34892;&#21452;&#21521;&#25216;&#33021;&#20256;&#36882;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#22823;&#33041;&#21644;&#34892;&#20026;&#25552;&#20379;&#20102;&#19968;&#20010;&#20016;&#23500;&#30340;&#22330;&#26223;&#65292;&#21487;&#20197;&#21551;&#21457;&#26426;&#22120;&#20154;&#30340;&#26032;&#22411;&#25511;&#21046;&#21644;&#23398;&#20064;&#26041;&#27861;&#12290;&#20026;&#20102;&#36890;&#36807;&#28608;&#21457;&#20154;&#31867;&#22914;&#20309;&#33719;&#21462;&#30693;&#35782;&#24182;&#22312;&#20219;&#21153;&#20043;&#38388;&#20256;&#36882;&#25216;&#33021;&#26469;&#23637;&#31034;&#36825;&#26679;&#19968;&#31181;&#21457;&#23637;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#20855;&#26377;&#21452;&#21521;&#36827;&#27493;&#31070;&#32463;&#32593;&#32476;&#30340;&#24773;&#33410;&#22238;&#24402;&#36827;&#23637;&#65288;ERP-BPNN&#65289;&#30340;&#26032;&#22411;&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#12290;&#25552;&#20986;&#30340;ERP-BPNN&#27169;&#22411;&#65288;1&#65289;&#36890;&#36807;&#20154;&#31867;&#33324;&#20132;&#21449;&#30340;&#26041;&#24335;&#23398;&#20064;&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#20869;&#22312;&#28608;&#21169;&#20449;&#21495;&#23454;&#29616;&#33258;&#20027;&#20219;&#21153;&#20999;&#25442;&#65292;&#24182;&#19988;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;&#65288;3&#65289;&#20801;&#35768;&#22312;&#20219;&#21153;&#20043;&#38388;&#36827;&#34892;&#21452;&#21521;&#25216;&#33021;&#20256;&#36882;&#12290;ERP-BPNN&#26159;&#19968;&#20010;&#36890;&#29992;&#26550;&#26500;&#65292;&#36866;&#29992;&#20110;&#20960;&#31181;&#22810;&#20219;&#21153;&#23398;&#20064;&#35774;&#32622;&#65307;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20854;&#31070;&#32463;&#32467;&#26500;&#30340;&#32454;&#33410;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#19981;&#21516;&#26426;&#22120;&#20154;&#24418;&#24577;&#30340;&#35302;&#30896;&#20219;&#21153;&#20013;&#23454;&#29616;&#26377;&#25928;&#23398;&#20064;&#21644;&#25216;&#33021;&#20256;&#36882;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04001v1 Announce Type: cross  Abstract: Human brain and behavior provide a rich venue that can inspire novel control and learning methods for robotics. In an attempt to exemplify such a development by inspiring how humans acquire knowledge and transfer skills among tasks, we introduce a novel multi-task reinforcement learning framework named Episodic Return Progress with Bidirectional Progressive Neural Networks (ERP-BPNN). The proposed ERP-BPNN model (1) learns in a human-like interleaved manner by (2) autonomous task switching based on a novel intrinsic motivation signal and, in contrast to existing methods, (3) allows bidirectional skill transfer among tasks. ERP-BPNN is a general architecture applicable to several multi-task learning settings; in this paper, we present the details of its neural architecture and show its ability to enable effective learning and skill transfer among morphologically different robots in a reaching task. The developed Bidirectional Progressiv
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24418;&#24335;&#21512;&#25104;&#22522;&#20934;&#19978;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#21015;&#20030;&#31639;&#27861;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21512;&#25104;&#31639;&#27861;&#65292;&#20197;&#22312;&#36845;&#20195;&#24490;&#29615;&#20013;&#25552;&#20379;&#35821;&#27861;&#25351;&#23548;&#21644;&#20449;&#24687;&#20132;&#27969;&#12290;</title><link>https://arxiv.org/abs/2403.03997</link><description>&lt;p&gt;
&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24341;&#23548;&#21015;&#20030;&#24335;&#31243;&#24207;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Guiding Enumerative Program Synthesis with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03997
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24418;&#24335;&#21512;&#25104;&#22522;&#20934;&#19978;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#21015;&#20030;&#31639;&#27861;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21512;&#25104;&#31639;&#27861;&#65292;&#20197;&#22312;&#36845;&#20195;&#24490;&#29615;&#20013;&#25552;&#20379;&#35821;&#27861;&#25351;&#23548;&#21644;&#20449;&#24687;&#20132;&#27969;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#24320;&#22987;&#20027;&#23548;&#22260;&#32469;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#35268;&#33539;&#29983;&#25104;&#20195;&#30721;&#30340;&#35752;&#35770;&#12290;&#19982;&#27492;&#30456;&#27604;&#65292;&#22312;&#31934;&#30830;&#36923;&#36753;&#35268;&#33539;&#39046;&#22495;&#20013;&#34920;&#29616;&#26368;&#20339;&#30340;&#21512;&#25104;&#22120;&#20173;&#28982;&#22522;&#20110;&#21015;&#20030;&#31639;&#27861;&#12290;&#26412;&#25991;&#36890;&#36807;&#31934;&#24515;&#21046;&#20316;&#39046;&#22495;&#25552;&#31034;&#24211;&#35780;&#20272;&#20102;LLMs&#35299;&#20915;&#24418;&#24335;&#21512;&#25104;&#22522;&#20934;&#30340;&#33021;&#21147;&#12290;&#24403;&#19968;&#27425;&#21512;&#25104;&#22833;&#36133;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21015;&#20030;&#24335;&#21512;&#25104;&#31639;&#27861;&#65292;&#23558;LLM&#30340;&#35843;&#29992;&#38598;&#25104;&#21040;&#21152;&#26435;&#27010;&#29575;&#25628;&#32034;&#20013;&#12290;&#36825;&#26679;&#65292;&#21512;&#25104;&#22120;&#21487;&#20197;&#21521;LLM&#25552;&#20379;&#26377;&#20851;&#26522;&#20030;&#22120;&#36827;&#23637;&#24773;&#20917;&#30340;&#20449;&#24687;&#65292;LLM&#21487;&#20197;&#21521;&#26522;&#20030;&#22120;&#22312;&#36845;&#20195;&#24490;&#29615;&#20013;&#25552;&#20379;&#35821;&#27861;&#25351;&#23548;&#12290;&#25105;&#20204;&#22312;&#26469;&#33258;&#35821;&#27861;&#24341;&#23548;&#21512;&#25104; (SyGuS) &#31454;&#36187;&#30340;&#22522;&#20934;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03997v1 Announce Type: new  Abstract: Pre-trained Large Language Models (LLMs) are beginning to dominate the discourse around automatic code generation with natural language specifications. In contrast, the best-performing synthesizers in the domain of formal synthesis with precise logical specifications are still based on enumerative algorithms. In this paper, we evaluate the abilities of LLMs to solve formal synthesis benchmarks by carefully crafting a library of prompts for the domain. When one-shot synthesis fails, we propose a novel enumerative synthesis algorithm, which integrates calls to an LLM into a weighted probabilistic search. This allows the synthesizer to provide the LLM with information about the progress of the enumerator, and the LLM to provide the enumerator with syntactic guidance in an iterative loop. We evaluate our techniques on benchmarks from the Syntax-Guided Synthesis (SyGuS) competition. We find that GPT-3.5 as a stand-alone tool for formal synthe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20581;&#24247;&#39046;&#22495;&#35270;&#35282;&#25552;&#20986;&#20102;&#26032;&#30340;&#22478;&#24066;&#27946;&#27700;&#39118;&#38505;&#35780;&#20272;&#26694;&#26550;&#65292;&#32858;&#28966;&#20110;&#22266;&#26377;&#26131;&#24863;&#24615;&#12289;&#32531;&#35299;&#31574;&#30053;&#21644;&#22806;&#37096;&#21387;&#21147;&#22240;&#32032;&#19977;&#22823;&#25903;&#26609;&#65292;&#26088;&#22312;&#20174;&#26032;&#30340;&#35270;&#35282;&#35780;&#20272;&#39640;&#27946;&#27700;&#39118;&#38505;&#36335;&#24452;&#12290;</title><link>https://arxiv.org/abs/2403.03996</link><description>&lt;p&gt;
&#36890;&#36807;&#35843;&#25972;&#20581;&#24247;&#39046;&#22495;&#35270;&#35282;&#37325;&#26032;&#24605;&#32771;&#22478;&#24066;&#27946;&#27700;&#39118;&#38505;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Rethinking Urban Flood Risk Assessment By Adapting Health Domain Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03996
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20581;&#24247;&#39046;&#22495;&#35270;&#35282;&#25552;&#20986;&#20102;&#26032;&#30340;&#22478;&#24066;&#27946;&#27700;&#39118;&#38505;&#35780;&#20272;&#26694;&#26550;&#65292;&#32858;&#28966;&#20110;&#22266;&#26377;&#26131;&#24863;&#24615;&#12289;&#32531;&#35299;&#31574;&#30053;&#21644;&#22806;&#37096;&#21387;&#21147;&#22240;&#32032;&#19977;&#22823;&#25903;&#26609;&#65292;&#26088;&#22312;&#20174;&#26032;&#30340;&#35270;&#35282;&#35780;&#20272;&#39640;&#27946;&#27700;&#39118;&#38505;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#20581;&#24247;&#39118;&#38505;&#35780;&#20272;&#24605;&#24819;&#21551;&#21457;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#27946;&#27700;&#39118;&#38505;&#35780;&#20272;&#30340;&#26032;&#35270;&#35282;&#12290;&#25552;&#20986;&#30340;&#35270;&#35282;&#20027;&#35201;&#20851;&#27880;&#19977;&#20010;&#25903;&#26609;&#26469;&#30740;&#31350;&#27946;&#27700;&#39118;&#38505;&#65306;(1)&#22266;&#26377;&#26131;&#24863;&#24615;&#65292;(2)&#32531;&#35299;&#31574;&#30053;&#65292;&#20197;&#21450;(3)&#22806;&#37096;&#21387;&#21147;&#22240;&#32032;&#12290;&#36825;&#20123;&#25903;&#26609;&#20849;&#21516;&#28085;&#30422;&#20102;&#22478;&#24066;&#22320;&#21306;&#30340;&#29289;&#29702;&#21644;&#29615;&#22659;&#29305;&#24449;&#65292;&#20154;&#31867;&#24178;&#39044;&#25514;&#26045;&#30340;&#26377;&#25928;&#24615;&#21644;&#26080;&#27861;&#25511;&#21046;&#30340;&#22806;&#37096;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#20026;&#35299;&#30721;&#27946;&#27700;&#39118;&#38505;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;&#23545;&#20110;&#27599;&#20010;&#25903;&#26609;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#20854;&#23545;&#27946;&#27700;&#39118;&#38505;&#30340;&#20010;&#21035;&#36129;&#29486;&#65292;&#24182;&#38416;&#26126;&#20102;&#23427;&#20204;&#30340;&#30456;&#20114;&#20316;&#29992;&#21644;&#24635;&#20307;&#24433;&#21709;&#12290;&#36825;&#19968;&#19977;&#25903;&#26609;&#27169;&#22411;&#20307;&#29616;&#20102;&#20174;&#31934;&#30830;&#24314;&#27169;&#21644;&#37327;&#21270;&#27946;&#27700;&#39118;&#38505;&#30340;&#36861;&#27714;&#21040;&#35780;&#20272;&#39640;&#27946;&#27700;&#39118;&#38505;&#36335;&#24452;&#30340;&#20851;&#27880;&#36716;&#21464;&#12290;&#36825;&#31181;&#35270;&#35282;&#30340;&#36716;&#21464;&#26088;&#22312;&#20943;&#36731;&#23545;&#32454;&#20998;&#36776;&#29575;&#37327;&#21270;&#21644;&#39044;&#27979;&#27946;&#27700;&#39118;&#38505;&#20316;&#20026;&#35299;&#33647;&#30340;&#36861;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03996v1 Announce Type: new  Abstract: Inspired by ideas from health risk assessment, this paper presents a new perspective for flood risk assessment. The proposed perspective focuses on three pillars for examining flood risk: (1) inherent susceptibility, (2) mitigation strategies, and (3) external stressors. These pillars collectively encompass the physical and environmental characteristics of urban areas, the effectiveness of human-intervention measures, and the influence of uncontrollable external factors, offering a fresh point of view for decoding flood risks. For each pillar, we delineate its individual contributions to flood risk and illustrate their interactive and overall impact. The three-pillars model embodies a shift in focus from the quest to precisely model and quantify flood risk to evaluating pathways to high flood risk. The shift in perspective is intended to alleviate the quest for quantifying and predicting flood risk at fine resolutions as a panacea for en
&lt;/p&gt;</description></item><item><title>&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20010;&#24615;&#21270;&#36127;&#37319;&#26679;&#25216;&#26415;&#22312;&#22686;&#37327;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#35299;&#20915;&#20102;&#26356;&#26032;&#25512;&#33616;&#31995;&#32479;&#27169;&#22411;&#26102;&#36935;&#21040;&#30340;&#36951;&#24536;&#28798;&#38590;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.03993</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#36127;&#37319;&#26679;&#22312;&#25512;&#33616;&#31995;&#32479;&#22686;&#37327;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Personalized Negative Reservoir for Incremental Learning in Recommender Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03993
&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20010;&#24615;&#21270;&#36127;&#37319;&#26679;&#25216;&#26415;&#22312;&#22686;&#37327;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#35299;&#20915;&#20102;&#26356;&#26032;&#25512;&#33616;&#31995;&#32479;&#27169;&#22411;&#26102;&#36935;&#21040;&#30340;&#36951;&#24536;&#28798;&#38590;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#24050;&#25104;&#20026;&#22312;&#32447;&#24179;&#21488;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#27599;&#22825;&#35757;&#32451;&#25968;&#25454;&#37327;&#19981;&#26029;&#25193;&#22823;&#65292;&#29992;&#25143;&#20114;&#21160;&#27425;&#25968;&#19981;&#26029;&#22686;&#21152;&#12290;&#25506;&#32034;&#26356;&#22823;&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;&#27169;&#22411;&#24050;&#25104;&#20026;&#25913;&#21892;&#29992;&#25143;&#20307;&#39564;&#30340;&#24517;&#35201;&#36861;&#27714;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#36827;&#23637;&#24102;&#26469;&#20102;&#26356;&#22823;&#30340;&#35745;&#31639;&#36127;&#25285;&#12290;&#22312;&#21830;&#19994;&#29615;&#22659;&#20013;&#65292;&#19968;&#26086;&#25512;&#33616;&#31995;&#32479;&#27169;&#22411;&#34987;&#35757;&#32451;&#21644;&#37096;&#32626;&#65292;&#36890;&#24120;&#38656;&#35201;&#39057;&#32321;&#26356;&#26032;&#20197;&#36866;&#24212;&#26032;&#30340;&#23458;&#25143;&#25968;&#25454;&#12290;&#32047;&#31215;&#36215;&#26469;&#65292;&#25968;&#25454;&#37327;&#30340;&#22686;&#21152;&#24517;&#23558;&#20351;&#24471;&#20174;&#22836;&#24320;&#22987;&#36827;&#34892;&#20840;&#37327;&#37325;&#35757;&#32451;&#21464;&#24471;&#35745;&#31639;&#19978;&#19981;&#21487;&#34892;&#12290;&#20165;&#20165;&#22312;&#26032;&#25968;&#25454;&#19978;&#36827;&#34892;&#31616;&#21333;&#24494;&#35843;&#20250;&#36935;&#21040;&#24050;&#34987;&#24191;&#27867;&#35760;&#24405;&#30340;&#36951;&#24536;&#28798;&#38590;&#38382;&#39064;&#12290;&#23613;&#31649;&#36127;&#37319;&#26679;&#22312;&#20351;&#29992;&#38544;&#24335;&#21453;&#39304;&#36827;&#34892;&#35757;&#32451;&#20013;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#19968;&#37096;&#20998;&#65292;&#20294;&#30446;&#21069;&#24182;&#19981;&#23384;&#22312;&#19987;&#38376;&#38024;&#23545;&#22686;&#37327;&#23398;&#20064;&#30340;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03993v1 Announce Type: cross  Abstract: Recommender systems have become an integral part of online platforms. Every day the volume of training data is expanding and the number of user interactions is constantly increasing. The exploration of larger and more expressive models has become a necessary pursuit to improve user experience. However, this progression carries with it an increased computational burden. In commercial settings, once a recommendation system model has been trained and deployed it typically needs to be updated frequently as new client data arrive. Cumulatively, the mounting volume of data is guaranteed to eventually make full batch retraining of the model from scratch computationally infeasible. Naively fine-tuning solely on the new data runs into the well-documented problem of catastrophic forgetting. Despite the fact that negative sampling is a crucial part of training with implicit feedback, no specialized technique exists that is tailored to the increme
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#36827;&#21270;&#31639;&#27861;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#29983;&#25104;&#29992;&#20110;&#35782;&#21035;&#20851;&#38190;&#33410;&#28857;&#30340;&#20989;&#25968;&#8220;score_nodes&#8221;&#65292;&#36890;&#36807;&#24378;&#22823;&#30340;&#19978;&#19979;&#25991;&#29702;&#35299;&#33021;&#21147;&#21644;&#20016;&#23500;&#30340;&#32534;&#31243;&#25216;&#33021;&#29983;&#25104;&#20986;&#33394;&#30340;&#26032;&#20989;&#25968;&#65292;&#20197;&#30830;&#20445;&#32676;&#20307;&#30340;&#31283;&#23450;&#21457;&#23637;&#21516;&#26102;&#20445;&#25345;&#22810;&#26679;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.03962</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35782;&#21035;&#22797;&#26434;&#32593;&#32476;&#20013;&#30340;&#20851;&#38190;&#33410;&#28857;
&lt;/p&gt;
&lt;p&gt;
Identify Critical Nodes in Complex Network with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03962
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#36827;&#21270;&#31639;&#27861;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#29983;&#25104;&#29992;&#20110;&#35782;&#21035;&#20851;&#38190;&#33410;&#28857;&#30340;&#20989;&#25968;&#8220;score_nodes&#8221;&#65292;&#36890;&#36807;&#24378;&#22823;&#30340;&#19978;&#19979;&#25991;&#29702;&#35299;&#33021;&#21147;&#21644;&#20016;&#23500;&#30340;&#32534;&#31243;&#25216;&#33021;&#29983;&#25104;&#20986;&#33394;&#30340;&#26032;&#20989;&#25968;&#65292;&#20197;&#30830;&#20445;&#32676;&#20307;&#30340;&#31283;&#23450;&#21457;&#23637;&#21516;&#26102;&#20445;&#25345;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32593;&#32476;&#20013;&#35782;&#21035;&#20851;&#38190;&#33410;&#28857;&#26159;&#19968;&#20010;&#32463;&#20856;&#30340;&#20915;&#31574;&#20219;&#21153;&#65292;&#35768;&#22810;&#26041;&#27861;&#22312;&#21487;&#36866;&#24212;&#24615;&#21644;&#25928;&#29992;&#20043;&#38388;&#38590;&#20197;&#21462;&#24471;&#24179;&#34913;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#36827;&#21270;&#31639;&#27861;&#65288;EA&#65289;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30456;&#32467;&#21512;&#65292;&#29983;&#25104;&#19968;&#20010;&#21517;&#20026;&#8220;score_nodes&#8221;&#30340;&#20989;&#25968;&#65292;&#35813;&#20989;&#25968;&#21487;&#20197;&#36827;&#19968;&#27493;&#29992;&#20110;&#26681;&#25454;&#20854;&#20998;&#37197;&#30340;&#20998;&#25968;&#35782;&#21035;&#20851;&#38190;&#33410;&#28857;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21253;&#25324;&#19977;&#20010;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#65306;&#25163;&#21160;&#21021;&#22987;&#21270;&#12289;&#32676;&#20307;&#31649;&#29702;&#21644;&#22522;&#20110;LLMs&#30340;&#36827;&#21270;&#12290;&#23427;&#20174;&#26368;&#21021;&#30340;&#20154;&#21475;&#32676;&#20307;&#20013;&#28436;&#21270;&#20986;&#19968;&#32452;&#25163;&#21160;&#21019;&#24314;&#30340;&#35774;&#35745;&#33410;&#28857;&#35780;&#20998;&#20989;&#25968;&#12290;LLMs&#21033;&#29992;&#20854;&#24378;&#22823;&#30340;&#19978;&#19979;&#25991;&#29702;&#35299;&#33021;&#21147;&#21644;&#20016;&#23500;&#30340;&#32534;&#31243;&#25216;&#33021;&#23545;&#20010;&#20307;&#25191;&#34892;&#20132;&#21449;&#21644;&#31361;&#21464;&#25805;&#20316;&#65292;&#29983;&#25104;&#20986;&#33394;&#30340;&#26032;&#20989;&#25968;&#12290;&#28982;&#21518;&#23545;&#36825;&#20123;&#20989;&#25968;&#36827;&#34892;&#20998;&#31867;&#12289;&#25490;&#21517;&#21644;&#28040;&#38500;&#65292;&#20197;&#30830;&#20445;&#32676;&#20307;&#30340;&#31283;&#23450;&#21457;&#23637;&#21516;&#26102;&#20445;&#25345;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03962v1 Announce Type: cross  Abstract: Identifying critical nodes in networks is a classical decision-making task, and many methods struggle to strike a balance between adaptability and utility. Therefore, we propose an approach that empowers Evolutionary Algorithm (EA) with Large Language Models (LLMs), to generate a function called "score\_nodes" which can further be used to identify crucial nodes based on their assigned scores. Our model consists of three main components: Manual Initialization, Population Management, and LLMs-based Evolution. It evolves from initial populations with a set of designed node scoring functions created manually. LLMs leverage their strong contextual understanding and rich programming skills to perform crossover and mutation operations on the individuals, generating excellent new functions. These functions are then categorized, ranked, and eliminated to ensure the stable development of the populations while preserving diversity. Extensive expe
&lt;/p&gt;</description></item><item><title>&#36816;&#29992;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#31354;&#38388;&#36164;&#28304;&#20998;&#37197;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#20855;&#26377;&#24555;&#36895;&#35299;&#20915;&#26041;&#27861;&#25910;&#25947;&#21644;&#24378;&#22823;&#30340;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#31561;&#20248;&#21183;&#65292;&#20026;&#36825;&#19968;&#38382;&#39064;&#39046;&#22495;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;</title><link>https://arxiv.org/abs/2403.03643</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#31354;&#38388;&#36164;&#28304;&#20998;&#37197;&#20013;&#30340;&#24212;&#29992;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey on Applications of Reinforcement Learning in Spatial Resource Allocation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03643
&lt;/p&gt;
&lt;p&gt;
&#36816;&#29992;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#31354;&#38388;&#36164;&#28304;&#20998;&#37197;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#20855;&#26377;&#24555;&#36895;&#35299;&#20915;&#26041;&#27861;&#25910;&#25947;&#21644;&#24378;&#22823;&#30340;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#31561;&#20248;&#21183;&#65292;&#20026;&#36825;&#19968;&#38382;&#39064;&#39046;&#22495;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#38388;&#36164;&#28304;&#20998;&#37197;&#30340;&#25361;&#25112;&#22312;&#20132;&#36890;&#36816;&#36755;&#12289;&#24037;&#19994;&#21644;&#26085;&#24120;&#29983;&#27963;&#31561;&#21508;&#20010;&#39046;&#22495;&#26222;&#36941;&#23384;&#22312;&#12290;&#38543;&#30528;&#29616;&#23454;&#19990;&#30028;&#38382;&#39064;&#35268;&#27169;&#19981;&#26029;&#25193;&#22823;&#20197;&#21450;&#23545;&#23454;&#26102;&#35299;&#20915;&#26041;&#26696;&#30340;&#38656;&#27714;&#22686;&#21152;&#65292;&#20256;&#32479;&#31639;&#27861;&#38754;&#20020;&#30528;&#24040;&#22823;&#30340;&#35745;&#31639;&#21387;&#21147;&#65292;&#38590;&#20197;&#23454;&#29616;&#26368;&#20339;&#25928;&#29575;&#21644;&#23454;&#26102;&#33021;&#21147;&#12290;&#36817;&#24180;&#26469;&#65292;&#38543;&#30528;&#35745;&#31639;&#26426;&#35745;&#31639;&#33021;&#21147;&#30340;&#19981;&#26029;&#25552;&#21319;&#65292;&#24378;&#21270;&#23398;&#20064;&#22312;&#35832;&#22914;&#22260;&#26827;&#21644;&#26426;&#22120;&#20154;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#23601;&#65292;&#23637;&#31034;&#20102;&#20854;&#24378;&#22823;&#30340;&#23398;&#20064;&#21644;&#24207;&#36143;&#20915;&#31574;&#33021;&#21147;&#12290;&#37492;&#20110;&#36825;&#20123;&#36827;&#23637;&#65292;&#36817;&#24180;&#26469;&#20986;&#29616;&#20102;&#22823;&#37327;&#36816;&#29992;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#31354;&#38388;&#36164;&#28304;&#20998;&#37197;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#20855;&#26377;&#24555;&#36895;&#35299;&#20915;&#26041;&#27861;&#25910;&#25947;&#21644;&#24378;&#22823;&#30340;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#31561;&#20248;&#21183;&#65292;&#20026;&#35299;&#20915;&#31354;&#38388;&#36164;&#28304;&#20998;&#37197;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03643v1 Announce Type: cross  Abstract: The challenge of spatial resource allocation is pervasive across various domains such as transportation, industry, and daily life. As the scale of real-world issues continues to expand and demands for real-time solutions increase, traditional algorithms face significant computational pressures, struggling to achieve optimal efficiency and real-time capabilities. In recent years, with the escalating computational power of computers, the remarkable achievements of reinforcement learning in domains like Go and robotics have demonstrated its robust learning and sequential decision-making capabilities. Given these advancements, there has been a surge in novel methods employing reinforcement learning to tackle spatial resource allocation problems. These methods exhibit advantages such as rapid solution convergence and strong model generalization abilities, offering a new perspective on resolving spatial resource allocation problems. Therefor
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;DLP-GAN&#27169;&#22411;&#65292;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#23454;&#29616;&#20102;&#29616;&#20195;&#20013;&#22269;&#39118;&#26223;&#29031;&#29255;&#30340;&#32472;&#21046;&#65292;&#24341;&#20837;&#20102;&#19981;&#23545;&#31216;&#24490;&#29615;&#26144;&#23556;&#21644;&#21452;&#19968;&#33268;&#24615;&#25439;&#22833;&#26469;&#24179;&#34913;&#29616;&#23454;&#24863;&#21644;&#25277;&#35937;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.03456</link><description>&lt;p&gt;
DLP-GAN&#65306;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#23398;&#20064;&#32472;&#21046;&#29616;&#20195;&#20013;&#22269;&#39118;&#26223;&#29031;&#29255;
&lt;/p&gt;
&lt;p&gt;
DLP-GAN: Learning to Draw Modern Chinese Landscape Photos with Generative Adversarial Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03456
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;DLP-GAN&#27169;&#22411;&#65292;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#23454;&#29616;&#20102;&#29616;&#20195;&#20013;&#22269;&#39118;&#26223;&#29031;&#29255;&#30340;&#32472;&#21046;&#65292;&#24341;&#20837;&#20102;&#19981;&#23545;&#31216;&#24490;&#29615;&#26144;&#23556;&#21644;&#21452;&#19968;&#33268;&#24615;&#25439;&#22833;&#26469;&#24179;&#34913;&#29616;&#23454;&#24863;&#21644;&#25277;&#35937;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20013;&#22269;&#23665;&#27700;&#30011;&#20855;&#26377;&#29420;&#29305;&#21644;&#33402;&#26415;&#30340;&#39118;&#26684;&#65292;&#20854;&#32472;&#30011;&#25216;&#26415;&#22312;&#33394;&#24425;&#20351;&#29992;&#21644;&#29289;&#20307;&#30340;&#36924;&#30495;&#34920;&#29616;&#19978;&#39640;&#24230;&#25277;&#35937;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#20174;&#29616;&#20195;&#29031;&#29255;&#36716;&#25442;&#21040;&#21476;&#20195;&#27700;&#22696;&#30011;&#19978;&#65292;&#20294;&#24456;&#23569;&#20851;&#27880;&#23558;&#39118;&#26223;&#30011;&#36716;&#21270;&#20026;&#29616;&#20195;&#29031;&#29255;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;DLP-GAN&#65288;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#32472;&#21046;&#29616;&#20195;&#20013;&#22269;&#39118;&#26223;&#29031;&#29255;&#65289;&#65292;&#19968;&#20010;&#20855;&#26377;&#26032;&#39062;&#30340;&#19981;&#23545;&#31216;&#24490;&#29615;&#26144;&#23556;&#30340;&#26080;&#30417;&#30563;&#36328;&#22495;&#22270;&#20687;&#36716;&#25442;&#26694;&#26550;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110;&#23494;&#38598;&#34701;&#21512;&#27169;&#22359;&#30340;&#29983;&#25104;&#22120;&#26469;&#21305;&#37197;&#19981;&#21516;&#30340;&#36716;&#25442;&#26041;&#21521;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#20102;&#21452;&#19968;&#33268;&#24615;&#25439;&#22833;&#20197;&#24179;&#34913;&#27169;&#22411;&#32472;&#30011;&#30340;&#36924;&#30495;&#24615;&#21644;&#25277;&#35937;&#24615;&#65292;&#20351;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#20197;&#29616;&#20195;&#24863;&#32472;&#21046;&#39118;&#26223;&#29031;&#29255;&#21644;&#32032;&#25551;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03456v1 Announce Type: cross  Abstract: Chinese landscape painting has a unique and artistic style, and its drawing technique is highly abstract in both the use of color and the realistic representation of objects. Previous methods focus on transferring from modern photos to ancient ink paintings. However, little attention has been paid to translating landscape paintings into modern photos. To solve such problems, in this paper, we (1) propose DLP-GAN (\textbf{D}raw Modern Chinese \textbf{L}andscape \textbf{P}hotos with \textbf{G}enerative \textbf{A}dversarial \textbf{N}etwork), an unsupervised cross-domain image translation framework with a novel asymmetric cycle mapping, and (2) introduce a generator based on a dense-fusion module to match different translation directions. Moreover, a dual-consistency loss is proposed to balance the realism and abstraction of model painting. In this way, our model can draw landscape photos and sketches in the modern sense. Finally, based o
&lt;/p&gt;</description></item><item><title>WMDP&#22522;&#20934;&#26159;&#19968;&#20010;&#20844;&#24320;&#21457;&#24067;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;4157&#20010;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#65292;&#29992;&#20316;&#29983;&#29289;&#23433;&#20840;&#12289;&#32593;&#32476;&#23433;&#20840;&#21644;&#21270;&#23398;&#23433;&#20840;&#21361;&#38505;&#30693;&#35782;&#30340;&#20195;&#29702;&#27979;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.03218</link><description>&lt;p&gt;
WMDP&#22522;&#20934;&#65306;&#36890;&#36807;&#36951;&#24536;&#27979;&#37327;&#21644;&#20943;&#23569;&#24694;&#24847;&#20351;&#29992;
&lt;/p&gt;
&lt;p&gt;
The WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03218
&lt;/p&gt;
&lt;p&gt;
WMDP&#22522;&#20934;&#26159;&#19968;&#20010;&#20844;&#24320;&#21457;&#24067;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;4157&#20010;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#65292;&#29992;&#20316;&#29983;&#29289;&#23433;&#20840;&#12289;&#32593;&#32476;&#23433;&#20840;&#21644;&#21270;&#23398;&#23433;&#20840;&#21361;&#38505;&#30693;&#35782;&#30340;&#20195;&#29702;&#27979;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03218v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;&#39046;&#22495; &#25688;&#35201;&#65306;&#30333;&#23467;&#20851;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#34892;&#25919;&#21629;&#20196;&#24378;&#35843;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36171;&#20104;&#24694;&#24847;&#34892;&#20026;&#32773;&#24320;&#21457;&#29983;&#29289;&#12289;&#32593;&#32476;&#21644;&#21270;&#23398;&#27494;&#22120;&#30340;&#39118;&#38505;&#12290;&#20026;&#20102;&#34913;&#37327;&#36825;&#20123;&#24694;&#24847;&#20351;&#29992;&#30340;&#39118;&#38505;&#65292;&#25919;&#24220;&#26426;&#26500;&#21644;&#20027;&#35201;&#20154;&#24037;&#26234;&#33021;&#23454;&#39564;&#23460;&#27491;&#22312;&#24320;&#21457;LLMs&#30340;&#21361;&#38505;&#33021;&#21147;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#35780;&#20272;&#26159;&#31169;&#20154;&#30340;&#65292;&#38459;&#30861;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#22914;&#20309;&#20943;&#23569;&#39118;&#38505;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#20165;&#19987;&#27880;&#20110;&#20960;&#26465;&#39640;&#24230;&#29305;&#23450;&#30340;&#24694;&#24847;&#20351;&#29992;&#36884;&#24452;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20123;&#31354;&#30333;&#65292;&#25105;&#20204;&#20844;&#24320;&#21457;&#24067;&#20102;&#22823;&#35268;&#27169;&#26432;&#20260;&#24615;&#27494;&#22120;&#20195;&#29702;&#65288;WMDP&#65289;&#22522;&#20934;&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;4157&#20010;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#65292;&#20316;&#20026;&#29983;&#29289;&#23433;&#20840;&#12289;&#32593;&#32476;&#23433;&#20840;&#21644;&#21270;&#23398;&#23433;&#20840;&#21361;&#38505;&#30693;&#35782;&#30340;&#20195;&#29702;&#27979;&#37327;&#12290;WMDP&#30001;&#19968;&#32452;&#23398;&#26415;&#30028;&#21644;&#25216;&#26415;&#39038;&#38382;&#32852;&#21512;&#24320;&#21457;&#65292;&#24182;&#22312;&#20844;&#24320;&#21457;&#24067;&#21069;&#20005;&#26684;&#36807;&#28388;&#20197;&#28040;&#38500;&#25935;&#24863;&#20449;&#24687;&#12290;WMDP&#26377;&#20004;&#20010;&#26381;&#21153;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03218v1 Announce Type: cross  Abstract: The White House Executive Order on Artificial Intelligence highlights the risks of large language models (LLMs) empowering malicious actors in developing biological, cyber, and chemical weapons. To measure these risks of malicious use, government institutions and major AI labs are developing evaluations for hazardous capabilities in LLMs. However, current evaluations are private, preventing further research into mitigating risk. Furthermore, they focus on only a few, highly specific pathways for malicious use. To fill these gaps, we publicly release the Weapons of Mass Destruction Proxy (WMDP) benchmark, a dataset of 4,157 multiple-choice questions that serve as a proxy measurement of hazardous knowledge in biosecurity, cybersecurity, and chemical security. WMDP was developed by a consortium of academics and technical consultants, and was stringently filtered to eliminate sensitive information prior to public release. WMDP serves two r
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#36890;&#29992;&#35745;&#31639;&#26426;&#25511;&#21046;&#65288;GCC&#65289;&#35774;&#32622;&#65292;&#36890;&#36807;&#22522;&#20110;&#23631;&#24149;&#22270;&#20687;&#21644;&#21487;&#33021;&#30340;&#38899;&#39057;&#36755;&#20837;&#30340;&#22522;&#37329;&#20195;&#29702;&#26694;&#26550;Cradle&#65292;&#23454;&#29616;&#20102;&#23545;&#12298;&#33618;&#37326;&#22823;&#38230;&#23458;2&#12299;&#20013;&#22797;&#26434;&#20219;&#21153;&#30340;&#25511;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.03186</link><description>&lt;p&gt;
&#36890;&#24448;&#36890;&#29992;&#35745;&#31639;&#26426;&#25511;&#21046;&#65306;&#22810;&#27169;&#24577;&#20195;&#29702;&#22312;&#12298;&#33618;&#37326;&#22823;&#38230;&#23458;2&#12299;&#20013;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards General Computer Control: A Multimodal Agent for Red Dead Redemption II as a Case Study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03186
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#36890;&#29992;&#35745;&#31639;&#26426;&#25511;&#21046;&#65288;GCC&#65289;&#35774;&#32622;&#65292;&#36890;&#36807;&#22522;&#20110;&#23631;&#24149;&#22270;&#20687;&#21644;&#21487;&#33021;&#30340;&#38899;&#39057;&#36755;&#20837;&#30340;&#22522;&#37329;&#20195;&#29702;&#26694;&#26550;Cradle&#65292;&#23454;&#29616;&#20102;&#23545;&#12298;&#33618;&#37326;&#22823;&#38230;&#23458;2&#12299;&#20013;&#22797;&#26434;&#20219;&#21153;&#30340;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22522;&#20110;&#22522;&#37329;&#30340;&#20195;&#29702;&#22312;&#29305;&#23450;&#20219;&#21153;&#25110;&#22330;&#26223;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#20195;&#29702;&#26080;&#27861;&#36328;&#19981;&#21516;&#22330;&#26223;&#27867;&#21270;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#23427;&#20204;&#22810;&#26679;&#21270;&#30340;&#35266;&#23519;&#21644;&#34892;&#21160;&#31354;&#38388;&#20197;&#21450;&#35821;&#20041;&#24046;&#36317;&#65292;&#25110;&#20381;&#36182;&#20110;&#29305;&#23450;&#20219;&#21153;&#30340;&#36164;&#28304;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#29992;&#35745;&#31639;&#26426;&#25511;&#21046;&#65288;GCC&#65289;&#35774;&#32622;&#65306;&#36890;&#36807;&#20165;&#33719;&#21462;&#35745;&#31639;&#26426;&#30340;&#23631;&#24149;&#22270;&#20687;&#65288;&#20197;&#21450;&#21487;&#33021;&#30340;&#38899;&#39057;&#65289;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#20135;&#29983;&#38190;&#30424;&#21644;&#40736;&#26631;&#25805;&#20316;&#20316;&#20026;&#36755;&#20986;&#65292;&#31867;&#20284;&#20110;&#20154;&#26426;&#20132;&#20114;&#65292;&#26500;&#24314;&#21487;&#20197;&#31934;&#36890;&#20219;&#20309;&#35745;&#31639;&#26426;&#20219;&#21153;&#30340;&#22522;&#37329;&#20195;&#29702;&#12290;&#20026;&#20102;&#38024;&#23545;GCC&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Cradle&#65292;&#19968;&#20010;&#20195;&#29702;&#26694;&#26550;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#21253;&#25324;&#33258;&#25105;&#21453;&#24605;&#12289;&#20219;&#21153;&#25512;&#29702;&#21644;&#25216;&#33021;&#25972;&#29702;&#65292;&#20197;&#30830;&#20445;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#27867;&#21270;&#21644;&#33258;&#25105;&#25913;&#36827;&#12290;&#20026;&#20102;&#23637;&#31034;Cradle&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#23558;&#20854;&#37096;&#32626;&#22312;&#22797;&#26434;&#30340;AAA&#28216;&#25103;&#12298;&#33618;&#37326;&#22823;&#38230;&#23458;2&#12299;&#20013;&#65292;&#20316;&#20026;&#36890;&#21521;G&#30340;&#21021;&#27493;&#23581;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03186v1 Announce Type: new  Abstract: Recent studies have demonstrated the success of foundation agents in specific tasks or scenarios. However, existing agents cannot generalize across different scenarios, mainly due to their diverse observation and action spaces and semantic gaps, or reliance on task-specific resources. In this work, we propose the General Computer Control (GCC) setting: building foundation agents that can master any computer task by taking only screen images (and possibly audio) of the computer as input, and producing keyboard and mouse operations as output, similar to human-computer interaction. To target GCC, we propose Cradle, an agent framework with strong reasoning abilities, including self-reflection, task inference, and skill curation, to ensure generalizability and self-improvement across various tasks. To demonstrate the capabilities of Cradle, we deploy it in the complex AAA game Red Dead Redemption II, serving as a preliminary attempt towards G
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#21160;&#24577;&#31639;&#27861;&#36873;&#25321;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#35757;&#32451;&#20195;&#29702;&#26681;&#25454;&#20248;&#21270;&#36807;&#31243;&#20013;&#35266;&#23519;&#21040;&#30340;&#29305;&#24449;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#31639;&#27861;&#65292;&#20197;&#35299;&#20915;&#21333;&#20010;&#31639;&#27861;&#26377;&#25928;&#24615;&#22312;&#19981;&#21516;&#38382;&#39064;&#23454;&#20363;&#19978;&#21464;&#21270;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.02131</link><description>&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#21160;&#24577;&#31639;&#27861;&#36873;&#25321;&#65306;&#20197;&#24494;&#20998;&#36827;&#21270;&#20026;&#20363;&#30340;&#21407;&#29702;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning for Dynamic Algorithm Selection: A Proof-of-Principle Study on Differential Evolution
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02131
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#21160;&#24577;&#31639;&#27861;&#36873;&#25321;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#35757;&#32451;&#20195;&#29702;&#26681;&#25454;&#20248;&#21270;&#36807;&#31243;&#20013;&#35266;&#23519;&#21040;&#30340;&#29305;&#24449;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#31639;&#27861;&#65292;&#20197;&#35299;&#20915;&#21333;&#20010;&#31639;&#27861;&#26377;&#25928;&#24615;&#22312;&#19981;&#21516;&#38382;&#39064;&#23454;&#20363;&#19978;&#21464;&#21270;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36827;&#21270;&#31639;&#27861;&#65292;&#22914;&#24494;&#20998;&#36827;&#21270;&#65292;&#22312;&#35299;&#20915;&#23454;&#25968;&#21442;&#25968;&#20248;&#21270;&#25361;&#25112;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#21333;&#20010;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#22312;&#19981;&#21516;&#38382;&#39064;&#23454;&#20363;&#19978;&#21464;&#21270;&#65292;&#38656;&#35201;&#22312;&#31639;&#27861;&#36873;&#25321;&#25110;&#37197;&#32622;&#26041;&#38754;&#25237;&#20837;&#30456;&#24403;&#22810;&#30340;&#21162;&#21147;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#19968;&#32452;&#31639;&#27861;&#30340;&#20114;&#34917;&#20248;&#21183;&#65292;&#24182;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#20026;&#29305;&#23450;&#38382;&#39064;&#21160;&#24577;&#35843;&#24230;&#23427;&#20204;&#26469;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#21160;&#24577;&#31639;&#27861;&#36873;&#25321;&#26694;&#26550;&#26469;&#23436;&#25104;&#36825;&#19968;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#21160;&#24577;&#31639;&#27861;&#36873;&#25321;&#24314;&#27169;&#20026;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#20197;&#31574;&#30053;&#26799;&#24230;&#26041;&#24335;&#35757;&#32451;&#20195;&#29702;&#36873;&#25321;&#26681;&#25454;&#20248;&#21270;&#36807;&#31243;&#20013;&#35266;&#23519;&#21040;&#30340;&#29305;&#24449;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#31639;&#27861;&#12290;&#20026;&#20102;&#20351;&#20195;&#29702;&#20855;&#22791;&#24517;&#35201;&#30340;&#20449;&#24687;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#32467;&#21512;&#20102;&#19968;&#20010;&#32463;&#36807;&#28145;&#24605;&#29087;&#34385;&#30340;&#26223;&#35266;&#21644;&#31639;&#27861;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02131v1 Announce Type: cross  Abstract: Evolutionary algorithms, such as Differential Evolution, excel in solving real-parameter optimization challenges. However, the effectiveness of a single algorithm varies across different problem instances, necessitating considerable efforts in algorithm selection or configuration. This paper aims to address the limitation by leveraging the complementary strengths of a group of algorithms and dynamically scheduling them throughout the optimization progress for specific problems. We propose a deep reinforcement learning-based dynamic algorithm selection framework to accomplish this task. Our approach models the dynamic algorithm selection a Markov Decision Process, training an agent in a policy gradient manner to select the most suitable algorithm according to the features observed during the optimization process. To empower the agent with the necessary information, our framework incorporates a thoughtful design of landscape and algorith
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#21512;&#24182;&#19981;&#21516;&#21021;&#22987;&#21270;&#30340;Transformer&#27169;&#22411;&#30340;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#21512;&#24182;&#25216;&#26415;&#20197;&#30740;&#31350;&#36825;&#20123;&#27169;&#22411;&#26497;&#23567;&#20540;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#21457;&#29616;&#19982;&#27169;&#22411;&#24179;&#22343;&#30456;&#27604;&#65292;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;&#21512;&#24182;&#36825;&#20123;&#27169;&#22411;&#22987;&#32456;&#21487;&#20197;&#33719;&#24471;&#36739;&#20302;&#30340;&#25439;&#22833;&#38556;&#30861;&#12290;</title><link>https://arxiv.org/abs/2403.00986</link><description>&lt;p&gt;
&#21512;&#24182;&#26469;&#33258;&#19981;&#21516;&#21021;&#22987;&#21270;&#30340;&#25991;&#26412;&#21464;&#25442;&#22120;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Merging Text Transformer Models from Different Initializations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00986
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#21512;&#24182;&#19981;&#21516;&#21021;&#22987;&#21270;&#30340;Transformer&#27169;&#22411;&#30340;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#21512;&#24182;&#25216;&#26415;&#20197;&#30740;&#31350;&#36825;&#20123;&#27169;&#22411;&#26497;&#23567;&#20540;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#21457;&#29616;&#19982;&#27169;&#22411;&#24179;&#22343;&#30456;&#27604;&#65292;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;&#21512;&#24182;&#36825;&#20123;&#27169;&#22411;&#22987;&#32456;&#21487;&#20197;&#33719;&#24471;&#36739;&#20302;&#30340;&#25439;&#22833;&#38556;&#30861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20851;&#20110;&#19968;&#27425;&#24615;&#22522;&#20110;&#25490;&#21015;&#30340;&#27169;&#22411;&#21512;&#24182;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#19981;&#21516;&#21021;&#22987;&#21270;&#30340;&#27169;&#22411;&#20043;&#38388;&#23384;&#22312;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#20302;&#25110;&#38646;&#38556;&#30861;&#27169;&#36830;&#25509;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;Transformer&#26550;&#26500;&#22312;&#35821;&#35328;&#39046;&#22495;&#20013;&#21344;&#20027;&#23548;&#22320;&#20301;&#65292;&#20294;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#23578;&#26410;&#24310;&#20280;&#21040;Transformer&#26550;&#26500;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#29420;&#31435;Transformer&#26497;&#23567;&#20540;&#23398;&#20064;&#31867;&#20284;&#29305;&#24449;&#30340;&#31243;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#21512;&#24182;&#25216;&#26415;&#65292;&#20197;&#30740;&#31350;&#25439;&#22833;&#26223;&#35266;&#20013;&#36825;&#20123;&#26497;&#23567;&#20540;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#26550;&#26500;&#30340;&#20855;&#20307;&#32454;&#33410;&#65292;&#22914;&#20854;&#27531;&#24046;&#36830;&#25509;&#12289;&#22810;&#22836;&#27880;&#24847;&#21147;&#21644;&#31163;&#25955;&#30340;&#39034;&#24207;&#36755;&#20837;&#65292;&#38656;&#35201;&#29305;&#23450;&#30340;&#24178;&#39044;&#25514;&#26045;&#65292;&#20197;&#20415;&#35745;&#31639;&#30041;&#22312;&#30456;&#21516;&#21151;&#33021;&#31561;&#20215;&#31867;&#20013;&#30340;&#27169;&#22411;&#25490;&#21015;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;&#21512;&#24182;&#36825;&#20123;&#27169;&#22411;&#65292;&#25105;&#20204;&#21457;&#29616;&#19982;&#23545;&#20960;&#20010;&#22312;&#19968;&#20010;maske&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#36827;&#34892;&#27169;&#22411;&#24179;&#22343;&#30456;&#27604;&#65292;&#26368;&#23567;&#20540;&#20043;&#38388;&#30340;&#25439;&#22833;&#38556;&#30861;&#19968;&#30452;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00986v1 Announce Type: cross  Abstract: Recent work on one-shot permutation-based model merging has shown impressive low- or zero-barrier mode connectivity between models from completely different initializations. However, this line of work has not yet extended to the Transformer architecture, despite its dominant popularity in the language domain. Therefore, in this work, we investigate the extent to which separate Transformer minima learn similar features, and propose a model merging technique to investigate the relationship between these minima in the loss landscape. The specifics of the architecture, like its residual connections, multi-headed attention, and discrete, sequential input, require specific interventions in order to compute model permutations that remain within the same functional equivalence class. In merging these models with our method, we consistently find lower loss barriers between minima compared to model averaging for several models trained on a maske
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#31181;&#29992;&#20110;Llama 2 Chat 7B&#25110;&#26356;&#22823;&#27169;&#22411;&#30340;&#33609;&#26696;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#21152;&#36895;&#25512;&#29702;&#65292;&#20165;&#21344;&#21407;&#22987;&#22823;&#23567;&#30340;1.64&#65285;&#12290;</title><link>https://arxiv.org/abs/2403.00858</link><description>&lt;p&gt;
&#30452;&#25509;&#19982;Chat-Fine-Tuned LLMs&#30340;&#33609;&#26696;&#27169;&#22411;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Direct Alignment of Draft Model for Speculative Decoding with Chat-Fine-Tuned LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00858
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#31181;&#29992;&#20110;Llama 2 Chat 7B&#25110;&#26356;&#22823;&#27169;&#22411;&#30340;&#33609;&#26696;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#21152;&#36895;&#25512;&#29702;&#65292;&#20165;&#21344;&#21407;&#22987;&#22823;&#23567;&#30340;1.64&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#29983;&#25104;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30001;&#20110;&#20854;&#33258;&#22238;&#24402;&#26412;&#36136;&#12289;&#24040;&#22823;&#30340;&#21442;&#25968;&#25968;&#37327;&#21644;&#26377;&#38480;&#30340;&#20869;&#23384;&#24102;&#23485;&#32780;&#34987;&#35748;&#20026;&#26159;&#20869;&#23384;&#23494;&#38598;&#22411;&#65292;&#36890;&#24120;&#23548;&#33268;&#20302;&#20196;&#29260;&#36895;&#29575;&#12290;&#29468;&#27979;&#35299;&#30721;&#24050;&#34987;&#25552;&#20986;&#20316;&#20026;LLM&#25512;&#29702;&#21152;&#36895;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#20195;&#24320;&#28304;LLM&#31995;&#21015;&#20013;&#65292;&#20363;&#22914;Llama 2 7B&#65292;&#30001;&#20110;&#33609;&#26696;&#27169;&#22411;&#36890;&#24120;&#19981;&#21487;&#29992;&#65292;&#22240;&#27492;&#38656;&#35201;&#35757;&#32451;&#39640;&#36136;&#37327;&#30340;&#33609;&#26696;&#27169;&#22411;&#20197;&#36890;&#36807;&#29468;&#27979;&#35299;&#30721;&#23454;&#29616;&#25512;&#29702;&#21152;&#36895;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#33609;&#26696;&#27169;&#22411;&#35757;&#32451;&#26694;&#26550;&#65292;&#29992;&#20110;&#30452;&#25509;&#19982;Chat-capable&#30446;&#26631;&#27169;&#22411;&#23545;&#40784;&#12290;&#36890;&#36807;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#35757;&#32451;&#20986;Llama 2 Chat Drafter 115M&#65292;&#36825;&#26159;&#19968;&#20010;&#36866;&#29992;&#20110;Llama 2 Chat 7B&#25110;&#26356;&#22823;&#27169;&#22411;&#30340;&#33609;&#26696;&#27169;&#22411;&#65292;&#20165;&#21344;&#21407;&#22987;&#22823;&#23567;&#30340;1.64&#65285;&#12290;&#25105;&#20204;&#30340;&#35757;&#32451;&#26694;&#26550;&#20165;&#21253;&#25324;&#39044;&#35757;&#32451;&#12289;&#33976;&#39311;&#25968;&#25454;&#38598;&#29983;&#25104;&#21644;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#36827;&#34892;&#24494;&#35843;&#65292;&#27809;&#26377;&#39069;&#22806;&#30340;&#23545;&#40784;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00858v1 Announce Type: cross  Abstract: Text generation with Large Language Models (LLMs) is known to be memory bound due to the combination of their auto-regressive nature, huge parameter counts, and limited memory bandwidths, often resulting in low token rates. Speculative decoding has been proposed as a solution for LLM inference acceleration. However, since draft models are often unavailable in the modern open-source LLM families, e.g., for Llama 2 7B, training a high-quality draft model is required to enable inference acceleration via speculative decoding. In this paper, we propose a simple draft model training framework for direct alignment to chat-capable target models. With the proposed framework, we train Llama 2 Chat Drafter 115M, a draft model for Llama 2 Chat 7B or larger, with only 1.64\% of the original size. Our training framework only consists of pretraining, distillation dataset generation, and finetuning with knowledge distillation, with no additional align
&lt;/p&gt;</description></item><item><title>&#36870;&#36716;&#35781;&#21650;&#38382;&#39064;&#26159;&#23548;&#33268;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#26080;&#27861;&#36827;&#34892;&#21452;&#21521;&#25512;&#29702;&#30340;&#26681;&#26412;&#21407;&#22240;&#20043;&#19968;&#65292;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#35821;&#20041;&#24863;&#30693;&#30340;&#32622;&#25442;&#35757;&#32451;&#26469;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.00758</link><description>&lt;p&gt;
&#36890;&#36807;&#35821;&#20041;&#24863;&#30693;&#32622;&#25442;&#35757;&#32451;&#26469;&#32531;&#35299;&#36870;&#36716;&#35781;&#21650;
&lt;/p&gt;
&lt;p&gt;
Mitigating Reversal Curse via Semantic-aware Permutation Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00758
&lt;/p&gt;
&lt;p&gt;
&#36870;&#36716;&#35781;&#21650;&#38382;&#39064;&#26159;&#23548;&#33268;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#26080;&#27861;&#36827;&#34892;&#21452;&#21521;&#25512;&#29702;&#30340;&#26681;&#26412;&#21407;&#22240;&#20043;&#19968;&#65292;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#35821;&#20041;&#24863;&#30693;&#30340;&#32622;&#25442;&#35757;&#32451;&#26469;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#34920;&#29616;&#65292;&#28982;&#32780;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22240;&#26524;&#20851;&#31995;&#30340;LLM&#36973;&#36935;&#20102;&#8220;&#36870;&#36716;&#35781;&#21650;&#8221;&#12290;&#19968;&#20010;&#20856;&#22411;&#30340;&#20363;&#23376;&#26159;&#65292;&#27169;&#22411;&#30693;&#36947;&#8220;A&#30340;&#29238;&#20146;&#26159;B&#8221;&#65292;&#20294;&#26080;&#27861;&#25512;&#29702;&#20986;&#8220;B&#30340;&#23401;&#23376;&#26159;A&#8221;&#12290;&#36825;&#19968;&#23616;&#38480;&#24615;&#23545;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#30340;&#36827;&#23637;&#26500;&#25104;&#20102;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#26263;&#31034;&#20102;&#27169;&#22411;&#22312;&#29702;&#35299;&#21644;&#24212;&#29992;&#21452;&#21521;&#25512;&#29702;&#26041;&#38754;&#23384;&#22312;&#24046;&#36317;&#12290;&#26412;&#25991;&#39318;&#20808;&#36827;&#34892;&#20102;&#22823;&#37327;&#35780;&#20272;&#65292;&#24182;&#30830;&#23450;&#20102;&#36870;&#36716;&#35781;&#21650;&#30340;&#26681;&#26412;&#21407;&#22240;&#22312;&#20110;&#35757;&#32451;&#21644;&#25512;&#26029;&#38454;&#27573;&#20043;&#38388;&#30340;&#35789;&#24207;&#19981;&#21516;&#65292;&#21363;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#39044;&#27979;&#20808;&#34892;&#35789;&#30340;&#33021;&#21147;&#19981;&#36275;&#12290;&#22240;&#27492;&#65292;&#32771;&#34385;&#21040;&#22312;&#35757;&#32451;&#25968;&#25454;&#19978;&#36827;&#34892;&#25490;&#21015;&#21487;&#20197;&#34987;&#35270;&#20026;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#65292;&#22240;&#20026;&#36825;&#21487;&#20197;&#20351;&#27169;&#22411;&#39044;&#27979;&#20808;&#34892;&#35789;&#25110;&#26631;&#35760;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#25490;&#21015;&#26041;&#27861;&#21487;&#33021;&#21463;&#21040;&#25130;&#26029;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00758v1 Announce Type: cross  Abstract: While large language models (LLMs) have achieved impressive performance across diverse tasks, recent studies showcase that causal LLMs suffer from the "reversal curse". It is a typical example that the model knows "A's father is B", but is unable to reason "B's child is A". This limitation poses a challenge to the advancement of artificial general intelligence (AGI), as it suggests a gap in the models' ability to comprehend and apply bidirectional reasoning. In this paper, we first conduct substantial evaluation and identify that the root cause of the reversal curse lies in the different word order between the training and inference stage, namely, the poor ability of causal language models to predict antecedent words within the training data. Accordingly, permutation on the training data is considered as a potential solution, since this can make the model predict antecedent words or tokens. However, previous permutation methods may dis
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;ByteComposer&#20195;&#29702;&#26694;&#26550;&#65292;&#22312;&#27169;&#25311;&#20154;&#31867;&#21019;&#20316;&#27969;&#31243;&#30340;&#22522;&#30784;&#19978;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;&#27169;&#22411;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#21487;&#19982;&#20154;&#31867;&#21019;&#20316;&#32773;&#30456;&#25552;&#24182;&#35770;&#30340;&#26059;&#24459;&#21019;&#20316;&#20195;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.17785</link><description>&lt;p&gt;
ByteComposer&#65306;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#30340;&#31867;&#20154;&#26059;&#24459;&#21019;&#20316;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ByteComposer: a Human-like Melody Composition Method based on Language Model Agent
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17785
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;ByteComposer&#20195;&#29702;&#26694;&#26550;&#65292;&#22312;&#27169;&#25311;&#20154;&#31867;&#21019;&#20316;&#27969;&#31243;&#30340;&#22522;&#30784;&#19978;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;&#27169;&#22411;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#21487;&#19982;&#20154;&#31867;&#21019;&#20316;&#32773;&#30456;&#25552;&#24182;&#35770;&#30340;&#26059;&#24459;&#21019;&#20316;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;Large Language Models&#65292;LLM&#65289;&#22312;&#22810;&#27169;&#24577;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#35774;&#35745;&#19968;&#20010;&#20154;&#31867;&#23545;&#40784;&#19988;&#21487;&#35299;&#37322;&#30340;&#26059;&#24459;&#21019;&#20316;&#31995;&#32479;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ByteComposer&#65292;&#19968;&#20010;&#20195;&#29702;&#26694;&#26550;&#65292;&#27169;&#25311;&#20154;&#31867;&#21019;&#20316;&#30340;&#22235;&#20010;&#29420;&#31435;&#27493;&#39588;&#65306;&#8220;&#27010;&#24565;&#20998;&#26512; - &#36215;&#33609;&#20316;&#21697; - &#33258;&#25105;&#35780;&#20272;&#21644;&#20462;&#25913; - &#32654;&#23398;&#36873;&#25321;&#8221;&#12290;&#35813;&#26694;&#26550;&#23558;LLM&#30340;&#20132;&#20114;&#21644;&#30693;&#35782;&#29702;&#35299;&#29305;&#24615;&#19982;&#29616;&#26377;&#30340;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;&#27169;&#22411;&#26080;&#32541;&#34701;&#21512;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#19968;&#20010;&#21487;&#19982;&#20154;&#31867;&#21019;&#20316;&#32773;&#23218;&#32654;&#30340;&#26059;&#24459;&#21019;&#20316;&#20195;&#29702;&#12290;&#25105;&#20204;&#22312;GPT4&#21644;&#20960;&#20010;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#35777;&#23454;&#20102;&#25105;&#20204;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#19987;&#19994;&#38899;&#20048;&#20316;&#26354;&#23478;&#21442;&#19982;&#20102;&#22810;&#32500;&#24230;&#35780;&#20272;&#65292;&#26368;&#32456;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21508;&#20010;&#26041;&#38754;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17785v1 Announce Type: cross  Abstract: Large Language Models (LLM) have shown encouraging progress in multimodal understanding and generation tasks. However, how to design a human-aligned and interpretable melody composition system is still under-explored. To solve this problem, we propose ByteComposer, an agent framework emulating a human's creative pipeline in four separate steps : "Conception Analysis - Draft Composition - Self-Evaluation and Modification - Aesthetic Selection". This framework seamlessly blends the interactive and knowledge-understanding features of LLMs with existing symbolic music generation models, thereby achieving a melody composition agent comparable to human creators. We conduct extensive experiments on GPT4 and several open-source large language models, which substantiate our framework's effectiveness. Furthermore, professional music composers were engaged in multi-dimensional evaluations, the final results demonstrated that across various facets
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#36830;&#32493;&#26102;&#38388;&#25511;&#21046;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#30452;&#25509;&#20998;&#26512;Bellman&#26368;&#20248;&#25439;&#22833;\emph{&#20808;&#39564;}&#27867;&#21270;&#35823;&#24046;&#30340;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#26377;&#30028;&#24615;&#20551;&#35774;&#65292;&#24182;&#36890;&#36807;&#26368;&#22823;&#31639;&#23376;&#30340;&#20998;&#35299;&#26041;&#27861;&#23454;&#29616;&#20102;&#25439;&#22833;&#20989;&#25968;&#30340;&#36716;&#25442;&#12290;</title><link>https://arxiv.org/abs/2402.16899</link><description>&lt;p&gt;
&#36830;&#32493;&#26102;&#38388;&#24378;&#21270;&#23398;&#20064;&#20013;&#28145;&#24230;&#27531;&#24046;&#32593;&#32476;&#30340;\emph{&#20808;&#39564;&#20272;&#35745;}
&lt;/p&gt;
&lt;p&gt;
A prior Estimates for Deep Residual Network in Continuous-time Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16899
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#36830;&#32493;&#26102;&#38388;&#25511;&#21046;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#30452;&#25509;&#20998;&#26512;Bellman&#26368;&#20248;&#25439;&#22833;\emph{&#20808;&#39564;}&#27867;&#21270;&#35823;&#24046;&#30340;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#26377;&#30028;&#24615;&#20551;&#35774;&#65292;&#24182;&#36890;&#36807;&#26368;&#22823;&#31639;&#23376;&#30340;&#20998;&#35299;&#26041;&#27861;&#23454;&#29616;&#20102;&#25439;&#22833;&#20989;&#25968;&#30340;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#35768;&#22810;&#22823;&#35268;&#27169;&#23454;&#38469;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24615;&#33021;&#20998;&#26512;&#24573;&#30053;&#20102;&#36830;&#32493;&#26102;&#38388;&#25511;&#21046;&#38382;&#39064;&#30340;&#29420;&#29305;&#29305;&#24449;&#65292;&#26080;&#27861;&#30452;&#25509;&#20272;&#35745;Bellman&#26368;&#20248;&#25439;&#22833;&#30340;&#27867;&#21270;&#35823;&#24046;&#65292;&#24182;&#19988;&#38656;&#35201;&#19968;&#20010;&#26377;&#30028;&#24615;&#20551;&#35774;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20391;&#37325;&#20110;&#36830;&#32493;&#26102;&#38388;&#25511;&#21046;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#25152;&#26377;&#28385;&#36275;&#21322;&#32676;&#21644;Lipschitz&#24615;&#36136;&#30340;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#22312;&#35813;&#26041;&#27861;&#19979;&#65292;&#25105;&#20204;&#33021;&#22815;&#30452;&#25509;&#20998;&#26512;Bellman&#26368;&#20248;&#25439;&#22833;&#30340;\emph{&#20808;&#39564;}&#27867;&#21270;&#35823;&#24046;&#12290;&#35813;&#26041;&#27861;&#30340;&#26680;&#24515;&#22312;&#20110;&#25439;&#22833;&#20989;&#25968;&#30340;&#20004;&#27425;&#36716;&#25442;&#12290;&#20026;&#20102;&#23436;&#25104;&#36716;&#25442;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26368;&#22823;&#31639;&#23376;&#30340;&#20998;&#35299;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#36825;&#20010;&#20998;&#26512;&#26041;&#27861;&#19981;&#38656;&#35201;&#26377;&#30028;&#24615;&#20551;&#35774;&#12290;&#26368;&#32456;&#25105;&#20204;&#32500;&#24471;&#21040;&#20102;&#19968;&#20010;&#27809;&#26377;&#8220;&#32500;&#24230;&#35781;&#21650;&#8221;&#30340;\emph{&#20808;&#39564;}&#27867;&#21270;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16899v1 Announce Type: cross  Abstract: Deep reinforcement learning excels in numerous large-scale practical applications. However, existing performance analyses ignores the unique characteristics of continuous-time control problems, is unable to directly estimate the generalization error of the Bellman optimal loss and require a boundedness assumption. Our work focuses on continuous-time control problems and proposes a method that is applicable to all such problems where the transition function satisfies semi-group and Lipschitz properties. Under this method, we can directly analyze the \emph{a priori} generalization error of the Bellman optimal loss. The core of this method lies in two transformations of the loss function. To complete the transformation, we propose a decomposition method for the maximum operator. Additionally, this analysis method does not require a boundedness assumption. Finally, we obtain an \emph{a priori} generalization error without the curse of dime
&lt;/p&gt;</description></item><item><title>&#19981;&#38656;&#35201;&#22522;&#20934;&#23454;&#20917;&#25110;&#21442;&#32771;&#21709;&#24212;&#30340;&#26465;&#20214;&#19979;&#65292;&#36890;&#36807;&#32771;&#34385;&#27169;&#22411;&#30340;&#19977;&#20803;&#32452;&#26469;&#25490;&#21517;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#25490;&#21517;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.14860</link><description>&lt;p&gt;
&#22312;&#27809;&#26377;&#22522;&#20934;&#23454;&#20917;&#30340;&#24773;&#20917;&#19979;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25490;&#21517;
&lt;/p&gt;
&lt;p&gt;
Ranking Large Language Models without Ground Truth
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14860
&lt;/p&gt;
&lt;p&gt;
&#19981;&#38656;&#35201;&#22522;&#20934;&#23454;&#20917;&#25110;&#21442;&#32771;&#21709;&#24212;&#30340;&#26465;&#20214;&#19979;&#65292;&#36890;&#36807;&#32771;&#34385;&#27169;&#22411;&#30340;&#19977;&#20803;&#32452;&#26469;&#25490;&#21517;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#25490;&#21517;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26222;&#21450;&#21644;&#24433;&#21709;&#21147;&#30340;&#22686;&#24378;&#65292;&#35780;&#20272;&#21644;&#25490;&#21517;LLMs&#24050;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#35780;&#20272;&#26041;&#27861;&#35201;&#20040;&#38656;&#35201;&#33719;&#21462;&#26114;&#36149;&#30340;&#20154;&#31867;&#21709;&#24212;&#65292;&#35201;&#20040;&#20351;&#29992;LLMs&#25104;&#23545;&#22320;&#20114;&#30456;&#35780;&#20272;&#65292;&#36825;&#21487;&#33021;&#19981;&#22815;&#21487;&#38752;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#35282;&#65292;&#22312;&#32473;&#23450;&#19968;&#32452;&#25552;&#31034;&#25968;&#25454;&#38598;&#65288;&#27604;&#22914;&#38382;&#39064;&#12289;&#35828;&#26126;&#31561;&#65289;&#21644;&#19968;&#32452;LLMs&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#22312;&#27809;&#26377;&#20219;&#20309;&#22522;&#20934;&#23454;&#20917;&#25110;&#21442;&#32771;&#21709;&#24212;&#30340;&#24773;&#20917;&#19979;&#23545;&#23427;&#20204;&#36827;&#34892;&#25490;&#21517;&#12290;&#21463;&#21040;&#29616;&#23454;&#29983;&#27963;&#30340;&#21551;&#21457;&#65292;&#20854;&#20013;&#19987;&#23478;&#21644;&#26377;&#30693;&#35782;&#30340;&#20154;&#37117;&#33021;&#35782;&#21035;&#19968;&#20010;&#26032;&#25163;&#65292;&#25105;&#20204;&#30340;&#20027;&#35201;&#24605;&#36335;&#26159;&#32771;&#34385;&#27169;&#22411;&#30340;&#19977;&#20803;&#32452;&#65292;&#20854;&#20013;&#27599;&#20010;&#27169;&#22411;&#35780;&#20272;&#20854;&#20182;&#20004;&#20010;&#27169;&#22411;&#65292;&#33021;&#22815;&#20197;&#24456;&#39640;&#30340;&#27010;&#29575;&#27491;&#30830;&#35782;&#21035;&#26368;&#24046;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#25105;&#20204;&#30340;&#24819;&#27861;&#24182;&#25552;&#20379;&#20102;&#25104;&#21151;&#30340;&#20805;&#20998;&#26465;&#20214;&#12290;&#36890;&#36807;&#21453;&#22797;&#24212;&#29992;&#36825;&#19968;&#24819;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#23545;LLMs&#36827;&#34892;&#25490;&#21517;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14860v1 Announce Type: cross  Abstract: Evaluation and ranking of large language models (LLMs) has become an important problem with the proliferation of these models and their impact. Evaluation methods either require human responses which are expensive to acquire or use pairs of LLMs to evaluate each other which can be unreliable. In this paper, we provide a novel perspective where, given a dataset of prompts (viz. questions, instructions, etc.) and a set of LLMs, we rank them without access to any ground truth or reference responses. Inspired by real life where both an expert and a knowledgeable person can identify a novice our main idea is to consider triplets of models, where each one of them evaluates the other two, correctly identifying the worst model in the triplet with high probability. We also analyze our idea and provide sufficient conditions for it to succeed. Applying this idea repeatedly, we propose two methods to rank LLMs. In experiments on different generati
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#33258;&#21160;&#25628;&#32034;&#21464;&#20998;&#30005;&#36335;&#30340;&#26368;&#20339;&#32467;&#26500;&#65292;&#25913;&#21892;&#20102;VQAs&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.13754</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#36741;&#21161;&#30340;&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;&#37327;&#23376;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning-assisted quantum architecture search for variational quantum algorithms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13754
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#33258;&#21160;&#25628;&#32034;&#21464;&#20998;&#30005;&#36335;&#30340;&#26368;&#20339;&#32467;&#26500;&#65292;&#25913;&#21892;&#20102;VQAs&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22024;&#26434;&#20013;&#31561;&#35268;&#27169;&#37327;&#23376;&#65288;NISQ&#65289;&#26102;&#20195;&#65292;&#19968;&#20010;&#37325;&#35201;&#38556;&#30861;&#26159;&#30830;&#23450;&#21151;&#33021;&#24615;&#37327;&#23376;&#30005;&#36335;&#12290;&#36825;&#20123;&#30005;&#36335;&#24517;&#39035;&#21516;&#26102;&#31526;&#21512;&#24403;&#21069;&#37327;&#23376;&#30828;&#20214;&#38480;&#21046;&#25152;&#26045;&#21152;&#30340;&#32422;&#26463;&#12290;&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;&#65288;VQA&#65289;&#26159;&#19968;&#31867;&#37327;&#23376;-&#32463;&#20856;&#20248;&#21270;&#31639;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#24403;&#21069;&#21487;&#29992;&#37327;&#23376;&#35774;&#22791;&#20013;&#30340;&#36825;&#20123;&#25361;&#25112;&#12290;&#26412;&#35770;&#25991;&#20391;&#37325;&#20110;&#30005;&#36335;&#32467;&#26500;&#65292;&#36890;&#36807;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#33258;&#21160;&#25628;&#32034;&#21464;&#20998;&#30005;&#36335;&#30340;&#26368;&#20248;&#32467;&#26500;&#65292;&#25913;&#21892;&#20102;VQAs&#30340;&#24615;&#33021;&#12290;&#35770;&#25991;&#20869;&#36890;&#36807;&#35780;&#20272;&#30005;&#36335;&#30340;&#28145;&#24230;&#12289;&#38376;&#21644;&#21442;&#25968;&#30340;&#24635;&#25968;&#20197;&#21450;&#20934;&#30830;&#24615;&#26469;&#30830;&#23450;&#30005;&#36335;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13754v1 Announce Type: cross  Abstract: A significant hurdle in the noisy intermediate-scale quantum (NISQ) era is identifying functional quantum circuits. These circuits must also adhere to the constraints imposed by current quantum hardware limitations. Variational quantum algorithms (VQAs), a class of quantum-classical optimization algorithms, were developed to address these challenges in the currently available quantum devices. However, the overall performance of VQAs depends on the initialization strategy of the variational circuit, the structure of the circuit (also known as ansatz), and the configuration of the cost function. Focusing on the structure of the circuit, in this thesis, we improve the performance of VQAs by automating the search for an optimal structure for the variational circuits using reinforcement learning (RL). Within the thesis, the optimality of a circuit is determined by evaluating its depth, the overall count of gates and parameters, and its accu
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#28151;&#21512;&#25512;&#29702;&#33021;&#21147;&#21487;&#20197;&#36890;&#36807;&#20998;&#26512;&#25968;&#25454;&#12289;&#29702;&#35299;&#35268;&#21017;&#21644;&#27861;&#21017;&#12289;&#25552;&#20379;&#35821;&#22659;&#31561;&#26041;&#24335;&#25552;&#39640;&#33258;&#21160;&#39542;&#39542;&#30340;&#20915;&#31574;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.13602</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28151;&#21512;&#25512;&#29702;&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Hybrid Reasoning Based on Large Language Models for Autonomous Car Driving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13602
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#28151;&#21512;&#25512;&#29702;&#33021;&#21147;&#21487;&#20197;&#36890;&#36807;&#20998;&#26512;&#25968;&#25454;&#12289;&#29702;&#35299;&#35268;&#21017;&#21644;&#27861;&#21017;&#12289;&#25552;&#20379;&#35821;&#22659;&#31561;&#26041;&#24335;&#25552;&#39640;&#33258;&#21160;&#39542;&#39542;&#30340;&#20915;&#31574;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22240;&#20854;&#29702;&#35299;&#25991;&#26412;&#21644;&#22270;&#20687;&#12289;&#29983;&#25104;&#31867;&#20154;&#25991;&#26412;&#20197;&#21450;&#25191;&#34892;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#30340;&#33021;&#21147;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23558;&#36825;&#31181;&#39640;&#32423;&#25512;&#29702;&#19982;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#30456;&#32467;&#21512;&#20197;&#29992;&#20110;&#21160;&#24577;&#24773;&#20917;&#19979;&#30340;&#20915;&#31574;&#30340;&#27867;&#21270;&#33021;&#21147;&#38656;&#35201;&#36827;&#19968;&#27493;&#25506;&#32034;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;LLMs&#22312;&#28151;&#21512;&#31639;&#26415;&#21644;&#24120;&#35782;&#25512;&#29702;&#26041;&#38754;&#30340;&#36866;&#24212;&#33021;&#21147;&#21644;&#24212;&#29992;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#33258;&#21160;&#39550;&#39542;&#22330;&#26223;&#20013;&#12290;&#25105;&#20204;&#20551;&#35774;LLMs&#30340;&#28151;&#21512;&#25512;&#29702;&#33021;&#21147;&#21487;&#20197;&#36890;&#36807;&#20351;&#23427;&#20204;&#20998;&#26512;&#26816;&#27979;&#21040;&#30340;&#29289;&#20307;&#21644;&#20256;&#24863;&#22120;&#25968;&#25454;&#12289;&#29702;&#35299;&#39550;&#39542;&#35268;&#23450;&#21644;&#29289;&#29702;&#27861;&#21017;&#65292;&#24182;&#25552;&#20379;&#39069;&#22806;&#30340;&#35821;&#22659;&#26469;&#25913;&#21892;&#33258;&#21160;&#39550;&#39542;&#12290;&#36825;&#35299;&#20915;&#20102;&#22797;&#26434;&#24773;&#26223;&#65292;&#22914;&#20302;&#33021;&#35265;&#24230;&#65288;&#30001;&#20110;&#22825;&#27668;&#26465;&#20214;&#65289;&#19979;&#30340;&#20915;&#31574;&#65292;&#20256;&#32479;&#26041;&#27861;&#21487;&#33021;&#19981;&#36275;&#20197;&#32988;&#20219;&#12290;&#25105;&#20204;&#36890;&#36807;&#20934;&#30830;&#24615;&#35780;&#20272;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36825;&#31181;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13602v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have garnered significant attention for their ability to understand text and images, generate human-like text, and perform complex reasoning tasks. However, their ability to generalize this advanced reasoning with a combination of natural language text for decision-making in dynamic situations requires further exploration. In this study, we investigate how well LLMs can adapt and apply a combination of arithmetic and common-sense reasoning, particularly in autonomous driving scenarios. We hypothesize that LLMs hybrid reasoning abilities can improve autonomous driving by enabling them to analyze detected object and sensor data, understand driving regulations and physical laws, and offer additional context. This addresses complex scenarios, like decisions in low visibility (due to weather conditions), where traditional methods might fall short. We evaluated Large Language Models (LLMs) based on accuracy by co
&lt;/p&gt;</description></item><item><title>AnyGPT&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#31163;&#25955;&#34920;&#31034;&#23454;&#29616;&#21508;&#31181;&#27169;&#24577;&#30340;&#32479;&#19968;&#22788;&#29702;&#65292;&#33021;&#22815;&#22312;&#19981;&#25913;&#21464;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#25110;&#35757;&#32451;&#26041;&#24335;&#30340;&#24773;&#20917;&#19979;&#31283;&#23450;&#35757;&#32451;&#65292;&#20026;&#26032;&#27169;&#24577;&#30340;&#26080;&#32541;&#25972;&#21512;&#25552;&#20379;&#20102;&#21487;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.12226</link><description>&lt;p&gt;
AnyGPT&#65306;&#32479;&#19968;&#30340;&#22810;&#27169;&#24335;&#31163;&#25955;&#24207;&#21015;&#24314;&#27169;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12226
&lt;/p&gt;
&lt;p&gt;
AnyGPT&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#31163;&#25955;&#34920;&#31034;&#23454;&#29616;&#21508;&#31181;&#27169;&#24577;&#30340;&#32479;&#19968;&#22788;&#29702;&#65292;&#33021;&#22815;&#22312;&#19981;&#25913;&#21464;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#25110;&#35757;&#32451;&#26041;&#24335;&#30340;&#24773;&#20917;&#19979;&#31283;&#23450;&#35757;&#32451;&#65292;&#20026;&#26032;&#27169;&#24577;&#30340;&#26080;&#32541;&#25972;&#21512;&#25552;&#20379;&#20102;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102; AnyGPT&#65292;&#36825;&#26159;&#19968;&#20010;&#20219;&#24847;&#22810;&#27169;&#24335;&#35821;&#35328;&#27169;&#22411;&#65292;&#21033;&#29992;&#31163;&#25955;&#34920;&#31034;&#32479;&#19968;&#22788;&#29702;&#21508;&#31181;&#27169;&#24577;&#65292;&#21253;&#25324;&#35821;&#38899;&#12289;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#38899;&#20048;&#12290;AnyGPT &#21487;&#20197;&#31283;&#23450;&#35757;&#32451;&#65292;&#26080;&#38656;&#23545;&#24403;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26550;&#26500;&#25110;&#35757;&#32451;&#33539;&#24335;&#36827;&#34892;&#20219;&#20309;&#25913;&#21160;&#12290;&#30456;&#21453;&#65292;&#23427;&#20165;&#20381;&#36182;&#20110;&#25968;&#25454;&#32423;&#39044;&#22788;&#29702;&#65292;&#20419;&#36827;&#20102;&#26032;&#27169;&#24577;&#30340;&#26080;&#32541;&#38598;&#25104;&#21040;LLM&#20013;&#65292;&#31867;&#20284;&#20110;&#26032;&#35821;&#35328;&#30340;&#25972;&#21512;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22810;&#27169;&#24335;&#25991;&#26412;&#20013;&#24515;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#22810;&#27169;&#24335;&#23545;&#40784;&#39044;&#35757;&#32451;&#12290;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#65292;&#25105;&#20204;&#21512;&#25104;&#20102;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#20219;&#24847;&#22810;&#27169;&#24335;&#25351;&#20196;&#25968;&#25454;&#38598;&#12290;&#23427;&#21253;&#25324;108k&#20010;&#22810;&#36718;&#23545;&#35805;&#31034;&#20363;&#65292;&#31934;&#32454;&#22320;&#20132;&#32455;&#21508;&#31181;&#27169;&#24577;&#65292;&#20174;&#32780;&#20351;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#22810;&#27169;&#24577;&#36755;&#20837;&#21644;&#36755;&#20986;&#30340;&#20219;&#24847;&#32452;&#21512;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;AnyGPT&#33021;&#22815;&#20419;&#36827;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12226v1 Announce Type: cross  Abstract: We introduce AnyGPT, an any-to-any multimodal language model that utilizes discrete representations for the unified processing of various modalities, including speech, text, images, and music. AnyGPT can be trained stably without any alterations to the current large language model (LLM) architecture or training paradigms. Instead, it relies exclusively on data-level preprocessing, facilitating the seamless integration of new modalities into LLMs, akin to the incorporation of new languages. We build a multimodal text-centric dataset for multimodal alignment pre-training. Utilizing generative models, we synthesize the first large-scale any-to-any multimodal instruction dataset. It consists of 108k samples of multi-turn conversations that intricately interweave various modalities, thus equipping the model to handle arbitrary combinations of multimodal inputs and outputs. Experimental results demonstrate that AnyGPT is capable of facilitat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#20174;&#28023;&#37327;&#30340;Web&#22270;&#25968;&#25454;&#20013;&#23545;&#32972;&#26223;&#33410;&#28857;&#36827;&#34892;&#21387;&#32553;&#65292;&#24182;&#23558;&#37325;&#28857;&#25918;&#22312;&#20998;&#26512;&#30446;&#26631;&#33410;&#28857;&#19978;&#65292;&#20197;&#35299;&#20915;&#22270;&#25968;&#25454;&#23384;&#20648;&#21644;&#35745;&#31639;&#33021;&#21147;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.09565</link><description>&lt;p&gt;
&#22270;&#39592;&#26550;&#65306;&#20165;&#26377;&#32422;1%&#30340;&#33410;&#28857;&#36275;&#20197;&#34920;&#31034;&#21313;&#20159;&#35268;&#27169;&#30340;&#22270;
&lt;/p&gt;
&lt;p&gt;
Graph-Skeleton: ~1% Nodes are Sufficient to Represent Billion-Scale Graph
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09565
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#20174;&#28023;&#37327;&#30340;Web&#22270;&#25968;&#25454;&#20013;&#23545;&#32972;&#26223;&#33410;&#28857;&#36827;&#34892;&#21387;&#32553;&#65292;&#24182;&#23558;&#37325;&#28857;&#25918;&#22312;&#20998;&#26512;&#30446;&#26631;&#33410;&#28857;&#19978;&#65292;&#20197;&#35299;&#20915;&#22270;&#25968;&#25454;&#23384;&#20648;&#21644;&#35745;&#31639;&#33021;&#21147;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;Web&#19978;&#22270;&#25968;&#25454;&#30340;&#26222;&#36941;&#23384;&#22312;&#65292;Web&#22270;&#25366;&#25496;&#24050;&#25104;&#20026;&#28909;&#38376;&#30740;&#31350;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#22823;&#35268;&#27169;Web&#22270;&#30340;&#26222;&#21450;&#32473;&#23384;&#20648;&#12289;&#35745;&#31639;&#33021;&#21147;&#21644;&#22270;&#27169;&#22411;&#35774;&#35745;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#23613;&#31649;&#24050;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#30740;&#31350;&#26469;&#25552;&#39640;&#22270;&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#20294;&#23398;&#26415;&#30740;&#31350;&#19982;&#23454;&#38469;Web&#22270;&#25366;&#25496;&#24212;&#29992;&#20043;&#38388;&#20173;&#23384;&#22312;&#26126;&#26174;&#24046;&#36317;&#12290;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#21407;&#22240;&#26159;&#65292;&#22312;&#22823;&#22810;&#25968;&#24037;&#19994;&#22330;&#26223;&#20013;&#65292;&#23454;&#38469;&#19978;&#21482;&#38656;&#35201;&#20998;&#26512;Web&#22270;&#20013;&#30340;&#19968;&#23567;&#37096;&#20998;&#33410;&#28857;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#33410;&#28857;&#31216;&#20026;&#30446;&#26631;&#33410;&#28857;&#65292;&#20854;&#20182;&#33410;&#28857;&#31216;&#20026;&#32972;&#26223;&#33410;&#28857;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#20174;&#28023;&#37327;Web&#22270;&#25968;&#25454;&#20013;&#24688;&#24403;&#22320;&#25552;&#21462;&#21644;&#21387;&#32553;&#32972;&#26223;&#33410;&#28857;&#21487;&#33021;&#26159;&#35299;&#20915;&#38382;&#39064;&#30340;&#26356;&#32463;&#27982;&#30340;&#25463;&#24452;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#27425;&#23581;&#35797;&#30740;&#31350;&#20102;&#30446;&#26631;&#33410;&#28857;&#20998;&#31867;&#30340;&#22823;&#35268;&#27169;&#32972;&#26223;&#33410;&#28857;&#21387;&#32553;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09565v1 Announce Type: new  Abstract: Due to the ubiquity of graph data on the web, web graph mining has become a hot research spot. Nonetheless, the prevalence of large-scale web graphs in real applications poses significant challenges to storage, computational capacity and graph model design. Despite numerous studies to enhance the scalability of graph models, a noticeable gap remains between academic research and practical web graph mining applications. One major cause is that in most industrial scenarios, only a small part of nodes in a web graph are actually required to be analyzed, where we term these nodes as target nodes, while others as background nodes. In this paper, we argue that properly fetching and condensing the background nodes from massive web graph data might be a more economical shortcut to tackle the obstacles fundamentally. To this end, we make the first attempt to study the problem of massive background nodes compression for target nodes classification
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;MUSTARD&#65292;&#19968;&#31181;&#25484;&#25569;&#23450;&#29702;&#21644;&#35777;&#26126;&#25968;&#25454;&#32479;&#19968;&#21512;&#25104;&#30340;&#25968;&#25454;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#19977;&#20010;&#38454;&#27573;&#30340;&#21512;&#25104;&#65292;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#21270;&#30340;&#38382;&#39064;&#21644;&#25512;&#29702;&#27493;&#39588;&#30340;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2402.08957</link><description>&lt;p&gt;
MUSTARD&#65306;&#25484;&#25569;&#23450;&#29702;&#21644;&#35777;&#26126;&#25968;&#25454;&#30340;&#32479;&#19968;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
MUSTARD: Mastering Uniform Synthesis of Theorem and Proof Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08957
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;MUSTARD&#65292;&#19968;&#31181;&#25484;&#25569;&#23450;&#29702;&#21644;&#35777;&#26126;&#25968;&#25454;&#32479;&#19968;&#21512;&#25104;&#30340;&#25968;&#25454;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#19977;&#20010;&#38454;&#27573;&#30340;&#21512;&#25104;&#65292;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#21270;&#30340;&#38382;&#39064;&#21644;&#25512;&#29702;&#27493;&#39588;&#30340;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#21253;&#25324;&#25968;&#23398;&#25512;&#29702;&#21644;&#23450;&#29702;&#35777;&#26126;&#12290;&#30001;&#20110;&#36825;&#20004;&#20010;&#20219;&#21153;&#38656;&#35201;&#20005;&#26684;&#21644;&#24418;&#24335;&#21270;&#30340;&#22810;&#27493;&#25512;&#29702;&#65292;&#23427;&#20204;&#26159;&#25506;&#32034;LLMs&#25512;&#29702;&#33021;&#21147;&#30340;&#21560;&#24341;&#39046;&#22495;&#65292;&#20294;&#20173;&#38754;&#20020;&#37325;&#35201;&#25361;&#25112;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#22914;Chain-of-Thought&#65288;CoT&#65289;&#25581;&#31034;&#20102;&#20013;&#38388;&#27493;&#39588;&#25351;&#23548;&#30340;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#36880;&#27493;&#27880;&#37322;&#38656;&#35201;&#22823;&#37327;&#30340;&#21171;&#21160;&#21147;&#65292;&#23548;&#33268;&#24403;&#21069;&#22522;&#20934;&#27979;&#35797;&#30340;&#35757;&#32451;&#27493;&#39588;&#19981;&#36275;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;MUSTARD&#65292;&#19968;&#31181;&#25968;&#25454;&#29983;&#25104;&#26694;&#26550;&#65292;&#21487;&#20197;&#20027;&#23548;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#21270;&#30340;&#23450;&#29702;&#21644;&#35777;&#26126;&#25968;&#25454;&#30340;&#32479;&#19968;&#21512;&#25104;&#12290;MUSTARD&#36890;&#36807;&#19977;&#20010;&#38454;&#27573;&#21512;&#25104;&#25968;&#25454;&#65306;&#65288;1&#65289;&#23427;&#38543;&#26426;&#36873;&#25321;&#20960;&#20010;&#25968;&#23398;&#27010;&#24565;&#20316;&#20026;&#38382;&#39064;&#30340;&#31867;&#21035;&#12290;&#65288;2&#65289;&#28982;&#21518;&#65292;&#23427;&#20351;&#29992;&#36873;&#23450;&#30340;&#27010;&#24565;&#25552;&#31034;&#29983;&#25104;&#24615;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#33719;&#24471;&#38382;&#39064;&#21644;&#23427;&#20204;&#30340;&#25512;&#29702;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08957v1 Announce Type: new Abstract: Recent large language models (LLMs) have witnessed significant advancement in various tasks, including mathematical reasoning and theorem proving. As these two tasks require strict and formal multi-step inference, they are appealing domains for exploring the reasoning ability of LLMs but still face important challenges. Previous studies such as Chain-of-Thought (CoT) have revealed the effectiveness of intermediate steps guidance. However, such step-wise annotation requires heavy labor, leading to insufficient training steps for current benchmarks. To fill this gap, this work introduces MUSTARD, a data generation framework that masters uniform synthesis of theorem and proof data of high quality and diversity. MUSTARD synthesizes data in three stages: (1) It samples a few mathematical concept seeds as the problem category. (2) Then, it prompts a generative language model with the sampled concepts to obtain both the problems and their step-w
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#20026;&#26234;&#33021;&#20307;&#25552;&#20379;&#22686;&#24378;&#24418;&#24335;&#30340;&#20219;&#21153;&#25351;&#23548;&#65292;&#20351;&#20854;&#33021;&#22815;&#29702;&#35299;&#28216;&#25103;&#25351;&#23548;&#24182;&#23454;&#29616;"&#35835;&#29609;&#28216;&#25103;"&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#23558;&#22810;&#27169;&#24577;&#25351;&#23548;&#35843;&#20248;&#30340;&#25104;&#21151;&#24212;&#29992;&#20110;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#65292;&#26500;&#24314;&#20102;&#19968;&#32452;... (&#20869;&#23481;&#22826;&#38271;&#65292;&#26080;&#27861;&#32487;&#32493;&#26174;&#31034;)</title><link>https://arxiv.org/abs/2402.04154</link><description>&lt;p&gt;
&#12298;&#35835;&#29609;&#28216;&#25103;&#65288;R2-Play&#65289;: &#22810;&#27169;&#24577;&#28216;&#25103;&#25351;&#23548;&#19979;&#30340;&#20915;&#31574; Transformer&#12299;
&lt;/p&gt;
&lt;p&gt;
Read to Play (R2-Play): Decision Transformer with Multimodal Game Instruction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04154
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#20026;&#26234;&#33021;&#20307;&#25552;&#20379;&#22686;&#24378;&#24418;&#24335;&#30340;&#20219;&#21153;&#25351;&#23548;&#65292;&#20351;&#20854;&#33021;&#22815;&#29702;&#35299;&#28216;&#25103;&#25351;&#23548;&#24182;&#23454;&#29616;"&#35835;&#29609;&#28216;&#25103;"&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#23558;&#22810;&#27169;&#24577;&#25351;&#23548;&#35843;&#20248;&#30340;&#25104;&#21151;&#24212;&#29992;&#20110;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#65292;&#26500;&#24314;&#20102;&#19968;&#32452;... (&#20869;&#23481;&#22826;&#38271;&#65292;&#26080;&#27861;&#32487;&#32493;&#26174;&#31034;)
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#24320;&#21457;&#19968;&#27454;&#36890;&#29992;&#26234;&#33021;&#20307;&#19968;&#30452;&#26159;&#19968;&#20010;&#38271;&#26399;&#30340;&#30446;&#26631;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#21033;&#29992;&#26469;&#33258;&#21508;&#31181;&#20219;&#21153;&#30340;&#22823;&#37327;&#31163;&#32447;&#25968;&#25454;&#38598;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#30340;&#22810;&#20219;&#21153;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24037;&#20316;&#22312;&#25193;&#23637;&#21040;&#26032;&#20219;&#21153;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#23558;&#25991;&#26412;&#25351;&#23548;&#25110;&#35270;&#35273;&#36712;&#36857;&#25972;&#21512;&#21040;&#20915;&#31574;&#32593;&#32476;&#20013;&#65292;&#25552;&#20379;&#20219;&#21153;&#29305;&#23450;&#30340;&#19978;&#19979;&#25991;&#25552;&#31034;&#65292;&#20195;&#34920;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;&#28982;&#32780;&#65292;&#35266;&#23519;&#21040;&#20165;&#20381;&#36182;&#20110;&#25991;&#26412;&#25351;&#23548;&#25110;&#35270;&#35273;&#36712;&#36857;&#23545;&#20110;&#20934;&#30830;&#20256;&#36798;&#20219;&#21153;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#26159;&#19981;&#36275;&#22815;&#30340;&#12290;&#26412;&#25991;&#25506;&#32034;&#20102;&#22686;&#24378;&#26234;&#33021;&#20307;&#20219;&#21153;&#25351;&#23548;&#30340;&#24418;&#24335;&#65292;&#20351;&#20854;&#33021;&#22815;&#29702;&#35299;&#28216;&#25103;&#25351;&#23548;&#65292;&#20174;&#32780;&#23454;&#29616;"&#35835;&#29609;&#28216;&#25103;"&#30340;&#33021;&#21147;&#12290;&#21463;&#21040;&#22810;&#27169;&#24577;&#25351;&#23548;&#35843;&#20248;&#22312;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#25104;&#21151;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;&#22522;&#20110;&#35270;&#35273;&#30340;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#35270;&#20026;&#19968;&#20010;&#38271;&#26399;&#35270;&#35273;&#20219;&#21153;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#32452;... (&#20869;&#23481;&#22826;&#38271;&#65292;&#26080;&#27861;&#32487;&#32493;&#26174;&#31034;)
&lt;/p&gt;
&lt;p&gt;
Developing a generalist agent is a longstanding objective in artificial intelligence. Previous efforts utilizing extensive offline datasets from various tasks demonstrate remarkable performance in multitasking scenarios within Reinforcement Learning.However, these works encounter challenges in extending their capabilities to new tasks.Recent approaches integrate textual guidance or visual trajectory into decision networks to provide task-specific contextual cues, representing a promising direction.However, it is observed that relying solely on textual guidance or visual trajectory is insufficient for accurately conveying the contextual information of tasks.This paper explores enhanced forms of task guidance for agents, enabling them to comprehend gameplay instructions, thereby facilitating a "read-to-play" capability.Drawing inspiration from the success of multimodal instruction tuning in visual tasks, we treat the visual-based RL task as a long-horizon vision task and construct a set 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#21462;&#20195;&#32463;&#27982;&#23454;&#39564;&#23460;&#36827;&#34892;&#36873;&#25321;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#30456;&#20851;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#21487;&#34892;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.17435</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#21462;&#20195;&#32463;&#27982;&#36873;&#25321;&#39044;&#27979;&#23454;&#39564;&#23460;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models Replace Economic Choice Prediction Labs?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17435
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#21462;&#20195;&#32463;&#27982;&#23454;&#39564;&#23460;&#36827;&#34892;&#36873;&#25321;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#30456;&#20851;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32463;&#27982;&#36873;&#25321;&#39044;&#27979;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#37325;&#35201;&#20219;&#21153;&#65292;&#24448;&#24448;&#21463;&#38480;&#20110;&#33719;&#21462;&#20154;&#31867;&#36873;&#25321;&#25968;&#25454;&#30340;&#22256;&#38590;&#12290;&#23454;&#39564;&#32463;&#27982;&#23398;&#30740;&#31350;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#19987;&#27880;&#20110;&#31616;&#21333;&#30340;&#36873;&#25321;&#29615;&#22659;&#12290;&#26368;&#36817;&#65292;&#20154;&#24037;&#26234;&#33021;&#30028;&#20197;&#20004;&#31181;&#26041;&#24335;&#20026;&#35813;&#21162;&#21147;&#20570;&#20986;&#20102;&#36129;&#29486;&#65306;&#32771;&#34385;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#21487;&#20197;&#20195;&#26367;&#20154;&#31867;&#22312;&#19978;&#36848;&#31616;&#21333;&#36873;&#25321;&#39044;&#27979;&#29615;&#22659;&#20013;&#65292;&#20197;&#21450;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#35270;&#35282;&#30740;&#31350;&#26356;&#22797;&#26434;&#20294;&#20173;&#20005;&#26684;&#30340;&#23454;&#39564;&#32463;&#27982;&#23398;&#29615;&#22659;&#65292;&#21253;&#25324;&#19981;&#23436;&#20840;&#20449;&#24687;&#12289;&#37325;&#22797;&#21338;&#24328;&#21644;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#20132;&#27969;&#30340;&#35828;&#26381;&#28216;&#25103;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#28789;&#24863;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#23436;&#20840;&#27169;&#25311;&#32463;&#27982;&#29615;&#22659;&#65292;&#24182;&#29983;&#25104;&#29992;&#20110;&#39640;&#25928;&#20154;&#31867;&#36873;&#25321;&#39044;&#27979;&#30340;&#25968;&#25454;&#65292;&#26367;&#20195;&#22797;&#26434;&#30340;&#32463;&#27982;&#23454;&#39564;&#23460;&#30740;&#31350;&#65311;&#25105;&#20204;&#22312;&#36825;&#20010;&#20027;&#39064;&#19978;&#24320;&#21019;&#20102;&#30740;&#31350;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#21487;&#34892;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#34920;&#26126;&#20165;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Economic choice prediction is an essential challenging task, often constrained by the difficulties in acquiring human choice data. Indeed, experimental economics studies had focused mostly on simple choice settings. The AI community has recently contributed to that effort in two ways: considering whether LLMs can substitute for humans in the above-mentioned simple choice prediction settings, and the study through ML lens of more elaborated but still rigorous experimental economics settings, employing incomplete information, repetitive play, and natural language communication, notably language-based persuasion games. This leaves us with a major inspiration: can LLMs be used to fully simulate the economic environment and generate data for efficient human choice prediction, substituting for the elaborated economic lab studies? We pioneer the study of this subject, demonstrating its feasibility. In particular, we show that a model trained solely on LLM-generated data can effectively predic
&lt;/p&gt;</description></item><item><title>DurFlex-EVC&#36890;&#36807;&#24341;&#20837;&#39118;&#26684;&#33258;&#32534;&#30721;&#22120;&#21644;&#20132;&#21449;&#27880;&#24847;&#21147;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#24773;&#32490;&#35821;&#38899;&#36716;&#25442;&#27169;&#22411;&#20013;&#23545;&#35821;&#35328;&#21644;&#35821;&#38899;&#20449;&#24687;&#30340;&#21516;&#27493;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2401.08095</link><description>&lt;p&gt;
DurFlex-EVC: &#20855;&#26377;&#24182;&#34892;&#29983;&#25104;&#30340;&#25345;&#32493;&#28789;&#27963;&#24773;&#32490;&#35821;&#38899;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
DurFlex-EVC: Duration-Flexible Emotional Voice Conversion with Parallel Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.08095
&lt;/p&gt;
&lt;p&gt;
DurFlex-EVC&#36890;&#36807;&#24341;&#20837;&#39118;&#26684;&#33258;&#32534;&#30721;&#22120;&#21644;&#20132;&#21449;&#27880;&#24847;&#21147;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#24773;&#32490;&#35821;&#38899;&#36716;&#25442;&#27169;&#22411;&#20013;&#23545;&#35821;&#35328;&#21644;&#35821;&#38899;&#20449;&#24687;&#30340;&#21516;&#27493;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#32490;&#35821;&#38899;&#36716;&#25442;&#65288;EVC&#65289;&#26088;&#22312;&#20462;&#25913;&#35828;&#35805;&#32773;&#22768;&#38899;&#30340;&#24773;&#32490;&#33394;&#24425;&#65292;&#21516;&#26102;&#20445;&#30041;&#21407;&#22987;&#30340;&#35821;&#35328;&#20869;&#23481;&#21644;&#35828;&#35805;&#32773;&#29420;&#29305;&#30340;&#22768;&#38899;&#29305;&#24449;&#12290;&#26368;&#36817;EVC&#30340;&#36827;&#23637;&#28041;&#21450;&#21516;&#26102;&#24314;&#27169;&#38899;&#39640;&#21644;&#25345;&#32493;&#26102;&#38388;&#65292;&#21033;&#29992;&#24207;&#21015;&#21040;&#24207;&#21015;&#65288;seq2seq&#65289;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;&#20026;&#20102;&#22686;&#24378;&#36716;&#25442;&#30340;&#21487;&#38752;&#24615;&#21644;&#25928;&#29575;&#65292;&#26412;&#30740;&#31350;&#23558;&#37325;&#28857;&#36716;&#21521;&#24182;&#34892;&#35821;&#38899;&#29983;&#25104;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;DurFlex-EVC&#65292;&#23427;&#38598;&#25104;&#20102;&#39118;&#26684;&#33258;&#32534;&#30721;&#22120;&#21644;&#21333;&#20803;&#23545;&#40784;&#22120;&#12290;&#20256;&#32479;&#27169;&#22411;&#34429;&#28982;&#34701;&#20837;&#20102;&#21253;&#21547;&#35821;&#35328;&#21644;&#35821;&#38899;&#20449;&#24687;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#34920;&#31034;&#65292;&#20294;&#21364;&#24573;&#35270;&#20102;&#36825;&#31181;&#21452;&#37325;&#24615;&#36136;&#65292;&#23548;&#33268;&#20102;&#21487;&#25511;&#24615;&#30340;&#38477;&#20302;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#20132;&#21449;&#27880;&#24847;&#21147;&#20197;&#23558;&#36825;&#20123;&#34920;&#31034;&#19982;&#19981;&#21516;&#24773;&#32490;&#36827;&#34892;&#21516;&#27493;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#39118;&#26684;&#33258;&#32534;&#30721;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.08095v2 Announce Type: replace-cross  Abstract: Emotional voice conversion (EVC) seeks to modify the emotional tone of a speaker's voice while preserving the original linguistic content and the speaker's unique vocal characteristics. Recent advancements in EVC have involved the simultaneous modeling of pitch and duration, utilizing the potential of sequence-to-sequence (seq2seq) models. To enhance reliability and efficiency in conversion, this study shifts focus towards parallel speech generation. We introduce Duration-Flexible EVC (DurFlex-EVC), which integrates a style autoencoder and unit aligner. Traditional models, while incorporating self-supervised learning (SSL) representations that contain both linguistic and paralinguistic information, have neglected this dual nature, leading to reduced controllability. Addressing this issue, we implement cross-attention to synchronize these representations with various emotions. Additionally, a style autoencoder is developed for t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#33268;&#21147;&#20110;&#36890;&#36807;&#24341;&#20837;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#26469;&#20248;&#21270;Transformer&#39044;&#27979;&#26694;&#26550;&#65292;&#20197;&#20943;&#23569;&#20887;&#20313;&#20449;&#24687;&#24182;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#36816;&#34892;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2401.00230</link><description>&lt;p&gt;
Transformer&#22810;&#20803;&#39044;&#27979;&#65306;&#23569;&#21363;&#26159;&#22810;&#65311;
&lt;/p&gt;
&lt;p&gt;
Transformer Multivariate Forecasting: Less is More?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.00230
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#33268;&#21147;&#20110;&#36890;&#36807;&#24341;&#20837;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#26469;&#20248;&#21270;Transformer&#39044;&#27979;&#26694;&#26550;&#65292;&#20197;&#20943;&#23569;&#20887;&#20313;&#20449;&#24687;&#24182;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#36816;&#34892;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#20803;&#39044;&#27979;&#39046;&#22495;&#65292;Transformer&#27169;&#22411;&#20316;&#20026;&#24378;&#22823;&#30340;&#24037;&#20855;&#33073;&#39062;&#32780;&#20986;&#65292;&#23637;&#29616;&#20986;&#22312;&#22788;&#29702;&#26469;&#33258;&#30495;&#23454;&#22330;&#26223;&#20013;&#26434;&#20081;&#25968;&#25454;&#38598;&#26041;&#38754;&#30340;&#24322;&#24120;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#30340;&#22266;&#26377;&#22797;&#26434;&#24615;&#65292;&#20197;&#20247;&#22810;&#21464;&#37327;&#21644;&#28459;&#38271;&#26102;&#38388;&#24207;&#21015;&#20026;&#29305;&#24449;&#65292;&#24102;&#26469;&#25361;&#25112;&#65292;&#21253;&#25324;&#22686;&#21152;&#30340;&#22122;&#38899;&#21644;&#24310;&#38271;&#30340;&#27169;&#22411;&#36816;&#34892;&#26102;&#38388;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#20943;&#23569;&#20887;&#20313;&#20449;&#24687;&#26469;&#25552;&#39640;&#39044;&#27979;&#31934;&#24230;&#65292;&#21516;&#26102;&#20248;&#21270;&#36816;&#34892;&#26102;&#38388;&#25928;&#29575;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Transformer&#39044;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#20116;&#31181;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#21644;&#22235;&#20010;&#19981;&#21516;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#22312;&#25152;&#26377;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#19978;&#20943;&#23569;&#39044;&#27979;&#35823;&#24046;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#26174;&#33879;&#38477;&#20302;&#36816;&#34892;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.00230v2 Announce Type: replace-cross  Abstract: In the domain of multivariate forecasting, transformer models stand out as powerful apparatus, displaying exceptional capabilities in handling messy datasets from real-world contexts. However, the inherent complexity of these datasets, characterized by numerous variables and lengthy temporal sequences, poses challenges, including increased noise and extended model runtime. This paper focuses on reducing redundant information to elevate forecasting accuracy while optimizing runtime efficiency. We propose a novel transformer forecasting framework enhanced by Principal Component Analysis (PCA) to tackle this challenge. The framework is evaluated by five state-of-the-art (SOTA) models and four diverse real-world datasets. Our experimental results demonstrate the framework's ability to minimize prediction errors across all models and datasets while significantly reducing runtime. From the model perspective, one of the PCA-enhanced m
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#20010;&#35745;&#31639;&#26694;&#26550;&#65292;&#21033;&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26469;&#25214;&#21040;&#38024;&#23545;&#20132;&#36890;&#32593;&#32476;&#30340;&#26368;&#22351;&#24773;&#20917;&#25968;&#25454;&#27880;&#20837;&#25915;&#20987;&#65292;&#20197;&#35780;&#20272;&#34394;&#20551;&#25968;&#25454;&#27880;&#20837;&#25915;&#20987;&#23545;&#20132;&#36890;&#32593;&#32476;&#30340;&#23041;&#32961;&#12290;</title><link>https://arxiv.org/abs/2312.14625</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#35780;&#20272;&#20132;&#36890;&#32593;&#32476;&#20013;&#30340;&#34394;&#20551;&#25968;&#25454;&#27880;&#20837;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Multi-Agent Reinforcement Learning for Assessing False-Data Injection Attacks on Transportation Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.14625
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#20010;&#35745;&#31639;&#26694;&#26550;&#65292;&#21033;&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26469;&#25214;&#21040;&#38024;&#23545;&#20132;&#36890;&#32593;&#32476;&#30340;&#26368;&#22351;&#24773;&#20917;&#25968;&#25454;&#27880;&#20837;&#25915;&#20987;&#65292;&#20197;&#35780;&#20272;&#34394;&#20551;&#25968;&#25454;&#27880;&#20837;&#25915;&#20987;&#23545;&#20132;&#36890;&#32593;&#32476;&#30340;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39550;&#39542;&#21592;&#23545;&#23548;&#33322;&#24212;&#29992;&#31243;&#24207;&#30340;&#26085;&#30410;&#20381;&#36182;&#20351;&#24471;&#20132;&#36890;&#32593;&#32476;&#26356;&#23481;&#26131;&#21463;&#21040;&#24694;&#24847;&#34892;&#20026;&#32773;&#30340;&#25968;&#25454;&#31713;&#25913;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#23545;&#25163;&#21487;&#33021;&#21033;&#29992;&#23548;&#33322;&#26381;&#21153;&#30340;&#25968;&#25454;&#25910;&#38598;&#25110;&#22788;&#29702;&#26041;&#38754;&#30340;&#28431;&#27934;&#26469;&#27880;&#20837;&#34394;&#20551;&#20449;&#24687;&#65292;&#20174;&#32780;&#24178;&#25200;&#39550;&#39542;&#21592;&#30340;&#36335;&#24452;&#36873;&#25321;&#12290;&#36825;&#31181;&#25915;&#20987;&#21487;&#33021;&#26174;&#33879;&#22686;&#21152;&#20132;&#36890;&#25317;&#22581;&#65292;&#23548;&#33268;&#26102;&#38388;&#21644;&#36164;&#28304;&#30340;&#22823;&#37327;&#28010;&#36153;&#65292;&#29978;&#33267;&#21487;&#33021;&#30772;&#22351;&#20381;&#36182;&#36947;&#36335;&#32593;&#32476;&#30340;&#22522;&#26412;&#26381;&#21153;&#12290;&#20026;&#20102;&#35780;&#20272;&#27492;&#31867;&#25915;&#20987;&#24102;&#26469;&#30340;&#23041;&#32961;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#35745;&#31639;&#26694;&#26550;&#26469;&#21457;&#29616;&#38024;&#23545;&#20132;&#36890;&#32593;&#32476;&#30340;&#26368;&#22351;&#24773;&#20917;&#25968;&#25454;&#27880;&#20837;&#25915;&#20987;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#25317;&#26377;&#23041;&#32961;&#34892;&#20026;&#32773;&#30340;&#23545;&#25239;&#27169;&#22411;&#65292;&#35813;&#34892;&#20026;&#32773;&#21487;&#20197;&#36890;&#36807;&#22686;&#21152;&#39550;&#39542;&#21592;&#22312;&#26576;&#20123;&#36947;&#36335;&#19978;&#24863;&#30693;&#30340;&#34892;&#39542;&#26102;&#38388;&#26469;&#25805;&#32437;&#20182;&#20204;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#37319;&#29992;&#20998;&#23618;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26469;&#25214;&#21040;&#19968;&#31181;&#36817;&#20284;&#26368;&#20339;&#30340;&#23545;&#25239;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.14625v2 Announce Type: replace  Abstract: The increasing reliance of drivers on navigation applications has made transportation networks more susceptible to data-manipulation attacks by malicious actors. Adversaries may exploit vulnerabilities in the data collection or processing of navigation services to inject false information, and to thus interfere with the drivers' route selection. Such attacks can significantly increase traffic congestions, resulting in substantial waste of time and resources, and may even disrupt essential services that rely on road networks. To assess the threat posed by such attacks, we introduce a computational framework to find worst-case data-injection attacks against transportation networks. First, we devise an adversarial model with a threat actor who can manipulate drivers by increasing the travel times that they perceive on certain roads. Then, we employ hierarchical multi-agent reinforcement learning to find an approximate optimal adversaria
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;Contrastive Activation Addition&#65288;CAA&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#20462;&#25913;&#35821;&#35328;&#27169;&#22411;&#30340;&#28608;&#27963;&#26469;&#31934;&#30830;&#25511;&#21046;&#30446;&#26631;&#34892;&#20026;&#30340;&#31243;&#24230;&#65292;&#26174;&#33879;&#25913;&#21464;&#27169;&#22411;&#34892;&#20026;&#24182;&#22312;&#24494;&#35843;&#21644;&#31995;&#32479;&#25552;&#31034;&#35774;&#35745;&#30340;&#22522;&#30784;&#19978;&#25552;&#20379;&#39069;&#22806;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2312.06681</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#28608;&#27963;&#21152;&#27861;&#25351;&#23548;Llama 2
&lt;/p&gt;
&lt;p&gt;
Steering Llama 2 via Contrastive Activation Addition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.06681
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;Contrastive Activation Addition&#65288;CAA&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#20462;&#25913;&#35821;&#35328;&#27169;&#22411;&#30340;&#28608;&#27963;&#26469;&#31934;&#30830;&#25511;&#21046;&#30446;&#26631;&#34892;&#20026;&#30340;&#31243;&#24230;&#65292;&#26174;&#33879;&#25913;&#21464;&#27169;&#22411;&#34892;&#20026;&#24182;&#22312;&#24494;&#35843;&#21644;&#31995;&#32479;&#25552;&#31034;&#35774;&#35745;&#30340;&#22522;&#30784;&#19978;&#25552;&#20379;&#39069;&#22806;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;Contrastive Activation Addition&#65288;CAA&#65289;&#65292;&#29992;&#20110;&#36890;&#36807;&#22312;&#21069;&#21521;&#20256;&#36882;&#36807;&#31243;&#20013;&#20462;&#25913;&#20854;&#28608;&#27963;&#26469;&#25351;&#23548;&#35821;&#35328;&#27169;&#22411;&#12290;CAA&#36890;&#36807;&#23545;&#26576;&#31181;&#34892;&#20026;&#30340;&#27491;&#38754;&#21644;&#36127;&#38754;&#31034;&#20363;&#20043;&#38388;&#27531;&#24046;&#27969;&#28608;&#27963;&#30340;&#24046;&#24322;&#27714;&#24179;&#22343;&#65292;&#35745;&#31639;&#20986;&#8220;&#25351;&#23548;&#21521;&#37327;&#8221;&#12290;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#65292;&#22312;&#29992;&#25143;&#25552;&#31034;&#21518;&#30340;&#25152;&#26377;token&#20301;&#32622;&#19978;&#20197;&#27491;&#36127;&#31995;&#25968;&#28155;&#21152;&#36825;&#20123;&#25351;&#23548;&#21521;&#37327;&#65292;&#20174;&#32780;&#31934;&#30830;&#25511;&#21046;&#30446;&#26631;&#34892;&#20026;&#30340;&#31243;&#24230;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#22810;&#39033;&#36873;&#25321;&#34892;&#20026;&#38382;&#39064;&#25968;&#25454;&#38598;&#21644;&#24320;&#25918;&#24335;&#29983;&#25104;&#20219;&#21153;&#22312;Llama 2 Chat&#19978;&#35780;&#20272;&#20102;CAA&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;CAA&#26174;&#30528;&#25913;&#21464;&#20102;&#27169;&#22411;&#34892;&#20026;&#65292;&#19981;&#20165;&#22312;&#20256;&#32479;&#26041;&#27861;&#22914;&#24494;&#35843;&#21644;&#31995;&#32479;&#25552;&#31034;&#35774;&#35745;&#30340;&#22522;&#30784;&#19978;&#26377;&#25928;&#65292;&#32780;&#19988;&#26368;&#23567;&#31243;&#24230;&#22320;&#38477;&#20302;&#20102;&#21151;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#27169;&#22411;&#30340;&#34892;&#20026;&#20570;&#20986;&#20102;&#26356;&#28145;&#20837;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.06681v3 Announce Type: replace-cross  Abstract: We introduce Contrastive Activation Addition (CAA), an innovative method for steering language models by modifying their activations during forward passes. CAA computes "steering vectors" by averaging the difference in residual stream activations between pairs of positive and negative examples of a particular behavior, such as factual versus hallucinatory responses. During inference, these steering vectors are added at all token positions after the user's prompt with either a positive or negative coefficient, allowing precise control over the degree of the targeted behavior. We evaluate CAA's effectiveness on Llama 2 Chat using multiple-choice behavioral question datasets and open-ended generation tasks. We demonstrate that CAA significantly alters model behavior, is effective over and on top of traditional methods like finetuning and system prompt design, and minimally reduces capabilities. Moreover, we gain deeper insights in
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DPMs&#65289;&#29983;&#25104;&#26032;&#29305;&#24449;&#32452;&#21512;&#30340;&#22270;&#20687;&#65292;&#21487;&#20197;&#22312;&#38598;&#25104;&#27169;&#22411;&#20013;&#22686;&#21152;&#27169;&#22411;&#22810;&#26679;&#24615;&#65292;&#24182;&#20943;&#36731;&#25463;&#24452;&#20559;&#35265;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30417;&#30563;&#20449;&#21495;&#12290;</title><link>https://arxiv.org/abs/2311.16176</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#26679;&#21270;&#21512;&#25104;&#21644;&#25193;&#25955;&#27169;&#22411;&#20943;&#36731;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Mitigating Biases with Diverse Ensembles and Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.16176
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DPMs&#65289;&#29983;&#25104;&#26032;&#29305;&#24449;&#32452;&#21512;&#30340;&#22270;&#20687;&#65292;&#21487;&#20197;&#22312;&#38598;&#25104;&#27169;&#22411;&#20013;&#22686;&#21152;&#27169;&#22411;&#22810;&#26679;&#24615;&#65292;&#24182;&#20943;&#36731;&#25463;&#24452;&#20559;&#35265;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30417;&#30563;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#20013;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#65292;&#21363;&#22810;&#20010;&#32447;&#32034;&#21487;&#20197;&#39044;&#27979;&#30446;&#26631;&#26631;&#31614;&#65292;&#24120;&#24120;&#23548;&#33268;&#19968;&#31181;&#31216;&#20026;&#25463;&#24452;&#20559;&#35265;&#30340;&#29616;&#35937;&#65292;&#21363;&#27169;&#22411;&#20381;&#36182;&#20110;&#38169;&#35823;&#30340;&#12289;&#26131;&#23398;&#30340;&#32447;&#32034;&#65292;&#32780;&#24573;&#30053;&#21487;&#38752;&#30340;&#32447;&#32034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DPMs&#65289;&#30340;&#38598;&#25104;&#22810;&#26679;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#20943;&#36731;&#25463;&#24452;&#20559;&#35265;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#29305;&#23450;&#30340;&#35757;&#32451;&#38388;&#38548;&#20013;&#65292;DPMs&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#26032;&#29305;&#24449;&#32452;&#21512;&#30340;&#22270;&#20687;&#65292;&#21363;&#20351;&#22312;&#26174;&#31034;&#30456;&#20851;&#36755;&#20837;&#29305;&#24449;&#30340;&#26679;&#26412;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#19968;&#20851;&#38190;&#23646;&#24615;&#36890;&#36807;&#38598;&#25104;&#19981;&#19968;&#33268;&#24615;&#29983;&#25104;&#21512;&#25104;&#21453;&#20107;&#23454;&#26469;&#22686;&#21152;&#27169;&#22411;&#30340;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;DPM&#24341;&#23548;&#30340;&#22810;&#26679;&#21270;&#36275;&#20197;&#28040;&#38500;&#23545;&#20027;&#35201;&#25463;&#24452;&#32447;&#32034;&#30340;&#20381;&#36182;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#30417;&#30563;&#20449;&#21495;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#22312;&#20960;&#20010;&#22810;&#26679;&#21270;&#30446;&#26631;&#19978;&#22312;&#23454;&#35777;&#19978;&#37327;&#21270;&#20854;&#26377;&#25928;&#24615;&#65292;&#24182;&#26368;&#32456;&#23637;&#31034;&#20102;&#25913;&#36827;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.16176v2 Announce Type: replace-cross  Abstract: Spurious correlations in the data, where multiple cues are predictive of the target labels, often lead to a phenomenon known as shortcut bias, where a model relies on erroneous, easy-to-learn cues while ignoring reliable ones. In this work, we propose an ensemble diversification framework exploiting Diffusion Probabilistic Models (DPMs) for shortcut bias mitigation. We show that at particular training intervals, DPMs can generate images with novel feature combinations, even when trained on samples displaying correlated input features. We leverage this crucial property to generate synthetic counterfactuals to increase model diversity via ensemble disagreement. We show that DPM-guided diversification is sufficient to remove dependence on primary shortcut cues, without a need for additional supervised signals. We further empirically quantify its efficacy on several diversification objectives, and finally show improved generalizati
&lt;/p&gt;</description></item><item><title>InteRACT&#36890;&#36807;&#22312;&#22823;&#22411;&#20154;&#31867;-&#20154;&#31867;&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;&#26465;&#20214;&#24847;&#22270;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#22312;&#23567;&#22411;&#20154;&#26426;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;&#65292;&#35299;&#20915;&#20102;&#20154;&#26426;&#20132;&#20114;&#20013;&#30340;&#20808;&#26377;&#40481;&#36824;&#26159;&#20808;&#26377;&#34507;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2311.12943</link><description>&lt;p&gt;
InteRACT&#65306;&#22522;&#20110;&#26426;&#22120;&#20154;&#21160;&#20316;&#30340;&#20154;&#31867;&#24847;&#22270;&#39044;&#27979;&#30340;Transformer&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
InteRACT: Transformer Models for Human Intent Prediction Conditioned on Robot Actions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.12943
&lt;/p&gt;
&lt;p&gt;
InteRACT&#36890;&#36807;&#22312;&#22823;&#22411;&#20154;&#31867;-&#20154;&#31867;&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;&#26465;&#20214;&#24847;&#22270;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#22312;&#23567;&#22411;&#20154;&#26426;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;&#65292;&#35299;&#20915;&#20102;&#20154;&#26426;&#20132;&#20114;&#20013;&#30340;&#20808;&#26377;&#40481;&#36824;&#26159;&#20808;&#26377;&#34507;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21327;&#20316;&#30340;&#20154;&#26426;&#25805;&#32437;&#20013;&#65292;&#26426;&#22120;&#20154;&#24517;&#39035;&#39044;&#27979;&#20154;&#31867;&#24847;&#22270;&#24182;&#30456;&#24212;&#35843;&#25972;&#20854;&#34892;&#21160;&#65292;&#20197;&#24179;&#31283;&#25191;&#34892;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#30340;&#24847;&#22270;&#21453;&#36807;&#26469;&#21448;&#21462;&#20915;&#20110;&#26426;&#22120;&#20154;&#37319;&#21462;&#30340;&#21160;&#20316;&#65292;&#36896;&#25104;&#20102;&#19968;&#20010;&#20808;&#26377;&#40481;&#36824;&#26159;&#20808;&#26377;&#34507;&#30340;&#38382;&#39064;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#24573;&#30053;&#20102;&#36825;&#31181;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#65292;&#32780;&#26159;&#35757;&#32451;&#29420;&#31435;&#20110;&#26426;&#22120;&#20154;&#34892;&#21160;&#30340;&#36793;&#38469;&#24847;&#22270;&#39044;&#27979;&#27169;&#22411;&#12290;&#36825;&#26159;&#22240;&#20026;&#22312;&#32570;&#20047;&#37197;&#23545;&#30340;&#20154;&#26426;&#20132;&#20114;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#65292;&#35757;&#32451;&#26465;&#20214;&#27169;&#22411;&#26159;&#22256;&#38590;&#30340;&#12290;&#25105;&#20204;&#33021;&#21542;&#36716;&#32780;&#21033;&#29992;&#26356;&#23481;&#26131;&#33719;&#21462;&#30340;&#22823;&#35268;&#27169;&#20154;&#31867;-&#20154;&#31867;&#20132;&#20114;&#25968;&#25454;&#65311;&#25105;&#20204;&#30340;&#20851;&#38190;&#35265;&#35299;&#26159;&#21033;&#29992;&#20154;&#31867;&#21644;&#26426;&#22120;&#20154;&#34892;&#21160;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#65292;&#23454;&#29616;&#20174;&#20154;&#31867;-&#20154;&#31867;&#21040;&#20154;&#31867;-&#26426;&#22120;&#20154;&#25968;&#25454;&#30340;&#36801;&#31227;&#23398;&#20064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26550;&#26500;InteRACT&#65292;&#35813;&#26550;&#26500;&#22312;&#22823;&#22411;&#20154;&#31867;-&#20154;&#31867;&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;&#26465;&#20214;&#24847;&#22270;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#22312;&#23567;&#22411;&#20154;&#26426;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#22312;&#19968;&#32452;&#30495;&#23454;&#19990;&#30028;&#30340;&#21327;&#20316;&#25968;&#25454;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.12943v2 Announce Type: replace-cross  Abstract: In collaborative human-robot manipulation, a robot must predict human intents and adapt its actions accordingly to smoothly execute tasks. However, the human's intent in turn depends on actions the robot takes, creating a chicken-or-egg problem. Prior methods ignore such inter-dependency and instead train marginal intent prediction models independent of robot actions. This is because training conditional models is hard given a lack of paired human-robot interaction datasets. Can we instead leverage large-scale human-human interaction data that is more easily accessible? Our key insight is to exploit a correspondence between human and robot actions that enables transfer learning from human-human to human-robot data. We propose a novel architecture, InteRACT, that pre-trains a conditional intent prediction model on large human-human datasets and fine-tunes on a small human-robot dataset. We evaluate on a set of real-world collabo
&lt;/p&gt;</description></item><item><title>&#22312;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#28982;&#35821;&#35328;&#25509;&#21475;&#26102;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29992;&#25143;&#24120;&#35774;&#25351;&#20196;&#30340;&#23545;&#35805;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#29992;&#25143;&#30340;&#32422;&#26463;&#21644;&#20559;&#22909;&#20316;&#20026;&#19978;&#19979;&#25991;&#65292;&#20174;&#32780;&#22312;&#31867;&#20284;&#35831;&#27714;&#20013;&#23454;&#29616;&#33258;&#21160;&#21270;&#12290;</title><link>https://arxiv.org/abs/2311.09796</link><description>&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#20132;&#20114;&#25351;&#20196;&#29615;&#22659;&#20013;&#35299;&#37322;&#29992;&#25143;&#35831;&#27714;
&lt;/p&gt;
&lt;p&gt;
Interpreting User Requests in the Context of Natural Language Standing Instructions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09796
&lt;/p&gt;
&lt;p&gt;
&#22312;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#28982;&#35821;&#35328;&#25509;&#21475;&#26102;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29992;&#25143;&#24120;&#35774;&#25351;&#20196;&#30340;&#23545;&#35805;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#29992;&#25143;&#30340;&#32422;&#26463;&#21644;&#20559;&#22909;&#20316;&#20026;&#19978;&#19979;&#25991;&#65292;&#20174;&#32780;&#22312;&#31867;&#20284;&#35831;&#27714;&#20013;&#23454;&#29616;&#33258;&#21160;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#25509;&#21475;&#30340;&#29992;&#25143;&#36890;&#24120;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39537;&#21160;&#65292;&#24182;&#19988;&#32463;&#24120;&#24517;&#39035;&#22312;&#27599;&#27425;&#36827;&#34892;&#31867;&#20284;&#35831;&#27714;&#26102;&#37325;&#22797;&#20182;&#20204;&#30340;&#20559;&#22909;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#23545;&#35805;&#24314;&#27169;&#26041;&#27861;&#65292;&#20854;&#20013;&#25345;&#20037;&#30340;&#29992;&#25143;&#32422;&#26463;&#21644;&#20559;&#22909; - &#32479;&#31216;&#20026;&#24120;&#35774;&#25351;&#20196; - &#20316;&#20026;&#36825;&#31181;&#25509;&#21475;&#30340;&#39069;&#22806;&#19978;&#19979;&#25991;&#12290;&#20363;&#22914;&#65292;&#24403;&#29992;&#25143;&#35828;&#8220;&#25105;&#39295;&#20102;&#8221;&#26102;&#65292;&#20808;&#21069;&#34920;&#36798;&#30340;&#27874;&#26031;&#39135;&#29289;&#20559;&#22909;&#21487;&#20197;&#33258;&#21160;&#28155;&#21152;&#21040;LLM&#25552;&#31034;&#20013;&#65292;&#24433;&#21709;&#25628;&#32034;&#30456;&#20851;&#39184;&#39302;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;NLSI&#65292;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;2.4K&#36328;&#36234;17&#20010;&#39046;&#22495;&#30340;&#23545;&#35805;&#30340;&#35821;&#35328;&#21040;&#31243;&#24207;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#27599;&#20010;&#23545;&#35805;&#37117;&#19982;&#29992;&#25143;&#37197;&#32622;&#25991;&#20214;&#65288;&#19968;&#32452;&#29992;&#25143;&#29305;&#23450;&#30340;&#24120;&#35774;&#25351;&#20196;&#65289;&#21644;&#30456;&#24212;&#30340;&#32467;&#26500;&#21270;&#34920;&#31034;&#24418;&#24335;&#65288;API&#35843;&#29992;&#65289;&#37197;&#23545;&#12290; NLSI&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#30830;&#23450;&#21738;&#20123;&#24120;&#35774;&#25351;&#20196;&#23376;&#38598;&#36866;&#29992;&#20110;&#32473;&#23450;&#30340;&#23545;&#35805;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09796v2 Announce Type: replace-cross  Abstract: Users of natural language interfaces, generally powered by Large Language Models (LLMs),often must repeat their preferences each time they make a similar request. We describe an approach to LLM-based dialogue modeling in which persistent user constraints and preferences -- collectively termed standing instructions -- as additional context for such interfaces. For example, when a user states "I'm hungry", a previously expressed preference for Persian food can be automatically added to the LLM prompt, influencing the search for relevant restaurants. We develop NLSI, a language-to-program dataset consisting of over 2.4K dialogues spanning 17 domains, where each dialogue is paired with a user profile (a set of users specific standing instructions) and corresponding structured representations (API calls). A key challenge in NLSI is to identify which subset of the standing instructions is applicable to a given dialogue. NLSI contains
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32771;&#23519;&#20102;VIT&#23545;&#22522;&#20110;&#39592;&#39612;&#30340;&#21160;&#20316;&#35782;&#21035;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#19977;&#32423;&#26550;&#26500;SkelVit&#65292;&#35813;&#26550;&#26500;&#33021;&#22815;&#24418;&#25104;&#19968;&#32452;&#20266;&#22270;&#20687;&#65292;&#24182;&#22312;&#27599;&#20010;&#34920;&#31034;&#19978;&#24212;&#29992;&#20998;&#31867;&#22120;&#65292;&#28982;&#21518;&#32467;&#21512;&#23427;&#20204;&#30340;&#32467;&#26524;&#26469;&#25214;&#21040;&#21160;&#20316;&#35782;&#21035;&#31995;&#32479;&#20013;&#30340;&#26368;&#20339;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2311.08094</link><description>&lt;p&gt;
SkelVIT&#65306;&#36731;&#37327;&#32423;&#22522;&#20110;&#39592;&#39612;&#30340;&#21160;&#20316;&#35782;&#21035;&#31995;&#32479;&#30340;&#35270;&#35273;&#21464;&#21387;&#22120;&#20849;&#35782;
&lt;/p&gt;
&lt;p&gt;
SkelVIT: Consensus of Vision Transformers for a Lightweight Skeleton-Based Action Recognition System
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.08094
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32771;&#23519;&#20102;VIT&#23545;&#22522;&#20110;&#39592;&#39612;&#30340;&#21160;&#20316;&#35782;&#21035;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#19977;&#32423;&#26550;&#26500;SkelVit&#65292;&#35813;&#26550;&#26500;&#33021;&#22815;&#24418;&#25104;&#19968;&#32452;&#20266;&#22270;&#20687;&#65292;&#24182;&#22312;&#27599;&#20010;&#34920;&#31034;&#19978;&#24212;&#29992;&#20998;&#31867;&#22120;&#65292;&#28982;&#21518;&#32467;&#21512;&#23427;&#20204;&#30340;&#32467;&#26524;&#26469;&#25214;&#21040;&#21160;&#20316;&#35782;&#21035;&#31995;&#32479;&#20013;&#30340;&#26368;&#20339;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39592;&#39612;&#22522;&#30784;&#21160;&#20316;&#35782;&#21035;&#24341;&#36215;&#20102;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#30340;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#23545;&#35270;&#35282;&#21644;&#20809;&#29031;&#21464;&#21270;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#20854;&#22788;&#29702;&#27604;&#35270;&#39057;&#24103;&#30340;&#22788;&#29702;&#25928;&#29575;&#39640;&#24471;&#22810;&#12290;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#20197;&#20266;&#22270;&#20687;&#24418;&#24335;&#34920;&#31034;&#39592;&#26550;&#25968;&#25454;&#24182;&#24212;&#29992;CNN&#36827;&#34892;&#21160;&#20316;&#35782;&#21035;&#21464;&#24471;&#38750;&#24120;&#27969;&#34892;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#38598;&#20013;&#22312;&#23547;&#25214;&#24418;&#25104;&#20266;&#22270;&#20687;&#30340;&#26377;&#25928;&#26041;&#27861;&#19978;&#12290;&#26368;&#36817;&#65292;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#26356;&#20855;&#20307;&#22320;&#35828;&#26159;&#21464;&#21387;&#22120;&#65292;&#22312;&#21508;&#31181;&#35270;&#35273;&#38382;&#39064;&#20013;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#26412;&#30740;&#31350;&#32771;&#23519;&#20102;VIT&#23545;&#22522;&#20110;&#39592;&#26550;&#30340;&#21160;&#20316;&#35782;&#21035;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#30740;&#31350;&#20102;&#20854;&#22312;&#20266;&#22270;&#20687;&#34920;&#31034;&#26041;&#26696;&#19978;&#30340;&#31283;&#20581;&#24615;&#12290;&#20026;&#27492;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#32423;&#26550;&#26500;&#65292;SkelVit&#65292;&#23427;&#24418;&#25104;&#19968;&#32452;&#20266;&#22270;&#20687;&#65292;&#23545;&#27599;&#20010;&#34920;&#31034;&#24212;&#29992;&#20998;&#31867;&#22120;&#65292;&#24182;&#32467;&#21512;&#23427;&#20204;&#30340;&#32467;&#26524;&#20197;&#25214;&#21040;f
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.08094v2 Announce Type: replace-cross  Abstract: Skeleton-based action recognition receives the attention of many researchers as it is robust to viewpoint and illumination changes, and its processing is much more efficient than the processing of video frames. With the emergence of deep learning models, it has become very popular to represent the skeleton data in pseudo-image form and apply CNN for action recognition. Thereafter, studies concentrated on finding effective methods for forming pseudo-images. Recently, attention networks, more specifically transformers have provided promising results in various vision problems. In this study, the effectiveness of VIT for skeleton-based action recognition is examined and its robustness on the pseudo-image representation scheme is investigated. To this end, a three-level architecture, SkelVit is proposed, which forms a set of pseudo images, applies a classifier on each of the representations, and combines their results to find the f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;SINGLE&#26041;&#27861;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#29992;&#25143;&#27983;&#35272;&#27969;&#36827;&#34892;&#24314;&#27169;&#65292;&#20197;&#26356;&#22909;&#22320;&#36827;&#34892;&#25991;&#31456;&#25512;&#33616;&#12290;</title><link>https://arxiv.org/abs/2311.07619</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#29992;&#25143;&#27983;&#35272;&#27969;&#36827;&#34892;&#24314;&#27169;&#20197;&#36827;&#34892;&#25991;&#31456;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Modeling User Viewing Flow Using Large Language Models for Article Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.07619
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;SINGLE&#26041;&#27861;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#29992;&#25143;&#27983;&#35272;&#27969;&#36827;&#34892;&#24314;&#27169;&#65292;&#20197;&#26356;&#22909;&#22320;&#36827;&#34892;&#25991;&#31456;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#29992;&#25143;&#27983;&#35272;&#27969;&#24314;&#27169;&#65288;SINGLE&#65289;&#26041;&#27861;&#29992;&#20110;&#25991;&#31456;&#25512;&#33616;&#20219;&#21153;&#65292;&#35813;&#26041;&#27861;&#20174;&#29992;&#25143;&#28857;&#20987;&#30340;&#25991;&#31456;&#20013;&#27169;&#25311;&#29992;&#25143;&#30340;&#25345;&#32493;&#20559;&#22909;&#21644;&#21363;&#26102;&#20852;&#36259;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#37319;&#29992;&#29992;&#25143;&#25345;&#32493;&#27983;&#35272;&#27969;&#24314;&#27169;&#26041;&#27861;&#24635;&#32467;&#29992;&#25143;&#30340;&#19968;&#33324;&#20852;&#36259;&#20197;&#25512;&#33616;&#25991;&#31456;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20174;&#20808;&#21069;&#28857;&#20987;&#30340;&#25991;&#31456;&#20013;&#25429;&#25417;&#29992;&#25143;&#30340;&#25345;&#32493;&#20559;&#22909;&#65292;&#22914;&#25216;&#33021;&#21644;&#32844;&#20301;&#12290;&#28982;&#21518;&#25105;&#20204;&#35774;&#35745;&#29992;&#25143;&#21363;&#26102;&#27983;&#35272;&#27969;&#24314;&#27169;&#26041;&#27861;&#26469;&#26500;&#24314;&#29992;&#25143;&#28857;&#20987;&#30340;&#25991;&#31456;&#21382;&#21490;&#19982;&#20505;&#36873;&#25991;&#31456;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;&#23427;&#19987;&#27880;&#20110;&#38405;&#35835;&#29992;&#25143;&#28857;&#20987;&#25991;&#31456;&#30340;&#34920;&#31034;&#65292;&#24182;&#26088;&#22312;&#23398;&#20064;&#29992;&#25143;&#19981;&#21516;&#30340;&#20852;&#36259;&#35266;&#28857;&#20197;&#21305;&#37197;&#20505;&#36873;&#25991;&#31456;&#12290;&#25105;&#20204;&#22312;&#38463;&#37324;&#24052;&#24052;&#25216;&#26415;&#21327;&#20250;&#65288;ATA&#65289;&#32593;&#31449;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;SINGLE&#30340;&#20248;&#21183;&#65292;&#30456;&#36739;&#20110;&#20808;&#21069;&#30340;&#22522;&#32447;&#23454;&#29616;&#20102;2.4%&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.07619v2 Announce Type: replace-cross  Abstract: This paper proposes the User Viewing Flow Modeling (SINGLE) method for the article recommendation task, which models the user constant preference and instant interest from user-clicked articles. Specifically, we first employ a user constant viewing flow modeling method to summarize the user's general interest to recommend articles. In this case, we utilize Large Language Models (LLMs) to capture constant user preferences from previously clicked articles, such as skills and positions. Then we design the user instant viewing flow modeling method to build interactions between user-clicked article history and candidate articles. It attentively reads the representations of user-clicked articles and aims to learn the user's different interest views to match the candidate article. Our experimental results on the Alibaba Technology Association (ATA) website show the advantage of SINGLE, achieving a 2.4% improvement over previous baseli
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;RuLES&#30340;&#31243;&#24207;&#26694;&#26550;&#65292;&#29992;&#20110;&#34913;&#37327;LLMs&#22312;&#19982;&#29992;&#25143;&#20132;&#20114;&#26102;&#36981;&#23432;&#35268;&#21017;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2311.04235</link><description>&lt;p&gt;
LLM&#33021;&#36981;&#23432;&#31616;&#21333;&#35268;&#21017;&#21527;?
&lt;/p&gt;
&lt;p&gt;
Can LLMs Follow Simple Rules?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.04235
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;RuLES&#30340;&#31243;&#24207;&#26694;&#26550;&#65292;&#29992;&#20110;&#34913;&#37327;LLMs&#22312;&#19982;&#29992;&#25143;&#20132;&#20114;&#26102;&#36981;&#23432;&#35268;&#21017;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#25215;&#25285;&#36234;&#26469;&#36234;&#22810;&#30340;&#36131;&#20219;&#65292;&#33021;&#22815;&#20197;&#21487;&#38752;&#30340;&#26041;&#24335;&#25351;&#23450;&#21644;&#32422;&#26463;&#36825;&#20123;&#31995;&#32479;&#30340;&#34892;&#20026;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#35268;&#21017;&#36981;&#24490;&#35821;&#35328;&#35780;&#20272;&#22330;&#26223;&#65288;RuLES&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#27979;&#37327;LLMs&#36981;&#24490;&#35268;&#21017;&#33021;&#21147;&#30340;&#31243;&#24207;&#26694;&#26550;&#65292;&#21253;&#25324;14&#20010;&#31616;&#21333;&#30340;&#25991;&#26412;&#22330;&#26223;&#65292;&#27169;&#22411;&#22312;&#19982;&#29992;&#25143;&#20132;&#20114;&#26102;&#34987;&#25351;&#31034;&#36981;&#23432;&#21508;&#31181;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.04235v2 Announce Type: replace  Abstract: As Large Language Models (LLMs) are deployed with increasing real-world responsibilities, it is important to be able to specify and constrain the behavior of these systems in a reliable manner. Model developers may wish to set explicit rules for the model, such as "do not generate abusive content", but these may be circumvented by jailbreaking techniques. Existing evaluations of adversarial attacks and defenses on LLMs generally require either expensive manual review or unreliable heuristic checks. To address this issue, we propose Rule-following Language Evaluation Scenarios (RuLES), a programmatic framework for measuring rule-following ability in LLMs. RuLES consists of 14 simple text scenarios in which the model is instructed to obey various rules while interacting with the user. Each scenario has a programmatic evaluation function to determine whether the model has broken any rules in a conversation. Our evaluations of proprietar
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23558;&#35270;&#35273;&#27010;&#24565;&#34701;&#20837;&#26631;&#39064;&#20013;&#30340;&#26041;&#24335;&#26469;&#25913;&#36827;CLIP&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;VeCLIP&#65292;&#35813;&#26041;&#27861;&#22312;&#22823;&#35268;&#27169;&#32593;&#32476;&#29228;&#21462;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2310.07699</link><description>&lt;p&gt;
VeCLIP&#65306;&#36890;&#36807;&#23500;&#21547;&#35270;&#35273;&#20449;&#24687;&#30340;&#26631;&#39064;&#25913;&#36827;CLIP&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
VeCLIP: Improving CLIP Training via Visual-enriched Captions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.07699
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23558;&#35270;&#35273;&#27010;&#24565;&#34701;&#20837;&#26631;&#39064;&#20013;&#30340;&#26041;&#24335;&#26469;&#25913;&#36827;CLIP&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;VeCLIP&#65292;&#35813;&#26041;&#27861;&#22312;&#22823;&#35268;&#27169;&#32593;&#32476;&#29228;&#21462;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#32593;&#32476;&#29228;&#21462;&#25968;&#25454;&#38598;&#23545;&#20110;&#39044;&#35757;&#32451;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;CLIP&#65289;&#30340;&#25104;&#21151;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#32593;&#32476;&#29228;&#21462;&#30340;AltTexts&#23384;&#22312;&#22266;&#26377;&#30340;&#22122;&#38899;&#21644;&#28508;&#22312;&#30340;&#19981;&#30456;&#20851;&#24615;&#65292;&#36896;&#25104;&#20102;&#31934;&#30830;&#30340;&#22270;&#20687;-&#25991;&#23383;&#23545;&#40784;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22024;&#26434;&#26631;&#39064;&#37325;&#20889;&#30340;&#21487;&#25193;&#23637;&#27969;&#31243;&#12290;&#19982;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#26631;&#39064;&#37325;&#20889;&#30340;&#29616;&#26377;&#26041;&#27861;&#22312;&#23567;&#22411;&#31574;&#21010;&#25968;&#25454;&#38598;&#65288;&#22914;CC3M&#21644;CC12M&#65289;&#19978;&#24050;&#32463;&#26174;&#31034;&#20986;&#20102;&#24076;&#26395;&#12290;&#25105;&#20204;&#24378;&#35843;&#23558;&#35270;&#35273;&#27010;&#24565;&#34701;&#20837;&#26631;&#39064;&#20013;&#65292;&#31216;&#20026;&#23500;&#21547;&#35270;&#35273;&#20449;&#24687;&#30340;&#26631;&#39064;&#65288;VeCap&#65289;&#65292;&#20197;&#30830;&#20445;&#25968;&#25454;&#22810;&#26679;&#24615;&#12290;&#20026;&#20102;&#20248;&#21270;AltTexts&#19982;&#26032;&#29983;&#25104;&#30340;VeCap&#30340;&#21033;&#29992;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#21512;&#35757;&#32451;&#26041;&#26696;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#22823;&#35268;&#27169;&#32593;&#32476;&#29228;&#21462;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;CLIP&#30340;&#36866;&#24212;&#24615;&#65292;&#31216;&#20026;VeCLIP&#12290;&#36890;&#36807;&#20351;&#29992;&#36825;&#31181;&#32463;&#27982;&#26377;&#25928;&#30340;&#27969;&#31243;&#65292;&#25105;&#20204;&#36731;&#26494;&#25193;&#23637;&#20102;&#25105;&#20204;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.07699v2 Announce Type: replace-cross  Abstract: Large-scale web-crawled datasets are fundamental for the success of pre-training vision-language models, such as CLIP. However, the inherent noise and potential irrelevance of web-crawled AltTexts pose challenges in achieving precise image-text alignment. Existing methods utilizing large language models (LLMs) for caption rewriting have shown promise on small, curated datasets like CC3M and CC12M. This study introduces a scalable pipeline for noisy caption rewriting. Unlike recent LLM rewriting techniques, we emphasize the incorporation of visual concepts into captions, termed as Visual-enriched Captions (VeCap). To ensure data diversity, we propose a novel mixed training scheme that optimizes the utilization of AltTexts alongside newly generated VeCap. We showcase the adaptation of this method for training CLIP on large-scale web-crawled datasets, termed VeCLIP. Employing this cost-effective pipeline, we effortlessly scale our
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#26681;&#25454;&#26377;&#25928;&#20449;&#24687;&#21644;&#36716;&#31227;&#27010;&#29575;&#30697;&#38453;&#30340;&#37327;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#22240;&#26524;&#20986;&#29616;&#30340;&#25968;&#20540;&#26465;&#20214;&#20316;&#20026;&#20854;&#21457;&#29983;&#30340;&#29702;&#35770;&#32422;&#26463;&#12290;</title><link>https://arxiv.org/abs/2212.01551</link><description>&lt;p&gt;
&#37327;&#21270;&#22240;&#26524;&#20986;&#29616;&#30340;&#21407;&#22240;&#65306;&#22240;&#26524;&#32467;&#26500;&#20013;&#19981;&#30830;&#23450;&#24615;&#21644;&#19981;&#23545;&#31216;&#24615;&#30340;&#20851;&#38190;&#26465;&#20214;
&lt;/p&gt;
&lt;p&gt;
Quantify the Causes of Causal Emergence: Critical Conditions of Uncertainty and Asymmetry in Causal Structure
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2212.01551
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#26681;&#25454;&#26377;&#25928;&#20449;&#24687;&#21644;&#36716;&#31227;&#27010;&#29575;&#30697;&#38453;&#30340;&#37327;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#22240;&#26524;&#20986;&#29616;&#30340;&#25968;&#20540;&#26465;&#20214;&#20316;&#20026;&#20854;&#21457;&#29983;&#30340;&#29702;&#35770;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#30410;&#20110;&#20808;&#36827;&#35745;&#31639;&#35774;&#22791;&#65292;&#20855;&#26377;&#22823;&#37327;&#21442;&#25968;&#30340;&#27169;&#22411;&#36234;&#26469;&#36234;&#34987;&#29992;&#26469;&#25552;&#21462;&#26356;&#22810;&#20449;&#24687;&#65292;&#20197;&#22686;&#24378;&#25551;&#36848;&#21644;&#39044;&#27979;&#23458;&#35266;&#31995;&#32479;&#27169;&#24335;&#30340;&#31934;&#24230;&#12290;&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#32479;&#35745;&#21644;&#20449;&#24687;&#29702;&#35770;&#23545;&#22240;&#26524;&#20851;&#31995;&#30340;&#30740;&#31350;&#32473;&#22823;&#35268;&#27169;&#27169;&#22411;&#25552;&#20986;&#20102;&#26377;&#36259;&#19988;&#26377;&#20215;&#20540;&#30340;&#25361;&#25112;&#12290;&#20855;&#26377;&#36739;&#23569;&#21442;&#25968;&#30340;&#23439;&#35266;&#27169;&#22411;&#21487;&#20197;&#22312;&#26377;&#25928;&#34920;&#31034;&#31995;&#32479;&#26041;&#38754;&#32988;&#36807;&#20855;&#26377;&#26356;&#22810;&#21442;&#25968;&#30340;&#24494;&#35266;&#27169;&#22411;&#12290;&#36825;&#31181;&#26377;&#20215;&#20540;&#30340;&#24773;&#20917;&#34987;&#31216;&#20026;&#8220;&#22240;&#26524;&#20986;&#29616;&#8221;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#37327;&#21270;&#26694;&#26550;&#65292;&#26681;&#25454;&#26377;&#25928;&#20449;&#24687;&#21644;&#36716;&#31227;&#27010;&#29575;&#30697;&#38453;&#65292;&#29992;&#20110;&#35780;&#20272;&#22240;&#26524;&#20986;&#29616;&#30340;&#25968;&#20540;&#26465;&#20214;&#20316;&#20026;&#20854;&#21457;&#29983;&#30340;&#29702;&#35770;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2212.01551v3 Announce Type: replace-cross  Abstract: Beneficial to advanced computing devices, models with massive parameters are increasingly employed to extract more information to enhance the precision in describing and predicting the patterns of objective systems. This phenomenon is particularly pronounced in research domains associated with deep learning. However, investigations of causal relationships based on statistical and informational theories have posed an interesting and valuable challenge to large-scale models in the recent decade. Macroscopic models with fewer parameters can outperform their microscopic counterparts with more parameters in effectively representing the system. This valuable situation is called "Causal Emergence." This paper introduces a quantification framework, according to the Effective Information and Transition Probability Matrix, for assessing numerical conditions of Causal Emergence as theoretical constraints of its occurrence. Specifically, o
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#33410;&#33021;&#21152;&#36895;&#35745;&#23545;&#25856;&#23721;&#36335;&#32447;&#36827;&#34892;&#32858;&#31867;&#65292;&#20026;&#25856;&#23721;&#20581;&#36523;&#25151;&#25552;&#20379;&#20102;&#25913;&#21892;&#26381;&#21153;&#21644;&#26368;&#22823;&#31243;&#24230;&#21033;&#29992;&#22522;&#30784;&#35774;&#26045;&#30340;&#26032;&#26041;&#27861;</title><link>https://arxiv.org/abs/2211.02680</link><description>&lt;p&gt;
&#21033;&#29992;&#33410;&#33021;&#21152;&#36895;&#35745;&#23545;&#25856;&#23721;&#36335;&#32447;&#36827;&#34892;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Climbing Routes Clustering Using Energy-Efficient Accelerometers Attached to the Quickdraws
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2211.02680
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#33410;&#33021;&#21152;&#36895;&#35745;&#23545;&#25856;&#23721;&#36335;&#32447;&#36827;&#34892;&#32858;&#31867;&#65292;&#20026;&#25856;&#23721;&#20581;&#36523;&#25151;&#25552;&#20379;&#20102;&#25913;&#21892;&#26381;&#21153;&#21644;&#26368;&#22823;&#31243;&#24230;&#21033;&#29992;&#22522;&#30784;&#35774;&#26045;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25856;&#23721;&#20581;&#36523;&#25151;&#38754;&#20020;&#30340;&#25361;&#25112;&#20043;&#19968;&#26159;&#25214;&#20986;&#21463;&#27426;&#36814;&#30340;&#25856;&#30331;&#36335;&#32447;&#65292;&#20197;&#25913;&#21892;&#20182;&#20204;&#30340;&#26381;&#21153;&#24182;&#26368;&#22823;&#31243;&#24230;&#22320;&#21033;&#29992;&#20182;&#20204;&#30340;&#22522;&#30784;&#35774;&#26045;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#30828;&#20214;&#21407;&#22411;&#26469;&#25910;&#38598;&#25968;&#25454;&#65292;&#35813;&#21407;&#22411;&#20351;&#29992;&#38468;&#21152;&#21040;&#22681;&#22721;&#19978;&#30340;&#25856;&#23721;&#35774;&#22791;&#30340;&#21152;&#36895;&#35745;&#20256;&#24863;&#22120;&#65292;&#31216;&#20026;&#24555;&#25346;&#65292;&#23558;&#25856;&#23721;&#32499;&#36830;&#25509;&#21040;&#34746;&#26643;&#38170;&#28857;&#12290;&#30456;&#24212;&#30340;&#20256;&#24863;&#22120;&#34987;&#37197;&#32622;&#20026;&#33410;&#33021;&#65292;&#22240;&#27492;&#22312;&#25856;&#23721;&#20581;&#36523;&#25151;&#22823;&#37327;&#20351;&#29992;&#26102;&#65292;&#22312;&#36153;&#29992;&#21644;&#26356;&#25442;&#26102;&#38388;&#19978;&#21464;&#24471;&#23454;&#29992;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#30828;&#20214;&#35268;&#26684;&#65292;&#30740;&#31350;&#20102;&#20256;&#24863;&#22120;&#22312;&#36229;&#20302;&#21151;&#32791;&#27169;&#24335;&#19979;&#27979;&#37327;&#30340;&#25968;&#25454;&#65292;&#26816;&#27979;&#20102;&#25856;&#30331;&#19981;&#21516;&#36335;&#32447;&#36807;&#31243;&#20013;&#25968;&#25454;&#20013;&#30340;&#27169;&#24335;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#36335;&#32447;&#32858;&#31867;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2211.02680v2 Announce Type: replace-cross  Abstract: One of the challenges for climbing gyms is to find out popular routes for the climbers to improve their services and optimally use their infrastructure. This problem must be addressed preserving both the privacy and convenience of the climbers and the costs of the gyms. To this aim, a hardware prototype is developed to collect data using accelerometer sensors attached to a piece of climbing equipment mounted on the wall, called quickdraw, that connects the climbing rope to the bolt anchors. The corresponding sensors are configured to be energy-efficient, hence becoming practical in terms of expenses and time consumption for replacement when used in large quantities in a climbing gym. This paper describes hardware specifications, studies data measured by the sensors in ultra-low power mode, detect patterns in data during climbing different routes, and develops an unsupervised approach for route clustering.
&lt;/p&gt;</description></item><item><title>&#20132;&#20114;&#24335;&#38382;&#31572;&#31995;&#32479;&#26159;&#38382;&#31572;&#21644;&#23545;&#35805;&#31995;&#32479;&#30340;&#32467;&#21512;&#65292;&#29992;&#25143;&#21487;&#20197;&#29992;&#33258;&#28982;&#35821;&#35328;&#25552;&#38382;&#24182;&#19982;&#31995;&#32479;&#21160;&#24577;&#20132;&#20114;&#65292;&#33719;&#24471;&#26356;&#31934;&#30830;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2209.01621</link><description>&lt;p&gt;
&#20132;&#20114;&#24335;&#38382;&#31572;&#31995;&#32479;&#65306;&#25991;&#29486;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Interactive Question Answering Systems: Literature Review
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2209.01621
&lt;/p&gt;
&lt;p&gt;
&#20132;&#20114;&#24335;&#38382;&#31572;&#31995;&#32479;&#26159;&#38382;&#31572;&#21644;&#23545;&#35805;&#31995;&#32479;&#30340;&#32467;&#21512;&#65292;&#29992;&#25143;&#21487;&#20197;&#29992;&#33258;&#28982;&#35821;&#35328;&#25552;&#38382;&#24182;&#19982;&#31995;&#32479;&#21160;&#24577;&#20132;&#20114;&#65292;&#33719;&#24471;&#26356;&#31934;&#30830;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2209.01621v2 &#20844;&#21578;&#31867;&#22411;: &#26367;&#25442;-&#36328;  &#25688;&#35201;: &#38382;&#31572;&#31995;&#32479;&#34987;&#20844;&#35748;&#20026;&#22312;&#32593;&#32476;&#19978;&#23547;&#27714;&#20449;&#24687;&#30340;&#27969;&#34892;&#19988;&#26377;&#25928;&#30340;&#25163;&#27573;&#12290;&#22312;&#36825;&#31181;&#31995;&#32479;&#20013;&#65292;&#20449;&#24687;&#23547;&#25214;&#32773;&#21487;&#20197;&#36890;&#36807;&#29992;&#33258;&#28982;&#35821;&#35328;&#25552;&#20986;&#38382;&#39064;&#26469;&#33719;&#24471;&#31616;&#27905;&#30340;&#22238;&#31572;&#12290;&#20132;&#20114;&#24335;&#38382;&#31572;&#26159;&#26368;&#36817;&#25552;&#20986;&#30340;&#24182;&#36234;&#26469;&#36234;&#27969;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20301;&#20110;&#38382;&#31572;&#21644;&#23545;&#35805;&#31995;&#32479;&#30340;&#20132;&#38598;&#22788;&#12290;&#19968;&#26041;&#38754;&#65292;&#29992;&#25143;&#21487;&#20197;&#29992;&#26222;&#36890;&#35821;&#35328;&#25552;&#38382;&#24182;&#25214;&#21040;&#22905;&#38382;&#39064;&#30340;&#23454;&#38469;&#22238;&#31572;&#65307;&#21478;&#19968;&#26041;&#38754;&#65292;&#22914;&#26524;&#21021;&#22987;&#35831;&#27714;&#20013;&#23384;&#22312;&#22810;&#20010;&#21487;&#33021;&#30340;&#22238;&#22797;&#12289;&#24456;&#23569;&#25110;&#27169;&#26865;&#20004;&#21487;&#65292;&#31995;&#32479;&#21487;&#20197;&#23558;&#38382;&#31572;&#20250;&#35805;&#24310;&#38271;&#20026;&#23545;&#35805;&#12290;&#36890;&#36807;&#20801;&#35768;&#29992;&#25143;&#25552;&#20986;&#26356;&#22810;&#38382;&#39064;&#65292;&#20132;&#20114;&#24335;&#38382;&#31572;&#20351;&#29992;&#25143;&#33021;&#22815;&#21160;&#24577;&#22320;&#19982;&#31995;&#32479;&#20132;&#20114;&#24182;&#33719;&#24471;&#26356;&#31934;&#30830;&#30340;&#32467;&#26524;&#12290;&#26412;&#32508;&#36848;&#25552;&#20379;&#20102;&#20132;&#20114;&#24335;&#38382;&#31572;&#31995;&#32479;&#30340;&#35814;&#32454;&#27010;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2209.01621v2 Announce Type: replace-cross  Abstract: Question answering systems are recognized as popular and frequently effective means of information seeking on the web. In such systems, information seekers can receive a concise response to their query by presenting their questions in natural language. Interactive question answering is a recently proposed and increasingly popular solution that resides at the intersection of question answering and dialogue systems. On the one hand, the user can ask questions in normal language and locate the actual response to her inquiry; on the other hand, the system can prolong the question-answering session into a dialogue if there are multiple probable replies, very few, or ambiguities in the initial request. By permitting the user to ask more questions, interactive question answering enables users to dynamically interact with the system and receive more precise results. This survey offers a detailed overview of the interactive question-ans
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#35752;&#20102;&#35821;&#35328;&#23398;&#20064;&#20013;&#65292;&#36890;&#36807;&#36328;&#24773;&#22659;&#35270;&#21548;&#23398;&#20064;&#65292;&#38899;&#32032;&#12289;&#38899;&#33410;&#21644;&#21333;&#35789;&#33021;&#21542;&#20316;&#20026;&#21103;&#20135;&#21697;&#20986;&#29616;&#65292;&#24182;&#25903;&#25345;&#19981;&#21516;&#24418;&#24335;&#34920;&#24449;&#38388;&#30340;&#36716;&#25442;&#12290;</title><link>https://arxiv.org/abs/2109.14200</link><description>&lt;p&gt;
&#25163;&#26426;&#12289;&#38899;&#33410;&#21644;&#21333;&#35789;&#33021;&#21542;&#20316;&#20026;&#36328;&#24773;&#22659;&#35270;&#21548;&#23398;&#20064;&#30340;&#21103;&#20135;&#21697;&#32780;&#20986;&#29616;&#65311;-- &#19968;&#39033;&#35745;&#31639;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Can phones, syllables, and words emerge as side-products of cross-situational audiovisual learning? -- A computational investigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2109.14200
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#35752;&#20102;&#35821;&#35328;&#23398;&#20064;&#20013;&#65292;&#36890;&#36807;&#36328;&#24773;&#22659;&#35270;&#21548;&#23398;&#20064;&#65292;&#38899;&#32032;&#12289;&#38899;&#33410;&#21644;&#21333;&#35789;&#33021;&#21542;&#20316;&#20026;&#21103;&#20135;&#21697;&#20986;&#29616;&#65292;&#24182;&#25903;&#25345;&#19981;&#21516;&#24418;&#24335;&#34920;&#24449;&#38388;&#30340;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#21313;&#24180;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#35821;&#35328;&#23398;&#20064;&#23156;&#20799;&#22914;&#20309;&#23398;&#20250;&#21306;&#20998;&#35821;&#38899;&#65292;&#20998;&#21106;&#21333;&#35789;&#65292;&#20197;&#21450;&#23558;&#21333;&#35789;&#19982;&#20854;&#21547;&#20041;&#20851;&#32852;&#36215;&#26469;&#12290;&#23613;&#31649;&#36825;&#20123;&#33021;&#21147;&#30340;&#36880;&#28176;&#21457;&#23637;&#26159;&#27595;&#24248;&#32622;&#30097;&#30340;&#65292;&#20294;&#36825;&#20123;&#25216;&#33021;&#30340;&#30830;&#20999;&#24615;&#36136;&#21644;&#28508;&#22312;&#30340;&#24515;&#29702;&#34920;&#24449;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#35745;&#31639;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#35821;&#38899;&#21644;&#21516;&#26102;&#20855;&#26377;&#25351;&#31216;&#19981;&#26126;&#30830;&#30340;&#35270;&#35273;&#36755;&#20837;&#20043;&#38388;&#30340;&#32479;&#35745;&#23398;&#20064;&#65292;&#21487;&#20197;&#23454;&#29616;&#23545;&#22522;&#26412;&#35821;&#38899;&#30340;&#29702;&#35299;&#12290;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#22312;&#27809;&#26377;&#35832;&#22914;&#35821;&#35328;&#21333;&#20301;&#30340;&#34920;&#24449;&#65292;&#20197;&#21450;&#27809;&#26377;&#19987;&#38376;&#38024;&#23545;&#36825;&#20123;&#21333;&#20301;&#30340;&#23398;&#20064;&#26426;&#21046;&#30340;&#24773;&#20917;&#19979;&#36816;&#34892;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#65292;&#31867;&#20284;&#38899;&#32032;&#12289;&#38899;&#33410;&#21644;&#21333;&#35789;&#30340;&#35821;&#35328;&#21333;&#20301;&#30340;&#30693;&#35782;&#23454;&#38469;&#19978;&#33021;&#22815;&#20316;&#20026;&#28508;&#22312;&#34920;&#24449;&#20986;&#29616;&#65292;&#25903;&#25345;&#35821;&#38899;&#19982;&#20854;&#20182;&#24418;&#24335;&#34920;&#24449;&#20043;&#38388;&#30340;&#36716;&#25442;&#65292;&#32780;&#19981;&#38656;&#35201;&#19987;&#38376;&#30340;&#23398;&#20064;&#26426;&#21046;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2109.14200v2 Announce Type: replace-cross  Abstract: Decades of research has studied how language learning infants learn to discriminate speech sounds, segment words, and associate words with their meanings. While gradual development of such capabilities is unquestionable, the exact nature of these skills and the underlying mental representations yet remains unclear. In parallel, computational studies have shown that basic comprehension of speech can be achieved by statistical learning between speech and concurrent referentially ambiguous visual input. These models can operate without prior linguistic knowledge such as representations of linguistic units, and without learning mechanisms specifically targeted at such units. This has raised the question of to what extent knowledge of linguistic units, such as phone(me)s, syllables, and words, could actually emerge as latent representations supporting the translation between speech and representations in other modalities, and withou
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35843;&#26597;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32842;&#22825;&#26426;&#22120;&#20154;&#36827;&#34892;&#24515;&#29702;&#20581;&#24247;&#25903;&#25345;&#30340;&#20154;&#30340;&#32463;&#21382;&#65292;&#20998;&#26512;&#20102;&#29992;&#25143;&#22914;&#20309;&#20026;&#32842;&#22825;&#26426;&#22120;&#20154;&#21019;&#24314;&#29420;&#29305;&#30340;&#25903;&#25345;&#35282;&#33394;&#65292;&#24182;&#20171;&#32461;&#20102;&#22312;&#24515;&#29702;&#20581;&#24247;&#32972;&#26223;&#19979;&#23558;&#20154;&#24037;&#26234;&#33021;&#19982;&#27835;&#30103;&#20215;&#20540;&#35266;&#30456;&#21305;&#37197;&#30340;&#27010;&#24565;&#12290;&#30740;&#31350;&#25552;&#20379;&#20102;&#35774;&#35745;&#24072;&#22788;&#29702;&#20262;&#29702;&#38382;&#39064;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2401.14362</link><description>&lt;p&gt;
&#25171;&#23383;&#30103;&#27861;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#24515;&#29702;&#20581;&#24247;&#25903;&#25345;&#20013;&#30340;&#24212;&#29992;&#32463;&#39564;
&lt;/p&gt;
&lt;p&gt;
The Typing Cure: Experiences with Large Language Model Chatbots for Mental Health Support. (arXiv:2401.14362v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14362
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35843;&#26597;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32842;&#22825;&#26426;&#22120;&#20154;&#36827;&#34892;&#24515;&#29702;&#20581;&#24247;&#25903;&#25345;&#30340;&#20154;&#30340;&#32463;&#21382;&#65292;&#20998;&#26512;&#20102;&#29992;&#25143;&#22914;&#20309;&#20026;&#32842;&#22825;&#26426;&#22120;&#20154;&#21019;&#24314;&#29420;&#29305;&#30340;&#25903;&#25345;&#35282;&#33394;&#65292;&#24182;&#20171;&#32461;&#20102;&#22312;&#24515;&#29702;&#20581;&#24247;&#32972;&#26223;&#19979;&#23558;&#20154;&#24037;&#26234;&#33021;&#19982;&#27835;&#30103;&#20215;&#20540;&#35266;&#30456;&#21305;&#37197;&#30340;&#27010;&#24565;&#12290;&#30740;&#31350;&#25552;&#20379;&#20102;&#35774;&#35745;&#24072;&#22788;&#29702;&#20262;&#29702;&#38382;&#39064;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#32842;&#22825;&#26426;&#22120;&#20154;&#20316;&#20026;&#24515;&#29702;&#20581;&#24247;&#25903;&#25345;&#24037;&#20855;&#65292;&#20294;&#26377;&#35777;&#25454;&#34920;&#26126;&#36890;&#29992;&#22411;LLM&#32842;&#22825;&#26426;&#22120;&#20154;&#20063;&#23384;&#22312;&#19968;&#23450;&#39118;&#38505;&#65292;&#22914;&#26524;&#35774;&#35745;&#19981;&#36127;&#36131;&#20219;&#21487;&#33021;&#20250;&#21361;&#21450;&#29992;&#25143;&#30340;&#31119;&#31049;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#20351;&#29992;LLM&#32842;&#22825;&#26426;&#22120;&#20154;&#36827;&#34892;&#24515;&#29702;&#20581;&#24247;&#25903;&#25345;&#30340;&#20154;&#30340;&#30495;&#23454;&#32463;&#21382;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#26469;&#33258;&#19981;&#21516;&#22269;&#23478;&#32972;&#26223;&#30340;21&#20010;&#20010;&#20154;&#36827;&#34892;&#35775;&#35848;&#65292;&#20998;&#26512;&#20102;&#29992;&#25143;&#22914;&#20309;&#20026;&#20182;&#20204;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#21019;&#24314;&#29420;&#29305;&#30340;&#25903;&#25345;&#35282;&#33394;&#65292;&#22635;&#34917;&#26085;&#24120;&#25252;&#29702;&#30340;&#31354;&#30333;&#65292;&#24182;&#22312;&#23547;&#27714;&#26469;&#33258;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#25903;&#25345;&#26102;&#22914;&#20309;&#23548;&#33322;&#30456;&#20851;&#30340;&#25991;&#21270;&#38480;&#21046;&#12290;&#25105;&#20204;&#23558;&#20998;&#26512;&#22522;&#20110;&#24515;&#29702;&#27835;&#30103;&#25991;&#29486;&#20013;&#26377;&#25928;&#25903;&#25345;&#30340;&#27010;&#24565;&#65292;&#24182;&#24341;&#20837;&#20102;AI&#19982;&#24515;&#29702;&#20581;&#24247;&#32972;&#26223;&#19979;&#30340;&#27835;&#30103;&#20215;&#20540;&#35266;&#23545;&#20854;&#36827;&#34892;&#21305;&#37197;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#35774;&#35745;&#24072;&#22914;&#20309;&#22788;&#29702;&#20262;&#29702;&#38382;&#39064;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
People experiencing severe distress increasingly use Large Language Model (LLM) chatbots as mental health support tools. Discussions on social media have described how engagements were lifesaving for some, but evidence suggests that general-purpose LLM chatbots also have notable risks that could endanger the welfare of users if not designed responsibly. In this study, we investigate the lived experiences of people who have used LLM chatbots for mental health support. We build on interviews with 21 individuals from globally diverse backgrounds to analyze how users create unique support roles for their chatbots, fill in gaps in everyday care, and navigate associated cultural limitations when seeking support from chatbots. We ground our analysis in psychotherapy literature around effective support, and introduce the concept of therapeutic alignment, or aligning AI with therapeutic values for mental health contexts. Our study offers recommendations for how designers can approach the ethica
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;Q&amp;A&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25366;&#25496;&#22270;&#20687;&#20013;&#30340;&#38382;&#39064;-&#22238;&#31572;&#23545;&#26469;&#21457;&#29616;&#20016;&#23500;&#30340;&#35270;&#35273;&#32447;&#32034;&#65292;&#20197;&#24110;&#21161;AI&#27169;&#22411;&#26356;&#22909;&#22320;&#29702;&#35299;&#22797;&#26434;&#35270;&#35273;&#38382;&#39064;&#65292;&#25552;&#39640;&#36328;&#27169;&#24577;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.10712</link><description>&lt;p&gt;
Q&amp;A&#25552;&#31034;&#65306;&#36890;&#36807;&#25366;&#25496;&#38382;&#39064;-&#22238;&#31572;&#25552;&#31034;&#26469;&#21457;&#29616;&#20016;&#23500;&#30340;&#35270;&#35273;&#32447;&#32034;&#65292;&#20197;&#28385;&#36275;&#23545;&#22810;&#26679;&#19990;&#30028;&#30693;&#35782;&#30340;&#35270;&#35273;&#38382;&#31572;&#30340;&#38656;&#27714;
&lt;/p&gt;
&lt;p&gt;
Q&amp;A Prompts: Discovering Rich Visual Clues through Mining Question-Answer Prompts for VQA requiring Diverse World Knowledge. (arXiv:2401.10712v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;Q&amp;A&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25366;&#25496;&#22270;&#20687;&#20013;&#30340;&#38382;&#39064;-&#22238;&#31572;&#23545;&#26469;&#21457;&#29616;&#20016;&#23500;&#30340;&#35270;&#35273;&#32447;&#32034;&#65292;&#20197;&#24110;&#21161;AI&#27169;&#22411;&#26356;&#22909;&#22320;&#29702;&#35299;&#22797;&#26434;&#35270;&#35273;&#38382;&#39064;&#65292;&#25552;&#39640;&#36328;&#27169;&#24577;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31361;&#30772;&#65292;&#22238;&#31572;&#38656;&#35201;&#39640;&#32423;&#25512;&#29702;&#33021;&#21147;&#21644;&#19990;&#30028;&#30693;&#35782;&#30340;&#22797;&#26434;&#35270;&#35273;&#38382;&#39064;&#27604;&#20197;&#24448;&#20219;&#20309;&#26102;&#20505;&#37117;&#26356;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20026;AI&#27169;&#22411;&#37197;&#22791;&#24378;&#22823;&#30340;&#36328;&#27169;&#24577;&#25512;&#29702;&#33021;&#21147;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#20154;&#31867;&#30340;&#35748;&#30693;&#26041;&#26696;&#23578;&#26410;&#31995;&#32479;&#22320;&#34987;&#29702;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30456;&#20449;&#65292;&#22914;&#26524;&#25105;&#20204;&#33021;&#23613;&#21487;&#33021;&#25910;&#38598;&#32473;&#23450;&#22270;&#20687;&#20013;&#30340;&#35270;&#35273;&#32447;&#32034;&#65292;&#25105;&#20204;&#23558;&#33021;&#26356;&#20934;&#30830;&#22320;&#35782;&#21035;&#22270;&#20687;&#65292;&#26356;&#22909;&#22320;&#29702;&#35299;&#38382;&#39064;&#65292;&#26356;&#23481;&#26131;&#22238;&#24518;&#30456;&#20851;&#30693;&#35782;&#65292;&#24182;&#26368;&#32456;&#25512;&#29702;&#20986;&#31572;&#26696;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#22270;&#20687;&#20013;&#25366;&#25496;&#38382;&#39064;-&#22238;&#31572;&#23545;&#26469;&#21457;&#29616;&#36825;&#20123;&#20016;&#23500;&#30340;&#35270;&#35273;&#32447;&#32034;&#65292;&#24182;&#23558;&#23427;&#20204;&#20316;&#20026;&#25552;&#31034;&#21457;&#36865;&#21040;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#12290;&#25105;&#20204;&#31216;&#20043;&#20026;Q&amp;A&#25552;&#31034;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#35757;&#32451;&#38598;&#20013;&#30340;&#22270;&#20687;-&#31572;&#26696;&#23545;&#21644;&#30456;&#24212;&#30340;&#38382;&#39064;&#20316;&#20026;&#36755;&#20837;&#21644;&#36755;&#20986;&#26469;&#35757;&#32451;&#19968;&#20010;&#35270;&#35273;&#38382;&#39064;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the breakthrough of multi-modal large language models, answering complex visual questions that demand advanced reasoning abilities and world knowledge has become a much more important testbed for developing AI models than ever. However, equipping AI models with robust cross-modality reasoning ability remains challenging since the cognition scheme of humans has not been understood systematically. In this paper, we believe that if we can collect visual clues in the given image as much as possible, we will recognize the image more accurately, understand the question better, recall relevant knowledge more easily, and finally reason out the answer. We discover these rich visual clues by mining question-answer pairs in images and sending them into multi-modal large language models as prompts. We call the proposed method Q&amp;A Prompts. Specifically, we first use the image-answer pairs and the corresponding questions in the training set as inputs and outputs to train a visual question gener
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#24863;&#30693;&#25439;&#22833;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#26080;&#20998;&#31867;&#22120;&#25351;&#23548;&#23454;&#29616;&#20102;&#29983;&#25104;&#26356;&#30495;&#23454;&#26679;&#26412;&#30340;&#30446;&#30340;&#12290;</title><link>http://arxiv.org/abs/2401.00110</link><description>&lt;p&gt;
&#20351;&#29992;&#24863;&#30693;&#25439;&#22833;&#30340;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Diffusion Model with Perceptual Loss. (arXiv:2401.00110v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00110
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#24863;&#30693;&#25439;&#22833;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#26080;&#20998;&#31867;&#22120;&#25351;&#23548;&#23454;&#29616;&#20102;&#29983;&#25104;&#26356;&#30495;&#23454;&#26679;&#26412;&#30340;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22343;&#26041;&#35823;&#24046;&#25439;&#22833;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#20542;&#21521;&#20110;&#29983;&#25104;&#19981;&#30495;&#23454;&#30340;&#26679;&#26412;&#12290;&#30446;&#21069;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#20381;&#38752;&#26080;&#20998;&#31867;&#22120;&#25351;&#23548;&#26469;&#25913;&#21892;&#26679;&#26412;&#36136;&#37327;&#65292;&#28982;&#32780;&#20854;&#24778;&#20154;&#30340;&#25928;&#26524;&#23578;&#26410;&#23436;&#20840;&#29702;&#35299;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26080;&#20998;&#31867;&#22120;&#25351;&#23548;&#30340;&#26377;&#25928;&#24615;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#28304;&#33258;&#20854;&#20316;&#20026;&#19968;&#31181;&#38544;&#24335;&#24863;&#30693;&#25351;&#23548;&#30340;&#24418;&#24335;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21487;&#20197;&#30452;&#25509;&#22312;&#25193;&#25955;&#35757;&#32451;&#20013;&#21152;&#20837;&#24863;&#30693;&#25439;&#22833;&#26469;&#25552;&#39640;&#26679;&#26412;&#36136;&#37327;&#12290;&#30001;&#20110;&#25193;&#25955;&#35757;&#32451;&#20013;&#20351;&#29992;&#30340;&#20998;&#25968;&#21305;&#37197;&#30446;&#26631;&#19982;&#26080;&#30417;&#30563;&#35757;&#32451;&#24863;&#30693;&#32593;&#32476;&#26102;&#20351;&#29992;&#30340;&#21435;&#22122;&#33258;&#21160;&#32534;&#30721;&#22120;&#30446;&#26631;&#38750;&#24120;&#30456;&#20284;&#65292;&#22240;&#27492;&#25193;&#25955;&#27169;&#22411;&#26412;&#36523;&#23601;&#26159;&#19968;&#20010;&#24863;&#30693;&#32593;&#32476;&#65292;&#24182;&#21487;&#20197;&#29992;&#20110;&#29983;&#25104;&#26377;&#24847;&#20041;&#30340;&#24863;&#30693;&#25439;&#22833;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#24863;&#30693;&#30446;&#26631;&#65292;&#20854;&#32467;&#26524;&#26159;&#25193;&#25955;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#26356;&#30495;&#23454;&#30340;&#26679;&#26412;&#12290;&#23545;&#20110;&#26465;&#20214;&#29983;&#25104;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20165;&#25913;&#21892;&#26679;&#26412;&#36136;&#37327;&#65292;&#32780;&#19981;&#19982;&#26465;&#20214;&#32465;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models trained with mean squared error loss tend to generate unrealistic samples. Current state-of-the-art models rely on classifier-free guidance to improve sample quality, yet its surprising effectiveness is not fully understood. In this paper, We show that the effectiveness of classifier-free guidance partly originates from it being a form of implicit perceptual guidance. As a result, we can directly incorporate perceptual loss in diffusion training to improve sample quality. Since the score matching objective used in diffusion training strongly resembles the denoising autoencoder objective used in unsupervised training of perceptual networks, the diffusion model itself is a perceptual network and can be used to generate meaningful perceptual loss. We propose a novel self-perceptual objective that results in diffusion models capable of generating more realistic samples. For conditional generation, our method only improves sample quality without entanglement with the condit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#22478;&#24066;&#29615;&#22659;&#19979;&#30340;&#33258;&#20027;&#22810;&#26234;&#33021;&#20307;&#20986;&#31199;&#36710;&#36335;&#24452;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36817;&#20284;&#28378;&#21160;&#20026;&#22522;&#30784;&#30340;&#20004;&#38454;&#27573;&#31639;&#27861;&#26469;&#20943;&#23569;&#35745;&#31639;&#37327;&#12290;</title><link>http://arxiv.org/abs/2311.01534</link><description>&lt;p&gt;
&#22823;&#22411;&#22320;&#22270;&#19978;&#30340;&#25353;&#38656;&#22478;&#24066;&#20986;&#34892;&#38382;&#39064;&#30340;&#36817;&#20284;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;&#25193;&#23637;&#29256;&#65289;
&lt;/p&gt;
&lt;p&gt;
Approximate Multiagent Reinforcement Learning for On-Demand Urban Mobility Problem on a Large Map (extended version). (arXiv:2311.01534v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01534
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#22478;&#24066;&#29615;&#22659;&#19979;&#30340;&#33258;&#20027;&#22810;&#26234;&#33021;&#20307;&#20986;&#31199;&#36710;&#36335;&#24452;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36817;&#20284;&#28378;&#21160;&#20026;&#22522;&#30784;&#30340;&#20004;&#38454;&#27573;&#31639;&#27861;&#26469;&#20943;&#23569;&#35745;&#31639;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#22823;&#22411;&#22478;&#24066;&#29615;&#22659;&#19979;&#30340;&#33258;&#20027;&#22810;&#26234;&#33021;&#20307;&#20986;&#31199;&#36710;&#36335;&#24452;&#38382;&#39064;&#65292;&#26410;&#26469;&#20056;&#36710;&#35831;&#27714;&#30340;&#20301;&#32622;&#21644;&#25968;&#37327;&#20107;&#20808;&#26410;&#30693;&#65292;&#20294;&#36981;&#24490;&#20272;&#35745;&#30340;&#32463;&#39564;&#20998;&#24067;&#12290;&#26368;&#36817;&#30340;&#29702;&#35770;&#34920;&#26126;&#65292;&#22914;&#26524;&#22522;&#30784;&#31574;&#30053;&#26159;&#31283;&#23450;&#30340;&#65292;&#37027;&#20040;&#22522;&#20110;&#28378;&#21160;&#30340;&#31639;&#27861;&#19982;&#36825;&#26679;&#30340;&#22522;&#30784;&#31574;&#30053;&#20135;&#29983;&#25509;&#36817;&#26368;&#20248;&#30340;&#31283;&#23450;&#31574;&#30053;&#12290;&#23613;&#31649;&#22522;&#20110;&#28378;&#21160;&#30340;&#26041;&#27861;&#38750;&#24120;&#36866;&#21512;&#23398;&#20064;&#20855;&#26377;&#23545;&#26410;&#26469;&#38656;&#27714;&#32771;&#34385;&#30340;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#31574;&#30053;&#65292;&#20294;&#23558;&#36825;&#20123;&#26041;&#27861;&#24212;&#29992;&#20110;&#22823;&#22411;&#22478;&#24066;&#29615;&#22659;&#21487;&#33021;&#35745;&#31639;&#19978;&#24456;&#26114;&#36149;&#12290;&#22823;&#22411;&#29615;&#22659;&#24448;&#24448;&#26377;&#22823;&#37327;&#35831;&#27714;&#65292;&#22240;&#27492;&#38656;&#35201;&#22823;&#22411;&#30340;&#20986;&#31199;&#36710;&#38431;&#20445;&#35777;&#31283;&#23450;&#24615;&#12290;&#26412;&#25991;&#26088;&#22312;&#35299;&#20915;&#22810;&#26234;&#33021;&#20307;&#65288;&#36880;&#19968;&#65289;&#28378;&#21160;&#30340;&#35745;&#31639;&#29942;&#39048;&#38382;&#39064;&#65292;&#20854;&#20013;&#35745;&#31639;&#22797;&#26434;&#24615;&#38543;&#20195;&#29702;&#25968;&#37327;&#32447;&#24615;&#22686;&#38271;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36817;&#20284;&#36880;&#19968;&#28378;&#21160;&#20026;&#22522;&#30784;&#30340;&#20004;&#38454;&#27573;&#31639;&#27861;&#65292;&#20943;&#23569;&#35745;&#31639;&#37327;
&lt;/p&gt;
&lt;p&gt;
In this paper, we focus on the autonomous multiagent taxi routing problem for a large urban environment where the location and number of future ride requests are unknown a-priori, but follow an estimated empirical distribution. Recent theory has shown that if a base policy is stable then a rollout-based algorithm with such a base policy produces a near-optimal stable policy. Although, rollout-based approaches are well-suited for learning cooperative multiagent policies with considerations for future demand, applying such methods to a large urban environment can be computationally expensive. Large environments tend to have a large volume of requests, and hence require a large fleet of taxis to guarantee stability. In this paper, we aim to address the computational bottleneck of multiagent (one-at-a-time) rollout, where the computational complexity grows linearly in the number of agents. We propose an approximate one-at-a-time rollout-based two-phase algorithm that reduces the computatio
&lt;/p&gt;</description></item><item><title>VeLO&#26159;&#36804;&#20170;&#20026;&#27490;&#35268;&#27169;&#26368;&#22823;&#30340;&#35757;&#32451;&#36890;&#29992;&#8220;&#22522;&#30784;&#8221;&#20248;&#21270;&#22120;&#30340;&#23581;&#35797;&#65292;&#20294;&#25105;&#20204;&#30340;&#35780;&#20272;&#21457;&#29616;&#23427;&#38656;&#35201;&#38382;&#39064;&#29305;&#23450;&#30340;&#35843;&#20248;&#65292;&#24182;&#19981;&#19968;&#23450;&#20248;&#20110;&#31454;&#20105;&#23545;&#25163;&#30340;&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#21644;&#35757;&#32451;&#35823;&#24046;&#38477;&#20302;&#36895;&#24230;&#65292;&#36825;&#23545;&#20110;VeLO&#30340;&#36890;&#29992;&#24615;&#21644;&#22521;&#35757;&#25237;&#36164;&#30340;&#20215;&#20540;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;</title><link>http://arxiv.org/abs/2310.18191</link><description>&lt;p&gt;
&#32553;&#25918;&#23398;&#20064;&#20248;&#21270;&#22120;&#26159;&#21542;&#20540;&#24471;&#65311;&#35780;&#20272; VeLO &#30340; 4000 &#20010; TPU &#26376;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Is Scaling Learned Optimizers Worth It? Evaluating The Value of VeLO's 4000 TPU Months. (arXiv:2310.18191v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18191
&lt;/p&gt;
&lt;p&gt;
VeLO&#26159;&#36804;&#20170;&#20026;&#27490;&#35268;&#27169;&#26368;&#22823;&#30340;&#35757;&#32451;&#36890;&#29992;&#8220;&#22522;&#30784;&#8221;&#20248;&#21270;&#22120;&#30340;&#23581;&#35797;&#65292;&#20294;&#25105;&#20204;&#30340;&#35780;&#20272;&#21457;&#29616;&#23427;&#38656;&#35201;&#38382;&#39064;&#29305;&#23450;&#30340;&#35843;&#20248;&#65292;&#24182;&#19981;&#19968;&#23450;&#20248;&#20110;&#31454;&#20105;&#23545;&#25163;&#30340;&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#21644;&#35757;&#32451;&#35823;&#24046;&#38477;&#20302;&#36895;&#24230;&#65292;&#36825;&#23545;&#20110;VeLO&#30340;&#36890;&#29992;&#24615;&#21644;&#22521;&#35757;&#25237;&#36164;&#30340;&#20215;&#20540;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20998;&#26512;&#20102; VeLO&#65288;&#19975;&#33021;&#23398;&#20064;&#20248;&#21270;&#22120;&#65289;&#65292;&#36825;&#26159;&#36804;&#20170;&#20026;&#27490;&#35268;&#27169;&#26368;&#22823;&#30340;&#35757;&#32451;&#36890;&#29992;&#8220;&#22522;&#30784;&#8221;&#20248;&#21270;&#22120;&#30340;&#23581;&#35797;&#12290;VeLO &#20351;&#29992;&#36229;&#36807; 4000 &#20010; TPU &#26376;&#30340;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#36827;&#34892;&#35757;&#32451;&#65292;&#30446;&#26631;&#26159;&#20135;&#29983;&#19968;&#20010;&#33021;&#22815;&#25512;&#24191;&#21040;&#26032;&#38382;&#39064;&#24182;&#19988;&#19981;&#38656;&#35201;&#36229;&#21442;&#25968;&#35843;&#25972;&#65292;&#24182;&#19988;&#36229;&#36807; Adam &#31561;&#34892;&#19994;&#26631;&#20934;&#30340;&#20248;&#21270;&#22120;&#12290;&#25105;&#20204;&#23545; MLCommons &#20248;&#21270;&#22120;&#22522;&#20934;&#22871;&#20214;&#29420;&#31435;&#35780;&#20272;&#20102; VeLO&#12290;&#25105;&#20204;&#21457;&#29616;&#19982;&#21021;&#27493;&#22768;&#26126;&#30456;&#21453;&#65306;&#65288;1&#65289;VeLO&#26377;&#19968;&#20010;&#20851;&#38190;&#30340;&#36229;&#21442;&#25968;&#38656;&#35201;&#26681;&#25454;&#20855;&#20307;&#38382;&#39064;&#36827;&#34892;&#35843;&#25972;&#65292;&#65288;2&#65289;VeLO&#22312;&#25214;&#21040;&#30340;&#35299;&#30340;&#36136;&#37327;&#19978;&#19981;&#19968;&#23450;&#20248;&#20110;&#31454;&#20105;&#23545;&#25163;&#65292;&#65288;3&#65289;VeLO&#22312;&#38477;&#20302;&#35757;&#32451;&#35823;&#24046;&#19978;&#24182;&#19981;&#27604;&#31454;&#20105;&#20248;&#21270;&#22120;&#26356;&#24555;&#12290;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#23545; VeLO &#30340;&#36890;&#29992;&#24615;&#21644;&#22521;&#35757;&#25237;&#36164;&#30340;&#20215;&#20540;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;
&lt;/p&gt;
&lt;p&gt;
We analyze VeLO (versatile learned optimizer), the largest scale attempt to train a general purpose "foundational" optimizer to date. VeLO was trained on thousands of machine learning tasks using over 4000 TPU months with the goal of producing an optimizer capable of generalizing to new problems while being hyperparameter free, and outperforming industry standards such as Adam. We independently evaluate VeLO on the MLCommons optimizer benchmark suite. We find that, contrary to initial claims: (1) VeLO has a critical hyperparameter that needs problem-specific tuning, (2) VeLO does not necessarily outperform competitors in quality of solution found, and (3) VeLO is not faster than competing optimizers at reducing the training loss. These observations call into question VeLO's generality and the value of the investment in training it.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GenTKG&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#19978;&#36827;&#34892;&#39044;&#27979;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#32467;&#21512;&#22522;&#20110;&#26102;&#38388;&#36923;&#36753;&#35268;&#21017;&#30340;&#26816;&#32034;&#31574;&#30053;&#21644;&#36731;&#37327;&#32423;&#30340;&#21442;&#25968;&#25928;&#29575;&#25351;&#23548;&#65292;&#20811;&#26381;&#20102;&#22797;&#26434;&#30340;&#26102;&#38388;&#22270;&#25968;&#25454;&#32467;&#26500;&#21644;&#24222;&#22823;&#30340;&#25968;&#25454;&#37327;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.07793</link><description>&lt;p&gt;
GenTKG: &#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
GenTKG: Generative Forecasting on Temporal Knowledge Graph. (arXiv:2310.07793v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07793
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GenTKG&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#19978;&#36827;&#34892;&#39044;&#27979;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#32467;&#21512;&#22522;&#20110;&#26102;&#38388;&#36923;&#36753;&#35268;&#21017;&#30340;&#26816;&#32034;&#31574;&#30053;&#21644;&#36731;&#37327;&#32423;&#30340;&#21442;&#25968;&#25928;&#29575;&#25351;&#23548;&#65292;&#20811;&#26381;&#20102;&#22797;&#26434;&#30340;&#26102;&#38388;&#22270;&#25968;&#25454;&#32467;&#26500;&#21644;&#24222;&#22823;&#30340;&#25968;&#25454;&#37327;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#24555;&#36895;&#21457;&#23637;&#24341;&#21457;&#20102;&#23545;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;(tKG)&#39046;&#22495;&#30340;&#20852;&#36259;&#65292;&#20854;&#20013;&#20256;&#32479;&#30340;&#22522;&#20110;&#23884;&#20837;&#21644;&#35268;&#21017;&#30340;&#27169;&#22411;&#21344;&#20027;&#23548;&#22320;&#20301;&#12290;&#30446;&#21069;&#20173;&#28982;&#23384;&#22312;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#39044;&#35757;&#32451;&#30340;LLM&#26159;&#21542;&#33021;&#22815;&#29702;&#35299;&#32467;&#26500;&#21270;&#30340;&#26102;&#38388;&#20851;&#31995;&#25968;&#25454;&#65292;&#24182;&#21462;&#20195;&#23427;&#20204;&#25104;&#20026;&#26102;&#38388;&#20851;&#31995;&#39044;&#27979;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23558;&#26102;&#38388;&#30693;&#35782;&#39044;&#27979;&#24341;&#20837;&#29983;&#25104;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#22312;&#22797;&#26434;&#30340;&#26102;&#38388;&#22270;&#25968;&#25454;&#32467;&#26500;&#21644;LLM&#21487;&#20197;&#22788;&#29702;&#30340;&#24207;&#21015;&#33258;&#28982;&#34920;&#36798;&#20043;&#38388;&#23384;&#22312;&#24040;&#22823;&#30340;&#40511;&#27807;&#65292;&#22312;tKG&#30340;&#24222;&#22823;&#25968;&#25454;&#37327;&#21644;&#24494;&#35843;LLM&#30340;&#24040;&#22823;&#35745;&#31639;&#25104;&#26412;&#20043;&#38388;&#20063;&#23384;&#22312;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#26694;&#26550;&#65292;&#31216;&#20026;GenTKG&#65292;&#23427;&#22312;tKG&#19978;&#25191;&#34892;&#29983;&#25104;&#24335;&#39044;&#27979;&#65292;&#32467;&#21512;&#20102;&#22522;&#20110;&#26102;&#38388;&#36923;&#36753;&#35268;&#21017;&#30340;&#26816;&#32034;&#31574;&#30053;&#21644;&#36731;&#37327;&#32423;&#30340;&#21442;&#25968;&#25928;&#29575;&#25351;&#23548;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;GenTKG&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid advancements in large language models (LLMs) have ignited interest in the temporal knowledge graph (tKG) domain, where conventional carefully designed embedding-based and rule-based models dominate. The question remains open of whether pre-trained LLMs can understand structured temporal relational data and replace them as the foundation model for temporal relational forecasting. Therefore, we bring temporal knowledge forecasting into the generative setting. However, challenges occur in the huge chasms between complex temporal graph data structure and sequential natural expressions LLMs can handle, and between the enormous data sizes of tKGs and heavy computation costs of finetuning LLMs. To address these challenges, we propose a novel retrieval augmented generation framework that performs generative forecasting on tKGs named GenTKG, which combines a temporal logical rule-based retrieval strategy and lightweight parameter-efficient instruction tuning. Extensive experiments hav
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22266;&#23450;&#21442;&#25968;&#21487;&#22788;&#29702;&#31639;&#27861;&#65292;&#29992;&#20110;&#35745;&#25968;&#20855;&#26377;&#30456;&#21516;&#39592;&#26550;&#30340;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#31867;&#12290;</title><link>http://arxiv.org/abs/2310.04218</link><description>&lt;p&gt;
&#19968;&#20010;&#21487;&#35745;&#25968;&#20855;&#26377;&#30456;&#21516;&#39592;&#26550;&#30340;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#31867;&#30340;&#22266;&#23450;&#21442;&#25968;&#21487;&#22788;&#29702;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Fixed-Parameter Tractable Algorithm for Counting Markov Equivalence Classes with the same Skeleton. (arXiv:2310.04218v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04218
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22266;&#23450;&#21442;&#25968;&#21487;&#22788;&#29702;&#31639;&#27861;&#65292;&#29992;&#20110;&#35745;&#25968;&#20855;&#26377;&#30456;&#21516;&#39592;&#26550;&#30340;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;&#20063;&#31216;&#20026;&#36125;&#21494;&#26031;&#32593;&#32476;&#65289;&#26159;&#32534;&#30721;&#38543;&#26426;&#21464;&#37327;&#20043;&#38388;&#26465;&#20214;&#20381;&#36182;&#20851;&#31995;&#30340;&#27969;&#34892;&#24037;&#20855;&#12290;&#22312;&#22240;&#26524;&#26377;&#21521;&#26080;&#29615;&#22270;&#20013;&#65292;&#38543;&#26426;&#21464;&#37327;&#34987;&#24314;&#27169;&#20026;&#26377;&#21521;&#22270;&#20013;&#30340;&#39030;&#28857;&#65292;&#24182;&#19988;&#35268;&#23450;&#27599;&#20010;&#38543;&#26426;&#21464;&#37327;&#22312;&#32473;&#23450;&#20854;&#29238;&#33410;&#28857;&#30340;&#24773;&#20917;&#19979;&#19982;&#20854;&#31062;&#20808;&#33410;&#28857;&#26080;&#20851;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#21516;&#19968;&#32452;&#38543;&#26426;&#21464;&#37327;&#19978;&#30340;&#20004;&#20010;&#19981;&#21516;&#30340;&#22240;&#26524;&#26377;&#21521;&#26080;&#29615;&#22270;&#21487;&#20197;&#20934;&#30830;&#32534;&#30721;&#30456;&#21516;&#30340;&#19968;&#32452;&#26465;&#20214;&#20381;&#36182;&#20851;&#31995;&#12290;&#36825;&#26679;&#30340;&#22240;&#26524;&#26377;&#21521;&#26080;&#29615;&#22270;&#34987;&#31216;&#20026;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#65292;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#30340;&#22240;&#26524;&#26377;&#21521;&#26080;&#29615;&#22270;&#30340;&#31561;&#20215;&#31867;&#34987;&#31216;&#20026;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#31867;&#65288;MEC&#65289;&#12290;&#22312;&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#65292;&#23545;&#20110;MEC&#24050;&#32463;&#21019;&#24314;&#20102;&#19968;&#20123;&#32654;&#20029;&#30340;&#32452;&#21512;&#29305;&#24449;&#65292;&#24182;&#19988;&#24050;&#30693;&#65292;&#29305;&#21035;&#26159;&#22312;&#21516;&#19968;MEC&#20013;&#30340;&#25152;&#26377;&#22240;&#26524;&#26377;&#21521;&#26080;&#29615;&#22270;&#24517;&#39035;&#20855;&#26377;&#30456;&#21516;&#30340;&#8220;&#39592;&#26550;&#8221;&#65288;&#24213;&#23618;&#26080;&#21521;&#22270;&#65289;&#21644;v-&#32467;&#26500;&#65288;&#24418;&#24335;&#20026;$a\rightarrow b \leftarrow c$&#30340;&#35825;&#23548;&#23376;&#22270;&#65289;&#12290;&#36825;&#20123;&#32452;&#21512;&#29305;&#24449;&#36824;&#25552;&#20986;&#20102;&#20960;&#20010;&#33258;&#28982;&#30340;&#31639;&#27861;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal DAGs (also known as Bayesian networks) are a popular tool for encoding conditional dependencies between random variables. In a causal DAG, the random variables are modeled as vertices in the DAG, and it is stipulated that every random variable is independent of its ancestors conditioned on its parents. It is possible, however, for two different causal DAGs on the same set of random variables to encode exactly the same set of conditional dependencies. Such causal DAGs are said to be Markov equivalent, and equivalence classes of Markov equivalent DAGs are known as Markov Equivalent Classes (MECs). Beautiful combinatorial characterizations of MECs have been developed in the past few decades, and it is known, in particular that all DAGs in the same MEC must have the same ''skeleton'' (underlying undirected graph) and v-structures (induced subgraph of the form $a\rightarrow b \leftarrow c$).  These combinatorial characterizations also suggest several natural algorithmic questions. On
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33033;&#20914;&#32047;&#31215;&#36716;&#21457;&#65288;SAF&#65289;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#35757;&#32451;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#12290;SAF&#19981;&#20165;&#21487;&#20197;&#20943;&#23569;&#21069;&#21521;&#36807;&#31243;&#20013;&#30340;&#25805;&#20316;&#27425;&#25968;&#65292;&#19982;Spike Representation&#21644;OTTT&#20445;&#25345;&#19968;&#33268;&#65292;&#32780;&#19988;&#21487;&#20197;&#35299;&#20915;SNNs&#35757;&#32451;&#20013;&#30340;&#38590;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.02772</link><description>&lt;p&gt;
&#29992;&#20110;&#26377;&#25928;&#35757;&#32451;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#33033;&#20914;&#32047;&#31215;&#36716;&#21457;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Spike Accumulation Forwarding for Effective Training of Spiking Neural Networks. (arXiv:2310.02772v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02772
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33033;&#20914;&#32047;&#31215;&#36716;&#21457;&#65288;SAF&#65289;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#35757;&#32451;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#12290;SAF&#19981;&#20165;&#21487;&#20197;&#20943;&#23569;&#21069;&#21521;&#36807;&#31243;&#20013;&#30340;&#25805;&#20316;&#27425;&#25968;&#65292;&#19982;Spike Representation&#21644;OTTT&#20445;&#25345;&#19968;&#33268;&#65292;&#32780;&#19988;&#21487;&#20197;&#35299;&#20915;SNNs&#35757;&#32451;&#20013;&#30340;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#35757;&#32451;&#33539;&#24335;&#65292;&#21363;&#33033;&#20914;&#32047;&#31215;&#36716;&#21457;&#65288;SAF&#65289;&#12290;&#24050;&#30693;SNNs&#20855;&#26377;&#39640;&#33021;&#25928;&#20294;&#38590;&#20197;&#35757;&#32451;&#30340;&#29305;&#28857;&#12290;&#35768;&#22810;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20854;&#20013;&#26102;&#38388;&#19978;&#30340;&#22312;&#32447;&#35757;&#32451;&#65288;OTTT&#65289;&#26159;&#19968;&#31181;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#25233;&#21046;&#20869;&#23384;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#22312;GPU&#19978;&#39640;&#25928;&#35745;&#31639;&#65292;OTTT&#38656;&#35201;&#36827;&#34892;&#33033;&#20914;&#24207;&#21015;&#25805;&#20316;&#21644;&#33033;&#20914;&#24207;&#21015;&#21152;&#26435;&#27714;&#21644;&#25805;&#20316;&#12290;&#27492;&#22806;&#65292;OTTT&#19982;Spike Representation&#65288;&#21478;&#19968;&#31181;&#35757;&#32451;&#26041;&#27861;&#65289;&#20043;&#38388;&#23384;&#22312;&#20851;&#32852;&#65292;&#20294;&#19982;Spike Representation&#30340;&#29702;&#35770;&#19968;&#33268;&#24615;&#23578;&#26410;&#24471;&#21040;&#35777;&#26126;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#21363;SAF&#21487;&#20197;&#22312;&#21069;&#21521;&#36807;&#31243;&#20013;&#20943;&#23569;&#19968;&#21322;&#30340;&#25805;&#20316;&#27425;&#25968;&#65292;&#24182;&#19988;&#21487;&#20197;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;SAF&#20998;&#21035;&#19982;Spike Representation&#21644;OTTT&#19968;&#33268;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30830;&#35748;&#20102;......
&lt;/p&gt;
&lt;p&gt;
In this article, we propose a new paradigm for training spiking neural networks (SNNs), spike accumulation forwarding (SAF). It is known that SNNs are energy-efficient but difficult to train. Consequently, many researchers have proposed various methods to solve this problem, among which online training through time (OTTT) is a method that allows inferring at each time step while suppressing the memory cost. However, to compute efficiently on GPUs, OTTT requires operations with spike trains and weighted summation of spike trains during forwarding. In addition, OTTT has shown a relationship with the Spike Representation, an alternative training method, though theoretical agreement with Spike Representation has yet to be proven. Our proposed method can solve these problems; namely, SAF can halve the number of operations during the forward process, and it can be theoretically proven that SAF is consistent with the Spike Representation and OTTT, respectively. Furthermore, we confirmed the a
&lt;/p&gt;</description></item><item><title>PointSSC&#26159;&#31532;&#19968;&#20010;&#20026;&#20102;&#35821;&#20041;&#22330;&#26223;&#34917;&#20840;&#32780;&#24341;&#20837;&#30340;&#36710;&#36742;&#22522;&#30784;&#35774;&#26045;&#28857;&#20113;&#21512;&#20316;&#22522;&#20934;&#65292;&#20855;&#22791;&#38271;&#36317;&#31163;&#24863;&#30693;&#21644;&#26368;&#23567;&#36974;&#25377;&#12290;&#36890;&#36807;&#20351;&#29992;Segment Anything&#36827;&#34892;&#33258;&#21160;&#21270;&#27880;&#37322;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28608;&#20809;&#38647;&#36798;&#30340;&#27169;&#22411;&#65292;&#32467;&#21512;&#34917;&#20840;&#21644;&#20998;&#21106;&#30340;&#21512;&#20316;&#27169;&#22359;&#65292;&#26469;&#25512;&#21160;&#35821;&#20041;&#28857;&#20113;&#34917;&#20840;&#22312;&#30495;&#23454;&#19990;&#30028;&#23548;&#33322;&#20013;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2309.12708</link><description>&lt;p&gt;
PointSSC&#65306;&#19968;&#20010;&#29992;&#20110;&#35821;&#20041;&#22330;&#26223;&#34917;&#20840;&#30340;&#36710;&#36742;&#22522;&#30784;&#35774;&#26045;&#28857;&#20113;&#21512;&#20316;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
PointSSC: A Cooperative Vehicle-Infrastructure Point Cloud Benchmark for Semantic Scene Completion. (arXiv:2309.12708v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12708
&lt;/p&gt;
&lt;p&gt;
PointSSC&#26159;&#31532;&#19968;&#20010;&#20026;&#20102;&#35821;&#20041;&#22330;&#26223;&#34917;&#20840;&#32780;&#24341;&#20837;&#30340;&#36710;&#36742;&#22522;&#30784;&#35774;&#26045;&#28857;&#20113;&#21512;&#20316;&#22522;&#20934;&#65292;&#20855;&#22791;&#38271;&#36317;&#31163;&#24863;&#30693;&#21644;&#26368;&#23567;&#36974;&#25377;&#12290;&#36890;&#36807;&#20351;&#29992;Segment Anything&#36827;&#34892;&#33258;&#21160;&#21270;&#27880;&#37322;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28608;&#20809;&#38647;&#36798;&#30340;&#27169;&#22411;&#65292;&#32467;&#21512;&#34917;&#20840;&#21644;&#20998;&#21106;&#30340;&#21512;&#20316;&#27169;&#22359;&#65292;&#26469;&#25512;&#21160;&#35821;&#20041;&#28857;&#20113;&#34917;&#20840;&#22312;&#30495;&#23454;&#19990;&#30028;&#23548;&#33322;&#20013;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#22330;&#26223;&#34917;&#20840;&#26088;&#22312;&#20026;&#22797;&#26434;&#30340;3D&#22330;&#26223;&#29983;&#25104;&#31354;&#38388;&#21344;&#29992;&#21644;&#35821;&#20041;&#26631;&#31614;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#35821;&#20041;&#22330;&#26223;&#34917;&#20840;&#27169;&#22411;&#37117;&#38598;&#20013;&#22312;&#20307;&#32032;&#34920;&#31034;&#19978;&#65292;&#23545;&#20110;&#22823;&#22411;&#23460;&#22806;&#31354;&#38388;&#26469;&#35828;&#23384;&#22312;&#20869;&#23384;&#25928;&#29575;&#20302;&#19979;&#30340;&#38382;&#39064;&#12290;&#28857;&#20113;&#25552;&#20379;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#20294;&#29616;&#26377;&#22522;&#20934;&#32570;&#20047;&#24102;&#26377;&#35821;&#20041;&#26631;&#31614;&#30340;&#23460;&#22806;&#28857;&#20113;&#22330;&#26223;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#35821;&#20041;&#22330;&#26223;&#34917;&#20840;&#30340;&#36710;&#36742;&#22522;&#30784;&#35774;&#26045;&#28857;&#20113;&#21512;&#20316;&#22522;&#20934;PointSSC&#12290;&#36825;&#20123;&#22330;&#26223;&#20855;&#26377;&#38271;&#36317;&#31163;&#24863;&#30693;&#21644;&#26368;&#23567;&#30340;&#36974;&#25377;&#12290;&#25105;&#20204;&#21033;&#29992;Segment Anything&#24320;&#21457;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#27880;&#37322;&#27969;&#31243;&#65292;&#20197;&#39640;&#25928;&#22320;&#20998;&#37197;&#35821;&#20041;&#26631;&#31614;&#12290;&#20026;&#20102;&#35780;&#20272;&#36827;&#23637;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28608;&#20809;&#38647;&#36798;&#30340;&#27169;&#22411;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#31354;&#38388;&#24863;&#30693;&#21464;&#25442;&#22120;&#29992;&#20110;&#20840;&#23616;&#21644;&#23616;&#37096;&#29305;&#24449;&#25552;&#21462;&#65292;&#20197;&#21450;&#19968;&#20010;&#34917;&#20840;&#21644;&#20998;&#21106;&#21512;&#20316;&#27169;&#22359;&#29992;&#20110;&#32852;&#21512;&#34917;&#20840;&#21644;&#20998;&#21106;&#12290;PointSSC&#25552;&#20379;&#20102;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#27979;&#35797;&#24179;&#21488;&#65292;&#25512;&#21160;&#20102;&#35821;&#20041;&#28857;&#20113;&#34917;&#20840;&#22312;&#30495;&#23454;&#19990;&#30028;&#23548;&#33322;&#20013;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic Scene Completion (SSC) aims to jointly generate space occupancies and semantic labels for complex 3D scenes. Most existing SSC models focus on volumetric representations, which are memory-inefficient for large outdoor spaces. Point clouds provide a lightweight alternative but existing benchmarks lack outdoor point cloud scenes with semantic labels. To address this, we introduce PointSSC, the first cooperative vehicle-infrastructure point cloud benchmark for semantic scene completion. These scenes exhibit long-range perception and minimal occlusion. We develop an automated annotation pipeline leveraging Segment Anything to efficiently assign semantics. To benchmark progress, we propose a LiDAR-based model with a Spatial-Aware Transformer for global and local feature extraction and a Completion and Segmentation Cooperative Module for joint completion and segmentation. PointSSC provides a challenging testbed to drive advances in semantic point cloud completion for real-world navi
&lt;/p&gt;</description></item><item><title>CoT-BERT&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24605;&#32500;&#38142;&#26465;&#22686;&#24378;&#26080;&#30417;&#30563;&#21477;&#23376;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#20010;&#38454;&#27573;&#30340;&#22788;&#29702;&#65292;&#24341;&#20837;&#24605;&#32500;&#38142;&#26465;&#30340;&#27010;&#24565;&#36827;&#34892;&#21521;&#37327;&#21270;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.11143</link><description>&lt;p&gt;
CoT-BERT: &#36890;&#36807;&#24605;&#32500;&#38142;&#26465;&#22686;&#24378;&#26080;&#30417;&#30563;&#21477;&#23376;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
CoT-BERT: Enhancing Unsupervised Sentence Representation through Chain-of-Thought. (arXiv:2309.11143v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11143
&lt;/p&gt;
&lt;p&gt;
CoT-BERT&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24605;&#32500;&#38142;&#26465;&#22686;&#24378;&#26080;&#30417;&#30563;&#21477;&#23376;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#20010;&#38454;&#27573;&#30340;&#22788;&#29702;&#65292;&#24341;&#20837;&#24605;&#32500;&#38142;&#26465;&#30340;&#27010;&#24565;&#36827;&#34892;&#21521;&#37327;&#21270;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;&#26088;&#22312;&#23558;&#36755;&#20837;&#21477;&#23376;&#36716;&#21270;&#20026;&#23500;&#21547;&#22797;&#26434;&#35821;&#20041;&#20449;&#24687;&#30340;&#22266;&#23450;&#38271;&#24230;&#21521;&#37327;&#65292;&#21516;&#26102;&#28040;&#38500;&#23545;&#26631;&#27880;&#25968;&#25454;&#30340;&#20381;&#36182;&#12290;&#36817;&#24180;&#26469;&#65292;&#22312;&#23545;&#27604;&#23398;&#20064;&#21644;&#25552;&#31034;&#24037;&#31243;&#30340;&#25512;&#21160;&#19979;&#65292;&#35813;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#26497;&#22823;&#22320;&#32553;&#23567;&#20102;&#26080;&#30417;&#30563;&#21644;&#26377;&#30417;&#30563;&#31574;&#30053;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#20010;&#36712;&#36857;&#20013;&#65292;&#20173;&#28982;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#24605;&#32500;&#38142;&#26465;&#30340;&#28508;&#22312;&#33021;&#21147;&#12290;&#20026;&#20102;&#37322;&#25918;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;&#22914;BERT&#65289;&#20013;&#30340;&#28508;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21477;&#23376;&#34920;&#31034;&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#65306;&#29702;&#35299;&#21644;&#25688;&#35201;&#12290;&#38543;&#21518;&#65292;&#21518;&#19968;&#38454;&#27573;&#30340;&#36755;&#20986;&#34987;&#21033;&#29992;&#20026;&#36755;&#20837;&#21477;&#23376;&#30340;&#21521;&#37327;&#21270;&#34920;&#31034;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#65292;&#25105;&#20204;&#23545;&#23545;&#27604;&#23398;&#20064;&#25439;&#22833;&#20989;&#25968;&#21644;&#27169;&#26495;&#21435;&#22122;&#25216;&#26415;&#36827;&#34892;&#20102;&#31934;&#32454;&#35843;&#25972;&#12290;&#20005;&#26684;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;CoT-BERT&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised sentence representation learning aims to transform input sentences into fixed-length vectors enriched with intricate semantic information while obviating the reliance on labeled data. Recent progress within this field, propelled by contrastive learning and prompt engineering, has significantly bridged the gap between unsupervised and supervised strategies. Nonetheless, the potential utilization of Chain-of-Thought, remains largely untapped within this trajectory. To unlock latent capabilities within pre-trained models, such as BERT, we propose a two-stage approach for sentence representation: comprehension and summarization. Subsequently, the output of the latter phase is harnessed as the vectorized representation of the input sentence. For further performance enhancement, we meticulously refine both the contrastive learning loss function and the template denoising technique for prompt engineering. Rigorous experimentation substantiates our method, CoT-BERT, transcending a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#26799;&#24230;&#21327;&#35843;&#26041;&#27861;&#35299;&#20915;&#20102;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;FedGH&#65292;&#36890;&#36807;&#20943;&#36731;&#26412;&#22320;&#28418;&#31227;&#26469;&#22686;&#24378;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#21644;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#22330;&#26223;&#19979;&#65292;FedGH&#22987;&#32456;&#33021;&#22815;&#26174;&#33879;&#25552;&#21319;&#32852;&#37030;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.06692</link><description>&lt;p&gt;
&#35299;&#20915;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#20013;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#38382;&#39064;&#30340;&#26799;&#24230;&#21327;&#35843;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Tackling the Non-IID Issue in Heterogeneous Federated Learning by Gradient Harmonization. (arXiv:2309.06692v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06692
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#26799;&#24230;&#21327;&#35843;&#26041;&#27861;&#35299;&#20915;&#20102;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;FedGH&#65292;&#36890;&#36807;&#20943;&#36731;&#26412;&#22320;&#28418;&#31227;&#26469;&#22686;&#24378;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#21644;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#22330;&#26223;&#19979;&#65292;FedGH&#22987;&#32456;&#33021;&#22815;&#26174;&#33879;&#25552;&#21319;&#32852;&#37030;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#20445;&#25252;&#38544;&#31169;&#30340;&#33539;&#24335;&#65292;&#29992;&#20110;&#20174;&#20998;&#25955;&#30340;&#23458;&#25143;&#31471;&#21327;&#20316;&#35757;&#32451;&#20840;&#23616;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#32852;&#37030;&#23398;&#20064;&#30340;&#24615;&#33021;&#21463;&#21040;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#25968;&#25454;&#21644;&#35774;&#22791;&#24322;&#26500;&#24615;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#26381;&#21153;&#22120;&#31471;&#30340;&#26799;&#24230;&#20914;&#31361;&#35270;&#35282;&#37325;&#26032;&#24605;&#32771;&#36825;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#35843;&#26597;&#20102;&#22810;&#20010;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#26799;&#24230;&#20914;&#31361;&#29616;&#35937;&#65292;&#24182;&#25581;&#31034;&#20102;&#26356;&#24378;&#30340;&#24322;&#26500;&#24615;&#20250;&#23548;&#33268;&#26356;&#20005;&#37325;&#30340;&#26799;&#24230;&#20914;&#31361;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FedGH&#65292;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26799;&#24230;&#21327;&#35843;&#26469;&#20943;&#36731;&#26412;&#22320;&#28418;&#31227;&#12290;&#36825;&#31181;&#25216;&#26415;&#23558;&#19968;&#20010;&#26799;&#24230;&#21521;&#37327;&#25237;&#24433;&#21040;&#19982;&#20854;&#20182;&#20914;&#31361;&#23458;&#25143;&#31471;&#23545;&#20043;&#38388;&#30340;&#27491;&#20132;&#24179;&#38754;&#19978;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;FedGH&#22312;&#19981;&#21516;&#22522;&#20934;&#21644;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#22330;&#26223;&#19979;&#22987;&#32456;&#33021;&#22815;&#26174;&#33879;&#25552;&#21319;&#22810;&#20010;&#26368;&#20808;&#36827;&#30340;&#32852;&#37030;&#23398;&#20064;&#22522;&#32447;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;FedGH&#22312;&#29305;&#23450;&#22330;&#26223;&#20013;&#21462;&#24471;&#20102;&#26356;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a privacy-preserving paradigm for collaboratively training a global model from decentralized clients. However, the performance of FL is hindered by non-independent and identically distributed (non-IID) data and device heterogeneity. In this work, we revisit this key challenge through the lens of gradient conflicts on the server side. Specifically, we first investigate the gradient conflict phenomenon among multiple clients and reveal that stronger heterogeneity leads to more severe gradient conflicts. To tackle this issue, we propose FedGH, a simple yet effective method that mitigates local drifts through Gradient Harmonization. This technique projects one gradient vector onto the orthogonal plane of the other within conflicting client pairs. Extensive experiments demonstrate that FedGH consistently enhances multiple state-of-the-art FL baselines across diverse benchmarks and non-IID scenarios. Notably, FedGH yields more significant improvements in scenarios 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31163;&#32447;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#30340;&#25552;&#31034;&#35780;&#20272;&#19982;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#38598;&#21644;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#65292;&#39044;&#27979;&#25552;&#31034;&#24615;&#33021;&#12289;&#25552;&#39640;&#25104;&#26412;&#25928;&#30410;&#12289;&#29983;&#25104;&#26131;&#35835;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.06553</link><description>&lt;p&gt;
&#31163;&#32447;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#19979;&#30340;&#25552;&#31034;&#35780;&#20272;&#19982;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Offline Prompt Evaluation and Optimization with Inverse Reinforcement Learning. (arXiv:2309.06553v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06553
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31163;&#32447;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#30340;&#25552;&#31034;&#35780;&#20272;&#19982;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#38598;&#21644;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#65292;&#39044;&#27979;&#25552;&#31034;&#24615;&#33021;&#12289;&#25552;&#39640;&#25104;&#26412;&#25928;&#30410;&#12289;&#29983;&#25104;&#26131;&#35835;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20687;ChatGPT&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21457;&#23637;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#21033;&#29992;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#20805;&#20998;&#25581;&#31034;LLMs&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#38656;&#35201;&#22312;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#30340;&#24191;&#38420;&#25628;&#32034;&#31354;&#38388;&#20013;&#36827;&#34892;&#23548;&#33322;&#12290;&#34429;&#28982;&#25552;&#31034;&#24037;&#31243;&#26174;&#31034;&#20986;&#28508;&#21147;&#65292;&#20294;&#35797;&#38169;&#23581;&#35797;&#20013;&#25152;&#38656;&#30340;&#20154;&#24037;&#35774;&#35745;&#25552;&#31034;&#21644;&#30456;&#20851;&#25104;&#26412;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#20851;&#38190;&#26159;&#65292;&#25552;&#31034;&#20248;&#21270;&#30340;&#25928;&#29575;&#21462;&#20915;&#20110;&#26114;&#36149;&#30340;&#25552;&#31034;&#35780;&#20272;&#36807;&#31243;&#12290;&#26412;&#24037;&#20316;&#20171;&#32461;&#20102;Prompt-OIRL&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#31163;&#32447;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#24357;&#21512;&#26377;&#25928;&#25552;&#31034;&#35780;&#20272;&#21644;&#21487;&#36127;&#25285;&#24615;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#19987;&#23478;&#35780;&#20272;&#30340;&#31163;&#32447;&#25968;&#25454;&#38598;&#65292;&#36816;&#29992;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#33719;&#24471;&#19968;&#20010;&#38024;&#23545;&#31163;&#32447;&#12289;&#26597;&#35810;&#20381;&#36182;&#22411;&#25552;&#31034;&#35780;&#20272;&#30340;&#22870;&#21169;&#27169;&#22411;&#12290;Prompt-OIRL&#30340;&#20248;&#28857;&#26159;&#22810;&#26041;&#38754;&#30340;&#65306;&#23427;&#39044;&#27979;&#25552;&#31034;&#30340;&#24615;&#33021;&#65292;&#25104;&#26412;&#39640;&#25928;&#65292;&#29983;&#25104;&#26131;&#35835;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent advances in the development of Large Language Models (LLMs) like ChatGPT have achieved remarkable performance by leveraging human expertise. Yet, fully eliciting LLMs' potential for complex tasks requires navigating the vast search space of natural language prompts. While prompt engineering has shown promise, the requisite human-crafted prompts in trial-and-error attempts and the associated costs pose significant challenges. Crucially, the efficiency of prompt optimization hinges on the costly procedure of prompt evaluation. This work introduces Prompt-OIRL, an approach rooted in offline inverse reinforcement learning that seeks to bridge the gap between effective prompt evaluation and affordability. Our method draws on offline datasets from expert evaluations, employing Inverse-RL to derive a reward model for offline, query-dependent prompt evaluations. The advantages of Prompt-OIRL are manifold: it predicts prompt performance, is cost-efficient, produces human-readable res
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#26657;&#20934;&#30340;&#22522;&#20934;&#30740;&#31350;&#65292;&#21033;&#29992;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#31354;&#38388;&#25506;&#32034;&#20102;&#27169;&#22411;&#26657;&#20934;&#23646;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#27169;&#22411;&#26657;&#20934;&#21487;&#20197;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#27867;&#21270;&#65292;&#24182;&#21487;&#20197;&#21516;&#26102;&#20860;&#39038;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#26657;&#20934;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.11838</link><description>&lt;p&gt;
&#19968;&#20010;&#20851;&#20110;&#26657;&#20934;&#30340;&#22522;&#20934;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Benchmark Study on Calibration. (arXiv:2308.11838v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11838
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#26657;&#20934;&#30340;&#22522;&#20934;&#30740;&#31350;&#65292;&#21033;&#29992;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#31354;&#38388;&#25506;&#32034;&#20102;&#27169;&#22411;&#26657;&#20934;&#23646;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#27169;&#22411;&#26657;&#20934;&#21487;&#20197;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#27867;&#21270;&#65292;&#24182;&#21487;&#20197;&#21516;&#26102;&#20860;&#39038;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#26657;&#20934;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#24191;&#27867;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#36825;&#20123;&#27169;&#22411;&#22797;&#26434;&#24615;&#30340;&#22686;&#21152;&#65292;&#23427;&#20204;&#24448;&#24448;&#38754;&#20020;&#26657;&#20934;&#38382;&#39064;&#65292;&#23613;&#31649;&#39044;&#27979;&#20934;&#30830;&#24615;&#26377;&#25152;&#25552;&#39640;&#12290;&#35768;&#22810;&#30740;&#31350;&#36890;&#36807;&#25968;&#25454;&#39044;&#22788;&#29702;&#12289;&#20351;&#29992;&#29305;&#23450;&#25439;&#22833;&#20989;&#25968;&#21644;&#35757;&#32451;&#26694;&#26550;&#26469;&#25913;&#21892;&#26657;&#20934;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23545;&#26657;&#20934;&#23646;&#24615;&#30340;&#30740;&#31350;&#26377;&#28857;&#34987;&#24573;&#35270;&#20102;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21033;&#29992;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#25628;&#32034;&#31354;&#38388;&#65292;&#22312;&#20840;&#38754;&#25506;&#32034;&#26657;&#20934;&#23646;&#24615;&#30340;&#27169;&#22411;&#26550;&#26500;&#31354;&#38388;&#20013;&#25552;&#20379;&#20102;&#19968;&#20010;&#35814;&#23613;&#30340;&#27169;&#22411;&#26550;&#26500;&#31354;&#38388;&#12290;&#25105;&#20204;&#29305;&#21035;&#21019;&#24314;&#20102;&#19968;&#20010;&#27169;&#22411;&#26657;&#20934;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;NATS-Bench&#25628;&#32034;&#31354;&#38388;&#20013;&#35780;&#20272;&#20102;90&#20010;&#22522;&#20110;&#21306;&#38388;&#30340;&#26657;&#20934;&#24230;&#37327;&#21644;12&#20010;&#20854;&#20182;&#26657;&#20934;&#24230;&#37327;&#65292;&#28085;&#30422;&#20102;117,702&#20010;&#29420;&#29305;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#26088;&#22312;&#36890;&#36807;&#25105;&#20204;&#25552;&#20986;&#30340;&#25968;&#25454;&#38598;&#22238;&#31572;&#35813;&#39046;&#22495;&#19968;&#20123;&#38271;&#26399;&#23384;&#22312;&#30340;&#38382;&#39064;&#65306;&#65288;i&#65289;&#27169;&#22411;&#26657;&#20934;&#33021;&#21542;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#27867;&#21270;&#65311;&#65288;ii&#65289;&#33021;&#21542;&#21516;&#26102;&#20860;&#39038;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#26657;&#20934;&#24615;&#33021;&#65311;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks are increasingly utilized in various machine learning tasks. However, as these models grow in complexity, they often face calibration issues, despite enhanced prediction accuracy. Many studies have endeavored to improve calibration performance through data preprocessing, the use of specific loss functions, and training frameworks. Yet, investigations into calibration properties have been somewhat overlooked. Our study leverages the Neural Architecture Search (NAS) search space, offering an exhaustive model architecture space for thorough calibration properties exploration. We specifically create a model calibration dataset. This dataset evaluates 90 bin-based and 12 additional calibration measurements across 117,702 unique neural networks within the widely employed NATS-Bench search space. Our analysis aims to answer several longstanding questions in the field, using our proposed dataset: (i) Can model calibration be generalized across different tasks? (ii) Can rob
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20219;&#21153;&#20998;&#35299;&#23398;&#20064;&#25277;&#35937;&#35270;&#35273;&#25512;&#29702;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#24418;&#22120;&#34013;&#22270;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#39044;&#27979;&#21333;&#20010;&#23545;&#35937;&#21450;&#20854;&#25490;&#21015;&#30340;&#35270;&#35273;&#29305;&#24615;&#65292;&#36890;&#36807;&#22810;&#32500;&#39044;&#27979;&#26469;&#36873;&#25321;&#31572;&#26696;&#12290;</title><link>http://arxiv.org/abs/2308.06528</link><description>&lt;p&gt;
&#36890;&#36807;&#20219;&#21153;&#20998;&#35299;&#23398;&#20064;&#25277;&#35937;&#35270;&#35273;&#25512;&#29702;&#65306;&#22522;&#20110;Raven&#28176;&#36827;&#30697;&#38453;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Learning Abstract Visual Reasoning via Task Decomposition: A Case Study in Raven Progressive Matrices. (arXiv:2308.06528v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06528
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20219;&#21153;&#20998;&#35299;&#23398;&#20064;&#25277;&#35937;&#35270;&#35273;&#25512;&#29702;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#24418;&#22120;&#34013;&#22270;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#39044;&#27979;&#21333;&#20010;&#23545;&#35937;&#21450;&#20854;&#25490;&#21015;&#30340;&#35270;&#35273;&#29305;&#24615;&#65292;&#36890;&#36807;&#22810;&#32500;&#39044;&#27979;&#26469;&#36873;&#25321;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#36827;&#34892;&#25277;&#35937;&#25512;&#29702;&#30340;&#25361;&#25112;&#20043;&#19968;&#26159;&#38382;&#39064;&#36890;&#24120;&#34987;&#25552;&#20986;&#20026;&#25972;&#20307;&#20219;&#21153;&#65292;&#27809;&#26377;&#20013;&#38388;&#30446;&#26631;&#12290;&#22312;Raven&#28176;&#36827;&#30697;&#38453;&#65288;RPM&#65289;&#20013;&#65292;&#20219;&#21153;&#26159;&#22312;&#32473;&#23450;&#19978;&#19979;&#25991;&#30340;&#24773;&#20917;&#19979;&#36873;&#25321;&#19968;&#20010;&#21487;&#29992;&#31572;&#26696;&#65292;&#20854;&#20013;&#19978;&#19979;&#25991;&#21644;&#31572;&#26696;&#37117;&#26159;&#22797;&#21512;&#22270;&#20687;&#65292;&#20855;&#26377;&#22810;&#20010;&#23545;&#35937;&#20197;&#21450;&#21508;&#31181;&#31354;&#38388;&#23433;&#25490;&#12290;&#30001;&#20110;&#21482;&#26377;&#36825;&#20010;&#39640;&#32423;&#30446;&#26631;&#20316;&#20026;&#25351;&#23548;&#65292;&#23398;&#20064;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22823;&#22810;&#25968;&#29616;&#20195;&#35299;&#20915;&#26041;&#26696;&#24448;&#24448;&#19981;&#36879;&#26126;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#24418;&#22120;&#34013;&#22270;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#19981;&#30452;&#25509;&#36827;&#34892;&#19978;&#36848;&#36873;&#25321;&#65292;&#32780;&#26159;&#39044;&#27979;&#21333;&#20010;&#23545;&#35937;&#21450;&#20854;&#25490;&#21015;&#30340;&#35270;&#35273;&#29305;&#24615;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#33719;&#24471;&#30340;&#22810;&#32500;&#39044;&#27979;&#30452;&#25509;&#24182;&#32622;&#20197;&#36873;&#25321;&#31572;&#26696;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#27169;&#22411;&#23558;&#35270;&#35273;&#36755;&#20837;&#35299;&#26512;&#20026;&#20196;&#29260;&#30340;&#20960;&#31181;&#26041;&#24335;&#65292;&#24182;&#37319;&#29992;&#20102;&#20960;&#31181;&#33258;&#30417;&#30563;&#35757;&#32451;&#20013;&#36755;&#20837;&#30340;&#23631;&#34109;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the challenges in learning to perform abstract reasoning is that problems are often posed as monolithic tasks, with no intermediate subgoals. In Raven Progressive Matrices (RPM), the task is to choose one of the available answers given a context, where both contexts and answers are composite images featuring multiple objects in various spatial arrangements. As this high-level goal is the only guidance available, learning is challenging and most contemporary solvers tend to be opaque. In this study, we propose a deep learning architecture based on the transformer blueprint which, rather than directly making the above choice, predicts the visual properties of individual objects and their arrangements. The multidimensional predictions obtained in this way are then directly juxtaposed to choose the answer. We consider a few ways in which the model parses the visual input into tokens and several regimes of masking parts of the input in self-supervised training. In experimental assess
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;GPU&#21457;&#36215;&#30452;&#25509;&#23384;&#20648;&#35775;&#38382;&#26469;&#21152;&#36895;GNN&#26694;&#26550;&#20013;&#30340;&#37319;&#26679;&#21644;&#32858;&#21512;&#25805;&#20316;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22312;&#35757;&#32451;&#22823;&#35268;&#27169;&#22270;&#19978;&#26102;CPU&#26080;&#27861;&#20805;&#20998;&#21033;&#29992;GPU&#36164;&#28304;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.16384</link><description>&lt;p&gt;
&#21152;&#36895;GNN&#26694;&#26550;&#20013;&#30340;&#37319;&#26679;&#21644;&#32858;&#21512;&#25805;&#20316;&#65306;&#21033;&#29992;GPU&#21457;&#36215;&#30452;&#25509;&#23384;&#20648;&#35775;&#38382;
&lt;/p&gt;
&lt;p&gt;
Accelerating Sampling and Aggregation Operations in GNN Frameworks with GPU Initiated Direct Storage Accesses. (arXiv:2306.16384v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16384
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;GPU&#21457;&#36215;&#30452;&#25509;&#23384;&#20648;&#35775;&#38382;&#26469;&#21152;&#36895;GNN&#26694;&#26550;&#20013;&#30340;&#37319;&#26679;&#21644;&#32858;&#21512;&#25805;&#20316;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22312;&#35757;&#32451;&#22823;&#35268;&#27169;&#22270;&#19978;&#26102;CPU&#26080;&#27861;&#20805;&#20998;&#21033;&#29992;GPU&#36164;&#28304;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#27491;&#22312;&#25104;&#20026;&#23398;&#20064;&#22270;&#32467;&#26500;&#25968;&#25454;&#21644;&#36827;&#34892;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#30340;&#19968;&#20010;&#24378;&#22823;&#24037;&#20855;&#65292;&#36866;&#29992;&#20110;&#21508;&#20010;&#24212;&#29992;&#39046;&#22495;&#12290;&#23613;&#31649;&#24050;&#32463;&#35777;&#26126;GNNs&#22312;&#20013;&#31561;&#35268;&#27169;&#30340;&#22270;&#19978;&#20855;&#26377;&#26377;&#25928;&#24615;&#65292;&#20294;&#22312;&#22823;&#35268;&#27169;&#22270;&#19978;&#35757;&#32451;&#20173;&#28982;&#38754;&#20020;&#30528;&#25968;&#25454;&#35775;&#38382;&#21644;&#25968;&#25454;&#31227;&#21160;&#26041;&#27861;&#30340;&#19981;&#36275;&#12290;&#29616;&#26377;&#30340;GNN&#35757;&#32451;&#26694;&#26550;&#20351;&#29992;CPU&#36827;&#34892;&#22270;&#37319;&#26679;&#21644;&#29305;&#24449;&#32858;&#21512;&#65292;&#32780;&#27169;&#22411;&#26435;&#37325;&#30340;&#35757;&#32451;&#21644;&#26356;&#26032;&#21017;&#30001;GPU&#25191;&#34892;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#28145;&#20837;&#20998;&#26512;&#21457;&#29616;CPU&#26080;&#27861;&#23454;&#29616;&#25152;&#38656;&#30340;&#21534;&#21520;&#37327;&#20197;&#20805;&#20998;&#21033;&#29992;&#26114;&#36149;&#30340;GPU&#36164;&#28304;&#12290;&#27492;&#22806;&#65292;&#24403;&#22270;&#21644;&#20854;&#23884;&#20837;&#19981;&#33021;&#36866;&#24212;CPU&#20869;&#23384;&#26102;&#65292;&#25805;&#20316;&#31995;&#32479;&#24341;&#20837;&#30340;&#24320;&#38144;&#65292;&#22914;&#22788;&#29702;&#39029;&#38754;&#38169;&#35823;&#65292;&#20250;&#25104;&#20026;&#20851;&#38190;&#36335;&#24452;&#30340;&#29942;&#39048;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GPU&#21457;&#36215;&#30340;&#30452;&#25509;&#23384;&#20648;&#35775;&#38382;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) are emerging as a powerful tool for learning from graph-structured data and performing sophisticated inference tasks in various application domains. Although GNNs have been shown to be effective on modest-sized graphs, training them on large-scale graphs remains a significant challenge due to lack of efficient data access and data movement methods. Existing frameworks for training GNNs use CPUs for graph sampling and feature aggregation, while the training and updating of model weights are executed on GPUs. However, our in-depth profiling shows the CPUs cannot achieve the throughput required to saturate GNN model training throughput, causing gross under-utilization of expensive GPU resources. Furthermore, when the graph and its embeddings do not fit in the CPU memory, the overhead introduced by the operating system, say for handling page-faults, comes in the critical path of execution.  To address these issues, we propose the GPU Initiated Direct Storage Ac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;EquiformerV2&#65292;&#36890;&#36807;&#20351;&#29992;&#26032;&#30340;&#21367;&#31215;&#31867;&#22411;&#21644;&#26550;&#26500;&#25913;&#36827;&#65292;&#25193;&#23637;&#20102;&#31561;&#21464;Transformer&#21040;&#26356;&#39640;&#30340;&#31561;&#21464;&#34920;&#31034;&#65292;&#22312;&#22788;&#29702;&#22823;&#22411;&#25968;&#25454;&#38598;&#26102;&#34920;&#29616;&#26356;&#22909;&#65292;&#33021;&#37327;&#21644;&#21147;&#30340;&#34920;&#29616;&#20063;&#24471;&#21040;&#20102;&#25552;&#39640;&#65292;&#35745;&#31639;&#25928;&#29575;&#20063;&#24471;&#21040;&#20102;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2306.12059</link><description>&lt;p&gt;
EquiformerV2: &#25913;&#36827;&#30340;&#31561;&#21464;Transformer&#65292;&#29992;&#20110;&#25193;&#23637;&#21040;&#26356;&#39640;&#27425;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
EquiformerV2: Improved Equivariant Transformer for Scaling to Higher-Degree Representations. (arXiv:2306.12059v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12059
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;EquiformerV2&#65292;&#36890;&#36807;&#20351;&#29992;&#26032;&#30340;&#21367;&#31215;&#31867;&#22411;&#21644;&#26550;&#26500;&#25913;&#36827;&#65292;&#25193;&#23637;&#20102;&#31561;&#21464;Transformer&#21040;&#26356;&#39640;&#30340;&#31561;&#21464;&#34920;&#31034;&#65292;&#22312;&#22788;&#29702;&#22823;&#22411;&#25968;&#25454;&#38598;&#26102;&#34920;&#29616;&#26356;&#22909;&#65292;&#33021;&#37327;&#21644;&#21147;&#30340;&#34920;&#29616;&#20063;&#24471;&#21040;&#20102;&#25552;&#39640;&#65292;&#35745;&#31639;&#25928;&#29575;&#20063;&#24471;&#21040;&#20102;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31561;&#21464;Transformer&#65288;&#20363;&#22914;Equiformer&#65289;&#24050;&#32463;&#35777;&#26126;&#20102;&#23558;Transformer&#24212;&#29992;&#20110;3D&#21407;&#23376;&#31995;&#32479;&#39046;&#22495;&#30340;&#21151;&#25928;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#23427;&#20204;&#20173;&#28982;&#23616;&#38480;&#20110;&#23567;&#25968;&#27425;&#31561;&#21464;&#34920;&#31034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#36825;&#20123;&#26550;&#26500;&#26159;&#21542;&#33021;&#22815;&#24456;&#22909;&#22320;&#25193;&#23637;&#21040;&#26356;&#39640;&#30340;&#27425;&#25968;&#12290;&#20174;Equiformer&#24320;&#22987;&#65292;&#25105;&#20204;&#39318;&#20808;&#29992;eSCN&#21367;&#31215;&#26367;&#25442;&#20102;$SO(3)$&#21367;&#31215;&#65292;&#20197;&#26377;&#25928;&#22320;&#21512;&#24182;&#26356;&#39640;&#27425;&#30340;&#24352;&#37327;&#12290;&#28982;&#21518;&#65292;&#20026;&#20102;&#26356;&#22909;&#22320;&#21033;&#29992;&#26356;&#39640;&#27425;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#20010;&#26550;&#26500;&#25913;&#36827;&#8212;&#8212;&#27880;&#24847;&#21147;&#37325;&#26631;&#20934;&#21270;&#12289;&#21487;&#20998;&#31163;&#30340;$S^2$&#28608;&#27963;&#21644;&#21487;&#20998;&#31163;&#23618;&#24402;&#19968;&#21270;&#12290;&#23558;&#36825;&#19968;&#20999;&#25918;&#22312;&#19968;&#36215;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EquiformerV2&#65292;&#22312;&#22823;&#22411;OC20&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#20197;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#22312;&#21147;&#19978;&#25552;&#39640;&#20102;&#26368;&#22810;$12\%$&#65292;&#33021;&#37327;&#19978;&#25552;&#39640;&#20102;$4\%$&#65292;&#25552;&#20379;&#26356;&#22909;&#30340;&#36895;&#24230;-&#20934;&#30830;&#24615;&#26435;&#34913;&#65292;&#24182;&#19988;&#22312;&#35745;&#31639;&#21560;&#38468;&#33021;&#25152;&#38656;&#30340;DFT&#35745;&#31639;&#37327;&#26041;&#38754;&#32553;&#20943;&#20102;2&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Equivariant Transformers such as Equiformer have demonstrated the efficacy of applying Transformers to the domain of 3D atomistic systems. However, they are still limited to small degrees of equivariant representations due to their computational complexity. In this paper, we investigate whether these architectures can scale well to higher degrees. Starting from Equiformer, we first replace $SO(3)$ convolutions with eSCN convolutions to efficiently incorporate higher-degree tensors. Then, to better leverage the power of higher degrees, we propose three architectural improvements -- attention re-normalization, separable $S^2$ activation and separable layer normalization. Putting this all together, we propose EquiformerV2, which outperforms previous state-of-the-art methods on the large-scale OC20 dataset by up to $12\%$ on forces, $4\%$ on energies, offers better speed-accuracy trade-offs, and $2\times$ reduction in DFT calculations needed for computing adsorption energies.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35299;&#20915;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#24403;&#22870;&#21169;&#21576;&#8220;&#37325;&#23614;&#8221;&#20998;&#24067;&#26102;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#31532;&#19968;&#31181;&#22788;&#29702;&#36825;&#31181;&#24773;&#20917;&#30340;&#23454;&#20363;&#30456;&#20851;&#31639;&#27861;&#65292;&#24182;&#24471;&#21040;&#20102;&#26497;&#23567;&#26368;&#22823;&#21270;&#30340;&#36951;&#25022;&#30028;&#12290;</title><link>http://arxiv.org/abs/2306.06836</link><description>&lt;p&gt;
&#29992;&#20989;&#25968;&#36924;&#36817;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#37325;&#23614;&#22870;&#21169;&#38382;&#39064;&#30340;&#26497;&#23567;&#26368;&#22823;&#21270;&#31639;&#27861;&#21644;&#23454;&#20363;&#30456;&#20851;&#36951;&#25022;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
Tackling Heavy-Tailed Rewards in Reinforcement Learning with Function Approximation: Minimax Optimal and Instance-Dependent Regret Bounds. (arXiv:2306.06836v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06836
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#24403;&#22870;&#21169;&#21576;&#8220;&#37325;&#23614;&#8221;&#20998;&#24067;&#26102;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#31532;&#19968;&#31181;&#22788;&#29702;&#36825;&#31181;&#24773;&#20917;&#30340;&#23454;&#20363;&#30456;&#20851;&#31639;&#27861;&#65292;&#24182;&#24471;&#21040;&#20102;&#26497;&#23567;&#26368;&#22823;&#21270;&#30340;&#36951;&#25022;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#26377;&#35768;&#22810;&#24037;&#20316;&#37117;&#19987;&#27880;&#20110;&#20026;&#26377;&#30028;&#22870;&#21169;&#30340;&#24378;&#21270;&#23398;&#20064;&#35774;&#35745;&#26377;&#25928;&#31639;&#27861;&#65292;&#20294;&#24403;&#22870;&#21169;&#21576;&#29616;&#8220;&#37325;&#23614;&#8221;&#20998;&#24067;&#26102;&#8212;&#8212;&#21363;&#23384;&#22312;&#26576;&#20010; $\epsilon\in(0,1]$ &#20351;&#24471;&#20165;&#26377;&#26377;&#38480;&#30340;$(1+\epsilon)$-&#38454;&#30697;&#8212;&#8212;&#26159;&#21542;&#23384;&#22312;&#23545;&#22823;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#36827;&#34892;&#37319;&#26679;&#25110;&#26102;&#25928;&#24615;&#31639;&#27861;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290; &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#20855;&#26377;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#30340; RL &#20013;&#30340;&#36825;&#31181;&#22870;&#21169;&#26426;&#21046;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#39318;&#20808;&#20026;&#37325;&#23614;&#32447;&#24615;&#36172;&#33218;&#35774;&#35745;&#20102;&#19968;&#31181;&#31639;&#27861;&#8212;&#8212;\textsc{Heavy-OFUL}&#65292;&#20854;&#23454;&#29616;&#20102;&#19968;&#31181;&#23454;&#20363;&#30456;&#20851;&#30340; $T$-round &#36951;&#25022;&#24230;&#37327;&#65292;&#20026; $\tilde{O}\big(d T^{\frac{1-\epsilon}{2(1+\epsilon)}} \sqrt{\sum_{t=1}^T \nu_t^2} + d T^{\frac{1-\epsilon}{2(1+\epsilon)}}\big)$&#65292;&#36825;&#26159;&#36825;&#31181;&#31867;&#22411;&#30340;\emph{&#31532;&#19968;&#31687;}&#25991;&#31456;&#12290;$\nu_t^{1+\epsilon}$&#26159;&#31532; $t$ &#36718;&#22870;&#21169;&#30340; $(1+\epsilon)$-&#38454;&#20013;&#24515;&#30697;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#22312;&#24212;&#29992;&#20110; st &#30340;&#26368;&#22351;&#24773;&#20917;&#26102;&#65292;&#19978;&#36848;&#30028;&#26159;&#26497;&#23567;&#20540;&#30340;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
While numerous works have focused on devising efficient algorithms for reinforcement learning (RL) with uniformly bounded rewards, it remains an open question whether sample or time-efficient algorithms for RL with large state-action space exist when the rewards are \emph{heavy-tailed}, i.e., with only finite $(1+\epsilon)$-th moments for some $\epsilon\in(0,1]$. In this work, we address the challenge of such rewards in RL with linear function approximation. We first design an algorithm, \textsc{Heavy-OFUL}, for heavy-tailed linear bandits, achieving an \emph{instance-dependent} $T$-round regret of $\tilde{O}\big(d T^{\frac{1-\epsilon}{2(1+\epsilon)}} \sqrt{\sum_{t=1}^T \nu_t^2} + d T^{\frac{1-\epsilon}{2(1+\epsilon)}}\big)$, the \emph{first} of this kind. Here, $d$ is the feature dimension, and $\nu_t^{1+\epsilon}$ is the $(1+\epsilon)$-th central moment of the reward at the $t$-th round. We further show the above bound is minimax optimal when applied to the worst-case instances in st
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#36130;&#21153;&#20449;&#24687;&#25552;&#21462;&#30340;&#26694;&#26550;&#65288;AFIE&#65289;&#65292;&#29992;&#20110;&#25552;&#21462;&#28151;&#21512;&#38271;&#25991;&#26723;&#20013;&#30340;&#20851;&#38190;&#19994;&#32489;&#25351;&#26631;&#65288;KPI&#65289;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;LLMs&#22686;&#24378;&#20102;&#36130;&#21153;&#25253;&#21578;&#20449;&#24687;&#30340;&#29702;&#35299;&#21644;&#25552;&#21462;&#33021;&#21147;&#65292;&#24182;&#32463;&#36807;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#39564;&#35777;&#65292;&#35777;&#26126;&#20854;&#22312;GPT-3.5&#21644;GPT-4&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#30456;&#23545;&#20110;&#26420;&#32032;&#26041;&#27861;&#65292;&#24179;&#22343;&#31934;&#24230;&#25552;&#39640;&#20102;53.94&#65285;&#21644;33.77&#65285;&#12290;</title><link>http://arxiv.org/abs/2305.16344</link><description>&lt;p&gt;
&#21033;&#29992;LLMs&#20174;&#28151;&#21512;&#38271;&#25991;&#26723;&#20013;&#26816;&#32034;KPI&#30340;&#20840;&#38754;&#26694;&#26550;&#19982;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Leveraging LLMs for KPIs Retrieval from Hybrid Long-Document: A Comprehensive Framework and Dataset. (arXiv:2305.16344v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16344
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#36130;&#21153;&#20449;&#24687;&#25552;&#21462;&#30340;&#26694;&#26550;&#65288;AFIE&#65289;&#65292;&#29992;&#20110;&#25552;&#21462;&#28151;&#21512;&#38271;&#25991;&#26723;&#20013;&#30340;&#20851;&#38190;&#19994;&#32489;&#25351;&#26631;&#65288;KPI&#65289;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;LLMs&#22686;&#24378;&#20102;&#36130;&#21153;&#25253;&#21578;&#20449;&#24687;&#30340;&#29702;&#35299;&#21644;&#25552;&#21462;&#33021;&#21147;&#65292;&#24182;&#32463;&#36807;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#39564;&#35777;&#65292;&#35777;&#26126;&#20854;&#22312;GPT-3.5&#21644;GPT-4&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#30456;&#23545;&#20110;&#26420;&#32032;&#26041;&#27861;&#65292;&#24179;&#22343;&#31934;&#24230;&#25552;&#39640;&#20102;53.94&#65285;&#21644;33.77&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25991;&#26412;&#29702;&#35299;&#21644;&#34920;&#26684;&#25512;&#29702;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#23545;&#21253;&#21547;&#25991;&#26412;&#21644;&#34920;&#26684;&#25968;&#25454;&#30340;&#28151;&#21512;&#25991;&#26412;&#30340;&#29702;&#35299;&#21644;&#20998;&#26512;&#33021;&#21147;&#20173;&#26410;&#34987;&#20805;&#20998;&#21457;&#25496;&#12290;&#26412;&#30740;&#31350;&#19987;&#27880;&#20110;&#21033;&#29992;LLMs&#30340;&#28508;&#21147;&#65292;&#20174;&#28151;&#26434;&#30340;&#38271;&#22411;&#36130;&#21153;&#25253;&#21578;&#20013;&#29702;&#35299;&#20851;&#38190;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#21160;&#21270;&#36130;&#21153;&#20449;&#24687;&#25552;&#21462;&#65288;AFIE&#65289;&#26694;&#26550;&#65292;&#22686;&#24378;&#20102;LLMs&#29702;&#35299;&#21644;&#25552;&#21462;&#36130;&#21153;&#25253;&#21578;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35780;&#20272;AFIE&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#37329;&#34701;&#25253;&#21578;&#25968;&#20540;&#25552;&#21462;&#65288;FINE&#65289;&#25968;&#25454;&#38598;&#65292;&#24182;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;GPT-3.5&#21644;GPT-4&#19978;&#24471;&#21040;&#20102;&#26377;&#25928;&#39564;&#35777;&#65292;&#30456;&#23545;&#20110;&#26420;&#32032;&#26041;&#27861;&#65292;&#24179;&#22343;&#31934;&#24230;&#25552;&#39640;&#20102;53.94&#65285;&#21644;33.77&#65285;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;AFIE&#26694;&#26550;&#20026;&#20174;&#22797;&#26434;&#30340;&#28151;&#21512;&#25991;&#26723;&#20013;&#33258;&#21160;&#25552;&#21462;&#25968;&#20540;&#25552;&#20379;&#20102;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) demonstrate exceptional performance in textual understanding and tabular reasoning tasks. However, their ability to comprehend and analyze hybrid text, containing textual and tabular data, remains underexplored. In this research, we specialize in harnessing the potential of LLMs to comprehend critical information from financial reports, which are hybrid long-documents. We propose an Automated Financial Information Extraction (AFIE) framework that enhances LLMs' ability to comprehend and extract information from financial reports. To evaluate AFIE, we develop a Financial Reports Numerical Extraction (FINE) dataset and conduct an extensive experimental analysis. Our framework is effectively validated on GPT-3.5 and GPT-4, yielding average accuracy increases of 53.94% and 33.77%, respectively, compared to a naive method. These results suggest that the AFIE framework offers accuracy for automated numerical extraction from complex, hybrid documents.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#36830;&#32493;&#25511;&#21046;&#29615;&#22659;&#20013;&#30340;&#25277;&#35937;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31574;&#30053;&#26799;&#24230;&#23450;&#29702;&#65292;&#20801;&#35768;&#21033;&#29992;&#29615;&#22659;&#30340;&#36817;&#20284;&#23545;&#31216;&#24615;&#36827;&#34892;&#31574;&#30053;&#20248;&#21270;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#36827;&#34892;&#31574;&#30053;&#21644;MDP&#21516;&#24577;&#26144;&#23556;&#30340;&#23398;&#20064;&#65292;&#26368;&#21518;&#23637;&#31034;&#20102;&#31639;&#27861;&#22312;&#36830;&#32493;&#23545;&#31216;&#24615;&#29615;&#22659;&#21644;&#35270;&#35273;&#25511;&#21046;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.05666</link><description>&lt;p&gt;
&#23384;&#22312;&#23545;&#31216;&#24615;&#21644;&#29366;&#24577;&#25277;&#35937;&#30340;&#25919;&#31574;&#26799;&#24230;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Policy Gradient Methods in the Presence of Symmetries and State Abstractions. (arXiv:2305.05666v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05666
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#36830;&#32493;&#25511;&#21046;&#29615;&#22659;&#20013;&#30340;&#25277;&#35937;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31574;&#30053;&#26799;&#24230;&#23450;&#29702;&#65292;&#20801;&#35768;&#21033;&#29992;&#29615;&#22659;&#30340;&#36817;&#20284;&#23545;&#31216;&#24615;&#36827;&#34892;&#31574;&#30053;&#20248;&#21270;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#36827;&#34892;&#31574;&#30053;&#21644;MDP&#21516;&#24577;&#26144;&#23556;&#30340;&#23398;&#20064;&#65292;&#26368;&#21518;&#23637;&#31034;&#20102;&#31639;&#27861;&#22312;&#36830;&#32493;&#23545;&#31216;&#24615;&#29615;&#22659;&#21644;&#35270;&#35273;&#25511;&#21046;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#39640;&#32500;&#24230;&#21644;&#22797;&#26434;&#38382;&#39064;&#65292;&#24378;&#21270;&#23398;&#20064;&#20381;&#38752;&#25277;&#35937;&#26469;&#25552;&#39640;&#25928;&#29575;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#36830;&#32493;&#25511;&#21046;&#29615;&#22659;&#20013;&#30340;&#25277;&#35937;&#65292;&#24182;&#23558;MDP&#21516;&#24577;&#30340;&#23450;&#20041;&#25193;&#23637;&#21040;&#36830;&#32493;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#38024;&#23545;&#25277;&#35937;MDP&#30340;&#38543;&#26426;&#21644;&#30830;&#23450;&#24615;&#31574;&#30053;&#23548;&#20986;&#20102;&#19968;&#31181;&#31574;&#30053;&#26799;&#24230;&#23450;&#29702;&#12290;&#25105;&#20204;&#30340;&#31574;&#30053;&#26799;&#24230;&#32467;&#26524;&#20801;&#35768;&#21033;&#29992;&#29615;&#22659;&#30340;&#36817;&#20284;&#23545;&#31216;&#24615;&#36827;&#34892;&#31574;&#30053;&#20248;&#21270;&#12290;&#22522;&#20110;&#36825;&#20123;&#23450;&#29702;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65292;&#36825;&#20123;&#31639;&#27861;&#33021;&#22815;&#21516;&#26102;&#23398;&#20064;&#31574;&#30053;&#21644;MDP&#21516;&#24577;&#26144;&#23556;&#65292;&#20351;&#29992;&#26494;&#25955;&#21452;&#20223;&#23556;&#24230;&#37327;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31995;&#21015;&#20855;&#26377;&#36830;&#32493;&#23545;&#31216;&#24615;&#30340;&#29615;&#22659;&#65292;&#20197;&#36827;&#19968;&#27493;&#23637;&#31034;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#23384;&#22312;&#36825;&#20123;&#23545;&#31216;&#24615;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#21160;&#20316;&#25277;&#35937;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#36825;&#20123;&#29615;&#22659;&#20197;&#21450;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35270;&#35273;&#25511;&#21046;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning on high-dimensional and complex problems relies on abstraction for improved efficiency and generalization. In this paper, we study abstraction in the continuous-control setting, and extend the definition of MDP homomorphisms to the setting of continuous state and action spaces. We derive a policy gradient theorem on the abstract MDP for both stochastic and deterministic policies. Our policy gradient results allow for leveraging approximate symmetries of the environment for policy optimization. Based on these theorems, we propose a family of actor-critic algorithms that are able to learn the policy and the MDP homomorphism map simultaneously, using the lax bisimulation metric. Finally, we introduce a series of environments with continuous symmetries to further demonstrate the ability of our algorithm for action abstraction in the presence of such symmetries. We demonstrate the effectiveness of our method on our environments, as well as on challenging visual contro
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#25968;&#25454;&#29983;&#25104;&#21644;&#21442;&#25968;&#30072;&#21464;&#23454;&#29616;&#38544;&#31169;&#20445;&#25252;&#32852;&#37030;&#23398;&#20064;&#25509;&#36817;&#26368;&#20248;&#25928;&#29992;&#30340;&#19978;&#38480;&#26041;&#27861;&#65292;&#20854;&#20013;&#36890;&#36807;&#38477;&#20302;&#26041;&#24046;&#21644;&#27169;&#22411;&#21442;&#25968;&#24046;&#24322;&#26469;&#34913;&#37327;&#25928;&#29992;&#25439;&#22833;&#12290;</title><link>http://arxiv.org/abs/2305.04288</link><description>&lt;p&gt;
&#36890;&#36807;&#25968;&#25454;&#29983;&#25104;&#21644;&#21442;&#25968;&#30072;&#21464;&#23454;&#29616;&#38544;&#31169;&#20445;&#25252;&#32852;&#37030;&#23398;&#20064;&#30340;&#25509;&#36817;&#26368;&#20248;&#25928;&#29992;
&lt;/p&gt;
&lt;p&gt;
Towards Achieving Near-optimal Utility for Privacy-Preserving Federated Learning via Data Generation and Parameter Distortion. (arXiv:2305.04288v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04288
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#25968;&#25454;&#29983;&#25104;&#21644;&#21442;&#25968;&#30072;&#21464;&#23454;&#29616;&#38544;&#31169;&#20445;&#25252;&#32852;&#37030;&#23398;&#20064;&#25509;&#36817;&#26368;&#20248;&#25928;&#29992;&#30340;&#19978;&#38480;&#26041;&#27861;&#65292;&#20854;&#20013;&#36890;&#36807;&#38477;&#20302;&#26041;&#24046;&#21644;&#27169;&#22411;&#21442;&#25968;&#24046;&#24322;&#26469;&#34913;&#37327;&#25928;&#29992;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20351;&#21442;&#19982;&#26041;&#33021;&#22815;&#21327;&#20316;&#26500;&#24314;&#20855;&#26377;&#25552;&#39640;&#25928;&#29992;&#30340;&#20840;&#23616;&#27169;&#22411;&#65292;&#32780;&#19981;&#27844;&#38706;&#31169;&#26377;&#25968;&#25454;&#20449;&#24687;&#12290;&#24517;&#39035;&#37319;&#29992;&#36866;&#24403;&#30340;&#20445;&#25252;&#26426;&#21046;&#26469;&#28385;&#36275;&#20445;&#25252;&#38544;&#31169;&#21644;&#32500;&#25252;&#39640;&#27169;&#22411;&#25928;&#29992;&#30340;&#35201;&#27714;&#12290;&#30446;&#21069;&#37319;&#29992;&#30340;&#20445;&#25252;&#26426;&#21046;&#30340;&#26412;&#36136;&#65292;&#21253;&#25324;&#8220;&#38543;&#26426;&#21270;&#26426;&#21046;&#8221;&#21644;&#8220;&#21387;&#32553;&#26426;&#21046;&#8221;&#65292;&#26159;&#36890;&#36807;&#30072;&#21464;&#27169;&#22411;&#21442;&#25968;&#26469;&#20445;&#25252;&#38544;&#31169;&#12290;&#25105;&#20204;&#36890;&#36807;&#21407;&#22987;&#27169;&#22411;&#21442;&#25968;&#21644;&#30072;&#21464;&#27169;&#22411;&#21442;&#25968;&#20043;&#38388;&#30340;&#24046;&#36317;&#26469;&#34913;&#37327;&#25928;&#29992;&#12290;&#25105;&#20204;&#24819;&#35201;&#30830;&#23450;&#22312;&#20160;&#20040;&#26222;&#36941;&#26465;&#20214;&#19979;&#65292;&#36890;&#36807;&#25968;&#25454;&#29983;&#25104;&#21644;&#21442;&#25968;&#30072;&#21464;&#65292;&#38544;&#31169;&#20445;&#25252;&#30340;&#32852;&#37030;&#23398;&#20064;&#21487;&#20197;&#23454;&#29616;&#25509;&#36817;&#26368;&#20248;&#30340;&#25928;&#29992;&#12290;&#20026;&#20102;&#25552;&#20379;&#25509;&#36817;&#26368;&#20248;&#25928;&#29992;&#30340;&#36884;&#24452;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25928;&#29992;&#25439;&#22833;&#30340;&#19978;&#38480;&#65292;&#29992;&#20004;&#20010;&#20027;&#35201;&#39033;&#31216;&#20026;&#38477;&#20302;&#26041;&#24046;&#21644;&#27169;&#22411;&#21442;&#25968;&#24046;&#24322;&#26469;&#34913;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) enables participating parties to collaboratively build a global model with boosted utility without disclosing private data information. Appropriate protection mechanisms have to be adopted to fulfill the requirements in preserving \textit{privacy} and maintaining high model \textit{utility}. The nature of the widely-adopted protection mechanisms including \textit{Randomization Mechanism} and \textit{Compression Mechanism} is to protect privacy via distorting model parameter. We measure the utility via the gap between the original model parameter and the distorted model parameter. We want to identify under what general conditions privacy-preserving federated learning can achieve near-optimal utility via data generation and parameter distortion. To provide an avenue for achieving near-optimal utility, we present an upper bound for utility loss, which is measured using two main terms called variance-reduction and model parameter discrepancy separately. Our analysis
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#23618;&#27425;&#25968;&#25454;&#26377;&#25928;&#34920;&#31034;&#23398;&#20064;&#30340;RNA&#35774;&#35745;&#27969;&#31243;&#65292;&#36890;&#36807;&#26500;&#24314;&#22823;&#22411;&#25968;&#25454;&#38598;&#24182;&#35774;&#35745;&#20840;&#38754;&#30340;&#32467;&#26500;&#24314;&#27169;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#30340;RNA&#24207;&#21015;&#35774;&#35745;&#12290;</title><link>http://arxiv.org/abs/2301.10774</link><description>&lt;p&gt;
&#22522;&#20110;&#23618;&#27425;&#25968;&#25454;&#26377;&#25928;&#34920;&#31034;&#23398;&#20064;&#30340;RNA&#19977;&#32423;&#32467;&#26500;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Data-efficient Representation Learning for Tertiary Structure-based RNA Design. (arXiv:2301.10774v2 [q-bio.BM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10774
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#23618;&#27425;&#25968;&#25454;&#26377;&#25928;&#34920;&#31034;&#23398;&#20064;&#30340;RNA&#35774;&#35745;&#27969;&#31243;&#65292;&#36890;&#36807;&#26500;&#24314;&#22823;&#22411;&#25968;&#25454;&#38598;&#24182;&#35774;&#35745;&#20840;&#38754;&#30340;&#32467;&#26500;&#24314;&#27169;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#30340;RNA&#24207;&#21015;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20154;&#24037;&#26234;&#33021;&#24050;&#22312;&#25581;&#31034;&#29983;&#29289;&#22823;&#20998;&#23376;&#30340;&#19968;&#32423;&#24207;&#21015;&#19982;&#19977;&#32423;&#32467;&#26500;&#20043;&#38388;&#30340;&#20851;&#31995;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#30528;&#36827;&#23637;&#65292;&#20294;&#22522;&#20110;&#29305;&#23450;&#19977;&#32423;&#32467;&#26500;&#35774;&#35745;RNA&#24207;&#21015;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#34429;&#28982;&#34507;&#30333;&#36136;&#35774;&#35745;&#20013;&#30340;&#29616;&#26377;&#26041;&#27861;&#24050;&#32463;&#24443;&#24213;&#25506;&#32034;&#20102;&#34507;&#30333;&#36136;&#20013;&#32467;&#26500;&#21040;&#24207;&#21015;&#30340;&#20381;&#36182;&#24615;&#65292;&#20294;RNA&#35774;&#35745;&#20173;&#38754;&#20020;&#32467;&#26500;&#22797;&#26434;&#24615;&#21644;&#25968;&#25454;&#31232;&#32570;&#24615;&#30340;&#22256;&#38590;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#34429;&#28982;RNA&#19982;&#34507;&#30333;&#36136;&#20849;&#20139;&#31867;&#20284;&#30340;&#32467;&#26500;&#32452;&#20998;&#65292;&#20294;&#30452;&#25509;&#23558;&#34507;&#30333;&#36136;&#35774;&#35745;&#26041;&#27861;&#31227;&#26893;&#21040;RNA&#35774;&#35745;&#20013;&#21364;&#26080;&#27861;&#21462;&#24471;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#31995;&#32479;&#26500;&#24314;&#25968;&#25454;&#39537;&#21160;&#30340;RNA&#35774;&#35745;&#27969;&#31243;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#22411;&#12289;&#31934;&#24515;&#31574;&#21010;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#32467;&#26500;&#24314;&#27169;&#26041;&#27861;&#26469;&#34920;&#31034;&#22797;&#26434;&#30340;RNA&#19977;&#32423;&#32467;&#26500;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23618;&#27425;&#25968;&#25454;&#26377;&#25928;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#23398;&#20064;&#32467;&#26500;&#34920;&#31034;&#30340;&#22810;&#20010;&#23618;&#27425;&#29305;&#24449;&#65292;&#20197;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;RNA&#24207;&#21015;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
While artificial intelligence has made remarkable strides in revealing the relationship between biological macromolecules' primary sequence and tertiary structure, designing RNA sequences based on specified tertiary structures remains challenging. Though existing approaches in protein design have thoroughly explored structure-to-sequence dependencies in proteins, RNA design still confronts difficulties due to structural complexity and data scarcity. Adding to the problem, direct transplantation of protein design methodologies into RNA design fails to achieve satisfactory outcomes although sharing similar structural components. In this study, we aim to systematically construct a data-driven RNA design pipeline. We crafted a large, well-curated benchmark dataset and designed a comprehensive structural modeling approach to represent the complex RNA tertiary structure. More importantly, we proposed a hierarchical data-efficient representation learning framework that learns structural repre
&lt;/p&gt;</description></item></channel></rss>