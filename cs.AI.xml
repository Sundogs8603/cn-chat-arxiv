<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31070;&#32463;&#20809;&#21051;&#25216;&#26415;&#26469;&#35299;&#20915;&#35745;&#31639;&#20809;&#23398;&#20013;&#30340;&#35774;&#35745;&#21040;&#21046;&#36896;&#38388;&#38548;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#39044;&#35757;&#32451;&#30340;&#20809;&#21051;&#27169;&#25311;&#22120;&#25972;&#21512;&#21040;&#20809;&#23398;&#35774;&#35745;&#24490;&#29615;&#20013;&#65292;&#25105;&#20204;&#25104;&#21151;&#23454;&#29616;&#20102;&#20809;&#23398;&#22120;&#20214;&#30340;&#35774;&#35745;&#21644;&#21046;&#36896;&#30340;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.17343</link><description>&lt;p&gt;
&#31070;&#32463;&#20809;&#21051;&#65306;&#29992;"Real2Sim"&#23398;&#20064;&#21040;&#30340;&#20809;&#21051;&#27169;&#25311;&#22120;&#26469;&#28040;&#38500;&#35745;&#31639;&#20809;&#23398;&#20013;&#30340;&#35774;&#35745;&#21040;&#21046;&#36896;&#38388;&#38548;
&lt;/p&gt;
&lt;p&gt;
Neural Lithography: Close the Design-to-Manufacturing Gap in Computational Optics with a 'Real2Sim' Learned Photolithography Simulator. (arXiv:2309.17343v1 [physics.optics])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17343
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31070;&#32463;&#20809;&#21051;&#25216;&#26415;&#26469;&#35299;&#20915;&#35745;&#31639;&#20809;&#23398;&#20013;&#30340;&#35774;&#35745;&#21040;&#21046;&#36896;&#38388;&#38548;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#39044;&#35757;&#32451;&#30340;&#20809;&#21051;&#27169;&#25311;&#22120;&#25972;&#21512;&#21040;&#20809;&#23398;&#35774;&#35745;&#24490;&#29615;&#20013;&#65292;&#25105;&#20204;&#25104;&#21151;&#23454;&#29616;&#20102;&#20809;&#23398;&#22120;&#20214;&#30340;&#35774;&#35745;&#21644;&#21046;&#36896;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#31070;&#32463;&#20809;&#21051;&#26469;&#35299;&#20915;&#35745;&#31639;&#20809;&#23398;&#20013;&#30340;&#8220;&#35774;&#35745;&#21040;&#21046;&#36896;&#8221;&#38388;&#38548;&#38382;&#39064;&#12290;&#20855;&#26377;&#22823;&#35774;&#35745;&#33258;&#30001;&#24230;&#30340;&#35745;&#31639;&#20809;&#23398;&#21487;&#20197;&#23454;&#29616;&#36229;&#36234;&#20256;&#32479;&#20809;&#23398;&#30340;&#20808;&#36827;&#21151;&#33021;&#21644;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#35774;&#35745;&#26041;&#27861;&#24448;&#24448;&#24573;&#35270;&#20102;&#21046;&#36896;&#36807;&#31243;&#30340;&#25968;&#20540;&#24314;&#27169;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#35774;&#35745;&#21644;&#21046;&#36896;&#30340;&#20809;&#23398;&#22120;&#20214;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#24615;&#33021;&#24046;&#24322;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#20840;&#21487;&#24494;&#20998;&#30340;&#35774;&#35745;&#26694;&#26550;&#65292;&#23558;&#32463;&#36807;&#39044;&#35757;&#32451;&#30340;&#20809;&#21051;&#27169;&#25311;&#22120;&#25972;&#21512;&#21040;&#22522;&#20110;&#27169;&#22411;&#30340;&#20809;&#23398;&#35774;&#35745;&#24490;&#29615;&#20013;&#12290;&#36890;&#36807;&#34701;&#21512;&#29289;&#29702;&#27169;&#25311;&#24314;&#27169;&#21644;&#22522;&#20110;&#23454;&#39564;&#25968;&#25454;&#30340;&#35757;&#32451;&#65292;&#25105;&#20204;&#30340;&#20809;&#21051;&#27169;&#25311;&#22120;&#22312;&#35774;&#35745;&#36807;&#31243;&#20013;&#20805;&#24403;&#20102;&#21046;&#36896;&#21487;&#34892;&#24615;&#30340;&#27491;&#21017;&#21270;&#22120;&#65292;&#34917;&#20607;&#20102;&#20809;&#21051;&#36807;&#31243;&#20013;&#24341;&#20837;&#30340;&#32467;&#26500;&#24046;&#24322;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#20856;&#22411;&#20219;&#21153;&#26469;&#35777;&#26126;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce neural lithography to address the 'design-to-manufacturing' gap in computational optics. Computational optics with large design degrees of freedom enable advanced functionalities and performance beyond traditional optics. However, the existing design approaches often overlook the numerical modeling of the manufacturing process, which can result in significant performance deviation between the design and the fabricated optics. To bridge this gap, we, for the first time, propose a fully differentiable design framework that integrates a pre-trained photolithography simulator into the model-based optical design loop. Leveraging a blend of physics-informed modeling and data-driven training using experimentally collected datasets, our photolithography simulator serves as a regularizer on fabrication feasibility during design, compensating for structure discrepancies introduced in the lithography process. We demonstrate the effectiveness of our approach through two typical tasks 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;MixQuant&#30340;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#31639;&#27861;&#65292;&#22312;&#27599;&#20010;&#23618;&#26435;&#37325;&#19978;&#25214;&#21040;&#20102;&#26368;&#20339;&#30340;&#37327;&#21270;&#20301;&#23485;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#37327;&#21270;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#38477;&#20302;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.17341</link><description>&lt;p&gt;
MixQuant: &#24102;&#26377;&#20301;&#23485;&#20248;&#21270;&#25628;&#32034;&#30340;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
MixQuant: Mixed Precision Quantization with a Bit-width Optimization Search. (arXiv:2309.17341v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17341
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;MixQuant&#30340;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#31639;&#27861;&#65292;&#22312;&#27599;&#20010;&#23618;&#26435;&#37325;&#19978;&#25214;&#21040;&#20102;&#26368;&#20339;&#30340;&#37327;&#21270;&#20301;&#23485;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#37327;&#21270;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#38477;&#20302;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#26159;&#19968;&#31181;&#21019;&#24314;&#39640;&#25928;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#30340;&#25216;&#26415;&#65292;&#23427;&#36890;&#36807;&#22312;&#27604;f32&#28014;&#28857;&#31934;&#24230;&#26356;&#20302;&#30340;&#20301;&#23485;&#19978;&#25191;&#34892;&#35745;&#31639;&#21644;&#23384;&#20648;&#24352;&#37327;&#26469;&#23454;&#29616;&#12290;&#37327;&#21270;&#20943;&#23569;&#20102;&#27169;&#22411;&#22823;&#23567;&#21644;&#25512;&#29702;&#24310;&#36831;&#65292;&#22240;&#27492;&#21487;&#20197;&#22312;&#35745;&#31639;&#36164;&#28304;&#21463;&#38480;&#21644;&#23454;&#26102;&#31995;&#32479;&#19978;&#37096;&#32626;DNNs&#12290;&#28982;&#32780;&#65292;&#37327;&#21270;&#21487;&#33021;&#20250;&#23548;&#33268;&#30001;&#33293;&#20837;&#35823;&#24046;&#24341;&#36215;&#30340;&#25968;&#20540;&#19981;&#31283;&#23450;&#24615;&#65292;&#20174;&#32780;&#23548;&#33268;&#35745;&#31639;&#19981;&#20934;&#30830;&#65292;&#36827;&#32780;&#38477;&#20302;&#20102;&#37327;&#21270;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#31867;&#20284;&#20110;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20559;&#32622;&#21644;&#28608;&#27963;&#23545;&#37327;&#21270;&#26356;&#25935;&#24863;&#65292;&#26368;&#22909;&#20445;&#25345;&#20840;&#31934;&#24230;&#25110;&#29992;&#26356;&#39640;&#30340;&#20301;&#23485;&#36827;&#34892;&#37327;&#21270;&#65292;&#25105;&#20204;&#34920;&#26126;&#19968;&#20123;&#26435;&#37325;&#27604;&#20854;&#20182;&#26435;&#37325;&#26356;&#25935;&#24863;&#65292;&#24212;&#22312;&#20854;&#37327;&#21270;&#20301;&#23485;&#19978;&#21453;&#26144;&#20986;&#26469;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MixQuant&#65292;&#19968;&#31181;&#22522;&#20110;&#33293;&#20837;&#35823;&#24046;&#30340;&#25628;&#32034;&#31639;&#27861;&#65292;&#20197;&#25214;&#21040;&#27599;&#20010;&#23618;&#26435;&#37325;&#30340;&#26368;&#20339;&#23450;&#21046;&#37327;&#21270;&#20301;&#23485;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantization is a technique for creating efficient Deep Neural Networks (DNNs), which involves performing computations and storing tensors at lower bit-widths than f32 floating point precision. Quantization reduces model size and inference latency, and therefore allows for DNNs to be deployed on platforms with constrained computational resources and real-time systems. However, quantization can lead to numerical instability caused by roundoff error which leads to inaccurate computations and therefore, a decrease in quantized model accuracy. Similarly to prior works, which have shown that both biases and activations are more sensitive to quantization and are best kept in full precision or quantized with higher bit-widths, we show that some weights are more sensitive than others which should be reflected on their quantization bit-width. To that end we propose MixQuant, a search algorithm that finds the optimal custom quantization bit-width for each layer weight based on roundoff error and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#33322;&#28857;&#21435;&#38500;&#25216;&#26415;&#20419;&#36827;&#20102;&#26174;&#24335;&#30340;&#26102;&#38388;&#23398;&#20064;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#36712;&#36857;&#39044;&#27979;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.17338</link><description>&lt;p&gt;
&#22312;&#21160;&#24577;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#36890;&#36807;&#21435;&#25481;&#33322;&#28857;&#26469;&#25913;&#36827;&#36712;&#36857;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Improving Trajectory Prediction in Dynamic Multi-Agent Environment by Dropping Waypoints. (arXiv:2309.17338v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17338
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#33322;&#28857;&#21435;&#38500;&#25216;&#26415;&#20419;&#36827;&#20102;&#26174;&#24335;&#30340;&#26102;&#38388;&#23398;&#20064;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#36712;&#36857;&#39044;&#27979;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36712;&#36857;&#30340;&#22810;&#26679;&#21644;&#19981;&#30830;&#23450;&#24615;&#26412;&#36136;&#32473;&#20934;&#30830;&#24314;&#27169;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#36816;&#21160;&#39044;&#27979;&#31995;&#32479;&#24517;&#39035;&#26377;&#25928;&#22320;&#20174;&#36807;&#21435;&#23398;&#20064;&#31354;&#38388;&#21644;&#26102;&#38388;&#20449;&#24687;&#65292;&#20197;&#39044;&#27979;&#26234;&#33021;&#20307;&#30340;&#26410;&#26469;&#36712;&#36857;&#12290;&#35768;&#22810;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#22534;&#21472;&#27169;&#22411;&#20013;&#30340;&#21333;&#29420;&#32452;&#20214;&#23398;&#20064;&#26102;&#38388;&#36816;&#21160;&#65292;&#20197;&#25429;&#25417;&#26102;&#38388;&#29305;&#24449;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;Temporal Waypoint Dropping&#65288;TWD&#65289;&#65292;&#36890;&#36807;&#33322;&#28857;&#21435;&#38500;&#25216;&#26415;&#20419;&#36827;&#26174;&#24335;&#30340;&#26102;&#38388;&#23398;&#20064;&#12290;&#36890;&#36807;&#33322;&#28857;&#21435;&#38500;&#23398;&#20064;&#21487;&#20197;&#36843;&#20351;&#27169;&#22411;&#25913;&#21892;&#20854;&#23545;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#26102;&#38388;&#20851;&#32852;&#30340;&#29702;&#35299;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#36712;&#36857;&#39044;&#27979;&#30340;&#25928;&#26524;&#12290;&#36712;&#36857;&#39044;&#27979;&#26041;&#27861;&#24120;&#24120;&#20551;&#35774;&#35266;&#27979;&#21040;&#30340;&#36712;&#36857;&#33322;&#28857;&#24207;&#21015;&#26159;&#23436;&#25972;&#30340;&#65292;&#24573;&#30053;&#20102;&#29616;&#23454;&#19990;&#30028;&#20013;&#21487;&#33021;&#23384;&#22312;&#32570;&#22833;&#20540;&#30340;&#24773;&#20917;&#65292;&#36825;&#21487;&#33021;&#20250;&#24433;&#21709;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The inherently diverse and uncertain nature of trajectories presents a formidable challenge in accurately modeling them. Motion prediction systems must effectively learn spatial and temporal information from the past to forecast the future trajectories of the agent. Many existing methods learn temporal motion via separate components within stacked models to capture temporal features. This paper introduces a novel framework, called Temporal Waypoint Dropping (TWD), that promotes explicit temporal learning through the waypoint dropping technique. Learning through waypoint dropping can compel the model to improve its understanding of temporal correlations among agents, thus leading to a significant enhancement in trajectory prediction. Trajectory prediction methods often operate under the assumption that observed trajectory waypoint sequences are complete, disregarding real-world scenarios where missing values may occur, which can influence their performance. Moreover, these models freque
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29616;&#26426;&#22120;&#23398;&#20064;&#20844;&#24179;&#24615;&#30340;&#31649;&#36947;&#24847;&#35782;&#26041;&#27861;&#65292;&#24182;&#24378;&#35843;&#38656;&#35201;&#25351;&#21335;&#21644;&#24037;&#20855;&#26469;&#22312;&#23454;&#36341;&#20013;&#24212;&#29992;&#36825;&#31181;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.17337</link><description>&lt;p&gt;
&#21521;&#23454;&#29616;&#31649;&#36947;&#24847;&#35782;&#30340;&#26426;&#22120;&#23398;&#20064;&#20844;&#24179;&#24615;&#36808;&#36827;&#65306;&#24320;&#21457;&#23454;&#38469;&#25351;&#21335;&#21644;&#24037;&#20855;&#30340;&#30740;&#31350;&#35758;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Toward Operationalizing Pipeline-aware ML Fairness: A Research Agenda for Developing Practical Guidelines and Tools. (arXiv:2309.17337v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17337
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29616;&#26426;&#22120;&#23398;&#20064;&#20844;&#24179;&#24615;&#30340;&#31649;&#36947;&#24847;&#35782;&#26041;&#27861;&#65292;&#24182;&#24378;&#35843;&#38656;&#35201;&#25351;&#21335;&#21644;&#24037;&#20855;&#26469;&#22312;&#23454;&#36341;&#20013;&#24212;&#29992;&#36825;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#31639;&#27861;&#20844;&#24179;&#24615;&#26159;&#19968;&#20010;&#34028;&#21187;&#21457;&#23637;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#20294;&#22312;&#23454;&#36341;&#20013;&#65292;&#20943;&#23569;&#20559;&#35265;&#38382;&#39064;&#24120;&#24120;&#34987;&#31616;&#21270;&#20026;&#36890;&#36807;&#24378;&#21046;&#25191;&#34892;&#20219;&#24847;&#36873;&#25321;&#30340;&#20844;&#24179;&#24615;&#24230;&#37327;&#26631;&#20934;&#26469;&#23454;&#29616;&#65292;&#35201;&#20040;&#26159;&#22312;&#20248;&#21270;&#38454;&#27573;&#24378;&#21046;&#25191;&#34892;&#20844;&#24179;&#24615;&#32422;&#26463;&#65292;&#35201;&#20040;&#26159;&#22312;&#21518;&#22788;&#29702;&#27169;&#22411;&#36755;&#20986;&#26102;&#65292;&#25110;&#32773;&#36890;&#36807;&#25805;&#32437;&#35757;&#32451;&#25968;&#25454;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#21628;&#21505;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#37319;&#21462;&#26356;&#20840;&#38754;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#31995;&#32479;&#22320;&#30740;&#31350;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#20013;&#25152;&#20570;&#30340;&#35768;&#22810;&#35774;&#35745;&#36873;&#25321;&#65292;&#24182;&#30830;&#23450;&#38024;&#23545;&#38382;&#39064;&#26681;&#26412;&#21407;&#22240;&#32780;&#19981;&#26159;&#20854;&#30151;&#29366;&#30340;&#24178;&#39044;&#25514;&#26045;&#12290;&#23613;&#31649;&#25105;&#20204;&#36190;&#21516;&#36825;&#31181;&#22522;&#20110;&#31649;&#36947;&#30340;&#26041;&#27861;&#26159;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#35299;&#20915;&#31639;&#27861;&#19981;&#20844;&#24179;&#24615;&#26368;&#21512;&#36866;&#30340;&#26041;&#27861;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;&#30446;&#21069;&#20960;&#20046;&#27809;&#26377;&#26041;&#27861;&#22312;&#23454;&#36341;&#20013;"&#23454;&#26045;"&#36825;&#31181;&#26041;&#27861;&#12290;&#26681;&#25454;&#25105;&#20204;&#20316;&#20026;&#25945;&#32946;&#32773;&#21644;&#23454;&#36341;&#32773;&#30340;&#32463;&#39564;&#65292;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#27809;&#26377;&#28165;&#26224;&#30340;&#25351;&#21335;&#21644;&#24037;&#20855;&#21253;&#65292;&#21363;&#20351;&#25317;&#26377;&#19987;&#19994;&#30340;&#26426;&#22120;&#23398;&#20064;&#30693;&#35782;&#30340;&#20010;&#20154;&#20063;&#20250;&#36935;&#21040;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
While algorithmic fairness is a thriving area of research, in practice, mitigating issues of bias often gets reduced to enforcing an arbitrarily chosen fairness metric, either by enforcing fairness constraints during the optimization step, post-processing model outputs, or by manipulating the training data. Recent work has called on the ML community to take a more holistic approach to tackle fairness issues by systematically investigating the many design choices made through the ML pipeline, and identifying interventions that target the issue's root cause, as opposed to its symptoms. While we share the conviction that this pipeline-based approach is the most appropriate for combating algorithmic unfairness on the ground, we believe there are currently very few methods of \emph{operationalizing} this approach in practice. Drawing on our experience as educators and practitioners, we first demonstrate that without clear guidelines and toolkits, even individuals with specialized ML knowled
&lt;/p&gt;</description></item><item><title>&#24322;&#27493;&#22270;&#29983;&#25104;&#22120;&#65288;AGG&#65289;&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#33410;&#28857;&#29983;&#25104;&#36827;&#34892;&#25968;&#25454;&#25554;&#34917;&#65292;&#24182;&#38544;&#24335;&#23398;&#20064;&#20256;&#24863;&#22120;&#27979;&#37327;&#30340;&#22240;&#26524;&#22270;&#34920;&#31034;&#65292;&#21462;&#24471;&#20102;state-of-the-art&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.17335</link><description>&lt;p&gt;
&#24322;&#27493;&#22270;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
Asynchronous Graph Generators. (arXiv:2309.17335v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17335
&lt;/p&gt;
&lt;p&gt;
&#24322;&#27493;&#22270;&#29983;&#25104;&#22120;&#65288;AGG&#65289;&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#33410;&#28857;&#29983;&#25104;&#36827;&#34892;&#25968;&#25454;&#25554;&#34917;&#65292;&#24182;&#38544;&#24335;&#23398;&#20064;&#20256;&#24863;&#22120;&#27979;&#37327;&#30340;&#22240;&#26524;&#22270;&#34920;&#31034;&#65292;&#21462;&#24471;&#20102;state-of-the-art&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#24322;&#27493;&#22270;&#29983;&#25104;&#22120;&#65288;AGG&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#22810;&#36890;&#36947;&#26102;&#38388;&#24207;&#21015;&#30340;&#26032;&#22411;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;AGG&#23558;&#35266;&#27979;&#20540;&#24314;&#27169;&#20026;&#21160;&#24577;&#22270;&#19978;&#30340;&#33410;&#28857;&#65292;&#24182;&#36890;&#36807;&#36716;&#23548;&#24335;&#33410;&#28857;&#29983;&#25104;&#36827;&#34892;&#25968;&#25454;&#25554;&#34917;&#12290;AGG&#19981;&#20381;&#36182;&#20110;&#24490;&#29615;&#32452;&#20214;&#25110;&#23545;&#26102;&#38388;&#35268;&#24459;&#30340;&#20551;&#35774;&#65292;&#20351;&#29992;&#21487;&#23398;&#20064;&#30340;&#23884;&#20837;&#23558;&#27979;&#37327;&#20540;&#12289;&#26102;&#38388;&#25139;&#21644;&#20803;&#25968;&#25454;&#30452;&#25509;&#34920;&#31034;&#22312;&#33410;&#28857;&#20013;&#65292;&#24182;&#21033;&#29992;&#27880;&#24847;&#26426;&#21046;&#26469;&#23398;&#20064;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#36825;&#26679;&#65292;&#25152;&#25552;&#20986;&#30340;&#26550;&#26500;&#38544;&#24335;&#22320;&#23398;&#20064;&#20256;&#24863;&#22120;&#27979;&#37327;&#30340;&#22240;&#26524;&#22270;&#34920;&#31034;&#65292;&#21487;&#20197;&#22522;&#20110;&#26410;&#35265;&#26102;&#38388;&#25139;&#21644;&#20803;&#25968;&#25454;&#23545;&#26032;&#30340;&#27979;&#37327;&#36827;&#34892;&#39044;&#27979;&#12290;&#25105;&#20204;&#23558;&#25152;&#25552;&#20986;&#30340;AGG&#22312;&#27010;&#24565;&#21644;&#23454;&#35777;&#20004;&#26041;&#38754;&#19982;&#20043;&#21069;&#30340;&#24037;&#20316;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#31616;&#35201;&#35752;&#35770;&#20102;&#25968;&#25454;&#22686;&#24378;&#23545;AGG&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;AGG&#22312;t
&lt;/p&gt;
&lt;p&gt;
We introduce the asynchronous graph generator (AGG), a novel graph neural network architecture for multi-channel time series which models observations as nodes on a dynamic graph and can thus perform data imputation by transductive node generation. Completely free from recurrent components or assumptions about temporal regularity, AGG represents measurements, timestamps and metadata directly in the nodes via learnable embeddings, to then leverage attention to learn expressive relationships across the variables of interest. This way, the proposed architecture implicitly learns a causal graph representation of sensor measurements which can be conditioned on unseen timestamps and metadata to predict new measurements by an expansion of the learnt graph. The proposed AGG is compared both conceptually and empirically to previous work, and the impact of data augmentation on the performance of AGG is also briefly discussed. Our experiments reveal that AGG achieved state-of-the-art results in t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#38544;&#24335;&#28857;&#22270;&#32593;&#32476;&#39640;&#25928;&#35299;&#21078;&#26631;&#35760;&#32954;&#37096;&#26641;&#29366;&#32467;&#26500;&#30340;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;SOTA&#20934;&#30830;&#24230;&#21644;&#21487;&#29992;&#30340;&#34920;&#38754;&#65292;&#21516;&#26102;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#35813;&#26041;&#27861;&#30340;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2309.17329</link><description>&lt;p&gt;
&#36890;&#36807;&#38544;&#24335;&#28857;&#22270;&#32593;&#32476;&#39640;&#25928;&#35299;&#21078;&#26631;&#35760;&#32954;&#37096;&#26641;&#29366;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Efficient Anatomical labeling of Pulmonary Tree Structures via Implicit Point-Graph Networks. (arXiv:2309.17329v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17329
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#38544;&#24335;&#28857;&#22270;&#32593;&#32476;&#39640;&#25928;&#35299;&#21078;&#26631;&#35760;&#32954;&#37096;&#26641;&#29366;&#32467;&#26500;&#30340;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;SOTA&#20934;&#30830;&#24230;&#21644;&#21487;&#29992;&#30340;&#34920;&#38754;&#65292;&#21516;&#26102;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#35813;&#26041;&#27861;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32954;&#37096;&#30142;&#30149;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#26159;&#23548;&#33268;&#27515;&#20129;&#30340;&#20027;&#35201;&#21407;&#22240;&#20043;&#19968;&#12290;&#27835;&#24840;&#32954;&#37096;&#30142;&#30149;&#38656;&#35201;&#26356;&#22909;&#22320;&#29702;&#35299;&#32954;&#37096;&#31995;&#32479;&#20869;&#30340;&#35768;&#22810;&#22797;&#26434;&#30340;3D&#26641;&#29366;&#32467;&#26500;&#65292;&#22914;&#27668;&#36947;&#12289;&#21160;&#33033;&#21644;&#38745;&#33033;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;&#23427;&#20204;&#21487;&#20197;&#36890;&#36807;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#22534;&#26632;&#36827;&#34892;&#24314;&#27169;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#23494;&#38598;&#20307;&#32032;&#32593;&#26684;&#30340;&#26631;&#20934;CNN&#26041;&#27861;&#20195;&#20215;&#36807;&#39640;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#28857;&#30340;&#26041;&#27861;&#65292;&#20445;&#30041;&#20102;&#26641;&#39592;&#26550;&#30340;&#22270;&#36830;&#36890;&#24615;&#65292;&#24182;&#32467;&#21512;&#20102;&#38544;&#24335;&#34920;&#38754;&#34920;&#31034;&#12290;&#23427;&#20197;&#36739;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#25552;&#20379;&#20102;SOTA&#20934;&#30830;&#24230;&#65292;&#29983;&#25104;&#30340;&#27169;&#22411;&#20855;&#26377;&#21487;&#29992;&#30340;&#34920;&#38754;&#12290;&#30001;&#20110;&#20844;&#24320;&#21487;&#35775;&#38382;&#30340;&#25968;&#25454;&#31232;&#32570;&#65292;&#25105;&#20204;&#36824;&#25972;&#29702;&#20102;&#19968;&#22871;&#24191;&#27867;&#30340;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#20844;&#24320;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pulmonary diseases rank prominently among the principal causes of death worldwide. Curing them will require, among other things, a better understanding of the many complex 3D tree-shaped structures within the pulmonary system, such as airways, arteries, and veins. In theory, they can be modeled using high-resolution image stacks. Unfortunately, standard CNN approaches operating on dense voxel grids are prohibitively expensive. To remedy this, we introduce a point-based approach that preserves graph connectivity of tree skeleton and incorporates an implicit surface representation. It delivers SOTA accuracy at a low computational cost and the resulting models have usable surfaces. Due to the scarcity of publicly accessible data, we have also curated an extensive dataset to evaluate our approach and will make it public.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22522;&#20110;GPT&#24773;&#24863;&#20998;&#26512;&#30340;&#36130;&#32463;&#26032;&#38395;&#26631;&#39064;&#30340;&#20132;&#26131;&#31574;&#30053;&#65292;&#35780;&#20272;&#20102;&#32929;&#31080;&#22238;&#25253;&#39044;&#27979;&#20013;&#30340;&#21069;&#30651;&#24615;&#20559;&#24046;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#21435;&#38500;&#30456;&#20851;&#20844;&#21496;&#26631;&#35782;&#31526;&#30340;&#21435;&#20559;&#31574;&#30053;&#19979;&#65292;&#21311;&#21517;&#21270;&#30340;&#26631;&#39064;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#20132;&#26131;&#32489;&#25928;&#12290;</title><link>http://arxiv.org/abs/2309.17322</link><description>&lt;p&gt;
&#36890;&#36807;GPT&#24773;&#24863;&#20998;&#26512;&#35780;&#20272;&#32929;&#31080;&#22238;&#25253;&#39044;&#27979;&#20013;&#30340;&#21069;&#30651;&#24615;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Assessing Look-Ahead Bias in Stock Return Predictions Generated By GPT Sentiment Analysis. (arXiv:2309.17322v1 [q-fin.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17322
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22522;&#20110;GPT&#24773;&#24863;&#20998;&#26512;&#30340;&#36130;&#32463;&#26032;&#38395;&#26631;&#39064;&#30340;&#20132;&#26131;&#31574;&#30053;&#65292;&#35780;&#20272;&#20102;&#32929;&#31080;&#22238;&#25253;&#39044;&#27979;&#20013;&#30340;&#21069;&#30651;&#24615;&#20559;&#24046;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#21435;&#38500;&#30456;&#20851;&#20844;&#21496;&#26631;&#35782;&#31526;&#30340;&#21435;&#20559;&#31574;&#30053;&#19979;&#65292;&#21311;&#21517;&#21270;&#30340;&#26631;&#39064;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#20132;&#26131;&#32489;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#21253;&#25324;ChatGPT&#65292;&#21487;&#20197;&#20174;&#26032;&#38395;&#25991;&#26412;&#30340;&#24773;&#32490;&#20013;&#25552;&#21462;&#26377;&#30410;&#30340;&#20132;&#26131;&#20449;&#21495;&#12290;&#28982;&#32780;&#65292;&#23545;&#36825;&#20123;&#31574;&#30053;&#36827;&#34892;&#22238;&#28335;&#27979;&#35797;&#26159;&#19968;&#39033;&#25361;&#25112;&#65292;&#22240;&#20026;LLMs&#26159;&#22312;&#22810;&#24180;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#22914;&#26524;&#35757;&#32451;&#21644;&#22238;&#28335;&#27979;&#35797;&#26399;&#38388;&#37325;&#21472;&#65292;&#22238;&#28335;&#27979;&#35797;&#23558;&#20135;&#29983;&#20559;&#20506;&#30340;&#32467;&#26524;&#12290;&#36825;&#31181;&#20559;&#24046;&#21487;&#20197;&#26377;&#20004;&#31181;&#24418;&#24335;&#65306;&#19968;&#31181;&#26159;&#21069;&#30651;&#24615;&#20559;&#24046;&#65292;&#21363;LLM&#21487;&#33021;&#20855;&#26377;&#20851;&#20110;&#26032;&#38395;&#25991;&#31456;&#20043;&#21518;&#30340;&#32929;&#31080;&#22238;&#25253;&#30340;&#20855;&#20307;&#30693;&#35782;&#65307;&#21478;&#19968;&#31181;&#26159;&#24178;&#25200;&#25928;&#24212;&#65292;&#21363;&#25152;&#36848;&#20844;&#21496;&#30340;&#19968;&#33324;&#30693;&#35782;&#24178;&#25200;&#20102;&#25991;&#26412;&#24773;&#24863;&#30340;&#27979;&#37327;&#12290;&#25105;&#20204;&#36890;&#36807;&#22522;&#20110;&#36130;&#32463;&#26032;&#38395;&#26631;&#39064;&#24773;&#24863;&#30340;&#20132;&#26131;&#31574;&#30053;&#26469;&#35843;&#26597;&#36825;&#20123;&#20559;&#24046;&#26469;&#28304;&#12290;&#25105;&#20204;&#23558;&#22522;&#20110;&#21407;&#22987;&#26631;&#39064;&#30340;&#20132;&#26131;&#32489;&#25928;&#19982;&#21435;&#38500;&#30456;&#20851;&#20844;&#21496;&#26631;&#35782;&#31526;&#30340;&#21435;&#20559;&#31574;&#30053;&#36827;&#34892;&#27604;&#36739;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#22312;&#26679;&#26412;&#20869;&#65288;LLM&#35757;&#32451;&#31383;&#21475;&#20869;&#65289;&#65292;&#25105;&#20204;&#21457;&#29616;&#21311;&#21517;&#21270;&#26631;&#39064;&#30340;&#32489;&#25928;&#20248;&#20110;&#21407;&#22987;&#26631;&#39064;&#65292;&#36825;&#34920;&#26126;&#24178;&#25200;&#25928;&#24212;&#30340;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), including ChatGPT, can extract profitable trading signals from the sentiment in news text. However, backtesting such strategies poses a challenge because LLMs are trained on many years of data, and backtesting produces biased results if the training and backtesting periods overlap. This bias can take two forms: a look-ahead bias, in which the LLM may have specific knowledge of the stock returns that followed a news article, and a distraction effect, in which general knowledge of the companies named interferes with the measurement of a text's sentiment. We investigate these sources of bias through trading strategies driven by the sentiment of financial news headlines. We compare trading performance based on the original headlines with de-biased strategies in which we remove the relevant company's identifiers from the text. In-sample (within the LLM training window), we find, surprisingly, that the anonymized headlines outperform, indicating that the distrac
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#22320;&#29702;&#31354;&#38388;&#20154;&#24037;&#26234;&#33021;&#22522;&#30784;&#27169;&#22411;&#30340;&#38544;&#31169;&#21644;&#23433;&#20840;&#39118;&#38505;&#65292;&#24182;&#25552;&#20986;&#20102;&#39044;&#38450;&#21644;&#25511;&#21046;&#31574;&#30053;&#34013;&#22270;&#12290;</title><link>http://arxiv.org/abs/2309.17319</link><description>&lt;p&gt;
&#26500;&#24314;&#20445;&#25252;&#38544;&#31169;&#21644;&#23433;&#20840;&#30340;&#22320;&#29702;&#31354;&#38388;&#20154;&#24037;&#26234;&#33021;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Building Privacy-Preserving and Secure Geospatial Artificial Intelligence Foundation Models. (arXiv:2309.17319v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17319
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#22320;&#29702;&#31354;&#38388;&#20154;&#24037;&#26234;&#33021;&#22522;&#30784;&#27169;&#22411;&#30340;&#38544;&#31169;&#21644;&#23433;&#20840;&#39118;&#38505;&#65292;&#24182;&#25552;&#20986;&#20102;&#39044;&#38450;&#21644;&#25511;&#21046;&#31574;&#30053;&#34013;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#25105;&#20204;&#22312;&#20154;&#24037;&#26234;&#33021;&#30340;&#22522;&#30784;&#27169;&#22411;&#20013;&#21462;&#24471;&#20102;&#23454;&#36136;&#24615;&#30340;&#36827;&#23637;&#65292;&#21253;&#25324;&#35821;&#35328;&#12289;&#35270;&#35273;&#21644;&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#31361;&#20986;&#20102;&#22312;&#22320;&#29702;&#31354;&#38388;&#20154;&#24037;&#26234;&#33021;&#20013;&#20351;&#29992;&#22522;&#30784;&#27169;&#22411;&#30340;&#28508;&#21147;&#65292;&#31216;&#20026;GeoAI&#22522;&#30784;&#27169;&#22411;&#25110;Geo-Foundation&#27169;&#22411;&#65292;&#29992;&#20110;&#22320;&#29702;&#38382;&#39064;&#22238;&#31572;&#12289;&#36965;&#24863;&#22270;&#20687;&#29702;&#35299;&#12289;&#22320;&#22270;&#29983;&#25104;&#21644;&#22522;&#20110;&#20301;&#32622;&#30340;&#26381;&#21153;&#31561;&#12290;&#28982;&#32780;&#65292;GeoAI&#22522;&#30784;&#27169;&#22411;&#30340;&#24320;&#21457;&#21644;&#24212;&#29992;&#21487;&#33021;&#24102;&#26469;&#20005;&#37325;&#30340;&#38544;&#31169;&#21644;&#23433;&#20840;&#39118;&#38505;&#65292;&#36825;&#20123;&#39118;&#38505;&#21040;&#30446;&#21069;&#20026;&#27490;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#30340;&#35752;&#35770;&#25110;&#35299;&#20915;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;GeoAI&#22522;&#30784;&#27169;&#22411;&#22312;&#25972;&#20010;&#29983;&#21629;&#21608;&#26399;&#20013;&#30340;&#28508;&#22312;&#38544;&#31169;&#21644;&#23433;&#20840;&#39118;&#38505;&#65292;&#24182;&#25552;&#20986;&#20102;&#20840;&#38754;&#30340;&#39044;&#38450;&#21644;&#25511;&#21046;&#31574;&#30053;&#34013;&#22270;&#12290;&#36890;&#36807;&#36825;&#31687;&#23637;&#26395;&#24615;&#35770;&#25991;&#65292;&#25105;&#20204;&#24076;&#26395;&#24341;&#36215;&#22320;&#29702;&#31354;&#38388;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#21644;&#20915;&#31574;&#32773;&#23545;GeoAI&#22522;&#30784;&#27169;&#22411;&#20013;&#22266;&#26377;&#30340;&#38544;&#31169;&#21644;&#23433;&#20840;&#39118;&#38505;&#30340;&#20851;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years we have seen substantial advances in foundation models for artificial intelligence, including language, vision, and multimodal models. Recent studies have highlighted the potential of using foundation models in geospatial artificial intelligence, known as GeoAI Foundation Models or Geo-Foundation Models, for geographic question answering, remote sensing image understanding, map generation, and location-based services, among others. However, the development and application of GeoAI foundation models can pose serious privacy and security risks, which have not been fully discussed or addressed to date. This paper introduces the potential privacy and security risks throughout the lifecycle of GeoAI foundation models and proposes a comprehensive blueprint for preventative and control strategies. Through this vision paper, we hope to draw the attention of researchers and policymakers in geospatial domains to these privacy and security risks inherent in GeoAI foundation models
&lt;/p&gt;</description></item><item><title>AutoAgents&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#23427;&#33021;&#22815;&#26681;&#25454;&#19981;&#21516;&#30340;&#20219;&#21153;&#33258;&#21160;&#29983;&#25104;&#21644;&#21327;&#35843;&#22810;&#20010;&#19987;&#19994;&#20195;&#29702;&#65292;&#20197;&#26500;&#24314;&#19968;&#20010;&#36866;&#24212;&#19981;&#21516;&#20219;&#21153;&#30340;AI&#22242;&#38431;&#65292;&#25552;&#39640;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#30340;&#36866;&#24212;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.17288</link><description>&lt;p&gt;
AutoAgents: &#19968;&#20010;&#33258;&#21160;&#29983;&#25104;&#26234;&#33021;&#20195;&#29702;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
AutoAgents: A Framework for Automatic Agent Generation. (arXiv:2309.17288v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17288
&lt;/p&gt;
&lt;p&gt;
AutoAgents&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#23427;&#33021;&#22815;&#26681;&#25454;&#19981;&#21516;&#30340;&#20219;&#21153;&#33258;&#21160;&#29983;&#25104;&#21644;&#21327;&#35843;&#22810;&#20010;&#19987;&#19994;&#20195;&#29702;&#65292;&#20197;&#26500;&#24314;&#19968;&#20010;&#36866;&#24212;&#19981;&#21516;&#20219;&#21153;&#30340;AI&#22242;&#38431;&#65292;&#25552;&#39640;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#30340;&#36866;&#24212;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#33258;&#21160;&#21270;&#20219;&#21153;&#35299;&#20915;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#22522;&#20110;LLM&#30340;&#22810;&#26234;&#33021;&#20307;&#26041;&#27861;&#20381;&#36182;&#20110;&#39044;&#23450;&#20041;&#30340;&#20195;&#29702;&#26469;&#22788;&#29702;&#31616;&#21333;&#20219;&#21153;&#65292;&#38480;&#21046;&#20102;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#22312;&#19981;&#21516;&#22330;&#26223;&#20013;&#30340;&#36866;&#24212;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;AutoAgents&#65292;&#19968;&#20010;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#26681;&#25454;&#19981;&#21516;&#20219;&#21153;&#33258;&#36866;&#24212;&#29983;&#25104;&#21644;&#21327;&#35843;&#22810;&#20010;&#19987;&#19994;&#20195;&#29702;&#20197;&#26500;&#24314;AI&#22242;&#38431;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;AutoAgents&#36890;&#36807;&#21160;&#24577;&#29983;&#25104;&#22810;&#20010;&#25152;&#38656;&#20195;&#29702;&#26469;&#32806;&#21512;&#20219;&#21153;&#21644;&#35282;&#33394;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#19988;&#26681;&#25454;&#29983;&#25104;&#30340;&#19987;&#23478;&#20195;&#29702;&#20026;&#24403;&#21069;&#20219;&#21153;&#35268;&#21010;&#35299;&#20915;&#26041;&#26696;&#12290;&#22810;&#20010;&#19987;&#19994;&#20195;&#29702;&#30456;&#20114;&#21327;&#20316;&#20197;&#39640;&#25928;&#23436;&#25104;&#20219;&#21153;&#12290;&#21516;&#26102;&#65292;&#35266;&#23519;&#32773;&#35282;&#33394;&#34987;&#32435;&#20837;&#26694;&#26550;&#20013;&#20197;&#21453;&#24605;&#25351;&#23450;&#30340;&#35745;&#21010;&#21644;&#20195;&#29702;&#30340;&#21709;&#24212;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#25913;&#36827;&#12290;&#25105;&#20204;&#23545;&#22810;&#31181;&#22522;&#20934;&#27979;&#35797;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;AutoAgents&#33021;&#22815;&#26681;&#25454;&#19981;&#21516;&#30340;&#20219;&#21153;&#36866;&#24212;&#24615;&#29983;&#25104;&#21644;&#21327;&#35843;&#22810;&#20010;&#19987;&#19994;&#20195;&#29702;&#65292;&#23454;&#29616;&#39640;&#25928;&#30340;&#20219;&#21153;&#23436;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have enabled remarkable advances in automated task-solving with multi-agent systems. However, most existing LLM-based multi-agent approaches rely on predefined agents to handle simple tasks, limiting the adaptability of multi-agent collaboration to different scenarios. Therefore, we introduce AutoAgents, an innovative framework that adaptively generates and coordinates multiple specialized agents to build an AI team according to different tasks. Specifically, AutoAgents couples the relationship between tasks and roles by dynamically generating multiple required agents based on task content and planning solutions for the current task based on the generated expert agents. Multiple specialized agents collaborate with each other to efficiently accomplish tasks. Concurrently, an observer role is incorporated into the framework to reflect on the designated plans and agents' responses and improve upon them. Our experiments on various benchmarks demonstrate that Au
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#21487;&#25511;&#30340;&#27861;&#24459;&#24847;&#35265;&#25688;&#35201;&#29983;&#25104;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#39044;&#27979;&#30340;&#35770;&#35777;&#35282;&#33394;&#20449;&#24687;&#26469;&#25351;&#23548;&#27169;&#22411;&#29983;&#25104;&#36830;&#36143;&#25688;&#35201;&#65292;&#20855;&#26377;&#20248;&#20110;&#24378;&#22522;&#20934;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.17280</link><description>&lt;p&gt;
STRONG - &#32467;&#26500;&#21487;&#25511;&#30340;&#27861;&#24459;&#24847;&#35265;&#25688;&#35201;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
STRONG -- Structure Controllable Legal Opinion Summary Generation. (arXiv:2309.17280v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17280
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#21487;&#25511;&#30340;&#27861;&#24459;&#24847;&#35265;&#25688;&#35201;&#29983;&#25104;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#39044;&#27979;&#30340;&#35770;&#35777;&#35282;&#33394;&#20449;&#24687;&#26469;&#25351;&#23548;&#27169;&#22411;&#29983;&#25104;&#36830;&#36143;&#25688;&#35201;&#65292;&#20855;&#26377;&#20248;&#20110;&#24378;&#22522;&#20934;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#38271;&#31687;&#27861;&#24459;&#24847;&#35265;&#32467;&#26500;&#21487;&#25511;&#30340;&#25688;&#35201;&#29983;&#25104;&#26041;&#27861;&#65292;&#32771;&#34385;&#20102;&#25991;&#26723;&#30340;&#35770;&#35777;&#32467;&#26500;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#39044;&#27979;&#30340;&#35770;&#35777;&#35282;&#33394;&#20449;&#24687;&#26469;&#25351;&#23548;&#27169;&#22411;&#29983;&#25104;&#20855;&#26377;&#19968;&#23450;&#32467;&#26500;&#27169;&#24335;&#30340;&#36830;&#36143;&#25688;&#35201;&#12290;&#25105;&#20204;&#22312;&#27861;&#24459;&#24847;&#35265;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#35777;&#26126;&#20854;&#22312;ROUGE&#12289;BERTScore&#21644;&#32467;&#26500;&#30456;&#20284;&#24615;&#26041;&#38754;&#20248;&#20110;&#20960;&#20010;&#24378;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an approach for the structure controllable summarization of long legal opinions that considers the argument structure of the document. Our approach involves using predicted argument role information to guide the model in generating coherent summaries that follow a provided structure pattern. We demonstrate the effectiveness of our approach on a dataset of legal opinions and show that it outperforms several strong baselines with respect to ROUGE, BERTScore, and structure similarity.
&lt;/p&gt;</description></item><item><title>Suspicion-Agent&#26159;&#19968;&#31181;&#21019;&#26032;&#20195;&#29702;&#31243;&#24207;&#65292;&#21033;&#29992;&#20855;&#22791;&#39640;&#38454;&#24515;&#28789;&#29702;&#35770;&#24847;&#35782;&#30340;GPT4&#22312;&#19981;&#23436;&#20840;&#20449;&#24687;&#28216;&#25103;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#36866;&#24212;&#24615;&#21644;&#24433;&#21709;&#20182;&#20154;&#34892;&#20026;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.17277</link><description>&lt;p&gt;
Suspicion-Agent: &#20351;&#29992;&#20855;&#22791;&#24515;&#28789;&#29702;&#35770;&#24847;&#35782;&#30340;GPT4&#22312;&#19981;&#23436;&#20840;&#20449;&#24687;&#28216;&#25103;&#20013;&#36827;&#34892;&#23545;&#23616;
&lt;/p&gt;
&lt;p&gt;
Suspicion-Agent: Playing Imperfect Information Games with Theory of Mind Aware GPT4. (arXiv:2309.17277v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17277
&lt;/p&gt;
&lt;p&gt;
Suspicion-Agent&#26159;&#19968;&#31181;&#21019;&#26032;&#20195;&#29702;&#31243;&#24207;&#65292;&#21033;&#29992;&#20855;&#22791;&#39640;&#38454;&#24515;&#28789;&#29702;&#35770;&#24847;&#35782;&#30340;GPT4&#22312;&#19981;&#23436;&#20840;&#20449;&#24687;&#28216;&#25103;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#36866;&#24212;&#24615;&#21644;&#24433;&#21709;&#20182;&#20154;&#34892;&#20026;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#21516;&#20110;&#23436;&#20840;&#20449;&#24687;&#28216;&#25103;&#65292;&#20854;&#20013;&#27599;&#20010;&#29609;&#23478;&#37117;&#30693;&#36947;&#25152;&#26377;&#20803;&#32032;&#65292;&#19981;&#23436;&#20840;&#20449;&#24687;&#28216;&#25103;&#27169;&#25311;&#20102;&#22312;&#19981;&#30830;&#23450;&#25110;&#19981;&#23436;&#25972;&#20449;&#24687;&#19979;&#36827;&#34892;&#20915;&#31574;&#30340;&#29616;&#23454;&#19990;&#30028;&#22797;&#26434;&#24615;&#12290;&#26368;&#36817;&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;GPT-4&#20197;&#20854;&#30693;&#35782;&#26816;&#32034;&#21644;&#25512;&#29702;&#33021;&#21147;&#32780;&#38395;&#21517;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;GPT-4&#30340;&#23398;&#20064;&#30693;&#35782;&#24212;&#29992;&#20110;&#19981;&#23436;&#20840;&#20449;&#24687;&#28216;&#25103;&#30340;&#21487;&#34892;&#24615;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#20195;&#29702;&#31243;&#24207;\textbf{Suspicion-Agent}&#65292;&#35813;&#20195;&#29702;&#31243;&#24207;&#21033;&#29992;GPT-4&#30340;&#33021;&#21147;&#22312;&#19981;&#23436;&#20840;&#20449;&#24687;&#28216;&#25103;&#20013;&#36827;&#34892;&#23545;&#23616;&#12290;&#36890;&#36807;&#21512;&#36866;&#30340;&#25552;&#31034;&#24037;&#31243;&#26469;&#23454;&#29616;&#19981;&#21516;&#30340;&#21151;&#33021;&#65292;&#22522;&#20110;GPT-4&#30340;Suspicion-Agent&#23637;&#31034;&#20102;&#22312;&#19968;&#31995;&#21015;&#19981;&#23436;&#20840;&#20449;&#24687;&#32440;&#29260;&#28216;&#25103;&#20013;&#30340;&#26174;&#33879;&#36866;&#24212;&#33021;&#21147;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;GPT-4&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#39640;&#38454;&#24515;&#28789;&#29702;&#35770;&#65288;Theory of Mind&#65289;&#33021;&#21147;&#65292;&#36825;&#24847;&#21619;&#30528;&#23427;&#33021;&#22815;&#29702;&#35299;&#20182;&#20154;&#24182;&#26377;&#24847;&#35782;&#22320;&#24433;&#21709;&#20182;&#20154;&#30340;&#34892;&#20026;&#12290;&#21033;&#29992;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#35745;&#21010;
&lt;/p&gt;
&lt;p&gt;
Unlike perfect information games, where all elements are known to every player, imperfect information games emulate the real-world complexities of decision-making under uncertain or incomplete information. GPT-4, the recent breakthrough in large language models (LLMs) trained on massive passive data, is notable for its knowledge retrieval and reasoning abilities. This paper delves into the applicability of GPT-4's learned knowledge for imperfect information games. To achieve this, we introduce \textbf{Suspicion-Agent}, an innovative agent that leverages GPT-4's capabilities for performing in imperfect information games. With proper prompt engineering to achieve different functions, Suspicion-Agent based on GPT-4 demonstrates remarkable adaptability across a range of imperfect information card games. Importantly, GPT-4 displays a strong high-order theory of mind (ToM) capacity, meaning it can understand others and intentionally impact others' behavior. Leveraging this, we design a plann
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#22810;&#35282;&#24230;&#33258;&#19968;&#33268;&#24615;&#65288;MPSC&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#25552;&#21319;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#26434;&#30340;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#20174;&#22810;&#20010;&#35282;&#24230;&#37319;&#26679;&#22810;&#20010;&#36755;&#20986;&#24182;&#26500;&#24314;&#19968;&#20010;&#22810;&#37096;&#20998;&#22270;&#65292;&#21033;&#29992;&#20132;&#21449;&#19968;&#33268;&#24615;&#21644;&#20869;&#19968;&#33268;&#24615;&#20449;&#24687;&#26469;&#36873;&#25321;&#26368;&#20248;&#36755;&#20986;&#12290;</title><link>http://arxiv.org/abs/2309.17272</link><description>&lt;p&gt;
&#25552;&#21319;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#32534;&#30721;&#20013;&#30340;&#33021;&#21147;&#36890;&#36807;&#22810;&#35282;&#24230;&#33258;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
Enhancing Large Language Models in Coding Through Multi-Perspective Self-Consistency. (arXiv:2309.17272v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17272
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#22810;&#35282;&#24230;&#33258;&#19968;&#33268;&#24615;&#65288;MPSC&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#25552;&#21319;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#26434;&#30340;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#20174;&#22810;&#20010;&#35282;&#24230;&#37319;&#26679;&#22810;&#20010;&#36755;&#20986;&#24182;&#26500;&#24314;&#19968;&#20010;&#22810;&#37096;&#20998;&#22270;&#65292;&#21033;&#29992;&#20132;&#21449;&#19968;&#33268;&#24615;&#21644;&#20869;&#19968;&#33268;&#24615;&#20449;&#24687;&#26469;&#36873;&#25321;&#26368;&#20248;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25991;&#26412;&#29983;&#25104;&#26041;&#38754;&#23637;&#29616;&#20102;&#21331;&#36234;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#22797;&#26434;&#30340;&#25512;&#29702;&#20219;&#21153;&#65292;&#22914;&#20195;&#30721;&#29983;&#25104;&#20013;&#65292;LLMs&#20173;&#28982;&#38590;&#20197;&#22312;&#19968;&#27425;&#23581;&#35797;&#20013;&#29983;&#25104;&#27491;&#30830;&#30340;&#31572;&#26696;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#36807;&#32858;&#21512;&#22810;&#20010;&#36755;&#20986;&#65292;&#21033;&#29992;&#23427;&#20204;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#26469;&#25506;&#32034;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#27809;&#26377;&#20840;&#38754;&#22320;&#20174;&#19981;&#21516;&#30340;&#35282;&#24230;&#25429;&#25417;&#36825;&#31181;&#19968;&#33268;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#35282;&#24230;&#33258;&#19968;&#33268;&#24615;&#65288;MPSC&#65289;&#26694;&#26550;&#30340;&#26032;&#30340;&#35299;&#30721;&#31574;&#30053;&#65292;&#29992;&#20110;LLM&#65292;&#23427;&#23558;&#26469;&#33258;&#22810;&#20010;&#35282;&#24230;&#30340;&#36755;&#20986;&#20043;&#38388;&#30340;&#20132;&#21449;&#19968;&#33268;&#24615;&#21644;&#21333;&#20010;&#35282;&#24230;&#20869;&#30340;&#20869;&#19968;&#33268;&#24615;&#32467;&#21512;&#36215;&#26469;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35201;&#27714;LLMs&#23545;&#32473;&#23450;&#26597;&#35810;&#20174;&#21508;&#20010;&#35282;&#24230;&#37319;&#26679;&#22810;&#20010;&#22810;&#26679;&#21270;&#30340;&#36755;&#20986;&#65292;&#24182;&#22522;&#20110;&#23427;&#20204;&#26500;&#24314;&#19968;&#20010;&#22810;&#37096;&#20998;&#22270;&#12290;&#36890;&#36807;&#20004;&#20010;&#39044;&#23450;&#20041;&#30340;&#19968;&#33268;&#24615;&#24230;&#37327;&#65292;&#25105;&#20204;&#23558;&#20132;&#21449;&#19968;&#33268;&#24615;&#21644;&#20869;&#19968;&#33268;&#24615;&#20449;&#24687;&#23884;&#20837;&#21040;&#22270;&#20013;&#12290;&#26368;&#20339;&#36873;&#25321;&#26159;&#26681;&#25454;&#36825;&#20123;&#19968;&#33268;&#24615;&#24230;&#37327;&#26469;&#36873;&#25321;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have exhibited remarkable ability in textual generation. However, in complex reasoning tasks such as code generation, generating the correct answer in a single attempt remains a formidable challenge for LLMs. Previous research has explored solutions by aggregating multiple outputs, leveraging the consistency among them. However, none of them have comprehensively captured this consistency from different perspectives. In this paper, we propose the Multi-Perspective Self-Consistency (MPSC) framework, a novel decoding strategy for LLM that incorporates both inter-consistency across outputs from multiple perspectives and intra-consistency within a single perspective. Specifically, we ask LLMs to sample multiple diverse outputs from various perspectives for a given query and then construct a multipartite graph based on them. With two predefined measures of consistency, we embed both inter- and intra-consistency information into the graph. The optimal choice is th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20013;&#31227;&#21160;&#30446;&#26631;&#20998;&#21106;&#30340;&#22522;&#30784;&#27169;&#22411;iMOS&#65292;&#36890;&#36807;&#23545;&#24207;&#21015;&#20013;&#21482;&#26377;&#23569;&#37327;&#22270;&#20687;&#36827;&#34892;&#27880;&#37322;&#65292;&#21363;&#21487;&#23454;&#29616;&#39640;&#31934;&#24230;&#30340;&#20998;&#21106;&#25928;&#26524;</title><link>http://arxiv.org/abs/2309.17264</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20013;&#19968;&#33324;&#31227;&#21160;&#30446;&#26631;&#20998;&#21106;&#30340;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Foundation Model for General Moving Object Segmentation in Medical Images. (arXiv:2309.17264v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17264
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20013;&#31227;&#21160;&#30446;&#26631;&#20998;&#21106;&#30340;&#22522;&#30784;&#27169;&#22411;iMOS&#65292;&#36890;&#36807;&#23545;&#24207;&#21015;&#20013;&#21482;&#26377;&#23569;&#37327;&#22270;&#20687;&#36827;&#34892;&#27880;&#37322;&#65292;&#21363;&#21487;&#23454;&#29616;&#39640;&#31934;&#24230;&#30340;&#20998;&#21106;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#26088;&#22312;&#25551;&#32472;&#24863;&#20852;&#36259;&#30340;&#35299;&#21078;&#25110;&#30149;&#29702;&#32467;&#26500;&#65292;&#22312;&#20020;&#24202;&#35786;&#26029;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#26500;&#24314;&#39640;&#31934;&#24230;&#30340;&#28145;&#24230;&#20998;&#21106;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#39640;&#36136;&#37327;&#30340;&#27880;&#37322;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#21307;&#23398;&#27880;&#37322;&#38750;&#24120;&#32321;&#29712;&#32791;&#26102;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#21307;&#23398;&#35270;&#39057;&#25110;3D&#20307;&#31215;&#65292;&#30001;&#20110;&#24040;&#22823;&#30340;&#26631;&#31614;&#31354;&#38388;&#21644;&#24046;&#30340;&#24103;&#38388;&#19968;&#33268;&#24615;&#12290;&#26368;&#36817;&#65292;&#22312;&#33258;&#28982;&#22270;&#20687;&#20013;&#65292;&#19968;&#20010;&#21517;&#20026;Moving Object Segmentation (MOS)&#30340;&#22522;&#26412;&#20219;&#21153;&#22312;&#25216;&#26415;&#19978;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#23427;&#30340;&#30446;&#26631;&#26159;&#22312;&#22270;&#20687;&#24207;&#21015;&#20013;&#20174;&#32972;&#26223;&#20013;&#25551;&#32472;&#31227;&#21160;&#29289;&#20307;&#65292;&#21482;&#38656;&#35201;&#26368;&#23567;&#30340;&#27880;&#37322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20013;MOS&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#21517;&#20026;iMOS&#12290;&#23545;&#19968;&#20010;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#21307;&#23398;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#30340;iMOS&#30340;&#26377;&#25928;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#21482;&#38656;&#23545;&#24207;&#21015;&#20013;&#23569;&#37327;&#30340;&#22270;&#20687;&#36827;&#34892;&#27880;&#37322;&#65292;iMOS&#23601;&#21487;&#20197;&#23454;&#29616;&#20102;
&lt;/p&gt;
&lt;p&gt;
Medical image segmentation aims to delineate the anatomical or pathological structures of interest, playing a crucial role in clinical diagnosis. A substantial amount of high-quality annotated data is crucial for constructing high-precision deep segmentation models. However, medical annotation is highly cumbersome and time-consuming, especially for medical videos or 3D volumes, due to the huge labeling space and poor inter-frame consistency. Recently, a fundamental task named Moving Object Segmentation (MOS) has made significant advancements in natural images. Its objective is to delineate moving objects from the background within image sequences, requiring only minimal annotations. In this paper, we propose the first foundation model, named iMOS, for MOS in medical images. Extensive experiments on a large multi-modal medical dataset validate the effectiveness of the proposed iMOS. Specifically, with the annotation of only a small number of images in the sequence, iMOS can achieve sati
&lt;/p&gt;</description></item><item><title>PlaceNav&#26159;&#19968;&#31181;&#36890;&#36807;&#22320;&#28857;&#35782;&#21035;&#36827;&#34892;&#25299;&#25169;&#23548;&#33322;&#30340;&#26041;&#27861;&#65292;&#23558;&#26426;&#22120;&#20154;&#26080;&#20851;&#37096;&#20998;&#20998;&#20026;&#23548;&#33322;&#29305;&#23450;&#21644;&#36890;&#29992;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#32452;&#20214;&#65292;&#36890;&#36807;&#20351;&#29992;&#38750;&#26426;&#22120;&#20154;&#26469;&#28304;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#65292;&#21516;&#26102;&#36890;&#36807;&#22320;&#28857;&#35782;&#21035;&#26469;&#25552;&#39640;&#23548;&#33322;&#24615;&#33021;&#12290;&#26032;&#27169;&#22411;&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;76%&#12290;</title><link>http://arxiv.org/abs/2309.17260</link><description>&lt;p&gt;
PlaceNav: &#36890;&#36807;&#22320;&#28857;&#35782;&#21035;&#36827;&#34892;&#25299;&#25169;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
PlaceNav: Topological Navigation through Place Recognition. (arXiv:2309.17260v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17260
&lt;/p&gt;
&lt;p&gt;
PlaceNav&#26159;&#19968;&#31181;&#36890;&#36807;&#22320;&#28857;&#35782;&#21035;&#36827;&#34892;&#25299;&#25169;&#23548;&#33322;&#30340;&#26041;&#27861;&#65292;&#23558;&#26426;&#22120;&#20154;&#26080;&#20851;&#37096;&#20998;&#20998;&#20026;&#23548;&#33322;&#29305;&#23450;&#21644;&#36890;&#29992;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#32452;&#20214;&#65292;&#36890;&#36807;&#20351;&#29992;&#38750;&#26426;&#22120;&#20154;&#26469;&#28304;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#65292;&#21516;&#26102;&#36890;&#36807;&#22320;&#28857;&#35782;&#21035;&#26469;&#25552;&#39640;&#23548;&#33322;&#24615;&#33021;&#12290;&#26032;&#27169;&#22411;&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;76%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#25299;&#25169;&#23548;&#33322;&#20998;&#20026;&#26426;&#22120;&#20154;&#26080;&#20851;&#21644;&#26426;&#22120;&#20154;&#29305;&#23450;&#30340;&#32452;&#20214;&#21487;&#20197;&#25552;&#39640;&#23548;&#33322;&#24615;&#33021;&#65292;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#31867;&#22411;&#26426;&#22120;&#20154;&#25910;&#38598;&#30340;&#25968;&#25454;&#26469;&#35757;&#32451;&#26426;&#22120;&#20154;&#26080;&#20851;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#23548;&#33322;&#26041;&#27861;&#20173;&#21463;&#21040;&#36866;&#21512;&#35757;&#32451;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#21644;&#35745;&#31639;&#32553;&#25918;&#24615;&#24046;&#30340;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PlaceNav&#30340;&#26041;&#27861;&#65292;&#23558;&#26426;&#22120;&#20154;&#26080;&#20851;&#37096;&#20998;&#20998;&#20026;&#23548;&#33322;&#29305;&#23450;&#21644;&#36890;&#29992;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#32452;&#20214;&#12290;&#25105;&#20204;&#21033;&#29992;&#35270;&#35273;&#22320;&#28857;&#35782;&#21035;&#26469;&#36873;&#25321;&#25299;&#25169;&#23548;&#33322;&#27969;&#31243;&#20013;&#30340;&#23376;&#30446;&#26631;&#12290;&#36825;&#20351;&#24471;&#23376;&#30446;&#26631;&#36873;&#25321;&#26356;&#39640;&#25928;&#65292;&#24182;&#33021;&#22815;&#21033;&#29992;&#38750;&#26426;&#22120;&#20154;&#26469;&#28304;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#12290;&#22320;&#28857;&#35782;&#21035;&#20351;&#24471;&#36125;&#21494;&#26031;&#28388;&#27874;&#25104;&#20026;&#21487;&#33021;&#65292;&#36827;&#19968;&#27493;&#36890;&#36807;&#22686;&#21152;&#23376;&#30446;&#26631;&#30340;&#26102;&#38388;&#19968;&#33268;&#24615;&#26469;&#25552;&#39640;&#23548;&#33322;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#36825;&#19968;&#35774;&#35745;&#65292;&#24182;&#19988;&#26032;&#27169;&#22411;&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;76%&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent results suggest that splitting topological navigation into robot-independent and robot-specific components improves navigation performance by enabling the robot-independent part to be trained with data collected by different robot types. However, the navigation methods are still limited by the scarcity of suitable training data and suffer from poor computational scaling. In this work, we present~\methodname, subdividing the robot-independent part into navigation-specific and generic computer vision components. We utilize visual place recognition for the subgoal selection of the topological navigation pipeline. This makes subgoal selection more efficient and enables leveraging large-scale datasets from non-robotics sources, increasing training data availability. Bayes filtering, enabled by place recognition, further improves navigation performance by increasing the temporal consistency of subgoals. Our experimental results verify the design and the new model obtains a 76% higher 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#22312;&#29983;&#21629;&#31185;&#23398;&#39046;&#22495;&#20013;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#30340;&#26368;&#26032;&#21457;&#23637;&#21644;&#36827;&#23637;&#65292;&#24182;&#23637;&#26395;&#20102;&#36825;&#20123;&#25216;&#26415;&#22312;&#26410;&#26469;&#23545;&#36825;&#20123;&#39046;&#22495;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2309.17255</link><description>&lt;p&gt;
&#29983;&#21629;&#31185;&#23398;&#39046;&#22495;&#30340;&#30693;&#35782;&#22270;&#35889;&#65306;&#26368;&#26032;&#21457;&#23637;&#12289;&#25361;&#25112;&#21644;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graphs for the Life Sciences: Recent Developments, Challenges and Opportunities. (arXiv:2309.17255v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17255
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#22312;&#29983;&#21629;&#31185;&#23398;&#39046;&#22495;&#20013;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#30340;&#26368;&#26032;&#21457;&#23637;&#21644;&#36827;&#23637;&#65292;&#24182;&#23637;&#26395;&#20102;&#36825;&#20123;&#25216;&#26415;&#22312;&#26410;&#26469;&#23545;&#36825;&#20123;&#39046;&#22495;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#21629;&#31185;&#23398;&#26159;&#30740;&#31350;&#29983;&#29289;&#21644;&#29983;&#21629;&#36807;&#31243;&#30340;&#23398;&#31185;&#65292;&#21253;&#25324;&#21270;&#23398;&#12289;&#29983;&#29289;&#23398;&#12289;&#21307;&#23398;&#21644;&#19968;&#31995;&#21015;&#20854;&#20182;&#30456;&#20851;&#23398;&#31185;&#12290;&#29983;&#21629;&#31185;&#23398;&#30340;&#30740;&#31350;&#24037;&#20316;&#38750;&#24120;&#20381;&#36182;&#25968;&#25454;&#65292;&#22240;&#20026;&#23427;&#20204;&#20135;&#29983;&#21644;&#28040;&#36153;&#22823;&#37327;&#31185;&#23398;&#25968;&#25454;&#65292;&#20854;&#20013;&#24456;&#22810;&#25968;&#25454;&#20855;&#26377;&#20851;&#31995;&#21644;&#22270;&#32467;&#26500;&#12290;&#25968;&#25454;&#30340;&#25968;&#37327;&#21644;&#20854;&#20013;&#28041;&#21450;&#30340;&#31185;&#23398;&#27010;&#24565;&#21644;&#20851;&#31995;&#30340;&#22797;&#26434;&#24615;&#25512;&#21160;&#20102;&#24212;&#29992;&#20808;&#36827;&#30340;&#30693;&#35782;&#39537;&#21160;&#25216;&#26415;&#26469;&#31649;&#29702;&#21644;&#35299;&#37322;&#25968;&#25454;&#65292;&#26368;&#32456;&#30446;&#26631;&#26159;&#25512;&#21160;&#31185;&#23398;&#21457;&#29616;&#12290;&#22312;&#36825;&#31687;&#32508;&#36848;&#21644;&#35266;&#28857;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#30693;&#35782;&#22270;&#35889;&#22312;&#29983;&#21629;&#31185;&#23398;&#20013;&#30340;&#26368;&#26032;&#21457;&#23637;&#21644;&#36827;&#23637;&#65292;&#24182;&#23637;&#26395;&#20102;&#36825;&#20123;&#25216;&#26415;&#22312;&#26410;&#26469;&#23545;&#36825;&#20123;&#39046;&#22495;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#37325;&#28857;&#20851;&#27880;&#19977;&#20010;&#20027;&#39064;&#65306;&#30693;&#35782;&#22270;&#35889;&#30340;&#26500;&#24314;&#21644;&#31649;&#29702;&#65292;&#20197;&#21450;&#22312;&#26032;&#21457;&#29616;&#30340;&#36807;&#31243;&#20013;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#21644;&#30456;&#20851;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
The term life sciences refers to the disciplines that study living organisms and life processes, and include chemistry, biology, medicine, and a range of other related disciplines. Research efforts in life sciences are heavily data-driven, as they produce and consume vast amounts of scientific data, much of which is intrinsically relational and graph-structured.  The volume of data and the complexity of scientific concepts and relations referred to therein promote the application of advanced knowledge-driven technologies for managing and interpreting data, with the ultimate aim to advance scientific discovery.  In this survey and position paper, we discuss recent developments and advances in the use of graph-based technologies in life sciences and set out a vision for how these technologies will impact these fields into the future. We focus on three broad topics: the construction and management of Knowledge Graphs (KGs), the use of KGs and associated technologies in the discovery of ne
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#26862;&#26519;&#28151;&#21512;&#26041;&#27861;&#23545;&#26412;&#20307;&#23398;&#20064;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#20010;&#25628;&#32034;&#26641;&#21644;&#20849;&#20139;&#30340;&#32454;&#21270;&#27744;&#26469;&#25552;&#39640;&#31639;&#27861;&#24615;&#33021;&#65292;&#20197;&#20419;&#36827;&#23545;&#26412;&#20307;&#20013;&#31867;&#34920;&#36798;&#24335;&#30340;&#26597;&#25214;&#21644;&#25506;&#32034;&#12290;</title><link>http://arxiv.org/abs/2309.17252</link><description>&lt;p&gt;
&#26862;&#26519;&#28151;&#21512;&#65306;&#30740;&#31350;&#22810;&#20010;&#25628;&#32034;&#26641;&#21644;&#20849;&#20139;&#30340;&#32454;&#21270;&#27744;&#23545;&#26412;&#20307;&#23398;&#20064;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Forest Mixing: investigating the impact of multiple search trees and a shared refinements pool on ontology learning. (arXiv:2309.17252v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17252
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#26862;&#26519;&#28151;&#21512;&#26041;&#27861;&#23545;&#26412;&#20307;&#23398;&#20064;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#20010;&#25628;&#32034;&#26641;&#21644;&#20849;&#20139;&#30340;&#32454;&#21270;&#27744;&#26469;&#25552;&#39640;&#31639;&#27861;&#24615;&#33021;&#65292;&#20197;&#20419;&#36827;&#23545;&#26412;&#20307;&#20013;&#31867;&#34920;&#36798;&#24335;&#30340;&#26597;&#25214;&#21644;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#26088;&#22312;&#24320;&#21457;&#30333;&#30418;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;&#26412;&#25991;&#20027;&#35201;&#20851;&#27880;&#23398;&#20064;&#25551;&#36848;&#36923;&#36753;&#20013;&#20844;&#29702;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;DL-Learner&#24037;&#20855;&#20013;&#30340;Class Expression Learning for Ontology Engineering (CELOE)&#31639;&#27861;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#22810;&#20010;&#25628;&#32034;&#26641;&#21644;&#19968;&#20010;&#20849;&#20139;&#30340;&#32454;&#21270;&#27744;&#65292;&#20197;&#23558;&#25628;&#32034;&#31354;&#38388;&#21010;&#20998;&#20026;&#36739;&#23567;&#30340;&#23376;&#31354;&#38388;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#26469;&#33258;&#27599;&#26869;&#26641;&#30340;&#26368;&#20339;&#31867;&#34920;&#36798;&#24335;&#30340;&#21512;&#21462;&#25805;&#20316;&#65292;&#20445;&#30041;&#32473;&#20986;&#26368;&#22810;&#20449;&#24687;&#30340;&#32467;&#26524;&#12290;&#20854;&#30446;&#30340;&#26159;&#20419;&#36827;&#20174;&#19981;&#21516;&#30340;&#36215;&#22987;&#31867;&#36827;&#34892;&#25506;&#32034;&#65292;&#24182;&#31616;&#21270;&#26412;&#20307;&#20013;&#26597;&#25214;&#31867;&#34920;&#36798;&#24335;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
We aim at development white-box machine learning algorithms. We focus here on algorithms for learning axioms in description logic. We extend the Class Expression Learning for Ontology Engineering (CELOE) algorithm contained in the DL-Learner tool. The approach uses multiple search trees and a shared pool of refinements in order to split the search space in smaller subspaces. We introduce the conjunction operation of best class expressions from each tree, keeping the results which give the most information. The aim is to foster exploration from a diverse set of starting classes and to streamline the process of finding class expressions in ontologies. %, particularly in large search spaces. The current implementation and settings indicated that the Forest Mixing approach did not outperform the traditional CELOE. Despite these results, the conceptual proposal brought forward by this approach may stimulate future improvements in class expression finding in ontologies. % and influence. % th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25209;&#37327;&#26657;&#20934;&#65288;BC&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#31034;&#33030;&#24369;&#24615;&#21644;&#20559;&#35265;&#22240;&#32032;&#23548;&#33268;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#12290;BC&#36890;&#36807;&#25511;&#21046;&#25209;&#37327;&#36755;&#20837;&#30340;&#19978;&#19979;&#25991;&#20559;&#35265;&#65292;&#32479;&#19968;&#20102;&#29616;&#26377;&#30340;&#26657;&#20934;&#26041;&#27861;&#65292;&#24182;&#20855;&#26377;&#38646;-shot&#21644;&#20165;&#25512;&#29702;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2309.17249</link><description>&lt;p&gt;
&#25209;&#37327;&#26657;&#20934;&#65306;&#37325;&#26032;&#24605;&#32771;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#25552;&#31034;&#24037;&#31243;&#30340;&#26657;&#20934;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Batch Calibration: Rethinking Calibration for In-Context Learning and Prompt Engineering. (arXiv:2309.17249v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17249
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25209;&#37327;&#26657;&#20934;&#65288;BC&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#31034;&#33030;&#24369;&#24615;&#21644;&#20559;&#35265;&#22240;&#32032;&#23548;&#33268;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#12290;BC&#36890;&#36807;&#25511;&#21046;&#25209;&#37327;&#36755;&#20837;&#30340;&#19978;&#19979;&#25991;&#20559;&#35265;&#65292;&#32479;&#19968;&#20102;&#29616;&#26377;&#30340;&#26657;&#20934;&#26041;&#27861;&#65292;&#24182;&#20855;&#26377;&#38646;-shot&#21644;&#20165;&#25512;&#29702;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#24050;&#25104;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#39640;&#25928;&#23398;&#20064;&#33539;&#24335;&#12290;&#28982;&#32780;&#65292;LLM&#23384;&#22312;&#25552;&#31034;&#33030;&#24369;&#24615;&#21644;&#21508;&#31181;&#20559;&#35265;&#22240;&#32032;&#65292;&#21253;&#25324;&#20294;&#19981;&#38480;&#20110;&#26684;&#24335;&#12289;&#36873;&#25321;&#24615;&#30340;&#34920;&#36798;&#26041;&#24335;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#31034;&#20363;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#26657;&#20934;&#26041;&#27861;&#26469;&#20943;&#36731;&#36825;&#20123;&#20559;&#35265;&#30340;&#24433;&#21709;&#24182;&#24674;&#22797;LLM&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#23545;&#29616;&#26377;&#30340;&#26657;&#20934;&#26041;&#27861;&#36827;&#34892;&#20102;&#31995;&#32479;&#20998;&#26512;&#65292;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#35266;&#28857;&#24182;&#25581;&#31034;&#20102;&#22833;&#36133;&#26696;&#20363;&#12290;&#21463;&#36825;&#20123;&#20998;&#26512;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25209;&#37327;&#26657;&#20934;&#65288;BC&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#30452;&#35266;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#25209;&#37327;&#36755;&#20837;&#20013;&#25511;&#21046;&#19978;&#19979;&#25991;&#20559;&#35265;&#65292;&#32479;&#19968;&#20102;&#21508;&#31181;&#20808;&#21069;&#30340;&#26041;&#27861;&#65292;&#24182;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#19978;&#36848;&#38382;&#39064;&#12290;BC&#26159;&#38646;-shot&#12289;&#20165;&#25512;&#29702;&#21644;&#39069;&#22806;&#25104;&#26412;&#21487;&#24573;&#30053;&#12290;&#22312;&#23569;-shot&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25193;&#23637;BC&#20197;&#23454;&#29616;&#20840;&#37096;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Prompting and in-context learning (ICL) have become efficient learning paradigms for large language models (LLMs). However, LLMs suffer from prompt brittleness and various bias factors in the prompt, including but not limited to the formatting, the choice verbalizers, and the ICL examples. To address this problem that results in unexpected performance degradation, calibration methods have been developed to mitigate the effects of these biases while recovering LLM performance. In this work, we first conduct a systematic analysis of the existing calibration methods, where we both provide a unified view and reveal the failure cases. Inspired by these analyses, we propose Batch Calibration (BC), a simple yet intuitive method that controls the contextual bias from the batched input, unifies various prior approaches, and effectively addresses the aforementioned issues. BC is zero-shot, inference-only, and incurs negligible additional costs. In the few-shot setup, we further extend BC to allo
&lt;/p&gt;</description></item><item><title>MORPH&#26159;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#30828;&#20214;&#35774;&#35745;&#21442;&#25968;&#21644;&#25511;&#21046;&#31574;&#30053;&#30340;&#21327;&#21516;&#20248;&#21270;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#24341;&#20837;&#21487;&#24494;&#20998;&#30340;&#30828;&#20214;&#27169;&#22411;&#20195;&#29702;&#65292;MORPH&#33021;&#22815;&#23454;&#29616;&#26377;&#25928;&#30340;&#20248;&#21270;&#24182;&#20445;&#25345;&#20248;&#21270;&#21518;&#30340;&#30828;&#20214;&#20195;&#29702;&#25509;&#36817;&#20854;&#30495;&#23454;&#23545;&#24212;&#29289;&#12290;</title><link>http://arxiv.org/abs/2309.17227</link><description>&lt;p&gt;
MORPH&#65306;&#36890;&#36807;&#21487;&#24494;&#20998;&#30340;&#30828;&#20214;&#27169;&#22411;&#20195;&#29702;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#35774;&#35745;&#21327;&#21516;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
MORPH: Design Co-optimization with Reinforcement Learning via a Differentiable Hardware Model Proxy. (arXiv:2309.17227v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17227
&lt;/p&gt;
&lt;p&gt;
MORPH&#26159;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#30828;&#20214;&#35774;&#35745;&#21442;&#25968;&#21644;&#25511;&#21046;&#31574;&#30053;&#30340;&#21327;&#21516;&#20248;&#21270;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#24341;&#20837;&#21487;&#24494;&#20998;&#30340;&#30828;&#20214;&#27169;&#22411;&#20195;&#29702;&#65292;MORPH&#33021;&#22815;&#23454;&#29616;&#26377;&#25928;&#30340;&#20248;&#21270;&#24182;&#20445;&#25345;&#20248;&#21270;&#21518;&#30340;&#30828;&#20214;&#20195;&#29702;&#25509;&#36817;&#20854;&#30495;&#23454;&#23545;&#24212;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MORPH&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#22312;&#20223;&#30495;&#20013;&#36827;&#34892;&#30828;&#20214;&#35774;&#35745;&#21442;&#25968;&#21644;&#25511;&#21046;&#31574;&#30053;&#30340;&#21327;&#21516;&#20248;&#21270;&#12290;&#19982;&#22823;&#22810;&#25968;&#21327;&#21516;&#20248;&#21270;&#26041;&#27861;&#31867;&#20284;&#65292;MORPH&#20381;&#36182;&#20110;&#27491;&#22312;&#20248;&#21270;&#30340;&#30828;&#20214;&#27169;&#22411;&#65292;&#36890;&#24120;&#26159;&#22522;&#20110;&#29289;&#29702;&#23450;&#24459;&#36827;&#34892;&#27169;&#25311;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#27169;&#22411;&#24448;&#24448;&#38590;&#20197;&#38598;&#25104;&#21040;&#26377;&#25928;&#30340;&#20248;&#21270;&#20363;&#31243;&#20013;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20195;&#29702;&#30828;&#20214;&#27169;&#22411;&#65292;&#23427;&#22987;&#32456;&#26159;&#21487;&#24494;&#20998;&#30340;&#65292;&#24182;&#19988;&#33021;&#22815;&#20351;&#29992;RL&#21644;&#38271;&#26399;&#25511;&#21046;&#31574;&#30053;&#26377;&#25928;&#22320;&#36827;&#34892;&#21327;&#21516;&#20248;&#21270;&#12290;MORPH&#30340;&#35774;&#35745;&#30446;&#26631;&#26159;&#30830;&#20445;&#20248;&#21270;&#21518;&#30340;&#30828;&#20214;&#20195;&#29702;&#19982;&#20854;&#30495;&#23454;&#23545;&#24212;&#29289;&#23613;&#21487;&#33021;&#25509;&#36817;&#65292;&#21516;&#26102;&#20173;&#28982;&#33021;&#22815;&#23436;&#25104;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;&#20223;&#30495;&#30340;2D&#20280;&#25163;&#21644;3D&#22810;&#25351;&#25805;&#32437;&#20219;&#21153;&#19978;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce MORPH, a method for co-optimization of hardware design parameters and control policies in simulation using reinforcement learning. Like most co-optimization methods, MORPH relies on a model of the hardware being optimized, usually simulated based on the laws of physics. However, such a model is often difficult to integrate into an effective optimization routine. To address this, we introduce a proxy hardware model, which is always differentiable and enables efficient co-optimization alongside a long-horizon control policy using RL. MORPH is designed to ensure that the optimized hardware proxy remains as close as possible to its realistic counterpart, while still enabling task completion. We demonstrate our approach on simulated 2D reaching and 3D multi-fingered manipulation tasks.
&lt;/p&gt;</description></item><item><title>RSAM&#26159;&#19968;&#31181;&#22312;&#27969;&#24418;&#19978;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;Sharpness-Aware Minimization (SAM)&#25512;&#24191;&#21040;Riemannian&#27969;&#24418;&#65292;&#24341;&#20837;&#20102;&#27969;&#24418;&#19978;sharpness&#30340;&#27010;&#24565;&#65292;&#24182;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#35777;&#26126;&#20102;&#20854;&#19982;&#27867;&#21270;&#33021;&#21147;&#30340;&#20851;&#31995;&#12290;&#36890;&#36807;&#35813;&#31639;&#27861;&#30340;&#24212;&#29992;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20854;&#22312;&#25552;&#21319;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.17215</link><description>&lt;p&gt;
RSAM&#65306;&#20351;&#29992;Riemannian Sharpness-aware Minimization&#22312;&#27969;&#24418;&#19978;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
RSAM: Learning on manifolds with Riemannian Sharpness-aware Minimization. (arXiv:2309.17215v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17215
&lt;/p&gt;
&lt;p&gt;
RSAM&#26159;&#19968;&#31181;&#22312;&#27969;&#24418;&#19978;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;Sharpness-Aware Minimization (SAM)&#25512;&#24191;&#21040;Riemannian&#27969;&#24418;&#65292;&#24341;&#20837;&#20102;&#27969;&#24418;&#19978;sharpness&#30340;&#27010;&#24565;&#65292;&#24182;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#35777;&#26126;&#20102;&#20854;&#19982;&#27867;&#21270;&#33021;&#21147;&#30340;&#20851;&#31995;&#12290;&#36890;&#36807;&#35813;&#31639;&#27861;&#30340;&#24212;&#29992;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20854;&#22312;&#25552;&#21319;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#20102;&#35299;&#25439;&#22833;&#20989;&#25968;&#31354;&#38388;&#30340;&#20960;&#20309;&#32467;&#26500;&#26377;&#26395;&#25552;&#21319;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20511;&#37492;&#20102;&#20043;&#21069;&#23558;&#20960;&#20309;&#21407;&#29702;&#24212;&#29992;&#20110;&#20248;&#21270;&#30340;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#21463;&#38480;&#21046;&#20248;&#21270;&#38382;&#39064;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#30340;&#26032;&#26041;&#27861;&#12290;&#20107;&#23454;&#19978;&#65292;&#26412;&#25991;&#26088;&#22312;&#23558;Sharpness-Aware Minimization (SAM)&#20248;&#21270;&#22120;&#25512;&#24191;&#21040;Riemannian&#27969;&#24418;&#12290;&#20026;&#20102;&#25903;&#25345;&#27969;&#24418;&#19978;&#30340;&#8220;sharpness&#8221;&#27010;&#24565;&#65292;&#25105;&#20204;&#39318;&#20808;&#23545;&#27969;&#24418;&#19978;&#30340;sharpness&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#23450;&#20041;&#12290;&#20026;&#20102;&#35777;&#26126;&#36825;&#20010;&#27010;&#24565;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#20998;&#26512;&#26469;&#25551;&#36848;&#27969;&#24418;sharpness&#19982;&#27867;&#21270;&#33021;&#21147;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#21576;&#29616;&#20102;&#19968;&#20010;&#26356;&#32039;&#23494;&#30340;&#27867;&#21270;&#32570;&#21475;&#19978;&#38480;&#65292;&#36825;&#26159;&#20043;&#21069;&#26410;&#30693;&#30340;&#32467;&#26524;&#12290;&#21463;&#21040;&#36825;&#20010;&#20998;&#26512;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#65292;Riemannian Sharpness-Aware Minimization (RSAM)&#12290;&#20026;&#20102;&#23637;&#31034;RSAM&#22312;&#25552;&#21319;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#19978;&#35780;&#20272;&#21644;&#23545;&#27604;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nowadays, understanding the geometry of the loss landscape shows promise in enhancing a model's generalization ability. In this work, we draw upon prior works that apply geometric principles to optimization and present a novel approach to improve robustness and generalization ability for constrained optimization problems. Indeed, this paper aims to generalize the Sharpness-Aware Minimization (SAM) optimizer to Riemannian manifolds. In doing so, we first extend the concept of sharpness and introduce a novel notion of sharpness on manifolds. To support this notion of sharpness, we present a theoretical analysis characterizing generalization capabilities with respect to manifold sharpness, which demonstrates a tighter bound on the generalization gap, a result not known before. Motivated by this analysis, we introduce our algorithm, Riemannian Sharpness-Aware Minimization (RSAM). To demonstrate RSAM's ability to enhance generalization ability, we evaluate and contrast our algorithm on a br
&lt;/p&gt;</description></item><item><title>ComSD&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#26356;&#21512;&#29702;&#30340;&#20114;&#20449;&#24687;&#20272;&#35745;&#21644;&#21160;&#24577;&#21152;&#26435;&#30340;&#20869;&#22312;&#22870;&#21169;&#26469;&#24179;&#34913;&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#20013;&#30340;&#34892;&#20026;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.17203</link><description>&lt;p&gt;
ComSD: &#22312;&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#20013;&#24179;&#34913;&#34892;&#20026;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;
&lt;/p&gt;
&lt;p&gt;
ComSD: Balancing Behavioral Quality and Diversity in Unsupervised Skill Discovery. (arXiv:2309.17203v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17203
&lt;/p&gt;
&lt;p&gt;
ComSD&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#26356;&#21512;&#29702;&#30340;&#20114;&#20449;&#24687;&#20272;&#35745;&#21644;&#21160;&#24577;&#21152;&#26435;&#30340;&#20869;&#22312;&#22870;&#21169;&#26469;&#24179;&#34913;&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#20013;&#30340;&#34892;&#20026;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#30340;&#29702;&#24819;&#26041;&#27861;&#33021;&#22815;&#22312;&#27809;&#26377;&#22806;&#37096;&#22870;&#21169;&#30340;&#24773;&#20917;&#19979;&#20135;&#29983;&#22810;&#26679;&#19988;&#21512;&#26684;&#30340;&#25216;&#33021;&#65292;&#21516;&#26102;&#21457;&#29616;&#30340;&#25216;&#33021;&#38598;&#33021;&#22815;&#20197;&#21508;&#31181;&#26041;&#24335;&#39640;&#25928;&#22320;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Contrastive multi-objectives Skill Discovery (ComSD)&#65292;&#36890;&#36807;&#26356;&#21512;&#29702;&#30340;&#20114;&#20449;&#24687;&#20272;&#35745;&#21644;&#21160;&#24577;&#21152;&#26435;&#30340;&#20869;&#22312;&#22870;&#21169;&#26469;&#20943;&#36731;&#21457;&#29616;&#30340;&#34892;&#20026;&#22312;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#20043;&#38388;&#30340;&#20914;&#31361;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning diverse and qualified behaviors for utilization and adaptation without supervision is a key ability of intelligent creatures. Ideal unsupervised skill discovery methods are able to produce diverse and qualified skills in the absence of extrinsic reward, while the discovered skill set can efficiently adapt to downstream tasks in various ways. Maximizing the Mutual Information (MI) between skills and visited states can achieve ideal skill-conditioned behavior distillation in theory. However, it's difficult for recent advanced methods to well balance behavioral quality (exploration) and diversity (exploitation) in practice, which may be attributed to the unreasonable MI estimation by their rigid intrinsic reward design. In this paper, we propose Contrastive multi-objectives Skill Discovery (ComSD) which tries to mitigate the quality-versus-diversity conflict of discovered behaviors through a more reasonable MI estimation and a dynamically weighted intrinsic reward. ComSD proposes
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#20351;&#29992;&#25918;&#23556;&#32452;&#23398;&#29305;&#24449;&#35757;&#32451;&#30340;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#20013;&#30340;&#31181;&#26063;&#20559;&#35265;&#65292;&#21457;&#29616;&#20174;DCE-MRI&#25968;&#25454;&#20013;&#25552;&#21462;&#30340;&#25918;&#23556;&#32452;&#23398;&#29305;&#24449;&#21253;&#21547;&#31181;&#26063;&#21487;&#36776;&#35782;&#30340;&#20449;&#24687;&#12290;&#22522;&#20110;&#36825;&#20123;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#33021;&#20197;60-70%&#30340;&#20934;&#30830;&#29575;&#39044;&#27979;&#30333;&#20154;&#21644;&#40657;&#20154;&#31181;&#26063;&#65292;&#19988;&#22522;&#20110;&#31181;&#26063;&#19981;&#24179;&#34913;&#25968;&#25454;&#30340;&#35757;&#32451;&#20250;&#23548;&#33268;&#27169;&#22411;&#20135;&#29983;&#20559;&#35265;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2309.17197</link><description>&lt;p&gt;
&#22522;&#20110;&#20083;&#33146;DCE-MRI&#29305;&#24449;&#30340;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#20013;&#30340;&#31181;&#26063;&#20559;&#35265;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
An Investigation Into Race Bias in Random Forest Models Based on Breast DCE-MRI Derived Radiomics Features. (arXiv:2309.17197v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17197
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#20351;&#29992;&#25918;&#23556;&#32452;&#23398;&#29305;&#24449;&#35757;&#32451;&#30340;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#20013;&#30340;&#31181;&#26063;&#20559;&#35265;&#65292;&#21457;&#29616;&#20174;DCE-MRI&#25968;&#25454;&#20013;&#25552;&#21462;&#30340;&#25918;&#23556;&#32452;&#23398;&#29305;&#24449;&#21253;&#21547;&#31181;&#26063;&#21487;&#36776;&#35782;&#30340;&#20449;&#24687;&#12290;&#22522;&#20110;&#36825;&#20123;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#33021;&#20197;60-70%&#30340;&#20934;&#30830;&#29575;&#39044;&#27979;&#30333;&#20154;&#21644;&#40657;&#20154;&#31181;&#26063;&#65292;&#19988;&#22522;&#20110;&#31181;&#26063;&#19981;&#24179;&#34913;&#25968;&#25454;&#30340;&#35757;&#32451;&#20250;&#23548;&#33268;&#27169;&#22411;&#20135;&#29983;&#20559;&#35265;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27169;&#22411;&#22312;&#20351;&#29992;&#21463;&#20445;&#25252;&#23646;&#24615;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#26102;&#21487;&#33021;&#20250;&#34920;&#29616;&#20986;&#20559;&#35265;&#12290;&#22823;&#37096;&#20998;&#29616;&#26377;&#30340;&#24037;&#20316;&#37117;&#38598;&#20013;&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19978;&#65292;&#20294;&#26159;&#21033;&#29992;&#25163;&#24037;&#29305;&#24449;&#30340;&#32463;&#20856;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#20063;&#21487;&#33021;&#21463;&#21040;&#36825;&#31181;&#20559;&#35265;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#25918;&#23556;&#32452;&#23398;&#29305;&#24449;&#35757;&#32451;&#30340;&#38543;&#26426;&#26862;&#26519;&#65288;RF&#65289;&#27169;&#22411;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#31181;&#26063;&#20559;&#35265;&#12290;&#25105;&#20204;&#30340;&#24212;&#29992;&#26159;&#21033;&#29992;&#20083;&#33146;&#30284;&#24739;&#32773;&#30340;&#21160;&#24577;&#22686;&#24378;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;DCE-MRI&#65289;&#39044;&#27979;&#32959;&#30244;&#20998;&#23376;&#20122;&#22411;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20174;DCE-MRI&#25968;&#25454;&#20013;&#25552;&#21462;&#30340;&#25918;&#23556;&#32452;&#23398;&#29305;&#24449;&#30830;&#23454;&#21253;&#21547;&#31181;&#26063;&#21487;&#36776;&#35782;&#30340;&#20449;&#24687;&#65292;&#24182;&#19988;&#22522;&#20110;&#36825;&#20123;&#25968;&#25454;&#35757;&#32451;&#30340;RF&#27169;&#22411;&#21487;&#20197;&#20197;60-70&#65285;&#30340;&#20934;&#30830;&#29575;&#39044;&#27979;&#30333;&#20154;&#21644;&#40657;&#20154;&#31181;&#26063;&#65292;&#20855;&#20307;&#21462;&#20915;&#20110;&#25152;&#20351;&#29992;&#30340;&#29305;&#24449;&#23376;&#38598;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#31181;&#26063;&#19981;&#24179;&#34913;&#25968;&#25454;&#35757;&#32451;&#30340;RF&#27169;&#22411;&#20284;&#20046;&#20250;&#20135;&#29983;&#20559;&#35265;&#34892;&#20026;&#65292;&#34920;&#29616;&#20986;&#26576;&#31181;&#31243;&#24230;&#30340;&#31181;&#26063;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research has shown that artificial intelligence (AI) models can exhibit bias in performance when trained using data that are imbalanced by protected attribute(s). Most work to date has focused on deep learning models, but classical AI techniques that make use of hand-crafted features may also be susceptible to such bias. In this paper we investigate the potential for race bias in random forest (RF) models trained using radiomics features. Our application is prediction of tumour molecular subtype from dynamic contrast enhanced magnetic resonance imaging (DCE-MRI) of breast cancer patients. Our results show that radiomics features derived from DCE-MRI data do contain race-identifiable information, and that RF models can be trained to predict White and Black race from these data with 60-70% accuracy, depending on the subset of features used. Furthermore, RF models trained to predict tumour molecular subtype using race-imbalanced data seem to produce biased behaviour, exhibiting bet
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20803;&#24863;&#30693;&#30340;&#36752;&#23556;&#34701;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#35821;&#20041;&#35299;&#26512;&#21644;&#22522;&#20803;&#25552;&#21462;&#26469;&#21152;&#36895;&#37325;&#24314;&#36807;&#31243;&#65292;&#24182;&#25104;&#21151;&#23558;&#35821;&#20041;&#12289;&#22522;&#20803;&#21644;&#36752;&#23556;&#20449;&#24687;&#34701;&#21512;&#21040;&#21333;&#19968;&#26694;&#26550;&#20013;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#37325;&#24314;&#12289;&#39640;&#36136;&#37327;&#28210;&#26579;&#21644;&#26041;&#20415;&#30340;&#32534;&#36753;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.17190</link><description>&lt;p&gt;
PARF: &#23460;&#20869;&#22330;&#26223;&#26032;&#35270;&#35282;&#21512;&#25104;&#30340;&#22522;&#20803;&#24863;&#30693;&#36752;&#23556;&#34701;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
PARF: Primitive-Aware Radiance Fusion for Indoor Scene Novel View Synthesis. (arXiv:2309.17190v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17190
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20803;&#24863;&#30693;&#30340;&#36752;&#23556;&#34701;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#35821;&#20041;&#35299;&#26512;&#21644;&#22522;&#20803;&#25552;&#21462;&#26469;&#21152;&#36895;&#37325;&#24314;&#36807;&#31243;&#65292;&#24182;&#25104;&#21151;&#23558;&#35821;&#20041;&#12289;&#22522;&#20803;&#21644;&#36752;&#23556;&#20449;&#24687;&#34701;&#21512;&#21040;&#21333;&#19968;&#26694;&#26550;&#20013;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#37325;&#24314;&#12289;&#39640;&#36136;&#37327;&#28210;&#26579;&#21644;&#26041;&#20415;&#30340;&#32534;&#36753;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#22330;&#26223;&#36752;&#23556;&#22330;&#37325;&#24314;&#26041;&#27861;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#26032;&#35270;&#35282;&#21512;&#25104;&#24615;&#33021;&#21644;&#26041;&#20415;&#30340;&#22330;&#26223;&#32534;&#36753;&#21151;&#33021;&#12290;&#20854;&#20851;&#38190;&#24605;&#24819;&#26159;&#20805;&#20998;&#21033;&#29992;&#35821;&#20041;&#35299;&#26512;&#21644;&#22522;&#20803;&#25552;&#21462;&#26469;&#32422;&#26463;&#21644;&#21152;&#36895;&#36752;&#23556;&#22330;&#37325;&#24314;&#36807;&#31243;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20803;&#24863;&#30693;&#28151;&#21512;&#28210;&#26579;&#31574;&#30053;&#65292;&#20197;&#20805;&#20998;&#21457;&#25381;&#20307;&#32032;&#28210;&#26579;&#21644;&#22522;&#20803;&#28210;&#26579;&#30340;&#20248;&#28857;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20379;&#20102;&#19968;&#20010;&#37325;&#24314;&#27969;&#31243;&#65292;&#36890;&#36807;&#36845;&#20195;&#22320;&#23545;&#27599;&#20010;&#36755;&#20837;&#24103;&#36827;&#34892;&#22522;&#20803;&#35299;&#26512;&#21644;&#36752;&#23556;&#22330;&#23398;&#20064;&#65292;&#25104;&#21151;&#22320;&#23558;&#35821;&#20041;&#12289;&#22522;&#20803;&#21644;&#36752;&#23556;&#20449;&#24687;&#34701;&#21512;&#21040;&#19968;&#20010;&#21333;&#19968;&#30340;&#26694;&#26550;&#20013;&#12290;&#24191;&#27867;&#30340;&#35780;&#20272;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#24555;&#36895;&#37325;&#24314;&#33021;&#21147;&#12289;&#39640;&#28210;&#26579;&#36136;&#37327;&#21644;&#26041;&#20415;&#30340;&#32534;&#36753;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a method for fast scene radiance field reconstruction with strong novel view synthesis performance and convenient scene editing functionality. The key idea is to fully utilize semantic parsing and primitive extraction for constraining and accelerating the radiance field reconstruction process. To fulfill this goal, a primitive-aware hybrid rendering strategy was proposed to enjoy the best of both volumetric and primitive rendering. We further contribute a reconstruction pipeline conducts primitive parsing and radiance field learning iteratively for each input frame which successfully fuses semantic, primitive, and radiance information into a single framework. Extensive evaluations demonstrate the fast reconstruction ability, high rendering quality, and convenient editing functionality of our method.
&lt;/p&gt;</description></item><item><title>Alphazero&#31867;&#20284;&#30340;&#26641;&#25628;&#32034;&#26694;&#26550;TS-LLM&#21487;&#20197;&#21033;&#29992;&#23398;&#20064;&#30340;&#20215;&#20540;&#20989;&#25968;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#30721;&#21644;&#35757;&#32451;&#65292;&#19981;&#20165;&#36866;&#29992;&#20110;&#25512;&#29702;&#20219;&#21153;&#65292;&#36824;&#36866;&#29992;&#20110;&#20854;&#20182;&#20219;&#21153;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#22823;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#19978;&#20855;&#26377;&#26222;&#36866;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;</title><link>http://arxiv.org/abs/2309.17179</link><description>&lt;p&gt;
Alphazero&#31867;&#20284;&#30340;&#26641;&#25628;&#32034;&#21487;&#20197;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#30721;&#21644;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Alphazero-like Tree-Search can Guide Large Language Model Decoding and Training. (arXiv:2309.17179v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17179
&lt;/p&gt;
&lt;p&gt;
Alphazero&#31867;&#20284;&#30340;&#26641;&#25628;&#32034;&#26694;&#26550;TS-LLM&#21487;&#20197;&#21033;&#29992;&#23398;&#20064;&#30340;&#20215;&#20540;&#20989;&#25968;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#30721;&#21644;&#35757;&#32451;&#65292;&#19981;&#20165;&#36866;&#29992;&#20110;&#25512;&#29702;&#20219;&#21153;&#65292;&#36824;&#36866;&#29992;&#20110;&#20854;&#20182;&#20219;&#21153;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#22823;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#19978;&#20855;&#26377;&#26222;&#36866;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#36890;&#24120;&#37319;&#29992;&#37319;&#26679;&#25110;&#26463;&#25628;&#32034;&#65292;&#32467;&#21512; Chain-of-Thought (CoT) &#31561;&#25552;&#31034;&#26469;&#25552;&#39640;&#25512;&#29702;&#21644;&#35299;&#30721;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#22914; Tree-of-Thought (ToT) &#21644; Reasoning via Planning (RAP) &#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#26641;&#25628;&#32034;&#31639;&#27861;&#26469;&#24341;&#23548;&#22810;&#27493;&#25512;&#29702;&#65292;&#26469;&#22686;&#24378;LLM&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#36825;&#20123;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;LLM&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#19988;&#20005;&#37325;&#20381;&#36182;&#20154;&#20026;&#35774;&#35745;&#30340;&#25552;&#31034;&#26469;&#28608;&#27963;LLM&#20316;&#20026;&#19968;&#20010;&#20215;&#20540;&#20989;&#25968;&#65292;&#32570;&#20047;&#26222;&#36866;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;AlphaZero&#31867;&#20284;&#30340;&#29992;&#20110;LLM&#30340;&#26641;&#25628;&#32034;&#26694;&#26550; (&#31216;&#20026;TS-LLM)&#65292;&#31995;&#32479;&#22320;&#35828;&#26126;&#20102;&#22914;&#20309;&#36890;&#36807;&#23398;&#20064;&#30340;&#20215;&#20540;&#20989;&#25968;&#21033;&#29992;&#26641;&#25628;&#32034;&#26469;&#25351;&#23548;LLM&#30340;&#35299;&#30721;&#33021;&#21147;&#12290;TS-LLM&#22312;&#20004;&#20010;&#20851;&#38190;&#26041;&#38754;&#19982;&#20247;&#19981;&#21516;&#65306;(1)&#36890;&#36807;&#21033;&#29992;&#23398;&#20064;&#30340;&#20215;&#20540;&#20989;&#25968;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26222;&#36866;&#22320;&#24212;&#29992;&#20110;&#38500;&#20102;&#25512;&#29702;&#20043;&#22806;&#30340;&#19981;&#21516;&#20219;&#21153; (&#20363;&#22914;RLHF&#23545;&#40784;)&#65292;&#20197;&#21450;&#20219;&#20309;&#22823;&#23567;&#30340;LLM&#65292;&#32780;&#19981;&#38656;&#35201;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) typically employ sampling or beam search, accompanied by prompts such as Chain-of-Thought (CoT), to boost reasoning and decoding ability. Recent work like Tree-of-Thought (ToT) and Reasoning via Planning (RAP) aim to augment the reasoning capabilities of LLMs by utilizing tree-search algorithms to guide multi-step reasoning. These methods mainly focus on LLMs' reasoning ability during inference and heavily rely on human-designed prompts to activate LLM as a value function, which lacks general applicability and scalability. To address these limitations, we present an AlphaZero-like tree-search framework for LLMs (termed TS-LLM), systematically illustrating how tree-search with a learned value function can guide LLMs' decoding ability. TS-LLM distinguishes itself in two key ways: (1) Leveraging a learned value function, our approach can be generally applied to different tasks beyond reasoning (such as RLHF alignment), and LLMs of any size, without prompting a
&lt;/p&gt;</description></item><item><title>RLAdapter&#24341;&#20837;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#24378;&#21270;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;&#22312;&#31232;&#30095;&#22870;&#21169;&#29615;&#22659;&#20013;&#30340;&#31574;&#30053;&#23398;&#20064;&#24615;&#33021;&#12290;&#36825;&#36890;&#36807;&#35299;&#20915;LLM&#22312;&#29702;&#35299;&#19979;&#28216;&#20219;&#21153;&#26041;&#38754;&#30340;&#22256;&#38590;&#65292;&#20197;&#21450;&#36890;&#36807;&#36991;&#20813;&#20351;&#29992;&#19981;&#21487;&#35775;&#38382;&#30340;&#27169;&#22411;&#26435;&#37325;&#25110;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#26469;&#24494;&#35843;LLM&#30340;&#26041;&#24335;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2309.17176</link><description>&lt;p&gt;
RLAdapter&#65306;&#22312;&#24320;&#25918;&#29615;&#22659;&#20013;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#24378;&#21270;&#23398;&#20064;&#30456;&#32467;&#21512;
&lt;/p&gt;
&lt;p&gt;
RLAdapter: Bridging Large Language Models to Reinforcement Learning in Open Worlds. (arXiv:2309.17176v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17176
&lt;/p&gt;
&lt;p&gt;
RLAdapter&#24341;&#20837;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#24378;&#21270;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;&#22312;&#31232;&#30095;&#22870;&#21169;&#29615;&#22659;&#20013;&#30340;&#31574;&#30053;&#23398;&#20064;&#24615;&#33021;&#12290;&#36825;&#36890;&#36807;&#35299;&#20915;LLM&#22312;&#29702;&#35299;&#19979;&#28216;&#20219;&#21153;&#26041;&#38754;&#30340;&#22256;&#38590;&#65292;&#20197;&#21450;&#36890;&#36807;&#36991;&#20813;&#20351;&#29992;&#19981;&#21487;&#35775;&#38382;&#30340;&#27169;&#22411;&#26435;&#37325;&#25110;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#26469;&#24494;&#35843;LLM&#30340;&#26041;&#24335;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#20915;&#31574;&#38382;&#39064;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#36890;&#24120;&#38656;&#35201;&#19982;&#29615;&#22659;&#36827;&#34892;&#22823;&#37327;&#30340;&#20132;&#20114;&#65292;&#22312;&#31232;&#30095;&#22870;&#21169;&#29615;&#22659;&#20013;&#23398;&#20064;&#26377;&#24847;&#20041;&#30340;&#31574;&#30053;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21487;&#20197;&#20026;&#20195;&#29702;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#25351;&#23548;&#65292;&#20174;&#32780;&#22686;&#24378;RL&#31639;&#27861;&#22312;&#36825;&#20123;&#29615;&#22659;&#20013;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;LLM&#36890;&#24120;&#22312;&#29702;&#35299;&#19979;&#28216;&#20219;&#21153;&#26041;&#38754;&#36935;&#21040;&#22256;&#38590;&#65292;&#36825;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#26368;&#20248;&#22320;&#24110;&#21161;&#20195;&#29702;&#30340;&#33021;&#21147;&#12290;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#30340;&#24120;&#35265;&#26041;&#27861;&#26159;&#20351;&#29992;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#25968;&#25454;&#26469;&#24494;&#35843;LLM&#65292;&#20351;&#20854;&#33021;&#22815;&#20026;RL&#20195;&#29702;&#25552;&#20379;&#26377;&#29992;&#30340;&#25351;&#23548;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#36935;&#21040;&#20102;&#19968;&#20123;&#22256;&#38590;&#65292;&#27604;&#22914;&#26080;&#27861;&#35775;&#38382;&#30340;&#27169;&#22411;&#26435;&#37325;&#25110;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#20351;&#20854;&#19981;&#20999;&#23454;&#38469;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;RLAdapter&#65292;&#36825;&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#22312;RL&#31639;&#27861;&#21644;LLM&#20043;&#38388;&#24314;&#31435;&#26356;&#22909;&#30340;&#36830;&#25509;&#65292;&#20174;&#32780;&#25972;&#21512;&#23427;&#20204;&#30340;&#20248;&#21183;&#21644;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
While reinforcement learning (RL) shows remarkable success in decision-making problems, it often requires a lot of interactions with the environment, and in sparse-reward environments, it is challenging to learn meaningful policies. Large Language Models (LLMs) can potentially provide valuable guidance to agents in learning policies, thereby enhancing the performance of RL algorithms in such environments. However, LLMs often encounter difficulties in understanding downstream tasks, which hinders their ability to optimally assist agents in these tasks. A common approach to mitigating this issue is to fine-tune the LLMs with task-related data, enabling them to offer useful guidance for RL agents. However, this approach encounters several difficulties, such as inaccessible model weights or the need for significant computational resources, making it impractical. In this work, we introduce RLAdapter, a framework that builds a better connection between RL algorithms and LLMs by incorporating
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;&#28151;&#20081;&#29615;&#22659;&#20013;&#25235;&#21462;&#24050;&#37319;&#25688;&#30340;&#35199;&#32418;&#26623;&#31319;&#26524;&#30340;&#35270;&#35273;&#24341;&#23548;&#26426;&#22120;&#20154;&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#21033;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35270;&#35273;&#31995;&#32479;&#26469;&#35782;&#21035;&#31319;&#26524;&#24182;&#30830;&#23450;&#36866;&#21512;&#25235;&#21462;&#30340;&#20301;&#32622;&#65292;&#36890;&#36807;&#22312;&#32447;&#23398;&#20064;&#26469;&#25490;&#24207;&#25235;&#21462;&#23039;&#21183;&#65292;&#24182;&#23454;&#29616;&#26080;&#35302;&#35273;&#20256;&#24863;&#22120;&#25110;&#20960;&#20309;&#27169;&#22411;&#30340;&#22841;&#25345;&#25235;&#21462;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#31995;&#32479;&#20855;&#26377;100%&#30340;&#28165;&#29702;&#29575;&#21644;93%&#30340;&#19968;&#27425;&#24615;&#25104;&#21151;&#25235;&#21462;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.17170</link><description>&lt;p&gt;
&#29992;&#20110;&#22312;&#28151;&#20081;&#29615;&#22659;&#20013;&#25235;&#21462;&#24050;&#37319;&#25688;&#30340;&#35199;&#32418;&#26623;&#31319;&#26524;&#30340;&#35270;&#35273;&#24341;&#23548;&#26426;&#22120;&#20154;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
A Vision-Guided Robotic System for Grasping Harvested Tomato Trusses in Cluttered Environments. (arXiv:2309.17170v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17170
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;&#28151;&#20081;&#29615;&#22659;&#20013;&#25235;&#21462;&#24050;&#37319;&#25688;&#30340;&#35199;&#32418;&#26623;&#31319;&#26524;&#30340;&#35270;&#35273;&#24341;&#23548;&#26426;&#22120;&#20154;&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#21033;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35270;&#35273;&#31995;&#32479;&#26469;&#35782;&#21035;&#31319;&#26524;&#24182;&#30830;&#23450;&#36866;&#21512;&#25235;&#21462;&#30340;&#20301;&#32622;&#65292;&#36890;&#36807;&#22312;&#32447;&#23398;&#20064;&#26469;&#25490;&#24207;&#25235;&#21462;&#23039;&#21183;&#65292;&#24182;&#23454;&#29616;&#26080;&#35302;&#35273;&#20256;&#24863;&#22120;&#25110;&#20960;&#20309;&#27169;&#22411;&#30340;&#22841;&#25345;&#25235;&#21462;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#31995;&#32479;&#20855;&#26377;100%&#30340;&#28165;&#29702;&#29575;&#21644;93%&#30340;&#19968;&#27425;&#24615;&#25104;&#21151;&#25235;&#21462;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#23545;&#20110;&#35199;&#32418;&#26623;&#30340;&#31216;&#37325;&#21644;&#21253;&#35013;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#24037;&#25805;&#20316;&#12290;&#33258;&#21160;&#21270;&#30340;&#20027;&#35201;&#38556;&#30861;&#22312;&#20110;&#24320;&#21457;&#19968;&#20010;&#21487;&#38752;&#30340;&#29992;&#20110;&#24050;&#37319;&#25688;&#30340;&#31319;&#26524;&#30340;&#26426;&#22120;&#20154;&#25235;&#21462;&#31995;&#32479;&#30340;&#22256;&#38590;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#25235;&#21462;&#22534;&#25918;&#22312;&#35013;&#31665;&#20013;&#30340;&#31319;&#26524;&#65292;&#36825;&#26159;&#23427;&#20204;&#22312;&#37319;&#25688;&#21518;&#24120;&#35265;&#30340;&#23384;&#20648;&#21644;&#36816;&#36755;&#26041;&#24335;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35270;&#35273;&#31995;&#32479;&#65292;&#39318;&#20808;&#35782;&#21035;&#20986;&#35013;&#31665;&#20013;&#30340;&#21333;&#20010;&#31319;&#26524;&#65292;&#28982;&#21518;&#30830;&#23450;&#33550;&#37096;&#30340;&#36866;&#21512;&#25235;&#21462;&#30340;&#20301;&#32622;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20855;&#26377;&#22312;&#32447;&#23398;&#20064;&#33021;&#21147;&#30340;&#25235;&#21462;&#23039;&#21183;&#25490;&#24207;&#31639;&#27861;&#12290;&#22312;&#36873;&#25321;&#20102;&#26368;&#26377;&#21069;&#26223;&#30340;&#25235;&#21462;&#23039;&#21183;&#20043;&#21518;&#65292;&#26426;&#22120;&#20154;&#25191;&#34892;&#19968;&#31181;&#26080;&#38656;&#35302;&#35273;&#20256;&#24863;&#22120;&#25110;&#20960;&#20309;&#27169;&#22411;&#30340;&#22841;&#25345;&#25235;&#21462;&#12290;&#23454;&#39564;&#23460;&#23454;&#39564;&#35777;&#26126;&#65292;&#37197;&#22791;&#20102;&#19968;&#20010;&#25163;&#30524;&#19968;&#20307;&#30340;RGB-D&#30456;&#26426;&#30340;&#26426;&#22120;&#20154;&#25805;&#32437;&#22120;&#20174;&#22534;&#20013;&#25441;&#36215;&#25152;&#26377;&#30340;&#31319;&#26524;&#30340;&#28165;&#29702;&#29575;&#36798;&#21040;100%&#12290;93%&#30340;&#31319;&#26524;&#22312;&#31532;&#19968;&#27425;&#23581;&#35797;&#26102;&#25104;&#21151;&#25235;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Currently, truss tomato weighing and packaging require significant manual work. The main obstacle to automation lies in the difficulty of developing a reliable robotic grasping system for already harvested trusses. We propose a method to grasp trusses that are stacked in a crate with considerable clutter, which is how they are commonly stored and transported after harvest. The method consists of a deep learning-based vision system to first identify the individual trusses in the crate and then determine a suitable grasping location on the stem. To this end, we have introduced a grasp pose ranking algorithm with online learning capabilities. After selecting the most promising grasp pose, the robot executes a pinch grasp without needing touch sensors or geometric models. Lab experiments with a robotic manipulator equipped with an eye-in-hand RGB-D camera showed a 100% clearance rate when tasked to pick all trusses from a pile. 93% of the trusses were successfully grasped on the first try,
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#26368;&#26032;&#30340;GPT&#27169;&#22411;&#22312;&#20020;&#24202;&#28145;&#24230;&#34920;&#22411;&#23398;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#30446;&#21069;&#36825;&#20123;&#27169;&#22411;&#23578;&#26410;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2309.17169</link><description>&lt;p&gt;
&#35780;&#20272;GPT&#27169;&#22411;&#22312;&#34920;&#22411;&#27010;&#24565;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
An evaluation of GPT models for phenotype concept recognition. (arXiv:2309.17169v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17169
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#26368;&#26032;&#30340;GPT&#27169;&#22411;&#22312;&#20020;&#24202;&#28145;&#24230;&#34920;&#22411;&#23398;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#30446;&#21069;&#36825;&#20123;&#27169;&#22411;&#23578;&#26410;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#65306;&#20020;&#24202;&#28145;&#24230;&#34920;&#22411;&#23398;&#22312;&#32597;&#35265;&#30142;&#30149;&#24739;&#32773;&#30340;&#35786;&#26029;&#20197;&#21450;&#26500;&#24314;&#21327;&#35843;&#25252;&#29702;&#35745;&#21010;&#20013;&#36215;&#21040;&#20851;&#38190;&#20316;&#29992;&#12290;&#35813;&#36807;&#31243;&#20381;&#36182;&#20110;&#20351;&#29992;&#26412;&#20307;&#27010;&#24565;&#23545;&#24739;&#32773;&#26723;&#26696;&#36827;&#34892;&#24314;&#27169;&#21644;&#25972;&#29702;&#65292;&#36890;&#24120;&#20351;&#29992;&#20154;&#31867;&#34920;&#22411;&#26412;&#20307;&#36827;&#34892;&#12290;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#24050;&#34987;&#24191;&#27867;&#37319;&#29992;&#26469;&#25903;&#25345;&#36825;&#39033;&#34920;&#22411;&#27010;&#24565;&#35782;&#21035;&#20219;&#21153;&#12290;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22823;&#22810;&#25968;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#26174;&#33879;&#24212;&#29992;&#65292;&#25105;&#20204;&#22312;&#26412;&#30740;&#31350;&#20013;&#35780;&#20272;&#20102;&#26368;&#26032;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#65288;GPT&#65289;&#27169;&#22411;&#22312;&#20020;&#24202;&#28145;&#24230;&#34920;&#22411;&#23398;&#20013;&#30340;&#24615;&#33021;&#12290;&#26448;&#26009;&#21644;&#26041;&#27861;&#65306;&#30740;&#31350;&#30340;&#23454;&#39564;&#35774;&#32622;&#21253;&#25324;&#19971;&#20010;&#19981;&#21516;&#29305;&#24322;&#24615;&#32423;&#21035;&#30340;&#25552;&#31034;&#12289;&#20004;&#20010;GPT&#27169;&#22411;&#65288;gpt-3.5&#21644;gpt-4.0&#65289;&#20197;&#21450;&#19968;&#20010;&#24050;&#24314;&#31435;&#30340;&#34920;&#22411;&#35782;&#21035;&#40644;&#37329;&#26631;&#20934;&#12290;&#32467;&#26524;&#65306;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#30446;&#21069;&#36825;&#20123;&#27169;&#22411;&#23578;&#26410;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#26368;&#20339;&#36816;&#34892;&#32467;&#26524;&#37319;&#29992;&#20102;&#23569;&#37327;&#26679;&#26412;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#26368;&#20339;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Objective: Clinical deep phenotyping plays a critical role in both the diagnosis of patients with rare disorders as well as in building care coordination plans. The process relies on modelling and curating patient profiles using ontology concepts, usually from the Human Phenotype Ontology. Machine learning methods have been widely adopted to support this phenotype concept recognition task. With the significant shift in the use of large language models (LLMs) for most NLP tasks, herewithin, we examine the performance of the latest Generative Pre-trained Transformer (GPT) models underpinning ChatGPT in clinical deep phenotyping. Materials and Methods: The experimental setup of the study included seven prompts of various levels of specificity, two GPT models (gpt-3.5 and gpt-4.0) and an established gold standard for phenotype recognition. Results: Our results show that, currently, these models have not yet achieved state of the art performance. The best run, using few-shots learning, achi
&lt;/p&gt;</description></item><item><title>DyVal&#26159;&#19968;&#31181;&#22522;&#20110;&#22270;&#24418;&#20449;&#24687;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21160;&#24577;&#35780;&#20272;&#21327;&#35758;&#65292;&#36890;&#36807;&#21160;&#24577;&#29983;&#25104;&#20855;&#26377;&#21487;&#25511;&#22797;&#26434;&#24615;&#30340;&#35780;&#20272;&#26679;&#26412;&#65292;&#35780;&#20272;&#20102;&#21508;&#31181;LLM&#22312;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#36825;&#20123;&#25361;&#25112;&#24615;&#26679;&#26412;&#19978;&#34920;&#29616;&#26356;&#24046;&#12290;</title><link>http://arxiv.org/abs/2309.17167</link><description>&lt;p&gt;
DyVal: &#22522;&#20110;&#22270;&#24418;&#20449;&#24687;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21160;&#24577;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
DyVal: Graph-informed Dynamic Evaluation of Large Language Models. (arXiv:2309.17167v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17167
&lt;/p&gt;
&lt;p&gt;
DyVal&#26159;&#19968;&#31181;&#22522;&#20110;&#22270;&#24418;&#20449;&#24687;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21160;&#24577;&#35780;&#20272;&#21327;&#35758;&#65292;&#36890;&#36807;&#21160;&#24577;&#29983;&#25104;&#20855;&#26377;&#21487;&#25511;&#22797;&#26434;&#24615;&#30340;&#35780;&#20272;&#26679;&#26412;&#65292;&#35780;&#20272;&#20102;&#21508;&#31181;LLM&#22312;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#36825;&#20123;&#25361;&#25112;&#24615;&#26679;&#26412;&#19978;&#34920;&#29616;&#26356;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#35780;&#20272;&#22522;&#20934;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20154;&#20204;&#23545;&#23427;&#20204;&#30340;&#24615;&#33021;&#25552;&#20986;&#20102;&#25285;&#24551;&#65292;&#22240;&#20026;&#23427;&#20204;&#24222;&#22823;&#30340;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#21487;&#33021;&#23384;&#22312;&#25968;&#25454;&#27745;&#26579;&#12290;&#27492;&#22806;&#65292;&#24403;&#21069;&#22522;&#20934;&#30340;&#38745;&#24577;&#24615;&#36136;&#21644;&#22266;&#23450;&#22797;&#26434;&#24615;&#21487;&#33021;&#26080;&#27861;&#20805;&#20998;&#34913;&#37327;LLM&#30340;&#36827;&#27493;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;DyVal&#65292;&#19968;&#31181;&#26032;&#39062;&#12289;&#36890;&#29992;&#19988;&#28789;&#27963;&#30340;&#21160;&#24577;&#35780;&#20272;LLM&#30340;&#21327;&#35758;&#12290;&#22522;&#20110;&#25105;&#20204;&#25552;&#20986;&#30340;&#21160;&#24577;&#35780;&#20272;&#26694;&#26550;&#65292;&#25105;&#20204;&#21033;&#29992;&#26377;&#21521;&#26080;&#29615;&#22270;&#30340;&#32467;&#26500;&#20248;&#21183;&#26500;&#24314;&#20102;&#22522;&#20110;&#22270;&#24418;&#20449;&#24687;&#30340;DyVal&#65292;&#20197;&#21160;&#24577;&#29983;&#25104;&#20855;&#26377;&#21487;&#25511;&#22797;&#26434;&#24615;&#30340;&#35780;&#20272;&#26679;&#26412;&#12290;DyVal&#29983;&#25104;&#20102;&#21253;&#25324;&#25968;&#23398;&#12289;&#36923;&#36753;&#25512;&#29702;&#21644;&#31639;&#27861;&#38382;&#39064;&#22312;&#20869;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25512;&#29702;&#20219;&#21153;&#30340;&#35780;&#20272;&#38598;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20174;Flan-T5-large&#21040;ChatGPT&#21644;GPT4&#30340;&#21508;&#31181;LLM&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;LLM&#22312;DyVal&#29983;&#25104;&#30340;&#35780;&#20272;&#26679;&#26412;&#19978;&#34920;&#29616;&#26356;&#24046;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have achieved remarkable performance in various evaluation benchmarks. However, concerns about their performance are raised on potential data contamination in their considerable volume of training corpus. Moreover, the static nature and fixed complexity of current benchmarks may inadequately gauge the advancing capabilities of LLMs. In this paper, we introduce DyVal, a novel, general, and flexible evaluation protocol for dynamic evaluation of LLMs. Based on our proposed dynamic evaluation framework, we build graph-informed DyVal by leveraging the structural advantage of directed acyclic graphs to dynamically generate evaluation samples with controllable complexities. DyVal generates challenging evaluation sets on reasoning tasks including mathematics, logical reasoning, and algorithm problems. We evaluate various LLMs ranging from Flan-T5-large to ChatGPT and GPT4. Experiments demonstrate that LLMs perform worse in DyVal-generated evaluation samples with di
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23494;&#38598;&#23454;&#20363;&#20998;&#21106;&#30340;&#32958;&#33039;&#27963;&#26816;&#32467;&#26500;&#35780;&#20272;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#33258;&#21160;&#32479;&#35745;&#35299;&#21078;&#32467;&#26500;&#19978;&#30340;&#32479;&#35745;&#25968;&#25454;&#65292;&#20174;&#32780;&#20943;&#23569;&#24037;&#20316;&#37327;&#21644;&#35266;&#23519;&#32773;&#38388;&#21464;&#24322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.17166</link><description>&lt;p&gt;
&#36890;&#36807;&#23494;&#38598;&#23454;&#20363;&#20998;&#21106;&#22312;&#32958;&#33039;&#27963;&#26816;&#32467;&#26500;&#35780;&#20272;&#26041;&#38754;&#30340;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;
Advances in Kidney Biopsy Structural Assessment through Dense Instance Segmentation. (arXiv:2309.17166v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17166
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23494;&#38598;&#23454;&#20363;&#20998;&#21106;&#30340;&#32958;&#33039;&#27963;&#26816;&#32467;&#26500;&#35780;&#20272;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#33258;&#21160;&#32479;&#35745;&#35299;&#21078;&#32467;&#26500;&#19978;&#30340;&#32479;&#35745;&#25968;&#25454;&#65292;&#20174;&#32780;&#20943;&#23569;&#24037;&#20316;&#37327;&#21644;&#35266;&#23519;&#32773;&#38388;&#21464;&#24322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32958;&#33039;&#27963;&#26816;&#26159;&#32958;&#33039;&#30142;&#30149;&#35786;&#26029;&#30340;&#37329;&#26631;&#20934;&#12290;&#19987;&#23478;&#32958;&#33039;&#30149;&#29702;&#23398;&#23478;&#21046;&#23450;&#30340;&#30149;&#21464;&#35780;&#20998;&#26159;&#21322;&#23450;&#37327;&#30340;&#65292;&#24182;&#19988;&#23384;&#22312;&#39640;&#30340;&#35266;&#23519;&#32773;&#38388;&#21464;&#24322;&#24615;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#23545;&#20998;&#21106;&#30340;&#35299;&#21078;&#23545;&#35937;&#36827;&#34892;&#33258;&#21160;&#32479;&#35745;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#24037;&#20316;&#37327;&#21644;&#36825;&#31181;&#35266;&#23519;&#32773;&#38388;&#21464;&#24322;&#24615;&#12290;&#28982;&#32780;&#65292;&#27963;&#26816;&#30340;&#23454;&#20363;&#20998;&#21106;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#21407;&#22240;&#26377;&#65306;&#65288;a&#65289;&#24179;&#22343;&#25968;&#37327;&#36739;&#22823;&#65288;&#32422;300&#33267;1000&#20010;&#65289;&#23494;&#38598;&#25509;&#35302;&#30340;&#35299;&#21078;&#32467;&#26500;&#65292;&#65288;b&#65289;&#20855;&#26377;&#22810;&#20010;&#31867;&#21035;&#65288;&#33267;&#23569;3&#20010;&#65289;&#65292;&#65288;c&#65289;&#23610;&#23544;&#21644;&#24418;&#29366;&#21508;&#24322;&#12290;&#30446;&#21069;&#20351;&#29992;&#30340;&#23454;&#20363;&#20998;&#21106;&#27169;&#22411;&#19981;&#33021;&#20197;&#39640;&#25928;&#36890;&#29992;&#30340;&#26041;&#24335;&#21516;&#26102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#19981;&#38656;&#35201;&#38170;&#28857;&#30340;&#23454;&#20363;&#20998;&#21106;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#23558;&#25193;&#25955;&#27169;&#22411;&#12289;&#21464;&#25442;&#22120;&#27169;&#22359;&#21644;RCNN&#65288;&#21306;&#22495;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65289;&#32467;&#21512;&#36215;&#26469;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#19968;&#21488;NVIDIA GeForce RTX 3090 GPU&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20294;&#21487;&#20197;&#25552;&#20379;&#21487;&#35266;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The kidney biopsy is the gold standard for the diagnosis of kidney diseases. Lesion scores made by expert renal pathologists are semi-quantitative and suffer from high inter-observer variability. Automatically obtaining statistics per segmented anatomical object, therefore, can bring significant benefits in reducing labor and this inter-observer variability. Instance segmentation for a biopsy, however, has been a challenging problem due to (a) the on average large number (around 300 to 1000) of densely touching anatomical structures, (b) with multiple classes (at least 3) and (c) in different sizes and shapes. The currently used instance segmentation models cannot simultaneously deal with these challenges in an efficient yet generic manner. In this paper, we propose the first anchor-free instance segmentation model that combines diffusion models, transformer modules, and RCNNs (regional convolution neural networks). Our model is trained on just one NVIDIA GeForce RTX 3090 GPU, but can 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#36793;&#35848;&#21028;&#20013;&#30340;&#20840;&#29699;&#22949;&#21327;&#65292;&#35299;&#37322;&#20102;&#23613;&#31649;&#32852;&#21512;&#22269;&#25945;&#31185;&#25991;&#32452;&#32455;&#25104;&#21592;&#22269;&#20195;&#34920;&#20102;&#22810;&#31181;&#20559;&#22909;&#65292;&#20294;&#22312;&#20154;&#24037;&#26234;&#33021;&#35268;&#33539;&#26041;&#38754;&#22914;&#20309;&#23454;&#29616;&#20840;&#29699;&#22949;&#21327;&#12290;&#36890;&#36807;&#29420;&#29305;&#30340;&#20027;&#35201;&#26469;&#28304;&#65292;&#26412;&#25991;&#25581;&#31034;&#20102;&#23454;&#29616;&#25240;&#34935;&#30340;&#23454;&#36341;&#12290;</title><link>http://arxiv.org/abs/2309.17158</link><description>&lt;p&gt;
&#22810;&#36793;&#35848;&#21028;&#20013;&#30340;&#25240;&#34935;&#19982;&#20840;&#29699;&#20154;&#24037;&#26234;&#33021;&#35268;&#33539;
&lt;/p&gt;
&lt;p&gt;
Compromise in Multilateral Negotiations and the Global Regulation of Artificial Intelligence. (arXiv:2309.17158v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17158
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#36793;&#35848;&#21028;&#20013;&#30340;&#20840;&#29699;&#22949;&#21327;&#65292;&#35299;&#37322;&#20102;&#23613;&#31649;&#32852;&#21512;&#22269;&#25945;&#31185;&#25991;&#32452;&#32455;&#25104;&#21592;&#22269;&#20195;&#34920;&#20102;&#22810;&#31181;&#20559;&#22909;&#65292;&#20294;&#22312;&#20154;&#24037;&#26234;&#33021;&#35268;&#33539;&#26041;&#38754;&#22914;&#20309;&#23454;&#29616;&#20840;&#29699;&#22949;&#21327;&#12290;&#36890;&#36807;&#29420;&#29305;&#30340;&#20027;&#35201;&#26469;&#28304;&#65292;&#26412;&#25991;&#25581;&#31034;&#20102;&#23454;&#29616;&#25240;&#34935;&#30340;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#25216;&#26415;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#30340;&#20256;&#25773;&#65292;&#22269;&#38469;&#35752;&#35770;&#36234;&#26469;&#36234;&#38598;&#20013;&#20110;AI&#23545;&#27665;&#20027;&#12289;&#20154;&#26435;&#12289;&#22522;&#26412;&#33258;&#30001;&#12289;&#23433;&#20840;&#20197;&#21450;&#32463;&#27982;&#21644;&#31038;&#20250;&#21457;&#23637;&#30340;&#24433;&#21709;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#32852;&#21512;&#22269;&#25945;&#31185;&#25991;&#32452;&#32455;&#20110;2021&#24180;11&#26376;&#36890;&#36807;&#30340;&#12298;&#20154;&#24037;&#26234;&#33021;&#20262;&#29702;&#24314;&#35758;&#12299;&#24050;&#25104;&#20026;&#20154;&#24037;&#26234;&#33021;&#21457;&#23637;&#21644;&#37096;&#32626;&#30340;&#39318;&#20010;&#20840;&#29699;&#35268;&#33539;&#26694;&#26550;&#12290;&#26412;&#25991;&#36890;&#36807;&#29420;&#29305;&#30340;&#19968;&#32452;&#20027;&#35201;&#26469;&#28304;&#65292;&#21253;&#25324;&#20070;&#38754;&#31435;&#22330;&#21644;&#35760;&#24405;&#30340;&#35752;&#35770;&#65292;&#35299;&#37322;&#20102;&#23613;&#31649;&#32852;&#21512;&#22269;&#25945;&#31185;&#25991;&#32452;&#32455;&#25104;&#21592;&#22269;&#20195;&#34920;&#20102;&#22810;&#31181;&#33258;&#30001;&#20027;&#20041;&#21644;&#20027;&#26435;&#20027;&#20041;&#20559;&#22909;&#30340;&#31435;&#22330;&#65292;&#20294;&#22312;&#20154;&#24037;&#26234;&#33021;&#35268;&#33539;&#26041;&#38754;&#22914;&#20309;&#23454;&#29616;&#20840;&#29699;&#22949;&#21327;&#12290;&#20511;&#37492;&#21338;&#23572;&#21776;&#26031;&#22522;&#30340;&#23454;&#29992;&#31038;&#20250;&#23398;&#65292;&#26412;&#25991;&#23545;&#22810;&#36793;&#35848;&#21028;&#23454;&#36341;&#36827;&#34892;&#20102;&#27010;&#24565;&#21270;&#65292;&#24182;&#23558;&#22810;&#36793;&#22949;&#21327;&#24402;&#22240;&#20110;&#21508;&#26041;&#30340;&#23454;&#38469;&#20570;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
As artificial intelligence (AI) technologies spread worldwide, international discussions have increasingly focused on their consequences for democracy, human rights, fundamental freedoms, security, and economic and social development. In this context, UNESCO's Recommendation on the Ethics of Artificial Intelligence, adopted in November 2021, has emerged as the first global normative framework for AI development and deployment. The intense negotiations of every detail of the document brought forth numerous controversies among UNESCO member states. Drawing on a unique set of primary sources, including written positions and recorded deliberations, this paper explains the achievement of global compromise on AI regulation despite the multiplicity of UNESCO member-state positions representing a variety of liberal and sovereignist preferences. Building upon Boltanski's pragmatic sociology, it conceptualises the practice of multilateral negotiations and attributes the multilateral compromise t
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#25163;&#20889;&#25968;&#25454;&#21644;&#35745;&#31639;&#30456;&#20851;&#25351;&#26631;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20202;&#22120;&#21270;&#22696;&#27700;&#31508;&#23545;&#32769;&#40836;&#20154;&#32676;&#36827;&#34892;&#24180;&#40836;&#20998;&#31867;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.17156</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#30001;&#25163;&#20889;&#25351;&#26631;&#36827;&#34892;&#24180;&#40836;&#32676;&#20307;&#27495;&#35270;
&lt;/p&gt;
&lt;p&gt;
Age Group Discrimination via Free Handwriting Indicators. (arXiv:2309.17156v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17156
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#25163;&#20889;&#25968;&#25454;&#21644;&#35745;&#31639;&#30456;&#20851;&#25351;&#26631;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20202;&#22120;&#21270;&#22696;&#27700;&#31508;&#23545;&#32769;&#40836;&#20154;&#32676;&#36827;&#34892;&#24180;&#40836;&#20998;&#31867;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20840;&#29699;&#32769;&#40836;&#20154;&#21475;&#30340;&#22686;&#38271;&#65292;&#32769;&#24369;&#32676;&#20307;&#30340;&#24739;&#30149;&#29575;&#39044;&#35745;&#23558;&#22686;&#21152;&#65292;&#32473;&#21307;&#30103;&#31995;&#32479;&#24102;&#26469;&#37325;&#22823;&#25361;&#25112;&#12290;&#32769;&#24369;&#26159;&#19968;&#31181;&#19982;&#34928;&#32769;&#30456;&#20851;&#30340;&#32508;&#21512;&#24449;&#65292;&#20854;&#29305;&#24449;&#26159;&#20581;&#24247;&#36880;&#28176;&#19979;&#38477;&#65292;&#23545;&#21387;&#21147;&#30340;&#33030;&#24369;&#24615;&#22686;&#21152;&#65292;&#27515;&#20129;&#39118;&#38505;&#22686;&#21152;&#12290;&#32769;&#24369;&#23545;&#20844;&#20849;&#21355;&#29983;&#36896;&#25104;&#20102;&#37325;&#22823;&#36127;&#25285;&#65292;&#38477;&#20302;&#20102;&#24739;&#32773;&#29983;&#27963;&#36136;&#37327;&#12290;&#30446;&#21069;&#32570;&#20047;&#19968;&#31181;&#26222;&#36941;&#25509;&#21463;&#30340;&#35780;&#20272;&#32769;&#24369;&#31243;&#24230;&#30340;&#26041;&#27861;&#21644;&#26631;&#20934;&#21270;&#23450;&#20041;&#65292;&#36825;&#31361;&#26174;&#20102;&#19968;&#20010;&#20851;&#38190;&#30340;&#30740;&#31350;&#32570;&#21475;&#12290;&#37492;&#20110;&#36825;&#19968;&#32570;&#21475;&#21644;&#26089;&#26399;&#39044;&#38450;&#30340;&#37325;&#35201;&#24615;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#20202;&#22120;&#21270;&#22696;&#27700;&#31508;&#23545;&#25163;&#20889;&#36827;&#34892;&#29983;&#24577;&#23398;&#35780;&#20272;&#65292;&#20197;&#23545;&#19981;&#21516;&#24180;&#40836;&#32676;&#20307;&#36827;&#34892;&#20998;&#31867;&#12290;&#20998;&#26512;&#20102;&#26469;&#33258;80&#21517;&#19981;&#21516;&#24180;&#40836;&#32452;&#65288;20-40&#23681;&#12289;41-60&#23681;&#12289;61-70&#23681;&#21644;70+&#23681;&#65289;&#30340;&#20581;&#24247;&#21442;&#19982;&#32773;&#30340;&#38750;&#22270;&#20687;&#25163;&#20889;&#25968;&#25454;&#12290;&#20174;&#21407;&#22987;&#25968;&#25454;&#20013;&#35745;&#31639;&#20102;14&#20010;&#19982;&#25163;&#21183;&#21644;&#38663;&#39076;&#30456;&#20851;&#30340;&#25351;&#26631;&#65292;&#24182;&#22312;&#20116;&#20010;&#20998;&#31867;&#20219;&#21153;&#20013;&#20351;&#29992;&#20102;&#36825;&#20123;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
The growing global elderly population is expected to increase the prevalence of frailty, posing significant challenges to healthcare systems. Frailty, a syndrome associated with ageing, is characterised by progressive health decline, increased vulnerability to stressors and increased risk of mortality. It represents a significant burden on public health and reduces the quality of life of those affected. The lack of a universally accepted method to assess frailty and a standardised definition highlights a critical research gap. Given this lack and the importance of early prevention, this study presents an innovative approach using an instrumented ink pen to ecologically assess handwriting for age group classification. Content-free handwriting data from 80 healthy participants in different age groups (20-40, 41-60, 61-70 and 70+) were analysed. Fourteen gesture- and tremor-related indicators were computed from the raw data and used in five classification tasks. These tasks included discr
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#23450;&#24615;&#20998;&#26512;&#36827;&#34892;&#27880;&#37322;&#21487;&#33021;&#24341;&#20837;&#20559;&#35265;&#21644;&#27979;&#37327;&#35823;&#24046;&#65292;&#30456;&#27604;&#20043;&#19979;&#65292;&#20351;&#29992;&#39640;&#36136;&#37327;&#30340;&#20154;&#24037;&#27880;&#37322;&#21644;&#28789;&#27963;&#32534;&#30721;&#23545;&#31616;&#21333;&#30417;&#30563;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#21487;&#20197;&#26356;&#22909;&#22320;&#20943;&#23569;&#20559;&#35265;&#21644;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2309.17147</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23450;&#24615;&#20998;&#26512;&#21487;&#33021;&#24341;&#20837;&#20005;&#37325;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Using Large Language Models for Qualitative Analysis can Introduce Serious Bias. (arXiv:2309.17147v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17147
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#23450;&#24615;&#20998;&#26512;&#36827;&#34892;&#27880;&#37322;&#21487;&#33021;&#24341;&#20837;&#20559;&#35265;&#21644;&#27979;&#37327;&#35823;&#24046;&#65292;&#30456;&#27604;&#20043;&#19979;&#65292;&#20351;&#29992;&#39640;&#36136;&#37327;&#30340;&#20154;&#24037;&#27880;&#37322;&#21644;&#28789;&#27963;&#32534;&#30721;&#23545;&#31616;&#21333;&#30417;&#30563;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#21487;&#20197;&#26356;&#22909;&#22320;&#20943;&#23569;&#20559;&#35265;&#21644;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#27491;&#22312;&#36805;&#36895;&#26222;&#21450;&#65292;&#20294;&#23545;&#31038;&#20250;&#31185;&#23398;&#30740;&#31350;&#30340;&#24433;&#21709;&#23578;&#26410;&#34987;&#24456;&#22909;&#29702;&#35299;&#12290;&#26412;&#25991;&#25506;&#35752;LLMs&#26159;&#21542;&#33021;&#22815;&#24110;&#21161;&#25105;&#20204;&#20998;&#26512;&#22823;&#37327;&#24320;&#25918;&#24615;&#38754;&#35848;&#25968;&#25454;&#65292;&#20197;&#32599;&#20852;&#20122;&#38590;&#27665;&#22312;&#23391;&#21152;&#25289;&#22269;&#31185;&#20811;&#26031;&#24052;&#25166;&#30340;&#35775;&#35848;&#35760;&#24405;&#20026;&#24212;&#29992;&#26696;&#20363;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#20351;&#29992;LLMs&#23545;&#25991;&#26412;&#36827;&#34892;&#27880;&#37322;&#26102;&#38656;&#35201;&#38750;&#24120;&#35880;&#24910;&#65292;&#22240;&#20026;&#23384;&#22312;&#24341;&#20837;&#20559;&#35265;&#30340;&#39118;&#38505;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#35823;&#23548;&#24615;&#25512;&#26029;&#12290;&#25105;&#20204;&#36825;&#37324;&#25152;&#25351;&#30340;&#20559;&#35265;&#26159;&#25216;&#26415;&#24847;&#20041;&#19978;&#30340;&#65292;&#21363;LLMs&#22312;&#27880;&#37322;&#35775;&#35848;&#35760;&#24405;&#26102;&#30340;&#38169;&#35823;&#19981;&#26159;&#19982;&#35775;&#35848;&#23545;&#35937;&#30340;&#29305;&#24449;&#26080;&#20851;&#30340;&#38543;&#26426;&#35823;&#24046;&#12290;&#36890;&#36807;&#20351;&#29992;&#39640;&#36136;&#37327;&#30340;&#20154;&#24037;&#27880;&#37322;&#21644;&#28789;&#27963;&#32534;&#30721;&#23545;&#31616;&#21333;&#30417;&#30563;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#21487;&#20197;&#20943;&#23569;&#27979;&#37327;&#35823;&#24046;&#21644;&#20559;&#35265;&#65292;&#20248;&#20110;LLMs&#30340;&#27880;&#37322;&#12290;&#22240;&#27492;&#65292;&#32771;&#34385;&#21040;&#24517;&#39035;&#26377;&#19968;&#20123;&#39640;&#36136;&#37327;&#30340;&#27880;&#37322;&#20197;&#35780;&#20272;LLM&#26159;&#21542;&#24341;&#20837;&#20559;&#35265;&#65292;&#25105;&#20204;&#35748;&#20026;&#26368;&#22909;&#30340;&#36873;&#25321;&#21487;&#33021;&#26159;&#20351;&#29992;&#36739;&#31616;&#21333;&#30340;&#30417;&#30563;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are quickly becoming ubiquitous, but the implications for social science research are not yet well understood. This paper asks whether LLMs can help us analyse large-N qualitative data from open-ended interviews, with an application to transcripts of interviews with Rohingya refugees in Cox's Bazaar, Bangladesh. We find that a great deal of caution is needed in using LLMs to annotate text as there is a risk of introducing biases that can lead to misleading inferences. We here mean bias in the technical sense, that the errors that LLMs make in annotating interview transcripts are not random with respect to the characteristics of the interview subjects. Training simpler supervised models on high-quality human annotations with flexible coding leads to less measurement error and bias than LLM annotations. Therefore, given that some high quality annotations are necessary in order to asses whether an LLM introduces bias, we argue that it is probably preferable to
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#21407;&#22411;&#29983;&#25104;&#65292;&#19968;&#31181;&#38024;&#23545;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#25968;&#25454;&#26080;&#20851;&#35299;&#37322;&#24615;&#30340;&#26356;&#20005;&#26684;&#21644;&#31283;&#20581;&#30340;&#29305;&#24449;&#21487;&#35270;&#21270;&#26041;&#27861;&#12290;&#36890;&#36807;&#29983;&#25104;&#33258;&#28982;&#28608;&#27963;&#36335;&#24452;&#30340;&#36755;&#20837;&#26469;&#21453;&#39539;&#20043;&#21069;&#19981;&#21487;&#20449;&#30340;&#29305;&#24449;&#21487;&#35270;&#21270;&#31639;&#27861;&#30340;&#35266;&#28857;&#65292;&#24182;&#36890;&#36807;&#23450;&#37327;&#27979;&#37327;&#29983;&#25104;&#30340;&#21407;&#22411;&#30340;&#20869;&#37096;&#28608;&#27963;&#19982;&#33258;&#28982;&#22270;&#20687;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#26469;&#35777;&#23454;&#36825;&#19968;&#28857;&#12290;&#35299;&#37322;&#29983;&#25104;&#30340;&#21407;&#22411;&#21487;&#20197;&#25581;&#31034;&#27169;&#22411;&#23398;&#20064;&#21040;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#21644;&#20559;&#35265;&#65292;&#36825;&#26159;&#23450;&#37327;&#26041;&#27861;&#26080;&#27861;&#35782;&#21035;&#30340;&#12290;</title><link>http://arxiv.org/abs/2309.17144</link><description>&lt;p&gt;
&#21407;&#22411;&#29983;&#25104;: &#38024;&#23545;&#25968;&#25454;&#26080;&#20851;&#35299;&#37322;&#24615;&#30340;&#31283;&#20581;&#29305;&#24449;&#21487;&#35270;&#21270;
&lt;/p&gt;
&lt;p&gt;
Prototype Generation: Robust Feature Visualisation for Data Independent Interpretability. (arXiv:2309.17144v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17144
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#21407;&#22411;&#29983;&#25104;&#65292;&#19968;&#31181;&#38024;&#23545;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#25968;&#25454;&#26080;&#20851;&#35299;&#37322;&#24615;&#30340;&#26356;&#20005;&#26684;&#21644;&#31283;&#20581;&#30340;&#29305;&#24449;&#21487;&#35270;&#21270;&#26041;&#27861;&#12290;&#36890;&#36807;&#29983;&#25104;&#33258;&#28982;&#28608;&#27963;&#36335;&#24452;&#30340;&#36755;&#20837;&#26469;&#21453;&#39539;&#20043;&#21069;&#19981;&#21487;&#20449;&#30340;&#29305;&#24449;&#21487;&#35270;&#21270;&#31639;&#27861;&#30340;&#35266;&#28857;&#65292;&#24182;&#36890;&#36807;&#23450;&#37327;&#27979;&#37327;&#29983;&#25104;&#30340;&#21407;&#22411;&#30340;&#20869;&#37096;&#28608;&#27963;&#19982;&#33258;&#28982;&#22270;&#20687;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#26469;&#35777;&#23454;&#36825;&#19968;&#28857;&#12290;&#35299;&#37322;&#29983;&#25104;&#30340;&#21407;&#22411;&#21487;&#20197;&#25581;&#31034;&#27169;&#22411;&#23398;&#20064;&#21040;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#21644;&#20559;&#35265;&#65292;&#36825;&#26159;&#23450;&#37327;&#26041;&#27861;&#26080;&#27861;&#35782;&#21035;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#21407;&#22411;&#29983;&#25104;&#65292;&#36825;&#26159;&#19968;&#31181;&#23545;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#36827;&#34892;&#27169;&#22411;&#26080;&#20851;&#12289;&#25968;&#25454;&#26080;&#20851;&#35299;&#37322;&#24615;&#30340;&#26356;&#20005;&#26684;&#21644;&#26356;&#31283;&#20581;&#30340;&#29305;&#24449;&#21487;&#35270;&#21270;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#33021;&#22815;&#29983;&#25104;&#23548;&#33268;&#33258;&#28982;&#28608;&#27963;&#36335;&#24452;&#30340;&#36755;&#20837;&#65292;&#20174;&#32780;&#21453;&#39539;&#20102;&#20043;&#21069;&#22768;&#31216;&#29305;&#24449;&#21487;&#35270;&#21270;&#31639;&#27861;&#19981;&#21487;&#20449;&#30340;&#35266;&#28857;&#65292;&#21407;&#22240;&#26159;&#30001;&#20110;&#20854;&#20869;&#37096;&#28608;&#27963;&#26159;&#19981;&#33258;&#28982;&#30340;&#12290;&#25105;&#20204;&#36890;&#36807;&#23450;&#37327;&#27979;&#37327;&#25105;&#20204;&#29983;&#25104;&#30340;&#21407;&#22411;&#30340;&#20869;&#37096;&#28608;&#27963;&#19982;&#33258;&#28982;&#22270;&#20687;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#26469;&#35777;&#23454;&#36825;&#20123;&#35266;&#28857;&#12290;&#25105;&#20204;&#36824;&#28436;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#35299;&#37322;&#29983;&#25104;&#30340;&#21407;&#22411;&#26469;&#33719;&#24471;&#37325;&#35201;&#30340;&#27934;&#23519;&#65292;&#31361;&#20986;&#26174;&#31034;&#20986;&#23450;&#37327;&#26041;&#27861;&#26080;&#27861;&#35782;&#21035;&#30340;&#27169;&#22411;&#23398;&#20064;&#21040;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#21644;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Prototype Generation, a stricter and more robust form of feature visualisation for model-agnostic, data-independent interpretability of image classification models. We demonstrate its ability to generate inputs that result in natural activation paths, countering previous claims that feature visualisation algorithms are untrustworthy due to the unnatural internal activations. We substantiate these claims by quantitatively measuring similarity between the internal activations of our generated prototypes and natural images. We also demonstrate how the interpretation of generated prototypes yields important insights, highlighting spurious correlations and biases learned by models which quantitative methods over test-sets cannot identify.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#19982;&#39045;&#20391;&#23450;&#20301;&#30340;&#20851;&#31995;&#65292;&#24182;&#24320;&#21457;&#20986;&#22522;&#20110;&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#30340;&#31283;&#20581;&#22522;&#20934;&#65292;&#20197;&#25552;&#39640;&#39045;&#20391;&#23450;&#20301;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.17143</link><description>&lt;p&gt;
&#20174;&#36731;&#37327;&#32423;&#36229;&#20998;&#36776;&#22836;&#37096;&#30340;&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#35282;&#24230;&#37325;&#26032;&#23457;&#35270;&#39045;&#20391;&#23450;&#20301;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
Revisiting Cephalometric Landmark Detection from the view of Human Pose Estimation with Lightweight Super-Resolution Head. (arXiv:2309.17143v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17143
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#19982;&#39045;&#20391;&#23450;&#20301;&#30340;&#20851;&#31995;&#65292;&#24182;&#24320;&#21457;&#20986;&#22522;&#20110;&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#30340;&#31283;&#20581;&#22522;&#20934;&#65292;&#20197;&#25552;&#39640;&#39045;&#20391;&#23450;&#20301;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39045;&#20391;&#23450;&#20301;&#30340;&#31934;&#30830;&#23450;&#20301;&#22312;&#27491;&#30072;&#23398;&#21644;&#27491;&#39052;&#23398;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#22240;&#20854;&#26377;&#28508;&#21147;&#33258;&#21160;&#21270;&#20851;&#38190;&#28857;&#26631;&#35760;&#12290;&#22312;&#39045;&#20391;&#23450;&#20301;&#23588;&#20854;&#26159;&#39045;&#20391;&#27979;&#37327;&#26041;&#38754;&#65292;&#24050;&#32463;&#35266;&#23519;&#21040;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#32570;&#20047;&#26631;&#20934;&#21270;&#30340;&#27969;&#31243;&#21644;&#35774;&#35745;&#21512;&#29702;&#30340;&#20559;&#24046;&#20943;&#23569;&#36807;&#31243;&#65292;&#36825;&#23545;&#23427;&#20204;&#30340;&#24615;&#33021;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#30456;&#20851;&#20219;&#21153;&#8212;&#8212;&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#65288;HPE&#65289;&#65292;&#23427;&#19982;&#39045;&#20391;&#23450;&#20301;&#30340;&#30456;&#20284;&#20043;&#22788;&#24456;&#22810;&#65292;&#24182;&#24378;&#35843;&#20174;&#21069;&#32773;&#39046;&#22495;&#20013;&#36716;&#31227;&#25216;&#26415;&#20197;&#26377;&#30410;&#20110;&#21518;&#32773;&#30340;&#28508;&#21147;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#30693;&#21517;HPE&#20195;&#30721;&#24211;MMPose&#30340;&#31283;&#20581;&#19988;&#21487;&#36866;&#24212;&#30340;&#22522;&#20934;&#65292;&#35813;&#22522;&#20934;&#21487;&#20197;&#20316;&#20026;&#23454;&#29616;&#20986;&#33394;&#39045;&#20391;&#23450;&#20301;&#24615;&#33021;&#30340;&#21487;&#38752;&#22522;&#20934;&#32447;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#26694;&#26550;&#20013;&#24341;&#20837;&#20102;&#19968;&#20010;&#25552;&#21319;&#35774;&#35745;&#65292;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate localization of cephalometric landmarks holds great importance in the fields of orthodontics and orthognathics due to its potential for automating key point labeling. In the context of landmark detection, particularly in cephalometrics, it has been observed that existing methods often lack standardized pipelines and well-designed bias reduction processes, which significantly impact their performance. In this paper, we revisit a related task, human pose estimation (HPE), which shares numerous similarities with cephalometric landmark detection (CLD), and emphasize the potential for transferring techniques from the former field to benefit the latter. Motivated by this insight, we have developed a robust and adaptable benchmark based on the well-established HPE codebase known as MMPose. This benchmark can serve as a dependable baseline for achieving exceptional CLD performance. Furthermore, we introduce an upscaling design within the framework to further enhance performance. This 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;&#21508;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;RDF&#30693;&#35782;&#22270;&#21019;&#24314;&#21644;&#29702;&#35299;&#20013;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#32452;&#20219;&#21153;&#26469;&#25506;&#31350;&#27169;&#22411;&#30340;&#35299;&#26512;&#12289;&#29702;&#35299;&#12289;&#20998;&#26512;&#21644;&#21019;&#24314;&#33021;&#21147;&#65292;&#24182;&#19988;&#23545;&#21830;&#19994;&#21487;&#29992;&#21644;&#20813;&#36153;&#30340;&#31163;&#32447;&#27169;&#22411;&#36827;&#34892;&#20102;&#35780;&#20272;&#21644;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2309.17122</link><description>&lt;p&gt;
&#22522;&#20934;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;RDF&#30693;&#35782;&#22270;&#21019;&#24314;&#21644;&#29702;&#35299;&#20013;&#30340;&#33021;&#21147;&#65306;LLMs&#22914;&#20309;&#36827;&#34892;Turtle&#35821;&#35328;&#35299;&#26512;&#65311;
&lt;/p&gt;
&lt;p&gt;
Benchmarking the Abilities of Large Language Models for RDF Knowledge Graph Creation and Comprehension: How Well Do LLMs Speak Turtle?. (arXiv:2309.17122v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17122
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;&#21508;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;RDF&#30693;&#35782;&#22270;&#21019;&#24314;&#21644;&#29702;&#35299;&#20013;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#32452;&#20219;&#21153;&#26469;&#25506;&#31350;&#27169;&#22411;&#30340;&#35299;&#26512;&#12289;&#29702;&#35299;&#12289;&#20998;&#26512;&#21644;&#21019;&#24314;&#33021;&#21147;&#65292;&#24182;&#19988;&#23545;&#21830;&#19994;&#21487;&#29992;&#21644;&#20813;&#36153;&#30340;&#31163;&#32447;&#27169;&#22411;&#36827;&#34892;&#20102;&#35780;&#20272;&#21644;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#32534;&#30721;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#28982;&#32780;&#23427;&#20204;&#22312;&#34920;&#31034;&#25968;&#25454;&#30340;&#24418;&#24335;&#35821;&#35328;&#20013;&#30340;&#24037;&#20316;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#30693;&#35782;&#22270;&#24037;&#31243;&#39046;&#22495;&#20013;&#65292;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#30740;&#31350;&#12290;&#20026;&#20102;&#35780;&#20272;&#21508;&#31181;LLMs&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#32452;&#20116;&#20010;&#20219;&#21153;&#65292;&#20197;&#25506;&#31350;&#23427;&#20204;&#22788;&#29702;Turtle&#35821;&#27861;&#30340;&#30693;&#35782;&#22270;&#35299;&#26512;&#12289;&#29702;&#35299;&#12289;&#20998;&#26512;&#21644;&#21019;&#24314;&#30340;&#33021;&#21147;&#12290;&#36825;&#20123;&#20219;&#21153;&#20855;&#26377;&#19981;&#21516;&#30340;&#22797;&#26434;&#24615;&#31243;&#24230;&#65292;&#24182;&#33021;&#38543;&#38382;&#39064;&#35268;&#27169;&#32780;&#25193;&#23637;&#65292;&#24050;&#38598;&#25104;&#21040;&#25105;&#20204;&#30340;&#33258;&#21160;&#35780;&#20272;&#31995;&#32479;LLM-KG-Bench&#20013;&#12290;&#35780;&#20272;&#21253;&#25324;&#22235;&#20010;&#21830;&#19994;&#21487;&#29992;&#30340;LLMs - GPT-3.5&#12289;GPT-4&#12289;Claude 1.3 &#21644; Claude 2.0&#65292;&#20197;&#21450;&#20004;&#20010;&#21487;&#20813;&#36153;&#20351;&#29992;&#30340;&#31163;&#32447;&#27169;&#22411; GPT4All Vicuna &#21644; GPT4All Falcon 13B&#12290;&#36825;&#39033;&#20998;&#26512;&#28145;&#20837;&#20102;&#35299;&#20102;LLMs&#22312;&#20854;&#22312;R&#20013;&#24212;&#29992;&#30340;&#20248;&#21183;&#21644;&#19981;&#36275;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are advancing at a rapid pace, with significant improvements at natural language processing and coding tasks. Yet, their ability to work with formal languages representing data, specifically within the realm of knowledge graph engineering, remains under-investigated. To evaluate the proficiency of various LLMs, we created a set of five tasks that probe their ability to parse, understand, analyze, and create knowledge graphs serialized in Turtle syntax. These tasks, each embodying distinct degrees of complexity and being able to scale with the size of the problem, have been integrated into our automated evaluation system, the LLM-KG-Bench. The evaluation encompassed four commercially available LLMs - GPT-3.5, GPT-4, Claude 1.3, and Claude 2.0, as well as two freely accessible offline models, GPT4All Vicuna and GPT4All Falcon 13B. This analysis offers an in-depth understanding of the strengths and shortcomings of LLMs in relation to their application within R
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#23398;&#20064;&#20855;&#26377;&#39640;&#20934;&#30830;&#24615;&#30340;&#20803;&#36335;&#24452;&#21644;&#20803;&#36335;&#24452;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#20851;&#38190;&#26159;&#20351;&#29992;&#35780;&#20998;&#20989;&#25968;&#26469;&#34913;&#37327;&#20851;&#31995;&#30340;&#28508;&#22312;&#20449;&#24687;&#37327;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#35813;&#26041;&#27861;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#27604;&#29616;&#26377;&#30340;&#22810;&#20851;&#31995;GNNs&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.17113</link><description>&lt;p&gt;
&#22810;&#20851;&#31995;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20803;&#36335;&#24452;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Meta-Path Learning for Multi-relational Graph Neural Networks. (arXiv:2309.17113v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17113
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#23398;&#20064;&#20855;&#26377;&#39640;&#20934;&#30830;&#24615;&#30340;&#20803;&#36335;&#24452;&#21644;&#20803;&#36335;&#24452;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#20851;&#38190;&#26159;&#20351;&#29992;&#35780;&#20998;&#20989;&#25968;&#26469;&#34913;&#37327;&#20851;&#31995;&#30340;&#28508;&#22312;&#20449;&#24687;&#37327;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#35813;&#26041;&#27861;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#27604;&#29616;&#26377;&#30340;&#22810;&#20851;&#31995;GNNs&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#22810;&#20851;&#31995;&#22270;&#31070;&#32463;&#32593;&#32476;&#20351;&#29992;&#20004;&#31181;&#31574;&#30053;&#26469;&#30830;&#23450;&#20449;&#24687;&#30456;&#20851;&#30340;&#20851;&#31995;&#65306;&#35201;&#20040;&#23558;&#36825;&#20010;&#38382;&#39064;&#31616;&#21270;&#20026;&#20302;&#32423;&#26435;&#37325;&#23398;&#20064;&#65292;&#35201;&#20040;&#20381;&#36182;&#20110;&#25163;&#24037;&#35774;&#35745;&#30340;&#20851;&#31995;&#20381;&#36182;&#38142;&#65292;&#31216;&#20026;&#20803;&#36335;&#24452;&#12290;&#28982;&#32780;&#65292;&#21069;&#19968;&#31181;&#26041;&#27861;&#22312;&#23384;&#22312;&#22823;&#37327;&#20851;&#31995;&#30340;&#24773;&#20917;&#19979;&#65288;&#20363;&#22914;&#65292;&#30693;&#35782;&#22270;&#35889;&#65289;&#38754;&#20020;&#25361;&#25112;&#65292;&#32780;&#21518;&#19968;&#31181;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#26469;&#30830;&#23450;&#30456;&#20851;&#30340;&#20803;&#36335;&#24452;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#23398;&#20064;&#20803;&#36335;&#24452;&#21644;&#20803;&#36335;&#24452;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#36825;&#20123;&#32593;&#32476;&#22522;&#20110;&#23569;&#37327;&#26377;&#20449;&#24687;&#37327;&#30340;&#20803;&#36335;&#24452;&#20855;&#26377;&#39640;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20851;&#38190;&#35201;&#32032;&#26159;&#19968;&#20010;&#35780;&#20998;&#20989;&#25968;&#65292;&#29992;&#20110;&#34913;&#37327;&#22312;&#20803;&#36335;&#24452;&#30340;&#22686;&#37327;&#26500;&#24314;&#20013;&#20851;&#31995;&#30340;&#28508;&#22312;&#20449;&#24687;&#37327;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;&#21363;&#20351;&#26377;&#22823;&#37327;&#20851;&#31995;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20173;&#33021;&#27491;&#30830;&#35782;&#21035;&#30456;&#20851;&#30340;&#20803;&#36335;&#24452;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#23454;&#39564;&#20013;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#22810;&#20851;&#31995;GNNs&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing multi-relational graph neural networks use one of two strategies for identifying informative relations: either they reduce this problem to low-level weight learning, or they rely on handcrafted chains of relational dependencies, called meta-paths. However, the former approach faces challenges in the presence of many relations (e.g., knowledge graphs), while the latter requires substantial domain expertise to identify relevant meta-paths. In this work we propose a novel approach to learn meta-paths and meta-path GNNs that are highly accurate based on a small number of informative meta-paths. Key element of our approach is a scoring function for measuring the potential informativeness of a relation in the incremental construction of the meta-path. Our experimental evaluation shows that the approach manages to correctly identify relevant meta-paths even with a large number of relations, and substantially outperforms existing multi-relational GNNs on synthetic and real-world exper
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;DeltaXplainer&#65292;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25551;&#36848;&#20004;&#20010;&#20108;&#20803;&#20998;&#31867;&#22120;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;DeltaXplainer&#22312;&#28041;&#21450;&#19981;&#21516;&#31867;&#22411;&#27010;&#24565;&#28418;&#31227;&#30340;&#21508;&#31181;&#27169;&#22411;&#27604;&#36739;&#22330;&#26223;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.17095</link><description>&lt;p&gt;
&#36890;&#36807;&#20915;&#31574;&#35268;&#21017;&#36827;&#34892;&#27169;&#22411;&#27604;&#36739;&#30340;&#21160;&#24577;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Dynamic Interpretability for Model Comparison via Decision Rules. (arXiv:2309.17095v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17095
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;DeltaXplainer&#65292;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25551;&#36848;&#20004;&#20010;&#20108;&#20803;&#20998;&#31867;&#22120;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;DeltaXplainer&#22312;&#28041;&#21450;&#19981;&#21516;&#31867;&#22411;&#27010;&#24565;&#28418;&#31227;&#30340;&#21508;&#31181;&#27169;&#22411;&#27604;&#36739;&#22330;&#26223;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26041;&#27861;&#22823;&#22810;&#34987;&#29992;&#26469;&#30740;&#31350;&#21644;&#38416;&#26126;&#21333;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#27809;&#26377;&#34987;&#35774;&#35745;&#25104;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#21644;&#35299;&#37322;&#22810;&#20010;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#36825;&#31687;&#35770;&#25991;&#35299;&#20915;&#20102;&#29702;&#35299;&#21644;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20043;&#38388;&#24046;&#24322;&#30340;&#25361;&#25112;&#65292;&#23545;&#20110;&#27169;&#22411;&#36873;&#25321;&#12289;&#30417;&#25511;&#21644;&#29983;&#21629;&#21608;&#26399;&#31649;&#29702;&#22312;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;DeltaXplainer&#65292;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#22522;&#20110;&#35268;&#21017;&#30340;&#35299;&#37322;&#65292;&#25551;&#36848;&#20004;&#20010;&#20108;&#20803;&#20998;&#31867;&#22120;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#20026;&#20102;&#35780;&#20272;DeltaXplainer&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#28085;&#30422;&#20102;&#28041;&#21450;&#19981;&#21516;&#31867;&#22411;&#27010;&#24565;&#28418;&#31227;&#30340;&#21508;&#31181;&#27169;&#22411;&#27604;&#36739;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainable AI (XAI) methods have mostly been built to investigate and shed light on single machine learning models and are not designed to capture and explain differences between multiple models effectively. This paper addresses the challenge of understanding and explaining differences between machine learning models, which is crucial for model selection, monitoring and lifecycle management in real-world applications. We propose DeltaXplainer, a model-agnostic method for generating rule-based explanations describing the differences between two binary classifiers. To assess the effectiveness of DeltaXplainer, we conduct experiments on synthetic and real-world datasets, covering various model comparison scenarios involving different types of concept drift.
&lt;/p&gt;</description></item><item><title>GAIA-1&#26159;&#19968;&#20010;&#20351;&#29992;&#35270;&#39057;&#12289;&#25991;&#26412;&#21644;&#21160;&#20316;&#36755;&#20837;&#29983;&#25104;&#36924;&#30495;&#39550;&#39542;&#22330;&#26223;&#30340;&#29983;&#25104;&#19990;&#30028;&#27169;&#22411;&#65292;&#33021;&#22815;&#31934;&#32454;&#25511;&#21046;&#33258;&#36710;&#34892;&#20026;&#21644;&#22330;&#26223;&#29305;&#24449;&#65292;&#24182;&#22312;&#23398;&#20064;&#19978;&#19979;&#25991;&#24863;&#30693;&#12289;&#27867;&#21270;&#33021;&#21147;&#21644;&#20960;&#20309;&#29702;&#35299;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.17080</link><description>&lt;p&gt;
GAIA-1: &#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#30340;&#29983;&#25104;&#19990;&#30028;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
GAIA-1: A Generative World Model for Autonomous Driving. (arXiv:2309.17080v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17080
&lt;/p&gt;
&lt;p&gt;
GAIA-1&#26159;&#19968;&#20010;&#20351;&#29992;&#35270;&#39057;&#12289;&#25991;&#26412;&#21644;&#21160;&#20316;&#36755;&#20837;&#29983;&#25104;&#36924;&#30495;&#39550;&#39542;&#22330;&#26223;&#30340;&#29983;&#25104;&#19990;&#30028;&#27169;&#22411;&#65292;&#33021;&#22815;&#31934;&#32454;&#25511;&#21046;&#33258;&#36710;&#34892;&#20026;&#21644;&#22330;&#26223;&#29305;&#24449;&#65292;&#24182;&#22312;&#23398;&#20064;&#19978;&#19979;&#25991;&#24863;&#30693;&#12289;&#27867;&#21270;&#33021;&#21147;&#21644;&#20960;&#20309;&#29702;&#35299;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#25215;&#35834;&#20026;&#20132;&#36890;&#24102;&#26469;&#21464;&#38761;&#24615;&#30340;&#25913;&#36827;&#65292;&#20294;&#26500;&#24314;&#33021;&#22815;&#23433;&#20840;&#23548;&#33322;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#30340;&#31995;&#32479;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#22312;&#20110;&#26377;&#25928;&#39044;&#27979;&#19990;&#30028;&#38543;&#30528;&#36710;&#36742;&#34892;&#20026;&#30340;&#28436;&#21464;&#21487;&#33021;&#20986;&#29616;&#30340;&#21508;&#31181;&#28508;&#22312;&#32467;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;GAIA-1&#65288;&#29983;&#25104;&#26234;&#33021;&#19982;&#33258;&#20027;&#39550;&#39542;&#65289;&#65292;&#19968;&#31181;&#29983;&#25104;&#19990;&#30028;&#27169;&#22411;&#65292;&#23427;&#21033;&#29992;&#35270;&#39057;&#12289;&#25991;&#26412;&#21644;&#21160;&#20316;&#36755;&#20837;&#29983;&#25104;&#36924;&#30495;&#30340;&#39550;&#39542;&#22330;&#26223;&#65292;&#21516;&#26102;&#23545;&#33258;&#36710;&#34892;&#20026;&#21644;&#22330;&#26223;&#29305;&#24449;&#25552;&#20379;&#31934;&#32454;&#25511;&#21046;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#19990;&#30028;&#24314;&#27169;&#35270;&#20026;&#19968;&#20010;&#26080;&#30417;&#30563;&#30340;&#24207;&#21015;&#24314;&#27169;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#36755;&#20837;&#26144;&#23556;&#21040;&#31163;&#25955;&#26631;&#35760;&#65292;&#24182;&#39044;&#27979;&#24207;&#21015;&#20013;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#26469;&#36827;&#34892;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20174;&#20013;&#33719;&#24471;&#30340;&#29305;&#24449;&#21253;&#25324;&#23398;&#20064;&#39640;&#32423;&#32467;&#26500;&#21644;&#22330;&#26223;&#21160;&#24577;&#12289;&#19978;&#19979;&#25991;&#24863;&#30693;&#12289;&#27867;&#21270;&#33021;&#21147;&#21644;&#20960;&#20309;&#29702;&#35299;&#12290;GAIA-1&#23398;&#21040;&#30340;&#34920;&#31034;&#30340;&#23041;&#21147;
&lt;/p&gt;
&lt;p&gt;
Autonomous driving promises transformative improvements to transportation, but building systems capable of safely navigating the unstructured complexity of real-world scenarios remains challenging. A critical problem lies in effectively predicting the various potential outcomes that may emerge in response to the vehicle's actions as the world evolves.  To address this challenge, we introduce GAIA-1 ('Generative AI for Autonomy'), a generative world model that leverages video, text, and action inputs to generate realistic driving scenarios while offering fine-grained control over ego-vehicle behavior and scene features. Our approach casts world modeling as an unsupervised sequence modeling problem by mapping the inputs to discrete tokens, and predicting the next token in the sequence. Emerging properties from our model include learning high-level structures and scene dynamics, contextual awareness, generalization, and understanding of geometry. The power of GAIA-1's learned representati
&lt;/p&gt;</description></item><item><title>SCALE&#26159;&#19968;&#31181;&#23558;&#19987;&#19994;&#32763;&#35793;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36830;&#25509;&#20026;&#32479;&#19968;&#32763;&#35793;&#24341;&#25806;&#30340;&#21327;&#21516;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;STM&#30340;&#32763;&#35793;&#36827;&#19968;&#27493;&#25552;&#21319;LLM&#30340;&#24615;&#33021;&#65292;&#20174;&#32780;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.17061</link><description>&lt;p&gt;
SCALE: &#19981;&#23545;&#31216;&#35821;&#35328;&#32763;&#35793;&#24341;&#25806;&#30340;&#21327;&#21516;&#21512;&#20316;
&lt;/p&gt;
&lt;p&gt;
SCALE: Synergized Collaboration of Asymmetric Language Translation Engines. (arXiv:2309.17061v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17061
&lt;/p&gt;
&lt;p&gt;
SCALE&#26159;&#19968;&#31181;&#23558;&#19987;&#19994;&#32763;&#35793;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36830;&#25509;&#20026;&#32479;&#19968;&#32763;&#35793;&#24341;&#25806;&#30340;&#21327;&#21516;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;STM&#30340;&#32763;&#35793;&#36827;&#19968;&#27493;&#25552;&#21319;LLM&#30340;&#24615;&#33021;&#65292;&#20174;&#32780;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;SCALE&#65292;&#19968;&#31181;&#23558;&#32039;&#20945;&#30340;&#19987;&#19994;&#32763;&#35793;&#27169;&#22411;&#65288;STM&#65289;&#21644;&#36890;&#29992;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36830;&#25509;&#20026;&#32479;&#19968;&#32763;&#35793;&#24341;&#25806;&#30340;&#21327;&#21516;&#26694;&#26550;&#12290;&#36890;&#36807;&#23558;STM&#30340;&#32763;&#35793;&#24341;&#20837;&#19977;&#20803;&#32452;&#30340;&#19978;&#19979;&#25991;&#28436;&#31034;&#20013;&#65292;SCALE&#23454;&#29616;&#20102;LLM&#30340;&#32454;&#21270;&#21644;&#26530;&#36724;&#33021;&#21147;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;LLM&#30340;&#35821;&#35328;&#20559;&#24046;&#21644;STM&#30340;&#24182;&#34892;&#25968;&#25454;&#20559;&#24046;&#65292;&#22312;&#19981;&#29306;&#29298;&#24191;&#27867;&#24615;&#30340;&#21069;&#25552;&#19979;&#22686;&#24378;&#20102;LLM&#30340;&#19987;&#19994;&#24615;&#65292;&#20419;&#36827;&#20102;&#26080;&#38656;&#26114;&#36149;LLM&#31934;&#35843;&#30340;&#25345;&#32493;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#32508;&#21512;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25361;&#25112;&#24615;&#30340;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#65292;SCALE&#26126;&#26174;&#20248;&#20110;&#23569;&#26679;&#26412;LLMs&#65288;GPT-4&#65289;&#21644;&#19987;&#19994;&#27169;&#22411;&#65288;NLLB&#65289;&#12290;&#27492;&#22806;&#65292;&#22312;Xhosa&#21040;&#33521;&#35821;&#30340;&#32763;&#35793;&#20013;&#65292;SCALE&#30340;&#24615;&#33021;&#25345;&#32493;&#25552;&#21319;4&#20010;BLEURT&#20998;&#25968;&#65292;&#24403;&#37197;&#22791;&#20165;600M&#21442;&#25968;&#30340;&#32039;&#20945;&#27169;&#22411;&#26102;&#65292;SCALE&#22312;COMET&#20998;&#25968;&#19978;&#36229;&#36807;&#20102;&#23569;&#26679;&#26412;GPT-4&#30340;2.5&#20010;&#20998;&#25968;&#21644;BLEURT&#20998;&#25968;&#19978;3.8&#20010;&#20998;&#25968;&#12290;SCALE&#36824;&#33021;&#26377;&#25928;&#22320;&#36827;&#34892;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce SCALE, a collaborative framework that connects compact Specialized Translation Models (STMs) and general-purpose Large Language Models (LLMs) as one unified translation engine. By introducing translation from STM into the triplet in-context demonstrations, SCALE unlocks refinement and pivoting ability of LLM, thus mitigating language bias of LLM and parallel data bias of STM, enhancing LLM speciality without sacrificing generality, and facilitating continual learning without expensive LLM fine-tuning. Our comprehensive experiments show that SCALE significantly outperforms both few-shot LLMs (GPT-4) and specialized models (NLLB) in challenging low-resource settings. Moreover, in Xhosa to English translation, SCALE experiences consistent improvement by a 4 BLEURT score without tuning LLM and surpasses few-shot GPT-4 by 2.5 COMET score and 3.8 BLEURT score when equipped with a compact model consisting of merely 600M parameters. SCALE could also effectively expl
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;XAIstories&#26694;&#26550;&#65292;&#36890;&#36807;&#21465;&#20107;&#26041;&#24335;&#35299;&#37322;AI&#39044;&#27979;&#65292;&#20854;&#20013;SHAPstories&#22522;&#20110;SHAP&#35299;&#37322;&#35299;&#37322;&#39044;&#27979;&#24471;&#20998;&#65292;CFstories&#22522;&#20110;CF&#35299;&#37322;&#35299;&#37322;&#20915;&#31574;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36229;&#36807;90%&#30340;&#26222;&#36890;&#35835;&#32773;&#35748;&#21487;SHAPstories&#29983;&#25104;&#30340;&#21465;&#20107;&#30340;&#35828;&#26381;&#21147;&#65292;92%&#30340;&#25968;&#25454;&#31185;&#23398;&#23478;&#35748;&#20026;SHAPstories&#33021;&#22815;&#25552;&#39640;&#38750;&#19987;&#19994;&#20154;&#22763;&#30340;&#26131;&#29992;&#24615;&#21644;&#20449;&#24515;&#12290;</title><link>http://arxiv.org/abs/2309.17057</link><description>&lt;p&gt;
&#35762;&#32473;&#25105;&#21548;&#65281;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21465;&#20107;&#39537;&#21160;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Tell Me a Story! Narrative-Driven XAI with Large Language Models. (arXiv:2309.17057v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17057
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;XAIstories&#26694;&#26550;&#65292;&#36890;&#36807;&#21465;&#20107;&#26041;&#24335;&#35299;&#37322;AI&#39044;&#27979;&#65292;&#20854;&#20013;SHAPstories&#22522;&#20110;SHAP&#35299;&#37322;&#35299;&#37322;&#39044;&#27979;&#24471;&#20998;&#65292;CFstories&#22522;&#20110;CF&#35299;&#37322;&#35299;&#37322;&#20915;&#31574;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36229;&#36807;90%&#30340;&#26222;&#36890;&#35835;&#32773;&#35748;&#21487;SHAPstories&#29983;&#25104;&#30340;&#21465;&#20107;&#30340;&#35828;&#26381;&#21147;&#65292;92%&#30340;&#25968;&#25454;&#31185;&#23398;&#23478;&#35748;&#20026;SHAPstories&#33021;&#22815;&#25552;&#39640;&#38750;&#19987;&#19994;&#20154;&#22763;&#30340;&#26131;&#29992;&#24615;&#21644;&#20449;&#24515;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20170;&#37325;&#35201;&#39046;&#22495;&#20013;&#65292;&#40657;&#30418;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#30427;&#34892;&#21152;&#22823;&#20102;&#23545;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#30340;&#38656;&#27714;&#12290;&#24191;&#27867;&#20351;&#29992;&#30340;SHAP&#20540;&#34429;&#28982;&#37327;&#21270;&#20102;&#29305;&#24449;&#37325;&#35201;&#24615;&#65292;&#20294;&#24448;&#24448;&#36807;&#20110;&#22797;&#26434;&#65292;&#32570;&#20047;&#20154;&#24615;&#21270;&#30340;&#35299;&#37322;&#12290;&#27492;&#22806;&#65292;&#21453;&#20107;&#23454;&#65288;CF&#65289;&#35299;&#37322;&#23637;&#31034;&#20102;&#8220;&#22914;&#26524;&#8221;&#20294;&#27809;&#26377;&#35299;&#37322;&#8220;&#20026;&#20160;&#20040;&#8221;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;XAIstories&#12290;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;XAIstories&#25552;&#20379;&#20102;&#21465;&#20107;&#26469;&#38416;&#26126;AI&#39044;&#27979;&#65306;&#22522;&#20110;SHAP&#35299;&#37322;&#30340;SHAPstories&#35299;&#37322;&#39044;&#27979;&#24471;&#20998;&#65292;&#32780;&#22522;&#20110;CF&#35299;&#37322;&#30340;CFstories&#35299;&#37322;&#20915;&#31574;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20196;&#20154;&#38663;&#24778;&#65306;&#36229;&#36807;90%&#30340;&#35843;&#26597;&#26222;&#36890;&#35835;&#32773;&#35748;&#20026;SHAPstories&#29983;&#25104;&#30340;&#21465;&#20107;&#26159;&#26377;&#35828;&#26381;&#21147;&#30340;&#12290;&#25968;&#25454;&#31185;&#23398;&#23478;&#20027;&#35201;&#35748;&#20026;SHAPstories&#22312;&#21521;&#26222;&#36890;&#35835;&#32773;&#20256;&#36798;&#35299;&#37322;&#26041;&#38754;&#20855;&#26377;&#20215;&#20540;&#65292;92%&#30340;&#25968;&#25454;&#31185;&#23398;&#23478;&#34920;&#31034;&#36825;&#23558;&#26377;&#21161;&#20110;&#38750;&#19987;&#19994;&#20154;&#22763;&#30340;&#26131;&#29992;&#24615;&#21644;&#20449;&#24515;&#12290;
&lt;/p&gt;
&lt;p&gt;
In today's critical domains, the predominance of black-box machine learning models amplifies the demand for Explainable AI (XAI). The widely used SHAP values, while quantifying feature importance, are often too intricate and lack human-friendly explanations. Furthermore, counterfactual (CF) explanations present `what ifs' but leave users grappling with the 'why'. To bridge this gap, we introduce XAIstories. Leveraging Large Language Models, XAIstories provide narratives that shed light on AI predictions: SHAPstories do so based on SHAP explanations to explain a prediction score, while CFstories do so for CF explanations to explain a decision. Our results are striking: over 90% of the surveyed general audience finds the narrative generated by SHAPstories convincing. Data scientists primarily see the value of SHAPstories in communicating explanations to a general audience, with 92% of data scientists indicating that it will contribute to the ease and confidence of nonspecialists in under
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31283;&#20581;&#21644;&#20934;&#30830;&#20998;&#31867;&#22120;&#30340;&#36830;&#32493;&#24615;&#65292;&#25552;&#20986;&#20102;&#24403;&#20551;&#35774;&#36830;&#32493;&#26102;&#65292;&#20854;&#31283;&#20581;&#24615;&#21644;&#20934;&#30830;&#24615;&#26159;&#19981;&#20860;&#23481;&#30340;&#35266;&#28857;&#12290;</title><link>http://arxiv.org/abs/2309.17048</link><description>&lt;p&gt;
&#20851;&#20110;&#31283;&#20581;&#21644;&#20934;&#30830;&#20998;&#31867;&#22120;&#36830;&#32493;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Continuity of Robust and Accurate Classifiers. (arXiv:2309.17048v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17048
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31283;&#20581;&#21644;&#20934;&#30830;&#20998;&#31867;&#22120;&#30340;&#36830;&#32493;&#24615;&#65292;&#25552;&#20986;&#20102;&#24403;&#20551;&#35774;&#36830;&#32493;&#26102;&#65292;&#20854;&#31283;&#20581;&#24615;&#21644;&#20934;&#30830;&#24615;&#26159;&#19981;&#20860;&#23481;&#30340;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#26159;&#25104;&#21151;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#20110;&#21508;&#31181;&#39046;&#22495;&#30340;&#20851;&#38190;&#12290;&#21019;&#24314;&#19968;&#20010;&#31283;&#20581;&#30340;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#19968;&#20010;&#19981;&#21463;&#23545;&#25239;&#25915;&#20987;&#24433;&#21709;&#30340;&#27169;&#22411;&#65292;&#38656;&#35201;&#20840;&#38754;&#29702;&#35299;&#23545;&#25239;&#26679;&#26412;&#29616;&#35937;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26426;&#22120;&#23398;&#20064;&#20013;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#65292;&#24456;&#38590;&#25551;&#36848;&#36825;&#19968;&#29616;&#35937;&#12290;&#22312;&#25991;&#29486;&#20013;&#24050;&#32463;&#35777;&#26126;&#20102;&#23545;&#25239;&#24615;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#20551;&#35774;&#30340;&#31283;&#20581;&#24615;&#65292;&#20294;&#36825;&#31181;&#25913;&#36827;&#26159;&#20197;&#33258;&#28982;&#26679;&#26412;&#24615;&#33021;&#19979;&#38477;&#20026;&#20195;&#20215;&#30340;&#12290;&#22240;&#27492;&#65292;&#26377;&#20154;&#25552;&#20986;&#20551;&#35774;&#30340;&#31283;&#20581;&#24615;&#21644;&#20934;&#30830;&#24615;&#26159;&#30456;&#20114;&#30683;&#30462;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#21478;&#19968;&#31181;&#35266;&#28857;&#65292;&#20551;&#35774;&#30340;&#36830;&#32493;&#24615;&#19982;&#20854;&#31283;&#20581;&#24615;&#21644;&#20934;&#30830;&#24615;&#26159;&#19981;&#20860;&#23481;&#30340;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#36830;&#32493;&#20989;&#25968;&#19981;&#33021;&#26377;&#25928;&#22320;&#23398;&#20064;&#26368;&#20339;&#31283;&#20581;&#20551;&#35774;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23558;&#24341;&#20837;&#19968;&#20010;&#31995;&#32479;&#30740;&#31350;&#35856;&#27874;&#21644;ho&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
The reliability of a learning model is key to the successful deployment of machine learning in various applications. Creating a robust model, particularly one unaffected by adversarial attacks, requires a comprehensive understanding of the adversarial examples phenomenon. However, it is difficult to describe the phenomenon due to the complicated nature of the problems in machine learning. It has been shown that adversarial training can improve the robustness of the hypothesis. However, this improvement comes at the cost of decreased performance on natural samples. Hence, it has been suggested that robustness and accuracy of a hypothesis are at odds with each other. In this paper, we put forth the alternative proposal that it is the continuity of a hypothesis that is incompatible with its robustness and accuracy. In other words, a continuous function cannot effectively learn the optimal robust hypothesis. To this end, we will introduce a framework for a rigorous study of harmonic and ho
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#35299;&#26512;&#12289;&#28436;&#21270;&#21644;&#38543;&#26426;&#31070;&#32463;&#32593;&#32476;&#30340;&#31185;&#23572;&#33707;&#21733;&#27931;&#22827;&#22797;&#26434;&#24615;&#36827;&#34892;&#21051;&#30011;&#65292;&#25552;&#20379;&#20102;&#23545;&#20854;&#36229;&#22270;&#28789;&#35745;&#31639;&#33021;&#21147;&#30340;&#31934;&#30830;&#29702;&#35299;&#65292;&#21457;&#29616;&#20102;&#35299;&#26512;&#32593;&#32476;&#12289;&#28436;&#21270;&#32593;&#32476;&#21644;&#38543;&#26426;&#32593;&#32476;&#20043;&#38388;&#30340;&#23618;&#32423;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2309.17032</link><description>&lt;p&gt;
&#25913;&#36827;&#30340;&#35299;&#26512;&#12289;&#28436;&#21270;&#21644;&#38543;&#26426;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#31185;&#23572;&#33707;&#21733;&#27931;&#22827;&#22797;&#26434;&#24615;
&lt;/p&gt;
&lt;p&gt;
Refined Kolmogorov Complexity of Analog, Evolving and Stochastic Recurrent Neural Networks. (arXiv:2309.17032v1 [cs.CC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17032
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#35299;&#26512;&#12289;&#28436;&#21270;&#21644;&#38543;&#26426;&#31070;&#32463;&#32593;&#32476;&#30340;&#31185;&#23572;&#33707;&#21733;&#27931;&#22827;&#22797;&#26434;&#24615;&#36827;&#34892;&#21051;&#30011;&#65292;&#25552;&#20379;&#20102;&#23545;&#20854;&#36229;&#22270;&#28789;&#35745;&#31639;&#33021;&#21147;&#30340;&#31934;&#30830;&#29702;&#35299;&#65292;&#21457;&#29616;&#20102;&#35299;&#26512;&#32593;&#32476;&#12289;&#28436;&#21270;&#32593;&#32476;&#21644;&#38543;&#26426;&#32593;&#32476;&#20043;&#38388;&#30340;&#23618;&#32423;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#35299;&#26512;&#26435;&#37325;&#12289;&#28436;&#21270;&#26435;&#37325;&#21644;&#23454;&#38469;&#27010;&#29575;&#30340;&#31185;&#23572;&#33707;&#21733;&#27931;&#22827;&#22797;&#26434;&#24615;&#65292;&#25552;&#20379;&#20102;&#23545;&#35299;&#26512;&#12289;&#28436;&#21270;&#21644;&#38543;&#26426;&#31070;&#32463;&#32593;&#32476;&#36229;&#22270;&#28789;&#35745;&#31639;&#33021;&#21147;&#30340;&#31934;&#30830;&#21051;&#30011;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#19968;&#31995;&#21015;&#35299;&#26512;&#32593;&#32476;&#30340;&#26080;&#31351;&#23618;&#32423;&#65292;&#36825;&#20123;&#23618;&#32423;&#26159;&#36890;&#36807;&#20854;&#24213;&#23618;&#23454;&#38469;&#26435;&#37325;&#30340;&#31185;&#23572;&#33707;&#21733;&#27931;&#22827;&#22797;&#26434;&#24615;&#23450;&#20041;&#30340;&#12290;&#36825;&#20123;&#23618;&#32423;&#20301;&#20110;&#22797;&#26434;&#24615;&#31867; $\mathbf{P}$ &#21644; $\mathbf{P/poly}$ &#20043;&#38388;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#32467;&#26524;&#25512;&#24191;&#21040;&#28436;&#21270;&#32593;&#32476;&#12290;&#25105;&#20204;&#24471;&#21040;&#20102;&#19968;&#31995;&#21015;&#22522;&#20110;&#31185;&#23572;&#33707;&#21733;&#27931;&#22827;&#22797;&#26434;&#24615;&#30340;&#28436;&#21270;&#32593;&#32476;&#30340;&#23618;&#32423;&#32467;&#26500;&#65292;&#36825;&#20123;&#23618;&#32423;&#20063;&#20301;&#20110; $\mathbf{P}$ &#21644; $\mathbf{P/poly}$ &#20043;&#38388;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#32467;&#26524;&#25193;&#23637;&#21040;&#20351;&#29992;&#23454;&#38469;&#27010;&#29575;&#20316;&#20026;&#38543;&#26426;&#24615;&#28304;&#30340;&#38543;&#26426;&#32593;&#32476;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#19968;&#31995;&#21015;&#22522;&#20110;&#27010;&#29575;&#30340;&#38543;&#26426;&#32593;&#32476;&#30340;&#26080;&#31351;&#23618;&#32423;&#65292;&#36825;&#20123;&#23618;&#32423;&#22312;&#31185;&#23572;&#33707;&#21733;&#27931;&#22827;&#22797;&#26434;&#24615;&#19978;&#24314;&#31435;&#20102;&#26725;&#26753;&#12290;
&lt;/p&gt;
&lt;p&gt;
We provide a refined characterization of the super-Turing computational power of analog, evolving, and stochastic neural networks based on the Kolmogorov complexity of their real weights, evolving weights, and real probabilities, respectively. First, we retrieve an infinite hierarchy of classes of analog networks defined in terms of the Kolmogorov complexity of their underlying real weights. This hierarchy is located between the complexity classes $\mathbf{P}$ and $\mathbf{P/poly}$. Then, we generalize this result to the case of evolving networks. A similar hierarchy of Kolomogorov-based complexity classes of evolving networks is obtained. This hierarchy also lies between $\mathbf{P}$ and $\mathbf{P/poly}$. Finally, we extend these results to the case of stochastic networks employing real probabilities as source of randomness. An infinite hierarchy of stochastic networks based on the Kolmogorov complexity of their probabilities is therefore achieved. In this case, the hierarchy bridges
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#29983;&#25104;&#27169;&#22411;&#23454;&#29616;&#30340;&#21487;&#25193;&#23637;&#30340;&#22810;&#26102;&#26399;&#36965;&#24863;&#21464;&#21270;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#25311;&#38543;&#26426;&#21464;&#21270;&#36807;&#31243;&#65292;&#35299;&#20915;&#20102;&#25910;&#38598;&#12289;&#39044;&#22788;&#29702;&#21644;&#27880;&#37322;&#22810;&#26102;&#26399;&#36965;&#24863;&#22270;&#20687;&#30340;&#38590;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.17031</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#22810;&#26102;&#26399;&#36965;&#24863;&#21464;&#21270;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#36890;&#36807;&#27169;&#25311;&#38543;&#26426;&#21464;&#21270;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Scalable Multi-Temporal Remote Sensing Change Data Generation via Simulating Stochastic Change Process. (arXiv:2309.17031v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17031
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#29983;&#25104;&#27169;&#22411;&#23454;&#29616;&#30340;&#21487;&#25193;&#23637;&#30340;&#22810;&#26102;&#26399;&#36965;&#24863;&#21464;&#21270;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#25311;&#38543;&#26426;&#21464;&#21270;&#36807;&#31243;&#65292;&#35299;&#20915;&#20102;&#25910;&#38598;&#12289;&#39044;&#22788;&#29702;&#21644;&#27880;&#37322;&#22810;&#26102;&#26399;&#36965;&#24863;&#22270;&#20687;&#30340;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20102;&#35299;&#22320;&#34920;&#30340;&#26102;&#24577;&#21160;&#24577;&#26159;&#22810;&#26102;&#26399;&#36965;&#24863;&#22270;&#20687;&#20998;&#26512;&#30340;&#20219;&#21153;&#65292;&#28145;&#24230;&#35270;&#35273;&#27169;&#22411;&#36890;&#36807;&#26631;&#35760;&#30340;&#22810;&#26102;&#26399;&#22270;&#20687;&#22823;&#22823;&#25512;&#21160;&#20102;&#36825;&#19968;&#20219;&#21153;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22823;&#35268;&#27169;&#25910;&#38598;&#12289;&#39044;&#22788;&#29702;&#21644;&#27880;&#37322;&#22810;&#26102;&#26399;&#36965;&#24863;&#22270;&#20687;&#24182;&#19981;&#23481;&#26131;&#65292;&#22240;&#20026;&#36825;&#38656;&#35201;&#33457;&#36153;&#22823;&#37327;&#30340;&#36164;&#28304;&#21644;&#30693;&#35782;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#29983;&#25104;&#27169;&#22411;&#23454;&#29616;&#30340;&#21487;&#25193;&#23637;&#30340;&#22810;&#26102;&#26399;&#36965;&#24863;&#21464;&#21270;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26082;&#24265;&#20215;&#21448;&#33258;&#21160;&#21270;&#65292;&#20174;&#32780;&#32531;&#35299;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#24605;&#24819;&#26159;&#36890;&#36807;&#26102;&#38388;&#19978;&#30340;&#38543;&#26426;&#21464;&#21270;&#36807;&#31243;&#36827;&#34892;&#27169;&#25311;&#12290;&#25105;&#20204;&#23558;&#38543;&#26426;&#21464;&#21270;&#36807;&#31243;&#35270;&#20026;&#27010;&#29575;&#35821;&#20041;&#29366;&#24577;&#36716;&#25442;&#65292;&#21363;&#29983;&#25104;&#27010;&#29575;&#21464;&#21270;&#27169;&#22411;&#65288;GPCM&#65289;&#65292;&#23558;&#22797;&#26434;&#30340;&#27169;&#25311;&#38382;&#39064;&#20998;&#35299;&#20026;&#20004;&#20010;&#26356;&#26131;&#36861;&#36394;&#30340;&#23376;&#38382;&#39064;&#65292;&#21363;&#21464;&#21270;&#20107;&#20214;&#27169;&#25311;&#21644;&#35821;&#20041;&#21464;&#21270;&#21512;&#25104;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21464;&#21270;&#29983;&#25104;&#22120;&#65288;Changen&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;GAN&#30340;GPCM&#65292;&#21487;&#20197;&#23454;&#29616;&#23545;&#29983;&#25104;&#36807;&#31243;&#30340;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the temporal dynamics of Earth's surface is a mission of multi-temporal remote sensing image analysis, significantly promoted by deep vision models with its fuel -- labeled multi-temporal images. However, collecting, preprocessing, and annotating multi-temporal remote sensing images at scale is non-trivial since it is expensive and knowledge-intensive. In this paper, we present a scalable multi-temporal remote sensing change data generator via generative modeling, which is cheap and automatic, alleviating these problems. Our main idea is to simulate a stochastic change process over time. We consider the stochastic change process as a probabilistic semantic state transition, namely generative probabilistic change model (GPCM), which decouples the complex simulation problem into two more trackable sub-problems, \ie, change event simulation and semantic change synthesis. To solve these two problems, we present the change generator (Changen), a GAN-based GPCM, enabling contro
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;15&#20010;&#19981;&#21516;&#22823;&#23567;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21457;&#29616;&#23427;&#20204;&#20316;&#20026;&#35780;&#20272;&#22120;&#23384;&#22312;&#35748;&#30693;&#20559;&#24046;&#65292;&#23588;&#20854;&#22312;&#25991;&#26412;&#36136;&#37327;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#36739;&#24378;&#30340;&#20559;&#35265;&#65292;&#36825;&#23545;&#20854;&#40065;&#26834;&#24615;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;&#20102;&#20154;&#31867;&#21644;&#26426;&#22120;&#20559;&#22909;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.17012</link><description>&lt;p&gt;
&#20316;&#20026;&#35780;&#20272;&#22120;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#35748;&#30693;&#20559;&#24046;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Cognitive Biases in Large Language Models as Evaluators. (arXiv:2309.17012v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;15&#20010;&#19981;&#21516;&#22823;&#23567;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21457;&#29616;&#23427;&#20204;&#20316;&#20026;&#35780;&#20272;&#22120;&#23384;&#22312;&#35748;&#30693;&#20559;&#24046;&#65292;&#23588;&#20854;&#22312;&#25991;&#26412;&#36136;&#37327;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#36739;&#24378;&#30340;&#20559;&#35265;&#65292;&#36825;&#23545;&#20854;&#40065;&#26834;&#24615;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;&#20102;&#20154;&#31867;&#21644;&#26426;&#22120;&#20559;&#22909;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#31616;&#21333;&#30340;&#25552;&#31034;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#20316;&#20026;&#33258;&#21160;&#35780;&#20272;&#22120;&#38750;&#24120;&#26377;&#25928;&#12290;&#26412;&#30740;&#31350;&#32452;&#35013;&#20102;15&#20010;&#22823;&#23567;&#19981;&#21516;&#30340;LLMs&#65292;&#24182;&#36890;&#36807;&#20854;&#20182;LLMs&#30340;&#20559;&#22909;&#25490;&#21517;&#26469;&#35780;&#20272;&#23427;&#20204;&#30340;&#36755;&#20986;&#21709;&#24212;&#65292;&#20363;&#22914;System Star&#27604;System Square&#26356;&#22909;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#29992;&#20110;&#35780;&#20272;LLMs&#36755;&#20986;&#20013;&#20845;&#31181;&#19981;&#21516;&#35748;&#30693;&#20559;&#24046;&#30340;&#35748;&#30693;&#20559;&#24046;&#22522;&#20934;&#27979;&#35797;&#65288;CoBBLEr&#65289;&#65292;&#22914;&#33258;&#25105;&#20013;&#24515;&#20559;&#24046;&#65292;&#21363;&#27169;&#22411;&#26356;&#21916;&#27426;&#23558;&#33258;&#24049;&#30340;&#36755;&#20986;&#22312;&#35780;&#20272;&#20013;&#25490;&#21517;&#36739;&#39640;&#12290;&#25105;&#20204;&#21457;&#29616;LLMs&#26159;&#26377;&#20559;&#35265;&#30340;&#25991;&#26412;&#36136;&#37327;&#35780;&#20272;&#22120;&#65292;&#22312;&#27599;&#20010;&#35780;&#20272;&#20013;&#37117;&#34920;&#29616;&#20986;&#23545;&#25105;&#20204;&#20559;&#35265;&#22522;&#20934;&#30340;&#24378;&#28872;&#36857;&#35937;&#65288;&#22312;&#25152;&#26377;&#27169;&#22411;&#19978;&#30340;&#24179;&#22343;&#27604;&#36739;&#32422;&#20026;40%&#65289;&#65292;&#36825;&#23545;&#23427;&#20204;&#20316;&#20026;&#35780;&#20272;&#22120;&#30340;&#40065;&#26834;&#24615;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#20154;&#31867;&#21644;&#26426;&#22120;&#20559;&#22909;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#35745;&#31639;&#20102;&#24179;&#22343;&#30340;Rank-Biased O&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have recently been shown to be effective as automatic evaluators with simple prompting and in-context learning. In this work, we assemble 15 LLMs of four different size ranges and evaluate their output responses by preference ranking from the other LLMs as evaluators, such as System Star is better than System Square. We then evaluate the quality of ranking outputs introducing the Cognitive Bias Benchmark for LLMs as Evaluators (CoBBLEr), a benchmark to measure six different cognitive biases in LLM evaluation outputs, such as the Egocentric bias where a model prefers to rank its own outputs highly in evaluation. We find that LLMs are biased text quality evaluators, exhibiting strong indications on our bias benchmark (average of 40% of comparisons across all models) within each of their evaluations that question their robustness as evaluators. Furthermore, we examine the correlation between human and machine preferences and calculate the average Rank-Biased O
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#21307;&#23398;&#39046;&#22495;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#33030;&#24369;&#24615;&#65292;&#36890;&#36807;&#25805;&#32437;&#27169;&#22411;&#26435;&#37325;&#30340;&#24456;&#23567;&#27604;&#20363;&#65292;&#21487;&#20197;&#25925;&#24847;&#27880;&#20837;&#38169;&#35823;&#30340;&#29983;&#29289;&#21307;&#23398;&#20107;&#23454;&#65292;&#24182;&#19988;&#36825;&#20123;&#38169;&#35823;&#20449;&#24687;&#20250;&#34987;&#27169;&#22411;&#36755;&#20986;&#20256;&#25773;&#12290;&#38754;&#23545;&#36825;&#31181;&#26131;&#21463;&#25915;&#20987;&#24615;&#65292;&#25105;&#20204;&#38656;&#35201;&#37319;&#21462;&#25514;&#26045;&#30830;&#20445;&#36825;&#20123;&#27169;&#22411;&#22312;&#21307;&#30103;&#23454;&#36341;&#20013;&#30340;&#21487;&#38752;&#21644;&#23433;&#20840;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.17007</link><description>&lt;p&gt;
&#21307;&#23398;&#22522;&#30784;&#27169;&#22411;&#26131;&#21463;&#26377;&#38024;&#23545;&#24615;&#30340;&#38169;&#35823;&#20449;&#24687;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Medical Foundation Models are Susceptible to Targeted Misinformation Attacks. (arXiv:2309.17007v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17007
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#21307;&#23398;&#39046;&#22495;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#33030;&#24369;&#24615;&#65292;&#36890;&#36807;&#25805;&#32437;&#27169;&#22411;&#26435;&#37325;&#30340;&#24456;&#23567;&#27604;&#20363;&#65292;&#21487;&#20197;&#25925;&#24847;&#27880;&#20837;&#38169;&#35823;&#30340;&#29983;&#29289;&#21307;&#23398;&#20107;&#23454;&#65292;&#24182;&#19988;&#36825;&#20123;&#38169;&#35823;&#20449;&#24687;&#20250;&#34987;&#27169;&#22411;&#36755;&#20986;&#20256;&#25773;&#12290;&#38754;&#23545;&#36825;&#31181;&#26131;&#21463;&#25915;&#20987;&#24615;&#65292;&#25105;&#20204;&#38656;&#35201;&#37319;&#21462;&#25514;&#26045;&#30830;&#20445;&#36825;&#20123;&#27169;&#22411;&#22312;&#21307;&#30103;&#23454;&#36341;&#20013;&#30340;&#21487;&#38752;&#21644;&#23433;&#20840;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22312;&#21307;&#23398;&#39046;&#22495;&#25317;&#26377;&#24191;&#27867;&#30340;&#30693;&#35782;&#65292;&#24182;&#33021;&#22815;&#36328;&#36234;&#22810;&#20010;&#39046;&#22495;&#36827;&#34892;&#21307;&#23398;&#20449;&#24687;&#30340;&#25512;&#29702;&#65292;&#23545;&#26410;&#26469;&#30340;&#21307;&#23398;&#24212;&#29992;&#20855;&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;LLM&#22312;&#21307;&#23398;&#39046;&#22495;&#23384;&#22312;&#30340;&#19968;&#20010;&#20196;&#20154;&#25285;&#24551;&#30340;&#33030;&#24369;&#24615;&#12290;&#36890;&#36807;&#26377;&#38024;&#23545;&#24615;&#22320;&#25805;&#32437;&#27169;&#22411;&#26435;&#37325;&#30340;1.1&#65285;&#65292;&#25105;&#20204;&#21487;&#20197;&#25925;&#24847;&#27880;&#20837;&#19968;&#20010;&#38169;&#35823;&#30340;&#29983;&#29289;&#21307;&#23398;&#20107;&#23454;&#12290;&#38169;&#35823;&#20449;&#24687;&#20250;&#22312;&#27169;&#22411;&#30340;&#36755;&#20986;&#20013;&#20256;&#25773;&#65292;&#21516;&#26102;&#23545;&#20854;&#20182;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#30340;&#24615;&#33021;&#27809;&#26377;&#24433;&#21709;&#12290;&#25105;&#20204;&#22312;&#19968;&#32452;1,038&#20010;&#38169;&#35823;&#30340;&#29983;&#29289;&#21307;&#23398;&#20107;&#23454;&#20013;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#21457;&#29616;&#12290;&#36825;&#31181;&#29305;&#27530;&#30340;&#26131;&#21463;&#25915;&#20987;&#24615;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#22312;&#21307;&#30103;&#29615;&#22659;&#20013;&#24212;&#29992;LLM&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#20449;&#24230;&#30340;&#20005;&#37325;&#20851;&#27880;&#12290;&#36825;&#20984;&#26174;&#20102;&#30830;&#20445;&#36825;&#20123;&#27169;&#22411;&#22312;&#21307;&#23398;&#23454;&#36341;&#20013;&#21487;&#38752;&#21644;&#23433;&#20840;&#20351;&#29992;&#30340;&#38656;&#27714;&#65292;&#38656;&#35201;&#37319;&#21462;&#24378;&#26377;&#21147;&#30340;&#20445;&#25252;&#25514;&#26045;&#65292;&#36827;&#34892;&#24443;&#24213;&#30340;&#39564;&#35777;&#26426;&#21046;&#65292;&#24182;&#23545;&#36825;&#20123;&#27169;&#22411;&#30340;&#35775;&#38382;&#36827;&#34892;&#20005;&#26684;&#31649;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have broad medical knowledge and can reason about medical information across many domains, holding promising potential for diverse medical applications in the near future. In this study, we demonstrate a concerning vulnerability of LLMs in medicine. Through targeted manipulation of just 1.1% of the model's weights, we can deliberately inject an incorrect biomedical fact. The erroneous information is then propagated in the model's output, whilst its performance on other biomedical tasks remains intact. We validate our findings in a set of 1,038 incorrect biomedical facts. This peculiar susceptibility raises serious security and trustworthiness concerns for the application of LLMs in healthcare settings. It accentuates the need for robust protective measures, thorough verification mechanisms, and stringent management of access to these models, ensuring their reliable and safe use in medical practice.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#39044;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#26631;&#31614;&#22122;&#22768;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#22312;&#21512;&#25104;&#22122;&#22768;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#39044;&#35757;&#32451;&#20013;&#30340;&#36731;&#24494;&#22122;&#22768;&#21487;&#20197;&#25552;&#39640;&#39046;&#22495;&#20869;&#30340;&#24615;&#33021;&#65292;&#20294;&#20250;&#25439;&#23475;&#39046;&#22495;&#22806;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#20943;&#36731;&#22122;&#22768;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#40657;&#30418;&#35843;&#25972;&#26041;&#27861;&#65288;NMTune&#65289;&#12290;</title><link>http://arxiv.org/abs/2309.17002</link><description>&lt;p&gt;
&#29702;&#35299;&#21644;&#20943;&#36731;&#39044;&#35757;&#32451;&#20013;&#30340;&#26631;&#31614;&#22122;&#22768;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Understanding and Mitigating the Label Noise in Pre-training on Downstream Tasks. (arXiv:2309.17002v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17002
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#39044;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#26631;&#31614;&#22122;&#22768;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#22312;&#21512;&#25104;&#22122;&#22768;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#39044;&#35757;&#32451;&#20013;&#30340;&#36731;&#24494;&#22122;&#22768;&#21487;&#20197;&#25552;&#39640;&#39046;&#22495;&#20869;&#30340;&#24615;&#33021;&#65292;&#20294;&#20250;&#25439;&#23475;&#39046;&#22495;&#22806;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#20943;&#36731;&#22122;&#22768;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#40657;&#30418;&#35843;&#25972;&#26041;&#27861;&#65288;NMTune&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#65292;&#20808;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26631;&#20934;&#20570;&#27861;&#12290;&#28982;&#32780;&#65292;&#39044;&#35757;&#32451;&#25968;&#25454;&#36890;&#24120;&#21253;&#21547;&#26631;&#31614;&#22122;&#22768;&#65292;&#36825;&#21487;&#33021;&#23545;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#26412;&#25991;&#26088;&#22312;&#20102;&#35299;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#22122;&#22768;&#30340;&#24615;&#36136;&#65292;&#24182;&#20943;&#36731;&#20854;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#24433;&#21709;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#22312;&#21512;&#25104;&#22122;&#22768;&#30340;ImageNet-1K&#21644;YFCC15M&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#39044;&#35757;&#32451;&#20013;&#30340;&#36731;&#24494;&#22122;&#22768;&#21487;&#20197;&#20419;&#36827;&#39046;&#22495;&#20869;&#30340;&#36716;&#31227;&#24615;&#33021;&#65292;&#21363;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#20855;&#26377;&#30456;&#21516;&#30340;&#20998;&#24067;&#65307;&#28982;&#32780;&#65292;&#23427;&#24635;&#26159;&#20250;&#25439;&#23475;&#39046;&#22495;&#22806;&#30340;&#24615;&#33021;&#65292;&#21363;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#20855;&#26377;&#19981;&#21516;&#30340;&#20998;&#24067;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#65292;&#39044;&#35757;&#32451;&#20013;&#30340;&#22122;&#22768;&#20250;&#19981;&#21516;&#22320;&#22609;&#36896;&#29305;&#24449;&#31354;&#38388;&#12290;&#28982;&#21518;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#40657;&#30418;&#35843;&#25972;&#26041;&#27861;&#65288;NMTune&#65289;&#26469;&#20351;&#29305;&#24449;&#31354;&#38388;&#36798;&#21040;&#26144;&#23556;&#24182;&#20943;&#36731;&#22122;&#22768;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-training on large-scale datasets and then fine-tuning on downstream tasks have become a standard practice in deep learning. However, pre-training data often contain label noise that may adversely affect the generalization of the model. This paper aims to understand the nature of noise in pre-training datasets and to mitigate its impact on downstream tasks. More specifically, through extensive experiments of supervised pre-training models on synthetic noisy ImageNet-1K and YFCC15M datasets, we demonstrate that while slight noise in pre-training can benefit in-domain (ID) transfer performance, where the training and testing data share the same distribution, it always deteriorates out-of-domain (OOD) performance, where training and testing data distribution are different. We empirically verify that the reason behind is noise in pre-training shapes the feature space differently. We then propose a lightweight black-box tuning method (NMTune) to affine the feature space to mitigate the m
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23545;&#36724;&#25215;&#25925;&#38556;&#20998;&#31867;&#26041;&#27861;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#65292;&#20851;&#27880;&#20102;&#36724;&#25215;&#25925;&#38556;&#35786;&#26029;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#21450;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#25391;&#21160;&#25968;&#25454;&#26469;&#39044;&#27979;&#25925;&#38556;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.17001</link><description>&lt;p&gt;
&#23545;&#36724;&#25215;&#25925;&#38556;&#20998;&#31867;&#26041;&#27861;&#30340;&#28145;&#20837;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Closer Look at Bearing Fault Classification Approaches. (arXiv:2309.17001v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17001
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23545;&#36724;&#25215;&#25925;&#38556;&#20998;&#31867;&#26041;&#27861;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#65292;&#20851;&#27880;&#20102;&#36724;&#25215;&#25925;&#38556;&#35786;&#26029;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#21450;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#25391;&#21160;&#25968;&#25454;&#26469;&#39044;&#27979;&#25925;&#38556;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#30001;&#20110;&#36724;&#25215;&#23384;&#22312;&#20110;&#21508;&#20010;&#34892;&#19994;&#30340;&#26059;&#36716;&#35774;&#22791;&#20013;&#65292;&#24182;&#19988;&#23545;&#39640;&#25928;&#36816;&#33829;&#30340;&#38656;&#27714;&#19981;&#26029;&#22686;&#21152;&#65292;&#36724;&#25215;&#25925;&#38556;&#35786;&#26029;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#21450;&#26102;&#26816;&#27979;&#21644;&#20934;&#30830;&#39044;&#27979;&#36724;&#25215;&#25925;&#38556;&#21487;&#20197;&#24110;&#21161;&#20943;&#23569;&#24847;&#22806;&#26426;&#22120;&#20572;&#26426;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#25913;&#36827;&#32500;&#25252;&#35745;&#21010;&#65292;&#20174;&#32780;&#36991;&#20813;&#25439;&#22833;&#20135;&#33021;&#12290;&#26368;&#36817;&#30340;&#25216;&#26415;&#36827;&#27493;&#20351;&#24471;&#21487;&#20197;&#20351;&#29992;&#21508;&#31181;&#20256;&#24863;&#22120;&#23545;&#36825;&#20123;&#35774;&#22791;&#30340;&#20581;&#24247;&#29366;&#20917;&#36827;&#34892;&#35268;&#27169;&#21270;&#30417;&#27979;&#65292;&#24182;&#20351;&#29992;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#21253;&#25324;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#26469;&#39044;&#27979;&#25925;&#38556;&#12290;&#36890;&#36807;&#21152;&#36895;&#36816;&#34892;&#33267;&#25925;&#38556;&#30340;&#36807;&#31243;&#20013;&#37319;&#38598;&#25391;&#21160;&#25968;&#25454;&#65292;&#25110;&#32773;&#36890;&#36807;&#24341;&#20837;&#24050;&#30693;&#25925;&#38556;&#26469;&#37319;&#38598;&#25391;&#21160;&#25968;&#25454;&#65292;&#21487;&#20197;&#22312;&#21508;&#31181;&#24037;&#20316;&#26465;&#20214;&#19979;&#65288;&#22914;&#36716;&#36895;&#12289;&#36724;&#25215;&#36127;&#36733;&#12289;&#36724;&#25215;&#25925;&#38556;&#31867;&#22411;&#21644;&#25968;&#25454;&#37319;&#38598;&#39057;&#29575;&#65289;&#36827;&#34892;&#25968;&#25454;&#37319;&#38598;&#12290;&#28982;&#32780;&#65292;&#22312;&#20351;&#29992;&#25391;&#21160;&#25968;&#25454;&#24320;&#21457;&#36724;&#25215;&#25925;&#38556;&#20998;&#31867;&#27169;&#22411;&#30340;&#36807;&#31243;&#20013;&#23384;&#22312;&#20197;&#19979;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rolling bearing fault diagnosis has garnered increased attention in recent years owing to its presence in rotating machinery across various industries, and an ever increasing demand for efficient operations. Prompt detection and accurate prediction of bearing failures can help reduce the likelihood of unexpected machine downtime and enhance maintenance schedules, averting lost productivity. Recent technological advances have enabled monitoring the health of these assets at scale using a variety of sensors, and predicting the failures using modern Machine Learning (ML) approaches including deep learning architectures. Vibration data has been collected using accelerated run-to-failure of overloaded bearings, or by introducing known failure in bearings, under a variety of operating conditions such as rotating speed, load on the bearing, type of bearing fault, and data acquisition frequency. However, in the development of bearing failure classification models using vibration data there is 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#37327;&#21270;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25511;&#21046;&#31995;&#32479;&#21487;&#38752;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20004;&#20010;&#31070;&#32463;&#32593;&#32476;&#26469;&#35780;&#20272;&#25511;&#21046;&#30340;&#21487;&#38752;&#24615;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2309.16977</link><description>&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#25511;&#21046;&#31995;&#32479;&#20013;&#30340;&#21487;&#38752;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Reliability Quantification of Deep Reinforcement Learning-based Control. (arXiv:2309.16977v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16977
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#37327;&#21270;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25511;&#21046;&#31995;&#32479;&#21487;&#38752;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20004;&#20010;&#31070;&#32463;&#32593;&#32476;&#26469;&#35780;&#20272;&#25511;&#21046;&#30340;&#21487;&#38752;&#24615;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#22312;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;&#20013;&#30340;&#21487;&#38752;&#24615;&#37327;&#21270;&#26159;&#20154;&#24037;&#26234;&#33021;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#37325;&#35201;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#37327;&#21270;DRL&#25511;&#21046;&#21487;&#38752;&#24615;&#30340;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#24212;&#29992;&#20102;&#19968;&#31181;&#29616;&#26377;&#26041;&#27861;&#8212;&#8212;&#38543;&#26426;&#22122;&#22768;&#25552;&#21462;&#65292;&#20197;&#26126;&#30830;&#38656;&#35201;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#20854;&#27425;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#38752;&#24615;&#37327;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#20004;&#20010;&#31070;&#32463;&#32593;&#32476;&#26469;&#37327;&#21270;&#21487;&#38752;&#24615;&#65306;&#21442;&#32771;&#32593;&#32476;&#21644;&#35780;&#20272;&#32593;&#32476;&#12290;&#23427;&#20204;&#20855;&#26377;&#30456;&#21516;&#30340;&#32467;&#26500;&#21644;&#30456;&#21516;&#30340;&#21021;&#22987;&#21442;&#25968;&#12290;&#22312;&#35757;&#32451;&#20043;&#21069;&#65292;&#20004;&#20010;&#32593;&#32476;&#30340;&#36755;&#20986;&#30456;&#21516;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#35780;&#20272;&#32593;&#32476;&#30340;&#21442;&#25968;&#34987;&#26356;&#26032;&#65292;&#20197;&#26368;&#22823;&#21270;&#35757;&#32451;&#25968;&#25454;&#19978;&#30340;&#21442;&#32771;&#32593;&#32476;&#21644;&#35780;&#20272;&#32593;&#32476;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#22240;&#27492;&#65292;&#21487;&#20197;&#22522;&#20110;&#20004;&#20010;&#32593;&#32476;&#30340;&#36755;&#20986;&#24046;&#24322;&#35780;&#20272;&#29305;&#23450;&#29366;&#24577;&#19979;DRL&#25511;&#21046;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reliability quantification of deep reinforcement learning (DRL)-based control is a significant challenge for the practical application of artificial intelligence (AI) in safety-critical systems. This study proposes a method for quantifying the reliability of DRL-based control. First, an existing method, random noise distillation, was applied to the reliability evaluation to clarify the issues to be solved. Second, a novel method for reliability quantification was proposed to solve these issues. The reliability is quantified using two neural networks: reference and evaluator. They have the same structure with the same initial parameters. The outputs of the two networks were the same before training. During training, the evaluator network parameters were updated to maximize the difference between the reference and evaluator networks for trained data. Thus, the reliability of the DRL-based control for a state can be evaluated based on the difference in output between the two networks. The
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24046;&#20998;&#39537;&#21160;&#22686;&#24378;&#23398;&#20064;&#30340;&#37327;&#23376;&#24577;&#21046;&#22791;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#22870;&#21169;&#20989;&#25968;&#21644;&#34892;&#20026;&#36873;&#25321;&#31574;&#30053;&#65292;&#20197;&#25552;&#39640;&#20004;&#27604;&#29305;&#37327;&#23376;&#31995;&#32479;&#30340;&#21046;&#22791;&#36895;&#24230;&#21644;&#20445;&#30495;&#24230;&#12290;</title><link>http://arxiv.org/abs/2309.16972</link><description>&lt;p&gt;
&#22522;&#20110;&#24046;&#20998;&#39537;&#21160;&#22686;&#24378;&#23398;&#20064;&#30340;&#37327;&#23376;&#24577;&#21046;&#22791;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Quantum States Preparation Method Based on Difference-Driven Reinforcement Learning. (arXiv:2309.16972v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16972
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24046;&#20998;&#39537;&#21160;&#22686;&#24378;&#23398;&#20064;&#30340;&#37327;&#23376;&#24577;&#21046;&#22791;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#22870;&#21169;&#20989;&#25968;&#21644;&#34892;&#20026;&#36873;&#25321;&#31574;&#30053;&#65292;&#20197;&#25552;&#39640;&#20004;&#27604;&#29305;&#37327;&#23376;&#31995;&#32479;&#30340;&#21046;&#22791;&#36895;&#24230;&#21644;&#20445;&#30495;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20004;&#27604;&#29305;&#37327;&#23376;&#31995;&#32479;&#30340;&#29366;&#24577;&#31354;&#38388;&#36739;&#22823;&#65292;&#24182;&#19988;&#29616;&#26377;&#37327;&#23376;&#24577;&#21046;&#22791;&#26041;&#27861;&#37319;&#29992;&#38454;&#26799;&#22411;&#22870;&#21169;&#20989;&#25968;&#65292;&#25910;&#25947;&#36895;&#24230;&#24930;&#65292;&#22312;&#26377;&#38480;&#26465;&#20214;&#19979;&#24456;&#38590;&#39640;&#20445;&#30495;&#24230;&#21046;&#22791;&#25152;&#38656;&#30340;&#30446;&#26631;&#37327;&#23376;&#24577;&#12290;&#20026;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#22870;&#21169;&#20989;&#25968;&#21644;&#34892;&#20026;&#36873;&#25321;&#31574;&#30053;&#30340;&#24046;&#20998;&#39537;&#21160;&#22686;&#24378;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#20004;&#27604;&#29305;&#37327;&#23376;&#31995;&#32479;&#30340;&#37327;&#23376;&#24577;&#21046;&#22791;&#12290;&#39318;&#20808;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#21253;&#25324;&#23545;&#37327;&#23376;&#38376;&#31867;&#22411;&#21644;&#37327;&#23376;&#24577;&#28436;&#21270;&#26102;&#38388;&#30340;&#38480;&#21046;&#12290;&#22312;&#21046;&#22791;&#36807;&#31243;&#20013;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#21152;&#26435;&#24046;&#20998;&#21160;&#24577;&#22870;&#21169;&#20989;&#25968;&#65292;&#36741;&#21161;&#31639;&#27861;&#24555;&#36895;&#33719;&#24471;&#26368;&#22823;&#26399;&#26395;&#32047;&#31215;&#22870;&#21169;&#12290;&#28982;&#21518;&#65292;&#37319;&#21462;&#33258;&#36866;&#24212;&#949;-greedy&#34892;&#20026;&#36873;&#25321;&#31574;&#30053;&#65292;&#20197;&#22312;&#25506;&#32034;&#21644;&#21033;&#29992;&#20043;&#38388;&#21462;&#24471;&#19968;&#23450;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the large state space of the two-qubit system, and the adoption of ladder reward function in the existing quantum state preparation methods, the convergence speed is slow and it is difficult to prepare the desired target quantum state with high fidelity under limited conditions. To solve the above problems, a difference-driven reinforcement learning (RL) algorithm for quantum state preparation of two-qubit system is proposed by improving the reward function and action selection strategy. Firstly, a model is constructed for the problem of preparing quantum states of a two-qubit system, with restrictions on the type of quantum gates and the time for quantum state evolution. In the preparation process, a weighted differential dynamic reward function is designed to assist the algorithm quickly obtain the maximum expected cumulative reward. Then, an adaptive e-greedy action selection strategy is adopted to achieve a balance between exploration and utilization to a certain extent, the
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24191;&#20041;&#21487;&#21152;&#27169;&#22411;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#31216;&#20026;&#24191;&#20041;&#21487;&#21152;&#25928;&#29992;&#32593;&#32476;&#65288;GAUNet&#65289;&#65292;&#29992;&#20110;&#31163;&#25955;&#36873;&#25321;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#19978;&#21487;&#20197;&#19982;ASU-DNN&#30456;&#23218;&#32654;&#65292;&#24182;&#19988;&#30456;&#27604;&#20197;&#21069;&#30340;&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.16970</link><description>&lt;p&gt;
&#20855;&#26377;&#24191;&#20041;&#21487;&#21152;&#25928;&#29992;&#32593;&#32476;&#30340;&#31163;&#25955;&#36873;&#25321;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Discrete-Choice Model with Generalized Additive Utility Network. (arXiv:2309.16970v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16970
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24191;&#20041;&#21487;&#21152;&#27169;&#22411;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#31216;&#20026;&#24191;&#20041;&#21487;&#21152;&#25928;&#29992;&#32593;&#32476;&#65288;GAUNet&#65289;&#65292;&#29992;&#20110;&#31163;&#25955;&#36873;&#25321;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#19978;&#21487;&#20197;&#19982;ASU-DNN&#30456;&#23218;&#32654;&#65292;&#24182;&#19988;&#30456;&#27604;&#20197;&#21069;&#30340;&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#25955;&#36873;&#25321;&#27169;&#22411;&#26159;&#20998;&#26512;&#20915;&#31574;&#34892;&#20026;&#30340;&#24378;&#22823;&#26694;&#26550;&#65292;&#20026;&#25919;&#31574;&#21046;&#23450;&#32773;&#21644;&#20225;&#19994;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#20351;&#29992;&#32447;&#24615;&#25928;&#29992;&#20989;&#25968;&#30340;&#22810;&#39033;&#24335;&#36923;&#36753;&#27169;&#22411;&#65288;MNLs&#65289;&#22240;&#20854;&#26131;&#20110;&#20351;&#29992;&#21644;&#21487;&#35299;&#37322;&#24615;&#32780;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#26368;&#36817;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#20855;&#26377;&#31070;&#32463;&#32593;&#32476;&#65288;&#20363;&#22914;ASU-DNN&#65289;&#30340;MNLs&#65292;&#24182;&#19988;&#22312;&#34892;&#20026;&#36873;&#25321;&#30340;&#39044;&#27979;&#31934;&#24230;&#19978;&#27604;&#20256;&#32479;MNLs&#26356;&#39640;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#30001;&#20110;&#22797;&#26434;&#32467;&#26500;&#32780;&#32570;&#20047;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#22522;&#20110;&#24191;&#20041;&#21487;&#21152;&#27169;&#22411;&#24320;&#21457;&#20102;&#19968;&#31181;&#20855;&#26377;&#26032;&#39062;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#25928;&#29992;&#20989;&#25968;&#65292;&#31216;&#20026;&#24191;&#20041;&#21487;&#21152;&#25928;&#29992;&#32593;&#32476;&#65288;GAUNet&#65289;&#65292;&#29992;&#20110;&#31163;&#25955;&#36873;&#25321;&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;&#22312;&#19996;&#20140;&#25910;&#38598;&#30340;&#20986;&#34892;&#35843;&#26597;&#25968;&#25454;&#35780;&#20272;&#20102;&#20855;&#26377;GAUNet&#30340;MNL&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#19978;&#19982;ASU-DNN&#30456;&#24403;&#65292;&#24182;&#19988;&#30456;&#27604;&#20197;&#21069;&#30340;&#27169;&#22411;&#20855;&#26377;&#25913;&#36827;&#30340;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discrete-choice models are a powerful framework for analyzing decision-making behavior to provide valuable insights for policymakers and businesses. Multinomial logit models (MNLs) with linear utility functions have been used in practice because they are ease to use and interpretable. Recently, MNLs with neural networks (e.g., ASU-DNN) have been developed, and they have achieved higher prediction accuracy in behavior choice than classical MNLs. However, these models lack interpretability owing to complex structures. We developed utility functions with a novel neural-network architecture based on generalized additive models, named generalized additive utility network ( GAUNet), for discrete-choice models. We evaluated the performance of the MNL with GAUNet using the trip survey data collected in Tokyo. Our models were comparable to ASU-DNN in accuracy and exhibited improved interpretability compared to previous models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#32452;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#65288;LTL&#65289;&#20844;&#24335;&#65292;&#20171;&#32461;&#20102;&#19968;&#31181;&#29983;&#25104;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#27169;&#25311;&#22842;&#26071;&#29615;&#22659;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.16960</link><description>&lt;p&gt;
&#29983;&#25104;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#35299;&#37322;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Generating Explanations for Reinforcement Learning Policies: An Empirical Study. (arXiv:2309.16960v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16960
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#32452;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#65288;LTL&#65289;&#20844;&#24335;&#65292;&#20171;&#32461;&#20102;&#19968;&#31181;&#29983;&#25104;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#27169;&#25311;&#22842;&#26071;&#29615;&#22659;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#32452;&#35774;&#35745;&#29992;&#20110;&#25552;&#20379;&#31574;&#30053;&#35299;&#37322;&#30340;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#65288;LTL&#65289;&#20844;&#24335;&#12290;&#25105;&#20204;&#30340;&#37325;&#28857;&#26159;&#26500;&#24314;&#26082;&#38416;&#26126;&#31574;&#30053;&#25152;&#23454;&#29616;&#30340;&#26368;&#32456;&#30446;&#26631;&#21448;&#38416;&#26126;&#20854;&#25191;&#34892;&#36807;&#31243;&#20013;&#25152;&#32500;&#25345;&#30340;&#21069;&#25552;&#26465;&#20214;&#30340;&#35299;&#37322;&#12290;&#36825;&#20123;&#22522;&#20110;LTL&#30340;&#35299;&#37322;&#20855;&#26377;&#32467;&#26500;&#21270;&#34920;&#31034;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#23616;&#37096;&#25628;&#32034;&#25216;&#26415;&#12290;&#36890;&#36807;&#27169;&#25311;&#30340;&#22842;&#26071;&#29615;&#22659;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#35770;&#25991;&#26368;&#21518;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#24314;&#35758;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a set of \textit{Linear Temporal Logic} (LTL) formulae designed to provide explanations for policies. Our focus is on crafting explanations that elucidate both the ultimate objectives accomplished by the policy and the prerequisites it upholds throughout its execution. These LTL-based explanations feature a structured representation, which is particularly well-suited for local-search techniques. The effectiveness of our proposed approach is illustrated through a simulated capture the flag environment. The paper concludes with suggested directions for future research.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21435;&#22122;&#25193;&#25955;&#26725;&#27169;&#22411;&#65288;DDBMs&#65289;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#23398;&#20064;&#25193;&#25955;&#26725;&#30340;&#20998;&#25968;&#65292;&#24182;&#22522;&#20110;&#23398;&#20064;&#21040;&#30340;&#20998;&#25968;&#27714;&#35299;&#24494;&#20998;&#26041;&#31243;&#26469;&#23454;&#29616;&#20174;&#19968;&#20010;&#20998;&#24067;&#21040;&#21478;&#19968;&#20010;&#20998;&#24067;&#30340;&#26144;&#23556;&#65292;&#20174;&#32780;&#23558;&#22810;&#31867;&#29983;&#25104;&#27169;&#22411;&#32479;&#19968;&#36215;&#26469;&#12290;</title><link>http://arxiv.org/abs/2309.16948</link><description>&lt;p&gt;
&#21435;&#22122;&#25193;&#25955;&#26725;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Denoising Diffusion Bridge Models. (arXiv:2309.16948v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16948
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21435;&#22122;&#25193;&#25955;&#26725;&#27169;&#22411;&#65288;DDBMs&#65289;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#23398;&#20064;&#25193;&#25955;&#26725;&#30340;&#20998;&#25968;&#65292;&#24182;&#22522;&#20110;&#23398;&#20064;&#21040;&#30340;&#20998;&#25968;&#27714;&#35299;&#24494;&#20998;&#26041;&#31243;&#26469;&#23454;&#29616;&#20174;&#19968;&#20010;&#20998;&#24067;&#21040;&#21478;&#19968;&#20010;&#20998;&#24067;&#30340;&#26144;&#23556;&#65292;&#20174;&#32780;&#23558;&#22810;&#31867;&#29983;&#25104;&#27169;&#22411;&#32479;&#19968;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26159;&#24378;&#22823;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#20351;&#29992;&#38543;&#26426;&#36807;&#31243;&#23558;&#22122;&#22768;&#26144;&#23556;&#21040;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#35768;&#22810;&#24212;&#29992;&#65292;&#22914;&#22270;&#20687;&#32534;&#36753;&#65292;&#27169;&#22411;&#30340;&#36755;&#20837;&#19981;&#26159;&#38543;&#26426;&#22122;&#22768;&#30340;&#20998;&#24067;&#12290;&#22240;&#27492;&#65292;&#25193;&#25955;&#27169;&#22411;&#24517;&#39035;&#20381;&#36182;&#20110;&#32321;&#29712;&#30340;&#26041;&#27861;&#65292;&#22914;&#24341;&#23548;&#25110;&#25237;&#24433;&#37319;&#26679;&#65292;&#20197;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#21152;&#20837;&#36825;&#20123;&#20449;&#24687;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21435;&#22122;&#25193;&#25955;&#26725;&#27169;&#22411;&#65288;DDBMs&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#26725;&#30340;&#33539;&#20363;&#65292;&#25193;&#25955;&#26725;&#26159;&#19968;&#26063;&#36807;&#31243;&#65292;&#20854;&#22312;&#32473;&#23450;&#30340;&#31471;&#28857;&#19979;&#25554;&#20540;&#20004;&#20010;&#37197;&#23545;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#25193;&#25955;&#26725;&#30340;&#20998;&#25968;&#65292;&#22522;&#20110;&#23398;&#20064;&#21040;&#30340;&#20998;&#25968;&#27714;&#35299;&#19968;&#20010;&#65288;&#38543;&#26426;&#30340;&#65289;&#24494;&#20998;&#26041;&#31243;&#65292;&#20174;&#19968;&#20010;&#31471;&#28857;&#20998;&#24067;&#26144;&#23556;&#21040;&#21478;&#19968;&#20010;&#31471;&#28857;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#33258;&#28982;&#22320;&#32479;&#19968;&#20102;&#20960;&#31867;&#29983;&#25104;&#27169;&#22411;&#65292;&#22914;&#22522;&#20110;&#20998;&#25968;&#30340;&#25193;&#25955;&#27169;&#22411;&#21644;OT-Flow-Matching&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#29616;&#26377;&#30340;&#35774;&#35745;&#21644;&#26550;&#26500;&#36873;&#25321;&#36866;&#24212;&#21040;&#26356;&#36890;&#29992;&#30340;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models are powerful generative models that map noise to data using stochastic processes. However, for many applications such as image editing, the model input comes from a distribution that is not random noise. As such, diffusion models must rely on cumbersome methods like guidance or projected sampling to incorporate this information in the generative process. In our work, we propose Denoising Diffusion Bridge Models (DDBMs), a natural alternative to this paradigm based on diffusion bridges, a family of processes that interpolate between two paired distributions given as endpoints. Our method learns the score of the diffusion bridge from data and maps from one endpoint distribution to the other by solving a (stochastic) differential equation based on the learned score. Our method naturally unifies several classes of generative models, such as score-based diffusion models and OT-Flow-Matching, allowing us to adapt existing design and architectural choices to our more general 
&lt;/p&gt;</description></item><item><title>CoBEVFlow&#26159;&#19968;&#31181;&#40065;&#26834;&#30340;&#24322;&#27493;&#21327;&#20316;&#24335;&#19977;&#32500;&#26816;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#34917;&#20607;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#24322;&#27493;&#21327;&#20316;&#20449;&#24687;&#23545;&#40784;&#26469;&#35299;&#20915;&#20449;&#24687;&#19981;&#21305;&#37197;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.16940</link><description>&lt;p&gt;
&#40065;&#26834;&#30340;&#24322;&#27493;&#21327;&#20316;&#24335;&#40479;&#30640;&#35270;&#35282;&#19977;&#32500;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Robust Asynchronous Collaborative 3D Detection via Bird's Eye View Flow. (arXiv:2309.16940v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16940
&lt;/p&gt;
&lt;p&gt;
CoBEVFlow&#26159;&#19968;&#31181;&#40065;&#26834;&#30340;&#24322;&#27493;&#21327;&#20316;&#24335;&#19977;&#32500;&#26816;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#34917;&#20607;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#24322;&#27493;&#21327;&#20316;&#20449;&#24687;&#23545;&#40784;&#26469;&#35299;&#20915;&#20449;&#24687;&#19981;&#21305;&#37197;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20419;&#36827;&#22810;&#20010;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#27807;&#36890;&#65292;&#21327;&#20316;&#24863;&#30693;&#21487;&#20197;&#22823;&#22823;&#25552;&#21319;&#27599;&#20010;&#26234;&#33021;&#20307;&#30340;&#24863;&#30693;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#30001;&#20110;&#36890;&#20449;&#24310;&#36831;&#12289;&#20013;&#26029;&#21644;&#26102;&#38047;&#19981;&#23545;&#40784;&#65292;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#26102;&#38388;&#19981;&#21516;&#27493;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#12290;&#36825;&#20010;&#38382;&#39064;&#22312;&#22810;&#26234;&#33021;&#20307;&#34701;&#21512;&#36807;&#31243;&#20013;&#23548;&#33268;&#20449;&#24687;&#19981;&#21305;&#37197;&#65292;&#20005;&#37325;&#21160;&#25671;&#20102;&#21327;&#20316;&#30340;&#22522;&#30784;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CoBEVFlow&#65292;&#19968;&#31181;&#22522;&#20110;&#40479;&#30640;&#35270;&#35282;&#27969;&#30340;&#24322;&#27493;&#40065;&#26834;&#21327;&#20316;&#24335;&#19977;&#32500;&#24863;&#30693;&#31995;&#32479;&#12290;CoBEVFlow&#30340;&#20851;&#38190;&#35266;&#28857;&#26159;&#36890;&#36807;&#34917;&#20607;&#36816;&#21160;&#26469;&#20351;&#22810;&#20010;&#26234;&#33021;&#20307;&#21457;&#36865;&#30340;&#24322;&#27493;&#21327;&#20316;&#20449;&#24687;&#23545;&#40784;&#12290;&#20026;&#20102;&#24314;&#27169;&#22330;&#26223;&#20013;&#30340;&#36816;&#21160;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#40479;&#30640;&#35270;&#35282;&#27969;&#65292;&#23427;&#26159;&#19982;&#27599;&#20010;&#31354;&#38388;&#20301;&#32622;&#23545;&#24212;&#30340;&#36816;&#21160;&#21521;&#37327;&#30340;&#38598;&#21512;&#12290;&#22522;&#20110;&#40479;&#30640;&#35270;&#35282;&#27969;&#65292;&#24322;&#27493;&#24863;&#30693;&#29305;&#24449;&#21487;&#20197;&#37325;&#26032;&#20998;&#37197;&#21040;&#36866;&#24403;&#30340;&#20301;&#32622;&#65292;&#20943;&#36731;&#24322;&#27493;&#30340;&#24433;&#21709;&#12290;CoBEVFlow&#20855;&#26377;&#20004;&#20010;&#20248;&#28857;&#65306;(i) CoBEVFlow&#21487;&#20197;&#22788;&#29702;&#24322;&#27493;&#21327;&#20316;
&lt;/p&gt;
&lt;p&gt;
By facilitating communication among multiple agents, collaborative perception can substantially boost each agent's perception ability. However, temporal asynchrony among agents is inevitable in real-world due to communication delays, interruptions, and clock misalignments. This issue causes information mismatch during multi-agent fusion, seriously shaking the foundation of collaboration. To address this issue, we propose CoBEVFlow, an asynchrony-robust collaborative 3D perception system based on bird's eye view (BEV) flow. The key intuition of CoBEVFlow is to compensate motions to align asynchronous collaboration messages sent by multiple agents. To model the motion in a scene, we propose BEV flow, which is a collection of the motion vector corresponding to each spatial location. Based on BEV flow, asynchronous perceptual features can be reassigned to appropriate positions, mitigating the impact of asynchrony. CoBEVFlow has two advantages: (i) CoBEVFlow can handle asynchronous collabor
&lt;/p&gt;</description></item><item><title>PC-Adapter&#26159;&#19968;&#31181;&#25299;&#25169;&#24863;&#30693;&#36866;&#37197;&#22120;&#65292;&#36890;&#36807;&#20445;&#30041;&#20840;&#23616;&#24418;&#29366;&#20449;&#24687;&#21644;&#23398;&#20064;&#30446;&#26631;&#39046;&#22495;&#30340;&#23616;&#37096;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#28857;&#20113;&#39046;&#22495;&#33258;&#36866;&#24212;&#12290;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#25193;&#23637;&#30340;&#20266;&#26631;&#31614;&#30699;&#27491;&#31574;&#30053;&#65292;&#20197;&#35299;&#20915;&#20266;&#26631;&#31614;&#30340;&#35823;&#31867;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.16936</link><description>&lt;p&gt;
PC-Adapter: &#22522;&#20110;&#25311;&#21512;&#20266;&#26631;&#31614;&#30340;&#28857;&#20113;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#25299;&#25169;&#24863;&#30693;&#36866;&#37197;&#22120;
&lt;/p&gt;
&lt;p&gt;
PC-Adapter: Topology-Aware Adapter for Efficient Domain Adaption on Point Clouds with Rectified Pseudo-label. (arXiv:2309.16936v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16936
&lt;/p&gt;
&lt;p&gt;
PC-Adapter&#26159;&#19968;&#31181;&#25299;&#25169;&#24863;&#30693;&#36866;&#37197;&#22120;&#65292;&#36890;&#36807;&#20445;&#30041;&#20840;&#23616;&#24418;&#29366;&#20449;&#24687;&#21644;&#23398;&#20064;&#30446;&#26631;&#39046;&#22495;&#30340;&#23616;&#37096;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#28857;&#20113;&#39046;&#22495;&#33258;&#36866;&#24212;&#12290;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#25193;&#23637;&#30340;&#20266;&#26631;&#31614;&#30699;&#27491;&#31574;&#30053;&#65292;&#20197;&#35299;&#20915;&#20266;&#26631;&#31614;&#30340;&#35823;&#31867;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#29289;&#20307;&#23610;&#24230;&#65292;&#20256;&#24863;&#22120;&#35282;&#24230;&#21644;&#33258;&#36974;&#25377;&#24341;&#36215;&#30340;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#65292;&#29702;&#35299;&#20174;&#30495;&#23454;&#19990;&#30028;&#20013;&#25429;&#33719;&#30340;&#28857;&#20113;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#36890;&#36807;&#32467;&#21512;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#33258;&#35757;&#32451;&#21644;&#23545;&#25239;&#35757;&#32451;&#31561;&#26368;&#26032;&#23398;&#20064;&#21407;&#21017;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#36825;&#23548;&#33268;&#20102;&#26174;&#33879;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;&#20026;&#20102;&#31616;&#27905;&#32780;&#24378;&#22823;&#22320;&#23545;&#28857;&#20113;&#36827;&#34892;&#39046;&#22495;&#33258;&#36866;&#24212;&#65292;&#25105;&#20204;&#37325;&#26032;&#24605;&#32771;&#20102;&#28857;&#20113;&#25968;&#25454;&#22312;&#39046;&#22495;&#20559;&#31227;&#22330;&#26223;&#19979;&#30340;&#29420;&#29305;&#25361;&#25112;&#65292;&#24182;&#21457;&#29616;&#20445;&#25345;&#28304;&#39046;&#22495;&#30340;&#20840;&#23616;&#20960;&#20309;&#24418;&#29366;&#20449;&#24687;&#20197;&#21450;&#30446;&#26631;&#20266;&#26631;&#31614;&#36235;&#21183;&#23545;&#28304;&#26631;&#31614;&#20998;&#24067;&#26377;&#20559;&#30340;&#37325;&#35201;&#24615;&#12290;&#22312;&#25105;&#20204;&#30340;&#35266;&#23519;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#37197;&#22120;&#24341;&#23548;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;PC-Adapter&#65292;&#36890;&#36807;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#36866;&#37197;&#22120;&#20445;&#30041;&#28304;&#39046;&#22495;&#30340;&#20840;&#23616;&#24418;&#29366;&#20449;&#24687;&#65292;&#21516;&#26102;&#36890;&#36807;&#21478;&#19968;&#20010;&#37197;&#22791;&#20102;&#22270;&#21367;&#31215;&#30340;&#36866;&#37197;&#22120;&#23398;&#20064;&#30446;&#26631;&#39046;&#22495;&#30340;&#23616;&#37096;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#25193;&#23637;&#30340;&#20266;&#26631;&#31614;&#30699;&#27491;&#31574;&#30053;&#65292;&#26377;&#25928;&#20943;&#23569;&#20266;&#26631;&#31614;&#30340;&#35823;&#31867;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding point clouds captured from the real-world is challenging due to shifts in data distribution caused by varying object scales, sensor angles, and self-occlusion. Prior works have addressed this issue by combining recent learning principles such as self-supervised learning, self-training, and adversarial training, which leads to significant computational overhead.Toward succinct yet powerful domain adaptation for point clouds, we revisit the unique challenges of point cloud data under domain shift scenarios and discover the importance of the global geometry of source data and trends of target pseudo-labels biased to the source label distribution. Motivated by our observations, we propose an adapter-guided domain adaptation method, PC-Adapter, that preserves the global shape information of the source domain using an attention-based adapter, while learning the local characteristics of the target domain via another adapter equipped with graph convolution. Additionally, we propo
&lt;/p&gt;</description></item><item><title>TranDRL&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#39537;&#21160;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25903;&#25345;&#30340;&#39044;&#38450;&#24615;&#32500;&#25252;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#22797;&#26434;&#26102;&#38388;&#27169;&#24335;&#25429;&#25417;&#21644;&#32463;&#27982;&#39640;&#25928;&#32500;&#25252;&#24314;&#35758;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#21097;&#20313;&#23551;&#21629;&#65288;RUL&#65289;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#32500;&#25252;&#34892;&#21160;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2309.16935</link><description>&lt;p&gt;
TranDRL&#65306;&#19968;&#31181;&#22522;&#20110;Transformer&#39537;&#21160;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25903;&#25345;&#30340;&#39044;&#38450;&#24615;&#32500;&#25252;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
TranDRL: A Transformer-Driven Deep Reinforcement Learning Enabled Prescriptive Maintenance Framework. (arXiv:2309.16935v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16935
&lt;/p&gt;
&lt;p&gt;
TranDRL&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#39537;&#21160;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25903;&#25345;&#30340;&#39044;&#38450;&#24615;&#32500;&#25252;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#22797;&#26434;&#26102;&#38388;&#27169;&#24335;&#25429;&#25417;&#21644;&#32463;&#27982;&#39640;&#25928;&#32500;&#25252;&#24314;&#35758;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#21097;&#20313;&#23551;&#21629;&#65288;RUL&#65289;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#32500;&#25252;&#34892;&#21160;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#19994;&#31995;&#32479;&#38656;&#35201;&#21487;&#38752;&#30340;&#39044;&#27979;&#24615;&#32500;&#25252;&#31574;&#30053;&#26469;&#25552;&#39640;&#36816;&#33829;&#25928;&#29575;&#24182;&#20943;&#23569;&#20572;&#26426;&#26102;&#38388;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32508;&#21512;&#26694;&#26550;&#65292;&#21033;&#29992;Transformer&#31070;&#32463;&#32593;&#32476;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#31639;&#27861;&#26469;&#20248;&#21270;&#32500;&#25252;&#34892;&#21160;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;Transformer&#27169;&#22411;&#26469;&#26377;&#25928;&#25429;&#25417;&#20256;&#24863;&#22120;&#25968;&#25454;&#20013;&#30340;&#22797;&#26434;&#26102;&#38388;&#27169;&#24335;&#65292;&#20174;&#32780;&#20934;&#30830;&#39044;&#27979;&#35774;&#22791;&#30340;&#21097;&#20313;&#23551;&#21629;&#65288;RUL&#65289;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#26694;&#26550;&#20013;&#30340;DRL&#32452;&#20214;&#25552;&#20379;&#20102;&#32463;&#27982;&#39640;&#25928;&#21644;&#21450;&#26102;&#30340;&#32500;&#25252;&#24314;&#35758;&#12290;&#25105;&#20204;&#22312;NASA C-MPASS&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#65292;&#32467;&#26524;&#26174;&#31034;&#22312;RUL&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#32500;&#25252;&#34892;&#21160;&#20248;&#21270;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#21019;&#26032;&#26041;&#27861;&#20026;&#39044;&#38450;&#24615;&#32500;&#25252;&#25552;&#20379;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#24037;&#19994;&#36816;&#33829;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#24182;&#24102;&#26469;&#20102;&#26356;&#22810;&#21457;&#23637;&#26426;&#36935;&#12290;
&lt;/p&gt;
&lt;p&gt;
Industrial systems demand reliable predictive maintenance strategies to enhance operational efficiency and reduce downtime. This paper introduces a novel, integrated framework that leverages the power of transformer neural networks and deep reinforcement learning (DRL) algorithms to optimize maintenance actions. Our approach employs the transformer model to effectively capture complex temporal patterns in sensor data, thereby accurately predicting the Remaining Useful Life (RUL) of equipment. Simultaneously, the DRL component of our framework provides cost-effective and timely maintenance recommendations. We validate the efficacy of our framework on the NASA C-MPASS dataset, where it demonstrates significant advancements in both RUL prediction accuracy and the optimization of maintenance actions. Consequently, our pioneering approach provides an innovative data-driven methodology for prescriptive maintenance, addressing key challenges in industrial operations and leading the way to mor
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24178;&#39044;&#24863;&#30693;&#30340;&#27010;&#24565;&#23884;&#20837;&#27169;&#22411;&#65292;&#29992;&#20110;&#25552;&#39640;&#31070;&#32463;&#26550;&#26500;&#23545;&#27010;&#24565;&#24178;&#39044;&#30340;&#21709;&#24212;&#24615;&#65292;&#24182;&#35299;&#20915;&#20102;&#27010;&#24565;&#24178;&#39044;&#39034;&#24207;&#21644;&#27169;&#22411;&#26550;&#26500;&#30340;&#20381;&#36182;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.16928</link><description>&lt;p&gt;
&#23398;&#20064;&#25509;&#21463;&#24110;&#21161;&#65306;&#24178;&#39044;&#24863;&#30693;&#30340;&#27010;&#24565;&#23884;&#20837;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning to Receive Help: Intervention-Aware Concept Embedding Models. (arXiv:2309.16928v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16928
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24178;&#39044;&#24863;&#30693;&#30340;&#27010;&#24565;&#23884;&#20837;&#27169;&#22411;&#65292;&#29992;&#20110;&#25552;&#39640;&#31070;&#32463;&#26550;&#26500;&#23545;&#27010;&#24565;&#24178;&#39044;&#30340;&#21709;&#24212;&#24615;&#65292;&#24182;&#35299;&#20915;&#20102;&#27010;&#24565;&#24178;&#39044;&#39034;&#24207;&#21644;&#27169;&#22411;&#26550;&#26500;&#30340;&#20381;&#36182;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65288;CBMs&#65289;&#36890;&#36807;&#20351;&#29992;&#19968;&#32452;&#39640;&#32423;&#27010;&#24565;&#26500;&#24314;&#21644;&#35299;&#37322;&#31070;&#32463;&#26550;&#26500;&#30340;&#39044;&#27979;&#65292;&#20197;&#35299;&#20915;&#20854;&#19981;&#36879;&#26126;&#24615;&#30340;&#38382;&#39064;&#12290;&#36825;&#20123;&#27169;&#22411;&#30340;&#19968;&#20010;&#29305;&#27530;&#23646;&#24615;&#26159;&#23427;&#20204;&#20801;&#35768;&#27010;&#24565;&#24178;&#39044;&#65292;&#29992;&#25143;&#21487;&#20197;&#32416;&#27491;&#34987;&#38169;&#35823;&#39044;&#27979;&#30340;&#27010;&#24565;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#24178;&#39044;&#26377;&#25928;&#24615;&#21487;&#33021;&#20005;&#37325;&#20381;&#36182;&#20110;&#24178;&#39044;&#27010;&#24565;&#30340;&#39034;&#24207;&#20197;&#21450;&#27169;&#22411;&#30340;&#26550;&#26500;&#21644;&#35757;&#32451;&#36229;&#21442;&#25968;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36825;&#28304;&#20110;CBM&#22312;&#35757;&#32451;&#26102;&#32570;&#20047;&#27169;&#22411;&#36866;&#24212;&#27010;&#24565;&#24178;&#39044;&#30340;&#28608;&#21169;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24178;&#39044;&#24863;&#30693;&#30340;&#27010;&#24565;&#23884;&#20837;&#27169;&#22411;&#65288;IntCEMs&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;CBM&#30340;&#26032;&#22411;&#26550;&#26500;&#21644;&#35757;&#32451;&#33539;&#24335;&#65292;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#23545;&#27979;&#35797;&#26102;&#24178;&#39044;&#30340;&#21709;&#24212;&#24615;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#23398;&#20064;&#20102;&#19968;&#20010;&#27010;&#24565;&#24178;&#39044;&#31574;&#30053;&#65292;&#20174;&#20013;&#21487;&#20197;&#37319;&#26679;&#26377;&#24847;&#20041;&#30340;&#24178;&#39044;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Concept Bottleneck Models (CBMs) tackle the opacity of neural architectures by constructing and explaining their predictions using a set of high-level concepts. A special property of these models is that they permit concept interventions, wherein users can correct mispredicted concepts and thus improve the model's performance. Recent work, however, has shown that intervention efficacy can be highly dependent on the order in which concepts are intervened on and on the model's architecture and training hyperparameters. We argue that this is rooted in a CBM's lack of train-time incentives for the model to be appropriately receptive to concept interventions. To address this, we propose Intervention-aware Concept Embedding models (IntCEMs), a novel CBM-based architecture and training paradigm that improves a model's receptiveness to test-time interventions. Our model learns a concept intervention policy in an end-to-end fashion from where it can sample meaningful intervention trajectories a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#27169;&#24335;&#36830;&#36890;&#24615;&#30340;&#32463;&#39564;&#21644;&#29702;&#35770;&#30740;&#31350;&#21457;&#29616;&#65292;&#20943;&#23569;&#25968;&#25454;&#24322;&#36136;&#24615;&#21487;&#20197;&#22686;&#21152;&#23458;&#25143;&#31471;&#21644;&#20840;&#23616;&#27169;&#24335;&#20043;&#38388;&#30340;&#36830;&#36890;&#24615;&#65292;&#24314;&#31435;&#20102;&#20840;&#23616;&#27169;&#24335;&#36830;&#36890;&#24615;&#30340;&#23450;&#37327;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2309.16923</link><description>&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#30340;&#27169;&#24335;&#36830;&#36890;&#24615;&#21644;&#25968;&#25454;&#24322;&#36136;&#24615;
&lt;/p&gt;
&lt;p&gt;
Mode Connectivity and Data Heterogeneity of Federated Learning. (arXiv:2309.16923v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16923
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#27169;&#24335;&#36830;&#36890;&#24615;&#30340;&#32463;&#39564;&#21644;&#29702;&#35770;&#30740;&#31350;&#21457;&#29616;&#65292;&#20943;&#23569;&#25968;&#25454;&#24322;&#36136;&#24615;&#21487;&#20197;&#22686;&#21152;&#23458;&#25143;&#31471;&#21644;&#20840;&#23616;&#27169;&#24335;&#20043;&#38388;&#30340;&#36830;&#36890;&#24615;&#65292;&#24314;&#31435;&#20102;&#20840;&#23616;&#27169;&#24335;&#36830;&#36890;&#24615;&#30340;&#23450;&#37327;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#21487;&#20197;&#35753;&#22810;&#20010;&#23458;&#25143;&#31471;&#22312;&#20445;&#25345;&#25968;&#25454;&#31169;&#23494;&#24615;&#30340;&#21516;&#26102;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#25968;&#25454;&#24322;&#36136;&#24615;&#23548;&#33268;&#20102;&#26356;&#26032;&#20043;&#38388;&#30340;&#28418;&#31227;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#23458;&#25143;&#31471;&#21644;&#20840;&#23616;&#27169;&#24335;&#20043;&#38388;&#30340;&#20851;&#31995;&#30740;&#31350;&#36739;&#23569;&#65292;&#19981;&#28165;&#26970;&#36825;&#20123;&#26356;&#26032;&#28418;&#31227;&#21040;&#20102;&#21738;&#37324;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#27169;&#24335;&#36830;&#36890;&#24615;&#36827;&#34892;&#32463;&#39564;&#21644;&#29702;&#35770;&#30740;&#31350;&#65292;&#27169;&#24335;&#36830;&#36890;&#24615;&#21487;&#20197;&#34913;&#37327;&#22312;&#19981;&#21516;&#27169;&#24335;&#20043;&#38388;&#30340;&#21442;&#25968;&#36335;&#24452;&#19978;&#24615;&#33021;&#30340;&#21464;&#21270;&#65288;&#21363;&#36830;&#36890;&#24615;&#65289;&#12290;&#32463;&#39564;&#35777;&#26126;&#65292;&#20943;&#23569;&#25968;&#25454;&#24322;&#36136;&#24615;&#20351;&#24471;&#19981;&#21516;&#36335;&#24452;&#19978;&#30340;&#36830;&#36890;&#24615;&#26356;&#21152;&#30456;&#20284;&#65292;&#24418;&#25104;&#20102;&#23458;&#25143;&#31471;&#21644;&#20840;&#23616;&#27169;&#24335;&#20043;&#38388;&#26356;&#22810;&#30340;&#20302;&#35823;&#24046;&#37325;&#21472;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#22312;&#32447;&#24615;&#36830;&#25509;&#20004;&#20010;&#20840;&#23616;&#27169;&#24335;&#26102;&#23384;&#22312;&#36830;&#36890;&#24615;&#38556;&#30861;&#65292;&#32780;&#32771;&#34385;&#38750;&#32447;&#24615;&#27169;&#24335;&#36830;&#36890;&#24615;&#21518;&#36830;&#36890;&#24615;&#38556;&#30861;&#28040;&#22833;&#12290;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#20351;&#29992;&#22343;&#22330;&#29702;&#35770;&#25110;dropout&#26041;&#27861;&#65292;&#24314;&#31435;&#20102;&#20840;&#23616;&#27169;&#24335;&#36830;&#36890;&#24615;&#30340;&#23450;&#37327;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) enables multiple clients to train a model while keeping their data private collaboratively. Previous studies have shown that data heterogeneity between clients leads to drifts across client updates. However, there are few studies on the relationship between client and global modes, making it unclear where these updates end up drifting. We perform empirical and theoretical studies on this relationship by utilizing mode connectivity, which measures performance change (i.e., connectivity) along parametric paths between different modes. Empirically, reducing data heterogeneity makes the connectivity on different paths more similar, forming more low-error overlaps between client and global modes. We also find that a barrier to connectivity occurs when linearly connecting two global modes, while it disappears with considering non-linear mode connectivity. Theoretically, we establish a quantitative bound on the global-mode connectivity using mean-field theory or dropou
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#26041;&#27861;ACGAN-GNNExplainer&#65292;&#23558;&#36741;&#21161;&#20998;&#31867;&#22120;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;ACGAN&#65289;&#24341;&#20837;&#21040;GNN&#35299;&#37322;&#39046;&#22495;&#12290;&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#22120;&#20026;&#21407;&#22987;&#36755;&#20837;&#22270;&#29983;&#25104;&#35299;&#37322;&#65292;&#24182;&#20511;&#21161;&#37492;&#21035;&#22120;&#30417;&#30563;&#29983;&#25104;&#36807;&#31243;&#65292;&#25552;&#39640;&#35299;&#37322;&#30340;&#20934;&#30830;&#24615;&#21644;&#20445;&#30495;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.16918</link><description>&lt;p&gt;
ACGAN-GNNExplainer&#65306;&#29992;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#36741;&#21161;&#26465;&#20214;&#29983;&#25104;&#35299;&#37322;&#22120;
&lt;/p&gt;
&lt;p&gt;
ACGAN-GNNExplainer: Auxiliary Conditional Generative Explainer for Graph Neural Networks. (arXiv:2309.16918v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16918
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#26041;&#27861;ACGAN-GNNExplainer&#65292;&#23558;&#36741;&#21161;&#20998;&#31867;&#22120;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;ACGAN&#65289;&#24341;&#20837;&#21040;GNN&#35299;&#37322;&#39046;&#22495;&#12290;&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#22120;&#20026;&#21407;&#22987;&#36755;&#20837;&#22270;&#29983;&#25104;&#35299;&#37322;&#65292;&#24182;&#20511;&#21161;&#37492;&#21035;&#22120;&#30417;&#30563;&#29983;&#25104;&#36807;&#31243;&#65292;&#25552;&#39640;&#35299;&#37322;&#30340;&#20934;&#30830;&#24615;&#21644;&#20445;&#30495;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#24050;&#32463;&#22312;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#20013;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#20294;&#20854;&#22522;&#26412;&#26426;&#21046;&#20173;&#28982;&#26159;&#19968;&#20010;&#35868;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#24182;&#23454;&#29616;&#21487;&#38752;&#30340;&#20915;&#31574;&#65292;&#36817;&#24180;&#26469;&#25552;&#20986;&#20102;&#35768;&#22810;GNN&#35299;&#37322;&#22120;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24120;&#24120;&#38754;&#20020;&#19968;&#20123;&#38480;&#21046;&#65292;&#21253;&#25324;&#23545;&#29305;&#23450;&#23454;&#20363;&#30340;&#20381;&#36182;&#24615;&#65292;&#23545;&#26410;&#35265;&#36807;&#30340;&#22270;&#30340;&#19968;&#33324;&#24615;&#19981;&#36275;&#65292;&#21487;&#33021;&#20135;&#29983;&#26080;&#25928;&#30340;&#35299;&#37322;&#20197;&#21450;&#29983;&#25104;&#36807;&#31243;&#20013;&#20135;&#29983;&#30340;&#19981;&#20805;&#20998;&#30340;&#20445;&#30495;&#24230;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#26412;&#25991;&#23558;&#36741;&#21161;&#20998;&#31867;&#22120;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;ACGAN&#65289;&#24341;&#20837;&#21040;GNN&#35299;&#37322;&#39046;&#22495;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;GNN&#35299;&#37322;&#22120;ACGAN-GNNExplainer&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#29983;&#25104;&#22120;&#20026;&#21407;&#22987;&#36755;&#20837;&#22270;&#29983;&#25104;&#35299;&#37322;&#65292;&#24182;&#36816;&#29992;&#37492;&#21035;&#22120;&#26469;&#30417;&#30563;&#29983;&#25104;&#36807;&#31243;&#65292;&#30830;&#20445;&#35299;&#37322;&#30340;&#20445;&#30495;&#24230;&#24182;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#35780;&#20272;&#20998;&#21035;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#22270;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) have proven their efficacy in a variety of real-world applications, but their underlying mechanisms remain a mystery. To address this challenge and enable reliable decision-making, many GNN explainers have been proposed in recent years. However, these methods often encounter limitations, including their dependence on specific instances, lack of generalizability to unseen graphs, producing potentially invalid explanations, and yielding inadequate fidelity. To overcome these limitations, we, in this paper, introduce the Auxiliary Classifier Generative Adversarial Network (ACGAN) into the field of GNN explanation and propose a new GNN explainer dubbed~\emph{ACGAN-GNNExplainer}. Our approach leverages a generator to produce explanations for the original input graphs while incorporating a discriminator to oversee the generation process, ensuring explanation fidelity and improving accuracy. Experimental evaluations conducted on both synthetic and real-world graph
&lt;/p&gt;</description></item><item><title>ONNXExplainer&#26159;&#19968;&#20010;&#22522;&#20110;ONNX&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#20351;&#29992;Shapley&#20540;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#12290;&#23427;&#25552;&#20379;&#20102;&#33258;&#21160;&#24494;&#20998;&#21644;&#20248;&#21270;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#19968;&#27425;&#24615;&#37096;&#32626;&#21644;&#39640;&#25928;&#30340;&#35299;&#37322;&#35745;&#31639;&#12290;</title><link>http://arxiv.org/abs/2309.16916</link><description>&lt;p&gt;
ONNXExplainer:&#22522;&#20110;ONNX&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#20351;&#29992;Shapley&#20540;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
ONNXExplainer: an ONNX Based Generic Framework to Explain Neural Networks Using Shapley Values. (arXiv:2309.16916v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16916
&lt;/p&gt;
&lt;p&gt;
ONNXExplainer&#26159;&#19968;&#20010;&#22522;&#20110;ONNX&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#20351;&#29992;Shapley&#20540;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#12290;&#23427;&#25552;&#20379;&#20102;&#33258;&#21160;&#24494;&#20998;&#21644;&#20248;&#21270;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#19968;&#27425;&#24615;&#37096;&#32626;&#21644;&#39640;&#25928;&#30340;&#35299;&#37322;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20026;&#20160;&#20040;&#20250;&#20570;&#20986;&#26576;&#20123;&#20915;&#31574;&#19982;&#25512;&#29702;&#24615;&#33021;&#19968;&#26679;&#37325;&#35201;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#26469;&#24110;&#21161;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#39044;&#27979;&#65292;&#20854;&#20013;Shapley&#20540;&#26368;&#21463;&#27426;&#36814;&#12290;SHAP&#21253;&#26159;&#35299;&#37322;&#20351;&#29992;TensorFlow&#25110;PyTorch&#23454;&#29616;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;Shapley&#20540;&#30340;&#39046;&#20808;&#23454;&#29616;&#65292;&#20294;&#32570;&#20047;&#36328;&#24179;&#21488;&#25903;&#25345;&#12289;&#19968;&#27425;&#24615;&#37096;&#32626;&#19988;&#25928;&#29575;&#20302;&#19979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ONNXExplainer&#65292;&#23427;&#26159;&#19968;&#20010;&#20351;&#29992;ONNX&#29983;&#24577;&#31995;&#32479;&#20013;&#30340;Shapley&#20540;&#26469;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#30340;&#36890;&#29992;&#26694;&#26550;&#12290;&#22312;ONNXExplainer&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#33258;&#24049;&#30340;&#33258;&#21160;&#24494;&#20998;&#21644;&#20248;&#21270;&#26041;&#27861;&#65292;&#19981;&#20165;&#23454;&#29616;&#20102;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#21644;&#35299;&#37322;&#30340;&#19968;&#27425;&#24615;&#37096;&#32626;&#65292;&#36824;&#26174;&#33879;&#25552;&#39640;&#20102;&#35299;&#37322;&#30340;&#25928;&#29575;&#65292;&#24182;&#20943;&#23569;&#20102;&#20869;&#23384;&#28040;&#32791;&#12290;&#20026;&#20102;&#20844;&#24179;&#27604;&#36739;&#30446;&#30340;&#65292;&#25105;&#20204;&#36824;&#22312;TensorFlow&#21644;PyTorch&#20013;&#23454;&#29616;&#20102;&#30456;&#21516;&#30340;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding why a neural network model makes certain decisions can be as important as the inference performance. Various methods have been proposed to help practitioners explain the prediction of a neural network model, of which Shapley values are most popular. SHAP package is a leading implementation of Shapley values to explain neural networks implemented in TensorFlow or PyTorch but lacks cross-platform support, one-shot deployment and is highly inefficient. To address these problems, we present the ONNXExplainer, which is a generic framework to explain neural networks using Shapley values in the ONNX ecosystem. In ONNXExplainer, we develop its own automatic differentiation and optimization approach, which not only enables One-Shot Deployment of neural networks inference and explanations, but also significantly improves the efficiency to compute explanation with less memory consumption. For fair comparison purposes, we also implement the same optimization in TensorFlow and PyTorch
&lt;/p&gt;</description></item><item><title>ASAP&#26159;&#19968;&#20010;&#22522;&#20110;&#29289;&#29702;&#30340;&#35745;&#21010;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#19968;&#33324;&#24418;&#29366;&#32452;&#35013;&#30340;&#29289;&#29702;&#21487;&#34892;&#24615;&#24207;&#21015;&#12290;&#23427;&#36890;&#36807;&#32771;&#34385;&#37325;&#21147;&#21644;&#20351;&#29992;&#39640;&#25928;&#30340;&#26641;&#25628;&#32034;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#29983;&#25104;&#29289;&#29702;&#23454;&#38469;&#30340;&#32452;&#35013;&#24207;&#21015;&#35268;&#21010;&#65292;&#36866;&#29992;&#20110;&#20223;&#30495;&#21644;&#30495;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#35774;&#32622;&#12290;</title><link>http://arxiv.org/abs/2309.16909</link><description>&lt;p&gt;
ASAP: &#33258;&#21160;&#21270;&#22797;&#26434;&#26426;&#22120;&#20154;&#32452;&#35013;&#30340;&#29289;&#29702;&#21487;&#34892;&#24615;&#24207;&#21015;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
ASAP: Automated Sequence Planning for Complex Robotic Assembly with Physical Feasibility. (arXiv:2309.16909v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16909
&lt;/p&gt;
&lt;p&gt;
ASAP&#26159;&#19968;&#20010;&#22522;&#20110;&#29289;&#29702;&#30340;&#35745;&#21010;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#19968;&#33324;&#24418;&#29366;&#32452;&#35013;&#30340;&#29289;&#29702;&#21487;&#34892;&#24615;&#24207;&#21015;&#12290;&#23427;&#36890;&#36807;&#32771;&#34385;&#37325;&#21147;&#21644;&#20351;&#29992;&#39640;&#25928;&#30340;&#26641;&#25628;&#32034;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#29983;&#25104;&#29289;&#29702;&#23454;&#38469;&#30340;&#32452;&#35013;&#24207;&#21015;&#35268;&#21010;&#65292;&#36866;&#29992;&#20110;&#20223;&#30495;&#21644;&#30495;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#20135;&#21697;&#30340;&#33258;&#21160;&#21270;&#32452;&#35013;&#38656;&#35201;&#19968;&#20010;&#31995;&#32479;&#33021;&#22815;&#33258;&#21160;&#35268;&#21010;&#19968;&#20010;&#29289;&#29702;&#21487;&#34892;&#30340;&#21160;&#20316;&#24207;&#21015;&#26469;&#32452;&#35013;&#22810;&#20010;&#37096;&#20214;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ASAP&#65292;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#30340;&#35745;&#21010;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#19968;&#33324;&#24418;&#29366;&#32452;&#35013;&#30340;&#24207;&#21015;&#12290;ASAP&#32771;&#34385;&#20102;&#37325;&#21147;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#24207;&#21015;&#65292;&#20854;&#20013;&#27599;&#20010;&#23376;&#32452;&#20214;&#22312;&#26377;&#38480;&#25968;&#37327;&#30340;&#38646;&#20214;&#34987;&#20445;&#25345;&#21644;&#25903;&#25745;&#34920;&#38754;&#30340;&#24773;&#20917;&#19979;&#20445;&#25345;&#29289;&#29702;&#31283;&#23450;&#12290;&#25105;&#20204;&#24212;&#29992;&#39640;&#25928;&#30340;&#26641;&#25628;&#32034;&#31639;&#27861;&#26469;&#20943;&#23569;&#30830;&#23450;&#36825;&#26679;&#19968;&#20010;&#32452;&#35013;&#24207;&#21015;&#30340;&#32452;&#21512;&#22797;&#26434;&#24615;&#12290;&#25628;&#32034;&#21487;&#20197;&#30001;&#20960;&#20309;&#21551;&#21457;&#24335;&#25110;&#35757;&#32451;&#26377;&#27169;&#25311;&#26631;&#31614;&#25968;&#25454;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#24341;&#23548;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;ASAP&#22312;&#25968;&#30334;&#20010;&#22797;&#26434;&#20135;&#21697;&#32452;&#35013;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#29983;&#25104;&#29289;&#29702;&#23454;&#38469;&#30340;&#32452;&#35013;&#24207;&#21015;&#35268;&#21010;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;ASAP&#22312;&#20223;&#30495;&#21644;&#30495;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#35774;&#32622;&#19978;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The automated assembly of complex products requires a system that can automatically plan a physically feasible sequence of actions for assembling many parts together. In this paper, we present ASAP, a physics-based planning approach for automatically generating such a sequence for general-shaped assemblies. ASAP accounts for gravity to design a sequence where each sub-assembly is physically stable with a limited number of parts being held and a support surface. We apply efficient tree search algorithms to reduce the combinatorial complexity of determining such an assembly sequence. The search can be guided by either geometric heuristics or graph neural networks trained on data with simulation labels. Finally, we show the superior performance of ASAP at generating physically realistic assembly sequence plans on a large dataset of hundreds of complex product assemblies. We further demonstrate the applicability of ASAP on both simulation and real-world robotic setups. Project website: asa
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;Transformer-based Multivariate Time Series Classifier (TMTSC)&#26469;&#39044;&#27979;&#39118;&#38505;&#25237;&#36164;&#21644;&#25104;&#38271;&#36164;&#26412;&#30340;&#20505;&#36873;&#20844;&#21496;&#30340;&#25104;&#21151;&#21487;&#33021;&#24615;&#65292;&#20197;&#20248;&#21270;&#25237;&#36164;&#30446;&#26631;&#30340;&#36873;&#25321;&#12290;&#36890;&#36807;&#23545;&#30456;&#20851;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#22238;&#39038;&#21644;&#23454;&#39564;&#39564;&#35777;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.16888</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#36716;&#25442;&#22120;&#33719;&#21462;&#39118;&#38505;&#25237;&#36164;&#21644;&#25104;&#38271;&#36164;&#26412;&#30340;&#25237;&#36164;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;
Sourcing Investment Targets for Venture and Growth Capital Using Multivariate Time Series Transformer. (arXiv:2309.16888v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16888
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;Transformer-based Multivariate Time Series Classifier (TMTSC)&#26469;&#39044;&#27979;&#39118;&#38505;&#25237;&#36164;&#21644;&#25104;&#38271;&#36164;&#26412;&#30340;&#20505;&#36873;&#20844;&#21496;&#30340;&#25104;&#21151;&#21487;&#33021;&#24615;&#65292;&#20197;&#20248;&#21270;&#25237;&#36164;&#30446;&#26631;&#30340;&#36873;&#25321;&#12290;&#36890;&#36807;&#23545;&#30456;&#20851;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#22238;&#39038;&#21644;&#23454;&#39564;&#39564;&#35777;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#22312;&#31169;&#21215;&#32929;&#26435;&#65288;PE&#65289;&#34892;&#19994;&#20013;&#30340;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#20026;&#39118;&#38505;&#25237;&#36164;&#65288;VC&#65289;&#21644;&#25104;&#38271;&#36164;&#26412;&#65288;GC&#65289;&#23547;&#25214;&#25237;&#36164;&#30446;&#26631;&#65288;&#21363;&#20844;&#21496;&#65289;&#26041;&#38754;&#12290;&#25105;&#20204;&#23545;&#30456;&#20851;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#22238;&#39038;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22522;&#20110;Transformer&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#22120;&#65288;TMTSC&#65289;&#26469;&#39044;&#27979;&#20219;&#20309;&#20505;&#36873;&#20844;&#21496;&#30340;&#25104;&#21151;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#30446;&#26631;&#26159;&#36890;&#36807;&#23558;&#23547;&#25214;&#25237;&#36164;&#38382;&#39064;&#27491;&#24335;&#23450;&#20041;&#20026;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20219;&#21153;&#65292;&#20248;&#21270;VC&#21644;GC&#25237;&#36164;&#30340;&#23547;&#25214;&#25928;&#26524;&#12290;&#25105;&#20204;&#20381;&#27425;&#20171;&#32461;&#20102;&#25105;&#20204;&#23454;&#29616;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#36825;&#20123;&#37096;&#20998;&#20849;&#21516; contribut &#21040;&#20102;&#22312;VC/GC&#23547;&#25214;&#20013;&#25104;&#21151;&#24212;&#29992;TMTSC&#65306;&#36755;&#20837;&#29305;&#24449;&#12289;&#27169;&#22411;&#26550;&#26500;&#12289;&#20248;&#21270;&#30446;&#26631;&#20197;&#21450;&#22522;&#20110;&#25237;&#36164;&#32773;&#30340;&#25968;&#25454;&#22686;&#24378;&#21644;&#21010;&#20998;&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#24182;&#23558;&#20854;&#19982;&#19977;&#20010;&#27969;&#34892;&#30340;&#22522;&#20934;&#32447;&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses the growing application of data-driven approaches within the Private Equity (PE) industry, particularly in sourcing investment targets (i.e., companies) for Venture Capital (VC) and Growth Capital (GC). We present a comprehensive review of the relevant approaches and propose a novel approach leveraging a Transformer-based Multivariate Time Series Classifier (TMTSC) for predicting the success likelihood of any candidate company. The objective of our research is to optimize sourcing performance for VC and GC investments by formally defining the sourcing problem as a multivariate time series classification task. We consecutively introduce the key components of our implementation which collectively contribute to the successful application of TMTSC in VC/GC sourcing: input features, model architecture, optimization target, and investor-centric data augmentation and split. Our extensive experiments on four datasets, benchmarked towards three popular baselines, demonstrat
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25581;&#31034;&#20102;&#23545;&#25239;&#25200;&#21160;&#20013;&#38544;&#34255;&#30340;&#20154;&#31867;&#21487;&#35782;&#21035;&#29305;&#24449;&#65292;&#24182;&#21457;&#29616;&#20102;&#25513;&#34109;&#25928;&#24212;&#21644;&#29983;&#25104;&#25928;&#24212;&#22312;&#19981;&#21516;&#31867;&#22411;&#25915;&#20987;&#20013;&#30340;&#34920;&#29616;&#12290;&#36890;&#36807;&#20998;&#26512;&#22810;&#20010;&#25915;&#20987;&#31639;&#27861;&#29983;&#25104;&#30340;&#25200;&#21160;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#23384;&#22312;&#30456;&#20284;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.16878</link><description>&lt;p&gt;
&#25581;&#31034;&#23545;&#25239;&#25200;&#21160;&#20013;&#38544;&#34255;&#30340;&#20154;&#31867;&#21487;&#35782;&#21035;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Investigating Human-Identifiable Features Hidden in Adversarial Perturbations. (arXiv:2309.16878v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16878
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25581;&#31034;&#20102;&#23545;&#25239;&#25200;&#21160;&#20013;&#38544;&#34255;&#30340;&#20154;&#31867;&#21487;&#35782;&#21035;&#29305;&#24449;&#65292;&#24182;&#21457;&#29616;&#20102;&#25513;&#34109;&#25928;&#24212;&#21644;&#29983;&#25104;&#25928;&#24212;&#22312;&#19981;&#21516;&#31867;&#22411;&#25915;&#20987;&#20013;&#30340;&#34920;&#29616;&#12290;&#36890;&#36807;&#20998;&#26512;&#22810;&#20010;&#25915;&#20987;&#31639;&#27861;&#29983;&#25104;&#30340;&#25200;&#21160;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#23384;&#22312;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23545;&#25239;&#25200;&#21160;&#24182;&#19981;&#20813;&#30123;&#12290;&#36825;&#31181;&#33030;&#24369;&#24615;&#23545;&#23454;&#38469;&#24212;&#29992;&#26377;&#30528;&#37325;&#35201;&#24433;&#21709;&#12290;&#34429;&#28982;&#24050;&#32463;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#30740;&#31350;&#65292;&#20294;&#31070;&#32463;&#32593;&#32476;&#20026;&#20309;&#26131;&#21463;&#23545;&#25239;&#25915;&#20987;&#30340;&#26681;&#26412;&#21407;&#22240;&#23578;&#26410;&#23436;&#20840;&#29702;&#35299;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#30340;&#26680;&#24515;&#26159;&#25506;&#32034;&#23545;&#25239;&#25200;&#21160;&#20013;&#30340;&#20154;&#31867;&#21487;&#35782;&#21035;&#29305;&#24449;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#20116;&#31181;&#25915;&#20987;&#31639;&#27861;&#21644;&#19977;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#22312;&#20154;&#31867;&#21487;&#35782;&#21035;&#29305;&#24449;&#20013;&#34920;&#29616;&#20986;&#30340;&#20004;&#31181;&#19981;&#21516;&#25928;&#24212;&#12290;&#22312;&#38750;&#23450;&#21521;&#25915;&#20987;&#20013;&#65292;&#25513;&#34109;&#25928;&#24212;&#26356;&#20026;&#26174;&#33879;&#65292;&#32780;&#22312;&#23450;&#21521;&#25915;&#20987;&#20013;&#65292;&#29983;&#25104;&#25928;&#24212;&#26356;&#20026;&#24120;&#35265;&#12290;&#36890;&#36807;&#20687;&#32032;&#32423;&#27880;&#37322;&#65292;&#25105;&#20204;&#25552;&#21462;&#36825;&#20123;&#29305;&#24449;&#24182;&#35777;&#26126;&#23427;&#20204;&#33021;&#22815;&#30772;&#22351;&#30446;&#26631;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22810;&#20010;&#25915;&#20987;&#31639;&#27861;&#29983;&#25104;&#30340;&#25200;&#21160;&#22312;&#24179;&#22343;&#24773;&#20917;&#19979;&#23384;&#22312;&#26174;&#33879;&#30340;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks perform exceedingly well across various machine learning tasks but are not immune to adversarial perturbations. This vulnerability has implications for real-world applications. While much research has been conducted, the underlying reasons why neural networks fall prey to adversarial attacks are not yet fully understood. Central to our study, which explores up to five attack algorithms across three datasets, is the identification of human-identifiable features in adversarial perturbations. Additionally, we uncover two distinct effects manifesting within human-identifiable features. Specifically, the masking effect is prominent in untargeted attacks, while the generation effect is more common in targeted attacks. Using pixel-level annotations, we extract such features and demonstrate their ability to compromise target models. In addition, our findings indicate a notable extent of similarity in perturbations across different attack algorithms when averaged over multiple m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#20307;&#31215;&#21270;&#20154;&#33080;&#20808;&#39564;&#65292;&#20351;&#24471;&#33021;&#21512;&#25104;&#20808;&#21069;&#35757;&#32451;&#20998;&#24067;&#22806;&#30340;&#36229;&#39640;&#20998;&#36776;&#29575;&#30340;&#26032;&#35270;&#22270;&#12290;&#36890;&#36807;&#35757;&#32451;&#36523;&#20221;&#25968;&#37327;&#26377;&#38480;&#65292;&#32467;&#21512;&#22522;&#20110;&#31232;&#30095;&#26631;&#35760;&#28857;&#30340;3D&#23545;&#40784;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#21040;&#19968;&#20010;&#24179;&#28369;&#30340;&#20960;&#20309;&#21644;&#22806;&#35266;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#24182;&#36890;&#36807;&#25311;&#21512;&#21040;2-3&#20010;&#25668;&#20687;&#26426;&#35270;&#22270;&#26469;&#33719;&#21462;&#39640;&#36136;&#37327;&#30340;&#26032;&#23545;&#35937;&#30340;&#20307;&#31215;&#21270;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2309.16859</link><description>&lt;p&gt;
&#21069;&#35328;&#65306;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#20307;&#31215;&#21270;&#20808;&#39564;&#29992;&#20110;&#23569;&#26679;&#26412;&#36229;&#39640;&#20998;&#36776;&#29575;&#20154;&#33080;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Preface: A Data-driven Volumetric Prior for Few-shot Ultra High-resolution Face Synthesis. (arXiv:2309.16859v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16859
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#20307;&#31215;&#21270;&#20154;&#33080;&#20808;&#39564;&#65292;&#20351;&#24471;&#33021;&#21512;&#25104;&#20808;&#21069;&#35757;&#32451;&#20998;&#24067;&#22806;&#30340;&#36229;&#39640;&#20998;&#36776;&#29575;&#30340;&#26032;&#35270;&#22270;&#12290;&#36890;&#36807;&#35757;&#32451;&#36523;&#20221;&#25968;&#37327;&#26377;&#38480;&#65292;&#32467;&#21512;&#22522;&#20110;&#31232;&#30095;&#26631;&#35760;&#28857;&#30340;3D&#23545;&#40784;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#21040;&#19968;&#20010;&#24179;&#28369;&#30340;&#20960;&#20309;&#21644;&#22806;&#35266;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#24182;&#36890;&#36807;&#25311;&#21512;&#21040;2-3&#20010;&#25668;&#20687;&#26426;&#35270;&#22270;&#26469;&#33719;&#21462;&#39640;&#36136;&#37327;&#30340;&#26032;&#23545;&#35937;&#30340;&#20307;&#31215;&#21270;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
NeRFs&#24050;&#32463;&#23454;&#29616;&#20102;&#21253;&#25324;&#22797;&#26434;&#30340;&#22836;&#21457;&#21644;&#30382;&#32932;&#22312;&#20869;&#30340;&#20154;&#33080;&#39640;&#24230;&#30495;&#23454;&#30340;&#21512;&#25104;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#22810;&#35270;&#22270;&#36755;&#20837;&#22270;&#20687;&#65292;&#20351;&#35813;&#36807;&#31243;&#23545;&#30828;&#20214;&#35201;&#27714;&#36739;&#39640;&#19988;&#32321;&#29712;&#65292;&#38480;&#21046;&#20102;&#23427;&#22312;&#26080;&#32422;&#26463;&#29615;&#22659;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20307;&#31215;&#21270;&#20154;&#33080;&#20808;&#39564;&#65292;&#20351;&#24471;&#33021;&#21512;&#25104;&#20808;&#21069;&#35757;&#32451;&#20998;&#24067;&#22806;&#30340;&#36229;&#39640;&#20998;&#36776;&#29575;&#30340;&#26032;&#35270;&#22270;&#12290;&#36825;&#20010;&#20808;&#39564;&#27169;&#22411;&#30001;&#19968;&#20010;&#19982;&#36523;&#20221;&#30456;&#20851;&#30340;NeRF&#32452;&#25104;&#65292;&#32463;&#36807;&#22312;&#24050;&#30693;&#25668;&#20687;&#26426;&#26631;&#23450;&#30340;&#22810;&#26679;&#20154;&#31867;&#20302;&#20998;&#36776;&#29575;&#22810;&#35270;&#22270;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#12290;&#23613;&#31649;&#35757;&#32451;&#36523;&#20221;&#25968;&#37327;&#26377;&#38480;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#22522;&#20110;&#31232;&#30095;&#26631;&#35760;&#28857;&#30340;3D&#23545;&#40784;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#21040;&#19968;&#20010;&#24179;&#28369;&#30340;&#20960;&#20309;&#21644;&#22806;&#35266;&#30340;&#28508;&#22312;&#31354;&#38388;&#12290;&#36890;&#36807;&#23558;&#27169;&#22411;&#25311;&#21512;&#21040;&#20219;&#24847;&#20998;&#36776;&#29575;&#30340;2&#25110;3&#20010;&#25668;&#20687;&#26426;&#35270;&#22270;&#65292;&#21487;&#20197;&#33719;&#24471;&#39640;&#36136;&#37327;&#30340;&#26032;&#23545;&#35937;&#30340;&#20307;&#31215;&#21270;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
NeRFs have enabled highly realistic synthesis of human faces including complex appearance and reflectance effects of hair and skin. These methods typically require a large number of multi-view input images, making the process hardware intensive and cumbersome, limiting applicability to unconstrained settings. We propose a novel volumetric human face prior that enables the synthesis of ultra high-resolution novel views of subjects that are not part of the prior's training distribution. This prior model consists of an identity-conditioned NeRF, trained on a dataset of low-resolution multi-view images of diverse humans with known camera calibration. A simple sparse landmark-based 3D alignment of the training dataset allows our model to learn a smooth latent space of geometry and appearance despite a limited number of training identities. A high-quality volumetric representation of a novel subject can be obtained by model fitting to 2 or 3 camera views of arbitrary resolution. Importantly,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#22810;Bellman&#31639;&#23376;&#65292;&#23545;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#19979;&#30340;$Q$-learning&#36827;&#34892;&#20102;&#25910;&#25947;&#24615;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;&#22810;$Q$-learning&#31639;&#27861;&#12290;&#36890;&#36807;&#25506;&#32034;&#22810;Bellman&#31639;&#23376;&#30340;&#24615;&#36136;&#65292;&#25105;&#20204;&#25214;&#21040;&#20102;&#20351;&#20854;&#25104;&#20026;&#21387;&#32553;&#26144;&#23556;&#30340;&#26465;&#20214;&#65292;&#33719;&#24471;&#20102;&#27604;&#20256;&#32479;Bellman&#31639;&#23376;&#26356;&#22909;&#30340;&#22266;&#23450;&#28857;&#20445;&#35777;&#12290;&#35813;&#31639;&#27861;&#25910;&#25947;&#21040;&#22810;Bellman&#31639;&#23376;&#30340;&#19981;&#21160;&#28857;&#65292;&#21487;&#20197;&#24471;&#21040;&#20219;&#24847;&#31934;&#24230;&#30340;&#35299;&#12290;&#22312;&#39564;&#35777;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#24120;&#29992;&#29615;&#22659;&#20013;&#65292;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.16819</link><description>&lt;p&gt;
&#29992;&#20110;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#30340;&#22810;Bellman&#31639;&#23376;&#23545;$Q$-learning&#30340;&#25910;&#25947;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Multi-Bellman operator for convergence of $Q$-learning with linear function approximation. (arXiv:2309.16819v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16819
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#22810;Bellman&#31639;&#23376;&#65292;&#23545;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#19979;&#30340;$Q$-learning&#36827;&#34892;&#20102;&#25910;&#25947;&#24615;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;&#22810;$Q$-learning&#31639;&#27861;&#12290;&#36890;&#36807;&#25506;&#32034;&#22810;Bellman&#31639;&#23376;&#30340;&#24615;&#36136;&#65292;&#25105;&#20204;&#25214;&#21040;&#20102;&#20351;&#20854;&#25104;&#20026;&#21387;&#32553;&#26144;&#23556;&#30340;&#26465;&#20214;&#65292;&#33719;&#24471;&#20102;&#27604;&#20256;&#32479;Bellman&#31639;&#23376;&#26356;&#22909;&#30340;&#22266;&#23450;&#28857;&#20445;&#35777;&#12290;&#35813;&#31639;&#27861;&#25910;&#25947;&#21040;&#22810;Bellman&#31639;&#23376;&#30340;&#19981;&#21160;&#28857;&#65292;&#21487;&#20197;&#24471;&#21040;&#20219;&#24847;&#31934;&#24230;&#30340;&#35299;&#12290;&#22312;&#39564;&#35777;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#24120;&#29992;&#29615;&#22659;&#20013;&#65292;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#19979;$Q$-learning&#30340;&#25910;&#25947;&#24615;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;Bellman&#31639;&#23376;&#65292;&#35813;&#31639;&#23376;&#25193;&#23637;&#20102;&#20256;&#32479;&#30340;Bellman&#31639;&#23376;&#12290;&#36890;&#36807;&#25506;&#32034;&#35813;&#31639;&#23376;&#30340;&#24615;&#36136;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#22312;&#25237;&#24433;&#30340;&#22810;Bellman&#31639;&#23376;&#21464;&#20026;&#21387;&#32553;&#26144;&#23556;&#26102;&#30340;&#26465;&#20214;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#27604;Bellman&#31639;&#23376;&#26356;&#22909;&#30340;&#22266;&#23450;&#28857;&#20445;&#35777;&#12290;&#20026;&#20102;&#21033;&#29992;&#36825;&#20123;&#27934;&#23519;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20855;&#26377;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#30340;&#22810;$Q$-learning&#31639;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#25910;&#25947;&#21040;&#25237;&#24433;&#30340;&#22810;Bellman&#31639;&#23376;&#30340;&#19981;&#21160;&#28857;&#65292;&#20174;&#32780;&#33719;&#24471;&#20102;&#20219;&#24847;&#31934;&#24230;&#30340;&#35299;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#20247;&#25152;&#21608;&#30693;&#30340;&#29615;&#22659;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#21457;&#29616;&#30340;&#26377;&#25928;&#24615;&#21644;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the convergence of $Q$-learning with linear function approximation. Our key contribution is the introduction of a novel multi-Bellman operator that extends the traditional Bellman operator. By exploring the properties of this operator, we identify conditions under which the projected multi-Bellman operator becomes contractive, providing improved fixed-point guarantees compared to the Bellman operator. To leverage these insights, we propose the multi $Q$-learning algorithm with linear function approximation. We demonstrate that this algorithm converges to the fixed-point of the projected multi-Bellman operator, yielding solutions of arbitrary accuracy. Finally, we validate our approach by applying it to well-known environments, showcasing the effectiveness and applicability of our findings.
&lt;/p&gt;</description></item><item><title>Promptbreeder&#26159;&#19968;&#31181;&#36890;&#36807;&#25552;&#31034;&#28436;&#21270;&#36827;&#34892;&#33258;&#25105;&#25913;&#36827;&#30340;&#36890;&#29992;&#26426;&#21046;&#65292;&#23427;&#21487;&#20197;&#20248;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#22312;&#31639;&#26415;&#21644;&#24120;&#35782;&#25512;&#29702;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#25552;&#31034;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2309.16797</link><description>&lt;p&gt;
Promptbreeder: &#36890;&#36807;&#25552;&#31034;&#28436;&#21270;&#23454;&#29616;&#33258;&#25105;&#21442;&#29031;&#30340;&#33258;&#25105;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Promptbreeder: Self-Referential Self-Improvement Via Prompt Evolution. (arXiv:2309.16797v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16797
&lt;/p&gt;
&lt;p&gt;
Promptbreeder&#26159;&#19968;&#31181;&#36890;&#36807;&#25552;&#31034;&#28436;&#21270;&#36827;&#34892;&#33258;&#25105;&#25913;&#36827;&#30340;&#36890;&#29992;&#26426;&#21046;&#65292;&#23427;&#21487;&#20197;&#20248;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#22312;&#31639;&#26415;&#21644;&#24120;&#35782;&#25512;&#29702;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#25552;&#31034;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20687;Chain-of-Thought Prompting&#36825;&#26679;&#30340;&#27969;&#34892;&#25552;&#31034;&#31574;&#30053;&#21487;&#20197;&#26174;&#33879;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#25163;&#24037;&#21046;&#20316;&#30340;&#25552;&#31034;&#31574;&#30053;&#36890;&#24120;&#19981;&#22815;&#20248;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Promptbreeder&#65292;&#19968;&#31181;&#36890;&#29992;&#30340;&#33258;&#25105;&#21442;&#29031;&#33258;&#25105;&#25913;&#36827;&#26426;&#21046;&#65292;&#29992;&#20110;&#36827;&#21270;&#21644;&#35843;&#25972;&#32473;&#23450;&#39046;&#22495;&#30340;&#25552;&#31034;&#12290;Promptbreeder&#36890;&#36807;LLM&#39537;&#21160;&#65292;&#23545;&#19968;&#32452;&#20219;&#21153;&#25552;&#31034;&#36827;&#34892;&#31361;&#21464;&#65292;&#24182;&#22312;&#35757;&#32451;&#38598;&#19978;&#35780;&#20272;&#20854;&#36866;&#24212;&#24615;&#12290;&#20851;&#38190;&#26159;&#65292;&#36825;&#20123;&#20219;&#21153;&#25552;&#31034;&#30340;&#31361;&#21464;&#26159;&#30001;LLM&#29983;&#25104;&#24182;&#22312;&#28436;&#21270;&#36807;&#31243;&#20013;&#33258;&#25105;&#25913;&#36827;&#30340;&#31361;&#21464;&#25552;&#31034;&#26469;&#25511;&#21046;&#30340;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;Promptbreeder&#19981;&#20165;&#25913;&#36827;&#20219;&#21153;&#25552;&#31034;&#65292;&#36824;&#25913;&#36827;&#20102;&#25913;&#36827;&#36825;&#20123;&#20219;&#21153;&#25552;&#31034;&#30340;&#31361;&#21464;&#25552;&#31034;&#12290;Promptbreeder&#22312;&#24120;&#29992;&#30340;&#31639;&#26415;&#21644;&#24120;&#35782;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;&#20013;&#20248;&#20110;Chain-of-Thought&#21644;Plan-and-Solve Prompting&#31561;&#26368;&#20808;&#36827;&#30340;&#25552;&#31034;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Popular prompt strategies like Chain-of-Thought Prompting can dramatically improve the reasoning abilities of Large Language Models (LLMs) in various domains. However, such hand-crafted prompt-strategies are often sub-optimal. In this paper, we present Promptbreeder, a general-purpose self-referential self-improvement mechanism that evolves and adapts prompts for a given domain. Driven by an LLM, Promptbreeder mutates a population of task-prompts, and subsequently evaluates them for fitness on a training set. Crucially, the mutation of these task-prompts is governed by mutation-prompts that the LLM generates and improves throughout evolution in a self-referential way. That is, Promptbreeder is not just improving task-prompts, but it is also improving the mutationprompts that improve these task-prompts. Promptbreeder outperforms state-of-the-art prompt strategies such as Chain-of-Thought and Plan-and-Solve Prompting on commonly used arithmetic and commonsense reasoning benchmarks. Furth
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20809;&#23376;&#21152;&#36895;&#22120;&#22312;&#33258;&#21160;&#39550;&#39542;&#21644;&#32570;&#38519;&#26816;&#27979;&#20013;&#30340;&#22270;&#20687;&#20998;&#21106;&#24212;&#29992;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#20351;&#29992;&#20809;&#23376;&#21152;&#36895;&#22120;&#25191;&#34892;&#29305;&#23450;&#30340;&#20998;&#21106;&#27169;&#22411;&#21487;&#20197;&#22312;&#20934;&#30830;&#24615;&#19978;&#24573;&#30053;&#25439;&#22833;&#65292;&#24182;&#35752;&#35770;&#20102;&#22312;&#27169;&#22411;&#34987;&#24178;&#25200;&#26102;&#20462;&#22797;&#20934;&#30830;&#24615;&#30340;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2309.16783</link><description>&lt;p&gt;
&#20809;&#23376;&#21152;&#36895;&#22120;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#22270;&#20687;&#20998;&#21106;&#21644;&#32570;&#38519;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Photonic Accelerators for Image Segmentation in Autonomous Driving and Defect Detection. (arXiv:2309.16783v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16783
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20809;&#23376;&#21152;&#36895;&#22120;&#22312;&#33258;&#21160;&#39550;&#39542;&#21644;&#32570;&#38519;&#26816;&#27979;&#20013;&#30340;&#22270;&#20687;&#20998;&#21106;&#24212;&#29992;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#20351;&#29992;&#20809;&#23376;&#21152;&#36895;&#22120;&#25191;&#34892;&#29305;&#23450;&#30340;&#20998;&#21106;&#27169;&#22411;&#21487;&#20197;&#22312;&#20934;&#30830;&#24615;&#19978;&#24573;&#30053;&#25439;&#22833;&#65292;&#24182;&#35752;&#35770;&#20102;&#22312;&#27169;&#22411;&#34987;&#24178;&#25200;&#26102;&#20462;&#22797;&#20934;&#30830;&#24615;&#30340;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20809;&#23376;&#35745;&#31639;&#25215;&#35834;&#27604;&#20256;&#32479;&#30340;&#25968;&#23383;&#30828;&#20214;&#26356;&#24555;&#36895;&#21644;&#33021;&#37327;&#26356;&#39640;&#25928;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#25512;&#26029;&#12290;&#20809;&#23376;&#35745;&#31639;&#30340;&#36827;&#27493;&#21487;&#20197;&#23545;&#20381;&#36182;&#20110;&#24555;&#36895;&#12289;&#20934;&#30830;&#21644;&#33021;&#37327;&#39640;&#25928;&#25191;&#34892;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#30340;&#24212;&#29992;&#65292;&#22914;&#33258;&#21160;&#39550;&#39542;&#21644;&#32570;&#38519;&#26816;&#27979;&#65292;&#20135;&#29983;&#28145;&#36828;&#24433;&#21709;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20809;&#23376;&#21152;&#36895;&#22120;&#19978;&#36827;&#34892;&#22270;&#20687;&#20998;&#21106;&#65292;&#25506;&#32034;&#20102;&#36866;&#21512;&#20809;&#23376;&#21152;&#36895;&#22120;&#30340;&#22270;&#20687;&#20998;&#21106;DNN&#26550;&#26500;&#31867;&#22411;&#20197;&#21450;&#22312;&#20809;&#23376;&#21152;&#36895;&#22120;&#19978;&#25191;&#34892;&#19981;&#21516;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#30340;&#21534;&#21520;&#37327;&#21644;&#33021;&#37327;&#25928;&#29575;&#65292;&#20197;&#21450;&#20854;&#20013;&#30340;&#26435;&#34913;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403;&#22312;&#20809;&#23376;&#21152;&#36895;&#22120;&#19978;&#25191;&#34892;&#26102;&#65292;&#26576;&#20123;&#20998;&#21106;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#25439;&#22833;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#65288;&#19982;&#25968;&#23383;float32&#27169;&#22411;&#30456;&#27604;&#65289;&#65292;&#24182;&#25506;&#35752;&#20102;&#23427;&#20204;&#30340;&#31283;&#20581;&#24615;&#30340;&#32463;&#39564;&#25512;&#29702;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#22312;&#27169;&#22411;&#30340;&#20462;&#22797;&#20934;&#30830;&#24615;&#30340;&#25216;&#26415;&#65288;&#22312;&#27169;&#22411;&#34987;&#24178;&#25200;&#26102;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Photonic computing promises faster and more energy-efficient deep neural network (DNN) inference than traditional digital hardware. Advances in photonic computing can have profound impacts on applications such as autonomous driving and defect detection that depend on fast, accurate and energy efficient execution of image segmentation models. In this paper, we investigate image segmentation on photonic accelerators to explore: a) the types of image segmentation DNN architectures that are best suited for photonic accelerators, and b) the throughput and energy efficiency of executing the different image segmentation models on photonic accelerators, along with the trade-offs involved therein. Specifically, we demonstrate that certain segmentation models exhibit negligible loss in accuracy (compared to digital float32 models) when executed on photonic accelerators, and explore the empirical reasoning for their robustness. We also discuss techniques for recovering accuracy in the case of mod
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#20998;&#31867;&#22120;&#23637;&#31034;&#20102;&#35760;&#24405;&#30772;&#32426;&#24405;&#30340;&#20154;&#31867;&#24418;&#29366;&#20559;&#22909;&#12289;&#25509;&#36817;&#20154;&#31867;&#32423;&#21035;&#30340;&#36229;&#20986;&#20998;&#24067;&#20934;&#30830;&#24615;&#12289;&#19982;&#20154;&#31867;&#20998;&#31867;&#38169;&#35823;&#30340;&#26368;&#20808;&#36827;&#23545;&#40784;&#20197;&#21450;&#29702;&#35299;&#26576;&#20123;&#30693;&#35273;&#24187;&#35937;&#30340;&#26032;&#20852;&#29305;&#24615;&#65292;&#25581;&#31034;&#20102;&#38646;&#26679;&#26412;&#29983;&#25104;&#27169;&#22411;&#20986;&#22855;&#22320;&#25509;&#36817;&#20154;&#31867;&#29289;&#20307;&#35782;&#21035;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2309.16779</link><description>&lt;p&gt;
&#29983;&#25104;&#20998;&#31867;&#22120;&#30340;&#26377;&#36259;&#23646;&#24615;
&lt;/p&gt;
&lt;p&gt;
Intriguing properties of generative classifiers. (arXiv:2309.16779v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16779
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#20998;&#31867;&#22120;&#23637;&#31034;&#20102;&#35760;&#24405;&#30772;&#32426;&#24405;&#30340;&#20154;&#31867;&#24418;&#29366;&#20559;&#22909;&#12289;&#25509;&#36817;&#20154;&#31867;&#32423;&#21035;&#30340;&#36229;&#20986;&#20998;&#24067;&#20934;&#30830;&#24615;&#12289;&#19982;&#20154;&#31867;&#20998;&#31867;&#38169;&#35823;&#30340;&#26368;&#20808;&#36827;&#23545;&#40784;&#20197;&#21450;&#29702;&#35299;&#26576;&#20123;&#30693;&#35273;&#24187;&#35937;&#30340;&#26032;&#20852;&#29305;&#24615;&#65292;&#25581;&#31034;&#20102;&#38646;&#26679;&#26412;&#29983;&#25104;&#27169;&#22411;&#20986;&#22855;&#22320;&#25509;&#36817;&#20154;&#31867;&#29289;&#20307;&#35782;&#21035;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35782;&#21035;&#23545;&#35937;&#30340;&#26368;&#20339;&#33539;&#24335;&#26159;&#21028;&#21035;&#24335;&#25512;&#29702;&#65288;&#24555;&#36895;&#20294;&#28508;&#22312;&#23481;&#26131;&#20986;&#29616;&#24555;&#25463;&#23398;&#20064;&#65289;&#36824;&#26159;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#65288;&#36739;&#24930;&#20294;&#28508;&#22312;&#26356;&#31283;&#20581;&#65289;&#65311;&#25105;&#20204;&#20511;&#37492;&#20102;&#26368;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#36827;&#23637;&#65292;&#23558;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#36716;&#21270;&#20026;&#20998;&#31867;&#22120;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#30740;&#31350;&#20854;&#34892;&#20026;&#65292;&#24182;&#23558;&#20854;&#19982;&#21028;&#21035;&#27169;&#22411;&#21644;&#20154;&#31867;&#24515;&#29702;&#29289;&#29702;&#25968;&#25454;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#25253;&#36947;&#20102;&#29983;&#25104;&#20998;&#31867;&#22120;&#30340;&#22235;&#20010;&#26377;&#36259;&#30340;&#26032;&#20852;&#29305;&#24615;&#65306;&#23427;&#20204;&#26174;&#31034;&#20986;&#30772;&#32426;&#24405;&#30340;&#20154;&#31867;&#24418;&#29366;&#20559;&#22909;&#65288;&#23545;&#20110;Imagen&#36798;&#21040;99%&#65289;&#65292;&#25509;&#36817;&#20154;&#31867;&#32423;&#21035;&#30340;&#36229;&#20986;&#20998;&#24067;&#20934;&#30830;&#24615;&#65292;&#19982;&#20154;&#31867;&#20998;&#31867;&#38169;&#35823;&#30340;&#26368;&#20808;&#36827;&#23545;&#40784;&#20197;&#21450;&#23427;&#20204;&#29702;&#35299;&#26576;&#20123;&#30693;&#35273;&#24187;&#35937;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#30446;&#21069;&#27169;&#25311;&#20154;&#31867;&#29289;&#20307;&#35782;&#21035;&#30340;&#20027;&#23548;&#33539;&#24335;&#26159;&#21028;&#21035;&#24335;&#25512;&#29702;&#65292;&#38646;&#26679;&#26412;&#29983;&#25104;&#27169;&#22411;&#20986;&#22855;&#22320;&#25509;&#36817;&#20154;&#31867;&#29289;&#20307;&#35782;&#21035;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
What is the best paradigm to recognize objects -- discriminative inference (fast but potentially prone to shortcut learning) or using a generative model (slow but potentially more robust)? We build on recent advances in generative modeling that turn text-to-image models into classifiers. This allows us to study their behavior and to compare them against discriminative models and human psychophysical data. We report four intriguing emergent properties of generative classifiers: they show a record-breaking human-like shape bias (99% for Imagen), near human-level out-of-distribution accuracy, state-of-the-art alignment with human classification errors, and they understand certain perceptual illusions. Our results indicate that while the current dominant paradigm for modeling human object recognition is discriminative inference, zero-shot generative models approximate human object recognition data surprisingly well.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#24320;&#21457;ChatWords&#65292;&#19968;&#20010;&#33258;&#21160;&#21270;&#27979;&#35797;&#31995;&#32479;&#65292;&#20026;&#35780;&#20272;ChatGPT&#21644;&#20854;&#20182;NLP AI&#24037;&#20855;&#30340;&#35789;&#27719;&#30693;&#35782;&#20570;&#20986;&#20102;&#36129;&#29486;&#65292;&#20197;&#35299;&#20915;&#23427;&#20204;&#22312;&#20135;&#29983;&#38169;&#35823;&#31572;&#26696;&#26041;&#38754;&#30340;&#38480;&#21046;&#21644;&#38169;&#35273;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.16777</link><description>&lt;p&gt;
ChatGPT&#30693;&#36947;&#22810;&#23569;&#20010;&#21333;&#35789;&#65311;&#31572;&#26696;&#26159;ChatWords&#12290;
&lt;/p&gt;
&lt;p&gt;
How many words does ChatGPT know? The answer is ChatWords. (arXiv:2309.16777v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16777
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#24320;&#21457;ChatWords&#65292;&#19968;&#20010;&#33258;&#21160;&#21270;&#27979;&#35797;&#31995;&#32479;&#65292;&#20026;&#35780;&#20272;ChatGPT&#21644;&#20854;&#20182;NLP AI&#24037;&#20855;&#30340;&#35789;&#27719;&#30693;&#35782;&#20570;&#20986;&#20102;&#36129;&#29486;&#65292;&#20197;&#35299;&#20915;&#23427;&#20204;&#22312;&#20135;&#29983;&#38169;&#35823;&#31572;&#26696;&#26041;&#38754;&#30340;&#38480;&#21046;&#21644;&#38169;&#35273;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#30340;&#24341;&#20837;&#24341;&#36215;&#20102;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;&#20851;&#27880;&#12290;ChatGPT&#30340;&#37319;&#29992;&#29575;&#21576;&#25351;&#25968;&#22686;&#38271;&#65292;&#25968;&#30334;&#19975;&#29992;&#25143;&#22312;&#21508;&#31181;&#20219;&#21153;&#21644;&#24212;&#29992;&#39046;&#22495;&#20013;&#36827;&#34892;&#23454;&#39564;&#65292;&#24182;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;ChatGPT&#23384;&#22312;&#30528;&#38480;&#21046;&#21644;&#38169;&#35273;&#65292;&#20363;&#22914;&#20135;&#29983;&#30475;&#20284;&#21487;&#20449;&#20294;&#23436;&#20840;&#38169;&#35823;&#30340;&#31572;&#26696;&#12290;&#35780;&#20272;ChatGPT&#21644;&#31867;&#20284;&#30340;AI&#24037;&#20855;&#30340;&#24615;&#33021;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#38382;&#39064;&#65292;&#27491;&#20174;&#19981;&#21516;&#30340;&#35270;&#35282;&#36827;&#34892;&#25506;&#32034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;ChatWords&#65292;&#19968;&#20010;&#33258;&#21160;&#21270;&#27979;&#35797;&#31995;&#32479;&#65292;&#20026;&#35780;&#20272;ChatGPT&#23545;&#20219;&#24847;&#21333;&#35789;&#38598;&#30340;&#30693;&#35782;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;ChatWords&#35774;&#35745;&#20026;&#21487;&#25193;&#23637;&#12289;&#26131;&#20110;&#20351;&#29992;&#21644;&#36866;&#24212;&#24615;&#24378;&#65292;&#20063;&#21487;&#20197;&#35780;&#20272;&#20854;&#20182;NLP AI&#24037;&#20855;&#12290;ChatWords&#24050;&#20844;&#24320;&#21487;&#29992;&#65292;&#20854;&#20027;&#35201;&#30446;&#26631;&#26159;&#20419;&#36827;&#23545;AI&#24037;&#20855;&#30340;&#35789;&#27719;&#30693;&#35782;&#30340;&#30740;&#31350;&#12290;&#36890;&#36807;&#20004;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#35777;&#26126;&#20102;ChatWords&#30340;&#22909;&#22788;&#65306;&#35780;&#20272;&#23545;AI&#24037;&#20855;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
The introduction of ChatGPT has put Artificial Intelligence (AI) Natural Language Processing (NLP) in the spotlight. ChatGPT adoption has been exponential with millions of users experimenting with it in a myriad of tasks and application domains with impressive results. However, ChatGPT has limitations and suffers hallucinations, for example producing answers that look plausible but they are completely wrong. Evaluating the performance of ChatGPT and similar AI tools is a complex issue that is being explored from different perspectives. In this work, we contribute to those efforts with ChatWords, an automated test system, to evaluate ChatGPT knowledge of an arbitrary set of words. ChatWords is designed to be extensible, easy to use, and adaptable to evaluate also other NLP AI tools. ChatWords is publicly available and its main goal is to facilitate research on the lexical knowledge of AI tools. The benefits of ChatWords are illustrated with two case studies: evaluating the knowledge tha
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#35268;&#27169;&#23545;&#20110;&#36741;&#21161;&#23567;&#20998;&#23376;&#33647;&#29289;&#21457;&#29616;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;DNN&#22312;&#34920;&#22411;&#33647;&#29289;&#21457;&#29616;&#20219;&#21153;&#20013;&#24182;&#27809;&#26377;&#25345;&#32493;&#25913;&#36827;&#65292;&#36890;&#36807;&#24341;&#20837;&#36870;&#29983;&#29289;&#36807;&#31243;&#39044;&#35757;&#32451;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.16773</link><description>&lt;p&gt;
&#31070;&#32463;&#35268;&#27169;&#23450;&#24459;&#22312;&#34920;&#22411;&#33647;&#29289;&#21457;&#29616;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Neural scaling laws for phenotypic drug discovery. (arXiv:2309.16773v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16773
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#35268;&#27169;&#23545;&#20110;&#36741;&#21161;&#23567;&#20998;&#23376;&#33647;&#29289;&#21457;&#29616;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;DNN&#22312;&#34920;&#22411;&#33647;&#29289;&#21457;&#29616;&#20219;&#21153;&#20013;&#24182;&#27809;&#26377;&#25345;&#32493;&#25913;&#36827;&#65292;&#36890;&#36807;&#24341;&#20837;&#36870;&#29983;&#29289;&#36807;&#31243;&#39044;&#35757;&#32451;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#30340;&#31361;&#30772;&#26159;&#36890;&#36807;&#27169;&#22411;&#21644;&#25968;&#25454;&#35268;&#27169;&#30340;&#25552;&#21319;&#32780;&#38750;&#26032;&#30340;&#35745;&#31639;&#33539;&#20363;&#30340;&#21457;&#29616;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#35268;&#27169;&#26159;&#21542;&#20063;&#33021;&#23545;&#29992;&#20110;&#36741;&#21161;&#23567;&#20998;&#23376;&#33647;&#29289;&#21457;&#29616;&#30340;&#27169;&#22411;&#20135;&#29983;&#31867;&#20284;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36890;&#36807;&#22823;&#35268;&#27169;&#21644;&#31995;&#32479;&#21270;&#30340;&#20998;&#26512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35268;&#27169;&#12289;&#25968;&#25454;&#35757;&#32451;&#38598;&#21644;&#23398;&#20064;&#26041;&#27861;&#22914;&#20309;&#30456;&#20114;&#24433;&#21709;&#65292;&#24182;&#23545;&#25105;&#20204;&#30340;&#34920;&#22411;&#21270;&#23398;&#31454;&#25216;&#22330;&#65288;Pheno-CA&#65289;&#22522;&#20934;&#36827;&#34892;&#31934;&#30830;&#24230;&#27979;&#35797;&#65306;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#22270;&#20687;&#39640;&#20869;&#28085;&#31579;&#36873;&#25968;&#25454;&#30340;&#22810;&#26679;&#21270;&#33647;&#29289;&#24320;&#21457;&#20219;&#21153;&#38598;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;Pheno-CA&#20219;&#21153;&#20013;&#26126;&#30830;&#34987;&#30417;&#30563;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#25968;&#25454;&#21644;&#27169;&#22411;&#35268;&#27169;&#25193;&#22823;&#26102;&#24182;&#27809;&#26377;&#25345;&#32493;&#25913;&#36827;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#39033;&#26032;&#30340;&#21069;&#20307;&#20219;&#21153;&#65292;&#21363;&#36870;&#29983;&#29289;&#36807;&#31243;&#65288;IBP&#65289;&#65292;&#23427;&#35774;&#35745;&#25104;&#31867;&#20284;&#20110;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#21462;&#24471;&#25104;&#21151;&#30340;&#22240;&#26524;&#30446;&#26631;&#20989;&#25968;&#12290;&#25105;&#20204;&#30830;&#23454;&#21457;&#29616;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#21462;&#20915;&#20110;&#24341;&#20837;IBP&#20219;&#21153;&#30340;&#26041;&#24335;&#65292;&#24403;&#20351;&#29992;IBP&#39044;&#35757;&#32451;&#26102;&#65292;&#27169;&#22411;&#22312;Pheno-CA&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#24471;&#21040;&#20102;&#26174;&#33879;&#25913;&#21892;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25581;&#31034;&#20102;&#22312;&#34920;&#22411;&#33647;&#29289;&#21457;&#29616;&#20013;&#65292;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35268;&#27169;&#21644;&#23398;&#20064;&#26041;&#24335;&#30456;&#20851;&#30340;&#22240;&#32032;&#23545;&#27169;&#22411;&#30340;&#24615;&#33021;&#20135;&#29983;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent breakthroughs by deep neural networks (DNNs) in natural language processing (NLP) and computer vision have been driven by a scale-up of models and data rather than the discovery of novel computing paradigms. Here, we investigate if scale can have a similar impact for models designed to aid small molecule drug discovery. We address this question through a large-scale and systematic analysis of how DNN size, data diet, and learning routines interact to impact accuracy on our Phenotypic Chemistry Arena (Pheno-CA) benchmark: a diverse set of drug development tasks posed on image-based high content screening data. Surprisingly, we find that DNNs explicitly supervised to solve tasks in the Pheno-CA do not continuously improve as their data and model size is scaled-up. To address this issue, we introduce a novel precursor task, the Inverse Biological Process (IBP), which is designed to resemble the causal objective functions that have proven successful for NLP. We indeed find that DNNs
&lt;/p&gt;</description></item><item><title>XVO&#26159;&#19968;&#31181;&#36890;&#36807;&#36328;&#27169;&#24577;&#33258;&#25105;&#35757;&#32451;&#30340;&#27867;&#21270;&#35270;&#35273;&#37324;&#31243;&#35745;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#21644;&#29615;&#22659;&#35774;&#32622;&#19979;&#20855;&#26377;&#24378;&#22823;&#30340;&#33258;&#32473;&#33258;&#36275;&#25805;&#20316;&#30340;&#35757;&#32451;&#27169;&#22411;&#12290;&#20854;&#20851;&#38190;&#21019;&#26032;&#21644;&#36129;&#29486;&#21253;&#25324;&#36890;&#36807;&#21322;&#30417;&#30563;&#35757;&#32451;&#23398;&#20064;&#36890;&#29992;&#30340;&#30452;&#25509;VO&#22238;&#24402;&#32593;&#32476;&#20197;&#21450;&#20351;&#29992;&#22810;&#27169;&#24335;&#30417;&#30563;&#20219;&#21153;&#26469;&#20419;&#36827;&#27867;&#21270;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2309.16772</link><description>&lt;p&gt;
XVO: &#36890;&#36807;&#36328;&#27169;&#24577;&#33258;&#25105;&#35757;&#32451;&#30340;&#27867;&#21270;&#35270;&#35273;&#37324;&#31243;&#35745;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
XVO: Generalized Visual Odometry via Cross-Modal Self-Training. (arXiv:2309.16772v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16772
&lt;/p&gt;
&lt;p&gt;
XVO&#26159;&#19968;&#31181;&#36890;&#36807;&#36328;&#27169;&#24577;&#33258;&#25105;&#35757;&#32451;&#30340;&#27867;&#21270;&#35270;&#35273;&#37324;&#31243;&#35745;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#21644;&#29615;&#22659;&#35774;&#32622;&#19979;&#20855;&#26377;&#24378;&#22823;&#30340;&#33258;&#32473;&#33258;&#36275;&#25805;&#20316;&#30340;&#35757;&#32451;&#27169;&#22411;&#12290;&#20854;&#20851;&#38190;&#21019;&#26032;&#21644;&#36129;&#29486;&#21253;&#25324;&#36890;&#36807;&#21322;&#30417;&#30563;&#35757;&#32451;&#23398;&#20064;&#36890;&#29992;&#30340;&#30452;&#25509;VO&#22238;&#24402;&#32593;&#32476;&#20197;&#21450;&#20351;&#29992;&#22810;&#27169;&#24335;&#30417;&#30563;&#20219;&#21153;&#26469;&#20419;&#36827;&#27867;&#21270;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;XVO&#65292;&#19968;&#31181;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#21644;&#29615;&#22659;&#35774;&#32622;&#19979;&#20855;&#26377;&#24378;&#22823;&#30340;&#33258;&#32473;&#33258;&#36275;&#25805;&#20316;&#30340;&#27867;&#21270;&#21333;&#30446;&#35270;&#35273;&#37324;&#31243;&#35745;&#65288;VO&#65289;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;&#19982;&#36890;&#24120;&#30740;&#31350;&#21333;&#20010;&#25968;&#25454;&#38598;&#20869;&#24050;&#30693;&#26657;&#20934;&#30340;&#26631;&#20934;&#21333;&#30446;VO&#26041;&#27861;&#19981;&#21516;&#65292;XVO&#21487;&#20197;&#39640;&#25928;&#22320;&#36890;&#36807;&#35270;&#35273;&#22330;&#26223;&#35821;&#20041;&#65288;&#21363;&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#24050;&#30693;&#30456;&#26426;&#21442;&#25968;&#65289;&#23398;&#20064;&#24674;&#22797;&#30456;&#23545;&#20301;&#23039;&#65292;&#24182;&#20174;YouTube&#19978;&#30340;&#22823;&#37327;&#26080;&#32422;&#26463;&#21644;&#24322;&#26500;&#30340;&#36710;&#36733;&#25668;&#20687;&#22836;&#35270;&#39057;&#36827;&#34892;&#33258;&#25105;&#35757;&#32451;&#26469;&#20248;&#21270;&#36816;&#21160;&#20272;&#35745;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#36129;&#29486;&#26377;&#20004;&#20010;&#26041;&#38754;&#65306;&#31532;&#19968;&#65292;&#25105;&#20204;&#32463;&#39564;&#35777;&#26126;&#20102;&#21322;&#30417;&#30563;&#35757;&#32451;&#23545;&#20110;&#23398;&#20064;&#36890;&#29992;&#30340;&#30452;&#25509;VO&#22238;&#24402;&#32593;&#32476;&#30340;&#22909;&#22788;&#12290;&#31532;&#20108;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22810;&#27169;&#24335;&#30417;&#30563;&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#20998;&#21106;&#12289;&#20809;&#27969;&#12289;&#28145;&#24230;&#21644;&#38899;&#39057;&#36741;&#21161;&#39044;&#27979;&#20219;&#21153;&#65292;&#20197;&#20419;&#36827;VO&#20219;&#21153;&#30340;&#27867;&#21270;&#34920;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21457;&#29616;&#38899;&#39057;&#39044;&#27979;&#20219;&#21153;&#23545;&#20110;&#24635;&#32467;&#25688;&#35201;&#30340;&#20851;&#38190;&#21019;&#26032;&#21644;&#36129;&#29486;&#26377;&#20419;&#36827;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose XVO, a semi-supervised learning method for training generalized monocular Visual Odometry (VO) models with robust off-the-self operation across diverse datasets and settings. In contrast to standard monocular VO approaches which often study a known calibration within a single dataset, XVO efficiently learns to recover relative pose with real-world scale from visual scene semantics, i.e., without relying on any known camera parameters. We optimize the motion estimation model via self-training from large amounts of unconstrained and heterogeneous dash camera videos available on YouTube. Our key contribution is twofold. First, we empirically demonstrate the benefits of semi-supervised training for learning a general-purpose direct VO regression network. Second, we demonstrate multi-modal supervision, including segmentation, flow, depth, and audio auxiliary prediction tasks, to facilitate generalized representations for the VO task. Specifically, we find audio prediction task to
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Persona&#32534;&#30721;&#22810;&#27969;&#31243;&#23545;&#35805;&#21477;&#23376;&#35780;&#20998;&#26041;&#27861;&#65292;&#21033;&#29992;&#20010;&#20154;&#35282;&#33394;&#20449;&#24687;&#26469;&#25552;&#39640;&#23545;&#35805;&#29983;&#25104;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2309.16770</link><description>&lt;p&gt;
Persona&#32534;&#30721;&#22810;&#27969;&#31243;&#23545;&#35805;&#21477;&#23376;&#35780;&#20998;&#65306;Persona&#24341;&#23548;&#30340;&#22810;&#27969;&#31243;&#23545;&#35805;&#21477;&#23376;&#35780;&#20998;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Persona-Coded Poly-Encoder: Persona-Guided Multi-Stream Conversational Sentence Scoring. (arXiv:2309.16770v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16770
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Persona&#32534;&#30721;&#22810;&#27969;&#31243;&#23545;&#35805;&#21477;&#23376;&#35780;&#20998;&#26041;&#27861;&#65292;&#21033;&#29992;&#20010;&#20154;&#35282;&#33394;&#20449;&#24687;&#26469;&#25552;&#39640;&#23545;&#35805;&#29983;&#25104;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#24050;&#32463;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#24191;&#27867;&#24212;&#29992;&#20110;&#23545;&#35805;AI&#12290;&#28982;&#32780;&#65292;&#35201;&#21033;&#29992;&#21487;&#20197;&#25552;&#20379;&#23545;&#35805;&#32972;&#26223;&#25110;&#20010;&#24615;&#21270;&#35843;&#25972;&#30340;&#36741;&#21161;&#20449;&#24687;&#20197;&#25552;&#39640;&#23545;&#35805;&#36136;&#37327;&#20173;&#28982;&#24456;&#20855;&#25361;&#25112;&#24615;&#12290;&#20363;&#22914;&#65292;&#20851;&#20110;&#20351;&#29992;&#20010;&#20154;&#35282;&#33394;&#20449;&#24687;&#26469;&#25552;&#39640;&#23545;&#35805;&#36136;&#37327;&#30340;&#30740;&#31350;&#20165;&#26377;&#38480;&#65292;&#21363;&#20351;&#26159;&#26368;&#20808;&#36827;&#30340;&#23545;&#35805;AI&#25216;&#26415;&#20063;&#26080;&#27861;&#26377;&#25928;&#22320;&#21033;&#29992;&#26469;&#33258;&#22810;&#31181;&#26469;&#28304;&#30340;&#36741;&#21161;&#25968;&#25454;&#20449;&#21495;&#65292;&#20363;&#22914;&#22810;&#27169;&#24335;&#20132;&#20114;&#25968;&#25454;&#12289;&#20154;&#21475;&#32479;&#35745;&#23398;&#25968;&#25454;&#21644;&#31038;&#20250;&#30830;&#23450;&#22240;&#32032;&#25968;&#25454;&#31561;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Persona&#32534;&#30721;&#22810;&#27969;&#31243;&#23545;&#35805;&#21477;&#23376;&#35780;&#20998;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#22810;&#27969;&#31243;&#32534;&#30721;&#26041;&#26696;&#20013;&#30340;&#20010;&#20154;&#35282;&#33394;&#20449;&#24687;&#26469;&#25552;&#39640;&#23545;&#35805;&#29983;&#25104;&#30340;&#36136;&#37327;&#12290;&#20026;&#20102;&#23637;&#31034;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#22312;&#20004;&#20010;&#19981;&#21516;&#30340;&#22522;&#20110;&#20010;&#20154;&#35282;&#33394;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#19982;&#21442;&#32771;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in machine learning and deep learning have led to the widespread use of Conversational AI in many practical applications. However, it is still very challenging to leverage auxiliary information that can provide conversational context or personalized tuning to improve the quality of conversations. For example, there has only been limited research on using an individuals persona information to improve conversation quality, and even state-of-the-art conversational AI techniques are unable to effectively leverage signals from heterogeneous sources of auxiliary data, such as multi-modal interaction data, demographics, SDOH data, etc. In this paper, we present a novel Persona-Coded Poly-Encoder method that leverages persona information in a multi-stream encoding scheme to improve the quality of response generation for conversations. To show the efficacy of the proposed method, we evaluate our method on two different persona-based conversational datasets, and compared against 
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#26597;&#32508;&#36848;&#20102;&#25193;&#25955;&#27169;&#22411;&#65288;DMs&#65289;&#21644;&#20851;&#32852;&#35760;&#24518;&#65288;AMs&#65289;&#20043;&#38388;&#30340;&#25968;&#23398;&#32852;&#31995;&#65292;&#25581;&#31034;&#20102;DMs&#26159;&#22914;&#20309;&#21033;&#29992;&#33021;&#37327;&#20989;&#25968;&#36827;&#34892;&#21435;&#22122;&#25968;&#25454;&#30340;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2309.16750</link><description>&lt;p&gt;
&#30524;&#20013;&#35760;&#24518;&#65306;&#25193;&#25955;&#27169;&#22411;&#21644;&#20851;&#32852;&#35760;&#24518;&#20043;&#38388;&#30340;&#31070;&#31192;&#30456;&#20284;&#20043;&#22788;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Memory in Plain Sight: A Survey of the Uncanny Resemblances between Diffusion Models and Associative Memories. (arXiv:2309.16750v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16750
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#32508;&#36848;&#20102;&#25193;&#25955;&#27169;&#22411;&#65288;DMs&#65289;&#21644;&#20851;&#32852;&#35760;&#24518;&#65288;AMs&#65289;&#20043;&#38388;&#30340;&#25968;&#23398;&#32852;&#31995;&#65292;&#25581;&#31034;&#20102;DMs&#26159;&#22914;&#20309;&#21033;&#29992;&#33021;&#37327;&#20989;&#25968;&#36827;&#34892;&#21435;&#22122;&#25968;&#25454;&#30340;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#65288;DMs&#65289;&#26368;&#36817;&#22312;&#35768;&#22810;&#29983;&#25104;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#23545;&#23427;&#20204;&#30340;&#25968;&#23398;&#25551;&#36848;&#26377;&#24456;&#22810;&#31181;&#26041;&#24335;&#65292;&#36825;&#20351;&#24471;&#20154;&#20204;&#24456;&#38590;&#23545;&#20854;&#24037;&#20316;&#21407;&#29702;&#36827;&#34892;&#31616;&#21333;&#29702;&#35299;&#12290;&#22312;&#36825;&#39033;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#20174;&#21160;&#21147;&#31995;&#32479;&#21644;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#30340;&#35282;&#24230;&#25552;&#20379;&#20102;DMs&#30340;&#31616;&#26126;&#27010;&#36848;&#65292;&#25581;&#31034;&#20102;&#19968;&#31181;&#19982;&#20854;&#39640;&#24230;&#30456;&#20851;&#20294;&#24120;&#24120;&#34987;&#24573;&#35270;&#30340;&#33021;&#37327;&#27169;&#22411;&#31867;&#21035;&#65292;&#31216;&#20026;&#20851;&#32852;&#35760;&#24518;&#65288;AMs&#65289;&#30340;&#25968;&#23398;&#32852;&#31995;&#12290;&#22522;&#20110;&#33021;&#37327;&#30340;AMs&#26159;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#20854;&#34892;&#20026;&#19982;&#21435;&#22122;DMs&#38750;&#24120;&#30456;&#20284;&#65292;&#20294;&#23427;&#20204;&#20351;&#25105;&#20204;&#33021;&#22815;&#30452;&#25509;&#35745;&#31639;&#19968;&#20010;Lyapunov&#33021;&#37327;&#20989;&#25968;&#65292;&#22312;&#20854;&#19978;&#21487;&#20197;&#25191;&#34892;&#26799;&#24230;&#19979;&#38477;&#20197;&#21435;&#22122;&#25968;&#25454;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#33021;&#37327;AMs&#30340;40&#24180;&#21382;&#21490;&#65292;&#20174;&#26368;&#21021;&#30340;Hopfield&#32593;&#32476;&#24320;&#22987;&#65292;&#24182;&#35752;&#35770;&#20102;&#36890;&#36807;&#25551;&#36848;&#23427;&#20204;&#30340;&#30456;&#20284;&#24615;&#21644;&#24046;&#24322;&#31243;&#24230;&#25581;&#31034;&#20986;&#26469;&#30340;AMs&#21644;DMs&#30340;&#26032;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion Models (DMs) have recently set state-of-the-art on many generation benchmarks. However, there are myriad ways to describe them mathematically, which makes it difficult to develop a simple understanding of how they work. In this survey, we provide a concise overview of DMs from the perspective of dynamical systems and Ordinary Differential Equations (ODEs) which exposes a mathematical connection to the highly related yet often overlooked class of energy-based models, called Associative Memories (AMs). Energy-based AMs are a theoretical framework that behave much like denoising DMs, but they enable us to directly compute a Lyapunov energy function on which we can perform gradient descent to denoise data. We then summarize the 40 year history of energy-based AMs, beginning with the original Hopfield Network, and discuss new research directions for AMs and DMs that are revealed by characterizing the extent of their similarities and differences
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21457;&#29616;&#29615;&#22659;&#30340;&#31639;&#27861; XRM&#65292;&#23427;&#36890;&#36807;&#35757;&#32451;&#20004;&#20010;&#23402;&#29983;&#32593;&#32476;&#65292;&#27599;&#20010;&#32593;&#32476;&#20174;&#35757;&#32451;&#25968;&#25454;&#30340;&#19968;&#21322;&#20013;&#23398;&#20064;&#65292;&#24182;&#27169;&#20223;&#20854;&#20804;&#24351;&#32593;&#32476;&#30340;&#38169;&#35823;&#20998;&#31867;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#38656;&#35201;&#20381;&#36182;&#20154;&#24037;&#27880;&#37322;&#29615;&#22659;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.16748</link><description>&lt;p&gt;
&#29992;XRM&#21457;&#29616;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
Discovering environments with XRM. (arXiv:2309.16748v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16748
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21457;&#29616;&#29615;&#22659;&#30340;&#31639;&#27861; XRM&#65292;&#23427;&#36890;&#36807;&#35757;&#32451;&#20004;&#20010;&#23402;&#29983;&#32593;&#32476;&#65292;&#27599;&#20010;&#32593;&#32476;&#20174;&#35757;&#32451;&#25968;&#25454;&#30340;&#19968;&#21322;&#20013;&#23398;&#20064;&#65292;&#24182;&#27169;&#20223;&#20854;&#20804;&#24351;&#32593;&#32476;&#30340;&#38169;&#35823;&#20998;&#31867;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#38656;&#35201;&#20381;&#36182;&#20154;&#24037;&#27880;&#37322;&#29615;&#22659;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25104;&#21151;&#30340;&#36328;&#39046;&#22495;&#27867;&#21270;&#38656;&#35201;&#29615;&#22659;&#27880;&#37322;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27880;&#37322;&#30340;&#33719;&#21462;&#26159;&#36164;&#28304;&#23494;&#38598;&#22411;&#30340;&#65292;&#24182;&#19988;&#23427;&#20204;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#21463;&#20154;&#31867;&#27880;&#37322;&#32773;&#30340;&#26399;&#26395;&#21644;&#24863;&#30693;&#20559;&#24046;&#30340;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#23454;&#29616;&#24212;&#29992;&#39046;&#22495;&#20840;&#38754;&#27867;&#21270;&#30340;&#40065;&#26834;&#24615;AI&#31995;&#32479;&#65292;&#25105;&#20204;&#24517;&#39035;&#24320;&#21457;&#19968;&#31181;&#31639;&#27861;&#26469;&#33258;&#21160;&#21457;&#29616;&#24341;&#21457;&#24191;&#27867;&#27867;&#21270;&#30340;&#29615;&#22659;&#12290;&#30446;&#21069;&#30340;&#25552;&#26696;&#26681;&#25454;&#35757;&#32451;&#35823;&#24046;&#23558;&#31034;&#20363;&#21010;&#20998;&#20026;&#19981;&#21516;&#30340;&#31867;&#65292;&#20294;&#23384;&#22312;&#19968;&#20010;&#26681;&#26412;&#38382;&#39064;&#12290;&#36825;&#20123;&#26041;&#27861;&#28155;&#21152;&#20102;&#36229;&#21442;&#25968;&#21644;&#26089;&#20572;&#31574;&#30053;&#65292;&#32780;&#36825;&#20123;&#21442;&#25968;&#26159;&#26080;&#27861;&#22312;&#27809;&#26377;&#20154;&#31867;&#27880;&#37322;&#29615;&#22659;&#30340;&#39564;&#35777;&#38598;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#35843;&#25972;&#30340;&#65292;&#32780;&#36825;&#20123;&#20449;&#24687;&#27491;&#26159;&#35201;&#21457;&#29616;&#30340;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; Cross-Risk-Minimization (XRM) &#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;XRM &#35757;&#32451;&#20004;&#20010;&#23402;&#29983;&#32593;&#32476;&#65292;&#27599;&#20010;&#32593;&#32476;&#20174;&#35757;&#32451;&#25968;&#25454;&#30340;&#19968;&#20010;&#38543;&#26426;&#19968;&#21322;&#20013;&#23398;&#20064;&#65292;&#21516;&#26102;&#27169;&#20223;&#20854;&#20804;&#24351;&#32593;&#32476;&#25152;&#20570;&#30340;&#33258;&#20449;&#30340;&#38169;&#35823;&#20998;&#31867;&#12290;XRM &#25552;&#20379;&#20102;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#20381;&#36182;&#20154;&#24037;&#27880;&#37322;&#30340;&#29615;&#22659;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Successful out-of-distribution generalization requires environment annotations. Unfortunately, these are resource-intensive to obtain, and their relevance to model performance is limited by the expectations and perceptual biases of human annotators. Therefore, to enable robust AI systems across applications, we must develop algorithms to automatically discover environments inducing broad generalization. Current proposals, which divide examples based on their training error, suffer from one fundamental problem. These methods add hyper-parameters and early-stopping criteria that are impossible to tune without a validation set with human-annotated environments, the very information subject to discovery. In this paper, we propose Cross-Risk-Minimization (XRM) to address this issue. XRM trains two twin networks, each learning from one random half of the training data, while imitating confident held-out mistakes made by its sibling. XRM provides a recipe for hyper-parameter tuning, does not 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24335;&#28798;&#23475;&#39044;&#27979;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#22825;&#27668;&#32479;&#35745;&#25968;&#25454;&#12289;&#21355;&#26143;&#22270;&#20687;&#21644;&#25991;&#26412;&#20449;&#24687;&#12290;&#36890;&#36807;&#25972;&#21512;&#22810;&#20010;&#25968;&#25454;&#28304;&#65292;&#23545;&#20110;&#19981;&#21516;&#31867;&#22411;&#30340;&#28798;&#23475;&#39044;&#27979;&#65292;&#27169;&#22411;&#30340;&#24615;&#33021;&#24471;&#21040;&#20102;&#22686;&#24378;&#12290;</title><link>http://arxiv.org/abs/2309.16747</link><description>&lt;p&gt;
&#21033;&#29992;&#22810;&#26679;&#25968;&#25454;&#36827;&#34892;&#20840;&#29699;&#28798;&#23475;&#39044;&#27979;&#30340;&#22810;&#27169;&#24335;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Harnessing Diverse Data for Global Disaster Prediction: A Multimodal Framework. (arXiv:2309.16747v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16747
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24335;&#28798;&#23475;&#39044;&#27979;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#22825;&#27668;&#32479;&#35745;&#25968;&#25454;&#12289;&#21355;&#26143;&#22270;&#20687;&#21644;&#25991;&#26412;&#20449;&#24687;&#12290;&#36890;&#36807;&#25972;&#21512;&#22810;&#20010;&#25968;&#25454;&#28304;&#65292;&#23545;&#20110;&#19981;&#21516;&#31867;&#22411;&#30340;&#28798;&#23475;&#39044;&#27979;&#65292;&#27169;&#22411;&#30340;&#24615;&#33021;&#24471;&#21040;&#20102;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#27668;&#20505;&#21464;&#21270;&#30340;&#21152;&#21095;&#65292;&#20934;&#30830;&#30340;&#20840;&#29699;&#35268;&#27169;&#28798;&#23475;&#39044;&#27979;&#21464;&#24471;&#36234;&#26469;&#36234;&#32039;&#36843;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24335;&#28798;&#23475;&#39044;&#27979;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#22825;&#27668;&#32479;&#35745;&#25968;&#25454;&#12289;&#21355;&#26143;&#22270;&#20687;&#21644;&#25991;&#26412;&#20449;&#24687;&#12290;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;"&#27946;&#27700;"&#21644;"&#28369;&#22369;"&#30340;&#39044;&#27979;&#65292;&#22240;&#20026;&#23427;&#20204;&#19982;&#27668;&#35937;&#21644;&#22320;&#24418;&#22240;&#32032;&#26377;&#20851;&#12290;&#35813;&#27169;&#22411;&#26681;&#25454;&#21487;&#29992;&#25968;&#25454;&#36827;&#34892;&#31934;&#32454;&#35774;&#35745;&#65292;&#24182;&#37319;&#29992;&#31574;&#30053;&#26469;&#35299;&#20915;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#23613;&#31649;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#25972;&#21512;&#22810;&#20010;&#25968;&#25454;&#28304;&#21487;&#20197;&#22686;&#24378;&#27169;&#22411;&#24615;&#33021;&#65292;&#20294;&#22686;&#24378;&#30340;&#31243;&#24230;&#21462;&#20915;&#20110;&#27599;&#31181;&#28798;&#23475;&#30340;&#20855;&#20307;&#24615;&#36136;&#21644;&#20854;&#29420;&#29305;&#30340;&#28508;&#22312;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
As climate change intensifies, the urgency for accurate global-scale disaster predictions grows. This research presents a novel multimodal disaster prediction framework, combining weather statistics, satellite imagery, and textual insights. We particularly focus on "flood" and "landslide" predictions, given their ties to meteorological and topographical factors. The model is meticulously crafted based on the available data and we also implement strategies to address class imbalance. While our findings suggest that integrating multiple data sources can bolster model performance, the extent of enhancement differs based on the specific nature of each disaster and their unique underlying causes.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#21152;&#36895;&#25968;&#20540;&#35299;&#31639;&#22120;&#30340;&#28145;&#24230;&#20195;&#29702;&#30340;&#39640;&#36890;&#37327;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#22823;&#37327;&#38598;&#21512;&#36816;&#34892;&#30340;&#27169;&#25311;&#20013;&#22312;&#32447;&#35757;&#32451;&#27169;&#22411;&#65292;&#21033;&#29992;&#22810;&#32423;&#24182;&#34892;&#24615;&#29983;&#25104;&#20016;&#23500;&#30340;&#25968;&#25454;&#38598;&#65292;&#30452;&#25509;&#27969;&#24335;&#20256;&#36755;&#25968;&#25454;&#20197;&#36991;&#20813;I/O&#29942;&#39048;&#21644;&#23384;&#20648;&#38382;&#39064;&#65292;&#21516;&#26102;&#20351;&#29992;&#35757;&#32451;&#20648;&#22791;&#27744;&#26469;&#20943;&#36731;&#27969;&#24335;&#20256;&#36755;&#30340;&#22266;&#26377;&#20559;&#24046;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#20351;&#29992;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;2&#23567;&#26102;&#20869;&#22312;8TB&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#20986;&#20934;&#30830;&#29575;&#25552;&#21319;&#20102;47%&#12289;&#25209;&#37327;&#21534;&#21520;&#37327;&#25552;&#39640;&#20102;13&#20493;&#30340;&#28909;&#26041;&#31243;&#20195;&#29702;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2309.16743</link><description>&lt;p&gt;
&#20174;&#22823;&#22411;&#38598;&#21512;&#36816;&#34892;&#20013;&#39640;&#36890;&#37327;&#35757;&#32451;&#28145;&#24230;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
High Throughput Training of Deep Surrogates from Large Ensemble Runs. (arXiv:2309.16743v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16743
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#21152;&#36895;&#25968;&#20540;&#35299;&#31639;&#22120;&#30340;&#28145;&#24230;&#20195;&#29702;&#30340;&#39640;&#36890;&#37327;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#22823;&#37327;&#38598;&#21512;&#36816;&#34892;&#30340;&#27169;&#25311;&#20013;&#22312;&#32447;&#35757;&#32451;&#27169;&#22411;&#65292;&#21033;&#29992;&#22810;&#32423;&#24182;&#34892;&#24615;&#29983;&#25104;&#20016;&#23500;&#30340;&#25968;&#25454;&#38598;&#65292;&#30452;&#25509;&#27969;&#24335;&#20256;&#36755;&#25968;&#25454;&#20197;&#36991;&#20813;I/O&#29942;&#39048;&#21644;&#23384;&#20648;&#38382;&#39064;&#65292;&#21516;&#26102;&#20351;&#29992;&#35757;&#32451;&#20648;&#22791;&#27744;&#26469;&#20943;&#36731;&#27969;&#24335;&#20256;&#36755;&#30340;&#22266;&#26377;&#20559;&#24046;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#20351;&#29992;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;2&#23567;&#26102;&#20869;&#22312;8TB&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#20986;&#20934;&#30830;&#29575;&#25552;&#21319;&#20102;47%&#12289;&#25209;&#37327;&#21534;&#21520;&#37327;&#25552;&#39640;&#20102;13&#20493;&#30340;&#28909;&#26041;&#31243;&#20195;&#29702;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#21152;&#36895;&#25968;&#20540;&#35299;&#31639;&#22120;&#26041;&#38754;&#21462;&#24471;&#20102;&#31361;&#30772;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#25552;&#20379;&#23545;&#29289;&#29702;&#19990;&#30028;&#30340;&#24544;&#23454;&#20294;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#27169;&#25311;&#12290;&#36825;&#20123;&#28145;&#24230;&#20195;&#29702;&#36890;&#24120;&#36890;&#36807;&#21516;&#19968;&#35299;&#31639;&#22120;&#29983;&#25104;&#30340;&#26377;&#38480;&#37327;&#30340;&#25968;&#25454;&#26377;&#30417;&#30563;&#22320;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24320;&#28304;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#22823;&#37327;&#38598;&#21512;&#36816;&#34892;&#30340;&#27169;&#25311;&#20013;&#22312;&#32447;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#12290;&#23427;&#21033;&#29992;&#22810;&#20010;&#32423;&#21035;&#30340;&#24182;&#34892;&#24615;&#26469;&#29983;&#25104;&#20016;&#23500;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#36890;&#36807;&#30452;&#25509;&#27969;&#24335;&#20256;&#36755;&#29983;&#25104;&#30340;&#25968;&#25454;&#26469;&#36991;&#20813;I/O&#29942;&#39048;&#21644;&#23384;&#20648;&#38382;&#39064;&#12290;&#35757;&#32451;&#20648;&#22791;&#27744;&#22312;&#26368;&#22823;&#21270;GPU&#21534;&#21520;&#37327;&#30340;&#21516;&#26102;&#65292;&#20943;&#36731;&#20102;&#27969;&#24335;&#20256;&#36755;&#30340;&#22266;&#26377;&#20559;&#24046;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#22312;2&#23567;&#26102;&#20869;&#33021;&#22815;&#22312;8TB&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#23436;&#20840;&#36830;&#25509;&#32593;&#32476;&#20316;&#20026;&#28909;&#26041;&#31243;&#30340;&#20195;&#29702;&#65292;&#30456;&#27604;&#20256;&#32479;&#30340;&#31163;&#32447;&#36807;&#31243;&#65292;&#20934;&#30830;&#29575;&#25552;&#21319;&#20102;47&#65285;&#65292;&#25209;&#37327;&#21534;&#21520;&#37327;&#25552;&#39640;&#20102;13&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have seen a surge in deep learning approaches to accelerate numerical solvers, which provide faithful but computationally intensive simulations of the physical world. These deep surrogates are generally trained in a supervised manner from limited amounts of data slowly generated by the same solver they intend to accelerate. We propose an open-source framework that enables the online training of these models from a large ensemble run of simulations. It leverages multiple levels of parallelism to generate rich datasets. The framework avoids I/O bottlenecks and storage issues by directly streaming the generated data. A training reservoir mitigates the inherent bias of streaming while maximizing GPU throughput. Experiment on training a fully connected network as a surrogate for the heat equation shows the proposed approach enables training on 8TB of data in 2 hours with an accuracy improved by 47% and a batch throughput multiplied by 13 compared to a traditional offline proced
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;2&#22411;&#31958;&#23615;&#30149;&#24739;&#32773;&#30333;&#34507;&#30333;&#23615;&#30340;&#26089;&#26399;&#39044;&#27979;&#38382;&#39064;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#12290;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#30340;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#23545;184&#26465;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#24471;&#20986;&#20102;&#39044;&#27979;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.16742</link><description>&lt;p&gt;
&#26089;&#26399;&#26816;&#27979;2&#22411;&#31958;&#23615;&#30149;&#24739;&#32773;&#30333;&#34507;&#30333;&#23615;&#39118;&#38505;&#30340;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Supervised Learning Models for Early Detection of Albuminuria Risk in Type-2 Diabetes Mellitus Patients. (arXiv:2309.16742v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16742
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;2&#22411;&#31958;&#23615;&#30149;&#24739;&#32773;&#30333;&#34507;&#30333;&#23615;&#30340;&#26089;&#26399;&#39044;&#27979;&#38382;&#39064;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#12290;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#30340;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#23545;184&#26465;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#24471;&#20986;&#20102;&#39044;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31958;&#23615;&#30149;&#65292;&#23588;&#20854;&#26159;2&#22411;&#31958;&#23615;&#30149;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#30340;&#20581;&#24247;&#38382;&#39064;&#12290;&#19982;&#31958;&#23615;&#30149;&#30456;&#20851;&#30340;&#19968;&#20010;&#20027;&#35201;&#38382;&#39064;&#26159;&#20854;&#24182;&#21457;&#30151;&#30340;&#21457;&#23637;&#12290;&#31958;&#23615;&#30149;&#32958;&#30149;&#26159;&#31958;&#23615;&#30149;&#30340;&#19968;&#31181;&#24930;&#24615;&#24182;&#21457;&#30151;&#65292;&#19981;&#21033;&#22320;&#24433;&#21709;&#32958;&#33039;&#65292;&#23548;&#33268;&#32958;&#33039;&#25439;&#20260;&#12290;&#35786;&#26029;&#31958;&#23615;&#30149;&#32958;&#30149;&#28041;&#21450;&#32771;&#34385;&#21508;&#31181;&#26631;&#20934;&#20043;&#19968;&#65292;&#20854;&#20013;&#20043;&#19968;&#26159;&#23615;&#28082;&#20013;&#30333;&#34507;&#30333;&#30340;&#30149;&#29702;&#23398;&#30149;&#29702;&#23398;&#25968;&#37327;&#65292;&#31216;&#20026;&#30333;&#34507;&#30333;&#23615;&#12290;&#22240;&#27492;&#65292;&#23545;&#31958;&#23615;&#30149;&#24739;&#32773;&#23615;&#28082;&#20013;&#30333;&#34507;&#30333;&#23615;&#30340;&#26089;&#26399;&#39044;&#27979;&#20855;&#26377;&#21450;&#26102;&#39044;&#38450;&#25514;&#26045;&#30340;&#28508;&#21147;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#39044;&#27979;2&#22411;&#31958;&#23615;&#30149;&#24739;&#32773;&#24739;&#26377;&#30333;&#34507;&#30333;&#23615;&#30340;&#39118;&#38505;&#12290;&#25152;&#36873;&#30340;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#21253;&#25324;&#26420;&#32032;&#36125;&#21494;&#26031;&#65292;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#65292;&#20915;&#31574;&#26641;&#65292;&#38543;&#26426;&#26862;&#26519;&#65292;AdaBoost&#65292;XGBoost&#21644;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#12290;&#25105;&#20204;&#30340;&#31169;&#26377;&#25968;&#25454;&#38598;&#21253;&#25324;184&#26465;&#31958;&#23615;&#30149;&#24182;&#21457;&#30151;&#39118;&#38505;&#22240;&#32032;&#30340;&#26465;&#30446;&#34987;&#29992;&#26469;&#35757;&#32451;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Diabetes, especially T2DM, continues to be a significant health problem. One of the major concerns associated with diabetes is the development of its complications. Diabetic nephropathy, one of the chronic complication of diabetes, adversely affects the kidneys, leading to kidney damage. Diagnosing diabetic nephropathy involves considering various criteria, one of which is the presence of a pathologically significant quantity of albumin in urine, known as albuminuria. Thus, early prediction of albuminuria in diabetic patients holds the potential for timely preventive measures. This study aimed to develop a supervised learning model to predict the risk of developing albuminuria in T2DM patients. The selected supervised learning algorithms included Na\"ive Bayes, Support Vector Machine (SVM), decision tree, random forest, AdaBoost, XGBoost, and Multi-Layer Perceptron (MLP). Our private dataset, comprising 184 entries of diabetes complications risk factors, was used to train the algorithm
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#28145;&#24230;&#32534;&#30721;&#22120;&#22312;&#20302;&#32500;&#28508;&#31354;&#38388;&#20013;&#23384;&#20648;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#26694;&#26550;&#65292;&#20197;&#25429;&#25417;&#25968;&#25454;&#30340;&#37325;&#35201;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2309.16741</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#36890;&#36807;&#28508;&#31354;&#38388;&#25237;&#24433;&#30340;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Multi-Modal Financial Time-Series Retrieval Through Latent Space Projections. (arXiv:2309.16741v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16741
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#28145;&#24230;&#32534;&#30721;&#22120;&#22312;&#20302;&#32500;&#28508;&#31354;&#38388;&#20013;&#23384;&#20648;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#26694;&#26550;&#65292;&#20197;&#25429;&#25417;&#25968;&#25454;&#30340;&#37325;&#35201;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37329;&#34701;&#20844;&#21496;&#36890;&#24120;&#22788;&#29702;&#21644;&#23384;&#20648;&#20135;&#29983;&#36830;&#32493;&#19988;&#39640;&#39057;&#30340;&#25968;&#21313;&#20159;&#26465;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;&#20026;&#20102;&#25903;&#25345;&#39640;&#25928;&#30340;&#25968;&#25454;&#23384;&#20648;&#21644;&#26816;&#32034;&#65292;&#20986;&#29616;&#20102;&#19987;&#38376;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#24211;&#21644;&#31995;&#32479;&#12290;&#36825;&#20123;&#25968;&#25454;&#24211;&#25903;&#25345;&#36890;&#36807;&#31867;&#20284;&#20110;&#32422;&#26463;&#21270;&#32467;&#26500;&#21270;&#26597;&#35810;&#35821;&#35328;&#65288;SQL&#65289;&#30340;&#26684;&#24335;&#23545;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#32034;&#24341;&#21644;&#26597;&#35810;&#65292;&#20197;&#23454;&#29616;&#20687;&#8220;&#26376;&#24230;&#20215;&#26684;&#22238;&#25253;&#22823;&#20110;5%&#30340;&#32929;&#31080;&#8221;&#36825;&#26679;&#30340;&#26597;&#35810;&#65292;&#24182;&#20197;&#20005;&#26684;&#30340;&#26684;&#24335;&#34920;&#36798;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#26597;&#35810;&#19981;&#33021;&#25429;&#25417;&#21040;&#39640;&#32500;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#20869;&#22312;&#22797;&#26434;&#24615;&#65292;&#23427;&#20204;&#24448;&#24448;&#21487;&#20197;&#36890;&#36807;&#22270;&#20687;&#25110;&#35821;&#35328;&#65288;&#20363;&#22914;&#8220;&#22788;&#20110;&#20302;&#27874;&#21160;&#24615;&#29366;&#24577;&#30340;&#32929;&#31080;&#8221;&#65289;&#26356;&#22909;&#22320;&#25551;&#36848;&#12290;&#32780;&#19988;&#65292;&#22312;&#26102;&#38388;&#24207;&#21015;&#31354;&#38388;&#20013;&#36827;&#34892;&#25628;&#32034;&#25152;&#38656;&#30340;&#23384;&#20648;&#12289;&#35745;&#31639;&#26102;&#38388;&#21644;&#26816;&#32034;&#22797;&#26434;&#24230;&#24448;&#24448;&#26159;&#38750;&#24179;&#20961;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#28436;&#31034;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#32534;&#30721;&#22120;&#22312;&#20302;&#32500;&#28508;&#31354;&#38388;&#20013;&#23384;&#20648;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#26694;&#26550;&#65292;&#20351;&#24471;&#28508;&#31354;&#38388;&#25237;&#24433;&#21487;&#20197;&#25429;&#25417;&#21040;&#25968;&#25454;&#30340;&#37325;&#35201;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Financial firms commonly process and store billions of time-series data, generated continuously and at a high frequency. To support efficient data storage and retrieval, specialized time-series databases and systems have emerged. These databases support indexing and querying of time-series by a constrained Structured Query Language(SQL)-like format to enable queries like "Stocks with monthly price returns greater than 5%", and expressed in rigid formats. However, such queries do not capture the intrinsic complexity of high dimensional time-series data, which can often be better described by images or language (e.g., "A stock in low volatility regime"). Moreover, the required storage, computational time, and retrieval complexity to search in the time-series space are often non-trivial. In this paper, we propose and demonstrate a framework to store multi-modal data for financial time-series in a lower-dimensional latent space using deep encoders, such that the latent space projections ca
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#37096;&#32626;&#22312;6G&#36793;&#32536;&#30340;&#28508;&#21147;&#21644;&#25361;&#25112;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#30001;LLMs&#25903;&#25345;&#30340;&#20851;&#38190;&#24212;&#29992;&#65292;&#24182;&#20174;&#21709;&#24212;&#26102;&#38388;&#12289;&#24102;&#23485;&#25104;&#26412;&#21644;&#25968;&#25454;&#38544;&#31169;&#31561;&#26041;&#38754;&#20998;&#26512;&#20102;&#20113;&#31471;&#37096;&#32626;&#38754;&#20020;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;6G&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;(MEC)&#31995;&#32479;&#21487;&#33021;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#26041;&#26696;&#65292;&#24182;&#35752;&#35770;&#20102;&#36793;&#32536;&#35757;&#32451;&#21644;&#36793;&#32536;&#25512;&#29702;&#30340;&#21019;&#26032;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2309.16739</link><description>&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#33267;6G&#36793;&#32536;&#65306;&#35270;&#37326;&#12289;&#25361;&#25112;&#21644;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
Pushing Large Language Models to the 6G Edge: Vision, Challenges, and Opportunities. (arXiv:2309.16739v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16739
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#37096;&#32626;&#22312;6G&#36793;&#32536;&#30340;&#28508;&#21147;&#21644;&#25361;&#25112;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#30001;LLMs&#25903;&#25345;&#30340;&#20851;&#38190;&#24212;&#29992;&#65292;&#24182;&#20174;&#21709;&#24212;&#26102;&#38388;&#12289;&#24102;&#23485;&#25104;&#26412;&#21644;&#25968;&#25454;&#38544;&#31169;&#31561;&#26041;&#38754;&#20998;&#26512;&#20102;&#20113;&#31471;&#37096;&#32626;&#38754;&#20020;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;6G&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;(MEC)&#31995;&#32479;&#21487;&#33021;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#26041;&#26696;&#65292;&#24182;&#35752;&#35770;&#20102;&#36793;&#32536;&#35757;&#32451;&#21644;&#36793;&#32536;&#25512;&#29702;&#30340;&#21019;&#26032;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#33021;&#21147;&#65292;&#27491;&#22312;&#25913;&#21464;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#24182;&#26377;&#21487;&#33021;&#22609;&#36896;&#25105;&#20204;&#30340;&#26410;&#26469;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;LLMs&#30340;&#22810;&#27169;&#24577;&#29305;&#24615;&#65292;&#24403;&#21069;&#30340;&#22522;&#20110;&#20113;&#30340;&#37096;&#32626;&#38754;&#20020;&#30528;&#19968;&#20123;&#20851;&#38190;&#25361;&#25112;&#65306;1) &#21709;&#24212;&#26102;&#38388;&#38271;&#65307;2) &#39640;&#24102;&#23485;&#25104;&#26412;&#65307;&#20197;&#21450;3) &#36829;&#21453;&#25968;&#25454;&#38544;&#31169;&#12290;6G&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;(MEC)&#31995;&#32479;&#21487;&#33021;&#35299;&#20915;&#36825;&#20123;&#36843;&#20999;&#38382;&#39064;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;6G&#36793;&#32536;&#37096;&#32626;LLMs&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;&#30001;&#22810;&#27169;&#24577;LLMs&#25552;&#20379;&#25903;&#25345;&#30340;&#20851;&#38190;&#24212;&#29992;&#65292;&#21253;&#25324;&#26426;&#22120;&#20154;&#25216;&#26415;&#21644;&#21307;&#30103;&#20445;&#20581;&#65292;&#20197;&#31361;&#20986;&#22312;&#32456;&#31471;&#29992;&#25143;&#38468;&#36817;&#37096;&#32626;LLMs&#30340;&#38656;&#27714;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#22312;&#36793;&#32536;&#37096;&#32626;LLMs&#26102;&#38754;&#20020;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#24182;&#35774;&#24819;&#20102;&#36866;&#29992;&#20110;LLMs&#30340;6G MEC&#26550;&#26500;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#20004;&#20010;&#35774;&#35745;&#26041;&#38754;&#65292;&#21363;LLMs&#30340;&#36793;&#32536;&#35757;&#32451;&#21644;&#36793;&#32536;&#25512;&#29702;&#12290;&#22312;&#36825;&#20004;&#20010;&#26041;&#38754;&#65292;&#32771;&#34385;&#21040;&#36793;&#32536;&#30340;&#22266;&#26377;&#36164;&#28304;&#38480;&#21046;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#21508;&#31181;&#21069;&#27839;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), which have shown remarkable capabilities, are revolutionizing AI development and potentially shaping our future. However, given their multimodality, the status quo cloud-based deployment faces some critical challenges: 1) long response time; 2) high bandwidth costs; and 3) the violation of data privacy. 6G mobile edge computing (MEC) systems may resolve these pressing issues. In this article, we explore the potential of deploying LLMs at the 6G edge. We start by introducing killer applications powered by multimodal LLMs, including robotics and healthcare, to highlight the need for deploying LLMs in the vicinity of end users. Then, we identify the critical challenges for LLM deployment at the edge and envision the 6G MEC architecture for LLMs. Furthermore, we delve into two design aspects, i.e., edge training and edge inference for LLMs. In both aspects, considering the inherent resource limitations at the edge, we discuss various cutting-edge techniques, i
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#31995;&#32479;&#35843;&#26597;&#20102;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#23545;&#24213;&#23618;&#30828;&#20214;&#25925;&#38556;&#30340;&#38887;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2309.16733</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#30340;&#38887;&#24615;&#65306;&#20998;&#26512;&#21644;&#21152;&#22266;&#25216;&#26415;&#30340;&#31995;&#32479;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Resilience of Deep Learning applications: a systematic survey of analysis and hardening techniques. (arXiv:2309.16733v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16733
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#31995;&#32479;&#35843;&#26597;&#20102;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#23545;&#24213;&#23618;&#30828;&#20214;&#25925;&#38556;&#30340;&#38887;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30446;&#21069;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#39046;&#22495;&#65292;&#26159;&#26368;&#26377;&#25928;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#25216;&#26415;&#20043;&#19968;&#65292;&#22914;&#35270;&#35273;&#12289;&#33258;&#20027;&#31995;&#32479;&#31561;&#12290;&#36825;&#19968;&#36235;&#21183;&#20419;&#20351;&#20154;&#20204;&#23545;ML&#24212;&#29992;&#22312;&#24213;&#23618;&#30828;&#20214;&#25925;&#38556;&#24433;&#21709;&#19979;&#30340;&#20998;&#26512;&#21644;&#35774;&#35745;&#20570;&#20986;&#20102;&#22823;&#37327;&#36129;&#29486;&#12290;&#26412;&#25991;&#36890;&#36807;&#19968;&#27425;&#28145;&#20837;&#30340;&#22238;&#39038;&#31995;&#32479;&#22320;&#35843;&#26597;&#20102;&#20851;&#20110;&#28145;&#24230;&#23398;&#20064;&#65288;ML&#25216;&#26415;&#20043;&#19968;&#65289;&#23545;&#25239;&#30828;&#20214;&#25925;&#38556;&#30340;&#38887;&#24615;&#30340;&#24050;&#26377;&#30693;&#35782;&#65292;&#28165;&#26224;&#22320;&#21576;&#29616;&#20102;&#36825;&#19968;&#25991;&#29486;&#27969;&#30340;&#20248;&#28857;&#21644;&#32570;&#28857;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#25991;&#31456;&#22522;&#20110;2019&#24180;1&#26376;&#33267;2023&#24180;3&#26376;&#38388;&#21457;&#34920;&#30340;163&#31687;&#31185;&#23398;&#35770;&#25991;&#65292;&#37319;&#29992;&#20998;&#31867;&#26694;&#26550;&#26469;&#35299;&#35835;&#21644;&#31361;&#20986;&#30740;&#31350;&#30340;&#30456;&#20284;&#20043;&#22788;&#21644;&#29305;&#28857;&#65292;&#20174;&#24037;&#20316;&#30340;&#20027;&#35201;&#33539;&#22260;&#12289;&#37319;&#29992;&#30340;&#25925;&#38556;&#21644;&#38169;&#35823;&#27169;&#22411;&#31561;&#22810;&#20010;&#21442;&#25968;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Learning (ML) is currently being exploited in numerous applications being one of the most effective Artificial Intelligence (AI) technologies, used in diverse fields, such as vision, autonomous systems, and alike. The trend motivated a significant amount of contributions to the analysis and design of ML applications against faults affecting the underlying hardware. The authors investigate the existing body of knowledge on Deep Learning (among ML techniques) resilience against hardware faults systematically through a thoughtful review in which the strengths and weaknesses of this literature stream are presented clearly and then future avenues of research are set out. The review is based on 163 scientific articles published between January 2019 and March 2023. The authors adopt a classifying framework to interpret and highlight research similarities and peculiarities, based on several parameters, starting from the main scope of the work, the adopted fault and error models, to the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20223;&#30495;&#39537;&#21160;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;(SimPINNs)&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22686;&#24378;&#38750;&#32447;&#24615;&#21453;&#38382;&#39064;&#30340;&#24615;&#33021;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#21644;&#34701;&#21512;&#35266;&#27979;&#25968;&#25454;&#21644;&#20223;&#30495;&#25968;&#25454;&#30340;&#28151;&#21512;&#25439;&#22833;&#20989;&#25968;&#26469;&#25512;&#26029;&#25511;&#21046;&#29289;&#29702;&#31995;&#32479;&#30340;&#26410;&#30693;&#21442;&#25968;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#36712;&#36947;&#22797;&#20301;&#38382;&#39064;&#19978;&#65292;SimPINNs&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#20248;&#20110;&#26631;&#20934;PINNs&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2309.16729</link><description>&lt;p&gt;
SimPINNs: &#29992;&#20110;&#22686;&#24378;&#38750;&#32447;&#24615;&#21453;&#38382;&#39064;&#24615;&#33021;&#30340;&#22522;&#20110;&#20223;&#30495;&#39537;&#21160;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
SimPINNs: Simulation-Driven Physics-Informed Neural Networks for Enhanced Performance in Nonlinear Inverse Problems. (arXiv:2309.16729v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16729
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20223;&#30495;&#39537;&#21160;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;(SimPINNs)&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22686;&#24378;&#38750;&#32447;&#24615;&#21453;&#38382;&#39064;&#30340;&#24615;&#33021;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#21644;&#34701;&#21512;&#35266;&#27979;&#25968;&#25454;&#21644;&#20223;&#30495;&#25968;&#25454;&#30340;&#28151;&#21512;&#25439;&#22833;&#20989;&#25968;&#26469;&#25512;&#26029;&#25511;&#21046;&#29289;&#29702;&#31995;&#32479;&#30340;&#26410;&#30693;&#21442;&#25968;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#36712;&#36947;&#22797;&#20301;&#38382;&#39064;&#19978;&#65292;SimPINNs&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#20248;&#20110;&#26631;&#20934;PINNs&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#35299;&#20915;&#21453;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#12290;&#30446;&#26631;&#26159;&#26681;&#25454;&#35266;&#27979;&#25968;&#25454;&#25512;&#26029;&#25511;&#21046;&#29289;&#29702;&#31995;&#32479;&#30340;&#26410;&#30693;&#21442;&#25968;&#12290;&#25105;&#20204;&#20851;&#27880;&#22312;&#28508;&#22312;&#30340;&#27491;&#21521;&#27169;&#22411;&#34920;&#29616;&#20986;&#26174;&#33879;&#38750;&#32447;&#24615;&#34892;&#20026;&#19988;&#26410;&#30693;&#21442;&#25968;&#31354;&#38388;&#30340;&#32500;&#24230;&#26126;&#26174;&#23567;&#20110;&#35266;&#27979;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;(PINNs)&#65292;&#36890;&#36807;&#19968;&#20010;&#28151;&#21512;&#25439;&#22833;&#20989;&#25968;&#35757;&#32451;&#65292;&#35813;&#25439;&#22833;&#20989;&#25968;&#32467;&#21512;&#20102;&#35266;&#27979;&#25968;&#25454;&#21644;&#30001;&#24050;&#30693;(&#36817;&#20284;)&#29289;&#29702;&#27169;&#22411;&#29983;&#25104;&#30340;&#20223;&#30495;&#25968;&#25454;&#12290;&#22312;&#36712;&#36947;&#22797;&#20301;&#38382;&#39064;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36229;&#36807;&#20102;&#26631;&#20934;PINNs&#30340;&#24615;&#33021;&#65292;&#25552;&#20379;&#20102;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel approach to solve inverse problems by leveraging deep learning techniques. The objective is to infer unknown parameters that govern a physical system based on observed data. We focus on scenarios where the underlying forward model demonstrates pronounced nonlinear behaviour, and where the dimensionality of the unknown parameter space is substantially smaller than that of the observations. Our proposed method builds upon physics-informed neural networks (PINNs) trained with a hybrid loss function that combines observed data with simulated data generated by a known (approximate) physical model. Experimental results on an orbit restitution problem demonstrate that our approach surpasses the performance of standard PINNs, providing improved accuracy and robustness.
&lt;/p&gt;</description></item><item><title>GPT-Lab&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;GPT&#27169;&#22411;&#36171;&#20104;&#26426;&#22120;&#20154;&#20154;&#31867;&#33324;&#26234;&#33021;&#30340;&#33539;&#24335;&#65292;&#20854;&#36890;&#36807;&#25366;&#25496;&#25991;&#29486;&#12289;&#20998;&#26512;&#26448;&#26009;&#21644;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#39640;&#36890;&#37327;&#21512;&#25104;&#39564;&#35777;&#32467;&#26524;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#26448;&#26009;&#21457;&#29616;&#21644;&#39564;&#35777;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.16721</link><description>&lt;p&gt;
GPT-Lab: &#21033;&#29992;GPT&#39537;&#21160;&#30340;&#26426;&#22120;&#20154;&#23454;&#39564;&#23460;&#23454;&#29616;&#19979;&#19968;&#20195;&#26368;&#20248;&#21270;&#23398;&#21270;&#23398;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
GPT-Lab: Next Generation Of Optimal Chemistry Discovery By GPT Driven Robotic Lab. (arXiv:2309.16721v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16721
&lt;/p&gt;
&lt;p&gt;
GPT-Lab&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;GPT&#27169;&#22411;&#36171;&#20104;&#26426;&#22120;&#20154;&#20154;&#31867;&#33324;&#26234;&#33021;&#30340;&#33539;&#24335;&#65292;&#20854;&#36890;&#36807;&#25366;&#25496;&#25991;&#29486;&#12289;&#20998;&#26512;&#26448;&#26009;&#21644;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#39640;&#36890;&#37327;&#21512;&#25104;&#39564;&#35777;&#32467;&#26524;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#26448;&#26009;&#21457;&#29616;&#21644;&#39564;&#35777;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#26426;&#22120;&#20154;&#25972;&#21512;&#21040;&#21270;&#23398;&#23454;&#39564;&#20013;&#25552;&#39640;&#20102;&#23454;&#39564;&#25928;&#29575;&#65292;&#20294;&#30001;&#20110;&#32570;&#20047;&#29702;&#35299;&#25991;&#29486;&#30340;&#20154;&#31867;&#26234;&#33021;&#65292;&#23427;&#20204;&#24456;&#23569;&#22312;&#23454;&#39564;&#35774;&#35745;&#20013;&#25552;&#20379;&#24110;&#21161;&#12290;&#22240;&#27492;&#65292;&#23454;&#29616;&#20174;&#23454;&#39564;&#35774;&#35745;&#21040;&#39564;&#35777;&#30340;&#20840;&#36807;&#31243;&#33258;&#20027;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#24341;&#20837;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;&#65288;GPT&#65289;&#65292;&#29305;&#21035;&#26159;GPT-4&#65292;&#21040;&#26426;&#22120;&#20154;&#23454;&#39564;&#20013;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;GPT-Lab&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;GPT&#27169;&#22411;&#36171;&#20104;&#26426;&#22120;&#20154;&#20154;&#31867;&#33324;&#26234;&#33021;&#30340;&#33539;&#24335;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#26426;&#22120;&#20154;&#23454;&#39564;&#24179;&#21488;&#65292;GPT-Lab&#20174;&#25991;&#29486;&#20013;&#25366;&#25496;&#26448;&#26009;&#21644;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#39640;&#36890;&#37327;&#21512;&#25104;&#39564;&#35777;&#32467;&#26524;&#12290;&#20316;&#20026;&#31034;&#33539;&#65292;GPT-Lab&#20998;&#26512;&#20102;500&#31687;&#25991;&#31456;&#65292;&#37492;&#23450;&#20986;18&#31181;&#28508;&#22312;&#35797;&#21058;&#65292;&#24182;&#25104;&#21151;&#21046;&#22791;&#20986;&#20855;&#26377;&#22343;&#26041;&#26681;&#35823;&#24046;&#65288;RMSE&#65289;&#20026;2.68&#65285;&#30340;&#20934;&#30830;&#28287;&#24230;&#27604;&#33394;&#20256;&#24863;&#22120;&#12290;&#36825;&#23637;&#31034;&#20102;&#25105;&#20204;&#31995;&#32479;&#24555;&#36895;&#26448;&#26009;&#21457;&#29616;&#21644;&#39564;&#35777;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The integration of robots in chemical experiments has enhanced experimental efficiency, but lacking the human intelligence to comprehend literature, they seldom provide assistance in experimental design. Therefore, achieving full-process autonomy from experiment design to validation in self-driven laboratories (SDL) remains a challenge. The introduction of Generative Pre-trained Transformers (GPT), particularly GPT-4, into robotic experimentation offers a solution. We introduce GPT-Lab, a paradigm that employs GPT models to give robots human-like intelligence. With our robotic experimentation platform, GPT-Lab mines literature for materials and methods and validates findings through high-throughput synthesis. As a demonstration, GPT-Lab analyzed 500 articles, identified 18 potential reagents, and successfully produced an accurate humidity colorimetric sensor with a root mean square error (RMSE) of 2.68%. This showcases the rapid materials discovery and validation potential of our syste
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#28151;&#21512;&#20132;&#36890;&#20013;&#23454;&#29616;&#23433;&#20840;&#33258;&#20027;&#24615;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#20449;&#24687;&#20849;&#20139;&#26816;&#27979;&#20154;&#31867;&#39550;&#39542;&#21592;&#30340;&#19981;&#21487;&#39044;&#27979;&#24322;&#24120;&#34892;&#20026;&#65292;&#25552;&#39640;&#20102;&#36712;&#36857;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#33021;&#24555;&#36895;&#26816;&#27979;&#24322;&#24120;&#30340;&#20154;&#31867;&#39550;&#39542;&#27169;&#24335;&#20999;&#25442;&#12290;</title><link>http://arxiv.org/abs/2309.16716</link><description>&lt;p&gt;
&#22312;&#28151;&#21512;&#20132;&#36890;&#20013;&#23454;&#29616;&#23433;&#20840;&#33258;&#20027;&#24615;: &#36890;&#36807;&#20449;&#24687;&#20849;&#20139;&#26816;&#27979;&#20154;&#31867;&#39550;&#39542;&#21592;&#30340;&#19981;&#21487;&#39044;&#27979;&#24322;&#24120;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Towards Safe Autonomy in Hybrid Traffic: Detecting Unpredictable Abnormal Behaviors of Human Drivers via Information Sharing. (arXiv:2309.16716v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16716
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#28151;&#21512;&#20132;&#36890;&#20013;&#23454;&#29616;&#23433;&#20840;&#33258;&#20027;&#24615;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#20449;&#24687;&#20849;&#20139;&#26816;&#27979;&#20154;&#31867;&#39550;&#39542;&#21592;&#30340;&#19981;&#21487;&#39044;&#27979;&#24322;&#24120;&#34892;&#20026;&#65292;&#25552;&#39640;&#20102;&#36712;&#36857;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#33021;&#24555;&#36895;&#26816;&#27979;&#24322;&#24120;&#30340;&#20154;&#31867;&#39550;&#39542;&#27169;&#24335;&#20999;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#20132;&#36890;&#21363;&#21253;&#25324;&#33258;&#20027;&#36710;&#36742;&#21644;&#20154;&#39550;&#39542;&#36710;&#36742;&#65292;&#23558;&#22312;&#19968;&#27573;&#26102;&#38388;&#20869;&#25104;&#20026;&#33258;&#20027;&#36710;&#36742;&#23454;&#36341;&#30340;&#24120;&#24577;&#12290;&#19982;&#33258;&#20027;&#36710;&#36742;&#19981;&#21516;&#65292;&#20154;&#39550;&#39542;&#36710;&#36742;&#21487;&#33021;&#34920;&#29616;&#20986;&#31361;&#28982;&#30340;&#24322;&#24120;&#34892;&#20026;&#65292;&#22914;&#19981;&#21487;&#39044;&#27979;&#22320;&#20999;&#25442;&#21040;&#21361;&#38505;&#39550;&#39542;&#27169;&#24335;&#65292;&#23558;&#20854;&#21608;&#22260;&#30340;&#36710;&#36742;&#32622;&#20110;&#39118;&#38505;&#20043;&#20013;&#65307;&#36825;&#31181;&#19981;&#24076;&#26395;&#30340;&#27169;&#24335;&#20999;&#25442;&#21487;&#33021;&#26469;&#33258;&#22810;&#31181;&#20154;&#31867;&#39550;&#39542;&#21592;&#22240;&#32032;&#65292;&#21253;&#25324;&#30130;&#21171;&#12289;&#37202;&#21518;&#39550;&#39542;&#12289;&#20998;&#24515;&#12289;&#25915;&#20987;&#24615;&#31561;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#29616;&#20195;&#36710;&#36742;&#38388;&#36890;&#20449;&#25216;&#26415;&#20351;&#24471;&#33258;&#20027;&#36710;&#36742;&#33021;&#22815;&#26377;&#25928;&#21644;&#21487;&#38752;&#22320;&#20849;&#20139;&#31232;&#32570;&#30340;&#23454;&#26102;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#30446;&#21069;&#24050;&#30693;&#26368;&#26377;&#25928;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#65288;1&#65289;&#36890;&#36807;&#26377;&#25928;&#34701;&#21512;&#21608;&#22260;&#33258;&#20027;&#36710;&#36742;&#20849;&#20139;&#30340;&#23454;&#26102;&#20449;&#24687;&#65292;&#26174;&#33879;&#25552;&#39640;&#36712;&#36857;&#39044;&#27979;&#65307;&#65288;2&#65289;&#20934;&#30830;&#24555;&#36895;&#22320;&#26816;&#27979;&#24322;&#24120;&#30340;&#20154;&#31867;&#39550;&#39542;&#27169;&#24335;&#20999;&#25442;&#25110;...
&lt;/p&gt;
&lt;p&gt;
Hybrid traffic which involves both autonomous and human-driven vehicles would be the norm of the autonomous vehicles practice for a while. On the one hand, unlike autonomous vehicles, human-driven vehicles could exhibit sudden abnormal behaviors such as unpredictably switching to dangerous driving modes, putting its neighboring vehicles under risks; such undesired mode switching could arise from numbers of human driver factors, including fatigue, drunkenness, distraction, aggressiveness, etc. On the other hand, modern vehicle-to-vehicle communication technologies enable the autonomous vehicles to efficiently and reliably share the scarce run-time information with each other. In this paper, we propose, to the best of our knowledge, the first efficient algorithm that can (1) significantly improve trajectory prediction by effectively fusing the run-time information shared by surrounding autonomous vehicles, and can (2) accurately and quickly detect abnormal human driving mode switches or 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MV-DeepSDF&#30340;&#26032;&#26694;&#26550;&#65292;&#21033;&#29992;&#22810;&#27425;&#25195;&#25551;&#30340;&#28857;&#20113;&#26469;&#37325;&#24314;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#19977;&#32500;&#36710;&#36742;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#20027;&#35201;&#35299;&#20915;&#20102;&#22024;&#26434;&#21644;&#31232;&#30095;&#28857;&#20113;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#22810;&#27425;&#25195;&#25551;&#30340;&#19968;&#33268;&#24615;&#21644;&#20114;&#34917;&#24615;&#26469;&#25552;&#39640;&#37325;&#24314;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2309.16715</link><description>&lt;p&gt;
MV-DeepSDF&#65306;&#20351;&#29992;&#22810;&#27425;&#25195;&#25551;&#30340;&#28857;&#20113;&#36827;&#34892;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#19977;&#32500;&#36710;&#36742;&#37325;&#24314;&#30340;&#38544;&#24335;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
MV-DeepSDF: Implicit Modeling with Multi-Sweep Point Clouds for 3D Vehicle Reconstruction in Autonomous Driving. (arXiv:2309.16715v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16715
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MV-DeepSDF&#30340;&#26032;&#26694;&#26550;&#65292;&#21033;&#29992;&#22810;&#27425;&#25195;&#25551;&#30340;&#28857;&#20113;&#26469;&#37325;&#24314;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#19977;&#32500;&#36710;&#36742;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#20027;&#35201;&#35299;&#20915;&#20102;&#22024;&#26434;&#21644;&#31232;&#30095;&#28857;&#20113;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#22810;&#27425;&#25195;&#25551;&#30340;&#19968;&#33268;&#24615;&#21644;&#20114;&#34917;&#24615;&#26469;&#25552;&#39640;&#37325;&#24314;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#22024;&#26434;&#21644;&#31232;&#30095;&#30340;&#37096;&#20998;&#28857;&#20113;&#20013;&#37325;&#24314;&#19977;&#32500;&#36710;&#36742;&#23545;&#20110;&#33258;&#21160;&#39550;&#39542;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#19977;&#32500;&#37325;&#24314;&#26041;&#27861;&#19981;&#33021;&#30452;&#25509;&#24212;&#29992;&#20110;&#36825;&#20010;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#22788;&#29702;&#24102;&#26377;&#24494;&#23567;&#22122;&#22768;&#30340;&#23494;&#38598;&#36755;&#20837;&#26102;&#32463;&#36807;&#31934;&#24515;&#35774;&#35745;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;MV-DeepSDF&#65292;&#23427;&#20174;&#22810;&#27425;&#25195;&#25551;&#30340;&#28857;&#20113;&#20013;&#20272;&#35745;&#26368;&#20248;&#30340;&#31526;&#21495;&#36317;&#31163;&#20989;&#25968;(SDF)&#34920;&#31034;&#65292;&#20197;&#37325;&#24314;&#37326;&#22806;&#36710;&#36742;&#12290;&#23613;&#31649;&#24050;&#32463;&#23384;&#22312;&#19968;&#20123;&#22522;&#20110;SDF&#30340;&#38544;&#24335;&#24314;&#27169;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#21482;&#20851;&#27880;&#21333;&#35270;&#22270;&#30340;&#37325;&#24314;&#65292;&#23548;&#33268;&#37325;&#24314;&#36136;&#37327;&#36739;&#20302;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#39318;&#20808;&#20998;&#26512;&#20102;&#28508;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#22810;&#27425;&#25195;&#25551;&#19968;&#33268;&#24615;&#21644;&#20114;&#34917;&#24615;&#65292;&#25552;&#20986;&#23558;&#38544;&#24335;&#31354;&#38388;&#24418;&#29366;&#20272;&#35745;&#38382;&#39064;&#36716;&#21270;&#20026;&#20803;&#32032;&#21040;&#38598;&#21512;&#29305;&#24449;&#25552;&#21462;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#26469;&#25552;&#21462;&#21333;&#29420;&#30340;&#20803;&#32032;&#32423;&#34920;&#31034;&#24182;&#23558;&#23427;&#20204;&#32858;&#21512;&#36215;&#26469;&#29983;&#25104;&#19968;&#20010;&#38598;&#21512;&#32423;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reconstructing 3D vehicles from noisy and sparse partial point clouds is of great significance to autonomous driving. Most existing 3D reconstruction methods cannot be directly applied to this problem because they are elaborately designed to deal with dense inputs with trivial noise. In this work, we propose a novel framework, dubbed MV-DeepSDF, which estimates the optimal Signed Distance Function (SDF) shape representation from multi-sweep point clouds to reconstruct vehicles in the wild. Although there have been some SDF-based implicit modeling methods, they only focus on single-view-based reconstruction, resulting in low fidelity. In contrast, we first analyze multi-sweep consistency and complementarity in the latent feature space and propose to transform the implicit space shape estimation problem into an element-to-set feature extraction problem. Then, we devise a new architecture to extract individual element-level representations and aggregate them to generate a set-level predic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#26080;&#20154;&#26426;&#30340;&#19978;&#34892;&#35821;&#20041;&#36890;&#20449;&#26041;&#26696;&#65292;&#36890;&#36807;&#28151;&#21512;&#21160;&#20316;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#23454;&#29616;&#20102;&#22312;&#25968;&#25454;&#25910;&#38598;&#25928;&#29575;&#21644;&#35745;&#31639;&#33021;&#37327;&#25104;&#26412;&#20043;&#38388;&#30340;&#24179;&#34913;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#21892;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.16713</link><description>&lt;p&gt;
&#26080;&#20154;&#26426;&#36741;&#21161;&#35821;&#20041;&#36890;&#20449;&#19982;&#28151;&#21512;&#21160;&#20316;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
UAV-assisted Semantic Communication with Hybrid Action Reinforcement Learning. (arXiv:2309.16713v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16713
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#26080;&#20154;&#26426;&#30340;&#19978;&#34892;&#35821;&#20041;&#36890;&#20449;&#26041;&#26696;&#65292;&#36890;&#36807;&#28151;&#21512;&#21160;&#20316;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#23454;&#29616;&#20102;&#22312;&#25968;&#25454;&#25910;&#38598;&#25928;&#29575;&#21644;&#35745;&#31639;&#33021;&#37327;&#25104;&#26412;&#20043;&#38388;&#30340;&#24179;&#34913;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#21892;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#25506;&#32034;&#21033;&#29992;&#26080;&#20154;&#26426;&#36827;&#34892;&#19978;&#34892;&#35821;&#20041;&#36890;&#20449;&#65292;&#20197;&#25552;&#39640;&#20559;&#36828;&#22320;&#21306;&#20803;&#23431;&#23449;&#29992;&#25143;&#30340;&#25968;&#25454;&#25910;&#38598;&#25928;&#29575;&#12290;&#20026;&#20102;&#22312;&#24179;&#34913;&#37325;&#24314;&#36136;&#37327;&#21644;&#35745;&#31639;&#33021;&#37327;&#25104;&#26412;&#20043;&#38388;&#20943;&#23569;&#19978;&#34892;&#25968;&#25454;&#25910;&#38598;&#26102;&#38388;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#21160;&#20316;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#35821;&#20041;&#27169;&#22411;&#35268;&#27169;&#12289;&#20449;&#36947;&#20998;&#37197;&#12289;&#20256;&#36755;&#21151;&#29575;&#21644;&#26080;&#20154;&#26426;&#36712;&#36857;&#19978;&#20570;&#20986;&#20915;&#31574;&#12290;&#21464;&#37327;&#34987;&#21010;&#20998;&#20026;&#31163;&#25955;&#31867;&#22411;&#21644;&#36830;&#32493;&#31867;&#22411;&#65292;&#24182;&#36890;&#36807;&#20004;&#20010;&#19981;&#21516;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#36827;&#34892;&#20248;&#21270;&#20197;&#29983;&#25104;&#32452;&#21512;&#21160;&#20316;&#12290;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#28151;&#21512;&#21160;&#20316;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#33021;&#22815;&#22312;&#19981;&#21516;&#21442;&#25968;&#35774;&#32622;&#19979;&#26377;&#25928;&#25552;&#39640;&#19978;&#34892;&#35821;&#20041;&#25968;&#25454;&#25910;&#38598;&#30340;&#25928;&#29575;&#65292;&#24182;&#20248;&#20110;&#22522;&#20934;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we aim to explore the use of uplink semantic communications with the assistance of UAV in order to improve data collection effiicency for metaverse users in remote areas. To reduce the time for uplink data collection while balancing the trade-off between reconstruction quality and computational energy cost, we propose a hybrid action reinforcement learning (RL) framework to make decisions on semantic model scale, channel allocation, transmission power, and UAV trajectory. The variables are classified into discrete type and continuous type, which are optimized by two different RL agents to generate the combined action. Simulation results indicate that the proposed hybrid action reinforcement learning framework can effectively improve the efficiency of uplink semantic data collection under different parameter settings and outperforms the benchmark scenarios.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#8220;&#19968;&#33324;&#26446;&#26222;&#24076;&#33576;&#65288;GL&#65289;&#8221;&#65292;&#29992;&#20110;&#23545;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#35748;&#35777;&#65292;&#20197;&#25269;&#24481;&#21487;&#32452;&#21512;&#30340;&#21487;&#35299;&#37322;&#30340;&#35821;&#20041;&#24178;&#25200;&#12290;&#26041;&#27861;&#22312;ImageNet&#25968;&#25454;&#38598;&#19978;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#34920;&#29616;&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2309.16710</link><description>&lt;p&gt;
&#36890;&#36807;&#20381;&#36182;&#20110;&#21464;&#25442;&#30340;&#38543;&#26426;&#24179;&#28369;&#65292;&#25552;&#20379;&#19968;&#33324;&#26446;&#26222;&#24076;&#33576;&#65306;&#38024;&#23545;&#21487;&#35299;&#37322;&#30340;&#35821;&#20041;&#21464;&#25442;&#30340;&#35748;&#35777;&#20581;&#22766;&#24615;
&lt;/p&gt;
&lt;p&gt;
General Lipschitz: Certified Robustness Against Resolvable Semantic Transformations via Transformation-Dependent Randomized Smoothing. (arXiv:2309.16710v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16710
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#8220;&#19968;&#33324;&#26446;&#26222;&#24076;&#33576;&#65288;GL&#65289;&#8221;&#65292;&#29992;&#20110;&#23545;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#35748;&#35777;&#65292;&#20197;&#25269;&#24481;&#21487;&#32452;&#21512;&#30340;&#21487;&#35299;&#37322;&#30340;&#35821;&#20041;&#24178;&#25200;&#12290;&#26041;&#27861;&#22312;ImageNet&#25968;&#25454;&#38598;&#19978;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#34920;&#29616;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#24179;&#28369;&#26159;&#30446;&#21069;&#26500;&#24314;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#21487;&#20197;&#35777;&#26126;&#20854;&#23545;&#20110;&#26377;&#30028;&#24178;&#25200;&#30340;&#25239;&#24615;&#12290;&#28982;&#32780;&#65292;&#26500;&#24314;&#38024;&#23545;&#35821;&#20041;&#21464;&#25442;&#65288;&#20363;&#22914;&#65292;&#22270;&#20687;&#27169;&#31946;&#12289;&#24179;&#31227;&#12289;Gamma&#30699;&#27491;&#65289;&#21450;&#20854;&#32452;&#21512;&#30340;&#21512;&#29702;&#35777;&#20070;&#26356;&#20026;&#22797;&#26434;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#8220;&#19968;&#33324;&#26446;&#26222;&#24076;&#33576;&#65288;GL&#65289;&#8221;&#65292;&#29992;&#20110;&#23545;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#35748;&#35777;&#65292;&#20197;&#25269;&#24481;&#21487;&#32452;&#21512;&#30340;&#21487;&#35299;&#37322;&#30340;&#35821;&#20041;&#24178;&#25200;&#12290;&#22312;&#35813;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#24179;&#28369;&#20998;&#31867;&#22120;&#19982;&#21464;&#25442;&#21442;&#25968;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#25512;&#23548;&#20986;&#30456;&#24212;&#30340;&#20581;&#22766;&#24615;&#35777;&#20070;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;ImageNet&#25968;&#25454;&#38598;&#19978;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#34920;&#29616;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Randomized smoothing is the state-of-the-art approach to construct image classifiers that are provably robust against additive adversarial perturbations of bounded magnitude. However, it is more complicated to construct reasonable certificates against semantic transformation (e.g., image blurring, translation, gamma correction) and their compositions. In this work, we propose \emph{General Lipschitz (GL),} a new framework to certify neural networks against composable resolvable semantic perturbations. Within the framework, we analyze transformation-dependent Lipschitz-continuity of smoothed classifiers w.r.t. transformation parameters and derive corresponding robustness certificates. Our method performs comparably to state-of-the-art approaches on the ImageNet dataset.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#23545;&#25239;&#24773;&#20917;&#19979;&#65292;&#28145;&#24230;&#23398;&#20064;&#20449;&#24687;&#24674;&#22797;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#23545;&#26368;&#20808;&#36827;&#30340;DeepReceiver&#27169;&#22411;&#36827;&#34892;&#23545;&#25239;&#25915;&#20987;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;DeepReceiver&#23545;&#35774;&#35745;&#30340;&#25915;&#20987;&#26041;&#27861;&#26159;&#33030;&#24369;&#30340;&#12290;</title><link>http://arxiv.org/abs/2309.16706</link><description>&lt;p&gt;
AIR: &#28145;&#24230;&#23398;&#20064;&#20449;&#24687;&#24674;&#22797;&#20013;&#23545;&#25239;&#25915;&#20987;&#30340;&#23041;&#32961;
&lt;/p&gt;
&lt;p&gt;
AIR: Threats of Adversarial Attacks on Deep Learning-Based Information Recovery. (arXiv:2309.16706v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16706
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#23545;&#25239;&#24773;&#20917;&#19979;&#65292;&#28145;&#24230;&#23398;&#20064;&#20449;&#24687;&#24674;&#22797;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#23545;&#26368;&#20808;&#36827;&#30340;DeepReceiver&#27169;&#22411;&#36827;&#34892;&#23545;&#25239;&#25915;&#20987;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;DeepReceiver&#23545;&#35774;&#35745;&#30340;&#25915;&#20987;&#26041;&#27861;&#26159;&#33030;&#24369;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#32447;&#36890;&#20449;&#31995;&#32479;&#36890;&#24120;&#30001;&#19968;&#20010;&#20256;&#36755;&#22120;&#21644;&#19968;&#20010;&#25509;&#25910;&#22120;&#32452;&#25104;&#65292;&#20256;&#36755;&#22120;&#23558;&#20449;&#24687;&#20256;&#36755;&#65292;&#25509;&#25910;&#22120;&#20174;&#25509;&#25910;&#21040;&#30340;&#25197;&#26354;&#20449;&#21495;&#20013;&#24674;&#22797;&#21407;&#22987;&#20449;&#24687;&#12290;&#28145;&#24230;&#23398;&#20064;&#24050;&#34987;&#29992;&#20110;&#25913;&#21892;&#22797;&#26434;&#20449;&#36947;&#29615;&#22659;&#19979;&#25509;&#25910;&#22120;&#30340;&#24615;&#33021;&#65292;&#24182;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20854;&#40065;&#26834;&#24615;&#23578;&#26410;&#34987;&#30740;&#31350;&#12290;&#20026;&#20102;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#20449;&#24687;&#24674;&#22797;&#27169;&#22411;&#22312;&#23545;&#25239;&#24773;&#20917;&#19979;&#30340;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23545;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#20449;&#24687;&#24674;&#22797;&#27169;&#22411;DeepReceiver&#30340;&#23545;&#25239;&#25915;&#20987;&#12290;&#25105;&#20204;&#23558;&#35813;&#38382;&#39064;&#24314;&#27169;&#20026;&#19968;&#20010;&#24102;&#26377;&#21151;&#29575;&#21644;&#23792;&#22343;&#27604; (PAPR) &#32422;&#26463;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#26681;&#25454;&#23545;&#25163;&#23545;DeepReceiver&#27169;&#22411;&#21644;/&#25110;&#27979;&#35797;&#26679;&#26412;&#30340;&#20102;&#35299;&#35774;&#35745;&#20102;&#19981;&#21516;&#30340;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#12290;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#25152;&#26377;&#32771;&#34385;&#30340;&#22330;&#26223;&#20013;&#65292;DeepReceiver&#23545;&#35774;&#35745;&#30340;&#25915;&#20987;&#26041;&#27861;&#26159;&#33030;&#24369;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
A wireless communications system usually consists of a transmitter which transmits the information and a receiver which recovers the original information from the received distorted signal. Deep learning (DL) has been used to improve the performance of the receiver in complicated channel environments and state-of-the-art (SOTA) performance has been achieved. However, its robustness has not been investigated. In order to evaluate the robustness of DL-based information recovery models under adversarial circumstances, we investigate adversarial attacks on the SOTA DL-based information recovery model, i.e., DeepReceiver. We formulate the problem as an optimization problem with power and peak-to-average power ratio (PAPR) constraints. We design different adversarial attack methods according to the adversary's knowledge of DeepReceiver's model and/or testing samples. Extensive experiments show that the DeepReceiver is vulnerable to the designed attack methods in all of the considered scenari
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22270;&#35889;&#39046;&#22495;&#20013;&#39044;&#27979;&#36710;&#36742;&#36712;&#36857;&#30340;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#65292;&#23427;&#20351;&#29992;&#20102;&#22270;&#35889;&#34920;&#31034;&#30340;&#26377;&#30410;&#29305;&#24449;&#24182;&#22312;&#24615;&#33021;&#19978;&#21462;&#24471;&#20102;&#22810;&#36798;25%&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2309.16702</link><description>&lt;p&gt;
&#22312;&#22270;&#35889;&#39046;&#22495;&#20013;&#39044;&#27979;&#21644;&#35299;&#37322;&#36710;&#36742;&#36712;&#36857;
&lt;/p&gt;
&lt;p&gt;
Prediction and Interpretation of Vehicle Trajectories in the Graph Spectral Domain. (arXiv:2309.16702v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16702
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22270;&#35889;&#39046;&#22495;&#20013;&#39044;&#27979;&#36710;&#36742;&#36712;&#36857;&#30340;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#65292;&#23427;&#20351;&#29992;&#20102;&#22270;&#35889;&#34920;&#31034;&#30340;&#26377;&#30410;&#29305;&#24449;&#24182;&#22312;&#24615;&#33021;&#19978;&#21462;&#24471;&#20102;&#22810;&#36798;25%&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#20132;&#36890;&#22330;&#26223;&#30340;&#22270;&#35889;&#34920;&#31034;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#21644;&#35299;&#37322;&#12290;&#36890;&#36807;&#22522;&#20110;&#26102;&#31354;&#36710;&#36742;&#20132;&#20114;&#22270;&#30340;&#22810;&#32500;&#22270;&#20613;&#37324;&#21494;&#21464;&#25442;&#65292;&#35266;&#27979;&#21040;&#30340;&#20132;&#36890;&#22330;&#26223;&#21487;&#20197;&#36716;&#21270;&#21040;&#22270;&#35889;&#39046;&#22495;&#12290;&#30001;&#20110;&#36825;&#20123;&#35889;&#22330;&#26223;&#34920;&#31034;&#25104;&#21151;&#22320;&#34701;&#20837;&#20102;&#20132;&#36890;&#22330;&#26223;&#30340;&#22797;&#26434;&#21644;&#20132;&#20114;&#24615;&#36136;&#65292;&#22240;&#27492;&#36825;&#31181;&#26377;&#30410;&#30340;&#29305;&#24449;&#34920;&#31034;&#34987;&#29992;&#20110;&#39044;&#27979;&#36710;&#36742;&#36712;&#36857;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;GFTNNv2&#65292;&#19968;&#31181;&#22312;&#22270;&#35889;&#39046;&#22495;&#20013;&#39044;&#27979;&#36710;&#36742;&#36712;&#36857;&#30340;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#12290;&#22312;&#20844;&#24320;&#21487;&#29992;&#30340;&#39640;&#28165;&#25968;&#25454;&#38598;&#21644;NGSIM&#19978;&#35780;&#20272;&#20102;GFTNNv2&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#39044;&#27979;&#26041;&#27861;&#30456;&#27604;&#65292;&#24615;&#33021;&#25552;&#21319;&#20102;&#22810;&#36798;25%&#12290;
&lt;/p&gt;
&lt;p&gt;
This work provides a comprehensive analysis and interpretation of the graph spectral representation of traffic scenarios. Based on a spatio-temporal vehicle interaction graph, an observed traffic scenario can be transformed into the graph spectral domain by means of the multidimensional Graph Fourier Transformation. Since these spectral scenario representations have shown to successfully incorporate the complex and interactive nature of traffic scenarios, the beneficial feature representation is employed for the purpose of predicting vehicle trajectories. This work introduces GFTNNv2, a deep learning network predicting vehicle trajectories in the graph spectral domain. Evaluation of the GFTNNv2 on the publicly available datasets highD and NGSIM shows a performance gain of up to 25% in comparison to state-of-the-art prediction approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MVMR&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#32473;&#23450;&#25991;&#26412;&#26597;&#35810;&#20174;&#22823;&#37327;&#35270;&#39057;&#38598;&#20013;&#23450;&#20301;&#35270;&#39057;&#24103;&#12290;&#25105;&#20204;&#36890;&#36807;&#24050;&#26377;&#25968;&#25454;&#38598;&#36827;&#34892;&#30456;&#20284;&#24615;&#31579;&#36873;&#26469;&#26500;&#24314;&#25968;&#25454;&#38598;&#65292;&#24182;&#24341;&#20837;&#19977;&#20010;MVMR&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#23884;&#20837;&#24335;&#25991;&#26412;&#30456;&#20284;&#24230;&#21305;&#37197;&#21644;&#35270;&#39057;-&#35821;&#35328;&#23545;&#40784;&#25216;&#26415;&#26469;&#35745;&#31639;&#30456;&#20851;&#24615;&#24471;&#20998;&#65292;&#24182;&#20026;MVMR&#20219;&#21153;&#24320;&#21457;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#27169;&#22411;&#65292;Reliable Mutual Matching Network (RMMN)&#12290;</title><link>http://arxiv.org/abs/2309.16701</link><description>&lt;p&gt;
MVMR: &#22312;&#22810;&#20010;&#21487;&#38752;&#35270;&#39057;&#38598;&#20013;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#35270;&#39057;&#23450;&#20301;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
MVMR: Evaluating Natural Language Video Localization Bias over Multiple Reliable Videos Pool. (arXiv:2309.16701v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16701
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MVMR&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#32473;&#23450;&#25991;&#26412;&#26597;&#35810;&#20174;&#22823;&#37327;&#35270;&#39057;&#38598;&#20013;&#23450;&#20301;&#35270;&#39057;&#24103;&#12290;&#25105;&#20204;&#36890;&#36807;&#24050;&#26377;&#25968;&#25454;&#38598;&#36827;&#34892;&#30456;&#20284;&#24615;&#31579;&#36873;&#26469;&#26500;&#24314;&#25968;&#25454;&#38598;&#65292;&#24182;&#24341;&#20837;&#19977;&#20010;MVMR&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#23884;&#20837;&#24335;&#25991;&#26412;&#30456;&#20284;&#24230;&#21305;&#37197;&#21644;&#35270;&#39057;-&#35821;&#35328;&#23545;&#40784;&#25216;&#26415;&#26469;&#35745;&#31639;&#30456;&#20851;&#24615;&#24471;&#20998;&#65292;&#24182;&#20026;MVMR&#20219;&#21153;&#24320;&#21457;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#27169;&#22411;&#65292;Reliable Mutual Matching Network (RMMN)&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36817;&#24180;&#26469;&#22810;&#23186;&#20307;&#20869;&#23481;&#30340;&#28608;&#22686;&#65292;&#33258;&#28982;&#35821;&#35328;&#35270;&#39057;&#23450;&#20301;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#23427;&#33268;&#21147;&#20110;&#26816;&#27979;&#19982;&#32473;&#23450;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#21305;&#37197;&#30340;&#35270;&#39057;&#29255;&#27573;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#30740;&#31350;&#37117;&#27809;&#26377;&#25506;&#32034;&#22312;&#23384;&#22312;&#22810;&#20010;&#27491;&#36127;&#35270;&#39057;&#30340;&#22823;&#37327;&#35821;&#26009;&#24211;&#20013;&#23450;&#20301;&#19968;&#20010;&#26102;&#21051;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MVMR&#65288;Massive Videos Moment Retrieval&#65289;&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#32473;&#23450;&#25991;&#26412;&#26597;&#35810;&#20174;&#22823;&#37327;&#35270;&#39057;&#38598;&#20013;&#23450;&#20301;&#35270;&#39057;&#24103;&#12290;&#23545;&#20110;&#36825;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23545;&#29616;&#26377;&#35270;&#39057;&#23450;&#20301;&#25968;&#25454;&#38598;&#36827;&#34892;&#30456;&#20284;&#24615;&#31579;&#36873;&#26469;&#26500;&#24314;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19977;&#20010;MVMR&#25968;&#25454;&#38598;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;&#23884;&#20837;&#30340;&#25991;&#26412;&#30456;&#20284;&#24230;&#21305;&#37197;&#21644;&#35270;&#39057;-&#35821;&#35328;&#23545;&#40784;&#25216;&#26415;&#26469;&#35745;&#31639;&#30446;&#26631;&#26597;&#35810;&#19982;&#35270;&#39057;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#24471;&#20998;&#65292;&#20174;&#32780;&#23450;&#20041;&#27491;&#36127;&#38598;&#12290;&#38024;&#23545;&#25552;&#20986;&#30340;MVMR&#20219;&#21153;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#24320;&#21457;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#27169;&#22411;&#65292;Reliable Mutual Matching Network (RMMN)&#12290;
&lt;/p&gt;
&lt;p&gt;
With the explosion of multimedia content in recent years, natural language video localization, which focuses on detecting video moment that matches a given natural language query, has become a critical problem. However, none of the previous research explores localizing a moment from a large corpus where multiple positive and negative videos exist. In this paper, we propose an MVMR (Massive Videos Moment Retrieval) task, which aims to localize video frames from a massive set of videos given a text query. For this task, we suggest methods for constructing datasets by employing similarity filtering on the existing video localization datasets and introduce three MVMR datasets. Specifically, we employ embedding-based text similarity matching and video-language grounding techniques to calculate the relevance score between a target query and videos to define positive and negative sets. For the proposed MVMR task, we further develop a strong model, Reliable Mutual Matching Network (RMMN), whic
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#21463;&#25511;&#23454;&#39564;&#21457;&#29616;&#65292;ChatGPT&#22312;&#32534;&#31243;&#27979;&#35797;&#20013;&#34987;&#28389;&#29992;&#21487;&#33021;&#23548;&#33268;&#38750;&#27491;&#24403;&#21033;&#30410;&#21644;&#25220;&#34989;&#34892;&#20026;&#65292;&#24182;&#25552;&#20986;&#20102;&#25163;&#21160;&#35782;&#21035;ChatGPT&#36741;&#21161;&#31243;&#24207;&#21644;&#23398;&#29983;&#23545;ChatGPT&#30340;&#30475;&#27861;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#20351;&#29992;ChatGPT&#30340;&#23398;&#29983;&#23436;&#25104;&#32534;&#31243;&#27979;&#35797;&#30340;&#36895;&#24230;&#26159;&#27809;&#26377;&#20351;&#29992;ChatGPT&#30340;&#23398;&#29983;&#30340;&#20004;&#20493;&#12290;</title><link>http://arxiv.org/abs/2309.16697</link><description>&lt;p&gt;
&#22312;&#32534;&#31243;&#27979;&#35797;&#20013;&#38750;&#27491;&#24403;&#21033;&#30410;&#21644;ChatGPT&#28389;&#29992;&#30340;&#35782;&#21035;&#65306;&#19968;&#20010;&#21463;&#25511;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
Inappropriate Benefits and Identification of ChatGPT Misuse in Programming Tests: A Controlled Experiment. (arXiv:2309.16697v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16697
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#21463;&#25511;&#23454;&#39564;&#21457;&#29616;&#65292;ChatGPT&#22312;&#32534;&#31243;&#27979;&#35797;&#20013;&#34987;&#28389;&#29992;&#21487;&#33021;&#23548;&#33268;&#38750;&#27491;&#24403;&#21033;&#30410;&#21644;&#25220;&#34989;&#34892;&#20026;&#65292;&#24182;&#25552;&#20986;&#20102;&#25163;&#21160;&#35782;&#21035;ChatGPT&#36741;&#21161;&#31243;&#24207;&#21644;&#23398;&#29983;&#23545;ChatGPT&#30340;&#30475;&#27861;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#20351;&#29992;ChatGPT&#30340;&#23398;&#29983;&#23436;&#25104;&#32534;&#31243;&#27979;&#35797;&#30340;&#36895;&#24230;&#26159;&#27809;&#26377;&#20351;&#29992;ChatGPT&#30340;&#23398;&#29983;&#30340;&#20004;&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;ChatGPT&#21487;&#20197;&#24110;&#21161;&#23398;&#29983;&#23398;&#20064;&#32534;&#31243;&#65292;&#20294;&#23427;&#20063;&#21487;&#33021;&#34987;&#28389;&#29992;&#26469;&#36827;&#34892;&#25220;&#34989;&#65292;&#36825;&#26159;&#36829;&#21453;&#23398;&#26415;&#35802;&#20449;&#30340;&#34892;&#20026;&#12290;&#23398;&#29983;&#21487;&#20197;&#35201;&#27714;ChatGPT&#23436;&#25104;&#32534;&#31243;&#20219;&#21153;&#65292;&#24182;&#20174;&#20182;&#20154;&#30340;&#20316;&#21697;&#20013;&#29983;&#25104;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#21364;&#26410;&#33021;&#27491;&#30830;&#33268;&#35874;&#26469;&#28304;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#31181;&#26032;&#22411;&#30340;&#25220;&#34989;&#34892;&#20026;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#21463;&#25511;&#23454;&#39564;&#65292;&#20197;&#34913;&#37327;&#20351;&#29992;ChatGPT&#30340;&#38750;&#27491;&#24403;&#21033;&#30410;&#65292;&#21253;&#25324;&#23436;&#25104;&#26102;&#38388;&#21644;&#32534;&#31243;&#34920;&#29616;&#12290;&#25105;&#20204;&#36824;&#25253;&#21578;&#20102;&#22914;&#20309;&#36890;&#36807;&#23398;&#29983;&#22312;&#20351;&#29992;ChatGPT&#26102;&#30340;&#34892;&#20026;&#21644;&#36890;&#36807;&#35843;&#26597;&#20102;&#35299;&#23398;&#29983;&#23545;ChatGPT&#30340;&#30475;&#27861;&#26469;&#25163;&#21160;&#35782;&#21035;&#36890;&#36807;ChatGPT&#36741;&#21161;&#23436;&#25104;&#30340;&#31243;&#24207;&#12290;&#23454;&#39564;&#20013;&#26377;17&#21517;&#23398;&#29983;&#21442;&#19982;&#65292;&#20182;&#20204;&#34987;&#35201;&#27714;&#23436;&#25104;&#20004;&#20010;&#32534;&#31243;&#27979;&#35797;&#65292;&#26681;&#25454;&#27979;&#35797;&#34987;&#20998;&#20026;&#20004;&#32452;&#65306;&#19968;&#32452;&#26080;&#38656;&#24110;&#21161;&#23436;&#25104;&#27979;&#35797;&#65292;&#21478;&#19968;&#32452;&#21487;&#20351;&#29992;ChatGPT&#36741;&#21161;&#23436;&#25104;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;ChatGPT&#30340;&#23398;&#29983;&#23436;&#25104;&#32534;&#31243;&#27979;&#35797;&#30340;&#36895;&#24230;&#26159;&#27809;&#26377;&#20351;&#29992;ChatGPT&#30340;&#23398;&#29983;&#30340;&#20004;&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
While ChatGPT may help students to learn to program, it can be misused to do plagiarism, a breach of academic integrity. Students can ask ChatGPT to complete a programming task, generating a solution from other people's work without proper acknowledgment of the source(s). To help address this new kind of plagiarism, we performed a controlled experiment measuring the inappropriate benefits of using ChatGPT in terms of completion time and programming performance. We also reported how to manually identify programs aided with ChatGPT (via student behavior while using ChatGPT) and student perspective of ChatGPT (via a survey). Seventeen students participated in the experiment. They were asked to complete two programming tests. They were divided into two groups per the test: one group should complete the test without help while the other group should complete it with ChatGPT. Our study shows that students with ChatGPT complete programming tests two times faster than those without ChatGPT, th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#19968;&#31181;&#35268;&#33539;&#26041;&#27861;&#65292;&#30456;&#20284;&#24615;&#21305;&#37197;&#65292;&#29992;&#20110;&#25512;&#23548;&#21644;&#29702;&#35299;&#31070;&#32463;&#35745;&#31639;&#30340;&#31639;&#27861;&#22522;&#30784;&#65292;&#24182;&#19988;&#21457;&#29616;&#34920;&#31034;&#23450;&#29702;&#26159;&#30740;&#31350;&#29983;&#29289;&#21512;&#29702;&#23398;&#20064;&#31639;&#27861;&#30340;&#37325;&#35201;&#35270;&#35282;&#12290;</title><link>http://arxiv.org/abs/2309.16687</link><description>&lt;p&gt;
&#23545;&#20598;&#21407;&#29702;&#21644;&#29983;&#29289;&#21512;&#29702;&#23398;&#20064;&#65306;&#36830;&#25509;&#34920;&#31034;&#23450;&#29702;&#21644;Hebbian&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Duality Principle and Biologically Plausible Learning: Connecting the Representer Theorem and Hebbian Learning. (arXiv:2309.16687v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16687
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#19968;&#31181;&#35268;&#33539;&#26041;&#27861;&#65292;&#30456;&#20284;&#24615;&#21305;&#37197;&#65292;&#29992;&#20110;&#25512;&#23548;&#21644;&#29702;&#35299;&#31070;&#32463;&#35745;&#31639;&#30340;&#31639;&#27861;&#22522;&#30784;&#65292;&#24182;&#19988;&#21457;&#29616;&#34920;&#31034;&#23450;&#29702;&#26159;&#30740;&#31350;&#29983;&#29289;&#21512;&#29702;&#23398;&#20064;&#31639;&#27861;&#30340;&#37325;&#35201;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#24341;&#20837;&#20102;&#19968;&#31181;&#21483;&#20570;&#30456;&#20284;&#24615;&#21305;&#37197;&#30340;&#35268;&#33539;&#26041;&#27861;&#65292;&#29992;&#20110;&#25512;&#23548;&#21644;&#29702;&#35299;&#31070;&#32463;&#35745;&#31639;&#30340;&#31639;&#27861;&#22522;&#30784;&#65292;&#30528;&#37325;&#20110;&#26080;&#30417;&#30563;&#38382;&#39064;&#12290;&#23427;&#28041;&#21450;&#20174;&#35745;&#31639;&#30446;&#26631;&#20013;&#25512;&#23548;&#31639;&#27861;&#65292;&#24182;&#35780;&#20272;&#20854;&#19982;&#35299;&#21078;&#21644;&#29983;&#29702;&#35266;&#23519;&#30340;&#20860;&#23481;&#24615;&#12290;&#29305;&#21035;&#22320;&#65292;&#23427;&#36890;&#36807;&#32771;&#34385;&#21452;&#37325;&#26367;&#20195;&#32780;&#38750;&#20027;&#35201;&#24418;&#24335;&#30340;&#27969;&#34892;&#27169;&#22411;&#65288;&#22914;PCA&#65289;&#26469;&#24341;&#20837;&#31070;&#32463;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#23427;&#19982;&#34920;&#31034;&#23450;&#29702;&#30340;&#20851;&#32852;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#36825;&#31181;&#26041;&#27861;&#30340;&#25945;&#23548;&#26469;&#25506;&#32034;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#38416;&#36848;&#20102;Hebbian&#23398;&#20064;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#27491;&#21017;&#21270;&#30340;&#30417;&#30563;&#23398;&#20064;&#65292;&#24182;&#38416;&#26126;&#20102;&#31070;&#32463;&#32467;&#26500;&#30340;&#20986;&#29616;&#20197;&#21450;&#21152;&#24615;&#26356;&#26032;&#21644;&#20056;&#24615;&#26356;&#26032;&#35268;&#21017;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30340;&#37325;&#28857;&#19981;&#22312;&#20110;&#24320;&#21457;&#26032;&#31639;&#27861;&#65292;&#32780;&#22312;&#20110;&#23637;&#31034;&#34920;&#31034;&#23450;&#29702;&#25552;&#20379;&#20102;&#30740;&#31350;&#29983;&#29289;&#21512;&#29702;&#23398;&#20064;&#31639;&#27861;&#30340;&#23436;&#32654;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
A normative approach called Similarity Matching was recently introduced for deriving and understanding the algorithmic basis of neural computation focused on unsupervised problems. It involves deriving algorithms from computational objectives and evaluating their compatibility with anatomical and physiological observations. In particular, it introduces neural architectures by considering dual alternatives instead of primal formulations of popular models such as PCA. However, its connection to the Representer theorem remains unexplored. In this work, we propose to use teachings from this approach to explore supervised learning algorithms and clarify the notion of Hebbian learning. We examine regularized supervised learning and elucidate the emergence of neural architecture and additive versus multiplicative update rules. In this work, we focus not on developing new algorithms but on showing that the Representer theorem offers the perfect lens to study biologically plausible learning alg
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20132;&#26367;&#23398;&#20064;&#30340;&#31232;&#30095;&#35821;&#20041;&#20256;&#36755;&#31995;&#32479;SparseSBC&#65292;&#36890;&#36807;&#20004;&#20010;&#29420;&#31435;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36827;&#34892;&#32534;&#30721;&#21644;&#35299;&#30721;&#65292;&#35299;&#20915;&#20102;&#20449;&#36947;&#30340;&#38750;&#21487;&#24494;&#24615;&#38382;&#39064;&#65292;&#22312;&#35270;&#35273;&#20256;&#36755;&#20013;&#23637;&#29616;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.16681</link><description>&lt;p&gt;
&#22522;&#20110;&#20132;&#26367;&#23398;&#20064;&#30340;&#31232;&#30095;&#35821;&#20041;&#20256;&#36755;&#30340;&#35270;&#35273;&#20256;&#36755;
&lt;/p&gt;
&lt;p&gt;
Alternate Learning based Sparse Semantic Communications for Visual Transmission. (arXiv:2309.16681v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16681
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20132;&#26367;&#23398;&#20064;&#30340;&#31232;&#30095;&#35821;&#20041;&#20256;&#36755;&#31995;&#32479;SparseSBC&#65292;&#36890;&#36807;&#20004;&#20010;&#29420;&#31435;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36827;&#34892;&#32534;&#30721;&#21644;&#35299;&#30721;&#65292;&#35299;&#20915;&#20102;&#20449;&#36947;&#30340;&#38750;&#21487;&#24494;&#24615;&#38382;&#39064;&#65292;&#22312;&#35270;&#35273;&#20256;&#36755;&#20013;&#23637;&#29616;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#36890;&#20449;&#65288;SemCom&#65289;&#36890;&#36807;&#20165;&#23581;&#35797;&#24674;&#22797;&#25968;&#25454;&#30340;&#22522;&#26412;&#35821;&#20041;&#20449;&#24687;&#65292;&#30456;&#23545;&#20110;&#20256;&#32479;&#30340;&#20301;&#32423;&#20934;&#30830;&#20256;&#36755;&#23637;&#29616;&#20986;&#24456;&#22823;&#30340;&#20248;&#21183;&#12290;&#20026;&#20102;&#35299;&#20915;&#20449;&#36947;&#30340;&#38750;&#21487;&#24494;&#24615;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SparseSBC&#30340;&#22522;&#20110;&#20132;&#26367;&#23398;&#20064;&#30340;SemCom&#31995;&#32479;&#65292;&#29992;&#20110;&#35270;&#35273;&#20256;&#36755;&#12290;SparseSBC&#22312;&#21457;&#23556;&#31471;&#21644;&#25509;&#25910;&#31471;&#20998;&#21035;&#21033;&#29992;&#20004;&#20010;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#27169;&#22411;&#65292;&#24182;&#20197;&#20132;&#26367;&#30340;&#26041;&#24335;&#23398;&#20064;&#32534;&#30721;&#21644;&#35299;&#30721;&#65292;&#32780;&#19981;&#26159;&#29616;&#26377;&#25991;&#29486;&#20013;&#30340;&#32852;&#21512;&#20248;&#21270;&#65292;&#20197;&#35299;&#20915;&#20449;&#36947;&#30340;&#38750;&#21487;&#24494;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36824;&#37319;&#29992;&#20102;&#8220;&#33258;&#25209;&#35780;&#8221;&#35757;&#32451;&#26041;&#26696;&#20197;&#23454;&#29616;&#31283;&#23450;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;DNN&#30340;&#21457;&#23556;&#26426;&#20351;&#29992;&#20108;&#36827;&#21046;&#37327;&#21270;&#27169;&#22359;&#65292;&#22312;&#23545;&#35821;&#20041;&#20934;&#30830;&#24615;&#24433;&#21709;&#26368;&#23567;&#30340;&#22522;&#30784;&#19978;&#29983;&#25104;&#19968;&#32452;&#31232;&#30095;&#30340;&#20301;&#20110;&#25512;&#23548;&#8220;&#35821;&#20041;&#22522;&#8221;&#30340;&#20301;&#12290;&#24191;&#27867;&#30340;&#20223;&#30495;&#32467;&#26524;&#39564;&#35777;&#20102;SparseSBC&#30340;&#24615;&#33021;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic communication (SemCom) demonstrates strong superiority over conventional bit-level accurate transmission, by only attempting to recover the essential semantic information of data. In this paper, in order to tackle the non-differentiability of channels, we propose an alternate learning based SemCom system for visual transmission, named SparseSBC. Specially, SparseSBC leverages two separate Deep Neural Network (DNN)-based models at the transmitter and receiver, respectively, and learns the encoding and decoding in an alternate manner, rather than the joint optimization in existing literature, so as to solving the non-differentiability in the channel. In particular, a ``self-critic" training scheme is leveraged for stable training. Moreover, the DNN-based transmitter generates a sparse set of bits in deduced ``semantic bases", by further incorporating a binary quantization module on the basis of minimal detrimental effect to the semantic accuracy. Extensive simulation results val
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SupReMix&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28151;&#21512;&#26679;&#26412;&#65292;&#29305;&#21035;&#26159;&#28151;&#21512;&#36127;&#26679;&#26412;&#21644;&#28151;&#21512;&#27491;&#26679;&#26412;&#65292;&#26469;&#35299;&#20915;&#22238;&#24402;&#38382;&#39064;&#20013;&#34920;&#31034;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25552;&#20379;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#26356;&#20934;&#30830;&#30340;&#22238;&#24402;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.16633</link><description>&lt;p&gt;
&#28151;&#21512;&#20320;&#33258;&#24049;&#30340;&#23545;&#27604;&#23545;
&lt;/p&gt;
&lt;p&gt;
Mixup Your Own Pairs. (arXiv:2309.16633v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16633
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SupReMix&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28151;&#21512;&#26679;&#26412;&#65292;&#29305;&#21035;&#26159;&#28151;&#21512;&#36127;&#26679;&#26412;&#21644;&#28151;&#21512;&#27491;&#26679;&#26412;&#65292;&#26469;&#35299;&#20915;&#22238;&#24402;&#38382;&#39064;&#20013;&#34920;&#31034;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25552;&#20379;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#26356;&#20934;&#30830;&#30340;&#22238;&#24402;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#34920;&#31034;&#23398;&#20064;&#20013;&#65292;&#22238;&#24402;&#38382;&#39064;&#20256;&#32479;&#19978;&#27604;&#20998;&#31867;&#38382;&#39064;&#21463;&#21040;&#30340;&#20851;&#27880;&#36739;&#23569;&#12290;&#30452;&#25509;&#24212;&#29992;&#20026;&#20998;&#31867;&#35774;&#35745;&#30340;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#21040;&#22238;&#24402;&#38382;&#39064;&#24448;&#24448;&#20250;&#23548;&#33268;&#28508;&#31354;&#38388;&#20013;&#30862;&#29255;&#21270;&#30340;&#34920;&#31034;&#65292;&#20174;&#32780;&#20135;&#29983;&#27425;&#20248;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#35748;&#20026;&#65292;&#30001;&#20110;&#24573;&#35270;&#20102;&#20004;&#20010;&#20851;&#38190;&#26041;&#38754;&#65306;&#24207;&#24207;&#24863;&#30693;&#21644;&#38590;&#24230;&#65292;&#23545;&#20110;&#22238;&#24402;&#38382;&#39064;&#32780;&#35328;&#65292;&#23545;&#27604;&#23398;&#20064;&#30340;&#28508;&#33021;&#34987;&#24573;&#35270;&#20102;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20513;&#8220;&#28151;&#21512;&#33258;&#24049;&#30340;&#23545;&#27604;&#23545;&#36827;&#34892;&#30417;&#30563;&#24615;&#23545;&#27604;&#22238;&#24402;&#8221;&#65292;&#32780;&#19981;&#20165;&#20165;&#20381;&#38752;&#30495;&#23454;/&#22686;&#24378;&#26679;&#26412;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#28151;&#21512;&#24335;&#30417;&#30563;&#23545;&#27604;&#22238;&#24402;&#23398;&#20064;&#65288;SupReMix&#65289;&#12290;&#23427;&#22312;&#23884;&#20837;&#32423;&#21035;&#19978;&#20197;&#38170;&#28857;&#21253;&#21547;&#30340;&#28151;&#21512;&#65288;&#38170;&#28857;&#21644;&#19968;&#20010;&#19981;&#21516;&#30340;&#36127;&#26679;&#26412;&#30340;&#28151;&#21512;&#65289;&#20316;&#20026;&#22256;&#38590;&#36127;&#23545;&#65292;&#20197;&#38170;&#28857;&#25490;&#38500;&#30340;&#28151;&#21512;&#65288;&#20004;&#20010;&#19981;&#21516;&#30340;&#36127;&#26679;&#26412;&#30340;&#28151;&#21512;&#65289;&#20316;&#20026;&#22256;&#38590;&#27491;&#23545;&#12290;&#36825;&#19968;&#31574;&#30053;&#24418;&#25104;&#20102;&#22256;&#38590;&#26679;&#26412;&#23545;&#23398;&#20064;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
In representation learning, regression has traditionally received less attention than classification. Directly applying representation learning techniques designed for classification to regression often results in fragmented representations in the latent space, yielding sub-optimal performance. In this paper, we argue that the potential of contrastive learning for regression has been overshadowed due to the neglect of two crucial aspects: ordinality-awareness and hardness. To address these challenges, we advocate "mixup your own contrastive pairs for supervised contrastive regression", instead of relying solely on real/augmented samples. Specifically, we propose Supervised Contrastive Learning for Regression with Mixup (SupReMix). It takes anchor-inclusive mixtures (mixup of the anchor and a distinct negative sample) as hard negative pairs and anchor-exclusive mixtures (mixup of two distinct negative samples) as hard positive pairs at the embedding level. This strategy formulates harde
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AutoCLIP&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#35843;&#35856;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#12290;AutoCLIP&#36890;&#36807;&#20026;&#27599;&#20010;&#25552;&#31034;&#27169;&#26495;&#20998;&#37197;&#22270;&#20687;&#29305;&#23450;&#30340;&#26435;&#37325;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#20174;&#32534;&#30721;&#31867;&#21035;&#25551;&#36848;&#31526;&#25512;&#23548;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#30340;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2309.16414</link><description>&lt;p&gt;
AutoCLIP: &#33258;&#21160;&#35843;&#35856;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
AutoCLIP: Auto-tuning Zero-Shot Classifiers for Vision-Language Models. (arXiv:2309.16414v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16414
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AutoCLIP&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#35843;&#35856;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#12290;AutoCLIP&#36890;&#36807;&#20026;&#27599;&#20010;&#25552;&#31034;&#27169;&#26495;&#20998;&#37197;&#22270;&#20687;&#29305;&#23450;&#30340;&#26435;&#37325;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#20174;&#32534;&#30721;&#31867;&#21035;&#25551;&#36848;&#31526;&#25512;&#23548;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;CLIP&#65289;&#26500;&#24314;&#30340;&#20998;&#31867;&#22120;&#22312;&#24191;&#27867;&#30340;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#20986;&#33394;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#30740;&#31350;&#20102;&#26681;&#25454;&#25552;&#31034;&#27169;&#26495;&#33258;&#21160;&#21019;&#24314;&#27599;&#20010;&#31867;&#21035;&#30340;&#25551;&#36848;&#31526;&#38598;&#30340;&#19981;&#21516;&#26041;&#24335;&#65292;&#21253;&#25324;&#25163;&#24037;&#35774;&#35745;&#30340;&#27169;&#26495;&#12289;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33719;&#21462;&#30340;&#27169;&#26495;&#20197;&#21450;&#20174;&#38543;&#26426;&#21333;&#35789;&#21644;&#23383;&#31526;&#26500;&#24314;&#30340;&#27169;&#26495;&#12290;&#28982;&#32780;&#65292;&#20174;&#30456;&#24212;&#30340;&#32534;&#30721;&#31867;&#21035;&#25551;&#36848;&#31526;&#23548;&#20986;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#20960;&#20046;&#27809;&#26377;&#25913;&#21464;&#65306;&#23558;&#22270;&#20687;&#30340;&#24179;&#22343;&#32534;&#30721;&#31867;&#21035;&#25551;&#36848;&#31526;&#19982;&#32534;&#30721;&#22270;&#20687;&#20043;&#38388;&#30340;&#20313;&#24358;&#30456;&#20284;&#24230;&#26368;&#22823;&#21270;&#20197;&#36827;&#34892;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#24403;&#26576;&#20123;&#25551;&#36848;&#31526;&#27604;&#20854;&#20182;&#25551;&#36848;&#31526;&#26356;&#22909;&#22320;&#21305;&#37197;&#32473;&#23450;&#22270;&#20687;&#19978;&#30340;&#35270;&#35273;&#32447;&#32034;&#26102;&#65292;&#23558;&#25152;&#26377;&#31867;&#21035;&#25551;&#36848;&#31526;&#31561;&#26435;&#37325;&#21487;&#33021;&#19981;&#26159;&#26368;&#20248;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#35843;&#35856;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;AutoCLIP&#12290;AutoCLIP&#20026;&#27599;&#20010;&#25552;&#31034;&#27169;&#26495;&#20998;&#37197;&#20102;&#22270;&#20687;&#29305;&#23450;&#30340;&#26435;&#37325;&#65292;&#36825;&#20123;&#26435;&#37325;&#26159;&#20174;s
&lt;/p&gt;
&lt;p&gt;
Classifiers built upon vision-language models such as CLIP have shown remarkable zero-shot performance across a broad range of image classification tasks. Prior work has studied different ways of automatically creating descriptor sets for every class based on prompt templates, ranging from manually engineered templates over templates obtained from a large language model to templates built from random words and characters. In contrast, deriving zero-shot classifiers from the respective encoded class descriptors has remained nearly unchanged, that is: classify to the class that maximizes the cosine similarity between its averaged encoded class descriptors and the encoded image. However, weighting all class descriptors equally can be suboptimal when certain descriptors match visual clues on a given image better than others. In this work, we propose AutoCLIP, a method for auto-tuning zero-shot classifiers. AutoCLIP assigns to each prompt template per-image weights, which are derived from s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LogicMP&#30340;&#26032;&#39062;&#31070;&#32463;&#23618;&#65292;&#35813;&#23618;&#36890;&#36807;&#22343;&#22330;&#21464;&#20998;&#25512;&#26029;&#23558;&#19968;&#38454;&#36923;&#36753;&#32422;&#26463;&#32534;&#30721;&#36827;&#31070;&#32463;&#32593;&#32476;&#20013;&#12290;&#36890;&#36807;&#26377;&#25928;&#32531;&#35299;&#19968;&#38454;&#36923;&#36753;&#27169;&#22411;&#30340;&#25512;&#26029;&#22256;&#38590;&#65292;LogicMP&#22312;&#22270;&#24418;&#12289;&#22270;&#20687;&#21644;&#25991;&#26412;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#27604;&#31454;&#20105;&#23545;&#25163;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.15458</link><description>&lt;p&gt;
LogicMP: &#19968;&#31181;&#23558;&#19968;&#38454;&#36923;&#36753;&#32422;&#26463;&#32534;&#30721;&#30340;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
LogicMP: A Neuro-symbolic Approach for Encoding First-order Logic Constraints. (arXiv:2309.15458v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15458
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LogicMP&#30340;&#26032;&#39062;&#31070;&#32463;&#23618;&#65292;&#35813;&#23618;&#36890;&#36807;&#22343;&#22330;&#21464;&#20998;&#25512;&#26029;&#23558;&#19968;&#38454;&#36923;&#36753;&#32422;&#26463;&#32534;&#30721;&#36827;&#31070;&#32463;&#32593;&#32476;&#20013;&#12290;&#36890;&#36807;&#26377;&#25928;&#32531;&#35299;&#19968;&#38454;&#36923;&#36753;&#27169;&#22411;&#30340;&#25512;&#26029;&#22256;&#38590;&#65292;LogicMP&#22312;&#22270;&#24418;&#12289;&#22270;&#20687;&#21644;&#25991;&#26412;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#27604;&#31454;&#20105;&#23545;&#25163;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#19968;&#38454;&#36923;&#36753;&#32422;&#26463;&#19982;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#26159;&#19968;&#20010;&#20851;&#38190;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#28041;&#21450;&#24314;&#27169;&#22797;&#26434;&#30340;&#30456;&#20851;&#24615;&#20197;&#28385;&#36275;&#32422;&#26463;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#23618;LogicMP&#65292;&#20854;&#23618;&#23545;MLN&#36827;&#34892;&#22343;&#22330;&#21464;&#20998;&#25512;&#26029;&#12290;&#23427;&#21487;&#20197;&#25554;&#20837;&#20219;&#20309;&#29616;&#25104;&#30340;&#31070;&#32463;&#32593;&#32476;&#20197;&#32534;&#30721;&#19968;&#38454;&#36923;&#36753;&#32422;&#26463;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22359;&#21270;&#21644;&#25928;&#29575;&#12290;&#36890;&#36807;&#21033;&#29992;MLN&#20013;&#30340;&#32467;&#26500;&#21644;&#23545;&#31216;&#24615;&#65292;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#25105;&#20204;&#35774;&#35745;&#33391;&#22909;&#12289;&#39640;&#25928;&#30340;&#22343;&#22330;&#36845;&#20195;&#33021;&#22815;&#26377;&#25928;&#32531;&#35299;MLN&#25512;&#26029;&#30340;&#22256;&#38590;&#65292;&#23558;&#25512;&#26029;&#20174;&#39034;&#24207;&#35745;&#31639;&#38477;&#20302;&#20026;&#19968;&#31995;&#21015;&#24182;&#34892;&#30340;&#24352;&#37327;&#25805;&#20316;&#12290;&#22312;&#22270;&#24418;&#12289;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#19977;&#31867;&#20219;&#21153;&#19978;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;LogicMP&#22312;&#24615;&#33021;&#21644;&#25928;&#29575;&#19978;&#37117;&#20248;&#20110;&#20808;&#36827;&#30340;&#31454;&#20105;&#23545;&#25163;&#12290;
&lt;/p&gt;
&lt;p&gt;
Integrating first-order logic constraints (FOLCs) with neural networks is a crucial but challenging problem since it involves modeling intricate correlations to satisfy the constraints. This paper proposes a novel neural layer, LogicMP, whose layers perform mean-field variational inference over an MLN. It can be plugged into any off-the-shelf neural network to encode FOLCs while retaining modularity and efficiency. By exploiting the structure and symmetries in MLNs, we theoretically demonstrate that our well-designed, efficient mean-field iterations effectively mitigate the difficulty of MLN inference, reducing the inference from sequential calculation to a series of parallel tensor operations. Empirical results in three kinds of tasks over graphs, images, and text show that LogicMP outperforms advanced competitors in both performance and efficiency.
&lt;/p&gt;</description></item><item><title>&#26368;&#22823;&#25193;&#25955;&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#31181;&#20811;&#26381;&#24378;&#21270;&#23398;&#20064;&#20013;&#25968;&#25454;&#30456;&#20851;&#24615;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#32806;&#20195;&#29702;&#30340;&#32463;&#39564;&#23454;&#29616;&#25345;&#32493;&#23398;&#20064;&#65292;&#24182;&#22312;&#21508;&#31181;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2309.15293</link><description>&lt;p&gt;
&#26368;&#22823;&#25193;&#25955;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Maximum Diffusion Reinforcement Learning. (arXiv:2309.15293v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15293
&lt;/p&gt;
&lt;p&gt;
&#26368;&#22823;&#25193;&#25955;&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#31181;&#20811;&#26381;&#24378;&#21270;&#23398;&#20064;&#20013;&#25968;&#25454;&#30456;&#20851;&#24615;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#32806;&#20195;&#29702;&#30340;&#32463;&#39564;&#23454;&#29616;&#25345;&#32493;&#23398;&#20064;&#65292;&#24182;&#22312;&#21508;&#31181;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25152;&#26377;&#26426;&#22120;&#23398;&#20064;&#37117;&#24314;&#31435;&#22312;&#25968;&#25454;&#29420;&#31435;&#19988;&#21516;&#20998;&#24067;&#30340;&#20551;&#35774;&#19978;&#12290;&#28982;&#32780;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#24403;&#25968;&#25454;&#26159;&#20381;&#27425;&#20174;&#20195;&#29702;&#32463;&#39564;&#20013;&#25910;&#38598;&#32780;&#26469;&#26102;&#65292;&#36825;&#19968;&#20551;&#35774;&#36890;&#24120;&#19981;&#25104;&#31435;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#26368;&#22823;&#25193;&#25955;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#32479;&#35745;&#21147;&#23398;&#20013;&#30340;&#36941;&#21382;&#36807;&#31243;&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#35299;&#32806;&#20195;&#29702;&#30340;&#32463;&#39564;&#65292;&#21487;&#35777;&#26126;&#22320;&#20351;&#20195;&#29702;&#22312;&#21333;&#27425;&#37096;&#32626;&#20013;&#33021;&#22815;&#25345;&#32493;&#23398;&#20064;&#65292;&#32780;&#19981;&#21463;&#21021;&#22987;&#21270;&#26041;&#24335;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#25512;&#24191;&#20102;&#20247;&#25152;&#21608;&#30693;&#30340;&#26368;&#22823;&#29109;&#25216;&#26415;&#65292;&#24182;&#19988;&#36890;&#36807;&#22312;&#27969;&#34892;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#31283;&#23450;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25104;&#26524;&#26497;&#22823;&#22320;&#20419;&#36827;&#20102;&#29289;&#29702;&#23398;&#12289;&#23398;&#20064;&#21644;&#25511;&#21046;&#30340;&#20132;&#21449;&#39046;&#22495;&#65292;&#20026;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#65288;&#22914;&#34892;&#36208;&#26426;&#22120;&#20154;&#21644;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#65289;&#30340;&#36879;&#26126;&#21487;&#38752;&#20915;&#31574;&#25552;&#20379;&#20102;&#19968;&#26465;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
The assumption that data are independent and identically distributed underpins all machine learning. When data are collected sequentially from agent experiences this assumption does not generally hold, as in reinforcement learning. Here, we derive a method that overcomes these limitations by exploiting the statistical mechanics of ergodic processes, which we term maximum diffusion reinforcement learning. By decorrelating agent experiences, our approach provably enables agents to learn continually in single-shot deployments regardless of how they are initialized. Moreover, we prove our approach generalizes well-known maximum entropy techniques, and show that it robustly exceeds state-of-the-art performance across popular benchmarks. Our results at the nexus of physics, learning, and control pave the way towards more transparent and reliable decision-making in reinforcement learning agents, such as locomoting robots and self-driving cars.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22330;&#26223;&#20808;&#39564;&#30340;&#21487;&#25512;&#24191;&#31070;&#32463;&#22330;&#26041;&#27861;&#65292;&#21033;&#29992;&#21333;&#20010;RGB-D&#22270;&#20687;&#26144;&#23556;&#22330;&#26223;&#65292;&#26080;&#38656;&#34701;&#21512;&#27169;&#22359;&#21363;&#21487;&#37325;&#24314;&#23436;&#25972;&#30340;&#22330;&#26223;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#28789;&#27963;&#24615;&#21644;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.15164</link><description>&lt;p&gt;
&#20351;&#29992;&#22330;&#26223;&#20808;&#39564;&#30340;&#21487;&#25512;&#24191;&#31070;&#32463;&#22330;&#36827;&#34892;3D&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
3D Reconstruction with Generalizable Neural Fields using Scene Priors. (arXiv:2309.15164v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15164
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22330;&#26223;&#20808;&#39564;&#30340;&#21487;&#25512;&#24191;&#31070;&#32463;&#22330;&#26041;&#27861;&#65292;&#21033;&#29992;&#21333;&#20010;RGB-D&#22270;&#20687;&#26144;&#23556;&#22330;&#26223;&#65292;&#26080;&#38656;&#34701;&#21512;&#27169;&#22359;&#21363;&#21487;&#37325;&#24314;&#23436;&#25972;&#30340;&#22330;&#26223;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#28789;&#27963;&#24615;&#21644;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#31070;&#32463;&#22330;&#30340;&#36827;&#23637;&#26497;&#22823;&#22320;&#25512;&#21160;&#20102;&#39640;&#20445;&#30495;&#24230;&#30340;3D&#22330;&#26223;&#37325;&#24314;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#23545;&#27599;&#20010;&#22330;&#26223;&#37117;&#35201;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#19968;&#20010;&#29420;&#31435;&#30340;&#32593;&#32476;&#65292;&#36825;&#31181;&#26041;&#27861;&#19981;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#65292;&#25928;&#29575;&#20302;&#19979;&#65292;&#19988;&#22312;&#26377;&#38480;&#35270;&#35282;&#19979;&#26080;&#27861;&#20135;&#29983;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;&#32780;&#22522;&#20110;&#23398;&#20064;&#30340;&#22810;&#35270;&#22270;&#31435;&#20307;&#26041;&#27861;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#20854;&#22810;&#35270;&#22270;&#35774;&#32622;&#20351;&#24471;&#23427;&#22312;&#25193;&#23637;&#21644;&#24191;&#27867;&#24212;&#29992;&#26041;&#38754;&#19981;&#22826;&#28789;&#27963;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#35757;&#32451;&#21487;&#25512;&#24191;&#30340;&#34701;&#20837;&#22330;&#26223;&#20808;&#39564;&#30340;&#31070;&#32463;&#22330;&#65288;NFPs&#65289;&#26041;&#27861;&#12290;NFP&#32593;&#32476;&#23558;&#20219;&#20309;&#21333;&#35270;&#35282;&#30340;RGB-D&#22270;&#20687;&#26144;&#23556;&#25104;&#26377;&#31526;&#21495;&#36317;&#31163;&#21644;&#36752;&#23556;&#20540;&#12290;&#36890;&#36807;&#22312;&#20307;&#31215;&#31354;&#38388;&#20013;&#21512;&#24182;&#21333;&#24103;&#65292;&#21487;&#20197;&#37325;&#24314;&#23436;&#25972;&#30340;&#22330;&#26223;&#65292;&#26080;&#38656;&#34701;&#21512;&#27169;&#22359;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#22909;&#30340;&#28789;&#27963;&#24615;&#12290;&#22330;&#26223;&#20808;&#39564;&#21487;&#20197;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20351;&#24471;&#33021;&#22815;&#24555;&#36895;&#36866;&#24212;&#20351;&#29992;&#36739;&#23569;&#35270;&#35282;&#37325;&#24314;&#26032;&#22330;&#26223;&#12290;NFP&#19981;&#20165;&#23637;&#31034;&#20102;SOTA&#22330;&#26223;&#37325;&#24314;&#24615;&#33021;&#65292;
&lt;/p&gt;
&lt;p&gt;
High-fidelity 3D scene reconstruction has been substantially advanced by recent progress in neural fields. However, most existing methods train a separate network from scratch for each individual scene. This is not scalable, inefficient, and unable to yield good results given limited views. While learning-based multi-view stereo methods alleviate this issue to some extent, their multi-view setting makes it less flexible to scale up and to broad applications. Instead, we introduce training generalizable Neural Fields incorporating scene Priors (NFPs). The NFP network maps any single-view RGB-D image into signed distance and radiance values. A complete scene can be reconstructed by merging individual frames in the volumetric space WITHOUT a fusion module, which provides better flexibility. The scene priors can be trained on large-scale datasets, allowing for fast adaptation to the reconstruction of a new scene with fewer views. NFP not only demonstrates SOTA scene reconstruction performa
&lt;/p&gt;</description></item><item><title>Supersonic &#26159;&#19968;&#20010;&#31070;&#32463;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;C/C++&#20013;&#36827;&#34892;&#28304;&#20195;&#30721;&#20248;&#21270;&#12290;&#19982;GPT-3.5-Turbo&#21644;GPT-4&#30456;&#27604;&#65292;&#23427;&#22312;&#20195;&#30721;&#20248;&#21270;&#20219;&#21153;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#19988;&#25913;&#21464;&#30340;&#31243;&#24230;&#26356;&#23567;&#12290;</title><link>http://arxiv.org/abs/2309.14846</link><description>&lt;p&gt;
Supersonic: &#23398;&#20064;&#22312;C/C++&#20013;&#29983;&#25104;&#28304;&#20195;&#30721;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Supersonic: Learning to Generate Source Code Optimisations in C/C++. (arXiv:2309.14846v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14846
&lt;/p&gt;
&lt;p&gt;
Supersonic &#26159;&#19968;&#20010;&#31070;&#32463;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;C/C++&#20013;&#36827;&#34892;&#28304;&#20195;&#30721;&#20248;&#21270;&#12290;&#19982;GPT-3.5-Turbo&#21644;GPT-4&#30456;&#27604;&#65292;&#23427;&#22312;&#20195;&#30721;&#20248;&#21270;&#20219;&#21153;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#19988;&#25913;&#21464;&#30340;&#31243;&#24230;&#26356;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36719;&#20214;&#20248;&#21270;&#22312;&#20445;&#25345;&#21151;&#33021;&#30340;&#21516;&#26102;&#25913;&#21892;&#36164;&#28304;&#25928;&#29575;&#12290;&#20256;&#32479;&#19978;&#65292;&#36825;&#26159;&#30001;&#24320;&#21457;&#20154;&#21592;&#21644;&#32534;&#35793;&#22120;&#23436;&#25104;&#30340;&#36807;&#31243;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#31532;&#19977;&#31181;&#36873;&#25321;&#65292;&#21363;&#22312;&#28304;&#20195;&#30721;&#32423;&#21035;&#36827;&#34892;&#33258;&#21160;&#20248;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Supersonic&#65292;&#19968;&#20010;&#38024;&#23545;&#20248;&#21270;&#30340;&#36731;&#24494;&#28304;&#20195;&#30721;&#20462;&#25913;&#30340;&#31070;&#32463;&#26041;&#27861;&#12290;&#20351;&#29992;seq2seq&#27169;&#22411;&#65292;Supersonic&#22312;C / C ++&#31243;&#24207;&#23545;&#65288;$x_{t}$&#65292;$x_{t+1}$&#65289;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20854;&#20013;$x_{t+1}$&#26159;$x_{t}$&#30340;&#20248;&#21270;&#29256;&#26412;&#65292;&#24182;&#36755;&#20986;&#19968;&#20010;&#24046;&#24322;&#12290;Supersonic&#30340;&#24615;&#33021;&#22312;&#31454;&#25216;&#32534;&#31243;&#20219;&#21153;&#19978;&#19982;OpenAI&#30340;GPT-3.5-Turbo&#21644;GPT-4&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;Supersonic&#19981;&#20165;&#22312;&#20195;&#30721;&#20248;&#21270;&#20219;&#21153;&#19978;&#32988;&#36807;&#20102;&#36825;&#20004;&#20010;&#27169;&#22411;&#65292;&#32780;&#19988;&#25913;&#21464;&#30340;&#31243;&#24230;&#27604;GPT-3.5-Turbo&#23567;&#20102;600&#22810;&#20493;&#65292;&#27604;GPT-4&#23567;&#20102;3700&#22810;&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Software optimization refines programs for resource efficiency while preserving functionality. Traditionally, it is a process done by developers and compilers. This paper introduces a third option, automated optimization at the source code level. We present Supersonic, a neural approach targeting minor source code modifications for optimization. Using a seq2seq model, Supersonic is trained on C/C++ program pairs ($x_{t}$, $x_{t+1}$), where $x_{t+1}$ is an optimized version of $x_{t}$, and outputs a diff. Supersonic's performance is benchmarked against OpenAI's GPT-3.5-Turbo and GPT-4 on competitive programming tasks. The experiments show that Supersonic not only outperforms both models on the code optimization task, but also minimizes the extent of change with a more than 600x smaller than GPT-3.5-Turbo and 3700x smaller than GPT-4.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31639;&#27861;&#21644;&#24037;&#20855;&#65292;&#21487;&#20197;&#20026;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#36755;&#20986;&#35745;&#31639;&#22810;&#20010;&#35299;&#37322;&#65292;&#20174;&#32780;&#25552;&#39640;&#23545;&#20998;&#31867;&#22120;&#34892;&#20026;&#30340;&#27934;&#23519;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.14309</link><description>&lt;p&gt;
&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#22810;&#20010;&#19981;&#21516;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Multiple Different Explanations for Image Classifiers. (arXiv:2309.14309v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14309
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31639;&#27861;&#21644;&#24037;&#20855;&#65292;&#21487;&#20197;&#20026;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#36755;&#20986;&#35745;&#31639;&#22810;&#20010;&#35299;&#37322;&#65292;&#20174;&#32780;&#25552;&#39640;&#23545;&#20998;&#31867;&#22120;&#34892;&#20026;&#30340;&#27934;&#23519;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#22270;&#20687;&#20998;&#31867;&#22120;&#35299;&#37322;&#24037;&#20855;&#36890;&#24120;&#21482;&#20250;&#32473;&#20986;&#19968;&#31181;&#23545;&#20110;&#22270;&#20687;&#30340;&#35299;&#37322;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#35768;&#22810;&#22270;&#20687;&#26469;&#35828;&#65292;&#26080;&#35770;&#26159;&#20154;&#31867;&#36824;&#26159;&#22270;&#20687;&#20998;&#31867;&#22120;&#37117;&#25509;&#21463;&#22810;&#20010;&#35299;&#37322;&#26469;&#35299;&#37322;&#22270;&#20687;&#26631;&#31614;&#12290;&#22240;&#27492;&#65292;&#38480;&#21046;&#35299;&#37322;&#30340;&#25968;&#37327;&#21482;&#26377;&#19968;&#20010;&#20005;&#37325;&#38480;&#21046;&#20102;&#23545;&#20998;&#31867;&#22120;&#34892;&#20026;&#30340;&#27934;&#23519;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#31639;&#27861;&#21644;&#24037;&#20855;REX&#65292;&#29992;&#20110;&#35745;&#31639;&#40657;&#30418;&#22270;&#20687;&#20998;&#31867;&#22120;&#23545;&#32473;&#23450;&#22270;&#20687;&#30340;&#36755;&#20986;&#30340;&#22810;&#20010;&#35299;&#37322;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#22522;&#20110;&#22240;&#26524;&#29702;&#35770;&#30340;&#21487;&#38752;&#26041;&#27861;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#20854;&#29702;&#35770;&#22797;&#26434;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#23454;&#39564;&#32467;&#26524;&#65292;&#26174;&#31034;REX&#22312;ImageNet-mini&#22522;&#20934;&#27979;&#35797;&#20013;&#25214;&#21040;&#30340;&#22810;&#20010;&#35299;&#37322;&#27604;&#20043;&#21069;&#30340;&#24037;&#20316;&#22810;7&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing explanation tools for image classifiers usually give only one single explanation for an image. For many images, however, both humans and image classifiers accept more than one explanation for the image label. Thus, restricting the number of explanations to just one severely limits the insight into the behavior of the classifier. In this paper, we describe an algorithm and a tool, REX, for computing multiple explanations of the output of a black-box image classifier for a given image. Our algorithm uses a principled approach based on causal theory. We analyse its theoretical complexity and provide experimental results showing that REX finds multiple explanations on 7 times more images than the previous work on the ImageNet-mini benchmark.
&lt;/p&gt;</description></item><item><title>Fast-HuBERT&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#26512;HuBERT&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#24341;&#20837;&#19968;&#31995;&#21015;&#25928;&#29575;&#20248;&#21270;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#22312;Librispeech 960h&#22522;&#20934;&#19978;&#30340;5.2&#20493;&#21152;&#36895;&#65292;&#24182;&#19988;&#22312;&#21069;&#20154;&#24037;&#20316;&#20013;&#24471;&#21040;&#20102;&#19968;&#33268;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2309.13860</link><description>&lt;p&gt;
Fast-HuBERT: &#19968;&#31181;&#39640;&#25928;&#30340;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#35757;&#32451;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Fast-HuBERT: An Efficient Training Framework for Self-Supervised Speech Representation Learning. (arXiv:2309.13860v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13860
&lt;/p&gt;
&lt;p&gt;
Fast-HuBERT&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#26512;HuBERT&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#24341;&#20837;&#19968;&#31995;&#21015;&#25928;&#29575;&#20248;&#21270;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#22312;Librispeech 960h&#22522;&#20934;&#19978;&#30340;5.2&#20493;&#21152;&#36895;&#65292;&#24182;&#19988;&#22312;&#21069;&#20154;&#24037;&#20316;&#20013;&#24471;&#21040;&#20102;&#19968;&#33268;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26041;&#27861;&#22312;&#35821;&#38899;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#21508;&#31181;&#22522;&#20110;&#35821;&#38899;&#30340;SSL&#27169;&#22411;&#24050;&#32463;&#24320;&#21457;&#20986;&#26469;&#65292;&#24182;&#22312;&#19968;&#31995;&#21015;&#19979;&#28216;&#20219;&#21153;&#65288;&#21253;&#25324;&#35821;&#38899;&#35782;&#21035;&#65289;&#19978;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#35821;&#38899;&#30340;SSL&#27169;&#22411;&#22312;&#35745;&#31639;&#25104;&#26412;&#26041;&#38754;&#38754;&#20020;&#30528;&#20849;&#21516;&#30340;&#22256;&#22659;&#65292;&#36825;&#21487;&#33021;&#20250;&#38459;&#30861;&#23427;&#20204;&#30340;&#28508;&#22312;&#24212;&#29992;&#21644;&#28145;&#20837;&#23398;&#26415;&#30740;&#31350;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#20998;&#26512;&#20102;HuBERT&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#19981;&#21516;&#27169;&#22359;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#28982;&#21518;&#24341;&#20837;&#20102;&#19968;&#31995;&#21015;&#25928;&#29575;&#20248;&#21270;&#31574;&#30053;&#65292;&#21363;&#26412;&#25991;&#20013;&#25552;&#20986;&#30340;Fast-HuBERT&#12290;&#25152;&#25552;&#20986;&#30340;Fast-HuBERT&#21487;&#20197;&#22312;Librispeech 960h&#22522;&#20934;&#19978;&#20351;&#29992;8&#20010;V100 GPU&#22312;1.1&#22825;&#20869;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#19981;&#20250;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#65292;&#30456;&#27604;&#21407;&#22987;&#23454;&#29616;&#65292;&#21152;&#36895;&#20102;5.2&#20493;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;Fast-HuBERT&#20013;&#30340;&#20004;&#31181;&#32463;&#36807;&#20805;&#20998;&#30740;&#31350;&#30340;&#25216;&#26415;&#65292;&#24182;&#23637;&#31034;&#20102;&#19982;&#20043;&#21069;&#24037;&#20316;&#20013;&#25253;&#21578;&#30340;&#19968;&#33268;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed significant advancements in self-supervised learning (SSL) methods for speech-processing tasks. Various speech-based SSL models have been developed and present promising performance on a range of downstream tasks including speech recognition. However, existing speech-based SSL models face a common dilemma in terms of computational cost, which might hinder their potential application and in-depth academic research. To address this issue, we first analyze the computational cost of different modules during HuBERT pre-training and then introduce a stack of efficiency optimizations, which is named Fast-HuBERT in this paper. The proposed Fast-HuBERT can be trained in 1.1 days with 8 V100 GPUs on the Librispeech 960h benchmark, without performance degradation, resulting in a 5.2x speedup, compared to the original implementation. Moreover, we explore two well-studied techniques in the Fast-HuBERT and demonstrate consistent improvements as reported in previous work.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;AI-&#20225;&#19994;&#20248;&#21270;&#30340;&#21327;&#21516;&#36741;&#21161;&#31995;&#32479;&#65292;&#36890;&#36807;&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#20943;&#23569;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#38656;&#27714;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2309.13218</link><description>&lt;p&gt;
AI-&#20225;&#19994;&#20248;&#21270;&#30340;&#21327;&#21516;&#36741;&#21161;&#65306;&#19968;&#20010;&#26694;&#26550;&#21644;&#22312;&#29983;&#20135;&#35843;&#24230;&#20013;&#30340;&#26696;&#20363;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
AI-Copilot for Business Optimisation: A Framework and A Case Study in Production Scheduling. (arXiv:2309.13218v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13218
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;AI-&#20225;&#19994;&#20248;&#21270;&#30340;&#21327;&#21516;&#36741;&#21161;&#31995;&#32479;&#65292;&#36890;&#36807;&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#20943;&#23569;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#38656;&#27714;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20225;&#19994;&#20248;&#21270;&#26159;&#23547;&#25214;&#21644;&#23454;&#26045;&#39640;&#25928;&#21644;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#30340;&#36816;&#33829;&#26041;&#24335;&#65292;&#20197;&#20026;&#20225;&#19994;&#24102;&#26469;&#31454;&#20105;&#20248;&#21183;&#30340;&#36807;&#31243;&#12290;&#32508;&#21512;&#38382;&#39064;&#34920;&#36848;&#26159;&#20225;&#19994;&#20248;&#21270;&#30340;&#19968;&#20010;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#23427;&#22260;&#32469;&#30528;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#23637;&#24320;&#65292;&#22240;&#27492;&#24456;&#26377;&#21487;&#33021;&#25104;&#20026;&#29942;&#39048;&#12290;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#36890;&#36807;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21487;&#20197;&#28508;&#22312;&#22320;&#20943;&#23569;&#38382;&#39064;&#34920;&#36848;&#20013;&#25152;&#38656;&#30340;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#24320;&#21457;&#29992;&#20110;&#38382;&#39064;&#34920;&#36848;&#30340;LLM&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#30001;&#20110;&#35757;&#32451;&#25968;&#25454;&#35201;&#27714;&#12289;&#20196;&#29260;&#38480;&#21046;&#20197;&#21450;LLM&#20013;&#32570;&#20047;&#36866;&#24403;&#30340;&#24615;&#33021;&#24230;&#37327;&#12290;&#20026;&#20102;&#20943;&#23569;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#38656;&#27714;&#65292;&#26368;&#36817;&#20154;&#20204;&#24320;&#22987;&#20851;&#27880;&#23545;&#39044;&#35757;&#32451;&#30340;LLM&#36827;&#34892;&#24494;&#35843;&#20197;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#65292;&#32780;&#19981;&#26159;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#19968;&#20010;&#29305;&#23450;&#20219;&#21153;&#30340;LLM&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#36825;&#31181;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;AI-&#20225;&#19994;&#20248;&#21270;&#30340;&#21327;&#21516;&#36741;&#21161;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Business optimisation is the process of finding and implementing efficient and cost-effective means of operation to bring a competitive advantage for businesses. Synthesizing problem formulations is an integral part of business optimisation which is centred around human expertise, thus with a high potential of becoming a bottleneck. With the recent advancements in Large Language Models (LLMs), human expertise needed in problem formulation can potentially be minimized using Artificial Intelligence (AI). However, developing a LLM for problem formulation is challenging, due to training data requirements, token limitations, and the lack of appropriate performance metrics in LLMs. To minimize the requirement of large training data, considerable attention has recently been directed towards fine-tuning pre-trained LLMs for downstream tasks, rather than training a LLM from scratch for a specific task. In this paper, we adopt this approach and propose an AI-Copilot for business optimisation by 
&lt;/p&gt;</description></item><item><title>DRG-LLaMA&#26159;&#19968;&#20010;&#36890;&#36807;&#22312;&#20020;&#24202;&#31508;&#35760;&#19978;&#32454;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#25913;&#36827;&#20303;&#38498;&#24739;&#32773;&#30340;&#35786;&#26029;&#30456;&#20851;&#20998;&#32452;&#39044;&#27979;&#12290;&#22312;&#22810;&#20010;&#35780;&#20272;&#25351;&#26631;&#19978;&#65292;DRG-LLaMA-7B&#30456;&#23545;&#20110;&#20854;&#20182;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#24182;&#33021;&#22815;&#39640;&#20934;&#30830;&#24230;&#22320;&#39044;&#27979;&#22522;&#26412;DRG&#21644;&#24182;&#21457;&#30151;/&#21512;&#24182;&#30151;&#65288;CC&#65289;/&#37325;&#22823;&#24182;&#21457;&#30151;&#25110;&#21512;&#24182;&#30151;&#65288;MCC&#65289;&#30340;&#29366;&#24577;&#12290;</title><link>http://arxiv.org/abs/2309.12625</link><description>&lt;p&gt;
DRG-LLaMA: &#35843;&#20248;LLaMA&#27169;&#22411;&#20197;&#39044;&#27979;&#20303;&#38498;&#24739;&#32773;&#30340;&#35786;&#26029;&#30456;&#20851;&#20998;&#32452;
&lt;/p&gt;
&lt;p&gt;
DRG-LLaMA : Tuning LLaMA Model to Predict Diagnosis-related Group for Hospitalized Patients. (arXiv:2309.12625v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12625
&lt;/p&gt;
&lt;p&gt;
DRG-LLaMA&#26159;&#19968;&#20010;&#36890;&#36807;&#22312;&#20020;&#24202;&#31508;&#35760;&#19978;&#32454;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#25913;&#36827;&#20303;&#38498;&#24739;&#32773;&#30340;&#35786;&#26029;&#30456;&#20851;&#20998;&#32452;&#39044;&#27979;&#12290;&#22312;&#22810;&#20010;&#35780;&#20272;&#25351;&#26631;&#19978;&#65292;DRG-LLaMA-7B&#30456;&#23545;&#20110;&#20854;&#20182;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#24182;&#33021;&#22815;&#39640;&#20934;&#30830;&#24230;&#22320;&#39044;&#27979;&#22522;&#26412;DRG&#21644;&#24182;&#21457;&#30151;/&#21512;&#24182;&#30151;&#65288;CC&#65289;/&#37325;&#22823;&#24182;&#21457;&#30151;&#25110;&#21512;&#24182;&#30151;&#65288;MCC&#65289;&#30340;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32654;&#22269;&#20303;&#38498;&#20184;&#36153;&#31995;&#32479;&#20013;&#65292;&#35786;&#26029;&#30456;&#20851;&#20998;&#32452;&#65288;DRG&#65289;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#20294;&#30446;&#21069;&#30340;&#20998;&#32452;&#36807;&#31243;&#32791;&#26102;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;DRG-LLaMA&#65292;&#19968;&#20010;&#22312;&#20020;&#24202;&#31508;&#35760;&#19978;&#36827;&#34892;&#32454;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#20197;&#25913;&#21892;DRG&#39044;&#27979;&#12290;&#20351;&#29992;Meta&#30340;LLaMA&#20316;&#20026;&#22522;&#30784;&#27169;&#22411;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;236,192&#20010;MIMIC-IV&#20986;&#38498;&#25688;&#35201;&#19978;&#36827;&#34892;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#20248;&#21270;&#12290;&#22312;&#36755;&#20837;&#20196;&#29260;&#38271;&#24230;&#20026;512&#30340;&#24773;&#20917;&#19979;&#65292;DRG-LLaMA-7B&#23454;&#29616;&#30340;&#23439;&#24179;&#22343;F1&#20998;&#25968;&#20026;0.327&#65292;&#39030;&#32423;&#39044;&#27979;&#20934;&#30830;&#24230;&#20026;52.0&#65285;&#65292;&#23439;&#24179;&#22343;AUC&#20026;0.986&#12290;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#26159;&#65292;DRG-LLaMA-7B&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#36229;&#36807;&#20102;&#20043;&#21069;&#25253;&#36947;&#30340;&#39046;&#20808;&#27169;&#22411;&#65292;&#30456;&#23545;&#20110;ClinicalBERT&#30340;&#23439;&#24179;&#22343;F1&#20998;&#25968;&#25552;&#39640;&#20102;40.3&#65285;&#65292;&#30456;&#23545;&#20110;CAML&#25552;&#39640;&#20102;35.7&#65285;&#12290;&#24403;&#24212;&#29992;DRG-LLaMA&#26469;&#39044;&#27979;&#22522;&#26412;DRG&#21644;&#24182;&#21457;&#30151;/&#21512;&#24182;&#30151;&#65288;CC&#65289;/&#37325;&#22823;&#24182;&#21457;&#30151;&#25110;&#21512;&#24182;&#30151;&#65288;MCC&#65289;&#26102;&#65292;&#22522;&#26412;DRG&#30340;&#39030;&#32423;&#39044;&#27979;&#20934;&#30830;&#24230;&#36798;&#21040;&#20102;67.8&#65285;&#65292;&#32780;CC/MCC&#29366;&#24577;&#30340;&#39044;&#27979;&#20934;&#30830;&#24230;&#36798;&#21040;&#20102;67.5&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the U.S. inpatient payment system, the Diagnosis-Related Group (DRG) plays a key role but its current assignment process is time-consuming. We introduce DRG-LLaMA, a large language model (LLM) fine-tuned on clinical notes for improved DRG prediction. Using Meta's LLaMA as the base model, we optimized it with Low-Rank Adaptation (LoRA) on 236,192 MIMIC-IV discharge summaries. With an input token length of 512, DRG-LLaMA-7B achieved a macro-averaged F1 score of 0.327, a top-1 prediction accuracy of 52.0% and a macro-averaged Area Under the Curve (AUC) of 0.986. Impressively, DRG-LLaMA-7B surpassed previously reported leading models on this task, demonstrating a relative improvement in macro-averaged F1 score of 40.3% compared to ClinicalBERT and 35.7% compared to CAML. When DRG-LLaMA is applied to predict base DRGs and complication or comorbidity (CC) / major complication or comorbidity (MCC), the top-1 prediction accuracy reached 67.8% for base DRGs and 67.5% for CC/MCC status. DRG-L
&lt;/p&gt;</description></item><item><title>&#23545;&#27604;&#35299;&#30721;&#26159;&#19968;&#31181;&#31616;&#21333;&#12289;&#35745;&#31639;&#37327;&#36731;&#12289;&#26080;&#38656;&#35757;&#32451;&#30340;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#24378;&#27169;&#22411;&#21644;&#24369;&#27169;&#22411;&#20043;&#38388;&#30340;&#20284;&#28982;&#24046;&#24322;&#65292;&#22312;&#21508;&#31181;&#25512;&#29702;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2309.09117</link><description>&lt;p&gt;
&#23545;&#27604;&#35299;&#30721;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Contrastive Decoding Improves Reasoning in Large Language Models. (arXiv:2309.09117v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09117
&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#35299;&#30721;&#26159;&#19968;&#31181;&#31616;&#21333;&#12289;&#35745;&#31639;&#37327;&#36731;&#12289;&#26080;&#38656;&#35757;&#32451;&#30340;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#24378;&#27169;&#22411;&#21644;&#24369;&#27169;&#22411;&#20043;&#38388;&#30340;&#20284;&#28982;&#24046;&#24322;&#65292;&#22312;&#21508;&#31181;&#25512;&#29702;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#27604;&#35299;&#30721;&#8212;&#8212;&#19968;&#31181;&#30001;Li&#31561;&#20154;&#25552;&#20986;&#30340;&#31616;&#21333;&#12289;&#35745;&#31639;&#37327;&#36731;&#12289;&#26080;&#38656;&#35757;&#32451;&#30340;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#8212;&#8212;&#22312;&#21508;&#31181;&#25512;&#29702;&#20219;&#21153;&#19978;&#36229;&#36807;&#36138;&#23146;&#35299;&#30721;&#30340;&#24615;&#33021;&#12290;&#26368;&#21021;&#34987;&#35777;&#26126;&#21487;&#20197;&#25913;&#21892;&#38271;&#25991;&#26412;&#29983;&#25104;&#30340;&#24863;&#30693;&#36136;&#37327;&#65292;&#23545;&#27604;&#35299;&#30721;&#25628;&#32034;&#26368;&#22823;&#21270;&#24378;&#27169;&#22411;&#21644;&#24369;&#27169;&#22411;&#20043;&#38388;&#20284;&#28982;&#24046;&#24322;&#21152;&#26435;&#30340;&#23383;&#31526;&#20018;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#27604;&#35299;&#30721;&#22312;HellaSwag&#24120;&#35782;&#25512;&#29702;&#22522;&#20934;&#19978;&#20351;LLaMA-65B&#36229;&#36807;&#20102;LLaMA 2&#12289;GPT-3.5&#21644;PaLM 2-L&#65292;&#24182;&#19988;&#22312;GSM8K&#25968;&#23398;&#21333;&#35789;&#25512;&#29702;&#22522;&#20934;&#19978;&#36229;&#36807;&#20102;LLaMA 2&#12289;GPT-3.5&#21644;PaLM-540B&#65292;&#27492;&#22806;&#36824;&#22312;&#19968;&#31995;&#21015;&#20854;&#20182;&#20219;&#21153;&#19978;&#26377;&#20102;&#25913;&#36827;&#12290;&#20998;&#26512;&#34920;&#26126;&#65292;&#23545;&#27604;&#35299;&#30721;&#36890;&#36807;&#38450;&#27490;&#26576;&#20123;&#25277;&#35937;&#25512;&#29702;&#38169;&#35823;&#20197;&#21450;&#36991;&#20813;&#22312;&#24605;&#32500;&#38142;&#26465;&#20013;&#22797;&#21046;&#36755;&#20837;&#30340;&#37096;&#20998;&#31561;&#31616;&#21333;&#27169;&#24335;&#26469;&#25913;&#36827;&#29616;&#26377;&#26041;&#27861;&#12290;&#25972;&#20307;&#32780;&#35328;&#65292;&#23545;&#27604;&#35299;&#30721;&#34920;&#29616;&#31361;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
We demonstrate that Contrastive Decoding -- a simple, computationally light, and training-free text generation method proposed by Li et al 2022 -- achieves large out-of-the-box improvements over greedy decoding on a variety of reasoning tasks. Originally shown to improve the perceived quality of long-form text generation, Contrastive Decoding searches for strings that maximize a weighted difference in likelihood between strong and weak models. We show that Contrastive Decoding leads LLaMA-65B to outperform LLaMA 2, GPT-3.5 and PaLM 2-L on the HellaSwag commonsense reasoning benchmark, and to outperform LLaMA 2, GPT-3.5 and PaLM-540B on the GSM8K math word reasoning benchmark, in addition to improvements on a collection of other tasks. Analysis suggests that Contrastive Decoding improves over existing methods by preventing some abstract reasoning errors, as well as by avoiding simpler modes such as copying sections of the input during chain-of-thought. Overall, Contrastive Decoding outp
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31163;&#32447;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#30340;&#25552;&#31034;&#35780;&#20272;&#19982;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#38598;&#21644;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#65292;&#39044;&#27979;&#25552;&#31034;&#24615;&#33021;&#12289;&#25552;&#39640;&#25104;&#26412;&#25928;&#30410;&#12289;&#29983;&#25104;&#26131;&#35835;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.06553</link><description>&lt;p&gt;
&#31163;&#32447;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#19979;&#30340;&#25552;&#31034;&#35780;&#20272;&#19982;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Offline Prompt Evaluation and Optimization with Inverse Reinforcement Learning. (arXiv:2309.06553v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06553
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31163;&#32447;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#30340;&#25552;&#31034;&#35780;&#20272;&#19982;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#38598;&#21644;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#65292;&#39044;&#27979;&#25552;&#31034;&#24615;&#33021;&#12289;&#25552;&#39640;&#25104;&#26412;&#25928;&#30410;&#12289;&#29983;&#25104;&#26131;&#35835;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20687;ChatGPT&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21457;&#23637;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#21033;&#29992;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#20805;&#20998;&#25581;&#31034;LLMs&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#38656;&#35201;&#22312;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#30340;&#24191;&#38420;&#25628;&#32034;&#31354;&#38388;&#20013;&#36827;&#34892;&#23548;&#33322;&#12290;&#34429;&#28982;&#25552;&#31034;&#24037;&#31243;&#26174;&#31034;&#20986;&#28508;&#21147;&#65292;&#20294;&#35797;&#38169;&#23581;&#35797;&#20013;&#25152;&#38656;&#30340;&#20154;&#24037;&#35774;&#35745;&#25552;&#31034;&#21644;&#30456;&#20851;&#25104;&#26412;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#20851;&#38190;&#26159;&#65292;&#25552;&#31034;&#20248;&#21270;&#30340;&#25928;&#29575;&#21462;&#20915;&#20110;&#26114;&#36149;&#30340;&#25552;&#31034;&#35780;&#20272;&#36807;&#31243;&#12290;&#26412;&#24037;&#20316;&#20171;&#32461;&#20102;Prompt-OIRL&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#31163;&#32447;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#24357;&#21512;&#26377;&#25928;&#25552;&#31034;&#35780;&#20272;&#21644;&#21487;&#36127;&#25285;&#24615;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#19987;&#23478;&#35780;&#20272;&#30340;&#31163;&#32447;&#25968;&#25454;&#38598;&#65292;&#36816;&#29992;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#33719;&#24471;&#19968;&#20010;&#38024;&#23545;&#31163;&#32447;&#12289;&#26597;&#35810;&#20381;&#36182;&#22411;&#25552;&#31034;&#35780;&#20272;&#30340;&#22870;&#21169;&#27169;&#22411;&#12290;Prompt-OIRL&#30340;&#20248;&#28857;&#26159;&#22810;&#26041;&#38754;&#30340;&#65306;&#23427;&#39044;&#27979;&#25552;&#31034;&#30340;&#24615;&#33021;&#65292;&#25104;&#26412;&#39640;&#25928;&#65292;&#29983;&#25104;&#26131;&#35835;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent advances in the development of Large Language Models (LLMs) like ChatGPT have achieved remarkable performance by leveraging human expertise. Yet, fully eliciting LLMs' potential for complex tasks requires navigating the vast search space of natural language prompts. While prompt engineering has shown promise, the requisite human-crafted prompts in trial-and-error attempts and the associated costs pose significant challenges. Crucially, the efficiency of prompt optimization hinges on the costly procedure of prompt evaluation. This work introduces Prompt-OIRL, an approach rooted in offline inverse reinforcement learning that seeks to bridge the gap between effective prompt evaluation and affordability. Our method draws on offline datasets from expert evaluations, employing Inverse-RL to derive a reward model for offline, query-dependent prompt evaluations. The advantages of Prompt-OIRL are manifold: it predicts prompt performance, is cost-efficient, produces human-readable res
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25552;&#31034;&#26816;&#32034;&#22686;&#24378;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#22686;&#24378;&#20113;&#20107;&#20214;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#24037;&#20855;&#20013;&#32622;&#20449;&#24230;&#20272;&#35745;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.05833</link><description>&lt;p&gt;
PACE: &#20351;&#29992;GPT-4&#36827;&#34892;&#20113;&#20107;&#20214;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#20013;&#30340;&#25552;&#31034;&#21644;&#22686;&#21152;&#20197;&#36827;&#34892;&#26657;&#20934;&#30340;&#32622;&#20449;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
PACE: Prompting and Augmentation for Calibrated Confidence Estimation with GPT-4 in Cloud Incident Root Cause Analysis. (arXiv:2309.05833v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05833
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25552;&#31034;&#26816;&#32034;&#22686;&#24378;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#22686;&#24378;&#20113;&#20107;&#20214;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#24037;&#20855;&#20013;&#32622;&#20449;&#24230;&#20272;&#35745;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;IT&#34892;&#19994;&#21521;&#22522;&#20110;&#20113;&#30340;&#24179;&#21488;&#30340;&#36716;&#21464;&#24378;&#35843;&#20102;&#20113;&#20107;&#20214;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#30830;&#20445;&#26381;&#21153;&#30340;&#21487;&#38752;&#24615;&#21644;&#32500;&#25252;&#23458;&#25143;&#20449;&#20219;&#12290;&#26680;&#24515;&#38382;&#39064;&#26159;&#26377;&#25928;&#30830;&#23450;&#26681;&#26412;&#21407;&#22240;&#65292;&#30001;&#20110;&#24403;&#20195;&#20113;&#22522;&#30784;&#35774;&#26045;&#30340;&#22797;&#26434;&#24615;&#65292;&#36825;&#19968;&#20219;&#21153;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#23613;&#31649;&#20986;&#29616;&#20102;&#35768;&#22810;&#29992;&#20110;&#26681;&#26412;&#21407;&#22240;&#35782;&#21035;&#30340;&#22522;&#20110;AI&#30340;&#24037;&#20855;&#65292;&#20294;&#23427;&#20204;&#30340;&#36866;&#29992;&#24615;&#20173;&#21463;&#21040;&#20854;&#36755;&#20986;&#36136;&#37327;&#19981;&#19968;&#33268;&#30340;&#38480;&#21046;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#25552;&#31034;&#26816;&#32034;&#22686;&#24378;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#22686;&#24378;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#24037;&#20855;&#20013;&#32622;&#20449;&#24230;&#20272;&#35745;&#30340;&#26041;&#27861;&#12290;&#27492;&#26041;&#27861;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#12290;&#39318;&#20808;&#65292;&#27169;&#22411;&#26681;&#25454;&#21382;&#21490;&#20107;&#20214;&#25968;&#25454;&#35780;&#20272;&#33258;&#36523;&#30340;&#32622;&#20449;&#24230;&#65292;&#32771;&#34385;&#20854;&#23545;&#35777;&#25454;&#30340;&#35780;&#20272;&#24378;&#24230;&#12290;&#28982;&#21518;&#65292;&#27169;&#22411;&#23457;&#26680;&#30001;&#39044;&#27979;&#22120;&#29983;&#25104;&#30340;&#26681;&#26412;&#21407;&#22240;&#12290;&#28982;&#21518;&#65292;&#20248;&#21270;&#27493;&#39588;&#23558;&#36825;&#20123;&#35780;&#20272;&#32467;&#21512;&#36215;&#26469;&#30830;&#23450;&#26368;&#32456;&#30340;&#32622;&#20449;&#24230;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the transition to cloud-based platforms in the IT sector has emphasized the significance of cloud incident root cause analysis to ensure service reliability and maintain customer trust. Central to this process is the efficient determination of root causes, a task made challenging due to the complex nature of contemporary cloud infrastructures. Despite the proliferation of AI-driven tools for root cause identification, their applicability remains limited by the inconsistent quality of their outputs. This paper introduces a method for enhancing confidence estimation in root cause analysis tools by prompting retrieval-augmented large language models (LLMs). This approach operates in two phases. Initially, the model evaluates its confidence based on historical incident data, considering its assessment of the evidence strength. Subsequently, the model reviews the root cause generated by the predictor. An optimization step then combines these evaluations to determine the fin
&lt;/p&gt;</description></item><item><title>Neural-Hidden-CRF&#26159;&#19968;&#31181;&#40065;&#26834;&#30340;&#24369;&#30417;&#30563;&#24207;&#21015;&#26631;&#27880;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#38544;&#34255;CRF&#23618;&#21644;&#21033;&#29992;&#24378;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#22312;&#21508;&#39033;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#26368;&#20248;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.05086</link><description>&lt;p&gt;
Neural-Hidden-CRF: &#19968;&#31181;&#40065;&#26834;&#30340;&#24369;&#30417;&#30563;&#24207;&#21015;&#26631;&#27880;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Neural-Hidden-CRF: A Robust Weakly-Supervised Sequence Labeler. (arXiv:2309.05086v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05086
&lt;/p&gt;
&lt;p&gt;
Neural-Hidden-CRF&#26159;&#19968;&#31181;&#40065;&#26834;&#30340;&#24369;&#30417;&#30563;&#24207;&#21015;&#26631;&#27880;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#38544;&#34255;CRF&#23618;&#21644;&#21033;&#29992;&#24378;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#22312;&#21508;&#39033;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#26368;&#20248;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26080;&#21521;&#22270;&#27169;&#22411; Neural-Hidden-CRF &#26469;&#35299;&#20915;&#24369;&#30417;&#30563;&#24207;&#21015;&#26631;&#27880;&#38382;&#39064;&#12290;&#22312;&#27010;&#29575;&#26080;&#21521;&#22270;&#29702;&#35770;&#30340;&#26694;&#26550;&#19979;&#65292;Neural-Hidden-CRF&#34701;&#21512;&#20102;&#19968;&#20010;&#38544;&#34255;CRF&#23618;&#65292;&#29992;&#20110;&#24314;&#27169;&#21333;&#35789;&#24207;&#21015;&#12289;&#28508;&#22312;&#30495;&#23454;&#24207;&#21015;&#21644;&#24369;&#26631;&#31614;&#24207;&#21015;&#20043;&#38388;&#30340;&#21464;&#37327;&#20851;&#31995;&#24182;&#20855;&#26377;&#20840;&#23616;&#35270;&#35282;&#12290;&#22312;Neural-Hidden-CRF&#20013;&#65292;&#25105;&#20204;&#21487;&#20197;&#21033;&#29992;&#24378;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;BERT&#25110;&#20854;&#20182;&#28145;&#24230;&#27169;&#22411;&#20026;&#28508;&#22312;&#30495;&#23454;&#24207;&#21015;&#25552;&#20379;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#35821;&#20041;&#30693;&#35782;&#65292;&#24182;&#20351;&#29992;&#38544;&#34255;&#30340;CRF&#23618;&#26469;&#25429;&#25417;&#20869;&#37096;&#26631;&#31614;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;Neural-Hidden-CRF&#22312;&#27010;&#24565;&#19978;&#31616;&#21333;&#32780;&#22312;&#23454;&#36341;&#20013;&#24378;&#22823;&#12290;&#23427;&#22312;&#19968;&#20010;&#20247;&#21253;&#22522;&#20934;&#27979;&#35797;&#21644;&#19977;&#20010;&#24369;&#30417;&#30563;&#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#26368;&#20248;&#32467;&#26524;&#65292;&#21253;&#25324;&#22312;&#24179;&#22343;&#27867;&#21270;&#21644;&#25512;&#29702;&#24615;&#33021;&#26041;&#38754;&#36229;&#36807;&#20102;&#26368;&#26032;&#30340;&#39640;&#32423;&#27169;&#22411;CHMM&#30340;2.80 F1&#20998;&#21644;2.23 F1&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a neuralized undirected graphical model called Neural-Hidden-CRF to solve the weakly-supervised sequence labeling problem. Under the umbrella of probabilistic undirected graph theory, the proposed Neural-Hidden-CRF embedded with a hidden CRF layer models the variables of word sequence, latent ground truth sequence, and weak label sequence with the global perspective that undirected graphical models particularly enjoy. In Neural-Hidden-CRF, we can capitalize on the powerful language model BERT or other deep models to provide rich contextual semantic knowledge to the latent ground truth sequence, and use the hidden CRF layer to capture the internal label dependencies. Neural-Hidden-CRF is conceptually simple and empirically powerful. It obtains new state-of-the-art results on one crowdsourcing benchmark and three weak-supervision benchmarks, including outperforming the recent advanced model CHMM by 2.80 F1 points and 2.23 F1 points in average generalization and inference perfo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#19987;&#19994;&#28151;&#38899;&#24072;&#19982;&#23458;&#25143;&#30340;&#20114;&#21160;&#20197;&#21450;&#20182;&#20204;&#22914;&#20309;&#21033;&#29992;&#23458;&#25143;&#30340;&#21453;&#39304;&#25351;&#23548;&#28151;&#38899;&#36807;&#31243;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#28151;&#38899;&#24072;&#36890;&#36807;&#20351;&#29992;&#21442;&#32771;&#26354;&#30446;&#21644;&#21512;&#20316;&#27807;&#36890;&#31561;&#26041;&#24335;&#65292;&#19982;&#23458;&#25143;&#24314;&#31435;&#36215;&#23545;&#28151;&#38899;&#26399;&#26395;&#38899;&#25928;&#30340;&#40664;&#22865;&#32422;&#23450;&#12290;</title><link>http://arxiv.org/abs/2309.03404</link><description>&lt;p&gt;
&#36890;&#35759;&#21644;&#21442;&#32771;&#26354;&#30446;&#22312;&#28151;&#38899;&#36807;&#31243;&#20013;&#30340;&#20316;&#29992;&#65306;&#26469;&#33258;&#19987;&#19994;&#28151;&#38899;&#24072;&#30340;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;
The Role of Communication and Reference Songs in the Mixing Process: Insights from Professional Mix Engineers. (arXiv:2309.03404v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03404
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#19987;&#19994;&#28151;&#38899;&#24072;&#19982;&#23458;&#25143;&#30340;&#20114;&#21160;&#20197;&#21450;&#20182;&#20204;&#22914;&#20309;&#21033;&#29992;&#23458;&#25143;&#30340;&#21453;&#39304;&#25351;&#23548;&#28151;&#38899;&#36807;&#31243;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#28151;&#38899;&#24072;&#36890;&#36807;&#20351;&#29992;&#21442;&#32771;&#26354;&#30446;&#21644;&#21512;&#20316;&#27807;&#36890;&#31561;&#26041;&#24335;&#65292;&#19982;&#23458;&#25143;&#24314;&#31435;&#36215;&#23545;&#28151;&#38899;&#26399;&#26395;&#38899;&#25928;&#30340;&#40664;&#22865;&#32422;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#30340;&#38899;&#20048;&#28151;&#38899;&#38656;&#35201;&#25216;&#26415;&#21644;&#21019;&#36896;&#21147;&#30340;&#31934;&#28251;&#65292;&#20294;&#19982;&#23458;&#25143;&#30340;&#28165;&#26224;&#27807;&#36890;&#33267;&#20851;&#37325;&#35201;&#12290;&#28151;&#38899;&#24072;&#24517;&#39035;&#29702;&#35299;&#23458;&#25143;&#30340;&#26399;&#26395;&#21644;&#20559;&#22909;&#65292;&#24182;&#20849;&#21516;&#21162;&#21147;&#23454;&#29616;&#25152;&#38656;&#30340;&#38899;&#25928;&#12290;&#36890;&#36807;&#20351;&#29992;&#21442;&#32771;&#26354;&#30446;&#21644;&#33402;&#26415;&#23478;&#19982;&#24037;&#31243;&#24072;&#20043;&#38388;&#20132;&#25442;&#30340;&#28436;&#31034;&#28151;&#38899;&#31561;&#25351;&#21335;&#65292;&#36890;&#24120;&#21487;&#20197;&#36798;&#25104;&#23545;&#28151;&#38899;&#26399;&#26395;&#38899;&#25928;&#30340;&#40664;&#22865;&#32422;&#23450;&#65292;&#26377;&#26102;&#36824;&#21487;&#20197;&#20351;&#29992;&#35821;&#20041;&#26415;&#35821;&#36827;&#34892;&#35328;&#35828;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#30001;&#20004;&#20010;&#38454;&#27573;&#26500;&#25104;&#30340;&#25506;&#32034;&#24615;&#30740;&#31350;&#30340;&#21457;&#29616;&#65292;&#26088;&#22312;&#20102;&#35299;&#19987;&#19994;&#28151;&#38899;&#24072;&#22914;&#20309;&#19982;&#23458;&#25143;&#20114;&#21160;&#65292;&#24182;&#21033;&#29992;&#20182;&#20204;&#30340;&#21453;&#39304;&#25351;&#23548;&#28151;&#38899;&#36807;&#31243;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#23545;&#20116;&#21517;&#28151;&#38899;&#24072;&#36827;&#34892;&#20102;&#21322;&#32467;&#26500;&#21270;&#38754;&#35848;&#65292;&#26088;&#22312;&#20102;&#35299;&#20182;&#20204;&#30340;&#27807;&#36890;&#31574;&#30053;&#12289;&#21019;&#36896;&#36807;&#31243;&#21644;&#20915;&#31574;&#26631;&#20934;&#12290;&#22522;&#20110;&#36825;&#20123;&#35775;&#35848;&#30340;&#25512;&#35770;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#22312;&#32447;&#38382;&#21367;&#65292;&#24182;&#23545;22&#21517;&#28151;&#38899;&#24072;&#36827;&#34892;&#20102;&#35843;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;
Effective music mixing requires technical and creative finesse, but clear communication with the client is crucial. The mixing engineer must grasp the client's expectations, and preferences, and collaborate to achieve the desired sound. The tacit agreement for the desired sound of the mix is often established using guides like reference songs and demo mixes exchanged between the artist and the engineer and sometimes verbalised using semantic terms. This paper presents the findings of a two-phased exploratory study aimed at understanding how professional mixing engineers interact with clients and use their feedback to guide the mixing process. For phase one, semi-structured interviews were conducted with five mixing engineers with the aim of gathering insights about their communication strategies, creative processes, and decision-making criteria. Based on the inferences from these interviews, an online questionnaire was designed and administered to a larger group of 22 mixing engineers 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#35757;&#32451;&#30340;&#31070;&#32463;&#25512;&#33616;&#31995;&#32479;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#26032;&#39046;&#22495;&#20013;&#26500;&#24314;&#25512;&#33616;&#31995;&#32479;&#65292;&#20943;&#23569;&#25110;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#20351;&#29992;&#20219;&#20309;&#36741;&#21161;&#20449;&#24687;&#12290;&#36890;&#36807;&#21033;&#29992;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#30697;&#38453;&#30340;&#32479;&#35745;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#38646;&#26679;&#26412;&#25512;&#33616;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.01188</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#31070;&#32463;&#25512;&#33616;&#31995;&#32479;&#65306;&#19968;&#31181;&#21487;&#36801;&#31227;&#30340;&#38646;&#26679;&#26412;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Pre-trained Neural Recommenders: A Transferable Zero-Shot Framework for Recommendation Systems. (arXiv:2309.01188v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01188
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#35757;&#32451;&#30340;&#31070;&#32463;&#25512;&#33616;&#31995;&#32479;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#26032;&#39046;&#22495;&#20013;&#26500;&#24314;&#25512;&#33616;&#31995;&#32479;&#65292;&#20943;&#23569;&#25110;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#20351;&#29992;&#20219;&#20309;&#36741;&#21161;&#20449;&#24687;&#12290;&#36890;&#36807;&#21033;&#29992;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#30697;&#38453;&#30340;&#32479;&#35745;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#38646;&#26679;&#26412;&#25512;&#33616;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#31070;&#32463;&#21327;&#21516;&#36807;&#28388;&#25216;&#26415;&#23545;&#20110;&#30005;&#23376;&#21830;&#21153;&#12289;&#31038;&#20132;&#23186;&#20307;&#21644;&#20869;&#23481;&#20849;&#20139;&#24179;&#21488;&#30340;&#25104;&#21151;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#25216;&#26415;&#26377;&#25152;&#36827;&#27493;&#65292;&#20294;&#23545;&#20110;&#27599;&#20010;&#26032;&#30340;&#24212;&#29992;&#39046;&#22495;&#65292;&#25105;&#20204;&#20173;&#38656;&#35201;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#19968;&#20010;NCF&#27169;&#22411;&#12290;&#30456;&#21453;&#65292;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#30452;&#25509;&#24212;&#29992;&#20110;&#21508;&#31181;&#24212;&#29992;&#31243;&#24207;&#65292;&#35201;&#20040;&#26159;&#38646;&#26679;&#26412;&#24773;&#20917;&#65292;&#35201;&#20040;&#26159;&#26377;&#38480;&#30340;&#24494;&#35843;&#12290;&#21463;&#21040;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#22312;&#26032;&#39046;&#22495;&#20013;&#25903;&#25345;&#26500;&#24314;&#25512;&#33616;&#31995;&#32479;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21487;&#33021;&#24615;&#65292;&#21482;&#38656;&#26368;&#23569;&#25110;&#19981;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#65292;&#32780;&#26080;&#38656;&#20351;&#29992;&#20219;&#20309;&#36741;&#21161;&#29992;&#25143;&#25110;&#39033;&#30446;&#20449;&#24687;&#12290;&#38646;&#26679;&#26412;&#25512;&#33616;&#22312;&#27809;&#26377;&#36741;&#21161;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#24403;&#27809;&#26377;&#37325;&#21472;&#30340;&#29992;&#25143;&#25110;&#39033;&#30446;&#26102;&#65292;&#25105;&#20204;&#26080;&#27861;&#22312;&#25968;&#25454;&#38598;&#20043;&#38388;&#24314;&#31435;&#29992;&#25143;&#21644;&#39033;&#30446;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#25105;&#20204;&#30340;&#22522;&#26412;&#35265;&#35299;&#26159;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#30697;&#38453;&#30340;&#32479;&#35745;&#29305;&#24449;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#26222;&#36941;&#21487;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern neural collaborative filtering techniques are critical to the success of e-commerce, social media, and content-sharing platforms. However, despite technical advances -- for every new application domain, we need to train an NCF model from scratch. In contrast, pre-trained vision and language models are routinely applied to diverse applications directly (zero-shot) or with limited fine-tuning. Inspired by the impact of pre-trained models, we explore the possibility of pre-trained recommender models that support building recommender systems in new domains, with minimal or no retraining, without the use of any auxiliary user or item information. Zero-shot recommendation without auxiliary information is challenging because we cannot form associations between users and items across datasets when there are no overlapping users or items. Our fundamental insight is that the statistical characteristics of the user-item interaction matrix are universally available across different domains 
&lt;/p&gt;</description></item><item><title>Jais&#21644;Jais-chat&#26159;&#26032;&#30340;&#20197;&#38463;&#25289;&#20271;&#35821;&#20026;&#20013;&#24515;&#30340;&#24320;&#25918;&#24335;&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#26377;13&#20159;&#21442;&#25968;&#65292;&#22312;&#38463;&#25289;&#20271;&#35821;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#19988;&#22312;&#33521;&#35821;&#26041;&#38754;&#20063;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#36825;&#20123;&#27169;&#22411;&#30340;&#21457;&#24067;&#26088;&#22312;&#20419;&#36827;&#38463;&#25289;&#20271;&#35821;LLMs&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2308.16149</link><description>&lt;p&gt;
Jais&#21644;Jais-chat&#65306;&#20197;&#38463;&#25289;&#20271;&#35821;&#20026;&#20013;&#24515;&#30340;&#22522;&#30784;&#21644;&#25351;&#23548;&#35843;&#20248;&#30340;&#24320;&#25918;&#24335;&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Jais and Jais-chat: Arabic-Centric Foundation and Instruction-Tuned Open Generative Large Language Models. (arXiv:2308.16149v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16149
&lt;/p&gt;
&lt;p&gt;
Jais&#21644;Jais-chat&#26159;&#26032;&#30340;&#20197;&#38463;&#25289;&#20271;&#35821;&#20026;&#20013;&#24515;&#30340;&#24320;&#25918;&#24335;&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#26377;13&#20159;&#21442;&#25968;&#65292;&#22312;&#38463;&#25289;&#20271;&#35821;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#19988;&#22312;&#33521;&#35821;&#26041;&#38754;&#20063;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#36825;&#20123;&#27169;&#22411;&#30340;&#21457;&#24067;&#26088;&#22312;&#20419;&#36827;&#38463;&#25289;&#20271;&#35821;LLMs&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Jais&#21644;Jais-chat&#65292;&#36825;&#26159;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#20197;&#38463;&#25289;&#20271;&#35821;&#20026;&#20013;&#24515;&#30340;&#22522;&#30784;&#21644;&#25351;&#23548;&#35843;&#20248;&#30340;&#24320;&#25918;&#24335;&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;&#36825;&#20123;&#27169;&#22411;&#22522;&#20110;GPT-3&#30340;&#20165;&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#24182;&#22312;&#38463;&#25289;&#20271;&#35821;&#21644;&#33521;&#35821;&#25991;&#26412;&#30340;&#28151;&#21512;&#29289;&#20013;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#21253;&#25324;&#21508;&#31181;&#32534;&#31243;&#35821;&#35328;&#30340;&#28304;&#20195;&#30721;&#12290;&#36825;&#20123;&#27169;&#22411;&#20855;&#26377;130&#20159;&#20010;&#21442;&#25968;&#65292;&#26681;&#25454;&#24191;&#27867;&#30340;&#35780;&#20272;&#32467;&#26524;&#65292;&#22312;&#38463;&#25289;&#20271;&#35821;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#20219;&#20309;&#29616;&#26377;&#30340;&#24320;&#25918;&#24335;&#38463;&#25289;&#20271;&#35821;&#21644;&#22810;&#35821;&#35328;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;&#22312;&#35757;&#32451;&#26102;&#20351;&#29992;&#30340;&#33521;&#35821;&#25968;&#25454;&#35201;&#23569;&#24471;&#22810;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#22312;&#33521;&#35821;&#26041;&#38754;&#19982;&#31867;&#20284;&#35268;&#27169;&#30340;&#20197;&#33521;&#35821;&#20026;&#20013;&#24515;&#30340;&#24320;&#25918;&#27169;&#22411;&#30456;&#27604;&#20173;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#27169;&#22411;&#30340;&#35757;&#32451;&#12289;&#35843;&#20248;&#12289;&#23433;&#20840;&#23545;&#40784;&#21644;&#35780;&#20272;&#30340;&#35814;&#32454;&#25551;&#36848;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#27169;&#22411;&#30340;&#20004;&#20010;&#24320;&#25918;&#29256;&#26412;--&#22522;&#30784;Jais&#27169;&#22411;&#21644;&#25351;&#23548;&#35843;&#20248;&#30340;Jais-chat&#21464;&#31181;--&#26088;&#22312;&#20419;&#36827;&#38463;&#25289;&#20271;&#35821;LLMs&#30340;&#30740;&#31350;&#12290;&#35814;&#35265;https://hugging
&lt;/p&gt;
&lt;p&gt;
We introduce Jais and Jais-chat, new state-of-the-art Arabic-centric foundation and instruction-tuned open generative large language models (LLMs). The models are based on the GPT-3 decoder-only architecture and are pretrained on a mixture of Arabic and English texts, including source code in various programming languages. With 13 billion parameters, they demonstrate better knowledge and reasoning capabilities in Arabic than any existing open Arabic and multilingual models by a sizable margin, based on extensive evaluation. Moreover, the models are competitive in English compared to English-centric open models of similar size, despite being trained on much less English data. We provide a detailed description of the training, the tuning, the safety alignment, and the evaluation of the models. We release two open versions of the model -- the foundation Jais model, and an instruction-tuned Jais-chat variant -- with the aim of promoting research on Arabic LLMs. Available at https://hugging
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23610;&#24230;&#19981;&#21464;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#65288;SI-MTL&#65289;&#65292;&#36890;&#36807;&#23545;&#20219;&#21153;&#25439;&#22833;&#36827;&#34892;&#23545;&#25968;&#21464;&#25442;&#21644;&#23545;&#20219;&#21153;&#26799;&#24230;&#36827;&#34892;&#24402;&#19968;&#21270;&#65292;&#35299;&#20915;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#20219;&#21153;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#39046;&#20808;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.12029</link><description>&lt;p&gt;
&#19968;&#31181;&#38024;&#23545;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#23610;&#24230;&#19981;&#21464;&#20219;&#21153;&#24179;&#34913;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Scale-Invariant Task Balancing Approach for Multi-Task Learning. (arXiv:2308.12029v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12029
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23610;&#24230;&#19981;&#21464;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#65288;SI-MTL&#65289;&#65292;&#36890;&#36807;&#23545;&#20219;&#21153;&#25439;&#22833;&#36827;&#34892;&#23545;&#25968;&#21464;&#25442;&#21644;&#23545;&#20219;&#21153;&#26799;&#24230;&#36827;&#34892;&#24402;&#19968;&#21270;&#65292;&#35299;&#20915;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#20219;&#21153;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#39046;&#20808;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#26159;&#19968;&#31181;&#21516;&#26102;&#23398;&#20064;&#22810;&#20010;&#30456;&#20851;&#20219;&#21153;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#20219;&#21153;&#24179;&#34913;&#20173;&#28982;&#26159;MTL&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#65292;&#25439;&#22833;/&#26799;&#24230;&#23610;&#24230;&#30340;&#19981;&#24179;&#34913;&#32463;&#24120;&#23548;&#33268;&#24615;&#33021;&#25240;&#20013;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23610;&#24230;&#19981;&#21464;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;SI-MTL&#65289;&#26041;&#27861;&#65292;&#20174;&#25439;&#22833;&#21644;&#26799;&#24230;&#35282;&#24230;&#32531;&#35299;&#20102;&#20219;&#21153;&#24179;&#34913;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;SI-MTL&#21253;&#21547;&#23545;&#25152;&#26377;&#20219;&#21153;&#25439;&#22833;&#36827;&#34892;&#30340;&#23545;&#25968;&#21464;&#25442;&#65292;&#20197;&#30830;&#20445;&#22312;&#25439;&#22833;&#27700;&#24179;&#19978;&#20855;&#26377;&#23610;&#24230;&#19981;&#21464;&#24615;&#65292;&#20197;&#21450;&#19968;&#31181;&#26799;&#24230;&#24179;&#34913;&#26041;&#27861;SI-G&#65292;&#23427;&#23558;&#25152;&#26377;&#20219;&#21153;&#30340;&#26799;&#24230;&#24402;&#19968;&#21270;&#20026;&#19982;&#26368;&#22823;&#26799;&#24230;&#33539;&#25968;&#30456;&#21516;&#30340;&#22823;&#23567;&#12290;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#19968;&#33268;&#35777;&#26126;&#20102;SI-G&#30340;&#26377;&#25928;&#24615;&#21644;SI-MTL&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-task learning (MTL), a learning paradigm to learn multiple related tasks simultaneously, has achieved great success in various fields. However, task-balancing remains a significant challenge in MTL, with the disparity in loss/gradient scales often leading to performance compromises. In this paper, we propose a Scale-Invariant Multi-Task Learning (SI-MTL) method to alleviate the task-balancing problem from both loss and gradient perspectives. Specifically, SI-MTL contains a logarithm transformation which is performed on all task losses to ensure scale-invariant at the loss level, and a gradient balancing method, SI-G, which normalizes all task gradients to the same magnitude as the maximum gradient norm. Extensive experiments conducted on several benchmark datasets consistently demonstrate the effectiveness of SI-G and the state-of-the-art performance of SI-MTL.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#36752;&#23556;&#22330;&#21644;&#31232;&#30095;&#30340;&#25668;&#20687;&#26426;&#32452;&#26469;&#37325;&#24314;&#21160;&#24577;&#20154;&#20307;&#36816;&#21160;&#21644;&#24418;&#29366;&#30340;&#26041;&#27861;&#12290;&#19982;&#29616;&#26377;&#30340;&#35282;&#33394;&#27169;&#22411;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#39057;&#22495;&#20013;&#26159;&#33258;&#36866;&#24212;&#21644;&#26174;&#24335;&#30340;&#65292;&#24182;&#36890;&#36807;&#24314;&#27169;&#36523;&#20307;&#37096;&#20301;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#26469;&#35299;&#20915;&#34915;&#29289;&#21644;&#30382;&#32932;&#30340;&#21464;&#24418;&#24314;&#27169;&#25361;&#25112;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#32593;&#32476;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.11951</link><description>&lt;p&gt;
&#20174;&#35270;&#39057;&#20013;&#29983;&#25104;&#23039;&#21183;&#35843;&#33410;&#30340;&#34394;&#25311;&#35282;&#33394;
&lt;/p&gt;
&lt;p&gt;
Pose Modulated Avatars from Video. (arXiv:2308.11951v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11951
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#36752;&#23556;&#22330;&#21644;&#31232;&#30095;&#30340;&#25668;&#20687;&#26426;&#32452;&#26469;&#37325;&#24314;&#21160;&#24577;&#20154;&#20307;&#36816;&#21160;&#21644;&#24418;&#29366;&#30340;&#26041;&#27861;&#12290;&#19982;&#29616;&#26377;&#30340;&#35282;&#33394;&#27169;&#22411;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#39057;&#22495;&#20013;&#26159;&#33258;&#36866;&#24212;&#21644;&#26174;&#24335;&#30340;&#65292;&#24182;&#36890;&#36807;&#24314;&#27169;&#36523;&#20307;&#37096;&#20301;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#26469;&#35299;&#20915;&#34915;&#29289;&#21644;&#30382;&#32932;&#30340;&#21464;&#24418;&#24314;&#27169;&#25361;&#25112;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#32593;&#32476;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#30001;&#39592;&#26550;&#39537;&#21160;&#30340;&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRF&#65289;&#21644;&#31232;&#30095;&#30340;&#25668;&#20687;&#26426;&#32452;&#21487;&#20197;&#37325;&#24314;&#21160;&#24577;&#20154;&#20307;&#36816;&#21160;&#21644;&#24418;&#29366;&#12290;&#28982;&#32780;&#65292;&#27169;&#25311;&#34915;&#29289;&#21644;&#30382;&#32932;&#30340;&#21464;&#24418;&#19982;&#39592;&#26550;&#23039;&#21183;&#20043;&#38388;&#30340;&#20851;&#31995;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#19982;&#38544;&#24335;&#23398;&#20064;&#25110;&#20381;&#36182;&#20195;&#29702;&#34920;&#38754;&#30340;&#29616;&#26377;&#35282;&#33394;&#27169;&#22411;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#28304;&#20110;&#35266;&#23519;&#21040;&#19981;&#21516;&#30340;&#23039;&#21183;&#38656;&#35201;&#19981;&#21516;&#30340;&#39057;&#29575;&#20998;&#37197;&#12290;&#24573;&#35270;&#36825;&#31181;&#21306;&#21035;&#20250;&#22312;&#24179;&#28369;&#21306;&#22495;&#20135;&#29983;&#22122;&#28857;&#20266;&#24433;&#65292;&#25110;&#20351;&#38160;&#21033;&#21306;&#22495;&#30340;&#32454;&#31890;&#24230;&#32441;&#29702;&#21644;&#24418;&#29366;&#32454;&#33410;&#27169;&#31946;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22312;&#39057;&#22495;&#20013;&#26159;&#33258;&#36866;&#24212;&#21644;&#26174;&#24335;&#30340;&#21452;&#20998;&#25903;&#31070;&#32463;&#32593;&#32476;&#12290;&#31532;&#19968;&#20010;&#20998;&#25903;&#26159;&#19968;&#20010;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#26412;&#22320;&#24314;&#27169;&#36523;&#20307;&#37096;&#20301;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#20197;&#39592;&#26550;&#23039;&#21183;&#20316;&#20026;&#36755;&#20837;&#12290;&#31532;&#20108;&#20010;&#20998;&#25903;&#23558;&#36825;&#20123;&#30456;&#20851;&#29305;&#24449;&#32452;&#21512;&#21040;&#19968;&#32452;&#20840;&#23616;&#39057;&#29575;&#20013;&#65292;&#28982;&#21518;&#35843;&#33410;&#29305;&#24449;&#32534;&#30721;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#32593;&#32476;&#20248;&#20110;&#29616;&#26377;&#30340;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is now possible to reconstruct dynamic human motion and shape from a sparse set of cameras using Neural Radiance Fields (NeRF) driven by an underlying skeleton. However, a challenge remains to model the deformation of cloth and skin in relation to skeleton pose. Unlike existing avatar models that are learned implicitly or rely on a proxy surface, our approach is motivated by the observation that different poses necessitate unique frequency assignments. Neglecting this distinction yields noisy artifacts in smooth areas or blurs fine-grained texture and shape details in sharp regions. We develop a two-branch neural network that is adaptive and explicit in the frequency domain. The first branch is a graph neural network that models correlations among body parts locally, taking skeleton pose as input. The second branch combines these correlation features to a set of global frequencies and then modulates the feature encoding. Our experiments demonstrate that our network outperforms state
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#21453;&#21521;&#25512;&#29702;&#36827;&#34892;&#39564;&#35777;&#30340;&#26041;&#27861;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#23631;&#34109;&#38382;&#39064;&#20013;&#30340;&#19968;&#20010;&#26631;&#35760;&#65292;&#24182;&#35201;&#27714;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#34987;&#23631;&#34109;&#30340;&#26631;&#35760;&#26469;&#39564;&#35777;&#20505;&#36873;&#31572;&#26696;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#27491;&#21521;&#21644;&#21453;&#21521;&#25512;&#29702;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#20505;&#36873;&#31572;&#26696;&#30340;&#27010;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.07758</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#21453;&#21521;&#25512;&#29702;&#36827;&#34892;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Backward Reasoning in Large Language Models for Verification. (arXiv:2308.07758v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07758
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#21453;&#21521;&#25512;&#29702;&#36827;&#34892;&#39564;&#35777;&#30340;&#26041;&#27861;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#23631;&#34109;&#38382;&#39064;&#20013;&#30340;&#19968;&#20010;&#26631;&#35760;&#65292;&#24182;&#35201;&#27714;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#34987;&#23631;&#34109;&#30340;&#26631;&#35760;&#26469;&#39564;&#35777;&#20505;&#36873;&#31572;&#26696;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#27491;&#21521;&#21644;&#21453;&#21521;&#25512;&#29702;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#20505;&#36873;&#31572;&#26696;&#30340;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38142;&#24335;&#24605;&#32771;&#65288;Chain-of-Though, CoT&#65289;&#25552;&#31034;&#22312;&#21508;&#31181;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;&#26368;&#36817;&#65292;Self-Consistency&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#37319;&#26679;&#19968;&#32452;&#19981;&#21516;&#30340;&#25512;&#29702;&#38142;&#65292;&#36825;&#20123;&#38142;&#21487;&#33021;&#23548;&#33268;&#19981;&#21516;&#30340;&#31572;&#26696;&#65292;&#28982;&#21518;&#36873;&#25321;&#24471;&#31080;&#26368;&#22810;&#30340;&#31572;&#26696;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;&#22312;&#39564;&#35777;&#20505;&#36873;&#31572;&#26696;&#26102;&#20351;&#29992;&#21453;&#21521;&#25512;&#29702;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#31616;&#21333;&#30340;&#27169;&#26495;&#65292;&#21363;``&#22914;&#26524;&#25105;&#20204;&#30693;&#36947;&#19978;&#36848;&#38382;&#39064;&#30340;&#31572;&#26696;&#26159;&#20505;&#36873;&#31572;&#26696;&#65292;&#37027;&#20040;&#26410;&#30693;&#21464;&#37327;x&#30340;&#20540;&#26159;&#22810;&#23569;&#65311;''&#65292;&#23558;&#38382;&#39064;&#20013;&#30340;&#19968;&#20010;&#26631;&#35760;&#23631;&#34109;&#65292;&#24182;&#35201;&#27714;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#34987;&#23631;&#34109;&#30340;&#26631;&#35760;&#12290;&#30452;&#35266;&#19978;&#35762;&#65292;&#22914;&#26524;&#25552;&#20379;&#30340;&#20505;&#36873;&#31572;&#26696;&#26159;&#27491;&#30830;&#30340;&#65292;&#35821;&#35328;&#27169;&#22411;&#24212;&#35813;&#33021;&#22815;&#25104;&#21151;&#39044;&#27979;&#34987;&#23631;&#34109;&#30340;&#26631;&#35760;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;FOBAR&#26041;&#27861;&#65292;&#23558;&#27491;&#21521;&#21644;&#21453;&#21521;&#25512;&#29702;&#32467;&#21512;&#36215;&#26469;&#20272;&#35745;&#20505;&#36873;&#31572;&#26696;&#30340;&#27010;&#29575;&#12290;&#25105;&#20204;&#22312;&#20845;&#20010;&#25968;&#25454;&#38598;&#21644;&#19977;&#20010;&#23454;&#39564;&#20013;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Though (CoT) prompting has shown promising performance in various reasoning tasks. Recently, Self-Consistency \citep{wang2023selfconsistency} proposes to sample a diverse set of reasoning chains which may lead to different answers while the answer that receives the most votes is selected. In this paper, we propose a novel method to use backward reasoning in verifying candidate answers. We mask a token in the question by ${\bf x}$ and ask the LLM to predict the masked token when a candidate answer is provided by \textit{a simple template}, i.e., ``\textit{\textbf{If we know the answer of the above question is \{a candidate answer\}, what is the value of unknown variable ${\bf x}$?}}'' Intuitively, the LLM is expected to predict the masked token successfully if the provided candidate answer is correct. We further propose FOBAR to combine forward and backward reasoning for estimating the probability of candidate answers. We conduct extensive experiments on six data sets and three
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22240;&#26524;&#24615;&#27010;&#29575;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#26426;&#22120;&#20154;&#22534;&#31215;&#26041;&#22359;&#20219;&#21153;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#32467;&#21512;&#22240;&#26524;&#25512;&#26029;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#29702;&#35299;&#12289;&#25512;&#29702;&#21644;&#35299;&#37322;&#20854;&#29615;&#22659;&#12290;</title><link>http://arxiv.org/abs/2308.06203</link><description>&lt;p&gt;
&#20026;&#26426;&#22120;&#20154;&#22534;&#31215;&#26041;&#22359;&#20219;&#21153;&#26500;&#24314;&#22240;&#26524;&#24615;&#27010;&#29575;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Towards a Causal Probabilistic Framework for Prediction, Action-Selection &amp; Explanations for Robot Block-Stacking Tasks. (arXiv:2308.06203v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06203
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22240;&#26524;&#24615;&#27010;&#29575;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#26426;&#22120;&#20154;&#22534;&#31215;&#26041;&#22359;&#20219;&#21153;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#32467;&#21512;&#22240;&#26524;&#25512;&#26029;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#29702;&#35299;&#12289;&#25512;&#29702;&#21644;&#35299;&#37322;&#20854;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#24847;&#21619;&#30528;&#31995;&#32479;&#35774;&#35745;&#32773;&#26080;&#27861;&#39044;&#27979;&#24182;&#26126;&#30830;&#35774;&#35745;&#20986;&#26426;&#22120;&#20154;&#21487;&#33021;&#36935;&#21040;&#30340;&#25152;&#26377;&#22330;&#26223;&#12290;&#22240;&#27492;&#65292;&#20197;&#36825;&#31181;&#26041;&#24335;&#35774;&#35745;&#30340;&#26426;&#22120;&#20154;&#22312;&#39640;&#24230;&#21463;&#25511;&#30340;&#29615;&#22659;&#20043;&#22806;&#23481;&#26131;&#20986;&#29616;&#25925;&#38556;&#12290;&#22240;&#26524;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#20010;&#21407;&#21017;&#24615;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#32534;&#30721;&#26426;&#22120;&#20154;&#19982;&#20854;&#29615;&#22659;&#30456;&#20114;&#20316;&#29992;&#30340;&#22240;&#26524;&#20851;&#31995;&#30340;&#24418;&#24335;&#21270;&#30693;&#35782;&#65292;&#24182;&#32467;&#21512;&#29616;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#36890;&#24120;&#36935;&#21040;&#30340;&#22122;&#22768;&#21644;&#19981;&#30830;&#23450;&#24615;&#30340;&#27010;&#29575;&#34920;&#31034;&#12290;&#32467;&#21512;&#22240;&#26524;&#25512;&#26029;&#65292;&#36825;&#20123;&#27169;&#22411;&#20351;&#33258;&#20027;&#20195;&#29702;&#33021;&#22815;&#29702;&#35299;&#12289;&#25512;&#29702;&#21644;&#35299;&#37322;&#20854;&#29615;&#22659;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#26426;&#22120;&#20154;&#22534;&#31215;&#26041;&#22359;&#20219;&#21153;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#23637;&#31034;&#20102;&#35768;&#22810;&#24212;&#29992;&#25152;&#38656;&#30340;&#22522;&#26412;&#24863;&#30693;&#21644;&#25805;&#20316;&#33021;&#21147;&#65292;&#21253;&#25324;&#20179;&#24211;&#29289;&#27969;&#21644;&#23478;&#24237;&#20154;&#24037;&#25903;&#25345;&#26426;&#22120;&#20154;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22240;&#26524;&#24615;&#27010;&#29575;&#26694;&#26550;&#65292;&#23558;&#29289;&#29702;&#27169;&#25311;&#21151;&#33021;&#23884;&#20837;&#21040;&#36825;&#20010;&#20219;&#21153;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Uncertainties in the real world mean that is impossible for system designers to anticipate and explicitly design for all scenarios that a robot might encounter. Thus, robots designed like this are fragile and fail outside of highly-controlled environments. Causal models provide a principled framework to encode formal knowledge of the causal relationships that govern the robot's interaction with its environment, in addition to probabilistic representations of noise and uncertainty typically encountered by real-world robots. Combined with causal inference, these models permit an autonomous agent to understand, reason about, and explain its environment. In this work, we focus on the problem of a robot block-stacking task due to the fundamental perception and manipulation capabilities it demonstrates, required by many applications including warehouse logistics and domestic human support robotics. We propose a novel causal probabilistic framework to embed a physics simulation capability int
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#26102;&#38388;&#24120;&#35782;&#25512;&#29702;&#39046;&#22495;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#37325;&#28857;&#20851;&#27880;&#36890;&#36807;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#26469;&#25552;&#39640;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#23545;&#22810;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22686;&#24378;&#27169;&#22411;&#20173;&#28982;&#38590;&#20197;&#36798;&#21040;&#20154;&#31867;&#27700;&#24179;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.00002</link><description>&lt;p&gt;
&#26102;&#38388;&#24120;&#35782;&#25512;&#29702;&#19982;&#33719;&#21462;&#27010;&#36848;
&lt;/p&gt;
&lt;p&gt;
An Overview Of Temporal Commonsense Reasoning and Acquisition. (arXiv:2308.00002v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00002
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#26102;&#38388;&#24120;&#35782;&#25512;&#29702;&#39046;&#22495;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#37325;&#28857;&#20851;&#27880;&#36890;&#36807;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#26469;&#25552;&#39640;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#23545;&#22810;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22686;&#24378;&#27169;&#22411;&#20173;&#28982;&#38590;&#20197;&#36798;&#21040;&#20154;&#31867;&#27700;&#24179;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24120;&#35782;&#25512;&#29702;&#26159;&#25351;&#29702;&#35299;&#30701;&#35821;&#12289;&#21160;&#20316;&#21644;&#20107;&#20214;&#30340;&#20856;&#22411;&#26102;&#38388;&#32972;&#26223;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#38656;&#35201;&#36825;&#31181;&#30693;&#35782;&#30340;&#38382;&#39064;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#36825;&#31181;&#33021;&#21147;&#22312;&#26102;&#38388;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#21487;&#33021;&#24212;&#29992;&#20110;&#26102;&#38388;&#32447;&#25688;&#35201;&#12289;&#26102;&#38388;&#38382;&#31572;&#21644;&#26102;&#38388;&#33258;&#28982;&#35821;&#35328;&#25512;&#26029;&#31561;&#26041;&#38754;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34429;&#28982;&#21892;&#20110;&#29983;&#25104;&#35821;&#27861;&#27491;&#30830;&#30340;&#21477;&#23376;&#21644;&#35299;&#20915;&#20998;&#31867;&#20219;&#21153;&#65292;&#20294;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#24448;&#24448;&#20250;&#37319;&#21462;&#25463;&#24452;&#65292;&#24182;&#38519;&#20837;&#31616;&#21333;&#30340;&#35821;&#35328;&#38519;&#38449;&#12290;&#26412;&#25991;&#31456;&#27010;&#36848;&#20102;&#22312;&#26102;&#38388;&#24120;&#35782;&#25512;&#29702;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#29305;&#21035;&#20851;&#27880;&#36890;&#36807;&#21508;&#31181;&#22686;&#24378;&#26041;&#24335;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#20197;&#21450;&#23545;&#36234;&#26469;&#36234;&#22810;&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22686;&#24378;&#27169;&#22411;&#22312;&#25512;&#29702;&#20219;&#21153;&#19978;&#20173;&#28982;&#38590;&#20197;&#36798;&#21040;&#20154;&#31867;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal commonsense reasoning refers to the ability to understand the typical temporal context of phrases, actions, and events, and use it to reason over problems requiring such knowledge. This trait is essential in temporal natural language processing tasks, with possible applications such as timeline summarization, temporal question answering, and temporal natural language inference. Recent research on the performance of large language models suggests that, although they are adept at generating syntactically correct sentences and solving classification tasks, they often take shortcuts in their reasoning and fall prey to simple linguistic traps. This article provides an overview of research in the domain of temporal commonsense reasoning, particularly focusing on enhancing language model performance through a variety of augmentations and their evaluation across a growing number of datasets. However, these augmented models still struggle to approach human performance on reasoning task
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;XSkill&#65292;&#19968;&#31181;&#36328;&#20307;&#29616;&#30340;&#25216;&#33021;&#21457;&#29616;&#26694;&#26550;&#65292;&#33021;&#22815;&#20174;&#26080;&#26631;&#31614;&#30340;&#20154;&#31867;&#21644;&#26426;&#22120;&#20154;&#25805;&#32437;&#35270;&#39057;&#20013;&#32431;&#31929;&#22320;&#21457;&#29616;&#36328;&#20307;&#29616;&#25216;&#33021;&#21407;&#22411;&#65292;&#24182;&#36890;&#36807;&#26465;&#20214;&#25193;&#25955;&#31574;&#30053;&#23558;&#36825;&#20123;&#25216;&#33021;&#36716;&#31227;&#21040;&#26426;&#22120;&#20154;&#21160;&#20316;&#20013;&#65292;&#22312;&#26410;&#35265;&#20219;&#21153;&#20013;&#23436;&#25104;&#23398;&#20064;&#21040;&#30340;&#25216;&#33021;&#30340;&#32452;&#21512;&#12290;&#20223;&#30495;&#21644;&#30495;&#23454;&#29615;&#22659;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#21457;&#29616;&#30340;&#25216;&#33021;&#21407;&#22411;&#33021;&#22815;&#26377;&#25928;&#22320;&#20419;&#36827;&#25216;&#33021;&#36716;&#31227;&#21644;&#32452;&#21512;&#65292;&#20174;&#32780;&#26500;&#24314;&#20986;&#26356;&#36890;&#29992;&#21644;&#21487;&#25193;&#23637;&#30340;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2307.09955</link><description>&lt;p&gt;
XSkill&#65306;&#36328;&#20307;&#29616;&#25216;&#33021;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
XSkill: Cross Embodiment Skill Discovery. (arXiv:2307.09955v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09955
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;XSkill&#65292;&#19968;&#31181;&#36328;&#20307;&#29616;&#30340;&#25216;&#33021;&#21457;&#29616;&#26694;&#26550;&#65292;&#33021;&#22815;&#20174;&#26080;&#26631;&#31614;&#30340;&#20154;&#31867;&#21644;&#26426;&#22120;&#20154;&#25805;&#32437;&#35270;&#39057;&#20013;&#32431;&#31929;&#22320;&#21457;&#29616;&#36328;&#20307;&#29616;&#25216;&#33021;&#21407;&#22411;&#65292;&#24182;&#36890;&#36807;&#26465;&#20214;&#25193;&#25955;&#31574;&#30053;&#23558;&#36825;&#20123;&#25216;&#33021;&#36716;&#31227;&#21040;&#26426;&#22120;&#20154;&#21160;&#20316;&#20013;&#65292;&#22312;&#26410;&#35265;&#20219;&#21153;&#20013;&#23436;&#25104;&#23398;&#20064;&#21040;&#30340;&#25216;&#33021;&#30340;&#32452;&#21512;&#12290;&#20223;&#30495;&#21644;&#30495;&#23454;&#29615;&#22659;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#21457;&#29616;&#30340;&#25216;&#33021;&#21407;&#22411;&#33021;&#22815;&#26377;&#25928;&#22320;&#20419;&#36827;&#25216;&#33021;&#36716;&#31227;&#21644;&#32452;&#21512;&#65292;&#20174;&#32780;&#26500;&#24314;&#20986;&#26356;&#36890;&#29992;&#21644;&#21487;&#25193;&#23637;&#30340;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#31034;&#33539;&#35270;&#39057;&#26159;&#26426;&#22120;&#20154;&#23398;&#20064;&#30340;&#24191;&#27867;&#25968;&#25454;&#28304;&#65292;&#24182;&#19988;&#26159;&#34920;&#36798;&#25152;&#38656;&#34892;&#20026;&#30340;&#30452;&#35266;&#29992;&#25143;&#30028;&#38754;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#20174;&#38750;&#32467;&#26500;&#21270;&#30340;&#20154;&#31867;&#35270;&#39057;&#20013;&#25552;&#21462;&#21487;&#37325;&#29992;&#30340;&#26426;&#22120;&#20154;&#25805;&#32437;&#25216;&#33021;&#38754;&#20020;&#30528;&#20307;&#29616;&#24046;&#24322;&#21644;&#26410;&#35266;&#23519;&#21040;&#30340;&#34892;&#21160;&#21442;&#25968;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#31181;&#20307;&#29616;&#24046;&#36317;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;XSkill&#65292;&#19968;&#31181;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#20174;&#26080;&#26631;&#31614;&#30340;&#20154;&#31867;&#21644;&#26426;&#22120;&#20154;&#25805;&#32437;&#35270;&#39057;&#20013;&#32431;&#31929;&#22320;&#21457;&#29616;&#21517;&#20026;&#25216;&#33021;&#21407;&#22411;&#30340;&#36328;&#20307;&#29616;&#34920;&#31034;&#65292;&#20351;&#29992;&#26465;&#20214;&#25193;&#25955;&#31574;&#30053;&#23558;&#25216;&#33021;&#34920;&#31034;&#36716;&#31227;&#21040;&#26426;&#22120;&#20154;&#21160;&#20316;&#65292;&#24182;&#26368;&#32456;&#20351;&#29992;&#20154;&#31867;&#25552;&#31034;&#35270;&#39057;&#23436;&#25104;&#23398;&#20064;&#21040;&#30340;&#25216;&#33021;&#26469;&#23436;&#25104;&#26410;&#35265;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;&#20223;&#30495;&#21644;&#30495;&#23454;&#29615;&#22659;&#20013;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#21457;&#29616;&#30340;&#25216;&#33021;&#21407;&#22411;&#20419;&#36827;&#20102;&#26410;&#35265;&#20219;&#21153;&#30340;&#25216;&#33021;&#36716;&#31227;&#21644;&#32452;&#21512;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26356;&#36890;&#29992;&#21644;&#21487;&#25193;&#23637;&#30340;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human demonstration videos are a widely available data source for robot learning and an intuitive user interface for expressing desired behavior. However, directly extracting reusable robot manipulation skills from unstructured human videos is challenging due to the big embodiment difference and unobserved action parameters. To bridge this embodiment gap, this paper introduces XSkill, an imitation learning framework that 1) discovers a cross-embodiment representation called skill prototypes purely from unlabeled human and robot manipulation videos, 2) transfers the skill representation to robot actions using conditional diffusion policy, and finally, 3) composes the learned skill to accomplish unseen tasks specified by a human prompt video. Our experiments in simulation and real-world environments show that the discovered skill prototypes facilitate both skill transfer and composition for unseen tasks, resulting in a more general and scalable imitation learning framework. The performan
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#35268;&#27169;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#20013;&#21033;&#29992;&#38271;&#31243;&#20381;&#36182;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#38271;&#31243;&#20803;&#36335;&#24452;&#25628;&#32034;&#21644;&#28176;&#36827;&#25277;&#26679;&#65288;LMSPS&#65289;&#30340;&#33258;&#21160;&#26694;&#26550;&#12290;&#36890;&#36807;&#21160;&#24577;&#32553;&#23567;&#25628;&#32034;&#31354;&#38388;&#21644;&#19987;&#38376;&#30340;&#20803;&#36335;&#24452;&#36873;&#25321;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;LMSPS&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.08430</link><description>&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#19978;&#36890;&#36807;&#28176;&#36827;&#25277;&#26679;&#36827;&#34892;&#38271;&#31243;&#20803;&#36335;&#24452;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Long-range Meta-path Search through Progressive Sampling on Large-scale Heterogeneous Information Networks. (arXiv:2307.08430v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08430
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#35268;&#27169;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#20013;&#21033;&#29992;&#38271;&#31243;&#20381;&#36182;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#38271;&#31243;&#20803;&#36335;&#24452;&#25628;&#32034;&#21644;&#28176;&#36827;&#25277;&#26679;&#65288;LMSPS&#65289;&#30340;&#33258;&#21160;&#26694;&#26550;&#12290;&#36890;&#36807;&#21160;&#24577;&#32553;&#23567;&#25628;&#32034;&#31354;&#38388;&#21644;&#19987;&#38376;&#30340;&#20803;&#36335;&#24452;&#36873;&#25321;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;LMSPS&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#31243;&#20381;&#36182;&#30340;&#21033;&#29992;&#22312;&#21516;&#36136;&#22270;&#20013;&#26377;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#22312;&#22823;&#35268;&#27169;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#20013;&#24456;&#23569;&#30740;&#31350;&#65292;&#36825;&#20027;&#35201;&#26159;&#30001;&#20110;&#39640;&#25104;&#26412;&#21644;&#26377;&#25928;&#20449;&#24687;&#21033;&#29992;&#30340;&#22256;&#38590;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#20803;&#36335;&#24452;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#20013;&#21033;&#29992;&#38271;&#31243;&#20381;&#36182;&#30340;&#33258;&#21160;&#26694;&#26550;&#65292;&#31216;&#20026;&#38271;&#31243;&#20803;&#36335;&#24452;&#25628;&#32034;&#21644;&#28176;&#36827;&#25277;&#26679;&#65288;LMSPS&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#20026;&#20102;&#22312;&#27809;&#26377;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#21457;&#29616;&#21508;&#31181;&#25968;&#25454;&#38598;&#25110;&#20219;&#21153;&#30340;&#20803;&#36335;&#24452;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21253;&#21547;&#25152;&#26377;&#30446;&#26631;&#33410;&#28857;&#30456;&#20851;&#20803;&#36335;&#24452;&#30340;&#25628;&#32034;&#31354;&#38388;&#12290;&#36890;&#36807;&#28176;&#36827;&#25277;&#26679;&#31639;&#27861;&#65292;&#25105;&#20204;&#21160;&#24577;&#22320;&#32553;&#23567;&#25628;&#32034;&#31354;&#38388;&#65292;&#20197;&#36339;&#25968;&#26080;&#20851;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#39537;&#21160;&#65292;&#20174;&#32780;&#24471;&#21040;&#19968;&#20010;&#30001;&#24403;&#21069;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#21644;&#20219;&#21153;&#39537;&#21160;&#30340;&#32039;&#20945;&#25628;&#32034;&#31354;&#38388;&#12290;&#21033;&#29992;&#25277;&#26679;&#35780;&#20272;&#31574;&#30053;&#20316;&#20026;&#25351;&#23548;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19987;&#38376;&#21644;&#20855;&#26377;&#34920;&#36798;&#21147;&#30340;&#20803;&#36335;&#24452;&#36873;&#25321;&#12290;&#23545;&#20843;&#20010;&#24322;&#26500;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;LMSPS&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Utilizing long-range dependency, though extensively studied in homogeneous graphs, is rarely studied in large-scale heterogeneous information networks (HINs), whose main challenge is the high costs and the difficulty in utilizing effective information. To this end, we investigate the importance of different meta-paths and propose an automatic framework for utilizing long-range dependency in HINs, called Long-range Meta-path Search through Progressive Sampling (LMSPS). Specifically, to discover meta-paths for various datasets or tasks without prior, we develop a search space with all target-node-related meta-paths. With a progressive sampling algorithm, we dynamically shrink the search space with hop-independent time complexity, leading to a compact search space driven by the current HIN and task. Utilizing a sampling evaluation strategy as the guidance, we conduct a specialized and expressive meta-path selection. Extensive experiments on eight heterogeneous datasets demonstrate that LM
&lt;/p&gt;</description></item><item><title>Espaloma-0.3.0&#26159;&#19968;&#20010;&#29992;&#20110;&#34507;&#30333;&#36136;-&#37197;&#20307;&#31995;&#32479;&#27169;&#25311;&#30340;&#26426;&#22120;&#23398;&#20064;&#20998;&#23376;&#21147;&#23398;&#21147;&#22330;&#65292;&#36890;&#36807;&#33021;&#37327;&#21644;&#21147;&#30340;&#25311;&#21512;&#32435;&#20837;&#37327;&#23376;&#21270;&#23398;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#20855;&#26377;&#28789;&#27963;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.07085</link><description>&lt;p&gt;
Espaloma-0.3.0: &#29992;&#20110;&#34507;&#30333;&#36136;-&#37197;&#20307;&#31995;&#32479;&#27169;&#25311;&#30340;&#26426;&#22120;&#23398;&#20064;&#20998;&#23376;&#21147;&#23398;&#21147;&#22330;&#21450;&#20854;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Espaloma-0.3.0: Machine-learned molecular mechanics force field for the simulation of protein-ligand systems and beyond. (arXiv:2307.07085v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07085
&lt;/p&gt;
&lt;p&gt;
Espaloma-0.3.0&#26159;&#19968;&#20010;&#29992;&#20110;&#34507;&#30333;&#36136;-&#37197;&#20307;&#31995;&#32479;&#27169;&#25311;&#30340;&#26426;&#22120;&#23398;&#20064;&#20998;&#23376;&#21147;&#23398;&#21147;&#22330;&#65292;&#36890;&#36807;&#33021;&#37327;&#21644;&#21147;&#30340;&#25311;&#21512;&#32435;&#20837;&#37327;&#23376;&#21270;&#23398;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#20855;&#26377;&#28789;&#27963;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#21147;&#23398;&#65288;MM&#65289;&#21147;&#22330;&#26159;&#36890;&#36807;&#31616;&#21333;&#30340;&#19968;&#23545;&#19968;&#21644;&#22810;&#39033;&#24335;&#39033;&#26469;&#34920;&#24449;&#20998;&#23376;&#31995;&#32479;&#33021;&#37327;&#26223;&#35266;&#30340;&#27169;&#22411;&#12290;&#20256;&#32479;&#19978;&#65292;&#23427;&#20204;&#20381;&#36182;&#20110;&#20154;&#24037;&#19987;&#23478;&#31574;&#21010;&#12289;&#19981;&#28789;&#27963;&#19988;&#38590;&#20197;&#25193;&#23637;&#30340;&#31163;&#25955;&#21270;&#21270;&#23398;&#21442;&#25968;&#36171;&#20540;&#35268;&#21017;&#65292;&#21363;&#21407;&#23376;&#25110;&#20215;&#24577;&#31867;&#22411;&#12290;&#26368;&#36817;&#65292;&#20154;&#20204;&#23545;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#26367;&#20195;&#27492;&#36807;&#31243;&#65292;&#24182;&#20351;&#21442;&#25968;&#21270;&#26041;&#26696;&#33021;&#22815;&#30452;&#25509;&#20174;&#37327;&#23376;&#21270;&#23398;&#35745;&#31639;&#25110;&#20957;&#32858;&#30456;&#25968;&#25454;&#20013;&#20197;&#31471;&#21040;&#31471;&#30340;&#21487;&#24494;&#20998;&#26041;&#24335;&#36827;&#34892;&#23398;&#20064;&#34920;&#31034;&#20986;&#20102;&#24040;&#22823;&#20852;&#36259;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#33021;&#37327;&#21644;&#21147;&#30340;&#36866;&#24212;&#24615;&#25311;&#21512;&#30452;&#25509;&#32435;&#20837;&#35757;&#32451;&#36807;&#31243;&#65292;&#25193;&#23637;&#20102;Espaloma&#30340;&#31471;&#21040;&#31471;&#21487;&#24494;&#20998;&#21147;&#22330;&#26500;&#24314;&#26041;&#27861;&#12290;&#22522;&#20110;OpenMM SPICE&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#31574;&#21010;&#20102;&#19968;&#20010;&#21253;&#21547;&#19982;&#29983;&#29289;&#20998;&#23376;&#24314;&#27169;&#24191;&#27867;&#30456;&#20851;&#30340;&#21270;&#23398;&#31354;&#38388;&#30340;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#23567;&#20998;&#23376;&#12289;&#34507;&#30333;&#36136;&#21644;RNA&#12290;&#26368;&#32456;&#24471;&#21040;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#34507;&#30333;&#36136;-&#37197;&#20307;&#31995;&#32479;&#27169;&#25311;&#30340;&#26426;&#22120;&#23398;&#20064;&#20998;&#23376;&#21147;&#23398;&#21147;&#22330;&#12290;
&lt;/p&gt;
&lt;p&gt;
Molecular mechanics (MM) force fields -- the models that characterize the energy landscape of molecular systems via simple pairwise and polynomial terms -- have traditionally relied on human expert-curated, inflexible, and poorly extensible discrete chemical parameter assignment rules, namely atom or valence types. Recently, there has been significant interest in using graph neural networks to replace this process, while enabling the parametrization scheme to be learned in an end-to-end differentiable manner directly from quantum chemical calculations or condensed-phase data. In this paper, we extend the Espaloma end-to-end differentiable force field construction approach by incorporating both energy and force fitting directly to quantum chemical data into the training process. Building on the OpenMM SPICE dataset, we curate a dataset containing chemical spaces highly relevant to the broad interest of biomolecular modeling, covering small molecules, proteins, and RNA. The resulting for
&lt;/p&gt;</description></item><item><title>ProbVLM&#26159;&#19968;&#31181;&#27010;&#29575;&#36866;&#37197;&#22120;&#65292;&#29992;&#20110;&#20272;&#35745;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#23884;&#20837;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#20197;&#35299;&#20915;&#22266;&#26377;&#30340;&#23884;&#20837;&#27495;&#20041;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20854;&#22312;&#26816;&#32034;&#20219;&#21153;&#20013;&#30340;&#20248;&#36234;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2307.00398</link><description>&lt;p&gt;
ProbVLM: &#20923;&#32467;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#27010;&#29575;&#36866;&#37197;&#22120;
&lt;/p&gt;
&lt;p&gt;
ProbVLM: Probabilistic Adapter for Frozen Vison-Language Models. (arXiv:2307.00398v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00398
&lt;/p&gt;
&lt;p&gt;
ProbVLM&#26159;&#19968;&#31181;&#27010;&#29575;&#36866;&#37197;&#22120;&#65292;&#29992;&#20110;&#20272;&#35745;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#23884;&#20837;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#20197;&#35299;&#20915;&#22266;&#26377;&#30340;&#23884;&#20837;&#27495;&#20041;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20854;&#22312;&#26816;&#32034;&#20219;&#21153;&#20013;&#30340;&#20248;&#36234;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#22914;CLIP&#25104;&#21151;&#22320;&#22312;&#22270;&#20687;&#21644;&#25991;&#26412;&#20043;&#38388;&#25214;&#21040;&#23545;&#24212;&#20851;&#31995;&#12290;&#36890;&#36807;&#26631;&#20934;&#30340;&#30830;&#23450;&#24615;&#26144;&#23556;&#36807;&#31243;&#65292;&#23558;&#22270;&#20687;&#25110;&#25991;&#26412;&#26679;&#26412;&#26144;&#23556;&#21040;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#19968;&#20010;&#21521;&#37327;&#12290;&#36825;&#26159;&#26377;&#38382;&#39064;&#30340;&#65306;&#30001;&#20110;&#22810;&#20010;&#26679;&#26412;&#65288;&#22270;&#20687;&#25110;&#25991;&#26412;&#65289;&#21487;&#20197;&#25277;&#35937;&#20986;&#29289;&#29702;&#19990;&#30028;&#20013;&#30340;&#30456;&#21516;&#27010;&#24565;&#65292;&#30830;&#23450;&#24615;&#23884;&#20837;&#19981;&#21453;&#26144;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#22266;&#26377;&#27495;&#20041;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;ProbVLM&#65292;&#19968;&#31181;&#27010;&#29575;&#36866;&#37197;&#22120;&#65292;&#36890;&#36807;&#20107;&#21518;&#26041;&#24335;&#22312;&#39044;&#35757;&#32451;&#30340;VLM&#20013;&#36890;&#36807;&#20869;&#37096;/&#22806;&#37096;&#27169;&#24577;&#23545;&#40784;&#20272;&#35745;&#23884;&#20837;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#32780;&#26080;&#38656;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#25110;&#35745;&#31639;&#12290;&#22312;&#22235;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#19978;&#65292;&#21363;COCO&#12289;Flickr&#12289;CUB&#21644;Oxford-flowers&#65292;&#25105;&#20204;&#20272;&#35745;&#20102;&#20004;&#20010;VLM&#65288;CLIP&#21644;BLIP&#65289;&#30340;&#22810;&#27169;&#24577;&#23884;&#20837;&#19981;&#30830;&#23450;&#24615;&#65292;&#37327;&#21270;&#20102;&#23884;&#20837;&#19981;&#30830;&#23450;&#24615;&#22312;&#26816;&#32034;&#20219;&#21153;&#20013;&#30340;&#26657;&#20934;&#65292;&#24182;&#34920;&#26126;ProbVLM&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20027;&#21160;&#23398;&#20064;&#21644;&#27169;&#22411;...
&lt;/p&gt;
&lt;p&gt;
Large-scale vision-language models (VLMs) like CLIP successfully find correspondences between images and text. Through the standard deterministic mapping process, an image or a text sample is mapped to a single vector in the embedding space. This is problematic: as multiple samples (images or text) can abstract the same concept in the physical world, deterministic embeddings do not reflect the inherent ambiguity in the embedding space. We propose ProbVLM, a probabilistic adapter that estimates probability distributions for the embeddings of pre-trained VLMs via inter/intra-modal alignment in a post-hoc manner without needing large-scale datasets or computing. On four challenging datasets, i.e., COCO, Flickr, CUB, and Oxford-flowers, we estimate the multi-modal embedding uncertainties for two VLMs, i.e., CLIP and BLIP, quantify the calibration of embedding uncertainties in retrieval tasks and show that ProbVLM outperforms other methods. Furthermore, we propose active learning and model 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#24341;&#20837;&#31532;&#19968;&#20010;&#22823;&#22411;&#22810;&#26679;&#21270;&#30340;&#35270;&#35273;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#27169;&#22411;&#20013;&#24187;&#35273;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#35774;&#35745;&#21253;&#21547;&#27491;&#36127;&#25351;&#20196;&#30340;&#25968;&#25454;&#38598;&#21644;&#25552;&#20986;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#34913;&#37327;&#27169;&#22411;&#20135;&#29983;&#30340;&#24187;&#35273;&#12290;</title><link>http://arxiv.org/abs/2306.14565</link><description>&lt;p&gt;
&#36890;&#36807;&#24378;&#21270;&#25351;&#20196;&#35843;&#25972;&#26469;&#20943;&#36731;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning. (arXiv:2306.14565v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14565
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#24341;&#20837;&#31532;&#19968;&#20010;&#22823;&#22411;&#22810;&#26679;&#21270;&#30340;&#35270;&#35273;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#27169;&#22411;&#20013;&#24187;&#35273;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#35774;&#35745;&#21253;&#21547;&#27491;&#36127;&#25351;&#20196;&#30340;&#25968;&#25454;&#38598;&#21644;&#25552;&#20986;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#34913;&#37327;&#27169;&#22411;&#20135;&#29983;&#30340;&#24187;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22810;&#27169;&#24577;&#20219;&#21153;&#21462;&#24471;&#20102;&#21487;&#21916;&#30340;&#36827;&#23637;&#65292;&#20294;&#24403;&#21069;&#30340;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMM&#65289;&#24456;&#23481;&#26131;&#22312;&#25551;&#36848;&#22270;&#20687;&#21644;&#20154;&#31867;&#25351;&#20196;&#26102;&#20135;&#29983;&#19981;&#19968;&#33268;&#30340;&#24187;&#35273;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#31532;&#19968;&#20010;&#22823;&#22411;&#22810;&#26679;&#21270;&#30340;&#35270;&#35273;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#38598;LRV-Instruction&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#21547;&#30001;GPT4&#29983;&#25104;&#30340;12&#19975;&#20010;&#35270;&#35273;&#25351;&#20196;&#65292;&#28085;&#30422;&#20102;16&#20010;&#24320;&#25918;&#24335;&#25351;&#20196;&#21644;&#31572;&#26696;&#30340;&#35270;&#35273;&#19982;&#35821;&#35328;&#20219;&#21153;&#12290;&#19982;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#27491;&#25351;&#20196;&#26679;&#26412;&#19981;&#21516;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;LRV-Instruction&#20197;&#21253;&#21547;&#26356;&#22810;&#38024;&#23545;&#26356;&#24378;&#30340;&#35270;&#35273;&#25351;&#20196;&#35843;&#25972;&#30340;&#27491;&#36127;&#25351;&#20196;&#12290;&#25105;&#20204;&#30340;&#36127;&#25351;&#20196;&#22312;&#20004;&#20010;&#35821;&#20041;&#23618;&#27425;&#19978;&#35774;&#35745;&#65306;&#65288;i&#65289;&#19981;&#23384;&#22312;&#20803;&#32032;&#25805;&#20316;&#21644;&#65288;ii&#65289;&#23384;&#22312;&#20803;&#32032;&#25805;&#20316;&#12290;&#20026;&#20102;&#26356;&#26377;&#25928;&#22320;&#34913;&#37327;LMM&#25152;&#20135;&#29983;&#30340;&#24187;&#35273;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;GPT4&#36741;&#21161;&#30340;&#35270;&#35273;&#25351;&#20196;&#35780;&#20272;&#65288;GAVIE&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the promising progress in multi-modal tasks, current large multi-modal models (LMM) are prone to hallucinating inconsistent descriptions with respect to the associated image and human instructions. This paper addresses this issue by introducing the first large and diverse visual instruction tuning dataset, named Large-scale Robust Visual (LRV)-Instruction. Our dataset consists of 120k visual instructions generated by GPT4, covering 16 vision-and-language tasks with open-ended instructions and answers. Unlike existing studies that primarily focus on positive instruction samples, we design LRV-Instruction to include both positive and negative instructions for more robust visual instruction tuning. Our negative instructions are designed at two semantic levels: (i) Nonexistent Element Manipulation and (ii) Existent Element Manipulation. To efficiently measure the hallucination generated by LMMs, we propose GPT4-Assisted Visual Instruction Evaluation (GAVIE), a novel approach to eva
&lt;/p&gt;</description></item><item><title>&#35895;&#27468;2021&#24180;&#22312;&#12298;&#33258;&#28982;&#12299;&#26434;&#24535;&#19978;&#21457;&#34920;&#30340;&#19968;&#31687;&#35770;&#25991;&#22768;&#31216;&#20854;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#22312;&#33455;&#29255;&#35774;&#35745;&#39046;&#22495;&#36827;&#34892;&#20102;&#21019;&#26032;&#65292;&#20294;&#20004;&#39033;&#29420;&#31435;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#35895;&#27468;&#30340;&#26041;&#27861;&#19981;&#22914;&#20154;&#31867;&#35774;&#35745;&#24072;&#12289;&#19981;&#22914;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#31639;&#27861;&#65288;&#27169;&#25311;&#36864;&#28779;&#65289;&#65292;&#24182;&#19988;&#20063;&#19981;&#22914;&#26222;&#36941;&#21487;&#29992;&#30340;&#21830;&#19994;&#36719;&#20214;&#65292;&#25991;&#31456;&#30340;&#23436;&#25972;&#24615;&#20063;&#36973;&#21040;&#20102;&#20005;&#37325;&#30340;&#25439;&#23475;&#12290;</title><link>http://arxiv.org/abs/2306.09633</link><description>&lt;p&gt;
&#34394;&#20551;&#40654;&#26126;&#65306;&#37325;&#26032;&#35780;&#20272;&#35895;&#27468;&#24378;&#21270;&#23398;&#20064;&#22312;&#33455;&#29255;&#23439;&#35266;&#24067;&#23616;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
The False Dawn: Reevaluating Google's Reinforcement Learning for Chip Macro Placement. (arXiv:2306.09633v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09633
&lt;/p&gt;
&lt;p&gt;
&#35895;&#27468;2021&#24180;&#22312;&#12298;&#33258;&#28982;&#12299;&#26434;&#24535;&#19978;&#21457;&#34920;&#30340;&#19968;&#31687;&#35770;&#25991;&#22768;&#31216;&#20854;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#22312;&#33455;&#29255;&#35774;&#35745;&#39046;&#22495;&#36827;&#34892;&#20102;&#21019;&#26032;&#65292;&#20294;&#20004;&#39033;&#29420;&#31435;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#35895;&#27468;&#30340;&#26041;&#27861;&#19981;&#22914;&#20154;&#31867;&#35774;&#35745;&#24072;&#12289;&#19981;&#22914;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#31639;&#27861;&#65288;&#27169;&#25311;&#36864;&#28779;&#65289;&#65292;&#24182;&#19988;&#20063;&#19981;&#22914;&#26222;&#36941;&#21487;&#29992;&#30340;&#21830;&#19994;&#36719;&#20214;&#65292;&#25991;&#31456;&#30340;&#23436;&#25972;&#24615;&#20063;&#36973;&#21040;&#20102;&#20005;&#37325;&#30340;&#25439;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35895;&#27468;2021&#24180;&#22312;&#12298;&#33258;&#28982;&#12299;&#26434;&#24535;&#19978;&#21457;&#34920;&#30340;&#26377;&#20851;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#35774;&#35745;&#33455;&#29255;&#30340;&#35770;&#25991;&#65292;&#22240;&#20026;&#25152;&#22768;&#31216;&#30340;&#32467;&#26524;&#32570;&#20047;&#20805;&#20998;&#30340;&#25991;&#20214;&#35760;&#24405;&#21644;&#20851;&#38190;&#27493;&#39588;&#30340;&#35828;&#26126;&#65292;&#24341;&#21457;&#20105;&#35758;&#24182;&#21463;&#21040;&#23186;&#20307;&#30340;&#25209;&#35780;&#25253;&#36947;&#12290; &#32780;&#20004;&#39033;&#29420;&#31435;&#30340;&#35780;&#20272;&#22635;&#34917;&#20102;&#31354;&#30333;&#65292;&#35777;&#26126;&#35895;&#27468;&#24378;&#21270;&#23398;&#20064;&#33853;&#21518;&#20110;&#20154;&#31867;&#35774;&#35745;&#24072;&#12289;&#33853;&#21518;&#20110;&#19968;&#31181;&#20247;&#25152;&#21608;&#30693;&#30340;&#31639;&#27861;&#65288;&#27169;&#25311;&#36864;&#28779;&#65289;&#65292;&#24182;&#19988;&#36824;&#33853;&#21518;&#20110;&#26222;&#36941;&#21487;&#29992;&#30340;&#21830;&#19994;&#36719;&#20214;&#12290;&#20132;&#21449;&#26816;&#26597;&#30340;&#25968;&#25454;&#34920;&#26126;&#65292;&#30001;&#20110;&#34892;&#20026;&#12289;&#20998;&#26512;&#21644;&#25253;&#21578;&#20013;&#30340;&#38169;&#35823;&#65292;&#35813;&#12298;&#33258;&#28982;&#12299;&#25991;&#31456;&#30340;&#23436;&#25972;&#24615;&#21463;&#21040;&#20102;&#20005;&#37325;&#30340;&#25439;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) for physical design of silicon chips in a Google 2021 Nature paper stirred controversy due to poorly documented claims that raised eyebrows and attracted critical media coverage. The Nature paper withheld most inputs needed to produce reported results and some critical steps in the methodology. But two independent evaluations filled in the gaps and demonstrated that Google RL lags behind human designers, behind a well-known algorithm (Simulated Annealing), and also behind generally-available commercial software. Crosschecked data indicate that the integrity of the Nature paper is substantially undermined owing to errors in the conduct, analysis and reporting.
&lt;/p&gt;</description></item><item><title>Ada-NAV&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#36712;&#36857;&#20248;&#21270;&#31574;&#30053;&#23398;&#20064;&#26041;&#27861;&#65292;&#37319;&#29992;&#38477;&#20302;&#31574;&#30053;&#38543;&#26426;&#24615;&#30340;&#26041;&#27861;&#24179;&#34913;&#25506;&#32034;&#19982;&#21033;&#29992;&#65292;&#25552;&#39640;&#26426;&#22120;&#20154;&#23548;&#33322;&#20219;&#21153;&#30340;&#37319;&#26679;&#25928;&#29575;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#21487;&#20197;&#22312;&#26356;&#30701;&#30340;&#37319;&#26679;&#26102;&#38388;&#20869;&#21462;&#24471;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.06192</link><description>&lt;p&gt;
Ada-NAV&#65306;&#29992;&#20110;&#26426;&#22120;&#20154;&#23548;&#33322;&#30340;&#33258;&#36866;&#24212;&#36712;&#36857;&#20248;&#21270;&#31574;&#30053;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Ada-NAV: Adaptive Trajectory-Based Sample Efficient Policy Learning for Robotic Navigation. (arXiv:2306.06192v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06192
&lt;/p&gt;
&lt;p&gt;
Ada-NAV&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#36712;&#36857;&#20248;&#21270;&#31574;&#30053;&#23398;&#20064;&#26041;&#27861;&#65292;&#37319;&#29992;&#38477;&#20302;&#31574;&#30053;&#38543;&#26426;&#24615;&#30340;&#26041;&#27861;&#24179;&#34913;&#25506;&#32034;&#19982;&#21033;&#29992;&#65292;&#25552;&#39640;&#26426;&#22120;&#20154;&#23548;&#33322;&#20219;&#21153;&#30340;&#37319;&#26679;&#25928;&#29575;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#21487;&#20197;&#22312;&#26356;&#30701;&#30340;&#37319;&#26679;&#26102;&#38388;&#20869;&#21462;&#24471;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#23398;&#20064;&#26426;&#22120;&#20154;&#23548;&#33322;&#31574;&#30053;&#26041;&#38754;&#21313;&#20998;&#26377;&#25928;&#65292;&#20294;&#20854;&#37319;&#26679;&#25928;&#29575;&#20302;&#30340;&#38382;&#39064;&#20063;&#21313;&#20998;&#26126;&#26174;&#12290;&#22312;&#31574;&#30053;&#20248;&#21270;&#20013;&#65292;&#36825;&#31181;&#25928;&#29575;&#20302;&#19979;&#37096;&#20998;&#26469;&#33258;&#20110;&#26410;&#33021;&#36866;&#24403;&#22320;&#24179;&#34913;&#25506;&#32034;&#19982;&#21033;&#29992;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#38754;&#23545;&#38750;&#38745;&#24577;&#26102;&#12290;&#20026;&#20102;&#21152;&#20837;&#25506;&#32034;&#19982;&#21033;&#29992;&#30340;&#24179;&#34913;&#20197;&#25552;&#39640;&#37319;&#26679;&#25928;&#29575;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Ada-NAV&#65292;&#19968;&#31181;&#33258;&#36866;&#24212;&#36712;&#36857;&#38271;&#24230;&#26041;&#26696;&#65292;&#20854;&#20013;&#38271;&#24230;&#38543;&#30528;&#31574;&#30053;&#30340;&#38543;&#26426;&#24615;&#65288;&#29992;&#20854;Shannon&#25110;&#24046;&#20998;&#29109;&#34920;&#31034;&#65289;&#30340;&#20943;&#23567;&#32780;&#22686;&#21152;&#12290;&#25105;&#20204;&#30340;&#33258;&#36866;&#24212;&#36712;&#36857;&#38271;&#24230;&#26041;&#26696;&#30001;&#20110;&#26356;&#39057;&#32321;&#30340;&#26799;&#24230;&#26356;&#26032;&#24378;&#35843;&#20102;&#35757;&#32451;&#24320;&#22987;&#26102;&#30340;&#25506;&#32034;&#65292;&#21518;&#26469;&#21017;&#26356;&#24378;&#35843;&#21033;&#29992;&#12290;&#22312;&#32593;&#26684;&#19990;&#30028;&#65292;&#20223;&#30495;&#26426;&#22120;&#20154;&#29615;&#22659;&#21644;&#30495;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#28857;&#65292;&#34920;&#29616;&#22312;&#24615;&#33021;&#21644;&#37319;&#26679;&#25928;&#29575;&#19978;&#22343;&#20248;&#20110;&#24120;&#25968;&#21644;&#38543;&#26426;&#37319;&#26679;&#30340;&#36712;&#36857;&#38271;&#24230;&#12290;&#22312;&#22266;&#23450;&#30340;&#26679;&#26412;&#39044;&#31639;&#19979;&#65292;&#30456;&#23545;&#20110;&#29616;&#26377;&#30340;&#22522;&#20934;&#26041;&#27861;&#65292;Ada-NAV&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;&#39640;&#36798;46&#65285;&#65292;&#37319;&#26679;&#25968;&#37327;&#20943;&#23569;&#20102;&#39640;&#36798;80&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning methods, while effective for learning robotic navigation strategies, are known to be highly sample inefficient. This sample inefficiency comes in part from not suitably balancing the explore-exploit dilemma, especially in the presence of non-stationarity, during policy optimization. To incorporate a balance of exploration-exploitation for sample efficiency, we propose Ada-NAV, an adaptive trajectory length scheme where the length grows as a policy's randomness, represented by its Shannon or differential entropy, decreases. Our adaptive trajectory length scheme emphasizes exploration at the beginning of training due to more frequent gradient updates and emphasizes exploitation later on with longer trajectories. In gridworld, simulated robotic environments, and real-world robotic experiments, we demonstrate the merits of the approach over constant and randomly sampled trajectory lengths in terms of performance and sample efficiency. For a fixed sample budget, Ada-N
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#29305;&#24449;&#37325;&#26500;&#20449;&#21495;&#22270;&#27861;&#65292;&#22312;&#26059;&#36716;&#26426;&#26800;&#25925;&#38556;&#35786;&#26029;&#27169;&#22411;&#20013;&#21462;&#24471;&#20102;&#20851;&#38190;&#36827;&#23637;&#65292;&#33021;&#22815;&#21160;&#24577;&#22320;&#36873;&#25321;&#26368;&#20248;&#23376;&#24102;&#30340;&#29305;&#24449;&#31995;&#25968;&#30697;&#38453;&#65292;&#36827;&#34892;&#36866;&#24212;&#24615;&#20449;&#21495;&#37325;&#26500;&#65292;&#24182;&#20174;&#20013;&#25552;&#21462;&#28145;&#23618;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2306.05281</link><description>&lt;p&gt;
&#22522;&#20110;&#21160;&#24577;&#29305;&#24449;&#37325;&#26500;&#20449;&#21495;&#22270;&#30340;&#26059;&#36716;&#26426;&#26800;&#25925;&#38556;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Fault Identification of Rotating Machinery Based on Dynamic Feature Reconstruction Signal Graph. (arXiv:2306.05281v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05281
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#29305;&#24449;&#37325;&#26500;&#20449;&#21495;&#22270;&#27861;&#65292;&#22312;&#26059;&#36716;&#26426;&#26800;&#25925;&#38556;&#35786;&#26029;&#27169;&#22411;&#20013;&#21462;&#24471;&#20102;&#20851;&#38190;&#36827;&#23637;&#65292;&#33021;&#22815;&#21160;&#24577;&#22320;&#36873;&#25321;&#26368;&#20248;&#23376;&#24102;&#30340;&#29305;&#24449;&#31995;&#25968;&#30697;&#38453;&#65292;&#36827;&#34892;&#36866;&#24212;&#24615;&#20449;&#21495;&#37325;&#26500;&#65292;&#24182;&#20174;&#20013;&#25552;&#21462;&#28145;&#23618;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25552;&#39640;&#22312;&#26059;&#36716;&#26426;&#26800;&#24378;&#22122;&#22768;&#19979;&#35782;&#21035;&#25925;&#38556;&#30340;&#24615;&#33021;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#29305;&#24449;&#37325;&#26500;&#20449;&#21495;&#22270;&#27861;&#65292;&#23427;&#22312;&#25152;&#25552;&#20986;&#30340;&#31471;&#21040;&#31471;&#25925;&#38556;&#35786;&#26029;&#27169;&#22411;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#39318;&#20808;&#21033;&#29992;&#23567;&#27874;&#21253;&#20998;&#35299;&#65288;WPD&#65289;&#23558;&#21407;&#22987;&#26426;&#26800;&#20449;&#21495;&#20998;&#35299;&#20026;&#21253;&#25324;&#31995;&#25968;&#30697;&#38453;&#22312;&#20869;&#30340;&#22810;&#20010;&#23376;&#24102;&#12290;&#28982;&#21518;&#65292;&#21033;&#29992;&#26368;&#21021;&#23450;&#20041;&#30340;&#20004;&#20010;&#35201;&#32032;MDD&#21644;DDD&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;L2&#33021;&#37327;&#33539;&#25968;&#30340;&#21160;&#24577;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65288;DFSL&#65289;&#65292;&#23427;&#21487;&#20197;&#26681;&#25454;&#33021;&#37327;&#33539;&#25968;&#20998;&#24067;&#30340;&#24046;&#24322;&#21160;&#24577;&#22320;&#36873;&#25321;WPD&#30340;&#29305;&#24449;&#31995;&#25968;&#30697;&#38453;&#65292;&#20351;&#27599;&#20010;&#23376;&#20449;&#21495;&#33021;&#22815;&#36827;&#34892;&#36866;&#24212;&#24615;&#20449;&#21495;&#37325;&#26500;&#12290;&#25509;&#19979;&#26469;&#65292;&#23558;&#26368;&#20248;&#29305;&#24449;&#23376;&#24102;&#30340;&#31995;&#25968;&#30697;&#38453;&#36827;&#34892;&#37325;&#26500;&#21644;&#37325;&#26032;&#32452;&#21512;&#65292;&#24471;&#21040;&#29305;&#24449;&#20449;&#21495;&#22270;&#12290;&#26368;&#21518;&#65292;&#21033;&#29992;2D-&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;2D-CNN&#65289;&#20174;&#29305;&#24449;&#20449;&#21495;&#22270;&#20013;&#25552;&#21462;&#28145;&#23618;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
To improve the performance in identifying the faults under strong noise for rotating machinery, this paper presents a dynamic feature reconstruction signal graph method, which plays the key role of the proposed end-to-end fault diagnosis model. Specifically, the original mechanical signal is first decomposed by wavelet packet decomposition (WPD) to obtain multiple subbands including coefficient matrix. Then, with originally defined two feature extraction factors MDD and DDD, a dynamic feature selection method based on L2 energy norm (DFSL) is proposed, which can dynamically select the feature coefficient matrix of WPD based on the difference in the distribution of norm energy, enabling each sub-signal to take adaptive signal reconstruction. Next the coefficient matrices of the optimal feature sub-bands are reconstructed and reorganized to obtain the feature signal graphs. Finally, deep features are extracted from the feature signal graphs by 2D-Convolutional neural network (2D-CNN). Ex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#20174;&#20687;&#32032;&#20013;&#25552;&#21462;&#27169;&#24335;&#30340;&#38382;&#39064;&#65292;&#24182;&#30740;&#31350;&#20102;&#29305;&#24449;&#21487;&#35270;&#21270;&#30340;&#21487;&#38752;&#24615;&#12290;&#23454;&#39564;&#35777;&#25454;&#34920;&#26126;&#65292;&#30001;&#20110;&#20248;&#21270;&#36807;&#31243;&#20013;&#22266;&#26377;&#30340;&#38480;&#21046;&#65292;&#29305;&#24449;&#21487;&#35270;&#21270;&#33021;&#22815;&#21487;&#38752;&#29702;&#35299;&#30340;&#21151;&#33021;&#38598;&#38750;&#24120;&#26377;&#38480;&#65292;&#23545;&#20110;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#22788;&#29702;&#33258;&#28982;&#22270;&#20687;&#30340;&#35299;&#37322;&#33021;&#21147;&#20135;&#29983;&#24576;&#30097;&#12290;</title><link>http://arxiv.org/abs/2306.04719</link><description>&lt;p&gt;
&#19981;&#35201;&#30456;&#20449;&#20320;&#30340;&#30524;&#30555;&#65306;&#20851;&#20110;&#29305;&#24449;&#21487;&#35270;&#21270;&#30340;&#65288;&#19981;&#65289;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Don't trust your eyes: on the (un)reliability of feature visualizations. (arXiv:2306.04719v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04719
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#20174;&#20687;&#32032;&#20013;&#25552;&#21462;&#27169;&#24335;&#30340;&#38382;&#39064;&#65292;&#24182;&#30740;&#31350;&#20102;&#29305;&#24449;&#21487;&#35270;&#21270;&#30340;&#21487;&#38752;&#24615;&#12290;&#23454;&#39564;&#35777;&#25454;&#34920;&#26126;&#65292;&#30001;&#20110;&#20248;&#21270;&#36807;&#31243;&#20013;&#22266;&#26377;&#30340;&#38480;&#21046;&#65292;&#29305;&#24449;&#21487;&#35270;&#21270;&#33021;&#22815;&#21487;&#38752;&#29702;&#35299;&#30340;&#21151;&#33021;&#38598;&#38750;&#24120;&#26377;&#38480;&#65292;&#23545;&#20110;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#22788;&#29702;&#33258;&#28982;&#22270;&#20687;&#30340;&#35299;&#37322;&#33021;&#21147;&#20135;&#29983;&#24576;&#30097;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#26159;&#22914;&#20309;&#20174;&#20687;&#32032;&#20013;&#25552;&#21462;&#27169;&#24335;&#30340;&#65311;&#29305;&#24449;&#21487;&#35270;&#21270;&#36890;&#36807;&#20248;&#21270;&#26469;&#21487;&#35270;&#21270;&#39640;&#28608;&#27963;&#30340;&#27169;&#24335;&#65292;&#35797;&#22270;&#22238;&#31572;&#36825;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#22914;&#20170;&#65292;&#21487;&#35270;&#21270;&#26041;&#27861;&#26500;&#25104;&#20102;&#25105;&#20204;&#23545;&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#24037;&#20316;&#30340;&#20102;&#35299;&#30340;&#22522;&#30784;&#65292;&#20316;&#20026;&#19968;&#31181;&#26426;&#26800;&#24335;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#38382;&#65306;&#29305;&#24449;&#21487;&#35270;&#21270;&#26377;&#22810;&#21487;&#38752;&#65311;&#25105;&#20204;&#36890;&#36807;&#24320;&#21457;&#32593;&#32476;&#30005;&#36335;&#26469;&#35784;&#39575;&#29305;&#24449;&#21487;&#35270;&#21270;&#65292;&#20351;&#20854;&#26174;&#31034;&#23436;&#20840;&#19982;&#33258;&#28982;&#36755;&#20837;&#30340;&#27491;&#24120;&#32593;&#32476;&#34892;&#20026;&#27627;&#26080;&#32852;&#31995;&#30340;&#20219;&#24847;&#27169;&#24335;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#35777;&#25454;&#34920;&#26126;&#22312;&#26631;&#20934;&#65292;&#26410;&#25805;&#32437;&#32593;&#32476;&#20013;&#21457;&#29983;&#20102;&#31867;&#20284;&#30340;&#29616;&#35937;&#65306;&#29305;&#24449;&#21487;&#35270;&#21270;&#19982;&#26631;&#20934;&#36755;&#20837;&#22788;&#29702;&#38750;&#24120;&#19981;&#21516;&#65292;&#23545;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#22788;&#29702;&#33258;&#28982;&#22270;&#20687;&#30340;&#35299;&#37322;&#33021;&#21147;&#20135;&#29983;&#24576;&#30097;&#12290;&#25105;&#20204;&#36890;&#36807;&#29702;&#35770;&#35777;&#26126;&#25903;&#25745;&#36825;&#19968;&#32463;&#39564;&#21457;&#29616;&#65292;&#30001;&#20110;&#20248;&#21270;&#36807;&#31243;&#20013;&#22266;&#26377;&#30340;&#38480;&#21046;&#65292;&#21487;&#20197;&#36890;&#36807;&#29305;&#24449;&#21487;&#35270;&#21270;&#21487;&#38752;&#29702;&#35299;&#30340;&#21151;&#33021;&#38598;&#26497;&#20854;&#26377;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
How do neural networks extract patterns from pixels? Feature visualizations attempt to answer this important question by visualizing highly activating patterns through optimization. Today, visualization methods form the foundation of our knowledge about the internal workings of neural networks, as a type of mechanistic interpretability. Here we ask: How reliable are feature visualizations? We start our investigation by developing network circuits that trick feature visualizations into showing arbitrary patterns that are completely disconnected from normal network behavior on natural input. We then provide evidence for a similar phenomenon occurring in standard, unmanipulated networks: feature visualizations are processed very differently from standard input, casting doubt on their ability to "explain" how neural networks process natural images. We underpin this empirical finding by theory proving that the set of functions that can be reliably understood by feature visualization is extr
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#22312;&#33258;&#28982;&#35821;&#35328;&#20013;&#36827;&#34892;&#36125;&#21494;&#26031;&#25512;&#29702;&#26469;&#27169;&#25311;&#20154;&#31867;&#31867;&#20154;&#27010;&#24565;&#23398;&#20064;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25552;&#35758;&#20998;&#24067;&#24182;&#25311;&#21512;&#20808;&#39564;&#20197;&#26356;&#22909;&#22320;&#27169;&#25311;&#20154;&#31867;&#23398;&#20064;&#32773;&#65292;&#24182;&#22312;&#29983;&#25104;&#24615;&#21644;&#36923;&#36753;&#24615;&#27010;&#24565;&#19978;&#36827;&#34892;&#23454;&#39564;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2306.02797</link><description>&lt;p&gt;
&#29992;&#36125;&#21494;&#26031;&#25512;&#29702;&#27169;&#25311;&#20154;&#31867;&#31867;&#20154;&#27010;&#24565;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Modeling Human-like Concept Learning with Bayesian Inference over Natural Language. (arXiv:2306.02797v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02797
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#22312;&#33258;&#28982;&#35821;&#35328;&#20013;&#36827;&#34892;&#36125;&#21494;&#26031;&#25512;&#29702;&#26469;&#27169;&#25311;&#20154;&#31867;&#31867;&#20154;&#27010;&#24565;&#23398;&#20064;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25552;&#35758;&#20998;&#24067;&#24182;&#25311;&#21512;&#20808;&#39564;&#20197;&#26356;&#22909;&#22320;&#27169;&#25311;&#20154;&#31867;&#23398;&#20064;&#32773;&#65292;&#24182;&#22312;&#29983;&#25104;&#24615;&#21644;&#36923;&#36753;&#24615;&#27010;&#24565;&#19978;&#36827;&#34892;&#23454;&#39564;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#22312;&#33258;&#28982;&#35821;&#35328;&#20013;&#36827;&#34892;&#36125;&#21494;&#26031;&#25512;&#29702;&#26469;&#27169;&#25311;&#23545;&#25277;&#35937;&#31526;&#21495;&#27010;&#24565;&#30340;&#23398;&#20064;&#12290;&#20026;&#20102;&#39640;&#25928;&#25512;&#29702;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25552;&#35758;&#20998;&#24067;&#12290;&#25105;&#20204;&#26681;&#25454;&#20154;&#31867;&#25968;&#25454;&#25311;&#21512;&#20808;&#39564;&#20197;&#26356;&#22909;&#22320;&#27169;&#25311;&#20154;&#31867;&#23398;&#20064;&#32773;&#65292;&#24182;&#22312;&#29983;&#25104;&#24615;&#21644;&#36923;&#36753;&#24615;&#27010;&#24565;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
We model learning of abstract symbolic concepts by performing Bayesian inference over utterances in natural language. For efficient inference, we use a large language model as a proposal distribution. We fit a prior to human data to better model human learners, and evaluate on both generative and logical concepts.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#26080;&#27861;&#22788;&#29702;&#30340;&#32467;&#26500;&#21270;&#22823;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#65288;SLDAS&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21160;&#24577;&#37051;&#22495;&#26500;&#24314;&#65288;DNC&#65289;&#30340;&#26032;&#22411;&#21033;&#29992;&#31574;&#30053;&#65292;&#36890;&#36807;&#21487;&#25193;&#23637;&#30340;&#37051;&#22495;&#25506;&#32034;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#39640;&#25928;&#22320;&#25506;&#32034;&#36830;&#32493;&#20195;&#29702;&#21160;&#20316;&#21608;&#22260;&#30340;&#31163;&#25955;&#37051;&#22495;&#12290;</title><link>http://arxiv.org/abs/2305.19891</link><description>&lt;p&gt;
&#32467;&#26500;&#21270;&#22823;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#30340;&#21160;&#24577;&#37051;&#22495;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
Dynamic Neighborhood Construction for Structured Large Discrete Action Spaces. (arXiv:2305.19891v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19891
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#26080;&#27861;&#22788;&#29702;&#30340;&#32467;&#26500;&#21270;&#22823;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#65288;SLDAS&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21160;&#24577;&#37051;&#22495;&#26500;&#24314;&#65288;DNC&#65289;&#30340;&#26032;&#22411;&#21033;&#29992;&#31574;&#30053;&#65292;&#36890;&#36807;&#21487;&#25193;&#23637;&#30340;&#37051;&#22495;&#25506;&#32034;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#39640;&#25928;&#22320;&#25506;&#32034;&#36830;&#32493;&#20195;&#29702;&#21160;&#20316;&#21608;&#22260;&#30340;&#31163;&#25955;&#37051;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#65288;LDAS&#65289;&#26159;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#26680;&#24515;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#22788;&#29702;&#22810;&#36798;&#20960;&#30334;&#19975;&#20010;&#21160;&#20316;&#30340;&#38750;&#32467;&#26500;&#21270;LDAS&#12290;&#28982;&#32780;&#65292;&#22312;&#29289;&#27969;&#12289;&#29983;&#20135;&#21644;&#36816;&#36755;&#31995;&#32479;&#31561;&#35768;&#22810;&#29616;&#23454;&#24212;&#29992;&#20013;&#65292;&#21160;&#20316;&#31354;&#38388;&#20855;&#26377;&#32452;&#21512;&#32467;&#26500;&#65292;&#20854;&#35268;&#27169;&#29978;&#33267;&#22312;&#23567;&#35268;&#27169;&#23454;&#20363;&#19978;&#20063;&#36229;&#36807;&#20102;&#25968;&#30334;&#19975;&#20010;&#21160;&#20316;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#36825;&#26679;&#30340;&#21160;&#20316;&#31354;&#38388;&#21576;&#29616;&#20986;&#19968;&#23450;&#30340;&#32467;&#26500;&#65292;&#20363;&#22914;&#31561;&#38388;&#36317;&#30340;&#31163;&#25955;&#36164;&#28304;&#21333;&#20301;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#22788;&#29702;&#24403;&#21069;&#22522;&#20934;&#27979;&#35797;&#26080;&#27861;&#22788;&#29702;&#30340;&#32467;&#26500;&#21270;LDAS&#65288;SLDAS&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21160;&#24577;&#37051;&#22495;&#26500;&#24314;&#65288;DNC&#65289;&#30340;&#26032;&#22411;&#21033;&#29992;&#31574;&#30053;&#65292;&#29992;&#20110;SLDAS&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#37051;&#22495;&#25506;&#32034;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#21033;&#29992;&#36825;&#31181;&#31574;&#30053;&#65292;&#22312;&#20855;&#26377;&#39640;&#36798;$10^{73}$&#20010;&#21160;&#20316;&#30340;&#32467;&#26500;&#21270;&#21160;&#20316;&#31354;&#38388;&#20013;&#39640;&#25928;&#22320;&#25506;&#32034;&#36830;&#32493;&#20195;&#29702;&#21160;&#20316;&#21608;&#22260;&#30340;&#31163;&#25955;&#37051;&#22495;&#12290;&#25105;&#20204;&#36890;&#36807;&#19982;&#19977;&#20010;&#26631;&#26438;&#31639;&#27861;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#26469;&#23637;&#31034;&#25105;&#20204;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large discrete action spaces (LDAS) remain a central challenge in reinforcement learning. Existing solution approaches can handle unstructured LDAS with up to a few million actions. However, many real-world applications in logistics, production, and transportation systems have combinatorial action spaces, whose size grows well beyond millions of actions, even on small instances. Fortunately, such action spaces exhibit structure, e.g., equally spaced discrete resource units. With this work, we focus on handling structured LDAS (SLDAS) with sizes that cannot be handled by current benchmarks: we propose Dynamic Neighborhood Construction (DNC), a novel exploitation paradigm for SLDAS. We present a scalable neighborhood exploration heuristic that utilizes this paradigm and efficiently explores the discrete neighborhood around the continuous proxy action in structured action spaces with up to $10^{73}$ actions. We demonstrate the performance of our method by benchmarking it against three sta
&lt;/p&gt;</description></item><item><title>&#19978;&#19979;&#25991;&#35270;&#35273;&#21464;&#25442;&#22120;(ContextViT)&#29992;&#20110;&#29983;&#25104;&#22270;&#20687;&#30340;&#40065;&#26834;&#29305;&#24449;&#34920;&#31034;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#39069;&#22806;&#30340;&#19978;&#19979;&#25991;&#20196;&#29260;&#65292;&#21487;&#20197;&#35299;&#37322;&#25481;&#29305;&#23450;&#20110;&#32452;&#30340;&#21327;&#21464;&#37327;&#32467;&#26500;&#65292;&#21516;&#26102;&#20445;&#25345;&#36328;&#32452;&#20849;&#20139;&#30340;&#26680;&#24515;&#35270;&#35273;&#29305;&#24449;&#65292;&#33021;&#22815;&#22312;&#30417;&#30563;&#24494;&#35843;&#12289;&#21322;&#30417;&#30563;&#23398;&#20064;&#20197;&#21450;&#20027;&#21160;&#23398;&#20064;&#31561;&#26041;&#38754;&#24471;&#21040;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.19402</link><description>&lt;p&gt;
&#19978;&#19979;&#25991;&#35270;&#35273;&#21464;&#25442;&#22120;&#29992;&#20110;&#24378;&#20581;&#30340;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Contextual Vision Transformers for Robust Representation Learning. (arXiv:2305.19402v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19402
&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#35270;&#35273;&#21464;&#25442;&#22120;(ContextViT)&#29992;&#20110;&#29983;&#25104;&#22270;&#20687;&#30340;&#40065;&#26834;&#29305;&#24449;&#34920;&#31034;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#39069;&#22806;&#30340;&#19978;&#19979;&#25991;&#20196;&#29260;&#65292;&#21487;&#20197;&#35299;&#37322;&#25481;&#29305;&#23450;&#20110;&#32452;&#30340;&#21327;&#21464;&#37327;&#32467;&#26500;&#65292;&#21516;&#26102;&#20445;&#25345;&#36328;&#32452;&#20849;&#20139;&#30340;&#26680;&#24515;&#35270;&#35273;&#29305;&#24449;&#65292;&#33021;&#22815;&#22312;&#30417;&#30563;&#24494;&#35843;&#12289;&#21322;&#30417;&#30563;&#23398;&#20064;&#20197;&#21450;&#20027;&#21160;&#23398;&#20064;&#31561;&#26041;&#38754;&#24471;&#21040;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#35270;&#35273;&#21464;&#25442;&#22120;(ContextViT)&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#22270;&#20687;&#30340;&#40065;&#26834;&#29305;&#24449;&#34920;&#31034;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#34920;&#29616;&#20986;&#20998;&#32452;&#32467;&#26500;&#30340;&#22270;&#20687;&#65292;&#22914;&#21327;&#21464;&#37327;&#12290;ContextViT&#24341;&#20837;&#20102;&#19968;&#20010;&#39069;&#22806;&#30340;&#19978;&#19979;&#25991;&#20196;&#29260;&#26469;&#32534;&#30721;&#29305;&#23450;&#20110;&#32452;&#30340;&#20449;&#24687;&#65292;&#20801;&#35768;&#27169;&#22411;&#35299;&#37322;&#25481;&#29305;&#23450;&#20110;&#32452;&#30340;&#21327;&#21464;&#37327;&#32467;&#26500;&#65292;&#21516;&#26102;&#20445;&#25345;&#36328;&#32452;&#20849;&#20139;&#30340;&#26680;&#24515;&#35270;&#35273;&#29305;&#24449;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#32473;&#23450;&#36755;&#20837;&#22270;&#20687;&#30340;&#24773;&#20917;&#19979;&#65292;Context-ViT&#23558;&#20849;&#20139;&#30456;&#21516;&#21327;&#21464;&#37327;&#30340;&#22270;&#20687;&#26144;&#23556;&#21040;&#35813;&#19978;&#19979;&#25991;&#20196;&#29260;&#65292;&#24182;&#28155;&#21152;&#21040;&#36755;&#20837;&#22270;&#20687;&#20196;&#29260;&#20013;&#65292;&#20197;&#25429;&#33719;&#23558;&#27169;&#22411;&#26465;&#20214;&#21270;&#20026;&#32452;&#25104;&#21592;&#36523;&#20221;&#30340;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#19978;&#19979;&#25991;&#25512;&#26029;&#32593;&#32476;&#65292;&#21487;&#20197;&#22312;&#36816;&#34892;&#26102;&#39044;&#27979;&#36825;&#20123;&#20196;&#29260;&#65292;&#21482;&#38656;&#35201;&#32473;&#20986;&#19968;&#20123;&#26469;&#33258;&#32452;&#20998;&#24067;&#30340;&#26679;&#26412;&#21363;&#21487;&#65292;&#20351;&#24471;ContextViT&#21487;&#20197;&#25512;&#24191;&#21040;&#26032;&#30340;&#27979;&#35797;&#20998;&#24067;&#12290;&#25105;&#20204;&#36890;&#36807;&#21508;&#31181;&#24212;&#29992;&#31243;&#24207;&#35828;&#26126;&#20102;ContextViT&#30340;&#24615;&#33021;&#12290;&#22312;&#30417;&#30563;&#24494;&#35843;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23558;&#39044;&#35757;&#32451;&#30340;ViTs&#19982;&#39069;&#22806;&#30340;&#19978;&#19979;&#25991;&#20196;&#29260;&#30456;&#32467;&#21512;&#21487;&#20197;&#25552;&#39640;&#22270;&#20687;&#20998;&#31867;&#22522;&#20934;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;ContextViT&#21487;&#20197;&#29992;&#20110;&#21322;&#30417;&#30563;&#23398;&#20064;&#65292;&#22312;CIFAR-10&#25968;&#25454;&#38598;&#19978;&#20165;&#20351;&#29992;10%&#30340;&#26631;&#35760;&#26679;&#26412;&#21363;&#21487;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;ContextViT&#21487;&#20197;&#36890;&#36807;&#24341;&#23548;&#36873;&#25321;&#26469;&#33258;&#23569;&#25968;&#32676;&#20307;&#30340;&#26356;&#20855;&#20449;&#24687;&#20215;&#20540;&#30340;&#26679;&#26412;&#65292;&#20174;&#32780;&#25552;&#39640;&#20027;&#21160;&#23398;&#20064;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Contextual Vision Transformers (ContextViT), a method for producing robust feature representations for images exhibiting grouped structure such as covariates. ContextViT introduces an extra context token to encode group-specific information, allowing the model to explain away group-specific covariate structures while keeping core visual features shared across groups. Specifically, given an input image, Context-ViT maps images that share the same covariate into this context token appended to the input image tokens to capture the effects of conditioning the model on group membership. We furthermore introduce a context inference network to predict such tokens on the fly given a few samples from a group distribution, enabling ContextViT to generalize to new testing distributions at inference time. We illustrate the performance of ContextViT through a diverse range of applications. In supervised fine-tuning, we demonstrate that augmenting pre-trained ViTs with additional context 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#27491;&#36127;&#26679;&#26412;&#30340;&#35757;&#32451;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#22810;&#23610;&#24230;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#26816;&#27979;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#30701;&#26426;&#22120;&#25991;&#26412;&#26631;&#35760;&#20026;&#8220;&#26410;&#26631;&#35760;&#8221;&#26469;&#37325;&#26032;&#34920;&#36848;&#25991;&#26412;&#20998;&#31867;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#35268;&#21017;&#21270;&#25439;&#22833;&#20989;&#25968;&#26469;&#20248;&#21270;&#26816;&#27979;&#24615;&#33021;&#65292;&#26377;&#25928;&#24615;&#33021;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.18149</link><description>&lt;p&gt;
&#22810;&#23610;&#24230;&#27491;&#36127;&#26679;&#26412;&#26816;&#27979;AI&#29983;&#25104;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
Multiscale Positive-Unlabeled Detection of AI-Generated Texts. (arXiv:2305.18149v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18149
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#27491;&#36127;&#26679;&#26412;&#30340;&#35757;&#32451;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#22810;&#23610;&#24230;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#26816;&#27979;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#30701;&#26426;&#22120;&#25991;&#26412;&#26631;&#35760;&#20026;&#8220;&#26410;&#26631;&#35760;&#8221;&#26469;&#37325;&#26032;&#34920;&#36848;&#25991;&#26412;&#20998;&#31867;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#35268;&#21017;&#21270;&#25439;&#22833;&#20989;&#25968;&#26469;&#20248;&#21270;&#26816;&#27979;&#24615;&#33021;&#65292;&#26377;&#25928;&#24615;&#33021;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#21457;&#24067;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;ChatGPT&#31561;&#22312;&#29983;&#25104;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#25991;&#26412;&#26041;&#38754;&#20196;&#20154;&#24778;&#35766;&#65292;&#20294;&#23427;&#20204;&#21487;&#33021;&#34987;&#29992;&#20110;&#21046;&#36896;&#34394;&#20551;&#30340;&#23398;&#26415;&#25991;&#26412;&#12289;&#34394;&#20551;&#26032;&#38395;&#12289;&#34394;&#20551;&#25512;&#29305;&#31561;&#12290;&#20808;&#21069;&#30340;&#20316;&#21697;&#25552;&#20986;&#20102;&#26816;&#27979;&#36825;&#20123;&#22810;&#23610;&#24230;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#31616;&#21333;&#30340;ML&#20998;&#31867;&#22120;&#12289;&#22522;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#35757;&#32451;&#19981;&#21487;&#30693;&#26041;&#27861;&#21644;&#31934;&#35843;&#30340;&#35821;&#35328;&#20998;&#31867;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#20027;&#27969;&#26816;&#27979;&#22120;&#22312;&#26500;&#24314;&#26102;&#27809;&#26377;&#32771;&#34385;&#21040;&#25991;&#26412;&#38271;&#24230;&#30340;&#22240;&#32032;&#65306;&#30701;&#25991;&#26412;&#30340;&#32570;&#20047;&#20449;&#24687;&#29305;&#24449;&#65292;&#20351;&#20854;&#26356;&#38590;&#26816;&#27979;&#12290;&#38024;&#23545;&#22810;&#23610;&#24230;&#25991;&#26412;&#26816;&#27979;&#30340;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#23610;&#24230;&#27491;&#36127;&#26679;&#26412;&#65288;MPU&#65289;&#35757;&#32451;&#26694;&#26550;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25215;&#35748;&#30701;&#30340;&#26426;&#22120;&#25991;&#26412;&#20855;&#26377;&#31867;&#20154;&#23646;&#24615;&#65292;&#24182;&#23558;&#25991;&#26412;&#20998;&#31867;&#37325;&#26032;&#34920;&#36848;&#20026;&#19968;&#20010;&#27491;&#36127;&#26679;&#26412;&#38382;&#39064;&#65292;&#21363;&#36890;&#36807;&#22312;&#35757;&#32451;&#26399;&#38388;&#26631;&#35760;&#36825;&#20123;&#30701;&#30340;&#26426;&#22120;&#25991;&#26412;&#20026;"unlabeled"&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#22312;&#36825;&#20010;&#27491;&#36127;&#26679;&#26412;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;
&lt;/p&gt;
&lt;p&gt;
Recent releases of Large Language Models (LLMs), e.g. ChatGPT, are astonishing at generating human-like texts, but they may get misused for fake scholarly texts, fake news, fake tweets, et cetera. Previous works have proposed methods to detect these multiscale AI-generated texts, including simple ML classifiers, pretrained-model-based training-agnostic methods, and finetuned language classification models. However, mainstream detectors are formulated without considering the factor of corpus length: shorter corpuses are harder to detect compared with longer ones for shortage of informative features. In this paper, a Multiscale Positive-Unlabeled (MPU) training framework is proposed to address the challenge of multiscale text detection. Firstly, we acknowledge the human-resemblance property of short machine texts, and rephrase text classification as a Positive-Unlabeled (PU) problem by marking these short machine texts as "unlabeled" during training. In this PU context, we propose the le
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Kernel-SSL&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#22810;&#31181;&#29616;&#26377;&#38750;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#24314;&#31435;&#22312;&#20102;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65288;RKHS&#65289;&#29702;&#35299;&#20043;&#19978;&#24182;&#20248;&#21270;&#20102;&#20854;&#20013;&#30340;&#22343;&#20540;&#23884;&#20837;&#21644;&#21327;&#26041;&#24046;&#31639;&#23376;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;ImageNet&#25968;&#25454;&#38598;&#19979;&#34920;&#29616;&#26174;&#33879;&#36229;&#36234;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;4.6%&#12290;</title><link>http://arxiv.org/abs/2305.17326</link><description>&lt;p&gt;
&#22522;&#20110;&#20869;&#26680;KL&#25955;&#24230;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;Kernel-SSL
&lt;/p&gt;
&lt;p&gt;
Kernel-SSL: Kernel KL Divergence for Self-supervised Learning. (arXiv:2305.17326v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17326
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Kernel-SSL&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#22810;&#31181;&#29616;&#26377;&#38750;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#24314;&#31435;&#22312;&#20102;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65288;RKHS&#65289;&#29702;&#35299;&#20043;&#19978;&#24182;&#20248;&#21270;&#20102;&#20854;&#20013;&#30340;&#22343;&#20540;&#23884;&#20837;&#21644;&#21327;&#26041;&#24046;&#31639;&#23376;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;ImageNet&#25968;&#25454;&#38598;&#19979;&#34920;&#29616;&#26174;&#33879;&#36229;&#36234;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;4.6%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#36890;&#24120;&#23558;&#19968;&#20010;&#27491;&#38170;&#28857;&#26679;&#26412;&#19982;&#35768;&#22810;&#36127;&#26679;&#26412;&#36827;&#34892;&#27604;&#36739;&#65292;&#26469;&#23436;&#25104;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#12290;&#30456;&#21453;&#65292;&#38750;&#23545;&#27604;&#23398;&#20064;&#65292;&#20363;&#22914;BYOL&#12289;SimSiam&#21644;Barlow Twins&#31561;&#26041;&#27861;&#65292;&#22312;&#27809;&#26377;&#26174;&#24335;&#20351;&#29992;&#36127;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#23436;&#25104;SSL&#12290;&#21463;&#23545;&#27604;&#23398;&#20064;&#29616;&#26377;&#20998;&#26512;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22810;&#31181;&#29616;&#26377;&#38750;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#30340;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65288;RKHS&#65289;&#29702;&#35299;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;Kernel-SSL&#65292;&#30452;&#25509;&#20248;&#21270;RKHS&#20013;&#30340;&#22343;&#20540;&#23884;&#20837;&#21644;&#21327;&#26041;&#24046;&#31639;&#23376;&#12290;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;Kernel-SSL&#22312;&#32447;&#24615;&#35780;&#20272;&#35774;&#32622;&#19979;&#22312;ImageNet&#25968;&#25454;&#38598;&#19978;&#22823;&#24133;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#22312;&#36827;&#34892;100&#20010;epoch&#30340;&#39044;&#35757;&#32451;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;SimCLR&#34920;&#29616;&#25552;&#39640;&#20102;4.6%&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive learning usually compares one positive anchor sample with lots of negative samples to perform Self-Supervised Learning (SSL). Alternatively, non-contrastive learning, as exemplified by methods like BYOL, SimSiam, and Barlow Twins, accomplishes SSL without the explicit use of negative samples. Inspired by the existing analysis for contrastive learning, we provide a reproducing kernel Hilbert space (RKHS) understanding of many existing non-contrastive learning methods. Subsequently, we propose a novel loss function, Kernel-SSL, which directly optimizes the mean embedding and the covariance operator within the RKHS. In experiments, our method Kernel-SSL outperforms state-of-the-art methods by a large margin on ImageNet datasets under the linear evaluation settings. Specifically, when performing 100 epochs pre-training, our method outperforms SimCLR by 4.6%.
&lt;/p&gt;</description></item><item><title>&#22312;$L_{2}$-&#27491;&#21017;&#21270;&#32447;&#24615;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#20351;&#29992;SGD&#20250;&#20135;&#29983;&#20174;&#26356;&#39640;&#31209;&#26368;&#23567;&#20540;&#21040;&#26356;&#20302;&#31209;&#26368;&#23567;&#20540;&#30340;&#21333;&#21521;&#36339;&#36291;&#65292;&#24182;&#19988;&#19981;&#20250;&#36339;&#22238;&#12290;</title><link>http://arxiv.org/abs/2305.16038</link><description>&lt;p&gt;
$L_{2}$&#27491;&#21017;&#32447;&#24615;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#38544;&#24615;SGD&#20559;&#24046;&#65306;&#20174;&#39640;&#31209;&#21040;&#20302;&#31209;&#30340;&#21333;&#21521;&#36339;&#36291;&#12290;
&lt;/p&gt;
&lt;p&gt;
Implicit bias of SGD in $L_{2}$-regularized linear DNNs: One-way jumps from high to low rank. (arXiv:2305.16038v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16038
&lt;/p&gt;
&lt;p&gt;
&#22312;$L_{2}$-&#27491;&#21017;&#21270;&#32447;&#24615;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#20351;&#29992;SGD&#20250;&#20135;&#29983;&#20174;&#26356;&#39640;&#31209;&#26368;&#23567;&#20540;&#21040;&#26356;&#20302;&#31209;&#26368;&#23567;&#20540;&#30340;&#21333;&#21521;&#36339;&#36291;&#65292;&#24182;&#19988;&#19981;&#20250;&#36339;&#22238;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#22810;&#20010;&#38544;&#34255;&#23618;&#30340;&#28145;&#24230;&#32447;&#24615;&#32593;&#32476;&#65288;DLN&#65289;&#30340;$L_{2}$&#27491;&#21017;&#21270;&#25439;&#22833;&#20855;&#26377;&#22810;&#20010;&#23616;&#37096;&#26368;&#23567;&#20540;&#65292;&#23545;&#24212;&#20110;&#20855;&#26377;&#19981;&#21516;&#31209;&#30340;&#30697;&#38453;&#12290;&#22312;&#30697;&#38453;&#23436;&#25104;&#31561;&#20219;&#21153;&#20013;&#65292;&#30446;&#26631;&#26159;&#25910;&#25947;&#21040;&#26368;&#23567;&#31209;&#23616;&#37096;&#26368;&#23567;&#20540;&#65292;&#35813;&#23616;&#37096;&#26368;&#23567;&#20540;&#20173;&#36866;&#21512;&#35757;&#32451;&#25968;&#25454;&#12290;&#34429;&#28982;&#21487;&#20197;&#36731;&#26494;&#36991;&#20813;&#20302;&#20272;&#31209;&#30340;&#23616;&#37096;&#26368;&#23567;&#20540;&#65292;&#22240;&#20026;&#23427;&#20204;&#19981;&#36866;&#21512;&#25968;&#25454;&#65292;&#20294;&#26799;&#24230;&#19979;&#38477;&#21487;&#33021;&#20250;&#38519;&#20837;&#39640;&#20272;&#31209;&#30340;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#20351;&#29992;SGD&#65292;&#24635;&#26159;&#26377;&#20174;&#26356;&#39640;&#31209;&#26368;&#23567;&#20540;&#36339;&#36291;&#21040;&#26356;&#20302;&#31209;&#26368;&#23567;&#20540;&#30340;&#27010;&#29575;&#65292;&#20294;&#36339;&#22238;&#30340;&#27010;&#29575;&#20026;&#38646;&#12290;&#26356;&#31934;&#30830;&#22320;&#35828;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#31995;&#21015;&#38598;&#21512;$B_{1}\subset B_{2}\subset\cdots\subset B_{R}$&#65292;&#20351;&#24471;$B_{r}$&#21253;&#21547;&#31209;$r$&#25110;&#26356;&#23569;&#30340;&#25152;&#26377;&#26368;&#23567;&#20540;&#65288;&#32780;&#19981;&#26159;&#26356;&#22810;&#65289;&#65292;&#23545;&#20110;&#36275;&#22815;&#23567;&#30340;&#23725;&#21442;&#25968;$\lambda$&#21644;&#23398;&#20064;&#29575;$\eta$&#65292;&#23427;&#20204;&#26159;&#21560;&#25910;&#30340;&#65306;SGD&#31163;&#24320;$B_{r}$&#30340;&#27010;&#29575;&#20026;0&#65292;&#20174;&#20219;&#20309;&#36215;&#28857;&#24320;&#22987;&#65292;SGD&#36827;&#20837;$B_{r}$&#30340;&#27010;&#29575;&#38750;&#38646;&#12290;
&lt;/p&gt;
&lt;p&gt;
The $L_{2}$-regularized loss of Deep Linear Networks (DLNs) with more than one hidden layers has multiple local minima, corresponding to matrices with different ranks. In tasks such as matrix completion, the goal is to converge to the local minimum with the smallest rank that still fits the training data. While rank-underestimating minima can easily be avoided since they do not fit the data, gradient descent might get stuck at rank-overestimating minima. We show that with SGD, there is always a probability to jump from a higher rank minimum to a lower rank one, but the probability of jumping back is zero. More precisely, we define a sequence of sets $B_{1}\subset B_{2}\subset\cdots\subset B_{R}$ so that $B_{r}$ contains all minima of rank $r$ or less (and not more) that are absorbing for small enough ridge parameters $\lambda$ and learning rates $\eta$: SGD has prob. 0 of leaving $B_{r}$, and from any starting point there is a non-zero prob. for SGD to go in $B_{r}$.
&lt;/p&gt;</description></item><item><title>&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;Sinkhorn&#36317;&#31163;&#30340;&#29305;&#24449;&#23545;&#20854;N-BEATS&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#23545;&#40784;&#22534;&#26632;&#20013;&#30340;&#36793;&#38469;&#29305;&#24449;&#27010;&#29575;&#27979;&#24230;&#26469;&#36827;&#34892;&#39046;&#22495;&#24191;&#20041;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;N-BEATS&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#39044;&#27979;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.15196</link><description>&lt;p&gt;
&#22522;&#20110;Sinkhorn&#36317;&#31163;&#30340;&#29305;&#24449;&#23545;&#20854;N-BEATS
&lt;/p&gt;
&lt;p&gt;
Feature-aligned N-BEATS with Sinkhorn divergence. (arXiv:2305.15196v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15196
&lt;/p&gt;
&lt;p&gt;
&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;Sinkhorn&#36317;&#31163;&#30340;&#29305;&#24449;&#23545;&#20854;N-BEATS&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#23545;&#40784;&#22534;&#26632;&#20013;&#30340;&#36793;&#38469;&#29305;&#24449;&#27010;&#29575;&#27979;&#24230;&#26469;&#36827;&#34892;&#39046;&#22495;&#24191;&#20041;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;N-BEATS&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;Sinkhorn&#36317;&#31163;&#30340;&#29305;&#24449;&#23545;&#20854;N-BEATS&#20316;&#20026;&#19968;&#20010;&#39046;&#22495;&#24191;&#20041;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#12290;&#23427;&#26159;N-BEATS&#30340;&#38750;&#24179;&#20961;&#25193;&#23637;&#65292;&#37319;&#29992;&#20102;&#21452;&#37325;&#27531;&#24046;&#21472;&#21152;&#21407;&#21017;&#65288;Oreshkin&#31561;&#20154;[42]&#65289;&#24182;&#23558;&#20854;&#36716;&#21270;&#20026;&#19968;&#20010;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23427;&#22260;&#32469;&#30528;&#30001;N-BEATS&#27599;&#20010;&#22534;&#26632;&#30340;&#27531;&#24046;&#21644;&#29305;&#24449;&#25552;&#21462;&#31639;&#23376;&#30340;&#22797;&#26434;&#32452;&#21512;&#20135;&#29983;&#30340;&#36793;&#38469;&#29305;&#24449;&#27010;&#29575;&#27979;&#24230;&#65292;&#24182;&#36890;&#36807;&#19968;&#31181;&#36817;&#20284;&#26368;&#20248;&#20256;&#36755;&#36317;&#31163;&#65288;Sinkhorn&#36317;&#31163;&#65289;&#23558;&#23427;&#20204;&#22534;&#21472;&#22320;&#23545;&#40784;&#12290;&#35757;&#32451;&#25439;&#22833;&#30001;&#26469;&#33258;&#22810;&#20010;&#28304;&#22495;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;&#21363;&#39044;&#27979;&#25439;&#22833;&#65289;&#21644;Sinkhorn&#36317;&#31163;&#35745;&#31639;&#30340;&#23545;&#40784;&#25439;&#22833;&#32452;&#25104;&#65292;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#22312;&#22810;&#20010;&#28304;&#25968;&#25454;&#24207;&#21015;&#20013;&#22534;&#21472;&#22320;&#23398;&#20064;&#19981;&#21464;&#29305;&#24449;&#65292;&#21516;&#26102;&#20445;&#30041;N-BEATS&#30340;&#21487;&#35299;&#37322;&#35774;&#35745;&#21644;&#39044;&#27979;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#35780;&#20272;&#21644;&#28040;&#34701;&#30740;&#31350;&#65292;&#24182;&#23637;&#31034;&#20102;&#30456;&#24212;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Feature-aligned N-BEATS as a domain-generalized time series forecasting model. It is a nontrivial extension of N-BEATS with doubly residual stacking principle (Oreshkin et al.[42]) into a representation learning framework. In particular, it revolves around marginal feature probability measures induced by the intricate composition of residual and feature extracting operators of N-BEATS in each stack and aligns them stack-wisely via an approximate of an optimal transport distance referred to as the Sinkhorn divergence. The training loss consists of an empirical risk minimization from multiple source domains, i.e., forecasting loss, and an alignment loss calculated with the Sinkhorn divergence, which allows the model to learn invariant features stack-wisely across multiple source data sequences while retaining N-BEATS's interpretable design and forecasting power. Comprehensive experimental evaluations with ablation studies are provided and the corresponding results demonstrate 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19981;&#31934;&#30830;&#26631;&#31614;&#23398;&#20064;&#65288;ILL&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;&#26399;&#26395;&#26368;&#22823;&#21270;&#31639;&#27861;&#23545;&#19981;&#31934;&#30830;&#26631;&#31614;&#20449;&#24687;&#36827;&#34892;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65292;&#20026;&#21508;&#31181;&#19981;&#31934;&#30830;&#26631;&#31614;&#37197;&#32622;&#38382;&#39064;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2305.12715</link><description>&lt;p&gt;
&#19981;&#31934;&#30830;&#26631;&#31614;&#23398;&#20064;&#65306;&#23398;&#20064;&#21508;&#31181;&#19981;&#31934;&#30830;&#26631;&#31614;&#37197;&#32622;&#30340;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Imprecise Label Learning: A Unified Framework for Learning with Various Imprecise Label Configurations. (arXiv:2305.12715v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12715
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19981;&#31934;&#30830;&#26631;&#31614;&#23398;&#20064;&#65288;ILL&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;&#26399;&#26395;&#26368;&#22823;&#21270;&#31639;&#27861;&#23545;&#19981;&#31934;&#30830;&#26631;&#31614;&#20449;&#24687;&#36827;&#34892;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65292;&#20026;&#21508;&#31181;&#19981;&#31934;&#30830;&#26631;&#31614;&#37197;&#32622;&#38382;&#39064;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19981;&#31934;&#30830;&#26631;&#31614;&#23398;&#20064;&#65288;ILL&#65289;&#26694;&#26550;&#65292;&#36825;&#26159;&#19968;&#31181;&#22788;&#29702;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#21508;&#31181;&#19981;&#31934;&#30830;&#26631;&#31614;&#37197;&#32622;&#30340;&#32479;&#19968;&#26041;&#27861;&#12290;ILL&#21033;&#29992;&#26399;&#26395;&#26368;&#22823;&#21270;&#65288;EM&#65289;&#31639;&#27861;&#23545;&#19981;&#31934;&#30830;&#26631;&#31614;&#20449;&#24687;&#36827;&#34892;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65288;MLE&#65289;&#65292;&#23558;&#31934;&#30830;&#26631;&#31614;&#35270;&#20026;&#28508;&#22312;&#21464;&#37327;&#12290;&#19982;&#20197;&#21069;&#35797;&#22270;&#20174;&#19981;&#31934;&#30830;&#26631;&#31614;&#20449;&#24687;&#20013;&#25512;&#26029;&#27491;&#30830;&#26631;&#31614;&#30340;&#22810;&#21151;&#33021;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;ILL&#26694;&#26550;&#32771;&#34385;&#20102;&#19981;&#31934;&#30830;&#26631;&#31614;&#20449;&#24687;&#24378;&#21152;&#30340;&#25152;&#26377;&#21487;&#33021;&#26631;&#31614;&#65292;&#20801;&#35768;&#23545;&#20219;&#20309;&#19981;&#31934;&#30830;&#26631;&#31614;&#30340;&#32479;&#19968;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;ILL&#21487;&#20197;&#26080;&#32541;&#22320;&#36866;&#24212;&#21508;&#31181;&#24773;&#20917;&#65292;&#21253;&#25324;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;&#12289;&#21322;&#30417;&#30563;&#23398;&#20064;&#12289;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#20197;&#21450;&#36825;&#20123;&#37197;&#32622;&#30340;&#28151;&#21512;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#31616;&#21333;&#26041;&#27861;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#22788;&#29702;&#19981;&#31934;&#30830;&#26631;&#31614;&#30340;&#25216;&#26415;&#65292;&#26631;&#24535;&#30528;&#31532;&#19968;&#20010;&#32479;&#19968;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce the imprecise label learning (ILL) framework, a unified approach to handle various imprecise label configurations, which are commonplace challenges in machine learning tasks. ILL leverages an expectation-maximization (EM) algorithm for the maximum likelihood estimation (MLE) of the imprecise label information, treating the precise labels as latent variables. Compared to previous versatile methods attempting to infer correct labels from the imprecise label information, our ILL framework considers all possible labeling imposed by the imprecise label information, allowing a unified solution to deal with any imprecise labels. With comprehensive experimental results, we demonstrate that ILL can seamlessly adapt to various situations, including partial label learning, semi-supervised learning, noisy label learning, and a mixture of these settings. Notably, our simple method surpasses the existing techniques for handling imprecise labels, marking the first unified 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26174;&#33879;&#24615;&#22270;&#26469;&#20419;&#36827;&#28145;&#24230;&#38598;&#25104;&#22810;&#26679;&#24615;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#21892;OOD&#26816;&#27979;&#12289;&#26657;&#20934;&#21644;&#20934;&#30830;&#24615;&#65292;&#33021;&#22815;&#20248;&#20110;&#20256;&#32479;&#30340;&#38598;&#25104;&#25216;&#26415;&#65292;&#24182;&#22312;OpenOOD&#22522;&#20934;&#27979;&#35797;&#19978;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.11616</link><description>&lt;p&gt;
&#22810;&#26679;&#21270;&#28145;&#24230;&#38598;&#25104;&#65306;&#19968;&#31181;&#20351;&#29992;&#26174;&#33879;&#24615;&#22270;&#30340;&#26041;&#27861;&#20197;&#22686;&#24378;OOD&#26816;&#27979;&#12289;&#26657;&#20934;&#21644;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Diversifying Deep Ensembles: A Saliency Map Approach for Enhanced OOD Detection, Calibration, and Accuracy. (arXiv:2305.11616v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11616
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26174;&#33879;&#24615;&#22270;&#26469;&#20419;&#36827;&#28145;&#24230;&#38598;&#25104;&#22810;&#26679;&#24615;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#21892;OOD&#26816;&#27979;&#12289;&#26657;&#20934;&#21644;&#20934;&#30830;&#24615;&#65292;&#33021;&#22815;&#20248;&#20110;&#20256;&#32479;&#30340;&#38598;&#25104;&#25216;&#26415;&#65292;&#24182;&#22312;OpenOOD&#22522;&#20934;&#27979;&#35797;&#19978;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#38598;&#25104;&#22312;&#20998;&#31867;&#21644; OOD &#26816;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#25104;&#26524;&#65307;&#28982;&#32780;&#65292;&#30001;&#20110;&#38598;&#25104;&#20013;&#23398;&#20064;&#30340;&#27169;&#24335;&#30340;&#21516;&#36136;&#24615;&#65292;&#23427;&#20204;&#30340;&#25928;&#26524;&#20173;&#28982;&#26377;&#38480;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#20419;&#36827;&#38598;&#25104;&#25104;&#21592;&#20043;&#38388;&#22810;&#26679;&#24615;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#26174;&#33879;&#24615;&#22270;&#12290;&#36890;&#36807;&#25972;&#21512;&#26174;&#33879;&#24615;&#22270;&#22810;&#26679;&#21270;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#20998;&#31867;&#21644;OOD&#26816;&#27979;&#20219;&#21153;&#20013;&#20248;&#20110;&#20256;&#32479;&#30340;&#38598;&#25104;&#25216;&#26415;&#65292;&#21516;&#26102;&#20063;&#25552;&#39640;&#20102;&#26657;&#20934;&#24615;&#12290;&#22312;&#24050;&#24314;&#31435;&#30340;OpenOOD&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#20984;&#26174;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep ensembles achieved state-of-the-art results in classification and out-of-distribution (OOD) detection; however, their effectiveness remains limited due to the homogeneity of learned patterns within the ensemble. To overcome this challenge, our study introduces a novel approach that promotes diversity among ensemble members by leveraging saliency maps. By incorporating saliency map diversification, our method outperforms conventional ensemble techniques in multiple classification and OOD detection tasks, while also improving calibration. Experiments on well-established OpenOOD benchmarks highlight the potential of our method in practical applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36328;&#35821;&#35328;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#27169;&#22411;PESTS&#65292;&#24182;&#36890;&#36807;&#27874;&#26031;&#35821;-&#33521;&#35821;&#30340;&#36328;&#35821;&#35328;&#35821;&#26009;&#24211;&#26469;&#39564;&#35777;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.07893</link><description>&lt;p&gt;
PESTS: &#27874;&#26031;&#35821;-&#33521;&#35821;&#36328;&#35821;&#35328;&#35821;&#26009;&#24211;&#29992;&#20110;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24230;
&lt;/p&gt;
&lt;p&gt;
PESTS: Persian_English Cross Lingual Corpus for Semantic Textual Similarity. (arXiv:2305.07893v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07893
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36328;&#35821;&#35328;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#27169;&#22411;PESTS&#65292;&#24182;&#36890;&#36807;&#27874;&#26031;&#35821;-&#33521;&#35821;&#30340;&#36328;&#35821;&#35328;&#35821;&#26009;&#24211;&#26469;&#39564;&#35777;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26469;&#65292;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24230;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#22791;&#21463;&#20851;&#27880;&#30340;&#32452;&#20214;&#12290;&#22312;&#35745;&#31639;&#35821;&#35328;&#23398;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#35780;&#20272;&#21333;&#35789;&#12289;&#30701;&#35821;&#12289;&#27573;&#33853;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#24456;&#37325;&#35201;&#12290;&#21516;&#26102;&#65292;&#35821;&#20041;&#30456;&#20284;&#24615;&#24230;&#37327;&#35201;&#27714;&#22312;&#28304;&#21644;&#30446;&#26631;&#35821;&#35328;&#20013;&#25552;&#20379;&#20855;&#26377;&#19968;&#23450;&#35821;&#20041;&#30456;&#20284;&#24615;&#30340;&#21477;&#23376;&#23545;&#12290;&#35768;&#22810;&#36328;&#35821;&#35328;&#30340;&#35821;&#20041;&#30456;&#20284;&#24230;&#27169;&#22411;&#20351;&#29992;&#26426;&#22120;&#32763;&#35793;&#26469;&#24357;&#34917;&#36328;&#35821;&#35328;&#35821;&#26009;&#24211;&#19981;&#21487;&#29992;&#30340;&#19981;&#36275;&#65292;&#20294;&#26426;&#22120;&#32763;&#35793;&#30340;&#35823;&#24046;&#20250;&#38477;&#20302;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#20351;&#29992;&#35821;&#20041;&#30456;&#20284;&#24230;&#29305;&#24449;&#23454;&#29616;&#26426;&#22120;&#32763;&#35793;&#26102;&#65292;&#29992;&#30456;&#21516;&#30340;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#21487;&#20197;&#25552;&#39640;&#32467;&#26524;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the components of natural language processing that has received a lot of investigation recently is semantic textual similarity. In computational linguistics and natural language processing, assessing the semantic similarity of words, phrases, paragraphs, and texts is crucial. Calculating the degree of semantic resemblance between two textual pieces, paragraphs, or phrases provided in both monolingual and cross-lingual versions is known as semantic similarity. Cross lingual semantic similarity requires corpora in which there are sentence pairs in both the source and target languages with a degree of semantic similarity between them. Many existing cross lingual semantic similarity models use a machine translation due to the unavailability of cross lingual semantic similarity dataset, which the propagation of the machine translation error reduces the accuracy of the model. On the other hand, when we want to use semantic similarity features for machine translation the same machine t
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;CAAFE&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#33258;&#21160;&#29305;&#24449;&#24037;&#31243;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26681;&#25454;&#25968;&#25454;&#38598;&#25551;&#36848;&#29983;&#25104;&#26356;&#22810;&#20855;&#26377;&#35821;&#20041;&#24847;&#20041;&#30340;&#29305;&#24449;&#65292;&#33021;&#22815;&#25552;&#39640;&#22823;&#22810;&#25968;&#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#65292;&#24179;&#22343;ROC AUC&#34920;&#29616;&#25552;&#39640;&#33267;0.822&#12290;</title><link>http://arxiv.org/abs/2305.03403</link><description>&lt;p&gt;
GPT&#29992;&#20110;&#21322;&#33258;&#21160;&#21270;&#25968;&#25454;&#31185;&#23398;&#65306;&#24341;&#20837;CAAFE&#23454;&#29616;&#19978;&#19979;&#25991;&#24863;&#30693;&#33258;&#21160;&#29305;&#24449;&#24037;&#31243;
&lt;/p&gt;
&lt;p&gt;
GPT for Semi-Automated Data Science: Introducing CAAFE for Context-Aware Automated Feature Engineering. (arXiv:2305.03403v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03403
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;CAAFE&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#33258;&#21160;&#29305;&#24449;&#24037;&#31243;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26681;&#25454;&#25968;&#25454;&#38598;&#25551;&#36848;&#29983;&#25104;&#26356;&#22810;&#20855;&#26377;&#35821;&#20041;&#24847;&#20041;&#30340;&#29305;&#24449;&#65292;&#33021;&#22815;&#25552;&#39640;&#22823;&#22810;&#25968;&#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#65292;&#24179;&#22343;ROC AUC&#34920;&#29616;&#25552;&#39640;&#33267;0.822&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#65288;AutoML&#65289;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#23558;&#39046;&#22495;&#30693;&#35782;&#32435;&#20837;&#36825;&#20123;&#31995;&#32479;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#25105;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24378;&#22823;&#21151;&#33021;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#29305;&#24449;&#24037;&#31243;&#26041;&#27861;&#65292;&#21517;&#20026;&#19978;&#19979;&#25991;&#24863;&#30693;&#33258;&#21160;&#29305;&#24449;&#24037;&#31243;&#65288;CAAFE&#65289;&#65292;&#23427;&#21033;&#29992;LLM&#26681;&#25454;&#25968;&#25454;&#38598;&#30340;&#25551;&#36848;&#29983;&#25104;&#26356;&#22810;&#20855;&#26377;&#35821;&#20041;&#24847;&#20041;&#30340;&#29305;&#24449;&#12290;&#35813;&#26041;&#27861;&#20135;&#29983;&#29992;&#20110;&#21019;&#24314;&#26032;&#29305;&#24449;&#30340;Python&#20195;&#30721;&#65292;&#24182;&#25552;&#20379;&#29983;&#25104;&#29305;&#24449;&#30340;&#25928;&#29992;&#35828;&#26126;&#12290;&#23613;&#31649;&#26041;&#27861;&#35770;&#19978;&#24456;&#31616;&#21333;&#65292;&#20294;CAAFE&#25552;&#39640;&#20102;14&#20010;&#25968;&#25454;&#38598;&#20013;11&#20010;&#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#65292;&#19982;2&#20010;&#25968;&#25454;&#38598;&#24182;&#21015;&#65292;&#21482;&#26377;1&#20010;&#25968;&#25454;&#38598;&#24615;&#33021;&#19979;&#38477;&#65292;&#20174;&#32780;&#20351;&#25152;&#26377;&#25968;&#25454;&#38598;&#30340;&#24179;&#22343;ROC AUC&#34920;&#29616;&#20174;0.798&#25552;&#21319;&#33267;0.822&#12290;&#23545;&#20110;&#25152;&#35780;&#20272;&#30340;&#25968;&#25454;&#38598;&#65292;&#36825;&#19968;&#25913;&#36827;&#19982;&#20351;&#29992;&#38543;&#26426;&#26862;&#26519;&#65288;AUC 0.782&#65289;&#20195;&#26367;&#36923;&#36753;&#22238;&#24402;&#65288;AUC 0.754&#65289;&#25152;&#33719;&#24471;&#30340;&#24179;&#22343;&#25913;&#36827;&#30456;&#20284;&#12290;&#27492;&#22806;&#65292;
&lt;/p&gt;
&lt;p&gt;
As the field of automated machine learning (AutoML) advances, it becomes increasingly important to include domain knowledge within these systems. We present an approach for doing so by harnessing the power of large language models (LLMs). Specifically, we introduce Context-Aware Automated Feature Engineering (CAAFE), a feature engineering method for tabular datasets that utilizes an LLM to generate additional semantically meaningful features for tabular datasets based on their descriptions. The method produces both Python code for creating new features and explanations for the utility of the generated features.  Despite being methodologically simple, CAAFE enhances performance on 11 out of 14 datasets, ties on 2 and looses on 1 - boosting mean ROC AUC performance from 0.798 to 0.822 across all datasets. On the evaluated datasets, this improvement is similar to the average improvement achieved by using a random forest (AUC 0.782) instead of logistic regression (AUC 0.754).  Furthermore,
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#31215;&#26497;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;PAL&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#26597;&#35810;&#26679;&#26412;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#26469;&#35299;&#20915;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#38656;&#35201;&#30693;&#36947;&#27491;&#35270;&#22270;&#30340;&#38480;&#21046;&#12290;PAL&#19981;&#20165;&#25552;&#20379;&#20102;&#29702;&#35770;&#19978;&#26377;&#22522;&#30784;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#36824;&#33021;&#22815;&#23884;&#20837;&#20808;&#39564;&#30693;&#35782;&#24182;&#25903;&#25345;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2303.15256</link><description>&lt;p&gt;
&#20027;&#21160;&#33258;&#30417;&#30563;&#23398;&#20064;: &#21482;&#38656;&#23569;&#37327;&#20302;&#25104;&#26412;&#30340;&#20851;&#31995;&#21363;&#21487;&#12290;
&lt;/p&gt;
&lt;p&gt;
Active Self-Supervised Learning: A Few Low-Cost Relationships Are All You Need. (arXiv:2303.15256v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15256
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#31215;&#26497;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;PAL&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#26597;&#35810;&#26679;&#26412;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#26469;&#35299;&#20915;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#38656;&#35201;&#30693;&#36947;&#27491;&#35270;&#22270;&#30340;&#38480;&#21046;&#12290;PAL&#19981;&#20165;&#25552;&#20379;&#20102;&#29702;&#35770;&#19978;&#26377;&#22522;&#30784;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#36824;&#33021;&#22815;&#23884;&#20837;&#20808;&#39564;&#30693;&#35782;&#24182;&#25903;&#25345;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#24050;&#25104;&#20026;&#23398;&#20064;&#26080;&#26631;&#35760;&#25968;&#25454;&#30340;&#21487;&#36716;&#31227;&#34920;&#31034;&#30340;&#39318;&#36873;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;SSL&#38656;&#35201;&#26500;&#24314;&#24050;&#30693;&#35821;&#20041;&#30456;&#20284;&#30340;&#26679;&#26412;&#65292;&#21363;&#27491;&#35270;&#22270;&#12290;&#36825;&#31181;&#38656;&#27714;&#26159;SSL&#30340;&#20027;&#35201;&#38480;&#21046;&#65292;&#36890;&#24120;&#36890;&#36807;&#20020;&#26102;&#31574;&#30053;&#26469;&#22788;&#29702;&#65292;&#20363;&#22914;&#23545;&#30456;&#21516;&#36755;&#20837;&#24212;&#29992;&#24050;&#30693;&#30340;&#25968;&#25454;&#22686;&#24378;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#31215;&#26497;&#20027;&#21160;&#23398;&#20064;&#65288;PAL&#65289;&#23558;&#36825;&#20010;&#21407;&#21017;&#24418;&#24335;&#21270;&#21644;&#25512;&#24191;&#65292;&#20854;&#20013;&#19968;&#20010;oracle&#26597;&#35810;&#26679;&#26412;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#12290;PAL&#23454;&#29616;&#20102;&#19977;&#20010;&#20027;&#35201;&#30446;&#26631;&#12290;&#39318;&#20808;&#65292;&#23427;&#25581;&#31034;&#20102;&#19968;&#20010;&#22522;&#20110;&#30456;&#20284;&#24615;&#22270;&#30340;&#29702;&#35770;&#19978;&#26377;&#22522;&#30784;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#36229;&#36234;&#20102;SSL&#65292;&#21487;&#26681;&#25454;&#25152;&#20351;&#29992;&#30340;oracle&#25193;&#23637;&#21040;&#22788;&#29702;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#12290;&#20854;&#27425;&#65292;&#23427;&#20026;&#23884;&#20837;&#20808;&#39564;&#30693;&#35782;&#65288;&#22914;&#35266;&#27979;&#30340;&#26631;&#31614;&#65289;&#25552;&#20379;&#20102;&#19968;&#33268;&#30340;&#31639;&#27861;&#65292;&#32780;&#26080;&#38656;&#26356;&#25913;&#35757;&#32451;&#27969;&#31243;&#20013;&#30340;&#20219;&#20309;&#20869;&#23481;&#12290;&#31532;&#19977;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#36866;&#24403;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Learning (SSL) has emerged as the solution of choice to learn transferable representations from unlabeled data. However, SSL requires to build samples that are known to be semantically akin, i.e. positive views. Requiring such knowledge is the main limitation of SSL and is often tackled by ad-hoc strategies e.g. applying known data-augmentations to the same input. In this work, we formalize and generalize this principle through Positive Active Learning (PAL) where an oracle queries semantic relationships between samples. PAL achieves three main objectives. First, it unveils a theoretically grounded learning framework beyond SSL, based on similarity graphs, that can be extended to tackle supervised and semi-supervised learning depending on the employed oracle. Second, it provides a consistent algorithm to embed a priori knowledge, e.g. some observed labels, into any SSL losses without any change in the training pipeline. Third, it provides a proper active learning framew
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#23545;&#20110;&#32422;&#26463;&#28385;&#36275;&#38382;&#39064;&#65292;&#37319;&#29992;&#19968;&#31181;&#39640;&#25928;&#30340;&#31639;&#27861;&#20197;&#26131;&#20110;&#29702;&#35299;&#30340;&#26041;&#24335;&#35299;&#37322;&#35299;&#20915;&#26041;&#26696;&#65292;&#36825;&#19968;&#26041;&#27861;&#21487;&#20197;&#35753;&#20154;&#31867;&#26356;&#22909;&#22320;&#29702;&#35299;&#26426;&#22120;&#25152;&#20570;&#30340;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2303.11712</link><description>&lt;p&gt;
&#39640;&#25928;&#35299;&#37322; CSPs &#30340;&#19981;&#21487;&#28385;&#36275;&#23376;&#38598;&#20248;&#21270;&#65288;&#25193;&#23637;&#31639;&#27861;&#21644;&#31034;&#20363;&#65289;
&lt;/p&gt;
&lt;p&gt;
Efficiently Explaining CSPs with Unsatisfiable Subset Optimization (extended algorithms and examples). (arXiv:2303.11712v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#23545;&#20110;&#32422;&#26463;&#28385;&#36275;&#38382;&#39064;&#65292;&#37319;&#29992;&#19968;&#31181;&#39640;&#25928;&#30340;&#31639;&#27861;&#20197;&#26131;&#20110;&#29702;&#35299;&#30340;&#26041;&#24335;&#35299;&#37322;&#35299;&#20915;&#26041;&#26696;&#65292;&#36825;&#19968;&#26041;&#27861;&#21487;&#20197;&#35753;&#20154;&#31867;&#26356;&#22909;&#22320;&#29702;&#35299;&#26426;&#22120;&#25152;&#20570;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#26368;&#36817;&#25552;&#20986;&#30340;&#26041;&#27861;&#22522;&#30784;&#19978;&#65292;&#20026;&#36880;&#27493;&#20197;&#26131;&#20110;&#29702;&#35299;&#26041;&#24335;&#35299;&#37322;&#32422;&#26463;&#28385;&#36275;&#38382;&#39064;&#65288;CSP&#65289;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36825;&#37324;&#30340;&#35299;&#37322;&#26159;&#19968;&#31995;&#21015;&#31616;&#21333;&#30340;&#25512;&#26029;&#27493;&#39588;&#65292;&#20854;&#20013;&#31616;&#21333;&#24615;&#20351;&#29992;&#25104;&#26412;&#20989;&#25968;&#37327;&#21270;&#12290;&#35299;&#37322;&#29983;&#25104;&#31639;&#27861;&#20381;&#36182;&#20110;&#20174;&#23548;&#20986;&#30340;&#19981;&#21487;&#28385;&#36275;&#20844;&#24335;&#20013;&#25552;&#21462;&#26368;&#23567;&#19981;&#28385;&#23376;&#38598;&#65288;MUS&#65289;&#65292;&#21033;&#29992;&#25152;&#35859;&#30340;&#38750;&#20887;&#20313;&#35299;&#37322;&#21644; MUS &#20043;&#38388;&#30340;&#19968;&#19968;&#23545;&#24212;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;MUS &#25552;&#21462;&#31639;&#27861;&#19981;&#25552;&#20379;&#20219;&#20309;&#38024;&#23545;&#32473;&#23450;&#25104;&#26412;&#20989;&#25968;&#30340;&#23376;&#38598;&#26368;&#23567;&#24615;&#25110;&#26368;&#20248;&#24615;&#20445;&#35777;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#22312;&#36825;&#20123;&#24418;&#24335;&#22522;&#30784;&#19978;&#24314;&#31435;&#65292;&#24182;&#30528;&#25163;&#25913;&#36827;&#30340;&#20027;&#35201;&#35201;&#28857;&#65292;&#21363;&#22914;&#20309;&#39640;&#25928;&#22320;&#29983;&#25104;&#21487;&#35777;&#26126;&#26159;&#26368;&#20248;&#30340;&#35299;&#37322;&#65288;&#19982;&#32473;&#23450;&#25104;&#26412;&#24230;&#37327;&#30456;&#20851;&#65289;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#65288;1&#65289;&#22522;&#20110;&#21629;&#20013;&#38598;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#26597;&#25214;&#26368;&#20339;&#21463;&#38480;&#19981;&#21487;&#28385;&#36275;&#23376;&#38598;&#65307;&#65288;2&#65289;&#19968;&#31181;&#37325;&#29992;&#30456;&#20851;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#21487;&#22312;&#19981;&#21516;&#30340;&#35299;&#37322;&#29983;&#25104;&#38454;&#27573;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We build on a recently proposed method for stepwise explaining solutions of Constraint Satisfaction Problems (CSP) in a human-understandable way. An explanation here is a sequence of simple inference steps where simplicity is quantified using a cost function. The algorithms for explanation generation rely on extracting Minimal Unsatisfiable Subsets (MUS) of a derived unsatisfiable formula, exploiting a one-to-one correspondence between so-called non-redundant explanations and MUSs. However, MUS extraction algorithms do not provide any guarantee of subset minimality or optimality with respect to a given cost function. Therefore, we build on these formal foundations and tackle the main points of improvement, namely how to generate explanations efficiently that are provably optimal (with respect to the given cost metric). For that, we developed (1) a hitting set-based algorithm for finding the optimal constrained unsatisfiable subsets; (2) a method for re-using relevant information over m
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;hGAIL&#30340;&#26550;&#26500;&#65292;&#29992;&#20110;&#35299;&#20915;&#36710;&#36742;&#30340;&#33258;&#20027;&#23548;&#33322;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#24863;&#30693;&#20449;&#24687;&#30452;&#25509;&#26144;&#23556;&#21040;&#20302;&#32423;&#21160;&#20316;&#30340;&#21516;&#26102;&#65292;&#23398;&#20064;&#36710;&#36742;&#29615;&#22659;&#30340;&#20013;&#32423;&#36755;&#20837;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2302.04823</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#23618;&#29983;&#25104;&#23545;&#25239;&#27169;&#25311;&#23398;&#20064;&#30340;&#33258;&#21160;&#39550;&#39542;&#22312;&#22478;&#24066;&#29615;&#22659;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Generative Adversarial Imitation Learning with Mid-level Input Generation for Autonomous Driving on Urban Environments. (arXiv:2302.04823v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04823
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;hGAIL&#30340;&#26550;&#26500;&#65292;&#29992;&#20110;&#35299;&#20915;&#36710;&#36742;&#30340;&#33258;&#20027;&#23548;&#33322;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#24863;&#30693;&#20449;&#24687;&#30452;&#25509;&#26144;&#23556;&#21040;&#20302;&#32423;&#21160;&#20316;&#30340;&#21516;&#26102;&#65292;&#23398;&#20064;&#36710;&#36742;&#29615;&#22659;&#30340;&#20013;&#32423;&#36755;&#20837;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#29616;&#23454;&#20013;&#30340;&#22478;&#24066;&#23548;&#33322;&#22330;&#26223;&#65292;&#35774;&#35745;&#20581;&#22766;&#30340;&#25511;&#21046;&#31574;&#30053;&#24182;&#19981;&#26159;&#19968;&#39033;&#31616;&#21333;&#30340;&#20219;&#21153;&#12290;&#22312;&#31471;&#21040;&#31471;&#30340;&#26041;&#27861;&#20013;&#65292;&#36825;&#20123;&#31574;&#30053;&#24517;&#39035;&#23558;&#36710;&#36742;&#25668;&#20687;&#22836;&#33719;&#24471;&#30340;&#39640;&#32500;&#22270;&#20687;&#26144;&#23556;&#21040;&#20302;&#32423;&#21160;&#20316;&#65292;&#22914;&#36716;&#21521;&#21644;&#27833;&#38376;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;hGAIL&#30340;&#26550;&#26500;&#65292;&#29992;&#20110;&#35299;&#20915;&#36710;&#36742;&#30340;&#33258;&#20027;&#23548;&#33322;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#24863;&#30693;&#20449;&#24687;&#30452;&#25509;&#26144;&#23556;&#21040;&#20302;&#32423;&#21160;&#20316;&#30340;&#21516;&#26102;&#65292;&#23398;&#20064;&#36710;&#36742;&#29615;&#22659;&#30340;&#20013;&#32423;&#36755;&#20837;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deriving robust control policies for realistic urban navigation scenarios is not a trivial task. In an end-to-end approach, these policies must map high-dimensional images from the vehicle's cameras to low-level actions such as steering and throttle. While pure Reinforcement Learning (RL) approaches are based exclusively on rewards,Generative Adversarial Imitation Learning (GAIL) agents learn from expert demonstrations while interacting with the environment, which favors GAIL on tasks for which a reward signal is difficult to derive. In this work, the hGAIL architecture was proposed to solve the autonomous navigation of a vehicle in an end-to-end approach, mapping sensory perceptions directly to low-level actions, while simultaneously learning mid-level input representations of the agent's environment. The proposed hGAIL consists of an hierarchical Adversarial Imitation Learning architecture composed of two main modules: the GAN (Generative Adversarial Nets) which generates the Bird's-
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#36866;&#24212;&#24615;&#35843;&#24230;&#35268;&#21017;&#26469;&#35299;&#20915;&#20013;&#26029;&#20132;&#25442;&#20801;&#35768;&#30340;&#38459;&#22622;&#24037;&#20214;&#36710;&#38388;&#38382;&#39064;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#27169;&#25311;&#22120;&#26469;&#27169;&#25311;&#23454;&#38469;&#24037;&#19994;&#29983;&#20135;&#20013;&#30340;&#20013;&#26029;&#21644;&#20132;&#25442;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2302.02506</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#24378;&#21270;&#23398;&#20064;&#29983;&#25104;&#33021;&#20013;&#26029;&#20132;&#25442;&#30340;&#38459;&#22622;&#24037;&#20214;&#36710;&#38388;&#38382;&#39064;&#20013;&#30340;&#35843;&#24230;&#35268;&#21017;
&lt;/p&gt;
&lt;p&gt;
Generating Dispatching Rules for the Interrupting Swap-Allowed Blocking Job Shop Problem Using Graph Neural Network and Reinforcement Learning. (arXiv:2302.02506v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02506
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#36866;&#24212;&#24615;&#35843;&#24230;&#35268;&#21017;&#26469;&#35299;&#20915;&#20013;&#26029;&#20132;&#25442;&#20801;&#35768;&#30340;&#38459;&#22622;&#24037;&#20214;&#36710;&#38388;&#38382;&#39064;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#27169;&#25311;&#22120;&#26469;&#27169;&#25311;&#23454;&#38469;&#24037;&#19994;&#29983;&#20135;&#20013;&#30340;&#20013;&#26029;&#21644;&#20132;&#25442;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20013;&#26029;&#20132;&#25442;&#20801;&#35768;&#30340;&#38459;&#22622;&#24037;&#20214;&#36710;&#38388;&#38382;&#39064;&#65288;ISBJSSP&#65289;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#35843;&#24230;&#38382;&#39064;&#65292;&#33021;&#22815;&#36890;&#36807;&#35299;&#20915;&#23384;&#20648;&#33021;&#21147;&#19981;&#36275;&#21644;&#24847;&#22806;&#29983;&#20135;&#20013;&#26029;&#30340;&#38382;&#39064;&#65292;&#23454;&#38469;&#27169;&#25311;&#35768;&#22810;&#21046;&#36896;&#35268;&#21010;&#21644;&#29289;&#27969;&#24212;&#29992;&#12290;&#30001;&#20110;&#26426;&#22120;&#25925;&#38556;&#25110;&#32500;&#25252;&#30340;&#38543;&#26426;&#24178;&#25200;&#65292;&#24037;&#19994;&#29983;&#20135;&#29615;&#22659;&#36890;&#24120;&#36873;&#25321;&#37319;&#29992;&#35843;&#24230;&#35268;&#21017;&#26469;&#23454;&#29616;&#33258;&#36866;&#24212;&#30340;&#23454;&#26102;&#37325;&#35843;&#24230;&#65292;&#32780;&#19981;&#26159;&#38656;&#35201;&#22312;&#38382;&#39064;&#26465;&#20214;&#21160;&#24577;&#21464;&#21270;&#26102;&#36827;&#34892;&#26114;&#36149;&#30340;&#37325;&#26032;&#35745;&#31639;&#30340;&#20256;&#32479;&#26041;&#27861;&#12290;&#20026;&#20102;&#29983;&#25104;ISBJSSP&#38382;&#39064;&#30340;&#35843;&#24230;&#35268;&#21017;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21160;&#24577;&#30340;&#20998;&#31163;&#22270;&#20844;&#24335;&#65292;&#20854;&#20013;&#30340;&#33410;&#28857;&#21644;&#36793;&#21487;&#25345;&#32493;&#36827;&#34892;&#21024;&#38500;&#21644;&#28155;&#21152;&#12290;&#36825;&#31181;&#20844;&#24335;&#20351;&#24471;&#33021;&#22815;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#24378;&#21270;&#23398;&#20064;&#26469;&#35757;&#32451;&#33258;&#36866;&#24212;&#35843;&#24230;&#22120;&#12290;&#27492;&#22806;&#65292;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#27169;&#25311;&#22120;&#26469;&#27169;&#25311;&#20013;&#26029;&#12289;&#20132;&#25442;&#31561;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
The interrupting swap-allowed blocking job shop problem (ISBJSSP) is a complex scheduling problem that is able to model many manufacturing planning and logistics applications realistically by addressing both the lack of storage capacity and unforeseen production interruptions. Subjected to random disruptions due to machine malfunction or maintenance, industry production settings often choose to adopt dispatching rules to enable adaptive, real-time re-scheduling, rather than traditional methods that require costly re-computation on the new configuration every time the problem condition changes dynamically. To generate dispatching rules for the ISBJSSP problem, we introduce a dynamic disjunctive graph formulation characterized by nodes and edges subjected to continuous deletions and additions. This formulation enables the training of an adaptive scheduler utilizing graph neural networks and reinforcement learning. Furthermore, a simulator is developed to simulate interruption, swapping, 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;&#23398;&#20064;&#30340;&#22810;&#35270;&#35282;&#35270;&#35273;&#34701;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#22312;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#38388;&#27463;&#24615;&#20449;&#24687;&#32570;&#22833;&#38382;&#39064;&#12290;&#36890;&#36807;&#35774;&#35745;&#23398;&#20064;&#26694;&#26550;&#21644;&#25552;&#20986;&#30340;&#25554;&#34917;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#23545;&#39550;&#39542;&#21592;&#25163;&#21183;&#27963;&#21160;&#30340;&#20934;&#30830;&#20998;&#31867;&#21644;&#20301;&#32622;&#20272;&#35745;&#65292;&#25552;&#39640;&#20102;&#33258;&#21160;&#39550;&#39542;&#30340;&#23433;&#20840;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.12592</link><description>&lt;p&gt;
&#22810;&#35270;&#35282;&#35270;&#35273;&#34701;&#21512;&#30340;&#38598;&#25104;&#23398;&#20064;&#22312;&#36974;&#25377;&#21644;&#32570;&#22833;&#20449;&#24687;&#24773;&#20917;&#19979;&#30340;&#24212;&#29992;&#65306;&#26694;&#26550;&#21644;&#22522;&#20110;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#21496;&#26426;&#25163;&#21183;&#35782;&#21035;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Ensemble Learning for Fusion of Multiview Vision with Occlusion and Missing Information: Framework and Evaluations with Real-World Data and Applications in Driver Hand Activity Recognition. (arXiv:2301.12592v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12592
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;&#23398;&#20064;&#30340;&#22810;&#35270;&#35282;&#35270;&#35273;&#34701;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#22312;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#38388;&#27463;&#24615;&#20449;&#24687;&#32570;&#22833;&#38382;&#39064;&#12290;&#36890;&#36807;&#35774;&#35745;&#23398;&#20064;&#26694;&#26550;&#21644;&#25552;&#20986;&#30340;&#25554;&#34917;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#23545;&#39550;&#39542;&#21592;&#25163;&#21183;&#27963;&#21160;&#30340;&#20934;&#30830;&#20998;&#31867;&#21644;&#20301;&#32622;&#20272;&#35745;&#65292;&#25552;&#39640;&#20102;&#33258;&#21160;&#39550;&#39542;&#30340;&#23433;&#20840;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20256;&#24863;&#22120;&#26694;&#26550;&#20026;&#38598;&#25104;&#23398;&#20064;&#21644;&#20256;&#24863;&#22120;&#34701;&#21512;&#25552;&#20379;&#20102;&#26426;&#20250;&#65292;&#20197;&#21033;&#29992;&#20887;&#20313;&#21644;&#34917;&#20805;&#20449;&#24687;&#65292;&#26377;&#21161;&#20110;&#22312;&#30495;&#23454;&#19990;&#30028;&#23433;&#20840;&#24212;&#29992;&#20013;&#36827;&#34892;&#36830;&#32493;&#21496;&#26426;&#29366;&#24577;&#30417;&#27979;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#30001;&#36974;&#25377;&#12289;&#22122;&#22768;&#25110;&#20256;&#24863;&#22120;&#25925;&#38556;&#24341;&#36215;&#30340;&#38388;&#27463;&#24615;&#20449;&#24687;&#32570;&#22833;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#22260;&#32469;&#36825;&#20123;&#25968;&#25454;&#32570;&#21475;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#25552;&#20986;&#24182;&#20998;&#26512;&#20102;&#19968;&#31181;&#22635;&#34917;&#20449;&#24687;&#32570;&#22833;&#30340;&#25554;&#34917;&#26041;&#26696;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#24605;&#24819;&#24212;&#29992;&#20110;&#22522;&#20110;&#25668;&#20687;&#22836;&#30340;&#25163;&#21183;&#27963;&#21160;&#20998;&#31867;&#20219;&#21153;&#65292;&#20197;&#23454;&#29616;&#33258;&#21160;&#39550;&#39542;&#30340;&#23433;&#20840;&#24615;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22522;&#20110;&#24182;&#34892;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#21518;&#26399;&#34701;&#21512;&#26041;&#27861;&#65292;&#22312;&#21516;&#32452;&#21463;&#35797;&#32773;&#39564;&#35777;&#26102;&#65292;&#29978;&#33267;&#21487;&#20197;&#36229;&#36807;&#26368;&#20339;&#21333;&#19968;&#25668;&#20687;&#22836;&#27169;&#22411;&#22312;&#20272;&#35745;&#25163;&#20013;&#29289;&#20307;&#30340;&#20301;&#32622;&#26102;&#65292;&#25105;&#20204;&#30340;&#22810;&#25668;&#20687;&#22836;&#26694;&#26550;&#34920;&#29616;&#26368;&#22909;&#65292;&#32780;&#19988;&#22312;&#36328;&#32452;&#39564;&#35777;&#26102;&#20063;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-sensor frameworks provide opportunities for ensemble learning and sensor fusion to make use of redundancy and supplemental information, helpful in real-world safety applications such as continuous driver state monitoring which necessitate predictions even in cases where information may be intermittently missing. We define this problem of intermittent instances of missing information (by occlusion, noise, or sensor failure) and design a learning framework around these data gaps, proposing and analyzing an imputation scheme to handle missing information. We apply these ideas to tasks in camera-based hand activity classification for robust safety during autonomous driving. We show that a late-fusion approach between parallel convolutional neural networks can outperform even the best-placed single camera model in estimating the hands' held objects and positions when validated on within-group subjects, and that our multi-camera framework performs best on average in cross-group validat
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;MixUp-MIL&#29992;&#20110;&#22810;&#31034;&#20363;&#23398;&#20064;&#65292;&#36890;&#36807;&#24212;&#29992;&#20999;&#29255;&#20869;&#25554;&#20540;&#26041;&#27861;&#21487;&#20197;&#25913;&#21892;&#30002;&#29366;&#33146;&#30284;&#35786;&#26029;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.05862</link><description>&lt;p&gt;
MixUp-MIL&#65306;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#29992;&#20110;&#22810;&#31034;&#20363;&#23398;&#20064;&#24182;&#22312;&#30002;&#29366;&#33146;&#30284;&#35786;&#26029;&#20013;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
MixUp-MIL: Novel Data Augmentation for Multiple Instance Learning and a Study on Thyroid Cancer Diagnosis. (arXiv:2211.05862v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05862
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;MixUp-MIL&#29992;&#20110;&#22810;&#31034;&#20363;&#23398;&#20064;&#65292;&#36890;&#36807;&#24212;&#29992;&#20999;&#29255;&#20869;&#25554;&#20540;&#26041;&#27861;&#21487;&#20197;&#25913;&#21892;&#30002;&#29366;&#33146;&#30284;&#35786;&#26029;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20687;&#32032;&#25110;&#34917;&#19969;&#32423;&#21035;&#27880;&#37322;&#32570;&#22833;&#30340;&#24773;&#20917;&#19979;&#65292;&#22810;&#31034;&#20363;&#23398;&#20064;&#23637;&#31034;&#20102;&#19968;&#31181;&#22522;&#20110;&#25972;&#24352;&#20999;&#29255;&#22270;&#20687;&#30340;&#35786;&#26029;&#30340;&#24378;&#22823;&#26041;&#27861;&#12290;&#23613;&#31649;&#25972;&#24352;&#20999;&#29255;&#22270;&#20687;&#30340;&#22823;&#23567;&#24456;&#22823;&#65292;&#20294;&#20010;&#21035;&#20999;&#29255;&#30340;&#25968;&#37327;&#24448;&#24448;&#30456;&#23545;&#36739;&#23567;&#65292;&#23548;&#33268;&#26631;&#35760;&#26679;&#26412;&#25968;&#37327;&#23569;&#12290;&#20026;&#20102;&#25552;&#39640;&#35757;&#32451;&#25928;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#30740;&#31350;&#20102;&#22522;&#20110;&#29305;&#24449;&#21521;&#37327;&#30340;&#32447;&#24615;&#25554;&#20540;&#65288;&#21363;MixUp&#65289;&#24605;&#24819;&#30340;&#19981;&#21516;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#12290;&#22522;&#20110;&#26368;&#20808;&#36827;&#30340;&#22810;&#31034;&#20363;&#23398;&#20064;&#26550;&#26500;&#21644;&#20004;&#20010;&#30002;&#29366;&#33146;&#30284;&#25968;&#25454;&#38598;&#65292;&#36827;&#34892;&#20102;&#19968;&#39033;&#35814;&#23613;&#30340;&#30740;&#31350;&#65292;&#32771;&#34385;&#20102;&#22810;&#31181;&#24120;&#35265;&#30340;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#12290;&#23613;&#31649;&#22522;&#20110;&#21407;&#22987;MixUp&#26041;&#27861;&#30340;&#31574;&#30053;&#34920;&#29616;&#20986;&#20934;&#30830;&#24230;&#38477;&#20302;&#65292;&#20294;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#20999;&#29255;&#20869;&#25554;&#20540;&#26041;&#27861;&#21364;&#23454;&#29616;&#20102;&#19968;&#33268;&#30340;&#20934;&#30830;&#24230;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multiple instance learning exhibits a powerful approach for whole slide image-based diagnosis in the absence of pixel- or patch-level annotations. In spite of the huge size of hole slide images, the number of individual slides is often rather small, leading to a small number of labeled samples. To improve training, we propose and investigate different data augmentation strategies for multiple instance learning based on the idea of linear interpolations of feature vectors (known as MixUp). Based on state-of-the-art multiple instance learning architectures and two thyroid cancer data sets, an exhaustive study is conducted considering a range of common data augmentation strategies. Whereas a strategy based on to the original MixUp approach showed decreases in accuracy, the use of a novel intra-slide interpolation method led to consistent increases in accuracy.
&lt;/p&gt;</description></item></channel></rss>