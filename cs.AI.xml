<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#22312;&#21327;&#21464;&#37327;&#36716;&#31227;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#34920;&#31034;&#21305;&#37197;&#25439;&#22833;&#26469;&#20248;&#21270;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#22312;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#26435;&#34913;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#31639;&#27861;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#19968;&#31181;&#26410;&#32463;&#30740;&#31350;&#30340;&#38750;&#23545;&#31216;&#21327;&#21464;&#37327;&#36716;&#31227;&#35774;&#32622;&#12290;</title><link>http://arxiv.org/abs/2310.07535</link><description>&lt;p&gt;
&#22312;&#21327;&#21464;&#37327;&#36716;&#31227;&#19979;&#65292;&#20165;&#20973;&#23569;&#37327;&#27979;&#35797;&#26679;&#26412;&#25913;&#21892;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#30340;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Improving Fairness-Accuracy tradeoff with few Test Samples under Covariate Shift. (arXiv:2310.07535v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07535
&lt;/p&gt;
&lt;p&gt;
&#22312;&#21327;&#21464;&#37327;&#36716;&#31227;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#34920;&#31034;&#21305;&#37197;&#25439;&#22833;&#26469;&#20248;&#21270;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#22312;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#26435;&#34913;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#31639;&#27861;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#19968;&#31181;&#26410;&#32463;&#30740;&#31350;&#30340;&#38750;&#23545;&#31216;&#21327;&#21464;&#37327;&#36716;&#31227;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27979;&#35797;&#25968;&#25454;&#20013;&#30340;&#21327;&#21464;&#37327;&#36716;&#31227;&#20250;&#26174;&#33879;&#38477;&#20302;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#34920;&#29616;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#30830;&#20445;&#19981;&#21516;&#25935;&#24863;&#32676;&#20307;&#20043;&#38388;&#30340;&#20844;&#24179;&#24615;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#36825;&#28041;&#21450;&#21040;&#35832;&#22914;&#21009;&#20107;&#21496;&#27861;&#31561;&#31038;&#20250;&#24433;&#21709;&#12290;&#25105;&#20204;&#22312;&#26080;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#25805;&#20316;&#65292;&#21482;&#26377;&#19968;&#23567;&#32452;&#26080;&#26631;&#31614;&#30340;&#27979;&#35797;&#26679;&#26412;&#21644;&#19968;&#20010;&#24102;&#26631;&#31614;&#30340;&#35757;&#32451;&#38598;&#21487;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#20010;&#36129;&#29486;&#12290;&#31532;&#19968;&#20010;&#36129;&#29486;&#26159;&#22522;&#20110;&#26032;&#22411;&#22797;&#21512;&#21152;&#26435;&#29109;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#29992;&#20110;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#24182;&#36890;&#36807;&#34920;&#31034;&#21305;&#37197;&#25439;&#22833;&#26469;&#20248;&#21270;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#20351;&#29992;&#25105;&#20204;&#30340;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#20248;&#21270;&#65292;&#22312;&#20960;&#20010;&#26631;&#20934;&#25968;&#25454;&#38598;&#19978;&#22312;&#20844;&#24179;&#24615;-&#20934;&#30830;&#24615;&#26435;&#34913;&#26041;&#38754;&#20248;&#20110;&#35768;&#22810;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#31532;&#20108;&#20010;&#36129;&#29486;&#26159;&#19968;&#20010;&#26032;&#30340;&#35774;&#32622;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#38750;&#23545;&#31216;&#21327;&#21464;&#37327;&#36716;&#31227;&#65292;&#22312;&#25105;&#20204;&#25152;&#30693;&#30340;&#33539;&#22260;&#20869;&#23578;&#26410;&#30740;&#31350;&#36807;&#38750;&#23545;&#31216;&#21327;&#21464;&#37327;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;
Covariate shift in the test data can significantly downgrade both the accuracy and the fairness performance of the model. Ensuring fairness across different sensitive groups in such settings is of paramount importance due to societal implications like criminal justice. We operate under the unsupervised regime where only a small set of unlabeled test samples along with a labeled training set is available. Towards this problem, we make three contributions. First is a novel composite weighted entropy based objective for prediction accuracy which is optimized along with a representation matching loss for fairness. We experimentally verify that optimizing with our loss formulation outperforms a number of state-of-the-art baselines in the pareto sense with respect to the fairness-accuracy tradeoff on several standard datasets. Our second contribution is a new setting we term Asymmetric Covariate Shift that, to the best of our knowledge, has not been studied before. Asymmetric covariate shift
&lt;/p&gt;</description></item><item><title>&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#65292;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#40657;&#30418;&#23376;&#30340;&#20915;&#31574;&#36807;&#31243;&#26159;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#65292;&#23458;&#35266;&#35780;&#20272;&#20102;&#19977;&#31181;&#39046;&#20808;&#30340;&#35299;&#37322;&#26041;&#27861;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#37117;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.07534</link><description>&lt;p&gt;
XAI&#26041;&#27861;&#30340;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Human-Centered Evaluation of XAI Methods. (arXiv:2310.07534v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07534
&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#65292;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#40657;&#30418;&#23376;&#30340;&#20915;&#31574;&#36807;&#31243;&#26159;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#65292;&#23458;&#35266;&#35780;&#20272;&#20102;&#19977;&#31181;&#39046;&#20808;&#30340;&#35299;&#37322;&#26041;&#27861;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#37117;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#26029;&#21457;&#23637;&#30340;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#65292;&#19968;&#20010;&#20851;&#38190;&#30340;&#25361;&#25112;&#26159;&#35299;&#26512;&#28145;&#24230;&#23398;&#20064;&#20013;&#25152;&#35859;&#30340;&#8220;&#40657;&#30418;&#23376;&#8221;&#20013;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#36817;&#24180;&#26469;&#65292;&#20986;&#29616;&#20102;&#35768;&#22810;&#26041;&#27861;&#65292;&#19987;&#38376;&#29992;&#20110;&#35299;&#37322;&#21508;&#31181;&#20219;&#21153;&#30340;&#20915;&#31574;&#12290;&#29305;&#21035;&#26159;&#22312;&#22270;&#20687;&#20998;&#31867;&#31561;&#20219;&#21153;&#20013;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#20250;&#35782;&#21035;&#24182;&#24378;&#35843;&#23545;&#20998;&#31867;&#22120;&#39044;&#27979;&#24433;&#21709;&#26368;&#22823;&#30340;&#20851;&#38190;&#20687;&#32032;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#36825;&#31181;&#26041;&#27861;&#19982;&#20154;&#31867;&#34892;&#20026;&#30456;&#20284;&#65306;&#24403;&#25105;&#20204;&#34987;&#35201;&#27714;&#35299;&#37322;&#20998;&#31867;&#22270;&#20687;&#30340;&#29702;&#30001;&#26102;&#65292;&#25105;&#20204;&#36890;&#24120;&#20250;&#25351;&#20986;&#26368;&#26174;&#33879;&#30340;&#29305;&#24449;&#25110;&#26041;&#38754;&#12290;&#21033;&#29992;&#36825;&#31181;&#31867;&#20284;&#24615;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#36827;&#34892;&#20102;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#35797;&#22270;&#23458;&#35266;&#22320;&#35780;&#20272;&#19977;&#31181;&#39046;&#20808;&#30340;&#35299;&#37322;&#26041;&#27861;&#30340;&#21487;&#35299;&#37322;&#24615;&#65306;&#65288;1&#65289;&#20856;&#22411;&#23616;&#37096;&#32593;&#32476;&#12289;&#65288;2&#65289;&#36974;&#25377;&#21644;&#65288;3&#65289;&#23618;&#27425;&#30456;&#20851;&#20256;&#25773;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#25152;&#31361;&#20986;&#30340;&#21306;&#22495;&#21487;&#33021;&#24046;&#24322;&#24456;&#22823;&#65292;&#20294;&#23427;&#20204;&#37117;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the ever-evolving field of Artificial Intelligence, a critical challenge has been to decipher the decision-making processes within the so-called "black boxes" in deep learning. Over recent years, a plethora of methods have emerged, dedicated to explaining decisions across diverse tasks. Particularly in tasks like image classification, these methods typically identify and emphasize the pivotal pixels that most influence a classifier's prediction. Interestingly, this approach mirrors human behavior: when asked to explain our rationale for classifying an image, we often point to the most salient features or aspects. Capitalizing on this parallel, our research embarked on a user-centric study. We sought to objectively measure the interpretability of three leading explanation methods: (1) Prototypical Part Network, (2) Occlusion, and (3) Layer-wise Relevance Propagation. Intriguingly, our results highlight that while the regions spotlighted by these methods can vary widely, they all offe
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20272;&#35745;&#21644;&#20998;&#26512;&#20102;&#20174;&#35774;&#22791;&#21040;&#31639;&#27861;&#30340;&#35745;&#31639;&#23618;&#32423;&#20013;&#30340;&#33021;&#37327;&#28040;&#32791;&#12290;&#22312;&#19977;&#20010;&#22823;&#35268;&#27169;&#35745;&#31639;&#24212;&#29992;&#20013;&#65292;&#33021;&#37327;&#28040;&#32791;&#26082;&#23384;&#22312;&#20110;&#25351;&#20196;&#23618;&#38754;&#65292;&#20063;&#23384;&#22312;&#20110;&#27169;&#25311;&#23618;&#38754;&#12290;&#27492;&#22806;&#65292;&#23545;&#20351;&#29992;&#19981;&#21516;&#26550;&#26500;&#30340;AI/ML&#21152;&#36895;&#22120;&#36827;&#34892;&#20102;&#33021;&#37327;&#25928;&#29575;&#27604;&#36739;&#12290;&#19982;&#28909;&#21147;&#23398;&#21644;&#29983;&#29289;&#23398;&#30028;&#38480;&#30456;&#27604;&#65292;&#35745;&#31639;&#31995;&#32479;&#30340;&#33021;&#37327;&#28040;&#32791;&#30456;&#24046;27-36&#20010;&#25968;&#37327;&#32423;&#12290;</title><link>http://arxiv.org/abs/2310.07516</link><description>&lt;p&gt;
&#36328;&#23618;&#32423;&#35745;&#31639;&#30340;&#33021;&#37327;&#20272;&#35745;&#65306;&#20174;&#35774;&#22791;&#21040;&#26426;&#22120;&#23398;&#20064;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#31185;&#23398;&#35745;&#31639;&#21644;&#21152;&#23494;&#36135;&#24065;&#25366;&#25496;&#20013;&#30340;&#22823;&#35268;&#27169;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Energy Estimates Across Layers of Computing: From Devices to Large-Scale Applications in Machine Learning for Natural Language Processing, Scientific Computing, and Cryptocurrency Mining. (arXiv:2310.07516v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07516
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20272;&#35745;&#21644;&#20998;&#26512;&#20102;&#20174;&#35774;&#22791;&#21040;&#31639;&#27861;&#30340;&#35745;&#31639;&#23618;&#32423;&#20013;&#30340;&#33021;&#37327;&#28040;&#32791;&#12290;&#22312;&#19977;&#20010;&#22823;&#35268;&#27169;&#35745;&#31639;&#24212;&#29992;&#20013;&#65292;&#33021;&#37327;&#28040;&#32791;&#26082;&#23384;&#22312;&#20110;&#25351;&#20196;&#23618;&#38754;&#65292;&#20063;&#23384;&#22312;&#20110;&#27169;&#25311;&#23618;&#38754;&#12290;&#27492;&#22806;&#65292;&#23545;&#20351;&#29992;&#19981;&#21516;&#26550;&#26500;&#30340;AI/ML&#21152;&#36895;&#22120;&#36827;&#34892;&#20102;&#33021;&#37327;&#25928;&#29575;&#27604;&#36739;&#12290;&#19982;&#28909;&#21147;&#23398;&#21644;&#29983;&#29289;&#23398;&#30028;&#38480;&#30456;&#27604;&#65292;&#35745;&#31639;&#31995;&#32479;&#30340;&#33021;&#37327;&#28040;&#32791;&#30456;&#24046;27-36&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20174;&#35774;&#22791;&#21040;&#31639;&#27861;&#30340;&#35745;&#31639;&#23618;&#32423;&#20013;&#30340;&#33021;&#37327;&#28040;&#32791;&#36827;&#34892;&#20102;&#20272;&#35745;&#21644;&#20998;&#26512;&#12290;&#22312;&#20043;&#21069;&#30340;&#20998;&#26512;&#22522;&#30784;&#19978;[3]&#65292;&#20272;&#35745;&#20102;&#21253;&#25324;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;/&#26426;&#22120;&#23398;&#20064;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#31185;&#23398;&#27169;&#25311;&#21644;&#21152;&#23494;&#36135;&#24065;&#25366;&#25496;&#31561;&#19977;&#20010;&#22823;&#35268;&#27169;&#35745;&#31639;&#24212;&#29992;&#20013;&#25152;&#38656;&#30340;&#33021;&#37327;&#12290;&#19982;&#27604;&#29305;&#32423;&#20999;&#25442;&#19981;&#21516;&#65292;&#22312;&#24212;&#29992;&#30340;&#25351;&#20196;&#21644;&#27169;&#25311;&#23618;&#38754;&#19978;&#37117;&#28040;&#32791;&#20102;&#26356;&#22810;&#30340;&#33021;&#37327;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;AI/ML&#21152;&#36895;&#22120;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#20351;&#29992;&#36739;&#26087;&#30340;&#21322;&#23548;&#20307;&#25216;&#26415;&#33410;&#28857;&#30340;&#26550;&#26500;&#19982;&#20351;&#29992;&#36739;&#26032;&#25216;&#26415;&#30340;&#19981;&#21516;&#26550;&#26500;&#20855;&#26377;&#21487;&#27604;&#36739;&#30340;&#33021;&#37327;&#25928;&#29575;&#12290;&#36827;&#19968;&#27493;&#23558;&#35745;&#31639;&#31995;&#32479;&#30340;&#33021;&#37327;&#19982;&#28909;&#21147;&#23398;&#21644;&#29983;&#29289;&#23398;&#30028;&#38480;&#36827;&#34892;&#27604;&#36739;&#65292;&#34920;&#26126;&#22823;&#32422;&#26377;27-36&#20010;&#25968;&#37327;&#32423;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimates of energy usage in layers of computing from devices to algorithms have been determined and analyzed. Building on the previous analysis [3], energy needed from single devices and systems including three large-scale computing applications such as Artificial Intelligence (AI)/Machine Learning for Natural Language Processing, Scientific Simulations, and Cryptocurrency Mining have been estimated. In contrast to the bit-level switching, in which transistors achieved energy efficiency due to geometrical scaling, higher energy is expended both at the at the instructions and simulations levels of an application. Additionally, the analysis based on AI/ML Accelerators indicate that changes in architectures using an older semiconductor technology node have comparable energy efficiency with a different architecture using a newer technology. Further comparisons of the energy in computing systems with the thermodynamic and biological limits, indicate that there is a 27-36 orders of magnitud
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20855;&#26377;&#23454;&#26102;&#24863;&#30693;&#33021;&#21147;&#30340;IoT&#32593;&#32476;&#35774;&#35745;&#30340;&#22522;&#20110;&#26679;&#26412;&#39537;&#21160;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#25511;&#21046;&#37319;&#26679;&#36807;&#31243;&#26469;&#20943;&#36731;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#25552;&#39640;&#25972;&#20307;&#20934;&#30830;&#24615;&#65292;&#24182;&#35299;&#20915;&#33021;&#25928;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.07497</link><description>&lt;p&gt;
&#22522;&#20110;&#26679;&#26412;&#39537;&#21160;&#30340;&#32852;&#37030;&#23398;&#20064;&#29992;&#20110;&#33021;&#25928;&#21644;&#23454;&#26102;IoT&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
Sample-Driven Federated Learning for Energy-Efficient and Real-Time IoT Sensing. (arXiv:2310.07497v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07497
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20855;&#26377;&#23454;&#26102;&#24863;&#30693;&#33021;&#21147;&#30340;IoT&#32593;&#32476;&#35774;&#35745;&#30340;&#22522;&#20110;&#26679;&#26412;&#39537;&#21160;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#25511;&#21046;&#37319;&#26679;&#36807;&#31243;&#26469;&#20943;&#36731;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#25552;&#39640;&#25972;&#20307;&#20934;&#30830;&#24615;&#65292;&#24182;&#35299;&#20915;&#33021;&#25928;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#39046;&#22495;&#65292;&#26368;&#36817;&#30340;&#21069;&#27839;&#26041;&#27861;&#22312;&#25910;&#25947;&#20998;&#26512;&#20013;&#20005;&#37325;&#20381;&#36182;&#20110;&#29702;&#24819;&#26465;&#20214;&#12290;&#29305;&#21035;&#22320;&#65292;&#36825;&#20123;&#26041;&#27861;&#20551;&#35774;IoT&#35774;&#22791;&#19978;&#30340;&#35757;&#32451;&#25968;&#25454;&#20855;&#26377;&#19982;&#20840;&#23616;&#25968;&#25454;&#20998;&#24067;&#30456;&#20284;&#30340;&#23646;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#26102;&#24863;&#30693;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#20013;&#65292;&#36825;&#31181;&#26041;&#27861;&#26080;&#27861;&#25429;&#25417;&#21040;&#25968;&#25454;&#29305;&#24449;&#30340;&#20840;&#38754;&#33539;&#22260;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20855;&#26377;&#23454;&#26102;&#24863;&#30693;&#33021;&#21147;&#30340;IoT&#32593;&#32476;&#35774;&#35745;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32771;&#34385;&#20102;&#30001;&#29992;&#25143;&#25968;&#25454;&#37319;&#26679;&#36807;&#31243;&#24341;&#36215;&#30340;&#27867;&#21270;&#24046;&#36317;&#12290;&#36890;&#36807;&#26377;&#25928;&#22320;&#25511;&#21046;&#36825;&#20010;&#37319;&#26679;&#36807;&#31243;&#65292;&#25105;&#20204;&#21487;&#20197;&#20943;&#36731;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#24182;&#25552;&#39640;&#25972;&#20307;&#20934;&#30830;&#24615;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#39318;&#20808;&#21046;&#23450;&#20102;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#65292;&#21033;&#29992;&#37319;&#26679;&#36807;&#31243;&#21516;&#26102;&#20943;&#23569;&#36807;&#25311;&#21512;&#21644;&#26368;&#22823;&#21270;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#36798;&#21040;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#30340;&#26367;&#20195;&#20248;&#21270;&#38382;&#39064;&#25797;&#38271;&#22788;&#29702;&#33021;&#25928;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the domain of Federated Learning (FL) systems, recent cutting-edge methods heavily rely on ideal conditions convergence analysis. Specifically, these approaches assume that the training datasets on IoT devices possess similar attributes to the global data distribution. However, this approach fails to capture the full spectrum of data characteristics in real-time sensing FL systems. In order to overcome this limitation, we suggest a new approach system specifically designed for IoT networks with real-time sensing capabilities. Our approach takes into account the generalization gap due to the user's data sampling process. By effectively controlling this sampling process, we can mitigate the overfitting issue and improve overall accuracy. In particular, We first formulate an optimization problem that harnesses the sampling process to concurrently reduce overfitting while maximizing accuracy. In pursuit of this objective, our surrogate optimization problem is adept at handling energy ef
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#36890;&#36807;&#23398;&#20064;&#22810;&#26679;&#30340;&#34892;&#20026;&#26469;&#23454;&#29616;&#36866;&#24212;&#21644;&#36801;&#31227;&#12290;&#36890;&#36807;&#36845;&#20195;&#23398;&#20064;&#19968;&#32452;&#31574;&#30053;&#65292;&#24182;&#21033;&#29992;&#32422;&#26463;&#26469;&#21457;&#29616;&#32473;&#23450;&#20219;&#21153;&#30340;&#25152;&#26377;&#21487;&#33021;&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#36801;&#31227;&#35774;&#32622;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#24555;&#36895;&#36866;&#24212;&#20219;&#21153;&#25110;&#36716;&#25442;&#21160;&#21147;&#23398;&#30340;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2310.07493</link><description>&lt;p&gt;
&#22810;&#26679;&#24615;&#23545;&#20110;&#24212;&#21464;&#30340;&#37325;&#35201;&#24615;&#65306;&#23398;&#20064;&#22810;&#26679;&#34892;&#20026;&#20197;&#23454;&#29616;&#39640;&#25928;&#36866;&#24212;&#21644;&#36801;&#31227;
&lt;/p&gt;
&lt;p&gt;
Diversity for Contingency: Learning Diverse Behaviors for Efficient Adaptation and Transfer. (arXiv:2310.07493v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07493
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#36890;&#36807;&#23398;&#20064;&#22810;&#26679;&#30340;&#34892;&#20026;&#26469;&#23454;&#29616;&#36866;&#24212;&#21644;&#36801;&#31227;&#12290;&#36890;&#36807;&#36845;&#20195;&#23398;&#20064;&#19968;&#32452;&#31574;&#30053;&#65292;&#24182;&#21033;&#29992;&#32422;&#26463;&#26469;&#21457;&#29616;&#32473;&#23450;&#20219;&#21153;&#30340;&#25152;&#26377;&#21487;&#33021;&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#36801;&#31227;&#35774;&#32622;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#24555;&#36895;&#36866;&#24212;&#20219;&#21153;&#25110;&#36716;&#25442;&#21160;&#21147;&#23398;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#29616;&#32473;&#23450;&#20219;&#21153;&#30340;&#25152;&#26377;&#26377;&#29992;&#35299;&#20915;&#26041;&#26696;&#23545;&#20110;&#21487;&#36801;&#31227;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#24212;&#23545;&#20219;&#21153;&#25110;&#36716;&#25442;&#21160;&#21147;&#23398;&#30340;&#21464;&#21270;&#12290;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21482;&#20851;&#27880;&#22312;&#24403;&#21069;&#20219;&#21153;&#21644;&#21160;&#21147;&#23398;&#19979;&#25214;&#21040;&#26368;&#20248;&#31574;&#30053;&#65292;&#32780;&#19981;&#32771;&#34385;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#21457;&#29616;&#32473;&#23450;&#20219;&#21153;&#30340;&#25152;&#26377;&#21487;&#33021;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#33719;&#24471;&#22312;&#36801;&#31227;&#35774;&#32622;&#20013;&#34920;&#29616;&#33391;&#22909;&#24182;&#24555;&#36895;&#36866;&#24212;&#20219;&#21153;&#25110;&#36716;&#25442;&#21160;&#21147;&#23398;&#21464;&#21270;&#30340;&#20195;&#29702;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#36845;&#20195;&#23398;&#20064;&#19968;&#32452;&#31574;&#30053;&#65292;&#20854;&#20013;&#27599;&#20010;&#21518;&#32493;&#31574;&#30053;&#34987;&#38480;&#21046;&#20026;&#22312;&#25152;&#26377;&#20808;&#21069;&#31574;&#30053;&#19979;&#37117;&#19981;&#22826;&#21487;&#33021;&#23384;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#19982;&#20808;&#21069;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#23398;&#20064;&#39069;&#22806;&#30340;&#27169;&#22411;&#36827;&#34892;&#26032;&#39062;&#24615;&#26816;&#27979;&#65292;&#24182;&#36890;&#36807;&#30452;&#25509;&#23558;&#32422;&#26463;&#25972;&#21512;&#21040;&#21160;&#20316;&#36873;&#25321;&#21644;&#20248;&#21270;&#27493;&#39588;&#20013;&#36991;&#20813;&#20219;&#21153;&#21644;&#26032;&#39062;&#24615;&#22870;&#21169;&#20449;&#21495;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discovering all useful solutions for a given task is crucial for transferable RL agents, to account for changes in the task or transition dynamics. This is not considered by classical RL algorithms that are only concerned with finding the optimal policy, given the current task and dynamics. We propose a simple method for discovering all possible solutions of a given task, to obtain an agent that performs well in the transfer setting and adapts quickly to changes in the task or transition dynamics. Our method iteratively learns a set of policies, while each subsequent policy is constrained to yield a solution that is unlikely under all previous policies. Unlike prior methods, our approach does not require learning additional models for novelty detection and avoids balancing task and novelty reward signals, by directly incorporating the constraint into the action selection and optimization steps.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#40657;&#30418;&#25915;&#20987;&#31574;&#30053;&#65292;&#36890;&#36807;&#26465;&#20214;&#36716;&#25442;&#29983;&#25104;&#21487;&#25509;&#21463;&#30340;&#23545;&#25239;&#26679;&#26412;&#65292;&#20197;&#25552;&#39640;&#22312;&#26597;&#35810;&#21463;&#38480;&#24773;&#20917;&#19979;&#29983;&#25104;&#23545;&#25239;&#26679;&#26412;&#30340;&#26597;&#35810;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.07492</link><description>&lt;p&gt;
&#29992;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#25552;&#21319;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#40657;&#30418;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Boosting Black-box Attack to Deep Neural Networks with Conditional Diffusion Models. (arXiv:2310.07492v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07492
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#40657;&#30418;&#25915;&#20987;&#31574;&#30053;&#65292;&#36890;&#36807;&#26465;&#20214;&#36716;&#25442;&#29983;&#25104;&#21487;&#25509;&#21463;&#30340;&#23545;&#25239;&#26679;&#26412;&#65292;&#20197;&#25552;&#39640;&#22312;&#26597;&#35810;&#21463;&#38480;&#24773;&#20917;&#19979;&#29983;&#25104;&#23545;&#25239;&#26679;&#26412;&#30340;&#26597;&#35810;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#40657;&#30418;&#25915;&#20987;&#24050;&#32463;&#23637;&#29616;&#20986;&#22312;&#27450;&#39575;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#21019;&#24314;&#23545;&#25239;&#26679;&#26412;&#65288;AE&#65289;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#20854;&#20013;&#22823;&#37096;&#20998;&#25915;&#20987;&#38656;&#35201;&#22788;&#29702;&#24222;&#22823;&#30340;&#20248;&#21270;&#31354;&#38388;&#24182;&#38656;&#35201;&#22823;&#37327;&#30340;&#26597;&#35810;&#65292;&#22240;&#27492;&#22312;&#23454;&#38469;&#24773;&#20917;&#19979;&#20855;&#26377;&#38480;&#21046;&#30340;&#23454;&#38469;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#40657;&#30418;&#25915;&#20987;&#31574;&#30053;&#65292;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#25915;&#20987;&#65288;CDMA&#65289;&#65292;&#20197;&#25552;&#39640;&#22312;&#26597;&#35810;&#21463;&#38480;&#24773;&#20917;&#19979;&#29983;&#25104;AE&#30340;&#26597;&#35810;&#25928;&#29575;&#12290;CDMA&#30340;&#20851;&#38190;&#27934;&#23519;&#26159;&#23558;AE&#21512;&#25104;&#30340;&#20219;&#21153;&#24418;&#24335;&#21270;&#20026;&#20998;&#24067;&#36716;&#25442;&#38382;&#39064;&#65292;&#21363;&#33391;&#24615;&#31034;&#20363;&#21450;&#20854;&#23545;&#24212;&#30340;AE&#21487;&#20197;&#34987;&#35270;&#20026;&#26469;&#33258;&#20004;&#20010;&#19981;&#21516;&#30340;&#20998;&#24067;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#29305;&#23450;&#30340;&#36716;&#25442;&#22120;&#30456;&#20114;&#36716;&#25442;&#12290;&#19982;&#20256;&#32479;&#30340;&#8220;&#26597;&#35810;&#21644;&#20248;&#21270;&#8221;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#20351;&#29992;&#19978;&#36848;&#25968;&#25454;&#36716;&#25442;&#22120;&#30452;&#25509;&#36827;&#34892;&#26465;&#20214;&#36716;&#25442;&#26469;&#29983;&#25104;&#31526;&#21512;&#35201;&#27714;&#30340;AE&#65292;&#36825;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#25152;&#38656;&#30340;&#26597;&#35810;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing black-box attacks have demonstrated promising potential in creating adversarial examples (AE) to deceive deep learning models. Most of these attacks need to handle a vast optimization space and require a large number of queries, hence exhibiting limited practical impacts in real-world scenarios. In this paper, we propose a novel black-box attack strategy, Conditional Diffusion Model Attack (CDMA), to improve the query efficiency of generating AEs under query-limited situations. The key insight of CDMA is to formulate the task of AE synthesis as a distribution transformation problem, i.e., benign examples and their corresponding AEs can be regarded as coming from two distinctive distributions and can transform from each other with a particular converter. Unlike the conventional \textit{query-and-optimization} approach, we generate eligible AEs with direct conditional transform using the aforementioned data converter, which can significantly reduce the number of queries needed. 
&lt;/p&gt;</description></item><item><title>KwaiYiiMath&#26159;&#19968;&#20010;&#29992;&#20110;&#22686;&#24378;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#24212;&#29992;&#30417;&#30563;&#24494;&#35843;&#21644;&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#65292;&#22312;&#33521;&#35821;&#21644;&#20013;&#25991;&#25968;&#23398;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#33021;&#22815;&#27491;&#30830;&#35299;&#20915;&#29983;&#25104;&#30340;&#38382;&#39064;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2310.07488</link><description>&lt;p&gt;
KwaiYiiMath: &#25216;&#26415;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
KwaiYiiMath: Technical Report. (arXiv:2310.07488v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07488
&lt;/p&gt;
&lt;p&gt;
KwaiYiiMath&#26159;&#19968;&#20010;&#29992;&#20110;&#22686;&#24378;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#24212;&#29992;&#30417;&#30563;&#24494;&#35843;&#21644;&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#65292;&#22312;&#33521;&#35821;&#21644;&#20013;&#25991;&#25968;&#23398;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#33021;&#22815;&#27491;&#30830;&#35299;&#20915;&#29983;&#25104;&#30340;&#38382;&#39064;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22788;&#29702;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#19979;&#28216;&#20219;&#21153;&#26041;&#38754;&#23637;&#31034;&#20986;&#20102;&#26174;&#33879;&#30340;&#33021;&#21147;&#65292;&#29978;&#33267;&#21487;&#20197;&#22788;&#29702;&#38656;&#35201;&#22810;&#27493;&#25512;&#29702;&#30340;&#25968;&#23398;&#20219;&#21153;&#12290;&#22312;&#26412;&#25253;&#21578;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;KwaiYiiMath&#65292;&#36890;&#36807;&#24212;&#29992;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#21644;&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#65292;&#22686;&#24378;&#20102;KwaiYiiBase1&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#21253;&#25324;&#33521;&#35821;&#21644;&#20013;&#25991;&#30340;&#25968;&#23398;&#20219;&#21153;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#26500;&#24314;&#20102;&#19968;&#20010;&#23567;&#35268;&#27169;&#30340;&#20013;&#23567;&#23398;&#25968;&#23398;&#27979;&#35797;&#38598;&#65288;&#21629;&#21517;&#20026;KMath&#65289;&#65292;&#21253;&#21547;188&#20010;&#20363;&#23376;&#65292;&#29992;&#26469;&#35780;&#20272;&#27169;&#22411;&#29983;&#25104;&#30340;&#38382;&#39064;&#35299;&#20915;&#36807;&#31243;&#30340;&#27491;&#30830;&#24615;&#12290;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#19982;&#31867;&#20284;&#35268;&#27169;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;KwaiYiiMath&#22312;GSM8k&#12289;CMath&#21644;KMath&#19978;&#22343;&#33021;&#21462;&#24471;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65288;SOTA&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in large language models (LLMs) have demonstrated remarkable abilities in handling a variety of natural language processing (NLP) downstream tasks, even on mathematical tasks requiring multi-step reasoning. In this report, we introduce the KwaiYiiMath which enhances the mathematical reasoning abilities of KwaiYiiBase1, by applying Supervised Fine-Tuning (SFT) and Reinforced Learning from Human Feedback (RLHF), including on both English and Chinese mathematical tasks. Meanwhile, we also constructed a small-scale Chinese primary school mathematics test set (named KMath), consisting of 188 examples to evaluate the correctness of the problem-solving process generated by the models. Empirical studies demonstrate that KwaiYiiMath can achieve state-of-the-art (SOTA) performance on GSM8k, CMath, and KMath compared with the similar size models, respectively.
&lt;/p&gt;</description></item><item><title>&#22810;&#27169;&#24577;&#22270;&#23398;&#20064;&#26159;&#19968;&#31181;&#36890;&#29992;&#19988;&#31995;&#32479;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#25429;&#25417;&#22810;&#27169;&#24577;&#25968;&#25454;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#65292;&#24182;&#22312;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.07478</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#22270;&#23398;&#20064;&#29992;&#20110;&#29983;&#25104;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Multimodal Graph Learning for Generative Tasks. (arXiv:2310.07478v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07478
&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22270;&#23398;&#20064;&#26159;&#19968;&#31181;&#36890;&#29992;&#19988;&#31995;&#32479;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#25429;&#25417;&#22810;&#27169;&#24577;&#25968;&#25454;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#65292;&#24182;&#22312;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#23398;&#20064;&#32467;&#21512;&#22810;&#31181;&#25968;&#25454;&#27169;&#24577;&#65292;&#25193;&#22823;&#20102;&#27169;&#22411;&#21487;&#20197;&#21033;&#29992;&#30340;&#25968;&#25454;&#31867;&#22411;&#21644;&#22797;&#26434;&#24230;&#65292;&#20363;&#22914;&#20174;&#32431;&#25991;&#26412;&#21040;&#22270;&#20687;-&#23383;&#24149;&#23545;&#12290;&#22823;&#22810;&#25968;&#22810;&#27169;&#24577;&#23398;&#20064;&#31639;&#27861;&#19987;&#27880;&#20110;&#23545;&#20004;&#31181;&#27169;&#24577;&#30340;&#31616;&#21333;&#19968;&#23545;&#19968;&#25968;&#25454;&#36827;&#34892;&#24314;&#27169;&#65292;&#22914;&#22270;&#20687;-&#23383;&#24149;&#23545;&#25110;&#38899;&#39057;-&#25991;&#26412;&#23545;&#12290;&#28982;&#32780;&#65292;&#22312;&#22823;&#22810;&#25968;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#19981;&#21516;&#27169;&#24577;&#30340;&#23454;&#20307;&#20197;&#26356;&#22797;&#26434;&#21644;&#22810;&#26679;&#21270;&#30340;&#26041;&#24335;&#30456;&#20114;&#20316;&#29992;&#65292;&#36229;&#36234;&#20102;&#19968;&#23545;&#19968;&#26144;&#23556;&#12290;&#25105;&#20204;&#25552;&#20986;&#23558;&#36825;&#20123;&#22797;&#26434;&#20851;&#31995;&#34920;&#31034;&#20026;&#22270;&#24418;&#65292;&#20801;&#35768;&#25105;&#20204;&#25429;&#25417;&#20219;&#24847;&#25968;&#37327;&#30340;&#27169;&#24577;&#25968;&#25454;&#65292;&#24182;&#25429;&#25417;&#27169;&#24577;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#65292;&#36825;&#20123;&#20851;&#31995;&#21487;&#20197;&#20174;&#19968;&#20010;&#26679;&#26412;&#21040;&#21478;&#19968;&#20010;&#26679;&#26412;&#28789;&#27963;&#21464;&#21270;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#27169;&#24577;&#22270;&#23398;&#20064;&#65288;MMGL&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#29992;&#19988;&#31995;&#32479;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#25429;&#33719;&#20855;&#26377;&#20851;&#31995;&#32467;&#26500;&#30340;&#22810;&#20010;&#22810;&#27169;&#24577;&#37051;&#23621;&#30340;&#20449;&#24687;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#20851;&#27880;&#22522;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;MMGL&#29992;&#20110;&#29983;&#25104;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal learning combines multiple data modalities, broadening the types and complexity of data our models can utilize: for example, from plain text to image-caption pairs. Most multimodal learning algorithms focus on modeling simple one-to-one pairs of data from two modalities, such as image-caption pairs, or audio-text pairs. However, in most real-world settings, entities of different modalities interact with each other in more complex and multifaceted ways, going beyond one-to-one mappings. We propose to represent these complex relationships as graphs, allowing us to capture data with any number of modalities, and with complex relationships between modalities that can flexibly vary from one sample to another. Toward this goal, we propose Multimodal Graph Learning (MMGL), a general and systematic framework for capturing information from multiple multimodal neighbors with relational structures among them. In particular, we focus on MMGL for generative tasks, building upon pretraine
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20849;&#21019;AI&#31995;&#32479;&#30340;&#26412;&#20307;&#35770;&#65292;&#36890;&#36807;&#22686;&#21152;&#35745;&#31639;&#26426;&#20316;&#20026;&#20998;&#21253;&#21830;&#12289;&#35745;&#31639;&#26426;&#20316;&#20026;&#25209;&#35780;&#32773;&#21644;&#35745;&#31639;&#26426;&#20316;&#20026;&#38431;&#21451;&#31561;&#26032;&#31867;&#21035;&#65292;&#26469;&#25193;&#23637;&#23545;&#21019;&#36896;&#21147;&#25903;&#25345;&#24037;&#20855;&#30340;&#26412;&#20307;&#35770;&#12290;</title><link>http://arxiv.org/abs/2310.07472</link><description>&lt;p&gt;
&#19968;&#20010;&#20849;&#21019;AI&#31995;&#32479;&#30340;&#26412;&#20307;&#35770;
&lt;/p&gt;
&lt;p&gt;
An Ontology of Co-Creative AI Systems. (arXiv:2310.07472v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07472
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20849;&#21019;AI&#31995;&#32479;&#30340;&#26412;&#20307;&#35770;&#65292;&#36890;&#36807;&#22686;&#21152;&#35745;&#31639;&#26426;&#20316;&#20026;&#20998;&#21253;&#21830;&#12289;&#35745;&#31639;&#26426;&#20316;&#20026;&#25209;&#35780;&#32773;&#21644;&#35745;&#31639;&#26426;&#20316;&#20026;&#38431;&#21451;&#31561;&#26032;&#31867;&#21035;&#65292;&#26469;&#25193;&#23637;&#23545;&#21019;&#36896;&#21147;&#25903;&#25345;&#24037;&#20855;&#30340;&#26412;&#20307;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#8220;&#20849;&#21019;&#24615;&#8221;&#19968;&#35789;&#34987;&#29992;&#26469;&#25551;&#36848;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20849;&#21516;&#21442;&#19982;&#21019;&#20316;&#30340;&#21508;&#31181;&#32452;&#21512;&#12290;&#20026;&#20102;&#24110;&#21161;&#28040;&#38500;&#30740;&#31350;&#21162;&#21147;&#30340;&#28151;&#28102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20849;&#21019;&#31995;&#32479;&#30340;&#26412;&#20307;&#35770;&#65292;&#20851;&#27880;&#20154;&#31867;&#19982;AI&#31995;&#32479;&#20043;&#38388;&#30340;&#36131;&#20219;&#21010;&#20998;&#21644;&#20449;&#24687;&#20132;&#27969;&#12290;&#25105;&#20204;&#22312;Lubart&#23545;&#21019;&#36896;&#21147;&#25903;&#25345;&#24037;&#20855;&#30340;&#26412;&#20307;&#35770;&#19978;&#36827;&#34892;&#20102;&#25193;&#23637;&#65292;&#22686;&#21152;&#20102;&#19977;&#20010;&#24378;&#35843;&#20154;&#24037;&#26234;&#33021;&#30340;&#26032;&#31867;&#21035;&#65306;&#35745;&#31639;&#26426;&#20316;&#20026;&#20998;&#21253;&#21830;&#65292;&#35745;&#31639;&#26426;&#20316;&#20026;&#25209;&#35780;&#32773;&#21644;&#35745;&#31639;&#26426;&#20316;&#20026;&#38431;&#21451;&#65292;&#20854;&#20013;&#19968;&#20123;&#36824;&#26377;&#32454;&#20998;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
The term co-creativity has been used to describe a wide variety of human-AI assemblages in which human and AI are both involved in a creative endeavor. In order to assist with disambiguating research efforts, we present an ontology of co-creative systems, focusing on how responsibilities are divided between human and AI system and the information exchanged between them. We extend Lubart's original ontology of creativity support tools with three new categories emphasizing artificial intelligence: computer-as-subcontractor, computer-as-critic, and computer-as-teammate, some of which have sub-categorizations.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#23558;&#32852;&#37030;&#23398;&#20064;&#30340;&#21327;&#35843;&#22806;&#21253;&#32473;&#21306;&#22359;&#38142;&#31561;&#20998;&#25955;&#32593;&#32476;&#30340;&#23454;&#38469;&#24433;&#21709;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#30001;&#21306;&#22359;&#38142;&#30340;&#36816;&#20316;&#26041;&#24335;&#25903;&#25345;&#30340;&#27169;&#22411;&#38472;&#26087;&#21644;&#19981;&#19968;&#33268;&#23545;&#24322;&#27493;FL&#35757;&#32451;&#36807;&#31243;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.07471</link><description>&lt;p&gt;
&#21306;&#22359;&#38142;&#32852;&#37030;&#23398;&#20064;&#20013;&#20998;&#25955;&#21270;&#30340;&#24433;&#21709;&#65306;&#35780;&#20272;&#27169;&#22411;&#38472;&#26087;&#21644;&#19981;&#19968;&#33268;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The Implications of Decentralization in Blockchained Federated Learning: Evaluating the Impact of Model Staleness and Inconsistencies. (arXiv:2310.07471v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07471
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#23558;&#32852;&#37030;&#23398;&#20064;&#30340;&#21327;&#35843;&#22806;&#21253;&#32473;&#21306;&#22359;&#38142;&#31561;&#20998;&#25955;&#32593;&#32476;&#30340;&#23454;&#38469;&#24433;&#21709;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#30001;&#21306;&#22359;&#38142;&#30340;&#36816;&#20316;&#26041;&#24335;&#25903;&#25345;&#30340;&#27169;&#22411;&#38472;&#26087;&#21644;&#19981;&#19968;&#33268;&#23545;&#24322;&#27493;FL&#35757;&#32451;&#36807;&#31243;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21306;&#22359;&#38142;&#25215;&#35834;&#36890;&#36807;&#25552;&#20379;&#36827;&#19968;&#27493;&#30340;&#20998;&#25955;&#21270;&#12289;&#23433;&#20840;&#24615;&#12289;&#19981;&#21487;&#21464;&#24615;&#21644;&#20449;&#20219;&#26469;&#22686;&#24378;&#32852;&#37030;&#23398;&#20064;&#31561;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36825;&#20123;&#29305;&#24615;&#23545;&#20110;&#23454;&#29616;&#19979;&#19968;&#20195;&#24212;&#29992;&#20013;&#30340;&#21327;&#20316;&#26234;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#28857;&#23545;&#28857;&#65288;P2P&#65289;&#21306;&#22359;&#38142;&#33410;&#28857;&#30340;&#20869;&#22312;&#20998;&#25955;&#21270;&#25805;&#20316;&#20351;&#24471;&#32852;&#37030;&#23398;&#20064;&#22788;&#20110;&#26410;&#30693;&#30340;&#29366;&#24577;&#65292;FL&#36718;&#27425;&#21644;&#20840;&#23616;&#27169;&#22411;&#30340;&#27010;&#24565;&#21464;&#24471;&#26080;&#24847;&#20041;&#65292;&#22240;&#20026;&#35774;&#22791;&#30340;&#21516;&#27493;&#20002;&#22833;&#20102;&#20013;&#24515;&#21327;&#35843;&#26381;&#21153;&#22120;&#30340;&#24418;&#35937;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#23558;FL&#30340;&#21327;&#35843;&#22806;&#21253;&#32473;&#21306;&#22359;&#38142;&#31561;&#27665;&#20027;&#32593;&#32476;&#30340;&#23454;&#38469;&#24433;&#21709;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20851;&#27880;&#30001;&#21306;&#22359;&#38142;&#30340;&#36816;&#20316;&#26041;&#24335;&#25903;&#25345;&#30340;&#27169;&#22411;&#38472;&#26087;&#21644;&#19981;&#19968;&#33268;&#23545;&#24322;&#27493;&#36827;&#34892;&#30340;FL&#35757;&#32451;&#36807;&#31243;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#27169;&#25311;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;FL&#22312;&#33879;&#21517;&#30340;CIFAR-10&#25968;&#25454;&#38598;&#19978;&#30340;&#36816;&#20316;&#24773;&#20917;&#65292;&#24182;&#30528;&#37325;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Blockchain promises to enhance distributed machine learning (ML) approaches such as federated learning (FL) by providing further decentralization, security, immutability, and trust, which are key properties for enabling collaborative intelligence in next-generation applications. Nonetheless, the intrinsic decentralized operation of peer-to-peer (P2P) blockchain nodes leads to an uncharted setting for FL, whereby the concepts of FL round and global model become meaningless, as devices' synchronization is lost without the figure of a central orchestrating server. In this paper, we study the practical implications of outsourcing the orchestration of FL to a democratic network such as in a blockchain. In particular, we focus on the effects that model staleness and inconsistencies, endorsed by blockchains' modus operandi, have on the training procedure held by FL devices asynchronously. Using simulation, we evaluate the blockchained FL operation on the well-known CIFAR-10 dataset and focus 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#23454;&#38469;&#20225;&#19994;Wi-Fi&#32593;&#32476;&#20013;&#37319;&#29992;&#22522;&#20110;AI/ML&#30340;&#36127;&#36733;&#39044;&#27979;&#30340;&#36866;&#29992;&#24615;&#21644;&#21487;&#34892;&#24615;&#65292;&#24182;&#21457;&#29616;&#21463;&#30828;&#20214;&#38480;&#21046;&#30340;AI/ML&#27169;&#22411;&#21487;&#20197;&#22312;&#24179;&#22343;&#35823;&#24046;&#23567;&#20110;20%&#21644;85th&#30334;&#20998;&#20301;&#35823;&#24046;&#23567;&#20110;3%&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#32593;&#32476;&#36127;&#36733;&#12290;</title><link>http://arxiv.org/abs/2310.07467</link><description>&lt;p&gt;
AI/ML&#22312;IEEE 802.11&#20225;&#19994;&#32593;&#32476;&#20013;&#30340;&#36127;&#36733;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
AI/ML-based Load Prediction in IEEE 802.11 Enterprise Networks. (arXiv:2310.07467v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07467
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#23454;&#38469;&#20225;&#19994;Wi-Fi&#32593;&#32476;&#20013;&#37319;&#29992;&#22522;&#20110;AI/ML&#30340;&#36127;&#36733;&#39044;&#27979;&#30340;&#36866;&#29992;&#24615;&#21644;&#21487;&#34892;&#24615;&#65292;&#24182;&#21457;&#29616;&#21463;&#30828;&#20214;&#38480;&#21046;&#30340;AI/ML&#27169;&#22411;&#21487;&#20197;&#22312;&#24179;&#22343;&#35823;&#24046;&#23567;&#20110;20%&#21644;85th&#30334;&#20998;&#20301;&#35823;&#24046;&#23567;&#20110;3%&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#32593;&#32476;&#36127;&#36733;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20225;&#19994;Wi-Fi&#32593;&#32476;&#21487;&#20197;&#36890;&#36807;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;AI/ML&#65289;&#33719;&#24471;&#24040;&#22823;&#30340;&#22909;&#22788;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#23436;&#21892;&#30340;&#31649;&#29702;&#21644;&#36816;&#33829;&#33021;&#21147;&#12290;&#21516;&#26102;&#65292;&#22522;&#20110;AI/ML&#30340;&#27969;&#37327;/&#36127;&#36733;&#39044;&#27979;&#26159;&#25913;&#21892;Wi-Fi&#20307;&#39564;&#26368;&#26377;&#21560;&#24341;&#21147;&#30340;&#25968;&#25454;&#39537;&#21160;&#35299;&#20915;&#26041;&#26696;&#20043;&#19968;&#65292;&#26080;&#35770;&#26159;&#36890;&#36807;&#23454;&#29616;&#33258;&#20027;&#36816;&#34892;&#36824;&#26159;&#36890;&#36807;&#25552;&#20379;&#39044;&#27979;&#30340;&#32593;&#32476;&#21033;&#29992;&#29575;&#26469;&#25552;&#21319;&#25925;&#38556;&#25490;&#38500;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#23454;&#38469;&#20225;&#19994;Wi-Fi&#32593;&#32476;&#20013;&#37319;&#29992;&#22522;&#20110;AI/ML&#30340;&#36127;&#36733;&#39044;&#27979;&#30340;&#36866;&#29992;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;&#34429;&#28982;&#21033;&#29992;AI/ML&#35299;&#20915;&#26041;&#26696;&#26377;&#21487;&#33021;&#22312;&#33021;&#28304;&#25928;&#29575;&#12289;&#24615;&#33021;&#21644;&#21487;&#38752;&#24615;&#26041;&#38754;&#20248;&#21270;Wi-Fi&#32593;&#32476;&#65292;&#20294;&#20854;&#26377;&#25928;&#37319;&#29992;&#21463;&#21040;&#25968;&#25454;&#21487;&#29992;&#24615;&#21644;&#36136;&#37327;&#12289;&#35745;&#31639;&#33021;&#21147;&#21644;&#33021;&#37327;&#28040;&#32791;&#31561;&#22240;&#32032;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#21463;&#30828;&#20214;&#38480;&#21046;&#30340;AI/ML&#27169;&#22411;&#21487;&#20197;&#22312;&#24179;&#22343;&#35823;&#24046;&#23567;&#20110;20%&#21644;85th&#30334;&#20998;&#20301;&#35823;&#24046;&#23567;&#20110;3%&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#32593;&#32476;&#36127;&#36733;&#12290;
&lt;/p&gt;
&lt;p&gt;
Enterprise Wi-Fi networks can greatly benefit from Artificial Intelligence and Machine Learning (AI/ML) thanks to their well-developed management and operation capabilities. At the same time, AI/ML-based traffic/load prediction is one of the most appealing data-driven solutions to improve the Wi-Fi experience, either through the enablement of autonomous operation or by boosting troubleshooting with forecasted network utilization. In this paper, we study the suitability and feasibility of adopting AI/ML-based load prediction in practical enterprise Wi-Fi networks. While leveraging AI/ML solutions can potentially contribute to optimizing Wi-Fi networks in terms of energy efficiency, performance, and reliability, their effective adoption is constrained to aspects like data availability and quality, computational capabilities, and energy consumption. Our results show that hardware-constrained AI/ML models can potentially predict network load with less than 20% average error and 3% 85th-per
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#38376;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#22495;&#20998;&#35299;&#21644;&#25299;&#25169;&#23884;&#20837;&#22120;&#38477;&#20302;&#35757;&#32451;&#25104;&#26412;&#65292;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#22312;&#22823;&#35268;&#27169;&#22320;&#36136;&#20648;&#23384;&#24212;&#29992;&#20013;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.07461</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#22320;&#36136;&#30899;&#21644;&#33021;&#28304;&#20648;&#23384;&#30340;&#39640;&#25928;&#26426;&#22120;&#23398;&#20064;&#26367;&#20195;&#21697;
&lt;/p&gt;
&lt;p&gt;
Efficient machine-learning surrogates for large-scale geological carbon and energy storage. (arXiv:2310.07461v1 [cs.CE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07461
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#38376;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#22495;&#20998;&#35299;&#21644;&#25299;&#25169;&#23884;&#20837;&#22120;&#38477;&#20302;&#35757;&#32451;&#25104;&#26412;&#65292;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#22312;&#22823;&#35268;&#27169;&#22320;&#36136;&#20648;&#23384;&#24212;&#29992;&#20013;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22320;&#36136;&#30899;&#21644;&#33021;&#28304;&#20648;&#23384;&#23545;&#20110;&#23454;&#29616;&#20928;&#38646;&#30899;&#25490;&#25918;&#21644;&#24212;&#23545;&#27668;&#20505;&#21464;&#21270;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22320;&#36136;&#22240;&#32032;&#21644;&#25805;&#20316;&#38480;&#21046;&#65292;&#23427;&#20204;&#38754;&#20020;&#19981;&#30830;&#23450;&#24615;&#65292;&#21487;&#33021;&#24341;&#21457;&#22320;&#38663;&#20107;&#20214;&#25110;&#22320;&#19979;&#27700;&#27745;&#26579;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#38376;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#65292;&#20197;&#39640;&#25928;&#22320;&#31649;&#29702;&#24191;&#27867;&#30340;&#20648;&#23618;&#27169;&#22411;&#12290;&#34429;&#28982;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#22320;&#36136;&#30899;&#20648;&#23384;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#22823;&#35268;&#27169;&#20998;&#26512;&#25152;&#38656;&#30340;&#35745;&#31639;&#36164;&#28304;&#26159;&#19968;&#20010;&#38556;&#30861;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#22495;&#20998;&#35299;&#21644;&#25299;&#25169;&#23884;&#20837;&#22120;&#26469;&#38477;&#20302;&#28145;&#24230;&#31070;&#32463;&#31639;&#23376;&#27169;&#22411;&#30340;&#35757;&#32451;&#25104;&#26412;&#65292;&#20197;&#38142;&#25509;&#26102;&#31354;&#28857;&#65292;&#20174;&#32780;&#23454;&#29616;&#22312;&#27169;&#22411;&#39046;&#22495;&#20869;&#23545;&#26410;&#32463;&#35757;&#32451;&#30340;&#25968;&#25454;&#36827;&#34892;&#31934;&#30830;&#39044;&#27979;&#65292;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#22312;&#22823;&#35268;&#27169;&#22320;&#36136;&#20648;&#23384;&#24212;&#29992;&#20013;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Geological carbon and energy storage are pivotal for achieving net-zero carbon emissions and addressing climate change. However, they face uncertainties due to geological factors and operational limitations, resulting in possibilities of induced seismic events or groundwater contamination. To overcome these challenges, we propose a specialized machine-learning (ML) model to manage extensive reservoir models efficiently.  While ML approaches hold promise for geological carbon storage, the substantial computational resources required for large-scale analysis are the obstacle. We've developed a method to reduce the training cost for deep neural operator models, using domain decomposition and a topology embedder to link spatio-temporal points. This approach allows accurate predictions within the model's domain, even for untrained data, enhancing ML efficiency for large-scale geological storage applications.
&lt;/p&gt;</description></item><item><title>HealthWalk&#36890;&#36807;&#38598;&#25104;&#20256;&#24863;&#22120;&#21040;&#28378;&#36718;&#34892;&#36208;&#32773;&#35774;&#35745;&#20013;&#65292;&#21487;&#20197;&#35299;&#20915;&#28378;&#36718;&#34892;&#36208;&#32773;&#29992;&#25143;&#23039;&#21183;&#19981;&#22909;&#23548;&#33268;&#30340;&#20581;&#24247;&#38382;&#39064;&#65292;&#24182;&#23454;&#29616;&#20854;&#20182;&#26377;&#36259;&#30340;&#29992;&#20363;&#12290;</title><link>http://arxiv.org/abs/2310.07434</link><description>&lt;p&gt;
HealthWalk&#65306;&#36890;&#36807;&#22522;&#20110;&#20256;&#24863;&#22120;&#30340;&#28378;&#36718;&#34892;&#36208;&#32773;&#36741;&#21161;&#20419;&#36827;&#20581;&#24247;&#19982;&#31227;&#21160;&#24615;
&lt;/p&gt;
&lt;p&gt;
HealthWalk: Promoting Health and Mobility through Sensor-Based Rollator Walker Assistance. (arXiv:2310.07434v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07434
&lt;/p&gt;
&lt;p&gt;
HealthWalk&#36890;&#36807;&#38598;&#25104;&#20256;&#24863;&#22120;&#21040;&#28378;&#36718;&#34892;&#36208;&#32773;&#35774;&#35745;&#20013;&#65292;&#21487;&#20197;&#35299;&#20915;&#28378;&#36718;&#34892;&#36208;&#32773;&#29992;&#25143;&#23039;&#21183;&#19981;&#22909;&#23548;&#33268;&#30340;&#20581;&#24247;&#38382;&#39064;&#65292;&#24182;&#23454;&#29616;&#20854;&#20182;&#26377;&#36259;&#30340;&#29992;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28378;&#36718;&#34892;&#36208;&#32773;&#33021;&#22815;&#24110;&#21161;&#36523;&#20307;&#21463;&#38480;&#21046;&#30340;&#20154;&#25552;&#39640;&#31227;&#21160;&#33021;&#21147;&#65292;&#24182;&#32473;&#20182;&#20204;&#20449;&#24515;&#21644;&#29420;&#31435;&#24615;&#65292;&#20351;&#20182;&#20204;&#33021;&#26356;&#38271;&#26102;&#38388;&#22320;&#21442;&#19982;&#31038;&#20250;&#12290;&#28982;&#32780;&#65292;&#28378;&#36718;&#34892;&#36208;&#32773;&#29992;&#25143;&#24448;&#24448;&#23039;&#21183;&#19981;&#22909;&#65292;&#23548;&#33268;&#36827;&#19968;&#27493;&#30340;&#20581;&#24247;&#38382;&#39064;&#65292;&#26368;&#22351;&#30340;&#24773;&#20917;&#26159;&#25684;&#20498;&#12290;&#23558;&#20256;&#24863;&#22120;&#38598;&#25104;&#21040;&#28378;&#36718;&#34892;&#36208;&#32773;&#35774;&#35745;&#20013;&#21487;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#19988;&#21487;&#20197;&#23454;&#29616;&#20854;&#20182;&#19968;&#20123;&#26377;&#36259;&#30340;&#29992;&#20363;&#12290;&#26412;&#25991;&#31616;&#35201;&#20171;&#32461;&#20102;&#29616;&#26377;&#31995;&#32479;&#20197;&#21450;&#35813;&#39046;&#22495;&#30340;&#24403;&#21069;&#30740;&#31350;&#26041;&#21521;&#21644;&#25361;&#25112;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#25105;&#20204;&#26089;&#26399;&#30340;HealthWalk&#28378;&#36718;&#34892;&#36208;&#32773;&#21407;&#22411;&#65292;&#29992;&#20110;&#19982;&#32769;&#24180;&#20154;&#12289;&#39118;&#28287;&#30149;&#12289;&#22810;&#21457;&#24615;&#30828;&#21270;&#30151;&#21644;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#20197;&#21450;&#35270;&#21147;&#38556;&#30861;&#32773;&#36827;&#34892;&#25968;&#25454;&#25910;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rollator walkers allow people with physical limitations to increase their mobility and give them the confidence and independence to participate in society for longer. However, rollator walker users often have poor posture, leading to further health problems and, in the worst case, falls. Integrating sensors into rollator walker designs can help to address this problem and results in a platform that allows several other interesting use cases. This paper briefly overviews existing systems and the current research directions and challenges in this field. We also present our early HealthWalk rollator walker prototype for data collection with older people, rheumatism, multiple sclerosis and Parkinson patients, and individuals with visual impairments.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35266;&#23519;&#23398;&#20064;&#27169;&#20223;(ILfO)&#26694;&#26550;&#65292;&#33021;&#22815;&#35299;&#20915;&#30001;&#20110;&#22870;&#21169;&#20449;&#21495;&#20998;&#37197;&#38169;&#35823;&#23548;&#33268;&#20195;&#29702;&#26080;&#27861;&#23398;&#20064;&#21021;&#22987;&#34892;&#20026;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.07433</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#21160;&#25240;&#25187;&#35843;&#24230;&#20174;&#35266;&#23519;&#20013;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Imitation Learning from Observation with Automatic Discount Scheduling. (arXiv:2310.07433v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07433
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35266;&#23519;&#23398;&#20064;&#27169;&#20223;(ILfO)&#26694;&#26550;&#65292;&#33021;&#22815;&#35299;&#20915;&#30001;&#20110;&#22870;&#21169;&#20449;&#21495;&#20998;&#37197;&#38169;&#35823;&#23548;&#33268;&#20195;&#29702;&#26080;&#27861;&#23398;&#20064;&#21021;&#22987;&#34892;&#20026;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#36890;&#24120;&#36890;&#36807;&#35266;&#23519;&#21644;&#27169;&#20223;&#26469;&#33719;&#24471;&#26032;&#30340;&#25216;&#33021;&#12290;&#23545;&#20110;&#26426;&#22120;&#20154;&#20195;&#29702;&#65292;&#20174;&#20114;&#32852;&#32593;&#19978;&#21487;&#29992;&#30340;&#22823;&#37327;&#26080;&#26631;&#31614;&#35270;&#39057;&#28436;&#31034;&#25968;&#25454;&#20013;&#36827;&#34892;&#23398;&#20064;&#65292;&#38656;&#35201;&#22312;&#27809;&#26377;&#35775;&#38382;&#20854;&#21160;&#20316;&#30340;&#24773;&#20917;&#19979;&#27169;&#20223;&#19987;&#23478;&#65292;&#36825;&#26159;&#19968;&#31181;&#31216;&#20026;&#35266;&#23519;&#23398;&#20064;&#27169;&#20223;&#65288;ILfO&#65289;&#30340;&#25361;&#25112;&#12290;&#35299;&#20915;ILfO&#38382;&#39064;&#30340;&#24120;&#35265;&#26041;&#27861;&#26159;&#23558;&#20854;&#36716;&#21270;&#20026;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#21033;&#29992;&#20174;&#20195;&#29702;&#21644;&#19987;&#23478;&#35266;&#23519;&#20013;&#35745;&#31639;&#20986;&#30340;&#20195;&#29702;&#22870;&#21169;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#20855;&#26377;&#36827;&#23637;&#20381;&#36182;&#24615;&#23646;&#24615;&#30340;&#20219;&#21153;&#20013;&#65292;&#36825;&#26679;&#30340;&#26041;&#27861;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#65307;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#65292;&#20195;&#29702;&#38656;&#35201;&#22312;&#25484;&#25569;&#21518;&#32493;&#34892;&#20026;&#20043;&#21069;&#20808;&#23398;&#20064;&#19987;&#23478;&#30340;&#21069;&#24207;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20027;&#35201;&#21407;&#22240;&#26159;&#20998;&#37197;&#32473;&#21518;&#32493;&#27493;&#39588;&#30340;&#22870;&#21169;&#20449;&#21495;&#22952;&#30861;&#20102;&#23545;&#21021;&#22987;&#34892;&#20026;&#30340;&#23398;&#20064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;ILfO&#26694;&#26550;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#25484;&#25569;&#26089;&#26399;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans often acquire new skills through observation and imitation. For robotic agents, learning from the plethora of unlabeled video demonstration data available on the Internet necessitates imitating the expert without access to its action, presenting a challenge known as Imitation Learning from Observations (ILfO). A common approach to tackle ILfO problems is to convert them into inverse reinforcement learning problems, utilizing a proxy reward computed from the agent's and the expert's observations. Nonetheless, we identify that tasks characterized by a progress dependency property pose significant challenges for such approaches; in these tasks, the agent needs to initially learn the expert's preceding behaviors before mastering the subsequent ones. Our investigation reveals that the main cause is that the reward signals assigned to later steps hinder the learning of initial behaviors. To address this challenge, we present a novel ILfO framework that enables the agent to master earl
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#20165;&#35843;&#25972;&#25991;&#26412;&#23884;&#20837;&#65292;&#32780;&#19981;&#38656;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#33258;&#28982;&#30340;&#22810;&#27010;&#24565;&#29983;&#25104;&#12290;&#36890;&#36807;&#35299;&#20915;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#25991;&#26412;&#23884;&#20837;&#30340;&#23616;&#38480;&#24615;&#65292;&#22914;&#27010;&#24565;&#25903;&#37197;&#21644;&#38750;&#23450;&#20301;&#36129;&#29486;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#24265;&#20215;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#25552;&#39640;&#20102;&#22810;&#27010;&#24565;&#29983;&#25104;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.07419</link><description>&lt;p&gt;
&#22810;&#27010;&#24565; T2I-Zero: &#20165;&#35843;&#25972;&#25991;&#26412;&#23884;&#20837;&#65292;&#19981;&#20570;&#20854;&#20182;&#25913;&#21464;
&lt;/p&gt;
&lt;p&gt;
Multi-Concept T2I-Zero: Tweaking Only The Text Embeddings and Nothing Else. (arXiv:2310.07419v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07419
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#20165;&#35843;&#25972;&#25991;&#26412;&#23884;&#20837;&#65292;&#32780;&#19981;&#38656;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#33258;&#28982;&#30340;&#22810;&#27010;&#24565;&#29983;&#25104;&#12290;&#36890;&#36807;&#35299;&#20915;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#25991;&#26412;&#23884;&#20837;&#30340;&#23616;&#38480;&#24615;&#65292;&#22914;&#27010;&#24565;&#25903;&#37197;&#21644;&#38750;&#23450;&#20301;&#36129;&#29486;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#24265;&#20215;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#25552;&#39640;&#20102;&#22810;&#27010;&#24565;&#29983;&#25104;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#36827;&#23637;&#20351;&#24471;&#20174;&#25991;&#26412;&#25552;&#31034;&#29983;&#25104;&#36924;&#30495;&#30340;&#22270;&#20687;&#25104;&#20026;&#21487;&#33021;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#24456;&#22823;&#36827;&#27493;&#65292;&#29616;&#26377;&#27169;&#22411;&#20173;&#28982;&#38590;&#20197;&#33258;&#28982;&#22320;&#29983;&#25104;&#32452;&#21512;&#30340;&#22810;&#27010;&#24565;&#22270;&#20687;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#21487;&#35270;&#21270;&#20154;&#31867;&#24819;&#35937;&#21147;&#30340;&#33021;&#21147;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#19968;&#20123;&#30740;&#31350;&#23581;&#35797;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#35201;&#20040;&#24341;&#20837;&#39069;&#22806;&#30340;&#35757;&#32451;&#65292;&#35201;&#20040;&#22312;&#25512;&#29702;&#26102;&#37319;&#29992;&#25351;&#23548;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#26356;&#20855;&#38596;&#24515;&#30340;&#30446;&#26631;&#65306;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#33258;&#28982;&#22810;&#27010;&#24565;&#29983;&#25104;&#65292;&#24182;&#19988;&#20960;&#20046;&#27809;&#26377;&#39069;&#22806;&#30340;&#25104;&#26412;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#20351;&#29992;&#30340;&#25991;&#26412;&#23884;&#20837;&#30340;&#23616;&#38480;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#27010;&#24565;&#25903;&#37197;&#21644;&#38750;&#23450;&#20301;&#36129;&#29486;&#20005;&#37325;&#38477;&#20302;&#20102;&#22810;&#27010;&#24565;&#29983;&#25104;&#24615;&#33021;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35774;&#35745;&#20102;&#19968;&#20010;&#26368;&#23567;&#24265;&#20215;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#35843;&#25972;&#65288;&#32780;&#19981;&#26159;&#37325;&#26032;&#35757;&#32451;&#65289;&#25991;&#26412;&#23884;&#20837;&#26469;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#20197;&#33719;&#24471;&#26356;&#30495;&#23454;&#30340;&#29983;&#25104;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in text-to-image diffusion models have enabled the photorealistic generation of images from text prompts. Despite the great progress, existing models still struggle to generate compositional multi-concept images naturally, limiting their ability to visualize human imagination. While several recent works have attempted to address this issue, they either introduce additional training or adopt guidance at inference time. In this work, we consider a more ambitious goal: natural multi-concept generation using a pre-trained diffusion model, and with almost no extra cost. To achieve this goal, we identify the limitations in the text embeddings used for the pre-trained text-to-image diffusion models. Specifically, we observe concept dominance and non-localized contribution that severely degrade multi-concept generation performance. We further design a minimal low-cost solution that overcomes the above issues by tweaking (not re-training) the text embeddings for more realistic m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#21487;&#22609;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21457;&#29616;&#25968;&#25454;&#22686;&#24378;&#23545;&#20445;&#25345;&#21487;&#22609;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#35780;&#35770;&#32773;&#30340;&#21487;&#22609;&#24615;&#25439;&#22833;&#26159;&#39640;&#25928;&#35757;&#32451;&#30340;&#20027;&#35201;&#38480;&#21046;&#22240;&#32032;&#65292;&#24182;&#19988;&#26410;&#21450;&#26102;&#24674;&#22797;&#35780;&#35770;&#32773;&#30340;&#21487;&#22609;&#24615;&#23558;&#23548;&#33268;&#28798;&#38590;&#24615;&#32467;&#26524;&#12290;&#36825;&#20026;&#35299;&#20915;&#39640;&#37325;&#25918;&#27604;&#22256;&#22659;&#25552;&#20379;&#20102;&#26032;&#30340;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2310.07418</link><description>&lt;p&gt;
&#37325;&#23457;&#35270;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#21487;&#22609;&#24615;&#65306;&#25968;&#25454;&#12289;&#27169;&#22359;&#21644;&#35757;&#32451;&#38454;&#27573;
&lt;/p&gt;
&lt;p&gt;
Revisiting Plasticity in Visual Reinforcement Learning: Data, Modules and Training Stages. (arXiv:2310.07418v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07418
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#21487;&#22609;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21457;&#29616;&#25968;&#25454;&#22686;&#24378;&#23545;&#20445;&#25345;&#21487;&#22609;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#35780;&#35770;&#32773;&#30340;&#21487;&#22609;&#24615;&#25439;&#22833;&#26159;&#39640;&#25928;&#35757;&#32451;&#30340;&#20027;&#35201;&#38480;&#21046;&#22240;&#32032;&#65292;&#24182;&#19988;&#26410;&#21450;&#26102;&#24674;&#22797;&#35780;&#35770;&#32773;&#30340;&#21487;&#22609;&#24615;&#23558;&#23548;&#33268;&#28798;&#38590;&#24615;&#32467;&#26524;&#12290;&#36825;&#20026;&#35299;&#20915;&#39640;&#37325;&#25918;&#27604;&#22256;&#22659;&#25552;&#20379;&#20102;&#26032;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#22609;&#24615;&#65292;&#31070;&#32463;&#32593;&#32476;&#38543;&#26032;&#25968;&#25454;&#28436;&#36827;&#30340;&#33021;&#21147;&#65292;&#23545;&#20110;&#39640;&#24615;&#33021;&#21644;&#26679;&#26412;&#39640;&#25928;&#30340;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;(VRL)&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#37325;&#32622;&#21644;&#27491;&#21017;&#21270;&#31561;&#26041;&#27861;&#21487;&#33021;&#33021;&#22815;&#32531;&#35299;&#21487;&#22609;&#24615;&#25439;&#22833;&#65292;&#20294;VRL&#26694;&#26550;&#20869;&#21508;&#31181;&#32452;&#20214;&#23545;&#20195;&#29702;&#30340;&#21487;&#22609;&#24615;&#30340;&#24433;&#21709;&#20173;&#28982;&#30693;&#20043;&#29978;&#23569;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#32463;&#39564;&#24615;&#25506;&#32034;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#19977;&#20010;&#20027;&#35201;&#23578;&#26410;&#20805;&#20998;&#25506;&#32034;&#30340;&#26041;&#38754;&#65292;&#24182;&#24471;&#20986;&#20197;&#19979;&#26377;&#28145;&#20837;&#35265;&#35299;&#30340;&#32467;&#35770;&#65306;(1)&#25968;&#25454;&#22686;&#24378;&#23545;&#20110;&#20445;&#25345;&#21487;&#22609;&#24615;&#33267;&#20851;&#37325;&#35201;&#65307;(2)&#35780;&#35770;&#32773;&#30340;&#21487;&#22609;&#24615;&#25439;&#22833;&#26159;&#38459;&#30861;&#39640;&#25928;&#35757;&#32451;&#30340;&#20027;&#35201;&#29942;&#39048;&#65307;(3)&#22312;&#26089;&#26399;&#38454;&#27573;&#27809;&#26377;&#21450;&#26102;&#24178;&#39044;&#20197;&#24674;&#22797;&#35780;&#35770;&#32773;&#30340;&#21487;&#22609;&#24615;&#65292;&#20854;&#25439;&#22833;&#23558;&#21464;&#24471;&#28798;&#38590;&#24615;&#12290;&#36825;&#20123;&#35265;&#35299;&#25552;&#20986;&#20102;&#19968;&#31181;&#24212;&#23545;&#39640;&#37325;&#25918;&#27604;&#65288;RR&#65289;&#22256;&#22659;&#30340;&#26032;&#31574;&#30053;&#65292;&#20854;&#20013;&#21152;&#21095;&#30340;&#21487;&#22609;&#24615;&#25439;&#22833;&#22952;&#30861;&#20102;&#36890;&#36807;&#22686;&#21152;&#37325;&#25918;&#25968;&#37327;&#24102;&#26469;&#30340;&#26679;&#26412;&#25928;&#29575;&#30340;&#28508;&#22312;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Plasticity, the ability of a neural network to evolve with new data, is crucial for high-performance and sample-efficient visual reinforcement learning (VRL). Although methods like resetting and regularization can potentially mitigate plasticity loss, the influences of various components within the VRL framework on the agent's plasticity are still poorly understood. In this work, we conduct a systematic empirical exploration focusing on three primary underexplored facets and derive the following insightful conclusions: (1) data augmentation is essential in maintaining plasticity; (2) the critic's plasticity loss serves as the principal bottleneck impeding efficient training; and (3) without timely intervention to recover critic's plasticity in the early stages, its loss becomes catastrophic. These insights suggest a novel strategy to address the high replay ratio (RR) dilemma, where exacerbated plasticity loss hinders the potential improvements of sample efficiency brought by increased
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#30693;&#35782;&#22270;&#35889;&#23545;&#40784;&#30340;&#29616;&#29366;&#19982;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#26041;&#27861;&#30340;&#28508;&#21147;&#65292;&#20197;&#23454;&#29616;&#35299;&#37322;&#24615;&#12289;&#21487;&#39564;&#35777;&#24615;&#21644;&#39640;&#36136;&#37327;&#30340;&#23545;&#40784;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.07417</link><description>&lt;p&gt;
&#20351;&#29992;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#26041;&#27861;&#65292;&#30693;&#35782;&#22270;&#35889;&#23545;&#40784;&#21487;&#20197;&#33719;&#24471;&#20160;&#20040;&#65311;
&lt;/p&gt;
&lt;p&gt;
What can knowledge graph alignment gain with Neuro-Symbolic learning approaches?. (arXiv:2310.07417v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07417
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#30693;&#35782;&#22270;&#35889;&#23545;&#40784;&#30340;&#29616;&#29366;&#19982;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#26041;&#27861;&#30340;&#28508;&#21147;&#65292;&#20197;&#23454;&#29616;&#35299;&#37322;&#24615;&#12289;&#21487;&#39564;&#35777;&#24615;&#21644;&#39640;&#36136;&#37327;&#30340;&#23545;&#40784;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#26159;&#35768;&#22810;&#25968;&#25454;&#23494;&#38598;&#22411;&#24212;&#29992;&#31243;&#24207;&#30340;&#22522;&#30784;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#34920;&#31034;&#25968;&#25454;&#19982;&#20854;&#24847;&#20041;&#21644;&#19978;&#19979;&#25991;&#30340;&#20851;&#32852;&#12290;&#23545;&#40784;&#19981;&#21516;&#39046;&#22495;&#21644;&#25552;&#20379;&#32773;&#30340;&#30693;&#35782;&#22270;&#35889;&#26159;&#20026;&#20102;&#23454;&#29616;&#26356;&#20840;&#38754;&#21644;&#19968;&#20307;&#21270;&#30340;&#34920;&#31034;&#12290;&#30446;&#21069;&#30693;&#35782;&#22270;&#35889;&#23545;&#40784;&#31639;&#27861;&#30340;&#19968;&#20010;&#20005;&#37325;&#23616;&#38480;&#26159;&#23427;&#20204;&#26080;&#27861;&#23558;&#36923;&#36753;&#24605;&#32771;&#21644;&#25512;&#29702;&#19982;&#35789;&#27719;&#12289;&#32467;&#26500;&#21644;&#35821;&#20041;&#25968;&#25454;&#23398;&#20064;&#30456;&#32467;&#21512;&#12290;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#30693;&#35782;&#22270;&#35889;&#23545;&#40784;&#26041;&#38754;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#20511;&#37492;&#20102;&#23427;&#20204;&#22312;&#20854;&#20182;&#20219;&#21153;&#20013;&#30340;&#33391;&#22909;&#34920;&#29616;&#65292;&#20294;&#23427;&#20204;&#22312;&#35299;&#37322;&#24615;&#12289;&#25512;&#29702;&#33021;&#21147;&#21644;&#25968;&#25454;&#25928;&#29575;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#12290;&#28151;&#21512;&#30340;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#27169;&#22411;&#26377;&#26395;&#23558;&#36923;&#36753;&#21644;&#25968;&#25454;&#35270;&#35282;&#25972;&#21512;&#36215;&#26469;&#65292;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#39640;&#36136;&#37327;&#23545;&#40784;&#32467;&#26524;&#65292;&#24182;&#25903;&#25345;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#39564;&#35777;&#26041;&#27861;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#30693;&#35782;&#22270;&#35889;&#23545;&#40784;&#39046;&#22495;&#30340;&#29616;&#29366;&#65292;&#24182;&#25506;&#35752;&#20102;&#31070;&#32463;&#31526;&#21495;&#25972;&#21512;&#30340;&#28508;&#21147;&#65292;&#24378;&#35843;&#20102;&#26377;&#21069;&#26223;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graphs (KG) are the backbone of many data-intensive applications since they can represent data coupled with its meaning and context. Aligning KGs across different domains and providers is necessary to afford a fuller and integrated representation. A severe limitation of current KG alignment (KGA) algorithms is that they fail to articulate logical thinking and reasoning with lexical, structural, and semantic data learning. Deep learning models are increasingly popular for KGA inspired by their good performance in other tasks, but they suffer from limitations in explainability, reasoning, and data efficiency. Hybrid neurosymbolic learning models hold the promise of integrating logical and data perspectives to produce high-quality alignments that are explainable and support validation through human-centric approaches. This paper examines the current state of the art in KGA and explores the potential for neurosymbolic integration, highlighting promising research directions for co
&lt;/p&gt;</description></item><item><title>DASpeech&#26159;&#19968;&#20010;&#38750;&#33258;&#22238;&#24402;&#30340;&#30452;&#25509;&#35821;&#38899;&#32763;&#35793;&#27169;&#22411;&#65292;&#20351;&#29992;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;DAG&#65289;&#26469;&#23454;&#29616;&#24555;&#36895;&#21644;&#39640;&#36136;&#37327;&#30340;&#35821;&#38899;&#32763;&#35793;&#12290;</title><link>http://arxiv.org/abs/2310.07403</link><description>&lt;p&gt;
DASpeech&#65306;&#29992;&#20110;&#24555;&#36895;&#39640;&#36136;&#37327;&#35821;&#38899;&#32763;&#35793;&#30340;&#26377;&#21521;&#26080;&#29615;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
DASpeech: Directed Acyclic Transformer for Fast and High-quality Speech-to-Speech Translation. (arXiv:2310.07403v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07403
&lt;/p&gt;
&lt;p&gt;
DASpeech&#26159;&#19968;&#20010;&#38750;&#33258;&#22238;&#24402;&#30340;&#30452;&#25509;&#35821;&#38899;&#32763;&#35793;&#27169;&#22411;&#65292;&#20351;&#29992;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;DAG&#65289;&#26469;&#23454;&#29616;&#24555;&#36895;&#21644;&#39640;&#36136;&#37327;&#30340;&#35821;&#38899;&#32763;&#35793;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30452;&#25509;&#35821;&#38899;&#32763;&#35793;&#65288;S2ST&#65289;&#20351;&#29992;&#21333;&#20010;&#27169;&#22411;&#23558;&#19968;&#31181;&#35821;&#35328;&#30340;&#35821;&#38899;&#32763;&#35793;&#25104;&#21478;&#19968;&#31181;&#35821;&#35328;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23384;&#22312;&#35821;&#35328;&#21644;&#22768;&#23398;&#22810;&#26679;&#24615;&#65292;&#30446;&#26631;&#35821;&#38899;&#36981;&#24490;&#19968;&#20010;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#20998;&#24067;&#65292;&#32473;S2ST&#27169;&#22411;&#23454;&#29616;&#39640;&#36136;&#37327;&#32763;&#35793;&#21644;&#24555;&#36895;&#35299;&#30721;&#36895;&#24230;&#24102;&#26469;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DASpeech&#65292;&#36825;&#26159;&#19968;&#20010;&#38750;&#33258;&#22238;&#24402;&#30340;&#30452;&#25509;S2ST&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#21644;&#39640;&#36136;&#37327;&#30340;S2ST&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#25429;&#25417;&#30446;&#26631;&#35821;&#38899;&#30340;&#22797;&#26434;&#20998;&#24067;&#65292;DASpeech&#37319;&#29992;&#20102;&#20004;&#27493;&#35299;&#30721;&#30340;&#26550;&#26500;&#65292;&#20808;&#30001;&#35821;&#35328;&#35299;&#30721;&#22120;&#29983;&#25104;&#30446;&#26631;&#25991;&#26412;&#65292;&#28982;&#21518;&#30001;&#22768;&#23398;&#35299;&#30721;&#22120;&#26681;&#25454;&#35821;&#35328;&#35299;&#30721;&#22120;&#30340;&#38544;&#34255;&#29366;&#24577;&#29983;&#25104;&#30446;&#26631;&#35821;&#38899;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;DA-Transformer&#30340;&#35299;&#30721;&#22120;&#20316;&#20026;&#35821;&#35328;&#35299;&#30721;&#22120;&#65292;&#23558;FastSpeech 2&#20316;&#20026;&#22768;&#23398;&#35299;&#30721;&#22120;&#12290;DA-Transformer&#20351;&#29992;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;DAG&#65289;&#27169;&#25311;&#32763;&#35793;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Direct speech-to-speech translation (S2ST) translates speech from one language into another using a single model. However, due to the presence of linguistic and acoustic diversity, the target speech follows a complex multimodal distribution, posing challenges to achieving both high-quality translations and fast decoding speeds for S2ST models. In this paper, we propose DASpeech, a non-autoregressive direct S2ST model which realizes both fast and high-quality S2ST. To better capture the complex distribution of the target speech, DASpeech adopts the two-pass architecture to decompose the generation process into two steps, where a linguistic decoder first generates the target text, and an acoustic decoder then generates the target speech based on the hidden states of the linguistic decoder. Specifically, we use the decoder of DA-Transformer as the linguistic decoder, and use FastSpeech 2 as the acoustic decoder. DA-Transformer models translations with a directed acyclic graph (DAG). To co
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#37319;&#29992;Transformer&#26550;&#26500;&#21644;&#25968;&#20540;&#22810;&#23610;&#24230;&#23884;&#20837;&#27169;&#22359;&#65292;&#20351;&#26102;&#38388;&#24207;&#21015;&#33258;&#30417;&#30563;&#27169;&#22411;&#33021;&#22815;&#25193;&#23637;&#21040;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2310.07402</link><description>&lt;p&gt;
NuTime: &#22823;&#35268;&#27169;&#26102;&#38388;&#24207;&#21015;&#39044;&#35757;&#32451;&#30340;&#25968;&#20540;&#22810;&#23610;&#24230;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
NuTime: Numerically Multi-Scaled Embedding for Large-Scale Time Series Pretraining. (arXiv:2310.07402v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07402
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#37319;&#29992;Transformer&#26550;&#26500;&#21644;&#25968;&#20540;&#22810;&#23610;&#24230;&#23884;&#20837;&#27169;&#22359;&#65292;&#20351;&#26102;&#38388;&#24207;&#21015;&#33258;&#30417;&#30563;&#27169;&#22411;&#33021;&#22815;&#25193;&#23637;&#21040;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20851;&#20110;&#26102;&#38388;&#24207;&#21015;&#33258;&#30417;&#30563;&#27169;&#22411;&#30340;&#30740;&#31350;&#26174;&#31034;&#20986;&#23398;&#20064;&#35821;&#20041;&#34920;&#31034;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#20165;&#38480;&#20110;&#23567;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#20363;&#22914;&#25968;&#21315;&#20010;&#26102;&#38388;&#24207;&#21015;&#12290;&#26412;&#25991;&#30340;&#20851;&#38190;&#25216;&#26415;&#36129;&#29486;&#38024;&#23545;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#25968;&#20540;&#29305;&#24615;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#25193;&#23637;&#21040;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#20363;&#22914;&#30334;&#19975;&#20010;&#26102;&#38388;&#24207;&#21015;&#12290;&#25105;&#20204;&#37319;&#29992;Transformer&#26550;&#26500;&#65292;&#39318;&#20808;&#23558;&#36755;&#20837;&#21010;&#20998;&#20026;&#38750;&#37325;&#21472;&#31383;&#21475;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#31383;&#21475;&#30340;&#26631;&#20934;&#21270;&#24418;&#29366;&#21644;&#20004;&#20010;&#26631;&#37327;&#20540;&#34920;&#31034;&#27599;&#20010;&#31383;&#21475;&#20869;&#30340;&#22343;&#20540;&#21644;&#26631;&#20934;&#24046;&#12290;&#20026;&#20102;&#23558;&#21487;&#33021;&#20855;&#26377;&#20219;&#24847;&#25968;&#20540;&#23610;&#24230;&#30340;&#26631;&#37327;&#20540;&#23884;&#20837;&#21040;&#39640;&#32500;&#21521;&#37327;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#20540;&#22810;&#23610;&#24230;&#23884;&#20837;&#27169;&#22359;&#65292;&#26522;&#20030;&#25152;&#26377;&#21487;&#33021;&#30340;&#26631;&#37327;&#20540;&#23610;&#24230;&#12290;&#35813;&#27169;&#22411;&#20351;&#29992;&#25552;&#20986;&#30340;&#25968;&#20540;&#22810;&#23610;&#24230;&#23884;&#20837;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#37319;&#29992;&#31616;&#21333;&#30340;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research on time-series self-supervised models shows great promise in learning semantic representations. However, it has been limited to small-scale datasets, e.g., thousands of temporal sequences. In this work, we make key technical contributions that are tailored to the numerical properties of time-series data and allow the model to scale to large datasets, e.g., millions of temporal sequences. We adopt the Transformer architecture by first partitioning the input into non-overlapping windows. Each window is then characterized by its normalized shape and two scalar values denoting the mean and standard deviation within each window. To embed scalar values that may possess arbitrary numerical scales to high-dimensional vectors, we propose a numerically multi-scaled embedding module enumerating all possible scales for the scalar values. The model undergoes pretraining using the proposed numerically multi-scaled embedding with a simple contrastive objective on a large-scale dataset
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#25968;&#25454;&#38598;&#31579;&#36873;&#26694;&#26550;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#20010;&#24615;&#21270;&#38754;&#21521;&#30446;&#26631;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#21517;&#20026;TopDial&#12290;&#35813;&#25968;&#25454;&#38598;&#36136;&#37327;&#39640;&#65292;&#26377;&#21161;&#20110;&#25506;&#32034;&#20010;&#24615;&#21270;&#30446;&#26631;&#23548;&#21521;&#30340;&#23545;&#35805;&#12290;</title><link>http://arxiv.org/abs/2310.07397</link><description>&lt;p&gt;
&#38754;&#21521;&#30446;&#26631;&#30340;&#20010;&#24615;&#21270;&#20027;&#21160;&#23545;&#35805;&#31995;&#32479;&#65306;&#38382;&#39064;&#24418;&#24335;&#21270;&#19982;&#25968;&#25454;&#38598;&#31579;&#36873;
&lt;/p&gt;
&lt;p&gt;
Target-oriented Proactive Dialogue Systems with Personalization: Problem Formulation and Dataset Curation. (arXiv:2310.07397v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07397
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#25968;&#25454;&#38598;&#31579;&#36873;&#26694;&#26550;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#20010;&#24615;&#21270;&#38754;&#21521;&#30446;&#26631;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#21517;&#20026;TopDial&#12290;&#35813;&#25968;&#25454;&#38598;&#36136;&#37327;&#39640;&#65292;&#26377;&#21161;&#20110;&#25506;&#32034;&#20010;&#24615;&#21270;&#30446;&#26631;&#23548;&#21521;&#30340;&#23545;&#35805;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#21521;&#30446;&#26631;&#30340;&#23545;&#35805;&#31995;&#32479;&#26088;&#22312;&#20027;&#21160;&#24341;&#23548;&#23545;&#35805;&#26397;&#21521;&#39044;&#23450;&#30340;&#30446;&#26631;&#25110;&#36798;&#25104;&#29305;&#23450;&#30340;&#31995;&#32479;&#30446;&#26631;&#65292;&#22312;&#23545;&#35805;&#23436;&#25104;&#36807;&#31243;&#20013;&#32771;&#34385;&#20010;&#24615;&#21270;&#12290;&#28982;&#32780;&#65292;&#38656;&#35201;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#65292;&#20174;&#38646;&#24320;&#22987;&#26500;&#24314;&#38656;&#35201;&#22823;&#37327;&#20154;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#35282;&#33394;&#25198;&#28436;&#26041;&#27861;&#30340;&#33258;&#21160;&#25968;&#25454;&#38598;&#31579;&#36873;&#26694;&#26550;&#12290;&#22522;&#20110;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#20010;&#24615;&#21270;&#38754;&#21521;&#30446;&#26631;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#21517;&#20026;TopDial&#65292;&#21253;&#21547;&#32422;18K&#20010;&#22810;&#36718;&#23545;&#35805;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#25968;&#25454;&#38598;&#20855;&#26377;&#24456;&#39640;&#30340;&#36136;&#37327;&#65292;&#21487;&#20197;&#29992;&#20110;&#25506;&#32034;&#20010;&#24615;&#21270;&#30446;&#26631;&#23548;&#21521;&#30340;&#23545;&#35805;&#12290;
&lt;/p&gt;
&lt;p&gt;
Target-oriented dialogue systems, designed to proactively steer conversations toward predefined targets or accomplish specific system-side goals, are an exciting area in conversational AI. In this work, by formulating a &lt;dialogue act, topic&gt; pair as the conversation target, we explore a novel problem of personalized target-oriented dialogue by considering personalization during the target accomplishment process. However, there remains an emergent need for high-quality datasets, and building one from scratch requires tremendous human effort. To address this, we propose an automatic dataset curation framework using a role-playing approach. Based on this framework, we construct a large-scale personalized target-oriented dialogue dataset, TopDial, which comprises about 18K multi-turn dialogues. The experimental results show that this dataset is of high quality and could contribute to exploring personalized target-oriented dialogue.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#36870;&#24378;&#21270;&#23398;&#20064;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#32456;&#31471;&#29992;&#25143;&#36807;&#21435;&#30340;&#28040;&#36153;&#25968;&#25454;&#65292;&#24110;&#21161;&#21019;&#24314;&#29992;&#25143;&#27599;&#26085;&#30340;&#30005;&#22120;&#35745;&#21010;&#65292;&#20174;&#32780;&#40723;&#21169;&#32456;&#31471;&#29992;&#25143;&#21442;&#19982;&#38656;&#27714;&#21709;&#24212;&#26381;&#21153;&#30340;&#25552;&#20379;&#12290;</title><link>http://arxiv.org/abs/2310.07389</link><description>&lt;p&gt;
&#23398;&#20064;&#29992;&#25143;&#20248;&#20808;&#30340;&#30005;&#22120;&#35843;&#24230;&#22870;&#21169;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Learning a Reward Function for User-Preferred Appliance Scheduling. (arXiv:2310.07389v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07389
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#36870;&#24378;&#21270;&#23398;&#20064;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#32456;&#31471;&#29992;&#25143;&#36807;&#21435;&#30340;&#28040;&#36153;&#25968;&#25454;&#65292;&#24110;&#21161;&#21019;&#24314;&#29992;&#25143;&#27599;&#26085;&#30340;&#30005;&#22120;&#35745;&#21010;&#65292;&#20174;&#32780;&#40723;&#21169;&#32456;&#31471;&#29992;&#25143;&#21442;&#19982;&#38656;&#27714;&#21709;&#24212;&#26381;&#21153;&#30340;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23621;&#27665;&#37096;&#38376;&#21152;&#36895;&#21457;&#23637;&#38656;&#27714;&#21709;&#24212;&#26381;&#21153;&#23545;&#20110;&#38477;&#20302;&#30005;&#21147;&#37096;&#38376;&#30340;&#30899;&#25490;&#25918;&#33267;&#20851;&#37325;&#35201;&#12290;&#38543;&#30528;&#22522;&#30784;&#35774;&#26045;&#30340;&#36827;&#27493;&#65292;&#40723;&#21169;&#32456;&#31471;&#29992;&#25143;&#21442;&#19982;&#33267;&#20851;&#37325;&#35201;&#12290;&#32456;&#31471;&#29992;&#25143;&#38750;&#24120;&#37325;&#35270;&#33258;&#24049;&#30340;&#38544;&#31169;&#21644;&#25511;&#21046;&#26435;&#65292;&#24182;&#24076;&#26395;&#22312;&#21019;&#24314;&#27599;&#26085;&#30005;&#22120;&#25805;&#20316;&#35745;&#21010;&#26102;&#21442;&#19982;&#21040;&#26381;&#21153;&#35774;&#35745;&#21644;&#20915;&#31574;&#36807;&#31243;&#20013;&#12290;&#27492;&#22806;&#65292;&#38500;&#38750;&#20182;&#20204;&#26377;&#32463;&#27982;&#25110;&#29615;&#22659;&#21160;&#26426;&#65292;&#20182;&#20204;&#36890;&#24120;&#19981;&#20250;&#20934;&#22791;&#29306;&#29298;&#33258;&#24049;&#30340;&#33298;&#36866;&#24230;&#26469;&#24110;&#21161;&#24179;&#34913;&#30005;&#21147;&#31995;&#32479;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36870;&#24378;&#21270;&#23398;&#20064;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#19981;&#35201;&#27714;&#29992;&#25143;&#26126;&#30830;&#38472;&#36848;&#38656;&#27714;&#21644;&#24895;&#26395;&#30340;&#24773;&#20917;&#19979;&#24110;&#21161;&#21019;&#24314;&#29992;&#25143;&#27599;&#26085;&#30340;&#30005;&#22120;&#35745;&#21010;&#12290;&#36890;&#36807;&#20351;&#29992;&#20182;&#20204;&#36807;&#21435;&#30340;&#28040;&#36153;&#25968;&#25454;&#65292;&#32456;&#31471;&#28040;&#36153;&#32773;&#23558;&#38544;&#24335;&#21442;&#19982;&#36825;&#20123;&#20915;&#31574;&#30340;&#21046;&#23450;&#65292;&#24182;&#22240;&#27492;&#24471;&#21040;&#32487;&#32493;&#21442;&#19982;&#38656;&#27714;&#21709;&#24212;&#26381;&#21153;&#25552;&#20379;&#30340;&#21160;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accelerated development of demand response service provision by the residential sector is crucial for reducing carbon-emissions in the power sector. Along with the infrastructure advancement, encouraging the end users to participate is crucial. End users highly value their privacy and control, and want to be included in the service design and decision-making process when creating the daily appliance operation schedules. Furthermore, unless they are financially or environmentally motivated, they are generally not prepared to sacrifice their comfort to help balance the power system. In this paper, we present an inverse-reinforcement-learning-based model that helps create the end users' daily appliance schedules without asking them to explicitly state their needs and wishes. By using their past consumption data, the end consumers will implicitly participate in the creation of those decisions and will thus be motivated to continue participating in the provision of demand response services.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#65292;&#24212;&#29992;&#20110;&#30382;&#32932;&#30284;&#25968;&#25454;&#38598;&#65292;&#21457;&#29616;&#35813;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.07380</link><description>&lt;p&gt;
&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#32452;&#32455;&#30149;&#29702;&#22270;&#20687;&#20998;&#31867;&#21644;&#33030;&#24369;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Histopathological Image Classification and Vulnerability Analysis using Federated Learning. (arXiv:2310.07380v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07380
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#65292;&#24212;&#29992;&#20110;&#30382;&#32932;&#30284;&#25968;&#25454;&#38598;&#65292;&#21457;&#29616;&#35813;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#30103;&#20445;&#20581;&#26159;&#26426;&#22120;&#23398;&#20064;(Machine Learning&#65292;ML)&#30340;&#20027;&#35201;&#24212;&#29992;&#20043;&#19968;&#12290;&#20256;&#32479;&#19978;&#65292;ML&#27169;&#22411;&#30001;&#20013;&#22830;&#26381;&#21153;&#22120;&#35757;&#32451;&#65292;&#36890;&#36807;&#27719;&#24635;&#26469;&#33258;&#21508;&#20010;&#20998;&#24067;&#24335;&#35774;&#22791;&#30340;&#25968;&#25454;&#26469;&#39044;&#27979;&#26032;&#29983;&#25104;&#25968;&#25454;&#30340;&#32467;&#26524;&#12290;&#36825;&#26159;&#19968;&#20010;&#20027;&#35201;&#20851;&#27880;&#28857;&#65292;&#22240;&#20026;&#27169;&#22411;&#21487;&#20197;&#35775;&#38382;&#25935;&#24863;&#30340;&#29992;&#25143;&#20449;&#24687;&#65292;&#24341;&#21457;&#38544;&#31169;&#38382;&#39064;&#12290;&#32852;&#37030;&#23398;&#20064;(FL)&#26041;&#27861;&#21487;&#20197;&#24110;&#21161;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65306;&#20840;&#23616;&#27169;&#22411;&#23558;&#20854;&#21103;&#26412;&#21457;&#36865;&#32473;&#25152;&#26377;&#23458;&#25143;&#31471;&#65292;&#36825;&#20123;&#23458;&#25143;&#31471;&#35757;&#32451;&#36825;&#20123;&#21103;&#26412;&#65292;&#24182;&#23558;&#26356;&#26032;(&#26435;&#37325;)&#21457;&#36865;&#22238;&#32473;&#20840;&#23616;&#27169;&#22411;&#12290;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#65292;&#20840;&#23616;&#27169;&#22411;&#21464;&#24471;&#26356;&#21152;&#20934;&#30830;&#12290;&#35757;&#32451;&#36807;&#31243;&#20013;&#20445;&#25252;&#20102;&#25968;&#25454;&#38544;&#31169;&#65292;&#22240;&#20026;&#35757;&#32451;&#26159;&#22312;&#23458;&#25143;&#31471;&#35774;&#22791;&#19978;&#26412;&#22320;&#36827;&#34892;&#30340;&#12290;&#28982;&#32780;&#65292;&#20840;&#23616;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#25968;&#25454;&#27745;&#26579;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#20445;&#25252;&#38544;&#31169;&#30340;&#32852;&#37030;&#23398;&#20064;&#25216;&#26415;&#65292;&#24212;&#29992;&#20110;&#19968;&#20010;&#30382;&#32932;&#30284;&#25968;&#25454;&#38598;&#65292;&#24182;&#23637;&#31034;&#20102;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#30340;&#24773;&#20917;&#12290;&#21313;&#20010;&#23458;&#25143;&#31471;&#35757;&#32451;&#27169;&#22411;&#65292;&#20294;&#20854;&#20013;&#19968;&#20010;&#25925;&#24847;&#24341;&#20837;&#20102;&#32763;&#36716;&#26631;&#31614;&#20316;&#20026;&#25915;&#20987;&#12290;&#36825;&#38477;&#20302;&#20102;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Healthcare is one of the foremost applications of machine learning (ML). Traditionally, ML models are trained by central servers, which aggregate data from various distributed devices to forecast the results for newly generated data. This is a major concern as models can access sensitive user information, which raises privacy concerns. A federated learning (FL) approach can help address this issue: A global model sends its copy to all clients who train these copies, and the clients send the updates (weights) back to it. Over time, the global model improves and becomes more accurate. Data privacy is protected during training, as it is conducted locally on the clients' devices.  However, the global model is susceptible to data poisoning. We develop a privacy-preserving FL technique for a skin cancer dataset and show that the model is prone to data poisoning attacks. Ten clients train the model, but one of them intentionally introduces flipped labels as an attack. This reduces the accurac
&lt;/p&gt;</description></item><item><title>&#22240;&#26524;&#26080;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#65288;CAUSE&#65289;&#26159;&#19968;&#20010;&#21033;&#29992;&#22240;&#26524;&#25512;&#26029;&#30340;&#26032;&#26694;&#26550;&#65292;&#26088;&#22312;&#23454;&#29616;&#26080;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#26500;&#24314;&#27010;&#24565;&#32858;&#31867;&#34920;&#20316;&#20026;&#20013;&#20171;&#65292;&#24182;&#19982;&#27010;&#24565;&#33258;&#30417;&#30563;&#23398;&#20064;&#24314;&#31435;&#32852;&#31995;&#65292;&#35299;&#20915;&#20102;&#26080;&#30417;&#30563;&#20998;&#21106;&#20013;&#36866;&#24403;&#32858;&#31867;&#27700;&#24179;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.07379</link><description>&lt;p&gt;
&#22240;&#26524;&#26080;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Causal Unsupervised Semantic Segmentation. (arXiv:2310.07379v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07379
&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#26080;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#65288;CAUSE&#65289;&#26159;&#19968;&#20010;&#21033;&#29992;&#22240;&#26524;&#25512;&#26029;&#30340;&#26032;&#26694;&#26550;&#65292;&#26088;&#22312;&#23454;&#29616;&#26080;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#26500;&#24314;&#27010;&#24565;&#32858;&#31867;&#34920;&#20316;&#20026;&#20013;&#20171;&#65292;&#24182;&#19982;&#27010;&#24565;&#33258;&#30417;&#30563;&#23398;&#20064;&#24314;&#31435;&#32852;&#31995;&#65292;&#35299;&#20915;&#20102;&#26080;&#30417;&#30563;&#20998;&#21106;&#20013;&#36866;&#24403;&#32858;&#31867;&#27700;&#24179;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#26088;&#22312;&#22312;&#27809;&#26377;&#20154;&#24037;&#26631;&#27880;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#35821;&#20041;&#20998;&#32452;&#12290;&#38543;&#30528;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;&#20986;&#29616;&#65292;&#21508;&#31181;&#26694;&#26550;&#21033;&#29992;&#39044;&#35757;&#32451;&#29305;&#24449;&#35757;&#32451;&#26080;&#30417;&#30563;&#23494;&#38598;&#39044;&#27979;&#30340;&#39044;&#27979;&#22836;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#31181;&#26080;&#30417;&#30563;&#35774;&#32622;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#26159;&#30830;&#23450;&#29992;&#20110;&#20998;&#21106;&#27010;&#24565;&#25152;&#38656;&#30340;&#36866;&#24403;&#32858;&#31867;&#27700;&#24179;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#22240;&#26524;&#26080;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#65288;CAUSE&#65289;&#65292;&#23427;&#21033;&#29992;&#20102;&#22240;&#26524;&#25512;&#26029;&#30340;&#35265;&#35299;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26725;&#25509;&#20102;&#38754;&#21521;&#24178;&#39044;&#30340;&#26041;&#27861;&#65288;&#21363;&#21069;&#38376;&#35843;&#25972;&#65289;&#65292;&#20197;&#23450;&#20041;&#36866;&#21512;&#26080;&#30417;&#30563;&#39044;&#27979;&#30340;&#20004;&#27493;&#20219;&#21153;&#12290;&#31532;&#19968;&#27493;&#28041;&#21450;&#26500;&#24314;&#19968;&#20010;&#27010;&#24565;&#32858;&#31867;&#34920;&#20316;&#20026;&#20013;&#20171;&#65292;&#20197;&#31163;&#25955;&#24418;&#24335;&#34920;&#31034;&#19981;&#21516;&#31890;&#24230;&#23618;&#27425;&#19978;&#30340;&#21487;&#33021;&#27010;&#24565;&#21407;&#22411;&#12290;&#28982;&#21518;&#65292;&#20013;&#20171;&#19982;&#38543;&#21518;&#30340;&#27010;&#24565;&#33258;&#30417;&#30563;&#23398;&#20064;&#24314;&#31435;&#20102;&#26126;&#30830;&#30340;&#32852;&#31995;...
&lt;/p&gt;
&lt;p&gt;
Unsupervised semantic segmentation aims to achieve high-quality semantic grouping without human-labeled annotations. With the advent of self-supervised pre-training, various frameworks utilize the pre-trained features to train prediction heads for unsupervised dense prediction. However, a significant challenge in this unsupervised setup is determining the appropriate level of clustering required for segmenting concepts. To address it, we propose a novel framework, CAusal Unsupervised Semantic sEgmentation (CAUSE), which leverages insights from causal inference. Specifically, we bridge intervention-oriented approach (i.e., frontdoor adjustment) to define suitable two-step tasks for unsupervised prediction. The first step involves constructing a concept clusterbook as a mediator, which represents possible concept prototypes at different levels of granularity in a discretized form. Then, the mediator establishes an explicit link to the subsequent concept-wise self-supervised learning for 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21160;&#24577;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#23616;&#37096;&#20960;&#20309;&#32467;&#26500;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#28857;&#20113;&#30340;&#21435;&#22122;&#21644;&#24322;&#24120;&#26816;&#27979;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.07376</link><description>&lt;p&gt;
&#20351;&#29992;&#21160;&#24577;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#23616;&#37096;&#20960;&#20309;&#32467;&#26500;&#23545;&#28857;&#20113;&#21435;&#22122;&#21644;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Point Cloud Denoising and Outlier Detection with Local Geometric Structure by Dynamic Graph CNN. (arXiv:2310.07376v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07376
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21160;&#24577;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#23616;&#37096;&#20960;&#20309;&#32467;&#26500;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#28857;&#20113;&#30340;&#21435;&#22122;&#21644;&#24322;&#24120;&#26816;&#27979;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20250;&#30340;&#25968;&#23383;&#21270;&#24555;&#36895;&#21457;&#23637;&#27491;&#22312;&#26397;&#30528;&#23454;&#29616;&#25968;&#23383;&#23402;&#29983;&#21644;&#20803;&#23431;&#23449;&#30340;&#30446;&#26631;&#36808;&#36827;&#12290;&#29305;&#21035;&#26159;&#65292;&#28857;&#20113;&#20316;&#20026;3D&#31354;&#38388;&#30340;&#19968;&#31181;&#23186;&#20307;&#26684;&#24335;&#27491;&#22312;&#24341;&#36215;&#20851;&#27880;&#12290;&#30001;&#20110;&#27979;&#37327;&#35823;&#24046;&#65292;&#28857;&#20113;&#25968;&#25454;&#21463;&#21040;&#22122;&#22768;&#21644;&#24322;&#24120;&#20540;&#30340;&#27745;&#26579;&#12290;&#22240;&#27492;&#65292;&#23545;&#28857;&#20113;&#36827;&#34892;&#21435;&#22122;&#21644;&#24322;&#24120;&#26816;&#27979;&#26159;&#24517;&#35201;&#30340;&#12290;&#20854;&#20013;&#65292;PointCleanNet&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#28857;&#20113;&#21435;&#22122;&#21644;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23427;&#27809;&#26377;&#32771;&#34385;&#21040;&#34917;&#19969;&#30340;&#23616;&#37096;&#20960;&#20309;&#32467;&#26500;&#12290;&#25105;&#20204;&#36890;&#36807;&#24212;&#29992;&#22522;&#20110;&#21160;&#24577;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#30340;&#20004;&#31181;&#31867;&#22411;&#30340;&#22270;&#21367;&#31215;&#23618;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#34913;&#37327;&#24322;&#24120;&#26816;&#27979;&#20934;&#30830;&#24615;&#30340;AUPR&#21644;&#34913;&#37327;&#21435;&#22122;&#20934;&#30830;&#24615;&#30340;Chamfer&#36317;&#31163;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The digitalization of society is rapidly developing toward the realization of the digital twin and metaverse. In particular, point clouds are attracting attention as a media format for 3D space. Point cloud data is contaminated with noise and outliers due to measurement errors. Therefore, denoising and outlier detection are necessary for point cloud processing. Among them, PointCleanNet is an effective method for point cloud denoising and outlier detection. However, it does not consider the local geometric structure of the patch. We solve this problem by applying two types of graph convolutional layer designed based on the Dynamic Graph CNN. Experimental results show that the proposed methods outperform the conventional method in AUPR, which indicates outlier detection accuracy, and Chamfer Distance, which indicates denoising accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24037;&#19994;&#29289;&#32852;&#32593;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#30340;&#32852;&#37030;&#36801;&#31227;&#23398;&#20064;&#65288;FTL&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#32452;&#21512;&#31070;&#32463;&#32593;&#32476;&#23545;IIoT&#25968;&#25454;&#36827;&#34892;&#20998;&#21106;&#24182;&#26356;&#26032;&#26381;&#21153;&#22120;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#39640;&#24615;&#33021;&#30340;&#20837;&#20405;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2310.07354</link><description>&lt;p&gt;
&#32473;&#19982;&#21462;&#65306;&#29992;&#20110;&#24037;&#19994;&#29289;&#32852;&#32593;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#30340;&#32852;&#37030;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Give and Take: Federated Transfer Learning for Industrial IoT Network Intrusion Detection. (arXiv:2310.07354v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07354
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24037;&#19994;&#29289;&#32852;&#32593;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#30340;&#32852;&#37030;&#36801;&#31227;&#23398;&#20064;&#65288;FTL&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#32452;&#21512;&#31070;&#32463;&#32593;&#32476;&#23545;IIoT&#25968;&#25454;&#36827;&#34892;&#20998;&#21106;&#24182;&#26356;&#26032;&#26381;&#21153;&#22120;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#39640;&#24615;&#33021;&#30340;&#20837;&#20405;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20114;&#32852;&#32593;&#29289;&#32852;&#32593;&#65288;IoT&#65289;&#25216;&#26415;&#30340;&#24555;&#36895;&#22686;&#38271;&#24050;&#25104;&#20026;&#24403;&#20170;&#24037;&#19994;&#30340;&#19968;&#37096;&#20998;&#65292;&#24418;&#25104;&#20102;&#24037;&#19994;&#29289;&#32852;&#32593;&#65288;IIoT&#65289;&#20513;&#35758;&#65292;&#24037;&#19994;&#21033;&#29992;IoT&#36890;&#36807;&#25968;&#25454;&#20998;&#26512;&#21644;&#20113;&#35745;&#31639;&#31561;&#26032;&#20852;&#35299;&#20915;&#26041;&#26696;&#25913;&#36827;&#36890;&#20449;&#21644;&#36830;&#25509;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;IoT&#30340;&#24555;&#36895;&#20351;&#29992;&#20351;&#20854;&#25104;&#20026;&#32593;&#32476;&#29359;&#32618;&#20998;&#23376;&#30340;&#26377;&#21560;&#24341;&#21147;&#30446;&#26631;&#12290;&#22240;&#27492;&#65292;&#20445;&#25252;&#36825;&#20123;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#37030;&#36801;&#31227;&#23398;&#20064;&#65288;FTL&#65289;&#26041;&#27861;&#26469;&#36827;&#34892;IIoT&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#12290;&#20316;&#20026;&#30740;&#31350;&#30340;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#32452;&#21512;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#36827;&#34892;FTL&#30340;&#26680;&#24515;&#12290;&#25152;&#25552;&#20986;&#30340;&#25216;&#26415;&#23558;IoT&#25968;&#25454;&#20998;&#21106;&#22312;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#35774;&#22791;&#20043;&#38388;&#29983;&#25104;&#30456;&#24212;&#30340;&#27169;&#22411;&#65292;&#23458;&#25143;&#27169;&#22411;&#30340;&#26435;&#37325;&#34987;&#32452;&#21512;&#20197;&#26356;&#26032;&#26381;&#21153;&#22120;&#27169;&#22411;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;FTL&#30340;&#35774;&#32622;&#22312;IIoT&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#20043;&#38388;&#30340;&#36845;&#20195;&#20013;&#34920;&#29616;&#20986;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid growth in Internet of Things (IoT) technology has become an integral part of today's industries forming the Industrial IoT (IIoT) initiative, where industries are leveraging IoT to improve communication and connectivity via emerging solutions like data analytics and cloud computing. Unfortunately, the rapid use of IoT has made it an attractive target for cybercriminals. Therefore, protecting these systems is of utmost importance. In this paper, we propose a federated transfer learning (FTL) approach to perform IIoT network intrusion detection. As part of the research, we also propose a combinational neural network as the centerpiece for performing FTL. The proposed technique splits IoT data between the client and server devices to generate corresponding models, and the weights of the client models are combined to update the server model. Results showcase high performance for the FTL setup between iterations on both the IIoT clients and the server. Additionally, the proposed F
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#25968;&#23383;&#23402;&#29983;&#20013;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#21644;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#35821;&#20041;&#20851;&#32852;&#35268;&#21017;&#23398;&#20064;&#30340;&#27969;&#31243;&#65292;&#24182;&#19988;&#24341;&#20837;&#20102;&#26032;&#30340;&#35821;&#20041;&#20851;&#32852;&#35268;&#21017;&#20934;&#21017;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#24037;&#19994;&#27700;&#32593;&#32476;&#22330;&#26223;&#20013;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#33021;&#22815;&#23398;&#20064;&#21040;&#26356;&#20855;&#27867;&#21270;&#24615;&#30340;&#21253;&#21547;&#35821;&#20041;&#20449;&#24687;&#30340;&#20851;&#32852;&#35268;&#21017;&#65292;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#24037;&#19994;&#24212;&#29992;&#32972;&#26223;&#19979;&#30340;&#35821;&#20041;&#20851;&#32852;&#35268;&#21017;&#23398;&#20064;&#25171;&#19979;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2310.07348</link><description>&lt;p&gt;
&#20174;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#21644;&#30693;&#35782;&#22270;&#35889;&#20013;&#23398;&#20064;&#35821;&#20041;&#20851;&#32852;&#35268;&#21017;
&lt;/p&gt;
&lt;p&gt;
Semantic Association Rule Learning from Time Series Data and Knowledge Graphs. (arXiv:2310.07348v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07348
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#25968;&#23383;&#23402;&#29983;&#20013;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#21644;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#35821;&#20041;&#20851;&#32852;&#35268;&#21017;&#23398;&#20064;&#30340;&#27969;&#31243;&#65292;&#24182;&#19988;&#24341;&#20837;&#20102;&#26032;&#30340;&#35821;&#20041;&#20851;&#32852;&#35268;&#21017;&#20934;&#21017;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#24037;&#19994;&#27700;&#32593;&#32476;&#22330;&#26223;&#20013;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#33021;&#22815;&#23398;&#20064;&#21040;&#26356;&#20855;&#27867;&#21270;&#24615;&#30340;&#21253;&#21547;&#35821;&#20041;&#20449;&#24687;&#30340;&#20851;&#32852;&#35268;&#21017;&#65292;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#24037;&#19994;&#24212;&#29992;&#32972;&#26223;&#19979;&#30340;&#35821;&#20041;&#20851;&#32852;&#35268;&#21017;&#23398;&#20064;&#25171;&#19979;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#23402;&#29983;&#65288;DT&#65289;&#26159;&#29289;&#29702;&#31995;&#32479;&#30740;&#31350;&#20013;&#30340;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#27010;&#24565;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#21253;&#25324;&#30417;&#27979;&#21644;&#33258;&#21160;&#25512;&#29702;&#22312;&#20869;&#30340;&#20808;&#36827;&#21151;&#33021;&#12290;&#35821;&#20041;&#25216;&#26415;&#22914;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#26368;&#36817;&#22312;DT&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#20449;&#24687;&#24314;&#27169;&#26041;&#38754;&#12290;&#22522;&#20110;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;KG&#21644;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#22312;DT&#20013;&#36827;&#34892;&#35821;&#20041;&#20851;&#32852;&#35268;&#21017;&#23398;&#20064;&#30340;&#27969;&#31243;&#12290;&#38500;&#20102;&#36825;&#20010;&#21021;&#22987;&#27969;&#31243;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#26032;&#30340;&#35821;&#20041;&#20851;&#32852;&#35268;&#21017;&#20934;&#21017;&#12290;&#35813;&#26041;&#27861;&#22312;&#19968;&#20010;&#24037;&#19994;&#27700;&#32593;&#32476;&#22330;&#26223;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#21021;&#27493;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#22815;&#23398;&#20064;&#21040;&#21253;&#21547;&#35821;&#20041;&#20449;&#24687;&#30340;&#22823;&#37327;&#20851;&#32852;&#35268;&#21017;&#65292;&#20855;&#26377;&#26356;&#24378;&#30340;&#27867;&#21270;&#24615;&#12290;&#26412;&#25991;&#26088;&#22312;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#29305;&#21035;&#26159;&#22312;&#24037;&#19994;&#24212;&#29992;&#32972;&#26223;&#19979;&#20351;&#29992;&#35821;&#20041;&#20851;&#32852;&#35268;&#21017;&#23398;&#20064;&#22880;&#23450;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Digital Twins (DT) are a promising concept in cyber-physical systems research due to their advanced features including monitoring and automated reasoning. Semantic technologies such as Knowledge Graphs (KG) are recently being utilized in DTs especially for information modelling. Building on this move, this paper proposes a pipeline for semantic association rule learning in DTs using KGs and time series data. In addition to this initial pipeline, we also propose new semantic association rule criterion. The approach is evaluated on an industrial water network scenario. Initial evaluation shows that the proposed approach is able to learn a high number of association rules with semantic information which are more generalizable. The paper aims to set a foundation for further work on using semantic association rule learning especially in the context of industrial applications.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;ELECTRA&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#29616;&#26377;&#30340;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#36741;&#21161;&#27169;&#22411;&#65292;&#36890;&#36807;&#28201;&#24230;&#32553;&#25918;&#24179;&#28369;&#20027;&#27169;&#22411;&#30340;&#36755;&#20986;&#20998;&#24067;&#65292;&#36798;&#21040;&#19982;&#26368;&#20808;&#36827;&#30340;ELECTRA&#39044;&#35757;&#32451;&#26041;&#27861;&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#65292;&#24182;&#26174;&#33879;&#20943;&#23569;&#20102;&#36741;&#21161;&#27169;&#22411;&#20849;&#21516;&#35757;&#32451;&#24102;&#26469;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2310.07347</link><description>&lt;p&gt;
&#39640;&#25928;&#39044;&#35757;&#32451;&#30340;&#24555;&#36895;ELECTRA
&lt;/p&gt;
&lt;p&gt;
Fast-ELECTRA for Efficient Pre-training. (arXiv:2310.07347v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07347
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;ELECTRA&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#29616;&#26377;&#30340;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#36741;&#21161;&#27169;&#22411;&#65292;&#36890;&#36807;&#28201;&#24230;&#32553;&#25918;&#24179;&#28369;&#20027;&#27169;&#22411;&#30340;&#36755;&#20986;&#20998;&#24067;&#65292;&#36798;&#21040;&#19982;&#26368;&#20808;&#36827;&#30340;ELECTRA&#39044;&#35757;&#32451;&#26041;&#27861;&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#65292;&#24182;&#26174;&#33879;&#20943;&#23569;&#20102;&#36741;&#21161;&#27169;&#22411;&#20849;&#21516;&#35757;&#32451;&#24102;&#26469;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ELECTRA&#36890;&#36807;&#26816;&#27979;&#24207;&#21015;&#20013;&#34987;&#36741;&#21161;&#27169;&#22411;&#26367;&#25442;&#30340;&#26631;&#35760;&#26469;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#12290;&#34429;&#28982;ELECTRA&#22823;&#22823;&#25552;&#39640;&#20102;&#25928;&#29575;&#65292;&#20294;&#20854;&#28508;&#21147;&#21463;&#21040;&#20102;&#36741;&#21161;&#27169;&#22411;&#24102;&#26469;&#30340;&#35757;&#32451;&#25104;&#26412;&#30340;&#38480;&#21046;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#20010;&#19982;&#20027;&#27169;&#22411;&#20849;&#21516;&#35757;&#32451;&#30340;&#27169;&#22411;&#20165;&#29992;&#20110;&#36741;&#21161;&#20027;&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#24182;&#19988;&#22312;&#35757;&#32451;&#21518;&#34987;&#20002;&#24323;&#12290;&#36825;&#23548;&#33268;&#22823;&#37327;&#30340;&#35757;&#32451;&#25104;&#26412;&#34987;&#30333;&#30333;&#28010;&#36153;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Fast-ELECTRA&#65292;&#23427;&#21033;&#29992;&#29616;&#26377;&#30340;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#36741;&#21161;&#27169;&#22411;&#12290;&#20026;&#20102;&#26500;&#24314;&#20027;&#27169;&#22411;&#30340;&#23398;&#20064;&#35838;&#31243;&#65292;&#25105;&#20204;&#36890;&#36807;&#28201;&#24230;&#32553;&#25918;&#21644;&#36882;&#20943;&#30340;&#26041;&#27861;&#24179;&#28369;&#20854;&#36755;&#20986;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#26368;&#20808;&#36827;&#30340;ELECTRA&#39118;&#26684;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#30456;&#23218;&#32654;&#65292;&#21516;&#26102;&#26174;&#33879;&#20943;&#23569;&#20102;&#36741;&#21161;&#27169;&#22411;&#20849;&#21516;&#35757;&#32451;&#24102;&#26469;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#38477;&#20302;&#20102;&#27169;&#22411;&#23545;&#35757;&#32451;&#25968;&#25454;&#30340;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
ELECTRA pre-trains language models by detecting tokens in a sequence that have been replaced by an auxiliary model. Although ELECTRA offers a significant boost in efficiency, its potential is constrained by the training cost brought by the auxiliary model. Notably, this model, which is jointly trained with the main model, only serves to assist the training of the main model and is discarded post-training. This results in a substantial amount of training cost being expended in vain. To mitigate this issue, we propose Fast-ELECTRA, which leverages an existing language model as the auxiliary model. To construct a learning curriculum for the main model, we smooth its output distribution via temperature scaling following a descending schedule. Our approach rivals the performance of state-of-the-art ELECTRA-style pre-training methods, while significantly eliminating the computation and memory cost brought by the joint training of the auxiliary model. Our method also reduces the sensitivity t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#31038;&#20132;&#36816;&#21160;&#28508;&#31354;&#38388;&#26469;&#29983;&#25104;&#26426;&#22120;&#20154;&#25511;&#21046;&#30340;&#26032;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#25317;&#25380;&#29615;&#22659;&#20013;&#30340;&#26377;&#25928;&#23548;&#33322;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#20154;&#31867;&#23545;&#26426;&#22120;&#20154;&#30340;&#24847;&#35782;&#27010;&#24565;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#31181;&#24847;&#35782;&#30340;&#34701;&#20837;&#33021;&#22815;&#25913;&#21892;&#23548;&#33322;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.07335</link><description>&lt;p&gt;
&#22312;&#25317;&#25380;&#29615;&#22659;&#20013;&#25506;&#32034;&#31038;&#20132;&#36816;&#21160;&#28508;&#31354;&#38388;&#21644;&#20154;&#31867;&#24847;&#35782;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#26426;&#22120;&#20154;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
Exploring Social Motion Latent Space and Human Awareness for Effective Robot Navigation in Crowded Environments. (arXiv:2310.07335v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07335
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#31038;&#20132;&#36816;&#21160;&#28508;&#31354;&#38388;&#26469;&#29983;&#25104;&#26426;&#22120;&#20154;&#25511;&#21046;&#30340;&#26032;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#25317;&#25380;&#29615;&#22659;&#20013;&#30340;&#26377;&#25928;&#23548;&#33322;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#20154;&#31867;&#23545;&#26426;&#22120;&#20154;&#30340;&#24847;&#35782;&#27010;&#24565;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#31181;&#24847;&#35782;&#30340;&#34701;&#20837;&#33021;&#22815;&#25913;&#21892;&#23548;&#33322;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31038;&#20132;&#26426;&#22120;&#20154;&#23548;&#33322;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#31038;&#20132;&#36816;&#21160;&#28508;&#31354;&#38388;&#20013;&#23398;&#20064;&#29983;&#25104;&#26426;&#22120;&#20154;&#25511;&#21046;&#65292;&#23454;&#29616;&#20102;&#31038;&#20132;&#23548;&#33322;&#25351;&#26631;&#30340;&#26174;&#33879;&#25913;&#21892;&#65292;&#22914;&#25104;&#21151;&#29575;&#12289;&#23548;&#33322;&#26102;&#38388;&#21644;&#36712;&#36857;&#38271;&#24230;&#65292;&#21516;&#26102;&#20135;&#29983;&#20102;&#26356;&#24179;&#32531;&#30340;&#36712;&#36857;&#65288;&#20943;&#23569;&#25238;&#21160;&#21644;&#35282;&#24230;&#20559;&#24046;&#65289;&#21644;&#26356;&#20855;&#39044;&#35265;&#24615;&#30340;&#36712;&#36857;&#12290;&#36890;&#36807;&#19982;&#22522;&#20934;&#27169;&#22411;&#22312;&#21508;&#31181;&#22330;&#26223;&#20013;&#30340;&#27604;&#36739;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;&#27492;&#22806;&#65292;&#23558;&#20154;&#31867;&#23545;&#26426;&#22120;&#20154;&#30340;&#24847;&#35782;&#27010;&#24565;&#24341;&#20837;&#31038;&#20132;&#26426;&#22120;&#20154;&#23548;&#33322;&#26694;&#26550;&#20013;&#65292;&#26174;&#31034;&#20986;&#20154;&#31867;&#24847;&#35782;&#30340;&#34701;&#20837;&#33021;&#22815;&#20943;&#23569;&#36712;&#36857;&#38271;&#24230;&#21644;&#24179;&#28369;&#36712;&#36857;&#65292;&#22240;&#20026;&#20154;&#31867;&#33021;&#22815;&#19982;&#26426;&#22120;&#20154;&#31215;&#26497;&#20114;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work proposes a novel approach to social robot navigation by learning to generate robot controls from a social motion latent space. By leveraging this social motion latent space, the proposed method achieves significant improvements in social navigation metrics such as success rate, navigation time, and trajectory length while producing smoother (less jerk and angular deviations) and more anticipatory trajectories. The superiority of the proposed method is demonstrated through comparison with baseline models in various scenarios. Additionally, the concept of humans' awareness towards the robot is introduced into the social robot navigation framework, showing that incorporating human awareness leads to shorter and smoother trajectories owing to humans' ability to positively interact with the robot.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36827;&#34892;&#20102;&#23545;&#20013;&#25991;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#23548;&#35843;&#25972;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#25506;&#32034;&#20102;LLM&#22522;&#30784;&#12289;&#21442;&#25968;&#39640;&#25928;&#30340;&#26041;&#27861;&#21644;&#25351;&#20196;&#25968;&#25454;&#31867;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#30740;&#31350;&#20102;&#20854;&#20182;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#20026;&#23450;&#21046;&#33021;&#26356;&#22909;&#21709;&#24212;&#20013;&#25991;&#25351;&#20196;&#30340;LLM&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#21457;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.07328</link><description>&lt;p&gt;
&#23545;&#20013;&#25991;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#23548;&#35843;&#25972;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Empirical Study of Instruction-tuning Large Language Models in Chinese. (arXiv:2310.07328v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07328
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36827;&#34892;&#20102;&#23545;&#20013;&#25991;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#23548;&#35843;&#25972;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#25506;&#32034;&#20102;LLM&#22522;&#30784;&#12289;&#21442;&#25968;&#39640;&#25928;&#30340;&#26041;&#27861;&#21644;&#25351;&#20196;&#25968;&#25454;&#31867;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#30740;&#31350;&#20102;&#20854;&#20182;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#20026;&#23450;&#21046;&#33021;&#26356;&#22909;&#21709;&#24212;&#20013;&#25991;&#25351;&#20196;&#30340;LLM&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#30340;&#25104;&#21151;&#39564;&#35777;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#20013;&#30340;&#28508;&#21147;&#12290;&#38543;&#21518;&#65292;LLM&#30340;&#21457;&#24067;&#24341;&#36215;&#20102;&#24320;&#28304;&#31038;&#21306;&#23545;&#25351;&#23548;&#35843;&#25972;&#30340;&#20852;&#36259;&#65292;&#36825;&#34987;&#35748;&#20026;&#21487;&#20197;&#21152;&#24555;ChatGPT&#30340;&#22797;&#21046;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#20013;&#25991;&#30340;LLM&#25351;&#23548;&#35843;&#25972;&#30340;&#30740;&#31350;&#20173;&#22788;&#20110;&#26089;&#26399;&#38454;&#27573;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#23545;&#20013;&#25991;LLM&#30340;&#25351;&#23548;&#35843;&#25972;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#36825;&#21487;&#20197;&#20316;&#20026;&#19968;&#26412;&#25552;&#20379;&#26377;&#20215;&#20540;&#21457;&#29616;&#30340;&#28921;&#39274;&#20070;&#26469;&#26377;&#25928;&#22320;&#23450;&#21046;LLM&#65292;&#20351;&#20854;&#33021;&#22815;&#26356;&#22909;&#22320;&#21709;&#24212;&#20013;&#25991;&#25351;&#20196;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#25506;&#32034;&#20102;LLM&#22522;&#30784;&#12289;&#21442;&#25968;&#39640;&#25928;&#30340;&#26041;&#27861;&#21644;&#25351;&#20196;&#25968;&#25454;&#31867;&#22411;&#36825;&#19977;&#20010;&#23545;&#25351;&#23548;&#35843;&#25972;&#33267;&#20851;&#37325;&#35201;&#30340;&#22240;&#32032;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#23454;&#39564;&#26469;&#30740;&#31350;&#20854;&#20182;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#20363;&#22914;&#24605;&#32500;&#38142;&#25968;&#25454;&#21644;&#20154;&#31867;&#20215;&#20540;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#20010;&#23454;&#35777;&#30740;&#31350;&#33021;&#22815;
&lt;/p&gt;
&lt;p&gt;
The success of ChatGPT validates the potential of large language models (LLMs) in artificial general intelligence (AGI). Subsequently, the release of LLMs has sparked the open-source community's interest in instruction-tuning, which is deemed to accelerate ChatGPT's replication process. However, research on instruction-tuning LLMs in Chinese, the world's most spoken language, is still in its early stages. Therefore, this paper makes an in-depth empirical study of instruction-tuning LLMs in Chinese, which can serve as a cookbook that provides valuable findings for effectively customizing LLMs that can better respond to Chinese instructions. Specifically, we systematically explore the impact of LLM bases, parameter-efficient methods, instruction data types, which are the three most important elements for instruction-tuning. Besides, we also conduct experiment to study the impact of other factors, e.g., chain-of-thought data and human-value alignment. We hope that this empirical study can
&lt;/p&gt;</description></item><item><title>&#22312;gelu-4l&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#35777;&#25454;&#34920;&#26126;&#20869;&#23384;&#31649;&#29702;&#23545;&#20110;transformer&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#35828;&#26126;&#20102;Direct Logit Attribution&#25216;&#26415;&#30340;&#19981;&#20934;&#30830;&#20043;&#22788;&#12290;</title><link>http://arxiv.org/abs/2310.07325</link><description>&lt;p&gt;
&#30452;&#25509;&#36923;&#36753;&#23646;&#24615;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#65306;gelu-4l&#20013;&#30340;&#20869;&#23384;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
An Adversarial Example for Direct Logit Attribution: Memory Management in gelu-4l. (arXiv:2310.07325v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07325
&lt;/p&gt;
&lt;p&gt;
&#22312;gelu-4l&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#35777;&#25454;&#34920;&#26126;&#20869;&#23384;&#31649;&#29702;&#23545;&#20110;transformer&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#35828;&#26126;&#20102;Direct Logit Attribution&#25216;&#26415;&#30340;&#19981;&#20934;&#30830;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;4&#23618;transformer&#20013;&#20869;&#23384;&#31649;&#29702;&#30340;&#20855;&#20307;&#35777;&#25454;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#21069;&#21521;&#20256;&#25773;&#36807;&#31243;&#20013;&#65292;&#27169;&#22411;&#32452;&#20214;&#19968;&#33268;&#22320;&#31227;&#38500;&#21069;&#38754;&#32452;&#20214;&#30340;&#36755;&#20986;&#65292;&#36825;&#26159;&#19968;&#31181;&#28165;&#29702;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35299;&#37322;&#24615;&#25216;&#26415;Direct Logit Attribution&#25552;&#20379;&#20102;&#35823;&#23548;&#24615;&#32467;&#26524;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26126;&#30830;&#30340;&#20363;&#23376;&#65292;&#35777;&#26126;&#36825;&#31181;&#25216;&#26415;&#26159;&#19981;&#20934;&#30830;&#30340;&#65292;&#22240;&#20026;&#23427;&#27809;&#26377;&#32771;&#34385;&#21040;&#28165;&#29702;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
We provide concrete evidence for memory management in a 4-layer transformer. Specifically, we identify clean-up behavior, in which model components consistently remove the output of preceeding components during a forward pass. Our findings suggest that the interpretability technique Direct Logit Attribution provides misleading results. We show explicit examples where this technique is inaccurate, as it does not account for clean-up behavior.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#24503;&#35821;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#65292;&#21457;&#29616;&#23558;&#25968;&#25454;&#22810;&#26679;&#24615;&#32622;&#20110;&#25968;&#25454;&#36136;&#37327;&#20043;&#19978;&#30340;&#20132;&#21449;&#39046;&#22495;&#25968;&#25454;&#38598;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#36229;&#36807;&#20102;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.07321</link><description>&lt;p&gt;
&#20851;&#20110;&#20132;&#21449;&#39046;&#22495;&#25968;&#25454;&#23545;&#24503;&#35821;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
On the Impact of Cross-Domain Data on German Language Models. (arXiv:2310.07321v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07321
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#24503;&#35821;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#65292;&#21457;&#29616;&#23558;&#25968;&#25454;&#22810;&#26679;&#24615;&#32622;&#20110;&#25968;&#25454;&#36136;&#37327;&#20043;&#19978;&#30340;&#20132;&#21449;&#39046;&#22495;&#25968;&#25454;&#38598;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#36229;&#36807;&#20102;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35201;&#20040;&#22312;&#36890;&#29992;&#32593;&#32476;&#25235;&#21462;&#25968;&#25454;&#19978;&#35757;&#32451;&#65292;&#35201;&#20040;&#22312;&#29305;&#23450;&#39046;&#22495;&#30340;&#25968;&#25454;&#19978;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#36817;&#25104;&#21151;&#31361;&#26174;&#20102;&#20132;&#21449;&#39046;&#22495;&#25968;&#25454;&#38598;&#30340;&#22909;&#22788;&#12290;&#20026;&#20102;&#32771;&#23519;&#25968;&#25454;&#22810;&#26679;&#24615;&#39640;&#20110;&#36136;&#37327;&#30340;&#37325;&#35201;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#20116;&#20010;&#39046;&#22495;&#25991;&#26412;&#30340;&#24503;&#35821;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#19968;&#20010;&#26088;&#22312;&#21253;&#21547;&#39640;&#36136;&#37327;&#25968;&#25454;&#30340;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#22312;&#36825;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#21442;&#25968;&#33539;&#22260;&#20174;122M&#21040;750M&#30340;&#19968;&#31995;&#21015;&#27169;&#22411;&#65292;&#25105;&#20204;&#23545;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#20132;&#21449;&#39046;&#22495;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#27169;&#22411;&#20248;&#20110;&#20165;&#20351;&#29992;&#36136;&#37327;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#22312;&#20808;&#21069;&#26368;&#20808;&#36827;&#32467;&#26524;&#19978;&#25552;&#20986;&#20102;&#39640;&#36798;4.45%&#30340;&#25913;&#36827;&#12290;&#36825;&#20123;&#27169;&#22411;&#21487;&#22312;https://huggingface.co/ikim-uk-essen&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditionally, large language models have been either trained on general web crawls or domain-specific data. However, recent successes of generative large language models, have shed light on the benefits of cross-domain datasets. To examine the significance of prioritizing data diversity over quality, we present a German dataset comprising texts from five domains, along with another dataset aimed at containing high-quality data. Through training a series of models ranging between 122M and 750M parameters on both datasets, we conduct a comprehensive benchmark on multiple downstream tasks. Our findings demonstrate that the models trained on the cross-domain dataset outperform those trained on quality data alone, leading to improvements up to $4.45\%$ over the previous state-of-the-art. The models are available at https://huggingface.co/ikim-uk-essen
&lt;/p&gt;</description></item><item><title>WiGenAI&#36890;&#36807;&#24341;&#20837;&#25193;&#25955;&#27169;&#22411;&#65292;&#23558;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20110;&#26080;&#32447;&#36890;&#20449;&#31995;&#32479;&#20013;&#65292;&#20026;&#30740;&#31350;&#22880;&#23450;&#22522;&#30784;&#12290;&#36825;&#31687;&#25991;&#31456;&#20171;&#32461;&#20102;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#29983;&#25104;&#27169;&#22411;&#30340;&#26368;&#26032;&#33539;&#24335;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#22312;&#26080;&#32447;&#36890;&#20449;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#20004;&#20010;&#26696;&#20363;&#30740;&#31350;&#23637;&#31034;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#24320;&#21457;&#38887;&#24615;&#30340;AI&#26412;&#22320;&#36890;&#20449;&#31995;&#32479;&#20013;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.07312</link><description>&lt;p&gt;
WiGenAI: &#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#26080;&#32447;&#21644;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#20132;&#32455;
&lt;/p&gt;
&lt;p&gt;
WiGenAI: The Symphony of Wireless and Generative AI via Diffusion Models. (arXiv:2310.07312v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07312
&lt;/p&gt;
&lt;p&gt;
WiGenAI&#36890;&#36807;&#24341;&#20837;&#25193;&#25955;&#27169;&#22411;&#65292;&#23558;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20110;&#26080;&#32447;&#36890;&#20449;&#31995;&#32479;&#20013;&#65292;&#20026;&#30740;&#31350;&#22880;&#23450;&#22522;&#30784;&#12290;&#36825;&#31687;&#25991;&#31456;&#20171;&#32461;&#20102;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#29983;&#25104;&#27169;&#22411;&#30340;&#26368;&#26032;&#33539;&#24335;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#22312;&#26080;&#32447;&#36890;&#20449;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#20004;&#20010;&#26696;&#20363;&#30740;&#31350;&#23637;&#31034;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#24320;&#21457;&#38887;&#24615;&#30340;AI&#26412;&#22320;&#36890;&#20449;&#31995;&#32479;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21019;&#26032;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#22914;GPT-3&#21644;&#31283;&#23450;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#24050;&#32463;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#23454;&#29616;&#20102;&#33539;&#24335;&#36716;&#21464;&#65292;&#21521;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#21457;&#23637;&#12290;&#20174;&#25968;&#25454;&#36890;&#20449;&#21644;&#32593;&#32476;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#39044;&#35745;&#23558;&#24191;&#27867;&#24212;&#29992;&#20110;&#26410;&#26469;&#26080;&#32447;&#36890;&#20449;&#31995;&#32479;&#30340;&#26032;&#19968;&#20195;&#20013;&#65292;&#24378;&#35843;&#20102;&#22312;&#26032;&#20852;&#36890;&#20449;&#22330;&#26223;&#20013;&#38656;&#35201;&#26032;&#39062;&#30340;AI&#26412;&#22320;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#20171;&#32461;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#26080;&#32447;&#36890;&#20449;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#65292;&#20026;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#22880;&#23450;&#22522;&#30784;&#12290;&#20171;&#32461;&#20102;&#25193;&#25955;&#22411;&#29983;&#25104;&#27169;&#22411;&#20316;&#20026;&#29983;&#25104;&#27169;&#22411;&#30340;&#26368;&#26032;&#33539;&#24335;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#22312;&#26080;&#32447;&#36890;&#20449;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#12290;&#36824;&#25552;&#20379;&#20102;&#20004;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#24320;&#21457;&#20855;&#26377;&#38887;&#24615;&#30340;AI&#26412;&#22320;&#36890;&#20449;&#31995;&#32479;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#23637;&#31034;&#20854;&#22312;&#29983;&#25104;&#27169;&#22411;&#30340;&#24212;&#29992;&#20013;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Innovative foundation models, such as GPT-3 and stable diffusion models, have made a paradigm shift in the realm of artificial intelligence (AI) towards generative AI-based systems. In unison, from data communication and networking perspective, AI and machine learning (AI/ML) algorithms are envisioned to be pervasively incorporated into the future generations of wireless communications systems, highlighting the need for novel AI-native solutions for the emergent communication scenarios. In this article, we outline the applications of generative AI in wireless communication systems to lay the foundations for research in this field. Diffusion-based generative models, as the new state-of-the-art paradigm of generative models, are introduced, and their applications in wireless communication systems are discussed. Two case studies are also presented to showcase how diffusion models can be exploited for the development of resilient AI-native communication systems. Specifically, we propose de
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21483;&#20570;RobustGEC&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#31995;&#32479;&#30340;&#19978;&#19979;&#25991;&#40065;&#26834;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#31995;&#32479;&#20173;&#28982;&#26080;&#27861;&#24456;&#22909;&#22320;&#24212;&#23545;&#19978;&#19979;&#25991;&#25200;&#21160;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.07299</link><description>&lt;p&gt;
RobustGEC: &#40065;&#26834;&#30340;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#31995;&#32479;&#25269;&#25239;&#24494;&#23567;&#30340;&#19978;&#19979;&#25991;&#25200;&#21160;
&lt;/p&gt;
&lt;p&gt;
RobustGEC: Robust Grammatical Error Correction Against Subtle Context Perturbation. (arXiv:2310.07299v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07299
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21483;&#20570;RobustGEC&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#31995;&#32479;&#30340;&#19978;&#19979;&#25991;&#40065;&#26834;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#31995;&#32479;&#20173;&#28982;&#26080;&#27861;&#24456;&#22909;&#22320;&#24212;&#23545;&#19978;&#19979;&#25991;&#25200;&#21160;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#65288;GEC&#65289;&#31995;&#32479;&#22312;&#24110;&#21161;&#20154;&#20204;&#26085;&#24120;&#20889;&#20316;&#20219;&#21153;&#20013;&#36215;&#21040;&#20102;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#29992;&#25143;&#26377;&#26102;&#21487;&#33021;&#20250;&#36935;&#21040;&#19968;&#20123;&#21021;&#22987;&#34920;&#29616;&#33391;&#22909;&#20294;&#36935;&#21040;&#36755;&#20837;&#24494;&#23567;&#20462;&#25913;&#26102;&#26080;&#27861;&#20462;&#27491;&#38169;&#35823;&#30340;GEC&#31995;&#32479;&#12290;&#20026;&#20102;&#30830;&#20445;&#29702;&#24819;&#30340;&#29992;&#25143;&#20307;&#39564;&#65292;&#21487;&#38752;&#30340;GEC&#31995;&#32479;&#24212;&#33021;&#22815;&#22312;&#36935;&#21040;&#26080;&#20851;&#19978;&#19979;&#25991;&#25200;&#21160;&#26102;&#25552;&#20379;&#19968;&#33268;&#19988;&#20934;&#30830;&#30340;&#24314;&#35758;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#19978;&#19979;&#25991;&#40065;&#26834;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;RobustGEC&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;GEC&#31995;&#32479;&#19978;&#19979;&#25991;&#40065;&#26834;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;RobustGEC&#21253;&#21547;5,000&#20010;GEC&#26696;&#20363;&#65292;&#27599;&#20010;&#26696;&#20363;&#30001;&#19968;&#21477;&#21407;&#22987;&#30340;&#38169;&#35823;-&#20462;&#27491;&#21477;&#23376;&#23545;&#21644;&#30001;&#20154;&#24037;&#26631;&#27880;&#32773;&#31934;&#24515;&#35774;&#35745;&#30340;&#20116;&#20010;&#21464;&#20307;&#32452;&#25104;&#12290;&#21033;&#29992;RobustGEC&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;GEC&#31995;&#32479;&#22312;&#38754;&#23545;&#19978;&#19979;&#25991;&#25200;&#21160;&#26102;&#20173;&#28982;&#32570;&#20047;&#36275;&#22815;&#30340;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Grammatical Error Correction (GEC) systems play a vital role in assisting people with their daily writing tasks. However, users may sometimes come across a GEC system that initially performs well but fails to correct errors when the inputs are slightly modified. To ensure an ideal user experience, a reliable GEC system should have the ability to provide consistent and accurate suggestions when encountering irrelevant context perturbations, which we refer to as context robustness. In this paper, we introduce RobustGEC, a benchmark designed to evaluate the context robustness of GEC systems. RobustGEC comprises 5,000 GEC cases, each with one original error-correct sentence pair and five variants carefully devised by human annotators. Utilizing RobustGEC, we reveal that state-of-the-art GEC systems still lack sufficient robustness against context perturbations. In addition, we propose a simple yet effective method for remitting this issue.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#39318;&#27425;&#20840;&#38754;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#25991;&#26412;&#20013;&#25512;&#26029;&#20010;&#20154;&#23646;&#24615;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#24403;&#21069;&#30340;&#27169;&#22411;&#21487;&#20197;&#20197;&#36739;&#20302;&#30340;&#25104;&#26412;&#21644;&#26102;&#38388;&#27604;&#20363;&#65292;&#20934;&#30830;&#22320;&#25512;&#26029;&#20986;&#22810;&#31181;&#20010;&#20154;&#23646;&#24615;&#65292;&#36825;&#24341;&#21457;&#20102;&#38544;&#31169;&#27844;&#38706;&#30340;&#26032;&#23041;&#32961;&#12290;</title><link>http://arxiv.org/abs/2310.07298</link><description>&lt;p&gt;
&#36229;&#36234;&#35760;&#24518;&#65306;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25512;&#29702;&#26469;&#20405;&#29359;&#38544;&#31169;
&lt;/p&gt;
&lt;p&gt;
Beyond Memorization: Violating Privacy Via Inference with Large Language Models. (arXiv:2310.07298v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07298
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#39318;&#27425;&#20840;&#38754;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#25991;&#26412;&#20013;&#25512;&#26029;&#20010;&#20154;&#23646;&#24615;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#24403;&#21069;&#30340;&#27169;&#22411;&#21487;&#20197;&#20197;&#36739;&#20302;&#30340;&#25104;&#26412;&#21644;&#26102;&#38388;&#27604;&#20363;&#65292;&#20934;&#30830;&#22320;&#25512;&#26029;&#20986;&#22810;&#31181;&#20010;&#20154;&#23646;&#24615;&#65292;&#36825;&#24341;&#21457;&#20102;&#38544;&#31169;&#27844;&#38706;&#30340;&#26032;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#38544;&#31169;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#25552;&#21462;&#35760;&#24518;&#35757;&#32451;&#25968;&#25454;&#30340;&#38382;&#39064;&#19978;&#12290;&#21516;&#26102;&#65292;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#24050;&#22823;&#24133;&#22686;&#24378;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#21363;&#24403;&#21069;&#30340;LLMs&#26159;&#21542;&#33021;&#22815;&#36890;&#36807;&#20174;&#25512;&#29702;&#26102;&#32473;&#20986;&#30340;&#25991;&#26412;&#20013;&#25512;&#26029;&#20010;&#20154;&#23646;&#24615;&#26469;&#20405;&#29359;&#20010;&#20154;&#38544;&#31169;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#23545;&#39044;&#35757;&#32451;LLMs&#20174;&#25991;&#26412;&#20013;&#25512;&#26029;&#20010;&#20154;&#23646;&#24615;&#30340;&#33021;&#21147;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#30495;&#23454;Reddit&#20010;&#20154;&#36164;&#26009;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#19988;&#26174;&#31034;&#24403;&#21069;&#30340;LLMs&#21487;&#20197;&#25512;&#26029;&#20986;&#21508;&#31181;&#21508;&#26679;&#30340;&#20010;&#20154;&#23646;&#24615;&#65288;&#20363;&#22914;&#65292;&#20301;&#32622;&#12289;&#25910;&#20837;&#12289;&#24615;&#21035;&#65289;&#65292;&#22312;&#25104;&#26412;&#65288;100&#20493;&#65289;&#21644;&#26102;&#38388;&#65288;240&#20493;&#65289;&#19978;&#20165;&#38656;&#20154;&#31867;&#30340;&#19968;&#23567;&#37096;&#20998;&#65292;&#36798;&#21040;&#20102;&#26368;&#39640;1&#30340;&#20934;&#30830;&#29575;&#36798;&#21040;85&#65285;&#65292;&#26368;&#39640;3&#30340;&#20934;&#30830;&#29575;&#36798;&#21040;95.8&#65285;&#12290;&#38543;&#30528;&#20154;&#20204;&#36234;&#26469;&#36234;&#22810;&#22320;&#19982;&#30001;LLM&#39537;&#21160;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#29983;&#27963;&#30340;&#21508;&#20010;&#26041;&#38754;&#36827;&#34892;&#20114;&#21160;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#20405;&#29359;&#38544;&#31169;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#36890;&#36807;&#20284;&#20046;&#26080;&#20851;&#30340;&#23545;&#35805;&#35797;&#22270;&#25552;&#21462;&#20010;&#20154;&#20449;&#24687;&#30340;&#26032;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current privacy research on large language models (LLMs) primarily focuses on the issue of extracting memorized training data. At the same time, models' inference capabilities have increased drastically. This raises the key question of whether current LLMs could violate individuals' privacy by inferring personal attributes from text given at inference time. In this work, we present the first comprehensive study on the capabilities of pretrained LLMs to infer personal attributes from text. We construct a dataset consisting of real Reddit profiles, and show that current LLMs can infer a wide range of personal attributes (e.g., location, income, sex), achieving up to $85\%$ top-1 and $95.8\%$ top-3 accuracy at a fraction of the cost ($100\times$) and time ($240\times$) required by humans. As people increasingly interact with LLM-powered chatbots across all aspects of life, we also explore the emerging threat of privacy-invasive chatbots trying to extract personal information through seemi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#23588;&#20854;&#26159;BioBERT&#65289;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#38024;&#23545;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#24494;&#35843;&#26041;&#27861;&#12290;&#30740;&#31350;&#31361;&#20986;&#20102;BioBERT&#23545;&#20110;&#35299;&#20915;&#19982;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#25366;&#25496;&#30456;&#20851;&#20219;&#21153;&#30340;&#29305;&#23450;&#35201;&#27714;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.07282</link><description>&lt;p&gt;
&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20013;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20998;&#26512;&#65306;BioBERT &#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Analysis on Large Language Models in Healthcare: A Case Study of BioBERT. (arXiv:2310.07282v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07282
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#23588;&#20854;&#26159;BioBERT&#65289;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#38024;&#23545;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#24494;&#35843;&#26041;&#27861;&#12290;&#30740;&#31350;&#31361;&#20986;&#20102;BioBERT&#23545;&#20110;&#35299;&#20915;&#19982;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#25366;&#25496;&#30456;&#20851;&#20219;&#21153;&#30340;&#29305;&#23450;&#35201;&#27714;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#23588;&#20854;&#26159;BioBERT&#65289;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#30740;&#31350;&#12290;&#23427;&#39318;&#20808;&#24443;&#24213;&#26816;&#26597;&#20102;&#20808;&#21069;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20013;&#24212;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#26041;&#27861;&#30340;&#24773;&#20917;&#65292;&#25581;&#31034;&#20102;&#36825;&#20123;&#26041;&#27861;&#38754;&#20020;&#30340;&#23616;&#38480;&#21644;&#25361;&#25112;&#12290;&#38543;&#21518;&#65292;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;BioBERT&#25972;&#21512;&#21040;&#21307;&#30103;&#20445;&#20581;&#24212;&#29992;&#20013;&#30340;&#36335;&#24452;&#65292;&#31361;&#20986;&#20854;&#36866;&#29992;&#20110;&#35299;&#20915;&#19982;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#25366;&#25496;&#30456;&#20851;&#20219;&#21153;&#30340;&#29305;&#23450;&#35201;&#27714;&#12290;&#35813;&#20998;&#26512;&#27010;&#36848;&#20102;&#29992;&#20110;&#38024;&#23545;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#29420;&#29305;&#38656;&#27714;&#24494;&#35843;BioBERT&#30340;&#31995;&#32479;&#26041;&#27861;&#35770;&#12290;&#36825;&#20010;&#26041;&#27861;&#21253;&#25324;&#20174;&#21508;&#31181;&#21307;&#30103;&#20445;&#20581;&#26469;&#28304;&#25910;&#38598;&#25968;&#25454;&#65292;&#20026;&#35782;&#21035;&#21307;&#30103;&#23454;&#20307;&#21644;&#23545;&#20854;&#36827;&#34892;&#20998;&#31867;&#31561;&#20219;&#21153;&#36827;&#34892;&#25968;&#25454;&#27880;&#37322;&#65292;&#24182;&#24212;&#29992;&#19987;&#38376;&#38024;&#23545;&#22788;&#29702;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#20013;&#22797;&#26434;&#24615;&#30340;&#39044;&#22788;&#29702;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper conducts a comprehensive investigation into applying large language models, particularly on BioBERT, in healthcare. It begins with thoroughly examining previous natural language processing (NLP) approaches in healthcare, shedding light on the limitations and challenges these methods face. Following that, this research explores the path that led to the incorporation of BioBERT into healthcare applications, highlighting its suitability for addressing the specific requirements of tasks related to biomedical text mining. The analysis outlines a systematic methodology for fine-tuning BioBERT to meet the unique needs of the healthcare domain. This approach includes various components, including the gathering of data from a wide range of healthcare sources, data annotation for tasks like identifying medical entities and categorizing them, and the application of specialized preprocessing techniques tailored to handle the complexities found in biomedical texts. Additionally, the pape
&lt;/p&gt;</description></item><item><title>BioT5&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#22312;&#29983;&#29289;&#23398;&#20013;&#21033;&#29992;&#21270;&#23398;&#30693;&#35782;&#21644;&#33258;&#28982;&#35821;&#35328;&#20851;&#32852;&#20016;&#23500;&#20102;&#36328;&#27169;&#24577;&#25972;&#21512;&#65292;&#36890;&#36807;&#40065;&#26834;&#30340;&#20998;&#23376;&#34920;&#31034;&#21644;&#19978;&#19979;&#25991;&#30693;&#35782;&#25552;&#21462;&#65292;&#23454;&#29616;&#20102;&#26356;&#26377;&#25928;&#30340;&#20449;&#24687;&#21033;&#29992;&#65292;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.07276</link><description>&lt;p&gt;
BioT5&#65306;&#22312;&#29983;&#29289;&#23398;&#20013;&#21033;&#29992;&#21270;&#23398;&#30693;&#35782;&#21644;&#33258;&#28982;&#35821;&#35328;&#20851;&#32852;&#20016;&#23500;&#36328;&#27169;&#24577;&#25972;&#21512;
&lt;/p&gt;
&lt;p&gt;
BioT5: Enriching Cross-modal Integration in Biology with Chemical Knowledge and Natural Language Associations. (arXiv:2310.07276v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07276
&lt;/p&gt;
&lt;p&gt;
BioT5&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#22312;&#29983;&#29289;&#23398;&#20013;&#21033;&#29992;&#21270;&#23398;&#30693;&#35782;&#21644;&#33258;&#28982;&#35821;&#35328;&#20851;&#32852;&#20016;&#23500;&#20102;&#36328;&#27169;&#24577;&#25972;&#21512;&#65292;&#36890;&#36807;&#40065;&#26834;&#30340;&#20998;&#23376;&#34920;&#31034;&#21644;&#19978;&#19979;&#25991;&#30693;&#35782;&#25552;&#21462;&#65292;&#23454;&#29616;&#20102;&#26356;&#26377;&#25928;&#30340;&#20449;&#24687;&#21033;&#29992;&#65292;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#29983;&#29289;&#30740;&#31350;&#39046;&#22495;&#30340;&#36827;&#23637;&#21033;&#29992;&#20998;&#23376;&#12289;&#34507;&#30333;&#36136;&#21644;&#33258;&#28982;&#35821;&#35328;&#30340;&#25972;&#21512;&#26469;&#22686;&#24378;&#33647;&#29289;&#21457;&#29616;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#27169;&#22411;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#22914;&#29983;&#25104;&#26080;&#25928;&#30340;&#20998;&#23376;SMILES&#12289;&#23545;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#21033;&#29992;&#19981;&#36275;&#20197;&#21450;&#23545;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#30693;&#35782;&#30340;&#31561;&#37327;&#22788;&#29702;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;BioT5&#65292;&#23427;&#36890;&#36807;&#21270;&#23398;&#30693;&#35782;&#21644;&#33258;&#28982;&#35821;&#35328;&#20851;&#32852;&#20016;&#23500;&#20102;&#29983;&#29289;&#23398;&#20013;&#30340;&#36328;&#27169;&#24577;&#25972;&#21512;&#12290;BioT5&#21033;&#29992;SELFIES&#36827;&#34892;100%&#40065;&#26834;&#30340;&#20998;&#23376;&#34920;&#31034;&#65292;&#24182;&#20174;&#38750;&#32467;&#26500;&#21270;&#30340;&#29983;&#29289;&#25991;&#29486;&#20013;&#25552;&#21462;&#29983;&#29289;&#23454;&#20307;&#21608;&#22260;&#19978;&#19979;&#25991;&#30340;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;BioT5&#21306;&#20998;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#30693;&#35782;&#65292;&#20174;&#32780;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#20449;&#24687;&#12290;&#22312;&#24494;&#35843;&#21518;&#65292;BioT5&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#34920;&#26126;&#20854;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in biological research leverage the integration of molecules, proteins, and natural language to enhance drug discovery. However, current models exhibit several limitations, such as the generation of invalid molecular SMILES, underutilization of contextual information, and equal treatment of structured and unstructured knowledge. To address these issues, we propose $\mathbf{BioT5}$, a comprehensive pre-training framework that enriches cross-modal integration in biology with chemical knowledge and natural language associations. $\mathbf{BioT5}$ utilizes SELFIES for $100%$ robust molecular representations and extracts knowledge from the surrounding context of bio-entities in unstructured biological literature. Furthermore, $\mathbf{BioT5}$ distinguishes between structured and unstructured knowledge, leading to more effective utilization of information. After fine-tuning, BioT5 shows superior performance across a wide range of tasks, demonstrating its strong capability 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#26426;&#22120;&#20154;&#21160;&#20316;&#32416;&#27491;&#35268;&#21010;&#31995;&#32479;&#65292;&#36890;&#36807;&#22788;&#29702;&#29983;&#25104;&#35745;&#21010;&#20013;&#30340;&#29289;&#29702;&#22522;&#30784;&#12289;&#36923;&#36753;&#21644;&#35821;&#20041;&#38169;&#35823;&#30340;&#20877;&#35268;&#21010;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#30340;&#20219;&#21153;&#21644;&#21160;&#20316;&#35268;&#21010;&#12290;&#36890;&#36807;&#20223;&#30495;&#21644;&#23454;&#38469;&#22330;&#26223;&#30340;&#39564;&#35777;&#65292;&#35777;&#26126;&#20102;&#35813;&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.07263</link><description>&lt;p&gt;
CoPAL:&#20855;&#26377;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#26426;&#22120;&#20154;&#21160;&#20316;&#32416;&#27491;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
CoPAL: Corrective Planning of Robot Actions with Large Language Models. (arXiv:2310.07263v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07263
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#26426;&#22120;&#20154;&#21160;&#20316;&#32416;&#27491;&#35268;&#21010;&#31995;&#32479;&#65292;&#36890;&#36807;&#22788;&#29702;&#29983;&#25104;&#35745;&#21010;&#20013;&#30340;&#29289;&#29702;&#22522;&#30784;&#12289;&#36923;&#36753;&#21644;&#35821;&#20041;&#38169;&#35823;&#30340;&#20877;&#35268;&#21010;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#30340;&#20219;&#21153;&#21644;&#21160;&#20316;&#35268;&#21010;&#12290;&#36890;&#36807;&#20223;&#30495;&#21644;&#23454;&#38469;&#22330;&#26223;&#30340;&#39564;&#35777;&#65292;&#35777;&#26126;&#20102;&#35813;&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#23454;&#29616;&#23436;&#20840;&#33258;&#20027;&#30340;&#26426;&#22120;&#20154;&#31995;&#32479;&#33021;&#22815;&#25509;&#31649;&#20154;&#31867;&#20256;&#32479;&#25191;&#34892;&#30340;&#20219;&#21153;&#65292;&#24320;&#25918;&#19990;&#30028;&#29615;&#22659;&#30340;&#22797;&#26434;&#24615;&#25552;&#20986;&#20102;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#22312;&#36825;&#19968;&#32972;&#26223;&#19979;&#65292;&#26412;&#30740;&#31350;&#20026;&#24212;&#29992;&#20110;&#26426;&#22120;&#20154;&#20219;&#21153;&#21644;&#21160;&#20316;&#35268;&#21010;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#39046;&#22495;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#26550;&#26500;&#65292;&#21327;&#35843;&#22810;&#20010;&#35748;&#30693;&#23618;&#27425;&#20043;&#38388;&#30340;&#26080;&#32541;&#20132;&#20114;&#65292;&#21253;&#25324;&#25512;&#29702;&#12289;&#35268;&#21010;&#21644;&#21160;&#20316;&#29983;&#25104;&#12290;&#20854;&#26680;&#24515;&#26159;&#19968;&#31181;&#22788;&#29702;&#29983;&#25104;&#30340;&#35745;&#21010;&#20013;&#30340;&#29289;&#29702;&#22522;&#30784;&#12289;&#36923;&#36753;&#21644;&#35821;&#20041;&#38169;&#35823;&#30340;&#26032;&#22411;&#20877;&#35268;&#21010;&#31574;&#30053;&#12290;&#36890;&#36807;&#22312;&#20223;&#30495;&#29615;&#22659;&#21644;&#20004;&#20010;&#22797;&#26434;&#30340;&#23454;&#38469;&#22330;&#26223;&#65288;&#26041;&#22359;&#19990;&#30028;&#12289;&#37202;&#21543;&#21644;&#27604;&#33832;&#21046;&#20316;&#65289;&#20013;&#36827;&#34892;&#23454;&#35777;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#21453;&#39304;&#26550;&#26500;&#30340;&#26377;&#25928;&#24615;&#65292;&#23588;&#20854;&#26159;&#23545;&#21487;&#25191;&#34892;&#24615;&#12289;&#27491;&#30830;&#24615;&#21644;&#26102;&#38388;&#22797;&#26434;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the pursuit of fully autonomous robotic systems capable of taking over tasks traditionally performed by humans, the complexity of open-world environments poses a considerable challenge. Addressing this imperative, this study contributes to the field of Large Language Models (LLMs) applied to task and motion planning for robots. We propose a system architecture that orchestrates a seamless interplay between multiple cognitive levels, encompassing reasoning, planning, and motion generation. At its core lies a novel replanning strategy that handles physically grounded, logical, and semantic errors in the generated plans. We demonstrate the efficacy of the proposed feedback architecture, particularly its impact on executability, correctness, and time complexity via empirical evaluation in the context of a simulation and two intricate real-world scenarios: blocks world, barman and pizza preparation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#36319;&#36394;&#21644;&#25512;&#29702;&#31574;&#30053;&#65292;&#32467;&#21512;&#25991;&#26412;&#32534;&#30721;&#22120;&#21644;&#35270;&#35273;&#32534;&#30721;&#22120;&#20197;&#29983;&#25104;&#20934;&#30830;&#30340;&#21709;&#24212;&#65292;&#35299;&#20915;&#20102;&#35270;&#39057;&#23545;&#35805;&#20013;&#36880;&#27493;&#29702;&#35299;&#23545;&#35805;&#21382;&#21490;&#21644;&#21560;&#25910;&#35270;&#39057;&#20449;&#24687;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.07259</link><description>&lt;p&gt;
&#25581;&#31034;&#38544;&#34255;&#30340;&#32852;&#31995;&#65306;&#29992;&#20110;&#35270;&#39057;&#23545;&#35805;&#30340;&#36845;&#20195;&#36319;&#36394;&#21644;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Uncovering Hidden Connections: Iterative Tracking and Reasoning for Video-grounded Dialog. (arXiv:2310.07259v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07259
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#36319;&#36394;&#21644;&#25512;&#29702;&#31574;&#30053;&#65292;&#32467;&#21512;&#25991;&#26412;&#32534;&#30721;&#22120;&#21644;&#35270;&#35273;&#32534;&#30721;&#22120;&#20197;&#29983;&#25104;&#20934;&#30830;&#30340;&#21709;&#24212;&#65292;&#35299;&#20915;&#20102;&#35270;&#39057;&#23545;&#35805;&#20013;&#36880;&#27493;&#29702;&#35299;&#23545;&#35805;&#21382;&#21490;&#21644;&#21560;&#25910;&#35270;&#39057;&#20449;&#24687;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#20256;&#32479;&#30340;&#35270;&#35273;&#38382;&#31572;&#30456;&#27604;&#65292;&#35270;&#39057;&#23545;&#35805;&#38656;&#35201;&#23545;&#23545;&#35805;&#21382;&#21490;&#21644;&#35270;&#39057;&#20869;&#23481;&#36827;&#34892;&#28145;&#20837;&#29702;&#35299;&#65292;&#20197;&#29983;&#25104;&#20934;&#30830;&#30340;&#21709;&#24212;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#20196;&#20154;&#31216;&#36190;&#30340;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#24120;&#24120;&#38754;&#20020;&#36880;&#27493;&#29702;&#35299;&#22797;&#26434;&#30340;&#23545;&#35805;&#21382;&#21490;&#21644;&#21560;&#25910;&#35270;&#39057;&#20449;&#24687;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#36319;&#36394;&#21644;&#25512;&#29702;&#31574;&#30053;&#65292;&#23558;&#25991;&#26412;&#32534;&#30721;&#22120;&#12289;&#35270;&#35273;&#32534;&#30721;&#22120;&#21644;&#29983;&#25104;&#22120;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#20197;&#36335;&#24452;&#36319;&#36394;&#21644;&#32858;&#21512;&#26426;&#21046;&#20026;&#26680;&#24515;&#65292;&#33021;&#22815;&#20174;&#23545;&#35805;&#21382;&#21490;&#20013;&#33719;&#21462;&#37325;&#35201;&#30340;&#32454;&#24494;&#24046;&#21035;&#65292;&#20197;&#35299;&#37322;&#25152;&#25552;&#20986;&#30340;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#21033;&#29992;&#36845;&#20195;&#25512;&#29702;&#32593;&#32476;&#65292;&#31934;&#24515;&#35774;&#35745;&#20197;&#20174;&#35270;&#39057;&#20013;&#25552;&#21462;&#21644;&#24378;&#35843;&#20851;&#38190;&#35270;&#35273;&#26631;&#35760;&#65292;&#22686;&#24378;&#23545;&#35270;&#35273;&#29702;&#35299;&#30340;&#28145;&#24230;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;GPT-&#27169;&#22411;&#23558;&#36825;&#20123;&#20016;&#23500;&#30340;&#20449;&#24687;&#32508;&#21512;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
In contrast to conventional visual question answering, video-grounded dialog necessitates a profound understanding of both dialog history and video content for accurate response generation. Despite commendable strides made by existing methodologies, they often grapple with the challenges of incrementally understanding intricate dialog histories and assimilating video information. In response to this gap, we present an iterative tracking and reasoning strategy that amalgamates a textual encoder, a visual encoder, and a generator. At its core, our textual encoder is fortified with a path tracking and aggregation mechanism, adept at gleaning nuances from dialog history that are pivotal to deciphering the posed questions. Concurrently, our visual encoder harnesses an iterative reasoning network, meticulously crafted to distill and emphasize critical visual markers from videos, enhancing the depth of visual comprehension. Culminating this enriched information, we employ the pre-trained GPT-
&lt;/p&gt;</description></item><item><title>ADMEOOD&#26159;&#19968;&#20010;&#35774;&#35745;&#29992;&#20110;&#33647;&#29289;&#23646;&#24615;&#39044;&#27979;&#30340;&#36229;&#20998;&#24067;&#22522;&#20934;&#65292;&#21253;&#21547;27&#20010;&#33647;&#29289;&#23646;&#24615;&#21644;&#20004;&#31181;&#25968;&#25454;&#36716;&#31227;&#65288;&#22122;&#22768;&#36716;&#31227;&#21644;&#27010;&#24565;&#20914;&#31361;&#28418;&#31227;&#65289;&#12290;</title><link>http://arxiv.org/abs/2310.07253</link><description>&lt;p&gt;
ADMEOOD: &#33647;&#29289;&#23646;&#24615;&#39044;&#27979;&#30340;&#36229;&#20998;&#24067;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
ADMEOOD: Out-of-Distribution Benchmark for Drug Property Prediction. (arXiv:2310.07253v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07253
&lt;/p&gt;
&lt;p&gt;
ADMEOOD&#26159;&#19968;&#20010;&#35774;&#35745;&#29992;&#20110;&#33647;&#29289;&#23646;&#24615;&#39044;&#27979;&#30340;&#36229;&#20998;&#24067;&#22522;&#20934;&#65292;&#21253;&#21547;27&#20010;&#33647;&#29289;&#23646;&#24615;&#21644;&#20004;&#31181;&#25968;&#25454;&#36716;&#31227;&#65288;&#22122;&#22768;&#36716;&#31227;&#21644;&#27010;&#24565;&#20914;&#31361;&#28418;&#31227;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33719;&#24471;&#20934;&#30830;&#26377;&#25928;&#30340;&#33647;&#29289;&#20998;&#23376;&#20449;&#24687;&#26159;&#19968;&#39033;&#20851;&#38190;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36807;&#21435;100&#24180;&#26469;&#65292;&#21270;&#23398;&#30693;&#35782;&#21644;&#20449;&#24687;&#26469;&#33258;&#19981;&#21516;&#22320;&#21306;&#12289;&#23454;&#39564;&#23460;&#21644;&#23454;&#39564;&#30446;&#30340;&#30340;&#31215;&#32047;&#12290;&#22312;&#36229;&#20998;&#24067;&#65288;OOD&#65289;&#38382;&#39064;&#20013;&#65292;&#22122;&#38899;&#21644;&#19981;&#19968;&#33268;&#24615;&#24456;&#23569;&#34987;&#25506;&#32034;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#24369;&#40065;&#26834;&#24615;&#21644;&#19981;&#28385;&#24847;&#30340;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;ADMEOOD, &#19968;&#20010;&#29305;&#21035;&#35774;&#35745;&#29992;&#20110;&#33647;&#29289;&#23646;&#24615;&#39044;&#27979;&#30340;&#31995;&#32479;&#36229;&#20998;&#24067;&#25968;&#25454;&#38598;&#31574;&#21010;&#32773;&#21644;&#22522;&#20934;&#12290;ADMEOOD&#20174;Chembl&#21644;&#30456;&#20851;&#25991;&#29486;&#20013;&#33719;&#24471;&#20102;27&#20010;&#33647;&#29289;&#23646;&#24615;&#12290;&#21478;&#22806;&#65292;&#23427;&#36824;&#21253;&#25324;&#20004;&#31181;&#36229;&#20998;&#24067;&#25968;&#25454;&#36716;&#31227;&#65306;&#22122;&#22768;&#36716;&#31227;&#21644;&#27010;&#24565;&#20914;&#31361;&#28418;&#31227;&#65288;CCD&#65289;&#12290;&#22122;&#22768;&#36716;&#31227;&#36890;&#36807;&#23558;&#29615;&#22659;&#20998;&#31867;&#20026;&#19981;&#21516;&#30340;&#32622;&#20449;&#27700;&#24179;&#26469;&#21709;&#24212;&#22122;&#22768;&#27700;&#24179;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;CCD&#25551;&#36848;&#20102;&#21407;&#22987;&#25968;&#25454;&#20013;&#26631;&#31614;&#19981;&#19968;&#33268;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Obtaining accurate and valid information for drug molecules is a crucial and challenging task. However, chemical knowledge and information have been accumulated over the past 100 years from various regions, laboratories, and experimental purposes. Little has been explored in terms of the out-of-distribution (OOD) problem with noise and inconsistency, which may lead to weak robustness and unsatisfied performance. This study proposes a novel benchmark ADMEOOD, a systematic OOD dataset curator and benchmark specifically designed for drug property prediction. ADMEOOD obtained 27 ADME (Absorption, Distribution, Metabolism, Excretion) drug properties from Chembl and relevant literature. Additionally, it includes two kinds of OOD data shifts: Noise Shift and Concept Conflict Drift (CCD). Noise Shift responds to the noise level by categorizing the environment into different confidence levels. On the other hand, CCD describes the data which has inconsistent label among the original data. Finall
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;LLM&#20013;&#24212;&#29992;&#36890;&#29992;&#20262;&#29702;&#25512;&#29702;&#33021;&#21147;&#32780;&#38750;&#29305;&#23450;&#20262;&#29702;&#21407;&#21017;&#30340;&#26041;&#27861;&#65292;&#20197;&#22788;&#29702;&#20840;&#29699;&#33539;&#22260;&#30340;&#20215;&#20540;&#22810;&#20803;&#24615;&#12290;&#20316;&#32773;&#24320;&#21457;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;&#36947;&#24503;&#22256;&#22659;&#19982;&#19981;&#21516;&#24418;&#24335;&#30340;&#35268;&#33539;&#20262;&#29702;&#20197;&#21450;&#19981;&#21516;&#25277;&#35937;&#23618;&#27425;&#30340;&#36947;&#24503;&#21407;&#21017;&#30456;&#32467;&#21512;&#12290;&#21021;&#27493;&#23454;&#39564;&#34920;&#26126;&#65292;&#23613;&#31649;GPT-4&#20960;&#20046;&#21487;&#20197;&#23436;&#32654;&#22320;&#36827;&#34892;&#20262;&#29702;&#25512;&#29702;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#20173;&#28982;&#23384;&#22312;&#23545;&#35199;&#26041;&#21644;&#20197;&#33521;&#35821;&#20026;&#27597;&#35821;&#30340;&#31038;&#20250;&#36947;&#24503;&#20215;&#20540;&#30340;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2310.07251</link><description>&lt;p&gt;
&#22522;&#20110;&#20262;&#29702;&#25512;&#29702;&#30340;&#36947;&#24503;&#23545;&#40784;&#65306;&#20851;&#20110;&#22312;LLM&#20013;&#19978;&#19979;&#25991;&#20262;&#29702;&#25919;&#31574;&#30340;&#26696;&#20363;&#21644;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Ethical Reasoning over Moral Alignment: A Case and Framework for In-Context Ethical Policies in LLMs. (arXiv:2310.07251v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07251
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;LLM&#20013;&#24212;&#29992;&#36890;&#29992;&#20262;&#29702;&#25512;&#29702;&#33021;&#21147;&#32780;&#38750;&#29305;&#23450;&#20262;&#29702;&#21407;&#21017;&#30340;&#26041;&#27861;&#65292;&#20197;&#22788;&#29702;&#20840;&#29699;&#33539;&#22260;&#30340;&#20215;&#20540;&#22810;&#20803;&#24615;&#12290;&#20316;&#32773;&#24320;&#21457;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;&#36947;&#24503;&#22256;&#22659;&#19982;&#19981;&#21516;&#24418;&#24335;&#30340;&#35268;&#33539;&#20262;&#29702;&#20197;&#21450;&#19981;&#21516;&#25277;&#35937;&#23618;&#27425;&#30340;&#36947;&#24503;&#21407;&#21017;&#30456;&#32467;&#21512;&#12290;&#21021;&#27493;&#23454;&#39564;&#34920;&#26126;&#65292;&#23613;&#31649;GPT-4&#20960;&#20046;&#21487;&#20197;&#23436;&#32654;&#22320;&#36827;&#34892;&#20262;&#29702;&#25512;&#29702;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#20173;&#28982;&#23384;&#22312;&#23545;&#35199;&#26041;&#21644;&#20197;&#33521;&#35821;&#20026;&#27597;&#35821;&#30340;&#31038;&#20250;&#36947;&#24503;&#20215;&#20540;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#19982;&#20854;&#23545;LLM&#36827;&#34892;&#36947;&#24503;&#23545;&#40784;&#21040;&#29305;&#23450;&#30340;&#20262;&#29702;&#21407;&#21017;&#65292;&#25105;&#20204;&#24212;&#35813;&#27880;&#20837;&#36890;&#29992;&#30340;&#20262;&#29702;&#25512;&#29702;&#33021;&#21147;&#65292;&#20351;&#20854;&#33021;&#22815;&#22788;&#29702;&#20840;&#29699;&#33539;&#22260;&#30340;&#20215;&#20540;&#22810;&#20803;&#24615;&#12290;&#24403;&#25552;&#20379;&#20262;&#29702;&#25919;&#31574;&#26102;&#65292;LLM&#24212;&#35813;&#33021;&#22815;&#20570;&#20986;&#19982;&#25919;&#31574;&#19968;&#33268;&#30340;&#20262;&#29702;&#20915;&#31574;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;&#36947;&#24503;&#22256;&#22659;&#19982;&#19981;&#21516;&#24418;&#24335;&#30340;&#35268;&#33539;&#20262;&#29702;&#20197;&#21450;&#19981;&#21516;&#25277;&#35937;&#23618;&#27425;&#30340;&#36947;&#24503;&#21407;&#21017;&#30456;&#32467;&#21512;&#12290;&#23545;GPT-x&#27169;&#22411;&#30340;&#21021;&#27493;&#23454;&#39564;&#34920;&#26126;&#65292;&#34429;&#28982;GPT-4&#20960;&#20046;&#21487;&#20197;&#23436;&#32654;&#22320;&#36827;&#34892;&#20262;&#29702;&#25512;&#29702;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#20173;&#28982;&#23384;&#22312;&#23545;&#35199;&#26041;&#21644;&#20197;&#33521;&#35821;&#20026;&#27597;&#35821;&#30340;&#31038;&#20250;&#36947;&#24503;&#20215;&#20540;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this position paper, we argue that instead of morally aligning LLMs to specific set of ethical principles, we should infuse generic ethical reasoning capabilities into them so that they can handle value pluralism at a global scale. When provided with an ethical policy, an LLM should be capable of making decisions that are ethically consistent to the policy. We develop a framework that integrates moral dilemmas with moral principles pertaining to different foramlisms of normative ethics, and at different levels of abstractions. Initial experiments with GPT-x models shows that while GPT-4 is a nearly perfect ethical reasoner, the models still have bias towards the moral values of Western and English speaking societies.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20195;&#29702;&#27169;&#22411;&#29992;&#20110;&#39044;&#27979;&#32467;&#26500;&#20013;&#35010;&#32441;&#30340;&#25193;&#23637;&#65292;&#24182;&#25104;&#21151;&#22320;&#32534;&#30721;&#20102;&#19981;&#21516;&#30340;&#38543;&#26426;&#19981;&#30830;&#23450;&#24615;&#26469;&#28304;&#12290;&#35813;&#27169;&#22411;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#27169;&#22411;&#65292;&#33021;&#22815;&#29983;&#25104;&#20808;&#39564;&#20998;&#24067;&#29992;&#20110;&#36125;&#21494;&#26031;&#32467;&#26500;&#20581;&#24247;&#30417;&#27979;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2310.07241</link><description>&lt;p&gt;
&#32467;&#26500;&#20581;&#24247;&#30417;&#27979;&#24212;&#29992;&#20013;&#30340;&#38543;&#26426;&#35010;&#32441;&#25193;&#23637;&#36807;&#31243;&#30340;&#20195;&#29702;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Surrogate modeling for stochastic crack growth processes in structural health monitoring applications. (arXiv:2310.07241v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07241
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20195;&#29702;&#27169;&#22411;&#29992;&#20110;&#39044;&#27979;&#32467;&#26500;&#20013;&#35010;&#32441;&#30340;&#25193;&#23637;&#65292;&#24182;&#25104;&#21151;&#22320;&#32534;&#30721;&#20102;&#19981;&#21516;&#30340;&#38543;&#26426;&#19981;&#30830;&#23450;&#24615;&#26469;&#28304;&#12290;&#35813;&#27169;&#22411;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#27169;&#22411;&#65292;&#33021;&#22815;&#29983;&#25104;&#20808;&#39564;&#20998;&#24067;&#29992;&#20110;&#36125;&#21494;&#26031;&#32467;&#26500;&#20581;&#24247;&#30417;&#27979;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30130;&#21171;&#35010;&#32441;&#25193;&#23637;&#26159;&#37329;&#23646;&#32467;&#26500;&#20013;&#26368;&#24120;&#35265;&#30340;&#19968;&#31181;&#30772;&#22351;&#31867;&#22411;&#65292;&#23545;&#20854;&#21487;&#38752;&#24615;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#26368;&#36817;&#22312;&#32467;&#26500;&#20581;&#24247;&#30417;&#27979;&#39046;&#22495;&#30340;&#36827;&#23637;&#20419;&#20351;&#20351;&#29992;&#32467;&#26500;&#21709;&#24212;&#25968;&#25454;&#26469;&#39044;&#27979;&#19981;&#30830;&#23450;&#26465;&#20214;&#19979;&#26410;&#26469;&#30340;&#35010;&#32441;&#25193;&#23637;&#65292;&#20197;&#23454;&#29616;&#21521;&#39044;&#27979;&#24615;&#32500;&#20462;&#30340;&#36807;&#28193;&#12290;&#20934;&#30830;&#22320;&#34920;&#31034;&#38543;&#26426;&#35010;&#32441;&#25193;&#23637;&#36807;&#31243;&#20013;&#19981;&#21516;&#30340;&#19981;&#30830;&#23450;&#24615;&#26469;&#28304;&#26159;&#19968;&#39033;&#38750;&#24120;&#22256;&#38590;&#30340;&#20219;&#21153;&#12290;&#26412;&#30740;&#31350;&#22312;&#22522;&#20110;&#29289;&#29702;&#27169;&#22411;&#30340;&#38543;&#26426;&#35010;&#32441;&#25193;&#23637;&#24314;&#27169;&#30340;&#22522;&#30784;&#19978;&#36827;&#34892;&#20102;&#25506;&#32034;&#65292;&#32771;&#34385;&#20102;&#26448;&#26009;&#21644;&#36733;&#33655;&#30456;&#20851;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#26412;&#25991;&#26088;&#22312;&#26500;&#24314;&#35745;&#31639;&#25928;&#29575;&#39640;&#12289;&#27010;&#29575;&#20195;&#29702;&#27169;&#22411;&#65292;&#33021;&#22815;&#25104;&#21151;&#22320;&#32534;&#30721;&#36825;&#20123;&#19981;&#21516;&#30340;&#19981;&#30830;&#23450;&#24615;&#26469;&#28304;&#12290;&#37319;&#29992;&#20102;&#21463;&#28508;&#21464;&#37327;&#24314;&#27169;&#21551;&#21457;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#27169;&#22411;&#20351;&#20195;&#29702;&#27169;&#22411;&#21487;&#20197;&#29992;&#20110;&#20026;&#19981;&#21516;&#30340;&#36125;&#21494;&#26031;&#32467;&#26500;&#20581;&#24247;&#30417;&#27979;&#20219;&#21153;&#29983;&#25104;&#20808;&#39564;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fatigue crack growth is one of the most common types of deterioration in metal structures with significant implications on their reliability. Recent advances in Structural Health Monitoring (SHM) have motivated the use of structural response data to predict future crack growth under uncertainty, in order to enable a transition towards predictive maintenance. Accurately representing different sources of uncertainty in stochastic crack growth (SCG) processes is a non-trivial task. The present work builds on previous research on physics-based SCG modeling under both material and load-related uncertainty. The aim here is to construct computationally efficient, probabilistic surrogate models for SCG processes that successfully encode these different sources of uncertainty. An approach inspired by latent variable modeling is employed that utilizes Gaussian Process (GP) regression models to enable the surrogates to be used to generate prior distributions for different Bayesian SHM tasks as th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21487;&#23398;&#20064;&#30340;&#29289;&#29702;&#23398;&#30340;&#31639;&#27861;&#27969;&#31243;&#65292;&#33021;&#22815;&#22312;&#23454;&#26102;&#29615;&#22659;&#20013;&#35786;&#26029;&#36816;&#21160;&#23039;&#21183;&#38382;&#39064;&#24182;&#25552;&#20379;&#30699;&#27491;&#24314;&#35758;&#65292;&#36890;&#36807;&#23039;&#21183;&#35782;&#21035;&#12289;&#37325;&#22797;&#27425;&#25968;&#35745;&#31639;&#21644;&#21160;&#20316;&#28436;&#21464;&#36319;&#36394;&#23454;&#29616;&#12290;&#35813;&#31995;&#32479;&#22312;&#20845;&#20010;&#19981;&#21516;&#30340;&#36816;&#21160;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#36890;&#36807;&#20302;&#25104;&#26412;&#35774;&#22791;&#22914;&#26234;&#33021;&#25163;&#26426;&#25552;&#20379;&#23454;&#26102;&#24314;&#35758;&#65292;&#20351;&#33258;&#25105;&#32451;&#20064;&#25104;&#20026;&#21487;&#33021;&#65292;&#21516;&#26102;&#38477;&#20302;&#21463;&#20260;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2310.07221</link><description>&lt;p&gt;
&#20351;&#29992;&#21487;&#23398;&#20064;&#30340;&#29289;&#29702;&#23398;&#36827;&#34892;&#23454;&#26102;&#36816;&#21160;&#23039;&#21183;&#24314;&#35758;
&lt;/p&gt;
&lt;p&gt;
Using Learnable Physics for Real-Time Exercise Form Recommendations. (arXiv:2310.07221v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07221
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21487;&#23398;&#20064;&#30340;&#29289;&#29702;&#23398;&#30340;&#31639;&#27861;&#27969;&#31243;&#65292;&#33021;&#22815;&#22312;&#23454;&#26102;&#29615;&#22659;&#20013;&#35786;&#26029;&#36816;&#21160;&#23039;&#21183;&#38382;&#39064;&#24182;&#25552;&#20379;&#30699;&#27491;&#24314;&#35758;&#65292;&#36890;&#36807;&#23039;&#21183;&#35782;&#21035;&#12289;&#37325;&#22797;&#27425;&#25968;&#35745;&#31639;&#21644;&#21160;&#20316;&#28436;&#21464;&#36319;&#36394;&#23454;&#29616;&#12290;&#35813;&#31995;&#32479;&#22312;&#20845;&#20010;&#19981;&#21516;&#30340;&#36816;&#21160;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#36890;&#36807;&#20302;&#25104;&#26412;&#35774;&#22791;&#22914;&#26234;&#33021;&#25163;&#26426;&#25552;&#20379;&#23454;&#26102;&#24314;&#35758;&#65292;&#20351;&#33258;&#25105;&#32451;&#20064;&#25104;&#20026;&#21487;&#33021;&#65292;&#21516;&#26102;&#38477;&#20302;&#21463;&#20260;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33391;&#22909;&#30340;&#23039;&#21183;&#21644;&#24418;&#24335;&#23545;&#20110;&#23433;&#20840;&#21644;&#39640;&#25928;&#30340;&#36816;&#21160;&#33267;&#20851;&#37325;&#35201;&#12290;&#21363;&#20351;&#22312;&#20581;&#36523;&#25151;&#29615;&#22659;&#19979;&#65292;&#25945;&#32451;&#21487;&#33021;&#26080;&#27861;&#21450;&#26102;&#25552;&#20379;&#21453;&#39304;&#12290;&#22240;&#27492;&#65292;&#24247;&#22797;&#30103;&#27861;&#21644;&#20581;&#36523;&#35757;&#32451;&#21487;&#20197;&#20174;&#25552;&#20379;&#23454;&#26102;&#35780;&#20272;&#30340;&#25512;&#33616;&#31995;&#32479;&#20013;&#21463;&#30410;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#27969;&#31243;&#65292;&#21487;&#20197;&#22312;&#23454;&#26102;&#29615;&#22659;&#20013;&#23545;&#36816;&#21160;&#25216;&#26415;&#20013;&#30340;&#38382;&#39064;&#36827;&#34892;&#35786;&#26029;&#65292;&#24182;&#32473;&#20986;&#30699;&#27491;&#24314;&#35758;&#65292;&#20855;&#26377;&#39640;&#25935;&#24863;&#24615;&#21644;&#29305;&#24322;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;MediaPipe&#36827;&#34892;&#23039;&#21183;&#35782;&#21035;&#65292;&#20351;&#29992;&#23792;&#20540;&#31361;&#20986;&#26816;&#27979;&#35745;&#31639;&#37325;&#22797;&#27425;&#25968;&#65292;&#24182;&#20351;&#29992;&#21487;&#23398;&#20064;&#30340;&#29289;&#29702;&#27169;&#25311;&#22120;&#36319;&#36394;&#27599;&#20010;&#36816;&#21160;&#30340;&#21160;&#20316;&#28436;&#21464;&#12290;&#26681;&#25454;&#19982;&#20856;&#22411;&#23398;&#20064;&#21160;&#20316;&#30340;&#20559;&#24046;&#65292;&#22522;&#20110;&#32479;&#35745;&#23398;&#20064;&#23545;&#27979;&#35797;&#35270;&#39057;&#36827;&#34892;&#35786;&#26029;&#12290;&#35813;&#31995;&#32479;&#22312;&#20845;&#20010;&#20840;&#36523;&#21644;&#19978;&#21322;&#36523;&#36816;&#21160;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#36890;&#36807;&#20302;&#25104;&#26412;&#35774;&#22791;&#22914;&#26234;&#33021;&#25163;&#26426;&#25552;&#20379;&#30340;&#23454;&#26102;&#24314;&#35758;&#65292;&#36816;&#21160;&#32773;&#21487;&#20197;&#32416;&#27491;&#28508;&#22312;&#30340;&#38169;&#35823;&#65292;&#20351;&#33258;&#25105;&#32451;&#20064;&#25104;&#20026;&#21487;&#33021;&#65292;&#21516;&#26102;&#38477;&#20302;&#21463;&#20260;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
Good posture and form are essential for safe and productive exercising. Even in gym settings, trainers may not be readily available for feedback. Rehabilitation therapies and fitness workouts can thus benefit from recommender systems that provide real-time evaluation. In this paper, we present an algorithmic pipeline that can diagnose problems in exercise techniques and offer corrective recommendations, with high sensitivity and specificity in real-time. We use MediaPipe for pose recognition, count repetitions using peak-prominence detection, and use a learnable physics simulator to track motion evolution for each exercise. A test video is diagnosed based on deviations from the prototypical learned motion using statistical learning. The system is evaluated on six full and upper body exercises. These real-time recommendations, counseled via low-cost equipment like smartphones, will allow exercisers to rectify potential mistakes making self-practice feasible while reducing the risk of wo
&lt;/p&gt;</description></item><item><title>&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#35821;&#35328;&#20998;&#31867;&#27169;&#22411;&#36827;&#34892;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#12290;&#36890;&#36807;&#21033;&#29992;&#38598;&#25104;&#26041;&#27861;&#65292;&#29983;&#25104;&#22810;&#20010;&#19987;&#38376;&#30340;&#25915;&#20987;&#27169;&#22411;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#32463;&#20856;&#21644;&#35821;&#35328;&#20998;&#31867;&#20219;&#21153;&#19978;&#27604;&#21333;&#20010;&#25915;&#20987;&#27169;&#22411;&#25110;&#27599;&#20010;&#31867;&#21035;&#26631;&#31614;&#30340;&#25915;&#20987;&#27169;&#22411;&#26356;&#20934;&#30830;&#12290;</title><link>http://arxiv.org/abs/2310.07219</link><description>&lt;p&gt;
&#25913;&#36827;&#30340;&#23545;&#35821;&#35328;&#20998;&#31867;&#27169;&#22411;&#30340;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Improved Membership Inference Attacks Against Language Classification Models. (arXiv:2310.07219v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07219
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#35821;&#35328;&#20998;&#31867;&#27169;&#22411;&#36827;&#34892;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#12290;&#36890;&#36807;&#21033;&#29992;&#38598;&#25104;&#26041;&#27861;&#65292;&#29983;&#25104;&#22810;&#20010;&#19987;&#38376;&#30340;&#25915;&#20987;&#27169;&#22411;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#32463;&#20856;&#21644;&#35821;&#35328;&#20998;&#31867;&#20219;&#21153;&#19978;&#27604;&#21333;&#20010;&#25915;&#20987;&#27169;&#22411;&#25110;&#27599;&#20010;&#31867;&#21035;&#26631;&#31614;&#30340;&#25915;&#20987;&#27169;&#22411;&#26356;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#20855;&#26377;&#38646;&#21806;&#12289;&#21046;&#36896;&#12289;&#20581;&#24247;&#31561;&#35768;&#22810;&#39046;&#22495;&#30340;&#29992;&#20363;&#12290;&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#37319;&#29992;&#30340;&#22686;&#21152;&#65292;&#24050;&#32463;&#21457;&#29616;&#20102;&#30456;&#20851;&#30340;&#39118;&#38505;&#65292;&#21253;&#25324;&#23545;&#20351;&#29992;&#20854;&#25968;&#25454;&#35757;&#32451;&#27169;&#22411;&#30340;&#20154;&#30340;&#38544;&#31169;&#39118;&#38505;&#12290;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#38544;&#31169;&#39118;&#38505;&#23545;&#20110;&#26159;&#21542;&#20351;&#29992;&#12289;&#37096;&#32626;&#25110;&#20849;&#20139;&#27169;&#22411;&#20570;&#20986;&#30693;&#24773;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#38544;&#31169;&#39118;&#38505;&#35780;&#20272;&#30340;&#19968;&#31181;&#24120;&#35265;&#26041;&#27861;&#26159;&#23545;&#27169;&#22411;&#36827;&#34892;&#19968;&#20010;&#25110;&#22810;&#20010;&#24050;&#30693;&#25915;&#20987;&#65292;&#24182;&#27979;&#37327;&#23427;&#20204;&#30340;&#25104;&#21151;&#29575;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#20998;&#31867;&#27169;&#22411;&#36827;&#34892;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21033;&#29992;&#38598;&#25104;&#26041;&#27861;&#65292;&#20026;&#19981;&#21516;&#25968;&#25454;&#23376;&#38598;&#29983;&#25104;&#35768;&#22810;&#19987;&#38376;&#30340;&#25915;&#20987;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#32463;&#20856;&#21644;&#35821;&#35328;&#20998;&#31867;&#20219;&#21153;&#19978;&#27604;&#21333;&#20010;&#25915;&#20987;&#27169;&#22411;&#25110;&#27599;&#20010;&#31867;&#21035;&#26631;&#31614;&#30340;&#25915;&#20987;&#27169;&#22411;&#37117;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence systems are prevalent in everyday life, with use cases in retail, manufacturing, health, and many other fields. With the rise in AI adoption, associated risks have been identified, including privacy risks to the people whose data was used to train models. Assessing the privacy risks of machine learning models is crucial to enabling knowledgeable decisions on whether to use, deploy, or share a model. A common approach to privacy risk assessment is to run one or more known attacks against the model and measure their success rate. We present a novel framework for running membership inference attacks against classification models. Our framework takes advantage of the ensemble method, generating many specialized attack models for different subsets of the data. We show that this approach achieves higher accuracy than either a single attack model or an attack model per class label, both on classical and language classification tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#27867;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#21270;&#26234;&#33021;&#20307;&#30456;&#20114;&#20316;&#29992;&#30340;&#25351;&#26631;&#65292;&#24182;&#36890;&#36807;&#35813;&#25351;&#26631;&#35774;&#35745;&#20102;&#36164;&#28304;&#20998;&#37197;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#26377;&#38480;&#39044;&#31639;&#19979;&#35757;&#32451;&#36866;&#29992;&#20110;&#22810;&#26679;&#21270;&#22330;&#26223;&#30340;&#26234;&#33021;&#20307;&#31574;&#30053;&#38598;&#21512;&#12290;</title><link>http://arxiv.org/abs/2310.07218</link><description>&lt;p&gt;
&#22312;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#37327;&#21270;&#26234;&#33021;&#20307;&#30456;&#20114;&#20316;&#29992;&#20197;&#23454;&#29616;&#25104;&#26412;&#25928;&#30410;&#30340;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Quantifying Agent Interaction in Multi-agent Reinforcement Learning for Cost-efficient Generalization. (arXiv:2310.07218v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07218
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#27867;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#21270;&#26234;&#33021;&#20307;&#30456;&#20114;&#20316;&#29992;&#30340;&#25351;&#26631;&#65292;&#24182;&#36890;&#36807;&#35813;&#25351;&#26631;&#35774;&#35745;&#20102;&#36164;&#28304;&#20998;&#37197;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#26377;&#38480;&#39044;&#31639;&#19979;&#35757;&#32451;&#36866;&#29992;&#20110;&#22810;&#26679;&#21270;&#22330;&#26223;&#30340;&#26234;&#33021;&#20307;&#31574;&#30053;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27867;&#21270;&#26159;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#19968;&#20010;&#26234;&#33021;&#20307;&#21463;&#26410;&#30693;&#21512;&#20316;&#26234;&#33021;&#20307;&#30340;&#24433;&#21709;&#31243;&#24230;&#21462;&#20915;&#20110;&#35813;&#26234;&#33021;&#20307;&#30340;&#31574;&#30053;&#21644;&#20855;&#20307;&#22330;&#26223;&#12290;&#23545;&#36825;&#31181;&#20851;&#31995;&#30340;&#23450;&#37327;&#30740;&#31350;&#26377;&#21161;&#20110;&#26377;&#25928;&#22521;&#35757;&#36866;&#29992;&#20110;&#22810;&#26679;&#21270;&#22330;&#26223;&#30340;&#26234;&#33021;&#20307;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24433;&#21709;&#27700;&#24179;&#65288;Level of Influence&#65292;LoI&#65289;&#65292;&#19968;&#31181;&#24230;&#37327;&#32473;&#23450;&#22330;&#26223;&#21644;&#29615;&#22659;&#20013;&#26234;&#33021;&#20307;&#20043;&#38388;&#20132;&#20114;&#24378;&#24230;&#30340;&#25351;&#26631;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#19968;&#33324;&#26469;&#35828;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#26356;&#22810;&#26679;&#21270;&#30340;&#21512;&#20316;&#26234;&#33021;&#20307;&#21487;&#20197;&#25552;&#39640;&#33258;&#25105;&#26234;&#33021;&#20307;&#30340;&#27867;&#21270;&#24615;&#33021;&#65307;&#28982;&#32780;&#65292;&#36825;&#31181;&#25913;&#36827;&#22240;&#19981;&#21516;&#22330;&#26223;&#21644;&#29615;&#22659;&#32780;&#24322;&#12290;LoI&#22312;&#39044;&#27979;&#29305;&#23450;&#22330;&#26223;&#20013;&#36825;&#20123;&#25913;&#36827;&#24046;&#24322;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20197;LoI&#20026;&#25351;&#23548;&#30340;&#36164;&#28304;&#20998;&#37197;&#26041;&#27861;&#65292;&#38024;&#23545;&#26377;&#38480;&#39044;&#31639;&#35757;&#32451;&#36866;&#29992;&#20110;&#22810;&#26679;&#21270;&#22330;&#26223;&#30340;&#31574;&#30053;&#38598;&#21512;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25112;&#30053;&#24615;&#36164;&#28304;&#20998;&#37197;&#21487;&#20197;&#26126;&#26174;&#25552;&#39640;&#26234;&#33021;&#20307;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generalization poses a significant challenge in Multi-agent Reinforcement Learning (MARL). The extent to which an agent is influenced by unseen co-players depends on the agent's policy and the specific scenario. A quantitative examination of this relationship sheds light on effectively training agents for diverse scenarios. In this study, we present the Level of Influence (LoI), a metric quantifying the interaction intensity among agents within a given scenario and environment. We observe that, generally, a more diverse set of co-play agents during training enhances the generalization performance of the ego agent; however, this improvement varies across distinct scenarios and environments. LoI proves effective in predicting these improvement disparities within specific scenarios. Furthermore, we introduce a LoI-guided resource allocation method tailored to train a set of policies for diverse scenarios under a constrained budget. Our results demonstrate that strategic resource allocatio
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20219;&#21153;&#23398;&#20064;&#21551;&#29992;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#29992;&#20110;&#20934;&#30830;&#39640;&#21487;&#38752;&#22320;&#29983;&#25104;&#33337;&#21482;&#21507;&#27700;&#35835;&#25968;&#65292;&#20197;&#25903;&#25345;&#26234;&#33021;&#28023;&#19978;&#30417;&#25511;&#12290;</title><link>http://arxiv.org/abs/2310.07212</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;&#25903;&#25345;&#30340;&#26234;&#33021;&#28023;&#19978;&#30417;&#25511;&#33337;&#21482;&#21507;&#27700;&#35835;&#25968;&#33258;&#21160;&#21270;
&lt;/p&gt;
&lt;p&gt;
Multi-Task Learning-Enabled Automatic Vessel Draft Reading for Intelligent Maritime Surveillance. (arXiv:2310.07212v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07212
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20219;&#21153;&#23398;&#20064;&#21551;&#29992;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#29992;&#20110;&#20934;&#30830;&#39640;&#21487;&#38752;&#22320;&#29983;&#25104;&#33337;&#21482;&#21507;&#27700;&#35835;&#25968;&#65292;&#20197;&#25903;&#25345;&#26234;&#33021;&#28023;&#19978;&#30417;&#25511;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#39640;&#25928;&#30340;&#33337;&#21482;&#21507;&#27700;&#35835;&#25968;&#65288;VDR&#65289;&#26159;&#26234;&#33021;&#28023;&#19978;&#30417;&#25511;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#21487;&#20197;&#29992;&#26469;&#21028;&#26029;&#33337;&#21482;&#26159;&#21542;&#27491;&#24120;&#35013;&#36733;&#25110;&#36807;&#36733;&#12290;&#35745;&#31639;&#26426;&#35270;&#35273;&#25216;&#26415;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#20215;&#27604;&#65292;&#24050;&#25104;&#20026;&#20272;&#35745;&#33337;&#21482;&#21507;&#27700;&#28145;&#24230;&#30340;&#27969;&#34892;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#20272;&#35745;&#26041;&#27861;&#23481;&#26131;&#21463;&#21040;&#22810;&#31181;&#38480;&#21046;&#30340;&#24433;&#21709;&#65292;&#22914;&#23545;&#20302;&#36136;&#37327;&#22270;&#20687;&#30340;&#25935;&#24863;&#24615;&#65292;&#39640;&#35745;&#31639;&#25104;&#26412;&#31561;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20219;&#21153;&#23398;&#20064;&#21551;&#29992;&#30340;&#35745;&#31639;&#26041;&#27861;&#65288;&#31216;&#20026;MTL-VDR&#65289;&#65292;&#29992;&#20110;&#29983;&#25104;&#39640;&#21487;&#38752;&#30340;VDR&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30340;MTL-VDR&#20027;&#35201;&#21253;&#25324;&#22235;&#20010;&#32452;&#25104;&#37096;&#20998;&#65292;&#21363;&#21507;&#27700;&#26631;&#24535;&#26816;&#27979;&#65292;&#21507;&#27700;&#23610;&#24230;&#35782;&#21035;&#65292;&#33337;&#20307;/&#27700;&#22495;&#20998;&#21106;&#21644;&#26368;&#32456;&#21507;&#27700;&#28145;&#24230;&#20272;&#35745;&#12290;&#25105;&#20204;&#39318;&#20808;&#26500;&#24314;&#19982;&#21507;&#27700;&#26631;&#24535;&#26816;&#27979;&#30456;&#20851;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#37319;&#29992;&#24378;&#22823;&#39640;&#25928;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26469;&#20934;&#30830;&#25191;&#34892;&#26816;&#27979;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
The accurate and efficient vessel draft reading (VDR) is an important component of intelligent maritime surveillance, which could be exploited to assist in judging whether the vessel is normally loaded or overloaded. The computer vision technique with an excellent price-to-performance ratio has become a popular medium to estimate vessel draft depth. However, the traditional estimation methods easily suffer from several limitations, such as sensitivity to low-quality images, high computational cost, etc. In this work, we propose a multi-task learning-enabled computational method (termed MTL-VDR) for generating highly reliable VDR. In particular, our MTL-VDR mainly consists of four components, i.e., draft mark detection, draft scale recognition, vessel/water segmentation, and final draft depth estimation. We first construct a benchmark dataset related to draft mark detection and employ a powerful and efficient convolutional neural network to accurately perform the detection task. The mul
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#26088;&#22312;&#20171;&#32461;&#26368;&#26032;&#30340;&#35270;&#35273;&#35745;&#31639;&#39046;&#22495;&#20013;&#25193;&#25955;&#27169;&#22411;&#30340;&#21457;&#23637;&#21644;&#24212;&#29992;&#65292;&#28085;&#30422;&#20102;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#30340;&#26680;&#24515;&#27010;&#24565;&#21644;&#23454;&#29616;&#32454;&#33410;&#65292;&#24182;&#24635;&#32467;&#20102;&#20010;&#20154;&#21270;&#12289;&#26465;&#20214;&#32422;&#26463;&#21644;&#21453;&#28436;&#31561;&#37325;&#35201;&#26041;&#38754;&#12290;</title><link>http://arxiv.org/abs/2310.07204</link><description>&lt;p&gt;
&#35270;&#35273;&#35745;&#31639;&#25193;&#25955;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;
State of the Art on Diffusion Models for Visual Computing. (arXiv:2310.07204v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07204
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#26088;&#22312;&#20171;&#32461;&#26368;&#26032;&#30340;&#35270;&#35273;&#35745;&#31639;&#39046;&#22495;&#20013;&#25193;&#25955;&#27169;&#22411;&#30340;&#21457;&#23637;&#21644;&#24212;&#29992;&#65292;&#28085;&#30422;&#20102;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#30340;&#26680;&#24515;&#27010;&#24565;&#21644;&#23454;&#29616;&#32454;&#33410;&#65292;&#24182;&#24635;&#32467;&#20102;&#20010;&#20154;&#21270;&#12289;&#26465;&#20214;&#32422;&#26463;&#21644;&#21453;&#28436;&#31561;&#37325;&#35201;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#30340;&#20986;&#29616;&#65292;&#35270;&#35273;&#35745;&#31639;&#39046;&#22495;&#27491;&#22312;&#36805;&#36895;&#21457;&#23637;&#65292;&#23427;&#20026;&#22270;&#20687;&#12289;&#35270;&#39057;&#21644;3D&#22330;&#26223;&#30340;&#29983;&#25104;&#12289;&#32534;&#36753;&#21644;&#37325;&#24314;&#25552;&#20379;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#33021;&#21147;&#12290;&#22312;&#36825;&#20123;&#39046;&#22495;&#20013;&#65292;&#25193;&#25955;&#27169;&#22411;&#26159;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#30340;&#39318;&#36873;&#26550;&#26500;&#12290;&#20165;&#22312;&#36807;&#21435;&#19968;&#24180;&#20013;&#65292;&#22522;&#20110;&#25193;&#25955;&#30340;&#24037;&#20855;&#21644;&#24212;&#29992;&#30340;&#25991;&#29486;&#25968;&#37327;&#21576;&#25351;&#25968;&#22686;&#38271;&#65292;&#24182;&#19988;&#28041;&#21450;&#35745;&#31639;&#26426;&#22270;&#24418;&#23398;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#20154;&#24037;&#26234;&#33021;&#31038;&#21306;&#30340;&#30456;&#20851;&#35770;&#25991;&#27599;&#22825;&#37117;&#22312;arXiv&#19978;&#21457;&#34920;&#12290;&#36825;&#20010;&#39046;&#22495;&#30340;&#24555;&#36895;&#22686;&#38271;&#20351;&#24471;&#36319;&#19978;&#25152;&#26377;&#26368;&#26032;&#21457;&#23637;&#21464;&#24471;&#22256;&#38590;&#12290;&#36825;&#20221;&#26368;&#26032;&#25216;&#26415;&#25253;&#21578;&#65288;STAR&#65289;&#30340;&#30446;&#26631;&#26159;&#20171;&#32461;&#25193;&#25955;&#27169;&#22411;&#30340;&#22522;&#26412;&#25968;&#23398;&#27010;&#24565;&#12289;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#30340;&#23454;&#29616;&#32454;&#33410;&#21644;&#35774;&#35745;&#36873;&#25321;&#65292;&#20197;&#21450;&#27010;&#36848;&#36825;&#20123;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#30340;&#37325;&#35201;&#26041;&#38754;&#65292;&#21253;&#25324;&#20010;&#24615;&#21270;&#12289;&#26465;&#20214;&#32422;&#26463;&#12289;&#21453;&#28436;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of visual computing is rapidly advancing due to the emergence of generative artificial intelligence (AI), which unlocks unprecedented capabilities for the generation, editing, and reconstruction of images, videos, and 3D scenes. In these domains, diffusion models are the generative AI architecture of choice. Within the last year alone, the literature on diffusion-based tools and applications has seen exponential growth and relevant papers are published across the computer graphics, computer vision, and AI communities with new works appearing daily on arXiv. This rapid growth of the field makes it difficult to keep up with all recent developments. The goal of this state-of-the-art report (STAR) is to introduce the basic mathematical concepts of diffusion models, implementation details and design choices of the popular Stable Diffusion model, as well as overview important aspects of these generative AI tools, including personalization, conditioning, inversion, among others. Mor
&lt;/p&gt;</description></item><item><title>MatChat&#26159;&#19968;&#20010;&#29992;&#20110;&#26448;&#26009;&#31185;&#23398;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#24212;&#29992;&#26381;&#21153;&#24179;&#21488;&#65292;&#21033;&#29992;LLaMA2-7B&#27169;&#22411;&#21644;13,878&#26465;&#32467;&#26500;&#21270;&#26448;&#26009;&#30693;&#35782;&#25968;&#25454;&#65292;&#33021;&#22815;&#39044;&#27979;&#26080;&#26426;&#26448;&#26009;&#30340;&#21512;&#25104;&#36335;&#24452;&#12290;</title><link>http://arxiv.org/abs/2310.07197</link><description>&lt;p&gt;
MatChat: &#29992;&#20110;&#26448;&#26009;&#31185;&#23398;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#24212;&#29992;&#26381;&#21153;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
MatChat: A Large Language Model and Application Service Platform for Materials Science. (arXiv:2310.07197v1 [cond-mat.mtrl-sci])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07197
&lt;/p&gt;
&lt;p&gt;
MatChat&#26159;&#19968;&#20010;&#29992;&#20110;&#26448;&#26009;&#31185;&#23398;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#24212;&#29992;&#26381;&#21153;&#24179;&#21488;&#65292;&#21033;&#29992;LLaMA2-7B&#27169;&#22411;&#21644;13,878&#26465;&#32467;&#26500;&#21270;&#26448;&#26009;&#30693;&#35782;&#25968;&#25454;&#65292;&#33021;&#22815;&#39044;&#27979;&#26080;&#26426;&#26448;&#26009;&#30340;&#21512;&#25104;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21270;&#23398;&#21512;&#25104;&#36335;&#24452;&#30340;&#39044;&#27979;&#22312;&#26448;&#26009;&#31185;&#23398;&#30740;&#31350;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#21512;&#25104;&#36335;&#24452;&#30340;&#22797;&#26434;&#24615;&#21644;&#32570;&#20047;&#20840;&#38754;&#30340;&#25968;&#25454;&#38598;&#31561;&#25361;&#25112;&#38459;&#30861;&#20102;&#25105;&#20204;&#20934;&#30830;&#39044;&#27979;&#36825;&#20123;&#21270;&#23398;&#36807;&#31243;&#30340;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;LLaMA2-7B&#27169;&#22411;&#30340;&#21147;&#37327;&#65292;&#24182;&#36890;&#36807;&#32467;&#21512;13,878&#26465;&#32467;&#26500;&#21270;&#26448;&#26009;&#30693;&#35782;&#25968;&#25454;&#30340;&#23398;&#20064;&#36807;&#31243;&#21152;&#24378;&#20102;&#23427;&#12290;&#36825;&#20010;&#21517;&#20026;MatChat&#30340;&#19987;&#38376;&#30340;AI&#27169;&#22411;&#19987;&#27880;&#20110;&#39044;&#27979;&#26080;&#26426;&#26448;&#26009;&#30340;&#21512;&#25104;&#36335;&#24452;&#12290;MatChat&#22312;&#26448;&#26009;&#31185;&#23398;&#30340;&#30693;&#35782;&#29983;&#25104;&#21644;&#25512;&#29702;&#26041;&#38754;&#23637;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#33021;&#21147;&#12290;&#34429;&#28982;MatChat&#38656;&#35201;&#36827;&#19968;&#27493;&#25913;&#36827;&#20197;&#28385;&#36275;&#22810;&#26679;&#21270;&#26448;&#26009;&#35774;&#35745;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
The prediction of chemical synthesis pathways plays a pivotal role in materials science research. Challenges, such as the complexity of synthesis pathways and the lack of comprehensive datasets, currently hinder our ability to predict these chemical processes accurately. However, recent advancements in generative artificial intelligence (GAI), including automated text generation and question-answering systems, coupled with fine-tuning techniques, have facilitated the deployment of large-scale AI models tailored to specific domains. In this study, we harness the power of the LLaMA2-7B model and enhance it through a learning process that incorporates 13,878 pieces of structured material knowledge data. This specialized AI model, named MatChat, focuses on predicting inorganic material synthesis pathways. MatChat exhibits remarkable proficiency in generating and reasoning with knowledge in materials science. Although MatChat requires further refinement to meet the diverse material design n
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#38376;&#25511;&#30340;&#28151;&#21512;&#19987;&#23478;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#31574;&#30053;&#65292;&#36890;&#36807;&#26681;&#25454;&#26631;&#35760;&#30340;&#19987;&#23478;&#27010;&#29575;&#20998;&#24067;&#23558;&#26631;&#35760;&#20998;&#37197;&#32473;&#21464;&#37327;&#25968;&#37327;&#30340;&#19987;&#23478;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#30340;&#31232;&#30095;&#24615;&#21644;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.07188</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#38376;&#25511;&#22312;&#22522;&#20110;&#28151;&#21512;&#19987;&#23478;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Adaptive Gating in Mixture-of-Experts based Language Models. (arXiv:2310.07188v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07188
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#38376;&#25511;&#30340;&#28151;&#21512;&#19987;&#23478;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#31574;&#30053;&#65292;&#36890;&#36807;&#26681;&#25454;&#26631;&#35760;&#30340;&#19987;&#23478;&#27010;&#29575;&#20998;&#24067;&#23558;&#26631;&#35760;&#20998;&#37197;&#32473;&#21464;&#37327;&#25968;&#37327;&#30340;&#19987;&#23478;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#30340;&#31232;&#30095;&#24615;&#21644;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;OpenAI&#30340;ChatGPT&#65289;&#22312;&#21508;&#31181;NLP&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#12290;&#31232;&#30095;&#28608;&#27963;&#30340;&#28151;&#21512;&#19987;&#23478;&#65288;MoE&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#35745;&#31639;&#25805;&#20316;&#25968;&#37327;&#24658;&#23450;&#30340;&#21516;&#26102;&#25193;&#22823;&#27169;&#22411;&#35268;&#27169;&#12290;&#29616;&#26377;&#30340;MoE&#27169;&#22411;&#37319;&#29992;&#20102;&#22266;&#23450;&#30340;&#38376;&#25511;&#32593;&#32476;&#65292;&#27599;&#20010;&#26631;&#35760;&#37117;&#30001;&#30456;&#21516;&#25968;&#37327;&#30340;&#19987;&#23478;&#35745;&#31639;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#19982;&#25105;&#20204;&#30340;&#30452;&#35273;&#30456;&#30683;&#30462;&#65292;&#22240;&#20026;&#27599;&#20010;&#24207;&#21015;&#20013;&#30340;&#26631;&#35760;&#22312;&#35821;&#35328;&#22797;&#26434;&#24615;&#26041;&#38754;&#26377;&#25152;&#19981;&#21516;&#65292;&#22240;&#27492;&#38656;&#35201;&#19981;&#21516;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20013;&#24456;&#23569;&#35752;&#35770;&#27599;&#20010;&#26631;&#35760;&#30340;&#35745;&#31639;&#21644;&#27169;&#22411;&#24615;&#33021;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;MoE&#20013;&#20351;&#29992;&#33258;&#36866;&#24212;&#38376;&#25511;&#30340;&#28789;&#27963;&#35757;&#32451;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#21487;&#20197;&#26681;&#25454;&#19987;&#23478;&#27010;&#29575;&#20998;&#24067;&#23558;&#26631;&#35760;&#22788;&#29702;&#20026;&#21487;&#21464;&#25968;&#37327;&#30340;&#19987;&#23478;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#25913;&#36827;&#35757;&#32451;&#25928;&#29575;&#30340;&#21516;&#26102;&#20445;&#25345;&#31232;&#30095;&#24615;&#12290;&#27492;&#22806;&#65292;&#35838;&#31243;&#23398;&#20064;&#20063;&#22312;&#35813;&#26694;&#26550;&#20013;&#24212;&#29992;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models, such as OpenAI's ChatGPT, have demonstrated exceptional language understanding capabilities in various NLP tasks. Sparsely activated mixture-of-experts (MoE) has emerged as a promising solution for scaling models while maintaining a constant number of computational operations. Existing MoE model adopts a fixed gating network where each token is computed by the same number of experts. However, this approach contradicts our intuition that the tokens in each sequence vary in terms of their linguistic complexity and, consequently, require different computational costs. Little is discussed in prior research on the trade-off between computation per token and model performance. This paper introduces adaptive gating in MoE, a flexible training strategy that allows tokens to be processed by a variable number of experts based on expert probability distribution. The proposed framework preserves sparsity while improving training efficiency. Additionally, curriculum learning 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#35270;&#35282;Transformer&#29992;&#20110;&#39640;&#20809;&#35889;&#22270;&#20687;&#20998;&#31867;&#65292;&#35299;&#20915;&#20102;HSI&#31435;&#26041;&#20307;&#20013;&#21487;&#33021;&#35760;&#24405;&#21040;&#30340;&#22330;&#26223;&#29305;&#23450;&#20294;&#38750;&#26412;&#36136;&#30456;&#20851;&#24615;&#24102;&#26469;&#30340;&#31354;&#38388;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#22810;&#35270;&#35282;&#20027;&#25104;&#20998;&#20998;&#26512;&#12289;&#20809;&#35889;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#21644;&#31354;&#38388;&#27744;&#21270;&#26631;&#35760;&#21270;Transformer&#30340;&#32452;&#21512;&#26469;&#25552;&#21462;&#20302;&#32500;&#30340;&#31354;&#38388;-&#20809;&#35889;&#29305;&#24449;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2310.07186</link><description>&lt;p&gt;
&#22810;&#35270;&#35282;Transformer: &#37325;&#26032;&#24605;&#32771;&#39640;&#20809;&#35889;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#31354;&#38388;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Multiview Transformer: Rethinking Spatial Information in Hyperspectral Image Classification. (arXiv:2310.07186v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07186
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#35270;&#35282;Transformer&#29992;&#20110;&#39640;&#20809;&#35889;&#22270;&#20687;&#20998;&#31867;&#65292;&#35299;&#20915;&#20102;HSI&#31435;&#26041;&#20307;&#20013;&#21487;&#33021;&#35760;&#24405;&#21040;&#30340;&#22330;&#26223;&#29305;&#23450;&#20294;&#38750;&#26412;&#36136;&#30456;&#20851;&#24615;&#24102;&#26469;&#30340;&#31354;&#38388;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#22810;&#35270;&#35282;&#20027;&#25104;&#20998;&#20998;&#26512;&#12289;&#20809;&#35889;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#21644;&#31354;&#38388;&#27744;&#21270;&#26631;&#35760;&#21270;Transformer&#30340;&#32452;&#21512;&#26469;&#25552;&#21462;&#20302;&#32500;&#30340;&#31354;&#38388;-&#20809;&#35889;&#29305;&#24449;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#39640;&#20809;&#35889;&#22270;&#20687;(HSI)&#20013;&#30340;&#27599;&#20010;&#20687;&#32032;&#65292;&#35782;&#21035;&#20854;&#22303;&#22320;&#35206;&#30422;&#31867;&#21035;&#20381;&#36182;&#20110;&#20809;&#35889;&#21644;&#31354;&#38388;&#20449;&#24687;&#12290;&#21033;&#29992;&#20855;&#26377;&#29305;&#23450;&#22359;&#22823;&#23567;&#30340;HSI&#31435;&#26041;&#20307;&#26469;&#25552;&#21462;&#20013;&#24515;&#20687;&#32032;&#30340;&#31354;&#38388;-&#20809;&#35889;&#29305;&#24449;&#34920;&#31034;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;HSI&#31435;&#26041;&#20307;&#20013;&#21487;&#33021;&#35760;&#24405;&#21040;&#30340;&#22330;&#26223;&#29305;&#23450;&#20294;&#38750;&#26412;&#36136;&#30456;&#20851;&#24615;&#12290;&#36825;&#20123;&#39069;&#22806;&#20449;&#24687;&#25913;&#21892;&#20102;&#29616;&#26377;HSI&#25968;&#25454;&#38598;&#19978;&#30340;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#20351;&#24471;&#23545;&#27169;&#22411;&#33021;&#21147;&#30340;&#24688;&#24403;&#35780;&#20272;&#21464;&#24471;&#22256;&#38590;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#38382;&#39064;&#31216;&#20026;&#31354;&#38388;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#20005;&#26684;&#30340;&#23454;&#39564;&#35774;&#32622;&#26469;&#36991;&#20813;&#23427;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;HSI&#20998;&#31867;&#30340;&#22810;&#35270;&#35282;Transformer&#65292;&#21253;&#25324;&#22810;&#35270;&#35282;&#20027;&#25104;&#20998;&#20998;&#26512;(MPCA)&#12289;&#20809;&#35889;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;(SED)&#21644;&#31354;&#38388;&#27744;&#21270;&#26631;&#35760;&#21270;Transformer(SPTT)&#12290;MPCA&#36890;&#36807;&#26500;&#24314;&#20809;&#35889;&#22810;&#35270;&#35282;&#35266;&#27979;&#24182;&#23545;&#27599;&#20010;&#35270;&#35282;&#25968;&#25454;&#24212;&#29992;PCA&#36827;&#34892;&#38477;&#32500;&#26469;&#23545;HSI&#36827;&#34892;&#38477;&#32500;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Identifying the land cover category for each pixel in a hyperspectral image (HSI) relies on spectral and spatial information. An HSI cuboid with a specific patch size is utilized to extract spatial-spectral feature representation for the central pixel. In this article, we investigate that scene-specific but not essential correlations may be recorded in an HSI cuboid. This additional information improves the model performance on existing HSI datasets and makes it hard to properly evaluate the ability of a model. We refer to this problem as the spatial overfitting issue and utilize strict experimental settings to avoid it. We further propose a multiview transformer for HSI classification, which consists of multiview principal component analysis (MPCA), spectral encoder-decoder (SED), and spatial-pooling tokenization transformer (SPTT). MPCA performs dimension reduction on an HSI via constructing spectral multiview observations and applying PCA on each view data to extract low-dimensional
&lt;/p&gt;</description></item><item><title>rpcPRF&#26159;&#19968;&#31181;&#29992;&#20110;&#21355;&#26143;&#30456;&#26426;&#30340;&#21487;&#25512;&#24191;MPI&#31070;&#32463;&#36752;&#23556;&#22330;&#65292;&#36890;&#36807;&#37325;&#25237;&#24433;&#30417;&#30563;&#21644;&#36752;&#23556;&#22330;&#28210;&#26579;&#25216;&#26415;&#65292;&#20351;&#27169;&#22411;&#36866;&#29992;&#20110;&#21333;&#20010;&#25110;&#23569;&#37327;&#36755;&#20837;&#65292;&#24182;&#22312;&#26410;&#35265;&#22330;&#26223;&#30340;&#22270;&#20687;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2310.07179</link><description>&lt;p&gt;
rpcPRF: &#29992;&#20110;&#21355;&#26143;&#30456;&#26426;&#30340;&#21487;&#25512;&#24191;MPI&#31070;&#32463;&#36752;&#23556;&#22330;
&lt;/p&gt;
&lt;p&gt;
rpcPRF: Generalizable MPI Neural Radiance Field for Satellite Camera. (arXiv:2310.07179v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07179
&lt;/p&gt;
&lt;p&gt;
rpcPRF&#26159;&#19968;&#31181;&#29992;&#20110;&#21355;&#26143;&#30456;&#26426;&#30340;&#21487;&#25512;&#24191;MPI&#31070;&#32463;&#36752;&#23556;&#22330;&#65292;&#36890;&#36807;&#37325;&#25237;&#24433;&#30417;&#30563;&#21644;&#36752;&#23556;&#22330;&#28210;&#26579;&#25216;&#26415;&#65292;&#20351;&#27169;&#22411;&#36866;&#29992;&#20110;&#21333;&#20010;&#25110;&#23569;&#37327;&#36755;&#20837;&#65292;&#24182;&#22312;&#26410;&#35265;&#22330;&#26223;&#30340;&#22270;&#20687;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21355;&#26143;&#22270;&#20687;&#30340;&#26032;&#35270;&#35282;&#32508;&#21512;&#20855;&#26377;&#24191;&#27867;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#34429;&#28982;&#26368;&#36817;&#31070;&#32463;&#36752;&#23556;&#22330;&#30340;&#36827;&#23637;&#20027;&#35201;&#38024;&#23545;&#38024;&#23380;&#30456;&#26426;&#65292;&#32780;&#21355;&#26143;&#30456;&#26426;&#30340;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#36275;&#22815;&#30340;&#36755;&#20837;&#35270;&#22270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;rpcPRF&#65292;&#19968;&#31181;&#22522;&#20110;&#22810;&#24179;&#38754;&#22270;&#20687;(MPI)&#30340;&#26377;&#29702;&#22810;&#39033;&#24335;&#30456;&#26426;(RPC)&#30340;&#24179;&#38754;&#31070;&#32463;&#36752;&#23556;&#22330;&#12290;&#19982;&#38656;&#35201;&#36275;&#22815;&#22810;&#22330;&#26223;&#35270;&#22270;&#30340;&#22522;&#20110;&#22352;&#26631;&#30340;&#31070;&#32463;&#36752;&#23556;&#22330;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#36866;&#29992;&#20110;&#21333;&#20010;&#25110;&#23569;&#37327;&#36755;&#20837;&#65292;&#24182;&#22312;&#26410;&#35265;&#22330;&#26223;&#30340;&#22270;&#20687;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#20026;&#20102;&#23454;&#29616;&#36328;&#22330;&#26223;&#30340;&#27867;&#21270;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#37325;&#25237;&#24433;&#30417;&#30563;&#26469;&#35825;&#23548;&#39044;&#27979;&#30340;MPI&#23398;&#20064;3D&#22352;&#26631;&#19982;&#22270;&#20687;&#20043;&#38388;&#30340;&#27491;&#30830;&#20960;&#20309;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#36752;&#23556;&#22330;&#28210;&#26579;&#25216;&#26415;&#65292;&#28040;&#38500;&#20102;&#28145;&#24230;&#22810;&#35270;&#22270;&#31435;&#20307;&#26041;&#27861;&#23545;&#23494;&#38598;&#28145;&#24230;&#30417;&#30563;&#30340;&#20005;&#26684;&#35201;&#27714;&#12290;rpcPRF&#32467;&#21512;&#20102;&#38544;&#24335;&#34920;&#31034;&#30340;&#20248;&#21183;&#21644;&#28210;&#26579;&#25216;&#26415;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Novel view synthesis of satellite images holds a wide range of practical applications. While recent advances in the Neural Radiance Field have predominantly targeted pin-hole cameras, and models for satellite cameras often demand sufficient input views. This paper presents rpcPRF, a Multiplane Images (MPI) based Planar neural Radiance Field for Rational Polynomial Camera (RPC). Unlike coordinate-based neural radiance fields in need of sufficient views of one scene, our model is applicable to single or few inputs and performs well on images from unseen scenes. To enable generalization across scenes, we propose to use reprojection supervision to induce the predicted MPI to learn the correct geometry between the 3D coordinates and the images. Moreover, we remove the stringent requirement of dense depth supervision from deep multiview-stereo-based methods by introducing rendering techniques of radiance fields. rpcPRF combines the superiority of implicit representations and the advantages o
&lt;/p&gt;</description></item><item><title>&#22312;&#32447;&#25512;&#27979;&#35299;&#30721;&#26159;&#36890;&#36807;&#21033;&#29992;&#22810;&#20313;&#35745;&#31639;&#33021;&#21147;&#65292;&#22312;LLM&#26381;&#21153;&#38598;&#32676;&#20013;&#25345;&#32493;&#26356;&#26032;&#33609;&#31295;&#27169;&#22411;&#65292;&#20174;&#32780;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.07177</link><description>&lt;p&gt;
&#22312;&#32447;&#25512;&#27979;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Online Speculative Decoding. (arXiv:2310.07177v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07177
&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#25512;&#27979;&#35299;&#30721;&#26159;&#36890;&#36807;&#21033;&#29992;&#22810;&#20313;&#35745;&#31639;&#33021;&#21147;&#65292;&#22312;LLM&#26381;&#21153;&#38598;&#32676;&#20013;&#25345;&#32493;&#26356;&#26032;&#33609;&#31295;&#27169;&#22411;&#65292;&#20174;&#32780;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#27979;&#35299;&#30721;&#26159;&#36890;&#36807;&#21033;&#29992;&#36739;&#23567;&#30340;&#33609;&#31295;&#27169;&#22411;&#26469;&#39044;&#27979;&#30446;&#26631;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#20174;&#32780;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25512;&#29702;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#22312;&#38754;&#23545;&#22810;&#26679;&#30340;&#25991;&#26412;&#36755;&#20837;&#21644;&#33609;&#31295;&#27169;&#22411;&#19982;&#30446;&#26631;&#27169;&#22411;&#20043;&#38388;&#30340;&#26174;&#33879;&#33021;&#21147;&#24046;&#36317;&#26102;&#65292;&#20854;&#26377;&#25928;&#24615;&#21487;&#33021;&#21463;&#21040;&#38480;&#21046;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#22312;&#32447;&#25512;&#27979;&#35299;&#30721;&#65288;OSD&#65289;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#20854;&#20027;&#35201;&#24605;&#24819;&#26159;&#21033;&#29992;LLM&#26381;&#21153;&#38598;&#32676;&#20013;&#20016;&#23500;&#30340;&#22810;&#20313;&#35745;&#31639;&#33021;&#21147;&#65292;&#26681;&#25454;&#35266;&#23519;&#21040;&#30340;&#29992;&#25143;&#26597;&#35810;&#25968;&#25454;&#25345;&#32493;&#26356;&#26032;&#65288;&#22810;&#20010;&#65289;&#33609;&#31295;&#27169;&#22411;&#12290;&#30001;&#20110;LLM&#25512;&#29702;&#21463;&#20869;&#23384;&#38480;&#21046;&#65292;&#20856;&#22411;&#30340;LLM&#26381;&#21153;&#38598;&#32676;&#20013;&#30340;&#21097;&#20313;&#35745;&#31639;&#33021;&#21147;&#21487;&#20197;&#29992;&#20110;&#22312;&#32447;&#37325;&#26032;&#35757;&#32451;&#33609;&#31295;&#27169;&#22411;&#65292;&#20174;&#32780;&#20351;&#35757;&#32451;&#25104;&#26412;&#20445;&#25345;&#20013;&#24615;&#12290;&#30001;&#20110;LLM&#26381;&#21153;&#30340;&#26597;&#35810;&#20998;&#24067;&#30456;&#23545;&#31616;&#21333;&#65292;&#26681;&#25454;&#26597;&#35810;&#20998;&#24067;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#21487;&#20197;&#20351;&#33609;&#31295;&#27169;&#22411;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#30446;&#26631;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speculative decoding is a pivotal technique to accelerate the inference of large language models (LLMs) by employing a smaller draft model to predict the target model's outputs. However, its efficacy can be limited due to the low predictive accuracy of the draft model, particularly when faced with diverse text inputs and a significant capability gap between the draft and target models. We introduce online speculative decoding (OSD) to address this challenge. The main idea is to continually update (multiple) draft model(s) on observed user query data using the abundant excess computational power in an LLM serving cluster. Given that LLM inference is memory-bounded, the surplus computational power in a typical LLM serving cluster can be repurposed for online retraining of draft models, thereby making the training cost-neutral. Since the query distribution of an LLM service is relatively simple, retraining on query distribution enables the draft model to more accurately predict the target
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26053;&#34892;&#31363;&#36156;&#38382;&#39064;&#30340;&#21327;&#35843;&#26041;&#27861;&#65292;&#21435;&#25913;&#21892;&#22478;&#24066;&#36873;&#25321;&#21644;&#29289;&#21697;&#36873;&#25321;&#20043;&#38388;&#30340;&#21327;&#35843;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.07156</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#21327;&#35843;&#26041;&#27861;&#35299;&#20915;&#26053;&#34892;&#31363;&#36156;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Solving Travelling Thief Problems using Coordination Based Methods. (arXiv:2310.07156v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07156
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26053;&#34892;&#31363;&#36156;&#38382;&#39064;&#30340;&#21327;&#35843;&#26041;&#27861;&#65292;&#21435;&#25913;&#21892;&#22478;&#24066;&#36873;&#25321;&#21644;&#29289;&#21697;&#36873;&#25321;&#20043;&#38388;&#30340;&#21327;&#35843;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26053;&#34892;&#31363;&#36156;&#38382;&#39064;&#65288;TTP&#65289;&#26159;&#23454;&#38469;&#38382;&#39064;&#65288;&#22914;&#37038;&#20214;&#25910;&#38598;&#65289;&#30340;&#19968;&#20010;&#20195;&#29702;&#12290;TTP&#30001;&#26053;&#34892;&#25512;&#38144;&#21592;&#38382;&#39064;&#65288;TSP&#65289;&#21644;&#32972;&#21253;&#38382;&#39064;&#65288;KP&#65289;&#30340;&#32452;&#21512;&#26500;&#25104;&#65292;&#22240;&#20026;KP&#30340;&#29289;&#21697;&#20998;&#25955;&#22312;TSP&#30340;&#22478;&#24066;&#20013;&#65292;&#23567;&#20599;&#24517;&#39035;&#35775;&#38382;&#22478;&#24066;&#26469;&#25910;&#38598;&#29289;&#21697;&#12290;&#22312;TTP&#20013;&#65292;&#22478;&#24066;&#36873;&#25321;&#21644;&#29289;&#21697;&#36873;&#25321;&#20915;&#31574;&#38656;&#35201;&#32039;&#23494;&#21327;&#35843;&#65292;&#22240;&#20026;&#23567;&#20599;&#30340;&#26053;&#34892;&#36895;&#24230;&#21462;&#20915;&#20110;&#32972;&#21253;&#30340;&#37325;&#37327;&#65292;&#35775;&#38382;&#22478;&#24066;&#30340;&#39034;&#24207;&#20250;&#24433;&#21709;&#29289;&#21697;&#25910;&#38598;&#30340;&#39034;&#24207;&#12290;&#29616;&#26377;&#30340;TTP&#27714;&#35299;&#22120;&#23558;&#22478;&#24066;&#36873;&#25321;&#21644;&#29289;&#21697;&#36873;&#25321;&#20998;&#24320;&#22788;&#29702;&#65292;&#20445;&#25345;&#19968;&#31181;&#31867;&#22411;&#30340;&#20915;&#31574;&#19981;&#21464;&#65292;&#32780;&#22788;&#29702;&#21478;&#19968;&#31181;&#31867;&#22411;&#12290;&#36825;&#31181;&#20998;&#31163;&#23454;&#38469;&#19978;&#24847;&#21619;&#30528;&#20004;&#31181;&#20915;&#31574;&#20043;&#38388;&#30340;&#21327;&#35843;&#24456;&#24046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#22312;TTP&#20013;&#19968;&#20010;&#31616;&#21333;&#30340;&#22522;&#20110;&#23616;&#37096;&#25628;&#32034;&#30340;&#21327;&#35843;&#26041;&#27861;&#19981;&#36215;&#20316;&#29992;&#12290;&#28982;&#21518;&#65292;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20154;&#24037;&#35774;&#35745;&#30340;&#21327;&#35843;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#22312;&#24320;&#21457;&#26399;&#38388;&#26356;&#25913;&#25910;&#38598;&#35745;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
A travelling thief problem (TTP) is a proxy to real-life problems such as postal collection. TTP comprises an entanglement of a travelling salesman problem (TSP) and a knapsack problem (KP) since items of KP are scattered over cities of TSP, and a thief has to visit cities to collect items. In TTP, city selection and item selection decisions need close coordination since the thief's travelling speed depends on the knapsack's weight and the order of visiting cities affects the order of item collection. Existing TTP solvers deal with city selection and item selection separately, keeping decisions for one type unchanged while dealing with the other type. This separation essentially means very poor coordination between two types of decision. In this paper, we first show that a simple local search based coordination approach does not work in TTP. Then, to address the aforementioned problems, we propose a human designed coordination heuristic that makes changes to collection plans during exp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;TEE&#20445;&#25252;&#19979;&#35774;&#22791;&#31471;&#26426;&#22120;&#23398;&#20064;&#30340;&#23433;&#20840;&#24615;&#38382;&#39064;&#12290;&#36890;&#36807;&#23545;&#29616;&#26377;&#30340;TSDP&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#35780;&#20272;&#65292;&#21457;&#29616;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#23481;&#26131;&#21463;&#21040;&#38544;&#31169;&#31363;&#21462;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.07152</link><description>&lt;p&gt;
&#35774;&#22791;&#22806;&#27809;&#26377;&#38544;&#31169;&#65306;&#20851;&#20110;TEE&#20445;&#25252;&#19979;&#30340;&#35774;&#22791;&#31471;&#26426;&#22120;&#23398;&#20064;&#30340;&#65288;&#19981;&#65289;&#23433;&#20840;&#24615;
&lt;/p&gt;
&lt;p&gt;
No Privacy Left Outside: On the (In-)Security of TEE-Shielded DNN Partition for On-Device ML. (arXiv:2310.07152v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07152
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;TEE&#20445;&#25252;&#19979;&#35774;&#22791;&#31471;&#26426;&#22120;&#23398;&#20064;&#30340;&#23433;&#20840;&#24615;&#38382;&#39064;&#12290;&#36890;&#36807;&#23545;&#29616;&#26377;&#30340;TSDP&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#35780;&#20272;&#65292;&#21457;&#29616;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#23481;&#26131;&#21463;&#21040;&#38544;&#31169;&#31363;&#21462;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#22791;&#31471;&#26426;&#22120;&#23398;&#20064;&#24341;&#20837;&#20102;&#26032;&#30340;&#23433;&#20840;&#25361;&#25112;&#65306;DNN&#27169;&#22411;&#21487;&#20197;&#34987;&#35774;&#22791;&#29992;&#25143;&#30333;&#30418;&#35775;&#38382;&#12290;&#22522;&#20110;&#30333;&#30418;&#20449;&#24687;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36827;&#34892;&#26377;&#25928;&#30340;&#27169;&#22411;&#31363;&#21462;&#21644;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#12290;&#20351;&#29992;&#21487;&#20449;&#25191;&#34892;&#29615;&#22659;&#65288;TEE&#65289;&#26469;&#20445;&#25252;&#35774;&#22791;&#31471;&#30340;DNN&#27169;&#22411;&#26088;&#22312;&#23558;&#65288;&#26131;&#20110;&#36827;&#34892;&#30340;&#65289;&#30333;&#30418;&#25915;&#20987;&#38477;&#32423;&#20026;&#65288;&#26356;&#38590;&#36827;&#34892;&#30340;&#65289;&#40657;&#30418;&#25915;&#20987;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#20027;&#35201;&#30340;&#32570;&#28857;&#26159;&#22823;&#22823;&#22686;&#21152;&#20102;&#24310;&#36831;&#65288;&#39640;&#36798;50&#20493;&#65289;&#12290;&#20026;&#20102;&#21152;&#36895;&#20351;&#29992;GPU&#36827;&#34892;TEE&#20445;&#25252;&#30340;DNN&#35745;&#31639;&#65292;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#20960;&#31181;&#27169;&#22411;&#20998;&#21306;&#25216;&#26415;&#12290;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#34987;&#31216;&#20026;TEE&#20445;&#25252;&#30340;DNN&#20998;&#21306;&#65288;TSDP&#65289;&#65292;&#23558;DNN&#27169;&#22411;&#20998;&#20026;&#20004;&#20010;&#37096;&#20998;&#65292;&#23558;&#38544;&#31169;&#19981;&#25935;&#24863;&#30340;&#37096;&#20998;&#21368;&#36733;&#21040;GPU&#19978;&#65292;&#21516;&#26102;&#23558;&#38544;&#31169;&#25935;&#24863;&#30340;&#37096;&#20998;&#20445;&#25252;&#22312;TEE&#20869;&#12290;&#26412;&#25991;&#35780;&#20272;&#20102;&#29616;&#26377;&#30340;TSDP&#35299;&#20915;&#26041;&#26696;&#22312;&#21508;&#31181;DNN&#27169;&#22411;&#12289;&#25968;&#25454;&#38598;&#21644;&#25351;&#26631;&#19978;&#36827;&#34892;&#20102;&#27169;&#22411;&#31363;&#21462;&#21644;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#30340;&#23454;&#39564;&#12290;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;TSDP&#35299;&#20915;&#26041;&#26696;&#23481;&#26131;&#21463;&#21040;&#38544;&#31169;&#31363;&#21462;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
On-device ML introduces new security challenges: DNN models become white-box accessible to device users. Based on white-box information, adversaries can conduct effective model stealing (MS) and membership inference attack (MIA). Using Trusted Execution Environments (TEEs) to shield on-device DNN models aims to downgrade (easy) white-box attacks to (harder) black-box attacks. However, one major shortcoming is the sharply increased latency (up to 50X). To accelerate TEE-shield DNN computation with GPUs, researchers proposed several model partition techniques. These solutions, referred to as TEE-Shielded DNN Partition (TSDP), partition a DNN model into two parts, offloading the privacy-insensitive part to the GPU while shielding the privacy-sensitive part within the TEE. This paper benchmarks existing TSDP solutions using both MS and MIA across a variety of DNN models, datasets, and metrics. We show important findings that existing TSDP solutions are vulnerable to privacy-stealing attack
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35752;&#35770;&#20102;&#22312;&#36873;&#20030;&#20013;&#19968;&#20123;&#36873;&#31080;&#32570;&#24109;&#24773;&#20917;&#19979;&#30830;&#23450;&#20505;&#36873;&#20154;&#26159;&#21542;&#21487;&#20197;&#33719;&#32988;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#30740;&#31350;&#39030;&#37096;&#25130;&#26029;&#26102;&#19981;&#21516;&#30340;&#36873;&#31080;&#35268;&#21017;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#36825;&#20010;&#38382;&#39064;&#30340;&#22797;&#26434;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#22312;&#29305;&#27530;&#24773;&#20917;&#19979;&#21487;&#20197;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#35745;&#31639;&#20986;&#32467;&#26524;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.07150</link><description>&lt;p&gt;
&#22312;&#32570;&#24109;&#25237;&#31080;&#30340;&#36873;&#20030;&#20013;&#30830;&#23450;&#33719;&#32988;&#32773;
&lt;/p&gt;
&lt;p&gt;
Determining Winners in Elections with Absent Votes. (arXiv:2310.07150v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07150
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35752;&#35770;&#20102;&#22312;&#36873;&#20030;&#20013;&#19968;&#20123;&#36873;&#31080;&#32570;&#24109;&#24773;&#20917;&#19979;&#30830;&#23450;&#20505;&#36873;&#20154;&#26159;&#21542;&#21487;&#20197;&#33719;&#32988;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#30740;&#31350;&#39030;&#37096;&#25130;&#26029;&#26102;&#19981;&#21516;&#30340;&#36873;&#31080;&#35268;&#21017;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#36825;&#20010;&#38382;&#39064;&#30340;&#22797;&#26434;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#22312;&#29305;&#27530;&#24773;&#20917;&#19979;&#21487;&#20197;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#35745;&#31639;&#20986;&#32467;&#26524;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36873;&#20030;&#20013;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#26159;&#30830;&#23450;&#22312;&#26576;&#20123;&#36873;&#31080;&#32570;&#24109;&#26102;&#20505;&#20505;&#36873;&#20154;&#26159;&#21542;&#21487;&#20197;&#33719;&#32988;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#24403;&#36873;&#31080;&#26159;&#39030;&#37096;&#25130;&#26029;&#26102;&#65292;&#36890;&#36807;&#32570;&#24109;&#25237;&#31080;&#30830;&#23450;&#33719;&#32988;&#32773;&#65288;WAV&#65289;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#21333;&#19968;&#21487;&#36716;&#25442;&#36873;&#31080;&#12289;&#26368;&#23567;&#20540;&#21644;Copeland&#65292;WAV&#38382;&#39064;&#37117;&#26159;NP-complete&#30340;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#27530;&#24773;&#20917;&#19979;&#30340;&#20301;&#32622;&#35780;&#20998;&#35268;&#21017;&#65292;&#36825;&#26679;&#38382;&#39064;&#21487;&#20197;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#35745;&#31639;&#20986;&#26469;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#22312;&#39030;&#37096;&#25130;&#26029;&#25490;&#24207;&#20013;&#19982;&#23436;&#25972;&#25490;&#24207;&#30340;&#32467;&#26524;&#19981;&#21516;&#65292;&#22240;&#20026;&#24403;&#20505;&#36873;&#20154;&#25968;&#25110;&#32570;&#24109;&#36873;&#31080;&#25968;&#26377;&#38480;&#26102;&#65292;&#23427;&#20204;&#30340;&#38590;&#24230;&#32467;&#26524;&#20173;&#28982;&#25104;&#31435;&#65292;&#32780;&#25105;&#20204;&#35777;&#26126;&#36825;&#20010;&#38382;&#39064;&#21487;&#20197;&#22312;&#36825;&#20004;&#31181;&#24773;&#20917;&#19979;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;
An important question in elections is the determine whether a candidate can be a winner when some votes are absent. We study this determining winner with the absent votes (WAV) problem when the votes are top-truncated. We show that the WAV problem is NP-complete for the single transferable vote, Maximin, and Copeland, and propose a special case of positional scoring rule such that the problem can be computed in polynomial time. Our results in top-truncated rankings differ from the results in full rankings as their hardness results still hold when the number of candidates or the number of missing votes are bounded, while we show that the problem can be solved in polynomial time in either case.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21435;&#22122;&#20219;&#21153;&#36335;&#30001;&#30340;&#31574;&#30053;&#65292;&#36890;&#36807;&#20026;&#25193;&#25955;&#27169;&#22411;&#30340;&#19981;&#21516;&#20219;&#21153;&#24314;&#31435;&#29420;&#31435;&#30340;&#20449;&#24687;&#36335;&#24452;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#26126;&#30830;&#32435;&#20837;&#12290;&#35813;&#26041;&#27861;&#23558;&#21435;&#22122;&#20219;&#21153;&#30340;&#20808;&#39564;&#30693;&#35782;&#26080;&#32541;&#38598;&#25104;&#21040;&#26694;&#26550;&#20013;&#65292;&#36890;&#36807;&#28608;&#27963;&#30456;&#20284;&#30340;&#36890;&#36947;&#21644;&#28369;&#21160;&#31383;&#21475;&#30340;&#26041;&#24335;&#65292;&#20805;&#20998;&#21033;&#29992;&#20102;&#30456;&#37051;&#26102;&#38388;&#27493;&#20219;&#21153;&#38388;&#30340;&#20146;&#21644;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2310.07138</link><description>&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#30340;&#21435;&#22122;&#20219;&#21153;&#36335;&#30001;
&lt;/p&gt;
&lt;p&gt;
Denoising Task Routing for Diffusion Models. (arXiv:2310.07138v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07138
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21435;&#22122;&#20219;&#21153;&#36335;&#30001;&#30340;&#31574;&#30053;&#65292;&#36890;&#36807;&#20026;&#25193;&#25955;&#27169;&#22411;&#30340;&#19981;&#21516;&#20219;&#21153;&#24314;&#31435;&#29420;&#31435;&#30340;&#20449;&#24687;&#36335;&#24452;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#26126;&#30830;&#32435;&#20837;&#12290;&#35813;&#26041;&#27861;&#23558;&#21435;&#22122;&#20219;&#21153;&#30340;&#20808;&#39564;&#30693;&#35782;&#26080;&#32541;&#38598;&#25104;&#21040;&#26694;&#26550;&#20013;&#65292;&#36890;&#36807;&#28608;&#27963;&#30456;&#20284;&#30340;&#36890;&#36947;&#21644;&#28369;&#21160;&#31383;&#21475;&#30340;&#26041;&#24335;&#65292;&#20805;&#20998;&#21033;&#29992;&#20102;&#30456;&#37051;&#26102;&#38388;&#27493;&#20219;&#21153;&#38388;&#30340;&#20146;&#21644;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#36890;&#36807;&#23398;&#20064;&#22810;&#27493;&#21435;&#22122;&#36807;&#31243;&#29983;&#25104;&#39640;&#24230;&#36924;&#30495;&#30340;&#22270;&#20687;&#65292;&#33258;&#28982;&#22320;&#20307;&#29616;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#30340;&#21407;&#29702;&#12290;&#23613;&#31649;&#25193;&#25955;&#27169;&#22411;&#21644;MTL&#20043;&#38388;&#23384;&#22312;&#22266;&#26377;&#30340;&#36830;&#25509;&#65292;&#20294;&#22312;&#35774;&#35745;&#26126;&#30830;&#23558;MTL&#32435;&#20837;&#25193;&#25955;&#27169;&#22411;&#26694;&#26550;&#30340;&#31070;&#32463;&#32467;&#26500;&#26041;&#38754;&#20173;&#23384;&#22312;&#19968;&#20010;&#26410;&#34987;&#25506;&#32034;&#30340;&#39046;&#22495;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21435;&#22122;&#20219;&#21153;&#36335;&#30001;&#65288;DTR&#65289;&#65292;&#19968;&#31181;&#23545;&#29616;&#26377;&#25193;&#25955;&#27169;&#22411;&#26550;&#26500;&#36827;&#34892;&#31616;&#21333;&#38468;&#21152;&#30340;&#31574;&#30053;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#22320;&#28608;&#27963;&#27169;&#22411;&#20013;&#30340;&#23376;&#36890;&#36947;&#26469;&#20026;&#21333;&#20010;&#20219;&#21153;&#24314;&#31435;&#29420;&#31435;&#30340;&#20449;&#24687;&#36335;&#24452;&#12290;DTR&#30340;&#29305;&#21035;&#21560;&#24341;&#20154;&#20043;&#22788;&#22312;&#20110;&#23427;&#23558;&#21435;&#22122;&#20219;&#21153;&#30340;&#20808;&#39564;&#30693;&#35782;&#26080;&#32541;&#38598;&#25104;&#21040;&#26694;&#26550;&#20013;&#65306;&#65288;1&#65289;&#20219;&#21153;&#20146;&#21644;&#24615;&#65306;DTR&#20026;&#30456;&#37051;&#26102;&#38388;&#27493;&#30340;&#20219;&#21153;&#28608;&#27963;&#30456;&#20284;&#30340;&#36890;&#36947;&#65292;&#24182;&#23558;&#28608;&#27963;&#30340;&#36890;&#36947;&#20316;&#20026;&#28369;&#21160;&#31383;&#21475;&#36890;&#36807;&#26102;&#38388;&#27493;&#36827;&#34892;&#31227;&#21160;&#65292;&#21033;&#29992;&#30456;&#37051;&#26102;&#38388;&#27493;&#20219;&#21153;&#38388;&#22266;&#26377;&#30340;&#24378;&#20146;&#21644;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models generate highly realistic images through learning a multi-step denoising process, naturally embodying the principles of multi-task learning (MTL). Despite the inherent connection between diffusion models and MTL, there remains an unexplored area in designing neural architectures that explicitly incorporate MTL into the framework of diffusion models. In this paper, we present Denoising Task Routing (DTR), a simple add-on strategy for existing diffusion model architectures to establish distinct information pathways for individual tasks within a single architecture by selectively activating subsets of channels in the model. What makes DTR particularly compelling is its seamless integration of prior knowledge of denoising tasks into the framework: (1) Task Affinity: DTR activates similar channels for tasks at adjacent timesteps and shifts activated channels as sliding windows through timesteps, capitalizing on the inherent strong affinity between tasks at adjacent timestep
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#20154;&#31867;&#21453;&#39304;&#30340;&#38750;&#31574;&#30053;&#35780;&#20272;&#65288;OPEHF&#65289;&#26694;&#26550;&#65292;&#21487;&#20197;&#20934;&#30830;&#35780;&#20272;&#20154;&#31867;&#21453;&#39304;&#20449;&#21495;&#12290;&#36825;&#20010;&#26694;&#26550;&#35299;&#20915;&#20102;&#29616;&#26377;OPE&#26041;&#27861;&#22312;&#20272;&#35745;&#20154;&#31867;&#21453;&#39304;&#20449;&#21495;&#19978;&#30340;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2310.07123</link><description>&lt;p&gt;
&#29992;&#20110;&#20154;&#31867;&#21453;&#39304;&#30340;&#38750;&#31574;&#30053;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Off-Policy Evaluation for Human Feedback. (arXiv:2310.07123v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07123
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#20154;&#31867;&#21453;&#39304;&#30340;&#38750;&#31574;&#30053;&#35780;&#20272;&#65288;OPEHF&#65289;&#26694;&#26550;&#65292;&#21487;&#20197;&#20934;&#30830;&#35780;&#20272;&#20154;&#31867;&#21453;&#39304;&#20449;&#21495;&#12290;&#36825;&#20010;&#26694;&#26550;&#35299;&#20915;&#20102;&#29616;&#26377;OPE&#26041;&#27861;&#22312;&#20272;&#35745;&#20154;&#31867;&#21453;&#39304;&#20449;&#21495;&#19978;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#31574;&#30053;&#35780;&#20272;&#65288;OPE&#65289;&#23545;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#31163;&#32447;&#35757;&#32451;&#21644;&#35780;&#20272;&#20043;&#38388;&#30340;&#24046;&#36317;&#30340;&#32553;&#23567;&#38750;&#24120;&#37325;&#35201;&#65292;&#23427;&#36890;&#36807;&#20165;&#20351;&#29992;&#31163;&#32447;&#36712;&#36857;&#20272;&#35745;&#30446;&#26631;&#65288;&#35780;&#20272;&#65289;&#31574;&#30053;&#30340;&#24615;&#33021;&#21644;/&#25110;&#25490;&#21517;&#12290;&#23427;&#21487;&#20197;&#25552;&#39640;&#25968;&#25454;&#25910;&#38598;&#21644;&#31574;&#30053;&#27979;&#35797;&#36807;&#31243;&#30340;&#23433;&#20840;&#24615;&#21644;&#25928;&#29575;&#65292;&#22312;&#22312;&#32447;&#37096;&#32626;&#25104;&#26412;&#36739;&#39640;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#21307;&#30103;&#20445;&#20581;&#20013;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;OPE&#26041;&#27861;&#22312;&#20272;&#35745;&#20154;&#31867;&#21453;&#39304;&#65288;HF&#65289;&#20449;&#21495;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#65292;&#22240;&#20026;HF&#21487;&#33021;&#20250;&#21463;&#21040;&#22810;&#20010;&#28508;&#22312;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#32780;&#19988;&#21482;&#26159;&#31232;&#30095;&#21487;&#29992;&#30340;&#65307;&#32780;&#19981;&#21516;&#20110;&#20195;&#29702;&#23450;&#20041;&#30340;&#29615;&#22659;&#22870;&#21169;&#65288;&#29992;&#20110;&#31574;&#30053;&#20248;&#21270;&#65289;&#65292;&#29615;&#22659;&#22870;&#21169;&#36890;&#24120;&#26159;&#22312;&#21442;&#25968;&#20989;&#25968;&#25110;&#20998;&#24067;&#19978;&#20915;&#23450;&#30340;&#12290;&#22240;&#27492;&#65292;&#30001;&#20110;HF&#20449;&#21495;&#30340;&#24615;&#36136;&#65292;&#20934;&#30830;&#22320;&#25512;&#26029;OPE&#20272;&#35745;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#29992;&#20110;HF&#30340;OPE&#26694;&#26550;&#65292;&#23427;&#37325;&#26032;&#20351;&#29992;&#29616;&#26377;&#30340;OPE&#26041;&#27861;&#65292;&#20197;&#20934;&#30830;&#35780;&#20272;HF&#20449;&#21495;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26041;&#27861;&#26469;
&lt;/p&gt;
&lt;p&gt;
Off-policy evaluation (OPE) is important for closing the gap between offline training and evaluation of reinforcement learning (RL), by estimating performance and/or rank of target (evaluation) policies using offline trajectories only. It can improve the safety and efficiency of data collection and policy testing procedures in situations where online deployments are expensive, such as healthcare. However, existing OPE methods fall short in estimating human feedback (HF) signals, as HF may be conditioned over multiple underlying factors and is only sparsely available; as opposed to the agent-defined environmental rewards (used in policy optimization), which are usually determined over parametric functions or distributions. Consequently, the nature of HF signals makes extrapolating accurate OPE estimations to be challenging. To resolve this, we introduce an OPE for HF (OPEHF) framework that revives existing OPE methods in order to accurately evaluate the HF signals. Specifically, we deve
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#28145;&#24230;&#35821;&#35328;&#27169;&#22411;(DLMs)&#30340;&#20998;&#23618;&#32467;&#26500;&#19982;&#20154;&#33041;&#35821;&#35328;&#29702;&#35299;&#30340;&#26102;&#38388;&#21160;&#24577;&#20043;&#38388;&#23384;&#22312;&#24378;&#30456;&#20851;&#24615;&#65292;&#36890;&#36807;&#37319;&#29992;&#30005;&#23376;&#30382;&#23618;&#22270;(ECoG)&#25968;&#25454;&#26469;&#20248;&#21270;&#26102;&#38388;&#20998;&#36776;&#29575;&#65292;&#20026;&#28145;&#20837;&#20102;&#35299;&#20154;&#33041;&#35821;&#35328;&#22788;&#29702;&#26426;&#21046;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;</title><link>http://arxiv.org/abs/2310.07106</link><description>&lt;p&gt;
&#12298;&#20154;&#33041;&#20013;&#35821;&#35328;&#22788;&#29702;&#30340;&#26102;&#24577;&#32467;&#26500;&#19982;&#28145;&#24230;&#35821;&#35328;&#27169;&#22411;&#30340;&#23618;&#27425;&#32467;&#26500;&#30456;&#31526;&#12299;
&lt;/p&gt;
&lt;p&gt;
The Temporal Structure of Language Processing in the Human Brain Corresponds to The Layered Hierarchy of Deep Language Models. (arXiv:2310.07106v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07106
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#28145;&#24230;&#35821;&#35328;&#27169;&#22411;(DLMs)&#30340;&#20998;&#23618;&#32467;&#26500;&#19982;&#20154;&#33041;&#35821;&#35328;&#29702;&#35299;&#30340;&#26102;&#38388;&#21160;&#24577;&#20043;&#38388;&#23384;&#22312;&#24378;&#30456;&#20851;&#24615;&#65292;&#36890;&#36807;&#37319;&#29992;&#30005;&#23376;&#30382;&#23618;&#22270;(ECoG)&#25968;&#25454;&#26469;&#20248;&#21270;&#26102;&#38388;&#20998;&#36776;&#29575;&#65292;&#20026;&#28145;&#20837;&#20102;&#35299;&#20154;&#33041;&#35821;&#35328;&#22788;&#29702;&#26426;&#21046;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#35821;&#35328;&#27169;&#22411;(DLMs)&#25552;&#20379;&#20102;&#19968;&#31181;&#29702;&#35299;&#20154;&#33041;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26426;&#21046;&#30340;&#26032;&#30340;&#35745;&#31639;&#33539;&#24335;&#12290;&#19982;&#20256;&#32479;&#30340;&#24515;&#29702;&#35821;&#35328;&#23398;&#27169;&#22411;&#19981;&#21516;&#65292;DLMs&#20351;&#29992;&#20998;&#23618;&#30340;&#36830;&#32493;&#25968;&#20540;&#21521;&#37327;&#24207;&#21015;&#26469;&#34920;&#31034;&#21333;&#35789;&#21644;&#19978;&#19979;&#25991;&#65292;&#20351;&#24471;&#35832;&#22810;&#26032;&#30340;&#24212;&#29992;&#25104;&#20026;&#21487;&#33021;&#65292;&#22914;&#31867;&#20154;&#29983;&#25104;&#25991;&#26412;&#12290;&#26412;&#25991;&#36890;&#36807;&#23637;&#31034;DLMs&#30340;&#20998;&#23618;&#32467;&#26500;&#21487;&#33021;&#29992;&#20110;&#27169;&#25311;&#22823;&#33041;&#35821;&#35328;&#29702;&#35299;&#30340;&#26102;&#38388;&#21160;&#24577;&#65292;&#35777;&#26126;&#20102;DLM&#23618;&#27425;&#28145;&#24230;&#19982;&#26368;&#33021;&#39044;&#27979;&#20154;&#33041;&#30340;&#23618;&#27425;&#26102;&#38388;&#20043;&#38388;&#30340;&#24378;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#20043;&#25152;&#20197;&#33021;&#22815;&#22312;&#26102;&#38388;&#19978;&#35299;&#26512;&#20986;&#27599;&#20010;&#23618;&#27425;&#30340;&#20248;&#21183;&#22312;&#20110;&#25105;&#20204;&#37319;&#29992;&#20102;&#30005;&#23376;&#30382;&#36136;&#22270;(ECoG)&#25968;&#25454;&#65292;&#20854;&#26102;&#38388;&#20998;&#36776;&#29575;&#27604;&#21151;&#33021;&#24615;&#30913;&#20849;&#25391;&#25104;&#20687;(fMRI)&#31561;&#26080;&#25439;&#27979;&#37327;&#26041;&#27861;&#26356;&#39640;&#12290;&#25105;&#20204;&#21033;&#29992;ECoG&#20174;&#21442;&#19982;&#32773;&#21548;30&#20998;&#38047;&#21465;&#36848;&#26102;&#35760;&#24405;&#31070;&#32463;&#27963;&#21160;&#65292;&#21516;&#26102;&#23558;&#30456;&#21516;&#21465;&#36848;&#25552;&#20379;&#32473;&#39640;&#24615;&#33021;DLM(GPT2-XL)&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Language Models (DLMs) provide a novel computational paradigm for understanding the mechanisms of natural language processing in the human brain. Unlike traditional psycholinguistic models, DLMs use layered sequences of continuous numerical vectors to represent words and context, allowing a plethora of emerging applications such as human-like text generation. In this paper we show evidence that the layered hierarchy of DLMs may be used to model the temporal dynamics of language comprehension in the brain by demonstrating a strong correlation between DLM layer depth and the time at which layers are most predictive of the human brain. Our ability to temporally resolve individual layers benefits from our use of electrocorticography (ECoG) data, which has a much higher temporal resolution than noninvasive methods like fMRI. Using ECoG, we record neural activity from participants listening to a 30-minute narrative while also feeding the same narrative to a high-performing DLM (GPT2-XL)
&lt;/p&gt;</description></item><item><title>ClausewitzGPT&#26694;&#26550;&#25552;&#20379;&#20102;&#24212;&#23545;&#20449;&#24687;&#25805;&#20316;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#39118;&#38505;&#30340;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#24182;&#24378;&#35843;&#20102;&#33258;&#20027;AI&#20195;&#29702;&#20154;&#22312;&#20854;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.07099</link><description>&lt;p&gt;
ClausewitzGPT&#26694;&#26550;&#65306;&#29702;&#35770;&#19978;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#20449;&#24687;&#25805;&#20316;&#30340;&#26032;&#21069;&#27839;
&lt;/p&gt;
&lt;p&gt;
ClausewitzGPT Framework: A New Frontier in Theoretical Large Language Model Enhanced Information Operations. (arXiv:2310.07099v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07099
&lt;/p&gt;
&lt;p&gt;
ClausewitzGPT&#26694;&#26550;&#25552;&#20379;&#20102;&#24212;&#23545;&#20449;&#24687;&#25805;&#20316;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#39118;&#38505;&#30340;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#24182;&#24378;&#35843;&#20102;&#33258;&#20027;AI&#20195;&#29702;&#20154;&#22312;&#20854;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#23383;&#26102;&#20195;&#65292;&#32593;&#32476;&#31354;&#38388;&#25104;&#20026;&#22320;&#32536;&#25919;&#27835;&#20105;&#31471;&#30340;&#26032;&#20852;&#26680;&#24515;&#30340;&#32972;&#26223;&#19979;&#65292;&#20449;&#24687;&#25805;&#20316;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#34701;&#21512;&#26631;&#24535;&#30528;&#19968;&#20010;&#33539;&#24335;&#36716;&#21464;&#65292;&#20805;&#28385;&#20102;&#24040;&#22823;&#30340;&#26426;&#36935;&#21644;&#22797;&#26434;&#30340;&#25361;&#25112;&#12290;&#38543;&#30528;Mistral 7B LLM&#65288;Mistral&#65292;2023&#65289;&#31561;&#24037;&#20855;&#20351;&#24471;&#35775;&#38382;LLM&#33021;&#21147;&#65288;Jin et al.&#65292;2023&#65289;&#27665;&#20027;&#21270;&#65292;&#20174;&#20027;&#26435;&#22269;&#23478;&#21040;&#27969;&#27667;&#23454;&#20307;&#65288;Howard et al.&#65292;2023&#65289;&#65292;&#19968;&#31995;&#21015;&#34892;&#20026;&#32773;&#37197;&#22791;&#20102;&#24378;&#22823;&#30340;&#25925;&#20107;&#22609;&#36896;&#24037;&#20855;&#65288;Goldstein et al.&#65292;2023&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#23548;&#33322;&#36825;&#20010;&#23853;&#26032;&#19990;&#30028;&#30340;&#8220;ClausewitzGPT&#8221;&#26041;&#31243;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#26500;&#24819;&#19981;&#20165;&#26088;&#22312;&#37327;&#21270;&#26426;&#22120;&#36895;&#24230;&#30340;LLM&#22686;&#24378;&#25805;&#20316;&#20013;&#22266;&#26377;&#30340;&#39118;&#38505;&#65292;&#36824;&#24378;&#35843;&#20102;&#33258;&#20027;AI&#20195;&#29702;&#20154;&#65288;Wang, Xie, et al., 2023&#65289;&#30340;&#20851;&#38190;&#35282;&#33394;&#12290;&#36825;&#20123;&#20195;&#29702;&#20154;&#20307;&#29616;&#20102;&#20262;&#29702;&#32771;&#34385;&#65288;Hendrycks et al.&#65292;2021&#65289;&#65292;&#25104;&#20026;&#19981;&#21487;&#25110;&#32570;&#30340;&#32452;&#25104;&#37096;&#20998;&#65288;Wang, Ma, et al.&#65292;2023&#65289;&#65292;&#30830;&#20445;&#25105;&#20204;&#21521;&#21069;&#36808;&#36827;&#30340;&#21516;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
In a digital epoch where cyberspace is the emerging nexus of geopolitical contention, the melding of information operations and Large Language Models (LLMs) heralds a paradigm shift, replete with immense opportunities and intricate challenges. As tools like the Mistral 7B LLM (Mistral, 2023) democratise access to LLM capabilities (Jin et al., 2023), a vast spectrum of actors, from sovereign nations to rogue entities (Howard et al., 2023), find themselves equipped with potent narrative-shaping instruments (Goldstein et al., 2023). This paper puts forth a framework for navigating this brave new world in the "ClausewitzGPT" equation. This novel formulation not only seeks to quantify the risks inherent in machine-speed LLM-augmented operations but also underscores the vital role of autonomous AI agents (Wang, Xie, et al., 2023). These agents, embodying ethical considerations (Hendrycks et al., 2021), emerge as indispensable components (Wang, Ma, et al., 2023), ensuring that as we race forw
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#31232;&#30095;&#36890;&#29992;Transformer&#65288;SUT&#65289;&#65292;&#23427;&#36890;&#36807;&#21033;&#29992;&#31232;&#30095;&#28151;&#21512;&#19987;&#23478;&#65288;SMoE&#65289;&#21644;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20999;&#26829;&#27861;&#30340;&#21160;&#24577;&#20572;&#27490;&#26426;&#21046;&#26469;&#20943;&#23569;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#21442;&#25968;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;SUT&#22312;&#24418;&#24335;&#35821;&#35328;&#20219;&#21153;&#19978;&#20855;&#26377;&#19982;&#24378;&#22522;&#32447;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#24182;&#33021;&#26174;&#33879;&#38477;&#20302;&#35745;&#31639;&#36164;&#28304;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.07096</link><description>&lt;p&gt;
&#31232;&#30095;&#36890;&#29992;Transformer
&lt;/p&gt;
&lt;p&gt;
Sparse Universal Transformer. (arXiv:2310.07096v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07096
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#31232;&#30095;&#36890;&#29992;Transformer&#65288;SUT&#65289;&#65292;&#23427;&#36890;&#36807;&#21033;&#29992;&#31232;&#30095;&#28151;&#21512;&#19987;&#23478;&#65288;SMoE&#65289;&#21644;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20999;&#26829;&#27861;&#30340;&#21160;&#24577;&#20572;&#27490;&#26426;&#21046;&#26469;&#20943;&#23569;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#21442;&#25968;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;SUT&#22312;&#24418;&#24335;&#35821;&#35328;&#20219;&#21153;&#19978;&#20855;&#26377;&#19982;&#24378;&#22522;&#32447;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#24182;&#33021;&#26174;&#33879;&#38477;&#20302;&#35745;&#31639;&#36164;&#28304;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#29992;Transformer&#65288;UT&#65289;&#26159;Transformer&#30340;&#19968;&#31181;&#21464;&#20307;&#65292;&#20854;&#22312;&#21508;&#23618;&#20043;&#38388;&#20849;&#20139;&#21442;&#25968;&#12290;&#23454;&#35777;&#35777;&#25454;&#34920;&#26126;&#65292;&#22312;&#24418;&#24335;&#35821;&#35328;&#20219;&#21153;&#20013;&#65292;UT&#27604;Vanilla Transformer&#65288;VT&#65289;&#20855;&#26377;&#26356;&#22909;&#30340;&#32452;&#21512;&#27867;&#21270;&#33021;&#21147;&#12290;&#21442;&#25968;&#20849;&#20139;&#36824;&#20351;&#20854;&#20855;&#26377;&#27604;VT&#26356;&#22909;&#30340;&#21442;&#25968;&#25928;&#29575;&#12290;&#23613;&#31649;&#20855;&#26377;&#35768;&#22810;&#20248;&#28857;&#65292;&#20294;&#25193;&#23637;UT&#21442;&#25968;&#27604;&#25193;&#23637;VT&#26356;&#38656;&#35201;&#35745;&#31639;&#21644;&#20869;&#23384;&#36164;&#28304;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#31232;&#30095;&#36890;&#29992;Transformer&#65288;SUT&#65289;&#65292;&#23427;&#21033;&#29992;&#31232;&#30095;&#28151;&#21512;&#19987;&#23478;&#65288;SMoE&#65289;&#21644;&#22522;&#20110;&#20999;&#26829;&#27861;&#30340;&#21160;&#24577;&#20572;&#27490;&#26426;&#21046;&#26469;&#20943;&#23569;UT&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#21442;&#25968;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;SUT&#22312;WMT'14&#19978;&#20165;&#20351;&#29992;&#19968;&#21322;&#30340;&#35745;&#31639;&#36164;&#28304;&#21644;&#21442;&#25968;&#23601;&#33021;&#36798;&#21040;&#19982;&#24378;&#22522;&#32447;&#27169;&#22411;&#30456;&#21516;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#24418;&#24335;&#35821;&#35328;&#20219;&#21153;&#65288;&#36923;&#36753;&#25512;&#29702;&#21644;CFQ&#65289;&#19978;&#20855;&#26377;&#24378;&#22823;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#26032;&#30340;&#20572;&#27490;&#26426;&#21046;&#36824;&#33021;&#20351;&#35745;&#31639;&#36164;&#28304;&#20943;&#23569;&#32422;50&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Universal Transformer (UT) is a variant of the Transformer that shares parameters across its layers. Empirical evidence shows that UTs have better compositional generalization than Vanilla Transformers (VTs) in formal language tasks. The parameter-sharing also affords it better parameter efficiency than VTs. Despite its many advantages, scaling UT parameters is much more compute and memory intensive than scaling up a VT. This paper proposes the Sparse Universal Transformer (SUT), which leverages Sparse Mixture of Experts (SMoE) and a new stick-breaking-based dynamic halting mechanism to reduce UT's computation complexity while retaining its parameter efficiency and generalization ability. Experiments show that SUT achieves the same performance as strong baseline models while only using half computation and parameters on WMT'14 and strong generalization results on formal language tasks (Logical inference and CFQ). The new halting mechanism also enables around 50\% reduction in compu
&lt;/p&gt;</description></item><item><title>Jaeger&#26159;&#19968;&#31181;&#22522;&#20110;&#36830;&#25509;&#30340;&#22810;&#21464;&#25442;&#22120;VQA&#27169;&#22411;&#65292;&#21033;&#29992;RoBERTa large&#21644;GPT2-xl&#20316;&#20026;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#36890;&#36807;&#24182;&#34892;&#32771;&#34385;&#22810;&#28304;&#20449;&#24687;&#26469;&#22686;&#24378;&#27169;&#22411;&#34920;&#24449;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.07091</link><description>&lt;p&gt;
Jaeger:&#19968;&#31181;&#22522;&#20110;&#36830;&#25509;&#30340;&#22810;&#21464;&#25442;&#22120;VQA&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Jaeger: A Concatenation-Based Multi-Transformer VQA Model. (arXiv:2310.07091v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07091
&lt;/p&gt;
&lt;p&gt;
Jaeger&#26159;&#19968;&#31181;&#22522;&#20110;&#36830;&#25509;&#30340;&#22810;&#21464;&#25442;&#22120;VQA&#27169;&#22411;&#65292;&#21033;&#29992;RoBERTa large&#21644;GPT2-xl&#20316;&#20026;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#36890;&#36807;&#24182;&#34892;&#32771;&#34385;&#22810;&#28304;&#20449;&#24687;&#26469;&#22686;&#24378;&#27169;&#22411;&#34920;&#24449;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25991;&#26723;&#30340;&#35270;&#35273;&#38382;&#31572;&#22312;&#35821;&#35328;&#24847;&#20041;&#28040;&#27495;&#21644;&#32454;&#31890;&#24230;&#22810;&#27169;&#24577;&#26816;&#32034;&#20043;&#38388;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#34429;&#28982;&#30001;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#21644;&#24320;&#25918;&#19990;&#30028;&#20808;&#39564;&#27169;&#22411;&#30340;&#21033;&#29992;&#65292;&#25991;&#26723;&#38382;&#31572;&#21462;&#24471;&#20102;&#40723;&#33310;&#20154;&#24515;&#30340;&#36827;&#23637;&#65292;&#20294;&#20173;&#23384;&#22312;&#19968;&#20123;&#25361;&#25112;&#65292;&#21253;&#25324;&#21709;&#24212;&#26102;&#38388;&#24310;&#38271;&#12289;&#25512;&#26029;&#25345;&#32493;&#26102;&#38388;&#24310;&#38271;&#21644;&#21305;&#37197;&#19981;&#20934;&#30830;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36830;&#25509;&#30340;&#22810;&#21464;&#25442;&#22120;VQA&#27169;&#22411;Jaegar&#12290;&#20026;&#20102;&#25552;&#21462;&#38382;&#39064;&#29305;&#24449;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;RoBERTa large&#21644;GPT2-xl&#31561;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24378;&#22823;&#33021;&#21147;&#20316;&#20026;&#29305;&#24449;&#25552;&#21462;&#22120;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23558;&#20004;&#31181;&#27169;&#22411;&#30340;&#36755;&#20986;&#36827;&#34892;&#36830;&#25509;&#25805;&#20316;&#12290;&#36825;&#20010;&#25805;&#20316;&#20351;&#24471;&#27169;&#22411;&#21487;&#20197;&#21516;&#26102;&#32771;&#34385;&#26469;&#33258;&#19981;&#21516;&#26469;&#28304;&#30340;&#20449;&#24687;&#65292;&#22686;&#24378;&#20102;&#20854;&#34920;&#24449;&#33021;&#21147;&#12290;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#21487;&#33021;&#22686;&#24378;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Document-based Visual Question Answering poses a challenging task between linguistic sense disambiguation and fine-grained multimodal retrieval. Although there has been encouraging progress in document-based question answering due to the utilization of large language and open-world prior models\cite{1}, several challenges persist, including prolonged response times, extended inference durations, and imprecision in matching. In order to overcome these challenges, we propose Jaegar, a concatenation-based multi-transformer VQA model. To derive question features, we leverage the exceptional capabilities of RoBERTa large\cite{2} and GPT2-xl\cite{3} as feature extractors. Subsequently, we subject the outputs from both models to a concatenation process. This operation allows the model to consider information from diverse sources concurrently, strengthening its representational capability. By leveraging pre-trained models for feature extraction, our approach has the potential to amplify the pe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#25913;&#21464;&#36755;&#20837;&#25552;&#31034;&#26469;&#25552;&#39640;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20174;&#32780;&#25913;&#21892;&#27169;&#22411;&#22312;&#22797;&#26434;&#25512;&#29702;&#22330;&#26223;&#20013;&#30340;&#34920;&#29616;&#12290;&#36825;&#31181;&#26041;&#27861;&#33258;&#21160;&#37319;&#38598;&#27169;&#22411;&#21453;&#39304;&#65292;&#29983;&#25104;&#36866;&#21512;&#38382;&#39064;&#30340;&#22810;&#26679;&#21270;&#25552;&#31034;&#65292;&#24182;&#36890;&#36807;&#22810;&#27425;&#25512;&#29702;&#35843;&#29992;&#26469;&#38598;&#25104;&#36825;&#20123;&#22810;&#26679;&#21270;&#30340;&#25552;&#31034;&#12290;</title><link>http://arxiv.org/abs/2310.07088</link><description>&lt;p&gt;
&#24605;&#32500;&#22810;&#26679;&#24615;&#25552;&#21319;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Diversity of Thought Improves Reasoning Abilities of Large Language Models. (arXiv:2310.07088v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07088
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#25913;&#21464;&#36755;&#20837;&#25552;&#31034;&#26469;&#25552;&#39640;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20174;&#32780;&#25913;&#21892;&#27169;&#22411;&#22312;&#22797;&#26434;&#25512;&#29702;&#22330;&#26223;&#20013;&#30340;&#34920;&#29616;&#12290;&#36825;&#31181;&#26041;&#27861;&#33258;&#21160;&#37319;&#38598;&#27169;&#22411;&#21453;&#39304;&#65292;&#29983;&#25104;&#36866;&#21512;&#38382;&#39064;&#30340;&#22810;&#26679;&#21270;&#25552;&#31034;&#65292;&#24182;&#36890;&#36807;&#22810;&#27425;&#25512;&#29702;&#35843;&#29992;&#26469;&#38598;&#25104;&#36825;&#20123;&#22810;&#26679;&#21270;&#30340;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#38656;&#35201;&#22797;&#26434;&#25512;&#29702;&#30340;&#29615;&#22659;&#20013;&#34920;&#29616;&#19981;&#20339;&#12290;&#28982;&#32780;&#65292;&#23558;&#27169;&#22411;&#25351;&#23548;&#20998;&#35299;&#38382;&#39064;&#20026;&#26356;&#23567;&#30340;&#25512;&#29702;&#27493;&#39588;&#25110;&#36890;&#36807;&#20462;&#25913;&#35299;&#30721;&#27493;&#39588;&#20351;&#21508;&#31181;&#29983;&#25104;&#32467;&#26524;&#21512;&#24182;&#21487;&#20197;&#25552;&#21319;&#24615;&#33021;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#37117;&#20551;&#35774;&#36755;&#20837;&#25552;&#31034;&#26159;&#22266;&#23450;&#30340;&#65292;&#24182;&#26399;&#26395;&#35299;&#30721;&#31574;&#30053;&#24341;&#20837;&#25152;&#38656;&#30340;&#22810;&#26679;&#24615;&#12290;&#26412;&#25991;&#25918;&#26494;&#20102;&#36825;&#20010;&#20551;&#35774;&#65292;&#24182;&#35752;&#35770;&#20102;&#22914;&#20309;&#36890;&#36807;&#21019;&#24314;&#21644;&#21033;&#29992;&#36755;&#20837;&#25552;&#31034;&#30340;&#21464;&#21270;&#26469;&#25552;&#21319;&#24605;&#32500;&#22810;&#26679;&#24615;&#20197;&#25913;&#21892;&#27169;&#22411;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#21521;&#35821;&#35328;&#27169;&#22411;&#24449;&#27714;&#21453;&#39304;&#26469;&#26500;&#24605;&#36866;&#21512;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#33258;&#21160;&#25552;&#39640;&#25552;&#31034;&#30340;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;DIV-SE (DIVerse reasoning path Self-Ensemble)&#20013;&#23545;&#22810;&#26679;&#30340;&#25552;&#31034;&#36827;&#34892;&#21512;&#25104;&#65292;&#36890;&#36807;&#22810;&#27425;&#25512;&#29702;&#35843;&#29992;&#23454;&#29616;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#32463;&#27982;&#39640;&#25928;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#21363;&#22312;&#19968;&#20010;&#25512;&#29702;&#20013;&#20351;&#29992;&#22810;&#26679;&#30340;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are documented to struggle in settings that require complex reasoning. Nevertheless, instructing the model to break down the problem into smaller reasoning steps (Wei et al., 2022), or ensembling various generations through modifying decoding steps (Wang et al., 2023) boosts performance. Current methods assume that the input prompt is fixed and expect the decoding strategies to introduce the diversity needed for ensembling. In this work, we relax this assumption and discuss how one can create and leverage variations of the input prompt as a means to diversity of thought to improve model performance. We propose a method that automatically improves prompt diversity by soliciting feedback from the LLM to ideate approaches that fit for the problem. We then ensemble the diverse prompts in our method DIV-SE (DIVerse reasoning path Self-Ensemble) across multiple inference calls. We also propose a cost-effective alternative where diverse prompts are used within a s
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#25512;&#29305;&#25968;&#25454;&#36827;&#34892;&#20132;&#36890;&#29992;&#25143;&#21453;&#39304;&#30340;&#24773;&#24863;&#20998;&#26512;&#12290;&#36890;&#36807;&#23569;&#26679;&#26412;&#23398;&#20064;&#35782;&#21035;&#25512;&#29305;&#20013;&#30340;&#38382;&#39064;&#65292;&#24182;&#37319;&#29992;&#35789;&#20856;&#24773;&#24863;&#20998;&#26512;&#27169;&#22411;&#35780;&#20272;&#25512;&#29305;&#24773;&#24863;&#30340;&#24378;&#24230;&#21644;&#26497;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.07086</link><description>&lt;p&gt;
&#21033;&#29992;&#25512;&#29305;&#25968;&#25454;&#36827;&#34892;&#20132;&#36890;&#29992;&#25143;&#21453;&#39304;&#30340;&#24773;&#24863;&#20998;&#26512;&#65306;&#19968;&#20010;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Leveraging Twitter Data for Sentiment Analysis of Transit User Feedback: An NLP Framework. (arXiv:2310.07086v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07086
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#25512;&#29305;&#25968;&#25454;&#36827;&#34892;&#20132;&#36890;&#29992;&#25143;&#21453;&#39304;&#30340;&#24773;&#24863;&#20998;&#26512;&#12290;&#36890;&#36807;&#23569;&#26679;&#26412;&#23398;&#20064;&#35782;&#21035;&#25512;&#29305;&#20013;&#30340;&#38382;&#39064;&#65292;&#24182;&#37319;&#29992;&#35789;&#20856;&#24773;&#24863;&#20998;&#26512;&#27169;&#22411;&#35780;&#20272;&#25512;&#29305;&#24773;&#24863;&#30340;&#24378;&#24230;&#21644;&#26497;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#36890;&#36807;&#20132;&#36890;&#35843;&#26597;&#25910;&#38598;&#29992;&#25143;&#21453;&#39304;&#30340;&#26041;&#27861;&#24448;&#24448;&#32791;&#26102;&#12289;&#36164;&#28304;&#23494;&#38598;&#19988;&#26114;&#36149;&#12290;&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#25512;&#29305;&#31561;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#24191;&#27867;&#12289;&#20016;&#23500;&#19988;&#24265;&#20215;&#30340;&#25968;&#25454;&#65292;&#26469;&#20102;&#35299;&#29992;&#25143;&#23545;&#21508;&#31181;&#26381;&#21153;&#38382;&#39064;&#30340;&#24863;&#30693;&#12290;&#25512;&#29305;&#20316;&#20026;&#19968;&#20010;&#24494;&#21338;&#24179;&#21488;&#65292;&#25176;&#31649;&#20102;&#22823;&#37327;&#23454;&#26102;&#30340;&#29992;&#25143;&#29983;&#25104;&#20869;&#23481;&#65292;&#20854;&#20013;&#32463;&#24120;&#21253;&#21547;&#26377;&#20851;&#21508;&#31181;&#20135;&#21697;&#12289;&#26381;&#21153;&#21644;&#20307;&#39564;&#30340;&#26377;&#20215;&#20540;&#30340;&#21453;&#39304;&#21644;&#24847;&#35265;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#36890;&#36807;&#20004;&#31181;&#25216;&#26415;&#31616;&#21270;&#20102;&#25910;&#38598;&#21644;&#20998;&#26512;&#29992;&#25143;&#21453;&#39304;&#30340;&#36807;&#31243;&#65292;&#26080;&#38656;&#26114;&#36149;&#19988;&#32791;&#26102;&#30340;&#29992;&#25143;&#21453;&#39304;&#35843;&#26597;&#12290;&#39318;&#20808;&#65292;&#23427;&#21033;&#29992;&#23569;&#26679;&#26412;&#23398;&#20064;&#36827;&#34892;&#25512;&#29305;&#20998;&#31867;&#65292;&#26377;&#25928;&#22320;&#35782;&#21035;&#25512;&#29305;&#20013;&#25551;&#36848;&#30340;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#23427;&#37319;&#29992;&#22522;&#20110;&#35789;&#20856;&#30340;&#24773;&#24863;&#20998;&#26512;&#27169;&#22411;&#26469;&#35780;&#20272;&#25512;&#29305;&#24773;&#24863;&#30340;&#24378;&#24230;&#21644;&#26497;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional methods of collecting user feedback through transit surveys are often time-consuming, resource intensive, and costly. In this paper, we propose a novel NLP-based framework that harnesses the vast, abundant, and inexpensive data available on social media platforms like Twitter to understand users' perceptions of various service issues. Twitter, being a microblogging platform, hosts a wealth of real-time user-generated content that often includes valuable feedback and opinions on various products, services, and experiences. The proposed framework streamlines the process of gathering and analyzing user feedback without the need for costly and time-consuming user feedback surveys using two techniques. First, it utilizes few-shot learning for tweet classification within predefined categories, allowing effective identification of the issues described in tweets. It then employs a lexicon-based sentiment analysis model to assess the intensity and polarity of the tweet sentiments, d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25299;&#23637;&#20102;&#23545;COVID-19&#34394;&#20551;&#20449;&#24687;&#25968;&#25454;&#38598;&#36827;&#34892;&#23457;&#35745;&#21644;&#24378;&#21270;&#30340;&#30740;&#31350;&#12290;&#39318;&#20808;&#65292;&#21457;&#29616;&#22312;&#23567;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#19987;&#19994;&#20998;&#31867;&#22120;&#22312;&#37326;&#22806;&#29615;&#22659;&#20013;&#30340;&#24615;&#33021;&#34920;&#29616;&#26377;&#38480;&#12290;&#20854;&#27425;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#20154;&#24037;&#27880;&#37322;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#21152;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25239;&#20869;&#23481;&#26469;&#22686;&#24378;&#20998;&#31867;&#22120;&#12290;</title><link>http://arxiv.org/abs/2310.07078</link><description>&lt;p&gt;
&#36890;&#36807;&#25239;&#20869;&#23481;&#37319;&#26679;&#23545;COVID-19&#34394;&#20551;&#20449;&#24687;&#25968;&#25454;&#38598;&#36827;&#34892;&#23457;&#35745;&#21644;&#24378;&#21270;
&lt;/p&gt;
&lt;p&gt;
Auditing and Robustifying COVID-19 Misinformation Datasets via Anticontent Sampling. (arXiv:2310.07078v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07078
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25299;&#23637;&#20102;&#23545;COVID-19&#34394;&#20551;&#20449;&#24687;&#25968;&#25454;&#38598;&#36827;&#34892;&#23457;&#35745;&#21644;&#24378;&#21270;&#30340;&#30740;&#31350;&#12290;&#39318;&#20808;&#65292;&#21457;&#29616;&#22312;&#23567;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#19987;&#19994;&#20998;&#31867;&#22120;&#22312;&#37326;&#22806;&#29615;&#22659;&#20013;&#30340;&#24615;&#33021;&#34920;&#29616;&#26377;&#38480;&#12290;&#20854;&#27425;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#20154;&#24037;&#27880;&#37322;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#21152;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25239;&#20869;&#23481;&#26469;&#22686;&#24378;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26377;&#20004;&#20010;&#20851;&#38190;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#23427;&#35748;&#20026;&#22312;&#23567;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#39640;&#24230;&#19987;&#19994;&#30340;&#32597;&#35265;&#20869;&#23481;&#20998;&#31867;&#22120;&#36890;&#24120;&#23545;&#37326;&#22806;&#35266;&#23519;&#21040;&#30340;&#36127;&#38754;&#31867;&#21035;&#30340;&#20016;&#23500;&#24615;&#21644;&#35805;&#39064;&#22810;&#26679;&#24615;&#65288;&#31216;&#20026;&#25239;&#20869;&#23481;&#65289;&#26377;&#38480;&#26292;&#38706;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#20998;&#31867;&#22120;&#22312;&#27979;&#35797;&#38598;&#19978;&#35266;&#23519;&#21040;&#30340;&#24378;&#22823;&#24615;&#33021;&#21487;&#33021;&#26080;&#27861;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#29615;&#22659;&#20013;&#26377;&#25928;&#36716;&#21270;&#12290;&#22312;COVID-19&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#23545;&#22810;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#37326;&#22806;&#23457;&#35745;&#65292;&#24182;&#35777;&#26126;&#20102;&#20351;&#29992;&#20960;&#20010;&#37325;&#35201;&#24341;&#29992;&#30340;&#26368;&#36817;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#37326;&#22806;&#35780;&#20272;&#26102;&#23481;&#26131;&#21463;&#21040;&#25239;&#20869;&#23481;&#30340;&#25915;&#20987;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20027;&#21160;&#23398;&#20064;&#27969;&#31243;&#65292;&#23427;&#19981;&#38656;&#35201;&#20219;&#20309;&#20154;&#24037;&#27880;&#37322;&#65292;&#24182;&#36890;&#36807;&#25361;&#25112;&#24615;&#30340;&#25239;&#20869;&#23481;&#19981;&#26029;&#22686;&#24378;&#35757;&#32451;&#25968;&#25454;&#65292;&#20174;&#32780;&#24378;&#21270;&#36825;&#20123;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper makes two key contributions. First, it argues that highly specialized rare content classifiers trained on small data typically have limited exposure to the richness and topical diversity of the negative class (dubbed anticontent) as observed in the wild. As a result, these classifiers' strong performance observed on the test set may not translate into real-world settings. In the context of COVID-19 misinformation detection, we conduct an in-the-wild audit of multiple datasets and demonstrate that models trained with several prominently cited recent datasets are vulnerable to anticontent when evaluated in the wild. Second, we present a novel active learning pipeline that requires zero manual annotation and iteratively augments the training data with challenging anticontent, robustifying these classifiers.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#35821;&#27861;&#38169;&#35823;&#19988;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#30340;LLM&#24037;&#20855;&#20351;&#29992;&#26041;&#27861;ToolDec&#65292;&#36890;&#36807;&#26377;&#38480;&#29366;&#24577;&#35299;&#30721;&#31639;&#27861;&#28040;&#38500;&#20102;&#24037;&#20855;&#30456;&#20851;&#38169;&#35823;&#65292;&#20351;LLM&#33021;&#22815;&#26377;&#25928;&#36873;&#25321;&#24037;&#20855;&#65292;&#32780;&#26080;&#38656;&#24494;&#35843;&#25110;&#19978;&#19979;&#25991;&#25991;&#26723;&#12290;</title><link>http://arxiv.org/abs/2310.07075</link><description>&lt;p&gt;
&#36890;&#36807;&#26377;&#38480;&#29366;&#24577;&#35299;&#30721;&#23454;&#29616;&#26080;&#35821;&#27861;&#38169;&#35823;&#21644;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#30340;LLM&#24037;&#20855;&#20351;&#29992;
&lt;/p&gt;
&lt;p&gt;
Syntax Error-Free and Generalizable Tool Use for LLMs via Finite-State Decoding. (arXiv:2310.07075v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07075
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#35821;&#27861;&#38169;&#35823;&#19988;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#30340;LLM&#24037;&#20855;&#20351;&#29992;&#26041;&#27861;ToolDec&#65292;&#36890;&#36807;&#26377;&#38480;&#29366;&#24577;&#35299;&#30721;&#31639;&#27861;&#28040;&#38500;&#20102;&#24037;&#20855;&#30456;&#20851;&#38169;&#35823;&#65292;&#20351;LLM&#33021;&#22815;&#26377;&#25928;&#36873;&#25321;&#24037;&#20855;&#65292;&#32780;&#26080;&#38656;&#24494;&#35843;&#25110;&#19978;&#19979;&#25991;&#25991;&#26723;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#23637;&#31034;&#20986;&#20351;&#29992;&#22806;&#37096;&#24037;&#20855;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#30340;&#26377;&#24076;&#26395;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#35201;&#20040;&#28041;&#21450;&#23545;&#24037;&#20855;&#28436;&#31034;&#36827;&#34892;&#24494;&#35843;&#65292;&#36825;&#26679;&#22312;&#27809;&#26377;&#39069;&#22806;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#26080;&#27861;&#25512;&#24191;&#21040;&#26032;&#30340;&#24037;&#20855;&#65292;&#35201;&#20040;&#22312;&#19978;&#19979;&#25991;&#20013;&#25552;&#20379;&#24037;&#20855;&#25991;&#26723;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#24037;&#20855;&#25968;&#37327;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#24120;&#24120;&#20135;&#29983;&#35821;&#27861;&#26080;&#25928;&#30340;&#24037;&#20855;&#35843;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ToolDec&#65292;&#19968;&#31181;&#26377;&#38480;&#29366;&#24577;&#26426;&#24341;&#23548;&#30340;&#35299;&#30721;&#31639;&#27861;&#65292;&#29992;&#20110;&#24037;&#20855;&#22686;&#24378;&#30340;LLMs&#12290;ToolDec&#36890;&#36807;&#30830;&#20445;&#26377;&#25928;&#30340;&#24037;&#20855;&#21517;&#31216;&#21644;&#31867;&#22411;&#19968;&#33268;&#30340;&#21442;&#25968;&#65292;&#28040;&#38500;&#20102;&#20219;&#20309;&#24037;&#20855;&#22686;&#24378;&#30340;LLMs&#20013;&#30340;&#24037;&#20855;&#30456;&#20851;&#38169;&#35823;&#12290;&#27492;&#22806;&#65292;ToolDec&#20351;LLM&#33021;&#22815;&#20165;&#20165;&#20351;&#29992;&#23427;&#20204;&#30340;&#21517;&#31216;&#20013;&#21253;&#21547;&#30340;&#20449;&#24687;&#26377;&#25928;&#22320;&#36873;&#25321;&#24037;&#20855;&#65292;&#32780;&#26080;&#38656;&#24494;&#35843;&#25110;&#19978;&#19979;&#25991;&#25991;&#26723;&#12290;&#25105;&#20204;&#22312;&#28041;&#21450;&#25968;&#23398;&#20989;&#25968;&#12289;&#30693;&#35782;&#22270;&#35889;&#20851;&#31995;&#21644;&#22797;&#26434;&#30340;&#29616;&#23454;&#19990;&#30028;RESTful API&#30340;&#21508;&#31181;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#22810;&#31181;&#20808;&#21069;&#30340;&#26041;&#27861;&#21450;&#20854;ToolDec&#22686;&#24378;&#29256;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown promising capabilities in using external tools to solve complex problems. However, existing approaches either involve fine-tuning on tool demonstrations, which do not generalize to new tools without additional training, or providing tool documentation in context, limiting the number of tools. Both approaches often generate syntactically invalid tool calls. In this paper, we propose ToolDec, a finite-state machine-guided decoding algorithm for tool-augmented LLMs. ToolDec eliminates tool-related errors for any tool-augmented LLMs by ensuring valid tool names and type-conforming arguments. Furthermore, ToolDec enables LLM to effectively select tools using only the information contained in their names, with no need for fine-tuning or in-context documentation. We evaluated multiple prior methods and their ToolDec-enhanced versions on a variety of tasks involving tools like math functions, knowledge graph relations, and complex real-world RESTful APIs
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#31181;&#25512;&#29702;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#25552;&#39640;&#25552;&#31034;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#21644;&#19968;&#33268;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Hypotheses-to-Theories (HtT)&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;LLMs&#25512;&#29702;&#30340;&#35268;&#21017;&#24211;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#29616;&#26377;&#30340;&#25552;&#31034;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.07064</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#35268;&#21017;
&lt;/p&gt;
&lt;p&gt;
Large Language Models can Learn Rules. (arXiv:2310.07064v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07064
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#31181;&#25512;&#29702;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#25552;&#39640;&#25552;&#31034;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#21644;&#19968;&#33268;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Hypotheses-to-Theories (HtT)&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;LLMs&#25512;&#29702;&#30340;&#35268;&#21017;&#24211;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#29616;&#26377;&#30340;&#25552;&#31034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#32473;&#20986;&#19968;&#20123;&#31034;&#20363;&#21644;&#20013;&#38388;&#27493;&#39588;&#26102;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#31181;&#25512;&#29702;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20381;&#36182;LLM&#20013;&#30340;&#38544;&#24335;&#30693;&#35782;&#30340;&#25552;&#31034;&#26041;&#27861;&#22312;&#38544;&#24335;&#30693;&#35782;&#38169;&#35823;&#25110;&#19982;&#20219;&#21153;&#19981;&#19968;&#33268;&#26102;&#24448;&#24448;&#20250;&#20135;&#29983;&#38169;&#35823;&#30340;&#31572;&#26696;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;"&#20551;&#35774;&#21040;&#29702;&#35770;" (HtT) &#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;LLMs&#25512;&#29702;&#30340;&#35268;&#21017;&#24211;&#12290;HtT&#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#65292;&#24402;&#32435;&#38454;&#27573;&#21644;&#28436;&#32462;&#38454;&#27573;&#12290;&#22312;&#24402;&#32435;&#38454;&#27573;&#65292;&#39318;&#20808;&#35201;&#27714;LLM&#26681;&#25454;&#19968;&#32452;&#35757;&#32451;&#31034;&#20363;&#29983;&#25104;&#21644;&#39564;&#35777;&#35268;&#21017;&#12290;&#20986;&#29616;&#24182;&#23548;&#33268;&#27491;&#30830;&#31572;&#26696;&#30340;&#35268;&#21017;&#23558;&#34987;&#25910;&#38598;&#24418;&#25104;&#19968;&#20010;&#35268;&#21017;&#24211;&#12290;&#22312;&#28436;&#32462;&#38454;&#27573;&#65292;&#28982;&#21518;&#35201;&#27714;LLM&#20351;&#29992;&#23398;&#20064;&#30340;&#35268;&#21017;&#24211;&#36827;&#34892;&#25512;&#29702;&#20197;&#22238;&#31572;&#27979;&#35797;&#38382;&#39064;&#12290;&#22312;&#25968;&#20540;&#25512;&#29702;&#21644;&#20851;&#31995;&#25512;&#29702;&#38382;&#39064;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;HtT&#25913;&#36827;&#20102;&#29616;&#26377;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#20351;&#20854;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
When prompted with a few examples and intermediate steps, large language models (LLMs) have demonstrated impressive performance in various reasoning tasks. However, prompting methods that rely on implicit knowledge in an LLM often hallucinate incorrect answers when the implicit knowledge is wrong or inconsistent with the task. To tackle this problem, we present Hypotheses-to-Theories (HtT), a framework that learns a rule library for reasoning with LLMs. HtT contains two stages, an induction stage and a deduction stage. In the induction stage, an LLM is first asked to generate and verify rules over a set of training examples. Rules that appear and lead to correct answers sufficiently often are collected to form a rule library. In the deduction stage, the LLM is then prompted to employ the learned rule library to perform reasoning to answer test questions. Experiments on both numerical reasoning and relational reasoning problems show that HtT improves existing prompting methods, with an 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;DKEC&#65292;&#19968;&#31181;&#39046;&#22495;&#30693;&#35782;&#22686;&#24378;&#30340;&#20998;&#31867;&#22120;&#65292;&#29992;&#20110;&#21307;&#23398;&#35786;&#26029;&#39044;&#27979;&#12290;&#23427;&#21033;&#29992;&#26631;&#31614;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#32452;&#20869;&#35757;&#32451;&#26041;&#27861;&#26469;&#25429;&#25417;&#21307;&#23398;&#23454;&#20307;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#24182;&#22686;&#21152;&#32597;&#35265;&#31867;&#21035;&#30340;&#26679;&#26412;&#25968;&#37327;&#12290;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#20854;&#22312;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2310.07059</link><description>&lt;p&gt;
DKEC: &#39046;&#22495;&#30693;&#35782;&#22686;&#24378;&#30340;&#30005;&#23376;&#30149;&#21382;&#22810;&#26631;&#31614;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
DKEC: Domain Knowledge Enhanced Multi-Label Classification for Electronic Health Records. (arXiv:2310.07059v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07059
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;DKEC&#65292;&#19968;&#31181;&#39046;&#22495;&#30693;&#35782;&#22686;&#24378;&#30340;&#20998;&#31867;&#22120;&#65292;&#29992;&#20110;&#21307;&#23398;&#35786;&#26029;&#39044;&#27979;&#12290;&#23427;&#21033;&#29992;&#26631;&#31614;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#32452;&#20869;&#35757;&#32451;&#26041;&#27861;&#26469;&#25429;&#25417;&#21307;&#23398;&#23454;&#20307;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#24182;&#22686;&#21152;&#32597;&#35265;&#31867;&#21035;&#30340;&#26679;&#26412;&#25968;&#37327;&#12290;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#20854;&#22312;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#39046;&#22495;&#30340;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#32463;&#24120;&#38754;&#20020;&#38271;&#23614;&#26631;&#31614;&#20998;&#24067;&#65292;&#21363;&#32597;&#35265;&#31867;&#21035;&#30340;&#35757;&#32451;&#26679;&#26412;&#23569;&#20110;&#39057;&#32321;&#31867;&#21035;&#12290;&#34429;&#28982;&#20043;&#21069;&#30340;&#24037;&#20316;&#24050;&#32463;&#25506;&#32034;&#20102;&#19981;&#21516;&#30340;&#27169;&#22411;&#26550;&#26500;&#21644;&#23618;&#27425;&#21270;&#26631;&#31614;&#32467;&#26500;&#26469;&#25214;&#21040;&#37325;&#35201;&#29305;&#24449;&#65292;&#20294;&#22823;&#22810;&#25968;&#24573;&#30053;&#20102;&#20174;&#21307;&#23398;&#25351;&#21335;&#20013;&#34701;&#20837;&#39046;&#22495;&#30693;&#35782;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;DKEC&#65292;&#19968;&#31181;&#22686;&#24378;&#21307;&#23398;&#35786;&#26029;&#39044;&#27979;&#30340;&#39046;&#22495;&#30693;&#35782;&#22686;&#24378;&#20998;&#31867;&#22120;&#65292;&#20854;&#20013;&#21253;&#25324;&#20004;&#20010;&#21019;&#26032;&#28857;&#65306;&#65288;1&#65289;&#19968;&#20010;&#22522;&#20110;&#26631;&#31614;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#32467;&#21512;&#24322;&#26500;&#22270;&#21644;&#39046;&#22495;&#26412;&#20307;&#26469;&#25429;&#25417;&#21307;&#23398;&#23454;&#20307;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#65288;2&#65289;&#19968;&#31181;&#22522;&#20110;&#26631;&#31614;&#30456;&#20284;&#24615;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#32452;&#20869;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#22686;&#21152;&#32597;&#35265;&#31867;&#21035;&#30340;&#26679;&#26412;&#25968;&#37327;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#30495;&#23454;&#30340;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;DKEC&#65306;RAA&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#26469;&#33258;&#24613;&#25937;&#26381;&#21153;&#65288;EMS&#65289;&#20107;&#20214;&#30340;4,417&#20010;&#24739;&#32773;&#25252;&#29702;&#25253;&#21578;&#30340;&#25910;&#38598;&#65292;&#21644;&#26469;&#33258;53898&#25253;&#21578;&#30340;&#23376;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-label text classification (MLTC) tasks in the medical domain often face long-tail label distribution, where rare classes have fewer training samples than frequent classes. Although previous works have explored different model architectures and hierarchical label structures to find important features, most of them neglect to incorporate the domain knowledge from medical guidelines. In this paper, we present DKEC, Domain Knowledge Enhanced Classifier for medical diagnosis prediction with two innovations: (1) a label-wise attention mechanism that incorporates a heterogeneous graph and domain ontologies to capture the semantic relationships between medical entities, (2) a simple yet effective group-wise training method based on similarity of labels to increase samples of rare classes. We evaluate DKEC on two real-world medical datasets: the RAA dataset, a collection of 4,417 patient care reports from emergency medical services (EMS) incidents, and a subset of 53,898 reports from the 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#22823;&#35268;&#27169;&#26080;&#26631;&#31614;&#25968;&#25454;&#38598;&#35757;&#32451;&#33258;&#30417;&#30563;&#22522;&#30784;&#27169;&#22411;&#24182;&#22312;&#22823;&#22411;&#20020;&#24202;&#30149;&#29702;&#23398;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#26088;&#22312;&#35757;&#32451;&#26368;&#22823;&#30340;&#23398;&#26415;&#22522;&#30784;&#27169;&#22411;&#24182;&#35780;&#20272;&#26368;&#26174;&#33879;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.07033</link><description>&lt;p&gt;
&#20581;&#24247;&#31995;&#32479;&#35268;&#27169;&#30340;&#35745;&#31639;&#30149;&#29702;&#23398;&#8212;&#8212;&#26469;&#33258;30&#20159;&#22270;&#20687;&#30340;&#33258;&#30417;&#30563;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Computational Pathology at Health System Scale -- Self-Supervised Foundation Models from Three Billion Images. (arXiv:2310.07033v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07033
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#22823;&#35268;&#27169;&#26080;&#26631;&#31614;&#25968;&#25454;&#38598;&#35757;&#32451;&#33258;&#30417;&#30563;&#22522;&#30784;&#27169;&#22411;&#24182;&#22312;&#22823;&#22411;&#20020;&#24202;&#30149;&#29702;&#23398;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#26088;&#22312;&#35757;&#32451;&#26368;&#22823;&#30340;&#23398;&#26415;&#22522;&#30784;&#27169;&#22411;&#24182;&#35780;&#20272;&#26368;&#26174;&#33879;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#31361;&#30772;&#20351;&#24471;&#21487;&#20197;&#20351;&#29992;&#22823;&#22411;&#26080;&#26631;&#31614;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#65292;&#20174;&#32780;&#21487;&#20197;&#25512;&#24191;&#21040;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#12290;&#34429;&#28982;&#36825;&#31181;&#35757;&#32451;&#33539;&#24335;&#38750;&#24120;&#36866;&#21512;&#21307;&#23398;&#39046;&#22495;&#65292;&#22240;&#20026;&#26631;&#27880;&#24448;&#24448;&#31232;&#32570;&#65292;&#20294;&#26159;&#22312;&#21307;&#23398;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#30149;&#29702;&#23398;&#39046;&#22495;&#65292;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#30740;&#31350;&#36824;&#19981;&#22815;&#20805;&#20998;&#12290;&#20197;&#24448;&#30340;&#30149;&#29702;&#23398;&#33258;&#30417;&#30563;&#23398;&#20064;&#24037;&#20316;&#21033;&#29992;&#36739;&#23567;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#35780;&#20272;&#19979;&#28216;&#24615;&#33021;&#12290;&#35813;&#39033;&#30446;&#30340;&#30446;&#26631;&#26159;&#35757;&#32451;&#26368;&#22823;&#30340;&#23398;&#26415;&#22522;&#30784;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#22312;&#22823;&#35268;&#27169;&#20020;&#24202;&#30149;&#29702;&#23398;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#35780;&#20272;&#19979;&#28216;&#24615;&#33021;&#26469;&#35780;&#20272;&#26368;&#26174;&#33879;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;&#30149;&#29702;&#23398;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#26469;&#33258;&#36229;&#36807;423,000&#20010;&#26174;&#24494;&#38236;&#24187;&#28783;&#29255;&#30340;30&#20159;&#24352;&#22270;&#20687;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#20351;&#29992;&#25513;&#34109;&#33258;&#32534;&#30721;&#22120;&#65288;MAE&#65289;&#21644;D&#26469;&#39044;&#35757;&#32451;&#35270;&#35273;&#21464;&#25442;&#22120;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent breakthroughs in self-supervised learning have enabled the use of large unlabeled datasets to train visual foundation models that can generalize to a variety of downstream tasks. While this training paradigm is well suited for the medical domain where annotations are scarce, large-scale pre-training in the medical domain, and in particular pathology, has not been extensively studied. Previous work in self-supervised learning in pathology has leveraged smaller datasets for both pre-training and evaluating downstream performance. The aim of this project is to train the largest academic foundation model and benchmark the most prominent self-supervised learning algorithms by pre-training and evaluating downstream performance on large clinical pathology datasets. We collected the largest pathology dataset to date, consisting of over 3 billion images from over 423 thousand microscopy slides. We compared pre-training of visual transformer models using the masked autoencoder (MAE) and D
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32454;&#31890;&#24230;&#29305;&#24449;&#30340;Deepfake&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#25233;&#21046;&#32972;&#26223;&#22122;&#22768;&#21644;&#23398;&#20064;&#21306;&#20998;&#24615;&#29305;&#24449;&#26469;&#25552;&#39640;&#26816;&#27979;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.07028</link><description>&lt;p&gt;
&#20351;&#29992;&#32454;&#31890;&#24230;&#29305;&#24449;&#36827;&#34892;&#38754;&#37096;&#20266;&#36896;&#30340;Deepfake&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Facial Forgery-based Deepfake Detection using Fine-Grained Features. (arXiv:2310.07028v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07028
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32454;&#31890;&#24230;&#29305;&#24449;&#30340;Deepfake&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#25233;&#21046;&#32972;&#26223;&#22122;&#22768;&#21644;&#23398;&#20064;&#21306;&#20998;&#24615;&#29305;&#24449;&#26469;&#25552;&#39640;&#26816;&#27979;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;Deepfake&#30340;&#38754;&#37096;&#20266;&#36896;&#24050;&#32463;&#24341;&#21457;&#20102;&#37325;&#22823;&#30340;&#23433;&#20840;&#39118;&#38505;&#24182;&#24341;&#36215;&#20102;&#20005;&#37325;&#30340;&#31038;&#20250;&#20851;&#20999;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#35768;&#22810;Deepfake&#26816;&#27979;&#26041;&#27861;&#12290;&#20854;&#20013;&#22823;&#22810;&#25968;&#26041;&#27861;&#23558;Deepfake&#26816;&#27979;&#24314;&#27169;&#20026;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#39592;&#24178;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#26550;&#26500;&#36827;&#34892;&#20108;&#20998;&#31867;&#30340;&#38382;&#39064;&#12290;&#36825;&#20123;&#22522;&#20110;CNN&#30340;&#26041;&#27861;&#22312;Deepfake&#26816;&#27979;&#26041;&#38754;&#34920;&#29616;&#20986;&#38750;&#24120;&#39640;&#30340;&#25928;&#26524;&#65292;&#26354;&#32447;&#19979;&#38754;&#31215;(AUC)&#39640;&#36798;0.99&#12290;&#28982;&#32780;&#65292;&#24403;&#34987;&#24212;&#29992;&#20110;&#19981;&#21516;&#25968;&#25454;&#38598;&#21644;Deepfake&#25805;&#32437;&#25216;&#26415;&#36827;&#34892;&#35780;&#20272;&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#30340;&#24615;&#33021;&#20250;&#26174;&#33879;&#19979;&#38477;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#38656;&#35201;&#23398;&#20064;&#26356;&#21152;&#24494;&#22937;&#12289;&#23616;&#37096;&#21644;&#26377;&#36776;&#21035;&#21147;&#30340;&#29305;&#24449;&#26469;&#36827;&#34892;Deepfake&#26816;&#27979;&#12290;&#26412;&#25991;&#23558;Deepfake&#26816;&#27979;&#38382;&#39064;&#35270;&#20026;&#32454;&#31890;&#24230;&#20998;&#31867;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#32454;&#31890;&#24230;&#35299;&#20915;&#26041;&#26696;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#26377;&#25928;&#22320;&#25233;&#21046;&#32972;&#26223;&#22122;&#22768;&#21644;&#23398;&#20064;&#21306;&#20998;&#24615;&#29305;&#24449;&#26469;&#23398;&#20064;&#24494;&#22937;&#19988;&#20855;&#26377;&#19968;&#33324;&#24615;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Facial forgery by deepfakes has caused major security risks and raised severe societal concerns. As a countermeasure, a number of deepfake detection methods have been proposed. Most of them model deepfake detection as a binary classification problem using a backbone convolutional neural network (CNN) architecture pretrained for the task. These CNN-based methods have demonstrated very high efficacy in deepfake detection with the Area under the Curve (AUC) as high as $0.99$. However, the performance of these methods degrades significantly when evaluated across datasets and deepfake manipulation techniques. This draws our attention towards learning more subtle, local, and discriminative features for deepfake detection. In this paper, we formulate deepfake detection as a fine-grained classification problem and propose a new fine-grained solution to it. Specifically, our method is based on learning subtle and generalizable features by effectively suppressing background noise and learning di
&lt;/p&gt;</description></item><item><title>NEWTON&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29289;&#29702;&#25512;&#29702;&#33021;&#21147;&#30340;&#20179;&#24211;&#21644;&#22522;&#20934;&#65292;&#21253;&#21547;2800&#20010;&#29289;&#20307;-&#23646;&#24615;&#23545;&#21644;160K&#20010;&#38382;&#31572;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.07018</link><description>&lt;p&gt;
NEWTON: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36827;&#34892;&#29289;&#29702;&#25512;&#29702;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
NEWTON: Are Large Language Models Capable of Physical Reasoning?. (arXiv:2310.07018v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07018
&lt;/p&gt;
&lt;p&gt;
NEWTON&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29289;&#29702;&#25512;&#29702;&#33021;&#21147;&#30340;&#20179;&#24211;&#21644;&#22522;&#20934;&#65292;&#21253;&#21547;2800&#20010;&#29289;&#20307;-&#23646;&#24615;&#23545;&#21644;160K&#20010;&#38382;&#31572;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20854;&#35821;&#22659;&#21270;&#34920;&#31034;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#34987;&#23454;&#35777;&#22320;&#35777;&#26126;&#21253;&#21547;&#21477;&#27861;&#12289;&#35821;&#20041;&#12289;&#35789;&#20041;&#21644;&#24120;&#35782;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#23427;&#20204;&#22312;&#29289;&#29702;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#30340;&#25506;&#32034;&#36824;&#26377;&#19968;&#23450;&#38480;&#21046;&#65292;&#29305;&#21035;&#26159;&#28041;&#21450;&#29702;&#35299;&#26085;&#24120;&#29289;&#20307;&#30340;&#20851;&#38190;&#23646;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;NEWTON&#65292;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLM&#30340;&#29289;&#29702;&#25512;&#29702;&#33021;&#21147;&#30340;&#20179;&#24211;&#21644;&#22522;&#20934;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#23454;&#29616;&#27492;&#22522;&#20934;&#30340;&#39046;&#22495;&#29305;&#23450;&#36866;&#24212;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#27969;&#31243;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#29983;&#25104;&#19968;&#20010;&#26681;&#25454;&#20182;&#20204;&#24212;&#29992;&#31243;&#24207;&#30456;&#20851;&#29289;&#20307;&#21644;&#23646;&#24615;&#23450;&#21046;&#30340;&#22522;&#20934;&#21464;&#20307;&#12290;NEWTON&#20179;&#24211;&#21253;&#25324;2800&#20010;&#29289;&#20307;-&#23646;&#24615;&#23545;&#30340;&#38598;&#21512;&#65292;&#20026;&#29983;&#25104;&#26080;&#38480;&#35268;&#27169;&#30340;&#35780;&#20272;&#27169;&#26495;&#22880;&#23450;&#22522;&#30784;&#12290;NEWTON&#22522;&#20934;&#21253;&#21547;160K&#20010;&#38382;&#31572;&#38382;&#39064;&#65292;&#20351;&#29992;NEWTON&#20179;&#24211;&#31574;&#21010;&#65292;&#20197;&#35843;&#26597;&#29289;&#29702;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), through their contextualized representations, have been empirically proven to encapsulate syntactic, semantic, word sense, and common-sense knowledge. However, there has been limited exploration of their physical reasoning abilities, specifically concerning the crucial attributes for comprehending everyday objects. To address this gap, we introduce NEWTON, a repository and benchmark for evaluating the physics reasoning skills of LLMs. Further, to enable domain-specific adaptation of this benchmark, we present a pipeline to enable researchers to generate a variant of this benchmark that has been customized to the objects and attributes relevant for their application. The NEWTON repository comprises a collection of 2800 object-attribute pairs, providing the foundation for generating infinite-scale assessment templates. The NEWTON benchmark consists of 160K QA questions, curated using the NEWTON repository to investigate the physical reasoning capabilities of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#25991;&#26412;&#38382;&#31572;&#31995;&#32479;&#29983;&#25104;&#30340;&#20505;&#36873;&#31572;&#26696;&#22522;&#20110;&#20854;&#31867;&#22411;&#36827;&#34892;&#36807;&#28388;&#21644;&#37325;&#26032;&#25490;&#24207;&#65292;&#20197;&#35299;&#20915;&#22312;&#30693;&#35782;&#22270;&#35889;&#38382;&#31572;&#20219;&#21153;&#20013;&#65292;&#27169;&#22411;&#23481;&#37327;&#26377;&#38480;&#19988;&#23545;&#20110;&#21547;&#26377;&#19981;&#22826;&#27969;&#34892;&#23454;&#20307;&#30340;&#38382;&#39064;&#36136;&#37327;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.07008</link><description>&lt;p&gt;
&#31572;&#26696;&#20505;&#36873;&#31867;&#22411;&#36873;&#25321;&#65306;&#38381;&#20070;&#38382;&#31572;&#20013;&#30340;&#25991;&#26412;&#21040;&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#28385;&#36275;&#30693;&#35782;&#22270;&#35889;
&lt;/p&gt;
&lt;p&gt;
Answer Candidate Type Selection: Text-to-Text Language Model for Closed Book Question Answering Meets Knowledge Graphs. (arXiv:2310.07008v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07008
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#25991;&#26412;&#38382;&#31572;&#31995;&#32479;&#29983;&#25104;&#30340;&#20505;&#36873;&#31572;&#26696;&#22522;&#20110;&#20854;&#31867;&#22411;&#36827;&#34892;&#36807;&#28388;&#21644;&#37325;&#26032;&#25490;&#24207;&#65292;&#20197;&#35299;&#20915;&#22312;&#30693;&#35782;&#22270;&#35889;&#38382;&#31572;&#20219;&#21153;&#20013;&#65292;&#27169;&#22411;&#23481;&#37327;&#26377;&#38480;&#19988;&#23545;&#20110;&#21547;&#26377;&#19981;&#22826;&#27969;&#34892;&#23454;&#20307;&#30340;&#38382;&#39064;&#36136;&#37327;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;T5&#25110;BART&#65289;&#22312;&#30693;&#35782;&#22270;&#35889;&#38382;&#31572;&#65288;KGQA&#65289;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#30340;&#23481;&#37327;&#26377;&#38480;&#65292;&#23545;&#20110;&#21253;&#21547;&#19981;&#22826;&#27969;&#34892;&#23454;&#20307;&#30340;&#38382;&#39064;&#65292;&#36136;&#37327;&#19979;&#38477;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#25991;&#26412;&#38382;&#31572;&#31995;&#32479;&#30340;&#22522;&#30784;&#19978;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#26681;&#25454;&#20505;&#36873;&#31572;&#26696;&#30340;&#31867;&#22411;&#65288;&#26469;&#33258;Wikidata&#30340;"instance_of"&#23646;&#24615;&#65289;&#36827;&#34892;&#31579;&#36873;&#21644;&#37325;&#26032;&#25490;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained Text-to-Text Language Models (LMs), such as T5 or BART yield promising results in the Knowledge Graph Question Answering (KGQA) task. However, the capacity of the models is limited and the quality decreases for questions with less popular entities. In this paper, we present a novel approach which works on top of the pre-trained Text-to-Text QA system to address this issue. Our simple yet effective method performs filtering and re-ranking of generated candidates based on their types derived from Wikidata "instance_of" property.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29983;&#25104;&#25915;&#20987;&#25216;&#26415;&#65292;&#36890;&#36807;&#25805;&#32437;&#35299;&#30721;&#26041;&#27861;&#30340;&#21464;&#21270;&#65292;&#21487;&#20197;&#23548;&#33268;&#24320;&#28304;LLMs&#30340;&#28798;&#38590;&#24615;&#36234;&#29425;&#65292;&#23558;&#38169;&#20301;&#29575;&#20174;0%&#25552;&#39640;&#21040;&#20102;&#36229;&#36807;95%&#12290;&#36825;&#39033;&#25915;&#20987;&#26041;&#27861;&#31616;&#21333;&#32780;&#26377;&#25928;&#65292;&#22312;11&#20010;&#35821;&#35328;&#27169;&#22411;&#20013;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#19988;&#35745;&#31639;&#37327;&#38477;&#20302;&#20102;30&#20493;&#12290;</title><link>http://arxiv.org/abs/2310.06987</link><description>&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#25216;&#26415;&#23454;&#26045;&#24320;&#28304;LLM&#30340;&#28798;&#38590;&#24615;&#36234;&#29425;
&lt;/p&gt;
&lt;p&gt;
Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation. (arXiv:2310.06987v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06987
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29983;&#25104;&#25915;&#20987;&#25216;&#26415;&#65292;&#36890;&#36807;&#25805;&#32437;&#35299;&#30721;&#26041;&#27861;&#30340;&#21464;&#21270;&#65292;&#21487;&#20197;&#23548;&#33268;&#24320;&#28304;LLMs&#30340;&#28798;&#38590;&#24615;&#36234;&#29425;&#65292;&#23558;&#38169;&#20301;&#29575;&#20174;0%&#25552;&#39640;&#21040;&#20102;&#36229;&#36807;95%&#12290;&#36825;&#39033;&#25915;&#20987;&#26041;&#27861;&#31616;&#21333;&#32780;&#26377;&#25928;&#65292;&#22312;11&#20010;&#35821;&#35328;&#27169;&#22411;&#20013;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#19988;&#35745;&#31639;&#37327;&#38477;&#20302;&#20102;30&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#28304;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#26497;&#22823;&#22320;&#25512;&#36827;&#20102;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#12290;&#22312;&#21457;&#24067;&#27169;&#22411;&#20043;&#21069;&#65292;&#20154;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#21162;&#21147;&#65292;&#20197;&#30830;&#20445;&#27169;&#22411;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#19968;&#33268;&#24615;&#65292;&#20027;&#35201;&#30446;&#26631;&#26159;&#30830;&#20445;&#20854;&#26377;&#30410;&#19988;&#26080;&#23475;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#32463;&#36807;&#31934;&#24515;&#23545;&#40784;&#30340;&#27169;&#22411;&#20063;&#21487;&#33021;&#34987;&#24694;&#24847;&#25805;&#32437;&#65292;&#23548;&#33268;&#24847;&#22806;&#34892;&#20026;&#65292;&#21363;&#25152;&#35859;&#30340;&#8220;&#36234;&#29425;&#8221;&#12290;&#36825;&#20123;&#36234;&#29425;&#36890;&#24120;&#30001;&#29305;&#23450;&#30340;&#25991;&#26412;&#36755;&#20837;&#35302;&#21457;&#65292;&#36890;&#24120;&#31216;&#20026;&#23545;&#25239;&#24615;&#25552;&#31034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#25915;&#20987;&#25216;&#26415;&#65292;&#36825;&#26159;&#19968;&#31181;&#26497;&#20854;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#20165;&#36890;&#36807;&#25805;&#32437;&#35299;&#30721;&#26041;&#27861;&#30340;&#21464;&#21270;&#26469;&#25200;&#20081;&#27169;&#22411;&#30340;&#23545;&#40784;&#12290;&#36890;&#36807;&#21033;&#29992;&#19981;&#21516;&#30340;&#29983;&#25104;&#31574;&#30053;&#65292;&#21253;&#25324;&#21464;&#21270;&#30340;&#35299;&#30721;&#36229;&#21442;&#25968;&#21644;&#37319;&#26679;&#26041;&#27861;&#65292;&#25105;&#20204;&#23558;11&#20010;&#35821;&#35328;&#27169;&#22411;&#65292;&#21253;&#25324;LLaMA2&#12289;Vicuna&#12289;Falcon&#21644;MPT&#23478;&#26063;&#30340;&#38169;&#20301;&#29575;&#20174;0%&#25552;&#39640;&#21040;&#20102;95%&#20197;&#19978;&#65292;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#35745;&#31639;&#37327;&#38477;&#20302;&#20102;30&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid progress in open-source large language models (LLMs) is significantly advancing AI development. Extensive efforts have been made before model release to align their behavior with human values, with the primary goal of ensuring their helpfulness and harmlessness. However, even carefully aligned models can be manipulated maliciously, leading to unintended behaviors, known as "jailbreaks". These jailbreaks are typically triggered by specific text inputs, often referred to as adversarial prompts. In this work, we propose the generation exploitation attack, an extremely simple approach that disrupts model alignment by only manipulating variations of decoding methods. By exploiting different generation strategies, including varying decoding hyper-parameters and sampling methods, we increase the misalignment rate from 0% to more than 95% across 11 language models including LLaMA2, Vicuna, Falcon, and MPT families, outperforming state-of-the-art attacks with $30\times$ lower computat
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#20174;&#20154;&#31867;&#35282;&#24230;&#35780;&#20272;&#22522;&#20110;&#37096;&#20998;&#21407;&#22411;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#30340;&#26694;&#26550;&#65292;&#24182;&#20351;&#29992;&#20122;&#39532;&#36874;&#26426;&#26800;&#22303;&#32819;&#20854;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#12290;&#36825;&#20123;&#23454;&#39564;&#19981;&#20165;&#23637;&#31034;&#20102;&#26694;&#26550;&#22312;&#35780;&#20272;&#21508;&#31181;&#22522;&#20110;&#37096;&#20998;&#21407;&#22411;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#36824;&#26159;&#26368;&#20840;&#38754;&#30340;&#24037;&#20316;&#12290;</title><link>http://arxiv.org/abs/2310.06966</link><description>&lt;p&gt;
&#20851;&#20110;&#22522;&#20110;&#37096;&#20998;&#21407;&#22411;&#30340;&#20998;&#31867;&#22120;&#30340;&#21487;&#35299;&#37322;&#24615;&#65306;&#20154;&#31867;&#20013;&#24515;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
On the Interpretability of Part-Prototype Based Classifiers: A Human Centric Analysis. (arXiv:2310.06966v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06966
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#20174;&#20154;&#31867;&#35282;&#24230;&#35780;&#20272;&#22522;&#20110;&#37096;&#20998;&#21407;&#22411;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#30340;&#26694;&#26550;&#65292;&#24182;&#20351;&#29992;&#20122;&#39532;&#36874;&#26426;&#26800;&#22303;&#32819;&#20854;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#12290;&#36825;&#20123;&#23454;&#39564;&#19981;&#20165;&#23637;&#31034;&#20102;&#26694;&#26550;&#22312;&#35780;&#20272;&#21508;&#31181;&#22522;&#20110;&#37096;&#20998;&#21407;&#22411;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#36824;&#26159;&#26368;&#20840;&#38754;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#37096;&#20998;&#21407;&#22411;&#30340;&#32593;&#32476;&#24050;&#25104;&#20026;&#35768;&#22810;&#40657;&#30418;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#21487;&#35299;&#37322;&#24615;&#26367;&#20195;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20174;&#20154;&#31867;&#29992;&#25143;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#20123;&#26041;&#27861;&#30340;&#21487;&#35299;&#37322;&#24615;&#36824;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20174;&#20154;&#31867;&#35282;&#24230;&#35780;&#20272;&#22522;&#20110;&#37096;&#20998;&#21407;&#22411;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#30340;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#30001;&#19977;&#20010;&#21487;&#25805;&#20316;&#30340;&#24230;&#37327;&#21644;&#23454;&#39564;&#32452;&#25104;&#12290;&#20026;&#20102;&#23637;&#31034;&#25105;&#20204;&#30340;&#26694;&#26550;&#30340;&#23454;&#29992;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#20122;&#39532;&#36874;&#26426;&#26800;&#22303;&#32819;&#20854;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;&#23427;&#20204;&#19981;&#20165;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#35780;&#20272;&#21508;&#31181;&#22522;&#20110;&#37096;&#20998;&#21407;&#22411;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#32780;&#19988;&#26159;&#25105;&#20204;&#25152;&#30693;&#30340;&#22312;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#20013;&#35780;&#20272;&#36825;&#20123;&#26041;&#27861;&#26368;&#20840;&#38754;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Part-prototype networks have recently become methods of interest as an interpretable alternative to many of the current black-box image classifiers. However, the interpretability of these methods from the perspective of human users has not been sufficiently explored. In this work, we have devised a framework for evaluating the interpretability of part-prototype-based models from a human perspective. The proposed framework consists of three actionable metrics and experiments. To demonstrate the usefulness of our framework, we performed an extensive set of experiments using Amazon Mechanical Turk. They not only show the capability of our framework in assessing the interpretability of various part-prototype-based models, but they also are, to the best of our knowledge, the most comprehensive work on evaluating such methods in a unified framework.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20934;&#30830;&#31232;&#30095;&#24494;&#35843;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;L2&#33539;&#25968;&#30340;&#33976;&#39311;&#26041;&#27861;SquareHead&#65292;&#21487;&#20197;&#22312;&#39640;&#31232;&#30095;&#24615;&#19979;&#23454;&#29616;&#20934;&#30830;&#30340;&#24674;&#22797;&#65307;&#21516;&#26102;&#23637;&#31034;&#20102;&#31232;&#30095;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#38469;&#25928;&#29575;&#65292;&#21487;&#22312;CPU&#21644;GPU&#36816;&#34892;&#26102;&#23454;&#29616;&#21152;&#36895;&#65292;&#24182;&#19988;&#35266;&#23519;&#21040;&#22312;&#21463;&#20869;&#23384;&#38480;&#21046;&#30340;&#27169;&#22411;&#20013;&#65292;&#31232;&#30095;&#24615;&#20063;&#21487;&#29992;&#20110;&#20943;&#23569;&#20869;&#23384;&#24102;&#23485;&#12290;</title><link>http://arxiv.org/abs/2310.06927</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31232;&#30095;&#24494;&#35843;&#30340;&#25512;&#29702;&#21152;&#36895;
&lt;/p&gt;
&lt;p&gt;
Sparse Finetuning for Inference Acceleration of Large Language Models. (arXiv:2310.06927v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06927
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20934;&#30830;&#31232;&#30095;&#24494;&#35843;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;L2&#33539;&#25968;&#30340;&#33976;&#39311;&#26041;&#27861;SquareHead&#65292;&#21487;&#20197;&#22312;&#39640;&#31232;&#30095;&#24615;&#19979;&#23454;&#29616;&#20934;&#30830;&#30340;&#24674;&#22797;&#65307;&#21516;&#26102;&#23637;&#31034;&#20102;&#31232;&#30095;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#38469;&#25928;&#29575;&#65292;&#21487;&#22312;CPU&#21644;GPU&#36816;&#34892;&#26102;&#23454;&#29616;&#21152;&#36895;&#65292;&#24182;&#19988;&#35266;&#23519;&#21040;&#22312;&#21463;&#20869;&#23384;&#38480;&#21046;&#30340;&#27169;&#22411;&#20013;&#65292;&#31232;&#30095;&#24615;&#20063;&#21487;&#29992;&#20110;&#20943;&#23569;&#20869;&#23384;&#24102;&#23485;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#35757;&#32451;&#36807;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#31934;&#30830;&#30340;&#31232;&#30095;&#24494;&#35843;&#65292;&#21363;&#22312;&#19987;&#38376;&#20219;&#21153;&#19978;&#23545;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#21516;&#26102;&#22312;&#26435;&#37325;&#19978;&#24341;&#20837;&#31232;&#30095;&#24615;&#12290;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22522;&#20110;&#25439;&#22833;&#30340;&#26631;&#20934;&#24494;&#35843;&#21487;&#33021;&#26080;&#27861;&#24674;&#22797;&#20934;&#30830;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#31232;&#30095;&#24773;&#20917;&#19979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23545;&#33976;&#39311;&#31867;&#22411;&#30340;&#25439;&#22833;&#36827;&#34892;&#20102;&#35814;&#32454;&#30740;&#31350;&#65292;&#30830;&#23450;&#20102;&#19968;&#31181;&#22522;&#20110;L2&#33539;&#25968;&#30340;&#33976;&#39311;&#26041;&#27861;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;SquareHead&#65292;&#21363;&#20351;&#22312;&#26356;&#39640;&#30340;&#31232;&#30095;&#24615;&#19979;&#65292;&#23427;&#20063;&#33021;&#23454;&#29616;&#20934;&#30830;&#30340;&#24674;&#22797;&#65292;&#36866;&#29992;&#20110;&#25152;&#26377;&#27169;&#22411;&#31867;&#22411;&#12290;&#22312;&#23454;&#38469;&#25928;&#29575;&#26041;&#38754;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#31232;&#30095;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#31232;&#30095;&#24615;&#22312;CPU&#21644;GPU&#36816;&#34892;&#26102;&#23454;&#29616;&#21152;&#36895;&#12290;&#34429;&#28982;&#26631;&#20934;&#26041;&#27861;&#26159;&#21033;&#29992;&#31232;&#30095;&#24615;&#36827;&#34892;&#35745;&#31639;&#20943;&#23569;&#65292;&#20294;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22312;&#21463;&#20869;&#23384;&#38480;&#21046;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#31232;&#30095;&#24615;&#20063;&#21487;&#20197;&#29992;&#20110;&#20943;&#23569;&#20869;&#23384;&#24102;&#23485;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#30001;&#20110;&#31232;&#30095;&#24615;&#23548;&#33268;&#30340;&#36895;&#24230;&#25552;&#21319;&#20197;&#21450;&#24674;&#22797;&#20934;&#30830;&#24615;&#30340;&#31471;&#21040;&#31471;&#32467;&#26524;&#65292;&#24212;&#29992;&#20110;T5 (&#35821;&#35328;&#32763;&#35793;)&#20219;&#21153;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of accurate sparse finetuning of large language models (LLMs), that is, finetuning pretrained LLMs on specialized tasks, while inducing sparsity in their weights. On the accuracy side, we observe that standard loss-based finetuning may fail to recover accuracy, especially at high sparsities. To address this, we perform a detailed study of distillation-type losses, determining an L2-based distillation approach we term SquareHead which enables accurate recovery even at higher sparsities, across all model types. On the practical efficiency side, we show that sparse LLMs can be executed with speedups by taking advantage of sparsity, for both CPU and GPU runtimes. While the standard approach is to leverage sparsity for computational reduction, we observe that in the case of memory-bound LLMs sparsity can also be leveraged for reducing memory bandwidth. We exhibit end-to-end results showing speedups due to sparsity, while recovering accuracy, on T5 (language translati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PICProp&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#21452;&#23618;&#20248;&#21270;&#65292;&#29992;&#20110;&#22312;&#28145;&#24230;&#23398;&#20064;&#21644;&#29289;&#29702;&#20449;&#24687;&#23398;&#20064;&#20013;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#19981;&#36827;&#34892;&#24378;&#22823;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#35745;&#31639;&#26377;&#25928;&#30340;&#32622;&#20449;&#21306;&#38388;&#65288;CI&#65289;&#65292;&#24182;&#19988;&#36890;&#36807;&#20256;&#25773;&#32622;&#20449;&#24230;&#23454;&#29616;&#20102;&#25968;&#25454;&#20301;&#32622;&#21040;&#25972;&#20010;&#22495;&#30340;&#32622;&#20449;&#24230;&#20256;&#25773;&#12290;</title><link>http://arxiv.org/abs/2310.06923</link><description>&lt;p&gt;
PICProp&#65306;&#29992;&#20110;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#29289;&#29702;&#20449;&#24687;&#32622;&#20449;&#20256;&#25773;
&lt;/p&gt;
&lt;p&gt;
PICProp: Physics-Informed Confidence Propagation for Uncertainty Quantification. (arXiv:2310.06923v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06923
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PICProp&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#21452;&#23618;&#20248;&#21270;&#65292;&#29992;&#20110;&#22312;&#28145;&#24230;&#23398;&#20064;&#21644;&#29289;&#29702;&#20449;&#24687;&#23398;&#20064;&#20013;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#19981;&#36827;&#34892;&#24378;&#22823;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#35745;&#31639;&#26377;&#25928;&#30340;&#32622;&#20449;&#21306;&#38388;&#65288;CI&#65289;&#65292;&#24182;&#19988;&#36890;&#36807;&#20256;&#25773;&#32622;&#20449;&#24230;&#23454;&#29616;&#20102;&#25968;&#25454;&#20301;&#32622;&#21040;&#25972;&#20010;&#22495;&#30340;&#32622;&#20449;&#24230;&#20256;&#25773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#21644;&#29289;&#29702;&#20449;&#24687;&#23398;&#20064;&#20013;&#65292;&#26631;&#20934;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#23384;&#22312;&#30528;&#25345;&#20037;&#30340;&#23616;&#38480;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#38656;&#35201;&#23545;&#25968;&#25454;&#30340;&#21487;&#33021;&#24615;&#20570;&#20986;&#24378;&#28872;&#30340;&#20551;&#35774;&#65292;&#24615;&#33021;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#20808;&#39564;&#30340;&#36873;&#25321;&#65292;&#24182;&#19988;&#21518;&#39564;&#21482;&#33021;&#20197;&#36817;&#20284;&#30340;&#26041;&#24335;&#36827;&#34892;&#37319;&#26679;&#65292;&#20174;&#32780;&#30001;&#20110;&#30456;&#20851;&#30340;&#35745;&#31639;&#25104;&#26412;&#23548;&#33268;&#36817;&#20284;&#31934;&#24230;&#36739;&#24046;&#12290;&#26412;&#25991;&#25552;&#20986;&#24182;&#30740;&#31350;&#20102;&#23545;&#30830;&#23450;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#32622;&#20449;&#21306;&#38388;&#65288;CI&#65289;&#20272;&#35745;&#20316;&#20026;&#19968;&#20010;&#26032;&#30340;&#38382;&#39064;&#12290;&#21363;&#65292;&#22312;&#25972;&#20010;&#21306;&#22495;&#20013;&#20197;&#27010;&#29575;&#25285;&#20445;&#30340;&#24418;&#24335;&#20256;&#25773;&#32622;&#20449;&#24230;&#65292;&#20197;&#36798;&#21040;&#25968;&#25454;&#20301;&#32622;&#21040;&#25972;&#20010;&#22495;&#30340;&#32622;&#20449;&#24230;&#20256;&#25773;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#29289;&#29702;&#20449;&#24687;&#32622;&#20449;&#20256;&#25773;&#65288;PICProp&#65289;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#21452;&#23618;&#20248;&#21270;&#26469;&#35745;&#31639;&#19968;&#20010;&#26377;&#25928;&#30340;CI&#65292;&#32780;&#19981;&#38656;&#35201;&#36827;&#34892;&#22823;&#37327;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#25105;&#20204;&#26041;&#27861;&#26377;&#25928;&#24615;&#30340;&#23450;&#29702;&#20197;&#21450;&#38024;&#23545;&#29289;&#29702;&#20449;&#24687;&#23398;&#20064;&#30340;&#35745;&#31639;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Standard approaches for uncertainty quantification in deep learning and physics-informed learning have persistent limitations. Indicatively, strong assumptions regarding the data likelihood are required, the performance highly depends on the selection of priors, and the posterior can be sampled only approximately, which leads to poor approximations because of the associated computational cost. This paper introduces and studies confidence interval (CI) estimation for deterministic partial differential equations as a novel problem. That is, to propagate confidence, in the form of CIs, from data locations to the entire domain with probabilistic guarantees. We propose a method, termed Physics-Informed Confidence Propagation (PICProp), based on bi-level optimization to compute a valid CI without making heavy assumptions. We provide a theorem regarding the validity of our method, and computational experiments, where the focus is on physics-informed learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#31532;&#22235;&#20195;&#33521;&#29305;&#23572;&#38887;&#24615;&#22788;&#29702;&#22120;&#36827;&#34892;&#20998;&#24067;&#24335;&#36801;&#31227;&#23398;&#20064;&#65292;&#36890;&#36807;&#20351;&#29992;&#33521;&#29305;&#23572;&#39640;&#32423;&#30697;&#38453;&#25193;&#23637;&#65288;AMX&#65289;&#21644;Horovod&#22312;Image Classification TensorFlow&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#25509;&#36817;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.06916</link><description>&lt;p&gt;
&#21033;&#29992;&#31532;&#22235;&#20195;&#33521;&#29305;&#23572;&#38887;&#24615;&#22788;&#29702;&#22120;&#36827;&#34892;&#20998;&#24067;&#24335;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Distributed Transfer Learning with 4th Gen Intel Xeon Processors. (arXiv:2310.06916v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#31532;&#22235;&#20195;&#33521;&#29305;&#23572;&#38887;&#24615;&#22788;&#29702;&#22120;&#36827;&#34892;&#20998;&#24067;&#24335;&#36801;&#31227;&#23398;&#20064;&#65292;&#36890;&#36807;&#20351;&#29992;&#33521;&#29305;&#23572;&#39640;&#32423;&#30697;&#38453;&#25193;&#23637;&#65288;AMX&#65289;&#21644;Horovod&#22312;Image Classification TensorFlow&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#25509;&#36817;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#21644;&#33521;&#29305;&#23572;&#38887;&#24615;&#22788;&#29702;&#22120;&#65292;&#29305;&#21035;&#26159;&#31532;&#22235;&#20195;&#33521;&#29305;&#23572;&#38887;&#24615;&#21487;&#25193;&#23637;&#22788;&#29702;&#22120;&#65292;&#25171;&#30772;&#20102;&#20256;&#32479;&#35266;&#24565;&#65292;&#21363;&#35757;&#32451;&#20027;&#35201;&#20381;&#36182;&#20110;GPU&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#33521;&#29305;&#23572;&#39640;&#32423;&#30697;&#38453;&#25193;&#23637;&#65288;AMX&#65289;&#21644;Horovod&#36827;&#34892;&#20998;&#24067;&#24335;&#35757;&#32451;&#65292;&#22312;&#20844;&#24320;&#21487;&#29992;&#30340;Image Classification TensorFlow&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#25509;&#36817;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#20998;&#31867;&#20934;&#30830;&#24615;&#30340;&#26696;&#20363;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we explore how transfer learning, coupled with Intel Xeon, specifically 4th Gen Intel Xeon scalable processor, defies the conventional belief that training is primarily GPU-dependent. We present a case study where we achieved near state-of-the-art accuracy for image classification on a publicly available Image Classification TensorFlow dataset using Intel Advanced Matrix Extensions(AMX) and distributed training with Horovod.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#23433;&#20840;&#23884;&#20837;&#24335;MDP&#20013;&#32467;&#21512;&#36712;&#36857;&#20248;&#21270;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#23433;&#20840;&#32422;&#26463;&#23884;&#20837;&#21160;&#20316;&#31354;&#38388;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#22312;&#26368;&#22823;&#21270;&#22870;&#21169;&#21644;&#36981;&#23432;&#23433;&#20840;&#32422;&#26463;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#65292;&#24182;&#22312;&#25361;&#25112;&#24615;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20248;&#24322;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.06903</link><description>&lt;p&gt;
&#22312;&#20855;&#26377;&#36712;&#36857;&#20248;&#21270;&#30340;&#23433;&#20840;&#23884;&#20837;&#24335;MDP&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning in a Safety-Embedded MDP with Trajectory Optimization. (arXiv:2310.06903v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06903
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#23433;&#20840;&#23884;&#20837;&#24335;MDP&#20013;&#32467;&#21512;&#36712;&#36857;&#20248;&#21270;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#23433;&#20840;&#32422;&#26463;&#23884;&#20837;&#21160;&#20316;&#31354;&#38388;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#22312;&#26368;&#22823;&#21270;&#22870;&#21169;&#21644;&#36981;&#23432;&#23433;&#20840;&#32422;&#26463;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#65292;&#24182;&#22312;&#25361;&#25112;&#24615;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20248;&#24322;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#22312;&#23558;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#24212;&#29992;&#20110;&#23433;&#20840;&#20851;&#38190;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#36215;&#30528;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#35299;&#20915;&#20102;&#26368;&#22823;&#21270;&#22870;&#21169;&#21644;&#36981;&#23432;&#23433;&#20840;&#32422;&#26463;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#24378;&#21270;&#23398;&#20064;&#19982;&#36712;&#36857;&#20248;&#21270;&#30456;&#32467;&#21512;&#65292;&#26377;&#25928;&#22320;&#31649;&#29702;&#36825;&#31181;&#26435;&#34913;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#23433;&#20840;&#32422;&#26463;&#23884;&#20837;&#21040;&#20462;&#25913;&#21518;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#30340;&#21160;&#20316;&#31354;&#38388;&#20013;&#12290;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#36890;&#36807;&#36712;&#36857;&#20248;&#21270;&#22120;&#20135;&#29983;&#19968;&#31995;&#21015;&#34892;&#21160;&#65292;&#36825;&#20123;&#34892;&#21160;&#36716;&#21270;&#20026;&#23433;&#20840;&#36712;&#36857;&#65292;&#20174;&#32780;&#26377;&#25928;&#30830;&#20445;&#23433;&#20840;&#24182;&#25552;&#39640;&#35757;&#32451;&#31283;&#23450;&#24615;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#22312;&#25361;&#25112;&#24615;&#30340;Safety Gym&#20219;&#21153;&#30340;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#23454;&#29616;&#20102;&#26174;&#33879;&#26356;&#39640;&#30340;&#22870;&#21169;&#21644;&#20960;&#20046;&#38646;&#30340;&#23433;&#20840;&#36829;&#35268;&#12290;&#35813;&#26041;&#27861;&#22312;&#19968;&#20010;&#30495;&#23454;&#26426;&#22120;&#20154;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#24615;&#24471;&#21040;&#20102;&#35777;&#26126;&#65292;&#35813;&#20219;&#21153;&#28041;&#21450;&#25512;&#21160;&#31665;&#23376;&#31359;&#36807;&#38556;&#30861;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Safe Reinforcement Learning (RL) plays an important role in applying RL algorithms to safety-critical real-world applications, addressing the trade-off between maximizing rewards and adhering to safety constraints. This work introduces a novel approach that combines RL with trajectory optimization to manage this trade-off effectively. Our approach embeds safety constraints within the action space of a modified Markov Decision Process (MDP). The RL agent produces a sequence of actions that are transformed into safe trajectories by a trajectory optimizer, thereby effectively ensuring safety and increasing training stability. This novel approach excels in its performance on challenging Safety Gym tasks, achieving significantly higher rewards and near-zero safety violations during inference. The method's real-world applicability is demonstrated through a safe and effective deployment in a real robot task of box-pushing around obstacles.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20061;&#22825;&#26234;&#33021;&#32593;&#32476;&#20223;&#30495;&#24179;&#21488;&#65292;&#25552;&#20379;&#20102;&#26080;&#32447;&#36890;&#20449;&#20223;&#30495;&#25968;&#25454;&#26381;&#21153;&#65292;&#21253;&#21547;&#21487;&#25193;&#23637;&#30340;&#20223;&#30495;&#22120;&#21151;&#33021;&#65292;&#25903;&#25345;&#24320;&#25918;&#26381;&#21153;&#21644;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#35757;&#32451;&#65292;&#24182;&#20801;&#35768;&#29992;&#25143;&#19978;&#20256;&#21644;&#26356;&#26032;&#21442;&#25968;&#37197;&#32622;&#12290;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#21253;&#25324;&#26356;&#28145;&#20837;&#22320;&#25506;&#32034;&#19994;&#21153;&#22330;&#26223;&#21644;&#20248;&#21270;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2310.06858</link><description>&lt;p&gt;
&#20061;&#22825;&#26234;&#33021;&#32593;&#32476;&#20223;&#30495;&#24179;&#21488;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Design of JiuTian Intelligent Network Simulation Platform. (arXiv:2310.06858v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06858
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20061;&#22825;&#26234;&#33021;&#32593;&#32476;&#20223;&#30495;&#24179;&#21488;&#65292;&#25552;&#20379;&#20102;&#26080;&#32447;&#36890;&#20449;&#20223;&#30495;&#25968;&#25454;&#26381;&#21153;&#65292;&#21253;&#21547;&#21487;&#25193;&#23637;&#30340;&#20223;&#30495;&#22120;&#21151;&#33021;&#65292;&#25903;&#25345;&#24320;&#25918;&#26381;&#21153;&#21644;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#35757;&#32451;&#65292;&#24182;&#20801;&#35768;&#29992;&#25143;&#19978;&#20256;&#21644;&#26356;&#26032;&#21442;&#25968;&#37197;&#32622;&#12290;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#21253;&#25324;&#26356;&#28145;&#20837;&#22320;&#25506;&#32034;&#19994;&#21153;&#22330;&#26223;&#21644;&#20248;&#21270;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20061;&#22825;&#26234;&#33021;&#32593;&#32476;&#20223;&#30495;&#24179;&#21488;&#65292;&#35813;&#24179;&#21488;&#21487;&#20197;&#20026;&#24320;&#25918;&#21019;&#26032;&#24179;&#21488;&#25552;&#20379;&#26080;&#32447;&#36890;&#20449;&#20223;&#30495;&#25968;&#25454;&#26381;&#21153;&#12290;&#35813;&#24179;&#21488;&#21253;&#21547;&#19968;&#31995;&#21015;&#21487;&#25193;&#23637;&#30340;&#20223;&#30495;&#22120;&#21151;&#33021;&#65292;&#25552;&#20379;&#24320;&#25918;&#26381;&#21153;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#22522;&#20110;&#20223;&#30495;&#29615;&#22659;&#21644;&#25968;&#25454;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#21644;&#25512;&#29702;&#12290;&#27492;&#22806;&#65292;&#23427;&#20801;&#35768;&#29992;&#25143;&#36890;&#36807;&#19978;&#20256;&#21644;&#26356;&#26032;&#21442;&#25968;&#37197;&#32622;&#26469;&#35299;&#20915;&#19981;&#21516;&#22330;&#26223;&#19979;&#30340;&#20248;&#21270;&#20219;&#21153;&#12290;&#26412;&#25991;&#20027;&#35201;&#20174;&#32972;&#26223;&#12289;&#25972;&#20307;&#26550;&#26500;&#12289;&#20223;&#30495;&#22120;&#12289;&#19994;&#21153;&#22330;&#26223;&#21644;&#26410;&#26469;&#26041;&#21521;&#31561;&#26041;&#38754;&#20171;&#32461;&#20102;&#35813;&#24179;&#21488;&#21450;&#20854;&#24320;&#25918;&#26381;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduced the JiuTian Intelligent Network Simulation Platform, which can provide wireless communication simulation data services for the Open Innovation Platform. The platform contains a series of scalable simulator functionalities, offering open services that enable users to use reinforcement learning algorithms for model training and inference based on simulation environments and data. Additionally, it allows users to address optimization tasks in different scenarios by uploading and updating parameter configurations. The platform and its open services were primarily introduced from the perspectives of background, overall architecture, simulator, business scenarios, and future directions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#22312;&#25945;&#23398;&#21644;&#23398;&#20064;&#20013;&#22914;&#20309;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#35752;&#35770;&#20102;&#25945;&#32946;&#39046;&#22495;&#20013;&#24050;&#32463;&#21457;&#29983;&#30340;&#20154;&#24037;&#26234;&#33021;&#20107;&#20214;&#12290;&#25105;&#20204;&#20027;&#24352;&#24341;&#20837;&#20154;&#24037;&#26234;&#33021;&#25919;&#31574;&#21644;&#35268;&#33539;&#65292;&#20197;&#25552;&#39640;&#25945;&#32946;&#24037;&#20855;&#30340;&#24847;&#35782;&#65292;&#24182;&#20943;&#23569;&#25945;&#32946;&#20013;&#30340;&#20154;&#24037;&#26234;&#33021;&#20107;&#20214;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2310.06856</link><description>&lt;p&gt;
&#21191;&#25954;&#30340;&#26032;&#19990;&#30028;&#65306;&#20154;&#24037;&#26234;&#33021;&#22312;&#25945;&#23398;&#19982;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Brave new world: Artificial Intelligence in teaching and learning. (arXiv:2310.06856v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06856
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#22312;&#25945;&#23398;&#21644;&#23398;&#20064;&#20013;&#22914;&#20309;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#35752;&#35770;&#20102;&#25945;&#32946;&#39046;&#22495;&#20013;&#24050;&#32463;&#21457;&#29983;&#30340;&#20154;&#24037;&#26234;&#33021;&#20107;&#20214;&#12290;&#25105;&#20204;&#20027;&#24352;&#24341;&#20837;&#20154;&#24037;&#26234;&#33021;&#25919;&#31574;&#21644;&#35268;&#33539;&#65292;&#20197;&#25552;&#39640;&#25945;&#32946;&#24037;&#20855;&#30340;&#24847;&#35782;&#65292;&#24182;&#20943;&#23569;&#25945;&#32946;&#20013;&#30340;&#20154;&#24037;&#26234;&#33021;&#20107;&#20214;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20030;&#20363;&#35828;&#26126;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25945;&#23398;&#21644;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#25945;&#32946;&#39046;&#22495;&#24050;&#32463;&#21457;&#29983;&#30340;&#20154;&#24037;&#26234;&#33021;&#20107;&#20214;&#65292;&#24182;&#25552;&#20986;&#36843;&#20999;&#38656;&#35201;&#22312;&#22823;&#23398;&#24341;&#20837;&#20154;&#24037;&#26234;&#33021;&#25919;&#31574;&#20197;&#21450;&#36827;&#34892;&#35268;&#33539;&#20154;&#24037;&#26234;&#33021;&#30340;&#25345;&#32493;&#31574;&#30053;&#12290;&#20851;&#20110;&#20154;&#24037;&#26234;&#33021;&#25919;&#31574;&#65292;&#25105;&#20204;&#35748;&#20026;&#27599;&#20010;&#26426;&#26500;&#37117;&#24212;&#35813;&#21046;&#23450;&#19968;&#39033;&#20851;&#20110;&#20154;&#24037;&#26234;&#33021;&#22312;&#25945;&#23398;&#21644;&#23398;&#20064;&#20013;&#30340;&#25919;&#31574;&#12290;&#36825;&#19968;&#28857;&#33267;&#20851;&#37325;&#35201;&#65292;&#33267;&#23569;&#26377;&#20004;&#20010;&#21407;&#22240;&#65306;&#65288;&#19968;&#65289;&#25552;&#39640;&#23545;&#21508;&#31181;&#25945;&#32946;&#24037;&#20855;&#30340;&#24847;&#35782;&#65292;&#36825;&#20123;&#24037;&#20855;&#26082;&#21487;&#20197;&#31215;&#26497;&#22320;&#65292;&#20063;&#21487;&#33021;&#23545;&#25945;&#32946;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#65307;&#65288;&#20108;&#65289;&#26368;&#23567;&#21270;&#25945;&#32946;&#20013;&#30340;&#20154;&#24037;&#26234;&#33021;&#20107;&#20214;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
We exemplify how Large Language Models are used in both teaching and learning. We also discuss the AI incidents that have already occurred in the education domain, and we argue for the urgent need to introduce AI policies in universities and for the ongoing strategies to regulate AI. Regarding policy for AI, our view is that each institution should have a policy for AI in teaching and learning. This is important from at least twofolds: (i) to raise awareness on the numerous educational tools that can both positively and negatively affect education; (ii) to minimise the risk of AI incidents in education.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#32593;&#32476;&#27969;&#37327;&#20998;&#31867;&#20013;&#30340;&#21160;&#24577;&#21518;&#38376;&#25915;&#20987;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36951;&#20256;&#31639;&#27861;&#30340;&#26032;&#22411;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;GABAttack&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#20248;&#21270;&#21518;&#38376;&#35302;&#21457;&#22120;&#21442;&#25968;&#30340;&#20540;&#21644;&#20301;&#32622;&#65292;&#20197;&#23454;&#29616;&#23545;&#20840;&#23616;&#27169;&#22411;&#30340;&#31713;&#25913;&#12290;</title><link>http://arxiv.org/abs/2310.06855</link><description>&lt;p&gt;
&#22522;&#20110;&#36951;&#20256;&#31639;&#27861;&#30340;&#21160;&#24577;&#21518;&#38376;&#25915;&#20987;&#23545;&#20110;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#32593;&#32476;&#27969;&#37327;&#20998;&#31867;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Genetic Algorithm-Based Dynamic Backdoor Attack on Federated Learning-Based Network Traffic Classification. (arXiv:2310.06855v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06855
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#32593;&#32476;&#27969;&#37327;&#20998;&#31867;&#20013;&#30340;&#21160;&#24577;&#21518;&#38376;&#25915;&#20987;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36951;&#20256;&#31639;&#27861;&#30340;&#26032;&#22411;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;GABAttack&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#20248;&#21270;&#21518;&#38376;&#35302;&#21457;&#22120;&#21442;&#25968;&#30340;&#20540;&#21644;&#20301;&#32622;&#65292;&#20197;&#23454;&#29616;&#23545;&#20840;&#23616;&#27169;&#22411;&#30340;&#31713;&#25913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#20351;&#22810;&#20010;&#23458;&#25143;&#31471;&#33021;&#22815;&#20849;&#21516;&#20026;&#30001;&#20013;&#22830;&#26381;&#21153;&#22120;&#32452;&#32455;&#30340;&#20840;&#23616;&#27169;&#22411;&#30340;&#23398;&#20064;&#20570;&#20986;&#36129;&#29486;&#12290;&#36825;&#31181;&#23398;&#20064;&#26041;&#26696;&#20419;&#36827;&#20102;&#23458;&#25143;&#31471;&#30340;&#25968;&#25454;&#38544;&#31169;&#24182;&#20943;&#23569;&#20102;&#36890;&#20449;&#24320;&#38144;&#12290;&#22312;&#32593;&#32476;&#27969;&#37327;&#20998;&#31867;&#31561;&#24212;&#29992;&#20013;&#65292;&#36825;&#26377;&#21161;&#20110;&#38544;&#34255;&#32593;&#32476;&#30340;&#28431;&#27934;&#21644;&#24369;&#28857;&#12290;&#28982;&#32780;&#65292;&#32852;&#37030;&#23398;&#20064;&#23545;&#21518;&#38376;&#25915;&#20987;&#26159;&#33030;&#24369;&#30340;&#65292;&#21363;&#23545;&#25163;&#21521;&#20840;&#23616;&#27169;&#22411;&#20013;&#27880;&#20837;&#31713;&#25913;&#30340;&#27169;&#22411;&#26356;&#26032;&#12290;&#36825;&#20123;&#26356;&#26032;&#22312;&#20840;&#23616;&#27169;&#22411;&#20013;&#27880;&#20837;&#20102;&#26174;&#33879;&#30340;&#21151;&#33021;&#65292;&#21487;&#20197;&#36890;&#36807;&#29305;&#23450;&#30340;&#36755;&#20837;&#27169;&#24335;&#26469;&#21551;&#21160;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#32593;&#32476;&#27969;&#37327;&#20998;&#31867;&#27169;&#22411;&#23545;&#20110;&#36825;&#20123;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#20173;&#26410;&#34987;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GABAttack&#65292;&#19968;&#31181;&#22522;&#20110;&#36951;&#20256;&#31639;&#27861;&#30340;&#38024;&#23545;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#32593;&#32476;&#27969;&#37327;&#20998;&#31867;&#30340;&#26032;&#22411;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#12290;GABAttack&#21033;&#29992;&#36951;&#20256;&#31639;&#27861;&#26469;&#20248;&#21270;&#21518;&#38376;&#35302;&#21457;&#22120;&#21442;&#25968;&#30340;&#20540;&#21644;&#20301;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning enables multiple clients to collaboratively contribute to the learning of a global model orchestrated by a central server. This learning scheme promotes clients' data privacy and requires reduced communication overheads. In an application like network traffic classification, this helps hide the network vulnerabilities and weakness points. However, federated learning is susceptible to backdoor attacks, in which adversaries inject manipulated model updates into the global model. These updates inject a salient functionality in the global model that can be launched with specific input patterns. Nonetheless, the vulnerability of network traffic classification models based on federated learning to these attacks remains unexplored. In this paper, we propose GABAttack, a novel genetic algorithm-based backdoor attack against federated learning for network traffic classification. GABAttack utilizes a genetic algorithm to optimize the values and locations of backdoor trigger pa
&lt;/p&gt;</description></item><item><title>&#32852;&#21512;&#25552;&#20986;&#30340;JoCoT&#26041;&#27861;&#22312;&#20154;&#31867;&#36300;&#20498;&#20107;&#20214;&#20998;&#31867;&#20013;&#24212;&#29992;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#65292;&#36890;&#36807;&#20351;&#29992;&#20004;&#20010;&#25945;&#24072;&#27169;&#22359;&#21644;&#19968;&#20010;&#23398;&#29983;&#27169;&#22359;&#65292;&#25552;&#39640;&#20102;&#40065;&#26834;&#24615;&#21644;&#24615;&#33021;&#65292;&#24182;&#19988;&#37319;&#29992;&#20154;&#20307;&#39592;&#26550;&#25968;&#25454;&#20197;&#20445;&#25252;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2310.06854</link><description>&lt;p&gt;
&#26080;&#24178;&#25200;&#26631;&#31614;&#23398;&#20064;&#22312;&#20154;&#31867;&#36300;&#20498;&#20107;&#20214;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;&#65306;&#32852;&#21512;&#21512;&#20316;&#35757;&#32451;&#19982;&#19977;&#20301;&#19968;&#20307;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Learning with Noisy Labels for Human Fall Events Classification: Joint Cooperative Training with Trinity Networks. (arXiv:2310.06854v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06854
&lt;/p&gt;
&lt;p&gt;
&#32852;&#21512;&#25552;&#20986;&#30340;JoCoT&#26041;&#27861;&#22312;&#20154;&#31867;&#36300;&#20498;&#20107;&#20214;&#20998;&#31867;&#20013;&#24212;&#29992;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#65292;&#36890;&#36807;&#20351;&#29992;&#20004;&#20010;&#25945;&#24072;&#27169;&#22359;&#21644;&#19968;&#20010;&#23398;&#29983;&#27169;&#22359;&#65292;&#25552;&#39640;&#20102;&#40065;&#26834;&#24615;&#21644;&#24615;&#33021;&#65292;&#24182;&#19988;&#37319;&#29992;&#20154;&#20307;&#39592;&#26550;&#25968;&#25454;&#20197;&#20445;&#25252;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#21475;&#32769;&#40836;&#21270;&#38382;&#39064;&#30340;&#21152;&#21095;&#65292;&#36300;&#20498;&#20107;&#20214;&#30340;&#20998;&#31867;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#20851;&#27880;&#12290;&#22312;&#28145;&#24230;&#23398;&#20064;&#30340;&#21457;&#23637;&#20013;&#65292;&#25968;&#25454;&#26631;&#31614;&#30340;&#36136;&#37327;&#33267;&#20851;&#37325;&#35201;&#12290;&#22823;&#22810;&#25968;&#25968;&#25454;&#38598;&#37117;&#26159;&#33258;&#21160;&#25110;&#21322;&#33258;&#21160;&#26631;&#35760;&#30340;&#65292;&#26679;&#26412;&#21487;&#33021;&#23384;&#22312;&#26631;&#27880;&#38169;&#35823;&#65292;&#36825;&#38480;&#21046;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#24615;&#33021;&#12290;&#26368;&#36817;&#20851;&#20110;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#30340;&#30740;&#31350;&#35777;&#23454;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#35757;&#32451;&#38454;&#27573;&#39318;&#20808;&#20851;&#27880;&#24178;&#20928;&#31616;&#21333;&#30340;&#23454;&#20363;&#65292;&#28982;&#21518;&#20851;&#27880;&#22122;&#22768;&#21644;&#22256;&#38590;&#30340;&#23454;&#20363;&#12290;&#20026;&#20102;&#35299;&#20915;&#23398;&#20064;&#22122;&#22768;&#26631;&#31614;&#30340;&#38382;&#39064;&#24182;&#20445;&#25252;&#20154;&#31867;&#21463;&#35797;&#32773;&#30340;&#38544;&#31169;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#32852;&#21512;&#21512;&#20316;&#35757;&#32451;&#19982;&#19977;&#20301;&#19968;&#20307;&#32593;&#32476;&#65288;JoCoT&#65289;&#12290;&#20026;&#20102;&#20943;&#36731;&#38544;&#31169;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#20154;&#20307;&#39592;&#26550;&#25968;&#25454;&#12290;&#22312;&#25552;&#20986;&#30340;JoCoT&#20013;&#65292;&#36890;&#36807;&#20351;&#29992;&#20004;&#20010;&#25945;&#24072;&#27169;&#22359;&#21644;&#19968;&#20010;&#23398;&#29983;&#27169;&#22359;&#26469;&#25913;&#21892;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#26694;&#26550;&#30340;&#40065;&#26834;&#24615;&#21644;&#24615;&#33021;&#12290;&#20026;&#20102;&#20943;&#36731;&#38169;&#35823;&#36873;&#25321;&#30340;&#38382;&#39064;&#65292;&#39044;&#27979;&#36807;&#31243;&#21644;rae&#30340;&#36873;&#25321;&#36981;&#24490;&#20102;&#22266;&#26377;&#30340;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increasing ageing population, fall events classification has drawn much research attention. In the development of deep learning, the quality of data labels is crucial. Most of the datasets are labelled automatically or semi-automatically, and the samples may be mislabeled, which constrains the performance of Deep Neural Networks (DNNs). Recent research on noisy label learning confirms that neural networks first focus on the clean and simple instances and then follow the noisy and hard instances in the training stage. To address the learning with noisy label problem and protect the human subjects' privacy, we propose a simple but effective approach named Joint Cooperative training with Trinity Networks (JoCoT). To mitigate the privacy issue, human skeleton data are used. The robustness and performance of the noisy label learning framework is improved by using the two teacher modules and one student module in the proposed JoCoT. To mitigate the incorrect selections, the predicti
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#33258;&#21160;&#20174;&#35821;&#38899;&#20013;&#21512;&#25104;3D&#20154;&#20307;&#25163;&#21183;&#12290;&#36890;&#36807;&#24341;&#20837;&#21464;&#20998;Transformer&#21644;&#27169;&#24577;&#20301;&#32622;&#23884;&#20837;&#23618;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#24182;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#25163;&#21183;&#12290;&#21478;&#22806;&#65292;&#36890;&#36807;&#20869;&#37096;&#27169;&#24577;&#39044;&#35757;&#32451;&#26041;&#26696;&#26469;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.06851</link><description>&lt;p&gt;
BodyFormer: &#22522;&#20110;Transformer&#30340;&#35821;&#20041;&#24341;&#23548;&#30340;3D&#20154;&#20307;&#25163;&#21183;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
BodyFormer: Semantics-guided 3D Body Gesture Synthesis with Transformer. (arXiv:2310.06851v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06851
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#33258;&#21160;&#20174;&#35821;&#38899;&#20013;&#21512;&#25104;3D&#20154;&#20307;&#25163;&#21183;&#12290;&#36890;&#36807;&#24341;&#20837;&#21464;&#20998;Transformer&#21644;&#27169;&#24577;&#20301;&#32622;&#23884;&#20837;&#23618;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#24182;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#25163;&#21183;&#12290;&#21478;&#22806;&#65292;&#36890;&#36807;&#20869;&#37096;&#27169;&#24577;&#39044;&#35757;&#32451;&#26041;&#26696;&#26469;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#20174;&#35821;&#38899;&#20013;&#21512;&#25104;&#25163;&#21183;&#26159;&#19968;&#20010;&#21560;&#24341;&#30740;&#31350;&#20154;&#21592;&#20851;&#27880;&#30340;&#35805;&#39064;&#65292;&#29992;&#20110;&#36828;&#31243;&#36890;&#20449;&#12289;&#35270;&#39057;&#28216;&#25103;&#21644;&#20803;&#23431;&#23449;&#24212;&#29992;&#12290;&#30001;&#20110;&#38382;&#39064;&#30340;&#38543;&#26426;&#24615;&#21644;&#35757;&#32451;&#25152;&#38656;&#30340;&#20016;&#23500;&#36328;&#27169;&#24577;&#25968;&#25454;&#38598;&#30340;&#32570;&#20047;&#65292;&#23398;&#20064;&#35821;&#38899;&#21644;3D&#20840;&#36523;&#25163;&#21183;&#20043;&#38388;&#30340;&#26144;&#23556;&#26159;&#22256;&#38590;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26032;&#39062;&#26694;&#26550;&#65292;&#29992;&#20110;&#33258;&#21160;&#20174;&#35821;&#38899;&#29983;&#25104;3D&#20154;&#20307;&#25163;&#21183;&#12290;&#20026;&#20102;&#23398;&#20064;&#35821;&#38899;&#26102;&#30340;&#25163;&#21183;&#30340;&#38543;&#26426;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21464;&#20998;Transformer&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#24314;&#27169;&#25163;&#21183;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#22312;&#25512;&#29702;&#26102;&#33021;&#22815;&#20135;&#29983;&#22810;&#26679;&#21270;&#30340;&#25163;&#21183;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#27169;&#24577;&#20301;&#32622;&#23884;&#20837;&#23618;&#65292;&#29992;&#20110;&#25429;&#25417;&#19981;&#21516;&#35821;&#38899;&#27169;&#24335;&#20013;&#30340;&#19981;&#21516;&#36816;&#21160;&#36895;&#24230;&#12290;&#20026;&#20102;&#24212;&#23545;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#20869;&#37096;&#27169;&#24577;&#39044;&#35757;&#32451;&#26041;&#26696;&#65292;&#21487;&#20197;&#20174;&#26377;&#38480;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#35821;&#38899;&#21644;3D&#25163;&#21183;&#20043;&#38388;&#30340;&#22797;&#26434;&#26144;&#23556;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Automatic gesture synthesis from speech is a topic that has attracted researchers for applications in remote communication, video games and Metaverse. Learning the mapping between speech and 3D full-body gestures is difficult due to the stochastic nature of the problem and the lack of a rich cross-modal dataset that is needed for training. In this paper, we propose a novel transformer-based framework for automatic 3D body gesture synthesis from speech. To learn the stochastic nature of the body gesture during speech, we propose a variational transformer to effectively model a probabilistic distribution over gestures, which can produce diverse gestures during inference. Furthermore, we introduce a mode positional embedding layer to capture the different motion speeds in different speaking modes. To cope with the scarcity of data, we design an intra-modal pre-training scheme that can learn the complex mapping between the speech and the 3D gesture from a limited amount of data. Our system
&lt;/p&gt;</description></item><item><title>DeepTriNet&#26159;&#19968;&#31181;&#22522;&#20110;&#19977;&#32423;&#27880;&#24847;&#21147;&#30340;DeepLabv3+&#26550;&#26500;&#65292;&#29992;&#20110;&#21355;&#26143;&#22270;&#20687;&#30340;&#35821;&#20041;&#20998;&#21106;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#32467;&#21512;SENets&#21644;TAUs&#26725;&#25509;&#20102;&#32534;&#35299;&#30721;&#22120;&#36755;&#20986;&#19982;&#30456;&#20851;&#29305;&#24449;&#20043;&#38388;&#30340;&#35821;&#20041;&#24046;&#36317;&#65292;&#21516;&#26102;&#36890;&#36807;&#33258;&#25105;&#30417;&#30563;&#30830;&#23450;&#20102;&#26356;&#37325;&#35201;&#21644;&#26356;&#36890;&#29992;&#30340;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2310.06848</link><description>&lt;p&gt;
DeepTriNet:&#19968;&#31181;&#22522;&#20110;&#19977;&#32423;&#27880;&#24847;&#21147;&#30340;DeepLabv3+&#32467;&#26500;&#29992;&#20110;&#21355;&#26143;&#22270;&#20687;&#30340;&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
DeepTriNet: A Tri-Level Attention Based DeepLabv3+ Architecture for Semantic Segmentation of Satellite Images. (arXiv:2310.06848v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06848
&lt;/p&gt;
&lt;p&gt;
DeepTriNet&#26159;&#19968;&#31181;&#22522;&#20110;&#19977;&#32423;&#27880;&#24847;&#21147;&#30340;DeepLabv3+&#26550;&#26500;&#65292;&#29992;&#20110;&#21355;&#26143;&#22270;&#20687;&#30340;&#35821;&#20041;&#20998;&#21106;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#32467;&#21512;SENets&#21644;TAUs&#26725;&#25509;&#20102;&#32534;&#35299;&#30721;&#22120;&#36755;&#20986;&#19982;&#30456;&#20851;&#29305;&#24449;&#20043;&#38388;&#30340;&#35821;&#20041;&#24046;&#36317;&#65292;&#21516;&#26102;&#36890;&#36807;&#33258;&#25105;&#30417;&#30563;&#30830;&#23450;&#20102;&#26356;&#37325;&#35201;&#21644;&#26356;&#36890;&#29992;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21355;&#26143;&#22270;&#20687;&#30340;&#20998;&#21106;&#22312;&#36965;&#24863;&#24212;&#29992;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#26041;&#27861;&#22312;&#21355;&#26143;&#22270;&#20687;&#30340;&#35821;&#20041;&#20998;&#21106;&#20013;&#38754;&#20020;&#35782;&#21035;&#23567;&#23610;&#24230;&#30446;&#26631;&#30340;&#25361;&#25112;&#65292;&#20027;&#35201;&#21407;&#22240;&#26159;&#24573;&#30053;&#20102;&#24213;&#23618;&#32593;&#32476;&#30340;&#20302;&#32423;&#29305;&#24449;&#21644;&#30001;&#19981;&#21516;&#29305;&#24449;&#22270;&#21253;&#21547;&#19981;&#21516;&#25968;&#37327;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19977;&#32423;&#27880;&#24847;&#21147;&#30340;DeepLabv3+&#26550;&#26500;&#65288;DeepTriNet&#65289;&#29992;&#20110;&#21355;&#26143;&#22270;&#20687;&#30340;&#35821;&#20041;&#20998;&#21106;&#12290;&#25152;&#25552;&#20986;&#30340;&#28151;&#21512;&#26041;&#27861;&#32467;&#21512;&#20102;&#25380;&#21387;&#28608;&#21169;&#32593;&#32476;&#65288;SENets&#65289;&#21644;&#19977;&#32423;&#27880;&#24847;&#21147;&#21333;&#20803;&#65288;TAUs&#65289;&#19982;&#26222;&#36890;&#30340;DeepLabv3+&#26550;&#26500;&#65292;&#20854;&#20013;TAUs&#29992;&#20110;&#24357;&#21512;&#32534;&#35299;&#30721;&#22120;&#36755;&#20986;&#19982;SENets&#29992;&#20110;&#32473;&#30456;&#20851;&#29305;&#24449;&#20998;&#37197;&#26356;&#22810;&#26435;&#37325;&#20043;&#38388;&#30340;&#35821;&#20041;&#29305;&#24449;&#24046;&#36317;&#12290;&#25152;&#25552;&#20986;&#30340;DeepTriNet&#36890;&#36807;&#33258;&#25105;&#30417;&#30563;&#25214;&#21040;&#21738;&#20123;&#29305;&#24449;&#26159;&#26356;&#30456;&#20851;&#19988;&#26356;&#36890;&#29992;&#30340;&#65292;&#32780;&#19981;&#26159;&#25105;&#20204;&#23545;&#23427;&#20204;&#36827;&#34892;&#27880;&#37322;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;DeepTriNet&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The segmentation of satellite images is crucial in remote sensing applications. Existing methods face challenges in recognizing small-scale objects in satellite images for semantic segmentation primarily due to ignoring the low-level characteristics of the underlying network and due to containing distinct amounts of information by different feature maps. Thus, in this research, a tri-level attention-based DeepLabv3+ architecture (DeepTriNet) is proposed for the semantic segmentation of satellite images. The proposed hybrid method combines squeeze-and-excitation networks (SENets) and tri-level attention units (TAUs) with the vanilla DeepLabv3+ architecture, where the TAUs are used to bridge the semantic feature gap among encoders output and the SENets used to put more weight on relevant features. The proposed DeepTriNet finds which features are the more relevant and more generalized way by its self-supervision rather we annotate them. The study showed that the proposed DeepTriNet perfor
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35748;&#30693;&#26234;&#33021;&#20307;&#30340;&#20219;&#21153;&#30693;&#35782;&#26469;&#28304;&#65292;&#25506;&#32034;&#20102;&#23558;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#22806;&#37096;&#30693;&#35782;&#28304;&#29992;&#20110;&#35748;&#30693;&#31995;&#32479;&#30340;&#25361;&#25112;&#21644;&#26426;&#20250;&#65292;&#24182;&#25552;&#20986;&#20102;&#36890;&#36807;&#25972;&#21512;&#30693;&#35782;&#25552;&#21462;&#21644;&#35748;&#30693;&#26550;&#26500;&#33021;&#21147;&#26469;&#25552;&#39640;&#30693;&#35782;&#25552;&#21462;&#25928;&#26524;&#30340;&#21487;&#33021;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.06846</link><description>&lt;p&gt;
&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35748;&#30693;&#26234;&#33021;&#20307;&#30340;&#30693;&#35782;&#26469;&#28304;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Exploiting Language Models as a Source of Knowledge for Cognitive Agents. (arXiv:2310.06846v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06846
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35748;&#30693;&#26234;&#33021;&#20307;&#30340;&#20219;&#21153;&#30693;&#35782;&#26469;&#28304;&#65292;&#25506;&#32034;&#20102;&#23558;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#22806;&#37096;&#30693;&#35782;&#28304;&#29992;&#20110;&#35748;&#30693;&#31995;&#32479;&#30340;&#25361;&#25112;&#21644;&#26426;&#20250;&#65292;&#24182;&#25552;&#20986;&#20102;&#36890;&#36807;&#25972;&#21512;&#30693;&#35782;&#25552;&#21462;&#21644;&#35748;&#30693;&#26550;&#26500;&#33021;&#21147;&#26469;&#25552;&#39640;&#30693;&#35782;&#25552;&#21462;&#25928;&#26524;&#30340;&#21487;&#33021;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#19981;&#20165;&#21487;&#20197;&#23436;&#25104;&#21477;&#23376;&#34917;&#20840;&#65292;&#36824;&#21487;&#20197;&#36827;&#34892;&#38382;&#31572;&#12289;&#25688;&#35201;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#31561;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35748;&#30693;&#26234;&#33021;&#20307;&#30340;&#20219;&#21153;&#30693;&#35782;&#26469;&#28304;&#65292;&#21363;&#36890;&#36807;&#35748;&#30693;&#26550;&#26500;&#23454;&#29616;&#30340;&#26234;&#33021;&#20307;&#12290;&#25105;&#20204;&#35782;&#21035;&#20102;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35748;&#30693;&#31995;&#32479;&#22806;&#37096;&#30693;&#35782;&#28304;&#30340;&#25361;&#25112;&#21644;&#26426;&#20250;&#65292;&#20197;&#21450;&#36890;&#36807;&#23558;&#30693;&#35782;&#25552;&#21462;&#19982;&#35748;&#30693;&#26550;&#26500;&#33021;&#21147;&#25972;&#21512;&#26469;&#25552;&#39640;&#30693;&#35782;&#25552;&#21462;&#25928;&#26524;&#30340;&#21487;&#33021;&#26041;&#27861;&#65292;&#24182;&#20030;&#20363;&#20171;&#32461;&#25105;&#20204;&#26368;&#36817;&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) provide capabilities far beyond sentence completion, including question answering, summarization, and natural-language inference. While many of these capabilities have potential application to cognitive systems, our research is exploiting language models as a source of task knowledge for cognitive agents, that is, agents realized via a cognitive architecture. We identify challenges and opportunities for using language models as an external knowledge source for cognitive systems and possible ways to improve the effectiveness of knowledge extraction by integrating extraction with cognitive architecture capabilities, highlighting with examples from our recent work in this area.
&lt;/p&gt;</description></item><item><title>RobustEdge&#26159;&#19968;&#31181;&#20302;&#21151;&#32791;&#12289;&#36866;&#29992;&#20110;&#36793;&#32536;&#35774;&#22791;&#30340;&#23545;&#25239;&#26816;&#27979;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22312;&#20113;&#36793;&#31995;&#32479;&#20013;&#33021;&#37327;&#28010;&#36153;&#21644;&#35745;&#31639;&#24320;&#38144;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.06845</link><description>&lt;p&gt;
RobustEdge&#65306;&#38754;&#21521;&#20113;&#36793;&#31995;&#32479;&#30340;&#20302;&#21151;&#32791;&#23545;&#25239;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
RobustEdge: Low Power Adversarial Detection for Cloud-Edge Systems. (arXiv:2310.06845v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06845
&lt;/p&gt;
&lt;p&gt;
RobustEdge&#26159;&#19968;&#31181;&#20302;&#21151;&#32791;&#12289;&#36866;&#29992;&#20110;&#36793;&#32536;&#35774;&#22791;&#30340;&#23545;&#25239;&#26816;&#27979;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22312;&#20113;&#36793;&#31995;&#32479;&#20013;&#33021;&#37327;&#28010;&#36153;&#21644;&#35745;&#31639;&#24320;&#38144;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#30340;&#20113;&#36793;&#22330;&#26223;&#20013;&#65292;&#36793;&#32536;&#35774;&#22791;&#36827;&#34892;&#25968;&#25454;&#37319;&#38598;&#65292;&#25317;&#26377;&#36275;&#22815;&#36164;&#28304;&#30340;&#20113;&#31995;&#32479;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#25512;&#29702;&#20219;&#21153;&#65292;&#23545;&#25239;&#40065;&#26834;&#24615;&#23545;&#20110;&#21487;&#38752;&#24615;&#21644;&#26222;&#21450;&#37096;&#32626;&#33267;&#20851;&#37325;&#35201;&#12290;&#23545;&#25239;&#26816;&#27979;&#26159;&#20808;&#21069;&#25991;&#29486;&#20013;&#20351;&#29992;&#30340;&#20027;&#35201;&#23545;&#25239;&#38450;&#24481;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#22312;&#20808;&#21069;&#30340;&#26816;&#27979;&#26041;&#27861;&#20013;&#65292;&#26816;&#27979;&#22120;&#38468;&#21152;&#22312;&#20998;&#31867;&#22120;&#27169;&#22411;&#19978;&#65292;&#26816;&#27979;&#22120;&#21644;&#20998;&#31867;&#22120;&#19968;&#36215;&#36827;&#34892;&#23545;&#25239;&#26816;&#27979;&#65292;&#36825;&#38656;&#35201;&#24456;&#39640;&#30340;&#35745;&#31639;&#24320;&#38144;&#65292;&#22312;&#20302;&#21151;&#32791;&#36793;&#32536;&#35774;&#22791;&#19978;&#19981;&#21487;&#34892;&#12290;&#22240;&#27492;&#65292;&#20808;&#21069;&#30340;&#26041;&#27861;&#21482;&#33021;&#22312;&#20113;&#31471;&#36827;&#34892;&#23545;&#25239;&#26816;&#27979;&#65292;&#32780;&#19981;&#33021;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#36827;&#34892;&#12290;&#36825;&#24847;&#21619;&#30528;&#22312;&#36973;&#21463;&#23545;&#25239;&#25915;&#20987;&#26102;&#65292;&#19981;&#21033;&#30340;&#23545;&#25239;&#26679;&#26412;&#24517;&#39035;&#20256;&#36865;&#21040;&#20113;&#31471;&#65292;&#23548;&#33268;&#36793;&#32536;&#35774;&#22791;&#30340;&#33021;&#37327;&#28010;&#36153;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#19968;&#31181;&#20302;&#21151;&#32791;&#12289;&#36866;&#29992;&#20110;&#36793;&#32536;&#35774;&#22791;&#30340;&#23545;&#25239;&#26816;&#27979;&#26041;&#27861;&#26469;&#25552;&#39640;&#33021;&#37327;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
In practical cloud-edge scenarios, where a resource constrained edge performs data acquisition and a cloud system (having sufficient resources) performs inference tasks with a deep neural network (DNN), adversarial robustness is critical for reliability and ubiquitous deployment. Adversarial detection is a prime adversarial defence technique used in prior literature. However, in prior detection works, the detector is attached to the classifier model and both detector and classifier work in tandem to perform adversarial detection that requires a high computational overhead which is not available at the low-power edge. Therefore, prior works can only perform adversarial detection at the cloud and not at the edge. This means that in case of adversarial attacks, the unfavourable adversarial samples must be communicated to the cloud which leads to energy wastage at the edge device. Therefore, a low-power edge-friendly adversarial detection method is required to improve the energy efficiency
&lt;/p&gt;</description></item><item><title>Meta-CoT&#26159;&#19968;&#31181;&#22312;&#28151;&#21512;&#20219;&#21153;&#22330;&#26223;&#20013;&#33021;&#22815;&#36890;&#29992;&#24605;&#32500;&#38142;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#22312;&#21313;&#20010;&#20844;&#20849;&#22522;&#20934;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#21644;&#20248;&#36234;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.06692</link><description>&lt;p&gt;
Meta-CoT:&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#28151;&#21512;&#20219;&#21153;&#22330;&#26223;&#20013;&#30340;&#36890;&#29992;&#24605;&#32500;&#38142;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Meta-CoT: Generalizable Chain-of-Thought Prompting in Mixed-task Scenarios with Large Language Models. (arXiv:2310.06692v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06692
&lt;/p&gt;
&lt;p&gt;
Meta-CoT&#26159;&#19968;&#31181;&#22312;&#28151;&#21512;&#20219;&#21153;&#22330;&#26223;&#20013;&#33021;&#22815;&#36890;&#29992;&#24605;&#32500;&#38142;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#22312;&#21313;&#20010;&#20844;&#20849;&#22522;&#20934;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#21644;&#20248;&#36234;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36890;&#36807;&#21033;&#29992;&#38142;&#24335;&#24605;&#32500;&#25552;&#31034;&#23637;&#31034;&#20986;&#20102;&#21331;&#36234;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#36825;&#31181;&#25552;&#31034;&#29983;&#25104;&#20013;&#38388;&#25512;&#29702;&#38142;&#20197;&#20316;&#20026;&#24471;&#20986;&#31572;&#26696;&#30340;&#22522;&#26412;&#29702;&#30001;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;CoT&#26041;&#27861;&#35201;&#20040;&#20165;&#20165;&#20351;&#29992;&#31867;&#20284;&#8220;&#35753;&#25105;&#20204;&#36880;&#27493;&#24605;&#32771;&#8221;&#30340;&#36890;&#29992;&#25552;&#31034;&#65292;&#35201;&#20040;&#36807;&#20110;&#20381;&#36182;&#25163;&#24037;&#35774;&#35745;&#30340;&#20219;&#21153;&#29305;&#23450;&#28436;&#31034;&#26469;&#36798;&#21040;&#29702;&#24819;&#30340;&#24615;&#33021;&#65292;&#20174;&#32780;&#23548;&#33268;&#24615;&#33021;&#21644;&#27867;&#21270;&#20043;&#38388;&#30340;&#19981;&#21487;&#36991;&#20813;&#30340;&#40511;&#27807;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#40511;&#27807;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Meta-CoT&#65292;&#19968;&#31181;&#22312;&#26410;&#30693;&#36755;&#20837;&#38382;&#39064;&#31867;&#22411;&#30340;&#28151;&#21512;&#20219;&#21153;&#22330;&#26223;&#20013;&#20855;&#26377;&#36890;&#29992;&#24615;&#30340;CoT&#25552;&#31034;&#26041;&#27861;&#12290;Meta-CoT&#39318;&#20808;&#26681;&#25454;&#36755;&#20837;&#38382;&#39064;&#23545;&#22330;&#26223;&#36827;&#34892;&#20998;&#31867;&#65292;&#28982;&#21518;&#20197;&#33258;&#21160;&#27169;&#24335;&#20174;&#30456;&#24212;&#30340;&#25968;&#25454;&#27744;&#20013;&#26500;&#24314;&#22810;&#26679;&#30340;&#28436;&#31034;&#12290;Meta-CoT&#22312;&#21313;&#20010;&#20844;&#20849;&#22522;&#20934;&#25512;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#21644;&#20248;&#36234;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;Meta-CoT&#23454;&#29616;&#20102;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have unveiled remarkable reasoning capabilities by exploiting chain-of-thought (CoT) prompting, which generates intermediate reasoning chains to serve as the rationale for deriving the answer. However, current CoT methods either simply employ general prompts such as Let's think step by step, or heavily rely on handcrafted task-specific demonstrations to attain preferable performances, thereby engendering an inescapable gap between performance and generalization. To bridge this gap, we propose Meta-CoT, a generalizable CoT prompting method in mixed-task scenarios where the type of input questions is unknown. Meta-CoT firstly categorizes the scenario based on the input question and subsequently constructs diverse demonstrations from the corresponding data pool in an automatic pattern. Meta-CoT simultaneously enjoys remarkable performances on ten public benchmark reasoning tasks and superior generalization capabilities. Notably, Meta-CoT achieves the state-of-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;&#24314;&#26500;&#24615;&#21644;&#22810;&#26679;&#21270;&#21453;&#39304;&#65288;CDF&#65289;&#65292;&#29992;&#20110;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#23545;&#40784;&#25928;&#26524;&#12290;&#25105;&#20204;&#36890;&#36807;&#25910;&#38598;&#19981;&#21516;&#31867;&#22411;&#30340;&#21453;&#39304;&#65292;&#24182;&#26681;&#25454;&#38382;&#39064;&#30340;&#38590;&#24230;&#32423;&#21035;&#36827;&#34892;&#22788;&#29702;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.06450</link><description>&lt;p&gt;
&#29992;&#22810;&#26679;&#21270;&#21453;&#39304;&#26500;&#24314;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#40784;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Constructive Large Language Models Alignment with Diverse Feedback. (arXiv:2310.06450v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06450
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;&#24314;&#26500;&#24615;&#21644;&#22810;&#26679;&#21270;&#21453;&#39304;&#65288;CDF&#65289;&#65292;&#29992;&#20110;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#23545;&#40784;&#25928;&#26524;&#12290;&#25105;&#20204;&#36890;&#36807;&#25910;&#38598;&#19981;&#21516;&#31867;&#22411;&#30340;&#21453;&#39304;&#65292;&#24182;&#26681;&#25454;&#38382;&#39064;&#30340;&#38590;&#24230;&#32423;&#21035;&#36827;&#34892;&#22788;&#29702;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#30740;&#31350;&#20013;&#65292;&#23545;&#20854;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#23545;&#40784;&#36234;&#26469;&#36234;&#37325;&#35270;&#65292;&#20197;&#20943;&#23569;&#26377;&#23475;&#20869;&#23481;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#23545;&#40784;&#26041;&#27861;&#36890;&#24120;&#20165;&#20381;&#36182;&#20110;&#20154;&#31867;&#21453;&#39304;&#30340;&#21333;&#19968;&#24418;&#24335;&#65292;&#22914;&#20559;&#22909;&#12289;&#27880;&#37322;&#26631;&#31614;&#25110;&#33258;&#28982;&#35821;&#35328;&#25209;&#35780;&#65292;&#24573;&#35270;&#20102;&#32467;&#21512;&#36825;&#20123;&#21453;&#39304;&#31867;&#22411;&#30340;&#28508;&#22312;&#20248;&#21183;&#12290;&#36825;&#31181;&#38480;&#21046;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#65292;&#21363;&#20351;&#26377;&#20016;&#23500;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#24314;&#26500;&#24615;&#21644;&#22810;&#26679;&#21270;&#21453;&#39304;&#65288;CDF&#65289;&#20316;&#20026;&#22686;&#24378;LLM&#23545;&#40784;&#30340;&#26032;&#26041;&#27861;&#65292;&#21463;&#24314;&#26500;&#23398;&#20064;&#29702;&#35770;&#30340;&#21551;&#21457;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#25910;&#38598;&#36866;&#29992;&#20110;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#19981;&#21516;&#38590;&#24230;&#38382;&#39064;&#30340;&#19977;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#21453;&#39304;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;&#25209;&#35780;&#21453;&#39304;&#35299;&#20915;&#31616;&#21333;&#38382;&#39064;&#65292;&#21033;&#29992;&#25913;&#36827;&#21453;&#39304;&#35299;&#20915;&#20013;&#31561;&#38382;&#39064;&#65292;&#21033;&#29992;&#20559;&#22909;&#21453;&#39304;&#35299;&#20915;&#22256;&#38590;&#38382;&#39064;&#12290;&#36890;&#36807;&#29992;&#36825;&#31181;&#22810;&#26679;&#21270;&#21453;&#39304;&#35757;&#32451;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent research on large language models (LLMs), there has been a growing emphasis on aligning these models with human values to reduce the impact of harmful content. However, current alignment methods often rely solely on singular forms of human feedback, such as preferences, annotated labels, or natural language critiques, overlooking the potential advantages of combining these feedback types. This limitation leads to suboptimal performance, even when ample training data is available. In this paper, we introduce Constructive and Diverse Feedback (CDF) as a novel method to enhance LLM alignment, inspired by constructivist learning theory. Our approach involves collecting three distinct types of feedback tailored to problems of varying difficulty levels within the training dataset. Specifically, we exploit critique feedback for easy problems, refinement feedback for medium problems, and preference feedback for hard problems. By training our model with this diversified feedback, we a
&lt;/p&gt;</description></item><item><title>&#36817;&#20284;&#27880;&#24847;&#21147;&#26426;&#21046;HyperAttention&#35299;&#20915;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#30340;&#38271;&#19978;&#19979;&#25991;&#30340;&#35745;&#31639;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#20004;&#20010;&#21442;&#25968;&#26469;&#34913;&#37327;&#38382;&#39064;&#30340;&#38590;&#24230;&#12290;HyperAttention&#20855;&#26377;&#27169;&#22359;&#21270;&#35774;&#35745;&#65292;&#21487;&#36731;&#26494;&#38598;&#25104;&#20854;&#20182;&#24555;&#36895;&#20302;&#32423;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.05869</link><description>&lt;p&gt;
&#36229;&#32423;&#20851;&#27880;&#21147;&#65306;&#36817;&#20284;&#32447;&#24615;&#26102;&#38388;&#19979;&#30340;&#38271;&#19978;&#19979;&#25991;&#27880;&#24847;&#21147;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
HyperAttention: Long-context Attention in Near-Linear Time. (arXiv:2310.05869v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05869
&lt;/p&gt;
&lt;p&gt;
&#36817;&#20284;&#27880;&#24847;&#21147;&#26426;&#21046;HyperAttention&#35299;&#20915;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#30340;&#38271;&#19978;&#19979;&#25991;&#30340;&#35745;&#31639;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#20004;&#20010;&#21442;&#25968;&#26469;&#34913;&#37327;&#38382;&#39064;&#30340;&#38590;&#24230;&#12290;HyperAttention&#20855;&#26377;&#27169;&#22359;&#21270;&#35774;&#35745;&#65292;&#21487;&#36731;&#26494;&#38598;&#25104;&#20854;&#20182;&#24555;&#36895;&#20302;&#32423;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HyperAttention&#30340;&#36817;&#20284;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#20197;&#24212;&#23545;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#20351;&#29992;&#30340;&#38271;&#19978;&#19979;&#25991;&#30340;&#26085;&#30410;&#22797;&#26434;&#30340;&#35745;&#31639;&#25361;&#25112;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#65292;&#38500;&#38750;&#27880;&#24847;&#21147;&#30697;&#38453;&#30340;&#26465;&#30446;&#34987;&#38480;&#21046;&#25110;&#30697;&#38453;&#20855;&#26377;&#20302;&#31283;&#23450;&#31209;&#65292;&#21542;&#21017;&#20108;&#27425;&#26102;&#38388;&#26159;&#24517;&#35201;&#30340;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#21442;&#25968;&#65292;&#29992;&#20110;&#34913;&#37327;&#65306;&#65288;1&#65289;&#26631;&#20934;&#21270;&#27880;&#24847;&#21147;&#30697;&#38453;&#20013;&#30340;&#26368;&#22823;&#21015;&#33539;&#25968;&#65292;&#20197;&#21450;&#65288;2&#65289;&#22312;&#26816;&#27979;&#21644;&#21024;&#38500;&#22823;&#26465;&#30446;&#21518;&#65292;&#38750;&#26631;&#20934;&#21270;&#27880;&#24847;&#21147;&#30697;&#38453;&#20013;&#34892;&#33539;&#25968;&#30340;&#27604;&#29575;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#32454;&#31890;&#24230;&#30340;&#21442;&#25968;&#26469;&#25429;&#25417;&#38382;&#39064;&#30340;&#38590;&#24230;&#12290;&#23613;&#31649;&#20808;&#21069;&#23384;&#22312;&#19979;&#30028;&#65292;&#20294;&#25105;&#20204;&#33021;&#22815;&#23454;&#29616;&#19968;&#20010;&#32447;&#24615;&#26102;&#38388;&#30340;&#37319;&#26679;&#31639;&#27861;&#65292;&#21363;&#20351;&#30697;&#38453;&#20855;&#26377;&#26080;&#30028;&#30340;&#26465;&#30446;&#25110;&#36739;&#22823;&#30340;&#31283;&#23450;&#31209;&#65292;&#21482;&#35201;&#19978;&#36848;&#21442;&#25968;&#36739;&#23567;&#12290;HyperAttention&#20855;&#26377;&#27169;&#22359;&#21270;&#35774;&#35745;&#65292;&#36731;&#26494;&#23481;&#32435;&#20854;&#20182;&#24555;&#36895;&#20302;&#32423;&#23454;&#29616;&#65292;&#29305;&#21035;&#26159;FlashAttention&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an approximate attention mechanism named HyperAttention to address the computational challenges posed by the growing complexity of long contexts used in Large Language Models (LLMs). Recent work suggests that in the worst-case scenario, quadratic time is necessary unless the entries of the attention matrix are bounded or the matrix has low stable rank. We introduce two parameters which measure: (1) the max column norm in the normalized attention matrix, and (2) the ratio of row norms in the unnormalized attention matrix after detecting and removing large entries. We use these fine-grained parameters to capture the hardness of the problem. Despite previous lower bounds, we are able to achieve a linear time sampling algorithm even when the matrix has unbounded entries or a large stable rank, provided the above parameters are small. HyperAttention features a modular design that easily accommodates integration of other fast low-level implementations, particularly FlashAttention.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#35299;&#37322;&#20854;&#20182;&#39044;&#27979;&#27169;&#22411;&#26377;&#25928;&#24615;&#30340;&#26694;&#26550;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#22810;&#20010;&#25552;&#31034;&#31574;&#30053;&#65292;&#22635;&#34917;&#20102;&#24403;&#21069;&#23545;&#20110;LLMs&#22312;&#35299;&#37322;&#20854;&#20182;&#27169;&#22411;&#34892;&#20026;&#26041;&#38754;&#30340;&#32570;&#22833;&#12290;</title><link>http://arxiv.org/abs/2310.05797</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#20107;&#21518;&#35299;&#37322;&#22120;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Large Language Models Post Hoc Explainers?. (arXiv:2310.05797v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05797
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#35299;&#37322;&#20854;&#20182;&#39044;&#27979;&#27169;&#22411;&#26377;&#25928;&#24615;&#30340;&#26694;&#26550;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#22810;&#20010;&#25552;&#31034;&#31574;&#30053;&#65292;&#22635;&#34917;&#20102;&#24403;&#21069;&#23545;&#20110;LLMs&#22312;&#35299;&#37322;&#20854;&#20182;&#27169;&#22411;&#34892;&#20026;&#26041;&#38754;&#30340;&#32570;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36234;&#26469;&#36234;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#24212;&#29992;&#20013;&#12290;&#26368;&#36817;&#30340;&#19968;&#39033;&#21019;&#26032;&#65292;&#21363;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#65292;&#20351;&#24471;LLM&#33021;&#22815;&#22312;&#25512;&#29702;&#38454;&#27573;&#36890;&#36807;&#22312;&#25552;&#31034;&#20013;&#25552;&#20379;&#23569;&#37327;&#31034;&#20363;&#26469;&#23398;&#20064;&#26032;&#20219;&#21153;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#27169;&#22411;&#24494;&#35843;&#30340;&#38656;&#35201;&#12290;&#34429;&#28982;LLM&#24050;&#32463;&#34987;&#24212;&#29992;&#20110;&#22810;&#20010;&#39046;&#22495;&#65292;&#20294;&#20854;&#22312;&#35299;&#37322;&#20854;&#20182;&#27169;&#22411;&#34892;&#20026;&#26041;&#38754;&#30340;&#36866;&#29992;&#24615;&#20173;&#30456;&#23545;&#26410;&#34987;&#25506;&#32034;&#12290;&#23613;&#31649;&#23384;&#22312;&#36234;&#26469;&#36234;&#22810;&#30340;&#26032;&#35299;&#37322;&#25216;&#26415;&#65292;&#20294;&#24456;&#22810;&#25216;&#26415;&#35201;&#27714;&#23545;&#27169;&#22411;&#20855;&#26377;&#30333;&#30418;&#35775;&#38382;&#26435;&#38480;&#21644;/&#25110;&#35745;&#31639;&#25104;&#26412;&#36739;&#39640;&#65292;&#20984;&#26174;&#20102;&#19979;&#19968;&#20195;&#20107;&#21518;&#35299;&#37322;&#22120;&#30340;&#38656;&#27714;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#30740;&#31350;LLM&#35299;&#37322;&#20854;&#20182;&#39044;&#27979;&#27169;&#22411;&#26377;&#25928;&#24615;&#30340;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#22810;&#31181;&#25552;&#31034;&#31574;&#30053;&#30340;&#26032;&#39062;&#26694;&#26550;&#65306;i&#65289;&#22522;&#20110;&#25200;&#21160;&#30340;ICL&#65292;ii&#65289;&#22522;&#20110;&#39044;&#27979;&#30340;ICL&#65292;iii&#65289;&#22522;&#20110;&#25351;&#20196;&#30340;ICL&#65292;&#21644;iv&#65289;&#22522;&#20110;&#35299;&#37322;&#30340;ICL&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are increasingly used as powerful tools for a plethora of natural language processing (NLP) applications. A recent innovation, in-context learning (ICL), enables LLMs to learn new tasks by supplying a few examples in the prompt during inference time, thereby eliminating the need for model fine-tuning. While LLMs have been utilized in several applications, their applicability in explaining the behavior of other models remains relatively unexplored. Despite the growing number of new explanation techniques, many require white-box access to the model and/or are computationally expensive, highlighting a need for next-generation post hoc explainers. In this work, we present the first framework to study the effectiveness of LLMs in explaining other predictive models. More specifically, we propose a novel framework encompassing multiple prompting strategies: i) Perturbation-based ICL, ii) Prediction-based ICL, iii) Instruction-based ICL, and iv) Explanation-based I
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;&#20174;&#27861;&#24459;&#26696;&#20214;&#20013;&#33258;&#21160;&#29983;&#25104;&#35770;&#35777;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#26368;&#20339;&#26041;&#27861;&#29983;&#25104;&#30340;&#35770;&#35777;&#19982;&#22522;&#20934;&#38598;&#30340;&#40644;&#37329;&#26631;&#20934;&#27880;&#37322;&#24179;&#22343;&#37325;&#21472;&#29575;&#20026;63%&#12290;</title><link>http://arxiv.org/abs/2310.05680</link><description>&lt;p&gt;
&#20174;&#27861;&#24459;&#20107;&#23454;&#20013;&#33258;&#21160;&#29983;&#25104;&#35770;&#35777;&#30340;&#33258;&#21160;&#21270;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated Argument Generation from Legal Facts. (arXiv:2310.05680v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05680
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;&#20174;&#27861;&#24459;&#26696;&#20214;&#20013;&#33258;&#21160;&#29983;&#25104;&#35770;&#35777;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#26368;&#20339;&#26041;&#27861;&#29983;&#25104;&#30340;&#35770;&#35777;&#19982;&#22522;&#20934;&#38598;&#30340;&#40644;&#37329;&#26631;&#20934;&#27880;&#37322;&#24179;&#22343;&#37325;&#21472;&#29575;&#20026;63%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#65292;&#24453;&#22788;&#29702;&#26696;&#20214;&#25968;&#37327;&#21576;&#25351;&#25968;&#22686;&#38271;&#65288;&#20363;&#22914;&#65292;&#20165;&#22312;&#21360;&#24230;&#23601;&#26377;&#36229;&#36807;1000&#19975;&#20010;&#24453;&#22788;&#29702;&#26696;&#20214;&#65289;&#12290;&#20027;&#35201;&#38382;&#39064;&#22312;&#20110;&#25552;&#20132;&#32473;&#27861;&#24459;&#31995;&#32479;&#30340;&#26696;&#20214;&#25968;&#37327;&#36828;&#36828;&#36229;&#36807;&#19968;&#20010;&#22269;&#23478;&#20013;&#29616;&#26377;&#30340;&#27861;&#24459;&#19987;&#19994;&#20154;&#21592;&#30340;&#25968;&#37327;&#12290;&#22312;&#36825;&#20010;&#20840;&#29699;&#32972;&#26223;&#19979;&#65292;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#26469;&#25552;&#39640;&#27861;&#24459;&#31243;&#24207;&#30340;&#25928;&#29575;&#21644;&#36895;&#24230;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#24110;&#21161;&#27861;&#24459;&#19987;&#19994;&#20154;&#21592;&#20998;&#26512;&#27861;&#24459;&#26696;&#20214;&#30340;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#20855;&#20307;&#35843;&#26597;&#25506;&#32034;&#20102;&#21033;&#29992;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;&#20174;&#27861;&#24459;&#26696;&#20214;&#20013;&#25552;&#21462;&#20986;&#35770;&#35777;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#26368;&#20339;&#26041;&#27861;&#29983;&#25104;&#30340;&#35770;&#35777;&#19982;&#22522;&#20934;&#38598;&#30340;&#40644;&#37329;&#26631;&#20934;&#27880;&#37322;&#24179;&#22343;&#37325;&#21472;&#29575;&#20026;63%&#12290;
&lt;/p&gt;
&lt;p&gt;
The count of pending cases has shown an exponential rise across nations (e.g., with more than 10 million pending cases in India alone). The main issue lies in the fact that the number of cases submitted to the law system is far greater than the available number of legal professionals present in a country. Given this worldwide context, the utilization of AI technology has gained paramount importance to enhance the efficiency and speed of legal procedures. In this study we partcularly focus on helping legal professionals in the process of analyzing a legal case. Our specific investigation delves into harnessing the generative capabilities of open-sourced large language models to create arguments derived from the facts present in legal cases. Experimental results show that the generated arguments from the best performing method have on average 63% overlap with the benchmark set gold standard annotations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#32593;&#32476;&#22823;&#23567;&#21644;L2&#27491;&#21017;&#21270;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#35266;&#23519;&#21040;&#20102;&#21452;&#19979;&#38477;&#29616;&#35937;&#12290;&#36890;&#36807;&#20351;&#29992;&#38543;&#26426;&#29305;&#24449;&#21644;&#25042;&#24816;&#35757;&#32451;&#31574;&#30053;&#65292;&#22312;&#21442;&#25968;&#21644;&#29366;&#24577;&#25968;&#26080;&#38480;&#22823;&#30340;&#24773;&#20917;&#19979;&#30740;&#31350;&#20102;&#27491;&#21017;&#21270;&#30340;&#26368;&#23567;&#20108;&#20056;&#26102;&#38388;&#24046;&#20998;&#31639;&#27861;&#65292;&#24471;&#20986;&#20102;&#20854;&#25910;&#25947;&#24615;&#21644;&#26368;&#20248;&#24615;&#65292;&#24182;&#38416;&#36848;&#20102;&#21452;&#19979;&#38477;&#29616;&#35937;&#22312;&#35813;&#31639;&#27861;&#20013;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.05518</link><description>&lt;p&gt;
&#20851;&#20110;&#20351;&#29992;LSTD&#21644;&#38543;&#26426;&#29305;&#24449;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#21452;&#19979;&#38477;&#29616;&#35937;
&lt;/p&gt;
&lt;p&gt;
On Double-Descent in Reinforcement Learning with LSTD and Random Features. (arXiv:2310.05518v1 [cs.LG] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05518
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#32593;&#32476;&#22823;&#23567;&#21644;L2&#27491;&#21017;&#21270;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#35266;&#23519;&#21040;&#20102;&#21452;&#19979;&#38477;&#29616;&#35937;&#12290;&#36890;&#36807;&#20351;&#29992;&#38543;&#26426;&#29305;&#24449;&#21644;&#25042;&#24816;&#35757;&#32451;&#31574;&#30053;&#65292;&#22312;&#21442;&#25968;&#21644;&#29366;&#24577;&#25968;&#26080;&#38480;&#22823;&#30340;&#24773;&#20917;&#19979;&#30740;&#31350;&#20102;&#27491;&#21017;&#21270;&#30340;&#26368;&#23567;&#20108;&#20056;&#26102;&#38388;&#24046;&#20998;&#31639;&#27861;&#65292;&#24471;&#20986;&#20102;&#20854;&#25910;&#25947;&#24615;&#21644;&#26368;&#20248;&#24615;&#65292;&#24182;&#38416;&#36848;&#20102;&#21452;&#19979;&#38477;&#29616;&#35937;&#22312;&#35813;&#31639;&#27861;&#20013;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24046;&#20998;&#31639;&#27861;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20854;&#24615;&#33021;&#21463;&#31070;&#32463;&#32593;&#32476;&#22823;&#23567;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#22312;&#30417;&#30563;&#23398;&#20064;&#20013;&#36807;&#21442;&#25968;&#21270;&#21644;&#20854;&#24102;&#26469;&#30340;&#22909;&#22788;&#24050;&#32463;&#24471;&#21040;&#20102;&#24456;&#22909;&#30340;&#29702;&#35299;&#65292;&#20294;&#26159;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#24773;&#20917;&#21017;&#19981;&#22826;&#28165;&#26970;&#12290;&#26412;&#25991;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#25506;&#35752;&#20102;&#32593;&#32476;&#22823;&#23567;&#21644;L2&#27491;&#21017;&#21270;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#23558;&#21442;&#25968;&#20010;&#25968;&#19982;&#35775;&#38382;&#29366;&#24577;&#20010;&#25968;&#20043;&#27604;&#23450;&#20041;&#20026;&#20851;&#38190;&#22240;&#32032;&#65292;&#24403;&#35813;&#27604;&#20540;&#22823;&#20110;1&#26102;&#31216;&#20026;&#36807;&#21442;&#25968;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20102;&#21452;&#19979;&#38477;&#29616;&#35937;&#65292;&#21363;&#22312;&#21442;&#25968;/&#29366;&#24577;&#27604;&#20026;1&#38468;&#36817;&#20250;&#31361;&#28982;&#24615;&#33021;&#19979;&#38477;&#12290;&#36890;&#36807;&#21033;&#29992;&#38543;&#26426;&#29305;&#24449;&#21644;&#25042;&#24816;&#35757;&#32451;&#31574;&#30053;&#65292;&#25105;&#20204;&#22312;&#26080;&#38480;&#22823;&#30340;&#21442;&#25968;&#21644;&#29366;&#24577;&#25968;&#19979;&#30740;&#31350;&#20102;&#27491;&#21017;&#21270;&#30340;&#26368;&#23567;&#20108;&#20056;&#26102;&#38388;&#24046;&#20998;&#31639;&#27861;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#20854;&#25910;&#25947;&#24615;&#21644;&#26368;&#20248;&#24615;&#65292;&#24182;&#38416;&#36848;&#20102;&#21452;&#19979;&#38477;&#29616;&#35937;&#22312;&#35813;&#31639;&#27861;&#20013;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal Difference (TD) algorithms are widely used in Deep Reinforcement Learning (RL). Their performance is heavily influenced by the size of the neural network. While in supervised learning, the regime of over-parameterization and its benefits are well understood, the situation in RL is much less clear. In this paper, we present a theoretical analysis of the influence of network size and $l_2$-regularization on performance. We identify the ratio between the number of parameters and the number of visited states as a crucial factor and define over-parameterization as the regime when it is larger than one. Furthermore, we observe a double-descent phenomenon, i.e., a sudden drop in performance around the parameter/state ratio of one. Leveraging random features and the lazy training regime, we study the regularized Least-Square Temporal Difference (LSTD) algorithm in an asymptotic regime, as both the number of parameters and states go to infinity, maintaining a constant ratio. We derive 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23545;&#35821;&#20041;&#20998;&#21106;&#20013;&#30340;&#32463;&#20856;&#27979;&#35797;&#26102;&#36866;&#24212;&#26041;&#27861;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#25506;&#31350;&#65292;&#25581;&#31034;&#20102;&#20998;&#21106;TTA&#25152;&#38754;&#20020;&#30340;&#29420;&#29305;&#25361;&#25112;&#65292;&#24182;&#21457;&#29616;&#32463;&#20856;TTA&#31574;&#30053;&#22312;&#36825;&#19968;&#20219;&#21153;&#20013;&#24182;&#19981;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2310.05341</link><description>&lt;p&gt;
&#23545;&#35821;&#20041;&#20998;&#21106;&#20013;&#32463;&#20856;&#30340;&#27979;&#35797;&#26102;&#36866;&#24212;&#26041;&#27861;&#30340;&#25209;&#21028;&#24615;&#25506;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Critical Look at Classic Test-Time Adaptation Methods in Semantic Segmentation. (arXiv:2310.05341v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05341
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23545;&#35821;&#20041;&#20998;&#21106;&#20013;&#30340;&#32463;&#20856;&#27979;&#35797;&#26102;&#36866;&#24212;&#26041;&#27861;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#25506;&#31350;&#65292;&#25581;&#31034;&#20102;&#20998;&#21106;TTA&#25152;&#38754;&#20020;&#30340;&#29420;&#29305;&#25361;&#25112;&#65292;&#24182;&#21457;&#29616;&#32463;&#20856;TTA&#31574;&#30053;&#22312;&#36825;&#19968;&#20219;&#21153;&#20013;&#24182;&#19981;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27979;&#35797;&#26102;&#36866;&#24212;&#65288;TTA&#65289;&#26088;&#22312;&#23558;&#26368;&#21021;&#22312;&#35757;&#32451;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#36866;&#24212;&#20110;&#27979;&#35797;&#25968;&#25454;&#20013;&#30340;&#21487;&#33021;&#20998;&#24067;&#21464;&#21270;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;TTA&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#20998;&#31867;&#20219;&#21153;&#19978;&#65292;&#23545;&#20110;&#35821;&#20041;&#20998;&#21106;&#30340;TTA&#25506;&#32034;&#38750;&#24120;&#26377;&#38480;&#12290;&#36825;&#31181;&#23545;&#20998;&#31867;&#30340;&#31361;&#20986;&#37325;&#35270;&#21487;&#33021;&#23548;&#33268;&#35768;&#22810;&#26032;&#25163;&#21644;&#24037;&#31243;&#24072;&#38169;&#35823;&#22320;&#35748;&#20026;&#20026;&#20998;&#31867;&#35774;&#35745;&#30340;&#32463;&#20856;TTA&#26041;&#27861;&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#20998;&#21106;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#20551;&#35774;&#20173;&#26410;&#32463;&#39564;&#35777;&#65292;&#26159;&#19968;&#20010;&#24453;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#31995;&#32479;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#20998;&#21106;TTA&#30340;&#29420;&#29305;&#25361;&#25112;&#65292;&#24182;&#30830;&#23450;&#32463;&#20856;TTA&#31574;&#30053;&#26159;&#21542;&#21487;&#20197;&#26377;&#25928;&#24212;&#23545;&#36825;&#19968;&#20219;&#21153;&#12290;&#25105;&#20204;&#20840;&#38754;&#30340;&#32467;&#26524;&#24471;&#20986;&#20102;&#19977;&#20010;&#20851;&#38190;&#35266;&#23519;&#32467;&#26524;&#12290;&#39318;&#20808;&#65292;&#24120;&#29992;&#20110;&#20998;&#31867;TTA&#30340;&#32463;&#20856;&#25209;&#24402;&#19968;&#21270;&#26356;&#26032;&#31574;&#30053;&#21482;&#33021;&#24102;&#26469;&#36731;&#24494;&#30340;&#24615;&#33021;&#25913;&#21892;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#29978;&#33267;&#20250;&#23545;&#32467;&#26524;&#20135;&#29983;&#36870;&#21521;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Test-time adaptation (TTA) aims to adapt a model, initially trained on training data, to potential distribution shifts in the test data. Most existing TTA studies, however, focus on classification tasks, leaving a notable gap in the exploration of TTA for semantic segmentation. This pronounced emphasis on classification might lead numerous newcomers and engineers to mistakenly assume that classic TTA methods designed for classification can be directly applied to segmentation. Nonetheless, this assumption remains unverified, posing an open question. To address this, we conduct a systematic, empirical study to disclose the unique challenges of segmentation TTA, and to determine whether classic TTA strategies can effectively address this task. Our comprehensive results have led to three key observations. First, the classic batch norm updating strategy, commonly used in classification TTA, only brings slight performance improvement, and in some cases it might even adversely affect the resu
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;&#23545;&#35805;&#31995;&#32479;&#20013;&#30340;&#20154;&#26684;&#20559;&#35265;&#23545;&#31038;&#20132;&#20559;&#35265;&#30340;&#24433;&#21709;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#32508;&#21512;&#35780;&#20272;&#26694;&#26550;&#26469;&#34913;&#37327;&#19981;&#21516;&#20154;&#26684;&#37319;&#29992;&#19979;&#30340;&#20559;&#35265;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.05280</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#38543;&#26426;&#40550;&#40521;&#26356;&#21361;&#38505;&#21527;&#65311;&#35780;&#20272;&#23545;&#35805;&#31995;&#32479;&#20013;&#30340;&#20154;&#26684;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Are Personalized Stochastic Parrots More Dangerous? Evaluating Persona Biases in Dialogue Systems. (arXiv:2310.05280v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05280
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;&#23545;&#35805;&#31995;&#32479;&#20013;&#30340;&#20154;&#26684;&#20559;&#35265;&#23545;&#31038;&#20132;&#20559;&#35265;&#30340;&#24433;&#21709;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#32508;&#21512;&#35780;&#20272;&#26694;&#26550;&#26469;&#34913;&#37327;&#19981;&#21516;&#20154;&#26684;&#37319;&#29992;&#19979;&#30340;&#20559;&#35265;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#20351;&#20854;&#33021;&#22815;&#25353;&#29031;&#33258;&#30001;&#24418;&#24335;&#30340;&#25351;&#20196;&#36827;&#34892;&#25805;&#20316;&#65292;&#21253;&#25324;&#22312;&#23545;&#35805;&#20013;&#27169;&#20223;&#36890;&#29992;&#25110;&#29305;&#23450;&#20154;&#21475;&#32676;&#20307;&#30340;&#20154;&#26684;&#12290;&#36890;&#29992;&#20154;&#26684;&#25351;&#30340;&#26159;&#26469;&#33258;&#26576;&#19968;&#20154;&#21475;&#32676;&#20307;&#30340;&#20010;&#20307;&#65288;&#20363;&#22914;&#20122;&#27954;&#20154;&#65289;&#65292;&#32780;&#29305;&#23450;&#20154;&#26684;&#21487;&#20197;&#26159;&#21382;&#21490;&#20154;&#29289;&#30340;&#23454;&#38469;&#22995;&#21517;&#12290;&#34429;&#28982;&#37319;&#29992;&#20154;&#26684;&#20351;&#23545;&#35805;&#31995;&#32479;&#26356;&#20855;&#21560;&#24341;&#21147;&#21644;&#20146;&#21644;&#21147;&#65292;&#20294;&#20063;&#23384;&#22312;&#28508;&#22312;&#39118;&#38505;&#65292;&#21487;&#33021;&#36890;&#36807;&#19982;&#29992;&#25143;&#30340;&#20132;&#20114;&#32780;&#21152;&#21095;&#31038;&#20250;&#20559;&#35265;&#65292;&#36827;&#19968;&#27493;&#36896;&#25104;&#31038;&#20250;&#20260;&#23475;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#8220;&#20154;&#26684;&#20559;&#35265;&#8221;&#65292;&#25105;&#20204;&#23558;&#20854;&#23450;&#20041;&#20026;&#26377;&#23475;&#23545;&#35805;&#27169;&#22411;&#34892;&#20026;&#23545;&#19981;&#21516;&#20154;&#26684;&#37319;&#29992;&#30340;&#25935;&#24863;&#24615;&#12290;&#25105;&#20204;&#23558;&#20154;&#26684;&#20559;&#35265;&#20998;&#20026;&#26377;&#23475;&#34920;&#36798;&#21644;&#26377;&#23475;&#35748;&#21516;&#20004;&#31867;&#65292;&#21516;&#26102;&#24314;&#31435;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#20197;&#34913;&#37327;&#20116;&#20010;&#26041;&#38754;&#30340;&#20154;&#26684;&#20559;&#35265;&#65306;&#20882;&#29359;&#24615;&#12289;&#26377;&#27602;&#24310;&#32493;&#12289;&#20851;&#24576;&#12289;&#21051;&#26495;&#21360;&#35937;&#30340;&#35748;&#21516;&#20197;&#21450;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in Large Language Models empower them to follow freeform instructions, including imitating generic or specific demographic personas in conversations. Generic personas refer to an individual from a demographic group (e.g. an Asian person), whereas specific personas can be actual names of historical figures. While the adoption of personas allows dialogue systems to be more engaging and approachable to users, it also carries the potential risk of exacerbating social biases in model responses, further causing societal harms through interactions with users. In this paper, we systematically study "persona biases", which we define to be the sensitivity of harmful dialogue model behaviors to different persona adoptions. We categorize persona biases into biases in harmful expression and harmful agreement, as well as establish a comprehensive evaluation framework to measure persona biases in five aspects: Offensiveness, Toxic Continuation, Regard, Stereotype Agreement, and To
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InstructDET&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#22810;&#26679;&#21270;&#30340;&#25351;&#20196;&#23450;&#20301;&#30446;&#26631;&#23545;&#35937;&#24182;&#36827;&#34892;&#25351;&#31216;&#23545;&#35937;&#26816;&#27979;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#22270;&#20687;&#12289;&#36793;&#30028;&#26694;&#21644;&#27867;&#21270;&#25351;&#20196;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21033;&#29992;&#20102;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25351;&#20196;&#12290;</title><link>http://arxiv.org/abs/2310.05136</link><description>&lt;p&gt;
InstructDET: &#36890;&#29992;&#25351;&#20196;&#30340;&#24341;&#23548;&#19979;&#30340;&#25351;&#31216;&#23545;&#35937;&#26816;&#27979;&#30340;&#22810;&#26679;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
InstructDET: Diversifying Referring Object Detection with Generalized Instructions. (arXiv:2310.05136v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05136
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InstructDET&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#22810;&#26679;&#21270;&#30340;&#25351;&#20196;&#23450;&#20301;&#30446;&#26631;&#23545;&#35937;&#24182;&#36827;&#34892;&#25351;&#31216;&#23545;&#35937;&#26816;&#27979;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#22270;&#20687;&#12289;&#36793;&#30028;&#26694;&#21644;&#27867;&#21270;&#25351;&#20196;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21033;&#29992;&#20102;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25351;&#20196;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;InstructDET&#65292;&#19968;&#31181;&#22522;&#20110;&#29992;&#25143;&#25351;&#20196;&#26469;&#23450;&#20301;&#30446;&#26631;&#23545;&#35937;&#30340;&#25351;&#31216;&#23545;&#35937;&#26816;&#27979;&#65288;ROD&#65289;&#30340;&#25968;&#25454;&#20013;&#24515;&#26041;&#27861;&#12290;&#25105;&#20204;&#21033;&#29992;&#20102;&#22810;&#26679;&#21270;&#30340;&#25351;&#20196;&#65292;&#28085;&#30422;&#19982;&#23545;&#35937;&#26816;&#27979;&#30456;&#20851;&#30340;&#24120;&#35265;&#29992;&#25143;&#24847;&#22270;&#12290;&#23545;&#20110;&#19968;&#24352;&#22270;&#20687;&#65292;&#25105;&#20204;&#29983;&#25104;&#20102;&#22823;&#37327;&#30340;&#25351;&#20196;&#65292;&#28041;&#21450;&#27599;&#20010;&#21333;&#29420;&#30340;&#23545;&#35937;&#21644;&#22810;&#20010;&#23545;&#35937;&#30340;&#19981;&#21516;&#32452;&#21512;&#12290;&#27599;&#20010;&#25351;&#20196;&#21450;&#20854;&#23545;&#24212;&#30340;&#23545;&#35937;&#36793;&#30028;&#26694;&#26500;&#25104;&#19968;&#20010;&#35757;&#32451;&#25968;&#25454;&#23545;&#12290;&#20026;&#20102;&#21253;&#21547;&#24120;&#35265;&#30340;&#26816;&#27979;&#34920;&#36798;&#24335;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#26032;&#20852;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#36890;&#36807;&#25991;&#26412;&#25552;&#31034;&#21644;&#23545;&#35937;&#36793;&#30028;&#26694;&#29983;&#25104;&#25351;&#20196;&#65292;&#22240;&#20026;&#22522;&#30784;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21487;&#20197;&#20135;&#29983;&#31867;&#20284;&#20154;&#31867;&#30340;&#34920;&#36798;&#65288;&#20363;&#22914;&#65292;&#25551;&#36848;&#23545;&#35937;&#23646;&#24615;&#12289;&#31867;&#21035;&#21644;&#20851;&#31995;&#65289;&#12290;&#25105;&#20204;&#23558;&#26500;&#24314;&#30340;&#25968;&#25454;&#38598;&#21629;&#21517;&#20026;InDET&#65292;&#21253;&#21547;&#22270;&#20687;&#12289;&#36793;&#30028;&#26694;&#21644;&#27867;&#21270;&#25351;&#20196;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose InstructDET, a data-centric method for referring object detection (ROD) that localizes target objects based on user instructions. While deriving from referring expressions (REC), the instructions we leverage are greatly diversified to encompass common user intentions related to object detection. For one image, we produce tremendous instructions that refer to every single object and different combinations of multiple objects. Each instruction and its corresponding object bounding boxes (bbxs) constitute one training data pair. In order to encompass common detection expressions, we involve emerging vision-language model (VLM) and large language model (LLM) to generate instructions guided by text prompts and object bbxs, as the generalizations of foundation models are effective to produce human-like expressions (e.g., describing object property, category, and relationship). We name our constructed dataset as InDET. It contains images, bbxs and generalized instructions that are 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36328;&#19981;&#21516;&#26465;&#20214;&#31934;&#30830;&#39044;&#27979;&#30005;&#27744;&#23551;&#21629;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25429;&#25417;&#30446;&#26631;&#30005;&#27744;&#21644;&#21442;&#32771;&#30005;&#27744;&#20043;&#38388;&#30340;&#30005;&#20449;&#21495;&#24046;&#24322;&#65292;&#26080;&#35770;&#26448;&#26009;&#21644;&#32769;&#21270;&#26465;&#20214;&#22914;&#20309;&#65292;&#22312;&#25193;&#23637;&#29305;&#24449;&#31354;&#38388;&#30340;&#21516;&#26102;&#20026;&#36890;&#29992;&#30340;&#30005;&#27744;&#23551;&#21629;&#39044;&#27979;&#26694;&#26550;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;</title><link>http://arxiv.org/abs/2310.05052</link><description>&lt;p&gt;
&#36328;&#19981;&#21516;&#26465;&#20214;&#31934;&#30830;&#39044;&#27979;&#30005;&#27744;&#23551;&#21629;&#30340;&#23398;&#20064;&#32454;&#32990;&#20869;&#21644;&#32454;&#32990;&#38388;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
Learning Intra- and Inter-Cell Differences for Accurate Battery Lifespan Prediction across Diverse Conditions. (arXiv:2310.05052v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05052
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36328;&#19981;&#21516;&#26465;&#20214;&#31934;&#30830;&#39044;&#27979;&#30005;&#27744;&#23551;&#21629;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25429;&#25417;&#30446;&#26631;&#30005;&#27744;&#21644;&#21442;&#32771;&#30005;&#27744;&#20043;&#38388;&#30340;&#30005;&#20449;&#21495;&#24046;&#24322;&#65292;&#26080;&#35770;&#26448;&#26009;&#21644;&#32769;&#21270;&#26465;&#20214;&#22914;&#20309;&#65292;&#22312;&#25193;&#23637;&#29305;&#24449;&#31354;&#38388;&#30340;&#21516;&#26102;&#20026;&#36890;&#29992;&#30340;&#30005;&#27744;&#23551;&#21629;&#39044;&#27979;&#26694;&#26550;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#27744;&#23551;&#21629;&#39044;&#27979;&#23545;&#30005;&#27744;&#30740;&#31350;&#21644;&#24320;&#21457;&#20855;&#26377;&#37325;&#35201;&#30340;&#23454;&#38469;&#20215;&#20540;&#12290;&#30446;&#21069;&#65292;&#35768;&#22810;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#20381;&#36182;&#20110;&#29305;&#23450;&#30446;&#26631;&#30005;&#27744;&#30340;&#26089;&#26399;&#30005;&#20449;&#21495;&#26469;&#39044;&#27979;&#23427;&#20204;&#30340;&#23551;&#21629;&#12290;&#19968;&#20010;&#24120;&#35265;&#30340;&#19981;&#36275;&#26159;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#37117;&#26159;&#22522;&#20110;&#29305;&#23450;&#32769;&#21270;&#26465;&#20214;&#24320;&#21457;&#30340;&#65292;&#36825;&#19981;&#20165;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#27169;&#22411;&#33021;&#21147;&#65292;&#32780;&#19988;&#38477;&#20302;&#20102;&#23427;&#20204;&#22312;&#39044;&#27979;&#19981;&#21516;&#26465;&#20214;&#19979;&#30340;&#36864;&#21270;&#25928;&#26524;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#26080;&#27861;&#20805;&#20998;&#21033;&#29992;&#20854;&#20182;&#26465;&#20214;&#19979;&#21487;&#29992;&#30340;&#20016;&#23500;&#21382;&#21490;&#25968;&#25454;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#26126;&#30830;&#25429;&#25417;&#30446;&#26631;&#30005;&#27744;&#21644;&#21442;&#32771;&#30005;&#27744;&#20043;&#38388;&#30340;&#30005;&#20449;&#21495;&#24046;&#24322;&#65292;&#26080;&#35770;&#23427;&#20204;&#30340;&#26448;&#26009;&#21644;&#32769;&#21270;&#26465;&#20214;&#22914;&#20309;&#65292;&#26469;&#39044;&#27979;&#30446;&#26631;&#30005;&#27744;&#30340;&#23551;&#21629;&#12290;&#36890;&#36807;&#36825;&#31181;&#32454;&#32990;&#38388;&#24046;&#24322;&#65292;&#25105;&#20204;&#19981;&#20165;&#25193;&#23637;&#20102;&#29305;&#24449;&#31354;&#38388;&#65292;&#36824;&#20026;&#36890;&#29992;&#30340;&#30005;&#27744;&#23551;&#21629;&#39044;&#27979;&#26694;&#26550;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#26174;&#33879;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#19981;&#21516;&#26465;&#20214;&#19979;&#31934;&#30830;&#39044;&#27979;&#30005;&#27744;&#23551;&#21629;&#12290;
&lt;/p&gt;
&lt;p&gt;
Battery life prediction holds significant practical value for battery research and development. Currently, many data-driven models rely on early electrical signals from specific target batteries to predict their lifespan. A common shortfall is that most existing methods are developed based on specific aging conditions, which not only limits their model's capability but also diminishes their effectiveness in predicting degradation under varied conditions. As a result, these models often miss out on fully benefiting from the rich historical data available under other conditions. Here, to address above, we introduce an approach that explicitly captures differences between electrical signals of a target battery and a reference battery, irrespective of their materials and aging conditions, to forecast the target battery life. Through this inter-cell difference, we not only enhance the feature space but also pave the way for a universal battery life prediction framework. Remarkably, our mode
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37325;&#26032;&#23457;&#35270;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20316;&#20026;&#38646;-shot&#20851;&#31995;&#25277;&#21462;&#22120;&#30340;&#28508;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#36890;&#36807;&#24635;&#32467;&#21644;&#25552;&#38382;(\textsc{SumAsk})&#25552;&#31034;&#26041;&#27861;&#26469;&#25913;&#36827;&#38646;-shot&#20851;&#31995;&#25277;&#21462;&#12290;&#23454;&#39564;&#35777;&#26126;LLMs&#22312;&#36825;&#19968;&#20219;&#21153;&#19978;&#20855;&#26377;&#33391;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.05028</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#38646;-shot&#20851;&#31995;&#25277;&#21462;&#22120;
&lt;/p&gt;
&lt;p&gt;
Revisiting Large Language Models as Zero-shot Relation Extractors. (arXiv:2310.05028v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05028
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37325;&#26032;&#23457;&#35270;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20316;&#20026;&#38646;-shot&#20851;&#31995;&#25277;&#21462;&#22120;&#30340;&#28508;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#36890;&#36807;&#24635;&#32467;&#21644;&#25552;&#38382;(\textsc{SumAsk})&#25552;&#31034;&#26041;&#27861;&#26469;&#25913;&#36827;&#38646;-shot&#20851;&#31995;&#25277;&#21462;&#12290;&#23454;&#39564;&#35777;&#26126;LLMs&#22312;&#36825;&#19968;&#20219;&#21153;&#19978;&#20855;&#26377;&#33391;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#25277;&#21462;(RE)&#21363;&#20351;&#22312;&#38646;-shot&#35774;&#23450;&#19979;&#65292;&#19968;&#30452;&#28041;&#21450;&#19968;&#23450;&#31243;&#24230;&#30340;&#26631;&#35760;&#25110;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#33021;&#22815;&#22312;&#32473;&#23450;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#30340;&#24773;&#20917;&#19979;&#65292;&#26080;&#38656;&#20219;&#20309;&#25968;&#25454;&#21644;&#21442;&#25968;&#35843;&#25972;&#65292;&#33258;&#21160;&#36866;&#24212;&#26032;&#20219;&#21153;&#65292;&#36825;&#20026;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#20851;&#31995;&#25552;&#20379;&#20102;&#21487;&#33021;&#24615;&#12290;&#26412;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#23558;LLMs&#65292;&#22914;ChatGPT&#65292;&#20316;&#20026;&#38646;-shot&#20851;&#31995;&#25277;&#21462;&#22120;&#30340;&#30740;&#31350;&#12290;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#29616;&#26377;RE&#25552;&#31034;&#30340;&#32570;&#28857;&#65292;&#24182;&#23581;&#35797;&#23558;&#26368;&#36817;&#30340;&#25552;&#31034;&#25216;&#26415;&#65292;&#22914;CoT&#65292;&#32435;&#20837;&#20854;&#20013;&#20197;&#25552;&#39640;&#38646;-shot&#20851;&#31995;&#25277;&#21462;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#24635;&#32467;&#21644;&#25552;&#38382;(\textsc{SumAsk})&#25552;&#31034;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#25552;&#31034;&#65292;&#36890;&#36807;&#36882;&#24402;&#20351;&#29992;LLMs&#23558;RE&#36755;&#20837;&#36716;&#25442;&#20026;&#26377;&#25928;&#30340;&#38382;&#31572;(QA)&#26684;&#24335;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#23545;&#21508;&#31181;&#22522;&#20934;&#21644;&#35774;&#32622;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#20197;&#35843;&#26597;LLMs&#22312;&#38646;-shot&#20851;&#31995;&#25277;&#21462;&#19978;&#30340;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26377;&#20197;&#19979;&#30340;followi
&lt;/p&gt;
&lt;p&gt;
Relation extraction (RE) consistently involves a certain degree of labeled or unlabeled data even if under zero-shot setting. Recent studies have shown that large language models (LLMs) transfer well to new tasks out-of-the-box simply given a natural language prompt, which provides the possibility of extracting relations from text without any data and parameter tuning. This work focuses on the study of exploring LLMs, such as ChatGPT, as zero-shot relation extractors. On the one hand, we analyze the drawbacks of existing RE prompts and attempt to incorporate recent prompt techniques such as chain-of-thought (CoT) to improve zero-shot RE. We propose the summarize-and-ask (\textsc{SumAsk}) prompting, a simple prompt recursively using LLMs to transform RE inputs to the effective question answering (QA) format. On the other hand, we conduct comprehensive experiments on various benchmarks and settings to investigate the capabilities of LLMs on zero-shot RE. Specifically, we have the followi
&lt;/p&gt;</description></item><item><title>Compresso&#26159;&#19968;&#31181;&#32467;&#26500;&#21270;&#20462;&#21098;LLMs&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21327;&#20316;&#20419;&#36827;&#23398;&#20064;&#26368;&#20248;&#30340;&#20462;&#21098;&#20915;&#31574;&#65292;&#35299;&#20915;&#20102;&#35757;&#32451;&#25104;&#26412;&#39640;&#21644;&#25968;&#25454;&#25910;&#38598;&#22256;&#38590;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.05015</link><description>&lt;p&gt;
Compresso&#65306;&#32467;&#26500;&#21270;&#20462;&#21098;&#19982;&#21327;&#20316;&#20419;&#36827;&#23398;&#20064;&#32039;&#20945;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Compresso: Structured Pruning with Collaborative Prompting Learns Compact Large Language Models. (arXiv:2310.05015v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05015
&lt;/p&gt;
&lt;p&gt;
Compresso&#26159;&#19968;&#31181;&#32467;&#26500;&#21270;&#20462;&#21098;LLMs&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21327;&#20316;&#20419;&#36827;&#23398;&#20064;&#26368;&#20248;&#30340;&#20462;&#21098;&#20915;&#31574;&#65292;&#35299;&#20915;&#20102;&#35757;&#32451;&#25104;&#26412;&#39640;&#21644;&#25968;&#25454;&#25910;&#38598;&#22256;&#38590;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25104;&#21151;&#65292;&#20294;&#24222;&#22823;&#30340;&#27169;&#22411;&#23610;&#23544;&#32473;&#36164;&#28304;&#21463;&#38480;&#30828;&#20214;&#24102;&#26469;&#20102;&#37325;&#22823;&#37096;&#32626;&#25361;&#25112;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;LLM&#21387;&#32553;&#26041;&#27861;&#20851;&#27880;&#37327;&#21270;&#65292;&#20294;&#20462;&#21098;&#22312;&#35757;&#32451;&#25104;&#26412;&#39640;&#21644;&#25968;&#25454;&#25910;&#38598;&#22256;&#38590;&#31561;&#26041;&#38754;&#30456;&#23545;&#26410;&#34987;&#25506;&#32034;&#12290;&#21333;&#27425;&#20462;&#21098;&#26041;&#27861;&#34429;&#28982;&#25104;&#26412;&#20302;&#19988;&#26080;&#38656;&#25968;&#25454;&#65292;&#24050;&#25104;&#20026;LLM&#20462;&#21098;&#30340;&#20027;&#23548;&#26041;&#24335;&#65292;&#20294;&#22312;&#32467;&#26500;&#21270;&#20462;&#21098;&#35774;&#32622;&#19979;&#20250;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#32467;&#26500;&#21270;&#20462;&#21098;LLMs&#30340;&#33539;&#20363;&#65292;&#31216;&#20026;Compresso&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#25552;&#20986;&#30340;&#36164;&#28304;&#39640;&#25928;&#20462;&#21098;&#31639;&#27861;&#21644;LLM&#33258;&#36523;&#30340;&#21327;&#20316;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23398;&#20064;&#26368;&#20248;&#30340;&#20462;&#21098;&#20915;&#31574;&#12290;Compresso&#36890;&#36807;&#22312;&#25351;&#20196;&#35843;&#25972;&#36807;&#31243;&#20013;&#23558;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#19982;$L_0$&#27491;&#21017;&#21270;&#30456;&#32467;&#21512;&#65292;&#35299;&#20915;&#20102;&#26114;&#36149;&#30340;&#22521;&#35757;&#25104;&#26412;&#21644;&#25968;&#25454;&#25910;&#38598;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the remarkable success of Large Language Models (LLMs), the massive size poses significant deployment challenges, particularly on resource-constrained hardware. While existing LLM compression methods focus on quantization, pruning remains relatively unexplored due to the high cost of training-based approaches and data collection challenges. One-shot pruning methods, although cost-effective and data-free, have become dominant in LLM pruning, but lead to performance decline under the structured pruning setting. In this work, we introduce a new paradigm for structurally pruning LLMs, called Compresso. Our approach, through the collaboration of the proposed resource-efficient pruning algorithm and the LLM itself, learns optimal pruning decisions during the training process. Compresso addresses the challenges of expensive training costs and data collection by incorporating Low-Rank Adaptation (LoRA) into the $L_0$ regularization during the instruction tuning process. Then, we furthe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32463;&#20856;&#26816;&#27979;&#26041;&#27861;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#32454;&#32990;&#24418;&#29366;&#36817;&#20284;&#20026;&#23450;&#21521;&#26925;&#22278;&#24182;&#20351;&#29992;&#36890;&#29992;&#23450;&#21521;&#23545;&#35937;&#26816;&#27979;&#22120;&#26469;&#35782;&#21035;&#32454;&#32990;&#65292;&#23454;&#29616;&#20102;&#23545;&#32454;&#32990;&#30340;&#26816;&#27979;&#21644;&#36319;&#36394;&#65292;&#20943;&#36731;&#20102;&#23545;&#26631;&#27880;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2310.04895</link><description>&lt;p&gt;
&#21033;&#29992;&#26925;&#22278;&#36793;&#30028;&#26694;&#36827;&#34892;&#32454;&#32990;&#36319;&#36394;&#30340;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Cell Tracking-by-detection using Elliptical Bounding Boxes. (arXiv:2310.04895v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04895
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32463;&#20856;&#26816;&#27979;&#26041;&#27861;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#32454;&#32990;&#24418;&#29366;&#36817;&#20284;&#20026;&#23450;&#21521;&#26925;&#22278;&#24182;&#20351;&#29992;&#36890;&#29992;&#23450;&#21521;&#23545;&#35937;&#26816;&#27979;&#22120;&#26469;&#35782;&#21035;&#32454;&#32990;&#65292;&#23454;&#29616;&#20102;&#23545;&#32454;&#32990;&#30340;&#26816;&#27979;&#21644;&#36319;&#36394;&#65292;&#20943;&#36731;&#20102;&#23545;&#26631;&#27880;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32454;&#32990;&#30340;&#26816;&#27979;&#21644;&#36319;&#36394;&#23545;&#20110;&#29983;&#29289;&#20998;&#26512;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#22522;&#20110;&#27169;&#22411;&#28436;&#36827;&#30340;&#36319;&#36394;&#65292;&#36890;&#24120;&#36890;&#36807;&#35757;&#32451;&#31471;&#21040;&#31471;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#22312;&#22270;&#20687;&#24103;&#19978;&#26816;&#27979;&#21644;&#36319;&#36394;&#32454;&#32990;&#65292;&#24182;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#30340;&#26631;&#27880;&#25968;&#25454;&#65292;&#32780;&#33719;&#21462;&#36825;&#20123;&#25968;&#25454;&#32791;&#26102;&#19988;&#38656;&#35201;&#19987;&#19994;&#30340;&#26631;&#27880;&#21592;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32463;&#20856;&#26816;&#27979;&#26041;&#27861;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#20943;&#36731;&#23545;&#26631;&#27880;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#23427;&#23558;&#32454;&#32990;&#24418;&#29366;&#36817;&#20284;&#20026;&#23450;&#21521;&#26925;&#22278;&#65292;&#28982;&#21518;&#20351;&#29992;&#36890;&#29992;&#23450;&#21521;&#23545;&#35937;&#26816;&#27979;&#22120;&#26469;&#35782;&#21035;&#27599;&#20010;&#24103;&#20013;&#30340;&#32454;&#32990;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20381;&#38752;&#19968;&#20010;&#20840;&#23616;&#25968;&#25454;&#20851;&#32852;&#31639;&#27861;&#65292;&#20351;&#29992;&#27010;&#29575;&#36317;&#31163;&#24230;&#37327;&#26469;&#25506;&#32034;&#32454;&#32990;&#30340;&#26102;&#38388;&#30456;&#20284;&#24615;&#65292;&#32771;&#34385;&#21040;&#26925;&#22278;&#19982;&#20108;&#32500;&#39640;&#26031;&#20998;&#24067;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#23454;&#29616;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#26816;&#27979;&#21644;&#36319;&#36394;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cell detection and tracking are paramount for bio-analysis. Recent approaches rely on the tracking-by-model evolution paradigm, which usually consists of training end-to-end deep learning models to detect and track the cells on the frames with promising results. However, such methods require extensive amounts of annotated data, which is time-consuming to obtain and often requires specialized annotators. This work proposes a new approach based on the classical tracking-by-detection paradigm that alleviates the requirement of annotated data. More precisely, it approximates the cell shapes as oriented ellipses and then uses generic-purpose oriented object detectors to identify the cells in each frame. We then rely on a global data association algorithm that explores temporal cell similarity using probability distance metrics, considering that the ellipses relate to two-dimensional Gaussian distributions. Our results show that our method can achieve detection and tracking results competiti
&lt;/p&gt;</description></item><item><title>LauraGPT&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;GPT&#27169;&#22411;&#65292;&#29992;&#20110;&#38899;&#39057;&#35782;&#21035;&#12289;&#29702;&#35299;&#21644;&#29983;&#25104;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#33539;&#22260;&#65292;&#21253;&#25324;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#12289;&#35821;&#38899;&#32763;&#35793;&#12289;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#12289;&#26426;&#22120;&#32763;&#35793;&#31561;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2310.04673</link><description>&lt;p&gt;
LauraGPT&#65306;&#20351;&#29992;GPT&#36827;&#34892;&#21548;&#12289;&#20851;&#27880;&#12289;&#29702;&#35299;&#21644;&#20877;&#29983;&#38899;&#39057;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
LauraGPT: Listen, Attend, Understand, and Regenerate Audio with GPT. (arXiv:2310.04673v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04673
&lt;/p&gt;
&lt;p&gt;
LauraGPT&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;GPT&#27169;&#22411;&#65292;&#29992;&#20110;&#38899;&#39057;&#35782;&#21035;&#12289;&#29702;&#35299;&#21644;&#29983;&#25104;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#33539;&#22260;&#65292;&#21253;&#25324;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#12289;&#35821;&#38899;&#32763;&#35793;&#12289;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#12289;&#26426;&#22120;&#32763;&#35793;&#31561;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#25442;&#22120;&#65288;GPT&#65289;&#27169;&#22411;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23558;&#31867;&#20284;&#30340;&#26694;&#26550;&#24212;&#29992;&#20110;&#38899;&#39057;&#20219;&#21153;&#30340;&#30740;&#31350;&#26377;&#38480;&#12290;&#20197;&#21069;&#25552;&#20986;&#30340;&#29992;&#20110;&#38899;&#39057;&#20219;&#21153;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35201;&#20040;&#32570;&#20047;&#20805;&#20998;&#30340;&#23450;&#37327;&#35780;&#20272;&#65292;&#35201;&#20040;&#23616;&#38480;&#20110;&#35782;&#21035;&#21644;&#29702;&#35299;&#38899;&#39057;&#20869;&#23481;&#30340;&#20219;&#21153;&#65292;&#35201;&#20040;&#26126;&#26174;&#19981;&#21450;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#65288;SOTA&#65289;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LauraGPT&#65292;&#19968;&#20010;&#29992;&#20110;&#38899;&#39057;&#35782;&#21035;&#12289;&#29702;&#35299;&#21644;&#29983;&#25104;&#30340;&#32479;&#19968;GPT&#27169;&#22411;&#12290;LauraGPT&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#22788;&#29702;&#38899;&#39057;&#21644;&#25991;&#26412;&#36755;&#20837;&#65292;&#24182;&#22312;&#20219;&#24847;&#27169;&#24335;&#19979;&#29983;&#25104;&#36755;&#20986;&#12290;&#23427;&#21487;&#20197;&#36827;&#34892;&#19982;&#20869;&#23481;&#12289;&#35821;&#20041;&#12289;&#35821;&#38899;&#23398;&#21644;&#38899;&#39057;&#20449;&#21495;&#20998;&#26512;&#30456;&#20851;&#30340;&#21508;&#31181;&#20219;&#21153;&#12290;&#20854;&#20013;&#19968;&#20123;&#20540;&#24471;&#27880;&#24847;&#30340;&#20219;&#21153;&#21253;&#25324;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#12289;&#35821;&#38899;&#21040;&#25991;&#26412;&#32763;&#35793;&#12289;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#12289;&#26426;&#22120;&#32763;&#35793;&#12289;&#35821;&#38899;&#22686;&#24378;&#12289;&#33258;&#21160;&#38899;&#39057;&#25429;&#33719;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Pre-trained Transformer (GPT) models have achieved remarkable performance on various natural language processing tasks. However, there has been limited research on applying similar frameworks to audio tasks. Previously proposed large language models for audio tasks either lack sufficient quantitative evaluations, or are limited to tasks for recognizing and understanding audio content, or significantly underperform existing state-of-the-art (SOTA) models. In this paper, we propose LauraGPT, a unified GPT model for audio recognition, understanding, and generation. LauraGPT is a versatile language model that can process both audio and text inputs and generate outputs in either modalities. It can perform a wide range of tasks related to content, semantics, paralinguistics, and audio-signal analysis. Some of its noteworthy tasks include automatic speech recognition, speech-to-text translation, text-to-speech synthesis, machine translation, speech enhancement, automated audio capt
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#19968;&#33268;&#24615;&#25913;&#36827;&#25968;&#25454;&#26377;&#25928;&#30340;&#25674;&#20313;&#36125;&#21494;&#26031;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#21453;&#36716;&#36125;&#21494;&#26031;&#23450;&#29702;&#24182;&#21033;&#29992;&#36817;&#20284;&#34920;&#31034;&#30340;&#32852;&#21512;&#27169;&#22411;&#20272;&#35745;&#36793;&#38469;&#20284;&#28982;&#65292;&#21152;&#36895;&#26465;&#20214;&#31070;&#32463;&#23494;&#24230;&#20272;&#35745;&#22120;&#30340;&#23398;&#20064;&#21160;&#21147;&#23398;&#12290;</title><link>http://arxiv.org/abs/2310.04395</link><description>&lt;p&gt;
&#21033;&#29992;&#33258;&#19968;&#33268;&#24615;&#25552;&#39640;&#25968;&#25454;&#26377;&#25928;&#30340;&#25674;&#20313;&#36125;&#21494;&#26031;&#25512;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Leveraging Self-Consistency for Data-Efficient Amortized Bayesian Inference. (arXiv:2310.04395v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04395
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#19968;&#33268;&#24615;&#25913;&#36827;&#25968;&#25454;&#26377;&#25928;&#30340;&#25674;&#20313;&#36125;&#21494;&#26031;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#21453;&#36716;&#36125;&#21494;&#26031;&#23450;&#29702;&#24182;&#21033;&#29992;&#36817;&#20284;&#34920;&#31034;&#30340;&#32852;&#21512;&#27169;&#22411;&#20272;&#35745;&#36793;&#38469;&#20284;&#28982;&#65292;&#21152;&#36895;&#26465;&#20214;&#31070;&#32463;&#23494;&#24230;&#20272;&#35745;&#22120;&#30340;&#23398;&#20064;&#21160;&#21147;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21442;&#25968;$\theta$&#21644;&#25968;&#25454;$y$&#30340;&#27010;&#29575;&#32852;&#21512;&#27169;&#22411;$p(\theta, y)$&#20013;&#30340;&#36890;&#29992;&#23545;&#31216;&#24615;&#65292;&#25913;&#36827;&#20102;&#25674;&#20313;&#36125;&#21494;&#26031;&#25512;&#29702;&#65288;ABI&#65289;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;&#31616;&#35328;&#20043;&#65292;&#25105;&#20204;&#21453;&#36716;&#36125;&#21494;&#26031;&#23450;&#29702;&#65292;&#24182;&#22522;&#20110;&#36817;&#20284;&#34920;&#31034;&#30340;&#32852;&#21512;&#27169;&#22411;&#20272;&#35745;&#36793;&#38469;&#20284;&#28982;&#12290;&#22312;&#23436;&#32654;&#36817;&#20284;&#24773;&#20917;&#19979;&#65292;&#36793;&#38469;&#20284;&#28982;&#22312;&#25152;&#26377;&#21442;&#25968;&#20540;&#19978;&#37117;&#26159;&#24120;&#25968;&#23450;&#20041;&#30340;&#12290;&#28982;&#32780;&#65292;&#36817;&#20284;&#35823;&#24046;&#23548;&#33268;&#19981;&#21516;&#21442;&#25968;&#20540;&#30340;&#36793;&#38469;&#20284;&#28982;&#20272;&#35745;&#20013;&#23384;&#22312;&#19981;&#21487;&#21462;&#30340;&#26041;&#24046;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#23545;&#31216;&#24615;&#30340;&#36829;&#21453;&#24418;&#24335;&#21270;&#20026;&#25439;&#22833;&#20989;&#25968;&#65292;&#21152;&#36895;&#26465;&#20214;&#31070;&#32463;&#23494;&#24230;&#20272;&#35745;&#22120;&#30340;&#23398;&#20064;&#21160;&#21147;&#23398;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#20855;&#26377;&#26174;&#24335;&#20284;&#28982;&#65288;&#22522;&#20110;&#20284;&#28982;&#65289;&#30340;&#21452;&#23792;&#29609;&#20855;&#38382;&#39064;&#21644;&#20855;&#26377;&#38544;&#24335;&#20284;&#28982;&#65288;&#22522;&#20110;&#27169;&#25311;&#65289;&#30340;&#29616;&#23454;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a method to improve the efficiency and accuracy of amortized Bayesian inference (ABI) by leveraging universal symmetries in the probabilistic joint model $p(\theta, y)$ of parameters $\theta$ and data $y$. In a nutshell, we invert Bayes' theorem and estimate the marginal likelihood based on approximate representations of the joint model. Upon perfect approximation, the marginal likelihood is constant across all parameter values by definition. However, approximation error leads to undesirable variance in the marginal likelihood estimates across different parameter values. We formulate violations of this symmetry as a loss function to accelerate the learning dynamics of conditional neural density estimators. We apply our method to a bimodal toy problem with an explicit likelihood (likelihood-based) and a realistic model with an implicit likelihood (simulation-based).
&lt;/p&gt;</description></item><item><title>Hermes&#26159;&#19968;&#20010;&#33258;&#21160;&#29983;&#25104;&#26377;&#38480;&#29366;&#24577;&#26426;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#38145;&#31227;&#21160;&#32593;&#32476;&#21327;&#35758;&#30340;&#23433;&#20840;&#20998;&#26512;&#12290;&#36890;&#36807;&#22788;&#29702;&#33258;&#28982;&#35821;&#35328;&#35268;&#33539;&#24182;&#29983;&#25104;&#36923;&#36753;&#20844;&#24335;&#65292;Hermes&#33021;&#22815;&#21457;&#29616;&#26032;&#30340;&#28431;&#27934;&#21644;&#25915;&#20987;&#65292;&#24182;&#23545;&#29616;&#26377;&#35268;&#33539;&#21644;&#21830;&#19994;&#22522;&#24102;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2310.04381</link><description>&lt;p&gt;
Hermes&#65306;&#36890;&#36807;&#20174;&#33258;&#28982;&#35821;&#35328;&#35268;&#33539;&#21512;&#25104;&#26377;&#38480;&#29366;&#24577;&#26426;&#26469;&#35299;&#38145;&#31227;&#21160;&#32593;&#32476;&#21327;&#35758;&#30340;&#23433;&#20840;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Hermes: Unlocking Security Analysis of Cellular Network Protocols by Synthesizing Finite State Machines from Natural Language Specifications. (arXiv:2310.04381v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04381
&lt;/p&gt;
&lt;p&gt;
Hermes&#26159;&#19968;&#20010;&#33258;&#21160;&#29983;&#25104;&#26377;&#38480;&#29366;&#24577;&#26426;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#38145;&#31227;&#21160;&#32593;&#32476;&#21327;&#35758;&#30340;&#23433;&#20840;&#20998;&#26512;&#12290;&#36890;&#36807;&#22788;&#29702;&#33258;&#28982;&#35821;&#35328;&#35268;&#33539;&#24182;&#29983;&#25104;&#36923;&#36753;&#20844;&#24335;&#65292;Hermes&#33021;&#22815;&#21457;&#29616;&#26032;&#30340;&#28431;&#27934;&#21644;&#25915;&#20987;&#65292;&#24182;&#23545;&#29616;&#26377;&#35268;&#33539;&#21644;&#21830;&#19994;&#22522;&#24102;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Hermes&#65292;&#19968;&#20010;&#33258;&#21160;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#31227;&#21160;&#35268;&#33539;&#30340;&#24418;&#24335;&#34920;&#36798;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#12290;&#25105;&#20204;&#39318;&#20808;&#24320;&#21457;&#20102;&#31070;&#32463;&#32452;&#25104;&#20998;&#26512;&#22120;NEUTREX&#65292;&#29992;&#20110;&#22788;&#29702;&#19982;&#36716;&#25442;&#30456;&#20851;&#30340;&#25991;&#26412;&#24182;&#25552;&#21462;&#36716;&#25442;&#32452;&#20214;&#65288;&#21363;&#29366;&#24577;&#12289;&#26465;&#20214;&#21644;&#21160;&#20316;&#65289;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#31181;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#65292;&#36890;&#36807;&#21033;&#29992;&#20381;&#23384;&#35299;&#26512;&#26641;&#23558;&#36825;&#20123;&#36716;&#25442;&#32452;&#20214;&#36716;&#21270;&#25104;&#36923;&#36753;&#20844;&#24335;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#36923;&#36753;&#20844;&#24335;&#32534;&#35793;&#25104;&#36716;&#25442;&#21644;&#21019;&#24314;&#24418;&#24335;&#27169;&#22411;&#20316;&#20026;&#26377;&#38480;&#29366;&#24577;&#26426;&#12290;&#20026;&#20102;&#35777;&#26126;Hermes&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#22312;4G NAS&#12289;5G NAS&#21644;5G RRC&#35268;&#33539;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#33719;&#24471;&#20102;81-87%&#30340;&#24635;&#20307;&#20934;&#30830;&#29575;&#65292;&#36825;&#26159;&#23545;&#29616;&#26377;&#25216;&#26415;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;&#25105;&#20204;&#23545;&#25552;&#21462;&#30340;&#27169;&#22411;&#36827;&#34892;&#30340;&#23433;&#20840;&#20998;&#26512;&#25581;&#31034;&#20986;&#20102;3&#20010;&#26032;&#30340;&#28431;&#27934;&#12289;&#21457;&#29616;&#20102;19&#20010;&#20043;&#21069;&#30340;&#25915;&#20987;4G&#21644;5G&#35268;&#33539;&#65292;&#20197;&#21450;7&#20010;&#21830;&#19994;4G&#22522;&#24102;&#30340;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present Hermes, an end-to-end framework to automatically generate formal representations from natural language cellular specifications. We first develop a neural constituency parser, NEUTREX, to process transition-relevant texts and extract transition components (i.e., states, conditions, and actions). We also design a domain-specific language to translate these transition components to logical formulas by leveraging dependency parse trees. Finally, we compile these logical formulas to generate transitions and create the formal model as finite state machines. To demonstrate the effectiveness of Hermes, we evaluate it on 4G NAS, 5G NAS, and 5G RRC specifications and obtain an overall accuracy of 81-87%, which is a substantial improvement over the state-of-the-art. Our security analysis of the extracted models uncovers 3 new vulnerabilities and identifies 19 previous attacks in 4G and 5G specifications, and 7 deviations in commercial 4G basebands.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#23558;&#22810;&#31181;&#21487;&#33021;&#30340;&#24402;&#32435;&#35299;&#37322;&#27719;&#24635;&#20026;&#29305;&#24449;&#37325;&#35201;&#24615;&#20998;&#25968;&#30340;&#19977;&#31181;&#32858;&#21512;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#23384;&#22312;&#22810;&#20010;&#26377;&#25928;&#30340;&#24402;&#32435;&#35299;&#37322;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.03131</link><description>&lt;p&gt;
Axiomatic Aggregations of Abductive Explanations. (arXiv:2310.03131v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
Axiomatic Aggregations of Abductive Explanations. (arXiv:2310.03131v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03131
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#23558;&#22810;&#31181;&#21487;&#33021;&#30340;&#24402;&#32435;&#35299;&#37322;&#27719;&#24635;&#20026;&#29305;&#24449;&#37325;&#35201;&#24615;&#20998;&#25968;&#30340;&#19977;&#31181;&#32858;&#21512;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#23384;&#22312;&#22810;&#20010;&#26377;&#25928;&#30340;&#24402;&#32435;&#35299;&#37322;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;&#21518;&#39564;&#27169;&#22411;&#36817;&#20284;&#35299;&#37322;&#26041;&#27861;&#65288;&#22914;LIME&#21644;SHAP&#65289;&#20581;&#22766;&#24615;&#30340;&#25209;&#35780;&#23548;&#33268;&#20102;&#27169;&#22411;&#31934;&#30830;&#38416;&#37322;&#30340;&#20852;&#36215;&#12290;&#38024;&#23545;&#27599;&#20010;&#25968;&#25454;&#28857;&#65292;&#24402;&#32435;&#35299;&#37322;&#25552;&#20379;&#20102;&#33021;&#22815;&#29983;&#25104;&#32467;&#26524;&#30340;&#26368;&#23567;&#29305;&#24449;&#23376;&#38598;&#12290;&#23613;&#31649;&#22312;&#29702;&#35770;&#19978;&#26159;&#21487;&#38752;&#19988;&#20005;&#35880;&#30340;&#65292;&#20294;&#26159;&#24402;&#32435;&#35299;&#37322;&#23384;&#22312;&#19968;&#20010;&#20027;&#35201;&#38382;&#39064; - &#23545;&#20110;&#21516;&#19968;&#25968;&#25454;&#28857;&#21487;&#33021;&#23384;&#22312;&#22810;&#20010;&#26377;&#25928;&#30340;&#24402;&#32435;&#35299;&#37322;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25552;&#20379;&#21333;&#19968;&#30340;&#24402;&#32435;&#35299;&#37322;&#21487;&#33021;&#26159;&#19981;&#36275;&#22815;&#30340;&#65307;&#21478;&#19968;&#26041;&#38754;&#65292;&#25552;&#20379;&#25152;&#26377;&#26377;&#25928;&#30340;&#24402;&#32435;&#35299;&#37322;&#21487;&#33021;&#30001;&#20110;&#20854;&#25968;&#37327;&#24222;&#22823;&#32780;&#38590;&#20197;&#29702;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#22810;&#31181;&#21487;&#33021;&#30340;&#24402;&#32435;&#35299;&#37322;&#27719;&#24635;&#20026;&#29305;&#24449;&#37325;&#35201;&#24615;&#20998;&#25968;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#32858;&#21512;&#26041;&#27861;&#65306;&#20004;&#31181;&#22522;&#20110;&#21512;&#20316;&#21338;&#24328;&#35770;&#30340;&#26435;&#21147;&#25351;&#25968;&#21644;&#19968;&#31181;&#22522;&#20110;&#33879;&#21517;&#30340;&#22240;&#26524;&#24378;&#24230;&#24230;&#37327;&#12290;&#25105;&#20204;&#36890;&#36807;&#20844;&#29702;&#21270;&#34920;&#24449;&#36825;&#19977;&#31181;&#26041;&#27861;&#65292;&#35777;&#26126;&#23427;&#20204;&#27599;&#19968;&#20010;&#37117;&#20855;&#26377;
&lt;/p&gt;
&lt;p&gt;
The recent criticisms of the robustness of post hoc model approximation explanation methods (like LIME and SHAP) have led to the rise of model-precise abductive explanations. For each data point, abductive explanations provide a minimal subset of features that are sufficient to generate the outcome. While theoretically sound and rigorous, abductive explanations suffer from a major issue -- there can be several valid abductive explanations for the same data point. In such cases, providing a single abductive explanation can be insufficient; on the other hand, providing all valid abductive explanations can be incomprehensible due to their size. In this work, we solve this issue by aggregating the many possible abductive explanations into feature importance scores. We propose three aggregation methods: two based on power indices from cooperative game theory and a third based on a well-known measure of causal strength. We characterize these three methods axiomatically, showing that each of 
&lt;/p&gt;</description></item><item><title>HyperMask&#26159;&#19968;&#31181;&#29992;&#20110;&#25345;&#32493;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#22522;&#20110;&#36229;&#32593;&#32476;&#30340;&#25513;&#30721;&#26469;&#35757;&#32451;&#19968;&#20010;&#21333;&#19968;&#32593;&#32476;&#65292;&#20197;&#20811;&#26381;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#22312;&#22810;&#20219;&#21153;&#19978;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.00113</link><description>&lt;p&gt;
HyperMask: &#33258;&#36866;&#24212;&#30340;&#22522;&#20110;&#36229;&#32593;&#32476;&#30340;&#25513;&#30721;&#29992;&#20110;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
HyperMask: Adaptive Hypernetwork-based Masks for Continual Learning. (arXiv:2310.00113v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00113
&lt;/p&gt;
&lt;p&gt;
HyperMask&#26159;&#19968;&#31181;&#29992;&#20110;&#25345;&#32493;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#22522;&#20110;&#36229;&#32593;&#32476;&#30340;&#25513;&#30721;&#26469;&#35757;&#32451;&#19968;&#20010;&#21333;&#19968;&#32593;&#32476;&#65292;&#20197;&#20811;&#26381;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#22312;&#22810;&#20219;&#21153;&#19978;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#39034;&#24207;&#35757;&#32451;&#26102;&#65292;&#24448;&#24448;&#20250;&#20986;&#29616;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#24050;&#32463;&#23384;&#22312;&#35768;&#22810;&#25345;&#32493;&#23398;&#20064;&#31574;&#30053;&#65292;&#20854;&#20013;&#26368;&#26377;&#25928;&#30340;&#20043;&#19968;&#26159;&#22522;&#20110;&#36229;&#32593;&#32476;&#30340;&#26041;&#27861;&#12290;&#36229;&#32593;&#32476;&#26681;&#25454;&#20219;&#21153;&#30340;&#29305;&#24449;&#29983;&#25104;&#30446;&#26631;&#27169;&#22411;&#30340;&#26435;&#37325;&#12290;&#28982;&#32780;&#65292;&#35813;&#27169;&#22411;&#30340;&#20027;&#35201;&#38480;&#21046;&#26159;&#36229;&#32593;&#32476;&#23545;&#20110;&#27599;&#20010;&#20219;&#21153;&#21487;&#20197;&#20135;&#29983;&#23436;&#20840;&#19981;&#21516;&#30340;&#32593;&#32476;&#32467;&#26500;&#65292;&#22240;&#27492;&#27599;&#20010;&#20219;&#21153;&#37117;&#26159;&#21333;&#29420;&#35299;&#20915;&#30340;&#12290;&#27169;&#22411;&#22312;&#23398;&#20064;&#21518;&#32493;&#20219;&#21153;&#26102;&#19981;&#20351;&#29992;&#20043;&#21069;&#20219;&#21153;&#25152;&#20851;&#32852;&#30340;&#32593;&#32476;&#20449;&#24687;&#65292;&#24182;&#23454;&#38469;&#19978;&#20135;&#29983;&#20102;&#26032;&#30340;&#32593;&#32476;&#26550;&#26500;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#24425;&#31080;&#31080;&#35777;&#20551;&#35774;&#65292;&#35813;&#20551;&#35774;&#35748;&#20026;&#23384;&#22312;&#31232;&#30095;&#30340;&#23376;&#32593;&#32476;&#65288;&#21363;&#20013;&#22870;&#31080;&#65289;&#65292;&#21487;&#20197;&#20445;&#25345;&#23436;&#25972;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HyperMask&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20026;&#25152;&#26377;&#20219;&#21153;&#35757;&#32451;&#19968;&#20010;&#21333;&#19968;&#32593;&#32476;&#12290;&#36229;&#32593;&#32476;&#20135;&#29983;&#21322;&#20108;&#36827;&#21046;&#25513;&#30721;&#65292;&#20197;&#33719;&#21462;&#30446;&#26631;&#23376;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial neural networks suffer from catastrophic forgetting when they are sequentially trained on multiple tasks. To overcome this problem, there exist many continual learning strategies. One of the most effective is the hypernetwork-based approach. The hypernetwork generates the weights of a target model based on the task's identity. The model's main limitation is that hypernetwork can produce completely different nests for each task. Consequently, each task is solved separately. The model does not use information from the network dedicated to previous tasks and practically produces new architectures when it learns the subsequent tasks. To solve such a problem, we use the lottery ticket hypothesis, which postulates the existence of sparse subnetworks, named winning tickets, that preserve the performance of a full network.  In the paper, we propose a method called HyperMask, which trains a single network for all tasks. Hypernetwork produces semi-binary masks to obtain target subnetw
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;"RAFA"&#30340;&#21407;&#21017;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;LLM&#20013;&#23558;&#25512;&#29702;&#35270;&#20026;&#23398;&#20064;&#21644;&#35268;&#21010;&#30340;&#36125;&#21494;&#26031;&#38382;&#39064;&#65292;&#21327;&#35843;&#25512;&#29702;&#21644;&#34892;&#21160;&#12290;&#36890;&#36807;&#19968;&#20010;&#25552;&#31034;&#27169;&#26495;&#36827;&#34892;&#25512;&#29702;&#65292;&#23398;&#20064;&#24182;&#21046;&#23450;&#26410;&#26469;&#30340;&#36712;&#36857;&#35268;&#21010;&#65292;&#28982;&#21518;&#22312;&#27599;&#19968;&#27493;&#20013;&#37319;&#21462;&#35745;&#21010;&#36712;&#36857;&#30340;&#21021;&#22987;&#34892;&#21160;&#24182;&#37325;&#26032;&#35268;&#21010;&#26410;&#26469;&#36712;&#36857;&#12290;&#36825;&#20010;&#26694;&#26550;&#20855;&#26377;&#21487;&#35777;&#26126;&#30340;&#36951;&#25022;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2309.17382</link><description>&lt;p&gt;
&#26410;&#26469;&#30340;&#21407;&#22240;&#65292;&#29616;&#22312;&#30340;&#34892;&#21160;&#65306;&#19968;&#31181;&#21487;&#35777;&#26126;&#26679;&#26412;&#25928;&#29575;&#30340;&#33258;&#20027;LLM&#26234;&#33021;&#20307;&#30340;&#21407;&#21017;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Reason for Future, Act for Now: A Principled Framework for Autonomous LLM Agents with Provable Sample Efficiency. (arXiv:2309.17382v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17382
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;"RAFA"&#30340;&#21407;&#21017;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;LLM&#20013;&#23558;&#25512;&#29702;&#35270;&#20026;&#23398;&#20064;&#21644;&#35268;&#21010;&#30340;&#36125;&#21494;&#26031;&#38382;&#39064;&#65292;&#21327;&#35843;&#25512;&#29702;&#21644;&#34892;&#21160;&#12290;&#36890;&#36807;&#19968;&#20010;&#25552;&#31034;&#27169;&#26495;&#36827;&#34892;&#25512;&#29702;&#65292;&#23398;&#20064;&#24182;&#21046;&#23450;&#26410;&#26469;&#30340;&#36712;&#36857;&#35268;&#21010;&#65292;&#28982;&#21518;&#22312;&#27599;&#19968;&#27493;&#20013;&#37319;&#21462;&#35745;&#21010;&#36712;&#36857;&#30340;&#21021;&#22987;&#34892;&#21160;&#24182;&#37325;&#26032;&#35268;&#21010;&#26410;&#26469;&#36712;&#36857;&#12290;&#36825;&#20010;&#26694;&#26550;&#20855;&#26377;&#21487;&#35777;&#26126;&#30340;&#36951;&#25022;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20294;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#23558;&#25512;&#29702;&#36716;&#21270;&#20026;&#34892;&#21160;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#22914;&#20309;&#36890;&#36807;&#25512;&#29702;&#30340;&#20869;&#37096;&#26426;&#21046;&#22312;&#19982;&#22806;&#37096;&#29615;&#22659;&#30340;&#26368;&#23569;&#20132;&#20114;&#27425;&#25968;&#20869;&#21487;&#35777;&#26126;&#22320;&#23436;&#25104;&#32473;&#23450;&#20219;&#21153;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#21487;&#35777;&#26126;&#36951;&#25022;&#20445;&#35777;&#30340;&#21407;&#21017;&#26694;&#26550;&#26469;&#21327;&#35843;&#25512;&#29702;&#21644;&#34892;&#21160;&#65292;&#31216;&#20043;&#20026;&#8220;&#20026;&#26410;&#26469;&#32780;&#25512;&#29702;&#65292;&#20026;&#29616;&#22312;&#32780;&#34892;&#21160;&#8221;&#65288;RAFA&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#25512;&#29702;&#30340;&#25552;&#31034;&#27169;&#26495;&#65292;&#20174;&#20869;&#23384;&#32531;&#20914;&#21306;&#20013;&#23398;&#20064;&#24182;&#21046;&#23450;&#26410;&#26469;&#30340;&#38271;&#26399;&#36712;&#36857;&#35268;&#21010;&#65288;&#8220;&#20026;&#26410;&#26469;&#32780;&#25512;&#29702;&#8221;&#65289;&#12290;&#22312;&#27599;&#19968;&#27493;&#20013;&#65292;LLM&#26234;&#33021;&#20307;&#37319;&#21462;&#35745;&#21010;&#36712;&#36857;&#30340;&#21021;&#22987;&#34892;&#21160;&#65288;&#8220;&#20026;&#29616;&#22312;&#32780;&#34892;&#21160;&#8221;&#65289;&#65292;&#23558;&#25910;&#38598;&#21040;&#30340;&#21453;&#39304;&#23384;&#20648;&#22312;&#20869;&#23384;&#32531;&#20914;&#21306;&#20013;&#65292;&#24182;&#37325;&#26032;&#35843;&#29992;&#25512;&#29702;&#36807;&#31243;&#20174;&#26032;&#29366;&#24577;&#37325;&#26032;&#35268;&#21010;&#26410;&#26469;&#30340;&#36712;&#36857;&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#23558;LLM&#20013;&#30340;&#25512;&#29702;&#35270;&#20026;&#23398;&#20064;&#21644;&#35268;&#21010;&#30340;&#36125;&#21494;&#26031;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) demonstrate impressive reasoning abilities, but translating reasoning into actions in the real world remains challenging. In particular, it remains unclear how to complete a given task provably within a minimum number of interactions with the external environment, e.g., through an internal mechanism of reasoning. To this end, we propose a principled framework with provable regret guarantees to orchestrate reasoning and acting, which we call "reason for future, act for now" (\texttt{RAFA}). Specifically, we design a prompt template for reasoning that learns from the memory buffer and plans a future trajectory over a long horizon ("reason for future"). At each step, the LLM agent takes the initial action of the planned trajectory ("act for now"), stores the collected feedback in the memory buffer, and reinvokes the reasoning routine to replan the future trajectory from the new state.  The key idea is to cast reasoning in LLMs as learning and planning in Bayes
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#26041;&#27861;ACGAN-GNNExplainer&#65292;&#23558;&#36741;&#21161;&#20998;&#31867;&#22120;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;ACGAN&#65289;&#24341;&#20837;&#21040;GNN&#35299;&#37322;&#39046;&#22495;&#12290;&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#22120;&#20026;&#21407;&#22987;&#36755;&#20837;&#22270;&#29983;&#25104;&#35299;&#37322;&#65292;&#24182;&#20511;&#21161;&#37492;&#21035;&#22120;&#30417;&#30563;&#29983;&#25104;&#36807;&#31243;&#65292;&#25552;&#39640;&#35299;&#37322;&#30340;&#20934;&#30830;&#24615;&#21644;&#20445;&#30495;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.16918</link><description>&lt;p&gt;
ACGAN-GNNExplainer&#65306;&#29992;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#36741;&#21161;&#26465;&#20214;&#29983;&#25104;&#35299;&#37322;&#22120;
&lt;/p&gt;
&lt;p&gt;
ACGAN-GNNExplainer: Auxiliary Conditional Generative Explainer for Graph Neural Networks. (arXiv:2309.16918v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16918
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#26041;&#27861;ACGAN-GNNExplainer&#65292;&#23558;&#36741;&#21161;&#20998;&#31867;&#22120;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;ACGAN&#65289;&#24341;&#20837;&#21040;GNN&#35299;&#37322;&#39046;&#22495;&#12290;&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#22120;&#20026;&#21407;&#22987;&#36755;&#20837;&#22270;&#29983;&#25104;&#35299;&#37322;&#65292;&#24182;&#20511;&#21161;&#37492;&#21035;&#22120;&#30417;&#30563;&#29983;&#25104;&#36807;&#31243;&#65292;&#25552;&#39640;&#35299;&#37322;&#30340;&#20934;&#30830;&#24615;&#21644;&#20445;&#30495;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#24050;&#32463;&#22312;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#20013;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#20294;&#20854;&#22522;&#26412;&#26426;&#21046;&#20173;&#28982;&#26159;&#19968;&#20010;&#35868;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#24182;&#23454;&#29616;&#21487;&#38752;&#30340;&#20915;&#31574;&#65292;&#36817;&#24180;&#26469;&#25552;&#20986;&#20102;&#35768;&#22810;GNN&#35299;&#37322;&#22120;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24120;&#24120;&#38754;&#20020;&#19968;&#20123;&#38480;&#21046;&#65292;&#21253;&#25324;&#23545;&#29305;&#23450;&#23454;&#20363;&#30340;&#20381;&#36182;&#24615;&#65292;&#23545;&#26410;&#35265;&#36807;&#30340;&#22270;&#30340;&#19968;&#33324;&#24615;&#19981;&#36275;&#65292;&#21487;&#33021;&#20135;&#29983;&#26080;&#25928;&#30340;&#35299;&#37322;&#20197;&#21450;&#29983;&#25104;&#36807;&#31243;&#20013;&#20135;&#29983;&#30340;&#19981;&#20805;&#20998;&#30340;&#20445;&#30495;&#24230;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#26412;&#25991;&#23558;&#36741;&#21161;&#20998;&#31867;&#22120;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;ACGAN&#65289;&#24341;&#20837;&#21040;GNN&#35299;&#37322;&#39046;&#22495;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;GNN&#35299;&#37322;&#22120;ACGAN-GNNExplainer&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#29983;&#25104;&#22120;&#20026;&#21407;&#22987;&#36755;&#20837;&#22270;&#29983;&#25104;&#35299;&#37322;&#65292;&#24182;&#36816;&#29992;&#37492;&#21035;&#22120;&#26469;&#30417;&#30563;&#29983;&#25104;&#36807;&#31243;&#65292;&#30830;&#20445;&#35299;&#37322;&#30340;&#20445;&#30495;&#24230;&#24182;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#35780;&#20272;&#20998;&#21035;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#22270;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) have proven their efficacy in a variety of real-world applications, but their underlying mechanisms remain a mystery. To address this challenge and enable reliable decision-making, many GNN explainers have been proposed in recent years. However, these methods often encounter limitations, including their dependence on specific instances, lack of generalizability to unseen graphs, producing potentially invalid explanations, and yielding inadequate fidelity. To overcome these limitations, we, in this paper, introduce the Auxiliary Classifier Generative Adversarial Network (ACGAN) into the field of GNN explanation and propose a new GNN explainer dubbed~\emph{ACGAN-GNNExplainer}. Our approach leverages a generator to produce explanations for the original input graphs while incorporating a discriminator to oversee the generation process, ensuring explanation fidelity and improving accuracy. Experimental evaluations conducted on both synthetic and real-world graph
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#21327;&#20316;&#20195;&#29702;&#20043;&#38388;&#22996;&#27966;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#20195;&#29702;&#31649;&#29702;&#22120;&#26681;&#25454;&#20219;&#21153;&#32489;&#25928;&#32570;&#38519;&#36827;&#34892;&#22996;&#27966;&#20915;&#31574;&#65292;&#24182;&#19988;&#21487;&#20197;&#22788;&#29702;&#19981;&#21516;&#29615;&#22659;&#34920;&#31034;&#19979;&#30340;&#22242;&#38431;&#25805;&#20316;&#12290;&#26041;&#27861;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#31649;&#29702;&#22242;&#38431;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.14718</link><description>&lt;p&gt;
&#20248;&#21270;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#21327;&#20316;&#20195;&#29702;&#20043;&#38388;&#30340;&#22996;&#27966;
&lt;/p&gt;
&lt;p&gt;
Optimizing delegation between human and AI collaborative agents. (arXiv:2309.14718v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14718
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#21327;&#20316;&#20195;&#29702;&#20043;&#38388;&#22996;&#27966;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#20195;&#29702;&#31649;&#29702;&#22120;&#26681;&#25454;&#20219;&#21153;&#32489;&#25928;&#32570;&#38519;&#36827;&#34892;&#22996;&#27966;&#20915;&#31574;&#65292;&#24182;&#19988;&#21487;&#20197;&#22788;&#29702;&#19981;&#21516;&#29615;&#22659;&#34920;&#31034;&#19979;&#30340;&#22242;&#38431;&#25805;&#20316;&#12290;&#26041;&#27861;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#31649;&#29702;&#22242;&#38431;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#31867;&#19982;&#20154;&#24037;&#26234;&#33021;&#25110;&#33258;&#20027;&#20195;&#29702;&#24418;&#25104;&#28151;&#21512;&#22242;&#38431;&#30340;&#24773;&#26223;&#20013;&#65292;&#31934;&#30830;&#22320;&#30830;&#23450;&#20309;&#26102;&#25480;&#26435;&#22242;&#38431;&#25104;&#21592;&#25191;&#34892;&#34892;&#21160;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#37492;&#20110;&#36807;&#21435;&#30340;&#20363;&#23376;&#20013;&#65292;&#20154;&#31867;&#21644;&#33258;&#20027;&#31995;&#32479;&#22312;&#20219;&#21153;&#19978;&#21487;&#33021;&#25104;&#21151;&#20063;&#21487;&#33021;&#22833;&#36133;&#65292;&#25105;&#20204;&#35797;&#22270;&#35757;&#32451;&#19968;&#20010;&#20195;&#29702;&#31649;&#29702;&#22120;&#26469;&#26681;&#25454;&#36825;&#20123;&#28508;&#22312;&#30340;&#32489;&#25928;&#32570;&#38519;&#20570;&#20986;&#22996;&#27966;&#20915;&#31574;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#19981;&#33021;&#24635;&#26159;&#26399;&#26395;&#21508;&#31181;&#20195;&#29702;&#22312;&#30456;&#21516;&#30340;&#29615;&#22659;&#27169;&#22411;&#20013;&#36816;&#34892;&#12290;&#21487;&#33021;&#20250;&#36935;&#21040;&#20195;&#29702;&#20043;&#38388;&#30340;&#34892;&#21160;&#21644;&#36716;&#25442;&#26377;&#25152;&#19981;&#21516;&#30340;&#24773;&#20917;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#25552;&#20379;&#20102;&#19968;&#20010;&#32463;&#36807;&#35266;&#23519;&#22242;&#38431;&#32489;&#25928;&#23398;&#20064;&#30340;&#31649;&#29702;&#27169;&#22411;&#65292;&#32780;&#19981;&#38480;&#21046;&#20195;&#29702;&#19982;&#21305;&#37197;&#30340;&#21160;&#24577;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31649;&#29702;&#22120;&#33021;&#22815;&#22312;&#25805;&#20316;&#29615;&#22659;&#30340;&#19981;&#21516;&#34920;&#31034;&#19979;&#36827;&#34892;&#22996;&#27966;&#20915;&#31574;&#65292;&#36828;&#36828;&#36229;&#36807;&#20102;&#20854;&#20182;&#31649;&#29702;&#22242;&#38431;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the context of humans operating with artificial or autonomous agents in a hybrid team, it is essential to accurately identify when to authorize those team members to perform actions. Given past examples where humans and autonomous systems can either succeed or fail at tasks, we seek to train a delegating manager agent to make delegation decisions with respect to these potential performance deficiencies. Additionally, we cannot always expect the various agents to operate within the same underlying model of the environment. It is possible to encounter cases where the actions and transitions would vary between agents. Therefore, our framework provides a manager model which learns through observations of team performance without restricting agents to matching dynamics. Our results show our manager learns to perform delegation decisions with teams of agents operating under differing representations of the environment, significantly outperforming alternative methods to manage the team.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20174;&#31070;&#32463;&#32467;&#26500;&#21644;&#35270;&#35273;&#26426;&#21046;&#30340;&#21551;&#31034;&#20986;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#29289;&#32593;&#32476;&#26550;&#26500;&#29992;&#20110;&#36229;&#20687;&#32032;&#20998;&#21106;&#65292;&#20854;&#20013;&#21253;&#25324;&#22686;&#24378;&#31579;&#36873;&#27169;&#22359;&#65288;ESM&#65289;&#21644;&#36793;&#30028;&#24863;&#30693;&#26631;&#31614;&#65288;BAL&#65289;&#65292;&#26088;&#22312;&#20135;&#29983;&#20005;&#26684;&#36981;&#24490;&#29289;&#20307;&#36793;&#30028;&#19988;&#20256;&#36798;&#20016;&#23500;&#35270;&#35273;&#31526;&#21495;&#30340;&#36229;&#20687;&#32032;&#12290;</title><link>http://arxiv.org/abs/2309.13438</link><description>&lt;p&gt;
&#20174;&#29983;&#29289;&#23398;&#21551;&#21457;&#26426;&#21046;&#37325;&#26032;&#24605;&#32771;&#36229;&#20687;&#32032;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Rethinking superpixel segmentation from biologically inspired mechanisms. (arXiv:2309.13438v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13438
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20174;&#31070;&#32463;&#32467;&#26500;&#21644;&#35270;&#35273;&#26426;&#21046;&#30340;&#21551;&#31034;&#20986;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#29289;&#32593;&#32476;&#26550;&#26500;&#29992;&#20110;&#36229;&#20687;&#32032;&#20998;&#21106;&#65292;&#20854;&#20013;&#21253;&#25324;&#22686;&#24378;&#31579;&#36873;&#27169;&#22359;&#65288;ESM&#65289;&#21644;&#36793;&#30028;&#24863;&#30693;&#26631;&#31614;&#65288;BAL&#65289;&#65292;&#26088;&#22312;&#20135;&#29983;&#20005;&#26684;&#36981;&#24490;&#29289;&#20307;&#36793;&#30028;&#19988;&#20256;&#36798;&#20016;&#23500;&#35270;&#35273;&#31526;&#21495;&#30340;&#36229;&#20687;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#36229;&#20687;&#32032;&#20998;&#21106;&#26041;&#27861;&#30340;&#21457;&#23637;&#22312;&#20998;&#21106;&#30340;&#25928;&#29575;&#21644;&#24615;&#33021;&#26041;&#38754;&#37117;&#21462;&#24471;&#20102;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#22312;&#20110;&#20135;&#29983;&#20005;&#26684;&#36981;&#24490;&#29289;&#20307;&#36793;&#30028;&#19988;&#20256;&#36798;&#20016;&#23500;&#35270;&#35273;&#31526;&#21495;&#30340;&#36229;&#20687;&#32032;&#65292;&#29305;&#21035;&#26159;&#24403;&#20132;&#21449;&#34920;&#38754;&#39068;&#33394;&#30456;&#20851;&#24615;&#21487;&#33021;&#24178;&#25200;&#29289;&#20307;&#26102;&#12290;&#21463;&#31070;&#32463;&#32467;&#26500;&#21644;&#35270;&#35273;&#26426;&#21046;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29983;&#29289;&#32593;&#32476;&#26550;&#26500;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#22686;&#24378;&#31579;&#36873;&#27169;&#22359;&#65288;ESM&#65289;&#21644;&#19968;&#20010;&#26032;&#39062;&#30340;&#36793;&#30028;&#24863;&#30693;&#26631;&#31614;&#65288;BAL&#65289;&#65292;&#29992;&#20110;&#36229;&#20687;&#32032;&#20998;&#21106;&#12290;ESM&#36890;&#36807;&#27169;&#25311;&#35270;&#35273;&#30382;&#36136;&#30340;&#20132;&#20114;&#24335;&#25237;&#24433;&#26426;&#21046;&#26469;&#22686;&#24378;&#35821;&#20041;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;BAL&#27169;&#25311;&#20102;&#35270;&#35273;&#30382;&#36136;&#32454;&#32990;&#30340;&#31354;&#38388;&#39057;&#29575;&#29305;&#24615;&#65292;&#20197;&#20419;&#36827;&#29983;&#25104;&#20855;&#26377;&#24378;&#36793;&#30028;&#36981;&#24490;&#24615;&#30340;&#36229;&#20687;&#32032;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;BSDS500&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#26469;&#35777;&#26126;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, advancements in deep learning-based superpixel segmentation methods have brought about improvements in both the efficiency and the performance of segmentation. However, a significant challenge remains in generating superpixels that strictly adhere to object boundaries while conveying rich visual significance, especially when cross-surface color correlations may interfere with objects. Drawing inspiration from neural structure and visual mechanisms, we propose a biological network architecture comprising an Enhanced Screening Module (ESM) and a novel Boundary-Aware Label (BAL) for superpixel segmentation. The ESM enhances semantic information by simulating the interactive projection mechanisms of the visual cortex. Additionally, the BAL emulates the spatial frequency characteristics of visual cortical cells to facilitate the generation of superpixels with strong boundary adherence. We demonstrate the effectiveness of our approach through evaluations on both the BSDS500 dataset
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;LPML&#65292;&#19968;&#31181;&#29992;&#20110;&#25968;&#23398;&#25512;&#29702;&#30340;LLM&#25552;&#31034;&#26631;&#35760;&#35821;&#35328;&#12290;&#36890;&#36807;&#23558;Chain-of-Thought&#26041;&#27861;&#21644;Python REPL&#19982;&#35813;&#26631;&#35760;&#35821;&#35328;&#32467;&#21512;&#65292;&#25105;&#20204;&#33021;&#22815;&#25511;&#21046;LLM&#29983;&#25104;&#25991;&#26412;&#20013;&#30340;&#38169;&#35823;&#65292;&#24182;&#22686;&#24378;&#20854;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#23454;&#29616;&#21033;&#29992;Python&#35745;&#31639;&#32416;&#27491;&#38169;&#35823;&#21644;&#35299;&#20915;&#25361;&#25112;&#24615;&#25968;&#23398;&#38382;&#39064;&#65292;&#32780;&#21482;&#38656;&#35201;&#38646;&#26679;&#26412;&#25552;&#31034;&#12290;</title><link>http://arxiv.org/abs/2309.13078</link><description>&lt;p&gt;
LPML: &#25968;&#23398;&#25512;&#29702;&#30340;LLM&#25552;&#31034;&#26631;&#35760;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
LPML: LLM-Prompting Markup Language for Mathematical Reasoning. (arXiv:2309.13078v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13078
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;LPML&#65292;&#19968;&#31181;&#29992;&#20110;&#25968;&#23398;&#25512;&#29702;&#30340;LLM&#25552;&#31034;&#26631;&#35760;&#35821;&#35328;&#12290;&#36890;&#36807;&#23558;Chain-of-Thought&#26041;&#27861;&#21644;Python REPL&#19982;&#35813;&#26631;&#35760;&#35821;&#35328;&#32467;&#21512;&#65292;&#25105;&#20204;&#33021;&#22815;&#25511;&#21046;LLM&#29983;&#25104;&#25991;&#26412;&#20013;&#30340;&#38169;&#35823;&#65292;&#24182;&#22686;&#24378;&#20854;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#23454;&#29616;&#21033;&#29992;Python&#35745;&#31639;&#32416;&#27491;&#38169;&#35823;&#21644;&#35299;&#20915;&#25361;&#25112;&#24615;&#25968;&#23398;&#38382;&#39064;&#65292;&#32780;&#21482;&#38656;&#35201;&#38646;&#26679;&#26412;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#25968;&#23398;&#25512;&#29702;&#26102;&#65292;&#35299;&#20915;LLMs&#29983;&#25104;&#25991;&#26412;&#20013;&#30340;&#25512;&#29702;&#21644;&#35745;&#31639;&#38169;&#35823;&#26159;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#23558;Chain-of-Thought&#65288;CoT&#65289;&#26041;&#27861;&#19982;&#22806;&#37096;&#24037;&#20855;&#65288;Python REPL&#65289;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36890;&#36807;&#25552;&#31034;LLMs&#29983;&#25104;&#31867;&#20284;XML&#26631;&#35760;&#35821;&#35328;&#30340;&#32467;&#26500;&#21270;&#25991;&#26412;&#65292;&#25105;&#20204;&#21487;&#20197;&#26080;&#32541;&#22320;&#38598;&#25104;CoT&#21644;&#22806;&#37096;&#24037;&#20855;&#65292;&#24182;&#25511;&#21046;LLMs&#30340;&#19981;&#33391;&#34892;&#20026;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;LLMs&#21487;&#20197;&#21033;&#29992;Python&#35745;&#31639;&#26469;&#32416;&#27491;CoT&#20013;&#30340;&#38169;&#35823;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;ChatGPT&#65288;GPT-3.5&#65289;&#26469;&#35299;&#20915;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#23398;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#36890;&#36807;&#26631;&#35760;&#35821;&#35328;&#23558;CoT&#21644;Python REPL&#32467;&#21512;&#36215;&#26469;&#21487;&#20197;&#22686;&#24378;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;LLMs&#33021;&#22815;&#20351;&#29992;&#38646;&#26679;&#26412;&#25552;&#31034;&#32534;&#20889;&#26631;&#35760;&#35821;&#35328;&#65292;&#24182;&#36827;&#34892;&#39640;&#32423;&#25968;&#23398;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
In utilizing large language models (LLMs) for mathematical reasoning, addressing the errors in the reasoning and calculation present in the generated text by LLMs is a crucial challenge. In this paper, we propose a novel framework that integrates the Chain-of-Thought (CoT) method with an external tool (Python REPL). We discovered that by prompting LLMs to generate structured text in XML-like markup language, we could seamlessly integrate CoT and the external tool and control the undesired behaviors of LLMs. With our approach, LLMs can utilize Python computation to rectify errors within CoT. We applied our method to ChatGPT (GPT-3.5) to solve challenging mathematical problems and demonstrated that combining CoT and Python REPL through the markup language enhances the reasoning capability of LLMs. Our approach enables LLMs to write the markup language and perform advanced mathematical reasoning using only zero-shot prompting.
&lt;/p&gt;</description></item><item><title>DiscoverPath&#26159;&#19968;&#20010;&#22522;&#20110;&#30693;&#35782;&#22270;&#30340;&#29983;&#29289;&#21307;&#23398;&#30740;&#31350;&#35770;&#25991;&#25628;&#32034;&#24341;&#25806;&#65292;&#36890;&#36807;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#35789;&#24615;&#26631;&#27880;&#20174;&#25991;&#31456;&#25688;&#35201;&#20013;&#25552;&#21462;&#26415;&#35821;&#21644;&#20851;&#31995;&#65292;&#24182;&#23637;&#31034;&#32473;&#29992;&#25143;&#19968;&#20010;&#20851;&#27880;&#26597;&#35810;&#23454;&#20307;&#21450;&#20854;&#37051;&#36817;&#33410;&#28857;&#30340;&#23376;&#22270;&#65292;&#20197;&#21450;&#26597;&#35810;&#25512;&#33616;&#31995;&#32479;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#24490;&#24207;&#28176;&#36827;&#22320;&#32454;&#21270;&#26597;&#35810;&#12290;</title><link>http://arxiv.org/abs/2309.01808</link><description>&lt;p&gt;
DiscoverPath&#65306;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#30740;&#31350;&#30340;&#30693;&#35782;&#32454;&#21270;&#21644;&#26816;&#32034;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
DiscoverPath: A Knowledge Refinement and Retrieval System for Interdisciplinarity on Biomedical Research. (arXiv:2309.01808v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01808
&lt;/p&gt;
&lt;p&gt;
DiscoverPath&#26159;&#19968;&#20010;&#22522;&#20110;&#30693;&#35782;&#22270;&#30340;&#29983;&#29289;&#21307;&#23398;&#30740;&#31350;&#35770;&#25991;&#25628;&#32034;&#24341;&#25806;&#65292;&#36890;&#36807;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#35789;&#24615;&#26631;&#27880;&#20174;&#25991;&#31456;&#25688;&#35201;&#20013;&#25552;&#21462;&#26415;&#35821;&#21644;&#20851;&#31995;&#65292;&#24182;&#23637;&#31034;&#32473;&#29992;&#25143;&#19968;&#20010;&#20851;&#27880;&#26597;&#35810;&#23454;&#20307;&#21450;&#20854;&#37051;&#36817;&#33410;&#28857;&#30340;&#23376;&#22270;&#65292;&#20197;&#21450;&#26597;&#35810;&#25512;&#33616;&#31995;&#32479;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#24490;&#24207;&#28176;&#36827;&#22320;&#32454;&#21270;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#26415;&#20986;&#29256;&#29289;&#30340;&#25351;&#25968;&#22686;&#38271;&#38656;&#35201;&#39640;&#32423;&#24037;&#20855;&#26469;&#23454;&#29616;&#39640;&#25928;&#30340;&#25991;&#31456;&#26816;&#32034;&#65292;&#23588;&#20854;&#22312;&#36328;&#23398;&#31185;&#39046;&#22495;&#20013;&#65292;&#19981;&#21516;&#30340;&#26415;&#35821;&#34987;&#29992;&#26469;&#25551;&#36848;&#30456;&#20284;&#30340;&#30740;&#31350;&#12290;&#20256;&#32479;&#30340;&#22522;&#20110;&#20851;&#38190;&#35789;&#30340;&#25628;&#32034;&#24341;&#25806;&#24448;&#24448;&#26080;&#27861;&#24110;&#21161;&#37027;&#20123;&#23545;&#29305;&#23450;&#26415;&#35821;&#19981;&#29087;&#24713;&#30340;&#29992;&#25143;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#22270;&#30340;&#29983;&#29289;&#21307;&#23398;&#30740;&#31350;&#35770;&#25991;&#25628;&#32034;&#24341;&#25806;&#65292;&#20197;&#22686;&#24378;&#29992;&#25143;&#22312;&#21457;&#29616;&#30456;&#20851;&#26597;&#35810;&#21644;&#25991;&#31456;&#26041;&#38754;&#30340;&#20307;&#39564;&#12290;&#35813;&#31995;&#32479;&#34987;&#31216;&#20026;DiscoverPath&#65292;&#37319;&#29992;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#21644;&#35789;&#24615;&#26631;&#27880;&#65288;POS&#65289;&#26469;&#20174;&#25991;&#31456;&#25688;&#35201;&#20013;&#25552;&#21462;&#26415;&#35821;&#21644;&#20851;&#31995;&#65292;&#21019;&#24314;&#30693;&#35782;&#22270;&#35889;&#12290;&#20026;&#20102;&#20943;&#23569;&#20449;&#24687;&#36229;&#36733;&#65292;DiscoverPath&#32473;&#29992;&#25143;&#23637;&#31034;&#20102;&#19968;&#20010;&#20851;&#27880;&#26597;&#35810;&#23454;&#20307;&#21450;&#20854;&#37051;&#36817;&#33410;&#28857;&#30340;&#23376;&#22270;&#65292;&#24182;&#19988;&#36824;&#32467;&#21512;&#20102;&#26597;&#35810;&#25512;&#33616;&#31995;&#32479;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#24490;&#24207;&#28176;&#36827;&#22320;&#32454;&#21270;&#26597;&#35810;&#12290;&#35813;&#31995;&#32479;&#37197;&#22791;&#20102;&#19968;&#20010;&#26131;&#20110;&#35775;&#38382;&#30340;&#22270;&#24418;&#29992;&#25143;&#30028;&#38754;&#65288;GUI&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
The exponential growth in scholarly publications necessitates advanced tools for efficient article retrieval, especially in interdisciplinary fields where diverse terminologies are used to describe similar research. Traditional keyword-based search engines often fall short in assisting users who may not be familiar with specific terminologies. To address this, we present a knowledge graph-based paper search engine for biomedical research to enhance the user experience in discovering relevant queries and articles. The system, dubbed DiscoverPath, employs Named Entity Recognition (NER) and part-of-speech (POS) tagging to extract terminologies and relationships from article abstracts to create a KG. To reduce information overload, DiscoverPath presents users with a focused subgraph containing the queried entity and its neighboring nodes and incorporates a query recommendation system, enabling users to iteratively refine their queries. The system is equipped with an accessible Graphical Us
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#21453;&#28436;&#36807;&#31243;&#65288;ZIP&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#22270;&#20687;&#23646;&#24615;&#32534;&#36753;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#29983;&#25104;&#30340;&#35270;&#35273;&#21442;&#32771;&#21644;&#25991;&#26412;&#24341;&#23548;&#27880;&#20837;&#25193;&#25955;&#27169;&#22411;&#30340;&#35821;&#20041;&#28508;&#31354;&#38388;&#65292;&#21487;&#20197;&#22312;&#25991;&#26412;&#25552;&#31034;&#30340;&#30452;&#35266;&#25511;&#21046;&#19979;&#20135;&#29983;&#22810;&#26679;&#30340;&#20869;&#23481;&#21644;&#23646;&#24615;&#65292;&#24182;&#23637;&#29616;&#20986;&#23545;&#19981;&#21516;&#23646;&#24615;&#25805;&#20316;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.15854</link><description>&lt;p&gt;
&#22270;&#20687;&#23646;&#24615;&#32534;&#36753;&#30340;&#38646;&#26679;&#26412;&#21453;&#28436;&#36807;&#31243;&#19982;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Zero-shot Inversion Process for Image Attribute Editing with Diffusion Models. (arXiv:2308.15854v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15854
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#21453;&#28436;&#36807;&#31243;&#65288;ZIP&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#22270;&#20687;&#23646;&#24615;&#32534;&#36753;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#29983;&#25104;&#30340;&#35270;&#35273;&#21442;&#32771;&#21644;&#25991;&#26412;&#24341;&#23548;&#27880;&#20837;&#25193;&#25955;&#27169;&#22411;&#30340;&#35821;&#20041;&#28508;&#31354;&#38388;&#65292;&#21487;&#20197;&#22312;&#25991;&#26412;&#25552;&#31034;&#30340;&#30452;&#35266;&#25511;&#21046;&#19979;&#20135;&#29983;&#22810;&#26679;&#30340;&#20869;&#23481;&#21644;&#23646;&#24615;&#65292;&#24182;&#23637;&#29616;&#20986;&#23545;&#19981;&#21516;&#23646;&#24615;&#25805;&#20316;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38477;&#22122;&#25193;&#25955;&#27169;&#22411;&#22312;&#22270;&#20687;&#32534;&#36753;&#20013;&#34920;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#20542;&#21521;&#20110;&#20351;&#29992;&#22270;&#20687;&#24341;&#23548;&#26041;&#27861;&#65292;&#25552;&#20379;&#35270;&#35273;&#21442;&#32771;&#20294;&#32570;&#20047;&#35821;&#20041;&#36830;&#36143;&#24615;&#30340;&#25511;&#21046;&#65292;&#25110;&#32773;&#20351;&#29992;&#25991;&#26412;&#24341;&#23548;&#26041;&#27861;&#65292;&#30830;&#20445;&#23545;&#25991;&#26412;&#24341;&#23548;&#30340;&#24544;&#23454;&#65292;&#20294;&#32570;&#20047;&#35270;&#35273;&#36136;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38646;&#26679;&#26412;&#21453;&#28436;&#36807;&#31243;&#65288;ZIP&#65289;&#26694;&#26550;&#65292;&#23427;&#23558;&#29983;&#25104;&#30340;&#35270;&#35273;&#21442;&#32771;&#21644;&#25991;&#26412;&#24341;&#23548;&#30340;&#34701;&#21512;&#27880;&#20837;&#21040;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#30340;&#35821;&#20041;&#28508;&#31354;&#38388;&#20013;&#12290;&#20165;&#20351;&#29992;&#19968;&#20010;&#24494;&#23567;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#25552;&#20986;&#30340;ZIP&#22312;&#25991;&#26412;&#25552;&#31034;&#30340;&#30452;&#35266;&#25511;&#21046;&#19979;&#20135;&#29983;&#22810;&#26679;&#30340;&#20869;&#23481;&#21644;&#23646;&#24615;&#12290;&#27492;&#22806;&#65292;ZIP&#22312;&#30495;&#23454;&#22270;&#20687;&#19978;&#23637;&#31034;&#20102;&#23545;&#22495;&#20869;&#21644;&#22495;&#22806;&#23646;&#24615;&#25805;&#20316;&#30340;&#26174;&#33879;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#23454;&#39564;&#12290;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;ZIP&#20135;&#29983;&#20102;&#19982;&#20043;&#30456;&#24403;&#36136;&#37327;&#30340;&#22270;&#20687;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#36924;&#30495;&#30340;&#32534;&#36753;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Denoising diffusion models have shown outstanding performance in image editing. Existing works tend to use either image-guided methods, which provide a visual reference but lack control over semantic coherence, or text-guided methods, which ensure faithfulness to text guidance but lack visual quality. To address the problem, we propose the Zero-shot Inversion Process (ZIP), a framework that injects a fusion of generated visual reference and text guidance into the semantic latent space of a \textit{frozen} pre-trained diffusion model. Only using a tiny neural network, the proposed ZIP produces diverse content and attributes under the intuitive control of the text prompt. Moreover, ZIP shows remarkable robustness for both in-domain and out-of-domain attribute manipulation on real images. We perform detailed experiments on various benchmark datasets. Compared to state-of-the-art methods, ZIP produces images of equivalent quality while providing a realistic editing effect.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#26333;&#20809;&#20559;&#24046;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Epsilon Scaling&#30340;&#20813;&#35757;&#32451;&#26041;&#27861;&#26469;&#20943;&#36731;&#36825;&#19968;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.15321</link><description>&lt;p&gt;
&#38416;&#26126;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#26333;&#20809;&#20559;&#24046;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Elucidating the Exposure Bias in Diffusion Models. (arXiv:2308.15321v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15321
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#26333;&#20809;&#20559;&#24046;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Epsilon Scaling&#30340;&#20813;&#35757;&#32451;&#26041;&#27861;&#26469;&#20943;&#36731;&#36825;&#19968;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#30340;&#8220;&#26333;&#20809;&#20559;&#24046;&#8221;&#38382;&#39064;&#65292;&#21363;&#35757;&#32451;&#21644;&#37319;&#26679;&#20043;&#38388;&#30340;&#36755;&#20837;&#19981;&#21305;&#37197;&#65292;&#32570;&#20047;&#28145;&#20837;&#25506;&#32034;&#12290;&#26412;&#25991;&#36890;&#36807;&#39318;&#20808;&#23545;&#37319;&#26679;&#20998;&#24067;&#36827;&#34892;&#20998;&#26512;&#24314;&#27169;&#65292;&#28982;&#21518;&#23558;&#27599;&#20010;&#37319;&#26679;&#27493;&#39588;&#30340;&#39044;&#27979;&#35823;&#24046;&#24402;&#22240;&#20026;&#26333;&#20809;&#20559;&#24046;&#38382;&#39064;&#30340;&#26681;&#26412;&#21407;&#22240;&#65292;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#26333;&#20809;&#20559;&#24046;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#28508;&#22312;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#30452;&#35266;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#38500;&#20102;&#38416;&#26126;&#26333;&#20809;&#20559;&#24046;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#20813;&#35757;&#32451;&#26041;&#27861;&#65292;&#31216;&#20026;Epsilon Scaling&#65292;&#20197;&#20943;&#36731;&#26333;&#20809;&#20559;&#24046;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;Epsilon Scaling&#36890;&#36807;&#32553;&#23567;&#32593;&#32476;&#36755;&#20986;&#65288;Epsilon&#65289;&#26126;&#30830;&#22320;&#23558;&#37319;&#26679;&#36712;&#36857;&#31227;&#36817;&#35757;&#32451;&#38454;&#27573;&#23398;&#20064;&#21040;&#30340;&#21521;&#37327;&#22330;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#35757;&#32451;&#21644;&#37319;&#26679;&#20043;&#38388;&#30340;&#36755;&#20837;&#19981;&#21305;&#37197;&#12290;&#22312;&#21508;&#31181;&#25193;&#25955;&#26694;&#26550;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have demonstrated impressive generative capabilities, but their 'exposure bias' problem, described as the input mismatch between training and sampling, lacks in-depth exploration. In this paper, we systematically investigate the exposure bias problem in diffusion models by first analytically modelling the sampling distribution, based on which we then attribute the prediction error at each sampling step as the root cause of the exposure bias issue. Furthermore, we discuss potential solutions to this issue and propose an intuitive metric for it. Along with the elucidation of exposure bias, we propose a simple, yet effective, training-free method called Epsilon Scaling to alleviate the exposure bias. We show that Epsilon Scaling explicitly moves the sampling trajectory closer to the vector field learned in the training phase by scaling down the network output (Epsilon), mitigating the input mismatch between training and sampling. Experiments on various diffusion framework
&lt;/p&gt;</description></item><item><title>InstructionGPT-4&#36890;&#36807;&#20165;&#20351;&#29992;200&#20010;&#20363;&#23376;&#36827;&#34892;&#24494;&#35843;&#65292;&#22312;&#22810;&#27169;&#24335;&#25351;&#20196;&#25968;&#25454;&#36136;&#37327;&#24230;&#37327;&#21644;&#36873;&#25321;&#22120;&#30340;&#24110;&#21161;&#19979;&#65292;&#22312;&#21508;&#31181;&#35780;&#20272;&#20219;&#21153;&#20013;&#20248;&#20110;&#21407;&#22987;&#30340;MiniGPT-4&#12290;</title><link>http://arxiv.org/abs/2308.12067</link><description>&lt;p&gt;
InstructionGPT-4: &#19968;&#20010;200&#25351;&#20196;&#33539;&#24335;&#29992;&#20110;&#24494;&#35843;MiniGPT-4
&lt;/p&gt;
&lt;p&gt;
InstructionGPT-4: A 200-Instruction Paradigm for Fine-Tuning MiniGPT-4. (arXiv:2308.12067v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12067
&lt;/p&gt;
&lt;p&gt;
InstructionGPT-4&#36890;&#36807;&#20165;&#20351;&#29992;200&#20010;&#20363;&#23376;&#36827;&#34892;&#24494;&#35843;&#65292;&#22312;&#22810;&#27169;&#24335;&#25351;&#20196;&#25968;&#25454;&#36136;&#37327;&#24230;&#37327;&#21644;&#36873;&#25321;&#22120;&#30340;&#24110;&#21161;&#19979;&#65292;&#22312;&#21508;&#31181;&#35780;&#20272;&#20219;&#21153;&#20013;&#20248;&#20110;&#21407;&#22987;&#30340;MiniGPT-4&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#20004;&#38454;&#27573;&#30340;&#35757;&#32451;&#36807;&#31243;&#33719;&#21462;&#20854;&#36981;&#24490;&#25351;&#20196;&#30340;&#33021;&#21147;&#65306;&#22312;&#22270;&#20687;-&#25991;&#26412;&#23545;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#30417;&#30563;&#24335;&#35270;&#35273;-&#35821;&#35328;&#25351;&#20196;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21363;&#20351;&#21482;&#26377;&#26377;&#38480;&#37327;&#30340;&#39640;&#36136;&#37327;&#36981;&#24490;&#25351;&#20196;&#25968;&#25454;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20063;&#33021;&#21462;&#24471;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;InstructionGPT-4&#65292;&#23427;&#32463;&#36807;&#24494;&#35843;&#30340;&#25968;&#25454;&#38598;&#21482;&#21253;&#21547;200&#20010;&#20363;&#23376;&#65292;&#32422;&#21344;MiniGPT-4&#23545;&#40784;&#25968;&#25454;&#38598;&#20013;&#20351;&#29992;&#30340;&#36981;&#24490;&#25351;&#20196;&#25968;&#25454;&#30340;6%&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#20960;&#20010;&#29992;&#20110;&#35780;&#20272;&#22810;&#27169;&#24335;&#25351;&#20196;&#25968;&#25454;&#36136;&#37327;&#30340;&#24230;&#37327;&#25351;&#26631;&#12290;&#22522;&#20110;&#36825;&#20123;&#24230;&#37327;&#25351;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25968;&#25454;&#36873;&#25321;&#22120;&#65292;&#33258;&#21160;&#35782;&#21035;&#21644;&#36807;&#28388;&#20302;&#36136;&#37327;&#30340;&#35270;&#35273;-&#35821;&#35328;&#25968;&#25454;&#12290;&#36890;&#36807;&#37319;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;InstructionGPT-4&#22312;&#21508;&#31181;&#35780;&#20272;&#65288;&#22914;&#35270;&#35273;&#38382;&#31572;&#12289;GPT-4&#20559;&#22909;&#65289;&#19978;&#20248;&#20110;&#21407;&#22987;&#30340;MiniGPT-4&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;...
&lt;/p&gt;
&lt;p&gt;
Multimodal large language models acquire their instruction-following capabilities through a two-stage training process: pre-training on image-text pairs and fine-tuning on supervised vision-language instruction data. Recent studies have shown that large language models can achieve satisfactory results even with a limited amount of high-quality instruction-following data. In this paper, we introduce InstructionGPT-4, which is fine-tuned on a small dataset comprising only 200 examples, amounting to approximately 6% of the instruction-following data used in the alignment dataset for MiniGPT-4. We first propose several metrics to access the quality of multimodal instruction data. Based on these metrics, we present a simple and effective data selector to automatically identify and filter low-quality vision-language data. By employing this method, InstructionGPT-4 outperforms the original MiniGPT-4 on various evaluations (e.g., visual question answering, GPT-4 preference). Overall, our findi
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#26657;&#20934;&#30340;&#22522;&#20934;&#30740;&#31350;&#65292;&#21033;&#29992;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#31354;&#38388;&#25506;&#32034;&#20102;&#27169;&#22411;&#26657;&#20934;&#23646;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#27169;&#22411;&#26657;&#20934;&#21487;&#20197;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#27867;&#21270;&#65292;&#24182;&#21487;&#20197;&#21516;&#26102;&#20860;&#39038;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#26657;&#20934;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.11838</link><description>&lt;p&gt;
&#19968;&#20010;&#20851;&#20110;&#26657;&#20934;&#30340;&#22522;&#20934;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Benchmark Study on Calibration. (arXiv:2308.11838v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11838
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#26657;&#20934;&#30340;&#22522;&#20934;&#30740;&#31350;&#65292;&#21033;&#29992;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#31354;&#38388;&#25506;&#32034;&#20102;&#27169;&#22411;&#26657;&#20934;&#23646;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#27169;&#22411;&#26657;&#20934;&#21487;&#20197;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#27867;&#21270;&#65292;&#24182;&#21487;&#20197;&#21516;&#26102;&#20860;&#39038;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#26657;&#20934;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#24191;&#27867;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#36825;&#20123;&#27169;&#22411;&#22797;&#26434;&#24615;&#30340;&#22686;&#21152;&#65292;&#23427;&#20204;&#24448;&#24448;&#38754;&#20020;&#26657;&#20934;&#38382;&#39064;&#65292;&#23613;&#31649;&#39044;&#27979;&#20934;&#30830;&#24615;&#26377;&#25152;&#25552;&#39640;&#12290;&#35768;&#22810;&#30740;&#31350;&#36890;&#36807;&#25968;&#25454;&#39044;&#22788;&#29702;&#12289;&#20351;&#29992;&#29305;&#23450;&#25439;&#22833;&#20989;&#25968;&#21644;&#35757;&#32451;&#26694;&#26550;&#26469;&#25913;&#21892;&#26657;&#20934;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23545;&#26657;&#20934;&#23646;&#24615;&#30340;&#30740;&#31350;&#26377;&#28857;&#34987;&#24573;&#35270;&#20102;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21033;&#29992;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#25628;&#32034;&#31354;&#38388;&#65292;&#22312;&#20840;&#38754;&#25506;&#32034;&#26657;&#20934;&#23646;&#24615;&#30340;&#27169;&#22411;&#26550;&#26500;&#31354;&#38388;&#20013;&#25552;&#20379;&#20102;&#19968;&#20010;&#35814;&#23613;&#30340;&#27169;&#22411;&#26550;&#26500;&#31354;&#38388;&#12290;&#25105;&#20204;&#29305;&#21035;&#21019;&#24314;&#20102;&#19968;&#20010;&#27169;&#22411;&#26657;&#20934;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;NATS-Bench&#25628;&#32034;&#31354;&#38388;&#20013;&#35780;&#20272;&#20102;90&#20010;&#22522;&#20110;&#21306;&#38388;&#30340;&#26657;&#20934;&#24230;&#37327;&#21644;12&#20010;&#20854;&#20182;&#26657;&#20934;&#24230;&#37327;&#65292;&#28085;&#30422;&#20102;117,702&#20010;&#29420;&#29305;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#26088;&#22312;&#36890;&#36807;&#25105;&#20204;&#25552;&#20986;&#30340;&#25968;&#25454;&#38598;&#22238;&#31572;&#35813;&#39046;&#22495;&#19968;&#20123;&#38271;&#26399;&#23384;&#22312;&#30340;&#38382;&#39064;&#65306;&#65288;i&#65289;&#27169;&#22411;&#26657;&#20934;&#33021;&#21542;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#27867;&#21270;&#65311;&#65288;ii&#65289;&#33021;&#21542;&#21516;&#26102;&#20860;&#39038;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#26657;&#20934;&#24615;&#33021;&#65311;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks are increasingly utilized in various machine learning tasks. However, as these models grow in complexity, they often face calibration issues, despite enhanced prediction accuracy. Many studies have endeavored to improve calibration performance through data preprocessing, the use of specific loss functions, and training frameworks. Yet, investigations into calibration properties have been somewhat overlooked. Our study leverages the Neural Architecture Search (NAS) search space, offering an exhaustive model architecture space for thorough calibration properties exploration. We specifically create a model calibration dataset. This dataset evaluates 90 bin-based and 12 additional calibration measurements across 117,702 unique neural networks within the widely employed NATS-Bench search space. Our analysis aims to answer several longstanding questions in the field, using our proposed dataset: (i) Can model calibration be generalized across different tasks? (ii) Can rob
&lt;/p&gt;</description></item><item><title>PokerKit&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;Python&#24211;&#65292;&#29992;&#20110;&#32454;&#31890;&#24230;&#22810;&#21464;&#20307;&#25169;&#20811;&#28216;&#25103;&#27169;&#25311;&#65292;&#25552;&#20379;&#24191;&#27867;&#30340;&#25169;&#20811;&#21464;&#20307;&#25903;&#25345;&#21644;&#28789;&#27963;&#30340;&#28216;&#25103;&#29366;&#24577;&#25511;&#21046;&#65292;&#23545;&#25169;&#20811;AI&#24320;&#21457;&#12289;&#24037;&#20855;&#21019;&#24314;&#21644;&#22312;&#32447;&#25169;&#20811;&#36172;&#22330;&#23454;&#29616;&#31561;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2308.07327</link><description>&lt;p&gt;
PokerKit: &#19968;&#31181;&#29992;&#20110;&#32454;&#31890;&#24230;&#22810;&#21464;&#20307;&#25169;&#20811;&#28216;&#25103;&#27169;&#25311;&#30340;&#20840;&#38754;&#30340;Python&#24211;
&lt;/p&gt;
&lt;p&gt;
PokerKit: A Comprehensive Python Library for Fine-Grained Multi-Variant Poker Game Simulations. (arXiv:2308.07327v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07327
&lt;/p&gt;
&lt;p&gt;
PokerKit&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;Python&#24211;&#65292;&#29992;&#20110;&#32454;&#31890;&#24230;&#22810;&#21464;&#20307;&#25169;&#20811;&#28216;&#25103;&#27169;&#25311;&#65292;&#25552;&#20379;&#24191;&#27867;&#30340;&#25169;&#20811;&#21464;&#20307;&#25903;&#25345;&#21644;&#28789;&#27963;&#30340;&#28216;&#25103;&#29366;&#24577;&#25511;&#21046;&#65292;&#23545;&#25169;&#20811;AI&#24320;&#21457;&#12289;&#24037;&#20855;&#21019;&#24314;&#21644;&#22312;&#32447;&#25169;&#20811;&#36172;&#22330;&#23454;&#29616;&#31561;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
PokerKit&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;Python&#24211;&#65292;&#26088;&#22312;&#20811;&#26381;&#29616;&#26377;&#25169;&#20811;&#28216;&#25103;&#27169;&#25311;&#21644;&#25163;&#29260;&#35780;&#20272;&#24037;&#20855;&#30340;&#38480;&#21046;&#65292;&#36825;&#20123;&#24037;&#20855;&#36890;&#24120;&#21482;&#25903;&#25345;&#23569;&#37327;&#25169;&#20811;&#21464;&#20307;&#65292;&#24182;&#19988;&#22312;&#28216;&#25103;&#29366;&#24577;&#25511;&#21046;&#26041;&#38754;&#32570;&#20047;&#28789;&#27963;&#24615;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;PokerKit&#36890;&#36807;&#25903;&#25345;&#24191;&#27867;&#30340;&#25169;&#20811;&#21464;&#20307;&#65292;&#24182;&#25552;&#20379;&#28789;&#27963;&#30340;&#26550;&#26500;&#20379;&#29992;&#25143;&#23450;&#20041;&#33258;&#23450;&#20041;&#28216;&#25103;&#65292;&#26174;&#33879;&#25193;&#22823;&#20102;&#36825;&#19968;&#33539;&#22260;&#12290;&#26412;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;PokerKit&#30340;&#35774;&#35745;&#21644;&#23454;&#29616;&#65292;&#21253;&#25324;&#20854;&#30452;&#35266;&#30340;&#32534;&#31243;API&#65292;&#22810;&#21464;&#20307;&#28216;&#25103;&#25903;&#25345;&#20197;&#21450;&#32479;&#19968;&#30340;&#25163;&#29260;&#35780;&#20272;&#22871;&#20214;&#22312;&#19981;&#21516;&#25163;&#29260;&#31867;&#22411;&#38388;&#30340;&#24212;&#29992;&#12290;PokerKit&#30340;&#28789;&#27963;&#24615;&#20351;&#20854;&#33021;&#22815;&#22312;&#25169;&#20811;AI&#24320;&#21457;&#12289;&#24037;&#20855;&#21019;&#24314;&#21644;&#22312;&#32447;&#25169;&#20811;&#36172;&#22330;&#23454;&#29616;&#31561;&#22810;&#20010;&#39046;&#22495;&#20013;&#20351;&#29992;&#12290;PokerKit&#30340;&#21487;&#38752;&#24615;&#36890;&#36807;&#38745;&#24577;&#31867;&#22411;&#26816;&#26597;&#12289;&#24191;&#27867;&#30340;doctest&#21644;&#21333;&#20803;&#27979;&#35797;&#26469;&#30830;&#20445;&#65292;&#36798;&#21040;&#20102;97%&#30340;&#20195;&#30721;&#35206;&#30422;&#29575;&#12290;&#24341;&#20837;PokerKit&#20195;&#34920;&#20102;&#23545;&#35813;&#39046;&#22495;&#30340;&#37325;&#35201;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
PokerKit is an open-source Python library designed to overcome the restrictions of existing poker game simulation and hand evaluation tools, which typically support only a handful of poker variants and lack flexibility in game state control. In contrast, PokerKit significantly expands this scope by supporting an extensive array of poker variants and it provides a flexible architecture for users to define their custom games. This paper details the design and implementation of PokerKit, including its intuitive programmatic API, multi-variant game support, and a unified hand evaluation suite across different hand types. The flexibility of PokerKit allows for applications in diverse areas, such as poker AI development, tool creation, and online poker casino implementation. PokerKit's reliability has been established through static type checking, extensive doctests, and unit tests, achieving 97\% code coverage. The introduction of PokerKit represents a significant contribution to the field 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;ChatReport&#30340;&#22522;&#20110;LLM&#30340;&#31995;&#32479;&#65292;&#23427;&#36890;&#36807;&#23454;&#29616;&#21487;&#36861;&#28335;&#30340;&#31572;&#26696;&#21644;&#35299;&#20915;&#39046;&#22495;&#19987;&#23478;&#21442;&#19982;&#20302;&#25928;&#24615;&#30340;&#38382;&#39064;&#65292;&#26088;&#22312;&#36890;&#36807;&#33258;&#21160;&#20998;&#26512;&#20225;&#19994;&#21487;&#25345;&#32493;&#24615;&#25253;&#21578;&#65292;&#23454;&#29616;&#21487;&#25345;&#32493;&#24615;&#25259;&#38706;&#20998;&#26512;&#27665;&#20027;&#21270;&#12290;</title><link>http://arxiv.org/abs/2307.15770</link><description>&lt;p&gt;
CHATREPORT&#65306;&#36890;&#36807;&#22522;&#20110;LLM&#24037;&#20855;&#23454;&#29616;&#21487;&#25345;&#32493;&#24615;&#25259;&#38706;&#20998;&#26512;&#30340;&#27665;&#20027;&#21270;
&lt;/p&gt;
&lt;p&gt;
CHATREPORT: Democratizing Sustainability Disclosure Analysis through LLM-based Tools. (arXiv:2307.15770v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15770
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;ChatReport&#30340;&#22522;&#20110;LLM&#30340;&#31995;&#32479;&#65292;&#23427;&#36890;&#36807;&#23454;&#29616;&#21487;&#36861;&#28335;&#30340;&#31572;&#26696;&#21644;&#35299;&#20915;&#39046;&#22495;&#19987;&#23478;&#21442;&#19982;&#20302;&#25928;&#24615;&#30340;&#38382;&#39064;&#65292;&#26088;&#22312;&#36890;&#36807;&#33258;&#21160;&#20998;&#26512;&#20225;&#19994;&#21487;&#25345;&#32493;&#24615;&#25253;&#21578;&#65292;&#23454;&#29616;&#21487;&#25345;&#32493;&#24615;&#25259;&#38706;&#20998;&#26512;&#27665;&#20027;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#23545;&#27668;&#20505;&#21464;&#21270;&#65292;&#20844;&#21496;&#30495;&#30340;&#22312;&#26397;&#30528;&#26356;&#21487;&#25345;&#32493;&#32463;&#33829;&#36808;&#20986;&#23454;&#36136;&#24615;&#30340;&#27493;&#20240;&#21527;&#65311;&#19968;&#20010;&#20840;&#38754;&#30340;&#31572;&#26696;&#21487;&#20197;&#22312;&#20225;&#19994;&#21487;&#25345;&#32493;&#24615;&#25253;&#21578;&#30340;&#23494;&#38598;&#20449;&#24687;&#20013;&#25214;&#21040;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25253;&#21578;&#30340;&#25968;&#37327;&#21644;&#22797;&#26434;&#24615;&#20351;&#20154;&#24037;&#20998;&#26512;&#25104;&#26412;&#38750;&#24120;&#39640;&#26114;&#12290;&#22240;&#27492;&#65292;&#21482;&#26377;&#23569;&#25968;&#30340;&#26426;&#26500;&#25317;&#26377;&#36164;&#28304;&#33021;&#22815;&#22823;&#35268;&#27169;&#20998;&#26512;&#36825;&#20123;&#25253;&#21578;&#65292;&#36825;&#23548;&#33268;&#21487;&#25345;&#32493;&#24615;&#25253;&#21578;&#32570;&#20047;&#36879;&#26126;&#24230;&#12290;&#36890;&#36807;&#22522;&#20110;LLM&#33258;&#21160;&#20998;&#26512;&#24037;&#20855;&#36171;&#33021;&#21033;&#30410;&#30456;&#20851;&#32773;&#21487;&#33021;&#26159;&#23454;&#29616;&#21487;&#25345;&#32493;&#24615;&#25253;&#21578;&#20998;&#26512;&#27665;&#20027;&#21270;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;&#24320;&#21457;&#36825;&#26679;&#30340;&#24037;&#20855;&#38754;&#20020;&#25361;&#25112;&#65292;&#20027;&#35201;&#21407;&#22240;&#26159;LLM&#30340;&#24187;&#35273;&#38382;&#39064;&#21644;&#23558;&#39046;&#22495;&#19987;&#23478;&#24341;&#20837;AI&#24320;&#21457;&#36807;&#31243;&#30340;&#20302;&#25928;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;ChatReport&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#26032;&#22411;&#31995;&#32479;&#65292;&#29992;&#20110;&#33258;&#21160;&#21270;&#20998;&#26512;&#20225;&#19994;&#21487;&#25345;&#32493;&#24615;&#25253;&#21578;&#65292;&#36890;&#36807;&#20351;&#31572;&#26696;&#21487;&#36861;&#28335;&#26469;&#20943;&#23569;&#24187;&#35273;&#30340;&#21361;&#23475;&#65292;&#24182;&#35299;&#20915;&#39046;&#22495;&#19987;&#23478;&#21442;&#19982;AI&#24320;&#21457;&#36807;&#31243;&#30340;&#20302;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the face of climate change, are companies really taking substantial steps toward more sustainable operations? A comprehensive answer lies in the dense, information-rich landscape of corporate sustainability reports. However, the sheer volume and complexity of these reports make human analysis very costly. Therefore, only a few entities worldwide have the resources to analyze these reports at scale, which leads to a lack of transparency in sustainability reporting. Empowering stakeholders with LLM-based automatic analysis tools can be a promising way to democratize sustainability report analysis. However, developing such tools is challenging due to (1) the hallucination of LLMs and (2) the inefficiency of bringing domain experts into the AI development loop. In this paper, we ChatReport, a novel LLM-based system to automate the analysis of corporate sustainability reports, addressing existing challenges by (1) making the answers traceable to reduce the harm of hallucination and (2) a
&lt;/p&gt;</description></item><item><title>&#24377;&#24615;&#20915;&#31574;&#21464;&#21387;&#22120;&#65288;EDT&#65289;&#36890;&#36807;&#22312;&#27979;&#35797;&#26102;&#38388;&#36827;&#34892;&#21160;&#20316;&#25512;&#26029;&#26102;&#35843;&#25972;&#21382;&#21490;&#38271;&#24230;&#26469;&#23454;&#29616;&#36712;&#36857;&#25340;&#25509;&#65292;&#22635;&#34917;&#20102;&#20915;&#31574;&#21464;&#21387;&#22120;&#65288;DT&#65289;&#22312;&#36825;&#19968;&#26041;&#38754;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#24182;&#19988;&#22312;&#22810;&#20219;&#21153;&#24773;&#20917;&#19979;&#32988;&#36807;&#22522;&#20110;Q-Learning&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.02484</link><description>&lt;p&gt;
&#24377;&#24615;&#20915;&#31574;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
Elastic Decision Transformer. (arXiv:2307.02484v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02484
&lt;/p&gt;
&lt;p&gt;
&#24377;&#24615;&#20915;&#31574;&#21464;&#21387;&#22120;&#65288;EDT&#65289;&#36890;&#36807;&#22312;&#27979;&#35797;&#26102;&#38388;&#36827;&#34892;&#21160;&#20316;&#25512;&#26029;&#26102;&#35843;&#25972;&#21382;&#21490;&#38271;&#24230;&#26469;&#23454;&#29616;&#36712;&#36857;&#25340;&#25509;&#65292;&#22635;&#34917;&#20102;&#20915;&#31574;&#21464;&#21387;&#22120;&#65288;DT&#65289;&#22312;&#36825;&#19968;&#26041;&#38754;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#24182;&#19988;&#22312;&#22810;&#20219;&#21153;&#24773;&#20917;&#19979;&#32988;&#36807;&#22522;&#20110;Q-Learning&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#24377;&#24615;&#20915;&#31574;&#21464;&#21387;&#22120;&#65288;EDT&#65289;&#65292;&#23427;&#26159;&#29616;&#26377;&#20915;&#31574;&#21464;&#21387;&#22120;&#65288;DT&#65289;&#21450;&#20854;&#21464;&#20307;&#30340;&#37325;&#22823;&#36827;&#23637;&#12290;&#23613;&#31649;DT&#22768;&#31216;&#33021;&#22815;&#29983;&#25104;&#26368;&#20339;&#36712;&#36857;&#65292;&#20294;&#23454;&#35777;&#35777;&#25454;&#34920;&#26126;&#23427;&#22312;&#36712;&#36857;&#25340;&#25509;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#36712;&#36857;&#25340;&#25509;&#26159;&#25351;&#20174;&#19968;&#32452;&#27425;&#20248;&#36712;&#36857;&#20013;&#29983;&#25104;&#26368;&#20248;&#25110;&#25509;&#36817;&#26368;&#20248;&#36712;&#36857;&#30340;&#36807;&#31243;&#12290;&#25552;&#20986;&#30340;EDT&#36890;&#36807;&#22312;&#27979;&#35797;&#26102;&#38388;&#36827;&#34892;&#21160;&#20316;&#25512;&#26029;&#26102;&#35843;&#25972;DT&#20013;&#32500;&#25252;&#30340;&#21382;&#21490;&#38271;&#24230;&#26469;&#23454;&#29616;&#36712;&#36857;&#25340;&#25509;&#65292;&#20174;&#32780;&#20351;&#33258;&#24049;&#19982;&#20247;&#19981;&#21516;&#12290;&#27492;&#22806;&#65292;&#24403;&#21069;&#36712;&#36857;&#26159;&#26368;&#20248;&#30340;&#26102;&#20505;&#65292;EDT&#36890;&#36807;&#20445;&#25345;&#36739;&#38271;&#30340;&#21382;&#21490;&#65292;&#24403;&#24403;&#21069;&#36712;&#36857;&#26159;&#27425;&#20248;&#30340;&#26102;&#20505;&#65292;EDT&#36890;&#36807;&#20445;&#25345;&#36739;&#30701;&#30340;&#21382;&#21490;&#26469;&#20248;&#21270;&#36712;&#36857;&#65292;&#20351;&#20854;&#33021;&#22815;&#19982;&#26356;&#20248;&#30340;&#36712;&#36857;&#36827;&#34892;&#8220;&#25340;&#25509;&#8221;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;EDT&#33021;&#22815;&#22635;&#34917;&#22522;&#20110;DT&#21644;&#22522;&#20110;Q-Learning&#26041;&#27861;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#29305;&#21035;&#26159;&#65292;EDT&#22312;&#22810;&#20219;&#21153;&#24773;&#20917;&#19979;&#32988;&#36807;&#22522;&#20110;Q-Learning&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces Elastic Decision Transformer (EDT), a significant advancement over the existing Decision Transformer (DT) and its variants. Although DT purports to generate an optimal trajectory, empirical evidence suggests it struggles with trajectory stitching, a process involving the generation of an optimal or near-optimal trajectory from the best parts of a set of sub-optimal trajectories. The proposed EDT differentiates itself by facilitating trajectory stitching during action inference at test time, achieved by adjusting the history length maintained in DT. Further, the EDT optimizes the trajectory by retaining a longer history when the previous trajectory is optimal and a shorter one when it is sub-optimal, enabling it to "stitch" with a more optimal trajectory. Extensive experimentation demonstrates EDT's ability to bridge the performance gap between DT-based and Q Learning-based approaches. In particular, the EDT outperforms Q Learning-based methods in a multi-task regi
&lt;/p&gt;</description></item><item><title>DifFSS&#26159;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#25913;&#36827;&#23567;&#26679;&#26412;&#35821;&#20041;&#20998;&#21106;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#36741;&#21161;&#25903;&#25345;&#22270;&#20687;&#65292;&#32780;&#19981;&#38656;&#35201;&#20462;&#25913;&#32593;&#32476;&#32467;&#26500;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#26368;&#20808;&#36827;&#30340;&#23567;&#26679;&#26412;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.00773</link><description>&lt;p&gt;
DifFSS: &#19968;&#31181;&#29992;&#20110;&#23567;&#26679;&#26412;&#35821;&#20041;&#20998;&#21106;&#30340;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DifFSS: Diffusion Model for Few-Shot Semantic Segmentation. (arXiv:2307.00773v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00773
&lt;/p&gt;
&lt;p&gt;
DifFSS&#26159;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#25913;&#36827;&#23567;&#26679;&#26412;&#35821;&#20041;&#20998;&#21106;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#36741;&#21161;&#25903;&#25345;&#22270;&#20687;&#65292;&#32780;&#19981;&#38656;&#35201;&#20462;&#25913;&#32593;&#32476;&#32467;&#26500;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#26368;&#20808;&#36827;&#30340;&#23567;&#26679;&#26412;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#22270;&#20687;&#29983;&#25104;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#32467;&#26500;&#19981;&#21516;&#30340;&#23567;&#26679;&#26412;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#65292;&#20294;&#26159;&#24615;&#33021;&#30340;&#25913;&#36827;&#24050;&#32463;&#36798;&#21040;&#20102;&#29942;&#39048;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39318;&#27425;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#23567;&#26679;&#26412;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;DifFSS&#12290;DifFSS&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#23567;&#26679;&#26412;&#35821;&#20041;&#20998;&#21106;&#33539;&#24335;&#65292;&#21487;&#20197;&#22312;&#19981;&#20462;&#25913;&#32593;&#32476;&#32467;&#26500;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#26368;&#20808;&#36827;&#30340;&#23567;&#26679;&#26412;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#24378;&#22823;&#29983;&#25104;&#33021;&#21147;&#65292;&#36890;&#36807;&#20351;&#29992;&#35821;&#20041;&#25513;&#30721;&#12289;&#28034;&#25273;&#25110;&#36719;HED&#36793;&#30028;&#30340;&#25903;&#25345;&#22270;&#20687;&#20316;&#20026;&#25511;&#21046;&#26465;&#20214;&#65292;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#36741;&#21161;&#25903;&#25345;&#22270;&#20687;&#12290;&#36825;&#20010;&#29983;&#25104;&#36807;&#31243;&#27169;&#25311;&#20102;&#26597;&#35810;&#22270;&#20687;&#31867;&#21035;&#20869;&#30340;&#22810;&#26679;&#24615;&#65292;&#22914;&#39068;&#33394;&#12289;&#32441;&#29702;&#21464;&#21270;&#21644;&#20809;&#29031;&#31561;&#12290;&#22240;&#27492;&#65292;&#23567;&#26679;&#26412;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#21487;&#20197;&#21442;&#32771;&#26356;&#22810;&#22810;&#26679;&#30340;&#25903;&#25345;&#22270;&#20687;&#65292;&#20135;&#29983;&#26356;&#20855;&#40065;&#26834;&#24615;&#30340;&#34920;&#31034;&#65292;&#20174;&#32780;&#23454;&#29616;&#19968;&#33268;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have demonstrated excellent performance in image generation. Although various few-shot semantic segmentation (FSS) models with different network structures have been proposed, performance improvement has reached a bottleneck. This paper presents the first work to leverage the diffusion model for FSS task, called DifFSS. DifFSS, a novel FSS paradigm, can further improve the performance of the state-of-the-art FSS models by a large margin without modifying their network structure. Specifically, we utilize the powerful generation ability of diffusion models to generate diverse auxiliary support images by using the semantic mask, scribble or soft HED boundary of the support image as control conditions. This generation process simulates the variety within the class of the query image, such as color, texture variation, lighting, $etc$. As a result, FSS models can refer to more diverse support images, yielding more robust representations, thereby achieving a consistent improv
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38382;&#39064;&#35774;&#32622;&#65306;&#24341;&#29992;&#20154;&#31867;&#33310;&#36424;&#29983;&#25104;&#12290;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#33310;&#36424;&#22330;&#26223;&#20013;&#65292;&#36890;&#36807;&#35299;&#32806;&#25511;&#21046;&#26469;&#35299;&#20915;&#33310;&#36424;&#21512;&#25104;&#20013;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#24544;&#23454;&#24615;&#12289;&#27867;&#21270;&#33021;&#21147;&#21644;&#32452;&#21512;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.00040</link><description>&lt;p&gt;
DisCo: &#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#20013;&#22522;&#20110;&#20154;&#31867;&#33310;&#36424;&#30340;&#24341;&#29992;&#29983;&#25104;&#30340;&#35299;&#32806;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
DisCo: Disentangled Control for Referring Human Dance Generation in Real World. (arXiv:2307.00040v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00040
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38382;&#39064;&#35774;&#32622;&#65306;&#24341;&#29992;&#20154;&#31867;&#33310;&#36424;&#29983;&#25104;&#12290;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#33310;&#36424;&#22330;&#26223;&#20013;&#65292;&#36890;&#36807;&#35299;&#32806;&#25511;&#21046;&#26469;&#35299;&#20915;&#33310;&#36424;&#21512;&#25104;&#20013;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#24544;&#23454;&#24615;&#12289;&#27867;&#21270;&#33021;&#21147;&#21644;&#32452;&#21512;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;AI&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#29305;&#21035;&#26159;&#22312;&#22522;&#20110;&#25991;&#26412;&#25551;&#36848;&#30340;&#22270;&#20687;/&#35270;&#39057;&#21512;&#25104;&#26041;&#38754;&#12290;&#23613;&#31649;&#26377;&#20102;&#36825;&#20123;&#36827;&#27493;&#65292;&#20294;&#22312;&#29983;&#25104;&#20154;&#31867;&#20013;&#24515;&#20869;&#23481;&#65288;&#22914;&#33310;&#36424;&#21512;&#25104;&#65289;&#26041;&#38754;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#33310;&#36424;&#21512;&#25104;&#26041;&#27861;&#22312;&#21512;&#25104;&#20869;&#23481;&#19982;&#29616;&#23454;&#19990;&#30028;&#33310;&#36424;&#22330;&#26223;&#20043;&#38388;&#23384;&#22312;&#24046;&#36317;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#26032;&#30340;&#38382;&#39064;&#35774;&#32622;&#65306;&#24341;&#29992;&#20154;&#31867;&#33310;&#36424;&#29983;&#25104;&#65292;&#37325;&#28857;&#20851;&#27880;&#20855;&#26377;&#19977;&#20010;&#37325;&#35201;&#23646;&#24615;&#30340;&#29616;&#23454;&#19990;&#30028;&#33310;&#36424;&#22330;&#26223;&#65306;&#65288;i&#65289;&#24544;&#23454;&#24615;&#65306;&#21512;&#25104;&#24212;&#35813;&#20445;&#30041;&#24341;&#29992;&#22270;&#20687;&#20013;&#20154;&#31867;&#20027;&#20307;&#21069;&#26223;&#21644;&#32972;&#26223;&#30340;&#22806;&#35266;&#65292;&#24182;&#31934;&#30830;&#22320;&#36981;&#24490;&#30446;&#26631;&#23039;&#21183;&#65307;&#65288;ii&#65289;&#27867;&#21270;&#33021;&#21147;&#65306;&#27169;&#22411;&#24212;&#35813;&#36866;&#29992;&#20110;&#26410;&#35265;&#36807;&#30340;&#20154;&#31867;&#20027;&#20307;&#12289;&#32972;&#26223;&#21644;&#23039;&#21183;&#65307;&#65288;iii&#65289;&#32452;&#21512;&#24615;&#65306;&#23427;&#24212;&#35813;&#20801;&#35768;&#26469;&#33258;&#19981;&#21516;&#26469;&#28304;&#30340;&#24050;&#35265;/&#26410;&#35265;&#20027;&#20307;&#12289;&#32972;&#26223;&#21644;&#23039;&#21183;&#30340;&#32452;&#21512;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;D
&lt;/p&gt;
&lt;p&gt;
Generative AI has made significant strides in computer vision, particularly in image/video synthesis conditioned on text descriptions. Despite the advancements, it remains challenging especially in the generation of human-centric content such as dance synthesis. Existing dance synthesis methods struggle with the gap between synthesized content and real-world dance scenarios. In this paper, we define a new problem setting: Referring Human Dance Generation, which focuses on real-world dance scenarios with three important properties: (i) Faithfulness: the synthesis should retain the appearance of both human subject foreground and background from the reference image, and precisely follow the target pose; (ii) Generalizability: the model should generalize to unseen human subjects, backgrounds, and poses; (iii) Compositionality: it should allow for composition of seen/unseen subjects, backgrounds, and poses from different sources. To address these challenges, we introduce a novel approach, D
&lt;/p&gt;</description></item><item><title>DCdetector&#26159;&#19968;&#31181;&#22810;&#23610;&#24230;&#21452;&#37325;&#20851;&#27880;&#30340;&#23545;&#27604;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#12290;&#23427;&#21033;&#29992;&#21452;&#37325;&#20851;&#27880;&#19981;&#23545;&#31216;&#35774;&#35745;&#21644;&#32431;&#23545;&#27604;&#25439;&#22833;&#23398;&#20064;&#32622;&#25442;&#19981;&#21464;&#34920;&#31034;&#65292;&#20174;&#32780;&#26377;&#25928;&#21306;&#20998;&#24322;&#24120;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2306.10347</link><description>&lt;p&gt;
DCdetector: &#21452;&#37325;&#20851;&#27880;&#23545;&#27604;&#34920;&#31034;&#23398;&#20064;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
DCdetector: Dual Attention Contrastive Representation Learning for Time Series Anomaly Detection. (arXiv:2306.10347v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10347
&lt;/p&gt;
&lt;p&gt;
DCdetector&#26159;&#19968;&#31181;&#22810;&#23610;&#24230;&#21452;&#37325;&#20851;&#27880;&#30340;&#23545;&#27604;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#12290;&#23427;&#21033;&#29992;&#21452;&#37325;&#20851;&#27880;&#19981;&#23545;&#31216;&#35774;&#35745;&#21644;&#32431;&#23545;&#27604;&#25439;&#22833;&#23398;&#20064;&#32622;&#25442;&#19981;&#21464;&#34920;&#31034;&#65292;&#20174;&#32780;&#26377;&#25928;&#21306;&#20998;&#24322;&#24120;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#23545;&#20110;&#35768;&#22810;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#20854;&#30446;&#26631;&#26159;&#22312;&#26102;&#38388;&#24207;&#21015;&#20013;&#35782;&#21035;&#20986;&#19982;&#27491;&#24120;&#26679;&#26412;&#20998;&#24067;&#26377;&#24046;&#24322;&#30340;&#24322;&#24120;&#26679;&#26412;&#12290;&#36825;&#20010;&#20219;&#21153;&#30340;&#26368;&#22522;&#26412;&#25361;&#25112;&#26159;&#23398;&#20064;&#19968;&#20010;&#33021;&#22815;&#26377;&#25928;&#21306;&#20998;&#24322;&#24120;&#30340;&#34920;&#31034;&#26144;&#23556;&#12290;&#22522;&#20110;&#37325;&#24314;&#30340;&#26041;&#27861;&#20173;&#28982;&#20027;&#23548;&#30528;&#35813;&#39046;&#22495;&#65292;&#20294;&#26159;&#20351;&#29992;&#24322;&#24120;&#25968;&#25454;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;&#21487;&#33021;&#20250;&#23545;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#23545;&#27604;&#23398;&#20064;&#26088;&#22312;&#25214;&#21040;&#19968;&#31181;&#33021;&#22815;&#26126;&#26174;&#21306;&#20998;&#20219;&#20309;&#23454;&#20363;&#30340;&#34920;&#31034;&#65292;&#36825;&#21487;&#20197;&#20026;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#25552;&#20379;&#26356;&#33258;&#28982;&#21644;&#26377;&#21069;&#26223;&#30340;&#34920;&#31034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DCdetector&#65292;&#19968;&#31181;&#22810;&#23610;&#24230;&#21452;&#37325;&#20851;&#27880;&#30340;&#23545;&#27604;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#12290;DCdetector&#21033;&#29992;&#26032;&#39062;&#30340;&#21452;&#37325;&#20851;&#27880;&#19981;&#23545;&#31216;&#35774;&#35745;&#21019;&#24314;&#32622;&#25442;&#29615;&#22659;&#65292;&#24182;&#20351;&#29992;&#32431;&#23545;&#27604;&#25439;&#22833;&#26469;&#24341;&#23548;&#23398;&#20064;&#36807;&#31243;&#65292;&#20174;&#32780;&#23398;&#20064;&#19968;&#20010;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#30340;&#32622;&#25442;&#19981;&#21464;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series anomaly detection is critical for a wide range of applications. It aims to identify deviant samples from the normal sample distribution in time series. The most fundamental challenge for this task is to learn a representation map that enables effective discrimination of anomalies. Reconstruction-based methods still dominate, but the representation learning with anomalies might hurt the performance with its large abnormal loss. On the other hand, contrastive learning aims to find a representation that can clearly distinguish any instance from the others, which can bring a more natural and promising representation for time series anomaly detection. In this paper, we propose DCdetector, a multi-scale dual attention contrastive representation learning model. DCdetector utilizes a novel dual attention asymmetric design to create the permutated environment and pure contrastive loss to guide the learning process, thus learning a permutation invariant representation with superior d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32418;&#38431;&#34892;&#21160;&#65292;&#36890;&#36807;&#20174;&#39640;&#23618;&#27425;&#12289;&#25277;&#35937;&#30340;&#35268;&#33539;&#20986;&#21457;&#26469;&#32771;&#34385;&#35821;&#35328;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#20197;&#25506;&#31350;&#27169;&#22411;&#30340;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2306.09442</link><description>&lt;p&gt;
&#20174;&#38646;&#24320;&#22987;&#23454;&#29616;&#32418;&#38431;&#23545;&#25239;&#35821;&#35328;&#27169;&#22411;&#30340;&#25506;&#32034;&#19982;&#24314;&#31435;
&lt;/p&gt;
&lt;p&gt;
Explore, Establish, Exploit: Red Teaming Language Models from Scratch. (arXiv:2306.09442v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09442
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32418;&#38431;&#34892;&#21160;&#65292;&#36890;&#36807;&#20174;&#39640;&#23618;&#27425;&#12289;&#25277;&#35937;&#30340;&#35268;&#33539;&#20986;&#21457;&#26469;&#32771;&#34385;&#35821;&#35328;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#20197;&#25506;&#31350;&#27169;&#22411;&#30340;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#32626;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#33021;&#20250;&#20135;&#29983;&#26377;&#23475;&#36755;&#20986;&#65292;&#20363;&#22914;&#26377;&#27602;&#25110;&#19981;&#35802;&#23454;&#38472;&#36848;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#24341;&#20837;&#20102;&#24037;&#20855;&#20197;&#35843;&#26597;&#26377;&#23475;&#36755;&#20986;&#65292;&#20197;&#35782;&#21035;&#21644;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#12290;&#34429;&#28982;&#36825;&#26159;&#30830;&#20445;&#35821;&#35328;&#27169;&#22411;&#23433;&#20840;&#30340;&#26377;&#20215;&#20540;&#27493;&#39588;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#29616;&#26377;&#30340;&#38024;&#23545;&#19981;&#24076;&#26395;&#30340;&#36755;&#20986;&#30340;&#20998;&#31867;&#22120;&#12290;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#21482;&#26377;&#39044;&#20808;&#30693;&#36947;&#26377;&#23475;&#34892;&#20026;&#31867;&#22411;&#30340;&#24773;&#20917;&#19979;&#30340;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#36339;&#36807;&#20102;&#32418;&#38431;&#34892;&#21160;&#30340;&#26680;&#24515;&#25361;&#25112;&#65306;&#24320;&#21457;&#27169;&#22411;&#21487;&#33021;&#23637;&#31034;&#30340;&#34892;&#20026;&#30340;&#19978;&#19979;&#25991;&#29702;&#35299;&#12290;&#27492;&#22806;&#65292;&#24403;&#36825;&#26679;&#30340;&#20998;&#31867;&#22120;&#24050;&#32463;&#23384;&#22312;&#26102;&#65292;&#32418;&#38431;&#34892;&#21160;&#30340;&#36793;&#38469;&#20215;&#20540;&#26377;&#38480;&#65292;&#22240;&#20026;&#20998;&#31867;&#22120;&#21487;&#20197;&#29992;&#20110;&#36807;&#28388;&#35757;&#32451;&#25968;&#25454;&#25110;&#27169;&#22411;&#36755;&#20986;&#12290;&#26412;&#25991;&#32771;&#34385;&#22312;&#20551;&#35774;&#23545;&#25163;&#20174;&#39640;&#32423;&#12289;&#25277;&#35937;&#30340;&#19981;&#33391;&#34892;&#20026;&#35268;&#33539;&#20986;&#21457;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#32418;&#38431;&#34892;&#21160;&#12290;&#32418;&#38431;&#24212;&#35813;&#22312;&#31934;&#21270;/&#25193;&#23637;&#27492;&#35268;&#33539;&#30340;&#21516;&#26102;&#23545;&#25239;&#35813;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deploying Large language models (LLMs) can pose hazards from harmful outputs such as toxic or dishonest speech. Prior work has introduced tools that elicit harmful outputs in order to identify and mitigate these risks. While this is a valuable step toward securing language models, these approaches typically rely on a pre-existing classifier for undesired outputs. This limits their application to situations where the type of harmful behavior is known with precision beforehand. However, this skips a central challenge of red teaming: developing a contextual understanding of the behaviors that a model can exhibit. Furthermore, when such a classifier already exists, red teaming has limited marginal value because the classifier could simply be used to filter training data or model outputs. In this work, we consider red teaming under the assumption that the adversary is working from a high-level, abstract specification of undesired behavior. The red team is expected to refine/extend this spec
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#20854;&#32467;&#21512;&#20102;&#25968;&#25454;&#20013;&#32534;&#30721;&#30340;&#26102;&#38388;&#21160;&#24577;&#65292;&#33258;&#28982;&#22320;&#32534;&#30721;&#20102;&#22810;&#27493;&#21644;&#38271;&#31243;&#39044;&#27979;&#33021;&#21147;&#65292;&#20855;&#26377;&#28789;&#27963;&#30340;&#37319;&#26679;&#36712;&#36857;&#21644;&#25240;&#34935;&#24615;&#33021;&#19982;&#21152;&#36895;&#37319;&#26679;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#65292;&#21487;&#22312;&#26102;&#31354;&#39044;&#27979;&#26041;&#38754;&#21462;&#24471;&#31454;&#20105;&#24615;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.01984</link><description>&lt;p&gt;
DYffusion&#65306;&#38754;&#21521;&#26102;&#31354;&#39044;&#27979;&#30340;&#21160;&#24577;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DYffusion: A Dynamics-informed Diffusion Model for Spatiotemporal Forecasting. (arXiv:2306.01984v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01984
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#20854;&#32467;&#21512;&#20102;&#25968;&#25454;&#20013;&#32534;&#30721;&#30340;&#26102;&#38388;&#21160;&#24577;&#65292;&#33258;&#28982;&#22320;&#32534;&#30721;&#20102;&#22810;&#27493;&#21644;&#38271;&#31243;&#39044;&#27979;&#33021;&#21147;&#65292;&#20855;&#26377;&#28789;&#27963;&#30340;&#37319;&#26679;&#36712;&#36857;&#21644;&#25240;&#34935;&#24615;&#33021;&#19982;&#21152;&#36895;&#37319;&#26679;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#65292;&#21487;&#22312;&#26102;&#31354;&#39044;&#27979;&#26041;&#38754;&#21462;&#24471;&#31454;&#20105;&#24615;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#25104;&#21151;&#22320;&#29983;&#25104;&#25968;&#25454;&#21644;&#20570;&#20986;&#39044;&#27979;&#65292;&#20294;&#23427;&#20204;&#20027;&#35201;&#26159;&#20026;&#38745;&#24577;&#22270;&#20687;&#35774;&#35745;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#35757;&#32451;&#29992;&#20110;&#21160;&#24577;&#39044;&#27979;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#21033;&#29992;&#32534;&#30721;&#22312;&#25968;&#25454;&#20013;&#30340;&#26102;&#38388;&#21160;&#24577;&#65292;&#30452;&#25509;&#23558;&#20854;&#19982;&#32593;&#32476;&#20013;&#30340;&#25193;&#25955;&#27493;&#39588;&#32806;&#21512;&#12290;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#38543;&#26426;&#30340;&#12289;&#26102;&#38388;&#26465;&#20214;&#30340;&#25554;&#20540;&#22120;&#21644;&#19968;&#20010;&#39592;&#24178;&#39044;&#27979;&#32593;&#32476;&#65292;&#20998;&#21035;&#27169;&#20223;&#20256;&#32479;&#25193;&#25955;&#27169;&#22411;&#30340;&#21069;&#21521;&#21644;&#21518;&#21521;&#36807;&#31243;&#12290;&#36825;&#31181;&#35774;&#35745;&#36873;&#25321;&#33258;&#28982;&#22320;&#32534;&#30721;&#20102;&#22810;&#27493;&#21644;&#38271;&#31243;&#39044;&#27979;&#33021;&#21147;&#65292;&#20801;&#35768;&#39640;&#24230;&#28789;&#27963;&#30340;&#36830;&#32493;&#26102;&#38388;&#37319;&#26679;&#36712;&#36857;&#65292;&#24182;&#22312;&#25512;&#29702;&#26102;&#33021;&#22815;&#25240;&#34935;&#24615;&#33021;&#19982;&#21152;&#36895;&#37319;&#26679;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#38754;&#21521;&#21160;&#24577;&#30340;&#25193;&#25955;&#36807;&#31243;&#24378;&#21152;&#20102;&#24378;&#30340;&#24402;&#32435;&#20559;&#24046;&#65292;&#30456;&#27604;&#20256;&#32479;&#22522;&#20110;&#39640;&#26031;&#22122;&#22768;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27010;&#29575;&#28369;&#38634;&#39044;&#27979;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
While diffusion models can successfully generate data and make predictions, they are predominantly designed for static images. We propose an approach for training diffusion models for dynamics forecasting that leverages the temporal dynamics encoded in the data, directly coupling it with the diffusion steps in the network. We train a stochastic, time-conditioned interpolator and a backbone forecaster network that mimic the forward and reverse processes of conventional diffusion models, respectively. This design choice naturally encodes multi-step and long-range forecasting capabilities, allowing for highly flexible, continuous-time sampling trajectories and the ability to trade-off performance with accelerated sampling at inference time. In addition, the dynamics-informed diffusion process imposes a strong inductive bias, allowing for improved computational efficiency compared to traditional Gaussian noise-based diffusion models. Our approach performs competitively on probabilistic ski
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;"&#22240;&#26524;&#31354;&#38388;"&#30340;&#27010;&#24565;&#65292;&#26088;&#22312;&#20197;&#26607;&#23572;&#33707;&#25096;&#32599;&#22827;&#30340;&#27010;&#29575;&#27979;&#24230;&#20844;&#29702;&#21270;&#20026;&#36215;&#28857;&#65292;&#23454;&#29616;&#23545;&#22240;&#26524;&#20851;&#31995;&#30340;&#20844;&#29702;&#21270;&#65292;&#24182;&#25104;&#21151;&#22320;&#35299;&#20915;&#20102;&#29616;&#26377;&#26694;&#26550;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2305.17139</link><description>&lt;p&gt;
&#22240;&#26524;&#20851;&#31995;&#30340;&#27979;&#24230;&#35770;&#20844;&#29702;&#21270;
&lt;/p&gt;
&lt;p&gt;
A Measure-Theoretic Axiomatisation of Causality. (arXiv:2305.17139v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17139
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;"&#22240;&#26524;&#31354;&#38388;"&#30340;&#27010;&#24565;&#65292;&#26088;&#22312;&#20197;&#26607;&#23572;&#33707;&#25096;&#32599;&#22827;&#30340;&#27010;&#29575;&#27979;&#24230;&#20844;&#29702;&#21270;&#20026;&#36215;&#28857;&#65292;&#23454;&#29616;&#23545;&#22240;&#26524;&#20851;&#31995;&#30340;&#20844;&#29702;&#21270;&#65292;&#24182;&#25104;&#21151;&#22320;&#35299;&#20915;&#20102;&#29616;&#26377;&#26694;&#26550;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#20851;&#31995;&#26159;&#35768;&#22810;&#30740;&#31350;&#39046;&#22495;&#20013;&#30340;&#26680;&#24515;&#27010;&#24565;&#65292;&#20294;&#20173;&#28982;&#27809;&#26377;&#26222;&#36941;&#35748;&#21487;&#30340;&#22240;&#26524;&#20851;&#31995;&#20844;&#29702;&#21270;&#12290;&#25105;&#20204;&#23558;&#22240;&#26524;&#20851;&#31995;&#35270;&#20026;&#27010;&#29575;&#29702;&#35770;&#30340;&#25193;&#23637;&#65292;&#24182;&#20316;&#20026;&#30740;&#31350;&#22312;&#31995;&#32479;&#19978;&#24178;&#39044;&#26102;&#20250;&#21457;&#29983;&#20160;&#20040;&#30340;&#30740;&#31350;&#65292;&#24182;&#25552;&#35758;&#20197;&#26607;&#23572;&#33707;&#25096;&#32599;&#22827;&#30340;&#27010;&#29575;&#27979;&#24230;&#20844;&#29702;&#21270;&#20316;&#20026;&#22240;&#26524;&#20851;&#31995;&#20844;&#29702;&#21270;&#30340;&#36215;&#28857;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;"&#22240;&#26524;&#31354;&#38388;"&#30340;&#27010;&#24565;&#65292;&#21253;&#25324;&#19968;&#20010;&#27010;&#29575;&#31354;&#38388;&#21644;&#31216;&#20026;"&#22240;&#26524;&#26680;"&#30340;&#36716;&#31227;&#27010;&#29575;&#26680;&#30340;&#38598;&#21512;&#65292;&#29992;&#26469;&#32534;&#30721;&#35813;&#31354;&#38388;&#30340;&#22240;&#26524;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#19981;&#20165;&#22312;&#27979;&#24230;&#35770;&#19978;&#20005;&#26684;&#22320;&#22522;&#20110;&#65292;&#36824;&#25581;&#31034;&#20102;&#29616;&#26377;&#26694;&#26550;&#30340;&#38271;&#26399;&#38480;&#21046;&#65292;&#20363;&#22914;&#65292;&#24490;&#29615;&#12289;&#28508;&#22312;&#21464;&#37327;&#21644;&#38543;&#26426;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causality is a central concept in a wide range of research areas, yet there is still no universally agreed axiomatisation of causality. We view causality both as an extension of probability theory and as a study of \textit{what happens when one intervenes on a system}, and argue in favour of taking Kolmogorov's measure-theoretic axiomatisation of probability as the starting point towards an axiomatisation of causality. To that end, we propose the notion of a \textit{causal space}, consisting of a probability space along with a collection of transition probability kernels, called \textit{causal kernels}, that encode the causal information of the space. Our proposed framework is not only rigorously grounded in measure theory, but it also sheds light on long-standing limitations of existing frameworks including, for example, cycles, latent variables and stochastic processes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#27573;&#24490;&#29615;Transformer&#65288;SRformer&#65289;&#26469;&#20943;&#23569;&#35745;&#31639;/&#20869;&#23384;&#25104;&#26412;&#65292;&#24182;&#20351;&#29992;RAF&#23618;&#22788;&#29702;&#36328;&#27573;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#25552;&#39640;&#24207;&#21015;&#22788;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.16340</link><description>&lt;p&gt;
&#20998;&#27573;&#24490;&#29615;Transformer:&#19968;&#31181;&#39640;&#25928;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Segmented Recurrent Transformer: An Efficient Sequence-to-Sequence Model. (arXiv:2305.16340v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16340
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#27573;&#24490;&#29615;Transformer&#65288;SRformer&#65289;&#26469;&#20943;&#23569;&#35745;&#31639;/&#20869;&#23384;&#25104;&#26412;&#65292;&#24182;&#20351;&#29992;RAF&#23618;&#22788;&#29702;&#36328;&#27573;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#25552;&#39640;&#24207;&#21015;&#22788;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#35821;&#35328;&#21644;&#35270;&#35273;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#24207;&#21015;&#38271;&#24230;&#30340;&#22686;&#21152;&#65292;&#23427;&#20204;&#30340;&#35745;&#31639;&#25104;&#26412;&#21576;&#20108;&#27425;&#22686;&#38271;&#65292;&#20351;&#24471;&#23427;&#20204;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#24212;&#29992;&#20013;&#20351;&#29992;&#25104;&#20026;&#19981;&#21487;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#23558;&#25972;&#20010;&#24207;&#21015;&#21010;&#20998;&#25104;&#33509;&#24178;&#27573;&#12290;&#28982;&#21518;&#20351;&#29992;&#20855;&#26377;&#24490;&#29615;&#32467;&#26500;&#30340;&#31070;&#32463;&#20803;&#26469;&#32858;&#21512;&#36328;&#27573;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#23454;&#29616;&#20855;&#26377;&#36739;&#20302;&#35745;&#31639;/&#20869;&#23384;&#25104;&#26412;&#30340;&#24207;&#21015;&#22788;&#29702;&#33021;&#21147;&#27169;&#22411;&#12290;&#20026;&#20102;&#39564;&#35777;&#36825;&#20010;&#24819;&#27861;&#65292;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#20102;&#20351;&#29992;&#23616;&#37096;Attention&#26426;&#21046;&#23545;&#21333;&#20010;&#27573;&#30340;&#24433;&#21709;&#12290;&#28982;&#21518;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#27573;&#24490;&#29615;Transformer&#65288;SRformer&#65289;&#65292;&#23427;&#23558;&#20998;&#27573;Attention&#21644;&#24490;&#29615;Attention&#30456;&#32467;&#21512;&#12290;&#23427;&#20351;&#29992;&#24490;&#29615;accumulate and fire&#65288;RAF&#65289;&#23618;&#22312;&#30456;&#37051;&#27573;&#20043;&#38388;&#22788;&#29702;&#20449;&#24687;&#12290;&#36890;&#36807;&#26356;&#26032;key&#30340;&#20135;&#21697;&#26469;&#34917;&#20607;&#20943;&#23569;Attention&#31383;&#21475;&#38271;&#24230;&#20135;&#29983;&#30340;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers have shown dominant performance across a range of domains including language and vision. However, their computational cost grows quadratically with the sequence length, making their usage prohibitive for resource-constrained applications. To counter this, our approach is to divide the whole sequence into segments. The information across segments can then be aggregated using neurons with recurrence leveraging their inherent memory. Such an approach leads to models with sequential processing capability at a lower computation/memory cost. To investigate this idea, first, we examine the effects of using local attention mechanism on the individual segments. Then we propose a segmented recurrent transformer (SRformer) that combines segmented attention with recurrent attention. It uses recurrent accumulate and fire (RAF) layers to process information between consecutive segments. The loss caused by reducing the attention window length is compensated by updating the product of key
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;JEEBench&#65292;&#19968;&#20010;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;&#36890;&#36807;&#35780;&#20272;&#21508;&#31181;&#27169;&#22411;&#65292;&#32467;&#26524;&#26174;&#31034;&#30446;&#21069;&#26368;&#22909;&#30340;&#27169;&#22411;&#22312;&#35299;&#20915;&#38382;&#39064;&#26102;&#23384;&#22312;&#20195;&#25968;&#25805;&#20316;&#38169;&#35823;&#12289;&#25277;&#35937;&#27010;&#24565;&#36716;&#21270;&#19981;&#20934;&#30830;&#21644;&#38590;&#20197;&#26816;&#32034;&#30456;&#20851;&#27010;&#24565;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.15074</link><description>&lt;p&gt;
LLM&#20204;&#36827;&#27493;&#21040;&#20102;&#20160;&#20040;&#31243;&#24230;&#65311;&#19968;&#20010;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#35299;&#20915;&#22522;&#20934;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Have LLMs Advanced Enough? A Challenging Problem Solving Benchmark For Large Language Models. (arXiv:2305.15074v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15074
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;JEEBench&#65292;&#19968;&#20010;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;&#36890;&#36807;&#35780;&#20272;&#21508;&#31181;&#27169;&#22411;&#65292;&#32467;&#26524;&#26174;&#31034;&#30446;&#21069;&#26368;&#22909;&#30340;&#27169;&#22411;&#22312;&#35299;&#20915;&#38382;&#39064;&#26102;&#23384;&#22312;&#20195;&#25968;&#25805;&#20316;&#38169;&#35823;&#12289;&#25277;&#35937;&#27010;&#24565;&#36716;&#21270;&#19981;&#20934;&#30830;&#21644;&#38590;&#20197;&#26816;&#32034;&#30456;&#20851;&#27010;&#24565;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#37324;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29616;&#26377;&#30340;&#25512;&#29702;&#22522;&#20934;&#19978;&#30340;&#24615;&#33021;&#26174;&#33879;&#25552;&#39640;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;JEEBench&#65292;&#19968;&#20010;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;LLMs&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;&#25105;&#20204;&#20174;&#39640;&#31454;&#20105;&#30340;&#21360;&#24230;&#29702;&#24037;&#23398;&#38498;&#65288;IIT&#65289;JEE-Advanced&#32771;&#35797;&#20013;&#31934;&#36873;&#20986;&#20102;515&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#39044;&#24037;&#31243;&#25968;&#23398;&#12289;&#29289;&#29702;&#21644;&#21270;&#23398;&#38382;&#39064;&#12290;&#22312;&#36825;&#20010;&#22522;&#20934;&#20013;&#65292;&#38271;&#26399;&#25512;&#29702;&#21644;&#28145;&#20837;&#39046;&#22495;&#30693;&#35782;&#30340;&#36816;&#29992;&#23545;&#38382;&#39064;&#30340;&#35299;&#20915;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#23545;&#21508;&#31181;&#24320;&#28304;&#21644;&#19987;&#26377;&#27169;&#22411;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#65292;&#21363;&#20351;&#20351;&#29992;&#20102;&#33258;&#19968;&#33268;&#24615;&#12289;&#33258;&#25105;&#23436;&#21892;&#21644;&#24605;&#32500;&#38142;&#25552;&#31034;&#31561;&#25216;&#26415;&#65292;&#26368;&#39640;&#24615;&#33021;&#20063;&#19981;&#21040;40\%&#12290;&#26368;&#22909;&#30340;&#27169;&#22411;GPT-4&#30340;&#20856;&#22411;&#22833;&#36133;&#27169;&#24335;&#21253;&#25324;&#20195;&#25968;&#25805;&#20316;&#38169;&#35823;&#12289;&#23558;&#25277;&#35937;&#27010;&#24565;&#20934;&#30830;&#22320;&#36716;&#21270;&#20026;&#25968;&#23398;&#26041;&#31243;&#20197;&#21450;&#26080;&#27861;&#26816;&#32034;&#30456;&#20851;&#30340;&#39046;&#22495;&#29305;&#23450;&#27010;&#24565;&#12290;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#65292;&#20165;&#20165;&#36890;&#36807;&#36755;&#20837;&#25552;&#31034;&#19981;&#33021;&#35753;&#27169;&#22411;&#25104;&#21151;&#35299;&#20915;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The performance of large language models (LLMs) on existing reasoning benchmarks has significantly improved over the past years. In response, we present JEEBench, a considerably more challenging benchmark dataset for evaluating the problem solving abilities of LLMs. We curate 515 challenging pre-engineering mathematics, physics and chemistry problems from the highly competitive IIT JEE-Advanced exam. Long-horizon reasoning on top of deep in-domain knowledge is essential for solving problems in this benchmark. Our evaluation on various open-source and proprietary models reveals that the highest performance, even after using techniques like self-consistency, self-refinement and chain-of-thought prompting, is less than 40\%. The typical failure modes of GPT-4, the best model, are errors in algebraic manipulation, difficulty in grounding abstract concepts into mathematical equations accurately and failure in retrieving relevant domain-specific concepts. We also observe that by mere prompti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;FACTSCORE&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#23558;&#29983;&#25104;&#30340;&#20869;&#23481;&#20998;&#35299;&#20026;&#21407;&#23376;&#20107;&#23454;&#65292;&#24182;&#35745;&#31639;&#34987;&#21487;&#38752;&#30693;&#35782;&#28304;&#25903;&#25345;&#30340;&#27604;&#20363;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#38271;&#25991;&#26412;&#30340;&#20107;&#23454;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#20154;&#24037;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#21830;&#19994;&#35821;&#35328;&#27169;&#22411;&#20013;&#20165;&#26377;58%&#30340;ChatGPT&#20256;&#35760;&#36798;&#21040;&#20102;&#39640;&#27700;&#24179;&#30340;&#20107;&#23454;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#27169;&#22411;&#65292;&#21033;&#29992;&#26816;&#32034;&#21644;&#24378;&#35821;&#35328;&#27169;&#22411;&#20272;&#35745;FACTSCORE&#65292;&#35823;&#24046;&#29575;&#20302;&#20110;2%&#12290;</title><link>http://arxiv.org/abs/2305.14251</link><description>&lt;p&gt;
FActScore: &#23545;&#38271;&#25991;&#26412;&#29983;&#25104;&#20013;&#20107;&#23454;&#20934;&#30830;&#24615;&#30340;&#32454;&#31890;&#24230;&#21407;&#23376;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation. (arXiv:2305.14251v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14251
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;FACTSCORE&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#23558;&#29983;&#25104;&#30340;&#20869;&#23481;&#20998;&#35299;&#20026;&#21407;&#23376;&#20107;&#23454;&#65292;&#24182;&#35745;&#31639;&#34987;&#21487;&#38752;&#30693;&#35782;&#28304;&#25903;&#25345;&#30340;&#27604;&#20363;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#38271;&#25991;&#26412;&#30340;&#20107;&#23454;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#20154;&#24037;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#21830;&#19994;&#35821;&#35328;&#27169;&#22411;&#20013;&#20165;&#26377;58%&#30340;ChatGPT&#20256;&#35760;&#36798;&#21040;&#20102;&#39640;&#27700;&#24179;&#30340;&#20107;&#23454;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#27169;&#22411;&#65292;&#21033;&#29992;&#26816;&#32034;&#21644;&#24378;&#35821;&#35328;&#27169;&#22411;&#20272;&#35745;FACTSCORE&#65292;&#35823;&#24046;&#29575;&#20302;&#20110;2%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#38271;&#25991;&#26412;&#30340;&#20107;&#23454;&#24615;&#26159;&#19968;&#39033;&#26840;&#25163;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#65288;1&#65289;&#29983;&#25104;&#30340;&#20869;&#23481;&#36890;&#24120;&#21253;&#21547;&#25903;&#25345;&#21644;&#19981;&#25903;&#25345;&#30340;&#20449;&#24687;&#65292;&#20351;&#24471;&#20108;&#20803;&#21028;&#26029;&#36136;&#37327;&#19981;&#36275;&#65292;&#65288;2&#65289;&#20154;&#24037;&#35780;&#20272;&#32791;&#26102;&#19988;&#25104;&#26412;&#39640;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;FACTSCORE&#65292;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#23427;&#23558;&#29983;&#25104;&#20869;&#23481;&#20998;&#35299;&#20026;&#19968;&#31995;&#21015;&#21407;&#23376;&#20107;&#23454;&#65292;&#24182;&#35745;&#31639;&#34987;&#21487;&#38752;&#30693;&#35782;&#28304;&#25903;&#25345;&#30340;&#21407;&#23376;&#20107;&#23454;&#30340;&#30334;&#20998;&#27604;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#20154;&#24037;&#35780;&#20272;&#65292;&#24471;&#20986;&#20102;&#20960;&#20010;&#26368;&#20808;&#36827;&#21830;&#19994;&#35821;&#35328;&#27169;&#22411;&#65288;InstructGPT&#12289;ChatGPT&#21644;&#22686;&#24378;&#25552;&#21462;PerplexityAI&#65289;&#29983;&#25104;&#30340;&#20154;&#29289;&#20256;&#35760;&#30340;FACTSCORE&#65292;&#24182;&#25253;&#36947;&#20102;&#26032;&#30340;&#20998;&#26512;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;&#23545;&#20110;&#36825;&#26679;&#30340;&#32454;&#31890;&#24230;&#35780;&#20998;&#30340;&#38656;&#27714;&#65288;&#20363;&#22914;&#65292;ChatGPT&#21482;&#36798;&#21040;58%&#65289;&#12290;&#30001;&#20110;&#20154;&#24037;&#35780;&#20272;&#36153;&#26102;&#36153;&#21147;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#20351;&#29992;&#26816;&#32034;&#21644;&#24378;&#35821;&#35328;&#27169;&#22411;&#20272;&#35745;FACTSCORE&#30340;&#33258;&#21160;&#21270;&#27169;&#22411;&#65292;&#35823;&#24046;&#29575;&#19981;&#36229;&#36807;2%&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluating the factuality of long-form text generated by large language models (LMs) is non-trivial because (1) generations often contain a mixture of supported and unsupported pieces of information, making binary judgments of quality inadequate, and (2) human evaluation is time-consuming and costly. In this paper, we introduce FACTSCORE, a new evaluation that breaks a generation into a series of atomic facts and computes the percentage of atomic facts supported by a reliable knowledge source. We conduct an extensive human evaluation to obtain FACTSCOREs of people biographies generated by several state-of-the-art commercial LMs -- InstructGPT, ChatGPT, and the retrieval-augmented PerplexityAI -- and report new analysis demonstrating the need for such a fine-grained score (e.g., ChatGPT only achieves 58%). Since human evaluation is costly, we also introduce an automated model that estimates FACTSCORE using retrieval and a strong language model, with less than a 2% error rate. Finally, w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#32534;&#36753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38382;&#39064;&#12289;&#26041;&#27861;&#21644;&#26426;&#20250;&#65292;&#25552;&#20379;&#20102;&#20219;&#21153;&#23450;&#20041;&#21644;&#25361;&#25112;&#30340;&#27010;&#36848;&#12289;&#20808;&#36827;&#26041;&#27861;&#30340;&#23454;&#35777;&#20998;&#26512;&#65292;&#20197;&#21450;&#26500;&#24314;&#20102;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#36825;&#20123;&#32467;&#26524;&#26377;&#21161;&#20110;&#25913;&#36827;LLMs&#30340;&#32534;&#36753;&#25216;&#26415;&#65292;&#25552;&#39640;&#20854;&#25928;&#26524;&#21644;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.13172</link><description>&lt;p&gt;
&#32534;&#36753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#38382;&#39064;&#12289;&#26041;&#27861;&#21644;&#26426;&#20250;
&lt;/p&gt;
&lt;p&gt;
Editing Large Language Models: Problems, Methods, and Opportunities. (arXiv:2305.13172v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13172
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#32534;&#36753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38382;&#39064;&#12289;&#26041;&#27861;&#21644;&#26426;&#20250;&#65292;&#25552;&#20379;&#20102;&#20219;&#21153;&#23450;&#20041;&#21644;&#25361;&#25112;&#30340;&#27010;&#36848;&#12289;&#20808;&#36827;&#26041;&#27861;&#30340;&#23454;&#35777;&#20998;&#26512;&#65292;&#20197;&#21450;&#26500;&#24314;&#20102;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#36825;&#20123;&#32467;&#26524;&#26377;&#21161;&#20110;&#25913;&#36827;LLMs&#30340;&#32534;&#36753;&#25216;&#26415;&#65292;&#25552;&#39640;&#20854;&#25928;&#26524;&#21644;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#33021;&#22815;&#35757;&#32451;&#20986;&#34920;&#29616;&#20248;&#31168;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#20294;&#20854;&#20445;&#25345;&#30456;&#20851;&#24615;&#21644;&#32416;&#27491;&#38169;&#35823;&#30340;&#26041;&#27861;&#20173;&#28982;&#38590;&#20197;&#30830;&#23450;&#12290;&#20026;&#27492;&#65292;&#26368;&#36817;&#20960;&#24180;&#20986;&#29616;&#20102;&#35768;&#22810;&#32534;&#36753;LLMs&#30340;&#25216;&#26415;&#65292;&#20854;&#30446;&#26631;&#26159;&#22312;&#29305;&#23450;&#39046;&#22495;&#20869;&#39640;&#25928;&#22320;&#25913;&#21464;LLMs&#30340;&#34892;&#20026;&#65292;&#21516;&#26102;&#19981;&#23545;&#20854;&#20182;&#36755;&#20837;&#30340;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#19982;LLMs&#27169;&#22411;&#32534;&#36753;&#30456;&#20851;&#30340;&#38382;&#39064;&#12289;&#26041;&#27861;&#21644;&#26426;&#20250;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#27169;&#22411;&#32534;&#36753;&#20219;&#21153;&#23450;&#20041;&#21644;&#30456;&#20851;&#25361;&#25112;&#30340;&#20840;&#38754;&#27010;&#36848;&#65292;&#20197;&#21450;&#23545;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30340;&#28145;&#20837;&#23454;&#35777;&#20998;&#26512;&#12290;&#25105;&#20204;&#36824;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20197;&#20419;&#36827;&#26356;&#24378;&#22823;&#30340;&#35780;&#20272;&#65292;&#24182;&#25351;&#20986;&#29616;&#26377;&#25216;&#26415;&#22266;&#26377;&#30340;&#25345;&#20037;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20026;&#27599;&#31181;&#32534;&#36753;&#25216;&#26415;&#30340;&#25928;&#26524;&#21644;&#21487;&#34892;&#24615;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#65292;&#20174;&#32780;&#24110;&#21161;&#31038;&#21306;&#22312;LLMs&#30340;&#31649;&#29702;&#20013;&#21462;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the ability to train capable LLMs, the methodology for maintaining their relevancy and rectifying errors remains elusive. To this end, the past few years have witnessed a surge in techniques for editing LLMs, the objective of which is to efficiently alter the behavior of LLMs within a specific domain without negatively impacting performance across other inputs. This paper embarks on a deep exploration of the problems, methods, and opportunities related to model editing for LLMs. In particular, we provide an exhaustive overview of the task definition and challenges associated with model editing, along with an in-depth empirical analysis of the most progressive methods currently at our disposal. We also build a new benchmark dataset to facilitate a more robust evaluation and pinpoint enduring issues intrinsic to existing techniques. Our objective is to provide valuable insights into the effectiveness and feasibility of each editing technique, thereby assisting the community in ma
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#24341;&#20837;&#20102;Clifford&#32676;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#65292;&#23427;&#21487;&#20197;&#26500;&#24314;O(n)&#21644;E(n)&#31561;&#21464;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35843;&#25972;Clifford&#32676;&#30340;&#23450;&#20041;&#20197;&#21450;&#20445;&#25345;&#21521;&#37327;&#31354;&#38388;&#21644;&#20056;&#27861;&#32467;&#26500;&#30340;&#20316;&#29992;&#26469;&#23454;&#29616;&#22810;&#20010;&#26377;&#21033;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.11141</link><description>&lt;p&gt;
Clifford&#32676;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Clifford Group Equivariant Neural Networks. (arXiv:2305.11141v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11141
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;Clifford&#32676;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#65292;&#23427;&#21487;&#20197;&#26500;&#24314;O(n)&#21644;E(n)&#31561;&#21464;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35843;&#25972;Clifford&#32676;&#30340;&#23450;&#20041;&#20197;&#21450;&#20445;&#25345;&#21521;&#37327;&#31354;&#38388;&#21644;&#20056;&#27861;&#32467;&#26500;&#30340;&#20316;&#29992;&#26469;&#23454;&#29616;&#22810;&#20010;&#26377;&#21033;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;Clifford&#32676;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#65306;&#19968;&#31181;&#26500;&#24314;O(n)&#21644;E(n)&#31561;&#21464;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30830;&#23450;&#24182;&#30740;&#31350;&#20102;Clifford&#32676;&#65292;&#23427;&#26159;Clifford&#20195;&#25968;&#20013;&#30340;&#19968;&#20010;&#23376;&#32676;&#65292;&#20854;&#23450;&#20041;&#32463;&#36807;&#35843;&#25972;&#20197;&#23454;&#29616;&#22810;&#20010;&#26377;&#21033;&#23646;&#24615;&#12290;&#20027;&#35201;&#22320;&#65292;&#35813;&#32676;&#30340;&#20316;&#29992;&#24418;&#25104;&#20102;&#19968;&#20010;&#27491;&#20132;&#33258;&#21516;&#26500;&#65292;&#25193;&#23637;&#21040;&#25972;&#20010;Clifford&#20195;&#25968;&#65292;&#21516;&#26102;&#23562;&#37325;&#22810;&#30690;&#20998;&#32423;&#12290;&#36825;&#23548;&#33268;&#20102;&#23545;&#24212;&#20110;&#22810;&#30690;&#20998;&#35299;&#30340;&#22810;&#20010;&#38750;&#31561;&#20215;&#23376;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#35813;&#20316;&#29992;&#19981;&#20165;&#23562;&#37325;Clifford&#20195;&#25968;&#30340;&#21521;&#37327;&#31354;&#38388;&#32467;&#26500;&#65292;&#36824;&#23562;&#37325;&#20854;&#20056;&#27861;&#32467;&#26500;&#65292;&#21363;&#20960;&#20309;&#20056;&#31215;&#12290;&#36825;&#20123;&#21457;&#29616;&#24847;&#21619;&#30528;&#25105;&#20204;&#21487;&#20197;&#24471;&#21040;&#22312;&#20219;&#24847;&#32500;&#30340;&#20869;&#31215;&#31354;&#38388;&#20013;&#20248;&#38597;&#22320;&#25512;&#24191;&#30340;&#34920;&#36798;&#23618;&#12290;&#25105;&#20204;&#29305;&#21035;&#23637;&#31034;&#20102;&#20174;&#19968;&#20010;sin
&lt;/p&gt;
&lt;p&gt;
We introduce Clifford Group Equivariant Neural Networks: a novel approach for constructing $\mathrm{O}(n)$- and $\mathrm{E}(n)$-equivariant models. We identify and study the $\textit{Clifford group}$, a subgroup inside the Clifford algebra whose definition we adjust to achieve several favorable properties. Primarily, the group's action forms an orthogonal automorphism that extends beyond the typical vector space to the entire Clifford algebra while respecting the multivector grading. This leads to several non-equivalent subrepresentations corresponding to the multivector decomposition. Furthermore, we prove that the action respects not just the vector space structure of the Clifford algebra but also its multiplicative structure, i.e., the geometric product. These findings imply that every polynomial in multivectors, An advantage worth mentioning is that we obtain expressive layers that can elegantly generalize to inner-product spaces of any dimension. We demonstrate, notably from a sin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;$\lambda$-\textit{equitune}&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;\textit{&#37325;&#35201;&#24615;&#26435;&#37325;}$\lambda$&#23545;&#29305;&#24449;&#36827;&#34892;&#24179;&#22343;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#31561;&#21464;&#23567;&#26679;&#26412;&#23398;&#20064;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.09900</link><description>&lt;p&gt;
&#22522;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#31561;&#21464;&#23567;&#26679;&#26412;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Equivariant Few-Shot Learning from Pretrained Models. (arXiv:2305.09900v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09900
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;$\lambda$-\textit{equitune}&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;\textit{&#37325;&#35201;&#24615;&#26435;&#37325;}$\lambda$&#23545;&#29305;&#24449;&#36827;&#34892;&#24179;&#22343;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#31561;&#21464;&#23567;&#26679;&#26412;&#23398;&#20064;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#30340;&#36801;&#31227;&#23398;&#20064;&#31639;&#27861;&#26159;&#22522;&#30784;&#27169;&#22411;&#22312;&#26377;&#38480;&#25968;&#25454;&#24773;&#20917;&#19979;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#19978;&#21462;&#24471;&#25104;&#21151;&#30340;&#20851;&#38190;&#12290;&#26368;&#36817;&#30340;&#20316;&#21697; \cite{basu2022equi} &#21644; \cite{kaba2022equivariance} &#20998;&#21035;&#25552;&#20986;&#20102;&#20351;&#29992;&#20174;&#32676;&#21464;&#25442;&#36755;&#20837;&#24471;&#21040;&#30340;&#29305;&#24449;&#30340;&#32676;&#24179;&#22343;&#20540;&#65288;\textit{equitune}&#65289;&#21644;&#22522;&#20110;&#20248;&#21270;&#30340;&#26041;&#27861;&#26469;&#20174;&#19981;&#31561;&#21464;&#30340;&#31070;&#32463;&#32593;&#32476;&#33719;&#21462;&#31561;&#21464;&#36755;&#20986;&#12290;&#34429;&#28982; \cite{kaba2022equivariance} &#21482;&#20851;&#27880;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#21363;&#20351;&#22312;&#33391;&#22909;&#30340;&#24494;&#35843;&#32467;&#26524;&#19979;&#65292;\textit{equitune} &#22312;&#31561;&#21464;&#38646;&#26679;&#26412;&#20219;&#21153;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#26159;&#22240;&#20026;&#39044;&#35757;&#32451;&#27169;&#22411;&#20026;&#26576;&#20123;&#36716;&#25442;&#25552;&#20379;&#20102;&#26356;&#39640;&#36136;&#37327;&#30340;&#29305;&#24449;&#65292;&#32780;&#23545;&#20854;&#36827;&#34892;&#31616;&#21333;&#24179;&#22343;&#20250;&#20135;&#29983;&#19981;&#33391;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;\textit{&#37325;&#35201;&#24615;&#26435;&#37325;}$\lambda$&#23545;&#29305;&#24449;&#36827;&#34892;&#24179;&#22343;&#30340;$\lambda$-\textit{equitune} &#26041;&#27861;&#12290;&#36825;&#20123;&#26435;&#37325;&#26159;&#20351;&#29992;&#19968;&#20010;&#23567;&#22411;&#31070;&#32463;&#32593;&#32476;&#30452;&#25509;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#65292;&#20174;&#32780;&#23548;&#33268;&#20986;&#33394;&#30340;&#38646;&#26679;&#26412;&#21644;&#24494;&#35843;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficient transfer learning algorithms are key to the success of foundation models on diverse downstream tasks even with limited data. Recent works of \cite{basu2022equi} and \cite{kaba2022equivariance} propose group averaging (\textit{equitune}) and optimization-based methods, respectively, over features from group-transformed inputs to obtain equivariant outputs from non-equivariant neural networks. While \cite{kaba2022equivariance} are only concerned with training from scratch, we find that equitune performs poorly on equivariant zero-shot tasks despite good finetuning results. We hypothesize that this is because pretrained models provide better quality features for certain transformations than others and simply averaging them is deleterious. Hence, we propose $\lambda$-\textit{equitune} that averages the features using \textit{importance weights}, $\lambda$s. These weights are learned directly from the data using a small neural network, leading to excellent zero-shot and finetuned 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#30693;&#35782;&#21453;&#24605;&#30340;&#26032;&#33539;&#24335;&#65292;&#26088;&#22312;&#24110;&#21161;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21033;&#29992;&#24050;&#32463;&#32534;&#30721;&#22312;&#20854;&#39044;&#35757;&#32451;&#21442;&#25968;&#20013;&#30340;&#30456;&#20851;&#28508;&#22312;&#30693;&#35782;&#65292;&#32780;&#19981;&#38656;&#35201;&#20174;&#22806;&#37096;&#35821;&#26009;&#24211;&#20013;&#26816;&#32034;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#22312;&#27169;&#22411;&#20013;&#28155;&#21152;&#25552;&#31034;&#65292;&#24182;&#23558;&#30456;&#20851;&#30693;&#35782;&#27880;&#20837;&#27169;&#22411;&#36827;&#34892;&#25972;&#21512;&#65292;&#21462;&#24471;&#20102;&#22312;&#24120;&#35782;&#25512;&#29702;&#20219;&#21153;&#21644;GLUE&#22522;&#20934;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.08732</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#21453;&#24605;
&lt;/p&gt;
&lt;p&gt;
Knowledge Rumination for Pre-trained Language Models. (arXiv:2305.08732v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08732
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#30693;&#35782;&#21453;&#24605;&#30340;&#26032;&#33539;&#24335;&#65292;&#26088;&#22312;&#24110;&#21161;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21033;&#29992;&#24050;&#32463;&#32534;&#30721;&#22312;&#20854;&#39044;&#35757;&#32451;&#21442;&#25968;&#20013;&#30340;&#30456;&#20851;&#28508;&#22312;&#30693;&#35782;&#65292;&#32780;&#19981;&#38656;&#35201;&#20174;&#22806;&#37096;&#35821;&#26009;&#24211;&#20013;&#26816;&#32034;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#22312;&#27169;&#22411;&#20013;&#28155;&#21152;&#25552;&#31034;&#65292;&#24182;&#23558;&#30456;&#20851;&#30693;&#35782;&#27880;&#20837;&#27169;&#22411;&#36827;&#34892;&#25972;&#21512;&#65292;&#21462;&#24471;&#20102;&#22312;&#24120;&#35782;&#25512;&#29702;&#20219;&#21153;&#21644;GLUE&#22522;&#20934;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#26222;&#36890;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#21333;&#29420;&#22788;&#29702;&#30693;&#35782;&#23494;&#38598;&#22411;NLP&#20219;&#21153;&#30340;&#33021;&#21147;&#19981;&#36275;&#65292;&#22240;&#27492;&#65292;&#19968;&#20123;&#24037;&#20316;&#23581;&#35797;&#23558;&#22806;&#37096;&#30693;&#35782;&#38598;&#25104;&#21040;PLMs&#20013;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#26377;&#30528;&#26377;&#21069;&#36884;&#30340;&#32467;&#26524;&#65292;&#20294;&#25105;&#20204;&#32463;&#39564;&#24615;&#22320;&#35266;&#23519;&#21040;&#65292;PLM&#21487;&#33021;&#24050;&#32463;&#22312;&#20854;&#39044;&#35757;&#32451;&#21442;&#25968;&#20013;&#32534;&#30721;&#20102;&#20016;&#23500;&#30340;&#30693;&#35782;&#65292;&#20294;&#22312;&#24212;&#29992;&#21040;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#26102;&#26410;&#33021;&#20805;&#20998;&#21033;&#29992;&#23427;&#20204;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#30693;&#35782;&#21453;&#24605;&#30340;&#26032;&#33539;&#24335;&#65292;&#20197;&#24110;&#21161;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21033;&#29992;&#30456;&#20851;&#30340;&#28508;&#22312;&#30693;&#35782;&#65292;&#32780;&#19981;&#38656;&#35201;&#20174;&#22806;&#37096;&#35821;&#26009;&#24211;&#20013;&#26816;&#32034;&#23427;&#20204;&#12290;&#36890;&#36807;&#31616;&#21333;&#22320;&#22312;PLMs&#20013;&#28155;&#21152;&#19968;&#20010;&#22914;&#8220;&#25454;&#25105;&#25152;&#30693;&#8221;&#30340;&#25552;&#31034;&#65292;&#25105;&#20204;&#35797;&#22270;&#22238;&#39038;&#30456;&#20851;&#30340;&#28508;&#22312;&#30693;&#35782;&#65292;&#24182;&#23558;&#20854;&#27880;&#20837;&#27169;&#22411;&#20197;&#36827;&#34892;&#30693;&#35782;&#25972;&#21512;&#12290;&#25105;&#20204;&#23558;&#25552;&#20986;&#30340;&#30693;&#35782;&#21453;&#24605;&#24212;&#29992;&#20110;&#21508;&#31181;&#35821;&#35328;&#27169;&#22411;&#65292;&#21253;&#25324;RoBERTa&#12289;DeBERTa&#21644;GPT-3&#12290;&#22312;&#20845;&#20010;&#24120;&#35782;&#25512;&#29702;&#20219;&#21153;&#21644;GLUE&#22522;&#20934;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;.....
&lt;/p&gt;
&lt;p&gt;
Previous studies have revealed that vanilla pre-trained language models (PLMs) lack the capacity to handle knowledge-intensive NLP tasks alone; thus, several works have attempted to integrate external knowledge into PLMs. However, despite the promising outcome, we empirically observe that PLMs may have already encoded rich knowledge in their pre-trained parameters but fail to fully utilize them when applying them to knowledge-intensive tasks. In this paper, we propose a new paradigm dubbed Knowledge Rumination to help the pre-trained language model utilize that related latent knowledge without retrieving it from the external corpus. By simply adding a prompt like "As far as I know" to the PLMs, we try to review related latent knowledge and inject them back into the model for knowledge consolidation. We apply the proposed knowledge rumination to various language models, including RoBERTa, DeBERTa, and GPT-3. Experimental results on six commonsense reasoning tasks and GLUE benchmarks dem
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;ChatGPT&#21644;GPT-4&#22312;&#37329;&#34701;&#25991;&#26412;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#65292;&#32467;&#26524;&#26174;&#31034;&#23427;&#20204;&#22312;&#25968;&#20540;&#25512;&#29702;&#19978;&#34920;&#29616;&#20986;&#33394;&#20294;&#22312;&#38656;&#35201;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#30340;&#20219;&#21153;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;</title><link>http://arxiv.org/abs/2305.05862</link><description>&lt;p&gt;
ChatGPT&#21644;GPT-4&#26159;&#21542;&#26159;&#37329;&#34701;&#25991;&#26412;&#20998;&#26512;&#30340;&#36890;&#29992;&#27714;&#35299;&#22120;&#65311;&#23545;&#20960;&#31181;&#20856;&#22411;&#20219;&#21153;&#36827;&#34892;&#26816;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Are ChatGPT and GPT-4 General-Purpose Solvers for Financial Text Analytics? An Examination on Several Typical Tasks. (arXiv:2305.05862v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05862
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;ChatGPT&#21644;GPT-4&#22312;&#37329;&#34701;&#25991;&#26412;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#65292;&#32467;&#26524;&#26174;&#31034;&#23427;&#20204;&#22312;&#25968;&#20540;&#25512;&#29702;&#19978;&#34920;&#29616;&#20986;&#33394;&#20294;&#22312;&#38656;&#35201;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#30340;&#20219;&#21153;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;ChatGPT&#21644;GPT-4&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#24191;&#27867;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#23545;&#35805;&#22238;&#24212;&#12290;&#23613;&#31649;ChatGPT&#21644;GPT-4&#22312;&#36890;&#29992;&#25991;&#26412;&#35821;&#26009;&#24211;&#19978;&#32463;&#36807;&#20102;&#24191;&#27867;&#30340;&#27979;&#35797;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;&#36824;&#27809;&#26377;&#23545;&#37329;&#34701;&#35821;&#26009;&#24211;&#36827;&#34892;&#30740;&#31350;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#22312;&#38646;&#26679;&#26412;&#25110;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#32771;&#23519;ChatGPT&#21644;GPT-4&#20316;&#20026;&#20856;&#22411;&#37329;&#34701;&#25991;&#26412;&#20998;&#26512;&#38382;&#39064;&#27714;&#35299;&#22120;&#30340;&#28508;&#21147;&#26469;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#23427;&#20204;&#22312;&#20116;&#20010;&#19981;&#21516;&#30340;&#37329;&#34701;&#25991;&#26412;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#22235;&#39033;&#20195;&#34920;&#24615;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#21021;&#27493;&#30740;&#31350;&#34920;&#26126;&#65292;ChatGPT&#21644;GPT-4&#22312;&#38656;&#35201;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#30340;&#37329;&#34701;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#21644;&#24773;&#24863;&#20998;&#26512;&#31561;&#20219;&#21153;&#19978;&#34920;&#29616;&#19981;&#20339;&#65292;&#32780;&#22312;&#25968;&#20540;&#25512;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#24403;&#21069;&#29256;&#26412;ChatGPT&#21644;GPT-4&#30340;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#29616;&#26377;&#25216;&#26415;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
The most recent large language models such as ChatGPT and GPT-4 have garnered significant attention, as they are capable of generating high-quality responses to human input. Despite the extensive testing of ChatGPT and GPT-4 on generic text corpora, showcasing their impressive capabilities, a study focusing on financial corpora has not been conducted. In this study, we aim to bridge this gap by examining the potential of ChatGPT and GPT-4 as a solver for typical financial text analytic problems in the zero-shot or few-shot setting. Specifically, we assess their capabilities on four representative tasks over five distinct financial textual datasets. The preliminary study shows that ChatGPT and GPT-4 struggle on tasks such as financial named entity recognition (NER) and sentiment analysis, where domain-specific knowledge is required, while they excel in numerical reasoning tasks. We report both the strengths and limitations of the current versions of ChatGPT and GPT-4, comparing them to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#26426;&#22120;&#20154;&#36827;&#34892;&#23478;&#24237;&#28165;&#25195;&#30340;&#20010;&#24615;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23569;&#26679;&#26412;&#25688;&#35201;&#33021;&#21147;&#65292;&#26426;&#22120;&#20154;&#21487;&#20197;&#23398;&#20064;&#29992;&#25143;&#30340;&#20559;&#22909;&#24182;&#23558;&#20854;&#25512;&#24191;&#21040;&#26410;&#26469;&#30340;&#22330;&#26223;&#20013;&#65292;&#20174;&#32780;&#23454;&#29616;&#24555;&#36895;&#36866;&#24212;&#12290;</title><link>http://arxiv.org/abs/2305.05658</link><description>&lt;p&gt;
TidyBot: &#24212;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20010;&#24615;&#21270;&#26426;&#22120;&#20154;&#29289;&#29702;&#36741;&#21161;
&lt;/p&gt;
&lt;p&gt;
TidyBot: Personalized Robot Assistance with Large Language Models. (arXiv:2305.05658v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05658
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#26426;&#22120;&#20154;&#36827;&#34892;&#23478;&#24237;&#28165;&#25195;&#30340;&#20010;&#24615;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23569;&#26679;&#26412;&#25688;&#35201;&#33021;&#21147;&#65292;&#26426;&#22120;&#20154;&#21487;&#20197;&#23398;&#20064;&#29992;&#25143;&#30340;&#20559;&#22909;&#24182;&#23558;&#20854;&#25512;&#24191;&#21040;&#26410;&#26469;&#30340;&#22330;&#26223;&#20013;&#65292;&#20174;&#32780;&#23454;&#29616;&#24555;&#36895;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#26377;&#25928;&#20010;&#24615;&#21270;&#22320;&#25552;&#20379;&#29289;&#29702;&#36741;&#21161;&#65292;&#23427;&#24517;&#39035;&#23398;&#20064;&#29992;&#25143;&#30340;&#20010;&#20154;&#21916;&#22909;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#26410;&#26469;&#30340;&#22330;&#26223;&#20013;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#26426;&#22120;&#20154;&#36827;&#34892;&#23478;&#24237;&#28165;&#25195;&#30340;&#20010;&#24615;&#21270;&#38382;&#39064;&#65292;&#36825;&#20123;&#26426;&#22120;&#20154;&#33021;&#22815;&#36890;&#36807;&#25441;&#36215;&#29289;&#21697;&#24182;&#23558;&#20854;&#25918;&#22238;&#21407;&#22788;&#26469;&#25972;&#29702;&#25151;&#38388;&#12290;&#19968;&#20010;&#20851;&#38190;&#30340;&#25361;&#25112;&#26159;&#30830;&#23450;&#27599;&#20010;&#29289;&#21697;&#30340;&#27491;&#30830;&#20301;&#32622;&#65292;&#22240;&#20026;&#20154;&#20204;&#30340;&#21916;&#22909;&#21487;&#20197;&#22240;&#20010;&#20154;&#21697;&#21619;&#25110;&#25991;&#21270;&#32972;&#26223;&#32780;&#22823;&#19981;&#30456;&#21516;&#12290;&#20363;&#22914;&#65292;&#19968;&#20010;&#20154;&#21487;&#33021;&#21916;&#27426;&#25226;&#34924;&#34923;&#25918;&#22312;&#25277;&#23625;&#37324;&#65292;&#32780;&#21478;&#19968;&#20010;&#20154;&#21487;&#33021;&#21916;&#27426;&#25226;&#34924;&#34923;&#25918;&#22312;&#26550;&#23376;&#19978;&#12290;&#25105;&#20204;&#26088;&#22312;&#24314;&#31435;&#31995;&#32479;&#65292;&#36825;&#20123;&#31995;&#32479;&#21487;&#20197;&#36890;&#36807;&#19982;&#29305;&#23450;&#20154;&#30340;&#20808;&#21069;&#20132;&#20114;&#23398;&#20064;&#36825;&#26679;&#30340;&#21916;&#22909;&#65292;&#32780;&#21482;&#38656;&#35201;&#20960;&#20010;&#31034;&#20363;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26426;&#22120;&#20154;&#21487;&#20197;&#23558;&#22522;&#20110;&#35821;&#35328;&#30340;&#35268;&#21010;&#21644;&#24863;&#30693;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#23569;&#26679;&#26412;&#25688;&#35201;&#33021;&#21147;&#30456;&#32467;&#21512;&#65292;&#20174;&#32780;&#25512;&#26029;&#20986;&#24191;&#27867;&#36866;&#29992;&#20110;&#26410;&#26469;&#20132;&#20114;&#30340;&#29992;&#25143;&#20559;&#22909;&#12290;&#36825;&#31181;&#26041;&#27861;&#23454;&#29616;&#20102;&#24555;&#36895;&#36866;&#24212;&#65292;&#24182;&#21462;&#24471;&#20102;91.2%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
For a robot to personalize physical assistance effectively, it must learn user preferences that can be generally reapplied to future scenarios. In this work, we investigate personalization of household cleanup with robots that can tidy up rooms by picking up objects and putting them away. A key challenge is determining the proper place to put each object, as people's preferences can vary greatly depending on personal taste or cultural background. For instance, one person may prefer storing shirts in the drawer, while another may prefer them on the shelf. We aim to build systems that can learn such preferences from just a handful of examples via prior interactions with a particular person. We show that robots can combine language-based planning and perception with the few-shot summarization capabilities of large language models (LLMs) to infer generalized user preferences that are broadly applicable to future interactions. This approach enables fast adaptation and achieves 91.2% accurac
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#34701;&#21512;&#22312;&#19981;&#21516;&#27169;&#24577;&#19978;&#35757;&#32451;&#30340;transformer&#36827;&#34892;&#22810;&#27169;&#24577;&#27169;&#22411;&#34701;&#21512;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#21442;&#25968;&#26377;&#25928;&#30340;&#27169;&#24577;&#19981;&#21487;&#30693;&#26550;&#26500;&#65292;&#24418;&#25104;&#26377;&#25928;&#30340;&#35757;&#32451;&#37197;&#26041;&#12290;</title><link>http://arxiv.org/abs/2304.14933</link><description>&lt;p&gt;
&#19968;&#39033;&#22810;&#27169;&#24577;&#27169;&#22411;&#34701;&#21512;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Empirical Study of Multimodal Model Merging. (arXiv:2304.14933v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14933
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#34701;&#21512;&#22312;&#19981;&#21516;&#27169;&#24577;&#19978;&#35757;&#32451;&#30340;transformer&#36827;&#34892;&#22810;&#27169;&#24577;&#27169;&#22411;&#34701;&#21512;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#21442;&#25968;&#26377;&#25928;&#30340;&#27169;&#24577;&#19981;&#21487;&#30693;&#26550;&#26500;&#65292;&#24418;&#25104;&#26377;&#25928;&#30340;&#35757;&#32451;&#37197;&#26041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#34701;&#21512;&#65288;&#20363;&#22914;&#25554;&#20540;&#25110;&#20219;&#21153;&#31639;&#26415;&#65289;&#23558;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#35757;&#32451;&#30340;&#22810;&#20010;&#27169;&#22411;&#21512;&#24182;&#20197;&#29983;&#25104;&#22810;&#20219;&#21153;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#25216;&#26415;&#22312;&#20808;&#21069;&#30340;&#30740;&#31350;&#20013;&#24050;&#32463;&#34987;&#35777;&#26126;&#25104;&#21151;&#65292;&#20854;&#20013;&#27169;&#22411;&#26159;&#22312;&#30456;&#20284;&#30340;&#20219;&#21153;&#21644;&#30456;&#21516;&#30340;&#21021;&#22987;&#21270;&#19979;&#35757;&#32451;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#22312;&#19981;&#21516;&#27169;&#24577;&#19978;&#35757;&#32451;&#30340;transformer&#36827;&#34892;&#34701;&#21512;&#65292;&#23558;&#27492;&#27010;&#24565;&#25193;&#23637;&#21040;&#22810;&#27169;&#24577;&#35774;&#32622;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#38024;&#23545;&#19968;&#20010;&#26032;&#39062;&#30340;&#30446;&#26631;&#36827;&#34892;&#30740;&#31350;&#65292;&#22312;&#35813;&#30446;&#26631;&#20013;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#35270;&#35273;&#12289;&#35821;&#35328;&#21644;&#36328;&#27169;&#24577;&#30340;transformer&#21512;&#24182;&#21040;&#29305;&#23450;&#27169;&#24577;&#30340;&#26550;&#26500;&#20013;&#65292;&#20197;&#21019;&#24314;&#19968;&#20010;&#21442;&#25968;&#26377;&#25928;&#30340;&#27169;&#24577;&#19981;&#21487;&#30693;&#26550;&#26500;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#24433;&#21709;&#27169;&#22411;&#34701;&#21512;&#21518;&#24615;&#33021;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#21253;&#25324;&#21021;&#22987;&#21270;&#12289;&#34701;&#21512;&#26426;&#21046;&#21644;&#27169;&#22411;&#26550;&#26500;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#24471;&#20986;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#35757;&#32451;&#37197;&#26041;&#65292;&#21487;&#20197;&#36890;&#36807;&#27169;&#22411;&#34701;&#21512;&#26469;&#21305;&#37197;&#27169;&#24577;&#19981;&#21487;&#30693;&#22522;&#32447;&#30340;&#24615;&#33021;&#65288;&#21363;&#20174;&#22836;&#24320;&#22987;&#39044;&#35757;&#32451;&#65289;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#20379;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model merging (e.g., via interpolation or task arithmetic) fuses multiple models trained on different tasks to generate a multi-task solution. The technique has been proven successful in previous studies, where the models are trained on similar tasks and with the same initialization. In this paper, we expand on this concept to a multimodal setup by merging transformers trained on different modalities. Furthermore, we conduct our study for a novel goal where we can merge vision, language, and cross-modal transformers of a modality-specific architecture to create a parameter-efficient modality-agnostic architecture. Through comprehensive experiments, we systematically investigate the key factors impacting model performance after merging, including initialization, merging mechanisms, and model architectures. Our analysis leads to an effective training recipe for matching the performance of the modality-agnostic baseline (i.e. pre-trained from scratch) via model merging. Our code is availa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#29702;&#35770;&#26041;&#27861;&#8212;&#8212;&#34892;&#20026;&#26399;&#26395;&#36793;&#30028;&#65288;BEB&#65289;&#65292;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23545;&#40784;&#30340;&#22522;&#26412;&#38480;&#21046;&#65292;&#24182;&#35777;&#26126;&#20219;&#20309;&#23545;&#40784;&#36807;&#31243;&#37117;&#26080;&#27861;&#26681;&#38500;&#19981;&#24076;&#26395;&#30340;&#34892;&#20026;&#65292;&#36825;&#23545;&#20110;&#38450;&#27490;&#24694;&#24847;&#25915;&#20987;&#26159;&#19981;&#23433;&#20840;&#30340;&#12290;</title><link>http://arxiv.org/abs/2304.11082</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#30340;&#22522;&#26412;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Fundamental Limitations of Alignment in Large Language Models. (arXiv:2304.11082v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11082
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#29702;&#35770;&#26041;&#27861;&#8212;&#8212;&#34892;&#20026;&#26399;&#26395;&#36793;&#30028;&#65288;BEB&#65289;&#65292;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23545;&#40784;&#30340;&#22522;&#26412;&#38480;&#21046;&#65292;&#24182;&#35777;&#26126;&#20219;&#20309;&#23545;&#40784;&#36807;&#31243;&#37117;&#26080;&#27861;&#26681;&#38500;&#19981;&#24076;&#26395;&#30340;&#34892;&#20026;&#65292;&#36825;&#23545;&#20110;&#38450;&#27490;&#24694;&#24847;&#25915;&#20987;&#26159;&#19981;&#23433;&#20840;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#19982;&#20154;&#20132;&#20114;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#37325;&#35201;&#26041;&#38754;&#26159;&#23545;&#40784;&#20854;&#34892;&#20026;&#65292;&#20351;&#20854;&#23545;&#20854;&#20154;&#31867;&#29992;&#25143;&#26377;&#29992;&#19988;&#26080;&#23475;&#12290;&#36825;&#36890;&#24120;&#36890;&#36807;&#35843;&#25972;&#27169;&#22411;&#30340;&#26041;&#24335;&#26469;&#23454;&#29616;&#65292;&#20197;&#22686;&#24378;&#25152;&#38656;&#30340;&#34892;&#20026;&#24182;&#25233;&#21046;&#19981;&#24076;&#26395;&#30340;&#34892;&#20026;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#34892;&#20026;&#26399;&#26395;&#36793;&#30028;(BEB)&#30340;&#29702;&#35770;&#26041;&#27861;&#65292;&#23427;&#20801;&#35768;&#25105;&#20204;&#27491;&#24335;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20960;&#20010;&#20869;&#22312;&#29305;&#24449;&#21644;&#23545;&#40784;&#30340;&#38480;&#21046;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#35777;&#26126;&#23545;&#20110;&#20219;&#20309;&#20855;&#26377;&#34987;&#35813;&#27169;&#22411;&#34920;&#29616;&#20986;&#30340;&#26377;&#38480;&#27010;&#29575;&#30340;&#34892;&#20026;&#65292;&#37117;&#23384;&#22312;&#21487;&#20197;&#35302;&#21457;&#27169;&#22411;&#36755;&#20986;&#27492;&#34892;&#20026;&#30340;&#25552;&#31034;&#65292;&#20854;&#27010;&#29575;&#38543;&#25552;&#31034;&#30340;&#38271;&#24230;&#22686;&#21152;&#32780;&#22686;&#21152;&#12290;&#36825;&#24847;&#21619;&#30528;&#20219;&#20309;&#20943;&#24369;&#19981;&#24076;&#26395;&#30340;&#34892;&#20026;&#20294;&#26410;&#23558;&#20854;&#23436;&#20840;&#28040;&#38500;&#30340;&#23545;&#40784;&#36807;&#31243;&#37117;&#26080;&#27861;&#25269;&#24481;&#38024;&#23545;&#24615;&#25915;&#20987;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#25552;&#31034;&#20102;&#39046;&#20808;&#30340;
&lt;/p&gt;
&lt;p&gt;
An important aspect in developing language models that interact with humans is aligning their behavior to be useful and unharmful for their human users. This is usually achieved by tuning the model in a way that enhances desired behaviors and inhibits undesired ones, a process referred to as alignment. In this paper, we propose a theoretical approach called Behavior Expectation Bounds (BEB) which allows us to formally investigate several inherent characteristics and limitations of alignment in large language models. Importantly, we prove that for any behavior that has a finite probability of being exhibited by the model, there exist prompts that can trigger the model into outputting this behavior, with probability that increases with the length of the prompt. This implies that any alignment process that attenuates undesired behavior but does not remove it altogether, is not safe against adversarial prompting attacks. Furthermore, our framework hints at the mechanism by which leading al
&lt;/p&gt;</description></item><item><title>YOLO-Drone&#26159;&#19968;&#31181;&#33021;&#22815;&#39640;&#25928;&#26816;&#27979;&#23567;&#23610;&#24230;&#29289;&#20307;&#30340;&#23454;&#26102;&#29289;&#20307;&#26816;&#27979;&#31639;&#27861;&#65292;&#24182;&#22312;&#26080;&#20154;&#26426;&#24179;&#21488;&#19978;&#24471;&#21040;&#20102;&#24212;&#29992;&#65292;&#20197;&#24191;&#20041;&#20132;&#38598;&#32852;&#30431;(GIOU)&#20316;&#20026;&#25439;&#22833;&#20989;&#25968;&#65292;&#21462;&#24471;&#20102;&#26368;&#20339;&#30340;&#26816;&#27979;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.06925</link><description>&lt;p&gt;
YOLO-Drone: &#39640;&#31354;&#21363;&#26102;&#26816;&#27979;&#23494;&#38598;&#23567;&#29289;&#20307;&#30340;&#26080;&#20154;&#26426;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
YOLO-Drone:Airborne real-time detection of dense small objects from high-altitude perspective. (arXiv:2304.06925v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06925
&lt;/p&gt;
&lt;p&gt;
YOLO-Drone&#26159;&#19968;&#31181;&#33021;&#22815;&#39640;&#25928;&#26816;&#27979;&#23567;&#23610;&#24230;&#29289;&#20307;&#30340;&#23454;&#26102;&#29289;&#20307;&#26816;&#27979;&#31639;&#27861;&#65292;&#24182;&#22312;&#26080;&#20154;&#26426;&#24179;&#21488;&#19978;&#24471;&#21040;&#20102;&#24212;&#29992;&#65292;&#20197;&#24191;&#20041;&#20132;&#38598;&#32852;&#30431;(GIOU)&#20316;&#20026;&#25439;&#22833;&#20989;&#25968;&#65292;&#21462;&#24471;&#20102;&#26368;&#20339;&#30340;&#26816;&#27979;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#20154;&#26426;&#36828;&#31243;&#24863;&#24212;&#30446;&#26631;&#25506;&#27979;&#25216;&#26415;&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#35768;&#22810;&#39046;&#22495;&#65292;&#24182;&#25104;&#20026;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#30340;&#20027;&#35201;&#30740;&#31350;&#26041;&#21521;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#29289;&#20307;&#22823;&#23567;&#12289;&#22270;&#20687;&#38477;&#22122;&#21644;&#23454;&#26102;&#24615;&#31561;&#22240;&#32032;&#30340;&#25361;&#25112;&#65292;&#23567;&#23610;&#24230;&#29289;&#20307;&#30340;&#21487;&#38752;&#26816;&#27979;&#19968;&#30452;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#26102;&#29289;&#20307;&#26816;&#27979;&#31639;&#27861;(YOLO-Drone)&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#20004;&#31181;&#26032;&#30340;&#26080;&#20154;&#26426;&#24179;&#21488;&#20197;&#21450;&#19968;&#31181;&#29305;&#23450;&#30340;&#20809;&#28304;(&#30789;&#22522;&#37329;LED)&#12290;YOLO-Drone&#25552;&#20986;&#20102;&#20960;&#20010;&#21019;&#26032;&#65306;1)&#21253;&#25324;&#19968;&#20010;&#26032;&#30340;&#39592;&#24178;Darknet59&#65307;2)&#19968;&#20010;&#26032;&#30340;&#22797;&#26434;&#29305;&#24449;&#32858;&#21512;&#27169;&#22359;MSPP-FPN&#65292;&#23427;&#21253;&#25324;&#19968;&#20010;&#31354;&#38388;&#37329;&#23383;&#22612;&#27744;&#21270;&#21644;&#19977;&#20010;&#25193;&#24352;&#31354;&#38388;&#37329;&#23383;&#22612;&#27744;&#21270;&#27169;&#22359;&#65307;3)&#20351;&#29992;&#24191;&#20041;&#20132;&#38598;&#32852;&#30431;(Generalized Intersection over Union&#65292;GIOU)&#20316;&#20026;&#25439;&#22833;&#20989;&#25968;&#12290;&#20026;&#20102;&#35780;&#20272;&#24615;&#33021;&#65292;&#20351;&#29992;&#20102;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#32467;&#26524;&#34920;&#26126;YOLO-Drone&#22312;&#23567;&#29289;&#20307;&#26816;&#27979;&#26041;&#38754;&#23454;&#29616;&#20102;&#26368;&#20248;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unmanned Aerial Vehicles (UAVs), specifically drones equipped with remote sensing object detection technology, have rapidly gained a broad spectrum of applications and emerged as one of the primary research focuses in the field of computer vision. Although UAV remote sensing systems have the ability to detect various objects, small-scale objects can be challenging to detect reliably due to factors such as object size, image degradation, and real-time limitations. To tackle these issues, a real-time object detection algorithm (YOLO-Drone) is proposed and applied to two new UAV platforms as well as a specific light source (silicon-based golden LED). YOLO-Drone presents several novelties: 1) including a new backbone Darknet59; 2) a new complex feature aggregation module MSPP-FPN that incorporated one spatial pyramid pooling and three atrous spatial pyramid pooling modules; 3) and the use of Generalized Intersection over Union (GIoU) as the loss function. To evaluate performance, two bench
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;SQL&#26597;&#35810;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;LLMs&#20013;&#30340;&#20449;&#24687;&#65292;&#21487;&#20197;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#25552;&#21462;&#25968;&#25454;&#24182;&#36827;&#34892;&#26597;&#35810;&#12290;&#36890;&#36807;Galois&#21407;&#22411;&#23454;&#29616;&#20102;&#26597;&#35810;LLMs&#30340;&#26032;&#29289;&#29702;&#36816;&#31639;&#31526;&#65292;&#24182;&#21462;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.00472</link><description>&lt;p&gt;
&#20351;&#29992;SQL&#26597;&#35810;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Querying Large Language Models with SQL. (arXiv:2304.00472v2 [cs.DB] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00472
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;SQL&#26597;&#35810;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;LLMs&#20013;&#30340;&#20449;&#24687;&#65292;&#21487;&#20197;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#25552;&#21462;&#25968;&#25454;&#24182;&#36827;&#34892;&#26597;&#35810;&#12290;&#36890;&#36807;Galois&#21407;&#22411;&#23454;&#29616;&#20102;&#26597;&#35810;LLMs&#30340;&#26032;&#29289;&#29702;&#36816;&#31639;&#31526;&#65292;&#24182;&#21462;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#20351;&#29992;&#22330;&#26223;&#20013;&#65292;&#20449;&#24687;&#23384;&#20648;&#22312;&#25991;&#26412;&#20013;&#65292;&#20294;&#26080;&#27861;&#22312;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#33719;&#21462;&#12290;&#28982;&#32780;&#65292;&#20174;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#20013;&#25552;&#21462;&#25968;&#25454;&#20197;&#31934;&#30830;&#36866;&#37197;&#27169;&#24335;&#65292;&#24182;&#23454;&#29616;&#26597;&#35810;&#65292;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#38543;&#30528;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20852;&#36215;&#65292;&#29616;&#22312;&#26377;&#20102;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#23384;&#20648;&#21644;&#20351;&#29992;&#20174;&#22823;&#35268;&#27169;&#25991;&#26412;&#25991;&#26723;&#20013;&#25552;&#21462;&#30340;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35774;&#24819;&#20351;&#29992;SQL&#26597;&#35810;&#26469;&#28085;&#30422;&#20256;&#32479;&#25968;&#25454;&#24211;&#26080;&#27861;&#25552;&#21462;&#30340;&#24191;&#27867;&#25968;&#25454;&#65292;&#36890;&#36807;&#21033;&#29992;LLMs&#20013;&#30340;&#20449;&#24687;&#12290;&#20026;&#20102;&#25903;&#25745;&#36825;&#20010;&#24895;&#26223;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#20256;&#32479;&#25968;&#25454;&#24211;&#20307;&#31995;&#32467;&#26500;&#30340;Galois&#21407;&#22411;&#65292;&#20294;&#20855;&#26377;&#29992;&#20110;&#26597;&#35810;&#24213;&#23618;LLM&#30340;&#26032;&#29289;&#29702;&#31639;&#23376;&#12290;&#20027;&#35201;&#24605;&#24819;&#26159;&#20351;&#29992;&#25552;&#31034;&#31526;&#25191;&#34892;&#26597;&#35810;&#35745;&#21010;&#20013;&#30340;&#26576;&#20123;&#25805;&#20316;&#31526;&#65292;&#20174;LLM&#20013;&#26816;&#32034;&#25968;&#25454;&#12290;&#23545;&#20110;&#22823;&#31867;&#21035;&#30340;SQL&#26597;&#35810;&#65292;&#26597;&#35810;LLMs&#36820;&#22238;&#32467;&#26500;&#33391;&#22909;&#30340;&#20851;&#31995;&#65292;&#21462;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#23450;&#24615;&#32467;&#26524;&#12290;&#21021;&#27493;&#23454;&#39564;&#32467;&#26524;&#20351;&#39044;&#35757;&#32451;&#30340;LLMs&#25104;&#20026;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many use-cases, information is stored in text but not available in structured data. However, extracting data from natural language text to precisely fit a schema, and thus enable querying, is a challenging task. With the rise of pre-trained Large Language Models (LLMs), there is now an effective solution to store and use information extracted from massive corpora of text documents. Thus, we envision the use of SQL queries to cover a broad range of data that is not captured by traditional databases by tapping the information in LLMs. To ground this vision, we present Galois, a prototype based on a traditional database architecture, but with new physical operators for querying the underlying LLM. The main idea is to execute some operators of the the query plan with prompts that retrieve data from the LLM. For a large class of SQL queries, querying LLMs returns well structured relations, with encouraging qualitative results. Preliminary experimental results make pre-trained LLMs a prom
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#32467;&#26500;&#21644;&#22806;&#35266;&#37197;&#23545;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#23545;&#35937;&#32423;&#22270;&#20687;&#32534;&#36753;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#31934;&#32454;&#25511;&#21046;&#22270;&#20687;&#20013;&#30340;&#19981;&#21516;&#23545;&#35937;&#23646;&#24615;&#65292;&#21516;&#26102;&#33258;&#21160;&#20256;&#25773;&#27880;&#20837;&#30340;&#22806;&#35266;&#21040;&#20855;&#26377;&#30456;&#20284;&#32467;&#26500;&#30340;&#23545;&#35937;&#12290;</title><link>http://arxiv.org/abs/2303.17546</link><description>&lt;p&gt;
PAIR-Diffusion: &#37319;&#29992;&#32467;&#26500;&#21644;&#22806;&#35266;&#37197;&#23545;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#23545;&#35937;&#32423;&#22270;&#20687;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
PAIR-Diffusion: Object-Level Image Editing with Structure-and-Appearance Paired Diffusion Models. (arXiv:2303.17546v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17546
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#32467;&#26500;&#21644;&#22806;&#35266;&#37197;&#23545;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#23545;&#35937;&#32423;&#22270;&#20687;&#32534;&#36753;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#31934;&#32454;&#25511;&#21046;&#22270;&#20687;&#20013;&#30340;&#19981;&#21516;&#23545;&#35937;&#23646;&#24615;&#65292;&#21516;&#26102;&#33258;&#21160;&#20256;&#25773;&#27880;&#20837;&#30340;&#22806;&#35266;&#21040;&#20855;&#26377;&#30456;&#20284;&#32467;&#26500;&#30340;&#23545;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#22270;&#20687;&#32534;&#36753;&#21457;&#23637;&#36805;&#36895;&#12290;&#20197;&#21069;&#30340;&#20316;&#21697;&#21487;&#20197;&#36890;&#36807;&#21508;&#31181;&#26041;&#24335;&#36827;&#34892;&#25511;&#21046;&#21644;&#32534;&#36753;&#22270;&#20687;&#65292;&#26576;&#20123;&#20316;&#21697;&#20351;&#29992;&#39640;&#32423;&#26465;&#20214;&#65288;&#20363;&#22914;&#25991;&#26412;&#65289;&#65292;&#32780;&#20854;&#20182;&#20316;&#21697;&#20351;&#29992;&#20302;&#32423;&#26465;&#20214;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20316;&#21697;&#32570;&#20047;&#23545;&#22270;&#20687;&#20013;&#19981;&#21516;&#23545;&#35937;&#30340;&#23646;&#24615;&#36827;&#34892;&#31934;&#32454;&#21270;&#25511;&#21046;&#65292;&#21363;&#23545;&#35937;&#32423;&#22270;&#20687;&#32534;&#36753;&#12290;&#26412;&#25991;&#23558;&#22270;&#20687;&#35270;&#20026;&#30001;&#22810;&#20010;&#23545;&#35937;&#32452;&#25104;&#65292;&#27599;&#20010;&#23545;&#35937;&#30001;&#19981;&#21516;&#23646;&#24615;&#23450;&#20041;&#12290;&#25105;&#20204;&#21457;&#29616;&#32467;&#26500;&#21644;&#22806;&#35266;&#26159;&#26368;&#30452;&#35266;&#19988;&#26368;&#26377;&#29992;&#20110;&#32534;&#36753;&#30340;&#23646;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#32467;&#26500;&#21644;&#22806;&#35266;&#37197;&#23545;&#25193;&#25955;&#27169;&#22411;&#65288;PAIR-Diffusion&#65289;&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#20174;&#22270;&#20687;&#20013;&#26126;&#30830;&#25552;&#21462;&#30340;&#32467;&#26500;&#21644;&#22806;&#35266;&#20449;&#24687;&#36827;&#34892;&#35757;&#32451;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#20351;&#29992;&#25143;&#33021;&#22815;&#22312;&#23545;&#35937;&#21644;&#20840;&#23616;&#32423;&#21035;&#23558;&#21442;&#32771;&#22270;&#20687;&#30340;&#22806;&#35266;&#27880;&#20837;&#36755;&#20837;&#22270;&#20687;&#20013;&#12290;&#27492;&#22806;&#65292;PAIR-Diffusion&#33258;&#21160;&#23558;&#27880;&#20837;&#30340;&#22806;&#35266;&#20256;&#25773;&#21040;&#36755;&#20837;&#22270;&#20687;&#20013;&#20855;&#26377;&#31867;&#20284;&#32467;&#26500;&#30340;&#23545;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image editing using diffusion models has witnessed extremely fast-paced growth recently. There are various ways in which previous works enable controlling and editing images. Some works use high-level conditioning such as text, while others use low-level conditioning. Nevertheless, most of them lack fine-grained control over the properties of the different objects present in the image, i.e. object-level image editing. In this work, we consider an image as a composition of multiple objects, each defined by various properties. Out of these properties, we identify structure and appearance as the most intuitive to understand and useful for editing purposes. We propose Structure-and-Appearance Paired Diffusion model (PAIR-Diffusion), which is trained using structure and appearance information explicitly extracted from the images. The proposed model enables users to inject a reference image's appearance into the input image at both the object and global levels. Additionally, PAIR-Diffusion a
&lt;/p&gt;</description></item><item><title>GOOD&#26159;&#19968;&#20010;&#22522;&#20110;&#20248;&#21270;&#30340;&#34701;&#21512;&#26694;&#26550;&#65292;&#21487;&#23454;&#29616;&#39640;&#31934;&#24230;&#12289;&#39640;&#40065;&#26834;&#24615;&#30340;3D&#29289;&#20307;&#26816;&#27979;&#65292;&#36866;&#29992;&#20110;&#20219;&#24847;2D&#21644;3D&#26816;&#27979;&#22120;&#30340;&#32452;&#21512;&#65292;&#24182;&#19988;&#26080;&#38656;&#39069;&#22806;&#27169;&#22411;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2303.09800</link><description>&lt;p&gt;
GOOD&#65306;&#22522;&#20110;&#20248;&#21270;&#30340;&#34701;&#21512;&#26041;&#27861;&#29992;&#20110;&#36890;&#36807;LiDAR&#21644;&#30456;&#26426;&#30340;3D&#29289;&#20307;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
GOOD: General Optimization-based Fusion for 3D Object Detection via LiDAR-Camera Object Candidates. (arXiv:2303.09800v1 [cs.CV] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09800
&lt;/p&gt;
&lt;p&gt;
GOOD&#26159;&#19968;&#20010;&#22522;&#20110;&#20248;&#21270;&#30340;&#34701;&#21512;&#26694;&#26550;&#65292;&#21487;&#23454;&#29616;&#39640;&#31934;&#24230;&#12289;&#39640;&#40065;&#26834;&#24615;&#30340;3D&#29289;&#20307;&#26816;&#27979;&#65292;&#36866;&#29992;&#20110;&#20219;&#24847;2D&#21644;3D&#26816;&#27979;&#22120;&#30340;&#32452;&#21512;&#65292;&#24182;&#19988;&#26080;&#38656;&#39069;&#22806;&#27169;&#22411;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
3D&#29289;&#20307;&#26816;&#27979;&#26159;&#33258;&#21160;&#39550;&#39542;&#20013;&#24863;&#30693;&#20219;&#21153;&#30340;&#26680;&#24515;&#22522;&#30784;&#12290;&#36817;&#24180;&#26469;&#65292;&#22810;&#27169;&#24577;&#34701;&#21512;&#31574;&#30053;&#22312;&#25552;&#39640;3D&#29289;&#20307;&#26816;&#27979;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#24555;&#36895;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#40065;&#26834;&#34701;&#21512;&#30740;&#31350;&#37117;&#26159;&#22522;&#20110;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#19988;&#22312;&#26032;&#22330;&#26223;&#20013;&#23454;&#26045;&#19981;&#26041;&#20415;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GOOD&#65292;&#19968;&#20010;&#36890;&#29992;&#30340;&#22522;&#20110;&#20248;&#21270;&#30340;&#34701;&#21512;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#19981;&#35757;&#32451;&#39069;&#22806;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#36798;&#21040;&#20196;&#20154;&#28385;&#24847;&#30340;&#26816;&#27979;&#25928;&#26524;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#20219;&#24847;2D&#21644;3D&#26816;&#27979;&#22120;&#30340;&#32452;&#21512;&#65292;&#20197;&#25552;&#39640;3D&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;&#20114;&#30456;&#37051;&#36817;&#30340;&#27010;&#29575;&#27169;&#22411;&#23454;&#29616;&#20102;3D-2D&#25968;&#25454;&#20851;&#32852;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20248;&#21270;&#27969;&#31243;&#65292;&#21487;&#20197;&#26681;&#25454;&#21305;&#37197;&#32467;&#26524;&#20998;&#21035;&#20248;&#21270;&#19981;&#21516;&#31867;&#22411;&#30340;&#23454;&#20363;&#12290;&#27492;&#22806;&#65292;&#36824;&#24341;&#20837;&#20102;3D MOT&#26041;&#27861;&#65292;&#36890;&#36807;&#20043;&#21069;&#30340;&#24103;&#26469;&#22686;&#24378;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
3D object detection serves as the core basis of the perception tasks in autonomous driving. Recent years have seen the rapid progress of multi-modal fusion strategies for more robust and accurate 3D object detection. However, current researches for robust fusion are all learning-based frameworks, which demand a large amount of training data and are inconvenient to implement in new scenes. In this paper, we propose GOOD, a general optimization-based fusion framework that can achieve satisfying detection without training additional models and is available for any combinations of 2D and 3D detectors to improve the accuracy and robustness of 3D detection. First we apply the mutual-sided nearest-neighbor probability model to achieve the 3D-2D data association. Then we design an optimization pipeline that can optimize different kinds of instances separately based on the matching result. Apart from this, the 3D MOT method is also introduced to enhance the performance aided by previous frames.
&lt;/p&gt;</description></item><item><title>&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#38754;&#20020;&#30528;&#22686;&#37327;&#26356;&#26032;&#21644;&#27493;&#39588;&#20043;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#20027;&#35201;&#22270;&#27169;&#22411;&#21644;&#26500;&#24314;&#27969;&#31243;&#38656;&#27714;&#65292;&#23545;&#26500;&#24314;&#39640;&#36136;&#37327;&#30693;&#35782;&#22270;&#35889;&#30340;&#24517;&#35201;&#27493;&#39588;&#36827;&#34892;&#20102;&#27010;&#36848;&#65292;&#24182;&#35780;&#20272;&#20102;&#29616;&#26377;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#30340;&#30740;&#31350;&#29616;&#29366;&#12290;</title><link>http://arxiv.org/abs/2302.11509</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#30340;&#26500;&#24314;&#65306;&#29616;&#29366;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Construction of Knowledge Graphs: State and Challenges. (arXiv:2302.11509v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11509
&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#38754;&#20020;&#30528;&#22686;&#37327;&#26356;&#26032;&#21644;&#27493;&#39588;&#20043;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#20027;&#35201;&#22270;&#27169;&#22411;&#21644;&#26500;&#24314;&#27969;&#31243;&#38656;&#27714;&#65292;&#23545;&#26500;&#24314;&#39640;&#36136;&#37327;&#30693;&#35782;&#22270;&#35889;&#30340;&#24517;&#35201;&#27493;&#39588;&#36827;&#34892;&#20102;&#27010;&#36848;&#65292;&#24182;&#35780;&#20272;&#20102;&#29616;&#26377;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#30340;&#30740;&#31350;&#29616;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#30693;&#35782;&#22270;&#35889;&#22312;&#35832;&#22810;&#24212;&#29992;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#65292;&#22914;&#25512;&#33616;&#31995;&#32479;&#21644;&#38382;&#31572;&#31995;&#32479;&#65292;&#26500;&#24314;&#21644;&#25345;&#32493;&#26356;&#26032;&#30693;&#35782;&#22270;&#35889;&#30340;&#36890;&#29992;&#27969;&#31243;&#30340;&#38656;&#27714;&#27491;&#22312;&#22686;&#21152;&#12290;&#34429;&#28982;&#20174;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#65288;&#22914;&#25991;&#26412;&#65289;&#21644;&#32467;&#26500;&#21270;&#25968;&#25454;&#28304;&#65288;&#22914;&#25968;&#25454;&#24211;&#65289;&#21019;&#24314;&#30693;&#35782;&#22270;&#35889;&#30340;&#21508;&#20010;&#27493;&#39588;&#24050;&#32463;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#26159;&#30446;&#21069;&#23545;&#20110;&#22686;&#37327;&#26356;&#26032;&#30693;&#35782;&#22270;&#35889;&#21644;&#21508;&#20010;&#27493;&#39588;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#30340;&#30740;&#31350;&#36824;&#24456;&#26377;&#38480;&#12290;&#26412;&#25991;&#39318;&#20808;&#35752;&#35770;&#20102;&#30693;&#35782;&#22270;&#35889;&#30340;&#20027;&#35201;&#22270;&#27169;&#22411;&#65292;&#24182;&#20171;&#32461;&#20102;&#26410;&#26469;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#27969;&#31243;&#30340;&#20027;&#35201;&#38656;&#27714;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#26500;&#24314;&#39640;&#36136;&#37327;&#30693;&#35782;&#22270;&#35889;&#25152;&#38656;&#30340;&#24517;&#35201;&#27493;&#39588;&#65292;&#21253;&#25324;&#20803;&#25968;&#25454;&#31649;&#29702;&#12289;&#26412;&#20307;&#21457;&#23637;&#21644;&#36136;&#37327;&#20445;&#35777;&#31561;&#20132;&#21449;&#20027;&#39064;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#19982;&#24050;&#20171;&#32461;&#30340;&#38656;&#27714;&#30456;&#19968;&#33268;&#30340;&#29305;&#23450;&#30693;&#35782;&#22270;&#35889;&#30340;&#26500;&#24314;&#29616;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;
With knowledge graphs (KGs) at the center of numerous applications such as recommender systems and question answering, the need for generalized pipelines to construct and continuously update such KGs is increasing. While the individual steps that are necessary to create KGs from unstructured (e.g. text) and structured data sources (e.g. databases) are mostly well-researched for their one-shot execution, their adoption for incremental KG updates and the interplay of the individual steps have hardly been investigated in a systematic manner so far. In this work, we first discuss the main graph models for KGs and introduce the major requirement for future KG construction pipelines. Next, we provide an overview of the necessary steps to build high-quality KGs, including cross-cutting topics such as metadata management, ontology development, and quality assurance. We then evaluate the state of the art of KG construction w.r.t the introduced requirements for specific popular KGs as well as so
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#22836;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;&#25277;&#35937;&#20195;&#25968;&#21644;&#23450;&#24615;&#27010;&#24565;&#30340;&#30456;&#20284;&#24615;&#27010;&#24565;&#65292;&#24182;&#23558;&#20854;&#36890;&#36807;&#27169;&#22411;&#35770;&#31867;&#22411;&#33258;&#28982;&#22320;&#23884;&#20837;&#21040;&#19968;&#38454;&#36923;&#36753;&#20013;&#12290;</title><link>http://arxiv.org/abs/2302.10096</link><description>&lt;p&gt;
&#22522;&#20110;&#27867;&#21270;&#30340;&#30456;&#20284;&#24615;
&lt;/p&gt;
&lt;p&gt;
Generalization-based similarity. (arXiv:2302.10096v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10096
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#22836;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;&#25277;&#35937;&#20195;&#25968;&#21644;&#23450;&#24615;&#27010;&#24565;&#30340;&#30456;&#20284;&#24615;&#27010;&#24565;&#65292;&#24182;&#23558;&#20854;&#36890;&#36807;&#27169;&#22411;&#35770;&#31867;&#22411;&#33258;&#28982;&#22320;&#23884;&#20837;&#21040;&#19968;&#38454;&#36923;&#36753;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#27979;&#21644;&#21033;&#29992;&#30475;&#20284;&#19981;&#30456;&#20851;&#30340;&#23545;&#35937;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#26159;&#31867;&#27604;&#25512;&#29702;&#30340;&#26680;&#24515;&#65292;&#32780;&#31867;&#27604;&#25512;&#29702;&#21448;&#26159;&#20154;&#24037;&#26234;&#33021;&#30340;&#26680;&#24515;&#12290;&#26412;&#25991;&#20174;&#22836;&#24320;&#22987;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#25277;&#35937;&#20195;&#25968;&#21644;&#23450;&#24615;&#27010;&#24565;&#30340;&#30456;&#20284;&#24615;&#27010;&#24565;&#65292;&#22522;&#20110;&#35266;&#23519;&#21040;&#27867;&#21270;&#38598;&#21512;&#21487;&#20197;&#32534;&#30721;&#20803;&#32032;&#30340;&#37325;&#35201;&#23646;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20197;&#36825;&#31181;&#26041;&#24335;&#23450;&#20041;&#30340;&#30456;&#20284;&#24615;&#20855;&#26377;&#21560;&#24341;&#20154;&#30340;&#25968;&#23398;&#23646;&#24615;&#12290;&#36890;&#36807;&#20174;&#22522;&#26412;&#27010;&#24565;&#20986;&#21457;&#26500;&#24314;&#30456;&#20284;&#24615;&#27010;&#24565;&#65292;&#24182;&#23558;&#20854;&#33258;&#28982;&#22320;&#23884;&#20837;&#21040;&#27169;&#22411;&#35770;&#31867;&#22411;&#20013;&#30340;&#19968;&#38454;&#36923;&#36753;&#20013;&#65292;&#25105;&#20204;&#20351;&#35835;&#32773;&#30456;&#20449;&#20854;&#21512;&#29702;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detecting and exploiting similarities between seemingly distant objects is at the core of analogical reasoning which itself is at the core of artificial intelligence. This paper develops {\em from the ground up} an abstract algebraic and {\em qualitative} notion of similarity based on the observation that sets of generalizations encode important properties of elements. We show that similarity defined in this way has appealing mathematical properties. As we construct our notion of similarity from first principles using only elementary concepts of universal algebra, to convince the reader of its plausibility, we show that it can be naturally embedded into first-order logic via model-theoretic types.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#38754;&#37096;&#34920;&#31034;&#22686;&#24378;&#65288;FRA&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#25805;&#32437;&#20219;&#20309;&#38754;&#37096;&#34920;&#31034;&#23398;&#20064;&#31639;&#27861;&#29983;&#25104;&#30340;&#38754;&#37096;&#23884;&#20837;&#65292;&#21019;&#36896;&#20986;&#20855;&#26377;&#25913;&#21464;&#23039;&#21183;&#30340;&#26032;&#23884;&#20837;&#65292;&#34920;&#31034;&#30456;&#21516;&#36523;&#20221;&#21644;&#38754;&#37096;&#24773;&#24863;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#25552;&#20379;&#26080;&#22122;&#22768;&#12289;&#20840;&#26032;&#30340;&#38754;&#37096;&#34920;&#31034;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2301.11986</link><description>&lt;p&gt;
&#36890;&#36807;&#28508;&#22312;&#31354;&#38388;&#25968;&#25454;&#22686;&#24378;&#21644;&#38754;&#37096;&#23039;&#21183;&#37325;&#24314;&#25913;&#36827;&#20154;&#33080;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Enhancing Face Recognition with Latent Space Data Augmentation and Facial Posture Reconstruction. (arXiv:2301.11986v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11986
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#38754;&#37096;&#34920;&#31034;&#22686;&#24378;&#65288;FRA&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#25805;&#32437;&#20219;&#20309;&#38754;&#37096;&#34920;&#31034;&#23398;&#20064;&#31639;&#27861;&#29983;&#25104;&#30340;&#38754;&#37096;&#23884;&#20837;&#65292;&#21019;&#36896;&#20986;&#20855;&#26377;&#25913;&#21464;&#23039;&#21183;&#30340;&#26032;&#23884;&#20837;&#65292;&#34920;&#31034;&#30456;&#21516;&#36523;&#20221;&#21644;&#38754;&#37096;&#24773;&#24863;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#25552;&#20379;&#26080;&#22122;&#22768;&#12289;&#20840;&#26032;&#30340;&#38754;&#37096;&#34920;&#31034;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#22823;&#37096;&#20998;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20154;&#33080;&#35782;&#21035;&#31995;&#32479;&#30001;&#20110;&#35757;&#32451;&#25968;&#25454;&#37327;&#19981;&#36275;&#32780;&#34920;&#29616;&#20986;&#26126;&#26174;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#23613;&#31649;&#24456;&#22810;&#30740;&#31350;&#24050;&#32463;&#30740;&#21457;&#20102;&#26032;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#21253;&#25324;&#36755;&#20837;&#31354;&#38388;&#30340;&#21464;&#25442;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#30340;&#29305;&#24449;&#31354;&#38388;&#22686;&#24378;&#65292;&#20294;&#36825;&#20123;&#25216;&#26415;&#20173;&#26080;&#27861;&#28385;&#36275;&#39044;&#26399;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#38754;&#37096;&#34920;&#31034;&#22686;&#24378;&#65288;FRA&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22686;&#24378;&#20154;&#33080;&#25968;&#25454;&#38598;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;FRA&#26159;&#31532;&#19968;&#20010;&#23558;&#37325;&#28857;&#25918;&#22312;&#36890;&#36807;&#20219;&#20309;&#38754;&#37096;&#34920;&#31034;&#23398;&#20064;&#31639;&#27861;&#29983;&#25104;&#30340;&#38754;&#37096;&#23884;&#20837;&#26469;&#21019;&#24314;&#20195;&#34920;&#30456;&#21516;&#36523;&#20221;&#21644;&#38754;&#37096;&#24773;&#24863;&#20294;&#20855;&#26377;&#25913;&#21464;&#23039;&#21183;&#30340;&#26032;&#23884;&#20837;&#30340;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#20854;&#25552;&#20379;&#26080;&#22122;&#22768;&#12289;&#20840;&#26032;&#30340;&#38754;&#37096;&#34920;&#31034;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The small amount of training data for many state-of-the-art deep learning-based Face Recognition (FR) systems causes a marked deterioration in their performance. Although a considerable amount of research has addressed this issue by inventing new data augmentation techniques, using either input space transformations or Generative Adversarial Networks (GAN) for feature space augmentations, these techniques have yet to satisfy expectations. In this paper, we propose an approach named the Face Representation Augmentation (FRA) for augmenting face datasets. To the best of our knowledge, FRA is the first method that shifts its focus towards manipulating the face embeddings generated by any face representation learning algorithm to create new embeddings representing the same identity and facial emotion but with an altered posture. Extensive experiments conducted in this study convince of the efficacy of our methodology and its power to provide noiseless, completely new facial representations
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#27979;&#37327;&#22522;&#20110;&#20687;&#32032;&#30340;&#30456;&#21516;-&#19981;&#21516;&#21028;&#26029;&#24182;&#22522;&#20110;&#27169;&#22411;&#36827;&#34892;&#37325;&#24314;&#65292;&#26469;&#27979;&#37327;&#20154;&#31867;&#35270;&#35273;&#20998;&#21106;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#34920;&#26126;&#22270;&#20687;&#30340;&#19981;&#30830;&#23450;&#24615;&#24433;&#21709;&#20102;&#20154;&#31867;&#30340;&#21487;&#21464;&#24615;&#30340;&#27979;&#37327;&#65292;&#24182;&#24433;&#21709;&#20102;&#21442;&#19982;&#32773;&#30340;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2301.07807</link><description>&lt;p&gt;
&#20154;&#31867;&#35270;&#35273;&#20998;&#21106;&#30340;&#19981;&#30830;&#23450;&#24615;&#27979;&#37327;
&lt;/p&gt;
&lt;p&gt;
Measuring uncertainty in human visual segmentation. (arXiv:2301.07807v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07807
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#27979;&#37327;&#22522;&#20110;&#20687;&#32032;&#30340;&#30456;&#21516;-&#19981;&#21516;&#21028;&#26029;&#24182;&#22522;&#20110;&#27169;&#22411;&#36827;&#34892;&#37325;&#24314;&#65292;&#26469;&#27979;&#37327;&#20154;&#31867;&#35270;&#35273;&#20998;&#21106;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#34920;&#26126;&#22270;&#20687;&#30340;&#19981;&#30830;&#23450;&#24615;&#24433;&#21709;&#20102;&#20154;&#31867;&#30340;&#21487;&#21464;&#24615;&#30340;&#27979;&#37327;&#65292;&#24182;&#24433;&#21709;&#20102;&#21442;&#19982;&#32773;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#35270;&#35273;&#21050;&#28608;&#20998;&#21106;&#20026;&#29305;&#24449;&#21644;&#35270;&#35273;&#23545;&#35937;&#30340;&#19981;&#21516;&#32452;&#26159;&#35270;&#35273;&#21151;&#33021;&#30340;&#26680;&#24515;&#12290;&#32463;&#20856;&#30340;&#24515;&#29702;&#29289;&#29702;&#26041;&#27861;&#25581;&#31034;&#20102;&#35768;&#22810;&#26377;&#20851;&#20154;&#31867;&#24863;&#30693;&#20998;&#21106;&#30340;&#35268;&#21017;&#65292;&#32780;&#26368;&#36817;&#22312;&#26426;&#22120;&#23398;&#20064;&#26041;&#38754;&#30340;&#36827;&#23637;&#24050;&#32463;&#20135;&#29983;&#20102;&#25104;&#21151;&#30340;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#20998;&#21106;&#30340;&#35745;&#31639;&#36923;&#36753;&#20173;&#19981;&#28165;&#26970;&#65292;&#37096;&#20998;&#21407;&#22240;&#26159;&#22240;&#20026;&#25105;&#20204;&#32570;&#20047;&#21487;&#25511;&#21046;&#30340;&#33539;&#20363;&#26469;&#27979;&#37327;&#24863;&#30693;&#20998;&#21106;&#22270;&#24182;&#23450;&#37327;&#27604;&#36739;&#27169;&#22411;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32508;&#21512;&#26041;&#27861;&#65306;&#32473;&#23450;&#19968;&#24352;&#22270;&#29255;&#65292;&#25105;&#20204;&#27979;&#37327;&#22810;&#20010;&#22522;&#20110;&#20687;&#32032;&#30340;&#30456;&#21516;-&#19981;&#21516;&#21028;&#26029;&#65292;&#24182;&#23545;&#28508;&#22312;&#30340;&#20998;&#21106;&#22270;&#36827;&#34892;&#22522;&#20110;&#27169;&#22411;&#30340;&#37325;&#24314;&#12290;&#37325;&#24314;&#23545;&#22810;&#31181;&#23454;&#39564;&#25805;&#20316;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#25429;&#25417;&#20102;&#20010;&#20307;&#21442;&#19982;&#32773;&#30340;&#21464;&#24322;&#24615;&#12290;&#25105;&#20204;&#22312;&#20154;&#31867;&#23545;&#33258;&#28982;&#22270;&#20687;&#21644;&#22797;&#21512;&#26448;&#26009;&#32441;&#29702;&#30340;&#20998;&#21106;&#19978;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#34920;&#26126;&#22270;&#20687;&#30340;&#19981;&#30830;&#23450;&#24615;&#24433;&#21709;&#20102;&#20154;&#31867;&#30340;&#21487;&#21464;&#24615;&#30340;&#27979;&#37327;&#65292;&#24182;&#24433;&#21709;&#20102;&#21442;&#19982;&#32773;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
Segmenting visual stimuli into distinct groups of features and visual objects is central to visual function. Classical psychophysical methods have helped uncover many rules of human perceptual segmentation, and recent progress in machine learning has produced successful algorithms. Yet, the computational logic of human segmentation remains unclear, partially because we lack well-controlled paradigms to measure perceptual segmentation maps and compare models quantitatively. Here we propose a new, integrated approach: given an image, we measure multiple pixel-based same--different judgments and perform model--based reconstruction of the underlying segmentation map. The reconstruction is robust to several experimental manipulations and captures the variability of individual participants. We demonstrate the validity of the approach on human segmentation of natural images and composite textures. We show that image uncertainty affects measured human variability, and it influences how partici
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#38598;&#20307;&#20915;&#31574;&#27169;&#22411;&#65292;&#32771;&#34385;&#20102;&#20195;&#29702;&#20154;&#23545;&#20915;&#31574;&#32467;&#26524;&#21644;&#36807;&#31243;&#30340;&#20851;&#27880;&#65292;&#36890;&#36807;&#24320;&#21457;&#32858;&#21512;&#20559;&#22909;&#30340;&#26426;&#21046;&#26469;&#26368;&#22823;&#21270;&#20915;&#31574;&#30340;&#21487;&#25509;&#21463;&#24615;&#65292;&#24182;&#22312;&#29305;&#23450;&#24773;&#26223;&#20013;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2211.08501</link><description>&lt;p&gt;
&#31038;&#20250;&#26426;&#21046;&#35774;&#35745;&#65306;&#23454;&#29616;&#26368;&#22823;&#21487;&#25509;&#21463;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Social Mechanism Design: Making Maximally Acceptable Decisions. (arXiv:2211.08501v2 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08501
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#38598;&#20307;&#20915;&#31574;&#27169;&#22411;&#65292;&#32771;&#34385;&#20102;&#20195;&#29702;&#20154;&#23545;&#20915;&#31574;&#32467;&#26524;&#21644;&#36807;&#31243;&#30340;&#20851;&#27880;&#65292;&#36890;&#36807;&#24320;&#21457;&#32858;&#21512;&#20559;&#22909;&#30340;&#26426;&#21046;&#26469;&#26368;&#22823;&#21270;&#20915;&#31574;&#30340;&#21487;&#25509;&#21463;&#24615;&#65292;&#24182;&#22312;&#29305;&#23450;&#24773;&#26223;&#20013;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#29702;&#20154;&#19981;&#20165;&#20851;&#24515;&#38598;&#20307;&#20915;&#31574;&#30340;&#32467;&#26524;&#65292;&#36824;&#20851;&#24515;&#20915;&#31574;&#30340;&#26041;&#24335;&#12290;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#20915;&#31574;&#30340;&#32467;&#26524;&#21644;&#36807;&#31243;&#37117;&#20250;&#24433;&#21709;&#20195;&#29702;&#20154;&#26159;&#21542;&#35748;&#20026;&#20915;&#31574;&#26159;&#21512;&#27861;&#12289;&#21487;&#36777;&#35299;&#25110;&#21487;&#25509;&#21463;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#38598;&#20307;&#20915;&#31574;&#27169;&#22411;&#65292;&#32771;&#34385;&#20102;&#20195;&#29702;&#20154;&#30340;&#20559;&#22909;&#21644;&#23545;&#20559;&#22909;&#32858;&#21512;&#36807;&#31243;&#30340;&#39640;&#38454;&#20851;&#27880;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#65288;1&#65289;&#25552;&#20986;&#20102;&#33258;&#28982;&#12289;&#21487;&#20449;&#30340;&#20559;&#22909;&#32467;&#26500;&#24182;&#24314;&#31435;&#20102;&#20854;&#20851;&#38190;&#23646;&#24615;&#65307;&#65288;2&#65289;&#24320;&#21457;&#20102;&#32858;&#21512;&#36825;&#20123;&#20559;&#22909;&#20197;&#26368;&#22823;&#21270;&#20915;&#31574;&#30340;&#25509;&#21463;&#24230;&#30340;&#26426;&#21046;&#65307;&#65288;3&#65289;&#23545;&#25105;&#20204;&#30340;&#26368;&#22823;&#21270;&#25509;&#21463;&#26426;&#21046;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#34920;&#24449;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#36890;&#29992;&#26041;&#27861;&#24212;&#29992;&#20110;&#20108;&#36873;&#19968;&#36873;&#25321;&#30340;&#20855;&#20307;&#24773;&#26223;&#65292;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;&#31867;&#22411;&#30340;&#20195;&#29702;&#20154;&#32676;&#20307;&#20013;&#21487;&#23454;&#29616;&#30340;&#26368;&#24046;&#25509;&#21463;&#29575;&#12290;&#25105;&#20204;&#36824;&#22312;&#20462;&#27491;&#31243;&#24207;&#30340;&#29305;&#27530;&#24773;&#20917;&#19979;&#23637;&#31034;&#20102;&#36890;&#36807;&#38463;&#24067;&#25289;&#33707;&#32500;&#33576;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Agents care not only about the outcomes of collective decisions but also about how decisions are made. In many cases, both the outcome and the procedure affect whether agents see a decision as legitimate, justifiable, or acceptable. We propose a novel model for collective decisions that takes into account both the preferences of the agents and their higher order concerns about the process of preference aggregation. To this end we (1) propose natural, plausible preference structures and establish key properties thereof, (2) develop mechanisms for aggregating these preferences to maximize the acceptability of decisions, and (3) characterize the performance of our acceptance-maximizing mechanisms. We apply our general approach to the specific setting of dichotomous choice, and compare the worst-case rates of acceptance achievable among populations of agents of different types. We also show in the special case of rule selection, i.e., amendment procedures, the method proposed by Abramowitz
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#24320;&#25918;&#24335;&#23398;&#20064;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#21516;&#26102;&#28436;&#21270;&#26234;&#33021;&#20307;&#21644;&#26356;&#20855;&#25361;&#25112;&#24615;&#29615;&#22659;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20801;&#35768;&#26234;&#33021;&#20307;&#30340;&#25511;&#21046;&#22120;&#21464;&#24471;&#26356;&#21152;&#22797;&#26434;&#65292;&#25105;&#20204;&#21457;&#29616;&#21487;&#20197;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20135;&#29983;&#36890;&#29992;&#26234;&#33021;&#20307;&#65292;&#33021;&#22815;&#35299;&#20915;&#26356;&#22810;&#29615;&#22659;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2210.11442</link><description>&lt;p&gt;
&#23545;&#24320;&#25918;&#24335;&#23398;&#20064;&#36827;&#34892;&#22686;&#24378;&#25299;&#25169;&#26234;&#33021;&#20307;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Augmentative Topology Agents For Open-Ended Learning. (arXiv:2210.11442v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.11442
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#24320;&#25918;&#24335;&#23398;&#20064;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#21516;&#26102;&#28436;&#21270;&#26234;&#33021;&#20307;&#21644;&#26356;&#20855;&#25361;&#25112;&#24615;&#29615;&#22659;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20801;&#35768;&#26234;&#33021;&#20307;&#30340;&#25511;&#21046;&#22120;&#21464;&#24471;&#26356;&#21152;&#22797;&#26434;&#65292;&#25105;&#20204;&#21457;&#29616;&#21487;&#20197;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20135;&#29983;&#36890;&#29992;&#26234;&#33021;&#20307;&#65292;&#33021;&#22815;&#35299;&#20915;&#26356;&#22810;&#29615;&#22659;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#21516;&#26102;&#28436;&#21270;&#26234;&#33021;&#20307;&#21644;&#26085;&#30410;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29615;&#22659;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#24320;&#25918;&#24335;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;&#19982;&#20043;&#21069;&#20248;&#21270;&#26234;&#33021;&#20307;&#20351;&#29992;&#22266;&#23450;&#31070;&#32463;&#32593;&#32476;&#25299;&#25169;&#30340;&#24320;&#25918;&#24335;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#20551;&#35774;&#36890;&#36807;&#20801;&#35768;&#26234;&#33021;&#20307;&#30340;&#25511;&#21046;&#22120;&#22312;&#36935;&#21040;&#26356;&#22256;&#38590;&#30340;&#29615;&#22659;&#26102;&#21464;&#24471;&#26356;&#21152;&#22797;&#26434;&#65292;&#21487;&#20197;&#25913;&#21892;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#22686;&#24378;&#25299;&#25169;EPOET&#65288;ATEP&#65289;&#65292;&#36890;&#36807;&#20801;&#35768;&#26234;&#33021;&#20307;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#28436;&#21270;&#20986;&#33258;&#24049;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#26681;&#25454;&#38656;&#35201;&#22686;&#21152;&#22797;&#26434;&#24615;&#21644;&#23481;&#37327;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;ATEP&#21487;&#20197;&#20135;&#29983;&#33021;&#22815;&#35299;&#20915;&#26356;&#22810;&#29615;&#22659;&#30340;&#36890;&#29992;&#26234;&#33021;&#20307;&#65292;&#32780;&#22266;&#23450;&#25299;&#25169;&#22522;&#32447;&#21017;&#26080;&#27861;&#20570;&#21040;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#26234;&#33021;&#20307;&#22312;&#29615;&#22659;&#20043;&#38388;&#30340;&#36716;&#31227;&#26426;&#21046;&#65292;&#21457;&#29616;&#22522;&#20110;&#29289;&#31181;&#30340;&#26041;&#27861;&#36827;&#19968;&#27493;&#25913;&#21892;&#20102;&#26234;&#33021;&#20307;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we tackle the problem of open-ended learning by introducing a method that simultaneously evolves agents and increasingly challenging environments. Unlike previous open-ended approaches that optimize agents using a fixed neural network topology, we hypothesize that generalization can be improved by allowing agents' controllers to become more complex as they encounter more difficult environments. Our method, Augmentative Topology EPOET (ATEP), extends the Enhanced Paired Open-Ended Trailblazer (EPOET) algorithm by allowing agents to evolve their own neural network structures over time, adding complexity and capacity as necessary. Empirical results demonstrate that ATEP results in general agents capable of solving more environments than a fixed-topology baseline. We also investigate mechanisms for transferring agents between environments and find that a species-based approach further improves the performance and generalization of agents.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#30452;&#27604;&#20195;&#25968;&#65292;&#25506;&#35752;&#20102;&#20445;&#25345;&#27604;&#25311;&#27604;&#20363;&#30340;&#20989;&#25968;&#30340;&#25968;&#23398;&#24615;&#36136;&#65292;&#23558;&#20854;&#19982;&#30452;&#27604;&#21516;&#24577;&#12289;&#21516;&#20313;&#21644;&#30452;&#27604;&#20989;&#23376;&#32852;&#31995;&#36215;&#26469;&#65292;&#20026;&#27169;&#25311;&#27604;&#20363;&#21644;&#27169;&#25311;&#25512;&#29702;&#30340;&#25968;&#23398;&#29702;&#35770;&#25552;&#20379;&#20102;&#36827;&#19968;&#27493;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2210.01751</link><description>&lt;p&gt;
&#30452;&#27604;&#20195;&#25968;
&lt;/p&gt;
&lt;p&gt;
Proportional algebras. (arXiv:2210.01751v4 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.01751
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#30452;&#27604;&#20195;&#25968;&#65292;&#25506;&#35752;&#20102;&#20445;&#25345;&#27604;&#25311;&#27604;&#20363;&#30340;&#20989;&#25968;&#30340;&#25968;&#23398;&#24615;&#36136;&#65292;&#23558;&#20854;&#19982;&#30452;&#27604;&#21516;&#24577;&#12289;&#21516;&#20313;&#21644;&#30452;&#27604;&#20989;&#23376;&#32852;&#31995;&#36215;&#26469;&#65292;&#20026;&#27169;&#25311;&#27604;&#20363;&#21644;&#27169;&#25311;&#25512;&#29702;&#30340;&#25968;&#23398;&#29702;&#35770;&#25552;&#20379;&#20102;&#36827;&#19968;&#27493;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27604;&#25311;&#27604;&#20363;&#26159;&#24418;&#24335;&#20026;"$a$&#23545;$b$&#65292;&#27491;&#22914;$c$&#23545;$d$"&#30340;&#34920;&#36798;&#24335;&#65292;&#26159;&#27169;&#25311;&#25512;&#29702;&#30340;&#26680;&#24515;&#65292;&#32780;&#27169;&#25311;&#25512;&#29702;&#21448;&#26159;&#20154;&#24037;&#26234;&#33021;&#30340;&#26680;&#24515;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#30452;&#27604;&#20195;&#25968;&#20316;&#20026;&#19968;&#31181;&#24102;&#26377;4&#20803;&#27604;&#25311;&#27604;&#20363;&#20851;&#31995;$a:b::c:d$&#28385;&#36275;&#19968;&#32452;&#36866;&#24403;&#20844;&#29702;&#30340;&#20195;&#25968;&#32467;&#26500;&#12290;&#22312;&#20154;&#24037;&#26234;&#33021;&#20013;&#65292;&#20445;&#25345;&#27604;&#25311;&#27604;&#20363;&#30340;&#20989;&#25968;&#24050;&#32463;&#34987;&#35777;&#26126;&#20855;&#26377;&#23454;&#38469;&#24847;&#20041;&#65292;&#32780;&#30740;&#31350;&#23427;&#20204;&#30340;&#25968;&#23398;&#24615;&#36136;&#23545;&#20110;&#29702;&#35299;&#27604;&#20363;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#30452;&#27604;&#21516;&#24577;&#21450;&#20854;&#20851;&#32852;&#30340;&#21516;&#20313;&#21644;&#30452;&#27604;&#20989;&#23376;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#23494;&#20999;&#20851;&#32852;&#12290;&#20174;&#26356;&#24191;&#27867;&#30340;&#24847;&#20041;&#19978;&#35828;&#65292;&#26412;&#25991;&#26159;&#36808;&#21521;&#27169;&#25311;&#27604;&#20363;&#21644;&#27169;&#25311;&#25512;&#29702;&#30340;&#25968;&#23398;&#29702;&#35770;&#30340;&#36827;&#19968;&#27493;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
Analogical proportions are expressions of the form "$a$ is to $b$ what $c$ is to $d$" at the core of analogical reasoning which itself is at the core of artificial intelligence. This paper introduces proportional algebras as algebras endowed with a 4-ary analogical proportion relation $a:b::c:d$ satisfying a suitable set of axioms. Functions preserving analogical proportions have already proven to be of practical interest in artificial intelligence and studying their mathematical properties is essential for understanding proportions. We therefore introduce proportional homomorphisms and their associated congruences and proportional functors, and show that they are closely related notions. In a broader sense, this paper is a further step towards a mathematical theory of analogical proportions and analogical reasoning in general.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21338;&#24328;&#35770;&#30446;&#26631;&#31354;&#38388;&#35268;&#21010;&#26041;&#27861;&#65292;&#21516;&#26102;&#29983;&#25104;&#31454;&#20105;&#31574;&#30053;&#21644;&#25191;&#34892;&#36830;&#32493;&#36816;&#21160;&#35268;&#21010;&#65292;&#36890;&#36807;&#25277;&#35937;&#23454;&#29616;&#26234;&#33021;&#20307;&#21160;&#20316;&#31163;&#25955;&#21270;&#65292;&#25552;&#20379;&#28165;&#26224;&#30340;&#24847;&#22270;&#65292;&#20197;&#21450;&#20351;&#29992;&#23545;&#31574;&#22238;&#24402;&#26368;&#23567;&#21270;&#30340;&#35268;&#21010;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2209.07758</link><description>&lt;p&gt;
&#21338;&#24328;&#35770;&#30446;&#26631;&#31354;&#38388;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Game-theoretic Objective Space Planning. (arXiv:2209.07758v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.07758
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21338;&#24328;&#35770;&#30446;&#26631;&#31354;&#38388;&#35268;&#21010;&#26041;&#27861;&#65292;&#21516;&#26102;&#29983;&#25104;&#31454;&#20105;&#31574;&#30053;&#21644;&#25191;&#34892;&#36830;&#32493;&#36816;&#21160;&#35268;&#21010;&#65292;&#36890;&#36807;&#25277;&#35937;&#23454;&#29616;&#26234;&#33021;&#20307;&#21160;&#20316;&#31163;&#25955;&#21270;&#65292;&#25552;&#20379;&#28165;&#26224;&#30340;&#24847;&#22270;&#65292;&#20197;&#21450;&#20351;&#29992;&#23545;&#31574;&#22238;&#24402;&#26368;&#23567;&#21270;&#30340;&#35268;&#21010;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23545;&#25239;&#24615;&#29615;&#22659;&#20013;&#21516;&#26102;&#29983;&#25104;&#31454;&#20105;&#31574;&#30053;&#21644;&#25191;&#34892;&#36830;&#32493;&#36816;&#21160;&#35268;&#21010;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#22312;&#23545;&#25239;&#24615;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#37096;&#32626;&#33258;&#20027;&#31995;&#32479;&#38656;&#35201;&#29702;&#35299;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#24847;&#22270;&#12290;&#29616;&#26377;&#26041;&#27861;&#35201;&#20040;&#36890;&#36807;&#23545;&#31867;&#20284;&#25511;&#21046;&#36755;&#20837;&#36827;&#34892;&#20998;&#32452;&#26469;&#31163;&#25955;&#21270;&#26234;&#33021;&#20307;&#21160;&#20316;&#65292;&#20174;&#32780;&#29306;&#29298;&#36816;&#21160;&#35268;&#21010;&#30340;&#24615;&#33021;&#65292;&#35201;&#20040;&#22312;&#38590;&#20197;&#29702;&#35299;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#36827;&#34892;&#35268;&#21010;&#65292;&#20135;&#29983;&#38590;&#20197;&#29702;&#35299;&#30340;&#26234;&#33021;&#20307;&#34892;&#20026;&#12290;&#27492;&#22806;&#65292;&#26368;&#27969;&#34892;&#30340;&#31574;&#30053;&#20248;&#21270;&#26694;&#26550;&#27809;&#26377;&#24847;&#35782;&#21040;&#34892;&#21160;&#30340;&#38271;&#26399;&#24433;&#21709;&#24182;&#19988;&#21464;&#24471;&#30446;&#20809;&#30701;&#27973;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25277;&#35937;&#23454;&#29616;&#26234;&#33021;&#20307;&#21160;&#20316;&#31163;&#25955;&#21270;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#25552;&#20379;&#20102;&#26234;&#33021;&#20307;&#21160;&#20316;&#30340;&#28165;&#26224;&#24847;&#22270;&#65292;&#19968;&#20010;&#39640;&#25928;&#30340;&#31163;&#32447;&#26234;&#33021;&#20307;&#32676;&#20307;&#21512;&#25104;&#27969;&#31243;&#20197;&#21450;&#20351;&#29992;&#20989;&#25968;&#36924;&#36817;&#30340;&#23545;&#31574;&#22238;&#24402;&#26368;&#23567;&#21270;&#30340;&#35268;&#21010;&#31574;&#30053;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#25353;&#27604;&#20363;&#32553;&#23567;&#30340;&#33258;&#20027;&#39550;&#39542;&#36710;&#36742;&#36827;&#34892;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating competitive strategies and performing continuous motion planning simultaneously in an adversarial setting is a challenging problem. In addition, understanding the intent of other agents is crucial to deploying autonomous systems in adversarial multi-agent environments. Existing approaches either discretize agent action by grouping similar control inputs, sacrificing performance in motion planning, or plan in uninterpretable latent spaces, producing hard-to-understand agent behaviors. Furthermore, the most popular policy optimization frameworks do not recognize the long-term effect of actions and become myopic. This paper proposes an agent action discretization method via abstraction that provides clear intentions of agent actions, an efficient offline pipeline of agent population synthesis, and a planning strategy using counterfactual regret minimization with function approximation. Finally, we experimentally validate our findings on scaled autonomous vehicles in a head-to-h
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#30740;&#32508;&#21512;&#24635;&#32467;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#26041;&#27861;&#21644;&#24212;&#29992;&#30740;&#31350;&#65292;&#21253;&#25324;&#39640;&#25928;&#37319;&#26679;&#12289;&#20284;&#28982;&#20272;&#35745;&#19982;&#29305;&#27530;&#32467;&#26500;&#25968;&#25454;&#22788;&#29702;&#65292;&#20171;&#32461;&#20102;&#25193;&#25955;&#27169;&#22411;&#19982;&#20854;&#20182;&#29983;&#25104;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#28508;&#21147;&#24182;&#22238;&#39038;&#20102;&#20854;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#25552;&#20986;&#20102;&#21487;&#33021;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2209.00796</link><description>&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#65306;&#26041;&#27861;&#21644;&#24212;&#29992;&#30340;&#32508;&#21512;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Diffusion Models: A Comprehensive Survey of Methods and Applications. (arXiv:2209.00796v10 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.00796
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#30740;&#32508;&#21512;&#24635;&#32467;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#26041;&#27861;&#21644;&#24212;&#29992;&#30740;&#31350;&#65292;&#21253;&#25324;&#39640;&#25928;&#37319;&#26679;&#12289;&#20284;&#28982;&#20272;&#35745;&#19982;&#29305;&#27530;&#32467;&#26500;&#25968;&#25454;&#22788;&#29702;&#65292;&#20171;&#32461;&#20102;&#25193;&#25955;&#27169;&#22411;&#19982;&#20854;&#20182;&#29983;&#25104;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#28508;&#21147;&#24182;&#22238;&#39038;&#20102;&#20854;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#25552;&#20986;&#20102;&#21487;&#33021;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#24050;&#25104;&#20026;&#19968;&#31867;&#20855;&#26377;&#35760;&#24405;&#24615;&#33021;&#30340;&#24378;&#22823;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#21253;&#25324;&#22270;&#20687;&#21512;&#25104;&#12289;&#35270;&#39057;&#29983;&#25104;&#21644;&#20998;&#23376;&#35774;&#35745;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#22312;&#36825;&#20221;&#35843;&#30740;&#20013;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#20851;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#24555;&#36895;&#25193;&#23637;&#30740;&#31350;&#65292;&#23558;&#30740;&#31350;&#20998;&#31867;&#20026;&#19977;&#20010;&#20851;&#38190;&#39046;&#22495;&#65306;&#39640;&#25928;&#37319;&#26679;&#12289;&#25913;&#36827;&#30340;&#20284;&#28982;&#20272;&#35745;&#21644;&#22788;&#29702;&#20855;&#26377;&#29305;&#27530;&#32467;&#26500;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#23558;&#25193;&#25955;&#27169;&#22411;&#19982;&#20854;&#20182;&#29983;&#25104;&#27169;&#22411;&#30456;&#32467;&#21512;&#20197;&#23454;&#29616;&#22686;&#24378;&#32467;&#26524;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#22238;&#39038;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#12289;&#26102;&#38388;&#25968;&#25454;&#24314;&#27169;&#20197;&#21450;&#20854;&#20182;&#31185;&#23398;&#23398;&#31185;&#30340;&#36328;&#23398;&#31185;&#24212;&#29992;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#36825;&#20010;&#35843;&#30740;&#26088;&#22312;&#25552;&#20379;&#19968;&#20010;&#20855;&#26377;&#32972;&#26223;&#30340;&#28145;&#20837;&#20102;&#35299;&#25193;&#25955;&#27169;&#22411;&#30340;&#29616;&#29366;&#65292;&#30830;&#23450;&#20851;&#38190;&#30340;&#30740;&#31350;&#37325;&#28857;&#65292;&#25351;&#26126;&#21487;&#33021;&#30340;&#36827;&#19968;&#27493;&#30740;&#31350;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have emerged as a powerful new family of deep generative models with record-breaking performance in many applications, including image synthesis, video generation, and molecule design. In this survey, we provide an overview of the rapidly expanding body of work on diffusion models, categorizing the research into three key areas: efficient sampling, improved likelihood estimation, and handling data with special structures. We also discuss the potential for combining diffusion models with other generative models for enhanced results. We further review the wide-ranging applications of diffusion models in fields spanning from computer vision, natural language generation, temporal data modeling, to interdisciplinary applications in other scientific disciplines. This survey aims to provide a contextualized, in-depth look at the state of diffusion models, identifying the key areas of focus and pointing to potential areas for further exploration. Github: https://github.com/Yan
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#25968;&#25454;&#21024;&#38500;&#35831;&#27714;&#30340;&#32972;&#26223;&#19979;&#65292;&#21487;&#34892;&#35299;&#37322;&#19982;&#34987;&#36951;&#24536;&#26435;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#12290;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#35777;&#20998;&#26512;&#65292;&#21457;&#29616;&#24403;&#23569;&#37327;&#25968;&#25454;&#21024;&#38500;&#35831;&#27714;&#35302;&#21457;&#39044;&#27979;&#27169;&#22411;&#30340;&#26356;&#26032;&#26102;&#65292;&#30446;&#21069;&#27969;&#34892;&#30340;&#26368;&#20808;&#36827;&#31639;&#27861;&#29983;&#25104;&#30340;&#21487;&#34892;&#35299;&#37322;&#21487;&#33021;&#22833;&#25928;&#12290;</title><link>http://arxiv.org/abs/2208.14137</link><description>&lt;p&gt;
&#35770;&#21487;&#34892;&#35299;&#37322;&#19982;&#34987;&#36951;&#24536;&#26435;&#20043;&#38388;&#30340;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
On the Trade-Off between Actionable Explanations and the Right to be Forgotten. (arXiv:2208.14137v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.14137
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#25968;&#25454;&#21024;&#38500;&#35831;&#27714;&#30340;&#32972;&#26223;&#19979;&#65292;&#21487;&#34892;&#35299;&#37322;&#19982;&#34987;&#36951;&#24536;&#26435;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#12290;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#35777;&#20998;&#26512;&#65292;&#21457;&#29616;&#24403;&#23569;&#37327;&#25968;&#25454;&#21024;&#38500;&#35831;&#27714;&#35302;&#21457;&#39044;&#27979;&#27169;&#22411;&#30340;&#26356;&#26032;&#26102;&#65292;&#30446;&#21069;&#27969;&#34892;&#30340;&#26368;&#20808;&#36827;&#31639;&#27861;&#29983;&#25104;&#30340;&#21487;&#34892;&#35299;&#37322;&#21487;&#33021;&#22833;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#39640;&#39118;&#38505;&#24212;&#29992;&#20013;&#30340;&#37096;&#32626;&#36234;&#26469;&#36234;&#22810;&#65292;&#20915;&#31574;&#32773;&#25552;&#20986;&#20102;&#26356;&#20005;&#26684;&#30340;&#25968;&#25454;&#20445;&#25252;&#27861;&#35268;&#65288;&#20363;&#22914;GDPR&#12289;CCPA&#65289;&#12290;&#20854;&#20013;&#19968;&#20010;&#20851;&#38190;&#21407;&#21017;&#26159;&#8220;&#34987;&#36951;&#24536;&#26435;&#8221;&#65292;&#21363;&#29992;&#25143;&#26377;&#26435;&#35201;&#27714;&#21024;&#38500;&#20854;&#25968;&#25454;&#12290;&#21478;&#19968;&#20010;&#20851;&#38190;&#21407;&#21017;&#26159;&#21487;&#34892;&#35299;&#37322;&#26435;&#65292;&#20063;&#31216;&#20026;&#31639;&#27861;&#36861;&#32034;&#26435;&#65292;&#20801;&#35768;&#29992;&#25143;&#25764;&#38144;&#19981;&#21033;&#30340;&#20915;&#23450;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#23578;&#19981;&#28165;&#26970;&#36825;&#20004;&#20010;&#21407;&#21017;&#26159;&#21542;&#21487;&#20197;&#21516;&#26102;&#23454;&#26045;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#22312;&#25968;&#25454;&#21024;&#38500;&#35831;&#27714;&#30340;&#32972;&#26223;&#19979;&#24341;&#20837;&#24182;&#30740;&#31350;&#20102;&#36861;&#32034;&#26080;&#25928;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20174;&#29702;&#35770;&#21644;&#23454;&#35777;&#35282;&#24230;&#20998;&#26512;&#20102;&#27969;&#34892;&#30340;&#26368;&#20808;&#36827;&#31639;&#27861;&#30340;&#34892;&#20026;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20123;&#31639;&#27861;&#29983;&#25104;&#30340;&#36861;&#32034;&#24456;&#21487;&#33021;&#20250;&#22312;&#23569;&#37327;&#25968;&#25454;&#21024;&#38500;&#35831;&#27714;&#65288;&#20363;&#22914;1&#25110;2&#20010;&#65289;&#24341;&#21457;&#23545;&#39044;&#27979;&#27169;&#22411;&#30340;&#26356;&#26032;&#26102;&#22833;&#25928;&#12290;&#23545;&#20110;&#19981;&#21487;&#21306;&#20998;&#27169;&#22411;&#30340;&#35774;&#32622;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
As machine learning (ML) models are increasingly being deployed in high-stakes applications, policymakers have suggested tighter data protection regulations (e.g., GDPR, CCPA). One key principle is the "right to be forgotten" which gives users the right to have their data deleted. Another key principle is the right to an actionable explanation, also known as algorithmic recourse, allowing users to reverse unfavorable decisions. To date, it is unknown whether these two principles can be operationalized simultaneously. Therefore, we introduce and study the problem of recourse invalidation in the context of data deletion requests. More specifically, we theoretically and empirically analyze the behavior of popular state-of-the-art algorithms and demonstrate that the recourses generated by these algorithms are likely to be invalidated if a small number of data deletion requests (e.g., 1 or 2) warrant updates of the predictive model. For the setting of differentiable models, we suggest a fra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#36164;&#28304;&#21463;&#38480;&#36793;&#32536;&#35774;&#22791;&#19979;&#22823;&#22411;&#27169;&#22411;&#32852;&#37030;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20027;&#20307;&#23376;&#27169;&#22411;&#35757;&#32451;&#65292;&#22312;&#19981;&#36829;&#21453;&#38544;&#31169;&#25215;&#35834;&#30340;&#21069;&#25552;&#19979;&#65292;&#20351;&#24369;&#20294;&#37325;&#35201;&#30340;&#23458;&#25143;&#31471;&#33021;&#22815;&#21442;&#19982;&#21040;&#21327;&#20316;&#35757;&#32451;&#20013;&#12290;</title><link>http://arxiv.org/abs/2208.13141</link><description>&lt;p&gt;
&#22312;&#36793;&#32536;&#36827;&#34892;&#20027;&#20307;&#23376;&#27169;&#22411;&#35757;&#32451;&#30340;&#22823;&#22411;&#27169;&#22411;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Learning of Large Models at the Edge via Principal Sub-Model Training. (arXiv:2208.13141v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.13141
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#36164;&#28304;&#21463;&#38480;&#36793;&#32536;&#35774;&#22791;&#19979;&#22823;&#22411;&#27169;&#22411;&#32852;&#37030;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20027;&#20307;&#23376;&#27169;&#22411;&#35757;&#32451;&#65292;&#22312;&#19981;&#36829;&#21453;&#38544;&#31169;&#25215;&#35834;&#30340;&#21069;&#25552;&#19979;&#65292;&#20351;&#24369;&#20294;&#37325;&#35201;&#30340;&#23458;&#25143;&#31471;&#33021;&#22815;&#21442;&#19982;&#21040;&#21327;&#20316;&#35757;&#32451;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#20998;&#25955;&#24335;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#23454;&#29616;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#21327;&#20316;&#35757;&#32451;&#65292;&#26080;&#38656;&#20849;&#20139;&#31169;&#26377;&#25968;&#25454;&#65292;&#20063;&#26080;&#38656;&#20256;&#36755;&#33267;&#20013;&#24515;&#26381;&#21153;&#22120;&#12290;&#28982;&#32780;&#65292;&#32771;&#34385;&#21040;&#35768;&#22810;&#36793;&#32536;&#23458;&#25143;&#31471;&#32570;&#20047;&#36275;&#22815;&#30340;&#35745;&#31639;&#12289;&#20869;&#23384;&#25110;&#36890;&#20449;&#33021;&#21147;&#65292;&#32852;&#37030;&#23398;&#20064;&#22823;&#35268;&#27169;&#27169;&#22411;&#20173;&#38754;&#20020;&#37325;&#22823;&#29942;&#39048;&#12290;&#20026;&#20102;&#20445;&#25345;&#24369;&#20294;&#37325;&#35201;&#30340;&#23458;&#25143;&#31471;&#21442;&#19982;&#65292;&#20043;&#21069;&#30340;&#24037;&#20316;&#35201;&#20040;&#32771;&#34385;&#20102;&#24322;&#26500;&#23458;&#25143;&#31471;&#35774;&#32622;&#65292;&#20854;&#20013;&#23458;&#25143;&#31471;&#20351;&#29992;&#19981;&#21516;&#22823;&#23567;&#30340;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65307;&#35201;&#20040;&#23558;&#35757;&#32451;&#20219;&#21153;&#36716;&#31227;&#21040;&#26381;&#21153;&#31471;&#12290;&#28982;&#32780;&#65292;&#24322;&#26500;&#23458;&#25143;&#31471;&#35774;&#32622;&#35201;&#27714;&#26576;&#20123;&#23458;&#25143;&#31471;&#35757;&#32451;&#23436;&#25972;&#27169;&#22411;&#65292;&#36825;&#19982;&#36164;&#28304;&#21463;&#38480;&#30340;&#35774;&#32622;&#19981;&#19968;&#33268;&#65307;&#32780;&#21518;&#32773;&#22312;&#19982;&#26381;&#21153;&#22120;&#20849;&#20139;&#20013;&#38388;&#34920;&#31034;&#25110;&#26631;&#31614;&#26102;&#36829;&#21453;&#20102;FL&#30340;&#38544;&#31169;&#25215;&#35834;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22312;&#29616;&#23454;&#20013;&#25552;&#20986;&#20102;&#19968;&#20010;&#26356;&#23569;&#25506;&#32034;&#30340;&#36328;&#35774;&#22791;FL&#35774;&#32622;&#65292;&#20854;&#20013;&#27809;&#26377;&#20219;&#20309;&#23458;&#25143;&#31471;&#38656;&#35201;&#35757;&#32451;&#23436;&#25972;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is emerging as a popular, promising decentralized learning framework that enables collaborative training among clients, with no need to share private data between them or to a centralized server. However, considering many edge clients do not have sufficient computing, memory, or communication capabilities, federated learning of large models still faces significant bottlenecks. To keep such weak but crucial clients in the loop, prior works either consider a heterogeneous-client setting where clients train models with different sizes; or offload training to the server. However, the heterogeneous-client setting requires some clients to train full model, which is not aligned with the resource-constrained setting; while the latter ones break privacy promises in FL when sharing intermediate representations or labels with the server. To overcome these limitations, in this work, we formulate a realistic, but much less explored, cross-device FL setting in which no client
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21333;&#19968;&#19968;&#20803;&#20195;&#25968;&#20013;&#30340;&#31867;&#27604;&#27604;&#20363;&#65292;&#22312;&#30001;&#33258;&#28982;&#25968;&#21644;&#21518;&#32487;&#20989;&#25968;&#32452;&#25104;&#30340;&#26080;&#38480;&#21333;&#19968;&#19968;&#20803;&#20195;&#25968;&#20013;&#65292;&#31867;&#27604;&#27604;&#20363;&#20851;&#31995;&#21487;&#20197;&#36890;&#36807;&#24046;&#21035;&#27604;&#20363;&#36827;&#34892;&#25551;&#36848;&#12290;</title><link>http://arxiv.org/abs/2208.06829</link><description>&lt;p&gt;
&#21333;&#19968;&#19968;&#20803;&#20195;&#25968;&#20013;&#30340;&#31867;&#27604;&#27604;&#20363;
&lt;/p&gt;
&lt;p&gt;
Analogical proportions in monounary algebras. (arXiv:2208.06829v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.06829
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21333;&#19968;&#19968;&#20803;&#20195;&#25968;&#20013;&#30340;&#31867;&#27604;&#27604;&#20363;&#65292;&#22312;&#30001;&#33258;&#28982;&#25968;&#21644;&#21518;&#32487;&#20989;&#25968;&#32452;&#25104;&#30340;&#26080;&#38480;&#21333;&#19968;&#19968;&#20803;&#20195;&#25968;&#20013;&#65292;&#31867;&#27604;&#27604;&#20363;&#20851;&#31995;&#21487;&#20197;&#36890;&#36807;&#24046;&#21035;&#27604;&#20363;&#36827;&#34892;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#30001;&#19968;&#20010;&#23431;&#23449;&#21644;&#19968;&#20010;&#21333;&#19968;&#19968;&#20803;&#20989;&#25968;&#32452;&#25104;&#30340;&#21333;&#19968;&#19968;&#20803;&#20195;&#25968;&#20013;&#30340;&#31867;&#27604;&#27604;&#20363;&#12290;&#25105;&#20204;&#36890;&#36807;&#24046;&#21035;&#27604;&#20363;&#34920;&#26126;&#65292;&#31867;&#27604;&#27604;&#20363;&#20851;&#31995;&#21487;&#20197;&#22312;&#30001;&#33258;&#28982;&#25968;&#21644;&#21518;&#32487;&#20989;&#25968;&#32452;&#25104;&#30340;&#26080;&#38480;&#21333;&#19968;&#19968;&#20803;&#20195;&#25968;&#20013;&#36827;&#34892;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies analogical proportions in monounary algebras consisting only of a universe and a single unary function. We show that the analogical proportion relation is characterized in the infinite monounary algebra formed by the natural numbers together with the successor function via difference proportions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#30693;&#35782;&#26174;&#33879;&#36328;&#24230;&#25513;&#30721;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20840;&#33258;&#30417;&#30563;&#23398;&#20064;&#24110;&#21161;&#35821;&#35328;&#27169;&#22411;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#33719;&#21462;&#26356;&#22810;&#30340;&#30693;&#35782;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2204.07994</link><description>&lt;p&gt;
&#30693;&#35782;&#26174;&#33879;&#36328;&#24230;&#25513;&#30721;&#65306;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#30693;&#35782;&#24211;
&lt;/p&gt;
&lt;p&gt;
Knowledgeable Salient Span Mask for Enhancing Language Models as Knowledge Base. (arXiv:2204.07994v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.07994
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#30693;&#35782;&#26174;&#33879;&#36328;&#24230;&#25513;&#30721;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20840;&#33258;&#30417;&#30563;&#23398;&#20064;&#24110;&#21161;&#35821;&#35328;&#27169;&#22411;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#33719;&#21462;&#26356;&#22810;&#30340;&#30693;&#35782;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22914;BERT&#22312;&#21508;&#31181;&#19979;&#28216;NLP&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#35201;&#27714;&#27169;&#22411;&#36827;&#34892;&#22635;&#31354;&#24335;&#27979;&#35797;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;PLMs&#22312;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#33719;&#21462;&#30693;&#35782;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#12290;&#20026;&#20102;&#20102;&#35299;PLMs&#22312;&#26816;&#32034;&#30693;&#35782;&#26102;&#30340;&#20869;&#37096;&#34892;&#20026;&#65292;&#25105;&#20204;&#39318;&#20808;&#20026;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#23450;&#20041;&#20102;&#30693;&#35782;&#21487;&#35265;&#65288;K-B&#65289;&#26631;&#35760;&#21644;&#26080;&#30693;&#35782;&#65288;K-F&#65289;&#26631;&#35760;&#65292;&#24182;&#35831;&#19987;&#19994;&#26631;&#27880;&#21592;&#25163;&#21160;&#26631;&#35760;&#20102;&#19968;&#20123;&#26679;&#26412;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21457;&#29616;PLMs&#26356;&#26377;&#21487;&#33021;&#23545;K-B&#26631;&#35760;&#32473;&#20986;&#38169;&#35823;&#39044;&#27979;&#65292;&#24182;&#19988;&#22312;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#20869;&#37096;&#23545;&#36825;&#20123;&#26631;&#35760;&#20851;&#27880;&#30340;&#26356;&#23569;&#12290;&#22522;&#20110;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#65292;&#25105;&#20204;&#20197;&#20840;&#33258;&#30417;&#30563;&#30340;&#26041;&#24335;&#24320;&#21457;&#20102;&#20004;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#24110;&#21161;&#27169;&#22411;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#23398;&#20064;&#26356;&#22810;&#30340;&#30693;&#35782;&#12290;&#22312;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#25506;&#32034;&#20840;&#33258;&#30417;&#30563;&#23398;&#20064;&#25345;&#32493;&#39044;&#35757;&#32451;&#30693;&#35782;&#30340;&#30740;&#31350;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models (PLMs) like BERT have made significant progress in various downstream NLP tasks. However, by asking models to do cloze-style tests, recent work finds that PLMs are short in acquiring knowledge from unstructured text. To understand the internal behaviour of PLMs in retrieving knowledge, we first define knowledge-baring (K-B) tokens and knowledge-free (K-F) tokens for unstructured text and ask professional annotators to label some samples manually. Then, we find that PLMs are more likely to give wrong predictions on K-B tokens and attend less attention to those tokens inside the self-attention module. Based on these observations, we develop two solutions to help the model learn more knowledge from unstructured text in a fully self-supervised manner. Experiments on knowledge-intensive tasks show the effectiveness of the proposed methods. To our best knowledge, we are the first to explore fully self-supervised learning of knowledge in continual pre-training.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20855;&#26377;&#36801;&#31227;&#23398;&#20064;&#21644;&#26080;&#36801;&#31227;&#23398;&#20064;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20197;&#21450;&#20854;&#20182;&#20256;&#32479;&#26041;&#27861;&#22312;&#26376;&#24230;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#26174;&#31034;&#22522;&#20110;TCN&#12289;LSTM&#21644;CNN&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21152;&#19978;&#36801;&#31227;&#23398;&#20064;&#30340;&#24615;&#33021;&#36229;&#36807;&#20102;&#20854;&#20182;&#20256;&#32479;&#26041;&#27861;&#65292;&#24182;&#19988;&#30452;&#25509;&#22312;&#30446;&#26631;&#26102;&#38388;&#24207;&#21015;&#19978;&#35757;&#32451;&#30340;TCN&#21644;LSTM&#22312;&#26576;&#20123;&#39044;&#27979;&#26102;&#27573;&#30340;&#24615;&#33021;&#30456;&#24403;&#25110;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2203.11196</link><description>&lt;p&gt;
&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#26376;&#24230;&#26102;&#38388;&#24207;&#21015;&#30340;&#22810;&#27493;&#39044;&#27979;&#20013;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Performance of Deep Learning models with transfer learning for multiple-step-ahead forecasts in monthly time series. (arXiv:2203.11196v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.11196
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20855;&#26377;&#36801;&#31227;&#23398;&#20064;&#21644;&#26080;&#36801;&#31227;&#23398;&#20064;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20197;&#21450;&#20854;&#20182;&#20256;&#32479;&#26041;&#27861;&#22312;&#26376;&#24230;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#26174;&#31034;&#22522;&#20110;TCN&#12289;LSTM&#21644;CNN&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21152;&#19978;&#36801;&#31227;&#23398;&#20064;&#30340;&#24615;&#33021;&#36229;&#36807;&#20102;&#20854;&#20182;&#20256;&#32479;&#26041;&#27861;&#65292;&#24182;&#19988;&#30452;&#25509;&#22312;&#30446;&#26631;&#26102;&#38388;&#24207;&#21015;&#19978;&#35757;&#32451;&#30340;TCN&#21644;LSTM&#22312;&#26576;&#20123;&#39044;&#27979;&#26102;&#27573;&#30340;&#24615;&#33021;&#30456;&#24403;&#25110;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#27169;&#22411;&#34987;&#29992;&#20110;&#29983;&#25104;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65292;&#28982;&#32780;&#23545;&#20110;&#26376;&#24230;&#26102;&#38388;&#24207;&#21015;&#30340;&#24615;&#33021;&#39044;&#27979;&#65292;&#32570;&#20047;&#35777;&#25454;&#12290;&#26412;&#25991;&#26088;&#22312;&#27604;&#36739;&#20855;&#26377;&#36801;&#31227;&#23398;&#20064;&#21644;&#27809;&#26377;&#36801;&#31227;&#23398;&#20064;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20197;&#21450;&#20854;&#20182;&#20256;&#32479;&#26041;&#27861;&#22312;&#26376;&#24230;&#39044;&#27979;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#22238;&#31572;&#19977;&#20010;&#20851;&#20110;&#28145;&#24230;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#29983;&#25104;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#20351;&#29992;&#20102;M4&#21644;M3&#31454;&#36187;&#30340;&#26102;&#38388;&#24207;&#21015;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;TCN&#12289;LSTM&#21644;CNN&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21152;&#19978;&#36801;&#31227;&#23398;&#20064;&#24448;&#24448;&#33021;&#22815;&#36229;&#36807;&#20854;&#20182;&#20256;&#32479;&#26041;&#27861;&#30340;&#24615;&#33021;&#39044;&#27979;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#23545;&#20110;&#26576;&#20123;&#39044;&#27979;&#26102;&#27573;&#65292;&#30452;&#25509;&#22312;&#30446;&#26631;&#26102;&#38388;&#24207;&#21015;&#19978;&#35757;&#32451;&#30340;TCN&#21644;LSTM&#30340;&#24615;&#33021;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#24403;&#25110;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Learning and transfer learning models are being used to generate time series forecasts; however, there is scarce evidence about their performance prediction that it is more evident for monthly time series. The purpose of this paper is to compare Deep Learning models with transfer learning and without transfer learning and other traditional methods used for monthly forecasts to answer three questions about the suitability of Deep Learning and Transfer Learning to generate predictions of time series. Time series of M4 and M3 competitions were used for the experiments. The results suggest that deep learning models based on TCN, LSTM, and CNN with transfer learning tend to surpass the performance prediction of other traditional methods. On the other hand, TCN and LSTM, trained directly on the target time series, got similar or better performance than traditional methods for some forecast horizons.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25551;&#36848;&#20102;&#19968;&#20010;&#26426;&#22120;&#20154;&#23398;&#20064;&#31995;&#32479;&#65292;&#21487;&#29992;&#20110;&#33258;&#20027;&#25506;&#32034;&#21644;&#23548;&#33322;&#19981;&#21516;&#30340;&#24320;&#25918;&#19990;&#30028;&#29615;&#22659;&#12290;&#35813;&#31995;&#32479;&#36890;&#36807;&#23398;&#20064;&#24471;&#21040;&#19968;&#20010;&#28508;&#22312;&#21464;&#37327;&#27169;&#22411;&#21644;&#22270;&#20687;&#30340;&#25299;&#25169;&#35760;&#24518;&#65292;&#20351;&#29992;&#20449;&#24687;&#29942;&#39048;&#32422;&#26463;&#31574;&#30053;&#65292;&#33021;&#22815;&#22312;&#19981;&#21040;20&#20998;&#38047;&#20869;&#36890;&#36807;&#35270;&#35273;&#30446;&#26631;&#34920;&#31034;&#22312;&#24320;&#25918;&#19990;&#30028;&#20013;&#25506;&#32034;&#24182;&#21457;&#29616;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2104.05859</link><description>&lt;p&gt;
&#20351;&#29992;&#28508;&#22312;&#30446;&#26631;&#27169;&#22411;&#36827;&#34892;&#24555;&#36895;&#24320;&#25918;&#19990;&#30028;&#23548;&#33322;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Rapid Exploration for Open-World Navigation with Latent Goal Models. (arXiv:2104.05859v5 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2104.05859
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25551;&#36848;&#20102;&#19968;&#20010;&#26426;&#22120;&#20154;&#23398;&#20064;&#31995;&#32479;&#65292;&#21487;&#29992;&#20110;&#33258;&#20027;&#25506;&#32034;&#21644;&#23548;&#33322;&#19981;&#21516;&#30340;&#24320;&#25918;&#19990;&#30028;&#29615;&#22659;&#12290;&#35813;&#31995;&#32479;&#36890;&#36807;&#23398;&#20064;&#24471;&#21040;&#19968;&#20010;&#28508;&#22312;&#21464;&#37327;&#27169;&#22411;&#21644;&#22270;&#20687;&#30340;&#25299;&#25169;&#35760;&#24518;&#65292;&#20351;&#29992;&#20449;&#24687;&#29942;&#39048;&#32422;&#26463;&#31574;&#30053;&#65292;&#33021;&#22815;&#22312;&#19981;&#21040;20&#20998;&#38047;&#20869;&#36890;&#36807;&#35270;&#35273;&#30446;&#26631;&#34920;&#31034;&#22312;&#24320;&#25918;&#19990;&#30028;&#20013;&#25506;&#32034;&#24182;&#21457;&#29616;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#20010;&#29992;&#20110;&#33258;&#20027;&#25506;&#32034;&#21644;&#23548;&#33322;&#22810;&#26679;&#21270;&#24320;&#25918;&#19990;&#30028;&#29615;&#22659;&#30340;&#26426;&#22120;&#20154;&#23398;&#20064;&#31995;&#32479;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#23398;&#20064;&#24471;&#21040;&#30340;&#36317;&#31163;&#21644;&#21160;&#20316;&#30340;&#28508;&#22312;&#21464;&#37327;&#27169;&#22411;&#65292;&#20197;&#21450;&#19968;&#20010;&#38750;&#21442;&#25968;&#30340;&#22270;&#20687;&#25299;&#25169;&#35760;&#24518;&#12290;&#25105;&#20204;&#20351;&#29992;&#20449;&#24687;&#29942;&#39048;&#26469;&#35268;&#33539;&#23398;&#20064;&#30340;&#31574;&#30053;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#24471;&#21040;(i)&#32039;&#20945;&#30340;&#30446;&#26631;&#35270;&#35273;&#34920;&#31034;&#65292;(ii)&#25913;&#36827;&#30340;&#27867;&#21270;&#33021;&#21147;&#20197;&#21450;(iii)&#29992;&#20110;&#25506;&#32034;&#30340;&#21487;&#34892;&#30446;&#26631;&#37319;&#26679;&#26426;&#21046;&#12290;&#36890;&#36807;&#22312;&#22823;&#35268;&#27169;&#31163;&#32447;&#32463;&#39564;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#35813;&#27169;&#22411;&#33719;&#24471;&#20102;&#23545;&#20110;&#20219;&#21153;&#26080;&#20851;&#24178;&#25200;&#29289;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#35270;&#35273;&#30446;&#26631;&#34920;&#31034;&#12290;&#25105;&#20204;&#22312;&#24320;&#25918;&#19990;&#30028;&#25506;&#32034;&#22330;&#26223;&#20013;&#21033;&#29992;&#31227;&#21160;&#22320;&#38754;&#26426;&#22120;&#20154;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#32473;&#23450;&#19968;&#20010;&#36317;&#31163;&#39640;&#36798;80&#31859;&#30340;&#30446;&#26631;&#22270;&#20687;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20854;&#34920;&#31034;&#22312;&#19981;&#21040;20&#20998;&#38047;&#20869;&#25506;&#32034;&#24182;&#21457;&#29616;&#20102;&#30446;&#26631;&#65292;&#21363;&#20351;&#22312;&#20197;&#21069;&#26410;&#35265;&#30340;&#38556;&#30861;&#21644;&#22825;&#27668;&#26465;&#20214;&#19979;&#20063;&#33021;&#23454;&#29616;&#12290;&#35831;&#26597;&#30475;&#39033;&#30446;&#32593;&#31449;&#19978;&#30340;&#35270;&#39057;&#12290;
&lt;/p&gt;
&lt;p&gt;
We describe a robotic learning system for autonomous exploration and navigation in diverse, open-world environments. At the core of our method is a learned latent variable model of distances and actions, along with a non-parametric topological memory of images. We use an information bottleneck to regularize the learned policy, giving us (i) a compact visual representation of goals, (ii) improved generalization capabilities, and (iii) a mechanism for sampling feasible goals for exploration. Trained on a large offline dataset of prior experience, the model acquires a representation of visual goals that is robust to task-irrelevant distractors. We demonstrate our method on a mobile ground robot in open-world exploration scenarios. Given an image of a goal that is up to 80 meters away, our method leverages its representation to explore and discover the goal in under 20 minutes, even amidst previously-unseen obstacles and weather conditions. Please check out the project website for videos o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#21629;&#39064;&#36923;&#36753;&#31243;&#24207;&#30340;&#39034;&#24207;&#32452;&#21512;&#21644;&#20998;&#35299;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26725;&#25509;&#21629;&#39064;&#36923;&#36753;&#31243;&#24207;&#30340;&#35821;&#27861;&#19982;&#35821;&#20041;&#30340;&#25968;&#23398;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2009.05774</link><description>&lt;p&gt;
&#21629;&#39064;&#36923;&#36753;&#31243;&#24207;&#30340;&#39034;&#24207;&#32452;&#21512;
&lt;/p&gt;
&lt;p&gt;
Sequential composition of propositional logic programs. (arXiv:2009.05774v7 [cs.LO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2009.05774
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#21629;&#39064;&#36923;&#36753;&#31243;&#24207;&#30340;&#39034;&#24207;&#32452;&#21512;&#21644;&#20998;&#35299;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26725;&#25509;&#21629;&#39064;&#36923;&#36753;&#31243;&#24207;&#30340;&#35821;&#27861;&#19982;&#35821;&#20041;&#30340;&#25968;&#23398;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#21644;&#30740;&#31350;&#20102;&#21629;&#39064;&#36923;&#36753;&#31243;&#24207;&#30340;&#39034;&#24207;&#32452;&#21512;&#21644;&#20998;&#35299;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26080;&#29615;&#31243;&#24207;&#21487;&#20197;&#34987;&#20998;&#35299;&#20026;&#21333;&#35268;&#21017;&#31243;&#24207;&#65292;&#24182;&#32473;&#20986;&#20102;&#20219;&#24847;&#31243;&#24207;&#30340;&#19968;&#33324;&#20998;&#35299;&#32467;&#26524;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#31243;&#24207;&#30340;&#30452;&#25509;&#25512;&#35770;&#31639;&#23376;&#21487;&#20197;&#36890;&#36807;&#32452;&#21512;&#26469;&#34920;&#31034;&#65292;&#20174;&#32780;&#21487;&#20197;&#35745;&#31639;&#20854;&#26368;&#23567;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#26174;&#24335;&#24341;&#29992;&#31639;&#23376;&#12290;&#36825;&#22312;&#25968;&#23398;&#19978;&#28385;&#24847;&#22320;&#22635;&#34917;&#20102;&#21629;&#39064;&#36923;&#36753;&#31243;&#24207;&#30340;&#35821;&#27861;&#19982;&#35821;&#20041;&#20043;&#38388;&#30340;&#27010;&#24565;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces and studies the sequential composition and decomposition of propositional logic programs. We show that acyclic programs can be decomposed into single-rule programs and provide a general decomposition result for arbitrary programs. We show that the immediate consequence operator of a program can be represented via composition which allows us to compute its least model without any explicit reference to operators. This bridges the conceptual gap between the syntax and semantics of a propositional logic program in a mathematically satisfactory way.
&lt;/p&gt;</description></item></channel></rss>